{
  "paperId": "716290601bbe5b90b785a72887b79c17d62ee9cd",
  "title": "Accelerating String-key Learned Index Structures via Memoization-based Incremental Training",
  "pdfPath": "716290601bbe5b90b785a72887b79c17d62ee9cd.pdf",
  "text": "Accelerating String-key Learned Index Structures\nvia Memoization-based Incremental Training\nMinsu Kim\nKAIST\nmskim@casys.kaist.ac.krJinwoo Hwang\nKAIST\njwhwang@casys.kaist.ac.krGuseul Heo\nKAIST\ngsheo@casys.kaist.ac.kr\nSeiyeon Cho\nKAIST\nsycho@casys.kaist.ac.krDivya Mahajan\nGeorgia Tech\ndivya.mahajan@gatech.eduJongse Park\nKAIST\njspark@casys.kaist.ac.kr\nABSTRACT\nLearned indexes use machine learning models to learn the mappings\nbetween keys and their corresponding positions in key-value in-\ndexes. These indexes use the mapping information as training data.\nLearned indexes require frequent retrainings of their models to in-\ncorporate the changes introduced by update queries. To efficiently\nretrain the models, existing learned index systems often harness a\nlinear algebraic QR factorization technique that performs matrix de-\ncomposition. This factorization approach processes all key-position\npairs during each retraining, resulting in compute operations that\ngrow linearly with the total number of keys and their lengths. Con-\nsequently, the retrainings create a severe performance bottleneck,\nespecially for variable-length string keys, while the retrainings are\ncrucial for maintaining high prediction accuracy and in turn, ensur-\ning low query service latency.\nTo address this performance problem, we develop an algorithm-\nhardware co-designed string-key learned index system, dubbed SIA.\nIn designing SIA, we leverage a unique algorithmic property of the\nmatrix decomposition-based training method. Exploiting the prop-\nerty, we develop a memoization-based incremental training scheme,\nwhich only requires computation over updated keys, while decompo-\nsition results of non-updated keys from previous computations can\nbe reused. We further enhance SIAto offload a portion of this training\nprocess to an FPGA accelerator to not only relieve CPU resources for\nserving index queries (i.e., inference), but also accelerate the training\nitself. Our evaluation shows that compared to ALEX, LIPP, and SIn-\ndex, a state-of-the-art learned index systems, SIA-accelerated learned\nindexes offer 2.6Ã—and 3.4Ã—higher throughput on the two real-world\nbenchmark suites, YCSB and Twitter cache trace, respectively.\nPVLDB Reference Format:\nMinsu Kim, Jinwoo Hwang, Guseul Heo, Seiyeon Cho, Divya Mahajan,\nand Jongse Park. Accelerating String-key Learned Index Structures\nvia Memoization-based Incremental Training. PVLDB, 17(1): XXX-XXX, 2024.\ndoi:XX.XX/XXX.XX\nPVLDB Artifact Availability:\nThis work is licensed under the Creative Commons BY-NC-ND\n4.0 International License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/\nto view a copy of this license. For any use beyond\nthose covered by this license, obtain permission by emailing info@vldb.org. Copyright\nis held by the owner/author(s). Publication rights licensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 17, No. 1 ISSN 2150-8097.\ndoi:XX.XX/XXX.XX\nTimeFigure 1: Increasing retraining time as the size of a learned\nindex system grows, resulting from a stream of update queries.\nMarkers on the same line represent sequential retraining\nruns, where markers positioned to the left precede those on\nthe right. We use various key lengths - 16, 32, 64, and 96.\nThe source code, data, and/or other artifacts have been made available at\nhttps://github.com/sia-index/sia.\n1 INTRODUCTION\nMachine learning for system infrastructure is growing particularly in\nareas where data-driven decisions can make meaningful strides [ 7,19,\n59]. Efficient data access is one such avenue, where learning indexes\nhaveproventobeeffectiveandpractical[ 1,11,12,16,26,27,29,33â€“36,\n38,41â€“43,51,52,54â€“58,61,62,69,70,72]. The pioneering work [ 27]\nproposed in this space uses a collection of machine learning models to\ncreate a read-only ordered index for integer keys. Due to its popular-\nity and applicability, numerous follow-up research projects have ex-\ntended the initial work to support read-write (updatable) indexes [ 11,\n29,33,35,56,58,61,62,65], string keys [ 53,58,60], multi-dimensional\nindexes [ 12,16,43,57], spatial indexes [ 34,51,69,72], and other vari-\nants [ 36,38,54,70]. This paper focuses on identifying performance\nchallenges of updatable string-key learned indexes and addressing\nthe challenges through an algorithm-hardware co-designed solution.\nRegardless of the data types of keys, an algorithmic common-\nality among most existing learned indexes is that the indexes are\nconstructed as a hierarchical structure where each node is a lin-\near model [ 10â€“12,27,33,35,36,56,58,61,62,70,71]. These linear\nmodels are designed to collaboratively learn the mappings between\nkeys and their corresponding positions, using this information as\ntraining data. The training process is inherently repetitive since the\nkey-position mappings constantly change due to the update queries\n(e.g., insert or delete), which necessitates retrainings to incorporate\nthe changes into the models.arXiv:2403.11472v1  [cs.LG]  18 Mar 2024\n\nIn learned indexes, training of linear models is essentially solving\nthe following linear equation, ğ‘‹ğ›½=ğ‘Œwhereğ‘‹is a key matrix, ğ›½is\na learnable parameter vector, and ğ‘Œis the corresponding position\nvector. When learned indexes only support integer keys, the training\nprocess is computationally trivial since ğ‘‹is a vector of integer key\nvalues (i.e.,ğ‘›Ã—1matrix). However, when the keys are variable-length\nstrings,ğ‘‹becomes ağ‘›Ã—ğ‘˜-size matrix where ğ‘˜is the key length,\nwhich makes solving the equation a computationally non-trivial\ntask. To algorithmically reduce the compute load of this training,\nexisting string-key learned indexes [27, 53, 58, 60] employ a matrix\nfactorization strategy known as QR decomposition , which enables\ntraining to be free from the burdens of matrix inversion.\nDespite the algorithmic optimization, we observe that in the ex-\nisting systems, the repetitive retrainings incur a severe performance\nbottleneck, since (1) the complexity of QR decomposition, although\nlower than matrix inversion, remains high, and (2) retrainings and\nindex query servicing for existing keys (i.e., inference) compete for\nthe limited CPU resource. Figure 1 shows that retraining time pro-\ngressively grows as the number of keys and key lengths increase, on\na state-of-the-art string-key learned index, SIndex [ 58]. Each point\nin the graph represents a retraining run. Increased retraining times\nnegatively impact the inference throughput, as they result in an\noutdated index. This, in turn, lowers the index prediction accuracy\nand necessitates a costly linear search to locate the correct position.\nThus, retraining is crucial for reducing service latency as well as\nimproving index throughput.\nTo address the aforementioned bottlenecks, we introduce SIA:\nString-key Learned Index Acceleration. SIAenables efficient and\nscalable indexing by reducing the compute load of the retraining\nprocess through an algorithmic technique and judiciously offloads\na portion of the training computation onto an FPGA accelerator.\nThe challenge is that current learned indexes need to perform costly\nmatrix decomposition using the entire key-position mappings as\ninput to maintain model accuracy, which is pivotal for achieving\nhigh index performance. To tackle this challenge, SIAutilizes a mod-\nified parallel decomposition technique that allows for piecewise\ncomputation of matrix decomposition. In designing SIA, we leverage\nthe insight that these retrainings occur on progressively updated\nindexes, thus offering an opportunity to reuse computations from\nprior results via memoization. It is important to note that training\nusing the memoized decomposition results produces mathemati-\ncally identical outcomes to those obtained if the models were fully\nretrained from scratch using the complete set of keys.\nBuilding on the memoization-based decomposition, we develop\na learned index training algorithm that incrementally retrains the\nmodels by leveraging the results of prior matrix decomposition. This\nenhanced algorithm reduces the computational complexity and re-\ntraining time, which in turn frees up CPU resources for servicing\nqueries. However, our empirical analyses suggest that the algorith-\nmic optimization, while helpful, offers a limited benefit since the\nretrainings still compete over the limited CPU resource. To further\nreduce the retraining time, we enable the retrainings to be accel-\nerated using an FPGA. We choose FPGA over GPU owing to its\ncustomizability to index-specific algorithm configurations, leading\nto enhanced energy efficiency. SIAcombines these elements to offer\na novel learned index mechanism that aims to improve system querythroughput through both algorithmic and hardware innovations.\nThis work makes the following contributions:\nâ€¢Identifiesthesystembottlenecksincurrentupdatablelearned\nindex structures for string-keys, specifically, retraining the\nensemble models in the hierarchical structure. We observe\nthat as the retraining time grows, it progressively leads to\nlower performance of learned index systems.\nâ€¢Introducesanovellearnedindexsystem, SIA,thataccelerates\nthe retraining process through an enhanced mathematical\napproach to matrix decomposition, enabling incremental\ntraining. With incremental training, only updated keys are\nused for computation, while the computation result for old\nkeys is reused.\nâ€¢Further accelerates SIAâ€™s incremental training process using\nan FPGA-based design that reduces training time and frees\nup CPU resources for index query servicing.\nWe demonstrate the effectiveness of SIAusing two real-world\nbenchmark suites, YCSB and Twitter cache trace. For YCSB, we use\ntwo datasets available to the public, Amazon review and Meme-\nTracker datasets, as well as a synthetic dataset. We integrate SIA\ninto the three recent updatable string-key learned indexes, includ-\ning ALEX [ 11], LIPP [ 61], and SIndex [ 58]. Compared to baseline\nlearned indexes, SIA-accelerated learned indexes provide 2.6 Ã—and\n3.4Ã—higher throughput for YCSB and Twitter cache trace workloads,\nrespectively. From an in-depth ablation study using SIndex that\nbreaksdownthebenefitsof SIA,weobservethatemployingsolelythe\nmemoization decomposition-based incremental learning algorithm\noffers 1.6Ã—and 1.9Ã—higher throughput. However, when the FPGA-\nbased SIAaccelerator is employed, it offers 2.8 Ã—and 4.3Ã—higher\nthroughput than the baselines, which are respectively 1.8 Ã—and 2.3Ã—\nadditional speedup, a substantial performance boost compared to\nthe software-only counterpart. These results suggest that taking an\nalgorithm-hardware co-design approach, SIAenables heterogeneous\nCPU-FPGA architecture to operate as a platform of choice to achieve\nhigh throughput for updatable string-key learned indexes.\n2 A PRIMER ON LEARNED INDEX\nKey-value stores are widely deployed in data management appli-\ncations, where the index maps keys to their corresponding posi-\ntions in a list of records. This pairing can be denoted as a function,\nğ‘“(ğ‘˜ğ‘’ğ‘¦,ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›), with key as the input and position as the output.\nConventionally, hash-map and B-tree structures are commonly used\nto store this mapping in an array of records. Despite its popular-\nity, they still have shown several limitations, which prevent their\nâ€œone-size-fits-allâ€ deployment. While hash-maps typically offer low\naverage access time, they can be susceptible to hash collisions that\nmay lead to unpredictable increases in lookup and construction\ntime. Additionally, hash-maps may not perform as well as other data\nstructures for range queries. On the other hand, B-trees and their\nvariants do not have the same limitations as hash-maps, but their\naverage-case performance, in terms of both latency and throughput,\nis generally lower than that of hash-maps. To overcome these lim-\nitations, the community has explored the use of machine learning\n(ML) approaches to develop learned indexes as an alternative index\nstructure [10â€“13, 27, 34, 41, 56, 58, 61].\n2\n\nG\u0011\u0011G\u0012\u0012G\u0012\u0011G\u0013\u0011G\u0013\u0012G\u0013\u0013G\u0013\u0014,FZQPT(a).PEFMU\f\u0012.PEFMU3U\f\u0012$16LFZ6QEBUF\n(c).FNPJ[F3U6TFNFNPJ[FESFTVMU\u0001'1(\".PEFMU\f\u0012.PEFMU*OGFSFODF\u00015ISFBE5SBJOJOH\u00015ISFBE6QEBUF\n(b)$16LFZFigure 2: (a) Read-only learned index in a hierarchical structure, (b) updatable learned index, and (c) SIA: the proposed updatable\nstring-key learned index that leverages computation reuse and hardware acceleration to improve the system throughput.\nThroughput (ops/sec)WormholeCuckoo TrieALEXLIPPXIndex/SIndex\n(a) Integer keys(b) String keys\nFigure 3: Throughput results of two conventional indexes\nand three learned indexes for YCSB workloads.\nLearning to index keys. The initial work on learned indexes [ 27]\ndemonstrated that it is possible for ML models to learn the map-\npings between keys and their corresponding positions, using this\ninformation as training data. Unlike traditional machine learning\nmodels, which aim to generalize to unseen inputs, learned index\nmodels are intentionally overfit to the training data, as index struc-\ntures mostly encounter keys that have been inserted. Despite the\noverfitting during training, inference-based indexing can still pro-\nduce incorrect predictions due to the inherent approximation nature\nof machine learning. When the queried key is not found at the pre-\ndicted position, learned indexes search for the key within a bounded\nrange around the predicted position (i.e., [ ğ‘+ğ‘’ğ‘Ÿğ‘Ÿğ‘šğ‘–ğ‘›,ğ‘+ğ‘’ğ‘Ÿğ‘Ÿğ‘šğ‘ğ‘¥]),\nensuring accurate indexing functionality [27].\nWhen designing a ML model for learned indexes, there are vari-\nous alternatives that trade off accuracy (model size and architec-\nture) against cost (inference latency and training time). Several\nworks [ 13,27,56,58] have shown that a hierarchical structure of\nlinear models effectively balances this tradeoff. Each node in the hier-\narchical structure is a linear regression model that needs to be trained\nfor a subset of the key-position mappings. These initial works focus\non read-only indexes, and hence training is carried out once when\nbuilding the indexes before deployment. The hierarchical structure\nfor read-only indexing is depicted in Figure 2(a).\nUpdatable string-key learned index. Although restricting the\nscope to read-only indexes was an effective setting to demonstrate\ninitial applicability of the â€œlearningâ€ approach, practical data man-\nagement necessitates support for â€œupdateâ€ queries (e.g., insert and\ndelete ). Follow-up works overcome this limitation and devise â€œup-\ndatableâ€ learned indexes [ 11,56,58,61]. ALEX [ 11] expands nodes\nwith deliberately-reserved empty spaces for unseen future keys,which hold the newly inserted keys until the updated keys are re-\ntrained. LIPP [ 61] ensures precise model prediction results and re-\nmoves costly local search usually used in other learned indexes. XIn-\ndex [ 56] is another variant that maintains reserved spaces for future\nkeys, while unlike ALEX, the new keys are stored in separate tempo-\nrarybuffers.SIndex[ 58]isoneoftheinitialeffortstosupportvariable-\nlength string keys in learned indexes. As string keys are an important\ndatatype used in diverse applications such as web servers, sequence\nanalysis, and genomics, modern key-value stores often have strong\nsupport for this datatype [ 3,18,20,23,28,31,37,58,63,64,67,68].\nDespite its importance, its performance implication on updatable\nstring-key learned index systems remains under-examined in exist-\ning literature, which is the primary focus of this work.\nIntertwinement of retraining and inference. Unlike traditional\nmachine learning, training and inference phases in these updatable\nlearned indexes are not clearly demarcated. Instead, learned indexes\nrequire iterative retrainings , because the training data is constantly\nchanging due to update queries. Concurrently, the index systems\nmust serve index queries by performing inference . This convergence\nof training and inference can influence each otherâ€™s performance,\npotentially resulting in a marked degradation of overall efficiency.\nFigure 2(b) delineates the common execution flow where certain\nthreads are dedicated to query servicing and certain to retraining.\nEffectiveness of learned indexes. To better understand the effec-\ntiveness of learned indexes, we conduct preliminary experiments\ncomparing the throughput of learned index structures with two\nnon-learned indexes, Wormhole [ 63] and Cuckoo Trie [ 68]. We use\nthe Yahoo! Cloud Serving Benchmark (YCSB) [ 8], a key-value store\nbenchmark suite with six different workloads (see Section 7.1 for\ndetails). Figure 3 shows that learned indexes generally offer com-\nparable or higher performance than the two baseline indexes for\nboth integer and string key cases. However, the notable observation\nis that when keys are string, learned indexes perform much worse\nthan the baseline indexes for workload DandE. Workload DandE\ncontain insert queries, which necessitate the constant retrainings\nfor index updates. While these retrainings impose marginal perfor-\nmance overhead when keys are integers, retrainings for string keys\nbecome severe performance bottleneck, cancelling the performance\nbenefits of learned indexes, as will be deeply analyzed in Section 3.\nThis is the very challenge we aim to tackle in this work through SIA.\nOur approach to tackle the challenge. SIAsets out to tackle the\nchallenges posed by current updatable string-key learned indexes,\n3\n\n0 10 20 30 40 50\nInsertion Ratio (%)00.2M0.4M0.6M0.8M1.0MThroughput (ops/sec)5s 30s 100s 300sFigure 4: Throughput as training time varies from 5 to 300\nseconds. Training does not utilize any CPU cycles. The\ninsertion ratio sweeps from 0% (read-only) to 50%.\nwith the following objectives: (1) SIAaims to reduce the cost of train-\ning linear models without any mathematical implication on model\nquality, and (2) it aims to enhance the system with an FPGA accelera-\ntor that can execute the compute-intensive portion of training, thus\nrelieving CPU resources for inference. Figure 2(c) depicts SIAâ€™s sys-\ntem architecture, which is built upon existing learned index systems.\n3 ANALYSES OF LEARNED INDEXES\nWe conduct in-depth performance characterizations through a set\nof preliminary experiments and obtain three main insights from the\nresults. These insights form the key driving forces behind SIA. For\nthese analyses, we use a SIndex system running on a 16-core server,\nthe details of which are provided in Section 7.1. We use a work-\nload with uniformly distributed keys, generating read andinsert\nqueries based on a predetermined ratio (e.g., 70% read and 30%\ninsert queries). Insert queries raise the retraining complexity by\nadding more keys to the index. We initialize the index with 1M keys.\n3.1 Retraining-Time Scalability Analysis\nExisting updatable string-key learned indexes suffer from a limi-\ntation in that they aggregate all keys into a single dataset, making\ncomputations more demanding as the number of keys increases. To\nexamine the scalability aspect of learned indexes, we measure the\nretraining time as we gradually increase the total number of keys\nfrom 1M to approximately 100M. Figure 1 shows the results with\neach marker representing a retraining invocation. The experiment\nshows that for total numbers of keys reaching 100M, the retraining\ntime for learned indexes becomes prohibitively long. Retraining time\nfor the shortest key length of 16 increases up to 100 seconds, while\nit exceeds 5 minutes for the case of key length 96. These extended\nretraining times for indexing are infeasible as they result in the index\nbeing significantly outdated. The results also show that progres-\nsively prolonged retraining time ends up leading to longer intervals\nbetween retraining invocations. This delay occurs because the grow-\ning retraining time increases the number of keys waiting for the next\nround of retraining, resulting in a lower frequency of model updates.\nThis analysis shows that the existing updatable string-key learned\nindex systems face scalability issues. Thus, there is a need for a solution\nthat minimizes the retraining time for linear models, especially when\ndealing with large index sizes and long key lengths.\n3.2 Impact of Slow Retraining on Throughput\nGiven the aforementioned insight, a subsequent research question\ncouldbe,â€œWhyisretrainingvitalfortheoverallefficacyofthelearnedindex system?â€ The response to this inquiry is that training influ-\nences throughput in two significant ways. (1) First, slow retraining\ncauses the models to become outdated, resulting in reduced index\nprediction accuracy and requiring a costly linear search to locate\nthe correct position. This, in turn, leads to prolonged index search\nlatencies for more read queries, negatively impacting the overall\nsystem throughput. (2) Second, as retraining and inference run si-\nmultaneously on the same system and compete for CPU resources,\nthe inference throughput is adversely affected. We discuss the first\nimplication in this section and leave the discussion for the second\neffect to Section 3.3.\nTo demonstrate the impact of slow training over throughput, we\ndevelop a â€œfictitiousâ€ system that can retrain linear models within\na predetermined training time without using any CPU resources for\ntraining. This method allows us to isolate the impact of slow retrain-\ning separate from the implications of CPU resource contention. Fig-\nure 4 depicts the throughput of this fictitious system as the retraining\nduration shifts between 5s and 300s. The results show a consistent de-\ncline in throughput as the retraining time lengthens, since learned in-\ndexes must use outdated models during the retraining period, which\nwould increase the frequency and degree of linear search to locate\nthe correct position. Additionally, we observe that as the insertion\nratio rises, the system sees a decline in throughput. This is because a\ngreater number of inserted keys await in the buffer before integration\ninto the learned index, which again requires more overhead on linear\nsearch at the non-trained key buffers. While the reported throughput\naverages over time, in a practical scenario, throughput would grad-\nually drop as runtime progresses, because, unlike our hypothetical\nsystem, a real system would face an ever-increasing retraining time.\nOur study suggests that a long retraining period hurts the end-to-end\nsystem throughput of updatable string-key learned index systems.\nTherefore, fast retraining of linear models is imperative.\n3.3 Implication of CPU Resource Allocation\nA straightforward solution to reduce training time would be to al-\nlocate more CPU resources. To better understand the correlation\nbetween throughput and CPU resources, we perform an experiment\nthat measures the system throughput as we vary the number of\nthreads allocated for inference (index serving) and training, while\nmaintaining the number of threads assigned to the other task at 1.\nThis approach allows us to determine the performance benefits that\ninference and training could achieve with additional CPU resources,\nrespectively. As our system has 16 cores, we vary the number of\ncores allocated to either inference or training threads from 1 to 15,\nmaintaining the insertion ratio at 50% and the key length at 32.\nFigure 5(a) and Figure 5(b) show throughput trends. When the\nnumber of threads for training is 1, the additional CPU threads allo-\ncated for inference result in sub-linear yet substantial performance\nscaling. This is because inference is read-only and multiple infer-\nences can be executed independently and in parallel across threads.\nHowever, when the inference process is restricted to a single thread\nwhile training utilizes an increasing number of cores, the additional\nresources only yield marginal benefits. The limited effectiveness of\nCPU for training can be attributed to the limited parallelism in the\n4\n\n(a) (b)Figure 5: Throughput with varying threads for (a) inference\nand 1 for training, (b) training and 1 for inference.\nmatrix decomposition algorithm used for linear regression training,\nas explained in further detail in Section 5.2.\nWe note that inference gains more from extra CPU resources compared\nto training. As a result, we propose a heterogeneous system that\nallocates CPU resources primarily for inference, while employing an\nFPGA accelerator for the training process.\n4SIADESIGN PRINCIPLES\nBuilding upon the insights, we propose a hardware-accelerated up-\ndatable string-key learned index system, dubbed SIA. First, SIApro-\nposes a novel incremental index learning algorithm, which reduces\nthe computing complexity and execution time of each retraining\nprocess. SIAthen dedicates most of the CPU resource for inference\nserving by offloading the training to an FPGA accelerator, thus col-\nlaboratively achieving high throughput. This section outlines the\ndesign principles of each SIAcomponent.\nAlgorithm design principle: Performing only necessary com-\nputations for learned indexes. The fundamental challenge ad-\ndressed in this work is the lack of scalability in learned index training\nsince the compute operations for training compounds as the num-\nber of keys grows. In learned index systems, every retraining run\nnecessitates the processing of the entire dataset. The current state-of-\nthe-art approach involves performing matrix decomposition, matrix\ninverse, dot product, and transpose operations over the entire dataset\nto determine the parameters of the linear models. To reduce the com-\nputing complexity of the training, we devise an incremental index\nlearning algorithm that memoizes the results of previous retraining\ncomputations and reuses them in combination with the new results\nobtained from the augmented training data. With this algorithm, the\ncomputational load is not determined by the total number of keys,\nbut rather by the number of updated keys.\nHardware design principle: Designing the accelerator specif-\nically for training to ensure high energy efficiency as index\nsystems are often dedicated for the exclusive purpose and\nare consistently operational. Updatable learned indexes must\nceaselessly perform training to keep up with the changes made by\nupdate queries, which makes achieving high energy efficiency a\nprimary concern in designing the systems. Although employing\na GPU is seemingly a straightforward approach to attaining high\nthroughput, the advantage is offset by substantial energy consump-\ntion. Thus, in this work, we choose FPGA as our platform. FPGA not\nonly allows us to customize accelerators for diverse algorithm/sys-\ntem constraints and thus achieve high energy efficiency, but also it is\nalready available in the form of off-the-shelf cards, which facilitates\nG\u0011\u0011G\u0012\u0012G\u0012\u0011G\u0013\u0011G\u0013\u0012G\u0013\u0013G\u0013\u0014,FZ\u000e1PTJUJPO\u00015SBJOJOH\u0001%BUBTFUQPT\tB\n\tC\n-JOFBS\u0001SFHSFTTJPO\u0001VTJOH23\u0001EFDPNQPTJUJPOX\u0000=yw h e r eX=QRXTX\u0000=XTY\u0000=(XTX)\u00001XTY\u0000=( (QR)T(QR))\u00001XTY\u0000=(RTQTQR)\u00001XTY\u0000=(RTR)\u00001XTY\u0000=(R\u00001R\u00001T)XTY\n<latexit sha1_base64=\"En/CHzgnUBBpI/KdoXO0NZvs/ug=\">AAAC3XichZLLTttAFIbH5p5yCbBkMyICJQsiOyC1m0qobFiSiIBpHKLx5CQZZXzRzJgqsix1wwJUddv36o734AEYOxGXEMGR7PP7/7+R5+ZFnEllWQ+GOTe/sLi0vFL4srq2vlHc3LqQYSwoNGnIQ+F4RAJnATQVUxycSADxPQ6X3vAkyy9vQEgWBudqFEHbJ/2A9RglSlud4iN2XA8Uwfv4Ox5hF/8agADdsaONegO7bsG5Ts7TV1j+fZUlL155DFWukwM7nQmU641K5mf9A6yRmfX81fgM+wTQUTJuaUZUnrlOsWRVrbzwe2FPRAlN6qxT/O92Qxr7ECjKiZQt24pUOyFCMcohLbixhIjQIelDS8uA+CDbSX46Kd7TThf3QqGfQOHcfT0iIb6UI9/TpE/UQE5nmTkra8Wq962dsCCKFQR0/KNezLEKcXbUuMsEUMVHWhAqmJ4rpgMiCFX6QhT0JtjTS34vLmpV+7Baqx+Vjn9MtmMZ7aBdVEY2+oqO0Sk6Q01EjZ/Gb+POuDc75q35x/w7Rk1jMmYbvSnz3xPUCdjA</latexit>Figure 6: (a) Recursive Model Index, and (b) Linear regression\ntraining operations required per model.\nintegration with the existing systems [ 24,45]. To effectively utilize\nFPGAs for changing training configurations and index model sizes,\nwe develop a hand-optimized design specifically for the proposed\nmemoization-based incremental training algorithm.\nSoftware design principle: Enabling plug-and-play based run-\ntime software for generality and non-invasiveness. While hard-\nware acceleration can offer significant performance gains, the pro-\nposed technique needs to be integrated seamlessly with existing\nlearned index systems. Thus, SIAcannot be specific to a certain up-\ndatable learned index. SIAâ€™s system software is built by determining\nthe commonalities of existing updatable learned indexes and inte-\ngrating the FPGA-based accelerator with minimal modifications to\nthe existing software stack. To accomplish this objective, we utilize\nthe fact that although various learned indexes may have different\nmodel structures and index management mechanisms, they all rely\non linear regression models as the fundamental kernel, which can\nbe readily separable from the other components of the index system.\nGiven this insight, we encapsulate the accelerator and its driver as a\nlinear model training library, which is customized for the case where\nthe training data incrementally grows or shrinks.\n5 INCREMENTAL INDEX LEARNING\nReducing the training workload of the updatable learned index struc-\ntures is a key challenge tackled by this work. In this section, we\nfirst provide the background on training hierarchically structured\nlearned indexes, which requires linear regression training using\nmatrix decomposition. We then introduce SIAâ€™s novel index learning\nalgorithm, which effectively reduces the computational load of the\ntraining process via reuse, without any changes to model quality.\n5.1 Hierarchical Model Index Training\nMost learned indexes [ 11,13,25,27,33,35,36,38,56â€“58,61,62] share\na unique commonality by employing a hierarchical model index\nstructure, as illustrated in Figure 6(a). In the hierarchical structure,\nthe internal and leaf nodes have different roles: learned models at\ninternal nodes predict which node to traverse among the children and\nlearned models at leaf nodes predict the positions for the queried keys.\nThis structure splits the entire key range into a series of small and\npossibly overlapping ranges, where each range is assigned to a leaf\nnode and learned with the associated model. Note that due to update\nqueries, the index structure can potentially expand or shrink, as the\ntotal number of keys handled by the system increases and decreases.\n5\n\nThe hierarchical index is trained in two main ways: (1) cold train-\ning from scratch, which is for new nodes created due to the index\nstructure changes, and (2) updating pre-existing models within the\nexisting nodes due to key additions or deletions without any alter-\nations to the hierarchical index structure. Cold training is infrequent,\ntypically triggered only when the prediction accuracy falls below\na set threshold. Mostly, keys are updated without the need to add\nor remove any nodes. Hence, SIAfocuses on optimizing the latter,\nreserving conventional training techniques for the former.\n5.2 Linear Regression Training\nLinear regression (LR) models the relationship between variables by\nfitting a linear equation to training data. Formally, given an input\nğ‘‹=((ğ‘¥11,..,ğ‘¥ 1ğ‘),..,(ğ‘¥ğ‘›1,..,ğ‘¥ ğ‘›ğ‘))and outputğ‘Œ=(ğ‘¦1,..,ğ‘¦ ğ‘›), a LR\nmodel isğ‘Œ=ğ‘‹ğ›½whereğ›½=(ğ›½1,..,ğ›½ ğ‘). Training determines ğ›½for\na given dataset. In the context of learned index that uses variable-\nlength string keys, the input to the models is a matrix ğ‘‹withğ‘›rows\nwhere each row is a numerically encoded key vector of length ğ‘,\nandğ‘Œ(output) is a vector of integer values that represent the keysâ€™\npositions in the sorted key array. Even for updating the pre-existing\nmodels, the entire ğ‘‹andğ‘Œare required to retrain all the models\ntraversed in the hierarchical structure and determine their new ğ›½ğ‘ .\nAfter retraining, the index for a given key can be predicted by per-\nforming a series of dot products between the traversed model input\nğ‘‹and their corresponding ğ›½ğ‘ .\nLearning the parameters. Everyğ›½can be obtained by inversing the\nmatrixğ‘‹and multiplying it with the output vector ğ‘Œ(i.e.,ğ›½=ğ‘‹âˆ’1ğ‘Œ).\nHowever, computing the inverse matrix ğ‘‹âˆ’1can be computationally\nprohibitive, especially when the matrix size is large. To tackle the\nchallenge, an existing alternative approach commonly and widely\nused in practice is to employ a matrix factorization method, known\nas QR decomposition (QRD) technique. QRD decomposes a matrix\nğ‘‹into a multiplication of two matrices: ğ‘„, anğ‘›xğ‘-sized matrix with\nğ‘„ğ‘‡ğ‘„=ğ‘„ğ‘„ğ‘‡=ğ¼, andğ‘…, ağ‘xğ‘-sized upper triangular matrix.\nFigure 6(b) illustrates the linear algebra operations required to\ndetermineğ›½, leveraging the QRD technique. At first, the QRD of the\ninput dataset ğ‘‹is performed, which produces ğ‘„andğ‘…. After the\ndecomposition, the following operations are performed: (1) comput-\ning the inverse of the upper triangular matrix ( ğ‘…âˆ’1), (2) transposing\nmatrices (ğ‘…âˆ’1ğ‘‡andğ‘‹ğ‘‡), (3) multiplying the resulting small matrices\n(ğ‘…âˆ’1ğ‘…âˆ’1ğ‘‡), and (4) matrix-vector multiplication ( ğ‘‹ğ‘‡ğ‘Œ). Note that\nduring training, only the ğ‘…matrix is required.\nHouseholder QR decomposition. QR decomposition can be com-\nputed using various algorithmic methods [ 14,17,22]. Among these\nmethods, we base SIAon the Householder algorithm [ 22] owing to\nits relatively enhanced numerical stability, while SIAremains com-\npatible with other alternatives due to their algorithmically similar\ntraits. Algorithm 1 illustrates the Householder algorithm [ 22]. The\nalgorithm has two loops. In the outer loop, the algorithm iterates\nover the columns of the input matrix and calculates a vector, called a\nreflector (ğ‘Ÿğ‘’ğ‘“ğ‘–), and a scalar value ğ›¾. For each column, the inner loop\nvisits all the columns located on the right of the current column one\nby one, and updates the visiting column while producing the ğ‘…[ğ‘–][ğ‘—]\nvalues. The nature of this process is fundamentally serial.Input : ğ‘‹: Matrix of size ğ‘šÃ—ğ‘›\nOutput : ğ‘…: Upper triangular matrix of size ğ‘›Ã—ğ‘›\n1for(ğ‘–â†0toğ‘›âˆ’2)do\n2ğ‘ğ‘œğ‘™ğ‘–=ğ‘‹[ğ‘–:ğ‘š,ğ‘–]\n3ğ‘‘=âˆšï¸\ndot(ğ‘ğ‘œğ‘™ğ‘–,ğ‘ğ‘œğ‘™ğ‘–)\n4ğ‘Ÿğ‘’ğ‘“ğ‘–=cal_reflector (ğ‘ğ‘œğ‘™ğ‘–,ğ‘‘)\n5ğ›¾=âˆ’2/dot(ğ‘Ÿğ‘’ğ‘“ğ‘–,ğ‘Ÿğ‘’ğ‘“ğ‘–)\n6 for(ğ‘—â†ğ‘–toğ‘›âˆ’1)do\n7ğ‘ğ‘œğ‘™ğ‘—=ğ‘‹[ğ‘–:ğ‘š,ğ‘—]\n8ğ›¼=ğ›¾Ã—dot(ğ‘Ÿğ‘’ğ‘“ğ‘–,ğ‘ğ‘œğ‘™ğ‘—)\n9ğ‘ğ‘œğ‘™ğ‘—=axpy (ğ›¼,ğ‘Ÿğ‘’ğ‘“ğ‘–,ğ‘ğ‘œğ‘™ğ‘—)\n10ğ‘…[ğ‘–,ğ‘—]=ğ‘‹[ğ‘–,ğ‘—]\n11 end\n12end\nAlgorithm 1: Householder QR decomposition.\nQ1,2Q1,3Q1,1\nQ1,4Q2,1Q2,2Q3,1X2X3X1\nX431,131,231,331,432,232,133,1âœ•âŠ\u0001\u0012TU\u00014UBHFâ‹\u0001\u0013OE\u00014UBHFâŒ\u0001\u0014SE\u00014UBHFâœ•âœ•âœ•âœ•\nâœ•âœ•9\nFigure 7: Parallel QR decomposition.\nParallelizing QR decomposition. Vanilla QRD algorithms, in-\ncluding Algorithm 1, execute sequentially by sweeping through\nthe columns of an input matrix and gradually filling the rows and\ncolumns of the ğ‘„andğ‘…matrices, respectively. Thus, QRD can be\nslow for large matrices, as is the case with learned index. As the\nnumber of keys grows, the height of the key matrix ğ‘‹also increases\n(ğ‘›Ã—ğ‘matrix where ğ‘›Â»ğ‘), making it a tall-and-skinny matrix.\nPrior works [ 6,15,47] offer a parallelization mechanism cus-\ntomized for tall-and-skinny matrices. The parallelization mechanism\nexploits a mathematical property of orthogonal matrix ğ‘„that its\ntransposeisequaltoitsinversematrix,asdepictedwithanexamplein\nFigure 7. Let ğ‘‹be an input matrix for an LR model within the tree. ğ‘‹is\ndecomposed through three steps: (1) ğ‘‹is vertically split into smaller\nsub-matrices ( ğ‘‹1,ğ‘‹2,ğ‘‹3,ğ‘‹4) and decomposed into QR matrices in\nparallel; (2) the QR decomposition is performed on the vertically\nconcatenated ğ‘…matrices ( concat (ğ‘…1,1,ğ‘…1,2) and concat (ğ‘…1,3,ğ‘…1,4)); (3)\nfinally, the last QR decomposition is applied over concat (ğ‘…2,1,ğ‘…2,2)\nto produceğ‘…3,1. The resulting ğ‘…3,1is mathematically equivalent to ğ‘…,\nobtainable by decomposing the ğ‘‹as a whole without parallelization.\n5.3 SIAâ€™s Incremental Index Learning\nMemoized QRD via computation reuse. Exploiting the math-\nematical insight of parallelized QRD, we modify the vanilla QRD\nthat incurs a heavy amount of computation and devise a memoized\nQRD. Figure 8 shows the memoized QRD algorithm. We exclusively\nconsider the case that the number of keys grows due to the insert\n6\n\n22âˆ‹U2U\f\u001299âˆ‹U2U3Uâœ•QRD at time 0QRD at time tQRD at time t+133âˆ‹U3U3U\f\u00123U\u000e\u00123âˆ‹U\nâœ•âœ•âœ•âœ•\n2âˆ‹U\f\u00123âˆ‹U\f\u00129âˆ‹U\f\u00123âˆ‹U\f\u0012Figure 8: Memoized QR decomposition.\nInput : ğ‘€ğ‘œğ‘™ğ‘‘: Current linear models\nğ‘‹ğ‘œğ‘™ğ‘‘: Current key matrices\nğ‘‹Î”: Newly inserted key matrices\nğ‘Œğ‘œğ‘™ğ‘‘: Current index vectors\nğ‘…ğ‘œğ‘™ğ‘‘: Memoized R matrices\nOutput : ğ‘€ğ‘›ğ‘’ğ‘¤: Updated linear models\nğ‘‹ğ‘›ğ‘’ğ‘¤: Updated key matrices\nğ‘Œğ‘›ğ‘’ğ‘¤: Updated index vectors\nğ‘…ğ‘›ğ‘’ğ‘¤: Newly memoized R matrices\n1Initialize ğ‘€ğ‘›ğ‘’ğ‘¤â†âˆ…,ğ‘‹ğ‘›ğ‘’ğ‘¤â†âˆ…,ğ‘…ğ‘›ğ‘’ğ‘¤â†âˆ…\n2while (ğ‘šâˆˆğ‘€ğ‘œğ‘™ğ‘‘)do\n3 ğ‘šğ‘–ğ‘‘â†ğ‘š.ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ _ğ‘–ğ‘‘\n4 ğ‘‹ğ‘›ğ‘’ğ‘¤[ğ‘šğ‘–ğ‘‘]â†concat (ğ‘‹ğ‘œğ‘™ğ‘‘[ğ‘šğ‘–ğ‘‘],ğ‘‹Î”[ğ‘šğ‘–ğ‘‘])\n5 ğ‘Œğ‘›ğ‘’ğ‘¤[ğ‘šğ‘–ğ‘‘]â†calc_index (ğ‘Œğ‘œğ‘™ğ‘‘[ğ‘šğ‘–ğ‘‘],ğ‘‹ğ‘›ğ‘’ğ‘¤[ğ‘šğ‘–ğ‘‘])\n6 ğ‘¡ğ‘šğ‘ =(ğ‘‹ğ‘›ğ‘’ğ‘¤[ğ‘šğ‘–ğ‘‘])ğ‘‡Ã—ğ‘Œğ‘›ğ‘’ğ‘¤[ğ‘šğ‘–ğ‘‘]\n7 ğ‘…Î”â†QR(ğ‘‹Î”[ğ‘šğ‘–ğ‘‘])\n8 ğ‘…ğ‘¡ğ‘šğ‘â†concat (ğ‘…ğ‘œğ‘™ğ‘‘[ğ‘šğ‘–ğ‘‘],ğ‘…Î”)\n9 ğ‘…ğ‘›ğ‘’ğ‘¤[ğ‘šğ‘–ğ‘‘]â†QR(ğ‘…ğ‘¡ğ‘šğ‘)\n10 ğ›½= ((ğ‘…ğ‘›ğ‘’ğ‘¤[ğ‘šğ‘–ğ‘‘])âˆ’1Ã—((ğ‘…ğ‘›ğ‘’ğ‘¤[ğ‘šğ‘–ğ‘‘])âˆ’1)ğ‘‡)Ã—ğ‘¡ğ‘šğ‘\n11 ğ‘€ğ‘›ğ‘’ğ‘¤[ğ‘šğ‘–ğ‘‘].ğ›½â†ğ›½\n12end\nAlgorithm 2: Incremental index learning algorithm.\nqueries1. When a learned index is retrained, we require the ğ‘…matrix\ncorresponding to the current ğ‘‹. To do so, we memoize the computed\nğ‘…matrix in memory at every retraining invocation ( ğ‘…ğ‘¡). When a\nretraining is invoked, the rows of collected additional keys ğ‘‹Î”ğ‘¡+1is\ndecomposed. Then, similar to the parallelized QRD, we concatenate\ntheğ‘…ğ‘¡andğ‘…Î”ğ‘¡+1, and perform one more QRD to obtain the final ğ‘…ğ‘¡+1.\nNow,ğ‘…ğ‘¡+1is used for linear model training and cached in memory for\nthe next retraining run. Note that SIAâ€™s QRD algorithm involves only\ntwo small QR decompositions, which significantly reduces the com-\npute load by reusing the performed computations. Moreover, the size\nof eachğ‘…matrix isğ‘Ã—ğ‘whereğ‘is the key length, thus is very small\nand does not incur large memory footprint overhead. For instance,\nwith a key length of 96, the size of ğ‘…is merely 72 KB (=96 Ã—96Ã—8).\nSIAâ€™s incremental index learning algorithm. SIAâ€™s incremen-\ntal index learning algorithm uses the memoized QRD to train the\nmodels in the updatable learned indexes. Algorithm 2 describes\nSIAâ€™s training process. The algorithm loops over the list of linear\nmodels in the hierarchical structure, which need to be updated. It\nconcatenates the existing keys ğ‘‹ğ‘œğ‘™ğ‘‘with new keys ğ‘‹Î”to obtain\nğ‘‹ğ‘›ğ‘’ğ‘¤, calculates indexes for the new keys to update ğ‘Œğ‘œğ‘™ğ‘‘withğ‘Œğ‘›ğ‘’ğ‘¤,\n1We will discuss the delete query handling in Section 6.4.\nTimeFigure 9: Increasing retraining time as the total number of\nkeys increases with CPU-based memoized QRD on SIndex.\nMarkers on the same line represent sequential retraining\nruns, where markers positioned to the left precede those on\nthe right. Key lengths are 16, 32, 64, and 96. For comparison,\nthe shaded lines depict the results presented in Figure 1.\n16 32 64 96\nKey Length0.00.250.50.751.0FractionPre-processing\nQR decomposition\nR Inverse + GEMM\nGEMV\nPost-processing\nFigure 10: Breakdown of linear model training runtime.\nand computes the(ğ‘‹ğ‘›ğ‘’ğ‘¤)ğ‘‡ğ‘Œğ‘›ğ‘’ğ‘¤. Then, the algorithm performs the\nmemoized QRD, which results in ğ‘…ğ‘›ğ‘’ğ‘¤. Usingğ‘…ğ‘›ğ‘’ğ‘¤, the algorithm\nobtains theğ›½and updates the model parameters with the new ğ›½. The\nobtainedğ‘…ğ‘›ğ‘’ğ‘¤is memoized for next retrainings. The same training\nprocess is repeated until the models of all leaf and internal nodes in\nthe index structure are updated.\nLimitation of software-only solution. We observe that the SIAâ€™s\nlearning algorithm already substantially reduces the computational\ncost of training the learned index, even when implemented in soft-\nware without hardware acceleration. Figure 9 shows the improved\ntraining time with the proposed memoized algorithm. Compared\nto the baseline reported in Figure 1, which is presented as dimmed\nlines in Figure 9, the retraining time is reduced for all the evaluated\nkey lengths, as reflected by the slopes of line graphs. Moreover, this\nshortens the retraining interval, as shown in Figure 9 reporting a\ngreater number of data points (markers), each corresponding to a\nretraining. However, Figure 9 also shows that the resulting reduction\nin training time is insufficient, still extending up to 200s.\nAcceleration target determination. This observation motivates\nus to devise an efficient and performant hardware accelerator for\ntraining. However, the first crucial step is to determine the accelera-\ntion targets for offloading to the hardware. For this purpose, we first\ncharacterize the core compute kernels of training. Figure 10 shows\nthe results as we vary the key length from 16 to 96. We look into four\nkernels: (1) training data matricization, (2) QR decomposition, (3)\nR matrix inverse calculation and matrix-matrix multiplication, and\n(4) matrix-vector multiplication. As the data matricization is mostly\nmemory copy, it needs to be performed by CPU. We also rule out\nmatrix-vector multiplication from the acceleration targets since it\nrequires a memory copy for the entire ğ‘‹matrix from host to FPGA.\n7\n\n(a)\n4DSBUDIQBE\n4ZTUPMJD\u0001\"SSBZ4DSBUDIQBE\n*OOFS\u0001-PPQ1&/16\u0012\n16.23%\u00016OJU\u0001$POUSPMMFS3FGMFDUPS#VGGFS4DBMBS3FHJTUFS.BUSJY\u0001#VGGFS\n3\u0001.BUSJY\u0001#VGGFS$PODBU*OOFS\u0001-PPQ1&\u0012*OOFS\u0001-PPQ1&\u00130VUFS\u0001-PPQ1&\n(b)\u0016NJEÂ¦3NJEÂ¦3\"DDFMFSBUPS$POUSPMMFS\n23%\u00016OJU4ZTUPMJD\"SSBZ4DSBUDIQBE%3\".%3\".$POUSPMMFS%.\"$POUSPMMFS$PODBU\u00155SBJOJOH\u0001&OHJOF\nUNQ3NJEÂ¦3\nOFX35SBJOJOH5ISFBE$16\n\u0019\u001a\u0014\u0017\u0016\n\u0018\u0012\n1$*FNJE%3\".\n\u0013'1(\"\n5SBOTQPTF6OJUNJEÂ¦3NJEPME3NJEÂ¦YNJEÂ¦Y\u0011PME3\u0012PME3NJEPME3Figure 11: (a) SIAâ€™s system where the CPU runs the training thread to issue jobs to the accelerator. Accelerator executes the\ntraining operations on the training engine; (b) Microarchitecture of QR decomposition unit.\nTo this end, this work focuses on accelerating QR decomposition, R\ninverse, and GEMM operations on the FPGA.\n6SIASYSTEM DESIGN\nWhile SIAemploys the incremental learning algorithm to reduce the\ncomputation load, we enhance this algorithmic approach by incor-\nporating an FPGA accelerator and customized runtime software to\nfurther accelerate SIAâ€™s training. We first describe the overview of\nSIAâ€™s system, and then, elaborate each component in detail.\n6.1 FPGA-Accelerated Training Infrastructure\nFPGAs have been commonly used as a successful platforms for ac-\nceleration [ 39,40,46,50] and are even deployed in cloud datacen-\nters [ 48]. Figure 11(a) depicts the SIAsystem accelerated using FPGA.\nAs in existing learned index systems, SIAemploys a multi-core CPU\nthat can serve both inference and training. However, SIAalso comes\nwith an FPGA accelerator to offload training computation. We chose\nFPGA as the acceleration platform owing to its customizability to\nindex-specific algorithms and high energy efficiency, which is cru-\ncial for index systems since training computations are consistently\nconducted throughout their lifespan. Unlike the existing systems,\nSIAonly runs a single training thread to not only compute the non-\naccelerated memory-bound kernels, but also manage the data trans-\nfer between host and FPGA and control the accelerator invocations.\nThe training thread iterates over a list of linear models within the\nhierarchical structure and initiates the retrainings of these models\none by one on available Training Engines (TEs). To train a model,\nthe newly inserted keys accumulated in the modelâ€™s buffer ( ğ‘‹Î”) are\nfirst copied from host to FPGA. FPGAâ€™s off-chip memory maintains\nan array ofğ‘…ğ‘œğ‘™ğ‘‘matrices, which are memoized from the previous\nretraining runs. In the figure, the superscript ğ‘šğ‘–ğ‘‘on theğ‘…andğ‘‹\nmatrices refers to the model ID. After the memory copy from the\nhost to FPGA is completed, the training thread sets a control register\nin the accelerator controller, scheduling the training computationto an available TE. The training thread is also responsible for updat-\ning the model parameters, which occurs repeatedly during runtime,\nallowing the index to integrate new keys.\n6.2 Accelerator Architecture\nTraining Engine. Figure 11(a) also depicts the TE architecture. The\nfirst computation performed by TE is the SIAâ€™s QRD algorithm de-\nscribed in Section 5.2. The TE feeds ğ‘‹Î”to the QRD unit. It then\nobtains the ğ‘…Î”, which is concatenated with the memoized ğ‘…ğ‘œğ‘™ğ‘‘in\nthe scratchpad memory to produce the ğ‘…ğ‘¡ğ‘šğ‘. Thisğ‘…ğ‘¡ğ‘šğ‘is then fed\nto the QRD unit as an input that produces ğ‘…ğ‘›ğ‘’ğ‘¤. The next step is to\nperformğ‘…ğ‘›ğ‘’ğ‘¤matrix inversion and matrix-matrix multiplication be-\ntween the inverse and its transpose. We exploit a parallelized matrix\ninverse algorithm, namely Hellerâ€™s algorithm [ 21], which effectively\nconverts a matrix inverse into a series of recursive matrix-matrix mul-\ntiplications. As we transform all needed operations into a series of\nmatrix-matrix multiplications, a systolic-array accelerator equipped\nwith a transpose unit can complete all the necessary kernel execu-\ntions. Once the computation is completed, the accelerator controller\nuses a control flag to inform the training thread about the completion.\nQRD Unit. Due to its computational intensity in mathematical prob-\nlems, QRD has been a target for hardware acceleration [ 6,30,49]. We\ndevise the architecture of our QRD unit inspired by an existing QRD\naccelerator [ 6], which executes the Householder algorithm described\nin Algorithm 1. Figure 11(b) shows the microarchitecture of the QRD\nunit in each Training Engine. QRD unit constitutes an array of Pro-\ncessing Units (PUs), each of which executes a QRD. The results of PUs\nare concatenated and stored back to the matrix buffer for the next\nstage of QRD (Figure 7). Each PU first gets its input data from scratch-\npad memory ( ğ‘‹Î”orğ‘…Î”) and stores them in the matrix buffer. Then,\nthe outer loop in Algorithm 1 is performed at the â€œOuter Loop PEâ€,\nwhich calculates the reflector and ğ›¾. These two inputs are sent to a set\nof \"Inner Loop PEs\", which are responsible for calculating ğ‘…ğ‘›ğ‘’ğ‘¤[ğ‘–][ğ‘—]\nfor different columns in parallel. Each â€œOuter Loop PEâ€ and â€œInner\nLoop PEâ€ is equipped with a vector of multiply-and-accumulate\n8\n\n(MACC) units for dot products. The resulting ğ‘…ğ‘›ğ‘’ğ‘¤matrix is sent to\nthe scratchpad memory and replaces ğ‘…ğ‘œğ‘™ğ‘‘for future retrainings.\n6.3 Runtime Software Interface\nAs emphasized in Section 5.1, in designing the SIAsystem, we lever-\nage a commonality of most learned indexes that they use linear\nregression as their backend machine learning models. This unique\nproperty enables us to build an abstraction between various learned\nindexes and our hardware accelerator solution. Therefore, SIAcould\nbe readily adopted by any linear model-based learned index systems.\nTo transparently develop the abstraction and facilitate the use of\nunderlying acceleration solution, we encapsulate the SIAaccelera-\ntor along with its device driver and accelerator invocation runtime\nsoftware as a library. In fact, as existing learned index systems often\nemploy LAPACK, a famous linear algebra library, we propose SIAâ€™s\ninterfaces to be equivalent to the LAPACKâ€™s, so that the integration of\nSIAwith the existing systems becomes straightforward. The runtime\ninterface of SIAincludes two functions: (1) cold_train : a function\nfor full model training with key matrix and keyâ€™s position vector, and\n(2)incre_train : a function for incremental learning with memo-\nization that takes the memoized ğ‘…matrix as an additional argument.\nThese two functions closely resemble the LAPACKâ€™s gelsfunction,\nenabling existing updatable learned indexes to leverage SIAâ€™s incre-\nmental index learning algorithm and hardware acceleration with\nminimal software modifications.\n6.4 Lazy Delete Query Handling\nWhile this paper has focused on the insert query handling thus far,\nupdatable learned indexes must be able to handle delete queries as\nwell. Conventional updatable learned indexes handle these delete\nqueries through retraining, similarly to the insert queries. In con-\ntrast, our incremental learning algorithm exploits a memoization\ntechnique, which relies on the assumption that the existing keys used\nto compute the memoized ğ‘…matrix are not changed. Therefore, the\nremoval of keys from the index inevitably forces SIAto discard the\nmemoizedğ‘…matrix and necessitates a cold training, which undercuts\nthe advantages of our proposed technique.\nTo tackle this problem, we employ a lazy delete handling tech-\nnique where the deleted keys are simply flagged as â€œdeletedâ€, yet the\ninformation of these deleted keys still remains in the memoized ğ‘…\nmatrices. This way, our incremental training method remains effec-\ntive during retraining. It is important to note, however, that upon\nmarking as â€œdeletedâ€, the key string and associated value data are im-\nmediately erased from the indexes for security purposes. Memoized\nR matrices for the â€œdeletedâ€ keys are eliminated during cold training,\nwhere the models are trained from scratch without utilizing the mem-\noized matrices. Note that our lazy deletion technique does not affect\nthe functionality of indexes, but only influences performance, since\ndeleted-yet-unremoved information would lower the prediction ac-\ncuracy and end up increasing the linear search cost for mispredicted\naccesses. However, we observe that lazy deletion has a marginal im-\npact on performance, with less than 5% overhead (see Section 7.2.8).\n6.5 Implication of Node Split and Merge\nThe hierarchical structure of learned index necessitates structural\nmodifications as new keys are inserted or deleted. Hierarchical\nlearned index structures undergo structural modifications througheither split ormerge . Model split involves partitioning the keys as-\nsigned to a node into two nodes when the accuracy of the correspond-\ning model drops, while model merge combines two nodes into one\nwhen both have sufficiently high accuracy. SIAemploys the same\nthreshold determination mechanism for split and merge as the de-\nfault learned index system, without any modifications. Note that SIA\nshould perform cold trainings for split nodes as they lack memoized\nğ‘…matrices, while for merged nodes, SIAcan merge the ğ‘…matrices\nand use the merged ğ‘…matrix for further incremental training.\n7 EVALUATION\nTo evaluate the effectiveness of SIA, we harness two open-source\nbenchmark suites, YCSB and Twitter cache trace, using two real-\nworld datasets, Amazon review and MemeTracker. We evaluate\nthroughput, system-level energy efficiency, and memory usage of\nSIA-accelerated learned indexes, compared to other index structures.\n7.1 Methodology\nYCSB. To evaluate SIA, we primarily use a real-world key-value store\nbenchmark suite, YCSB [ 8]. YCSB contains six diverse workloads\n(A-F), each characterized by its unique mix of query types. As SIAis\nfor updatable learned index systems, we focus on the two workloads\namong the six, which include insert queries: (D) read latest that tend\nto have read queries for recently inserted keys, along with roughly\nthe 5% of insert queries, and (E) short ranges that consists of 95%\nrange queries, and 5% of insert queries. Note that while YCSBâ€™s\nquery compositions mirror real-world application patterns, the key\nlengths do not. To better emulate real-world key-value stores, we\nemploy two genuine string datasets: Amazon review data and the\nMemeTracker dataset. Amazon review data ( amaz ) [44] is collected\nfromuserreviewsonproductsfromAmazonwiththeuserIDsaskeys\nof length 12. MemeTracker dataset ( meme ) [32] comprises quotes\nand phrases collected from the web and online news URLs referring\nto them with the URLs as keys of length 128. We use these datasets\nsince they are widely used in prior works [ 58,63,67] to evaluate\nthe string-key key-value stores. Additionally, we use a randomly\nsynthesized dataset ( rand ) with a uniform key distribution.\nTwitter cache trace. Complementing YCSB, we also utilize the\nTwitter cache trace [ 66] to enrich our experimental methodology.\nTwitter cache trace constitutes a pile of indexing traces collected\nfrom Twitter clusters, which allows it to concurrently serve as a\nworkload and a dataset. Among the provided 54 cluster traces, we\nspecifically select four cluster traces with a relatively significant\nvolume of update queries, each of which exhibits a distinct query\ncomposition, represented by the following tuples of (cluster ID, up-\ndate query ratio): (12.2, 43%), (15.5, 59%), (31.1, 56%), (37.3, 42%).\nBaselines. As baselines, we use three state-of-the-art learned in-\ndexes, ALEX [ 11], LIPP [ 61], and SIndex [ 58], all of which are chosen\nfor their open-source implementations available at our disposal. We\nadded the variable-length string key and multi-threading support on\ntop of ALEX and LIPP, as they lack the features. We built their corre-\nsponding SIA-accelerated counterparts by integrating the implemen-\ntation with our SIAlibrary. Note that while the three systems have dis-\nparitiesinhowtoinitiallybuildtheindexesthroughbulkloading(e.g.,\n9\n\nWormholeCuckoo TrieALEXALEX-SIALIPPLIPP-SIASIndexSIndex-SIA\n(a) YCSB(b) T witterFigure 12: Throughput comparison of non-learned (conventional) and learned indexes for YCSB and Twitter cache trace.\nTable 1: Hardware specifications and resource utilization of\nthe Intel Arria 10 with the configuration of with 4 TEs, each\nhaving 2 PUs, and each PU containing 3 Outer Loop PEs.\nALMRAM BlkPLLBlk RAMDSPUsed278,7592,123484,648,464244Total427,2002,71317655,562,2401,518Utilization65.3%78.3%27.3%8.4%16.1%IntelArria 10GX-1150\ntop-down vs. bottom-up), it does not affect our performance evalua-\ntionsbecausetheindexbuildingonlyrequirescoldretrainings,which\ncannot exploit the proposed incremental index learning algorithm.\nFurthermore, we include comparisons between SIA-accelerated\nlearned indexes and two state-of-the-art non-learned indexes, Worm-\nhole [ 63] and Cuckoo Trie [ 68], all of which support variable-length\nstring keys. Wormhole [ 63] is an optimized B-tree in which part of\nthe tree is replaced with a trie utilizing hashes. Cuckoo Trie [ 68]\nis a hash-based trie index that achieves high performance through\noverlapping memory accesses.\nSystem specifications. TheSIA-accelerated learned index systems\nare equipped with a 16-core Intel Xeon Gold6226R and 128 GB DRAM.\nFor building SIndex-GPU , GPU-accelerated variant of SIndex, we\nemploy NVIDIA GeForce RTX 2080 TI GPU along with the same\nCPU and memory configuration. SIndex-GPU uses CuSolver library\nin CUDA version 11.7. For the runtime measurement of the baseline\nlearned index systems, we use a highly-optimized, parallel linear\nalgebra library, Intel oneAPI Math Kernel Library (MKL) 2019.0.\nFPGA platform details. Table 1 shows the hardware resource spec-\nification of the evaluated FPGA platform, Intel Arria 10 GX-1150, and\nits utilization when we program our accelerator on it. We develop a\ncustom accelerator controller on the programmable logic to interface\nwith the deviceâ€™s main memory. We synthesize the hardware with\nQuartus II v20.1, and achieve a frequency of 272 MHz.\nPowermeasurement. To measure the end-to-end system power, we\nuse an off-the-shelf power meter, WATTMAN HPM-100A [ 2]. This\npower meter is placed between the power outlets and the servers,\nwhich are configured with various processor combinations, includ-\ning CPU-only, CPU-GPU, and CPU-FPGA setups. The measured\nsystem power can be monitored per each second through the vendor-\nprovided software, which we average over the experiment runtime.\n7.2 Experimental Results\n7.2.1 Throughput. Figure 12 shows the throughput comparison re-\nsults among two non-learned indexes ( Wormhole andCuckoo Trie ),\nTree traversalHashingML inferenceLocal searchBuï¬€er searchRange scan10.727.3\n(a) YCSB-D(b) YCSB-EFigure 13: Latency breakdown for YCSB D/E workloads using\nrand dataset. Range scan includes buffer search for YCSB-E.\nthree learned indexes ( ALEX ,LIPP ,SIndex ), and their SIA-accelerated\ncounterparts ( ALEX-SIA ,LIPP-SIA ,SIndex-SIA ).\nYCSB results. Figure 12(a) illustrates the results using two YCSB\nworkloads across three datasets: rand,amaz , and meme . Although\nthere is some variability in the results, we observe a consistent trend\nthat the SIA-accelerated indexes outperform the learned index base-\nlines, as well as the conventional, non-learned index baselines. This\ntranslates to approximately an average 2.6 Ã—throughput improve-\nment over CPU-only learned index systems. This substantial en-\nhancement is attributed to SIAâ€™s utilization of both iterative learning\nalgorithm and customized hardware accelerator. This approach ded-\nicates the majority of CPU cores to inferences, while the system\nallocates only one training thread for memory-bound kernels and\naccelerator management, not performing any expensive operations.\nTwitter cache trace results. Figure 12(b) reports the throughput\nresults for Twitter cache trace. Twitter cache trace has diverse key\nlengths that range from 19 to 82. As the key length directly affects\nthe computational load, there are variations among clusters in the\nthroughput results. On average, the SIA-accelerated learned indexes\noffer 3.4Ã—throughput improvement over CPU-only systems, repre-\nsenting a more substantial performance improvement than observed\nin the YCSB scenario. The larger gain comes from that the dataset\nof Twitter cache trace has generally longer keys, making the key\nmatrix larger, which can be better parallelized by the accelerator.\nOverall, the results suggest that SIAis an effective solution for en-\nabling updatable string-key learned indexes without suffering from\nperformance bottlenecks caused by training computations.\n7.2.2 Query Latency. To understand the source of performance\nimprovements, we further analyze the query latency for YCSB work-\nload (D) and (E), and present the breakdown results in Figure 13.\n10\n\nFigure 14: Memory consumption of traditional (non-learned)\nindexes, baseline learned indexes, and learned indexes with\nSIA. Key and value data is excluded.\n(a) YCSB(b) T witterWorkloadsClustersD-randE-randD-amazE-amazD-memeE-meme12.215.531.137.3SIndex-CPUSIndex-SIA-HWSIndex-SIA-SWSIndex-IdealSIndex-GPU\nFigure 15: Ablation study results using SIndex variants.\nNon-learned indexes, Wormhole andCuckoo Trie , require traversal\nthrough their tree structures, which often involve multiple DRAM\naccesses, leading to high query latency. In contrast, learned indexes\n(SIndex andSIndex-SIA ) require much fewer memory accesses for\ngraph traversal. In fact, the depth of hierarchical learned index struc-\nture of SIndex is only two, which imposes significantly lower mem-\nory access overhead than the alternatives. As the cost of these ben-\nefits, the learned indexes must pay other costs such as ML inference ,\nlocal search in case of misprediction, and buffer search for seeking\nthe â€œnot-yet-trainedâ€ keys. The outcomes of the study reveal that the\nbuffer search is the largest overhead, especially for SIndex , because\nit piles up a large number of keys in the buffer due to slow retrain-\ning. On the contrary, SIAaccelerates the retrainings and frequently\nempties the buffers of SIndex-SIA , which substantially reduces the\nbuffer search latency, directly leading to the total latency reduction.\n7.2.3 Memory Usage. Figure 14 reports the memory usage of five\nbaselines(learnedand non-learned)andthree SIA-acceleratedlearned\nindexes. To specifically assess the memory usage difference among\nthe indexes, we exclusively measure the memory usage for indexes,\nnot key and values. Learned indexes typically require significantly\nless memory because they efficiently compress the key-position\nmapping information from hierarchical data structures into a series\nof compact machine learning models. SIAincurs marginal overhead\nin memory usage as it must additionally store the ğ‘…matrices for\nmemoized computation. However, the average overhead measured\nin our experiments is merely 6.0%, which is negligible and justifiable\nwith the significant performance improvements.\n7.2.4 Ablation Study. For a more thorough analysis of the factors\ncontributing to performance improvements, we focus on the SIA-\naccelerated SIndex andconductanablationstudy.Figure15compares\nthe throughput of the five SIndex variants. SIndex-GPU is a system\nFigure 16: Average power consumption of SIndex-CPU ,\nSIndex-GPU , and SIndex-SIA end-to-end systems. Vertical\nlines indicate minimum and maximum power consumption.\nWormholeCuckoo TrieALEXALEX-SIALIPPLIPP-SIASIndexSIndex-SIA\nDistributions\nFigure 17: Throughput of non-learned and learned indexes\nfor queries with different request distributions.\noffloading retraining computation to GPU, while SIndex-Ideal is a\nsystem equipped with an infinitely fast accelerator that trains models\nin zero time. On the other hand, SIndex-SIA-SW andSIndex-SIA-\nHW are the SIA-accelerated SIndex systems with algorithm-only\nand algorithm-hardware co-designed SIAsolutions, respectively.\nSIndex-GPU achieves 2.3Ã—throughput improvement compared to\nthe default CPU baseline, SIndex-CPU . While SIndex-SIA-SW of-\nfers 1.7Ã—improvement over SIndex-CPU , the benefit is 56.5% lower\nthan that of SIndex-GPU , which demonstrates the limitation of the\nsoftware-only solution. However, SIndex-SIA-HW achieves 2.0Ã—\nadditional improvement over SIndex-SIA-SW , closely approaching\ntoSIndex-Ideal , 11.6% higher than what SIndex-GPU offers, which\npresents the effectiveness of hardware acceleration. These results\nshow the effectiveness and necessity of SIAas a solution that syn-\nergizes algorithm and hardware designs for acceleration.\n7.2.5 System Power Consumption. We choose FPGA due to its capa-\nbility to tailor the hardware architecture for the given task, incremen-\ntal training, delivering notably higher energy efficiency compared\nto GPU. Figure 16 illustrates the system-level power consumption of\nSIndex variants: SIndex-CPU ,SIndex-GPU , and SIndex-SIA , which\ndemonstrates the advantages of FPGA acceleration in power effi-\nciency. We observe that the CPU-only system, SIndex-CPU , operates\nat 150W, with a significant portion of this power attributed to CPU-\nbased training. SIndex-GPU operates at 203W, dissipating 79W for\ntraining at GPU and the remaining 123W for the CPU-based system.\nIn contrast, SIndex-SIA , a CPU-FPGA heterogeneous system, con-\nsumes only 126W as the FPGA accelerator adds only 3W to the CPU-\nonly system, which demonstrates the power efficiency of the FPGA.\n7.2.6 Throughput-per-watt. As noted in the power consumption\nanalysis, if we only consider the accelerator itself instead of the\nentire system, FPGA offers 28 Ã—less power consumption compared\n11\n\nTable 2: Performance degradation as we vary training interval\nfrom 5s to 300s and deletion ratio from 5% to 15%.\n5 30 100 300\n5% 0.0% 0.6% 2.0% 3.2%\n10% 0.0% 1.0% 4.0% 4.1%\n15% 0.0% 2.0% 4.2% 4.6%Training Interval (sec) Deletion\nRatio\nTHR3/4 THR2/4 THR1/4 THR\nSIndexSIndex-SIA\nFigure 18: Throughput of SIndex andSIndex-SIA for YCSB\nworkloads as the node accuracy threshold (THR) varies. As\nTHR decreases, the learned index is more inclined to divide\nnodes, resulting in smaller nodes containing fewer keys.\nto GPU. However, when we consider the system-level power con-\nsumption with their throughput together, SIndex-SIA achieves only\n1.76Ã—higher throughput-per-watt compared to SIndex-GPU . While\nthe gain may be deemed modest, the 76% gap could translate into\nsubstantial cost disparities in terms of actual monetary expendi-\nture since index systems tend to remain operational continuously,\nconsistently dissipating considerable amounts of energy. These re-\nsults suggest that for the given task, the continuous retrainings of\nupdatable learned indexes, FPGA is a more attractive option as an\nacceleration platform compared to GPU.\n7.2.7 Implication of Request Distributions. Figure 17 illustrates the\nthroughput of each index across six different request distributions\nas used in prior works [ 5,10]:sequential ,zipfian ,hotspot ,exponent ,\nuniform andlatest . Across all six query distributions, learned indexes\naccelerated with SIAconsistently show significant performance im-\nprovement, which ranges from 3.9 Ã—to 6.2Ã—in comparison with the\nbaselines. Note that zipfian ,hotspot , and exponent distributions ex-\nhibit skewed patterns, resulting in certain key ranges being accessed\nmore frequently than others, causing more node splits. As node splits\ntrigger cold trainings, it imposes performance overhead, while we\nobserve that its impact on the end throughput results is negligible.\n7.2.8 Implication of Lazy Delete Query Handling. Table 2 illustrates\nthe impact of lazy delete query handling on the performance of\nSIA-accelerated learned indexes. For this experiment, we configure\nthe cold training interval to various durations: 5, 30, 100, and 300\nseconds. We sweep the delete query ratio from 5% to 15%, filling\nthe remaining queries with read queries. The results show that at\na deletion ratio of 5%, there is a performance degradation of 3.2%\nwhen the training interval is 300 seconds. Meanwhile, with a dele-\ntion ratio of 15%, the larger number of unhandled keys results in a\ngreater performance loss, which increases up to 4.6%. Nonetheless,\nthe performance degradation remains at a marginal level, which\nvalidates the viability of the lazy approach, particularly considering\nthe significant costs associated with complete cold training.7.2.9 Implication of Node Size on Throughput. As discussed in Sec-\ntion 6.5, SIAemploys the same model split mechanism as in the exist-\ning system, where a node is split into two if its model accuracy drops\nbelow a certain threshold. To evaluate the implication, we gradually\nreduce the node accuracy threshold from the default value (THR) to\nits 1/4 (25%) and observe the changes in throughput. Figure 18 shows\nthe results on SIndex andSIndex-SIA . As the threshold decreases,\nthe learned indexes split the nodes more aggressively, resulting in\nsmaller nodes with fewer keys. While such changes incur overhead,\nwe observe that the performance impact is negligible for both SIndex\nandSIndex-SIA , since the model splitting does not alter the total num-\nber of keys, which determines the computational load for training.\n8 ADDITIONAL RELATED WORK\nLearned index structures. There has been a large body of prior\nworks [ 1,9,10,12,13,25,34,41,43,54,61,69,71,72] for learned index\nsystems. RadixSpline [ 25] and PLEX [ 54] further optimize learned\nindex construction. Flood [ 43] and Tsunami [ 12] exploit the learning\napproach for multi-dimensional indexes to automatically optimize\nthe index structure for the given data and query distributions. On\nthe other hand, SIAoptimizes training via memoized QRD algorithm\nenhanced by an accelerator and builds a system for integration with\nlearned index structure.\nLearned index acceleration. Colin [ 71] builds and manages CPU\ncache-friendly learned index structure on top of PGM-index. Colin\nperforms key insertions in place to better utilize the caching. An-\nderson et al. [ 4] perform micro-architectural analysis of ALEX on\ncommodity CPU and show the impact of memory hierarchy on\nread/write latency. Unlike these works that aim to benefit from mi-\ncroarchitectural optimizations on the CPU, SIAdevises iterative\nQRD to leverage computation reuse and further enhances the index\nsystem by offloading the training process to a separate accelerator.\nQR decomposition accelerator. As the QR decomposition makes\nup an essential building block of many modern applications, several\narchitectural design for accelerators has been studied in the litera-\nture [ 6,30]. Although the QRD unit is motivated from past works,\nnone of them use these in the context of learned index systems. More-\nover, SIAâ€™s accelerator is designed to execute multi-dimensional par-\nallelism in the context of retraining models in learned indexes, while\nQRD accelerator is a small function unit.\n9 CONCLUSION\nThis work offers SIA, an accelerated string-key learned index system.\nThese index structures require constant retraining of their machine\nlearning models to determine the mapping between keys and their\npositions. SIAmitigates the bottleneck of the current systems that\nincur huge overhead of training when the keys are updated. Training\nobserves multi-fold issues, where it is inefficient to execute on the\nCPU, is serial across runs as it writes to the model, and cannibalizes\nCPU resources from inference queries. Based on these insights, SIA\nenhances the learned index training by leveraging the mathematical\nproperty that keys can be updated incrementally, and thus, can bene-\nfit from computation reuse via memoization. SIAfurther boosts this\ntraining on an energy-efficient FPGA accelerator and relieves CPU\nresources for inference, collaboratively offering significant speedup.\n12\n\nREFERENCES\n[1]Hussam Abu-Libdeh, Deniz AltÄ±nbÃ¼ken, Alex Beutel, Ed H. Chi, Lyric Pankaj\nDoshi, Tim Klas Kraska, Xiaozhou (Steve) Li, Andy Ly, and Chris Ol-\nston (Eds.). 2020. Learned Indexes for a Google-scale Disk-based Database .\nhttps://arxiv.org/pdf/2012.12501.pdf\n[2] ADpower. 2023. Wattman (HPM-100A). http://adpower21com.cafe24.com/shop2/\nproduct/wattman-hpm-100a/17.\n[3]Jung-Sang Ahn, Chiyoung Seo, Ravi Mayuram, Rahim Yaseen, Jin-Soo Kim,\nand Seungryoul Maeng. 2016. ForestDB: A Fast Key-Value Storage System for\nVariable-Length String Keys. IEEE Trans. Comput. 65, 3 (2016), 902â€“915.\n[4]Mikkel MÃ¸ller Andersen and Pinar TÃ¶zÃ¼n. 2022. Micro-Architectural Analysis\nof a Learned Index. In Proceedings of the Fifth International Workshop on\nExploiting Artificial Intelligence Techniques for Data Management (Philadelphia,\nPennsylvania) (aiDM â€™22) . Association for Computing Machinery, New York, NY,\nUSA, Article 5, 12 pages. https://doi.org/10.1145/3533702.3534917\n[5] Esmail Asyabi, Yuanli Wang, John Liagouris, Vasiliki Kalavri, and Azer Bestavros.\n2022. A New Benchmark Harness for Systematic and Robust Evaluation of Stream-\ning State Stores. In Proceedings of the Seventeenth European Conference on Computer\nSystems (Rennes, France) (EuroSys â€™22) . Association for Computing Machinery,\nNew York, NY, USA, 559â€“574. https://doi.org/10.1145/3492321.3519592\n[6] Jose M. Rodriguez Borbon, Junjie Huang, Bryan M. Wong, and Walid Najjar. 2021.\nAcceleration of Parallel-Blocked QR Decomposition of Tall-and-Skinny Matrices\non FPGAs. ACM Trans. Archit. Code Optim. 18, 3, Article 27 (may 2021), 25 pages.\nhttps://doi.org/10.1145/3447775\n[7]Tianqi Chen, Lianmin Zheng, Eddie Yan, Ziheng Jiang, Thierry Moreau, Luis\nCeze, Carlos Guestrin, and Arvind Krishnamurthy. 2018. Learning to Optimize\nTensor Programs. In Proceedings of the 32nd International Conference on Neural\nInformation Processing Systems (MontrÃ©al, Canada) (NIPSâ€™18) . Curran Associates\nInc., Red Hook, NY, USA, 3393â€“3404.\n[8] Brian F. Cooper, Adam Silberstein, Erwin Tam, Raghu Ramakrishnan, and Russell\nSears. 2010. Benchmarking Cloud Serving Systems with YCSB. In Proceedings\nof the 1st ACM Symposium on Cloud Computing (Indianapolis, Indiana, USA)\n(SoCC â€™10) . Association for Computing Machinery, New York, NY, USA, 143â€“154.\nhttps://doi.org/10.1145/1807128.1807152\n[9] Andrew Crotty. 2021. Hist-Tree: Those Who Ignore It Are Doomed to Learn. In\nConference on Innovative Data Systems Research (CIDRâ€™21) .\n[10] Yifan Dai, Yien Xu, Aishwarya Ganesan, Ramnatthan Alagappan, Brian Kroth,\nAndrea C. Arpaci-Dusseau, and Remzi H. Arpaci-Dusseau. 2020. From WiscKey\nto Bourbon: A Learned Index for Log-Structured Merge Trees. In Proceedings of\nthe 14th USENIX Conference on Operating Systems Design and Implementation\n(OSDIâ€™20) . USENIX Association, USA, Article 9, 17 pages.\n[11] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\nDavid Lomet, and Tim Kraska. 2020. ALEX: An Updatable Adaptive Learned Index.\nInProceedings of the 2020 ACM SIGMOD International Conference on Management\nof Data (Portland, OR, USA) (SIGMOD â€™20) . Association for Computing Machinery,\nNew York, NY, USA, 969â€“984. https://doi.org/10.1145/3318464.3389711\n[12] Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska.\n2020. Tsunami: A Learned Multi-Dimensional Index for Correlated\nData and Skewed Workloads. Proc. VLDB Endow. 14, 2 (oct 2020), 74â€“86.\nhttps://doi.org/10.14778/3425879.3425880\n[13] Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-Index: A Fully-Dynamic\nCompressed Learned Index with Provable Worst-Case Bounds. Proc. VLDB Endow.\n13, 8 (apr 2020), 1162â€“1175. https://doi.org/10.14778/3389133.3389135\n[14] L. FOX, H. D. HUSKEY, and J. H. WILKINSON. 1948. NOTES ON THE SOLUTION\nOF ALGEBRAIC LINEAR SIMULTANEOUS EQUATIONS. The Quarterly\nJournal of Mechanics and Applied Mathematics 1, 1 (01 1948), 149â€“173. https:\n//doi.org/10.1093/qjmam/1.1.149 arXiv:https://academic.oup.com/qjmam/article-\npdf/1/1/149/5322943/1-1-149.pdf\n[15] K. A. Gallivan, R. J. Plemmons, and A. H. Sameh. 1990. Parallel Algorithms\nfor Dense Linear Algebra Computations. SIAM Rev. 32, 1 (mar 1990), 54â€“135.\nhttps://doi.org/10.1137/1032002\n[16] JianGao,XinCao,XinYao,GongZhang,andWeiWang.2023. LMSFC:ANovelMul-\ntidimensional Index Based on Learned Monotonic Space Filling Curves. Proc. VLDB\nEndow. 16, 10 (aug 2023), 2605â€“2617. https://doi.org/10.14778/3603581.3603598\n[17] Gene H. Golub and Charles F. Van Loan. 1996. Matrix Computations (third ed.).\nThe Johns Hopkins University Press.\n[18] Tim Gubner, Viktor Leis, and Peter Boncz. 2021. Optimistically Compressed\nHash Tables & Strings in TheUSSR. SIGMOD Rec. 50, 1 (jun 2021), 60â€“67.\nhttps://doi.org/10.1145/3471485.3471500\n[19] Milad Hashemi, Kevin Swersky, Jamie A. Smith, Grant Ayers, Heiner Litz, Jichuan\nChang, Christos Kozyrakis, and Parthasarathy Ranganathan. 2018. Learning\nMemory Access Patterns. CoRR abs/1803.02329 (2018). arXiv:1803.02329\nhttp://arxiv.org/abs/1803.02329\n[20] Steffen Heinz, Justin Zobel, and Hugh E. Williams. 2002. Burst Tries: A Fast,\nEfficient Data Structure for String Keys. ACM Transactions on Information Systems\n20, 2 (2002), 902â€“915.[21] Don Heller. 1978. A Survey of Parallel Algorithms in Numerical Linear Algebra.\nSIAM Rev. 20, 4 (1978), 740â€“777. https://doi.org/10.1137/1020096\n[22] Alston S. Householder. 1958. Unitary Triangularization of a Nonsymmetric\nMatrix. J. ACM 5, 4 (oct 1958), 339â€“342. https://doi.org/10.1145/320941.320947\n[23] Junsu Im, Jinwook Bae, Chanwoo Chung, Arvind Arvind, and Sungjin Lee. 2020.\nPinK: High-Speed in-Storage Key-Value Store with Bounded Tails. In Proceedings\nof the 2020 USENIX Conference on Usenix Annual Technical Conference (USENIX\nATCâ€™20) . USENIX Association, USA, Article 12, 15 pages.\n[24] Intel. 2023. Intel FPGA. https://www.intel.com/content/www/us/en/products/\ndetails/fpga.html.\n[25] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2020. RadixSpline: A Single-Pass Learned\nIndex. In Proceedings of the Third International Workshop on Exploiting Artificial\nIntelligence Techniques for Data Management (Portland, Oregon) (aiDM â€™20) .\nAssociation for Computing Machinery, New York, NY, USA, Article 5, 5 pages.\nhttps://doi.org/10.1145/3401071.3401659\n[26] Evgenios M. Kornaropoulos, Silei Ren, and Roberto Tamassia. 2022. The Price of\nTailoring the Index to Your Data: Poisoning Attacks on Learned Index Structures.\nInProceedings of the 2022 International Conference on Management of Data\n(Philadelphia, PA, USA) (SIGMOD â€™22) . Association for Computing Machinery,\nNew York, NY, USA, 1331â€“1344. https://doi.org/10.1145/3514221.3517867\n[27] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis.\n2018. The Case for Learned Index Structures. In Proceedings of the 2018\nInternational Conference on Management of Data (Houston, TX, USA) (SIGMOD\nâ€™18). Association for Computing Machinery, New York, NY, USA, 489â€“504.\nhttps://doi.org/10.1145/3183713.3196909\n[28] Branimir Lambov. 2022. Trie Memtables in Cassandra. Proc. VLDB Endow. 15,\n12 (aug 2022), 3359â€“3371. https://doi.org/10.14778/3554821.3554828\n[29] Hai Lan, Zhifeng Bao, J. Shane Culpepper, and Renata Borovica-Gajic. 2023.\nUpdatable Learned Indexes Meet Disk-Resident DBMS - From Evaluations to\nDesign Choices. Proc. ACM Manag. Data 1, 2, Article 139 (jun 2023), 22 pages.\nhttps://doi.org/10.1145/3589284\n[30] Martin Langhammer and Bogdan Pasca. 2018. High-Performance QR De-\ncomposition for FPGAs. In Proceedings of the 2018 ACM/SIGDA International\nSymposium on Field-Programmable Gate Arrays (Monterey, CALIFORNIA, USA)\n(FPGA â€™18) . Association for Computing Machinery, New York, NY, USA, 183â€“188.\nhttps://doi.org/10.1145/3174243.3174273\n[31] Se Kwon Lee, Jayashree Mohan, Sanidhya Kashyap, Taesoo Kim, and Vijay\nChidambaram. 2019. Recipe: Converting Concurrent DRAM Indexes to Persistent-\nMemory Indexes. In Proceedings of the 27th ACM Symposium on Operating Systems\nPrinciples (Huntsville, Ontario, Canada) (SOSP â€™19) . Association for Computing Ma-\nchinery, New York, NY, USA, 462â€“477. https://doi.org/10.1145/3341301.3359635\n[32] Jure Leskovec, Lars Backstrom, and Jon Kleinberg. 2009. Meme-Tracking and\nthe Dynamics of the News Cycle. In Proceedings of the 15th ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining (Paris, France)\n(KDD â€™09) . Association for Computing Machinery, New York, NY, USA, 497â€“506.\nhttps://doi.org/10.1145/1557019.1557077\n[33] Pengfei Li, Yu Hua, Jingnan Jia, and Pengfei Zuo. 2021. FINEdex: A Fine-Grained\nLearned Index Scheme for Scalable and Concurrent Memory Systems. Proc. VLDB\nEndow. 15, 2 (oct 2021), 321â€“334. https://doi.org/10.14778/3489496.3489512\n[34] Pengfei Li, Hua Lu, Qian Zheng, Long Yang, and Gang Pan. 2020. LISA: A\nLearned Index Structure for Spatial Data. In Proceedings of the 2020 ACM SIGMOD\nInternational Conference on Management of Data (Portland, OR, USA) (SIGMOD\nâ€™20). Association for Computing Machinery, New York, NY, USA, 2119â€“2133.\nhttps://doi.org/10.1145/3318464.3389703\n[35] Pengfei Li, Hua Lu, Rong Zhu, Bolin Ding, Long Yang, and Gang Pan. 2023. DILI: A\nDistribution-Driven Learned Index. Proc. VLDB Endow. 16, 9 (jul 2023), 2212â€“2224.\nhttps://doi.org/10.14778/3598581.3598593\n[36] Baotong Lu, Jialin Ding, Eric Lo, Umar Farooq Minhas, and Tianzheng Wang. 2021.\nAPEX: A High-Performance Learned Index on Persistent Memory. Proc. VLDB\nEndow. 15, 3 (nov 2021), 597â€“610. https://doi.org/10.14778/3494124.3494141\n[37] Siqiang Luo, Subarna Chatterjee, Rafael Ketsetsidis, Niv Dayan, Wilson Qin,\nand Stratos Idreos. 2020. Rosetta: A Robust Space-Time Optimized Range\nFilter for Key-Value Stores. In Proceedings of the 2020 ACM SIGMOD Inter-\nnational Conference on Management of Data (Portland, OR, USA) (SIGMOD\nâ€™20). Association for Computing Machinery, New York, NY, USA, 2071â€“2086.\nhttps://doi.org/10.1145/3318464.3389731\n[38] Chaohong Ma, Xiaohui Yu, Yifan Li, Xiaofeng Meng, and Aishan Maoliniyazi.\n2022. FILM: A Fully Learned Index for Larger-Than-Memory Databases. Proc.\nVLDB Endow. 16, 3 (nov 2022), 561â€“573. https://doi.org/10.14778/3570690.3570704\n[39] Divya Mahajan, Emmanuel Amaro, Hardik Sharma, Amir Yazdanbakhsh,\nJoon Kyung Kim, and Hadi Esmaeilzadeh. 2016. TABLA: A Unified Template-\nbased Framework for Accelerating Statistical Machine Learning. In High\nPerformance Computer Architecture .\n[40] Divya Mahajan, Joon Kyung Kim, Jacob Sacks, Adel Ardalan, Arun Kumar,\nand Hadi Esmaeilzadeh. 2018. In-RDBMS Hardware Acceleration of Advanced\nAnalytics. In PVLDB .\n[41] Ryan Marcus, Andreas Kipf, Alexander van Renen, Mihail Stoian, San-\nchit Misra, Alfons Kemper, Thomas Neumann, and Tim Kraska. 2020.13\n\nBenchmarking Learned Indexes. Proc. VLDB Endow. 14, 1 (sep 2020), 1â€“13.\nhttps://doi.org/10.14778/3421424.3421425\n[42] Ryan Marcus, Emily Zhang, and Tim Kraska. 2020. CDFShop: Exploring and\nOptimizing Learned Index Structures. In Proceedings of the 2020 ACM SIGMOD\nInternational Conference on Management of Data (Portland, OR, USA) (SIGMOD\nâ€™20). Association for Computing Machinery, New York, NY, USA, 2789â€“2792.\nhttps://doi.org/10.1145/3318464.3384706\n[43] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020.\nLearning Multi-Dimensional Indexes. In Proceedings of the 2020 ACM SIGMOD\nInternational Conference on Management of Data (Portland, OR, USA) (SIGMOD\nâ€™20). Association for Computing Machinery, New York, NY, USA, 985â€“1000.\nhttps://doi.org/10.1145/3318464.3380579\n[44] Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying Recommendations\nusing Distantly-Labeled Reviews and Fine-Grained Aspects. In Proceedings\nof the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP) . Association for Computational Linguistics, Hong Kong, China,\n188â€“197. https://doi.org/10.18653/v1/D19-1018\n[45] Nvidia. 2023. Nvidia Flex NIC with FPGA. https://www.nvidia.com/en-\nus/networking/ethernet/innova-2-flex/.\n[46] Jongse Park, Hardik Sharma, Divya Mahajan, Joon Kyung Kim, Preston Olds, and\nHadiEsmaeilzadeh.2016. Scale-OutAccelerationforMachineLearning.In MICRO .\n[47] Robert J Plemmons. 1988. Parallel Block Schemes for Large-Scale Least-Squares\nComputations . Urbana: University of Illinois Press.\n[48] Andrew Putnam, Adrian Caulfield, Eric Chung, Derek Chiou, Kypros Constan-\ntinides, John Demme, Hadi Esmaeilzadeh, Jeremy Fowers, Gopi Prashanth, Jan\nGray, Michael Haselman, Scott Hauck, Stephen Heil, Amir Hormati, Joo-Young\nKim, Sitaram Lanka, James R. Larus, Eric Peterson, Aaron Smith, Jason Thong,\nPhillip Yi Xiao, and Doug Burger. 2014. A Reconfigurable Fabric for Accelerating\nLarge-Scale Datacenter Services. In International Symposium on Computer (ISCA) .\n[49] Abid Rafique, Nachiket Kapre, and George A. Constantinides. 2012. Enhancing\nperformance of Tall-Skinny QR factorization using FPGAs. In 22nd International\nConference on Field Programmable Logic and Applications (FPL) . 443â€“450.\nhttps://doi.org/10.1109/FPL.2012.6339142\n[50] Hardik Sharma, Jongse Park, Divya Mahajan, Emmanuel Amaro, Joon Kyung\nKim, Chenkai Shao, Asit Misra, and Hadi Esmaeilzadeh. 2016. From High-Level\nDeep Neural Models to FPGAs. In MICRO .\n[51] Yufan Sheng, Xin Cao, Yixiang Fang, Kaiqi Zhao, Jianzhong Qi, Gao Cong,\nand Wenjie Zhang. 2023. WISK: A Workload-Aware Learned Index for Spatial\nKeyword Queries. Proc. ACM Manag. Data 1, 2, Article 187 (jun 2023), 27 pages.\nhttps://doi.org/10.1145/3589332\n[52] Jiachen Shi, Gao Cong, and Xiao-Li Li. 2022. Learned Index Benefits: Machine\nLearning Based Index Performance Estimation. Proc. VLDB Endow. 15, 13 (sep\n2022), 3950â€“3962. https://doi.org/10.14778/3565838.3565848\n[53] Benjamin Spector, Andreas Kipf, Kapil Vaidya, Chi Wang, Umar Farooq Minhas,\nand Tim Kraska. 2021. Bounding the Last Mile: Efficient Learned String Indexing.\nArXiv abs/2111.14905 (2021). https://api.semanticscholar.org/CorpusID:\n244729906\n[54] Mihail Stoian, Andreas Kipf, Ryan Marcus, and Tim Kraska. 2021. Towards\nPractical Learned Indexing. In Workshop on Databases and AI (AIDBâ€™21) .\n[55] Zhaoyan Sun, Xuanhe Zhou, and Guoliang Li. 2023. Learned Index: A Compre-\nhensive Experimental Evaluation. Proc. VLDB Endow. 16, 8 (jun 2023), 1992â€“2004.\nhttps://doi.org/10.14778/3594512.3594528\n[56] Chuzhe Tang, Youyun Wang, Zhiyuan Dong, Gansen Hu, Zhaoguo Wang,\nMinjie Wang, and Haibo Chen. 2020. XIndex: A Scalable Learned Index for\nMulticore Data Storage. In Proceedings of the 25th ACM SIGPLAN Symposium on\nPrinciples and Practice of Parallel Programming (San Diego, California) (PPoPP\nâ€™20). Association for Computing Machinery, New York, NY, USA, 308â€“320.\nhttps://doi.org/10.1145/3332466.3374547\n[57] Yifan Wang, Haodi Ma, and Daisy Zhe Wang. 2022. LIDER: An Efficient\nHigh-Dimensional Learned Index for Large-Scale Dense Passage Retrieval. Proc.\nVLDB Endow. 16, 2 (oct 2022), 154â€“166. https://doi.org/10.14778/3565816.3565819[58] Youyun Wang, Chuzhe Tang, Zhaoguo Wang, and Haibo Chen. 2020. SIn-\ndex: A Scalable Learned Index for String Keys. In Proceedings of the 11th\nACM SIGOPS Asia-Pacific Workshop on Systems (Tsukuba, Japan) (APSys\nâ€™20). Association for Computing Machinery, New York, NY, USA, 17â€“24.\nhttps://doi.org/10.1145/3409963.3410496\n[59] Zheng Wang and Michael Oâ€™Boyle. 2018. Machine Learning in Com-\npiler Optimization. Proc. IEEE 106, 11 (2018), 1879â€“1901. https:\n//doi.org/10.1109/JPROC.2018.2817118\n[60] Xingda Wei, Rong Chen, Haibo Chen, and Binyu Zang. 2021. XStore: Fast\nRDMA-Based Ordered Key-Value Store Using Remote Learned Cache. ACM Trans.\nStorage 17, 3, Article 18 (aug 2021), 32 pages. https://doi.org/10.1145/3468520\n[61] Jiacheng Wu, Yong Zhang, Shimin Chen, Jin Wang, Yu Chen, and Chunxiao Xing.\n2021. Updatable Learned Index with Precise Positions. Proc. VLDB Endow. 14,\n8 (apr 2021), 1276â€“1288. https://doi.org/10.14778/3457390.3457393\n[62] Shangyu Wu, Yufei Cui, Jinghuan Yu, Xuan Sun, Tei-Wei Kuo, and Chun Jason Xue.\n2022. NFL: Robust Learned Index via Distribution Transformation. Proc. VLDB\nEndow. 15, 10 (jun 2022), 2188â€“2200. https://doi.org/10.14778/3547305.3547322\n[63] Xingbo Wu, Fan Ni, and Song Jiang. 2019. Wormhole: A Fast Ordered Index for\nIn-Memory Data Management. In Proceedings of the Fourteenth EuroSys Conference\n2019 (Dresden, Germany) (EuroSys â€™19) . Association for Computing Machinery,\nNew York, NY, USA, Article 18, 16 pages. https://doi.org/10.1145/3302424.3303955\n[64] Giorgos Xanthakis, Giorgos Saloustros, Nikos Batsaras, Anastasios Papagiannis,\nand Angelos Bilas. 2021. Parallax: Hybrid Key-Value Placement in LSM-Based\nKey-Value Stores. In Proceedings of the ACM Symposium on Cloud Computing\n(Seattle, WA, USA) (SoCC â€™21) . Association for Computing Machinery, New York,\nNY, USA, 305â€“318. https://doi.org/10.1145/3472883.3487012\n[65] Jin Yang, Heejin Yoon, Gyeongchan Yun, Sam H. Noh, and Young-ri Choi. 2023.\nDyTIS: A Dynamic Dataset Targeted Index Structure Simultaneously Efficient\nfor Search, Insert, and Scan. In Proceedings of the Eighteenth European Conference\non Computer Systems (Rome, Italy) (EuroSys â€™23) . Association for Computing Ma-\nchinery, New York, NY, USA, 800â€“816. https://doi.org/10.1145/3552326.3587434\n[66] Juncheng Yang, Yao Yue, and K. V. Rashmi. 2020. A Large Scale Analysis of\nHundreds of In-Memory Cache Clusters at Twitter. In Proceedings of the 14th\nUSENIX Conference on Operating Systems Design and Implementation (OSDIâ€™20) .\nUSENIX Association, USA, Article 11, 18 pages.\n[67] Geoffrey X. Yu, Markos Markakis, Andreas Kipf, Per-Ã…ke Larson, Umar Farooq\nMinhas, and Tim Kraska. 2022. TreeLine: An Update-in-Place Key-Value\nStore for Modern Storage. Proc. VLDB Endow. 16, 1 (sep 2022), 99â€“112.\nhttps://doi.org/10.14778/3561261.3561270\n[68] Adar Zeitak and Adam Morrison. 2021. Cuckoo Trie: Exploiting Memory-Level\nParallelism for Efficient DRAM Indexing. In Proceedings of the ACM SIGOPS\n28th Symposium on Operating Systems Principles (Virtual Event, Germany)\n(SOSP â€™21) . Association for Computing Machinery, New York, NY, USA, 147â€“162.\nhttps://doi.org/10.1145/3477132.3483551\n[69] Songnian Zhang, Suprio Ray, Rongxing Lu, and Yandong Zheng. 2021.\nSPRIG: A Learned Spatial Index for Range and KNN Queries. In 17th Inter-\nnational Symposium on Spatial and Temporal Databases (virtual, USA) (SSTD\nâ€™21). Association for Computing Machinery, New York, NY, USA, 96â€“105.\nhttps://doi.org/10.1145/3469830.3470892\n[70] Zhou Zhang, Zhaole Chu, Peiquan Jin, Yongping Luo, Xike Xie, Shouhong Wan,\nYun Luo, Xufei Wu, Peng Zou, Chunyang Zheng, Guoan Wu, and Andy Rudoff.\n2022. PLIN: A Persistent Learned Index for Non-Volatile Memory with High\nPerformance and Instant Recovery. Proc. VLDB Endow. 16, 2 (oct 2022), 243â€“255.\nhttps://doi.org/10.14778/3565816.3565826\n[71] Zhou Zhang, Peiquan Jin, Xiaoliang Wang, Yanqi Lv, Shouhong Wan, and Xike Xie.\n2021. COLIN: A Cache-Conscious Dynamic Learned Index with High Read/Write\nPerformance. Journal of Computer Science and Technology 36 (2021), 721â€“740.\n[72] Zejian Zhang, Yan Wang, and Shunzhi Zhu. 2021. LIDUSA â€“ A Learned Index\nStructure for Dynamical Uneven Spatial Data. In Algorithms and Architectures\nfor Parallel Processing: 21st International Conference, ICA3PP 2021, Virtual Event,\nDecember 3â€“5, 2021, Proceedings, Part III . Springer-Verlag, Berlin, Heidelberg,\n737â€“753. https://doi.org/10.1007/978-3-030-95391-1_46\n14",
  "textLength": 87179
}