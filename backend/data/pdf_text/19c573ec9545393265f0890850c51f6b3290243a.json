{
  "paperId": "19c573ec9545393265f0890850c51f6b3290243a",
  "title": "LSI: a learned secondary index structure",
  "pdfPath": "19c573ec9545393265f0890850c51f6b3290243a.pdf",
  "text": "LSI: A Learned Secondary Index Structure\nAndreas Kipf\nMIT CSAIL\nCambridge, MA, USA\nkipf@mit.eduDominik Horn\nMIT CSAIL\nCambridge, MA, USA\nhorndo@mit.eduPascal Pfeil\nMIT CSAIL\nCambridge, MA, USA\npfeil@mit.edu\nRyan Marcus\nUniversity of Pennsylvania, Intel Labs\nCambridge, MA, USA\nryanmarcus@mit.eduTim Kraska\nMIT CSAIL\nCambridge, MA, USA\nkraska@mit.edu\nABSTRACT\nLearned index structures have been shown to achieve favorable\nlookup performance and space consumption compared to their\ntraditional counterparts such as B-trees. However, most learned\nindex studies have focused on the primary indexing setting, where\nthe base data is sorted. In this work, we investigate whether learned\nindexes sustain their advantage in the secondary indexing setting.\nWe introduce Learned Secondary Index (LSI), a first attempt to use\nlearned indexes for indexing unsorted data. LSI works by building\na learned index over a permutation vector, which allows binary\nsearch to performed on the unsorted base data using random access.\nWe additionally augment LSI with a fingerprint vector to accelerate\nequality lookups. We show that LSI achieves comparable lookup\nperformance to state-of-the-art secondary indexes while being up\nto 6√ómore space efficient.\nACM Reference Format:\nAndreas Kipf, Dominik Horn, Pascal Pfeil, Ryan Marcus, and Tim Kraska.\n2022. LSI: A Learned Secondary Index Structure. In Exploiting Artificial\nIntelligence Techniques for Data (aiDM‚Äô22 ), June 17, 2022, Philadelphia, PA,\nUSA. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3533702.\n3534912\n1 INTRODUCTION\nUnlike traditional index structures such as B-trees, learned in-\ndexes [ 14] build a model over the underlying data to predict the\nposition of a lookup key in a sorted array. Learned indexes effec-\ntively compress the cumulative distribution function (CDF) of the\ndata. When the underlying data has a learnable pattern, the result-\ning learned index can be both faster and smaller than its traditional\ncounterpart. Learned index structures can be built in a variety of\nways (e.g., top down [ 14,19], bottom up [ 9,13]), but all learned\nindex structures provide (1) a mapping from keys to predicted posi-\ntions, and (2) the maximum error that prediction can incur. Exact\nlookups can thus be performed via binary search on the underlying\ndata, restricted to the area around the predicted position. While\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\naiDM‚Äô22 , June 17, 2022, Philadelphia, PA, USA\n¬©2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-9377-5/22/06. . . $15.00\nhttps://doi.org/10.1145/3533702.3534912the initial proposal [ 14] considered neural networks as a building\nblock, current proposals use simple functions which are fast to build\nand evaluate [ 23]. Extensive studies including the Search on Sorted\nData benchmark [ 12,18] and an independent analysis by Maltry\nand Dittrich [ 17] have shown that learned indexes are competitive\nwith their traditional counterparts [ 11,16] in at least one specific\nscenario: equality and range lookups of integer keys in a sorted,\nin-memory array.\nSince their initial conception, learned indexes have been ex-\ntended to support updates [ 7,20,26], strings [ 24], spatial data [ 22,\n28], and disk-based systems [ 3,6]. However, all of these proposals\nuse learned indexes in a ‚Äúclustered index‚Äù setting: where the un-\nderlying data is already sorted. In other words, existing proposals\nare primary index structures. The case when the underlying data is\nnot sorted is also important (and arguably more common, as an in-\nstance of a table can only be sorted by a single key). Unfortunately,\nthis case has received very little attention. As Ferragina and Vin-\nciguerra point out [ 8], the most obvious way to use learned indexes\nin a secondary index scenario is to store sorted (key, pointer)\npairs, very much like what is being stored in the leaf nodes of a\nB+-tree. These sorted pairs are the dominant size component of a\ntraditional index, so the space overhead between a traditional and\nlearned secondary index would be roughly similar. This raises the\nquestion whether the superior lookup efficiency and space savings\nof learned indexes prevail in the secondary indexing case.\nIn this paper, we introduce Learned Secondary Index (LSI)1. LSI\nis based on PLEX [ 25], which is a bottom-up learned index com-\nbining RadixSpline [ 13] and Hist-Tree [ 5]. We selected PLEX for\nits simplicity, since PLEX uses only one hyperparameter (the maxi-\nmum prediction error). LSI addresses the following problem: given\nan unsorted, in-memory array of integer keys, find the smallest\nkey that is greater than or equal to the lookup key (lower-bound\nlookup). Our key insight is that we do not need to explicitly store\nthe key array (like in B+-tree leaf nodes) ‚Äì instead, we store a per-\nmutation vector that stores a mapping between the key‚Äôs position in\na sorted order and the unsorted position of each key. Using PLEX‚Äôs\nprediction, we index into this permutation vector and perform a bi-\nnary search on the underlying (unsorted) data array using random\naccess. For equality lookups, we provide an additional optimization:\nsince the learned index is imperfect (and thus provides a range of\nvalues in the permutation vector instead of the exact value), mul-\ntiple entries of the underlying unsorted data need to be checked.\n1https://github.com/learnedsystems/LearnedSecondaryIndexarXiv:2205.05769v1  [cs.DB]  11 May 2022\n\naiDM‚Äô22 , June 17, 2022, Philadelphia, PA, USA Kipf, et al.\nLearned Index\nBase DataPermutationVector (TIDs)Sorted Array (Keys)69854032711942312115121347515712151921313442\n0123456789Lookup1Search Bound2Local Search3\nOffsetsNot explicitly stored10011000111001001001Fingerprint Vector\nFigure 1: Learned Secondary Index and its lookup procedure.\nThe local search uses the permutation vector to locate cor-\nresponding entries in the base data. The fingerprint vector\nprunes unnecessary accesses.\nTo reduce these ‚Äúfalse positives,‚Äù LSI additionally stores hash fin-\ngerprints (typically a few bits) for each entry in the permutation\nvector. These fingerprints introduce an interesting trade off: If we\nwant to answer a search using fingerprint bits, we need to resort to\na linear scan over the predicted (error-bounded) range, while we\ncould use binary search otherwise. And, obviously, fingerprint bits\nonly help with equality lookups (where the lookup key is part of\nthe data) and do not accelerate lower-bound lookups in general.\nWe show that LSI can compete with established secondary in-\ndexes such as ART [ 16] in terms of lookup performance while\nconsuming up to 6√óless space.\nRelated Work. There is limited work on using learning for sec-\nondary indexing. HERMIT [ 27] learns a mapping between a pri-\nmary and correlated secondary columns using linear functions. That\nway it can reuse a primary index for indexing secondary columns.\nCortex [ 21] follows a similar approach but can also capture more\ncomplex correlations.\n2 LEARNED SECONDARY INDEX\nLSI consists of three parts: a permutation vector, a learned index,\nand a fingerprint vector, which we will describe in the following\n(see Figure 1).\nPermutation Vector. The permutation vector is a compressed rep-\nresentation of the sorted order of the underlying unsorted base data\nùëë. Specifically, the permutation vector ùëùis set so that every entry\nùëù[ùëñ]contains the index into ùëëcorresponding to the ùëñth smallest key.\nIn other words, if one wished to access the 6th smallest key, one\nwould access ùëë[ùëù[6]]. The permutation vector allows us to build\na learned index over the sorted keys, and then map predictions\nfrom that learned index into the underlying unsorted base data. We\nbitpack these permutation vectors, allowing each entry to be stored\nusing approximately ùëôùëúùëî2ùëõbits, making the size of the permutation\nvector ùëÇ(ùëõlogùëõ). Additional techniques, like Lehmer codes [ 15],\ncould be used to further compress this vector, at the cost of higher\ndecompression time.Learned Index. The learned (or approximate) index maps a lookup\nkey to a bounded search range. One could think of the inner nodes\nof a B+-tree as such an approximate index. A lookup in a B+-tree‚Äôs\ninner node structure will identify a leaf node, which depending on\nthe fanout contains ùëòentries. In other words, a lookup will bound\nthe last-level search to ùëòentries. In our implementation, we use the\nlearned index PLEX [ 25], selected for its simplicity. PLEX builds a\nspline model over the cumulative distribution function (CDF) of\nthe data and bounds the of the last-level search to a user-defined\nrange (defined by the model‚Äôs maximum error). Note that LSI does\nnotstore an explicit representation of the sorted key array. This is\nin contrast to a B+-tree, which stores actual keys in its leaf nodes.\nInstead, LSI uses the permutation vector to map PLEX‚Äôs predictions\ninto the underlying data. While this approach can save significant\nspace, the downside is that LSI produces false positives, since PLEX\nonly produces approximate ranges. We use the permutation vector\nto perform binary search within the approximate range. Note that\nthe cost of this binary search is higher than for sorted data, as all\nmemory accesses to the unsorted base data are likely out of cache.\nFingerprint Vector. For equality lookups, the approximate range\nreturned by the learned index needs to be entirely searched. As\na result, many non-relevant keys may be scanned. To mitigate\nthis, we create a fingerprint vector , which stores fixed-sized hash\nfingerprints (e.g., 8 bits) for each key. We use the Murmur3 hash\nfunction to generate hash values and extract the first ùë•bits as hash\nfingerprints. When performing an equality lookup, we first ensure\nthat the fingerprint of the lookup key matches the fingerprint stored\nin the fingerprint vector ‚Äì if it does, we then access the permutation\nvector to get the index into the underlying data, and then access\nthe underlying data. However, if the fingerprints do not match, we\nskip accessing both the permutation vector and the underlying data,\npotentially saving cache misses.\n2.1 Building LSI\nBuilding LSI involves multiple steps. First, we create a sorted copy\nof the base data. Then we build a cumulative distribution function\n(CDF) on the sorted data, which maps each key to its position in the\nsorted array. Note that the data may contain duplicates, which will\nresult in a ‚Äústeeper‚Äù slope in the CDF that can in turn be more diffi-\ncult to approximate. An alternative would be to remove duplicates\nupfront and maintain a rank structure to map from the duplicate-\nfree representation to the base data. However, we found that to\nnot be worthwhile, both in terms of space and lookup performance.\nOnce we have created the CDF, we build an error-bounded learned\nindex (PLEX) over it. Next, we create a bit-packed permutation\nvector that maps from the sorted data (over which we have built the\nindex) to the unsorted base data. Finally, we build an array contain-\ning fingerprints. The sorted copy of the data is then discarded, as it\nis no longer needed. Overall, LSI has two parameters, the maximum\nerror of its learned index and the number of fingerprint bits.\n2.2 Lookup Procedure\nLookups proceed as follows (see Figure 1). First, we query the\nlearned index model with the lookup key which returns a range\nthat is guaranteed to contain the lookup key in a sorted version\nof the array. Next, we perform a local search within the search\n\nLSI: A Learned Secondary Index Structure aiDM‚Äô22 , June 17, 2022, Philadelphia, PA, USA\n02040 148amzn\n148osm\n02040 148fb\n148wikiBuild time (s)\nart btree lsi robinhash\nFigure 2: Build time in seconds. The text annotations denote\nthe error bounds.\nrange. If it is an equality lookup, we perform a linear search on the\nfingerprint vector and, if a fingerprint matches, perform a random\naccess to the base data using the permutation vector. Once we\nhave found a matching key in the base data, we keep scanning\nthe fingerprint vector for matching fingerprints (as there may be\nduplicates in the base data). However, after having found a first\nqualifying key in the base data, we can stop scanning the fingerprint\nvector once we found a non-matching fingerprint. If the lookup is\na lower-bound lookup, we cannot use the fingerprint vector and\nuse binary search within the search range.\n3 EVALUATION\nWe evaluate Learned Secondary Index (LSI) using a variation of\nthe SOSD benchmark [ 12,18] on a c5.9xlarge AWS machine with\n36 vCPUs and 72 GiB of RAM. We perform single-threaded lower-\nbound and equality lookups on four real-world datasets from SOSD.\nTo prevent out-of-order execution, we use a memory barrier in\nbetween individual lookups. Each dataset consists of 200 M 64-bit\nunsigned integer keys: amzn (book popularity data), face (randomly\nsampled Facebook user IDs), osm(cell IDs from Open Street Map),\nandwiki (timestamps of edits from Wikipedia). We generate ran-\ndom 8-byte payloads for each key. We compare LSI with several\nbaselines: the STX B-Tree (BTree) [ 2], the Adaptive Radix Tree\n(ART) [16], and a robin-hood hash table (RobinHash) [1].\nBuild Times. While BTree and ART can index unsorted keys out of\nthe box, LSI‚Äôs learned index (PLEX) needs a sorted copy of the data\nto build a CDF and train its model on the CDF. However, inserting\nkeys in random order into the BTree is about 8 √óslower than first\nsorting the keys and then using its bulk loading functionality. We\nhence bulk load the BTree, which also yields denser nodes. Likewise,\nART achieves a speedup of almost 2 √ówhen inserting keys in sorted\norder, despite not having a separate bulk loading interface. Figure 2\nshows the total build times, which includes the time for sorting the\ndata (except for RobinHash where sorting does not have an effect).\nBTree achieves the lowest build times, followed by RobinHash and\nLSI with an error bound of 8. As one would expect, decreasing the\nerror bound increases the build time since the learned index model\nrequires more spline points to satisfy the error bound. ART does\nnot offer bulk loading functionality and has the highest build times,\nexcept for amzn where RobinHash requires the most time to build.\n050010001500\n148amzn\n148osm\nart\nbtreelsi\n01234567050010001500\n148fb\n01234567148wiki\nSize (GiB)Latency (ns)Figure 3: Lower-bound lookups using non-existing keys.\nThe text annotations denote the error bounds.\nLower-Bound Lookups. We first study the performance of lower-\nbound lookups, i.e., returning the value of the first key that is not\nless than the lookup key. We remove a random subset of keys (10%)\nfrom the dataset and use them as lookup keys. For all datasets,\nexcept wiki which contains duplicates, lookups will be with keys\nthat do not exist in the data. Hence, we cannot use fingerprint\nhashes and also cannot compare against hash tables. Figure 3 shows\nthe results. LSI achieves the best trade off between size and lookup\nlatency. It matches ART‚Äôs lookup latency while consuming up to\n6√óless space. Note that both BTree and ART support updates while\nLSI does not. Also, compressing the leaf layer of BTree in a similar\nfashion (using bit-packed TIDs instead of 8-byte payloads) would\nyield space savings but would not necessarily improve performance.\nTIDs in ART cannot be compressed as easily as they are inlined\ninto 8-byte child pointers.\nEquality Lookups. We now look at the special case of equality\nlookups. In this experiment, we enable hash fingerprints (8 bits) in\nLSI and also compare against RobinHash. Figure 4 shows the results\non the amzn dataset. RobinHash achieves a latency of around 440 ns\nper lookup which is faster than LSI‚Äôs 660 ns but also consumes 4 √ó\nthe amount of space. Using hash fingerprint bits requires LSI to\nperform a linear scan through the search range. We now perform\na micro experiment to study the trade off between using finger-\nprints (with linear search) and binary search. We train LSI with\nfour different error (4, 16, 64, and 256) and six different fingerprint\nconfigurations (0, 1, 2, 4, 8, and 16 bits). If there are fingerprint bits,\nwe use linear search and use binary search otherwise. As shown in\nFigure 5, the variants using 4 and 16 fingerprint bits are faster than\nbinary search for certain error configurations.\nSpace Breakdown. Table 1 shows the space breakdown of LSI\nwith errors of 4 (LSI4) and 8 (LSI8) compared to ART, BTree, and\nRobinHash for the amzn dataset. LSI spends 64% and 80% of its\nspace on the permutation vector with an error bound of 4 and 8,\nrespectively. This raises the questions whether we can compress\nthe permutation vector.\nCompressing the Permutation Vector. The information-theoretic\nlower bound for storing a permutation of ùëõelements is ùëÇ(ùëôùëúùëî2(ùëõ!))\nbits. Compared to our current bit-packed representation which re-\nquires ùëÇ(ùëõ‚àóùëôùëúùëî2(ùëõ))bits, this saves at most 1.44 bits per key (see\nFigure 6). Using a Bene≈° network [ 4], we can get arbitrarily close\n\naiDM‚Äô22 , June 17, 2022, Philadelphia, PA, USA Kipf, et al.\n0 2 4 6 8\nSize (GiB)0200400600Latency (ns)148\nlsi\nrobinhash\nFigure 4: Equality lookups on the amzn dataset comparing\nLSI to RobinHash.\n4 16 64 256\nModel error0500100015002000Latency (ns)\nbinary linear (1) linear (4) linear (16)\nFigure 5: Binary search vs. linear search with varying finger-\nprint sizes (in brackets).\nTable 1: Space breakdown of LSI for the amzn dataset.\nIndex Overall Size (MiB) Model (MiB) Permutation (MiB)\nLSI4 943 342 601\nLSI8 754 153 601\nART 4,343 - -\nBTree 2,923 - -\nRobinHash 8,192 - -\nto the information-theoretic lower bound, however, we would sac-\nrifice lookup time due to the compressed representation. An access\nto the compressed bitvector would cost ùëÇ(ùëôùëúùëî2(ùëõ))time instead of\ntheùëÇ(1)random access if we merely bitpack. Hence, in the best\ncase, we can reduce the size of the permutation vector in Table 1\nfrom 601 MiB to roughly 557 MiB.\nUsing a Different Approximate Index. The approximate index\nlayer of LSI requires an index structure that given a lookup key re-\nturns an error-bounded range. Besides other error-bounded learned\nindexes such as PGM [ 9], one could also use a Recursive Model\nIndex (RMI) [ 14] and remember the maximum model error. An-\nother option is the recently proposed Compact Hist-Tree (CHT) [ 5].\nCHT is a compact, read-only radix tree with a fixed fanout and also\nreturns an error-bounded range. Figure 7 shows the results when\nreplacing PLEX in LSI with CHT on the amzn dataset. In summary,\nLSI achieves a better space-performance trade off when using PLEX\nas learned index.\n102104106108\nNumber of keys08162432Bits per keybitpacking\noptimalFigure 6: Size of the permutation vector. Information-\ntheoretic lower bound vs. our bit-packed representation.\n0 2 4 6 8 10\nSize (GiB)600650700750800850Latency (ns)\n148\n1 481616cht\nplex\nFigure 7: Using PLEX vs. CHT as models in LSI for lower-\nbound lookups. The text annotations denote the error\nbounds.\n4 CONCLUSIONS\nWe have introduced LSI, a new learned data structure that can\nindex unsorted data. LSI is a first step towards learned secondary\nindexing. We have shown that our approach can compete with\nstate-of-the-art secondary indexes while being more space efficient.\nIn future work, we plan to extend LSI into multiple directions. First,\nwe want to explore indexing data blocks instead of individual tuples\nwhich will lower the overhead of the permutation vector for low\nand medium cardinality columns. Second, we plan to integrate\nmodel error correction techniques [ 10] to narrow the search range\nand hence reduce the number of false positives. Finally, we want to\nexplore applications to disk-based systems.\nAcknowledgments. This research is supported by Google, Intel,\nand Microsoft as part of DSAIL at MIT, and NSF IIS 1900933. This re-\nsearch was also sponsored by the United States Air Force Research\nLaboratory and the United States Air Force Artificial Intelligence\nAccelerator and was accomplished under Cooperative Agreement\nNumber FA8750-19-2-1000. The views and conclusions contained\nin this document are those of the authors and should not be in-\nterpreted as representing the official policies, either expressed or\nimplied, of the United States Air Force or the U.S. Government. The\nU.S. Government is authorized to reproduce and distribute reprints\nfor Government purposes notwithstanding any copyright notation\nherein. Dominik Horn and Pascal Pfeil were supported by a fellow-\nship within the IFI programme of the German Academic Exchange\nService (DAAD).\n\nLSI: A Learned Secondary Index Structure aiDM‚Äô22 , June 17, 2022, Philadelphia, PA, USA\nREFERENCES\n[1] RobinMap, https://github.com/Tessil/robin-map.\n[2] STX B+ Tree, https://panthema.net/2007/stx-btree/.\n[3]H. Abu-Libdeh, D. Altinb√ºken, A. Beutel, E. H. Chi, L. Doshi, T. Kraska, X. Li,\nA. Ly, and C. Olston. Learned indexes for a google-scale disk-based database.\nCoRR , abs/2012.12501, 2020.\n[4]B. Cohen and D. Boneh. How to Store a Permutation Compactly,\nhttps://hackmd.io/@dabo/rkP8Pcf9t.\n[5]A. Crotty. Hist-tree: Those who ignore it are doomed to learn. In 11th Conference\non Innovative Data Systems Research, CIDR 2021, Virtual Event, January 11-15,\n2021, Online Proceedings . www.cidrdb.org, 2021.\n[6]Y. Dai, Y. Xu, A. Ganesan, R. Alagappan, B. Kroth, A. C. Arpaci-Dusseau, and\nR. H. Arpaci-Dusseau. From WiscKey to Bourbon: A learned index for log-\nstructured merge trees. In 14th USENIX Symposium on Operating Systems Design\nand Implementation, OSDI 2020, Virtual Event, November 4-6, 2020 , pages 155‚Äì171.\nUSENIX Association, 2020.\n[7]J. Ding, U. F. Minhas, J. Yu, C. Wang, J. Do, Y. Li, H. Zhang, B. Chandramouli,\nJ. Gehrke, D. Kossmann, D. B. Lomet, and T. Kraska. ALEX: an updatable adaptive\nlearned index. In D. Maier, R. Pottinger, A. Doan, W. Tan, A. Alawini, and H. Q.\nNgo, editors, Proceedings of the 2020 International Conference on Management of\nData, SIGMOD Conference 2020, online conference [Portland, OR, USA], June 14-19,\n2020, pages 969‚Äì984. ACM, 2020.\n[8]P. Ferragina and G. Vinciguerra. Learned data structures. In Recent Trends in\nLearning From Data , volume 896 of Studies in Computational Intelligence . Springer,\n2020.\n[9]P. Ferragina and G. Vinciguerra. The PGM-index: a fully-dynamic compressed\nlearned index with provable worst-case bounds. Proc. VLDB Endow. , 13(8):1162‚Äì\n1175, 2020.\n[10] A. Hadian and T. Heinis. Shift-Table: A low-latency learned index for range\nqueries using model correction. In Y. Velegrakis, D. Zeinalipour-Yazti, P. K.\nChrysanthis, and F. Guerra, editors, Proceedings of the 24th International Confer-\nence on Extending Database Technology, EDBT 2021, Nicosia, Cyprus, March 23 -\n26, 2021 , pages 253‚Äì264. OpenProceedings.org, 2021.\n[11] C. Kim, J. Chhugani, N. Satish, E. Sedlar, A. D. Nguyen, T. Kaldewey, V. W. Lee,\nS. A. Brandt, and P. Dubey. FAST: fast architecture sensitive tree search on\nmodern cpus and gpus. In A. K. Elmagarmid and D. Agrawal, editors, Proceedings\nof the ACM SIGMOD International Conference on Management of Data, SIGMOD\n2010, Indianapolis, Indiana, USA, June 6-10, 2010 , pages 339‚Äì350. ACM, 2010.\n[12] A. Kipf, R. Marcus, A. van Renen, M. Stoian, A. Kemper, T. Kraska, and T. Neu-\nmann. SOSD: A benchmark for learned indexes. NeurIPS Workshop on Machine\nLearning for Systems , 2019.\n[13] A. Kipf, R. Marcus, A. van Renen, M. Stoian, A. Kemper, T. Kraska, and T. Neu-\nmann. RadixSpline: a single-pass learned index. In R. Bordawekar, O. Shmueli,\nN. Tatbul, and T. K. Ho, editors, Proceedings of the Third International Workshop on\nExploiting Artificial Intelligence Techniques for Data Management, aiDM@SIGMOD\n2020, Portland, Oregon, USA, June 19, 2020 , pages 5:1‚Äì5:5. ACM, 2020.\n[14] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for learned index\nstructures. In G. Das, C. M. Jermaine, and P. A. Bernstein, editors, Proceedings ofthe 2018 International Conference on Management of Data, SIGMOD Conference\n2018, Houston, TX, USA, June 10-15, 2018 , pages 489‚Äì504. ACM, 2018.\n[15] D. H. Lehmer. Teaching combinatorial tricks to a computer. In Proc. Sympos.\nAppl. Math. Combinatorial Analysis , volume 10, pages 179‚Äì193, 1960.\n[16] V. Leis, A. Kemper, and T. Neumann. The Adaptive Radix Tree: ARTful indexing\nfor main-memory databases. In C. S. Jensen, C. M. Jermaine, and X. Zhou, editors,\n29th IEEE International Conference on Data Engineering, ICDE 2013, Brisbane,\nAustralia, April 8-12, 2013 , pages 38‚Äì49. IEEE Computer Society, 2013.\n[17] M. Maltry and J. Dittrich. A critical analysis of recursive model indexes. CoRR ,\nabs/2106.16166, 2021.\n[18] R. Marcus, A. Kipf, A. van Renen, M. Stoian, S. Misra, A. Kemper, T. Neumann,\nand T. Kraska. Benchmarking learned indexes. Proc. VLDB Endow. , 14(1):1‚Äì13,\n2020.\n[19] R. Marcus, E. Zhang, and T. Kraska. CDFShop: Exploring and optimizing learned\nindex structures. In D. Maier, R. Pottinger, A. Doan, W. Tan, A. Alawini, and H. Q.\nNgo, editors, Proceedings of the 2020 International Conference on Management of\nData, SIGMOD Conference 2020, online conference [Portland, OR, USA], June 14-19,\n2020, pages 2789‚Äì2792. ACM, 2020.\n[20] M. Mishra and R. Singhal. RUSLI: real-time updatable spline learned index. In\nR. Bordawekar, Y. Amsterdamer, O. Shmueli, and N. Tatbul, editors, aiDM ‚Äô21:\nFourth Workshop in Exploiting AI Techniques for Data Management, Virtual Event,\nChina, 25 June, 2021 , pages 1‚Äì8. ACM, 2021.\n[21] V. Nathan, J. Ding, T. Kraska, and M. Alizadeh. Cortex: Harnessing correlations\nto boost query performance. CoRR , abs/2012.06683, 2020.\n[22] V. Pandey, A. van Renen, A. Kipf, J. Ding, I. Sabek, and A. Kemper. The case for\nlearned spatial indexes. 2nd International Workshop on Applied AI for Database\nSystems and Applications , 2020.\n[23] N. F. Setiawan, B. I. P. Rubinstein, and R. Borovica-Gajic. Function interpolation\nfor learned index structures. In R. Borovica-Gajic, J. Qi, and W. Wang, editors,\nDatabases Theory and Applications - 31st Australasian Database Conference, ADC\n2020, Melbourne, VIC, Australia, February 3-7, 2020, Proceedings , volume 12008 of\nLecture Notes in Computer Science , pages 68‚Äì80. Springer, 2020.\n[24] B. Spector, A. Kipf, K. Vaidya, C. Wang, U. F. Minhas, and T. Kraska. Bounding\nthe last mile: Efficient learned string indexing. 3rd International Workshop on\nApplied AI for Database Systems and Applications , 2021.\n[25] M. Stoian, A. Kipf, R. Marcus, and T. Kraska. PLEX: towards practical learned\nindexing. 3rd International Workshop on Applied AI for Database Systems and\nApplications , 2021.\n[26] J. Wu, Y. Zhang, S. Chen, Y. Chen, J. Wang, and C. Xing. Updatable learned index\nwith precise positions. Proc. VLDB Endow. , 14(8):1276‚Äì1288, 2021.\n[27] Y. Wu, J. Yu, Y. Tian, R. Sidle, and R. Barber. Designing succinct secondary index-\ning mechanism by exploiting column correlations. In P. A. Boncz, S. Manegold,\nA. Ailamaki, A. Deshpande, and T. Kraska, editors, Proceedings of the 2019 Interna-\ntional Conference on Management of Data, SIGMOD Conference 2019, Amsterdam,\nThe Netherlands, June 30 - July 5, 2019 , pages 1223‚Äì1240. ACM, 2019.\n[28] E. T. Zacharatou, A. Kipf, I. Sabek, V. Pandey, H. Doraiswamy, and V. Markl.\nThe case for distance-bounded spatial approximations. In 11th Conference on\nInnovative Data Systems Research, CIDR 2021, Virtual Event, January 11-15, 2021,\nOnline Proceedings . www.cidrdb.org, 2021.",
  "textLength": 27847
}