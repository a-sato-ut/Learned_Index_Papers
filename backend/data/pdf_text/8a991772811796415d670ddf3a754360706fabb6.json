{
  "paperId": "8a991772811796415d670ddf3a754360706fabb6",
  "title": "Multi-Temporal Analysis and Scaling Relations of 100,000,000,000 Network Packets",
  "pdfPath": "8a991772811796415d670ddf3a754360706fabb6.pdf",
  "text": "Multi-Temporal Analysis and Scaling Relations of\n100,000,000,000 Network Packets\nJeremy Kepner1;2;3, Chad Meiners4, Chansup Byun1, Sarah McGuire4, Timothy Davis5, William Arcand1,\nJonathan Bernays4, David Bestor1, William Bergeron1, Vijay Gadepally1;2, Raul Harnasch4, Matthew Hubbell1,\nMicheal Houle1, Micheal Jones1, Andrew Kirby1, Anna Klein1, Lauren Milechin6, Julie Mullen1, Andrew Prout1,\nAlbert Reuther1, Antonio Rosa1, Siddharth Samsi1, Doug Stetson4, Adam Tse4, Charles Yee1, Peter Michaleas1\n1MIT Lincoln Laboratory Supercomputing Center,2MIT Computer Science & AI Laboratory,\n3MIT Mathematics Department,4MIT Lincoln Laboratory Cyber Operations & Analysis Technology Group,\n5Texas A&M,6MIT Dept. of Earth, Atmospheric, & Planetary Sciences\nAbstract —Our society has never been more dependent on\ncomputer networks. Effective utilization of networks requires\na detailed understanding of the normal background behaviors\nof network trafﬁc. Large-scale measurements of networks are\ncomputationally challenging. Building on prior work in interac-\ntive supercomputing and GraphBLAS hypersparse hierarchical\ntrafﬁc matrices, we have developed an efﬁcient method for\ncomputing a wide variety of streaming network quantities on\ndiverse time scales. Applying these methods to 100,000,000,000\nanonymized source-destination pairs collected at a network gate-\nway reveals many previously unobserved scaling relationships.\nThese observations provide new insights into normal network\nbackground trafﬁc that could be used for anomaly detection, AI\nfeature engineering, and testing theoretical models of streaming\nnetworks.\nIndex Terms —Internet modeling, packet capture, streaming\ngraphs, power-law networks, hypersparse matrices\nI. I NTRODUCTION\nGlobal usage of the Internet is expected to exceed 5 billion\npeople in 2023 [1]. Accordingly, cyberspace is a frontier\nas worthy of scientiﬁc study as land, sea, air, and space\nwere during past eras of exploration [2]–[6]. Deepening our\nscientiﬁc understanding of cyberspace is expected to yield\ncorrespondingly equivalent societal beneﬁts [7]–[9]. More\npragmatically, effective utilization of cyberspace requires a\ndetailed understanding of its principal characteristic: network\ntrafﬁc [10]. Figure 1 illustrates essential quantities found in all\nstreaming dynamic networks. These quantities are derivable\nfrom the source and destinations in packet headers that are\nthe foundational transactional unit of the Internet [11].\nPrecise measurements of these quantities on networks as\nvast as the Internet are computationally challenging [12]–[14].\nPrior work has developed massively parallel data processing\nThis material is based upon work supported by the Assistant Secretary of\nDefense for Research and Engineering under Air Force Contract No. FA8702-\n15-D-0001, National Science Foundation CCF-1533644, and United States\nAir Force Research Laboratory Cooperative Agreement Number FA8750-19-\n2-1000. Any opinions, ﬁndings, conclusions or recommendations expressed\nin this material are those of the author(s) and do not necessarily reﬂect the\nviews of the Assistant Secretary of Defense for Research and Engineering,\nthe National Science Foundation, or the United States Air Force. The U.S.\nGovernment is authorized to reproduce and distribute reprints for Government\npurposes notwithstanding any copyright notation herein.\nvalidsource packetwindowNV=217,218,…,227(time window)validdestination packetwindowNV=217,218,…,227(time window)source packets(packets from a source)\ndestination packets(packets to a destination)uniquedestinationsuniquesourcesuniquelinkssourcefan-out\ndestinationfan-inlinkpacketsFig. 1. Streaming network trafﬁc quantities. Internet trafﬁc streams of\nNVvalid packets are divided into a variety of quantities for analysis: source\npackets, source fan-out, unique source-destination pair packets (or links),\ndestination fan-in, and destination packets.\ncapabilities that can effectively use thousands of processors to\nrapidly process billions of packets in a few hours [15]–[20].\nUsing these capabilities, 50,000,000,000 anonymized packets\nfrom the largest publicly available collections of Internet\npacket data were analyzed, demonstrating the ubiquity of the\nZipf-Mandelbrot distribution and the importance of the size\nof the packet sampling window NV. These data consisted of\nfour 1-hour and two 2-day collections from Internet “trunk”\nlines, providing a high-level view of their portions of the\nInternet spanning years and continents [21]–[26]. These results\nunderscore the need for processing longer time periods from\nadditional vantage points.\nNetwork gateways are a common view into the Internet, are\nwidely monitored, and represent a natural observation point of\nnetwork trafﬁc. Using such a gateway we were able to collect\ntrillions of packet headers over several years. Processing such a\nlarge volume of data requires additional computational innova-\ntions. The incorporation of GraphBLAS hypersparse hierarchi-\ncal trafﬁc matrices has enabled the processing of hundreds of\nbillions of packets in minutes [27]–[30]. This paper presents an\ninitial analysis of one month of anonymized source-destination\npairs derived from approximately 100,000,000,000 packets.\nThe contiguous nature of these data allows the exploration of\na wide range of packet windows from NV= 217(seconds) toarXiv:2008.00307v1  [cs.NI]  1 Aug 2020\n\nNV= 227(hours), providing a unique view into how network\nquantities depend upon time. These observations provide new\ninsights into normal network background trafﬁc that could be\nused for anomaly detection, AI feature engineering, polystore\nindex learning, and testing theoretical models of streaming\nnetworks [31]–[33].\nThe outline of the rest of the paper is as follows. First,\nthe network quantities and their distributions are deﬁned in\nterms of trafﬁc matrices. Second, multi-temporal analysis of\nhypersparse hierarchical trafﬁc matrices is described in the\ncontext of network gateways. Third, the method for determin-\ning scaling relations as a function of the packet window NVis\npresented along with the resulting scaling relations observed in\nthe gateway trafﬁc data. Fourth, an analysis of simple network\ntopologies is performed to illustrate that the observed relations\nfall between well-deﬁned models. Finally, our conclusions and\ndirections for further work are presented.\nII. N ETWORK QUANTITIES AND DISTRIBUTIONS FROM\nTRAFFIC MATRICES\nThe network quantities depicted in Figure 1 are computable\nfrom origin-destination matrices that are widely used to rep-\nresent network trafﬁc [34]–[37]. It is common to ﬁlter the\npackets down to a valid set for any particular analysis. Such\nﬁlters may limit particular sources, destinations, protocols, and\ntime windows. To reduce statistical ﬂuctuations, the streaming\ndata should be partitioned so that for any chosen time window\nall data sets have the same number of valid packets [20]. At\na given time t,NVconsecutive valid packets are aggregated\nfrom the trafﬁc into a sparse matrix At, where At(i;j)is the\nnumber of valid packets between the source iand destination\nj. The sum of all the entries in Atis equal toNV\nX\ni;jAt(i;j) =NV (1)\nAll the network quantities depicted in Figure 1 can be readily\ncomputed from Atusing the formulas listed in Table I.\nEach network quantity will produce a distribution of values\nwhose magnitude is often called the degree d. The correspond-\ning histogram of the network quantity is denoted by nt(d). The\nlargest observed value in the distribution is denoted dmax. The\nnormalization factor of the distribution is given by\nX\ndnt(d) (2)\nwith corresponding probability\npt(d) =nt(d)=X\ndnt(d) (3)\nand cumulative probability\nPt(d) =X\ni=1;dpt(d) (4)\nBecause of the relatively large values of dobserved, the mea-\nsured probability at large doften exhibits large ﬂuctuations.\nHowever, the cumulative probability lacks sufﬁcient detail toTABLE I\nNETWORK QUANTITIES FROM TRAFFIC MATRICES\nFormulas for computing network quantities from trafﬁc matrix Atat time t\nin both summation and matrix notation. 1is a column vector of all 1’s,T\nis the transpose operation, and j j0is the zero-norm that sets each nonzero\nvalue of its argument to 1 [38].\nAggregate Summation Matrix\nProperty Notation Notation\nValid packets NVP\niP\njAt(i; j)1TAt1\nUnique linksP\niP\njjAt(i; j)j01TjAtj01\nLink packets from itoj At(i; j) At\nMax link packets ( dmax) maxijAt(i; j) max( At)\nUnique sourcesP\nijP\njAt(i; j)j01TjAt1j0\nPackets from source iP\njAt(i; j) At1\nMax source packets ( dmax) maxiP\njAt(i; j) max( At1)\nSource fan-out from iP\njjAt(i; j)j0jAtj01\nMax source fan-out ( dmax) maxiP\njjAt(i; j)j0max(jAtj01)\nUnique destinationsP\njjP\niAt(i; j)j0j1TAtj01\nDestination packets to jP\niAt(i; j)1TjAtj0\nMax destination packets ( dmax)maxjP\niAt(i; j) max( 1TjAtj0)\nDestination fan-in to jP\nijAt(i; j)j01TAt\nMax destination fan-in ( dmax) maxjP\nijAt(i; j)j0max( 1TAt)\nsee variations around speciﬁc values of d, so it is typical to\npool the differential cumulative probability with logarithmic\nbins ind\nDt(di) =Pt(di)\u0000Pt(di\u00001) (5)\nwheredi= 2i[39]. All computed probability distributions\nuse the same binary logarithmic binning to allow for con-\nsistent statistical comparison across data sets [39], [40]. The\ncorresponding mean and standard deviation of Dt(di)over\nmany different consecutive values of tfor a given data set\nare denoted D(di)and\u001b(di). Figure 2 provides an example\ndistribution of external !internal source packets using a\npacket window of NV= 217. The means and standard devi-\nations are computed using 1024 consecutive packet windows.\nThe resulting distribution exhibits the power-law frequently\nobserved in network data [41]–[47].\nIII. M ULTI -TEMPORAL ANALYSIS\nNetwork trafﬁc is dynamic and exhibits varying behavior\non a wide range of time scales. A given packet window\nsizeNVwill be sensitive to phenomena on its corresponding\ntimescale. Determining how network quantities scale with NV\nprovides insight into the temporal behavior of network trafﬁc.\nEfﬁcient computation of network quantities on multiple time\nscales can be achieved by hierarchically aggregating data in\ndifferent time windows [20]. Figure 3 illustrates a binary\naggregation of different streaming trafﬁc matrices. Computing\neach quantity at each hierarchy level eliminates redundant\ncomputations that would be performed if each packet window\nwas computed separately. Hierarchy also ensures that most\ncomputations are performed on smaller matrices residing in\nfaster memory. Correlations among the matrices mean that\nadding two matrices each with NVentries results in a matrix\nwith fewer than 2NVentries, reducing the relative number of\noperations as the matrices grow.\n\n100101102103104105source packets10-410-310-210-1100differential cumaltive probabilityexternal  internal (NV = 217)\nsupernodeleaf nodes\ndmaxFig. 2. External !internal source packet degree distribution. Differential\ncumulative probability (normalized histogram) of the number (degree) of\nsource packets from each source using logarithmic bins di= 2i. Circles\nrepresent the averages of 1024 packet windows each with NV= 217. Error\nbars represent one standard deviation. Sources sending out a single packet are\ndenoted “leaf nodes”. The source with the largest number of packets dmax\nis referred to as the “supernode”.\nstream of trafficmatricesNV=217sourcessparsetrafficmatrixAt:t+TNV=218sourcessparsetrafficmatrixAt:t+2T++++\nNV=219sourcessparsetrafficmatrixAt:t+4T++sourcesdestinations\nsourcesdestinations\nsourcesdestinations\nFig. 3. Multi-temporal streaming trafﬁc matrices. Efﬁcient computation of\nnetwork quantities on multiple time scales can be achieved by hierarchically\naggregating data in different time windows.\nOne of the important capabilities of the SuiteSparse Graph-\nBLAS library is support for hypersparse matrices where the\nnumber of nonzero entries is signiﬁcantly less than either\ndimensions of the matrix. If the packet source and destination\nidentiﬁers are drawn from a large numeric range, such as those\nused in the Internet protocol, then a hypersparse representation\nofAteliminates the need to keep track of additional indices\nand can signiﬁcantly accelerate the computations [30].\nNetwork gateways are a common view into the Internet,\nare widely monitored, and represent a natural observation\npoint on network trafﬁc. Using such a gateway we were able\nto collect trillions of packets over several years. The trafﬁc\nmatrix of a gateway can be partitioned into four quadrants (see\nFigure 4). These quadrants represent different ﬂows between\nnodes internal and external to the gateway. A gateway will\ntypically see the external !internal and internal !external\ntrafﬁc.\nIV. R ESULTS\nSeveral trillion packet headers have been collected at a\ngateway over several years. This work focuses on analyzing\none month of anonymized source-destination pairs derived\nfrom approximately 100,000,000,000 packet headers. The\nsourcesdestinationssourcesdestinationssparse traffic matrix A\ninternalàexternalinternalàinternalexternalàexternalexternalàinternal\ninternal                       externalinternal                       externalFig. 4. Gateway network trafﬁc matrix. The trafﬁc matrix of a gateway can\nbe divided into quadrants separating internal and external trafﬁc. The gateway\nwill observe the external !internal trafﬁc (upper left) and the internal !\nexternal trafﬁc (lower right).\n0 200 400 600 800 1000 1200\ntime (packet window) [duration ~ 1 month]10-310-2unique source fractionexternal  internal\nNV = 217\nNV = 219\nNV = 221\nNV = 223\nNV = 225\nNV = 227\n0 200 400 600 800 1000 1200\ntime (packet window) [duration ~ 1 month]0.010.0150.020.0250.030.0350.04(unique source fraction)*(NV/217)2/5external  internal\nNV = 217\nNV = 219\nNV = 221\nNV = 223\nNV = 225\nNV = 227\nFig. 5. (top) Unique external !internal source fraction. Average total\nnumber of unique sources in a packet window of width NVmeasured at\neach time over a month. (bottom) Normalized external !internal unique\nsource fraction. Scaling (top) data by (NV=217)2=5aligns the different\npacket windows, indicating that the number of uniques sources is proportional\ntoN1\u00002=5\nV=N3=5\nV(see Table II).\n\nTABLE II\nAPPROXIMATE SCALING RELATIONS .\nAnalysis of network quantities over packet windows NV= 217; : : : ; 227reveals a strong dependence of many of these quantities on NV. Blank entries\nindicate that no simple scaling relation was observed.\nInternal àExternal(Average)Internal àExternal(Standard Deviation)External àInternal(Average)External àInternal(Standard Deviation)Unique links2 x NV1/214 x NV1/411 x NV1/2Max link packets (dmax)0.03 x NV10.03 x NV10.1 x NV10.1 x NV1Link packets norm (Sdn(d))2 x NV1/210 x NV1/40.01 x NV5/611 x NV1/2Unique sources5 x NV1/58 x NV012 x NV3/50.2 x NV1/3Max source packets (dmax)0.15 x NV10.03 x NV10.1 x NV10.1 x NV1Source packets norm (Sdn(d))6 x NV1/58 x NV01 x NV3/58 x NV1/3Max source fan-out (dmax)3 x NV3/712 x NV1/60.03 x NV5/611 x NV1/2Source fan-out norm (Sdn(d))5 x NV1/59 x NV01 x NV3/510 x NV1/3Unique destinations2 x NV1/27 x NV1/4Max destination packets (dmax)0.04 x NV10.03 x NV10.3 x NV12 x NV3/4Destination packets norm (Sdn(d))2 x NV1/212 x NV1/53 x NV3/535 x NV2/5Max destination fan-in (dmax)0.03 x NV2/50.05 x NV1/34 x NV3/79 x NV1/5Destination fan-in norm (Sdn(d))2 x NV1/212 x NV1/53 x NV3/535 x NV2/5\ncontiguous nature of these data allows the exploration of a\nwide range of packet windows from NV= 217(seconds)\ntoNV= 227(hours), providing a unique view into how\nnetwork quantities depend upon time. Figure 5 (top) shows\nthe average total number of unique sources in a packet\nwindow of width NVmeasured at each time over a month\nnormalized by NV. Figure 5 (bottom) is the result of scaling\nthe data by (NV=217)2=5to align the different packet windows,\nindicating that the number of uniques sources is proportional to\nN1\u00002=5\nV =N3=5\nV. Performing a similar analysis across many\nnetwork quantities produced the scaling relations shown in\nTable II. These results reveal a strong dependence on these\nquantities as function of the packet window size NV. To our\nknowledge, these scaling relations have not been previously\nreported and represent a new view on the background behavior\nof network trafﬁc.\nV. S IMPLE TOPOLOGY ANALYSIS\nPower-law network data are a result of complex network\ntopologies that are an ongoing area of investigation [48], [49].\nBounds on the underlying network topologies can be derived\nby analyzing several simple topologies. Figure 6 shows a\nschematic of networks and gateway trafﬁc matrices for four\nsimple topologies whose NVscaling behavior can be readily\ncomputed. Isolated links are source-destination pairs that have\nonly one packet. Single link implies all trafﬁc ﬂows between\none internal and one external node. An internal supernode has\nall trafﬁc occurring between a single internal node and many\nexternal nodes. An external supernode has all trafﬁc occurring\nbetween a single external node and many internal nodes.\nThe resulting scaling relationships are shown in Table III. In\nall cases the simple trafﬁc topologies produce relationships\nthat scale as N0\nV(constant) or N1\nV(linear). The observed\nrelationships in Table II are between these two extremes\nconsistent with observed power-law topologies. In particular,the observed maximum link, source, and destination packets\nare consistent with the single link model, suggesting that a\nfraction of the trafﬁc is on a single link perhaps combining an\ninternal supernode and an external supernode.\nVI. C ONCLUSIONS AND FUTURE WORK\nWe have developed an efﬁcient method for computing\na wide variety of streaming network quantities on diverse\ntime scales. Applying these methods to 100,000,000,000\nanonymized source-destination pairs collected at a network\ngateway reveals many previously unobserved scaling relation-\nships as a function of the packet window NV. These observa-\ntions provide new insights into normal network background\ntrafﬁc. While the details of these scaling relationships are\nlikely to be speciﬁc to the gateway, the number and vari-\nety of relationships suggests that scaling relationships could\nbe observed at other network vantage points. The observed\nrelationships provide simple, low-dimensional models of the\nnetwork trafﬁc that could be used for anomaly detection.\nThese relationships can also assist with feature engineering\nfor the development of AI based trafﬁc categorization. Deeper\ntheoretical models can also be constrained and tested, and\nthe observed relationships can provide a useful target for\ntheoreticians. Future work will expand this analysis to the full\nseveral trillion packet data set and other data sets collected\nat different vantage points, apply these results to anomaly\ndetection problems, and test theoretical models of dynamic\nstreaming networks.\nACKNOWLEDGMENTS\nThe authors wish to acknowledge the following individuals\nfor their contributions and support: Bob Bond, K Claffy, Cary\nConrad, David Clark, Alan Edelman, Jeff Gottschalk, Tim\nKraska, Charles Leiserson, Dave Martinez, Mimi McClure,\nSteve Rejto, Marc Zissman.\n\nexternal supernodeisolated linkssingle linkinternal supernodeexternalinternalsourcesdestinationssourcesdestinationsinternal       externalinternal external\nsourcesdestinationssourcesdestinationsinternal       externalinternal external\nsourcesdestinationssourcesdestinationsinternal       externalinternal external\nsourcesdestinationssourcesdestinationsinternal       externalinternal externalsparse traffic matrix AFig. 6. Simple network trafﬁc topologies and trafﬁc matrices. Isolated links are source-destination pairs that have only one packet. Single link implies all\ntrafﬁc ﬂows between one internal and one external node. Internal supernode has all trafﬁc occurring between a single internal node and many external nodes.\nExternal supernode has all trafﬁc occurring between a single external node and many internal nodes.\nTABLE III\nSIMPLE TOPOLOGY SCALING RELATIONS .\nFor balanced trafﬁc with equal numbers of packets ﬂowing in each direction, the simple trafﬁc topologies produce relationships that scale as N0\nV(constant)\norN1\nV(linear).\nIsolatedLinksInternal àExternalIsolatedLinksExternal àInternalSingleLinkInternal àExternalSingleLinkExternal àInternalInternal SupernodeInternal àExternalInternal SupernodeExternal àInternalInternal SupernodeInternal àExternalInternal SupernodeExternal àInternalUnique links0.5 x NV10.5 x NV11.0 x NV01.0 x NV00.5 x NV10.5 x NV10.5 x NV10.5 x NV1Max link packets (dmax)1.0 x NV01.0 x NV00.5 x NV10.5 x NV11.0 x NV01.0 x NV01.0 x NV01.0 x NV0Link packets norm (Sdn(d))0.5 x NV10.5 x NV11.0 x NV01.0 x NV00.5 x NV10.5 x NV10.5 x NV10.5 x NV1Unique sources0.5 x NV10.5 x NV11.0 x NV01.0 x NV01.0 x NV00.5 x NV10.5 x NV11.0 x NV0Max source packets (dmax)1.0 x NV01.0 x NV00.5 x NV10.5 x NV10.5 x NV11.0 x NV01.0 x NV00.5 x NV1Source packets norm (Sdn(d))0.5 x NV10.5 x NV11.0 x NV01.0 x NV01.0 x NV00.5 x NV10.5 x NV11.0 x NV0Max source fan-out (dmax)1.0 x NV01.0 x NV01.0 x NV01.0 x NV00.5 x NV11.0 x NV01.0 x NV00.5 x NV1Source fan-out norm (Sdn(d))0.5 x NV10.5 x NV11.0 x NV01.0 x NV01.0 x NV00.5 x NV10.5 x NV11.0 x NV0Unique destinations0.5 x NV10.5 x NV11.0 x NV01.0 x NV00.5 x NV11.0 x NV01.0 x NV00.5 x NV1Max destination packets (dmax)1.0 x NV01.0 x NV00.5 x NV10.5 x NV11.0 x NV00.5 x NV10.5 x NV11.0 x NV0Destination packets norm (Sdn(d))0.5 x NV10.5 x NV11.0 x NV01.0 x NV00.5 x NV11.0 x NV01.0 x NV00.5 x NV1Max destination fan-in (dmax)1.0 x NV01.0 x NV01.0 x NV01.0 x NV01.0 x NV00.5 x NV10.5 x NV11.0 x NV0Destination fan-in norm (Sdn(d))0.5 x NV10.5 x NV11.0 x NV01.0 x NV00.5 x NV11.0 x NV01.0 x NV00.5 x NV1\nREFERENCES\n[1] “ Cisco Visual Networking Index: Forecast and Trends, 20182023 .”\nhttps://www.cisco.com/c/en/us/solutions/collateral/executive-\nperspectives/annual-internet-report/white-paper-c11-741490.html.\n[2] V . Bush, “Science The Endless Frontier.” Report to the President of the\nUnited States, 1945.\n[3] K. Claffy, “Internet tomography,” Nature, Web Matter , 1999.\n[4] R. Prasad, C. Dovrolis, M. Murray, and K. Claffy, “Bandwidth esti-\nmation: metrics, measurement techniques, and tools,” IEEE network ,\nvol. 17, no. 6, pp. 27–35, 2003.\n[5] H. Kim, K. C. Claffy, M. Fomenkov, D. Barman, M. Faloutsos, and\nK. Lee, “Internet trafﬁc classiﬁcation demystiﬁed: myths, caveats, and\nthe best practices,” in Proceedings of the 2008 ACM CoNEXT confer-\nence, p. 11, ACM, 2008.\n[6] M. Boguna, D. Krioukov, and K. C. Claffy, “Navigability of complex\nnetworks,” Nature Physics , vol. 5, no. 1, p. 74, 2009.\n[7] K. Claffy, “Measuring the internet,” IEEE Internet Computing , vol. 4,\nno. 1, pp. 73–75, 2000.[8] B. Li, J. Springer, G. Bebis, and M. H. Gunes, “A survey of network ﬂow\napplications,” Journal of Network and Computer Applications , vol. 36,\nno. 2, pp. 567–581, 2013.\n[9] M. Rabinovich and M. Allman, “Measuring the internet,” IEEE Internet\nComputing , vol. 20, no. 4, pp. 6–8, 2016.\n[10] k. claffy and D. Clark, “Workshop on internet economics (wie 2019)\nreport,” SIGCOMM Comput. Commun. Rev. , vol. 50, p. 5359, May 2020.\n[11] D. Huang, A. Chowdhary, and S. Pisharody, Software-Deﬁned network-\ning and security: from theory to practice . CRC Press, 2018.\n[12] A. Lumsdaine, D. Gregor, B. Hendrickson, and J. Berry, “Challenges in\nparallel graph processing,” Parallel Processing Letters , vol. 17, no. 01,\npp. 5–20, 2007.\n[13] T. G. Kolda and B. W. Bader, “Tensor decompositions and applications,”\nSIAM review , vol. 51, no. 3, pp. 455–500, 2009.\n[14] M. Hilbert and P. L ´opez, “The world’s technological capacity to store,\ncommunicate, and compute information,” Science , p. 1200970, 2011.\n[15] J. Kepner, Parallel MATLAB for Multicore and Multinode Computers .\nSIAM, 2009.\n[16] J. Kepner and J. Gilbert, Graph algorithms in the language of linear\nalgebra . SIAM, 2011.\n\n[17] J. Kepner and H. Jananthan, Mathematics of big data: Spreadsheets,\ndatabases, matrices, and graphs . MIT Press, 2018.\n[18] A. Reuther, J. Kepner, C. Byun, S. Samsi, W. Arcand, D. Bestor,\nB. Bergeron, V . Gadepally, M. Houle, M. Hubbell, M. Jones, A. Klein,\nL. Milechin, J. Mullen, A. Prout, A. Rosa, C. Yee, and P. Michaleas,\n“Interactive supercomputing on 40,000 cores for machine learning and\ndata analysis,” in 2018 IEEE High Performance extreme Computing\nConference (HPEC) , pp. 1–6, 2018.\n[19] V . Gadepally, J. Kepner, L. Milechin, W. Arcand, D. Bestor, B. Bergeron,\nC. Byun, M. Hubbell, M. Houle, M. Jones, P. Michaleas, J. Mullen,\nA. Prout, A. Rosa, C. Yee, S. Samsi, and A. Reuther, “Hyperscaling\ninternet graph analysis with d4m on the mit supercloud,” in 2018 IEEE\nHigh Performance extreme Computing Conference (HPEC) , pp. 1–6,\nSep. 2018.\n[20] J. Kepner, V . Gadepally, L. Milechin, S. Samsi, W. Arcand, D. Bestor,\nW. Bergeron, C. Byun, M. Hubbell, M. Houle, M. Jones, A. Klein,\nP. Michaleas, J. Mullen, A. Prout, A. Rosa, C. Yee, and A. Reuther,\n“Streaming 1.9 billion hypersparse network updates per second with\nd4m,” in 2019 IEEE High Performance Extreme Computing Conference\n(HPEC) , pp. 1–6, 2019.\n[21] K. Cho, K. Mitsuya, and A. Kato, “Trafﬁc data repository at the wide\nproject,” in Proceedings of USENIX 2000 Annual Technical Conference:\nFREENIX Track , pp. 263–270, 2000.\n[22] P. Borgnat, G. Dewaele, K. Fukuda, P. Abry, and K. Cho, “Seven years\nand one day: Sketching the evolution of internet trafﬁc,” in INFOCOM\n2009, IEEE , pp. 711–719, IEEE, 2009.\n[23] A. Dainotti, A. Pescape, and K. C. Claffy, “Issues and future directions\nin trafﬁc classiﬁcation,” IEEE network , vol. 26, no. 1, 2012.\n[24] Y . Himura, K. Fukuda, K. Cho, P. Borgnat, P. Abry, and H. Esaki,\n“Synoptic graphlet: Bridging the gap between supervised and unsuper-\nvised proﬁling of host-level network trafﬁc,” IEEE/ACM Transactions\non Networking , vol. 21, no. 4, pp. 1284–1297, 2013.\n[25] R. Fontugne, P. Abry, K. Fukuda, D. Veitch, K. Cho, P. Borgnat, and\nH. Wendt, “Scaling in internet trafﬁc: a 14 year and 3 day longitudinal\nstudy, with multiscale analyses and random projections,” IEEE/ACM\nTransactions on Networking (TON) , vol. 25, no. 4, pp. 2152–2165, 2017.\n[26] A. Dhamdhere, D. D. Clark, A. Gamero-Garrido, M. Luckie, R. K.\nMok, G. Akiwate, K. Gogia, V . Bajpai, A. C. Snoeren, and K. Claffy,\n“Inferring persistent interdomain congestion,” in Proceedings of the 2018\nConference of the ACM Special Interest Group on Data Communication ,\npp. 1–15, ACM, 2018.\n[27] J. Kepner, P. Aaltonen, D. Bader, A. Bulu, F. Franchetti, J. Gilbert,\nD. Hutchison, M. Kumar, A. Lumsdaine, H. Meyerhenke, S. McMillan,\nC. Yang, J. D. Owens, M. Zalewski, T. Mattson, and J. Moreira, “Math-\nematical foundations of the graphblas,” in 2016 IEEE High Performance\nExtreme Computing Conference (HPEC) , pp. 1–9, 2016.\n[28] A. Buluc, T. Mattson, S. McMillan, J. Moreira, and C. Yang, “Design\nof the graphblas api for c,” in 2017 IEEE International Parallel and\nDistributed Processing Symposium Workshops (IPDPSW) , pp. 643–652,\n2017.\n[29] T. A. Davis, “Algorithm 1000: Suitesparse:graphblas: Graph algorithms\nin the language of sparse linear algebra,” ACM Trans. Math. Softw. ,\nvol. 45, Dec. 2019.\n[30] J. Kepner, T. Davis, C. Byun, W. Arcand, D. Bestor, W. Bergeron,\nV . Gadepally, M. Hubbell, M. Houle, M. Jones, A. Klein, P. Michaleas,\nL. Milechin, J. Mullen, A. Prout, A. Rosa, S. Samsi, C. Yee, and\nA. Reuther, “75,000,000,000 streaming inserts/second using hierarchical\nhypersparse graphblas matrices,” IPDPSW GrAPL , 2020.\n[31] A. J. Elmore, J. Duggan, M. Stonebraker, M. Balazinska, U. Cetintemel,\nV . Gadepally, J. Heer, B. Howe, J. Kepner, T. Kraska, et al. , “A\ndemonstration of the bigdawg polystore system,” Proceedings of the\nVLDB Endowment , vol. 8, no. 12, p. 1908, 2015.\n[32] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The case\nfor learned index structures,” in Proceedings of the 2018 International\nConference on Management of Data , SIGMOD 18, (New York, NY ,\nUSA), p. 489504, Association for Computing Machinery, 2018.\n[33] E. H. Do and V . N. Gadepally, “Classifying anomalies for network\nsecurity,” in ICASSP 2020 - 2020 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) , pp. 2907–2911,\n2020.\n[34] A. Soule, A. Nucci, R. Cruz, E. Leonardi, and N. Taft, “How to\nidentify and estimate the largest trafﬁc matrix elements in a dynamic\nenvironment,” in ACM SIGMETRICS Performance Evaluation Review ,\nvol. 32, pp. 73–84, ACM, 2004.[35] Y . Zhang, M. Roughan, C. Lund, and D. L. Donoho, “Estimating point-\nto-point and point-to-multipoint trafﬁc matrices: an information-theoretic\napproach,” IEEE/ACM Transactions on Networking (TON) , vol. 13,\nno. 5, pp. 947–960, 2005.\n[36] P. J. Mucha, T. Richardson, K. Macon, M. A. Porter, and J.-P. Onnela,\n“Community structure in time-dependent, multiscale, and multiplex\nnetworks,” science , vol. 328, no. 5980, pp. 876–878, 2010.\n[37] P. Tune, M. Roughan, H. Haddadi, and O. Bonaventure, “Internet trafﬁc\nmatrices: A primer,” Recent Advances in Networking , vol. 1, pp. 1–56,\n2013.\n[38] J. Karvanen and A. Cichocki, “Measuring sparseness of noisy signals,”\nin4th International Symposium on Independent Component Analysis\nand Blind Signal Separation , pp. 125–130, 2003.\n[39] A. Clauset, C. R. Shalizi, and M. E. Newman, “Power-law distributions\nin empirical data,” SIAM review , vol. 51, no. 4, pp. 661–703, 2009.\n[40] A.-L. Barab ´asiet al. ,Network science . Cambridge university press,\n2016.\n[41] W. E. Leland, M. S. Taqqu, W. Willinger, and D. V . Wilson, “On the\nself-similar nature of ethernet trafﬁc (extended version),” IEEE/ACM\nTransactions on Networking (ToN) , vol. 2, no. 1, pp. 1–15, 1994.\n[42] M. Faloutsos, P. Faloutsos, and C. Faloutsos, “On power-law relation-\nships of the internet topology,” in ACM SIGCOMM computer commu-\nnication review , vol. 29-4, pp. 251–262, ACM, 1999.\n[43] R. Albert, H. Jeong, and A.-L. Barab ´asi, “Internet: Diameter of the\nworld-wide web,” Nature , vol. 401, no. 6749, p. 130, 1999.\n[44] A.-L. Barab ´asi and R. Albert, “Emergence of scaling in random net-\nworks,” Science , vol. 286, no. 5439, pp. 509–512, 1999.\n[45] L. A. Adamic and B. A. Huberman, “Power-law distribution of the world\nwide web,” science , vol. 287, no. 5461, pp. 2115–2115, 2000.\n[46] A.-L. Barab ´asi, “Scale-free networks: a decade and beyond,” science ,\nvol. 325, no. 5939, pp. 412–413, 2009.\n[47] A. Mahanti, N. Carlsson, A. Mahanti, M. Arlitt, and C. Williamson, “A\ntale of the tails: Power-laws in internet measurements,” IEEE Network ,\nvol. 27, no. 1, pp. 59–64, 2013.\n[48] Z. Cao, Z. He, and N. F. Johnson, “Impact on the topology of power-law\nnetworks from anisotropic and localized access to information,” Phys.\nRev. E , vol. 98, p. 042307, Oct 2018.\n[49] A. D. Broido and A. Clauset, “Scale-free networks are rare,” Nature\ncommunications , vol. 10, no. 1, pp. 1–10, 2019.",
  "textLength": 30807
}