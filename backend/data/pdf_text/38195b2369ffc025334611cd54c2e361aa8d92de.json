{
  "paperId": "38195b2369ffc025334611cd54c2e361aa8d92de",
  "title": "Improved Cardinality Estimation by Learning Queries Containment Rates",
  "pdfPath": "38195b2369ffc025334611cd54c2e361aa8d92de.pdf",
  "text": "Improved Cardinality Estimation by Learning Queries\nContainment Rates\nRojeh Hayek\nCS Department, Technion\nHaifa 3200003, Israel\nsrojeh@cs.technion.ac.ilOded Shmueli\nCS Department, Technion\nHaifa 3200003, Israel\noshmu@cs.technion.ac.il\nABSTRACT\nThe containment rate of query Q1 in query Q2 over database\nDis the percentage of Q1's result tuples over Dthat are also\ninQ2's result over D. We directly estimate containment\nrates between pairs of queries over a speci\fc database. For\nthis, we use a specialized deep learning scheme, CRN, which\nis tailored to representing pairs of SQL queries. Result-\ncardinality estimation is a core component of query op-\ntimization. We describe a novel approach for estimating\nqueries result-cardinalities using estimated containment rates\namong queries. This containment rate estimation may rely\non CRN or embed, unchanged, known cardinality estimation\nmethods. Experimentally, our novel approach for estimating\ncardinalities, using containment rates between queries, on a\nchallenging real-world database, realizes signi\fcant improve-\nments to state of the art cardinality estimation methods.\nPVLDB Reference Format:\nRojeh Hayek, Oded Shmuile. Improved Cardinality Estimation\nby Learning Queries Containment Rates. PVLDB , 12(xxx): xxxx-\nyyyy, 2020.\nDOI: https://doi.org/10.14778/xxxxxxx.xxxxxxx\n1. INTRODUCTION\nQuery Q1 is contained in (resp. equivalent to), query Q2,\nanalytically, if for all the database states D,Q1's result over\nDis contained in (resp., equals) Q2's result over D. Query\ncontainment is a well-known concept that has applications\nin query optimization. It has been extensively researched\nin database theory, and many algorithms were proposed for\ndetermining containment under di\u000berent assumptions [44,\n11, 12, 19]. However, determining query containment an-\nalytically is not practically su\u000ecient. Two queries may be\nanalytically unrelated by containment, although, the execu-\ntion result on a speci\fc database of one query may actually\nbe contained in the other. For example, consider the queries:\nQ1: select * from movies where title = 'Titanic'\nQ2: select * from movies where release = 1997 and director\n= 'James Cameron'\nThis work is licensed under the Creative Commons Attribution-\nNonCommercial-NoDerivatives 4.0 International License. To view a copy\nof this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. For\nany use beyond those covered by this license, obtain permission by emailing\ninfo@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 12, No. xxx\nISSN 2150-8097.\nDOI: https://doi.org/10.14778/xxxxxxx.xxxxxxxBoth queries execution results are identical since there is\nonly one movie called Titanic that was released in 1997 and\ndirected by James Cameron (he has not directed any other\nmovie in 1997). Yet, using the analytic criterion, the queries\nare unrelated at all by containment.\nTo our knowledge, while query containment and equiva-\nlence have been well researched in past decades, determin-\ning the containment rate between two queries on a speci\fc\ndatabase, has not been considered by past research.\nBy de\fnition, the containment rate of query Q1 in query\nQ2 on database Dis the percentage of rows in Q1's execution\nresult over Dthat are also in Q2's execution result over D.\nDetermining containment rates allows us to solve other prob-\nlems, such as determining equivalence between two queries,\nor whether one query is fully contained in another, on the\nsame speci\fc database. In addition, containment rates can\nbe used in many practical applications, for instance, query\nclustering, query recommendation [14, 18], and in cardinal-\nity estimation as will be described subsequently.\nOur approach for estimating containment rates is based\non a specialized deep learning model, CRN, which enables us\nto express query features using sets and vectors. An input\nquery is converted into three sets, T,JandPrepresenting\nthe query's tables, joins and column predicates, respectively.\nEach element of these sets is represented by a vector. Using\nthese vectors, CRN generates a single vector that represents\nthe whole input query. Finally, to estimate the containment\nrate of two represented queries, CRN measures the distance\nbetween the representative vectors of both queries, using\nanother specialized neural network. Thus, the CRN model\nrelies on the ability of the neural network to learn the vector\nrepresentation of queries relative to the speci\fc database. As\na result, we obtain a small and accurate model for estimating\ncontainment rates.\nIn addition to the CRN model, we introduce a novel tech-\nnique for estimating queries' cardinalities using estimated\nquery containment rates. We show that using the proposed\ntechnique we improve current cardinality estimation tech-\nniques signi\fcantly. This is especially the case when there\nare multiple joins, where the known cardinality estimation\ntechniques su\u000ber from under-estimated results and errors\nthat grow exponentially as the number of joins increases\n[17]. Our technique estimates the cardinalities more ro-\nbustly (x150/x175 with 4 joins queries, and x1650/x120 with\n5 joins queries, compared with PostgreSQL and MSCN, re-\nspectively). We compare our technique with PostgreSQL\n[1], and the pioneering MSCN model [24], by examining,\non the real-world IMDb database [29], join crossing correla-\n1arXiv:1908.07723v1  [cs.DB]  21 Aug 2019\n\ntions queries which are known to present a tough challenge\nto cardinality estimation methods [29, 31, 38].\nWe show that by employing known existing cardinality\nestimation methods for containment estimation, we can im-\nprove on their cardinality estimates as well, without chang-\ning the methods themselves. Thus, our novel approach is\nhighly promising for solving the cardinality estimation prob-\nlem, the \"Achilles heel\" of query optimization [34], a cause\nof many performance issues [29].\nThe rest of this paper is organized as follows. In Section\n2 we de\fne the containment rate problem and in Sections\n3-4 we describe and evaluate the CRN model for solving\nthis problem. In Sections 5-6 we describe and evaluate our\nnew approach for estimating cardinalities using containment\nrates. In Section 7 we show how one can adapt the new ideas\nto improve existing cardinality estimation models. Sections\n8-9 present related work, conclusions and future work.\n2. CONTAINMENT RATE DEFINITION\nWe de\fne the containment rate between two queries Q1,\nandQ2 on a speci\fc database D.Query Q1isx%-contained\nin query Q2on database Dif precisely x%ofQ1's execution\nresult rows on database Dare also in Q2's execution result\non database D.The containment rate is formally a function\nfromQxQxD toR, where Qis the set of all queries, Dof\nall databases, and Rthe Real numbers. This function can\nbe directly calculated using the cardinality of the results of\nqueries Q1 and Q2 as follows:\nx% =jQ1(D)\\Q2(D)j\njQ1(D)j\u0003100\nWhere, Q(D) denotes Q's execution result on database D.\n(in case Q1's execution result is empty, then Q1 is 0%-\ncontained in Q2). Note that the containment rate is de\fned\nonly on pairs of queries whose SELECT and FROM clauses\nareidentical .\n2.1 Containment Rate Operator\nWe denote the containment rate operator between queries\nQ1 and Q2 on database Das:\nQ1\u001aD\n%Q2\nOperator\u001aD\n%returns the containment rate between the given\ninput queries on database D. That is, Q1\u001aD\n%Q2 returns\nx%, if Q1 isx%-contained in query Q2 on database D. For\nsimplicity, we do not mention the speci\fc database, as it is\nusually clear from context. Therefore, we write the contain-\nment rate operator as \u001a%.\n3. LEARNED CONTAINMENT RATES\nFrom a high-level perspective, applying machine learn-\ning to the containment rate estimation problem is straight-\nforward. Following the training of the CRN model with\npairs of queries ( Q1; Q2) and the actual containment rates\nQ1\u001a%Q2, the model is used as an estimator for other,\nunseen pairs of queries. There are, however, several ques-\ntions whose answers determine whether the machine learn-\ning model (CRN) will be successful. (1) Which supervised\nlearning algorithm/model should be used. (2) How to rep-\nresent queries as input and the containment rates as output\nto the model (\"featurization\"). (3) How to obtain the initial\ntraining dataset (\"cold start problem\"). Next, we describe\nhow we address each one of these questions.3.1 Cold Start Problem\n3.1.1 DeÔ¨Åning the Database\nWe generated a training-set, and later on evaluated our\nmodel on it, using the IMDb database. IMDb contains\nmany correlations and has been shown to be very challeng-\ning for cardinality estimators [29]. This database contains a\nplethora of information about movies and related facts about\nactors, directors, and production companies, with more than\n2.5M movie titles produced over 130 years (starting from\n1880) by 235,000 di\u000berent companies with over 4M actors.\n3.1.2 Generating the Development Dataset\nOur approach for solving the \"cold start problem\" is to ob-\ntain an initial training corpus using a specialized queries gen-\nerator that randomly generates queries based on the IMDB\nschema and the actual columns values. Our queries gener-\nator generates the dataset in three main steps. In the \frst\nstep, it repeatedly generates multiple SQL queries as follows.\nIt randomly chooses a set of tables t, that can join with each\nother in the database. Then, it adds the corresponding join\nedges to the query. For each base table btin the chosen set\nof tables t, it uniformly draws the number of query pred-\nicates pbt(0\u0014pbt\u0014number of non-key columns in table\nbt). Subsequently, for each predicate it uniformly draws a\nnon-key column from the relevant table bt, a predicate type\n(<;=; or > ), and a value from the corresponding column\nvalues range in the database. To avoid a combinatorial ex-\nplosion, and to simplify the problem that the model needs\nto learn, we force the queries generator to create queries\nwith up to two joins and let the model generalize to a larger\nnumber of joins. Note that all the generated queries include\na SELECT * clause. They are denoted as initial-queries .\nTo create pairs of queries that are contained in each other\nwith di\u000berent containment rates, we generate, in the sec-\nond step, queries that are \"similar\" to the initial-queries ,\nbut still, di\u000berent from them, as follows. For each query\nQininitial-queries , the generator repeatedly creates mul-\ntiple queries by randomly changing query Q's predicates'\ntypes, or the predicates' values, and by randomly adding\nadditional predicates to the original query Q. This way, we\ncreate a \"hard\" dataset, which includes pairs of queries that\nlook \"similar\", but having mutual containment rates that\nvary signi\fcantly. Finally, in the third and last step, using\nthe queries obtained from both previous steps, the queries\ngenerator generates pairs of queries whose FROM clauses\nare identical. (Note that our queries generator's \frst step\nis similar to MSCN's generator [24], however, in the second\nstep we create more complicated queries).\nAfter generating the dataset, we execute the dataset queries\non the IMDb database, to obtain their true containment\nrates. Using this process, we obtain an initial training set\nfor our model, which consists of 100,000 pairs of queries with\nzero to two joins. We split the training samples into 80%\ntraining samples and 20% validation samples.\n3.2 Model\nFeaturizing all the queries' literals and predicates as one\n\"big hot vector\", over all the possible words that may appear\nin the queries, is impractical. Also, serializing the queries'\nSELECT, FROM, and WHERE clauses elements into an\nordered sequence of elements, is not practical, since the order\nin these clauses is arbitrary.\n2\n\nThus, standard deep neural network architectures such\nas simple multi-layer perceptrons [10], convolutional neural\nnetworks [10], or recurrent neural networks [10], are not di-\nrectly proper to our problem.\nOur Containment Rate Network (CRN) model uses a spe-\ncialized vector representation for representing the input queries\nand the output containment rates. As depicted in Figure 1,\nthe CRN model runs in three main stages. Consider an\ninput queries pair ( Q1; Q2). In the \frst stage, we convert\nQ1 (resp., Q2) into a set of vectors V1 (resp., V2). Thus\n(Q1; Q2) is represented by ( V1; V2). In the second stage,\nwe convert set V1 (resp., V2) into a unique single repre-\nsentative vector Qvec 1 (resp., Qvec 2), using a specialized\nneural network, MLP i, for each set separately. In the third\nstage, we estimate the containment rate Q1\u001a%Q2, using\nthe representative vectors Qvec 1 and Qvec 2, and another\nspecialized neural network, MLP out.\nFigure 1: CRN Model Archeticture.\n3.2.1 First Stage, from (Q1; Q2)to(V1; V2)\nIn the same way as MSCN model [24], we represent each\nquery Qas a collection of three sets ( T; J; P ).Tis the set\nof all the tables in Q's FROM clause. Jis the set of all the\njoins (i.e., join clauses) in Q's WHERE clause. Pis the set\nof all the (column) predicates in Q's WHERE clause. Using\nsetsT,J, and P, we obtain a set of vectors Vrepresenting\nthe query, as described later. Unlike MSCN, in our model\nall the vectors of set Vhave the same dimension and the\nsame segmentation as depicted in Table 1, where # Tis the\nnumber of all the tables in the database, # Cis the number\nof all the columns in all the database tables, and # Ois the\nnumber of possible predicates operators. In total, the vector\ndimension is # T+ 3\u0003#C+ #O+ 1, denoted as L.\nThe queries tables, joins and column predicates (sets T,J\nandP) are inseparable, hence, treating each set individually\nusing di\u000berent neural networks may disorientate the model.\nTherefore, we choose to featurize these sets using the same\nvector format in order to ease learning.\nType Table Join Column Predicate\nSegment T-seg J1-seg J2-seg C-seg O-seg V-seg\nSegment size #T #C #C #C #O 1\nFeaturization one hot one hot one hot one hot one hot norm:\nTable 1: Vector Segmentation.\nElement of sets T,J, and P, are represented by vectors as\nfollows (see a simple example in Figure 2). All the vectors\nhave the same dimension L. Each table t2Tis represented\nby a unique one-hot vector (a binary vector of length # T\nwith a single non-zero entry, uniquely identifying a speci\fc\ntable) placed in the T-seg segment. Each join clause of theform ( col1;=; col2)2Jis represented as follows. col1 and\ncol2 are represented by a unique one-hot vectors placed in\nJ1-seg and J2-seg segments, respectively. Each predicate of\nthe form ( col; op; val )2Pis represented as follows. coland\nopare represented by a unique one-hot vectors placed in the\nC-seg and V-seg segments, respectively. valis represented as\na normalized value 2[0;1], normalized using the minimum\nand maximum values of the respective column, placed in the\nV-seg segment. For each vector, all the other unmentioned\nsegments are zeroed. Given input queries pair, ( Q1; Q2), we\nconvert query Q1 (resp., Q2) into sets T,JandP, and each\nelement of these sets is represented by a vector as described\nabove, together generating set V1 (resp., V2).\n3.2.2 Second Stage, from (V1; V2)to(Qvec 1; Qvec 2)\nGiven set of vectors Vi, we present each vector of the set\ninto a fully-connected one-layer neural network, denoted as\nMLP i, converting each vector into a vector of dimension H.\nThe \fnal representation Qvec ifor this set is then given by\nthe average over the individual transformed representations\nof its elements, i.e.,\nQvec i=1\njVijX\nv2ViMLP i(v)\nMLP i(v) =Relu (vUi+bi)\nWhere Ui2RLxH,bi2RHare the learned weights and\nbias, and v2RLis the input row vector. We choose an av-\nerage (instead of, e.g., sum) to ease generalization to di\u000ber-\nent numbers of elements in the sets, as otherwise the overall\nmagnitude of Qvec would vary depending on the number of\nelements in the set Vi.\n3.2.3 Third Stage, from (Qvec 1; Qvec 2)toQ1\u001a%Q2\nGiven the representative vectors of the input queries,\n(Qvec 1; Qvec 2), we aim to predict the containment rate\nQ1\u001a%Q2 as accurately as possible. Since we do not know\nwhat a \"natural\" distance measure is in the representative\nqueries vector space, encoded by the neural networks of the\nsecond step, we use a fully-connected two-layer neural net-\nwork, denoted as MLP out, to compute the estimated con-\ntainment rate of the input queries, leaving it up to this neu-\nral network to learn the correct distance function. MLP out\ntakes as input a vector of size 4 Hwhich is constructed from\nQvec 1 and Qvec 2. The \frst layer converts the input vector\ninto a vector of size 2 H. The second layer converts the ob-\ntained vector of size 2 H, into a single value representing the\ncontainment rate.\n^y=MLP out(Expand (Qvec 1; Qvec 2))\nMLP out(v) =Sigmoid (ReLU (vUout1+bout1)Uout2+bout2)\nExpand (v1; v2) = [v1; v 2; abs (v1\u0000v2); v 1\fv2]\nHere, ^ yis the estimated containment rate, Uout12R4Hx2H,\nbout12R2HandUout22R2Hx1,bout22R1are the learned\nweights and bias, and \fis the dot-product function.\nIn order to estimate the containment rates more accu-\nrately, we use the Expand function which creates a row con-\ncatenated vector of size 4 Husing vectors Qvec 1 and Qvec 2.\nWe use the ReLU1activation function for hidden layers\nin all the neural networks, as they show strong empirical\nperformance advantages and are fast to evaluate.\n1ReLU(x) = max(0,x); see [40].\n3\n\nFigure 2: Query featurization as sets of feature vectors obtained from sets T,JandP(Rest denotes zeroed portions\nof a vector).\nIn the \fnal step, we apply the Sigmoid2activation func-\ntion in the second layer to output a \roat value in the range\n[0,1], as the containment rate values are within this inter-\nval. Therefore, we do not apply any featurization on the\ncontainment rates (the output of the model) and the model\nis trained with the actual containment rate values without\nany featurization steps.\n3.2.4 Loss Function\nSince we are interested in minimizing the ratio between\nthe predicted and the actual containment rates, we use the\nq-error metric in our evaluation. We train our model to\nminimize the mean q-error [37], which is the ratio between\nan estimated and the actual contaminate rate (or vice versa).\nLetybe the true containment rate, and ^ ythe estimated rate,\nthen the q-error is de\fned as follows.\nq\u0000error (y;^y) = ^ y > y ?^y\ny:y\n^y\nIn addition to optimizing the mean q-error, we also exam-\nined the mean squared error (MSE) and the mean absolute\nerror (MAE) as optimization goals. MSE and MAE would\noptimize the squared/absolute di\u000berences between the pre-\ndicted and the actual containment rates. Optimizing with\ntheses metrics makes the model put less emphasis on heavy\noutliers (that lead to large errors). Therefore, we decided to\noptimize our model using the q-error metric which yielded\nbetter results.\n3.3 Training and Testing Interface\nBuilding CRN involves two main steps. (1) Generating\na random training set using the schema and data informa-\ntion as described in Section 3.1. (2) Repeatedly using this\ntraining data, we train the CRN model as described in Sec-\ntion 3.2 until the mean q-error of the validation test starts\nto converges to its best absolute value. That is, we use the\nearly stopping technique [43] and stop the training before\nconvergence to avoid over-\ftting. Both steps are performed\non an immutable snapshot of the database.\nAfter the training phase, to predict the containment rate\nof an input query pair, the queries \frst need to be trans-\nformed into their feature representation, and then they are\npresented as input to the model, and the model outputs the\nestimated containment rate (Section 3.2).\nWe train and test our model using the Tensor-Flow frame-\nwork [2], and make use of the e\u000ecient Adam optimizer [23]\nfor training the model.\n3.4 Hyperparameter Search\nTo optimize our model's performance, we conducted a\nsearch over its hyperparameter space. In particular, we fo-\ncused on tuning the neural networks hidden layer size (H).\n2Sigmoid(x) = 1 =(1 +e\u0000x); see [40].Note that the same H value is shared in all the neural\nnetworks of the CRN model, as described in section 3.2.\nDuring the tuning of the size hyperparameter of the neural\nnetwork hidden layer, we found that increasing the size of\nour hidden layer generally led to an increase in the model\naccuracy, till it reached the best mean q-error on the valida-\ntion test. Afterwards, the results began to decline in quality\nbecause of over-\ftting (See Figure 3). Hence, we choose a\nhidden layer of size 512, as a good balance between accu-\nracy and training time. Overall, we found that our model\nperforms similarly well across a wide range of settings when\nconsidering di\u000berent batch sizes and learning rates.\nFigure 3: The mean q-error on the validation set with\ndi\u000berent hidden layer sizes.\n3.5 Model Computational Costs\nWe analyze the training, prediction, and space costs of\nthe CRN model with the default hyperparameters (H=512,\nbatch size=128, learning rate=0.001).\n3.5.1 Training Time\nFigure 4 shows how the mean q-error of the validation\nset decreases with additional epochs, until convergence to a\nmean q-error of around 4.5. The CRN model requires almost\n120 passes on the training set to converge. On average,\nmeasured across six runs, a training run with 120 epochs\ntakes almost 200 minutes.\n3.5.2 Prediction Time\nThe prediction process is dominated by converting the in-\nput queries into the corresponding vectors, and then present-\ning these vectors as input to the CRN model. On average,\nthe prediction time is 0.5ms per single pair of queries, includ-\ning the overhead introduced by the Tensor-Flow framework.\n3.5.3 Model Size\nThe CRN model includes all the learned parameters men-\ntioned in Section 3.2 ( U1,U2,Uout1,Uout2,b1,b2,bout1,\nbout2). In total, there are 2 \u0003L\u0003H+ 8\u0003H2+ 6\u0003H+ 1\nlearned parameters. In practice, the size of the model, when\nserialized to disk, is roughly 1 :5MB.\n4\n\nFigure 4: Convergence of the mean q-error on the vali-\ndation set.\n4. CONTAINMENT EVALUATION\nIn this section we describe how we compared the CRN\nmodel to other (baseline) methods. Since to the best of our\nknowledge, the problem of determining containment rate has\nnot been addressed till now, we used a transformation as\ndescribed in Section 4.1 below.\n4.1 From Cardinality to Containment\nTo our knowledge, this is the \frst work to address the\nproblem of containment rate estimation. In order to com-\npare our results with di\u000berent baseline methods, we used\nexisting cardinality estimation methods to predict the con-\ntainment rates, using the Crd2Cnt transformation, as de-\npicted in the middle part diagram in Figure 7.\n4.1.1 The Crd2Cnt Transformation\nGiven a cardinality estimation model3M, we can con-\nvert it to a containment rate estimation model using the\nCrd2Cnt transformation which returns a model M0for es-\ntimating containment rates. The obtained model M0func-\ntions as follows. Given input queries Q1 and Q2, whose\ncontainment rate Q1\u001a%Q2 needs to be estimated:\n\u000fCalculate the cardinality of query Q1\\Q2 using M.\n\u000fCalculate the cardinality of query Q1 using M.\n\u000fThen, the containment rate estimate is:\nQ1\u001a%Q2 =jQ1\\Q2j\njQ1j\nHere, Q1\\Q2 is the intersection query of Q1 and Q2\nwhose SELECT and FROM clauses are identical to Q1's\n(orQ2's) clauses, and whose WHERE clause is Q1's AND\nQ2's WHERE clauses. Note that, by de\fnition, if jQ1j= 0\nthen Q1\u001a%Q2 = 0.\nGiven model M, we denote the obtained model M0, via\nthe Crd2Cnt transformation, as Crd2Cnt( M).\nWe compared the CRN model predictions to those based\non the other examined cardinality estimation models, using\nthe Crd2Cnt transformation. We examined the PostgreSQL\nversion 11 cardinality estimation component [1], a simple\nand commonly used method for cardinality estimation. In\naddition, we examined the MSCN model [24]. MSCN was\nshown to be superior to the best methods for estimating\ncardinalities such as Random Sampling (RS) [9, 41] and the\nstate-of-the-art Index-Based Join Sampling (IBJS) [30].\n3Here \"model\" may refer to an ML model or simply to a\nmethod.4.1.2 Comparing with MSCN\nIn order to make a fair comparison between the CRN\nmodel and the MSCN model, we train the MSCN model\nwith the same data that was used to train the CRN model.\nThe CRN model takes two queries as input, whereas the\nMSCN model takes one query as input. Therefore, to ad-\ndress this issue, we created the training dataset for the\nMSCN model as follows. For each pair of queries ( Q1; Q2)\nused in training the CRN model, we added the following two\ninput queries to the MSCN training set:\n\u000fQ1\\Q2, along with its actual cardinality.\n\u000fQ1, along with its actual cardinality.\nFinally, we ensure that the training set includes only unique\nqueries without repetition. This way, we both models, MSCN\nand CRN, are trained with the same information.\n4.1.3 Comparing with PostgreSQL\nComparing with PostgreSQL does not require generating\nan appropriate training set, since the PostgreSQL cardi-\nnality estimation component is based on database pro\fling\ntechniques and does not require training.\n4.2 Evaluation Workloads\nWe evaluate CRN on the IMDb dataset as described in\nSection 3.1.1, using two di\u000berent query workloads:\n\u000fcnttest1, a synthetic workload generated by the same\nqueries generator as the one used for generating the\ntraining data (using a di\u000berent random seed) with 1200\nunique query pairs, with zero to two joins.\n\u000fcnttest2, a synthetic workload generated by the same\nqueries generator as the one used for generating the\ntraining data (using a di\u000berent random seed) with 1200\nunique query pairs, with zero to \fvejoins. This dataset\nis used to examine how CRN generalizes to additional\njoins.\nnumber of joins 0 1 3 3 4 5 overall\ncnttest1 400 400 400 0 0 0 1200\ncnttest2 200 200 200 200 200 200 1200\nTable 2: Distribution of joins.\n4.3 The Quality of Estimates\nFigure 5 depicts the q-error of the CRN model compared\nto the Crd2Cnt(PostgreSQL) and Crd2Cnt(MSCN) models\non the cnt test1 workload. While Crd2Cnt(PostgreSQL)'s\nerrors are more skewed towards the positive spectrum,\nCrd2Cnt(MSCN) performs extremely well as does the CRN\nmodel. Observe that we make sure to train MSCN in such a\nway that it will predict containment rates e\u000eciently, while\nthe primary purpose of the MSCN model, as described in\n[24] is estimating cardinalities. That is, had we trained\nthe MSCN model for its main purpose, with \"independent\"\nqueries, we might have ended up with worse results for\nMSCN.\nTo provide a fuller picture, we also show the percentiles,\nmaximum, and mean q-errors. As depicted in Table 3, CRN\nprovides the best results in 75% of the tests, whereas MSCN\nis more robust in the margins, resulting in a better mean.\n5\n\n50th 75th 90th 95th 99th max mean\nCrd2Cnt (PostgreSQL ) 3 :5 41 :18 365 3399 268745 493474 5492\nCrd2Cnt (MSCN ) 2 :84 7 :38 19:95 41 :43 274 3258 17 :08\nCRN 2 :52 6 :17 23:04 44 :85 991 51873 111\nTable 3: Estimation errors on the cnt test1 workload. In\nall the similar tables presented in this paper, we provide\nthe percentiles, maximum, and the mean q-errors of the\ntests. The p'th percentile, is the q-error value below\nwhich p% of the test q-errors are found.\nFigure 5: Estimation errors on the cnt test1 workload.\nIn all the similar plots presented in this paper, the box\nboundaries are at the 25th/75th percentiles and the hor-\nizontal lines mark the 5th/95th percentiles. Hence, 50%\nof the tests results are located within the box bound-\naries, and 90% are located between the horizontal lines.\nThe orange horizontal line mark the 50th percentile.\n4.4 Generalizing to Additional Joins\nIn this section we examine how the CRN model generalizes\nto queries with a higher number of joins without having\nseen such queries during training. To do so, we use the\ncrdtest2 workload which includes queries with zero to \fve\njoins. Recall that we trained both the CRN model and the\nMSCN model only with query pairs that have between zero\nand two joins. Examining the results, described in Table\n4 and Figure 6, the CRN model is noticeably more robust\nin generalizing to queries with additional joins. The mean\nq-error of the CRN model is smaller by a factor of almost 8\nthan the mean q-errors of the other models.\n50th 75th 90th 95th 99th max mean\nCrd2Cnt (PostgreSQL ) 4 :5 46 :22 322 1330 39051 316122 1345\nCrd2Cnt (MSCN ) 4 :1 17 :85 157 754 14197 768051 1238\nCRN 3 :64 13 :19 96 :6 255 2779 56965 161\nTable 4: Estimation errors on the cnt test2 workload.\nFigure 6: Estimation errors on the cnt test2 workload.5. CARDINALITY ESTIMATION USING\nCONTAINMENT RATES\nIn this section we consider one application of the proposed\ncontainment rate estimation model: cardinality estimation.\nWe introduce a novel approach for estimating cardinalities\nusing query containment rates, and we show that using the\nproposed approach, we improve cardinality estimations sig-\nni\fcantly, especially in the case when there are multiple\njoins.\nSince a traditional query optimizer is crucially dependent\non cardinality estimation, which enables choosing among\ndi\u000berent plan alternatives by using the cardinality estima-\ntion of intermediate results within query execution plans.\nTherefore, the query optimizer must use reasonably good\nestimates. However, estimates produced by all widely-used\ndatabase cardinality estimation models are routinely signi\f-\ncantly wrong (under/over-estimated), resulting in not choos-\ning the best plans, leading to slow executions [29].\nTwo principal approaches for estimating cardinalities have\nemerged. (1) Using database pro\fling [1]. (2) Using sam-\npling techniques [9, 41, 30]. Recently, deep learning neural\nnetworks were also used for solving this problem [24, 50].\nHowever, all these approaches, with all the many attempts\nto improve them, have conceptually addressed the problem\ndirectly in the same way, as a black box, where the input\nis a query, and the output is its cardinality estimation, as\ndescribed in the leftmost diagram in Figure 7. In our pro-\nposed approach, we address the problem di\u000berently, and we\nobtain better estimates as described in Section 6.\nIn today's databases, the answer to a previous query is\nrarely useful for speeding up new queries, and the work per-\nformed in answering past queries is often ignored afterwards.\nUsing the CRN model for predicting containment rates, we\nare able to change this by revealing the underlying relations\nbetween the new queries and the previous ones.\nOur new technique for estimating cardinalities mainly re-\nlies on two key ideas. The \frst one is the new framework\nin which we solve the problem. The second is the use of\naqueries pool that maintains multiple previously executed\nqueries along with their actual cardinalities, as part of the\ndatabase meta information. The queries pool provides new\ninformation that enables our technique to achieve better es-\ntimates. Using a containment rate estimation model, we\nmake use of previously executed queries along with their ac-\ntual cardinalities to estimate the result-cardinality of a new\nquery. This is done with the help of a simple transforma-\ntion from the problem of containment rate estimation to the\nproblem of cardinality estimation (see Section 5.1).\n5.1 From Containment to Cardinality\nUsing a containment rate estimation models, we can ob-\ntain cardinality estimates using the Cnt2Crd transforma-\ntion, as depicted in the rightmost diagram in Figure 7.\n5.1.1 The Cnt2Crd Transformation\nGiven a containment rate estimation model4M, we con-\nvert it to a cardinality estimation model using the Cnt2Crd\ntransformation which returns a model M0for estimating\ncardinalities. The obtained model M0functions as follows.\n4Here \"model\" may refer to an ML model or simply to a\nmethod.\n6\n\nFigure 7: A novel approach, from cardinality estimation to containment rate estimation, and back to cardinality\nestimation by using a queries pool.\nGiven a \"new\" query, denoted as Qnew, as input to car-\ndinality estimation, and assuming that there is an \"old\"\nquery denoted as Qold, whose FROM clause is the same as\nQnew's FROM clause, that has already been executed over\nthe database, and therefore jQoldjis known, we:\n\u000fCalculate xrate =Qold\u001a%Qnewusing M.\n\u000fCalculate yrate =Qnew\u001a%Qoldusing M.\n\u000fThen, the cardinality estimate equals to:\njQnewj=xrate\nyrate\u0003jQoldj\nprovided that yrate =Qnew\u001a%Qold6= 0.\nGiven model M, we denote the obtained model M0, via\nthe Cnt2Crd transformation, as Cnt2Crd( M).\n5.2 Queries Pool\nOur technique for estimating cardinality mainly relies on\na queries pool that includes records of multiple queries.\nThe queries pool is envisioned to be an additional com-\nponent of the DBMS, along with all the other customary\ncomponents. It includes multiple queries with their actual\ncardinalities5, without the queries execution results. There-\nfore, holding such a pool in the DBMS as part of its meta\ninformation does not require signi\fcant storage space or\nother computing resources. Maintaining a queries pool in\nthe DBMS is thus a reasonable expectation. The DBMS\ncontinuously executes queries, and therefore, we can easily\ncon\fgure the DBMS to store these queries along with their\nactual cardinalities in the queries pool.\nIn addition, we may generate in advance a queries pool\nusing a queries generator that randomly creates multiple\nqueries with many of the possible joins, and with di\u000berent\ncolumn predicates. We then execute these queries on the\ndatabase to obtain and save their actual cardinalities in the\nqueries pool.\nNotice that, we can combine both approaches (actual com-\nputing and a generator) to create the queries pool. The ad-\nvantage of the \frst approach is that in a real-world situation,\nqueries that are posed in sequence by the same user, may\nbe similar and therefore we can get more accurate cardinal-\nity estimates. The second approach helps in cases where\nthe queries posed by users are diverse (e.g., di\u000berent FROM\nclauses). Therefore, in such cases, we need to make sure,\nin advance, that the queries pool contains su\u000eciently many\nqueries that cover all the possible cases.\n5Due to limited space, we do not detail the e\u000ecient hash-\nbased data structures used to implement the queries pool.Given a query Qwhose cardinality is to be estimated , it\nis possible that we fail to \fnd any appropriate query, in the\nqueries pool, to match with query Q. That happens when\nall the queries in the queries pool have a di\u000berent FROM\nclause than that of query Q, or that they are not contained\nat all in query Q. In such cases we can always rely on the\nknown basic cardinality estimation models. In addition, we\ncan make sure that the queries pool includes queries with\nthe most frequent used FROM clauses, with empty column\npredicates. That is, queries of the following form.\nSELECT * FROM -set of tables- WHERE TRUE . In this\ncase, for most of the queries posed in the database, there\nis at least one query that matches in the queries pool with\nthe given query, and hence, we can estimate the cardinality\nwithout resorting to the basic cardinality estimation models.\n5.3 A Cardinality Estimation Technique\nConsider a new query Qnew, and assume that the DBMS\nincludes a queries pool as previously described. To esti-\nmate the cardinality of Qnewaccurately, we use multiple\nold queries instead of onequery, using the same Cnt2Crd\ntransformation of Section 5.1.1, as described in Figure 8.\nEstimateCardinality(Query Qnew, Queries Pool QP):\nresults = empty list\nFor every pair ( Qold;jQoldj) inQP:\nifQold's FROM clause6=Qnew's FROM clause:\ncontinue\nCalculate xrate =Qold\u001a%Qnew\nCalculate yrate =Qnew\u001a%Qold\nifyrate < =epsilon : /* y equals zero */\ncontinue\nresults .append( xrate=y rate\u0003jQoldj)\nreturn F( results )\nFigure 8: Cardinality Estimation Technique.\nEstimating cardinality considers all the matching queries\nwhose FROM clauses are identical to Qnew's FROM clause.\nFor each matching query, we estimate Qnew's cardinality\nusing the Cnt2Crd transformation and save the estimated\nresult in the results list. The \fnal cardinality is obtained\nby applying the \fnal function, F, that converts all the esti-\nmated results recorded in the results list, into a single \fnal\nestimation value. Note that the technique can be easily par-\nallelized since each iteration in the For loop is independent,\nand thus can be calculated in parallel.\n7\n\n5.3.1 Comparing Different Final Functions\nWe examined various \fnal functions ( F), including:\n\u000fMedian, returning the median value of the results list.\n\u000fMean, returning the mean value of the results list.\n\u000fTrimmed mean, returning the trimmed mean of the\nresults list without the 25% outliers (trimmed mean\nremoves a designated percentage of the largest and\nsmallest values before calculating the mean).\nExperimentally, the cardinality estimates using the vari-\nous functions were very similar in terms of q-error. But the\nMedian function yielded the best estimates (we do not detail\nthese experiments due to limited space).\n6. CARDINALITY EVALUATION\nWe evaluate our proposed technique for estimating cardi-\nnality, with di\u000berent test sets, while using the CRN model\nas de\fned in Section 3.2 for estimating containment rates.\nWe compare our cardinality estimates with those of the\nPostgreSQL version 11 cardinality estimation component\n[1], and the MSCN model [24].\nWe train both the CRN model and the MSCN model with\nthesame training set as described in Section 4.1.2. Also, we\ncreate the test workloads using the same queries generator\nused for creating the training set of the CRN and the MSCN\nmodels (described in Section 3.1.2), while skipping its last\nstep. That is, we only run the \frst two steps of the genera-\ntor. The third step creates query pairs which are irrelevant\nfor the cardinality estimation task.\n6.1 Evaluation Workloads\nWe evaluate our approach on the (challenging) IMDb dataset,\nusing three di\u000berent query workloads:\n\u000fcrdtest1, a synthetic workload generated by the same\nqueries generator that was used for creating the train-\ning data of the CRN model, as described in Section\n3.1 (using a di\u000berent random seed) with 450 unique\nqueries, with zero to two joins.\n\u000fcrdtest2, a synthetic workload generated by the same\nqueries generator as the training data of the CRN\nmodel, as described in Section 3.1 (using a di\u000berent\nrandom seed) with 450 unique queries, with zero to\n\fvejoins. This dataset is designed to examine how\nthe technique generalizes to additional joins.\n\u000fscale, another synthetic workload, with 500 unique\nqueries, derived from the MSCN test set as introduced\nin [24]. This dataset is designed to examine how the\ntechnique generalizes to queries that were notcreated\nwith the same trained queries' generator.\nnumber of joins 0 1 3 3 4 5 overall\ncrdtest1 150 150 150 0 0 0 450\ncrdtest2 75 75 75 75 75 75 450\nscale 115 115 107 88 75 0 500\nTable 5: Distribution of joins.6.2 Queries Pool\nOur technique relies on a queries pool, we thus created a\nsynthetic queries pool, QP, generated by the same queries\ngenerator as the training data of the containment rate esti-\nmation model, as described in Section 3.1 (using a di\u000berent\nrandom seed) with 300 queries, equally distributed among\nall the possible FROM clauses over the database. In par-\nticular, QP, covers all the possible FROM clauses that are\nused in the tests workloads. Note that, there are no shared\nqueries between QPqueries and the test workloads queries.\nConsider a query Qwhose cardinality needs to be esti-\nmated. On the one hand, the generated QPcontains \"sim-\nilar\" queries to query Q, these can help the machine in pre-\ndicting the cardinality. On the other hand, it also includes\nqueries that are not similar at all to query Q, that may\ncause erroneous cardinality estimates. Therefore, the gen-\nerated queries pool QP, faithfully represents a real-world\nsituation.\n6.3 Experimental Environment\nIn all the following cardinality estimation experiments, for\npredicting the cardinality of a given query Qin a workload\nW, we use the whole queries pool QPas described in Section\n6.2 with all its 300 queries. That is, the old queries used for\npredicting cardinalities, are the queries of QP. In addition,\nin all the experiments we use the Median function as the\n\fnalFfunction.\n6.4 The Quality of Estimates\nFigure 9 depicts the q-error of the Cnt2Crd(CRN) model\nas compared to MSCN and PostgreSQL on the crd test1\nworkload. While PostgreSQL's errors are more skewed to-\nwards the positive spectrum, MSCN is competitive with\nCnt2Crd(CRN) in all the described values. As can be seen\nin Table 6, while MSCN provides the best results in the mar-\ngins, the Cnt2Crd(CRN) model is more accurate in 75% of\nthe test. In addition, we show in the next section (Section\n6.5) that the Cnt2Crd(CRN) model is more robust when\nconsidering queries with more joins than in the training\ndataset.\n50th 75th 90th 95th 99th max mean\nPostgreSQL 1:74 3 :72 22 :46 149 1372 499266 1623\nMSCN 2:11 4 :13 7:79 12 :24 51 :04 184 4 :66\nCnt2Crd (CRN )1:83 3 :71 10:01 18 :16 76 :54 1106 9 :63\nTable 6: Estimation errors on the crd test1 workload.\nFigure 9: Estimation errors on the crd test1 workload.\n8\n\n6.5 Generalizing to Additional Joins\nWe examine how our technique generalizes to queries with\nadditional joins, without having seen such queries during\ntraining. To do so, we use the crd test2 workload which in-\ncludes queries with zero to \fvejoins. Recall that we trained\nboth the CRN model and the MSCN model only with query\npairs that have between zero and two joins.\nFrom Tables 7 and 8, and Figure 10, it is clear that\nCnt2Crd(CRN) model is signi\fcantly more robust in gen-\neralizing to queries with additional joins. In terms of mean\nq-error, the Cnt2Crd(CRN) model reduces the mean by a\nfactor x100 compared with MSCN and by a factor of x1000\ncompared with PostgreSQL.\n50th 75th 90th 95th 99th max mean\nPostgreSQL 9:22 289 5189 21202 576147 4573136 35169\nMSCN 4:49 119 3018 6880 61479 388328 3402\nCnt2Crd (CRN )2:66 6 :50 18 :72 72 :74 528 6004 34 :42\nTable 7: Estimation errors on the crd test2 workload.\n50th 75th 90th 95th 99th max mean\nPostgreSQL 229 3326 22249 166118 2069214 4573136 70569\nMSCN 121 1810 6900 25884 83809 388328 6801\nCnt2Crd (CRN )4:28 10 :84 43 :71 93 :11 1103 6004 61 :26\nTable 8: Estimation errors on the crd test2 workload\nconsidering only queries with three to \fve joins.\nFigure 10: Estimation errors on the crd test2 workload.\nAs depicted in Figure 10, the Cnt2Crd(CRN) model gen-\neralizes more accurately to additional joins (note that the\nboxes are still on the same q-error interval). To highlight\nthese improvements, we describe, in Table 9 and Figure 11,\nthe mean and median q-error for each possible number of\njoins separately (note the logarithmic y-axis scale in Figure\n11). The known cardinality estimation models su\u000ber from\nunder-estimated results and errors that grow exponentially\nas the number of joins increases [17], as also happens in the\ncases we examined. The Cnt2Crd(CRN) model was better\nat handling additional joins (even though CRN was trained\nonly with queries with up to two joins, as was MSCN).\nnumber of joins 0 1 2 3 4 5\nPostgreSQL 10:41 216 25 :38 355 4430 210657\nMSCN 3 :44 3:56 3:31 81:95 5427 14895\nCnt2Crd (CRN ) 12 :433:54 6:77 23:24 30 :51 129\nTable 9: Q-error means for each number of joins.\nFigure 11: Q-error medians for each number of joins.\n6.6 Generalizing to Different Kinds of Queries\nIn this experiment, we explore how the Cnt2Crd(CRN)\nmodel generalizes to a workload that was not generated by\nthe same queries generator that was used for creating the\nCRN model training set. To do so, we examine the scale\nworkload that was generated using another queries genera-\ntor in [24]. As shown in Table 10, clearly Cnt2Crd(CRN)\nis more robust than MSCN and PostgreSQL in all the de-\nscribed values. Examining Figure 12, it is clear that the\nCnt2Crd(CRN) model is signi\fcantly more robust with queries\nwith 3 and 4 joins. Recall that the QPqueries pool in this\nexperiment was not changed, while the scale workload is\nderived from another queries generator. In summary, this\nexperiment shows that Cnt2Crd(CRN) generalizes to work-\nloads that were created with a di\u000berent generator than the\none used to create the training data.\n50th 75th 90th 95th 99th max mean\nPostgreSQL 2:62 15 :42 183 551 2069 233863 586\nMSCN 3:76 16 :84 100 448 3467 47847 204\nCnt2Crd (CRN )2:53 5 :88 24 :02 95 :26 598 19632 69 :85\nTable 10: Estimation errors on the scale workload.\nFigure 12: Estimation errors on the scale workload.\nTo further examine how Cnt2Crd(CRN) generalizes, we\nconducted the following experiment. We compared the\nCnt2Crd(CRN) model with an improved version of the MSCN\nmodel that combines the deep learning approach and sam-\npling techniques by using samples of 1000 materialized base\ntables, as described in [24]. For simplicity we denote this\nmodel as MSCN1000.\n9\n\nFigure 13: Estimation errors on the crd test2 workload, compared with all models.\nWe make the test easier for MSCN1000 model by training\nthe MSCN1000 model with a training set that was created\nwith the same queries generator that was used for generat-\ning the scale workload. As depicted in Figure 12, while the\nMSCN1000 model is more robust in queries with zero to two\njoins, still, the Cnt2Crd(CRN) model was found to be supe-\nrior on queries with additional joins. Recall that the CRN\nmodel training set was not changed , while the MSCN1000\nmodel was trained with queries obtained from the same\nqueries generator that was used for creating the testing (i.e.,\nscale) workload. In addition, note that MSCN1000 model\nuses sampling techniques whereas Cnt2Crd(CRN) does not.\nThus, this experiment demonstrates the superiority of\nCnt2Crd(CRN) in generalizing to additional joins.\n7. IMPROVING EXISTING CARDINALITY\nESTIMATION MODELS\nIn this section we describe how existing cardinality esti-\nmation models can be improved using the idea underlining\nour proposed technique. The proposed technique for im-\nproving existing cardinality estimation models relies on the\nsame technique for predicting cardinalities using a contain-\nment rate estimation model, as described in Section 5.3.\nIn the previous section we used the CRN model in pre-\ndicting containment rates. CRN can be replaced with any\nother method for predicting containment rates. In particu-\nlar, it can be replaced with any existing cardinality estima-\ntion model after \"converting\" it to estimating containment\nrates using the Crd2Cnt transformation, as described in Sec-\ntion 4.1.\nAt \frst glance, our proposed technique seems to be more\ncomplicated for solving the problem of estimating cardinal-\nities. However, we show that by applying it to known ex-\nisting models, we improve their estimates, without changing\nthe models themselves. These results indicate that the tra-\nditional approach, which directly addressed this problem,\nstraightforwardly, using models to predict cardinalities, can\nbe improved upon.\nIn the remainder of this section, we described the pro-\nposed approach, and show how existing cardinality estima-\ntion methods are signi\fcantly improved upon, by using this\ntechnique.7.1 Approach Demonstration\nGiven an existing cardinality estimation model M, we \frst\nconvert Mto a model M0for estimating containment rates,\nusing the Crd2Cnt transformation, as described in Section\n4.1. Afterwards, given the obtained containment rate esti-\nmation model M0, we convert it to a model M00for esti-\nmating cardinalities, using the Cnt2Crd transformation, as\ndescribed in Section 5.3, which uses a queries pool.\nTo summarize, our technique converts an existing cardi-\nnality estimation model Mto an intermediate model M0for\nestimating containment rates, and then, using M0we create\na model M00for estimating cardinalities with the help of the\nqueries pool, as depicted in Figure 7 from left to right.\nFor simplicity, given cardinality estimation model M, we\ndenote the model M00described above, i.e., model\nCnt2Crd(Crd2Cnt( M)), as Improved Mmodel.\n7.2 Existing Models vs. Improved Models\nWe examine how our proposed technique improves the\nPostgreSQL and the MSCN models, by using the crd test2\nworkload as de\fned in Section 6.1, as it includes the most\nnumber of joins. Table 11 depicts the estimates when us-\ning directly the PostgreSQL model, compared with the esti-\nmates when adopting our technique with PostgreSQL (i.e.,\nthe Improved PostgreSQL model). Similarly, Table 12 de-\npicts the estimates when using directly the MSCN model,\ncompared with the Improved MSCN model. From both ta-\nbles, it is clear that the proposed technique signi\fcantly im-\nproves the estimates (by factor x7 for PostgreSQL and x122\nfor MSCN in terms of mean q-error) without changing the\nmodels themselves (embedded within the Improved version).\nThese results highlight the power of our proposed ap-\nproach that provides an e\u000bective and simple technique for\nimproving existing cardinality estimation models. By adopt-\ning our approach and crating a queries pool in the database,\ncardinality estimates can be improved signi\fcantly.\n50th 75th 90th 95th 99th max mean\nPostgreSQL 9:22 289 5189 21202 576147 4573136 35169\nImproved PostgreSQL 2 :61 19 :3 155 538 17697 1892732 5081\nTable 11: Estimation errors on the crd test2 workload.\n50th 75th 90th 95th 99th max mean\nMSCN 4:49 119 3018 6880 61479 388328 3402\nImproved MSCN 2 :89 7 :43 25 :26 55 :73 196 3184 27 :78\nTable 12: Estimation errors on the crd test2 workload.\n10\n\n7.3 Improved Models vs. Cnt2Crd(CRN)\nUsing the crd test2 workloade, we examine how our tech-\nnique improves PostgreSQL and MSCN, compared with\nCnt2Crd(CRN). Adopting our technique improves the ex-\nisting models as described in Section 7.2. Examining Table\n13, it is clear that in 90% of the tests, the best estimates are\nthose obtained when directly using the CRN model to esti-\nmate the containment rates, instead of converting existing\ncardinality estimation models to obtain containment rates\n(Improved MSCN and Improved PostgreSQL).\n50th 75th 90th 95th 99th max mean\nImproved PostgreSQL 2:61 19 :3 155 538 17697 1892732 5081\nImproved MSCN 2:89 7 :43 25 :2655:73 196 3184 27 :78\nCnt2Crd (CRN ) 2:66 6 :50 18 :72 72:74 528 6004 34 :42\nTable 13: Estimation errors on the crd test2 workload.\n7.4 Cardinality Prediction Computation Time\nUsing the proposed idea of using containment rates esti-\nmations to predict cardinalities, the cardinality prediction\nprocess is dominated by calculating the containment rates\nof the given input query with the relevant queries in the\nqueries pool, and calculating the \fnal function Fon these\nresults to obtain the predicted cardinality, as described in\nSection 5.3. Therefore, the larger the queries pool is, the\nmore accurate the predictions are, and the longer the pre-\ndiction time is. Table 14, shows the medians and the means\nestimation errors on the crd test2 workload, along with the\naverage prediction time for a single query, when using the\nCnt2Crd(CRN) model for estimating cardinalities, with dif-\nferent sizes of QP(equally distributed over all the possible\nFROM clauses in the database) while using the same \fnal\nfunction F(the Median function)).\nQP Size 50 100 150 200 250 300\nMedian 3:68 2 :55 2 :63 2 :55 2 :61 2 :66\nMean 1894 90 41 40 35 34\nPrediction Time 3.2ms 7.1ms 9.8ms 11.3ms 14.5ms 16.1ms\nTable 14: Median and mean estimation errors on the\ncrdtest2 workload, and the average prediction time,\nconsidering di\u000berent queries pool (QP) sizes.\nIn table 15, we compare the average prediction time for\nestimating the cardinality of a single query using all the\nexamined models (when using the whole QPqueries pool\nof size 300). While the default MSCN model is the fastest\nmodel, since it directly estimates the cardinalities without\nusing a queries pool, the Cnt2Crd(CRN) model is the fastest\namong all the models that use a queries pool. That is, the\nCnt2Crd(CRN) model is faster than the Improved MSCN\nmodel and the Improved PostgreSQL model. This is the\ncase, since in the Improved MSCN model or the Improved\nPostgreSQL model, to obtain the containment rates, both\nmodels need to estimate cardinalities of two di\u000berent queries\nas described in Section 4.1, whereas the CRN model directly\nobtains a containment rate in one pass.\nAlthough the prediction time of the models that use queries\npools is higher than the most common cardinality estimation\nmodel (PostgreSQL), the prediction time is still in the order\nof a few milliseconds. In particular, it is similar to the aver-\nage prediction time of models that use sampling techniques,\nsuch as the MSCN version with 1000 base tables samples.Recall that for the results in Table 15, we used a queries\npool ( QP) of size 300. We could have used a smaller pool,\nresulting in faster prediction time, and still obtaining better\nresults using the models which employ a queries pool, as\ndepicted in Table 14. Furthermore, all the the models that\nuse queries pools may be easily parallelized as discussed in\nSection 5.3, and thus, reducing the prediction time (in our\ntests we ran these models serially).\nModel Prediction Time\nPostgreSQL 2.1ms\nMSCN 1.1ms\nMSCN with 1000 samples 33ms\nImproved PostgreSQL 70ms\nImproved MSCN 35ms\nCnt2Crd (CRN ) 16ms\nTable 15: Average prediction time of a single query.\n8. RELATED WORK\nOver the past \fve decades, conjunctive queries have been\nstudied in the contexts of database theory and database\nsystems. Conjunctive queries constitute a broad class of\nfrequently used queries. Their expressive power is roughly\nequivalent to that of the Select-Join-Project queries of rela-\ntional algebra. Therefore, several problems and algorithms\nhave been researched in depth in this context. Chandra and\nMerlin [13] showed that determining containment of con-\njunctive queries is an NP-complete problem. Finding the\nminimal number of conditions that need to be added to a\nquery in order to ensure containment in another query is\nalso an NP-complete problem [49]. This also holds under\nadditional settings involving inclusion and functional depen-\ndencies [49, 3, 22].\nAlthough determining whether query Q1 is contained in\nquery Q2 (analytically) in the case of conjunctive queries is\nan intractable problem in its full generality, there are many\ntractable cases for this problem. For instance, in [46, 47] it\nwas shown that query containment in the case of conjunc-\ntive queries could be solved in linear time, if every database\n(edb) predicate occurs at most twice in the body of Q1. In\n[15] it was proved that for every k\u00151, conjunctive query\ncontainment could be solved in polynomial time, if Q2 has\nquerywidth smaller than k+1. In addition to the mentioned\ncases, there are many other tractable cases [44, 11, 12, 19].\nSuch cases are obtained by imposing syntactic or structural\nrestrictions on the input queries Q1 and Q2.\nWhereas this problem was well researched in the past, to\nour knowledge, the problem of determining the containment\nrate on a speci\fc database has not been investigated. In this\npaper, we address this problem using ML techniques.\nLately, we have witnessed extensive adoption of machine\nlearning, and deep neural networks in particular, in many\ndi\u000berent areas and systems, and in particular in databases.\nRecent research investigates machine learning for classical\ndatabase problems such as join ordering [35], index struc-\ntures [26], query optimization [27, 42], concurrency control\n[7], and recently in cardinality estimation [24, 50]. In this\npaper, we propose a deep learning-based approach for pre-\ndicting containment rates on a speci\fc database and show\nhow containment rates can be used to predict cardinalities\nmore accurately.\n11\n\nThere were many attempts to tackle the problem of car-\ndinality estimation; for example, Random Sampling tech-\nniques [9, 41], Index based Sampling [30], and recently deep\nlearning [24, 50]. However, all these attempts have ad-\ndressed, conceptually, the problem directly in the same way,\nas a black box, where the input is a query, and the output\nis the cardinality estimate. We address this problem di\u000ber-\nently by using information about queries that have already\nbeen executed in the database, together with their actual\nresult cardinalities, and the predicted containment rates be-\ntween them and the new examined query. Using this new\napproach, we improve cardinality estimates signi\fcantly.\n9. CONCLUSIONS AND FUTURE WORK\nWe introduced a new problem, that of estimating contain-\nment rates between queries over a speci\fc database, and in-\ntroduced the CRN model, a new deep learning model for\nsolving it. We trained CRN with generated queries, uni-\nformly distributed within a constrained space, and showed\nthat CRN usually obtains the best results in estimating con-\ntainment rates as compared with other examined models.\nWe introduced a novel approach for cardinality estima-\ntion, based on the CRN-based containment rate estimation\nmodel, and with the help of a queries pool. We showed the\nsuperiority of our new approach in estimating cardinalities\nmore accurately than state-of-the-art approaches. Further,\nwe showed that it addresses the weak spot of existing cardi-\nnality estimation models, which is handling multiple joins.\nIn addition, we proposed a technique for improving any\nexisting cardinality estimation model without the need to\nchange the model itself, by embedding it within a three step\nmethod. Given that the estimates of state-of-the-art models\nare quite fragile, and that our new approach for estimating\ncardinalities is simple, has low overhead, and is quite e\u000bec-\ntive, we believe that it is highly promising and practical for\nsolving the cardinality estimation problem.\nTo make our containment based approach suitable for\nmore general queries, the CRN model for estimating con-\ntainment rates can be extended to support other types of\nqueries, such as the union queries, and queries that include\ncomplex predicates. In addition, the CRN model can be\ncon\fgured to support databases that are updated from time\nto time. Next, we discuss some of these extensions, and\nsketch possible future research directions.\nStrings. A simple addition to our current implementation\nmay support equality predicates on strings. To do so, we\ncould hash all the possible string literals in the database\ninto the integer domain (similarly to MSCN). This way, an\nequality predicate on strings can be converted to an equality\npredicate on integers, which the CRN model can handle.\nComplex predicates. Complex predicates, such as LIKE,\nare not supported since they are not represented in the CRN\nmodel. To support such predicates we need to change the\nmodel architecture to handle such predicates. Note that\npredicates such as BETWEEN and IN, may be converted to\nordinary predicates.\nSELECT clause. In this work we addressed only queries\nwith a SELECT * clause. We can handle queries with SE-\nLECT clauses that include speci\fc columns. Given such a\nquery Q,Q's cardinality is equivalent to the cardinality of\nthe query with a SELECT * clause instead, as long as the\nDISTINCT keyword is not used.EXCEPT Operator. Given a query Qof the form\nQ1 EXCEPT Q2, the CRN model can handle Q. In terms\nof containment rates:\n(Q1EXCEPT Q 2)\u0012%Q3 =Q1\u0012%Q3\u0000(Q1\\Q2)\u0012%Q3\nUsing the same idea, we can handle the opposite contain-\nment direction case, and the case where there are more than\ntwo queries, recursively. The case when considering cardi-\nnalities is similar.\njQ1EXCEPT Q 2j=jQ1j\u0000jQ1\\Q2j\nUnion Queries. Given a query Qof the from Q1 UNION\nQ2, the CRN model can handle Qas follows:\n(Q1UNION Q 2)\u0012%Q3\n=Q1\u0012%Q3 +Q2\u0012%Q3\u0000(Q1\\Q2)\u0012%Q3\nUsing the same idea, we can handle the opposite contain-\nment direction case, and the case where there are more than\ntwo queries, recursively. The case when considering cardi-\nnalities is similar.\njQ1UNION Q 2j=jQ1j+jQ2j\nThe OR operator. Given queries that include the operator\nOR in their WHERE clause, the CRN model does not handle\nsuch queries straightforwardly. But, we can handle such\nqueries using a recursive algorithm that converts the queries\ninto multiple conjunctive queries by converting the WHERE\nclause to DNF, and considering every conjunctive clause as\na separate query.\n(Q1OR Q 2)\u0012%Q3 = (Q1UNION Q 2)\u0012%Q3\nUsing the same idea, we can handle the opposite contain-\nment direction case, and the case where there are more than\ntwo queries, recursively. The case when considering cardi-\nnalities is similar.\njQ1OR Q 2j=jQ1UNION Q 2j\u0000jQ1\\Q2j\nDatabase updates. Thus far, we assumed that the database\nis static (read-only database). However, in many real world\ndatabases, updates occur frequently. In addition, the database\nschema itself may be changed. To handle updates we can\nuse one of the following approaches:\n(1) We can always completely re-train the CRN model\nwith a new updated training set. This comes with a consid-\nerable compute cost for re-executing queries pairs to obtain\nup-to-date containment rates and the cost for re-training the\nmodel itself. In this approach, we can easily handle changes\nin the database schema, since we can change the model en-\ncodings prior to re-training it.\n(2) We can incrementally train the model starting from\nits current state, by applying new updated training sam-\nples, instead of re-training the model from scratch. While\nthis approach is more practical, a key challenge here is to ac-\ncommodate changes in the database schema. To handle this\nissue, we could hold, in advance, additional place holders in\nour model to be used for future added columns or tables.\nIn addition, the values ranges of each column may change\nwhen updating the database, and thus, the normalized val-\nues may be modi\fed as well. Ways to handle this problem\nare the subject of current research.\n12\n\n10. REFERENCES\n[1] Postgresql, the world's most advanced open source\nrelational database. https://www.postgresql.org/ .\n[2] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis,\nJ. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard,\nM. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G.\nMurray, B. Steiner, P. Tucker, V. Vasudevan,\nP. Warden, M. Wicke, Y. Yu, and X. Zheng.\nTensor\row: A system for large-scale machine learning.\nIn12th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI 16) , pages\n265{283, 2016.\n[3] S. Abiteboul, R. Hull, and V. Vianu. Foundations of\nDatabases . Addison-Wesley, 1995.\n[4] A. V. Aho, Y. Sagiv, and J. D. Ullman. Equivalences\namong relational expressions. SIAM J. Comput. ,\n8(2):218{246, 1979.\n[5] G. Aurelien. Hands-on machine learning with\nScikit-Learn and TensorFlow : concepts, tools, and\ntechniques to build intelligent systems . O'Reilly Media,\n2017.\n[6] D. Bogdanova, C. N. dos Santos, L. Barbosa, and\nB. Zadrozny. Detecting semantically equivalent\nquestions in online user forums. In Proceedings the\n19th Conference on Computational Natural Language\nLearning, CoNLL , pages 123{131, 2015.\n[7] R. Bordawekar and O. Shmueli, editors. Proceedings of\nthe Second International Workshop on Exploiting\nArti\fcial Intelligence Techniques for Data\nManagement, aiDM@SIGMOD . ACM, 2019.\n[8] S. R. Bowman, J. Gauthier, A. Rastogi, R. Gupta,\nC. D. Manning, and C. Potts. A fast uni\fed model for\nparsing and sentence understanding. CoRR ,\nabs/1603.06021, 2016.\n[9] M. Bressan, E. Peserico, and L. Pretto. Simple set\ncardinality estimation through random sampling.\nCoRR , abs/1512.07901, 2015.\n[10] J. Brownlee. When to use mlp, cnn, and rnn neural\nnetworks. https://machinelearningmastery.com/when-\nto-use-mlp-cnn-and-rnn-neural-networks/ ,\n2018.\n[11] A. Cal\u0012 \u0010. Containment the conjunctive queries over\nconceptual schemata. In Proceedings the Database\nSystems for Advanced Applications\nconference,DASFAA , pages 628{643, 2006.\n[12] E. P. F. Chan. Containment and minimization of\npositive conjunctive queries in oodb's. In Proceedings\nthe Eleventh ACM SIGACT-SIGMOD-SIGART , pages\n202{211, 1992.\n[13] A. K. Chandra and P. M. Merlin. Optimal\nimplementation of conjunctive queries in relational\ndata bases. In Proceedings of the 9th Annual ACM\nSymposium on Theory of Computing , pages 77{90,\n1977.\n[14] G. Chatzopoulou, M. Eirinaki, and N. Polyzotis.\nQuery recommendations for interactive database\nexploration. In Scienti\fc and Statistical Database\nManagement, 21st International Conference, SSDBM ,\npages 3{18, 2009.\n[15] C. Chekuri and A. Rajaraman. Conjunctive query\ncontainment revisited. Theor. Comput. Sci. ,\n239(2):211{229, 2000.[16] J. Chung, C \u0018 . G ul\u0018 cehre, K. Cho, and Y. Bengio.\nEmpirical evaluation of gated recurrent neural\nnetworks on sequence modeling. CoRR ,\nabs/1412.3555, 2014.\n[17] J. Cli\u000bord and R. King, editors. Proceedings the 1991\nACM SIGMOD International Conference on\nManagement of Data . ACM Press, 1991.\n[18] M. Eirinaki, S. Abraham, N. Polyzotis, and N. Shaikh.\nQuerie: Collaborative database exploration. IEEE\nTrans. Knowl. Data Eng. , 26(7):1778{1790, 2014.\n[19] C. Farr\u0013 e, W. Nutt, E. Teniente, and T. Urp\u0013 \u0010.\nContainment of conjunctive queries over databases\nwith null values. In Proceedings ICDT , pages 389{403,\n2007.\n[20] A. Ganapathi, H. A. Kuno, U. Dayal, J. L. Wiener,\nA. Fox, M. I. Jordan, and D. A. Patterson. Predicting\nmultiple metrics for queries: Better decisions enabled\nby machine learning. In Proceedings ICDE , pages\n592{603, 2009.\n[21] I. J. Goodfellow, Y. Bengio, and A. C. Courville. Deep\nLearning . Adaptive computation and machine\nlearning. MIT Press, 2016.\n[22] D. S. Johnson and A. C. Klug. Testing containment of\nconjunctive queries under functional and inclusion\ndependencies. J. Comput. Syst. Sci. , 28(1):167{189,\n1984.\n[23] D. P. Kingma and J. Ba. Adam: A method for\nstochastic optimization. In Proceedings the 3rd\nInternational Conference on Learning\nRepresentations, ICLR , 2015.\n[24] A. Kipf, T. Kipf, B. Radke, V. Leis, P. A. Boncz, and\nA. Kemper. Learned cardinalities: Estimating\ncorrelated joins with deep learning. In Proceedings\nCIDR , 2019.\n[25] J. Kirkpatrick, R. Pascanu, N. C. Rabinowitz,\nJ. Veness, G. Desjardins, A. A. Rusu, K. Milan,\nJ. Quan, T. Ramalho, A. Grabska-Barwinska,\nD. Hassabis, C. Clopath, D. Kumaran, and\nR. Hadsell. Overcoming catastrophic forgetting in\nneural networks. CoRR , abs/1612.00796, 2016.\n[26] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and\nN. Polyzotis. The case for learned index structures. In\nProceedings SIGMOD Conference , pages 489{504,\n2018.\n[27] S. Krishnan, Z. Yang, K. Goldberg, J. M. Hellerstein,\nand I. Stoica. Learning to optimize join queries with\ndeep reinforcement learning. CoRR , abs/1808.03196,\n2018.\n[28] M. S. Lakshmi and S. Zhou. Selectivity estimation in\nextensible databases - A neural network approach. In\nProceedings VLDB'98 , pages 623{627, 1998.\n[29] V. Leis, A. Gubichev, A. Mirchev, P. A. Boncz,\nA. Kemper, and T. Neumann. How good are query\noptimizers, really? PVLDB , 9(3):204{215, 2015.\n[30] V. Leis, B. Radke, A. Gubichev, A. Kemper, and\nT. Neumann. Cardinality estimation done right:\nIndex-based join sampling. In Proceedings CIDR , 2017.\n[31] V. Leis, B. Radke, A. Gubichev, A. Mirchev, P. A.\nBoncz, A. Kemper, and T. Neumann. Query\noptimization through the looking glass, and what we\nfound running the join order benchmark. VLDB J. ,\n27(5):643{668, 2018.\n13\n\n[32] J. Li, A. C. K onig, V. R. Narasayya, and\nS. Chaudhuri. Robust estimation of resource\nconsumption for SQL queries using statistical\ntechniques. PVLDB , 5(11):1555{1566, 2012.\n[33] H. Liu, M. Xu, Z. Yu, V. Corvinelli, and C. Zuzarte.\nCardinality estimation using neural networks. In\nProceedings the 25th Annual International Conference\non Computer Science and Software Engineering,\nCASCON , pages 53{59, 2015.\n[34] G. Lohman. Is query optimization a solved problem ?\nhttps://wp.sigmod.org/?p=1075 , 2014.\n[35] R. Marcus and O. Papaemmanouil. Deep\nreinforcement learning for join order enumeration. In\nProceedings the First International Workshop on\nExploiting Arti\fcial Intelligence Techniques for Data\nManagement, aiDM@SIGMOD, TX, USA, June 10,\n2018, pages 3:1{3:4, 2018.\n[36] A. O. Mendelzon and J. Paredaens, editors.\nProceedings the Seventeenth ACM\nSIGACT-SIGMOD-SIGART Symposium on Principles\nof Database Systems . ACM Press, 1998.\n[37] G. Moerkotte, T. Neumann, and G. Steidl. Preventing\nbad plans by bounding the impact of cardinality\nestimation errors. PVLDB , 2(1):982{993, 2009.\n[38] R. Nambiar and M. Poess, editors. Performance\nEvaluation and Benchmarking for the Analytics Era -\n9th TPC Technology Conference, TPCTC , volume\n10661 of Lecture Notes in Computer Science . Springer,\n2018.\n[39] M. A. Nielsen. Neural Networks and Deep Learning .\nDetermination Press, 2017.\n[40] C. Nwankpa, W. Ijomah, A. Gachagan, and\nS. Marshall. Activation functions: Comparison of\ntrends in practice and research for deep learning.\nCoRR , abs/1811.03378, 2018.\n[41] F. Olken and D. Rotem. Random sampling from\ndatabase \fles: A survey. In Proccedings the Statistical\nand Scienti\fc Database Management, 5th\nInternational Conference SSDBM , pages 92{111, 1990.[42] J. Ortiz, M. Balazinska, J. Gehrke, and S. S. Keerthi.\nLearning state representations for query optimization\nwith deep reinforcement learning. In Proceedings the\nSecond Workshop on Data Management for\nEnd-To-End Machine Learning, DEEM@SIGMOD\n2018, pages 4:1{4:4, 2018.\n[43] L. Prechelt. Early stopping - but when? In Neural\nNetworks: Tricks of the Trade - Second Edition , pages\n53{67. 2012.\n[44] G. Rull, P. A. Bernstein, I. G. dos Santos, Y. Katsis,\nS. Melnik, and E. Teniente. Query containment in\nentity SQL. In Proceedings ACM SIGMOD , pages\n1169{1172, 2013.\n[45] Y. Sagiv and M. Yannakakis. Equivalences among\nrelational expressions with the union and di\u000berence\noperators. J. ACM , 27(4):633{655, 1980.\n[46] Y. Saraiya. Subtree elimination algorithms in\ndeductive databases. PhD thesis, Department of\nComputer Science, Stanford University. , 1991.\n[47] Y. P. Saraiya. Polynomial-time program\ntransformations in deductive databases. In Proceedings\nof the Ninth ACM SIGACT-SIGMOD-SIGART\nSymposium on Principles of Database Systems , PODS\n'90, pages 132{144. ACM, 1990.\n[48] C. Sirangelo. Positive relational algebra. In\nEncyclopedia of Database Systems, Second Edition .\nComputer Science Press, 2018.\n[49] J. D. Ullman. Principles the Database and\nKnowledge-Base Systems, Volume II . Computer\nScience Press, 1989.\n[50] L. Woltmann, C. Hartmann, M. Thiele, D. Habich,\nand W. Lehner. Cardinality estimation with local deep\nlearning models. In Proceedings the Second\nInternational Workshop on Exploiting Arti\fcial\nIntelligence Techniques for Data Management,\naiDM@SIGMOD , pages 5:1{5:8, 2019.\n[51] W. E. Zhang, Q. Z. Sheng, Y. Qin, K. Taylor, and\nL. Yao. Learning-based SPARQL query performance\nmodeling and prediction. World Wide Web ,\n21(4):1015{1035, 2018.\n14",
  "textLength": 70506
}