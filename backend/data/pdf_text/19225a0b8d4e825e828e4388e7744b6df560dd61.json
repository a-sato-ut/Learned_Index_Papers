{
  "paperId": "19225a0b8d4e825e828e4388e7744b6df560dd61",
  "title": "GeoBlocks: A Query-Driven Storage Layout for Geospatial Data",
  "pdfPath": "19225a0b8d4e825e828e4388e7744b6df560dd61.pdf",
  "text": "GeoBlocks: A Query-Cache Accelerated Data Structure\nfor Spatial Aggregation over Polygons\nChristian Winter Andreas Kipfâ˜…Christoph Anneser\nEleni Tzirita Zacharatouâ‹„Thomas Neumann Alfons Kemper\nTechnische UniversitÃ¤t MÃ¼nchen MIT CSAILâ˜…Technische UniversitÃ¤t Berlinâ‹„\n{winterch, anneser, neumann, kemper}@in.tum.de kipf@mit.edu eleni.tziritazacharatou@tu-berlin.de\nABSTRACT\nAs individual traffic and public transport in cities are changing,\ncity authorities need to analyze urban geospatial data to improve\ntransportation and infrastructure. To that end, they highly rely on\nspatial aggregation queries that extract summarized information\nfrom point data (e.g., Uber rides) contained in a given polygo-\nnal region (e.g., a city neighborhood). To support such queries,\ncurrent analysis tools either allow only predefined aggregates\non predefined regions and are thus unsuitable for exploratory\nanalyses, or access the raw data to compute aggregate results\non-the-fly, which severely limits the interactivity. At the same\ntime, existing pre-aggregation techniques are inadequate since\nthey maintain aggregates over rectangular regions. As a result,\nwhen applied over arbitrary polygonal regions, they induce an\napproximation error that cannot be bounded.\nIn this paper, we introduce GeoBlocks, a novel pre-aggregating\ndata structure that supports spatial aggregation over arbitrary\npolygons. GeoBlocks closely approximate polygons using a set\nof fine-grained grid cells and, in contrast to prior work, allow\nto bound the approximation error by adjusting the cell size. Fur-\nthermore, GeoBlocks employ a trie-like cache that caches aggre-\ngate results of frequently queried regions, thereby dynamically\nadapting to the skew inherently present in query workloads\nand improving performance over time. In summary, GeoBlocks\noutperform on-the-fly aggregation by up to three orders of mag-\nnitude, achieving the sub-second query latencies required for\ninteractive exploratory analytics.\n1 INTRODUCTION\nNowadays, the amount of geospatial data collected in cities is in-\ncreasing rapidly, thanks to the widespread use of mobility applica-\ntions such as Uber [ 53]. To analyze this data and make data-driven\ndecisions, city officials and planners often rely on visualization\nframeworks that allow users to visualize data of interest at differ-\nent spatial and temporal resolutions [ 4,8,41,50,53]. To generate\ncommon visualizations, such as heatmaps, visual tools perform\nspatial aggregation queries that partition the data over differ-\nent polygonal-shaped regions and then compute summarized\naggregate information for each region. To support exploratory\nanalyses , visual tools must provide interactive response times as\nhigh latency reduces the rate at which users make observations,\ndraw generalizations, and generate hypotheses [ 22]. However,\nthe sheer size of the data combined with the complexity of spatial\nqueries prohibit interactivity, which severely limits analyses. As\nshown in [ 28], current tools operating over raw geospatial data\ncannot produce results fast enough for interactive analysis.\nÂ©2021 Copyright held by the owner/author(s). Published in Proceedings of the\n24th International Conference on Extending Database Technology (EDBT), March\n23-26, 2021, ISBN 978-3-89318-084-4 on OpenProceedings.org.\nDistribution of this paper is permitted under the terms of the Creative Commons\nlicense CC-by-nc-nd 4.0.\nError along top edge\nÂ© OpenStreetMap contributors Wikipedia (CC-BY-SA)fare_amount distance tip_rate\nmin\nmax\nsum4.11\n140\n76500.314282.1351.40.2 0.0\n1.2\n523.4\ncount: 5114 key: 0x89c2597decFigure 1: Cell covering (blue) of the Lower East Side (bor-\nder in orange) with bounded error (red), a cell aggregate\n(green), and a cached commonly queried region (purple).\nOn the bright side, interactive analyses are often repetitive\nin nature. Analysts, for example, typically run multiple aggre-\ngate queries for the same area (e.g., the city center) in a sequence,\nchanging only the aggregate function (e.g., count, sum) or the data\nattribute over which the aggregation is performed. Furthermore,\nthey often focus on certain geospatial regions during their analy-\nsis. They might, for example, iteratively resize the boundary of\nthe spatial region of interest, extracting an aggregate every time,\nor calculate aggregates for neighboring, potentially overlapping,\nregions. Such analyses can greatly benefit from query-driven\nmaterialization approaches that store and reuse intermediate or\neven full query results.\nNaturally, in classical OLAP settings, query-driven materi-\nalization and result recycling are widely used and well under-\nstood [ 24,35,42,45]. However, these methods do not address\nmulti-dimensional spatial data. While methods have also been\nproposed for spatio-temporal OLAP queries, such as nanocubes\n[21] and the aR-tree [ 30,31], these do not provide precision guar-\nantees for spatial aggregation queries over arbitrary polygons .\nBoth nanocubes and the aR-tree store aggregate information in\na hierarchy of rectangles, maintained using a quadtree and an\nR-tree, respectively. Therefore, they are designed for aggregate\nqueries over rectangular regions while their precision depends\non the granularity of the underlying index structure. Using them\nto compute aggregates over polygonal regions introduces an ap-\nproximation error, which cannot be bounded . There are also some\nanalysis tools, such as Uber Movement [ 53], that rely on pre-\ncomputation to provide exact results for spatial aggregations\nover polygons. However, they require the polygonal regions to\nbe pre-defined at aggregation time. This assumes a priori knowl-\nedge of the workload and is thus not applicable in exploratory\nanalyses, where the query polygons are chosen ad-hoc .\nWe propose GeoBlocks, a novel pre-aggregating data structure\nfor geospatial point data that guarantees error-bounded results\nfor spatial aggregation queries over arbitrarily shaped polygons.\nEssentially, GeoBlocks are materialized views on geospatial point\ndata that pre-compute filters and aggregations on pre-definedarXiv:1908.07753v3  [cs.DB]  16 Mar 2021\n\nl5l6l1\nl4\nxy\nv0\nv1\nv2\nv3\nv412\n...\n...\n...\n...l0l2\nl3Point Data (Known)\nlq1lq2\nlq3\nxy\nlq4Queries (Unknown)\na1\na23\n...\n...\nl7lq5a0Figure 2: Problem overview: Calculating unknown aggre-\ngatesğ‘from known points ğ‘ƒcontained within an un-\nknown query polygon ğ‘…(specified by its vertices ğ‘™ğ‘).\ncolumns. Instead of pre-computing aggregates over a hierarchy of\nrectangles as in prior work, GeoBlocks pre-compute aggregates\nover fine-grained grid cells . As depicted in Figure 1, GeoBlocks sub-\ndivide the spatial domain into grid cells, keeping aggregates for\neach individual cell. We allow the user to specify the geospatial\ngranularity, and thereby bound the spatial approximation error.\nIn addition, we propose a trie-like data structure that caches\naggregates for commonly queried regions in a compact manner,\nenabling even faster response times. GeoBlocks are designed for\nhistorical point data and are thus write-once/read-only. How-\never, while GeoBlocks currently do not support updates, they\ncan be adapted to do so, as we briefly discuss in Section 5. Our\ncontributions are summarized as follows:\nâ€¢We propose GeoBlocks, the first, to the best of our knowl-\nedge, data structure that supports spatial aggregation over\narbitrary polygons, while guaranteeing a bounded error.\nâ€¢We develop a query-driven caching mechanism that fur-\nther accelerates aggregate queries by leveraging the skew\ncommonly found in exploratory query workloads.\nThe advantages of our approach are amply clear from our\nextensive experimental evaluation on real-world data. The results\nshow that GeoBlocks achieve up to three orders of magnitude\nspeedup compared to on-the-fly aggregation approaches and\nsupport sub-second response times.\nIn the remainder of this paper, we first formalize the problem\nin Section 2. Section 3 describes our approach, which we then\nexperimentally evaluate in Section 4. Section 5 summarizes the\nkey points discovered in the evaluation and discusses updates\nfor GeoBlocks. Finally, we present an overview of related work\nin Section 6 before concluding in Section 7.\n2 PROBLEM STATEMENT\nIn this paper, we propose a new data structure to speed up the\nexecution of spatial aggregation queries. Formally, the query can\nbe defined in SQL-like notation as follows:\nSELECT AGG(P. ğ‘£0), . . . , AGG(P. ğ‘£ğ‘˜) FROM P\nWHERE P. ğ‘™INSIDE R( ğ‘™ğ‘1, ğ‘™ğ‘2, . . . , ğ‘™ ğ‘ğ‘š) [AND filterCondition]*\nGiven a set of annotated points of the form P( ğ‘™,ğ‘£0,ğ‘£1,...,ğ‘£ ğ‘›),\nwhereğ‘™=(ğ‘¥ğ‘™,ğ‘¦ğ‘™)is the location of the point and ğ‘£ğ‘–are numerical\nor temporal attributes, this query extracts multiple aggregates\nğ‘ğ‘–=ğ´ğºğº(ğ‘£ğ‘–)over all the points contained in a query region ğ‘….\nThe query region can be any arbitrary polygon , and its geometry is\ndefined by the locations of the polygonâ€™s vertices ğ‘™ğ‘1,ğ‘™ğ‘2,...,ğ‘™ ğ‘ğ‘š.\nThe aggregates are non-holistic functions such as count, sum,\nmin, max, or average. Finally, the query can have zero or more\nfilterConditions on the attributes.cell Rectangular area, hierarchically subdivid-\nable into four children\ncell level Number of subdivisions performed on the\nspatial domain to obtain the cell\ncell id/spatial key Unique one-dimensional identifier of a cell\nblock level Level of grid cells in a GeoBlock\ncell aggregate Aggregates of all tuples of a grid cell\ncell covering Error-bounded approximation of a poly-\ngon using cells\nTable 1: Terminology\n00 1101 10\n0000 \n0001 \n1110\n1111\nlevel i level i+1 \nFigure 3: Hierarchical cell decomposition [16].\nIn exploratory interactive analyses, users can dynamically\nand unpredictably change not only the filtering conditions and\nthe requested aggregates but also the polygonal query region.\nThe data points, on the other hand, are known a priori. Figure 2\npresents an example of this scenario: The left-hand side shows the\ninput points that are located at ( ğ‘™0,...,ğ‘™ 7) and have five attributes\neach. The right-hand side shows the query; the polygonal region\nis marked in blue, while three different aggregates are extracted.\nAs can be seen in the figure, this query applies the aggregation\nover the three points that are contained in the query region,\nlocated atğ‘™5,ğ‘™6, andğ‘™7.\nExisting approaches for spatial aggregation queries, such as\nthe aR-tree [ 30,31], are designed for rectangular regions, and thus\ndo not support arbitrary polygons . Applying them to the example\nof Figure 2 requires to approximate the query polygon with a\nminimum bounding rectangle, displayed in grey, over which the\naggregation is performed. This introduces an extra point in the\nresults,ğ‘™3, which is outside the actual query region.\n3 GEOBLOCKS\nIn this section, we first present the geospatial decomposition that\nforms the basis of our approach. We then discuss how we can\nquantify and bound the error that this decomposition introduces.\nNext, we explain the core concepts of GeoBlocks, their storage\nlayout, and the efficient evaluation of spatial aggregation queries\nusing GeoBlocks. Finally, Section 3.6 outlines our query-driven\ncaching mechanism that further improves performance by lever-\naging the characteristics of the query workload. Table 1 provides\nan overview of the concepts introduced in this section.\n3.1 Geospatial Decomposition\nGeoBlocks rely on a hierarchical, quadtree-based spatial decom-\nposition. In this decomposition, a given area (cf. the outer rec-\ntangle in Figure 3) is recursively subdivided into equally-sized\nsmaller areas that we call cells. Each cell has four children, which\n\nleads to an exponentially growing number of 4ğ‘›cells after re-\ncursively subdividing a cell ğ‘›times. We encode each subdivision\nusing two bits, which allows us to uniquely identify a cell at level\nğ‘›by concatenating the encoding of levels 0toğ‘›. Equivalently, all\ncells at a given level can be enumerated using an order-preserving\nspace-filling curve . Since children cells share a common prefix\nwith their parent cell, containment tests are reduced to efficient\nbitwise operations. This encoding further allows storing cell ids\nin prefix-encoded index structures such as radix trees [ 16,17] or\nin learned indices [ 52] to speed up containment queries. Figure 3\nshows the decomposition of a cell in four (level ğ‘–) and 16 (level\nğ‘–+1) sub-cells, and the corresponding enumeration with a Hil-\nbert curve. Applying our decomposition strategy to the Earthâ€™s\nsurface, we only need 64 bits to address every single square cen-\ntimeter. That way, we map two-dimensional geospatial locations\n(lat/long coordinates) to one-dimensional 64-bit keys. In our im-\nplementation, we use the Google S2 library [ 38] to perform the\nspatial decomposition and cell enumeration. Note, however, that\nour approach is not restricted to S2 or the Hilbert curve. Any\nother framework that supports recursive geospatial subdivisions\nand order-preserving cell enumerations can be used instead.\nPoint Approximation. We map locations (i.e., points ) to the\nsmallest cell that contains them. The imprecision introduced by\nthis approximation (e.g., at most 6.1 mm for any point in the US)\nis negligible, as the imprecision of GPS data is often orders of\nmagnitude worse [54].\nPolygon Approximation. Similarly, we approximate the query\npolygons on-the-fly by mapping them to a set of cells, possibly\nat different levels, as shown in Figure 4 (center and right). We\ncall this geometric approximation a cell covering . In our imple-\nmentation, we calculate cell coverings using the S2 library.\n3.2 Bounded Error\nSimilarly to all geometric approximations, our cell covering in-\ntroduces a spatial error. This is because all the cells that intersect\nthe polygon outline, even minimally, are considered to be part\nof the polygon. However, in contrast to other coverings like the\nwidely used minimum bounding rectangle (MBR), our cell cov-\nering is much more fine-grained. As can be seen in Figure 4,\nthe cell covering approximates the polygon outline much more\nclosely compared to the MBR. More importantly, the introduced\napproximation error can be bounded. In fact, any point on the cell\ncovering is within a distanceâˆšï¸ƒ\nğœ–2\n1+ğœ–2\n2from the polygon outline,\nwhereğœ–1,ğœ–2are the side lengths of the cell. Clearly, the smaller\nthe cell size, the smaller the approximation error. Consequently,\nour cell covering can guarantee a user-defined error bound, i.e., a\nbound on the spatial distance between the approximate and the\noriginal polygon, by using an appropriately small cell size. The\nMBR cannot guarantee such a bound, because its spatial extent,\nand thus its distance from the polygon outline, depends on the\npolygonâ€™s minimum and maximum coordinates in each dimen-\nsion and cannot be controlled [ 52]. The user can specify the error\nbound by choosing an appropriate cell level1so that the cellâ€™s di-\nagonal is not greater than her desired error. This user-controlled\nand bounded spatial error is the only error in GeoBlocks. All\nfurther operations are exact and do not introduce any additional\nerror. While the error bound should be the driving factor when\nselecting a cell level, there are other points to consider: (1) The\ncell diagonal is the maximum error, and the average error can\n1From the table at https://s2geometry.io/resources/s2cell_statistics\nMaximum Spatial ErrorFigure 4: MBR (left) and two cell coverings with increas-\ningly fine-grained resolution.\nExtract\nRaw DataClean & Sort\nBase DataFilter & \nAggregateBuild\nGeoBlock\nFigure 5: Creation of a GeoBlock in two phases. The ex-\ntract phase is run once per dataset. The build phase is run\nfor each filter and error bound combination.\nbe expected to be lower. (2) The cost of reducing the error is not\nlinear. Per each level, the diagonal, and thereby the error bound,\nreduces by a factor of 2. At the same time, the number of grid\ncells, and thus the query input, grows by a factor of 4.\n3.3 Preprocessing\nIn addition to transforming the two-dimensional input space to\none-dimensional spatial keys, we perform some additional pre-\nprocessing steps on the known point data. Our process, outlined\nin Figure 5, consists of two phases, extract and build, and is simi-\nlar to the ETL process traditionally applied in OLAP settings. In\nthe first phase, we prepare the raw data by filtering outliers in\nthe often dirty datasets and limiting the columns to those rele-\nvant and suitable for analysis. We furthermore sort the data by\nthe generated one-dimensional spatial key. This extract phase\nis run exactly once per dataset and allows us to cheaply build\nGeoBlocks from the extracted base data. The second phase, build,\nutilizes the clean and sorted base data to generate a GeoBlock in\na single pass and thus in linear time.\nUpdates and Filters. An important part of data analysis is fil-\ntering to gain insights into the desired subsets of the data. In\nour process, we could apply filters either before or after sorting\nthe raw data. While the first option seems tempting, as it would\nreduce the number of tuples over which the expensive sorting\nhas to be performed, we decided to filter the data in the build\nphase. This way, we can utilize the sorted base data to quickly\nbuild GeoBlocks for different filter predicates, aggregates, and\ngrid resolutions in a single pass. Building new GeoBlocks quickly\nis especially useful in exploratory analyses, where the data and\nfilters of interest might not be fully known a priori. However, the\nincreased cost of sorting all data has to be amortized over mul-\ntiple GeoBlocks and filter predicates. In reality, the sorting cost\nmight be amortized immediately, as some exploratory queries\nmight need to compare a subset of the data with the total. Con-\nsider, for example, a query comparing the tip rate of expensive\ntaxi rides ( WHERE fare_amount > 20 ) with that of all rides. In\nthis case, we would need to build a GeoBlock for all rides, and\ntherefore sort the entire dataset anyway.\n\nGivenğ‘˜different filter predicates with average selectivity ğ‘ \nand a total input size of ğ‘›tuples, we can calculate the runtime\nof building isolated GeoBlocks with filters before sorting, and\nincremental builds from sorted base data as follows:\nğ‘˜âˆ—(ğ‘‚(ğ‘›)+ğ‘‚(ğ‘ ğ‘›âˆ—ğ‘™ğ‘œğ‘”(ğ‘ ğ‘›))+ğ‘‚(ğ‘ ğ‘›)) (1)\nğ‘‚(ğ‘›âˆ—ğ‘™ğ‘œğ‘”(ğ‘›))+ğ‘˜âˆ—ğ‘‚(ğ‘›) (2)\nThe isolated build (1) has three phases, cleaning and filtering\ninğ‘‚(ğ‘›), sorting in ğ‘‚(ğ‘ ğ‘›âˆ—ğ‘™ğ‘œğ‘”(ğ‘ ğ‘›)), and finally aggregating in\nğ‘‚(ğ‘ ğ‘›). Incremental builds (2) have a fixed component composed\nof cleaning and sorting in ğ‘‚(ğ‘›âˆ—ğ‘™ğ‘œğ‘”(ğ‘›)), followed by the incre-\nmental filtering and aggregation of the GeoBlock in ğ‘‚(ğ‘›). For\nincremental builds to pay off, the sorting cost of the regular builds\n(ğ‘˜âˆ—(ğ‘‚(ğ‘ ğ‘›âˆ—ğ‘™ğ‘œğ‘”(ğ‘ ğ‘›)))has to outweigh the initial cost of the incre-\nmental builds ( ğ‘‚(ğ‘›âˆ—ğ‘™ğ‘œğ‘”(ğ‘›))). As we only have runtime classes\nfor each variant and de-facto runtimes will vary between systems\nand datasets, we cannot determine when amortization is reached\nsolely depending on ğ‘˜andğ‘ . However, we provide an in-depth\nexperimental analysis of the amortization in Section 4.\n3.4 Storage Layout\nOnce the filtering of the base data is completed, we can start\naggregating and building a GeoBlock. To build a GeoBlock, for\neach grid cell in the decomposed space, we compute a number of\naggregates over all the tuples that it contains. Empty cells that\ndo not contain any tuples are omitted during aggregation as they\nwould needlessly consume space. We refer to the aggregates of\na grid cell as cell aggregates . A GeoBlock stores cell aggregates\nin ascending order of the cellâ€™s spatial key, which is the same\nsorting order as the one applied to the base data. Moreover, a\nGeoBlock maintains a global header that combines all cell ag-\ngregates into a single GeoBlock-wide aggregate and contains\nadditional metadata required for querying, such as the minimum\nand maximum cell id in the GeoBlock.\nCell Aggregate. Each cell aggregate stores pre-computed an-\nswers for spatial aggregation queries at the grid cell level. A cell\naggregate consists of the cellâ€™s spatial key, the base data offset of\nthe first tuple contained in the cell, and the number of contained\ntuples. Furthermore, it maintains aggregates for all columns (both\nnumeric and temporal attributes) in the extracted data. The main-\ntained aggregates are the minimum, maximum, and sum of all\nvalues contained in the cell. Note that using the sum together\nwith the tuple count allows us to also compute the average as\nsum/count. Furthermore, the cell aggregate stores the minimum\nand maximum keys of the spatial column. The table in Figure 1\nshows an example of a cell aggregate.\nAggregate Granularity. As described in Section 3.2, the block\nlevel (i.e., the granularity of the space decomposition) is defined\nby the user at build time. However, it is also possible to adapt\nthe granularity at a later time. Building a more coarse-grained\nGeoBlock from an existing one is rather straightforward and does\nnot require re-scanning the base data. We can easily combine all\ncell aggregates of the finer-grained GeoBlock corresponding to\na more coarse-grained grid cell in a single pass over the aggre-\ngates. On the other hand, building a more fine-grained GeoBlock\nrequires scanning and further subdividing the base data.\n3.5 Querying\nGeoBlocks support two variants of spatial aggregation queries.\nOn the one hand, they support regular SQL SELECT queries that\ntake a query polygon and produce a user-defined subset of the1lastAgg = 0\n2def selectQuery(polygon):\n3 queryCells = s2.coverPolygon(polygon)\n4 # Prune search range\n5 queryCells.pruneLess(globalHeader.minCell)\n6 queryCells.pruneGreater(globalHeader.maxCell)\n7\n8 lastAgg = 0\n9 resultAggregates = initial\n10 for qcell inqueryCells:\n11 # Map qCell to smaller childCells at the block level\n12 childCells = s2.childrenAtLvl(qcell, BLOCK_LVL)\n13 for cell inchildCells:\n14 getAggregates(cell, resultAggregates)\n15 return result\n16\n17def getAggregates(cell, result):\n18 # Check the last results successor\n19 iflastAgg == 0:\n20 # Search initial header\n21 aggregate = allAggregates.upperBound(cell).prev\n22 ifaggregate.cell == cell:\n23 combineAggergates(aggregate, result)\n24 lastAgg = aggregate\n25 else :\n26 iflastAgg.next.cell == cell:\n27 lastAgg = lastAgg.next\n28 combineAggregates(lastAgg, result)\nListing 1: SELECT query\navailable aggregates. On the other hand, they support a special-\nized efficient implementation of COUNT queries that only report\nthe number of points contained in a query polygon. Such COUNT\nqueries are commonly used in analytics, especially in the context\nof visualization. Figure 1 shows an example query that extracts\na set of aggregates over the Lower East Side region, which is\napproximated by a cell covering (marked in blue). The answer is\ncalculated by extracting and combining all the aggregates con-\ntained in the blue cells.\nTo answer a spatial aggregation query over a polygonal region\n(Figure 6a), the polygon is approximated using a cell covering,\nas discussed in Section 3.1. We compute a cell covering that con-\nforms to the error bound (Figure 6b). Note that the cell covering\ncan have cells at different levels, and some of them might be\nlarger than our grid cells. Such larger cells can be easily mapped\nto smaller grid cells (Figure 6c) in the GeoBlock and offer fur-\nther optimization potential, as discussed next. The cell covering,\nhowever, cannot contain any cells smaller than the cells of the\nGeoBlock. Once we obtain the cell covering, we query the Geo-\nBlock for each of the covering cells, as visualized for a SELECT\nquery in Figure 6d. We then combine these partial results to\ncompute the final result for the entire query polygon. In the\nfollowing, we describe the query process for each cell of the cov-\nering. First, we use the GeoBlockâ€™s header to check if the cell\noverlaps with the GeoBlock at all. Thanks to the prefix-based\ncontainment checks, this is possible in constant time using the\nminimum and maximum cell id in the GeoBlock. Only if there\nis a possible overlap, we continue with the specific checks for\nSELECT andCOUNT queries as follows:\nSELECT Queries. SELECT queries have to look at all cell aggre-\ngates contained in the query cell. Listing 1 presents the pseudo-\ncode of the algorithm. After a query cell has passed the first\ncheck, we try to further limit the search space to the overlapping\narea (Lines 5 & 6). After splitting the query cell to smaller cells\nthat match the GeoBlockâ€™s granularity if needed (Line 12), we\nlocate the first intersecting grid cell using an upper-bound binary\nsearch (Lines 21 - 24). For all the following cells, we exploit the\n\n12022 21\n2022 21\n1012 11\n13a) b) c) d)11?\n502 411 112 220 523\n10? 12? 13?4+1=5id val\nsum(val)Figure 6: Query overview: Query polygon (a), cell covering\n(b), grid-cell representation of covering (c), and subquery\nfor covering cell 1in the cell aggregates (d, Listing 1 Line\n12 and following).\n1def countQuery(polygon):\n2 queryCells = s2.coverPolygon(polygon)\n3 result = 0\n4 for cinqueryCells:\n5 f_child = c.firstChildAtLvl(cell, BLOCK_LVL)\n6 l_child = c.lastChildAtLvl(cell, BLOCK_LVL)\n7 # Get first & last contained aggregate\n8 first = allAggregates.lowerBound(f_child)\n9 last = allAggregates.upperBound(l_child, first)\n10\n11 cnt = last.offset + last.count - first.offset\n12 result += cnt\n13 return result\nListing 2: COUNT query\nfact that cell aggregates are stored contiguously in ascending\norder. This allows us to iterate over the cell aggregates (Lines\n25 - 28) until we reach a grid cell not contained in the query cell,\ncombining all cell aggregates along the way into the query result.\nCOUNT Queries. Intuitively, we can answer COUNT queries faster\nthan SELECT queries, as we can exploit the sorted order of the\ncell aggregates to calculate the count without accessing the cell\naggregates of all grid cells that are contained in the query cell.\nSpecifically, COUNT queries can be answered using the count and\noffset values of only the first and the last cell aggregates that\nare contained in the query cell, as outlined in Listing 2. Note\nthat here we benefit from having larger query cells. The larger\nthe cells used in the covering, the fewer cell aggregates we need\nto access overall. To find the first and last cell aggregates, we\ncalculate the id of the first and last child of the query cell at our\ngrid level. We then locate the first child in the aggregates using a\nlower bound binary search (Line 8). Then, we use the position of\nthe first child as a search start to locate the last child, again with\na binary search (Line 9). Once we have located the aggregates of\nthe first and last contained child, we can calculate (Line 11) the\nresulting count in a range-sum manner as:\nchild last.offset+child last.countâˆ’child first.offset\n3.6 Query-Cache Acceleration\nWhile our cell aggregates can speedup queries significantly, there\nis further potential in pre-computing aggregates for frequently\nqueried areas. This is based on the following key observations:\n(1)Exploratory analyses are often repetitive in nature. Ana-\nlysts, e.g., may run consecutive queries for the same area\nto extract different aggregates (i.e., using a different aggre-\ngate function, or aggregating over a different attribute).\n(2)Furthermore, analysts might only iteratively change the\nshape or size of the query polygon. Consequently, part of\nthe polygonâ€™s interior area remains unchanged.\n(3)Lastly, analytical queries often focus on a geographic sub-\nset of the whole data. For the analysis of the NYC taxi data,\ne.g., the focus lies mostly on Manhattan, Brooklyn, and\nthe airport regions, ignoring most suburbs [40].In all the above cases, it is reasonable to pre-aggregate small\ngrid cells that are often queried together to avoid costly scans\nof individual cells. In our example in Figure 1, e.g., we want to\nkeep a single aggregate for the purple region, instead of having\nto consult all 64 contained cell aggregates.\nDetermining Relevant Aggregates. We want to determine the\nrelevant areas that are worth being additionally pre-aggregated\nand cached, without making any assumptions about the expected\nquery workload or the semantics of the indexed data. To achieve\nthat, we use all previously seen queries as hints. Precisely, to\ndetermine whether an area is worth aggregating, we consider (i)\nthe number of times it was queried, and (ii) its cell level.\nFor each query cell that intersects with the GeoBlock, we keep\ntrack of the number of times it was queried in a trie-like structure.\nWe then use these statistics to calculate cell scores. The score of a\ncell is the sum of the cellâ€™s hits and the hits of its parent. This score\ntakes into account that child cells can be used to speed up queries\nfor parent cells. We then sort all cells by descending score. When\nscores are identical, we sort by ascending level (coarser-grained\ncells come first). As the last criterion, to ensure determinism, we\nsort by spatial key. We chose the above metric as it is sufficient to\nproperly and repeatably represent the skew in the experiments\nin our evaluation while being easy to understand and implement.\nHowever, we also identified some weaknesses of our metric:\nâ€¢Smaller cells might overshadow slightly less frequently\nqueried bigger cells. Consider, for example, the green and\npurple cells of Figure 1 and assume that the green cell is\nqueried just once more than the purple one. Based on our\nmetric, we would then aggregate the green cell even if the\npurple cell could have an up to 64Ã—bigger impact.\nâ€¢The parent-child relationship is simplified: Children only\ncover parts of their parent but are treated as equally useful.\nFurthermore, we do not consider calculating aggregates\nby combining the aggregates of the parent and siblings of\na cell. For example, the count for a cell could be calculated\nby subtracting the count of its sibling cells from the count\nof its parent cell.\nOur evaluation showed that these shortcomings have a minor\nimpact, but we plan to investigate them further and address them,\nif needed, in our future work.\nAggregate Storage. We cache aggregates in a trie-like cache,\nwhich we call AggregateTrie. Further, we allow the user to control\nthe maximum size of the storage available for caching, and we\nstore the AggregateTrie in-place with our cell aggregates and the\nfiltered base data. As the cells are strictly ordered, we can simply\ninsert the most relevant unaggregated cell until the reserved area\nis filled. Figure 7 shows an example AggregateTrie.\nThe storage for the aggregates is split into two parts. The first\npart (up until 0x90 ) contains the trie structure, while the second\npart stores the actual aggregates. The root of the trie corresponds\nto the cell level that can enclose our input data, which is typically\njust a small fraction of the possible earth-wide input space. Each\nfollowing trie-level encodes exactly one cell level, resulting in a\nfanout of 4. Since we store the AggregateTrie in-place, we chose a\ncompact encoding storing all nodes contiguously. Nodes consist\nof just two 32-bit integers. The first one is the pointer to the\nfirst child in the AggregateTrie. The second one is the pointer\nto the corresponding aggregate in the aggregate storage (e.g.,\n0xb8 ). Pointers are encoded as 32-bit offsets from the start of\nthe allocated memory region. Both aggregates and nodes can\nbe sufficiently encoded with an offset, as they are of fixed size.\n\nroot\n0x00\n0x20\n0x40\n0x60\n0x80\n0xa0\n0xc0\n0x8\nn/a\nchild oï¬€set\naggregate oï¬€set\nn/a\n0x68\n0x28\nn/a\n0x48\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\n0xb8\nn/a\nn/a\nn/a\nn/a\n0x90\nmemory\nstarting at 0x8\nstarting at 0x28\nstarting at 0x48\ncontiguous memory\nsingle nodeFigure 7: AggregateTrie with 40 byte aggregates and in-\nmemory representation. Non-existent children (or aggre-\ngates) are marked with n/aand are encoded as 0x0.\nQuery for \nCell c\nNode for c? c cached?\nDirect children \ncached?Yes Yes\nNo No\nSomeAll None\nAnswer using\nGeoBlocks onlyCombine Cache\nand GeoBlocksAnswer using\n Cache only\nFigure 8: Overview of adapted query algorithm.\nNodes occupy 8 bytes, while the size of the aggregates depends\non the schema. Since we store only the offset to the first child, we\nneed to always allocate space for all children in a node, even for\nchildren that do not exist in the cache. This can be seen for the\nnode starting at 0x28 , where only one child has an aggregate and\nno other children or aggregates exist. While this seems wasteful\nat first, the alternative would be to store four individual child\noffsets per node. As children are only created and stored if they\nare needed, our encoding never occupies more storage than this\nalternative. In fact, our design is more space-efficient in all cases,\nexcept for this worst-case in the example above, where only one\nout of four children is required.\nAdapted Query Algorithm. We integrate the cached aggre-\ngates into the query algorithm (cf. Section 3.5). As the runtime\nofCOUNT queries is mostly independent of the cell level since\nonly the first and last grid cells are relevant, we do not expect\nnoticeable speedups for them. Therefore, the adapted process,\nhighlighted in Figure 8, is only used for SELECT queries.\nOnce the pre-query checks are completed, we first probe the\nquery cache and resort to the old algorithm only when necessary.\nFor each query cell, we traverse the AggregateTrie to locate the\ncorresponding node. If there is no node for this cell, we abort\nprobing and answer the query with the old algorithm. Once\nthe node corresponding to the cell is reached, there are two\npossible ways forward. If the cell is cached, i.e., if it has a valid\naggregate offset, the aggregate is extracted as a result. If the\ncell is not cached, there has to be at least one child at any levelresiding in our cache, as nodes are only created on demand.\nWhile, theoretically, all children could be used to reduce the\nnumber of grid cells of the GeoBlock to query, the number drops\nwith each level, while keeping track of the missing children\ngets increasingly expensive. Therefore, we only consider direct\nchildren for this optimization. If some of the direct children are\ncached, we combine their aggregates with the results of the old\nalgorithm for the non-aggregated ones to obtain the final result.\n4 EXPERIMENTAL EVALUATION\nWe compare GeoBlocks with on-the-fly aggregation approaches\non real-world data. To show that our advantage is not dependent\non the indexing strategy, we use different strategies to index\nthe base data of the on-the-fly approaches. We also compare\nGeoBlocks against a pre-aggregating approach, the aR-tree [ 30,\n31]. However, we do not include the aR-tree in all experiments,\nas it is designed for rectangular queries and does not directly\nsupport polygonal ones.\n4.1 Experimental Setup\nBaselines. To keep the experiments as fair as possible, we use\nthe mapping from geospatial space to linear space for the base-\nlines as an index key unless specified otherwise. Furthermore,\nwe keep all data in a columnar layout. Below, we describe the\nthree strategies that we use to index the raw data, as well as our\npre-aggregating baseline:\nBinarySearch: This is the simplest baseline. Instead of indexing\nthe data, we use binary search to locate the first and last con-\ntained raw tuple in the data. Afterward, we loop over all tuples\nin between and compute the requested aggregates. GeoBlocks\nuse binary search to locate the cell aggregate in a similar way.\nBTree: We use the BTree as a secondary index over the raw data.\nFor the experiments, we use an open-source B-tree implementa-\ntion by Google [ 7]. We probe the tree for the first child and scan\nthe sorted raw data until no further tuple qualifies.2\nPHTree: Our last non-aggregating baseline is a multidimensional\npoint index structure, the PH-tree [ 56]. Instead of the one-di-\nmensional spatial key, we use the latitude and longitude of the\npoints to index the data. As the PH-tree only supports rectan-\ngular range queries, we use S2 to get the interior rectangle of\nthe query polygon and use this as a query region. This way, we\nhope to keep the comparison fair, if not favorable for the PHTree,\nas this interior rectangle covers fewer points than our approach.\nAs a consequence, the PHTreeâ€™s query results differ from the\nresults of the other approaches. For the measurements, we use\nan open-source C++ implementation [36].\naRTree: We implement the aR-tree [ 30,31] based on the boost\nR-tree [ 5]. To minimize overlaps between nodes and thereby\noptimize the query performance, we use the ğ‘…âˆ—algorithm. In\nour implementation, each node covers a region ğ‘Ÿand has up\nto 16 child nodes, which further subdivide ğ‘Ÿinto smaller areas.\nFor each node, we store the aggregates in a cell aggregate cor-\nresponding to the region covered by the node, and reference it\nwith an offset (cf. Figure 9). That way, we can modify the RTree\nquery logic by adding early abortion exactly like in the aR-tree.\nGiven a search area ğ‘ and a node of the aR-tree that covers a\nregionğ‘Ÿ, we distinguish three cases, as shown in Listing 3: (a) If\nğ‘ is completely contained by the covered region ğ‘Ÿğ‘of one ofğ‘›â€™s\n2We first tried the PointIndex of the S2 library (https://s2geometry.io/devguide/\ncpp/quickstart.html#s2pointindex) that uses the same b-tree as point storage. Initial\nmeasurements showed that our optimized BTree implementation outperformed the\nPointIndex by 3Ã—, so we opted for our implementation.\n\n1def queryARTree(node, searchArea, result):\n2 partiallyOverlappingNodes = []\n3\n4 for child innode:\n5 ifchild.contains(searchArea):\n6 return queryARTree(child, searchArea, result)\n7 ifsearchArea.contains(child):\n8 result += child.aggregatedResult\n9 else if searchArea.intersects(child):\n10 partiallyOverlappingNodes.append(child)\n11\n12 for child inpartiallyOverlappingNodes:\n13 result += queryARTree(child, searchArea, result)\n14 return result\nListing 3: aR-tree lookup query\n(0,0) \n(6,7) (6,1) \n(9,9) \n0 1Oï¬€set \nBounding Box \n(0,0) \n(3,6) (4,0) \n(6,7) \n2 3(6,1) \n(7,8) (8,8) \n(9,9) \n4 5\n(0,0) \n(1,6) (2,0) \n(3,4) \n6 7(4,0) \n(5,1) (5,2) \n(6,7) \n8 9Cell Aggregates \nSpatial Key Tuple Oï¬€set ...\nSpatial Key Tuple Oï¬€set ...\nSpatial Key Tuple Oï¬€set ...\nSpatial Key Tuple Oï¬€set ...\nSpatial Key Tuple Oï¬€set ...\nSpatial Key Tuple Oï¬€set ...\nSpatial Key Tuple Oï¬€set ...0\n1\n2\n3\n4\n5\n6\nSpatial Key Tuple Oï¬€set ...\nSpatial Key Tuple Oï¬€set ...\nSpatial Key Tuple Oï¬€set ...7\n8\n9\nFigure 9: Illustration of aRTree with node size two and off-\nsets into the cell aggregates.\nchild nodes, we recursively continue the search at the child node\nand do not consider other overlapping child nodes as this would\nresult in counting values multiple times. (b) If the region covered\nby a child node is completely contained within the search area,\nwe add its aggregated value to the overall result and continue\nprocessing the next child node. (c) If ğ‘ and the child node region\nintersect, we mark the child node to be processed later iffno\nother child node fulfills criterion (a).\nBy accepting that points are counted multiple times in the\ncase of overlapping internal nodes, our aR-tree implementation\nfollows the query algorithm of the original aR-tree that does not\nconsider overlapping children. While the implementation delivers\nan upper-bound of the result, it visits the internal nodes in the\nsame way the aR-tree does, thus achieving the same performance.\nImplementation. We implement GeoBlocks in C++ as described\nin Section 3. Our implementation, as well as that of all baselines,\nis single-threaded. Throughout this section, especially in all fig-\nures, we will refer to GeoBlocks as Block. Furthermore, we will\ndifferentiate between the regular Block and BlockQC. Block de-\nnotes GeoBlocks without query caching using the basic query\nalgorithm. BlockQCis GeoBlocks using query caching with the\nAggregateTrie and adapted query process outlined in Figure 8.\nHardware. All experiments are run on a server machine with\ntwo Intel Xeon E5-2680 v4 processors clocked at 2.4 GHz. The\nmachine is equipped with 256 GiB of DDR4-2400 RAM. All per-\nformed experiments fit entirely into main memory.\nDataset. The primary dataset used in the experiments is com-\nposed of trip records from 12 million NYC yellow cab rides in\nthe time between January and March 2015, which we cleaned\nof outliers. It is openly available for download from the NYC\nTaxi and Limousine Commission (TLC) [ 49]. It contains data\nfrom individual rides like pickup and drop-off location and time,\npassenger count as well as trip distance.\n64x73x69x69x4 Aggregates8 Aggregates1 Aggregate2 Aggregates100102104106\n100102104106Runtime in Î¼s [log scale]\nAlgorithm BinarySearch Block BTreeFigure 10: Runtime with increasing number of aggregates.\nUnless otherwise specified, the queries consist of polygons\nrepresenting NYC neighborhoods taken from [ 25]. As a base\nworkload, we build a query containing each polygon once. For\nthe skewed workload, we select 10% of neighborhoods uniformly\nat random and query them multiple times. We select 7 aggregates,\nrequesting each column at least once, as query output.\nIn addition, we use 8 million geotagged tweets from the con-\ntiguous US and query them using polygons representing US\nstates. Finally, we use an extract of 389 million OpenStreetMap\n(OSM) points in the Americas and query them with polygons\nrepresenting countries. Both these datasets have randomly gen-\nerated integer values as payload. For both, we fix the level at 11\n(~7km diagonal). Unless otherwise specified, all experiments are\nconducted on the primary dataset only.\n4.2 Baseline Comparison\nImpact of Number of Aggregates. To show the impact of the\nnumber of aggregates on the performance of the baselines and the\nBlocks, we use a combined workload consisting of once the base\nand four times the skewed workload. We query this workload\nfor 1, 2, 4, and 8 aggregates and report the results in Figure 10.\nAs one can easily see, GeoBlocks outperform both the BTree\nand BinarySearch baseline in all cases. We omitted the PHTree\nand aRTree from these experiments, as the imprecise rectangular\napproximation of the skewed workload lead to a drastic increase\nin their runtime. Even for the base workload, the PHTree was\nslower by a factor of about 3 Ã—while covering fewer tuples.\nIndexing Overhead. We compare the build time, i.e., the prepa-\nration time required prior to running any query, in Figure 11a,\nwith the block level set to 17 (~100m diagonal). The reported times\nfor sorting are measured once for the optimized out-of-place sort-\ning for the Blocks and reported for each baseline. This step is\ncompletely identical in all sorting baselines. There is a notice-\nable gap in the sorting phase between the BTree/BinarySearch\nand the Block. This gap is caused by the collection of grid cell\nids to aggregate that we piggybacked on the sorting process to\nsave an additional pass on the data. Overall, the Block is built\nfaster than the BTree and the PHTree, and slightly slower than\nthe BinarySearch, which only needs to sort the input data. We\nexclude the aRTree baseline from this experiment as we only op-\ntimized the implementation for query performance, and the build\ntime was multiple orders of magnitude slower than the others\ndescribed. Most notably, the majority of the Block preparation is\nspent on sorting, indicating that once the data is sorted, building\nadditional Blocks with different filter sets is reasonably cheap.\nThe relative space overhead of each algorithm is depicted in\nFigure 11b. BinarySearch was omitted as it does not require any\nadditional storage. One could argue that this is not a fair compar-\nison to the BTree and PHTree as they index individual points, but\n\n1.37x1.37x1.37x1.37x1.37x1.37x1.37x1.37x\n0500010000\nBinarySearch Block BTree PHTree\nAlgorithmPrep Time (ms)Phase\nBuilding\nSorting(a) Build time of GeoBlocks and baselines.\n45%\n21%54%\n3%\n0%20%40%60%\nBlock BTree PHTree aRTree\nAlgorithmRelative OverheadAlgorithm\nBlock\nBTree\nPHTree\naRTree (b) Size overhead of GeoBlocks and baselines.\n13141516 1718192021\n0%20%40%\n6000 7000 8000\nPreparation Time in msRelative OverheadBlock Level\n13\n14\n15\n16\n1718\n19\n20\n21 (c) Level influence on GeoBlocks overhead.\nFigure 11: Index overhead in build time and space.\n6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—6Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—1667Ã—\n102103104105106107\n0 25 50 75 100\nSelectivity in %Runtime in Î¼s [log scale]BinarySearch\nBlock\nBlockQCBTree \nPHTree \naRTree\nFigure 12: Query runtime for varying selectivity.\nas our goal is to provide approximate results, we wanted to show\nthat storing intermediate results is less space-consuming than\none would assume for such fine-grained aggregates. While the\naRTree is more space-saving when compared to the single-point\nindices, it still introduces an order of magnitude higher storage\noverhead than GeoBlocks.\nImpact of Selectivity. Selectivity is usually defined based on\na single query, but in our context, it is hard to specify what a\nsingle query is. We break down query polygons, e.g., the orange\nbordered Lower East Side in Figure 1, to different-sized cells\ncovering the polygon (e.g., the purple cell), which in turn are\nbroken down into equally sized cells to query (blue cells). While\nthe intermediate cells of the query polygonâ€™s covering are the\nbest representation of individual queries, as each index is probed\nonce for them, they are artificial concepts introduced by our\nalgorithm. Furthermore, these are hard to map to the rectangular\nquery regions of the PHTree and the aRTree. Therefore, we define\nselectivity based on query polygons. For this experiment, we\nartificially select polygons covering a part of NYC, which contains\na certain percentage of the total rides. Figure 12 reports the\nruntime of the base workload at different selectivities using a\nlogarithmic scale. PHTreeâ€™s and aRTreeâ€™s measured selectivities\ndiffer slightly from the reported ones due to the less precise\ncovering using an interior rectangle. As this covering contains\nfewer points, this should slightly skew the experiment in favor\nof the PHTree and aRTree. Even though GeoBlocks can handle\nrectangular queries as well, since rectangles are just constrained\npolygons, we opted for the most-precise covering where possible.\nWhile runtime rises quickly for all baselines for selectivities\nabove 1%, the increase is much softer for both Block variants.\nEven though the workload is not skewed, and we only use 2%\nof additional storage for query caching, BlockQCstill outper-\nforms the non-caching Block across all selectivities. This is likely\nexplained by the shape of the polygons that are often simple\nquadrilaterals or pentagons. These can be covered using few cells\nand, therefore, most of these cells can be pre-aggregated. Binary-\nSearch can keep up with the BTree, reporting similar runtimes\nBetter\n0%20%40%60%\n1 10 20 30 40 50 60 70 80 90 100\nNumber of Points (in millions)Relative OverheadBlock\nBTree\nPHTree(a) Size overhead of GeoBlocks\nand baselines.\nBetter\n0255075100\n1 10 20 30 40 50 60 70 80 90 100\nNumber of Points (in millions)Runtime IncreaseBinarySearch\nBlock\nBTree\nPHTree(b) Relative runtime increase\nof GeoBlocks and competitors\ncompared to 1M points.\nFigure 13: Scaling with increasing input sizes.\nindependent of selectivity, while the PHTree lags behind quickly.\nEven if the relative runtime gap narrows for higher selectivity,\nthe absolute gap still favors GeoBlocks. The aRTree, our imple-\nmentation of the aR-tree, outperforms the on-the-fly aggregating\nbenchmarks easily while staying behind GeoBlocks for lower\nselectivities. However, it can catch up with Block at around 50%\nselectivity. At 100% selectivity, the aRTree needs to only access\nthe root aggregate, explaining the sharp drop in runtime. Overall,\nGeoBlocks outperform the non-aggregating baselines by at least\ntwo and up to three orders of magnitude, performing on-par with\nthe aR-tree while delivering far more precise results.\nScalability. To study the performance for different-sized datasets,\nwe collect 100M taxi rides spanning all of 2015 and build and\nquery the approaches for an increasing subset of these rides. We\nomit the aRTree as the build time exceeded reasonable limits up-\nward of 30 million points. As the build time is dominated by the\nsorting process, which is shared and identical in all approaches,\nthey scale identically in build time. When comparing the size\noverhead in Figure 13a, we can see that the BTree overhead is\nconstant as expected. For the PHTree, we see the positive impact\nof the integrated compression strategies for bigger datasets. Still,\nthe near fixed-size grid aggregates - the size of a GeoBlock is\ndetermined by the spatial distribution of points, not their num-\nber - enables even smaller overheads for GeoBlocks. To focus\non the individual scalability for queries, we analyze the query\nruntime normalized to the runtime of each approach for one\nmillion points. As shown in Figure 13b, both the BTree and the\nBinarySearch scale linearly with the input size, as the on-the-fly\naggregation dominates the runtime. We expect a similar behav-\nior from the PHTree, but as the covering is less accurate and\nchosen deliberately smaller, the increase is not fully linear. For\nGeoBlocks, the runtime stays nearly constant, since it depends\non the number of maintained aggregates, and not on the number\nof individual points. The number of aggregates is in turn deter-\nmined by the spatial distribution of the input. Since one million\n\nNYC TaxiUSA TweetsOSM Americas\n0 2 4 0 1 2 0 50 1000%20%40%60%\nRuntime (in s)Relative ErrorAlgorithm BinarySearch Block BTree PHTree aRTreeFigure 14: Query runtime and relative error for varying\ndatasets.\nStatesRectangles\n0 10 20 30 0 100 200 30025%50%75%\nAverage Runtime (in ms)Average Relative ErrorAlgorithm\nBinarySearch\nBlock\nBTree \nPHTree \naRTree\nFigure 15: Query runtime and relative error for US states\nand generated rectangles on the Twitter dataset.\npoints already cover most areas in NYC, the distribution does\nnot change when further increasing the number of points, i.e.,\nthe number of aggregates does not increase significantly. This\nexplains why query latency remains nearly constant for bigger\ndatasets.\nDatasets. To show that our approach is not limited to the NYC\ntaxi dataset, we evaluate it on the two additional datasets in\nFigure 14. We again query the whole area represented by the\nindividual polygons and report runtime, as well as the average\nerror defined as|# tuples in query result âˆ’# tuples in polygon |\n# tuples in polygon. For the\nOSM dataset, the aRTree again was excluded because of its ex-\ncessive build time. As the Block, BinarySearch, and BTree use\nthe same covering, the result and error are identical. While the\naRtree and PHTree use an identical rectangular representation,\nthe pre-aggregated nodes of the aRTree lead to a different result,\nand therefore error. Overall, the aRTree and Block are similarly\nfast with a slight advantage for the aRTree, outperforming the\nnon-aggregating approaches easily. However, the error for Block\nis far more stable.\nAccuracy. Finally, we want to study the influence of smaller\nindividual polygons, as well as rectangular areas, on both run-\ntime and relative error. Therefore, we query all US states and\n51 randomly generated rectangles within the US on the Twitter\ndataset and report the average runtime and error in Figure 15.\nIn contrast to the previous experiment, we query all areas in-\ndividually. For both polygons and rectangles, the same overall\ntrends are visible. The aRTree is slightly faster than Blocks as\nthe large polygons can be answered in the upper levels of the\ntree. However, this leads to high imprecision even for rectangular\nqueries as partially overlapping internal nodes might be counted\nmultiple times. Besides, we see that the individual errors canceled\nout in Figure 14, leading to a seemingly good error bound. While\nthe PHTree error also improves considerably for the rectangular\nworkload, we expected it to be exact. We suspect this is caused\nby our transformation of the coordinates to integer space, which\nis necessary for efficient queries. As expected, the performance\n13\n14\n15\n16\n1718 19 20 21\n0%5%10%15%20%\n0 1000 2000\nRuntime in Î¼sRelative ErrorBlock Level\n13\n14\n15\n16\n1718\n19\n20\n21Figure 16: Relative error and runtime at varying levels.\nLevel 13 14 15 16 17 18 19 20 21\nSorting 6020 6008 6317 6459 6633 6754 7028 7344 7666\nBuilding 376 499 376 356 411 408 538 666 1025\nTable 2: Index build times in ms at varying levels.\nof Blocks and the other approximating baselines does not de-\ngrade for rectangular areas. The aggregating approaches again\nfar outperform the point indexing approaches in runtime.\n4.3 Sensitivity Analysis\nAfter showing that GeoBlocks easily outperform all baselines,\nwe study the impact that the configuration of GeoBlocks has on\nthroughput, as well as the impact of data skew on the adaptive\nBlock version. The Block configuration is specified by three pa-\nrameters: The first setting we study is the level of the Block, i.e.,\nthe resolution of the grid overlying the spatial domain. Next, we\ntake a look at the impact of skew on both Block and BlockQC.\nFinally, we examine how the size of the AggregateTrie influences\nthe runtime of unskewed and skewed workloads.\nImpact of Block Level. We vary the block levels from 13 to\n21 (between ~1.5km and ~6m diagonal) while keeping the other\nconfiguration parameters fixed. From a runtime-only point of\nview, lower-level (coarser-grained) blocks are always preferable,\nas the query algorithm needs to take fewer cells into account.\nHowever, this comes at the price of precision loss. Figure 16 il-\nlustrates the connection between the block level, the runtime,\nand the relative error introduced by the cell covering. The cell\ncovering can introduce only false positive results, i.e., some re-\nported results are not contained in the actual polygon. The figure\nclearly shows the expected overall trend: the higher the level,\nthe lower the relative error and the higher the runtime. However,\nafter a certain point, decreasing the level further does not pay off.\nFurther, we see that the correlation between error and runtime is\nnot linear, as we already suspected in Section 3.2. The correlation\ndoes not even follow the discussed influences completely, which\nis likely caused by missing sparse children, and the non-uniform\ndistribution of points leading to a gap between the relative error\nand the configurable spatial error.\nThe block level influences not only the relative error and the\nruntime, but also the build time and size of GeoBlocks. Figure 11c\ndepicts the build time and size overhead for GeoBlocks from\nlevels 13 to 21. The build time seems to be only slightly affected\nby the level, rising slowly with it. Table 2 splits the runtime into\ntwo parts: sorting and building. There is a noticeable increase\nin sort time along with the block level, in addition to the ex-\npected increase in build time. This increase in sorting can be\nexplained through our grid cell extraction that we piggybacked\nto the sorting process, which has to extract more finer-grained\n\n1.2x2 skewed runs4 skewed runs8 skewed runs16 skewed runs\nBlock BlockQCBlock BlockQCBlock BlockQCBlock BlockQC0306090Runtime in msWorkload\nSkewed\nBaseFigure 17: Query runtime with increasing workload skew.\n010203040\n0%25%50%75%100%\n0 % 25 % 50 % 75 % 100 %\nAggregate ThresholdRuntime in msCache Hit Rate Aspect Cache Hit Rate Runtime\nWorkload Base Skewed\nApproach Block BlockQC\nFigure 18: Impact of threshold on workload runtime (solid\nline) and cache hit rate (dashed line).\ncells. The size overhead, however, grows exponentially due to\nthe exponentially growing number of cells along with the level.\nImpact of Skew. To study the impact of data skew on the effec-\ntiveness of query caching, we measure the query runtime when\nrunning the NYC workload once, and the skewed workload mul-\ntiple times. The number of times we run the skewed workload\nvaries in each experiment. We fix the block level to 17 (~100m diag-\nonal) and the size of the cache to 5% of the cell aggregates, which\nroughly corresponds to aggregating all cells of the skewed work-\nload. Figure 17 displays the absolute runtime for both the base\nand the skewed part of the workload. One can see that after four\nskewed runs, the cached aggregates start to pay off. With even\nmore skew in the total workload, our query-caching BlockQC\nquickly starts to outperform Block. Furthermore, as expected,\nthe runtime for the base workload stays nearly constant, and is\nalways slightly faster for Block. This is easily explained by the\noverhead of probing the AggregateTrie for each cell, regardless\nof whether the cell is aggregated or not.\nImpact of Aggregate Threshold. Having studied the impact\nof skew, we want to examine how the aggregate threshold, and\nthereby the size of the query cache (in BlockQC), influences the\nruntime of the base and the skewed workload. The aggregate\nthreshold denotes the relative size overhead that the query cache,\nthe AggregateTrie, introduces compared to the size of the cell\naggregates in the regular GeoBlock. We again fix the block level\nto 17, and the number of skewed runs to four. Figure 18 depicts\nthe measured runtimes and cache hit rates. The runtime of Block\nis unaffected by the changed threshold and only acts as a baseline\nto highlight the influence on BlockQC. Up until a threshold of\naround 5%, only queries from the skewed workload can be an-\nswered using the AggregateTrie. The small speedup in the base\nworkload can be explained by the inclusion of the skewed work-\nload in the base workload. Once all cells in the skewed workload\nare cached, and the cache hit rate for the skewed part reaches\n100%, other query cells of the base workload start to get cached\n5101520\ndistance >= 4 passenger_cnt == 1 passenger_cnt > 1\nFilter PredicatePayoï¬€pointBlock Level\n15\n16\n17\n18\n19\n1st25th50th75th99thFigure 19: Payoff point: Number of incremental builds re-\nquired to amortize the cost of sorting the raw data.\nas well. While this, of course, leads to further runtime improve-\nments, it is undesirable, especially when memory is scarce. In our\nexperiments, at around 50%, the cache hit rate reaches 100% for\nboth workloads, and there is no further speedup, even when the\ncache size is doubled. The cache hit rate, illustrated by the dashed\nline and shown on the right axis, shows the desired effect. The\nskewed part is cached almost immediately, and the hit rate for\nthe unskewed workload grows linear with increasing cache size.\nThe average lookup time slowly grows from 58ns at 1% to 81ns\nat 100%. As the lookup time depends on the number of levels (30\nin the maximum) and not on the size, this growth is attributable\nto more complex access patterns for larger cache trees.\n4.4 Changing Filters\nFinally, we compare our process of Figure 5, wherein we build\nmultiple GeoBlocks from the sorted base data, against building\nisolated GeoBlocks from scratch. We vary the block level from 15\nto 19 (between ~420m and ~27m diagonal), and build 15 GeoBlocks\nper level using three different predicates of varying selectivity:\nâ€¢distance >= 4: Long taxi trips, selectivity of ~16%\nâ€¢passenger_cnt == 1: Solo taxi trips, selectivity of ~70%\nâ€¢passenger_cnt > 1: Shared taxi trips, selectivity of ~30%\nFor this, we want to analyze how many different filter and\nlevel combinations are required to amortize the initial cost of\nsorting. Figure 19 shows the payoff point of filter changes for\nour three filter predicates. The payoff point is the number of\nincremental builds required to be, in sum, faster by creating\nincremental builds than building individual GeoBlocks from the\nraw data and filtering before sorting. We omitted the individual\nruntimes for the passenger_cnt == 1 predicate as they would\nbe too densely packed vertically.\nAs expected, the more selective the filter, the lower the speedup.\nOnce all tuples in the raw data have been filtered according to the\npredicate, the qualifying tuples have to be sorted. More selective\npredicates take longer to amortize as sorting few tuples is cheap ,\nwhereas for the 70% selectivity query passenger_cnt == 1 , the\nmore expensive sorting is amortized almost immediately. There\nis a correlation between the block level and amortization, most\nnotably for the most selective predicate distance >= 4 . Given\nthat the payoff point drastically rises with lower selectivity, we\nexpect that incremental builds will only pay off when the new\nfilters are less selective. If only a few highly selective queries are\nexpected, building regular GeoBlocks directly from the raw data\nwill still be the fastest option. However, the time to switch to a\nnew filter, and therefore the individual query latency, will always\nbe lower for incremental builds.\n\n5 DISCUSSION\nIn this section, we discuss the takeaways of the evaluation as\nwell as updates for GeoBlocks.\nEvaluation Summary. First, we showed that pre-aggregation\nin a spatial context pays off when a limited and bounded spatial\nerror is acceptable, independently of the number of aggregates\nqueried and the selectivity of the query polygons. Furthermore,\nGeoBlocks can be built fast, introducing only a small overhead\ncompared to the simple BinarySearch baseline. Even when the\ndata is already indexed with one of our baselines (i.e., without\ntaking the index build time into account), GeoBlockâ€™s build time\nof around 7 seconds can be amortized by fewer than 30 poly-\ngon queries with a selectivity of 10% (cf. Figures 11a and 12). In\naddition, building multiple GeoBlocks once the data is sorted is\npossible within one second for our dataset, cf. Figure 11a. Build-\ning new GeoBlocks for different filters is even faster when using\nsorted base data, often amortizing the initial extra cost of sorting\nall data in less than 10 filter changes (cf. Figure 19). Even though\nnot all configurations are optimal for GeoBlocks, there are ac-\nceptable error-runtime trade-offs, in our case around levels 17\nand 18. While the level does not significantly impact the index\nbuild time, the size overhead growth is almost exponential, cf.\nFigure 11c, indicating that it is wise to think about which error is\nacceptable for the given query workload when memory is scarce.\nUpdates. Up until now, we considered GeoBlocks to be read-only\nas they are designed for historical point data. However, the layout\nof GeoBlocks allows us to integrate updates easily, as long as a cell\naggregate for the region of the newly arriving tuple already exists.\nFor the non-adaptive version, all we have to do is locate the cell\naggregate containing the tuple and update all stored aggregates.\nIn the adaptive version, we additionally need to update all cached\nparents of the grid cell in the AggregateTrie as well. Thanks to\nthe prefix-based indexing property of the trie, we can do this\nin a single depth-first traversal. Only if tuples arrive for a new,\npreviously unaggregated region, we have to rebuild the aggregate\nlayout, as we rely on the cell aggregates to be sorted. However, as\nwe have shown in the evaluation, recalculating the cell aggregates\nis often possible within a second, so this operation would not\ninduce too much delay when updates are implemented in batches\ninstead of single tuples. Other indexing approaches on the cell\naggregates (e.g., a clustered B-tree) could eliminate the need to\nrebuild by reserving storage for new aggregates. Preliminary\nexperiments using std::map and a B-tree as an index showed\nsimilar lookup performance at the cost of increased size overhead.\n6 RELATED WORK\nOur approach builds on seminal work from decades of research\non spatial indexing. Decomposing space into hierarchical grid\ncells [ 1,9,39], as well as approximating polygons using simpler\nshapes [ 18], are all well-known approaches. Likewise, enumerat-\ning cells using a space-filling curve such as Hilbert or Z order [ 26,\n27] and storing aggregate information within cells [ 20,30,46] are\nideas that have been around for some time. However, while build-\ning on these established concepts, GeoBlocks present the first\npre-aggregating data structure that supports a bounded, distance-\nbased error on the results of polygonal queries . Specifically, prior\nwork on pre-aggregation [ 14,30,31,34] is limited to rectangular\nqueries and requires an expensive post-processing (refinement)\nstep to answer polygonal queries. GeoBlocks, on the other hand,\nyield error-bounded results and do not require expensive refine-\nment.Spatial Aggregation. Past work has proposed several approach-\nes for spatial aggregation queries [ 23]. These approaches mainly\nrely on pre-aggregation [ 14,34]: they pre-aggregate records at\nvarious spatial resolutions and store this summarized information\nin a hierarchy of rectangular regions, maintained using a spatial\nindex like the quadtree or the R-tree [ 19,30â€“32]. For instance, the\naRtree [ 30,31] enhances the R-tree by storing aggregate informa-\ntion for each node. This allows to directly extract the aggregate\nof all the records contained in a node, if the nodeâ€™s MBR is fully\nenclosed in the query region. Being a variant of the R-tree, the\naRtree constrains the supported queries to only rectangular re-\ngions. Furthermore, the computed aggregates are approximate\nand the error cannot be bounded , since the accuracy depends on\nthe resolution of the rectangular R-tree nodes. Providing preci-\nsion guarantees for arbitrary polygons requires accessing the\nraw data and involves additional processing. There are also ap-\nproaches that store aggregates inside a data cube [ 6,37], or using\nsketches [ 48]. Nanocubes [ 21], for example, store the CUBE op-\nerator for spatio-temporal datasets, and are specifically designed\nfor visualization systems. The data cube-based approaches suffer\nfrom the same limitations as the aRtree, since they also rely on a\nhierarchy of rectangular regions. Besides, accessing the raw data\nto refine the aggregates might require additional indices, as the\ncube does not store individual records. Vorona et al. [ 55] approxi-\nmate the distribution of geospatial points with an autoregressive\ndeep learning model to answer arbitrary polygonal queries, but\nthey cannot provide any error bounds. Pandey et al. [ 29] pro-\npose to use learned indices for query-efficient spatial indexing,\nalbeit limited to range queries. Finally, Raster Join [ 51] uses GPU\nrendering to compute aggregates over a point-polygon join. In\ncontrast, GeoBlocks support aggregation over spatial selections.\nPrefix sums [ 10] can be used in addition to pre-aggregation\nto enable fast range-sums. This is achieved by only inspecting\nthe aggregates in the two corners of a query region, rather than\nevery aggregate inside the query region. An example of this is\nourCOUNT algorithm. However, in contrast to our SELECT queries,\nthese range-sums are unable to extract min and max aggregates.\nMaterialized Views and OLAP Cubes. GeoBlocks are essen-\ntially materialized views over geospatial data with support for\nfilters and aggregations. In contrast to regular views [ 12,44], Geo-\nBlocks are designed for historical spatial data and can adapt to\nthe query workload at a fine-grained level using a trie-like cache.\nWork on materialized view selection [ 2] also makes materializa-\ntion decisions based on the query workload, but at a much coarser\ngranularity (e.g., what columns to aggregate). There has also been\na lot of work on data cubes and query caching [ 11,15,42], but\nthese do not support geospatial data as a first-class citizen.\nSpatial Point Indexing. Spatial point indexing approaches typ-\nically index points using a hierarchy of MBRs, most notably the\nR-tree [ 13], or by subdividing grid cells into equally-sized chil-\ndren, e.g., the quadtree [ 9,39]. Both of these index structures\nare queried using the dimension-wise min/max values, i.e., the\nquery regions are rectangular. Other approaches, like the UB-\ntree [ 3], assign univariate keys to the indexed regions first and\nrely on these keys for data access. While the UB-tree does not\nspecify how these keys have to generated, most approaches use\nspace-filling curves like the Z order [26, 27].\nBased on these concepts, more specialized indices have been\ndeveloped. The PH-tree [ 56] combines a quadtree with hyper-\ncubes to allow splitting all dimensions in each node, providing\na space-efficient index structure for multidimensional data. The\nspace efficiency can be partly attributed to the utilization of prefix\n\nsharing, similar to the one used in our trie-like cache. Alternating\nthe indexed dimensions in an in-memory tree structure, the BB-\ntree [ 47] offers fast point and range queries for multidimensional\ndata. While these structures require the index to be built a priori,\nthere are others like QUASII [ 33], where the index is built incre-\nmentally as a side product of query execution. As a result, QUASII\ncan adapt to the query workload at runtime. However, QUASII\nonly supports spatial range (window) queries. Recently, Shin et\nal. [43] proposed integrating grid indices into a tree structure to\nachieve faster node accesses and point operations.\n7 CONCLUSIONS\nWe have introduced GeoBlocks, a novel pre-aggregating data\nstructure for geospatial data. GeoBlocks pre-compute aggregates\nover fine-grained grid cells, thereby supporting arbitrarily shaped\npolygons. Using these aggregates, GeoBlocks can provide fast\nquery results with a user-controlled spatial error. Furthermore,\nGeoBlocks can speed up aggregate queries for commonly queried\nregions by dynamically adapting to any given workload using\nlimited additional storage.\nComparing GeoBlocks with on-the-fly aggregating indexing\nbaselines, we have shown that we can outperform them for any\nnumber of aggregates, in parts by three orders of magnitude. The\nintroduced storage overhead is comparable, and often even lower,\nto that of traditional indexing structures, while GeoBlocks can be\nbuilt equally fast. Looking at GeoBlocksâ€™ configuration options,\nwe have shown how they can be adapted to the given dataset and\nworkload, and how they influence the runtime, the overhead, and\nthe error in the result. Overall, GeoBlocks are materialized views\nover geospatial data that support filter predicates and aggregates\nwhile enabling fine-grained adaptation to the query workload.\nREFERENCES\n[1]W. G. Aref and H. Samet. Efficient processing of window queries in the\npyramid data structure. In PODS , pages 265â€“272. ACM Press, 1990.\n[2]E. Baralis, S. Paraboschi, and E. Teniente. Materialized views selection in a\nmultidimensional database. In VLDB , pages 156â€“165, 1997.\n[3]R. Bayer. The universal b-tree for multidimensional indexing: general concepts.\nInWWCA , pages 198â€“209. Springer, 1997.\n[4]How to analyse bike data for urban planning? https://www.bikecitizens.net/\nanalyse-bike-data-urban-planning/.\n[5]Boost R-tree . https://www.boost.org/doc/libs/1_69_0/libs/geometry/doc/html/\ngeometry/reference/spatial_indexes/boost__geometry__index__rtree.html.\n[6]F. Braz, S. Orlando, R. Orsini, A. RaffaetÃ , A. Roncato, and C. Silvestri. Ap-\nproximate aggregations in trajectory data warehouses. In ICDE Workshops ,\npages 536â€“545, 2007.\n[7] Google code archive. https://code.google.com/archive/p/cpp-btree/.\n[8]N. Ferreira, M. Lage, H. Doraiswamy, H. Vo, L. Wilson, H. Werner, M. Park,\nand C. Silva. Urbane: A 3d framework to support data driven decision making\nin urban development. In Proc. IEEE VAST , pages 97â€“104, 2015.\n[9]R. A. Finkel and J. L. Bentley. Quad trees: A data structure for retrieval on\ncomposite keys. Acta Informatica , 4:1â€“9, 1974.\n[10] S. Geffner, D. Agrawal, A. E. Abbadi, and T. R. Smith. Relative prefix sums:\nAn efficient approach for querying dynamic OLAP data cubes. In ICDE , pages\n328â€“335, 1999.\n[11] J. Gray, S. Chaudhuri, A. Bosworth, A. Layman, D. Reichart, M. Venkatrao,\nF. Pellow, and H. Pirahesh. Data cube: A relational aggregation operator\ngeneralizing group-by, cross-tab, and sub totals. Data Min. Knowl. Discov. ,\n1(1):29â€“53, 1997.\n[12] H. Gupta. Selection of views to materialize in a data warehouse. In ICDT ,\npages 98â€“112. Springer, 1997.\n[13] A. Guttman. R-trees: A dynamic index structure for spatial searching. In\nSIGMOD , pages 47â€“57. ACM Press, 1984.\n[14] J. Han, N. Stefanovic, and K. Koperski. Selective materialization: An efficient\nmethod for spatial data cube construction. In PAKDD , pages 144â€“158. Springer,\n1998.\n[15] V. Harinarayan, A. Rajaraman, and J. D. Ullman. Implementing data cubes\nefficiently. In SIGMOD , pages 205â€“216. ACM Press, 1996.\n[16] A. Kipf, H. Lang, V. Pandey, R. A. Persa, C. Anneser, E. Tzirita Zacharatou,\nH. Doraiswamy, P. A. Boncz, T. Neumann, and A. Kemper. Adaptive main-\nmemory indexing for high-performance point-polygon joins. In EDBT , pages\n347â€“358, 2020.[17] A. Kipf, H. Lang, V. Pandey, R. A. Persa, P. A. Boncz, T. Neumann, and A. Kem-\nper. Approximate geospatial joins with precision guarantees. In ICDE , pages\n1360â€“1363. IEEE Computer Society, 2018.\n[18] H. Kriegel, H. Horn, and M. Schiwietz. The performance of object decom-\nposition techniques for spatial query processing. In SSD, volume 525, pages\n257â€“276. Springer, 1991.\n[19] I. Lazaridis and S. Mehrotra. Progressive approximate aggregate queries with\na multi-resolution tree structure. In SIGMOD , pages 401â€“412. ACM, 2001.\n[20] I. Lazaridis and S. Mehrotra. Multi-resolution aggregate tree. In Encyclopedia\nof GIS , pages 764â€“765. Springer, 2008.\n[21] L. D. Lins, J. T. Klosowski, and C. E. Scheidegger. Nanocubes for real-time\nexploration of spatiotemporal datasets. IEEE Trans. Vis. Comput. Graph. ,\n19(12):2456â€“2465, 2013.\n[22] Z. Liu and J. Heer. The Effects of Interactive Latency on Exploratory Visual\nAnalysis. Proc. TVCG , 20(12):2122â€“2131, 2014.\n[23] I. F. V. LÃ³pez, R. T. Snodgrass, and B. Moon. Spatiotemporal aggregate compu-\ntation: a survey. IEEE Trans. Knowl. Data Eng. , 17(2):271â€“286, 2005.\n[24] F. Nagel, P. A. Boncz, and S. Viglas. Recycling in pipelined query evaluation.\nInICDE , pages 338â€“349, 2013.\n[25] NYC neighborhoods. https://data.cityofnewyork.us/City-Government/\nNeighborhood-Tabulation-Areas/cpf4-rkhq.\n[26] J. A. Orenstein. Spatial query processing in an object-oriented database system.\nInSIGMOD Conference , pages 326â€“336. ACM Press, 1986.\n[27] J. A. Orenstein and T. H. Merrett. A class of data structures for associative\nsearching. In PODS , pages 181â€“190. ACM, 1984.\n[28] V. Pandey, A. Kipf, T. Neumann, and A. Kemper. How good are modern spatial\nanalytics systems? PVLDB , 11(11):1661â€“1673, 2018.\n[29] V. Pandey, A. van Renen, A. Kipf, J. Ding, I. Sabek, and A. Kemper. The case\nfor learned spatial indexes. In AIDB@VLDB , 2020.\n[30] D. Papadias, P. Kalnis, J. Zhang, and Y. Tao. Efficient OLAP operations in\nspatial data warehouses. In SSTD , pages 443â€“459. Springer, 2001.\n[31] D. Papadias, Y. Tao, P. Kalnis, and J. Zhang. Indexing spatio-temporal data\nwarehouses. In ICDE , pages 166â€“175, 2002.\n[32] D. Papadias, Y. Tao, J. Zhang, N. Mamoulis, Q. Shen, and J. Sun. Indexing and\nretrieval of historical aggregate information about moving objects. IEEE Data\nEng. Bull. , 25(2):10â€“17, 2002.\n[33] M. Pavlovic, D. Sidlauskas, T. Heinis, and A. Ailamaki. QUASII: query-aware\nspatial incremental index. In EDBT , pages 325â€“336, 2018.\n[34] T. B. Pedersen and N. Tryfona. Pre-aggregation in spatial data warehouses.\nInSSTD , pages 460â€“480. Springer, 2001.\n[35] T. Phan and W. Li. Dynamic materialization of query views for data warehouse\nworkloads. In ICDE , pages 436â€“445, 2008.\n[36] mcxme/phtree . https://github.com/mcxme/phtree.\n[37] F. Rao, L. Zhang, X. Yu, Y. Li, and Y. Chen. Spatial hierarchy and olap-favored\nsearch in spatial data warehouse. In DOLAP , pages 48â€“55. ACM, 2003.\n[38] S2 geometry. https://s2geometry.io/.\n[39] H. Samet. The quadtree and related hierarchical data structures. ACM Comput.\nSurv. , 16(2):187â€“260, 1984.\n[40] T. Schneider. Analyzing 1.1 Billion NYC Taxi and Uber Trips, with a\nVengeance . https://toddwschneider.com/posts/analyzing-1-1-billion-nyc-taxi-\nand-uber-trips-with-a-vengeance/.\n[41] V. Shah. Citi Bike 2017 Analysis - Towards Data Science . https://\ntowardsdatascience.com/citi-bike-2017-analysis-efd298e6c22c.\n[42] J. Shim, P. Scheuermann, and R. Vingralek. Dynamic caching of query results\nfor decision support systems. In SSDBM , pages 254â€“263, 1999.\n[43] J. Shin, A. R. Mahmood, and W. G. Aref. An investigation of grid-enabled tree\nindexes for spatial query processing. In SIGSPATIAL , pages 169â€“178, 2019.\n[44] O. Shmueli and A. Itai. Maintenance of views. In SIGMOD , pages 240â€“255.\nACM Press, 1984.\n[45] A. Shukla, P. Deshpande, and J. F. Naughton. Materialized view selection for\nmultidimensional datasets. In VLDB , pages 488â€“499, 1998.\n[46] S. Singla, A. Eldawy, R. Alghamdi, and M. F. Mokbel. Raptor: Large scale\nanalysis of big raster and vector data. PVLDB , 12(12):1950â€“1953, 2019.\n[47] S. Sprenger, P. SchÃ¤fer, and U. Leser. Bb-tree: A main-memory index structure\nfor multidimensional range queries. In ICDE , pages 1566â€“1569, 2019.\n[48] Y. Tao, G. Kollios, J. Considine, F. Li, and D. Papadias. Spatio-temporal aggre-\ngation using sketches. In ICDE , pages 214â€“225, 2004.\n[49] Nyc tlc data. https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page.\n[50] TNCs TODAY . http://tncstoday.sfcta.org/.\n[51] E. Tzirita Zacharatou, H. Doraiswamy, A. Ailamaki, C. T. Silva, and J. Freire.\nGPU rasterization for real-time spatial aggregation over arbitrary polygons.\nPVLDB , 11(3):352â€“365, 2017.\n[52] E. Tzirita Zacharatou, A. Kipf, I. Sabek, V. Pandey, H. Doraiswamy, and\nV. Markl. The case for distance-bounded spatial approximations. In CIDR .\nhttp://cidrdb.org, 2021.\n[53] Uber Movement . https://movement.uber.com/.\n[54] F. van Diggelen and P. Enge. The worldâ€™s first GPS MOOC and worldwide\nlaboratory using smartphones. In Proc. ION GNSS+ , pages 361â€“369, 2015.\n[55] D. Vorona, A. Kipf, T. Neumann, and A. Kemper. DeepSPACE: Approximate\ngeospatial query processing with deep learning. In SIGSPATIAL/GIS , pages\n500â€“503. ACM, 2019.\n[56] T. ZÃ¤schke, C. Zimmerli, and M. C. Norrie. The ph-tree: a space-efficient\nstorage structure and multi-dimensional index. In SIGMOD , pages 397â€“408.\nACM, 2014.",
  "textLength": 78309
}