{
  "paperId": "73909f2f5ee7823f0b0fe1fb028d7b2de1085aad",
  "title": "Multi-Attribute Selectivity Estimation Using Deep Learning",
  "pdfPath": "73909f2f5ee7823f0b0fe1fb028d7b2de1085aad.pdf",
  "text": "Multi-Attribute Selectivity Estimation Using Deep\nLearning\nShohedul Hasanz, Saravanan Thirumuruganathanzy, Jees Augustinez, Nick Koudaszz, Gautam Dasz\nzUniversity of Texas at Arlington;zyQCRI, HBKU;zzUniversity of Toronto\nzfshohedul.hasan@mavs, jees.augustine@mavs, gdas@cse g.uta.edu,zysthirumuruganathan@hbku.edu.qa,zzkoudas@cs.toronto.edu\nAbstract —Selectivity estimation – the problem of estimating\nthe result size of queries – is a fundamental problem in\ndatabases. Accurate estimation of query selectivity involving\nmultiple correlated attributes is especially challenging. Poor\ncardinality estimates could result in the selection of bad plans\nby the query optimizer. We investigate the feasibility of using\ndeep learning based approaches for both point and range queries\nand propose two complementary approaches. Our ﬁrst approach\nconsiders selectivity as an unsupervised deep density estimation\nproblem. We successfully introduce techniques from neural\ndensity estimation for this purpose. The key idea is to decompose\nthe joint distribution into a set of tractable conditional probability\ndistributions such that they satisfy the autoregressive property.\nOur second approach formulates selectivity estimation as a\nsupervised deep learning problem that predicts the selectivity of a\ngiven query. We also introduce and address a number of practical\nchallenges arising when adapting deep learning for relational\ndata. These include query/data featurization, incorporating query\nworkload information in a deep learning framework and the\ndynamic scenario where both data and workload queries could\nbe updated. Our extensive experiments with a special emphasis\non queries with a large number of predicates and/or small result\nsizes demonstrates that our proposed techniques provide fast and\naccurate selective estimates with minimal space overhead.\nI. I NTRODUCTION\nSelectivity estimation – the problem of estimating the result\nsize of queries with multiple predicates – is a fundamental yet\nchallenging problem in databases. It has diverse applications in\nquery optimization, query proﬁling, database tuning, approx-\nimate query processing etc. Poor cardinality estimates could\nresult in the selection of bad plans by the query optimizer [24].\nDue to its importance, this problem has attracted intense\ninterest from the database community.\nCurrent Approaches and their Limitations. Accurate es-\ntimation of query selectivity involving multiple (correlated)\nattributes is especially challenging. Exactly representing the\njoint distribution is often infeasible when many attributes\nare involved or each attribute could take large number of\nvalues. Broadly speaking, major database systems tackle this\nproblem by approximating this joint distribution via synopses\nor sampling. Synopsis techniques such as histograms approx-\nimate the joint frequency distribution in a bounded space by\nmaking assumptions such as uniformity and attribute value\nindependence [34], [24]. These assumptions are often violated\nin real-world datasets resulting in large errors in selectivity\nestimation [24]. Building multidimensional histograms couldpartially ameliorate this issue but often has substantial space\nrequirements. Sampling based approaches could handle at-\ntribute dependencies and correlations more effectively. How-\never, it is not a panacea – for queries with low selectivity,\nthe optimizer could be made to rely on magic constants [24],\nresulting in poor estimates.\nA. Outline of Technical Results\nIn this paper, we investigate the suitability of Deep Learn-\ning for selectivity estimation. Building a DL model that is\nlightweight, fast to train and estimate, and optionally allow\ninjection of domain knowledge such as query workload is\nnon trivial. We propose two complementary approaches that\noperate in two phases. In the ofﬂine phase, we train an\nappropriate DL model from the data. During the online phase,\nthe model accepts a query and outputs its selectivity.\nSelectivity Estimation as Unsupervised Learning. Our ﬁrst\napproach models selectivity estimation as a density estimation\nproblem where one seeks to estimate the joint probability\ndistribution from a ﬁnite number of samples. Intuitively, the\ntraditional sampling and synopses approaches can be con-\nsidered as approximate non-parameteric density estimators.\nHowever, instead of directly estimating the joint probabil-\nity, we seek to decompose it into a series of simpler and\ntractable conditional probability distributions. Speciﬁcally, we\nconsider a speciﬁc decomposition with autoregressive property\n(formally deﬁned in Section III). We then build a single DL\nmodel to simultaneously learn the parameters for each of the\nconditional distributions.\nSelectivity Estimation as Supervised Learning. We investi-\ngate if, given a training set of queries along with their true\nselectivity, is it possible to build a DL model that accu-\nrately predicts the selectivity of unknown queries involving\nmultiple correlated attributes? Our proposed approach can be\nutilized to quickly train a model without having seen the\ndata! The training set of queries with their true selectivities\ncould be obtained from the query logs. Our DL models are\nlightweight and can provide selectivity estimates for datasets\nin few milliseconds. Our model outperforms other supervised\nestimation techniques such as Multiple Linear Regression and\nSupport Vector Regression that have been applied for the\nrelated problem of query performance prediction [1]. The key\nbeneﬁt factor is the ability of DL models to handle complexarXiv:1903.09999v2  [cs.DB]  17 Jun 2019\n\nnon linear relationships between queries involving correlated\nattributes and their selectivity.\nSummary of Experiments. We conducted an extensive set\nof experiments over two real-world datasets – Census and\nIMDB – that exhibit complex correlation and conditional\nindependence between attributes and have been extensively\nused in prior work [24]. We speciﬁcally focus on queries that\nhave multiple attributes and/or small selectivity. We evaluated\nour supervised and unsupervised DL models on a query\nworkload of 10K queries. Our supervised model was trained\non a training data of 5K queries. Our results demonstrate\nthat DL based approaches provide substantial improvement\n- for a ﬁxed space budget - over prior approaches for multi-\nattribute selectivity estimation which has been historically a\nhighly challenging scenario in database selectivity estimation.\nSummary of Contributions.\n\u000fDeep Learning for Selectivity Estimation. We intro-\nduce an alternate view of database selectivity estimation\nnamely as an neural density estimation problem and\nreport highly promising results. Our algorithms could\nhandle queries with both point and range predicates.\n\u000fMaking the approach suitable for Databases. We\ndescribe adaptations making these models suitable for\nvarious data types, large number of attributes and associ-\nated domain cardinalities, availability of query workload\nand incremental queries and data.\n\u000fExperimental Validation. We conduct extensive ex-\nperiments over real-world datasets establishing that our\napproach provides accurate selectivity estimates for chal-\nlenging queries, including the challenging cases of\nqueries involving large number of attributes.\nPaper Outline. Section II introduces relevant notations. Sec-\ntion III formulates selectivity estimation as an unsupervised\nneural density estimation problem and proposes an algorithm\nbased on autoregressive models. In Section IV, we introduce\nthe problem of selectivity estimation and propose a supervised\nDeep Learning based model for it. Section V describes our\nextensive experiments on real-world datasets, related work in\nSection VI and ﬁnally conclude in Section VII.\nII. P RELIMINARIES\nA. Notations\nLetRbe a relation with ntuples andmattributesA=\nfA1;A2;:::;A mg. The domain of the attribute Aiis given\nbyDom (Ai). We denote the value of attribute Aiof an\narbitrary tuple as t[Ai]. We consider conjunctive queries with\nboth point and range predicates. Point queries are of the form\nAi=aiANDAj=ajAND:::for attributesfAi;Ajg\u0012A\nwhereai2Dom (Ai)andaj2Dom (Aj). Range queries are\nof the form lbi\u0014Ai\u0014ubiANDlbj\u0014Aj\u0014ubjAND:::\nLetqdenote such a conjunctive query while Sel(q)represents\nthe result size. We use the normalized selectivity between [0;1]\nby dividing the result size by n, number of tuples.Performance Measures. Given a query q, let the estimate\nprovided by selectivity estimation algorithm be \\Sel(Q). We\nuseq-error for measuring the quality of estimates. Intuitively,\nq-error describes the factor by which the estimate differs from\ntrue selectivity. This metric is widely used for evaluating\nselectivity estimation approaches [24], [25], [19], [30] and is\nrelevant for applications such as query optimization where the\nrelative ordering is more important [24]. We do not consider\nthe use of relative error due to its asymmetric penalization of\nestimation error [30] that results in models that systematically\nunder-estimate selectivity.\nq-error = max \nSel(Q)\n\\Sel(Q);\\Sel(Q)\nSel(Q)!\n(1)\nB. Selectivity Estimation as Distribution Estimation\nGiven a set of attributes A0=fAi;Aj;:::;g, the normal-\nized selectivity distribution deﬁnes a valid (joint) probability\ndistribution. The selectivity of a query qwithfAi=ai;Aj=\naj;:::gcan be identiﬁed by locating the appropriate entry\nin the joint distribution table. Unfortunately, the number of\nentries in this table increases exponentially in the number of\nattributes and their domain cardinality.\nDistribution estimation is the problem of learning the joint\ndistribution from a set of ﬁnite samples. Often, distribution\nestimators seek to approximate the distribution by making\nsimplifying assumptions. There is a clear trade-off between\naccuracy and space. Storing the entire distribution table pro-\nduces accurate estimates but requires exponential space. On\nthe other hand, heuristics such as attribute value independence\n(A VI) assume that the distributions of individual attributes Ai\nare independent of each other. In this case one needs to only\nstore the individual attribute distributions and compute the\njoint probability as\np(Ai=ai;Aj=aj;:::) =Y\nAk2A0p(Ak=ak)\nOf course, this approach fails for most real-world datasets\nthat exhibit correlated attributes. Most popular selectivity esti-\nmators such as multidimensional histograms, wavelets, kernel\ndensity estimations and samples can be construed as simpliﬁed\nnon-parametric density estimators on their own.\nC. Desiderata for DL Estimator\nGiven that selectivity estimation is just one component\nin the larger query optimization framework, we would like\nto design a model that aids in the identiﬁcation of good\nquery plans. Ideally, the estimator should be able to avoid\nthe unrealistic assumptions of uniformity and attribute value\nindependence (afﬂicting most synopses based approaches) and\nameliorate issues caused by low selectivity queries (afﬂict-\ning sampling based approaches). We would like to decouple\ntraining-accuracy tradeoff. For example, increasing the sample\nsizes improves the accuracy - at the cost of increasing the\nestimation time. If necessary, the estimator could have a\nlarge training time to increase accuracy but should have near\n\nconstant estimation time. We would also like to decouple\nthe space-accuracy tradeoff. Multi-dimensional histograms can\nprovide reasonable estimates in almost constant time - but\nrequire very large space (that grows exponentially to the\nnumber of attributes) for accurate results. In other words, we\nwould like to achieve high accuracy through a lightweight\nmodel. The desired model must be fast to train and given the\nlatency requirements of query optimizer, generate estimates in\nmilliseconds. It must also be able to appropriately model the\ncomplex relationship between queries and their selectivities.\nFinally, it must be able to leverage additional information such\nas query workload and domain knowledge.\nIII. S ELECTIVITY ESTIMATION AS NEURAL DENSITY\nESTIMATION\nWe introduce an alternate view of selectivity estimation\nnamely as an neural density estimation problem. This new\nperspective allows us to leverage the powerful tools from\ndeep learning to get accurate selectivity estimation while also\nraising a number of non-trivial challenges in wrangling these\ntechniques for a relational database setting.\nPrior Approaches and Their Limitations. Past approaches\nto selectivity estimation include formulations as a density\nestimation problem. Sampling based approaches [27] approx-\nimate the density of the dataset Rusing a sample S. For an\nuniform random sample, the normalized selectivity of query\nqis estimated as SelD(q) =SelS(q). This approach is\nmeaningful if the sample is large and representative enough\nwhich is not often the case. A more promising avenue of\nresearch seeks to approximate the density through simpler\ndistributions. Recall from Section II-B that the two extremes\ninvolve storing the entire joint distribution or approximate it\nby assuming attribute value independence requiring\nmY\ni=1jDom (Ai)jandmX\ni=1jDom (Ai)j\nstorage. While the former has perfect accuracy, the latter\ncould provide inaccurate estimates for queries involving cor-\nrelated attributes. One approach investigated by the database\ncommunity uses Bayesian networks (BN) that approximates\nthe joint distribution through a set of conditional proba-\nbility distributions [10], [40]. This approach suffers from\ntwo drawbacks. First, learning the optimal structure of BN\nbased on conditional independence is prohibitively expensive.\nSecond, the conditional probability tables themselves could\nimpose large storage overhead if the attributes have a large\ndomain cardinality and/or are conditionally dependent on other\nattributes with large domain cardinality.\nWe address this using two key ideas: (a) we avoid the\nexpensive conditional independence decomposition using a\nsimpler autoregressive decomposition; (b) instead of storing\nthe conditional probability tables, we learn them. Neural\nnetworks are universal function approximators [11] and we\nleverage their powerful learning capacity to model the condi-\ntional distributions in a concise and accurate way.A. Density Estimation via Autoregressive Decomposition\nThe fundamental challenge is to construct density estimators\nthat are expressive enough to model complex distributions\nwhile still being tractable and efﬁcient to train. In this paper,\nwe focus on autoregressive models [11] that satisfy these\nproperties. Given a speciﬁc ordering of attributes , autoregres-\nsive models decompose the joint distribution into multiple\nconditional distributions that are combined via the chain rule\nfrom probability. Speciﬁcally,\np(A1=a1;A2;:::;A m=am)\n=mY\ni=1p(Ai=aijA1=a1;A2=a2;:::;A i\u00001=ai\u00001)(2)\nAutoregressive models decompose the joint distribution\nintomconditional distributions P(AijA1;:::;A i\u00001). Each\nof these conditional distributions is then learned using an\nappropriate DL architecture. One can control the expressive-\nness of this approach by controlling the DL model used to\nlearn each of these conditional distributions. For the attribute\norderingA1;A2;:::;A m, the DL model for estimating Ai\nonly accepts inputs from A1;A2;:::;A i\u00001. The DL model\nﬁrst learns the distribution p(A1), followed by conditional\ndistributions such as p(A2jA1),p(A3jA1;A2)and so on. This\nprocess of sequentially regressing each attribute through its\npredecessors is known as autoregression [11].\nGiven such a setting, we need to address two questions.\nFirst, which DL architecture should be used to learn the\nautoregressive conditional distributions? Second, how can we\nidentify an effective ordering of attributes needed for au-\ntoregressive decomposition? Different decompositions could\nhave divergent performances and it is important to choose an\nappropriate ordering efﬁciently.\nB. Autoregressive Density Estimators\nEncoding Tuples. The ﬁrst step in modelling is to encode\nthe tuples such that they can be used efﬁciently for density\nestimation by a DL model. A naive approach would be to use\none-hot encoding of the tuples. While effective, it is possible\nto design a denser encoding. Without loss of generality, let\nthe domain of attribute Ajbe[0;1;:::;jDom (Aj)j\u00001]. As\nan example, let Dom (Aj) =fvj1;vj2;vj3;vj4g= [0;1;2;3].\nOne-hot encoding represents them as 4dimensional vectors\n0001 ,0010 ,0100 and1000 . We could also use a binary\nencoding that represents them as a dlog2jDom (Aj)jedimen-\nsional vector. Continuing the example, we represent Dom (Aj)\nas00;01;10;11respectively. This approach is then repeated\nfor each attribute individually and the representation for\nthe tuple is simply the concatenation of the binary encod-\ning of each attribute. This approach requires less storage -Pm\ni=1dlog2jDom (Ai)jedimensions for mattributes instead ofPm\ni=1jDom (Ai)jrequired by one-hot encoding. As we shall\ndemonstrate experimentally, binary encoding is faster to train\ndue to a lower number of parameters to learn and yet generates\nbetter estimates than one-hot encoding.\n\nLoss Function. Using binary encoding, we represent a tuple\ntas a vector of binary observations xof dimension D. Each\nof these observations could be considered as a binary random\nvariablexi. We can specify an autoregressive distribution over\nthe binary encoding of the tuple as\np(x) =DY\ni=1p(xijx1;:::;x i\u00001) (3)\nA naive approach would be to store conditional probability\ntables for various values of x1;:::;x i\u00001. However, this ap-\nproach would impose a large storage overhead. An elegant\napproach is to treat the conditional distribution as a random\nvariable that - given values for x1;:::;x i\u00001- takes a value of 1\nwith probability bxiand0otherwise. In other words, let p(xi=\n1jx1;:::;x i\u00001) =bxiandp(xi= 0jx1;:::;x i\u00001) = 1\u0000bxi. If\nwe train a model to learn the value of bxiaccurately, then we\ncan forego the need to store the conditional probability table.\nIn order to train such a ML model, we need to specify the\nloss function. For a given tuple x, the negative log likelihood\nfor estimating the probabilities is\n`(x) =\u0000logp(x) =DX\ni=1\u0000logp(xijx1;:::;x i\u00001)\n\u0000(1\u0000xi)p(xi= 0jx1;:::;x i\u00001)\n(4)\nThe negative log likelihood for the relation is speciﬁed as\n\u0000logp(R) =X\nx2RDX\ni=1\u0000xilogbxi\u0000(1\u0000xi) log(1\u0000bxi)\n=X\nx2R`(x)(5)\nThe function `(x)corresponds to cross entropy loss [11]\nthat measures how the learned probability bxidiverges from\nthe actual value for xi.\nAutoregressive Density Estimators using MADE. We can\nnow utilize any DL architecture such as a fully connected\none to learn the conditional distributions by minimizing the\ncross entropy loss deﬁned over all the tuples as in Equation 5.\nHowever, since the underlying loss is deﬁned over an autore-\ngressive distribution, it is often more efﬁcient and effective\nto use one of the neural autoregressive density estimators\nspeciﬁcally designed for this purpose [13], [42], [9]. While\nour approach is agnostic to the speciﬁc estimator used, we\nadvocate for the masked autoencoder architecture from [9].\nMADE modiﬁes the autoencoders [11][43][36] for efﬁciently\nestimating autoregressive distributions. As we shall describe in\nSection III-D, its ﬂexible architecture allows us to effectively\nadapt it to relational domains.\nC. Answering Range Queries\nOnce the autoregressive density estimator has been trained\nit could be used to answer point queries. Given a query\nq:Ai=aiAND , we can encode this query and feed it to\nthe autoregressive density estimator model which will outputthe normalized selectivity. While these models cannot directly\nanswer range queries, it is possible to use their ability to\nanswer point queries in a sophisticated way to obtain accurate\nrange selectivity estimates.\nSpeciﬁcally, let us consider the question of answering range\nqueries of the form: q:A12R1ANDA22R2AND:::\nwhereRiis a range of the form lbi\u0014Ai\u0014ubi. Point queries\nof the form Ai=aicould be made into a range query ai\u0014\nAi\u0014ai. Finally, if an attribute Aiis unspeciﬁed then it could\nbe modeled as min(Dom (Ai))\u0014Ai\u0014max(Dom (Ai)).\nFor the rest of subsection, we consider a query Qof the\nformA12R1AND:::ANDAk2Rk.\nExhaustive Enumeration. If the range query is relative simple\n– involving small number of attributes and/or small ranges – a\nsimplistic solution is to enumerate all possible combinations of\nthe ranges and invoke the point query estimator. Speciﬁcally,\nsel(Q) =X\nx12R1:::X\nxk2Rkp(x1;:::;x k) (6)\nUniform Sampling. This approach is not feasible if many\nattributes with possibly large ranges are involved in the query.\nFor example, a query A12[1;100] ANDA22[1;100] would\nneed to enumerate over 10,000 point queries. One possible\nsolution is to generate random queries by uniformly sampling\nfrom from the ranges R1;:::;R kresulting in speciﬁc values\na1;:::a k. Then one can query the point query estimator for\nqi=p(x1=ai\n1;:::;x k=ai\nk). This is then repeated for jSj\ntimes to generate the selectivity estimate as\nsel(Q) =jR1j\u0002:::jRkj\njSjjSjX\ni=1sel(qi) (7)\nAdaptive Importance Sampling. While intuitive, uniform\nsampling provides bad selectivity estimates when the number\n(and range) of predicates increases due to curse of dimen-\nsionality [31]. The key insight to improve the naive uniform\nsampling is to make it adaptive andweighted . In other words,\neach sample could have a different weight and the probability\nwith which a new point is selected could vary based on\npreviously obtained samples. However, naively implementing\nthis idea results in biased and incorrect results.\nWe adapt an algorithm that was originally designed for\nMonte-Carlo multi-dimensional integration for the range se-\nlectivity estimation problem. Intuitively, we wish to select\nsamplesSin proportion to the contribution they make to\nsel(q). However, this leads to a chicken-and-egg problem as\nwe use sampling to estimate sel(q). The solution is to proceed\nin stages and use the information collected from samples of\nprevious stages to improve the next stage.\nLetf(\u0001)be the probability density function based on query\nqsuch that if we sample points proportional to f(\u0001), we might\nget accurate estimates. Of course, this information is not al-\nways available. Suppose that we have access to another simpler\nprobability density function g(\u0001)that is an approximation of\nf(\u0001)and is also easier to sample from. Obviously, sampling\n\nfromg(\u0001)would provide much better estimates than uniform\nsampling. Given sample queries q1;:::;q kgenerated using\ng(\u0001), we can derive the estimate as\nsel(Q) =jR1j\u0002:::jRkj\njSjjSjX\ni=1sel(qi)\ng(qi)(8)\nIntuitively, we generated random queries based on g(\u0001)\nand then appropriately corrected the bias to get an unbiased\nestimate. Now the remaining question is to obtain an efﬁcient\ninstantiation of g(\u0001). We propose a simple approach inspired\nby Attribute Value Independence (A VI) assumption where\nSel(A1=a1AND:::ANDAk=ak) =\nSel(A1=a1)\u0002:::\u0002Sel(Ak=ak)(9)\nIt is known that A VI assumption often provides an under-\nestimate for correlated attributes [35]. We leverage this fact\nto decompose g(A1;A2;:::;A k)askcomponent functions\ng1(A1),g2(A2):::. One can then approximate the density of\neach of these attributes individually through existing synopses\napproaches such as histograms.\nOur proposed approach operates in stages. We generate\nan initial batch of random queries through uniform sampling\nfrom the ranges. Using these random queries, we bootstrap the\nhistograms for individual attributes. In the future stages, we\ngenerate samples in a non-uniform way using the sampling\ndistribution imposed by the attribute wise histograms. For\nexample, a query qi=x1=ai\n1;:::;x k=ai\nkwill be picked\nproportional to the probability g1(ai\n1)\u0002:::\u0002gk(ai\nk). So if\nsome valueAi=aioccurs much more frequently then it will\nbe reﬂected in the histogram of Aiand thereby will occur\nmore frequently in randomly generated queries. Once all the\nsample queries are created, we then use Equation 9 to generate\nunbiased estimates for range selectivity.\nD. Attribute Ordering for Autoregression\nIn practice, the best attribute ordering for autoregressive\ndecomposition is not given to us and must be chosen ap-\npropriately for accurate selectivity estimation. Each of the\npermutations of the attributes forms a valid attribute ordering\nand could be used to estimate the joint distribution.\np(x) =p(x1)\u0001p(x2jx1)\u0001p(x3jx1;x2)\n=p(x2)\u0001p(x3jx2)\u0001p(x1jx2;x3)\n=p(x3)\u0001p(x2jx3)\u0001p(x1jx2;x3)\n=:::(10)\nRandom Attribute Ordering. Prior approaches such as\nBayesian Networks deploy an expensive approach to identify\na good ordering. We do away with this expensive step by\nchoosing several random orderings of attributes. As we shall\nshow experimentally, this approach works exceedingly well\nin practice. This is due to two facts: (a) the vast majority\nof thed!possible permutations is amenable to tractable and\naccurate learning; and (b) the powerful learning capacity of\nneural networks (and masked encoders) can readily learn evena challenging decomposition by increasing the depth of the\nMADE model. MADE architecture allows this to be easily and\nefﬁciently conducted by randomly permuting both the input\ntuple that is binary encoded and the internal mask vectors in\neach layer.\nEnsembles of Attribute Orderings. While a random ordering\noften provides good results, it is desirable to guard against the\nworst case scenario of a bad permutation. We observe from\nEquation 10 that numerous attribute orderings could be used\nfor estimating the joint distribution. We build on this insight\nby choosing \u0014random attribute orderings. Of course, different\norderings result in different models with their corresponding\nestimate and associated accuracy. MADE could be used to\nlearn the conditional distribution for each of these orderings\nand utilize them to estimate the value of p(x)by averaging the\nindividual estimates. An attribute ordering can be represented\nasm0= [m0(1);:::m0(D)]. In this,m0(d)represents the\nposition of the d-th dimension of input xin the product of\nconditionals. Thus multiple random orderings can be obtained\nby permuting [1;:::;D ].\nDuring training, before each minibatch [11] update of the\nmodel, we apply \u0014random permutations in parallel on the\ninput vectors and mask matrices. Each of these permutations\ncorresponds to a different ordering. The models are learned\nindependently and the joint probability is computed for each\nordering and averaged to produce the ﬁnal estimate. This\nensemble approach minimizes the likelihood of a bad estimates\ndue to an unlucky attribute ordering.\nInjecting Domain Knowledge. If a domain expert possesses\napriori knowledge that attributes AiandAjare order sensitive,\nthen we only chose permutations where the desired order is\nobserved. As a concrete example, assume one knows (say\nvia data proﬁling), that a functional dependency EmpID!\nDepartment exists on the schema. Then, we would prefer\npermutations where the Department occurs after EmpID .\nThis is due to the fact that the conditional distribution\np(DepartmentjEmpID )is simpler and thereby easier to learn\nthan the other way around.\nE. Incorporating Query Workload\nThe autoregressive approach outlined above does not re-\nquire a training dataset such as a query workload. However,\nit is possible to improve performance by leveraging query\nworkload if available. Suppose that we are given a query\nworkloadQ=fq1;:::;q lg. We associate a weight w(t)for\neach tuplet2Rthat corresponds to the number of queries that\nmatcht. Sow(t)can vary between 0andl. Next, we assign\nhigher penalties for poor estimates for tuples in the result\nset of multiple queries. The intuition is that a poor estimate\nfor tupletwas caused by sub-optimal learning of parameter\nweights of the conditional distributions corresponding to the\nattribute values of t. As an example, consider a tuple t= [0;1]\nwith two binary attributes A1andA2. Suppose that we use\na single attribute ordering A2;A1. If the selectivity of t\nwas incorrectly estimated, then the entries corresponding to\n\np(A2= 1) andp(A1= 0jA2= 1) must be improved. If tis in\nthe result set of by many queries, then we prioritize learning\nthe aforementioned parameter values through larger penalty.\nThis could be achieved using the weighted cross-entropy loss\nfunction deﬁned as,\n\u0000logp(R) =X\nt2Rw(t)\u0001`(t)(11)\nF . Incremental Data and Query Workload\nWe next consider the scenario where data is provided\nincrementally as well as new queries involving the new data\nbecome available.\nIncremental Learning. The naive solution of retraining the\nentire model from scratch becomes progressively expensive as\nmore and more batches of incremental data are added to R.\nWe propose an incremental learning approach that extends the\nexisting pre-trained model by training it further only on the\nnew data by initializing the model with the weights learned\nfrom the previous training, instead of performing the standard\nrandom initialization. We then continue training the model\non new data. This two-step process preserves the knowledge\ngained from the past, absorbing knowledge from new data\nand is also more efﬁcient. We use a smaller value for learning\nrate [11] and epochs than for the complete retraining so that\nthe model is ﬁne-tuned .\nWhile incremental learning is conceptually simple, it must\nbe done carefully. A naive training could cause catastrophic\nforgetting where the model “forgets” the old data and focuses\nexclusively on the new data. This is undesirable and must be\navoided [12]. We propose the use of Dropout [38] and related\ntechniques [26] to learn without forgetting . In our paper, we\nutilize a dropout value of p= 0:1when training over the new\nbatch of data. An additional complication arises from the fact\nthat we already uses masks for maintaining the autoregressive\nproperty. Hence, we apply the dropout only on the neurons\nfor which the masks are non-zero.\nIncremental Workload. An autoregressive approach does not\ndirectly utilize query workload and hence could not use the\ninformation available from an incremental query workload. It\nis possible to reuse the techniques from Section III-E for this\nscenario. For each tuple, we update the number of queries it\nsatisﬁes and retrain the model based on the new weights.\nIV. S ELECTIVITY ESTIMATION AS SUPERVISED LEARNING\nOur objective is to build a model that accepts an arbitrary\nquery as input and outputs its selectivity. This falls under the\numbrella of supervised learning methodologies using regres-\nsion. Each query is represented as a set of features and the\nmodel learns appropriate weights for these features utilizing\nthem to estimate the selectivity. The weights are learned by\ntraining the model on a dataset of past queries (such as from\nquery log or workload) and their true selectivities. Approaches\nsuch as linear regression, support vector regression etc that\nhave been utilized for query performance prediction [1] are not\nsuitable for building selectivity estimators. The impediment isthe complex relationship between queries and their selectiv-\nities where simplifying assumptions such as attribute value\nindependence do not hold. We leverage the powerful learning\ncapacity of neural networks - with appropriate architecture and\nloss functions - to model this relationship.\nA. Query Featurization\nThe ﬁrst step is to encode the queries and their selectivities\nin an appropriate form suitable for learning.\nTraining Set. We are given a query training dataset Q=\nf(q1;s1);:::;g. Each query q2Qcan be represented as\nan ordered list of mattribute pairs of (Ai;vi)wherevi2\nDom (Ai)[f\u0003g (where * is used when Aiis unspeciﬁed). si\ndenotes the normalized selectivity of qi(i.e.,Sel(qi)\nn) wheren\nis the number of tuples.\nExample. LetQ=f(fA1= 0;A2= 1g;0:3);(fA1=\n1;A2=\u0003g;0:2);(fA1=\u0003;A2=\u0003g;1:0)g.i.e., Queryq2\nwithA1= 1 ANDA2=\u0003has a selectivity of 0:2.\nEncoding Queries. An intuitive representation for categorical\nattributes is one-hot encoding. It represents attribute Aias\njDom (Ai)j+ 1dimensional vector that has 0 for all positions\nexcept the one corresponding to the value Aitakes. Given\nmattributes, the representation of the query is simply the\nconcatenation of one-hot encoding of each of the attributes.\nThe numeric attributes can be handled by treating them as\ncategorical attributes by automatic discretization [6]. Alterna-\ntively, they can be speciﬁed as a normalized value 2[0;1]by\nmin-max scaling. Note that this scheme can be easily extended\nto operators other than =. The only modiﬁcation required is to\nrepresent the triplet (Ai;operator i;vi)instead of just (Ai;vi).\nEach operator could be represented as a ﬁxed one-hot encoding\nof its own. Given doperators, each operator is represented\nas addimensional vector where the entry corresponding to\noperator iis set to 1. Of course, the rest of our discussion is\noblivious to other mechanisms to encode the queries.\nEncoding Selectivities. Each query q2Qis associated\nwith the normalized selectivity si2[0;1]. Selectivities of\nqueries often follow a skewed distribution where few queries\nhave a large selectivity and the vast majority of queries have\nmuch smaller selectivities. Building a predictive model for\nsuch skewed data is often quite challenging. We begin by\napplying log transformation over the selectivity by replacing\nthe selectivity siby its absolute log value as abs(log(si)).\nFor example, a selectivity of 0:00001 is speciﬁed as 5(using\nlog to the base of 10 for convenience). This has a smoothing\neffect on the values of a skewed distribution [4]. Our second\ntransformation is min-max scaling where we rescale the output\nof the log transformation back to [0;1]range. Given a set of\nselectivities S=fs1;s2;:::;gand a selectivity si, min-max\nscaling is computed as\ns0\ni=si\u0000min(S)\nmax(S)\u0000min(S)(12)\nWhile this transformation does not impact skew, it enables us\nto deploy well known activation functions such as sigmoid that\n\nare numerically stable. Prior works such as [19], [7] have also\nused log transformation to improve effectiveness of regression.\nExample. Let the selectivities of three queries be\n[0:1;0:01;0:002]. By applying the log transformation,\nwe get [1;2;3]. The corresponding min-max scaling gives\n[0:0;0:5;1:0]where 0:5 =2\u0000min([1;2;3])\nmax([1;2;3])\u0000min([1;2;3]).\nB. DL Model for Selectivity Estimation\nDL Architecture. Our DL architecture is based on a 2-\nlayer fully connected neural network with rectiﬁer activation\nfunction (ReLU) speciﬁed as f(x) =max(0;x)ReLU is a\nsimple non-linear activation function with known advantages\nsuch as faster training and sparser representations. The ﬁnal\nlayer uses a Sigmoid activation function f(x) =1\n1+e\u0000x\nSigmoid is a popular function that squashes its parameter into a\n[0;1]range. One can then convert this output to true selectivity\nby applying inverse of min-max and log scaling. We used the\nAdam optimizer [18] for training the model.\nLoss Function. Recall from Section II that the q-error metric\nis widely used to evaluate the selectivity estimator. Hence, it\nis desirable to train the DL model to directly minimize the\nmean Q-error of the training dataset.\nq-error (Q) =1\njQjjQjX\ni=1max\u0012si\nbsi;bsi\nsi\u0013\n(13)\nSelectivity Estimation via Inference Once the model is\ntrained, it can be used for estimating the selectivity. Given\na new query, we extract its features through one-hot encoding\nand feed it to the model. We apply the inverse transfor-\nmation of min-max and log scaling on the output so that\nit represents the actual selectivity. The model is lightweight\nand the inference process often takes few milli-seconds when\nrun on a GPU and/or CPU. Note that the time taken for\ntraining and estimation are decoupled. While the training time\nis proportional to the size of the training data, the inference\nis ﬁxed for a given model.\nC. Generating Training Data\nWe next describe how a training dataset could be con-\nstructed when query workload is not available. If query work-\nload is available, we describe a novel augmentation strategy\nsuch that the DL model can generate accurate estimates for\nunknown queries that are similar to the query workload.\nNo Query Workload Available. Naive sampling from the\nspace of all queries results in a highly non-uniform training\ndataset and a sub-optimal selectivity estimator. Thus one must\nobtain a training set of queries that are diverse both in the\nnumber of predicates and their selectivities. Let the query\nbudget beB-i.e., we wish to construct a dataset with B\nqueries and their selectivities. We begin by enumerating all\nqueries with 1predicates that are the atomic units from which\nmulti-predicate queries could be estimated. We then generate\nmulti-predicate queries where the predicates are chosen at\nrandom while the values are chosen based on their frequency.In order to generate a random query qi, we ﬁrst choose the\nnumber of predicates k2f2;:::;mguniformly at random.\nThen we choose kattributes uniformly at random from the set\nof attributes A=fA1;:::;A mg. Let the selected attributes be\nfAi1;:::;A ikg. These two steps ensure that we have a diverse\nset of multi predicate queries both in terms of the number of\npredicates and the chosen predicates. Next, we choose a tuple t\nuniformly at random from the relation R. We create a random\nqueryqias the conjunction of predicates Aij=t[Aij]. This\nprocess ensures that the random query is selected proportional\nto the selectivity of query qi.\nQuery Workload Available. If a query workload Qis avail-\nable, one could directly utilize it to train the DL model.\nHowever, one can do much better by augmenting it, obtaining\na more informative training set of queries. The key idea\nis to select queries from the distribution induced by the\nquery workload such that the model generalizes to unknown\nqueries from the same distribution. We need to address two\nissues. First, how can one generate random queries to augment\nthe query workload? Second, how do we tune the model\nsuch that it provides accurate results for the workload? The\nsolution involves importance sampling and weighted training\nrespectively.\nWe begin by assigning weights to attributes and attribute\nvalues based on their occurrence in the query workload. For\nexample, if A1occurs 100 times while A2occurs 50 times,\nthen weight of A1= 2=3andA2= 1=3. We repeat this\nprocess for attribute values also. If an attribute value does not\noccur in the query workload, we assign a token frequency\nof 1. For example, if A1= 1 occurs 100 times while A2\noccurred none, then their weights are 100=101 and1=101\nrespectively. We compute the frequency distribution of the\nnumber of predicates from the query workload (such as #\nqueries with 1, 2, 3, :::predicates). This information is used\nto perform weighted sampling of the queries by extending\nthe algorithm for the no workload scenario. This ensures\nthat queries involving popular attributes and attribute values\nare generated at a higher frequency. Of course, sampling\ntakes place without replacement so that all the queries in the\naugmented query workload are distinct.\nNext, we assign different weights w(qi)to the queries qi2\nQ0from the augmented workload to ensure that the model\nprioritizes the accuracy of queries from the workload.\nw(qi) =(\n1 ifqi2Q\njQj\njQ0jifqi62Q(14)\nWe then train the DL model where the penalty for a query qis\nweighed proportionally to whether it came from the original\nor the augmented query workload.\nq-error (Q) =1\njQjjQjX\ni=1w(qi)\u0002\u0012si\nbsi+bsi\nsi\u0013\n(15)\n\nD. Miscellaneous Issues\nRange Queries. The generic nature of our query featurization\nallows us to transparently include range queries. Instead of\none-hot encoding the attribute values of the point query, we\njust need to one-hot encode the speciﬁed range. Note that if\nan attribute Aiis not speciﬁed in the query, we encode the\nrange as the minimum and maximum value from the domain\nofAi.\nIncremental Data. Our supervised approach does not even\nlook at the data and only uses the query training dataset.\nWhen incremental data arrives, the selectivities of some of\nthese queries would change. We then train the model on the\ndataset with the updated selectivites.\nIncremental Query Workload. In this case, it is possible\nto use the incremental training algorithm as described in Sec-\ntion III-F. We initialize the supervised model with the weights\nfrom previous training run instead of random initialization.\nWe train the model on the new data with a reduced learning\nrate and a smaller number of epochs. We also use Dropout\nregularization technique with probability p= 0:1to avoid\ncatastrophic forgetting.\nV. E XPERIMENTS\nIn our evaluation, we consider the following key questions:\n(1) how do DL based methods compare against prior se-\nlectivity estimation approaches commonly used in database\nsystems? (2) how is the performance of our unsupervised\nand supervised methods affected by query characteristics such\nas number of predicates/attributes, selectivity, size of joint\nprobability distribution and correlated attributes; (3) how does\nchanging the DL model parameters such as number of neurons,\nlayers and training epochs affects performance?\nA. Experimental Setup\nHardware and Platform. All our experiments were per-\nformed on a NVidia Tesla K80 GPU. The CPU is a quad-core\n2.2 GHz machine with 16 GB of RAM. We used PyTorch for\nbuilding the DL models.\nDatasets. We conducted our experiments on two real-world\ndatasets: Census [10] and IMDB from Join Order Bench-\nmark [24]. Both datasets have complex correlated attributes\nand conditional dependencies. Selectivity estimation on mul-\ntiple predicates on these datasets are quite challenging and\nhence have been extensively used in prior work [24]. The\nCensus dataset has 50K rows, 8 categorical and 6 numerical\nattributes. Overall, the IMDB dataset consists of 21 tables\nwith information about movies, actors, directors etc. For\nour experiments, we used two large tables Title.akas and\nTitle.basics containing 3.4M and 5.3M tuples with 8 and 9\nattributes respectively.\nAlgorithms for Selectivity Estimation. The unsupervised\nmodel consists of a 2 layer masked autoencoder with 100\nneurons in each layer. Both our algorithms were trained for\n100 epochs by default. We used 1% sample of IMDB data fortraining the DL algorithms. The supervised model consists of 2\nfully connected layers with 100 neurons and ReLU activation\nfunction. The ﬁnal layer has sigmoid activation function to\nconvert the output in the range [0;1]. The training data consists\nof 10K queries (see details in Section IV).\nQuery Workload. We compared the algorithms on a test query\nlog of 10K queries. We generated the log to thoroughly eval-\nuate the performance of the estimators for various facets such\nas number of predicates, selectivity, size of joint probability\ndistribution, attribute correlation etc. Census has 8 categorical\nattributes thereby creating\u00008\n1\u0001\n+\u00008\n2\u0001\n+:::\u00008\n8\u0001\n= 255 possible\nattribute combinations. The 10K workload was equally allotted\nsuch that there are 1250 queries with exactly 1 predicate,\n1250 queries with 2 predicates and so on. There are\u00008\ni\u0001\ncombinations with exactly iattributes. We allocate 1250\nequally between\u00008\ni\u0001\ncombinations. For a speciﬁc attribute\ncombination - say education and marital-status - we pick their\nvalues randomly without replacement from their respective\ndomains.\nPerformance Measures. We used q-error deﬁned in Section II\nfor measuring the estimation quality. Recall that q-error of 1\ncorresponds to perfect estimate while a q-error of 2 corre-\nsponds to an under- or over-estimate by a factor of 2 and so\non. We also use box-plots to concisely describe the results\nof 10K queries. The middle line corresponds to the median\nq-error while the box boundaries correspond to the 25th and\n75th percentiles. The top and bottom whiskers are set to show\nthe 95th and 5th percentiles.\nB. Comparison with Baselines\nIn our ﬁrst set of experiments, we demonstrate the efﬁcacy\nof our approaches against popular baseline approaches such\nas multi-dimensional histograms [34], [39], [16], [15], [2],\n[3], wavelets [29], Bayesian networks [10], [40] and sam-\npling [27]. Our experiments were conducted on Postgres and\nleverage the recently introduced multi-column statistics feature\nfrom Postgres 10. We use the TABLESAMPLE command in\nPostgres 10 with Bernoulli sampling to obtain a 1% sample.\nHaar wavelets are widely used in selectivity estimation and\napproximate query processing [29], [5] as they are accurate\nand can be computed in linear time. We used standard Haar\nwavelet decomposition algorithm described in [5] for handling\nmulti-dimensional data. Finally, we used entropy based dis-\ncretization [6] for Bayesian networks (denoted as BN) so that\nit ﬁts into the space budget. We implemented the algorithm\ndescribed in [10]. We also evaluated our approach against\nLinear Regression (denoted as LR) [32] and Support Vector\nRegression (denoted as SR) [37] that has been previously used\nfor a related area of query performance monitoring [1]. For\nfair comparison, we ensured that all the selectivity estimators\nare allocated the same space budget. For example, our single\nsupervised model for the entire Census dataset requires around\n200 KB. However, we allocate for each multi-dimensional\nhistogram and wavelets for various attribute combinations the\nsame space budget.\n\nUnsupervised Supervised Sampling LR SVR BN Wavelet Histogram\nAlgorithms0.02.55.07.510.012.515.017.5log(q-error)\nFig. 1. Comparison with Baselines (Census)\nUnsupervised Supervised Sampling LR SVR BN Wavelet Histogram\nAlgorithms0.02.55.07.510.012.515.017.5log(q-error)\n Fig. 2. Comparison with Baselines (IMDB)\n1 2 3 4 5 6 7 8\nNumber of Predicates1234567891011121314q-error\nFig. 3. Varying #Predicates (Unsu-\npervised)\n[0-0.01) [0.01-0.05) [0.05-0.1) [0.10-1)\nSelectivity1234567q-errorFig. 4. Varying Selectivity (Unsu-\npervised)\n[0-100) [100-1K) [1K-10K) [10K-0.5M)\nSize of Domain Cardinality12345678910111213q-errorFig. 5. Varying Domain Cardinality\n(Unsupervised)\n[0-2) [2-4) [4-6) [6-10)\nEntropy12345678q-errorFig. 6. Varying Entropy (Unsuper-\nvised)\n25 50 75 100\nNumber of Neurons12345678q-error\nFig. 7. Varying #Neurons (Unsuper-\nvised)\n1 2 3\nNumber of Layers1234q-errorFig. 8. Varying #Layers (Unsuper-\nvised)\nCensus IMDB\nDataset12345q-errorEncoding\nBinary\nOne HotFig. 9. Impact of Encoding (Unsu-\npervised)\n1 5 10 15 25\nNumber of Attribute Orderings1234q-errorFig. 10. Varying #Masks (Unsuper-\nvised)\nFigures 1 and 2 present the results. The y-axis depicts\nthe q-error in log scale. So for Figures 1 and 2 a value\nof 0 corresponds to perfect estimate as log 1 = 0 . We can\nobserve that our DL based approaches dramatically outperform\nall the prior methods. The baseline approaches of LR [32]\nand SVR [37] provide inaccurate results; both Census and\nIMDB exhibit complex correlation and conditional depen-\ndencies, which these techniques are unable to adapt to. The\nsampling based approach provides good estimates for querieswith large/medium selectivities but dramatically drops off in\nquality/accuracy for queries with low/very-low selectivities.\nWavelets and histograms provide performance comparable\nto our methods. However, this is due to the fact that we\ndisproportionately allocated much more resources for them\nthan our approaches. Interestingly, the closest baseline is BN\nthat is related in principle to our algorithm. However, our\napproaches are superior to BN in both accuracy and time.\nSpeciﬁcally our approaches are 2 times more accurate on\n\nNormal With Domain Knowledge\nAlgorithm1234q-errorFig. 11. Injecting Domain Knowl-\nedge (Unsupervised)\n1 2 3 4 5 6 7 8\nNumber of Predicates12345q-errorFig. 12. Varying #Predicates (Super-\nvised)\n[0-0.01) [0.01-0.05) [0.05-0.1) [0.10-1)\nSelectivity12345q-errorFig. 13. Varying Selectivity (Super-\nvised)\n[0-100) [100-1K) [1K-10K) [10K-0.5M)\nSize of Domain Cardinality12345q-errorFig. 14. Varying Domain Cardinal-\nity (Supervised)\nCensus IMDB\nDataset1234q-errorLoss Function\nf(q-error)\nMSE\nFig. 15. MSE vs Q-Error (Super-\nvised)\nUnweighted Loss Weighted Loss\nAlgorithm12345q-errorFig. 16. Workload (Unsupervised)\nUnweighted Loss Weighted Loss\nAlgorithm12q-error Fig. 17. Workload (Supervised)\nOurs Naru\nDepth1.01.52.02.53.0Q Error\nDataset\nCensus\nFlights Fig. 18. Range Queries\naverage for Census and 100 times more accurate for the\nworst case error. Similar trends hold for IMDB in terms of\naccuracy. As a point of reference, it takes one minute to\ntrain our approaches on Census versus 16 minutes for BN\n(correspondingly 12 minutes of training for our approaches\nfor IMDB versus 516 minutes for BN). Recall that learning\nthe optimal structure of a BN is very expensive. In contrast,\nour approach is much faster due to the use of multiple random\nattribute orderings.\nOur methods begin to dramatically outperform the baseline\nfor queries involving 4 or more predicates. This is consistent\nwith the expectation that DL approaches are able to learn com-\nplex relationships in higher dimensions and depict superior\naccuracy. Our results were consistently better across various\nparameters of interest such as selectivity, number of predicates,\nsize of joint probability table and attribute correlation. We\ndemonstrate each of them with the following experiments.\nC. Unsupervised Density Estimation\nWe begin by investigating the performance of our masked\nautoencoder based approach for evaluating selectivity estima-\ntion. There are four key dimensions whose impact must be\nanalyzed. They include the number of predicates in the query,\nselectivity of the query, size of joint probability distribution\ntable and ﬁnally the correlation between attributes involved inthe query. Due to space limitations we provide the results for\nthe Census dataset only. The trends for the IMDB data set\nwere similar to that of Census.\nVarying #Predicates in Query. Figure 3 depicts how our un-\nsupervised approach behaves for queries with varying number\nof predicates. As expected, the approach is very accurate for\nqueries with small number of predicates. This is unsurprising\nas they could be easily learnt by most selectivity estimators.\nWe can observe however that our estimates are very accurate\nand within a factor of two even for queries with as much as 7-8\npredicates . Often queries with large number of predicates have\nsmall selectivities and exhibit complex correlations. Despite\nthose challenges our methods provide very good performance.\nVarying Query Selectivity. Next, we group the queries in\nthe 10K test set based on their selectivity. Figure 4 presents\nthat, our approach provides very accurate estimates for queries\nwith selectivity of 5% or more. Even when the selectivities are\nlow or very low, our method is still able to provide excellent\nestimates that are off by a factor of at most 2 for 75% percent\nof the query test set.\nVarying Size of Joint Probability Distribution. Recall that\nthe size of the joint probability distribution (JPD) increases\nexponentially with more and more attributes and/or attributes\n\nwith large domain cardinality. Hence, synopses based meth-\nods have to make simplifying assumptions such as attribute\nvalue independence for compactly representing distributions.\nFigure 5 demonstrates the performance of our approach. As\nexpected, our methods can produce very accurate estimates\nwhen the size of JPD is small. Nevertheless, even when the\nsize of JPD is very large (almost 3.8M for Census), it is still\noff only by a small factor.\nVarying Attribute Correlation. Another major factor is\nthe correlation and dependencies between attributes. If a set\nof attributes are correlated, then simplistic approaches such\nas attribute value independence yield very large estimation\nerrors. We use entropy to quantify the challenge in concisely\nmodeling the joint distribution. Intuitively, simple distributions\nsuch as uniform have a large entropy while highly correlated\nattributes have a small entropy. Figure 6 shows that our model\nperforms very accurate estimations for small entropy. This\ndemonstrates that it is able to successfully learn relationships\nbetween attribute distributions.\nVarying Model Hyperparameters. Figures 7 and 8 present\nthe impact of varying the two major hyperparameters. We\nbegin by varying the number of neurons in each layer from 25\nto 100. We observe that as the number of neurons increases,\nthe q-error decreases with a hint of diminishing returns. Larger\nnumber of neurons increases the model capacity available to\nlearn complex distributions at the cost of increased training\ntime. Figure 8 demonstrates that increasing the number of\nlayers has a milder impact on performance. A model with\n2 layers is already expressive enough to handle the data\ndistribution of Census.\nRange Queries. We next evaluated the performance of our\nadaptive importance sampling algorithm for answering range\nqueries with upto 4 range predicates. We compared our ap-\nproach with [44] that proposed a progressive sampling that also\nprovides unbiased estimates. We ﬁxed the sampling budget\nto 500 samples. Figure 18 shows that our approach provides\nslightly better estimates than Naru.\nMiscellaneous Experiments. Figure 9 presents the impact of\nthe tuple encoding for the two datasets. For simple datasets\nsuch as Census where most of the attributes have small domain\ncardinality, both approaches provide comparable performance.\nHowever, for datasets such as IMDB where domain cardi-\nnalities could be in the range of 10s of thousands, binary\nencoding outperforms simple one-hot encoding. Figure 10\nshows that performance drops when the number of random\nattribute orderings \u0014increases as it also simultaneously in-\ncreases the possibility of a bad ordering. Empirically, we found\nthat using a value between 1and5provides best results.\nFigure 11 presents the impact of injecting domain knowledge\nappropriately. For this experiment, we only considered queries\nwhose predicates are a superset of attributes involved in a\nfunctional dependency. We also set the value of \u0014to 1. The\nresults demonstrate that ﬁltering attribute orderings based on\ndomain knowledge provides a non-negligible improvement.D. Supervised Density Estimation\nWe next evaluate the performance of our supervised selec-\ntivity estimator. Overall, the trends are similar to that of the\nunsupervised case. Figure 12 presents the result of varying\nthe number of predicates. As expected, the q-error increases\nwith increasing number of predicates. Nevertheless, most of\nthe predicates have a q-error of at most 2. As discussed in\nthe baseline experiments, our proposed approach outperforms\nprior selectivity estimation approaches dramatically for queries\nwith large number of predicates. Figure 13 demonstrates that\nour q-error decreases as the query selectivity increases. Our\nmodel has a median q-error of less than 2 even for queries\nwith selectivity less than 1%. Figure 14 shows that as the size\nof the joint probability distribution increases, the q-error of our\nmodel increases; it is still within a factor of 2 however. A key\nfactor is our proposed algorithm to generate training datasets\nthat provide meaningful and diverse set of queries to train our\nsupervised model. Figure 15 presents the impact of the loss\nfunction. As described in Section IV, directly using q-error as\nthe loss function is desirable to using proxy metrics such as\nMSE. For both datasets, the performance of a DL model with\nq-error is superior to that of MSE.\nVI. R ELATED WORK\nDeep Learning for Databases. Recently, there has been\nextensive work on applying techniques from DL for solving\nchallenging database problems. One of the ﬁrst work was\nby Kraska et. al [21] to build fast indices. The key idea is\nto use a mixture of neural networks to effectively learn the\ndistribution of data. This has some obvious connection to\nour work. Similar to [21], we also use a mixture of models\nfor learning the data. However, the speciﬁc nature of the\nmixture is quite different. Speciﬁcally, we leverage mixtures\nto ameliorate the order sensitivity of neural density estimation.\nFurthermore, DL autoregressive based approaches often model\nthe data distribution more effectively than those proposed\nin [21]. There has been extensive work on using DL techniques\nincluding reinforcement learning for query optimization (and\njoin order enumeration) such as [33], [28], [41], [22]. DL has\nalso been applied to the problem of entity resolution in [8].\nSelectivity Estimation. Due to the importance of selectiv-\nity estimation, there has been extensive work on accurate\nestimation. Popular approaches include sampling [27], his-\ntograms [34], [39], [16], [15], [2], [3], wavelets [29], kernel\ndensity estimation [20], [17], [14] and graphical models [10],\n[40]. Due to its versatility, ML has been explored for the prob-\nlem of selectivity estimation. One of the earliest approaches\nto use neural networks is [23]. While promising, the recently\nproposed techniques such as neural density estimation are\nmuch more accurate. Another relevant recent work is [19]\nthat focuses on estimating correlated join selectivities. It\nproposes a novel set based DL model but focuses mostly on\nsupervised learning. In contrast we consider both supervised\nand unsupervised approaches.\n\nVII. F INAL REMARKS\nIn this paper, we investigated the feasibility of applying\ndeep learning based techniques for the fundamental problem\nof selectivity estimation. We proposed two complementary\napproaches that modeled the problem as an supervised and\nunsupervised learning respectively. Our extensive experiments\nshowed that the results are very promising and can address\nsome of the pain points of popular selectivity estimators. There\nare a number of promising avenues to explore. For one, how to\nextend the selectivity estimators over single tables to multiple\ntables involving correlated joins. Another intriguing direction\nis to investigate the possibility of other deep generative models\nsuch as deep belief networks (DBN), variational auto encoders\n(V AE) and generative adversarial networks (GANs) for the\npurpose of selectivity estimation.\nREFERENCES\n[1] M. Akdere, U. C ¸ etintemel, M. Riondato, E. Upfal, and S. B. Zdonik.\nLearning-based query performance modeling and prediction. In ICDE ,\npages 390–401. IEEE, 2012.\n[2] N. Bruno and S. Chaudhuri. Conditional selectivity for statistics on\nquery expressions. In SIGMOD , pages 311–322, 2004.\n[3] N. Bruno, S. Chaudhuri, and L. Gravano. Stholes: A multidimensional\nworkload-aware histogram. In SIGMOD , pages 211–222, 2001.\n[4] F. Changyong, W. Hongyue, L. Naiji, C. Tian, H. Hua, L. Ying, et al.\nLog-transformation and its implications for data analysis. Shanghai\narchives of psychiatry , 26(2):105, 2014.\n[5] G. Cormode, M. Garofalakis, P. J. Haas, C. Jermaine, et al. Synopses\nfor massive data: Samples, histograms, wavelets, sketches. Foundations\nand Trends in Databases , 4(1–3):1–294, 2011.\n[6] J. Dougherty, R. Kohavi, and M. Sahami. Supervised and unsupervised\ndiscretization of continuous features. In Machine Learning Proceedings\n1995 , pages 194–202. Elsevier, 1995.\n[7] A. Dutt, C. Wang, A. Nazi, S. Kandula, V . Narasayya, and S. Chaudhuri.\nSelectivity estimation for range predicates using lightweight models.\nPVLDB , 12(9):1044 – 1057, 2019.\n[8] M. Ebraheem, S. Thirumuruganathan, S. Joty, M. Ouzzani, and N. Tang.\nDistributed representations of tuples for entity resolution. PVLDB ,\n11(11):1454–1467, 2018.\n[9] M. Germain, K. Gregor, I. Murray, and H. Larochelle. Made: Masked\nautoencoder for distribution estimation. In ICML , pages 881–889, 2015.\n[10] L. Getoor, B. Taskar, and D. Koller. Selectivity estimation using\nprobabilistic models. In ACM SIGMOD Record , volume 30, 2001.\n[11] I. Goodfellow, Y . Bengio, and A. Courville. Deep Learning . MIT Press,\n2016. http://www.deeplearningbook.org.\n[12] I. J. Goodfellow, M. Mirza, D. Xiao, A. Courville, and Y . Bengio.\nAn empirical investigation of catastrophic forgetting in gradient-based\nneural networks. arXiv preprint arXiv:1312.6211 , 2013.\n[13] K. Gregor, I. Danihelka, A. Mnih, C. Blundell, and D. Wierstra. Deep\nautoregressive networks. In ICML , pages 1242–1250, 2014.\n[14] D. Gunopulos, G. Kollios, V . J. Tsotras, and C. Domeniconi. Selectivity\nestimators for multidimensional range queries over real attributes. VLDB\nJ., 14(2):137–154, 2005.\n[15] Y . E. Ioannidis. The history of histograms (abridged). In VLDB , 2003.\n[16] H. V . Jagadish, N. Koudas, S. Muthukrishnan, V . Poosala, K. C. Sevcik,\nand T. Suel. Optimal histograms with quality guarantees. In VLDB ,\npages 275–286, 1998.\n[17] M. Kiefer, M. Heimel, S. Breß, and V . Markl. Estimating join\nselectivities using bandwidth-optimized kernel density models. PVLDB ,\n10(13):2085–2096, 2017.[18] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization.\nICLR , abs/1412.6980, 2015.\n[19] A. Kipf, T. Kipf, B. Radke, V . Leis, P. Boncz, and A. Kemper. Learned\ncardinalities: Estimating correlated joins with deep learning. arXiv\npreprint arXiv:1809.00677 , 2018.\n[20] F. Korn, T. Johnson, and H. Jagadish. Range selectivity estimation for\ncontinuous attributes. In ssdbm , page 244. IEEE, 1999.\n[21] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case\nfor learned index structures. In SIGMOD , pages 489–504. ACM, 2018.\n[22] S. Krishnan, Z. Yang, K. Goldberg, J. Hellerstein, and I. Stoica. Learning\nto optimize join queries with deep reinforcement learning. arXiv preprint\narXiv:1808.03196 , 2018.\n[23] S. Lakshmi and S. Zhou. Selectivity estimation in extensible databases-a\nneural network approach. In VLDB , pages 623–627, 1998.\n[24] V . Leis, A. Gubichev, A. Mirchev, P. Boncz, A. Kemper, and T. Neu-\nmann. How good are query optimizers, really? PVLDB , 9(3), 2015.\n[25] V . Leis, B. Radke, A. Gubichev, A. Kemper, and T. Neumann. Car-\ndinality estimation done right: Index-based join sampling. In CIDR ,\n2017.\n[26] Z. Li and D. Hoiem. Learning without forgetting. IEEE TPAMI ,\n40(12):2935–2947, 2018.\n[27] R. J. Lipton, J. F. Naughton, and D. A. Schneider. Practical selectivity\nestimation through adaptive sampling , volume 19. ACM, 1990.\n[28] R. Marcus and O. Papaemmanouil. Deep reinforcement learning for join\norder enumeration. arXiv preprint arXiv:1803.00055 , 2018.\n[29] Y . Matias, J. S. Vitter, and M. Wang. Wavelet-based histograms for\nselectivity estimation. In ACM SIGMOD Record , volume 27, 1998.\n[30] M. M ¨uller, G. Moerkotte, and O. Kolb. Improved selectivity estima-\ntion by combining knowledge from sampling and synopses. PVLDB ,\n11(9):1016–1028, 2018.\n[31] K. P. Murphy. Machine learning: a probabilistic perspective . MIT press,\n2012.\n[32] J. Neter, W. Wasserman, and M. H. Kutner. Applied linear regression\nmodels. 1989.\n[33] J. Ortiz, M. Balazinska, J. Gehrke, and S. S. Keerthi. Learning state\nrepresentations for query optimization with deep reinforcement learning.\narXiv preprint arXiv:1803.08604 , 2018.\n[34] V . Poosala, P. J. Haas, Y . E. Ioannidis, and E. J. Shekita. Improved\nhistograms for selectivity estimation of range predicates. In ACM Sigmod\nRecord , volume 25, 1996.\n[35] V . Poosala and Y . E. Ioannidis. Selectivity estimation without the\nattribute value independence assumption. In VLDB , volume 97, pages\n486–495, 1997.\n[36] S. Rifai, P. Vincent, X. Muller, X. Glorot, and Y . Bengio. Contractive\nauto-encoders: Explicit invariance during feature extraction. In ICML ,\npages 833–840, 2011.\n[37] A. J. Smola and B. Sch ¨olkopf. A tutorial on support vector regression.\nStatistics and computing , 14(3):199–222, 2004.\n[38] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overﬁtting.\nJMLR , 15(1):1929–1958, 2014.\n[39] N. Thaper, S. Guha, P. Indyk, and N. Koudas. Dynamic multidimensional\nhistograms. In SIGMOD , pages 428–439, 2002.\n[40] K. Tzoumas, A. Deshpande, and C. S. Jensen. Lightweight graphical\nmodels for selectivity estimation without independence assumptions.\nPVLDB , 4(11):852–863, 2011.\n[41] K. Tzoumas, T. Sellis, and C. S. Jensen. A reinforcement learning\napproach for adaptive query processing. History , 2008.\n[42] B. Uria, I. Murray, and H. Larochelle. NADE: the real-valued neural\nautoregressive density-estimator. CoRR , abs/1306.0186, 2013.\n[43] P. Vincent, H. Larochelle, Y . Bengio, and P. Manzagol. Extracting and\ncomposing robust features with denoising autoencoders. In ICML , 2008.\n[44] Z. Yang, E. Liang, A. Kamsetty, C. Wu, Y . Duan, X. Chen, P. Abbeel,\nJ. M. Hellerstein, S. Krishnan, and I. Stoica. Selectivity estimation with\ndeep likelihood models. arXiv preprint arXiv:1905.04278 , 2019.",
  "textLength": 64410
}