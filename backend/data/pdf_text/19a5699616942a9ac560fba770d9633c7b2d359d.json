{
  "paperId": "19a5699616942a9ac560fba770d9633c7b2d359d",
  "title": "Database Meets Deep Learning: Challenges and Opportunities",
  "pdfPath": "19a5699616942a9ac560fba770d9633c7b2d359d.pdf",
  "text": "Database Meets Deep Learning: Challenges and\nOpportunities\nWei Wangy, Meihui Zhangz, Gang Chenx,\nH. V. Jagadish#, Beng Chin Ooiy, Kian-Lee Tany\nyNational University of SingaporezBeijing Institute of Technology\nxZhejiang University#University of Michigan\nyfwangwei, ooibc, tankl g@comp.nus.edu.sgzmeihui zhang@bit.edu.cn\nxcg@zju.edu.cn#jag@umich.edu\nABSTRACT\nDeep learning has recently become very popular on ac-\ncount of its incredible success in many complex data-\ndriven applications, such as image classiﬁcation and speech\nrecognition. The database community has worked on\ndata-driven applications for many years, and therefore\nshould be playing a lead role in supporting this new\nwave. However, databases and deep learning are differ-\nent in terms of both techniques and applications. In this\npaper, we discuss research problems at the intersection\nof the two ﬁelds. In particular, we discuss possible im-\nprovements for deep learning systems from a database\nperspective, and analyze database applications that may\nbeneﬁt from deep learning techniques.\n1. INTRODUCTION\nIn recent years, we have witnessed the success of\nnumerous data-driven machine-learning-based ap-\nplications. This has prompted the database com-\nmunity to investigate the opportunities for integrat-\ning machine learning techniques in the design of\ndatabase systems and applications [84]. A branch of\nmachine learning, called deep learning [57, 38], has\nattracted worldwide interest in recent years due to\nits excellent performance in multiple areas including\nspeech recognition, image classi\fcation and natural\nlanguage processing (NLP). The foundation of deep\nlearning was established about twenty years ago in\nthe form of neural networks. Its recent resurgence is\nmainly fueled by three factors: immense computing\npower, which reduces the time to train and deploy\nnew models, e.g. Graphic Processing Unit (GPU)\nenables the training systems to run much faster\nthan those in the 1990s; massive (labeled) training\ndatasets (e.g. ImageNet) enable a more comprehen-\nsive knowledge of the domain to be acquired; new\ndeep learning models (e.g. AlexNet [55]) improve\nthe ability to capture data regularities.\nDatabase researchers have been working on sys-tem optimization and large scale data-driven ap-\nplications since 1970s, which are closely related to\nthe \frst two factors. It is natural to think about\nthe relationships between databases and deep learn-\ning. First, are there any insights that the database\ncommunity can o\u000ber to deep learning? It has been\nshown that larger training datasets and a deeper\nmodel structure improve the accuracy of deep learn-\ning models. However, the side e\u000bect is that the\ntraining becomes more costly. Approaches have been\nproposed to accelerate the training speed from both\nthe system perspective [12, 42, 18, 80, 2] and the\ntheory perspective [120, 27]. Since the database\ncommunity has rich experience with system opti-\nmization, it would be opportune to discuss the ap-\nplicability of database techniques for optimizing deep\nlearning systems. For example, distributed com-\nputing and memory management are key database\ntechnologies also central to deep learning.\nSecond, are there any deep learning techniques\nthat can be adapted for database problems? Deep\nlearning emerged from the machine learning and\ncomputer vision communities. It has been success-\nfully applied to other domains, like NLP [28]. How-\never, few studies have been conducted using deep\nlearning techniques for traditional database prob-\nlems. This is partially because traditional database\nproblems | like indexing, transaction and storage\nmanagement | involve less uncertainty, whereas\ndeep learning is good at predicting over uncertain\nevents. Nevertheless, there are problems in databases\nlike knowledge fusion [21] and crowdsourcing [79],\nwhich are probabilistic problems. It is possible to\napply deep learning techniques in these areas. We\nwill discuss speci\fc problems like querying interface,\nknowledge fusion, etc. in this paper.\nThe previous version [108] of this paper has ap-\npeared in SIGMOD Record. In this version, we ex-\ntend it to include the recent developments in this\n\feld and references to recent work.arXiv:1906.08986v2  [cs.DB]  19 Jan 2020\n\ninitialize  \nparameters  \ncompute  \ngradients  \nupdate  \nparameters  \nread mini -\nbatch data  Figure 1: Stochastic Gradient Descent.\nThe rest of this paper is organized as follows: Sec-\ntion 2 provides background information about deep\nlearning models and training algorithms; Section 3\ndiscusses the application of database techniques for\noptimizing deep learning systems. Section 4 de-\nscribes research problems in databases where deep\nlearning techniques may help to improve perfor-\nmance. Some \fnal thoughts are presented in Sec-\ntion 5.\n2. BACKGROUND\nDeep learning refers to a set of machine learn-\ning models which try to learn high-level abstrac-\ntions (or representations) of raw data through mul-\ntiple feature transformation layers. Large training\ndatasets and deep complex structures [8] enhance\nthe ability of deep learning models for learning ef-\nfective representations for tasks of interest. There\nare three popular categories of deep learning models\naccording to the types of connections between lay-\ners [57], namely feedforward models (direct connec-\ntion), energy models (undirected connection) and\nrecurrent neural networks (recurrent connection).\nFeedforward models, including Convolution Neural\nNetwork (CNN), propagate input features through\neach layer to extract high-level features. CNN is\nthe state-of-the-art model for many computer vi-\nsion tasks. Energy models, including Deep Belief\nNetwork (DBN) are typically used to pre-train other\nmodels, e.g., feedforward models. Recurrent Neu-\nral Network (RNN) is widely used for modeling se-\nquential data. Machine translation and language\nmodeling are popular applications of RNN.\nBefore deploying a deep learning model, the model\nparameters involved in the transformation layers\nneed to be trained. The training turns out to be a\nnumeric optimization procedure to \fnd parameter\nvalues that minimize the discrepancy (loss function)\nbetween the expected output and the real output.\nStochastic Gradient Descent (SGD) is the most widely\nused training algorithm. As shown in Figure 1,\nSGD initializes the parameters with random val-\nues, and then iteratively re\fnes them based on the\ncomputed gradients with respect to the loss func-\ntion. There are three commonly used algorithms\nfor gradient computation corresponding to the three\nmodel categories above: Back Propagation (BP),\nContrastive Divergence (CD) and Back Propaga-\nApproach B  \n•TBD \ninput  inner -\nproduct  sigmoid  loss \nDB W \nb data  \ndata  \ngradient  \ngradient  data  \ndata  Figure 2: Data \row of Back-Propagation.\ntion Through Time (BPTT). By regarding the lay-\ners of a neural net as nodes of a graph, these algo-\nrithms can be evaluated by traversing the graph in\ncertain sequences. For instance, the BP algorithm\nis illustrated in Figure 2, where a simple feedfor-\nward model is trained by traversing along the solid\narrows to compute the data (feature) of each layer,\nand along the dashed arrows to compute the gradi-\nent of each layer and each parameter ( Wandb).\n3. DATABASES TO DEEP LEARNING\nIn this section, we discuss the optimization tech-\nniques used in deep learning systems, and research\nopportunities from the perspective of databases.\n3.1 Stand-alone Training\nCurrently, the most e\u000bective approach for im-\nproving the training speed of deep learning mod-\nels is using Nvidia GPU with the cuDNN library.\nResearchers are also working on other hardware,\ne.g. FPGA [56]. Besides exploiting advancements\nin hardware technology, operation scheduling and\nmemory management are two important components\nto consider.\n3.1.1 Operation Scheduling\nTraining algorithms of deep learning models typ-\nically involve expensive linear algebra operations as\nshown in Figure 3, where the matrix W1 and W2\ncould be larger than 4096 \u00034096. Operation schedul-\ning is to \frst detect the data dependency of oper-\nations and then place the operations without de-\npendencies onto executors, e.g., CUDA streams and\nCPU threads. Taking the operations in Figure 3 as\nan example, a1 and a2 in Figure 3 could be com-\nputed in parallel because they have no dependen-\ncies. The \frst step could be done statically based\non data\row graph or dynamically [10] by analyzing\nthe orders of read and write operations. Databases\nalso have this kind of problems in optimizing trans-\n\nEfficiency optimization  \n●Improve the speed of DL on a single device (GPU or CPU device)  \n○All operations of one (BP) iteration compose a dataflow graph . \n○Existing systems either do static (Theano[12] and TensorFlow[13]) or dynamic (MxNet[14]) \ndependency analysis to parallelize  operations without data dependencies.  \n \n \n \n○Possible improvements:  \n■When there are limited resources, i.e, executors (CUDA streams), there could be \nmultiple ways of placing the operations onto the executors.  \n■Runtime optimization by 1) collecting the cost (i.e., FLOPS) of each operation and the \nhardware statistics 2) estimating the total cost of all plans;  \n a1=x*W1+b1  a2=x*W2+b2  \nx=sigmoid(x)  y=concatenate(a1, a2)  \n[12] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. J. Goodfellow, A. Bergeron, N. Bouchard, and Y. Bengio. Theano: new features and speed \nimprovements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.  \n[13] M. A. et al. TensorFlow: Large -scale machine learning on heterogeneous systems, 2015  \n[14] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C. Zhang, and Z. Zhang. Mxnet: An extensible and effici ent machine learning library \nfor heterogeneous distributed systems. CoRR, abs/1512.01274, 2015  \n Figure 3: Sample operations from a deep\nlearning model.\naction execution [119] and query plans. Those so-\nlutions should be considered for deep learning sys-\ntems. For instance, databases use cost models to\nestimate query plans. For deep learning, we may\nalso create a cost model to \fnd an optimal opera-\ntion placing strategy for the second step of opera-\ntion scheduling given a \fxed computing resources\nincluding executors and memory.\nRecent developments : Mirhoseini et al. [74]\npropose to optimize the placement of operations\non heterogeneous hardware devices (e.g., CPU and\nGPU) using reinforcement learning. Jia et al.[45,\n43] go beyond simple operation parallelism to con-\nsider parallelism from multiple dimensions together,\nincluding data samples and channels, operations,\nattributes and parameters. In addition, operation\nsubstitution has been studied in [44], which sub-\nstitutes the original operations with new ones that\nretain the semantics but lead to better overall e\u000e-\nciency. Operation fusing is one example. A cost-\nbased search algorithm is introduced to \fnd op-\ntimized computation graphs. Similar fusing tech-\nniques are applied in open-source libraries including\nTensor\row [2] and PyTorch [82].\n3.1.2 Memory Management\nDeep learning models are becoming larger and\nlarger, and already occupy a huge amount of mem-\nory space. For example, the VGG model [88] can-\nnot be trained on normal GPU cards due to mem-\nory size constraints. Many approaches have been\nproposed towards reducing memory consumption.\nShorter data representation, e.g. 16-bit \roat [15] is\nnow supported by CUDA. Memory sharing is an\ne\u000bective approach for memory saving [10]. Tak-\ning Figure 3 as an example, the input and out-\nput of the sigmoid function share the same vari-\nable and thus the same memory space. Such opera-\ntions are called `in-place' operations. Recently, two\napproaches were proposed to trade-o\u000b computationtime for memory. Swapping memory between GPU\nand CPU resolves the problem of small GPU mem-\nory and large model size by swapping variables out\nto CPU and then swapping back manually[16]. An-\nother approach drops some variables to free memory\nand recomputes them when necessary based on the\nstatic data\row graph[11].\nMemory management is a hot topic in the database\ncommunity with a signi\fcant amount of research\ntowards in-memory databases [92, 122], including\nlocality, paging and cache optimization. To elabo-\nrate more, the paging strategies could be useful for\ndeciding when and which variables to swap. In addi-\ntion, failure recovery in databases is similar to the\nidea of dropping and recomputing approach, and\nhence the logging techniques in databases could be\nconsidered. If all operations (and execution time)\nare logged, we can then do runtime analysis with-\nout the static data\row graph. Other techniques, in-\ncluding garbage collection and memory pool, would\nalso be useful for deep learning systems, especially\nfor GPU memory management.\nRecent developments : The recomputing tech-\nnique has been adopted in PyTorch [83]. Wang et\nal. [101] combines recomputing and swapping to\noptimize the memory of convolutional neural net-\nworks. Zhang et al. [124] propose a smart mem-\nory pool and automatic swapping strategy for deep\nneural networks to replace manual swapping in [16,\n101]. Cai et al. [7] propose to slice the model for\nreducing the memory and computational resource\nconsumption.\n3.2 Distributed Training\nDistributed training is a natural solution for ac-\ncelerating the training speed of deep learning mod-\nels. The parameter server architecture [18] is typi-\ncally used, in which the workers compute parameter\ngradients and the servers update the parameter val-\nues after receiving gradients from workers. There\nare two basic parallelism schemes for distributed\ntraining, namely, data parallelism and model paral-\nlelism. In data parallelism, each worker is assigned a\ndata partition and a model replica, while for model\nparallelism, each worker is assigned a partition of\nthe model and the whole dataset. Two recent sur-\nvey papers [72, 5] give a comprehensive introduc-\ntion about parallel and distributed deep learning.\nThe database community has a long history of work-\ning on distributed environment, ranging from par-\nallel databases [59] and peer-to-peer systems [99]\nto cloud computing [61]. We will discuss some re-\nsearch problems relevant to databases arising from\ndistributed training in the following paragraphs.\n\n3.2.1 Communication\nGiven that deep learning models have a large\nset of parameters, the communication overhead be-\ntween workers and servers is likely to be the bot-\ntleneck of a training system, especially when the\nworkers are running on GPUs which decrease the\ncomputation time. In addition, for large clusters,\nthe synchronization between workers also accounts.\nConsequently, it is important to investigate e\u000ecient\ncommunication protocols for both single-node mul-\ntiple GPU training and training over a large clus-\nter. Possible research directions include : a) com-\npressing the parameters and gradients before trans-\nferring [85]; b) organizing servers in an optimized\ntopology to reduce the communication burden of\neach single node, e.g., tree structure [32] and AllRe-\nduce structure [113] (all-to-all connection); c) using\nmore e\u000ecient networking hardware like RDMA [12].\nRecent developments : Gradient compression\nhas shown to be e\u000bective in reducing the commu-\nnication cost [47, 30, 95, 64, 112, 3, 47]. Various\ndecentralized communication frameworks [46, 110,\n94, 86, 1] have been proposed to replace the central-\nized parameter server framework which is prone to\ncommunication bottleneck at the server side. For\nexample, [29, 41, 73] use AllReduce to train large-\nscale networks over thousands of GPUs.\n3.2.2 Concurrency and Consistency\nConcurrency and consistency are traditional re-\nsearch problems in databases. For distributed train-\ning of deep learning models, they also matter. Cur-\nrently, both declarative programming (e.g., Theano\nand Tensor\row) and imperative programming (e.g.,\nCa\u000be and SINGA) have been adopted in existing\nsystems for concurrency implementation. Most deep\nlearning systems use threads and locks directly. Other\nconcurrency implementation methods like actor model\n(good at failure recovery), co-routine and communi-\ncating sequential processes have not been explored.\nSequential consistency (from synchronous train-\ning) and eventual consistency (from asynchronous\ntraining) are typically used for distributed deep learn-\ning. Both approaches have scalability issues [105].\nRecently, there are studies for training convex mod-\nels (deep learning models are non-linear and non-\nconvex) using a value bounded consistency model [111].\nResearchers are starting to investigate the in\ruence\nof consistency models on distributed training [32,\n33, 9]. There remains much research to be done on\nhow to provide \rexible consistency models for dis-\ntributed training, and how each consistency model\na\u000bects the scalability of the system, including com-\nmunication overhead.Recent developments : In recent papers and\nthe benchmark testing [13], synchronous training is\npreferable to asynchronous training [121, 125, 35]\nbecause the former one is more stable in terms of\nconvergence. With warm-up, layer-wise adaptive\nrate scaling for the learning rate [29], label smooth-\ning, etc., synchronous SGD can scale to over 2000\nGPUs [117, 41] without sacri\fcing accuracy. Typi-\ncally, they increase the batch size gradually from a\nfew thousands to tens of thousands. FlexPS [37] is a\nsystem that support such training schemes that in-\nvolve multiple stages. For very large models, pipeline\ntraining [77, 36] can be adopted, which partitions\nthe model. However, data parallelism is still more\npopular than model parallelism since it is easier to\nimplement and incurs less communication as well as\nsynchronization overhead.\n3.2.3 Fault Tolerance\nDatabases systems have good durability via log-\nging (e.g., command log) and checkpointing. Cur-\nrent deep learning systems recover the training from\ncrashes mainly based on checkpointing \fles [2]. How-\never, frequent checkpointing would incur vast over-\nhead. In contrast with database systems, which en-\nforce strict consistency in transactions, the SGD al-\ngorithm used by deep learning training systems can\ntolerate a certain degree of inconsistency. There-\nfore, logging is not a must. How to exploit the SGD\nproperties and system architectures to implement\nfault tolerance e\u000eciently is an interesting problem.\nConsidering that distributed training would repli-\ncate the model status, it is thus possible to recover\nfrom a replica instead of checkpointing \fles. Ro-\nbust frameworks (or concurrency model) like actor\nmodel, could be adopted to implement this kind of\nfailure recovery.\n3.3 Optimization Techniques in Existing Sys-\ntems\nA summary of existing systems in terms of the\nabove mentioned optimization aspects is listed in\nTable 1. Many researchers have done ad hoc opti-\nmization using Ca\u000be, including memory swapping\nand communication optimization. However, the of-\n\fcial version is not well optimized. Similarly, Torch\nitself provides limited support for distributed train-\ning. MXNet has optimization for both memory and\noperations scheduling. Theano is typically used for\nstand-alone training. Tensor\row is potential for\nthe aforementioned static optimization based on the\ndata\row graph.\nWe are optimizing the Apache incubator SINGA\nsystem [80] starting from version 1.0. For stand-\n\nTable 1: Summary of optimization techniques used in existing systems as of July 18, 2016.\nSINGA Ca\u000be [42] MXNet [10] Tensor\row [2]Theano [4]Torch [14]\n1. operation scheduling X x X - - x\n2. memory management d+a+p i d+s p p -\n3. parallelism d + m d d + m d + m - d + m\n4. consistency s+a+h s/a s+a+h s+a+h - s\n-: unknown 1. x: not available: X: available 2. d: dynamic; a: swap; p:memory pool; i: in-place operation; s: static;\n3. d: data parallelism; m: model parallelism; 4. s: synchronous; a: asynchronous; h:hybrid\nalone training, cost models are explored for runtime\noperation scheduling. Memory optimization includ-\ning dropping, swapping and garbage collection with\nmemory pool will be implemented. OpenCL is sup-\nported to run SINGA on a wide range of hardware\nincluding GPU, FPGA and ARM. For distributed\ntraining, SINGA (V0.3) has done much work on\n\rexible parallelism and consistency; hence the fo-\ncus would be on optimization of communication and\nfault-tolerance, which are missing in almost all sys-\ntems.\n4. DEEP LEARNING TO DATABASES\nDeep learning applications, such as computer\nvision and NLP, may appear very di\u000berent from\ndatabase applications. However, the core idea of\ndeep learning, known as feature (or representation)\nlearning, is applicable to a wide range of applica-\ntions. Intuitively, once we have e\u000bective represen-\ntations for entities, e.g., images, words, table rows\nor columns, we can compute entity similarity, per-\nform clustering, train prediction models, and re-\ntrieve data with di\u000berent modalities [107, 106] etc.\nWe shall highlight a few deep learning models that\ncould be adapted for database applications below.\n4.1 Query Interface\nNatural language query interfaces have been at-\ntempted for decades [60], because of their great de-\nsirability, particularly for non-expert database users.\nHowever, it is challenging for database systems to\ninterpret (or understand) the semantics of natural\nlanguage queries. Recently, deep learning models\nhave achieved state-of-the-art performance for NLP\ntasks [28]. Moreover, RNN has been shown to be\nable to learn structured output [91, 97]. As one so-\nlution, we can apply RNN models for parsing nat-\nural language queries to generate SQL queries, and\nre\fne it using existing database approaches. The\nchallenge is that a large amount of (labeled) train-\ning samples is required to train the model. One\npossible solution is to train a baseline model with a\nsmall dataset, and gradually re\fning it with users'\nfeedback. For instance, users could help correct thegenerated SQL query, and these feedback essentially\nserve as labeled data for subsequent training.\nRecent developments : Multiple annotated datasets\nthat consist of text query and SQL query pairs have\nbeen created using templates [126, 6] and user feed-\nback [40]. State-of-the-art solutions over the Wik-\niSQL dataset [126] are listed here1. The solutions [126,\n40, 24, 116] generally extend the sequence-to-sequence\nmodel to encode the text query and then generate\nthe SQL query via the decoder. Domain knowledge\nlike the SQL grammar is exploited.\n4.2 Query Plans\nQuery plan optimization is a traditional database\nproblem. Most current database systems use com-\nplex heuristic and cost models to generate the query\nplan. According to [34], each query plan of a para-\nmetric SQL query template has an optimality re-\ngion. As long as the parameters of the SQL query\nare within this region, the optimal query plan does\nnot change. In other words, query plans are in-\nsensitive to small variations of the input parame-\nters. Therefore, we can train a query planner which\nlearns from a set of pairs of SQL queries and opti-\nmal plans to generate (similar) plans for new (sim-\nilar) queries. To elaborate more, we can learn a\nRNN model that accepts the SQL query elements\nand meta-data (like bu\u000ber size and primary key) as\ninput, and generates a tree structure [97] represent-\ning the query plan. Reinforcement learning (like Al-\nphaGo [87]) could also be applied to train the model\non-line using the execution time and memory foot-\nprint as the reward. Note that approaches purely\nbased on deep learning models may not be very ef-\nfective. First, the query plan is generated based\non probability, which is likely to have grammar er-\nrors. Second, the training dataset may not be com-\nprehensive to include all query patterns, e.g., some\npredicates could be missing in the training datasets.\nTo solve these problems, a better approach would\nbe combining database solutions and deep learn-\ning, e.g. using some heuristics to check and correct\ngrammar errors.\n1https://paperswithcode.com/task/text-to-sql\n\nRecent developments : Recently, there has been\nan increasing trend in applying deep learning tech-\nniques for optimizing database systems, including\nnot only query plan optimization but also data ac-\ncess (i.e. indexing) optimization and database con-\n\fguration tuning [123, 62]. Speci\fcally, researchers\nhave proposed to optimize the query plan by im-\nproving the join order selection [54, 69, 96], query\nperformance prediction [70], cardinality estimation\nfor join queries [65, 90, 51, 81, 118], and index rec-\nommendation [19] or search [114]. Neo [68] gener-\nates the query plan directly by deciding the join\norder, operator, and index selection together. Deep\nreinforcement learning is the key technique support-\ning these optimizations. Some challenges and pos-\nsible solutions are discussed in the vision paper [71].\nIn terms of data access optimization, Kraska et al.[53]\npropose a learned index for read-only, in-memory\ndatabase systems. It uses neural networks to map\nthe key to the location of the record. Subsequent\nworks have extended it for multi-dimensional in-\ndex [78], updatable index [20], dynamic workloads [93],\nand accessing data on disk [49, 26, 23]. To go one\nstep further, SageDB [52] puts forth a vision where\nevery component (such as query plan, data access,\nand query execution [127]) of a database system is\noptimized via machine learning models against the\ndata distribution and (query) workload [39].\n4.3 Crowdsourcing and Knowledge Bases\nMany crowdsourcing [115] and knowledge base [21]\napplications involve entity extraction, disambigua-\ntion and fusion problems, where the entity could\nbe a row of a database, a node in a graph, etc.\nWith the advancements of deep learning models in\nNLP [28], it is opportune to consider deep learn-\ning for these problems. For example, we can learn\nrepresentations for entities and then do entity rela-\ntionship reasoning [89] and similarity calculation.\nRecent developments : DeepER [22] exploits\nLSTM models to learn tuple embedding for entity\nresolution. IDEL [50] implements neural entity-\nlinking in MonetDB, where the entities are embed-\nded. Deep learning models like CNN and attention\nmodelling have been applied for concept linking [25,\n17]. Mudgal et al.[76] evaluate four di\u000berent deep\nlearning models for entity matching problems. This\nwebsite2keeps track of the recent papers [103] on\nknowledge representation learning and embedding.\n4.4 Spatial and Temporal Data\nSpatial and temporal data are common data types\nin database systems [31], and are commonly used\n2https://github.com/thunlp/KRLPapersfor trend analysis, progression modeling and predic-\ntive analytics. Spatial data is typically processed by\nmapping moving objects into rectangular blocks. If\nwe regard each block as a pixel of one image, then\ndeep learning models, e.g., CNN, could be exploited\nto extract the spatial locality between nearby blocks.\nFor instance, if we have the real-time location data\n(e.g., GPS data) of moving objects, we could learn a\nCNN model to capture the density relationships of\nnearby areas for predicting the tra\u000ec congestion for\na future time point. When temporal data is mod-\neled as features over a time matrix, deep learning\nmodels, e.g. RNN, can be designed to model time\ndependency and predict the occurrence in a future\ntime point. A particular example would be disease\nprogression modeling [75] based on historical med-\nical records, where doctors would want to estimate\nthe onset of certain severity of a known disease. In\nfact, most healthcare data is time-serise data, and\nthus deep learning can make great contribution in\nhealthcare data analysis [58, 66].\nRecent developments : Deep learning models\nincluding CNN and RNN have been applied in vari-\nous spatial-temporal problems, including tra\u000ec \row\nprediction [67, 48], travel time estimation [100, 63,\n109], driver behavior analysis [102], geospatial ag-\ngregation querying [98], etc. A comprehensive sur-\nvey of the recent progress of applying deep learning\nfor spatial-temporal data is presented in [104].\n5. CONCLUSIONS\nIn this paper, we have discussed databases and\ndeep learning. Databases have many techniques for\noptimizing system performance, while deep learn-\ning is good at learning e\u000bective representation for\ndata-driven applications. We note that these two\n\\di\u000berent\" areas share some common techniques for\nimproving the system performance, such as memory\noptimization and parallelism. We have discussed\nsome possible improvements for deep learning sys-\ntems using database techniques, and research prob-\nlems applying deep learning techniques in database\napplications. To make the database systems more\nautonomic, with the ability to learn and optimize,\nand to support complex analytics and predictions\nbeyond data aggregation, we foresee a seamless inte-\ngration of ML/DL and database technologies. With\nthe implementation of 5G mobility network, we fore-\nsee the distribution of databases, training and infer-\nence at the edge devices, which will lead to further\nintegration and adaptation of technologies. Let us\nnot miss the opportunity to contribute to the exist-\ning challenges ahead!\n\n6. ACKNOWLEDGEMENT\nWe would like to thank Divesh Srivastava for his\nvaluable comments. This work was supported by\nthe National Research Foundation, Prime Minis-\nter's O\u000ece, Singapore, under its Competitive Re-\nsearch Programme (CRP Award No. NRF-CRP8-\n2011-08), and Singapore Ministry of Education Aca-\ndemic Research Fund Tier 3 under MOEs o\u000ecial\ngrant number MOE2017-T3-1-007. Meihui Zhang\nwas supported by China Thousand Talents Program\nfor Young Professionals (3070011181811).\n7. REFERENCES\n[1] Nvidia collective communications library (nccl).\nhttps://developer.nvidia.com/nccl.\n[2] M. Abadi, A. Agarwal, P. Barham, E. Brevdo,\nZ. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean,\nM. Devin, S. Ghemawat, I. Goodfellow, A. Harp,\nG. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser,\nM. Kudlur, J. Levenberg, D. Man\u0013 e, R. Monga,\nS. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens,\nB. Steiner, I. Sutskever, K. Talwar, P. Tucker,\nV. Vanhoucke, V. Vasudevan, F. Vi\u0013 egas, O. Vinyals,\nP. Warden, M. Wattenberg, M. Wicke, Y. Yu, and\nX. Zheng. TensorFlow: Large-scale machine learning\non heterogeneous systems, 2015.\n[3] D. Alistarh, J. Li, R. Tomioka, and M. Vojnovic.\nQSGD: randomized quantization for\ncommunication-optimal stochastic gradient descent.\nCoRR , abs/1610.02132, 2016.\n[4] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. J.\nGoodfellow, A. Bergeron, N. Bouchard, and\nY. Bengio. Theano: new features and speed\nimprovements. Deep Learning and Unsupervised\nFeature Learning NIPS 2012 Workshop, 2012.\n[5] T. Ben-Nun and T. Hoe\rer. Demystifying parallel and\ndistributed deep learning: An in-depth concurrency\nanalysis. ACM Comput. Surv. , 52(4), 2019.\n[6] R. Cai, B. Xu, Z. Zhang, X. Yang, Z. Li, and\nZ. Liang. An encoder-decoder framework translating\nnatural language to database queries. In IJCAI , 2018.\n[7] S. Cai, G. Chen, B. C. Ooi, and J. Gao. Model slicing\nfor supporting complex analytics with elastic\ninference cost and resource constraints. CoRR ,\nabs/1904.01831, 2019.\n[8] S. Cai, Y. Shu, W. Wang, and B. C. Ooi. Isbnet:\nInstance-aware selective branching network. CoRR ,\nabs/1905.04849, 2019.\n[9] J. Chen, R. Monga, S. Bengio, and R. J\u0013 ozefowicz.\nRevisiting distributed synchronous SGD. CoRR ,\nabs/1604.00981, 2016.\n[10] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang,\nT. Xiao, B. Xu, C. Zhang, and Z. Zhang. MXNet: A\n\rexible and e\u000ecient machine learning library for\nheterogeneous distributed systems. CoRR ,\nabs/1512.01274, 2015.\n[11] T. Chen, B. Xu, C. Zhang, and C. Guestrin. Training\ndeep nets with sublinear memory cost. CoRR ,\nabs/1604.06174, 2016.\n[12] A. Coates, B. Huval, T. Wang, D. J. Wu, B. C.\nCatanzaro, and A. Y. Ng. Deep learning with COTS\nHPC systems. In ICML , pages 1337{1345, 2013.\n[13] C. A. Coleman, D. Narayanan, D. Kang, T. J. Zhao,\nJ. Zhang, L. Nardi, P. Bailis, K. Olukotun, C. R\u0013 e,\nand M. A. Zaharia. Dawnbench : An end-to-end deep\nlearning benchmark and competition. In NIPS ML\nSystems Workshop , 2017.[14] R. Collobert, K. Kavukcuoglu, and C. Farabet.\nTorch7: A matlab-like environment for machine\nlearning. In BigLearn, NIPS Workshop , 2011.\n[15] M. Courbariaux, Y. Bengio, and J.-P. David. Low\nprecision arithmetic for deep learning. CoRR ,\nabs/1412.7024, 2014.\n[16] H. Cui, H. Zhang, G. R. Ganger, P. B. Gibbons, and\nE. P. Xing. Geeps: scalable deep learning on\ndistributed gpus with a gpu-specialized parameter\nserver. In EuroSys 2016 , pages 4:1{4:16, 2016.\n[17] J. Dai, M. Zhang, G. Chen, J. Fan, K. Y. Ngiam, and\nB. C. Ooi. Fine-grained concept linking using neural\nnetworks in healthcare. In SIGMOD , pages 51{66.\nACM, 2018.\n[18] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin,\nQ. V. Le, M. Z. Mao, M. Ranzato, A. W. Senior,\nP. A. Tucker, K. Yang, and A. Y. Ng. Large scale\ndistributed deep networks. In NIPS , pages 1232{1240,\n2012.\n[19] B. Ding, S. Das, R. Marcus, W. Wu, S. Chaudhuri,\nand V. R. Narasayya. Ai meets ai: Leveraging query\nexecutions to improve index recommendations. In\nSIGMOD , pages 1241{1258, 2019.\n[20] J. Ding, U. F. Minhas, H. Zhang, Y. Li, C. Wang,\nB. Chandramouli, J. Gehrke, D. Kossmann, and\nD. Lomet. Alex: An updatable adaptive learned\nindex, 2019.\n[21] X. L. Dong, E. Gabrilovich, G. Heitz, W. Horn,\nK. Murphy, S. Sun, and W. Zhang. From data fusion\nto knowledge fusion. PVLDB , 7(10):881{892, 2014.\n[22] M. Ebraheem, S. Thirumuruganathan, S. R. Joty,\nM. Ouzzani, and N. Tang. Deeper - deep entity\nresolution. CoRR , abs/1710.00597, 2017.\n[23] P. Ferragina and G. Vinciguerra. The pgm-index: a\nmulticriteria, compressed and learned approach to\ndata indexing, 2019.\n[24] C. Finegan-Dollak, J. K. Kummerfeld, L. Zhang,\nK. Ramanathan, S. Sadasivam, R. Zhang, and\nD. Radev. Improving text-to-SQL evaluation\nmethodology. In ACL, pages 351{360, Melbourne,\nAustralia, July 2018.\n[25] M. Francis-Landau, G. Durrett, and D. Klein.\nCapturing semantic similarity for entity linking with\nconvolutional neural networks. In NAACL-HLT ,\npages 1256{1261, San Diego, California, June 2016.\n[26] A. Galakatos, M. Markovitch, C. Binnig, R. Fonseca,\nand T. Kraska. Fiting-tree: A data-aware index\nstructure. In SIGMOD , pages 1189{1206, 2019.\n[27] J. Gao, H. V. Jagadish, and B. C. Ooi. Active\nsampler: Light-weight accelerator for complex data\nanalytics at scale. CoRR , abs/1512.03880, 2015.\n[28] Y. Goldberg. A primer on neural network models for\nnatural language processing. CoRR , abs/1510.00726,\n2015.\n[29] P. Goyal, P. Doll\u0013 ar, R. B. Girshick, P. Noordhuis,\nL. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and\nK. He. Accurate, large minibatch SGD: training\nimagenet in 1 hour. CoRR , abs/1706.02677, 2017.\n[30] D. Grubic, L. K. Tam, D. Alistarh, and C. Zhang.\nSynchronous multi-gpu deep learning with\nlow-precision communication: An experimental study.\nInEDBT , pages 145{156. OpenProceedings, 2018.\n[31] C. Guo, C. S. Jensen, and B. Yang. Towards total\ntra\u000ec awareness. ACM SIGMOD Record ,\n43(3):18{23, 2014.\n[32] S. Gupta, W. Zhang, and F. Wang. Model accuracy\nand runtime tradeo\u000b in distributed deep learning: A\nsystematic study. In ICDM , pages 171{180. IEEE,\n2016.\n[33] S. Hadjis, C. Zhang, I. Mitliagkas, and C. R\u0013 e.\n\nOmnivore: An optimizer for multi-device deep\nlearning on cpus and gpus. CoRR , abs/1606.04487,\n2016.\n[34] J. R. Haritsa. The picasso database query optimizer\nvisualizer. PVLDB , 3(1-2):1517{1520, 2010.\n[35] A. Harlap, H. Cui, W. Dai, J. Wei, G. R. Ganger,\nP. B. Gibbons, G. A. Gibson, and E. P. Xing.\nAddressing the straggler problem for iterative\nconvergent parallel ml. In SoCC , pages 98{111, 2016.\n[36] Y. Huang, Y. Cheng, D. Chen, H. Lee, J. Ngiam,\nQ. V. Le, and Z. Chen. Gpipe: E\u000ecient training of\ngiant neural networks using pipeline parallelism.\nCoRR , abs/1811.06965, 2018.\n[37] Y. Huang, T. Jin, Y. Wu, Z. Cai, X. Yan, F. Yang,\nJ. Li, Y. Guo, and J. Cheng. Flexps: Flexible\nparallelism control in parameter server architecture.\nPVLDB , 11:566{579, 2018.\n[38] Y. B. Ian Goodfellow and A. Courville. Deep\nlearning. Book in preparation for MIT Press, 2016.\n[39] S. Idreos and T. Kraska. From auto-tuning one size\n\fts all to self-designed and learned data-intensive\nsystems. In SIGMOD , pages 2054{2059, 2019.\n[40] S. Iyer, I. Konstas, A. Cheung, J. Krishnamurthy, and\nL. Zettlemoyer. Learning a neural semantic parser\nfrom user feedback. CoRR , abs/1704.08760, 2017.\n[41] X. Jia, S. Song, W. He, Y. Wang, H. Rong, F. Zhou,\nL. Xie, Z. Guo, Y. Yang, L. Yu, T. Chen, G. Hu,\nS. Shi, and X. Chu. Highly scalable deep learning\ntraining system with mixed-precision: Training\nimagenet in four minutes. CoRR , abs/1807.11205,\n2018.\n[42] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev,\nJ. Long, R. Girshick, S. Guadarrama, and T. Darrell.\nCa\u000be: Convolutional architecture for fast feature\nembedding. arXiv:1408.5093 , 2014.\n[43] Z. Jia, S. Lin, C. R. Qi, and A. Aiken. Exploring\nhidden dimensions in parallelizing convolutional\nneural networks. CoRR , abs/1802.04924, 2018.\n[44] Z. Jia, J. O. Thomas, T. Warszawski, M. Gao, M. A.\nZaharia, and A. H. Aiken. Optimizing dnn\ncomputation with relaxed graph substitutions. In\nSysML , 2019.\n[45] Z. Jia, M. Zaharia, and A. Aiken. Beyond data and\nmodel parallelism for deep neural networks. In\nSysML , 2019.\n[46] J. Jiang, B. Cui, C. Zhang, and L. Yu.\nHeterogeneity-aware distributed parameter servers. In\nSIGMOD , pages 463{478. ACM, 2017.\n[47] J. Jiang, F. Fu, T. Yang, and B. Cui. Sketchml:\nAccelerating distributed machine learning with data\nsketches. In SIGMOD , pages 1269{1284, 2018.\n[48] R. Jiang, X. Song, Z. Fan, T. Xia, Q. Chen,\nS. Miyazawa, and R. Shibasaki.\nDeepUrbanMomentum: An online deep-learning\nsystem for short-term urban mobility prediction. In\nAAAI , 2018.\n[49] A. Kakaraparthy, J. M. Patel, K. Park, and B. Kroth.\nOptimizing databases by learning hidden parameters\nof solid state drives. PVLDB , 13(4):519{532, 2019.\n[50] T. Kilias, A. Lser, F. Gers, Y. Zhang,\nR. Koopmanschap, and M. Kersten. Idel: In-database\nneural entity linking. In IEEE BigComp , pages 1{8,\n2019.\n[51] A. Kipf, D. Vorona, J. M uller, T. Kipf, B. Radke,\nV. Leis, P. Boncz, T. Neumann, and A. Kemper.\nEstimating cardinalities with deep sketches. In\nSIGMOD , pages 1937{1940, 2019.\n[52] T. Kraska, M. Alizadeh, A. Beutel, E. H. hsin Chi,\nA. Kristo, G. Leclerc, S. Madden, H. Mao, and\nV. Nathan. Sagedb: A learned database system. InCIDR , 2019.\n[53] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and\nN. Polyzotis. The case for learned index structures. In\nSIGMOD , pages 489{504, 2018.\n[54] S. Krishnan, Z. Yang, K. Goldberg, J. M. Hellerstein,\nand I. Stoica. Learning to optimize join queries with\ndeep reinforcement learning. CoRR , abs/1808.03196,\n2018.\n[55] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\nImagenet classi\fcation with deep convolutional neural\nnetworks. In NIPS , pages 1097{1105, 2012.\n[56] G. Lacey, G. W. Taylor, and S. Areibi. Deep learning\non fpgas: Past, present, and future. CoRR ,\nabs/1602.04283, 2016.\n[57] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning.\nNature , 521(7553):436{444, 2015.\n[58] C. Lee, Z. Luo, K. Y. Ngiam, M. Zhang, K. Zheng,\nG. Chen, B. C. Ooi, and W. L. J. Yip. Big Healthcare\nData Analytics: Challenges and Applications , pages\n11{41. Springer International Publishing, Cham,\n2017.\n[59] M. L. Lee, M. Kitsuregawa, B. C. Ooi, K.-L. Tan, and\nA. Mondal. Towards self-tuning data placement in\nparallel database systems. In ACM SIGMOD Record ,\nvolume 29, pages 225{236. ACM, 2000.\n[60] F. Li and H. Jagadish. Constructing an interactive\nnatural language interface for relational databases.\nPVLDB , 8(1):73{84, 2014.\n[61] F. Li, B. C. Ooi, M. T. Ozsu, and S. Wu. Distributed\ndata management using mapreduce. ACM Comput.\nSurv. , 46(3):31:1{31:42, 2014.\n[62] G. Li, X. Zhou, S. Li, and B. Gao. Qtune: A\nquery-aware database tuning system with deep\nreinforcement learning. PVLDB , 12(12):2118{2130,\n2019.\n[63] Y. Li, K. Fu, Z. Wang, C. Shahabi, J. Ye, and Y. Liu.\nMulti-task representation learning for travel time\nestimation. In KDD , 2018.\n[64] Y. Lin, S. Han, H. Mao, Y. Wang, and W. J. Dally.\nDeep gradient compression: Reducing the\ncommunication bandwidth for distributed training.\nCoRR , abs/1712.01887, 2017.\n[65] H. Liu, M. Xu, Z. Yu, V. Corvinelli, and C. Zuzarte.\nCardinality estimation using neural networks. In\nCCSE , CASCON '15, pages 53{59, Riverton, NJ,\nUSA, 2015. IBM Corp.\n[66] Z. Luo, S. Cai, J. Gao, M. Zhang, K. Y. Ngiam,\nG. Chen, and W. Lee. Adaptive lightweight\nregularization tool for complex analytics. In ICDE ,\npages 485{496, 2018.\n[67] Y. Ma, X. Zhu, S. Zhang, R. Yang, W. Wang, and\nD. Manocha. Tra\u000ecpredict: Trajectory prediction for\nheterogeneous tra\u000ec-agents. CoRR , abs/1811.02146,\n2019.\n[68] R. Marcus, P. Negi, H. Mao, C. Zhang, M. Alizadeh,\nT. Kraska, O. Papaemmanouil, and N. Tatbul. Neo:\nA learned query optimizer. PVLDB ,\n12(11):1705{1718, 2019.\n[69] R. Marcus and O. Papaemmanouil. Deep\nreinforcement learning for join order enumeration. In\nProceedings of the First International Workshop on\nExploiting Arti\fcial Intelligence Techniques for Data\nManagement, aiDM@SIGMOD 2018, Houston, TX,\nUSA, June 10, 2018 , pages 3:1{3:4, 2018.\n[70] R. Marcus and O. Papaemmanouil. Plan-structured\ndeep neural network models for query performance\nprediction. PVLDB , 12(11):1733{1746, 2019.\n[71] R. Marcus and O. Papaemmanouil. Towards a\nhands-free query optimizer through deep learning.\nCIDR , 2019.\n\n[72] R. Mayer and H.-A. Jacobsen. Scalable deep learning\non distributed infrastructures: Challenges, techniques\nand tools. ArXiv , abs/1903.11314, 2019.\n[73] H. Mikami, H. Suganuma, P. U.-Chupala, Y. Tanaka,\nand Y. Kageyama. Imagenet/resnet-50 training in 224\nseconds. CoRR , abs/1811.05233, 2018.\n[74] A. Mirhoseini, H. Pham, Q. V. Le, B. Steiner,\nR. Larsen, Y. Zhou, N. Kumar, M. Norouzi,\nS. Bengio, and J. Dean. Device placement\noptimization with reinforcement learning. In ICML ,\npages 2430{2439, 2017.\n[75] D. R. Mould. Models for disease progression: New\napproaches and uses. Clinical Pharmacology &\nTherapeutics , 92(1):125{131, 2012.\n[76] S. Mudgal, H. Li, T. Rekatsinas, A. Doan, Y. Park,\nG. Krishnan, R. Deep, E. Arcaute, and\nV. Raghavendra. Deep learning for entity matching:\nA design space exploration. In SIGMOD , pages\n19{34, New York, NY, USA, 2018. ACM.\n[77] D. Narayanan, A. Harlap, A. Phanishayee,\nV. Seshadri, N. R. Devanur, G. R. Ganger, P. B.\nGibbons, and M. Zaharia. Pipedream: Generalized\npipeline parallelism for dnn training. In SOSP , pages\n1{15, 2019.\n[78] V. Nathan, J. Ding, M. Alizadeh, and T. Kraska.\nLearning multi-dimensional indexes, 2019.\n[79] B. C. Ooi, K. Tan, Q. T. Tran, J. W. L. Yip,\nG. Chen, Z. J. Ling, T. Nguyen, A. K. H. Tung, and\nM. Zhang. Contextual crowd intelligence. SIGKDD\nExplorations , 16(1):39{46, 2014.\n[80] B. C. Ooi, K.-L. Tan, S. Wang, W. Wang, Q. Cai,\nG. Chen, J. Gao, Z. Luo, A. K. H. Tung, Y. Wang,\nZ. Xie, M. Zhang, and K. Zheng. SINGA: A\ndistributed deep learning platform. In ACM\nMultimedia , 2015.\n[81] J. Ortiz, M. Balazinska, J. Gehrke, and S. S. Keerthi.\nAn empirical analysis of deep learning for cardinality\nestimation. CoRR , abs/1905.06425, 2019.\n[82] A. Paszke, S. Gross, S. Chintala, and G. Chanan.\nPytorch: Tensors and dynamic neural networks in\npython with strong gpu acceleration. PyTorch:\nTensors and dynamic neural networks in Python with\nstrong GPU acceleration , 6, 2017.\n[83] G. Pleiss, D. Chen, G. Huang, T. Li, L. van der\nMaaten, and K. Q. Weinberger. Memory-e\u000ecient\nimplementation of densenets. CoRR , abs/1707.06990,\n2017.\n[84] C. R\u0013 e, D. Agrawal, M. Balazinska, M. I. Cafarella,\nM. I. Jordan, T. Kraska, and R. Ramakrishnan.\nMachine learning and databases: The sound of things\nto come or a cacophony of hype? In SIGMOD , pages\n283{284, 2015.\n[85] F. Seide, H. Fu, J. Droppo, G. Li, and D. Yu. 1-bit\nstochastic gradient descent and its application to\ndata-parallel distributed training of speech dnns. In\nINTERSPEECH , pages 1058{1062, 2014.\n[86] A. Sergeev and M. D. Balso. Horovod: fast and easy\ndistributed deep learning in tensor\row. CoRR ,\nabs/1802.05799, 2018.\n[87] D. Silver and et al. Mastering the game of go with\ndeep neural networks and tree search. Nature ,\n529(7587):484{489, 2016.\n[88] K. Simonyan and A. Zisserman. Very deep\nconvolutional networks for large-scale image\nrecognition. CoRR , abs/1409.1556, 2014.\n[89] R. Socher, D. Chen, C. D. Manning, and A. Ng.\nReasoning with neural tensor networks for knowledge\nbase completion. In NIPS , pages 926{934, 2013.\n[90] J. Sun and G. Li. An end-to-end learning-based cost\nestimator. PVLDB , 13(3):307{319, 2019.[91] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to\nsequence learning with neural networks. In NIPS ,\npages 3104{3112, 2014.\n[92] K.-L. Tan, Q. Cai, B. C. Ooi, W.-F. Wong, C. Yao,\nand H. Zhang. In-memory databases: Challenges and\nopportunities from software and hardware\nperspectives. ACM SIGMOD Record , 44(2):35{40,\n2015.\n[93] C. Tang, Z. Dong, M. Wang, Z. Wang, and H. Chen.\nLearned indexes for dynamic workloads. CoRR ,\nabs/1902.00655, 2019.\n[94] H. Tang, S. Gan, C. Zhang, T. Zhang, and J. Liu.\nCommunication compression for decentralized\ntraining. In NIPS , pages 7663{7673, 2018.\n[95] H. Tang, C. Yu, X. Lian, T. Zhang, and J. Liu.\nDoubleSqueeze: Parallel stochastic gradient descent\nwith double-pass error-compensated compression. In\nICML , pages 6155{6165, 2019.\n[96] I. Trummer, S. Moseley, D. Maram, S. Jo, and\nJ. Antonakakis. Skinnerdb: Regret-bounded query\nevaluation via reinforcement learning. Proc. VLDB\nEndow. , 11(12):2074{2077, Aug. 2018.\n[97] O. Vinyals, L. Kaiser, T. Koo, S. Petrov, I. Sutskever,\nand G. Hinton. Grammar as a foreign language.\narXiv:1412.7449 , 2014.\n[98] D. Vorona, A. Kipf, T. Neumann, and A. Kemper.\nDeepSPACE: Approximate Geospatial Query\nProcessing with Deep Learning. arXiv e-prints , Jun\n2019.\n[99] Q. H. Vu, M. Lupu, and B. C. Ooi. Peer-to-peer\ncomputing . Springer, 2010.\n[100] D. Wang, J. Zhang, W. Cao, J. Li, and Y. Zheng.\nWhen will you arrive? estimating travel time based\non deep neural networks. In AAAI , 2018.\n[101] L. Wang, J. Ye, Y. Zhao, W. Wu, A. Li, S. L. Song,\nZ. Xu, and T. Kraska. Superneurons: Dynamic gpu\nmemory management for training deep neural\nnetworks. In ACM SIGPLAN Notices , volume 53,\npages 41{53. ACM, 2018.\n[102] P. Wang, Y. Fu, J. Zhang, P. Wang, Y. Zheng, and\nC. C. Aggarwal. You are how you drive: Peer and\ntemporal-aware representation learning for driving\nbehavior analysis. In KDD , 2018.\n[103] Q. Wang, Z. Mao, B. Wang, and L. Guo. Knowledge\ngraph embedding: A survey of approaches and\napplications. IEEE Trans. Knowl. Data Eng. ,\n29(12):2724{2743, Dec 2017.\n[104] S. Wang, J. Cao, and P. S. Yu. Deep learning for\nspatio-temporal data mining: A survey. CoRR ,\nabs/1906.04928, 2019.\n[105] W. Wang, G. Chen, T. T. A. Dinh, J. Gao, B. C. Ooi,\nK.-L. Tan, and S. Wang. SINGA: Putting deep\nlearning in the hands of multimedia users. In ACM\nMultimedia , 2015.\n[106] W. Wang, B. C. Ooi, X. Yang, D. Zhang, and\nY. Zhuang. E\u000bective multi-modal retrieval based on\nstacked auto-encoders. PVLDB , 7(8):649{660, 2014.\n[107] W. Wang, X. Yang, B. C. Ooi, D. Zhang, and\nY. Zhuang. E\u000bective deep learning-based multi-modal\nretrieval. The VLDB Journal , pages 1{23, 2015.\n[108] W. Wang, M. Zhang, G. Chen, H. V. Jagadish, B. C.\nOoi, and K.-L. Tan. Database meets deep learning:\nChallenges and opportunities. SIGMOD Rec. ,\n45(2):17{22, Sept. 2016.\n[109] Z. Wang, K. Fu, and J. Ye. Learning to estimate the\ntravel time. In KDD , 2018.\n[110] P. Watcharapichat, V. L. Morales, R. C. Fernandez,\nand P. Pietzuch. Ako: Decentralised deep learning\nwith partial gradient exchange. In SoCC , pages\n84{97, 2016.\n\n[111] J. Wei, W. Dai, A. Qiao, Q. Ho, H. Cui, G. R.\nGanger, P. B. Gibbons, G. A. Gibson, and E. P. Xing.\nManaged communication and consistency for fast\ndata-parallel iterative analytics. In SoCC , pages\n381{394, 2015.\n[112] W. Wen, C. Xu, F. Yan, C. Wu, Y. Wang, Y. Chen,\nand H. Li. Terngrad: Ternary gradients to reduce\ncommunication in distributed deep learning. CoRR ,\nabs/1705.07878, 2017.\n[113] R. Wu, S. Yan, Y. Shan, Q. Dang, and G. Sun. Deep\nimage: Scaling up image recognition. CoRR ,\nabs/1501.02876, 2015.\n[114] S. Wu, X. Yu, G. Chen, Y. Gao, X. Feng, and\nW. Cao. Progressive neural index search for database\nsystem. ArXiv , abs/1912.07001, 2019.\n[115] T. Wu, L. Chen, P. Hui, C. J. Zhang, and W. Li. Hear\nthe whole story: Towards the diversity of opinion in\ncrowdsourcing markets. PVLDB , 8(5):485{496, 2015.\n[116] X. Xu, C. Liu, and D. Song. Sqlnet: Generating\nstructured queries from natural language without\nreinforcement learning. CoRR , abs/1711.04436, 2017.\n[117] M. Yamazaki, A. Kasagi, A. Tabuchi, T. Honda,\nM. Miwa, N. Fukumoto, T. Tabaru, A. Ike, and\nK. Nakashima. Yet another accelerated sgd:\nResnet-50 training on imagenet in 74.7 seconds.\nCoRR , abs/1903.12650, 2019.\n[118] Z. Yang, E. Liang, A. Kamsetty, C. Wu, Y. Duan,\nP. Chen, P. Abbeel, J. M. Hellerstein, S. Krishnan,\nand I. Stoica. Deep unsupervised cardinality\nestimation. PVLDB , 13(3):279{292, 2019.\n[119] C. Yao, D. Agrawal, G. Chen, Q. Lin, B. C. Ooi,\nW. F. Wong, and M. Zhang. Exploiting\nsingle-threaded model in multi-core in-memory\nsystems. IEEE Trans. Knowl. Data Eng. , 2016.\n[120] M. D. Zeiler. Adadelta: An adaptive learning rate\nmethod. arXiv:1212.5701 , 2012.\n[121] C. Zhang, H. Tian, W. Wang, and F. Yan. Stay fresh:\nSpeculative synchronization for fast distributed\nmachine learning. In ICDCS , pages 99{109, 2018.\n[122] H. Zhang, G. Chen, B. C. Ooi, K. Tan, and\nM. Zhang. In-memory big data management and\nprocessing: A survey. IEEE Trans. Knowl. Data\nEng., 27(7):1920{1948, 2015.\n[123] J. Zhang, Y. Liu, K. Zhou, G. Li, Z. Xiao, B. Cheng,\nJ. Xing, Y. Wang, T. Cheng, L. Liu, and et al. An\nend-to-end automatic cloud database tuning system\nusing deep reinforcement learning. In SIGMOD ,\npages 415{432, 2019.\n[124] J. Zhang, S. Yeung, Y. Shu, B. He, and W. Wang.\nE\u000ecient memory management for gpu-based deep\nlearning systems. CoRR , abs/1903.06631, 2019.\n[125] W. Zhang, S. Gupta, X. Lian, and J. Liu.\nStaleness-aware async-sgd for distributed deep\nlearning. In IJCAI , pages 2350{2356, 2016.\n[126] V. Zhong, C. Xiong, and R. Socher. Seq2sql:\nGenerating structured queries from natural language\nusing reinforcement learning. CoRR , abs/1709.00103,\n2017.\n[127] X. Zhu, T. Chen, J. He, and W. Zhou. NNS: the case\nfor neural network-based sorting. CoRR ,\nabs/1907.08817, 2019.",
  "textLength": 50428
}