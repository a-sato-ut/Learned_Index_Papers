{
  "paperId": "a11af5fdfbb3b0dbe14846beded9e72863ae4774",
  "title": "A Parametrizable Algorithm for Distributed Approximate Similarity Search with Arbitrary Distances",
  "pdfPath": "a11af5fdfbb3b0dbe14846beded9e72863ae4774.pdf",
  "text": "A parametrizable algorithm for distributed approximate\nsimilarity search with arbitrary distances\nA Preprint\nElena Garcia-Moratoâˆ—\nCETINIA-DSLAB\nUniversidad Rey Juan Carlos\nMadrid, Spain\nelena.garciamorato@urjc.es\nMaria JesÃºs Algar\nCETINIA-DSLAB\nUniversidad Rey Juan Carlos\nMadrid, Spain\nmariajesus.algar@urjc.es\nCesar Alfaro\nCETINIA-DSLAB\nUniversidad Rey Juan Carlos\nMadrid, Spain\ncesar.alfaro@urjc.es\nFelipe Ortega\nCETINIA-DSLAB\nUniversidad Rey Juan Carlos\nMadrid, Spain\nfelipe.ortega@urjc.es\nJavier Gomez\nCETINIA-DSLAB\nUniversidad Rey Juan Carlos\nMadrid, Spain\njavier.gomez@urjc.es\nJavier M. Moguerza\nCETINIA-DSLAB\nUniversidad Rey Juan Carlos\nMadrid, Spain\njavier.moguerza@urjc.es\nApril 14, 2025\nAbstract\nRecentstudieshaveexploredalternativedistancemeasuresforsimilaritysearchinspaceswithdiverse\ntopologies, emphasizing the importance of selecting an appropriate distance function to improve\nthe performance of ğ‘˜-Nearest Neighbour search algorithms. However, a critical gap remains in\naccommodating such diverse similarity measures, as most existing methods for exact or approximate\nsimilarity search are explicitly designed for metric spaces.\nTo address this need, we propose PDASC (Parametrizable Distributed Approximate Similarity Search\nwithClustering),anovelApproximateNearestNeighboursearchalgorithm. PDASCcombinesan\ninnovativemultilevelindexingstructureparticularlyadeptatmanagingoutliers,highlyimbalanced\ndatasets, and sparse data distributions, with the flexibility to support arbitrary distance functions\nachieved through the integration of clustering algorithms that inherently accommodate them.\nExperimental results show that PDASC constitutes a reliable ANN search method, suitable for\noperatingin distributeddataenvironmentsandfor handlingdatasets definedindifferent topologies,\nwhere the selection of the most appropriate distance function is often non-trivial.\nKeywords Information Search and Retrieval Â·Similarity SearchÂ·Approximate nearest neighbour Â·Arbitrary distanceÂ·\nMachine LearningÂ·Clustering\n1 Introduction\nIn the context of similarity search, ğ‘˜-nearest neighbours ( ğ‘˜-NN) search involves identifying the ğ‘˜data objects within a\ndataset closest to a given query object according to a specified similarity or dissimilarity measure. In todayâ€™s era of\nlarge, complex datasets, ğ‘˜-NN search is a fundamental operation that enables the efficient identification and extraction\nofthemostrelevantitems,therebyunderpinningapplicationssuchasrecommendationsystems,patternrecognition,and\ninformation retrieval.\nA naive approachto ğ‘˜-NN search involves computing the distance betweenthe query and everyelement in the dataset\naccording to a provided distance function, and then selecting the ğ‘˜elements with the smallest distances. While this\nâˆ—Corresponding author. This article is currently under peer-review. All datasets are either publicly available or can be completely\nreproduced following the guidance provided in the associated references.arXiv:2405.13795v3  [cs.IR]  11 Apr 2025\n\nA parametrizable algorithm for distributed approximate similarity search with arbitrary distances A Preprint\nsequentialscan,whichevaluates ğ‘›distances,cantheoreticallysolveanysimilarityquery,itbecomescomputationally\nunfeasible for large or high-dimensional datasets.\nToavoidexaminingeveryelementinadataset,therebyreducingthenumberofdistancecomputations,itiscrucialto\npre-organizethedatabyconstructinganindexstructure. Althoughnumerousdataindexingmethodshavebeenproposed\nto accomplish this task more efficiently, previous studies [ 1] have empirically demonstrated that, when seeking an exact\nsolution,ğ‘˜-NNsearchmethodsrarelyoutperformexhaustivescanninginspacesexceedingtendimensionsduetothe\ncurse of dimensionality [2].\nThe Approximate Nearest Neighbour (ANN) search introduces a new paradigm to overcome this limitation. This\napproachrelaxesthestrictrequirementsofexactsearchbytradingoffsomeaccuracyforgreatercomputationalefficiency.\nInthiscontext,recalloftenmeasuressearchperformanceastheratioofcorrectnearestneighboursfoundtoadetermined\nğ‘˜. ANN search algorithms can be roughly categorized into four leading index families: graph-based, tree-based,\nquantization-based, and LSH-based. Although these methods are still affected by the curse of dimensionality , their\nreduced sensitivity equips them better for searching large datasets or spaces with high intrinsic dimensionality.\nMoreover, machine learning models can be incorporated to improve the efficiency of data structures and algorithms for\nsimilaritysearchinhigh-dimensionalspaces[ 3,4]. Thisevidencesupportsourgoalofexploringhybridapproachesthat\nintegrate traditional indexing methods with machine learning techniques.\nThisproposalwouldfacilitatehandlinglarge,heterogeneousdatasetsthatmustbestoredorprocessedindistributed\nenvironments. These increasingly prevalent datasets require not only more advanced indexing mechanisms but also the\nadoption of non-conventional distance functions that more accurately capture similarity between elements. In practice,\nthis additional requirement excludes many existing exact or approximate similarity search methods, primarily designed\nfor metric spaces.\nIn addition, several studies [ 5,6,7] have recently considered the use of alternative distance measures for similarity\nsearch in spaces with diverse topologies, thereby revealing the potential impact of selecting the most suitable distance\nmeasureonthe ğ‘˜-NNsearchalgorithmsperformance. Theopportunitytoachieveimprovedresults,coupledwiththe\nlackofpriorempiricalstudiesthatvalidatethisproposalandtheneedforlearningparadigmsapplicabletoabroader\nrangeofdissimilaritymeasures[ 8],justifyoureffortstoproposeanANNsearchmethodthattakesfulladvantageof\nalternative distance functions for data indexing.\nThus, we propose PDASC (Parametrizable Distributed Approximate Similarity Search with Clustering), a flexible ANN\nsearch algorithm combining an innovative multilevel index particularly adept at managing outliers, highly imbalanced\ndatasets, and sparse data distributions with the flexibility to support arbitrary distance functions through clustering\nalgorithms that can inherently accommodate them. Experimental results demonstrate that PDASC constitutes a reliable\nANN search method for distributed environments. Besides, it can handle datasets where selecting the most appropriate\ndistance function is non-trivial without requiring specific hardware acceleration strategies, such as GPUs or TPUs.\nThe main contributions of this paper are:\nâ€¢Introducing PDASC, a parametrizable ANN search algorithm that leverages a multilevel index that applies\nclusteringtechniquesinaninnovativeway,specificallydesignedtohandleoutliers,highlyimbalanceddatasets,\nand sparse data distributions.\nâ€¢Enabling the use of arbitrary distance measures in PDASC by integrating clustering algorithms that can\ninherently accommodate multiple distance functions into the index-building process.\nâ€¢Conductingaseriesofexperimentsonreal-worlddatasetsusingseveraldistancefunctionsthatdemonstrate\nPDASCâ€™sreliabilityandvalidateourcorehypothesisthatselectinganappropriatedistancefunctionsignificantly\nimpacts ANN search performance.\nThe paper is organized as follows. In Section 2, we introduce the fundamentals of similarity search, review existing\napproaches for approximate similarity search, and discuss the impact of the chosen distance function on similarity\nqueries. Section 3 presents PDASC and highlights the motivation for integrating clustering algorithms that support\ndistance functions beyond the Euclidean metric. In Section 4, we detail the empirical experiments conducted on\nreal-worlddatasetsandpresentthecorrespondingfindings. Finally,Section5summarizesthemaincontributionsofthis\nwork and outlines promising directions for future research.\n2 Background and related work\nThissectionoutlinesthekeyconceptsunderlyingourwork. First,thefundamentalsofsimilaritysearchareintroducedin\nSection2.1. Section2.2examinesapproximatesimilaritysearchmethods, highlightingtheiradvantagesandprovidinga\n2\n\nA parametrizable algorithm for distributed approximate similarity search with arbitrary distances A Preprint\ntaxonomy of available alternatives. Finally, Section 2.3 discusses the impact of distance function selection on similarity\nsearch queries, particularly in the context of high-dimensional data.\n2.1 Similarity search\nSearching for data objects closest to a given query element based on a similarity (or conversely, dissimilarity) measure,\noftenreferredtoasadistance,isafundamentaloperationthathasbecomeincreasinglysignificantintodayâ€™sdata-driven\nlandscape. This search paradigm, known as similarity search [ 9,10,11], is a cornerstone technique in various\ndisciplines,extensivelyappliedininformationretrieval[ 12],multimediacontent-basedsearch[ 13],machinelearning\n[14], bioinformatics [15] and cybersecurity [16].\nIn information retrieval, this technique involves a set of objects Xrepresented by feature vectors in a multidimensional\nspace and a function ğ›¿:XÃ—Xâ†¦â†’Rthat assigns a score ğ›¿(ğ‘,ğ‘)to each pair of objects ğ‘,ğ‘âˆˆXreflecting their degree\nofdissimilarityaccordingtothisfunction. Giventhatthisfunctionassignsahigherscore(oftenexpressedasadistance)\nto less similarobjects and a lower score tomore similar ones, the goalof a similarity query is to identifythe object or\nset of objects ğ‘†âŠ†Xwith the smallest distance to a given query object ğ‘âˆˆX.\nAs depicted in Figure 1, for a given query object ğ‘, the query result specifies a portion of the ordered dataset:\nâ€¢Rangequery : Findsallobjectsof Xwithinaspecifiedthresholdaround ğ‘. Formally,consistsonretrieving\nğ‘†={ğ‘œğ‘–|ğ›¿(ğ‘œğ‘–,ğ‘)â‰¤ğ‘Ÿ,âˆ€ğ‘œğ‘–âˆˆX}.\nâ€¢ğ‘˜-nearest-neighbour query : Finds the set of ğ‘˜objects within Xthat are closest to ğ‘, that is,ğ‘†={ğ‘œğ‘–âˆˆX|\nğ›¿(ğ‘œğ‘–,ğ‘)â‰¤ğ›¿(ğ‘œğ‘›,ğ‘),âˆ€ğ‘œğ‘›âˆˆX}. Thenearest-neighbour query is a particular case of this type, where ğ‘˜=1.\nFigure 1: Conceptual representation of several types of query searches.\nThe substantial computational cost associated with similarity queries arises from the need to process feature vectors and\nperform distance computations to evaluate object similarities [ 10]. While a sequential scan evaluating ğ‘›distances can\nresolveanysimilarityquery,thismethodbecomesimpracticalforlargeorhigh-dimensionaldatasetsduetoitsexcessive\ncomputationalburden. Tomitigatethisissue,itiscrucialtopre-organizethedatabyconstructinganaccessstructure,\nalsotermedasindex. Thisstrategyreducestheneedtoexhaustivelysearchtheentiredatasetforeachquery,thereby\nreducing the number of distance computations and avoiding the high costs associated with sequential scans.\nGiven a search space ( X,ğ›¿) defined by a set of objects Xand a distance function ğ›¿, access methods organize (or index)\nthedatasetâ€™sobjectsaccordingtothepropertiesofthesearchspaceinordertominimizethetimerequiredtoretrievethe\nqueryresults. Atquerytime, pruningstrategiesareemployedtofilteroutportionsoftheindexstructurethatcannot\ncontain valid results. When the employed distance function satisfies the properties of a metric, these methods are\nreferred to as Metric Access Methods (MAMs) [9, 11].\n2.2 Approximate similarity search approaches\nIn similarity search methods, a distinction is made between exact and approximate approaches. Exact search methods\nguarantee the retrieval of the true result set, regardless of the computational resources and time required to obtain them.\nIn contrast, approximate search methods strike a balance between accuracy and speed, offering significantly faster\nresponse times despite potential inaccuracies in the results.\n3\n\nA parametrizable algorithm for distributed approximate similarity search with arbitrary distances A Preprint\nApproximate similarity search methods are particularly effective in scenarios where exact methods cannot respond fast,\nsuchaswithhigh-dimensionalorlargedatasetswhereexactapproachesoftendefaulttoafullscan. Insteadofsearching\nfor the exact result set for a query, these approximate algorithms utilize techniques such as feature space transformation\nor space partitioning to identify a subset of candidate items that are likely to be highly similar, thereby reducing the\nnumber of distances computed. These methods can be roughly categorized into four index families: graph-based [ 17],\ntree-based [18], quantization-based [19] and LSH-based [20] [21].\nTree-basedmethodsarewidelyemployedforscalability,robustness,andefficientindexconstruction. Thesemethods\nhierarchicallypartitiondataspaces,groupingsimilarobjectsintoleafnodes. Eachnodeoftencontainsrepresentative\nobjects summarizing the partition and some information about the objects within it. During a search, the query\nobject is compared to these representatives, enabling the algorithm to efficiently exclude partitions unlikely to contain\nrelevantresults,therebyreducingoverallsearchtime. Whiletraditionalstructuressuchaskd-treesandball-treesare\nsignificantly affected by the curse of dimensionality , recent advances have led to redesigned indexes that better handle\nhigh-dimensional data. Examples include HD-Index [ 22] for optimized disk-based structures, Hercules [ 23] for parallel\nprocessing, and TARDIS [24] for distributed environments.\n2.3 Using different distance functions\nThedissimilarityvaluecapturestheclosenessbetweenapairofobjects,interpretedasadistanceinasuitablespace.\nAsthenumberofdimensionsincreases, thevolumeofthespacegrowsexponentially, andthedensityofpointsinthe\nspace decreases. This sparsity makes distances between elements tend to become more uniform, posing a challenge in\naccuratelymeasuringthesimilaritybetweendataelementssinceproximitybecomeslessmeaningful,aphenomenon\nwidely known as curse of dimensionality [2]. Consequently, elements can be very far apart, even though they are close\naccording to a distance function.\nEven if it has been demonstrated that, due to this phenomenon, it is very costly to find the exact nearest neighbour\nin high-dimensional Euclidean spaces [ 25,26], the Euclidean distance is one of the most widely used measures by\nindexing structures and search algorithms in such spaces [ 27], likely due to the conventional tendency to associate\ndistance between points with the Euclidean metric, which aligns with the intuitive notion of distance commonly applied\nin geospatial problems.\nAlthough Euclidean spaces reflect an intuitive conception of distance, it is challenging to extrapolate this natural\nassumptionwithhigh-dimensionaldata. Moreover,traditionalclusteringalgorithms,suchas ğ‘˜-means,oftenspecify\nthe Euclidean distance as the preferred one without considering other possibilities. This choice brings a self-imposed\nlimitation that may produce results far from the best [28, 29, 8].\nIn recent years, several studies [ 5,6,7] have addressed the use of alternative distance measures for similarity search in\nspaces with diverse topologies, thereby raising the issue of the potential impact that selecting the most suitable distance\nmeasuremayhaveontheperformanceof ğ‘˜-NNsearchalgorithms. Thepotentialforachievingimprovedresults,coupled\nwiththelackofpriorempiricalstudiesthatvalidatethisproposalandtheneedforlearningparadigmsapplicabletoa\nbroaderrangeofdissimilaritymeasures[ 8],justifiesoureffortstoproposeanANNsearchmethodthatleveragesthe\nadvantages of employing diverse distance functions for data indexing.\n3 Methodology\nThis section introduces the proposed parametrizable data indexing algorithm, beginning with an overview of its\npropertiesandimplementationinSection3.1. Section3.2examinesthecharacteristicsofvariousdistancefunctions.\nFinally,Section3.3highlightsthecloserelationshipbetweenEuclideandistanceandthe ğ‘˜-meansalgorithm,thereby\njustifying the need for alternative clustering algorithms.\n3.1 PDASC\nPDASC is a parametrizable algorithm for approximate ğ‘˜-NN search that leverages a multilevel index built by applying\nclustering techniques in a tailored manner. It also offers the flexibility to support arbitrary distance functions by\nintegrating compatible clustering algorithms. Its design, tailored explicitly for distributed environments, makes it\nparticularlyeffectiveathandlingoutliers,highlyimbalanceddatasets,andsparse datadistributionswithoutrequiring\nadditional hardware acceleration strategies such as GPUs or TPUs. According to the taxonomy described in Section 2.2,\nPDASC falls within the tree-based approximations.\nPDASC is implemented in Python and operates through two stages: the Multilevel Structure Algorithm (MSA) and the\nNeighbours Search Algorithm (NSA), with their pseudocode detailed in Algorithms 1 and 2, respectively.\n4\n\nA parametrizable algorithm for distributed approximate similarity search with arbitrary distances A Preprint\nAsoutlinedinAlgorithm1,MSAfollowsabottom-upapproachtoconstructamultilevelindexstructuretailoredfor\ndistributed,scalabledataindexing. Theprocessbeginsbyrandomlypartitioningthedatasetintomultiplesubsetsthat\ncan be distributed across different computational nodes. For each subset, a suitable clustering algorithm is applied\nto generate representative points (prototypes) that summarize the data within that subset. In contrast to traditional\nclusteringalgorithms,which seektogeneratetheminimalnumber ofprototypesneededfordata representation,MSA\nleveragesclusteringtechniquesfromadifferentperspective: aimingtoproduceasmanyprototypesascomputational\nresources permit. The density of the underlying data drives the distribution of prototypes: denser regions receive\na proportionally higher number of prototypes, while outliers and clusters distinct from the main concentrations are\nalso identified, This results in a richer, more detailed characterization of the underlying data points. These prototypes\nare recursively clustered layer by layer until the stop condition is met. At this point, the index structure is entirely\nconstructed.\nThe structure generated by MSA depends on several initial parameters:\nâ€¢Group length ( ğ‘”ğ‘™). This parameter specifies the number of points contained in each data partition at the lowest\nlevel. Itisconfiguredbasedontheavailablecomputingpowerandthetotalnumberofnodesindistributed\ncomputing systems.\nâ€¢Numberofprototypes( ğ‘›ğ‘ƒğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’ğ‘  ). Foreachgroup,thenumberofprototypesisdeterminedbymaintaining\na 2:1 ratio between the group length and the number of prototypes. This ratio also dictates the number of\nlayers in the multilevel index.\nâ€¢Clustering algorithm ( ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿğ´ğ‘™ğ‘” ). Although the only requirement for the clustering algorithm used to\nconstruct the index is that it partitions the data space into Voronoi regions and generates a representative point\nfor each, PDASCâ€™s ability to support multiple distance functions is achieved using clustering algorithms in the\nindex-buildingprocessthatcanacceptthem. Thisstudyadoptsthe ğ‘˜-medoidsclusteringmethodduetoits\ncompatibility with a wide range of distances, as further explained in Section 3.3.\nâ€¢Distancefunction ( ğ‘‘ğ‘–ğ‘ ğ‘¡ğ¹ğ‘¢ğ‘›ğ‘ )applied bytheclustering algorithm. Selectingthemostsuitablefunctionfora\ndatasetisoftenchallenging,yetitisacriticaldecisionbecauseitsignificantlyinfluencestheprototypeschosen.\nThe range of available distance functions is determined by those supported by the chosen clustering algorithm.\nAlgorithm 1 Multilevel Structure Algorithm\n1:procedure MSA(ğ‘‘ğ‘ğ‘¡ğ‘,ğ‘”ğ‘™,ğ‘›ğ‘ƒğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’ğ‘ ,ğ‘‘ğ‘–ğ‘ ğ‘¡ğ¹ğ‘¢ğ‘›ğ‘,ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿğ´ğ‘™ğ‘” )\n2:âŠ²Divide dataset into ğ‘›ğºğ‘Ÿğ‘œğ‘¢ğ‘ğ‘  groups ofğ‘”ğ‘™points\n3:ğ‘£ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿ,ğ‘›ğºğ‘Ÿğ‘œğ‘¢ğ‘ğ‘ â†ğ‘‘ğ‘ğ‘¡ğ‘,ğ‘”ğ‘™\n4:âŠ²Initialise multilevel structure\n5:ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ğ‘ƒğ‘œğ‘–ğ‘›ğ‘¡ğ‘ ,ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ğ¿ğ‘ğ‘ğ‘’ğ‘™ğ‘  â†[]\n6:ğ‘–ğ‘‘ğ¿ğ‘’ğ‘£ğ‘’ğ‘™â†0\n7:whileğ‘›ğºğ‘Ÿğ‘œğ‘¢ğ‘ğ‘ â‰¥1do\n8:âŠ²Initialise auxiliary structures for each level\n9:ğ‘”ğ‘Ÿğ‘œğ‘¢ğ‘ğ‘ ğ‘ƒğ‘œğ‘–ğ‘›ğ‘¡ğ‘ ,ğ‘”ğ‘Ÿğ‘œğ‘¢ğ‘ğ‘ ğ¿ğ‘ğ‘ğ‘’ğ‘™ğ‘ ,ğ‘ğ‘œğ‘–ğ‘›ğ‘¡ğ‘  â†[]\n10: forğ‘–ğ‘‘ğºğ‘Ÿğ‘œğ‘¢ğ‘ =1,ğ‘›ğºğ‘Ÿğ‘œğ‘¢ğ‘ğ‘  do\n11: ğ‘ğ‘œğ‘–ğ‘›ğ‘¡ğ‘ â†ğ‘£ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿ[ğ‘–ğ‘‘ğºğ‘Ÿğ‘œğ‘¢ğ‘]\n12: ğ‘”ğ‘Ÿğ‘œğ‘¢ğ‘ğ‘ ğ‘ƒğ‘œğ‘–ğ‘›ğ‘¡ğ‘ .ğ‘ğ‘‘ğ‘‘ (\n13: ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘–ğ‘›ğ‘”(ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿğ´ğ‘™ğ‘”,ğ‘‘ğ‘–ğ‘ ğ‘¡ğ¹ğ‘¢ğ‘›ğ‘,ğ‘›ğ‘ƒğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’ğ‘ ,ğ‘ğ‘œğ‘–ğ‘›ğ‘¡ğ‘  ))\n14: ğ‘”ğ‘Ÿğ‘œğ‘¢ğ‘ğ‘ ğ¿ğ‘ğ‘ğ‘’ğ‘™ğ‘ .ğ‘ğ‘‘ğ‘‘ (ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘ .ğ‘™ğ‘ğ‘ğ‘’ğ‘™ğ‘ )\n15: end for\n16:ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ğ‘ƒğ‘œğ‘–ğ‘›ğ‘¡ğ‘ .ğ‘ğ‘‘ğ‘‘(ğ‘”ğ‘Ÿğ‘œğ‘¢ğ‘ğ‘ ğ‘ƒğ‘œğ‘–ğ‘›ğ‘¡ğ‘ )\n17:ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ğ¿ğ‘ğ‘ğ‘’ğ‘™ğ‘ .ğ‘ğ‘‘ğ‘‘(ğ‘”ğ‘Ÿğ‘œğ‘¢ğ‘ğ‘ ğ¿ğ‘ğ‘ğ‘’ğ‘™ğ‘ )\n18:ğ‘£ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿ,ğ‘›ğºğ‘Ÿğ‘œğ‘¢ğ‘ğ‘ â†split(ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ğ‘ƒğ‘œğ‘–ğ‘›ğ‘¡ğ‘ ,ğ‘”ğ‘™ )\n19:ğ‘–ğ‘‘ğ¿ğ‘’ğ‘£ğ‘’ğ‘™+=1\n20:end while\n21:ğ‘›ğ¿ğ‘’ğ‘£ğ‘’ğ‘™ğ‘ â†ğ‘–ğ‘‘ğ¿ğ‘’ğ‘£ğ‘’ğ‘™âˆ’1\n22:returnlevelPoints, levelLabels, nLevels\n23:end procedure\nFigure2illustratesanexampleschemeoftheindex-buildingprocessfortheMSAalgorithm. Tosimplifythevisualization,\nthedatapointson therightsideofthefigure arerepresentedalongastraightline. Atthelowestlevel,thedataset Xis\npartitioned into several groups according to the ğ‘”ğ‘™parameter. In the example shown, a dataset of 74 elements is divided\ninto 8 groups (7 groups contain 10 elements each, and the last group contains 4 elements) since ğ‘”ğ‘™is set to 10. The\nnumberofprototypes, ğ‘›ğ‘ƒğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’ğ‘  ,isdeterminedbya2:1ratiowith ğ‘”ğ‘™(inthiscase, ğ‘›ğ‘ƒğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’ğ‘  =5). Therefore,\nthe points in the first group at level 0 are clustered into 5 clusters, as indicated by the grey dashed circles. For each\n5\n\nA parametrizable algorithm for distributed approximate similarity search with arbitrary distances A Preprint\ncluster, one point is selected as a prototype and promoted to the next level (level 1). At level 1, the prototypes from the\nfirstgrouparecombinedwiththosefromanothergroup,andthesamestrategyisappliedrecursively,creatingadditional\nprototype layersthat will summarize theprototypes of thelevel immediately below. Theprocedure terminates when a\nlayer containing ğ‘›ğ‘ƒğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’ğ‘  prototype points is reached, which occurs at level 4 in this example.\nNotably, the formed groups can have varying sizes without issue, especially in the lower layers. When a group or\nclustercontainsanumberofpointsthatislessthanorequalto ğ‘›ğ‘ƒğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’ğ‘  ,allpointsarepromotedasprototypes. For\ninstance,inthisexample,the8thgroupatlevel0hasfewerpointsthan ğ‘”ğ‘™,soallofitspointsbecomeprototypesand\narepromotedtolevel1. Asaresult,thegroupformedbycombiningtheprototypesofgroups7and8containsonly9\nprototypes, from which 5 will be promoted to level 3.\nFigure 2: Conceptual representation of the multilevel index creation in PDASC.\nTheNSAleveragesthemultilevelindexstructuregeneratedbyMSAtoefficientlyidentifythe ğ‘˜-NNofagivenquery\npoint(ğ‘)using Algorithm 2. For a given query point (ğ‘), the number of neighbours (ğ‘˜), and a radius(ğ‘Ÿ), it begins\nbycalculatingthedistancebetweenthequerypointandtheprototypesdefinedatthetopleveloftheindex,withonly\nthose satisfying the radius condition ğ‘Ÿconsidered for further exploration. The value of ğ‘Ÿcan be determined based\nonmeasuresthatprovideinsightintothedistributionofthedataset,suchastheCumulativeDistributionFunctionor\nthe maximum distance between elements, with its restrictiveness adjusted according to the desired accuracy of the\nresults. Thisprocessisrepeated,descendingthroughsuccessiveindexlevelsuntilreachingthebottomlevel,whereeach\nprototype maps a set of data points. Only those points that meet the ğ‘Ÿcondition are added to the pool of candidate\npoints. Oncetheentireindexhasbeenexploredandthecandidatelistiscomplete,thepointsaresortedbytheirdistance\nfrom the query point, and the ğ‘˜closest points are selected as neighbours.\n6\n\nA parametrizable algorithm for distributed approximate similarity search with arbitrary distances A Preprint\nAlgorithm 2 Neighbours Search Algorithm (NSA)\n1:procedure NSA(ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ğ‘ƒğ‘œğ‘–ğ‘›ğ‘¡ğ‘ ,ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ğ¿ğ‘ğ‘ğ‘’ğ‘™ğ‘ ,ğ‘›ğ¿ğ‘’ğ‘£ğ‘’ğ‘™ğ‘ ,ğ‘‘ğ‘–ğ‘ ğ‘¡ğ¹ğ‘¢ğ‘›ğ‘,ğ‘,ğ‘˜,ğ‘Ÿ )\n2:âŠ²Retrieve all points at the top level of the index\n3:ğ‘ğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’ğ‘ â†ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ğ‘ƒğ‘œğ‘–ğ‘›ğ‘¡ğ‘ [ğ‘›ğ¿ğ‘’ğ‘£ğ‘’ğ‘™ğ‘ ]\n4:âŠ²Find top-level points within the search radius\n5:ğ·â†distance (ğ‘‘ğ‘–ğ‘ ğ‘¡ğ¹ğ‘¢ğ‘›ğ‘,ğ‘,ğ‘ğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’ğ‘  )\n6:ğ‘–ğ‘‘ğ¶ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘ â†ğ· <ğ‘Ÿ\n7:âŠ²Recursively explore lower levels of the index\n8:ğ‘ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘ƒğ‘œğ‘–ğ‘›ğ‘¡ğ‘ â†\n9: ExploreCandidates (ğ‘–ğ‘‘ğ¶ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘ ,ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ğ‘ƒğ‘œğ‘–ğ‘›ğ‘¡ğ‘ ,ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ğ¿ğ‘ğ‘ğ‘’ğ‘™ğ‘ ,\nğ‘‘ğ‘–ğ‘ ğ‘¡ğ¹ğ‘¢ğ‘›ğ‘,ğ‘,ğ‘Ÿ,ğ‘›ğ¿ğ‘’ğ‘£ğ‘’ğ‘™ğ‘  âˆ’1)\n10:ğ·â†distance (ğ‘,ğ‘ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘ƒğ‘œğ‘–ğ‘›ğ‘¡ğ‘ ,ğ‘‘ğ‘–ğ‘ ğ‘¡ğ¹ğ‘¢ğ‘›ğ‘ );\n11:ğ‘ ğ‘œğ‘Ÿğ‘¡ğ‘’ğ‘‘ğ¶ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘  â†sort(ğ‘ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘ƒğ‘œğ‘–ğ‘›ğ‘¡ğ‘ ,ğ· );\n12:ğ‘›ğ‘’ğ‘–ğ‘”â„ğ‘ğ‘œğ‘¢ğ‘Ÿğ‘ â†ğ‘ ğ‘œğ‘Ÿğ‘¡ğ‘’ğ‘‘ğ¶ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘  [0 :ğ‘˜];\n13:returnğ‘›ğ‘’ğ‘–ğ‘”â„ğ‘ğ‘œğ‘¢ğ‘Ÿğ‘ \n14:end procedure\n15:\n16:procedure ExploreCandidates (ğ‘–ğ‘‘ğ¶ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘ ,ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ğ‘ƒğ‘œğ‘–ğ‘›ğ‘¡ğ‘ ,ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ğ¿ğ‘ğ‘ğ‘’ğ‘™ğ‘ ,ğ‘‘ğ‘–ğ‘ ğ‘¡ğ¹ğ‘¢ğ‘›ğ‘,ğ‘,ğ‘Ÿ,ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ )\n17:ifğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ =1then\n18:âŠ²At the lowest level, return only the specific points mapped by idCandidates\n19: returnğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ğ‘ƒğ‘œğ‘–ğ‘›ğ‘¡ğ‘ [0][ğ‘–ğ‘‘ğ¶ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘ ]\n20:end if\n21:ğ‘›ğ‘’ğ‘¤ğ¶ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘ â†âˆ…\n22:for eachğ‘–ğ‘‘âˆˆğ‘–ğ‘‘ğ¶ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘  do\n23:âŠ²Retrieve only the points mapped by the candidate centroids at this level\n24:ğ‘šğ‘ğ‘ğ‘ğ‘’ğ‘‘ğ‘ƒğ‘œğ‘–ğ‘›ğ‘¡ğ‘ â†ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ğ‘ƒğ‘œğ‘–ğ‘›ğ‘¡ğ‘ [ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™][ğ‘–ğ‘‘]\n25:âŠ²Filter points that fall within the search radius\n26:ğ·â†distance (ğ‘‘ğ‘–ğ‘ ğ‘¡ğ¹ğ‘¢ğ‘›ğ‘,ğ‘,ğ‘šğ‘ğ‘ğ‘ğ‘’ğ‘‘ğ‘ƒğ‘œğ‘–ğ‘›ğ‘¡ğ‘  )\n27:ğ‘“ğ‘–ğ‘™ğ‘¡ğ‘’ğ‘Ÿğ‘’ğ‘‘ğ¶ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘  â†ğ· <ğ‘Ÿ\n28: ifğ‘“ğ‘–ğ‘™ğ‘¡ğ‘’ğ‘Ÿğ‘’ğ‘‘ğ¶ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘  â‰ âˆ…then\n29: âŠ²Recursively explore the next lower level for selected candidates\n30: ğ‘›ğ‘’ğ‘¤ğ¶ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘ .ğ‘ğ‘‘ğ‘‘ (\n31: ExploreCandidates (ğ‘“ğ‘–ğ‘™ğ‘¡ğ‘’ğ‘Ÿğ‘’ğ‘‘ğ¶ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘ ,ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ğ‘ƒğ‘œğ‘–ğ‘›ğ‘¡ğ‘ ,ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ğ¿ğ‘ğ‘ğ‘’ğ‘™ğ‘ ,\nğ‘‘ğ‘–ğ‘ ğ‘¡ğ¹ğ‘¢ğ‘›ğ‘,ğ‘,ğ‘Ÿ,ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ âˆ’1))\n32: end if\n33:end for\n34:returnğ‘›ğ‘’ğ‘¤ğ¶ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘ \n35:end procedure\nFigure 3 represents the search process for identifying the ğ‘˜=2neighbours using a multilevel structure with four levels.\nThe process begins with the NSA algorithm computing the distance between the query point ğ‘and the prototypes at the\nhighest level of the index, in this case, level 3: ğ‘‘(ğ‘ƒ3âˆ’1,ğ‘),ğ‘‘(ğ‘ƒ3âˆ’2,ğ‘),ğ‘‘(ğ‘ƒ3âˆ’3,ğ‘),ğ‘‘(ğ‘ƒ3âˆ’4,ğ‘),ğ‘‘(ğ‘ƒ3âˆ’5,ğ‘). Only the\nprototypes satisfying ğ‘‘(ğ‘,ğ‘ƒ 3âˆ’ğ‘–)<ğ‘Ÿ(i.e.,ğ‘ƒ3âˆ’1,ğ‘ƒ3âˆ’4, andğ‘ƒ3âˆ’5) are considered for further exploration.\nNext, the algorithm calculates the distances between ğ‘and all points within the selected clusters and retains only\nprototypeswith distancesbelow ğ‘Ÿ. For instance,inthe caseof ğ‘ƒ3âˆ’1,which mapsthreepoints ( ğ‘ƒ2âˆ’1,ğ‘ƒ2âˆ’4,andğ‘ƒ2âˆ’5),\nonlyğ‘ƒ2âˆ’1andğ‘ƒ2âˆ’5satisfythethreshold. Tovisuallyrepresentthis,solidpurplelinesindicateconnectionsbetween\nğ‘ƒ3âˆ’1and these points, while a dotted grey line marks the connection to ğ‘ƒ2âˆ’4, whose distance exceeds ğ‘Ÿ.\nAt level 1, the algorithm proceeds by analysing the points within the clusters associated with ğ‘ƒ2âˆ’1,ğ‘ƒ2âˆ’5, andğ‘ƒ2âˆ’6,\nwhich include ğ‘ƒ1âˆ’2,ğ‘ƒ1âˆ’4,ğ‘ƒ1âˆ’6,ğ‘ƒ1âˆ’13, andğ‘ƒ1âˆ’14. When the lowest level is reached, a set of potential candidates is\nidentified:ğ‘ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘  ={ğ‘3,ğ‘4,ğ‘14,ğ‘20,ğ‘22,ğ‘29}. These candidates are then sorted by their distances to ğ‘, and the\ntwo closest points ( ğ‘˜=2) are selected as nearest neighbours: {ğ‘29,ğ‘4}.\n3.2 Distance functions\nIn the context of similarity search and other application domains, the term â€˜distanceâ€™ refers to a function that measures\nthedissimilaritybetweentwoobjectsinfeaturespace,satisfyingthepropertiesofnon-negativity,symmetry,andidentity\nof indiscernible. Besides, when a distance function also satisfies the triangle inequality, it qualifies as a metric [ 9]. This\nwork focuses on three specific distance functions: Minkowski distance, cosine distance and Haversine distance.\n7\n\nA parametrizable algorithm for distributed approximate similarity search with arbitrary distances A Preprint\nFigure 3: Conceptual representation of the ğ‘˜-NN search process in PDASC.\nMinkowski distance\nThe Minkowski distance or â„“ğ‘ğ‘›ğ‘œğ‘Ÿğ‘šis a metric that generalises a set of distance measures commonly used to compute\nthe dissimilarity of objects described by numerical attributes.\nLetâ„“ğ‘\nâˆ,ğ‘â‰¥1be a vector space of real sequences such thatÃâˆ\nğ‘–=1|ğ‘¥ğ‘–|ğ‘with the norm given by ||ğ‘¥||ğ‘(Ãâˆ\nğ‘–=1|ğ‘¥ğ‘–|ğ‘)1\nğ‘. This\nnorm induces the â„“ğ‘›ğ‘œğ‘Ÿğ‘šor Minkowski metric ğ‘‘ğ‘and consequently â„“ğ‘š\nğ‘andâ„“ğ‘š\nğ‘=(Rğ‘š,ğ‘‘ğ‘)are Banach spaces.\nThus, assuming ğ‘¥,ğ‘¦âˆˆRğ‘š, theâ„“ğ‘›ğ‘œğ‘Ÿğ‘šor Minkowski distance is defined as follows:\nğ‘‘ğ‘(ğ‘¥,ğ‘¦)=(ğ‘šâˆ‘ï¸\nğ‘–=1|ğ‘¥ğ‘–âˆ’ğ‘¦ğ‘–|ğ‘)1\nğ‘,ğ‘â‰¥1 (1)\nwhere depending on the ğ‘value we can induce:\nğ‘‘1(ğ‘¥,ğ‘¦)=Ãğ‘š\nğ‘–=1|ğ‘¥ğ‘–âˆ’ğ‘¦ğ‘–|, the Manhattan or city-block distance.\nğ‘‘2(ğ‘¥,ğ‘¦)=ğ‘‘ğ¸(ğ‘¥,ğ‘¦)=(Ãğ‘š\nğ‘–=1|ğ‘¥ğ‘–âˆ’ğ‘¦ğ‘–|2)1\n2, the Euclidean distance.\ndâˆ(ğ‘¥,ğ‘¦)=ğ‘‘ğ‘šğ‘ğ‘¥(ğ‘¥,ğ‘¦)=ğ‘šğ‘ğ‘¥ğ‘–|ğ‘¥ğ‘–âˆ’ğ‘¦ğ‘–|, the max-norm or Chebyshev distance.\nMinkowski distances with ğ‘ <1cannot be considered proper metrics since they do not hold the triangle inequality.\nNonetheless, several authors have shown that fractional distances can significantly improve the effectiveness of standard\nclustering algorithms such as ğ‘˜-means [29].\nCosine distance\nThe cosine distance is a dissimilarity measure between two non-zero vectors in an inner product space which calculates\nthecosineoftheanglebetweenthem. UnliketheEuclideandistance,whichconsidersthevectorsâ€™magnitude,cosine\ndistancefocusesontheirorientation,makingitlesssensitivetodifferencesinvectorlengthandmorerobusttovariations\nin data scaling [29]. Their interesting properties make it a widely used choice in text analysis applications.\nMathematically, for two vectors ğ‘¢andğ‘£, the cosine distance is defined as:\nğ‘‘ğ‘ğ‘œğ‘ (ğ‘¢,ğ‘£)=1âˆ’ğ‘ğ‘œğ‘ (ğœƒ)=1âˆ’ğ‘¢Â·ğ‘£\nâˆ¥ğ‘¢âˆ¥âˆ¥ğ‘£âˆ¥=1âˆ’Ãğ‘›\nğ‘–=1ğ‘¢ğ‘–ğ‘£ğ‘–âˆšï¸ƒÃğ‘›\nğ‘–=1ğ‘¢2\nğ‘–âˆšï¸ƒÃğ‘›\nğ‘–=1ğ‘£2\nğ‘–, (2)\n8\n\nA parametrizable algorithm for distributed approximate similarity search with arbitrary distances A Preprint\nwhereğœƒis the angle between ğ‘¢andğ‘£in theğ‘›-dimensional space, and ğ‘¢ğ‘–andğ‘£ğ‘–are theğ‘–-th components of vectors ğ‘¢and\nğ‘£, respectively. Thus, the cosine distance ranges from 0 to 2, where 0 means that the vectors are identical and 2 means\nthat they are diametrically opposed to each other.\nHaversine distance\nTheHaversinedistance[ 30]isadissimilaritymeasureusedtocalculatetheshortestdistancebetweentwopointson\nthesurfaceofasphere,suchastheEarth. UnliketheEuclideandistance,whichmeasuresstraight-lineseparationin\na Cartesian plane and can underestimate true distances on a curved surface, the Haversine distance accounts for the\nEarthâ€™scurvature. Itprovidesmoreaccuratemeasurementsoverlongdistancesorextremevalues,whichiscrucialin\nfields such as geospatial analysis [31] and navigation [32].\nMathematically,giventwopoints ğ‘¥andğ‘¦describedascoordinatescomposedoflatitudeandlongitudeinradians,the\nHaversine distance is defined as:\nğ‘‘â„ğ‘ğ‘£ğ‘’ğ‘Ÿğ‘ ğ‘–ğ‘›ğ‘’(ğ‘¥,ğ‘¦)=\n2 arcsin âˆšï¸‚\nsin2\u0010ğ‘¥ğ‘™ğ‘ğ‘¡âˆ’ğ‘¦ğ‘™ğ‘ğ‘¡\n2\u0011\n+cos(ğ‘¥ğ‘™ğ‘ğ‘¡)cos(ğ‘¦ğ‘™ğ‘ğ‘¡)sin2\u0010ğ‘¥ğ‘™ğ‘œğ‘›âˆ’ğ‘¦ğ‘™ğ‘œğ‘›\n2\u0011!\n,(3)\n3.3 Clustering algorithms\nClusteringisafundamentaltechniqueindataanalysisthatdividesasetofobjectsintogroupsorclusters. Objectswithin\nthe same cluster are more similar to each other than to those in different clusters.\nAmong the various clustering algorithms suitable for unsupervised learning, we found ğ‘˜-means [33], probably the most\nwidelyusedclusteringalgorithmofalltime. Givenasetofobjects XinEuclideanspace, ğ‘˜-meanspartitionsthedata\nintoğ‘˜non-overlapping clusters, often conceptualized as Voronoi cells, by identifying ğ‘˜centroids that minimize the sum\nof squared distances between each object in Xand its nearest centroid. Each centroid ğ‘ğ‘–represents the mean of its\nrespective cluster and may not necessarily be an element of X.\nDespiteitswidespreaduse, ğ‘˜-meansclusteringhasseveralwell-knownlimitations. Aprimarydrawbackistheusersâ€™\nneed to specify the number of clusters, which can be challenging without prior knowledge of the dataâ€™s structure.\nSelectinganinappropriatenumberofclusterscanleadtoineffectivesummarizationofthedata,eitherbyoverfitting\nwithtoomanycentroidsorbyoversimplifyingwithtoofew,leadingtogroupingdissimilardatapoints. Furthermore,\nthechoiceofdistancefunctionin ğ‘˜-meanscansignificantlyaffectclusteringresults,asitdetermineshowsimilarity\nbetween data points is calculated, leading to clusters with different shapes and separations.\nHowever, while it might appear theoretically feasible to apply ğ‘˜-means with various distance functions, the algorithmâ€™s\niterative optimization process is intrinsically linked to the Euclidean distance because the selection of prototype points\nis based on minimizing the sum of squared errors, calculated as the sum of squared Euclidean distances between each\ndata point and its assigned centroid, thus restricting the algorithmâ€™s application to Euclidean spaces.\nAs discussed in Section 2.3, the use of Euclidean distance in high-dimensional data analysis arises from its widespread\nintegrationintoexistingindexingstructuresandalgorithms[ 27]. However,thismetrichaswell-establishedlimitationsin\nhigh-dimensional spaces, as it may not accurately capture the actual proximity between data points [ 26]. Consequently,\nthereisacompellingneedtoexplorealternativedissimilaritymeasuresforreal-worlddataindexing,thereforeemploying\nalternative clustering methods suitable for different distance functions, such as ğ‘˜-medoids.\n3.3.1 k-medoids\nWhilethefirstpaperproposingageneralised ğ‘˜-meansmethodwas[ 34],itwasKaufmannandRousseeuw[ 35]whofirst\nproposed the widely used Partitioning Around Medoids (PAM) algorithm, also known as ğ‘˜-medoids. Data is modelled\nusingğ‘˜representativepoints ğ‘šğ‘–(calledmedoids)thatserveasprototypesfortheclusters. Theseprototypesareobtained\nusingtheabsoluteerrorcriterion(â€˜totaldeviationâ€™,TD).SincetheTDisdefinedasthesumofdissimilaritiesofeach\nobjectğ‘¥ğ‘—âˆˆğ¶ğ‘–to the medoid ğ‘šğ‘–of its cluster, the medoid of a cluster is, in fact, the object with the smallest sum of\ndissimilarities, an actual data element that has been chosen as the most representative of the group.\nğ‘šğ‘’ğ‘‘ğ‘œğ‘–ğ‘‘(ğ¶ğ‘–):=ğ‘ğ‘Ÿğ‘”ğ‘šğ‘–ğ‘›ğ‘¥ğ‘–âˆˆğ¶âˆ‘ï¸\nğ‘¥ğ‘—âˆˆğ¶ğ‘‘(ğ‘¥ğ‘–,ğ‘¥ğ‘—) (4)\n9\n\nA parametrizable algorithm for distributed approximate similarity search with arbitrary distances A Preprint\nLikeğ‘˜-means, theğ‘˜-medoids algorithm partitionsdata into clusters regarding a specific distancemeasure. But, while\nğ‘˜-meansdefinesaprototypeasacentroid,whichisusuallythemeanofagroupofpoints,on ğ‘˜-medoidstheprototypeis\namemberofthesubset,calledamedoid. Thisdistinctionmakes ğ‘˜-medoidsmoreversatile,asitdoesnotrequirethe\ndissimilarity measure to strictly adhere to metric properties, allowing it to accommodate alternative distance functions.\nDespite being a widely used algorithm in data analysis, implementing ğ‘˜-medoids remains non-trivial. However, its\nproven efficiency and suitability have led to the development of modern implementations such as FasterPAM [ 36,37],\nanenhanced versionofPAM algorithm. Originally proposedinJava, itintroduces aspeed-up for largernumbersof\nclusters by exploiting redundancies during the swap phase of medoid computation.\nTo integrate it with PDASC, entirely developed in Python, we will employ the FasterPAM Python implementation [ 38]\navailablethroughthe ğ‘˜ğ‘šğ‘’ğ‘‘ğ‘œğ‘–ğ‘‘ğ‘  package 2. IncorporatingthislibrarywillenablePDASCtoemploy ğ‘˜-medoidsasa\nclustering algorithm, thereby allowing the use of arbitrary dissimilarities and distances.\n4 Experimental evaluation\nThis section presents a comprehensive evaluation of PDASC through a series of experiments. The datasets and the\nimplemented experimental design are detailed in Section 4.1. Subsequently, evaluation metrics chosen to assess\nperformancearediscussedinSection4.3. Section4.4outlinesthealgorithmsselectedforcomparisonwithPDASC.\nFinally, the experimental results are discussed in Section 4.5, highlighting key findings and insights.\nAll experiments have been performed on a server with 2 AMD EPYC 7451 microprocessors (24 cores/48 threads, 2.30\nGHz, 64 MB L3 cache), 128 GB of DDR4 RAM and an Intel D3-S4510 SSD (capacity 480 GB) for secondary storage.\n4.1 Datasets\nWeconductedexperimentsonfourpubliclyaccessibledatasets,eachdifferingintype,size(n),dimensionality(Dim),\nand sparsity (HS), as summarised in Table 1. Except for the Municipalities dataset, which is explained below, these\ndatasets are part of a standard benchmarking suite for ANN search [39].\nTable 1: Descriptive traits characterising the datasets included in experiments.\nDataset Label n Dim HS Data Type Purpose\nMunicipalities Xğ‘šğ‘¢ğ‘›ğ‘– 8,130 2 No Geospatial Outliers & Haver-\nsine Distance\nMNIST Xğ‘€ğ‘ğ¼ğ‘†ğ‘‡ 69,000 784 Yes Image High Dimensional-\nity & Dispersion\nGLOVE Xğºğ¿ğ‘‚ğ‘‰ğ¸ 1,000,000 100 No Text HighNumberofEl-\nements\nNYtimes Xğ‘ğ‘Œğ‘¡ğ‘–ğ‘šğ‘’ğ‘  290,000 256 No Text Cosine Distance\nâ€¢Municipalities. The datasetis a two-dimensionalgeospatial collectioncomprising the latitudeand longitude\ncoordinates of all 8,130 municipalities in Spain. This dataset is extracted from the annual report published by\nthe Instituto GeogrÃ¡fico Nacional (National Geographic Institute) under the Spanish Ministry of Transport and\nSustainable Mobility 3. It provides updated geographic and administrative information, including municipality\nnames, provinces, coordinates, perimeters, and populations.\nTheinclusionofthisdataset,hostedontheZenodorepositoryandaccessibleviaDOI10.5281/zenodo.12759082,\ninourexperimentalstudyisjustifiedbyitsroleasarepresentativegeospatialtestcase,enablingtheapplication\nof the Haversine distance (a metric designed explicitly for two-dimensional spherical data).\nFigure 4 describes the dataset, highlighting three primary clusters: Mainland Spain, the Balearic Islands, and\ntheCanaryIslands. Duetotheirgeographicalseparationfromthemainland,thelocationsofmunicipalities\nin the Balearic and Canary Islands can be considered outliers. Despite this datasetâ€™s seemingly simple\ntwo-dimensional structure and low dispersion, the outliers pose significant challenges, underscoring its value\nin testing the resilience of similarity search methods.\n2https://pypi.org/project/kmedoids\n3https://ign.es/web/rcc-nomenclator-nacional\n10\n\nA parametrizable algorithm for distributed approximate similarity search with arbitrary distances A Preprint\nFigure 4: Visualisation for dataset Municipalities\nâ€¢MNIST. The MNIST_784 dataset 4is a widely used benchmark in machine learning, comprising 60,000\ntrainingand10,000testimagesofhandwrittendigitsfrom0to9. Each28x28pixelgrayscaleimageisflattened\ninto a 784-dimensional vector, facilitating the evaluation of algorithms in high-dimensional data scenarios.\nâ€¢GLOVE. The GloVe_100 dataset 5comprises one million pre-trained 100-dimensional word embeddings,\ntrainedon acombinedcorpusfrom Wikipedia2014 andtheGigaword5th Edition, encompassing 6billion\ntokens and a vocabulary of 400,000 words. This dataset includes a diverse range of English words, making it\nideal for evaluating the effectiveness of ANN methods in retrieving semantically similar items [40].\nâ€¢NYtimes. The NYtimes dataset 6, part of the Bags of Words dataset from the UCI repository, comprises a\ncollection of 299,752 New York Times news articles represented as a documentâ€“word matrix, where each row\ncorrespondstoadocumentandeachcolumntoauniqueword,withcellvaluesindicatingwordfrequencyin\nthe respective document.\n4.2 Experimental design\nTheeffectivenessoftheproposedmethodwillbevalidatedthroughatwo-stepevaluation: assessingitsperformance\nand comparing it with that of other methods. A series of experiments will be conducted on the datasets detailed in\nSection4.1,eachrandomlydividedintonon-overlappingtrainingandtestsetstoenhanceevaluationcomplexity. The\ntrainingsetsusedtobuildtheindexstructurewillberandomlypartitionedanddistributedacrossvariouscomputing\nnodes, effectively simulating a distributed environment to demonstrate PDASCâ€™s suitability for such scenarios.\nThedistancefunctionsutilizedwouldbethosedescribedinSection3.2withEuclidean,Manhattan,Chebyshev,and\ncosinedistancebeenappliedtoallofthem,whiletheHaversinedistancewouldonlybeusedwiththeMunicipalities\ndataset, as it is only adequate for two-dimensional data.\nAn index tree will be constructed for each dataset and distance function combination applying MSA to the training\nset. MSA constructs the indexbased on thechosenclustering algorithm, distancefunction, group length ( ğ‘”ğ‘™), and the\n4http://yann.lecun.com/exdb/mnist/\n5https://nlp.stanford.edu/data/glove.6B.zip\n6https://archive.ics.uci.edu/dataset/164/bag+of+words\n11\n\nA parametrizable algorithm for distributed approximate similarity search with arbitrary distances A Preprint\nnumber of prototype points ( ğ‘›ğ‘ƒğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’ğ‘  ) calculated for each group at the lowest level ( ğ‘›1). Subsequently, the search\nfor the 10-ANN of each query point ( ğ‘) in the corresponding test set is performed using NSA. The optimal radius\nparameter(ğ‘Ÿ)forconfiguringNSAisselectedbasedonthedatasetâ€™sdistributionandthedistancefunctionemployed.\nThe values of these parameters, determined for each experiment, are presented in Table 2.\nTable 2: PDASC configuration parameters used in our experiments\nDataset gl n Distance ğ‘Ÿ\nMunicipalities 60 30Manhattan 3.25\nEuclidean 2.25\nChebyshev 2.25\nCosine 0.01\nHaversine 0.05\nMNIST 1000 500Manhattan 40000\nEuclidean 2850\nChebyshev 265\nCosine 0.7\nGLOVE 1000 500Manhattan 105\nEuclidean 13\nChebyshev 5.2\nCosine 1.25\nNYTimes 1000 500Manhattan 21\nEuclidean 1.6\nChebyshev 1.2\nCosine 0.35\nThe performance of the algorithm in each experiment will be evaluated by computing its recall, as described in Section\n4.3. Additionally, to further assess the proposed method, the same experiments will be conducted using the two\nstate-of-the-art algorithms described in Section 4.4, employing their default parameter settings. The results will then be\ncompared to determine PDASCâ€™s effectiveness in ANN search.\n4.3 Performance evaluation\nA straightforward method to evaluate an algorithmâ€™s performance is to search for the approximate ğ‘˜-NN of every query\npoint in a test dataset against a previously indexed train dataset.\nWeusethe recalltomeasuretheaccuracyofthealgorithmâ€™sperformanceinthistask,therebyassessingitsabilityto\nretrieve the relevant elements during a search. This quality measure is defined as:\nğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ =Number of Relevant Items Retrieved\nTotal Number of Relevant Items in the Search\nInthiscontext,the RelevantItems aredefinedasthegroundtruthneighbours,correspondingtothecorrectsetof ğ‘˜points\nthealgorithmshouldideallyidentifyastheNNofagivenquerypoint. Theseitemsaretypicallyobtainedusinganexact\nsearch algorithm, with the KD-Tree ( ğ¾-Dimensional Tree)[ 41] being the preferred method due to its computational\nefficiency. However, when the KD-Tree does not support the chosen distance function, the Brute-Force method, which\naccommodates a broader range of distance functions, is employed instead.\n4.4 Algorithm comparison\nInourattempttocomparePDASCwithsimilaralgorithms,weaimedtoselectthosethat,likeourproposedmethod,\nofferuserparametrisationtoadapttospecificproblemrequirements,operateindistributedenvironments,andsupport\nanydistancefunction. However,wefound noalgorithmthatstrictlymeetsallthese criteria. Asaresult, wecompared\nthe performance of PDASC in each experiment with that of two robust and widely used algorithms. The main features\nof these algorithms, which also differ in their index construction strategies, are described below:\nâ€¢FLANN (Fast Library for Approximate Nearest Neighbors) [ 42], is an efficient tree-based method that applies\nrandom projection and hierarchical partitioning to create index structures that speed up search operations.\nUserscanmanuallyadjustparameterstofine-tunetheprocessaccordingtotheirpreferencesorusetheprovided\n12\n\nA parametrizable algorithm for distributed approximate similarity search with arbitrary distances A Preprint\nautomated approach for selecting the optimal algorithm and parameter settings for a given dataset. It also\nincludes a distributed framework, making it suitable for distributed environments. However, the range of\nsupported distance functions is quite limited. We utilized the Python bindings provided by the pyflann 7\nlibrary and employed its automated parameter setting to ensure optimal performance.\nâ€¢ThePyNNDescent(PyNN)algorithmisaPython-basedimplementationoftheNNDescentalgorithm[ 43]8\nthat efficiently performs nearest neighbour searches by building a graph-based index connecting each data\npoint to its ANN. PyNN also allows users to fine-tune parameters, optimising the trade-off between search\naccuracyandcomputationalefficiency. Itiswell-suitedforhigh-dimensionaldatasetsandsupportsvarious\ndistance functions. However, it does not natively support distributed environments.\n4.5 Experimental results\nUpon completing the experiments for each dataset-distance-method combination, we can assess the effectiveness of the\nproposed method and determine the suitability of specific distance functions for distinct datasets based on their inherent\nproperties. Theresults oftheseexperimentsare presentedinFigure 5andtheysupport oneofthemain hypothesesof\nthis research: data indexing using non-Euclidean distances is not only feasible but can also potentially yield improved\noutcomes.\nFirst, the series of experiments described in Section 4.2 were conducted using the Municipalities dataset. Figure 5a\npresentsapointplotillustratingtherecallachievedintheapproximate10 -NNsearchusingtheFLANN,PyNN,and\nPDASC methods with Manhattan, Euclidean, Chebyshev, cosine, and Haversine distance functions, respectively.\nIn this dataset, all methods achieve high recall rates with Minkowski distances, with PyNN and PDASC reaching\nnearly 100% recall. Notably, PDASC maintains this high performance with the cosine distance, whereas PyNNâ€™s recall\nsignificantly decreases to 54%, marking the lowest recall observed for this dataset. This dataset lets us evaluate the\nHaversine distance for two-dimensional data, where both PDASC and PyNN exhibit a recall of 100%, confirming\nitseffectivenessforthistypeofdata. SinceFLANNdoesnotsupportChebyshev,cosine,orHaversinedistances,its\nperformance in these scenarios cannot be evaluated.\nThesefindingsdemonstratethatPDASCâ€™sbottom-upindexingstrategyeffectivelyorganizesreal-worlddatasetsthat\noften exhibit complex structures and anomalies, including significant outliers.\nThe second experiment employs the MNIST dataset, which acts as an interesting case study thanks to its high\ndimensionalityandinherentsparsity. Theresultsobtainedinthis experimentarepresentedinFigure5b. Theanalysis\nreveals that the recall achieved by PyNN and PDASC are highly comparable, with PDASC consistently achieving\nslightly higher values. Both methods reach high recall values for all distances except for Chebyshev, which has a recall\nbelow70%. Additionally,theusefulnessofFLANNisquestionednotonlybyitslackofsupportforChebyshevand\ncosine distances but also by its low recall (less than 60%) for this dataset, which is more complex than the one analysed\npreviously.\nHereafter, the third experiment involves the analysis of the GLOVE dataset, a high-dimensional dataset that contains the\nlargest number of elements among the datasets included in these experiments. Figure 5c illustrates the recall achieved\nby each method across the evaluated distances.\nOnce again, the lack of efficiency is evident in FLANN Â´s results since it cannot reach 15% with either Euclidean or the\nManhattandistance. If we focus onPyNNmethod andcomparethe recallforthe four distances, wecan seethatthe\nManhattandistanceprovides agoodaccuracy andthe Chebyshev distancegives theworst recall,while Euclideanand\ncosinedistancesofferasimilarperformanceofover65%. However,inthecaseofPDASC,itmaintainsaconsistent\nrecall of 100%, regardless of the distance function used.\nFinally, the last experiment replicates the same analysis using a dataset characterised by high dimensionality, featuring a\ndata-to-dimensionality ratio lower than that of GLOVE but higher than MNIST: the NYtimes dataset.\nFigure 5dillustrates the recallachievedby FLANN, PyNNand PDASC, highlighting themost striking findingof this\nexperiment: achieving good performance depends not only on the use of a suitable method but also on the choice of an\nappropriate distance function. Although PDASC consistently outperforms the other two algorithms in all scenarios, the\ntrends observed for PyNN are similar to PDASC. Both algorithms exhibit the lowest recall with the Euclidean distance,\nfollowed by the Manhattan distance, and show considerable improvement with the Chebyshev distance. The cosine\ndistance achieves the best results with PDASC, attaining a recall close to 100% and significantly overtaking PyNN,\nreaching a recall slightly above 65%. Once again, FLANN exhibits the poorest performance.\n7https://github.com/primetang/pyflann\n8https://pynndescent.readthedocs.io/en/latest/\n13\n\nA parametrizable algorithm for distributed approximate similarity search with arbitrary distances A Preprint\n(a)Xğ‘šğ‘¢ğ‘›ğ‘–dataset\n (b)Xğ‘€ğ‘ğ¼ğ‘†ğ‘‡dataset\n(c)Xğºğ¿ğ‘‚ğ‘‰ğ¸dataset\n (d)Xğ‘ğ‘Œğ‘¡ğ‘–ğ‘šğ‘’ğ‘ dataset\nFigure5: Pointplotillustratingtherecallperformanceofthreealgorithmsacrossvariousdistancemetricsinapproximate\n10-NN searches for each dataset.\nThrough this experiment, we empirically demonstrate what has been stated theoretically in earlier sections of this\nmanuscript: defaulting to Euclidean distance in indexing structures and similarity queries is a self-imposed limitation\nthat can result in suboptimal performance.\n5 Conclusions and future work\nANNsearchisanessentialtechniqueinscenarioswhereexactapproachescannotprovidefastresponses,particularly\nwhendealingwithlarge,heterogeneousdatasetsthatoftenexhibithighdimensionalityandhighsparsityorareimpractical\nto process or store on a single machine.\nAlthoughithasbeendemonstratedthat,duetothe curseofdimensionality ,theEuclideandistancemaynotaccurately\ncapture the true proximity between data points in high-dimensional spaces, it remains one of the most widely used\nmeasuresinindexingstructuresandsearchalgorithms[ 27],highlightingtheneedtoexplorealternativedistancemetrics\nthat more effectively address these challenges.\nTo address this gap, we propose PDASC, a parametrizable algorithm for approximate ğ‘˜-NN search that combines\nan innovative multilevel index particularly adept at managing outliers, highly imbalanced datasets, and sparse data\n14\n\nA parametrizable algorithm for distributed approximate similarity search with arbitrary distances A Preprint\ndistributionswiththeflexibilitytosupportarbitrarydistancefunctionsachievedthroughtheintegrationofclustering\nalgorithms that inherently accommodate them.\nExperimental evaluation on real-world datasets varying in type, size, dimensionality, and sparsity, shows that PDASC,\nparametrisedwiththe ğ‘˜-medoidsclusteringalgorithm,consistentlyoutperformsalternativeindexingmethodsforall\ndistance functions considered in our study (see Section 4.5 for details).\nThese results validate PDASC as a reliable ANN search method, suitable for use in distributed data environments\nand handling datasets where selecting the most appropriate distance function is non-trivial, all without the need for\nadditionalhardwareaccelerationstrategiessuchasGPUsorTPUs. Additionally,theysupportthecorehypothesisof\nthiswork: insimilaritysearchproblems, itisimperativetofirstdeterminethemostappropriatedistancefunctionforthe\ndataset tobe indexed, as notall distance functionsare equally effectivefora given problem,and using thedefault one\nmay result in suboptimal performance.\nOurproposalprovidesasolidfoundationforfurtherimprovements,andaspartofourfuturework,weplantoextend\nthis research in several ways:\nâ€¢Oncethesupportforarbitrarydistancefunctionsisachieved,wewillevaluatePDASCâ€™sperformanceusing\nalternative distance functions aiming at assessing whether may be better suited for datasets with specific\nfeatures. For instance, exploring the effectiveness of the Jaccard distance for ANN searches in datasets\ncontaining qualitative dataâ€”or a combination of both qualitative and quantitative features.\nâ€¢WhilePDASCyieldssatisfactoryexperimentalresults,itscomputationalefficiencycouldbefurtherimproved,so\nwemayconsiderintegratingaPythonJITcompiler,suchasNumba 9,orportingPDASCtoahigh-performance\nlanguage such as Go 10, known for scalability and concurrency.\nâ€¢Althoughfurtherresearchisneededtopreciselyquantifyitsimpact,experimentalobservationssuggestthat\nthesearchradiuscanaltertheperformanceofthesearchalgorithm. Hence,exploringadynamicadjustment\nofitsvalueaccordingtotheindexlevelcouldleadtosignificantimprovements. Regardingthis,identifying\nthe optimal search radius for each level based on the local distribution of dataset elements can be particularly\nvaluable since this could allow our algorithm to fully exploit its inherent ability to adapt to heterogeneous\nvalue densities and distributions.\nCRediT authorship contribution statement\nElena Garcia-Morato: Writing - original draft, Writing - review and editing, Data curation, Formal analysis,\nInvestigation, Software, Validation. Maria JesÃºs Algar: Writing - original draft, Writing - review and editing,\nInvestigation,Methodology,Datacuration,Formalanalysis,Software,Validation. CesarAlfaro: Writing-original\ndraft, Writing - review and editing, Investigation. Felipe Ortega: Writing - original draft, Writing - review and editing,\nInvestigation, Methodology, Data curation, Formal analysis, Software, Validation, Project administration, Supervision.\nJavier Gomez: Writing - original draft, Writing - review and editing, Investigation, Supervision. Javier M. Moguerza:\nWriting - review and editing, Conceptualization, Funding acquisition, Investigation, Supervision.\nDeclaration of competing interest\nThe authors declare that they have no known competing financial interests or personal relationships that could have\nappeared to influence the work reported in this paper.\nAcknowledgments\nWe acknowledge the financial support from MCIN/AEI/10.13039/501100011033 and the European Union NextGenera-\ntionEU/PRTR through grant TED2021-131295B-C33, as well as from the Spanish Ministry of Science, Innovation and\nUniversities via grant XMIDAS (PID2021-122640OB-I00). Universidad Rey Juan Carlos provided additional support\nthrough the 2020 Predoctoral Research Trainees program (PREDOC 20-102).\nData Availability\nAll datasets used in experiments are publicly available.\n9https://numba.pydata.org\n10https://go.dev\n15\n\nA parametrizable algorithm for distributed approximate similarity search with arbitrary distances A Preprint\nAll code related to the proposed algorithm and the implementation of experiments is hosted in the elenagarciamorato/P-\nDASC GitHub repository, specifically in the release with DOI 10.5281/zenodo.15082683).\nReferences\n[1]Roger Weber, Hans-JÃ¶rg Schek, and Stephen Blott. A Quantitative Analysis and Performance Study for Similarity-\nSearch Methods in High-Dimensional Spaces. In Ashish Gupta, Oded Shmueli, and Jennifer Widom, editors,\nProceedings of the 24rd International Conference on Very Large Data Bases, VLDBâ€™98, August 24-27, 1998,\nNew York City, New York, USA , pages 194â€“205. Morgan Kaufmann, 1998. URL http://www.vldb.org/conf/\n1998/p194.pdf .\n[2]Christian BÃ¶hm, Stefan Berchtold, and Daniel A. Keim. Searching in High-Dimensional Spaces: Index Structures\nforImprovingthePerformanceofMultimediaDatabases. ACMComputingSurveys ,33(3):322â€“373,September\n2001. ISSN 0360-0300. doi:10.1145/502807.502809.\n[3]Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. The Case for Learned Index Structures.\nInProceedings of the 2018 International Conference on Management of Data, SIGMOD â€™18, Houston, TX, USA,\nJune 10-15, 2018 , page 489â€“504, New York, NY, USA, 2018. Association for Computing Machinery. ISBN\n9781450347037. doi:10.1145/3183713.3196909.\n[4]RenzhiWu,JingfanMeng,JieJeffXu,HuayiWang,andKexinRong. Rethinkingsimilaritysearch: Embracing\nsmarter mechanisms over smarter data, 2023. URL https://arxiv.org/abs/2308.00909 .\n[5]Sergey Shvydun. Models of similarity in complex networks. PeerJ Computer Science , 9:e1371, May 2023. ISSN\n2376-5992. doi:10.7717/peerj-cs.1371.\n[6]Rezvan Ehsani and Finn DrablÃ¸s. Robust Distance Measures for kNN Classification of Cancer Data. Cancer\nInformatics , 19:117693512096554, 1 2020. ISSN 1176-9351, 1176-9351. doi:10.1177/1176935120965542.\n[7]Martin AumÃ¼ller, Erik Bernhardsson, and Alexander Faithfull. ANN-Benchmarks: A benchmarking tool\nfor approximate nearest neighbor algorithms. Information Systems , 87:101374, 2020. ISSN 0306-4379.\ndoi:10.1016/j.is.2019.02.006.\n[8]Elzbieta Pekalska and Robert P. W. Duin. The Dissimilarity Representation for Pattern Recognition: Foundations\nand Applications , volume 64 of Series in Machine Perception and Artificial Intelligence . World Scientific,\nHackensack, N.J, 11 2005. ISBN 978-981-256-530-3. doi:10.1142/5965.\n[9]Edgar ChÃ¡vez, Gonzalo Navarro, Ricardo Baeza-Yates, and JosÃ© Luis MarroquÃ­n. Searching in Metric Spaces.\nACM Computing Surveys , 33(3):273â€“321, September 2001. ISSN 0360-0300. doi:10.1145/502807.502808.\n[10]HananSamet. FoundationsofMultidimensionalandMetricDataStructures . TheMorganKaufmannSeriesin\nData Management Systems. Academic Press, San Francisco, CA, USA, 2006. ISBN 978-0-12-369446-1.\n[11]Pavel Zezula, Giuseppe Amato, Vlastislav Dohnal, and Michal Batko. Similarity Search - The Metric Space\nApproach ,volume32of AdvancesinDatabaseSystems . Springer,NewYork,NY,2006. ISBN978-0-387-29146-8.\ndoi:10.1007/0-387-29151-2.\n[12]RicardoBaeza-YatesandBerthierRibeiro-Neto. ModernInformationRetrieval: TheConceptsandTechnology\nBehind Search . Pearson Education Ltd., Harlow, England, 2 edition, 2011. ISBN 978-0-321-41691-9. URL\nhttp://www.mir2ed.org/ .\n[13]RahelArnold,WernerBailer,RalphGasser,BjÃ¶rnÃ.JÃ³nsson,OmarShahbazKhan,HeikoSchuldt,FlorianSpiess,\nand Lucia Vadicamo. Multimedia information retrieval in xr. In Proceedings of the 32nd ACM International\nConference on Multimedia , MM â€™24, page 11285â€“11286, New York, NY, USA, 2024. Association for Computing\nMachinery. ISBN 9798400706868. doi:10.1145/3664647.3689176.\n[14]Hamed Zamani, Fernando Diaz, Mostafa Dehghani, Donald Metzler, and Michael Bendersky. Retrieval-enhanced\nmachinelearning. In Proceedingsofthe45thInternationalACMSIGIRConferenceonResearchandDevelopment\ninInformationRetrieval ,SIGIRâ€™22,page2875â€“2886,NewYork,NY,USA,2022.AssociationforComputing\nMachinery. ISBN 9781450387323. doi:10.1145/3477495.3531722.\n[15]MichelVanKempen,StephanieSKim,CharlotteTumescheit,MilotMirdita,JeongjaeLee,CameronLMGilchrist,\nJohannes SÃ¶ding, and Martin Steinegger. Fast and accurate protein structure search with Foldseek. Nature\nBiotechnology , 42(2):243â€“246, 2024. doi:10.1038/s41587-023-01773-0.\n[16]Siyuan Shang, Xuehui Du, Xiaohan Wang, and Aodi Liu. Private approximate nearest neighbor search for\non-chaindatabasedonlocality-sensitivehashing. FutureGenerationComputerSystems , 164:107586,2025. ISSN\n0167-739X. doi:https://doi.org/10.1016/j.future.2024.107586.\n16\n\nA parametrizable algorithm for distributed approximate similarity search with arbitrary distances A Preprint\n[17]Mengzhao Wang, Xiaoliang Xu, Qiang Yue, and Yuxiang Wang. A comprehensive survey and experimental\ncomparison of graph-based approximate nearest neighbor search. Proc. VLDB Endow. , 14(11):1964â€“1978, 2021.\ndoi:10.14778/3476249.3476255.\n[18]ZeyuWang,PengWang,ThemisPalpanas,andWeiWang. Graph-andtree-basedindexesforhigh-dimensional\nvectorsimilaritysearch: Analyses,comparisons,andfuturedirections. IEEEDataEngineeringBulletin ,46(3):\n3â€“21, 2023. URL http://sites.computer.org/debull/A23sept/p3.pdf .\n[19]Donna Xu, Ivor W. Tsang, and Ying Zhang. Online product quantization. IEEE Trans. on Knowl. and Data Eng. ,\n30(11),2018. ISSN1041-4347. doi:10.1109/TKDE.2018.2817526. URL https://doi.org/10.1109/TKDE.\n2018.2817526 .\n[20]Alexandr Andoni and Piotr Indyk. Locality-sensitive hashing scheme based on p-stable distributions. In\nProceedingsoftheeighteenthannualACM-SIAMsymposiumonDiscretealgorithms ,pages857â€“865.SIAM,2006.\ndoi:10.1145/997817.997857.\n[21]Xi Zhao, Yao Tian, Kai Huang, Bolong Zheng, and Xiaofang Zhou. Towards efficient index construction and\napproximate nearest neighbor search in high-dimensional spaces. Proc. VLDB Endow. , 16(8), 2023. ISSN\n2150-8097. doi:10.14778/3594512.3594527. URL https://doi.org/10.14778/3594512.3594527 .\n[22]Akhil Arora, Sakshi Sinha, Piyush Kumar, and Arnab Bhattacharya. HD-Index: Pushing the Scalability-Accuracy\nBoundary for Approximate kNN Search in High-Dimensional Spaces. Proc. VLDB Endow. , 11(8):906â€“919, 2018.\ndoi:10.14778/3204028.3204034.\n[23]Karima Echihabi, Panagiota Fatourou, Kostas Zoumpatianos, Themis Palpanas, and Houda Ben-\nbrahim. Hercules against data series similarity search. Proc. VLDB Endow. , 15(10):2005â€“2018, 2022.\ndoi:10.14778/3547305.3547308.\n[24]LiangZhang,NouraAlghamdi,MohamedY.Eltabakh,andElkeA.Rundensteiner. TARDIS:DistributedIndexing\nFrameworkforBig TimeSeriesData. In 35thIEEE InternationalConferenceonData Engineering,ICDE2019,\nMacao, China, April 8-11, 2019 , pages 1202â€“1213. IEEE, 2019. doi:10.1109/ICDE.2019.00110.\n[25]Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality.\nInProceedings of the thirtieth annual ACM symposium on Theory of computing - STOC â€™98 , pages 604â€“613,\nDallas, Texas, United States, 1998. ACM Press. ISBN 978-0-89791-962-3. doi:10.1145/276698.276876.\n[26]Kevin Beyer, Jonathan Goldstein, Raghu Ramakrishnan, and Uri Shaft. When Is â€œNearest Neighborâ€ Meaningful?\nIn Catriel Beeri and Peter Buneman, editors, Database Theory â€” ICDTâ€™99 , volume 1540 of Lecture Notes in\nComputerScience ,pages217â€“235.Springer,Berlin,Heidelberg,11999. ISBN978-3-540-49257-3. doi:10.1007/3-\n540-49257-7_15.\n[27]Wen Li, Ying Zhang, Yifang Sun, Wei Wang, Mingjie Li, Wenjie Zhang, and Xuemin Lin. Approximate Nearest\nNeighbor Search on High Dimensional Data â€” Experiments, Analyses, and Improvement. IEEE Transactions on\nKnowledge and Data Engineering , 32(8):1475â€“1488, 2020. doi:10.1109/TKDE.2019.2909204.\n[28]M.EmreCelebi,HassanA.Kingravi,andPatricioA.Vela. Acomparativestudyofefficientinitializationmethods\nfor the k-means clustering algorithm. Expert Systems with Applications , 40(1):200â€“210, 1 2013. ISSN 09574174.\ndoi:10.1016/j.eswa.2012.07.021.\n[29]Charu C. Aggarwal, Alexander Hinneburg, and Daniel A. Keim. On the Surprising Behavior of Distance Metrics\nin High Dimensional Space. In Jan Van den Bussche and Victor Vianu, editors, Database Theory â€” ICDT 2001 ,\npages 420â€“434, Berlin, Heidelberg, 2001. Springer. ISBN 978-3-540-44503-6. doi:10.1007/3-540-44503-X_27.\n[30] Roger W. Sinnott. Virtues of the haversine. Sky and Telescope , 68(2):159, 1984.\n[31]Rezania Azdy and Febriyanti Darnis. Use of haversine formula in finding distance between temporary shelter and\nwaste end processing sites. Journal of Physics: Conference Series , 1500:012104, 04 2020. doi:10.1088/1742-\n6596/1500/1/012104.\n[32]AndrewJamesCooper,Chelsea AnneRedman,DavidMarkStoneham,LuisFelipeGonzalez, andVictorKwesi\nEtse. A dynamic navigation model for unmanned aircraft systems and an application to autonomous front-on\nenvironmental sensing and photography using low-cost sensor systems. Sensors, 15(9):21537â€“21553, 2015. ISSN\n1424-8220. doi:10.3390/s150921537.\n[33]Stuart P. Lloyd. Least squares quantization in PCM. IEEE Trans. Inf. Theory , 28(2):129â€“136, 1982.\ndoi:10.1109/TIT.1982.1056489.\n[34]F.E.Maranzana. OntheLocationofSupplyPointstoMinimizeTransportCosts. OR,15(3):261, September1964.\nISSN 14732858. doi:10.2307/3007214.\n17\n\nA parametrizable algorithm for distributed approximate similarity search with arbitrary distances A Preprint\n[35]Leonard Kaufmann and Peter Rousseeuw. Clustering by Means of Medoids. Data Analysis based on the L1-Norm\nand Related Methods , pages 405â€“416, 01 1987.\n[36]Erich Schubert and Peter J. Rousseeuw. Faster k-Medoids Clustering: Improving the PAM, CLARA, and\nCLARANS Algorithms. In Similarity Search and Applications , pages 171â€“187, Switzerland, 2019. Springer\nCham. ISBN 978-3-030-32047-8. doi:10.1007/978-3-030-32047-8_16.\n[37]Erich Schubert and Peter J. Rousseeuw. Fast and eager k-medoids clustering: O(k) runtime improvement\nof the pam, clara, and clarans algorithms. Information Systems , 101:101804, 2021. ISSN 0306-4379.\ndoi:https://doi.org/10.1016/j.is.2021.101804. URL https://www.sciencedirect.com/science/article/\npii/S0306437921000557 .\n[38]ErichSchubertandLarsLenssen. Fastk-medoidsClusteringinRustandPython. JournalofOpenSourceSoftware ,\n7(75):4183, 7 2022. ISSN 2475-9066. doi:10.21105/joss.04183.\n[39]Martin AumÃ¼ller, Erik Bernhardsson, and Alexander John Faithfull. Ann-benchmarks: A benchmarking tool for\napproximate nearest neighbor algorithms. CoRR, abs/1807.05614, 2018. URL http://arxiv.org/abs/1807.\n05614.\n[40]PengfeiLi,HuaLu,QianZheng,LongYang,andGangPan. LISA:ALearnedIndexStructureforSpatialData. In\nDavid Maier, Rachel Pottinger, AnHai Doan, Wang-Chiew Tan, Abdussalam Alawini, and Hung Q. Ngo, editors,\nProceedings of the 2020 ACM International Conference on Management of Data, SIGMOD Conference 2020,\nonline conference, Portland, OR, USA, June 14-19, 2020 , SIGMOD â€™20, page 2119â€“2133, New York, NY, USA,\n2020. Association for Computing Machinery. ISBN 9781450367356. doi:10.1145/3318464.3389703.\n[41]JonLouisBentley. Multidimensionalbinarysearchtreesusedforassociativesearching. Commun.ACM ,18(9):\n509â€“517, September 1975. ISSN 0001-0782. doi:10.1145/361002.361007.\n[42]MariusMujaandDavidG.Lowe.ScalableNearestNeighborAlgorithmsforHighDimensionalData. IEEETransac-\ntions on Pattern Analysis and Machine Intelligence , 36(11):2227â€“2240, 2014. doi:10.1109/TPAMI.2014.2321376.\n[43]Wei Dong, Charikar Moses, and Kai Li. Efficient k-nearest neighbor graph construction for generic similarity\nmeasures. In Proceedingsofthe20thInternationalConferenceonWorldWideWeb,WWW2011,Hyderabad,India,\nMarch28-April 1,2011 ,WWWâ€™11, page577â€“586,NewYork, NY,USA,2011.ACM. ISBN9781450306324.\ndoi:10.1145/1963405.1963487.\n18",
  "textLength": 63786
}