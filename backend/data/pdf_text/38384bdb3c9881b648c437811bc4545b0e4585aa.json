{
  "paperId": "38384bdb3c9881b648c437811bc4545b0e4585aa",
  "title": "DILI: A Distribution-Driven Learned Index (Extended version)",
  "pdfPath": "38384bdb3c9881b648c437811bc4545b0e4585aa.pdf",
  "text": "DILI: A Distribution-Driven Learned Index (Extended version)\nPengfei Li\nAlibaba Group, China\nlpf367135@alibaba-inc.comHua Lu\nRoskilde University, Denmark\nluhua@ruc.dkRong Zhu\nAlibaba Group, China\nred.zr@alibaba-inc.com\nBolin Ding\nAlibaba Group, China\nbolin.ding@alibaba-inc.comLong Yang\nPeking University, China\nyanglong001@pku.edu.cnGang Pan\nZhejiang University, China\ngpan@zju.edu.cn\nABSTRACT\nTargeting in-memory one-dimensional search keys, we propose\na novel DIstribution-driven Learned Index tree ( DILI ), where a\nconcise and computation-efficient linear regression model is used\nfor each node. An internal nodeâ€™s key range is equally divided by its\nchild nodes such that a key search enjoys perfect model prediction\naccuracy to find the relevant leaf node. A leaf node uses machine\nlearning models to generate searchable data layout and thus accu-\nrately predicts the data record position for a key. To construct DILI,\nwe first build a bottom-up tree with linear regression models accord-\ning to global and local key distributions. Using the bottom-up tree,\nwe build DILI in a top-down manner, individualizing the fanouts\nfor internal nodes according to local distributions. DILI strikes a\ngood balance between the number of leaf nodes and the height\nof the tree, two critical factors of key search time. Moreover, we\ndesign flexible algorithms for DILI to efficiently insert and delete\nkeys and automatically adjust the tree structure when necessary.\nExtensive experimental results show that DILI outperforms the\nstate-of-the-art alternatives on different kinds of workloads.\n1 INTRODUCTION\nRecently, the learned index [ 29] is proposed to replace B+Tree [ 15]\nin database search. It stages machine learning models into a hierar-\nchy called Recursive Model Index (RMI). Given a search key ğ‘¥, RMI\npredicts, with some error bound, where ğ‘¥â€™s data is positioned in a\nmemory-resident dense array. Compared to B+Tree, RMI achieves\ncomparable and even better search performance. However, the lay-\nout of RMI, i.e., the number of stages and the number of models at\neach stage, must be fixed before the models are created. Also, RMI\nfails to support key insertions and deletions.\nTo support data updates, ALEX [ 18] extends RMI by using a\ngapped array layout for the leaf level models. Moreover, ALEX uses\ncost models to initiate the RMI structure and to dynamically adapt\nthe structure to updates. However, the stage layout of ALEX is not\nflexible enough as its fanout, i.e., the number of a nodeâ€™s child mod-\nels, is stipulated to a power of 2. This renders ALEXâ€™s internal nodesâ€™\nkey ranges relatively static, which may result in node layout not\ngood for particular key distributions, e.g.,lognormal distribution.\nAlso, ALEXâ€™s leaf level learned models do not guarantee accurate\npredictions. Thus, extra local search is needed to locate the required\ndata, which downgrades the search performance. More recently,\nLIPP [ 44] trains learned models for the whole dataset and places\ndata at the predicted positions. When multiple data records are\nassigned to the same position, a new node is created at the position\nHua Lu and Gang Pan are the corresponding authors.to hold them. However, this simple strategy ignores the data distri-\nbution and often results in long traversal paths. Also, compared to\nB+Tree and ALEX, LIPP consumes much more memory.\nIn this paper, we design a novel index treeâ€”DIstribution-driven\nLearned Index (DILI). Its each node features an individualized fanout\nand a model created for a data portion whose key sequence is cov-\nered by the nodeâ€™s range. For an internal node, its child nodes\nequally divide its range. Thus, the cost is minimized to locate the\nrelevant leaf node in a key search. In a leaf node, an entry array\nğ‘½holds the keys in the nodeâ€™s range and the pointers to the cor-\nresponding data records. In addition, a leaf node uses an efficient\nlinear regression model to map its keys to the positions in ğ‘½.\nA critical issue for constructing DILI is to determine its node\nlayout that is able to achieve good search performance. We design a\nsophisticated approach aware of data distributions and search costs.\nA key search in DILI involves two steps: 1) finding the leaf node\ncovering the given key and 2) local search inside the leaf node. Ac-\ncordingly, the general search performance depends on two factors:\nleaf nodesâ€™ depths and linear regression modelsâ€™ accuracy in the\nleaf nodes. Both factors should be considered in DILI construction.\nTo this end, we propose a two-phase bulk loading approach. The\nfirst phase creates a distribution-driven bottom-up tree (BU-Tree),\nwhose node layout is determined by a greedy merging algorithm\nthat considers both aforementioned factors. The merging creates\nlinear regression models, starting at the bottom level to fully utilize\nthe known key distribution. As a result, the models in the BU-Treeâ€™s\nleaf nodes guarantee good accuracy. Basically, we build DILI by\nmaking its node layout overall similar to that of the BU-Tree. How-\never, a BU internal nodeâ€™s range is not necessarily equally divided\nby its child nodes. Therefore, search in the BU-Treeâ€™s internal nodes\ncan incur extra time to decide which child node to visit. To this end,\nthe second phase converts the BU-Tree to a DILI by redistributing\nkeys among sibling nodes. When doing so, we carefully set different\nfanouts for DILIâ€™s different internal nodes according to their local\nkey distributions, such that each internal node is equally divided by\nits child nodes. Meanwhile, we retain good model accuracy in DILIâ€™s\nleaf nodes and keep them at the same level as the counterparts in\nthe BU-Tree. As a result, we obtain a DILI that is efficient at finding\nleaf nodes and have linear regression models of high accuracy in\nleaf nodes. In other words, we first create a mirror model (BU-Tree)\nthat exhibits a good node layout but cannot guarantee perfect accu-\nracy, and then we create another similar model (DILI) that avoids\nthe mirror modelâ€™s drawbacks but maintains its advantages.\nIt is noteworthy that we build the BU-Tree and DILI according\nto detailed analyses of search costs, which consider caching effects\nin the main-memory context. Though DILI and ALEX share somearXiv:2304.08817v2  [cs.DB]  18 May 2023\n\nPengfei Li, Hua Lu, Rong Zhu, Bolin Ding, Long Yang, and Gang Pan\n......\n : the entry array...\nğ–­ğ–£1=ğ–­ğ–³.ğ‘ª[âŒŠğ–­ğ–³.â„’â„›(ğ‘¥)âŒ‹] \nğ–­ğ–³.ğ–¿ğ—ˆ=4 \nğ–­ğ–³.ğ–»=0.05 \nğ–±.ğ–¿ğ—ˆ=3 \nğ–­ğ–³.ğ—…ğ–»=80 \nğ–­ğ–³.ğ–º=âˆ’4 \n:       , the linear regression model\nğ–­ğ–³ \nğ‘¥=101 \n \nâ„’â„› \nğ‘½ \n... : pair\n leaf node    \n   pointer :\n : NULL\n101Leaf \nNodeInternal\n Node\nğ–­ğ–³.ğ—ğ–»=160 \nğ–­ğ–£2 Search key\nParamsInternal Node\n: the children\n nodes array\nğ‘ª :       , the linear regression\nmodel with parameters a and b\nâ„’â„› \nImportant Components\nInternal Node ExampleLeaf Node\nğ–­ğ–³=ğ–±.ğ‘ª[ğ–±.â„’â„›(ğ‘¥)] with\nwith\nImportant ComponentsRoot node\nğ–± Input\nStep-1\nStep-2\nStep-3\nSearch stepSymbol \nmeaningsStep-1\nâŒŠğ–±.â„’â„›(ğ‘¥)âŒ‹ = 1 \nğ–­ğ–³=ğ–±.ğ‘ª[1] \nStep-2\nâŒŠğ–­ğ–³.â„’â„›(ğ‘¥)âŒ‹ = 1 \nğ–­ğ–£1=ğ–­ğ–³.ğ‘ª[1] \nStep-3\nâŒŠğ–­ğ–£1.â„’â„›(ğ‘¥)âŒ‹ = 1 \nğ–­ğ–£2=ğ–­ğ–£1.ğ‘½[1] \nLeaf Node \nExample\nğ‘ğ‘¥ Step-4Step-4\nâŒŠğ–­ğ–£2.â„’â„›(ğ‘¥)âŒ‹ = 0 \nğ‘=ğ–­ğ–£2.ğ‘½ 0  \nOutput record pointer102    \n...\nStep-i\nğ–­ğ–£1.ğ–»=0.1, \nğ–­ğ–£1.ğ–º=âˆ’9 Params\nğ–­ğ–£1 \nğ–±.ğ—…ğ–»=0, ğ–±.ğ—ğ–»=240 \nğ–­ğ–£1 \nğ–­ğ–£1 \nğ–­ğ–³ \nğ–­ğ–£1.ğ‘ª[âŒŠğ–­ğ–£1.â„’â„›(ğ‘¥)âŒ‹] \nğ–­ğ–£2= \nğ–­ğ–£2 \nğ–­ğ–³ \nğ–­ğ–£1.ğ—…ğ–»=100,  \nğ–­ğ–£1.ğ—ğ–»=120 \nğ‘ \nğ‘ğ‘¥=ğ‘.ğ‘ğ‘¡ğ‘Ÿ Pair\nFigure 1: The Structure of DILI\nstructural similarities, they are constructed according to different\nperspectives. ALEX is built top down, while the BU-Tree is bot-\ntom up and initially deals with all the keys. Thus, the BU-Tree\nunderstands the key distribution better and partitions them into\nleaf nodes more reasonably. This make the â€˜mirroredâ€™ DILI achieve\ngood local search performance. Also, our proposed cost function\nmakes the BU-Tree (and DILI) have a suitable height. As a result,\nfinding leaf nodes in DILI consumes less time.\nOur bulk loading approach makes the linear regression models\nin DILIâ€™s leaf nodes have high but not 100% accuracy. We find that\nthe â€˜last-mileâ€™ local search in the leaf nodes is often the bottleneck of\nan entire query. To this end, we conduct a local optimization at each\nleaf node after the bulk loading, forcibly making the key-to-position\nmapping precise. If multiple keys are mapped into the same position,\na new child node is created to hold them. Experimental results show\nthat local optimizations improve the query performance of DILI by\navoiding the local search inside the leaf nodes.\nOur local optimization is inspired by LIPP [ 44] and LISA [ 31].\nHowever, unlike LIPP, DILIâ€™s local optimization applies to leaf nodes\nonly. Also, the two phase bulk loading approach makes DILI rea-\nsonably partitions data such that the keys covered by leaf nodes\nare almost linearly distributed. Compared to LIPP, the linear re-\ngression models in DILIâ€™s leaf nodes assign fewer keys the same\nslots. Thus, DILI encounters less conflicts and achieves better search\nperformance and lower memory consumption.\nFurthermore, DILI supports data updates. When an inserted key\nconflicts with an existing key at a data slot of DILIâ€™s leaf node, our\ninsertion algorithm creates a new leaf node to cover the conflict-\ning data. Also, DILI redistributes data covered by a leaf node in\na balanced way, when insertions generate too many nodes and\ndegrades the query performance. Meanwhile, it allocates more data\nslots for those leaf nodes that encounter more frequent conflicts.\nIn this way, DILIâ€™s height is bounded and the query performance\ndowngrades slightly even for many insertions. In addition, when a\nleaf node covers only one key after some deletions, this node will be\ntrimmed to improve performance and save memory consumption.\nWe make the following major contributions in this paper.\nâ€¢We design a distribution-driven learned index DILI for in-memory\n1D keys, together with algorithms and cost analysis.â€¢Accordingly, we design a distribution-driven BU-Tree as a node\nlayout reference for DILI, and after more specific cost analyses\nwe devise an algorithm to construct DILI based on BU-Tree.\nâ€¢We propose a local optimization on DILIâ€™s leaf nodes to avoid\nthe local search and improve query performance.\nâ€¢To update DILI for key insertions and deletions, we devise effi-\ncient algorithms that retain search performance.\nâ€¢We experimentally validate DILIâ€™s performance advantage over\nstate-of-the-art alternatives on synthetic and real datasets.\nThe rest of the paper is organized as follows. Section 2 gives an\noverview of DILI. Section 3 analyses its search cost. Sections 4, 5\nand 6 elaborate on DILIâ€™s construction, local optimization and up-\ndates, respectively. Section 7 reports on the experimental studies.\nSection 8 reviews the related work. Section 9 concludes the paper.\n2 OVERVIEW OF DILI\nTable 1 lists the important notations used in the paper.\nTable 1: Notations\nN.fo Fanout of the node N.Ncan be an internal or a leaf node.\nN.LR Linear regression model of the node N\nNT.ğ‘ª The child node array of the internal node NT\nND.ğ‘½ The entry array of the leaf node ND\nND.Î© Number of pairs covered by the leaf node ND\nND.Î”Total number of entries to be accessed to search for all keys\ncovered by ND, starting from ND\nND.ğœ…Average number of entries to be accessed to search for a\nkey, starting from ND, after the last local optimization to ND\nND.ğ›¼ Number of adjustments of NDso far\nTs(ğ‘¥) Search cost of key ğ‘¥in DILI without local optimization.\nTBns(N,ğ‘¥,â„)Search cost of key ğ‘¥w.r.t. a BU node Nat heightâ„\nTBea(ğ‘¿â„,ğ‘¿)Estimated accumulated search cost of the break points list\nğ‘¿â„for the key set ğ‘¿in the BU-Tree\nDefinition 1 (Pair). A pair is a 2-tuple ğ‘=(ğ‘˜ğ‘’ğ‘¦,ğ‘ğ‘¡ğ‘Ÿ), where\nğ‘ğ‘¡ğ‘Ÿis a pointer to the data record identified by ğ‘˜ğ‘’ğ‘¦.\nLetğ‘·=[ğ‘0,ğ‘1,Â·Â·Â·,ğ‘|ğ‘·|âˆ’1]be an array of pairs, and keys (ğ‘·) =\n[ğ‘0.ğ‘˜ğ‘’ğ‘¦,ğ‘ 1.ğ‘˜ğ‘’ğ‘¦,Â·Â·Â·,ğ‘|ğ‘·|âˆ’1.ğ‘˜ğ‘’ğ‘¦]the key sequence from ğ‘·.\n\nDILI: A Distribution-Driven Learned Index (Extended version)\nDefinition 2 (Least square estimator). Givenğ¼âŠ†[Ëœğ‘›]={0,1,\nÂ·Â·Â·,ğ‘›âˆ’1}, two sequences ğ‘¿=[ğ‘¥0,Â·Â·Â·,ğ‘¥ğ‘›âˆ’1]andğ’€=[ğ‘¦0,Â·Â·Â·,ğ‘¦ğ‘›âˆ’1],\nthe least square estimator restricted to ğ¼is the linear function that\nminimizesÃ\nğ‘–âˆˆğ¼(ğ‘¦ğ‘–âˆ’ğ‘“(ğ‘¥ğ‘–))2over any linear function ğ‘“. We use\nleastSqares (ğ‘¿,ğ’€,ğ¼) to denote an algorithm that finds the least\nsquare estimator for the data points restricted to ğ¼. Whenğ¼=[Ëœğ‘›],\nwe simplify leastSqares (ğ‘¿,ğ’€,ğ¼) toleastSqares (ğ‘¿,ğ’€).\nFig. 1 illustrates the structure of DILI. The depths of its leaf\nnodes may be different, i.e., DILI is an unbalanced tree. Instead of\nhaving key-pointer pairs, a node in DILI contains a model for index-\ning purpose. Specifically, a node Nkeeps two numbers N.lband\nN.ubsuch that[N.lb,N.ub)forms Nâ€™srange ,i.e., the key sequence\ncovered by N. A node N, be internal or not, also stores a linear\nregression model N.LRparameterized by its intercept ğ‘and slope\nğ‘,i.e.,N.LR(ğ‘¥)=ğ‘+ğ‘ğ‘¥. Such models serve different purposes in\ninternal and leaf nodes.\nInternal Nodes. DILIâ€™s internal nodes are represented as red dot-\nted boxes in the bottom-middle part of Fig. 1. An internal node NT\nstores a linear regression model NT.LRand an array NT.ğ‘ªof point-\ners to NTâ€™s child nodes. Note that NT.ğ‘ªâ€™s each element is a simple\npointer, without any keys. In Fig. 1, an internal nodeâ€™s child nodes\nare represented as small equal sized rectangles. This is because\nNTâ€™s child nodes equally divide NTâ€™s range. Unlike B+Tree, we im-\npose no constraints on NTâ€™s fanout, i.e., the length of NT.ğ‘ª. Also,\nan internal node NTin DILI does not need to store an additional\nordered set of elements to describe the childrenâ€™s ranges, because\nthey are clearly described by the linear regression model. Given a\nkeyğ‘¥, we can easily know which child node covers ğ‘¥with a few\nsimple calculations. When a search goes downward in the tree, NT\nuses NT.LRto â€˜computeâ€™ the location (in NT.ğ‘ª) of the pointer to\nthe next child node to visit. Let NT.fodenote the fanout of NT. The\ninterceptğ‘and the slope ğ‘ofNT.LRare calculated as follows:\nğ‘=NT.fo/(NT.ubâˆ’NT.lb),ğ‘=âˆ’ğ‘Ã—NT.lb (1)\nAccordingly, NTâ€™sğ‘–th child nodeâ€™s range is [NT.lb+ğ‘–\nğ‘,NT.lb+ğ‘–+1\nğ‘\u0001,\ni.e.,[NT.LRâˆ’1(ğ‘–),NT.LRâˆ’1(ğ‘–+1)\u0001. All NTâ€™s child nodesâ€™ ranges\nare of equal length. For example, in Fig. 1, the internal node NThas\nfour children and its range is [80,160).NTâ€™s second child node N1\nD\nis assigned a range of [NT.lb+1\nNT.ğ‘,NT.lb+2\nNT.ğ‘\u0001=[100,120).\nLeaf Nodes. The leaf nodes in DILI are represented by circles in\nFig. 1, where the ellipse in the bottom-middle part gives the details.\nA leaf node NDstores an entry array ğ‘½and a linear regression\nmodelLR. Each entry is a pair, a pointer to another leaf node\nor a NULL flag indicating the corresponding slot of ğ‘½is empty.\nND.ğ‘½may cover leaf node pointers or NULL flags due to the local\noptimization strategy applied to the leaf nodes, which will be in-\ntroduced later in this section and Section 5. The learned model LR\nmaps a key to a position in ğ‘½. Unlike those models in internal nodes,\nLRis the solver to the mean squared error minimization problem,\nwhose input is the keys of ğ‘·ğ¿,i.e., the pairs covered by NDâ€™s range,\nand ground truth is the corresponding indices in ğ‘·ğ¿. Specifically,\nND.LRâ‰œleastSqaures(keys(ğ‘·ğ¿),Ëœ[|ğ‘·ğ¿|]). At present, we sim-\nply assume ND.ğ‘½=ğ‘·ğ¿. In other words, ğ‘½tightly stores the key-\npointer pairs only. Note that each internal or leaf node only needs\nto store two parameters ğ‘andğ‘for its linear model.Search without Optimization. To search for a key ğ‘¥, we first\nfind the leaf node whose range covers ğ‘¥, using the function locate-\nLeafNode (lines 5â€“8 in Algorithm 1). This function starts at DILIâ€™s\nroot, iteratively uses the linear regression model in the current\ninternal node to â€˜computeâ€™ a location in the nodeâ€™s pointer array\nğ‘ª. It follows the pointers to child nodes iteratively until reaching a\nleaf node ND. As the internal nodesâ€™ linear regression models have\nperfect accuracy, i.e., they always choose the child nodes covering\nğ‘¥. Thus, no local search is needed inside an internal node.\nAlgorithm 1 Search (Root,ğ‘¥)\n1:NDâ†locateLeafNode (Root,ğ‘¥)\n2:ğ‘ğ‘œğ‘ â€²â†âŒŠND.LR(ğ‘¥)âŒ‹\n3:ğ‘â†ND.ğ‘½[ğ‘ğ‘œğ‘ ],ğ‘ğ‘œğ‘ â†exponentialSearch (ND.ğ‘½,ğ‘¥,ğ‘ğ‘œğ‘ â€²)\n4:return (ğ‘.ğ‘˜ğ‘’ğ‘¦ =ğ‘¥?ğ‘.ğ‘ğ‘¡ğ‘Ÿ : NULL)\n5:function locateLeafNode (Root,ğ‘¥)\n6: while Npoints to an internal node do\n7:ğ‘ğ‘œğ‘ â†âŒŠN.LR(ğ‘¥)âŒ‹;Nâ†N.ğ‘ª[ğ‘ğ‘œğ‘ ]\n8: return N\nAfter finding the leaf node ND, we search the pair array ND.ğ‘½\nfor the pair whose key is ğ‘¥(lines 2â€“4). Suppose the pair ğ‘âˆˆND.ğ‘½\nis the least upper bound1ofğ‘¥inND.ğ‘½. We use the model ND.LR\nto estimate ğ‘â€™s positionğ‘ğ‘œğ‘ â€²inND.ğ‘½(line 2). From the position\nğ‘ğ‘œğ‘ â€², an exponential search (line 3) is performed to find the actual\nposition ofğ‘. At the returned position ğ‘ğ‘œğ‘ is the pairğ‘(line 3). If\nğ‘.ğ‘˜ğ‘’ğ‘¦ is notğ‘¥, we return NULL as no data record contains the key\nğ‘¥. Otherwise, we return ğ‘.ğ‘ğ‘¡ğ‘Ÿ that points to ğ‘¥â€™s data (line 5).\nConstruction. DILI is built by considering the keysâ€™ distribution\nto reduce the expected lookup time of queries. We propose a novel\nbulk loading algorithm to build DILI in Section 4. The algorithm\nâ€˜learnsâ€™ a good node layout for DILI from a given pair set ğ‘·.\nLocal Optimization strategy. In the experiments, we find that the\nâ€˜last-mile searchâ€™ in the leaf nodes (line 3 in Algorithm 1) is usually\na bottleneck of the entire query. While our bulk loading algorithm\nmake keys covered by a leaf node NDalmost linearly distributed,\nND.LRcannot guarantee perfect accuracy. To address this issue,\nwe put a local optimization on each leaf node after its range and\nlinear regression model is determined. Inspired by the novel idea\nof LISA [ 31] that uses ML models to directly determine keysâ€™\nstorage positions instead of approximating them , the local\noptimization makes DILI avoid local search by forcibly placing\npairs at the returned position by the linear regression model. If\nğ‘ğ‘œğ‘ =ND.LR(ğ‘.ğ‘˜ğ‘’ğ‘¦), the pairğ‘will be put at ND.ğ‘½[ğ‘ğ‘œğ‘ ]. If the\npredictions of multiple pairs by ND.LR are the same, i.e., they\nconflict , a new leaf node will be created to deal with them. In this\ncase, the original leaf node itself will have its own child node.\nIt is noteworthy that Algorithm 1 is only used in DILIâ€™s bulk load-\ning stage. In practice, a search algorithm with the local optimization\nwill be adopted. The details of the local optimization as well as the\noptimized search algorithm are to be introduced in Section 5.\nUpdates. DILI supports data updates. Our insertion algorithm will\ncreate new leaf nodes to cover conflicting pairs if insertions in-\ncur conflicts. Meanwhile, pairs covered by a leaf node will be re-\ndistributed when too many node creations degrades the search\nperformance. The details are to be given in Section 6.\n1In a pair array ğ‘·, a keyğ‘¥â€™sleast upper bound (LUB) is a pairğ‘âˆˆğ‘·satisfying two\nconditions: 1) ğ‘.ğ‘˜ğ‘’ğ‘¦â‰¥ğ‘¥and 2) Ifâˆƒğ‘â€²âˆˆğ‘·s.t.ğ‘â€².ğ‘˜ğ‘’ğ‘¦â‰¥ğ‘¥, thenğ‘â€².ğ‘˜ğ‘’ğ‘¦â‰¥ğ‘.ğ‘˜ğ‘’ğ‘¦ .\n\nPengfei Li, Hua Lu, Rong Zhu, Bolin Ding, Long Yang, and Gang Pan\nDiscussion. As described above and illustrated in Fig. 1, DILIâ€™s\ninternal and leaf nodes feature different structures. Their local\narrays keep different types of elements, due to their different roles\nin search. Internal nodesâ€™ role in the search process is to efficiently\nlocate the leaf node covering the search key. To fulfill this, an\ninternal node NTâ€™s children nodes are assigned equal-size ranges\nthrough its model NT.LR. Thus, NT.ğ‘½arranges NTâ€™s child nodes\ntightly. In contrast, the search process needs to find the pair from\na leaf node NDâ€™s entry array. However, ND.LRmay predict for\nmultiple keys the same position in ND.ğ‘½. To process the conflicts,\nthe local optimization is adopted to create new leaf nodes to store\nthe conflicting keys. Since multiple keys may conflict at the same\npredicted position and at least one slot is preserved for each pair, it\nis possible that some slots in ND.ğ‘½have no contents. Thus, unlike\nNT.ğ‘ª,ND.ğ‘½accommodates different kinds of elements.\n3 SEARCH COST ANALYSIS\nThis section is a preparation for the bulk loading algorithm in\nSection 4. We make a detailed cache-aware cost analysis of Algo-\nrithm 1. Note that no local optimization is assumed at present. In\nother words, given any leaf node ND,ND.ğ‘½contains no leaf node\npointer and ND.LRdoes not guarantee perfect prediction accuracy.\nCost Analysis. Algorithm 1 consists of two steps: 1) finding the\nleaf node covering the search key and 2) local search inside the\nleaf node. Given a pair ğ‘with keyğ‘¥, suppose ND(with depth ğ·)\nis the leaf node covering ğ‘¥. Let ND.LRâ€™s prediction error for ğ‘¥be\nğœ–ğ‘¥=|ND.LR(ğ‘¥)âˆ’ğ‘ğ‘œğ‘ |whereğ‘ğ‘œğ‘ isğ‘â€™s position in ND.ğ‘½. The\nestimated search cost of key ğ‘¥is denoted by Ts(ğ‘¥)as follows.\nTs(ğ‘¥)â‰ˆ\u0000(ğ·âˆ’1)Ã—Tis(ğ‘¥)\u0001+Tds(ND,ğ‘¥) (2)\nTis(ğ‘¥)=(ğœƒN+ğœ‚+ğœƒC),Tds(ND,ğ‘¥)=ğœƒN+ğœ‚+tE(ND,ğ‘¥)\nwhere Tis(ğ‘¥)andTds(ND,ğ‘¥)denote the time spent in an internal\nnode and the leaf node NDcoveringğ‘¥respectively; ğœƒNandğœ‚are\nthe estimated time of executing a linear function (lines 2 and 8 in\nAlgorithm 1) and loading a DILIâ€™s node from the main memory\nrespectively; ğœƒCis the estimated time of accessing the address of\nan internal node Nâ€™s child node. In particular, after calculating\nğ‘ğ‘œğ‘ =âŒŠN.LR(ğ‘¥)âŒ‹(line 2 in Algorithm 1), we need to get the ğ‘ğ‘œğ‘ -th\nelement from N.ğ‘ª, andğœƒCis the time of getting the corresponding\npointer. Usually, both ğœƒNandğœƒCequal the time of loading a cache\nline sized block from the main memory to the cache.\nAn exponential search needs about 2log2ğœ–ğ‘¥iterations. Each\niteration consists of the calculation of the middle position, an oper-\nation of pair addressing and a comparison of two keys. Thus, the\nestimated time of the local search in NDistE(ND,ğ‘¥)=2log2ğœ–ğ‘¥Ã—\n(ğœ‡E+ğœƒE), whereğœƒEandğœ‡Eare the average time of accessing a pair\nand executing the other operations in one iteration, respectively.\nAn exponential search fetches pairs mostly stored separately.\nDiscussion. In practice, pair access is much slower than other oper-\nations. Due to limited cache size, a new node or pair often triggers\na cache miss, which entails addressing in the heap. Addressing\ntakes two steps: finding the leaf node and local search inside the\nnode. However, less local search time in a leaf node often means the\nnode stores fewer pairs, which tends to increase the number of leaf\nnodes. All this means more and deeper leaf nodes, which in turn\nincurs more time cost for finding the correct leaf node. To strike atrade-off between DILIâ€™s leaf node depth and number of leaf nodes,\nwe proceed to design a bulk loading method to construct DILI by\ntaking into account the time cost of both steps together.\n4 CONSTRUCTION OF DILI\nFor a pair array ğ‘·sorted on all keys, we want to build DILI with a\ngood node layout having fast lookups for arbitrary search keys.\n4.1 Motivation and Overall Idea of BU-Tree\nAs described in Section 2, DILIâ€™s linear regression model in an\ninternal node NThas perfect accuracy because its children equally\ndivide its range by design. This design gives rise to a unique critical\nproblem in constructing DILI: deciding the suitable fanout for NT.\nOne idea is to follow the top-down construction of ALEX [ 18],\nusing a power of 2 for a internal nodeâ€™s fanout. If the whole key\nrange is[0,1), the length of a leaf nodeâ€™s range must be1\n2ğ‘˜for some\nintegerğ‘˜. For a complex key distribution ( e.g., a long-tail type), ğ‘˜\nmust be large in order to ensure high accuracy of the linear model\nin the leaf node. This tends to result in many leaf nodes and thus a\nhigh tree, making it slow to find the leaf node for a given key.\nAnother idea comes from the bottom-up bulk loading of B+Tree.\nFirst, we partition all pairs in ğ‘·into pieces and store each piece in a\nleaf node ND. In each ND, we build a linear model to map NDâ€™s keys\nto their positions in NDâ€™s piece. With an appropriate algorithm, we\ncan ensure a relatively low total loss of all linear models in all leaf\nnodes. Then, we partition the boundaries of the leaf nodes using\nthe same algorithm. We create internal nodes at height level 1 to\nsave the boundaries, use the boundaries as the separation values\nto group the leaf nodes, and make each group of leaf nodes the\nchildren of their corresponding parent node at level 1. Likewise, we\ncreate internal nodes at height level ğ‘–based on those at level ğ‘–âˆ’1,\nand repeat the process until we reach a level with only one node.\nThis approach reduces the local search time in the leaf nodes but\nit does not guarantee that child nodes equally divide their parent\nnodeâ€™s range, and thus the parent nodeâ€™s linear regression model\nfail to give perfect prediction accuracy. Rather, from the predicted\nposition, extra operations must be performed to find the child node\ncovering a given key, making the overall lookup time longer.\nTo build DILI that incurs low overall lookup time for arbitrary\nkeys, we combine both ideas in a two-phase bulk loading algorithm.\nFirst, we create a bottom-up tree (BU-Tree), starting from leaf nodes\nand growing the tree upwards. Second, we reuse the BU-Treeâ€™s\nnode layout to build DILI, and improve the latterâ€™s internal linear\nmodels such that they also obtain perfect prediction accuracy. The\nbuilt DILI is able to find the leaf node covering a search key with\nonly a few calculations of linear functions. Also, the local search\ntime cost in the leaf nodes is small as DILI has a leaf node layout\nsimilar to the BU-Tree. Thus, DILI is built in a novel paradigmâ€”we\nfirst design a mirror model that finds leaf nodes efficiently, and then\ncreate a similar model that further optimizes local search in a leaf\nnode. Fig. 2 illustrates the procedure of our bulk loading algorithm.\nWe use BU internal node orBU leaf node to refer to an internal or\nleaf node in the BU-tree. A BU internal node NTis structurally the\nsame as that in DILI, except that NTstores an additional array ğ‘©to\nrecord the ranges of all its child nodes. Specifically, NT.ğ‘ª[ğ‘–âˆ’1].ub=\nNT.ğ‘ª[ğ‘–].lb=NT.ğ‘©[ğ‘–]. Note that the child nodes may not equally\n\nDILI: A Distribution-Driven Learned Index (Extended version)\nBU leaf \nnodes BU internal \n nodes BU root BU internal \n nodes \nDILI leaf \nnodesDILI internal \nnodes DILI \nrootDILI internal \nnodes  g-m   Input: \n... ......\nDetermine the fanout of DILI \nnodes based on the BU nodes  \n(â„=1) \n(â„=ğ‘–âˆ’1) \n(â„=ğ‘–) \n(â„=1) \n(â„=ğ‘–)  \n(â„=ğ‘–âˆ’1) \nğ‘· ...g-m g-m g-m\ng-m: greedy merging\nFigure 2: Framework of the bulk loading algorithm\ndivide NTâ€™s range. All BU leaf nodes are at the same height level\nand they are reused as the basis of the leaf nodes in DILI.\nKey search in a BU-Tree is different from DILI. Finding the child\nnode covering key ğ‘¥in a BU internal node NTinvolves two steps.\nThe first step computes ğ‘—=NT.LR(ğ‘¥)and the second step searches\nNT.ğ‘©from position ğ‘—to find the index ğ‘–such that NT.ğ‘©[ğ‘–]â‰¤ğ‘¥<\nNT.ğ‘©[ğ‘–+1]. As a result, NT.C[ğ‘–]points to the correct child node.\n4.2 Building BU-Tree\nFor a given ğ‘·, BU-Tree is built by Algorithm 2. After initialization\n(line 1), it calls the function greedyMerging (Algorithm 3 to be\ndetailed in Section 4.2.2) to generate all leaf nodes (line 2). Subse-\nquently, Algorithm 2 creates all BU internal nodes (lines 3â€“11) in a\nbottom-up way, until an appropriate root node is found (lines 7â€“9).\nAt each height â„, we independently decide if the nodes at the cur-\nrent height should be the children of an immediate root node or not.\nFor both cases (lines 5 and 6), we calculate the average estimated\naccumulated search cost (to be detailed in Section 4.2.2). It is an\nestimate of the lookup time of the corresponding DILI from its root\nnode to the node at height â„that covers the search key. If having\nan immediate root node implies a smaller cost, we create a root\nnode and set its child nodes to be Nâ„, the BU nodes at height â„\n(lines 7â€“9). Otherwise, the BU-Tree grows to height â„+1(line 10).\nAlgorithm 2 BuildBUTree (ğ‘·)\n1:ğ‘â†|ğ‘·|,ğ‘¿â†[ğ‘¥0,Â·Â·Â·,ğ‘¥ğ‘âˆ’1]whereğ‘¥ğ‘–=ğ‘·[ğ‘–].ğ‘˜ğ‘’ğ‘¦\nâŠ²Generate BU leaf nodes\n2:ğ‘›0,ğ‘¿0,N0,ğœ€1â†greedyMerging (NULL, ğ‘¿)\n3:â„â†0\n4:whileğ‘›â„>1do\n5: Nğ‘Ÿ,ğœ€0â†generateRoot (Nâ„,ğ‘¿â„,ğ‘¿)\n6:ğ‘›â„+1,ğ‘¿â„+1,Nâ„+1,ğœ€1â†greedyMerging (Nâ„,ğ‘¿â„)\n7: ifğœ€0<ğœ€1then âŠ²Growing DILI will result in larger cost\n8: Set Nğ‘Ÿto be the root node of the BU-Tree\n9: break\n10: elseâ„â†â„+1\n11:return the root node\n12:function generateRoot (Nâ„âˆ’1,ğ‘¿â„âˆ’1,ğ‘¿)\n13: ğ’€â„âˆ’1â†ğ¼,ğ¼â† Ëœ[ğ‘›â„âˆ’1],ğ‘›â„âˆ’1â†|ğ‘¿â„âˆ’1|\n14:Fâ† leastSqares (ğ‘¿â„âˆ’1,ğ’€â„âˆ’1,ğ¼)\n15: Râ†an empty BU internal node\n16: R.LRâ†F ,R.foâ†ğ‘›â„âˆ’1,R.ğ‘ªâ†Nâ„âˆ’1\n17:ğœ€â†1\nğ‘Ãğ‘âˆ’1\nğ‘–=0TB\nns(N,ğ‘¥ğ‘–) âŠ²Calculate the search cost\n18: return R,ğœ€\n4.2.1 Bottom-up Node and Model Creation. Given the pair set ğ‘·, we\nhaveğ‘¿=keys(ğ‘·)=[ğ‘¥0,Â·Â·Â·,ğ‘¥ğ‘âˆ’1]andğ’€=Ëœ[ğ‘]=[0,Â·Â·Â·,ğ‘âˆ’1]\nwhereğ‘¥ğ‘–=ğ‘·[ğ‘–].ğ‘˜ğ‘’ğ‘¦. We first find a suitable integer ğ‘›0andğ‘›0âˆ’1\nbreak points[ğ›½0\n1,Â·Â·Â·,ğ›½0\nğ‘›0âˆ’1]to partition the key space keys(ğ‘·)into\nğ‘›0pieces. The ğ‘–th pieceâ€™s range is equal to [ğ›½0\nğ‘–,ğ›½0\nğ‘–+1)whereğ›½0\n0=\ninfkeys(ğ‘·)andğ›½0ğ‘›0=supkeys(ğ‘·). For theğ‘–th piece, supposing\nğ›½0\nğ‘–â‰¤ğ‘¥ğ‘™<Â·Â·Â·<ğ‘¥ğ‘Ÿ<ğ›½0\nğ‘–+1, we train a linear regression model F0\nğ‘–with input[ğ‘¥ğ‘™,Â·Â·Â·,ğ‘¥ğ‘Ÿ]and[ğ‘™,Â·Â·Â·,ğ‘Ÿ]. Then,ğ‘›0BU leaf nodes are\ncreated. The ğ‘–th node N0\nğ‘–is described as follows.\nN0\nğ‘–.lb=ğ›½0\nğ‘–,N0\nğ‘–.ub=ğ›½0\nğ‘–+1,N0\nğ‘–.LR(ğ‘¥)=F0\nğ‘–(ğ‘¥)âˆ’ğ‘™,N0\nğ‘–.ğ‘½=ğ‘·[ğ‘™:ğ‘Ÿ](3)\nSuppose that the BU nodes at height â„âˆ’1have been created.\nWe define two lists ğ‘¿â„âˆ’1=[Nâ„âˆ’1\n0.lbÂ·Â·Â·,Nâ„âˆ’1\nğ‘›â„âˆ’1âˆ’1.lb]andğ’€â„âˆ’1=\n[0,Â·Â·Â·,ğ‘›â„âˆ’1âˆ’1], where Nâ„âˆ’1\nğ‘–is theğ‘–th node at height â„âˆ’1and\nğ‘›â„âˆ’1is the number of the nodes at height â„âˆ’1. Similarly, we\ngenerateğ‘›â„âˆ’1break points[ğ›½â„\n1,Â·Â·Â·,ğ›½â„\nğ‘›â„âˆ’1], partition the space\nintoğ‘›â„pieces, and build ğ‘›â„linear regression models. Given a key ğ‘¥,\nsuppose Nâ„âˆ’1\nğ‘™.lbâ‰¤ğ‘¥<Nâ„âˆ’1\nğ‘™.ub. We define a function ğœâ„âˆ’1(ğ‘¥)=ğ‘™.\nTheğ‘–th node Nâ„\nğ‘–at heightâ„is described as follows.\nNâ„\nğ‘–.lb=ğ›½â„\nğ‘–,Nâ„\nğ‘–.ub=ğ›½â„\nğ‘–+1,Nâ„\nğ‘–.fo=ğ‘›â„, (4)\nNâ„\nğ‘–.LR(ğ‘¥)=Fâ„\nğ‘–(ğ‘¥)âˆ’ğœâ„âˆ’1(ğ‘¥),Nâ„\nğ‘–.C[ğ‘—]=Nâ„âˆ’1\nğ‘,\nNâ„\nğ‘–.ğ‘©[ğ‘—]=Nâ„âˆ’1\nğ‘.lb,whereğ‘=ğœâ„âˆ’1(ğ›½â„\nğ‘–)+ğ‘—\nA key challenge here is to decide ğ‘›â„(the number of nodes at\nheightâ„) and[ğ›½â„\n1,Â·Â·Â·,ğ›½â„\nğ‘›â„âˆ’1](the break points for these ğ‘›â„nodes),\nas they determine the node layout at height â„.\n4.2.2 Determining Node Layout at A Height. We want to have a\nsuitable BU node layout such that the corresponding DILI will\nhave a good node layout to minimize the average search time. As\nthe DILI has a similar node layout with the BU-Tree, we simulate\nthe DILIâ€™s querying process in the BU-Tree. To search for a key in\nthe BU-Tree, we observe which nodes are accessed as well as the\nlosses of their linear regression models. Based on those observations,\nwe estimate the cost of searching for a key in the DILI.\nGiven a key ğ‘¥, we define the estimated search cost TBns(N,ğ‘¥,â„)\nw.r.t. a BU node Nand a height â„as follows.\nTB\nns(N,ğ‘¥,â„)=ğœƒN+ğœ‚+ğœŒâ„Ã—tB\nE(N,ğ‘¥),whereğœŒâˆˆ(0,1)\ntB\nE(N,ğ‘¥)=log2|N.LR(ğ‘¥)âˆ’ğ‘–|Ã—(ğœ‡E+ğœƒE) (5)\nğ‘–=(\nN.ğ‘©[ğ‘–]â‰¤ğ‘¥<N.ğ‘©[ğ‘–+1], Nis a BU internal node\nN.ğ‘½[ğ‘–].ğ‘˜ğ‘’ğ‘¦â‰¤ğ‘¥<N.ğ‘½[ğ‘–+1].ğ‘˜ğ‘’ğ‘¦, otherwise.\nHere,ğœƒN,ğœ‚,ğœ‡EandğœƒEcarry the same meanings as those in Eq. 2;\ntB\nEis a simple extension of tEon BU nodes. If the node height is\nirrelevant, we simplify TBns(N,ğ‘¥,â„)toTBns(N,ğ‘¥).\nGiven a key ğ‘¥, suppose the search in BU-Tree visits the complete\nnode path Nğ‘˜,Nğ‘˜âˆ’1,Â·Â·Â·,N0, where Nğ‘˜is the root node and N0is\nthe leaf node covering ğ‘¥. We define the accumulated search cost till\nheightâ„TB\nal(â„,ğ‘¥)and the complete search cost TBs(ğ‘¥)as follows.\nTB\nal(â„,ğ‘¥)=ğ‘˜âˆ‘ï¸\nğ‘—=â„TB\nns(Nğ‘—,ğ‘¥),TB\ns(ğ‘¥)=TB\nal(0,ğ‘¥)=ğ‘˜âˆ‘ï¸\nğ‘–=0TB\nns(Nğ‘–,ğ‘¥)\nTo minimize the average lookup time of the search key, we try\nto minimize1\nğ‘Ãğ‘âˆ’1\nğ‘–=0TBs(ğ‘¥ğ‘–),i.e., the average complete search cost\nfor all keys. If the nodes under height â„have been created, minimiz-\ning1\nğ‘Ãğ‘âˆ’1\nğ‘–=0TBs(ğ‘¥ğ‘–)is equivalent to minimizing1\nğ‘Ãğ‘âˆ’1\nğ‘–=0TB\nal(â„,ğ‘¥ğ‘–).\nHowever, as a BU-Tree is grown upwards, we do not even know the\nheight of the BU-Tree when creating nodes at height â„, let alone\nestimating the search cost of a key in a node above height â„. To this\n\nPengfei Li, Hua Lu, Rong Zhu, Bolin Ding, Long Yang, and Gang Pan\n[0, 180)\n0 36 144\n0 9 27 3660 102 144 164 72[0, 36) [36, 144) [144, 180)\n[0, 9) [9, 27) [27, 36) [36, 60) [60, 72) [72, 102) [144, 164) [164, 180) [102, 144)[0, 180)\n[0, 60) [60, 120)[120, 180)\n[0, 15) [15, 30) [30, 45) [45, 60) [60, 80) [80, 100) [120, 150) [150, 180) [100, 120)[l, r)A BU leaf node \nwith range [l, r)[l, r) A BU internal node \nwith range [l, r)\nThe boundary array B\nof a BU internal node [l, r) A DILI internal node \nwith range [l, r)[l, r)A DILI leaf node \nwith range [l, r)\nğ–­ğ–¡ \nğ–­ğ–£ \nFigure 3: Building DILI based on the BU-Tree\nend, we introduce the estimated accumulated search cost of the break\npoints list ğ‘¿â„for the key set ğ‘¿, termed as TBea(ğ‘¿â„,ğ‘¿). It measures\nthe quality of the node layout generated from the break points list\nğ‘¿â„. For simplicity, we assume that each BU internal node has the\nsame number of child nodes, and define TBea(ğ‘¿â„,ğ‘¿)as follows.\nTB\nea(ğ‘¿â„,ğ‘¿)=1\nğ‘ğ‘âˆ’1âˆ‘ï¸\nğ‘–=0âŒˆğ›¿âŒ‰âˆ‘ï¸\nâ„â€²=â„min(1,ğ›¿+1âˆ’â„â€²)Ã—TB\nns(Nâ„\nğ‘¡ğ‘–,ğ‘¥ğ‘–,â„â€²)\nwhereğ‘¡ğ‘–=ğœâ„(ğ‘¥ğ‘–),ğ›¿=logğ‘›â„âˆ’1ğ‘›â„ğ‘›â„âˆ’1,ğ‘›âˆ’1=ğ‘\nAbove, Nâ„\nğ‘¡ğ‘–is the node at height â„whose range covers ğ‘¥ğ‘–. In other\nwords, ğ‘¿â„[ğ‘¡ğ‘–]â‰¤ğ‘¥<ğ‘¿â„[ğ‘¡ğ‘–+1]. Moreover,ğ›¿is the estimated depth\nof the nodes at height â„. We explain ğ›¿by an example. Suppose\nğ‘›â„âˆ’1=1000 andğ‘›â„=100,i.e., the number of nodes at height â„âˆ’1\nandâ„are 1000 and 100, respectively. A node at height â„has1000\n100=\n10 child nodes on average. The estimated ğ‘›â„+1=100\n10=10,ğ‘›â„+2=1\nand thus the root nodeâ€™s height is â„+2. Becauseğ›¿is the estimated\ndepth of the nodes at height â„,ğ›¿=(â„+2)âˆ’â„+1= 3 = log 1000\n1001000.\nAsğ›¿may not exactly be an integer, we add a multiplication factor\nğ›¿+1âˆ’â„â€²=ğ›¿âˆ’âŒŠğ›¿âŒ‹before TBns(Nâ„\nğ‘¡ğ‘–,ğ‘¥ğ‘–,â„â€²)whenâ„â€²=âŒˆğ›¿âŒ‰. Here,\nTBns(Nâ„\nğ‘¡ğ‘–,ğ‘¥ğ‘–,â„â€²)is an estimate of TBns(Nâ„â€²\nğ‘¡ğ‘–,ğ‘¥ğ‘–).\nGiven ğ‘¿â„âˆ’1andğ’€â„âˆ’1, to find the best ğ‘›â„andğ‘¿â„, a straightfor-\nward approach is to set ğ‘›â„to be different values. For each specific\nğ‘›â„, we solve a ğ‘›â„-piecewise linear regression problem with input\nğ‘¿â„âˆ’1andğ’€â„âˆ’1, and compute the estimated accumulated search cost\nof this configuration. Then, we choose the configuration with the\nsmallest accumulated search cost. However, it is costly to directly\nsolve a number of ğ‘˜-piecewise linear regression problems.\nInstead, we adapt an efficient greedy merging algorithm [ 11] to\niteratively solve a series of ğ‘˜-piecewise linear regression problems\nwith input ğ‘¿â„âˆ’1andğ’€â„âˆ’1. At each iteration, we generate ğ‘˜âˆ’1\nbreak points and calculate the estimated accumulated search cost\nfor them. Those break points induce the smallest cost form ğ‘¿â„,\nthe basis for creating the nodes at height â„. Algorithm 3 finds the\nsuitableğ‘›â„andğ‘¿â„and generates the nodes at height â„. In lines 6â€“7,\nğœ”is a pre-defined average maximum fanout for DILIâ€™s nodes. In\nour implementation, we set ğœ”to 4096 as a DILI with good search\nperformance cannot have too many nodes.\nThe value of ğ‘˜is decided as follows. Initially, ğ‘˜is set to beğ‘›â„âˆ’1\n2\nand the list ğ‘¿â„âˆ’1is partitioned into ğ‘˜pieces (lines 1-2). At each\niteration (lines 7â€“15), we merge two continuous pieces that result in\nthe most linear loss increase and decrease the value of ğ‘˜by 1 (lines 8-\n12). We do not need to calculate the linear loss w.r.t. every piece.\nInstead, we maintain two number ğ‘ ğ‘˜\nğ‘–andğ‘šğ‘˜\nğ‘–for theğ‘–th pieceğ¼ğ‘˜\nğ‘–.\nThey equal the linear loss w.r.t.ğ¼ğ‘˜\nğ‘–andğ¼ğ‘˜\nğ‘–Ãğ¼ğ‘˜\nğ‘–+1, respectively. Each\niteration involves the change of one piece only. Thus, after decidingAlgorithm 3 greedyMerging (Nâ„âˆ’1,ğ‘¿â„âˆ’1)\n1:ğ‘˜â†ğ‘›â„âˆ’1\n2,ğ’€â„âˆ’1â† Ëœ[ğ‘›â„âˆ’1],ğ‘›â„âˆ’1â†|ğ‘¿â„âˆ’1|\nâŠ²The last set may contain 3 elements\n2:Iğ‘˜â†{{ 0,1},{2,3},Â·Â·Â·,{2ğ‘˜âˆ’2,(2ğ‘˜âˆ’1),ğ‘›â„âˆ’1âˆ’1}}\n3:âˆ€ğ‘˜and0â‰¤ğ‘–<ğ‘˜, letğ¼ğ‘˜\nğ‘–denote theğ‘–th element ofIğ‘˜\n4: For any indices set ğ¼, letğ›¾(ğ¼)â† rmse(ğ‘¿â„âˆ’1,ğ’€â„âˆ’1,ğ¼)\n5:âˆ€ğ‘˜,ğ‘–,letğ‘ ğ‘˜\nğ‘–â†ğ›¾(ğ¼ğ‘˜\nğ‘–),ğ‘šğ‘˜\nğ‘–â†ğ›¾(ğ¼ğ‘˜\nğ‘–Ãğ¼ğ‘˜\nğ‘–+1)\n6: Setğœ”to be a large number and ğ‘˜ğ‘šğ‘–ğ‘›â†ğ‘›â„\nğœ”âŠ²In practice, we set ğœ”=2,048.\n7:whileğ‘˜â‰¥ğ‘˜ğ‘šğ‘–ğ‘›do âŠ²Iterative greedy merging\nâŠ²ğ‘šğ‘˜\nğ‘–=ğ›¾(ğ¼ğ‘˜\nğ‘—Ãğ¼ğ‘˜\nğ‘—+1),ğ‘ ğ‘˜\nğ‘–=ğ›¾(ğ¼ğ‘˜\nğ‘—),ğ‘ ğ‘˜\nğ‘–+1=ğ›¾(ğ¼ğ‘˜\nğ‘—+1)\n8:ğ‘¢=argminğ‘–ğ‘šğ‘˜\nğ‘–âˆ’ğ‘ ğ‘˜\nğ‘–âˆ’ğ‘ ğ‘˜\nğ‘–+1\n9:Iğ‘˜âˆ’1â†{ğ¼ğ‘˜\n0,ğ¼ğ‘˜\n1,Â·Â·Â·,ğ¼ğ‘˜\nğ‘¢Ãğ¼ğ‘˜\nğ‘¢+1,ğ¼ğ‘˜\nğ‘¢+2,Â·Â·Â·,ğ¼ğ‘˜\nğ‘˜âˆ’1}\n10:ğ‘ ğ‘˜âˆ’1\nğ‘¢â†ğ‘šğ‘˜\nğ‘¢and calculate ğ‘šğ‘˜âˆ’1\nğ‘¢âˆ’1andğ‘šğ‘˜âˆ’1\nğ‘¢\n11:âˆ€ğ‘¢<ğ‘–<ğ‘˜âˆ’1,ğ‘ ğ‘˜âˆ’1\nğ‘–â†ğ‘ ğ‘˜\nğ‘–+1,ğ‘šğ‘˜âˆ’1\nğ‘–â†ğ‘šğ‘˜\nğ‘–+1\n12:ğ‘˜â†ğ‘˜âˆ’1\nâŠ²Generate new break points\n13:âˆ€0â‰¤ğ‘–<ğ‘˜,ğ‘ğ‘–â†infğ¼ğ‘˜\nğ‘–\n14: ğ‘©ğ‘˜=[ğ‘¿â„âˆ’1[ğ‘0],ğ‘¿â„âˆ’1[ğ‘1],Â·Â·Â·,ğ‘¿â„âˆ’1[ğ‘ğ‘˜âˆ’1]]\n15:ğœ€ğ‘˜â†TB\nea(ğ‘©ğ‘˜,ğ‘¿) âŠ²Getğ‘©ğ‘˜â€™s estimated accumulated search cost\n16:ğ‘›â„â†argminğ‘˜ğœ€ğ‘˜\n17:ğ‘¿â„â†ğ‘©ğ‘›â„\n18:âˆ€0â‰¤ğ‘–<ğ‘›â„,Fâ„\nğ‘–â†leastSqares (ğ‘¿â„âˆ’1,ğ’€â„âˆ’1,ğ¼ğ‘›â„\nğ‘–)\n19:Nâ„â†the BU nodes at height â„described in Eq. 3 or Eq. 4\n20:returnğ‘›â„,ğ‘¿â„,Nâ„,ğœ€\nmergingğ¼ğ‘˜ğ‘¢andğ¼ğ‘˜\nğ‘¢+1, we only need to update the values of ğ‘ ğ‘˜âˆ’1ğ‘¢and\nğ‘šğ‘˜âˆ’1ğ‘¢w.r.t.ğ¼ğ‘˜âˆ’1ğ‘¢(i.e.,ğ¼ğ‘˜ğ‘¢Ãğ¼ğ‘˜\nğ‘¢+1) andğ‘šğ‘˜âˆ’1\nğ‘¢âˆ’1w.r.t.ğ¼ğ‘˜âˆ’1\nğ‘¢âˆ’1. Clearly,ğ‘ ğ‘˜âˆ’1ğ‘¢=\nğ‘šğ‘˜ğ‘¢and thus only two calculations of ğ‘šğ‘˜âˆ’1\nğ‘¢âˆ’1andğ‘šğ‘˜âˆ’1ğ‘¢are needed.\nMeanwhile, the break points of the ğ‘˜-piecewise linear function\nğ‘©ğ‘˜are generated (lines 13-14). We compute TBea(ğ‘©ğ‘˜,ğ‘¿)for each\nğ‘˜(line 13). After the iterations, we set ğ‘¿â„toğ‘©ğ‘›â„andğ‘›â„to theğ‘˜\nwith the smallest TBea(ğ‘©ğ‘˜,ğ‘¿)(lines 16-17). The nodes at height â„\nare then created (lines 18â€“19) according to Eq. 3 and Eq. 4.\nAt each iteration, we calculate ğ‘šğ‘˜âˆ’1\nğ‘¢âˆ’1andğ‘šğ‘˜âˆ’1ğ‘¢to estimate the\nlinear losses w.r.t. two pieces. In implementation, we make the\nnumber of items in each piece smaller than a pre-defined threshold\nof2ğœ”. Thus, both calculations run in time ğ‘‚(1). We use a priority\nqueue to store ğ‘‘ğ‘˜ğ‘¢=ğ‘šğ‘˜\nğ‘–âˆ’ğ‘ ğ‘˜\nğ‘–âˆ’ğ‘ ğ‘˜\nğ‘–+1for allğ‘–. Thus, the time complexity\nof selecting ğ‘¢(line 9) isğ‘‚(ğ‘˜)=ğ‘‚(ğ‘›â„). Besides, the calculation of\nthe estimated accumulated search cost of ğ‘©ğ‘˜runs in time ğ‘‚(1). In\nsummary, the time complexity of Algorithm 3 is ğ‘‚(ğ‘›â„log2ğ‘›â„).\n4.3 BU-Tree based Bulk Loading for DILI\nAlgorithm 4 formalizes bulk loading for DILI. In line 1, the BU-Tree\nis created by buildBUTree (Algorithm 2). Let ğ»be the BU-Treeâ€™s\nheight. At any height â„â‰¤ğ», DILI and the BU-Tree have the same\nnumber of nodes, but the node layouts may be different. Based\non the BU-Tree, we grow DILI top down (lines 3â€“7). The range\n\nDILI: A Distribution-Driven Learned Index (Extended version)\nof DILIâ€™s root node Root is set to the counterpart in the BU-Tree.\nRoot is created by the recursive function createInternal (line 7).\nAlgorithm 4 BulkLoading (ğ‘·)\n1:BURootâ†buildBUTree (ğ‘·)\n2:ğ»â†the height of BURoot\n3: Get N0,N1,Â·Â·Â·,Nğ»âˆ’1from Nâ€²\n4:forğ‘–âˆˆ{0,1,Â·Â·Â·,ğ»âˆ’1}do\n5: ğœ½ğ‘–=[Nğ‘–[0].lb,Nğ‘–[1].lb,Â·Â·Â·,Nğ‘–[|Nğ‘–|âˆ’1].lb]\n6:ğš¯â†[ğ‘·,ğœ½0,ğœ½1,Â·Â·Â·,ğœ½ğ»âˆ’1]\n7:Rootâ†createInternal (BURoot.lb,BURoot.ub,ğ»,ğš¯)\n8:return Root\n9:function createInternal (lb,ub,â„,ğš¯)\n10: NTâ†an empty DILI internal node\n11: NT.lbâ†lb,NT.ubâ†ub\n12: NT.foâ†#{ğ‘¥|ğ‘¥âˆˆğœ½,lbâ‰¤ğ‘¥<ub},ğœ½â†ğš¯[â„âˆ’1]\n13: NT.LR(ğ‘¥)=ğ‘+ğ‘ğ‘¥,ğ‘â†âˆ’ğ‘Ã—lb,ğ‘â†NT.fo\nubâˆ’lb\n14: forğ‘–âˆˆ{0,1,Â·Â·Â·,NT.foâˆ’1}do\n15:ğ‘™â†lb+ğ‘–\nğ‘,ğ‘¢â†lb+ğ‘–+1\nğ‘âŠ²The lower/upper bound\n16: ifâ„is 1then âŠ²Child nodes are leaf nodes\n17: NT.ğ‘ª(ğ‘–)=createLeafNode (ğ‘™,ğ‘¢,ğš¯[0]) âŠ²ğš¯[0]isğ‘·\n18: else NT.ğ‘ª(ğ‘–)=createInternal (ğ‘™,ğ‘¢,â„âˆ’1,ğš¯)\n19: return NT\n20:function createLeafNode (lb,ub,ğ‘·)\n21:ğ‘™â†argminğ‘–ğ‘·[ğ‘–].ğ‘˜ğ‘’ğ‘¦â‰¥lb,ğ‘¢â†argminğ‘–ğ‘·[ğ‘–].ğ‘˜ğ‘’ğ‘¦â‰¥ub\n22:ğ‘€â†|ğ‘·D|,ğ‘·Dâ†ğ‘·[ğ‘™:ğ‘¢]\n23: NDâ†an empty DILI leaf node, ND.Î©â†ğ‘€\n24: ND.LRâ† leastSqares (keys (ğ‘·D),Ëœ[ğ‘€])\n25: LocalOpt (ND,ğ‘·D)\n26: return ND\nTo create an internal node NT(lines 9â€“19), we set its range\naccording to the input bounds (line 11), its fanout to the number\nof BU nodes at height â„âˆ’1whose range is covered by NTâ€™s range\n(line 12), and its linear regression model accordingly (line 13). We\nrecursively create NT.fonodes and make them equally divide NTâ€™s\nrange (lines 14â€“18). These nodes compose NT.ğ‘ª.\nWhen creating a leaf node ND(lines 20â€“26), we include in ğ‘·Dthe\npairs with keys from NDâ€™s range (lines 21â€“22). The model ND.LRis\ntrained with the input keys (ğ‘·D) (lines 24). The function LocalOpt\ndistributes the pairs to the entry array ND.ğ‘½(line 25), performing\na local optimization on ND. The details will be given in Section 5.\nFig. 3 exemplifies building DILI. The ğ‘–th internal nodes of BU-\nTree and DILI at height â„may have different fanouts. For exam-\nple, whenâ„=1, node NB\nT(in the BU-Tree) has 3 child nodes but\nthe DILI node ND\nTâ€™s fanout is 4, because ND\nTâ€™s range[0,60)covers\nthe left boundaries of the first four BU leaf nodesâ€™ ranges.\n4.4 Remarks\nIn Section 4.2.2, TBns(Ni,ğ‘¥)estimates Tis(ğ‘¥)orTds(Ni,ğ‘¥)in Eq. 2,\ndepending on if Niis internal or not. If Niis a BU leaf node ( i.e.,\nâ„=0),TBns(Ni,ğ‘¥)andTds(Ni,ğ‘¥)are the same as long as Ni.ifLis\ntrue. Otherwise, TBns(Ni,ğ‘¥)is not the same as Tis(ğ‘¥). Our DILI bulk\nloading (Algorithm 4) makes the leaf node layouts of the BU-Tree\nand DILI as alike as possible. However, lower accuracy of BU inter-\nnal nodesâ€™ linear regression models would cause the two layouts\nto be more different. To this end, we modify TBns(Ni,ğ‘¥)to strike\na trade-off between the BU-Treeâ€™s height and the accuracy of its\nmodels. In addition, internal nodes at a higher height tend to have\nless impact on the layout of DILIâ€™s leaf nodes. Thus, we multiply\ntB\nE(Ni,ğ‘¥)by a factorğœŒâ„in Eq. 5 to reflect this effect.Our cost models of BU-Tree and DILI consider the effect of the\ncache locality and attempt to strike a good balance between the\nnode fanouts and the tree height. In contrast, the B+Tree restricts\nits node fanout to a pre-defined range [ğ‘š\n2,ğ‘š]. Thus, to find the\nnext child node to visit in an internal nodeâ€™s child array, the B+Tree\nbinary search needs to access the array âŒˆğ‘™ğ‘œğ‘”2ğ‘šâŒ‰times in the worst\ncase. Asğ‘šis relatively large, the local search inside a B+Tree node\noften triggers many cache misses. In contrast, finding the required\nchild node in DILI triggers only one cache miss. Also, compared to\nB+Tree, DILIâ€™s internal nodes have larger average fanouts. Thus,\nit usually has a wider and shallower structure such that a search\nneeds to traverse fewer nodes to locate the relevant leaf nodes.\nNote that ALEX partitions the key space in a relatively static\nway as its node fanout is always a power of 2. This causes its linear\nmodels to have relatively low accuracy. To this end, ALEX adopts\na gapped array to increase the model accuracy in its leaf nodes.\nStill, this cannot guarantee optimal model accuracyâ€”ALEX needs\nmore time and triggers more cache misses than DILI, as shown in\nSection 7. Unlike ALEX, our local optimization (to be detailed in\nSection 5) makes the models in DILIâ€™s leaf nodes perfectly accu-\nrate and thus shortens DILIâ€™s search time. Nevertheless, DILI still\noutperforms ALEX even without the local optimization.\n5 LOCAL OPTIMIZATION OF DILI\nA leaf node NDâ€™s linear model ND.LRapproximates the relation\nbetween keys and their positions in the array ND.ğ‘½. However, no\nmodel can always make perfect predictions. Our experiments imply\nthat the exponential search in the leaf nodes often forms a bottle-\nneck. Also, the leaf node structure does not consider insertions.\nOn average, half pairs covered by a leaf node needs to be shifted\nfor a single insertion. To address these issues, we propose a local\noptimization for DILIâ€™s leaf nodes. It helps the linear models make\n100% accurate predictions and avoid element shifting in insertions.\nAlgorithm 5 LocalOpt (ND,ğ‘·D)\n1:ğ‘â†|ğ‘·D|,ğ‘¿â†[ğ‘¥0,Â·Â·Â·,ğ‘¥ğ‘âˆ’1]whereğ‘¥ğ‘–=ğ‘·D[ğ‘–].ğ‘˜ğ‘’ğ‘¦\n2:ND.Î”â†0,ND.foâ†ğœ‚ND.Î©(ğœ‚>1)\n3:foâ†ND.foand allocate ND.ğ‘½with foNULLs\n4: Defineğ‘“D(ğ‘¥)â‰œmin(foâˆ’1,max(0,ND.LR(ğ‘¥))\n5:âˆ€ğ‘–=0toğ‘âˆ’1, setğ‘·ğ‘–\nDan empty list\n6:forğ‘–âˆˆ{0,1,Â·Â·Â·,ğ‘âˆ’1}do\n7:ğ‘¡â†ğ‘“D(ğ‘¥ğ‘–),ğ‘·ğ‘¡\nD.append( ğ‘·D[ğ‘–])\n8:forğ‘¡âˆˆ{0,1,Â·Â·Â·,foâˆ’1}do\n9: if|ğ‘·ğ‘¡\nD|=1then\n10: ND.ğ‘½[ğ‘¡]â†ğ‘·ğ‘¡\nD[0],ND.Î”+= 1\n11: else if |ğ‘·ğ‘¡\nD|>1then\n12: Create a new leaf node Nâ€²and train Nâ€².LR with the input ğ‘·ğ‘¡\nD\n13: LocalOpt (ğ‘·ğ‘¡\nD,Nâ€²)\n14: ND.ğ‘½[ğ‘¡]â† the pointer to Nâ€²,ND.Î”+= |ğ‘·ğ‘¡\nD| +Nâ€².Î”\n15: else ND.ğ‘½[ğ‘¡]â† NULL\n16:ND.ğœ…=ND.Î”\nND.Î©\nOur local optimization (Algorithm 5) starts at the final step in\nthe procedure of creating a leaf node ND(line 25 in Algorithm 4).\nSuppose there is one and only one pair whose predicted position\nbyND.LRisğ‘¡, we set ND.ğ‘½[ğ‘¡]to be this pair (lines 6-7, 9-10). If\nmultiple pairs conflict at position ğ‘¡, we create a new leaf node Nâ€²\nto cover them (lines 11-12). The LocalOpt function is recursively\n\nPengfei Li, Hua Lu, Rong Zhu, Bolin Ding, Long Yang, and Gang Pan\ncalled such that the entry array Nâ€².ğ‘½is created to organize the\nconflicting pairs (line 13). After that, the pointer to Nâ€²is assigned\ntoND.ğ‘½[ğ‘¡]. Those slots of ND.ğ‘½without a pair are set to NULL\n(lines 15). In practice, we set ND.fotoğœ‚Â·ND.Î©, whereğœ‚is an\nenlarging ratio, such that continuous keys are more likely assigned\nin different slots (line 2 ) and conflicts are reduced. Fig. 4 illustrates\nthe structure of a typical leaf node.\npair\nleaf node pointerNULL\nFigure 4: Leaf node structure\nThe local optimization renders search via DILI slightly different\nfrom Algorithm 1, as shown in Algorithm 6. After finding the high-\nest leaf node NDcoveringğ‘¥(line 1), the loop starts from obtaining\nthe value of the returned element ğ‘inND.ğ‘½through ND.LR. Ifğ‘\npoints to another leaf node Nâ€², the loop continues by setting NDto\nNâ€²(lines 5-6). Otherwise, the loop ends and returns the right result,\ndepending on if ğ‘is a pair with key equal to ğ‘¥(lines 7-9).\nAlgorithm 6 SearchWOpt (Root,ğ‘¥)\n1:NDâ†locateLeafNode (Root,ğ‘¥)\n2:while True do\n3:ğ‘ğ‘œğ‘ â†âŒŠND.LR(ğ‘¥)âŒ‹\n4:ğ‘â†ND.ğ‘½[ğ‘ğ‘œğ‘ ]\n5: ifğ‘points to a leaf node Nâ€²then\n6: NDâ†Nâ€²\n7: else ifğ‘â‰ NULL andğ‘.ğ‘˜ğ‘’ğ‘¦ ==ğ‘¥then\n8: returnğ‘.ğ‘˜ğ‘’ğ‘¦\n9: else return NULL\nFig. 1 gives an example of search via locally optimized DILI for\na keyğ‘¥=101. The root node Râ€™s key range is[0,240).Rhas three\nchild nodes. It is easy to derive that R.ğ‘=0,R.ğ‘=1\n80such that\nRâ€²ğ‘ linear model equally divides its key range into three parts. By\na simple calculation âŒŠR.ğ‘+R.ğ‘Ã—ğ‘¥âŒ‹=1, we knowğ‘¥is covered\nbyRâ€™s second child node, namely NTin Fig. 1 (Step-1). NTequally\ndivides its range[80,160)to its four children with NT.ğ‘=âˆ’4\nand NT.ğ‘=0.05. The search goes into NTâ€™s second child node\nN1\nDwhose range is[100,120)(Step-2). N1\nDis a leaf node whose\nlinear model is trained with the keys covered by its range, in a\ndifferent way from that of RandNT. At the predicted slot position\nâŒŠN1\nD.ğ‘+N1\nD.ğ‘Ã—ğ‘¥âŒ‹=âŒŠâˆ’9+0.1Ã—101âŒ‹=1is another leaf node N2\nD\n(Step-3). Note that two keys 101and102conflict at the same slot in\nN1\nD.ğ‘½. Thus, N2\nDis generated to store them as the local optimization.\nFinally, N2\nD.LRpredicts for ğ‘¥a pairğ‘at position 0 in N2\nD.ğ‘½. The\noutput pointer ğ‘ğ‘¥=ğ‘.ğ‘ğ‘¡ğ‘Ÿ points to the data record identified by ğ‘¥.\n6 DATA UPDATES IN DILI\n6.1 Insertions\nInsertions via DILI are logically simple and efficient. Our inser-\ntion algorithm avoids element shifting that happen to B+Tree andALEX, and it redistributes pairs when insertions degrade the search\nperformance. The details are shown in Algorithm 7.\nTo insert a pair ğ‘to DILI, the first step calls the function locate-\nLeafNode (defined in Algorithm 1) to find the leaf node NDthat\nsupposedly covers ğ‘.ğ‘˜ğ‘’ğ‘¦ (line 1). Next, the algorithm inserts ğ‘into\nNDby calling the recursive function insertToLeafNode (line 2).\nWe use the model ND.LRto calculate the position ğ‘ğ‘œğ‘ inND.ğ‘½for\nğ‘. Suppose that at position ğ‘ğ‘œğ‘ is the element ğ‘â€²(line 4). Ifğ‘â€²is\nNULL, theğ‘ğ‘œğ‘ -th slot of ND.LRis empty. We simply place ğ‘at the\nslot. Then, the cost of searching for all pairs except for ğ‘covered\nbyNDdoes not change. Searching for ğ‘from NDneeds only one\nextra entry access, so we simply add one to ND.Î”(lines 6-7). Here,\nND.Î”denotes the total number of entries to be accessed to search\nfor all keys covered by ND, starting from ND. Ifğ‘â€²points to another\nleaf node Nâ€², we insertğ‘into Nâ€²â€™s entry array (line 10). This time\nthe change of ND.Î”is related to those pairs in Nâ€². Thus, we record\nNâ€².Î”before the insertion (line 9). The increment of ND.Î”is the\nchange of Nâ€².Î”plus 1 (line 11). If ğ‘exists (line 12), we do nothing\n(line 13). If a conflict happens (line 14), we need to replace the pair\nğ‘â€²with a new leaf node covering ğ‘andğ‘â€²at theğ‘ğ‘œğ‘ -th position of\nND.ğ‘½(lines 15-17). In this case, ND.Î”is increased by three (line 18):\none for searching for ğ‘â€²and two for ğ‘.\nAlgorithm 7 Insert (Root,ğ‘)\n1:NDâ†locateLeafNode (Root,ğ‘.ğ‘˜ğ‘’ğ‘¦ )\n2:return insertToLeafNode (ND,ğ‘)\n3:function insertToLeafNode (ND,ğ‘)\n4:ğ‘â€²â†ND.ğ‘½[ğ‘ğ‘œğ‘ ],ğ‘ğ‘œğ‘ â†ND.LR(ğ‘.ğ‘˜ğ‘’ğ‘¦ )\n5: notExistâ†True\n6: ifğ‘â€²=NULL then\n7: ND.ğ‘½[ğ‘ğ‘œğ‘ ]â†ğ‘,ND.Î”+= 1 âŠ²insertğ‘to an empty slot\n8: else ifğ‘â€²points to another leaf node Nâ€²then\n9: Î”â€²â†Nâ€².Î”\n10: notExistâ†insertToLeafNode (Nâ€²,ğ‘)\n11: ND.Î”+=1+Nâ€².Î”âˆ’Î”â€²\n12: else ifğ‘â€².ğ‘˜ğ‘’ğ‘¦ =ğ‘.ğ‘˜ğ‘’ğ‘¦ then\n13: notExistâ†True âŠ²ğ‘exists\n14: else\n15: create a new leaf node Nâ€²to coverğ‘andğ‘â€²\n16: Nâ€².Î”â†2,Nâ€².Î©â†2and train Nâ€².LR\n17: ND.ğ‘½[ğ‘ğ‘œğ‘ ]â† the pointer to Nâ€²\n18: ND.Î”+=1+Nâ€².Î”\n19: ND.Î©+= (notExist =True ? 1 : 0)\n20: ifnotExist =True andND.Î”\nND.Î©>ğœ†Ã—ND.ğœ…then\n21: collect all pairs covered by NDand store them in list ğ‘·D\n22: ND.foâ†ND.Î©Ã—ğ‘Ÿ,ğ‘Ÿâ†ğœ‘(ND.ğ›¼),ND.ğ›¼+= 1\n23: ND.LRâ† leastSqares (keys (ğ‘·D), Ëœ[ND.Î©])\n24: ND.LR.ğ‘â†ND.LR.ğ‘Ã—ğ‘Ÿ,ND.LR.ğ‘â†ND.LR.ğ‘Ã—ğ‘Ÿ\n25: LocalOpt (ND,ğ‘·D)\n26: ND.ğœ…=ND.Î”\nND.Î©\n27: return notExist\nHowever, many new nodes for conflicting keys may increase the\ndepth of leaf nodes wildly. Thus, it is necessary to adjust the layout\nof some leaf nodes when the search performance degrades. We\nobserve the relationship among ND.Î”,ND.Î©andND.ğœ…, and uses\na flexible strategy to decide if a leaf node NDshould be adjusted.\nWhen inserting a pair ğ‘toND, from this node, if the average number\nof entries need to be accessed to search for a pair ( i.e.,ND.Î”\nND.Î©) is\nlarger than a pre-defined threshold (line 20), we think the insertions\ndegrade the search performance. Thus, NDis adjusted, which starts\nby collecting all pairs covered by ND(line 21). Then, the capacity\n\nDILI: A Distribution-Driven Learned Index (Extended version)\nofND.ğ‘½is enlarged and we train NDâ€™s linear model accordingly\nsuch that conflicts will happen more rarely (lines 22-24). Finally,\nwe redistribute the pairs with the local optimization (line 25).\nThe pre-defined threshold is set to ğœ†Â·ND.ğœ…(ğœ†>1) where ND.ğœ…\nis the average number of accessed entries in search for a pair covered\nbyNDafter executing the last local optimization at ND. In our\nexperiments, ğœ†is set to 2. As a result, if the average cost per search\nw.r.t. NDdoubles after a series of insertions, we deem some nodes\nunder NDbecome too deep and the performance of searching for\nrelevant keys degrades dramatically. In this case, it is better to\ncollect all pairs that NDcovers, retrain ND.LRand redistribute\nthose pairs. Finally, the value of ND.ğœ…will be updated (line 26).\nÎ©=5 \nfo=5 \nğœ…=1 \nÎ©=11 \nfo=5 \nğœ…=13\n11 \nÎ”=5 \nÎ”=23 \nğœ…=1 \nÎ©=11 \nfo=12 \nÎ”=13 Insert 6 pairs\nÎ”\nÎ©>ğœ†â‹…ğœ… Adjust\nfo=1.1Î© \nğœ†=2 \nFigure 5: Adjusting a leaf node after insertions\nWhen adjusting the leaf node ND, we also set ND.foto be larger\nthan ND.Î©. The gap between them grows with more adjustments\n(line 22). A simple yet reasonable assumption is that the more\nadjustments, the more frequently relevant pairs are accessed. Also,\nmore adjustments usually mean more conflicts. Thus, to reduce\nthe number of conflicts at ND, we enlarge the capacity of ND.ğ‘½,\nmaking more slots for pairs. In our experiments, the enlarging\nratioğœ‘(ND.ğ›¼)â‰œmin(ğœ‚+0.1Ã—ND.ğ›¼, 4), whereğœ‚carries the same\nmeaning with that in Algorithm 5. ğœ‘(Â·)can be any monotonically\nincreasing function and its derivative should consider the memory\nusage. Our strategy of having redundancy in frequently adjusted\nnodes is similar to but more flexible than the usage of gapped array\nin ALEX [18]. Fig. 5 gives an example of adjusting a leaf node.\n6.2 Deletions\nTo delete a pair with the key ğ‘¥, we call locateLeafNode to find\nthe highest leaf node NDcoveringğ‘¥(line 1). Next, we use delete-\nFromLeafNode to delete the pair from ND(line 2). It recursively\nchecks if there is a pair, covered by NDor a leaf node underneath,\nhaving the key ğ‘¥(lines 4-14). If the pair is found, we remove it\nby setting the corresponding slot in the leaf nodeâ€™s entry array to\nNULL (lines 5-6). Otherwise, deleteFromLeafNode simply returns\nFalse (lines 7-8). After the removal, the values of NDâ€™s some fields\nwill change, like the number of pairs contained in NDand the aver-\nage cost of searching keys from ND. Thus, we update the values of\nND.Î”,ND.Î©andND.ğœ…(lines 6, 10-12, 16). If NDâ€™s child leaf node\nNâ€²contains only one pair ğ‘â€²â€²after the removal, we simply delete\nNâ€²and replace the pointer to it with ğ‘â€²â€²inND.ğ‘½(lines 13-15).\n7 EXPERIMENTAL STUDIES\nDILI and all competitors are implemented in C++ [ 1] and evaluated\nusing a single thread on a Ubuntu server with a 96-core Xeon(R)\nPlatinum 8163 CPU and 376 GB memory. Due to space limit, we\npresent more experimental results and analyses in an extended\nversion [32].Algorithm 8 Delete (Root,ğ‘¥) âŠ²ğ‘¥is the key to be deleted\n1:NDâ†locateLeafNode (Root,ğ‘¥)\n2:return deleteFromLeafNode (ND,ğ‘¥)\n3:function deleteFromLeafNode (ND,ğ‘¥)\n4:ğ‘â€²â†ND.ğ‘½[ğ‘ğ‘œğ‘ ],ğ‘ğ‘œğ‘ â†ğ‘“D(ğ‘¥),existâ†True\n5: ifğ‘â€².ğ‘˜ğ‘’ğ‘¦=ğ‘¥then\n6: ND.ğ‘½[ğ‘ğ‘œğ‘ ]â† NULL, ND.Î”-= 1 âŠ²deleteğ‘â€²from ND.ğ‘½\n7: else ifğ‘â€²=NULL then\n8: existâ†False âŠ²corresponding pair does not exsit\n9: else ifğ‘â€²points to another leaf node Nâ€²then\n10: Î”â€²â†Nâ€².Î”\n11: existâ†deleteFromLeafNode (Nâ€²,ğ‘¥)\n12: ND.Î”-=1+Î”â€²âˆ’Nâ€².Î”\n13: ifNâ€².Î©=1then âŠ²Nâ€²covers only one pair ğ‘â€²â€²\n14: ND.ğ‘½[ğ‘ğ‘œğ‘ ]â† the remaining one pair ğ‘â€²â€²contained in Nâ€²\n15: ND.Î”-= 1 and delete Nâ€²\n16: ND.Î©-= (exist =True ? 1 : 0), ND.ğœ…=ND.Î”\nND.Î©\n17: return exist\n7.1 Experimental Settings\nDatasets. We use four real datasets from the SOSD benchmark [ 36]\nand one synthetic dataset.\nâ€¢FB[2] contains 200M Facebook user ids.\nâ€¢WikiTS [3] contains 200M unique request timestamps (in inte-\ngers) of log entries of the Wikipedia web-site.\nâ€¢OSM [4] contains 800M ids of OpenStreetMap cells.\nâ€¢Books [5] contains 800M ids of books in Amazon.\nâ€¢Logn contains 200M unique values sampled from a heavy-tail\nlog-normal distribution with ğœ‡=0andğœ=1.\nFor each key, we associate it with a random integer number and\npack them as a simulated record. The records are stored in an data\narray. For each record, its key and address together form a pair. For\nthe pairs for indexâ€™s bulk loading, we sort them according to their\nkeys and feed them to indexâ€™s bulk loading algorithm.\nCompetitors. We compare DILI with the following methods:\nâ€¢BinS does a binary search over the whole sorted key set to find\nthe position of the given search key.\nâ€¢B+Tree [15]: We use a production quality B+Tree implementa-\ntion stx::btree for comparison [6].\nâ€¢MassTree [35] is a variant of B-Tree which improves cache-\nawareness by employing a trie-like [12] structure.\nâ€¢RMI [29] is built through linear stages and cubic stages.\nâ€¢ALEX [18] is an in-memory learned index which partition keys\ninto leaf nodes in a relatively static way [7].\nâ€¢RS(RadixSpline) [ 26] uses a linear spline to approximate the\nCDF of the data and a radix table to index spline points.\nâ€¢PGM (PGM-index) [ 20] contains multiple levels, each represent-\ning an error-bounded piece-wise linear regression [8].\nâ€¢LIPP [44] can be seen as a special RMI. Its root node uses a\nlinear regression model with the range of [0,ğ‘), whereğ‘is\nthe dataset cardinality. At lower levels, LIPP recursively uses\nlinear regression models to partition search keys until each keyâ€™s\nposition is accurately predicted. LIPP aims to predict as many\nkeysâ€™ position as possible with only one model [9].\nFor RMI and RS, we adopt the implementations in SOSD [10, 36].\nTable 2 summarizes the properties of all indexes. The better\nperformance is indicated in bold.\nEvaluation Metrics. We use two performance metrics: Lookup is\nthe average lookup time per query, including the time spent in the\n\nPengfei Li, Hua Lu, Rong Zhu, Bolin Ding, Long Yang, and Gang Pan\nTable 2: Properties of different methods\nMethodSupport\nupdateConsider data\ndistributionExtra local\nsearchTree\nheightMemory\ncost\nB+Tree âœ“Ã— âœ“ medium medium\nRMIÃ—Ã— âœ“ low small\nRSÃ— âœ“ âœ“ low small\nPGM âœ“Ã— âœ“ high medium\nMasstree âœ“Ã— âœ“ medium medium\nALEX âœ“ âœ“ âœ“ medium medium\nLPP âœ“Ã—Ã— medium large\nDILI âœ“ âœ“Ã— low medium\nindex and in finding the records in the data array. Throughtput is\nthe number of operations, including query, insertion and deletion,\nthat of a method can handle per second.\nParameter Settings. Table 3 lists the parameter settings for B+Tree\nand ALEX. They are built with bulk loading for better lookup and\nthroughput performance. For RMI and RS, we follow [ 36] to use\ntwo settings with the largest (L) and smallest (S) memory costs.\nIn our machine, an LL-cache line is of 64 bytes and fetching a\ncache line from the memory costs 130 CPU cycles at worst [ 16,21,\n41]. A DILI (internal or leaf) node can be held in a single cache line.\nTherefore, we set ğœƒN=ğœƒC=130. Executing a linear function as well\nas type casting cost about ğœ‚=25cycles. Moreover, ğœ‡ğ¿=5andğœ‡ğ¸=\n17cycles are spent on executing operations except accessing pairs\nin linear search and exponential search, respectively. The decaying\nrateğœŒin Eq. 5, enlarging ratio ğœ‚in Algorithm 5 and maximum\nfanoutğœ”in Algorithm 3 are set to 0.2, 2 and 4,096, respectively.\nTable 3: Parameter settings in experiments\nParam Description Setting\nÎ© Node fanout of a B+Tree 16, 32, 64, 128, 256, 512\nÎ“ Max node size of ALEX 16KB, 64KB, 1MB, 16MB, 64MB\n7.2 Overall Query Performance\nFor each dataset, we build all indexes using the whole dataset ğ‘·\nand randomly select 100M keys in keys (ğ‘·) to form point queries.\nAll competitors are built with their preferred parameter settings.\nTable 4 reports on the overall performance results of all methods\non point queries. To investigate the effect of the local optimization,\nwe also include a DILI variant DILI-LO that applies no local opti-\nmization in its leaf nodes but tightly arranges pairs in the entry\narrays. The search via DILI-LO simply follows Algorithm 1. We\nchoose the LIPP as the fixed reference point as it is the best among\nall competitors. The color-encoding indicates how much faster or\nslower a model is against the reference point.\nDILI has clear advantages over other state-of-the-art methods.\nCompared to LIPP, DILI saves about 9% to 34% lookup time. The\ndesign of DILIâ€™s bulk loading algorithm make the keys in DILIâ€™s\nleaf nodes almost linearly distributed and the linear regression\nmodels well describe these distributions. Thus, conflicts happens\nmore rarely in DILI. The traversal path of DILI is shorter than that\nof LIPP and other competitors, which results in DILI has better\nperformance. Compared to ALEX and PGM, besides the shorter\ntraversal path, DILI is able to avoid the search inside the leaf nodes\nand have clearer advantages. BinS, MassTree and all variants ofTable 4: Lookup time (ns) of all methods after bulk loading\nModel Config FB WikiTS OSM Books Logn\nBinS 819 822 839 844 817\nB+TreeÎ©=16 629 633 578 584 624\nÎ©=32 620 616 589 611 629\nÎ©=64 658 649 641 651 653\nÎ©=128 722 719 693 699 725\nÎ©=256 794 790 776 775 790\nÎ©=512 995 980 979 982 984\nALEXÎ“=16KB 655 580 544 509 463\nÎ“=64KB 573 465 419 382 398\nÎ“=1MB 490 248 281 274 259\nÎ“=16MB 476 236 223 221 170\nÎ“=64MB 462 252 234 203 161\nRMI(S) 833 806 1255 540 907\n(L) 215 175 166 221 208\nRS(S) 398 313 358 355 172\n(L) 305 264 218 210 132\nMassTree 1245 1238 1500 1492 1220\nPGM 483 468 474 457 453\nLIPP 197 152 178 182 173\nDILI-LO 240 168 192 208 142\nDILI 150 139 126 153 116\nB+Tree even needs to take 4â€“10 times of lookup time to search for a\nkey on average. Also, DILI clearly outperforms RMI and RS on pro-\ncessing point queries. And it is noteworthy that RMI and RS do not\nsupport updates. These results illustrate DILI achieves large lookup\nsuperiority over other alternatives. In addition, DILI consumes less\nlookup time than DILI-LO over all the five datasets. This verifies\nthe effectiveness of the local optimization in DILIâ€™s leaf nodes.\nAs B+Tree with Î©=32, ALEX with Î“=16MB and the large RMI\nand RS perform best among their variants, we will choose them as\nrepresentatives and omit the evaluation of the other variants in the\nfollowing sections. The parameter settings of these methods will\nalso be omitted when we refer to them. Also, to save space, we will\nomit the comparable results on the datasets OSM and Books. The\nresults on both datasets are similar to that on other datasets.\nCache Misses. DILIâ€™s advantage is partly due to that the design\nof DILIâ€™s structure makes DILI triggers fewer cache misses. A sin-\ngle LL-cache miss incurs 50-200 additional cycles [ 16,29,41]. In\ncontrast, register operations like addition and multiplication cost\n1-3 cycles only. Avoiding cache misses clearly speeds up query pro-\ncessing for DILI. Table 5 reports the average number of LL-cache\nmisses for all methods. In particular, compared with ALEX and\nLIPP, DILI avoids up to 9.7 and 3.5 LL-cache misses per query.\nTable 5: #LL-cache misses of methods per point query\nDataset B+Tree RMI RS PGM MassTree ALEX LIPP DILI\nFB 10.27 5.25 8.43 10.73 9.84 14.91 7.94 4.88\nWikiTS 10.51 4.60 5.68 11.56 9.24 7.36 5.86 4.78\nLogn 10.19 5.28 3.22 9.88 9.48 4.47 7.17 3.80\nOSM 10.47 3.89 4.50 7.42 12.85 5.86 7.13 4.08\nBooks 10.46 6.02 4.74 7.38 13.02 4.27 7.81 4.31\nOffline Construction Time. On a 100M dataset, the bulk loading\nof B+Tree, ALEX, LIPP and DILI takes less than 1, 2, 1 and 6 minutes,\nrespectively. Each construction time grows almost linearly with the\nincrease of the data size. The most time-consuming step in DILIâ€™s\nconstruction is the greedy merging algorithm to get the BU nodes\n\nDILI: A Distribution-Driven Learned Index (Extended version)\nat the bottom layer. A direct yet effective approach to make this\nstep more efficient is sampling. When a piece ğ¼ğ‘˜ğ‘¢(in Algorithm 3)\ncovers many keys, we could randomly or selectively sample part of\nthe keys, e.g.,select one key out of two, to get the linear regression\nmodel and calculate the cost. The sampling strategy makes little\ninfluence on the whole BU-tree node layout or the performance\nof the generated DILI. However, it will make the the construction\ntime of the BU-tree and DILI decrease by over 1 minute. As the bulk\nloading is one-time, DILIâ€™s a few more minutes highly pay off.\nTable 6: The statistics of DILI\nDatasetMinimum\nheightMaximum\nheightAverage\nheight# of conflicts\nper 1K keys\nFB 3 8 3.45 227.1\nWikiTS 3 6 3.09 44.4\nLogn 3 4 3.01 1.2\nOSM 3 9 3.26 117.7\nBooks 3 8 3.44 220.4\nAnalysis of DILIâ€™s Construction. Table 6 shows DILIâ€™s minimum/\nmaximum/average heights and the number of conflicts in DILIâ€™s\nconstruction for different datasets. Apparently, DILI has a shallow\nstructure. The slot assignments for most pairs cause no conflicts.\nThe average heights of the DILIs built on the Logn and WikiTS\ndataset are smaller than that of others. The reason is that the keys\nin both datasets are more linearly or piecewise linearly distributed.\nThus, the linear regression models in the leaf nodes are able to\nmake more accurate predictions and thus result in less conflicts.\nIndex Size. Fig. 6 displays the memory cost of different methods.\nRMI and RS consume the least memory. However, they do not\nneed to store pairs in their structures and do not support data up-\ndates. DILI consumes more memory than B+Tree, PGM and ALEX\ndue to the local optimization in the leaf nodes. A conflict will result\nin a new leaf node creation and an empty slot in the entry array.\nNevertheless, our design strikes a trade-off between the memory\ncost and the query efficiency. Considering modern computers usu-\nally have huge memory, it is acceptable to improve the efficiency\nat the expense of some memory. Although LIPP also tries to strike\na memory-efficiency trade-off and adopts a similar strategy for\nconflicts, its node layout is not so optimized as ours. Thus, LIPP\nresults in more conflicts and memory costs. Its memory cost is\nat least one order of magnitude larger than others. On the other\nhand, after disabling the local optimization, the memory cost of the\nvariant DILI-LO becomes comparable with B+Tree. Meanwhile, the\nquery performance does not degrade much.\nFBW ikiTSL ognIndex size (bytes) B+Tree    RMI    RS       PGM    MassTree   \nALEX     LIPP    DILI    DILI-LO1\n0101\n091\n081\n071\n06\nFigure 6: Index sizes7.3 Performance on Different Workloads\nWe conduct experiments to compare DILI and the alternatives on\ndifferent types of workloads: (1) The Read-only workload contains\n100M point queries. (2) The Read-Heavy workload contains 50M\ninsertions and 100M point queries. (3) The Write-Heavy workload\ncontains 100M insertions and 50M point queries. (4) The Write-only\nworkload contains 100M insertions. In each dataset ğ‘·, we randomly\nselect 50% of the pairs as the initial dataset ğ‘·0. The other 50% of\nğ‘·is named ğ‘·1. All workloads are tested on an index with bulk\nloading of ğ‘·0. Besides, the query keys are randomly selected from\nthekeys(ğ‘·), and the pairs to be inserted are randomly chosen from\nğ‘·1. Each workload is a random mix of queries and insertions. We\nrun the workloads on different indexes for five times and obtain\ntheir average throughput. As RMI and RS do not support updates,\nthey are excluded from the experiments involving insertions. The\nexperimental results are shown in Fig. 7. Overall, DILI achieves the\nhighest throughput on all workloads.\nFor the read-only workloads, compared to the others, DILI achieves\nshorter average search path. In particular, DILI accesses only 0.2-\n1 node per point query on average. This indicates that DILI uti-\nlizes the data distribution well and thus the learned models in its\nleaf nodes incur few conflicts. The alternatives need longer search\npaths queries and extra steps to carry out local search. RMI and RS\nachieves comparably long search paths with DILI. However, the\neffort of correcting their prediction results in lower throughput.\nWhen more insertions are in the workloads, we see that DILI still\noutperforms others though its performance also degrades. The rea-\nson is that an insertion not only includes searching for a key but\nalso writing a pair to an entry array. Also, new node creations are\nrequired to process conflicts. Moreover, adjustments occasionally\nhappen to bound DILIâ€™s height. Even though insertions on DILI re-\nquires more time than queries, DILI is still able to deal with index\nstructure change well and more efficient at insertions than others.\nPGM performs worst in these workloads as it needs ğ‘‚(logğ‘)trees\nto support insertions and each query will search in all these trees.\nCompared to B+Tree and ALEX, DILI can avoid element shifting.\nAlso, the new node creation in DILI is light-weight. In addition,\ncompared to LIPP, DILI has shorter traversal path for insertions.\nOur strategy of setting more slot redundancy for leaf nodes more\nfrequently accessed also avoid unnecessary node adjustments.\n7.4 Effect of Many Deletions\nWe also experimentally investigate the effect of deletions on DILI,\nB+Tree, PGM MassTree and ALEX. LIPP is excluded as it does not\nsupport deletions. We first build each of them with bulk loading of\nthe whole ğ‘·. Then we repeatedly delete/search for random keys\nfrom ğ‘·via all methods and observe their changing throughput on\nthree workloads: (1) Read-Heavy workload, which contains 100M\nlookups and 50M deletions; (2) Deletion-Heavy workload, which\ncontains 100M deletions and 50M loopups. Fig. 8 shows the results.\nReferring to Figure 8, on Read-Heavy workload, DILI achieves up\nto 3.6Ã—, 2.3Ã—, 7.0Ã—and 2.3Ã—higher throughput than B+Tree, PGM,\nMassTree and ALEX, respectively. This illustrates that DILI main-\ntains high performance on queries with deletions happening. On\nDeletion-Heavy workload, only ALEX performs a little better than\nDILI on Logn dataset. As ALEX almost adopts lazy deletion strategy,\n\nPengfei Li, Hua Lu, Rong Zhu, Bolin Ding, Long Yang, and Gang Pan\nFBW ikiTSL ogn0246810(\nd) Write-Only( c) Write-Heavy( a) Read-only( b) Read-HeavyThroughput (million ops / s ) B+Tree                 RMI                  RS                 PGM                 MassTree                  ALEX               LIPP                DILIF\nBW ikiTSL ogn0246F\nBW ikiTSL ogn01234F\nBW ikiTSL ogn0123\nFigure 7: DILI vs. State-of-the-art methods: throughput comparisons on four workloads\nFBW ikiTSL ogn0246(\nb) Write-Heavy( a) Read-HeavyThroughput (million ops / s ) B+Tree     PGM      MassTree      ALEX       DILIF\nBW ikiTSL ogn0246\nFigure 8: Performance after deletions\ndeleting a pair from ALEX almost equals searching for it. However,\nthis strategy will cause its lookup time not decrease even through\nit index a small amount of data only. Actually, DILI performs much\nbetter than ALEX when the workload consists of more queries.\n8 RELATED WORK\nB-tree variants. A B+Tree [ 15] is the most popular B-tree variant\nin which each internal node contains only keys, and the leaf nodes\nare chained with extra links. Digital B-trees [ 33] allows a node to use\ntwo pages via a hashing-like technique. The B-trie [ 12] combines\nB-Tree and trie [ 27] to index strings stored in external memory.\nMassTree [ 35] employs a trie-like concatenation of B-trees to im-\nprove cache-awareness in indexing key-value pairs. A BF-tree [ 13]\nreplaces B-Tree leaf nodes with bloom filters to substantially reduce\nthe index size. Unlike all B-tree variants, our DILI stores models\ninstead of pointers in the nodes for indexing purpose.\nLearned indexes for 1D keys. The recursive model index (RMI) [ 29]\nuses staged models. An internal model directs a key search to one of\nits child models and a bottom-level model predicts an error-bounded\nposition in the database. RMI has inspired a number of learned in-\ndexes. To reduce index memory footprint, a FITing-Tree [22] uses\nlinear models to replace the leaf nodes of a B-Tree. CARMI [ 47]\napplies data partitioning to RMI construction and supports data up-\ndate. NFL [ 45] uses a normalizing flow techniques [ 40] to transform\nthe key space for better approximation on the CDF. PGM-index [ 20]\nemploys piecewise linear models to approximate the relationship\nbetween search keys and their positions in a database. Hermit [ 46]\ncreates a succinct tiered regression search tree (TRS-tree) which\npasses a search query to an existing index for correlated columns.\nRadixSpline [ 26] uses a set of spline functions as the learned in-\ndex that can be built in a single pass over sorted data. ALEX [ 18]\ntrains accurate linear regression models to split the key space, or-\nganizes all models also in a tree-like structure, and uses a gapped\narray for each leaf node. ALEX supports updates. LIPP [ 44] uses\nkernelized linear functions as learned models that make perfect pre-\ndictions. However, it does not make use of the information of datadistribution. SOSD [ 36] is a preliminary benchmark for 1D learned\nindexes. FINEdex [ 30] is a fine-grained learned index scheme, which\nconstructs independent models with a flattened data structure to\nprocess concurrent requests. APEX [ 34] combines the recently re-\nleased persistent memory optimization [ 24] and ALEX to support\npersistence and instant recovery. The on-disk learned index proto-\ntype AirIndex [ 14] uses with a storage-aware auto-tuning method\nto minimize accesses to the external memory. To validate the effec-\ntivenesss of the existing updatable learned indexes, Wongkham et\nal. [43] conduct a comprehensive evaluation.\nLearned indexes for multidimensional data. SageDB [ 28] ex-\ntends RMI to index multidimensional data in a transformed 1D space.\nZM-index [ 42] applies RMI to the Z-order curve [ 37] to process\nspatial point and range queries. ML-index [ 17] applies RMI to iDis-\ntance [ 25] to support queries on multidimensional data. Flood [ 38]\nand Tsunami [ 19] are learned indexes for in-memory multidimen-\nsional data, whereas LISA [ 31] and RSMI [ 39] are for disk-resident\ndynamic spatial data. In contrast, our DILI focuses on 1D data.\n9 CONCLUSION AND FUTURE WORK\nIn this work, we design for in-memory 1D keys a distribution-\ndriven learned tree DILI. Its nodes use linear regression models\nto map keys to corresponding children or records. An internal\nnodeâ€™s key range is equally divided by all its children, endowing\ninternal models with perfect accuracy for finding the leaf node\ncovering a key. We optimize DILIâ€™s node layout by a two-phase bulk\nloading approach. First, we create a bottom-up tree that balances the\nnumber of leaf nodes and tree height. Based on that, we determine\nfor each DILI internal node its best fanout and local model. Also, we\ndesign algorithms for DILI for data updates. Extensive experimental\nresults show that DILI clearly expand the state of the art.\nFor future research, it is relevant to adapt DILI to disk-resident\ndata. To this end, the cost model for the BU-Tree construction should\nconsider the expected IOs, striking a trade-off between the IO cost\nand the computational overhead. Also, the local optimization should\nbe disabled as it may create leaf nodes covering few keys. Moreover,\nit is interesting to consider concurrent data updates with DILI. Note\nthat an insertion or deletion operation in DILI involves only one\nleaf node. The node adjustment of DILI is much simpler than the\nrebalance operation of the B+Tree. Theoretically, the lock-free and\nlock-crabbing [23] approaches can be applied to DILI, in the same\nway as how they are applied to the B+Tree.\nACKNOWLEDGMENTS\nHua Luâ€™s work was partly supported by Independent Research Fund\nDenmark (No. 1032-00481B).\n\nDILI: A Distribution-Driven Learned Index (Extended version)\nREFERENCES\n[1] https://github.com/pfl-cs/DILI.\n[2] https://doi.org/10.7910/DVN/JGVF9A/Y54SI9.\n[3] https://doi.org/10.7910/DVN/JGVF9A/SVN8PI.\n[4]https://www.dropbox.com/s/j1d4ufn4fyb4po2/osm_cellids_800M_uint64.zst?dl=\n1.\n[5]https://www.dropbox.com/s/y2u3nbanbnbmg7n/books_800M_uint64.zst?dl=1.\n[6] https://panthema.net/2007/stx-btree.\n[7] https://github.com/microsoft/ALEX.\n[8] https://github.com/gvinciguerra/PGM-index.\n[9] https://github.com/Jiacheng-WU/lipp.\n[10] https://github.com/learnedsystems/SOSD.\n[11] Jayadev Acharya, Ilias Diakonikolas, Jerry Li, and Ludwig Schmidt. 2016. Fast Al-\ngorithms for Segmented Regression. In ICML , Maria-Florina Balcan and Kilian Q.\nWeinberger (Eds.), Vol. 48. 2878â€“2886.\n[12] Nikolas Askitis and Justin Zobel. 2009. B-tries for disk-based string management.\nVLDB J. 18, 1 (2009), 157â€“179.\n[13] Manos Athanassoulis and Anastasia Ailamaki. 2014. BF-Tree: Approximate Tree\nIndexing. Proc. VLDB Endow. 7, 14 (2014), 1881â€“1892.\n[14] Supawit Chockchowwat. 2022. Tuning Hierarchical Learned Indexes on Disk\nand Beyond. In SIGMOD . 2515â€“2517.\n[15] Douglas Comer. 1979. The Ubiquitous B-Tree. ACM Comput. Surv. 11, 2 (1979),\n121â€“137.\n[16] Intel Corporporation. 2018. Intel 64 and ia-32 architectures software developer\nmanuals. https://software.intel.com/content/www/us/en/develop/articles/intel-\nsdm.html.\n[17] Angjela Davitkova, Evica Milchevski, and Sebastian Michel. 2020. The ML-Index:\nA Multidimensional, Learned Index for Point, Range, and Nearest-Neighbor\nQueries. In EDBT . OpenProceedings.org, 407â€“410.\n[18] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\nDavid B. Lomet, and Tim Kraska. 2020. ALEX: An Updatable Adaptive Learned\nIndex. In SIGMOD . 969â€“984.\n[19] Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. 2020.\nTsunami: A Learned Multi-dimensional Index for Correlated Data and Skewed\nWorkloads. CoRR abs/2006.13282 (2020). arXiv:2006.13282 https://arxiv.org/abs/\n2006.13282\n[20] Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-index: a fully-dynamic\ncompressed learned index with provable worst-case bounds. Proc. VLDB Endow.\n13, 8 (2020), 1162â€“1175.\n[21] Agner Fog. 2018. Lists of Instruction Latencies, Throughputs and Micro-operation\nBreakdowns for Intel, AMD and VIA CPUs, Technical University of Denmar, Last\nupdated 2021-01-31. http://www.agner.org/optimize/instruction_tables.pdf,DoA.\n(2018).\n[22] Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and Tim\nKraska. 2019. FITing-Tree: A Data-aware Index Structure. In SIGMOD . 1189â€“1206.\n[23] Goetz Graefe. 2010. A survey of B-tree locking techniques. ACM Trans. Database\nSyst. 35, 3 (2010), 16:1â€“16:26.\n[24] Intel. 2021. Intel Optane Persistent Memory (PMem), Last updated 2021-\n11-13. https://www.intel.ca/content/www/ca/en/architecture-and-technology/\noptane-dcpersistent-memory.html.\n[25] H. V. Jagadish, Beng Chin Ooi, Kian-Lee Tan, Cui Yu, and Rui Zhang. 2005.\niDistance: An adaptive B+-tree based indexing method for nearest neighbor\nsearch. ACM Trans. Database Syst. 30, 2 (2005), 364â€“397.[26] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2020. RadixSpline: a single-pass learned\nindex. In aiDM@SIGMOD . 5:1â€“5:5.\n[27] Donald Ervin Knuth. 1997. The art of computer programming . Vol. 3. Pearson\nEducation.\n[28] Tim Kraska, Mohammad Alizadeh, Alex Beutel, Ed H. Chi, Ani Kristo, Guillaume\nLeclerc, Samuel Madden, Hongzi Mao, and Vikram Nathan. 2019. SageDB: A\nLearned Database System. In CIDR .\n[29] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In SIGMOD , Gautam Das, Christopher M.\nJermaine, and Philip A. Bernstein (Eds.). 489â€“504.\n[30] Pengfei Li, Yu Hua, Jingnan Jia, and Pengfei Zuo. 2021. FINEdex: A Fine-grained\nLearned Index Scheme for Scalable and Concurrent Memory Systems. Proc. VLDB\nEndow. 15, 2 (2021), 321â€“334.\n[31] Pengfei Li, Hua Lu, Qian Zheng, Long Yang, and Gang Pan. 2020. LISA: A Learned\nIndex Structure for Spatial Data. In SIGMOD . 2119â€“2133.\n[32] Pengfei Li, Hua Lu, Rong Zhu, Bolin Ding, Long Yang, and Gang Pan. 2023. DILI:\nA Distribution-Driven Learned Index (Extended version). CoRR abs/2304.08817\n(2023). https://doi.org/10.48550/arXiv.2304.08817 arXiv:2304.08817\n[33] David B. Lomet. 1981. Digital B-Trees. In VLDB . 333â€“344.\n[34] Baotong Lu, Jialin Ding, Eric Lo, Umar Farooq Minhas, and Tianzheng Wang.\n2021. APEX: A High-Performance Learned Index on Persistent Memory. Proc.\nVLDB Endow. 15, 3 (2021), 597â€“610.\n[35] Yandong Mao, Eddie Kohler, and Robert Tappan Morris. 2012. Cache craftiness\nfor fast multicore key-value storage. In EuroSys . 183â€“196.\n[36] Ryan Marcus, Andreas Kipf, Alexander van Renen, Mihail Stoian, Sanchit Misra,\nAlfons Kemper, Thomas Neumann, and Tim Kraska. 2020. Benchmarking Learned\nIndexes. Proc. VLDB Endow. 14, 1 (2020), 1â€“13.\n[37] Guy M Morton. 1966. A computer oriented geodetic data base and a new technique\nin file sequencing. (1966).\n[38] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020. Learn-\ning Multi-Dimensional Indexes. In SIGMOD . 985â€“1000.\n[39] Jianzhong Qi, Guanli Liu, Christian S. Jensen, and Lars Kulik. 2020. Effectively\nLearning Spatial Indices. Proc. VLDB Endow. 13, 11 (2020), 2341â€“2354.\n[40] Esteban G Tabak and Cristina V Turner. 2013. A family of nonparametric density\nestimation algorithms. Communications on Pure and Applied Mathematics 66, 2\n(2013), 145â€“164.\n[41] Vladimir Tsymbal. 2019. Tuning Guides and Performance Analysis Papers, Last\nupdated 2020-12-15. https://software.intel.com/content/www/us/en/develop/\narticles/processor-specific-performance-analysis-papers.html.\n[42] Haixin Wang, Xiaoyi Fu, Jianliang Xu, and Hua Lu. 2019. Learned Index for\nSpatial Queries. In MDM . 569â€“574.\n[43] Chaichon Wongkham, Baotong Lu, Chris Liu, Zhicong Zhong, Eric Lo, and\nTianzheng Wang. 2022. Are Updatable Learned Indexes Ready? Proc. VLDB\nEndow. 15, 11 (2022), 3004â€“3017.\n[44] Jiacheng Wu, Yong Zhang, Shimin Chen, Yu Chen, Jin Wang, and Chunxiao Xing.\n2021. Updatable Learned Index with Precise Positions. Proc. VLDB Endow. 14, 8\n(2021), 1276â€“1288.\n[45] Shangyu Wu, Yufei Cui, Jinghuan Yu, Xuan Sun, Tei-Wei Kuo, and Chun Jason\nXue. 2022. NFL: Robust Learned Index via Distribution Transformation. Proc.\nVLDB Endow. 15, 10 (2022), 2188â€“2200.\n[46] Yingjun Wu, Jia Yu, Yuanyuan Tian, Richard Sidle, and Ronald Barber. 2019.\nDesigning Succinct Secondary Indexing Mechanism by Exploiting Column Cor-\nrelations. In SIGMOD . 1223â€“1240.\n[47] Jiaoyi Zhang and Yihan Gao. 2022. CARMI: A Cache-Aware Learned Index with a\nCost-based Construction Algorithm. Proc. VLDB Endow. 15, 11 (2022), 2679â€“2691.\n\nPengfei Li, Hua Lu, Rong Zhu, Bolin Ding, Long Yang, and Gang Pan\nA APPENDIX\nThis appendix presents more experimental results and analyses that\ncannot be included in the conference version due to space limit.\nA.1 Index Scalability on Read-Only\nWorkloadss\nTo further investigate how different methods perform in terms of\nscalability, we again run the read-only workloads, initializing each\nindex with 50M, 100M, 150M and 200M keys, respectively. As the\nresults on all datasets are similar, we only report the results on FB in\nFig. 9 (a). As the number of indexed keys increases, DILI maintains\nhigher throughput than the alternatives. This indicates that DILI is\nmore adaptive to the size of data.\n50M1 00M1 50M2 00M02468Throughput (million ops / s ) B+Tree   RMI      RS    PGM    MassTree    ALEX    LIPP   DILIR\nead-HeavyW rite-Heavy0123(\nb)( a)\nFigure 9: (a) Throughput on FB dataset with varying data car-\ndinalities; (b) Performance with distribution shifting.\nA.2 Effect of Distribution Shifting\nTo investigate if DILI still works well when keys from a different dis-\ntribution are inserted, we design an experiment as follows. Suppose\nğ‘·ğ´is another pair set whose keys are in a different distribution from\nğ‘·. First, we build DILI and other indexes supporting inserts using\ntheir bulk loading algorithms on the whole ğ‘·. Then, we repeatedly\ninsert random keys from ğ‘·ğ´via all indexes. Meanwhile, search\noperations are alternatively conducted. We tested a representative\ncombinations of ğ‘·andğ‘·ğ´from different distributions: FBand\nLogn . We observe each methodâ€™s throughput on Read-Heavy and\nWrite-Heavy workloads. Fig. 9 (b) reports that DILI performs a bit\nworse than ALEX on Write-Heavy workload. The reason behind is\nthat DILI is built to grasp the distribution characteristics of specific\ndataset. Thus, inserting pairs from different distribution will incur\nmore conflicts and result in adjustments happen more frequently.\nHowever, on Read-Heavy workload, DILI still has clear advantages\nover other alternatives. Also, in real-life scenarios, queries are of-\nten more common than insertions. Thus, DILI is supposed to have\nbetter performance than other state-of-the-art methods in practice.\nA.3 Effect of Skewed Writes\nOur next experiment investigates the performance of DILI and\nother alternatives with skewed writes. Suppose ğ‘·, whose size is\n100M, is the pair set used in the bulk loading stage. The range of\nkeys (ğ‘·) is[ğ´,ğ´+10ğ›¿)andğ‘¸is another pair set whose keys are in a\ndifferent distribution from ğ‘·. We first compress the keys in keys (ğ‘¸)\nby mapping them to the range [ğ´,ğ´+ğ›¿). Next, a new pair set ğ‘¸â€²\nis generated by randomly selecting 100M distinct mapped keys and\npacking them with the original record pointers of ğ‘¸together. We\nuse a ğ‘¸with 200M elements to generate ğ‘¸â€². DILI, B+Tree, ALEXand LIPP are built using their bulk loading algorithms on the ğ‘·.\nAfterwards, random pairs from the skewed set ğ‘¸â€²are repeatedly\ninserted and then search operations are conducted via all indexes.\nWe test all possible combinations of ğ‘·andğ‘¸â€²from three different\ndistributions: FB, WikiTS and Logn. We observe each methodâ€™s\nthroughput on read and write operations. Fig. 10 reports the results\nof DILI and the competitors.\nWriteR ead0123456Throughput (million ops / sec ) B+Tree   PGM    MassTree    ALEX     LIPP    DILIW\nriteR ead0123456(\nb) P: FB, Q: Logn( a) P: FB, Q: WikiTSW\nriteR ead0123456Throughput (million ops / sec )W\nriteR ead0123456(\nf) P: Logn, Q: WikiTS( e) P: Logn, Q: FB(d) P: WikiTS, Q: Logn( c) P: WikiTS, Q: FBW\nriteR ead0123456Throughput (million ops / sec )W\nriteR ead0123456\nFigure 10: The effects of skewed insertions\nReferring to Fig. 10, DILI does not have clear advantages over\nother competitors as before. It is because that DILI is a distribution-\ndriven learned index which has customized node structure for each\ndataset. When the distribution of the inserted keys is skewed, more\nconflicts and node adjustments tend to happen, which result in a\nhigher tree. For example, after inserting 100M skewed keys from\nthe WikiTS dataset to the DILI built with the FB dataset, the average\nheight of the DILI is 4.22. In contrast, if the inserted keys are not\nout of the initial distribution, the height is 3.86. Nevertheless, in\ncomparison with LIPP and ALEX, DILI still achieves comparable or\neven better results.\nA.4 Memory Cost Analysis in Write-heavy\nWorkloads\nWe also conduct experiments to investigate the effect of insertions\non the memory costs of DILI and the alternatives. Similarly, we\nbuild all indexes with half of the dataset and use them to carry out\n100M insertions where the inserted pairs are from the remaining\nhalf of the dataset. Table 7 shows the memory cost comparisons\namong all indexes.\nAs shown in Table 7, the memory costs of B+Tree, MassTree and\nPGM are smaller than that of ALEX, DILI and LIPP. DILI achieves\ncomparable results with ALEX. After the same 100M insertions from\nthe Logn dataset, DILI uses less memory than ALEX. Compared\n\nDILI: A Distribution-Driven Learned Index (Extended version)\nTable 7: The memory costs ( 109bytes) of different indexes in\nwrite-heavy workloads\nDatasetBefore/after\ninsertionsB+Tree\nÎ©=32MassTree PGMALEX\nÎ“=16MBLIPP DILI\nFBBefore 1.56 1.56 1.60 2.35 12.68 5.07\nAfter 3.12 3.12 3.21 4.59 22.60 8.91\nWikiTSBefore 1.56 1.56 1.60 2.31 10.11 3.95\nAfter 3.12 3.12 3.20 4.31 21.22 5.79\nLognBefore 1.56 1.56 1.60 2.30 14.53 3.38\nAfter 3.12 3.12 3.20 4.10 18.04 3.67\nto DILI, the memory costs of LIPP on all of the three datasets are\nmuch larger. This indicates that DILI is able to avoid much more\nconflicts as well as the empty slots in the entry array.\nA.5 Range Query Performance\nThe range query via DILI is processed by a search for the lower\nbound key followed by a scan for the subsequent keys. Fig. 11\nreports the average response time of B+Tree, PGM, ALEX, LIPP,\nDILI and DILI-LO on short range queries. Following the settings\nin [18], we use less than 100 keys in a range. All methods are\nbuilt with bulk loading on a full ğ‘·on which 10M random range\nqueries are issued. The advantage of DILI is less apparent than that\nin the point query performance comparison. This is attributed to\nthat the pairs are not densely stored in the entry arrays in DILIâ€™s\nleaf nodes and DILI needs to distinguish between different entry\ntypes. Nevertheless, DILI achieves higher throughput than LIPP\non all cases, and it is comparable to other competitors. Also, the\nperformance of DILI could be improved by the variant DILI-LO. In\nthis case, the leaf nodesâ€™ entry arrays only cover pairs. Accordingly,\nwe only need to perform a sequential scan over the whole entry\narray, without handling other kinds of entry elements possible\nin DILI. Referring to Fig. 11, the average range query response time\nof DILI-LO is shorter than that of DILI.\nFBW ikiTSL ogn020040060080010002000 B+Tree    PGM     ALEX   \nLIPP        DILI      DILI-LO  Response time (ns)\nFigure 11: Range query results\nA.6 Hyperparameter Studies\nTo study the effects of hyperparameters, we build different DILI ver-\nsions accordingly and observe their performance on the FB dataset.\nThe results on the other datasets are similar and thus omitted.\nEffects ofğœ”.ğœ”is used to control the least number of nodes at DILIâ€™s\neach level. According to our observations, the generated DILIs withthe value of ğœ”varied from 1,024 to 8,192 have the same node lay-\nout. DILI tends to have a wide structure. Thus, as long as the value\nofğœ”is large enough, it slightly influences the performance.\nEffects ofğœŒ.Table 8 shows the lookup time and memory costs\nof DILI with different values of ğœŒ, the decaying rate of the impact of\nBU internal nodes at higher level on the layout of DILIâ€™s leaf nodes.\nApparently, the value of ğœŒhas little influence on DILIâ€™s overall\nstructure and query performance. When the value of ğœŒis set to\naround 0.1, DILI performs the best.\nTable 8: The effects of the hyperparameter ğœŒ\nParamlookup\ntime (ns)Memory cost\n(109bytes)Average\nheight\nğœŒ=0.05 154 9.327 3.441\nğœŒ=0.1 151 9.325 3.439\nğœŒ=0.2 153 9.328 3.441\nğœŒ=0.5 162 9.369 3.442\nEffects ofğœ†.Also, we investigate how ğœ†influences the insertion\nperformance of DILI, where ğœ†is the pre-defined enlarging ratio for\nthe fanouts of DILIâ€™s leaf node if a node adjustment is performed.\nFirst, the DILI is built with half of the dataset. Then, we vary the\nvalue ofğœ†and insert 100M keys to DILI. After that, search operations\nare conducted. Table 9 shows the average insertion time, the average\nresults with different ğœ†values on the FB dataset. Results on other\ndtaasets are similar and thus omitted. The insertion performance\nof DILI is almost not influenced by ğœ†. When the value of ğœ†is set to\n2, DILI achieves the best lookup time and the shortest tree structure.\nTable 9: The effects of the hyperparameter ğœ†\nParamInsertion\ntime (ns)lookup\ntime (ns)Memory cost\n(109bytes)Average\nheight\nğœ†=1.5 483 182 8.924 3.866\nğœ†=2 487 180 8.911 3.865\nğœ†=4 488 184 8.912 3.868\nğœ†=8 485 185 8.913 3.869\nA.7 DILI vs RMI, RS and BU-Tree\nOur another experiment investigates why DILI outperforms RMI, RS\nand the BU-Tree (Section 4.1) on point queries. Note that DILI and\nBU-Tree process point queries in the same two-step fashion: finding\nthe relevant leaf node ( Step -1), followed by search inside the leaf\nnode ( Step -2). Similarly, the search process with RMI and RS can\nbe decomposed into two steps: the computation of the predicted\nposition ( Step -1) and the local search around the prediction ( Step -\n2). We create the BU-Tree using the whole dataset ğ‘·with Algo-\nrithm 2. DILI is built based on it. Then, we compare the breakdown\ntime cost of the three models. Table 10 reports the experimental\nresults.\nDILI vs RMI DILI has clear advantage over RMI at Step -1. RMI\nis built through three linear stages or a cubic stage and two linear\nstages. Also, RMI needs to calculate the error bound when making\npredictions. In contrast, all DILI versions have two layers of internal\nnodes and the step of locating the leaf node requires the calculations\nof two linear models only. Thus, DILI requires less calculations.\n\nPengfei Li, Hua Lu, Rong Zhu, Bolin Ding, Long Yang, and Gang Pan\nTable 10: DILI vs RMI, RS & BU-Tree\nDataset Model Step -1(ns) Step -2(ns) Total (ns)\nFBRMI 139 76 215\nRS 264 41 305\nBU-Tree 386 210 596\nDILI 75 75 150\nWikiTSRMI 138 37 175\nRS 212 52 264\nBU-Tree 377 110 487\nDILI 75 64 139\nLognRMI 135 73 208\nRS 101 31 132\nBU-Tree 273 57 330\nDILI 73 43 116\nAtStep -2, RMI uses a local search to access the true position. The\ntime cost of this step is correlated to the gap between the predicted\nposition and the true position. In our experiments, the average gaps\non the three datasets are 4.93, 2.97 and 53.9, respectively. DILI needs\nto access one or more leaf nodes and do a calculations by the linear\nmodel per leaf node. Thus, the time cost of DILIâ€™s Step -2depends\non the number of traversed leaf nodes. According to our statistics,\nthe average numbers of leaf nodes accessed per search on the three\ndatasets are only 1.16, 1.05 and 1.02, respectively. Thus, DILI is also\nmore efficient at Step -2in most cases.\nDILI vs RS The average response times of the first step with RS\nare larger than the counterpart with DILI. At the second step, RS\nadditionally needs 41 ns, 52 ns, 31 ns on average to carry out the lo-\ncal search on the three datasets, respectively, whereas the response\ntimes of DILI at the second step are smaller or comparable. Thus,\nRSâ€™s query performance is overall worse than DILI.\nDILI vs BU-Tree Compared to the BU-Tree, DILI incurs less time\ncost on both steps. In particular at Step -2, the time gap between\nboth trees is much larger. All in all, DILI incurs considerably less\ntotal time as it is more efficient at finding the leaf nodes. This\nverifies the effectiveness of the design of DILIâ€™s internal nodes and\nthe algorithm of building DILI from the BU-Tree.\nThe BU-Treeâ€™s construction time is about 5 minutes. Based on\nthat, building DILI needs less than 1 minute extra. BU-Tree and DILI\nconsume 1.90-1.98Ã—109and 3.38-5.07Ã—109bytes respectively on the\nthree datasets. Nevertheless, DILI is much more efficient at search.\nA.8 Frequency and Effect of Node Adjustments\nTo investigate the effects of the adjusting strategy, we conduct ex-\nperiments using DILI and its variant DILI-AD which does not adopt\nthe adjusting strategies when performing insertions. Following the\nWrite-only workload evaluation process in Section 7.2 in the paper,\nboth indexes are first built with half of the datasets. Then, for each\ndataset, we use both indexes to sequentially execute a Write-only\nworkload containing 100M insertions and a Read-only workload\ncontaining 100M queries. The comparisons of their response times\nper insertion, memory costs, average heights, lookup time after\ninsertions are shown in Table 11 below. In particular, the third\ncolumn of this table shows the average number of insertions per\nadjustment with DILI, which is the frequency of adjustments in\nDILIâ€™s leaf nodes.Table 11: The effects of the adjusting strategy\nDataset ModelAvg # of\ninsertions per\nadjustmentInsertion\ntime\n(ns)Memory\ncost\n(109bytes)Avg\nheightLookup\ntime\n(ns)\nFBDILI-AD - 442 9.31 3.91 187\nDILI 229.1 486 8.91 3.86 180\nWikiTSDILI-AD - 349 5.86 3.58 172\nDILI 758.4 361 5.78 3.55 168\nLognDILI-AD - 304 3.71 3.11 130\nDILI 1304.6 310 3.67 3.09 126\nApparently, the adjusting strategy in DILIâ€™s insertions will result\nin longer insertion time as it sometimes requires extra operations to\ncollect all pairs covered by a node and train a new linear regression\nmodel. However, the adjusting strategy makes DILI avoid more\nconflicts such that DILI will have a shorter structure and achieves\nbetter lookup time and memory costs.\nA.9 A Possible Way to Reduce DILIâ€™s\nConstruction Time\nThe most time-consuming step in DILIâ€™s construction is the greedy\nmerging algorithm for getting the BU nodes at the bottom layer.\nBecause we need to solve a MSE minimization problem for the\nunion of each continuous two pieces, i.e.Iğ‘˜ğ‘¢ÃIğ‘˜\nğ‘¢+1and calculate\nthe MSE to select the next two pieces to merge. Also, after merging\nIğ‘˜ğ‘¢andIğ‘˜\nğ‘¢+1into a new piece, namely ËœIğ‘˜ğ‘¢, we also need to conduct\nsimilar operations for Iğ‘˜\nğ‘¢âˆ’1ÃËœIğ‘˜ğ‘¢(Algorithm 3 in the paper).\nA direct yet effective approach to make the BU node generation\nmore efficient is sampling. When a piece covers many keys, we\ncould randomly or selectively sample part of the keys, e.g.,select\none key out of two, to get the linear regression model and calculate\nthe cost. The sampling strategy makes little influence on the whole\nBU-tree node layout and the performance of the generated DILI.\nHowever, it will clearly reduce the construction time of the BU-tree\nand DILI. An experiment is conducted to show this.\nWe apply the sampling strategy on the BU-treeâ€™s construction as\nfollows. When a piece Iğ‘˜ğ‘¢covers more than 8 keys, we only use half\nkeys get the linear regression models. A DILI is then built based\non this BU-tree. As a comparison, we build another DILI without\nadopting the sampling strategy. According to our statistics, the\nconstruction time of the former DILI is 1 minute more less than\nthat of the latter. Their comparison results on the lookup time are\nshown in Table 12. Apparently, the lookup time of the DILI with\nsampling is only slightly larger than that of the ordinary DILI.\nTable 12: The lookup time of DILI variants\nMethod FBWikiTS OSM Books Logn\nDILI-W-Sampling 160 145 121 154 117\nDILI 150 139 117 148 116\nConsidering the construction process of a DILI will be executed\nonce only, a slightly higher construction time is acceptable.\n\nDILI: A Distribution-Driven Learned Index (Extended version)\nA.10 Concurrent Insertions and Deletions\nin DILI\nIt is possible that DILI support concurrent data updates. It is note-\nworthy that either an insertion or deletion operation involves only\none leaf node. The node adjustment of DILI is much simpler than\nthe rebalance operation of the B+Tree. Theoretically, the lock-free\nand lock-crabbing approaches can also be applied to DILI, in the\nsame way as how they are applied to the B+Tree. For example,\nthe lock-crabbing protocol for an insertion can be simply applied\nto DILI as follows:\n(I)Get the lock for the lowest leaf node, namely ND, covering\nthe pair to be inserted.\n(II)IfNDhas an empty slot for the pair, put the pair here and\nrelease the lock for ND.\n(III) If a conflict happens, the following steps are sequentially\nexecuted:\n1)If a node adjustment is required, carry out the adjustment\nand exit. Otherwise, execute the following operations.2)Create a new leaf node Nâ€²\nDand put the node at the con-\nflicted slot.\n3) Get the lock for Nâ€²\nD.\n4) Release the lock for ND.\n5) Store the conflicting pairs in Nâ€²\nD.\n6) Release the lock for Nâ€²\nD.\nThe lock-crabbing protocal for a deletion is similar:\n(I)Get the locks for the lowest leaf node and its parent node,\nnamely NDand NP, respectively, covering the key to be\ndeleted.\n(II) Delete the corresponding pair from ND.\n(III) IfNTis an internal node or NDcovers more than one pairs,\nrelease the locks for NDandNP, and exit. Otherwise, execute\nthe on the following operations:\n1)Suppose NP.ğ‘½[ğ‘˜]stores the pointer to ND. Set NP.ğ‘½[ğ‘˜]=\nğ‘.\n2) Delete the node ND.\n3) Release the lock for NPand exit.",
  "textLength": 95479
}