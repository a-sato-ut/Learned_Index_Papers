{
  "paperId": "5333c7cd3590778eb4dee77f387082c8e7254b9e",
  "title": "LIDER: An Efficient High-dimensional Learned Index for Large-scale Dense Passage Retrieval",
  "pdfPath": "5333c7cd3590778eb4dee77f387082c8e7254b9e.pdf",
  "text": "LIDER: An Efficient High-dimensional Learned Index for\nLarge-scale Dense Passage Retrieval\nYifan Wang\nUniversity of Florida\nwangyifan@ufl.eduHaodi Ma\nUniversity of Florida\nma.haodi@ufl.eduDaisy Zhe Wang\nUniversity of Florida\ndaisyw@ufl.edu\nABSTRACT\nPassage retrieval has been studied for decades, and many recent ap-\nproaches of passage retrieval are using dense embeddings generated\nfrom deep neural models, called â€œdense passage retrievalâ€. The state-\nof-the-art end-to-end dense passage retrieval systems normally\ndeploy a deep neural model followed by an approximate nearest\nneighbor (ANN) search module. The model generates embeddings\nof the corpus and queries, which are then indexed and searched by\nthe high-performance ANN module. With the increasing data scale,\nthe ANN module unavoidably becomes the bottleneck on efficiency.\nAn alternative is the learned index, which achieves significantly\nhigh search efficiency by learning the data distribution and predict-\ning the target data location. But most of the existing learned indexes\nare designed for low dimensional data, which are not suitable for\ndense passage retrieval with high-dimensional dense embeddings.\nIn this paper, we propose LIDER , an efficient high-dimensional\nLearned Index for large-scale DEnse passage Retrieval. LIDER has\na clustering-based hierarchical architecture formed by two layers\nof core models. As the basic unit of LIDER to index and search data,\nacore model includes an adapted recursive model index (RMI) and\na dimension reduction component which consists of an extended\nSortingKeys-LSH (SK-LSH) and a key re-scaling module. The di-\nmension reduction component reduces the high-dimensional dense\nembeddings into one-dimensional keys and sorts them in a specific\norder, which are then used by the RMI to make fast prediction.\nExperiments show that LIDER has a higher search speed with high\nretrieval quality comparing to the state-of-the-art ANN indexes\non passage retrieval tasks, e.g., on large-scale data it achieves 1.2x\nsearch speed and significantly higher retrieval quality than the\nfastest baseline in our evaluation. Furthermore, LIDER has a better\ncapability of speed-quality trade-off.\nPVLDB Reference Format:\nYifan Wang, Haodi Ma, and Daisy Zhe Wang. LIDER: An Efficient\nHigh-dimensional Learned Index for Large-scale Dense Passage Retrieval.\nPVLDB, 14(1): XXX-XXX, 2020.\ndoi:XX.XX/XXX.XX\n1 INTRODUCTION\nAs one of the most important types of information retrieval, pas-\nsage retrieval finds and returns relevant passages to a given query.\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license. For any use beyond those covered by this license, obtain permission by\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 14, No. 1 ISSN 2150-8097.\ndoi:XX.XX/XXX.XXThe typical applications of passage retrieval include question an-\nswering (QA) systems [ 14] (which retrieve relevant answers to the\nquestions), dialogue response selection [ 19] (which selects proper\nresponses given a dialogue context and multiple response candi-\ndates) and search engines. A passage retrieval pipeline normally\nconsists of two stages, the first-stage retrieval and second-stage\nreranking, where the former retrieves a collection of candidate pas-\nsages and the latter reranks them by relevance to the query. In this\npaper we focus on the first-stage retrieval and simply denote it by\nretrieval . A typical implementation of the retrieval uses bag-of-word\nvectors to represent the query and passages, where each element\nin a vector stands for a term in the vocabulary and length of the\nvector equals the vocabulary size. So the representation is usually\nvery sparse and such an implementation is called sparse retrieval .\nIn recent years dense retrieval has emerged and shown its great\npotential in effective passage retrieval. This new implementation\nof passage retrieval uses dense neural embeddings to represent\nthe text, by which significant improvement has been made, due\nto the capability of neural embeddings on capturing semantic in-\nformation. One of the biggest problems in sparse retrieval is the\nvocabulary mismatch . When the query and passage use different\nterms (like synonyms) to express similar meanings, term-based\nsparse retrieval cannot match them. But in such a case the neural\nembeddings of the query and passage are likely to be still close to\neach other. Therefore more and more passage retrieval systems are\ndeploying dense retrieval today. Specifically, the state-of-the-art\ndense retrieval pipelines commonly consist of a deep neural model\n(which is normally a two-tower deep model, e.g., two-tower BERT)\nand an approximate nearest neighbor (ANN) index [ 14,15,21]. The\nembeddings of the corpus are generated by the model and indexed\nby the ANN module offline, then online the embeddings of queries\nare computed and their top-k nearest neighbors are retrieved by\nthe efficient ANN index from the corpus embeddings as results, by\nwhich the pipelines can support low-latency online serving.\nBut no matter how high-performance the ANN index is on spe-\ncific sizes of datasets, with the continuous data explosion today, it\nstill becomes the bottleneck of retrieval speed. Given a query, the\nretrieval time mainly consists of two parts, the query embedding\ngeneration time and ANN search time, where the former takes\n0.0007âˆ¼0.0008 seconds while the latter takes 0.04 âˆ¼more than 0.2\nseconds in our evaluation, i.e., the ANN search usually costs more\nthan 50x time of the query embedding generation.\nA potential high-performance alternative to the typical ANN\nindexes [ 7,10,11,20] is the learned index. Unlike traditional index\ntechniques, instead of â€œlooking upâ€ the location of a key, the learned\nindex â€œpredictsâ€ the location after learning the key-location distribu-\ntion of the dataset, which makes its search process highly efficient.\nAs the first work to propose the concept â€œlearned index structureâ€,arXiv:2205.00970v3  [cs.IR]  10 Oct 2022\n\n[17] introduces a learned index structure called Recursive-Model\nIndex (RMI) that outperforms traditional B-tree index on efficiency\nand memory usage for range queries on one-dimensional data.\nSpecifically, RMI is a hierarchical structure including multiple lay-\ners of machine learning models. Given data and their corresponding\nindexing keys, RMI assumes the data records are sorted as a dense\narray by the keys and learns the location distribution of the keys in\nthe array. Then for any given key, RMI can predict its location in\nthe array with a bounded error. In the case that prediction error is\nbeyond the required bound, a hybrid index mixing RMI and B-tree\nwill be built to reduce the overall error.\nSince RMI requires the data records to be stored in a one-dimensional\nsorted array, it cannot be directly applied to multi-dimensional data\nas such data has no natural order for sorting. Therefore, following\nstudies normally utilize dimension reduction methods to convert\nthe multi-dimensional data into sortable one-dimensional points\nfor RMI to fit, based on which several multi-dimensional learned\nindexes are proposed, e.g., ZM index [ 40], ML-Index [ 5], Flood [ 28],\netc. But most of them are still designed for very low-dimensional\ndata, like 2D and 3D spatial data or multi-dimensional data with\nonly tens of dimensions. On dense neural embeddings with hun-\ndreds to thousands of dimensions, their dimension reduction meth-\nods are no longer effective due to the curse of dimensionality, mak-\ning them not suitable for dense retrieval tasks.\nAn effective dimension reduction method for high-dimensional\ndata is locality-sensitive hashing (LSH). It is able to convert high-\ndimensional data into one-dimensional hash code string, which is\ncalled â€œhashkeyâ€ in this paper. But there is still a gap between LSH\nand learned index: RMI not only requires one-dimensional data, but\nalso requires the data can be sorted meaningfully. Unfortunately,\nthere is no explicit order for the hashkeys generated by LSH. This\nproblem can be solved by SortingKeys-LSH (SK-LSH) [ 23], which\ndefines a specialized order on the LSH hashkeys such that the data\npoints can be sorted by their hashkeys to form a continuous array\nwhere the positions meaningfully reflect the similarities between\nthe data points.\nHowever, to make the index system practical, we must solve one\nmore problem which we call â€œthe curse of space sizeâ€: when search\nspace (i.e., dataset size) becomes larger, both of SK-LSH and RMI\nperform worse. Specifically, SK-LSH needs a larger hashkey length\nand more sorted arrays to guarantee the effectiveness, leading to a\nsignificantly larger memory usage, and the capability of RMI to ac-\ncurately fit a dataset normally degrades with the increasing dataset\nsize. Our solution to this problem is the clustering. By clustering\nthe whole dataset into smaller groups and building index inside\neach group, the search space of each index is shrunk effectively.\nFurthermore, this solution can benefit from parallelization as the\nclusters are independent from each other.\nIn this paper we propose LIDER , a novel high-dimensional\nLearned Index for efficient large-scale DEnse passage Retrieval.\nLIDER has a clustering-based two-layer architecture that consists\nof multiple core models. As the basic unit in LIDER, a core model is\na module combining modified RMI and adapted SK-LSH together.\nSpecifically, a core model mainly consists of two components, a\ncouple of simplified RMIs and an extended SK-LSH module (called\nESK-LSH). The ESK-LSH module works on reducing data dimen-\nsion, i.e., converting the high-dimensional dense embeddings toone-dimensional hashkeys and sorting them to form several sorted\narrays by a specialized order, while the RMIs learn the hashkey-\nlocation distributions in those sorted arrays. During ANN search, in\na core model, ESK-LSH converts the query embedding into hashkeys\nand feeds them to RMIs to predict the locations, starting from where\na range search on the sorted arrays will finally retrieve the approx-\nimate nearest neighbors. To have the two major components better\ninteract with each other and better fit the demands of dense re-\ntrieval, we make several critical adaptions and improvements. The\ntechnical details are included in Section 3, 4 and 5.\nThe two-layer structure of LIDER is built as follows: first the\ntarget dataset is clustered, then one core model is created over the\ncluster centroids, called â€œcentroids retrieverâ€ and forming the first\nlayer. And one core model per cluster is created to index the data\ninside that cluster, called â€œin-cluster retrieverâ€. All these in-cluster\nretrievers fill the second layer. During the retrieval process, the\ncentroids retriever is first called to find the target clusters (i.e., the\nclusters possibly including correct answers to the query) and then\nthe in-cluster retrievers in those targets will retrieve the final results.\nIn such a way, each single core model works on a smaller subspace,\nwhich effectively tackles â€œthe curse of space sizeâ€. And the retrieval\nin this architecture is simple to be parallelized between clusters\nsince the in-cluster retrievers work independently from each other.\nWe conduct evaluation based on the passage retrieval scenario,\nwhich shows that LIDER is highly effective with higher efficiency\nthan the state-of-the-art ANN indexes on large-scale dense retrieval.\nComparing to the fastest baseline method, LIDER achieves signifi-\ncantly higher retrieval quality with 1.2x search speed on the largest\nevaluation dataset. To our best knowledge, LIDER is one of the first\nlearned index structures for ANN queries on high-dimensional data.\nAnd LIDER is the first implementation of such a learned index to\nsolve dense retrieval tasks.\nThe main contributions of this paper are shown below:\n(1)We build LIDER, an efficient and effective high-dimensional\nlearned index for ANN search in dense retrieval. This is one\nof the first learned indexes for ANN search on very high-\ndimensional data, and one of the first dense retrieval solutions\nutilizing learned index.\n(2)We design a clustering-based two-layer hierarchical architec-\nture to address the performance issues raised by large search\nspace and optimize the retrieval efficiency by parallelization\nbetween clusters.\n(3)We extend SK-LSH to more distance metrics, improve its par-\nallelism and hashkey sorting method, design an effective re-\nscaling method on the hashkeys to better train RMI, and sim-\nplify RMI for better predicting efficiency and effectiveness.\n(4)We conduct experiments based on commonly used passage\nretrieval benchmarks to evaluate the performance of LIDER,\nwhich shows LIDER outperforms all baseline methods on both\nefficiency and effectiveness, especially for large-scale data.\nThis paper is organized as follows: Section 2 introduces works re-\nlated to LIDER. Section 3 introduces the architecture and workflow\nof LIDER. The following Sections 4 and 5 present technical details\nfor the major components. Section 6 provides the time complexity\nanalysis. Finally Section 7 presents experiments to evaluate the\n2\n\nperformance of LIDER, the effect of some important factors and\nparameters, and the memory footprint and index construction cost.\n2 RELATED WORK\nThere are two major categories of retrieval methods, sparse and\ndense retrieval. Sparse retrieval normally uses bag-of-word (BOW)\nmodels, where the document representations are sparse vectors,\nwhile dense retrieval mostly utilizes neural embeddings from deep\nneural models which are dense vectors. Typical sparse retrieval\nmethods include BM25, DeepCT [ 4], Doc2query [ 31] and docTTTT-\nTquery [ 30], which are commonly used in recent retrieval studies\nas strong baselines. Due to the power of dense neural embeddings\nin semantic search, many state-of-the-art retrieval researches focus\non dense retrieval. After BERT [ 6] was proposed, most of the recent\ndense retrieval models are designed based on it and achieve signifi-\ncant improvement on retrieval quality, e.g., Sentence-BERT [ 34] and\nMarkedBERT [ 2]. But as a heavy model, BERT has high inference\nlatency, which limits its application on online retrieval that re-\nquires low-latency serving. To solve this problem, following works\nhave proposed several variants to reduce its complexity, including\nDistilBERT [36], ColBERT [15], TCT-ColBERT [22], etc.\nTo better support the low-latency online retrieval, in addition to\ndeploying more lightweight neural models, most state-of-the-art\nend-to-end dense retrieval systems also arrange a high-performance\nANN search module following the neural model to fast look up the\nclosest documents to the queries based on their embeddings. ANN\nindexes include four major categories, i.e., hashing, graph, quantiza-\ntion and tree based indexes. Among them the tree based indexes are\nmore suitable to low-dimensional space, so dense retrieval systems\nnormally choose from the other three types of indexes. For exam-\nple, DPR [ 14] utilizes a graph based index, HNSW [ 26], ColBERT\ndeploys IVFADC index which is based on product quantization,\nBPR [ 43] integrates learning-to-hash technique into DPR [ 14], etc.\nFAISS [ 13] is one of the most popular ANN index libraries in todayâ€™s\ndense retrieval, as it implements high-performance indexes of all\nthe three classes. It has been used by many recent dense retrieval\nstudies [ 25,33,38,41,44]. There are also many other representative\nANN indexes [7, 10, 11, 20].\nLearned index is first proposed by [ 17], where the structure is\ncalled Recursive-model index (RMI). RMI is designed to replace the\ntraditional range search indexes on one-dimensional data. To han-\ndle multi-dimensional data, following works propose many multi-\ndimensional learned index structures based on RMI. Their main dif-\nferences are on the dimension reduction methods used to reduce the\nmulti-dimensional data into one-dimension. SageDB [ 16] uses a di-\nmension reduction method with some similarities to LSH (which is\nnot explicitly discussed in the paper). ZM-index [ 40], ML-index [ 5]\nand Flood [ 28] reduce data dimension by Z-order, iDistance, and\na multi-dimensional grid based method respectively. [ 32] applies\nthe idea of Flood to several traditional multi-dimensional indexes\nto make them better handle spatial queries. [ 39] proposes LIMS, a\nlearned index for efficient similarity search in metric spaces. LIMS\napplies a clustering-based strategy to split the dataset into more\nuniformly distributed subsets to improve the search performance.\nThough LIDER also has a clustering-based architecture, it is quite\ndifferent from LIMS: (1) LIMS is an exact similarity search indexwhile LIDER is for approximate similarity search, (2) LIMS is a\ndisk-based index while LIDER is in-memory index, and (3) the data\ndimension of LIMS is not very high, i.e., less than 100 in their exper-\niments, which is significantly lower than that of LIDER. To our best\nknowledge, all these existing learned indexes mentioned above are\ndesigned for relatively low-dimensional data and there is no learned\nindex for very high-dimensional data like neural embeddings.\n3 LIDER\nIn this section we introduce the overall architecture and workflow\nof LIDER and each core model in it. Then in the following sections\nwe discuss important technical details about the major components\nin LIDER.\nLinear\nRegressionLinear\nRegressionLinear\nRegressionLinear\nRegressionLinear\nRegressionRMI\nk1 k2 k3 ...Key re-scaling Hashing\nSortingHashkeysESK-LSH\nHashkeysDense\nembeddings\nFigure 1: Core model structure\n3.1 Core model structure\nThe structure of core model is illustrated in Figure 1. There are two\nmajor modules in the core model, the dimension reduction module\n(illustrated as the pink and green boxes) and the location prediction\nmodule (i.e., the RMI, marked as the blue box). The dimension\nreduction module includes two components, the extended SK-LSH\n(ESK-LSH), which is extended from the original SK-LSH, and the Key\nre-scaling component, which is used to convert the string hashkeys\ninto numeric keys within a proper range to train the RMIs. The\nyellow boxes stand for one of the sorted arrays generated by ESK-\nLSH. Essentially, one RMI corresponds to one sorted array. And\nESK-LSH may maintain multiple sorted arrays, in which case there\nwill be multiple RMIs existing in a core model. More details are\npresented in Section 4 and 5.\n3.2 Clustering-based architecture\nAs shown in Figure 2, LIDER has a clustering-based two-layer\narchitecture. The first layer (the higher grey box, marked as â€œLayer-\n1â€) includes one core model, named centroids retriever , to index all\n3\n\nIn-cluster\nretrieverCentroids\nretriever\nCentroids ... C3 C1 ... C2\nClustersLIDERQueries\nLayer-1\nIn-cluster\nretrieverIn-cluster\nretrieverLayer-2Figure 2: LIDER architecture (the centroids retriever and\neach in-cluster retriever are both a core model as shown in\nFigure 1)\nthe cluster centroids. And the second layer (the lower grey box,\nmarked as â€œLayer-2â€) consists of multiple core models, one inside\neach data cluster, named in-cluster retriever . Each in-cluster retriever\nindexes the data within the corresponding cluster.\nComparing to building only one core model to index the whole\ndataset, such a clustering-based layered architecture allows each\ncore model to fit a significantly smaller search space, e.g., roughly\nless than 1/1000 of the whole dataset since we normally set the\nnumber of clusters to be more than 1000. A smaller dataset is better\nfor RMI to fit and predict more accurately, and allows ESK-LSH\nto reduce the memory usage and search latency by shrinking the\nhashkey length and number of hashkey arrays while guaranteeing\nthe effectiveness. In our implementation, the clusters are generated\nsimply by k-means clustering. So unlike the components in core\nmodel, this paper does not have a specific section to show more\ndetails about the clustering strategy. All important details for it\nhave been included in this section and Section 3.3.\n3.3 Workflow\nIn this section, we introduce the workflow of building LIDER and\nquery processing in LIDER, from perspectives of single core model\nand the whole system respectively.\n3.3.1 Workflow in each core model. For a single core model, to\nmake it simple, here we introduce the indexing and querying work-\nflow in the case that only one sorted array and one RMI are main-\ntained, as it is in Figure 1. When a core model is indexing the\ncorpus of documents, document embeddings generated by the up-\nstream deep neural model are first input to ESK-LSH to generate\ntheir hashkeys. Then the hashkeys are sorted to form the sorted\nhashkey array, and also passed to the key re-scaling component to\nbe converted into numeric keys. For each of these numeric keys, its\nlocation is that of the original hashkey in the sorted array. These\nkey-location pairs are then used to train the RMI, where the key is\ndata and location is label. RMI will learn to predict the location ofa given key by the training. When querying ğ‘˜nearest neighbors\nwith the core model, the query embedding is first input to ESK-LSH\nto generate the query hashkey, then query hashkey is converted\ninto numeric query key, which will be passed to RMI to predict\nits location. After the prediction, ESK-LSH will do a bi-directional\nrange search (with a pre-determined range width purely depending\nonğ‘˜) starting from the predicted location on the sorted array to\nretrieve a pre-determined number of candidate hashkeys (where\nthe number is normally several times of ğ‘˜). Finally the original\ndocument embeddings corresponding to the candidate hashkeys\nwill be scored (e.g., computing cosine similarity to the query em-\nbedding) and the top- ğ‘˜of them will be returned. In the case that\nmultiple arrays and RMIs are used, the core model will simply do a\nparallel execution of such a process on each array and RMI then\nmerge the results.\n3.3.2 Overall workflow in LIDER. LIDER is built as follows: first\nthe whole dataset is clustered into several subsets by k-means clus-\ntering algorithm, then a core model is created on the collection of\nthe cluster centroids, and within each data cluster, one core model\nis also created to index that subset. The core model indexing the\ncentroids is the centroids retriever while each core model inside\na cluster is an in-cluster retriever. When using LIDER for ANN\nqueries, the query embedding is first passed into the centroids re-\ntriever to find the approximate nearest centroids to it, then the\nquery will be searched by the in-cluster retrievers in the clusters\ncorresponding to those centroids. This in-cluster retrieval process\nis parallelized between clusters, since the in-cluster retrievers work\nindependently from each other, therefore LIDER makes them exe-\ncute the retrieval simultaneously to improve the efficiency. Finally\nthe results from in-cluster retrievers will be merged and the top- ğ‘˜of\nthem by some score (like cosine similarity to the query embedding)\nwill be returned.\n4 EXTENDED SK-LSH\nLocality-sensitive hashing (LSH) is an effective hashing framework\nfor approximate similarity search on high-dimensional data. It gen-\nerates a signature (called â€œhashkeyâ€) for each data vector by hashing\nit with several hashing functions â„1(Â·),...,â„ğ‘š(Â·)and concatenating\nthe resulting hashing values. Then the similarity between two data\nvectors can be approximated using their hashkeys. The specialized\nhashing function â„ğ‘–(Â·)is so called LSH function while the sequence\nof themğº(Â·)=(â„1(Â·),...,â„ğ‘š(Â·))is normally called compound LSH\nfunction . LSH function is specially designed to achieve both of\nrandomization and locality-preserving. Its formal definition is:\nDefinition 4.1 (LSH function). Given a metric space and its dis-\ntance metric ğ‘‘, a threshold ğ‘¡>0, and any two data points Â®ğ‘¢andÂ®ğ‘£in\nthe metric space, an LSH function â„(Â·)should satisfy the following\ntwo conditions:\n(1) ifğ‘‘(Â®ğ‘¢,Â®ğ‘£)â‰¤ğ‘¡, thenâ„(Â®ğ‘¢)=â„(Â®ğ‘£)with a probability at least ğ‘1\n(2) ifğ‘‘(Â®ğ‘¢,Â®ğ‘£)â‰¥ğ‘ğ‘¡, thenâ„(Â®ğ‘¢)=â„(Â®ğ‘£)with probability at most ğ‘2\nwhereğ‘>1is an approximation factor, and the probabilities\nğ‘1>ğ‘2.\nThe definition means an LSH function maps similar/close data\npoints into the same hash bucket with a higher probability than\nmapping dissimilar points into the same bucket. As a result, the\n4\n\ndata points within the same bucket are very likely to be similar to\neach other. By using the compound LSH function, the false negative\nrate can be further reduced to increase the recall.\nBased on the essential LSH model, SK-LSH [ 23] defines a linear\norder over the hashkeys to sort them such that the data points with\nsmaller Euclidean distance are placed closer on the sorted hashkey\narray. To find nearest neighbors of the query vector, it is sufficient to\nfind data vectors whose hashkeys are close to the query hashkey on\nthe array, and it is easy for SK-LSH to expand the nearest neighbor\nsearch by scanning farther locations from the initial hashkey (i.e.,\nthe closest hashkey to that of the query) towards both of its left\nand right sides along the array, called â€œbi-directional expansionâ€,\nwhich is basically a fixed length range search on the array, where\nthe range length is pre-determined by ğ‘˜(normally several times\nofğ‘˜). To let SK-LSH better fit dense retrieval scenario and have a\nhigher performance, we make several extensions and improvement\non it.\n4.1 Extension on similarity metrics\nMany of the state-of-the-art embedding based research and applica-\ntions [ 6,18,27,37] rely on cosine similarity to accurately measure\nthe embedding similarities. For dense retrieval, in addition to co-\nsine similarity [ 14,15,35], inner product is another commonly used\nsimilarity metric [ 21,38,42]. However, SK-LSH is designed for Eu-\nclidean distance. Therefore, we extend SK-LSH to also work with\ncosine similarity and name the extended algorithm as ESK-LSH.\nWe do not make a specific extension for inner product since it is\nequivalent to cosine similarity on normalized vectors.\nSK-LSH requires a base LSH model to perform the hashing. To\nsupport cosine similarity, we use the hyperplane-based random\nprojection LSH [ 3] model. Since [ 23] only guarantees the proper-\nties of SK-LSH for Euclidean distance and the corresponding LSH\nmodel, it is necessary to show those properties still hold for cosine\nsimilarity and hyperplane-based random projection LSH model.\nFor any two vectors Â®ğ‘1,Â®ğ‘2âˆˆğ‘…ğ‘‘, we defineğ‘ ğ‘–ğ‘š(Â®ğ‘1,Â®ğ‘2)as their\ncosine similarity:\nğ‘ ğ‘–ğ‘š(Â®ğ‘1,Â®ğ‘2):=cos(ğœƒ1,2)=Â®ğ‘1Â·Â®ğ‘2\r\rÂ®ğ‘1\r\rÂ·\r\rÂ®ğ‘2\r\r(1)\nWe also know from [ 3] that, the probability of generating identi-\ncal hash values for two vectors by the LSH model is\nğ‘(ğœƒ1,2):=ğ‘ƒ[â„(Â®ğ‘1)=â„(Â®ğ‘2)]=1âˆ’ğœƒ1,2\nğœ‹(2)\nFollowing [ 23], we useğº(Â·)=(â„1(Â·),...,â„ğ‘š(Â·))to denote a com-\npound LSH function, where â„ğ‘–:ğ‘…ğ‘‘â†’{ 0,1}are randomly se-\nlected hash functions defined in [ 3].KL(ğ¾1,ğ¾2)is the non-prefix\nlength [23] betweenğ¾1=ğº(Â®ğ‘1)andğ¾2=ğº(Â®ğ‘2).dist(ğ¾1,ğ¾2)is the\ndistance between ğ¾1andğ¾2. Please refer to Equation 6 of [ 23] for the\ncomplete definition of dist(ğ¾1,ğ¾2), and Equation 4 for KL(ğ¾1,ğ¾2).\nLemma 4.2. For any two arbitrary random vectors Â®ğ‘1,Â®ğ‘2âˆˆğ‘…ğ‘‘\nwith angleğœƒ1,2, the probability of dist(ğº(Â®ğ‘1),ğº(Â®ğ‘2))being less than\nğ‘šâˆ’ğ‘™+1(âˆ€ğ‘™: 0â‰¤ğ‘™â‰¤ğ‘š)is[ğ‘(ğœƒ1,2)]ğ‘™.\nProof. According to Definition 5 in [23],\ndist(ğº(Â®ğ‘1),ğº(Â®ğ‘2))<ğ‘šâˆ’ğ‘™+1â‡”KL(ğº(Â®ğ‘1),ğº(Â®ğ‘2))â‰¤ğ‘šâˆ’ğ‘™LetKL(ğº(Â®ğ‘1),ğº(Â®ğ‘2))=ğ‘šâˆ’ğ¿, then we have ğ‘šâˆ’ğ¿â‰¤ğ‘šâˆ’ğ‘™, then\nğ‘™â‰¤ğ¿, also trivially ğ¿â‰¤ğ‘š. Thus, by definition of ğ¾ğ¿,ğº(Â®ğ‘1)and\nğº(Â®ğ‘2)share common prefix whose length is ğ¿. This implies that\nâ„ğ‘–(Â®ğ‘1)=â„ğ‘–(Â®ğ‘2)holds for 1â‰¤ğ‘–â‰¤ğ¿. Due to the fact that each hash\nfunctionâ„ğ‘–is independently and randomly selected, the desired\nprobability can be computed as follows:\nğ‘ƒ[dist(ğº(Â®ğ‘1),ğº(Â®ğ‘2))<ğ‘šâˆ’ğ‘™+1]=ğ‘ƒ[KL(ğº(Â®ğ‘1),ğº(Â®ğ‘2))â‰¤ğ‘šâˆ’ğ‘™]\n=ğ‘šâˆ‘ï¸\nğ¿=ğ‘™ğ‘ƒ[KL(ğº(Â®ğ‘1),ğº(Â®ğ‘2))=ğ‘šâˆ’ğ¿]\n=ğ‘šâˆ’1âˆ‘ï¸\nğ¿=ğ‘™ ğ¿Ã–\nğ‘–=1ğ‘ƒ[â„ğ‘–(Â®ğ‘1)=â„ğ‘–(Â®ğ‘2)](1âˆ’ğ‘ƒ[â„ğ¿+1(Â®ğ‘1)=â„ğ¿+1(Â®ğ‘2)])!\n+ğ‘šÃ–\nğ‘–=1ğ‘ƒ[â„ğ‘–(Â®ğ‘1)=â„ğ‘–(Â®ğ‘2)]\n=ğ‘šâˆ’1âˆ‘ï¸\nğ¿=ğ‘™ğ‘(ğœƒ1,2)ğ¿(1âˆ’ğ‘(ğœƒ1,2))+ğ‘(ğœƒ1,2)ğ‘š=ğ‘(ğœƒ1,2)ğ‘™(3)\nâ–¡\nAccording to Equation 3, for any given ğ‘™, the probability of two\nhashkeysğº(Â®ğ‘1)andğº(Â®ğ‘2)being close monotonically increases\nwhen the cosine similarity ğ‘ ğ‘–ğ‘š(Â®ğ‘1,Â®ğ‘2)increases (i.e., ğœƒ1,2decreases).\nTherefore, distance of compound hashkeys is a reasonable metric\nto estimate cosine similarity between the original vectors. In such\ncontext, to find similar data points to the query, searching a small\nvicinity of the query hashkey is possibly enough.\n4.2 Extension on hashkey distance\nThough we prove that by using a random projection LSH model as\nthe base model, SK-LSH works for cosine similarity, there is a new\nproblem which does not exist in the original SK-LSH: when using\ncosine similarity, the hashkey distance defined in [ 23] is too coarse\nto distinguish the actual similarities of many different document\nembeddings to the query embedding. We name such a problem as\nâ€œlow resolution problemâ€. In [ 23], the hashkey distance is defined as\nğ‘‘ğ‘–ğ‘ ğ‘¡(ğ¾1,ğ¾2)=ğ¾ğ¿(ğ¾1,ğ¾2)+ğ¾ğ·(ğ¾1,ğ¾2)\nğ¶(4)\nwhereğ¾1,ğ¾2are two hashkeys, ğ¾ğ¿(ğ¾1,ğ¾2)is the non-prefix length\n[23] betweenğ¾1andğ¾2(i.e., the length of the sub-sequence after\ntheir common prefix), and ğ¾ğ·(ğ¾1,ğ¾2)is the(ğ‘™+1)-thelement\ndistance betweenğ¾1andğ¾2(i.e, the absolute difference of the first\nnon-identical elements between the two hashkeys), while ğ¶is a\nconstant factor that is set to be larger than the maximum of ğ¾ğ·\nsuch thatğ¾ğ·(ğ¾1,ğ¾2)\nğ¶<1always holds. When the similarity metric\nis Euclidean distance, each element in a hashkey is an integer within\na wide bounded range, which means the range of ğ¾ğ·is also wide.\nBut when the similarity metric is cosine similarity, each hashing\nvalue must be either 0 or 1, in which case ğ¾ğ·(ğ¾1,ğ¾2)â‰¡1,âˆ€ğ¾1,ğ¾2.\nTherefore, no matter how different ğ¾1andğ¾2are after the common\nprefix,ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ¾1,ğ¾2)cannot reflect it.\nFor example, supposing we use hashkeys of length 6. Given a\nquery embedding Â®ğ‘£ğ‘with hashkey ğ¾ğ‘=000000 , one document\nembeddingÂ®ğ‘£1with hashkey ğ¾1=111111 and anotherÂ®ğ‘£2with\nğ¾2=100000 , according to Equation 4, we have ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ¾ğ‘,ğ¾1)=\n5\n\nğ‘‘ğ‘–ğ‘ ğ‘¡(ğ¾ğ‘,ğ¾2)=6+1\nğ¶. In such case, relying on the hashkey distance,\nÂ®ğ‘£1andÂ®ğ‘£2are the same in terms of similarity to Â®ğ‘£ğ‘. But obviously\nÂ®ğ‘£2is probably more similar to Â®ğ‘£ğ‘thanÂ®ğ‘£1toÂ®ğ‘£ğ‘, since the Hamming\ndistance between ğ¾2andğ¾ğ‘is much smaller than that between\nğ¾1andğ¾ğ‘. Therefore the original hashkey distance is not able to\ndistinguish between Â®ğ‘£1andÂ®ğ‘£2in such situations, or in another word,\nit has a low resolution under the settings of cosine similarity metric\nand random projection LSH.\nTo solve this problem, an intuitive way is replacing ğ¾ğ·with\nHamming distance. But unfortunately, this will cause the linear\norder of hashkeys to no longer hold. Specifically, the linear order\ndefined by SK-LSH is based on element-wise comparison from\nthe most significant element to the least significant element of\nthe hashkeys. When using random projection LSH (where each\nelement is either 0 or 1), the order is actually a dictionary order\n(a.k.a., lexicographic order). By sorting hashkeys in this order, SK-\nLSH guarantees that for any three ordered hashkeys in one array, ğ¾2,\nğ¾1,ğ¾,ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ¾2,ğ¾)â‰¥ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ¾1,ğ¾)always holds, which is essential to\nthe bi-directional expansion search. However, Hamming distance\ndoes not consider the element significance, instead, it treats all the\nelements equally. This may cause that for some ordered ğ¾2,ğ¾1,ğ¾,\nğ‘‘ğ‘–ğ‘ ğ‘¡(ğ¾2,ğ¾)<ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ¾1,ğ¾), which breaks the theoretical foundation\nof SK-LSH. And we do observe many occurrences of this issue in\nour experiments.\nWe tackle the low resolution problem successfully by extending\nthe length of sub-sequence used by ğ¾ğ·. In [23],ğ¾ğ·is computed as\nğ¾ğ·(ğ¾1,ğ¾2)=|ğ‘˜1,ğ‘™+1âˆ’ğ‘˜2,ğ‘™+1| (5)\nwhereğ‘™is the length of the common prefix between hashkeys ğ¾1\nandğ¾2, andğ‘˜1,ğ‘™+1,ğ‘˜2,ğ‘™+1stand for the first non-identical elements\nbetweenğ¾1andğ¾2. We extend it to\nğ¾ğ·ğ‘’(ğ¾1,ğ¾2)=|ğ·ğ‘’ğ‘ğ‘–ğ‘šğ‘ğ‘™(ğ¾1,ğ‘™+1:ğ‘™+1+ğµ)âˆ’ğ·ğ‘’ğ‘ğ‘–ğ‘šğ‘ğ‘™(ğ¾2,ğ‘™+1:ğ‘™+1+ğµ)|\n(6)\nwhereğ·ğ‘’ğ‘ğ‘–ğ‘šğ‘ğ‘™(Â·)is the operation to convert a binary hashkey\n(which only includes 0 and 1 and can be seen as a binary number)\ninto a decimal integer number, while ğ¾1,ğ‘™+1:ğ‘™+1+ğµstands for the\nsub-sequence of length ğµstarting from the(ğ‘™+1)-th element in ğ¾1,\ni.e., right after the common prefix, and the same for ğ¾2,ğ‘™+1:ğ‘™+1+ğµ.\nThen we have a new definition of hashkey distance\nğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘’(ğ¾1,ğ¾2)=ğ¾ğ¿(ğ¾1,ğ¾2)+ğ¾ğ·ğ‘’(ğ¾1,ğ¾2)\nğ¶(7)\nhereğ¾ğ¿keeps original, and we set ğ¶=2ğµto make sureğ¾ğ·ğ‘’(ğ¾1,ğ¾2)\nğ¶<\n1still holds. Comparing to the original ğ¾ğ·which is always 1, ğ¾ğ·ğ‘’\nhas more possible values ranging from 0 to 2ğµâˆ’1, thus it can\nbetter distinguish between different hashkeys. Using ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘’, if we\nsetğµ=3, the example above becomes ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘’(ğ¾ğ‘,ğ¾1)=6+7\nğ¶,\nğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘’(ğ¾ğ‘,ğ¾2)=6+4\nğ¶, successfully reflecting that ğ¾2is more simi-\nlar toğ¾ğ‘. In addition, ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘’does not change the conclusion from\nEquation 3, as ğ¾ğ¿is the same andğ¾ğ·ğ‘’\nğ¶is still less than 1. Further-\nmore, unlike Hamming distance, the linear order still holds given\nğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘’. Formally, it can be stated as two lemmas\nLemma 4.3. In a hashkey array sorted by the original SK-LSH\nlinear order, for any three ordered hashkeys ğ¾2,ğ¾1,ğ¾,ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘’(ğ¾2,ğ¾)â‰¥\nğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘’(ğ¾1,ğ¾).Lemma 4.4. In a hashkey array sorted by the original SK-LSH\nlinear order, for any three ordered hashkeys ğ¾,ğ¾1,ğ¾2,ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘’(ğ¾2,ğ¾)â‰¥\nğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘’(ğ¾1,ğ¾).\nLemma 4.3 and 4.4 are straightforward to be proven following\nsimilar steps to the proof of Lemma 4 and 5 in [ 23], so we just skip\nthe proof in this paper. In conclusion, by extending ğ‘‘ğ‘–ğ‘ ğ‘¡andğ¾ğ·,\nwe effectively improve the resolution of the hashkey distance.\n4.3 Improvement on parallelism\nIn addition to extending the similarity metrics and hashkey dis-\ntance, ESK-LSH also achieves a higher efficiency than SK-LSH by\nimproving its parallelism. In the original SK-LSH, though it may\nmaintain multiple sorted arrays, the expansion search is not parallel\nbut iterative on the arrays. Specifically, it iteratively looks for the\nnext globally closest hashkey to the query hashkey across all the\narrays, which limits its recall within a specific period of search\ntime. In order to efficiently retrieve more candidate hashkeys, we\nincrease the parallelism by making each array independent from\nothers, i.e., ESK-LSH does not look for the globally closest hashkey,\nbut the next closest hashkey locally on each array. Therefore ESK-\nLSH can do the expansion search parallelly on each array. Such\nan improvement makes it possible to use more sorted arrays to in-\ncrease retrieval quality with only tiny time overhead (e.g., the cost\nof operating system to manage more threads), as long as the hard-\nware resources are sufficient, e.g., enough number of CPU cores.\nThough in practice the hardware resources might be insufficient\nsuch that the parallelism may be limited, we show in the evaluation\nthat a low-end to middle-end machine (with less than 30 cores) is\nenough for LIDER to outperform the baselines.\n5 RMI AND KEY RE-SCALING\nAs the first learned index for range query, recursive-model index\n(RMI) [ 17] works on finding out the locations of given keys in some\ngiven ordered array, just like what a B-tree index does. But unlike\ntraditional indexes that lookup the locations, RMI â€œpredictsâ€ the\nlocations. Specifically, as illustrated in Figure 1, RMI is a hierarchical\nstructure consists of several layers of simple machine learning\nmodels, e.g., shallow neural network or linear regression model.\nWithin one layer, the whole search space (i.e., all locations in the\nordered array) is partitioned by all the models in this layer, where\neach model takes responsibility of a subspace. The key will be first\ninput to the root model. The root model predicts its location, and\npasses the key to one of the next layer models which corresponds to\nthe subspace containing the location. Then such a predict-and-pass\nprocess is repeated layer by layer until one of the final layer models\naccepts the key and makes the final prediction of the location as the\noutput of RMI. In this process the prediction is gradually refined and\nfinally the error is minimized. Comparing to traditional indexes like\nB-tree, RMI has a nearly constant search time with the dataset size.\nThis is because RMI can handle larger dataset by only increasing\nthe width (i.e., the number of models in each layer) while fixing the\ndepth (i.e., the number of layers), and the prediction time mainly\ndepends on the depth. Specifically, placing more models in one\nlayer will reduce the size of each subspace, such that each model\ncan better fit the smaller subspace. So increasing the width of RMI\n6\n\nis often an effective way to improve search quality with only tiny\ngrowth on search time.\n5.1 Key re-scaling\nIn order to let RMI accept the hashkeys generated by ESK-LSH,\nwe need to convert each hashkey to a numeric value, so called\nâ€œRMI keyâ€ in this paper. We design a key re-scaling module for this\nconversion. In the conversion, the binary hashkey is first seen as\na binary number and converted into a decimal integer number,\nthen the decimal integer number is further re-scaled to a floating-\npoint number in the range [0,ğ¿ğ‘ğ‘Ÿğ‘Ÿğ‘ğ‘¦âˆ’1], whereğ¿ğ‘ğ‘Ÿğ‘Ÿğ‘ğ‘¦ is the\nlength of each sorted hashkey array in ESK-LSH. The re-scaling\nis the most critical step in the conversion, which has a significant\neffect on the learning of RMI. This is because on large-scale dataset,\nthe length of hashkey is normally long to have a large enough\ncapacity for encoding the data embeddings. Thus after the first-\nstep conversion, the decimal integer numbers are mostly very big.\nHowever, comparing to the big integer RMI keys at this step, the\nlabels, i.e., the locations, are much smaller. So most predictions of\nRMI will be out of range (i.e., much less than 0 or much larger than\nğ¿ğ‘ğ‘Ÿğ‘Ÿğ‘ğ‘¦âˆ’1), making RMI ineffective.\nTo solve this issue, we use min-max normalization as the second-\nstep re-scaling method. Formally, the normalization is defined as\nğ‘¥ğ‘›ğ‘œğ‘Ÿğ‘š =ğ‘¥âˆ’ğ‘¥ğ‘šğ‘–ğ‘›\nğ‘¥ğ‘šğ‘ğ‘¥âˆ’ğ‘¥ğ‘šğ‘–ğ‘›(ğ‘âˆ’ğ‘)+ğ‘ (8)\nwhereğ‘¥is the original integer key, ğ‘¥ğ‘šğ‘–ğ‘›andğ‘¥ğ‘šğ‘ğ‘¥ are the minimum\nand maximum of ğ‘¥,ğ‘¥ğ‘›ğ‘œğ‘Ÿğ‘š is the key after normalization, and [ğ‘,ğ‘]\nis the range of ğ‘¥ğ‘›ğ‘œğ‘Ÿğ‘š . In LIDER, we set ğ‘=0,ğ‘=ğ¿ğ‘ğ‘Ÿğ‘Ÿğ‘ğ‘¦âˆ’1. The\neffect of such normalization is evaluated in Section 7.4.\nThere might be another issue of duplicate RMI keys. Because\nthere is a possibility (which is very low) that LSH generates the same\nhashkey for different embeddings, duplicate RMI keys might exist.\nAnd due to the float-point number precision, after normalization,\nsome originally different RMI keys may also become duplicate. In\nsuch cases, duplicate keys still correspond to different locations,\ni.e., the same training data points have different training labels,\nwhich may lower the training quality of RMI. But fortunately, these\nduplicate keys are adjacent in each sorted array, meaning the error\ncaused by such an issue is bounded in a local range. As long as the\nlocal range is small enough, this issue will not affect RMI too much.\nFor instance, we can set the length of hashkey to be large enough\nto reduce the number of duplicate hashkeys.\n5.2 Simplified RMI\nThe original RMI in [17] uses two layers of models, where the top\nlevel includes one neural network and the second level includes\nseveral linear regression models. We also use a similar two-layer\nstructure, but based on the re-scaled RMI keys, we simplify RMI\nto only include linear regression models since the RMI keys and\ntheir labels/locations are almost linearly distributed, as illustrated\nin Figure 3. Thus a linear regression on the top can fit better than a\nneural network, which can also reduce the prediction time. Further-\nmore, we do not deploy the hybrid index strategy (as introduced\nin Section 1) in the RMI for strict error bounding, due to the re-\nquirement on high efficiency and the fact that prediction error hasbeen reduced significantly by the key re-scaling module (which is\nfurther discussed in Section 7.4).\nFigure 3: RMI training data distribution (MS-500k dataset as\nexample)\n6 TIME COMPLEXITY\nWe derive the search time complexity of LIDER in this section. Ta-\nble 1 lists major notations used in this and all the following sections.\nThe search process of LIDER includes three stages, the centroids\nretrieval, in-cluster retrieval and the final verification, where the\nfirst is simply a standard search process of a single core model, the\nsecond is a parallel search with multiple core models (whose time\ncomplexity is the same as single core model theoretically), and the\nthird is a simple top- ğ‘˜selection process.\nNotation Description\nğ‘˜ğ‘š,ğ‘˜ Number of output points by a core model, and that by LIDER\nğ»,ğ‘€ Number of hashkey arrays, and hashkey length in ESK-LSH\nğ‘…,ğ‘Ÿ0 Length of ESK-LSH expansion range on each hashkey array,\nand a user-specific factor ( ğ‘…=ğ‘Ÿ0ğ‘˜ğ‘š)\nğ‘,ğ‘0 Total number of clusters, and number of retrieved centroids\nby the centroids retriever\nğ‘ Dataset size\nTable 1: List of notations used in this and following sections\n6.1 Time complexity of a single core model\nThe search in a single core model includes five steps: (1) query\nhashkey generation, (2) query hashkey rescaling, (3) RMI prediciton,\n(4) ESK-LSH expansion and (5) candidate verification, where the\nstep 2 and 3 are constant with the data scale. So we analyze the\ntime complexity for the rest three steps as follows: In query hashkey\ngeneration step, ESK-LSH generates one query hashkey per array,\nwhere each needs ğ‘€hashings, costing ğ‘‚(ğ»ğ‘€)time. As a linear\nscanning on range ğ‘…, complexity of ESK-LSH expansion step is\nsimplyğ‘‚(ğ‘…)(since the expansion is parallel on all hashkey arrays).\nThe candidate verification computes exact distances between the\n7\n\nquery and candidates (found by ESK-LSH expansion), and selects\ntop-ğ‘˜ğ‘šof them as output. Since there are ğ‘…ğ»candidates and the\noutput top-ğ‘˜ğ‘šare not required to be sorted, this step takes ğ‘‚(ğ‘…ğ»)\ntime. We set ğ‘€=âŒˆğ‘™ğ‘œğ‘”ğ‘âŒ‰to guarantee the hashing space has\nenough capacity, and ğ»andğ‘Ÿ0are fixed. Given that ğ‘…=ğ‘Ÿ0ğ‘˜ğ‘š, we\nhave the overall search time complexity of a single core model\nğ‘‚(ğ»âŒˆğ‘™ğ‘œğ‘”ğ‘âŒ‰+ğ»ğ‘Ÿ0ğ‘˜ğ‘š) (9)\n6.2 Time complexity of LIDER search process\nFor the centroids retriever, ğ‘˜ğ‘š=ğ‘0. And as discussed in Section 7.5,\nwe recommend to set ğ‘0=ğ‘/100âˆ¼ğ‘/50, andğ‘=ğ‘/(5Ã—104)âˆ¼\nğ‘/(104), i.e.,ğ‘0=ğ‘/(5Ã—106)âˆ¼ğ‘/(5Ã—105). We present it as\nğ‘0=ğ‘¡0ğ‘for simplification. So the time complexity of centroids\nretrieval stage is\nğ‘‚(ğ»âŒˆğ‘™ğ‘œğ‘”ğ‘âŒ‰+ğ»ğ‘Ÿ0ğ‘¡0ğ‘) (10)\nFor each in-cluster retriever we set ğ‘˜ğ‘š=ğ‘˜, then time complexity of\nin-cluster retrieval stage is ğ‘‚(ğ»âŒˆğ‘™ğ‘œğ‘”ğ‘âŒ‰)as termğ»ğ‘Ÿ0ğ‘˜is constant\nwithğ‘and this stage is parallelized. In addition, an in-cluster re-\ntriever can optionally sort the ğ‘˜results, which is not a burden on\ntime complexity but benefits the next stage.\nThe third stage is simply selecting top- ğ‘˜from the totally ğ‘0ğ‘˜neigh-\nbors found by the ğ‘0in-cluster retrievers, with the exact distances\ncomputed in the in-cluster retrieval stage. As mentioned above, the\nğ‘0neighbor lists can be sorted by the in-cluster retrievers, based on\nwhich the top- ğ‘˜selection here can be completed in ğ‘‚(ğ‘0+ğ‘˜ğ‘™ğ‘œğ‘”ğ‘ 0)\ntime, just by utilizing a heap of size ğ‘0that maintains the head of\neach list.\nIn summary, the overall search time complexity of LIDER is\nğ‘‚(2ğ»âŒˆğ‘™ğ‘œğ‘”ğ‘âŒ‰+ğ‘˜ğ‘™ğ‘œğ‘”(ğ‘¡0ğ‘)+(ğ»ğ‘Ÿ0+1)ğ‘¡0ğ‘) (11)\nGiven the fact that ğ‘¡0is tiny ( 2Ã—10âˆ’7âˆ¼2Ã—10âˆ’6) andğ»andğ‘Ÿ0are\nalso small (like ğ»=10andğ‘Ÿ0<10in our evaluation), the factor\n(ğ»ğ‘Ÿ0+1)ğ‘¡0is around 10âˆ’6in practice. This means before ğ‘reaches\ntensâˆ¼hundreds of millions, the complexity is close to logarithmic,\nthen it gets closer to linear (but with a tiny factor) on larger ğ‘,\npresenting the high efficiency and scalability of LIDER theoretically.\nAnd our evaluation proves this in practice.\n7 EXPERIMENTS\n7.1 Experiment settings\nAll experiments are evaluated on a Lambda Quad workstation with\n28 3.30GHz Intel Core i9-9940X CPUs, 4 RTX 2080 Ti GPUs and 128\nGB RAM. Note that all the experiments are conducted purely on\nthe ANN search stage of dense retrieval, excluding the embedding\ngenerating stage, i.e., all of them start from the step where the dense\nembeddings are already generated. We do not include the deep\nneural model in the evaluation, but directly use their generated\nembeddings as input. In addition, all the baseline methods and\nLIDER are evaluated purely on CPU. We do not utilize GPU for the\nANN search in this paper since the linear regressions in LIDER are\nsimple to compute such that it is not necessary to use GPU, though\nthey have the potential to be further accelerated by GPU.\n7.1.1 Evaluation datasets, tasks and metrics.\nWe use two datasets for our evaluation, MS MARCO and Wiki-21M.MS MARCO: The MS MARCO passage retrieval (a.k.a., passage\nranking) dataset [ 29] is one of the most commonly used dataset in\npassage retrieval research. It includes a collection of 8.8M passages\nfrom online webpages, several collections of queries used by differ-\nent tasks and the corresponding groundtruth collections to those\nqueries (i.e., for each query collection, there is a relevant passage\ncollection including relevant (query, passage) pairs). We conduct\nour experiments on two tasks from MS MARCO, MS MARCO Dev\nandTREC2019 DL , which share the same passage collection but use\ndifferent queries and different performance metrics. Specifically,\n(1) MS MARCO Dev has 6980 queries where each query has one\nor more (but only a few) relevant passages, and we measure the\nperformance by MRR@10 for quality and average query processing\ntime (AQT) for efficiency on this task. (2) TREC2019 DL has 43 valid\nqueries and about 9000 (query, passage) pairs in its groundtruth\ncollection. We evaluate the performance using the quality metric\nNDCG@10 and the efficiency metric AQT on this task.\nTo explore the impact of data scale on different methods, we\nsample the MS MARCO dataset into several subsets whose number\nof passages are respectively 100k, 500k, 1M, 4M (while the full MS\nMARCO dataset includes 8.8M passages), named as â€œMS-â€ followed\nby the size (like â€œMS-500kâ€), where the â€œMS-8.8Mâ€ is the full dataset\nitself. Both of MS MARCO Dev and TREC2019 DL tasks are evalu-\nated on all those subsets. The query and passage embeddings are\ngenerated by deep neural model â€œmsmarco-distilbert-base-v3â€1,\npre-trained on MS MARCO dataset.\nWiki-21M: Another dataset is Wiki-21M from [ 14]. It includes\n21,015,324 passages collected from Wikipedia dump. The evaluation\nqueries we use on this dataset is Natural Questions (NQ) [ 14] which\nis designed for end-to-end question answering, with questions\ncollected from real Google search queries and the answers from\nWikipedia articles. The test set of NQ includes 3610 queries. We\nname this task (i.e., retrieving passages from Wiki-21M dataset to\nanswer the queries in NQ test set) as â€œWiki-21M NQâ€. Similar to\nMS MARCO Dev, the experiment metrics on this task are MRR@10\nfor quality and average query processing time (AQT) for efficiency.\nThe embeddings of this dataset are generated by pre-trained DPR\nmodel [14]2\nThe embeddings for both MS MARCO and Wiki-21M are 768-\ndimensional. The total size of MS MARCO passage embeddings is\n26GB while that of Wiki-21M embeddings is 62GB. The embedding\nsimilarity metric in all the experiments is cosine similarity. Since\nour baseline methods do not support cosine similarity but work\nfor inner product similarity, we normalize all the query and pas-\nsage embeddings such that cosine similarity is equivalent to inner\nproduct over the normalized embeddings.\nNote that all the time results reported by the experiments are\nmeasured after the embedding generation, i.e., they are only the\nANN search time, excluding the embedding generation time of the\nneural model.\n1available at https://www.sbert.net/docs/pretrained-models/msmarco-v3.html\n2the pre-trained model (named as â€œcheckpoint.retriever.single-adv-hn.nq.bert-base-\nencoderâ€) and corresponding Wiki-21M passage embeddings are available at https:\n//github.com/facebookresearch/DPR\n8\n\n7.1.2 Baselines.\nWe select several widely used high-dimensional ANN indexes as\nbaselines. They are introduced as follows:\n(1)Flat: It simply searches the exact KNN by brute-force. So we\ntreat its retrieval quality as the upper bound of effectiveness\nfor LIDER and other baseline methods.\n(2)PQ: This is an ANN index that encodes high-dimensional\ndata into shorter codes by product quantization, and does\nANN search using the codes to reduce computation.\n(3)OPQ [8]: This is an improved variant of PQ index which\noptimizes PQ to better fit the data by applying a rotation. It\nhas a better effectiveness than PQ.\n(4)PCA-PQ [12]: This is another improved variant of PQ index\nthat applies PCA dimension reduction to the data before\nencoding it using PQ.\n(5)IVFPQ [11]: It implements the classic â€œinverted index + prod-\nuct quantizationâ€ ANN index, IVFADC in [ 11], which is one\nof the fastest high-dimensional ANN indexes today.\n(6)IVFPQ-HNSW : This method further optimizes the IVFPQ\nindex by using HNSW [ 26] to do the cluster assignment and\nmanagement for the inverted index, which further improves\nthe search efficiency.\n(7)FALCONN [1]: This is a mature and high-performance LSH\nindex library based on the classic multi-probe LSH [ 24],\nwhich is one of the most practical LSH methods in real-world\napplications.\n(8)SK-LSH : We include the original SK-LSH index as one of\nour baselines, in order to evaluate the improvement to it by\nour idea.\nThere are also other popular high-dimensional ANN indexes, but\ndue to some reasons we do not include them. For example, ScaNN\n[9] requires users to compile it with AVX support (which provides\na non-trivial acceleration using the facility of modern hardware),\nwhile LIDER does not utilize such a technique currently since it\nneeds much effort on the engineering. Therefore, to guarantee a\nfair comparison, we do not include ScaNN in the baselines.\nThe Flat and all PQ-based baselines are implemented using FAISS\n[13], a high-performance industrial ANN index library. FALCONN\nalso provides a public codebase, while SK-LSH is implemented by us\nas there is no open-source implementation. We set the parameters\nof the baseline indexes as such: (1) Flat is an exhaustively exact\nsearch index, so there is no specific parameter to set. (2) For IVFPQ\nand IVFPQ-HNSW, ğ¶=âˆš\nğ‘,ğ‘š=32,ğ‘=8,ğ‘=500, where\nğ‘is the number of passage embeddings in current dataset (as in\nTable 1),ğ¶is the number of centroids associated with the coarse\nquantizer in IVFADC, ğ‘šis the number of segments into which each\nembedding will be split, ğ‘is the number of bits used to encode the\ncentroids associated with the product quantizer in IVFADC, and ğ‘\nis the number of nearest inverted file entries to be inspected during\nsearch. As recommended by [ 13], we dynamically compute ğ¶based\non dataset size ğ‘instead of fixing it. The number of neighbors per\nnode and search depth of the HNSW in IVFPQ-HNSW are both set\nto be 32. (3) For PQ and OPQ, their parameters are just a subset of\nthose for IVFPQ, i.e., the ğ‘šandğ‘, which are same as IVFPQ. (4) For\nPCA-PQ, the parameters of its PQ component are the same as OPQ,\nwhile its PCA component reduces the data dimension to 192. (5) ForFALCONN and SK-LSH, we set the number of hash tables/hashkey\narraysğ»=24and the hashkey length ğ‘€=âŒˆğ‘™ğ‘œğ‘”2(ğ‘)âŒ‰. Since the\nmemory requirement of SK-LSH on Wiki-21M exceeds the machine\nlimit, we reduce its ğ»to 14 on Wiki-21M. For all the experiments\nin this paper, we set ğ‘˜=100, i.e.,always retrieving top-100 relevant\npassages to each query.\n7.1.3 Experiment categories.\nWe design two major categories of experiments,\n(1)end-to-end retrieval evaluation : This evaluation is con-\nducted on all of MS MARCO Dev, TREC2019 DL and Wiki-21M\nNQ tasks. It reports retrieval efficiency and quality by the cor-\nresponding metrics introduced in Section 7.1.1 for LIDER and\nthe baselines. To make it complete, this evaluation consists\nof two parts: (a) evaluation on varying datasets with fixed\nmethod parameters, and (b) evaluation on fixed datasets with\nvarying parameters.\n(2)evaluation of critical parameters, memory usage and con-\nstruction cost : This evaluation includes several experiments\nfor the impact of critical parameters on LIDER (Section 7.3,\n7.4 and 7.5) as well as the memory footprint and construction\ncost of LIDER (Section 7.6).\n7.2 End-to-end retrieval evaluation\nThis section introduces the two parts of the end-to-end retrieval\nevaluation, where the first is on varying datasets and the second is\non varying method parameters.\n7.2.1 Evaluation on varying datasets and tasks.\nIn the first part of the end-to-end retrieval evaluation, the parame-\nters of baseline methods are set as stated in Section 7.1.2, and we fix\nthe parameters of LIDER as such (which are selected by grid search):\nthe number of clusters ğ‘=1000, the number of retrieved centroids\nby centroids retriever ğ‘0=20, the number of ESK-LSH arrays in\nany core model ğ»=10, and the RMI width (i.e., the number of\nthe second-layer models in an RMI) in centroids retriever ğ‘Šğ‘=10\nwhile that of each in-cluster retriever ğ‘Šğ‘–=5. Normally larger ğ»\nandğ‘Šlead to better retrieval quality with slightly lower efficiency,\nwhile the effect of the clustering related parameters ğ‘andğ‘0is more\ncomplex, which will be explored in Section 7.5.\nTable 2 reports the scores to measure retrieval quality in the\nthree tasks. Flat is annotated as exact to highlight that it finds\nthe exact k nearest neighbors to the query, therefore we consider\nits retrieval quality scores as the upper bound for other methods.\nThe other methods are all approximate search methods. We also\nhighlight the highest score (among the approximate methods only)\non each dataset in Table 2. OPQ has no score for Wiki-21M NQ,\nsince it requires huge memory space on Wiki-21M which exceeds\nthe memory capacity of our evaluation machine. According to the\nscores, LIDER achieves the best retrieval quality among all ANN\nmethods in most cases. Though OPQ is competitive to LIDER in\nthe smaller datasets, LIDER outperforms it on the larger datasets,\nespecially given the fact that OPQ cannot be performed on the\nlargest dataset. Therefore, we can still conclude that LIDER has\nhigher effectiveness than all ANN baselines on large-scale data.\nFigure 4 illustrates the average query processing time, AQT, for\nend-to-end retrieval by LIDER and the baselines on MS MARCO\n9\n\nMS MARCO Dev\n(MRR@10)Wiki-21M NQ\n(MRR@10)TREC2019 DL\n(NDCG@10)\nMethod MS-100k MS-500k MS-1M MS-4M MS-8.8M Wiki-21M MS-100k MS-500k MS-1M MS-4M MS-8.8M\nFlat ( exact ) 0.8511 0.7227 0.6496 0.4922 0.3314 0.5518 0.5681 0.4726 0.4769 0.5762 0.6707\nPQ 0.7721 0.6304 0.5588 0.4129 0.2734 0.2145 0.4585 0.4204 0.4083 0.4533 0.5802\nOPQ 0.8143 0.6742 0.5994 0.4392 0.2907 - 0.5658 0.4360 0.4100 0.4893 0.5974\nPCA-PQ 0.8001 0.6575 0.5778 0.4248 0.2816 0.4513 0.5438 0.4013 0.4080 0.4824 0.5997\nIVFPQ 0.6152 0.4811 0.4349 0.3107 0.2154 0.2066 0.4228 0.3420 0.3584 0.4215 0.4973\nIVFPQ-HNSW 0.6151 0.4784 0.4274 0.3138 0.212 0.2133 0.3963 0.3230 0.3433 0.3911 0.4929\nFALCONN 0.7543 0.6402 0.5765 0.426 0.2882 0.3175 0.4712 0.3453 0.3767 0.5232 0.5595\nSK-LSH 0.7893 0.5785 0.5045 0.3225 0.2226 0.2702 0.5386 0.3429 0.3988 0.3806 0.4662\nLIDER 0.7428 0.6225 0.5667 0.4292 0.2908 0.4671 0.4684 0.4548 0.4366 0.5363 0.5861\nTable 2: End-to-end retrieval quality for all three evaluation tasks. The MS MARCO Dev and TREC2019 DL tasks are based on\nthe same subsets of the MS MARCO dataset, while Wiki-21M NQ task is based on the Wiki-21M dataset.\n0.0010.010.1110\nMS-100K MS-500K MS-1M MS-4M MS-8.8M Wiki-21MAQT (s)\nDatasetsFlat PQ\nOPQ PCA-PQ\nIVFPQ IVFPQ-HNSW\nFALCONN SK-LSH\nLIDER\nFigure 4: Average query processing time on MS MARCO Dev\nand Wiki-21M NQ for all the methods\nDev and Wiki-21M NQ tasks. AQT results of all the methods on\nTREC2019 DL task are very similar to those on MS MARCO Dev,\ntherefore we do not show them in this paper.\nBy the figure, Flat (the exact search method, presented by the\ngrey dashed line on the top) takes the longest query processing\ntime, which also increases fastest with the data scale. LIDER (the red\nsolid line with circle points) outperforms all baselines on the largest\ndatasets (i.e., MS-8.8M and Wiki-21M) since its average retrieval\ntime grows slowest with data scale, which complies to the time\ncomplexity analysis in Section 6 and proves the high efficiency of\nLIDER in practice. Particularly, on the largest datasets (MS-8.8M and\nWiki-21M), comparing to the fastest baseline method (i.e., IVFPQ-\nHNSW which is shown as the blue dashed line), LIDER achieves 15%\nâˆ¼20% speedup and much higher retrieval quality. Comparing to\nthe highest-quality ANN baseline methods (i.e., OPQ and PCA-PQ\nwhich are shown as orange dashed line with triangle points and\norange dotted line), LIDER still has better quality on the largest\ndatasets with more significant speedup, i.e., 300% âˆ¼500%. We also\nobserve that SK-LSH has a similar AQT growth trend to LIDER, but\nits base AQT is too high. Note that in the figure, the distributionof dataset labels on the x-axis are not proportional to the dataset\nsizes, instead, the labels are evenly placed to have a better view. But\nthis does not affect the comparison of AQT growth trends between\ndifferent methods. Since in real-world applications the data scale\ncan be much larger than that in our experiments, LIDER does have\na great potential in highly efficient dense retrieval for real-world\nscenarios, due to its slow AQT growth trend with the data scale.\n7.2.2 Evaluation on varying parameters.\nIn the first part experiments above, we fix key parameters of the\nmethods and compare their performance on different datasets and\ntasks. To further evaluate the effectiveness and efficiency of LIDER,\nin this part we fix the datasets and tasks (i.e., MS MARCO Dev and\nWiki-21M NQ), and vary those parameters to draw the AQT-MRR\ncurves for all the approximate methods. The exact method Flat has\nno parameters to vary its performance, so this part of evaluation\ndoes not include it.\nFigure 5 and 6 illustrate the AQT-MRR curves for the two tasks\nrespectively. There are some observations: (1) in the range of low\nMRR, IVFPQ and IVFPQ-HNSW have better efficiency than LIDER,\nbut (2) in the range of high MRR, LIDER is significantly more effi-\ncient than the baselines. Actually in our experiments, we find that\nIVFPQ and IVFPQ-HNSW are hard to achieve a high MRR under\nthe similar resource constraints (e.g., an acceptable length of index\nbuilding time) to other methods. And that is the reason why the\npoints on the curves of IVFPQ and IVFPQ-HNSW are concentrated\nin a relatively low MRR area. But it is enough to show the trends, i.e.,\nLIDER has the best performance on effectiveness-efficiency trade-\noff. In most cases it takes the shortest search time to achieve the\nsame retrieval quality as the baselines, and its efficiency is easy to\nbe further improved with only tiny sacrifice on the quality. Or vice\nversa, it is also able to significantly enhance the retrieval quality\nwith a tiny loss on search speed. Such a good capability of trade-off\ndoes make LIDER practical.\n7.3 Impact of ğ»in ESK-LSH\nThe ESK-LSH expansion time takes a large portion in the end-to-\nend retrieval time of LIDER, so in this experiment, we explore the\neffect of the critical ESK-LSH parameter, ğ», on the core model per-\nformance. To simplify the evaluation, we build a standalone core\n10\n\n0.00060.0060.060.6\n0.03 0.08 0.13 0.18 0.23 0.28 0.33AQT (s)\nMRR@10PQ OPQ\nPCA-PQ IVFPQ\nIVFPQ-HNSW FALCONN\nSK-LSH LIDERFigure 5: Average query processing time vs MRR@10 for all\nANN methods on MS MARCO Dev\n0.00070.0070.070.7\n0.03 0.13 0.23 0.33 0.43 0.53AQT (s)\nMRR@10PQ PCA-PQ\nIVFPQ IVFPQ-HNSW\nFALCONN SK-LSH\nLIDER\nFigure 6: Average query processing time vs MRR@10 for all\nANN methods except OPQ on Wiki-21M NQ\nğ» MRR@10 Average expansion time\n32 0.4928 0.0375s\n48 0.5569 0.0399s\n64 0.5912 0.0492s\nTable 3: Retrieval MRR@10 and average ESK-LSH expansion\ntime of the standalone core model with different values of\nğ»on MS-1M dataset\nmodel on MS-1M dataset without the clustering-based architec-\nture. Soğ»may be different from the end-to-end evaluation, but\nthe trend of its effect on the performance remains the same. We set\nğ»=32,48,64and report MRR@10 and average ESK-LSH expansion\ntime of this single core model in Table 3, which shows that by using\nmore arrays, the retrieval quality of the core model is significantly\nimproved, with only tiny time overhead added on ESK-LSH expan-\nsion. This proves the way to increase the parallelism of SK-LSH (as\ndiscussed in Section 4.3) is effective.7.4 Impact of the key re-scaling module\nIn this section, we evaluate the effect of key re-scaling module\non reducing out-of-range predictions of RMI (as discussed in Sec-\ntion 5.1). Specifically, we define a predicted location as out-of-range\nprediction (OOR) if it equals 0 or ğ¿ğ‘ğ‘Ÿğ‘Ÿğ‘ğ‘¦âˆ’1, as RMI will truncate\nbig prediction to ğ¿ğ‘ğ‘Ÿğ‘Ÿğ‘ğ‘¦âˆ’1and round negative prediction to 0.\nWe also define a predicted location as large-error prediction (LE)\nif the gap between it and the true location is larger than 100, i.e.,\nany prediction error exceeding ğ‘˜is a large error where ğ‘˜=100in\nour evaluation. Then we check the overlap between OOR and LE\npredictions, which reflects if the large errors are mainly caused by\nthe out-of-range problem. We denote the numbers of OOR, LE and\ntheir overlapped predictions by ğ‘ğ‘‚ğ‘‚ğ‘…,ğ‘ğ¿ğ¸andğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘™ğ‘ğ‘ . Similar\nto Section 7.3, this experiment is also conducted on a standalone\ncore model.\nUsing key re-scaling ğ‘ğ‘‚ğ‘‚ğ‘…ğ‘ğ¿ğ¸ğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘™ğ‘ğ‘\nNo 4846 4733 4245\nYes 3 2536 0\nTable 4: RMI prediction quality (the number of out-of-range\npredictions ğ‘ğ‘‚ğ‘‚ğ‘…, the number of large-error predictions\nğ‘ğ¿ğ¸and the size of their overlap ğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘™ğ‘ğ‘ ) before and after\nusing key re-scaling module on the MS-100k dataset\nAs shown in Table 4, on MS-100k dataset and the 6980 queries\nof MS MARCO Dev (where for each query, one prediction is made),\nbefore using key re-scaling module, the out-of-range and large-\nerror predictions are heavily overlapped, meaning that most of the\nlarge-error predictions are probably caused by the out-of-range\nproblem. And after using the module, all of the three numbers\nsignificantly decrease, which means the large errors from out-of-\nrange problem have been successfully reduced, and the remaining\nerrors are likely to be from RMI itself. In conclusion, key re-scaling\nmodule effectively improves RMI prediction quality.\n7.5 Impact of the clustering related parameters\nIn this section we investigate the effect of the two clustering related\nparameters, the number of clusters ğ‘and the number of retrieved\ncentroidsğ‘0, on the end-to-end performance of LIDER. Experiments\nare conducted on MS MARCO Dev task using MS-8.8M dataset.\nFirst we fix ğ‘=1000 and setğ‘0=1,5,10,15,20,30,40,50,100to\nobserve the end-to-end retrieval performance of LIDER. As shown\nin Figure 7, overall the quality and search time both increase with\nğ‘0, because increasing ğ‘0will let LIDER retrieve more candidates\non which the exact vector distances are computed and the top-\nk are finally selected. So more candidates normally lead to more\naccurate results and longer search time. Another observation in the\nfigure is that the effect of ğ‘0on the retrieval quality improvement\nis degrading with its growth, i.e., when ğ‘0is small, increasing it\nwill bring more gain on the quality than when it is large, and also\nbring less latency growth than when it is large. So we recommend\nusers of LIDER to keep ğ‘0relatively small comparing to the total\nnumber of clusters ğ‘, like around 1/100âˆ¼1/50ofğ‘.\nSecond we fix ğ‘0=10and vary the total number of clusters ğ‘=\n50,100,200,400,600,800,1000,1200,1600,2000,2400,3000,4000. Fig-\nure 8 presents the MRR@10 and AQT on each ğ‘value. The AQT\n11\n\n00.020.040.060.080.10.120.14\n00.050.10.150.20.250.30.35\n1 5 10 15 20 30 40 50 100\nAQT (s)MRR@10\nThe number of retrieved centroidsMRR@10\nAQT(s)Figure 7: The retrieval quality and average query processing\ntime of different ğ‘0values when ğ‘=1000\n00.0050.010.0150.020.0250.030.0350.04\n00.050.10.150.20.250.3\n50 100 200 400 600 800 1000 1200 1600 2000 2400 3000 4000\nAQT (s)MRR@10\nThe number of clustersMRR@10\nAQT(s)\nFigure 8: The retrieval quality and average query processing\ntime of different ğ‘values when ğ‘0=10\nis decreasing with the increasing ğ‘, which is straightforward to\nexplain: the dataset size is fixed, therefore increasing ğ‘means the\naverage size of each cluster will decrease. Given that ğ‘0is fixed,\nfewer candidates will be retrieved and verified, so AQT decreases.\nBut the MRR@10 is not monotonic, instead, it first increases then\nfalls with the increasing ğ‘. This is because (1) when ğ‘is small, each\ncluster will include a large number of data points, making it harder\nfor RMI to accurately learn the distribution, so the in-cluster re-\ntrieval quality degrades. And (2) when ğ‘is large, each cluster will be\nsmall, meaning that the number of candidates is likely not enough\nsince an in-cluster retriever will just stop and return whatever it\nfound when the corresponding cluster is exhausted. In such a case\nthe recall will be probably low, resulting a low MRR. Thus we rec-\nommend to set a proper ğ‘based on the dataset size, such that each\ncluster includes around 10k âˆ¼50k data vectors.\n7.6 Memory footprint and index construction\ntime\nIn this section we report and analyze the memory footprint and\nindex construction time cost of LIDER. Table 5 reports elapsed\ntime for the three main construction stages of LIDER (where the\nconstruction finishes after Stage 3), as well as memory footprint\nof LIDER after each stage. In addition, Table 5 also includes theconstruction time and memory usage of the original SK-LSH to\nshow the improvement made by LIDER on the index size. All the\nmemory results in the table have excluded the memory of data\nembeddings, i.e., they are purely the spaces used to maintain the\nindexes. The parameters of LIDER and SK-LSH are the same as\nthose in the end-to-end retrieval evaluation (Section 7.2).\nMemory footprint: First, we have the following observations on\nthe LIDER construction stages. By Table 5, memory usage of LIDER\nafter building the centroids retriever (i.e., after Stage 2) is close\nto the previous stage, meaning that the centroids retriever has a\nsmall size, while the memory usage significantly increases after\nStage 3, reflecting that the in-cluster retrievers take up the major\nfraction of the index size. Second, comparing to the original SK-\nLSH, LIDER shows its significance on reducing memory overhead.\nSpecifically, it saves 53% and 58% memory space comparing with SK-\nLSH on the two largest datasets, enabling it to process larger scale\ndata than original SK-LSH on the same hardware. Such a saving is\nachieved mainly by its cluster-based architecture. As each cluster is\nsignificantly smaller than the whole dataset, the required number of\nhashkey arrays and the length of hashkey can be both reduced while\nstill guaranteeing high effectiveness, e.g., in Section 7.2 each core\nmodel of LIDER only needs 10 arrays to outperform the SK-LSH\nbaseline with 24 arrays.\nConstruction time: LIDER presents its significant impact on sav-\ning memory usage, with a cost of construction time. As in Table 5,\nwe have observed that LIDER has a longer construction time than\nSK-LSH, where the bottleneck is the k-means clustering (Stage\n1). However, this is not an issue in practice since there are many\napproaches to speed up the k-means clustering. For example, the\nFAISS library provides GPU-based k-means clustering, which can\ncomplete LIDER Stage 1 in a few seconds.\nMS-8.8M Wiki-21M\nTime Memory Time Memory\nLIDER Stage 1 - Clustering 818s 306MB 1336s 725MB\nLIDER Stage 2 - Building CR 0.1s 309MB 0.1s 727MB\nLIDER Stage 3 - Building all IRs 83s 14.2GB 224s 26.4GB\nSK-LSH 472s 30.2GB 1181s 62.7GB\nTable 5: Elapsed time of each construction stage in LIDER\nand memory usage by LIDER when each stage finishes,\nwith comparison to the original SK-LSH on the two largest\ndatasets. The CRin LIDER Stage 2 means Centroids Retriever\nwhile the IRin Stage 3 stands for In-cluster Retriever .\n8 CONCLUSION\nIn this paper, we introduce LIDER, an efficient high-dimensional\nlearned index for large-scale dense passage retrieval. Experiments\npresent that LIDER outperforms the state-of-the-art ANN indexes\non large-scale dense retrieval tasks by achieving higher efficiency\nwith high retrieval quality. It also has a much better capability of\neffectiveness-efficiency trade-off. Therefore, LIDER can serve as a\npractical and powerful component in very large-scale real-world\ndense retrieval applications.\n12\n\nACKNOWLEDGMENTS\nThis work is partially supported by DARPA under Award #FA8750-\n18-2-0014 (AIDA/GAIA).\nREFERENCES\n[1]Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and Ludwig\nSchmidt. 2015. Practical and optimal LSH for angular distance. Advances in\nneural information processing systems 28 (2015).\n[2]Lila Boualili, Jose G. Moreno, and Mohand Boughanem. 2020. MarkedBERT:\nIntegrating Traditional IR Cues in Pre-Trained Language Models for Passage Re-\ntrieval . Association for Computing Machinery, New York, NY, USA, 1977â€“1980.\nhttps://doi.org/10.1145/3397271.3401194\n[3]Moses S Charikar. 2002. Similarity estimation techniques from rounding algo-\nrithms. In Proceedings of the thirty-fourth annual ACM symposium on Theory of\ncomputing . 380â€“388.\n[4]Zhuyun Dai and Jamie Callan. 2020. Context-Aware Term Weighting For First\nStage Passage Retrieval . Association for Computing Machinery, New York, NY,\nUSA, 1533â€“1536. https://doi.org/10.1145/3397271.3401204\n[5]Angjela Davitkova, Evica Milchevski, and Sebastian Michel. 2020. The ML-Index:\nA Multidimensional, Learned Index for Point, Range, and Nearest-Neighbor\nQueries.. In EDBT . 407â€“410.\n[6]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding.\narXiv:1810.04805 [cs.CL]\n[7]Wei Dong, Charikar Moses, and Kai Li. 2011. Efficient k-nearest neighbor graph\nconstruction for generic similarity measures. In Proceedings of the 20th interna-\ntional conference on World wide web . 577â€“586.\n[8]Tiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun. 2013. Optimized Product\nQuantization for Approximate Nearest Neighbor Search. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) .\n[9]Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and\nSanjiv Kumar. 2020. Accelerating large-scale inference with anisotropic vector\nquantization. In International Conference on Machine Learning . PMLR, 3887â€“3896.\n[10] Kiana Hajebi, Yasin Abbasi-Yadkori, Hossein Shahbazi, and Hong Zhang. 2011.\nFast approximate nearest-neighbor search with k-nearest neighbor graph. In\nTwenty-Second International Joint Conference on Artificial Intelligence .\n[11] Herve Jegou, Matthijs Douze, and Cordelia Schmid. 2010. Product quantization\nfor nearest neighbor search. IEEE transactions on pattern analysis and machine\nintelligence 33, 1 (2010), 117â€“128.\n[12] HervÃ© JÃ©gou, Matthijs Douze, Cordelia Schmid, and Patrick PÃ©rez. 2010. Aggregat-\ning local descriptors into a compact image representation. In 2010 IEEE computer\nsociety conference on computer vision and pattern recognition . IEEE, 3304â€“3311.\n[13] Jeff Johnson, Matthijs Douze, and HervÃ© JÃ©gou. 2017. Billion-scale similarity\nsearch with GPUs. arXiv preprint arXiv:1702.08734 (2017).\n[14] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey\nEdunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-\nDomain Question Answering. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) . Association for Computational\nLinguistics, Online, 6769â€“6781. https://doi.org/10.18653/v1/2020.emnlp-main.550\n[15] Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage\nSearch via Contextualized Late Interaction over BERT . Association for Computing\nMachinery, New York, NY, USA, 39â€“48. https://doi.org/10.1145/3397271.3401075\n[16] Tim Kraska, Mohammad Alizadeh, Alex Beutel, H Chi, Ani Kristo, Guillaume\nLeclerc, Samuel Madden, Hongzi Mao, and Vikram Nathan. 2019. Sagedb: A\nlearned database system. In CIDR .\n[17] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. arXiv:1712.01208 [cs.DB]\n[18] TimothÃ©e Lacroix, Nicolas Usunier, and Guillaume Obozinski. 2018. Canon-\nical tensor decomposition for knowledge base completion. arXiv preprint\narXiv:1806.07297 (2018).\n[19] Tian Lan, Deng Cai, Yan Wang, Yixuan Su, Heyan Huang, and Xian-Ling Mao.\n2021. Exploring Dense Retrieval for Dialogue Response Selection. https:\n//doi.org/10.48550/ARXIV.2110.06612\n[20] Victor Lempitsky and A Babenko. 2012. The inverted multi-index. In 2012 IEEE\nConference on Computer Vision and Pattern Recognition . IEEE Computer Society,\n3069â€“3076.\n[21] Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. 2020. Distilling Dense Repre-\nsentations for Ranking using Tightly-Coupled Teachers. arXiv:2010.11386 [cs.IR]\n[22] Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. 2020. Distilling Dense Repre-\nsentations for Ranking using Tightly-Coupled Teachers. arXiv:2010.11386 [cs.IR]\n[23] Yingfan Liu, Jiangtao Cui, Zi Huang, Hui Li, and Heng Tao Shen. 2014. SK-LSH:\nan efficient index structure for approximate nearest neighbor search. Proceedings\nof the VLDB Endowment 7, 9 (2014), 745â€“756.[24] Qin Lv, William Josephson, Zhe Wang, Moses Charikar, and Kai Li. 2007. Multi-\nProbe LSH: Efficient Indexing for High-Dimensional Similarity Search. In Pro-\nceedings of the 33rd International Conference on Very Large Data Bases (Vienna,\nAustria) (VLDB â€™07) . VLDB Endowment, 950â€“961.\n[25] Craig Macdonald and Nicola Tonellotto. 2021. On Approximate Nearest Neighbour\nSelection for Multi-Stage Dense Retrieval . Association for Computing Machinery,\nNew York, NY, USA, 3318â€“3322. https://doi.org/10.1145/3459637.3482156\n[26] Yu A Malkov and DA Yashunin. 2016. Efficient and robust approximate nearest\nneighbor search using Hierarchical Navigable Small World graphs. arXiv preprint\narXiv:1603.09320 (2016).\n[27] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient\nestimation of word representations in vector space. arXiv preprint arXiv:1301.3781\n(2013).\n[28] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020. Learn-\ning Multi-Dimensional Indexes. Proceedings of the 2020 ACM SIGMOD Interna-\ntional Conference on Management of Data (May 2020). https://doi.org/10.1145/\n3318464.3380579\n[29] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan\nMajumder, and Li Deng. 2016. MS MARCO: A human generated machine reading\ncomprehension dataset. In CoCo@ NIPS .\n[30] Rodrigo Nogueira. 2019. From doc2query to docTTTTTquery.\n[31] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document\nExpansion by Query Prediction. arXiv:1904.08375 [cs.IR]\n[32] Varun Pandey, Alexander van Renen, Andreas Kipf, Ibrahim Sabek, Jialin Ding,\nand Alfons Kemper. 2020. The case for learned spatial indexes. arXiv preprint\narXiv:2008.10349 (2020).\n[33] Prafull Prakash, Julian Killingback, and Hamed Zamani. 2021. Learning Robust\nDense Retrieval Models from Incomplete Relevance Labels . Association for Comput-\ning Machinery, New York, NY, USA, 1728â€“1732. https://doi.org/10.1145/3404835.\n3463106\n[34] Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings\nusing siamese bert-networks. arXiv preprint arXiv:1908.10084 (2019).\n[35] Nils Reimers and Iryna Gurevych. 2020. The Curse of Dense Low-Dimensional\nInformation Retrieval for Large Index Sizes. arXiv preprint arXiv:2012.14210\n(2020).\n[36] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2020.\nDistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.\narXiv:1910.01108 [cs.CL]\n[37] Yichun Shi and Anil K Jain. 2019. Probabilistic face embeddings. In Proceedings\nof the IEEE International Conference on Computer Vision . 6902â€“6911.\n[38] Hongyin Tang, Xingwu Sun, Beihong Jin, Jingang Wang, Fuzheng Zhang, and\nWei Wu. 2021. Improving Document Representations by Generating Pseudo\nQuery Embeddings for Dense Retrieval. arXiv preprint arXiv:2105.03599 (2021).\n[39] Yao Tian, Tingyun Yan, Xi Zhao, Kai Huang, and Xiaofang Zhou. 2022. A Learned\nIndex for Exact Similarity Search in Metric Spaces. https://doi.org/10.48550/\nARXIV.2204.10028\n[40] Haixin Wang, Xiaoyi Fu, Jianliang Xu, and Hua Lu. 2019. Learned Index for Spatial\nQueries. In 2019 20th IEEE International Conference on Mobile Data Management\n(MDM) . 569â€“574. https://doi.org/10.1109/MDM.2019.00121\n[41] Ledell Wu, Fabio Petroni, Martin Josifoski, Sebastian Riedel, and Luke Zettlemoyer.\n2019. Scalable zero-shot entity linking with dense entity retrieval. arXiv preprint\narXiv:1911.03814 (2019).\n[42] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett,\nJunaid Ahmed, and Arnold Overwijk. 2020. Approximate nearest neighbor nega-\ntive contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808\n(2020).\n[43] Ikuya Yamada, Akari Asai, and Hannaneh Hajishirzi. 2021. Efficient\nPassage Retrieval with Hashing for Open-domain Question Answering.\narXiv:2106.00882 [cs.CL]\n[44] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma.\n2021. Learning Discrete Representations via Constrained Clustering for Effective\nand Efficient Dense Retrieval. arXiv preprint arXiv:2110.05789 (2021).\n13",
  "textLength": 77154
}