{
  "paperId": "64bdf8c017217ba20f008460900e68c8e65e04ba",
  "title": "Learned-Database Systems Security",
  "pdfPath": "64bdf8c017217ba20f008460900e68c8e65e04ba.pdf",
  "text": "arXiv:2212.10318v4  [cs.CR]  2 Jul 2025Published in Transactions on Machine Learning Research (07/2025)\nLearned-Database Systems Security\nRoei Schuster rschuster@gmail.com\nContext AI\nJin Peng Zhou jz563@cornell.edu\nCornell University Department of Computer Science\nThorsten Eisenhofer thorsten.eisenhofer@tu-berlin.de\nBIFOLD & TU Berlin\nPaul Grubbs paulgrub@umich.edu\nUniversity of Michigan\nNicolas Papernot nicolas.papernot@utoronto.ca\nUniversity of Toronto & Vector Institute\nReviewed on OpenReview: https: // openreview. net/ forum? id= XNVBSbtcKB\nAbstract\nA learned database system uses machine learning (ML) internally to improve performance.\nWe can expect such systems to be vulnerable to some adversarial-ML attacks. Often, the\nlearned component is shared between mutually-distrusting users or processes, much like mi-\ncroarchitectural resources such as caches, potentially giving rise to highly-realistic attacker\nmodels. However, compared to attacks on other ML-based systems, attackers face a level of\nindirection as they cannot interact directly with the learned model. Additionally, the differ-\nence between the attack surface of learned and non-learned versions of the same system is\noften subtle. These factors obfuscate the de-facto risks that the incorporation of ML carries.\nWe analyze the root causes of potentially-increased attack surface in learned database sys-\ntems and develop a framework for identifying vulnerabilities that stem from the use of ML.\nWe apply our framework to a broad set of learned components currently being explored in\nthe database community. To empirically validate the vulnerabilities surfaced by our frame-\nwork, we choose 3 of them and implement and evaluate exploits against these. We show\nthat the use of ML cause leakage of past queries in a database, enable a poisoning attack\nthat causes exponential memory blowup in an index structure and crashes it in seconds, and\nenable index users to snoop on each others’ key distributions by timing queries over their\nown keys. We find that adversarial ML is an universal threat against learned components\nin database systems, point to open research gaps in our understanding of learned-systems\nsecurity, and conclude by discussing mitigations, while noting that data leakage is inherent\nin systems whose learned component is shared between multiple parties.\n1 Introduction\nLearned-database systems incorporateMLmodelsinternallytoimproveperformance. Thesemodelsadaptthe\nsystem’s inner workings based on data about recent inputs, such as observed workloads. Recent years have\nseen ML-based enhancements, or outright replacements, for core components such as data structures Kraska\net al. (2018); Ding et al. (2020a); Galakatos et al. (2019), query optimizers Marcus et al. (2022; 2019);\nVan Aken et al. (2017), memory allocators Yang et al. (2020), and schedulers Mao et al. (2019); Holze et al.\n(2010). At the same time, the use of machine learning introduces well-known security vulnerabilities, such as\nadversarial inputs that cause misprediction Szegedy et al. (2013); Goodfellow et al. (2014); data-poisoning\n1\n\nPublished in Transactions on Machine Learning Research (07/2025)\nML\nInput\nTraining\nDataTraining\nDataML\nOutputSystem\nInput\nSystem\nOutputNon-ML\nFunctionalityTraining Inference\nML ComponentPrediction affects \nsystem performance\nFigure 1: Learned database systems use ML models whose predictions are used only internally, yet indirectly\naffect externally-observable performance metrics.\nattacks Chen et al. (2017b); Yang et al. (2017); Shafahi et al. (2018); or training-data leakage Shokri et al.\n(2017); Song & Shmatikov (2019).\nIn this paper, we study how the incorporation of ML exposes database systems to attacks on privacy and\navailability. This is rooted in the facts that, first, fitting a model to historical data distributions increases\ninformation flows within the system; second, ML models are often trained for average-case objectives, and\nhavereduced robustness to pathological input cases. To assess the security threats introduced by the incor-\nporation of ML, we give a simple framework for identifying categories of vulnerabilities which could logically\narise when including learned components into a system: adversarial inputs and data poisoning can target the\nsystem’savailability/performance. Thesearecarefullycrafteddatapointsdesignedtomisleadamodelattest\ntime respectively malicious points injected into the training data to affect model performance. Furthermore,\ntiming attacks can be used to leak sensitive information about a models training-data. To examine whether\nthese vulnerabilities lead to attacks, we survey a broad set of prominent classes of learned components being\nactively researched. We examine how adding ML increases a systems’ attack surface, and how this could\nconceivably be exploited.\nTo validate our framework, we perform three case studies of building proof-of-concept exploits for the at-\ntacks we identified. Each exploiting a widely-cited learned component published in a top venue: the BAO\nquery optimizer Marcus et al. (2022), and the key-value index structures ALEX Ding et al. (2020a) and\nPGM Ferragina & Vinciguerra (2020). We demonstrate that the conceptual vulnerabilities identified in our\nsurvey can indeed manifest in real systems.\nFor BAO, which is used in production by Microsoft Negi et al. (2021a), we show the attacker can reliably\ninfer that a specific query was executed by observing its current runtime. In other words, the very success of\nBAO — using ML to minimize execution time of previously observed queries — introduces an information\nleak. This demonstrates how leakage naturally occurs in learned components that act as a shared resource\nby multiple clients, and therefore adapt themselves to the observed workload by all clients. This is similar\nto how a shared cache adapts itself to the memory workload of all operating-system processes (and users),\nregardless of process-memory isolation.\nWeanalyzetheALEXindexstructure, whichpresentsanimprovedaverage-caseperformanceovertraditional\nnon-learnedbaselinesbuthasnoworst-caseguarantees. Wediscoveraclassofpathologicalinputscaseswhich\ncause exponential memory blowup, and use them to crash the system within seconds when running on a\n16GB RAM machine, or in about 10 minutes when running on a server equipped with 512GB RAM. This\nillustrates the dangers of optimizing for average-case performance and disregarding edge cases that can be\nmaliciously invoked.\nNext, we consider the PGM index, which has excellent performance andamortized worst-case guarantees\non all operations. Worst-case guarantees can mitigate attacks on availability, yet we show that they still\ndo not protect against privacy attacks, by demonstrating that an attacker can infer information about the\ndistribution of victim keys by querying otherkeys.\n2\n\nPublished in Transactions on Machine Learning Research (07/2025)\nInformation flow from training data to output: \nprecondition for data poisoning, leakageLack of robustness guarantees: precondition \nfor adversarial inputs, data poisoning\nML\nInput\nTraining\nDataTraining\nDataML\nOutputTraining Inference\nInformation flow from \ninput features to output: \nprecondition for \nadversarial inputs\nFigure 2: Information flows and lack of robustness are preconditions for ML vulnerabilities.\nLastly, we discuss mitigations. Some attacks might be prevented by careful use of existing defenses against\nadversarial ML, such as differentially-private training Abadi et al. (2016); Song et al. (2013) or robust\ntraining Xie et al. (2019); Chen et al. (2017a); Cohen et al. (2019). However some of the information-leakage\nwe observe are inherent to the very quality that makes ML desirable, namely, the ability to adapt to observed\ninput patterns.\nContributions. We make the following key contributions.\n•We systematize the study of the security of learned components in ML-enhanced database systems.\nWe build a framework (Section 2) to identify and analyze ML vulnerabilities in these systems, which\narecorollariesofincreasedinformationflowsandlackofrobustness. Wesurvey(Section3)prominent\nclasses of learned components and explain how these vulnerabilities are exploited in context.\n•We validate our framework by demonstrating 3 exploits for attacks it identified: first, BAO (Sec-\ntion 4), a query optimizer that learns from past query executions and thereby allows attackers to\nidentify which queries were executed; Second, ALEX (Section 5), a learned index structure that\nhas excellent average-case performance but can be crashed in seconds using a meticulously-crafted\nseries of inputs; finally, we consider PGM (Section 6), an index structure that improves on ALEX\nby providing excellent worst-case guarantees. Despite this best effort, PGM still exhibits a vulnera-\nbility compared to traditional approaches: PGM leaks, through query-latency measurements of the\nattacker’s own stored data, information about underlying distribution of other stored keys.\n•We discuss potential mitigations, explain why some attacks are inherent to learned components\nthat are shared between multiple mutually-distrusting users, and identify significant gaps in current\nresearch as well as in the security practices of designing learned systems.\nAs learned components become increasingly adopted in practice, where they handle real-world sensitive data\nand their performance is critically relied upon, there is an urgent need to understand their risks. Our work\nprovides an important step towards this goal.\n2 A Framework\nIn this work, we define learned systems as systems that uses ML components internally to improve perfor-\nmance metrics such as runtime or memory usage without affecting the system’s output (cf. Figure 1). In\ncontrast, other ML-based software employs ML to directly influence outputs. For instance, approximate\nquery processing Li & Li (2018); Chaudhuri et al. (2017) use model predictions to compute and return\napproximate results directly to the user. Conversely, in a learned index structure (§5, 6), the learned com-\nponent predicts the location of the key in the index, only to help locate its record, which is the system’s\noutput. Model accuracy does not affect the output, only the answer’s latency.\n3\n\nPublished in Transactions on Machine Learning Research (07/2025)\nML increases systems’ attack surfaces. Learned systems inherit key weaknesses of their underlying\nML components — (1) lack of robustness to adversarial manipulation, and (2) increased information flows\nbetween ML inputs and outputs. The latter occurs due to the addition of (possibly multiple) training steps,\nas well as the fact that deep ML model architectures are designed to be able to take extraneous features as\ninput and perform (learned) feature selection LeCun et al. (2015). Figure 2 illustrates these issues.\nTwo key features of learned systems exacerbate these weaknesses. First, the line between ML and the system\nis often blurry . The abstraction in Figure 1, where a single well-delineated model provides a prediction to an\nencapsulating system, is an oversimplification. Learned systems can include many sub-models that work to-\ngether and are inter-dependent, to the point where the system’s entire internal object hierarchy, architecture,\nand memory footprint become subject to the learned (and potentially malleable) data distribution. These\nlevels could even interact across abstraction boundaries—for example, a learned database index accessed by\na process using a learned memory allocator.\nSecond,learned components are often shared resources . Production databases have flexible access control\npermission systems MongoDB (2024b); MSSQL (2024) to support multiple users and roles with varying\naccess-privilege levels; however components such as the underlying index structure MongoDB (2024a), query\noptimizer, or configuration knobs Xu et al. (2015) are shared between all users. When these components\nare learned from multiple users/roles’ data and serve them at the same time, privilege-level information\ncompartmentalization breaks.\nSome age-old systems are learned. An astute reader might have noticed that the defining characteristic\nof learned systems, i.e. adapting to observed input patterns in order to improve performance, is not new to\nthe deep-learning era. In fact, it may precede neural networks altogether, as several “classical” hardware and\nsoftware components such as branch predictors or caches contain components that adapt to input patterns.\nWe argue that it is no coincidence that these same microarchitectural elements have been targets to countless\ntiming attacks Yarom & Falkner (2014); Tromer et al. (2010); Osvik et al. (2006) — they carry the very\nrisks we discuss in this paper. Importantly, our paper’s focus is the growing trend of incorporating modern\nML in database systems. We see a future where instead of a few well-known shared resources that adapt to\ninput patterns across multiple users, more and more systems incorporates this principle, thereby becoming\nmore vulnerable. Our goal is therefore to systemize our understanding of this trend.\n2.1 Identifying attacks\nOur observation about the root causes of security risks in learned systems will be the starting point for\na methodology for identifying attacks on learned components in database systems. The methodology is\nillustrated in Figure 3, and described next.\nStep 1: Identify ML components, their added information flows, and robustness. It is crucial to\nunderstand the ML components of the learned system, focusing especially on the information flows that are\nadded, and the resulting system’s worst-case performance bounds. Here, information flows refer to the paths\nthrough which input data or workload observations influence system behavior via ML models—introducing\nnew dependencies that may not exist in traditional systems.\nStep 2: Consider ML attacks on components. Armed with an understanding of how the ML\ncomponents interact with the rest of the system, the next step is to identify which, if any, preconditions exist\nfor attacks on the ML components of the system. In this paper, we will focus on three kinds of attacks. They\nall exploit new information flows introduced by ML in learned systems, or the lack of worst-case guarantees\nprovided by the ML component. Figure 2 illustrates how information flow and lack of robustness lead to\nthese vulnerabilities.\nAdversarial inputs. These are inference-time inputs crafted to make a model produce attacker-chosen or\nerroneous outputs Goodfellow et al. (2014); Szegedy et al. (2013). A precondition of these attacks is an\ninformation flow from an attacker-supplied input to output, or that the ML component is not robust to\nadversarially-crafted inputs.\n4\n\nPublished in Transactions on Machine Learning Research (07/2025)\nAdversarial inputs\nTamperingPoisoningPreconditions AttacksSide channelsTimingShared\nresources\nNon-sharedAdded information \nflow from training \ndata to outputWorse worst-case \nguaranteesAdded information \nflow from input \nfeatures to output\nFigure 3: Our methodology for surfacing learned systems’ vulnerabilities to new attacks identifies (1)\nwhether a system is used as a shared resource between multiple participants, (2) which information flows are\nadded, and (3) if the system has worsened worst-case guarantees as a result of using ML. Then we consider\nthe appropriate attacker models.\nData leakage. Leakage of the ML component’s training data can be inferred from the model’s outputs Shokri\net al. (2017); Song & Shmatikov (2019). In particular, membership inference attacks Shokri et al. (2017)\ncan infer whether a given input was used to train a model. A precondition of this attack is that there is an\ninformation flow from the training data to the component’s output.\nData poisoning. Finally, the ML component could be vulnerable to data poisoning attacks Chen et al.\n(2017b); Yang et al. (2017); Shafahi et al. (2018); Schuster et al. (2020a); Wallace et al. (2020b) if the\ntraining data can be influenced by the adversary to cause mispredictions. Data poisoning could arise if the\ncomponent’s inputs can be chosen by the attacker, and the ML’s predictions can be arbitrarily skewed (i.e.,\nlack of robustness).\nStep 3: Lift component attacks to system attacks. The final step, after identifying the preconditions\nfor vulnerabilities of the ML components, is to understand how attackers can exploit these vulnerabilities\nin context. Attack goals could be either to undermine the availability of the learned system, or violate the\nconfidentiality of a victim by inferring protected information on their data. In this work, there are two attack\nmodels we will focus on.\nConcurrent user attackers. For shared-resource learned systems, we consider a multi-user setting where the\nattacker is employing the system concurrently with the victim. We assume, here, that the system exposes\nan interface to both the attacker and the victim, and that standard access control is in place to prevent\nthe attacker from accessing victim data directly. We consider three types of attackers on shared-resource\nlearned systems: poisoning attackers who issue requests undermine the availability of the system by altering\nits state, adversarial-input attackers who undermine system availability by dispatching requests that cause\nlarge prediction errors in its internal model inference procedure, and timing attackers who infer information\nthrough query latency.\nExternal attackers. Third parties who are not using the system can still leverage the attack surface that\nML exposes, though they must do this indirectly, through either tampering attacks on the training proce-\ndure or data (e.g., externally affect the reward-signal measurements, see Section 3.4) or by utilizing a side\nchannel attack to infer training-data properties. Tampering attacks can have the same goals as poisoning or\nadversarial-input attacks, and side-channel attacks can have the same goals as timing attacks, except that\ntampering/side-channel attacks are without direct access to dispatch requests to the system. Tampering\nand side channels correspond to strictly stronger threat models than poisoning/adversarial-inputs or timing\nattacks; when both are possible.\n2.2 Traditional adversarial ML is not sufficient\nAttack models of adversarial machine learning typically target ML-as-a-service systems, where the attacker\ndirectly interacts with the model, or crowd-sourced training data, malleable even by weak attackers. This\ndoes not model learned-systems attackers, due to the following distinctive characteristics.\n5\n\nPublished in Transactions on Machine Learning Research (07/2025)\nLabel-less black-box attacks. Attackers often only hold black-box access to learned systems. On the\nface of it, this is similar to the case of (well-studied) “label-only” or “decision-only” attackers on ML-as-\na-service. In contrast, however, learned-systems attackers do not even directly see model decisions. This\nmakes traditional black-box approaches such as gradient estimation or building a surrogate model Zhao\net al. (2019); Chen et al. (2020) not directly applicable.\nLimited malleability of model inputs. Attackers’ effect on ML-model inputs is indirect. Adversaries\ncan only control system inputs (which are distinct from model inputs, see Figure 1) or externally tamper\nwith it to modify model inputs.\nLimited malleability of training data. When an attacker can affect a learned system’s training data,\nthey usually cannot directly modify it or add data points to it. To illustrate this, consider a reinforcement-\nlearning learned system that improves by training on data from its past executions. Reinforcement learning\nis particularly natural in learned systems, because the performance under the metric that the model is meant\nto optimize through its predictions, can be measured by the system post-hoc, i.e., after the model has already\nmade its prediction Marcus & Papaemmanouil (2018); Krishnan et al. (2018); Mao et al. (2016). On one\nhand, on-line reinforcement learning exposes the system to poisoning attacks as long as untrusted entities\ncan affect the system’s inputs (or tamper with it to affect model inputs). On the other hand, the fact that\nthe reward signal is measured by the system itself can pose a challenge for attackers who can only control\nits inputs and try to poison the model.\n3 Learned-Database Systems Security Survey\nNext, we want to apply the framework to several prominent classes of learned components in database\nsystems. Figure 3 illustrates our methodology, and Table 1 summarizes the findings. Crucially, we analyze\nthe risk of adversarial ML not in vacuum, but within the context of the system where the ML models are\ndeployed, and we exclude attacks for which the use of ML does not increase the attack surface.\n3.1 Learned query optimizers\nDatabases must determine and execute an execution plan , which is a sequence of basic operations—such as\njoin, sort, or filter—that computes the result of a given query. Aquery optimizer is a component responsi-\nble for generating an execution plan that minimizes resource usage, for example on memory, I/O, or CPU.\nPlans are typically expressed as computation trees where nodes are operations and leaves are relations in the\ndatabase. Query optimization is challenging because the optimizer must choose the best-performing plan\nwithout actually executing any plan. Specifically, optimizers must estimate the performance of computa-\ntion tree nodes before their inputs, or their cardinality, are known. Consequently, popular optimizers are\nthe product of massive engineering effort, with many carefully-crafted heuristics for estimating node-input\ncardinalities, choosing between different operation implementations and selecting operation order.\nQuery optimization can also be approached as a learning problem: as execution plans are carried out, the\nsystem gathers performance data. This data can then be used to refine and enhance future execution plans.\nBy continuously learning from experience, the optimizer can dynamically adapt to changing workloads.\nOnline-learned models are becoming increasingly popular Wang et al. (2024); Krishnan et al. (2018); Marcus\n& Papaemmanouil (2018); Marcus et al. (2019; 2022); Park et al. (2020).\nVulnerability preconditions met. Learnedqueryoptimizerslearnfrompast-queryexperiencetooptimize\nthe latency of future ones, thereby adding information flow from the former to the latter.\nAttacks that become possible. When databases are used by multiple users with varying privilege-\nlevels, their shared query optimizers are at risk of timing attacks that reveal information on past queries,\nand data poisoning that degrades the performance of future queries. This implies that tampering andside\nchannels are also possible (see Section 2.2). In Section 4, we demonstrate proof-of-concept of a timing attack\nagainst BAO Marcus et al. (2022), a state-of-the-art learned query optimizer.\n6\n\nPublished in Transactions on Machine Learning Research (07/2025)\nTable 1: Overview of attacks that become likely due to use of ML in internal components. We could exclude\nvery few attacks, implying that these issues are universal in learned systems.\nLearned systemAdversarial\ninputsTraining-data leakage Adversarial training data\nTiming Side channels Poisoning Tampering\nQuery optimizationa✓ ✓ ✓ ✓\nIndex structuresb✓ ✓ ✓ ✓ ✓\nLearned configurationc✓ ✓ ✓ ✓\nWorkload forecastingd✓ ✓ ✓ ✓ ✓\naWang et al. (2024); Park et al. (2020); Krishnan et al. (2018); Marcus & Papaemmanouil (2018); Marcus et al. (2019; 2022)\nbKraska et al. (2018); Nathan et al. (2020); Ding et al. (2020b); Kipf et al. (2019); Abu-Libdeh et al. (2020)\nWei et al. (2020); Ding et al. (2020a); Ferragina & Vinciguerra (2020); Galakatos et al. (2019)\ncVan Aken et al. (2017); Kanellis et al. (2020); Ma et al. (2018)\ndMa et al. (2018); Elnaffar & Martin (2004); Holze et al. (2010; 2009)\n3.2 Learned index structures\nIndex structures are key-value stores designed to support key queries, range queries, insertions, and deletions.\nCommon implementations of these structures are based on B-trees, which are height-balanced trees where\neach node has a limited number of children, and the keys are stored in sorted order in the leaves.\nKraska et al. (2018) proposed a novel perspective on index structures by viewing them as cumulative distri-\nbution functions (CDFs) that map keys to their location. They suggested using optimization techniques for\ncontinuous functions, such as gradient-based or convex optimization, to fit a CDF model to perform lookups.\nThe resulting approaches Kraska et al. (2018); Nathan et al. (2020); Ding et al. (2020b) have shown superior\nperformance compared to traditional index structures, particularly on read-only benchmarks. Specifically,\ncompared to B-trees, these learned index structures offer faster reads and require less space, which is espe-\ncially beneficial for access over networks Abu-Libdeh et al. (2020); Wei et al. (2020). More recent approaches\nalso support updates Ding et al. (2020a); Ferragina & Vinciguerra (2020); Galakatos et al. (2019).\nVulnerability preconditions met. Learned index structures have an information flow from the keys\nstored in the dataset (the training data) to the location of the record (the prediction) and the accuracy of\nthe prediction directly impacts the lookup time. Kraska et al.’s recursive model index (RMI) Kraska et al.\n(2018) and similar subsequent approaches do not offer robustness guarantees, making them less reliable\ncompared to traditional index structures. Among newer methods that support updates, PGM Ferragina\n& Vinciguerra (2020) includes robustness guarantees for all operations, but ALEX Ding et al. (2020a) and\nFITing-Tree Galakatos et al. (2019) have exponentially-worse bounds in certain insertion scenarios and, thus,\nreduced robustness compared to conventional approaches.\nAttacks that become possible. Index structures underlie many common databases MongoDB (2024a)\nthat can be shared by multiple users with varying privileges. Due to the implicit information flow from\ntraining data to the model’s predictions, learned index structures are exposed to both data poisoning attacks\nthat degrade their performance and timing attacks that snoop on data of victim users. Kornaropoulos et al.\n(2020) demonstrated a data poisoning attack that degrade the (read-only) RMI index’ performance; in\nSections 5, we show that ALEX, a newer write-enabled index, is even more vulnerable. Our attack causes\nan out-of-memory crash within seconds using a sequence of adversarial insertions. In Section 6, we further\ndemonstrate a timing attack on the PGM index, revealing that even data structures with excellent worst-case\nperformance can be exposed to attacks that their non-learned counterparts are immune to.\n3.3 Learned configuration\nDatabases have hundreds of “knobs”, i.e., parameters which control aspects such as the amount of caching\nmemory or how frequently data is flushed to storage. Tuning these parameters is a complex and time-\nconsuming task, yet it lends itself well to automation by replaying past workloads, measuring performance,\n7\n\nPublished in Transactions on Machine Learning Research (07/2025)\nand using the measurements to guide parameter-space search. Consequently, ML-based approaches are\nincreasingly popular for parameter tuning Van Aken et al. (2017); Kanellis et al. (2020); Ma et al. (2018).\nVulnerability preconditions met. Optimal parameterization can depend on (1) the stored data, (2)\nthe workload, and (3) the hardware and software stack. As a result, during automated tuning, a potential\ninformation flow is added from each of these onto virtually any decision that depend on parameterization.\nAttacks that become possible. Storeddataandworkloadmayleakbetweendifferentusersinasystemvia\ntiming measurements (i.e., timing ), or be poisoned by adversarially-crafted workloads to affect these metrics\n(i.e., data poisoning ). Perhaps more dangerously, however, learned configurations are highly susceptible to\ntampering attacksthatadversariallyaffectthehardware/softwarestackcharacteristicslearnedduringtuning.\nFor example, a process on the tuning machine may intentionally consume more memory than normal during\ntuning, so that tuning sub-optimally optimize for low memory usage. Similarly, physical adversaries can try\nto compromise the hardware during tuning (e.g. cause faults Boneh et al. (1997)). Notably, as opposed to\ntraditional tampering attacks that invoke computation or memory faults that are unintended implementation\nartifacts and physical effects Boneh et al. (1997); Gruss et al. (2016), here, the tuning procedure is actively\ntryingto learn the environment’s characteristics, which can be manipulated by an attacker.\n3.4 Workload forecasting\nWorkload forecasting predicts future workloads based on recent patterns, enabling the database to adjust its\nconfigurable parameters accordingly Ma et al. (2018); Elnaffar & Martin (2004); Holze et al. (2010; 2009).\nFor example, Ma et al. (2018) proposed to group queries to a database with highly correlated appearances\ninto clusters and then predict the short-term and long-term workloads for these clusters.\nVulnerability preconditions met. Workload forecasting introduces an information flow from past\nrequests (training data) to the database performance metrics (output).\nAttacks that become possible. Due to the added information flow, workload forecasting may introduce\nvulnerability to data poisoning and data leakage in multi-user settings. For example, in a database system,\nworkload forecasting may result in a construction of a temporary column index whenever the column is\nexpected to be heavily used. An attacker who can submit queries that use this column and observe reduced\nlatency whenever its index exists might be able to infer when the column is heavily used by others.\n3.5 Discussion\nThese areas discussed so far represent the most prominent directions we identified in the research on learned\ndatabase systems, with each being supported by multiple papers. Table 1 summarizes our findings. In almost\nall systems, the attack surface increases for almost all attack types of interest. For shared-resource systems,\nwe were able to exclude increased risk for only a few adversarial-input attacks, for two reasons: first, because\nattackers who issue requests to query optimizers can overload these systems without attacking the ML\ncomponent, by simply submitting actually-resouce-intensive queries. Second, because learned configuration\nproduces knob values that are unlikely to be any less robust than those produced by manual tuning.\nTakeaway: The incorporation of ML universally compromises shared-resource systems to adversarial\ninputs, timing, and poisoning.\nIn the upcoming sections, we validate our findings by showing that exploiting the increased attack surface\nis indeed feasible. To this end, we implement and evaluate proof-of-concept attacks in Sections 4 through 6.\nThe targets for these attacks were selected conservatively: we focus on work that was published in top-tier\nvenuesandhadopen-sourceimplementations, andselectattackgoalsthatarenon-trivialandwhereempirical\nevaluation carries actual value (for example, poisoning the BAO query optimizer by issuing the same query\nenough times will, by BAO’s design, completely eliminate the value of the learned component; we therefore\nfocus on evaluating a timing attack instead).\n8\n\nPublished in Transactions on Machine Learning Research (07/2025)\nMember Non-member\nq19 q35 q32 q39 q15 q33 q1 q37 q11 q24 q16 q3 q14 q22 q18 q26 q7 q2 q10 q5100101Seconds\n(a)SetA queries using BAO.\nq12 q21 q34 q20 q6 q25 q8 q17 q23 q4 q36 q28 q13 q27 q30 q40 q31 q9 q38 q29100101Seconds\n(b)SetB queries using BAO.\nq19 q35 q32 q39 q15 q33 q1 q37 q11 q24 q16 q3 q14 q22 q18 q26 q7 q2 q10 q510-1100101102Seconds\n(c)SetA, with BAO ML disabled.\nFigure 4: Query-answer latency for memberandnonmember queries with BAO enabled and disabled for two\ndisjoint 20-query sets, SetAandSetB. For many queries, execution time differs significantly depending on\nwhether the query was part of the model’s training set, revealing a timing side-channel. With BAO disabled,\nthis distinction disappears, confirming the leakage is due to the learned model.\n4 Case Study: The BAO Query Optimizer\nWe start by demonstrating query leakage i.e.whether a particular query was executed by the database in\nthe pastNqueries. To this end, we target the online-learned query optimizer BAO Marcus et al. (2022).\nBackground: BAO. Recallthatfortraditionalqueryoptimization, differentexecutionplansareconsidered\nto minimize resource load. BAO introduces ML to perform this selection Krishnan et al. (2018); Marcus\n& Papaemmanouil (2018); Marcus et al. (2019). In Postgres, each execution plan, known as an “arm”, is\nproduced deterministically with one set of configuration parameters that include whether to enable merge\njoin, loop join, etc. BAO improves the conventional query optimizer by selecting “arms” that steer it to\nproduce better plans. Therefore, BAO uses a variant of Thompson sampling Thompson (1933) to balance\nexploration and exploitation of its experience. Simply put, BAO retrains every Nqueries, using the N\nqueries as the training set.\nObserving and characterizing the leakage. We first assess potential leakage by performing the\nfollowing experiment. We use the Cardinality Estimation Benchmark (CEB) Negi et al. (2021b), a 13K-\nquery benchmark generated from 16 templates by mutating filter predicates. As far as we are aware, this\nis similar to the dataset on which BAO was originally evaluated by Marcus et al. (2022) (the authors did\nnot respond to our email inquiry). We use the 40-query subset of CEB used for training and benchmarking\nin Marcus et al. (2022). We divide the queries into two disjoint 20-query sets, SetAandSetB. For each set,\nwe measure each query’s latency in two scenarios, memberandnonmember , such that in memberthe query is\nin the training set, and in nonmember it is not. The former simulates the case where the model has already\nseen the queries, whereas the latter simulates a scenario where it has not. We expect the query latency to\nbe lower in the member scenario, suggesting that past experience information (i.e., the training set) leaks\nvia query latency. To isolate the effect of BAO’s learning from potential effects such as Postgres’ “native”\noptimizations like query or computation caching, we repeat this experiment but with BAO disabled.\nFigures 4a and 4b depict the results of BAO-enabled experiment for sets SetAandSetBrespectively. For\nmany queries in the BAO-enabled databases, latency in the nonmember scenario is often much larger than\nthat in the memberscenario (note the log scale), suggesting that the attacker can distinguish between the\nset using a single query. Notably, the difference is larger for slower queries that naturally require more time\n9\n\nPublished in Transactions on Machine Learning Research (07/2025)\n101102103Time (s)\nQueryArms 2 1 3 4 5\nFigure 5: Query-answer latency of different arms for 10 sampled complex queries. For each query, the latency\nis very distinct across different arms.\nto execute, implying that slower queries are more vulnerable to past-query inference attack. Figure 4c shows\nthe results for SetAwhen BAO is disabled, there is hardly any difference in the execution time between\nmemberandnonmember , suggesting that, indeed, the leakage is due to ML. The results for SetBare similar\nand not reported for conciseness.\nComplex queries leak via latency measurement. In Figure 4a and 4b, leakage is more noticeable for\nqueries with longer execution time. These queries are often complex and take more time to execute even\nwith an optimized execution plan selected by BAO. Consequently, the latency gap between member and\nnonmember setting is well above several seconds. This would allow the attacker to first estimate the latency\nof a query by training a shadow model replicating BAO, and then compare this latency estimate with the\nactual latency of execution on the deployment of BAO being attacked. Furthermore, we observe that for such\ncomplex queries, different arms lead to distinct latency values, allowing us to infer which arm was chosen by\nBAO based on the latency. We show latency measurement of 10 queries sampled from the set of complex\nqueries in Figure 5 with each of the query executed 10 times and the 95% CI for each arm shown. Clearly,\nthe runtime for each query among the five arms are very distinct with both large absolute gaps (recall the\nlog scale) and non-overlapping CIs. This observation enables us to confidently determine the arm chosen by\nBAO based on the query execution latency.\nAttacker model. Weenvisionanattackerwhosegoalistodetermineifa target query wasrecentlyexecuted\n(and thus included in the last N-query training batch). This attacker is capable of making queries to the\ndatabase using BAO, and observing their execution time. Furthermore, we assume the attacker knows the\ndetails of the learned system (including the data and different types of arms) but not the received queries.\nAttack method. Our attack performs the following steps: ❶In an off-line phase, record the execution\nlatency for each query with different arms by executing the query multiple times, forming a mapping between\narm and execution latency for each query. ❷In an off-line phase, for each query, characterize the distribution\nof arm selection by BAO by training it with various datasets. ❸Choose a target query. ❹Dispatch the\nquery to the BAO-enhanced database. ❺Measure the query execution latency, and determine which arm\nwas chosen by looking up the mapping developed in ❶. We assume our attack can measure execution latency\nwith 1-second accuracy (which is highly plausible even for a remote adversary). ❻Depending on the arm\nBAO chose, we can either be certain it was recently evaluated, be certain it was not recently evaluated, or\nquantify the precision and recall of making such predictions. We note that ❶requires that the execution\nlatency between arms for a particular query are distinct enough. Our empirical results from Figure 5 confirm\nthat this is a reasonable assumption.\nExperimental setup. We now perform a simulation of our attack scenario. We assume BAO trains using\nN= 100queries. We select 167 target queries from CEB by choosing queries whose execution plan contains\n16 join operations, which are considered fairly complex Leis et al. (2015). We construct a benign training set\nby randomly sampling 100 queries (benign queries) from CEB, and an adversarial training set by replacing\n1 randomly-chosen benign query with one of our target queries. We train BAO on the benign set and the\nadversarial set, and repeat this entire process for each target query 50 times with randomness over both\n10\n\nPublished in Transactions on Machine Learning Research (07/2025)\nTable 2: Recall and query distribution across different precision levels for both member and nonmember\nsettings. Precision groups reflect the confidence level with which the attack determines whether a query\nwas part of BAO’s recent training set. For each group, we report two key metrics: the proportion of total\nqueries that fall into the group and the average recall—that is, the frequency with which the classification\nwas correct. Our attack achieves relatively high precision while maintaining satisfying recall.\nSetting Precision Group 0.5-0.6 0.6-0.7 0.7-0.8 0.8-0.9 0.9-1.0\nmemberAverage Recall 0.56 0.54 0.37 0.28 0.24\n% of Queries 22% 24% 17% 12% 25%\nnonmemberAverage Recall 0.29 0.33 0.37 0.38 0.15\n% of Queries 2% 11% 16% 24% 47%\nbenign queries and BAO initialization. After training BAO 50 times for each target query, we obtain an arm\ndistribution for both memberandnonmember setting. We then execute the target query on the victim BAO\nmodel and identify the arm it chose based on the mapping between arm and latency obtained in the off-line\nsetting. We aim to maximize the precision of our predictions for both memberandnonmember cases.\nResults. We calculate the precision and recall of each target query for both of the memberandnonmember\nsettings. Specifically, for each query, we find the arm that leads to maximum precision classifying the two\nsettings. By doing so, we emphasize precision for each setting since being able to confidently determine\neither member ornonmember setting with some sacrifices of recall is acceptable by the attacker. Since we\nfocus on precision, we divide target queries into five precision groups and plot the portion of target queries\nfalling into those groups as well as the average recall for them. In Table 2, it can be seen that many target\nqueries belong to high precision group especially for nonmember setting with more than 40% having precision\nbetween 0.9–1.0. The corresponding average recall for these precision groups are also satisfying. For member\nsetting, average recall decreases as precision group gets higher whereas in nonmember setting, average recall\nis the highest for precision group 0.8–0.9. Altogether, these results demonstrate how sensitive information\n(the past execution of a query) may leak due to the introduction of ML in the query optimizer.\nTakeaway: Sensitive information can leak in shared-resource systems due to the introduction of ML.\n5 Case Study: The ALEX Learned Index\nNext, we examine a distinct characteristic of learned systems: optimizing for average-case performance can\nunintentionally lead to catastrophic outcomes in worst-case scenarios. To illustrate this, we consider an\nattack against the ALEX learned index.\nBackground: ALEX. ALEX is a learned index structure that supports lookups, range queries, insertions,\nbulk insertions, and deletions Ding et al. (2020a). The ALEX structure is organized in a tree hierarchy where\neach node can store a range of keys. Sibling nodes’ ranges are disjoint; internal nodes point to an array of\nnodes sorted by their ranges; leaf nodes point to a sorted array of keys. Each node contains a linear function\n(of the form f(k) =ak+b, witha,b∈R), that maps a key to its containing-range sub-tree (for internal\nnodes) or its position (for leaf nodes). Lookups are performed by following the linear models with predictions\nrounded to the nearest integer since array positions are discrete. In case of prediction errors, exponential\nsearch is used to find the correct position in the sorted array, starting from the predicted position.\nInsertion internals. ALEX optimizes for fast insertions and therefore includes redundant positions for key\nvalues and internal-node pointers. The first type of redundancy is the use of gapped arrays in leaf nodes.\nThese allow quick insertions into locations where there is a gap; when a key needs to be inserted within a\ncontiguous sequence of keys, ALEX shifts keys toward the nearest gaps to create space. This operation can\nbe costly in the worst case, where the number of shifts equals the number of keys in the node. However,\nthere are even worse scenarios discussed next.\n11\n\nPublished in Transactions on Machine Learning Research (07/2025)\nP\nA B B'PAB, B' 1\nC B' C'PBC, C' 2\nD' D B' C'PCD, D' 3\nFigure 6: Repeated sideway splits to a subtree’s leftmost node cause exponential memory blowup. First,\nnode A, child node of Pwith 4 redundant pointers, is split into B,B′(Step❶), then BtoC,C′(Step❷),\nthenCtoD,D′(Step❸). On the last split, there are no redundant pointers into D, and more memory is\nallocated by doubling P’s pointer array size.\nSpecifically, when a node reaches its upper density bound (i.e., becomes too full for further insertions), or\nwhen ALEX’s internal heuristics determine that the node’s lookup or insertion performance is suboptimal\ndue to overpopulation, ALEX performs a split. The preferred method is a sideways split , which requires\na second type of redundancy: duplicate pointers. In this context, internal nodes’ pointer arrays contain\nduplicate pointers to the same child nodes. During a sideways split, a child node with duplicate pointers is\ndivided into two new nodes, each covering half of the original node’s key range. The duplicate pointers in\nthe parent node are then split between these new nodes. This method ensures that internal nodes do not\nneed to be retrained because the linear model mapping keys to subtrees remains accurate after the split. If\nno duplicate pointers exist during a sideways split, ALEX allocates more pointers by doubling the size of\nthe parent node, using the extra slots to double the number of doubled pointers to each of each child nodes.\nOnce the parent array reaches a maximum size, the parent can also perform a sideway split. Only when all\npredecessors’sizesreachthemaximumsize, ALEXwillperforma downward split . Inadownwardsplit, asplit\nnode’s keys are reallocated into two new nodes, and the original node converts into an internal node whose\nchildren are the two new nodes. Downwards splits are straightforward and do not require any redundancy\nbut they increase the tree’s depth. For this reason, downward splits are only used as a last resort when a\nsideways split is no longer possible.\nWorst-case trade-off. The splitting mechanism can lead to exponential memory growth . Consider the\nscenario illustrated in Figure 6. Here a child node Ahas four duplicate keys into it, and sideway-splits due\nto overpopulation into new nodes B(on the right) and B′(on the left), and then Bsideway-splits again\nintoCandC′, again on the right and left, then Ccan no longer split sideway unless the parent node array\ndoubles its size to allocate a duplicate pointer into C. We observe, that if Cnow splits again into D,D′,\nandDintoE,E′and so forth, then the parent array size grows exponentially with each split, until it hits\nthe maximal node size (for ALEX’s default parameterization, 16MB).\nAs a result, ALEX permits a worst-case scenario where memory usage can blowup exponentially to attain\ngood average-case performance. This occurs because the parent node is never retrained and cannot reallocate\nthe leftmost key range to subtrees other than the leftmost child’s. According to Ding et al., this choice was\nmade because retraining is costly.\nPoisoning attack on ALEX. Our attack has one simple capability: insert keys into the index . The\nattacker’s goal is to leverage the above-described effect to jeopardize the availability of ALEX by increasing\nits CPU and memory consumption. This can result in (1) much higher maintenance costs, for example\nbecause the cloud instance running ALEX must be of a higher tier, and (2) if the index is serving other users\nin addition to the attacker, their requests will be delayed, and (3) it can ultimately lead to a full denial of\nservice, if ALEX crashes due to being out of memory.\nWe assume the attack has minimal knowledge of the key distribution the ALEX structure contains and only\nroughly knows the range of keys used. For the attack, the adversary then guesses a random position in the\nkey range and start inserting keys to the left of this position. Our observation is that this causes repeated\ninsertions to the leftmost node within a sub-tree, and therefore, as described before, exponential memory\nblowup due to a series of sideway splits where, for each split, the parent node doubles the size of its pointer\narray. For more details refer to Appendix A.1 where we prove this informally.\n12\n\nPublished in Transactions on Machine Learning Research (07/2025)\n101102Memory Usage (in GB)0 300 200 100 600 800 400 200 0\nCumulative Insertions \n(in Millions)Runtime \n(in Seconds)\n(a) Longitudes Dataset\nBenign Adversarial\n0 250 200 150 100 600 400 200 0 50\nCumulative Insertions \n(in Millions)Runtime \n(in Seconds) (b) YCSB Dataset\nFigure 7: Memory usage as a function of number of insertions and runtime.\nEvaluation. We evaluate the attack on two datasets from the original publication Ding et al. (2020a).\nFirst, the YCSBdataset, containing user IDs from the YCSB benchmark Cooper et al. (2010), and second\nthelongitudes dataset with longitudes of locations around the world from Open Street Maps. Each dataset\ncontains 200 million records. In both cases, we first build an ALEX index for the first 10 million elements\nby instantiating the index and bulk-loading the keys. Then, we simulate an adversary who inserts keys\nusing the method described above. For comparison, we also simulate a scenario where a benign user inserts\nnon-maliciously-crafted keys drawn sequentially, similarly to the experiments from Ding et al.\nWe run ALEX on a server with two Intel Xeon E5-2686 CPUs and 512GB of memory. We use ALEX’s\nbenchmark program and supply a binary file containing (1) the keys to bulk-load and (2) the attacker and\nvictim’s keys for insert/lookup operations. We set up the benchmark program to read 1 million operations\nat a time from the file, and report their performance statistics. To prototype how ALEX would behave on\na machine that is not geared with an unusually high amount of RAM, we additionally run the attack on a\nmachine with an Intel i7-7600U CPU and 16GB of memory.\nResults. In Figure 7a and 7b, we plot memory usage as a function of number of insertions and runtime on\nLongitudes and YCSB dataset. In the benign scenarios, ALEX’s memory usage peaks at just over 10 GB\nthroughout the experiment. However, under the attack, memory usage exceeds 100 GB within 136 and 107\nseconds, and crashes due to out-of-memory in 15 and 12 minutes (resp.). Since the adversary’s insertions\nrequire many expensive memory allocations, it slows down ALEX by a factor of almost ×4. The attack fills\nup ALEX’s memory slightly slower for the YCSB dataset, likely due to its sparser key range. We empirically\nverify this in Appendix A.2.\nOn the machine with 16GB of memory, ALEX crashes within 14 seconds after the attack initiated, after 14\nmillion insertion operations by the attacker.\nDetectability. ALEX can try to detect the scenario of a single user who dispatches many insertion oper-\nations, of which a large proportion cause node splits, and block the user. This would require breaking the\nstandard abstractions in many systems, in which index structures are decoupled from user authentication/i-\ndentity mechanisms (e.g. of a database that internally uses the index). The defense’s detection threshold\nwould then have to be carefully tuned for each deployment, depending on the expected benign workload and\nmemory increase rate. Unfortunately, even by paying that price, this would not be sufficient: the adversary\ncan slow down the attack as much as needed, and interleave their insertions with other operations, to stay\nundetected while still causing the highest memory increase rate allowed by the system. If at any point the\nattacker is detected, they can potentially create, or collude with, another account.\nTakeaway: Replacing worst-case guarantees with average-case performance is a reliable signal of a\nvulnerability to adversarial ML in learned systems.\n13\n\nPublished in Transactions on Machine Learning Research (07/2025)\n6 Case Study: The PGM Learned Index\nThe attack on ALEX leverages our ability to craft insertion operations that cause a worst-case scenario, in\nterms of memory and time. We would not be able to cause such a catastrophic failing in a data structure\nthat has tighter worst-case behavior bounds. The PGM index Ferragina & Vinciguerra (2020) is a dynamic\nlearned index that supports updates and insertions, while being meticulously engineered to bound worst-case\n(amortized) complexity, in both memory and time. For example, index construction works in linear time,\nmembership queries work in squared-logarithmic time, and insertions work in logarithmic amortized time.\nIt is tempting to deduce that, due to its excellent worst-case behavior, the PGM index will be spared from\nML vulnerabilities.\nIn this section, we explain and demonstrate that the very property that makes it so performant — namely\nfitting the index structure to the key distribution — inherently introduces information leakage when a\nPGM index is used by multiple users whose data is meant to be segregated (e.g., users of a database, see\nSection 3.2). PGM’s worst-case guarantees are provided for availability without consideration of the risk for\nprivacy leakage. Our results show that it is possible for mechanisms that provide worst-case guarantees to\nstill introduce new vulnerability dimensions for security through runtime differences.\nBackground: the PGM index. At the core of PGM is a linear-time streaming algorithm that finds the\noptimal piecewise-linear function which approximates, up to an additive error parameter ϵ, the key-to-index\nmapping for a (sorted) array of keys. PGM applies this algorithm to build a multi-level index, where each\nlevelisapiecewise-linearapproximationofthedataonthenextlevel, andthepenultimatelayerapproximates\nthe actual key-to-index mapping. This structure does not contain gaps like ALEX, and additional machinery\nis required to support efficient insertions (see Ferragina & Vinciguerra (2020)).\nSince each level’s linear model is an ϵ-approximation of the key positions in the level below it, lookups\nare performed by traversing the index from top to bottom, getting the model’s prediction pfor the key’s\nposition, and performing a binary search for the key’s actual position within [p−ϵ,p+ϵ]. This implies\nthat the number of operations performed during a lookup depends only on ϵ, which is a constant parameter\nregardless of the data, and the index’s height.\nAttack: distinguishing two key distributions. Indexes are often underlying databases serving multiple\nusers with varying privilege levels MongoDB (2024a;b), where it is required that users’ data is properly\ncompartmentalized. We will show that, simply by measuring query latency of accessing their own keys , an\nattacker can easily tell between two different victim key distributions, due to the index size being dependent\non them, dramatically affecting attacker’s latency.\nExperimental setup. We load datasets of equal size, drawn from two key distributions, onto PGM’s\nopen-source implementation: A, a normal distribution with mean µ1= 5e6and standard deviation σ1= 1e6,\nand B, a mixture of 50 normal distributions with σ2=σ1/50and means µ2∈{0,2µ2,...,50·2µ}where\neach of the distributions is weighted equally. We add 100 attacker keys to each dataset, uniformly from/bracketleftbig\nµ1−2σ1,µ1+ 2σ1/parenrightbig\n(within the approximate range of keys in both AandB). We make the assumption that\nthose are the only keys the attacker is allowed to access.\nWe generate the datasets by sampling from the above distributions, built the index, and measure (1) index\nheight (number of traversed levels while searching queries, akin to tree height for tree-based indexes) and\n(2) the attack’s query latency. We repeat this 50 times for multiple values of ϵ, and average the results.\nWe also take the same latency measurements for a popular B-tree CPP implementation B-tree (2011),\nand randomized insertion order when constructing the B-tree. We note that, by construction, B-trees are\noblivious to key values, and only observe the key order (and insertion order). Therefore, we expect the\nB-tree to be far less sensitive to the differences between key distributions, as long as the datasets contain\nthe same number of keys. We repeat this entire experiment again, this time freeing the index’s memory\nand reallocating it between each two attacker queries, to mitigate any effects of leakage stemming from\nmicroarchitectural elements such as caches. We run our experiments on a machine with a Intel i9-9940X\nCPU and 128 GB of memory.\n14\n\nPublished in Transactions on Machine Learning Research (07/2025)\nTable 3: PGM’s height and attacker’s latency measurements (nanoseconds) for datasets of different key\ndistributions. Distribution Bresults in comparatively flatter indexes, resulting in quicker query times for\nPGM. Measurements are averaged across 50 experiments.\nModelA B\nHeight Latency Height Latency\nPGM,ϵ= 8 3.0 165.14 3.0 146.52\nPGM,ϵ= 16 3.0 126.46 3.0 120.32\nPGM,ϵ= 32 3.0 134.18 2.52 113.98\nPGM,ϵ= 64 3.0 142.62 2.2 118.06\nPGM,ϵ= 128 3.0 157.64 2.0 124.3\nPGM,ϵ= 256 3.0 170.06 2.0 126.68\nPGM,ϵ= 512 3.0 183.24 2.0 135.08\nPGM,ϵ= 1024 3.0 191.46 2.0 142.36\nB-tree - 112.28 - 99.32\nTable 4: PGM’s height and attacker’s latency measurements (nanoseconds) for datasets of different key\ndistributions, in the setting when we free the index’s memory and rebuild it between every two queries.\nDistribution Bresults in comparatively flatter indexes, resulting in quicker query times for PGM. B-trees do\nnot exhibit this leakage. Measurements are averaged across 50 experiments.\nModelA B\nHeight Latency Height Latency\nPGM,ϵ= 8 3.0 204.38 3.0 181.54\nPGM,ϵ= 16 3.0 206.76 3.0 183.08\nPGM,ϵ= 32 3.0 227.32 2.66 187.84\nPGM,ϵ= 64 3.0 240.12 2.06 194.1\nPGM,ϵ= 128 3.0 270.84 2.0 219.04\nPGM,ϵ= 256 3.0 287.48 2.0 233.04\nPGM,ϵ= 512 3.0 308.64 2.0 256.92\nPGM,ϵ= 1024 3.0 329.96 2.0 276.12\nB-tree - 185.2 - 185.52\nResults. Table3summarizestheresultsforthefirstexperiment. Asexpected,distribution Aresultsinmore\nlevels for PGM to traverse, and correspondingly, higher query latency. There is somedifference in latency\nmeasurements for B-trees, as well, albeit much smaller, and it stems from microarchitectural constraints and\noptimizations. In contrast, the leakage for PGM is algorithmic : PGM has to perform more operations for\ndistributions whose functions are more difficult to approximate using its piecewise-linear approach. Table 4\nshows the results for the experiment where we rebuild the index between every two queries, to mitigate\noperating system and hardware side-effects. PGM remains leaky, whereas B-trees exhibit consistent timing\nacross the two distributions.\nTakeaway: Excellent worst-case behavior does not inoculate a system against attacks. In learned\nsystems that are shared between multiple users, leakage is inherent to the very quality that makes ML\nattractive: the system’s ability to adapt to observed input patterns.\n15\n\nPublished in Transactions on Machine Learning Research (07/2025)\n7 Related Work\nIn this work, we systematically examine the security implications of integrating learned components into\ndatabase systems and, specifically, focus on adversarial inputs, poisoning, and leakage attacks.\nAdversarial ML. Other types of attacks also exist, such as distribution-chain attacks , where the adver-\nsary directly manipulates software at its source (like manipulating weight values Gu et al. (2017); Kurita\net al. (2020); Schuster et al. (2020b) or training code Bagdasaryan & Shmatikov (2020)), or model stealing\nattacks Juuti et al. (2019); Wallace et al. (2020a); Tramèr et al. (2016) that train a model to imitate an\nexposed prediction API. We note that these latter ML vulnerabilities are less pertinent to our investiga-\ntion, as they are not products of learning per se and not logically exclusive learned models — for example,\ndistribution-chain attacks exist for all software, and non-learned algorithms can also be “extracted” (i.e.,\nstolen) via reverse engineering.\nLearned systems. Complementary to our exploration, Apruzzese et al. (2023) investigate the gap between\nresearch on attacking ML-as-a-service systems and actual attacks on deployed ML models in practice. Like\nour work, they consider ML models as components of a broader system. However, their focus is on the\ndiscrepancy between research scenarios and practical applications. Furthermore, Debenedetti et al. (2024)\ninvestigate privacy-related side channels created by learned components.\nMicroarchitectural side-channels. Microarchitectural attacks such as cache attacks Osvik et al. (2006);\nYarom & Falkner (2014) are a universal threat to any software that handles sensitive data, illustrating the\ndanger of inadvertent information flow through shared resources, i.e. the same property that characterizes\nhow ML is being used in software systems.\nAttacks on data structures. Crosby & Wallach (2003) observed in 2003 that many commonly used data\nstructures are optimized for average-case performance yet have expensive edge cases that can be maliciously\ninvoked, leading to attacks like HashDoS attack Wälde & Klink (2011). Our attack on ALEX shows how\nthe advent of learned systems can reintroduce this risk. Other prior works considered attacks on traditional\ndata structures, for example a data-leakage attack that recovers B-tree training data (i.e., the indexed keys)\nbut requires writing into the index to detect node splits Futoransky et al. (2007). Kornaropoulos et al.\n(2020) mounted the first attack on a recently-proposed learned system, a poisoning attack on the original\nread-only RMI index. In a more recent and concurrent work, Yang et al. (2023) present a poisoning attack\nagainst ALEX similar to our investigation. Both works, however, did not consider other learned systems, nor\nsystematically study the root causes of ML vulnerabilities and their manifestation across learned-systems.\n8 Mitigations\nOur core framework of thinking about learned components helps audit systems and identify potential vul-\nnerabilities; however, it is a manual process which requires domain expertise in both ML and the system.\nHere, we consider other approaches.\nWorst-case complexity guarantees. Poorperformanceonedgecasescanhavecatastrophicconsequences\nsince it can be adversarially invoked, as our attack on ALEX demonstrates. It is thus important to require\nthat replacements of traditional systems maintain performance bounds even at the worst case. However, our\nattack on BAO does not rely on worst-case behavior, and our attack on PGM shows that adversarial-ML\nleakage is possible even with excellent worst-case bounds, so robustness is not a panacea.\nConstant-time responses. Timing attacks can be mitigated by adopting constant-time responses;\nhowever, this would also nullify any benefit from the use of ML that is meant to optimize response times.\nAlgorithmic ML defenses. An extensive line of work revolves around fortifying ML against privacy\nleakage Abadi et al. (2016); Song et al. (2013), adversarial inputs Xie et al. (2019); Chen et al. (2017a), and\npoisoning Udeshi et al. (2019); Qiao et al. (2019); Liu et al. (2018); Xu et al. (2019). While certain data\n16\n\nPublished in Transactions on Machine Learning Research (07/2025)\nleakage is inherent to shared learned systems (see Section 6), it may be somewhat mitigated by the use of,\nfor example, differential privacy. Such solutions have expensive trade-offs, and they are never considered\nin work on learned systems including, to our knowledge, caches or other leaky microarchitectural elements.\nTherefore, the impact of this on performance remains an open question. Furthermore, separate mechanisms\nare required to identifythe threats, before algorithmic approaches can be used.\nCall to action: The efficacy and costs of using algorithmic defenses such as differential privacy on\nlearned systems requires more extensive research.\nTracking information flows. Many ML vulnerabilities stem from increased information flows, not\ndecreased robustness. While robustness guarantees are often clearly analyzed when constructing learned\nsystems Kraska et al. (2018); Ding et al. (2020a); Galakatos et al. (2019); Ferragina & Vinciguerra (2020),\ninformation flows never are, so their risks remain implicit. Approaches such as taint analysis Enck et al.\n(2014); Arzt et al. (2014) or information-flow control Zdancewic et al. (2002); Myers (1999); Krohn et al.\n(2007) may be helpful in understanding ML deployment in context. Such approaches pose a burden for devel-\nopers Zdancewic (2004), which may be significantly alleviated in ML by tailoring them for the programming\nabstractions common in ML, for example by adding security labels to (coarse-grained) processing steps or\n(finer-grained) Tensor variables. Combining such approaches with ML defenses that act as label declassifiers\nor endorsements is an entirely unstudied direction with fascinating open questions, for example “what effect\nshould anϵ-differentially-private mechanism have on it’s input’s privacy label”? Exploring these questions\nhas the potential to influence ML pipelines more broadly and are not limited to learned systems.\nCall to action: Work on learned systems need to analyze and make explicit added information flows\nand their dangers, similarly to the way worst-case bounds (or lack thereof) should be reported.\nCall to action: We call for information-flow analysis and control for ML abstractions like data-pipeline\nprocessing steps and Tensor variables. We envision approaches that incorporate algorithmic defenses\nas declassifiers and endorsements.\n9 Conclusion\nThis work systematically discusses the security implications of a new and rapidly-growing trend in the use of\nML in database system: employing ML models as internal system components to improve performance. We\npropose a framework for identifying and reasoning about vulnerabilities this practice can introduce, use it to\nidentify potential threats to various classes of learned components, and validate it by evaluating identified\nattacks against three prominent ones. Our findings highlight the dangers of incorporating learned system\ncomponents, especially those that are shared between mutually-distrusting users. We discuss mitigations and\ncall to action on explicitly addressing known risks when developing learned systems, as well as on studying\nkey types of attacks and defenses to understand their real-world security implications.\nImpact Statement\nAs ML becomes a core part of database systems, its integration raises new security and privacy concerns—\nespecially when these systems handle sensitive data or are shared by untrusted users. This work exposes\na novel attack surface introduced by learned components and provides a framework for analyzing such\nvulnerabilities. In doing so, it enables more secure design of future systems and guides research on defenses\ntailored to this emerging threat model. While any disclosure of vulnerabilities carries some risk of misuse,\nwe believe that transparent analysis is critical to building trustworthy systems and advancing the security\nof learning-based infrastructure.\n17\n\nPublished in Transactions on Machine Learning Research (07/2025)\nAcknowledgments\nWe would like to acknowledge our sponsors, who support our research with financial and in-kind contri-\nbutions: CIFAR through the Canada CIFAR AI Chair program, DARPA through the GARD program,\nIntel, NFRF through an Exploration grant, NSERC through the Discovery Grant and COHESA Strategic\nAlliance, Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) through the project AL-\nISON (492020528), and the European Research Council (ERC) through the consolidator grant MALFOY\n(101043410). Resources used in preparing this research were provided, in part, by the Province of Ontario,\nthe Government of Canada through CIFAR, and companies sponsoring the Vector Institute. We would like\nto thank members of the CleverHans Lab for their feedback.\nReferences\nMartinAbadi, AndyChu, IanGoodfellow, HBrendanMcMahan, IlyaMironov, KunalTalwar, andLiZhang.\nDeep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer\nand communications security , pp. 308–318, 2016.\nHussam Abu-Libdeh, Deniz Altınbüken, Alex Beutel, Ed H Chi, Lyric Doshi, Tim Kraska, Andy Ly,\nChristopher Olston, et al. Learned indexes for a google-scale disk-based database. arXiv preprint\narXiv:2012.12501 , 2020.\nGiovanni Apruzzese, Hyrum S. Anderson, Savino Dambra, David Freeman, Fabio Pierazzi, and Kevin A.\nRoundy. \"real attackers don’t compute gradients\": Bridging the gap between adversarial ML research and\npractice. In IEEE Conference on Secure and Trustworthy Machine Learning (SaTML) , 2023.\nSteven Arzt, Siegfried Rasthofer, Christian Fritz, Eric Bodden, Alexandre Bartel, Jacques Klein, Yves\nLe Traon, Damien Octeau, and Patrick McDaniel. Flowdroid: Precise context, flow, field, object-sensitive\nand lifecycle-aware taint analysis for android apps. Acm Sigplan Notices , 49(6):259–269, 2014.\nB-tree. Google c++ b-tree. https://code.google.com/archive/p/cpp-btree/ , 2011.\nEugene Bagdasaryan and Vitaly Shmatikov. Blind backdoors in deep learning models. arXiv preprint\narXiv:2005.03823 , 2020.\nDan Boneh, Richard A DeMillo, and Richard J Lipton. On the importance of checking cryptographic\nprotocols for faults. In International conference on the theory and applications of cryptographic techniques ,\npp. 37–51. Springer, 1997.\nSurajit Chaudhuri, Bolin Ding, and Srikanth Kandula. Approximate query processing: No silver bullet. In\nProceedings of the 2017 ACM International Conference on Management of Data, SIGMOD Conference\n2017, Chicago, IL, USA, May 14-19, 2017 , 2017.\nJianbo Chen, Michael I Jordan, and Martin J Wainwright. Hopskipjumpattack: A query-efficient decision-\nbased attack. In 2020 ieee symposium on security and privacy (sp) , pp. 1277–1294. IEEE, 2020.\nRobert Chen, Brendan Lucier, Yaron Singer, and Vasilis Syrgkanis. Robust optimization for non-convex\nobjectives. arXiv preprint arXiv:1707.01047 , 2017a.\nXinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning\nsystems using data poisoning. arXiv:1712.05526 , 2017b.\nJeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing.\nInInternational Conference on Machine Learning , pp. 1310–1320. PMLR, 2019.\nBrian F Cooper, Adam Silberstein, Erwin Tam, Raghu Ramakrishnan, and Russell Sears. Benchmarking\ncloud serving systems with ycsb. In Proceedings of the 1st ACM symposium on Cloud computing , pp.\n143–154, 2010.\nScott A Crosby and Dan S Wallach. Denial of service via algorithmic complexity attacks. In USENIX\nSecurity Symposium , pp. 29–44, 2003.\n18\n\nPublished in Transactions on Machine Learning Research (07/2025)\nEdoardo Debenedetti, Giorgio Severi, Milad Nasr, Christopher A. Choquette-Choo, Matthew Jagielski, Eric\nWallace, Nicholas Carlini, and Florian Tramèr. Privacy side channels in machine learning systems. In\n33rd USENIX Security Symposium, USENIX Security , 2024.\nJialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li, Hantian Zhang, Badrish\nChandramouli, Johannes Gehrke, Donald Kossmann, et al. Alex: an updatable adaptive learned index. In\nProceedings of the 2020 ACM SIGMOD International Conference on Management of Data , pp. 969–984,\n2020a.\nJialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. Tsunami: A learned multi-dimensional\nindex for correlated data and skewed workloads. arXiv preprint arXiv:2006.13282 , 2020b.\nSaid S Elnaffar and Patrick Martin. An intelligent framework for predicting shifts in the workloads of\nautonomic database management systems. In Proceedings of IEEE international conference on advances\nin intelligent systems–theory and applications , pp. 15–18, 2004.\nWilliam Enck, Peter Gilbert, Seungyeop Han, Vasant Tendulkar, Byung-Gon Chun, Landon P Cox, Jaeyeon\nJung, PatrickMcDaniel, and Anmol NSheth. Taintdroid: an information-flow trackingsystem for realtime\nprivacy monitoring on smartphones. ACM Transactions on Computer Systems (TOCS) , 32(2):1–29, 2014.\nPaolo Ferragina and Giorgio Vinciguerra. The pgm-index: a fully-dynamic compressed learned index with\nprovable worst-case bounds. Proceedings of the VLDB Endowment , 13(8):1162–1175, 2020.\nAriel Futoransky, Damián Saura, and Ariel Waissbein. The nd2db attack: Database content extraction using\ntiming attacks on the indexing algorithms. In WOOT, 2007.\nAlex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and Tim Kraska. Fiting-tree: A\ndata-aware index structure. In Proceedings of the 2019 International Conference on Management of Data ,\npp. 1189–1206, 2019.\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples.\narXiv preprint arXiv:1412.6572 , 2014.\nDaniel Gruss, Clémentine Maurice, and Stefan Mangard. Rowhammer. js: A remote software-induced fault\nattack in javascript. In International conference on detection of intrusions and malware, and vulnerability\nassessment , pp. 300–321. Springer, 2016.\nTianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the machine\nlearning model supply chain. arXiv preprint arXiv:1708.06733 , 2017.\nMarc Holze, Claas Gaidies, and Norbert Ritter. Consistent on-line classification of dbs workload events.\nInProceedings of the 18th ACM conference on Information and knowledge management , pp. 1641–1644,\n2009.\nMarc Holze, Ali Haschimi, and Norbert Ritter. Towards workload-aware self-management: Predicting sig-\nnificant workload shifts. In 2010 IEEE 26th International Conference on Data Engineering Workshops\n(ICDEW 2010) , pp. 111–116. IEEE, 2010.\nMika Juuti, Sebastian Szyller, Samuel Marchal, and N Asokan. Prada: protecting against dnn model stealing\nattacks. In 2019 IEEE European Symposium on Security and Privacy (EuroS&P) , pp. 512–527. IEEE,\n2019.\nKonstantinos Kanellis, Ramnatthan Alagappan, and Shivaram Venkataraman. Too many knobs to tune?\ntowards faster database tuning by pre-selecting important knobs. In 12th{USENIX}Workshop on Hot\nTopics in Storage and File Systems (HotStorage 20) , 2020.\nAndreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper, Tim Kraska, and Thomas\nNeumann. Sosd: A benchmark for learned indexes. arXiv preprint arXiv:1911.13014 , 2019.\n19\n\nPublished in Transactions on Machine Learning Research (07/2025)\nEvgenios M. Kornaropoulos, Silei Ren, and Roberto Tamassia. The price of tailoring the index to your data:\nPoisoning attacks on learned index structures, 2020.\nTim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned index\nstructures. In Proceedings of the 2018 International Conference on Management of Data , pp. 489–504,\n2018.\nSanjay Krishnan, Zongheng Yang, Ken Goldberg, Joseph Hellerstein, and Ion Stoica. Learning to optimize\njoin queries with deep reinforcement learning. arXiv preprint arXiv:1808.03196 , 2018.\nMaxwellKrohn, AlexanderYip, MicahBrodsky, NatanCliffer, MFransKaashoek, EddieKohler, andRobert\nMorris. Information flow control for standard os abstractions. ACM SIGOPS Operating Systems Review ,\n41(6):321–334, 2007.\nKeita Kurita, Paul Michel, and Graham Neubig. Weight poisoning attacks on pre-trained models. arXiv\npreprint arXiv:2004.06660 , 2020.\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.\nViktor Leis, Andrey Gubichev, Atanas Mirchev, Peter Boncz, Alfons Kemper, and Thomas Neumann. How\ngood are query optimizers, really? Proceedings of the VLDB Endowment , 9(3):204–215, 2015.\nKaiyu Li and Guoliang Li. Approximate query processing: What is new and where to go? - A survey on\napproximate query processing. Data Sci. Eng. , 2018.\nKangLiu, BrendanDolan-Gavitt, andSiddharthGarg. Fine-pruning: Defendingagainstbackdooringattacks\non deep neural networks. In RAID, 2018.\nLin Ma, Dana Van Aken, Ahmed Hefny, Gustavo Mezerhane, Andrew Pavlo, and Geoffrey J Gordon. Query-\nbased workload forecasting for self-driving database management systems. In Proceedings of the 2018\nInternational Conference on Management of Data , pp. 631–645, 2018.\nHongzi Mao, Mohammad Alizadeh, Ishai Menache, and Srikanth Kandula. Resource management with deep\nreinforcement learning. In Proceedings of the 15th ACM Workshop on Hot Topics in Networks , pp. 50–56,\n2016.\nHongzi Mao, Malte Schwarzkopf, Shaileshh Bojja Venkatakrishnan, Zili Meng, and Mohammad Alizadeh.\nLearning scheduling algorithms for data processing clusters. In Proceedings of the ACM Special Interest\nGroup on Data Communication , pp. 270–288. 2019.\nRyan Marcus and Olga Papaemmanouil. Deep reinforcement learning for join order enumeration. Proceedings\nof the First International Workshop on Exploiting Artificial Intelligence Techniques for Data Management\n- aiDM’18 , 2018. doi: 10.1145/3211954.3211957. URL http://dx.doi.org/10.1145/3211954.3211957 .\nRyan Marcus, Parimarjan Negi, Hongzi Mao, Chi Zhang, Mohammad Alizadeh, Tim Kraska, Olga Pa-\npaemmanouil, and Nesime Tatbul. Neo: A learned query optimizer. arXiv preprint arXiv:1904.03711 ,\n2019.\nRyan Marcus, Parimarjan Negi, Hongzi Mao, Nesime Tatbul, Mohammad Alizadeh, and Tim Kraska. Bao:\nMaking learned query optimization practical. ACM SIGMOD Record , 51(1):6–13, 2022.\nMongoDB. Indexes — mongodb manual. https://docs.mongodb.com/manual/indexes/ , 2024a.\nMongoDB. Manage users and roles — mongodb manual. https://docs.mongodb.com/manual/tutorial/\nmanage-users-and-roles/ , 2024b.\nMSSQL. Permissions (database engine) — sql server | microsoft docs. https://docs.\nmicrosoft.com/en-us/sql/relational-databases/security/permissions-database-engine?\nview=sql-server-ver15 , 2024.\n20\n\nPublished in Transactions on Machine Learning Research (07/2025)\nAndrew C Myers. Jflow: Practical mostly-static information flow control. In Proceedings of the 26th ACM\nSIGPLAN-SIGACT symposium on Principles of programming languages , pp. 228–241, 1999.\nVikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. Learning multi-dimensional indexes. In\nProceedings of the 2020 ACM SIGMOD International Conference on Management of Data , SIGMOD ’20,\npp. 985–1000, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450367356.\ndoi: 10.1145/3318464.3380579. URL https://doi.org/10.1145/3318464.3380579 .\nParimarjan Negi, Matteo Interlandi, Ryan Marcus, Mohammad Alizadeh, Tim Kraska, Marc\nFriedman, and Alekh Jindal. Steering query optimizers: A practical take on big data\nworkloads. In 2021 International Conference on Management of Data , pp. 2557–2569.\nACM, June 2021a. URL https://www.microsoft.com/en-us/research/publication/\nsteering-query-optimizers-a-practical-take-on-big-data-workloads/ .\nParimarjan Negi, Ryan Marcus, Andreas Kipf, Hongzi Mao, Nesime Tatbul, Tim Kraska, and Mohammad\nAlizadeh. Flow-loss: Learning cardinality estimates that matter. arXiv preprint arXiv:2101.04964 , 2021b.\nDag Arne Osvik, Adi Shamir, and Eran Tromer. Cache attacks and countermeasures: the case of aes. In\nCryptographers’ track at the RSA conference , pp. 1–20. Springer, 2006.\nYongjoo Park, Shucheng Zhong, and Barzan Mozafari. Quicksel: Quick selectivity learning with mixture\nmodels. In Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data , pp.\n1017–1033, 2020.\nXiming Qiao, Yukun Yang, and Hai Li. Defending neural backdoors via generative distribution modeling.\nInNeurIPS , 2019.\nRoei Schuster, Tal Schuster, Yoav Meri, and Vitaly Shmatikov. Humpty dumpty: Controlling word meanings\nvia corpus poisoning. arXiv preprint arXiv:2001.04935 , 2020a.\nRoei Schuster, Congzheng Song, Eran Tromer, and Vitaly Shmatikov. You autocomplete me: Poisoning\nvulnerabilities in neural code completion. arXiv preprint arXiv:2007.02220 , 2020b.\nAli Shafahi, W. Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom\nGoldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks, 2018.\nReza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against\nmachine learning models. In 2017 IEEE Symposium on Security and Privacy (SP) , pp. 3–18. IEEE, 2017.\nCongzheng Song and Vitaly Shmatikov. Auditing data provenance in text-generation models. In Proceedings\nof the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pp.196–206,\n2019.\nShuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with differentially\nprivate updates. In 2013 IEEE Global Conference on Signal and Information Processing , pp. 245–248.\nIEEE, 2013.\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and\nRob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199 , 2013.\nWilliam R Thompson. On the likelihood that one unknown probability exceeds another in view of the\nevidence of two samples. Biometrika , 25(3/4):285–294, 1933.\nFlorian Tramèr, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart. Stealing machine learning\nmodels via prediction apis. In 25th{USENIX}Security Symposium ( {USENIX}Security 16) , pp. 601–618,\n2016.\nEran Tromer, Dag Arne Osvik, and Adi Shamir. Efficient cache attacks on aes, and countermeasures. Journal\nof Cryptology , 23(1):37–71, 2010.\n21\n\nPublished in Transactions on Machine Learning Research (07/2025)\nSakshi Udeshi, Shanshan Peng, Gerald Woo, Lionell Loh, Louth Rawshan, and Sudipta Chattopadhyay.\nModel agnostic defence against backdoor attacks in machine learning. arXiv:1908.02203 , 2019.\nDana Van Aken, Andrew Pavlo, Geoffrey J Gordon, and Bohan Zhang. Automatic database management\nsystem tuning through large-scale machine learning. In Proceedings of the 2017 ACM International Con-\nference on Management of Data , pp. 1009–1024, 2017.\nJ Wälde and A Klink. Hash collision dos attacks. 28c3, 2011.\nEric Wallace, Mitchell Stern, and Dawn Song. Imitation attacks and defenses for black-box machine trans-\nlation systems. arXiv preprint arXiv:2004.15015 , 2020a.\nEricWallace, TonyZZhao, ShiFeng, andSameerSingh. Customizingtriggerswithconcealeddatapoisoning.\narXiv preprint arXiv:2010.12563 , 2020b.\nJiayi Wang, Chengliang Chai, Jiabin Liu, and Guoliang Li. Cardinality estimation using normalizing flow.\nVLDB J. , 2024.\nXingda Wei, Rong Chen, and Haibo Chen. Fast rdma-based ordered key-value store using remote learned\ncache. In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20) , pp.\n117–135. USENIX Association, November 2020. ISBN 978-1-939133-19-9. URL https://www.usenix.\norg/conference/osdi20/presentation/wei .\nCihang Xie, Yuxin Wu, Laurens van der Maaten, Alan L Yuille, and Kaiming He. Feature denoising for\nimproving adversarial robustness. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pp. 501–509, 2019.\nTianyin Xu, Long Jin, Xuepeng Fan, Yuanyuan Zhou, Shankar Pasupathy, and Rukma Talwadker. Hey, you\nhave given me too many knobs!: Understanding and dealing with over-designed configuration in system\nsoftware. In Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering , pp.\n307–319, 2015.\nXiaojun Xu, Qi Wang, Huichen Li, Nikita Borisov, Carl A Gunter, and Bo Li. Detecting AI trojans using\nmeta neural analysis. arXiv:1910.03137 , 2019.\nChaofei Yang, Qing Wu, Hai Li, and Yiran Chen. Generative poisoning attack method against neural\nnetworks. arXiv:1703.01340 , 2017.\nRui Yang, Evgenios M. Kornaropoulos, and Yue Cheng. Algorithmic complexity attacks on dynamic learned\nindexes. International Conference on Very Large Data Bases (VLDB) , 2023.\nZongheng Yang, Badrish Chandramouli, Chi Wang, Johannes Gehrke, Yinan Li, Umar Farooq Minhas,\nPer-Åke Larson, Donald Kossmann, and Rajeev Acharya. Qd-tree: Learning data layouts for big data\nanalytics. In Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data ,\npp. 193–208, 2020.\nYuval Yarom and Katrina Falkner. {FLUSH+ RELOAD }: A high resolution, low noise, l3 cache {Side-\nChannel}attack. In 23rd USENIX security symposium (USENIX security 14) , pp. 719–732, 2014.\nSteve Zdancewic. Challenges for information-flow security. In Proceedings of the 1st International Workshop\non the Programming Language Interference and Dependence (PLID’04) , pp. 6. Citeseer, 2004.\nSteve Zdancewic, Lantian Zheng, Nathaniel Nystrom, and Andrew C Myers. Secure program partitioning.\nACM Transactions on Computer Systems (TOCS) , 20(3):283–328, 2002.\nYiren Zhao, Ilia Shumailov, Han Cui, Xitong Gao, Robert Mullins, and Ross Anderson. Blackbox attacks\non reinforcement learning agents using approximated temporal information, 2019.\n22\n\nPublished in Transactions on Machine Learning Research (07/2025)\nA Poisoning attack on ALEX\nA.1 Attack Construction\nOur attack guesses a random position x0\n0in the key range and start inserting keys x0\n1,x0\n2,...to the left of\nthis position. The following argument proves informally that this results in exponential memory blowup: let\ny0be the largest key in the index that is smaller than x0\n0. Assume they are in different leaf nodes such that\nα∈/parenleftbig\ny0,x0\n0)whereαis the border between y0andx0(i.e., smaller keys than αare mapped to y0’s node, and\nthose larger to x0\n0), and letAx0be the leaf node that x0\n0was inserted into. Since subtree ranges are static,\nas long as the attacker’s keys {x0\nn}are all larger than α, each new insertion will hit the leftmost position\nwithin the range that was allocated to Ax0(even ifAx0has split many times by then). If x0\n0andy0are in\nthe same leaf node initially, then due to their large gap and ALEX’s policy of splitting nodes in the middle\nof their key range, there is likely an insertion xi\n0that causes a split such that xi\n0andy0are no longer in the\nsame node, let Ax0be that node and the same argument follows from there.\nAt some point, depending on the sparsity of the indexed keys, the attacker may insert a key that is smaller\nthanα. To avoid this, our attacker simply chooses a new random location x1\n0after everykinsertions, causing\na new parent node size to grow exponentially. Our attacker’s locations may hit two child nodes of the same\nparent node, i.e., i<j,parent (Axi) =parent (Axj), in which case Axjalready has many redundant pointers\n(because its parent already doubled its size multiple times). Initially, there would be no need to double the\nsize of the parent array, but soon enough, the exponentially-consumed pointers will be depleted and — since\nthe parent array is likely already at its maximal size — a downwards split will be performed, after which a\nnew parent node starts growing exponentially (that is, at least until insertion nwherexn<α).\nA.2 Attack Evaluation\nOne notable difference between the two datasets considered in the evaluation, is that it takes more insertions\nand consequently more time to exhaust 512GB memory for the Longitudes dataset (350 million vs 280\nmillion).\nTo verify if this is due to its relatively higher key density, we check how many of the attacker’s keys, on\naverage across the attacker iterations {i}, fall closer to yithan toxi\n0. For the YCSB dataset, this was only\nabout 1400 on average, whereas Longtitudes, around 5000 out of 10000 keys on average fall closer to yi.\nRecall that once the keys cross some α∈/parenleftbig\nxi\n0,yi\n0/parenrightbig\n, the attack ceases to be effective until the next iteration,\nand this likely happens more frequently for the Longtitudes dataset.\n23",
  "textLength": 86694
}