{
  "paperId": "782283b30081dc55438cfb4c3a74d0d331dd3206",
  "title": "Understanding ML Driven HPC: Applications and Infrastructure",
  "pdfPath": "782283b30081dc55438cfb4c3a74d0d331dd3206.pdf",
  "text": "arXiv:1909.02363v1  [cs.LG]  5 Sep 2019Understanding ML driven HPC: Applications and\nInfrastructure\nGeoffrey Fox1, Shantenu Jha2,3\n1Indiana University, Bloomington, IN\n2Rutgers, State University of New Jersey, Piscataway, NJ 088 54, USA\n3Brookhaven National Laboratory, Upton, New York, 11973\nAbstract —We recently outlined the vision of ”Learning Every-\nwhere” which captures the possibility and impact of how lear ning\nmethods and traditional HPC methods can be coupled together .\nA primary driver of such coupling is the promise that Machine\nLearning (ML) will give major performance improvements for\ntraditional HPC simulations. Motivated by this potential, the ML\naround HPC class of integration is of particular signiﬁcanc e. In\na related follow-up paper, we provided an initial taxonomy f or\nintegrating learning around HPC methods. In this paper, whi ch\nis part of the Learning Everywhere series, we discuss “how”\nlearning methods and HPC simulations are being integrated\nto enhance effective performance of computations. This pap er\nidentiﬁes several modes — substitution, assimilation, and control,\nin which learning methods integrate with HPC simulations an d\nprovide representative applications in each mode. This pap er\ndiscusses some open research questions and we hope will moti vate\nand clear the ground for MLaroundHPC benchmarks.\nI. I NTRODUCTION AND MOTIVATION\nThe convergence of HPC and learning methodologies pro-\nvides a promising approach to major performance improve-\nments. Traditional HPC simulations are reaching the limits of\noriginal progress. The end of Dennard scaling of transistor\npower usage, and the end of Moores Law as originally\nformulated has yielded fundamentally different processor ar-\nchitectures. The architectures continue to evolve, result ing in\nhighly costly, if not damaging churn in scientiﬁc codes that\nneed to be ﬁnely tuned to extract the last iota of parallelism\nand performance. This approach to high-performance scient iﬁc\ncomputing is simply unsustainable.\nIn domain sciences such as biomolecular sciences, advances\nin statistical algorithms and runtime systems have enabled\nextreme scale ensemble based applications [1] to overcome\nlimitations of traditional monolithic simulations. Howev er, in\nspite of several orders of magnitude improvement in efﬁcien cy\nfrom these adaptive ensemble algorithms, the complexity\nof phase space and dynamics for modest physical systems,\nrequire additional orders of magnitude improvements and pe r-\nformance gains. Integrating traditional HPC approaches wi th\nmachine learning methods holds signiﬁcant promise towards\novercoming these barriers.\nIt has always been necessary to improve the effectiveness of\nsimulations; however, its necessity and signiﬁcance incre ases\ndrastically at large-scales. First, there is a need to enhan ce, if\nnot preserve computational efﬁciency at scale. Applying hi gh-\nperformance computing capabilities at (exa-)scale, leads to thepossibility of greater scientiﬁc inefﬁciency in computati onal\ncampaigns. For example, greater computational capacity mi ght\ngenerate relatively greater correlations and lower sampli ng,\nand thus less independent data and exploration. Algorithms ,\nmethods and campaign strategies that worked at lower scales\nare not necessarily suitable at greater scales. Second, tra di-\ntional computational campaigns have not exploited the in-\ntermediate data from high-performance computing to their\nfullest: computational campaigns have been conducted in a\nstatic, if not ad hoc fashion based upon initial assumptions and\nstates. The implications of static computational campaign s will\nbe exacerbated at scale, and thus novel algorithms, methods\nand campaign strategies are needed that employ sophisticat ed\nlearning to utilize and adapt to intermediate data products .\nIn many application domains, the integration of ML into\ncomputations is a promising way to obtain large performance\ngains, and presents an opportunity to jump a generation of\nsimulation enhancements. For example, one can view the use\nof learned surrogates as a performance boost that can lead to\nhuge speedups, as calculation of a prediction from a trained\nnetwork can be many orders of magnitude faster than full\nexecution of the simulation [2], [3]. In addition to the use\nof learning for advanced sampling as illustrated above, sim ple\nexamples are the use of a surrogate to represent a chemistry\npotential, or a larger grain size to solve the diffusion equa tion\nunderlying cellular and tissue level simulations.\nThis paper explores opportunities at the interface between\nhigh-performance simulations and machine learning. Speci ﬁ-\ncally, it investigates how ML driven HPC simulations — which\nbased upon the taxonomy introduced in Ref. [2] is referred\nto as the “ML around HPC” — can pervasively enhance\nhigh-performance computational science. It attempts to an swer\nquestions such as: How and where can ML effectively enhance\nHPC simulations? When should ML methods substitute tradi-\ntional simulations? Which ML methods are promising? What\nare the general motifs or patterns of interaction between ML\nand HPC?\nIn order to provide a quantitative metric by which to\nmeasure and answer some of these questions, it is necessary\nto distinguish between traditional performance measured b y\noperations per second or benchmark scores, from the effec-\ntive performance that one gets by combining learning with\nsimulation which gives increased scientiﬁc performance — a s\n\ndetermined by a suitable metric and measure, without changi ng\nthe traditional system characteristics.\nIn general, there are three types of performance that re-\nquire distinction: the ﬁrst, traditional system or applica tion\nperformance, which is measured by typical scaling, utiliza tion\nor operations by second and benchmark scores. The second\nis the improvement in the computational investigation of th e\nscientiﬁc process, as measured by time-to-solution (for a\ngiven resource amount) or another scientiﬁc metric. The thi rd\nmeasure of performance, is the increase in either the learni ng\nphase due to being trained with (physically meaningfully)\nsimulations, or the improvement in the simulation due to bei ng\ninteraction with learning phase.\nIn cases where there is a (tight) coupling between the\nlearning and simulation components, the second measure of\nperformance is of paramount importance. It motivates the\nnotion of crossover point deﬁned as the point in conﬁguratio n\nspace at which the learning method is either more performant\n— efﬁcient (e.g., same quality of results produced with less\ncomputing), or better (e.g., produces better results than p ossi-\nble via ﬁrst principles / simulations), or faster than simul ations\n(e.g., speeding up or classic effective performance a la red uced\norder modeling). Crossover points are dependent upon sever al\nfactors including the complexity of underlying model and\nproblem, availability of surrogate, the sensitivity to the ratio\nof cost / value of data (e.g, is it better to have lots of cheap\nand low quality data, or small amount of high quality data).\nThis paper is a follow on from Learning Everywhere [2]\nand an accompaniment to the article on “Taxonomy of\nMLaroundHPC” as part of the IEEE eScience 2019. In Section\nII, we summarize the high-level organization of MLforHPC,\nfollowed by an taxonomy of different MLaroundHPC ap-\nplications. In Section III, we focus on MLaroundHPC —\ninvestigating the different modes, mechanisms and functio nal\nmotivations of integration of ML around HPC. We also discuss\nsome canonical examples of the different modes of integrati on.\nWe will use insights gained from an investigation of these\nissues to discuss open issues and research challenges in\ncyberinfrastructure — algorithms & methods, software and\nhardware, that must be addressed in the near and intermediat e\nterm.\nWe thank the organizers of the IEEE eScience 2019 for\nthe opportunity to contribute to the Vision Track. We believ e\neScience conference series has an important and distinguis hed\ntrack record of bringing the data sciences – methods and\ninfrastructure, closer to traditional simulation based sc ience.\nWe hope this article will help the eScience Steering Committ ee\nto keep the conference series aligned with the thinking, nee ds\nand future directions of the community to push the boundarie s\nof computational and data driven discovery.\nII. L EARNING EVERYWHERE\nWe have identiﬁed [2] several important distinctly differe nt\nlinks between machine learning (ML) and HPC. We term\nthe full area MLandHPC and deﬁne two broad categories\n[4]: HPCforML and MLforHPC . HPCforML uses HPCto execute and enhance ML performance, or using HPC\nsimulations to train ML algorithms (theory guided machine\nlearning), which are then used to understand experimental\ndata or simulations. On the other hand MLforHPC uses ML\nto enhance HPC applications and systems, where big data\ncomes from the computation and/or experimental sources tha t\nare coupled to ML and/or HPC elements. MLforHPC can be\nfurther subdivided as MLaroundHPC ,MLControl ,MLAutotun-\ningHPC , and MLafterHPC described in detail below.\nThe MLforHPC category covers all aspects of machine\nlearning interacting with computation typically implemen ted\nas HPC. The sub-categories are useful but incomplete, and\ndeﬁnitely not always precise. There is a need to improve\nthe conceptual understanding of the different facets and di -\nmensions of MLforHPC. We delineate the initial types of\nMLforHPC we have identiﬁed:\nA. MLaroundHPC\nUsing ML to learn from simulations and produce learned\nsurrogates for the simulations. This increases effective p erfor-\nmance for strong scaling where we keep the problem ﬁxed\nbut run it faster or run the simulation for a longer time\ninterval such as that relevant for biological systems. It in cludes\nSimulationTrainedML where the simulations are performed to\ndirectly train an AI system rather than the AI system being\nadded to learn a simulation. Some common ways in which\nMLaroundHPC is used, include:\n1)MLaroundHPC: Learning Outputs from Inputs Sim-\nulations performed to directly train an AI system, rather th an\nAI system being added to learn a simulation [5], [6].\n2)MLaroundHPC: Learning Simulation Behavior ML\nlearns behavior replacing detailed computations by ML surr o-\ngates [7], [8]\n3)MLaroundHPC: Faster and Accurate PDE Solutions\nEfﬁcient numerical solution of PDEs is one of the most\ncostly computations in many simulations, and solving high\ndimensional PDEs such as the diffusion equation has been\nnotoriously difﬁcult. Recent ML accelerated algorithms [8 ]\nfor solving high-dimensional nonlinear PDEs are effective for\na wide variety of problems, in terms of both accuracy and\nspeed. These algorithms [9] approximate the solution high-\ndimensional PDEs such as the diffusion equation using a\n“Deep Galerkin Method (DGM)”, and train their network on\nbatches of randomly sampled time and space points. These\nnew AI accelerated approaches [10], [11] open up a host of\npossibilities in materials, physics and cosmology, and sci entiﬁc\ncomputing more generally.\n4)MLaroundHPC: New Approach to Multi-Scale Mod-\neling Effective potential is an analytic, quasi-emperical or\nquasi-phenomological potential that combines multiple, p er-\nhaps opposing, effects into a single potential. For example ,\nwe have a model speciﬁed at a microscopic scale and we\ndeﬁne a coarse graining to a different scale with macroscopi c\nentities deﬁned to interact with effective dynamics speciﬁ ed in\nsome fashion such as an effective potential or effective int er-\naction graph. Machine learning is ideally suited for deﬁning\n\neffective potentials and order parameter dynamics, and sho ws\nsigniﬁcant promise to deliver orders of magnitude performa nce\nincreases over traditional coarse-graining and order para meter\napproaches. See well established methods [12]–[17]\nB. MLControl\nTwo representative scenarios are:\n1)Experiment Control Using simulations (possibly with\nHPC) in control of experiments and in objective driven com-\nputational campaigns [18]. Here the simulation surrogates are\nvery valuable to allow real-time predictions. Examples abo ut:\nMaterial Science [19]–[21], Fusion [22], Nano [23]\n2)Experiment Design A big challenge is the uncertainty\nin the precise model structures and parameters. Model-base d\ndesign of experiments (MBDOE) assists in the planning of\nhighly effective and efﬁcient experiments – it capitalizes on\nthe uncertainty in the models to investigate how to perturb\nthe real system to maximize the information obtained from\nexperiments. MBDOE with new ML assistance [24] identiﬁes\nthe optimal conditions for stimuli and measurements that\nyield the most information about the system given practical\nlimitations on realistic experiments.\nC. MLAutoTuning\nCaptures the scenario where ML is used to efﬁciently con-\nﬁgure the HPC computations. MLAutoTuning can be applied\nat multiple distinct points, and can be used for a range of\ntuning and optimization objectives. For example: (i) mix of\nperformance and quality of results using parameters provid ed\nby learning network [4], [25]–[28]; (ii) choose the best set\nof “computation deﬁning parameters” to achieve some goal\nsuch as providing the most efﬁcient training set with deﬁnin g\nparameters spread well over the relevant phase space [29],\n[30]; (iii) tuning model parameters to optimize model outpu ts\nto available empirical data [31]–[34].\nD. MLafterHPC\nML analyzing results of HPC as in trajectory analysis and\nstructure identiﬁcation in biomolecular simulations [35] .\nIII. ML AROUND HPC C LASSIFICATION AND EXEMPLARS\nThe interaction between models and simulation data occurs\nin two directions: (i) The problem of how to use multi-\nmodal data to inform complex models in the presence of\nuncertainty, and (ii) How, where, when, and from which sourc e\nto acquire simulation data to optimally inform models with\nrespect to a particular goal or goals is fundamentally an\noptimal experimental design problem. Creating the concept ual\nand technological framework in which models optimally lear n\nfrom data and data acquisition is optimally guided by models\npresents signiﬁcant challenges systems of interest are com plex,\nmultiscale, strongly interacting/correlated, and uncert ain.\nIt is important to separate the modes and mechanics of\nhow learning is integrated with HPC simulations, from the\nfunctional motivations of doing so. Based upon an extensive\nanalysis of the current state of the ﬁeld, the three primarymodes and mechanisms for integrating learning with HPC\nsimulations are— substitution, assimilation and control. Each\nrepresents a broad range of subcategories and possibilitie s,\nwhich no doubt will change rapidly as the state of theory and\npractice evolves.\nIndependent of the modes and mechanisms of integration,\nwe ﬁnd that there are three functional drivers of the integra tion:\n1)Improving Simulations: The essence of this driver is\nto use learning to conﬁgure and select simulations effectiv ely.\nThere are several approaches to learning the conﬁguration\nof physical system being studied, ranging from improving\nthe learning models using simulation data dynamically, to\nusing models to determine simulation conﬁgurations and/or\nparameters, as well as possibly learn conﬁgurations of syst em\nand software for improved performance on particular hardwa re\nand input parameters [1].\n2)Learn Structure, Theory and Model for Simulation:\nHere the simulations are used to gradually improve the model\nor theory, which are in turn used to improve simulations in\nsome fashion (e.g., as per previous point). As simulations\nproceed the model learns the structure or even underlying\nprinciples, and is gradually reﬁned either by coarse-grain ing\nor using better approximations to the effective potentials [36].\n3)Learn to make Surrogates: An increasingly common\nand important driver is the use of ML (which are often deep\nnetworks) to learn the function representing the output of t he\nsimulation. Such learned representations also often refer red to\nas surrogates, can be used to determine either the parameter s\nor the effective “ﬁelds” [7], [8].\nIt is no surprise that the ﬁrst driver is the most widely\ninvestigated and applied; the rate of progress in the second\nand third drivers is rapid and impressive. We now discuss\nthe primary modes and mechanisms in which the above three\nscenarios are often implemented.\nA. Substitution\nIn this mode, a surrogate model is used to substitute an\nessential element of the original simulation (method). The\nsurrogate model is used to create multi-scale or coarse grai ned\nsurrogate modeling, which could either learn the structure or\ntheory of original simulation.\nExample: Roitberg et al. [15] trained a network on using\nﬁne grained Quantum Mechanical DFT calculations. The\nresulting ANI-1 model was shown to be chemically accurate,\ntransferrable, with a performance similar to a classical fo rce\nﬁeld, thus enabling ab-initio molecular dynamics at a fract ion\nof the cost of “true” DFT ab-initio simulations. Extensions of\ntheir work with an active learning (AL) approach demonstrat ed\nthat proteins in an explicit water environment can be simula ted\nwith a NN potential at DFT accuracy [17].\nIn general the focus has been on achieving DFT-level\naccuracy because NN potentials are not cheaper to evaluate\nthan most classical empirical potentials. However, replac ing\nsolvent-solvent and solvent-solute interactions, which t ypically\nmake up 80%-90% of the computational effort in a classical\nall-atom, explicit solvent simulation, with a NN potential\n\npromises large performance gains at a fraction of the cost\nof traditional implicit solvent models and with an accuracy\ncomparable to the explicit simulations [37].\nB. Assimilation\nIn this mode, data from simulations, ofﬂine external con-\nstraints, or real-time experiments are integrated into phy sics-\nbased models, which are then assimilated into traditional\nsimulations. The canonical examples are improving the Hami l-\ntonian or Force Fields, or in classical data assimilation st udies\nsuch as in climate and weather prediction, where in data\nassimilation involves continuous integration of time depe ndent\nsimulations with observations to correct the model, which a re\ncombined and updated with traditional simulation model.\nExample: Current climate models are too coarse to re-\nsolve many of the atmospheres most important processes.\nTraditionally, these subgrid processes are heuristically approx-\nimated in so-called parameterizations. However, imperfec tions\nin these parameterizations, especially for clouds, have im peded\nprogress toward more accurate climate predictions for deca des.\nCloud resolving models alleviate many of the gravest issues of\ntheir coarse counterparts but will remain too computationa lly\ndemanding for climate change predictions for the foreseeab le\nfuture. In Ref. [38], a deep neural network is trained to\nrepresent all atmospheric subgrid processes in a climate mo del\nby learning from a multiscale model in which convection is\ntreated explicitly. The trained neural network then replac es\nthe traditional subgrid parameterizations in a global gene ral\ncirculation model in which it freely interacts with the reso lved\ndynamics and the surface-ﬂux scheme. The prognostic multi-\nyear simulations are stable and closely reproduce not only t he\nmean climate of the cloud-resolving simulation but also key\naspects of variability, including precipitation extremes and the\nequatorial wave spectrum. Furthermore, the neural network\napproximately conserves energy despite not being explicit ly\ninstructed to. Ref. [38] uses deep learning to leverage the\npower of short-term cloud-resolving simulations for clima te\nmodeling; the approach is fast and accurate, thereby showin g\nthe potential of machine-learningbased approaches to clim ate\nmodel development.\nC. Control and Adaptive Execution\nIn this mode, the simulation (or ensemble of simulations)\nare controlled towards important and interesting parts of\nsimulation phase space. Sometimes this involves determini ng\nthe parameters of the next stage (iteration) of simulations\nbased upon intermediate data. Sometimes the entire campaig n\ncan be adaptively steered towards an objective, which in tur n\ncould involve getting better data via active learning based upon\nan objective function, or use a policy-based reinforcement\nlearning approach to steer the computational campaign.\nExample: A fundamental problem that currently pervades\ndiverse areas of science and engineering is the need to desig n\nexpensive computational campaigns (experiments) that are\nrobust in the presence of substantial uncertainty. A partic ular\ninterest lies in effectively achieving speciﬁc objectives forsystems that cannot be completely identiﬁed. For example,\nthere may be big data but the data size may still pale in\ncomparison with the complexity of the system, or the availab le\ndata may be scarce due to the prohibitive cost of experiments .\nA framework for the objective driven experiment design\n(ODED) will support the integration of scientiﬁc prior know l-\nedge on the system with data generated via simulations,\nquantify the uncertainty relative to the objective, and des ign\noptimal experiments that can reduce the uncertainty and\nthereby directly contribute to the attainment of the object ive.\nIV. MLA ROUND HPC C YBER INFRASTRUCTURE\nWe distill the analysis and description of MLAroundHPC\nmodes and examples into three cyberinfrastructure categor ies:\n(i) algorthms, benchmarks and methods; (ii) system softwar e\nand runtime, and (iii) hardware.\nA. Algorithms, Benchmarks and Methods\nThe methodologies by which experiments inform theory,\nand theory guides experiments, remain ad hoc, particularly\nwhen the physical systems under study are multiscale, large -\nscale, and complex. Off-the-shelf machine learning method s\nare not the answer; these methods have been successful in\nproblems for which massive amounts of data are available and\nfor which a predictive capability does not rely upon the con-\nstraints of physical laws. The need to address this fundamen tal\nproblem has become urgent, as computational campaigns at\npre-exascale, and soon exascale, will entail models that sp an\nwider ranges of scales and represent richer interacting phy sics.\nOpen issues and research questions include:\n1) Does the crossover point — at which prediction based\napproaches are better than traditional HPC simulations, su g-\ngest or motivate a need to redesign some simulation algorith ms\nso that MLforHPC effective? Similarly, if HPC simulations\nare going to serve as important sources of data generation, i s\nthere an opportunity to devise novel learning algorithms an d\nmethods so as to support more effective MLforHPC?\n2) Simulations are simply 4D time-series data! Thus, there\nought to be important analogies between time series ML\nresearch and MLforHPC.\n3) Importance of canonical problems: Understanding of\nwhich learning methods work, why and for which problems.\nHow do we develop benchmarks to highlight different appli-\ncation and system features? By extension, how do we develop\nproxy apps to represent the applications?\n4) Understanding Performance: What are the performance\nmetrics that represent the integrated working of learning a nd\nsimulations? What is the comparison in scientiﬁc discovery\nbetween the large increase in performance possible with tru e\nexascale machines and the exascale (or zettascale) effecti ve\nperformance possible with MLforHPC? How does the inter-\nplay of raw performance and effective performance inﬂuence\nthe mapping of applications to compute systems?\n\nB. System Software and ML-HPC Runtime Systems:\nMLforHPC needs to support large scale simulations and\nlearning, and their integrated and concurrent execution. T he\ncombined workload — distinct ML and HPC computation\ntasks, will need to be run ﬂexibly. For example, sometime\nthe HPC simulation will be used to generate training data\nand then run ML; sometimes the ML will be responsible\nfor inference as HPC simulations are generating data. On\noccasions, HPC simulations will run after Learning (or vice\nversa), but sometimes they will be intertwined in a single jo b.\nThus, it is imperative to understand the general control and\ncoupling between Learning elements (L), HPC Simulation (S) .\nIn many cases a third general component — experiments or\nobservations (E) may also be needed.\nThere are several dimensions to characterize the coupling\nbetween these components, including temporal and data vol-\numes. The former will determine the type of algorithms and\nlearning approaches taken; the latter software scaling and\nperformance requirements. Furthermore, the speciﬁc type o f\ncoupling could yield steering or control. (Component X is\nsaid to steer component Y , when X provides the relevant\ninformation to determine the execution of Y . Whether Y\naccepts or not, is determined by additional considerations such\nas objective, policy, etc. Steering is a necessary conditio n for\ncontrol; not all steering represents control).\nDifferent scenarios for coupling information and control\nﬂow, between different elements E, L and S. Scenario III cov-\ners two possibilities: learning element controls experime ntal\ndata source, or Simulation controls experimental data sour ce.\nThe logical coupling disregards the physical location of th e\nelements, e.g., E could be on an Edge device or a HPC cluster.\nIn order to support the real-time application requirements , it\nis important to achieve near real-time training and predict ion\nto control or steer S or E. For example, build low dimensional\nrepresentation of states from trajectory analysis. The str ong\nscaling of just L is inadequate, and scaling properties of\nintegrated L + S elements are needed for MLaroundHPC\napplications. A preliminary analysis suggests that this ca n be\nachieved by adapting the ratio of the cardinality of L, S and E ,\nviz., N L(the number of learning) to N S(number of simulation\nelements) being time-dependent . These translate into supp ort\nfor coordinated execution of a large number of concurrent\nand heterogeneous simulations as well as enabling adaptive\nexecution and resource partitioning between simulation an d\nlearning elements.\nAdditional considerations that a runtime system to support\nthe concurrent execution of ML and HPC elements include: Is\na single run-time system possible that will be able to suppor t\nthe different classes of MLforHPC, varied data rates (from\ntrivial to O(100)GB/s) and latency tolerance (from <O(1)s to\nO(100)s)? Can a single runtime system support the full range\nof ﬁne-grained to coarse-grained coupling between learnin g\nand simulation components? What are the considerations and\nconstraints that inform performance guarantees and worklo ad\nbalancing (e.g., dynamically varying the number of learnin gelements and simulation elements)?\nC. Hardware and Platform Conﬁguration\nWhat fraction of time (resource) is spent in ML component\nand how does this change with scale? What is the frequency\nand extent of coupling between learning and simulations?\nInsight into the above questions could inﬂuence optimal arc hi-\ntecture, e.g., when the ratio of learning (training and infe rence)\nis small, a classic supercomputing architecture linked to a\nseparate learning system might be acceptable if not optimal .\nConversely, when the ratio is large, a tightly integrated sy stem\nsupercomputer might be more suitable? What are the quan-\ntitative determinants of an optimal platform? How should a\nbalanced system across a range of MLaroundHPC applications\nbe designed: ﬁxed dollars for learning vs simulations, or a\ndollar distribution that tracks the relative computing int ensity?\nOr one that optimizes inference phase versus training phase ?\nShould future HPC platforms be designed to support both\nphases, or is platform specialization for training and infe rence\nmost effective?\nHardware and platform considerations that arise from un-\ncertainty in technology roadmap and pricing include:\n1) Role and importance of heterogeneous accelerators, es-\npecially as a new generation of ML accelerators is developed\nthat may not be in simulations (currently GPU accelerators\noften useful in both ML and simulation); (ii) As we expect\ntime series in data assimilation likely to use RNNs and\nthe importance and pervasiveness of RNN to increase, when\nshould Recurrent neural networks RNN (commonly used in\nlearning sequences) need different accelerators from conv olu-\ntional neural nets)?\n2) Requirements also suggest the need for fast I/O and\ninternode communication to enable ML and Computation to\nrun together and exchange information with each other and\nwith sources of streaming data. It is not evident how large an d\nfast disks should be organized, but disks on each node seem\nrequired to hold data to be exchanged between simulation and\nML components of a job and for accumulating training data\nand NN weights.\n3) Need ML optimization and Simulation optimization\nspread through machine and fast ways for ML and simulation\nto exchange data. Given the emergence of cloudlets (aka fog\ncomputing), there is a need to support HPC/Cloud, Fog and\nEdge platforms, as well as their integration.\nV. D ISCUSSION AND CONCLUSION\nThe state of HPC in 2020 presents challenges and oppor-\ntunities. On the one hand, HPC methods and platforms are\nbecoming pervasive and necessary for scientiﬁc advances. O n\nthe other, traditional HPC computations are reaching vario us\nlimits. The implications of hardware and architectural tre nds\nare well known: the end of Dennard scaling and of Moore’s\nLaw as originally formulated, is yielding very different pr o-\ncessor architectures; achieving performance gains is beco ming\nharder, while requiring signiﬁcant, if not unsustainable s oft-\nware investment and algorithmic reformulation.\n\nThe HPC community has — somewhat naively, assumed\nthat as long as performance gains from hardware are possible ,\ntraditional simulation based methods will continue to prov ide\nincreased scientiﬁc insight. However, without careful exa m-\nination of the scientiﬁc efﬁciency or effective performanc e\nof existing simulation and ﬁrst principles methods, it is no t\nobvious that traditional simulations represent the optima l ap-\nproach at exascale and beyond, and on subsequent generation\nof supercomputers. In other words, we may be reaching limits\nof both hardware and methodological performance gains.\nThere is a need for major functionality and performance\nincreases that are independent of changes in hardware . In\ntraditional HPC the prevailing orthodoxy “Faster is Better ”\nor what is worse, the conﬂation of “bigger” with “better”\nhas driven the quest for hierarchical parallelism to speedi ng\nup single units of works. Relinquishing the orthodoxy of\nhierarchical parallelism as the primary route to performan ce\nis necessary. In fact, there is a need to carefully reconside r\ndiscredited approaches, while adopting the new paradigms.\nEnter “Learning Everywhere” — the essential idea of which,\nis that by embedding learning methods and approaches in all\naspects of the system conﬁguration and application executi on,\nthe effective performance can be dramatically improved.\nThere is a regime where learning based predictive\napproaches are going to outperform ﬁrst-principles and\nsimulation-based approaches. The exact sweet spot or\ncrossover point is non-trivial to determine: it will be appl i-\ncation speciﬁc, depend upon complexity of learned models,\nvolume and cost of data, as well as effectiveness and cost\nof simulations, inter alia. However, the underlying idea th at\nsurrogate learned models will represent effective perform ance\nimprovements over traditional approaches, is a powerful on e,\nand is an important generalization of the multi-scale, coar se-\ngrained approaches used in many physical sciences.\nLearning Everywhere is one speciﬁc example of the paradig-\nmatic shift in scientiﬁc computing that will be needed at\nextremes scales. Statistical computing, which incorporat es\nelements of approximate computing, uncertainty minimizat ion\nand other objective driven dynamic computational campaign s\nwill substitute predeﬁned “static” computational campaig ns.\nNowhere is the impact of this likely to be greater than in\nthose domains which require the assimilation of streaming a nd\ndynamic data, or computational campaigns that are statisti cal\nin nature and driven towards optimality or objectives. Thes e\nmethodological innovations will heighten the importance o f\nadaptive execution of ensembles of heterogeneous models, a nd\nwill require novel scalable middleware systems.\nLooking Ahead: The pace of innovation in learning for\nscience is intense and rapid. No surprises it is difﬁcult to\npredict the exact trajectory or state for anything but the\nimmediate future. It is safe to expect major impact of ML\non science in essentially all areas and in multiple modes:\nmany traditional physics applications including simulati ons\nand Monte Carlo methods are being reformulated using learn-\ning approaches [39].\nIntegrating learning with HPC provides an opportunity toenhance methods and for some domains such as molecular\nscience to jump ahead. For other ﬁeld, such as high-energy\nphysics, that did not invest and anticipate the disruptions\narising from end of Dennard and Moore’s law resulting in\nthe explosion of heterogeneous computing and accelerators ,\nit presents an important opportunity to simply by-pass and\nleapfrog a generation of simulation enhancements!\nImpressive, if not inspiring papers that apply learning to\nsocietally important problems such as climate change [40] a re\nvaluable harbingers. Molecular sciences has been an enthus i-\nastic adopter of learning methods: Machine Learning used in\nmaterials simulation to aid the design of new materials and t o\nunderstand properties [41]; predict reaction coordinates [42];\nand enhanced sampling [43] and dynamics on long time-\nscales [44]. Even as the use of ML in science changes, im-\nportant advances in the way ML is formulated are happening.\nFor example, Ref. [45] shows how to scale CNNs as problems\nscale — which will be crucial in using NN for complex physics\nsystems. In fact, Ref. [46] uses ODEs to build a continuous\nneural network rather than one built from a set of layers.\nEnhancements to ML will be necessary to deliver on new\nand promising uses of learning in science, such as the ap-\nplication of DL for time series — geospatial and simulation\ntrajecctory data (which are simply 4D time series). These pr ob-\nlems can be formulated as graphs spatially (with convolutio nal\nNNs) and as sequences (with recurrent NNs) in time. Many\nHPC Cloud-Edge systems will provide such time series, and\nalso reinforce the need for real-time response which raises\ndifﬁcult trade-offs between performance and functionalit y and\nhighlights the role of HPC [47]. In general, the pace of\nmethodological innovation and application requirements w ill\nhave important implications for the cyberinfrastructure d evel-\noped and deployed for the science of tomorrow.\nAcknowledgement This work was partially supported by NSF CIF21\nDIBBS 1443054 and nanoBIO 1720625. SJ was partially support ed by\nExaLearn – a DOE Exascale Computing project.\nREFERENCES\n[1] Peter M Kasson and Shantenu Jha. Adaptive ensemble simul ations\nof biomolecules. Current Opinion in Structural Biology , 52:87 – 94,\n2018. Cryo electron microscopy: the impact of the cryo-EM re volution\nin biology Biophysical and computational methods - Part A.\n[2] Geoffrey C. Fox, James A. Glazier, J. C. S. Kadupitiya, Vi kram Jadhao,\nMinje Kim, Judy Qiu, James P. Sluka, Endre Somogy, Madhav Mar athe,\nAbhijin Adiga, Jiangzhuo Chen, Oliver Beckstein, and Shant enu Jha.\nLearning everywhere: Pervasive machine learning for effec tive high-\nperformance computation. In IEEE International Parallel and Dis-\ntributed Processing Symposium Workshops, IPDPSW 2019, Rio de\nJaneiro, Brazil, May 20-24, 2019 , pages 422–429, 2019.\n[3] Jo˜ ao Marcelo Lamim Ribeiro and Pratyush Tiwary. Toward achieving\nefﬁcient and accurate Ligand-Protein unbinding with deep l earning and\nmolecular dynamics through RA VE. Journal of chemical theory and\ncomputation , 15(1):708–719, 8 January 2019.\n[4] Jeff Dean. Machine learning for systems and systems for m achine\nlearning. In Presentation at 2017 Conference on Neural Information\nProcessing Systems , 2017.\n[5] Frank No´ e, Simon Olsson, Jonas K¨ ohler, and Hao Wu. Bolt zmann\ngenerators – sampling equilibrium states of Many-Body syst ems with\ndeep learning. 4 December 2018.\n[6] Katsuhiro Endo, Katsufumi Tomobe, and Kenji Yasuoka. Mu lti-step\ntime series generator for molecular dynamics. In Thirty-Second AAAI\nConference on Artiﬁcial Intelligence . aaai.org, 2018.\n\n[7] Wolfgang Gentzsch. Deep learning for ﬂuid ﬂow predictio n in the cloud.\nhttps://www.linkedin.com/pulse/deep-learning-ﬂuid-ﬂ ow-prediction-cloud-wolfgang-gentzsch/,\nDecember 2018. Accessed: 2019-3-1.\n[8] Jiequn Han, Arnulf Jentzen, and Weinan E. Solving high-d imensional\npartial differential equations using deep learning. Proceedings of\nthe National Academy of Sciences of the United States of Amer ica,\n115(34):8505–8510, 21 August 2018.\n[9] Justin Sirignano and Konstantinos Spiliopoulos. DGM: A deep learning\nalgorithm for solving partial differential equations. Journal of compu-\ntational physics , 375:1339–1364, 15 December 2018.\n[10] Rohit K Tripathy and Ilias Bilionis. Deep UQ: Learning d eep neural net-\nwork surrogate models for high dimensional uncertainty qua ntiﬁcation.\nJournal of computational physics , 375:565–588, 15 December 2018.\n[11] Maziar Raissi, Paris Perdikaris, and George Em Karniad akis. Physics\ninformed deep learning (part II): Data-driven discovery of nonlinear\npartial differential equations. 28 November 2017.\n[12] Jrg Behler and Michele Parrinello. Generalized Neural -Network Rep-\nresentation of High-Dimensional Potential-Energy Surfac es. Physical\nReview Letters , 98(14):146401, April 2007.\n[13] Jrg Behler. First Principles Neural Network Potential s for Reactive\nSimulations of Large Molecular and Condensed Systems. Angewandte\nChemie International Edition , 56(42):12828–12840, 2017.\n[14] Keith T. Butler, Daniel W. Davies, Hugh Cartwright, Ole xandr Isayev,\nand Aron Walsh. Machine learning for molecular and material s science.\nNature , 559(7715):547, July 2018.\n[15] J. S.Smith, O. Isayev, and A. E.Roitberg. ANI-1: an exte nsible neural\nnetwork potential with DFT accuracy at force ﬁeld computati onal cost.\nChemical Science , 8(4):3192–3203, 2017.\n[16] Justin S Smith, Benjamin T. Nebgen, Roman Zubatyuk, Nic holas\nLubbers, Christian Devereux, Kipton Barros, Sergei Tretia k, Olexandr\nIsayev, and Adrian Roitberg. Outsmarting Quantum Chemistr y Through\nTransfer Learning. chemrxiv , doi: 10.26434/chemrxiv.6744440.v1, July\n2018.\n[17] Justin S. Smith, Ben Nebgen, Nicholas Lubbers, Olexand r Isayev, and\nAdrian E. Roitberg. Less is more: Sampling chemical space wi th active\nlearning. The Journal of Chemical Physics , 148(24):241733, May 2018.\n[18] Francis J. Alexander, Shantenu Jha. Objective driven c omputational\nexperiment design: An ExaLearn perspective. In Terry Moore , Geoffrey\nFox, editor, Online Resource for Big Data and Extreme-Scale Computing\nWorkshop , November 2018.\n[19] Kevin Yager. Autonomous experimentation as a paradigm for materials\ndiscovery. In Geoffrey Fox Terry Moore, editor, Online Resource for\nBig Data and Extreme-Scale Computing Workshop , November 2018.\n[20] Fang Ren, Logan Ward, Travis Williams, Kevin J Laws, Chr istopher\nWolverton, and Apurva Mehta. Accelerated discovery of meta llic glasses\nthrough iteration of machine learning and high-throughput experiments.\nScience advances , 4(4):eaaq1566, April 2018.\n[21] Logan Ward. Deep learning, HPC, and data for materials d esign. In\nTerry Moore, Geoffrey Fox, editor, Online Resource for Big Data and\nExtreme-Scale Computing Workshop , November 2018.\n[22] Bill Tang. New models for integrated inquiry: Fusion en ergy exemplar.\nIn Geoffrey Fox Terry Moore, editor, Online Resource for Big Data and\nExtreme-Scale Computing Workshop , November 2018.\n[23] Prasanna V Balachandran, Dezhen Xue, James Theiler, Jo hn Hogden,\nand Turab Lookman. Adaptive strategies for materials desig n using\nuncertainties. Scientiﬁc reports , 6:19660, 2016.\n[24] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practi cal bayesian\noptimization of machine learning algorithms. In Advances in Neural\nInformation Processing Systems , pages 2951–2959, 2012.\n[25] Microsoft Research. AI for database and data analytic s ystems at\nmicrosoft faculty summit. https://youtu.be/Tkl6ERLWAbA , 2018. Ac-\ncessed: 2019-1-29.\n[26] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neok lis Polyzotis.\nThe case for learned index structures. In Proceedings of the 2018\nInternational Conference on Management of Data , SIGMOD ’18, pages\n489–504, New York, NY , USA, 2018. ACM.\n[27] JCS Kadupitiya, Geoffrey C. Fox, Vikram Jadhao. Machin e learning\nfor parameter auto-tuning in molecular dynamics simulatio ns: Efﬁcient\ndynamics of ions near polarizable nanoparticles. Technica l report,\nIndiana University, November 2018.\n[28] Venkatesh Botu and Rampi Ramprasad. Adaptive machine l earning\nframework to accelerate ab initio molecular dynamics. International\nJournal of Quantum Chemistry , 115(16):1074–1083, 2015.[29] Valeriy Gavrishchaka, Olga Senyukova, and Mark Koepke . Synergy of\nphysics-based reasoning and machine learning in biomedica l applica-\ntions: towards unlimited deep learning with limited data. Advances in\nPhysics: X , 4(1):1582361, 1 January 2019.\n[30] Kreˇ simir Matkovi´ c, Denis Graˇ canin, and Helwig Haus er. Visual an-\nalytics for simulation ensembles. In Proceedings of the 2018 Winter\nSimulation Conference , WSC ’18, pages 321–335, Piscataway, NJ, USA,\n2018. IEEE Press.\n[31] Wolfgang Gentzsch. Deep learning for ﬂuid ﬂow predicti on in the cloud.\nhttps://www.linkedin.com/pulse/deep-learning-ﬂuid-ﬂ ow-prediction-cloud-wolfgang-gentzsch/,\n8 December 2018. Accessed: 2019-3-1.\n[32] Jonathan Ozik, Nicholson Collier, Randy Heiland, Gary An, and Paul\nMacklin. Learning-accelerated Discovery of Immune-Tumou r Interac-\ntions. bioRxiv , page 573972, January 2019.\n[33] JCS Kadupitiya , Geoffrey C. Fox , and Vikram Jadhao. Mac hine learn-\ning for performance enhancement of molecular dynamics simu lations.\n28 December 2018.\n[34] Matthew Spellings and Sharon C Glotzer. Machine learni ng for crystal\nidentiﬁcation and discovery. AIChE journal. American Institute of\nChemical Engineers , 64(6):2198–2206, 30 June 2018.\n[35] Oliver Beckstein, Geoffrey Fox, Shantenu Jha. Converg ence of data\ngeneration and analysis in the biomolecular simulation com munity. In\nTerry Moore, Geoffrey Fox, editor, Online Resource for Big Data and\nExtreme-Scale Computing Workshop , November 2018.\n[36] Eliodoro Chiavazzo, Roberto Covino, Ronald R Coifman, C William\nGear, Gerhard Hummer, and Ioannis G Kevrekidis. Intrinsic m ap\ndynamics exploration for uncharted effective free-energy landscapes.\nProceedings of the National Academy of Sciences of the Unite d States\nof America , 114(28):E5494–E5503, 11 July 2017.\n[37] Jiang Wang, Christoph Wehmeyer, Frank No´ e, , and Cecil ia Clementi.\nMachine learning of coarse-grained molecular dynamics for ce ﬁelds.\narXiv , 1812.01736v2, 2018.\n[38] Stephan Rasp, Michael S. Pritchard, and Pierre Gentine . Deep learning\nto represent subgrid processes in climate models. Proceedings of the\nNational Academy of Sciences , 115(39):9684–9689, 2018.\n[39] Pankaj Mehta, Marin Bukov, Ching-Hao Wang, Alexandre G R Day,\nClint Richardson, Charles K Fisher, and David J Schwab. A hig h-bias,\nlow-variance introduction to machine learning for physici sts. Physics\nreports , 810:1–124, 30 May 2019.\n[40] David Rolnick, Priya L Donti, Lynn H Kaack, Kelly Kochan ski, Alexan-\ndre Lacoste, Kris Sankaran, Andrew Slavin Ross, Nikola Milo jevic-\nDupont, Natasha Jaques, Anna Waldman-Brown, Alexandra Luc cioni,\nTegan Maharaj, Evan D Sherwin, S Karthik Mukkavilli, Konrad P\nKording, Carla Gomes, Andrew Y Ng, Demis Hassabis, John C Pla tt,\nFelix Creutzig, Jennifer Chayes, and Yoshua Bengio. Tackli ng climate\nchange with machine learning. 10 June 2019.\n[41] Daniel P Tabor, Lo¨ ıc M Roch, Semion K Saikin, Christoph Kreisbeck,\nDennis Sheberla, Joseph H Montoya, Shyam Dwaraknath, Murat a-\nhan Aykol, Carlos Ortiz, Hermann Tribukait, Carlos Amador- Bedolla,\nChristoph J Brabec, Kristin A Persson, and Al´ an Aspuru-Guz ik. Accel-\nerating the discovery of materials for clean energy in the er a of smart\nautomation. Nature Reviews Materials , 3(5):5–20, 1 May 2018.\n[42] Simon Brandt, Florian Sittel, Matthias Ernst, and Gerh ard Stock. Ma-\nchine learning of biomolecular reaction coordinates. Journal of Physical\nChemistry Letters , 9(9):2144–2150, 3 May 2018.\n[43] Wei Chen and Andrew L Ferguson. Molecular enhanced samp ling with\nautoencoders: On-the-ﬂy collective variable discovery an d accelerated\nfree energy landscape exploration. 30 December 2017.\n[44] Frank No´ e. Machine learning for molecular dynamics on long\ntimescales. 18 December 2018.\n[45] Mingxing Tan and Quoc V Le. EfﬁcientNet: Rethinking mod el scaling\nfor convolutional neural networks. arxiv.org, 28 May 2019.\n[46] Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and Da vid K Duve-\nnaud. Neural ordinary differential equations. In S Bengio, H Wallach,\nH Larochelle, K Grauman, N Cesa-Bianchi, and R Garnett, edit ors,\nAdvances in Neural Information Processing Systems 31 , pages 6571–\n6583. Curran Associates, Inc., 2018.\n[47] Chathura Widanage, Jiayu Li, Sahil Tyagi, Ravi Teja, Bo Peng, Supun\nKamburugamuve, Dan Baum, Dayle Smith, Judy Qiu, Jon Koskey.\nAnomaly detection over streaming data: Indy500 case study. InIEEE\nCloud 2019 Proceedings . IEEE.",
  "textLength": 45861
}