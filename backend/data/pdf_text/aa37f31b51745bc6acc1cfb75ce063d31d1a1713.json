{
  "paperId": "aa37f31b51745bc6acc1cfb75ce063d31d1a1713",
  "title": "Incremental Topological Ordering and Cycle Detection with Predictions",
  "pdfPath": "aa37f31b51745bc6acc1cfb75ce063d31d1a1713.pdf",
  "text": "Incremental Topological Ordering and Cycle Detection with Predictions\nSamuel McCauley1Benjamin Moseley2Aidin Niaparast2Shikha Singh1\nAbstract\nThis paper leverages the framework of algorithms-\nwith-predictions to design data structures for two\nfundamental dynamic graph problems: incremen-\ntal topological ordering and cycle detection. In\nthese problems, the input is a directed graph on\nnnodes, and the medges arrive one by one. The\ndata structure must maintain a topological order-\ning of the vertices at all times and detect if the\nnewly inserted edge creates a cycle. The theoret-\nically best worst-case algorithms for these prob-\nlems have high update cost (polynomial in nand\nm). In practice, greedy heuristics (that recompute\nthe solution from scratch each time) perform well\nbut can have high update cost in the worst case.\nIn this paper, we bridge this gap by leveraging\npredictions to design a learned new data structure\nfor the problems. Our data structure guarantees\nconsistency, robustness, and smoothness with re-\nspect to predictions—that is, it has the best possi-\nble running time under perfect predictions, never\nperforms worse than the best-known worst-case\nmethods, and its running time degrades smoothly\nwith the prediction error. Moreover, we demon-\nstrate empirically that predictions, learned from a\nvery small training dataset, are sufficient to pro-\nvide significant speed-ups on real datasets.\n1. Introduction\nA recent line of research has focused on how learned predic-\ntions can be used to enhance the running time of algorithms.\nThis novel approach, often referred to as warm starting , ini-\ntializes an algorithm with a machine-learned starting state\nto optimize efficiency on a new problem instance. This\n1Department of Computer Science, Williams College,\nWilliamstown, MA 01267 USA2Tepper School of Business,\nCarnegie Mellon University,Pittsburgh, PA 15213 USA. Cor-\nrespondence to: Samuel McCauley <sam@cs.williams.edu >,\nBenjamin Moseley <moseleyb@andrew.cmu.edu >, Aidin\nNiaparast <aniapara@andrew.cmu.edu >, Shikha Singh\n<shikha@cs.williams.edu >.starting state can significantly improve performance over\nthe conventional method of solving problems from scratch.\nWarm starting algorithms with machined-learned predic-\ntions can be viewed through the lens of beyond-worst-case\nanalysis. While the predominant algorithmic paradigm for\ndecades has been to use worst-case analysis, warm starting\ntakes into account that real-world applications repeatedly\nsolve a problem on similar instances that share a common\nunderlying structure. Predictions about these input instances\ncan be used to the improve running time of future computa-\ntions.\nThis new line of research, called algorithms with predictions\norlearning-augmented algorithms , leverages predictions to\nachieve strong guarantees–much like those achieved using\nworst-case analysis—for warm-started algorithms. Under\nthis setting, the performance of the algorithm is measured\nas a function of the prediction quality . This ensures that\nthe algorithm is robust to prediction inaccuracies and has\nperformance that interpolates smoothly between ideal and\nworst-case guarantees with respect to predictions.\nRecent proof-of-concept results have demonstrated the po-\ntential to enhance the running time of offline algorithms.\nThe area was empirically initiated by Kraska et al. (Kraska\net al., 2018). Theoretically, Dinitz et al. (Dinitz et al., 2021)\nwere the first to provide a theoretical framework for using\nwarm-start to improve the running time of the weighted\nbipartite matching problem. Follow-up works include the\napplication of learned predictions to improve the efficiency\nof computing flows using Ford-Fulkerson (Davies et al.,\n2023), shortest path computations using Bellman-Ford (Lat-\ntanzi et al., 2023), binary search (Bai & Coester, 2023),\nconvex optimization (Sakaue & Oki, 2022) and maintain-\ning a dynamic sorted array (McCauley et al., 2023). These\nresults showcase the promising potential to harness predic-\ntions more broadly for algorithmic efficiency.\nData structures are one of the most fundamental algorith-\nmic domains, forming the backbone of most computer sys-\ntems and databases. Leveraging predictions to improve data\nstructure design remains a nascent research area. Empir-\nical investigations, initiated by (Kraska et al., 2018) and\nfollow-ups such as (Ferragina et al., 2021), demonstrate the\nexciting potential of speeding up indexing data structures\nusing machine learning. More recently (McCauley et al.,\n1arXiv:2402.11028v1  [cs.DS]  16 Feb 2024\n\nIncremental Topological Ordering and Cycle Detection with Predictions\n2023) developed the first data structure in the new theoreti-\ncal framework of algorithms with predictions. They design\na learned data structure to maintain a sorted array efficiently\nunder insertions (aka online list labeling ). Since then, two\nconcurrent works (van den Brand et al., 2024) and (Hen-\nzinger et al., 2024) show how to leverage predictions for\nmaintaining dynamic graphs for problems such as shortest\npaths, reachability, and triangle detection via predictions for\nthe matrix-vector multiplication problem.\nThis paper focuses specifically on developing the area of\ndata structures for dynamic graph problems. We study the\nfundamental problems of maintaining an incremental topo-\nlogical ordering of the nodes of a directed-acyclic graph\n(DAG) and the related problem of incremental cycle de-\ntection . In the problem, a set of nnodes Vis given and\nthe edge set is initially empty. Over time, directed edges\narrive that are added to the graph. The algorithm must main-\ntain a topological ordering of Vat all times. A topological\nordering is a labeling L:V→Zof the vertices Vsuch\nthatL(v)< L(u)if there is a directed path from vtou. A\ntopological ordering exists if and only if the directed graph\nis acyclic. Thus, if an edge is inserted that creates a cycle,\nthe data structure must report that a cycle has been detected,\nafter which the algorithm ends.\nThe goal is to design an online algorithm that has small total\nupdate time for the medge insertions. Offline, when all\nedges are available a priori, the problem can be solved in\nO(m)(linear time) by running Depth-First-Search (DFS).\nA naive approach to the incremental problem is to use DFS\nfrom scratch each time an edge arrives, giving O(m2)total\ntime. The goal is to design dynamic data structures that can\nperform better than this naive approach.\nTopological ordering and cycle detection are foundational\ntextbook problems on DAGs. Incremental maintenance of\nDAGs is ubiquitous in database and scheduling applications\nwith dependencies between events (such as task scheduling,\nnetwork routing, and casual networks). Due to their wide\nuse, there has been substantial prior work on maintaining\nincremental topological ordering in the worst case (without\npredictions). Prior work can roughly be partitioned into the\ncases where the underlying graph is sparse or dense. A line\nof work (Bender et al., 2009; Haeupler et al., 2012; Bender\net al., 2015; Bernstein & Chechi, 2018; Bhattacharya &\nKulkarni, 2020) for sparse graphs led to (Bhattacharya &\nKulkarni, 2020) giving a randomized algorithm with total\nupdate time eO(m4/3). TheeOsuppresses logarithmic factors.\nFor dense graphs, a line of work (Cohen et al., 2013; Bender\net al., 2015) has total update time eO(n2). These results\nhold for both incremental topological ordering and cycle\ndetection. A recent breakthrough (Chen et al., 2023) uses\nnew techniques to improve the running time of incremental\ncycle detection to O(m1+o(1)); their results do not extendto topological ordering. At present there are no nontrivial\nlower bounds for either problem, that is, it is not known if\nthere exists an algorithm with update time eO(m).\nDespite the rich theoretical literature on the problem, there\nis limited empirical evidence of their success (Ajwani et al.,\n2008). As most practical data is non-worst-case, greedy\nbrute-force methods do well empirically (Baswana et al.,\n2018). The algorithms-with-predictions framework is moti-\nvated precisely by this disconnect between high-cost worst-\ncase methods and simple practical heuristics. The goal of\ndesigning learned algorithms in this framework is to extract\nbeyond-worst-case performance on typical instances, while\nbeing robust to bad predictions in the worst case.\nMore formally, in the algorithms-with-predictions frame-\nwork, an algorithm is (a) consistent if it matches the offline\noptimal (or outperforms the worst case) under perfect predic-\ntions, (b) robust if it is never worse than the best worst-case\nalgorithm under adversarial predictions, and (c) smooth if it\ninterpolates smoothly between these extremes. We call an\nalgorithm ideal if it is consistent, robust, and smooth.\nIn this paper, we initiate the study of how learned predic-\ntions can be leveraged for incremental topological ordering.\nWe propose a coarse-grained prediction model and use it\nto design a new ideal data structure for the problem; see\nSection 1.1. Moreover, we present a practical learned DFS\nalgorithm and our experiments show that using even mildly\naccurate predictions leads to significant speedups. All our\nresults extend the incremental cycle detection. Our results\ncomplement the concurrent theoretical work by (van den\nBrand et al., 2024) on dynamic graph data structures; see\nSection 1.2.\n1.1. Our Contributions\nWe first propose a prediction model for the problem and\nthen use it to formally describe our results.\nCoarse Prediction Model. For the incremental topological\nordering problem, it is natural to consider predictions on the\nnodes which give information about their relative ordering in\nthe final graph. Intuitively, a vertex is earlier in the ordering\nif it has few ancestors and many descendants. For technical\nreasons, instead of predicting the number of ancestor and\ndescendant vertices, we predict the number of ancestor and\ndescendant edges.1More formally, for each vertex v, let\nα(v)be the total number of ancestor edges of vafter all\nedges arrive, and let δ(v)be the number of descendant\nedges. An edge (u, w)is an ancestor edge ofvin there is a\ndirected path from wtov. The edge (u, w)is adescendent\nedge ofvin there is a directed path from vtou. At the\nbeginning of time, the algorithm is given predictions eα(v)\n1This is because the running time of the learned algorithm\ndepends on the number of edges traversed.\n2\n\nIncremental Topological Ordering and Cycle Detection with Predictions\nandeδ(v)forα(v)andδ(v)for each vertex v. The error in\nthe prediction for vertex visηv=|α(v)−eα(v)|+|δ(v)−\neδ(v)|. The overall prediction error of the input sequence is\nη= max v∈Vηv.2\nWe note that our prediction model predicts a small amount\nof information about the input, in contrast to models that\npredict the entire input sequence, e.g. (Brand et al., 2023;\nHenzinger et al., 2018). In particular, predictions that pre-\ndict the entire input are fine-grained —each possible input\nsequence maps to a unique perfect prediction. Our predic-\ntions are course-grained because there are many possible\ninput graphs that can map to a single perfect prediction.\nIntuitively, the more coarse-grained the prediction, the more\nrobust it is to small changes in the input.\nIdeal Learned Ordering. We present a new learned data\nstructure for the incremental topological ordering, called\nIdeal Learned Ordering . This data structure has total update\ntimeeO(min{nη+m, mη1/3, n2}). The data structure is\nideal with respect to predictions, in particular, it is:\n•Consistent: Ifη=O(1), its performance matches (up\nto logarithmic factors) the best possible running time\neO(m)of an offline optimal algorithm.\n•Robust: For any η≤m, the total running time is\neO(min{m4/3, n2}), and thus its performance is never\nworse than the best-known worst-case algorithms (Ben-\nder et al., 2015) and (Bhattacharya & Kulkarni, 2020).\n•Smooth: For any intermediate error η, the performance\nsmoothly interpolates as a function of η(andnandm),\nbetween the above two extremes.\nAt a high level, the ideal learned ordering decomposes the\nvertices into subproblems based on the predictions. On each\nsubproblem, it runs the best-known worst-case algorithm,\nwhich is warm-started with the predictions.\nLearned DFS Ordering and Empirical Results. In ad-\ndition to the above ideal algorithm, we present a simple\npractical data structure that essentially warm-starts depth-\nfirst search using predictions. We call this the learned DFS\nordering (LDFS). This data structure has total update time\nO(mη); thus each insert has running time O(η). We imple-\nment LDFS and our experiments show that with very little\ntraining data, the predictions deliver excellent speed-ups. In\nparticular, we demonstrate on real time-series data that us-\ning only 5% of training data, LDFS explores over 36x fewer\nvertices and edges than baselines, giving a 3.1x speedup\nin running time. Moreover, its performance is extremely\nrobust to prediction errors; see Figure 1b.\n2For simplicity, we assume throughout our analysis that η≥1;\nthis is to avoid η+ 1terms throughout our running times.1.2. Related Work\nRecently, (van den Brand et al., 2024) leverage predictions\nfor dynamic graph data structures. They give a general\nresult for the online matrix-vector multiplication problem\nwhere the matrix is given and a sequence of vectors arrive\nonline. They apply this to several dynamic graph prob-\nlems including cycle detection. Their data structure requires\nO(nω+nP\ni∈Vmin{δi, n})total time where δiis the er-\nror between when edge iarrives and when it is predicted\nto arrive and nωis the time to perform matrix multiplica-\ntion. Their prediction is the entire input, that is, the online\nsequence of vectors. The predictions used in this work are\nmore course-grained (only require a pair of numbers per\nvertex), and thus are robust to small perturbations to the\ninput sequence. Moreover, their work is purely theoretical\nand leaves open (a) how predictions can be leveraged for\nmaintaining topological ordering, and (b) how predictions\ncan be empirically leveraged for dynamic graph problems.\nOur work addresses both and complements their findings.\nThe Ideal Learned Ordering uses the best-known sparse\nalgorithm (Bhattacharya & Kulkarni, 2020) and the best-\nknown dense algorithm (Bender et al., 2015), referred to as\nBK and BFGT throughout. We briefly summarize them; we\nrefer to the papers for more details.\nThe BFGT algorithm maintains levels ℓ(u)for each vertex\nu: these are underestimates of the total number of ancestors\nofuin the final graph. The levels are initially set to 1.\nOn an edge insertion (x, y), ifℓ(x)> ℓ(y), they greedily\nupdate levels to maintain a topological ordering. To improve\nthe efficiency, they update y’s level even if ℓ(x)≤ℓ(y)if\na better underestimate of the number of ancestors of yis\navailable (based on its predecessors’ levels). The total time\nfor all insertions is bounded by eO(m+P\nvℓ(v)). Asℓ(v)\nis at most the number of ancestors, their total running time\niseO(n2). In Section 4, we use predictions to ensure that\nBFGT is run on subproblems containing vertices with O(η)\nancestors. Thus, the levels can only increase at most ηtimes,\nwhich leads to the total update time eO(m+nη).\nThe BK algorithm (which is based on (Bernstein & Chechi,\n2018)) also partitions the vertices into levels, but these are\nbased on their ancestors anddescendants. It is a randomized\nalgorithm and initializes the vertex levels using sampling.\nIn particular, they use an internal parameter τwhere each\nvertex v∈Vis sampled with probability Θ(log n/τ). A\nvertex is in a level (i, j)if it has iancestors and jdescen-\ndants among the sampled nodes. They bound the number\nof possible ancestors and descendants of a vertex within\na level using the parameter τ. In Section 4, we use their\nalgorithm as a blackbox with the exception that we set τ\nusing predictions. For the analysis, we give a tighter bound\nof Phase I and II of their algorithm.\n3\n\nIncremental Topological Ordering and Cycle Detection with Predictions\n1.3. Organization\nSection 2 defines the model. Learned DFS Ordering is\npresented in Section 3; which is generalized to the Ideal\nLearned Ordering in Section 4. Finally, Section 5 presents\nexperimental results. For space, many proofs and further\nexperiments are deferred to the Appendices A and B.\n2. Model and Definitions\nDirected Graphs. Consider a directed graph G= (V, E)\nwith|V|=nand|E|=m. Let (u, v)∈Edenote a\ndirected edge from utov. We say that a vertex vis an\nancestor of vertex wif there is a path from vtowin the\ngraph. We say wis adescendant ofvifvis an ancestor of\nw. A vertex is an ancestor and descendant of itself. We say\nthat an edge (u, v)is an ancestor edge of a vertex wifvis\nan ancestor of w. Similarly, an edge (u, v)is adescendant\nedge of a vertex wifuis a descendant of w. If(v, w)is an\nedge then we say that vis aparent ofwandwis achild of\nv. Atopological ordering of a directed acyclic graph (DAG)\nG= (V, E)is a labeling L:V→Zsuch that for every\nedge(v, w)∈E, we have L(v)< L(w).3A directed graph\nhas a cycle if there exist vertices uandwthat are mutually\nreachable from each other: that is, uis both an ancestor and\ndescendant of w. A topological ordering of a directed graph\nexists if and only if it is acyclic.\nIncremental Graph Problems. In the incremental topolog-\nical ordering and cycle detection problems, initially, there\narenvertices Vand no edges. The medges from the set\nEarrive one at a time and are inserted into the graph data\nstructure. Let Gtdenote the graph after tedges have been\ninserted (which we also refer to as timet). We assume that\nafter an edge is inserted, the graph continues to be acyclic.\nIf an edge insertion leads to a cycle, the algorithm must\nreport the cycle and terminate. Thus Gmdenotes the final\ngraph (after the last edge insertion that does not create a\ncycle).\nThe performance of the graph data structure is measured\nas its total running time to perform all medge insertions.\nIn Sections 3 and 4, we use the terms total cost andtotal\nupdate time and total running time interchangeably. We the\nnotation eOdefined as eO(f(n)) =O(f(n)·polylog (n)).\nPrediction Model. In the incremental topological ordering\nproblem with predictions, the data structure additionally\nobtains a prediction for each vertex v∈Vat the beginning.\nIntuitively, this prediction helps the data structure initialize\nthe label of vto be closer to a feasible topological ordering.\nFor a vertex v, letα(v)be the total number of ancestor\n3Such a topological ordering is also referred to as a weak topo-\nlogical ordering (Bender et al., 2015) as it does not require a total\nordering on the vertices.edges of vin the final graph Gm. Analogously, let δ(v)be\nthe total number of descendant edges of vin the final graph\nGm. The Learned-DFS Ordering in Section 3 receives a\nprediction eα(v)ofα(v)for each vertex v.4The prediction\nerror of a vertex visηv=|eα(v)−α(v)|.\nThe Ideal Learned Ordering in Section 4 receives a\nprediction eα(v)of the number of ancestors α(v)and\neδ(v)of the number of descendants δ(v)respectively, for\neach vertex v. The prediction error of the vertex vis\nηv=|eα(v)−α(v)|+|eδ(v)−δ(v)|.\nThe overall error is η= max vηvthroughout the paper.\n3. Learned-DFS Ordering\nIn this section, we give a simple and easy-to-implement data\nstructure that achieves O(mη)total update time. We refer\nto this algorithm as the Learned DFS Ordering (LDFS) .\n3.1. Algorithm Description\nAt all times, the algorithm maintains a level ℓ(v)for each\nvertex, which is a number from 0tom. For each vertex v,\nthe algorithm maintains a linked list in(v)ofv’s parents at\nthe same level (i.e. a linked list of all parents pofvwith\nℓ(p) =ℓ(v)). Finally, to maintain a topological ordering,\nthe algorithm additionally maintains a (global) counter a,\nand for each vertex van integer j(v)∈ {1, . . . , nm +n}.\nInitially, a=nm+1, and for each v,ℓ(v) =eα(v),in(v) =\n{}, and j(v) =nm+ 1. On insertion of an edge e=\n(u, v), ifℓ(u)> ℓ(v), setℓ(v)←ℓ(u)andin(v)← {u}.\nThen, do a forward search from vto recursively update v’s\ndescendants. That is, for each child wofv, ifℓ(v)> ℓ(w),\nupdate ℓ(w)andin(w)and recurse. Report a cycle if one\nis found; otherwise calculate a topological order Tfon all\nvertices whose levels changed during this search.\nCycle detection. After the above update concludes, if\nℓ(u) =ℓ(v), do a reverse DFS starting at u(i.e. a DFS\nwhere edges are followed backward) only following edges\nin(u)from vertices at the same level. If this search visits\nv, report a cycle. Otherwise, let Tbbe a topological order\non the vertices visited during this DFS (e.g., Tbcan be\ncomputed by ordering vertices in the order of their DFS\nfinish times).\nTopological ordering. The ordering imposed by the level\nℓ(v)on the vertices is a pseudo-topological ordering (Ben-\nder et al., 2015). Indeed, our algorithm can be viewed as a\nsimplification of the sparse algorithm in (Bender et al., 2015)\nwith the addition that levels are initialized using predictions.\n4Note that the algorithm also works if we instead receive a\nprediction of the number of ancestor vertices ofv. However, this\nincreases the running time to O(mηm\nn).\n4\n\nIncremental Topological Ordering and Cycle Detection with Predictions\nBender et al. describe how to extend their ordering to a\ntopological order by breaking ties between vertices on a\nlevel using the order in which they are traversed in the\nreverse DFS. We use a similar technique here. Concatenate\nTbandTfto create a single topological order T. Ifℓ(u) =\nℓ(v)andj(u)≥j(v), proceed through each vertex w∈T\nin reverse order. Set j(w) =a, then a=a−1, and then set\nwto the previous vertex in T.\nWe define the label of a vertex vto be L(v) =ℓ(v)(nm+\n2) +j(v). The algorithm maintains the following invariants.\nInvariant 3.1. For any edge e= (u, v)in the graph Gtat\ntimet,ℓ(u)≤ℓ(v).\nInvariant 3.2 ((Bender et al., 2015, Theorem 2.5)) .At all\ntimes, a∈ {1, . . . , nm + 1}; furthermore, ais nonincreas-\ning over the entire run of the algorithm.\nInvariant 3.3. At any time tand any vertex v, letAt(v)be\nthe set of ancestors of vinGt. Then, the level of vinGtis\nℓ(v) = max a∈At(v)eα(a).\n3.2. Analysis\nThe following proves that the algorithm is always correct.\nLemma 3.4. If the insertion of the last edge creates a cycle\ninGt, the simple learned algorithm correctly detects and\nreports it. Furthermore, for any edge e= (u, v)in the\ngraph Gtat time t,L(u)< L(v).\nWe bound the running time by bounding the cost of the\nforward search to update levels, and the reverse DFS within\na level to detect a cycle.\nWe first upper bound how big the levels can get using η.\nLemma 3.5. Letℓ0andℓmdenote the initial and final level\nof any vertex v. Then, ℓm−ℓ0≤2η.\nProof. By Invariant 3.3, vhas some ancestor u∈Gmwith\nℓm(v) =eα(u). Since ℓ0(v) =eα(v)by definition, we have\nthatℓm(v)−ℓ0(v) =eα(u)−eα(v). Any ancestor of uis\nalso an ancestor of v, soα(v)−α(u)≥0. Thus,\nℓm(v)−ℓ0(v) =eα(u)−eα(v)\n≤eα(u)−eα(v) + (α(v)−α(u))\n= (eα(u)−α(u)) + ( α(v)−eα(v))\n≤η+η.\nLemma 3.5 is sufficient to bound the cost of all level updates\nduring the forward search.\nLemma 3.6. The total cost to update the levels of all ver-\ntices is O(mη).\nProof. To obtain the total cost of updating the levels, note\nthat each time we update the level of a vertex v, the algo-\nrithm recursively updates its children, and then checks eachof its parents to update in(v). This takes O(∆(v))time,\nwhere ∆(v)is the sum of the outdegree and indegree of v.\nThus, using Lemma 3.5 the total cost of all level updates is\nO X\nv∆(v)·(ℓm(v)−ℓ0(v) + 1)!\n=O(mη)\nTo bound the cost of the reverse DFS on a level, we bound\nthe number of incoming edges on any level at any time.\nLemma 3.7. At any time, if a vertex vhaskancestor edges\non its level then η≥k/2.\nNow we can bound the cost of the reverse DFS. The algo-\nrithm maintains incoming edges in(v)ofvfrom vertices on\nits level in a linked list. Performing the reverse DFS from v\nthus has cost O(1 +at(v)), where at(v)is the number of\nancestor edges of vfrom vertices at level ℓ(v)at time t. By\nLemma 3.7, at(v1)≤ηand thus the reverse DFS costs O(η)\nfor each insertion. Finally, combining with Lemma 3.6 and\ntheO(n)time for initialization, we get the following result.\nTheorem 3.8. The Learned DFS Ordering solves the incre-\nmental topological ordering and cycle detection problem\nwith predictions in total running time O(mη+n).\n4. Ideal Learned Ordering\nIn this section, we give an ideal learned data structure for the\nincremental topological ordering and cycle detection prob-\nlem with total update time eO(m+ min {nη, n2, mη1/3})\nformedge insertions. We refer to this algorithm as Ideal\nLearned Ordering .\nThe algorithm receives a prediction eα(v)andeδ(v)of the\nnumber of ancestor and descendant edges of each vertex in\nthe final graph Gm. By definition, |eα(v)−α(v)| ≤ηand\n|eδ(v)−δ(v)| ≤ηfor all v.\nPrediction Decomposition. At a high level, the algorithm\ndecomposes the problem instance into smaller subproblems\nbased on each vertex’s prediction, and uses the state-of-the-\nart worst-case algorithm on each subproblem based on the\ninstance’s sparsity. Recall that BK and BFGT refer to the\nbest-known sparse algorithm by (Bhattacharya & Kulkarni,\n2020) and the best-known dense algorithm by (Bender et al.,\n2015); see Section 1.2. Using a tighter analysis for these\nalgorithms under predictions, we then bound the running\ntime of each subproblem using the prediction error.\n4.1. Algorithm Description\nThe algorithm maintains an estimate ˆηwhich is an estimate\nof the overall error ηbased on edges seen so far. It also\nmaintains a level ℓ(v)for each vertex, initialized using both\neα(v)andeδ(v). It maintains a pseudo-topological ordering\nover these levels greedily. We decompose the initial set of\n5\n\nIncremental Topological Ordering and Cycle Detection with Predictions\nvertices Vinto a sequence of subproblems based on the\npredictions for each vertex. When an edge e= (u, v)ar-\nrives, it is treated as an edge insertion into each subproblem\nthat contains both uandv. The algorithm invokes the BK\nor BFGT algorithm to perform this insertion and to assign\ninternal labels within each subproblem.\nIf an edge is inserted across subproblems that violates the\nordering over the levels, the algorithm updates its estimate\nofˆηand rebuilds with an improved decomposition.\nAlgorithm setup. Letˆηibe the value of ˆηafteriedges are\ninserted. We begin with ˆη0= 1.\nWe maintain a level ℓ(v)for each vertex v. Each ℓ(v)con-\nsists of a pair of numbers: ℓ(v) = ( ℓa(v), ℓd(v)); we call\nthis the ancestor level anddescendant level ofvrespec-\ntively. The idea is that ℓa(v)andℓd(v)are initialized using\nthe predicted ancestors and descendants of vrespectively\nand updated as edges are inserted.\nAt all times, the level ℓ(v)has four possible values satisfying\nthe constraints below. These are referred to as the possible\nlevels for v.\nℓ(v)a∈ {⌈eα(v)/ˆη⌉,⌈eα(v)/ˆη⌉+ 1}\nℓ(v)d∈ {⌊eδ(v)/ˆη⌋,⌊eδ(v)/ˆη⌋ −1}\nWe maintain that for any edge e= (u, v),ℓa(u)≤ℓa(v)\nandℓd(u)≥ℓd(v).\nAt any time, the vertex set Vis decomposed into subprob-\nlems Hj,k, where the indices j, k∈ {0, . . . ,⌈m/ˆηi⌉+ 1}.\nEach subproblem H:=Hj,kis a subgraph of Gtand rep-\nresents an instance of the incremental topological ordering\nproblem (possibly at an intermediate state with some edges\nalready inserted). A vertex vcan be part of at most four\nsubproblems, indexed by one of its possible levels:\nH(v) ={Hj,k|(j, k)is a possible level of v}.\nAs each vertex is in at most four subproblems, the algorithm\nmaintains O(n)subproblems at any point; note that “empty”\nsubproblems are not maintained.\nInitialization and Build. We first describe how to perform\naBUILD on a graph Gt;BUILD is called each time the\nestimate ˆηchanges. At initialization, BUILD (G0) adds each\nvertex vto the subproblems H(v). If a sequence of edge\ninsertions e1, . . . , e tare such that tth insertion causes ˆηtto\nbe updated, then BUILD (Gt) first updates H(v)for each\nvbased on the updated value of ˆηtand adds vtoH(v).\nThen it calls INSERT (ei)fori∈ {1, . . . , t}using the insert\nalgorithm described next.\nThe insert algorithm uses a further subroutine BUILD -\nBFGT (H), which is used to “switch” a subproblem from\nthe sparse case to the dense case. Let VHandEHbe thevertices and edges currently in H.BUILD -BFGT initializes\nan instance of BFGT on vertices VH, and then inserts all\nedges in EHone by one using BFGT.\nEdge Insertion. On the insertion of the tth edge et,IN-\nSERT(et)first recursively updates the ancestor and descen-\ndant levels of vanduinGt. That is, if ℓa(u)> ℓa(v), set\nℓa(v) =ℓa(u)and recurse on all out-edges of v. Similarly,\nifℓd(u)< ℓd(v), setℓd(u) =ℓd(v)and recurse on all\nin-edges of u. This maintains the following invariant.\nInvariant 4.1. For any edge e= (u, v),ℓa(u)≥ℓa(v)and\nℓd(u)≤ℓd(v).\nIf for any vertex v, the updated value of ℓ(v)is not one of\nthe possible levels of v, the algorithm doubles ˆη(i.e. set\nˆηi= 2ˆηi−1) and calls B UILD onGt.\nNext, we describe how the algorithm inserts etinto all\nsubproblems H∈ H(u)∩ H(v)using the BK or BFGT\nalgorithm based on whether the subproblem is sparse or\ndense. Without predictions, a graph is termed sparse if\nm=o(n3/2)and dense otherwise. To determine if a sub-\nproblem with predictions is sparse or dense, the algorithm\ntakes error ˆηinto account. More formally, let n′andm′\ndenote the number of vertices and edges in a subproblem H\nprior to the insertion of etintoH. Then:\n•(Sparse ) Ifm′+ 1< n′η2/3, it inserts etto the sub-\nproblem Husing BK;\n•(Dense ) ifm′> n′η2/3, it inserts etto subproblem H\nusing BFGT;\n•(Sparse to dense transition ) ifm′< n′η2/3and\nm′+ 1> n′η2/3, it calls BUILD -BFGT (H) first, then\ninserts etintoHusing BFGT.\nWe refer to the label within a subproblem assigned by the\nBFGT or BK algorithm as an internal label of the vertex.\nIf after tedges are inserted (for any t) we have ˆη > n and\ntˆη1/3> n2, the algorithm ignores all predictions and reverts\nto using the worst-case BFGT. The algorithm creates a new\ninstance of BFGT using the vertices in Gt, and inserts all t\nedges into this BFGT instance one by one. All future edges\nare inserted into this BFGT instance.\nDefining Labels. For any vertex v, leti(v)be the internal\nlabel of vin subproblem Hℓ(v). Letkbe a positive integer\nlarger than the internal label of any node in a graph with n\nvertices in either BFGT or BK (we note that both algorithms\nmaintain only nonnegative labels). Define the label Lofv\nasL(v) =k(ℓa(v) +m−ℓd(v)) +i(v).\n4.2. Analysis\nWe analyze the correctness and running time of Ideal\nLearned Ordering.\n6\n\nIncremental Topological Ordering and Cycle Detection with Predictions\nCorrectness. First, we show that if a cycle exists, then it is\ncorrectly reported by the algorithm.\nBy Invariant 4.1, if the insertion of an edge creates a cycle,\nall vertices in the cycle must have the same level ℓ. The\nalgorithm maintains the invariant that at all times Hℓ(v)∈\nH(v), so all vertices and edges in the cycle must be in some\nsubproblem Hand thus will be detected by BFGT or BK.\nLemma 4.2. For any edge e= (u, v)inGt,L(u)≤L(v).\nProof. Ifℓa(u) +m−ℓd(u)< ℓa(v) +m−ℓd(v)then the\nlemma holds since i(u)< k.\nOtherwise, suppose ℓa(u)+m−ℓd(u)≥ℓa(v)+m−ℓd(v).\nBy Invariant 4.1, ℓa(u)≤ℓa(v)andm−ℓd(u)≤m−\nℓd(v); thus we must have ℓ(u) =ℓ(v). Thus, i(u)andi(v)\nare both assigned by BFGT or BK on Hℓ(u). By correctness\nof BFGT and BK, i(u)< i(v).\nRunning Time Analysis. We give an overview of the\nrunning time analysis of Ideal Learned Ordering. Proofs are\ndeferred to Appendix A.\nLemma 4.3 bounds the number of ancestors and descendants\nof a vertex within the graph of any subproblem.\nLemma 4.3. For any subproblem Hj,kand vertex v∈\nHj,k,vhas at most 2(ˆη+η)ancestor edges and 2(ˆη+η)\ndescendant edges in Hj,k.\nLemma 4.4 shows that the estimate ˆηmaintained by the\nalgorithm is never more than 2η.\nLemma 4.4. At all times, ˆη≤2η\nLemma 4.5 and Lemma 4.6 bound the cost of running BFGT\nand BK any subproblem respectively.\nLemma 4.5. Consider a subproblem Hwithn′vertices\nandm′edges that are inserted into Hone by one. If each\nvertex in Hhas at most O(η)edge ancestors, then the total\nrunning time of running BFGT on HiseO(n′η+m′)time.\nLemma 4.6. Consider a subproblem Hwithn′nodes and\nm′edges, such that: (1) m′<ˆη2n′/log2n′, (2) each\nvertex in Hhas at most O(η)edge ancestors and O(η)\nedge descendants, and (3) ˆη=O(η). Then running BK\nonHwith parameter τ=n1/3ˆη2/3/m1/3takes total time\neO(m′η1/3)in expectation.\nFinally, Theorem 4.7 analyzes the total running time.\nTheorem 4.7. Ideal Learned Ordering has total expected\nrunning time eO(min{mη1/3, nη, n2}).\n5. Experiments\nThis section presents experimental results for the Learned\nDFS Ordering (LDFS) described in Section 3. Our exper-\niments show that using prediction significantly speeds upperformance over baseline solutions on real temporal data.\nMoreover, only a small amount of training dataset (e.g., 5%)\nis sufficient to see one or two orders of magnitude of im-\nprovement. Finally, we show that LDFS is extremely robust\nto errors in the predictions.\nOur implementation and datasets can be found at\nhttps://github.com/AidinNiaparast/\nLearned-Topological-Order .\nAlgorithms. We compare LDFS against two natural base-\nline solutions that we call DFS I and DFS II. Each of the\nthree algorithms use a greedy depth-first-search approach\nto maintain a topological ordering, with the difference that\nLDFS warm-starts its levels using predictions.\nDFS I. The first algorithm is equivalent to LDFS with zero\npredictions: that is, eα(v) = 0 for each v.\nDFS II. This algorithm was presented by (Marchetti-\nSpaccamela et al., 1993) for incremental topological order-\ning and revisited by (Franciosa et al., 1997) for incremental\nDFS. It has total update time O(mn). (Baswana et al., 2018)\nperform an empirical study on incremental DFS algorithms\nand show that DFS II (which they call FDFS) is the state-\nof-the-art on DAGs. DFS II maintains exactly one vertex at\neach level. When an edge (u, v)is inserted, if l(v)< l(u),\nthe algorithm performs a partial DFS to detect all the ver-\nticeswreachable from vsuch that l(v)< l(w)< l(u), and\nupdates their levels to be larger than l(u).\nDatasets. We use real directed temporal networks from\nthe SNAP Large Network Dataset Collection (Leskovec &\nKrevl, 2014). To obtain the final DAG G, we randomly\npermute the vertices and only keep the edges that go from\nsmaller to larger positions (this ensures Gis acyclic). Then,\nwe sort the edges in increasing order of their timestamps to\nobtain the sequence of edge insertions. Table 1 summarizes\nthese datasets. Note that these graphs are sparse.\nPredictions. To generate the predictions for LDFS, we use\na contiguous portion of the input sequence as the training set.\nConsider the graph that results from inserting the training\nset edges into an empty graph. For each node v, we define\neα(v)to be the number of v’s ancestor edges in that graph.\nExperimental Setup and Results. On real datasets, we\ncompare LDFS to DFS I and II in terms of the number\nof edges and vertices processed ( cost) in Table 2a and in\nterms of runtime in Table 2b. The last 50% of the data in\nincreasing order of the timestamps is used as the test data\nin all of the experiments in Table 2. The training data for\nLDFS is a contiguous subsequence of the data that comes\nright before the test data.\nWe include plots for the email-Eu-core5(Paranjape et al.,\n5https://snap.stanford.edu/data/email-Eu-core-temporal.html\n7\n\nIncremental Topological Ordering and Cycle Detection with Predictions\n2017) dataset, which contains the email communications\nin a large European research institution. A directed edge\n(u, v, t )in this dataset means that usent an e-mail to vat\ntimet. Figure 1a shows how the training data size affects\nthe runtime of LDFS. Figure 1b is a robustness experiment\nshowing performance versus the noise added to predictions.\nFor testing robustness to prediction error, we add noise to\nthe predictions. We first generate predictions as described.\nThen, we calculate the standard deviation of the prediction\nerror, which we denote by SD(predictions). Finally, we\nadd a normal noise with mean 0 and standard deviation\nSD(noise) = C·SD(predictions) (for some constant C) in-\ndependently to all of the predictions to obtain our noisy\npredictions. We repeat the experiment 10 times, each time\nregenerating the noisy predictions; we plot the mean and\nstandard deviation of the resulting running time in Figure 1b.\nTable 1. The number of nodes and edges in the real datasets from\nSNAP. The input sequence has duplicate edges (referred to as\ntemporal edges). The length of the sequence is the number of\ntemporal edges. Static edges are the number of distinct edges.\nNodes Static Edges Temporal Edges\nEmail-Eu-core 918 12320 171617\nCollegeMsg 1652 9790 27931\nMath Overflow 14839 45267 53499\nTable 2. Performance of LDFS against DFS I and II. The test data\nis the last 50% of the data. Columns LDFS(5) and LDFS(50)\ncorrespond to the performance of LDFS when 5% and 50% of the\ndata are used for training, respectively.\nLDFS(5) LDFS(50) DFS I DFS II\nEmail-Eu-core 8.4e+3 2.6e+3 5.0e+5 3.1e+5\nCollegeMsg 9.6e+3 5.4e+3 1.2e+5 6.8e+5\nMath Overflow 4.4e+4 2.8e+4 2.8e+5 3.9e+7\n(a) Cost (# nodes and edges processed)\nLDFS(5) LDFS(50) DFS I DFS II\nEmail-Eu-core 0.071 0.078 0.274 0.226\nCollegeMsg 0.021 0.016 0.101 0.336\nMath Overflow 0.094 0.078 0.241 18.373\n(b) Running Time (s)\nDiscussion. Results in Table 2 demonstrate that, in all\ncases, even a very basic prediction algorithm can signifi-\ncantly enhance performance over the baselines. Only 5%\nof historical data is needed to see a significant difference\nbetween our methods’s performance and the baselines; in\nsome cases up to a factor of 36 in cost. Better predictions\nobtained from 50% of historical data improve performance\nfurther, up to a factor of 116.\nFinally, Figure 1b shows that LDFS is very robust to\nbad predictions. For example, note that if SD(noise) ≥\n(a)\n(b)\nFigure 1. Total cost (number of nodes and edges processed) of\nLDFS compared to the two baselines for email-Eu-core dataset,\nin logarithmic scale. In Figure 1a, the x-axis is the percentage\nof the input sequence used as training data for LDFS. Figure 1b\nshows the effect of adding noise to predictions on the cost of LDFS.\nThe first 5% of the input is used as the training data and the last\n95% as the test data. For different values of C, a normal noise\nwith mean 0 and standard deviation ( SD) ofC·SD(predictions) is\nindependently added to each prediction. This noise is regenerated\n10 times. The x-axis is SD(noise)/ SD(predictions). The blue line\nis the mean and the cloud around it is the SD of these experiments.\n2·SD(predictions) , then ≈61% of the noisy predictions\nhave noise added to them that is at least SD(predictions)6—\nthus, the relative value of the predictions becomes largely\nrandom for many items. Since the LDFS algorithm’s perfor-\nmance only depends on how predictions for different nodes\nrelate to each other (not their value), this represents a signif-\nicant amount of noise, effectively nullifying the predictions\nof many nodes. Nonetheless, LDFS still outperforms the\nbaselines even for this extreme stress test. Moreover, in-\ncreasing the noise degrades the performance confirming that\nthe efficiency does depend on the quality of predictions.\nIn Appendix B, we include additional plots for the datasets\nin Table 1. We also investigate the effect of edge density\non performance for synthetic DAGs. These experiments\nfurther support our conclusions; in particular, even for very\ndense DAGs, our algorithm still outperforms the baselines,\nalthough with smaller margins.\n6In a normal distribution, ≈61% of items are more than half a\nstandard deviation from the mean.\n8\n\nIncremental Topological Ordering and Cycle Detection with Predictions\n6. Conclusion\nThis paper gave the first dynamic graph data structure that\nleverages predictions to maintain an incremental topological\nordering. We show that the data structure is ideal: that is, it\nis consistent, robust, and smooth with respect to errors in\nprediction. Thus, predictions deliver speedups on typical\ninstances while never performing worse than the state-of-\nthe-art worst case solutions. This paper is also the first\nempirical evaluation of using predictions on dynamic graph\ndata structures. Our experiments show that the theory is\npredictive of practice: predictions deliver up to 6x speedup\nfor LDFS compared to natural baselines.\nOur results demonstrate the incredible potential for improv-\ning the theoretical and empirical efficiency of data structures\nusing predictions. It would be interesting to explore how\npredictions can be leveraged for designing data structures\nfor other dynamic graph problems.\nReferences\nAjwani, D., Friedrich, T., and Meyer, U. An O(n2.75)algo-\nrithm for incremental topological ordering. ACM Trans-\nactions on Algorithms (TALG) , 4(4):1–14, 2008.\nBai, X. and Coester, C. Sorting with predictions. In Ad-\nvances in Neural Information Processing Systems , 2023.\nTo appear.\nBaswana, S., Goel, A., and Khan, S. Incremental dfs algo-\nrithms: a theoretical and experimental study. In Proceed-\nings of the Twenty-Ninth Annual ACM-SIAM Symposium\non Discrete Algorithms , pp. 53–72. SIAM, 2018.\nBender, M. A., Fineman, J. T., and Gilbert, S. A new ap-\nproach to incremental topological ordering. In Proc. 20th\nACM-SIAM Symposium on Discrete algorithms (SODA) ,\npp. 1108–1115. SIAM, 2009.\nBender, M. A., Fineman, J. T., Gilbert, S., and Tarjan,\nR. E. A new approach to incremental cycle detection\nand related problems. ACM Transactions on Algorithms\n(TALG) , 12(2):1–22, 2015.\nBernstein, A. and Chechi, S. Incremental topological sort\nand cycle detection in eO(m√n)expected total time. In\nProc. 29th ACM-SIAM Symposium on Discrete Algo-\nrithms (SODA) , pp. 21–34. SIAM, 2018.\nBhattacharya, S. and Kulkarni, J. An improved algorithm\nfor incremental cycle detection and topological ordering\nin sparse graphs. In Proc. 14th Annual ACM-SIAM Sym-\nposium on Discrete Algorithms (SODA) , pp. 2509–2521.\nSIAM, 2020.Brand, J. v. d., Forster, S., Nazari, Y ., and Polak, A. On dy-\nnamic graph algorithms with predictions. arXiv preprint\narXiv:2307.09961 , 2023.\nChen, L., Kyng, R., Liu, Y . P., Meierhans, S., and Guten-\nberg, M. P. Almost-linear time algorithms for incremen-\ntal graphs: Cycle detection, sccs, s-tshortest path, and\nminimum-cost flow, 2023.\nCohen, E., Fiat, A., Kaplan, H., and Roditty, L. A la-\nbeling approach to incremental cycle detection. CoRR ,\nabs/1310.8381, 2013. URL http://arxiv.org/\nabs/1310.8381 .\nDavies, S., Moseley, B., Vassilvitskii, S., and Wang,\nY . Predictive flows for faster ford-fulkerson. In\nKrause, A., Brunskill, E., Cho, K., Engelhardt, B.,\nSabato, S., and Scarlett, J. (eds.), Proc. of the\n40th International Conference on Machine Learn-\ning (ICML) , volume 202 of Proceedings of Machine\nLearning Research , pp. 7231–7248. PMLR, 23–29 Jul\n2023. URL https://proceedings.mlr.press/\nv202/davies23b.html .\nDinitz, M., Im, S., Lavastida, T., Moseley, B., and Vassilvit-\nskii, S. Faster matchings via learned duals. In Ranzato,\nM., Beygelzimer, A., Dauphin, Y . N., Liang, P., and\nVaughan, J. W. (eds.), Proc. 34th Conference on Advances\nin Neural Information Processing Systems (NeurIPS) , pp.\n10393–10406, 2021. URL https://proceedings.\nneurips.cc/paper/2021/hash/\n5616060fb8ae85d93f334e7267307664-Abstract.\nhtml .\nFerragina, P., Lillo, F., and Vinciguerra, G. On the perfor-\nmance of learned data structures. Theoretical Computer\nScience (TCS) , 871:107–120, 2021.\nFranciosa, P. G., Gambosi, G., and Nanni, U. The incremen-\ntal maintenance of a depth-first-search tree in directed\nacyclic graphs. Information processing letters , 61(2):\n113–120, 1997.\nHaeupler, B., Kavitha, T., Mathew, R., Sen, S., and Tarjan,\nR. E. Incremental cycle detection, topological ordering,\nand strong component maintenance. ACM Transactions\non Algorithms (TALG) , 8(1):1–33, 2012.\nHenzinger, M., Krinninger, S., and Nanongkai, D. Decre-\nmental single-source shortest paths on undirected graphs\nin near-linear total update time. Journal of the ACM\n(JACM) , 65(6):1–40, 2018.\nHenzinger, M., Lincoln, A., Saha, B., Seybold, M. P., and\nYe, C. On the complexity of algorithms with predictions\nfor dynamic graph problems. In Innovations in Theoreti-\ncal Computer Science (ITCS) , 2024.\n9\n\nIncremental Topological Ordering and Cycle Detection with Predictions\nItaliano, G. F. Amortized efficiency of a path retrieval data\nstructure. Theoretical Computer Science , 48:273–281,\n1986.\nKraska, T., Beutel, A., Chi, E. H., Dean, J., and Poly-\nzotis, N. The case for learned index structures. In\nDas, G., Jermaine, C. M., and Bernstein, P. A. (eds.),\nProc. 44th Annual International Conference on Manage-\nment of Data, (SIGMOD) , pp. 489–504. ACM, 2018.\ndoi: 10.1145/3183713.3196909. URL https://doi.\norg/10.1145/3183713.3196909 .\nLattanzi, S., Svensson, O., and Vassilvitskii, S. Speeding\nup bellman ford via minimum violation permutations.\nIn Krause, A., Brunskill, E., Cho, K., Engelhardt, B.,\nSabato, S., and Scarlett, J. (eds.), International Confer-\nence on Machine Learning, ICML 2023, 23-29 July 2023,\nHonolulu, Hawaii, USA , volume 202 of Proceedings of\nMachine Learning Research , pp. 18584–18598. PMLR,\n2023. URL https://proceedings.mlr.press/\nv202/lattanzi23a.html .\nLeskovec, J. and Krevl, A. SNAP Datasets:\nStanford large network dataset collection.\nhttp://snap.stanford.edu/data , June\n2014.Marchetti-Spaccamela, A., Nanni, U., and Rohnert, H. On-\nline graph algorithms for incremental compilation. In\nInternational Workshop on Graph-Theoretic Concepts in\nComputer Science , pp. 70–86. Springer, 1993.\nMcCauley, S., Moseley, B., Niaparast, A., and Singh, S.\nOnline list labeling with predictions. In Advances in\nNeural Information Processing Systems , 2023.\nPanzarasa, P., Opsahl, T., and Carley, K. M. Patterns and\ndynamics of users’ behavior and interaction: Network\nanalysis of an online community. Journal of the American\nSociety for Information Science and Technology , 60(5):\n911–932, 2009.\nParanjape, A., Benson, A. R., and Leskovec, J. Motifs in\ntemporal networks. In Proceedings of the tenth ACM\ninternational conference on web search and data mining ,\npp. 601–610, 2017.\nSakaue, S. and Oki, T. Discrete-convex-analysis-based\nframework for warm-starting algorithms with predictions.\nIn35th Conference on Neural Information Processing\nSystems (NeurIPS) , 2022.\nvan den Brand, J., Forster, S., Nazari, Y ., and Polak, A. On\ndynamic graph algorithms with predictions. In Sympo-\nsium on Discrete Algorithms (SODA) , 2024.\n10\n\nIncremental Topological Ordering and Cycle Detection with Predictions\nA. Omitted Proofs\nProof of Lemma 3.4. By Invariant 3.1, for any cycle Cin the\ngraph, all vertices in Cmust at the same level. Each time\nwe add an edge e= (u, v), ifℓ(u) =ℓ(v), the algorithm\nchecks whether the addition of this edge creates a cycle\nwithin that level through a reverse depth-first search.\nNow, assume there is no cycle in Gt; we show that a weak\ntopological sort is maintained. A weak topological sort\nis trivially maintained in G0, so assume inductively that\nthe algorithm correctly maintains a weak topological sort\ninGt−1. Consider an edge (u, v)∈Gt. Ifℓ(u)< ℓ(v),\nthen the label of uis less than the label of vsince\ni(u), i(v)≤nm+ 1. Ifℓ(u) =ℓ(v), then we split into\ncases based on if the label of uorvwas changed during\nthe updates after the tth edge was inserted. If neither u\norvwere updated, the labels continue to be a topological\nordering as in Gt−1. It is not possible that vis updated but\nuis not: for any vvisited during DFS, since ℓ(u) =ℓ(v),\nuis also visited; for any vwhose label is updated, umust\nhave a strictly larger label than any other parent of v. Ifuis\nupdated and vis not, then i(u)is set to a; since adecreases\neach time some i(w)is set, we must have i(u)< i(v).\nIf both uandvare updated, umust come before vinT.\nAgain, since adecreases each time some i(w)is set, we\nmust have i(u)< i(v).\nProof of Lemma 3.7. LetAdenote the set of all ancestors of\nvat level ℓ(v)at the current time. Consider the vertices in\nAafter all edges are inserted (in Gm): since Gmis acyclic,\nthere must be at least one vertex z∈Asuch that no vertex\nw∈Ahas that wis an ancestor of zinGm.\nSince zis an ancestor of v, all ancestor edges of zare\nancestor edges of v. However by definition of z, an ancestor\nedge of any w∈Ais never an ancestor edge of z. All k\nancestor edges of von its level are ancestor edges of some\nw∈A. Therefore, α(v)≥α(z) +k, soα(v)−α(z)≥k.\nAs levels only increase ℓ(v)≥eα(v). By Invariant 3.3,\nℓ(v)≤eα(z); equivalently, −eα(v) +eα(z)≥0. Summing\nthe above two inequalities, we get\n(α(v)−eα(v)) + (eα(z)−α(z))≥k.\nThus, either ηv≥k/2orηz≥k/2.\nProof of Lemma 4.3. LetHrefer to the subgraph Hj,k\nafter the last edge is inserted into it (thus, Hincludes edges\nthat are inserted in the future, whereas Hj,kdoes not). We\nuseαH(v)andδH(v)to denote the number of number of\nancestor and descendant edges of a vertex vinH.Letube an ancestor of vinH, such that no ancestor edge of\nvinHis an ancestor edge of uinH. Such a ualways exists\nasHis acyclic and can be found by recursively following\nin-edges of v.\nBy definition of u, all ancestor edges of uare ancestor edges\nofv(inGm); however, no ancestor edges of vinHare\nancestor edges of w(inGm). Thus, α(v)≥α(u) +αH(v),\nsoα(v)−α(u)≥aH(v).\nAs both vanduare in Hj,kwe can bound the difference\nof their predictions using ˆη. That is, j≥ ⌈eα(v)/ˆη⌉, and\ntherefore j≥eα(v)/ˆη. Similarly, j≤ ⌈eα(u)/ˆη⌉+ 1, so\nj≤eα(u)/ˆη+ 2. Combining, eα(u)/ˆη+ 2≥eα(v)/ˆη, so\neα(u)−eα(v)≥ −2ˆη.\nSumming the two above equations, we obtain that\n(α(v)−eα(v)) + (eα(u)−α(u))≥aH(v)−2ˆη\nBy the definition, α(v)−eα(v)≤ηandeα(u)−α(u)≤η.\nSubstituting, aH(v)≤2ˆη+ 2η.\nThe analysis for the number of descendants is analogous.\nLetwbe a descendant of vinH, such that no descendant\nedge of vinHis a descendant edge of winH. By definition\nofw, all descendant edges of ware descendant edges of\nv(inGm); however, no descendant edges of vinHare\ndescendant edges of w(inGm). Therefore, δ(v)≥δ(w) +\nδH(v), soδm(v)−δ(w)≥δH(v).\nAs both vandware in Hj,k, we have that j≤ ⌊eδ(w)/ˆη⌋,\nand therefore j≤eδ(w)/ˆη. Similarly, j≥ ⌊eδ(v)/ˆη⌋ −1,\nsoj≥eδ(v)/ˆη−2. Combining, eδ(w)/ˆη≥eδ(v)/ˆη−2, so\neδ(w)−eδ(v)≥ −2ˆη.\nSumming the two above equations, we obtain that\n\u0010\nδ(v)−eδ(v)\u0011\n+\u0010\neδ(w)−δ(w)\u0011\n≥δH(v)−2ˆη.\nBy the definition, δ(v)−eδ(v)≤ηandeδ(w)−δ(w)≤η.\nSubstituting, δH(v)≤2ˆη+ 2η. As the number of ancestor\nand descendant edges are nondecreasing, this upper bound\n(inHafter all edges are inserted), is also an upper bound at\nall times in Hj,k.\nProof of Lemma 4.4. We proceed by induction. The lemma\nis trivially satisfied at time 0(since ˆη0= 1), as well as any\ntime where ˆηdoes not change.\nConsider a time when ˆηis increased, from ˆηto2ˆη; we show\nthatˆη≤2η. When ˆηis increased, there is some vertex v\nwithℓ(v)/∈ H(v). We split into two cases based on if the\nancestor level or the descendant level constraint is violated:\n11\n\nIncremental Topological Ordering and Cycle Detection with Predictions\nℓa(v)>⌈eα(v)/ˆη⌉+1, andℓd(v)<⌊eδ(v)/ˆη⌋−1. We begin\nwith the first case. Without loss of generality, consider a\nvertex vthat violates the constraint such that no ancestor of v\nviolates the constraint. Specifically, ℓa(v)>⌈eα(v)/ˆη⌉+ 1,\nwhereas ℓ(u)≤ ⌈eα(v)/ˆη⌉+ 1for all ancestors uofv.\nWhen inserting an edge e= (x, y), the algorithm updates\nthe ancestor levels of all descendants of xto have the same\nancestor levels as x; no other ancestor levels are updated.\nThus, vhas an ancestor wwitheα(w) =ℓa(v).\nNoting that the label of vcan only increase, we must have\nthateα(w) =ℓ(v)>⌈eα(v)/ˆη⌉+1. Thus,eα(w)−eα(v)>ˆη.\nSince wis an ancestor of v,α(w)< α(v), soα(v)−\nα(w)≥0. Summing the above two equations,\n(eα(w)−α(w)) + ( α(v)−eα(v))>ˆη\nThus either ηw>ˆη/2orηv>ˆη/2, soˆη <2η.\nThe analysis for the descendant constraint is identical.\nProof of Lemma 4.5. For each vertex, BFGT maintains\na vertex level (that determines the internal label for our\nalgorithm), and a vertex count. In the proof of (Bender et al.,\n2015, Theorem 3.6), each edge traversal in BFGT increases\nthe vertex level or a vertex count, and the running time of\nBFGT is upper bounded by the number of edge traversals\nplusm′(i.e.O(1)additional time for each inserted edge,\neven if no edge is traversed). Thus, our goal is to bound the\nnumber of times a vertex level or vertex count increases in\nH.\nA vertex level begins at 0 and is nondecreasing for all ver-\ntices by definition. By (Bender et al., 2015, Theorem 3.5),\nthe level of each vertex is upper bounded by the number\nof (vertex) ancestors, which is in turn upper bounded by\nthe number of edge ancestors. Since each vertex has O(η)\nvertex ancestors by Lemma 4.5, the total number of vertex\nlevel increases is eO(η), giving eO(n′η)increases overall.\nNext, we summarize how a vertex count changes over time,\nand use this to show that it increases by the maximum\nvertex level. Let ℓ=eO(η)be the maximum vertex level\nof any vertex. See the proof of (Bender et al., 2015,\nTheorem 3.6) for more details. The data structure maintains\na parameter jfor each vertex v, where jis at most\nlog2(current vertex count of v). The count for a vertex v\nbegins at 0, and increases up to 3·2j, after which it is reset\nto0. This count must increase by at least 2jover the same\ntime. Thus, so far, the number of times a vertex count is\nincremented is at most 3ℓ. The count may be reset to 0one\nadditional time (at most 3ℓmore increases); furthermore,\nthe count may at the end of the algorithm increase up to\n3·2jwithout being reset (another 3ℓmore increases). Thus,\na vertex count can be incremented at most 9ℓtimes.Proof of Lemma 4.6. The cost of BK as shown in (Bhat-\ntacharya & Kulkarni, 2020) is\neO\u0010\nm′n′/τ+n′2/τ+p\nm′3τ/n′+√\nm′n′τ\u0011\n.\nFirst, we show that if all vertices in Hhave at most O(η)\nedge ancestors and O(η)edge descendants, then the running\ntime of BK on His\neO \nm′η\nτ+n′η\nτ+r\nm′3τ\nn′+√\nm′n′τ!\n. (1)\nLet us begin with the first term of Equation 1. This term\ncomes from (Bhattacharya & Kulkarni, 2020, Lemma 2.2).\nSpecifically, there are n/τ sampled vertices in expectation;\nwe maintain all ancestors and descendants of each sampled\nvertex. This can be done efficiently using the classic data\nstructure presented in (Italiano, 1986).\nThe result as stated in (Italiano, 1986) states that the de-\nscendants of all(rather than just sampled) vertices can be\nmaintained in O(nm)time. Our results require a slightly\nstronger analysis.7For completeness, we summarize this\ntighter analysis here. The bounds in (Italiano, 1986) are\nbased on a potential function analysis, where each ver-\ntexvhas (using the notation of (Italiano, 1986)) poten-\ntial−(|vis(x) + 3|desc(x)), where vis (x)is the number of\ndescendant edges of x, and desc (x)is the number of de-\nscendant vertices of x. They show that their amortized cost\n(the cost plus the change in potential) of an edge insert is\nO(1), and that the potential of all nodes is nonincreasing.\nWe observe that if we only want to maintain the descendants\nof sampled vertices, we can set the potential of non-sampled\nnodes to 0; their amortized analysis argument still holds\nunder this change. By Lemma 4.3 and Lemma 4.4, the\npotential of any node is at least −6η, so the total cost to\nmaintain the descendants of each sampled vertex is O(η).\nAn essentially-identical analysis shows that the total cost\nto maintain all ancestors of sampled nodes is O(η). Since\nthere are n/τ expected sampled nodes, we obtain a total\nexpected cost of O(nη/τ ).\nNow, the second term of Equation 1. In (Bhattacharya &\nKulkarni, 2020, Lemma 2.3), it is shown that the total time in\n“phase II” is eO(n2/τ). In short, they show that the cost for\na vertex viseO(AS(v) +DS(v)), where AS(v)andDS(v)\nare the number of sampled ancestor and descendant vertices\nofvrespectively. Since a vertex is sampled with probability\n7In fact, (Bhattacharya & Kulkarni, 2020) also need a stronger\nanalysis, simpler to that presented here, since they only maintain\nthe descendants of sampled vertices.\n12\n\nIncremental Topological Ordering and Cycle Detection with Predictions\nΘ(log n/τ), they obtain expected cost eO(n/τ)per vertex.\nA vertex in Hhas only O(η)ancestor or descendant edges,\nand therefore only O(η)ancestor or descendant vertices,\nand therefore expected cost eO(logn′η/τ). Summing over\nalln′vertices of Hwe obtain the desired second term.\nThe third and fourth term of Equation 1 remain unchanged;\nthus the running time of BK on His given by Equation 1.\nSubstituting τ=n′1/3ˆη2/3/m′1/3, we obtain running time\nmη1/3. Note that BK samples vertices with probability\nΘ(log n/τ), so we need that τ= Ω(log n). This is satisfied\nfor large n′due to m′<ˆη2n′log2n′. We note that if BK\nwas to sample vertices with a fixed probability C1logn′/τ,\nwe could replace the final logn′term in our bound on m′\nwithC1.\nProof of Theorem 4.7. We bound the cost of updat-\ning the levels first; then we bound the total cost of all\nsubgraphs.\nFirst, we consider the cost of updating levels after the ith\nedge is inserted. We only traverse an edge (u, v)while\nupdating levels if the level of uis updated.\nFirst, consider an update when ˆηdoes not increase; thus\neach vertex has one of its possible levels after the update.\nEach vertex has four possible levels, so each vertex can\nhave its levels updated once per value of ˆη; thus, each edge\ncan only be traversed once per value of ˆη. This leads to\neO(mlog ˆηm)time.\nNow, the other case: if ˆηincreases, the cost of the scan is\nat most O(m); since ˆηincreases log2ˆηtimes, this gives an\nadditional eO(mlog ˆηm)time.\nNow, the cost of inserting all edges into their correspond-\ning subgraphs. Let us begin with some observations about\nthe cost of a single subgraph Hi,jwith n′vertices and\n(after all insertions are complete) m′edges, for a fixed\nˆη. Ifm′<ˆη2/3n′/log2n, then by Lemma 4.6 (note that\nm′<ˆη2/3n′/log2nimplies m′<ˆη2n′/log2n′), all edge\ninsertions into Hi,jcostm′ˆη1/3. Ifm′≥ˆη2/3n′/log2n′,\nthen the first ˆη2/3n′/log2n′insertions into Hi,jhave cost\neO(ˆηn′)by Lemma 4.6. All remaining insertions (in-\ncluding reinserting the first ˆη2/3n′/log2n′edges during\nREBUILD ) have cost O(n′ˆη)by Lemma 4.5, for O(n′ˆη)\ntotal time. Overall, all edge insertions into Hi,jtake\nO(min{n′ˆη, m′ˆη1/3})time.\nNow, we sum over all subgraphs and over all values of ˆη\nto achieve the final running time. Let ℓη= log2ˆη; thus\nwhen ˆηdoubles ℓηis incremented. Let ni,j,ˆηandmi,j,ˆη\nbe respectively the number of vertices and total number of\nedges in Hi,junder a given ˆη. Then we can bound the totaltime spent in all subgraphs as:\n⌈log2η⌉+1X\nℓη=0m/ˆη+1X\ni=0m/ˆη+1X\nj=0eO(min{ˆηni,j,ˆη,ˆη1/3mi,j,ˆη})≤\nmin\n\n⌈log2η⌉+1X\nℓη=0m/ˆη+1X\ni=0m/ˆη+1X\nj=0eO(ˆηni,j,ˆη),\n⌈log2η⌉+1X\nℓη=0m/ˆη+1X\ni=0m/ˆη+1X\nj=0eO(ˆη1/3mi,j,ˆη).\n\n\nSince each vertex is in at most 4subgraphs,\nm/ˆη+1X\ni=0m/ˆη+1X\nj=0ni,j,η≤4n.\nand\nm/ˆη+1X\ni=0m/ˆη+1X\nj=0mi,j,η≤4m.\nSubstituting, the total running time on all subgraphs is\neO(min{nη, mη1/3}).\nIf at any time ˆη > n andmˆη1/3> n2we stop the above\nprocess and use BFGT. The cost of all edge inserts while\nˆη≤niseO(n2)by the above; the cost of all remaining\ninserts is eO(n2)(Bender et al., 2015). Thus, the overall\ntotal running time is eO(min{nη, mη1/3, n2}).\nB. Additional Experiments\nIn this section, we present additional experiments; in par-\nticular, we explore how the performance is influenced by\nthe edge density of the graph in synthetic DAGs. We also\nfurther describe the experimental setup and the datasets we\nuse.\nDataset Description. Here we describe the real temporal\ndatasets we use in our experiments.\n•email-Eu-core8(Paranjape et al., 2017): This network\ncontains the records of the email communications be-\ntween the members of a large European research insti-\ntution. A directed edge (u, v, t )means that person u\nsent an e-mail to person vat time t.\n•CollegeMsg9(Panzarasa et al., 2009): This dataset\nincludes records about the private messages sent on an\nonline social network at the University of California,\nIrvine. A timestamped arc (u, v, t )means that user u\nsent a private message to user vat time t.\n8https://snap.stanford.edu/data/email-Eu-core-temporal.html\n9https://snap.stanford.edu/data/CollegeMsg.html\n13\n\nIncremental Topological Ordering and Cycle Detection with Predictions\n•Math Overflow10(Paranjape et al., 2017): This is a tem-\nporal network of interactions on the stack exchange\nweb site Math Overflow11. We use the answers-to-\nquestions network, which includes arcs of the form\n(u, v, t ), meaning that user uanswered user v’s ques-\ntion at time t.\n(a)\n(b)\nFigure 2. Performance comparison for different edge densities on\nsynthetic DAGs (in logarithmic scale). The number of nodes is\nn= 1000 , and we increase pin the x-axis (in logarithmic scale).\nWe use the first 5% of the input as the training data for LDFS\n(our algorithm), and the last 95% is used as the test data for all the\nalgorithms. The blue lines correspond to the results for LDFS, with\ndifferent amounts of perturbation added to the predictions. The\nperturbation is a normal noise with mean 0 and standard deviation\nC.SD(predictions) that is independently added to each prediction,\nwhere SD(predictions) is the standard deviation of the initial pre-\ndictions. We include the results for C= 0,1,2. The blue lines are\nthe average of 5 different runs, each time regenerating the noise.\nFigures 2a and 2b illustrate the cost (number of nodes and edges\nprocessed) and the runtime of these experiments, respectively.\nExperimental Setup and Results. We use Python 3.10\non a machine with 11th Gen Intel Core i7 CPU 2.80GHz,\n32GB of RAM, 128GB NVMe KIOXIA disk drive, and\n64-bit Windows 10 Enterprise to run our experiments. Note\nthat the cost of the algorithms, i.e., the total number of edges\nand nodes processed, is hardware-independent.\nThe datasets we use might include duplicate arcs, but both\nour algorithm and the baselines skip duplicate edges, both\n10https://snap.stanford.edu/data/sx-mathoverflow.html\n11https://mathoverflow.net/in the training phase and the test phase. To check if an arc\nalready exists in the graph, we use the set data structure in\nPython, which has an average time complexity of O(1)for\nthe operations that we use.\nWe use a random permutation of the nodes for the initial\nlevels of the nodes in the DFS II algorithm. For all the ex-\nperiments on this algorithm, we regenerate this permutation\n5 times and report the average of these runs.\nTo generate the synthetic DAGs for the experiments on\nthe edge density, we set V={1, . . . , n }, and for each\n1≤u < v ≤n, we sample the edge (u, v)independently\nat random with some (constant) probability p. We randomly\npermute the edges to obtain the sequence of inserts.\nFigure 2 compares the performance of LDFS and the two\nbaselines on synthetic DAGs with n= 1000 nodes and\ndifferent edge densities. Only the first 5% of the data is used\nas the training set for LDFS, and the rest is used as the test\nset. We also show the effect of adding a huge perturbation\nto the predictions. Importantly, we show that the quality of\npredictions is essential to our algorithm’s performance: for\nvery dense graphs, and sufficient additional noise added to\nthe predictions, our algorithm’s performance degrades to be\nworse than the baseline.\n(a)\n(b)\nFigure 3. email-Eu-core\nIn Figure 3, we show the runtime plots for email-Eu-core.\nThe setup is the same as that of Figure 1, except that here\nwe measure the runtime instead of the cost. Figures 4 and 5\nshow the same experiments for the other two datasets in\nTable 1.\n14\n\nIncremental Topological Ordering and Cycle Detection with Predictions\n(a)\n(b)\n(c)\n(d)\nFigure 4. CollegeMsg\nDiscussion. Figure 2 suggests that our algorithm (without\nperturbation) outperforms the baselines, even for very dense\nDAGs (note that the last point in the x-axis corresponds to\np= 1, which means that the DAG is complete). However,\nas the edge density of the DAG increases, the gap between\nour algorithm and DFS II decreases. Also for high densitiesand high perturbations, our algorithm still performs reason-\nably compared to other baselines in terms of cost (which is\nthe main focus of the paper). Another observation is that\nLDFS is more robust to perturbation on sparse graphs. Fi-\nnally, Figures 3, 4, and 5 further support our conclusions in\nSection 5.\n(a)\n(b)\n(c)\n(d)\nFigure 5. Math Overflow\n15",
  "textLength": 64468
}