{
  "paperId": "a2b0a4f671315185ecd5836e781609bb49aecdc1",
  "title": "Towards Establishing Guaranteed Error for Learned Database Operations",
  "pdfPath": "a2b0a4f671315185ecd5836e781609bb49aecdc1.pdf",
  "text": "Published as a conference paper at ICLR 2024\nTOWARDS ESTABLISHING GUARANTEED ERROR FOR\nLEARNED DATABASE OPERATIONS\nSepanta Zeighami∗\nUC Berkeley\nzeighami@berkeley.eduCyrus Shahabi\nUniversity of Southern California\nshahabi@usc.edu\nABSTRACT\nMachine learning models have demonstrated substantial performance enhance-\nments over non-learned alternatives in various fundamental data management op-\nerations, including indexing (locating items in an array), cardinality estimation\n(estimating the number of matching records in a database), and range-sum esti-\nmation (estimating aggregate attribute values for query-matched records). How-\never, real-world systems frequently favor less efficient non-learned methods due\nto their ability to offer (worst-case) error guarantees — an aspect where learned\napproaches often fall short. The primary objective of these guarantees is to ensure\nsystem reliability, ensuring that the chosen approach consistently delivers the de-\nsired level of accuracy across all databases. In this paper, we embark on the first\ntheoretical study of such guarantees for learned methods, presenting the necessary\nconditions for such guarantees to hold when using machine learning to perform in-\ndexing, cardinality estimation and range-sum estimation. Specifically, we present\nthe first known lower bounds on the model size required to achieve the desired\naccuracy for these three key database operations. Our results bound the required\nmodel size for given average and worst-case errors in performing database oper-\nations, serving as the first theoretical guidelines governing how model size must\nchange based on data size to be able to guarantee an accuracy level. More broadly,\nour established guarantees pave the way for the broader adoption and integration\nof learned models into real-world systems.\n1 I NTRODUCTION\nRecent empirical results show that learned models perform many fundamental database operations\n(e.g., indexing, cardinality estimation) more efficiently than non-learned methods, providing signif-\nicant speed-ups and space savings (Galakatos et al., 2019; Kraska et al., 2018; Ferragina & Vin-\nciguerra, 2020; Zeighami et al., 2023; Kipf et al., 2018). Nevertheless, the lack of theoretical guar-\nantees on their performance poses a significant hurdle to their practical deployment, especially since\nthe non-learned alternatives often provide the required theoretical guarantees (Agarwal et al., 2013;\nP˘atras ¸cu & Thorup, 2006; Hellerstein et al., 1997; Bayer & McCreight, 1970). Such guarantees are\nneeded to ensure the reliability of the learned operations across all databases at deployment time,\nthat is, to ensure consistent performance of the learned model on databases where the learned model\nhad not been apriori evaluated. Thus, similar to existing worst-case bounds for non-learned methods,\na guarantee is needed that a learned operation will achieve the desired accuracy level on all possible\ndatabases. Providing such a guarantee depends on how large the learned model is (e.g., number\nof parameters of a neural network), the desired accuracy level, and the size and dimensionality of\nthe underlying databases. This paper takes the first step towards a theoretical understanding of the\nrelationship between these factors for three key database operations, offering theoretical bounds on\nthe required model size to achieve a desired accuracy on all possible databases of a certain size and\ndimensionality when using learned models to perform the operation.\nSpecifically, the three operations studied in this paper are (1) indexing: finding an item in an array,\n(2) cardinality estimation: estimating how many records in a database match a query, and (3) range-\nsum estimation: estimating the aggregate value of an attribute for the records that match a query.\nWe focus on numerical datasets and consider axis-aligned range queries for cardinality and range-\nsum estimation (i.e., queries that ask for the intersection of ranges across dimensions). Typical\n∗This work was completed when the author was a PhD student at USC’s Infolab\n1arXiv:2411.06243v1  [cs.DB]  9 Nov 2024\n\nPublished as a conference paper at ICLR 2024\nDatabase\nOperationWorst-Case\nErrorAverage-Case Error\n(Uniform Dist.)Average-Case Error\n(Arbitrary Dist.)\nIndexingn\n2ϵ+1log(1 +(2ϵ+1)u\nn)\nTHEOREM 1(√n−2) log(1 +1\n2ϵ)\nTHEOREM 2(√n−2) log(1 +1\n2ϵ)\nTHEOREM 5\nCardinality\nEstimationn\n2ϵ+1log(1 +(2ϵ+1)ud\nn)\nTHEOREM 1(√n−2) log(1 +√nd−1\n4d(d+1)ϵd−1√n)\nTHEOREM 3X\nLEMMA 2\nRange-Sum\nEstimationn\n2ϵ+1log(1 +(2ϵ+1)ud\nn)\nTHEOREM 1(√n−2) log(1 +√nd−1\n4d(d+1)ϵd−1√n)\nCOROLLARY 1X\nLEMMA 2\nTable 1: Our bounds on required model size in terms of data size, n, dimensionality, d, tolerable\nerror, ϵ, and domain size, u. Each column shows the result when ϵis the tolerable error for the\nspecified error scenario. X: No non-trivial bound possible. Base of logis 2.\nlearned approaches to the above database operations take a function approximation view of the\noperations. Let f(q)be a function that takes a query, q, as an input, and outputs the answer to the\nquery calculated from the database. For instance, in the case of cardinality estimation, f(q)will\nbe the number of records in the dataset that match the query q(andf(q)can be similarly defined\nfor indexing and range-sum estimation). At training time, a model, ˆf(q;θ)(e.g., a neural network)\nis trained to approximate f. Training is done using supervised learning, where training labels are\ncollected for different queries by performing the queries on the database using an existing method\n(e.g., for cardinality estimation, by iterating over the database and counting how many records match\na query). At test time, the models are used to obtain estimates directly (e.g., by performing a forward\npass of a neural network), providing ˆf(q;θ)as an estimate to the answer to a query q. For indexing,\nwhere the exact location of the query in the array is needed (not an estimated location returned by\nthe model), a local search around the model estimate is performed to find the exact answer.\nSuch learned approaches are currently state-of-the-art, with experimental results showing signifi-\ncantly faster query time and lower storage space when using learned methods compared with non-\nlearned methods for indexing (Kraska et al., 2018; Ferragina & Vinciguerra, 2020; Ding et al., 2020),\ncardinality estimation (Kipf et al., 2018; Negi et al., 2023) and range-sum estimation (Zeighami\net al., 2023). Furthermore, recent results also show theoretical advantages to using learned models\n(Zeighami & Shahabi, 2023; Ferragina et al., 2020; Zeighami et al., 2023), most significantly, with\nZeighami & Shahabi (2023) showing the existence of a learned index that can achieve expected\nquery time of O(log log n)under mild assumptions on the data distribution, asymptotically bet-\nter than the traditional O(logn)of non-learned methods such as binary search. However, there has\nbeen no theoretical understanding of the required modeling choices, such as the required model size,\nfor the learned approaches to provide an error guarantee across databases. Without any theoretical\nguidelines, design choices are made through empirical hyperparameter tuning, leading to choices\nwith unknown performance guarantees at deployment time.\n1.1 O URRESULTS\nIn this paper, we present the first known bounds on the model size needed to achieve a desired\naccuracy when using machine learning to perform indexing, cardinality estimation and range-sum\nestimation. We provide bounds on the required model size , defined as the smallest possible size for\na model to achieve error at most ϵon all d-dimensional datasets of size n. We measure model size in\nterms of number of bits required to store a model (which translates to the number of model param-\neters by considering the storage precision for the parameters). We refer to ϵas the tolerable error\nparameter , which denotes the maximum error that can be tolerated in the system. We thoroughly\nstudy the required model size in two different scenarios, namely when considering the worst-case\nand average-case error. That is, ϵcan be provided in terms of worst-case or average-case error across\nqueries that can be tolerated for alldatabases (i.e., worst-case across databases). Table 1 summarizes\nour main results, which we further discuss considering the two error scenarios in turn.\nFirst, suppose our goal is to answer all possible queries with error at most ϵacross alld-dimensional\ndatasets of size n. The results in the second column of Table 1, summarizing our Theorem 1 in\nSec 3.1, provide a lower bound on the required model size to achieve this. For example, for indexing,\nto be able to guarantee error at most ϵon all possible queries and datasets of size n, one must use a\nmodel whose size exceedsn\n2ϵ+1log(1 +(2ϵ+1)u\nn). Notably, the bounds depend on the domain size\nu, which is the number of possible values the records in the database can take, implicitly assuming\na finite data domain. We show in Lemma 1 in Sec. 3.1 that this is necessary: no model with finite\n2\n\nPublished as a conference paper at ICLR 2024\nsize can answer queries with a bounded worst-case error on all possible datasets with infinite domain\n(this result is mostly of theoretical interest, since data stored in a computer always has finite domain).\nIn the second scenario, our goal is to answer queries with average error of at most ϵonalld-\ndimensional datasets of size n. Assuming the queries are uniformly distributed, the third column\nin Table 1, summarizing Theorem 2, 3 and Corollary 1 in Sec. 3.2, presents our lower bounds on\nthe required model size. Our bounds in this scenario show a weaker dependency on data size and\ntolerable error parameter compared with the worst-case error scenario, and as expected, suggest\nsmaller required model size. Interestingly, bounds do not depend on the domain size and hold when\nthe data domain is the set of real numbers, showing a significant difference between model size\nrequirements when considering the two scenarios. Thus, our results formally show that robustness\nguarantees (i.e., guarantees on worst-case error) must come at the expense of larger model sizes.\nFurthermore, the results in the last column of Table 1, summarizing our Theorem 5 and Lemma 2 in\nSec. 3.3, show that we can extend our results to arbitrary query distribution (compared with uniform\ndistribution) in the case of indexing without affecting the bounds. However, for cardinality and\nrange-sum estimation, we show in Lemma 2 that when relaxing our assumption on data distribution,\none can construct arbitrarily easy distribution to answer queries from, so that no non-trivial lower\nbound on the model size can be obtained (surprisingly, this is not possible for learned indexing).\nFinally, not presented in Table 1, for average-case error, we complement our lower bounds on the\nrequired model size with corresponding upper bounds, showing tightness of our results. Theorem 2-\n5 show that our lower bounds are tight up to an O(√n)factor, asymptotically in data size.\n1.2 DISCUSSION\nIn practice, model size is often set in practice to a fixed value or through hyperparamter tuning (Lu\net al., 2021; Kraska et al., 2018; Kipf et al., 2018; Zeighami et al., 2023) without taking data size\ninto account1. Our results show that model size indeed needs to depend on data size to be able to\nguarantee any fixed error. More specifically, for practical purposes, our results can be interpreted in\ntwo ways. In the first interpretation, given a model size and data size, our results provide a lower\nbound on the worst-case possible error. This bound shows what error can be guaranteed by a model\nof a certain size (and how bad the model can get) after it is deployed in practice. This is important,\nbecause datasets change in practice and our bound on error help quantify if a model of a given size\ncan guarantee a desired accuracy level when the dataset changes. Experiments in Sec. 4 illustrate that\nthis bound on error is meaningful, showing that models achieve error values close to what the bound\nsuggests. In the second interpretation, our results provide a lower bound on the required model\nsize to achieve a desired accuracy level across datasets. This shows how large the model needs to\nbe, to guarantee the desired accuracy, and has significant implications for resource management in\ndatabase systems. For instance, it helps a cloud service provider decide how much resources it needs\nto allocate for models to be able to guarantee an accuracy level across all its database instances.\nOverall, our results are information theoretic, showing that it is not possible for anymodel to con-\ntain enough information to answer queries on all datasets accurately if they contain less than the\nspecified number of bits. The bounds are obtained by considering the parameters of a model as a\ndata representation, and showing bounds on the required size of any data representation to achieve\na desired accuracy when performing the specific operations. Our proofs provide a novel exploration\nof the function approximation view of database operations, connecting combinatorial properties of\ndatasets with function approximation concepts. Specifically, we prove novel bounds on packing\nand metric entropy of the metric space of database query functions to prove the bounds, which are\nparticularly challenging to obtain for the average-case error. In Sec.A, we discuss various possible\nextensions of our results to queries with joins, other aggregation function such as min/max/avg. and\nother error metrics not considered in this paper.\n2 P RELIMINARIES\nSetup . We are given a dataset, D∈ Dn×d, i.e., a dataset consisting of nrecords and in ddimensions\nwith each attribute in the data domain D, where nanddare integers greater than or equal to 1.\nUnless otherwise stated, we assume D= [0,1]so that D∈[0,1]n×d(attributes can be scaled to\n[0,1]if they fall outside the range). We use Dito refer to the i-th record of the dataset (which is a d-\ndimensional vector) and Di,jto refer to the j-th element of Di. Ifd= 1(i.e.,Dis 1-dimensional),\nthenDiis the i-th element of D(and is not a vector). We study the following database operations.\n1At least explicitly, as hyperparameter tuning can implicitly account for data size\n3\n\nPublished as a conference paper at ICLR 2024\nIndexing . The goal is to find an item in a sorted array. Formally, consider a 1-dimensional sorted\ndataset D(i.e., a sorted 1-dimensional array). Given a query q∈[0,1], return the index i∗=Pn\ni=1IDi≤q, where Iis the indicator function. i∗is the index of the largest element no greater than\nqand is 0 if no such element exists. Furthermore, if q∈D,qwill be at index i∗+ 1.i∗is referred\nto as the rank ofq. Define the rank function of the dataset DasrD(q) =Pn\ni=1IDi≤q, which takes\na query as an input and outputs its rank. We have Qr= [0,1]as the domain of the rank function.\nCardinality Estimation . Used mainly for query optimization, the goal is to find how many records\nin the dataset match a range query, where the query specifies lower and upper bound conditions\non the values of each attribute. Formally, consider a d-dimensional dataset. A query predicate\nq= (c1, ..., c d, r1, ..., r d), specifics the condition that the i-th attribute is in the interval [ci, ci+ri].\nDefine Ip,qas an indicator function equal to one if a d-dimensional point p= (p1, ..., p d)matches\na query predicate q= (c1, ..., c d, r1, ..., r d), that is, if cj≤pj≤cj+rj,∀j∈[d]([k]is defined as\n[k] ={1, ..., k}for integers k). Then, the answer to a cardinality estimation query is the number of\npoints in Dthat match the query q, i.e., cD(q) =P\ni∈[n]IDi,q. We refer to cDas the cardinality\nfunction of the dataset D, which take a query as an input and outputs the cardinality of the query. We\ndefine Qc={rj∈[0,1], cj∈[−rj,1−rj], j∈[d]}, where the definition ensures cj+rj∈[0,1]\nto avoid asking queries outside of the data domain.\nRange-Sum Estimation . The goal is to calculate the aggregate value of an attribute for the\nrecords that match a query. Formally, consider a (d+ 1) -dimensional dataset Dand a query\nq= (c1, ..., c d, r1, ..., r d), where q, similar to the case of cardinality estimation, defines lower\nand upper bounds on the data points. The goal is to return the total value of the (d+ 1)-th attributes\nof the points in Dthat match the query q, i.e., sD(q) =P\ni∈[n]IDi,qDi,d+1. Here, for simplicity,\nwe overload the notation and use Ip,q, when the dimensionality of the query and predicate doesn’t\nmatch to be defined as cj≤pj≤cj+rj,∀j∈[min{d, d′}], where dis dimensionality of the\npointpandd′is the dimensionality of the predicate q.sDis called the range-sum function of the\ndataset D, which takes a query as an input and outputs the range-sum of the query. We define the\nrange-sum function domain Qsto be the same as Qc.\nWe use the term query function to collectively refer to the rank, cardinality and range-sum functions,\nand use the notation fD∈ {rD, sD, cD}to refer to all the three functions, rD,cDandsD(for\ninstance, fD≥0is equivalent to the three independent statements that rD≥0,cD≥0and\nsD≥0). We drop the dependence on Dif it is clear from context and simply use f(q). We also\nuseQfto refer to Qs,QcandQrforf∈ {r, c, s}.\nFor cardinality and range-sum estimation, often only an estimate of the query result is needed,\nbecause many applications (e.g., query optimization and data analytics) prefer a fast estimate over\na slow but exact answer. For indexing, although exact answers are needed to locate an element in\nan array, one can do so through approximation. First, an estimate of the rank function is obtained,\nand then, a local search of the array around the provided estimate (e.g., using exponential or binary\nsearch) leads to the exact result. Thus, in all cases, approximating the query function with a desired\naccuracy is the main component in answering the query, which is the focus of the rest of this paper.\nLearned Database Operations . Learned database operations use machine learning to approximate\nthe database operations as follows. First, during training, a function approximator, ˆf(.;θ)is learned\nto approximate the function f, forf∈ {r, s, c}. This is typically done through supervised learning\n(although unsupervised approaches are also possible), where for different queries sampled from Qf,\nthe operations are performed on the database to find the ground-truth answer, and the models are\noptimized through a mean squared loss. Subsequently, at test time, for a test query q,ˆf(q;θ)is\nused as an estimate of the query answer, which is obtained by performing a forward pass of the\nmodel. In practice, models used have a much fewer number of parameters than the data size, so\nthat the models don’t memorize the data but rather utilize patterns in query answers to perform the\noperations, leading to the practical gains in query answering.\nThis procedure can be formally specified as follows (both for supervised and unsupervised ap-\nproaches). First, a function ρ(D), takes the dataset as an input and generates model parameters\nθ(e.g., through training with gradient descent). Then, to answer a query qat test time, the func-\ntionˆf(q;θ)takes both the model parameters and the query as input and provides the final query\nanswer (i.e., ˆfspecifies the model forward pass). From this perspective, the model parameters θis\n4\n\nPublished as a conference paper at ICLR 2024\na representation of the dataset Dand the function ˆfonly uses this representation to answer queries,\nwithout accessing the data itself. We call ρthetraining function andˆftheinference function .\nThe Model Size Problem . An important question is how large the model needs to be to be able to\nachieve error at most ϵon datasets of size n. We quantify the model size in terms of the number of\nbits needed to store the parameters θ. The required model size is formalized as below.\nDefinition 1 (Required Model Size) .Letf∈ {r, c, s}and consider an error norm ∥.∥for functions\napproximating f, and a data domain D. LetFσbe the set of all possible training and inference\nfunction pairs, where the training function generates a parameter set of size at most size σbits.\nLetΣbe the smallest σso that there exists (ρ,ˆf)∈ Fσsuch that ∥ˆf(.;ρ(D))−fD∥ ≤ϵfor all\nD∈ Dn×d. We call Σthe required model size to achieve ∥.∥-error of at most ϵin the worst-case\nacross all d-dimensional datasets of size n.\nΣis the size of the parameter set passed from the training function to the inference function. Thus,\nin the above formulation, the training/inference functions can be arbitrarily complex. The goal of\nthis paper is to present lower bounds on Σin terms of nandϵ, and depending on the error norm ∥.∥,\nwhich is an important factor impacting the lower bounds. One expects that larger models are needed\nif the worst-case error over all queries is considered, compared with the average error. Specifically,\nforf∈ {r, s, c}, we consider the 1-norm error of approximating fwith ˆfas∥f−ˆf∥1=R\nQf|f−ˆf|,\nthe∞-norm error as ∥f−ˆf∥∞= supq∈Qf|f(q)−ˆf(q)|and the µ-norm ∥f−ˆf∥µ=R\nQf|f−ˆf|dµ\nwhere µis a probability measure over Qf.∞-norm is also called the worst-case error and µ-norm\nerror is referred to as the average error with arbitrarily distributed queries and 1-norm is referred to\nas the average error with uniformly distributed queries (note that the volume of query space is 1 for\nall the function domains, so that 1-norm indeed corresponds to uniform distribution).\n3 L OWER BOUNDS ONMODEL SIZE FOR DATABASE OPERATIONS\nWe present lower bounds on the required model size to be able to provide worst-case and average-\ncase error guarantees. For all cases, our results provide lower bounds for achieving error ϵonall\ndatasets. In other words, we show that if the model size is smaller than a specific threshold, then\nthere exists a dataset such that the error is larger than ϵ. As such, our bounds consider the worst-\ncase error across datasets, while considering either average or worst-case error across queries. We\nfirst present our results considering the worst-case error in Sec. 3.1, then present results considering\naverage-case error in Secs. 3.2 and 3.3 for uniform and arbitrary query distributions, respectively.\nProof of our results are presented in Sec. B.\n3.1 B OUNDS CONSIDERING WORST -CASE ERROR\nWe present our results when considering the worst-case error, or ∞-norm in approximation.\nFirst, for the purpose of the following theorem, suppose the datasets are discretized at the unit1\nu,\nthat is datasets are from the set Du={i\nu, u∈[u]}n×d(this reduces the data domain from the set\nof real numbers in [0,1]to multiples of1\nuin [0, 1]). Define Σ∞\nfforf∈ {r, c, s}, as the required\nmodel size to answer queries to ∞-norm error at most ϵfor all datasets in Du. For instance, Σ∞\nris\nthe smallest possible model size to be able to approximate rank function with ∞-norm at most ϵon\nall possible datasets from the data domain Du.\nTheorem 1. For any error 1≤ϵ <n\n2,\n(i) For the case of learned indexing, we have that Σ∞\nr≥n\n2ϵ+1log(1 +(2ϵ+1)u\nn),\n(ii) For the case of learned cardinality estimation, we have that Σ∞\nc≥n\n2ϵ+1log(1+ud(2ϵ+1)\nn),\nand\n(iii) For the case of learned range-sum estimation, we have that Σ∞\ns≥n\n2ϵ+1log(1+ud(2ϵ+1)\nn).\nThe theorem provides lower bounds on the required model size to be able to perform database\noperations with a desired accuracy. For instance, for the case of learned indexing, the theorem states\nthat the model size must be larger thann\n2ϵ+1log(1 +(2ϵ+1)u\nn)to be able to guarantee ∞-norm error\nϵon all datasets, or alternatively, that if the model size is less thann\n2ϵ+1log(1 +(2ϵ+1)u\nn), then for\nany model approximating the rank function, there exists a database where the model’s ∞-norm error\nis more than ϵ. We see that the required model size is close to linearly dependent on data size and\n5\n\nPublished as a conference paper at ICLR 2024\ndimensionality, while inversely correlated with the tolerable error paramter. Besides dependence\non data size, dimensionality and error, the bound shows a dependence on u, the domain size. An\ninteresting question, then, is whether similar bounds on model size will hold if the data domain is\nnot finite. The next lemma shows that the answer is no.\nLemma 1. When ϵ <n\n2and the data domain D= [0 ,1], for any finite size σ, and any\ntraining/inference function pair (ρ,ˆf)∈ F σ, there exists a dataset D∈[0,1]nsuch that\n∥ˆf(.;ρ(D))−rD∥∞> ϵ.\nWe remark that Lemma 1 may not be surprising. Storing real numbers requires infinitely many bits,\nand, although we are interested in the size of the model required to answer queries (and not the\nspace required to store the data), one might expect the model size should be similar to the space\nrequired to store that data. Lemma 1 shows this to be true in the case of worst-case error. However,\nperhaps more surprisingly, the remainder of our results in the next sections show this is not true\nwhen considering the average-case error. As such, we consider the case where D∈[0,1]n×dfor\nthe remainder of this paper.\n3.2 B OUNDS CONSIDERING AVERAGE -CASE ERROR WITH UNIFORM DISTRIBUTION\nIn this section, our results provide the average-case error assuming uniform data distribution, or\n1-norm approximation error. Average-case error corresponds with the expected performance of the\nsystem, another important measure needed for real-world deployments. A bound on 1-norm error\nacross all possible databases provides a performance guarantee for all possible databases. Our results\nin this section show how large the model needs to be to provide such guarantees.\n3.2.1 L EARNED INDEXING\nWe first present our result showing a lower bound on the required model size for learned indexing.\nTheorem 2. LetΣ1\nrbe the required model size to achieve 1-norm error of at most ϵon datasets of\nsizenwhen approximating the rank function.\n(i) For any 0< ϵ≤√n\n2, we have that Σ1\nr≥(√n−2) log(1 +1\n2ϵ−1√n).\n(ii) For any 0< ϵ≤n, we have that Σ1\nr≤nlog (e+e\nϵ+e\nn).\nPart (i) of the theorem states that, if a model whose size is less than (√n−2) log(1 +1\n2ϵ−1√n)\nbits is used to approximate the rank function, then there will exist a dataset of size nfor which the\nmodel results in error larger than ϵ. As expected, the required model size increases both as data size\nincreases, and as error threshold decreases. Furthermore, for a constant error ϵ, the results shows that\nthe require model size is Ω(√n), providing the first known results showing the increase of model\nsize with data size has to be at least in the order of√n.\nPart (ii) of the theorem shows that the asymptotic dependence on nin the lower bound is tight up\nto a√nfactor and to achieve a desired accuracy, model size does not need to increase more than\nlinearly in data size. Overall, Theorem 2 shows that for a constant error, Σ1\nrisΩ(√n)andO(n).\nThe proof of part (ii) constructs a model that achieves the bound. The model can be seen as a nearest\nneighbor encoder, modeling a dataset based on its nearest neighbor in a constructed set of datasets.\nObserve that this, and the rest of our results considering average case error do not depend on the\ndomain size (as Theorem 1 did). Thus, a fundamental difference between answering queries accu-\nrately in the worst case, compared with average case is that in the first scenario the lower bounds\ndepend on the discritization unit, while in the second scenario, model size does not depend on the\ndiscretization unit (i.e., Theorems 2-4). Furthermore, our results show that the lower bound in the\ncase of the worst-case error has a stronger dependence on the tolerable error parameter, compared\nwith when average error is considered.\n3.2.2 L EARNED CARDINALITY ESTIMATION\nNext, we present an analog of Theorem 2 for the case of cardinality estiamtion.\nTheorem 3. LetΣ1\ncbe the required model size to achieve 1-norm error at most ϵond-dimensional\ndatasets of size nwhen approximating the cardinality function.\n(i) For any 0< ϵ≤√n\n4d, we have that Σ1\nc≥(√n−2) log(1 +√nd−1\n4d(d+1)ϵd−1√n).\n(ii) For any 0< ϵ≤n, we have that Σ1\nc≤nlog(e2d(d+1)dnd−1\nϵd +e−e\nn).\n6\n\nPublished as a conference paper at ICLR 2024\nThe above theorem shows that, in the case of cardinality estimation, bounds of a similar form to the\ncase of indexing hold, however, the bounds now also depend on data dimensionality. We see that,\nasymptotically in data size, Σ1\ncisΩ(d√nlog(√n\n4dϵ))andO(dnlog2dn\nϵ), where we see a close to lin-\near required dependency on dimensionality while there is also an additional logarithmic dependency\nonncompared with the case of indexing.\n3.2.3 R ANGE -SUMESTIMATION\nFinally, we extend our results to range-sum estimation. For the discussion in this section, let Σ1\ns\nbe the required model size to achieve 1-norm error at most ϵon(d+ 1) -dimensional datasets of\nsizenwhen approximating the range-sum function. Recall that we consider d+ 1 dimensional\ndatasets here, where query predicates apply to the first ddimensions and the query answers are the\naggregation of the (d+ 1) -th dimension, as defined in Sec. 2.\nTo prove a lower bound on Σ1\ns, observe that range-sum estimation can be seen as a generalization of\ncardinality estimation. Specifically, answering range-sum queries on a d+ 1-dimensional dataset,\nwhere the d+ 1-th attribute of all the records is set to 1, is equivalent to answering cardinality\nestimation queries on the d-dimensional dataset consisting only of the first ddimensions of the\noriginal dataset. Thus, if a model with size less than Σ1\nsis able to answer range-sum queries on all\ndatasets with error at most ϵ, then it can also answer cardinality estimation queries with error at most\nϵ. This means the lower bound on model size from Thoerem 3 (i) translates to range-sum estimation\nas well. Thus, we have the following result as a corollary to Thoerem 3.\nCorollary 1. For any 0< ϵ≤√n\n4d, we have that Σ1\ns≥(√n−2) log(1 +√nd−1\n4d(d+1)ϵd−1√n).\nNext, we show that an upper bound very similar to Theorem 3 (ii) on the required model size also\nholds for range-sum estimation.\nTheorem 4. For any 0< ϵ≤n, we have that Σ1\ns≤nlog(e(2(d+2)\nϵ)d+1nd+e−e\nn).\nObserve that the upper bound on Σ1\nsis similar to Σ1\nc, but slightly larger, showing a stronger de-\npendence on dimensionality in the case of Σ1\ns. This reflects the discussion above, that range-sum\nestimation is a generalization of cardinality estimation. Indeed, the proof of Theorem 4 is a gener-\nalization of the proof of Theorem 3 (ii).\n3.3 B OUNDS CONSIDERING AVERAGE -CASE ERROR WITH ARBITRARY DISTRIBUTION\nNext, we discuss extending the results in Sec. 3.2 to an arbitrary query distribution. The following\ntheorem shows that this generalization does not impact the bounds in the case of indexing, i.e., the\ntheorem below shows that the same bounds as in Theorem 2 also hold when considering µ-norm.\nTheorem 5. LetΣµ\nrbe the required model size to achieve µ-norm error at most ϵon datasets of size\nnwhen approximating the rank function, for any continuous probability measure µover[0,1].\n(i) For any 0< ϵ≤√n\n2, we have that Σµ\nr≥(√n−2) log(1 +1\n2ϵ−1√n).\n(ii) For any 0< ϵ≤n, we have that Σµ\nr≤nlog (e+e\nϵ+e\nn).\nHowever, the next lemma shows that the lower bounds do not hold for arbitrary distributions in the\ncase of cardinality and range sum estimation.\nLemma 2. Forf∈ {c, s}, there exists a query distribution, µ, such that for any error parameter\nϵ >0, we have ∥fD−fD′∥µ≤ϵfor all D,D′∈[0,1]n×d.\nThe above lemma shows that one can construct a distribution for which the µ-norm difference be-\ntween all datasets is arbitrarily small. As a result, one can answer queries independently of the\nobserved dataset, and therefore the required model size for achieving any error is 0. The proof of\nLemma 2 creates a data distribution consisting only of queries with small ranges, so that the answer\nto most queries is zero or close to zero for any dataset. Thus, comparing Lemma 2 with Theo-\nrem 5, we see that queries having both a lower and upper bound on the attributes leads to a different\ntheoretical characteristic for cardinality and range-sum estimation compared with indexing.\n4 E MPIRICAL RESULTS\nWe present experiments comparing our bounds with the error obtained by training different models\non datasets sampled from different distributions. We train a linear model, and two neural networks\nwith a single hidden layer where the two neural networks have 10 and 50 model parameters, re-\nspectively referred to as NN-S1 and NN-S2. Small neural networks and linear models are common\n7\n\nPublished as a conference paper at ICLR 2024\n103\n104\n105\n106\n107\n108\nData Size102104106108Maximum Error\n(a) Indexing (Worst-Case)\n103\n104\n105\n106\n107\n108\nData Size103105107Maximum Error\n(b) Card. Est. (Worst-Case)\n103\n104\n105\n106\n107\n108\nData Size101\n101103105107Avg. Error\n(c) Indexing (Avg-Case)\n103\n104\n105\n106\n107\n108\nData Size101\n101103105107Avg. Error\n(d) Card. Est. (Avg-Case)Linear NN-S1 NN-S2 Sample Uniform GMM Lower Bound\nFigure 1: Theoretical Bounds in Practice\nmodeling choices for learned database operations (Kraska et al., 2018; Ferragina & Vinciguerra,\n2020; Kipf et al., 2018; Zeighami et al., 2023). We also present results for using a random samples\nas a non-learned baseline, referred to as Sample. For direct comparison, the number of samples are\nset so that Sample takes the same space as the linear model. We consider 1-dimensional datasets\nsampled from uniform and 2-component Gaussian mixture model distributions.\nRecall that our theoretical results provide bounds of the form Σ> g(n, ϵ, d ), for some function g\nspecified in our theorems. Given a model size, σ, data size, nand dimensionality d, define ϵ∗as the\nlargest ϵsuch that σ≤g(n, ϵ, d )holds. For model size σ, this implies that for any model, there exists\nad-dimensional dataset of size nwhere the error of the model is at least ϵ∗. Thus, an interpretation\nof our theoretical results is that given a model size, σ, data size, nand dimensionality, d, our results\nprovide a lower bound on the worst-case error across all d-dimensional datasets of size nfor any\nmodel of size s, where this lower bound is equal to ϵ∗as defined above. Our experiments present\nresults following this view of our theoretical bounds.\nOur experimental results are presented in Fig. 1. The color of the lines/points in the figure corre-\nsponds to the specific models, and the points in the figures show either the maximum or average error\nacross queries, observed after training the specific models on datasets sampled from either GMM\nor uniform distributions. The solid lines in the figures plot the value of ϵ∗, i.e., lower bound on the\nworst-case error across datasets, for different model and data sizes. The results are presented for in-\ndexing and cardinality estimation and under 1-norm and ∞-norm errors. Our bounds on range-sum\nestimation are similar to cardinality estimation, and are thus omitted. Note that the error is often\n(much) larger than 1, since the error is based on the absolute difference between model prediction\nand true query answers. The true query answers can be large, and their values scale with data size,\nso that the absolute error of the models is also large and increases with data size. We perform no\nnormalization of the error values, to allow direct comparison with our theoretical results.\nFirst, consider our results on the worst-case error, shown in Figs. 1 (a) and (b). As expected, the\nobserved error of different trained models increases with data size, with the models achieving lower\nerror on uniform distribution compared with a GMM. Furthermore, our theoretical bounds on the\nmodel size lie close to the error of the models on GMMs, showing that the bounds are meaningful.\nIn fact, in the case of the linear model, all the observed errors lie below the theoretical lower bound.\nThis implies that, based on our theoretical result, there exists some dataset (on which the models\nhaven’t been evaluated in this experiment) whose error lies on or above the theoretical lower bound.\nThis shows a practical benefits of our bound: one can obtain a bound on the error of the model on\nall possible databases, and beyond the datasets on which the model has been empirically evaluated.\nNext, consider our results on the average-case error, shown in Figs. 1 (c) and (d). Compared with\nthe worst-case scenario, we see that the results show a large gap between error of the models and our\ntheoretical bounds, especially so for larger model sizes. Our tightness results in Sec. 3.2, not plotted\nhere, theoretically quantify how large this gap can be. We also note tha, for both worst-case and\naverage-case scenarios, the gap between the observed error and our lower bounds on large models\ndoes not necessarily imply that our bounds are looser for larger model sizes. Such a gap can also\nbe due to the models used in practice being wasteful of their storage space for larger model sizes.\nOur results support the latter hypothesis, since we observe marginal improvement in accuracy as\nmodel size increases across both distributions. This is also supported by observations that sparse\nneural networks can achieve similar accuracy as non-sparse models while using much less space\n(e.g., Frankle & Carbin (2019)), hinting at the suboptimality of fully connected networks.\nFinally, Fig. 1 shows that sampling performs worse than learned models for uniform distribution\nwhile it performs similarly for GMMs (Sample should be compared with Linear as they both have\nthe same size). The latter can be because a linear model is not a good modeling choice for GMMs.\n8\n\nPublished as a conference paper at ICLR 2024\nNonetheless, our theoretical bounds suggest (see Sec. 5 for theoretical comparison) the gap between\nlearned models and sampling will grow as dimensionality increases ( d= 1in our experiments).\n5 R ELATED WORK\nA large and growing body of work has focused on using machine learning to speed up database\noperations, among them, learned indexing (Galakatos et al., 2019; Kraska et al., 2018; Ferragina\n& Vinciguerra, 2020; Ding et al., 2020), learned cardinality estimation (Kipf et al., 2018; Wu &\nCong, 2021; Hu et al., 2022; Yang et al., 2019; 2020; Lu et al., 2021; Negi et al., 2021) and learned\nrange-sum estimation (Zeighami et al., 2023; Hilprecht et al., 2019; Ma & Triantafillou, 2019).\nMost existing work focus on improving modeling choices, with various modeling choices such as\nneural networks (Zeighami et al., 2023; Kipf et al., 2018; Kraska et al., 2018), piece-wise linear\napproximation (Ferragina & Vinciguerra, 2020), sum-product networks (Hilprecht et al., 2019) and\ndensity estimators (Ma & Triantafillou, 2019). In all existing work, modeling choices are based\non empirical observations and hyperparametr tuning, with no theoretical understanding of how the\nmodel size should be set for a dataset to achieve a desired accuracy.\nOn the theory side, existing results in learned database theory show existence of models that achieve\na desired accuracy (Zeighami et al., 2023; Zeighami & Shahabi, 2023; Ferragina et al., 2020), show-\ning bounds on the performance of specifically constructed models to perform database operation.\nAmong them, Zeighami & Shahabi (2023) shows that a learned model can perform indexing in\nO(log log n)expected query time (i.e., better than the traditional O(logn)). Our results complement\nsuch work, showing a lower bound on the required size for any modeling approach to perform the\ndatabase operations. We note that the bounds in Zeighami et al. (2023); Zeighami & Shahabi (2023);\nFerragina et al. (2020) either hold on expectation or with a certain probability, while our bounds are\nnon-probabilistic and consider the worst-case across all datasets. Orthogonal to our work, Hu et al.\n(2022); Agarwala et al. (2021) study the number of training samples needed to achieve a desired\naccuracy for different database operations.\nMore broadly, non-learned data models, such as samples, histograms, sketches, etc. (see e.g. Cor-\nmode et al. (2012); Cormode & Yi (2020); Shekelyan et al. (2017)) are also used to estimate query\nanswers. We discuss existing lower bounds which is the focus of our paper, and refer the reader\nto Cormode et al. (2012); Cormode & Yi (2020) for a complete treatment of non-learned meth-\nods. One approach is using ϵ-approximations, where a subset of the dataset is selected (through\nrandom sampling or deterministically) and used for query answering. Wei & Yi (2018); Matou ˇsek\n& Nikolov (2015) show that for cardinality estimation, the size of such a subset has to be at least\nΩ(n\nϵlogd−1(n\nϵ))to answer queries with worst-case error at most ϵ. Using the fact that VC dimen-\nsion of orthogonal range queries is 2d(Shalev-Shwartz & Ben-David, 2014), we have that random\nsampling uses O((n\nϵ)2(d+ log δ))samples to provide error at most ϵwith probability δMustafa\n& Varadarajan (2017), while Phillips (2008) provides a deterministic method using a subset of\nsizeO(n\nϵlog2d(n\nϵ) polylog(log(n\nϵ))). Compared to our bound in Theorem 1, we observe that ϵ-\napproximations can be much less space-efficient compared with other modeling choices in high\ndimensions. This is because ϵ-approximations correspond to a restricted class of models, where the\nmodel of the data is simply a subset of the data. Relaxing this restriction, Wei & Yi (2018) considers\na special case of our Theorem 1 (ii), where they provide a lower bound ofn\nϵ(log2(n\nϵ) + log( n))on\nthe required size when answering cardinality estimation queries in twodimensions and when u=n\n(recall that uis the discretization factor in Sec. 3.1). The lower bound is tighter than our bound in\nTheorem 1 (ii), but unlike our bound that applies to arbitrary dimensionsionality and granularity, is\nonly applicable to the specialized case of d= 2 andu=n. In this setting, Wei & Yi (2018) also\npresent a non-learned data structure that matches the lower bound. Orthogonal to our work, other\nlower bounds have been presented in the streaming setting (Cormode & Yi, 2020; Suri et al., 2004).\n6 C ONCLUSION\nWe presented the first known lower bounds on the required model size to achieve a desired accuracy\nwhen using machine learning to perform indexing, cardinality estimation and range-sum estimation.\nWe studied the required model size when considering average-case and worst-case error scenarios,\nshowing how the model size needs to change based on accuracy, data size and data dimensional-\nity. Our results highlight differences in model size requirements when considering average-case\nand worst-case error scenarios and when performing different database operations. Our theoreti-\ncal results provide necessary conditions for ensuring reliability of learned models when performing\ndatabase operations. Future work includes providing tighter bounds for average-case error, bounds\nbased on data characteristics such as data distribution, and studying other database operations.\n9\n\nPublished as a conference paper at ICLR 2024\nACKNOWLEDGMENTS\nThis research has been funded by NSF grant IIS-2128661 and NIH grant 5R01LM014026. Opinions,\nfindings, conclusions, or recommendations expressed in this material are those of the author(s) and\ndo not necessarily reflect the views of any sponsors, such as NSF.\nREFERENCES\nSameer Agarwal, Barzan Mozafari, Aurojit Panda, Henry Milner, Samuel Madden, and Ion Sto-\nica. Blinkdb: queries with bounded errors and bounded response times on very large data. In\nProceedings of the 8th ACM European conference on computer systems , pp. 29–42, 2013.\nAtish Agarwala, Abhimanyu Das, Brendan Juba, Rina Panigrahy, Vatsal Sharan, Xin Wang, and\nQiuyi Zhang. One network fits all? modular versus monolithic task formulations in neural\nnetworks. In International Conference on Learning Representations , 2021. URL https:\n//openreview.net/forum?id=uz5uw6gM0m .\nRudolf Bayer and Edward McCreight. Organization and maintenance of large ordered indices. In\nProceedings of the 1970 ACM SIGFIDET (Now SIGMOD) Workshop on Data Description, Access\nand Control , pp. 107–141, 1970.\nGraham Cormode and Ke Yi. Small summaries for big data . Cambridge University Press, 2020.\nGraham Cormode, Minos Garofalakis, Peter J. Haas, and Chris Jermaine. Synopses for massive\ndata: Samples, histograms, wavelets, sketches. Found. Trends Databases , 4(1–3):1–294, January\n2012. ISSN 1931-7883. doi: 10.1561/1900000004. URL https://doi.org/10.1561/\n1900000004 .\nJialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li, Hantian Zhang,\nBadrish Chandramouli, Johannes Gehrke, Donald Kossmann, et al. Alex: an updatable adaptive\nlearned index. In Proceedings of the 2020 ACM SIGMOD International Conference on Manage-\nment of Data , pp. 969–984, 2020.\nPaolo Ferragina and Giorgio Vinciguerra. The pgm-index: a fully-dynamic compressed learned\nindex with provable worst-case bounds. Proceedings of the VLDB Endowment , 13(8):1162–1175,\n2020.\nPaolo Ferragina, Fabrizio Lillo, and Giorgio Vinciguerra. Why are learned indexes so effective? In\nInternational Conference on Machine Learning , pp. 3123–3132. PMLR, 2020.\nJonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural\nnetworks. In International Conference on Learning Representations , 2019. URL https://\nopenreview.net/forum?id=rJl-b3RcF7 .\nAlex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and Tim Kraska. Fiting-\ntree: A data-aware index structure. In Proceedings of the 2019 International Conference on\nManagement of Data , pp. 1189–1206, 2019.\nJoseph M Hellerstein, Peter J Haas, and Helen J Wang. Online aggregation. In Proceedings of the\n1997 ACM SIGMOD international conference on Management of data , pp. 171–182, 1997.\nBenjamin Hilprecht, Andreas Schmidt, Moritz Kulessa, Alejandro Molina, Kristian Kersting, and\nCarsten Binnig. Deepdb: Learn from data, not from queries! Proceedings of the VLDB Endow-\nment , 13(7), 2019.\nXiao Hu, Yuxi Liu, Haibo Xiu, Pankaj K. Agarwal, Debmalya Panigrahi, Sudeepa Roy, and Jun\nYang. Selectivity functions of range queries are learnable. In Proceedings of the 2022 Interna-\ntional Conference on Management of Data , SIGMOD ’22, pp. 959–972, New York, NY , USA,\n2022. Association for Computing Machinery. ISBN 9781450392495. doi: 10.1145/3514221.\n3517896. URL https://doi.org/10.1145/3514221.3517896 .\nAndreas Kipf, Thomas Kipf, Bernhard Radke, Viktor Leis, Peter Boncz, and Alfons Kemper.\nLearned cardinalities: Estimating correlated joins with deep learning. CIDR 2019, 9th Biennial\nConference on Innovative Data Systems Research , 2018.\n10\n\nPublished as a conference paper at ICLR 2024\nTim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned\nindex structures. In Proceedings of the 2018 International Conference on Management of Data ,\npp. 489–504, 2018.\nYao Lu, Srikanth Kandula, Arnd Christian K ¨onig, and Surajit Chaudhuri. Pre-training summariza-\ntion models of structured datasets for cardinality estimation. Proceedings of the VLDB Endow-\nment , 15(3):414–426, 2021.\nQingzhi Ma and Peter Triantafillou. Dbest: Revisiting approximate query processing engines with\nmachine learning models. In Proceedings of the 2019 International Conference on Management\nof Data , pp. 1553–1570, 2019.\nJir´ı Matou ˇsek and Aleksandar Nikolov. Combinatorial discrepancy for boxes via the gamma 2 norm.\nIn31st International Symposium on Computational Geometry (SoCG 2015) . Schloss Dagstuhl-\nLeibniz-Zentrum fuer Informatik, 2015.\nNabil H Mustafa and Kasturi R Varadarajan. Epsilon-approximations and epsilon-nets. Chapter 47\nin Handbook of Discrete and Computational Geometry, 3rd edition , 2017.\nParimarjan Negi, Ryan Marcus, Andreas Kipf, Hongzi Mao, Nesime Tatbul, Tim Kraska, and Mo-\nhammad Alizadeh. Flow-loss: Learning cardinality estimates that matter. Proc. VLDB En-\ndow., 14(11):2019–2032, jul 2021. ISSN 2150-8097. doi: 10.14778/3476249.3476259. URL\nhttps://doi.org/10.14778/3476249.3476259 .\nParimarjan Negi, Ziniu Wu, Andreas Kipf, Nesime Tatbul, Ryan Marcus, Sam Madden, Tim Kraska,\nand Mohammad Alizadeh. Robust query driven cardinality estimation under changing workloads.\nProceedings of the VLDB Endowment , 16(6):1520–1533, 2023.\nMihai P ˘atras ¸cu and Mikkel Thorup. Time-space trade-offs for predecessor search. In Proceedings\nof the thirty-eighth annual ACM symposium on Theory of computing , pp. 232–240, 2006.\nJeff M Phillips. Algorithms for ε-approximations of terrains. In International Colloquium on Au-\ntomata, Languages, and Programming , pp. 447–458. Springer, 2008.\nShai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-\nrithms . Cambridge university press, 2014.\nMichael Shekelyan, Anton Dign ¨os, and Johann Gamper. Digithist: a histogram-based data summary\nwith tight error bounds. Proceedings of the VLDB Endowment , 10(11):1514–1525, 2017.\nSubhash Suri, Csaba D T ´oth, and Yunhong Zhou. Range counting over multidimensional data\nstreams. In Proceedings of the twentieth annual symposium on Computational geometry , pp.\n160–169, 2004.\nRoman Vershynin. High-dimensional probability: An introduction with applications in data science ,\nvolume 47. Cambridge university press, 2018.\nZhewei Wei and Ke Yi. Tight space bounds for two-dimensional approximate range counting. ACM\nTransactions on Algorithms (TALG) , 14(2):1–17, 2018.\nPeizhi Wu and Gao Cong. A unified deep model of learning from both data and queries for cardi-\nnality estimation. In Proceedings of the 2021 International Conference on Management of Data ,\npp. 2009–2022, 2021.\nZongheng Yang, Eric Liang, Amog Kamsetty, Chenggang Wu, Yan Duan, Xi Chen, Pieter Abbeel,\nJoseph M Hellerstein, Sanjay Krishnan, and Ion Stoica. Deep unsupervised cardinality estimation.\nProceedings of the VLDB Endowment , 13(3):279–292, 2019.\nZongheng Yang, Amog Kamsetty, Sifei Luan, Eric Liang, Yan Duan, Xi Chen, and Ion Stoica.\nNeurocard: one cardinality estimator for all tables. Proceedings of the VLDB Endowment , 14(1):\n61–73, 2020.\n11\n\nPublished as a conference paper at ICLR 2024\nSepanta Zeighami and Cyrus Shahabi. On distribution dependent sub-logarithmic query time of\nlearned indexing. In Proceedings of the 40th International Conference on Machine Learning ,\nProceedings of Machine Learning Research, pp. 40669–40680. PMLR, 23–29 Jul 2023. URL\nhttps://proceedings.mlr.press/v202/zeighami23a.html .\nSepanta Zeighami, Cyrus Shahabi, and Vatsal Sharan. Neurosketch: Fast and approximate evalua-\ntion of range aggregate queries with neural networks. Proceedings of the ACM on Management\nof Data , 1(1):1–26, 2023.\nA D ISCUSSION\nBounding log2error . Recall that our results consider the absolute error of prediction. In the case\nof indexing, one is often interested in log2of the error, since that’s the runtime of the binary search\nperformed to find the true element after obtaining an estimate from the model. Our worst-case\nabsolute error bound directly translates to worst-case log2error bound (this is because logis an\nincreasing function). That is, there exists a dataset such that worst-case absolute error is ϵif and\nonly if there exists a dataset such that worst-case log2error is log2ϵ. Thus, to ensure log2error is at\nmost an error parameter τ, we can directly set ϵ= 2τin the bound presented in Theorem 1 to obtain\nthe bound on required model size. Regarding the average-case error, the situation is slightly more\ncomplex. Using Jensen’s inequality, we have that the average log2error can be smaller than log2of\nthe average error. This implies that, to obtain average log2error of τthe required model size may\nindeed be smaller than the bounds in Corollary 1 and part (i) of Theorems 2, 3, 5 would suggest if\nwe set ϵ= 2τ. Nonetheless, our upper bounds on the required model size (i.e., Theorem 4, Lemma 2\nand part (ii) of Theorems 2, 3, 5) still apply by setting ϵ= 2τ.\nCardinality Estimation for Joins . Cardinality estimation is often used to estimate cardinality of\njoins, which is important for query optimization. Our bounds present lower bounds on cardinality\nestimation on a single table. Note that a naive extension of our bounds to the cardinality estimation\nfor joins is to apply the bound to the join of tables. That is, if two tables have respectively d1and\nd2dimensions and if their join consists of nJelements, then we can apply our bounds in Table 1\nwithd=d1+d2andn=nJto obtain a lower bound on the required model size for estimating\nthe cardinality of the join. However, we expect such an approach to overestimate the required model\nsize, as it does not consider the join relationship between the two tables. For instance, nJ×dmay be\nmuch larger than n1×d1+n2×d2, because of duplicate records created due to the join operations.\nConsidering the join relationship, one may be able to provide bounds that depend on the original\ntable sizes and not the join size.\nOther Aggregations . Our results consider count and sum aggregations. Although we expect similar\nproof techniques as what was used to apply to other aggregations such as min/max/avg, we note\nthat studying worst-case bounds for min/max/avg may not be very informative. Intuitively, this\nis because, for such aggregations, one can create arbitrary difficult queries that require the model\nto memorize all the data points. For instance, consider datasets where the (d+ 1) -th dimension,\nwhere the aggregation is applied, only takes 0 or 1 values, while the other dimensions can take any\nvalues in a domain of size u. Now avg/min/max for any range query will have an answer between\n0 and 1 (for min/max the answer be exactly 0 or 1). However, unless the model memorizes all the\npoints exactly (which requires a model size close to the data size), its worst-case error can be up\nto 0.5. This is because queries with a very small range can be constructed that match exactly one\npoint in the database, and unless the model knows the exact value of all the points, it will not be\nable to provide a correct answer for all such queries. Note that the error of 0.5 is very large for\navg/min/max queries, and a model that always predicts 0.5 also obtains worst-case error 0.5. This\nis not the case for sum/count aggregations whose query answers range between 0 to n, and a model\nwith an absolute error of 0.5 can be considered a very good model for most queries when answering\nsum/count queries.\nTo summarize, worst-case errors of min/avg/max queries are disproportionately affected by smaller\nranges, while smaller ranges have a limited impact on the worst-case error of count/sum queries. As\nsuch we expect that for the case of min/max/avg queries, one needs to study the error for a special\nset of queries (e.g., queries with a lower bound on their range, as done in Zeighami et al. (2023),\nwhich also makes similar observations) to be able to obtain meaningful bounds. We leave such a\nstudy to future work.\n12\n\nPublished as a conference paper at ICLR 2024\nB P ROOFS\nB.1 I NTUITION\nOur proofs study the characteristics of the space of the query functions to show the number of\nbits needed to represent the elements of this space with a desired accuracy. For f∈ {r, c, s}, let\nF={fD, D∈[0,1]n×d}, be the set of all possible query functions for d-dimensional datasets of\nsizen. Then, for a norm ∥.∥,M= (F,∥.∥)is a metric space. A learned model, ˆf(.;θ), represents\nthe elements of Mwith its parameters θ. Therefore, θneeds to be large enough (contain enough\nnumber of bits), to able to represent allthe possible elements of Fwith a desired accuracy. This in\nturn depends how largeMis and how far its elements are from each other. Thus, our bounds follow\nfrom bounding suitable measures of the size of M, specifically, the packing entropy and metric\nentropy of M. Bounding the packing entropy and metric entropy of Mis non-trivial, especially\nwhen considering the 1-norm error, and requires an in-depth analysis of the metric space, relating\ncombinatorial and approximation theoretic concepts. We note that the packing entropy argument\nis, in essence, the same argument also used by Wei & Yi (2018) in obtaining (part of) their lower\nbounds (see Sec. 5 for the difference in our results).\nIntutiveily, our proofs for the lower bounds on the required model size proceed as follows. Note that\nif a model has size σbits there are exactly 2σdifferent possible model parameter settings. Thus,\nwhen there are more than 2σdifferent possible datasets, multiple datasets must be mapped to the\nsame model parameters values (that is, after training, multiple datasets will have the exact same\nmodel parameter values). To obtain a bound on the error, our proof shows that some datasets that\nare mapped to the same parameter values will be too different from each other (i.e., queries on the\ndatasets have answers that are too different), so that the exact same parameter setting cannot lead to\nanswering queries accurately for both datasets. The proofs do this by constructing 2σ+ 1different\ndatasets such that query answers differ by more than 2 ϵon all the 2σ+1datasets. Thus, two of those\ndatasets must be mapped to the same model parameter values, and the model must have an error more\nthanϵon at least one of them, which completes the proof. The majority of the proofs are on how\nthis set of 2σ+ 1datasets is constructed. Specifically, the proofs construct a set of datasets, where\neach pair differs in some set of points, S. The main technical challenge is constructing/showing\nthat for any of the two datasets that differ in a set of points S, the maximum or average difference\nbetween the query answers is at least 2 ϵ. This last statement is query dependent, and our technical\nLemmas 3-8 in the appendix are proven to quantify the difference between query answers between\nthe datasets based on the properties of the set Sand the query type. This is especially difficult in the\naverage-case scenario, as one needs to study how the set Saffects all possible queries.\nB.2 B ACKGROUND\nOur proofs are based on the notions of metric and packing entropy which we briefly describe here.\nConsider a metric space, M= (F,∥.∥). LetM(F,∥.∥, ϵ)be the packing number of Mand let\nN(F,∥.∥, ϵ)be the covering number of M. The packing number is the maximum number of non-\noverlapping balls of radius ϵthat can be placed in M, and the covering number is the minimum\nnumber of balls of radius ϵthat can cover M(see Vershynin (2018) for rigorous definitions). We\nhave that for M=M(F,∥.∥, ϵ), that there exists f1, ...,fM∈ F s.t.∥fi,−fj∥> ϵfor all i̸=j,\nand for N=N(F,∥.∥, ϵ), that there exists f1, ...,fN∈ F s.t.∀f∈ F,∥f−fi∥ ≤ϵfor some\ni∈[N].log2M(F,∥.∥, ϵ)is called the packing entropy of Mandlog2N(F,∥.∥, ϵ)is called the\nmetric entropy of M.\nOur proofs are based on the following theorem, utilizing the metric and packing entropy of a metric\nspace. Let ρσ(f) :F → { 0,1}σbe a function that takes elements of the metric space as input and\noutputs a bit vector of size σ, and let ˆfσ:{0,1}σ→ F′, be a function that takes bit vector of size σ\nas input and outputs elements of space F′where ∥.∥is well-defined on F′∪ F.\nTheorem 6. Consider a metric space, M= (F,∥.∥)\n(i) For any ρσandˆfσsuch that ∥ˆfσ(ρσ(f))−f∥ ≤ ϵfor all f∈ F , we must have σ≥\nlog2M(F,∥.∥,2ϵ).\n(ii) There exists ρσand ˆfσsuch that ∥ˆfσ(ρσ(f))−f∥ ≤ ϵfor all f∈ F withσ≤\n⌈log2N(F,∥.∥, ϵ)⌉.\n13\n\nPublished as a conference paper at ICLR 2024\nProof of Part (i). LetM=M(F,∥.∥,2ϵ). By definition, there exists f1, ...,fM∈ F , such that\n∥fi−fj∥>2ϵfor all i̸=j. Now assume, for the purpose of contradiction that there exists ρσand\nˆfσwith2σ< M s.t.,∥ˆfσ(ρσ(f))−f∥ ≤ϵfor all f∈ F. Note that a bit vector of size σcan take\nat most 2σdifferent values. Since 2σ< M , then ρ(f)must create an identical output for at least\ntwo of f1, ...,fM. That is, there exist fiandfj,i, j∈[M],i̸=js.t.ρ(fi) =ρ(fj). Therefore,\nˆf(ρ(fi)) = ˆf(ρ(fj)). By assumption the error of approximation is less than ϵfor both fiandfj,\ni.e.,∥ˆf(ρ(fi))−fi∥ ≤ϵand∥ˆf(ρ(fj))−fj∥ ≤ϵ. Then\n∥fj−fi∥ ≤ ∥ fi−ˆf(ρ(fi))∥+∥ˆf(ρ(fi))−fj∥=∥fi−ˆf(ρ(fi))∥+∥ˆf(ρ(fj)−fj∥ ≤2ϵ,\nShowing ∥fj−fi∥ ≤2ϵwhich is a contradiction. Therefore, we must have 2σ≥Mwhich implies\nσ≥log2M(F,∥.∥,2ϵ)as desired.\nProof of Part (ii). LetN=N(F,∥.∥, ϵ). There exists f1, ...,fN∈ F s.t.∀f∈ F,∥f−fi∥ ≤ϵ\nfor some i∈[N]. Then, construct ρσas follows. For any f∈ F, find is.t.,∥f−fi∥ ≤ϵ. Let ρσ\nbe the binary representation of i. Since i∈[N],⌈log2N⌉bits are needed to represent i, so that σ=\n⌈log2N⌉. Then, for a binary representation b, define ˆfσ(b)as a function that finds the integer iwith\nrepresentation band returns fi. Thus, for any f∈ F, we have that ∥ˆfσ(ρσ(f))−f∥=∥fi−f∥ ≤ϵ,\nwhich completes the proof.\nB.3 R ESULTS WITH ∞-NORM\nB.3.1 P ROOF OF THEOREM 1\nFor the purpose of this section, define ¯ϵ=⌊ϵ⌋+ 1for the proof of all three parts.\nProof of Part (i). LetF={rD,D∈ Dn\nu}, be the set of all possible rank functions for datasets of\nsizen, and consider the metric space M= (F,∥.∥∞). We prove a lower bound on M(F,∥.∥∞, ϵ)\nwhich in turn proves the desired results using Theorem 6.\nLetP1,P2⊆ D⌊n\n¯ϵ⌋\nu,P1̸=P2, that is, P1andP2are datasets of size ⌊n\n¯ϵ⌋only containing points in\nDu. LetD1be the dataset of size n, where each point in P1is repeated ¯ϵtimes (let the remainder of\nn−⌊n\n¯ϵ⌋×¯ϵelements be equal to one), and similarly define D2. We have that ∥rD1−rD2∥∞≥¯ϵ > ϵ .\nLetSbe the set of all possible datasets generated using the above procedure. We have that Sis an\nϵ-Packing of M, and thus, |S| ≤M(F,∥.∥∞, ϵ).\nIt remains to find |S|. Each element of Sis created by selecting ⌊n\n¯ϵ⌋elements from u+ 1elements\ninDuwith repetition. The unique number of ways to perform this selection is C(⌊n\n¯ϵ⌋+u,⌊n\n¯ϵ⌋)≥\n(n\n¯ϵ+u\nn\n¯ϵ)n\n¯ϵ≥(n\nϵ+1+u\nn\nϵ+1)n\nϵ+1. Thus,M(F,∥.∥∞, ϵ)≥(1 +(ϵ+1)u\nn)n\nϵ+1\nCombining this with Theorem 6, we have that any model answering queries to ∞-error ϵmust have\nsize at leastn\n2ϵ+1log2(1 +(2ϵ+1)u\nn).\nProof of Part (ii) . LetF={cD,D∈ Dn\nu}, be the set of all possible cardinality functions for\ndatasets of size n, and consider the metric space M= (F,∥.∥∞). We prove a lower bound on\nM(F,∥.∥∞, ϵ)which in turn proves the desired results using Theorem 6.\nLetP1,P2⊆ D⌊n\n¯ϵ⌋×d\nu ,P1̸=P2, that is, P1andP2ared-dimensional datasets of size ⌊n\n¯ϵ⌋only\ncontaining values in Du. LetD1be the dataset of size n, where each point in P1is repeated ¯ϵtimes\n(let the remainder of n− ⌊n\n¯ϵ⌋ ×¯ϵelements be equal to one), and similarly define D2.\nFirst, we show that ∥cD1−cD2∥∞≥¯ϵ. Letpbe a point that appears more times in D1thanD2,\nand consider a query q= (c,r)withc=p−1\n2uandr=1\nu. The only point in P1∪P2that\nmatches qisp. Since pappears more times in D1thanD2, and each appearance of the point is\nrepeated ¯ϵtimes by definition we have |cD1(q)−cD2(q)| ≥¯ϵ.\nThus, we have ∥cD1−cD2∥∞≥¯ϵ > ϵ . LetSbe the set of all possible datasets generated using the\nabove procedure. We have that Sis anϵ-Packing of M, and thus, |S| ≤M(F,∥.∥∞, ϵ).\n14\n\nPublished as a conference paper at ICLR 2024\nIt remains to find |S|. Each element of Sis created by selecting ⌊n\n¯ϵ⌋elements from (u+ 1)d\nelements in Duwith repetition. The unique number of ways to perform this selection is at least\nC(⌊n\n¯ϵ⌋+ud,⌊n\n¯ϵ⌋)≥(n\n¯ϵ+ud\nn\n¯ϵ)n\n¯ϵ≥(n\nϵ+1+ud\nn\nϵ+1)n\nϵ+1. Thus,M(F,∥.∥∞, ϵ)≥(1 +(ϵ+1)ud\nn)n\nϵ+1.\nCombining this with Theorem 6, we have that any model answering queries to ∞-error ϵmust have\nsize at leastn\n2ϵ+1log2(1 +(2ϵ+1)ud\nn).\nProof of Part (iii). For the purpose of contradiction, assume there exists a training/inference func-\ntion pair (ρ,ˆf)with size less thann\n2ϵ+1log2(1 +(2ϵ+1)ud\nn)that for all datasets D∈ Dn×(d+1)\nu we\nhave∥ˆf(.;ρ(D))−sD∥∞≤ϵ. We use (ρ,ˆf)to construct a training/inference function pair (ρ′,ˆf)\nthat answers cardinality estimation queries for any dataset D∈ Dn×d\nu with error at most ϵ. Specif-\nically, define ρ′(D)as a function that takes D∈ Dn×d\nu as an input, constructs D′∈ Dn×(d+1)\nu\nasD′[i, j] =D[i, j]forj∈[d], i∈[n], andD′[i, d+ 1] = 1 , and returns ρ(D′). Here, D′is\na dataset with its first ddimensions identical to Dand but with it’s d+ 1-th dimension set to 1\nfor all data points. Then, we have that ∥ˆf(.;ρ′(D))−cD∥∞≤ϵfor all D∈ Dn×d\nu, because by\nconstruction cD=sD′,ˆf(.;ρ′(D)) = ˆf(.;ρ(D′))and that ∥ˆf(.;ρ(D′))−sD′∥∞≤ϵby assump-\ntion. However, ∥ˆf(.;ρ′(D))−cD∥∞≤ϵcontradicts Part (ii), and thus we have proven that no\ntraining/inference function pair (ρ,ˆf)with size less thann\n2ϵ+1log2(1 +(2ϵ+1)ud\nn)exists that for all\ndatasets D∈ Dn×(d+1)\nu we have ∥ˆf(.;ρ(D))−sD∥∞≤ϵ.\nB.3.2 P ROOF OF LEMMA 1\nFor any ρσ,ˆfσwith finite σ, we construct a dataset, D, such that ∥rD−ˆfσ(ρσ(D))∥∞>n\n2\nLetDpbe the dataset of size nwith point prepeated ntimes. For any k, consider ∆k={Di\nk,0≤\ni≤k}. Note that for any D,D′∈∆k,∥rD−rD′∥∞=n. Now for the purpose of contradiction,\nassume, σbits are sufficient for answering queries with error ϵacross all datasets of size n. Now\nconsider any ρσ,ˆfσ. Let k= 2σ, so that |∆k|= 2σ+ 1. Thus, there exists D,D′∈∆ks.t.\nρσ(D) =ρσ(D′), so that ˆfσ(ρσ(D)) = ˆfσ(ρσ(D′)). Now assume the error on either DorD′is\nless thann\n2, or otherwise the proof is complete. Therefore, w.l.o.g., we have ∥ˆfσ(ρσ(D))−rD∥∞<\nn\n2. We have that\nn=∥rD−rD′∥∞=∥(rD−ˆfσ(ρσ(D)))−(rD′−ˆfσ(ρσ(D′)))∥∞<n\n2+∥rD′−ˆfσ(ρσ(D′))∥∞\nSo that ∥rD′−ˆfσ(ρσ(D′))∥∞>n\n2. Thus, for any ρσ,ˆfσwith finite σ, there exists a dataset, D\nsuch that ∥rD−ˆfσ(ρσ(D))∥∞>n\n2.\nB.4 R ESULTS WITH 1-NORM\nB.4.1 P ROOF OF THEOREM 2\nWe first present the following lemma, whose proof can be found in Appendix C.\nLemma 3. LetD,D′∈[0,1]nbe datasets in sorted order. Then, ∥rD−rD′∥1=P\ni∈[n]|D[i]−\nD′[i]|. Note thatP\ni∈[n]|D[i]−D′[i]|=∥D−D′∥1, so that ∥rD−rD′∥1=∥D−D′∥1.\nThe lemma shows that 1-norm between rank functions has a closed-form solution that can be calcu-\nlated based on the difference between the points in the dataset. We use this lemma throughout for\nanalyzing the 1-norm error.\nLetF={rD,D∈[0,1]n}, be the set of all possible rank functions for datasets of size n, and\nconsider the metric space M= (F,∥.∥1).\nProof of Theorem 2 (i) . We prove a lower bound on M(F,∥.∥1, ϵ)which in turn proves the desired\nresults using Theorem 6.\nLetP={i\n⌈k\nϵ⌉−1,0≤i≤ ⌈k\nϵ⌉ −1}for an integer kspecified later. Let P,P′∈P⌊n\nk⌋,P̸=P′,\nthat is, PandP′are datasets of size ⌊n\nk⌋only containing points in P. LetDbe the dataset of size\n15\n\nPublished as a conference paper at ICLR 2024\nn, where each point in Pis repeated ktimes (let the remainder of n−⌊n\nk⌋×kelements be equal to\none), and similarly define D′usingP′, and consider DandD′in a sorted order.\nWe show that ∥rD−rD′∥1> ϵ. Observe that DandD′differ in at least kpoints, so that D[i]̸=\nD′[i]forkdifferent values of i. Using this observation and Lemma 3, we have that\n∥rD−rD′∥1=∥D−D′∥1≥1\n⌈k\nϵ⌉ −1×k >k\nk\nϵ=ϵ.\nThus, for any two different datasets, D,D′generated by the above procedure, we have ∥rD−rD′∥>\nϵ. Let Sbe the set of all datasets generated that way. We have that Sis an ϵ-Packing of M, and\nthus,|S| ≤M(F,∥.∥1, ϵ).\nIt remains to find |S|. Each element of Sis created by selecting ⌊n\nk⌋elements from ⌈k\nϵ⌉elements in\nPwith repetition. The unique number of ways to perform this selection is C(⌊n\nk⌋+⌈k\nϵ⌉ −1,⌊n\nk⌋).\nLetk=⌈√n⌉, so that we have\nC(⌊n\n⌈√n⌉⌋+⌈⌈√n⌉\nϵ⌉ −1,⌊n\n⌈√n⌉⌋)≥(⌊n\n⌈√n⌉⌋+⌈⌈√n⌉\nϵ⌉ −1\n⌊n\n⌈√n⌉⌋)⌊n\n⌈√n⌉⌋\n≥(√n+√n\nϵ−1√n)⌊n\n⌈√n⌉⌋= (1 +1\nϵ−1√n)⌊n\n⌈√n⌉⌋\n≥(1 +1\nϵ−1√n)√n−2.\nThus,M(F,∥.∥1, ϵ)≥(1 +1\nϵ−1√n)√n−2. Combining this with Theorem 6, we have that any\nmodel answering queries to 1-error ϵmust have size at least (√n−2) log2(1 +1\n2ϵ−1√n).\nProof of Theorem 2 (ii) . We prove an upper bound on the metric entropy N(F,∥.∥1, ϵ).\nLet¯D={i\n⌈n\nϵ⌉,0≤i≤ ⌈n\nϵ⌉}, and define ¯F={rD,D∈¯Dn}. We show that ¯Fis anϵ-cover of F,\nso that its size provides an upper bound for the covering number of F.\nSpecifically, for any D∈[0,1]n, we show that ∃r¯D∈¯F, s.t.∥rD−r¯D∥1≤ϵ. Consider ¯Dsuch\nthat¯D[i] =⌊⌈n\nϵ⌉×D[i]⌋\n⌈n\nϵ⌉for all i. Such a ¯Dexists in ¯Dnas all its points belong to ¯D. This is because\n⌊⌈n\nϵ⌉ ×D[i]⌋is an integer between 0 and ⌈n\nϵ⌉, forD[i]∈[0,1]. Furthermore, using Lemma 3,\n∥rD−r¯D∥1=X\ni∈[n]|D[i]−¯D[i]|\n=X\ni∈[n]|D[i]−⌊⌈n\nϵ⌉ ×D[i]⌋\n⌈n\nϵ⌉|\n<X\ni∈[n]1\n⌈n\nϵ⌉\n=n\n⌈n\nϵ⌉≤ϵ.\nTherefore, ¯Fis an ϵ-covering of F. It remains to calculate the size of ¯F, which is the number of\npossible ways to select nelements from ⌈n\nϵ⌉+ 1elements in ¯Dwith repetition. That is,\n|¯F|=C(n+⌈n\nϵ⌉, n)≤(en+⌈n\nϵ⌉\nn)n≤(en+n\nϵ+ 1\nn)n= (e+e\nϵ+e\nn)n,\nSo thatN(F,∥.∥1, ϵ)≤(e+e\nϵ+e\nn)n. Combining this with Theorem 6, we obtain that nlog2(e+\ne\nϵ+e\nn)is an upper bound on the required model size.\n16\n\nPublished as a conference paper at ICLR 2024\nB.4.2 P ROOF OF THEOREM 3\nWe first present two lemmas, whose proof can be found in Appendix C. These lemmas can be\nseen as substitute for Lemma 3 for the case of cardinality estimation, since we do not have such\na closed-form general statement for 1-norm difference between cardinality functions (as we did\nfor indexing in Lemma 3). However, the following lemmas provide scenarios where the 1-norm\ndifference between the cardinality functions are bounded, which are utilized in our Theorem’s proof.\nLemma 4. Consider two 1-dimensional databases DandD′of size nsuch that |D[i]−D′[i]| ≤ϵ\nnfori∈[n]. Then ∥cD−cD′∥1≤2ϵ.\nLemma 5. Consider a 1-dimensional database Dof size n. Assume that we are given two mask\nvectors, m1,m2∈ {0,1}nthat each create two new dataset D1andD2, s.t.,Diconsists of\nrecords in Dwithmi= 1fori∈ {1,2}. We have that ∥cD1−cD2∥1≤1\n2P\ni∈[n]|m1[i]−m2[i]|\nFor the remainder of this section, let F={cD,D∈[0,1]n×d}, be the set of all possible cardinality\nfunctions for d-dimensional datasets of size n, and consider the metric space M= (F,∥.∥1). Our\nproof in this case reduce the problem to a 1-dimensional setting and then utilize the lemmas stated\nabove to analyze the cardinality functions.\nProof of Theorem 3 Part (i) . We prove a lower bound on M(F,∥.∥1, ϵ)which in turn proves the\ndesired results using Theorem 6.\nLetu\n2=⌈k\n2ϵ⌉ −1and let P={(i1\nu, ...,id\nu),0≤i1, ..., i d≤u\n2}for an integer kspecified later. Let\nP,P′∈P⌊n\nk⌋,P̸=P′, that is, PandP′are datasets of size ⌊n\nk⌋only containing points in P. Let\nDbe the dataset of size n, where each point in Pis repeated ktimes, and similarly define D′using\nP′. W.l.o.g, assume DandD′differ on their i-th point and its d-th dimension. We have\n∥cD−cD′∥1=Z\nc1...Z\ncd−1Z\nr1...Z\nrd−1∥cD(c1, ..., c d−1, ., r1, ..., r d−1, .)−cD′(c1, ..., c d−1, ., r1, ..., r d−1, .)∥1\n≥Z\nc1...Z\ncd−1Z\nr1...Z\nrd−1Id−1\nq,D[i]∥cD(c1, ..., c d−1, ., r1, ..., r d−1, .)−cD′(c1, ..., c d−1, ., r1, ..., r d−1, .)∥1\nWhere Ii\nq,pis an indicator function equal to 1 if a record pmatches the first idimensions in q=\n(c1, ..., c d, r1, ..., r d), that is if cj≤pj≤cj+rjfor all j∈[i], and zero otherwise.\nNext, we show that for any q, we have\nId−1\nq,D[i]∥cD(c1, ..., c d−1, ., r1, ..., r d−1, .)−cD′(c1, ..., c d−1, ., r1, ..., r d−1, .)∥1≥ Id−1\nq,D[i]ϵ\n2.\nSpecifically, if D[i]does not match the first d−1dimensions of q, then both sides are zero.\nOtherwise, let ¯D={D[i, d]∀i,Id−1\nq,D[i]= 1}be a 1-dimensional dataset of the d-th at-\ntribute of the records of Dthat matches the first d−1dimensions of q, and similarly define\n¯D′={D′[i, d]∀i,Id−1\nq,D′[i]= 1}forD′. By, definition, we have that\n∥cD(c1, ..., c d−1, ., r1, ..., r d−1, .)−cD′(c1, ..., c d−1, ., r1, ..., r d−1, .)∥1=∥c¯D−c¯D′∥1.\nWe state the following lemma whose proof is deferred to Appendix C to bound the 1-norm difference\nbetween ¯Dand¯D′.\nLemma 6. Let¯Dand¯D′be as defined above. For ϵ < k , we have that ∥c¯D−c¯D′∥1>ϵ\n2.\nUsing Lemma 6 and the preceding discussion, we have that\n∥cD−cD′∥1>Z\nc1...Z\ncd−1Z\nr1...Z\nrd−1Id−1\nq,D[i]ϵ\n2\n=ϵ\n2Z\nc1...Z\ncd−1Z\nr1...Z\nrd−1Id−1\nq,D[i].\n17\n\nPublished as a conference paper at ICLR 2024\nWe have that Id−1\nq,D[i]= 1 for queries where, for all dimension, j,rj∈[0,1]andcj∈\n[D[i, j],min{D[i, j],1−rj}]. Calculating the total volume of such queries, we have\nZ1\n0Zmin{D[i,j],1−rj}\nD[i,j]−rj1 dcjdrj=Z1\n0(min{D[i, j],1−rj} −D[i, j] +rj) drj\n=Z1−D[i,j]\n0rjdrj+Z1\n1−D[i,j](1−D[i, j]) drj\n=1\n2(1−D[i, j])2+ (1−D[i, j])2\n=1\n2−D[i, j]2\n2\n≥1\n4,\nWhere the last inequality follows because all points in Phave all of their coordinates less than1\n2.\nRepeating the above for all the dimensions, we obtain that\n∥cD−cD′∥1> ϵ0.25d.\nThus, for any two different datasets, D,D′generated by the above procedure, we have ∥cD−cD′∥>\nϵ0.25d. LetSbe the set of all datasets generated that way.\nIt remains to find |S|. Each element of Sis created by selecting ⌊n\nk⌋from the (u\n2+ 1)dpossible\nelements in Pwith repetition. Set k=⌈√n⌉+ 1, which is\nC((u\n2+ 1)d+⌈√n⌉,⌈√n⌉+ 1)≥((u\n2+ 1)d+⌊n\n⌈√n⌉⌋ −1\n⌊n\n⌈√n⌉⌋)⌊n\n⌈√n⌉⌋\n≥((√n\n2ϵ)d+√n−1√n)√n−2\n= (√nd−1\n(2ϵ)d+ 1−1√n)√n−2.\nThus, we have that Scontains elements each of which are at least ϵ0.25dapart for ϵ <√n.\nDefine ϵ′=ϵ0.25dso thatϵ′\n0.25d=ϵ, and repeat tha above procedure for ϵ′, we have that there exists\na set of (√nd−1\n4d(d+1)ϵd+ 1−1√n)√n−2elements where they all differ by ϵ′for4dϵ′≤√n. Thus, we\nhave an ϵ-Packing of M, and thus, (√nd−1\n4d(d+1)ϵd+ 1−1√n)√n−2≤M(F,∥.∥1, ϵ). Using this together\nwith Theorem 6 proves the results.\nProof of Theorem 3 Part (ii) . We prove an upper bound on N(F,∥.∥1, ϵ)which in turn proves the\ndesired results using Theorem 6.\nLet¯D={(i1\n⌈n\nϵ⌉, ...,id\n⌈n\nϵ⌉),0≤i1, ..., i d≤ ⌈n\nϵ⌉}, and define ¯F={cD,D∈¯Dn}. We show that ¯F\nis anϵ-cover of F, so that its size provides an upper bound for the covering number of F.\nFor any D∈[0,1]n×d, we show that ∃c¯D∈¯F, s.t.∥cD−c¯D∥1≤ϵ. Specifically, consider ¯Dsuch\nthat¯D[i, j] =⌊⌈n\nϵ⌉×D[i,j]⌋\n⌈n\nϵ⌉for all i. Such a ¯Dexists in ¯Dnas all its points belong to ¯D. Our goal\nis to show that ∥cD−c¯D∥1≤ϵ.\nTo do so, consider the dataset ˆDthatˆD[i, j] =¯D[i, j]forj∈[d−1]andˆD[i, d] =D[i, d]. That\nisˆDis a dataset with its first d−1dimensions the same as ¯Dand its d-th dimension the same as\nD. We have that\n∥cD−c¯D∥1≤ ∥cD−cˆD∥1+∥cˆD−c¯D∥1\nWe study the first two terms separately. Consider the second term ∥cˆD−c¯D∥1. Observe that the\nfirstd−1dimensions of ¯DandˆDare identical, so that applying any predicate on the first d−1\n18\n\nPublished as a conference paper at ICLR 2024\nattributes of the datasets leads to the same filtered data points. Furthermore, the d-th dimension of\nthe points differ by at mostϵ\nn, so by Lemma 4, we have\n∥cˆD−c¯D∥1≤2ϵ.\nThe following Lemma, proven in Appendix C, studies ∥cD−cˆD∥1.\nLemma 7. LetDandˆDbe defined as above. We have ∥cD−cˆD∥1≤(d−1)ϵ.\nThus, putting everything together, we have that\n∥cD−c¯D∥1≤2ϵ+ (d−1)ϵ= (d+ 1)ϵ.\nWe have shown that for any D, there exists c¯D∈ F such that ∥cD−c¯D∥ ≤(d+ 1)ϵ. Next, we\ncalculate the size of F. it is equal to the number of ways nelements can be selected from a set of\nsize(⌈n\nϵ⌉+ 1)d. This is equal to\nC((⌈n\nϵ⌉+ 1)d+n−1, n)≤(e(⌈n\nϵ⌉+ 1)d+n−1\nn)n\nDefine ϵ′= (d+1)ϵ, we have that there exists an ϵ′-cover of Fwith(e(2n(d+1)\nϵ′)d+n−1\nn)nelements for\nϵ′≤2\n3n(d+ 1) (we have used the fact that 2x≥ ⌈x⌉+ 1forx≥3\n2), which proves N(F,∥.∥1, ϵ)≤\n(e(2n(d+1)\nϵ′)d+n−1\nn)n. Note that d≥1implies that ϵ′≤2\n3n(d+ 1) is satisfied for all ϵ′≤n.\nCombining this with Theorem 6, we obtain that nlog2(e(2(d+1)\nϵ)dnd−1+e−e\nn)is an upper bound\non the required model size.\nB.4.3 P ROOFS FOR RANGE -SUMESTIMATION\nSimilar to previous sections, we first present the following lemma that is used for bounding differ-\nence between range-sum functions, proved in Appendix C. Lemmas 9 and 10 are a direct general-\nization of Lemmas 4 and 5 to answering range count queries, with almost identical proofs. However,\nLemma 8 is specific to range-sum queries, allowing us to analyze the attribute that is being aggre-\ngated by the query.\nLemma 8. Assume DandD′are two (d+1)-dimensional datasets with identical first ddimensions,\nbut with |D[i, d+ 1]−D′[i, d+ 1]| ≤ϵ\nnfori∈[n]. Then, ∥sD−sD′∥ ≤ϵ.\nLemma 9. Consider two 2-dimensional databases DandD′of size nsuch that |D[i,1]−\nD′[i,1]| ≤ϵ\nnandD[i,2] =D′[i,2]fori∈[n]. Then ∥sD−sD′∥1≤2ϵ.\nLemma 10. Consider a 2-dimensional database Dof size n. Assume that we are given two mask\nvectors, m1,m2∈ {0,1}nthat each create two new dataset D1andD2, s.t.,Diconsists of\nrecords in Dwithmi= 1fori∈ {1,2}. We have that ∥sD1−sD2∥1≤1\n2P\ni∈[n]|m1[i]−m2[i]|.\nIn this section, let F={sD,D∈[0,1]n×d}, be the set of all possible range-sum functions for\nd+ 1-dimensional datasets of size n, and consider the metric space M= (F,∥.∥1).\nProof of Corollary 1 . For the purpose of contradiction, assume there exists a training/inference\nfunction pair (ρ,ˆf)with size less than (√n−2) log2(1 +√nd−1\n4d(d+1)ϵd−1√n)that for all datasets\nD∈[0,1]n×(d+1)we have ∥ˆf(.;ρ(D))−sD∥1≤ϵ. We use (ρ,ˆf)to construct a training/inference\nfunction pair (ρ′,ˆf)that answers cardinality estimation queries for any dataset D∈[0,1]n×dwith\nerror at most ϵ. Specifically, define ρ′(D)as a function that takes D∈[0,1]n×das an input,\nconstructs D′∈[0,1]n×(d+1)asD′[i, j] =D[i, j]forj∈[d], i∈[n], andD′[i, d+ 1] = 1 ,\nand returns ρ(D′). Here, D′is a dataset with its first ddimensions identical to Dand but with\nit’sd+ 1-th dimension set to 1 for all data points. Then, we have that ∥ˆf(.;ρ′(D))−cD∥1≤ϵ\nfor all D∈[0,1]n×d, because by construction cD=sD′and that ˆf(.;ρ′(D)) = ˆf(.;ρ(D′))and\nthat∥ˆf(.;ρ(D′))−sD′∥1≤ϵby assumption. However, ∥ˆf(.;ρ′(D))−cD∥1≤ϵcontradicts\nTheorem 3, and thus we have proven that no training/inference function pair (ρ,ˆf)with size less\nthan(√n−2) log2(1 +√nd−1\n4d(d+1)ϵd−1√n)exists that for all datasets D∈[0,1]n×(d+1)we have\n∥ˆf(.;ρ(D))−sD∥1≤ϵ. Thus, Σ1\ns≥(√n−2) log2(1 +√nd−1\n4d(d+1)ϵd−1√n).\n19\n\nPublished as a conference paper at ICLR 2024\nProof of Theorem 4 . Let ¯D={(i1\n⌈n\nϵ⌉, ...,id+1\n⌈n\nϵ⌉),0≤i1, ..., i d+1≤ ⌈n\nϵ⌉}, and define ¯F={sD,D∈\n¯Dn}. We show that ¯Fis anϵ-cover of F, so that its size provides an upper bound for the covering\nnumber of F. For any D∈[0,1]n×d, we show that ∃s¯D∈¯F, s.t.∥sD−s¯D∥1≤ϵ. Specifically,\nconsider ¯Dsuch that ¯D[i, j] =⌊⌈n\nϵ⌉×D[i,j]⌋\n⌈n\nϵ⌉for all i. Such a ¯Dexists in ¯Dnas all its points belong\nto¯D. Furthermore, define D′asD′[i, j] =Di,jfori∈[n],j∈[d]butD′[i, d+ 1] = ¯D[i, d+ 1].\nWe have that ∥sD−s¯D∥1≤ ∥sD−sD′∥1+∥sD′−s¯D∥1≤ ∥sD′−s¯D∥1+ϵby Lemma 8\nThe remainder of the proof follows closely the proof of Theorem 3 Part (ii) with a slight generaliza-\ntion. We state this as a lemma and differ the proof to Appendix C.\nLemma 11. Let¯DandD′be as defined as above. We have that ∥s¯D−sD′∥ ≤(d+ 1)ϵ\nThis, together with the above discussion implies that ∥sD−s¯D∥ ≤(d+ 2)ϵ. Thus, we have shown\nthat for any D, there exists s¯D∈ F such that ∥sD−s¯D∥ ≤(d+ 1)ϵ. Next, we calculate the size\nofF. it is equal to the number of ways nelements can be selected from a set of size (⌈n\nϵ⌉+ 1)d+1.\nThis is equal to\nC((⌈n\nϵ⌉+ 1)d+1+n−1, n)≤(e(⌈n\nϵ⌉+ 1)d+1+n−1\nn)n\nDefine ϵ′= (d+ 2)ϵ, we have that there exists an ϵ′-cover of Fwith(e(2n(d+2)\nϵ′)d+1+n−1\nn)nele-\nments for ϵ′≤2\n3n(d+ 2) (we have used the fact that 2x≥ ⌈x⌉+ 1 forx≥3\n2), which proves\nN(F,∥.∥1, ϵ)≤(e(2n(d+2)\nϵ′)d+1+n−1\nn)n. Note that d≥1implies that ϵ′≤2\n3n(d+ 2) is satisfied for\nallϵ′≤n. Combining this with Theorem 6, we obtain that nlog2(e(2(d+2)\nϵ)d+1nd+e−e\nn)is an\nupper bound on the required model size.\nB.5 R ESULTS WITH µ-NORM\nB.5.1 P ROOF OF THEOREM 5\nTheorem 5 can be seen as a direct generalization of Theorem 2. We first present a generalization of\nLemma 3 to the case of µ-norm.\nLemma 12. LetDandD′be1-dimensional datasets in sorted order. Then, ∥rD−rD′∥µ=\n∥D−D′∥µ, where ∥D1−D2∥µ=P\ni∈[n]µ([D1[i], D2[i]])andµ([D1[i], D2[i]])is the probability\nof observing a query in the range [D1[i], D2[i]].\nUsing this lemma, the remainder of the proof is a straightforward adaptation of arguments in the\nproof of Theorem 2. Let F={rD,D∈[0,1]n}, be the set of all possible rank functions for\ndatasets of size n, and consider the metric space M= (F,∥.∥µ).\nProof of Theorem 5 Part (i) . Let p0= 0 and define piinductively s.t. µ([pi−1, pi]) =1\n⌈k\nϵ⌉−1\nfori >0. Since µis a continuous distribution over [0,1], a total of ⌈k\nϵ⌉distinct such points in\n[0,1]exist. Let P={p0, ..., p 1}be the set of all such points, for an integer kspecified later. Let\nP,P′∈P⌊n\nk⌋,P̸=P′, that is, PandP′are datasets of size ⌊n\nk⌋only containing points in P.\nLetDbe the dataset of size n, where each point in Pis repeated ktimes, and similarly define P′.\nNote that DandD′differ in at least kpoints, so that D[i]̸=D′[i]forkdifferent i’s. This means\n∥rD−rD′∥µ=∥D−D′∥µ≥1\n⌈k\nϵ⌉−1>ϵ\nkk=ϵ. Therefore, as long as we generate datasets the\nway described above, for every two datasets we have ∥rD−rD′∥> ϵ. Similar to the case of 1-norm\nerror, the total number of such distinct datasets is greater than (1 +1\nϵ−1√n)√n−2which bounds the\npacking entropy and together with Theorem 6 proves the results (see proof of Theorem 2 for more\ndetails).\nProof of Theorem 5 Part (ii) . Let p0= 0 and define pis.t.µ([pi−1, pi]) =1\n⌈n\nϵ⌉and let Pbe the\nset of⌈n\nϵ⌉+ 1such points. For any D∈[0,1]n, we show that ∃¯D∈Pn, s.t.∥D−¯D∥µ≤ϵ.\nSpecifically, consider ¯Dsuch that ¯D[i] = arg min p∈P|p−D[i]|for all i. Note that for every i,\nwe have that µ([¯D[i],D[i]])≤ϵ\nn. Therefore, ∥rD−r¯D∥ ≤P\ni∈[n]ϵ\nn=ϵ. Similar to the case of\n1-norm error, we can calculate the total number of possible datasets in Pnto be at most (e+e\nϵ+e\nn)n,\nwhich combined with Theorem 6 proves the result.\n20\n\nPublished as a conference paper at ICLR 2024\nB.5.2 P ROOF OF LEMMA 2\nForf∈ {s, c}, we construct a query distribution such that for any error ϵ >0, we have ∥fD−fD′∥ ≤\nϵfor any DandD′. Specifically, consider the set Q={(i\nkn,1\nkn),0≤i≤kn−1}for an integer\nkdefined later.\nThen, define the p.d.f. as g(c, r) = 2 nkif∀x∈Q, x /∈[c, c+r]and 0 otherwise. Note that this is\na valid p.d.f that integrates to 1, as shown below.\nX\n0≤i≤kn−1Z1\nnk\nc=0Zr=1\nnk−c\nr=02nkdrdc= 2nkX\n0≤i≤kn−1Z1\nnk\nc=0(1\nnk−c) dc\n= 2nkX\n0≤i≤kn−1[1\nnkc−c2\n2]1\nnk\n0\n= 2nkX\n0≤i≤kn−11\nnk2\n−(1\nnk)2\n2\n=2nk\n2X\n0≤i≤kn−1(1\nnk)2= 1\nNow for any two dataset DandD′, we have\n∥fD−fD′∥= 2knX\niZi+1\nnk\nc=i\nnkZr=i+1\nnk−c\nr=0|fD(c, r)−fD′(c, r)|\n≤2knX\niZi+1\nnk\nc=i\nnkZr=i+1\nnk−c\nr=02fD(c, r)\n≤4knX\niZi+1\nnk\nc=i\nnkZr=i+1\nnk−c\nr=0fD(i\nnk,1\nnk)\n= 4knX\nifD(i\nnk,1\nnk)1\n(nk)2=4\n(nk)X\nifD(i\nnk,1\nnk)\n≤4\nk.\nThe first inequality follows by assuming, w.l.o.g that fD(q)> f D′(q). The second inequality\nfollows because both cardinality and range-sum functions are increasing functions in the length of\nthe query predicate and the last inequality follows sinceP\ni|fD1(i\nnk,1\nnk)| ≤nbecause all queries\ninQare disjoint, so that every point in Dwill contribute at most once to queries in Q. Now setting\nkso that4\nk< ϵcomplete the proof.\n21\n\nPublished as a conference paper at ICLR 2024\nC P ROOF OF TECHNICAL LEMMAS\nProof of Lemma 3 . The 1-norm error is the area between the two curves for DandD′. We break\ndown this area into rectangles whose area can be computed in closed form. The main intuition is to\ncalculate this area by considering vertically stacked rectangles on top of each other. Specifically, we\nhave∥rD−rD′∥1=R1\n0|rD(q)−rD′(q)|. Let Ii,qbe an indicator function denoting whether q∈\n[D[i],D′[i])(or[D′[i],D[i])ifD′[i]<D[i]). We have that |rD(q)−rD′(q)|=Pn\ni=0Ii,q. Thus,\nwe have ∥rD−rD′∥1=R1\n0Pn\ni=0Ii,q=Pn\ni=0R1\n0Ii,q=Pn\ni=0RD′[i]\nD[i]1 =P\ni|D[i]−D′[i]|.\nProof of Lemma 4 .\n∥cD−cD′∥1=Z\nrZ\nc|X\ni∈n(IDi∈[c,c+r]−ID′\ni∈[c,c+r])|\n≤Z\nrZ\ncX\ni∈n|(IDi∈[c,c+r]−ID′\ni∈[c,c+r])|\n=X\ni∈nZ\nrZ\nc|(IDi∈[c,c+r]−ID′\ni∈[c,c+r])|\n= 2X\ni∈nZ\nrmin{ϵ\nn, r}\n≤2X\ni∈nZ\nrϵ\nn\n= 2ϵ\nProof of Lemma 5 . We first state the following lemma, whose proof is deferred to the end.\nLemma 13. Consider two 1-d datasets DandD′over points p1, ..., p ks.t. the points in Dare each\nrepeated z1, ..., z ktimes for k∈[n]and0≤zi≤n, and the points D′are each repeated z′\n1, ..., z′\nk\ntimes for k∈[n]and0≤z′\ni≤n. Lett=P\ni∈[k]|zi−z′\ni|. We have that ∥cD−cD′∥1≤1\n2t.\nIn our setting, let p1, ...,pkbe the distinct element of D, letz1, ..., z kbe the number of times\neach element is repeated in D1andz′\n1, ..., z′\nkbe the number of times each element is repeated in\nD2. By Lemma 13, we have that ∥cD1−cD2∥1≤1\n2P\ni∈[k]|zi−z′\ni|. Now observe that zi=P\nj∈[n]m1[j]ID[j]=piand similarly z′\ni=P\nj∈[n]m2[j]ID[j]=pi. Thus, we have\n∥cD1−cD2∥1≤1\n2X\ni∈[k]|zi−z′\ni|\n=1\n2X\ni∈[k]|X\nj∈[n]ID[j]=pi(m1[j]−m2[j])|\n≤1\n2X\ni∈[k]X\nj∈[n]ID[j]=pi|m1[j]−m2[j]|\n=1\n2X\nj∈[n]|m1[j]−m2[j]|.\nProof of Lemma 6 . For simplicity of notation, we prove this lemma with ¯Drenamed as Dand\n¯D′renamed as D′. For any fixed r, we first bound ∥cD(., r)−cD′(., r)∥1. Let z=r− ⌊r\n1\nu⌋\n(recall that u= 2⌈k\n2ϵ⌉ −2). Let ibe the index of the first element in which DandD′differ, and\nletp= min {D[i],D′[i]}. Such an index exists as the datasets are different (also recall that any\ndifference is repeated ktimes, by construction). Therefore, the functions are identical in the range\n[−1, p−r). Now consider two cases, when r >1\nu, and when r≤1\nu.\n22\n\nPublished as a conference paper at ICLR 2024\nIn the first case consider the range [p−r, p−r+z]and[p−r+z, p−r+1\nu]. Over both ranges\nthe functions are constant, and the functions may only change at p−r+z. Since the functions\nare identical before p−rbut change at p−r, and one changes more than the other, the difference\nbetween the two functions in the range [p−r, p−r+z]is at least k, that is |cD(c, r)−cD′(c, r)| ≥k\nforq∈[p−r, p−r+z]. Now, observe that p−r+zis a multiple of1\nu, and that it is strictly\nless than p. Thus, all points in DandD′atp−r+zare identical. Therefore, both functions\nhave an identical change at p−r+zso that their difference remains the same. Thus, we have that\n|cD(q, r)−cD′(c, r)| ≥kforq∈[p−r, p−r+1\nu]. Thus, in this case ∥cD(., r)−cD′(., r)∥1≥\nk×1\nu>ϵ\nkforu= 2⌈k\n2ϵ⌉ −2.\nIn the second case, consider the range [p−r, p]. Both functions are identical right before at p−r,\nwhile one changes by at least kmore than the other at p−r, and the functions are constant over\n(p−r, p). Therefore, |cD(c, r)−cD′(c, r)| ≥kforq∈[p−r, p]. Thus, in this case ∥cD(., r)−\ncD′(., r)∥1≥k×r.\nNow we have\n∥cD−cD′∥1=Z\nrZ\nc|cD(c, r)−cD′(c, r)|\n=Z\nr<ϵ\nk∥cD(., r)−cD′(., r)∥1+Z\nr≥ϵ\nk∥cD(., r)−cD′(., r)∥1\n> k(Z\nr<ϵ\nkr+Z\nr≥ϵ\nkϵ\nk)\n=k((ϵ\nk)2\n2+ (ϵ\nk)(1−ϵ\nk))\n=k(ϵ\nk−(ϵ\nk)2\n2)\nNote that, for ϵ < k ,(ϵ\nk)2≤ϵ\nkso that\nϵ\nk−(ϵ\nk)2\n2≥ϵ\nk−ϵ\nk\n2=ϵ\nk\n2.\nAs such, we have ∥cD−cD′∥1≥ϵ\n2.\nProof of Lemma 7 . Recall that points in DandˆDhave identical last dimensions.\nDefine Ii\nq,p(similar to proof for Theorem 3 Part (i)) as an indicator function equal to 1 if a record p\nmatches the first idimensions in q= (c1, ..., c d, r1, ..., r d), that is if cj≤pj≤cj+rjfor all j∈[i],\nand let D∗={D[i, d]∀i,Id−1\nq,D[i]= 1}be a 1-dimensional dataset of the d-th attribute of the records\nofDthat matches the first d−1dimensions of q, and similarly define ˆD∗={ˆD[i, d]∀i,Id−1\nq,ˆD[i]=\n1}forˆD. Note that the d-th dimension of ˆD∗andD∗are identical, so that they both contain a\nsubset of elements of the d-th dimension of D, where the subset is selected based on the indicator\nfunction . Thus, we apply Lemma 5 to ˆD∗andD∗(the masks in the lemma are induced based on\nthe indicator functions, i.e., m1= (Id−1\nq,ˆD[1], ...,Id−1\nq,ˆD[n])andm2= (Id−1\nq,D[1], ...,Iq,D[n]d−1)) to\nobtain\n∥cD∗−cˆD∗∥1≤1\n2X\ni∈[n]|Id−1\nq,D[i]− Id−1\nq,ˆD[i]|.\nMoreover, by definition,\n∥cD(c1, ..., c d−1, ., r1, ..., r d−1, .)−cD′(c1, ..., c d−1, ., r1, ..., r d−1, .)∥1=∥cD∗−cˆD∗∥1.\n23\n\nPublished as a conference paper at ICLR 2024\nCombining the last two inequalities, we have\n∥cD−cˆD∥1≤Z\nc1...Z\ncd−1Z\nr1...Z\nrd−1X\ni∈[n]1\n2|Id−1\nq,D[i]− Id−1\nq,ˆD[i]|\n≤1\n2X\ni∈[n]Z\nc1...Z\ncd−1Z\nr1...Z\nrd−1|Id−1\nq,D[i]− Id−1\nq,ˆD[i]|.\nNow observe that for any qandi\n|Id−1\nq,D[i]− Id−1\nq,ˆD[i]| ≤X\nj∈[d−1]|Icj≤D[i,j]≤cj+rj−Icj≤ˆD[i,j]≤cj+rj|,\nWhere Iis the indicator function, so that\n∥cD−cˆD∥1≤1\n2X\nj∈[d−1]X\ni∈[n]Z\nc1...Z\ncd−1Z\nr1...Z\nrd−1|Icj≤D[i,j]≤cj+rj−Icj≤ˆD[i,j]≤cj+rj|.\nNote that for any j,|Icj≤D[i,j]≤cj+rj−Icj≤ˆD[i,j]≤cj+rj|= 1 only in the two following scenario:\n(1) if ˆD[i, j]≤cj≤D[i, j]andD[i, j]−cj≤rj≤1or (2) ˆD[i, j]−1≤cj≤ˆD[i, j]and\nˆD[i, j]−cj≤rj≤D[i, j]−cj. For the first scenario, we have that\nZD[i,j]\nˆD[i,j]Z1\nD[i,j]−cj1 drjdcj=ZD[i,j]\nˆD[i,j](1−D[i, j] +cj) dcj\n= [(1−D[i, j])cj+c2\nj\n2]D[i,j]\nˆD[i,j]dcj\n=−[(1−D[i, j])ˆD[i, j] +ˆD[i, j]2\n2] + [(1 −D[i, j])D[i, j] +D[i, j2]\n2]\n=−ˆD[i, j] +D[i, j]ˆD[i, j]−ˆD[i, j]2\n2+D[i, j]−D[i, j]2\n2\n= (D[i, j]−ˆD[i, j])−1\n2(D[i, j]−ˆD[i, j])2\n≤ϵ\nn\nAnd for the second scenario we have\nZˆD[i,j]\nˆD[i,j]−1ZD[i,j]−cj\nˆD[i,j]−cj1 drjdcj=ZˆD[i,j]\nˆD[i,j]−1(ˆD[i, j]−D[i, j]) dcj\n= (ˆD[i, j]−D[i, j])\n≤ϵ\nn,\nThus, we have\n∥cD−cˆD∥1≤1\n2X\nj∈[d−1]X\ni∈[n]Z\nc1...Z\ncd−1Z\nr1...Z\nrd−1|Icj≤D[i,j]≤cj+rj−Icj≤ˆD[i,j]≤cj+rj|\n≤1\n2X\nj∈[d−1]X\ni∈[n]2ϵ\nn\n= (d−1)ϵ.\nProof of Lemma 8 .\n24\n\nPublished as a conference paper at ICLR 2024\n∥sD−sD′∥1=Z\nq∈Qs|X\ni∈[n]IDi,q(D[i, d+ 1]−D′[i, d+ 1])|\n≤Z\nq∈QsX\ni∈[n]IDi,q|D[i, d+ 1]−D′[i, d+ 1]|\n≤Z\nq∈QsX\ni∈[n]IDi,qϵ\nn\n≤X\ni∈[n]ϵ\nnZ\nq∈Qs1\n=ϵ\nProof of Lemma 9 .\n∥sD−sD′∥1=Z\nrZ\nc|X\ni∈n(ID[i,1]∈[c,c+r]D[i,2]−ID′[i,1]i∈[c,c+r]D[i,2])|\n≤Z\nrZ\ncX\ni∈n|ID[i,1]∈[c,c+r]−ID′[i,1]∈[c,c+r]|D[i,2]\n≤Z\nrZ\ncX\ni∈n|ID[i,1]∈[c,c+r]−ID′[i,1]∈[c,c+r]|\n=X\ni∈nZ\nrZ\nc|ID[i,1]∈[c,c+r]−ID′[i,1]∈[c,c+r]|\n≤2X\ni∈nZ\nrmin{ϵ\nn, r}\n≤2X\ni∈nZ\nrϵ\nn\n= 2ϵ\nProof of Lemma 10 . We first state the following lemma, whose proof is deferred to the end.\nLemma 14. Consider two 2-dimensional datasets DandD′over points p1, ...,pks.t. the points\ninDare each repeated z1, ..., z ktimes for k∈[n]and0≤zi≤n, and the points D′are each\nrepeated z′\n1, ..., z′\nktimes for k∈[n]and0≤z′\ni≤n. Let t=P\ni∈[k]|zi−z′\ni|. We have that\n∥sD−sD′∥1≤1\n2t.\nIn our setting, let p1, ...,pkbe the distinct element of D, letz1, ..., z kbe the number of times\neach element is repeated in D1andz′\n1, ..., z′\nkbe the number of times each element is repeated in\nD2. By Lemma 14, we have that ∥sD1−sD2∥1≤1\n2P\ni∈[k]|zi−z′\ni|. Now observe that zi=\n25\n\nPublished as a conference paper at ICLR 2024\nP\nj∈[n]m1[j]ID[j]=piand similarly z′\ni=P\nj∈[n]m2[j]ID[j]=pi. Thus, we have\n∥sD1−sD2∥1≤1\n2X\ni∈[k]|zi−z′\ni|\n=1\n2X\ni∈[k]|X\nj∈[n]ID[j]=pi(m1[j]−m2[j])|\n≤1\n2X\ni∈[k]X\nj∈[n]ID[j]=pi|m1[j]−m2[j]|\n=1\n2X\nj∈[n]|m1[j]−m2[j]|.\nProof of Lemma 11 . For notational consistency with proof of Theorem 3, we rename D′asDin the\nproof here.\nConsider the dataset ˆDthatˆD[i, j] =¯D[i, j]forj∈[d−1]andˆD[i, d] =D[i, d]. That is ˆDis\na dataset with its first d−1dimensions the same as ¯Dand its d-th dimension the same as D. We\nhave that\n∥sD−s¯D∥1≤ ∥sD−sˆD∥1+∥sˆD−s¯D∥1\nWe study the first two terms separately. Consider the second term ∥sˆD−s¯D∥1. Observe that the\nfirstd−1dimensions of ¯DandˆDare identical, so that applying any predicate on the first d−1\nattributes of the datasets leads to the same filtered data points. Furthermore, the d-th dimension of\nthe points differ by at mostϵ\nn, so by Lemma 9, we have\n∥sˆD−s¯D∥1≤2ϵ.\nNext, consider the term ∥sD−sˆD∥1. Define Ii\nq,pan indicator function equal to 1 if a record p\nmatches the first idimensions in q= (c1, ..., c d, r1, ..., r d), that is if cj≤pj≤cj+rjfor all\nj∈[i], and let D∗={D[i, d:d+ 1]∀i,Id−1\nq,D[i]= 1}be a 2-dimensional dataset of the d-th and\nd+ 1-th attribute of the records of Dthat matches the first d−1dimensions of q, and similarly\ndefine ˆD∗={ˆD[i, d:d+ 1]∀i,Id−1\nq,ˆD[i]= 1}forˆD. Note that the d-th and d+ 1-th dimension\nofˆD∗andD∗are identical, so that they both contain a subset of elements of the d-th and d+ 1-th\ndimension of D, where the subset is selected based on the indicator function Id−1\nq,q. Thus, we apply\nLemma 10 to ˆD∗andD∗(the masks in the lemma are induced based on the indicator functions,\ni.e.,m1= (Id−1\nq,ˆD[1], ...,Id−1\nq,ˆD[n])andm2= (Id−1\nq,D[1], ...,Id−1\nq,D[n])) to obtain\n∥sD∗−sˆD∗∥1≤1\n2X\ni∈[n]|Id−1\nq,D[i]− Id−1\nq,ˆD[i]|.\nMoreover, by definition,\n∥sD(c1, ..., c d−1, ., r1, ..., r d−1, .)−sD′(c1, ..., c d−1, ., r1, ..., r d−1, .)∥1=∥sD∗−sˆD∗∥1.\nCombining the last two inequalities, we have\n∥sD−sˆD∥1≤Z\nc1...Z\ncd−1Z\nr1...Z\nrd−1X\ni∈[n]1\n2|Id−1\nq,D[i]− Id−1\nq,ˆD[i]|\n≤1\n2X\ni∈[n]Z\nc1...Z\ncd−1Z\nr1...Z\nrd−1|Id−1\nq,D[i]− Id−1\nq,ˆD[i]|\n≤(d−1)ϵ\nWhere the last inequality was shown in the proof of Theorem 3. Putting everything together, we\nhave that\n∥sD−s¯D∥1≤2ϵ+ (d−1)ϵ= (d+ 1)ϵ.\n26\n\nPublished as a conference paper at ICLR 2024\nProof of Lemma 12 . We have ∥rD−rD′∥µ=R1\n0|rD(q)−rD′(q)|dµ. LetIi,qbe an indicator variable\ndenoting whether q∈[D[i],D′[i])(or[D′[i],D[i])ifD′[i]<D[i]). We have that |rD(q)−\nrD′(q)|=Pn\ni=0Ii,q. Thus, we have ∥rD−rD′∥µ=R1\n0Pn\ni=0Ii,qdµ=Pn\ni=0R1\n0Ii,qdµ=Pn\ni=0Rpi,2\npi,1dµ=P\niµ([D[i],D′[i]]).\nProof of Lemma 13 . Let D∗=D∪D′, that is D∗is a dataset over points p1, ..., p keach repeated\nz∗\n1, ..., z∗\nkwithz∗\ni= max {zi, z′\ni}. We have ∥cD−cD′∥≤∥cD−cD∗∥1+∥cD′−cD∗∥1.\nBy Lemma 15 (stated and proved below) we have\n∥cD−cD∗∥1≤1\n2X\ni∈[k](z∗\ni−zi) =1\n2X\ni∈[k](max{zi, z′\ni} −zi)\nand similarly\n∥cD′−cD∗∥1≤1\n2X\ni∈[k](z∗\ni−z′\ni) =1\n2X\ni∈[k](max{zi, z′\ni} −z′\ni)\nso that\n∥cD−cD′∥1≤1\n2X\ni∈[k](max{zi, z′\ni} −z′\ni) +1\n2X\ni∈[k](max{zi, z′\ni} −zi)\n=1\n2X\ni∈[k](max{zi, z′\ni} −zi+ max {zi, z′\ni} −z′\ni)\n=1\n2X\ni∈[k]|zi−z′\ni|\nProof of Lemma 14 . LetD∗=D∪D′, that is D∗is a dataset over points p1, ...,pkeach repeated\nz∗\n1, ..., z∗\nkwithz∗\ni= max {zi, z′\ni}. We have ∥sD−sD′∥≤∥sD−sD∗∥1+∥sD′−sD∗∥1.\nBy Lemma 16 (stated and proved below) we have\n∥sD−sD∗∥1≤1\n2X\ni∈[k](z∗\ni−zi) =1\n2X\ni∈[k](max{zi, z′\ni} −zi)\nand similarly\n∥sD′−sD∗∥1≤1\n2X\ni∈[k](z∗\ni−z′\ni) =1\n2X\ni∈[k](max{zi, z′\ni} −z′\ni)\nso that\n∥sD−sD′∥1≤1\n2X\ni∈[k](max{zi, z′\ni} −z′\ni) +1\n2X\ni∈[k](max{zi, z′\ni} −zi)\n=1\n2X\ni∈[k](max{zi, z′\ni} −zi+ max {zi, z′\ni} −z′\ni)\n=1\n2X\ni∈[k]|zi−z′\ni|\nLemma 15. Consider a 1-dimensional database Dof size n, such that points p1, ..., p kinDare\neach repeated z1, ..., z ktimes for k, zi∈[n],P\ni∈[n]zi=n. Consider another dataset D′contain-\ning the a subset of points p1, ...p k, having 0≤z′\ni≤ziandP\ni∈[n]z′\ni=n′so that n′≤n. Let\nt=P\ni∈[k]|zi−z′\ni|. We have that ∥cD−cD′∥1=tR1\nr=0r=1\n2t.\nProof of Lemma 15 . Fix a value for r. LetX={i∈[k], zi̸=z′\ni}.\nObserve that cDandcD′only differ for queries where there exists an i∈Xfor which c∈[pi−r, pi].\nThus,\n27\n\nPublished as a conference paper at ICLR 2024\n∥cD−cD′∥=Z1\nr=0Z1\nq=−1X\ni∈[k]Ipi∈(q,r)(zi−z′\ni)\n=Z1\nr=0X\ni∈[k](zi−z′\ni)Z1\nq=−1Ip∈(q,r)\n=Z1\nr=0X\np∈[k](zi−z′\ni)Zpi\nq=pi−r1\n=Z1\nr=0rt=t\n2\n.\nLemma 16. Consider a 2-dimensional database Dof size n, such that points p1, ...,pkinD\nare each repeated z1, ..., z ktimes for k, zi∈[n],P\ni∈[n]zi=n. Consider another dataset D′\ncontaining the a subset of points p1, ...pk, having 0≤z′\ni≤ziandP\ni∈[n]z′\ni=n′so that n′≤n.\nLett=P\ni∈[k]|zi−z′\ni|. We have that ∥sD−sD′∥1=tR1\nr=0r=1\n2t.\nProof of Lemma 16 . Fix a value for r. LetX={i∈[k], zi̸=z′\ni}.\nObserve that sDandsD′only differ for queries where there exists an i∈Xfor which c∈[pi[1]−\nr,pi[1]]. Thus,\n∥sD−sD′∥=Z1\nr=0Z1\nc=−1X\ni∈[k]Ipi[1]∈(c,r)(zi−z′\ni)pi[2]\n≤Z1\nr=0X\ni∈[k](zi−z′\ni)Z1\nc=−1Ipi[1]∈(c,r)\n=Z1\nr=0X\ni∈[k](zi−z′\ni)Zpi[1]\nc=pi[1]−r1\n=Z1\nr=0rt=t\n2\n.\n28",
  "textLength": 94438
}