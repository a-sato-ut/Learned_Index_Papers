{
  "paperId": "3e58abf7975bc605f8a485b7a0a35e271b6b29a6",
  "title": "Adaptive Learned Bloom Filter (Ada-BF): Efficient Utilization of the Classifier",
  "pdfPath": "3e58abf7975bc605f8a485b7a0a35e271b6b29a6.pdf",
  "text": "Adaptive Learned Bloom Filter (Ada-BF):\nEfﬁcient Utilization of the Classiﬁer\nZhenwei Dai\u0003and Anshumali Shrivastavay\n\u0003Dept. of Statistics and Dept. of Computer Sciencey, Rice University\nOctober 22, 2019\nAbstract\nRecent work suggests improving the performance of Bloom ﬁlter by incorporating a machine learning\nmodel as a binary classiﬁer. However, such learned Bloom ﬁlter does not take full advantage of the\npredicted probability scores. We proposed new algorithms that generalize the learned Bloom ﬁlter by\nusing the complete spectrum of the scores regions. We proved our algorithms have lower False Positive\nRate (FPR) and memory usage compared with the existing approaches to learned Bloom ﬁlter. We also\ndemonstrated the improved performance of our algorithms on real-world datasets.\n1 Introduction\nBloom ﬁlter (BF) is a widely used data structure for low-memory and high-speed approximate membership\ntesting [Bloom, 1970]. Bloom ﬁlters compress a given set Sinto bit arrays, where we can approximately\ntest whether a given element (or query) xbelongs to a set S, i.e.,x2Sor otherwise. Several applications,\nin particular caching in memory constrained systems, have beneﬁted tremendously from BF [Broder et al.,\n2002].\nBloom ﬁlter ensures a zero false negative rate (FNR), which is a critical requirement for many applications.\nHowever, BF does not have a non-zero false positive rate (FPR) [Dillinger and Manolios, 2004] due to hashing\ncollisions, which measures the performance of BF. There is a known theoretical limit to this reduction. To\nachieve a FPR of \u000f, BF costs at least nlog2(1=\u000f) log2ebits (n=jSj), which is log2e\u001944% off from the\ntheoretical lower bound [Carter et al., 1978]. Mitzenmacher [2002] proposed Compressed Bloom ﬁlter to\naddress the suboptimal space usage of BF, where the space usage can reach the theoretical lower bound in the\noptimal case.\nTo achieve a more signiﬁcant reduction of FPR, researchers have generalized BF and incorporated information\nbeyond the query itself to break through the theoretical lower bound of space usage. Bruck et al. [2006] has\nmade use of the query frequency and varied the number of hash functions based on the query frequency to\nreduce the overall FPR. Recent work [Kraska et al., 2018, Mitzenmacher, 2018] has proposed to improve the\nperformance of standard Bloom ﬁlter by incorporating a machine learning model. This approach paves a new\nhope of reducing false positive rates beyond the theoretical limit, by using context-speciﬁc information in\nthe form of a machine learning model [Hsu et al., 2019]. Rae et al. [2019] further proposed Neural Bloom\nFilter that learns to write to memory using a distributed write scheme and achieves compression gains over\nthe classical Bloom ﬁlter.\nThe key idea behind Kraska et al. [2018] is to use the machine learning model as a pre-ﬁlter to give each\nqueryxa scores(x).s(x)is usually positively associated with the odds that x2S. The assumption is that\nin many practical settings, the membership of a query in the set Scan be ﬁgured out from observable features\nofxand such information is captured by the classiﬁer assigned score s(x). The proposal of Kraska et al.\n1arXiv:1910.09131v1  [cs.DS]  21 Oct 2019\n\nuses this score and treats query xwith scores(x)higher than a pre-determined threshold \u001c(high conﬁdence\npredictions) as a direct indicator of the correct membership. Queries with scores less than \u001care passed to the\nback-up Bloom ﬁlter.\nCompared to the standard Bloom ﬁlter, learned Bloom ﬁlter (LBF) uses a machine learning model to answer\nkeys with high score s(x). Thus, the classiﬁer reduces the number of the keys hashed into the Bloom ﬁlter.\nWhen the machine learning model has a reliable prediction performance, learned Bloom ﬁlter signiﬁcantly\nreduce the FPR and save memory usage [Kraska et al., 2018]. Mitzenmacher [2018] further provided a\nformal mathematical model for estimating the performance of LBF. In the same paper, the author proposed a\ngeneralization named sandwiched learned Bloom ﬁlter (sandwiched LBF), where an initial ﬁlter is added\nbefore the learned oracle to improve the FPR if the parameters are chosen optimally.\nWastage of Information: For existing learned Bloom ﬁlters to have a lower FPR, the classiﬁer score\ngreater than the threshold \u001cshould have a small probability of wrong answer. Also, a signiﬁcant fraction of\nthe keys should fall in this high threshold regime to ensure that the backup ﬁlter is small. However, when\nthe scores(x)is less than\u001c, the information in the score s(x)is never used. Thus, there is a clear waste\nof information. For instance, consider two elements x1andx2with\u001c > s (x1)\u001ds(x2). In the existing\nsolutions,x1andx2will be treated in the exact same way, even though there is enough prior to believing that\nx1is more likely positive compared to x2.\nStrong dependency on Generalization: It is natural to assume that prediction with high conﬁdence\nimplies a low FPR when the data distribution does not change. However, this assumption is too strong for\nmany practical settings. First and foremost, the data distribution is likely to change in an online streaming\nenvironment where Bloom ﬁlters are deployed. Data streams are known to have bursty nature with drift\nin distribution [Kleinberg, 2003]. As a result, the conﬁdence of the classiﬁer, and hence the threshold, is\nnot completely reliable. Secondly, the susceptibility of machine learning oracles to adversarial examples\nbrings new vulnerability in the system. Examples can be easily created where the classiﬁer with any given\nconﬁdence level \u001c, is incorrectly classiﬁed. Bloom ﬁlters are commonly used in networks where such\nincreased adversarial false positive rate can hurt the performance. An increased latency due to collisions can\nopen new possibilities of Denial-of-Service attacks (DoS) [Feinstein et al., 2003].\nMotivation: For a binary classiﬁer, the density of score distribution, f(s(x))shows a different trend for\nelements in the set and outside the set S. We observe that for keys, f(s(x)jx2S)shows ascending trend as\ns(x)increases while f(s(x)jx =2S)has an opposite trend. To reduce the overall FPR, we need lower FPRs\nfor groups with a high f(s(x)jx =2S). Hence, if we are tuning the number of hash functions differently, more\nhash functions are required for the corresponding groups. While for groups with a few non-keys, we allow\nhigher FPRs. This variability is the core idea to obtaining a sweeter trade-off.\nOur Contributions: Instead of only relying on the classiﬁer whether score s(x)is above a single speciﬁc\nthreshold, we propose two algorithms, Ada-BF and disjoint Ada-BF, that rely on the complete spectrum\nof scores regions by adaptively tuning Bloom ﬁlter parameters in different score regions. 1) Ada-BF tunes\nthe number of hash functions differently in different regions to adjust the FPR adaptively; disjoint Ada-BF\nallocates variable memory Bloom ﬁlters to each region. 2) Our theoretical analysis reveals a new set of\ntrade-offs that brings lower FPR with our proposed scheme compared to existing alternatives. 3) We evaluate\nthe performance of our algorithms on two datasets: malicious URLs and malware MD5 signatures, where\n2\n\nour methods reduce the FPR by over 80% and save 50% of the memory usage over existing learned Bloom\nﬁlters.\nNotations: Our paper includes some notations that need to be deﬁned here. Let [g]denote the index set\nf1;2;\u0001\u0001\u0001;gg. We deﬁne query xas a key ifx2S, or a non-key if x =2S. Letndenote the size of keys\n(n=jSj), andmdenote the size of non-keys. We denote Kas the number of hash functions used in the\nBloom ﬁlter.\n2 Review: Bloom Filter and Learned Bloom Filter\nBloom Filter: Standard Bloom ﬁlter for compressing a set Sconsists of an R-bits array and Kindepen-\ndent random hash function h1;h2;\u0001\u0001\u0001;hK, taking integer values between 0andR\u00001, i.e.,hi:S)\nf0;1;\u0001\u0001\u0001;R\u00001g. The bit array is initialized with all 0. For every item x2S, the bit value of hi(x) = 1 ,\nfor alli2f0;1;\u0001\u0001\u0001;Kg, is set to 1.\nTo check a membership of an item x0in the setS, we return true if all the bits hi(x0), for alli2f0;1;\u0001\u0001\u0001;Kg,\nhave been set to 1. It is clear that Bloom ﬁlter has zero FNR (false negative rate). However, due to lossy hash\nfunctions,x0may be wrongly identiﬁed to be positive when x0=2Swhile all the hi(x0)s are set to 1 due\nto random collisions. It can be shown that if the hash functions are independent, the expected FPR can be\nwritten as follows\nE(FPR) = \n1\u0000\u0012\n1\u00001\nR\u0013Kn!K\n:\nLearned Bloom ﬁlter: Learned Bloom ﬁlter adds a binary classiﬁcation model to reduce the effective\nnumber of keys going to the Bloom ﬁlter. The classiﬁer is pre-trained on some available training data to\nclassify whether any given query xbelongs toSor not based on its observable features. LBF sets a threshold,\n\u001c, wherexis identiﬁed as a key if s(x)\u0015\u001c. Otherwise, xwill be inserted into a Bloom ﬁlter to identify its\nmembership in a further step (Figure 1). Like standard Bloom ﬁlter, LBF also has zero FNR. And the false\npositives can be either caused by that false positives of the classiﬁcation model ( s(xjx =2S)\u0015\u001c) or that of\nthe Bloom ﬁlter.\nIt is clear than when the region s(x)\u0015\u001ccontains large number of keys, the number of keys inserted into\nthe Bloom ﬁlter decreases which leads to favorable FPR. However, since we identify the region s(x)\u0015\u001cas\npositives, higher values of \u001cis better. At the same time, large \u001cdecreases the number of keys in the region\ns(x)\u0015\u001c, increasing the load of the Bloom ﬁlter. Thus, there is a clear trade-off.\n3 A Strict Generalization: Adaptive Learned Bloom Filter (Ada-BF)\nWith the formulation of LBF in the previous section, LBF actually divides the xinto two groups. When\ns(x)\u0015\u001c,xwill be identiﬁed as a key directly without testing with the Bloom ﬁlter. In other words, it\nuses zero hash function to identify its membership. Otherwise, we will test its membership using Khash\nfunctions. In other view, LBF switches from Khash functions to no hash function at all, based on s(x)\u0015\u001c\nor not. Continuing with this mindset, we propose adaptive learned Bloom ﬁlter, where xis divided into g\n3\n\ngroups based on s(x), and for group j, we useKjhash functions to test its membership. The structure of\nAda-BF is represented in Figure 1(b).\nFigure 1: Panel A-C show the structure of LBF, Ada-BF and disjoint Ada-BF respectively.\nMore speciﬁcally, we divide the spectrum into gregions, where x2Groupjifs(x)2[\u001cj\u00001;\u001cj),j=\n1;2;\u0001\u0001\u0001;g. Without loss of generality, here, we assume 0 =\u001c0<\u001c1<\u0001\u0001\u0001<\u001cg\u00001<\u001cg= 1. Keys from\ngroupjare inserted into Bloom ﬁlter using Kjindependent hash functions. Thus, we use different number\nof universal hash functions for keys from different groups.\nFor a group j, the expected FPR can be expressed as,\nE(FPRj) = \n1\u0000\u0012\n1\u00001\nR\u0013Pg\nt=1ntKt!Kj\n=\u000bKj(1)\nwherent=Pn\nt=1I(\u001ct\u00001\u0014s(xijxi2S)< \u001ct)is the number of keys falling in group t, andKjis the\nnumber of hash functions used in group j. By varying Kj,E(FPRj)can be controlled differently for each\ngroup.\nVariable number of hash functions gives us enough ﬂexibility to tune the FPR of each region. To avoid the bit\narray being overloaded, we only increase the Kjfor groups with large number of keys nj, while decrease\nKjfor groups with small nj. It should be noted that f(s(x)jx2S)shows an opposite trend compared to\nf(s(x)jx =2S)ass(x)increases (Figure 2). Thus, there is a need for variable tuning, and a spectrum of\nregions gives us the room to exploit these variability efﬁciently. Clearly, Ada-BF generalizes the LBF. When\nAda-BF only divides the queries into two groups, by setting K1=K,K2= 0and\u001c1=\u001c, Ada-BF reduces\nto the LBF.\n3.1 Simplifying the Hyper-Parameters\nTo implement Ada-BF, there are some hyper-parameters to be determined, including the number of hash\nfunctions for each group Kjand the score thresholds to divide groups, \u001cj(\u001c0= 0,\u001cg= 1). Altogether, we\nneed to tune 2g\u00001hyper-parameters. Use these hyper-parameters, for Ada-BF, the expected overall FPR\ncan be expressed as,\nE(FPR) =gX\nj=1pjE(FPRj) =gX\nj=1pj\u000bKj(2)\n4\n\nwherepj=Pr(\u001cj\u00001\u0014s(xijxi=2S)<\u001cj). Empirically, pjcan be estimated by ^pj=1\nmPm\ni=1I(\u001cj\u00001\u0014\ns(xijxi=2S)<\u001cj) =mj\nm(mis size of non-keys in the training data and mjis size of non-keys belonging\nto groupj). It is almost impossible to ﬁnd the optimal hyper-parameters that minimize the E(FPR)in\nreasonable time. However, since the estimated false positive itemsPg\nj=1mj\u000bKj=O(maxj(mj\u000bKj)), we\nprefermj\u000bKjto be similar across groups when E(FPR)is minimized. While \u000bKjdecreases exponentially\nfast with larger Kj, to keepmj\u000bKjstable across different groups, we require mjto grow exponentially fast\nwithKj. Moreover, since f(s(x)jx =2S)increases as s(x)becomes smaller for most cases, Kjshould also\nbe larger for smaller s(x). Hence, to balance the number of false positive items, as jdiminishes, we should\nincreaseKjlinearly and let mjgrow exponentially fast.\nWith this idea, we provide a strategy to simplify the tuning procedure. We ﬁxpj\npj+1=candKj\u0000Kj+1= 1\nforj= 1;2;\u0001\u0001\u0001;g\u00001. Since the true density of s(xjx =2S)is unknown. To implement the strategy, we\nestimatepj\npj+1bydpj\npj+1=mj\nmj+1and ﬁxmj\nmj+1=c. This strategy ensures ^pjto grow exponentially fast with\nKj. Now, we only have three hyper-parameters, c,KminandKmax(Kmax=K1). By default, we may also\nsetKmin=Kg= 0, equivalent to identifying all the items in group gas keys.\nLemma 1: Assume 1) the scores of non-keys, s(x)jx =2S, are independently following a distribution f;\n2) The scores of non-keys in the training set are independently sampled from a distribution f. Then, the\noverall estimation error of ^pj,P\njj^pj\u0000pjj, converges to 0 in probability as mbecomes larger. Moreover, if\nm\u00152(k\u00001)\n\u000f2\u0014q\n1\n\u0019+q\n1\u00002=\u0019\n\u000e\u00152\n, with probability at least 1\u0000\u000e, we haveP\njj^pj\u0000pjj\u0014\u000f.\nEven though in the real application, we cannot access the exact value of pj, which may leads to the estimation\nerror of the real E(FPR). However, Lemma 1 shows that as soon as we can collect enough non-keys to\nestimate the pj, the estimation error is almost negligible. Especially for the large scale membership testing\ntask, collecting enough non-keys is easy to perform.\n3.2 Analysis of Adaptive Learned Bloom Filter\nCompared with the LBF, Ada-BF makes full use the of the density distribution s(x)and optimizes the FPR\nin different regions. Next, we will show Ada-BF can reduce the optimal FPR of the LBF without increasing\nthe memory usage.\nWhenpj=pj+1=cj\u0015c>1andKj\u0000Kj+1= 1, the expected FPR follows,\nE(FPR) =gX\nj=1pj\u000bKj=Pg\nj=1cg\u0000j\u000bKj\nPg\nj=1cg\u0000j\u00148\n><\n>:(1\u0000c)(1\u0000(c\u000b)g)\n(1\n\u000b\u0000c)(\u000bg\u0000(c\u000b)g)\u000bKmax; c\u000b6= 1\n1\u0000c\n1\u0000cg\u0001g; c\u000b = 1(3)\nwhereKmax=K1. To simplify the analysis, we assume c\u000b> 1in the following theorem. Given the number\nof groupsgis ﬁxed, this assumption is without loss of generality satisﬁed by raising csince\u000bwill increase\nascbecomes larger. For comparisons, we also need \u001cof the LBF to be equal to \u001cg\u00001of the Ada-BF. In this\ncase, queries with scores higher than \u001care identiﬁed as keys directly by the machine learning model. So, to\ncompare the overall FPR, we only need to compare the FPR of queries with scores lower than \u001c.\nTheorem 1: For Ada-BF, givenpj\npj+1\u0015c>1for allj2[g\u00001], if there exists \u0015>0such thatc\u000b\u00151 +\u0015\nholds, andnj+1\u0000nj>0for allj2[g\u00001](njis the number of keys in group j). When g is large enough\n5\n\nandg\u0014b2Kc, then Ada-BF has smaller FPR than the LBF. Here Kis the number of hash functions of the\nLBF.\nTheorem 1 requires the number of keys njkeeps increasing while pjdecreases exponentially fast with j. As\nshown in ﬁgure 2, on real dataset, we observe from the histogram that as score increases, f(s(x)jx =2S)\ndecreases very fast while f(s(x)jx2S)increases. So, the assumptions of Theorem 1 are more or less\nsatisﬁed.\nMoreover, when the number of buckets is large enough, the optimal Kof the LBF is large as well. Given\nthe assumptions hold, theorem 1 implies that we can choose a larger gto divide the spectrum into more\ngroups and get better FPR. The LBF is sub-optimal as it only has two regions. Our experiments clearly show\nthis trend. For ﬁgure 3(a), Ada-BF achieves 25% of the FPR of the LBF when the bitmap size =200Kb,\nwhile when the budget of buckets =500Kb, Ada-BF achieves 15% of the FPR of the LBF. For ﬁgure 3(b),\nAda-BF only reduces the FPR of the LBF by 50% when the budget of buckets =100Kb, while when the\nbudget of buckets =300Kb, Ada-BF reduces 70% of the FPR of the LBF. Therefore, both the analytical\nand experimental results indicate superior performance of Ada-BF by dividing the spectrum into more small\ngroups. On the contrary, when gis small, Ada-BF is more similar to the LBF, and their performances are less\ndifferentiable.\n4 Disjoint Adaptive Learned Bloom Filter (Disjoint Ada-BF)\nAda-BF divides keys into ggroups based on their scores and hashes the keys into the same Bloom ﬁlter using\ndifferent numbers of hash functions. With the similar idea, we proposed an alternative approach, disjoint\nAda-BF, which also divides the keys into ggroups, but hashes keys from different groups into independent\nBloom ﬁlters. The structure of disjoint Ada-BF is represented in Figure 1(c). Assume we have total budget of\nRbits for the Bloom ﬁlters and the keys are divided into ggroups using the same idea of that in Ada-BF.\nConsequently, the keys from group jare inserted into j-th Bloom ﬁlter whose length is Rj(R=Pg\nj=1Rj).\nThen, during the look up stage, we just need to identify a query’s group and check its membership in the\ncorresponding Bloom ﬁlter.\n4.1 Simplifying the Hyper-Parameters\nAnalogous to Ada-BF, disjoint Ada-BF also has a lot of hyper-parameters, including the thresholds of scores\nfor groups division and the lengths of each Bloom ﬁlters. To determine thresholds \u001cj, we use similar tuning\nstrategy discussed in the previous section of tuning the number of groups gandmj\nmj+1=c. To ﬁndRjthat\noptimizes the overall FPR, again, we refer to the idea in the previous section that the expected number of false\npositives should be similar across groups. For a Bloom ﬁlter with Rjbuckets, the optimal number of hash\nfunctionsKjcan be approximated as Kj=Rj\nnjlog(2), wherenjis the number of keys in group j. And the\ncorresponding optimal expected FPR is E(FPRj) =\u0016Rj=nj(\u0016\u00190:618). Therefore, to enforce the expected\nnumber of false items being similar across groups, Rjneeds to satisfy\nmj\u0001\u0016Rj\nnj=m1\u0001\u0016R1\nn1()Rj\nnj\u0000R1\nn1=(j\u00001)log(c)\nlog(\u0016)\nSincenjis known given the thresholds \u001cjand the total budget of buckets Rare known, thus, Rjcan be\nsolved accordingly. Moreover, when the machine learning model is accurate, to save the memory usage, we\nmay also set Rg= 0, which means the items in group jwill be identiﬁed as keys directly.\n6\n\n4.2 Analysis of Disjoint Adaptive Learned Bloom Filter\nThe disjoint Ada-BF uses a group of shorter Bloom ﬁlters to store the hash outputs of the keys. Though the\napproach to control the FPR of each group is different from the Ada-BF, where the Ada-BF varies Kand\ndisjoint Ada-BF changes the buckets allocation, both methods share the same core idea to lower the overall\nFPR by reducing the FPR of the groups dominated by non-keys. Disjoint Ada-BF allocates more buckets for\nthese groups to a achieve smaller FPR. In the following theorem, we show that to achieve the same optimal\nexpected FPR of the LBF, disjoint Ada-BF consumes less buckets. Again, for comparison we need \u001cof the\nLBF is equal to \u001cg\u00001of the disjoint Ada-BF.\nTheorem 2: Ifpj\npj+1=c>1andnj+1\u0000nj>0for allj2[g\u00001](njis the number of keys in group j),\nto achieve the optimal FPR of the LBF, the disjoint Ada-BF consumes less buckets compared with the LBF\nwhengis large.\n5 Experiment\nBaselines: We test the performance of four different learned Bloom ﬁlters: 1) standard Bloom ﬁlter, 2)\nlearned Bloom ﬁlter, 3) sandwiched learned Bloom ﬁlter, 4) adaptive learned Bloom ﬁlter, and 5) disjoint\nadaptive learned Bloom ﬁlter. We use two datasets which have different associated tasks, namely: 1)\nMalicious URLs Detection and 2) Virus Scan. Since all the variants of Bloom ﬁlter structures ensure zero\nFNR, the performance is measured by their FPRs and corresponding memory usage.\n5.1 Task1: Malicious URLs Detection\nWe explore using Bloom ﬁlters to identify malicious URLs. We used the URLs dataset downloaded from\nKaggle, including 485,730 unique URLs. 16.47% of the URLs are malicious, and others are benign. We\nrandomly sampled 30% URLs (145,719 URLs) to train the malicious URL classiﬁcation model. 17 lexical\nfeatures are extracted from URLs as the classiﬁcation features, such as “host name length”, “path length”,\n“length of top level domain”, etc. We used “sklearn.ensemble.RandomForestClassiﬁer1” to train a random\nforest model. After saving the model with “pickle”, the model ﬁle costs 146Kb in total. “sklearn.predict_prob\"\nwas used to give scores for queries.\nWe tested the optimal FPR for the four learned Bloom ﬁlter methods under the total memory budget =\n200Kb to 500Kb (kilobits). Since the standard BF does not need a machine learning model, to make a fair\ncomparison, the bitmap size of BF should also include the machine learning model size (146 Kb in this\nexperiment). Thus, the total bitmap size of BF is 346Kb to 646Kb. To implement the LBF, we tuned \u001c\nbetween 0and1, and picked the one giving the minimal FPR. The number of hash functions was determined\nbyK=Round (R\nn0log 2) , wheren0is the number of keys hashed into the Bloom ﬁlter conditional \u001c. To\nimplement the sandwiched LBF, we searched the optimal \u001cand calculated the corresponding initial and\nbackup ﬁlter size by the formula in Mitzenmacher [2018]. When the optimal backup ﬁlter size is larger than\nthe total bits budget, sandwiched LBF does not need a initial ﬁlter and reduces to a standard LBF. For the\nAda-BF, we used the tuning strategy described in the previous section. Kminwas set to 0by default. Thus,\nwe only need to tune the combination of (Kmax;c)that gives the optimal FPR. Similarly, for disjoint Ada-BF,\nwe ﬁxedRg= 0and searched for the optimal (g;c).\n1The Random Forest classiﬁer consists 10 decision trees, and each tree has at most 20 leaf nodes.\n7\n\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104CountScore Distribution of Malicious URLs\nMalicious(a)\n0.0 0.2 0.4 0.6 0.8 1.0\nScore100101102103104105CountScore Distribution of Benign URLs\nBenign (b)\nFigure 2: Histogram of the classiﬁer’s score distributions of keys (Malicious) and non-keys (Benign) for Task\n1. We can see that nj(number of keys in region j) is monotonic when score > 0.3. The partition was only\ndone to ensurepj\npj+1\u0015c\nResult: Our trained machine learning model has a classiﬁcation accuracy of 0.93. Considering the non-\ninformative frequent class classiﬁer (just classify as benign URL) gives accuracy of 0.84, our trained learner\nis not a strong classiﬁer. However, the distribution of scores is desirable (Figure 2), where as s(x)increases,\nthe empirical density of s(x)decreases for non-keys and also increases for keys. In our experiment, when the\nsandwiched LBF is optimized, the backup ﬁlter size always exceeds the total bitmap size. Thus, it reduces to\nthe LBF and has the same FPR (as suggested by Figure 4(a)).\nOur experiment shows that compared to the LBF and sandwiched LBF, both Ada-BF and disjoint Ada-BF\nachieve much lower FPRs. When ﬁlter size = 500 Kb, Ada-BF reduces the FPR by 81% compared to LBF or\nsandwiched LBF (disjoint FPR reduces the FPR by 84%). Moreover, to achieve a FPR \u00190:9%, Ada-BF and\ndisjoint Ada-BF only require 200Kb, while both LBF and the sandwiched LBF needs more than 350Kb. And\nto get a FPR\u00190:35%, Ada-BF and disjoint Ada-BF reduce the memory usage from over 500Kb of LBF to\n300Kb, which shows that our proposed algorithms save over 40% of the memory usage compared with LBF\nand sandwiched LBF.\n5.2 Task 2: Virus Scan\nBloom ﬁlter is widely used to match the ﬁle’s signature with the virus signature database. Our dataset includes\nthe information of 41323 benign ﬁles and 96724 viral ﬁles. The virus ﬁles are collected from VirusShare\ndatabase [Vir]. The dataset provides the MD5 signature of the ﬁles, legitimate status and other 53 variables\ncharacterizing the ﬁle, like “Size of Code”, “Major Link Version” and “Major Image Version”. We trained a\nmachine learning model with these variables to differentiate the benign ﬁles from the viral documents. We\nrandomly selected 20% samples as the training set to build a binary classiﬁcation model using Random Forest\nmodel2. We used “sklearn.ensemble.RandomForestClassiﬁer” to tune the model, and the Random Forest\nclassiﬁer costs about 136Kb. The classiﬁcation model achieves 0.98 prediction accuracy on the testing set.\nThe predicted the class probability (with the function “predict_prob” in “sklearn” library) is used as the score\ns(x). Other implementation details are similar to that in Task 1.\n2The Random Forest classiﬁer consists 15 decision trees, and each tree has at most 5 leaf nodes.\n8\n\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nScore102CountScore Distribution of Viral Files\nViral(a)\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nScore102103CountScore Distribution of Benign Files\nBenign (b)\nFigure 3: Histogram of the classiﬁer score distributions for the Virus Scan Dataset. The partition was only\ndone to ensurepj\npj+1\u0015c.\nResult: As the machine learning model achieves high prediction accuracy, ﬁgure 4 suggests that all the\nlearned Bloom ﬁlters show huge advantage over the standard BF where the FPR is reduced by over 98%.\nSimilar to the previous experiment results, we observe consistently lower FPRs of our algorithms although\nthe the score distributions are not smooth or continuous (Figure 3). Again, our methods show very similar\nperformance. Compared with LBF, our methods reduce the FPRs by over 80%. To achieve a 0.2% FPR, the\nLBF and sandwiched LBF cost about 300Kb bits, while Ada-BF only needs 150Kb bits, which is equivalent\nto 50% memory usage reduction compared to the previous methods.\n5.3 Sensitivity to Hyper-parameter Tuning\nCompared with the LBF and sandwiched LBF where we only need to search the space of \u001cto optimize\nthe FPR, our algorithms require to tune a series of score thresholds. In the previous sections, we have\nproposed a simple but useful tuning strategies where the score thresholds can be determined by only two\nhyper-parameters, (K;c). Though our hyper-parameter tuning technique may lead to a sub-optimal choice,\nour experiment results have shown we can still gain signiﬁcantly lower FPR compared with previous LBF.\nMoreover, if the number of groups Kis misspeciﬁed from the optimal choice (of K), we can still achieve\nvery similar FPR compared with searching both Kandc. Figure 5 shows that for both Ada-BF and disjoint\nAda-BF, tuning cwhile ﬁxing Khas already achieved similar FPRs compared with optimal case by tuning\nboth(K;c), which suggests our algorithm does not require very accurate hyper-parameter tuning to achieve\nsigniﬁcant reduction of the FPR.\n5.4 Discussion: Sandwiched Learned Bloom ﬁlter versus Learned Bloom ﬁlter\nSandwiched LBF is a generalization of LBF and performs no worse than LBF. Although Mitzenmacher\n[2018] has shown how to allocate bits for the initial ﬁlter and backup ﬁlter to optimize the expected FPR,\ntheir result is based on the a ﬁxed FNR and FPR. While for many classiﬁers, FNR and FPR are expressed as\nfunctions of the prediction score \u001c. Figure 4(a) shows that the sandwiched LBF always has the same FPR\nas LBF though we increase the bitmap size from 200Kb to 500Kb. This is because the sandwiched LBF is\noptimized when \u001ccorresponds to a small FPR and a large FNR, where the optimal backup ﬁlter size even\n9\n\n5.0%10.0%15.0%False Positive Rates Comparison\n200250300350400450500\nBitmap Size (Kb)0.00%0.50%1.00%1.50%2.00%2.50%False Positive RateBF\nLBF\nsandwiched LBF\nAda-BF\ndisjoint Ada-BF(a)\n20.0%40.0%False  Positive Rates Comparison\n100 150 200 250 300\nBitmap Size (Kb)0.00%0.10%0.20%0.30%0.40%0.50%0.60%0.70%0.80%False  Positive RateBF\nLBF\nsandwiched LBF\nAda-BF\ndisjoint Ada-BF (b)\nFigure 4: FPR with memory budget for all the ﬁve baselines (the bit budget of BF =bitmap size + learner\nsize). (a) FPRs comparison of Malicious URL detection experiment; (b) FPRs comparison of Virus scan\nexperiment.\nexceeds the total bitmap size. Hence, we should not allocate any bits to the initial ﬁlter, and the sandwiched\nLBF reduces to LBF. On the other hand, our second experiment suggests as the bitmap size becomes larger,\nsparing more bits to the initial ﬁlter is clever, and the sandwiched LBF shows the its advantage over the LBF\n(Figure 6(b)).\n6 Conclusion\nWe have presented new approaches to implement learned Bloom ﬁlters. We demonstrate analytically and\nempirically that our approaches signiﬁcantly reduce the FPR and save the memory usage compared with the\npreviously proposed LBF and sandwiched LBF even when the learner’s discrimination power . We envision\nthat our work will help and motivate integrating machine learning model into probabilistic algorithms in a\nmore efﬁcient way.\n10\n\nReferences\nVirusshare 2018. https://virusshare.com/research.4n6 .\nBurton H Bloom. Space/time trade-offs in hash coding with allowable errors. Communications of the ACM ,\n13(7):422–426, 1970.\nAndrei Broder, Michael Mitzenmacher, and Andrei Broder I Michael Mitzenmacher. Network applications\nof bloom ﬁlters: A survey. In Internet Mathematics . Citeseer, 2002.\nJehoshua Bruck, Jie Gao, and Anxiao Jiang. Weighted bloom ﬁlter. In 2006 IEEE International Symposium\non Information Theory , pages 2304–2308. IEEE, 2006.\nLarry Carter, Robert Floyd, John Gill, George Markowsky, and Mark Wegman. Exact and approximate\nmembership testers. In Proceedings of the tenth annual ACM symposium on Theory of computing , pages\n59–65. ACM, 1978.\nPeter C. Dillinger and Panagiotis Manolios. Bloom ﬁlters in probabilistic veriﬁcation. In Alan J. Hu and\nAndrew K. Martin, editors, Formal Methods in Computer-Aided Design , page 370, Berlin, Heidelberg,\n2004. Springer Berlin Heidelberg. ISBN 978-3-540-30494-4.\nLaura Feinstein, Dan Schnackenberg, Ravindra Balupari, and Darrell Kindred. Statistical approaches to ddos\nattack detection and response. In Proceedings DARPA information survivability conference and exposition ,\nvolume 1, pages 303–314. IEEE, 2003.\nChen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency estimation algorithms.\nInInternational Conference on Learning Representations , 2019. URL https://openreview.net/\nforum?id=r1lohoCqY7 .\nJon Kleinberg. Bursty and hierarchical structure in streams. Data Mining and Knowledge Discovery , 7(4):\n373–397, 2003.\nTim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned index\nstructures. In Proceedings of the 2018 International Conference on Management of Data , pages 489–504.\nACM, 2018.\nMichael Mitzenmacher. Compressed bloom ﬁlters. IEEE/ACM Transactions on Networking (TON) , 10(5):\n604–612, 2002.\nMichael Mitzenmacher. A model for learned bloom ﬁlters and optimizing by sandwiching. In Advances in\nNeural Information Processing Systems , pages 464–473, 2018.\nJack W Rae, Sergey Bartunov, and Timothy P Lillicrap. Meta-learning neural bloom ﬁlters. arXiv preprint\narXiv:1906.04304 , 2019.\n11\n\nAppendix A Sensitivity to hyper-parameter tuning\n200250300350400450500\nBudget of buckets (Kb)0.00%0.20%0.40%0.60%0.80%1.00%1.20%False Positive RateAda-BF: FPRs with different K\nK=10\nK=11\nK=12\nOptimal\n(a)\n200 250 300 350 400 450 500\nBudget of buckets (Kb )0.00%0.20%0.40%0.60%0.80%1.00%1.20%False Positive RateDisjoint Ada-BF: FPRs with different K\nK=10\nK=11\nK=12\nOptimal (b)\nFigure 5: FPR comparison of tuning cwhile ﬁxing the number of groups Kand tuning both (K;c)\nAppendix B More comparisons between the LBF and sandwiched LBF\n200250300350400450 600700800\nBudget of buckets (Kb)0.00%0.50%1.00%1.50%2.00%2.50%False Positive RateLBF vs sandwiched LBF\nLBF\nsandwiched LBF\n(a)\n100150200250300350400450500\nBudget of buckets (Kb)0.00%0.10%0.20%0.30%0.40%0.50%0.60%0.70%False Positive RateLBF vs sandwiched LBF\nLBF\nsandwiched LBF (b)\nFigure 6: FPR comparison between LBF and sandwiched LBF under different bitmap sizes. (a) malicious\nURL experiment; (b) malware detection experiment\nAppendix C Proof of the Statements\nProof of Lemma 1: LetZj(x) =Pm\ni=1 1(s(x)2[\u001cj\u00001;\u001cj)jx =2S), thenZj(x)\u0018Bernoulli (pj),\nandmj=Pm\ni=1Zj(xi)counts the number of non-keys falling in group jand^pj=mj\nm. To upper\n12\n\nbound the probability of the overall estimation error of pj, ﬁrst, we need to evaluate its expectation,\nE\u0010PK\nj=1j^pj\u0000pjj\u0011\n.\nSincemjis a binomial random variable, its exact cdf is hard to compute. But with central limit theorem,\nwhenmis large,mj\u0000mpjp\nmpj(1\u0000pj)\u0000!N(0;1). Thus, we can approximate E(j^pj\u0000pjj) =E\u0012\njmj\u0000mpjp\nmpj(1\u0000pj)j\u0013\n\u0001\nq\npj(1\u0000pj)\nm\u0019q\n2\n\u0019\u0001q\npj(1\u0000pj)\nm(ifZ\u0018N(0;1),E(jZj) =q\n2\n\u0019). Then, the expectation of overall error\nis approximated by E\u0010PK\nj=1j^pj\u0000pjj\u0011\n\u0019q\n2\nm\u0019\u0001\u0010PK\nj=1p\npj(1\u0000pj)\u0011\n, which goes to 0asmbecomes\nlarger.\nWe need to further upper bound the tail probability ofPK\nj=1j^pj\u0000pjj. First, we upper bound the variance ofPK\nj=1j^pj\u0000pjj,\nVar0\n@KX\nj=1j^pj\u0000pjj1\nA\u0014KKX\nj=1Var (j^pj\u0000pjj) =KKX\nj=1\u0010\nVar (^pj\u0000pj)\u0000E(j^pj\u0000pjj)2\u0011\n\u0019K\nmKX\nj=10\n@pj(1\u0000pj)\u00002\n\u0019 KX\ni=1q\npj(1\u0000pj)!21\nA,K\nmV(p)\nNow, by envoking the Chebyshev’s inequality,\nP2\n4KX\nj=1j^pj\u0000pjj\u0015\u000f3\n5= P2\n4KX\nj=1j^pj\u0000pjj\u0000E0\n@KX\nj=1j^pj\u0000pjj1\nA\u0015\u000f\u0000E0\n@KX\nj=1j^pj\u0000pjj1\nA3\n5\n\u0014Var\u0010PK\nj=1j^pj\u0000pjj\u0011\n\u0010\n\u000f\u0000E\u0010PK\nj=1j^pj\u0000pjj\u0011\u00112\n=KV(p)\nm\u0010\n\u000f\u0000E\u0010PK\nj=1j^pj\u0000pjj\u0011\u00112\u0000!0asm\u0000!1\nThus,PK\nj=1j^pj\u0000pjjconverges to 0 in probability as m\u0000!1 .\u0003\nMoreover, since we have\nE0\n@KX\nj=1j^pj\u0000pjj1\nA\u0019r\n2\nm\u0019(KX\nj=1q\npj(1\u0000pj))\u0014r\n2\nm\u0019(K\u00001) (4)\nV(p) =KX\nj=10\n@pj(1\u0000pj)\u00002\n\u0019 KX\ni=1q\npj(1\u0000pj)!21\nA\n\u0014KX\nj=1\u0012\npj(1\u0000pj)\u0012\n1\u00002\n\u0019\u0013\u0013\n\u0014\u0012\n1\u00002\n\u0019\u0013\u0012\n1\u00001\nK\u0013\n(5)\n13\n\nThen, by Eq 4 and Eq 5, we can upper bound PhPK\nj=1j^pj\u0000pjj\u0015\u000fi\nby,\nP2\n4KX\nj=1j^pj\u0000pjj\u0015\u000f3\n5\u0014KV(p)\nm\u0010\n\u000f\u0000E\u0010PK\nj=1j^pj\u0000pjj\u0011\u00112\n\u0014(1\u00002\n\u0019)(K\u00001)\nm\u0010\n\u000f\u0000q\n2\nm\u0019(K\u00001)\u00112(6)\nWhenm\u00152(k\u00001)\n\u000f2\u0014q\n1\n\u0019+q\n1\u00002=\u0019\n\u000e\u00152\n, we havem\u0010\n\u000f\u0000q\n2\nm\u0019(K\u00001)\u00112\n\u0015(K\u00001)(1\u00002\n\u0019)\n\u000e, thus,\nPhPK\nj=1j^pj\u0000pjj\u0015\u000fi\n\u0014\u000e.\u0003\nProof of Theorem 1: For comparison, we choose \u001c=\u001cg\u00001, for both LBF and Ada-BF, queries with scores\nlarger than\u001care identiﬁed as keys directly by the same machine learning model. Thus, to compare the overall\nFPR, we only need to evaluate the FPR of queries with score lower than \u001c.\nLetp0= P [s(x)<\u001cjx =2S]be the probability of a key with score lower than \u001c. Letn0denote the number\nof keys with score less than \u001c,n0=P\ni:xi2SI(s(xi)<\u001c). For learned Bloom ﬁlter using Khash functions,\nthe expected FPR follows,\nE(FPR) = (1\u0000p0) +p0 \n1\u0000\u0012\n1\u00001\nR\u0013Kn 0!K\n= 1\u0000p0+p0\fK; (7)\nwhereRis the length of the Bloom ﬁlter. For Ada-BF, assume we ﬁx the number of groups g. Then, we only\nneed to determine KmaxandKmin=Kmax\u0000g+ 1. Letpj=Pr(\u001cj\u00001\u0014s(x)<\u001cjjx =2S)The expected\nFPR of the Ada-BF is,\nE(FPRa) =gX\nj=1pj \n1\u0000\u0012\n1\u00001\nR\u0013Pg\u00001\nj=1Kjnj!K\nj=g\u00001X\nj=1pj\u000bKj; (8)\nwherePg\u00001\nj=1nj=n0. Next, we give a strategy to select Kmaxwhich ensures a lower FPR of Ada-BF than\nLBF.\n14\n\nSelectKmax=bK+g\n2\u00001c. Then, we have\nn0K=g\u00001X\nj=1njK=K2\n4n1+g\u00001X\ni=2(n1+j\u00001X\ni=1Ti) =n1(g\u00001) +g\u00002X\nj=1Tj(g\u0000j\u00001)3\n5\n=2K\ng\u000022\n4(g\u00001)(g\u00002)\n2n1+g\u00002X\nj=1(g\u00002)(g\u00001\u0000j)\n2Tj3\n5\n\u00142\ng\u000022\n4(g\u00001)(g\u00002)\n2n1+g\u00002X\nj=1(g+j\u00002)(g\u00001\u0000j)\n2Tj3\n5\n=2\ng\u00002g\u00001X\nj=1(j\u00001)nj (9)\nBy Eq 9. we further get the relationship between \u000band\f.\ng\u00001X\nj=1Kjnj=g\u00001X\nj=1(Kmax\u0000j+ 1)nj\u0014n0\u0010\nKmax\u0000g\n2+ 1\u0011\n\u0014n0K=)\u000b\u0014\f:\nMoreover, by Eq. 3, we have,\nE(FPRa) =(1\u0000c)(1\u0000(c\u000b)g)\n(1\n\u000b\u0000c)(\u000bg\u0000(c\u000b)g)\u000bKmax\u0014(1\u0000c)(1\u0000(c\u000b)g)\n(1\n\u000b\u0000c)(\u000bg\u0000(c\u000b)g)\fKmax\n\u0014\fKmax\u000b(c\u00001)\nc\u000b\u00001\n<E(FPR)\u00121 +\u0015\n\u0015\fKmax\u0000K\u0013\n\u0014E(FPR)\u00121 +\u0015\n\u0015\fbg=2\u00001c\u0013\n:\nTherefore, as gincreases, the upper bound of E(FPRa)decreases exponentially fast. Moreover, since1+\u0015\n\u0015\nis a constant, when gis large enough, we have1+\u0015\n\u0015\fbg=2\u00001c\u00141. Thus, the E(FPRe)is reduced to strictly\nlower than E(FPR).\u0003\nProof of Theorem 2: Let\u0011=log(c)\nlog(\u0016)\u0019log(c)\nlog(0:618)<0. By the tuning strategy described in the previous\nsection, we require the expected false positive items should be similar across the groups. Thus, we have\np1\u0001\u0016R1=n1=pj\u0001\u0016Rj=nj=)Rj=nj\u0012R1\nn1+ (j\u00001)\u0011\u0013\n;forj2[g\u00001]\nwhereRjis the budget of buckets for group j. For groupj, since all the queries are identiﬁed as keys by the\nmachine learning model directly, thus, Rg= 0. Given length of Bloom ﬁlter for group 1, R1, the total budget\n15\n\nof buckets can be expressed as,\ng\u00001X\nj=1Rj=g\u00001X\nj=1nj\nn1R1+ (j\u00001)nj\u0011\nLetp0=Pr(s(x)< \u001cjx =2S)andpj=Pr(\u001cj\u00001\u0014s(x)< \u001cjjx =2S). Letn0denote the number\nof keys with score less than \u001c,n0=P\ni:xi2SI(s(xi)< \u001c), andnjbe the number of keys in group j,\nnj=P\ni:xi2SI(\u001cj\u00001\u0014s(xi)<\u001cj). Due to\u001c=\u001cg\u00001, we havePg\u00001\nj=1nj=n0. Moreover, since \u001cg\u00001=\u001c,\nqueries with score higher than \u001chave the same FPR for both disjoint Ada-BF and LBF. So, we only need to\ncompare the FPR of the two methods when the score is lower than \u001c. If LBF and Ada-BF achieve the same\noptimal expected FPR, we have\np0\u0001\u0016R=n 0=g\u00001X\nj=1pj\u0001\u0016Rj=nj=g\u0001p1\u0001\u0016R1=n1\n=)R=n0\nn1R1\u0000n0log(p0=p1)\u0000log(g)\nlog(\u0016)\n=g\u00001X\nj=1\"\nnj\nn1R1\u0000njlog(1\u0000\u00001\nc)\u0001g\u0000log\u0000\n1\u00001\nc\u0001\n\u0000log(g)\nlog(\u0016)#\n;\nwhereRis the budget of buckets of LBF. Let Tj=nj+1\u0000nj\u00150. Next, we upper boundPg\u00001\nj=1njwithPg\u00001\nj=1(j\u00001)nj.\ng\u00001X\nj=1nj=n1+g\u00001X\ni=2(n1+j\u00001X\ni=1Ti) =n1(g\u00001) +g\u00002X\nj=1Tj(g\u0000j\u00001)\n=2\ng\u000022\n4(g\u00001)(g\u00002)\n2n1+g\u00002X\nj=1(g\u00002)(g\u00001\u0000j)\n2Tj3\n5\n\u00142\ng\u000022\n4(g\u00001)(g\u00002)\n2n1+g\u00002X\nj=1(g+j\u00002)(g\u00001\u0000j)\n2Tj3\n5\n=2\ng\u00002g\u00001X\nj=1(j\u00001)nj\nTherefore, we can lower bound R,\nR\u0015g\u00001X\nj=1\"\nnj\nn1R1\u0000(j\u00001)nj2(log(1\u0000\u00001\nc)\u0001g\u0000log\u0000\n1\u00001\nc\u0001\n\u0000log(g))\n(g\u00002) log(\u0016)#\n:\nNow, we can lower bound R\u0000Pg\u00001\nj=1Rj,\nR\u0000g\u00001X\nj=1Rj\u0015g\u00001X\nj=1(j\u00001)nj\"\n\u0000\u0011\u00002(log(1\u0000\u00001\nc)\u0001g\u0000log\u0000\n1\u00001\nc\u0001\n\u0000log(g))\n(g\u00002) log(\u0016)#\n:\n16\n\nSince\u0011is a negative constant, while2(log(1\u0000(1\nc))g\u0000log(1\u00001\nc)\u0000log(g))\n(g\u00002) log(\u0016)approaches to 0whengis large. There-\nfore, whengis large,\u0011\u00002(log(1\u0000(1\nc))g\u0000log(1\u00001\nc)\u0000log(g))\n(g\u00002) log(\u0016)<0andR\u0000Pg\u00001\nj=1Rjis strictly larger than 0. So,\ndisjoint Ada-BF consumes less memory than LBF to achieve the same expected FPR.\n17",
  "textLength": 37569
}