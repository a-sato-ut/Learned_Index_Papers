{
  "paperId": "a77da8b279b99969a632766d08384fd459a0152d",
  "title": "BMTree: Designing, Learning, and Updating Piecewise Space-Filling Curves for Multi-Dimensional Data Indexing",
  "pdfPath": "a77da8b279b99969a632766d08384fd459a0152d.pdf",
  "text": "1\nBMTree: Designing, Learning, and Updating\nPiecewise Space-Filling Curves for\nMulti-Dimensional Data Indexing\nJiangneng Li, Yuang Liu, Zheng Wang, Gao Cong, Cheng Long, Walid G. Aref, Fellow, IEEE, Han Mao Kiah,\nand Bin Cui, Fellow, IEEE\nAbstract ‚ÄîSpace-filling curves (SFC, for short) have been\nwidely applied to index multi-dimensional data, which first maps\nthe data to one dimension, and then a one-dimensional indexing\nmethod, e.g., the B-tree indexes the mapped data. Existing\nSFCs adopt a single mapping scheme for the whole data space.\nHowever, a single mapping scheme often does not perform well\non all the data space. In this paper, we propose a new type\nof SFC called piecewise SFCs that adopts different mapping\nschemes for different data subspaces. Specifically, we propose\na data structure termed the Bit Merging tree (BMTree) that\ncan generate data subspaces and their SFCs simultaneously,\nand achieve desirable properties of the SFC for the whole\ndata space. Furthermore, we develop a reinforcement learning-\nbased solution to build the BMTree, aiming to achieve excellent\nquery performance. To update the BMTree efficiently when the\ndistributions of data and/or queries change, we develop a new\nmechanism that achieves fast detection of distribution shifts in\ndata and queries, and enables partial retraining of the BMTree.\nThe retraining mechanism achieves performance enhancement\nefficiently since it avoids retraining the BMTree from scratch.\nExtensive experiments show the effectiveness and efficiency of\nthe BMTree with the proposed learning-based methods.\nIndex Terms ‚ÄîLearned Index, Space-Filling Curves.\nI. I NTRODUCTION\nASpace-filling curve (SFC, for short) is a way to map\na multi-dimensional data point xto a one-dimensional\nvalue, say vthat can be represented by a mapping func-\ntionT:x‚Ü¶‚Üív. SFC mappings been widely used for\nmulti-dimensional indexing. The idea is to first map multi-\ndimensional data points to one-dimensional values using the\nSFC mapping function, and then use one-dimensional indexing\nmethods, e.g., a conventional B-Tree [1] or any of the recent\nlearned indexes [2, 3, 4, 5], to index the mapped values. This\nhas been explored both in the literature [6, 7, 8, 9, 10, 11, 12]\nand by various database systems, e.g., PostgreSQL [13], Ama-\nzon DynamoDB [14], Apache HBase [15], and many other\nsystems.\nThere are extensive studies on designing SFCs, e.g., the\nZ-curve [16, 17, 18], the C-curve [19], and the Hilbert\ncurve [19, 20, 21, 22]. For example, the Z-curve adopts a\nbit interleaving mapping scheme [23] that first converts the\nJ. Li, Y . Liu, Z. Wang, G. Cong, C. Long, and H.M. Kiah are with Nanyang\nTechnological University, Singapore, 639798. E-mail: {jiangnen002@e.,\ns230084@e., zheng011@e., gaocong@, c.long@, HMKiah@ }ntu.edu.sg; W.\nG. Aref is with Purdue University, West Lafayette, IN, USA, 47907. E-\nmail: aref@purdue.edu; B. Cui is with School of Computer Science, Peking\nUniversity, Beijing, China, 100871. E-mail: bin.cui@pku.edu.cn.\n(10)(11)11012ùê±=(10!,11!)ùë£\"!=1101!P#=XYXY(10)(11)10112ùë£\"\"=1011!P$=XXYY(10)(11)11102ùë£\"#=1110!P!=XYYXFig. 1: The example illustrates mapping a point using various\nBit Merging Patterns (BMP), where each mapping corresponds\nto a difference SFC. The illustrated BMP mappings are as\nfollows: PZ(XYXY ) is the BMP of the Z-curve, XXYY is the\nBMP of the saw-tooth curve (a column-wise scan [19]), and\nXYYX is the third BMP.\ndimensions of input data to bit strings . Refer to Figure 1 for\nillustration, where data point x= (2,3)is converted into its\ntwo corresponding binary strings (one string per coordinate)\nwith 2 bits for each dimension: (102,112). Then, the bit\ninterleaving merges bits alternatively from the different bit\nstrings to form one SFC value (in Figure 1, the bit interleaving\nadopts the XYXY merging scheme that merges the bit strings\nXXandYYto the SFC value XYXY , e.g., mapping Point xto\n1101 2).\nHowever, one common problem is that each SFC has its\nown fixed mapping scheme/function that cannot be adjusted\nto fit different datasets. The choice of one SFC for a dataset\nsignificantly affects query performance, and no single SFC\ncan dominate the performance for all datasets and all query\nworkloads (as shown in Fig. 2). To tailor a new SFC to fit the\ndata and query workload properties, QUILTS [24] extends bit\ninterleaving by considering other ways of merging bit strings.\nFor example, instead of merging bits following XYXY , we can\nmerge bits by following XXYY orXYYX to generate different\nSFC values at different regions of the multi-dimensional data\nset. Each pattern of merging bits is termed a bit merging\npattern (BMP, for short), where each BMP can describe a\ndifferent SFC (as will be explained in Section II in greater\ndetailed). QUILTS evaluates all the candidate SFCs described\nby BMPs based on a given workload and data, and selects the\noptimal one using heuristic methods.\nQUILTS proposes to use multiple SFCs at the same time to\nindex one data set so that the resulting mixed SFC is query-\naware and is skew-tolerant for a given query pattern. However,\nthe resulting SFC is static and hence does not change if the\ndata distribution or the query workload changes over time.\n\n2\nQUILTS makes the first attempt to utilize data and query\nworkload properties to select an optimal SFC. However, like\nother SFCs, QUILTS applies a single BMP for the entire\ndata space (i.e., QUILTS applies one BMP to compute the\nSFC values of all data points). Optimal SFCs may differ for\ndifferent data subspaces. A detailed example of this situation\nis given in Section III-A that illustrates this problem. Another\nissue of QUILTS is that it does not provide an effective way\nof generating and evaluating candidate SFCs. The heuristic\nrules used by QUILTS are designed for very specific types of\nwindow queries (e.g., with a fixed area) and do not fit general\nquery processing scenarios, where the workload includes more\nthan one query type (with different areas or aspect ratios). For\nexample, a heuristic rule used by QUILTS assumes that grid\ncells intersecting with a query should be continuous in SFC\norder, which may not hold for queries with different aspect\nratios (this is elaborated on in greater detail in Section III-A).\nFurther, QUILTS does not consider the scenario where the\ndistributions of data and/or queries are changed, which results\nin sub-optimal query performance if the SFC is not updated.\nTo address the limitations of an SFC with a single BMP,\ni.e., a single mapping scheme, our idea is to design different\nBMPs for different subspaces based on the data and query\nworkload features, aiming to optimize query performance. The\nresulting SFCs used in the BMTree would comprise multiple\nBMPs, each corresponding to a subspace that we refer to by a\npiecewise SFC. We focus on three aspects of the piecewise\nSFC construction framework: (1) Designing the piecewise\nSFC: We propose a binary tree structure to help construct\nand design a piecewise SFC. (2) Learning the piecewise\nSFC. We design a data-driven learning-based method that can\nautomatically construct the piecewise SFC according to the\nquery workload to optimize query performance. (3) Updating\npiecewise SFC: We develop a mechanism aiming to efficiently\nupdate the piecewise SFC w.r.t. the updated query and data\nscenario.\nA. Designing Piecewise SFCs\nHow to design effective BMPs for different subspaces,\nwhile guaranteeing desirable properties of the overall mapping\nfunction to indexing data, is non-trivial. To achieve that,\nwe propose to seamlessly integrate subspace partitioning and\nBMP generation. We develop a new structure termed the Bit\nMerging Tree (BMTree, for short) to recursively generate both\nthe subspaces and the corresponding BMPs. In the BMTree,\n(1) Each node represents a bit from the binary string of the\nselected dimension, and its bit value ( 0or1) partitions the\ndata at the node into two child nodes, and (2) Each leaf node\nrepresents a subspace, and the sequence of bit string from the\nroot to the leaf node represents the BMP for the subspace.\nFurther, we prove that the Piecewise SFC modeled by the\nBMTree maintains two desirable properties: Monotonicity [25]\nand Injection. Monotonicity is a desirable property for design-\ning window query algorithms, which guarantees that the SFC\nvalues of data points in a query rectangle fall in the SFC value\nrange formed by two boundary points of the query rectangle.\nCombining different SFCs from different subspaces to obtain afinal SFC for the whole space may lead to the risk of breaking\nthe monotonicity property. Similarly, it may also lead to an\ninjection violation, i.e., that the mapping function may not\nreturn a unique mapped value for each input. We construct\nthe BMTree in a principled way such that the two properties\nare guaranteed.\nB. Learning Piecewise SFCs\nTo address the limitation of heuristic algorithms in the\nSFC design, we propose to model building the BMTree as\na Markov decision process (MDP, for short) [26], aiming to\ndevelop data-driven solutions for designing suitable BMPs for\ndifferent subspaces. Specifically, we define the states, the ac-\ntions, and the rewards signals of the MDP framework to build\nthe BMTree such that the generated BMTree can optimize\nquery performance. We leverage reinforcement learning and\nMonte Carlo Tree Search (MCTS, for short) [27], to learn\na performance-aware policy and avoid local optimal settings.\nTo improve performance, we design a greedy action selection\nalgorithm for MCTS. Moreover, to improve training efficiency,\nwe define a metric termed ScanRange as a proxy of the\nquery performance (e.g., I/O cost or query latency), and apply\nScanRange for the computation of rewards.\nC. Updating Piecewise SFCs\nIn situations where the distributions of data and queries\nchange [28], the previously learned module faces an issue of\nhaving sub-optimal performance. Fully retraining a BMTree\nposes efficiency challenges due to the BMTree training cost,\nand the need to update all SFC values of data points main-\ntained in the index. To address this issue, we propose a novel\nmechanism aligned with the BMTree structure that enables\npartial retraining, and hence reducing the overall cost. First,\nwe introduce a distribution shift score to quantify the shift\ndegree, and decide if retraining is necessary. Then, we develop\nan optimization potential score to identify which nodes of\nBMTree, when optimized, can significantly enhance query\nperformance. We partially delete the nodes of the BMTree\nthat need to be retrained, and develop an adapted training\nreinforcement learning environment (with the states, actions,\nand rewards adapted for partial retraining) and regenerate\nthe BMTree with respect to the updated data and query\nworkloads.\nThe main contributions of this paper are as follows:\n(1) We propose the idea of piecewise SFCs that allows to\ndesign different BMPs for different subspaces by considering\nthe data and query workload properties to deal with non-\nuniformly distributed data and query workloads.\n(2) To design piecewise SFCs, we introduce the BMTree to\npartition the data space into subspaces, and generate a BMP for\neach subspace. We prove that the piecewise SFC represented\nby a BMTree satisfies two properties, namely injection and\nmonotonicity.\n(3) To build a BMTree, we develop an RL-based solution by\nmodeling BMP design as a MDP, and design an MCTS-based\nBMTree construction algorithm. We develop the ScanRange\nmetric to efficiently measure the window query performance\n\n3\non an SFC. As a result, the ScanRange metric speeds up the\nlearning procedure.\n(4) To efficiently update a BMTree, we develop a mechanism\nthat allows partially retraining of the BMTree when data and/or\nquery distributions shift, and enhances the query performance\nwith reasonable retraining costs.\n(5) We integrate our learned SFCs into the B+-Tree in-\ndex inside PostgreSQL and inside the learned spatial index\nRSMI [12]. Experimental results under both settings consis-\ntently show that the BMTree outperforms the baselines in\nterms of query performance. Further, the partial retraining\nmechanism achieves notable performance enhancement that is\ncompetitive to full retraining while achieving over 2√óspeedup\ncompared to full retraining.\nCompared to the previously published paper [29], this paper\nintroduces over 35% new content. We extend the BMTree by\nincorporating a novel reconstruction mechanism that enables\nit to quickly adapt itself to distribution shifts and achieves\nbetter query performance, which is not supported by other\nSFC methods. We also include additional experiments to\nevaluate the proposed mechanism under different shift settings,\nincluding data shift, query shift, and their combination.\nII. PROBLEM STATEMENT & PRELIMINARIES\nA. Problem Definition\nLetDbe a database, where each data point x‚àà D\nhasndimensions, denoted by x= (d1, d2, . . . , d n). For\nease of presentation, we consider only 2-dimensional data\npoints x= (x, y), that can be easily extended to ndi-\nmensions. xcan be converted to bit strings as: x=\n((x1x2. . . x m)2,(y1y2. . . ym)2). where each xi,yj(1‚â§\ni, j‚â§m) are 0 or 1 (i.e., xi, yj‚àà {0,1}) and mis the length\nof the bit string that is dependent on the cardinality of the\ndimensions xandy. Take x= (4,5)for example, it can be\nconverted in base 2 to x= (100 2,1012). In previous studies on\nSFC based multidimensional indexes, e.g., [1, 23, 30], values\nof data points are typically mapped to fine-grained grid cells\nfor discretization. SFC maps xinto a scalar value v(called\nSFC value) with a mapping function C(x)‚Üív. An SFC value\nvcan be used as the key value of data xto determine the order\nofxinD.\nProblem 1 (SFC Design): Given a database Dand a query\nworkload Q, we aim to develop a mapping function Tthat\nmaps each data point x‚àà D into an SFC value v, such that\nwith an index structure (e.g., a B+-Tree) built on the SFC\nvalues of that data points in D, the query performance (e.g.,\nI/O cost and querying time) on Qis optimized.\nB. Preliminaries on SFC\nWe present two desired properties for a mapping function\nT, namely Injection andMonotonicity . Then, we describe the\ncurve design methods in the Z-curve and Quilts that also\nsatisfy these properties.\nInjection1.An SFC design is expected to satisfy the injection\nproperty that guarantees a unique mapping from xtov. This\n1This property is defined on discretized input. No injection is guaranteed in\ncontinuous space since no bijection mapping exists between RandRn[31].is to ensure that each SFC value vcan be used as a key value\nofxfor ordering and indexing data. It is defined as follows.\nDefinition 1 (Injection): Given a function C:x‚Üív,\nCis injective if xmaps to a unique value v, s.t.‚àÄx1Ã∏=\nx2, C(x1)Ã∏=C(x2).\nThe injection property is desirable for an index to narrow\nthe search space for better query performance. Consider an\nextreme situation where all data points map to the same value.\nThen, an index based on the SFC values cannot narrow the\nsearch space for a query.\nMonotonicity. The monotonicity [25] is defined as follows.\nDefinition 2 (Monotonicity): Given two n-dimensional data\npoints (denoted as x‚Ä≤andx‚Ä≤‚Ä≤), whose SFC values are denoted\nbyC(x‚Ä≤)andC(x‚Ä≤‚Ä≤). When a mapping function Cholds\nmonotonicity, if d‚Ä≤\ni‚â•d‚Ä≤‚Ä≤\niis satisfied for ‚àÄi‚àà[1, n], it always\nhasC(x‚Ä≤)‚â•C(x‚Ä≤‚Ä≤).\nMaintaining monotonicity is a desirable property for map-\nping data points to SFC values as explained below. As-\nsuming the origin of the space is at the lower left, given\na 2-dimensional window query represented by its minimum\n(bottom-left corner) and maximum (top-right corner) points\n(i.e., qmin = (xmin, ymin),qmax = (xmax, ymax)). Let\nP={(x, y)|xmin‚â§x‚â§xmax, ymin‚â§y‚â§ymax}\ndenotes the query results bounded by the query window. If the\nmonotonicity property holds, the result points in Pare within\nthe range bounded by the SFC values of qminandqmax. The\nreason is that for any data point p‚àà P, whose SFC value C(p)\nalways holds that C(qmin)‚â§C(p)‚â§C(qmax). The property\nis desirable since it enables us to design simple and efficient\nalgorithms for processing a window query by checking data\npoints whose SFC values are within the bounded range only;\nOtherwise, the algorithm does not work. For example, the\nHilbert curve and its variants [19, 20, 21] do not satisfy the\nmonotonicity property, which makes it hard to identify the\nscanning range for a window query in the space of their SFC\nvalues, and requires maintaining additional structure to design\nmore complicated algorithms [32].\nComputing SFC values in the Z-curve [16, 17, 18] and\nin QUILTS [24]. Both Z-curve and QUILTS guarantee the\ninjection and monotonicity properties. Figure 1 examplifies\nhow the Z-curve and QUILTS map a data point xto a scalar\nSFC value v. The curve design in the Z-curve and QUILTS\nare presented as follows.\nThe SFC value of xin the Z-curve is computed via bit\ninterleaving, which generates a binary number consisting of\nbits (0 or 1) filled alternatively from each dimension‚Äôs bit\nstring. The Z-curve value of a 2-dimensional data point xis\ncomputed by Function Cz:\nCz(x) = (x1y1x2y2. . . x mym)2 (1)\nIt assumes that all dimensions have the same bit-string length,\nand the zero-padding technique is usually applied to fit the\nlength equally by padding zeros at the head of each bit string.\nQUILTS generalizes the bit interleaving pattern of the Z-\ncurve to more general bit merging pattern , each of which\nrepresents a way of merging bits. For example, for two-\ndimensional data, QUILTS defines a bit merging pattern as\nfollows.\n\n4\nDefinition 3 (Bit Merging Pattern): A bit merging pattern\n(BMP) is a string Pof length 2mover the alphabet {X,Y}s.t.\nit contains exactly mX‚Äôs and mY‚Äôs. Given a BMP point P=\np1p2. . . p 2m, the SFC described by Pis defined as follows.\nWe set\nCP(x) = (b1b2. . . b 2m)2 (2)\naccording to the following rule: (1) Since Pcontains exactly\nmX‚Äôs, we let I={i1, . . . , i m}be the list of ordered indices\nsuch that pi‚Ñì=X. Then, we set bi‚Ñì=x‚Ñìfor1‚â§‚Ñì‚â§m. (2)\nSimilarly, for the value of y, we consider J={j1, . . . , j m}\nwhere pj‚Ñì=Y, and assign bj‚Ñìthe bit value of y‚Ñì. For example,\ngiven the BMP point P=XXYY , the value of data point x\ncomputed by CPisCP(x) = (x1x2y1y2)2. Notice that both x\nandyare subsequences of TP(x).\nSFCs represented with different BMPs form an SFCs set.\nQUILTS considers this set and selects the optimal SFC eval-\nuated on a given query workload as the output curve. We\nprove the monotonicity of SFCs with BMPs, which guarantees\nthe monotonicity property of our method in Section VII. The\ndetailed proof is given in [29].\nLemma 1 (Monotonicity of SFCs with BMPs): An SFC with\na BMP achieves the monotonicity property.\nIII. M OTIVATION AND METHOD OVERVIEW\nA. Motivations and Challenges\nMotivation 1: Piecewise SFC Design. QUILTS and earlier\nSFCs based on BMPs only use one BMP to compute SFC\nvalues for all data points, which may not perform well for\nquery processing.\nExample 1: Figure 2 shows a 4√ó4grid space, where the\ngreen and yellow dashed rectangles represent two window\nqueries Q1(horizontal) and Q2(vertical), respectively. The\nred lines represent the ordering of grid cells w.r.t. three\nSFCs. For example, consider SFC-1, whose P1=XYYX and\nthe computed value for input x= (( x1x2)2,(y1y2)2)is\nCP1(x) = ( x1y1y2x2)2. Notice that in SFC-1, x1is put as\nthe first bit in the combined bit string, and thus any data point\nwithx1= 0 (that resides in the left half of Figure 2 (a)) will\nhave a smaller mapped value than any data point with x1= 1\n(that resides in the right side of Figure 2(a)). We label the grid\nids based on the mapped values of grid cells computed by the\nSFC curves. As discussed in Section II-B, a typical algorithm\nfirst locates the grid ids on the minimum (bottom-left corner)\nand the maximum (top-right corner) points of a query window.\nDifferent SFCs will result in accesses of different grid\ncells for answering the two window queries Q1andQ2. For\ninstance, with SFC-1, Q1andQ2need 2 and 3 grid scans,\nrespectively. With SFC-2 ( P2=XYXY ),Q1andQ2need 3\nand 2 grid scans, respectively. Detailed computation can be\nfound in [29]. Further, with SFC-1, cells in window of Q1are\nconsecutive (cell 7and8form a contiguous sequence, noted as\n1runin [19]), while cell 13and15inQ2are not consecutive\n(2 runs). Query with 1run means a contiguous memory access\nis available, which is preferred.\nIn the example, SFC-1 performs better for Q1while SFC-2\nis better for Q2. A natural idea is whether we can combine\nthe advantages from the two BMPs of SFC-1 and SFC-2, i.e.,we use XYYX to organize the data at the left hand side and\nXYXY to organize the data at the right-hand side. The design\nwill result in a piecewise SFC, shown as SFC-3 in Figure 2(c).\nWith SFC-3, we need 2 grid scans for both Q1andQ2. This\nexample motivates the need for designing a piecewise SFC.\n12345678910111213141516ùëÑ!ùëÑ\"\n12345678910111213141516ùëÑ!ùëÑ\"\n12345678910111213141516ùëÑ!ùëÑ\"\n(a) SFC-1 ( XYYX ) (b) SFC-2 ( XYXY ) (c) SFC-3 (piecewise)\nGrid scans: 2forQ1, Grid scans: 3forQ1, Grid scans: 2forQ1,\n3forQ2 2forQ2 2forQ2\nFig. 2: Motivation for piecewise SFCs, SFC-1 is described\nby the BMP XYYX while SFC-2 by XYXY . In contrast, SFC-3\n(ours) is described by two BMPs: left by XYYX and right by\nXYYX , where the green shade highlights the scanned grids.\nMotivation 2: Learning-based Method for Piecewise SFC\nConstruction. Classic SFCs (Z-curve, Hilbert curve, etc.) are\nbased on a single scheme and fail to utilize the underlying\ndatabase instance to design the SFC. In contrast, QUILTS\nproposes to utilize the given database and query workload\nto evaluate and select an SFC from an SFC set in which\neach SFC is described by a BMP. However, QUILTS does not\ndirectly evaluate SFC w.r.t. query performance but uses instead\nheuristic rules to generate candidate SFCs. The heuristic rules\nwill select BMPs such that the resulting grid cells intersecting\nwith a query would be continuous in the curve order, and hence\nresults in fewer grid scans. These heuristics only work for\nquery workloads containing limited types of window queries\n(e.g., with the same aspect ratio), and are not effective under\ngeneral situations (where more than one query type with\ndifferent aspect ratios and region areas exist). Due to these\nlimitations, it calls for more principled solutions to utilize\ndatabase and query workloads for generating and selecting\nan SFC. Learning-based methods would be promising for this\npurpose.\nMotivation 3: Efficient Piecewise SFC Update. The distri-\nbution shift issue exists when maintaining data-driven learned\nindexes. When distribution shift happens, the performance of\nthe learned modules can become suboptimal. A retraining\nprocedure is preferred if the performance decreases to a\ncertain degree. However, retraining the BMTree from scratch\ncan be costly. Moreover, in cases where distribution shifts\noccur unevenly across subspaces (e.g., some subspaces with\nsignificant distribution shifts while others with mild shifts), not\nall BMPs require redesign. Two examples are given in Fig. 3\nto illustrate the uneven shifts of data and query, respectively.\nIn Fig. 3a, the data located in the left half of the space\nshifts from being uniformly distributed to being non-uniformly\ndistributed. In Fig. 3b, the query located in the left half of\nthe space has shifted not only the spatial distribution of the\nqueries (considering the center point of each query rectangle)\nbut also the categories of the queries (from Type 1 to Type 2\n\n5\nwith different aspect ratios). These distribution shifts can lead\nto a decrease in performance of the SFC learned for the case\nof historical data and query distributions. The issue calls for a\nsolution to provide an efficient way to update the BMTree w.r.t.\ndistribution shifts. Additionally, it is preferable if the solution\nallows for the partial redesign of the BMTree while keeping\na portion unchanged so that only the data points located in\nthe retrained subspaces need to update their SFC values to\nmaintain the corresponding indexes.\nData ShiftNon-uniformly distributed\n(a) Illustration of a data shift.\nQuery Shift: Query type 1: Query type 2 (b) Illustration of a query shift.\nFig. 3: Examples of distribution shifts of both data and query\nworkloads over different subspaces.\nChallenges. Piecewise SFC design gives rise to three chal-\nlenges as discussed in the introduction section. (1) How to\npartition the space and design an effective BMP for each\nsubspace? The piecewise SFC design needs to consider both\nspace partitioning and BMP generation. (2) How to design\npiecewise SFCs such that the two desirable properties, namely,\nmonotonicity and injection, still hold? (3) How to design a\ndata-driven approach to automatically build the BMTree, given\na database and query workloads? (4) How to identify the\nappropriate subspace for the partial retraining of the piecewise\nSFC without compromising its properties?\nB. Overview of the Proposed BMTree\nThe Bit Merging Tree (BMTree) for Piecewise SFC Design.\nTo address the first challenge, we propose a novel way of\nseamlessly integrating subspace partitioning and BMP gener-\nation by constructing the BMTree; a binary tree that models a\npiecewise SFC. Each node of the BMTree is filled with a bit\nfrom a dimension. The filled bit partitions the space into two\nsubspaces corresponding to two child nodes. The left branch\nis the subspace where data points have a bit value of 0 and\nthe right branch with 1. The BMTtree partitions the whole\ndata space into subspaces, each corresponding to a leaf node\nwith its BMP being the concatenated bit sequence from the\nroot to the leaf node. We present the BMTree structure in\nSection IV. Furthermore, the BMTree mechanism guarantees\nthat the generated piecewise SFC satisfies the two properties\nthat address the second challenge. We prove that the piecewise\nSFC represented by a BMTree satisfies both monotonicity and\ninjection in Section VII.\nRL-based Algorithm for Constructing a BMTree . To\naddress the third challenge, we design a learning-based method\nthat learns from data and query workloads to build the\nBMTree. We model the building of the BMTree as a Markov\nDecision Process [26]. The process of building a BMTree\ncomprises a sequence of actions to select bits for tree nodes\nby following a top-down order. To learn an effective policyfor building the BMTree, we propose a new approach for\nintegrating a greedy policy into the Monte Carlo Tree Search\n(MCTS) framework [27]. Specifically, we develop a greedy\npolicy that selects an action to fill a bit for each node during\ntree construction. For each node, the greedy policy chooses\nthe bit that achieves the most significant reward among all\nthe candidate bits. Afterwards, we apply the greedy policy as\na guidance policy and use MCTS to optimize the BMTree\nwith the objective of providing good query performance and\navoiding local optima. Moreover, we introduce a fast comput-\ning metric, termed ScanRange, to speedup reward generation.\nWe present the proposed solution in Section V and its time\ncomplexity analysis in Section VII.\nPartially Retraining a BMTree for Piecewise SFC Update.\nTo address the last challenge, we develop a mechanism to\nefficiently update the BMTree. First, we propose a principal\nway that measures the shift of the subspace modeled by\neach BMTree node on query and data. A shift score mea-\nsuring the distribution shift degree is introduced based on\nJensen‚ÄìShannon (JS) divergence [33] that is a widely used\ntool for measuring the similarity between distributions. Then,\nthe mechanism detects the nodes with largest performance\noptimization potentiality. Then, we partially delete the nodes\nof the BMTree that need to be retrained, and apply an adapted\nRL framework to regenerate the deleted BMTree parts with\nrespect to the updated database scenario. The structure of the\nBMTree ensures that the regenerated piecewise SFC retrains\nall desired properties, since the regenerated BMTree naturally\nmodels a piecewise SFC properly. We present the details of\nthe retraining mechanism in Section VI.\nIV. D ESIGNING PIECEWISE SFC:\nBITMERGING TREE (BMT REE)\nWe present how to develop a piecewise SFC, modeled by\nthe Bit Merging Tree (BMTree) that is a binary tree.\nDesigning a BMP. To design a BMP P, we need to decide\nwhich character ( XorYin the two-dimensional case) is filled\nin each position of P. A left-to-right design procedure decides\nthe filling characters in the order from p1top2m. The key to\nBMP design is to have a policy deciding which dimension ( X\norY) to fill into each position of P.\nDesigning piecewise SFC with multiple BMPs. Next, we\ndiscuss piecewise SFC design. As discussed in Section III-A,\none challenge in designing a piecewise SFC is: How to handle\ntwo subtasks that are intertwined together, namely subspace\npartitioning and BMP design within each subspace? It is also\nchallenging to guarantee that the piecewise SFC comprising\ndifferent BMPs for different subspaces still satisfies both the\ninjection and monotonicity properties. To address these chal-\nlenges, we introduce a new solution to simultaneously generate\nthe subspaces and design the BMPs for these subspaces.\nWe follow a left-to-right BMP design approach, and start\nwith an empty string P. For example, if we fill Xin the first\nposition of P, Bit x1will be filled to the b1th position of P;\nThen, the whole data space is partitioned into two subspaces\nw.r.t. the value of Bit x1, where one subspace corresponds to\nx1= 0and the other corresponds to x1= 1. This partitioning\n\n6\n(01)(01)00112ùë£ùêö=0011\"P#=XYXY(10)(01)10012ùë£ùêõ=1001\"P\"=XXYYùëÜ#ùëÜ\"ùêö=(01\",01\")ùêõ=(10\",01\")ùë•#=0ùë•#=1\n(a) A piecewise SFC.\nùë•!ùë¶!ùë•\"ùë•\"ùë•\"ùë¶!ùë¶!ùë¶\"ùë¶\"ùë¶\"ùë¶\"ùë¶\"ùë¶\"ùë¶\"ùë¶\"ùêöùêõ0010011\"1001\"P!=XYXYP\"=XXYY11001 (b) The BMTree.\nFig. 4: (a) An example of a piecewise SFC that comprises two\nBMPs P1andP2for computing values of Data Points aand\nb. (b) A BMTree that combines the two BMPs.\nenables us to separately design different BMPs for the two\nsubspaces. Notice that the BMPs for each subspace will share\nXas the first character, but can have distinct filling choices\nfor the next 2m‚àí1characters. By recursively repeating this\noperation, we fill in the subsequent characters for each BMP\nfor each subspace, thus generating multiple subspaces each\nwith a different BMP. One advantage of this approach is\nthat it seamlessly integrates subspace partitioning with BMP\ngeneration.\nExample 2: Figure 4a gives an example of a piecewise\nSFC, where Dimensions xandyare bit strings of Length 2.\nFirst, Xis selected, and then the whole space is partitioned\nw.r.t. value of Bit x1into two subspaces where Subspace\nS1corresponds to x1= 0 and Subspace S2corresponds to\nx1= 1. Next, we separately design BMPs for S1andS2,\nwhere all BMPs under S1share the first bit x1= 0 and\nBMPs under S2share the first bit x1= 1 . We generate\ntwo example BMPs: P1=XYXY forS1andP2=XXYY\nforS2. Finally, we get a piecewise SFC that comprises CP1\nforS1andCP2forS2. This piecewise SFC represents the\nfunction: C(x) ={Ô∏É(x1y1x2y2)2ifx1= 0\n(x1x2y1y2)2ifx1= 1. Therefore, if\nData Point ais located in S1, we apply CP1to compute a‚Äôs\nSFC value. Similarly, if Data Point bis inS2, we apply CP2\nto compute b‚Äôs SFC value.\nTo facilitate the process of designing piecewise SFCs, we\npropose the Bit Merging Tree (BMTree) structure that is\nused to simultaneously partition the space and to generate\nthe BMPs. Figure 4b gives the corresponding BMTree for\nthe example piecewise SFC of Figure 4a. Since the example\npiecewise SFC is developed with only 2 BMPs, the left subtree\nof the root node shares P1while the right subtree shares P2.\nNext, we present the BMTree.\nThe Bit Merging Tree (BMTree). The BMTree is a binary\ntree that models a piecewise SFC CT, and is denoted by T.\nThe depth of a BMTree Tequals the length of a BMP, and is\ndenoted by 2mfor the 2-dimensional space. Every node of T\ncorresponds to a bit of xioryi,1‚â§i‚â§m. The left (resp.\nright) child denotes the subspace with bit value 0 (resp. 1).\nEach path from the root node to a leaf node represents a BMP\nfor the subspace of the leaf node, which is the concatenation\nof all the bits of the nodes in the path from root to leaf. The\nSFC value CT(x)of a data point xis computed by traversinga path of Tas follows. We start from the root node, and for\neach traversed node, denoted by xi, ifxi= 0, we visit the left\nchild node; otherwise we visit the right child. When we reach\na leaf node, the corresponding BMP of the traversed path is\nused to compute CT(x). The green path in Figure 4b is the\npath traversed for Point athat represents BMP P1while the\nblue path traversed for brepresents P2.\nTo construct a BMTree, we develop a breadth-first construc-\ntion algorithm to assign bits to the BMTree‚Äôs nodes. Details\n(pseudo-code and the corresponding illustration) can be found\nin [29].\nV. L EARNING PIECEWISE SFC:\nMCTS- BASED BMT REE CONSTRUCTION\nIt is difficult to design heuristic methods to construct the\nBMTree to optimize the querying performance for a workload\non a database instance. This could be observed from QUILTS\nthat uses heuristic rules for a workload containing specific\ntypes of window queries only, and fails to directly optimize\nquery performance. In contrast, we propose a reinforcement\nlearning (RL) based method for learning a decision policy that\nbuilds the BMTree to optimize query performance directly.\nTo allow an RL policy to construct the BMTree, we model\nthe BMTree construction as a Markov decision process (MDP,\nfor short). Then, we design a BMTree construction framework\nwith a model-based RL method, termed Monte Carlo Tree\nSearch (MCTS, for short). Unlike traditional algorithms, e.g.,\ngreedy or A*, MCTS is an RL approach that demonstrates\nsuperior exploration-exploitation balance, mitigating the issue\nof local optimum. MCTS is well-suited for the problem at\nhand, and offers stable performance without extensive parame-\nter tuning, compared with other RL algorithms, e.g., PPO [34].\nFigure 5 gives the workflow of the MCTS-based BMTree\nconstruction framework. We define one action that RL takes to\nbe a series of bits that fill a level of nodes in the BMTree, and\nthe nodes of the next level are then generated. The action\nspace size grows exponentially with the number of nodes.\nIt becomes difficult for RL to learn a good policy with an\nenormously large action space. To address this issue, we design\na greedy action selection algorithm that helps guide MCTS to\nsearch for good actions. Moreover, we design a metric, termed\nScanRange to speed up reward computing.\nBMTree Construction as a Decision Making Process. We\nproceed to illustrate how we model BMTree construction as\na MDP, including the design of the states ,actions ,transitions\nandreward .\n‚Ä¢States . Each partially constructed BMTree structure Tis\nrepresented by a state to map each tree with its corresponding\nquery performance. The state of a BMTree is represented\nby the bits filled to the BMTree‚Äôs nodes. For example, in\nFigure 5, the current (partially constructed) BMTree‚Äôs state\nis represented as T={(1 :X),(2 :XY)}, where XandXYare\nbits filled to the nodes in Levels 1 and 2.\n‚Ä¢Actions . Consider a partially constructed BMTree Tthat\ncurrently has Nnodes to be filled. We define the actions\nas filling bits to these nodes. We aim to learn a policy that\ndecides which bit to be filled for each node. Furthermore, the\n\n7\nXYXS1:X, V1S2:Y, V2RootS4:XX, V4S3:XY, V3S5:XXYY , V5S6:XYYX, V6Rollouts on the policy tree\nS7S8S9S10S1:X, V1‚ÄôS2:Y, V2RootS4:XX, V4S3:XY, V3‚ÄôS5:XXYY , V5‚ÄôS6:XYYX, V6‚ÄôS7S8S9S10: V10(4) backpropagationXYXXYYXMCTS Action Selection for Constructing One Level of BMTreeInputPartially constructed BMTreeReward GeneratorRew=%SRZq,D‚àíSRT(q,D)!‚àà#Outputnode to be filledpath selectionvalue backpropagation\ncurrent stateBMTree constructed one level deeperChoose(S3) = S6: XYYX, V6‚ÄôSelect Action(1) selection and (2) expansion(3) simulationrepeat rolloutSR\nFig. 5: Workflow of Monte Carlo Tree Search-Based BMTree construction.\npolicy decides if the BMTree will split the subspace of one tree\nnode. If the policy decides to split, the tree node will generate\ntwo child nodes based on the filled bit b, and the action is\ndenoted by b(with an underline). Otherwise, the tree node\nonly generates one child node that corresponds to the same\nsubspace as its parent, the action is denoted by b. During the\nconstruction, the policy will assign bits to all Nnodes. The\naction is represented by A={a1, . . . , a N}, ai= (bi, spi),\nwhere bidenotes the bit for filling Node ni, and spidenotes\nwhether to split the subspace. Given Twith Nnodes to be\nfilled, the action space size is (2n)Nwhere nis the dimension\nnumber, and a factor of 2 comes from the decision of whether\nor not to split the subspace.\n‚Ä¢Transition . With the selected action Afor the unfilled nodes\ninT, the framework will construct Tbased on A. The transition\nis from the current partially constructed BMTree Tto the\nnewly constructed tree T‚Ä≤, denoted by T‚Ä≤‚ÜêTransition (T, A).\nIn our framework, we start from an empty tree, and construct\nthe BMTree level at a time during the decision process. In each\niteration, the action generated by the policy will fill one level\nof BMTree nodes (starting from Level 1), and will generate\nnodes one level deeper.\n‚Ä¢Rewards Design . After Tevolves into T‚Ä≤, we design the\nreward that reflects the expected query performance of T‚Ä≤\nto evaluate the goodness of Action A. One might consider\nexecuting queries using the corresponding BMTree to see\nhow well the SFC helps decrease I/O cost. However, this\nis time-consuming. Thus, we propose a metric, that we term\nScanRange (SR) that reflects the performance of executing a\nwindow query, and can be computed efficiently. We construct\nthe reward based on T‚Ä≤‚Äôs SR. We define the function SR T(q,D)\nas taking a query qand a dataset Das input, and outputs the\nScanRange of qoverD.\nEfficient Reward Computing. SRis computed as follows.\nGiven a BMTree T, we randomly sample data points from\nDwith a sampling rate rs. Then, the sampled data points\nare sorted according to their SFC values. To compute the\nSFC values on a partially constructed BMTree, we apply a\npolicy extended from the Z-curve to the unfilled portions of\nthe BMP in each subspace. Sorted data points are then evenly\npartitioned intors|D|\n|B|blocks, where |B|denotes the number\nof points per block. For a given Window Query qrepresentedby its minimum point qmin and maximum point qmax, we\ncalculate the SFC value of the minimum (resp. maximum)\npoint as vmin=CT(qmin)(resp. vmax =CT(qmax)). We\ndenote the blocks that vminandvmax fall into by IDminand\nIDmax, respectively. We calculate q‚ÄôsSRgiven TandDas\nSR T(q,D) =IDmax‚àíIDmin. The calculation of SR is way\ncheaper than actually evaluating Query q.\nWe develop a reward generator based on the evaluated SR.\nWe take the performance of the Z-curve as a baseline. Given\nthe dataset Dand a query workload Q. The generator sorts\nthe data points based on their SFC values, and computes the\nreward as:\nRew =‚àëÔ∏Ç\nq‚ààQ(SRZ(q,D)‚àíSRT(q,D)) (3)\nIntuitively, the reward is positive if the BMTree constructed\nby the policy achieves a lower SR than the Z-curve. This\ndesign allows the agent to assess the actual performance of the\nconstructed BMTree compared to Z-curve. Empirical studies\nshow that this choice helps the agent efficiently identify good\nactions. We normalize the reward by dividing the reward of\nthe Z-curve.\nExample 3: Refer to Figure 5. The partially constructed\nBMTree is represented by the bits filled to different levels,\ndenoted by T={(1 :X),(2 :XY)}, where each tuple is the bits\nfilled in the corresponding BMTree level. The learned policy\nselects the action A=XYYX. The next level of the BMTree is\nconstructed based on A. The reward signal is computed based\non the performance of the BMTree‚Äôs newly added level. The\nBMtree will continue to be provided as input for constructing\nits next level.\nWe proceed to present the proposed MCTS framework, includ-\ning a BMTree Tunder construction, a policy tree that helps\nto decide the action and gets updated gradually, and a reward\ngenerator that generates the reward based on T.\nPolicy Tree. MCTS [27, 35] is a model-based RL method.\nThe high-level idea of MCTS is to search in a tree structure,\nwhere each node of the tree structure denotes a state. Given\nthe current state, the objective of MCTS is to find the optimal\nchild node (i.e., the next state) that potentially achieves an\noptimal reward. The structure that records and updates the\nhistorical states and their associated rewards is named policy\n\n8\ntree[35], where each child node denotes a next possible action\n(the design choice for the next level of the BMTree), and we\ndefine it as follows:\nDefinition 4 (Policy Tree): The policy tree is a tree structure\nthat models the environment. Each node of the policy tree cor-\nresponds to a state, representing a current partially constructed\nBMTree. Moreover, every node stores: (1) Action Athat\nconstructs one more level of the BMTree, and (2) A reward\nvalue that reflects the goodness for choosing a node. The root\nnode of the policy tree corresponds to an empty BMTree, and\neach path of the policy tree from the root node to the leaf\nnode corresponds to a decision procedure of constructing a\nBMTree. The middle section of Fig. 5 illustrates an example\nof policy tree.\nRollouts. To choose an action, MCTS checks the reward that\ndifferent action choices can achieve. To achieve this, MCTS\nwill make several attempts in which it simulates several paths\nin the policy tree, and then checks if the attempted path\nresults in better performance. Then, MCTS updates the policy\ntree based on the simulations, referred to as rollout in [27],\nindicating the operation that involves repeatedly selecting\ndifferent actions and ultimately choosing the optimal one.\nA rollout consists of four phases: (1) Selection that selects\nthe attempted path corresponding to a BMTree construction\nprocedure, (2) Expansion that adds the unobserved state node\nto the policy tree, (3) Simulation that tests the selection‚Äôs query\nperformance, and (4) Backpropagation that updates the reward\nvalue. We proceed to present the design of each of these four\nsteps.\n(1)Selection . The selection step aims to select a path in\nthe policy tree that potentially achieves good performance.\nStarting from the current state Stwith the initialized path:\nPath ={St}, we first check if all child nodes have been\nobserved in the previous rollouts. If there are unobserved\nnodes, we choose one of them and add it to the path.\nOtherwise, we apply the Upper Confidence bounds applied to\nTrees (UCT) action selection algorithm [36] to select a child\nnode that balances exploration vs. exploitation. Specifically,\nUCT selects the child node with the maximum value vuct=\nVt+1\nnum (St+1)+c¬∑‚àöÔ∏Ç\nln(num (St))\nnum (St+1).St+1=Transition (St, A),\nwhere Vt+1is the reward value of the child node St+1evolving\nfrom Stby Action A;num(St+1)andnum(St)denote the\ntimes of observing Nodes St+1andSt, respectively, during\nrollouts; cis a factor defaulted by 1. Then, the selected node\nSt+1is added to the path: Path ={St‚ÜíSt+1}. The selection\nstep continues until the last node of Path is an unobserved\nnode (the policy does not know the expected value of this\nnode). Then, it returns the Path for the next step.\n(2)Expansion . In the expansion step, the unobserved nodes in\nPath are added to the policy tree. The number of observations\nof each node StinPath , denoted as num(St), is incremented\nby1. Then, this value is used to compute the average reward.\n(3)Simulation . We simulate the performance of the selected\nPath by constructing the BMTree based on the actions stored\nin the nodes of the path. Then, the constructed BMTree is\ninput to the reward generator to compute the tree‚Äôs SR metric.\n(4)Backpropagation . In this step, we update the value of eachnode in Path . We apply the maximum value update rule that\nupdates the value of a state Stwith the maximum reward\nit gains from simulation, computed by V‚Ä≤\nt= max( Vt, Rew ),\nwhere Vtis State St‚Äôs old value, Rew is the reward gained\nduring the simulation, and V‚Ä≤\ntis the updated value.\nExample 4: Refer to Figure 5. State S3corresponds to the\ninput partially constructed BMTree. During the rollouts, we\nselect Path {S3‚ÜíS6‚ÜíS10}in the selection step. Then,\nit expands the new observed State S8to the policy tree in\nthe expansion step. We construct the BMTree based on the\nselected path and compute SR. In the backpropagation step,\nthe values of S3,S6andS10are updated whose values are\nlisted in red, based on the computed SR in S10.\nAfter the rollouts procedure, the algorithm selects the action\nwith the highest reward, and then BMTree Tis constructed\naccordingly. In the example, S6is selected with the largest\nvalue V‚Ä≤\n6compared to the other child nodes. Then, it returns\nAction XYYXto build the BMTree one level deeper.\nGreedy Action Selection. We design the greedy action se-\nlection algorithm (GAS, for short) for the selection step in\nrollouts for MCTS to find a good action for a partially\nconstructed BMTree. Given TwithNnodes to be filled, GAS\ngenerates an action Agby greedily assigning a bit to each\nBMTree node that achieves the minimum SR compared to\nother bits when Tis filled with that bit.\nTo summarize, the MCTS-based BMTree construction has\ntwo steps: BMTree initialization and MCTS-based BMTree\nlearning. Its detailed pseudo-code can be found in [29].\nVI. U PDATING PIECEWISE SFC\nIn this section, we proceed to handle BMTree maintenance.\nAs described in Section III-A, with the change in distribution\nof the data and/or query workload, the performance of the\nBMTree is no longer optimal. Retraining the whole BMTree\nfrom scratch would be inefficient and would consume re-\nsources. To achieve efficient piecewise SFC update, we design\na mechanism that partially retrains a BMTree based on the pre-\ntrained BMTree instead of fully retraining a BMTree from\nscratch, while notably improving query performance under\nthe new data and query distributions. We proceed to intro-\nduce: (1) Measurement of the degree of distribution shift that\ndetermines whether the BMTree nodes should be retrained;\n(2) Detection of which BMTree nodes to be retrained that iden-\ntifies the nodes to be retrained for an optimal effectiveness-\nefficiency trade-off; and (3) Partial BMTree retraining that\nenables partial retraining of the selected BMTree nodes while\nmaintaining the rest of the BMTree unchanged.\nA. Assessment of the Distribution Shift\nPartial retraining of a piecewise SFC requires retraining a\nportion of the piecewise SFC while maintaining the overall\nstructure still a piecewise SFC. Furthermore, detecting the\nsubspace that could improve query performance after being\ntrained is non-trivial. To address these issues, we follow the\npre-designed BMTree structure modeling, where we split the\ndomain w.r.t. the structure of the BMTree. As in Section IV,\ndifferent nodes of BMTree represent different subspaces. To\n\n9\nXYXXYYX|ùíü!|:2210127366XYXXYYX|ùíü\"|:2211115683[7, 3, 6, 6][5, 6, 8, 3]Data shift score function Data drift score ùë†‚Ñéùëñùëìùë°#\n(a) Measuring data shift\nXYXXYYXùí¨!: {ùëû\", ùëû#}ùëû\"ùëû#ùëû\"ùëû#XYXXYYXùí¨$: {ùëû\", ùëû#, ùëû%}[ùëû#, ùëû%]ùëû\"ùëû\"{[ùëû\"], [], [ùëû\"], []}{[], [ùëû#, ùëû%], [ùëû\"], []}Query shift score functionOutput query drift score ùë†‚Ñéùëñùëìùë°&[ùëû#, ùëû%] (b) Measuring query shift.\nFig. 6: Measuring distribution shifts within the BMTree.\nachieve effective and efficient detection of the retraining\nsubspace, we model the degree by which data and query\ndistributions drift within the BMTree structure as follows.\nLetNbe a BMTree node that represents a subspace of the\nwhole data space domain. Suppose an action is applied to N,\nandNis split into the two child nodes N1andN2, where each\nchild node denotes half of the original subspace. With actions\nfurther assigned to N1andN2, the grandchild nodes of Nsplit\nfrom N1andN2(4 grandchild nodes if N1andN2are all split)\ndenote more fine-grained subspaces, respectively. We measure\nthe distribution difference of the subspace denoted by Nbefore\nand after the data or query is updated. We model the data and\nquery distribution shifts of Node Nas follows:\nModeling Data Shift. Suppose the split level is set to 2that\ndenotes that the data shift of a BMTree node Nis computed\nw.r.t. the granchild nodes 2levels deeper than N(4 grandchild\nnodes by default). The historical and updated datasets are\ndenoted as DoandDu, respectively. Refer to Fig. 6a. The data\npoints are split w.r.t. the nodes. On the left side (resp. right\nside) of the figure, the Do(resp.Du) is split into four parts\nat Level 2in the subtree, and we represent the distribution\nasld\no= [7\n22,3\n22,6\n22,6\n22](resp. ld\nu= [5\n22,6\n22,8\n22,3\n22]), where\nthe distribution is normalized by the cardinality of the dataset.\nNote that the representation of the data will change if the\naction assigned to the sub-tree is different. Then, we apply\nthe Jensen‚ÄìShannon (JS) divergence [33] to measure the data\nshift, defined as follows:\nshift d‚âúDJS(Ô∏Å\nld\no‚Éì‚Éì‚Éì‚Éìld\nu)Ô∏Å\n=1\n2(Ô∏Ç\nDKL(Ô∏Å\nld\no‚Éì‚Éì‚Éì‚Éìld\nmix)Ô∏Å\n+DKL(Ô∏Å\nld\nu‚Éì‚Éì‚Éì‚Éìld\nmix)Ô∏Å)Ô∏Ç\n,\n(4)\nwhere DKLdenotes the Kullback‚ÄìLeibler (KL) divergence\nfunction, defined as DKL(ld\no‚Éì‚Éì‚Éì‚Éìld\nu) =‚àëÔ∏Å\nild\no[i]¬∑log(Ô∏Å\nld\no[i]/ld\nu[i])Ô∏Å\n.\nld\nmix denotes the mixed distribution of ld\noandld\nu:ld\nmix =\n1\n2(ld\no+ld\nu). The larger the KL divergence DJS(ld\no‚Éì‚Éì‚Éì‚Éìld\nu)is, the\ngreater the difference between DoandDu.\nModeling Query Shift. For the query shift, we split the\nquery set w.r.t. the BMTree nodes. Specifically, we compute\nthe center point of each query, and divide the query set\naccording to the center point, e.g., if the query is denoted by:\n(xmin, ymin, xmax, ymax), the center point is computed as:\n(xmin+xmax\n2,ymin+ymax\n2). Then, according to the grandchild\nnodes of N, the queries are split into 4 parts. Refer to Fig. 6b.\nOn the left side of the figure, the old queryset Qo={q1, q2}\nis split into a list of four subsets lq\no={{q1},{},{q2},{}},\nwhile on right side the updated queryset Qu={q1, q2, q3}\nis split as lq\nu={{},{q2, q3},{q1},{}}. Unlike the measureof data shift that directly compares the number of data points\nof each list element, we cluster the queries in each node into\ndifferent clusters w.r.t. the area and the aspect ratio, and thus\ncompute the JS divergence of each list element. Then, the JS\ndivergence is averaged across the list elements:\nshift q‚âú1\n|lq\no|‚àëÔ∏Ç\niDJS(Ô∏Å\nlq\no[i]‚Éì‚Éì‚Éì‚Éìlq\nu[i])Ô∏Å\n(5)\nAfter modeling the distribution shifts of both data and\nqueries, next we introduce how to decide the subspaces to be\nretrained. When the retraining procedure begins, the method\nwill recursively compute the data and query shifts of nodes in\nthe BMTree Tunder a Breadth-First Search (BFS) order. We\nrestrict the distribution shift computation to a limited depth of\nnodes in T, since the nodes with larger depth represent small\ndata subspaces, and will contribute limited improvement in\nperformance. When both data and query are shifted, the shift\nscores of data and query are weight summed as the final shift\nscore: shift m=Œ±¬∑shift d+ (1‚àíŒ±)¬∑shift q, where Œ±is\nthe weight parameter and is set by default to 0.5. Then, the\nnodes to be retrained are filtered based on this score. A shift\nthreshold Œ∏sis set to filter the BMTree nodes. Nodes with shift\nscore lower than Œ∏sare not retrained.\nB. Deciding Which BMTree Nodes to Partially Retrain\nObserve that the performance optimization potential of each\nnode does not solely depend on the distribution shift degree.\nInstead, we propose to introduce a score based on the change\nin average ScanRange ( SR T) before and after the data and/or\nqueries change, to measure fast the possible optimization\npotentials when retraining a node. The optimization potentials\nscore OPon Node Nis computed based on SRas follows:\nOP(N,Do,Du,Q‚Ä≤\no,Q‚Ä≤\nu,T) = avg\nqu‚ààQ‚Ä≤uSR T(qu,Du)\n‚àíavg\nqo‚ààQ‚Ä≤oSR T(qo,Do),(6)\nwhere Q‚Ä≤\no(resp. Q‚Ä≤\nu) denotes the subset of historical (resp.\nupdated) query workloads that the BMTree node Ncontains.\navg\nqu‚ààQ‚Ä≤uSR T(qu)and avg\nqo‚ààQ‚Ä≤oSR T(qo)denote the average SRof\nQ‚Ä≤\noandQ‚Ä≤\nuon BMTree T, respectively. Then, we compare the\nfiltered BMTree nodes level at a time, and select the nodes\nwith maximum OPscore as the node to be retrained.\nDuring retraining, to ensure a certain degree of efficiency\nimprovement, a retraining constraint ratio Rrcis set to limit\nthe retraining area of the retrained subspaces compared to\na full retrain (e.g., if r= 0.5, the accumulated area of the\nretrained subspaces should not reach half the whole space).\nThe algorithm for detecting the BMTree nodes that need\nretraining is Listed in Algorithm 1. First, it initializes a queue\nNwith the root node of T(Line 1). Then, the algorithm\nprocesses level at a time of the BMTree (Lines 3 ‚Äì 14). The\nleftmost node of Nis popped (Line 4), then the shift score\nsis computed by the ShiftScore function (with information\nabout the dataset update and the BMTree given, as described\nbefore) (Line 5). Then, the nodes satisfying the threshold Œ∏are\nadded to L, and the OPofNis computed w.r.t. Eq. 6 (Lines 6\n\n10\nAlgorithm 1: Deciding on Which BMTree Nodes to\nRetrain.\ninput : n-dimensional old and updated dataset Do,Du, old\nand updated training workload Qo,Qu, BMTree T;\noutput : Retrain nodes of T;\n1Initialize L,R ‚Üê ‚àÖ ,‚àÖ;\n2N ‚Üê { T.root};\n3while (N Ã∏=‚àÖ)‚àß(N[0].depth < d m)do\n4 N‚Üê N .pop(0);\n5 s‚ÜêShiftScore (N,Do,Du,Qo,Qu,T);\n6 if(s‚â•Œ∏)then\n7 L.append(Ô∏Å\n(OPN,N))Ô∏Å\n;\n8N.append( N.child nodes );\n9 ifN[0].depth > N.depth then\n10 L.sort (OPN);\n11 for(OPN,N)‚àà L do\n12 if(SpaceRatio (R+N)< R rc)then\n13 R.append( N);\n14 L ‚Üê ‚àÖ\n15return R\n‚Äì 7). If a level of BMTree is evaluated (Line 9), the algorithm\nsortsLw.r.t. OPN(Line 10). Then, if the nodes with greater\nOSscore satisfy the retraining constraint ratio Rrc(Line 12),\nthey are added to the retraining nodes list (Line 13).\nC. BMTree Reconstruction and Retraining\nWe proceed to introduce the BMTree reconstruction and\nretraining procedures. First, we initialize the BMTree w.r.t. the\npre-trained BMTree and the BMTree nodes needing retraining\nthat have resulted from the retraining detection procedure.\nWhen the retrain domain (i.e., the to-be-retrained BMTree\nnodes) is decided, how to retrain the piecewise SFC while\nmaintaining the rest of the designed piecewise SFC portion\nunchanged is non-trivial. Revisiting the design of seamless\npartitioning and BMP generation introduced in Section IV,\nwe propose to partially maintain the BMTree structure, and\nconduct the retraining procedure.\nXYXXYYXXXXXYXXXXYInitializeRetrainN!:N\":\nFig. 7: Partial BMTree retraining procedure.\nTo conduct the retraining procedure, first, we manipulate\nthe BMTree so that the partial retraining procedure could be\nfinished by regenerating the BMTree structure. As in Fig. 7,\nsuppose N1andN2are two nodes that represent the subspaces\nto be retrained. In the initialization of the retraining procedure,\nwe delete the child nodes of N1andN2as well as the actions\nassigned to N1andN2while the other nodes remain unchanged.\nThe BMTree‚Äôs unchanged portion is in the middle of Fig. 7.\nThen, we input the partially deleted BMTree Tinto an\nRL environment for retraining. We apply the MCTS method\nfor retraining. Different from the environment introduced fortraining the BMTree from scratch (Section V), here the envi-\nronment is developed to support partially training the BMTree,\nand is designed as follows:\n(1)State . We design the state of the retraining RL environ-\nment as the nodes to be retrained. In Fig. 7, N1andN2are\ninitialized in the state: S={(N1:None ),(N2:None )}, where\nNone denotes that no action has been taken yet. (2) Action .\nThen, we design the action space as the actions assigned to N1\nandN2. After the action is decided, the child nodes of N1and\nN2are generated, and the child nodes of N1andN2represent\nthe transited state. The nodes of Tin one state do not require\nto have identical depth. (3) Reward Design . We apply the\nupdated dataset and query workloads to generate the reward as\nin Section V. The RL policy training environment will produce\na regenerated BMTree noted as T‚Ä≤w.r.t. the redesigned state,\naction, and reward. Further, to improve efficiency, during the\npartial retraining procedure, we only generate reward w.r.t. the\nqueries in Quthat fall in the retrained nodes.\nD. Workflow for Partial Retraining\nWe summarize the partial retraining procedure as follows:\n(1) If there exists BMTree node satisfying the shift score filter,\nthe node to retrain detection procedure is conducted (Alg. 1).\nThen, the retraining RL environment is initialized and the\nBMTree is regenerated based on the updated data and query.\nParticularly, if the retraining result does not meet expectation\n(e.g., with optimization ratio less than 1%), the procedure will\nselect and retrain more untrained nodes as in Section VI-C.\nAlgorithm 2: BMTree Structure Partial Retraining.\ninput : n-dimensional old and updated datasets Do,Du,\nold and updated training workloads Qo,Qu,\nBMTree T;\noutput : Retrained BMTree T‚Ä≤;\n1if‚àÉN‚ààTwithshift m(N)‚â•Œ∏sthen\n2R ‚Üê Retrain Detector (T,Do,Du,Qo,Qu);\n3 S,Tp=Initial (R,T);\n4Q‚Ä≤\nu‚Üê {q|q‚àà Q u‚àß ‚àÉNs.t.Ncontains q};\n5 T‚Ä≤‚ÜêMCTS(Tp,Du,Q‚Ä≤\nu);\n6if limited optimization then Retrain T‚Ä≤;\n7return T‚Ä≤\nThe pseudo-code of the retraining procedure is given in\nAlg. 2. If there exists a BMTree node that satisfy the shift\nscore requirement (Line 1), it first detects the BMTree nodes\nto be retrained (Line 2). Then, the RL retrain environment,\nthe partially deleted BMTree Tp, and the initial state for RL\ntraining are initialized (Line 3), and the queries contained\nby the to-be-retrained BMTree nodes are selected for retrain-\ning (Line 4). The MCTS algorithm with the environment\nredesigned as above is applied to regenerate the BMTree w.r.t.\nthe updated database and the query workload (Line 5). If\nT‚Ä≤has limited performance enhancement, when T‚Ä≤optimizes\nScanRange by less than 1%improvement compared with the\noriginal T(Line 6).\nAfter retraining the BMTree nodes is complete, data is\nneeded to update the SFC values. With BMTree being partially\nretrained, only the data located in the retrained subspaces\n\n11\nTABLE I: Experiment Parameters.\nParameter Value\nData GAU UNI OSM-US TIGER\nQuery GAUSKEUNI\nSampling rate 0.01 0.025 0.05 0.075 0.1\n#Training Q 100 500 1000 1500 2000\nMax depth 1 51015 20\nshould be updated. The retraining procedure also reduces the\ncost of the following index update procedure.\nVII. A NALYSIS AND DISCUSSION\nInjection and Monotonicity. We prove that piecewise SFCs\nmodeled by the BMTree satisfy both injection and monotonic-\nity properties. The proof is detailed in [29].\nTime Complexity Analysis. We provide time complexi-\nties for SFC value computation and MCTS-based BMTree\nconstruction. The time complexity for computing the SFC\nvalue of xusing the constructed BMTree is O(M), where\nMis the length of CT(x). This complexity is comparable\nto other SFCs described by BMPs. For BMTree construc-\ntion, the complexity of the MCTS BMTree construction is\nO(M¬∑(N+|Ds|(M+ log|Ds|) +|Q|)), where Nis the\nchild node size of the policy tree, |Ds|and|Q|correspond\nto the size of the sampled data and query workloads. It takes\nat most Mactions to construct the BMTree. In each step\nof choosing an action, the selection step is bounded by the\nchild node size O(N); the simulation time corresponds to\nthe computation of ScanRange that takes O(M¬∑ |Ds|)for\nSFC value computing, O(|Ds| ¬∑log(|Ds|)to sort data, and\nO(|Q|)to compute ScanRange for each query. For BMTree\nupdate, suppose the BMTree construction complexity is noted\nasT(BMT Train ). The BMTree retraining time is bounded by\nRrc¬∑T(BMT Train )withRrcas the retraining constraint ratio,\nsince the ratio will limit the retrain nodes and the depth of the\nBMTree that needs to be generated by the MCTS algorithm.\nVIII. E VALUATION\nExperiments aim to evaluate the following: (1) The ef-\nfectiveness of BMTree‚Äôs design, including (i) Evaluating the\nproposed piecewise SFC method vs. existing SFCs when\napplied to the SFC-based indexes vs. the other indexes, (ii) The\nBMTree under different settings (e.g., scalability, dimensional-\nity, and aspect ratio), and (iii) Components of the BMTree by\nevaluating different BMTree variants. (2) The effectiveness of\npartial retraining, including (i) Evaluating the performance of\npartial retraining while varying distribution shift settings, and\n(ii) Evaluating the choice of parameters, e.g., the retraining\nconstraint and the shift score threshold study during partial\nretraining.\nA. Experimental Setup\nDataset. We conduct experiments on both synthetic and real\ndatasets. For synthetic datasets, we generate data points in\nthe two-dimensional data space with a granularity size of\n220√ó220that follow either uniform (denoted as UNI) or\nGaussian distributions (denoted as GAU) with ¬µdas the centerpoint of the space domain. Real data OSM-US contains about\n100 Million spatial objects in the U.S. extracted from Open-\nStreetMap API [37], and TIGER [38] contains 2.3 Million\nwater areas in North America cleaned by SpatialHadoop [39].\nQuery Workload. We follow [24, 12] to generate query work-\nloads. We generate various types of window queries, where\neach query type has a fixed area selected from {230,232,234}\nand a fixed aspect ratio selected from {4,1,1/4}; Each work-\nload comprises multiple query types with different combina-\ntions of areas and ratios. We generate queries with Uniform\n(UNI) and Gausian ( GAU) distributions (same as in [4, 12]),\nWe also generate a skewed workload (denoted by SKE), in\nwhich queries follow Gaussian distributions with different ¬µ\nvalues.\nIndex Structures. To evaluate the performance of the pro-\nposed piecewise SFC compared with the existing SFCs, we\nintegrate the proposed piecewise SFC and the baseline SFCs\ninto both traditional indexes and learned index structures. First,\nwe integrate the piecewise SFC (and baseline SFCs) into the\nPostgreSQL database system and a built-in B+-Tree variant\nin PostgreSQL is employed with SFC values as key values.\nSecond, we use a learned spatial index, RSMI [40], to compare\nthe performance of the piecewise SFC against the baseline\nSFCs within RSMI. The B+-Tree of PostgreSQL is a disk-\nbased index while the released implementation of RSMI [40]\nis memory based. We choose them to evaluate the performance\nof the piecewise SFC under various scenarios.\nSFC Baselines. We choose the following SFC methods as\nour baselines: (1) Z-curve [20, 30]; (2) Hilbert Curve [21];\n(3) QUILTS [24].\nEvaluation Metrics. For experiments conducted with Post-\ngreSQL, we use the I/O cost (I/O) recorded by PostgreSQL\nand the Query Latency (QL). For experiments under RSMI,\nwe report the number of node accesses of its tree structure\nand QL for a fair comparison by following [40, 41].\nParameter Settings. Table I lists the parameters used in our\nexperiments, and the default settings are in bold. We set the\nnumber of rollouts (as described in Section V) in MCTS at 10\nby default. The max depth is the depth of the BMTree built\nvia the RL model; the sampling rate (0.05 by default) is the\nrate of sampling training data for computing the ScanRange.\nEvaluation Platform. We train the BMTree with PyTorch\n1.9, Python 3.8. The experiments are conducted on an 80-core\nserver with an Intel(R) Xeon(R) Gold 6248 CPU@2.50GHz\n64.0GB RAM, no GPU resource is leveraged.\nB. Evaluation of the BMTree\n1) Effectiveness: This experiment is to compare the effec-\ntiveness of the learned piecewise SFC in query processing\nagainst the other SFCs under both the PostgreSQL and the\nRSMI environments. We also compare an SFC-based index\ncombined with the BMTree against the other indexes. For\neach experiment, we use 1000 windows queries, which are\nrandomly generated by following respective distributions for\ntraining, and another 2000 different window queries following\nthe same distribution for evaluation.\nResults on PostgreSQL. Figures 8a and 8b show the I/O\nand QL on window queries. To ensure PostgreSQL conducts\n\n12\nUNI SKE GAU UNI SKE GAU00.51√ó103\nUNI GAUI/O Cost\nZ-curve QUILTS BMTree\nUNISKEGAU02468√ó104\nOSM-USUNISKEGAU12√ó103\nTIGER\n(a) Performance of I/O Cost.\nUNI SKE GAU UNI SKE GAU012√ó104\nUNI GAUQuery Latency ( ¬µs)\nZ-curve QUILTS BMTree\nUNISKEGAU00.511.5√ó106\nOSM-USUNISKEGAU024√ó104\nTIGER (b) Performance of Query Latency.\nFig. 8: Results under PostgreSQL, where the first and the second lines under the x bar denote the query and data distribution.\nUNI SKE GAU UNI SKE GAU0123√ó102\nUNI GAUNode Access\nZ-curve Hibert QUILTS\nBMTree\nUNISKEGAU012√ó104\nOSM-USUNISKEGAU00.511.52√ó102\nTIGER\n(a) Performance of Node Access.\nUNI SKE GAU UNI SKE GAU0246√ó102\nUNI GAUQuery Latency ( ¬µs)\nZ-curve Hibert QUILTS\nBMTree\nUNISKEGAU024√ó104\nOSM-USUNISKEGAU00.511.52√ó102\nTIGER (b) Performance of Query Latency.\nFig. 9: Results using RSMI learned index structure.\nindexscan during querying, both the bitmapscan and\nseqscan in PostgreSQL are disabled. We do not include the\nHilbert curve for this experiment as the Hilbert curve requires\nadditional structure and dedicated algorithm for returning\naccurate results for window queries, and PostgreSQL does not\nsupport them for the Hilbert curve.\nObserve that the proposed BMTree consistently outperforms\nthe baselines in all the combinations of data and query\ndistributions in terms of both I/O and QL. Between the two\nbaselines, QUILTS performs worse for the SKE workload, and\nperforms similarly as the Z-curve for UNI andGAU workloads.\nThe reason is that our query workload contains queries with\ndifferent aspect ratios (e.g., 4and1/4), rather than queries\nwith similar aspect ratios as it is used in QUILTS [24].\nQUILTS can only choose queries with a particular aspect ratio\nto optimize, and this results in poor performance for queries\nwith different aspect ratios. The BMTree outperforms the Z-\ncurve by 5.2%‚Äì39.1% (resp. 7.7%‚Äì59.8%, 6.3%‚Äì29.8% and\n25.1%‚Äì77.8%) in terms of I/O on UNI (resp. GAU, OSM-US\n,and TIGER) datasets across the various workloads. The results\nin terms of QL are consistent with those of I/O. BMTree‚Äôs\nsuperior performance is because (1) The BMTree generates\npiecewise SFCs to handle distinct query distributions, and\n(2) The BMTree is equipped with effective learning to generate\nBMPs and subspaces. Notice that under the UNI workload, the\nBMTree outperforms the Z-curve by 25.1% on TIGER while it\nonly outperforms Z-curve slightly on the other three datasets.\nThis is expected: Under the UNI query workload, the BMTree\ncan only make use of the data but not the query distributions to\noptimize performance; TIGER is very skewed and the BMTree\ncan capture TIGER‚Äôs skewed data nature.\nResults on RSMI. The original RSMI [12] uses the Hilbert\ncurve, and we include it as a baseline for this experiment as\nRSMI returns approximate results for all curves. All the curves\nachieve comparable recall (99.5% or above) using RSMI‚Äôsalgorithms for window queries. Figures 9a and 9b show the\nnumber of node accesses and QL for all curves when using\nRSMI. Observe that the BMTree consistently outperforms all\nbaselines. The BMTree outperforms the Z-curve by 18.2%‚Äì\n29.0% (resp. 13.7%‚Äì28.4%, 13.5%‚Äì26.5%, and 2.8%‚Äì25.3%)\nin terms of the number of node accesses on the UNI (resp.\nGAU, US-OSM, and TIGER) datasets. Also, observe that\nthe Hilbert curve achieves similar performance to that of the\nBMTree on the GAU dataset that could be attributed to its\ngood tolerance to data skew [42].\nComparison with Other Indexes. We compare against the\nperformance of two SFC-based indexes, RSMI and ZM com-\nbining our BMTree, with baseline indexes including (1) two\nR-tree variants: STR [43] and R* Tree [44]; and (2) two\npartition-based methods: Grid-File and Quad-Tree. The results\nare given in [29] that reveal the generality of the BMTree on\nenhancing query performance combining different indexes.\nUNI GAU OSM-US TIGER050100150I/O Cost Ratio (%)\nZ-curve QUILTS BMTree\n(a) I/O CostUNI GAU OSM-US TIGER050100150Query Latency Ratio (%)\nZ-curve QUILTS BMTree\n(b) Query Latency\nFig. 10: Performance of kNN queries.\n0% 25% 50% 75% 100%200300400500600\nPercentage of kNN training queriesWindow Query I/O Cost\nZ-curve BMTree\n(a) Window Query I/O\n0% 25% 50% 75% 100%78808284\nPercentage of kNN training querieskNN Query I/O Cost\nZ-curve BMTree (b)kNN Query I/O\nFig. 11: Optimization of window query & kNN query.\n\n13\nEffect on kNN Queries. The piecewise SFC is learned to\noptimize window queries. Here, we investigate its influence\non the performance of kNN queries. We generate 1,000 kNN\nquery points following the data distribution, and we apply the\nkNN algorithm [12] in PostgreSQL with kset to 25. We\nreport the I/O and QL ratios in Figure 10a and 10b that are\nthe ratio of results of the different curves divided by the result\nof the Z-curve. The BMTree performs slightly better than the\nbaselines on GAU and OSM-US while the Z-curve is slightly\nbetter on UNI and TIGER. Thus, while the piecewise SFC is\noptimized for window queries, its kNN query performance is\nnot compromised.\nOptimizing Window and kNN queries. We evaluate the\nperformance when window queries and kNN queries are\noptimized together. To optimize the BMTree for kNN queries,\nwe convert kNN queries into window queries by following\n[12] and include them in the training workload. Then, we vary\nthe weight of the objective based on kNN queries relative to\nwindow queries from 0%to100% during training. Figures 11a\nand 11b give the window and kNN query I/Os. Observe\nthat as the weight increases, the window query I/O tends to\nincrease while the kNN query I/O tends to decrease. Also,\nobserve that when the weight is between 25% and75%, the\nperformance of the window query only mildly degrades, while\nthe performance of kNN query is better than that based on\nthe Z-curve. The results show the potential of the BMTree to\noptimize the two query types together.\n2) Effect of Varying the Settings: We evaluate the perfor-\nmance of the BMTree under various settings: dataset/query\nsize, dimensionality, and window aspect ratio. More settings\ncan be found in [29].\n0.151020 50 100 1500.10.51¬∑105\nDataset Size (million)I/O Cost\nZ-curve\nQUILTS\nBMTree\n(a) I/O Cost0.151020 50 100 15005001,0001,500\nDataset Size (million)Query Latency (s)\nZ-curve\nQUILTS\nBMTree\n(b) Query Latency\nFig. 12: Performance vs dataset size.\nScalability of the Learned SFCs. To evaluate the scalability\nof the BMTree, we evaluate the performance of the SFCs by\nvarying data size from 0.1 to 150 Million. We construct the\nBMTree using 105sampled data points as input for RL training\nand the others follow the default settings. The results are in\nFigure 12. Observe that the BMTree displays a linear trend\nfor both I/O and QL when data size increases. We observe\nsimilar trends for baselines.\nEffect of Higher Dimensionality. To evaluate the effect of\ndimensionality on the effectiveness of the learned SFC, we\nvary the dimensionality from 2 to 6 on the datasets for both\nthe uniform and normal distributions. We report the IO in\nFigure 13. The BMTree consistently outperforms the baselines,\nand saves up to 54% of the I/O cost compared to the best Z-\ncurve baseline. This demonstrates that the BMTree generalizes\nwell on data with more than two dimensions.2 3 4 5 650100150200250\nDimensionalityI/O Cost\nZ-curve QUILTS BMTree\n(a) Uniform Data2 3 4 5 650100150200250\nDimensionalityI/O Cost\nZ-curve QUILTS BMTree\n(b) Normal Data\nFig. 13: I/O cost vs dimensionality.\nEffect of Varying Query Aspect Ratio and Selectivity.\n(1) We evaluate the BMTree performance by varying query\naspect ratios from {4,1\n4}to{128,1\n128}, and the results are\nreported in Figure 14a. Observe that the BMTree performs\nconsistently better than the other SFCs across different aspect\nratios including very wide ones. (2) We vary query selectivity\nfrom 0.0001% to1%, and report the results in Figure 14b.\nObserve that the improvement of the BMTree is subtle under\nvery small query range. This is due to the fact that for small\nquery ranges, the points that are within a range are few, and\nthe index tends to perform similarly for different SFCs.\n4&1\n48&1\n816&1\n1632&1\n3264&1\n64128&1\n12801,0002,000\nQuery Aspect RatioI/O Cost\nZ-curve\nQUILTS\nBMTree\n(a) Varying aspect ratio\n0.0001 0.001 0.01 0.1 1050100150\nQuery Selectivity (%)I/O Cost Ratio (%)\nZ-curve QUILTS BMTree (b) Varying selectivity\nFig. 14: Varying query aspect ratio and selectivity.\n3) Evaluating BMTree Variants: We study 4 BMTree\nvariants: BMTree-D ata D riven (with dataset only), BMTree-\nnoGAS (with no GAS algorithm), BMTree-greedy (pure\ngreedy), and BMTree-LMT (with limited BMPs). Results are\nin Figure 15. (1) BMTree-DD. We evaluate the BMTree\nUNISKEGAUUNISKEGAUUNISKEGAUUNISKEGAU50100\nUNI GAU OSM-US TIGERI/O Cost Ratio (%)\nBMTree BMTree-DD BMTree-noGAS BMTree-greedy\nBMTree-LMT\nFig. 15: I/O Cost on BMTree Variants.\nperformance when the query workload is not available. We\ngenerate training queries for the BMTree by following the\ndataset‚Äôs distribution. From Figure 15, observe that BMTree-\nDD performs comparable to the BMTree on the UNI work-\nloads for all datasets. However, on the SKE workload, the\nBMTree performs generally much better. (2) BMTree-noGAS.\nWe evaluate the effectiveness of the GAS algorithm. Observe\nthe performance drop compared to the MCTS using GAS.\nThis shows the effect of GAS. (3) BMTree-greedy. We apply\nGAS for all action selections, and build a purely greedy\nbased BMTree. Observe that MCTS with GAS outperforms\nboth BMTree-noGAS and BMTree-greedy. This indicates a\n\n14\nsynergistic improvement of MCTS over GAS. (4) BMTree-\nLMT. We consider all BMPs, and design a baseline BMTree\nwhere only the Z- and C-curves are allowed to be assigned\nto the subspaces. We observe a significant improvement in\nthe BMTree over using the Z- and C-curves alone. This\ndemonstrates the necessity of considering all BMPs.\nC. Effectiveness of Partial Retraining of the BMTree\n1) Varying Distribution Shift Settings: We evaluate the\neffectiveness of our proposed partial retraining mechanism.\nSpecifically, we evaluate three situations: data shift while the\nqueries are fixed, query shift while the data is fixed, and the\ncomposed scenario. We compare three methods: (1) Keeping\nthe original BMTree unchanged (noted as BMT-O ), (2) Fully\nretrained BMTree (noted as BMT-FR ), and (3) Partially re-\ntrained BMTree (noted as BMT-PR ). We restrict the partial\nretrain constraint ratio to 0.5, where at most half of the space\narea can be retrained. Also, we evaluate the performance while\nvarying the retrain constraints.\nEvaluation of Data Shift. We evaluate 3 different metrics: the\nI/O Cost and Query Latency of the constructed BMTree, and\nthe training time needed to retrain the BMTree. The data shifts\nfrom the GAUto the UNIdistribution. The results are in Fig. 16.\nIn Fig. 16a, the partially retrained BMTree BMT-PR achieves\na performance increase on I/O cost compared to the original\nBMTree BMT-O , while optimizing the percentage from 9.2%\nto12.1%and varying the shift percentage. Compared to the\nfully retrained BMTree BMT-FR ,BMT-PR achieves an aver-\nage of 90.6%performance improvement achieved by BMT-FR .\nUnder a 90% data shift, BMT-PR outperforms BMT-FR and\nachieves over 2.3√óreduction in I/O cost compared with\nBMT-FR . The reason is that BMT-PR allows the agent to focus\non optimizing the subspace with vital distribution changes,\nwhich allows for the partially retrained BMTree to achieve a\nbetter performance on this focused subspace. The results of\nthe query latency are generally consistent with the results of\nI/O Cost, in which BMT-PR achieves an average of 91.7%per-\nformance improvement achieved by BMT-TR , as in Fig. 16b.\nAs for the training time (Fig. 16c), BMT-FR costs from\n7857sto8314s(8125.5son average) to retrain the BMTree\nfrom scratch, while BMT-PR costs from 559.2sto3778.8s\n(1833.3son average). BMT-PR achieves approximately 4.4√ó\nreduction of training time compared with BMT-FR , that is\naligned with the time complexity estimation, where with a\nretraining constraint ratio of Rrc, the training time of BMT-PR\nis upper bounded by Rrc¬∑time (BMT-PR ).\nEvaluation of Query Shift. We proceed to evaluate the effect\nof query workload shift. Under the GAU data, we shift the\ndistribution of the query workload. Specifically, we vary the ¬µ\nvalues of the Gaussian distributions of queries, and generate\n2 skew query workloads, namely SKE 1andSKE 2, respectively.\nThe query workload is shifted from SKE 1toSKE 2. We estimate\nthe retrain performance by varying the shift percentage. The\nresults are in Fig. 17. Both the fully and partially retrained\nBMTrees BMT- FR andBMT-PR have limited optimization\ncompared to the original BMTree (less than 1%on I/O Cost)\nbefore the shift reaches 50% percentage. However, whenthe shift percentage reaches 70%, the optimizing potential\nbecomes vital. BMT-PR reduces I/O Cost from 7.3%to16.7%\ncompared with the BMT-O . We observe that BMT-PR achieves\nbetter performance on I/O Cost compared with BMT-FR from\nshift percentage 70% to90% (over 1.8√óat90%). This reveals\nthat retraining a subspace with a significant change in query\nworkload may significantly enhance performance compared\nwith training a BMTree for the whole data space. The query\nlatency results are similar in I/O Cost (as in Fig. 17b) that is\nconsistent with the data shift situation.\nThe training time (Fig. 17c) also aligns with the time com-\nplexity evaluation. BMT-FR spends from 7295.5sto8485.8s\n(on average, 7716.9s) to retrain the BMTree, while BMT-PR\nspends from 255.9sto1571.1s(on average, 1237.1s), achieves\napproximately over 6.2√óreduction in training time.\nEvaluation of Mixed Shift of Data and Query. We evaluate\nthe scenario when both data and query shift, and the shift\nsettings of data and query follow the former experiments. We\nselect shift percentages from: {25%,50%,75%}for both data\nand query. The results are in Fig. 18. The partially retrained\nBMTree BMT-PR achieves different levels of optimizations\nwhen varying data and query shift percentages. BMT-PR\nachieves remarkable I/O Cost reduction when query or data\nshift reaches 75% (the 3rd row and the 3rd column in Fig. 18\ndenote when data and query shift reaches 75%, respectively),\nwhich achieves on average 8.3%(resp. 16.5%) reduction in\nI/O cost under 75% data shift (resp. query shift) compared\nwithBMT-O .BMT-PR outperforms BMT-FR in certain cases\n(e.g., 25%√ó25% data-query shift), and achieves competitive\nperformance in the majority of situations. BMT-PR remains\nefficient compared with BMT-FR on training time. Particularly,\nunder 50% query shift, the training time reduction of BMT-PR\nis less than 2√ócompared with BMT-FR . Observe that the\nsecond partial retraining is triggered since the first partial\nretraining does not achieve notable optimization. Compared\nwithBMT-FR , a full retraining is preferred as the action in\nthe root node of the BMTree needs to be modified.\n2) Varying the Retraining Hyperparameters: We evalu-\nate how the retraining hyperparameters affect I/O Cost. We\nconsider the retraining constraint ratio and the shift score\nthreshold. We conduct the hyperparameter estimations under\n75%√ó75% data and query shifts. The results are in Fig. 19.\n(1) The Retraining Constraint Ratio Rrc.We vary Rrcfrom\n0.1to1to study how Rrcaffects the retraining performance.\nAs in Fig. 19a, BMT-PR achieves almost no performance\nenhancement compared to BMT-O , as the very small retraining\nconstraint ratios ( 0.1and0.2) limit the ability of the retrain\nmethod to retrain the important nodes that do not satisfy\nthe constraint. BMT-PR achieves optimal performance with a\nconstraint ratio of 0.5that allows retraining nodes that mostly\naffect query performance.\n(2) The Shift Score Threshold. We vary the shift score thresh-\nold from 0.1to0.5. As in Fig. 19b, observe that the threshold\nfrom 0.1to0.35,BMT-PR hassimilar performance compared\nto a full retrain. When the threshold is 0.4and above, it\nfilters nodes that achieve notable performance improvement\n(with threshold lower than 0.4) and become ineffective. This\nidentifies the best choices for the shift score threshold.\n\n15\n10 20 30 40 50 60 70 80 901.522.5¬∑102\n(%)I/O Cost\nBMT-O BMT-FR\nBMT-PR\n(a) Performance of I/O Cost.10 20 30 40 50 60 70 80 903456¬∑103\n(%)Query Latency ( ¬µs)\n(b) Performance of Query Latency.10 20 30 40 50 60 70 80 9002468¬∑103\n(%)Training Time (s)\n(c) Performance of Training Latency.\nFig. 16: Evaluation of data shift while fixing the queries. The data is shifted from a GAU toUNI with varying UNI percentage\nfrom 10% to90%.\n10 20 30 40 50 60 70 80 902468¬∑102\n(%)I/O Cost\nBMT-O BMT-FR\nBMT-PR\n(a) Performance of I/O Cost.10 20 30 40 50 60 70 80 900.511.5¬∑104\n(%)Query Latency ( ¬µs)\n(b) Performance of Query Latency.10 20 30 40 50 60 70 80 9002468¬∑103\n(%)Training Time (s)\n(c) Performance of Training Latency.\nFig. 17: Query shift while fixing the data. Query workload is shifted from 10% to90% of the new query workload.\n255075255075Query Shift (%)Data Shift (%)\nI/O CostTraining TimeI/O CostTraining TimeI/O CostTraining Time23‚á•10223‚á•102234‚á•102\n2345623456234562468‚á•1032468‚á•1032468‚á•103\n246824682468BMT-OBMT-FRBMT-PR\n234523452345246824682468\nFig. 18: Evaluation of mixed shift of data and query.\n0.10.20.30.40.50.60.70.80.91.0567¬∑102\nRetrain Constraint RatioI/O Cost\nBMT-O BMT-FR BMT-PR\n(a) Retrain constraint ratio..1.15 .2.25 .3.35 .4.45 .5567¬∑102\nShift Threshold ( 0.XX)I/O Cost\n(b) Shift score threshold.\nFig. 19: Varying retrain constraint ratio & shift score thresh-\nold.\nIX. R ELATED WORK\nSpace-Filling Curves (SFCs). Many SFCs have been devel-\noped. The C-curve [19] organizes the data points dimension at\na time. The Z and Hilbert curves [16, 17, 18, 19, 20, 21, 22]\nare widely used in index design. Despite the success of\nthese SFCs, they do not consider data and query workload\ndistributions. QUILTS [24] is proposed to consider data and\nquery distributions in designing the mapping function of\nSFCs. All these SFCs, including QUILTS, adopt a single\nmapping scheme that may not always be suitable for thewhole data space and query workload (Section I). This paper\nproposes the first piecewise SFC that uses different mapping\nfunctions for the different data subspaces. It considers both\ndata and query distributions. Furthermore, we propose a re-\ninforcement learning-based method to learn SFCs to directly\noptimize performance. Following the BMTree [29], there is\nwork [45, 46] that leverages learning to construct SFCs. The\nproposed piecewise SFC design potentially extends the design\nspace of SFCs. Moreover, these studies do not consider fast\nupdating of SFCs. This paper proposes partially regenerating\nthe BMTree, which reduces the update cost and only requires\npart of the data to update the SFC values.\nSFC-based Index Structures. SFCs are used for indexing\nmulti-dimensional data, and is widely adopted by DBMSs.\nAlso, SFCs are essential for learned multi-dimensional indexes\n(e.g., [11, 12, 47]). ZM [11] combines a Z-curve with a\nlearned index, namely RMI [2]. RSMI [12] applies the Hilbert\ncurve together with a learned index structure for spatial data.\nPai et al. [48] present preliminary results on the instance-\noptimal Z-index based on the Z-curve that adapts to data and\nworkload. SFC-based indexes can also be applied for data\nskipping [49, 50, 51] that aim to partition and organize data\ninto data pages so that querying algorithms only access pages\nthat are relevant to a query. An SFC-based approach [15] maps\nmultidimensional points to scalar values using an SFC, and\nuses the B+-Tree or range-partitioned key-value store (e.g.,\nH-base) for partitioning and organizing data.\nAnalysis of SFCs. Many studies, e.g., [52, 20, 53, 21, 42, 24]\nevaluate SFCs. Mokbel et al. [52, 20, 53] discuss the charac-\nteristics of good SFCs. Moon et al. [21] propose the number\nof disk seeks during query processing. Xu et al. [42] prove\nthat the Hilbert curve is a preferable SFC from that respect.\nNishimura et al. [24] propose a cohesion cost that evaluates\nhow good SFCs cluster data. Recently, Liu et al. [46] propose\na cost model that evaluates query performance of SFCs. It uses\nthe BMTree and speedups reward computing.\nReinforcement Learning (RL) in Indexing. Our method of\n\n16\ngenerating SFCs is based on RL techniques [54, 27]. There\nare several recent studies, e.g., [51, 55, 41] on applying RL\nto generate tree structures. Yang et al. [51] construct the Qd-\ntree for partitioning data into blocks on storage with Proximal\nPolicy Optimization networks (PPO) [34]. Gu et al. [41] utilize\nRL to construct the R-tree for answering spatial queries [17],\nand Neurocuts [55] constructs a decision tree using a RL agent.\nThese RL designs are not suitable for learning piecewise SFCs.\nOur design of RL models is based on MCTS and is different\nfrom the designs in these studies.\nX. C ONCLUSION\nIn this paper, we study the Space-Filling Curve Design prob-\nlem, and propose constructing piecewise SFCs that adopt dif-\nferent mapping schemes for different data subspaces. Specif-\nically, we propose the BMTree for maintaining multiple bit\nmerging patterns in which every path corresponds to a BMP.\nWe propose to construct the BMTree in a data-driven manner\nvia reinforcement learning. Further, we develop a partial re-\ntraining procedure that supports retraining parts of the BMTree\nwhile retaining the rest unchanged instead of training from\nscratch. We conduct extensive experiments on both synthetic\nand real datasets with various query workloads. Experiments\nshow that the piecewise SFCs are consistently superior over\nexisting SFCs, especially when data and/or queries have a\ncertain degree of skewness.\nREFERENCES\n[1] R. Bayer, ‚ÄúThe universal b-tree for multidimensional in-\ndexing: general concepts,‚Äù in Worldwide Computing and\nIts Applications, International Conference , ser. Lecture\nNotes in Computer Science, vol. 1274. Springer, 1997,\npp. 198‚Äì209.\n[2] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Poly-\nzotis, ‚ÄúThe case for learned index structures,‚Äù in ACM\nSIGMOD , 2018, pp. 489‚Äì504.\n[3] J. Ding, U. F. Minhas, J. Yu, C. Wang, J. Do, Y . Li,\nH. Zhang, B. Chandramouli, J. Gehrke, D. Kossmann,\nD. B. Lomet, and T. Kraska, ‚ÄúALEX: an updatable\nadaptive learned index,‚Äù in ACM SIGMOD , 2020, pp.\n969‚Äì984.\n[4] J. Ding, V . Nathan, M. Alizadeh, and T. Kraska,\n‚ÄúTsunami: A learned multi-dimensional index for corre-\nlated data and skewed workloads,‚Äù Proc. VLDB Endow. ,\nvol. 14, no. 2, pp. 74‚Äì86, 2020.\n[5] R. Marcus, A. Kipf, A. van Renen, M. Stoian, S. Misra,\nA. Kemper, T. Neumann, and T. Kraska, ‚ÄúBenchmarking\nlearned indexes,‚Äù Proc. VLDB Endow. , vol. 14, no. 1, pp.\n1‚Äì13, 2020.\n[6] C. Faloutsos, ‚ÄúGray codes for partial match and range\nqueries,‚Äù IEEE Trans. Software Eng. , vol. 14, no. 10, pp.\n1381‚Äì1393, 1988.\n[7] C. Faloutsos and S. Roseman, ‚ÄúFractals for secondary\nkey retrieval,‚Äù in Proceedings of the Eighth ACM\nSIGACT-SIGMOD-SIGART Symposium on Principles of\nDatabase Systems , 1989, pp. 247‚Äì252.[8] K. C. K. Lee, W. Lee, B. Zheng, H. Li, and Y . Tian, ‚ÄúZ-\nSKY: an efficient skyline query processing framework\nbased on z-order,‚Äù VLDB J. , vol. 19, no. 3, pp. 333‚Äì362,\n2010.\n[9] M. L. Yiu, Y . Tao, and N. Mamoulis, ‚ÄúThe Bdual-tree:\nindexing moving objects by space filling curves in the\ndual space,‚Äù VLDB J. , vol. 17, no. 3, pp. 379‚Äì400, 2008.\n[10] L. Zhou, C. R. Johnson, and D. Weiskopf, ‚ÄúData-driven\nspace-filling curves,‚Äù IEEE Trans. Vis. Comput. Graph. ,\nvol. 27, no. 2, pp. 1591‚Äì1600, 2021.\n[11] H. Wang, X. Fu, J. Xu, and H. Lu, ‚ÄúLearned index for\nspatial queries,‚Äù in 20th IEEE International Conference\non Mobile Data Management , 2019, pp. 569‚Äì574.\n[12] J. Qi, G. Liu, C. S. Jensen, and L. Kulik, ‚ÄúEffectively\nlearning spatial indices,‚Äù Proc. VLDB Endow. , vol. 13,\nno. 11, pp. 2341‚Äì2354, 2020.\n[13] n.d., ‚ÄúPostgis,‚Äù https://postgis.net/docs/using postgis\ndbmanagement.html, 2023, [Online; accessed 3-May-\n2023].\n[14] Z. Slayton, ‚ÄúZ-order indexing for multifaceted queries\nin amazon dynamodb,‚Äù https://aws.amazon.com/blogs/\ndatabase, 2017.\n[15] S. Nishimura, S. Das, D. Agrawal, and A. E. Abbadi,\n‚ÄúMd-hbase: A scalable multi-dimensional data infrastruc-\nture for location aware services,‚Äù in 12th IEEE Interna-\ntional Conference on Mobile Data Management . IEEE\nComputer Society, 2011, pp. 7‚Äì16.\n[16] J. A. Orenstein and T. H. Merrett, ‚ÄúA class of data struc-\ntures for associative searching,‚Äù in Proceedings of the\nThird ACM SIGACT-SIGMOD Symposium on Principles\nof Database Systems , 1984, pp. 181‚Äì190.\n[17] J. A. Orenstein, ‚ÄúSpatial query processing in an object-\noriented database system,‚Äù in ACM SIGMOD , 1986, pp.\n326‚Äì336.\n[18] ‚Äî‚Äî, ‚ÄúRedundancy in spatial databases,‚Äù in ACM SIG-\nMOD , 1989, pp. 295‚Äì305.\n[19] H. V . Jagadish, ‚ÄúLinear clustering of objects with multi-\nple atributes,‚Äù in ACM SIGMOD , 1990, pp. 332‚Äì342.\n[20] M. F. Mokbel, W. G. Aref, and I. Kamel, ‚ÄúAnalysis of\nmulti-dimensional space-filling curves,‚Äù GeoInformatica ,\nvol. 7, no. 3, pp. 179‚Äì209, 2003.\n[21] B. Moon, H. V . Jagadish, C. Faloutsos, and J. H. Saltz,\n‚ÄúAnalysis of the clustering properties of the hilbert space-\nfilling curve,‚Äù IEEE Trans. Knowl. Data Eng. , vol. 13,\nno. 1, pp. 124‚Äì141, 2001.\n[22] A. Kipf, H. Lang, V . Pandey, R. A. Persa, C. Anneser,\nE. T. Zacharatou, H. Doraiswamy, P. A. Boncz, T. Neu-\nmann, and A. Kemper, ‚ÄúAdaptive main-memory indexing\nfor high-performance point-polygon joins,‚Äù pp. 347‚Äì358,\n2020.\n[23] H. Samet, Foundations of multidimensional and metric\ndata structures , ser. Morgan Kaufmann series in data\nmanagement systems, 2006.\n[24] S. Nishimura and H. Yokota, ‚ÄúQUILTS: multidimen-\nsional data partitioning framework based on query-aware\nand skew-tolerant space-filling curves,‚Äù in ACM SIG-\nMOD , 2017, pp. 1525‚Äì1537.\n[25] K. C. K. Lee, B. Zheng, H. Li, and W. Lee, ‚ÄúApproaching\n\n17\nthe skyline in Z order,‚Äù in Proceedings of the 33rd\nInternational Conference on Very Large Data Bases .\nACM, 2007, pp. 279‚Äì290.\n[26] M. L. Puterman, Markov Decision Processes: Discrete\nStochastic Dynamic Programming , ser. Wiley Series in\nProbability and Statistics, 1994.\n[27] C. Browne, E. J. Powley, D. Whitehouse, S. M. Lucas,\nP. I. Cowling, P. Rohlfshagen, S. Tavener, D. P. Liebana,\nS. Samothrakis, and S. Colton, ‚ÄúA survey of monte carlo\ntree search methods,‚Äù IEEE Trans. Comput. Intell. AI\nGames , vol. 4, no. 1, pp. 1‚Äì43, 2012.\n[28] J. Lu, A. Liu, F. Dong, F. Gu, J. Gama, and G. Zhang,\n‚ÄúLearning under concept drift: A review,‚Äù IEEE Trans.\nKnowl. Data Eng. , vol. 31, no. 12, pp. 2346‚Äì2363, 2018.\n[29] J. Li, Z. Wang, G. Cong, C. Long, H. M. Kiah, and\nB. Cui, ‚ÄúTowards designing and learning piecewise\nspace-filling curves,‚Äù Proceedings of the VLDB Endow-\nment , vol. 16, no. 9, pp. 2158‚Äì2171, 2023.\n[30] T. Skopal, M. Kr ¬¥atk¬¥y, J. Pokorn ¬¥y, and V . Sn ¬¥asel, ‚ÄúA new\nrange query algorithm for universal b-trees,‚Äù Inf. Syst. ,\nvol. 31, no. 6, pp. 489‚Äì511, 2006.\n[31] G. Peano, ‚ÄúSur une courbe, qui remplit toute une aire\nplane,‚Äù Mathematische Annalen , vol. 36, no. 1, pp. 157‚Äì\n160, 1890.\n[32] J. K. Lawder and P. J. H. King, ‚ÄúQuerying multi-\ndimensional data indexed using the hilbert space-filling\ncurve,‚Äù SIGMOD Rec. , vol. 30, no. 1, pp. 19‚Äì24, 2001.\n[33] B. Fuglede and F. Tops√∏e, ‚ÄúJensen-shannon divergence\nand hilbert space embedding,‚Äù in Proc. IEEE ISIT .\nIEEE, 2004, p. 31.\n[34] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and\nO. Klimov, ‚ÄúProximal policy optimization algorithms,‚Äù\nCoRR , vol. abs/1707.06347, 2017.\n[35] X. Zhou, G. Li, C. Chai, and J. Feng, ‚ÄúA learned query\nrewrite system using monte carlo tree search,‚Äù Proc.\nVLDB Endow. , vol. 15, no. 1, pp. 46‚Äì58, 2021.\n[36] L. Kocsis and C. Szepesv ¬¥ari, ‚ÄúBandit based monte-\ncarlo planning,‚Äù in Machine Learning: ECML 2006, 17th\nEuropean Conference on Machine Learning , ser. Lecture\nNotes in Computer Science, vol. 4212. Springer, 2006,\npp. 282‚Äì293.\n[37] ‚ÄúOpenstreetmap,‚Äù http://www.openstreetmap.org/, 2023,\n[Online; accessed 3-May-2023].\n[38] ‚ÄúTiger/line shapefiles,‚Äù https://www.census.gov/\ngeographies/mapping-files/time-series/geo/tiger-line-file.\nhtml, 2022, [Online; accessed 3-May-2023].\n[39] A. Eldawy and M. F. Mokbel, ‚ÄúSpatialhadoop: A mapre-\nduce framework for spatial data,‚Äù in IEEE ICDE , 2015,\npp. 1352‚Äì1363.\n[40] G. Liu, ‚ÄúReleased RSMI Code,‚Äù https://github.com/\nLiuguanli/RSMI, 2020.\n[41] T. Gu, K. Feng, G. Cong, C. Long, Z. Wang, and\nS. Wang, ‚ÄúThe rlr-tree: A reinforcement learning based\nr-tree for spatial data,‚Äù ACM SIGMOD , vol. 1, no. 1, pp.\n1‚Äì26, 2023.\n[42] P. Xu and S. Tirthapura, ‚ÄúOptimality of clustering proper-\nties of space-filling curves,‚Äù ACM Trans. Database Syst. ,\nvol. 39, no. 2, pp. 10:1‚Äì10:27, 2014.[43] S. T. Leutenegger, J. M. Edgington, and M. A. L ¬¥opez,\n‚ÄúSTR: A simple and efficient algorithm for r-tree pack-\ning,‚Äù in ICDE , 1997, pp. 497‚Äì506.\n[44] N. Beckmann, H. Kriegel, R. Schneider, and B. Seeger,\n‚ÄúThe r*-tree: An efficient and robust access method for\npoints and rectangles,‚Äù in ACM SIGMOD , 1990, pp. 322‚Äì\n331.\n[45] J. Gao, X. Cao, X. Yao, G. Zhang, and W. Wang, ‚ÄúLmsfc:\nA novel multidimensional index based on learned mono-\ntonic space filling curves,‚Äù Proceedings of the VLDB\nEndowment , vol. 16, no. 10, pp. 2605‚Äì2617, 2023.\n[46] G. Liu, L. Kulik, C. S. Jensen, T. Li, and J. Qi, ‚ÄúEfficient\ncost modeling of space-filling curves,‚Äù arXiv preprint\narXiv:2312.16355 , 2023.\n[47] Z. Zhang, P. Jin, X. Wang, Y . Lv, S. Wan, and X. Xie,\n‚ÄúCOLIN: A cache-conscious dynamic learned index with\nhigh read/write performance,‚Äù J. Comput. Sci. Technol. ,\nvol. 36, no. 4, pp. 721‚Äì740, 2021.\n[48] S. G. Pai, M. Mathioudakis, and Y . Wang, ‚ÄúTowards an\ninstance-optimal z-index,‚Äù in 4th International Workshop\non Applied AI for Database Systems and Applications\n(AIDB@ VLDB2022) , 2022.\n[49] V . Raman, G. K. Attaluri, R. Barber, N. Chainani,\nD. Kalmuk, V . KulandaiSamy, J. Leenstra, S. Lightstone,\nS. Liu, G. M. Lohman, T. Malkemus, R. M ¬®uller, I. Pandis,\nB. Schiefer, D. Sharpe, R. Sidle, A. J. Storm, and\nL. Zhang, ‚ÄúDB2 with BLU acceleration: So much more\nthan just a column store,‚Äù Proc. VLDB Endow. , vol. 6,\nno. 11, pp. 1080‚Äì1091, 2013.\n[50] L. Sun, M. J. Franklin, S. Krishnan, and R. S. Xin,\n‚ÄúFine-grained partitioning for aggressive data skipping,‚Äù\ninACM SIGMOD . ACM, 2014, pp. 1115‚Äì1126.\n[51] Z. Yang, B. Chandramouli, C. Wang, J. Gehrke, Y . Li,\nU. F. Minhas, P. Larson, D. Kossmann, and R. Acharya,\n‚ÄúQd-tree: Learning data layouts for big data analytics,‚Äù\ninACM SIGMOD , 2020, pp. 193‚Äì208.\n[52] M. F. Mokbel and W. G. Aref, ‚ÄúIrregularity in multi-\ndimensional space-filling curves with applications in\nmultimedia databases,‚Äù in Proceedings of the 2001 ACM\nCIKM . ACM, 2001, pp. 512‚Äì519.\n[53] ‚Äî‚Äî, ‚ÄúOn query processing and optimality using spectral\nlocality-preserving mappings,‚Äù in International Sympo-\nsium on Spatial and Temporal Databases . Springer,\n2003, pp. 102‚Äì121.\n[54] R. S. Sutton and A. G. Barto, Reinforcement learning -\nan introduction , ser. Adaptive computation and machine\nlearning, 1998.\n[55] E. Liang, H. Zhu, X. Jin, and I. Stoica, ‚ÄúNeural packet\nclassification,‚Äù in Proceedings of the ACM Special Inter-\nest Group on Data Communication , 2019, pp. 256‚Äì269.",
  "textLength": 94884
}