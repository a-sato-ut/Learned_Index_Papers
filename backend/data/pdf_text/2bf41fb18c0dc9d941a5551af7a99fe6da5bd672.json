{
  "paperId": "2bf41fb18c0dc9d941a5551af7a99fe6da5bd672",
  "title": "Learned Sorted Table Search and Static Indexes in Small Model Space",
  "pdfPath": "2bf41fb18c0dc9d941a5551af7a99fe6da5bd672.pdf",
  "text": "Learned Sorted Table Search and Static Indexes in Small Model\nSpace∗\nDomenico Amato1Giosu´ e Lo Bosco1†Raﬀaele Giancarlo1\n1Dipartimento di Matematica e Informatica\nUniversit´ a degli Studi di Palermo, ITALY\nSeptember 20, 2022\nAbstract\nMachine Learning Techniques, properly combined with Data Structures, have resulted in Learned\nStatic Indexes, innovative and powerful tools that speed-up Binary Search, with the use of additional\nspace with respect to the table being searched into. Such space is devoted to the Machine Learning Model.\nAlthough in their infancy, they are methodologically and practically important, due to the pervasiveness\nof Sorted Table Search procedures. In modern applications, model space is a key factor and, in fact, a\nmajor open question concerning this area is to assess to what extent one can enjoy the speed-up of Binary\nSearch achieved by Learned Indexes while using constant or nearly constant space models. In this paper,\nwe investigate the mentioned question by (a) introducing two new models, i.e., the Learned k-ary Search\nModel and the Synoptic Recursive Model Index, respectively; (b) systematically exploring the time-space\ntrade-oﬀs of a hierarchy of existing models, i.e., the ones in the reference software platform Searching\non Sorted Data , together with the new ones proposed here. We document a novel and rather complex\ntime-space trade-oﬀ picture, which is very informative for users as well as designers of Learned Indexing\nData Structures. By adhering and extending the current benchmarking methodology, we experimentally\nshow that the Learned k-ary Search Model can speed up Binary Search in constant additional space. Our\nsecond model, together with the bi-criteria Piece-wise Geometric Model index, can achieve a speed-up\nof Binary Search with a model space of 0 .05% more than the one taken by the table, being competitive\nin terms of time-space trade-oﬀ with existing proposals. The Synoptic Recursive Model Index and the\nbi-criteria Piece-wise Geometric Model complement each other quite well across the various levels of the\ninternal memory hierarchy. Finally, our ﬁndings stimulate research in this area, since they highlight the\nneed for further studies regarding the time-space relation in Learned Indexes.\n1 Introduction\nWith the aim of obtaining time and space improvements in classic Data Structures, an emerging trend is\nto combine Machine Learning techniques with the ones proper of Data Structures. This new research area\ngoes under the name of Learned Data Structures , and it has been initiated in 2018 by Kraska et al. [24]. In\nparticular, in such a paper the Learned Data Structures have been used mainly for the case of searching\nin sorted sets. This particular problem can be solved in classic algorithmics by using a well-known and\noptimal routine, i.e. Binary Search [1, 22], or more sophisticated Data Structures, e.g. classic Indexes such\nas B-Trees [9]. Usually, the classic approaches consider all of the element positions in a sorted list as possible\ncandidates to be an answer to a search query. Such an initial list is then reﬁned in at most O(logn) iterations,\n∗An extended abstract related to this paper has been presented at 20th International Conference of the Italian Association\nfor Artiﬁcial Intelligence (AixIA 2021).\n†corresponding author, email: giosue.lobosco@unipa.it\n1arXiv:2107.09480v6  [cs.IR]  17 Sep 2022\n\nwherenis the size of the sorted set. The main novelty in the Learned Data Structures paradigm is the use of\na Machine Learning Model trained over the elements of a sorted set, that can learn the dataset distribution.\nThis Model uses such a knowledge to make a prediction of the position of the query element in the sorted\nset. The prediction may be inaccurate, so the Model returns an interval to search into that accounts for\nprediction errors. As a consequence, the output of the Model is an interval of positions, where to search\ninto. The better the Model, the smaller the interval. The ﬁnal search stage on the reduced table positions\ninterval is performed via, for the sake of exposition, Binary Search. This particular kind of Learned Data\nstructure is referred to as Learned Index and is the main object of this research. In what follows, in order to\nplace our contributions in the proper Literature context, we provide a brief Literature review, followed by a\nroad map of the paper highlighting also our contributions.\n1.1 Literature Review\nAlthough Learned Data Structure is a very novel research ﬁeld, it has already been extensively studied\nin the Literature [13, 29, 35]. In what follows, we mention the main methods which can be useful for a\nbetter comprehension of the contributions provided in this paper. To this end, the most signiﬁcant Learned\nIndexes are presented, with speciﬁc reference to their training procedures and relative benchmarking studies.\nMoreover, examples of real-world applications of Learned Indexes are provided, the important aspect of\ntime/space correlation is highlighted, and for completeness, examples of other Learned Data Structures\ndiﬀerent from Learned Indexes are given. However, The presentation is intended to be synoptic, since the\ninterested reader can ﬁnd details in the papers that are mentioned, including also a recent review on the\nsubject [13].\n1.1.1 Core Methods and Benchmarking Platform\nThe Recursive Model Index [24] ( RMI for short) is the ﬁrst Learned Index proposal. It is a hierarchical\nmodel which estimates the distribution of the data via a top-down approach. It can be considered as a\ntree-like structure, where the nodes are generic models, ranging from Neural Network Models [3] to simple\nlinear or polynomial regression models [24]. Given a query element, the internal nodes at each level identify\nthe index of the next model (node) to use in the hierarchy. Finally, leaves provide a reduced interval to\nsearch into. The tree structure of the RMI is characterized by the number of levels, the number of nodes\nfor each level and the kind of models adopted at each node. As a consequence, the RMI depends on a\nconsistent number of hyperparameters, whose estimation could be a serious issue in real-world contexts, as\nhighlighted by Maltry et al. [28]. To overcome these diﬃculties, Marcus et al. provide a platform, referred\nto as CDFShop [32], that can be used to generate the code of a speciﬁc RMI , given an input dataset and\nspeciﬁc values of its hyperparameters. In addition, given again an input dataset, the platform can provide\nup to ten RMI s.\nFollowing the seminal proposal of the RMI , various new versions of Learned Indexes have been designed.\nThis is the case of the Piece-wise Geometric Model Index [14] ( PGM for short) that estimates the data\ndistribution in a bottom-up fashion, by a Piece-wise Linear Approximation Algorithm [8]. Diﬀerently from\ntheRMI , it is based on only one hyperparameter /epsilon1, which represents the maximum error admitted for the\nindex prediction. Note that despite the value of /epsilon1guarantees an upper bound on the search time, it does\nnot provide any bound suggestion on the additional space used by any Learned Index with respect to the\nsize of the input data.\nTheFITing-Tree Model by Kraska et al. [17] has been designed to overcome the mentioned space issue.\nIt is an extension of the PGM using the maximum number of approximation segments as an additional\nparameter, so that it is possible to compute the maximum additional space used by this Model. Although\ncharacterized by this new space bound property, it is not considered in this study because of its poor\nperformance in terms of query time with respect to others Learned Indexes, as remarked in the Literature [19].\nThe Radix Spline Index [20] ( RSfor short) is another example of a bottom-up approach to Learned\nIndexing, that in a diﬀerent manner from the PGM estimates the distribution through a spline curve [36].\nAs for the FITing-Tree Model, both search time and space can be controlled through two hyperparameters,\n2\n\ni.e., the maximum error /epsilon1and the number of bits needed to index the spline points. However, we anticipate\nthat such a control of space is rather poor, as documented by our experiments.\nExcept for the PGM , all the Learned Indexes mentioned so far are static and need to be rebuilt in the\ncase the input dataset changes. Such a reconstruction could aﬀect seriously the Learned Index performances,\nso a new class of indexes, referred to as Dynamic, have been proposed. This is the case of the Adaptive\nLearned Index [12] ( ALEX for short), which provides a Dynamic Learned Index via an extension of the\nRMI .\nDue to the high number of Learned Index proposals, it is evident that it is necessary to determine the\nstrengths and weaknesses of each method. To this end, Marcus et al. [29] provide an exhaustive benchmarking\nstudy of the main Learned Indexes on real datasets, supported by the development of a software platform\nreferred to as Searching on Sorted Data [19] (SOSD for short). The mentioned study approaches the question\nby considering only Binary Search as the ﬁnal level of Learned Indexing. Additional pros/cons studies are\navailable at [2,5], considering also diﬀerent types of search procedures, such as Uniform Binary Search and\nk-ary Search. However, it is evident that no clear winner emerges, across the many datasets and search\nroutines used for experimentation. It is also evident that, as also summarized in a web platform [21], that in\nmost cases the best performing indexes are the RMI ,PGM andRS. As a consequence, these three Learned\nIndexes are the ones considered in this paper as a baseline to compare against.\n1.1.2 Applications\nClassic Indexes are widely used in a variety of real-world contexts, such as Databases [39] and Search\nEngines [18]. As a consequence, Learned Indexes can also make improvements in various related applications.\nIn particular, they are widely used for Databases, providing new challenges and opportunities [46], such as\nthe development of the so-called Learned Databases [23]. They have been applied also in speciﬁc kinds of\nDatabases, such as spatial [26, 45] and biological [37] ones. Finally, another very recent application is the\ndevelopment of frameworks for optimizing Database queries [30,31,33,47].\n1.1.3 Additional Learned Data Structures\nAnalogously to Learned Indexes, many methods can beneﬁt from the combined approach of Machine Learning\nand classic Data Structures. An example that has been extensively discussed in the Literature is the case\nof the Bloom Filters [6], whose learned version is introduced by Kraska et al. [24], extended with several\nvariants in [11,34,43] and more in-depth analysed by Fumagalli et al. [16]. Other examples are the Learned\nHash Functions [24,42], Learned Binary Trees [27] and Learned Rank/Select Dictionaries [7]. However, the\nimportance of using a Learning phase to improve the performance of a classic algorithm has not been limited\nonly to those concerned with searching in sorted sets, but recently also for caching, scheduling, counting on\ndata streams [35], and in the speciﬁc case of sorting operations [25].\n1.1.4 An Overlooked Issue: Time/Space Correlation in Learned Indexing\nAs we have mentioned, all Learned Indexes proposal oﬀer some kind of time/space trade-oﬀ. However, this\naspect has not been investigated in depth and rigorously, following the methodology coming from Classic\nData Structures [22]. Moreover, it is missing an assessment of how good would be constant space models at\nspeeding-up search procedures. Indeed, two related fundamental questions have been overlooked, which are\nstated here:\n•to what extent one can enjoy the speed-up of the search procedures provided by Learned Indexes with\nrespect to the additional space one needs to use.\n•how space-demanding should be a predictive model in order to speed up those procedures.\nThe main contribution of this paper is to provide answers to those two questions, putting this new\nalgorithmic methodology at par with the classic one, i.e. the study of, ﬁrst, the constant space models and\nthen of the more space-demanding ones.\n3\n\n1.2 Road Map of the Paper\nHere we provided a road map of the paper and our contributions.\nSection 2 is dedicated to a formal deﬁnition of the search on sorted data problem, also proposing via a\nclassic solution, i.e., Binary Search. Then, we provide and discuss a very simple approach to learning from\ndata to speed-up searching in sorted tables. Moreover, we propose a classiﬁcation of Learned Indexes that\nincludes two new ones as well as some that are leaders in the Literature, i.e., RMI ,RSandPGM . In\nparticular, the ﬁrst new model, referred to as Learnedk-ary Search (KO-US for short), uses constant space\nwhile the other new model referred to as Synoptic RMI (SY-RMI for short), uses a user-deﬁned amount\nof space. Apart from the novelty of the proposed models, the classiﬁcation is new and methodologically\nimportant, since it allows us to systematically and coherently study whether we can obtain Learned Indexes\nwith small space occupancy, i.e, close to constant such as a classic Binary Search, with the characteristic of\nbeing able to speed-ups Sorted Table Search procedures.\nSection 3 provides our experimental methodology, which extends the one recommended in the bench-\nmarking study by Marcus et al. [29]. In particular, in order to provide an evaluation of how Learned Indexes\nperform when the input table ﬁts the diﬀerent levels of the internal memory hierarchy, we have extended the\ndatasets used in the benchmarking study. This is another methodologically important contribution of this\nscientiﬁc research.\nSection 4 describes and analyses the training phase of the two novel models. In particular, we focus\non how the Synoptic RMI is able to learn, in small space, key features of a variety of real datasets for\nthe purpose of prediction. Moreover, we report useful indications, overlooked so far in the Literature, for\nLearned Indexes designers and practitioners about model training across diﬀerent memory levels, shedding\nadditional light on the training phase of the RSand the PGM . It is useful to recall that the RSis superior\nto the PGM in training time on large datasets [20]. Here we show that, on small datasets, this is no longer\nthe case.\nSection 5 describes and analyzes the Learned Indexes query phase, providing the main contributions of\nthis paper. In particular, concerning the additional space, we analyse two possible cases: constant or nearly\nconstant and parametric.\nFor the case of constant space, our main contribution is the study of the performance of the Learned\nk-ary Search Model in comparison with a Cubic Regression Model and the Binary Search alone. Indeed, we\nanticipate that the Learned k-ary Search Model performs better than the Binary Search alone and of the\nCubic Model, except in the case when the dataset distribution is very complex to approximate. This issue\nrepresents the main weakness of constant space models. In addition, the Learned k-ary Search Model has\nbeen compared with a top performing Biunary Search routine that uses a layout other than sorted, i.e., the\nEytzinger Layout [18]. Our ﬁndings provide evidence that the Eytzinger Layout, when possible to use, is\nalways competitive with respect to all the Models with constant or nearly constant space, even the Learned\nk-ary Search. Unfortunately, as indicated in what follows, such a lyaout cannot be used within the current\nLeaerned Indexing paradigm.\nFor the case of parametric space, we provide a conﬁrmation and an extension of the ﬁndings provided\nin the benchmarking study by Marcus et al. [29]. Indeed, the new model introduced in the study, i.e.\nthe Synoptic RMI , and the bi-criteria PGM , perform better than the Binary Search alone, across all the\ndatasets and memory levels, using very small additional space with respect to the input table. Moreover, even\nthe most complex models, excluding the RSon the lower memory levels, achieves very good performance\nconsidering a bound of at most 10% of additional space. It is also competitive with respect to the bi-criteria\nPGM . We investigate also Parametric Models’ time and space relationships, showing that while their query\ntimes can diﬀer by constant factors, the corresponding spaces can disagree by several orders of magnitude.\nThe main ﬁnding is that space seems to be the real key to the Model’s eﬃciency. This provides additional\ninsights into the time/space relationship of Learned Indexes, with respect to what is known in the Literature.\nFinally, our analysis also provides useful guidelines to the practitioners interested in using Learned\nIndexes.\n4\n\nQuery Element\n{1\n5\n11\n14\n58\n59\n60\n97\n100\n101ModelFigure 1: A general paradigm of Learned Searching in a Sorted Table [29] . The model is trained on the data in the table.\nThen, given a query element, it is used to predict the interval in the table where to search (included in brackets in the ﬁgure).\n2 Learning from a Static Sorted Set to Speed-Up Searching in It\nConsider a sorted table Aofnkeys, taken from a universe U. It is well known that Sorted Table Search\ncan be phrased as the Predecessor Search Problem: for a given query element x, return the A[j] such that\nA[j]≤x<A [j+ 1]. With reference to such a problem, in the following, we describe the classic solutions in\nthe Literature and how to transform it into a learning-prediction one.\n2.1 Solution with a Sorted Search Routine\nIt is well-known in Algorithmics [1,10,22,38] that the Predecessor Search Problem can be solved with Sorted\nTable Search routines, such as Binary and Interpolation Search. For the aim of this paper and according to\nthe benchmarking study, we use the C++ lower bound routine, denoted as BSand informally referred to\nas Standard. In addition to this method, we use the best routines that come out of the study by Khuong\nand Morin [18], i.e., Uniform Binary Search [22], denoted as US, and the Eytzinger Layout Search, denoted\nasEB. For the convenience of the reader, details about all the above-mentioned search procedures are in\nSection 1 of the Supplementary File. We anticipate that other routines may be considered in this study, such\nas Interpolation Search or its variant TIP [44], but the extensive experiments conducted in [4] show that\nthey are not competitive in the Learned Indexing scenario. Therefore, in order to keep this paper focused\non relevant contributions, they are omitted here.\n2.2 Learning from Data to Speed-Up Sorted Table Search: A simple View with\nan Example\nKraska et al. [24] have proposed an approach that transforms the Predecessor Search problem into a learning-\nprediction one. With reference to Figure 1, the model learned from the data is used as a predictor of where\na query element may be in the table. To ﬁx ideas, Binary Search is then performed only on the interval\nreturned by the model.\nWe now outline the simplest technique that can be used to build a model for A, providing also an\nexample. It relies on Linear Regression, with Mean Square Error Minimization [15]. We start with the\nexample. Consider Figure 2 and the table Ain the caption.\n•Ingredient One of Learned Indexing: The Cumulative Distribution Function ( CDF for\nShort) of a Sorted Table . With reference to Figure 2(a), we can plot the elements of Ain a graph,\nwhere the abscissa reports the value of the elements in the table and the ordinates are their corre-\nsponding ranks. The result of the plot is reminiscent of a discrete Cumulative Distribution Function\nCDF that underlines the table. The speciﬁc construction exempliﬁed here can be generalized to any\nsorted table, as discussed in Marcus et al. [29]. In the Literature, for a given table, such a discrete\ncurve is referenced as CDF .\n•Ingredient Two of Learned Indexing: A Model for the CDF . Now, it is essential to transform\nthe discrete CDF into a continuous curve. The simplest way to do this is to ﬁt a straight line of\n5\n\nequationF(x) =ax+bto the CDF (this process is shown in Figure 2(b)). In this example, we use\nLinear Regression with Mean Square Error Minimization in order to obtain aandb. They are 0.01\nand 0.85, respectively.\n•Ingredient Three of Learned Indexing: The Model Error Correction . SinceFis an approxi-\nmation of the ranks of the elements in the table, applying it to an element in order to predict its rank,\nwe may produce an error e. With reference to Figure 2(c), applying the model to the element 398, we\nobtain a predicted rank of 4 .68, instead of 7, which is the real rank. So, the error made by the model\nF(x) = 0.01∗x+ 0.85 on this element is e= 7−⌈4.68⌉= 2. Therefore, in order to use the equation\nFto predict where an element xis in the table, we must correct for this error. Indeed, we consider\nthe maximum error /epsilon1computed as the maximum distance between the real rank of the elements in\nthe table and the corresponding rank predicted by the model. The maximum error /epsilon1is used to set the\nsearch interval of an element xto be [F(x)−/epsilon1,F(x) +/epsilon1]. In the example we are discussing, /epsilon1is 3.\nMore in general, in order to perform a query, the model is consulted and an interval in which to search\nis returned. Then, Binary Search on that interval is performed. Diﬀerent models may use diﬀerent schemes\nto determine the required range, as outlined in Section 2.3. The reader interested in a rigorous presentation\nof those ideas can consult Marcus et al. [32]. In this paper, we characterize the accuracy in the prediction\nof a model via the reduction factor : the percentage of the table that is no longer considered for searching\nafter the prediction of a rank. Because of the diversity across models to determine the search interval, and\nin order to place all models on a par, we estimate empirically the reduction factor of a model. That is, with\nthe use of the model and over a batch of queries, we determine the length of the interval to search into for\neach query. Based on it, it is immediate to compute the reduction factor for that query. Then, we take the\naverage of those reduction factors over the entire set of queries as the reduction factor of the model for the\ngiven table.\n(a)\n (b)\n (c)\ne\nFigure 2: The Process of Learning a Simple Model via Linear Regression. Let the table A be\n[47,105,140,289,316,358,386,398,819,939]. (a) the empirical CDF ofA; (b) the line (in orange) associated with a linear model\nobtained via Linear Regression; and (c) the error emade by the model in predicting the query element 398.\n2.3 A Classiﬁcation of Learned Indexing Models\nWith the exception of the Eytzinger Binary Search, all procedures mentioned in Section 2.1 have a natural\nLearned version. Indeed, all models currently known in the Literature naturally ﬁt sorted table layouts for\nthe ﬁnal search stage but, for that purpose, array layouts other than sorted or more complex Data Structures\ncannot be used. Given a Learned version of the two mentioned procedures, its time and space performances\ndepend critically on the model used to predict the interval to search into. Here we propose a classiﬁcation\nof Models that comprises four classes. The ﬁrst two, shown in Figure 3, consist of models that use constant\nspace, while the other two, shown in Figure 4, consist of models that use space as a function of some model\nparameters. For each of them, the reduction factor is determined as described in Section 2. Moreover, as\nalready pointed out, the Learned k-ary Search and the Synoptic RMI Models are new and ﬁt quite naturally\nin the classiﬁcation that we present.\nAtomic Models: One Level and no Branching Factor\n6\n\n•Simple Regression [15]. We use linear, quadratic and cubic regression models. Each can be thought\nof as an atomic model in the sense that it cannot be divided into “sub-models”. Figure 3(a) provides an\nexample. We report that the most appropriate regression model in terms of query times and reduction\nfactor is the cubic one. We omit those results for brevity and to keep our contribution focused on the\nimportant ﬁndings. However, they can be found in [4]. For this reason, the cubic model, indicated in\nthe rest of the manuscript by C, is the only one that is included in what follows.\nA Two-Level Hybrid Model, with Constant Branching Factor\n•KO-US: Learned k-ary Search . This model partitions the table into a ﬁxed number of segments,\nbounded by a small constant, i.e. at most 20 in this study, in analogy with a single iteration of the\nk-ary Search routine [40,41]. An example is provided in Figure 3(b). For each segment, Atomic Models\nare computed to approximate the CDF of the table elements in that segment. Finally, the model that\nguarantees the best reduction factor is assigned to each segment. As for the prediction, a sequential\nsearch is performed for the second level segment to pick and the corresponding model is used for the\nprediction, followed by Uniform Binary Search, since it is superior to the Standard one (data not\nreported and available upon request). We anticipate that for the experiments conducted in this study,\nkhas been chosen in the interval [3 ,20]. For conciseness, only results for the model with k= 15 are\nreported, since it is the value with the best performance in terms of query time (data not reported and\navailable upon request). Accordingly, from now on, KO-US indicates the Model with k= 15.\nTwo-Level RMIs with Parametric Branching Factor\n•Heuristically Optimized RMIs. Informally, an RMI is a multi-level, directed graph, with Atomic\nModels at its nodes. When searching for a given key and starting with the ﬁrst level, a prediction at\neach level identiﬁes the model of the next level to use for the next prediction. This process continues\nuntil a ﬁnal level model is reached. This latter is used to predict the table interval to search into.\nAs pointed out in the benchmarking study, in most applications, a generic RMI with two layers, a\ntree-like structure and a branching factor bsuﬃces. An example is provided in Figure 4(a). It is\nto be noted that Atomic Models are RMI s. Moreover, the diﬀerence between Learned k-ary Search\nandRMI s is that the ﬁrst level in the former partitions the table, while that same level in the latter\npartitions the Universe of the elements. Following the benchmarking study and for a given table, we\nuse two-layers RMI s that we obtain using the optimization software provided in CDFShop , that\nreturns up to ten versions of the generic RMI , for a given input table. That is, for each model, the\noptimization software picks an appropriate branching factor and the type of regression to use within\neach part of the model, those latter quantities being the parameters that control the precision of its\nprediction as well as its space occupancy. It is also to be remarked, as pointed out in [32], that the\noptimization process provides only approximations to the real optimum and it is heuristic in nature,\nwith no theoretic approximation performance guarantees. The problem of ﬁnding an optimal model in\npolynomial time is open.\n•SY-RMI: A Synoptic RMI. For a given set of tables of approximately the same size, we use\nCDFShop as above to obtain a set of models (at most 10 for each table). For the entire set of models\nso obtained and each model in it, we compute the ratio (branching factor)/(model space) and we take\nthe median of those ratios as a measure of branching factor per unit of model space, denoted UB.\nAmong the RMI s returned by CDFShop , we pick the relative majority winner, i.e., the one that\nprovides the best query time, averaged over a set of simulations. When one uses such a model on\ntables of approximately the same size as the ones used as input to CDFShop , we set the branching\nfactor to be a multiple of UB, that depends on how much space the model is expected to use relative to\nthe input table size. Since this model can be intuitively considered as the one that best summarizes the\noutput of CDFShop in terms of query time, for the given set of tables. The ﬁnal model is informally\nreferred to as Synoptic.\n7\n\nCDF Approximation-Controlled Models\n•PGM [14]. It is also a multi-stage model, built bottom-up and queried top down. It uses a user-deﬁned\napproximation parameter /epsilon1, that controls the prediction error at each stage. With reference to Figure\n4(b), the table is subdivided into three pieces. A prediction in each piece can be provided via a linear\nmodel guaranteeing an error of /epsilon1. A new table is formed by selecting the minimum values in each of\nthe three pieces. This new table is possibly again partitioned into pieces, in which a linear model can\nmake a prediction within the given error. The process is iterated until only one linear model suﬃces,\nas in the case in the ﬁgure. A query is processed via a series of predictions, starting at the root of\nthe tree. Also in this case, for a given table, at most ten models have been built as prescribed in the\nbenchmarking study with the use of the parameters, software and methods provided there, i.e, SOSD .\nIt is to be noted that the PGM index, in its bi-criteria version, is able to return the best query\ntime index, within a given amount of space the model is supposed to use. Experiments are performed\nalso with this version of the PGM , denoted for brevity as B-PGM . The interested reader can ﬁnd a\ndiscussion regarding more variants of this PGM version in [4].\n•RS[20]. It is a two-stage model. It also uses a user-deﬁned approximation parameter /epsilon1. With reference\nto Figure 4(c), a spline curve approximating the CDF of the data is built. Then, the radix table is\nused to identify spline points to use to reﬁne the search interval. Also in this case, we have performed\nthe training as described in the benchmarking study.\nIn what follows, for ease of reference, models in the ﬁrst two classes are referred to as constant space\nmodels, while the ones in the remaining classes as parametric space models.\n(a)\n{1\n5\n11\n14\n58\n59\n60\n97\n100\n101Cubic (b)\nCubic Linear Quadratic1 511 14 59 60 97100 58\nFigure 3: Examples of Various Learned Indexes that Use Constant Space . (a) an Atomic Model, where the box Cubic means\nthat the CDF of the entire dataset is estimated by a cubic function via Regression, in analogy with the linear approximation exempliﬁed\nin Figure 2. (b) An example of a KO-US , with k= 3. The top part divides the table into three segments and it is used to determine\nthe model to pick at the second stage. Each box indicates which Atomic Model is used for prediction on the relevant portion of the\ntable.\n3 Experimental Methodology\nOur experimental set-up follows closely the one outlined in the already mentioned benchmarking study by\nMarcus et al [29]. Since an intent of this study is to gain deeper insights regarding the circumstances in\nwhich Learned versions of Sorted Table Search procedure and Indexes are proﬁtable in small additional space\nwith respect to the one taken by the input table, accross the main memory hierarchy, we derive our own\nbenchmark datasets from the ones in the study by Marcus et al [29].\n3.1 Hardware\nAll the experiments have been performed on a workstation equipped with an Intel Core i7-8700 3.2GHz CPU\nwith three levels of cache memory: (a) 64kb of L1cache; (b) 256kb of L2cache; (c)12Mb of shared L3\n8\n\n(a)\nLinear\nCubic1 2Cubic Cubicn.......Linear\n1 Linear1 2Quadraticb C u b ic\n151114 596097100100 578590630 58{{ {\n....... (b)\n{{ {\n{{\nkey:1 Model:fkey:58 Model:f key:97 Model:fkey:58 Model:f key:1 Model:f\n1 2 345\n15897\n1511 14 58 97 59 101 100 60 (c)\n0123 4 5 6 7\nkey\nIndex\nFigure 4: Examples of Various Learned Indexes that Use Space in Fuction of Some Parameters (see also [29]). (a) An\nexample of an RMI with two layers and branching factor equal to b. The top box indicates that the lower models are selected via a\nlinear function. As for the leaf boxes, each indicates which Atomic Model is used for prediction on the relevant portion of the table.\n(b) An example of a PGM Index. At the bottom, the table is divided into three parts. A new table is so constructed and the process\nis iterated. (c) An example of an RS Index. At the top, the buckets where elements fall, based on their three most signiﬁcant digits.\nAt the bottom, a linear spline approximating the CDF of the data, with suitably chosen spline points. Each bucket points to a spline\npoint so that, if a query element falls in a bucket (say six), the search interval is limited by the spline points pointed to by that bucket\nand the one preceding it (ﬁve in our case).\ncache. The clsandsize, deﬁned in Section 2.3 are respectively 64 and 8 bytes. The total amount of system\nmemory is 32 Gbyte of DDR4. The operating system is Ubuntu LTS 20.04.\n3.2 Datasets\nThe same real datasets of the benchmarking study are used. In particular, attention is restricted to integers\nonly, each represented with 64 bits unless otherwise speciﬁed. For the convenience of the reader, a list of\nthose datasets, with an outline of their content, is provided next.\n•amzn : book popularity data from Amazon. Each key represents the popularity of a particular book.\nAlthough two versions of this dataset, i.e., 32-bit and 64-bit, are used in the benchmarking, no particular\ndiﬀerences are observed in the results of our experiments, and for this reason we report only those for\nthe 64-bit dataset. The interested reader can ﬁnd the results for the 32 bits version in [4].\n•face: randomly sampled Facebook user IDs. Each key uniquely identiﬁes a user.\n•osm: cell IDs from Open Street Map. Each key represents an embedded location.\n•wiki: timestamps of edits from Wikipedia. Each key represents the time an edit was committed.\nMoreover, for the purpose of this research, as already mentioned above, additional datasets are extract\nfrom the ones just mentioned. For each of those datasets, three new ones are obtained in order to ﬁt each\nlower level of the internal memory hierarchy. In particular, each new dataset is obtained by sampling the\noriginal one so that the CDF is similar to the original one. The interested reader can ﬁnd more details of this\nextraction procedure in [4]. Letting nbe the number of elements in a table, for the computer architecture\nthat is been used, the details of the generated tables are the following.\n•Fitting in L1 cache: cache size 64Kb. Therefore,n= 3.7Kis chosen.\n•Fitting in L2 cache: cache size 256Kb. Therefore,n= 31.5Kis chosen.\n•Fitting in L3 cache: cache size 8Mb. Therefore,n= 750Kis chosen.\n•Fitting in PC Main Memory (L4): memory size 32Gb. Therefore,n= 200Mis chosen, i.e.,\nthe entire dataset.\n9\n\n(a)\n0 2 4 6 8\n1e180.00.51.01.52.01e8 amzn\n0.0 0.2 0.4 0.6 0.8 1.0\n1e190.00.51.01.52.01e8 face\n0.0 0.5 1.0\n1e190.00.51.01.52.01e8 osm\n1.00 1.05 1.10 1.15 1.20\n1e90.00.51.01.52.01e8 wiki (b)\n0 2 4 6 8\n1e180200000400000600000amzn\n0 2 4 6 8\n1e100200000400000600000face\n0.0 0.5 1.0\n1e190200000400000600000osm\n1.0 1.1 1.2\n1e90200000400000600000wiki\nFigure 5: TheCDF of the Main Datasets . For each dataset coming from the benchmarking study, the value of each of its elements\nis reported on the abscissa and its position on the ordinate. In particular, Figure (a) is referred to the L4memory level, while (b) to\nL3.\nThe rationale for the choice of those datasets, in particular the ones coming from the benchmarking\nstudy, is that they provide diﬀerent Empirical CDF , as shown in Figure 5(a), and this allows to measure\nthe performance of Learned Indexes considering diﬀerent possible characteristics of real-world data. It is\nto be noted that the face dataset is somewhat special. Indeed, the shape of its CDF (see Figure 5(a)) is\ndetermined by 21 outliers at the end of the table: all the elements of that dataset, up to the ﬁrst outlier,\nhave essentially the same distance between consecutive elements. That is, they are all on a straight line.\nThis regularity breaks with the ﬁrst outlier that, together with the other ones, do not follow such a nice\npattern. For lower memory levels, the CDF of the corresponding face datasets becomes a straight line, as\nexempliﬁed in Figure 5(b) for the L3memory level. As for the remaining datasets, their smaller versions\nfollow closely the CDF of the biggest datasets, as again exempliﬁed in Figure 5(b) for the L3memory level.\nAs for query dataset generation, for each of the tables built as described above, we extract uniformly\nand at random (with replacement) from the Universe Ua total of two million elements, 50% of which are\npresent and 50% absent, in each table.\n4 Training of the Novel Models: Analysis and Insights into Model\nTraining\nWe now focus on the training phase of the novel Models and we compare their performance with the Literature\nstandards included in this research. In order to assess how well a Learned Index Model can be trained, three\nindicators are important: the time required for learning, the reduction factor that one obtains and the time\nneeded to perform the prediction. A quantiﬁcation of the ﬁrst parameter is provided and discussed here.\nThe other two indicators are strongly dependent on each other, with the reduction factor being related to\nspace. In turn, those two indicators aﬀect query time. Therefore, they are best discussed in Section 5. We\nanticipate that our analysis of the training time performed here provides useful and novel insights into model\ntraining for Learned Indexing. All the training experiments have been performed on the datasets mentioned\nin Section 3.2, across all internal memory levels.\n4.1 Mining SOSD Output for the Synoptic RMI\nAs anticipated in Section 2.3, in order to set the levels and UBof the Synoptic RMI , it is necessary to\nprocess the output of SOSD for each dataset and memory level. Indeed, as described in Section 2.3, once\nit is set a space budget for the model, the corresponding branching factor is computed by multiplying it by\nUB. In particular, we compute three versions of a Synoptic RMI using a percentage of space of 0.05%, 0.7%\n10\n\nand 2% with respect to the input table size. With regard to the layers choice, the simulation to identify\nthe relative majority RMI s is performed on query datasets extracted as described in the previous Section,\nbut using only 1% of the number of query elements speciﬁed there. The statistics regarding the results of\nsuch a simulation are summarized in Figure 6. In particular, for each memory level, we report the computed\nUB. Furthermore, limited to the top layer of an RMI , we also report the models associated with the best\nones. The time it took to identify the proper Synoptic RMI (average time per element, over all RMI s\nreturned by CDFShop , denoted as mining time) is also reported, together with the same time required to\nobtain the output of CDFShop . As it is evident, the mining time is comparable with the performance of\nCDFShop . It is also evident from that Figure that the variety of best-performing models represents well\nvarious challenges for the learning of the CDF of real datasets. Therefore, given such a variety, it is far from\nobvious that the median UBis the same for each memory level. Moreover, the relative majority model is\nalso the same across memory levels, i.e., linear spline, with linear models for each segment of the second\nlayer.\n4.2 Training Time Comparison Between Novel Models and the State of the Art\nIn what follows, we divide the training time comparison into two groups: Constant and Parametric Space\nModels. As for the ﬁrst group, we consider the new Model and only the Cubic Atomic Model, excluding\nthe Linear and Quadratic ones for the reasons mentioned earlier in this paper. For the Cubic Model, the\ntraining time on a given dataset is due to the computation of its parameters via a Polynomial Regression.\nAs for the Learned k-ary Search Model, its training consists of partitioning the table into ksegments. Then,\nfor each segment, Atomic Models are used to approximate the local CDF of the elements belonging to that\nsegment, and among them, the Model with the best reduction factor is chosen. For each dataset and each\nmemory level, the resulting training times are reported in Table 1 and Tables 1-3 of the Supplementary File.\nAs expected, the Learned k-ary Search Model is slower than the Cubic Atomic Model, but the important\nfact is that the slowdown is due to constant multiplicative factors rather than order of magnitude. That\nis, the slow-down is tolerable. Another additional and counter intuitive ﬁnding is that the training time of\nboth Models, on average, is better for the cases of big datasets with respect to smaller ones. We analyzed\nthe training code in order to get insights into such a fact. It turns out that the cost of the matrix products\ninvolved in the training computation of both models depends on the size of the involved operands. As the\nsize of the dataset grows, such a cost is amortized on a larger and larger number of elements.\nRegarding the second group, we consider the new model and the ones described in Section 2.3, i.e., RMI ,\nPGM andRS. The training time is computed using two diﬀerent platforms: CDFShop in the case of the\nSynoptic RMI andRMI , and SOSD forPGM andRS. It is useful to recall that, in the case of the State\nof the Art Models, the result of a single execution of those two platforms returns a batch of up to ten models,\nso the reported times refer to the execution of the entire learning suite. That is, the training of those models\nconsists of a batch of model instances from which a user can choose. On the other hand, for the Synoptic\nRMI , it is referred to the training of a single RMI with a given branching factor and layers composition. For\neach dataset and each memory level, the results are reported in Table 2 and Tables 4-6 of the Supplementary\nFile. The time needed to train the Synoptic RMI is comparable to the one needed to train a batch of\nRMI Models. This latter, as already known, being worse than the time to train a batch of RSorPGM\nModels. Such a State of the Art is not considered problematic for the deployment of the RMI in application\ncontexts and the training time of the Synoptic RMI is in-line with the mentioned Literature Standards. For\ncompleteness, we mention that the reason for which the time needed to train a unique Synoptic RMI Model\nis very close to the training of a batch on RMI Models is due to a library start-up overhead time. Such a\ntime is mitigated for the case of the training of a batch of models, while it becomes dominant in training a\nsingle model. Fortunately, the CDFShop or the SOSD training executions are a “one time only” process,\nin which the output can then be reused over and over again, suggesting that this overhead time is of little\nrelevance for the case of a production environment.\n11\n\nlinear_spline robust_linearradix22cubic\nT op Layer050100%L1\nMining Time:        2.8e-06\nCDF Shop Time:    1.44e-06\nUB: 0.042\nradix\nlinear_splineradix18\nT op Layer050100%L2\nMining Time:        6.6e-06\nCDF Shop Time:    6.28e-06\nUB: 0.042\nlinear_splinelinear\nradix18\nT op Layer050100%L3\nMining Time:        1e-06\nCDF Shop Time:    7.50e-07\nUB: 0.042\nradix18\nlinear_spline robust_linearradix22\nT op Layer050100%L4\nMining Time:        1.1e-07\nCDF Shop Time:    2.12E-07\nUB: 0.042Figure 6: Time and UBfor the Identiﬁcation of the Synoptic RMIs. For each memory level, only the top layer of the various\nmodels is indicated in the abscissa, while the ordinate indicates the number of times, in percentage, the given model is the best in\nterms of query performance on a table. On top of each histogram, it is reported the branching factor per unit of space as well as the\nmining time to build the synoptic Models. For comparison, we also report the same time spent in obtaining the output of CDFShop.\n4.3 Insights into the Training Time of the RS and PGM Models\nAnother important contribution that this research provides is a more reﬁned assessment of the relation\nbetween the RSandPGM indexes, in terms of training time. In Table 3, for each dataset and memory\nlevel, we report the training time of those two indexes. As well discussed in [20], those two Learned Indexes\ncan both be built in one pass over the input, with important implications, one being that they can be\ntrained faster than the RMI s, even with one order of magnitude speed-up. However, in that study as well\nas in the benchmarking one, the RSis reported as superior to the PGM in terms of training time. It is\nto be noted that the datasets that they used are the largest ones in this study. With reference to Table\n3, our experiments conﬁrm such a ﬁnding. On the other hand, the PGM is “more eﬀective” in terms of\ntraining time across the lower memory hierarchy. The reason may be the following. Those two indexes both\nuse streaming procedures in order to approximate the CDF of the input dataset within a parameter /epsilon1, via\nthe use of straight line segments that partition the Universe. The main diﬀerence between the two is that\nthe latter ﬁnds an optimal partition, determined via a well-known algorithm (see references in [14]), while\nthe former ﬁnds a partition that approximates the optimal, as described in [36]. Such an approximation\nalgorithm is supposed to be faster than the optimal one but apparently this speed pays oﬀ on large datasets.\n12\n\nTable 1: Constant Space Models Training Time for L4 Tables . The ﬁrst column indicated the datasets. The remaining columns\nindicate the model used for the learning phase. Each entry reports the training time in seconds and per element.\nKO-US C\namzn 3.7e-08 1.4e-08\nface 3.6e-08 1.4e-08\nosm 3.6e-08 1.4e-08\nwiki 3.6e-08 1.4e-08\nTable 2: Paramentric Models Training Time for L4 Tables . The ﬁrst column indicated the datasets. The remaining columns\nindicate the model used for the learning phase. In particular, each entry reports the time to train the Synoptic RMI and an entire\nbatch of models via the CDFShop and the SOSD libraries as speciﬁed in the Main text. The time is in seconds and per element.\nCDFShop SY-RMI 2% CDFShop RMI SOSD RS SOSD PGM\namzn 1.1e-06 2.2e-06 2.1e-07 5.0e-07\nface 1.3e-06 2.5e-06 2.1e-07 6.5e-07\nosm 1.2e-06 2.5e-06 2.2e-07 7.4e-07\nwiki 1.1e-06 2.2e-06 1.9e-07 4.1e-07\nTable 3: Comparison between RS and PGM Training Time . For each dataset and memory level, we report the training time for\ntheRSandPGM models in seconds\nL1 L2 L3 L4\nSOSD RS SOSD PGM SOSD RS SOSD PGM SOSD RS SOSD PGM SOSD RS SOSD PGM\namzn 3.5e-06 5.0e-07 3.5e-07 5.0e-08 2.4e-08 3.4e-08 2.1e-07 5.0e-07\nface 1.1e-06 3.9e-07 1.1e-07 3.9e-08 1.4e-08 2.4e-08 2.1e-07 6.5e-07\nosm 6.9e-06 4.0e-07 6.9e-07 4.0e-08 3.5e-08 3.8e-08 2.2e-07 7.4e-07\nwiki 1.0e-05 3.7e-07 1.0e-06 3.7e-08 5.1e-08 3.7e-08 1.9e-07 4.1e-07\n13\n\n5 Query Experiments\nThe query experiments are performed using all the methods described in Sections 2.1 and 2.3. The query\ndatasets have been generated as described in Section 3.2 and the models have been trained as described\nin Section 4. Following that Section, we divide the presentation of the query experiments and the relative\ndiscussion into two groups. For both groups, for conciseness, we report here only the experiments for the\namzn and the osm datasets since they are representative of two diﬀerent levels of diﬃculty in learning their\nCDF s. The results regarding the other datasets are reported in the Supplementary File.\n5.1 Constant Space Models\nThe results of the experiments for this group of Models are reported in Figures 7 and 8 for the amzn and\ntheosm datasets, respectively, and in Figures 2 and 3 of the Supplementary File, for the remaining ones.\nIn those ﬁgures, only the query time for Uniform Binary Search is reported, since the results are analogous\nto the ones obtained by using the Standard routine. In addition, the query time for the Eytzinger Binary\nSearch is also reported, as an useful baseline, because of its superiority among the classic routines that take\nconstant additional space with respect to the size of the input table, as discussed in [18]. From the mentioned\nﬁgures, it is evident that the query performance of each model considered here is highly inﬂuenced by how\ndiﬃcult to learn is the CDF of the input table, as explained next.\n•The Cubic Model achieves a high reduction factor, i.e. ≈99%, on the versions of the face dataset for\nthe ﬁrst three levels of the internal memory hierarchy and it is also the best performing, even compared\nto the Eytzinger Layout routine. This is a quite remarkable achievement, but the involved datasets\nhave an almost uniform CDF , while a few outliers disrupt such a uniformity on the L4version of that\ndataset (see Figure 5 and the discussion regarding the face dataset in Section 3.2).\n•The Learned k-ary Search Model achieves a high reduction factor on all versions of the amzn and the\nwiki datasets, i.e.,≈99.73, and it is faster than Uniform Binary Search and the Cubic Model. Those\ndatasets have a regular CDF across all the internal memory levels. It is to be noted that the Eytzinger\nLayout routine is competitive with the Learned k-ary Search Model.\n•No constant space Learned Model wins on the diﬃcult to learn dataset. The osm dataset has a CDF\ndiﬃcult to learn (see Figure 5) and such a characteristic is preserved across the internal memory levels.\nThe Learned k-ary Search Model achieves a quite respectable reduction factor, i.e., ≈98%, but no\nspeed-up with respect to Uniform Binary Search. In order to get insights into such a counter-intuitive\nbehaviour, we have performed an additional experiment. For each representative dataset and as far as\nthe Learned k-ary Search Model is concerned, we have computed two kinds of reduction factors: the\nﬁrst is the “global” one, achieved considering the size of the entire table, while the second is the “local”\none, computed as the average among the reduction factors of each segment. Those results are reported\nin Table 4. For the osm dataset, it is evident that the “local” reduction factors are consistently lower\nthan the “global” ones, highlighting that its CDF is also locally diﬃcult to approximate, which in turn\nimplies an ineﬀective use of the local prediction for the Learned k-ary Search, resulting in poor time\nperformance. Finally, it is to be noted that the Eytzinger Layout routine is the best performing.\nIn conclusion, in applications where there is a constant space constraint with respect to the input table\nand a layout other than sorted can be used, then the Eytzinger Binary Search is still the best choice, unless\ntheCDF of the input dataset is particularly easy to approximate. If such a layout cannot be aﬀorded,\nthe best choice is the use of a constant space Model, in particular the Learned kary Search Model only for\ndatasets with a CDF simple to approximate, otherwise the use of Uniform Binary Search alone is indicated.\nIt is also of interest to pointed out that our research extends the results in [18] regarding the Eytzinger\nBinary Search routine: even compared to Learned Indexes that use constant space, it still results to be\ncompetitive and, many times superior to them.\n14\n\nNM-0% C-95.98% K0-US-99.94%00.20.40.60.811.2\n10-7 L1\nNM-0% C-96.02% K0-US-99.98%00.20.40.60.811.21.410-7 L2\nNM-0% C-95.94% K0-US-99.98%00.511.522.510-7 L3\nNM-0% C-96.03% K0-US-99.98%012345678\n10-7 L4US\nEBFigure 7: Constant Space Models Query Times for the amzn Dataset . For each memory level, the blue bar reports the average\nquery time of Uniform Binary Search using, from left to right, no model, Cubic model and KO-US . In addition, we report the average\nquery time also for the Eytzinger Binary Search in the orange bar.\n5.2 Parametric Space Models\nFor the convenience of the reader, we recall that the Model classes involved are: RMI ,RS,PGM , the\nSynoptic RMI and the bi-criteria PGM , which are trained on the input datasets (see Section 3.2), as\nreported in Section 4. The batch of queries used here are obtained as described in Section 3.2. For each\nof the ﬁrst three Model classes, we consider, among the trained Models, the fastest in terms of query time\nand that takes less than 10% of space, with respect to the one taken by the input table. For the other\ntwo Model classes, we consider three increasing bounds on space, i.e., 0 .05%, 0.7% and 2%, with respect\nto the space of the table alone, and take the average query time. Moreover, as a measure of the Learned\nIndexes speed-up, we report also the query time of Uniform Binary Search. The results of the corresponding\nexperiments are reported in Figures 9 and 10 for amzn andosm datasets, respectively, and in Figures 4\nand 5 of the Supplementary File, for the remaining ones.\nAn interesting ﬁnding is that both the Synoptic RMI and the bi-criteria PGM perform better than\nUniform Binary Search across datasets and memory levels using very little additional space. That is, one\ncan enjoy the speed of Learned Indexes with a very small space penalty. Moreover, it is important to note\nthat, except for the L1memory level, the space of those two Models is very close to the user-deﬁned bound.\nFurthermore, in terms of query performances, such two Models seem to be complementary. In fact, the bi-\ncriteria PGM performs better on the L1andL4memory levels, while the Synoptic RMI on the remaining\nones. This complementary and good control of space make those two models quite useful in practice.\nIn addition to those ﬁndings, our research provides some more insights into the relation time-space in\n15\n\nNM-0% C-22.84% K0-US-98.12%00.20.40.60.811.21.410-7 L1\nNM-0% C-21.19% K0-US-98.07%00.511.5210-7 L2\nNM-0% C-14.50% K0-US-97.98%00.511.522.5310-7 L3\nNM-0% C-22.70% K0-US-98.03%00.20.40.60.8110-6 L4US\nEBFigure 8: Constant Space Models Query Times for the osm Dataset. The ﬁgure legend is as in Figure 7.\nLearned Indexes, extending the results in the benchmark study, as we now discuss.\n•Space Constraints and the Models Provided by SOSD. We have ﬁxed a rather small space\nbudget, i.e., at most 10% of additional space in order for a Model returned by SOSD to be considered.\nTheRSIndex is not competitive with respect to the other Learned Indexes. Those latter consistently\nuse less space and time, across datasets and memory levels. As for the RMI s coming out of SOSD ,\nthey are not able to operate in a small space at the L1memory level. On the other memory levels,\nthey are competitive with respect to the bi-criteria PGM and the Synoptic RMI , but they require\nmore space with respect to them.\n•Space, Time, Accuracy of Models. As stated in the benchmarking study, a common view of\nLearned Indexing Data Structures is as a CDF lossy compressor, see also [14, 24]. In this view, the\nTable 4: Global and Local Reduction Factors . For the two representative datasets, i.e. amzn andosm, and for each memory\nlevel, in each entry, we report the global reduction factor (left) and the local one (right).\namzn osm\nL1 99.94 - 99.48 98.12 - 86.70\nL2 99.98 - 99.56 98.07 - 86.57\nL3 99.98 - 99.53 97.98 - 86.43\nL4 99.98 - 99.54 98.03 - 86.57\n16\n\nTable 5: A Synoptic Table of Space, Time and Accuracy of Models on amzn Dataset . For each memory level, we report in\nthe ﬁrst row the best performing method for that memory level. The columns named time, space and reduction factor indicate for this\nbest model, the average query time in seconds, the average additional space used in Kb and the average of the empirical reduction\nfactor. From the second row, we report the versions of the RMI ,RS,PGM and Synoptic RMI models that use the least space. In\nparticular, the number next to the Models represent in percentage the bound on the used space with respect to the input dataset. The\ncolumns indicate now the ratio Model/best Model of the time, space and reduction factor.\nL1\nTime Space Reduction Factor\nBest RMI 1.89e+01 3.09e+00 99.84\nB-PGM 0.05 4.03e+00 1.30e-02 2.50e-01\nSY-RMI 0.05 3.77e+00 2.85e-02 1.75e-01\nRS<10 2.58e+00 7.06e-01 9.29e-01\nBest RMI 1.00e+00 1.00e+00 1.00e+00\nL2\nTime Space Reduction Factor\nBest RMI 2.51e+01 6.16e+00 99.97\nB-PGM 0.05 3.78e+00 1.62e-02 9.16e-01\nSY-RMI 0.05 3.74e+00 2.60e-02 6.44e-01\nBest RS <10 2.38e+00 3.68e-01 9.92e-01\nBest RMI 1.00e+00 1.00e+00 1.00e+00L3\nTime Space Reduction Factor\nBest RMI 4.70e+01 6.29e+03 100.00\nB-PGM 0.05 2.07e+00 3.05e-04 1.00e+00\nSY-RMI 0.05 1.49e+00 4.79e-04 9.99e-01\nRS<10 1.59e+00 4.00e-02 1.00e+00\nRMI<10 1.03e+00 6.25e-02 1.00e+00\nL4\nTime Space Reduction Factor\nBest RMI 1.51e-07 2.01e+05 100.00\nB-PGM 0.05 1.85e+00 3.93e-03 1.00e+00\nSY-RMI 0.05 1.18e+00 3.97e-03 1.00e+00\nBest RS 1.19e+00 7.16e-02 1.00e+00\nRMI<10 1.03e+00 5.00e-01 1.00e+00\nquality of a Learned Index can be judged by the size of the structure and its reduction factor. In\nthat study, it is also argued that this view does not provide an accurate selection criterion for Learned\nIndexes. Indeed, it may very well be that an index structure with a excellent reduction factor takes\na long time to produce a search bound, while an index structure with a worse reduction factor that\nquickly generates an accurate search bound may be of better use. In the benchmarking study, they\nalso provide evidence that the space/time trade-oﬀ is the key factor in determining which Model to\nchoose. Our contribution is to provide additional results supporting those ﬁndings. To this end, we\nhave conducted several experiments, whose results are reported in Tables 5-6 and Tables 7-8 on the\nSupplementary File. In such Tables, for each dataset, we report a synopsis of three parameters, i.e.,\nquery time, space used in addition by the model and reduction factor, across all datasets and memory\nlevels. In particular, for each dataset, we compare the best-performing model with all the ones that use\nsmall space, taking, for each parameter, the ratio Model/Best Model. The ratio values are reported\nfrom the second row of the table, the ﬁrst row shows the average values of the parameters for the best\nmodel. First, it is useful to note that, even in a small space model, it is possible to obtain a very\nwell, if not nearly perfect, prediction (i.e. very high reduction factor). However, prediction power is\nsomewhat marginal to assess performance. Indeed, across memory levels, we see a space classiﬁcation\nof model conﬁgurations. The most striking feature of this classiﬁcation is that the gain in query time\nbetween the best model and the others is within small constant factors, while the diﬀerence in space\noccupancy may be in most cases several orders of magnitude. That is, space is the key to eﬃciency.\n6 Conclusions and Future Directions\nIn this research, we have provided a systematic experimental analysis regarding the ability of Learned Model\nIndexes to perform better than Binary Search in small space. Although not as simple as it seems, we show\nthat this is indeed possible. However, our results also indicate that there is a big gap between the best\nperforming methods and the others we have considered and that operate in small space. Indeed, the query\ntime performance of the latter with respect to the former is bounded by small constants, while the space\nusage may diﬀer even by ﬁve orders of magnitude. This brings to light the acute need to investigate the\nexistence of “small space” models that should close the time gap mentioned earlier. Another important\naspect, with potential practical impact, is to devise models that can work on layouts other than Sorted, i.e.,\nEytzinger. Finally, given that Eytzinger Binary Search is consistently faster than Binary Search for datasets\nﬁtting in main memory, an investigation of how diﬀerent variants of Binary Search perform in SOSD , using\n17\n\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%020406080100120\n        L1\n7.27%0.93% 0.13%0.29% 0.60%0.85% 0.60%2.05%\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%020406080100120140\n       L2\n2.44%0.90%0.27% 0.04%0.06% 0.15%0.75% 0.15%2.01%\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%050100150200250\n       L3\n6.55%4.20%0.15% 0.03%0.05% 0.03%0.74% 0.03%2.00%\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%0100200300400500600700\n       L4\n6.29%3.35%2.82% 0.05%0.05% 0.69%0.74% 0.96%2.00%RMI\nRS\nPGM\nB-PGM\nSY-RMI\nUSFigure 9: Query times for the amzn dataset on Learned Indexes in Small Space. The methods are the ones in the legend\n(middle of the four panels, the notation is as in the main text and each method has a distinct colour). For each memory level, the\nabscissa reports methods grouped by space occupancy, as speciﬁed in the main text. When no model in a class output by SOSD\ntakes at most 10% of additional space, that class is absent. The ordinate reports the average query time, with Uniform Binary Search\nexecuted in SOSD as baseline (horizontal lines).\nboth new and State of the Art models, also deserves to be investigated.\n7 Acknowledgement\nThe authors are deeply indebted to the Associate Editor and the two Referees, for the very stimulating\ncomments received during the entire revision process. This research is funded in part by MIUR Project\nof National Relevance 2017WR7SHH “Multicriteria Data Structures and Algorithms: from compressed to\nlearned indexes, and beyond”. Additional support to RG has been granted by Project INdAM - GNCS\n“Modellizazzione ed analisi di big knowledge graphs per la risoluzione di problemi in ambito medico e web”.\nReferences\n[1] Alfred V. Aho, John E. Hopcroft, and Jeﬀrey D. Ullman. The Design and Analysis of Computer\nAlgorithms. 1974.\n18\n\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%020406080100120\n    L1\n8.40% 0.13%0.29% 0.60%0.85% 1.73%2.05%\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%020406080100120140\n    L2\n4.88%9.88%7.56% 0.04%0.06% 0.67%0.75% 2.07%2.01%\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%050100150200250300\n    L3\n1.64%1.78%6.50% 0.05%0.05% 0.69%0.74% 1.77%2.00%\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%0100200300400500600700\n    L4\n6.29%7.95%7.43% 0.05%0.05% 0.70%0.74% 1.74%2.00%RMI\nRS\nPGM\nB-PGM\nSY-RMI\nUSFigure 10: Query times for the osm dataset on Learned Indexes in Small Space. The ﬁgure legend is as the one in Figure 9.\n[2] D. Amato, G. Lo Bosco, and R. Giancarlo. Learned sorted table search and static indexes in small\nmodel space (Extended Abstract). In Proc. of the 20-th Italian Conference in Artiﬁcial Intelligence\n(AIxIA), to appear in Lecture Notes in Computer Science , 2021.\n[3] D. Amato, G. Lo Bosco, and R. Giancarlo. On the suitability of neural networks as building blocks\nfor the design of eﬃcient learned indexes. In Lazaros Iliadis, Chrisina Jayne, Anastasios Tefas, and Elias\nPimenidis, editors, Engineering Applications of Neural Networks , pages 115–127, Cham, 2022. Springer\nInternational Publishing.\n[4] Domenico Amato. A Tour of Learned Static Sorted Sets Dictionaries: From Speciﬁc to Generic with\nan Experimental Performance Analysis . PhD thesis, 2022.\n[5] Domenico Amato, Giosu´ e Lo Bosco, and Raﬀaele Giancarlo. Standard versus uniform binary search\nand their variants in learned static indexing: The case of the searching on sorted data benchmarking\nsoftware platform. Software: Practice and Experience) , 2022.\n[6] Burton H. Bloom. Space/Time Trade-Oﬀs in Hash Coding with Allowable Errors. Commun. ACM ,\n13:422–426, 1970.\n[7] A. Boﬀa, P. Ferragina, and G. Vinciguerra. A “learned” approach to quicken and compress rank/select\ndictionaries. In Proceedings of the SIAM Symposium on Algorithm Engineering and Experiments\n(ALENEX) , 2021.\n19\n\nTable 6: A Synoptic Table of Space, Time and Accuracy of Models on osm Dataset . The legend is as in 5\nL1\nTime Space Reduction Factor\nBest RMI 2.72e+01 1.15e+03 99.87\nB-PGM 0.05 2.31e+00 3.49e-05 1.74e-01\nSY-RMI 0.05 2.60e+00 7.67e-05 2.30e-01\nBest RMI 1.00e+00 1.00e+00 1.00e+00\nBest RS 1.19e+00 4.33e+01 9.99e-01\nL2\nTime Space Reduction Factor\nBest RMI 3.93e+01 1.84e+03 99.97\nB-PGM 0.05 2.68e+00 5.45e-05 7.75e-01\nSY-RMI 0.05 3.11e+00 8.72e-05 7.24e-01\nRMI<10 1.73e+00 6.71e-03 9.87e-01\nRS<10 1.93e+00 1.36e-02 9.79e-01L3\nTime Space Reduction Factor\nBest RS 7.06e+01 4.63e+04 100.00\nB-PGM 0.05 2.40e+00 6.22e-05 9.98e-01\nSY-RMI 0.05 2.63e+00 6.52e-05 9.31e-01\nRMI<10 1.75e+00 2.12e-03 9.97e-01\nRS<10 1.55e+00 2.31e-03 1.00e+00\nL4\nTime Space Reduction Factor\nBest RS 2.04e-07 5.08e+05 100.00\nSY-RMI 0.05 2.52e+00 1.57e-03 9.99e-01\nB-PGM 0.05 2.03e+00 1.59e-03 1.00e+00\nRMI<10 1.18e+00 1.98e-01 1.00e+00\nRS<10 1.05e+00 2.50e-01 1.00e+00\n[8] Danny Ziyi Chen and Haitao Wang. Approximating points by a piecewise linear function. Algorithmica ,\n66:682–713, 2012.\n[9] D. Comer. Ubiquitous B-Tree. ACM Computing Surveys (CSUR) , 11(2):121–137, 1979.\n[10] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Introduction to Algorithms, Third Edition .\nThe MIT Press, 3rd edition, 2009.\n[11] Zhenwei Dai and Anshumali Shrivastava. Adaptive learned bloom ﬁlter (ada-bf): Eﬃcient utilization\nof the classiﬁer with application to real-time information ﬁltering on the web. Advances in Neural\nInformation Processing Systems , 33:11700–11710, 2020.\n[12] J. Ding, U. F. Minhas, J. Yu, C. Wang, J. Do, Y. Li, H. Zhang, B. Chandramouli, J. Gehrke, D. Koss-\nmann, D. Lomet, and T. Kraska. Alex: An updatable adaptive learned index. In Proceedings of the\n2020 ACM SIGMOD International Conference on Management of Data , SIGMOD ’20, page 969–984,\nNew York, NY, USA, 2020. Association for Computing Machinery.\n[13] P. Ferragina and G. Vinciguerra. Learned data structures. In Recent Trends in Learning From Data ,\npages 5–41. Springer International Publishing, 2020.\n[14] P. Ferragina and G. Vinciguerra. The PGM-index: a fully-dynamic compressed learned index with\nprovable worst-case bounds. PVLDB , 13(8):1162–1175, 2020.\n[15] D. Freedman. Statistical Models : Theory and Practice . Cambridge University Press, August 2005.\n[16] G. Fumagalli, D. Raimondi, R. Giancarlo, D. Malchiodi, and M. Frasca. On the choice of general\npurpose classiﬁers in learned bloom ﬁlters: An initial analysis within basic ﬁlters. In Proceedings of\nthe 11th International Conference on Pattern Recognition Applications and Methods (ICPRAM) , pages\n675–682, 2022.\n[17] A. Galakatos, M. Markovitch, C. Binnig, R. Fonseca, and T. Kraska. FITing-Tree: A data-aware index\nstructure. In Proceedings of the 2019 International Conference on Management of Data , SIGMOD ’19,\npage 1189–1206, New York, NY, USA, 2019. Association for Computing Machinery.\n[18] P.V. Khuong and P. Morin. Array layouts for comparison-based searching. J. Exp. Algorithmics ,\n22:1.3:1–1.3:39, 2017.\n[19] A. Kipf, R. Marcus, A. van Renen, M. Stoian, Kemper A., T. Kraska, and T. Neumann. SOSD: A\nbenchmark for learned indexes. In ML for Systems at NeurIPS, MLForSystems @ NeurIPS ’19 , 2019.\n20\n\n[20] A. Kipf, R. Marcus, A. van Renen, M. Stoian, A. Kemper, T. Kraska, and T. Neumann. Radixspline:\nA single-pass learned index. In Proceedings of the Third International Workshop on Exploiting Arti-\nﬁcial Intelligence Techniques for Data Management , aiDM ’20, pages 1–5. Association for Computing\nMachinery, 2020.\n[21] Kipf, A. and Marcus, R. and van Renen, A. and Stoian, M. and Kemper A. and Kraska, T. and Neumann,\nT. SOSD Leaderboard. https://learnedsystems .github.io/SOSDLeaderboard/leaderboard/ .\n[22] D. E. Knuth. The Art of Computer Programming, Vol. 3 (Sorting and Searching) , volume 3. Addison-\nWesley, 1973.\n[23] T. Kraska, M. Alizadeh, A. Beutel, E. H. Chi, J. Ding, A. Kristo, G. Leclerc, S. Madden, H. Mao, and\nV. Nathan. Sagedb: A learned database system, 2021.\n[24] T. Kraska, A. Beutel, E. H Chi, J. Dean, and N. Polyzotis. The case for learned index structures. In\nProceedings of the 2018 International Conference on Management of Data , pages 489–504. ACM, 2018.\n[25] A. Kristo, K. Vaidya, U. C ¸ etintemel, S. Misra, and T. Kraska. The case for a learned sorting algorithm.\nInProceedings of the 2020 ACM SIGMOD International Conference on Management of Data , SIGMOD\n’20, page 1001–1016, New York, NY, USA, 2020. Association for Computing Machinery.\n[26] P. Li, H. Lu, Q. Zheng, L. Yang, and G. Pan. Lisa: A learned index structure for spatial data. In\nProceedings of the 2020 ACM SIGMOD International Conference on Management of Data , SIGMOD\n’20, page 2119–2133, New York, NY, USA, 2020. Association for Computing Machinery.\n[27] H. Lin, T. Luo, and D. Woodruﬀ. Learning augmented binary search trees. In Kamalika Chaudhuri,\nStefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the\n39th International Conference on Machine Learning , volume 162 of Proceedings of Machine Learning\nResearch , pages 13431–13440. PMLR, 17–23 Jul 2022.\n[28] M. Maltry and J. Dittrich. A critical analysis of recursive model indexes. CoRR. To appear in:\nProceedings of the VLDB Endowment , abs/2106.16166, 2021.\n[29] R. Marcus, A. Kipf, A. van Renen, M. Stoian, S. Misra, A. Kemper, T. Neumann, and T. Kraska.\nBenchmarking learned indexes. Proc. VLDB Endow. , 14(1):1–13, sep 2020.\n[30] R. Marcus, P. Negi, H. Mao, N. Tatbul, M. Alizadeh, and T. Kraska. Bao: Making learned query\noptimization practical. SIGMOD Rec. , 51(1):6–13, jun 2022.\n[31] R. Marcus, P. Negi, H. Mao, C. Zhang, M. Alizadeh, T. Kraska, O. Papaemmanouil, and N. Tatbul.\nNeo: A learned query optimizer. Proc. VLDB Endow. , 12(11):1705–1718, jul 2019.\n[32] R. Marcus, E. Zhang, and T. Kraska. CDFShop: Exploring and optimizing learned index structures. In\nProceedings of the 2020 ACM SIGMOD International Conference on Management of Data , SIGMOD\n’20, page 2789–2792, 2020.\n[33] A. Mikhaylov, N. S. Mazyavkina, M. Salnikov, I. Troﬁmov, F. Qiang, and E. Burnaev. Learned query\noptimizers: Evaluation and improvement. IEEE Access , 10:75205–75218, 2022.\n[34] M. Mitzenmacher. A model for learned bloom ﬁlters and optimizing by sandwiching. In S. Bengio,\nH. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural\nInformation Processing Systems , volume 31. Curran Associates, Inc., 2018.\n[35] M. Mitzenmacher and S. Vassilvitskii. Algorithms with predictions. Commun. ACM , 65(7):33–35, jun\n2022.\n[36] T. Neumann and S. Michel. Smooth interpolating histograms with error guarantees. 07 2008.\n21\n\n[37] J. Ol’ha, T. Slanin´ akov´ a, M. Gendiar, M. Antol, and V. Dohnal. Learned indexing in proteins: Sub-\nstituting complex distance calculations with embedding and clustering techniques. arXiv preprint\narXiv:2208.08910 , 2022.\n[38] W. W. Peterson. Addressing for random-access storage. IBM Journal of Research and Development ,\n1(2):130–146, 1957.\n[39] J. Rao and K. A Ross. Cache conscious indexing for decision-support in main memory. In Proceedings of\nthe 25th International Conference on Very Large Data Bases , pages 78–89. Morgan Kaufmann Publishers\nInc., 1999.\n[40] B. Schlegel, R. Gemulla, and W. Lehner. K-ary search on modern processors. In Proceedings of the\nFifth International Workshop on Data Management on New Hardware , DaMoN ’09, page 52–60, New\nYork, NY, USA, 2009. Association for Computing Machinery.\n[41] L. Schulz, D. Broneske, and G. Saake. An eight-dimensional systematic evaluation of optimized search\nalgorithms on modern processors. Proc. VLDB Endow. , 11:1550–1562, 2018.\n[42] A. Singh and S. Gupta. Learning to hash: a comprehensive survey of deep learning-based hashing\nmethods. Knowledge and Information Systems , 08 2022.\n[43] K. Vaidya, E. Knorr, T. Kraska, and M. Mitzenmacher. Partitioned learned bloom ﬁlter. ArXiv ,\nabs/2006.03176, 2020.\n[44] P. Van Sandt, Y. Chronis, and J. M. Patel. Eﬃciently searching in-memory sorted arrays: Revenge of\nthe interpolation search? In Proceedings of the 2019 International Conference on Management of Data ,\nSIGMOD ’19, pages 36–53, New York, NY, USA, 2019. ACM.\n[45] H. Wang, X. Fu, J. Xu, and H. Lu. Learned index for spatial queries. In 2019 20th IEEE International\nConference on Mobile Data Management (MDM) , pages 569–574, 2019.\n[46] W. Wang, M. Zhang, G. Chen, H. V. Jagadish, B. C. Ooi, and K. Tan. Database meets deep learning:\nChallenges and opportunities. SIGMOD Rec. , 45(2):17–22, sep 2016.\n[47] M. Zhang and H. Wang. Laqp: Learning-based approximate query processing. Information Sciences ,\n546:1113–1134, 2021.\n22\n\nLearned Sorted Table Search and Static Indexes in Small Model\nSpace-Supplementary File∗\nDomenico Amato1Giosu´ e Lo Bosco1†Raﬀaele Giancarlo1\n1Dipartimento di Matematica e Informatica\nUniversit´ a degli studi di Palermo, ITALY\nSeptember 20, 2022\nAbstract\nThis document provides additional details with respect to the content of the Main Manuscript by the\nsame title.\n1 Binary Search and Its Variants\nWith reference to the routines mentioned in the Main Text (Section 2.1) and following research in [1], we\nprovide more details about two kind of layouts.\n1 Sorted. We use two version of Binary Search for this layout. The template of the lower bound routine\nis provide in Algorithm 1, while the Uniform Binary Search implementation is given in Algorithm 2.\nIn particular, such an implementation of Binary Search is as in [1].\n2 Eytzinger Layout [1]. The sorted table is now seen as stored in a virtual complete balanced binary\nsearch tree. Such a tree is laid out in Breadth-First Search order in an array. An example is provided\nin Fig. 1. The implementation is reported in Algorithm 3.\n5\n79\n1113\n15 316\n1819\n2021\n25\n22 27\n20 16 9 215131925371115 18 22 27\nFigure 1: An example of Eyzinger layout of a table with 15 elements. See also [1].\n∗This research is funded in part by MIUR Project of National Relevance 2017WR7SHH “Multicriteria Data Structures\nand Algorithms: from compressed to learned indexes, and beyond”. We also acknowledge an NVIDIA Higher Education and\nResearch Grant (donation of a Titan V GPU). Additional support to RG has been granted by Project INdAM - GNCS “Analysis\nand Processing of Big Data based on Graph Models”\n†corresponding author, email: giosue.lobosco@unipa.it\n1arXiv:2107.09480v6  [cs.IR]  17 Sep 2022\n\nAlgorithm 1 lower bound Template .\n1:ForwardIterator lower bound (ForwardIterator ﬁrst, ForwardIterator last, const T& val) {\n2:ForwardIterator it;\n3:iterator traits<ForwardIterator >::diﬀerence type count, step;\n4:count = distance(ﬁrst,last);\n5:while (count >0){\n6: it = ﬁrst; step=count/2; advance (it,step);\n7: if (*it<val){\n8: ﬁrst=++it;\n9: count-=step+1;\n10:}\n11: else count=step;\n12:}\n13: return ﬁrst;\n14:}\nAlgorithm 2 Implementation of Uniform Binary Search The code is as in [1] (see also [2,3] ).\n1:int UniformBinarySearch(int *A, int x, int left, int right) {\n2: const int *base = A;\n3: int n = right;\n4: while (n >1){\n5: const int half = n / 2;\n6: builtin prefetch(base + half/2, 0, 0);\n7: builtin prefetch(base + half + half/2, 0, 0);\n8: base = (base[half] <x) ? &base[half] : base;\n9: n -= half;\n10:}\n11: return (*base <x) + base - A;\n12:}\nAlgorithm 3 Uniform Binary Search with Eytzinger layout The code is as in [1].\nint EytzingerLayoutSearch(int *A, int x, int left, int right) {\nint i = 0;\n3: int n = right;\nwhile (i <n){\nbuiltin prefetch(A+(multiplier*i + oﬀset));\n6: i = (x <= A[i]) ? (2*i + 1) : (2*i + 2);\n}\nint j = (i+1) >> builtin ﬀs(∼(i+1));\n9: return (j == 0) ? n : j-1;\n}\n2\n\nTable 1: Constant Space Models Training time for L1 Tables . The ﬁrst column indicated the datasets.\nThe remaining columns indicate the model used for the learning phase. Each entry reports the training time\nin seconds and per element.\nKO-US C\namzn 5.3e-07 1.0e-07\nface 5.5e-07 8.5e-08\nosm 4.6e-07 9.9e-08\nwiki 9.0e-07 7.9e-08\nTable 2: Constant Space Models Training Time for L2 Tables . The table legend is as in Table 1\nKO-BFS C\namzn 1.8e-07 3.1e-07\nface 1.0e-07 2.8e-07\nosm 1.2e-07 2.9e-07\nwiki 1.0e-07 2.7e-07\n2 Training of the Novel Models: Analysis and Insights into Model\nTraining - Additional Results\nFollowing the same approach used in Section 4 of the Main Text, we divide the training time analysis into\ntwo groups: Constant and Parametric Space Models.\n•Tables 1-3 report the experiments concerning the Constant Space Models for the datasets L1, L2 and\nL3.\n•Tables 4-6 report the experiments concerning the Parametric Space Models for the datasets L1, L2\nand L3.\n3 Query Experiments - Additional Results\nIn this Section, we report the experiments described and discussed in Section 5 of the Main Text for the\nface andwiki datasets.\n•Figures 2-3 report the experiments concerning the Constant Space Models, as in Section 5.1.\n•Figures 4-5 report the experiments concerning the Parametric Space Models, as in Section 5.2\n•Tables 7-8 report a synopsis of three parameters, i.e., query time, space used in addition by the model\nand reduction factor, as described in Section 5.2.\nTable 3: Constant Space Models Training time for L3 Tables . The table legend is as in Table 1.\nKO-US C\namzn 6.3e-08 1.9e-08\nface 3.9e-08 1.9e-08\nosm 4.4e-08 2.0e-08\nwiki 4.1e-08 1.9e-08\n3\n\nTable 4: Parametric Space Models Training Time for L1 Tables . The ﬁrst column indicated the\ndatasets. The remaining columns indicate the model used for the learning phase. In particular, each entry\nreports the time used by CDFShop andSOSD libraries to train the entire batch of parametric models in\nseconds and per element.\nCDFShop SY-RMI 2% CDFShop RMI SOSD RS SOSD PGM\namzn 5.2e-06 5.6e-06 3.5e-06 5.0e-07\nface 4.1e-06 4.6e-06 1.1e-06 3.9e-07\nosm 2.8e-04 2.9e-04 6.9e-06 4.0e-07\nwiki 7.8e-06 9.3e-06 1.0e-05 3.7e-07\nTable 5: Parametric Space Models Training time for L2 Tables . The table legend is as in Table 4\nCDFShop SY-RMI 2% CDFShop RMI SOSD RS SOSD PGM\namzn 5.2e-06 5.6e-07 3.5e-07 5.0e-08\nface 4.1e-06 4.6e-07 1.1e-07 3.9e-08\nosm 2.8e-04 2.9e-05 6.9e-07 4.0e-08\nwiki 7.8e-06 9.3e-07 1.0e-06 3.7e-08\nTable 6: Parametric Space Models Training time for L3 Tables . The table legend is as in Table 4\nCDFShop SY-RMI 2% CDFShop RMI SOSD RS SOSD PGM\namzn 1.5e-06 1.3e-07 2.4e-08 3.4e-08\nface 1.5e-05 1.6e-06 1.4e-08 2.4e-08\nosm 1.2e-05 1.3e-06 3.5e-08 3.8e-08\nwiki 2.3e-06 2.2e-07 5.1e-08 3.7e-08\n4\n\nNM-0% C-99.68% K0-US-99.95%00.20.40.60.81\n10-7 L1\nNM-0% C-99.67% K0-US-99.96%00.20.40.60.811.21.410-7 L2\nNM-0% C-99.67% K0-US-99.96%00.511.522.510-7 L3\nNM-0% C-0.00% K0-US-99.52%0123456710-7 L4US\nEBFigure 2: Constant Space Models Query Times for the face Dataset . For each memory level, the\nblue bar reports the average query time of Uniform Binary Search using, from left to right, no model, Cubic\nmodel and KO-US . In addition, we report the average query time also for the Eytzinger Binary Search in\nthe orange bar.\nReferences\n[1] P.V. Khuong and P. Morin. Array layouts for comparison-based searching. J. Exp. Algorithmics , 22:1.3:1–\n1.3:39, 2017.\n[2] D. E. Knuth. The Art of Computer Programming, Vol. 3 (Sorting and Searching) , volume 3. Addison-\nWesley, 1973.\n[3] L. Schulz, D. Broneske, and G. Saake. An eight-dimensional systematic evaluation of optimized search\nalgorithms on modern processors. Proc. VLDB Endow. , 11:1550–1562, 2018.\n5\n\nNM-0% C-43.56% K0-US-99.85%00.20.40.60.811.21.410-7 L1\nNM-0% C-31.99% K0-US-99.76%00.20.40.60.811.21.41.6\n10-7 L2\nNM-0% C-30.93% K0-US-99.73%00.511.522.510-7 L3\nNM-0% C-30.54% K0-US-99.73%00.20.40.60.8110-6 L4US\nEBFigure 3: Constant Space Models Query Times for the wiki Dataset . The legend is as in Figure 2\nTable 7: A Synoptic Table of Space, Time and Accuracy of Models on face Dataset . For each\nmemory level, we report in the ﬁrst row the best performing method for that memory level. The columns\nnamed time, space and reduction factor indicate for this best model, the average query time in seconds,\nthe average additional space used and the average of the empirical reduction factor. From the second row,\nwe report the versions of the RMI ,RS,PGM and Synoptic RMI models that use the least space. In\nparticular, the number next to the Models represent the percentage of the used space with respect to the\ninput dataset. The columns indicate now the ratio Model/best Model of the time, space and reduction\nfactor.\nL1\nTime Space Reduction Factor\nBest PGM 2.26e+01 4.00e-02 99.52\nBest PGM 1.00e+00 1.00e+00 1.00e+00\nSY-RMI 0.05 2.33e+00 2.20e+00 6.30e-01\nBest RMI 1.16e+00 7.72e+01 1.00e+00\nBest RS 1.17e+00 5.90e+04 1.00e+00\nL2\nTime Space Reduction Factor\nBest RMI 3.02e+01 1.23e+01 99.98\nB-PGM 0.05 1.89e+00 8.13e-03 9.98e-01\nSY-RMI 0.05 1.89e+00 1.30e-02 9.40e-01\nBest RMI 1.00e+00 1.00e+00 1.00e+00\nBest RS 1.10e+00 1.92e+02 1.00e+00L3\nTime Space Reduction Factor\nBest RMI 6.11e+01 7.86e+02 100.00\nB-PGM 0.05 1.57e+00 3.33e-03 1.00e+00\nRMI<10 1.19e+00 3.13e-02 1.00e+00\nSY-RMI 0.7 1.22e+00 5.62e-02 1.00e+00\nRS<10 1.53e+00 7.54e-01 1.00e+00\nL4\nTime Space Reduction Factor\nBest RMI 1.80e-07 2.01e+05 100.00\nSY-RMI 0.05 3.74e+00 3.97e-03 1.32e-02\nB-PGM 0.05 2.14e+00 3.98e-03 1.00e+00\nBest RS 2.21e+00 3.96e-02 1.00e+00\nRMI<10 1.06e+00 5.00e-01 1.00e+00\n6\n\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%020406080100120\n    L1\n0.13% 0.13%0.29% 0.13%0.85% 0.13%2.05%\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%020406080100120140\n    L2\n9.76%0.40% 0.04%0.06% 0.06%0.75% 0.06%2.01%\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%050100150200250\n    L3\n0.41%9.89%5.24% 0.04%0.05% 0.51%0.74% 0.51%2.00%\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%0100200300400500600700    L4\n6.29%0.25%2.74% 0.05%0.05% 0.70%0.74% 1.61%2.00%RMI\nRS\nPGM\nB-PGM\nSY-RMI\nUSFigure 4: Query times for the face dataset on Learned Indexes in Small Space. The methods are\nthe ones in the legend (middle of the four panels, the notation is as in the main text and each method has\na distinct colour). For each memory level, the abscissa reports methods grouped by space occupancy, as\nspeciﬁed in the main text. When no model in a class output by SOSD takes at most 10% of additional space,\nthat class is absent. The ordinate reports the average query time, with Uniform Binary Search executed in\nSOSD as baseline (horizontal lines).\nTable 8: A Synoptic Table of Space, Time and Accuracy of Models on wiki Dataset . The legend\nis as in 7.\nL1\nTime Space Reduction Factor\nBest RMI 2.55e+01 3.09e+00 99.84\nB-PGM 0.05 2.50e+00 1.30e-02 2.06e-01\nSY-RMI 0.05 2.24e+00 2.85e-02 8.52e-01\nBest RMI 1.00e+00 1.00e+00 1.00e+00\nBest RS 1.70e+00 2.40e+00 9.77e-01\nL2\nTime Space Reduction Factor\nBest RMI 3.32e+01 9.83e+01 99.98\nB-PGM 0.05 2.66e+00 1.02e-03 9.26e-01\nSY-RMI 0.05 2.59e+00 1.63e-03 9.57e-01\nBest RS 1.60e+00 7.77e-02 9.97e-01\nRMI<10 1.05e+00 2.50e-01 1.00e+00L3\nTime Space Reduction Factor\nBest RMI 5.16e+01 7.86e+02 100.00\nB-PGM 0.05 2.26e+00 3.76e-03 1.00e+00\nSY-RMI 0.05 2.01e+00 3.83e-03 1.00e+00\nBest RS 1.74e+00 3.82e-02 1.00e+00\nRMI<10 1.14e+00 5.00e-01 1.00e+00\nL4\nTime Space Reduction Factor\nSY-RMI 2 1.61e-07 3.20e+04 100.00\nSY-RMI 0.05 1.39e+00 2.50e-02 1.00e+00\nB-PGM 0.05 1.82e+00 2.53e-02 1.00e+00\nBest RS 1.30e+00 4.97e-01 1.00e+00\nRMI<10 1.02e+00 7.86e-01 1.00e+00\n7\n\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%020406080100120\n    L1\n1.13% 0.13%0.29% 0.60%0.85% 0.60%2.05%\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%020406080100120140\n    L2\n9.76%3.03%1.00% 0.04%0.06% 0.21%0.75% 0.21%2.01%\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%050100150200250\n    L3\n6.55%0.50%1.45% 0.05%0.05% 0.63%0.74% 0.63%2.00%\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%0100200300400500600700\n    L4\n0.79%7.82%0.89% 0.05%0.05% 0.29%0.74% 0.29%2.00%RMI\nRS\nPGM\nB-PGM\nSY-RMI\nUSFigure 5: Query times for the wiki dataset on Learned Indexes in Small Space. The ﬁgure legend\nis as in Figure 4.\n8",
  "textLength": 83014
}