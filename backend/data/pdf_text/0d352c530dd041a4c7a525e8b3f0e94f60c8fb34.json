{
  "paperId": "0d352c530dd041a4c7a525e8b3f0e94f60c8fb34",
  "title": "Learning-Augmented k-means Clustering",
  "pdfPath": "0d352c530dd041a4c7a525e8b3f0e94f60c8fb34.pdf",
  "text": "Learning-Augmented k-means Clustering\nJon C. Ergun\nGeorgetown Day School∗Zhili Feng\nCarnegie Mellon University†Sandeep Silwal\nMIT‡\nDavid P. Woodru\u000b\nCarnegie Mellon University§Samson Zhou\nCarnegie Mellon University¶\nMarch 22, 2022\nAbstract\nk-means clustering is a well-studied problem due to its wide applicability. Unfortunately,\nthere exist strong theoretical limits on the performance of any algorithm for the k-means prob-\nlem on worst-case inputs. To overcome this barrier, we consider a scenario where \\advice\" is\nprovided to help perform clustering. Speci\fcally, we consider the k-means problem augmented\nwith a predictor that, given any point, returns its cluster label in an approximately optimal\nclustering up to some, possibly adversarial, error. We present an algorithm whose performance\nimproves along with the accuracy of the predictor, even though na \u0010vely following the accurate\npredictor can still lead to a high clustering cost. Thus if the predictor is su\u000eciently accurate,\nwe can retrieve a close to optimal clustering with nearly optimal runtime, breaking known com-\nputational barriers for algorithms that do not have access to such advice. We evaluate our\nalgorithms on real datasets and show signi\fcant improvements in the quality of clustering.\n1 Introduction\nClustering is a fundamental task in data analysis that is typically one of the \frst methods used\nto understand the structure of large datasets. The most common formulation of clustering is the\nk-means problem where given a set P\u001aRdofnpoints, the goal is to \fnd a set of centers C\u001aRd\nofkpoints to minimize the objective cost( P;C) =P\np2Pminc2Ckp\u0000ck2\n2:(1)\nDespite decades of work, there exist strong theoretical limitations about the performance of\nany algorithm for the k-means problem. Finding the optimal set Cis NP-hard even for the case\nofk= 2 (Dasgupta, 2008) and even \fnding an approximate solution with objective value that is\nwithin a factor 1 :07 of the optimal solution is NP-hard (Cohen-Addad & S., 2019; Lee et al., 2017).\nFurthermore, the best-known practical polynomial time algorithms can only provably achieve a\nlarge constant factor approximation to the optimal clustering, e.g., the 50-approximation in Song\n& Rajasekaran (2010), or use techniques such as linear programming that do not scale, e.g., the\n6:357-approximation in Ahmadian et al. (2020).\n∗E-mail: jergun22@gds.org\n†E-mail: zhilif@andrew.cmu.edu\n‡E-mail: silwal@mit.edu\n§E-mail: dwoodruf@andrew.cmu.edu\n¶E-mail: samsonzhou@gmail.com\n1arXiv:2110.14094v2  [cs.LG]  19 Mar 2022\n\nA natural approach to overcome these computational barriers is to leverage the fact that in\nmany applications, the input is often not arbitrary and contains auxiliary information that can\nbe used to construct a good clustering, e.g., in many applications, the input can be similar to\npast instances. Thus, it is reasonable to create a (possibly erroneous) predictor by using auxiliary\ninformation or through clusterings of similar datasets, which can inform the proper label of an item\nin our current dataset. Indeed, inspired by the developments in machine learning, many recent\npapers have studied algorithms augmented with predictions (Mitzenmacher & Vassilvitskii, 2020).\nSuch algorithms utilize a predictor that, when invoked, provides an (imperfect) prediction for future\ninputs. The predictions are then used by the algorithm to improve performance (see references in\nSection 1.3).\nHence, we consider the problem of k-means clustering given additional access to a predictor\nthat outputs advice for which points should be clustered together, by outputting a label for each\npoint. The goal is to \fnd kcenters that minimize objective (1) and assign each point to one of\nthese centers. The question is then whether one can utilize such predictions to boost the accuracy\nand runtime of clustering of new datasets. Our results demonstrate the answer in the a\u000ermative.\nFormal learning-augmented problem de\fnition. Given a set P\u0012Rdofnpoints, the goal\nis to \fnd a set of kpointsC(called centers) to minimize objective (1). In the learning-augmented\nsetting, we assume we have access to a predictor \u0005 that provides information about the label of\neach point consistent with a (1 + \u000b)-approximately optimal clustering C. We say that a predictor\nhaslabel error rate \u0015\u0014\u000bif for each label i2[k] :=f1;:::;kg, \u0005 errs on at most a \u0015\u0014\u000bfraction\nof all points in cluster iinC, and \u0005 errs on at most a \u0015\u0014\u000bfraction of all points given label iby\n\u0005. In other words, \u0005 has at least (1 \u0000\u0015) precision and recall for each label.\nOur predictor model subsumes both random and adversarial errors by the predictor. For ex-\nample if the cluster sizes are somewhat well-balanced, then a special case of our model is when\n\u0005(p) outputs the correct label of point p2Pwith some probability 1 \u0000\u0015and otherwise outputs\na random label in [ k] with probability \u0015. The example where the predictor outputs an adversarial\nlabel instead of a random label with probability \u0015also falls under our model. For more detail, see\nTheorems 2.1 and 3.4. We also adjust our algorithm to have better performance when the errors\nare random rather than adversarial in the supplementary material.\n1.1 Motivation for Our Work\nWe \frst motivate studying k-means clustering under the learning-augmented algorithms framework.\nOvercoming theoretical barriers. As stated above, nopolynomial time algorithm can\nachieve better than a constant factor approximation to the optimal clustering. In addition, the\nbest provable approximation guarantees by polynomial time algorithms have a large constant factor\n(for example the 50 approximation in Song & Rajasekaran (2010)), or use methods which do not\nscale (such as the linear programming based algorithm in Ahmadian et al. (2020) which gives\na 6:357-approximation). Therefore, it is of interest to study whether a natural assumption can\novercome these complexity barriers. In our work, we show that knowing the true labels up to some\npossibly adversarial noise can give us arbitrarily good clusterings, depending on the noise level,\nwhich breaks these computational barriers. Furthermore, we present an algorithm that runs in\nnearly linear time, rather than just polynomial time. Lastly, we introduce tools from the robust\nstatistics literature to study k-means clustering rather than the distance-based sampling procedure\nthat is commonly analyzed (this is the basis of kmeans++ ). This new toolkit and connection could\nhave further applications in other learning-augmented clustering problems.\n2\n\nPractical considerations. In practice, good predictors can be learned for datasets with\nauxiliary information. For a concrete example, we can take anydataset that has a train/test split\nand use a clustering on the training dataset to help us cluster the testing portion of the dataset.\nTherefore, datasets do not have to be speci\fcally curated to \ft our modelling assumption, which\nis a requirement in other modelling formulations that leverage extra information such as the SSAC\nmodel discussed in Section 1.3. A predictor can also be created from the natural class of datasets\nthat vary over time, such as Census data or spectral clustering for temporal graphs (graphs slowly\nvarying over time). For this class of datasets, a clustering from an earlier time step can function\nas a predictor for later time steps. Lastly, we can simply use the labels given by another clustering\nalgorithm (such as kmeans++ ) or heuristic as a predictor. Therefore, predictors are readily and\neasily available for a wide class of natural datasets.\nFollowing the predictor alone is insu\u000ecient. Given a predictor that outputs noisy labels,\nit is conceivable that its output alone can give us a good clustering relative to optimal. However,\nthis is not the case and na \u0010vely using the label provided by the predictor for each point can result\nin an arbitrarily bad solution, even when the predictor errs with low probability. For example,\nconsider a cluster ofn\n2points at the origin and a cluster ofn\n2points atx= 1. Then for k= 2,\nchoosing centers at the origin and at x= 1 induces a k-means clustering cost of zero. However,\neven for a predictor that errs with probability1\nn, some point will be mislabeled with constant\nprobability, which results in a positive k-means clustering cost, and so does not provide a relative\nerror approximation. Thus, using the provided labels by the predictor can induce an arbitrarily\nbad clustering, even as the label error rate of the predictor tends to zero. This subtlety makes the\nmodel rich and interesting, and requires us to create non-trivial clustering algorithms.\nPredictors with adversarial errors. Since the predictor is separate from the clustering\nalgorithm, interference with the output of the predictor following the clustering algorithm's query\ncan be a source of non-random noise. Thus any scenario in which communication is performed over\na noisy channel (for example, if the predictor is hosted at one server and the algorithm is hosted at\nanother server) is susceptible to such errors. Another source of adversarial failure by the predictor\nis when the predictor is trained on a dataset that can be generated by an adversary, such as in the\ncontext of adversarial machine learning. Moreover, our algorithms have better guarantees when\nthe predictor does not fail adversarially, e.g., see the supplementary material).\n1.2 Our Results\nIn this paper we study \\learning-augmented\" methods for e\u000ecient k-means clustering. Our con-\ntributions are both theoretical and empirical. On the theoretical side, we introduce an algorithm\nthat provably solves the k-means problem almost optimally, given access to a predictor that out-\nputs a label for each point p2Paccording to a (1 + \u000b)-approximately optimal clustering, up to\nsome noise. Speci\fcally, suppose we have access to a predictor \u0005 with label error rate \u0015upper\nbounded by a parameter \u000b. Then, Algorithm 1 outputs a set of centers eCin~O(knd) time1, such\nthat cost(P;eC)\u0014(1 +O(\u000b))\u0001cost(P;Copt), whereCoptis an optimal set of centers. We improve\nthe runtime in Section 3 by introducing Algorithm 3, which has the same error guarantees, but\nuses ~O(nd) runtime, which is nearly optimal since one needs at least ndtime to read the points for\ndense inputs (Theorem 3.4, and Remark A.14).\nTo output labels for all points, Algorithm 3 requires nqueries to the predictor. However, if\nthe goal is to just output centers for each cluster, then we only require eO(k=\u000b) queries. This is\n1The notation eOhides logarithmic factors.\n3\n\nessentially optimal; we show in Theorem 3.5 that any polynomial time algorithm must perform\napproximately e\n(k=\u000b) queries to output a 1 + \u000b-approximate solution assuming the Exponential\nTime Hypothesis, a well known complexity-theoretic assumption (Impagliazzo & Paturi, 2001).\nNote that one could ignore the oracle entirely, but then one is limited by the constant factor\nhardness for polynomial time algorithms, which we bypass with a small number of queries.\nSurprisingly, we do not require assumptions that the input is well-separated or approximation-\nstable (Braverman et al., 2011; Balcan et al., 2013), which are assumed in other works. Finally in\nthe supplementary material, we also give a learning-augmented algorithm for the related problem\nofk-median clustering, which has less algebraic structure than that of k-means clustering. We also\nconsider a deletion predictor, which either outputs a correct label or a failure symbol ?and give a\n(1 +\u000b)-approximation algorithm even when the \\deletion rate\" is 1 \u00001=poly(k).\nOn the empirical side, we evaluate our algorithms on real and synthetic datasets. We exper-\nimentally show that good predictors can be learned for all of our varied datasets, which can aid\nin clustering. We also show our methodology is more robust than other heuristics such as random\nsampling.\n1.3 Related Work\nLearning-augmented algorithms. Our paper adds to the growing body of work on learning-\naugmented algorithms. In this framework, additional \\advice\" from a possibly erroneous predictor\nis used to improve performance of classical algorithms. For example, a common predictor is a\n\\heaviness\" predictor that outputs how \\important\" a given input point is. It has been shown\nthat such predictors can be learned using modern machine learning techniques or other methods\non training datasets and can be successfully applied to similar testing datasets. This methodology\nhas found applications in improving data structures (Kraska et al., 2018; Mitzenmacher, 2018),\nstreaming algorithms (Hsu et al., 2019; Jiang et al., 2020), online algorithms (Lykouris & Vassil-\nvtiskii, 2018; Purohit et al., 2018), graph algorithms (Dai et al., 2017), and many other domains\n(Mousavi et al., 2015; Wang et al., 2016; Bora et al., 2017; Sablayrolles et al., 2019; Dong et al.,\n2020; Sanchez et al., 2020; Eden et al., 2021). See Mitzenmacher & Vassilvitskii (2020) for an\noverview and applications.\nClustering with additional information. There have been numerous works that study\nclustering in a semi-supervised setting where extra information is given. Basu et al. (2004) gave\nan active learning framework of clustering with \\must-link\"/\\cannot-link\" constraints, where an\nalgorithm is allowed to interact with a predictor that determines if two points must or cannot belong\nto the same cluster. Their objective function is di\u000berent than that of k-means and they do not\ngive theoretical bounds on the quality of their solution. Balcan & Blum (2008) and Awasthi et al.\n(2017) studied an interactive framework for clustering, where a predictor interactively provides\nfeedback about whether or not to split a current cluster or merge two clusters. Vikram & Dasgupta\n(2016) also worked with an interactive oracle but for the Bayesian hierarchical clustering problem.\nThese works di\u000ber from ours in their assumptions since their predictors must answer di\u000berent\nquestions about partitions of the input points. In contrast, Howe (2017) used logistic regression to\naidk-means clustering but do not give any theoretical guarantees.\nThe framework closest in spirit to ours is the semi-supervised active clustering framework\n(SSAC) introduced by Ashtiani et al. (2016) and further studied by Kim & Ghosh (2017); Mazum-\ndar & Saha (2017); Gamlath et al. (2018); Ailon et al. (2018); Chien et al. (2018); Huleihel et al.\n(2019). The goal of this framework is also to produce a (1 + \u000b)-approximate clustering while\n4\n\nminimizing the number of queries to a predictor that instead answers queries of the form \\same-\ncluster(u;v)\", which returns 1 if points u;v2Pare in the same cluster in a particular optimal\nclustering and 0 otherwise. Our work di\u000bers from the SSAC framework in terms of both runtime\nguarantees, techniques used, and model assumptions, as detailed below.\nWe brie\ry compare to the most relevant works in the SSAC framework, which are Ailon et al.\n(2018) and Mazumdar & Saha (2017). First, the runtime of Ailon et al. (2018) is O(ndk9=\u000b4) even\nfor a perfectly accurate predictor, while the algorithm of Mazumdar & Saha (2017) uses O(nk2)\nqueries and runtime ~O(ndk2). By comparison, we use signi\fcantly fewer queries, with near linear\nruntime ~O(nd) even for an erroneous predictor. Moreover, a predictor of Mazumdar & Saha (2017)\nindependently fails each query with probability pso that repeating with pairs containing the same\npoint can determine the correct label of a point whereas our oracle will always repeatedly fail with\nthe same query , so that repeated queries do not help.\nThe SSAC framework uses the predictor to perform importance sampling to obtain a su\u000ecient\nnumber of points from each cluster whereas we use techniques from robust mean estimation, di-\nmensionality reduction, and approximate nearest neighbor data structures. Moreover, it is unclear\nhow the SSAC predictor can be implemented in practice to handle adversarial corruptions. One\nmay consider simulating the SSAC predictor using information from individual points by simply\nchecking if the labels of the two input points are the same. However, if a particular input is mis-\nlabeled, then all of the pairs containing this input can also be reported incorrectly, which violates\ntheir independent noise assumption. Finally, the noisy predictor algorithm in Ailon et al. (2018)\ninvokes a step of recovering a hidden clique in a stochastic block model, making it prohibitively\ncostly to implement.\nLastly, in the SSAC framework, datasets need to be speci\fcally created to \ft into their model\nsince one requires pairwise information. In contrast, our predictor requires information about\nindividual points, which can be learned from either a training dataset, from past similar datasets,\nor from another approximate or heuristic clustering and is able to handle adversarial corruptions.\nThus, we obtain signi\fcantly faster algorithms while using an arguably more realistic predictor.\nApproximation stability. Another approach to overcome the NP-hardness of approximation\nfork-means clustering is the assumption that the underlying dataset follows certain distributional\nproperties. Introduced by Balcan et al. (2013), the notion of ( c;\u000b)-approximate stability (Agarwal\net al., 2015; Awasthi et al., 2019; Balcan et al., 2020) requires that every c-approximation is \u000b-close\nto the optimal solution in terms of the fraction of incorrectly clustered points. In contrast, we allow\ninputs so that an arbitrarily small fraction of incorrectly clustered points can induce arbitrarily bad\napproximations, as previously discussed, e.g., in Section 1.1.\n2 Learning-Augmented k-means Algorithm\nPreliminaries. We use [n] to denote the set f1;:::;ng. Given the set of cluster centers C, we can\npartition the input points PintokclustersfC1;:::;Ckgaccording to the closest center to each\npoint. If a point is grouped in Ciin the clustering, we refer to its label as i. Note that labels can\nbe arbitrarily permuted as long as the labeling across the points of each cluster is consistent. It is\nwell-known that in k-means clustering, the i-th center is given by the coordinate-wise mean of the\npoints inCi. Givenx2Rdand a setC\u001aRd, we de\fne d(x;C) = minc2Ckx\u0000ck2. Note that\nthere may be many approximately optimal clusterings but we consider a \fxed one for our analysis.\n5\n\nAlgorithm 1 Learning-augmented k-means clustering\nInput: A point set Xwith labels given by a predictor\n\u0005 with label error rate \u0015\nOutput: (1+O(\u000b))-approximate k-means clustering of\nX\n1:fori= 1 toi=kdo\n2: LetYibe the set of points with label i.\n3: RunCrdEst for each of the dcoordinates of Yi.\n4: LetC0\nibe the coordinate-wise outputs of\nCrdEst .\n5:end for\n6:ReturnC0\n1;:::;C0\nk.Algorithm 2 Coordinate-wise estima-\ntionCrdEst\nInput: Pointsx1;:::;x 2m2R, corrup-\ntion level\u0015\u0014\u000b\n1:Randomly partition the points into\ntwo groups X1;X2of sizem.\n2:LetI= [a;b] be the shortest interval\ncontainingm(1\u00005\u000b) points ofX1.\n3:Z X2\\I\n4:z 1\njZjP\nx2Zx\n5:Returnz\n2.1 Our Algorithm\nOur main result is an algorithm for outputting a clustering that achieves a (1+20 \u000b) approximation\n2to the optimal objective cost when given access to approximations of the correct labeling of the\npoints inP. We \frst present a suboptimal algorithm in Algorithm 1 for intuition and then optimize\nthe runtime in Algorithm 3, which is provided in Section 3.\nThe intuition for Algorithm 1 is as follows. We \frst address the problem of identifying an\napproximate center for each cluster. Let Copt\n1;\u0001\u0001\u0001;Copt\nkbe an optimal grouping of the points and\nconsider all the points labeled iby our predictor for some \fxed 1 \u0014i\u0014k. Since our predictor\ncan err, a large number of points that are not in Copt\nimay also be labeled i. This is especially\nproblematic when points that are \\signi\fcantly far\" from cluster Copt\niare given the label i, which\nmay increase the objective function arbitrarily if we simply take the mean of the points labeled i\nby the predictor.\nTo \flter out such outliers, we consider a two step view from the robust statistics literature,\ne.g., Prasad et al. (2019); these two steps can respectively be interpreted as a \\training\" phase and\na \\testing\" phase that removes \\bad\" outliers. We \frst randomly partition the points that are\ngiven label iinto two groups, X1andX2, of equal size. We then estimate the mean of Copt\niusing a\ncoordinate-wise approach through Algorithm 2 ( CrdEst ), decomposing the total cost as the sum\nof the costs in each dimension.\nFor each coordinate, we \fnd the smallest interval Ithat contains a (1 \u00004\u000b) fraction of the\npoints inX1. We show that for label error rate \u0015\u0014\u000b, this \\training\" phase removes any outliers\nand thus provides a rough estimation to the location of the \\true\" points that are labeled i. To\nremove dependency issues, we then \\test\" X2onIby computing the mean of X2\\I. This allows\nus to get empirical centers that are a su\u000eciently good approximation to the coordinates of the\ntrue center for each coordinate. We then repeat on the other labels. The key insight is that the\nerror from mean-estimation can be directly charged to the approximation error due to the special\nstructure of the k-means problem. Our main theoretical result considers predictors that err on at\nmost a\u0015-fraction of all cluster labels. Note that all omitted proofs appear in the supplementary\n2Note that we have not attempted to optimize the constant 20.\n6\n\nmaterial.\nTheorem 2.1. Let\u000b2(10 logn=pn;1=7),\u0005be a predictor with label error rate \u0015\u0014\u000b, and\r\u00151\na su\u000eciently large constant. If each cluster in the (1+\u000b)-approximately optimal k-means clustering\nof the predictor has at least \r\u0011k=\u000b points, then Algorithm 1 outputs a (1 + 20\u000b)-approximation to\nthek-means objective with prob. 1\u00001=\u0011, usingO(kdnlogn)total time.\nWe improve the running time to O(ndlogn+ poly(k;logn)) in Theorem 3.4 in Section 3.\nOur algorithms can also tolerate similar error rates when failures correspond to random labels,\nadversarial labels, or a special failure symbol.\nError rate \u0015vs. accuracy parameter \u000b.We emphasize that \u0015is the error rate of the\npredictor and \u000bis only some loose upper bound on \u0015. It is reasonable that some algorithms can\nprovide lossy guarantees on their outputs, which translates to the desired loose upper bound \u000bon\nthe accuracy of the predictor. Even if is not known, multiple instances of the algorithm can be run\nin parallel with separate exponentially decreasing \\guesses\" for the value \u000b. We can simply return\nthe best clustering among these algorithms, which will provide the same theoretical guarantees as\nif we set\u000b= 1:01\u0015, for example. Thus \u000bdoes not need to be known in advance and it does not\nneed to be tuned as a hyperparameter.\n3 Nearly Optimal Runtime Algorithm\nWe now describe Algorithm 3, which is an optimized runtime version of Algorithm 1 and whose\nguarantees we present in Theorem 3.4. The bottleneck for Algorithm 1 is that after selecting k\nempirical centers, it must still assign each of the npoints to the closest empirical center. The\nmain intuition for Algorithm 3 is that although reading all points uses O(nd) time, we do not need\nto spendO(dk) time per point to \fnd its closest empirical center, if we set up the correct data\nstructures. In fact, as long as we assign each point to a \\relatively good\" center, the assigned\nclustering is still a \\good\" approximation to the optimal solution. Thus we proceed in a similar\nmanner as before to sample a number of input points and \fnd the optimal kcenters for the sampled\npoints.\nWe use dimensionality reduction and an approximate nearest neighbor (ANN) data structure\nto e\u000eciently assign each point to a \\su\u000eciently close\" center. Namely if a point p2Pshould be\nassigned to its closest empirical Cithenpmust be assigned to some empirical center Cjsuch that\nkp\u0000Cjk2\u00142kp\u0000Cik2. Hence, points that are not assigned to their optimal centers only incur a\n\\small\" penalty due to the ANN data structure and so the cost of the clustering does not increase\n\\too much\" in expectation. Formally, we need the following de\fnitions.\nTheorem 3.1 (JL transform) .Johnson & Lindenstrauss (1984) Let d(\u0001;\u0001)be the standard Euclidean\nnorm. There exists a family of linear maps A:Rd!Rkand an absolute constant C > 0such that\nfor anyx;y2Rd,Pr[\u001e2A;d(\u001e(x);\u001e(y))2(1\u0006\u000b)d(x;y)]\u00151\u0000e\u0000C\u000b2k.\nDe\fnition 3.2 (Terminal dimension reduction) .Given a set of points called terminals C\u001aRd,\nwe call a map f:Rd!Rkaterminal dimension reduction with distortion Dif for every terminal\nc2Cand pointp2Rd, we haved(p;c)\u0014d(f(p);f(c))\u0014D\u0001d(p;c).\n7\n\nDe\fnition 3.3 (Approximate nearest neighbor search) .Given a set Pofnpoints in a metric space\n(X;d), a(c;r)-approximate nearest neighbor search (ANN) data structure takes any query point\nq2Xwith non-emptyfp2P: 0<d(p;q)\u0014rgand outputs a point in fp2P: 0<d(p;q)\u0014crg.\nAlgorithm 3 Fast learning-augmented algorithm for k-means clustering.\nInput: A point set X, a predictor \u0005 with label error rate \u0015\u0014\u000b, and a tradeo\u000b parameter \u0010\nOutput: A (1 +\u000b)-approximate k-means clustering of X\n1:FormSby sampling each point of Xwith probability100 logk\n\u000bjAxjwhereAxis the set of points with\nthe same label as xaccording to \u0005.\n2:LetC1;:::;Ckbe the output of Algorithm 1 on S.\n3:Let\u001e2be a random JL linear map with distortion5\n4, i.e., dimension O(logn).\n4:Let\u001e1be a terminal dimension reduction with distortion5\n4.\n5:Let\u001e:=\u001e1\u000e\u001e2be the composition map.\n6:LetAbe a (2;r)-ANN data structure on the points \u001e(C1);:::;\u001e (Ck).\n7:forx2Xdo\n8: Let`xbe the label of xfrom \u0005.\n9:% d(x;C`x)\n10: QueryAto \fnd the closest center \u001e(Cpx) toxwithr=%\n2.\n11: ifd(x;Cpx)<2d(x;C`x)then\n12: Assign label pxtox.\n13: else\n14: Assign label `xtox.\n15: end if\n16:end for\nTo justify the guarantees of Algorithm 3, we need runtime guarantees on creating a suitable\ndimensionality reduction map and an ANN data structure. These are from Makarychev et al.\n(2019) and Indyk & Motwani (1998); Har-Peled et al. (2012); Andoni et al. (2018) respectively,\nand are stated in Theorems A:12 andA:13 in the supplementary section. They ensure that each\npoint is mapped to a \\good\" center. Thus, we obtain our main result describing the guarantees of\nAlgorithm 3.\nTheorem 3.4. Let\u000b2(10 logn=pn;1=7),\u0005be a predictor with label error rate \u0015\u0014\u000b, and\r\u00151\nbe a su\u000eciently large constant. If each cluster in the optimal k-means clustering of the predictor\nhas at least \rklogk=\u000b points, then Algorithm 3 outputs a (1 + 20\u000b)-approximation to the k-means\nobjective with probability at least 3=4, usingO(ndlogn+ poly(k;logn))total time.\nNote that if we wish to only output the kcenters rather than labeling all of the input points, then\nthe query complexity of Algorithm 3 is eO(k=\u000b) (see Step 1 of Algorithm 3) with high probability.\nWe show in the supplementary material that this is nearly optimal.\nTheorem 3.5. For any\u000e2(0;1], any algorithm that makes O\u0010\nk1\u0000\u000e\n\u000blogn\u0011\nqueries to the predictor\nwith label error rate \u000bcannot output a (1 +C\u000b)-approximation to the optimal k-means clustering\ncost in time 2O(n1\u0000\u000e)time, assuming the Exponential Time Hypothesis.\n8\n\n4 Experiments\nIn this section we evaluate Algorithm 1 empirically on real datasets. We choose to implement\nAlgorithm 1, as opposed to the runtime optimal Algorithm 3, for simplicity and because the goal\nof our experiments is to highlight the error guarantees of our methodology, which both algorithms\nshare. Further, we will see that Algorithm 1 is already very fast compared to alternatives. Thus, we\nimplement the simpler of the two algorithms. We primarily \fx the number of clusters to be k= 10\nandk= 25 throughout our experiments for all datasets. Note that our predictors can readily\ngeneralize to other values of kbut we focus on these two values for clarity. All of our experiments\nwere done on a CPU with i5 2.7 GHz dual core and 8 GB RAM. Furthermore, all our experimental\nresults are averaged over 20 independent trials and \u0006one standard deviation error is shaded when\napplicable. We give the full details of our datasets below.\n1) Oregon : Dataset of 9 graph snapshots sampled across 3 months from an internet router\ncommunication network (Leskovec et al., 2005). We then use the top two eigenvectors of the\nnormalized Laplacian matrix to give us node embeddings into R2for each graph which gives us\n9 datasets, one for each graph. Each dataset has roughly n\u0018104points. This is an instance\nof spectral clustering. 2) PHY : Dataset from KDD cup 2004 (kdd, 2004). We take 104random\nsamples to form our dataset. 3) CIFAR10 : Testing portion of CIFAR-10 ( n= 104, dimension\n3072) (Krizhevsky, 2009).\nBaselines. We compare against the following algorithms. Additional experimental results on\nLloyd's heuristic are given in Section E.3 in the supplementary material.\n1)kmeans++ : We measure the performance of our algorithm in comparison to the kmeans++ seeding\nalgorithm. Since kmeans++ is a randomized algorithm, we take the average clustering cost after\nrunning kmeans++ seeding on 20 independent trials. We then standardize this value to have cost\n1:0 and report all other costs in terms of this normalization. For example, the cost 2 :0 means that\nthe clustering cost is twice that of the average kmeans++ clustering cost. We also use the labels\nofkmeans++ as the predictor in the input for Algorithm 1 (denoted as \\Alg + kmeans++ \") which\nserves to highlight the fact that one can use any heuristic or approximate clustering algorithm as\na predictor.\n2) Random sampling : For this algorithm, we subsample the predictor labels with probability q\nranging from 1% to 50%. We then construct the k-means centers using the labels of the sampled\npoints and measure the clustering cost using the whole dataset. We use the bestvalue ofqin our\nrange every time to give this baseline as much power as possible. We emphasize that random sam-\npling cannot have theoretical guarantees since the random sample can be corrupted (similarly as in\nthe example in Section 1.1). Thus some outlier detection steps (such as our algorithms) are required.\nPredictor Description. We use the following predictors in our experiments.\n1) Nearest neighbor : We use this predictor for the Oregon dataset. We \fnd the best clustering\nof the node embeddings in Graph #1. In practice, this means running many steps of Lloyd's\nalgorithm until convergence after initial seeding by kmeans++ . Our predictor takes as input a point\ninR2representing a node embedding of any of the later 8 graphs and outputs the label of the\nclosest node in Graph #1.\n2) Noisy predictor. This is the main predictor for PHY. We form this predictor by \frst \fnding\nthe bestk-means solution on our datasets. This again means initial seeding by kmeans++ and then\n9\n\nmany steps of Lloyd's algorithm until convergence. We then randomly corrupt the resulting labels\nby changing them to a uniformly random label independently with error probability ranging from\n0 to 1. We report the cost of clustering using only these noisy labels versus processing these labels\nusing Algorithm 1.\n3) Neural network. We use a standard neural network architecture (ResNet18) trained on\nthe training portion of the CIFAR-10 dataset as the oracle for the testing portion which we use\nin our experiments. We used a pretrained model obtained from Huy (2020). Note that the neural\nnetwork is predicting the class of the input image. However, the class value is highly correlated\nwith the optimal k-means cluster group.\nSummary of results. Our experiments show that our algorithm can leverage predictors to\nsigni\fcantly improve the cost of k-means clustering and that good predictors can be easily tailored to\nthe data at hand. The cost of k-means clustering reduces signi\fcantly after applying our algorithm\ncompared to just using the predictor labels for two of our predictors. Lastly, the quality of the\npredictor remains high for the Oregon dataset even though the later graphs have changed and\n\\moved away\" from Graph #1.\nSelecting \u000bin Algorithm 2. In practice, the choice of \u000bto use in our algorithm depends on\nthe given predictor whose properties may be unknown. Since our goal is to minimize the k-means\nclustering objective (1), we can simply pick the `best' value. To do so, we iterate over a small range\nof possible \u000bfrom:01 to:15 in Algorithm 2 with a step size of 0 :01 and select the clustering that\nresults in the lowest objective cost. The range is \fxed for all of our experiments. (See Paragraph\n2.1\n4.1 Results\n2 3 4 5 6 7 8 9\nGraph #0.00.10.20.30.40.5Clustering CostDataset: Oregon Spectral Clustering, k=10\nAlg + Predictor\nAlg + k-means++\nRandom Sampling\nPredictor\n(a)k= 10\n2 3 4 5 6 7 8 9\nGraph #0.00.20.40.60.81.01.21.41.6Clustering CostDataset: Oregon Spectral Clustering, k=25\nAlg + Predictor\nAlg + k-means++\nRandom Sampling\nPredictor (b)k= 25\n0 5 10 15 20 25\nCorruption %02468Clustering Cost\nDataset: Oregon Spectral Clustering, Graph #5, k=10\nAlg + Noisy Predictor\nRandom Sampling\nNoisy Predictor\nkmeans++ (c)k= 25\nFigure 1: Performance of Algorithm 1 on later graph embeddings using Graph #1 as predictor.\nOregon. We \frst compare our algorithm with Graph #1 as the predictor against various\nbaselines. This is shown in Figures 1(a) and Figure 1(b). In the k= 10 case, Figure 1(a) shows that\nthe predictor returns a clustering better than using just the kmeans++ seeding, which is normalized\nto have cost 1 :0. This is to be expected since the subsequent graphs represent a similar network as\nGraph #1, just sampled later in time. However, the clustering improves signi\fcantly after using\nour algorithm on the predictor labels as the average cost drops by 55%. We also see that using\nour algorithm after kmeans++ is also su\u000ecient to give signi\fcant decrease in clustering cost. Lastly,\n10\n\n0 10 20 30 40 50\nCorruption %0.00.51.01.52.02.5Clustering Cost\nDataset: PHY, k=10\nAlg + Noisy Predictor\nAlg + k-means++\nRandom Sampling\nNoisy Predictor\nkmeans++(a) PHY, k= 10\n0.0 0.2 0.4 0.6 0.8 1.0\nFraction Queried024681012Percent Increase in Clustering Cost\nDataset: CIFAR-10, k=10 (b) CIFAR-10, k= 10\nFigure 2: Our algorithm is able to recover a good clustering even for very high levels of noise.\nrandom sampling also gives comparable results. This can be explained because we are iterating\nover a large range of subsampling probabilities for random sampling.\nIn thek= 25 case, Figure 1(b) shows that the oracle performance degrades and is worse than\nthe baseline in 5 of the 8 graphs. However our algorithm again improves the quality of the clustering\nover the oracle across all graphs. Using kmeans++ as the predictor in our algorithm also improves\nthe cost of clustering. The performance of random sampling is also worse. For example in Graph\n#3 fork= 25, it performed the worst out of all the tested algorithms.\nOur algorithm also remains competitive with kmeans++ seeding even if the predictor for the\nOregon dataset is highly corrupted. We consider a later graph, Graph #5, and corrupt the labels\nof the predictor randomly with probability qranging from 1% to 25% for the k= 10 case in Figure\n1(c). While the cost of clustering using just the predictor labels can become increasingly worse, our\nalgorithm is able to su\u000eciently \\clean\" the predictions. In addition, the cost of random sampling\nalso gets worse as the corruptions increase, implying that it is much more sensitive to noise than\nour algorithm. The qualitatively similar plot for k= 25 is given in the supplementary section.\nNote that in spectral clustering, one may wish to get a mapping to Rdford>2. We envision that\nour results translate to those settings as well since having higher order spectral information only\nresults in a stronger predictor.\nPHY. We use the noisy predictor for this dataset. We see in Figure 2(a) that as the corruption\npercentage rises, the clustering given by just the predictor labels can have increasingly large cost.\nNevertheless, even if the clustering cost of the corrupted labels is rising, the cost decreases signi\f-\ncantly after applying Algorithm 1 by roughly a factor of 3x. Indeed, we see that our algorithm can\nbeat the kmeans++ seeding baseline for qas high as 50%. Just as in Figure 1(c), random sampling\nis sensitive to noise. Lastly, we also remain competitive with the purple line which uses the labels\noutput by kmeans++ as the predictor in our algorithm (no corruptions added). The qualitatively\nsimilar plot for k= 25 is given in the supplementary material.\nCIFAR-10. The cost of clustering on CIFAR-10 using only the predictor, the predictor with our\nalgorithm, random sampling, and kmeans++ as the predictor for our algorithm were 0 :733, 0:697,\n0:721, and 0:640, respectively, where 1 :0 represents the cost of kmeans++ . The neural network was\nvery accurate (\u001893%) in predicting the class of the input points which is highly correlated with the\noptimalk-means clusters. Nevertheless, our algorithm improved upon this highly precise predictor.\nNote that using kmeans++ as the predictor for our algorithm resulted in a clustering that was\n13%better than the one given by the neural network predictor. This highlights the fact that an\napproximate clustering combined with our algorithm can be competitive against a highly precise\n11\n\npredictor, such as a neural network, even though creating the highly accurate predictor can be\nexpensive. Indeed, obtaining a neural network predictor requires prior training data and also the\ntime to train the network. On the other hand, using a heuristic clustering as a predictor is extremely\n\rexible and can be applied to any dataset even if no prior training dataset is available (50 ;000 test\nimages were required to train the neural network predictor but kmeans++ as a predictor requires\nno test images), in addition to considerable savings in computation.\nFor example, the time taken to train the particular neural network we used was approximately\n18 minutes using the optimized PyTorch library (see training details under the \\Details Report\"\nsection in Huy (2020)). In general, the time can be much longer for more complicated datasets.\nOn the other hand, our unoptimized algorithm implementation which used the labels of a sample\nrun of kmeans++ was still able to achieve a better clustering than the neural network predictor\nwith\u000b= 0:01 in Algorithm 2 in 4 :4 seconds. In conclusion, we can achieve a better clustering by\ncombining a much weaker predictor with our algorithm with the additional bene\ft of using a more\n\rexible and computationally inexpensive methodology.\nWe also conducted an experiment where we only use a small fraction of the predictor labels\nin our algorithm. We select a pfraction of the images, query their labels, and run our algorithm\non only these points. We then report the cost of clustering on the entire dataset as pranges from\n1% to 100%. Figure 2(b) shows the percentage increase in clustering cost relative to querying the\nwhole dataset is quite low for moderate values of pbut increasingly worse as pbecomes smaller.\nThis suggests that the quality of our algorithm does not su\u000ber drastically by querying a smaller\nfraction of the dataset.\nComparison to Lloyd's Heuristic. In Section E.3, we provide additional results on experi-\nments using Lloyd's heuristic. In summary, we give both theoretical and empirical justi\fcations for\nwhy our algorithms are superior to blindly following a predictor and then running Lloyd's heuristic.\nAcknowledgements\nZhili Feng, David P. Woodruf, and Samson Zhou would like to thank partial support from NSF\ngrant No. CCF- 181584, O\u000ece of Naval Research (ONR) grant N00014-18-1-256, and a Simons\nInvestigator Award. Sandeep Silwal was supported in part by a NSF Graduate Research Fellowship\nProgram.\nReferences\nKdd cup. http://osmot.cs.cornell.edu/kddcup/datasets.html , 2004.\nManu Agarwal, Ragesh Jaiswal, and Arindam Pal. k-means++ under approximation stability.\nTheor. Comput. Sci. , 588:37{51, 2015.\nSara Ahmadian, Ashkan Norouzi-Fard, Ola Svensson, and Justin Ward. Better guarantees for\nk-means and euclidean k-median by primal-dual algorithms. SIAM J. Comput. , 49(4), 2020.\nNir Ailon, Anup Bhattacharya, Ragesh Jaiswal, and Amit Kumar. Approximate Clustering with\nSame-Cluster Queries. In 9th Innovations in Theoretical Computer Science Conference (ITCS) ,\npp. 40:1{40:21, 2018.\n12\n\nAlexandr Andoni, Piotr Indyk, and Ilya P. Razenshteyn. Approximate nearest neighbor search in\nhigh dimensions. CoRR , abs/1806.09823, 2018.\nMartin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations . Cam-\nbridge University Press, 1999. doi: 10.1017/CBO9780511624216.\nDavid Arthur and Sergei Vassilvitskii. How slow is the k-means method? In SCG '06 , 2006.\nDavid Arthur and Sergei Vassilvitskii. k-means++: the advantages of careful seeding. In Proceedings\nof the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA , pp. 1027{1035,\n2007.\nHassan Ashtiani, Shrinu Kushagra, and Shai Ben-David. Clustering with same-cluster queries. In\nAdvances in Neural Information Processing Systems 29 , pp. 3216{3224, 2016.\nPranjal Awasthi, Maria Florina Balcan, and Konstantin Voevodski. Local algorithms for interactive\nclustering. Journal of Machine Learning Research , 18(3):1{35, 2017.\nPranjal Awasthi, Ainesh Bakshi, Maria-Florina Balcan, Colin White, and David P. Woodru\u000b. Ro-\nbust communication-optimal distributed clustering algorithms. In 46th International Colloquium\non Automata, Languages, and Programming, ICALP , pp. 18:1{18:16, 2019.\nMaria-Florina Balcan and Avrim Blum. Clustering with interactive feedback. In Proceedings of the\n19th International Conference on Algorithmic Learning Theory , pp. 316{328, 2008.\nMaria-Florina Balcan, Avrim Blum, and Anupam Gupta. Clustering under approximation stability.\nJ. ACM , 60(2):8:1{8:34, 2013.\nMaria-Florina Balcan, Nika Haghtalab, and Colin White. k-center clustering under perturbation\nresilience. ACM Trans. Algorithms , 16(2):22:1{22:39, 2020.\nSugato Basu, Arindam Banerjee, and Raymond J. Mooney. Active semi-supervision for pairwise\nconstrained clustering. In Proceedings of the 2004 SIAM International Conference on Data\nMining (SDM-04) , April 2004.\nAshish Bora, Ajil Jalal, Eric Price, and Alexandros G. Dimakis. Compressed sensing using gener-\native models. In Proceedings of the 34th International Conference on Machine Learning, ICML ,\npp. 537{546, 2017.\nVladimir Braverman, Adam Meyerson, Rafail Ostrovsky, Alan Roytman, Michael Shindler, and\nBrian Tagiku. Streaming k-means on well-clusterable data. In Proceedings of the Twenty-Second\nAnnual ACM-SIAM Symposium on Discrete Algorithms, SODA , pp. 26{40. SIAM, 2011.\nI Chien, Chao Pan, and Olgica Milenkovic. Query k-means clustering and the double dixie cup\nproblem. In Advances in Neural Information Processing Systems 31 , pp. 6649{6658. 2018.\nMiroslav Chleb\u0013 \u0010k and Janka Chleb\u0013 \u0010kov\u0013 a. Complexity of approximating bounded variants of opti-\nmization problems. Theor. Comput. Sci. , 354(3):320{338, 2006.\nMichael B. Cohen, Yin Tat Lee, Gary L. Miller, Jakub Pachocki, and Aaron Sidford. Geometric\nmedian in nearly linear time. In Proceedings of the 48th Annual ACM SIGACT Symposium on\nTheory of Computing, STOC , pp. 9{21, 2016.\n13\n\nVincent Cohen-Addad and Karthik C. S. Inapproximability of clustering in lpmetrics. 2019 IEEE\n60th Annual Symposium on Foundations of Computer Science (FOCS) , pp. 519{539, 2019.\nHanjun Dai, Elias B. Khalil, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial\noptimization algorithms over graphs. In Proceedings of the 31st International Conference on\nNeural Information Processing Systems , pp. 6351{6361, 2017.\nSanjoy Dasgupta. How fast is k-means? In Computational Learning Theory and Kernel Ma-\nchines, 16th Annual Conference on Computational Learning Theory and 7th Kernel Workshop,\nCOLT/Kernel, Proceedings , pp. 735, 2003.\nSanjoy Dasgupta. The hardness of k-means clustering. 2008.\nMichael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Faster\nmatchings via learned duals. In Advances in Neural Information Processing Systems , 2021. URL\nhttps://arxiv.org/abs/2107.09770 .\nYihe Dong, Piotr Indyk, Ilya Razenshteyn, and Tal Wagner. Learning space partitions for nearest\nneighbor search. In International Conference on Learning Representations , 2020.\nTalya Eden, Piotr Indyk, Shyam Narayanan, Ronitt Rubinfeld, Sandeep Silwal, and Tal Wagner.\nLearning-based support estimation in sublinear time. In International Conference on Learning\nRepresentations , 2021.\nDan Feldman, Morteza Monemizadeh, and Christian Sohler. A PTAS for k-means clustering based\non weak coresets. In Proceedings of the 23rd ACM Symposium on Computational Geometry , pp.\n11{18, 2007.\nDimitris Fotakis, Michael Lampis, and Vangelis Th. Paschos. Sub-exponential approximation\nschemes for csps: From dense to almost sparse. In 33rd Symposium on Theoretical Aspects\nof Computer Science, STACS , pp. 37:1{37:14, 2016.\nBuddhima Gamlath, Sangxia Huang, and Ola Svensson. Semi-Supervised Algorithms for Ap-\nproximately Optimal and Accurate Clustering. In 45th International Colloquium on Automata,\nLanguages, and Programming (ICALP) , pp. 57:1{57:14, 2018.\nSariel Har-Peled and Bardia Sadri. How fast is the k-means method? Algorithmica , 41:185{202,\n2005.\nSariel Har-Peled, Piotr Indyk, and Rajeev Motwani. Approximate nearest neighbor: Towards\nremoving the curse of dimensionality. Theory Comput. , 8(1):321{350, 2012.\nJohan H\u0017 astad. Some optimal inapproximability results. J. ACM , 48(4):798{859, 2001.\nJ. A. Howe. Improved clustering with augmented k-means. arXiv: Machine Learning , 2017.\nChen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency estimation\nalgorithms. In International Conference on Learning Representations , 2019.\nWasim Huleihel, Arya Mazumdar, Muriel Medard, and Soumyabrata Pal. Same-cluster querying for\noverlapping clusters. In Advances in Neural Information Processing Systems 32 , pp. 10485{10495.\n2019.\n14\n\nPhan Huy. Pytorchcifar10. https://github.com/huyvnphan/PyTorch_CIFAR10 , 2020.\nRussell Impagliazzo and Ramamohan Paturi. On the complexity of k-sat. J. Comput. Syst. Sci. ,\n62(2):367{375, 2001.\nMary Inaba, Naoki Katoh, and Hiroshi Imai. Applications of weighted voronoi diagrams and\nrandomization to variance-based k-clustering (extended abstract). In Proceedings of the Tenth\nAnnual Symposium on Computational Geometry , pp. 332{339, 1994.\nPiotr Indyk and Rajeev Motwani. Approximate nearest neighbors: Towards removing the curse\nof dimensionality. In Proceedings of the Thirtieth Annual ACM Symposium on the Theory of\nComputing , pp. 604{613, 1998.\nZachary Izzo, Sandeep Silwal, and Samson Zhou. Dimensionality reduction for wasserstein barycen-\nter. In Advances in Neural Information Processing Systems , 2021.\nTanqiu Jiang, Yi Li, Honghao Lin, Yisong Ruan, and David P. Woodru\u000b. Learning-augmented\ndata stream algorithms. In International Conference on Learning Representations , 2020.\nWilliam B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space,\n1984.\nTaewan Kim and Joydeep Ghosh. Relaxed oracles for semi-supervised clustering. CoRR ,\nabs/1711.07433, 2017.\nTim Kraska, Alex Beutel, Ed H. Chi, Je\u000brey Dean, and Neoklis Polyzotis. The case for learned\nindex structures. In Proceedings of the 2018 International Conference on Management of Data ,\npp. 489{504, 2018.\nRobert Krauthgamer. Randomized algorithms course notes. http://www.wisdom.weizmann.ac.\nil/~robi/teaching/2019a-RandomizedAlgorithms/lecture8.pdf , 2019. Lecture 8.\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\nAmit Kumar, Yogish Sabharwal, and Sandeep Sen. A simple linear time (1 + \u000f)-approximation\nalgorithm for k-means clustering in any dimensions. pp. 454{462, 2004.\nSilvio Lattanzi and Christian Sohler. A better k-means++ algorithm via local search. In Proceedings\nof the 36th International Conference on Machine Learning, ICML , pp. 3662{3671, 2019.\nEuiwoong Lee, Melanie Schmidt, and John Wright. Improved and simpli\fed inapproximability for\nk-means. Inf. Process. Lett. , 120:40{43, 2017.\nJure Leskovec, Jon M. Kleinberg, and Christos Faloutsos. Graphs over time: densi\fcation laws,\nshrinking diameters and possible explanations. In Proceedings of the Eleventh ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining , pp. 177{187, 2005.\nMario Lucic, Matthew Faulkner, Andreas Krause, and Dan Feldman. Training gaussian mixture\nmodels at scale via coresets. Journal of Machine Learning Research , 18(160):1{25, 2018. URL\nhttp://jmlr.org/papers/v18/15-506.html .\n15\n\nThodoris Lykouris and Sergei Vassilvtiskii. Competitive caching with machine learned advice.\nvolume 80 of Proceedings of Machine Learning Research , pp. 3296{3305. PMLR, 2018.\nKonstantin Makarychev, Yury Makarychev, and Ilya P. Razenshteyn. Performance of johnson-\nlindenstrauss transform for k-means and k-medians clustering. In Proceedings of the 51st Annual\nACM SIGACT Symposium on Theory of Computing, STOC , pp. 1027{1038, 2019.\nArya Mazumdar and Barna Saha. Clustering with noisy queries. In Advances in Neural Information\nProcessing Systems 30: Annual Conference on Neural Information Processing Systems , pp. 5788{\n5799, 2017.\nMichael Mitzenmacher. A model for learned bloom \flters, and optimizing by sandwiching. In\nProceedings of the 32nd International Conference on Neural Information Processing Systems ,\npp. 462{471, 2018.\nMichael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions, 2020.\nAli Mousavi, Ankit B. Patel, and Richard G. Baraniuk. A deep learning approach to structured sig-\nnal recovery. In 53rd Annual Allerton Conference on Communication, Control, and Computing,\nAllerton , pp. 1336{1343, 2015.\nAdarsh Prasad, Sivaraman Balakrishnan, and Pradeep Ravikumar. A uni\fed approach to robust\nmean estimation. CoRR , abs/1907.00927, 2019.\nManish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ml predictions.\nInAdvances in Neural Information Processing Systems 31 , pp. 9661{9670. 2018.\nAlexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, and Herv\u0013 e J\u0013 egou. Spreading vectors for\nsimilarity search. In International Conference on Learning Representations , 2019.\nThomas Sanchez, Baran G ozc u, Ruud B. van Heeswijk, Armin Eftekhari, Efe Ilicak, Tolga C \u0018 ukur,\nand Volkan Cevher. Scalable learning-based sampling optimization for compressive dynamic\nMRI. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP ,\npp. 8584{8588, 2020.\nMingjun Song and S. Rajasekaran. Fast algorithms for constant approximation k-means clustering.\nTrans. Mach. Learn. Data Min. , 3:67{79, 2010.\nAndrea Vattani. k-means requires exponentially many iterations even in the plane. Discrete &\nComputational Geometry , 45:596{616, 2011.\nSharad Vikram and Sanjoy Dasgupta. Interactive bayesian hierarchical clustering. volume 48 of\nProceedings of Machine Learning Research , pp. 2081{2090. PMLR, 2016.\nJ. Wang, W. Liu, S. Kumar, and S. Chang. Learning to hash for indexing big data|a survey.\nProceedings of the IEEE , 104(1):34{57, 2016.\n16\n\nA Appendix\nTheorem A.1 (Cherno\u000b Bounds) .LetX1;:::;Xnbe independent random variables taking values\ninf0;1g. LetX=Pn\ni=1Xidenote their sum and let \u0016=E[X]denote the sum's expected value.\nThen for any \u000e2(0;1)andt>0,\nPr[X\u0014(1\u0000\u000e)\u0016]\u0014e\u0000\u000e2\u0016\n2:\nFor any\u000e>0,\nPr[X\u0015(1 +\u000e)\u0016]\u0014e\u0000\u000e2\u0016\n3:\nFurthermore,\nPr[jX\u0000\u0016j\u0015t]\u0014e\u0000t2\n4n:\nA.1 Proof of Theorem 2.1\nWe \frst prove Theorem 2.1, which shows that Algorithm 1 provides a (1+ \u000b)-approximation to the\noptimalk-means clustering, but uses suboptimal time compared to a faster algorithm we present\nin Section 3. All omitted proofs of lemmas appear in Section A.2.\nWe \frst show that for each coordinate, the empirical center for any (1 \u0000\u000b)-fraction of the input\npoints provides a good approximation to the optimal k-means clustering cost.\nLemma A.2. LetP;Q\u0012Rbe sets of points on the real line such that jPj \u0015 (1\u0000\u000b)nand\njQj\u0014\u000bn. LetX=P[Q,CPbe the mean of PandCXbe the mean of X. Then cost(X;CP)\u0014\u0010\n1 +\u000b\n1\u0000\u000b2\u0011\ncost(X;CX).\nWe now show that a conceptual interval I\u0003\u001aRwith \\small\" length contains a signi\fcant\nfraction of the true points. Ultimately, we will show that the interval Icomputed in the \\training\"\nphase in CrdEst has smaller length than I\u0003with high probability and yet Ialso contains a\nsigni\fcant fraction of the true points. The main purpose of I\u0003(and eventually I) is to \flter out\nextreme outliers because the \\testing\" phase only considers points in I\\X2.\nLemma A.3. For a \fxed set X\u0012R, letCbe the mean of Xand\u001b2=1\n2jXjP\nx2X(x\u0000C)2be\nthe variance. Then the interval I\u0003=h\nC\u0000\u001bp\u000b;C+\u001bp\u000bi\ncontains at least a (1\u00004\u000b)fraction of the\npoints inX.\nUsing Lemma A.3, we show that the interval Ithat is computed in the \\training\" phase contains\na signi\fcant fraction of the true points.\nLemma A.4. Letmbe a su\u000eciently large consatnt. We have that I:= [a;b]contains at least a\n1\u00006\u000bfraction of points of X2andb\u0000a\u00142\u001b=p\u000b, with high probability, i.e., 1\u00001=poly(m).\nWe next show that the optimal clustering on a subset obtained by independently sampling each\ninput point provides a rough approximation of the optimal clustering. That is, the optimal center\nis well-approximated by the empirical center of the sampled points.\n17\n\nLemma A.5. LetSbe a set of points obtained by independently sampling each point of X\u0012Rd\nwith probability p=1\n2. LetCbe the optimal center of these points and CSbe the empirical center\nof these points. Conditioned on jSj\u00151, then E[CS] =xand there exists a constant \rsuch that for\n\u0011\u00151andjXj>\u0011\rk\n\u000b,\nE\u0002\nkCS\u0000xk2\n2\u0003\n\u0014\r\njXj2\u0001 X\nx2Xkx\u0000xk2\n2!\nPr[cost(X;CS)>(1 +\u000b) cost(X;C)]<1=(\u0011k):\nUsing Lemma A.2, Lemma A.4, and Lemma A.5, we justify the correctness of the subroutine\nCrdEst .\nLemma A.6. Let\u000b2(10 logn=pn;1=7). LetP;Q\u0012Rbe sets of points on the real line such\nthatjPj\u0015(1\u0000\u000b)2mandjQj\u00142\u000bm, andX=P[Q. LetCbe the center of P. Then CrdEst\non input set Xoutputs a point C0such that with probability at least 1\u00001=(\u0011k),cost(P;C0)\u0014\n(1 + 18\u000b)(1 +\u000b)\u0010\n1 +\u000b\n(1\u0000\u000b)2\u0011\ncost(P;C).\nUsing CrdEst as a subroutine for each coordinate, we now prove Theorem 2.1, justifying the\ncorrectness of Algorithm 1 by generalizing to all coordinates and centers and analyzing the runtime\nof Algorithm 1.\nProof of Theorem 2.1. Since \u0005 has label error rate \u0015\u0014\u000b, then by de\fnition of label error rate,\nat least a (1\u0000\u000b) fraction of the points in each cluster are correctly labeled. Note that the k-\nmeans clustering cost can be decomposed into the sum of the costs induced by the centers in each\ndimension. Speci\fcally, for a set C=fC1;:::;Ckgof optimal centers,\ncost(X;C) :=X\nx2Xd(x;C)2=kX\ni=1X\nx2Sid(x;Ci)2;\nwhereSiis the set of points in Xthat are assigned to center Ci. For a particular i2[k], we have\nX\nx2Sid(x;Ci)2=X\nx2SidX\nj=1d(xj;(Ci)j)2;\nwherexjand (Ci)jare thej-th coordinate of xandCi, respectively.\nBy Lemma A.6, the cost induced by CrdEst for each dimension in each center C0\niis a (1 +\n\u000b)-approximation of the total clustering cost for the optimal center Ciin that dimension with\nprobability 1\u00001=(\u0011k). That is,\nX\nx2Sid(xj;(C0\ni)j)2\u0014(1 + 18\u000b)(1 +\u000b)(1 +\u000b=(1\u0000\u000b)2)X\nx2Sid(xj;(Ci)j)2\nfor eachj2[d]. Thus, taking a sum over all dimensions j2[d] and union bounding over all centers\ni2[k], we have that the total cost induced by Algorithm 1 is a (1 + 20 \u000b)-approximation to the\noptimalk-means clustering cost with probability at least 1 \u00001=\u0011.\nTo analyze the time complexity of Algorithm 1, \frst consider the subroutine CrdEst . It takes\nO(kdn) time to \frst split each of the points in each cluster and dimension into two disjoint groups.\n18\n\nFinding the smallest interval that contains a certain number of points can be done by \frst sorting\nthe points and then iterating from the smallest point to the largest point and taking the smallest\ninterval that contains enough points. This requires O(nlogn) time for each dimension and each\ncenter, which results in O(kdnlogn) total time. Once each of the intervals is found, computing the\napproximate center then takes O(kdn) total time. Hence, the total running time of Algorithm 1 is\nO(kdnlogn).\nA.2 Proof of Auxiliary Lemmas\nLemma A.2. LetP;Q\u0012Rbe sets of points on the real line such that jPj \u0015 (1\u0000\u000b)nand\njQj\u0014\u000bn. LetX=P[Q,CPbe the mean of PandCXbe the mean of X. Then cost(X;CP)\u0014\u0010\n1 +\u000b\n1\u0000\u000b2\u0011\ncost(X;CX).\nProof. Suppose without loss of generality, that CX= 0 andCP\u00140, so thatCQ\u00150, whereCQis\nthe mean of Q. Then it is well-known, e.g., see Inaba et al. (1994), that\ncost(X;CP) = cost(X;CX) +jXj\u0001jCP\u0000CXj2:\nHence, it su\u000eces to show that jXj\u0001jCP\u0000CXj2\u0014\u000b\n(1\u0000\u000b)2cost(X;CX).\nSinceCX= 0 we havejPj\u0001CP=\u0000jQj\u0001CQ, withjPj\u0015(1\u0000\u000b)nandjQj\u0014\u000bn. LetjPj= (1\u0000%)n\nandjQj=%nfor some%\u0014\u000b. Thus,CQ=\u00001\u0000%\n%\u0001CP. By convexity, we thus have that\ncost(Q;CX)\u0015jQj\u0001(1\u0000%)2\n%2\u0001jCPj2\n=n(1\u0000%)2\n%\u0001jCPj2\n\u0015n(1\u0000\u000b)2\n\u000b\u0001jCPj2:\nTherefore, we have\njCP\u0000CXj2=jCPj2\u0014\u000b\nn(1\u0000\u000b)2cost(Q;CX)\u0014\u000b\nn(1\u0000\u000b)2cost(X;CX):\nThus,\njXj\u0001jCP\u0000CXj2\u0014\u000b\n(1\u0000\u000b)2cost(X;CX);\nas desired.\nLemma A.3. For a \fxed set X\u0012R, letCbe the mean of Xand\u001b2=1\n2jXjP\nx2X(x\u0000C)2be\nthe variance. Then the interval I\u0003=h\nC\u0000\u001bp\u000b;C+\u001bp\u000bi\ncontains at least a (1\u00004\u000b)fraction of the\npoints inX.\nProof. Note that any point x2XnI\u0003satis\fesjx\u0000Cj2> \u001b2=(4\u000b). Thus, if more than a 4 \u000b\nfraction of the points of Xare outside of I\u0003, then the total variance is larger than \u001b2, which is a\ncontradiction.\nFor ease of presentation, we analyze \u0015=1\n2and we note that the analysis extends easily to\ngeneral\u0015. We now prove the technical lemma that we will use in the proof of Lemma A.8.\n19\n\nLemma A.7. We havemX\nj=1\u0000m\nj\u0001\nj\u00012m= \u0002\u00121\nm\u0013\n:\nProof. Letmbe su\u000eciently large. A Cherno\u000b bound implies that for a su\u000eciently large constant\nC,\nX\njj\u0000m=2j\u0015Cpm\u0000m\nj\u0001\n2m\u00141\nm2:\nFurthermore,\nX\nj\u0015C0m\u0000m\nj\u0001\nj\u00012m=O\u00121\nm\u0013\n\u0001X\nj\u00151\u0000m\nj\u0001\n2m=O\u00121\nm\u0013\nso the upper bound on the desired relation holds. A similar analysis provides a lower bound.\nLemma A.4. Letmbe a su\u000eciently large consatnt. We have that I:= [a;b]contains at least a\n1\u00006\u000bfraction of points of X2andb\u0000a\u00142\u001b=p\u000b, with high probability, i.e., 1\u00001=poly(m).\nProof. By Lemma A.3, I\u0003contains at least 2 m(1\u00004\u000b) of the points in X. Hence, by applying an\nadditive Cherno\u000b bound for t=O(pmlogm) and for su\u000eciently large m, we have that the number\nof points in I\u0003\\X1is at leastm(1\u00005\u000b) with high probability. Since Iis the interval of minimal\nlength with at least m(1\u00005\u000b) points, then the length of Iis at most the length of I\u0003. Moreover,\nagain applying Cherno\u000b bounds, we have that the number of points in I\\X2is at leastm(1\u00006\u000b).\nMore formally, suppose we have a set of 2 mpoints that we randomly partition into two sets X1\nandX2. Consider any \fxed interval Jthat has at least 2 cmtotal points for c\u00151\u00005\u000b(note there\nare at most O(m2) intervals in total since our points are in one dimension). Let J1andJ2denote\nthe number of points in Jthat are in X1andX2respectively. By a Cherno\u000b bound, we have that\nbothJ1andJ2are at least mc(1\u0000\u000b) with high probability. In particular, jJ1\u0000J2j\u0014\u000bmc with\nhigh probability. Thus by using a union bound, all intervals with at least cmtotal points satisfy\nthe property that the number of points partitioned to X1and the number of points partitioned to\nX2di\u000ber by at most \u000bmc with high probability. Conditioning on this event, Imust also contain\nm(1\u00006\u000b) points in X2since it contains at least m(1\u00005\u000b) points in X1, as desired.\nLemma A.8. LetSbe a set of points obtained by independently sampling each point of X\u0012Rd\nwith probability1\n2, and letCSbe the centroid of S. Letxbe the centroid of X. Conditioned on\njSj\u00151, we have E[CS] =x, and there exists a constant \rsuch that\nE\u0002\nkCS\u0000xk2\n2\u0003\n\u0014\r\njXj2\u0001 X\nx2Xkx\u0000xk2\n2!\n:\nProof. We \frst prove that E[CS] =x. Note that by the law of iterated expectations,\nE[CS] =EjSjE[CSjjSj]:\nLetxi1;:::;xijSjbe a random permutation of the elements in S, so that for each 1 \u0014j\u0014jSj, we\nhaveE[xij] =x. Now conditioning on the size of S, we can write\nCS=xi1+\u0001\u0001\u0001+xijSj\njSj:\n20\n\nTherefore,\nE[CSjjSj] =x\u0001jSj\njSj=x\nand it follows that E[CS] =x.\nTo prove that\nE\u0002\nkCS\u0000xk2\u0003\n\u0014\r\njXj2\u0001 X\nx2Xkx\u0000xk2!\n;\nwe again condition on jSj. Suppose thatjSj=j. Then,\nCS\u0000x=(xi1\u0000x) +\u0001\u0001\u0001+ (xij\u0000x)\nj\nNow letyit=xit\u0000xfor all 1\u0014t\u0014j. Therefore,\nE\njSj=j\u0002\nkCS\u0000xk2\u0003\n=1\nj2\u0001E\u0002\nkyi1+\u0001\u0001\u0001+yijk2\u0003\n=1\nj\u0001E[kyi1k2] +j\u00001\nj\u0001E[yT\ni1yi2]:\nNote thatxi1is uniform over elements in X, so it follows that\nE[kyi1k2] =1\njXjX\nx2Xkx\u0000xk2:\nNow ifj\u00152, we have that\nE[yT\ni1yi2] =P\na<byT\nayb\u0000jXj\n2\u0001 =kP\niyik2\u0000P\nikyik2\njXj(jXj\u00001)\u00140\nsinceP\niyi= 0 by de\fnition. Hence,\nE\njSj\u00152\u0002\nkCS\u0000xk2\u0003\n\u00141\nj\u0001jXjX\nx2Xkx\u0000xk2:\nNow the probability that jSj=jforj\u00152 is precisely\u0000jXj\nj\u0001\n=2jXj, so we have\nPr[jSj\u00152]\u0001E\njSj\u00152\u0002\nkCS\u0000xk2\u0003\n\u00141\njXj\u0001 X\nx2Xkx\u0000xk2!\n\u0001jXjX\nj=1\u0000jXj\nj\u0001\nj\u00012jXj:\nFrom Lemma A.7, we have that\njXjX\nj=1\u0000jXj\nj\u0001\nj\u00012jXj\u0014c\njXj\n21\n\nfor some constant cso it follows that\nEkCS\u0000xk2\u0014c0\njXj2\u0001 X\nx2Xkx\u0000xk2!\nfor some constant c0.\nForj= 1, note that\nE\njSj=j=1\u0002\nkCS\u0000xk2\u0003\n=1\njXjX\nx2Xkx\u0000xk2:\nMoreover, we have Pr[jSj= 1] =jXj\n2jXjandPr[jSj= 0] =1\n2jXj. Thus from the law of total expecta-\ntion, we have\nE\u0002\nkCS\u0000xk2\u0003\n=Pr[jSj<2]\u0001E\njSj<2\u0002\nkCS\u0000xk2\u0003\n+Pr[jSj\u00152]\u0001E\njSj\u00152\u0002\nkCS\u0000xk2\u0003\n\u0014jXj\n2jXj\u00011\njXjX\nx2X\u0000\nkx\u0000xk2\u0001\n+c0\njXj2\u0001 X\nx2Xkx\u0000xk2!\n\u0014\r\njXj2\u0001 X\nx2Xkx\u0000xk2!\nfor some constant \r, as desired.\nLemma A.9. LetSbe a set of points obtained by independently sampling each point of X\u0012Rd\nwith probability p=1\n2. LetCbe the optimal center of XandCSbe the empirical center of S. Let\n\r\u00151be the constant from Lemma A.8. Then for \u0011\u00151andjXj>\u0011\rk\n\u000b,\nPr[cost(X;CS)>(1 +\u000b) cost(X;C)]<1=(\u0011k):\nProof. By Lemma A.8 and Markov's inequality, we have\nPr\"\nkCS\u0000Ck2\n2\u0015\u0011\rk\njXj2X\nx2Xx2#\n\u00141\n\u0011k:\nWe have X\nx2Xkx\u0000CSk2\n2=X\nx2Xkx\u0000Ck2\n2+jXj\u0001kC\u0000CSk2\n2;\nso that by Lemma A.8\nX\nx2Xkx\u0000CSk2\n2\u0014\u0012\n1 +\u0011\rk\njXj\u0013X\nx2Xkx\u0000Ck2\n2\n=\u0012\n1 +\u0011\rk\njXj\u0013\ncost(X;C);\nwith probability at least 1 \u00001\n\u0011k. Hence forjXj\u0015\u0011\rk\n\u000b, the approximate centroid of each cluster\ninduces a (1 + \u000b)-approximation to the cost of the corresponding cluster.\n22\n\nLemma A.5. LetSbe a set of points obtained by independently sampling each point of X\u0012Rd\nwith probability p=1\n2. LetCbe the optimal center of these points and CSbe the empirical center\nof these points. Conditioned on jSj\u00151, then E[CS] =xand there exists a constant \rsuch that for\n\u0011\u00151andjXj>\u0011\rk\n\u000b,\nE\u0002\nkCS\u0000xk2\n2\u0003\n\u0014\r\njXj2\u0001 X\nx2Xkx\u0000xk2\n2!\nPr[cost(X;CS)>(1 +\u000b) cost(X;C)]<1=(\u0011k):\nProof. Lemma A.5 follows immediately from Lemma A.8 and Lemma A.9.\nLemma A.6. Let\u000b2(10 logn=pn;1=7). LetP;Q\u0012Rbe sets of points on the real line such\nthatjPj\u0015(1\u0000\u000b)2mandjQj\u00142\u000bm, andX=P[Q. LetCbe the center of P. Then CrdEst\non input set Xoutputs a point C0such that with probability at least 1\u00001=(\u0011k),cost(P;C0)\u0014\n(1 + 18\u000b)(1 +\u000b)\u0010\n1 +\u000b\n(1\u0000\u000b)2\u0011\ncost(P;C).\nProof. Let\u000b2(10 logn=pn;1=7). Then from Lemma A.4, we have that I\\Xcontains at least\n(1\u00006\u000b)mpoints ofP\\X2and at most 2 \u000bmpoints ofQin an interval of length 2 \u001b=p\u000b, where\n\u001b2=1\n2jPjX\np2p(p\u0000C)2=1\n2jPj\u0001cost(P;C):\nFrom Lemma A.2, we have that\ncost(P;C 0)\u0014\u0012\n1 +\u000b\n(1\u0000\u000b)2\u0013\ncost(P;C 1);\nwhereC0is the center of I\\P\\X2andC1is the center of P\\X2.\nFor su\u000eciently large mand from Lemma A.9, we have that\ncost(P;C 1)\u0014(1 +\u000b) cost(P;C);\nwith probability at least 1 \u00001=(\u0011k). Thus, it remains to show that cost( P;C0)\u0014(1+O(\u000b)) cost(P;C 0).\nSinceC0is the center of I\\P\\X2andC0is the center of I\\X2, then we have\njI\\P\\X2jC0+X\nq2I\\Q\\X2q=jI\\X2jC0:\nSinceIhas length 2 \u001b=p\u000b, thenq2h\nC0\u00002\u001bp\u000b;C0+2\u001bp\u000bi\n. BecausejI\\P\\X2j\u0015(1\u00006\u000b)mand\njQj= 2\u000bm, then for su\u000eciently small \u000b, we have that\njC0\u0000C0j\u00146p\u000b\u001b:\nNote that we have cost( P;C0) = cost(P;C 0) +jPj\u0001jC0\u0000C0j2, so that\ncost(P;C0)\u0014cost(P;C 0) +jPj\u000136\u000b\u001b2:\n23\n\nFinally,\u001b2=1\n2jPj\u0001cost(P;C) and cost(P;C)\u0014cost(P;C 0) due to the optimality of C. This implies\ncost(P;C0)\u0014cost(P;C 0) +jPj\u000136\u000b\u001b2\n\u0014cost(P;C 0) +jPj\u000136\u000b\u00011\n2jPj\u0001cost(P;C)\n\u0014cost(P;C 0) + 18\u000bcost(P;C 0)\n= (1 + 18\u000b) cost(P;C 0);\nas desired. Thus putting things together, we have\ncost(P;C0)\u0014(1 + 18\u000b)(1 +\u000b)\u0012\n1 +\u000b\n(1\u0000\u000b)2\u0013\ncost(P;C):\nA.3 Proof of Theorem 3.4\nWe now give the proofs for optimal query complexity and runtime. We \frst require the following\nanalogue to Lemma A.5:\nLemma A.10. LetSbe a set of points obtained by independently sampling each point of X\u0012Rd\nwith probability p= min\u0010\n1;100 logk\n\u000bjSj\u0011\n. LetCbe the optimal center of these points and CSbe the\nempirical center of these points. Conditioned on jSj\u00151, then E[CS] =xand forjXj>\rk\n\u000b,\nE\u0002\nkCS\u0000xk2\n2\u0003\n\u0014\r\npjXj2\u0001 X\nx2Xkx\u0000xk2\n2!\nfor some constant \r.\nLemma A.11. For\u000b2(10 logn=pn;1=7), let \u0005be a predictor with error rate \u0015\u0014\u000b=2. If each\ncluster has at least \rklogk=\u000b points, then Algorithm 3 outputs a (1 + 20\u000b)-approximation to the\nk-means objective value with probability at least 3=4.\nProof. SinceSsamples each of points independently with probability proportional to cluster sizes\ngiven by \u0005, for a \fxed i2[k] at least90 logk\n\u000bpoints with label iare sampled, with probability at\nleast 1\u00001\nk4from Cherno\u000b bounds. Let \r1;:::;\rkbe the empirical means corresponding to each of\nthe sampled points with labels 1 ;:::;k , respectively, and let \u0000 0=f\r1;:::;\rkg. LetC1;:::;Ckbe\ncenters of a (1 + \u000b)-approximate optimal solution Cwith corresponding clusters X1;:::;Xk. By\nLemma A.10, we have that\nE\u0002\nkCi\u0000\rik2\n2\u0003\n\u0014\r\npjXij2\u00010\n@X\nx2Xikx\u0000Cik2\n21\nA;\nwherep= min\u0010\n1;100 logk\n\u000bjSj\u0011\n. By Markov's inequality, we have that\nX\ni2[k]kCi\u0000\rik2\n2\u0014100X\ni2[k]\r\npjXij2\u00010\n@X\nx2Xikx\u0000Cik2\n21\nA\n24\n\nwith probability at least 0 :99. Similar to the proof of Lemma A.9, we use the identity\nX\nx2Xikx\u0000\rik2\n2=X\nx2Xikx\u0000Cik2\n2+jXij\u0001kCi\u0000\rik2\n2:\nHence, we have that\ncost(X;\u00000)\u0014(1 +\u000b)\u0001cost(X;C);\nwith probability at least 0 :99.\nSuppose \u0005 has error rate \u0015\u0014\u000band each error chooses a label uniformly at random from the k\npossible labels. Then by de\fnition of error rate, at most \u000b=2 fraction of the points are erroneously\nlabeled for each cluster. Each cluster in the optimal k-means clustering of the predictor \u0005 has at\nleastn=(\u0010k) points, so that at least a (1 \u0000\u000b) fraction of the points in each cluster are correctly\nlabeled. Thus, by the same argument as in the proof of Lemma A.6, we have that Algorithm 1\noutputs a set of centers C1;:::;Cksuch that for \u0000 = fC1;:::;Ckg, we have\ncost(X;\u0000)\u0014(1 + 18\u000b)\u0012\n1\u0000\u000b\n(1\u0000\u000b)2\u0013\n\u0001cost(X;\u00000);\nwith su\u000eciently large probability.\nLetEbe the event that cost( X;\u0000)\u0014(1+\u000b)(1+18\u000b)\u0010\n1\u0000\u000b\n(1\u0000\u000b)2\u0011\n\u0001cost(X;C), so that Pr[E]\u0015\n1\u00001=poly(k). Conditioned on E, letX1be the subset of Xthat is assigned the correct label by\n\u0005, and letX2be the subset of Xassigned the incorrect label. For each point x2X1assigned the\ncorrect label `xby \u0005, the closest center to xin \u0000 isC`x, so Algorithm 3 will always label xwith\n`x. Thus,\ncost(X1;\u0000)\u0014cost(X;\u0000)\u0014(1 +\u000b)(1 + 18\u000b)\u0012\n1\u0000\u000b\n(1\u0000\u000b)2\u0013\n\u0001cost(X;C);\nconditioned onE. On the other hand, if x2X2is assigned an incorrect label `xby \u0005, then\nthe (2;r)-approximate nearest neighbor data assigns the label pxtox, where\u001e(Cpx) is the closest\ncenter to\u001e(x) in the projected space. Recall that \u001eis the composition map \u001e1\u000e\u001e2, where\u001e1has a\nterminal dimension reduction with distortion 5 =4, and\u001e2is a random JL linear map with distortion\n5=4. Thus the distance between xandCpxis a 2-approximation between xand its closest center Ci.\nHence, by assigning all points xto their respective centers Cpx, we haved(x;Cpx)\u00142 cost(x;\u0000).\nSince each point x2Xis assigned the incorrect label with probability \u0015\u0014\u000b=2, the expected cost\nof the labels assigned to X2is\u000bcost(X;\u0000). By Markov's inequality, the cost of the labels assigned\ntoX2is at most\n10\u000bcost(X;\u0000)<10\u000b(1 +\u000b) cost(X;C);\nwith probability at least 1 \u00001\n5, conditioned onE.\nTherefore by a union bound, the total cost is at most (1 + 20 \u000b)\u0001cost(X;C), with probability at\nleast 3=4.\nWe need the following theorems on the quality of the data structures utilized in Algorithm 3.\nTheorem A.12. Makarychev et al. (2019) For every set C\u001aRdof sizek, a parameter 0<\u000b<1\n2\nand the standard Euclidean norm d(\u0001;\u0001), there exists a terminal dimension reduction f:C!\nRd0with distortion (1 +\u000b), whered0=O\u0010\nlogk\n\u000b2\u0011\n. The dimension reduction can be computed in\npolynomial time.\n25\n\nTheorem A.13. Indyk & Motwani (1998); Har-Peled et al. (2012); Andoni et al. (2018) For \u000b>0,\nthere exists a (1+\u000b;r)-ANN data structure over Requipped with the standard Euclidean norm that\nachieves query time O\u0010\nd\u0001logn\n\u000b2\u0011\nand spaceS:=O\u00001\n\u000b2log1\n\u000b+d(n+q)\u0001\n, whereq:=logn\n\u000b2. The\nruntime of building the data structure is O(S+ndq).\nWe now prove Theorem 3.4.\nTheorem 3.4. Let\u000b2(10 logn=pn;1=7),\u0005be a predictor with label error rate \u0015\u0014\u000b, and\r\u00151\nbe a su\u000eciently large constant. If each cluster in the optimal k-means clustering of the predictor\nhas at least \rklogk=\u000b points, then Algorithm 3 outputs a (1 + 20\u000b)-approximation to the k-means\nobjective with probability at least 3=4, usingO(ndlogn+ poly(k;logn))total time.\nProof. The approximation guarantee of the algorithm follows from Lemma A.11. To analyze the\nrunning time, we \frst note that we apply a JL matrix with dimension O(logn) to each of the\nnpoints in Rd, which uses O(ndlogn) time. As a result of the JL embedding, each of the n\npoints has dimension O(logn). Thus, by Theorem A.12, constructing the terminal embedding\nuses poly(k;logn) time. As a result of the terminal embedding, each of the kpossible centers\nhas dimension O(logk). Hence, by Theorem A.13, constructing the (2 ;r)-ANN data structure for\nthekpossible centers uses O(klog2k) time. Subsequently, each query to the data structure uses\nO(log2k) time. Therefore, the overall runtime is O(ndlogn+ poly(k;logn)).\nA.4 Remark on Truly-polynomial time algorithms vs. PTAS/PRAS.\nRemark A.14. We emphasize that the runtime of our algorithm in Theorem 2.1 is truly polyno-\nmial in all input parameters n;d;k and 1=\u000b(and even near-linear in the input size nd). Although\nthere exist polynomial-time randomized approximation schemes for k-means clustering, e.g., In-\naba et al. (1994); Feldman et al. (2007); Kumar et al. (2004), their runtimes all have exponential\ndependency on kand 1=\u000b, i.e., 2poly(k;1=\u000b). However, this does not su\u000ece for many applications,\nsincekand 1=\u000bshould be treated as input parameters rather than constants. For example, it\nis undesirable to pay an exponential amount of time to linearly improve the accuracy \u000bof the\nalgorithm. Similarly, if the number of desired clusters k=O(log2n), then the runtime would be\nexponential. Thus we believe the exponential improvement of Theorem 2.1 over existing PRAS in\nterms ofkand 1=\u000bis signi\fcant.\nA.5 Remark on Possible Instantiations of Predictor\nRemark A.15. We can instantiate Theorem 2.1 with various versions of the predictor. Assume\neach cluster in the (1 + \u000b)-approximately optimal k-means clustering of the predictor has size at\nleastn=(\u0010k) for some tradeo\u000b parameter \u00102[1;(pn)=(8klogn)]. Then the clustering quality and\nruntime guarantees of Theorem 2.1 hold if the predictor \u0005 is such that\n1. \u0005 outputs the right label for each point independently with probability 1 \u0000\u0015and otherwise\noutputs a random label for \u0015\u0014O(\u000b=\u0010),\n2. \u0005 outputs the right label for each point independently with probability 1 \u0000\u0015and otherwise\noutputs an adversarial label for\u0015\u0014O(\u000b=(k\u0010)).\nIn addition, if the predictor \u0005 outputs a failure symbol when it fails, then for constant \u0010 > 0,\nthere exists an algorithm (see supplementary material) that outputs a (1+ \u000b)-approximation to the\n26\n\nk-means objective with probability at least 2 =3, even when \u0005 has failure rate \u0015= 1\u00001=poly(k).\nNote that this remark (but not Theorem 2.1) assumes that each of the kclusters in the (1 + \u000b)-\napproximately optimal clustering has at leastn\n\u0010kpoints. This is a natural assumption that the\nclusters are \\roughly balanced\" which often holds in practice, e.g., for Zip\fan distributions.\nB Deletion Predictor\nIn this section, we present a fast and simple algorithm for k-means clustering, given access to a\nlabel predictor \u0005 with deletion rate \u0015. That is, for each point, the predictor \u0005 either outputs a\nlabel for the point consistent with an optimal k-means clustering algorithm with probability \u0015, or\noutputs nothing at all (or a failure symbol ?) with probability 1 \u0000\u0015. Since the deletion predictor\nfails explicitly, we can actually achieve a (1 + \u000b)-approximation even when \u0015= 1\u00001\npoly(k).\nOur algorithm \frst queries all points in the input X. Although the predictor does not output\nthe label for each point, for each cluster Ciwith a su\u000eciently large number of points, with high\nprobability, the predictor assigns at least\u0015\n2jCijpoints ofCito the correct label. We show that if\njCij= \n\u0000k\n\u000b\u0001\n, then with high probability, the empirical center is a good estimator for the true center.\nThat is, the k-means objective using the centroid of the points labeled iis a (1 +\u000b)-approximation\nto thek-means objective using the true center of Ci. We give the full details in Algorithm 4.\nTo show that the empirical center is a good estimator for the true center, recall that a common\napproach for mean estimation is to sample roughly an O\u00001\n\u000b2\u0001\nnumber of points uniformly at random\nwith replacement. The argument follows from observing that each sample is an unbiased estimator\nof the true mean, and repeating O\u00001\n\u000b2\u0001\ntimes su\u000eciently upper bounds the variance.\nObserve that the predictor can be viewed as sampling the points from each cluster without\nreplacement . Thus, for su\u000eciently large cluster sizes, we actually have a huge number of samples,\nwhich intuitively should su\u000eciently upper bound the variance. Moreover, the empirical mean is\nagain an unbiased estimator of the true mean. Thus, although the above analysis does not quite\nhold due to dependencies between the number of samples and the resulting averaging term, we\nshow that the above intuition does hold.\nAlgorithm 4 Linear time k-means algorithm with access to a label predictor \u0005 with deletion rate\n\u0015.\nInput: A point set x2Xwith labels given by a label predictor \u0005 with deletion rate \u0015.\nOutput: A (1 +\u000b)-approximate k-means clustering of X.\n1:foreach labeli2[k]do\n2: LetSibe the set of points labeled i.\n3:ci 1\njSij\u0001P\nx2Six\n4:end for\n5:forall pointsx2Xdo\n6:ifxis unlabeled then\n7:`x argmind(x;ci)\n8: Assign label `xtox.\n9:end if\n10:end for\nWe \frst show that independently sampling points uniformly at random from a su\u000eciently large\n27\n\npoint set guarantees a (1 + \u000b)-approximation to the objective cost. Inaba et al. (1994); Ailon et al.\n(2018) proved a similar statement for sampling with replacement.\nIt remains to justify the correctness of Algorithm 4 by arguing that with high probability, the\noverallk-means cost is preserved up to a (1 + \u000b)-factor by the empirical means. We also analyze\nthe running time of Algorithm 4.\nTheorem B.1. If each cluster in the optimal k-means clustering of the predictor \u0005has at least3k\n\u000b\npoints, then Algorithm 4 outputs a (1 +\u000b)-approximation to the k-means objective with probability\nat least2\n3, usingO(kdn)total time.\nProof. We \frst justify the correctness of Algorithm 4. Suppose each cluster in the optimal k-means\nclustering of the predictor \u0005 has at least3k\n\u000bpoints. LetC=fc1;:::;ckgbe the optimal centers\nselected by \u0005 and let CS=fc0\n1;:::;c0\nkgbe the empirical centers chosen by Algorithm 4. For each\ni2[k], letCibe the points of Xthat are assigned to Ciby the predictor \u0005. By Lemma A.9 with\n\u0011= 3, the approximate centroid of a cluster induces a (1 + \u000b)-approximation to the cost of the\ncorresponding cluster so that\ncost(Ci;c0\ni)\u0014(1 +\u000b) cost(Ci;ci);\nwith probability at least 1 \u00001\n3k. Taking a union bound over all kclusters, we have that\nX\ni2[k]cost(Ci;c0\ni)\u0014X\ni2[k](1 +\u000b) cost(Ci;ci);\nwith probability at least2\n3. Equivalently, cost( X;C)\u0014(1 +\u000b) cost(X;CS).\nTo analyze the running time of Algorithm 4, observe that the estimated centroids for all labels\ncan be computed in O(dn) time. Subsequently, assigning each unlabeled point to the closest\nestimated centroid uses O(kd) time for each unlabeled point. Thus, the total running time is\nO(kdn).\nC k-median Clustering\nWe \frst recall that a well-known result states that the geometric median that results from uniformly\nsampling a number of points from the input is a \\good\" approximation to the actual geometric\nmedian for the 1-median problem.\nTheorem C.1. Krauthgamer (2019) Given a set Pofnpoints in Rd, the geometric median of\na sample of O\u0000d\n\u000b2logd\n\u000b\u0001\npoints ofPprovides a (1 +\u000b)-approximation to the 1-median clustering\nproblem with probability at least 1\u00001=poly(d).\nNote that we can \frst apply Theorem A.12 to project all points to a space with dimension\nO\u00001\n\u000b2logk\n\u000b\u0001\nbefore applying Theorem C.1. Instead of computing the geometric median, we recall\nthe following procedure that produces a (1 + \u000b)-approximation to the geometric median.\nTheorem C.2. Cohen et al. (2016) There exists an algorithm that outputs a (1+\u000b)-approximation\nto the geometric median in O\u0000\nndlog3n\n\u000b\u0001\ntime.\nWe give our algorithm in full in Algorithm 5.\n28\n\nAlgorithm 5 Learning-Augmented k-median Clustering\nInput: A point set x2Xwith labels given by a predictor \u0005 with error rate \u0015.\nOutput: A (1 +\u000b)-approximate k-median clustering of X.\n1:Use a terminal embedding to project all points into a space with dimension O\u00001\n\u000b2logk\n\u000b\u0001\n.\n2:fori= 1 toi=kdo\n3: Let`ibe the most common remaining label.\n4: SampleO\u00001\n\u000b4log2k\n\u000b\u0001\npoints with label `i.\n5: LetC0\nibe a\u0000\n1 +\u000b\n4\u0001\n-approximation to the geometric median of the sampled points.\n6:end for\n7:ReturnC0\n1;:::;C0\nk.\nTheorem C.3. For\u000b2(0;1), let\u0005be a predictor with error rate \u0015=O\u0012\n\u000b4\nklogk\n\u000bloglogk\n\u000b\u0013\n. If each\ncluster in the optimal k-median clustering of the predictor has at least n=(\u0010k)points, then there\nexists an algorithm that outputs a (1 +\u000b)-approximation to the k-median objective with probability\nat least 1\u00001=poly(k), usingO(ndlog3n+ poly(k;logn))total time.\nProof. Observe that Algorithm 5 samples O\u00001\n\u000b4log2k\n\u000b\u0001\npoints for each of the clusters labeled i,\nwithi2[k]. Thus Algorithm 5 samples O\u0000k\n\u000b4log2k\n\u000b\u0001\npoints in total. For \u0015=O\u0012\n\u000b4\nklogk\n\u000bloglogk\n\u000b\u0013\nwith a su\u000eciently small constant, the expected number of incorrectly labeled points sampled by\nAlgorithm 5 is less than1\n32. Thus, by Markov's inequality, the probability that no incorrectly labeled\npoints are sampled by Algorithm 5 is at least3\n4. Conditioned on the event that no incorrectly labeled\npoints are sampled by Algorithm 5, then by Theorem C.1, the empirical geometric median for each\ncluster induces a\u0000\n1 +\u000b\n4\u0001\n-approximation to the optimal geometric median in the projected space.\nHence the set of kempirical geometric medians induces a\u0000\n1 +\u000b\n4\u0001\n-approximation to the optimal\nk-median clustering cost in the projected space. Since the projected space is the result of a terminal\nembedding, the set of kempirical geometric medians for the sampled points in the projected space\ninduces ak-median clustering cost that is a\u0000\n1 +\u000b\n4\u0001\n-approximation to the k-median clustering cost\ninduced by the set of kempirical geometric medians for the sampled points in the original space.\nTaking the set of kempirical geometric medians for the sampled points in the original space induces\na\u0000\n1 +\u000b\n4\u00012-approximation to the k-median clustering cost. We take a\u0000\n1 +\u000b\n4\u0001\n-approximation to\neach of the geometric medians. Thus for su\u000eciently small \u000b, Algorithm 5 outputs a (1 + \u000b)-\napproximation to the k-median clustering problem.\nTo embed the points into the space of dimension O\u00001\n\u000b2logk\n\u000b\u0001\n, Algorithm 5 spends O(ndlogn)\ntotal time. By Theorem C.2, it takes O(ndlog3n) total time to compute the approximate geometric\nmedians.\nD Lower Bounds\nMAX-E3-LIN-2 is the optimization problem of maximizing the number of equations satis\fed by\na system of linear equations of Z2with exactly 3 distinct variables in each equation. E K-MAX-\nE3-LIN-2 is the problem of MAX-E3-LIN-2 when each variable appears in exactly kequations.\nFotakis et al. (2016) showed that assuming the exponential time hypothesis (ETH) (Impagliazzo &\nPaturi, 2001), there exists an absolute constant C1such that MAX k-SAT (and thus MAX k-CSP)\n29\n\ninstances with fewer than O(nk\u00001) clauses cannot be approximated within a factor of C1in time\n2O(n1\u0000\u000e)for any\u000e >0. As a consequence, the reduction by H\u0017 astad (2001) shows that there exist\nabsolute constants C2;C3such that E K-MAX-E3-LIN-2 with k\u0015C2cannot be approximated\nwithin a factor of C3in time 2O(n1\u0000\u000e)for any\u000e>0. Hence, the reduction by Chleb\u0013 \u0010k & Chleb\u0013 \u0010kov\u0013 a\n(2006) shows that there exists a constant C4such that approximating the minimum vertex cover\nof 4-regular graphs within a factor of C4cannot be done in time 2O(n1\u0000\u000e)for any\u000e >0. Thus the\nreduction by Lee et al. (2017) shows that there exists a constant C5such that approximating k-\nmeans within a factor of C5cannot be done in time 2O(n1\u0000\u000e)for any\u000e>0, assuming ETH. Namely,\nthe reduction of Lee et al. (2017) shows that an algorithm that provides a C5-approximation to the\noptimalk-means clustering can be used to compute a C4-approximation to the minimum vertex\ncover.\nTheorem D.1. If ETH is true, then there does not exist an algorithm Athat takes a set Sofn1\u0000\u000e\nlogn\nvertices and \fnds a C4-approximation to the minimum vertex cover that contains Son a 4-regular\ngraphG, using 2O(n1\u0000\u000e)time for some constant \u000e2(0;1].\nProof. Suppose by way of contradiction that there exists an algorithm Athat takes a set Sofn1\u0000\u000e\nlogn\nvertices and \fnds a C4-approximation to the minimum vertex cover that contains Son a 4-regular\ngraphG, using 2O(n1\u0000\u000e)time for some constant \u000e2(0;1]. We claim that we can use Ato create\nan overall algorithm that violates ETH. Indeed, suppose we guess each subset ofn1\u0000\u000e\nlognvertices and\nwhich vertices of the subset are in the cover. There are\u0000n\nn1\u0000\u000e=logn\u0001\n\u00012n1\u0000\u000e=logn\u0014(en\u000elogn)n1\u0000\u000e=logn\u0001\n2n=lognsuch combinations of vertices. For each guess, we then run the purported algorithm Athat\nuses 2O(n1\u0000\u000e)time. Thus we can identify a C4-approximation to the minimum vertex cover in time\n(en\u000elogn)n1\u0000\u000e=logn\u00012n=logn\u00012O(n1\u0000\u000e)= 2O(n1\u0000\u000e)\u00012O(n1\u0000\u000e)= 2O(n1\u0000\u000e);\nwhich would contradict ETH.\nFinally, we show the query complexity of Algorithm 3 is nearly optimal. Lee et al. (2017)\nconstructed an instance of k-means that cannot be approximated within a factor of 1 :0013 in poly-\nnomial time. The reduction of Lee et al. (2017) creates 4 npoints in R3nthat must be clustered by\nO(n) centers and an algorithm that provides a C5-approximation to the optimal k-means clustering\ncan be used to compute a C4-approximation to the minimum vertex cover. Thus, there exists a\nconstantC5such that approximating k-means within a factor of C5cannot be done in time 2O(n1\u0000\u000e)\nfor any\u000e>0, assuming ETH.\nTheorem 3.5. For any\u000e2(0;1], any algorithm that makes O\u0010\nk1\u0000\u000e\n\u000blogn\u0011\nqueries to the predictor\nwith label error rate \u000bcannot output a (1 +C\u000b)-approximation to the optimal k-means clustering\ncost in time 2O(n1\u0000\u000e)time, assuming the Exponential Time Hypothesis.\nProof. Let\u000bbe a \fxed constant such that \u000b < C 5. Given an instance Iof ak-means clustering\nconstructed from the reduction of Lee et al. (2017), the optimal clustering cost is \n( n) andk1=\n\n(n). We embed this instance into a k-means clustering by adding an additional \n( n) points\narbitrarily far from I, so that the additional points contribute \n( n=\u000b) cost upon partitioning into\nk2= \n(n) clusters. We set k=k1+k2.\nIn summary, the resulting instance has O(n) points and k= \n(n). The optimal solution has\ncostO(n=\u000b) cost so thatIcontributes an \n( \u000b) fraction of the cost. By querying O\u0010\nk1\u0000\u000e\n\u000blogn\u0011\npoints\n30\n\nwith su\u000eciently small constant, at most O\u0010\nk1\u0000\u000e\n\u000blogn\u0011\nof the cluster centers in Iwill be revealed by the\nconstruction ofI. Each center corresponds to a selected vertex in the corresponding vertex cover in\nthe reduction from minimum vertex cover on 4-regular graphs. Hence, in the corresponding vertex\ncover instance, at most O\u0010\nk1\u0000\u000e\n\u000blogn\u0011\nvertices are revealed. Thus by Theorem D.1, any algorithm\nrunning in 2O(n1\u0000\u000e)time cannot determine a C5-approximation to the optimal k-means clustering\ncost onI, as it would correspond to a C4-approximation to the optimal vertex cover, assuming\nETH. SinceIinduces an \n( \u000b) fraction of the total clustering cost, it follows that any algorithm\nthat makes O\u0010\nk1\u0000\u000e\n\u000blogn\u0011\nqueries cannot output a (1 + C\u000b)-approximation to the optimal k-means\nclustering cost in time 2O(n1\u0000\u000e)time, assuming ETH.\nE Additional Experimental Results\nE.1 Omitted Discussion\nIn this section we continue the discussion from the experimental section of the main body. In\nFigure 3(a), we show the qualitatively similar version of Figure 2( c) of the main body for the case\nofk= 25. In Figure 3(b), we display the qualitatively similar version of Figure 3( a) of the main\nbody for the k= 25 case of dataset PHY.\nPHY. We use the noisy predictor for this dataset. We see in Figure 2(a) that as the corruption\npercentage rises, the clustering given by just the predictor labels can have increasingly large cost.\nNevertheless, even if the clustering cost of the corrupted labels is rising, the cost decreases signi\f-\ncantly after applying Algorithm 1 by roughly a factor of 3x. Indeed, we see that our algorithm can\nbeat the kmeans++ seeding baseline for qas high as 50%. Just as in Figure 1(c), random sampling\nis sensitive to noise. Lastly, we also remain competitive with the purple line which uses the labels\noutput by kmeans++ as the predictor in our algorithm (no corruptions added). The qualitatively\nsimilar plot for k= 25 is given in the supplementary material.\nCIFAR-10. The cost of clustering on CIFAR-10 using only the predictor, the predictor with our\nalgorithm, random sampling, and kmeans++ as the predictor for our algorithm were 0 :733, 0:697,\n0:721, and 0:640, respectively, where 1 :0 represents the cost of kmeans++ . The neural network was\nvery accurate (\u001893%) in predicting the class of the input points which is highly correlated with the\noptimalk-means clusters. Nevertheless, our algorithm improved upon this highly precise predictor.\nNote that using kmeans++ as the predictor for our algorithm resulted in a clustering that was\n13%better than the one given by the neural network predictor. This highlights the fact that an\napproximate clustering combined with our algorithm can be competitive against a highly precise\npredictor, such as a neural network, even though creating the highly accurate predictor can be\nexpensive. Indeed, obtaining a neural network predictor requires prior training data and also the\ntime to train the network. On the other hand, using a heuristic clustering as a predictor is extremely\n\rexible and can be applied to any dataset even if no prior training dataset is available (50 ;000 test\nimages were required to train the neural network predictor but kmeans++ as a predictor requires\nno test images), in addition to considerable savings in computation.\nFor example, the time taken to train the particular neural network we used was approximately\n18 minutes using the optimized PyTorch library (see training details under the \\Details Report\"\n31\n\nsection in Huy (2020)). In general, the time can be much longer for more complicated datasets.\nOn the other hand, our unoptimized algorithm implementation which used the labels of a sample\nrun of kmeans++ was still able to achieve a better clustering than the neural network predictor\nwith\u000b= 0:01 in Algorithm 2 in 4 :4 seconds. In conclusion, we can achieve a better clustering by\ncombining a much weaker predictor with our algorithm with the additional bene\ft of using a more\n\rexible and computationally inexpensive methodology.\nWe also conducted an experiment where we only use a small fraction of the predictor labels\nin our algorithm. We select a pfraction of the images, query their labels, and run our algorithm\non only these points. We then report the cost of clustering on the entire dataset as pranges from\n1% to 100%. Figure 2(b) shows the percentage increase in clustering cost relative to querying the\nwhole dataset is quite low for moderate values of pbut increasingly worse as pbecomes smaller.\nThis suggests that the quality of our algorithm does not su\u000ber drastically by querying a smaller\nfraction of the dataset.\n0 5 10 15 20 25\nCorruption %020406080Clustering Cost\nDataset: Oregon Spectral Clustering, Graph #5, k=25\nAlg + Noisy Predictor\nRandom Sampling\nkmeans++\nPredictor\n(a) Oregon Graph #5, k= 25\n0 10 20 30 40 50\nCorruption %0.00.20.40.60.81.01.21.4Clustering Cost\nDataset: PHY, k=25\nAlg + Noisy Predictor\nAlg + k-means++\nRandom Sampling\nNoisy Predictor\nkmeans++ (b) PHY, k= 25\nFigure 3: Our algorithm is able to recover a good clustering even for very high levels of noise.\nE.2 Synthetic Dataset\nWe use a dataset of 10010 points in Rdford= 103created using the kmeans++ lower bound\nconstruction presented in Arthur & Vassilvitskii (2007). The dataset consists of 10 well separated\nclusters in Rd: leteidenote the basis vectors. Our dataset is f1000eig[f 1000ei+ejgfor all\n1\u0014i\u001410;1\u0014j\u00141000.\nFrom the description of the dataset, we can explicitly calculate the optimal clustering and\nits cost. Our predictor for this dataset was to take the optimal clustering and randomly change\neach label with probability 1 =2 to another uniformly random label. Empirically, kmeans++ seeding\nreturned a clustering that had cost at least 1.9x the optimal clustering. Furthermore, using just the\npredictor labels na \u0010vely resulted in a clustering with cost up to \fve orders of magnitude larger than\nthe optimal clustering. In contrast, our algorithm was able to precisely recover the true clustering\nafter processing the predictor outputs. In addition, applying our algorithm using the labels of a\nsample run of kmeans++ was also able to precisely recover the true clustering.\n32\n\nE.3 Comparison to Lloyd's Heuristic\nWe give both theoretical and empiricial justi\fcations for why our algorithms could be superior to\nblindly following a predictor and then running Lloyd's heuristic.\n(a)\n (b)\n (c)\nFigure 4: Additional experimental results for comparison to Lloyd's heuristic.\nEmpirical Comparison. We \frst compared Lloyd's algorithm on kmeans++ seeding to our algo-\nrithm with the predictor on the PHY dataset. The predictor is a noisy predictor that has corruption\nlevel 50% as described in Section 4 so that outputting the clustering from the predictor alone has\ncost 1.9x the average kmeans++ cost. Hence, it is clear that the predictor is much worse than\nkmeans++ , yet our algorithm using the predictor (horizontal line in Figure 4(a)) is much better\nthan kmeans++ and Lloyd's algorithm (orange line in Figure 4(a)).\nNext, we took the noisy predictor for the PHY dataset with corruption level 50% and repeatedly\napplied Lloyd's algorithm. We observe that even with \u00185 Lloyd's iterations (Figure 4(b)), the\nclustering cost does not seem to improve upon kmeans++ , much less the clustering output by simply\napplying our algorithm to the noisy predictions (Figure 4(a)).\nFinally, we compared Lloyd's algorithm on kmeans++ seeding to Lloyd's algorithm on the seeding\noutput by our algorithm (with kmeans++ as the predictor) on the CIFAR-10 dataset, similar to\nthe experiments you suggested by Lattanzi & Sohler (2019). Lloyd's algorithm on the seeding\noutput by our algorithm exhibits superior performance than Lloyd's algorithm on kmeans++ (Figure\n4(c)), which is consistent with our previous experiments showing that our algorithm improves upon\nkmeans++ . This further strengthens our claim that our algorithm and methodology with provable\nworst-case guarantees can be applied in conjunction with heuristics such as Lloyd's that do not\nhave provable worst-case guarantees. Moreover, Figure 4(c) indicates that our approach may be\nmore advantageous than just running kmeans++ with heuristics. Note that a na \u0010ve implementation\nof Lloyd's algorithm is O(ndk) time while our algorithm can be implemented in nearly linear time.\nWe emphasize that all of our above experiments use a noisy predictor with corruption level\n50% as input to our algorithm. Our experiments exhibit even better behavior when a clustering\nproduced by kmeans++ is used for a predictor as input to our algorithm. Combined with our other\nexperiments in Section 4, this gives empirical evidence that there exist many scenarios in which\nrunning our algorithm with an erroneous predictor is advantageous.\nTheoretical Comparison. We now provide an example that demonstrates why blindly following\nthe predictor and then running Lloyd's heuristic would run into issues. We emphasize that it is well-\nknown e.g., see Dasgupta (2003); Har-Peled & Sadri (2005); Arthur & Vassilvitskii (2006); Vattani\n33\n\n(2011), that Lloyd's algorithm can take a large number of steps to converge. In particular, Vattani\n(2011) shows that an exponential number of Lloyd iterations can be required for the algorithm to\nconverge to the optimal solution. Nevertheless, we o\u000ber the following concrete answer:\nWe describe a simple set of points that guarantees Lloyd's algorithm will fail. This is based\non the example given in Har-Peled & Sadri (2005) and is also conceptually similar to the example\ngiven in 1.1. Consider 4 npoints on the real line x1;:::;x 2n;y1;:::;y 2nso thaty1\u0014:::\u0014y2n\u0014\nA < B\u0014x1\u0014:::\u0014x2nwhereB\u0000Ais large. Suppose k= 2 in which case the optimal\nclustering groups all the yipoints together and all the xipoints together as two separate clusters.\nSuppose the predictor initially gives label \\1\" to points y1;:::;ynand gives label \\2\" to points\nyn+1;:::;x 1;:::;x 2n, so that the predictor has corruption level \u0015= 1=2. Then our algorithm\nthat uses this predictor will get a constant-factor approximation in only one iteration. However,\nsinceB\u0000Acan be arbitrarily large without a\u000becting the optimal clustering cost, blindly listening\nto the predictor will give a worse clustering. Furthermore, Theorem 2 :1 in Har-Peled & Sadri\n(2005) implies that Lloyd's will take \u0002( n) iterations to converge if initialized using this predictor.\nNote that even a single (na \u0010ve) iteration of Lloyd's algorithm already uses O(ndk) time while our\nalgorithm only uses O(nd) + poly(k;1=\u000b) time. Note that there are also more complex examples in\nhigher-dimensional spaces in the literature which provably have even worse convergence rates for\nLloyd's method.\nE.4 Conclusion\nAlthough 1:07-approximation for k-means clustering in polynomial time is NP-hard and a clustering\nconsistent with the labels of any predictor with nonzero error can be arbitrarily bad, we give a\n(1 +\u000b)-approximation algorithm that uses the labels output by the predictor as \\advice\" and runs\nin nearly linear time. We use a linear number of queries to the predictor, which can be improved to\nnearly optimal under natural assumptions about the cluster sizes. Our results are well-supported by\nempirical evaluations and are an important step in demonstrating the power of learning-augmented\nalgorithms beyond the limitations of classical algorithms for clustering-based applications.\nF Learnability Results\nIn this section, we present formal learning bounds for learning a good predictor for the learning-\naugmented k-means problem. Namely, we show that a good predictor can be learned e\u000eciently if\nthe problem instances are drawn from a particular distribution. Our result is inspired by derived\nfrom data-driven algorithm design and utilizes the PAC learning framework. Formally, our setting\nis the following.\nSuppose there exists an underlying distribution Dthat generates independent k-means cluster-\ning instances, representing the case where similar instances of the k-means clustering problem are\nbeing solved. Note that this setting also mirrors some of our experiments in Section 4, speci\fcally\nin the case of our spectral clustering datasets which are derived from snapshots of a dynamic graph\nacross time.\nOur goal is to e\u000eciently learn a good predictor famong some family of functions F. The input\nto each predictor fis a clustering instance Cand the output is a feature vector. We assume that\nthe each input instance Cis encoded as a vector in Rnd, that is, we think of ndas the size of the\ndescription of the input. We also assume that the output of fis inndimensions, which represents\n34\n\nthe prediction of the oracle. For example in the case of k-means clustering, the output can be an\ninteger in [k] for each point in C.\nTo select the \\best\" f, we need to formally de\fne what we mean by best. In most learning\nsettings, a loss function is used to measure the quality of a solution. Indeed, suppose we have\na loss function L:f\u0002C!Rwhich represents how well a predictor fperforms on some input\nC. For example, fcan represent the cluster labels of a points in CandLoutputs the cost of\nthe clustering. Alternatively, Lcan represent an algorithm which uses the hints given by fand\nperforms a clustering algorithm such as some number of steps of Lloyd's heuristic.\nOur goal is to learn the best function f2F that minimizes the following objective:\nEC\u0018D[L(f;C)]: (2)\nWe de\fnef\u0003to be an optimal function f2F, so thatf\u0003= argmin EC\u0018D[L(f;C)]. We also assume\nthat for each clustering instance Cand eachf2F,f(C) andL(f;C) can be computed in time\nT(n;d) that should be interpreted as a (small) polynomial in nandd.\nOur main result is the following.\nTheorem F.1. There exists an algorithm that uses poly(T(n;d);1=\")samples and returns a ^fthat\nsatis\fes\nEC\u0018D[L(^f;C)]\u0014EC\u0018D[L(f\u0003;C)] +\"\nwith probability at least 9=10.\nThe above theorem is a PAC-style bound that shows only a small number of samples are needed\nin order to have a good probability of learning an approximately-optimal function ^f. The algorithm\nto compute ^fis the following: we simply minimize the empirical loss after an appropriate number of\nsamples are drawn, i.e., we perform empirical risk minimization. This result is proven by Theorem\nF.3. Before introducing it, we need to de\fne the concept of pseudo-dimension for a function class,\nwhich is the more familiar VC dimension, generalized to real functions.\nDe\fnition F.2 (Pseudo-Dimension, e.g., De\fnition 9 in Lucic et al. (2018)) .LetXbe a ground\nset andFbe a set of functions from Xto the interval [0;1]. Fix a set S=fx1;\u0001\u0001\u0001;xng\u001aX , a set\nof reals numbers R=fr1;\u0001\u0001\u0001;rngwithri2[0;1]and a function f2F. The setSf=fxi2Sj\nf(xi)\u0015rigis called the induced subset of Sformed byfandR. The setSwith associated values\nRis shattered byFifjfSfjf2Fgj = 2n. The pseudo-dimension ofFis the cardinality of the\nlargest shattered subset of X(or1).\nThe following theorem relates the performance of empirical risk minimization and the number\nof samples needed, to pseudo-dimension. We specialize the theorem statement to our situation at\nhand. For notational simplicity, we de\fne Gbe the class of functions in Fcomposed with L:\nG:=fL\u000ef:f2Fg:\nFurthermore, by normalizing, we can assume that the range of Lis equal to [0 ;1].\nTheorem F.3 (Anthony & Bartlett (1999)) .LetDbe a distribution over problem instances C\nandGbe a class of functions g:C! [0;1]with pseudo-dimension dG. Consider ti.i.d. samples\n35\n\nC1;C2;:::;CtfromD. There is a universal constant c0, such that for any \">0, ift\u0015c0\u0001dG=\"2;\nthen we have \f\f\f\f\f1\nttX\ni=1g(Ci)\u0000EC\u0018Dg(C)\f\f\f\f\f\u0014\"\nfor allg2G with probability at least 9=10:\nThe following corollary follows from the triangle inequality.\nCorollary F.4. Consider a set of tindependent samples C1;:::;CtfromDand let ^gbe a function\ninGthat minimizes1\ntPt\ni=1g(Ci). If the number of samples tis chosen as in Theorem F.3, then\nEC\u0018D[^g(C)]\u0014EC\u0018D[g\u0003(C)] + 2\"\nholds with probability at least 9=10.\nTherefore, the main challenge at hand is to bound the pseudo-dimension of our given function\nclassG. To do so, we \frst relate the pseudo-dimension to the VC dimension of a related class of\nthreshold functions. This relationship has been fruitful in obtaining learning bounds in a variety\nof works such as Lucic et al. (2018); Izzo et al. (2021).\nLemma F.5 (Pseudo-dimension to VC dimension, Lemma 10 in Lucic et al. (2018)) .For any\ng2G, letBgbe the indicator function of the region on or below the graph of g, i.e.,Bg(x;y) =\nsgn(g(x)\u0000y). The pseudo-dimension of Gis equivalent to the VC-dimension of the subgraph class\nBG=fBgjg2Gg .\nFinally, the following theorem relates the VC dimension of a given function class to its compu-\ntational complexity, i.e., the time complexity of computing a function in the class.\nLemma F.6 (Theorem 8 :14 in Anthony & Bartlett (1999)) .Leth:Ra\u0002Rb!f0;1g, determining\nthe class\nH=fx!h(\u0012;x) :\u00122Rag:\nSuppose that any hcan be computed by an algorithm that takes as input the pair (\u0012;x)2Ra\u0002Rb\nand returns h(\u0012;x)after no more than tof the following operations:\n•arithmetic operations +;\u0000;\u0002;and=on real numbers,\n•jumps conditioned on >;\u0015;<;\u0014;=;and=comparisons of real numbers, and\n•output 0;1,\nthen the VC dimension of HisO(a2t2+t2aloga).\nCombining the previous results allows us prove Theorem F.1. At a high level, we are instanti-\nating Lemma F.6 with the complexity of computing any function in the function class G.\nProof of Theorem F.1. First by Theorem F.3 and Corollary F.4, it su\u000eces to bound the pseudo-\ndimension of the class G=L\u000eF. Then from Lemmas F.5, the pseudo-dimension of Gis the VC\ndimension of threshold functions de\fned by G. Finally from Lemma F.6, the VC dimension of the\nappropriate class of threshold functions is polynomial in the complexity of computing a member\nof the function class. In other words, Lemma F.6 tells us that the VC dimension of BGde\fned in\nLemma F.5 is polynomial in the number of arithmetic operations needed to compute the threshold\nfunction associated to some g2G. By our de\fnition, this quantity is polynomial in T(n;d). Hence,\nthe pseudo-dimension of Gis also polynomial in T(n;d) and our desired result follows.\n36\n\nNote that we can consider initializing Theorem F.1 with speci\fc predictions. If each function in\nthe family of oracles we are interested can be computed e\u000eciently, which is the case of the predictors\nwe employ in our experiments, then Theorem F.1 assures us that we only require polynomially many\nsamples to be able to learn a nearly optimal oracle.\nNote that our result is in similar in spirit to the recent paper Dinitz et al. (2021). They derive\nsample complexity learning bounds for a di\u000berent algorithmic problem of computing matchings in a\ngraph. Since they specialize their analysis to a speci\fc function class and loss function, their bounds\nare possibly tighter rather than the possibly loose polynomial bounds we have stated. However,\nour analysis above is more general as it allows for a variety of predictors, as we employ in our\nexperiments, and loss functions to measure the quality of the predictions.\n37",
  "textLength": 101743
}