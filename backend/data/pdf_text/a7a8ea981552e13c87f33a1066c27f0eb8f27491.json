{
  "paperId": "a7a8ea981552e13c87f33a1066c27f0eb8f27491",
  "title": "Learned Interpolation for Better Streaming Quantile Approximation with Worst-Case Guarantees",
  "pdfPath": "a7a8ea981552e13c87f33a1066c27f0eb8f27491.pdf",
  "text": "Learned Interpolation for Better Streaming Quantile Approximation\nwith Worst-Case Guarantees\nNicholas Schiefer∗Justin Y. Chen†Piotr Indyk†Shyam Narayanan†\nSandeep Silwal†Tal Wagner‡\nAbstract\nAn\"-approximate quantile sketch over a stream of ninputs\napproximates the rank of any query point q|that is, the\nnumber of input points less than q|up to an additive error of\n\"n, generally with some probability of at least 1 \u00001=poly( n),\nwhile consuming o(n) space. While the celebrated KLL\nsketch of Karnin, Lang, and Liberty achieves a provably\noptimal quantile approximation algorithm over worst-case\nstreams, the approximations it achieves in practice are\noften far from optimal. Indeed, the most commonly used\ntechnique in practice is Dunning's t-digest, which often\nachieves much better approximations than KLL on real-\nworld data but is known to have arbitrarily large errors in\nthe worst case. We apply interpolation techniques to the\nstreaming quantiles problem to attempt to achieve better\napproximations on real-world data sets than KLL while\nmaintaining similar guarantees in the worst case.\n1 Introduction\nThe quantile approximation problem is one of the most\nfundamental problems in the streaming computational\nmodel, and also one of the most important streaming\nproblems in practice. Given a set of items x1;x2;:::;x n\nand a query point q, the rank ofq, denoted R(q),\nis the number of items in fxign\ni=1such thatxi\u0014q.\nAn\"-approximate quantile sketch is a data structure\nthat, given access to a single pass over the stream\nelements, can approximate the rank of all query points\nsimultaneously with additive error at most \"n.\nGiven its central importance, the streaming quan-\ntiles problem has been studied extensively by both the-\noreticians and practitioners. Early work by Manku, Ra-\njagopalan, and Lindsay [10] gave a randomized solution\nthat usedO((1=\") log2(n\")) space; their technique can\nalso be straightforwardly adapted to a deterministic so-\nlution that achieves the same bound [14]. Later, Green-\nwald and Khanna [4] developed a deterministic algo-\n∗Anthropic. Work done while at MIT.\n†Massachusetts Institute of Technology\n‡Amazon AWS. Work done while at Microsoft Research.rithm that requires only O((1=\") log(n\")) space. More\nrecently, Karnin, Lang, and Liberty (KLL) [7] developed\nthe randomized KLL sketch that succeeds at all points\nwith probability 1 \u0000\u000eand usesO((1=\") log log(1=\u000e))\nspace and gave a matching lower bound.\nMeanwhile, streaming quantile estimation is of sig-\nni\fcant interest to practitioners in databases, computer\nsystems, and data science who have studied the prob-\nlem as well. Most notably, Dunning [3] introduced the\ncelebrated t-digest, a heuristic quantile estimation tech-\nnique based on 1-dimensional k-means clustering that\nhas seen adoption in numerous systems, including In-\n\rux, Apache Arrow, and Apache Spark. Although t-\ndigest achieves remarkable accuracy on many real-world\ndata sets, it is known to have arbitrarily bad error in\nthe worst case [2].\nTo illustrate this core tradeo\u000b, Figure 1 shows the\nrank function of the books dataset from the SOSD\nbenchmark [8, 11], along with KLL and t-digest approx-\nimations that use the same amount of space when the\ndata set is randomly shu\u000fed, and when the same data\nset is streamed in an adversarial order that we found to\ninduce especially bad performance in t-digest.\nRecent advances in machine learning have led to the\ndevelopment of learning-augmented algorithms which\nseek to improve solutions to classical algorithms prob-\nlems by exploiting empirical properties of the input dis-\ntribution [12]. Typically, a learning-augmented algo-\nrithm retains worst-case guarantees similar to those of\nclassical algorithms while performing better on nicely\nstructured inputs that appear in practical applications.\nWe might hope that a similar technique could be used\nfor quantile estimation.\nIn fact, one of the seminal results in the \feld studied\nthe related problem of learned index structures . An\nindex is a data structure that maps a query point to\nits rank. Several model families have been tried for\nthis learning problem, including neural networks and\nthe successful recursive model index (RMI) that de\fne\na piecewise-linear approximation [9].\nAlthough learned indexes aim to answer rank\nCopyright ©2023\nCopyright for this paper is retained by authorsarXiv:2304.07652v1  [cs.DS]  15 Apr 2023\n\nFigure 1: Ground-truth and approximate rank functions for the SOSD books data set, with approximation by\nboth the KLL and t-digest sketches. For the approximations, the data were presented in both randomly shu\u000fed\n(top) and adversarial (bottom) order. In the adversarial case that we discovered, t-digest does much worse than\nKLL, demonstrating the value of worst-case correctness.\nqueries, they do not solve the streaming quantiles esti-\nmation problem because they do not operate on the data\nin a stream. For example, training a neural network or\n\ftting an RMI model require O(n) of the elements in\nthe stream to be present in memory simultaneously, or\nrequire multiple passes over the stream.\n1.1 Our contributions. We present an algorithm\nfor the streaming quantiles problem that achieves much\nlower error on real-world data sets than the KLL sketch\nwhile retaining similar worst case guarantees. This\nalgorithm, which we call the linear compactor sketch,\nuses linear interpolation in place of parts of the KLL\nsketch. Intuitively, this linear interpolation provides\na better approximation to the true cumulative density\nfunction when that function is relatively smooth, a\ncommon property of CDFs of many real world datasets.\nOn the theoretical side, we prove that the linear\ncompactor sketch achieves similar worst case error to the\nKLL sketch. That is, the linear compactor sketch com-\nputes an\"-approximation for the rank of a single item\nwith probability 1 \u0000\u000eand spaceO((1=\") log2log(1=\u000e)).\nThis is within a factor that is poly-log-logarithmic (in\n1=\u000e) of the known lower bounds and the (rather com-plex) version of the KLL sketch that matches it [7]. Our\nproof is a relatively straightforward modi\fcation of the\nanalysis of the original KLL sketch, due to the general\nsimilarity of the algorithms. In fact, we can view our al-\ngorithm as exploiting a place in the KLL sketch analysis\nthat left some \\slack\" in the algorithm design.\nIn our experiments, we demonstrate that the linear\ncompactor sketch achieves signi\fcantly lower error than\nthe KLL sketch on a variety of benchmark data sets\nfrom the SOSD benchmark library [8, 11] and for a\nwide variety of input orders that induce bad behaviour\nin other algorithms like t-digest. In many cases, the\nlinear compactor sketch achieves a space-error tradeo\u000b\nthat is competitive with t-digest, while also retaining\nworst-case guarantees.\n2 Understanding the KLL sketch\nThe complete KLL sketch that achieves optimal space\ncomplexity is complex: it involves several di\u000berent\ndata structures, including a Greenwald-Khanna (GK)\nsketch that replaces the top O(log log(1=\u000e)) compactors.\nHere, we present a simpler version of the KLL sketch\nthat usesO((1=\") log2log(1=\u000e)) space|just a factor of\nO(log log(1=\u000e)) away from optimal|and is commonly\nCopyright ©2023\nCopyright for this paper is retained by authors\n\nimplemented in practice [5], presented in Theorem 4\nof [7]. In the remainder of this paper, we refer to this\nsketch as the non-GK KLL sketch .\n2.1 The non-GK KLL sketch. The basic KLL\nsketch is composed of a hierarchy of compactors . Each\nof theHcompactors has a capacity k, which de\fnes\nthe number of items that it can store. Each item is\nalso associated with a (possibly implicit) weight which\nrepresents the number of points from the input stream\nthat it represents in the sketch. All points in the same\ncompactor have the same weight.\nWhen a compactor reaches its capacity, it is com-\npacted. A compaction begins by sorting the items.\nThen, either the even or odd elements in the compactor\nare chosen, and the unchosen items are discarded. The\nchoice to discard the even or odd items is made with\nequal probability. The chosen items are then placed into\nthe next compactor in the hierarchy and the points are\nall assigned a weight twice what they began with. This\ngeneral setup is common to many streaming quantiles\nsketches [10, 7].\nTo predict the rank of a query point q, we return\nthe sum of the weights of all points, in all compactors,\nthat are at most q.\nA key contribution of the KLL sketch is to use\ndi\u000berent capacities for di\u000berent compactors. We say\nthat the \frst compactor where points arrive from the\nstream has a height of 0, and each successive compactor\nhas a height one higher than the compactor below\nit, so that the top compactor has height H\u00001. In\nKLL, the compactor at height hhas capacity kh=\nmax(kcH\u0000h;2), where kis a space parameter that\nde\fnes the capacity of the highest compactor and cis a\nscale parameter that is generally set as c= 2=3.\n2.2 Analysis of the non-GK KLL sketch. Here,\nwe give a somewhat simpli\fed|to focus on the essential\ndetails|version of the analysis of the non-GK KLL\nsketch. Consider the non-GK KLL sketch described\nabove that terminates with Hdi\u000berent compactors.\nThe weight of the items at height hiswh= 2h. Let\nmhbe the number of compaction operations in the\ncompactor at height h.\nConsider a single compaction operation in the com-\npactor at height hand a point xin that compactor at\nthat time. If xwas one of the even elements in the com-\npactor, the total weight to the left of it, which de\fnes\nits rank, is unchanged by the compaction. If xis one\nof the odd elements in the compactor, the total weight\neither increases by wh(if the odd items are chosen) or\ndecreases by wh(if the even items are chosen). For the\nith compaction operation at level h, letXi;hbe 1 if theodd items were chosen and \u00001 if the even items were\nchosen. Observe that E[ Xi;h] = 0 andjXi;hj\u00141. Then\nthe total error introduced by all compactions at level\nhisPmh\ni=1whXi;h. Consider any point xin the stream.\nThe error in R(x) introduced by compaction at all levels\nup to a \fxed level H0is thereforePH0\u00001\ni=0Pmh\ni=1whXi;h.\nApplying a two-tailed Hoe\u000bding bound to this error,\nwe obtain that\nPr[error is>\"n ]\n= Pr2\n4\f\f\f\f\f\fH0\u00001X\ni=0mhX\ni=1whXi;h\f\f\f\f\f\f>\"n3\n5\n\u00142 exp \n\u0000\"2n2\n2PH0\u00001\ni=0Pmh\ni=1w2\nh!\n:\nThis addresses the error introduced by all layers\nup toH0. Notice that if we set H0=H, then the\nerror bound is dominated by the weight terms from\nthe highest compactors. To get around this, the non-\nGK KLL sketch sets the capacity of the \fnal s=\nO(log log(1=\u000e)) compactors to a \fxed constant kand\nanalyzes them separately: it is assumed to contribute its\nworst possible error of whfor reach compaction. This\nis the key lemma in the KLL analysis and the point of\ndeparture for the linear compactor sketch.\n3 The linear compactor sketch\nWe propose a streaming quantile approximation algo-\nrithm that combines our empirical and theoretical ob-\nservations about how KLL might be improved. We leave\nthe basic architecture of the non-GK KLL sketch un-\nchanged. Like the optimal KLL sketch, which replaces\nthe topO(log log(1=\u000e)) compactors with a Greenwald-\nKhanna sketch, we replace some of these top compactors\nwith another data structure. In our case, we replace the\ntopt=O(1) compactors with a structure that we call\nalinear compactor .\nLinear compactors. Alinear compactor is a\nsorted list of elements, each of which is a pair of an item\nfrom the stream and a weight. As in KLL, the weight\nrepresents the number of stream items that the item\nrepresents; unlike in KLL, this weight varies between el-\nements in the list and may be an arbitrary \roating point\nnumber, rather than a power of two. Like a KLL com-\npactor, a linear compactor has a capacity which we \fx\ntotk, the total capacity of the (\fxed-size) compactors it\nreplaces. When that capacity is exceeded, it undergoes\ncompaction and only half of its elements are retained.\nA KLL compactor Chat heighthimplicitly repre-\nCopyright ©2023\nCopyright for this paper is retained by authors\n\nsents a piecewise-constant function f: speci\fcally,\nf(q) =X\nx2Ch:x\u0014qwh:\nThis function is the contribution of this compactor\nto the approximated rank of a query point q. A\nlinear compactor implicitly represents a piecewise-linear\nfunction which also contributes to the rank of q. Given\na linear compactor L=f(y1;w1);(y2;w2);:::; (yk;wk)g\nwithy1\u0014y2\u0014\u0001\u0001\u0001\u0014yk, the contribution of Lto the\nthe rank of qis\n(3.1)fL(q) =i\u0003\u00001X\ni=1wi\n|{z}\nKLL-style term+wi\u0003q\u0000yi\u0003\u00001\nyi\u0003\u0000yi\u0003\u00001|{z}\ninterpolation term\nwherei\u0003is the smallest index such that yi\u0003> q. In\ne\u000bect, we spread the weight of yi\u0003over the entire interval\nbetweenyi\u0003\u00001andyi\u0003, with uniform density, rather\nthan treating it as a point mass at yi\u0003exactly. The\nresulting contribution fL(q) is a monotone, piecewise-\nlinear function, as desired.\nAdding points to a linear compactor. Our\nlinear compactor receives points from the last of the\nKLL-style compactors, each with a \fxed weight of\nwH\u0000t\u00001. These points and weights cannot be merged by\nmerely concatenating the arrays. To see this, consider\nadding a single point bwith unit weight to a compactor\nwith two points aandcwitha<b<c , and where chas\nweightw. The weight of cafter the compaction should\nnot bewsince the weight of cbefore the addition should\nbe spread uniformly over the entire interval [ a;c].\nInstead, we add a set of new points y1<y2<\u0001\u0001\u0001<\nymto an existing set of points x1< x 2<\u0001\u0001\u0001< xnby\nmerging the two lists of points into one list and sorting\nthem into the list z1< z2<\u0001\u0001\u0001< zm+n. Next, we set\nw(z1) equal to the weight of z1in the original list and\ncompute the new weights recursively. Assuming that\nzi=xiwithout loss of generality, we set\nw(zi) =w(xi)xi\u0000zi\u00001\nxi\u0000xi\u00001+w(y\u0003)xi\u0000zi\u00001\ny\u0003\u0000y\u0003\u00001\nwherey\u0003is the \frstyisuch thatyi>zi.\nEquivalently, we convert each of the weight func-\ntions into a rank function using Equation 3.1, sum those,\nand then compute the \fnite di\u000berences to obtain the \f-\nnal weight function.\nCompacting a linear compactor. Lastly, we de-\nscribe the process for compacting a linear compactor.\nGiven a parameter \u000b2[0;1] and a linear compactor C\ncontainingnpoints, we wish to obtain a new linear com-\npactorC0with\u000bnpoints with the following properties:•The points in C0are subset of the points in C.\n•The total weight of the points in both compactors\nis the same, so thatP\nx2Cw(x) =P\nx2C0w(x0).\n•For every point x2C0, the rankfC(x) =fC0(x).\n•The \\error\" introduced by the compaction is as\nsmall as possible. That is, for some loss function L,\nwe would likeP\nx2CL(fC0(x);fC(x)) to be as small\nas possible.\nIn this paper, we use \u000b= 1=2, although in principle\nother values could be used.\nIt is important that this procedure can be com-\npleted e\u000eciently. In our experiments, we primarily use\nsupremum ( `1) lossL(x;x0) = supxjx\u0000x0j. This can\nbe minimized using a dynamic programming technique\nintroduced by [6].\n4 Analysis\nWe give a worst-case analysis of our algorithm that\nmatches the worst-case analysis for the version of the\nnon-GK KLL sketch:\nTheorem 4.1. The linear compactor sketch described\nin Section 3 computes an \"-approximation for the rank\nof a single item with probability 1\u0000\u000ewith space com-\nplexityO((1=\") log2log(1=\u000e)).\nOur technique analyzes the error introduced by each\ncompactor, using two techniques. To analyze the error\nof the KLL-style compactors of the linear compactor\nsketch, we prove that they introduce precisely the same\nerror as they would in a non-GK KLL sketch run on\nthe same stream. We then apply the two-part analysis\nof the non-GK KLL sketch, analyzing the \frst H\u0000s\ncompactors and the ( H\u0000s)th through ( H\u0000s+t)th\ncompactors separately. To analyze the error of the linear\ncompactor at the top, we analyze the error introduced\nper compaction. We then analyze the number of\ncompactions of the linear compactor and therefore the\ntotal error introduced by the linear compactor.\nConsider a stream X=x1;x2;:::;x n. LetS(X)\nbe a non-GK KLL sketch computed on this stream\nthat terminates with Hcompactors and let Sb(X) be\nthebth compactor of S(x). Similarly, let S0(X) be a\nlinear compactor sketch computed on this stream with\nH\u0000tlevels of KLL-style compactors and one linear\ncompactor at level H\u0000t+ 1. LetS0\nb(X) be thebth\ncompactor S0(X).\nFollowing [7], let R(S;x;h ) be the rank of item x\namong all points in compactors in the sketch Sat\nheights at most h0\u0014hat the end of the stream. For\nconvenience, we set R(x;0) to be the true rank of x\nCopyright ©2023\nCopyright for this paper is retained by authors\n\nin the input stream. Let err( S;x;h ) =R(S;x;h )\u0000\nR(S;x;h\u00001) be the total change in the approximate\nrank ofxdue to the compactor at level h. The\ntotal error decomposes into this error per compactor\nas supxjR(x;0)\u0000S0(x)j=PH\nh=1err(S0;x;h).\nAnalyzing the KLL compactors. In bothSand\nS0, stream elements only move from lower compactors to\nhigher ones, and the compactor at level bat any point\nwhile processing the stream is de\fned entirely by the\ncompactors at lower levels up to that point. Therefore,\nfor allb<H\u0000t,S0\nb(X) =S(X).\nIn a KLL sketch, the lowest compactors all have a\ncapacity of exactly 2. As the authors note, a sequence\nofH00compactors that all have capacity 2 is essentially\na sampler: out of every 2H00elements they select\none uniformly and output it with weight 2H00. This\nmeans that these compactors|in both KLL and linear\ncompactor sketch|can be implemented in O(1) space.\nTo handle the other KLL compactors, we use a\ntheorem from [7] as a key lemma:\nTheorem 4.2. (Theorem 3 in [7]) Consider the\nnon-GK KLL sketch S(X)with height H, and where\nthe compactor at level hhas capacity kh\u0015kcH\u0000h. Let\nH00be the height at which the compactors have size\ngreater than 2 (i.e., where the compactors do not just\nperform sampling). For any H0>H00, we have\nPr\"HX\nh=1err(S;x;h )>2\"n#\n\u00142 exp\u0010\n\u0000c\"2k2H\u0000H00=32\u0011\n+ 2 exp\u0010\n\u0000C\"2k222(H\u0000H0)\u0011\n:\nAnalyzing the linear compactor. As men-\ntioned, we will analyze the error introduced by the linear\ncompactor compaction-by-compaction. Speci\fcally, we\nanalyze the linear compactor sketch between the end\nof one compaction and the end of the following com-\npaction. During this interval, a total of ditems of weight\n2H\u0000tare added to the linear compactor, where either\nd=tkif the linear compactor has never compacted or\nd=tk=2 if it has.\nLetfbe the piecewise linear rank function of the\nfull linear compactor right before the compaction with\nendpoint set Zcomprising z1< z 2<\u0001\u0001\u0001< ztkand\nweight function w. Letf0be the piecewise linear rank\nfunction of the linear compactor immediately after the\ncompaction, with endpoint set Z0\u001aZ, weight function\nw0, andjZ0j=jZj=2.\nThe linear compactor compaction procedure re-\nmoves some of the items in the linear compactor. A\nrunis a sequence of removed elements that are adja-\ncent in sorted order. We show that the error introducedby a linear compactor is bounded by the greatest run of\ndisplaced weight.\nLemma 4.1. OrganizeZnZ0into continuous runs of\nadjacent removed elements, and let Fibe the total weight\nof theith run. Then supz2Zjf(z)\u0000f0(z)j\u0014max iFi.\nProof. Fix a run with endpoints aandband let its\ntotal weight be F=Pb\u00001\ni=a+1w(zi). Consider any point\nzjin that run, so that a < j < b . Its original rank\nwasf(zj) =Pj\ni=1w(zi) while its new rank is, by\nconstruction, f0(zj) =Pa\ni=1w(zi) +F+w(zb)\nzb\u0000za(zj\u0000za).\nTherefore,\njf(zj)\u0000f0(zj)j=\f\f\f\f\fjX\ni=a+1w(zi)\u0000F+w(zb)\nzb\u0000za(zj\u0000za)\f\f\f\f\f\n=\f\f\f\f\fjX\ni=a+1w(zi)\u0000bX\ni=a+1w(zi)zj\u0000za\nzb\u0000za\f\f\f\f\f\n\u0014b\u00001X\ni=a+1w(zi)\n=F:\nNext, we show that the greater error introduced by\na linear compaction step occurs at one of the discarded\nendpoints :\nLemma 4.2. There is some zi2ZnZ0such that\nsupx2[z1;ztk]jf(x)\u0000f0(x)j=jf(zi)\u0000f0(zi)j.\nProof. Consider any point x2[z1;zd]. Ifxis one of\nthe endpoints retained after compaction zj2Z0, then\nby construction f(x) =P\ni\u0014jw(zi) =f0(x). Our claim\ndoes not depend on the error if xis one of the endpoints\ninZnZ0.\nSuppose then that xis not in the original endpoint\nsetZ. Letzaandzbbe the left and right neighbours of\nxinZ. By the de\fnition of the linear compactor,\nf(za) =aX\ni=1w(zi);\nf(x) =aX\ni=1w(zi) +w(zb)\nzb\u0000za(x\u0000za);\nf(zb) =bX\ni=1w(zb):\nLetza0andzb0be the left and right neighbours of xinZ0.\nBy de\fnition, the weight W:=w0(zb0) =Pb0\ni=a0+1w(zi)\nand so we have\nf0(x) =a0X\ni=1w(zi) +W\nzb0\u0000za0(x\u0000za0):\nCopyright ©2023\nCopyright for this paper is retained by authors\n\nTherefore,\nf(x)\u0000f(x0) =aX\ni=a0+1w(zi)+\n\u0012w(zb)\nzb\u0000za\u0000W\nzb0\u0000za0\u0013\n(x\u0000za0):\nObserve that this expression obtains its extremum\non the interval [ za;zb]\u001a[za0;zb0] at either zaorzb,\ndepending on the sign of D=w(zb)\nzb\u0000za\u0000W\nzb0\u0000za0. In either\ncase,jf(x)\u0000f(x0)jachieve its maximum at one of the\nendpointszaorzb, completing the proof.\nWe use a simple counting argument to bound the\nsize of the majority of the weights in a linear compactor:\nLemma 4.3. Consider a linear compactor that has just\ncompleted its cth compaction. At least half of the\nendpointsZin the linear compactor have weight at most\n(2c+ 3)2H\u0000t.\nProof. Every point enters the linear compactor with\nweight 2H\u0000t. Afterccompactions, a total of (2+ c)tk=2\nsuch points have entered the compactor. A compaction\noperation conserves the total weight of points so the\ntotal weight of the compactor is (2 + c)2H\u0000ttk=2.\nSuppose that more than half of the tkpoints\ncurrently in the compactor have weight at most T.\nThese points have a total weight greater than Ttk= 4\nwhile the remaining point s each have weight at least\n2H\u0000tand so have total weight at least 2H\u0000ttk=4.\nThe total weight is therefore ( T+ 2H\u0000t)tk=4. This\nweight must not exceed the total conserved weight\n(2 +c)2H\u0000ttk=2, and so we have\n(T+ 2H\u0000t)tk\n4\u0014(2 +c)2H\u0000ttk\n2:\nRearranging, we obtain that our result holds for any\nT\u0014(2c+ 3)2H\u0000t.\nCombining these lemmas, we obtain a bound on the\nerror introduced during a single compaction step.\nTheorem 4.3. Suppose that the compaction being stud-\nied is the (c+ 1)th compaction. The error introduced\nduring this compaction step is supxjf(x)\u0000f0(x)j \u0014\n(c+ 2)2H\u0000t+1.\nProof. We construct a particular post-compaction dis-\ntribution of weights as follows. Let f00be the rank func-\ntion for that post-compaction state. During this inter-\nval, there were tk=2 points with weight 2H\u0000tthat we\nadded to the linear compactor for the \frst time.In addition, there were tk=2 points remaining from\na previous linear compaction. We sort the tk=2 new\npoints and keep every fourth point, discarding the rest\nand reallocting their weight to the next highest retained\npoint (of either type). By Lemma 4.3, there exists at\nleasttk=4 of the existing points in the linear compactor\nwith weight at most 2H\u0000t+c. We sort these points and\ndiscard every other point. In total, we discard the\nrequiredtk=2 points.\nObserve that the longest possible run in this com-\npaction consists of one of the existing points and three\n(out of a sequence of four) of the new points that were\ndiscarded. By Lemma 4.1, the error introduced on any\nof the original endpoints by this compaction is bounded\nby the sum of the weights of the points in the run: in this\ncase, that sum is 2H\u0000t+3\u0001(2c+3)2H\u0000t\u0014(c+2)2H\u0000t+1.\nBy Lemma 4.2, we \fnd that the error introduced by f00\nis supxjf(x)\u0000f00(x)j\u0014(c+ 2)2H\u0000t+1.\nWe have exhibited a particular feasible solution to\nthe optimization problem in the linear compaction. Our\nactual algorithm \fnds, among all such feasible solutions,\nthe one that minimizes this error function; it follows\nthat\nsup\nxjf(x)\u0000f0(x)j\u0014sup\nxjf(x)\u0000f00(x)j\n\u0014(c+ 2)2H\u0000t+1:\nCombining KLL and linear compactors.\nLastly, we combine our analysis of the KLL and linear\ncompactor to obtain an overall error bound and prove\nTheorem 4.1. Our analysis closely follows the form of\nthe proof of Theorem 4 in [7].\nProof. [Proof of Theorem 4.1] First, we analyze the\ncompactors with height at most H\u0000t, including the\nsampling compactors. These are all KLL-style com-\npactors; by Theorem 4.2 these compactors will con-\ntribute error at most \"nwith probability 1 \u0000\u000eso long as\n\"k2s\u0015c0p\nlog(2=\u000e) for a su\u000eciently small c0. Second,\nwe analyze the top s\u0000tcompactors. The error intro-\nduced by these compactors is bounded by the error of\nthe equivalent non-GK KLL sketch where we have a full\nsequal-size compactors at the top. This error is in turn\nbounded byPh\nh=H\u0000s+1mhwh=PH\nh=H\u0000s+1n=k =\nsn=k , wheremhis the number of times that the KLL\ncompactor at level his compacted and wh= 2his the\nweight associated with that compactor; this is at most\n\"nso long ass\u0014k\". Takingk=O((1=\") log log(1=\u000e))\nands=O(log log(1=\u000e)) as in KLL, we satisfy both of\nthese conditions.\nLastly, we analyze the single linear compactor with\nsizetkthat replaces the top t < s KLL compactors.\nLetMbe the number of compactions of the linear\nCopyright ©2023\nCopyright for this paper is retained by authors\n\nFigure 2: The rank functions for the three SOSD data sets used in our experiments. The three data sets have\nrank functions with distinctive shapes, allowing us to compare the algorithms in a variety of settings.\ncompactor. Observe that since between each com-\npaction of the linear compactor we add tk=2 entries,\neach with weight 2H\u0000tto the compactor, and so M\u0014\n2n=(tk2H\u0000t). Applying Theorem 4.3, and summing the\nerror introduced per compaction, the total error is\nMX\nc=1(c+ 2)2H\u0000t+1=M2H\u0000t+ 2H\u0000tM(M+ 1)\n\u00142H\u0000t+1M2\n\u00148n2\nt2k22H\u0000t:\nOur compactors are sized at each level in the same\nway as a non-GK KLL-sketch. As in the KLL analysis,\nwe haveH\u0014log(n=ck ) + 2 for a constant 0 < c < 1.\nTherefore, our error is bounded by\n8n2\nt2k22log(n=ck )+2\u0000t\u00148ckn2\nt2k2n22\u0000t=cn2t+1\nt2k:\nFor constant tand anyk=O((1=\") log log(1=\u000e)) as in\nKLL, this is at most \"n. Therefore, the total error of\nthe sketch is O(\"n) as required.\nEach part of the sketch contributes some space.\nThe KLL compactors increase geometrically in size, so\nthe space used by the KLL portion of the sketch is\ndominated by the top s\u0000tcompactors and uses O(sk) =\nO((1=\") log2log(1=\u000e)) space. The linear compactor\nsketch uses twice as much space per element as a KLL\ncompactor, for a total of O(tk) =O(k) space, so the\ntotal space usage is O((1=\") log2log(1=\u000e)).\n5 Experiments\nWe wrote a performant implementation of our algorithm\nand evaluated its empirical error over a wide range\nof space parameters kand several linear compactor\nheightst. Our experiments were conducted on the\nrecent SOSD benchmarking suite [8, 11] for learned\nindex structures. Each SOSD benchmark consists ofa large number (generally 200 to 800 million) of 64-bit\nunsigned integer values. Of particular interest were the\nbooks ,osmcellid , and wiki tsdata sets, since the\nrank functions of these three data sets have distinctly\ndi\u000berent shapes, as shown in Figure 2.\nParameterization. The algorithm is parameter-\nized by the KLL space parameter k, which determines\nthe size of the largest compactors and the linear com-\npactor, and t, the number of KLL compactors that are\nreplaced by the linear compactor. Our worst-case bound\nholds for any constant tbut this bound is exponential\nint. In practice, we experimented with a variety of\nsmall but non-zero values ( t= 1;2;3). We see tas\na parameter that is tunable based on the desired em-\npirical performance and desired worst-case guarantees\nand expect that it will be selected appropriately on an\napplication-by-application basis.\nImplementation details. We implemented our\nalgorithms in C++ with Python bindings for experi-\nment management and data analysis. Our implementa-\ntion is reasonably performant: in informal experiments,\nit achieves a throughput that is only about three times\nless than that of highly-optimized, production-quality\nKLL implementations. This performant implementa-\ntion allowed us to work with the entirety of the SOSD\ndata sets; in our preliminary work, we found that many\npromising algorithms would only show improvements\nover KLL on moderately-sized data sets of less than a\nmillion points. Our implementation supports any inte-\ngert\u00150: whent= 0, our implementation is identical\nto the commonly implement variant of KLL without the\nGreenwald-Khanna sketch.\nBaselines. Our algorithm is most naturally com-\npared to KLL since the KLL sketch can be seen as an\ninstance of the linear compactor sketch with no linear\ncompactor. We ran our experiments on our implemen-\ntation of (non-GK) KLL (by setting t= 0) and val-\nidated those results with an open-source implementa-\nCopyright ©2023\nCopyright for this paper is retained by authors\n\ntion from Facebook's Folly library [1]. Like most im-\nplemented version of the KLL sketch, neither of these\ninclude the \fnal Greenwald-Khanna sketch that is re-\nquired to achieve space-optimality.\nIn addition to the non-GK KLL sketch, which o\u000bers\nworst-case guarantees, we ran experiments on the t-\ndigest [3], which is commonly used in practice but is\nknown to have arbitrarily bad worst-case performance\n[2]. We used the C++ implementation of t-digest in the\ndigestible library [13].\nStream order. We found that many streaming\nquantile approximation algorithms without worst-case\nguarantees achieve very low error compared to the\nKLL sketch if they are given an input stream in a\nparticular order but high error on other input orders.\nFor example, Figure 1 shows that, even for a \fxed\nset of inputs with a smooth rank function ( books ),\nthere exists an adversarial order that makes the t-digest\napproximation have high error. This observation might\nbe of independent interest.\nWe evaluated the linear compactor sketch and the\nbaselines on a variety of input orders for each data set:\n•Random: the data are shu\u000fed with a \fxed seed.\n•Sorted: the data are presented in a sorted order.\n•First half sorted, second half reverse-sorted: the\n\frst half of the stream has the \frst half of the sorted\ndata, in that order. The second half of the stream\nhas the second half of the sorted data presented in\nreverse-sorted order .\n•Flip \rop: the stream has the smallest element,\nthen the largest element, then the second-smallest\nelement, then the second-largest element, and so\non. This is the adversarial order from Figure 1.\n5.1 Experiment results and discussion. Our pri-\nmary tool for insight into our experiments is the space-\nerror tradeo\u000b curve that shows how the total space\nneeded for the sketch compares to the empirical error\nbetween the exact rank function and the approxima-\ntion de\fned by the sketch. We obtain these curves for\nthree di\u000berent data sets from SOSD, four di\u000berent sort\norders, and four di\u000berent algorithms; these curves are\nshown in Figure 4. We use average L1 error, de\fned\nfor a data set XasP\nx2Xjf(x)\u0000f0(x)j. Qualitatively,\nthe linear compactor sketch is never signi\fcantly worse\nthan KLL, even on our adversarial input orders like\n\rip \rop, and is often competitive with|or even better\nthan|t-digest. The di\u000berences are most pronounced\non the books dataset, which has a smooth CDF that\nis extremely well-approximated by the linear compactor\nsketch's piecewise linear representation.\nFigure 3: An example of the \\envelope\" or hull of the\nspace-error data points for the linear compactor sketch\nwitht= 2 ( books dataset, random order). It shows the\nrange of errors that a user can expect from the linear\ncompactor sketch given a certain amount of space.\nFor a more quantitative understanding of the per-\nformance of the linear compactor sketch compared to\nKLL and t-digest, we produced diagrams of the \\pos-\nsible error ratio hulls\", shown in Figure 5. To obtain\nsuch a hull, we \frst determined the upper and lower\nfrontiers for the data points (in Figure 4) for each algo-\nrithm. These frontiers form an \\envelope\" or hull that\nencompasses all of the points for each dataset: an ex-\nample of such a hull is shown in Figure 3. We then\ninterpolate the envelope to obtain smooth curves (as\nin Figure 3) and compute the ratios with respect to a\nanother algorithm's envelope (between the upper/lower\nand lower/upper pairs), producing a hull that shows the\nrange of behaviour between the \\worst case\" and \\best\ncase\" performance of the two algorithms. We see that\nthe linear compactor sketch achieves an error that is be-\ntween 3\u0002worse and 10\u0002better than KLL and between\n10\u0002worse and 20\u0002better than t-digest.\nAcknowledgements\nJustin Y. Chen was supported by a MathWorks Engi-\nneering Fellowship, a GIST-MIT Research Collabora-\ntion grant, and NSF award CCF-2006798. Justin Y.\nChen, Shyam Narayanan, and Sandeep Silwal were sup-\nported by NSF Graduate Research Fellowships under\nGrant No. 1745302. Nicholas Schiefer, Justin Y. Chen,\nPiotr Indyk, Shyam Narayanan, and Sandeep Silwal\nwere supported by a Simons Investigator Award. Piotr\nIndyk was supported by the NSF TRIPODS program\n(award DMS-2022448).\nWe thank Sylvia H urlimann, Jessica Balik, and the\nanonymous reviewers for their helpful suggestions.\nCopyright ©2023\nCopyright for this paper is retained by authors\n\nFigure 4: Space-error tradeo\u000b curves for the baselines and linear compactor sketch on three di\u000berent data sets\nfrom SOSD and four di\u000berent sort order, described above. Markers indicate individual sketches, while curves\nindicate the lower frontier of possibilities observed (that is, the lower envelope described above) to highlight the\ngeneral capabilities of each algorithm. A better algorithm has a curve that is further down and to the left,\nindicating lower error at a given amount of space.\nCopyright ©2023\nCopyright for this paper is retained by authors\n\nFigure 5: Hulls representing the possible ratios of the error achieved by a reference algorithm (either KLL or\nt-digest) to the error achieved by the linear compactor sketch with t= 2. The \flled-in area represents the area\nratios consistent with the experiments in Figure 4. We see that the linear compactor sketch always achieve error\nno worse than 3\u0002that of KLL, while often achieving and error that is competitive with|and sometimes much\nlower than|that achieved by the t-digest.\nCopyright ©2023\nCopyright for this paper is retained by authors\n\nReferences\n[1] Folly: Facebook open-source library. https://github.\ncom/facebook/folly .\n[2] Graham Cormode, Abhinav Mishra, Joseph Ross, and\nPavel Vesel\u0012 y. Theory meets practice at the median:\na worst case comparison of relative error quantile\nalgorithms. In Proceedings of the 27th ACM SIGKDD\nConference on Knowledge Discovery & Data Mining ,\npages 2722{2731, 2021.\n[3] Ted Dunning. The t-digest: E\u000ecient estimates of\ndistributions. Software Impacts , 7:100049, 2021.\n[4] Michael Greenwald and Sanjeev Khanna. Space-\ne\u000ecient online computation of quantile summaries.\nACM SIGMOD Record , 30(2):58{66, 2001.\n[5] Nikita Ivkin, Edo Liberty, Kevin Lang, Zohar Karnin,\nand Vladimir Braverman. Streaming quantiles algo-\nrithms with small space and update time. Sensors ,\n22(24), 2022.\n[6] Hosagrahar Visvesvaraya Jagadish, Nick Koudas,\nS Muthukrishnan, Viswanath Poosala, Kenneth C Sev-\ncik, and Torsten Suel. Optimal histograms with quality\nguarantees. In VLDB , volume 98, pages 24{27, 1998.\n[7] Zohar Karnin, Kevin Lang, and Edo Liberty. Optimal\nquantile approximation in streams. In 2016 IEEE\n57th Annual Symposium on Foundations of Computer\nScience (FOCS) , pages 71{78, 2016.\n[8] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mi-\nhail Stoian, Alfons Kemper, Tim Kraska, and Thomas\nNeumann. Sosd: A benchmark for learned indexes.\nNeurIPS Workshop on Machine Learning for Systems ,\n2019.\n[9] Tim Kraska, Alex Beutel, Ed H. Chi, Je\u000brey Dean,\nand Neoklis Polyzotis. The case for learned index\nstructures. In Proceedings of the 2018 International\nConference on Management of Data , SIGMOD '18,\npage 489{504, New York, NY, USA, 2018. Association\nfor Computing Machinery.\n[10] Gurmeet Singh Manku, Sridhar Rajagopalan, and\nBruce G Lindsay. Approximate medians and other\nquantiles in one pass and with limited memory. ACM\nSIGMOD Record , 27(2):426{435, 1998.\n[11] Ryan Marcus, Andreas Kipf, Alexander van Renen,\nMihail Stoian, Sanchit Misra, Alfons Kemper, Thomas\nNeumann, and Tim Kraska. Benchmarking learned\nindexes. Proc. VLDB Endow. , 14(1):1{13, 2020.\n[12] Michael Mitzenmacher and Sergei Vassilvitskii. Al-\ngorithms with Predictions , page 646{662. Cambridge\nUniversity Press, 2021.\n[13] Dan Morton. digestible: A modern c++ implemen-\ntation of a merging t-digest data structure. https:\n//github.com/SpirentOrion/digestible .\n[14] Lu Wang, Ge Luo, Ke Yi, and Graham Cormode.\nQuantiles over data streams: An experimental study.\nInProceedings of the 2013 ACM SIGMOD Interna-\ntional Conference on Management of Data , SIGMOD\n'13, page 737{748, New York, NY, USA, 2013. Associ-\nation for Computing Machinery.\nCopyright ©2023\nCopyright for this paper is retained by authors",
  "textLength": 36438
}