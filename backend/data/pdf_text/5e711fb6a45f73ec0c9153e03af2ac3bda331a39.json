{
  "paperId": "5e711fb6a45f73ec0c9153e03af2ac3bda331a39",
  "title": "Candidate Generation with Binary Codes for Large-Scale Top-N Recommendation",
  "pdfPath": "5e711fb6a45f73ec0c9153e03af2ac3bda331a39.pdf",
  "text": "Candidate Generation with Binary Codes for Large-Scale Top-N\nRecommendation\nWang-Cheng Kang\nUniversity of California, San Diego\nLa Jolla, CA, USA\nwckang@ucsd.eduJulian McAuley\nUniversity of California, San Diego\nLa Jolla, CA, USA\njmcauley@ucsd.edu\nABSTRACT\nGenerating the Top-N recommendations from a large corpus is com-\nputationally expensive to perform at scale. Candidate generation\nand re-ranking based approaches are often adopted in industrial\nsettings to alleviate efficiency problems. However it remains to\nbe fully studied how well such schemes approximate complete\nrankings (or how many candidates are required to achieve a good\napproximation), or to develop systematic approaches to generate\nhigh-quality candidates efficiently. In this paper, we seek to in-\nvestigate these questions via proposing a candidate generation\nandre-ranking based framework (CIGAR), which first learns a\npreference-preserving binary embedding for building a hash table\nto retrieve candidates, and then learns to re-rank the candidates\nusing real-valued ranking models with a candidate-oriented ob-\njective. We perform a comprehensive study on several large-scale\nreal-world datasets consisting of millions of users/items and hun-\ndreds of millions of interactions. Our results show that CIGAR\nsignificantly boosts the Top-N accuracy against state-of-the-art\nrecommendation models, while reducing the query time by orders\nof magnitude. We hope that this work could draw more attention\nto the candidate generation problem in recommender systems.\nACM Reference Format:\nWang-Cheng Kang and Julian McAuley. 2019. Candidate Generation with\nBinary Codes for Large-Scale Top-N Recommendation. In The 28th ACM\nInternational Conference on Information and Knowledge Management (CIKM\n’19), November 3–7, 2019, Beijing, China. ACM, New York, NY, USA, Article 4,\n10 pages. https://doi.org/10.1145/3357384.3357930\n1 INTRODUCTION\nTop-N recommendation is a fundamental task of a recommender\nsystem, which consists of generating a (short) list of N items that\nare highly likely to be interacted with (e.g. purchased, liked, etc.) by\nusers. Precisely identifying these Top-N items from a large corpus\nis highly challenging, both from an accuracy and efficiency perspec-\ntive. The vast number of items, both in terms of their variability and\nsparsity, makes the problem especially difficult when scaling up to\nreal-world datasets. In particular, exhaustively searching through\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nCIKM ’19, November 3–7, 2019, Beijing, China\n©2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-6976-3/19/11. . . $15.00\nhttps://doi.org/10.1145/3357384.3357930\n0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 \n1 1 1 1 … Hash  \nTable \nRanking Model \n(e.g. MF, CML, NeuMF) (1) Generate Candidates \n(2) Re-rank \n(3) Recommend user \nbinary \ncodes item \nbinary \ncodes \n#1 #2 #3 … \nFigure 1: A simplified illustration showing the candidate\ngeneration and re-ranking procedures in our CIGAR frame-\nwork. The binary codes and ranking model are both learned\nfrom user feedback.\nall items to generate the Top-N ranking becomes intractable at scale\ndue to its high latency.\nRecommender systems have received significant attention with\nvarious models being proposed, though generally focused on the\ngoal of achieving better accuracy [ 4,8,16,36,38]. For example, BPR-\nMF [ 32] adopts a conventional Matrix Factorization (MF) approach\nas its underlying preference model, CML [ 15] employs metric em-\nbeddings, TransRec [ 12] adopts translating vectors, and NeuMF [ 13]\nuses multi-layer perceptrons (MLP) to model user-item interactions.\nAs for the problem of latency/efficiency, a few works seek to\naccelerate the maximum inner product (MIP) search step (for MF-\nbased models), via pruning or tree-based data structures [ 21,31].\nSuch approaches are usually model-specific (e.g. they depend on\nthe specific structure of an inner-product space), and thus are hard\nto generalize when trying to accelerate other models. Another line\nof work seeks to directly learn binary codes to estimate user-item\ninteractions, and builds hash tables to accelerate retrieval time [ 24,\n27,44,46–48]. While using binary codes can significantly reduce\nquery time to constant or sublinear complexity, the accuracy of such\nmodels is still inferior to conventional (i.e., real-valued) models,\nas such models are highly constrained, and may lack sufficient\nflexibility when aiming to precisely rank the Top-N items.\nAs the vast majority of items will be irrelevant to most users at\nany given moment, candidate generation and re-ranking strategies\nhave been adopted in industry where high efficiency is required.\nSuch approaches first generate a small number of candidates in an\nefficient way, and then apply fine-grained re-ranking methods to\nobtain the final ranking. To achieve high efficiency, the candidate\n1arXiv:1909.05475v1  [cs.IR]  12 Sep 2019\n\nCIKM ’19, November 3–7, 2019, Beijing, China Wang-Cheng Kang and Julian McAuley\ngeneration stage is often based on rules or heuristics. For example,\nYoutube’s early recommender system treated users’ recent actions\nas seeds, and searched among relevant videos in the co-visitation\ngraphs with a heuristic relevance score [ 7].Pinterest performs ran-\ndom walks (again using recent actions as seeds) on the pin-board\ngraph to retrieve relevant candidates [ 10], and also considers other\ncandidate sources based on various signals like annotations, content,\netc. [ 26]. Recently, Youtube adopted deep neural networks (DNNs)\nto extract user embeddings from various features, and used an inner\nproduct function to estimate scores, such that candidate generation\ncan be accelerated by maximum inner product search [5].\nIn this paper, we propose a novel candidate generation andre-\nranking based framework called CIGAR . During the candidate gen-\neration stage, unlike existing work that adopts heuristics, or learns\nreal-valued embeddings first and then adopts indexing techniques\nto accelerate, we propose to directly learn binary codes for both\npreference ranking and hash table lookup. During the re-ranking\nstage, we learn to re-rank candidates using existing ranking models\nwith candidate-oriented sampling strategies. Figure 1 shows the\nprocedure of generating recommendations using CIGAR.\nOur main contributions are as follows:\n•We propose a novel framework ( CIGAR ) which learns to gener-\nate candidates with binary codes, and re-ranks candidates with\nreal-valued models. CIGAR thus exhibits both the efficiency of\nhashing and the accuracy of real-valued methods: binary codes\nare employed to estimate coarse-grained preference scores and\nefficiently retrieve candidates, while real-valued models are\nused for fine-grained re-ranking of a small number of candi-\ndates.\n•We propose a new hashing-based method— HashRec —for\nlearning binary codes with implicit feedback. HashRec is opti-\nmized via stochastic gradient descent, and can easily scale to\nlarge datasets. CIGAR adopts HashRec for fast candidate gener-\nation, as empirical results show that HashRec achieves superior\nperformance compared to other hashing-based methods.\n•We propose a candidate-oriented sampling strategy which en-\ncourages the models to focus on re-ranking candidates, rather\nthan treating all items equally. With such a sampling scheme,\nCIGAR can significantly boost the accuracy of various exiting\nranking models, including neural-based approaches.\n•Comprehensive experiments are conducted on several large-\nscale datasets. We find that CIGAR outperforms the existing\nstate-of-the-art models, including those that rank all items,\nwhile reducing the query time by orders of magnitude. Our\nresults suggest that it is possible to achieve similar or better\nperformance than existing approaches even when using only a\nsmall number of candidates.\n2 BACKGROUND\nIn this section, we briefly review relevant background including\nrepresentative ranking models for implicit feedback, and hashing-\nbased models for efficient recommendation.\n2.1 Preference Ranking Models\n2.1.1 Recommendation with Implicit Feedback. In our paper, we fo-\ncus on learning user preferences from implicit feedback (e.g. clicks,purchases, etc.). Specifically, we are given a user set Uand an\nitem setI, such that the set I+urepresents the items that user u\nhas interacted with, while I−u=I−I+urepresents unobserved\ninteractions. Unobserved interactions are not necessarily negative,\nrather for the majority of such items the user may simply be un-\naware of them. To interpret such in-actions, weighted loss [ 17] and\nlearning-to-rank [32] approaches have been proposed.\n2.1.2 Bayesian Personalized Ranking (BPR). BPR [ 32] is a classic\napproach for learning preference ranking models from implicit feed-\nback. The core idea is to rank observed actions (items) higher than\nunobserved items. BPR-MF is a popular variant that adopts con-\nventional Matrix Factorization (MF) approaches as its underlying\npreference estimator:\nsu,i=⟨pu,qi⟩, (1)\nwhere pu,qiarek-dimensional embeddings. BPR seeks to optimize\npairwise rankings by minimizing a contrastive objective:\n−Õ\n(u,i,j)∈Dlnσ(su,i−su,j)\nD={(u,i,j)|u∈U∧ i∈I+∧j∈I−}.(2)\nAs enumerating all triplets in Dis typically intractable, BPR-MF\nadopts stochastic gradient descent (SGD) to optimize the model.\nNamely, in each step of SGD, we dynamically sample a batch of\ntriplets fromD. Also, an ℓ2regularization on user and item embed-\ndings is adopted, which is crucial to alleviate overfitting.\n2.1.3 Collaborative Metric Learning (CML). Conventional MF-\nbased methods operate in inner product spaces, which are flexible\nbut can easily overfit. To this end, CML [ 15] imposes the triangle\ninequality constraint, by adopting metric embeddings to represent\nusers and items. Here the preference score is estimated by the\nnegative ℓ2distance:\nsu,i=−∥pu−qi∥2. (3)\nCML adopts a hinge loss to optimize pairwise rankings. A significant\nbenefit of CML is that retrieval can be accelerated by efficient\nnearest neighbor search, which has been heavily studied.\n2.1.4 Neural Matrix Factorization (NeuMF). To estimate more com-\nplex and non-linear preference scores, NeuMF [ 13] adopts multi-\nlayer perceptrons for modeling interactions:\nsu,i=wT\"\np(1)\nu⊙q(1)\ni\nMLP(p(2)\nu,q(2)\ni)#\n, (4)\nwhere⊙is the element-wise product, ‘MLP’ extracts a vector from\nuser and item embeddings, and wis used to project the concatenated\nvector to the final score. Essentially NeuMF combines generalized\nmatrix factorization (GMF) and MLPs. Due to the complexity of the\nscoring function, the retrieval process is generally hard to accelerate\nfor NeuMF.\n2.2 Hashing-based Recommendation\nTo achieve efficient recommendation, various hashing-based models\nhave been proposed. These methods use binary representations to\nrepresent users and items, and the retrieval time can be reduced to\nconstant or sublinear time by appropriate use of a hash table. We\n2\n\nCandidate Generation with Binary Codes for Large-Scale Top-N Recommendation CIKM ’19, November 3–7, 2019, Beijing, China\nTable 1: Notation.\nNotation Description\nU,I user and item set\nr∈N binary embedding length (#bits)\nk∈N real-valued embedding size\nc∈N number of candidates for re-ranking\nebu,edi∈Rrauxiliary embeddings for user uand item i\nbu,di∈{− 1,1}rbinary embeddings for user uand item i\npu,qi∈Rkreal embeddings for user uand item i\nm∈N the number of substrings in MIH\nh∈R the sampling ratio in eq. 10\nbriefly introduce the Hamming Space and two relevant hashing-\nbased recommendation method.\n2.2.1 Hamming Space. A Hamming space contains 2rbinary\nstrings with length r. Binary codes can be efficiently stored and\ncomputed in modern systems. In this paper we use binary codes\nbu,di∈{− 1,1}rto represent users and items.1The negative Ham-\nming distance measures the similarity between two binary strings:\nsH(bu,di)=rÕ\nz=1I(bu,z=di,z)\n=1\n2 rÕ\nz=1I(bu,z=di,z)+r−rÕ\nz=1I(bu,z,di,z)!\n=const +1\n2⟨bu,di⟩,(5)\nwhere I(·)is the indicator function. This provides a convenient way\nto formulate the problem with the inner product.\n2.2.2 Discrete Collaborative Filtering (DCF). DCF [ 44] is a repre-\nsentative method that estimates observed ratings (scaled to [−r,r])\nusing⟨bu,di⟩. Additional constraints of bit balance and bit uncor-\nrelation are adopted to learn efficient binary codes. DCF introduces\nreal-valued auxiliary variables, and adopts an optimization strategy\nconsisting of alternating sub-problems with closed-form solutions.\n2.2.3 Discrete Personalized Ranking (DPR). To our knowledge,\nDPR [ 46] is the only hashing-based method designed for implicit\nfeedback. DPR considers triplets Das in BPR, and optimizes\nrankings using a squared loss. DPR also optimizes sub-problems\nwith closed-form solutions. However, the solutions to these sub-\nproblems rely on computing all triplets in D, which makes opti-\nmization hard to scale to large datasets.\n3 CIGAR: LEARNING TO GENERATE\nCANDIDATES AND RE-RANK\nIn this section, we introduce CIGAR, a candidate generation and re-\nranking based framework. We propose a new method HashRec that\nlearns binary embeddings for users and items. CIGAR leverages the\nbinary codes generated by HashRec, to construct hash tables for fast\ncandidate generation. Finally, CIGAR learns to re-rank candidates\nvia real-valued ranking models with the proposed sampling strategy.\nOur notation is summarised in Table 1.\n1We use {-1,1} instead of {0,1} for convenience of formulations, though in practice we\ncan convert to binary codes (i.e., {0,1}) when storing them.\n3\n 2\n 1\n 0 1 2 3\n1\n01\nsgn(x)\nepoch 10\nepoch 2\nepoch 1(a)sgn(x) vs. tanh(βx)\n0 20 40 60 80 100\nepoch0.00.20.40.60.81.0loss\nL(eq. 7)\n/tildewideL(eq. 8)\n(b) Training loss\n0 20 40 60 80 100\nepoch0.00.20.40.60.81.0quantization loss (c) Quantization loss\nFigure 2: Training curves on MovieLens-20M. Figure (a) plots\nsgn(x)and its approximation tanh(βx). Figure (b) plots the de-\nsired loss Land surrogate loss eLthrough training. Figure (c)\nshows the quantization error (measured via mean squared\ndistances) between sgn(x)and tanh(βx).\n3.1 Learning Preference-preserving Binary\nCodes\nWe use binary codes bu,di∈{− 1,1}rto represent users and items,\nand estimate interactions between them via the Hamming distance.\nWe seek to learn preference-preserving binary codes such that\nsimilar binary codes (i.e., low Hamming distance) indicate high\npreference scores. For convenience, we use the conventional inner\nproduct (the connection is shown in eq. 5) in our formulation:\nsu,i=⟨bu,di⟩. (6)\nIn implicit feedback settings, we seek to rank observed interac-\ntions ( u,i) higher than unobserved interactions. To achieve this, we\nemploy the classic BPR [ 32] loss to learn our binary codes. However,\ndirectly optimizing such binary codes is generally NP-Hard [ 42].\nHence, we introduce auxiliary real-valued embeddings ebu,edi∈Rr\nas used by other learning-to-hash approaches [ 44]. Thus our objec-\ntive is equivalent to:\nL=−Õ\n(u,i,j)∈Dlnσα\u0010\n⟨sgn(ebu),sgn(edi)⟩−⟨ sgn(ebu),sgn(edj)⟩\u0011\n,\n(7)\nwhere sgn(x)=1ifx≥0(−1otherwise), and σα(x)=σ(αx)=\n1/(1+exp(−αx)). As the inner product between binary codes can\nbe large (i.e.,±r), we set α<1to reduce the saturation zone of the\nsigmoid function. Inspired by a recent study for image hashing [ 3,\n18], we seek to optimize the problem by approximating the sgn(·)\nfunction:\nsgn(x)=lim\nβ→∞tanh(βx),\nwhere βis a hyper-parameter that increases during training. With\nthis approximation, the objective becomes:\neL=−Õ\n(u,i,j)∈Dlnσα\u0010\n⟨tanh(βebu),tanh(βedi)⟩−\n⟨tanh(βebu),tanh(βedj)⟩\u0011\n.(8)\nAs shown in Figure 2, when we optimize the surrogate loss eL, the\ndesired loss Lis also minimized consistently. Also we can see that\nthe quantization loss (i.e., the mean squared distances between\nsgn(x)andtanh(βx)) drops significantly throughout the training\nprocess. Note that we also employ ℓ2-regularization on embeddings\nebuandedi, as in BPR.\nWe name this method HashRec ; the complete algorithm is given\nin Algorithm 1.\n3\n\nCIKM ’19, November 3–7, 2019, Beijing, China Wang-Cheng Kang and Julian McAuley\nAlgorithm 1 Optimization in HashRec\nInput: training dataD, code length r, regularization coefficient λ\nInitialize embeddings eB∈R|U|×randeD∈R|I|×r(at random)\nforepoch =1→num_epochs do\nβ←p\n10∗(epoch−1)\nforiter=1→num_iterations do\nSample a batch of triplets a from D\nOptimize loss (8) by updating ebuandediusing the Adam [ 19] opti-\nmizer\nObtain binary codes by bu←sgn(ebu)anddi←sgn(edi)\nOutput: B∈{− 1,1}|U|×r,D∈{− 1,1}|I|×r\n3.2 Building Multi-Index Hash Tables\nUsing binary codes to represent users and items can yield significant\nbenefits in terms of storage cost and retrieval speed. For example, in\nour experiments, HashRec achieves satisfactory accuracy with r=64\nbits, which is equivalent in space to only 4 single-precision floating-\npoint numbers (i.e., float16). Moreover, computing architectures are\namenable to calculating the Hamming distance of binary codes.2\nIn other words, performing exhaustive search with binary codes is\nmuch faster (albeit by a constant factor) compared to real-valued\nembeddings. However, using exhaustive search inevitably leads to\nlinear time complexity (in |I|), which still scales poorly.\nTo scale to large, real-world datasets, we seek to build hash tables\nto index all items according to their binary codes, such that we can\nperform hash table lookup to retrieve and recommend items for\na given user. Specifically, for a query code bu, we retrieve items\nfrom buckets within a small radius l(i.e., dH(bu,di)≤l). Hence\nthe returned items have low Hamming distances (i.e., high prefer-\nence scores) compared to the query codes, and search can be done\nin constant time. However, for large code lengths the number of\nbuckets grows exponentially, and furthermore such an approach\nmay return zero items as nearby buckets will frequently be empty\ndue to dataset sparsity.\nHence we employ Multi-Index Hashing (MIH) [ 30] as our index-\ning data structure. The core idea of MIH is to split binary codes to\nmsubstrings and index them by mhash tables. When we retrieve\nitems within Hamming radius l, we first retrieve items in each hash\ntable with radius⌊l\nm⌋, and then sort the retrieved items based on\ntheir Hamming distances with the full binary codes. It can be guar-\nanteed that such an approach can retrieve the desired items (i.e.,\nwithin Hamming radius l), and that the search time is sub-linear in\nthe number of items [30].\nSince we are interested in generating fixed-length (Top-N) rank-\nings, we seek to retrieve citems as candidates, instead of considering\nHamming radii. MIH proposes an adaptive solution that gradually\nincreases the radius luntil enough items are retrieved (i.e., at least\nc). Empirically we found that the query time of MIH is extremely\nfast and grows slowly with the number of items. The pseudo-code\nfor constructing and retrieving items in MIH, and more information\nabout this process is described in the appendix.\n2The Hamming distance can be efficiently calculated by two instructions: XOR and\nPOPCNT (count the number of bits set to 1).3.3 Candidate-oriented Re-ranking\nSo far we have learned preference-preserving binary codes for\nusers and items, and constructed hash tables to efficiently retrieve\nitems for users. However, as observed in previous hashing-based\nmethods, generating recommendations purely using binary codes\nleads to inferior accuracy compared with conventional real-valued\nranking models. To achieve satisfactory performance in terms of\nboth accuracy and efficiency, we propose to use the retrieved items\nas candidates, and adopt sophisticated ranking models to refine\nthe results. As the preference ranking problem has been heavily\nstudied [ 13,15,32], we employ existing models to study the effect of\nthe CIGAR framework, and propose a candidate-oriented sampling\nstrategy to further boost accuracy.\nA straightforward approach would be to adopt ‘off-the-shelf’\nranking models (e.g. BPR-MF) for re-ranking. However, we argue\nthat such an approach is sub-optimal as existing models are typi-\ncally trained to produce rankings for all items, while our re-ranking\nmodels only rank the cgenerated candidates. Moreover, the re-\ntrieved candidates are often ‘difficult’ items (i.e., items that are hard\nfor ranking models to discriminate) or at the very least are not a\nuniform sample of items. Hence, it might be better to train ranking\nmodels such that they are focused on the re-ranking objective. In\nthis section, we introduce our candidate-oriented sampling strategy,\nand show how to apply it to existing ranking models in general.\nThe loss functions of preference ranking models can generally\nbe described as3:\nminÕ\n(u,i,j)∈DL(u,i,j). (9)\nThe choice ofLis flexible, for instance, BPR uses lnσ(·), and CML\nadopts the margin loss. To make the model focus more on learning to\nre-rank the candidates, we propose a candidate-oriented sampling\nstrategy, which substitutes Dwith\nD+=\u001asample fromD,with probability 1−h\nsample fromC,with probability h, (10)\nwhereC={(u,i,j)|u∈U∧ i∈I+u∧j∈I−u∩Cu},Cucontains\nccandidates for user ugenerated by hashing, and hcontrols the\nprobability ratio. Note that the sampling is equivalent to assigning\nlarger weights to the candidates (for h>0). We empirically find that\nthe best performance is obtained with 0< h<1; when h=0, the model\nis not aware of the candidates that need to be ranked, while h=1\nmay lead to overfitting due to the limited number of samples.\nAs for constructing C, one approach is online generation, as we\ndid forD. Namely, in each step of SGD, we sample a batch of users\nand obtain candidates by hashtable lookup. Another approach is to\npre-compute and store all candidates. Both approaches are practical,\nthough we adopt the latter for better time efficiency.\nFinally, taking BPR-MF as an example, the candidate-oriented\nre-ranking model is trained with the following objective:\n−Õ\n(u,i,j)∈D+lnσ\u0000⟨pu,qi⟩−⟨pu,qj⟩\u0001. (11)\nWe denote this model as BPR-MF+, to distinguish against the vanilla\nmodel. CML+and NeuMF+(etc.) are denoted in the same way.\n3For pairwise learning based methods (e.g. NeuMF), we have L(u,i,j)=L+(u,i)+\nL−(u,j).\n4\n\nCandidate Generation with Binary Codes for Large-Scale Top-N Recommendation CIKM ’19, November 3–7, 2019, Beijing, China\n3.4 Summary\nTo summarize, the training process of CIGAR consists of: (1)\nLearning preference-preserving binary codes buanddiusing\nHashRec (Algorithm 1); (2) Constructing Multi-Index Hash tables\nto index all items; (3) Training a ranking model (e.g. BPR-MF+) for\nre-ranking candidates (i.e., using a candidate-oriented sampling\nstrategy). We adopt SGD to learn binary codes as well as our re-\nranking model, such that optimization easily scales to large datasets.\nDuring testing, for a given user u, we first retrieve ccandidates\nvia hashtable lookup, then we adopt a linear scan to calculate their\nscores estimated by the re-ranking model, and finally return the\nTop-N items with the largest scores. Using candidates generated by\nhash tables significantly reduces the time complexity from linear\n(i.e., exhaustive search) to sub-linear, and in practice is over 1,000\ntimes faster for the largest datasets in our experiments.\nFor hyper-parameter selection, by default, we set the number\nof candidates c=200, the number of bits r=64, the scaling factor\nα=10/r, and sampling ratio h=0.5. The ℓ2regularizer λis tuned via\ncross-validation. In MIH, the number of substrings mis manually\nset depending on dataset size. For example, we set m=4 for datasets\nwith millions of items. Further details are included in the appendix.\n4 EXPERIMENTS\nWe conduct comprehensive experiments to answer the following\nresearch questions:\nRQ1: Does CIGAR achieve similar or better Top-N accuracy com-\npared with state-of-the-art models that perform exhaustive\nrankings (i.e., which rank all items)?\nRQ2: Does CIGAR accelerate the retrieval time of inner-product-,\nmetric-, or neural-based models on large-scale datasets?\nRQ3: Does HashRec outperform alternative hashing-based ap-\nproaches? Do candidate-oriented sampling strategies\n(e.g. BPR-MF+) help?\nRQ4: What is the influence of key hyper-parameters in CIGAR?\nThe code and data processing script are available at https://\ngithub.com/kang205/CIGAR.\n4.1 Datasets\nWe evaluate our methods on four public benchmark datasets. A pro-\nprietary dataset from Flipkart is also employed to test the scalability\nof our approach on a representative industrial dataset. The datasets\nvary significantly in domains, sparsity, and number of users/items;\ndataset statistics are shown in Table 2. We consider the following\ndatasets:\n•MovieLens4A widely used benchmark dataset for evaluating\ncollaborative filtering algorithms [ 11]. We use the largest ver-\nsion that includes 20 million user ratings. We treat all ratings\nas implicit feedback instances (since we are trying to predict\nwhether users will interact with items, rather than their ratings).\n•Amazon5A series of datasets introduced in [ 29], including\nlarge corpora of product reviews crawled from Amazon.com .\nTop-level product categories on Amazon are treated as separate\n4https://grouplens.org/datasets/movielens/20m/\n5http://jmcauley.ucsd.edu/data/amazon/index.htmlTable 2: Dataset statistics (after preprocessing)\nDataset #Items #Users #Actions % Density\nMovieLens-20M 18K 138K 20M 0.81\nYelp 103K 244K 3.7M 0.015\nAmazon Books 368K 604K 8.9M 0.004\nGoodReads 1.6M 759K 167M 0.014\nFlipkart 2.9M 9.0M 274M 0.001\ndatasets, and we use the largest category ‘books.’ All reviews\nare treated as implicit feedback.\n•Yelp6Released by Yelp, containing various metadata about busi-\nnesses (e.g. location, category, opening hours) as well as user\nreviews. We use the Round-12 version, and regard all review\nactions as implicit feedback.\n•Goodreads.7A recently introduced large dataset containing\nbook metadata and user actions (e.g. shelve, read, rate) [ 39]. We\ntreat the most abundant action (‘shelve’) as implicit feedback.\nAs shown in [ 39], the dataset is dominated by a few popu-\nlar items (e.g. over 1/3 of users added Harry Potter #1 to their\nshelves), such that always recommending the most popular\nbooks achieves high Top-N accuracy; we ignore such outliers\nby discarding the 0.1% of most popular books.\n•Flipkart A large dataset of user sessions from Flipkart.com ,\na large online electronics and fashion retailer in India. The\nrecorded actions include ‘click, ’ ‘purchase, ’ and ‘add to wishlist. ’\nData was crawled over November 2018. We treat all actions as\nimplicit feedback.\nFor all datasets, we take the k-core of the graph to ensure that all\nusers and items have at least 5 interactions. Following [ 4,32], we\nadopt a leave-one-out strategy for data partitioning: for each user,\nwe randomly select two actions, put them into a validation set and\ntest set respectively, and use all remaining actions for training.\n4.2 Evaluation Protocol\nWe adopt two common Top-N metrics: Hit Rate (HR) and Mean Re-\nciprocal Rank (MRR), to evaluate recommendation performance [ 4,\n12]. HR@N counts the fraction of times that the single left-out item\n(i.e., the item in the test set) is ranked among the top N items, while\nMRR@N is a position-aware metric which assigns larger weights\nto higher positions (i.e., 1/ifor the i-th position). Note that since\nwe only have one test item for each user, HR@N is equivalent to\nRecall@N, and is proportional to Precision@N. Following [ 4], we\nset N to 10 by default.\n4.3 Baselines\nWe consider three representative recommendation models that es-\ntimate user-item preference scores with inner-products, Euclidean\ndistances, and neural networks:\n•Bayesian Personalized Ranking (BPR-MF) [32] A classic\nmodel that seeks to optimize a pairwise ranking loss. We employ\nMF as its preference predictor as shown in eq. 1. When recom-\nmending items, a maximum inner product search is needed.\n6https://www.yelp.com/dataset/challenge\n7https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/home\n5\n\nCIKM ’19, November 3–7, 2019, Beijing, China Wang-Cheng Kang and Julian McAuley\nTable 3: Recommendation performance. The best performing method in each row is boldfaced, and the second best method\nin each row is underlined. All the numbers are shown in percentage. - means the training fails due to lack of memory.\nDataset Metric(a-1)\nPOP(a-2)\nBPR-B(a-3)\nDCF(a-4)\nHashRecMetric(b-1)\nBPR-MF(b-2)\nCML(b-3)\nNeuMFCIGAR0\nHashRec+BPR-MFCIGAR\nHashRec+BPR-MF+Improv. %\nML-20MHR@10 8.15 11.85 6.90 15.72 HR@10 21.05 21.41 18.43 21.56 25.42 18.7\nHR@200 42.35 51.88 43.31 62.96 MRR@10 8.82 8.92 7.01 8.78 11.46 28.4\nYelpHR@10 1.03 0.21 1.30 1.39 HR@10 2.64 2.12 1.70 2.82 3.33 26.1\nHR@200 7.82 3.38 10.19 18.49 MRR@10 0.88 0.69 0.57 0.97 1.22 38.6\nAmazonHR@10 0.74 1.26 0.91 2.08 HR@10 4.17 3.47 1.59 3.73 4.56 9.4\nHR@200 5.14 6.56 6.45 11.69 MRR@10 1.73 1.41 0.67 1.91 2.23 28.9\nGoodReadsHR@10 0.06 0.44 0.40 1.19 HR@10 3.07 3.20 2.20 2.46 3.39 5.9\nHR@200 1.61 2.86 3.99 8.10 MRR@10 1.42 1.26 0.91 1.15 1.73 21.8\nFlipkartHR@10 0.05 0.34 - 0.92 HR@10 2.68 0.81 - 1.68 2.74 2.2\nHR@200 1.24 1.92 - 6.13 MRR@10 1.03 0.30 - 0.64 1.19 13.6\n0 100 200 300 400 5000.000.050.100.150.200.250.30HashRec+BPR-MF+(CIGAR) HashRec+BPR-MF (CIGAR0) All items+BPR-MF DCF+BPR-MF POP+BPR-MF RAND+BPR-MF\n1050100 200 300 400 500\n#Candidate Items0.000.050.100.150.200.250.30\n(a) ML-20M\n1050100 200 300 400 500\n#Candidate Items0.0000.0050.0100.0150.0200.0250.0300.035 (b) Yelp\n1050100 200 300 400 500\n#Candidate Items0.000.010.020.030.040.05 (c) Amazon Books\n1050100 200 300 400 500\n#Candidate Items0.0000.0050.0100.0150.0200.0250.0300.035 (d) Goodreads\n1050100 200 300 400 500\n#Candidate Items0.0000.0050.0100.0150.0200.0250.0300.035 (e) Flipkart\nFigure 3: Effect of the number of candidates.\n•Collaborative Metric Learning (CML) [15] CML represents\nusers and items in a metric space, and measures their com-\npatibility via the Euclidean distance (as shown in eq. 3). The\nrecommended items for a user can be retrieved via nearest\nneighbor search.\n•Neural Matrix Factorization (NeuMF) [13] NeuMF models\nnon-linear interactions between user and item embeddings via\na multi-layer perceptron (MLP). A generalized matrix factoriza-\ntion (GMF) term is also included in the model as in eq. 4.\nWe also compare HashRec with various hashing-based recom-\nmendation approaches:\n•POP A naïve popularity-oriented baseline that simply ranks\nitems by their global popularity.\n•BPR-B A simple baseline that directly quantizes embeddings\nfrom BPR-MF (i.e., applying sgn (x)to the embeddings).\n•Discrete Collaborative Filtering (DCF) [44] DCF learns bi-\nnary embeddings to estimate observed ratings. To adapt it to the\nimplicit feedback setting, we treat all actions as having value\n1 and randomly sample 100 unobserved items (for each user)\nwith value 0.\n•Discrete Personalized Ranking (DPR) [46] DPR is a\nhashing-based method designed for optimizing ranking with\nimplicit feedback. However, due to its high training complexity,\nwe only compare against this approach on MovieLens-1M.\nThe comparison against other hashing-based methods is omitted,\nas they are either content-based [ 24,27,48], designed for explicit\nfeedback [47], or outperformed by our baselines [28].Finally, our candidate generation and re-ranking based frame-\nwork CIGAR , which first retrieves c=200candidates from the\nMulti-Index Hash (MIH) table, and adopts ranking models to re-\nrank the candidates to obtain final recommendations. By default,\nCIGAR employs HashRec to learn binary user/item embeddings,\nand BPR-MF+as the re-ranking model. The effect of CIGAR with\ndifferent candidate generation methods, re-ranking models, and\nhyper-parameters are also studied in the experiments. More details\non hyperparameter tuning are included in the appendix.\n4.4 Recommendation Performance\nTable 3 shows Top-N recommendation accuracy on all datasets.\nColumns (a-1) to (a-4) contain ‘efficient’ recommendation methods\n(i.e., based on popularity or hashing), while (b-1) to (b-3) represent\nreal-valued ranking models. For hashing-based methods, we use\nHR@200 to evaluate the performance of candidate generation (i.e.,\nwhether the desired item appears in the 200 candidates).\nNot surprisingly, there is a clear gap between hashing-based\nmethods and real-valued methods in terms of HR@10, which con-\nfirms that using binary embeddings alone makes it difficult to iden-\ntify the fine-grained Top-10 ranking due to the compactness of\nthe binary representations. However, we find that the HR@200 of\nHashRec (and DCF) is significantly higher than the HR@10 of (b-1)\nto (b-3), which suggests the potential of using hashing-based meth-\nods to generate coarse-grained candidates, as the HR@200 during\nthe candidate generation stage is an upper bound for the Top-10\nperformance (e.g. if we have a perfect re-ranking model, the HR@10\nwould be equal to the HR@200) using the CIGAR framework.\n6\n\nCandidate Generation with Binary Codes for Large-Scale Top-N Recommendation CIKM ’19, November 3–7, 2019, Beijing, China\nTable 3 shows that HashRec significantly outperforms hashing-\nbased baselines, presumably due to the tanh(·)approximation and\nthe use of the advanced Adam optimizer. Hence, we choose HashRec\nas the candidate generation method in CIGAR by default. Finally, we\nsee that CIGAR (with HashRec and BPR-MF+) outperforms state-of-\nthe-art recommendation approaches. Note that CIGAR only ranks\n200 candidates (generated by HashRec), while BPR-MF, CML and\nNeuMF rank all items to obtain the Top-10 results. This suggests\nthat only considering a small number of high-quality candidates is\nsufficient to achieve satisfactory performance.\nComparison against DPR We perform a comparison with\nDPR [ 46] on the smaller dataset MovieLens-1M as the DPR is hard\nto scale to other datasets we considered. Following the same data fil-\ntering, partitioning, and evaluation scheme, DPR achives an HR@10\nof 8.9%, HR@200 of 55.7%. In comparison, HashRec’s HR@10 is\n13.5%, and HR@200 is 64.6%. This shows that HashRec outperforms\nDPR which is also designed for implicit recommendation.\n4.5 Effect of the Number of Candidates\nFigure 3 shows the HR@10 of various approaches with differ-\nent numbers of candidates. We can observe the effect of differ-\nent candidate generation methods by comparing HashRec, DCF,\nPOP, and RAND with a fixed ranking approach (BPR-MF). CIGAR0\n(HashRec+BPR-MF) clearly outperforms alternate approaches, and\nachieves satisfactory performance (similar to All Items+BPR-MF)\non the first three datasets. For larger datasets, more bits and more\ncandidates might be helpful to boost the performance (see sec. 4.8).\nHowever, CIGAR0 merely approximates the performance of All\nItems+BPR-MF. As we pointed out in section 3.3, this approach\nis suboptimal as the vanilla BPR-MF is trained to rank all items,\nwhereas we need to rank a small number of ‘hard’ candidates in the\nCIGAR framework. By adopting the candidate-oriented sampling\nstrategy to train a BPR-MF model focusing on ranking candidates,\nwe see that CIGAR achieves significant improvements over All\nItems+BPR-MF. This confirms that the proposed candidate-oriented\nre-ranking strategy is crucial in helping CIGAR to achieve better\nperformance than the original ranking model.\nNote that CIGAR is trained to re-rank the c=200 candidates. This\nmay cause the performance drop when ranking more candidates\non small datasets like ML-20M andYelp.\n4.6 Effects of Candidate-oriented Re-ranking\nIn previous sections, we have shown the performance of CIGAR\nwith different candidate generation methods (e.g. HashRec, DCF,\nPOP). Since CIGAR is a general framework, in this section we\nexamine the performance of CIGAR using CML and NeuMF as the\nre-ranking model (BPR-MF is omitted here, as results are included\nin Figure 3), so as to investigate whether the candidate-oriented\nsampling strategy is helpful in general.\nTable 4 lists the performance (HR@10) of CIGAR using CML\nand NeuMF as its ranking model. Due to the high quality of the\n200 candidates, HashRec+CML and HashRec+NeuMF can achieve\ncomparable performance compared to rank all items with the same\nmodel. Moreover, we can consistently boost the perfomance via re-\ntraining the model with the candidate-oriented sampling strategy\n(i.e., CML+and NeuMF+), which shows the mixed sampling is thekey factor for outperforming the vanilla models with exhaustive\nsearches (refer to section 4.8 for more analysis).\nTable 4: Effects of the candidate-oriented re-ranking sam-\npling with different ranking models. ↑indicates better per-\nformance than ranking all items with the same model.\nApproach ML-20M Yelp Amazon Goodreads\nAll items + CML 21.41 2.12 3.47 3.20\nHashRec + CML 21.27 2.33 ↑3.34 2.90\nHashRec + CML+23.62↑ 3.19↑4.22↑ 3.31↑\nAll items + NeuMF 18.43 1.70 1.59 2.20\nHashRec + NeuMF 18.29 2.17 ↑2.70↑ 1.96\nHashRec + NeuMF+20.83↑ 2.37↑2.78↑ 2.74↑\n4.7 Recommendation Efficiency\nEfficiently retrieving the Top-N recommended items for users is im-\nportant in real-world, interactive recommender systems. In Table 5,\nwe compare CIGAR with alternative retrieval approaches for dif-\nferent ranking models. For all ranking models, a linear scan can be\nadopted to retrieve the top-10 items. BPR-MF is based on inner prod-\nucts, hence we adopt the MIP (Maximum Innder Product) Tree [ 31]\nto accelerate search speed. As CML requires a nearest neighbor\nsearch in a Euclidean space, we employ the classic KD-Tree and\nBall Tree for retrieval. Since NeuMF utilizes neural networks to\nestimate preference scores, we use a GPU to accelerate the scan.\nTable 5: Running times for recommending the Top-10 items\nto 1,000 users.\nModelRetrieval\nApproachWall Clock Time(s)\nYelp Amazon Goodreads Flipkart\n#Items 0.1M 0.4M 1.6M 2.9M\nBPR-MFLinear Scan 109.0 375.7 1623.6 3076.4\nMIP Tree [31] 52.8 559.5 26.5 300.8\nCIGAR 1.2 1.5 1.6 1.9\nCMLLinear Scan 375.8 1439.9 5972.5 12367.1\nKD Tree 18.2 40.4 162.2 169.1\nBall Tree 15.4 46.3 210.7 227.8\nCIGAR 1.7 2.0 2.1 2.3\nNeuMFParallel Scan (GPU) 21.4 76.1 332.4 -\nCIGAR 1.8 2.1 2.3 -\nOn the largest dataset (Flipkart), CIGAR is at least 1,000 times\nfaster than linear scan for all models, and around 100 times faster\nthan tree-based methods. Furthermore, compared to other methods\nthe retrieval time of CIGAR increases very slowly with the number\nof items. Taking CML as an example, from Yelp to Flipkart, the query\ntime for linear scan, KD Tree, and CIGAR increases by around 30x,\n9x, and 1.4x (respectively). The fast retrieval speed of CIGAR is\nmainly due to the efficiency of hashtable lookup, and the small\nnumber of high-quality candidates for re-ranking.\nUnlike KD-Trees, which are specifically designed for accelerating\nsearch in models based in Euclidean spaces, CIGAR is a model-\nagnostic approach that can efficiently accelerate the retrieval time\nof almost any ranking model, including neural models. Meanwhile,\nas shown previously, CIGAR can achieve better accuracy compared\n7\n\nCIKM ’19, November 3–7, 2019, Beijing, China Wang-Cheng Kang and Julian McAuley\nwith models that rank all items. We note that MIP Tree performs\nextremely well on Goodreads. One possible reason is that a few\nlearned vectors have large length8, and hence the MIP Tree can\nquickly rule out most items.\nOur approach is also efficient for training, for example, the\nwhole training process of HashRec + BPR-MF+on the largest public\ndataset Goodreads can be finished in 3 hours (CPU only).\n4.8 Hyper-parameter Study\nIn Figure 4, we show the effects of two important hyper-parameters:\nthe number of bits rused in HashRec and the sample ratio hfor\nlearning the re-ranking model. For the number of bits, we can\nclearly observe that more bits leads to better performance. The\nimprovement is generally more significant on larger datasets. For\nthe sample ratio h, the best value is around 0.5 for most datasets,\nthus we choose h=0.5 by default. When h=1.0, the model seems\nto overfit due to limited data (i.e., a small number of candidates),\nand performance degrades. When h=0, the model is reduced to the\noriginal version which uniformly samples across all items. This\nagain verifies the effect of the proposed candidate-oriented sam-\npling strategy, as it significantly boosts performance compared to\nuniform sampling.\n16 32 64 128\n#bits50\n050100150200Improvement of HR@200 (%)Ml-20M Yelp Amazon Goodreads\n16 32 64 128\n#bits r050100150200250Improvement of HR@200 (%)\n(a) #bits r\n0.00 0.25 0.50 0.75 1.00\nsampling ratio h0102030405060Improvement of HR@10 (%) (b) Sampling ratio h\nFigure 4: Effects of the Hyper-parameters. (a) improvement\nof HashRec with more than 16 bits; (b) performance with\ndifferent sampling ratios.\n5 RELATED WORK\nEfficient Collaborative Filtering Hashing-based methods have\npreviously been adopted to accelerate retrieval for recommenda-\ntion. Early methods adopt ‘two-step’ approaches, which first solve\na relaxed optimization problem (i.e., binary codes are relaxed to\nreal values), and obtain binary codes by quantization [ 28]. Such\napproaches often lead to a large quantization error, as learned em-\nbeddings in the first stage may be not ideal for quantization. To this\nend, recent methods jointly optimize the recommendation problem\n(e.g. rating prediction) and the quantization loss [ 44,46], leading\nto better performance compared to two-step methods. For com-\nparison, we highlight the main differences between our method\nand existing methods (e.g. DCF[ 44] and DPR [ 46]) as follows: (1)\nexisting models are often optimized by closed-form solutions in-\nvolving various matrix operations, whereas HashRec uses a more\nscalable SGD-based approach; (2) we do not impose bit balance or\ndecorrelation constraints as in DCF and DPR, however in practice\n8As shown in the appendix, the regularization coefficient is set to 0 for Goodreads,\nwhich may cause such a phenomenon.we did not observe any performance degradation in terms of accu-\nracy and efficiency; (3) we use tanh(βx)to gradually close the gap\nbetween binary codes and continuous embeddings during training,\nwhich shows effective empirical approximation. To the best of our\nknowledge, HashRec is the first scalable hashing-based method for\nimplicit recommendation.\nAnother line of work seeks to accelerate the retrieval process\nof existing models. For example, approximate nearest neighbor\n(ANN) search has been heavily studied [ 6,43], which can be used\nto accelerate metric-based models like CML [ 15]. Recently, a few\napproaches have sought to accelerate the maximum inner product\nsearch (MIPS) operation for inner product based models [ 21,31,35].\nHowever, these approaches are generally model-dependant, and\ncan not easily be adapted to other models. For example, it is difficult\nto accelerate search for models that use complex scoring functions\n(e.g. MLP-based models such as NeuMF [ 13]). In comparison, CIGAR\nis a model-agnostic approach that can generally expedite search\nwithin any ranking model, as we only require the ranking model\nto scan a short list of candidates.\nInspired by knowledge distillation [ 14], a recent approach seeks\nto learn compact models (i.e., with smaller embedding sizes) while\nmaintaining recommendation accuracy [ 37]. However, the retrieval\ncomplexity is still linear in the number of items.\nCandidate Generation and Re-ranking To build real-time\nrecommender systems, candidate generation has been adopted in\nindustrial settings like Youtube ’s video recomendation [ 5,7],Pin-\nterest ’s related pin recommendation [ 10,26],Linkedin ’s job recom-\nmendation [ 2], and Taobao ’s product recommendation [ 49]. Such\nindustrial approaches often adopt heuristic rules, similarity mea-\nsurements, and feature engineering specially designed for their own\nplatforms. A closer work to our approach is Youtube ’s method [ 5]\nwhich learns two models for candidate generation and re-ranking.\nThe scoring function in the candidate generation model is the inner\nproduct of user and item embeddings, and thus can be accelerated\nby maximum inner product search via hashing- or tree-based meth-\nods. In comparison, our candidate generation method (HashRec)\ndirectly learns binary codes for representing preferences as well as\nbuilding hash tables. To our knowledge, this is the first attempt to\nadopt hash code learning techniques for candidate generation.\nOther than recommender systems, candidate generation has also\nbeen adopted in document retrieval [1], and NLP tasks[45].\nLearning to Hash Unlike conventional data structures where\nhash functions might be designed (or learned) so as to reduce con-\nflicts [ 9,20], we consider similarity-preserving hashing that seeks\nto map high-dimensional dense vectors to a low-dimensional Ham-\nming space while preserving similarity. Such approaches are often\nused to accelerate approximate nearest neighbor (ANN) search and\nreduce storage cost. A representative example is Locality Sensitive\nHashing (LSH) [ 6] which uses random projections as the hash func-\ntions. A few seminal works [ 33,42] propose to learn hash functions\nfrom data, which is generally more effective than LSH. Recent work\nfocuses on improving the performance, (e.g.) by using better quan-\ntization strategies, and adopting DNNs as hash functions [ 40,41].\nSuch approaches have been adopted for fast retrieval of various\ncontent including images [ 23,34], documents [ 22], videos [ 25], and\nproducts (e.g. DCF [ 44] and HashRec). HashRec directly learns bi-\nnary codes for users and items, which is essentially a hash function\n8\n\nCandidate Generation with Binary Codes for Large-Scale Top-N Recommendation CIKM ’19, November 3–7, 2019, Beijing, China\nprojecting one-hot vectors into the Hamming Space. We plan to\nlearn hash functions (e.g. based on DNNs) to map user and item\nfeatures to binary codes as future work, such that we can adapt to\nnew users and items, which may alleviate cold-start problems.\n6 CONCLUSION\nWe presented new techniques for candidate generation, a criti-\ncal (but somewhat overlooked) subroutine in recommender sys-\ntems that seek to efficiently generate Top-N recommendations.\nWe sought to bridge the gap between two existing modalities of\nresearch: methods that advance the state-of-the-art for Top-N rec-\nommendation, but are generally inefficient when trying to produce\na final ranking; and methods based on binary codes, which are\nefficient in both time and space but fall short of the state-of-the-art\nin terms of ranking performance. In this paper, we developed a new\nmethod based on binary codes to handle the candidate generation\nstep, allowing existing state-of-the-art recommendation modules to\nbe adopted to refine the results. A second contribution was to show\nthat performance can further be improved by adapting these mod-\nules to be aware of the generated candidates at training time, using a\nsimple weighted sampling scheme. We showed experiments on sev-\neral large-scale datasets, where we observed orders-of-magnitude\nimprovements in ranking efficiency, while maintaining or improv-\ning upon state-of-the-art accuracy. Ultimately, this means that we\nhave a general-purpose framework that can improve the scalability\nof existing recommender systems at test time, and surprisingly does\nnotrequire that we trade off accuracy for speed.\nAcknowledgements. This work is partly supported by NSF\n#1750063. We thank Surender Kumar and Lucky Dhakad for their\nhelp preparing the Flipkart dataset.\nREFERENCES\n[1]Nima Asadi and Jimmy J. Lin. 2012. Fast candidate generation for two-phase\ndocument ranking: postings list intersection with bloom filters. In CIKM .\n[2]Fedor Borisyuk, Krishnaram Kenthapadi, David Stein, and Bo Zhao. 2016. CaS-\nMoS: A Framework for Learning Candidate Selection Models over Structured\nQueries and Documents. In SIGKDD .\n[3]Zhangjie Cao, Mingsheng Long, Jianmin Wang, and Philip S. Yu. 2017. HashNet:\nDeep Learning to Hash by Continuation. In ICCV .\n[4]Evangelia Christakopoulou and George Karypis. 2018. Local Latent Space Models\nfor Top-N Recommendation. In SIGKDD .\n[5]Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep Neural Networks for\nYouTube Recommendations. In RecSys .\n[6]Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S. Mirrokni. 2004. Locality-\nsensitive hashing scheme based on p-stable distributions. In Symposium on Com-\nputational Geometry .\n[7]James Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van Vleet,\nUllas Gargi, Sujoy Gupta, Yu He, Mike Lambert, Blake Livingston, and Dasarathi\nSampath. 2010. The YouTube video recommendation system. In RecSys .\n[8]Mukund Deshpande and George Karypis. 2004. Item-based top- Nrecommenda-\ntion algorithms. ACM TOIS (2004).\n[9]Martin Dietzfelbinger, Anna R. Karlin, Kurt Mehlhorn, Friedhelm Meyer auf der\nHeide, Hans Rohnert, and Robert Endre Tarjan. 1988. Dynamic Perfect Hashing:\nUpper and Lower Bounds. In FOCS .\n[10] Chantat Eksombatchai, Pranav Jindal, Jerry Zitao Liu, Yuchen Liu, Rahul Sharma,\nCharles Sugnet, Mark Ulrich, and Jure Leskovec. 2018. Pixie: A System for\nRecommending 3+ Billion Items to 200+ Million Users in Real-Time. In WWW .\n[11] F. Maxwell Harper and Joseph A. Konstan. 2016. The MovieLens Datasets: History\nand Context. TiiS(2016).\n[12] Ruining He, Wang-Cheng Kang, and Julian McAuley. 2017. Translation-based\nRecommendation. In RecSys .\n[13] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng\nChua. 2017. Neural Collaborative Filtering. In WWW .\n[14] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. 2015. Distilling the Knowl-\nedge in a Neural Network. CoRR abs/1503.02531 (2015). arXiv:1503.02531http://arxiv.org/abs/1503.02531\n[15] Cheng-Kang Hsieh, Longqi Yang, Yin Cui, Tsung-Yi Lin, Serge J. Belongie, and\nDeborah Estrin. 2017. Collaborative Metric Learning. In WWW .\n[16] Binbin Hu, Chuan Shi, Wayne Xin Zhao, and Philip S Yu. 2018. Leveraging\nmeta-path based context for top-n recommendation with a neural co-attention\nmodel. In SIGKDD .\n[17] Yifan Hu, Yehuda Koren, and Chris Volinsky. 2008. Collaborative filtering for\nimplicit feedback datasets. In ICDM .\n[18] Qing-Yuan Jiang and Wu-Jun Li. 2018. Asymmetric Deep Supervised Hashing.\nInAAAI .\n[19] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Opti-\nmization. In ICLR .\n[20] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In SIGMOD .\n[21] Hui Li, Tsz Nam Chan, Man Lung Yiu, and Nikos Mamoulis. 2017. FEXIPRO: Fast\nand Exact Inner Product Retrieval in Recommender Systems. In SIGMOD .\n[22] Hao Li, Wei Liu, and Heng Ji. 2014. Two-Stage Hashing for Fast Document\nRetrieval. In ACL.\n[23] Wu-Jun Li, Sheng Wang, and Wang-Cheng Kang. 2016. Feature Learning Based\nDeep Supervised Hashing with Pairwise Labels. In IJCAI .\n[24] Defu Lian, Rui Liu, Yong Ge, Kai Zheng, Xing Xie, and Longbing Cao. 2017.\nDiscrete Content-aware Matrix Factorization. In SIGKDD .\n[25] Venice Erin Liong, Jiwen Lu, Yap-Peng Tan, and Jie Zhou. 2017. Deep Video\nHashing. IEEE TMM (2017).\n[26] David C. Liu, Stephanie Rogers, Raymond Shiau, Dmitry Kislyuk, Kevin C. Ma,\nZhigang Zhong, Jenny Liu, and Yushi Jing. 2017. Related Pins at Pinterest: The\nEvolution of a Real-World Recommender System. In WWW .\n[27] Han Liu, Xiangnan He, Fuli Feng, Liqiang Nie, Rui Liu, and Hanwang Zhang.\n2018. Discrete Factorization Machines for Fast Feature-based Recommendation.\nInIJCAI .\n[28] Xianglong Liu, Junfeng He, Cheng Deng, and Bo Lang. 2014. Collaborative\nHashing. In CVPR .\n[29] J. J. McAuley, C. Targett, Q. Shi, and A. van den Hengel. 2015. Image-based\nrecommendations on styles and substitutes. In SIGIR .\n[30] Mohammad Norouzi, Ali Punjani, and David J. Fleet. 2014. Fast Exact Search in\nHamming Space With Multi-Index Hashing. IEEE TPAMI (2014).\n[31] Parikshit Ram and Alexander G. Gray. 2012. Maximum Inner-Product Search\nusing Tree Data-structures. In SIGKDD .\n[32] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n2009. BPR: Bayesian personalized ranking from implicit feedback. In UAI.\n[33] Ruslan Salakhutdinov and Geoffrey E. Hinton. 2009. Semantic hashing. Int. J.\nApprox. Reasoning (2009).\n[34] Fumin Shen, Chunhua Shen, Wei Liu, and Heng Tao Shen. 2015. Supervised\nDiscrete Hashing. In CVPR .\n[35] Anshumali Shrivastava and Ping Li. 2014. Asymmetric LSH (ALSH) for Sublinear\nTime Maximum Inner Product Search (MIPS). In NIPS .\n[36] Jiaxi Tang and Ke Wang. 2018. Personalized Top-N Sequential Recommendation\nvia Convolutional Sequence Embedding. In WSDM .\n[37] Jiaxi Tang and Ke Wang. 2018. Ranking Distillation: Learning Compact Ranking\nModels With High Performance for Recommender System. In SIGKDD .\n[38] Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2018. Latent Relational Metric\nLearning via Memory-based Attention for Collaborative Ranking. In WWW .\n[39] Mengting Wan and Julian McAuley. 2018. Item recommendation on monotonic\nbehavior chains. In RecSys .\n[40] Jun Wang, Wei Liu, Sanjiv Kumar, and Shih-Fu Chang. 2016. Learning to Hash\nfor Indexing Big Data - A Survey. Proc. IEEE (2016).\n[41] Jingdong Wang, Ting Zhang, Jingkuan Song, Nicu Sebe, and Heng Tao Shen.\n2018. A Survey on Learning to Hash. IEEE TPAMI (2018).\n[42] Yair Weiss, Antonio Torralba, and Robert Fergus. 2008. Spectral Hashing. In\nNIPS .\n[43] Peter N. Yianilos. 1993. Data Structures and Algorithms for Nearest Neighbor\nSearch in General Metric Spaces. In SODA .\n[44] Hanwang Zhang, Fumin Shen, Wei Liu, Xiangnan He, Huanbo Luan, and Tat-Seng\nChua. 2016. Discrete Collaborative Filtering. In SIGIR .\n[45] Longkai Zhang, Houfeng Wang, and Xu Sun. 2014. Coarse-grained Candidate\nGeneration and Fine-grained Re-ranking for Chinese Abbreviation Prediction. In\nEMNLP .\n[46] Yan Zhang, Defu Lian, and Guowu Yang. 2017. Discrete Personalized Ranking\nfor Fast Collaborative Filtering from Implicit Feedback. In AAAI .\n[47] Yan Zhang, Haoyu Wang, Defu Lian, Ivor W. Tsang, Hongzhi Yin, and Guowu\nYang. 2018. Discrete Ranking-based Matrix Factorization with Self-Paced Learn-\ning. In SIGKDD .\n[48] Yan Zhang, Hongzhi Yin, Zi Huang, Xingzhong Du, Guowu Yang, and Defu\nLian. 2018. Discrete Deep Learning for Fast Content-Aware Recommendation. In\nWSDM .\n[49] Han Zhu, Xiang Li, Pengye Zhang, Guozheng Li, Jie He, Han Li, and Kun Gai.\n2018. Learning Tree-based Deep Model for Recommender Systems. In SIGKDD .\n9\n\nCIKM ’19, November 3–7, 2019, Beijing, China Wang-Cheng Kang and Julian McAuley\nTable 6: Hyper-parameters\nDCF HashRec BPR-MF BPR-MF+CML NeuMF\nr α β r λ m k λ k λ k margin MLP Arch. k\nMl-20M\n640.001 0.001\n640.001 16\n500.0001\n500.01\n500.5\n[200,100,50,25] 25Yelp 0.0001 0.0001 0.1 8 0.0001 0.1 1.0\nAmazon 0.001 0.001 1.0 4 0.01 0.1 2.0\nGoodreads 0.001 0.001 0.01 4 0.0 0.0001 1.0\nFlipkart - - 1.0 4 0.001 0.01 2.0\nAppendix\nA IMPLEMENTATION DETAILS\nWe implemented HashRec, BPR-MF, CML, and NeuMF in Tensor-\nflow (version 1.12). For DCF and DPR, we use the MATLAB imple-\nmentation from the corresponding authors.9\nFor HashRec, BPR-MF, CML, and NeuMF, we use the Adam\noptimizer with learning rate 0.001 and a batch size of 10000. A\nmulti-processing sampler is used for accelerating data sampling.\nAll models are trained for a maximum of 100 epochs. We evaluate\nthe validation performance10every 10 epochs, and terminate the\ntraining if it doesn’t improve after 20 epochs. The bit length rfor\nall hashing-based methods is set to 64, and the embedding size k\nfor ranking models is set to 50.11\nWe use a validation set to search for the best hyper-parameters.\nTheℓ2regularizer λin HashRec, BPR-MF, and BPR-MF+is selected\nfrom {1, 0.1, 0.01, 0.001, 0.0001, 0}. For DCF, the user and item\nregularizers αandβare selected from {0.01,0.001,0.0001}, and we set\nα=βfor fair comparison as other methods use only one regularizer\nfor both users and items. For CML, the norm of metric embeddings\nis set to 1 following the paper [ 15], and the margin in the hinge\nloss is selected from {0.1, 0.5, 1.0, 2.0}. For NeuMF, we follow the\ndefault configurations [ 13] and a 3-layer pyramid MLP architecture\nis adopted. As NeuMF has two embeddings for GMF and MLP, we\nset the embedding size to 25 for each. For DPR, we use the default\nsetting on MovieLens-1M. DPR training on MovieLens-20M did\nnot terminate in 24 hours, hence we only compare against it on\nMovieLens-1M.\nFor CIGAR, on all datasets, the number of candidates c=200, the\nscaling factor α=10/r, sampling ratio h=0.5, and βis increased as\nshown in Algorithm 1. For multi-index hashing, the number of\nsub-tables mis set to {16,8,4} depending on dataset sizes.\nTable 6 shows the hyper-parameters we used for each model on\nall datasets.\nB EFFICIENCY TEST\nWe performed the efficiency test (i.e., Section 4.7) on a workstation\nwith a quad-core Intel i7-6700 CPU and a GTX-1080Ti GPU. The\nGPU is not used except for NeuMF. For MIP Tree [ 31], we use the\nimplementation from the authors,12and the leaf node size is set\nto 20 following the default setting. For KD-Trees and Ball Trees,\n9Code for DCF is available online: https://github.com/hanwangzhang/\nDiscrete-Collaborative-Filtering\n10HR@200 for hashing-based methods, and HR@10 for ranking models.\n11we searched among {10,30,50} and found that 50 works best for all models on all\ndatasets.\n12https://github.com/gamboviol/miptreewe adopt the implementation from the scikit-learn library.13A\npriority queue is employed for choosing the top-k items in linear\nscan, which costs O(|I|logk). The query time of CIGAR consists of\nobtaining 200 candidates from MIH, performing linear scan on the\ncandidates, and choosing the top-10 items. We assume the queries\nare independent, and process them sequentially.\nC MULTI-INDEX HASHING\nWe show the procedure of building multi-index hash tables in Al-\ngorithm 2. For search, we gradually increase the search radius l\nuntil we obtain enough candidates or reach the maximum radius\nlmax. Larger radii may retrieve more accurate neighbors but would\ncost more time, and we set lmax=1 in our experiments. The search\nprocedure is shown in Algorithm 3.\nAlgorithm 2 Building MIH\nInput: item binary codes di∈{0,1}ri∈I, the number of substrings\nm\nInitialize mhash tables H1,H2,. . . ,Hm, each containing 2r/mbuckets\nfori=1→|I| do\nSplit diintomsubstrings ( s1,s2,. . . ,sm)\nforj=1→mdo\nInsert item iinto the bucket sjinHj\nOutput: Hash Tables H=(H1,H2,. . . ,Hm)\nAlgorithm 3 Querying in MIH\nInput: Hash Tables H=(H1,H2,. . . ,Hm), query codes bu∈{0,1}r, max-\nimum radius lmax, number of candidates c\nS←∅\nSplit buinto substrings ( s1,s2,. . . ,sm)\nforl=0→lmaxdo\nforj=1→mdo\nforbucket bwith dH(b,sj)=ldo\nS←SÐ{items in bucket bof table Hj}\nif|S|≥cthen\nbreak\nif|S|≤cthen\nreturn S\nelse\nsort items in Saccording to their Hamming distances to bu, and form\na set S′with the cnearest items\nreturn S′\n13https://scikit-learn.org/stable/modules/neighbors.html\n10",
  "textLength": 59186
}