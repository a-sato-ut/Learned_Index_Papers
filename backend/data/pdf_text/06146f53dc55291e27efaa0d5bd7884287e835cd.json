{
  "paperId": "06146f53dc55291e27efaa0d5bd7884287e835cd",
  "title": "Primal-Dual Algorithms with Predictions for Online Bounded Allocation and Ad-Auctions Problems",
  "pdfPath": "06146f53dc55291e27efaa0d5bd7884287e835cd.pdf",
  "text": "Proceedings of Machine Learning Research vol 201:1–18, 2023 34th International Conference on Algorithmic Learning Theory\nPrimal-Dual Algorithms with Predictions\nfor Online Bounded Allocation and Ad-Auctions Problems\nKevi Enik ˝o ENIKO .KEVI @UNIV -GRENOBLE -ALPES .FR\nLIG, INRIA, University Grenoble-Alpes, France\nNguyễn Kim Thắng KIM-THANG .NGUYEN @UNIV -GRENOBLE -ALPES .FR\nLIG, INRIA, CNRS, Grenoble INP, University Grenoble-Alpes, France\nEditors: Shipra Agrawal and Francesco Orabona\nAbstract\nMatching problems have been widely studied in the research community, especially Ad-Auctions\nwith many applications ranging from network design to advertising. Following the various ad-\nvancements in machine learning, one natural question is whether classical algorithms can benefit\nfrom machine learning and obtain better-quality solutions. Even a small percentage of performance\nimprovement in matching problems could result in significant gains for the studied use cases. For\nexample, the network throughput or the revenue of Ad-Auctions can increase remarkably. This pa-\nper presents algorithms with machine learning predictions for the Online Bounded Allocation and\nthe Online Ad-Auctions problems. We constructed primal-dual algorithms that achieve competi-\ntive performance depending on the quality of the predictions. When the predictions are accurate,\nthe algorithms’ performance surpasses previous performance bounds, while when the predictions\nare misleading, the algorithms maintain standard worst-case performance guarantees. We provide\nsupporting experiments on generated data for our theoretical findings.\nKeywords: online algorithm, predictions, matching problems, primal-dual\n1. Introduction\nThe matching problem is fundamental in combinatorial optimization and operations research with\nwide applications from student admission to colleges through kidney exchange to Ad-Auctions. Mo-\ntivated by different markets, for example, advertising and labor markets, online bipartite matching\nhas been intensively studied. Online bipartite matching revolves around matching a set of items\n(impressions) to a set of agents (advertisers). Online items arrive over time, and at the arrival of\nan item, one needs to make an irrevocable decision on the assignment of the item to an agent. For\nunweighted bipartite graphs, Karp et al. (1990) gave an elegant algorithm - the RANKING algorithm\n- which always outputs a matching of size at least (1−1/e)times that of the optimal solution (Karp\net al., 1990; Birnbaum and Mathieu, 2008). The ratio of (1−1/e)is the best achievable competitive\nratio in the worst-case paradigm. Besides, for online matchings in edge-weighted bipartite graphs,\nno online algorithm is competitive1to maximize the total (edge-) weight of the output matching,\neven though this problem is well-motivated by advertising and AdWords markets (see, for example,\nFahrbach et al. (2020)).\nTo circumvent the limitations of the worst-case paradigm, several models have been proposed\n(Roughgarden, 2019, 2020). The motivation for one of these models is the spectacular advance\n1. except in relaxed models such as disposal-free, etc.\n©2023 K. Enik ˝o & N.K. Thắng.arXiv:2402.08701v1  [cs.DS]  13 Feb 2024\n\nALGORITHMS WITH PREDICTIONS FOR MATCHING PROBLEMS\nof machine learning (ML). In particular, the capability of ML methods to predict patterns of fu-\nture requests could provide valuable information for online algorithms. Lykouris and Vassilvtiskii\n(2018) formally introduces a general framework to incorporate ML predictions into algorithms to\nimprove the worst-case performance guarantees. Other researchers followed their work, studying\nonline algorithms with predictions (Mitzenmacher and Vassilvitskii, 2020) in a large spectrum of\nproblems, such as scheduling (Lattanzi et al., 2020; Mitzenmacher, 2020), caching (paging) (Lyk-\nouris and Vassilvtiskii, 2018; Rohatgi, 2020; Antoniadis et al., 2020) and ski rental (Gollapudi and\nPanigrahi, 2019; Kumar et al., 2018; Angelopoulos et al., 2020). In this paper, we study the design\nof algorithms with predictions for variants of online edge-weighted matching problems.\n1.1. Model and Problems\nWe consider a model (formally stated in Bamas et al. (2020)) in which, whenever a request is re-\nleased, an algorithm receives some predictions and can use these predictions to make decisions. The\ndesigned algorithms aim to outperform the best-known algorithms when the predictions are correct,\nbut maintain close worst-case guarantees when the predictions are incorrect. Specifically, we study\nthe following problems in this model.\nOnline Bounded Allocation. In this problem, there are nbuyers, each buyer 1≤i≤nhas a\nbudget Bi. Items arrive over time; upon the arrival of item j, the price bjof the item as well as the\nset of buyers Sj⊆ {1,2, . . . , n }who are interested in purchasing jare revealed. In the standard\nonline setting, one needs to irrevocably sell item jto some buyer in Sj(or sell it to no one) while\nrespecting the budget constraints of all buyers. In a learning-enhanced model with predictions,\neach item has a predicted buyer to whom the item should be sold. This additional information may\ncome from a learning procedure based on the data analysis of buyers and items. We consider the\nlearning procedure as a black box and treat the predictions as outputs of an oracle. Concretely, at\nthe arrival of item j, the oracle provides a predicted buyer pred (j)to whom the item jshould be\nsold (conventionally, pred (j) = 0 if the item should not be sold according to the prediction). The\nobjective is to maximize the revenue, which is the total price of sold items.\nOnline Ad-Auctions. This problem is the generalization of Online Bounded Allocation. In this\nsetting, items do not have a fixed price; instead, at the arrival of item j, each buyer iproposes a\nprice (or bid) bijfor each item j. (Buyers can decline to buy an item by bidding 0value.) We\nassume that the bids are significantly smaller than the buyers’ budgets, so bij≪Bi∀i, j. Similarly,\nat the arrival of item j, the algorithm receives the prediction pred (j)and decides to whom to sell\nthe item. The objective is again to maximize the revenue, which is the total price of sold items.\nLearning augmented algorithms. We aim to design algorithms that incorporate predictions to\nachieve performance beyond the worst-case bounds. The predictions in our paper can be learned\nefficiently in practice. Ad-Auctions (or Ad-words) are routinely run a hundred times per day by\ndifferent search engine companies. At the end of a period (a day, a week, a month), such companies\ncan infer a good matching from their data that better matches the items to the buyers to increase their\nrevenue. In other words, they can leverage large amounts of data to build machine learning models\nto predict the best buyer for an advertisement slot based on previous transactions. Such predictions\ncan be used in our algorithms.\nThe learning augmentation depends on the algorithm’s confidence in the prediction oracle. We\nrepresent the algorithm’s doubt in the prediction by a parameter η∈[0,1]and assume that ηis fixed\n2\n\nALGORITHMS WITH PREDICTIONS FOR MATCHING PROBLEMS\nduring the algorithm’s execution. Large ηvalues mean high doubt, while small ηvalues show good\nconfidence.\nLetA(I)be the objective value of the solution produced by algorithm Aon an instance I. Sim-\nilarly, let P(I)andO(I)be the objective values of the prediction oracle and the optimal solution,\nrespectively. When the prediction oracle provides an infeasible solution, P(I) = 0 . Given a con-\nfidence parameter η∈[0,1], we say that an algorithm Aisc(η)-consistent andr(η)-robust if for\nevery instance I,\nA(I)≥max{c(η)· P(I), r(η)· O(I)}.\nIdeally, we would like c(η)to tend to 1when ηapproaches 0, meaning that with high confidence,\nthe algorithm performs at least as well as the prediction. Additionally, we would like r(η)to tend to\nthe best guarantee as in the standard online setting (without predictions) when ηapproaches 1.\n1.2. Our Approach and Contributions\nGiven an algorithm Athat blindly follows the predictions and another algorithm B, for example,\nthe best-known algorithm without predictions, a natural question is whether we can derive a new\nand more efficient algorithm Cby taking the linear combination of the two original algorithms.\nDue to the linear combination, algorithm Ccan only maintain a consistency of O(1−η)for Online\nBounded Allocation (or Ad-Auctions), if the robustness is Ω(η)times the worst-case guarantee of\nalgorithm B. In this paper, we aim for more substantial and non-trivial guarantees.\nWe rely on the primal-dual approach to design learning augmented algorithms for both stud-\nied problems. The primal-dual method is an elegant and powerful algorithm design technique\n(Williamson and Shmoys, 2011), especially for online algorithms (Buchbinder and Naor, 2009a).\nTo unify previous ad-hoc approaches, Bamas et al. (2020) presented a primal-dual framework to de-\nsign online algorithms with predictions for covering problems with linear objective functions. How-\never, their approach is not applicable for problems with packing constraints, particularly matching\nproblems and their variants. In this paper, we present learning augmented online algorithms with\npredictions for the Online Bounded Allocation and the Online Ad-Auctions problems, answering\nsome open questions raised by Bamas et al. (2020).\nSpecifically, in Section 2, we provide an algorithm which is (1−η)-consistent and\u0000e−1\ne·1\n1+(1−η)(1−eη−1)\u0001\n-robust for the Online Bounded Allocation problem. In Section 3, we give a\n(1−η)-consistent and (1−e−η)-robust algorithm for the Online Ad-Auctions problem. Similarly\nto Bamas et al. (2020), we provide algorithms that produce fractional solutions for the studied prob-\nlems. When the algorithms have high confidence in the prediction (meaning that ηis closed to 0),\nthe algorithms achieve a similar objective value as the prediction; and when the confidence is low\n(ηis close to 1), the algorithms guarantee the best worst-case bound, (e−1)/e. This robustness\nguarantee holds by our algorithms even if the predictions give an infeasible solution (realized during\nthe execution).\n1.3. Related work\nThe domain of algorithms with predictions (Mitzenmacher and Vassilvitskii, 2020) - or learning\naugmented algorithms - has recently emerged and rapidly grown at the intersection of (discrete)\nalgorithm design and machine learning (ML). Its main concept is to incorporate learning predictions\n3\n\nALGORITHMS WITH PREDICTIONS FOR MATCHING PROBLEMS\nand ML techniques in the algorithm design to achieve performance guarantees beyond the worst-\ncase analysis and provide tailored solutions to different problems. Recent studies showed interesting\nresults over a large spectrum of problems, such as scheduling (Lattanzi et al., 2020; Mitzenmacher,\n2020), caching (paging) (Lykouris and Vassilvtiskii, 2018; Rohatgi, 2020; Antoniadis et al., 2020),\nski rental (Gollapudi and Panigrahi, 2019; Kumar et al., 2018; Angelopoulos et al., 2020), counting\nsketches (Hsu et al., 2019) and bloom filters (Kraska et al., 2018; Mitzenmacher, 2018). Bamas\net al. (2020) recently proposed a primal-dual approach to design online algorithms with predictions\nfor linear problems with covering constraints. They raised an open question to extend their approach\nto packing constraints, which we answer in this paper.\nThe online matching and Ad-Auctions problems have been widely studied (see Mehta (2013)\nand references therein). Mehta et al. (2007) introduced the Online Ad-Auctions problem and gave\nan optimal (1−1/e)competitive ratio when Rmax= max i,j{bij/Bi} is small. Buchbinder et al.\n(2007) simplified their work by a primal-dual analysis. In the same paper, the authors gave an\nalgorithm with a refined competitive ratio for the Online Bounded Allocation problem assuming an\nupper bound on the maximum degree of the vertices in the matching. Aggarwal et al. (2011) studied\nanother particular case of Ad-Auctions, in which for each ithe bids ( bij) are the same for every\nj. They obtained the optimal (1−1/e)competitive ratio with the generalization of the RANKING\nalgorithm (Karp et al., 1990). In the Ad-Auctions problem (without any assumptions), the existence\nof an (1−1/e)-competitive algorithm has been conjectured, but remained an open problem. Huang\net al. (2020) recently presented a 0.5016 -competitive algorithm for this problem.\nMotivated by internet advertising applications, several works considered the Ad-Auctions prob-\nlem in various settings where forecasts or predictions are available or learnable. Esfandiari et al.\n(2018) proposed a model in which the input is stochastic, and the model gets a forecast for future\nitems. Intuitively, we can measure the forecast accuracy by the optimal solution’s fraction one can\nobtain from the stochastic input. They provide algorithms with provable bounds in this setting.\nSchild et al. (2019) introduced a semi-online model in which the unknown future has a predicted\nand an adversarial part. They gave algorithms with competitive ratios depending on the fraction of\nthe adversarial part in the input. Closely related to our work is the model by Mahdian et al. (2012)\nin which, given two algorithms, one needs to design a (new) algorithm that is robust to both given\nalgorithms. They derived an algorithm for the Ad-Auctions problem that achieves a fraction of the\nmaximum revenue of the given algorithms. The main difference compared to our model is that their\nalgorithm is notrobust if one of the given algorithms provides infeasible solutions (which could\nhappen with predictions), whereas our algorithm is.\n2. An Algorithm with Predictions for Online Bounded Allocation\nRecall that in this problem there are nbuyers and each buyer 1≤i≤nhas a budget Bi. Upon\nthe arrival of item j, the algorithm discovers the item’s fixed price bjand the subset of buyers\nSjinterested in purchasing the item. Additionally, the algorithm gets a predicted buyer pred (j)\nto whom item jshould be sold ( pred (j) = 0 if the item should not be sold according to the\nprediction). We are interested in fractional solutions as in Bamas et al. (2020), so we consider\nthe items to be splittable. Before the arrival of the next item, the algorithm needs to make an\nirrevocable decision and sell the current item in some fractions to some buyers. The objective is to\ngain maximum revenue from selling items to buyers.\n4\n\nALGORITHMS WITH PREDICTIONS FOR MATCHING PROBLEMS\nFormulation. Letxijbe the fraction of item jsold to buyer i. The problem can be cast to the\nfollowing primal linear program in Figure 1 (in which we also show its dual program).\nPrimal:\nmaxmX\nj=1X\ni∈Sjbjxij\nX\nj:i∈Sjbjxij≤Bi∀i (yi)\nX\ni∈Sjxij≤1∀j (zj)\nxij≥0∀i, jDual:\nminnX\ni=1Biyi+mX\nj=1zj\nbjyi+zj≥bj ∀j, i∈Sj(xij)\nyi≥0 ∀i\nzj≥0 ∀j\nFigure 1: Formulation of the Online Bounded Allocation problem\nAlgorithm. Buchbinder et al. (2007) proposes an algorithm for the Online Bounded Allocation\nproblem without predictions. They define buyer levels based on the fraction of the buyers’ spent\nbudget. Their algorithm splits each arriving item jequally among its interested buyers i∈Sjwho\nare on the lowest level. Intuitively, this algorithm corresponds to water-filling the buyer levels.\nWe propose an algorithm that allocates items using the water-filling strategy of Buchbinder\net al. (2007) and also subtly incorporates predictions. Recall that the parameter ηrepresents the\nconfidence in the predictions. When ηis close to 1, our algorithm resembles the water-filling\nalgorithm, while with ηclose to 0, the prediction has a stronger impact on the algorithm’s decision.\nTo do water-filling, our algorithm uses buyer levels as well. At any moment during the execution, a\nbuyer i’s level is ℓ, if the fraction of buyer i’s spent budget is within the range of\u0002ℓ\ndBi,ℓ+1\ndBi\u0001\n.\nWe formally describe the algorithm as follows. Upon the arrival of a new item j, leti∗be the\npredicted buyer ( pred (j)) suggested by the oracle. Do the following.\nAlgorithm 1 Learning Augmented Algorithm for the Online Bounded Allocation Problem.\nStage 1: As long as some interested buyers in Sjspend less than ηfraction of their budget, the\nalgorithm allocates item jequally to buyers in Sjwho are on the lowest level.\nStage 2: Once all interested buyers spend at least ηfraction of their budgets, the algorithm assigns\nthe remaining fraction of item jto the predicted buyer i∗. The prediction assignment ends\nwhen one of the two following conditions occurs.\n(a) the algorithm assigned to the predicted buyer either (1−η)fraction or the remaining\nfraction of j, whichever is smaller\n(b) buyer i∗exhausted its budget\nStage 3: If the algorithm does not assign item jcompletely during Stage 1and2and there exists at\nleast one buyer in Sjthat does not exhaust its budget, then, similarly to the first step, the\nalgorithm allocates the remaining fraction of item jequally to buyers in Sjwho are on the\nlowest level.\n5\n\nALGORITHMS WITH PREDICTIONS FOR MATCHING PROBLEMS\nAnalysis. We first prove the consistency and then the robustness of the algorithm.\nLemma 1 Algorithm 1 is (1−η)-consistent with the prediction.\nProof LetVbe the set of buyers who exhaust their budget at some point during the algorithm’s\nexecution. Let Ube the set of items whose buyers did not exhaust their budgets, formally, U={j:\npred (j)/∈V}. We can formulate the total gain of a feasible prediction as\nX\nj\npred(j)̸=∅bj=X\nj\npred(j)∈Vbj+X\nj\npred(j)/∈Vbj≤X\ni∈VBi+X\nj∈Ubj (1)\nwhere the inequality holds since any feasible allocation - including the prediction - can allocate\nitems of value at most Bi(the total budget) to buyer i.\nLetj∈Ube an arbitrary item. By the construction of the algorithm and the fact that the pre-\ndicted buyer for item jdid not exhaust its budget, the following can occur. The algorithm assigned\nitemjentirely during the limit assignment of Stage 1(to satisfy the condition of ηfraction spent by\neach interested buyer), or otherwise allocated the remaining fraction of item jto its predicted buyer\niup to (1−η)fraction. We note that the algorithm can always assign the remaining fraction, since\nbuyer idid not exhaust its budget by the end of the execution. In any case, we sell item jin at least\n(1−η)fraction, therefore the gain of the algorithm on jis at least (1−η)bj.\nBefore the prediction assignment of Stage 2, the total value of items in Usold to buyers in V\nis at mostP\ni∈VηBidue to the limit assignment rule of Stage 1. As a consequence, each predicted\nbuyer i∗where i∗/∈Vgetsat least (1−η)−\u0000\u0000\nηP\ni∈VBi\u0001\n/bj\u0001\nfraction of item j. Therefore,\nthe total gain of the algorithm is at least\nX\ni∈VBi+X\nj∈U(1−η)bj−ηX\ni∈VBi (2)\nwhere the first term is the gain of the algorithm restricted to buyers in V, the second one is the gain\nof the algorithm restricted to items in U, and the last term is the upper bound of the total value of\nitems in Usold to buyers in Vbefore the prediction assignment of Stage 2. By (1) and (2), the gain\nof the algorithm is at least (1−η)that of the prediction.\nLet us now establish the robustness of the algorithm. Let d= max {|Sj|}be the bound on\nthe number of interested buyers. We characterize the robustness as a function of d. Let us define a\npiece-wise linear function fd: [0,1]→R≥0such that fd(1) = 1 and for 0≤u <1, ifℓ−1\nd≤u <ℓ\nd\nfor some 1≤ℓ≤dthen\nfd(u) =daℓ\u0012\nu−ℓ−1\nd\u0013\n+daℓ−11\nd+daℓ−21\nd+. . .+da11\nd\n=daℓ\u0012\nu−ℓ−1\nd\u0013\n+aℓ−1+aℓ−2+. . .+a1\nwhere\na1=1\nd(1 +1\nd−1)d−1−(d−1)and aℓ=a1\u0012\n1 +1\nd−1\u0013ℓ−1\n∀2≤ℓ≤d.\n6\n\nALGORITHMS WITH PREDICTIONS FOR MATCHING PROBLEMS\nInformally, fdis linear with coefficient daℓon every interval\u0002ℓ−1\nd,ℓ\nd\u0001\nfor1≤ℓ≤d. Note that the\nmaximum derivative of fdis1/C(d), where\nC(d) =d(1 +1\nd−1)d−1−(d−1)\nd(1 +1\nd−1)d−1= 1−d−1\nd(1 +1\nd−1)d−1d→∞−→e−1\ne\nandC(d)is always larger than (e−1)/e.\nLemma 2 The robustness of Algorithm 1 is 1/\u00001\nC(d)+(1−η)(1−fd(η))\u0001\n. When dis large enough,\nfd(η)≈1 +e(eη−1−1)\ne−1and so the robustness is approximativelye−1\ne·1\n1+(1−η)(1−eη−1).\nProof We fix an arbitrary item jand bound the ratio of increase of the primal and the dual objective\nvalues caused by the arrival of item j. We set the dual variables as\nyi=fd\u0012P\njbjxij\nBi\u0013\n∀i, z j= (1−fd(min\ni∈Sj{ℓi}))bj\nwhere ℓiis the level of buyer iat the end of the algorithm’s execution.\nBy the definition of the dual variables, yi≥fd\u0000\nmini′∈Sj{ℓi′}\u0001\nfor every i∈Sj. Therefore,\nbjyi+zj≥bjholds for every i∈Sj, which indicates that the dual variables are feasible.\nLet us assume that item jis not entirely sold during the algorithm’s execution. This means that\nall interested buyers in Sjexhausted their budget. Hence, by the definition of the dual variables,\n∀i∈Sj:yi= 1andzj= 0. The rate of change of the dual objective value related only to the\ny-variables is at most:\nBi∂fd\n∂xij≤Bi·bj\nBi·f′\nd\u0012P\njbjxij\nBi\u0013\n≤bj\nC(d)\nwhere 1/C(d)is the maximum derivative of fd. Meanwhile, the increasing rate of the primal\nobjective value is bj. We obtain that the primal change is at least 1/C(d)that of the dual.\nIn the remaining part of the proof, we assume that the algorithm sold all items completely. Note\nthatzjcan be written as zj=R1\n0(1−fd(min i∈Sj{ℓi}))bjdy. In other words, one can imagine that\nduring the allocation of item j,zjis increasing at a rate of (1−fd(min i∈Sj{ℓi}))bj.\nThere are three stages in the algorithm. In Stage 1 and 3, the algorithm always allocates the\nfractions of item jequally to buyers in Sjon the lowest level. Buchbinder et al. (2007) showed\nthat during these allocations, the increasing rate of the dual is at most 1/C(d)times the primal. We\npresent our proof in Lemma 3 for completeness, which is similar to the proof of Buchbinder et al.\n(2007).\nWe are now interested in the allocation during Stage 2 of the algorithm. We denote the predicted\nbuyer i∗=pred (j). In this stage, the algorithm allocates a part of item jonly to the predicted\nbuyer i∗. The increasing rate of the dual objective value related only to yi∗is\nBi∗∂fd\n∂xi∗j≤Bi∗·bj\nBi∗·f′\nd\u0012P\njbjxi∗j\nB∗\ni\u0013\n≤bj\nC(d)\nsince f′\nd(u)≤1/C(d)for all 0≤u≤1. We note that every buyer i∈Sjhas spent at least η\nfraction of its budget before this stage, so zj≤(1−fd(η))bj. Therefore, the total increasing rate\n7\n\nALGORITHMS WITH PREDICTIONS FOR MATCHING PROBLEMS\nof the dual in this step is at most\nBi∗∂fd\n∂xi∗j+∂zj\n∂xi∗j≤bj\nC(d)+ (1−fd(η))bj.\nCombining all the cases, the increasing rate of the dual is at most\n(bj\nC(d)+ (1−fd(η))bjduring Stage 2 ,\nbj\nC(d)during Stage 1 and 3 .\nWe highlight that the algorithm allocates at most (1−η)fraction of item jduring Stage 2. Therefore,\nthe total increase of the dual due to the arrival of jis at most:\nZ(1−η)\n0\u0012bj\nC(d)+ (1−fd(η))bj\u0013\ndy+Zη\n0\u0012bj\nC(d)\u0013\ndy=\u00121\nC(d)+ (1−η)(1−fd(η))\u0013\nbj.\nThe increase of the primal is bj, so we can deduce that the robustness of the algorithm is 1/\u00001\nC(d)+\n(1−η)(1−fd(η))\u0001\n. To finish the proof, we compute and estimate the value of fd(η)for large values\nofd. We have\nfd(η) =fd\u0012⌊η·d⌋\nd\u0013\n=a1\u0012\nd\u0012\n1 +1\nd−1\u0013⌊η·d⌋−1\n−(d−1)\u0013\n=d(1 +1\nd−1)⌊η·d⌋−1−(d−1)\nd(1 +1\nd−1)d−1−(d−1)\n= 1 +d(1 +1\nd−1)⌊η·d⌋−1−d(1 +1\nd−1)d−1\nd(1 +1\nd−1)d−1−(d−1)\n= 1 + \nd(1 +1\nd−1)⌊η·d⌋−1\nd(1 +1\nd−1)d−1−1!\n·1\n1−(d−1)\nd(1+1\nd−1)d−1\n= 1 +1\nC(d) \u0012\n1 +1\nd−1\u0013⌊η·d⌋−d\n−1!\nd→∞−→1 +e(eη−1−1)\ne−1\nLemma 3 Assuming that item jis completely sold by Algorithm 1, during the allocations of Stage\n1 and 3, the increasing rate of the dual objective value is at most 1/C(d)times the primal.\nProof The proof follows the proof of Buchbinder and Naor (2009b, Theorem 13.1). We present the\ndetails in Appendix A.\nTheorem 1 Algorithm 1 is (1−η)-consistent and 1/\u00001\nC(d)+(1−η)(1−fd(η))\u0001\n-robust. When dis\nlarge enough, fd(η)≈1 +e(eη−1−1)\ne−1and so the robustness is approximatelye−1\ne·1\n1+(1−η)(1−eη−1).\nProof Follows from Lemma 1 and Lemma 2.\n8\n\nALGORITHMS WITH PREDICTIONS FOR MATCHING PROBLEMS\n3. An Algorithm with Predictions for Ad-Auctions\nRecall that in this problem there are nbuyers and each buyer 1≤i≤nhas a budget Bi. Upon\nthe arrival of item j, the algorithm discovers the bid2bij≥0of each buyer i, which is the price\nthat buyer iis willing to pay to purchase item j. Additionally, the algorithm gets a predicted buyer\npred (j)to whom item jshould be sold ( pred (j) = 0 if the item should not be sold according to\nthe prediction). We are interested in fractional solutions as in Bamas et al. (2020), so we consider\nthe items to be splittable. Before the arrival of the next item, the algorithm needs to make an\nirrevocable decision and sell the current item in some fractions to some buyers. The objective is to\ngain maximum revenue from selling items to buyers. In this setting, we assume that bij≪Bi∀i, j.\nFormulation. The formulation of the Online Ad-Auctions problem follows that of the Online\nBounded Allocation problem. Both the primal and the dual linear programs on Figure 2 now use\nthe buyer-dependent prices, the bids.\nPrimal:\nmaxnX\ni=1mX\nj=1bijxij\nmX\nj=1bijxij≤Bi∀i (yi)\nnX\ni=1xij≤1 ∀j (zj)\nxij≥0 ∀i, jDual:\nminnX\ni=1Biyi+mX\nj=1zj\nbijyi+zj≥bij ∀i, j (xij)\nyi≥0 ∀i\nzj≥0 ∀j\nFigure 2: Formulation of the Online Ad-Auctions problem\nAlgorithm. We introduce a fictitious buyer with identity 0, such that b0j= 0for all item j. When\nthe algorithm does not sell an item, it assigns it to the fictitious buyer 0. The purpose of buyer 0is\nto simplify the description of the algorithm. We use a constant Cin our algorithm that we define as\nC= (1 + Rmax)η\nRmax where Rmax= max\ni,j\u001abij\nBi\u001b\nWe present the pseudo-code of our algorithm in Algorithm 2. For each arriving item j, the algorithm\nconsiders two buyers: the predicted buyer i∗=pred (j)and buyer iwho maximizes the product\nbij(1−yj), where yjis an indicator to know how exhausted the buyer’s budget is. We make use of\nthe confidence parameter in the predictions. Our algorithm reserves (1−η)fraction of each buyer’s\nbudget for the prediction assignment. Whenever the bid of buyer i∗is greater than the bid of buyer\ni, the algorithm assigns (1−η)fraction of item jtoi∗andηfraction to i.\nIntuitively, our proposed algorithm attempts to reserve some fraction of each buyer’s budget\nfor future purchases. However, if the predicted buyer’s bid is high enough, the algorithm allows\nassignments to this buyer, even if the budget is close to saturation.\n2. we call the prices bids because of the motivations in the auctions setting\n9\n\nALGORITHMS WITH PREDICTIONS FOR MATCHING PROBLEMS\nAlgorithm 2 Learning Augmented Algorithm for the Online Ad-Auctions Problem.\nAll primal and dual variables are initially set to 0.\nWe maintain two sets N(i)andM(i)for each buyer ifor the purpose of analysis only.\nforeach new item jdo\nleti∗be the predicted buyer of item j, formally, pred (j) =i∗\nifthe prediction is not feasible theni∗= 0 //the fictitious buyer\ni←arg max i′{bi′j(1−yi′)} //weight bids with remaining budget\nifbij(1−yi)≤0theni= 0\nzj←max\b\n0, bij(1−yi)\t\nifbij< bi∗jthen\nxij←ηandxi∗j←(1−η)\nN(i∗)←N(i∗)∪ {j}\nend\nelse\nxij←1 //includes the case when pred (j)is infeasible\nend\nM(i) =M(i)∪ {j}\nyi=yi\u0010\n1 +bij\nBi\u0011\n+bij\nBi·1\nC−1\nend\nLemma 4 During the execution of Algorithm 2 the following always holds for every i\nyi≥1\nC−1\u0012\nCP\nj∈M(i)bij\nηBi−1\u0013\nProof We adapt the proof of Buchbinder and Naor (2009b) with a slight modification: we prove\nthe dual inequality by induction on the number of processed items. Initially, when no items arrived\nyet, the inequality is trivially true. Let us assume that the inequality holds right before the arrival\nof an item j. The inequality remains unchanged for all buyers, except for buyer i, who maximizes\nbij(1−yi). Let yidenote the value before the update triggered by the arrival of item jandy′\niits\nvalue after the update. We have\ny′\ni=yi\u0012\n1 +bij\nBi\u0013\n+bij\nBi·1\nC−1≥1\nC−1\u0012\nCP\nj′∈M(i)\\{j}bij′\nηBi −1\u0013\n·\u0012\n1 +bij\nBi\u0013\n+bij\nBi·1\nC−1\n=1\nC−1\u0012\nCP\nj′∈M(i)\\{j}bij′\nηBi ·\u0012\n1 +bij\nBi\u0013\n−1\u0013\n≥1\nC−1\u0012\nCP\nj′∈M(i)\\{j}bij′\nηBi ·Cbij\nηBi−1\u0013\n=1\nC−1\u0012\nCP\nj∈M(i)bij\nηBi−1\u0013\nThe first inequality holds due to the induction hypothesis. The second inequality holds by the\nfollowing sequence of transformations. For any yandzwhere 0< y≤z≤1we have\nln(1 + y)\ny≥ln(1 + z)\nz⇔ ln(1 + y)≥ln(1 + z)·y\nz⇔ 1 +y≥(1 +z)y/z\n10\n\nALGORITHMS WITH PREDICTIONS FOR MATCHING PROBLEMS\nWe apply the above transformation with y=bij/Biandz=Rmaxat the second inequality. By the\ndefinition of Cwe obtain\u0012\n1 +Rmax\u0013 1\nRmax·bij\nBi=Cbij\nηBi\nSince Rmax= max i,jnbij\nBio\nby definition, the induction step is complete and the lemma holds.\nLemma 5 The primal solution is feasible up to a factor of (1 +Rmax).\nProof The first primal constraint requiresPm\nj=1bijxij≤Bito hold for every i. By Lemma 4, we\nknow that for every ithe following holds.\nyi≥1\nC−1\u0012\nCP\nj∈M(i)bij\nηBi−1\u0013\nTherefore, wheneverP\nj∈M(i)bij≥ηBi, we have yi≥1and the algorithm stops allocating items\nto buyer i. The set M(i)and the value of yiare updated after the assignments. Therefore, buyer\nican receive at most one additional item fraction once its budget is already saturated. We obtainP\nj∈M(i)bij< ηB i+ max j{bij}and the following formula.\nmX\nj=1bijxij=X\nj∈M(i)bijxij+X\nj∈N(i)bijxij< B i+ max\nj{bij}\nThe inequality holds due to the feasibility of N(i), the set of items assigned by the prediction to\nbuyer i. We can bound the prediction assignments as follows.\nX\nj∈N(i)bijxij≤(1−η)X\nj|pred (j)=ibijxij≤(1−η)Bi\nTherefore,Pm\nj=1bijxij≤Bi(1 +Rmax), which means that the first primal constraint is feasible\nup to a factor of (1 +Rmax). The second primal constraint requiresP\nixij≤1to hold. During the\nallocations of Algorithm 2 the values of xijandxi∗jdo not exceed 1by the design of the algorithm.\nThe lemma follows.\nTheorem 2 Algorithm 2 is (1−η)-consistent and1−1/C\n1+Rmax-robust. The robustness tends to 1−e−η\nwhen Rmaxtends to 0.\nProof First, we establish robustness. Upon the arrival of item j, the increase in the primal is\n(\n(1−η)bi∗j+η bijifbij< bi∗j,\nbij ifbij≥bi∗j\nwhich is always larger than or equal to bij. Meanwhile, the increase in the dual is\nBi∆yi+zj=bijyi+bij\nC−1+bij(1−yi) =\u0012\n1 +1\nC−1\u0013\nbij=C\nC−1bij\nHence, by Lemma 5, the robustness isC−1\nC·1\n1+Rmax.\nFinally, we straightforwardly establish consistency. Every time the prediction solution gets a\nprofit of bi∗j, the algorithm achieves a profit of at least (1−η)bi∗j. The theorem follows.\n11\n\nALGORITHMS WITH PREDICTIONS FOR MATCHING PROBLEMS\n4. Experiments\nIn the following experiments, we evaluate the algorithms’ performance with the competitive ra-\ntio metric. We calculate the competitive ratio by dividing the objective value of the algorithm,\nALGO(I), with the objective value of the optimal fractional offline solution, OPT(I). The competi-\ntive ratio is visible on the y-axis of the figures. The x-axis corresponds to the ratio with which the\nalgorithms consider the prediction. We indicate with η= 0no doubt and η= 1complete doubt in\nthe prediction. Therefore, the left-hand side of the figures corresponds to a prediction dominated\nalgorithm, while the right-hand side is closer to a classical online algorithm. The different colors\non the figures correspond to different prediction error rates. The lines represent the average over\nseveral executions, while the colored areas show the 95% confidence intervals.\n4.1. Online Bounded Allocation\nWe present here four experiments. Table 1 below summarizes the configurations of the instances.\nFigure 3 displays the competitive ratio of the algorithm on different instances.\nPredictions. We create predictions for the experimental instances by introducing perturbations\nto the optimal offline integral solution. The perturbation is skipped when |Sj|= 1. When |Sj\\\n{i∗}|>1we choose uniformly at random a remaining buyer to replace the optimal buyer.\nName Type Buyers Items d= max {|Sj|}Budget range Item price range\nInstance 1manual 5 5 5 100−100 100−100\nInstance 2random 100 1.000 5 10−100 0.1−8\nInstance 3random 100 10.000 3 10−1.000 1−10\nInstance 4random 80 80 40 10−100 10−100\nTable 1: Properties of the experiment instances\nManual Instance. Instance 1corresponds to one of the pathological inputs for the water-filling\nstrategy of Buchbinder and Naor (2009b). We created this instance manually to observe the behav-\nior of Algorithm 1 when this strategy performs poorly. In Instance 1’s scenario, each buyer iis\ninterested in each item jwhen i≥j. The optimal solution is to sell each item jto buyer i, where\ni=j, while the water-filling strategy attempts to allocate each item equally.\nRandomized Instances. Instances 2-4are randomly generated based on their corresponding con-\nfiguration visible on Table 1. We executed each instance 20times. The lines on Figure 3 correspond\nto the average of these executions, and the colored areas correspond to the 95% confidence inter-\nvals. Instance 2and Instance 3mimic real-life instances, where the general expectation is to have a\nsmall bound don the interested buyers. The average item price over the average budget value across\nthe executions was 6.36% for Instance 2and0.98% for Instance 3. Finally, Instance 4shows an\nexample where the integral solution (and therefore the prediction) is not optimal. While Instance\n2and3have no integrality gap, Instance 4has an average of 17.99% observed integrality gap over\nthe20executions. Instance 4has a large bound on the number of interested buyers, and the items’\nprices vary greatly.\n12\n\nALGORITHMS WITH PREDICTIONS FOR MATCHING PROBLEMS\n0.0 0.2 0.4 0.6 0.8 1.0\n0.700.750.800.850.900.951.00ALGO(I) / OPT(I)\n0.0\n0.1\n0.2\n0.3\n0.4\nInstance 1\n0.0 0.2 0.4 0.6 0.8 1.0\n0.9880.9900.9920.9940.9960.9981.000ALGO(I) / OPT(I)\n0.0\n0.1\n0.2\n0.3\n0.4 Instance 2\n0.0 0.2 0.4 0.6 0.8 1.0\n0.960.970.980.991.00ALGO(I) / OPT(I)\n0.0\n0.01\n0.1 Instance 3\n0.0 0.2 0.4 0.6 0.8 1.0\n0.8000.8250.8500.8750.9000.9250.9500.975ALGO(I) / OPT(I)\n0.0\n0.01\n0.1 Instance 4\nFigure 3: Observed competitive ratios\nEvaluation. The experiments confirm the water-filling algorithm’s benefit from the prediction in-\nformation. We showed that Algorithm 1 has improved performance on the pathological input of\nthe water-filling strategy even with high prediction error rates. Besides, the algorithm demonstrated\nfirm robustness against elevated prediction error rates in the second instance, where the algorithm\nperformed close to optimality without predictions. Furthermore, we observed improved perfor-\nmance on the third instance, which represents a close to real-life use case. In the fourth instance,\nwe can remark the drawback of the predictions. When the integral solution of the linear program\nis not optimal - or even far from optimal - the prediction can significantly misguide our algorithm,\ndecreasing the performance towards the standard performance bounds.\n4.2. Online Ad-Auctions\nWe present one experiment for the Online Ad-Auctions problem here. Figure 4 displays the com-\npetitive ratio of the algorithm.\nPredictions. The predictions rely on the optimal offline integral solution, which is a partial map-\nping from items to buyers. We perturbed the solution as suggested by Bamas et al. (2020) and used\nthe error rate parameter as a probability to randomly choose a buyer among the buyers who placed a\nnon-zero bid on the item. However, the perturbation is only possible if the solution remains feasible.\nInstance. Our test instance is a randomized\ninstance with 100 buyers and 10,000 items,\nadapting the model described in Lavastida et al.\n(2020). For every item j, there are exactly 6ran-\ndom buyers proposing a bid. The values of the\nbids follow a lognormal distribution with mean\nand deviation set to 0.5. By choosing the bud-\nget of the bidders, we can tune the hardness of\nthe instance under the constraint that Rmaxre-\nmains reasonably small. We set the budget to\n0.1fraction of the total bids, leading to a value\nofRmax≈0.1. The integrality gap of the in-\nstance is close to 0.\n0.2 0.4 0.6 0.8 1.0\n0.880.900.920.940.960.98ALGO(I) / OPT(I)\n0.0\n0.01\n0.02\n0.09\n0.1\n0.11\n0.12 Figure 4: Observed competitive ratio\n13\n\nALGORITHMS WITH PREDICTIONS FOR MATCHING PROBLEMS\nEvaluation. We can observe the algorithm’s benefit of the prediction. We note that in the algo-\nrithm ηcan not be 0since, in this case, the update of yiwould require division by 0. The closer\nηgets to 0, the closer the performance of the algorithm gets to 1. As expected, the performance\ndegrades with increased prediction perturbation. The observed robustness is not monotone in η,\nunlike the bound shown in this paper. We think that this performance degradation for ηaround 0.5\nis due to the rather simplistic mixture between the primal-dual and the predicted solution.\n5. Conclusion\nThe presented algorithms in this paper incorporate prediction information and achieve competitive\nconsistency and robustness. However, it remains an open question to determine lower bounds and\nverify how tight our obtained bounds are.\nAn important research direction for the future is to study matching problems with dynamic con-\nfidence parameters, which change over time depending on the quality of the previous predictions.\nIn general, it might be possible to design more complete systems that evolve through the inter-\nactions between the algorithms and the learning oracles. The oracles provide useful information\n(predictions) for the algorithms to improve their performance (as studied in this paper), while the\nalgorithms could give feedback to the oracles to enhance their predictions.\nAcknowledgments\nThis research has been partly supported by the research program on Edge Intelligence at the Multi-\ndisciplinary Institute on Artificial Intelligence MIAI in Grenoble (ANR-19-P3IA-0003) and by the\nfrench research agency Energumen (ANR-18-CE25-0008).\nReferences\nGagan Aggarwal, Gagan Goel, Chinmay Karande, and Aranyak Mehta. Online vertex-weighted\nbipartite matching and single-bid budgeted allocations. In Proc. 22nd Symposium on Discrete\nAlgorithms , pages 1253–1264, 2011.\nSpyros Angelopoulos, Christoph D ¨urr, Shendan Jin, Shahin Kamali, and Marc Renault. Online\nComputation with Untrusted Advice. In 11th Innovations in Theoretical Computer Science Con-\nference (ITCS 2020) , volume 151, pages 52:1–52:15, 2020.\nAntonios Antoniadis, Christian Coester, Marek Elias, Adam Polak, and Bertrand Simon. Online\nmetric algorithms with untrusted predictions. In International Conference on Machine Learning ,\npages 345–355, 2020.\nEtienne Bamas, Andreas Maggiori, and Ola Svensson. The primal-dual method for learning aug-\nmented algorithms. In Proc. 34th Conference on Neural Information Processing Systems , 2020.\nBenjamin Birnbaum and Claire Mathieu. On-line bipartite matching made simple. Acm Sigact\nNews , 39(1):80–87, 2008.\nNiv Buchbinder and Joseph Naor. Online primal-dual algorithms for covering and packing. Mathe-\nmatics of Operations Research , 34(2):270–286, 2009a.\n14\n\nALGORITHMS WITH PREDICTIONS FOR MATCHING PROBLEMS\nNiv Buchbinder and Joseph Naor. The design of competitive online algorithms via a primal-dual\napproach. Foundations and Trends in Theoretical Computer Science , 3(2-3):93–263, 2009b.\nNiv Buchbinder, Kamal Jain, and Joseph Seffi Naor. Online primal-dual algorithms for maximizing\nad-auctions revenue. In European Symposium on Algorithms , pages 253–264, 2007.\nHossein Esfandiari, Nitish Korula, and Vahab Mirrokni. Allocation with traffic spikes: Mixing\nadversarial and stochastic models. ACM Trans. Econ. Comput. , 6(3–4), 2018.\nMatthew Fahrbach, Zhiyi Huang, Runzhou Tao, and Morteza Zadimoghaddam. Edge-weighted\nonline bipartite matching. In Proc. 61st Symposium on Foundations of Computer Science , 2020.\nSreenivas Gollapudi and Debmalya Panigrahi. Online algorithms for rent-or-buy with expert advice.\nInInternational Conference on Machine Learning , pages 2319–2327, 2019.\nChen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency estimation\nalgorithms. In Proc. Conference on Learning Representations , 2019.\nZhiyi Huang, Qiankun Zhang, and Yuhao Zhang. Adwords in a panorama. In Proc. 61st Symposium\non Foundations of Computer Science , 2020.\nRichard M Karp, Umesh V Vazirani, and Vijay V Vazirani. An optimal algorithm for on-line\nbipartite matching. In Proc. 22nd ACM symposium on Theory of computing , pages 352–358,\n1990.\nTim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned\nindex structures. In Proc. Conference on Management of Data , pages 489–504, 2018.\nRavi Kumar, Manish Purohit, and Zoya Svitkina. Improving online algorithms via ML predictions.\nInProc. 32nd Conference on Neural Information Processing Systems , pages 9684–9693, 2018.\nSilvio Lattanzi, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Online scheduling\nvia learned weights. In Proc. Symposium on Discrete Algorithms , pages 1859–1877, 2020.\nThomas Lavastida, Benjamin Moseley, R Ravi, and Chenyang Xu. Learnable and instance-robust\npredictions for online matching, flows and load balancing. Technical Report arXiv:2011.11743,\narXiv, 2020.\nThodoris Lykouris and Sergei Vassilvtiskii. Competitive caching with machine learned advice. In\nInternational Conference on Machine Learning , pages 3296–3305, 2018.\nMohammad Mahdian, Hamid Nazerzadeh, and Amin Saberi. Online optimization with uncertain\ninformation. ACM Trans. Algorithms , 8(1), 2012.\nAranyak Mehta. Online matching and ad allocation. Foundations and Trends ®in Theoretical\nComputer Science , 8(4):265–368, 2013.\nAranyak Mehta, Amin Saberi, Umesh Vazirani, and Vijay Vazirani. Adwords and generalized online\nmatching. Journal of the ACM , 54(5):22–es, 2007.\n15\n\nALGORITHMS WITH PREDICTIONS FOR MATCHING PROBLEMS\nMichael Mitzenmacher. A model for learned bloom filters, and optimizing by sandwiching. In Proc.\nConference on Neural Information Processing Systems , pages 464–473, 2018.\nMichael Mitzenmacher. Scheduling with predictions and the price of misprediction. In Proc. 11th\nInnovations in Theoretical Computer Science Conference , 2020.\nMichael Mitzenmacher and Sergei Vassilvitskii. Beyond the Worst-Case Analysis of Algorithms ,\nchapter Algorithms with Predictions. Cambridge University Press, 2020.\nDhruv Rohatgi. Near-optimal bounds for online caching with machine learned advice. In Proc.\nSymposium on Discrete Algorithms , pages 1834–1845, 2020.\nTim Roughgarden. Beyond worst-case analysis. Communications of the ACM , 62(3):88–96, 2019.\nTim Roughgarden. Beyond the Worst-Case Analysis of Algorithms . Cambridge University Press,\n2020.\nAaron Schild, Erik Vee, Manish Purohit, Ravi Kumar Ravikumar, and Zoya Svitkina. Semi-online\nbipartite matching. In Proc. 10th Innovations in Theoretical Computer Science Conference , 2019.\nDavid P Williamson and David B Shmoys. The design of approximation algorithms . Cambridge\nUniversity Press, 2011.\n16\n\nALGORITHMS WITH PREDICTIONS FOR MATCHING PROBLEMS\nAppendix A. for the Online Bounded Allocation problem\nLemma 3 Assuming that item jis completely sold by Algorithm 1, during the allocations of Stage\n1 and 3, the increasing rate of the dual objective value is at most 1/C(d)times the primal.\nProof The proof follows the proof of Buchbinder and Naor (2009b, Theorem 13.1). We highlight\nthat this proof considers items that were sold completely. There are two cases.\nCase 1 : The highest level item jwas sold at is ℓ, and at the end of the algorithm, all buyers in\nSjspent at leastℓ+1\ndfraction of their budget.\nBy the definition of the dual variables zj≤(1−fd\u0000ℓ+1\nd\u0001\n)bj. Since item jis sold entirely\n(P\nixij= 1), the increasing rate of zjat any time during the allocation of item jis at most (1−\nfd\u0000ℓ+1\nd\u0001\n)bj. Besides, as the highest level on which item jwas sold at is ℓ, the rate of change of the\ndual value due to the change in yiis:\nBi∂fd\n∂xij≤Bibj\nBid aℓ+1≤bjd aℓ+1\nTherefore, the total change of the dual is at most\nbjd aℓ+1+bj\u0012\n1−fd\u0012ℓ+ 1\nd\u0013\u0013\n=bjd a1\u0012\n1 +1\nd−1\u0013ℓ\n+bj \n1−a1\u0012\nd\u0012\n1 +1\nd−1\u0013ℓ\n−(d−1)\u0013!\n=bj(1 +a1(d−1)) =bj\nC(d)(3)\nCase 2: The highest level on which item jwas sold at is ℓand at the end of the execution at least\none buyer in Sjspent less thanℓ+1\ndfraction of its budget.\nIn this case we have to set zj= (1−fd(ℓ/d))bjto satisfy the dual constraint. During any\n(short) period of time ∆t, the increase ofP\niBiyioveriin the highest level is at most:\nbjaℓ+1dd−1\nd∆t=bj(d−1)aℓ+1∆t\nwhere the fraction (d−1)/dcomes from the fact that there are at most (d−1)buyers in the highest\nlevel. We recall that dis the bound on the size of all Sjand at least one buyer did not reach the\nhighest level as of the statement of this case. The increase ofP\niBiyioveriin the lower levels is\nat most bjd aℓ(1−∆t)by the definitions of aℓ+1=aℓ(1 + 1 /(d−1))andaℓ= ((d−1)/d)aℓ+1.\nTherefore, by adding the terms together we get that the increasing rate ofP\niBiyiis at most\nbj(d−1)aℓ+1. Hence, the increasing rate of the dual is at most:\nbj(d−1)aℓ+1+bj\u0012\n1−fd\u0012ℓ\nd\u0013\u0013\n=bj(d−1)aℓ+1+bj\u0012\n1−fd\u0012ℓ+ 1\nd\u0013\n+aℓ+1\u0013\n=bjd aℓ+1+bj\u0012\n1−fd\u0012ℓ+ 1\nd\u0013\u0013\n=bj\nC(d)\n17\n\nALGORITHMS WITH PREDICTIONS FOR MATCHING PROBLEMS\nwhere the first equality holds since fd\u0000ℓ+1\nd\u0001\n=fd\u0000ℓ\nd\u0001\n+aℓ+1and the last equality follows Equa-\ntion (3).\nThe increasing rate of the dual is at most bj/C(d), while for the primal it is bj. The lemma\nfollows from this conclusion.\n18",
  "textLength": 44956
}