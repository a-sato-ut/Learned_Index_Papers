{
  "paperId": "c20c6a8bf89bd6fc9b4d8bfa10d275b55b22c9af",
  "title": "LMSFC: A Novel Multidimensional Index based on Learned Monotonic Space Filling Curves",
  "pdfPath": "c20c6a8bf89bd6fc9b4d8bfa10d275b55b22c9af.pdf",
  "text": "LMSFC: A Novel Multidimensional Index based on Learned\nMonotonic Space Filling Curves (Extended Version)\nJian Gao\nUNSW, Australia\njian.gao2@unsw.edu.auXin Cao\nUNSW, Australia\nxin.cao@unsw.edu.auXin Yao\nHuawei Theory Lab, China\nyao.xin1@huawei.com\nGong Zhang\nHuawei Theory Lab, China\nnicholas.zhang@huawei.comWei Wang1,2\n1DSA & Guangzhou Municipal Key\nLaboratory of Materials Informatics,\nHKUST (Guangzhou), China\n2HKUST, HKSAR, China\nweiwcs@ust.hk\nABSTRACT\nThe recently proposed learned indexes have attracted much atten-\ntion as they can adapt to the actual data and query distributions to\nattain better search efficiency. Based on this technique, several exist-\ning works build up indexes for multi-dimensional data and achieve\nimproved query performance. A common paradigm of these works\nis to (i) map multi-dimensional data points to a one-dimensional\nspace using a fixed space-filling curve (SFC) or its variant and (ii)\nthen apply the learned indexing techniques. We notice that the first\nstep typically uses a fixed SFC method, such as row-major order and\nğ‘§-order. It definitely limits the potential of learned multi-dimen-\nsional indexes to adapt variable data distributions via different\nquery workloads.\nIn this paper, we propose a novel idea of learning a space-filling\ncurve that is carefully designed and actively optimized for efficient\nquery processing. We also identify innovative offline and online\noptimization opportunities common to SFC-based learned indexes\nand offer optimal and/or heuristic solutions. Experimental results\ndemonstrate that our proposed method, LMSFC, outperforms state-\nof-the-art non-learned or learned methods across three commonly\nused real-world datasets and diverse experimental settings.\nPVLDB Reference Format:\nJian Gao, Xin Cao, Xin Yao, Gong Zhang and Wei Wang. LMSFC: A Novel\nMultidimensional Index based on Learned Monotonic Space Filling Curves\n(Extended Version). PVLDB, 16(10): 2605 - 2617, 2023.\ndoi:10.14778/3603581.3603598\n1 INTRODUCTION\nNowadays, there are large volumes and a huge variety of multi-\ndimensional data. For example, in traditional data warehouses and\nanalytical databases, the majority of key data is stored in the multi-\ndimensional fact table. The wide deployments of location-based\nservices and sensors, such as Google Maps, generate huge amounts\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license. For any use beyond those covered by this license, obtain permission by\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 16, No. 10 ISSN 2150-8097.\ndoi:10.14778/3603581.3603598ofmulti-dimensional data. The data is typically two or three spatial\ndimensions, and one or several dimensions for various measure-\nments. The common and dominant type of query over these multi-\ndimensional datasets is the window query, which imposes range\nconstraints on several or all the dimensions.\nMulti-dimensional indexes are essential in answering window\nqueries efficiently for a large volume of multi-dimensional datasets.\nPrevious studies have proposed many traditional indexes, includ-\ningğ‘…-tree [ 12], kd-tree [ 3], and Quadtree [ 9]. They are all based\non spatial partitioning, while the major difference is whether the\noverlapping between partitions exists or not.\nA space-filling curve is one of the most commonly used methods\nin multi-dimensional indexes [ 16,30]. This is because SFCs have\nexcellent proximity-preserving properties, making them ideal for\nlinearizing data objects. SFCs can be classified into two categories:\nmonotonic SFCs and non-monotonic SFCs. Monotonic SFCs, such\nasğ‘§-order [ 25], enable quick location of the search range. However,\nnon-monotonic SFCs may result in more computational overhead\nduring query processing. For instance, Hilbert curve [14] requires\nthe enumeration of all values on the boundary of the query window\nto determine the search range. Therefore, it is more difficult to\nperform range searches when using non-monotonic SFCs as the\nlinearization method.\nRecently, initiated by the seminal work [ 19], there is a surge in\noptimizing database indexing via machine learning. It takes unique\nadvantage of optimizing for specific data and query workload in-\nstances. As a result, several works have investigated the learned\nmulti-dimensional index. The prevalent approach is to map the\nmulti-dimensional data points into one dimension, and further ap-\nply a learned index on the one-dimensional space.\nHowever, they have the following limitations: (1) The unique\nand critical part of multi-dimensional indexes is mapping from\nmulti-dimensional space to one-dimensional space, while this is not\nlearned or well-learned. Most methods [ 29,38] exploit an existing\nSFC, such as ğ‘§-order [ 25], as it possesses good proximity-preserving\ncapabilities. However, a fixed SFC does not necessarily work the\nbest on a given dataset instance. Other works, such as Flood [26],\nonly learn to select a special dimension and then follow the fixed\nrow-major orders on the rest of the ğ‘‘âˆ’1dimensions, hence missing\nthe opportunity to better preserve local proximities. (2) The physicalarXiv:2304.12635v4  [cs.DB]  9 Sep 2023\n\nlayout of the linearized data points, which are stored as disk pages, is\nnot fully optimized. Existing methods typically pack a fixed amount\nof data points into each page, which may cause the Minimum\nBounding Rectangles (MBRs) of the resulting pages to contain much\ndead space or heavily overlap with each other [ 34]. (3) Even if\nthe above two issues can be mitigated at index construction time,\nexisting query processing methods still need to visit many pages\nbecause their MBRs or ğ‘§-address ranges inevitably overlap with\nthat of the query.\nIn this paper, we provide a thorough and rigorous investigation\nof learned multi-dimensional indexes and propose LMSFC to ad-\ndress the above limitations. (i) It is a challenging task to formulate a\nsuitable family of parameterized SFCs that can be efficiently learned\nand possess salient properties for efficient query processing. For\nthis, we design a learnable monotonic SFC family. Based on the\nproposed SFC family, we devise an effective solution based on the\nSMBO to learn an optimal/sub-optimal SFC to adapt to different\ndata distributions and workloads. (ii) We study the physical stor-\nage optimization issue of packing multi-dimensional data points\ninto size-limited disk pages in a principled fashion. Thanks to the\nlinearization due to the (learned) SFC, we are able to solve the oth-\nerwise NP-hard problem optimally by dynamic programming. We\nfurther propose a heuristic algorithm that trades the optimality for\nimproved practical speed, which is suitable for large-scale datasets.\n(iii) In addition to the above offline optimization techniques, we\nfurther exploit the unique online query optimization opportunity\nby proposing a query splitting strategy. We demonstrate that our\nlearned SFC lends itself to an efficient algorithm of splitting the\nquery into two such that the total access to false negative data pages\nis minimized; this algorithm is then extended to allow multiple splits\nvia recursion.\nThe contributions of the paper are summarized below:\n(1)As far as we are aware, LMSFC is the first work to consider\nlearning a space-filling curve that is directly optimized for the\nleast cost query processing on a given instance of the dataset\nand query workload.\n(2)Based on the learned SFC, we propose both offline and online\noptimization techniques. For the offline optimization, we can\noptimally or sub-optimally pack multi-dimensional data points\ninto pages that minimize a density based cost function. For the\nonline optimization, we propose recursively splitting the query\ninto sub-queries such that it minimizes the access to pages that\ndo not contain any potential data points for a query.\n(3)We compare LMSFC with previous state-of-the-art multi-dimen-\nsional indexes in our experimental evaluation. LMSFC achieves\nthe best query performance on three real-world datasets under\nvarying query selectivity, data size, and query aspect ratio.\nLMSFC can achieve up to 38.2 Ã—, 7.2Ã—, and 2.0Ã—speedup against\nğ‘…âˆ—-tree ,ZM-index , and Flood , respectively.\nThe rest of this paper is organized as follows. In Section 2, we\ndefine the problem setting and introduce some key notations and\nconcepts. Then we review the related literature in Section 3. Sec-\ntion 4 starts by investigating the requirement of range query pro-\ncessing for an SFC-based index and motivates a parameterized SFC\nfamily. We then overview the proposed LMSFC index based on theparameterized SFC family and introduce key steps in its construc-\ntion in Section 5. We motivate and introduce our query processing\nmethod with the novel query splitting strategy in Section 6. Sec-\ntion 7 presents our experimental results and analyses. Finally, we\nconclude the paper and discuss a few extensions in Section 8.\n2 PRELIMINARIES\n2.1 Problem Definition\nIn this paper, we mainly focus on exact query processing of window\nqueries on a multi-dimensional dataset.\nDefinition 1 (Multi-dimensional Dataset). A multi-dimen-\nsional dataset ğ·consists ofğ‘›points in ağ‘‘-dimensional Euclidean\nspace. Each point ğ‘¥âˆˆğ·can be denoted as(ğ‘¥(1),Â·Â·Â·,ğ‘¥(ğ‘‘))where\nğ‘¥(ğ‘–)âˆˆRğ‘‘is theğ‘–-th dimensional value of ğ‘¥.\nWithout loss of generality, we assume that, with proper scaling,\nthe domain of each coordinate is an integer within [0,2ğ¾âˆ’1], hence\nevery dimension value can be represented using ğ¾binary bits.\nA multi-dimensional window is a hyper-rectangle in the ğ‘‘-di-\nmensional space, or formally as ğ‘¤=[ğ‘¥(1)\nğ¿,ğ‘¥(1)\nğ‘ˆ]Ã—Â·Â·Â·Ã—[ğ‘¥(ğ‘‘)\nğ¿,ğ‘¥(ğ‘‘)\nğ‘ˆ],\nwhereğ‘¥(ğ‘–)\nğ¿â‰¤ğ‘¥(ğ‘–)\nğ‘ˆ.\nDefinition 2 (Multi-dimensional Window Query). Given\na multi-dimensional dataset ğ·, a multi-dimensional window query\nğ‘with the window constraint ğ‘.ğ‘¤ returns the set of points ğ‘¥from\nğ·that is located inside the window ğ‘.ğ‘¤, i.e.ğ‘…(ğ‘)={ğ‘¥|ğ‘¥âˆˆğ·âˆ§\nâˆ€1â‰¤ğ‘–â‰¤ğ‘‘,ğ‘.ğ‘¤(ğ‘–)\nğ¿â‰¤ğ‘¥(ğ‘–)â‰¤ğ‘.ğ‘¤(ğ‘–)\nğ‘ˆ}.\nAs a query is uniquely characterized by its query window, we\nwill useğ‘andğ‘.ğ‘¤interchangeably hereafter.\n2.2 Notations\nGiven a multi-dimensional window ğ‘¤, it is uniquely character-\nized by(ğ‘¤ğ¿,ğ‘¤ğ‘ˆ), i.e., the lower-bound and upper-bound points\nare defined as ğ‘¤ğ¿=(ğ‘¥(1)\nğ¿,Â·Â·Â·,ğ‘¥(ğ‘‘)\nğ¿)andğ‘¤ğ‘ˆ=(ğ‘¥(1)\nğ‘ˆ,Â·Â·Â·,ğ‘¥(ğ‘‘)\nğ‘ˆ),\nrespectively. These apply to the window constraint of the query\nwindow ofğ‘too, and we will use the shorthand ğ‘ğ¿to denoteğ‘.ğ‘¤ğ¿.\nGiven a set of points, we define the multi-dimensionalal Mini-\nmum Bounding Rectangle (MBR) as the smallest window that en-\ncloses the set of points.\nFor a binary integer ğ‘£, we denote the ğ‘—-th bit of the binary repre-\nsentation of ğ‘£asğ‘£ğ‘—. Note thatğ‘—starts from 0, which corresponds to\nthe right-most bit of ğ‘£. The most significant bit of ğ‘£is the bit set to 1\nand with the maximum bit index. For example, let ğ‘£=(00101101)2\n(we use()2to represents binary strings), ğ‘£2=1and the most\nsignificant bit of ğ‘£is 5.\nTable 1 lists frequently used notations.\n3 RELATED WORK\nIndexes are essential for processing queries on large datasets. Tradi-\ntional indexes are optimized for the worst-case performance. More\nimportantly, they miss the opportunity to exploit statistical infor-\nmation about the data and query workloads to optimize their index\nstructure and physical layout. Recently, many Machine Learning-\nbased indexes have been proposed, which achieve smaller index\nsizes and/or faster query processing speed. RMI [ 19] is a well-known\n2\n\nTable 1: Table of Notations\nNotation Description\nğ· A set of multi-dimensional data records\nğ‘‘ The dimensionality of ğ·\nğ‘¥,ğ‘¥(ğ‘–)\nğ‘—A multi-dimensional data point, and the ğ‘—-th bit of the ğ‘–-th\ndimension value of ğ‘¥\nğ‘,ğ‘ğ¿,ğ‘ğ‘ˆ A multi-dimensional window query, and its lower-bounding\nand upper-bounding points, respectively\nğ‘“ A learned SFCâ€™s mapping function\nğœƒ The parameters of ğ‘“\nğ¾ The maximum number of bits to represent coordinate values\nofğ‘¥\nwork that first notices the similarity between the exact search on\none-dimensional array of non-descending values and the classic\nregression problem. It then proposes several instances of learned\nindexes, which use a complex model to predict the logical location\nof the targeted key and then perform a local search to fix possi-\nble errors bought by the ML model. Later works further improve\nthe modelâ€™s performance or consider other variants. For instance,\nPGM [ 8] lets the user specify the error bound a priori and then\nuses simpler linear regression models with optimal segmentation\nalgorithms to construct the learned index. Fiting-tree [ 10] also uses\na tunable error parameter to tradeoff the index size for lookup\nperformance. Given a dataset, Fiting-tree applies a cost model to\nestimate the space consumption and latency to find a suitable error\nbound. Radix Spline [ 18] considers a linear spline to approximate\nCDF. The prefixes of the selected spline points are stored in an\nauxiliary radix table to accelerate the search process. ALEX [ 5]\nand LIPP [ 40] supports data updates. LIPP further eliminates the\nlocal search via a novel adjustment strategy to redistribute keys in\neach node. SOSD [ 17,23] proposes some benchmarks to evaluate\ndifferent learned indexes.\nExisting learned index techniques cannot be directly applied\nto multi-dimensional data, as there is no natural ordering among\nmulti-dimensional data points. Currently, the prevalent approach\nis to apply a linearization method to convert the problem into a\none-dimensional search problem, on which existing learned indexes\ncan then be applied. ğ‘§-order [29, 38] and row-major order [22, 26]\nare most widely used. Other linearization methods use learned\nlinear or non-linear mapping [ 21], e.g., clustering followed by the\ndistance to the cluster center [ 4]. To reduce the challenges of query\nskewness and data correlations, Tsunami [ 6] extends Flood via par-\ntitioning data space and modeling conditional CDFs. Qd-tree [ 41]\nutilizes Reinforcement Learning to optimize the space partitioning.\nSPRIG [ 42] uses a spatial interpolation function to locate the search\nrange in the grid. [ 31] learns a quadtree structure and applies a\nğ‘§-order variant on each node. [ 28] extendsğ‘§-orderâ€™s bit interleaving\nmethod to more general cases and proposes the idea of learning\nSFC. In their work, they employ a heuristic approach to identify an\nSFC that is both query-aware and skew-tolerant for a given query\npattern. Diverging from their approach, we leverage ML methods\nto directly optimize query performance under a given workload of\nqueries to find an SFC with the least estimated query time.Similar to most space-filling curve-based approaches, an orthog-\nonal aspect for learned multi-dimensional indexes is to preprocess\nthe data using simple linear transformations or sophisticated non-\nlinear transformations, such as the Rank Space transformation [ 29].\nTraditional multi-dimensional indexes often recursively decom-\npose the space into disjoint or overlapping partitions. Grid File [ 27],\nkd-tree [ 3] and Quadtree [ 9] are typical examples of the former cat-\negory, andğ‘…-tree [ 12] and its variants [ 2,16,33] are typical for the\nlatter category. Since ğ‘…-tree [ 12] variants have been widely adopted\nin commercial systems, there are also proposals to integrate learn-\ning intoğ‘…-tree. AI+R [ 1] employs an ML model to predict the set\nof leaf nodes for a window query, together with a backup ğ‘…-tree.\n[11] aims at learning the key subtree splitting routine in ğ‘…-tree\nconstruction and adapts to the problem instance. In order to reduce\nthe search range on each leaf node, [ 13] embeds an ML model on\nthe selected sort dimension to accelerate the search procedure.\nThere are other ways of integrating learning into database in-\ndexing. [ 7] uses ML models to learn a balanced space partition that\npreserves spatial proximity well. LIMS [ 36] adopts an ML-based\ndata clustering method to solve similarity search in metric spaces.\n4 A FRAMEWORK FOR LEARNED SFCS\nIn this section, we first summarize window query processing issues\nfor an SFC to motivate us to design a family of parameterized SFCs\nthat possess salient properties for query processing.\n4.1 Window Query Processing with SFCs\nA space-filling curve (SFC) is a method of mapping the multi-dimen-\nsional data space into the one-dimensional data space. As we assume\nthe data points have integer coordinate values within [0,2ğ¾âˆ’1],\nthis naturally leads to a regular partitioning of multi-dimensional\nspace into 2ğ¾ğ‘‘possible points, or cells (SFCs can also be applied on\na coarser granularity. E.g., Flood [26] can be deemed as using a fixed\nSFC on grids ). an SFC is a bijective function ğ‘“between these cells\nand integers within [0,2ğ¾ğ‘‘âˆ’1]. We callğ‘¥in the multi-dimensional\nspace the original address andğ‘“(ğ‘¥)as its corresponding ğ‘§-address\nwith respect to an SFC ğ‘“. Intuitively, as ğ‘“(ğ‘¥)is a one-dimensional\ninteger, it specifies a way to traverse all the cells exactly once.\nSFCs are known for their ability to preserve the multi-dimensional\nproximity in the linear order [ 20]. Hence, they are widely used,\nespecially in applications with a need to linearize multi-dimensional\ndata such as images, tables and spatial data. Some well-known SFCs\nare Hilbert curve, Z-order curve, and Gray curve.\nTo answer a range query ğ‘onğ·, assuming that points in ğ·have\nbeen mapped to the corresponding ğ‘§-addresses, we can (i) compute\nthe queryâ€™s one-dimensional ğ‘§-address range ğ‘ğ‘§, (ii) retrieve every\npoint whose ğ‘§-address falls within the interval ğ‘ğ‘§, and (iii) filter out\nthose points that do not fall into the query window ğ‘. The tightest\nğ‘§-address range can be defined as: [minğ‘¥âˆˆğ‘ğ‘“(ğ‘¥),maxğ‘¥âˆˆğ‘ğ‘“(ğ‘¥)].\nHowever, computing these extreme values is difficult in general. In\nthe worst case, we may need to enumerate all ğ‘¥withinğ‘, hence\nwith a cost proportional to the volume of the query andbeating the\npurpose of efficient query processing. For certain SFCs with better\nproperties, such as the Hilbert curve, we still need to enumerate\nallğ‘¥on the boundary of the query window, hence inducing a cost\nproportional to the circumference of the query.\n3\n\nNevertheless, we identify a subclass of SFCs such that the above\nminimization and maximization can be computed efficiently in\nğ‘‚(ğ‘‘)time and hence do not depend on the size of the query.\n4.2 Monotonic Space-Filling Curves\nWe will first define the criterion that the mapping function of an\nSFC is monotonic, and show that the tightest ğ‘§-address range can\nbe computed from the lower and upper bounding points of the\nquery window.\nDefinition 3 (Monotonic Function in a Multi-dimensional\nSpace). Letğ‘âª¯ğ‘defined as true if and only if âˆ€ğ‘–,ğ‘(ğ‘–)â‰¤ğ‘(ğ‘–). Then a\nfunctionğ‘”is monotonic, if for all ğ‘andğ‘, ifğ‘âª¯ğ‘, thenğ‘”(ğ‘)â‰¤ğ‘”(ğ‘).\nTheorem 1. If an SFC corresponds to a monotonic mapping func-\ntionğ‘“, given a spatial query rectangle ğ‘, the query result ğ‘ŸâŠ†{ğ‘¥|ğ‘¥âˆˆ\nğ·âˆ§ğ‘“(ğ‘ğ¿)â‰¤ğ‘“(ğ‘¥)â‰¤ğ‘“(ğ‘ğ‘ˆ)}. In other words, the tightest ğ‘§-address\nrange ofğ‘can be efficiently computed as [ğ‘“(ğ‘ğ¿),ğ‘“(ğ‘ğ‘ˆ)].\nProof. Letğ‘mindef=min{ğ‘¥âˆˆğ‘}with respect toâª¯(i.e.,ğ‘min\nisğ‘ğ¿). Then by definition for any ğ‘¥âˆˆğ‘,ğ‘minâª¯ğ‘¥. Asğ‘“is mono-\ntonic, then ğ‘“(ğ‘min)â‰¤ğ‘“(ğ‘¥). Similarly, we can show that ğ‘maxdef=\nmax{ğ‘¥âˆˆğ‘}(i.e.,ğ‘maxisğ‘ğ‘ˆ) and for any ğ‘¥âˆˆğ‘,ğ‘“(ğ‘¥)â‰¤ğ‘“(ğ‘max).\nâ–¡\nExample 1. Among three commonly used SFCs, only the ğ‘§-order\ncurve has the monotonic property. We give counter-examples for the\nHilbert curve and the Gray curve in Figure 1.\nGray Curve Hilbert Curve Z-order Curve014\n267\n1\n054\n2\n31011\n13\n1298\n14\n1545\n3\n076\n2\n189\n13\n141110\n12\n155 7 13 15\n11 914 6\n3\n8 1012\nFigure 1: Hilbert and Gray Curves are not Monotonic (Bold\nBlack Rectangle is the Query Window; the Tightest ğ‘§-address\nRanges are Marked in Bold Font)\n4.3 Parameterized Z-Order SFCs\nInspired by the monotone property of the ğ‘§-order curve, we identify\na family of monotonic SFCs that generalizes the ğ‘§-order curve. In\naddition, any instance within the family has favorable properties\nto enable efficient query processing.\nWe consider the following SFC parameterized by a parameter\nğœƒ=[ğœƒ(1),...,ğœƒ(ğ‘‘)]:\nğ‘“(ğ‘¥;ğœƒ)=ğ‘‘âˆ‘ï¸\nğ‘–=1ğ¾âˆ‘ï¸\nğ‘—=1ğœƒ(ğ‘–)\nğ‘—Â·ğ‘¥(ğ‘–)\nğ‘—, (1)\nwhere each ğœƒ(ğ‘–)is ağ¾-dimensional vector, ğ‘¥(ğ‘–)\nğ‘—represents the ğ‘—-th\nbit of theğ‘–-th dimension value of ğ‘¥,ğ‘‘is the dimensionality and ğ¾is\n1   0   0 1   1   0\n1   1   1   0   0   0(a) Original Z-order\n1   0   0 1   1   0\n1   0   1   1   0   0 (b) Generalized Z-order\n1   0   0 1   1   0\n1   0   0   1   1   0 (c) Column-major order\nFigure 2:ğ‘§-address Calculation for Several SFCs within our\nParameterized SFC Family\nthe maximum number of bits for ğ‘¥(ğ‘–). In fact,ğœƒ(ğ‘–)\nğ‘—=2ğ‘™indicates that\nğ‘¥(ğ‘–)\nğ‘—will be mapped to the (ğ‘™+1)-th bit of the binary representation\nofğ‘“(ğ‘¥;ğœƒ). If the content is clear, we use ğ‘“(ğ‘¥)for short instead of\nğ‘“(ğ‘¥;ğœƒ).\nExample 2. Figure 2 demonstrates several instances of the SFCs\nwithin our parameterized family. Figure 2(a) shows the ordered bit-\ninterleaving way of ğ‘§-order to compute ğ‘“(ğ‘¥)for the 2-dimensional\ndata pointğ‘¥=(4,6). Here,ğ¾=3, and its parameter ğœƒğ‘§=[ğœƒ(1),ğœƒ(2)]=\n[[1,4,16],[2,8,32]], and the resulting ğ‘§-address is 56. Figure 2(b)\ndemonstrates a new SFC for the same data point, but with ğœƒğ‘”=\n[[1,16,32],[2,4,8]],ğ‘“(ğ‘¥;ğœƒğ‘”)=44. Finally, Figure 2(c) demonstrates\nyet another SFC, which is known as the column-major order, with\nğœƒğ‘=[[8,16,32],[1,2,4]],ğ‘“(ğ‘¥;ğœƒğ‘)=38.\nTo ensure the family of SFCs preserves the monotonic and bijec-\ntive properties, it suffices to impose the following constraints on\nğœƒ(ğ‘–)\nğ‘—s:\n(1)ğœƒ(ğ‘–)\nğ‘—âˆˆ{20,..., 2ğ¾ğ‘‘âˆ’1}.\n(2)âˆ€ğ‘–,ğ‘—,ğ‘–â€²,ğ‘—â€²,ğ‘–â‰ ğ‘–â€²âˆ¨ğ‘—â‰ ğ‘—â€², thenğœƒ(ğ‘–)\nğ‘—â‰ ğœƒ(ğ‘–â€²)\nğ‘—â€².\n(3)âˆ€ğ‘—<ğ‘—â€²,ğœƒ(ğ‘–)\nğ‘—<ğœƒ(ğ‘–)\nğ‘—â€².\nProof. According to the first two constraints, ğ‘“(ğ‘¥)can be rep-\nresented as a binary form that is actually the combination of the\nbits from all dimensions of a multidimensional data point ğ‘¥. Thus,\nğ‘“(ğ‘¥)can be uniquely determined by given ğ‘¥and vice versa. So we\ncan conclude ğ‘“is bijective. Next, we prove ğ‘“(ğ‘¥)is monotonic. As-\nsuming we have two different multidimensional data points ğ‘and\nğ‘, the resulting mapped addresses are ğ‘“(ğ‘)andğ‘“(ğ‘)respectively.\nWhenğ‘âª¯ğ‘, according to Definition 3, on any dimension ğ‘–we have\nğ‘(ğ‘–)â‰¤ğ‘(ğ‘–). Ifğ‘(ğ‘–)<ğ‘(ğ‘–)on dimension ğ‘–, we denote the left-most\nbit whereğ‘(ğ‘–)andğ‘(ğ‘–)are different by ğ‘™, so we have ğ‘(ğ‘–)\nğ‘™=0and\nğ‘(ğ‘–)\nğ‘™=1. The third constraint can ensure the bit order of each di-\nmension of a data point ğ‘¥inğ‘“(ğ‘¥)remains unchanged. Thus, in the\nbinary forms of ğ‘“(ğ‘)andğ‘“(ğ‘), the left-most different bit is also 0\nand 1, respectively, and we can conclude that ğ‘“(ğ‘)â‰¤ğ‘“(ğ‘), which\nshows that the function ğ‘“is monotonic. â–¡\nBoth bijective and monotonic properties are important conscious\nchoices in our work to balance (1) the potential of exploiting the\nspatial locality via learning an SFC and (2) retaining properties that\nfacilitate efficient query processing. Violating any constraint breaks\nthe properties of the mapping function. As an example, consider\nğœƒ=[[1,4],[2,10]], where the first constraint is violated. Take\n4\n\nğ‘“(ğ‘¥)=8. There is no point that can be mapped to this z-address,\nindicating that the mapping function is injective but not bijective.\nIn another example, when ğœƒ=[[1,4],[2,2]], the second constraint\nis not satisfied. Consider data points ğ´=(1,1)andğµ=(1,2), then\nğ‘“(ğ´)=ğ‘“(ğµ)=3. Therefore, the mapping function is not bijective.\nLastly, when ğœƒ=[[1,4],[8,2]], the third constraint is violated.\nGivenğ´=(1,1)andğµ=(2,2), thenğ‘“(ğ´)=9>ğ‘“(ğµ)=6, but\nğ´âª¯ğµaccording to Definition 3. Therefore, the mapping function\nis not monotonic.\nWe note that the Z-order curve is hence a special instance of\nthe above monotonic SFC. Specifically, it corresponds to ğœƒ(ğ‘–)\nğ‘—=\n2(ğ‘—âˆ’1)Â·ğ‘‘+(ğ‘–âˆ’1). We also note that computing the above ğ‘“(Â·)is effi-\ncient asğ‘“(ğ‘¥)can be computed by â€œscramblingâ€ the bits of ğ‘¥accord-\ning toğœƒusing bit operations efficiently.\nAs different SFCs induce different linear ordering of the data\npoints in the dataset, they will result in different query processing\ncosts for a given workload. This motivates our learning of a good\nSFC (detailed in the next Section). Below we provide an example\nwith visualizations to demonstrate this.\nExample 3. In Figure 3, we build and visualize query processing\ncosts on five indexes based on different linearization methods on the\nsame dataset and query workload. We generated a set of random\npoints in a 2D space, and used the two randomly generated queries\n(denoted as thick black boxes) as the query workload.\nFigures 3(a) and Figures 3(b) are the usual row-major and column-\nmajor order, respectively. Figure 3(c) uses the ğ‘§-order, which results in\ntheZM-index . Figure 3(d) is the Flood , which is based on learnable grid\npartitioning of the space, and then orders the grids using a row-major\norder (with a rearranged dimension order). Finally, Figure 3(e) shows\nour proposed method LMSFC, which is based on a learned monotonic\nSFC.\nWe shade the data points accesses during the query processing in\nred.1As we can see from Figure 3, learned indexes, i.e., Flood and our\nLMSFC, access fewer data points thanks to the learning on the specific\ninstance.\n5 LMSFC INDEX CONSTRUCTION\n5.1 Overview\nBased on the parameterized SFC family defined in the previous\nsection, we propose a novel multi-dimensional index, LMSFC, based\non Learned M onotonic S pace F illing C urves.\nWe give a high-level sketch of our LMSFC method in Figure 4.\nIn our method, we first learn a good monotonic SFC, ğ‘“(ğ‘¥;ğœƒ)in\nFigure 4, which minimizes the query processing cost of the sampled\nquery workload. The learned SFC gives us a total order for the multi-\ndimensional data points. We then propose both an optimal dynamic\nprogramming-based and a sub-optimal yet faster heuristic paging\nalgorithm to load data points into pages (the thick red rectangles\nin Figure 4). Finally, we extract the smallest ğ‘§-addresses from each\npage to form a sorted array, and employ a state-of-the-art learned\nindex (e.g., pgm [ 8]) on that, which facilitates the lookup from\nğ‘§-addresses to pages.\n1For all the methods in the figure, we pack a fixed number of data points into each\npage and the shaded area are based on pages whose MBRs overlap with the query\nwindow, hence the seemingly irregular shape of the shaded areas.In addition, we also include several optimizations to speed up\nthe query processing. Specifically, we present a novel query split-\nting strategy to minimize the access to spurious pages due to the\ndimension reduction effect of the SFC mapping. We also extend the\nsort dimension optimization [26] to the page-level granularity.\nIn the rest of this section, we focus on the three steps (i.e., learn-\ning an optimal SFC, Cost-based Paging and Page-level Sort Di-\nmension) in this section and leave the query processing related\ntechniques to the next Section.\n5.2 Learning an Optimal SFC\nThe goal of learning a parameterized SFC is to find the ğœƒâˆ—such that\nthe resulting query processing cost is minimized, i.e., we formulate\nthis as an optimization problem as:\nğœƒâˆ—=arg min\nğœƒEğ‘âˆ¼QJQueryTime(ğ·,ğ‘;ğœƒ)K (2)\nwhereQrepresents the distribution of the query workload, ğ‘is\nan i.i.d. sample from the distribution, and ğ‘„ğ‘¢ğ‘’ğ‘Ÿğ‘¦ğ‘‡ğ‘–ğ‘šğ‘’()function\nreturns the actual query execution time on the given dataset.\nTo optimize Equation (2), we can first apply the finite sample\napproximation, i.e., by taking sampled queries from Qand replac-\ning the expectation with the sample average. Nevertheless, the\nğ‘„ğ‘¢ğ‘’ğ‘Ÿğ‘¦ğ‘‡ğ‘–ğ‘šğ‘’ function is hard to model or approximate accurately as\nit includes various complex optimizations. Furthermore, directly\nevaluating the ğ‘„ğ‘¢ğ‘’ğ‘Ÿğ‘¦ğ‘‡ğ‘–ğ‘šğ‘’ function incurs exorbitant costs.\nAlso, note that ğœƒis a high-dimensional discrete parameter (with\nconstraints), the number of choices of ğœƒ(and henceğ‘“(Â·)) is exponen-\ntial both in the dimensionality ğ‘‘and inğ¾, rendering it impossible\nto solve the optimization problem via brute force in practice.\nLemma 1. The number of different monotonic SFCs for ğ‘‘-dimensional\nspace is Î©(ğ‘‘!)ğ¾.\nProof. First, we consider a permutation ğœ‹=(ğ‘¥1,ğ‘¥2,...,ğ‘¥ğ‘‘)of\ntheğ‘‘dimensions, where each dimension has ğ¾bits. For dimension\nğ‘¥ğ‘—, itsğ¾bits can choose any of the remaining ğ¾ğ‘‘âˆ’(ğ‘—âˆ’1)Â·ğ¾bits.\nTherefore, the total number of choices is:\n\u0012ğ¾ğ‘‘\nğ¾\u0013\nÂ·\u0012ğ¾ğ‘‘âˆ’ğ¾\nğ¾\u0013\nÂ·...Â·\u0012ğ¾\nğ¾\u0013\n=(ğ¾ğ‘‘)!\n(ğ¾!)ğ‘‘=\u00121\n1Â·...Â·ğ¾\nğ¾\u0013\nÂ·\u0012ğ¾+1\n1Â·...Â·2ğ¾\nğ¾\u0013\nÂ·...Â·\u0012ğ¾ğ‘‘âˆ’ğ¾+1\n1Â·...Â·ğ¾ğ‘‘\nğ¾\u0013\nâ‰¥(ğ‘‘!)ğ¾\nâ–¡\nDue to the discrete nature, gradient descent style algorithms,\nused in previous learned index work [ 26], cannot be applied to solve\nthis optimization problem either. Furthermore, the finite-sample\napproximate also introduces a small yet avoidable noise to the\noptimization.\nIn view of the above challenges, we adopt the state-of-the-art\nBayesian optimization algorithm, SMBO [ 15], to approximately\nsolve the above optimization problem and return a high-quality\nSFC that has a low average query time. SMBO builds a surrogate\nmodel to approximate the relationship between the parameters and\nthe actual objective function value.\nThe learning process follows the standard SMBO learning frame-\nwork. We use the Random Forest model as the surrogate model\ninstead of the typical Gaussian Process model, which improves the\n5\n\n0 10 20 30 40 50 60020406080100(a) Row-major Order\n0 10 20 30 40 50 60020406080100 (b) Column-major Order\n0 10 20 30 40 50 60020406080100 (c)ğ‘§-order (i.e., ZM-index )\n0 10 20 30 40 50 60020406080100\n(d) Learned Grid + Row-major Order (i.e.,\nFlood )\n0 10 20 30 40 50 60020406080100(e) Learned Monotonic SFC (i.e., LMSFC)\nFigure 3: Visualizing Five SFC-based Indexes (Thick black boxes are queries, and data pages accessed during query processing\nare shaded in red)\nLearned Index Headerpoint , point , ...76\n Headerpoint , point , ...89\n Headerpoint , point , ...100\n Headerpoint , point , ...128\n Headerpoint, point , ...145\n...... ......-address = 130?  \nFigure 4: Overview of LMSFC Index Construction\nlearning speed and does not rely on the multi-dimensional Gaussian\nassumption and the choice of the kernel function. One of the advan-\ntages of using a surrogate model is that it is cheap to evaluate while\nalso capturing the approximation noise in the objective function.\nAlgorithm 1 illustrates the SMBO learning framework, where\nË†ğ‘€is the surrogate model we need to learn. Firstly, the surrogate\nmodel is built between the initial candidates and their performance\nbased on the ğ‘„ğ‘¢ğ‘’ğ‘Ÿğ‘¦ğ‘‡ğ‘–ğ‘šğ‘’ evaluation results (line 1). Secondly, the\nSMBO algorithm uses an acquisition function (e.g., the Expected\nImprovement [ 15]) computed on the surrogate model to suggest\nother candidates for evaluation in the next iteration by automat-\nically balancing exploitation and exploration (line 3). Then, we\nevaluate the objective function by building the index on sampled\ndatasets to save the evaluation time (line 4). By default, we conser-\nvatively use the 5% sampled dataset as the training dataset, which\ncan maintain both low query time and learning cost. More learning\nprocess experiments are presented in Section 7.9. The surrogate\nmodel gets updated during each iteration (line 5). Finally, we choose\nthe candidate with the least cost in terms of the objective function.Algorithm 1: SMBO learning framework\nInput: training dataset ğ·, training query set ğ‘„, max iteration\nmaxIters\nOutput: an optimized order ğœƒğ‘ğ‘’ğ‘ ğ‘¡\n1Initialize Ë†ğ‘€,ğœƒğ‘ğ‘’ğ‘ ğ‘¡ andğ‘¦ğ‘ğ‘’ğ‘ ğ‘¡;\n2forğ‘–=0tomaxIters do\n3 Î˜ğ‘ğ‘ğ‘›ğ‘‘ğ‘  =SelectCands(Ë†ğ‘€,ğœƒğ‘ğ‘’ğ‘ ğ‘¡);/* selects candidates\nbased on the surrogate model */ ;\n4ğ‘Œğ‘ğ‘ğ‘›ğ‘‘ğ‘  =BatchEval(Î˜ğ‘ğ‘ğ‘›ğ‘‘ğ‘ ,ğ·,ğ‘„); /* evaluates the\nobjective function i.e., ğ‘„ğ‘¢ğ‘’ğ‘Ÿğ‘¦ğ‘‡ğ‘–ğ‘šğ‘’ */;\n5 Update Ë†ğ‘€based on(Î˜ğ‘ğ‘ğ‘›ğ‘‘ğ‘ ,ğ‘Œğ‘ğ‘ğ‘›ğ‘‘ğ‘ );\n6 ifğ‘šğ‘–ğ‘›(ğ‘Œğ‘ğ‘ğ‘›ğ‘‘ğ‘ )<ğ‘¦ğ‘ğ‘’ğ‘ ğ‘¡then\n7ğ‘¦ğ‘ğ‘’ğ‘ ğ‘¡â†min(ğ‘Œğ‘ğ‘ğ‘›ğ‘‘ğ‘ );\n8ğœƒğ‘ğ‘’ğ‘ ğ‘¡â†arg min(ğ‘Œğ‘ğ‘ğ‘›ğ‘‘ğ‘ );\n9returnğœƒğ‘ğ‘’ğ‘ ğ‘¡;\n5.3 Cost-based Paging\nIn order to accommodate external I/Os and allow for extra optimiza-\ntions2, we need to perform paging , which partitions the dataset ğ·\ninto multiple pages. As usual, we assume that each page has a max-\nimum size of ğµbytes and must satisfy a min fill factor constraint\nspecified by ğ‘“âˆˆ(0,1].3That is, the number of bytes used in each\npage must be within the range of [ğ‘“ğµ,ğµ]bytes.\nFinding optimal paging for multi-dimensional dataset is NP-\nhard [ 41], hence existing multi-dimensional indexing methods usu-\nally perform paging based on some heuristics, e.g., ğ‘…-tree and its\nvariants used the heuristics to minimize the margin, dead space\nand overlap area of the MBRs of the resulting pages [ 2,33]. Not\n2e.g., optimizations based on the MBR and/or the sort dimension of the pages.\n3Technically, we do allow at most one page to occupy less than ğ‘“ğµbytes.\n6\n\nsurprisingly, this practice is inherited by multi-dimensional indexes\nbased on SFCs. For example, RSMI [ 29] simply loads the maximum\nnumber of points into each page, which we term as fixed-sized\npaging .\nWe observe that paging is important and actually can be solved\noptimally for SFC-based multi-dimensional indexes. This is because\nwe can record the MBRs of the data points within each page and use\nthe MBR to further optimize the query processing. On one hand, a\npage can be skipped if a pageâ€™s MBR is disjoint with a query; on the\nother hand, if a pageâ€™s MBR is contained in a query, we can process\nthe data points on the page sequentially without other filtering\noverhead. In both cases, such optimizations are more likely if the\nMBR of a page is small. Default one-dimensional paging methods,\nsuch as the fixed-size paging method, are not aware of the MBR of\nthe pages and cannot perform active optimizations for it.\nBased on the above observations, we design a scoring function\nğ‘†(ğ‘ƒ)that is intuitively the density of a page ğ‘ƒ, orğ‘†(ğ‘ƒ)=ğ‘£ğ‘œğ‘™(ğ‘ƒ)\nğ‘ ğ‘–ğ‘§ğ‘’(ğ‘ƒ),\nwhereğ‘£ğ‘œğ‘™(ğ‘ƒ)andğ‘ ğ‘–ğ‘§ğ‘’(ğ‘ƒ)gives the volume of MBR of the page ğ‘ƒ\nand the number of data points in the page ğ‘ƒ, respectively.\nWe then formulate the optimal cost-based paging problem as\nfinding a paging solution, i.e., a partitioning ğ‘ƒdef={ğ‘ƒ1,...,ğ‘ƒğ‘˜(ğ‘ƒ)}\noverğ·(whereğ‘˜(ğ‘ƒ)denotes the number of resulting pages), such\nthat the total score of the solution ğ‘ƒis minimized, i.e.,\nğ‘ƒâˆ—=arg min\nğ‘ƒâˆ‘ï¸\nğ‘—âˆˆ{1,...,ğ‘˜(ğ‘ƒ)}ğ‘†(ğ‘ƒğ‘—),subject toğ‘ ğ‘–ğ‘§ğ‘’(ğ‘ƒğ‘—)âˆˆ[ğ‘“ğµ,ğµ]\nIn the following, we first give an algorithm to solve the above\nproblem optimally based on Dynamic Programming (DP), and then\ngive a sub-optimal but fast heuristic paging algorithm. Both meth-\nods achieve a better paging layout than the fixed-sized paging and\nhence improve the query performance.\n5.3.1 Dynamic Programming Paging Method. Thanks to the SFC\nwhich provides a linear order for the data points, we are able to\ncircumvent the NP-hardness of the multi-dimensional paging prob-\nlem by solving the one-dimensional paging problem optimally via\ndynamic programming.\nLetğ‘‚ğ‘ƒğ‘‡[ğ‘–]be the optimal cost obtained by an optimal cost-based\npaging algorithm for the first ğ‘–data points. Then we can derive the\nfollowing recurrent equations:\nğ‘‚ğ‘ƒğ‘‡[ğ‘–]=ğ‘†(ğ‘ƒğ‘ğ‘”ğ‘’(ğ·[1..ğ‘–])) ,ğ‘–<ğ‘“ğµ\n4ğ‘‘\nğ‘‚ğ‘ƒğ‘‡[ğ‘–]=min\nğ‘ âˆˆ[ğ‘“ğµ\n4ğ‘‘,ğµ\n4ğ‘‘](ğ‘‚ğ‘ƒğ‘‡[ğ‘–âˆ’ğ‘ ]+ğ‘†(ğ‘ƒğ‘ğ‘”ğ‘’(ğ·[ğ‘–âˆ’ğ‘ +1..ğ‘–])),otherwise\n, whereğ‘ƒğ‘ğ‘”ğ‘’(ğ‘§)denotes the page formed by a set of points, denoted\nasğ‘§and we assume each integer takes 4 bytes. Obviously, ğ‘‚ğ‘ƒğ‘‡[ğ‘›]\ngives the cost of the optimal paging for the entire dataset, and it is\neasy to use backtracking to report the optimal paging solution ğ‘ƒâˆ—.\nThe time complexity of the dynamic programming paging method\nisğ‘‚(ğ‘›ğµ\n4ğ‘‘)as the scoring function ğ‘†can be computed in ğ‘‚(1)time\nvia incremental computation. The above algorithm is implemented\nin Algorithm 2.\n5.3.2 Heuristic Paging Method. Although the DP algorithm is lin-\near inğ‘›, it is still time-consuming in practice as ğµis typically a\nlarge constant (e.g., ğµ=8192 in our experiment). There, we furtherAlgorithm 2: ğ·ğ‘¦ğ‘›ğ‘ğ‘šğ‘–ğ‘ğ‘ƒğ‘Ÿğ‘œğ‘”ğ‘Ÿğ‘ğ‘šğ‘šğ‘–ğ‘›ğ‘”ğ‘ƒğ‘ğ‘”ğ‘–ğ‘›ğ‘”ğ‘€ğ‘’ğ‘¡â„ğ‘œğ‘‘\nInput:ğ·is ağ‘‘-dimensional data array ordered by ğ‘-address, a fill\nfactor constraint ğ‘“, page size limit ğµ, and score function ğ‘†\nOutput: Return a page array ğ‘ƒ\n1ğ‘–â†1;\n2initializeğ‘‚ğ‘ƒğ‘‡ array;\n3whileğ‘–â‰¤|ğ·|do\n4 ifğ‘–<ğ‘“ğµ\n4ğ‘‘then\n5ğ‘‚ğ‘ƒğ‘‡[ğ‘–]â†ğ‘†(ğ‘ƒğ‘ğ‘”ğ‘’(ğ·[1..ğ‘–]));\n6 else\n7ğ‘‚ğ‘ƒğ‘‡[ğ‘–]â†\nmin\nğ‘ âˆˆ[ğ‘“ğµ\n4ğ‘‘,ğµ\n4ğ‘‘](ğ‘‚ğ‘ƒğ‘‡[ğ‘–âˆ’ğ‘ ]+ğ‘†(ğ‘ƒğ‘ğ‘”ğ‘’(ğ·[ğ‘–âˆ’ğ‘ +1..ğ‘–])));\n8ğ‘–â†ğ‘–+1;\n9ğ‘ƒâ†using backtracking on ğ‘‚ğ‘ƒğ‘‡ ;\n10returnğ‘ƒ;\npropose a heuristic paging method, which can achieve comparable\nquery performance and faster construction time compared with the\nDP method.\nThe heuristic algorithm is a greedy packing algorithm, which\npacks as many data points into the current page as possible until\nsome condition is violated. We give the pseudo-code of heuristic\npaging method in Algorithm 3. The condition stipulates that the\nnew MBR, formed by adding the current data point into the page\n(line 6), should notenlarge the old MBR by more than ğ›¼times (ğ›¼>1\nis a hyper-parameter) (line 7). This condition reduces the chance\nthat the MBR of the resulting page becomes too large (with respect\nto the number of data points within), where a large MBR may cause\nmuch dead space and increase the chance of intersecting with the\nqueries.\nAlgorithm 3: ğ»ğ‘’ğ‘¢ğ‘Ÿğ‘–ğ‘ ğ‘¡ğ‘–ğ‘ğ‘ƒğ‘ğ‘”ğ‘–ğ‘›ğ‘”ğ‘€ğ‘’ğ‘¡â„ğ‘œğ‘‘\nInput:ğ·is ağ‘‘-dimensional data array ordered by ğ‘-address, a fill\nfactor constraint ğ‘“, page size limit ğµ, andğ›¼is a float, used\nto control pageâ€™s MBR\nOutput:ğ‘ƒ\n1Initializeğ‘ƒarray;\n2Initialize a empty page ğ‘;\n3while Unvisited Points in ğ·do\n4 ifğ‘is empty then\n5 Loadğ‘“ğµ\n4ğ‘‘data points into ğ‘and updateğ‘â€™s mbr;\n6ğ‘šğ‘ğ‘Ÿâ€²â†the MBR ofğ‘containing the next data point;\n7 ifğ‘ ğ‘–ğ‘§ğ‘’(ğ‘)<ğµ\n4ğ‘‘andğ‘‰ğ‘œğ‘™ğ‘¢ğ‘šğ‘’(ğ‘šğ‘ğ‘Ÿâ€²)<ğ›¼Â·ğ‘‰ğ‘œğ‘™ğ‘¢ğ‘šğ‘’(ğ‘.ğ‘šğ‘ğ‘Ÿ)\nthen\n8 add the next point in ğ‘and updateğ‘.ğ‘šğ‘ğ‘Ÿ ;\n9 else\n10ğ‘ƒ.ğ‘ğ‘‘ğ‘‘(ğ‘);\n11 Initialize a empty page ğ‘;\n12if|ğ‘|>0then\n13ğ‘ƒ.ğ‘ğ‘‘ğ‘‘(ğ‘);\n14returnğ‘ƒ;\n7\n\n5.4 Page-level Sort Dimension\nFollowing Flood [26], we maintain the points in each page sorted\nin a chosen dimension named sort dimension . Unlike Flood where\nthe sort dimension is fixed for allpages, we allow using different\nsort dimensions in different pages, which provides more skipping\nopportunities to filter as many irrelevant points as possible when\nprocessing intersecting pages. This is because different sort dimen-\nsions may result in various sizes of the search area after refinement.\nThus, we can choose the sort dimension that can achieve the least\nsearch cost for each page. In a similar vein, we utilize the query\nworkload information to choose the sort dimension for each page\nas follows: for each page, we collect the set of intersecting queries\nin the query workload. We estimate the query cost using each of\ntheğ‘‘dimensions as the sorting dimension, and choose the one with\nthe least query cost. If there are no intersecting queries for a page,\nwe use a default order, which is determined in the same way as\nFlood .\nOnce we have determined the sort dimension for each page, we\norder the data points in the page by increasing order of the sort\ndimension. When we need to scan a page, we can first refine the\nsearch range (or physical storage range) according to the corre-\nsponding sort dimension. Specifically, given a window query ğ‘, the\nrange constraint over sort dimension ğ‘‘âˆ—isğ‘(ğ‘‘âˆ—)\nğ¿â‰¤ğ‘¥(ğ‘‘âˆ—)â‰¤ğ‘(ğ‘‘âˆ—)\nğ‘ˆ.\nThe points in each page are stored contiguously in increasing or-\nder of the corresponding sort dimension. Thus, we can use binary\nsearch or a one-dimensional index model to accelerate the search\nvia finding the lower-bound position of ğ‘(ğ‘‘âˆ—)\nğ¿and the upper-bound\nposition ofğ‘(ğ‘‘âˆ—)\nğ‘ˆin the physical storage range. As a result, points\nthat do not satisfy the range constraint on the sort dimension are\nfiltered out, which reduces the scanning overhead.\nBesides, sort dimension can also reduce the computation cost\nduring verification. Once we determine the search range, we can\nguarantee the points in this range are satisfied the queryâ€™s con-\nstraint over the sort dimension. Therefore, there is no need to verify\nthe value on the sort dimension, resulting in saved computation\noverhead.\n6 QUERY PROCESSING\nAs alluded to in Section 4, to answer a query ğ‘using an SFC-based\nindex, it takes two steps:\n(1)Projection: given a spatial query rectangle ğ‘, we determine its\nscan range in the ğ‘§-addresses as[ğ‘“(ğ‘ğ¿),ğ‘“(ğ‘ğ‘ˆ)]according to\nTheorem 1.\n(2)Scan with filtering: All pages whose ğ‘§-addresses fall within\nthe range need to be scanned. We can locate these pages using\nthe forward index. Additionally, as we maintain the MBR and\npage-specific sort dimension information for each page, we\ncan perform folklore optimizations such as skipping irrelevant\npages, and scanning only the relevant portion of the page.\nThe main limitation of the above framework is that it ignores\nthe potentially large numbers of false positive data points within\ntheğ‘§-address range due to the SFC mapping.\nExample 4. Consider the query (the black rectangle) in Figure 5(a).\nThe corresponding ğ‘§-address range can be decomposed into the blue\npart s(false positives) and the yellow part s. Scanning the whole range,\n0246810121402468101214\nqLqU(a) Learned Z-order w/o query splitting\n0246810121402468101214\nqLU\nLqU (b) Learned Z-order with query splitting\nFigure 5: Example of query partition (black rectangle is a\nquery window, yellow part only contains relevant points\nwithin the search range, blue part only contains irrelevant\npoints within the search range)\neven with filtering, incur much unnecessary overhead in accessing\nand filtering pages that contain only false positive points.\nThis phenomenon was also observed in a few methods such as\nUB-Tree [ 30], and a lazy skipping strategy was used. In this strategy,\ninstead of scanning all the pages in the ğ‘§-address range, it invokes\na skipping function, FindNextZaddress , after scanning the current\npage, to compute the next page that contains the first true positive\npoint with respect to ğ‘. While this strategy is guaranteed to skip all\nthe false positive pages , it has the following drawbacks: (I) It incurs\nsignificant overhead as this function has to be invoked for every\ntrue positive page. FindNextZaddress , technically, will compute the\nnextğ‘§-address (which may be virtual and does not correspond to\nany data point) after that of the last point in the current page, and\ntranslate it to a page. This will require accessing the forward index.\nAs we employ a learned index, this incurs a non-trivial overhead\nfor model estimation and local search. Even if we employ a ğµ+-tree,\nit may require accessing internal or leaf pages of the index. (II) The\nskipping function may only skip very few pages; in fact, in many\ncases, it will just return the next page.\nInstead, we propose a novel proactive skipping strategy based\non query splitting, which is especially efficient on monotonic SFCs\nsuch as ours. We illustrate its idea in the following example.\nExample 5. Consider the same query (the black rectangle) in Fig-\nure 5(a). We can split the query into two parts by cutting it at the\nvalue 8on theğ‘¥-axis, as illustrated in Figure 5(b). We still plot the\nfalse positive parts in blue. It reduces the number of false positive\nparts compared with the case without query splitting â€” e.g., the part\n[4,7]Ã—[ 12,15]is eliminated.\n6.1 Recursive Query Splitting\nWe start by introducing a procedure to compute the best way to split\na query into exactly two sub-queries (i.e., optimal 1-split), and then\nwe generalize it to obtain multiple sub-queries based on recursive\nsplitting.\nOptimal 1-Split Algorithm. Consider a query ğ‘that corresponds\nto ağ‘§-address range[ğ‘“(ğ‘ğ¿),ğ‘“(ğ‘ğ‘ˆ)]. Without loss of generality,\nassume that we split at the value ğ‘£on theğ›¿-th dimension. This\n8\n\nwill split the query into two sub-queries, with the corresponding\nğ‘§-address ranges as [ğ‘“(ğ‘ğ¿),ğ‘“(ğ‘ˆ)]and[ğ‘“(ğ¿),ğ‘“(ğ‘ğ‘ˆ)], respectively\n(See Figure 5(b)). We define the costof the split as ğ‘“(ğ‘ˆ)âˆ’ğ‘“(ğ‘ğ¿)+\nğ‘“(ğ‘ğ‘ˆ)âˆ’ğ‘“(ğ¿), or intuitively, the sum of the ğ‘§-address ranges of\nthe two resulting sub-queries. This cost function is chosen as it is\nhighly correlated with the actual query processing cost after the\nsplit and can be easily computed without accessing the data points.\nThen we formulate the optimal 1-split problem as finding the\nsplit (i.e., the dimension and the value) such that the cost of such\nsplit is the minimum.\nOr formally,\n(ğ›¿âˆ—,ğ‘£âˆ—)= arg min\nğ›¿âˆˆ[1,ğ‘‘],ğ‘£âˆˆ[ğ‘(ğ›¿)\nğ¿,ğ‘(ğ›¿)\nğ‘ˆ]ğ‘“(ğ‘ˆ)âˆ’ğ‘“(ğ‘ğ¿)+ğ‘“(ğ‘ğ‘ˆ)âˆ’ğ‘“(ğ¿)\nNote that both ğ‘ˆandğ¿are determined by ğ›¿andğ‘£, but we omit the\nnotational dependency for the easy of exposition.\nAsğ‘“(ğ‘ğ¿)andğ‘“(ğ‘ğ‘ˆ)are constants for the fixed query ğ‘, the\nabove minimization is equivalent to the following maximization\nproblem, i.e., finding the maximum â€œgapâ€ between ğ‘“(ğ‘ˆ)andğ‘“(ğ¿):\narg max\nğ›¿âˆˆ[1,ğ‘‘],ğ‘£âˆˆ[ğ‘(ğ›¿)\nğ¿,ğ‘(ğ›¿)\nğ‘ˆ]ğ‘“(ğ¿)âˆ’ğ‘“(ğ‘ˆ)\nPlugging in the definition of ğ‘“(Â·), it becomes:\narg max\nğ›¿âˆˆ[1,ğ‘‘],ğ‘£âˆˆ[ğ‘(ğ›¿)\nğ¿,ğ‘(ğ›¿)\nğ‘ˆ]ğ‘‘âˆ‘ï¸\nğ‘–=1ğ¾âˆ‘ï¸\nğ‘—=1ğœƒ(ğ‘–)\nğ‘—Â·(ğ¿(ğ‘–)\nğ‘—âˆ’ğ‘ˆ(ğ‘–)\nğ‘—) (3)\nFor a fixedğ›¿âˆˆ[1,ğ‘‘], we can find the optimal split value ğ‘£âˆ—as:\nğ‘£âˆ—=arg max\nğ‘£âˆˆ[ğ‘(ğ›¿)\nğ¿,ğ‘(ğ›¿)\nğ‘ˆ]ğ¾âˆ‘ï¸\nğ‘—=1ğœƒ(ğ›¿)\nğ‘—Â·(ğ¿(ğ›¿)\nğ‘—âˆ’ğ‘ˆ(ğ›¿)\nğ‘—)+ğ¶\n=arg max\nğ‘£âˆˆ[ğ‘(ğ›¿)\nğ¿,ğ‘(ğ›¿)\nğ‘ˆ]ğ¾âˆ‘ï¸\nğ‘—=1ğœƒ(ğ›¿)\nğ‘—Â·(ğ¿(ğ›¿)\nğ‘—âˆ’ğ‘ˆ(ğ›¿)\nğ‘—) (4)\nwhereğ¶=\u0010Ãğ‘‘\nğ‘–â‰ ğ›¿Ãğ¾\nğ‘—=1ğœƒ(ğ‘–)\nğ‘—Â·(ğ¿(ğ‘–)\nğ‘—âˆ’ğ‘ˆ(ğ‘–)\nğ‘—)\u0011\nis a constant.\nLemma 2. A solution to the optimization problem of Equation 4\nis(ğ‘(ğ›¿)\nğ‘ˆ>>ğ‘™)<<ğ‘™, whereğ‘™is the most significant bit of ğ‘(ğ›¿)\nğ¿âŠ•ğ‘(ğ›¿)\nğ‘ˆ,\nwhereâŠ•denotes XOR .\nTherefore, we can solve the optimal 1-split problem by finding\nthe optimal cut value for each of the ğ‘‘dimensions, hence, the\ncomplexity is only ğ‘‚(ğ‘‘).\nWe note that Lemma 2 holds because of the fact that ğ‘ˆ(ğ›¿)+\n1=ğ‘£=ğ¿(ğ›¿)and the fact that ğœƒğ‘—+1â‰¥2ğœƒğ‘—(derived easily from\nthe constraints introduced to guarantee the monotonic property).\nIf the monotonic property does nothold, then one may need to\ncheck every possible ğ‘£value to perform the optimization, hence\ntakingğ‘‚(âˆ¥ğ‘ğ‘ˆâˆ’ğ‘ğ¿âˆ¥1)time complexity, which means the resulting\nprocedure may be more expensive for â€œlargeâ€ queries.\nExample 6. Consider the example in Figure 5(a) again. The query\nğ‘=[4,11]Ã—[4,11], and the learned SFC corresponds to the parameter\nğœƒ=[[20,23,25,27],[21,22,24,26]]\nHence,ğ‘â€™sğ‘§-address range is[ğ‘“(ğ‘ğ¿),ğ‘“(ğ‘ğ‘ˆ)]=[48,207]. Our optimal\n1-split algorithm will first consider the ğ‘¥-axis. In this case, the most\nsignificant bit ğ‘™=3as(0100)2âŠ•(1011)2=(1111)2. Then theğ‘£âˆ—onthe axis is(1011)2>>3<<3=(1000)2=8, and then the cost of\nsplitting at 8 on the ğ‘¥-axis can be calculated.\nRecursive Splitting. As one split is often insufficient to reduce the\nnumber of irrelevant pages, we adopt our optimal 1-split algorithm\nrecursively to divide the query window into multiple parts. In our\nimplementation, the stopping condition is set as either reaching a\nrecursion depth of ğ‘˜maxsplit or when there is no gap to split. ğ‘˜maxsplit\nis a parameter that can balance the number of index accesses with\nthe skipping opportunity of disjoint pages. A higher ğ‘˜maxsplit can\neffectively filter out disjoint pages but causes more index access\noverhead. Conversely, a lower ğ‘˜maxsplit saves the cost on index\naccess but may not eliminate enough irrelevant pages. We provide\nthe pseudo-code of Recursive Query Splitting in Algorithm 4. In\neach recursion, we find the optimal cut for each ğ‘‘dimension and\nthen we choose the split with the maximum â€œgapâ€ (line 5). Based on\nthe split value ğ‘£âˆ—and corresponding dimension ğ›¿, a spatial rectangle\ncan be split into two and passed into the next iteration (lines 6-8).\nAlgorithm 4: ğ‘…ğ‘’ğ‘ğ‘¢ğ‘Ÿğ‘ ğ‘–ğ‘£ğ‘’ğ‘„ğ‘¢ğ‘’ğ‘Ÿğ‘¦ğ‘†ğ‘ğ‘™ğ‘–ğ‘¡ğ‘¡ğ‘–ğ‘›ğ‘”\nInput:ğ‘is a spatial rectangle, ğ‘˜is an integer, used to represent the\nnumber of remaining partition times\nOutput: Return an array of search range ğ‘ğ‘ \n1Initializeğ‘ğ‘ array;\n2ifğ‘˜=0âˆ¨there is no gap to split ğ‘then\n3ğ‘ğ‘ .ğ‘ğ‘‘ğ‘‘(ğ‘);\n4else\n5 Find the optimal cut ğ‘£âˆ—and corresponding dimension ğ›¿based\nonğ¿ğ‘’ğ‘šğ‘šğ‘ 2;\n6ğ‘1,ğ‘2â†splitğ‘based onğ‘£âˆ—andğ›¿;\n7ğ‘Ÿ1â†ğ‘…ğ‘’ğ‘ğ‘¢ğ‘Ÿğ‘ ğ‘–ğ‘£ğ‘’ğ‘„ğ‘¢ğ‘’ğ‘Ÿğ‘¦ğ‘†ğ‘ğ‘™ğ‘–ğ‘¡ğ‘¡ğ‘–ğ‘›ğ‘” (ğ‘1,ğ‘˜âˆ’1);\n8ğ‘Ÿ2â†ğ‘…ğ‘’ğ‘ğ‘¢ğ‘Ÿğ‘ ğ‘–ğ‘£ğ‘’ğ‘„ğ‘¢ğ‘’ğ‘Ÿğ‘¦ğ‘†ğ‘ğ‘™ğ‘–ğ‘¡ğ‘¡ğ‘–ğ‘›ğ‘” (ğ‘2,ğ‘˜âˆ’1);\n9 For each spatial rectangle from ğ‘Ÿ1andğ‘Ÿ2, we add it into ğ‘ğ‘ ;\n10returnğ‘ğ‘ ;\n7 EXPERIMENTS\n7.1 Experimental Settings\nDatasets We use three real-world datasets with different charac-\nteristics in our experiments (See Table 2) and they are also used in\nthe previous work. We preprocess the datasets to scale up all coor-\ndinates to integers and remove duplicates. OSM is a spatial dataset\nconsisting of 250M records randomly sampled from North America\nin the OpenStreetMap dataset4. We use the GPS coordinates (i.e.,\nlongitude and latitude) to form a 2D dataset. NYC is randomly\nsampled from records of yellow taxi trips in New York City in 2018\nand 20195. We used the pick-up locations, trip distances, and total\namounts to form a 3D dataset. STOCK consists of daily historical\nstock prices from 1970 to 20186. We select four features: the high\nprice, the low price, the adjusted close price, and trading volume.\nQuery Workload As the datasets do not come with their query\nworkloads, we generate the default query workloads as follows.\n4https://download.geofabrik.de/\n5https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n6https://www.kaggle.com/ehallmar/daily-historical-stock-prices-1970-2018\n9\n\nTable 2: Dataset Characteristics\nğ‘›(#-of-Points) ğ‘‘(#-of-Dimensions) Size (GB)\nOSM 250M 2 1.95\nNYC 30M 3 0.35\nSTOCK 30M 4 0.47\nA query is parameterized by its center and its range in every\ndimension. We generate query centers in one of the two modes:\n(i)Skewed , in which query centers are randomly sampled data\npoints. (ii) Uniform , in which query centers are randomly sampled\nwithin the data space. The width of the queries in each dimension\nis uniformly sampled from zero to the width of the data space of\nthat dimension scaled by 0.05. All query windows are clipped to be\nwithin the data space.\nFollowing [ 39], the final query workload is obtained by mixing\n90% of the skewed queries with 10% uniform queries. The resulting\nselectivities for the three datasets are about 0.7%, 0.07%, 0.01%,\nrespectively. For each dataset, we generate a training and a test\nquery workload of sizes 1000 independently. All queries use the\nCOUNT aggregate function, i.e., reporting the number of data points\nwithin the query window.\nAlgorithms We compare our proposed LMSFC with the follow-\ning algorithms:\nâ€¢ZM-index [38] combines the fixed ğ‘§-order curve and learned\nindex, together with the fixed-size paging.\nâ€¢ğ‘…âˆ—-tree [2] is a traditional and widely-used multi-dimensional\nindex. We use the implement ğ‘…âˆ—-tree in the Boost C++ Libraries\n(https://www.boost.org).\nâ€¢Flood [26] is another state-of-the-art learned index for multi-\ndimensional data, which learns an optimal configuration of multi-\ndimensional grids for a given data and query workload. It can be\napproximately viewed as a variable-size SFC that follows row-\nmajor order for an appropriate permutation of the ğ‘‘dimensions.\nWe adapt the Flood method with fixed-size paging for a fair\ncomparison.\nWe note that these algorithms represent prior state-of-the-art in-\ndexes in different categories. For example, Flood has outperformed\nother multi-dimensional indexes, such as Grid File [ 27], kd-tree [ 3],\nUB-tree [ 30] and Hyperoctree [ 24], in their experimental evaluation.\nWe do not consider other learned multi-dimensional indexes such as\nLISA [22] ,RSMI [29], as the original codes have various limitations\nor did not achieve competitive performance in our experiments.\n[31] is another related work, which learns a quadtree and ap-\nplies ağ‘§-order variant on each node to adapt query workload. One\ndifference is that LMSFC learns the mapping between data points\nand addresses, so we can directly locate the search range rather\nthan traversing a tree structure. In addition, we use a BO algorithm\nto learn better ordering. Another difference is that we directly use\nthe actual query time as the metric rather than the number of false\npositive points used in [ 31]. Since the performance of [ 31] is even\nworse than the baseline model (i.e., ZM-index ), we do not include\nit in our experiment.\nWe experimented with Tsunami [6] but did not report its perfor-\nmance here, because its splitting algorithm does not result in anysplit in any dimension on our query workloads7, in which case,\nTsunami â€™s performance degrades to that of Flood .\nWe use C++to implement all the methods. To compare these\nmethods fairly, we run all the experiments either in the in-memory\nmode (for those that do not support external I/O) or in the warm\nbuffer mode (for those that support external I/O). The page size\nis set toğµ=8192 bytes, and the min fill factor ğ‘“=0.25. For the\none-dimensional space mapped from all SFCs ( ğ‘§-order or learned\nSFCs), we use 64 bits, and ğ¾=\u000464\nğ‘‘\u0005. And we empirically choose\nğ‘˜maxsplit = 4 in recursive query splitting.\nFor learned indexes, we use the PGM [ 8] as a one-dimensional\nlearned index since PGM achieves the competitive range query\nperformance in the static dataset and is easily embedded in the dif-\nferent learned multi-dimensional indexes. The error bound in PGM\nis empirically set to 128, which is robust for different configurations\nand datasets.\nAll the experiments are performed on a machine with i9-7900X\nCPU @ 3.30GHz and 64 GB main memory running Ubuntu 20.04.4.\n7.2 Query Performance\nOSM NYC STOCK102103Avg Query Time ( s)\nZM-index Flood LMSFC R*tree\nFigure 6: Query Performance\nIn this section, we compare LMSFC with a traditional multi-\ndimensional index and other learned multi-dimensional indexes for\nmulti-dimensional range queries.\nFigure 6 reports the query time for different indexes on each\ndataset. LMSFC outperforms all other indexes across all the datasets.\nLMSFC achieves between 1.52 Ã—and 1.96Ã—speedup on query time\ncompared with the runner-up. The main advantage of LMSFC over\nthe baseline (i.e., the Z-order curve) is that the learned SFC preserves\nmulti-dimensional locality better after the mapping into the one-\ndimension address space. Consequently, close-by points in the multi-\ndimensional space are more likely assigned into the same page.\nThis leads to pages with smaller/compact MBRs, hence fewer page\naccesses when answering range queries. We note that LMSFC is\nmuch faster than the ZM-index , achieving 3.8Ã—speedup on the\nOSM dataset. This demonstrates the huge potential of a learned\nSFC versus a fixed SFC as this is the key difference between the two\nindexes. All learned multi-dimensional indexes are significantly\nsuperior to the traditional multi-dimensional index ğ‘…âˆ—-tree . This\nconfirms that there is a need to incorporate ML-based methods into\ndatabase components to improve the performance.\n7The skewness of our workload is based on data distribution while Tsunami is not.\n10\n\nIn addition, we further investigate false positive (FP) records\nscanned by each index. In OSM dataset, the number of scanned FP\npoints byğ‘…âˆ—-tree ,ZM-index ,Flood and LMSFC are 60940, 72291,\n25947, and 19067 separately per query. ZM-index scans more FP\npoints than ğ‘…âˆ—-tree sinceğ‘…âˆ—-tree utilizes a heuristic method to\nachieve good clustering during packing. By using a query workload\nas prior knowledge, Flood and LMSFC can adapt their structures to\naccess fewer FP points. Besides, LMSFC applies page optimizations\nto further reduce FP points. Thus, LMSFC achieves the smallest\nnumber of false positive points being scanned. In the other two\nhigher dimensional datasets, LMSFC shows superiority in this met-\nric, which are 17.4Ã—and 11.1Ã—less thanğ‘…âˆ—-tree , 3.8Ã—and 5.0Ã—less\nthan Flood , and 10.1Ã—and 6.4Ã—less than ZM-index . This is because\ndata points are sparse in the higher dimensional data space, result-\ning in poor clustering during paging. Thus, paging optimization\nneeds to be considered in multi-dimensional indexes.\n7.3 Selectivity\n104\n103\n102\n101\n100\nQuery Selectivity (%)102103Avg Query Time ( s)\nR*-tree\nZM-index\nFlood\nLMSFC\nFigure 7: Varying Query Se-\nlectivity\n104105106107108\nDataset Size100101102103Avg Query Time ( s)\nR*-tree\nZM-index\nFlood\nLMSFCFigure 8: Varying Dataset\nSize\nNext, we study the performances of all the methods with respect\nto query selectivity. We vary the selectivity from 0.0001% to 1% by\nuniformly scaling the query windows accordingly.\nFigure 7 shows the result on the OSM dataset (and similar results\nare present in other datasets), where both axes are in logarithmic\nscales. For all the methods, the query times grow approximately\nlinearly with the query selectivity since more data points are ac-\ncessed. We notice a deterioration with the ğ‘…âˆ—-tree when the query\nselectivity becomes large; this may be due to the fact that when\nthe query window grows, it is more likely to intersect with more\npages, and hence more page access and backtracking.\nConceptually, learned indexes only need to scan pages within\na certainğ‘§-address range, and there is no complex intersecting\nMBR test or back-tracking. However, ğ‘…-tree and its variants need\nto perform more complex MBR intersection queries per inner node\nand may result in many back-tracking (esp., for higher dimensional\ncases).\nZM-index achieves similar performance with ğ‘…âˆ—-tree , but per-\nforms consistently across the selectivity range. Flood has a sig-\nnificant improvement consistently over ZM-index , while LMSFC\nfurther improves the query performance consistently, demonstrat-\ning the wide applicability of learned indexes.\n7.4 Dataset Scalability\nTo investigate the scalability, we sub-sample the OSM dataset to\ncreate datasets of the same distribution but with varying sizes.Figure 8 shows the result, where both axes are in logarithmic scales.\nWe can see that all the indexes scale approximately linearly with\nthe data size, whereas LMSFC performs the best, followed by Flood ,\nZM-index , and finally ğ‘…âˆ—-tree . We also investigated the reason why\nFlood behaves noticeably worse than expected for 10M data points.\nIt is partly due to the sub-optimal configuration it learned from the\nsampled dataset. If we allow Flood to learn from, e.g., 30% of the\ndataset, the resulting performance matches the approximate linear\ntrend much more closely.\n7.5 Aspect Ratio\nWe investigate how the performance varies with the aspect ratio\nof the query window. We fix the selectivity to be 1% and then vary\nthe aspect ratio from 0.125 to 8.0. The aspect ratio is defined as\nthe ratio of the width of two dimensions of the query window. For\ndatasets of more than 2D (i.e., NYC and STOCK), we randomly\nselect one dimension, called variable dimension , for a given dataset\nto enforce the aspect ratio. We then start with a query window of\nequal size on all dimensions, and then modify the length of the\nvariable dimension to satisfy the ratio constraint. Finally, with the\naspect ratio fixed, we scale the query window to enforce the same\nselectivity. For example, the three sides of a 3D query window with\nratio of 4 and 0.25 will have a side length ratio of 4:1:1 and 0.25:1:1,\nrespectively, assuming the first dimension is the variable dimension.\nAs shown in Figure 9, LMSFC offers the fastest query speed\namong all the indexes. The two learned indexes, LMSFC and Flood\nshow much more stable performance than the other two non-\nlearned indexes, demonstrating that learned indexes can adapt\nwell to the query workload to achieve consistent and superior per-\nformance. Furthermore, we notice that LMSFC outperforms Flood\nacross all settings, especially in the STOCK dataset, which is partly\ndue to the fact that Flood has to learn a(ğ‘‘âˆ’1)-dimensional grid,\nwhich is harder for larger ğ‘‘. Finally, we notice that there are cases\nwhere non-learned indexes behave significantly worse even for\nâ€œsymmetricâ€ aspect ratios. E.g., on the NYC dataset, ğ‘…âˆ—-tree â€™s per-\nformance is almost 3x at aspect ratio of 8.0 as compared with that\nat aspect ratio of1\n8.0, demonstrating the need for learned indexes\nfor multi-dimensional datasets.\n7.6 Ablation Study\nIn this section, we investigate the impact of different optimization\ncomponents in LMSFC on the performance by performing ablation\nstudies.\nWe compared the following variants of the proposed method:\nâ€¢ZM-index . This baseline uses the fixed ğ‘§-order curve with a\nlearned index, with fixed-size paging.\nâ€¢LO. We replace the ğ‘§-order in ZM-index by our learned SFC.\nâ€¢LO + C1 . On top of LO, we add the sort dimension optimization\n(SD).\nâ€¢LO + C2 . On top of LO + C1 , we add the Recursive Query Splitting\n(RQS) optimization.\nâ€¢LMSFC. This is our proposed method, which has Dynamic Pro-\ngramming Paging (DP) optimization added to LO + C2 .\nFigure 10 illustrates that adding more components can consis-\ntently and continuously improve the baseline model. Across all\n11\n\n5000510052005300\n5006007008009001000\n1 4 80100200300\nAspect RatioAvg Query Time ( s)\n0.1250.25R*-tree ZM-index Flood LMSFC(a) OSM\n4000500060007000800090001000011000\n1 4 801000\nAspect RatioAvg Query Time ( s)\n0.1250.25R*-tree ZM-index Flood LMSFC (b) NYC\n1 4 8\nAspect Ratio5007501000125015001750200022502500Avg Query Time ( s)\n0.1250.25R*-tree ZM-index Flood LMSFC (c) STOCK\nFigure 9: Different Aspect Ratio\nOSM NYC STOCK050100150200250300350400Avg Query Time ( s)\nZM-index LO LO + C1 LO + C2 LMSFC\nFigure 10: Ablation Study\ndatasets, learned ğ‘§-order ( LO) almost achieves the biggest improve-\nment on ZM-index . This is because learned ğ‘§-order has the ability\nto adapt given query workload via optimizing.\nWe notice that LO + C1 andLO + C2 cannot improve too much\non STOCK dataset since the multi-dimensional data is too sparse\nin the higher dimensionality dataset. As they still use fixed-size\npaging, points in each page are more scattered and thus form a\nmuch larger MBR. As a result, sort dimension optimization cannot\nskip too many points and more pages intersect with the query\nwindow. Note that LMSFC replaces the fixed-size paging with DP-\nbased paging, and the above issue is alleviated, hence the noticeable\nperformance improvement.\n7.7 Query Splitting\nTable 3: Recursive Query Splitting ( RQS ) vs FindNextZad-\ndress ( FNZ)\nModel Index Accesses Avg Query Time ( ğœ‡ğ‘ )\nZM-index +RQS 18 306\nZM-index +FNZ 1807 380\nLMSFC + RQS 19 109\nLMSFC + FNZ 1927 206\nWe investigate the impact of different query splitting strate-\ngies. Existing Z-order-based indexes either do not use any querysplitting [ 26,38] or use the query splitting method via repeated\ninvocation of the FindNextZaddress (FNZ) function first proposed\nin [37] and had been used in UB-tree [ 30]. We name the recursive\nquery splitting method in our proposal as the RQS.\nWe experiment with the two splitting strategies on both ZM-\nindex and LMSFC and show the results in Table 3, where â€œindex\naccessesâ€ record the average number of times the forward index is\naccessed to perform the ğ‘§-address to page lookup (See explanation\nin Section 6). We can see that our RQS outperforms FNZ and the\nimprovement is especially significant for LMSFC. This is mainly\nbecause FNZ is invoked for every page that intersects the query\nand hence causes great overhead.\nTable 4: Effect of different ğ‘˜maxsplit\nğ‘˜maxsplit Avg Irrelevant Pages Avg Query Time ( ğœ‡ğ‘ )\n0 16991 150\n1 7531 126\n2 4359 117\n3 2478 112\n4 1288 109\n5 523 113\nWe further investigate the effects on different ğ‘˜maxsplit . Table 4\nshows we can achieve the best average query performance when\nğ‘˜maxsplit =4. Although larger ğ‘˜maxsplit can avoid scanning consid-\nerable irrelevant pages, the query performance slightly degrades.\nThis is because the query window has been divided into too many\nparts, which significantly increases the overhead on accessing the\nlearned index.\n7.8 Paging Methods\nWe investigate different paging methods on the OSM dataset, and\nthe results are shown in Table 5. For both ZM-index and LMSFC,\nDP paging shows the best query performance since DP can mini-\nmize our score function, which intuitively corresponds to relatively\ndensely packed pages.\nThis helps to reduce the dead space within the page as well as\ndecrease the probability of overlapping with the queries. Note that\n12\n\nTable 5: Comparing Different Paging Methods (FP, HP, and\nDP stands for fixed-size paging heuristic paging and dynamic\nprogramming paging, respectively)\nModel Avg Query Time ( ğœ‡ğ‘ ) Index Size (MB)\nZM-index + FP 419 6.8\nZM-index + HP 330 8.5\nZM-index + DP 309 8.8\nLMSFC + FP 150 6.8\nLMSFC + HP 116 7.4\nLMSFC + DP 109 7.7\nHP is only slightly worse than DP, but typically with much faster\npacking time (e.g., 35 seconds for HP versus 546 seconds for DP).\n7.9 Index Learning Process\n1% 5% 10% 15% 20%\nSampled Dataset Rate050100150200250Avg Query Time ( s)\nAvg Query Time\n01500300045006000\nLearning Time (s)Learning Time\nFigure 11: Varying Dataset\nSize\n100 500 1000 1500 2000\nSampled Query Workload Size0306090120 Avg Query Time ( s)\nAvg Query Time\n0800160024003200\nLearning Time (s)Learning TimeFigure 12: Varying Work-\nload Size\nLearning a good SFC using the entire dataset and query work-\nload is infeasible due to the prohibitively long time on sorting the\ndataset according to the given SFC. Thus, we use sampled datasets\nand workloads in the learning process, which can significantly re-\nduce the learning cost without significantly degrading the query\nperformance. Figure 11 and Figure 12 illustrate the learning cost and\nquery performance on OSM (other datasets show a similar trend)\nvia varying different sample dataset sizes and query workloads\nsizes over several trials (minimal and maximum result is shaded).\nIn Figure 11, when we sample a small portion of the dataset, the\nperformance has a large variance. Although a larger sample rate\ncan achieve better performance, the learning process is quite long.\nThus, adopting a 2.5%-7.5% sampled rate is good enough to maintain\nboth fast query time and low learning cost. Even if we reduce the\nsample rate to 0.5%, we can still achieve better performance than\nFlood but incur only half the learning time.\nBased on the 5% sampled dataset, we conduct the experiment on\nvarying query workload sizes to observe whether a large workload\nsize can achieve better query performance. The results are displayed\nin Figure 12. When a workload size is larger than 500, we can achieve\nrobust performance.\n7.10 Index Size and Index Construction\nIn Table 6, we report the index sizes for the three datasets. All\nthe index sizes are small relative to the respective data size. Our\nproposed LMSFC has a larger index size than ZM-index orFlood\npartly because we have optimized page layouts so that pages are\nnot fully filled. Nonetheless, our index size is still acceptable as it is\nstill significantly smaller than the traditional ğ‘…âˆ—-tree .Table 6: Index Size (MB)\nOSM NYC STOCK\nğ‘…âˆ—-tree 26.7 9.8 8.9\nFlood 0.9 0.2 0.4\nZM-index 6.8 1.6 2.6\nLMSFC 7.7 2.0 4.4\nTable 7: Index Learning and Construction Times (Seconds)\nOSM NYC STOCK\nğ‘…âˆ—-tree 9651 708 864\nZM-index 35 5 5\nFlood Learning 73 121 431\nFlood Building 44 6 10\nLMSFC Learning 1821 672 879\nLMSFC Building 546 87 117\nWe present the index construction times in Table 7. For learned\nindexes, we further distinguish the learning time and the index\nbuilding time. ğ‘…âˆ—-tree suffers from high index construction time in\nthe large dataset because its construction requires optimizing some\ncriteria (e.g., dead space, margin, the overlap between two pagesâ€™\nMBR) for each page. ZM-index has the fastest construction time\nas there is no learning or optimization involved. Flood has faster\nlearning and building times than LMSFC, because (i) Flood â€™s model\nis simpler in that the hyper-parameter space is much smaller than\nours. In addition, it also optimizes against a learned cost model,\nhence the hyper-parameter search is faster. (ii) Our LMSFC also\nincludes other optimizations (such as dynamic programming-based\npaging), hence affecting the index building time. Nonetheless, the\nindex construction is done once for a dataset.\nSimilar to Flood, if we can collect the training examples from\nhistory, we can train an offline cost model to select the learned\nSFC with low overhead. We use history instances as training ex-\namples to fit a neural network then we freeze the parameters of\nthe model. During learning SFC, we can utilize gradient descent to\ndirectly adjust the input to find the optimum. Consequently, the\nlearning process only takes a few minutes and the performance is\ncompetitive with the proposed BO algorithm.\n7.11 Handling Data Updates\nOur method can also handle updates by employing an updatable\nforward index (e.g., ALEX [ 5] or LIPP [ 40]). In fact, we can employ\nthe traditional ğµ+-tree index as the forward index as well.\nIn this section, we adopt ALEX as the forward index since PGM\nhas not an inherent update method8. Deletion can simply mark a\nrecord as \"deleted\". We only discuss insertion here. Once receiving\nan insertion query, LMSFC first locates the data page where the\nnewly inserted point should be located according to its mapped\nvalue. If there is still space, we insert this point into the page and\n8PGM adopts LSM-based methods to handle updates. Although the LSM tree shows\ngood write performance, it performs worse in range queries because all sorted runs in\nthe LSM tree are scanned [32].\n13\n\nbase 20% 40% 60% 80% 100%\nInserted Points Percentage102103104Avg Query Time ( s)\nRtree\nR*treeLMSFC\nLMSFCbFlood\nZM-indexLMSFCaFigure 13: Query Time vs\nInsert Points Percentage on\nOSM dataset\n20% 40% 60% 80% 100%\nInserted Points Percentage01000200030004000Througput (thousand ops/sec)\nRtree\nR*treeLMSFC\nLMSFCbFlood\nZM-indexFigure 14: Throughput vs\nInsert Points Percentage on\nOSM dataset\nupdate the meta-data. Otherwise, we evenly separate the page into\ntwo based on the ğ‘§-address and we also need to update the meta-\ndata of two pages. We initialize and learn LMSFC with the 128M\nsampled data from the OSM dataset. We compare with two R-tree\nvariants,ğ‘…âˆ—-tree and R-tree with the linear split method (denoted\nas R-tree), and we also implement an updatable Flood and ZM-\nindex (we also use ALEX to handle data updates) for comparison.\nWe insert 10%n to 100%n data points ( ğ‘›=128ğ‘€) and we test the\nwindow query performance and report the average query time and\nthroughput of the insertion query workload.\nThe results of query performance after each insertion are shown\nin Figure 13. We observe that updatable learned indexes still show\nbetter query time than traditional index models. LMSFC achieves\nthe best query performance than other index models. We recognize\nthat while such handling of updates is feasible, it may deteriorate\nthe quality of our index gradually over the long run. Thus, after\neach 10% data insertion, we trigger a learning process to find better\nlearned ordering in LMSFC, named LMSFCa. Thus, the performance\ncan be further improved. Although this incurs extra learning over-\nhead, the learning phase can happen in a separate instance in a\nreal-world system. Thus, the learning phase does not block the\nincoming queries.\nFigure 14 demonstrates high throughput on the learned indexes.\nDue to the heuristic split method, R-tree variants show lower\nthroughput. We observe the low throughput of LMSFC when we\ninsert the first 10% data. This is because lots of overflow pages need\nto be split. We can find a similar trend in ZM-index , but ZM-index\nshows the highest throughput due to the fact that ZM-index does\nnot include any optimization in each page (i.e., page-level sorting).\nAnother orthogonal idea that supports the insertion in the learned\nindex is to maintain a delta index to handle data insertions and\ndelay the merge/split process [ 35]. We borrow the idea of delayed\nupdate policy and design an LMSFC variant named LMSFCb. In\neach data page, we maintain a low-capacity (i.e., half of page size)\nunsorted delta array to handle overflow in each page. Once the\nsize of the delta array exceeds the predefined threshold, we trigger\nthe same splitting method that the updatable LMSFC does. The\nadvantage of this method is that the delayed update policy can\nimprove the insertion throughout while spending a low scan cost of\nthe delta array. Therefore, Figure 13 and Figure 14 report that LMS-\nFCb achieves competitive query performance and better insertion\nthroughput.8 CONCLUSIONS AND FUTURE WORK\nIn this paper, we study the problem of learned indexes for multi-\ndimensional data based on learned space-filling curves. We devised\na framework of learning a special class of space-filling curves that\nis amenable to efficient query processing. In addition, we perform\nboth offline and online optimizations by optimizing the data place-\nment into pages and query splitting to further improve the query\nprocessing efficiency. Extensive experimental results demonstrate\nthat the proposed method outperforms both non-learned indexes\n(such asğ‘…âˆ—-tree ) and prior state-of-the-art learned multi-dimen-\nsional indexes (such as ZM-index andFlood ) across a wide range\nof settings on three real-world datasets.\nAlthough we focus on learned monotonic SFCs in this paper, our\nidea and methods can be easily generalized to obtain learned non-\nmonotonic SFCs. For example, by dropping the constraints on ğœƒ,\nour method can learn a non-monotonic SFC. For another example,\nwe can consider other parameterized SFC families that generalize\nother well-known SFCs, such as the Hilbert Curve. We leave such\nexploration for future work.\nACKNOWLEDGMENTS\nWei Wang was supported by HKUST(GZ) Grant G0101000028,\nGZU-HKUST Joint Research Collaboration Grant GZU22EG04, and\nGuangzhou Municipal Science and Technology Project (No. 2023A0\n3J0003). Xin Cao was supported by ARC DP230101534. Jian Gao\nwould like to express his heartfelt gratitude to his girlfriend (Shi-\nman Wu) for her unwavering support throughout the completion\nof this research paper.\nREFERENCES\n[1] Abdullah-Al-Mamun, Ch. Md. Rakin Haider, Jianguo Wang, and Walid G. Aref.\n2022. The â€œAI+Râ€-tree: An Instance-optimized R-tree. In MDM .\n[2]Norbert Beckmann, Hans-Peter Kriegel, Ralf Schneider, and Bernhard Seeger.\n1990. The R*-Tree: An Efficient and Robust Access Method for Points and\nRectangles. In Proceedings of the 1990 ACM SIGMOD International Conference\non Management of Data, Atlantic City, NJ, USA, May 23-25, 1990 . ACM Press,\n322â€“331. https://doi.org/10.1145/93597.98741\n[3] Jon Louis Bentley. 1975. Multidimensional Binary Search Trees Used for Asso-\nciative Searching. Commun. ACM 18, 9 (1975), 509â€“517. http://doi.acm.org/10.1\n145/361002.361007\n[4] Angjela Davitkova, Evica Milchevski, and Sebastian Michel. 2020. The ML-Index:\nA Multidimensional, Learned Index for Point, Range, and Nearest-Neighbor\nQueries. In Proceedings of the 23rd International Conference on Extending Data-\nbase Technology, EDBT 2020, Copenhagen, Denmark, March 30 - April 02, 2020 .\nOpenProceedings.org, 407â€“410. https://doi.org/10.5441/002/edbt.2020.44\n[5]Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\nDavid B. Lomet, and Tim Kraska. 2020. ALEX: An Updatable Adaptive Learned\nIndex. In Proceedings of the 2020 International Conference on Management of Data,\nSIGMOD Conference 2020, online conference [Portland, OR, USA], June 14-19, 2020 .\nACM, 969â€“984. https://doi.org/10.1145/3318464.3389711\n[6]Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. 2020.\nTsunami: A Learned Multi-dimensional Index for Correlated Data and Skewed\nWorkloads. Proc. VLDB Endow. 14, 2 (2020), 74â€“86. http://www.vldb.org/pvldb/v\nol14/p74-ding.pdf\n[7]Yihe Dong, Piotr Indyk, Ilya P. Razenshteyn, and Tal Wagner. 2020. Learning\nSpace Partitions for Nearest Neighbor Search. In 8th International Conference\non Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 .\nOpenReview.net. https://openreview.net/forum?id=rkenmREFDr\n[8] Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-index: a fully-dynamic\ncompressed learned index with provable worst-case bounds. Proc. VLDB Endow.\n13, 8 (2020), 1162â€“1175. http://www.vldb.org/pvldb/vol13/p1162-ferragina.pdf\n[9]Raphael A Finkel and Jon Louis Bentley. 1974. Quad trees a data structure for\nretrieval on composite keys. Acta informatica 4, 1 (1974), 1â€“9.\n[10] Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and Tim\nKraska. 2019. FITing-Tree: A Data-aware Index Structure. In Proceedings of the\n14\n\n2019 International Conference on Management of Data, SIGMOD Conference 2019,\nAmsterdam, The Netherlands, June 30 - July 5, 2019 . ACM, 1189â€“1206.\n[11] Tu Gu, Kaiyu Feng, Gao Cong, Cheng Long, Zheng Wang, and Sheng Wang.\n[n.d.]. The RLR-Tree: A Reinforcement Learning Based R-Tree for Spatial Data.\n([n. d.]). https://arxiv.org/abs/2103.04541\n[12] Antonin Guttman. 1984. R-Trees: A Dynamic Index Structure for Spatial Search-\ning. In SIGMODâ€™84, Proceedings of Annual Meeting, Boston, Massachusetts, USA,\nJune 18-21, 1984 . ACM Press, 47â€“57. https://doi.org/10.1145/602259.602266\n[13] Ali Hadian, Ankit Kumar, and Thomas Heinis. 2020. Hands-off Model Integration\nin Spatial Index Structures (AIDB@VLDB) .\n[14] David Hilbert and David Hilbert. 1935. Ãœber die stetige Abbildung einer Linie\nauf ein FlÃ¤chenstÃ¼ck. Dritter Band: Analysis Â·Grundlagen der Mathematik Â·Physik\nVerschiedenes: Nebst Einer Lebensgeschichte (1935), 1â€“2.\n[15] Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. 2011. Sequential\nModel-Based Optimization for General Algorithm Configuration. In Learning\nand Intelligent Optimization - 5th International Conference, LION 5, Rome, Italy,\nJanuary 17-21, 2011. Selected Papers (Lecture Notes in Computer Science) , Vol. 6683.\nSpringer, 507â€“523. https://doi.org/10.1007/978-3-642-25566-3_40\n[16] Ibrahim Kamel and Christos Faloutsos. 1994. Hilbert R-tree: An Improved R-\ntree using Fractals. In VLDBâ€™94, Proceedings of 20th International Conference on\nVery Large Data Bases, September 12-15, 1994, Santiago de Chile, Chile . Morgan\nKaufmann, 500â€“509. http://www.vldb.org/conf/1994/P500.PDF\n[17] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. [n.d.]. SOSD: A Benchmark for Learned\nIndexes. ([n. d.]). http://arxiv.org/abs/1911.13014\n[18] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2020. RadixSpline: a single-pass learned\nindex. In Proceedings of the Third International Workshop on Exploiting Artificial\nIntelligence Techniques for Data Management, aiDM@SIGMOD 2020, Portland,\nOregon, USA, June 19, 2020 . ACM. https://doi.org/10.1145/3401071.3401659\n[19] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In Proceedings of the 2018 International\nConference on Management of Data, SIGMOD Conference 2018, Houston, TX, USA,\nJune 10-15, 2018 . ACM, 489â€“504. https://doi.org/10.1145/3183713.3196909\n[20] Jonathan K. Lawder and Peter J. H. King. 2001. Querying Multi-dimensional\nData Indexed Using the Hilbert Space-filling Curve. SIGMOD Rec. 30, 1 (2001),\n19â€“24. https://doi.org/10.1145/373626.373678\n[21] Mingjie Li, Ying Zhang, Yifang Sun, Wei Wang, Ivor W. Tsang, and Xuemin Lin.\n2020. I/O Efficient Approximate Nearest Neighbour Search based on Learned\nFunctions. In 36th IEEE International Conference on Data Engineering, ICDE 2020,\nDallas, TX, USA, April 20-24, 2020 . IEEE, 289â€“300. https://doi.org/10.1109/ICDE\n48307.2020.00032\n[22] Pengfei Li, Hua Lu, Qian Zheng, Long Yang, and Gang Pan. 2020. LISA: A\nLearned Index Structure for Spatial Data. In Proceedings of the 2020 International\nConference on Management of Data, SIGMOD Conference 2020, online conference\n[Portland, OR, USA], June 14-19, 2020 . ACM, 2119â€“2133. https://doi.org/10.1145/\n3318464.3389703\n[23] Ryan Marcus, Andreas Kipf, Alexander van Renen, Mihail Stoian, Sanchit Misra,\nAlfons Kemper, Thomas Neumann, and Tim Kraska. 2020. Benchmarking Learned\nIndexes. Proc. VLDB Endow. 14, 1 (2020), 1â€“13. https://doi.org/10.14778/3421424\n.3421425\n[24] Donald Meagher. [n.d.]. Octree encoding: a new technique for the representation,\nmanipulation and display of arbitrary 3-D objects by computer. Technical Report\n([n. d.]).\n[25] Guy M Morton. 1966. A computer oriented geodetic data base and a new tech-\nnique in file sequencing. (1966).\n[26] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020. Learn-\ning Multi-Dimensional Indexes. In Proceedings of the 2020 International Conference\non Management of Data, SIGMOD Conference 2020, online conference [Portland, OR,\nUSA], June 14-19, 2020 . ACM, 985â€“1000. https://doi.org/10.1145/3318464.3380579\n[27] JÃ¼rg Nievergelt, Hans Hinterberger, and Kenneth C. Sevcik. 1984. The Grid File:\nAn Adaptable, Symmetric Multikey File Structure. ACM Trans. Database Syst. 9,\n1 (1984), 38â€“71. https://doi.org/10.1145/348.318586\n[28] Shoji Nishimura and Haruo Yokota. 2017. QUILTS: Multidimensional Data\nPartitioning Framework Based on Query-Aware and Skew-Tolerant Space-Filling\nCurves. In Proceedings of the 2017 ACM International Conference on Management\nof Data (SIGMOD â€™17) . Association for Computing Machinery, 1525â€“1537. https:\n//doi.org/10.1145/3035918.3035934\n[29] Jianzhong Qi, Guanli Liu, Christian S. Jensen, and Lars Kulik. 2020. Effectively\nLearning Spatial Indices. Proc. VLDB Endow. 13, 11 (2020), 2341â€“2354. http:\n//www.vldb.org/pvldb/vol13/p2341-qi.pdf\n[30] Frank Ramsak, Volker Markl, Robert Fenk, Martin Zirkel, Klaus Elhardt, and\nRudolf Bayer. 2000. Integrating the UB-Tree into a Database System Kernel. In\nVLDB 2000, Proceedings of 26th International Conference on Very Large Data Bases,\nSeptember 10-14, 2000, Cairo, Egypt . Morgan Kaufmann, 263â€“272.\n[31] Yanhao Wang Sachith Gopalakrishna Pai, Michael Mathioudakis. 2022. Towards\nan Instance-Optimal Z-Index (AIDB@VLDB) .[32] Subhadeep Sarkar and Manos Athanassoulis. 2022. Dissecting, Designing, and\nOptimizing LSM-based Data Stores. In SIGMOD â€™22: International Conference on\nManagement of Data, Philadelphia, PA, USA, June 12 - 17, 2022 . ACM, 2489â€“2497.\nhttps://doi.org/10.1145/3514221.3522563\n[33] Timos K. Sellis, Nick Roussopoulos, and Christos Faloutsos. 1987. The R+-Tree:\nA Dynamic Index for Multi-Dimensional Objects. In VLDBâ€™87, Proceedings of 13th\nInternational Conference on Very Large Data Bases, September 1-4, 1987, Brighton,\nEngland . Morgan Kaufmann, 507â€“518. http://www.vldb.org/conf/1987/P507.PDF\n[34] Darius Sidlauskas, Sean Chester, Eleni Tzirita Zacharatou, and Anastasia Aila-\nmaki. 2018. Improving Spatial Data Processing by Clipping Minimum Bound-\ning Boxes. In 34th IEEE International Conference on Data Engineering, ICDE\n2018, Paris, France, April 16-19, 2018 . IEEE Computer Society, 425â€“436. https:\n//doi.org/10.1109/ICDE.2018.00046\n[35] Chuzhe Tang, Youyun Wang, Zhiyuan Dong, Gansen Hu, Zhaoguo Wang, Minjie\nWang, and Haibo Chen. 2020. XIndex: a scalable learned index for multicore data\nstorage. In PPoPP â€™20: 25th ACM SIGPLAN Symposium on Principles and Practice\nof Parallel Programming, San Diego, California, USA, February 22-26, 2020 . ACM,\n308â€“320. https://doi.org/10.1145/3332466.3374547\n[36] Yao Tian, Tingyun Yan, Xi Zhao, Kai Huang, and Xiaofang Zhou. 2022. A Learned\nIndex for Exact Similarity Search in Metric Spaces. CoRR abs/2204.10028 (2022).\nhttps://doi.org/10.48550/arXiv.2204.10028\n[37] Herbert Tropf and Helmut Herzog. 1981. Multidimensional Range Search in\nDynamically Balanced Trees. ANGEWANDTE INFO. 2 (1981), 71â€“77.\n[38] Haixin Wang, Xiaoyi Fu, Jianliang Xu, and Hua Lu. 2019. Learned Index for\nSpatial Queries. In 20th IEEE International Conference on Mobile Data Management,\nMDM 2019, Hong Kong, SAR, China, June 10-13, 2019 . IEEE, 569â€“574. https:\n//doi.org/10.1109/MDM.2019.00121\n[39] Xiaoying Wang, Changbo Qu, Weiyuan Wu, Jiannan Wang, and Qingqing Zhou.\n2021. Are We Ready For Learned Cardinality Estimation? Proc. VLDB Endow. 14,\n9 (2021), 1640â€“1654. http://www.vldb.org/pvldb/vol14/p1640-wang.pdf\n[40] Jiacheng Wu, Yong Zhang, Shimin Chen, Yu Chen, Jin Wang, and Chunxiao Xing.\n2021. Updatable Learned Index with Precise Positions. Proc. VLDB Endow. 14, 8\n(2021), 1276â€“1288. http://www.vldb.org/pvldb/vol14/p1276-wu.pdf\n[41] Zongheng Yang, Badrish Chandramouli, Chi Wang, Johannes Gehrke, Yinan Li,\nUmar Farooq Minhas, Per-Ã…ke Larson, Donald Kossmann, and Rajeev Acharya.\n2020. Qd-tree: Learning Data Layouts for Big Data Analytics. In Proceedings of\nthe 2020 International Conference on Management of Data, SIGMOD Conference\n2020, online conference [Portland, OR, USA], June 14-19, 2020 . ACM, 193â€“208.\nhttps://doi.org/10.1145/3318464.3389770\n[42] Songnian Zhang, Suprio Ray, Rongxing Lu, and Yandong Zheng. 2021. SPRIG:\nA Learned Spatial Index for Range and kNN Queries. In Proceedings of the 17th\nInternational Symposium on Spatial and Temporal Databases, SSTD 2021, Virtual\nEvent, USA, August 23-25, 2021 . ACM, 96â€“105. https://doi.org/10.1145/3469830.\n3470892\n15",
  "textLength": 83399
}