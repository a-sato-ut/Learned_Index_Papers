{
  "paperId": "ad39278ea271977a78c99a1f18fd16458b4abacf",
  "title": "A Survey of Machine Learning Applied to Computer Architecture Design",
  "pdfPath": "ad39278ea271977a78c99a1f18fd16458b4abacf.pdf",
  "text": "1\nA Survey of Machine Learning Applied to\nComputer Architecture Design\nDrew D. Penney, and Lizhong Chen\u0003,Senior Member, IEEE\nAbstract —Machine learning has enabled signiﬁcant beneﬁts in diverse ﬁelds, but, with a few exceptions, has had limited impact on\ncomputer architecture. Recent work, however, has explored broader applicability for design, optimization, and simulation. Notably,\nmachine learning based strategies often surpass prior state-of-the-art analytical, heuristic, and human-expert approaches. This paper\nreviews machine learning applied system-wide to simulation and run-time optimization, and in many individual components, including\nmemory systems, branch predictors, networks-on-chip, and GPUs. The paper further analyzes current practice to highlight useful\ndesign strategies and identify areas for future work, based on optimized implementation strategies, opportune extensions to existing\nwork, and ambitious long term possibilities. Taken together, these strategies and techniques present a promising future for increasingly\nautomated architectural design.\nF\n1 I NTRODUCTION\nIn the past decade, machine learning (ML) has rapidly\nbecome a revolutionary factor in many ﬁelds, ranging from\ncommercial applications, as in self-driving cars, to medical\napplications, improving disease screening and diagnosis. In\neach of these applications, an ML model is trained to make\npredictions or decisions without explicit programming by\ndiscovering embedded patterns or relationships in the data.\nNotably, ML models can perform well in tasks/applications\nwhere relationships are too complex to model using analyt-\nical methods. These powerful learning capabilities continue\nto enable rapid developments in diverse ﬁelds. Concur-\nrently, the exponential growth predicted by Moore’s law has\nslowed, putting increasing burden on architects to supplant\nMoore’s law with architectural advances. These opposing\ntrends suggest opportunities for a paradigm shift in which\ncomputer architecture enables ML and, simultaneously, ML\nimproves computer architecture, closing a positive-feedback\nloop with vast potential for both ﬁelds.\nTraditionally, the relationship between computer archi-\ntecture and ML has been relatively imbalanced, focusing\non architectural optimizations to accelerate ML algorithms.\nIn fact, the recent resurgence in AI research is, at least\npartly, attributed to improved processing capabilities. These\nimprovements are enhanced by hardware optimizations\nexploiting available parallelism, data reuse, sparsity, etc. in\nexisting ML algorithms. In contrast, there has been relatively\nlimited work applying ML to improve architectural design,\nwith branch prediction being one of a few mainstream\nexamples. This nascent work, although limited, presents an\nauspicious approach for architectural design.\nThis paper presents an overview of ML applied to archi-\ntectural design and analysis. As illustrated in Figure 1, this\nﬁeld has grown signiﬁcantly in both success and popularity,\nparticularly in the past few years. These works establish\nthe broad applicability and future potential of ML-enabled\narchitectural design; existing ML-based approaches, ranging\nfrom DVFS with simple classiﬁcation trees to design space\nexploration via deep reinforcement learning, have already\nsurpassed their respective state-of-the-art human expert and\nheuristic based designs. ML-based design will likely con-\n\u0003Corresponding author. Email: chenliz@oregonstate.edu\nThe authors are with Oregon State University, Corvallis, OR 97331\nCopyright 2019 by Drew D. Penney and Lizhong Chen\nAll Rights Reserved\nFig. 1. Publications on machine learning applied to architecture (for\nworks examined in Section 3)\ntinue to provide breakthroughs as promising applications\nare explored.\nThe paper is organized as follows. Section 2 provides\nbackground on ML and existing models to build intuition on\nML applicability to architectural issues. Section 3 presents\nexisting work on ML applied to architecture. Section 4\nthen compares and contrasts implementation strategies in\nexisting work to highlight signiﬁcant design considerations.\nSection 5 identiﬁes possible improvements and extensions\nto existing work as well as promising, new applications for\nfuture work. Finally, Section 6 concludes.\n2 B ACKGROUND\n2.1 Fundamental Applicability\nMachine learning has been rapidly adopted in many ﬁelds\nas an alternative approach for a diverse range of prob-\nlems. This fundamental applicability stems from the pow-\nerful relationship learning capabilities of ML algorithms.\nSpeciﬁcally, ML models leverage a generic framework in\nwhich these models learn from examples, rather than ex-\nplicit programming, enabling application in many tasks,\nincluding those too difﬁcult to representing using standard\nprogramming methods. Furthermore, using this generic\nframework, there may be many possible approaches for\nany given problem. For example, in the case of predicting\nIPC for a processor, one can experiment with a simple\nlinear regression model, which learns a linear relationship\nbetween features (such as core frequency and cache size)arXiv:1909.12373v1  [cs.AR]  26 Sep 2019\n\n2\nand the instructions-per-cycle (IPC). This approach may\nwork well or it may work poorly. In the case it works\npoorly, one can try different features, non-linear feature\ncombinations (such as core frequency times cache size),\nor a different model entirely, with another common choice\nbeing an artiﬁcial neural network (ANN). This diversity in\npossible approaches enables adjustment of models, model\nparameters, and training features to match the task at hand.\n2.2 Learning Approaches & Models\nThe learning approach and the model are both fundamental\nconsiderations in applying machine learning to any prob-\nlem. In general, there are four main categories of learn-\ning approaches: supervised learning, unsupervised learn-\ning, semi-supervised learning, and reinforcement learning.\nThese approaches can be differentiated by what data is used\nand how that data is used to facilitate learning. Similarly,\nmany appropriate models may exist for a given problem,\nthus enabling signiﬁcant diversity in application based\non the learning approach, hardware resources, available\ndata, etc. In the following, we introduce these learning\napproaches and several signiﬁcant models for each learning\napproach, focusing on approaches with proven applicability.\nImplementation details are considered later in Section 4.\nSupervised learning : In supervised learning, the model\nis trained using input features and output targets, with the\nresult being a model that can predict the output for new,\nunseen inputs. Common supervised learning applications\ninclude regression (predicting a value such as processor\nIPC) and classiﬁcation (predicting a label such as the op-\ntimal core conﬁguration for application execution). Feature\nselection, discussed in Section 2.3, is particularly important\nin these applications as the model must learn to predict\nsolely based on feature values.\nSupervised learning models can be generalized into\nfour categories: decision trees, Bayesian networks, support\nvector machines (SVMs), and artiﬁcial neural networks [1].\nDecision trees use a tree structure where each node repre-\nsents a feature and branches represent a value (or range of\nvalues) for that feature. Inputs are therefore classiﬁed by\nsequentially following branches based on the value of the\nfeature being considered at a given node. Bayesian networks\ninstead embed conditional relationships into a graphical\nstructure; nodes represent random variables and edges rep-\nresent conditional dependence between these variables. A\nperformance prediction model, for example, can condition\nprediction for new applications on learned distributions\nfor unobserved variables (i.e., underlying factors affecting\nperformance) from other applications, as in [2]. SVMs are\ngenerally known for their function rather than a particular\ngraphical structure (as in decision tree and Bayesian net-\nworks). Speciﬁcally, SVMs learn the best dividing line (in\n2-D) or hyperplane (in high dimensions) between examples,\nthen uses examples along this hyperplane to make new pre-\ndictions. SVMs can also be extended to non-linear problems\nusing kernel methods [3] as well as multi-class problems.\nFinally, artiﬁcial neural networks (or simply neural net-\nworks) represent a broad category of models that are, again,\ndeﬁned by their structure, which is reminiscent of neurons\nin the human brain; layers of nodes/neurons are connected\nusing links with learned weights, enabling particular nodes\nto respond to speciﬁc input features. Simple perceptron\nmodels contain just one weight layer, directly converting\nthe weighted sum of inputs into an output. More complex\nDNNs include several (or many) layers of these weightedsums. Additional variants such as convolutional neural net-\nworks (CNNs) incorporate convolution operations between\nsome layers to capture spatial locality while recurrent neural\nnetworks re-use the previous output to learn sequences and\nlong-term patterns. All these supervised learning models\ncan be used in both classiﬁcation and regression tasks,\nalthough there are some distinct high-level differences. Vari-\nants of SVMs and neural networks tend to perform better\nfor high-dimension and continuous features and also when\nfeatures may be nonlinear [1]. These models, however, tend\nto require more data compared to Bayesian networks and\ndecision trees.\nUnsupervised learning : Unsupervised learning uses just\ninput data to extract information without human effort.\nThese models can therefore be useful, for example, in reduc-\ning data dimensionality by ﬁnding appropriate alternative\nrepresentations or clustering data into classes that may not\nbe obvious for humans.\nThus far, the primary two unsupervised learning models\napplied to architecture are principal components analysis\n(PCA) and k-means clustering. PCA provides a method\nto extract signiﬁcant information from a dataset by deter-\nmining linear feature combinations with high variance [4].\nAs such, PCA can be applied as an initial step towards\nbuilding a model with reduced dimensionality, a highly\ndesirable feature in most applications, albeit at the cost of\ninterpretability (discussed in Section 4). K-means clustering\nis instead used to identify groups of data with similar fea-\ntures. These groups may be further processed to generalize\nbehavior or simplify representations for large datasets.\nSemi-supervised learning : Semi-supervised learning\nrepresents a mix of supervised and unsupervised methods,\nwith some paired input/output data, and some unpaired in-\nput data. Using this approach, learning can take advantage\nof limited labeled data and potentially signiﬁcant unlabeled\ndata. We note that this approach has, thus far, not yet\nfound application in architecture. Nevertheless, one work\non circuits analysis [5] presents a possible strategy that\ncould be adapted in future work.\nReinforcement Learning : In reinforcement learning, an\nagent is sequentially provided with input based on an\nenvironment state and learns to perform actions that op-\ntimize a reward. For example, in the context of memory\ncontrollers, the agent replaces traditional control logic. Input\ncould include pending reads and writes while actions could\ninclude standard memory controller commands (row read,\nwrite, pre-charge, etc.). Throughput could then be optimized\nby including it in the reward function. Given this setup,\nthe agent will potentially, over time, learn to choose control\nactions that maximize throughput.\nReinforcement learning models applied to architecture,\nas a whole, can be understood using a representation based\non states, actions, and rewards. The agent attempts to learn\na policy function \u0019, which deﬁnes the action ato take at a\ngiven state s, based on a received reward r[6]. A learned\nstate-value function, following the policy, is then given as\nV\u0019(s) =E[X\nt\u00150\rt\u0003rtjs0=s;\u0019] (1)\nwhere\ris a discount factor ( \u00141), which dictates how much\nthe model should consider future rewards. The cumulative\nrewards are then maximized by learning an optimal policy\n\u0019\u0003that satisﬁes\n\u0019\u0003(s) =argmax\n\u0019E[X\nt\u00150\rt\u0003rtjs0=s;\u0019]: (2)\n\n3\nVarious models may implement different approaches to\nlearn this optimal policy, but largely address the same\nproblem of maximizing rewards. Q-learning is a noteworthy\nexample that models an action-value function by estimating\nthe value of an individual action, from a given state.\n2.3 Feature Selection\nSupervised (and semi-supervised) learning methods rely\nupon input data features to model relationships and gener-\nate predictions. Consequently, approaches for feature selec-\ntion can substantially impact model performance, including\nconcerns such as over-ﬁtting and computational overhead,\nas well as more abstract concerns, such as feature inter-\npretability. In some works, feature selection is entirely based\non expert knowledge. Additional, more general, approaches\ncan either supplant or supplement expert knowledge.\nOne set of approaches, called ﬁlter methods, considers\nfeatures individually using metrics involving statistical cor-\nrelation or information theoretic criteria such as mutual\ninformation. These approaches are usually the least com-\nputationally intensive so may be preferred for very large\nfeature sets, but model performance may be sub-optimal\nsince evaluation criteria in ﬁlter methods do not consider\nfeature context [7]; two features that provide little beneﬁt\nindividually may be beneﬁcial together. Many alternative\napproaches therefore consider feature subsets.\nWrapper methods provide a black-box method for fea-\nture selection by directly assessing the performance of a\nlearning model [7]. Commonly applied greedy approaches\ninclude forward selection and backward elimination. In for-\nward selection, features are progressively added to selected\nfeature subset based on improvement to the overall learning\nmodel. Conversely, backward elimination removes features\nprogressively that provide little beneﬁt.\nEmbedded methods integrate feature selection into the\nlearning model to provide a trade-off between ﬁlter and\nwrapper methods [8]. Regularization is a widely used em-\nbedded method that allows the learning model to be ﬁt,\nwhile simultaneously forcing feature coefﬁcients to be small\n(or zero). Features with zero coefﬁcient values can then be\nremoved. This method eliminates iterative feature selection\npresent in wrapper methods, which can have high compu-\ntational requirements [7].\n3 L ITERATURE REVIEW\nThis section reviews existing work that applies machine\nlearning to architecture. Work is organized by sub-system\n(when applicable) or primary objective. We focus on design\nand optimization, but also introduce general performance\nprediction work.\n3.1 System Simulation\nCycle-accurate simulators are commonly used in system\nperformance prediction, but require several orders of mag-\nnitude more time than native execution. ML can offset this\npenalty through a trade-off between simulation time and\naccuracy. In general, ML can reduce execution time by 2-\n3 orders of magnitude with relatively high accuracy (task\ndependent, typically >90% ). Early work by Ipek et al. [9]\nmodeled architectural design spaces using an ANN ensem-\nble (a group of ANN predictors). Models were trained on\napproximately 1% of the design space, then predicted CMP\nperformance with 4-5% error for random points, albeit onlyin that speciﬁc conﬁguration space. When combined with\nSimPoints, predictions exhibit slightly higher error, but the\nsimulated instruction count is further reduced. Ozisikyilmaz\net al. [10] additionally predicted SPEC performance for\nfuture systems that may be poorly modeled by existing\nsimulators. Evaluation was limited to randomly-sampled\ndata with relatively simple linear regression and neural\nnetwork models, but nevertheless demonstrated advantages\nfor pruned neural networks compared to standard single-\nlayer models (as in [9]). Several other ML approaches have\nalso been tested. Eyerman et al. [11] proposed a mechanistic-\nempirical model for processor CPI prediction. In this ap-\nproach, they used a generic mechanistic model with param-\neters inferred by a regression model. Their model is limited\nto single-core performance prediction, but improves accu-\nracy, ease of implementation (compared to purely mechanis-\ntic models), and interpretability (compared to purely empir-\nical models). Zheng et al. [12], [13] explored cross-platform\nprediction from Intel/AMD to ARM processors using linear\nregression. Their ﬁrst approach [12] made predictions based\non a local neighborhood of examples around the target point\nto approximate non-linear behavior. They later [13] em-\nphasized phase-level prediction, assuming that phase-level\nbehavior would be approximately linear. Notably, average\nerror for cycle count predictions is less than 1% using phase-\nlevel proﬁling. This approach is, however, restricted to a\nsingle target architecture and requires source code for phase-\nlevel analysis, leaving signiﬁcant opportunities for future\nwork. Finally, recent work by Agarwal et al. [14] introduced\na method to predict parallel execution speedup using single-\nthreaded execution characteristics. They trained separate\nmodels for each thread count using application-level per-\nformance counters. Although neural networks were omitted\ndue to limited data, evaluation found that Gaussian process\nregression still provided promising results, particularly for\nhigh thread counts.\n3.2 GPUs\nDesign Space Exploration : GPU design space exploration\nhas proven to be a particularly favorable application for\nML due to a highly irregular design space; some kernels\nexhibit relatively linear scaling while others exhibit very\ncomplex relationships between conﬁguration parameters,\npower, and performance [15], [16], [17]. Jia et al. [15] pro-\nposed Stargazer, a regression-based framework based on\nnatural cubic splines. Stargazer randomly samples approx-\nimately 300 points from a target design space (933K points\nin evaluation) for each application, then applies stepwise\nregression on these points. Notably, the framework achieves\nunder 3.8% average performance prediction error. Wu et al.\n[16] instead explicitly modeled scaling for compute units,\ncore frequency, and memory frequency. Scaling data from\ntraining kernels was processed using k-means clustering to\ngroup kernels by scaling behavior. An ANN then classiﬁes\nkernels into these clusters, allowing new kernels to be\nclassiﬁed and predictions made using cluster scaling factors.\nThis approach, in contrast to Jia et al. [15], therefore requires\njust a few samples for new applications. Jooya et al. [17],\nsimilar to Jia et al. [15], considered a per-application perfor-\nmance/power prediction model, but additionally proposed\na scheme to predict per-application Pareto fronts. Many\nANN-based predictors were trained and the most accurate\nsubset was used as an ensemble for prediction. Prediction\naccuracy was later improved by sampling points within a\nthreshold of the previously predicted Pareto-optimal curve.\n\n4\nLin et al. [18] combined a performance predicting DNN with\na genetic search scheme to explore memory controller place-\nment. The DNN was used as a surrogate ﬁtness function,\nobviating slow system simulations. The resulting placement\nimproves system performance by 19.3%.\nCross-Platform Prediction : Porting applications for ex-\necution on GPUs is a challenging task with potentially\nuncertain beneﬁts over CPU execution. Work has there-\nfore examined methods to predict speedup or efﬁciency\nimprovements using just CPU execution behavior. Baldini\net al. [19] cast the problem as a classiﬁcation task, train-\ning a modiﬁed nearest-neighbor and a support vector ma-\nchine (SVM) model to determine, based on a threshold,\nwhether GPU implementation would be beneﬁcial. Using\nthis approach, they predicted near-optimal conﬁgurations\n91% of the time. In contrast, Ardalani et al. [20] trained\na large ensemble of regression models to directly predict\nGPU performance for the code segment. Although several\ncode segments exhibit high error, the geometric mean of\nthe absolute value of the relative error is still just 11.6%\nand the model successfully identiﬁes several code segments\n(both beneﬁcial and non-beneﬁcial) that are incorrectly pre-\ndicted by human experts. Later work by Ardalani et al. [21]\nintroduced a completely static-analysis-based framework\nusing a random forest model for binary classiﬁcation. This\napproach eliminates both dynamic proﬁling and human\nguidance, instead using features such as instruction mix,\nbranch divergence estimates, and kernel size to provide 94%\naccuracy for binary speedup classiﬁcation (using a speedup\nthreshold of 3).\nGPU Speciﬁc Prediction & Classiﬁcation : O’Neal et al.\n[22] presented a methodology for next-generation GPU per-\nformance prediction as cycles-per-frame (CPF) for DirectX\napplications. They focused on Intel GPUs, proﬁling earlier-\ngeneration architectures (e.g., Haswell GT2) to train next-\ngeneration predictors. They found that different models\n(i.e., linear vs non-linear) can produce more accurate results\ndepending on the prediction target (Broadwell GT2/GT3 vs\nSkylake GT3), with the best performing models achieving\nless than 10% CPF prediction error. Recent work by Li et\nal. [23] presented a re-evaluation of commonly accepted\nknowledge of GPU trafﬁc patterns. They used a CNN and\nt-distributed stochastic neighbor embedding on heatmap-\ntransformed trafﬁc data, identifying eight unique patterns\nwith 94% accuracy.\nScheduling : GPU processing-in-memory (PIM) archi-\ntectures can beneﬁt from high memory bandwidth with\nreduced data movement energy. Despite this beneﬁt, poten-\ntial limitations on PIM compute capabilities may introduce\ncomplex trade-offs between performance and energy when\nscheduling execution on various resources. For this reason,\nPattnaik et al. [24] proposed an approach using a regression\nmodel to classify core afﬁnity, thus dividing the workload,\nand an additional regression model to predict execution\ntime, enabling dynamic task migration. Performance and\nenergy efﬁciency are improved by 42% and 27%, respec-\ntively, over a baseline GPU architecture. Further improve-\nments are possible by improving core afﬁnity classiﬁcation\naccuracy (compared to regression).\n3.3 Memory Systems and Branch Prediction\nCaches : Heuristic approaches for caching can incur perfor-\nmance penalties due to dramatic workload variance. ML\napproaches can learn these intricacies and offer superiorperformance. Peled et al. [25] proposed a prefetcher ex-\nploiting semantic locality (data structures) using contex-\ntual bandits (a simple RL variant), correlating contextual\ninformation and candidate addresses for prefetching. Im-\nplementation uses a two-level indexing method to dynam-\nically control state information, allowing online feature\nselection with some additional overhead. Zeng and Guo\n[26] proposed a long short-term memory (LSTM) model\n(a recurrent neural network variant) for prefetching based\non local history and offset-delta tables. Evaluation showed\nthat the LSTM model enables accurate predictions over\nlonger sequence and higher noise resistance than prior\nwork. Several concerns relating to overhead and warm-up\ntime are addressed, with potential solutions remaining for\nfuture work. Similarly, Braun et al. [27] extensively explored\nLSTM prefetching accuracy under several common access\npatterns. Experiments considered the impact of lookback\nsize (access history window) and LSTM model size for\nseveral noise levels and independent access stream counts.\nRecent work by Bhatia et al. [28] synthesized traditional\nprefetchers with a perceptron-based prefetch ﬁlter, allowing\naggressive predictions without degrading accuracy. Evalua-\ntion conﬁrmed substantial coverage and IPC beneﬁts offered\nby the proposed scheme, with 9.7% IPC speedup over the\nnext best prefetcher when referenced to a no-prefetching\nfour-core baseline. ML has similarly been applied to data\nreuse policies. For example, Teran et al. [29] predicted LLC\nreuse with a perceptron model. In this approach, input\nfeatures are hashed to access saturating weight tables that\nare incremented/decremented based on correct/incorrect\nreuse prediction. These features are chosen empirically and\nshown to signiﬁcantly impact performance, thus presenting\nan option for further optimization. Wang et al. [30] predicted\nreuse prior to cache entry, only storing data in the cache\nif there was predicted reuse. They used decision trees as\na low-cost alternative to ensemble models, achieving 60-\n80% reduction in writes. Additional research has explored\nthe growing performance bottleneck in translation lookaside\nbuffers (TLBs). Margaritov et al. [31] proposed a scheme\nfor virtual address translation in TLBs based on learned\nindices [32]. Evaluation showed nearly 100% accuracy for\npredicted indices, but practical implementation will require\ndedicated hardware to reduce calculation overhead (and is\nleft for future work).\nSchedulers & Control : Controllers for memory and\nstorage systems inﬂuence both device performance and\nreliability, thus representing another strong application for\nML models compared with heuristics. Ipek et al. [33] ﬁrst\nproposed an RL approach for memory controllers to capture\nthe balance between concurrency, delay, and several other\nfactors. The model predicted optimal actions (precharge, ac-\ntivate, row read/write), improving system performance by\n15% (in a two-channel system) over prior work. Mukundan\nand Martinez [34] later built upon Ipek’s work, generalizing\nthe reward function to optimize energy, fairness, etc. They\nalso added power-up and power-down actions to enable a\nfurther 8.6% improvement in performance and a signiﬁcant\nimprovement in energy efﬁciency. Related work optimizes\ncommunication energy between memory/storage and other\nsystems using ML. Manoj et al. [35] proposed a Q-learning\nmethod for dynamic voltage swing control in through-\nsilicon-interposer transmission lines. Predictions for power\nand bit error rate were quantized, then provided as input\nto the model to determine a new voltage level. Although\ntheir approach requires signiﬁcant quantization to mini-\n\n5\nmize overhead, they still achieved 15.1% energy savings\ncompared to a static voltage baseline. Wang and Ipek [36]\nreduce data movement energy through online clustering\nand encoding. Several clusters are continuously updated at\na bit-level using majority voting for data in that cluster. The\ntotal number of transmitted 1s is then minimized by XORing\nnew data with the closest learned cluster center. Kang and\nYoo [37] applied Q-learning to manage garbage collection\nin SSDs by determining optimal periods of inactivity. Key\nstates are kept in the Q-table using LRU replacement, al-\nlowing a vast state space and, ultimately, a 22% average tail\nlatency reduction over the baseline. Many states are, how-\never, observed only once per workload, suggesting potential\nbeneﬁts using deep Q-learning (DQL). Other work directly\nconsidered system reliability. For example, Deng et al. [38]\nproposed a regression-based framework to dynamically op-\ntimize performance and lifetime in non-volatile memories.\nTheir approach used phase-based application statistics to\nmanage several conﬂicting policies for write latency, write\ncancellation, endurance, etc., guaranteeing a minimum life-\ntime with modest performance/energy improvements. Xiao\net al. [39] proposed a method for disk failure prediction\nusing an online random forest. They trained their model\nusing a disk status window to account for imprecision in\nrecorded failure date, enabling accurate predictions of soon-\nto-be faulty drives. Comparison against other random forest\nupdating schemes (e.g., updating once a month) highlighted\naccuracy beneﬁts from consistent training that may be ex-\ntended to related domains.\nBranch Prediction : Branch prediction is a noteworthy\nexample of current ML application in industry, with ac-\ncuracy surpassing prior state-of-the-art non-ML predictors.\nThe perceptron-based branch predictor was ﬁrst proposed\nby Jim ´enez and Lin [40] as a promising high-accuracy al-\nternative to two-level schemes using pattern history tables.\nLater research by St. Amant et al. introduced SNAP [41], a\nperceptron-based predictor implemented using analog cir-\ncuitry to enable an efﬁcient and practically feasible design.\nPerceptron weights and branch history were used to drive\ncurrent-steering DACs that perform the dot product as the\nsum of currents. Jim ´enez [42] further optimized this design\nusing a per-branch history table, dynamic coefﬁcients for\nhistory importance, and a dynamic learning threshold. The\noptimized design achieves 3.1% lower MKPI than L-TAGE.\nRecent work with perceptron-based predictors by Garza et\nal. [43] explored bit-level prediction for indirect branches.\nPossible branch targets are evaluated using their similarity\n(dot product) with the combined weights from eight feature\ntables incorporating local and global history, ultimately\nreducing MKPI by 5% compared to ITTAGE. Currently,\nstate-of-the-art conditional branch predictors (e.g., TAGE-\nSC-L [44]) still hide signiﬁcant IPC gains (14.0% for an Intel\nSkylake architecture) in just a few hard-to-predict (H2P)\nconditional branches [45]. Tarsa et al. [45] consequently pro-\nposed “CNN Helper” predictors that target speciﬁc H2Ps\nusing simple two-layer CNNs. Results indicate strong appli-\ncability across diverse workloads and present a promising\narea for future work.\n3.4 Networks-on-Chip\nDVFS & Link Control : Modern computing systems ex-\nploit complex power control schemes to enable increas-\ningly parallel architectural designs. Heuristic schemes may\nfail to exploit all energy-saving opportunities, particularly\nin dynamic network-on-chip (NoC) workloads, leading tosigniﬁcant beneﬁts through proactive ML-based control.\nSavva et al. [46] implemented dynamic link control using\nseveral ANNs, each of which monitors a NoC partition.\nThese ANNs used just link utilization to learn a dynamic\nthreshold to enable/disable links. Despite energy savings,\ntheir approach can cause high latency under dimension-\nordered routing. DiTomaso et al. [47] relocated ﬂit buffers\nto the links and dynamically controlled both link direc-\ntion and power-gating with per-router classiﬁcation trees.\nUsing a simple three-level tree to limit overhead, overall\nNoC power is reduced by 85% and latency is reduced by\n14% compared to a concentrated mesh. Winkle et al. [48]\nexplored ML-based power scaling in photonic interconnects.\nEven a simple linear regression model provided promising\nresults, negligibly reducing throughput (versus no power-\ngating) while reducing laser power consumption by 42%.\nReza et al. [49] proposed a multi-level ANN control scheme\nthat considered both power and thermal constraints on\ntask allocation, link allocation, and node DVFS. Individual\nANNs classiﬁed appropriate conﬁgurations for local NoC\npartitions while a global ANN classiﬁed optimal overall\nresource allocation. This scheme identiﬁes the global opti-\nmal NoC conﬁguration with high accuracy (88%), but uses\ncomplex ANNs that could impact implementation. Clark\net al. [50] proposed a router design for DVFS and eval-\nuated several regression-based control strategies. Variants\npredicted buffer utilization, change in buffer utilization, or\na combined energy and throughput metric. This work was\nexpanded by Fettes et al. [51], who introduced an RL control\nstrategy. Both regression and RL models enable beneﬁcial\ntradeoffs, although the RL strategy is most ﬂexible.\nAdmission & Flow Control : As with NoC DVFS, both\nadmission and ﬂow control can beneﬁt from proactive pre-\ndiction. Early work by Boyan and Littman [52] introduced\nQ-learning based routing in networks using delivery time\nestimates from neighboring nodes, noting throughput ad-\nvantages over traditional shortest path routing for high traf-\nﬁc intensity. Several works have expanded upon Q-routing,\nobserving application in dynamically changing NoC topolo-\ngies [53], improved capabilities in bufferless NoC fault-\ntolerant routing [54], and high-performance congestion-\naware non-minimal routing [55]. More recent works have\ninstead focused on injection throttling and hotspot preven-\ntion. For example, Daya et al. [56] proposed SCEPTER, a\nbufferless NoC using single-cycle multi-hop paths. They\ncontrolled injection throttling using Q-learning to maximize\nmulti-hop performance and improve fairness by reducing\ncontending ﬂits. Future work could reduce Q-table over-\nhead which scales with NoC size in their implementation.\nWang et al. [57] instead used an ANN to predict optimal\ninjection rates for a standard buffered NoC. Additional pre-\nprocessing (to capture both spatial and temporal trends) and\nnode grouping enables high accuracy predictions (90.2%)\nand reduces execution time by 17.8% compared to an un-\noptimized baseline. Soteriou et al. [58] similarly explored\nANN-based injection throttling to reduce NoC hotspots. The\nANN was trained to predict hotspots while recognizing\nthe impact of proposed injection throttling and dynamic\nrouting, providing a holistic mitigation strategy. The model\nprovides state-of-the-art results for throughput and latency\nunder synthetic trafﬁc, but limited improvement under real-\nworld benchmarks, suggesting the potential for further opti-\nmization. Another Q-learning approach, proposed by Yin et\nal. [59], used DQL to arbitrate NoC trafﬁc. They considered\na wide range of features and rewards while noting that\n\n6\nthe proposed DQL algorithm is impractical due to over-\nhead. Regardless, evaluation exhibited modest throughput\nimprovements over round-robin arbitration.\nTopology & General Design : Several works also applied\nML to higher-level NoC topology design, involving trade-\noffs between power and performance, with some further\nconsidering thermals. Das et al. [60] used a ML-based\nSTAGE algorithm to efﬁciently explore small-world in-\nspired 3D NoC designs. In this approach, design alternates\nbetween base/local search (adding/removing links in a hill-\nclimbing approach) and meta search (predicting beneﬁcial\nstarting points for local search using prior results). The\nsame model was used again by Das et al. [61] to balance\nlink utilization and address TSV reliability concerns. The\nSTAGE algorithm was then enhanced by Joardar et al. [62]\nto optimize a heterogeneous 3D NoC design. The models\nexplores multi-objective trade-offs between CPU latency,\nGPU throughput, and thermal/energy constraints. All three\nworks [60], [61], [62] still rely upon hill-climbing for opti-\nmization. Recent work by Lin et al. [63] instead explored\ndeep reinforcement learning in routerless NoC design. They\nused a Monte Carlo tree search to efﬁciently explore the\nsearch space and a deep convolutional neural network to\napproximate both the action and policy functions, thereby\noptimizing loop conﬁgurations. Further, the proposed deep\nreinforcement learning framework can strictly enforce de-\nsign constraints that may be violated by prior heuristic or\nevolutionary approaches. Rao et al. [64] investigated multi-\nobjective NoC design optimization across a broad SoC fea-\nture space (from bandwidth requirements to SoC area). ML\nmodels were trained using data from thousands of SoC con-\nﬁgurations to predict optimal NoC designs based on perfor-\nmance, area, or both. Limited comparisons against human-\nexpert designs did not consider alternative techniques (e.g.,\nAMOSA [65]), yet exhibited some promising results, moti-\nvating research into effective features and models as well as\nfurther comparisons against alternative techniques.\nPerformance Prediction : Existing NoC models based\non queuing theory are generally accurate, but rely on as-\nsumptions of trafﬁc distribution that may not hold for real\napplications [66]. Qian et al. [66] emphasized how ML-based\napproaches can relax the assumptions made by queueing\ntheory models. They constructed a mechanistic-empirical\nmodel based on a communication graph, using support\nvector regression (SVR) to relate several features and queu-\ning delays. Evaluation showed lower error (3% error vs\n10% error) than an existing analytical approach. Sangaiah\net al. [67] considered both NoC and memory conﬁguration\nfor performance prediction and design space exploration.\nFollowing a standard approach, they sampled a small por-\ntion of the design space, then trained a regression model\nto predict the resulting system CPI. Evaluation generally\nshowed high accuracy, but lower accuracy for high-trafﬁc\nworkloads (median error of 24%). Additional design space\nexploration exhibited promising results, reducing the design\nspace from 2.4M points to less than 1000.\nReliability & Error Correction : Overhead introduced by\nerror correction in NoCs can be signiﬁcant, especially when\nre-transmission is required. Several works have, therefore,\nexplored ML-based control schemes. DiTomaso et al. [68]\ntrained a decision tree to predict NoC faults using a wide\nrange of parameters including temperature, utilization, and\ndevice wear-out. These predictions allow proactive encod-\ning (on top of the baseline cyclic redundancy check) for\ntransmission that are likely to have errors. Wang et al. [69]adopted a similar strategy for dynamic error mitigation,\nbut used an RL-based control policy to eliminate the need\nfor labeled training examples. Their approach provides an\naverage of 46% dynamic power savings (17% better than\nthe decision tree method [68]) compared with a static CRC\nscheme. In both cases, ML-based proactive control chose\na more efﬁcient scheme than CRC only. Wang et al. [70]\nsubsequently proposed a holistic framework for NoC de-\nsign incorporating dynamic error mitigation, router power-\ngating, and multi-function adaptive channel buffers (MFAC\nbuffers). They emphasized comprehensive beneﬁts through\nsynergistic integration/control of several architectural inno-\nvations, thus achieving substantial improvements in latency\n(32%), energy-efﬁciency (67%), and reliability (77% higher\nMean Time to Failure) compared to a SECDED baseline.\n3.5 System-level Optimization\nEnergy Efﬁciency Optimization : Signiﬁcant work has be-\ngun to consider systems in which workload execution is\nconstrained by total energy consumption rather than pro-\ncessing resources. Control schemes incorporating ML have\nshown promise in optimizing energy efﬁciency with min-\nimal performance reduction, often enabling 60-80% reduc-\ntions in the energy-delay product compared to race-to-idle\nschemes. Won et al. [71] introduced a hybrid ANN + PI\n(proportional-integral) controller scheme for uncore DVFS.\nThey initially trained the ANN ofﬂine, then reﬁned pre-\ndictions online using the PI controller. This hybrid scheme\nwas shown to reduce the energy-delay product by 27%\ncompared to a PI controller alone, with less than 3% per-\nformance degradation compared to the highest V/F level.\nPan et al. [72] implemented a power management scheme\nusing a multi-level RL algorithm. Their method propagates\nindividual core states up a tree structure while aggregating\nQ-learning representations at each level. Global allocation is\nmade at the root, then decisions are propagated back down\nthe tree, enabling efﬁcient per-core control. Bailey et al. [73]\naddressed power efﬁciency in heterogenous systems. Simi-\nlar to Wu et al. [16], they clustered kernels by their scaling\nbehavior to train multiple linear regression models. Runtime\nprediction used two sample conﬁgurations, one from CPU\nexecution and one from GPU execution, to determine the\noptimal conﬁguration. Lo et al. [74] focused on energy-\nefﬁciency optimization for real-time interactive workloads.\nThey used linear regression to model execution time based\non annotations and code features, enabling stricter service\nlevel guarantees at the cost of applicability when source\ncode is unavailable. Mishra et al. [75] also addressed real-\ntime workloads, combining control theory and several ML-\nbased models. Their framework was realized by ofﬂoading\nlearning to a server, allowing low overhead DVFS that\nreduces energy consumption by 13% compared to the best\nprior approach. Related work by Mishra et al. [2] applied\na comparatively complex hierarchical Bayesian model to\ncombine both ofﬂine and online learning. In this approach,\nthey accepted a high execution time penalty (0.8s) in or-\nder to provide signiﬁcantly more accurate predictions than\nonline or ofﬂine training alone. This approach therefore\ntargeted longer executing workloads, but can provide more\nthan 24% energy savings over the next best approach. Bai\net al. [76] implemented a RL-based DVFS control policy\nadapted to a novel voltage regulator hierarchy using off-\nchip switching regulators and on-chip linear regulators. In-\ndividual RL agents adapt to a dynamically allocated power\nbudget determined by a heuristic bidding approach. The\n\n7\ndesign was enhanced using adaptive Kanerva coding [77]\nto limit area/power overhead and experience sharing to\naccelerate learning. Chen and Marculescu [78] (later Chen\net al. [79]) explored an alternative two-level strategy for RL-\nbased DVFS. Similar to Bai et al. [76], they used RL agents\nat a ﬁne-grain core level to select a V/F level based on an\nallocated share of the global power budget. They achieved\nfurther improvement by allocating power budget using a\nperformance-aware, albeit still heuristic-based, variant that\nconsiders relative application performance requirements.\nImes et al. [80] explored single-application system energy\noptimization for a broader range of conﬁgurations options\nincluding socket allocation, HyperThread usage, and pro-\ncessor DVFS. They identiﬁed several useful models, while\nnoting that further work could optimize models and pa-\nrameters. Analysis also provided insight into the beneﬁt\nfrom single-model multi-resource optimization, particularly\nfor neural networks. Finally, recent work by Tarsa et al.\n[81] considered an ML framework for post-silicon CPU\nadaptations using ﬁrmware updates to microcontroller-\nimplemented models. Signiﬁcant accommodations for sta-\ntistical blindspots limit the rate of service-level-agreement\nviolations while optimizing performance per watt for both\ngeneral-purpose and application-speciﬁc deployment.\nTask Allocation and Resource Management : In addi-\ntion to energy control, ML offers an approach to allocate\nresources to tasks or tasks to resources by predicting the\nimpact of various conﬁgurations on long-term performance.\nLu et al. [82] proposed a thermal-aware Q-learning method\nfor many-core task allocation. The agent considered only\ncurrent temperature (i.e., no application proﬁling or hard-\nware counters), receiving higher rewards for task assign-\nments resulting in greater thermal headroom. Evaluation\nindicated an average 4.3\u000eC reduction in peak temperature\ncompared to a heuristic approach. Nemirovsky et al. [83]\nintroduced a method for IPC prediction and task schedul-\ning on a heterogeneous architecture. They predicted IPC\nfor all task arrangements using ANNs, then selected the\narrangement with the highest IPC. Evaluation highlighted\nsigniﬁcant throughput gains ( >1:3x) using a deep (but high\noverhead) neural network, indicating one possible applica-\ntion for pruning (discussed in Section 5.2). Recent work has\nalso explored multi-level scheduling in hybrid CPU-GPU\nclusters. Zhang et al. [84] proposed a deep reinforcement\nlearning (DRL) framework to divide video workloads, ﬁrst\nat the cluster level (selecting a worker node) and then\nat the node level (CPU vs GPU). The two DRL models\nact separately, but still work together to optimize overall\nthroughput. Allocating resources to tasks is another possible\napproach. Early work by Bitirgen et al. [85] considered a\nsystem with four cores and four concurrent applications. In\ntheir approach, per-application ANN ensembles predicted\nIPC for 2,000 conﬁgurations at each interval (500K cycles).\nIPC predictions were then aggregated to choose the highest\nperforming overall system conﬁguration. Scaling concerns\nfor per-application ensembles and exponentially increasing\nconﬁguration spaces could be addressed in future work. Re-\ncent research has also considered low-level co-optimization\ninvolving multiple components/resources. For example,\nJain et al. [86] explored concurrent optimization of core\nDVFS, uncore DVFS, and dynamic LLC partitioning. These\noptions are optimized by individual agents (potentially\nlimiting co-optimization opportunities) at a relatively large\ninterval (1B instructions). Evaluation nevertheless indicated\nnoteworthy reductions in energy-delay-product throughmulti-resource optimization. Finally, work by Ding et al.\n[87] established a somewhat contradictory trend between\nmodel accuracy and system optimization goals based on\nimprovements for data scarcity and model bias. Speciﬁcally,\nthey found that state-of-the-art models exhibit diminish-\ning returns for accuracy and instead beneﬁt from domain\nknowledge (e.g., focus sampling on the optimal front).\nChip Layout : Work by Wu et al. [88] demonstrated\nuses for ML in chip layout, deviating from the common\napplications including control, prediction, and design space\nexploration. They used k-means to cluster ﬂip-ﬂops during\nphysical layout, minimizing clock wirelength at the expense\nof signal wirelength, noting that clock networks can con-\nsume more than 40% of chip power. They included con-\nstraints on maximum ﬂip-ﬂop displacement and cluster size,\ngenerating designs with 28.3% reduced displacement, 3.2%\nreduced total wirelength, and 4.8% reduced total switching\npower compared to the prior state-of-the-art approach.\nSecurity : Malware detection, a traditionally software-\nbased task, has been explored using machine learning to\nenable reliable hardware-based in-execution detection. For\nexample, Ozsoy et al. [89] test both logistic regression (LR)\nand neural network classiﬁers trained on low-level hard-\nware counters. Optimization based on reduced precision\nand feature selection provides high accuracy (100% malware\ndetection and less than 16% false positives) with minimal\noverhead (0.04% core power and 0.19% core logic area) for\nthe LR model.\n3.6 ML-Enabled Approximate Computing\nApproximate computing has many facets, including circuit\nlevel approximation (such as reduced precision adders),\ncontrol level approximation (relaxing timings, etc), and\ndata level approximation. Methods using ML generally fall\nwithin this last category, offering a powerful function/loop\napproximation technique that commonly provides 2-3 times\napplication speedup and energy reduction with limited im-\npact on output quality. Esmaeilzadeh et al. [90] introduced\nNPU, a new approach to programmable approximation\nusing neural networks. They developed a framework to\nrealize Parrot transformations that translate annotated code\nsegments into neural networks approximations. Tightly in-\ntegrating the NPU with the CPU allowed an average 2.3x\nspeedup and 3.0x energy reduction in studied applications.\nThis framework was later extended by Yazdanbakhsh et al.\n[91] to implement neural approximation on GPUs. Neu-\nral approximation was integrated into the existing GPU\npipeline, enabling component re-use and approximately\n2.5x speedup and 2.5x reduced energy. Grigorian et al.\n[92] presented a different approach for a multi-stage neu-\nral accelerator. Inputs are ﬁrst sent through a relatively\nlow accuracy/overhead neural accelerator, then checked for\nquality; acceptable results are committed, while low quality\napproximations are forwarded to an additional, more pre-\ncise, approximation stage. The problem with these works\nis that error is either constant [90], [91] or requires several\nstages with potentially redundant approximation [92]. For\nthat reason, Mahajan et al. [93] introduced MITHRA, a co-\ndesigned hardware-software control framework for neural\napproximation. MITHRA implements conﬁgurable output\nquality loss with statistical guarantees. ML classiﬁers predict\nindividual approximation error, allowing comparison to a\nquality threshold. Recent work by Oliveira et al. [94] also\nexplored approximation using low-overhead classiﬁcation\ntrees. Even with software-based execution, they achieved\n\n8\napplication speedup comparable to an NPU [90] hardware\nimplementation. Finally, ML has also been used to mitigate\nthe impact of faults in existing approximate accelerators.\nTaher et al. [95] observed that faults tend to manifest\nin a similar manner across many input test vectors. This\nobservations enables effective error compensation using a\nclassiﬁcation/regression model to correct output based on\npredicted faults for a given input.\n4 A NALYSIS OF CURRENT PRACTICE\nThis section examines varying techniques employed in ex-\nisting work. These comparisons emphasize potentially use-\nful design practices and strategies for future work.\nWork is divided into two categories that represent a nat-\nural division in design constraints and operating timescales\nand therefore correspond to differing design practices. The\nﬁrst category, online ML application, encompasses work\nthat directly applies ML techniques at run-time, even if\ntraining is performed ofﬂine. Design complexity in this\nwork is therefore inherently limited by practical constraints\nsuch as power, area, and real-time processing overhead. The\nsecond category, ofﬂine ML application, instead applies ML\nto guide architectural implementation, involving tasks such\nas design and simulation. Consequently, models for ofﬂine\nML application can exploit higher complexity and higher\noverhead options at the cost of training/prediction time.\n4.1 Online ML Application\nModel Selection : Online ML applications primarily use\neither decision trees or ANNs, in the case of supervised\nlearning models, and either Q-learning or deep Q-learning,\nin the case of RL models. Note that tasks for these learn-\ning approaches are not necessarily disjoint, particularly for\ncontrol. Fettes et al. [51] cast DVFS as both a supervised\nlearning regression task and as a reinforcement learning\ntask. The supervised learning approach predicted buffer\nutilization or change in buffer utilization to determine an\nappropriate DVFS mode. In contrast, the RL approach di-\nrectly used DVFS modes as the action space. Both models\ncan perform well, but the RL model is more universally\napplicable since the energy/throughput trade-off can be\ntailored to application needs and does not require threshold\ntuning. This certainly does not mean that RL is a one-model-\nﬁts-all solution. Supervised learning models ﬁnd strong\napplication in function approximation [90], [91], [92], [94]\nand branch prediction tasks [41], [42], which are far less\nsuitable (if not impossible) to approach using RL since these\ntasks cannot be represented well as a sequence of actions.\nImplementation & Overhead : Implementation of online\nML applications highlight limitations in data availability,\nstorage space for models, etc., indicating the need for an\nefﬁcient, and generally low complexity, model. These lim-\nitations will likely become more important to consider as\nmore research moves towards real-world implementation.\nSeveral NoC-based works [46], [56], [71] have applied\ndifferent methods for global data collection to support ML\nmodels. Daya et al. [56] implemented self-learning injection\nthrottling using a separate bufferless starvation network\nthat carries a starvation ﬂag, encoded as a one-hot N-bit\nvector for a network with N nodes. These starvation vectors\nare propagated to all nodes, allowing individual node-\nbased Q-learning agents to determine appropriate injection\nthrottling. Soteriou et al. [58] similarly used a dedicated\nnetworks to collect buffer utilization and VC occupancystatistics. The ANN-based DVFS control proposed by Won\net al. [71] eschewed an additional status/data network by\nencoding data into unused bits in standard packet headers.\nData is opportunistically collected by a central control unit\nas packets pass through its router. This method introduces\npotential concerns about data staleness, but prior work [96]\nobserved nearly identical performance to omniscient data\ncollection for sufﬁciently large (50K cycle) control windows.\nSmaller time windows can be accommodated by sending\ndedicated packets, as done by Savva et al. [46].\nImplementation can also consider the use of either hard-\nware or software models. Implementation using dedicated\nhardware will usually experience lower execution time\noverhead, but there are other considerations. Esmaeilzadeh\net al. [90] implemented a neural processor (NPU) for func-\ntion approximation using a dedicated hardware module.\nThey also considered a software implementation, but ob-\nserved a prohibitive increase in instruction count for soft-\nware execution compared to a baseline x86 function. Later\nwork by Oliveira et al. [94] found that function approxi-\nmation using a simple classiﬁcation tree can achieve com-\nparable results to NPU [90] for application speedup and\nerror rate in several applications (albeit somewhat worse on\naverage). Their purely software implementation highlights a\ntrade-off between area/power and accuracy/performance.\nWon et al. [71] observed a similar trade-off, choosing to\nimplement an ANN in software using an on-die microcon-\ntroller rather than dedicated hardware. This implementation\nconsumes several orders of magnitude more cycles (15K\ncycles for inference), but requires 50mW less average power\nthan a hardware implementation.\nApproaches for hardware implementation may also vary\nbased on the task. A “standard” ANN implementation is\nobserved in work by Savva et al. [46]. They incorporated\na ﬁnite state machine for control, an array of multiply-\naccumulate (MAC) units for calculation, a register array to\nload and store results, and a lookup-table-based activation\nfunction. Both MAC array width and calculation precision\ncan be adjusted to balance power/area and accuracy/speed.\nIn contrast, St. Amant et al. [41] implemented a percep-\ntron branch predictor using a mixed signal design. They\nrealized dot products in analog circuitry, leveraging tran-\nsistor sizing and current summing to achieve a feasible\noverhead. Variance also exists in hardware for RL mod-\nels. The “standard” Q-learning implementation requires a\nlookup table to store state-action values. Ipek et al. [33] as\nwell as Mukundan and Martinez [34] instead used CMAC\n[97], replacing a potentially extensive Q-learning table with\nmultiple coarse-grain overlapping tables. This approach also\nincluded hashing, using hashed state attributes to index the\nCMAC tables. Taken together, these two methods balance\ngeneralization and overhead, although may introduce colli-\nsions/interference depending on the task. Further pipelin-\ning the hashing, CMAC table lookup, and calculation allows\nmore possible action-values to be evaluated per cycle.\nOptimization : Online ML applications with online train-\ning beneﬁt from adaptivity to run-time workload charac-\nteristics. Despite these beneﬁts, low model accuracy can\nnegatively impact system performance, most notably at the\nstart of execution or during periods of high variance in\nworkload characteristics. Adaptations to control and learn-\ning can be considered to avoid these detrimental impacts.\nSome RL-based work [25] considered mitigating the impact\nof poor actions during exploration by introducing “shadow”\noperations. These operations are low conﬁdence actions\n\n9\nsuggested by the model that are still used in model updates\nbut not executed by the system. Consequently, the model\ngains feedback on the goodness of the action without nega-\ntively impacting the system. In a supervised learning based\ncontrol task, Won et al. [71] trained an ANN online using\ncontrol actions made by a PI controller, which exhibits far\nless start-up delay. Following training, control decisions are\nmade using a hybrid combination based on error and con-\nsistency, allowing complementary control. In the simplest\ncase, checking the performance of a default conﬁguration,\nas in [38], provides a guarantee that the ML model will not\nperform worse than the default, but can perform better.\nIn most works, ML models replace existing approaches\n(commonly a heuristic). Nevertheless, several recent works\n[28], [45] have demonstrated signiﬁcant advantages by\ncombining both traditional (non-ML) and ML approaches.\nThese improvements are derived from the orthogonal\nprediction/decision-making capabilities of the two ap-\nproaches, thus enabling synergistic performance improve-\nments. This method can also enable lower-cost ML appli-\ncation by focusing on particular shortcomings in traditional\napproaches. Both recent works [28], [45] consider just branch\nprediction, thus signiﬁcant opportunities exist to explore\nthis potential co-design paradigm.\n4.2 Ofﬂine ML Applications\nModel/Feature Selection : Ofﬂine ML applications gener-\nally exhibit substantial model/feature diversity since the\nmodel itself is not tied to a particular architecture. Model\nand feature selection therefore focuses more on maxi-\nmizing model accuracy while minimizing overall learn-\ning/prediction time. Design space exploration, in particular,\ncan be approached using either iterative search methods\nfor direct optimization or supervised learning methods to\nselect optimal points based on the predicted optimality of\na design. Several works [60], [61], [62] used an iterative\nSTAGE [98] algorithm that optimizes local search for 3D\nNoC links by learning an evaluation function to predict local\nsearch results from a given starting point. Recent work has\ninstead applied deep reinforcement learning [63] to router-\nless NoC design. The proposed Monte Carlo tree search,\nalong with actions suggested by a convolutional neural\nnetwork, provide a highly efﬁcient search process. Parallel\nthreads are also utilized to scale design space exploration\nwith increasing computational resources. System-level de-\nsign space exploration has favored more standard super-\nvised learning approaches [17], [64], [67]. Speciﬁc model\nchoices vary, with linear [17], [64] and non-linear [67] regres-\nsion models, as well as random forests and neural networks\n[64] ﬁnding implementation. As in online ML applications,\ndiscussed in Section 4.1, some tasks are naturally limited to\nsupervised learning methods. Cross-architecture prediction\nis an exemplar [12], [13], [15], [19], [20].\nOptimization : The usefulness of an ML model in ofﬂine\nML applications is largely determined by overhead relative\nto traditional design approaches. Optimization therefore\nprimarily focuses on improving data efﬁciency and overall\nmodel accuracy.\nEnsemble methods have been proposed in online ML ap-\nplications [38], but primarily ﬁnd application in ofﬂine ML\napplications as ensembles can be made arbitrarily large (rel-\native to available computation resources). Several optimiza-\ntions have been suggested to improve efﬁciency. Jooya et al.\n[17] trained many neural networks using slightly different\nconﬁgurations and generated an ensemble using a subset ofthe models that generalized well and were most insensitive\nto input noise. They further introduced outlier detection\nby ﬁltering predictions whose performance and/or power\npredictions differ greatly from the closest conﬁguration in\ntraining data. Ardalani et al. [20] instead kept all 100 models\nthat they trained, noting that models may be very strong\npredictors in one application but weak predictors in another.\nThey remedied this dilemma by selecting only the 60 closest\nindividual predictions to the median prediction.\nSampling method optimization, while not unique to\narchitecture tasks, are nevertheless important to consider\nin improving model accuracy. Sangaiah et al. [67] consid-\nered potential systematic biases in their uncore performance\nprediction model. Speciﬁcally, they observed that uniform\nrandom sampling may not adequately capture performance\nrelationships in a non-uniform conﬁguration space (as\nin cache conﬁgurations using powers of two for sizing).\nThey therefore used a low-discrepancy sampling technique,\nSOBOL [99], to remove this systematic bias and prevent\nperformance over-prediction for low-end conﬁgurations.\n4.3 Domain Knowledge & Model Interpretation\nThe powerful relationship learning capabilities offered by\nML algorithms enable black-box implementation in many\ntasks (i.e., without consideration for task-speciﬁc charac-\nteristics), but may fail to capitalize on additional domain\nknowledge that could improve interpretability or overall\nmodel performance. Additionally, in some applications, do-\nmain knowledge can help identify aberrant behavior and,\nagain, improve overall model usefulness. These themes are\nhighlighted in several speciﬁc works, but can be generally\napplicable for ML applied to architecture.\nOne approach uses mechanistic-empirical models, syn-\nthesizing a domain knowledge based mechanistic frame-\nwork with empirical ML based learning for speciﬁc param-\neters. These models simplify implementation compared to\npurely mechanistic models [11], can avoid incorrect assump-\ntions made in purely mechanistic models [66], and can offer\nhigher accuracy than purely empirical models by avoiding\noverﬁtting [11]. Eyerman et al. [11] also demonstrated how\nthese models can be used to construct CPI stacks, allowing\nmeaningful alternative design comparisons.\nDeng et al. [38], in their work predicting optimal NVM\nwrite strategies, presented a case for tuning ML models\nbased on task speciﬁc domain knowledge. Following ini-\ntial analysis, they discovered how a single conﬁguration\nparameter (wear quota) can result in higher complexity\nand sub-optimal prediction accuracy for IPC and system\nenergy, even with quadratic regression and gradient boost-\ning models. Excluding wear quota from the conﬁguration\nspace, then later applying it to the predicted optimal con-\nﬁguration, provided a 2-6% improvement in prediction\naccuracy. Ardalani et al. [20] similarly examined inherent\nimperfections in their learning model for cross-platform\nperformance prediction. Some predictions can be easy for\nlearning models and hard for humans, representing an ideal\nscenario for ML application; the converse can also be true.\nIn both cases, ML application is strengthened by considering\ntask characteristics.\n5 F UTURE WORK\nThis section synthesizes observations and analysis from\nSection 3 and Section 4 to identify opportunities and detail\nthe need for future work. These opportunities may come\n\n10\nat the model level, exploiting improved implementation\nstrategies and learning capabilities, or at the application\nlevel, addressing the need for generalized tools or exploring\naltogether new areas.\n5.1 Investigating Models & Algorithms\nExisting works generally apply ML at a single time-scale or\nlevel of abstraction. These limitations motivate investigation\ninto models and algorithms that capture the hierarchical\nnature of architecture, both in terms of system design and\nexecution characteristics.\nPerform Phase-level Prediction : Application analysis\nusing basic blocks [100] has long been a useful method for\nsimulation, made possible by identifying unique and repre-\nsentative phases in program execution. Phase-level predic-\ntion offers analogous beneﬁts for ML applied to architec-\nture. A few recent works, in particular, have demonstrated\npromising results, with high accuracy for both performance\nprediction [13] as well as energy and reliability (lifetime)\n[38]. In general, most work [2], [17], [67] has not yet adopted\nphase-level prediction techniques (or does not explicitly\nmention their methodology). Speciﬁcally, future work could\nexplore predictions for control and system reconﬁguration\nbased on phase-level behavior, rather than either static\nwindows [85] or application-level behavior [75], [101].\nExploit Nanosecond Scale : Coarse-grain ML, used in\nmany DVFS control schemes, provides signiﬁcant beneﬁts\nover standard control-theory based schemes, yet ﬁne grain\ncontrol can provide even greater efﬁciency. Speciﬁcally,\nanalysis by Bai et al. [76] indicated very rapid changes in\nenergy consumption, on the order of 1K instructions for\nsome applications. Exploiting these brief intervals requires\ncareful consideration for both the model and the algorithm.\nFuture work may optimize existing algorithms such as\nexperience sharing [102] and hybrid/tandem control [71],\nor consider approaches more suited for novel models (e.g.,\nhierarchical models). These approaches could also enable\nadditional nanosecond-scale co-optimization opportunities,\nsuch as dynamic LLC partitioning, to extract further efﬁ-\nciency gains.\nApply Hierarchical & Multi-agent Models : Application\nexecution in computer systems naturally follows a hierar-\nchical structure in which, at the top level, tasks are allo-\ncated to cores, then cores are assigned dynamic power and\nresource budgets (e.g., LLC space), and ﬁnally, at the bot-\ntom level, data/control packets are sent between cores and\nmemory. Consequently, a single machine learning model\nmay struggle to learn appropriate design/control strategies.\nFurthermore, in the case of reinforcement learning models,\nit can be exceedingly difﬁcult to accurately assign credits to\nspeciﬁc low-level actions based on their impact on overall\nexecution time, energy efﬁciency, etc. One promising ap-\nproach in recent work is hierarchical models [103]. Hier-\narchical reinforcement learning models enable goal-directed\nlearning that is particularly beneﬁcial in environments with\nsparse feedback (e.g., task allocation). Applying hierarchical\nlearning to architecture could therefore enable more effec-\ntive multi-level design and control. Multi-agent models are\nanother promising area in machine learning research. These\nmodels tend to focus on problems in which reinforcement\nlearning agents have only partial observability of their\nenvironment. Although partial-observability may not be a\nprimary concern in individual computer systems, recent\nwork [104] has applied this concept to internet packet rout-ing and demonstrated convergence beneﬁts via improved\ncooperation between individual agents.\n5.2 Enhancing Implementation Strategies\nIncreasingly complex models require effective strategies and\ntechniques to reduce overhead and enable practical imple-\nmentation. Model pruning and weight quantization, as dis-\ncussed below, are two particularly effective techniques with\nproven beneﬁts in accelerators, while many other promising\napproaches are also being explored [105].\nExplore Model Pruning : Model complexity can be a\nlimiting factor in online ML applications. A standard Q-\nlearning approach requires a potentially extensive table to\nstore action-values. Neural network based approaches for\nboth RL (in Deep Q-Networks) and supervised learning\nrequire network weight storage and additional processing\ncapabilities. Neural networks, in particular, are therefore\ngenerally constrained to a few layers in existing work, with\nmany using just one hidden layer [46], [71], [85], [93] and\nsome using one or two hidden layers [90], [91].\nRecent research on neural networks has demonstrated\npromising methods to reduce model complexity through\npruning [106], [107]. The general intuition is that many\nconnections are unnecessary and can therefore be pruned.\nIteratively pruning a high-complexity network, then re-\ntraining from scratch on the sparse architecture achieves\ngood results, with some work demonstrating very high\nsparsity (>90%) and little accuracy penalty [107].\nPruning applied to neural networks, either in deep Q-\nlearning or supervised learning regression/classiﬁcation,\noffers a method to train complex models for high accuracy,\nthen prune for feasible implementation. Deep Q-learning\napplication has, thus far, been limited to two works [51],\n[59], one of which is currently impractical to implement\n[59]. Future work may instead consider pruned deep Q-\nnetworks as a useful alternative to standard Q-learning\napproaches. Pruning also provides a substantial opportunity\nfor future work on performance prediction (as in DVFS con-\ntrol) and function approximation (as in ML-enabled approx-\nimate computing). System-level approximation (discussed\nin Section 5.4) may particularly beneﬁt from pruning high\ncomplexity models.\nExplore Quantization : Existing work primarily applies\nquantization to state values in Q-learning to enable practical\nQ-table implementation. Similarly, neural networks beneﬁt\nfrom potential reduction in execution time, power, and\narea by reducing multiply-accumulator precision. Recent\nworks, however, suggest a new spectrum of opportunities\nfor alternative hardware implementations based on reduced\nprecision models.\nBinary neural networks, for example, quantize weights\nto be either +1 or -1, enabling computation based on bit-\nwise operations rather than arithmetic operations [108]. An\nadditional approach considered quantizing neural network\nweights into ﬁnite (but non-binary) subsets in order to re-\nplace multiply operations with lookup-table accesses [109],\nallowing higher precision and lower execution time, albeit\nwith potentially higher area cost. Future work on ML appli-\ncation can exploit similar hardware implementations while\nexploring optimal quantization levels for various tasks and\ncontrol schemes.\n5.3 Developing Generalized Tools\nExisting machine learning tools (e.g., scikit-learn [110]) have\nproven useful for relatively simple ML applications. Never-\n\n11\ntheless, complex design and simulation tasks require more\nsophisticated tools to enable rapid task-speciﬁc optimiza-\ntions using general-purpose frameworks.\nEnable Broad Application & Optimization : Purpose-\nbuilt architectural tools, similar to heuristic design strate-\ngies, can be useful in enabling design, exploration, and sim-\nulation that satisﬁes a common use case. These approaches\nmay still be limited in their application to a speciﬁc problem,\noptimization criteria, system conﬁguration, etc. Given the\nfast-paced nature of architectural research (and machine\nlearning research), there is a need to develop more gener-\nalized tools and easily modiﬁable frameworks to address\nbroader applications and optimization options.\nML-based design tools are especially promising, with\nrecent works demonstrating successful application to im-\nmense design spaces (e.g., exceeding 1012in [63]). Oppor-\ntunities for new design tools are not, however, limited to\nspeciﬁc architectural components. Chip layout is a notable\nexample in which even simple clustering algorithms can\ndramatically outperform existing heuristic approaches [88].\nFuture work can also continue to develop more broadly\napplicable tools for performance and power prediction.\nIn particular, recent work on cross-platform performance\nprediction [21] suggests the possibility for high prediction\naccuracy with purely static features, thus representing an-\nother potential area for additional research.\nEnable Widespread Usage : Generalized tools enable\nadditional beneﬁt by facilitating rapid design and evalua-\ntion. Using a machine learning approach, one might simply\nmodify training data (in a supervised learning setting) or\naction/reward representation (in a reinforcement learning\nsetting) rather than exploring models, data representation\nstrategies, search approaches, etc., possibly without a priori\nmachine learning experience. For example, recent work\n[63] envisioned reuse of a deep reinforcement learning\nframework for diverse NoC-related design tasks involving\ninterposers, chiplets, and accelerators. While the framework\nmight not be compatible with all work, especially in novel\nareas, it may provide a better foundation for machine learn-\ning application to architectures, especially for individuals\nwith limited machine learning background.\n5.4 Embracing Novel Applications\nOpportunities abound for future work to apply ML to\nboth existing and emerging architectures, replace heuristic\napproaches to enable long-term scaling, and advance capa-\nbilities for automated design.\nExplore Emerging Technologies : Several proposals [30],\n[37], [38], [39] establish how ML can be used to optimize\nboth standard (energy, performance) and non-standard (life-\ntime, tail-latency) criteria. These non-standard criteria are\nshown to be particularly problematic in emerging technolo-\ngies as these technologies cannot easily ﬁnd widespread\nimplementation without some reliability guarantees. Apply-\ning ML to optimize both standard and non-standard criteria\ntherefore provides a method for future work to intelligently\nbalance control strategies dynamically, rather than relying\nupon a heuristic approach.\nExplore Emerging Architectures : ML application to\nemerging architectures presents a similar beneﬁt by en-\nabling rapid development, even with limited best-practice\nknowledge, which may take time to develop. Work in\nlong-standing design areas, such as task allocation and\nbranch prediction, may incorporate best-practice domain\nknowledge to guide approaches, whether applying ML orsome other traditional method. Best practices for emerging\narchitectures may not be immediately obvious. For example,\nML application to 2D photonic NoCs [48], 2.5D processing-\nin-memory designs [24], and 3D NoCs [60], [61], [62] have\nall shown strong performance over existing approaches.\nFuture work can explore ML application to novel concerns\nsuch as connectivity and reconﬁgurability in interposers and\ndomain-speciﬁc accelerators.\nExpand System-Level Approximate Computing : As\ndiscussed in Section 3.6, ML applications for approximate\ncomputing have been mostly limited to function approxima-\ntion. However, there are many other facets of approximate\ncomputing that have already been implemented in non-ML\nworks, which can be reap additional beneﬁts by utilizing\nML. For example, APPROX-NoC [111] reduces network\ntrafﬁc using approximated and encoded data. Another work\nexplored a multi-faceted approximation scheme for a smart\ncamera system [112] using approximate DRAM (lower re-\nfresh rate), approximate algorithms (loop skipping) and ap-\nproximate data (lower sensor resolution). Existing compiler-\nbased work [113] for system-wide approximation enhances\nprior capabilities to determine approximable code, but relies\nupon heuristic searches with representative inputs. Conse-\nquently, this method does not provide statistical guarantees,\nsuch as those in MITHRA [93]. Future work may explore\nsearches based on deep reinforcement learning (or perhaps\nhierarchical reinforcement learning) to incorporate existing\napproximation techniques into a scalable framework for\nhigh-dimensional approximation and co-optimization.\nImplement System-Wide, Component-Level Optimiza-\ntion: Recent work has begun to explore broader ML-based\ndesign and optimization strategies. MLNoC [64] explores\na wide SoC feature space for NoC design optimization.\nCore and uncore DVFS are combined in Machine Learned\nMachines [86], along with LLC dynamic cache partitioning\nto explore co-optimization potential at run-time. Related\nDNN accelerator research [114] proposed co-optimization of\nhardware-based (e.g., bitwidth) and neural network param-\neters (e.g., L2 regularization). These works motivate consid-\neration for system-wide, component-level ML application.\nExisting system-level optimization schemes (e.g., [80],\n[83], [101]) consider conﬁguration opportunities at just a\nsingle and very high level of abstraction (e.g., task allocation\nor big.LITTLE core conﬁgurations). Although these works\nmay include low-level features such as NoC utilization and\nDRAM bandwidth in their ML models, they do not account\nfor the impact of component-level optimization techniques\nsuch as NoC packet routing, cache prefetching, etc. We in-\nstead envision an ML-based system-wide, component-level\nframework for run-time optimization. In this framework,\ncontrol decisions would involve a larger hierarchy of both\ncomponent-level (or lower) features and control options as\nwell as higher-level decisions, allowing a more comprehen-\nsive and precise perspective for run-time optimization.\nAdvance Automated Design : While fully automated\ndesign might be the ultimate objective, increasingly auto-\nmated design is nevertheless an important milestone for\nfuture work. Speciﬁcally, as more tasks are automated,\nthere is greater potential to enable a positive-feedback loop\nbetween machine learning and architecture, providing im-\nmense practical beneﬁts for both ﬁelds. There are, of course,\na number of intervening challenges that must be solved,\neach of which represents a substantial area for future work.\nOne challenge involves modeling the hierarchical struc-\nture of architectural components. This model would likely\nbeneﬁt from integrating pertinent characteristics across the\n\n12\nsystem stack, from process technology to full-system behav-\nior, thus generating a highly accurate representation for real-\nworld systems. Another research direction could explore\nmethods for machine learning models to identify potential\ndesign aspects for improvement. Ideally, this model could\nexplore not just reconﬁguration of pre-existing options (as\nin [115]), but also generate novel conﬁguration options. Inte-\ngrating these and potentially other capabilities may provide\na framework to advance automated design.\n6 C ONCLUSION\nMachine learning has rapidly become a powerful tool in\narchitecture, with established applicability to design, opti-\nmization, simulation, and more. Notably, ML has already\nbeen successfully applied to many components, including\nthe core, cache, NoC, and memory, with performance often\nsurpassing prior state-of-the-art analytical, heuristic, and\nhuman-expert strategies. Widespread application is further\nfacilitated by diverse training methods and learning mod-\nels, allowing effective trade-offs between performance and\noverhead based on task requirements. These advancements\nare likely just the beginning of a revolutionary shift in\narchitecture.\nOptimization opportunities at the model level involv-\ning pruning and quantization offer broad beneﬁts by en-\nabling more practical implementation. Similarly, opportu-\nnities abound to extend existing work using ever-more-\npowerful ML models, enabling ﬁner granularity, system-\nwide implementation. Finally, ML may be applied to en-\ntirely new aspects of architecture, learning hierarchical or\nabstract representations to characterize full system behavior\nbased on both high and low level details. These extensive\nopportunities, along with yet to be envisioned possibilities,\nmay eventually close the loop on highly (or even fully)\nautomated architectural design.\nREFERENCES\n[1] S. Kotsiantis, “Supervised machine learning: A review of classiﬁ-\ncation techniques,” in Proceedings of the 2007 Conference on Emerg-\ning Artiﬁcial Intelligence Applications in Computer Engineering: Real\nWorld AI Systems with Applications in eHealth, HCI, Information\nRetrieval and Pervasive Technologies , pp. 3–24, 2007.\n[2] N. Mishra, H. Zhang, J. D. Lafferty, and H. Hoffman, “A proba-\nbilistic graphical model-based approach for minimizing energy\nunder performance constraints,” in International Conference on\nArchitectural Support for Programming Languages and Operating\nSystems (ASPLOS) , Mar. 2015.\n[3] V . N. Vapnik, “An overview of statistical learning theory,” IEEE\nTransactions on Neural Networks , vol. 10, Sep. 1999.\n[4] J. Shlens, “A tutorial on principal component analysis,” 2014.\narXiv:1404.1100.\n[5] M. Alawieh, F. Wang, and X. Li, “Efﬁcient hierarchical perfor-\nmance modeling for integrated circuits via bayesian co-learning,”\ninDesign Automation Conference (DAC) , June 2017.\n[6] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduc-\ntion. Cambridge, USA: MIT Press, 2nd ed., 1998.\n[7] I. Guyon and A. Elisseeff, “An introduction to variable and\nfeature selection,” The Journal of Machine Learning Research , vol. 3,\npp. 1157–1182, Mar. 2003.\n[8] J. Li, K. Chen, S. Wang, F. Morstatter, R. P . Trevino, J. Tang, and\nH. Liu, “Feature selection: A data perspective,” ACM Computing\nSurveys , vol. 50, Jan. 2018.\n[9] E. Ipek, S. A. McKee, B. R. de Supinski, M. Schulz, and R. Caru-\nana, “Efﬁciently exploring architectural design spaces via predic-\ntive modeling,” in International Conference on Architectural Support\nfor Programming Languages and Operating Systems (ASPLOS) , Oct.\n2006.\n[10] B. Ozisikyilmaz, G. Memik, and A. Choudhary, “Machine learn-\ning models to predict performance of computer system design\nalternatives,” in International Conference on Parallel Processing\n(ICPP) , Sept. 2008.[11] S. Eyerman, K. Hoste, and L. Eeckhout, “Mechanistic-empirical\nprocessor performance modeling for constructing cpi stacks on\nreal hardware,” in International Symposium on Performance Analy-\nsis of Systems and Software (ISP ASS) , Apr. 2011.\n[12] X. Zheng, P . Ravikumar, L. K. John, and A. Gerstlauer, “Learning-\nbased analytical cross-platform performance prediction,” in Inter-\nnational Conference on Embedded Computer Systems: Architectures,\nModeling, and Simulation (SAMOS) , July 2015.\n[13] X. Zheng, L. K. John, and A. Gerstlauer, “Accurate phase-level\ncross-platform power and performance estimation,” in Design\nAutomation Conference (DAC) , June 2016.\n[14] N. Agarwal, T. Jain, and M. Zahran, “Performance prediction\nfor multi-threaded applications,” in International Workshop on AI-\nassisted Design for Architecture (AIDArc), held in conjunction with\nISCA , June 2019.\n[15] W. Jia, K. A. Shaw, and M. Martonosi, “Stargazer: Automated\nregression-based gpu design space exploration,” in International\nSymposium on Performance Analysis of Systems and Software (IS-\nP ASS) , Apr. 2012.\n[16] G. Wu, J. L. Greathouse, A. Lyashevsky, N. Jayasena, and\nD. Chiou, “Gpgpu performance and power estimation using ma-\nchine learning,” in International Symposium on High-Performance\nComputer Architecture (HPCA) , Feb. 2015.\n[17] A. Jooya, N. Dimopoulos, and A. Baniasadi, “Multiobjective gpu\ndesign space exploration optimization,” in International Confer-\nence on High Performance Computing & Simulation (HPCS) , July\n2016.\n[18] T.-R. Lin, Y. Li, M. Pedram, and L. Chen, “Design space explo-\nration of memory controller placement in throughput processors\nwith deep learning,” in IEEE Computer Architecture Letters , vol. 18,\nMar. 2019.\n[19] I. Baldini, S. J. Fink, and E. Altman, “Predicting gpu performance\nfrom cpu runs using machine learning,” in International Sympo-\nsium on Computer Architecture and High Performance Computing\n(SBAC-P AD) , Oct. 2014.\n[20] N. Ardalani, C. Lestourgeon, K. Sankaralingam, and X. Zhu,\n“Cross-architecture performance prediction (xapp) using cpu\ncode to predict gpu performance,” in International Symposium on\nMicroarchitecture (MICRO) , June 2015.\n[21] N. Ardalani, U. Thakker, A. Albarghouthi, and K. Sankaralingam,\n“A static analysis-based cross-architecture performance predic-\ntion using machine learning,” in International Workshop on AI-\nassisted Design for Architecture (AIDArc), held in conjunction with\nISCA , June 2019.\n[22] K. O’Neal, P . Brisk, E. Shriver, and M. Kishinevsky, “Hal-\nwpe: Hardware-assisted light weight performance estimation for\ngpus,” in Design Automation Conference (DAC) , June 2017.\n[23] Y. Li, D. Penney, A. Ramamurthy, and L. Chen, “Characterizing\non-chip trafﬁc patterns in general-purpose gpus: A deep learning\napproach,” in International Conference on Computer Design (ICCD) ,\nNov. 2019.\n[24] A. Pattnaik, X. Tang, A. Jog, O. Kayran, A. K. Mishra, M. T.\nKandemir, O. Mutlu, and C. R. Das, “Scheduling techniques\nfor gpu architectures with processing-in-memory capabilities,”\ninInternational Conference on Parallel Architectures and Compilation\nTechniques (P ACT) , Sept. 2016.\n[25] L. Peled, S. Mannor, U. Weiser, and Y. Etsion, “Semantic locality\nand context-based prefetching using reinforcement learning,” in\nInternational Symposium on High-Performance Computer Architecture\n(HPCA) , June 2015.\n[26] Y. Zeng and X. Guo, “Long short term memory based hardware\nprefetcher,” in International Symposium on Memory Systems (Mem-\nSys), Oct. 2017.\n[27] P . Braun and H. Litz, “Understanding memory access patterns\nfor prefetching,” in International Workshop on AI-assisted Design\nfor Architecture (AIDArc), held in conjunction with ISCA , June 2019.\n[28] E. Bhatia, G. Chacon, S. Pugsley, E. Teran, P . V . Gratz, and D. A.\nJim´enez, “Perceptron-based prefetch ﬁltering,” in International\nSymposium on Computer Architecture (ISCA) , June 2019.\n[29] E. Teran, Z. Wang, and D. A. Jim ´enez, “Perceptron learning for\nreuse prediction,” in International Symposium on Microarchitecture\n(MICRO) , Oct. 2016.\n[30] H. Wang, X. Yi, P . Huang, B. Cheng, and K. Zhou, “Efﬁcient ssd\ncaching by avoiding unnecessary writes using machine learn-\ning,” in International Conference on Parallel Processing (ICPP) , Aug.\n2018.\n[31] A. Margaritov, D. Ustiugov, E. Bugnion, and B. Grot, “Virtual\naddress translation via learned page tables indexes,” in Conference\non Neural Information Processing Systems (NeurIPS) , Dec. 2018.\n[32] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The\ncase for learned index structures,” in International Conference on\nManagement of Data (SIGMOD) , June 2018.\n\n13\n[33] E. Ipek, O. Mutlu, J. F. Martinez, and R. Caruana, “Self-\noptimizing memory controllers: A reinforcement learning ap-\nproach,” in International Symposium on High-Performance Computer\nArchitecture (HPCA) , June 2008.\n[34] J. Mukundan and J. F. Martinez, “Morse: Multi-objective re-\nconﬁgurable self-optimizing memory scheduler,” in International\nSymposium on High-Performance Computer Architecture (HPCA) ,\nFeb. 2012.\n[35] S. Manoj, H. Yu, H. Huang, and D. Xu, “A q-learning based\nself-adaptive i/o communication for 2.5d integrated many-core\nmicroprocessor and memory,” IEEE Transactions on Computers ,\nvol. 65, June 2015.\n[36] S. Wang and E. Ipek, “Reducing data movement energy via\nonline data clustering and encoding,” in International Symposium\non Microarchitecture (MICRO) , Oct. 2016.\n[37] W. Kang and S. Yoo, “Dynamic management of key states for\nreinforcement learning-assisted garbage collection to reduce long\ntail latency in ssd,” in Design Automation Conference (DAC) , June\n2018.\n[38] Z. Deng, L. Zhang, N. Mishra, H. Hoffman, and F. T. Chong,\n“Memory cocktail therapy: A general learning-based framework\nto optimize dynamic tradeoffs in nvms,” in International Sympo-\nsium on Microarchitecture (MICRO) , Oct. 2017.\n[39] J. Xiao, Z. Xiong, S. Wu, Y. Yi, H. Jin, and K. Hu, “Disk failure\nprediction in data centers via online learning,” in International\nConference on Parallel Processing (ICPP) , June 2018.\n[40] D. A. Jim ´enez and C. Lin, “Dynamic branch prediction with per-\nceptrons,” in International Symposium on High-Performance Com-\nputer Architecture (HPCA) , Jan. 2001.\n[41] R. S. Amant, D. A. Jim ´enez, and D. Burger, “Low-power, high-\nperformance analog neural branch prediction,” in International\nSymposium on Microarchitecture (MICRO) , Nov. 2008.\n[42] D. A. Jim ´enez, “An optimized scaled neural branch predictor,” in\nInternational Conference on Computer Design (ICCD) , Oct. 2011.\n[43] E. Garza, S. Mirbagher-Ajorpaz, T. A. Khan, and D. A. Jim ´enez,\n“Bit-level perceptron prediction for indirect branches,” in Inter-\nnational Symposium on Computer Architecture (ISCA) , June 2019.\n[44] A. Seznec, “Tage-sc-l branch predictors again,” in 5th JILP Work-\nshop on Computer Architecture Competitions: Championship Branch\nPrediction, held in conjunction with ISCA , 2016.\n[45] S. J. Tarsa, C.-K. Lin, G. Keskin, G. Chinya, and H. Wang,\n“Improving branch prediction by modeling global history with\nconvolutional neural networks,” in International Workshop on AI-\nassisted Design for Architecture (AIDArc), held in conjunction with\nISCA , June 2019.\n[46] A. G. Savva, T. Theocharides, and V . Soteriou, “Intelligent on/off\ndynamic link management for on-chip networks,” in Journal of\nElectrical and Computer Engineering - Special issue on Networks-\non-Chip: Architectures, Design Methodologies, and Case Studies , Jan\n2012.\n[47] D. DiTomaso, A. Sikder, A. Kodi, and A. Louri, “Machine learn-\ning enabled power-aware network-on-chip design,” in Design,\nAutomation and Test in Europe (DATE) , Mar. 2017.\n[48] S. V . Winkle, A. Kodi, R. Bunescu, and A. Louri, “Extending\nthe power-efﬁciency and performance of photonic interconnects\nfor heterogeneous multicores with machine learning,” in In-\nternational Symposium on High-Performance Computer Architecture\n(HPCA) , Feb. 2018.\n[49] M. F. Reza, T. T. Le, B. De, M. Bayoumi, and D. Zhao, “Neuro-\nnoc: Energy optimization in heterogeneous many-core noc using\nneural networks in dark silicon era,” in International Symposium\non Circuits and Systems (ISCAS) , May 2018.\n[50] M. Clark, A. Kodi, R. Bunescu, and A. Louri, “Lead: Learning-\nenabled energy-aware dynamic voltage/frequency scaling in\nnocs,” in Design Automation Conference (DAC) , June 2018.\n[51] Q. Fettes, M. Clark, R. Bunescu, A. Karanth, and A. Louri,\n“Dynamic voltage and frequency scaling in nocs with supervised\nand reinforcement learning techniques,” IEEE Transactions on\nComputers , vol. 68, Mar. 2019.\n[52] J. A. Boyan and M. L. Littman, “Packet routing in dynami-\ncally changing networks: a reinforcement learning approach,”\nAdvances in Neural Information Processing Systems , vol. 6, pp. 671–\n678, 1994.\n[53] M. Majer, C. Bobda, A. Ahmadinia, and J. Teich, “Packet routing\nin dynamically changing networks on chip,” in International\nParallel and Distributed Processing Symposium (IPDPS) , Apr. 2005.\n[54] C. Feng, Z. Lu, A. Jantsch, J. Li, and M. zhang, “A reconﬁgurable\nfault-tolerant deﬂection routing algorithm based on reinforce-\nment learning for network-on-chip,” in International Workshop on\nNetwork on Chip Architectures (NoCArc), held in conjunction with\nMICRO , Dec. 2010.\n[55] M. Ebrahimi, M. Daneshtalab, and F. Farahnakian, “Haraq:\nCongestion-aware learning model for highly adaptive routingalgorithm in on-chip networks,” in International Symposium on\nNetworks-on-Chip (NOCS) , June 2012.\n[56] B. K. Daya, L.-S. Peh, and A. P . Chandrakasan, “Quest for high-\nperformance bufferless nocs with single-cycle express paths and\nself-learning throttling,” in Design Automation Conference (DAC) ,\nJune 2016.\n[57] B. Wang, Z. Lu, and S. Chen, “Ann based admission control for\non-chip networks,” in Design Automation Conference (DAC) , June\n2019.\n[58] V . Soteriou, T. Theocharides, and E. Kakoulli, “A holistic ap-\nproach towards intelligent hotspot prevention in network-on-\nchip-based multicores,” IEEE Transactions on Computers , vol. 65,\nMay 2015.\n[59] J. Yin, Y. Eckert, S. Che, M. Oskin, and G. H. Loh, “Toward\nmore efﬁcient noc arbitration: A deep reinforcement learning\napproach,” in International Workshop on AI-assisted Design for\nArchitecture (AIDArc), held in conjunction with ISCA , June 2018.\n[60] S. Das, J. R. Doppa, D. H. Kim, P . P . Pande, and K. Chakrabarty,\n“Optimizing 3d noc design for energy efﬁciency: A machine\nlearning approach,” in International Conference on Computer-Aided\nDesign (ICCAD) , Nov. 2015.\n[61] S. Das, J. R. Doppa, P . P . Pande, and K. Chakrabarty, “Energy-\nefﬁcient and reliable 3d network-on-chip (noc): Architectures and\noptimization algorithms,” in International Conference on Computer-\nAided Design (ICCAD) , Nov. 2016.\n[62] B. K. Joardar, R. G. Kim, J. R. Doppa, P . P . Pande, D. Marculescu,\nand R. Marculescu, “Learning-based application-agnostic 3d noc\ndesign for heterogeneous manycore systems,” IEEE Transactions\non Computers , vol. 68, June 2019.\n[63] T.-R. Lin, D. Penney, M. Pedram, and L. Chen, “Optimizing\nrouterless network-on-chip designs:an innovative learning-based\nframework,” May 2019. arXiv:1905.04423.\n[64] N. Rao, A. Ramachandran, and A. Shah, “Mlnoc: A machine\nlearning based approach to noc design,” in International Sym-\nposium on Computer Architecture and High Performance Computing\n(SBAC-P AD) , Sept. 2018.\n[65] S. Bandyopadhyay, S. Saha, U. Maulik, and K. Deb, “A simulated\nannealing-based multiobjective optimization algorithm: Amosa,”\nIEEE Transactions on Evolutionary Computation , vol. 12, May 2008.\n[66] Z. Qian, D.-C. Juan, P . Bogdan, C.-Y. Tsui, D. Marculescu,\nand R. Marculescu, “Svr-noc: A performance analysis tool for\nnetwork-on-chips using learning-based support vector regression\nmodel,” in Design, Automation and Test in Europe (DATE) , Mar.\n2013.\n[67] K. Sangaiah, M. Hempstead, and B. Taskin, “Uncore rpd: Rapid\ndesign space exploration of the uncore via regression modeling,”\ninInternational Conference on Computer-Aided Design (ICCAD) ,\nNov. 2015.\n[68] D. DiTomaso, T. Boraten, A. Kodi, and A. Louri, “Dynamic error\nmitigation in nocs using intelligent prediction techniques,” in\nInternational Symposium on Microarchitecture (MICRO) , Oct. 2016.\n[69] K. Wang, A. Louri, A. Karanth, and R. Bunescu, “High-\nperformance, energy-efﬁcient, fault-tolerant network-on-chip de-\nsign using reinforcement learning,” in Design, Automation and Test\nin Europe (DATE) , Mar. 2019.\n[70] K. Wang, A. Louri, A. Karanth, and R. Bunescu, “Intellinoc: A\nholistic design framework for energy-efﬁcient and reliable on-\nchip communication for manycores,” in International Symposium\non Computer Architecture (ISCA) , June 2019.\n[71] J.-Y. Won, X. Chen, P . Gratz, J. Hu, and V . Soteriou, “Up by their\nbootstraps: Online learning in artiﬁcial neural networks for cmp\nuncore power management,” in International Symposium on High-\nPerformance Computer Architecture (HPCA) , Feb. 2014.\n[72] G.-Y. Pan, J.-Y. Jou, and B.-C. Lai, “Scalable power management\nusing multilevel reinforcement learning for multiprocessors,” in\nACM Transactions on Design Automation of Electronic Systems , Aug.\n2014.\n[73] P . E. Bailey, D. K. Lowenthal, V . Ravi, B. Rountree, M. Schulz, and\nB. R. de Supinski, “Adaptive conﬁguration selection for power-\nconstrained heterogeneous systems,” in International Conference\non Parallel Processing (ICPP) , Sept. 2014.\n[74] D. Lo, T. Song, and G. E. Suh, “Prediction-guided performance-\nenergy trade-off for interactive applications,” in International\nSymposium on Microarchitecture (MICRO) , Dec. 2015.\n[75] N. Mishra, J. D. Lafferty, and H. Hoffman, “Caloree: Learning\ncontrol for predictable latency and low energy,” in International\nConference on Architectural Support for Programming Languages and\nOperating Systems (ASPLOS) , Mar. 2018.\n[76] Y. Bai, V . W. Lee, and E. Ipek, “Voltage regulator efﬁciency aware\npower management,” in International Conference on Architectural\nSupport for Programming Languages and Operating Systems (ASP-\nLOS) , Apr. 2017.\n\n14\n[77] M. Allen and P . Fritzsche, “Reinforcement learning with adaptive\nkanerva coding for xpilot game ai,” in IEEE Congress of Evolution-\nary Computation , June 2011.\n[78] Z. Chen and D. Marculescu, “Distributed reinforcement learning\nfor power limited many-core system performance optimization,”\ninDesign, Automation and Test in Europe (DATE) , Mar. 2015.\n[79] Z. Chen, D. Stamoulis, and D. Marculescu, “Proﬁt: Priority and\npower/performance optimization for many-core systems,” IEEE\nTransactions on Computer-Aided Design of Integrated Circuits and\nSystems , vol. 37, pp. 2064–2075, Oct. 2018.\n[80] C. Imes, S. Hofmeyr, and H. Hoffman, “Energy-efﬁcient applica-\ntion resource scheduling using machine learning classiﬁers,” in\nInternational Conference on Parallel Processing (ICPP) , Aug. 2018.\n[81] S. J. Tarsa, R. B. R. Chowdhury, J. Sebot, G. Chinya, J. Gaur,\nK. Sankaranarayanan, C.-K. Lin, R. Chappell, R. Singhal, and\nH. Wang, “Post-silicon cpu adaptation made practical using\nmachine learning,” in International Symposium on Computer Ar-\nchitecture (ISCA) , June 2019.\n[82] S. J. Lu, R. Tessier, and W. Burleson, “Reinforcement learning for\nthermal-aware many-core task allocation,” in Proceedings of the\n25th edition on Great Lakes Symposium on VLSI , May 2015.\n[83] D. Nemirovsky, T. Arkose, N. Markovic, M. Nemirovsky, O. Un-\nsal, and A. Cristal, “A machine learning approach for perfor-\nmance prediction and scheduling on heterogeneous cpus,” in\nInternational Symposium on Computer Architecture and High Per-\nformance Computing (SBAC-P AD) , Oct. 2017.\n[84] H. Zhang, B. Tang, X. Geng, and H. Ma, “Learning driven\nparallelization for large-scale video workload in hybrid cpu-gpu\ncluster,” in International Conference on Parallel Processing (ICPP) ,\nAug. 2018.\n[85] R. Bitirgen, E. Ipek, and J. F. Martinez, “Coordinated manage-\nment of multiple interacting resources in chip multiprocessors:\nA machine learning approach,” in International Symposium on\nMicroarchitecture (MICRO) , Nov. 2008.\n[86] R. Jain, P . R. Panda, and S. Subramoney, “Machine learned\nmachines: Adaptive co-optimization of caches, cores, and on-chip\nnetwork,” in Design, Automation and Test in Europe (DATE) , Mar.\n2016.\n[87] Y. Ding, N. Mishra, and H. Hoffmann, “Generative and multi-\nphase learning for computer systems optimization,” in Interna-\ntional Symposium on Computer Architecture (ISCA) , June 2019.\n[88] G. Wu, Y. Xu, D. Wu, M. Ragupathy, Y. yen Mo, and C. Chu,\n“Flip-ﬂop clustering by weighted k-means algorithm,” in Design\nAutomation Conference (DAC) , June 2016.\n[89] M. Ozsoy, K. N. Khasawneh, C. Donovick, I. Gorelik, N. Abu-\nGhazaleh, and D. Ponomarev, “Hardware-based malware detec-\ntion using low-level architectural features,” IEEE Transactions on\nComputers , vol. 65, Mar. 2016.\n[90] H. Esmaeilzadeh, A. Sampson, L. Ceze, and D. Burger, “Neural\nacceleration for general-purpose approximate programs,” in In-\nternational Symposium on Microarchitecture (MICRO) , Dec. 2012.\n[91] A. Yazdanbakhsh, J. Park, H. Sharma, P . Lotﬁ-Kamran, and\nH. Esmaeilzadeh, “Neural acceleration for gpu throughput pro-\ncessors,” in International Symposium on Microarchitecture (MICRO) ,\nDec. 2015.\n[92] B. Grigorian, N. Farahpour, and G. Reinman, “Brainiac: Bringing\nreliable accuracy into neurally-implemented approximate com-\nputing,” in International Symposium on High-Performance Computer\nArchitecture (HPCA) , Feb. 2015.\n[93] D. Mahajan, A. Yazdanbaksh, J. Park, B. Thwaites, and H. Es-\nmaeilzadeh, “Towards statistical guarantees in controlling qual-\nity tradeoffs for approximate acceleration,” in International Sym-\nposium on Computer Architecture (ISCA) , June 2016.\n[94] G. F. Oliveira, L. R. Goncalves, M. Brandalero, A. C. S. Beck,\nand L. Carro, “Employing classiﬁcation-based algorithms for\ngeneral-purpose approximate computing,” in Design Automation\nConference (DAC) , June 2018.\n[95] F. N. Taher, J. Callenes-Sloan, and B. C. Schafer, “A machine\nlearning based hard fault recuperation model for approximate\nhardware accelerators,” in Design Automation Conference (DAC) ,\nJune 2018.\n[96] X. Chen, Z. Xu, H. Kim, P . Gratz, J. Hu, M. Kishinevsky, and\nU. Ogras, “In-network monitoring and control policy for dvfs\nof cmp networks-on-chip and last level caches,” in International\nSymposium on Networks-on-Chip (NOCS) , May 2012.\n[97] R. Sutton, “Generalization in reinforcement learning: Successful\nexamples using sparse coarse coding,” in Conference on Neural\nInformation Processing Systems (NeurIPS) , June 1996.\n[98] J. A. Boyan and A. W. Moore, “Learning evaluation functions\nto improve optimization by local search,” The Journal of Machine\nLearning Research , Sep. 2001.\n[99] P . Bratley and B. L. Fox, “Algorithm 659: Implementing sobol’s\nquasirandom sequence generator,” ACM Transactions on Mathe-\nmatical Software , vol. 14, Mar. 1988.[100] T. Sherwood, E. Perelman, and B. Calder, “Basic block distribu-\ntion analysis to ﬁnd periodic behavior and simulation points in\napplications,” in International Conference on Parallel Architectures\nand Compilation Techniques (P ACT) , Sept. 2001.\n[101] W. Wang, J. W. Davidson, and M. L. Soffa, “Predicting the mem-\nory bandwidth and optimal core allocations for multi-threaded\napplications on large-scale numa machines,” in International Sym-\nposium on High-Performance Computer Architecture (HPCA) , Mar.\n2016.\n[102] R. M. Kretchmar, “Reinforcement learning algorithms for ho-\nmogenous multi-agent systems,” in Workshop on Agent and Swarm\nProgramming , 2003.\n[103] T. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum,\n“Hierarchical deep reinforcement learning: Integrating temporal\nabstraction and intrinsic motivation,” in Conference on Neural\nInformation Processing Systems (NeurIPS) , Dec. 2016.\n[104] H. Mao, Z. Gong, Z. Zhang, Z. Xiao, and Y. Ni, “Learning multi-\nagent communication under limited-bandwidth restriction for\ninternet packet routing,” Feb. 2019. arXiv:1903.05561.\n[105] V . Sze, Y.-H. Chen, T.-J. Yang, and J. Emer, “Efﬁcient processing\nof deep neural networks: A tutorial and survey,” Aug. 2017.\narXiv:1703.09039.\n[106] S. Han, J. Pool, J. Tran, and W. J. Dally, “Learning both\nweights and connections for efﬁcient neural networks,” Oct. 2015.\narXiv:1506.02626.\n[107] D. C. Mocanu, E. Mocanu, P . Stone, P . H. Nguyen, M. Gibescu,\nand A. Liotta, “Scalable training of artiﬁcial neural networks\nwith adaptive sparse connectivity inspired by network science,”\nNature Communications , vol. 9, June 2018.\n[108] M. Courbariaux, I. Hubara, D. Soudry, R. El-Yaniv, and Y. Bengio,\n“Binarized neural networks: Training deep neural networks with\nweights and activations constrained to +1 or -1,” Mar. 2016.\narXiv:1602.02830.\n[109] M. S. Razlighi, M. Imani, F. Koushanfar, and T. Rosing, “Looknn:\nNeural network with no multiplication,” in Design, Automation\nand Test in Europe (DATE) , Mar. 2017.\n[110] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion,\nO. Grisel, M. Blondel, P . Prettenhofer, R. Weiss, V . Dubourg,\nJ. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot,\nand E. Duchesnay, “Scikit-learn: Machine learning in Python,”\nJournal of Machine Learning Research , vol. 12, pp. 2825–2830, 2011.\n[111] R. Boyapati, J. Huang, P . Majumder, K. H. Yum, and E. J. Kim,\n“Approx-noc: A data approximation framework for network-\non-chip architectures,” in International Symposium on Computer\nArchitecture (ISCA) , June 2017.\n[112] A. Raha and V . Raghunathan, “Towards full-system energy-\naccuracy tradeoffs: A case study of an approximate smart camera\nsystem,” in Design Automation Conference (DAC) , June 2017.\n[113] A. Sampson, A. Baixo, B. Ransford, T. Moreau, J. Yip, L. Ceze, and\nM. Oskin, “Accept: A programmer-guided compiler framework\nfor practical approximate computing,” University of Washington\nTechnical Report , vol. 1, Jan. 2015.\n[114] B. Reagen, J. M. Hern ´andez-Lobato, R. Adolf, M. Gelbart, P . Waht-\nmoug, G.-Y. Wei, and D. Brooks, “A case for efﬁcient accelerator\ndesign space exploration via bayesian optimization,” in Interna-\ntional Symposium on Low Power Electronics and Design (ISLPED) ,\nJuly 2017.\n[115] A. Vallero, A. Savino, G. Politano, S. D. Carlo, A. Chatzidimitriou,\nS. Tselonis, M. Kaliorakis, D. Gizopoulos, M. Riera, R. Canal,\nA. Gonzalez, M. Kooli, A. Bosio, and G. D. Natale, “Cross-layer\nsystem reliability assessment framework for hardware faults,” in\nInternational Test Conference (ITC) , Nov. 2016.",
  "textLength": 103211
}