{
  "paperId": "199c56d4f0a7fbb7e7dfea315f925b31e473c3fb",
  "title": "Learning-Augmented Algorithms for Online Steiner Tree",
  "pdfPath": "199c56d4f0a7fbb7e7dfea315f925b31e473c3fb.pdf",
  "text": "Learning-Augmented Algorithms for Online Steiner Tree\nChenyang Xu1,*and Benjamin Moseley2\n1College of Computer Science, Zhejiang University\n2Tepper School of Business, Carnegie Mellon University\nxcy1995@zju.edu.cn, moseleyb@andrew.cmu.edu\nAbstract\nThis paper considers the recently popular beyond-worst-case\nalgorithm analysis model which integrates machine-learned\npredictions with online algorithm design. We consider the on-\nline Steiner tree problem in this model for both directed and\nundirected graphs. Steiner tree is known to have strong lower\nbounds in the online setting and any algorithm’s worst-case\nguarantee is far from desirable.\nThis paper considers algorithms that predict which terminal\narrives online. The predictions may be incorrect and the al-\ngorithms’ performance is parameterized by the number of in-\ncorrectly predicted terminals. These guarantees ensure that\nalgorithms break through the online lower bounds with good\npredictions and the competitive ratio gracefully degrades as\nthe prediction error grows. We then observe that the theory is\npredictive of what will occur empirically. We show on graphs\nwhere terminals are drawn from a distribution, the new on-\nline algorithms have strong performance even with modestly\ncorrect predictions.\nIntroduction\nAn emerging line of work on beyond-worst-case algorithms\nmakes use of machine learning for algorithmic design. This\nline of work suggests that there is an opportunity to advance\nthe area of beyond-worst-case algorithmics and analysis by\naugmenting combinatorial algorithms with machine learned\npredictions. Such algorithms perform better than worst-case\nbounds with accurate predictions while retaining the worst-\ncase guarantees even with erroneous predictions. There has\nbeen signiﬁcant interest in this area (e.g. (Gupta and Rough-\ngarden 2017; Balcan et al. 2018; Balcan, Dick, and White\n2018; Chawla et al. 2020; Kraska et al. 2018; Lykouris and\nVassilvtiskii 2018; Purohit, Svitkina, and Kumar 2018; Lat-\ntanzi et al. 2020)).\nOnline Learning-Augmented Algorithms. This paper\nconsiders the augmenting model in the online setting where\nalgorithms make decisions over time without knowledge of\nthe future. In this model, an algorithm is given access to a\nlearned prediction about the problem instance. The learned\nprediction is error prone and the performance of the algo-\nrithm is expected to be bounded in terms of the prediction’s\n*The corresponding author.\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.quality. The quality measure is prediction speciﬁc. The per-\nformance measure is the competitive ratio where an algo-\nrithm isc-competitive if the algorithm’s objective value is at\nmost acfactor larger than the optimal objective value for\nevery input. In the learning-augmented algorithms model,\nﬁnding appropriate parameters to predict and making the al-\ngorithm robust to the prediction error are usually key algo-\nrithmic challenges.\nMany online problems have been considered in this con-\ntext, such as caching (Lykouris and Vassilvtiskii 2018; Ro-\nhatgi 2020; Jiang, Panigrahi, and Sun 2020; Wei 2020), page\nmigration (Indyk et al. 2020), metrical task systems (Anto-\nniadis et al. 2020a), ski rental (Purohit, Svitkina, and Kumar\n2018; Gollapudi and Panigrahi 2019; Anand, Ge, and Pani-\ngrahi 2020), scheduling (Purohit, Svitkina, and Kumar 2018;\nIm et al. 2021), load balancing (Lattanzi et al. 2020), online\nlinear optimization (Bhaskara et al. 2020), online ﬂow al-\nlocation (Lavastida et al. 2021), speed scaling (Bamas et al.\n2020), set cover (Bamas, Maggiori, and Svensson 2020), and\nbipartite matching and secretary problems (Antoniadis et al.\n2020b).\nThe Steiner Tree Problem. Steiner tree is one of the\nmost fundamental combinatorial optimization problems. For\nundirected Steiner tree, there is an undirected graph G=\n(V;E)where each edge e∈Ehas a costceand a terminal\nsetT⊆V. We need to buy edges in Esuch that all termi-\nnals are connected via the bought edges and the goal is to\nminimize the total cost of the bought edges. For the directed\ncase, the edges are directed and there is a root node r. In this\nproblem all of the terminals must have a directed path to the\nroot via the edges bought.\nTheoretically, the problem has been of interest to the com-\nmunity for decades, starting with the inclusion in Karp’s 21\nNP-Complete problems (Karp 1972). Since then, it has been\nstudied extensively in approximation algorithm design (Kou,\nMarkowsky, and Berman 1981; Takakashi 1980; Wu, Wid-\nmayer, and Wong 1986; Byrka et al. 2010), stochastic algo-\nrithms (Gupta and P ´al 2005; Gupta, Hajiaghayi, and Kumar\n2007; Kurz, Mutzel, and Zey 2012; Leitner et al. 2018) and\nonline algorithms (Imase and Waxman 1991; Berman and\nCoulston 1997; Angelopoulos 2008, 2009). Practically, the\nSteiner tree problem is fundamental for many network prob-\nlems such as ﬁber optic networks (Bachhiesl et al. 2002),\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n8744\n\nsocial networks (Chiang et al. 2013; Lappas et al. 2010),\nand biological networks (Sadeghi and Fr ¨ohlich 2013). This\nproblem has so many uses practically, that recently there\nhave been competitions to ﬁnd fast algorithms for it and\nits variants, including the 11th DIMACS Implementation\nChallenge (2014) and the 3rd Parameterized Algorithms and\nComputational Experiments (PACE) Challenge (2018).\nThis paper focuses on the online version of Steiner tree.\nIn this case, the graph Gis known in advance, meaning that\nthe edges that can be bought are completely known as well\nas all the nodes in the graph. However, the nodes that ac-\ntually are the terminals Tare unknown. The terminals in T\narrive one at a time. Let t1;t2;:::t kbe the arriving order of\nterminals, where k=|T|. When terminal tiarrives, it must\nimmediately be connected to t1;t2;:::t i−1by buying edges\nofGand once an edge is bought, it is irrevocable. The goal\nis to minimize the total cost.\nThe online problem occurs often in practice. For instance,\nwhen building a network often new nodes are added to the\nnetwork over time. Not knowing which terminals will ar-\nrive makes the problem inherently hard. The algorithm with\nthe best worst-case guarantees is the simple greedy algo-\nrithm (Imase and Waxman 1991), which always chooses to\nconnect an arriving node via the cheapest feasible path. The\ncompetitive ratios of the greedy algorithm on undirected\ngraphs and directed graphs are, respectively, \u0002(logk)and\n\u0002(k), which are the best possible using worst-case analysis\n(see (Imase and Waxman 1991; Westbrook and Yan 1995)).\nHowever, these results are far from desirable. The question\nthus looms, is there the potential to go beyond worst-case\nlower bounds in the learning-augmented algorithms for on-\nline Steiner tree?\nResults\nWe consider the online Steiner tree problem in the learning-\naugmented model. The prediction is deﬁned to be the set of\nterminals. That is, the algorithm is supplied with a set of\nterminals ^Tat the beginning of time. Some of these may\nbe incorrect. Deﬁne the prediction error \u0011to be the number\nof incorrectly predicted terminals. Then the actual terminals\ninTarrive online. This paper shows the following results,\nbreaking through worst-case lower bounds.\n• In the undirected case, we propose an O(log\u0011)-\ncompetitive algorithm. That is, with accurate predic-\ntions, the algorithm is constant competitive. Then with\nthe worst predictions, the competitive ratio is O(logk),\nmatching the best worst-case bound. Between, the algo-\nrithm has slow degradation of performance in terms of\nthe prediction error. We further show that any algorithm\nhas competitive ratio \n(log\u0011)with this prediction and\nthus our algorithm is the best possible online algorithm\nusing this prediction.\n• In the directed case, we give an algorithm that is O(\u0011+\nlogk)-competitive. With near perfect predictions, the al-\ngorithm isO(logk)-competitive, which is exponentially\nbetter than the worst-case lower bound \n(k). With a large\nprediction error, the algorithm matches the O(k)bound\nof the best worst-case algorithm. Between, the algorithmhas slow degradation of performance in terms of the error\nas in the undirected case. As in the undirected case, we\nshow that any algorithm has competitive ratio \n(\u0011)with\nthis prediction. Our algorithm is close to the best possible\nwhen using this prediction.\nThe next question is if these theoretical results predict\nwhat will occur empirically on real graphs. For the undi-\nrected case we show that with modestly accurate predictions,\nthe algorithms indeed can outperform the baseline. Then the\nperformance degrades as there is more error in the predic-\ntion, never becoming much worse than the baseline. These\nempirical results corroborate the theory. Moreover, we give\na learning algorithm that is able to learn predictions from a\nsmall number of sample instances such that our Steiner tree\nalgorithms have strong performance.\nOnline Undirected Steiner Tree\nFor the brevity of the algorithms’ statement and analysis, we\nmake two assumptions. First, we assume that Gis a com-\nplete graph in metric space. This can be assumed by taking\nthe metric completion of any input graph and is standard for\nthe Steiner tree problem. Second, the predicted terminal set\n^Tand the real terminal set Tshare the same size k. The dis-\ncussion about this assumption is provided in the full version\nof this paper. We aim to show the following theorem in this\nsection.\nTheorem 1. Given a predicted terminal set ^T, there exists\nan algorithm with competitive ratio at most O(log\u0011), where\n\u0011:=k−|T∩^T|.\nPreliminaries\nThe input is an undirected graph G= (V;E), where each\nedgeehas costce≥0, and a terminal set T⊆Vthat arrives\nonline. Recall k:=|T|=|^T|. When a terminal tarrives,\nwe must buy some edges such that it is connected with all\nprevious terminals in the subgraph formed by bought edges.\nThe goal is to minimize the total cost of the bought edges.\nIn the analysis, we will leverage results on the online\ngreedy algorithm. The following theorem was shown in\n(Imase and Waxman 1991). The traditional online greedy\nalgorithm maintains a tree Tconnecting all the terminals.\nThis tree is initialized to ∅. Then when a terminal tarrives,\nthe edges on the shortest path from tto any node in Twill\nbe added into T.\nTheorem 2 ((Imase and Waxman 1991)). The online greedy\nalgorithm is O(logk)-competitive.\nWe will also use the following properties of minimum\nspanning trees.\nLemma 3. Consider an ofﬂine Steiner tree instance.\nA minimum spanning tree MST(T)on terminals is a\n2-approximated solution (Kou, Markowsky, and Berman\n1981). In addition, for any edge e, if{e}∪MST(T)contains\na cycle,ceis the maximum edge cost in the cycle (Schrijver\n2003).\n8745\n\nWarm-up: Analysis of a Simple Online Algorithm\nTowards proving Theorem 1, we ﬁrst introduce a simple and\nnatural algorithm whose competitive ratio is O(\u0011). This is\na far worse guarantee than the algorithm we develop, but it\nwill help build our techniques and give the intuition.\nIntuitively, if the prediction is error-free, the instance be-\ncomes an ofﬂine problem. Several constant approximation\nalgorithms can be employed for the ofﬂine case. For exam-\nple, we compute a minimum spanning tree MST( ^T)on the\naccurate predicted terminal set ^Tand each time when a new\nterminal arrives, connect it with all previous terminals only\nusing the edges in MST( ^T). This algorithm obtains a com-\npetitive ratio 2 if ^T=T.\nInspired by this, a natural online algorithm is the follow-\ning. This algorithm has poor performance when the error in\nthe predictions is large. This will then lead us to develop a\nmore robust algorithm.\nOnline Algorithm with Predicted Terminals (OAPT):\nLet^Tbe the predicted set of terminals and MST( ^T)be the\nminimum spanning tree on ^T. LetTibe the ﬁrst set of iter-\nminals that arrive online. Tkcontains all online terminals.\nInitializeA=∅to be the tree that the algorithm will con-\nstruct connecting the online terminals. The algorithm returns\nthe set of edges in Aafter all terminals arrive. We divide the\nedges ofA=A1∪A2into two sets, A1andA2depending\non the case that causes us to add edges to A. Consider when\nterminaltiarrives.\n•Case 1: Ifti=∈^Tortiis the ﬁrst terminal in ^Tto arrive,\nadd toA1the shortest edge in Gconnectingtito termi-\nnalsTi−1that have arrived. No edge is bought if this is\nthe ﬁrst terminal that arrives.\n•Case 2: Otherwise, add the shortest path in MST( ^T)to\nA2which connects tito a terminal in ^T∩Ti−1. In other\nwords, buy the shortest path in MST( ^T)connectingtito\napredicted terminal that has previously arrived.\nOur goal is to show that the competitive ratio of this algo-\nrithm is exactly \u0002(\u0011).\nTheorem 4. The competitive ratio of OAPT is \u0002(\u0011).\nFirst we observe that the algorithm is no better than \n(\u0011)-\ncompetitive. This lower bound will motivate the design of a\nmore robust algorithm in the next section.\nLemma 5. The competitive ratio of OAPT is \n(\u0011).\nTo prove Lemma 5, we ﬁrst construct an instance and then\nshow that algorithm OAPT is \u0011-competitive on it. The in-\nstance is shown in Fig. 1. Due to space, the detailed proof is\nomitted in this version.\nNext we prove the upper bound of the algorithm’s perfor-\nmance. The solution Ais partitioned into two sets A1and\nA2. We bound the cost of these sets separately. The follow-\ning lemma bounds the cost of A1. Essentially, these edges\ndo not cost most than O(log\u0011)because there are at most\n\u0011+1terminals that contribute to edges in A1and their cost is\nbounded by running the traditional online greedy algorithm\non these terminals, which is logarithmically competitive.\nFigure 1: An online Steiner tree instance. The graph has 2k−\n2nodes and 2k−2edges. The cost of edge (v1;vk)is1 +\n1=(k−2)2while each edge between v1andvi(2≤i≤\nk−1) has a cost 1=(k−2)2. The cost of each dash edge\nis 1. The terminal set Tconsists ofv1;v2;:::;v kwhile the\nprediction is ^T={v1;vk;vk+1;:::;v 2k−2}.\nLemma 6. Letc(A1)be the total cost of edges in A1. We\nhavec(A1)≤O(log\u0011)OPT , where OPT is the optimal ob-\njective value.\nProof. LetT/primebe the terminals which get connected to the\ntree via Case (1). By deﬁnition of \u0011, we have|T/prime|≤\u0011+ 1.\nThis is because all terminals that are in T\\^Tare in Case\n(1) and these contribute to \u0011. The only other terminal that\nexecutes Case (1) is the ﬁrst terminal in ^Tthat arrives.\nConsider a new problem instance I/primewhere the terminal\nset isT/prime. In this new instance, the graph along with edge\ncosts remains the same. Let OPT (I/prime)be the optimal cost of\nthis new instance and OPT the optimal cost of the original\nproblem. Notice that OPT (I/prime)≤OPT asT/prime⊆Tand OPT\nrepresents a feasible solution for the new instance.\nWe know that the cost of the traditional online greedy\nalgorithm is at most O(log(|T/prime|))OPT(I/prime)because it\nislog(|T/prime|)competitive on the instance OPT (I/prime)by\nTheorem 2. This holds no matter the arriving order\nof the terminals. Let c(Greedy) be the cost of the\ngreedy algorithm if the terminals in T/primearrive consis-\ntent with the arriving order for the original problem.\nWe have that c(Greedy)≤O(log(|T/prime|))OPT(I/prime)≤\nO(log(\u0011))OPT(I/prime)≤O(log(\u0011))OPT.\nNotice that the shortest edge from titoTi−1is deﬁnitely\nat most the shortest edge from titoTi−1∩T/prime. Thus,c(A1)≤\nc(Greedy)≤O(log(\u0011))OPT.\nNow we focus on the second set A2. These terminals po-\ntentially cause the bulk of the cost for the algorithm. We will\nboundc(A2)byO(\u0011)OPT, which proves Theorem 4. We\nﬁrst show that for each edge e∈A2has cost at most OPT.\nNotice that this is not enough to prove c(A2)≤O(\u0011)OPT\nbecause the number of edges in A2could be as large as\nk−1. For the remainder of this section, let Pidenote the\npath added into A2in iterationi.\n8746\n\nLemma 7. Each edge in A2has cost at most OPT .\nProof. Consider when terminal tiarrives, the algorithm ex-\necutes Case (2) and the path Pi/negationslash=∅. Notice that if Case (2)\nis executed then there is a terminal in tj∈^T∩Ti−1that has\narrived before ti. Moreover, for any terminal tj∈^T∩Ti−1,\nthe cost of edge (ti;tj)is at most OPT because these two\nnodes are connected in the optimal solution and c(ti;tj)is\nthe minimum cost to connect them. To show the lemma, we\nshow that for any edge e∈Pi,ce≤c(ti;tj). This then\nbounds the cost of any edge in A2by OPT.\nFix the terminal tj∈^T∩Ti−1thatticonnects to using\npathPi. If the edge (ti;tj)is inMST( ^T)then this will be\nthe unique edge in Pi. If(ti;tj)is not in MST( ^T)then by\nLemma 3 every edge on the cycle Pi∪{(ti;tj)}has cost at\nmostc(ti;tj)≤OPT.\nWe are ready to bound the cost of the edges in A2.\nLemma 8. The edges of A2can be partitioned into two sets\nB1andB2, wherec(B1)≤OPT and|B2|≤\u0011:Moreover,\nthe total cost of edges in A2is at mostO(\u0011)OPT .\nProof. We begin by partitioning the edges of A2into two\nsetsB1andB2. LetE/primecontain the edges in MST( ^T)∩\nMST( ^T∩T). InitializeS= MST( ^T). The setSwill always\nbe a spanning tree of ^T. We do the following iteratively. For\neach edgee∈MST( ^T∩T)\\MST( ^T), we add it to Sand\nremove an arbitrary edge e/prime∈MST( ^T)\\MST( ^T∩T)from\nSthat forms a cycle. The removed edge e/primeis added toE/prime.\nSetB1=E/prime∩A2andB2=A2\\E/prime.\nIntuitively, the above procedure obtains a spanning tree S\nof^Tby replacing some edges in MST( ^T)that are not in\nMST( ^T)∩MST( ^T∩T)with the edges in MST( ^T∩T).\nWe have that c(E/prime)≤c(MST( ^T∩T)). This is because\nc(e)≥c(e/prime)in each step of the algorithm by deﬁnition of\nMST( ^T)and Lemma 3. Knowing c(MST( ^T∩T))≤OPT,\nwe see thatc(B1)≤c(E/prime)≤OPT.\nAccording to the algorithm, the number of edges in E/primeis\nexactly the same as the number of edges in MST( ^T∩T). In\nother words,|E/prime|=k−\u0011−1and|MST( ^T)\\E/prime|=\u0011. Since\nA2is a subset of MST( ^T),|B2|≤| MST( ^T)\\E/prime|=\u0011.\nNamely, the number of edges in the second partition is at\nmost\u0011. Using Lemma 7, we have c(B2)≤\u0011OPT, complet-\ning the proof of this lemma.\nProof of Theorem 4. The theorem can be proved directly\nby Lemma 6 and Lemma 8: c(A)≤c(A1) +c(A2)≤\nO(log\u0011)OPT +O(\u0011)OPT =O(\u0011)OPT:The lower bound\nin the theorem is given in Lemma 5. Altogether, we have the\nmain theorem.\nAn Improved Online Algorithm Leveraging\nPredictions\nIn this section, we will build on the simple algorithm to give\na more robust online algorithm that has a competitive ratio of\nO(log\u0011). Notice that in the prior proof, the large cost arisesdue to the edges that are added in Case (2), especially the\nedges inB2=A2\\E/primein proof of the ﬁnal lemma. The new\nalgorithm is designed to mitigate this cost.\nImproved Online Algorithm with Predicted Terminals\n(IOAPT): Let^Tbe the predicted set of terminals and\nMST( ^T)be the minimum spanning tree on ^T. LetTibe\nthe ﬁrst set of iterminals that arrive online. Tkcontains all\nonline terminals.\nInitializeA=∅to be the subgraph that the algorithm will\nconstruct connecting the online terminals. The algorithm re-\nturns the set of edges in Aafter all terminals arrive. We di-\nvide the edges of A=A1∪A2into two sets, A1andA2\ndepending on the case that causes us to add edges to A. Con-\nsider when terminal tiarrives.\n•Case 1: Ifti=∈^Tortiis the ﬁrst terminal in ^Tto arrive,\nadd toA1the shortest edge in Gconnectingtito termi-\nnalsTi−1that have arrived. No edge is bought if this is\nthe ﬁrst terminal that arrives.\n•Case 2: Otherwise, ﬁnd the shortest path Piconnecting\ntito^T∩Ti−1inMST( ^T). Useeito denote the shortest\nedge connecting tito^T∩Ti−1inG. We add to A2a\nsub-pathP/prime\ni⊆Pisuch that its endpoints contain tiwhile\nits total cost is in [cei;2cei]. Next, addeitoA2iftiis not\nconnected to the tree after adding P/prime\ni.\nNotice that in Case 2, we can always ﬁnd such a sub-\npathP/prime\nidue to the property of the minimum spanning tree\nand the assumption that Gis a metric. Thus, the algorithm\nalways computes a feasible solution. We have the follow-\ning two lemmas. The proofs are identical to Lemma 7 and\nLemma 8 respectively.\nLemma 9. The cost of any edge in A2computed by IOAPT\nis at most OPT .\nLemma 10. The edges of A2can be partitioned into two\nsetsB1andB2wherec(B1)≤OPT and|B2|≤\u0011:\nWith these lemmas, we can prove the theorem.\nTheorem 11. The competitive ratio of IOAPT is O(log\u0011).\nProof. The analysis of c(A1)is the same as that in OAPT.\nThe proof of Lemma 6 immediately implies c(A1)≤\nO(log\u0011)OPT. Next we focus on bounding the cost of A2.\nLet\u0001\nic(A2)be the increase in c(A2)in when terminal ti\narrives. According to deﬁnition of the algorithm, we know\n\u0001\nic(A2)≤2c(P/prime\ni)and\u0001\nic(A2)≤3cei. Lemma 10 states\nthe edges in MST( ^T)can be partitioned into two sets E0\nandE1:= MST( ^T)\\E0, where the cost of E0is at most\n2OPT and the number of edges in E1is\u0011.\nLetTGbe the ‘good’ terminals that execute Case (2)\nandP/prime\ni⊆E0. LetTBbe the remaining ‘bad’ terminals.\nWe see the following for the good terminals, c(A2) =P\ni∈TG\u0001\nic(A2)≤P\ni∈TG2c(P/prime\ni)≤2c(E0)≤O(OPT):In\nother words, if the sub-path added in each iteration always\nbelongs toE0, the total cost of c(A2)is bounded by a con-\nstant factor of OPT. Say an iteration is good if the sub-path\n8747\n\nadded in it belongs to E0. The total increment of all good\niterations is at most O(OPT).\nWe use the second upper bound to analyze the cost of\nthe bad terminals. This follows similarly to the proof of\nLemma 6. Indeed, we know the following,P\ni∈TB\u0001\nic(A2)≤\nP\ni∈TB3cei:If iterationiis bad, there exists at least one edge\nin sub-pathP/prime\nibelonging to E1. Since|E1|=\u0011, the num-\nber of bad iterations is at most \u0011. The total cost of these\niterationsP\ni∈TB3ceiis at most 3multiplied by the cost of\nrunning the greedy algorithm on the terminals in TB. Let\nOPT(TB)be the optimal solution on TB. We know that\nOPT≥OPT(TB). Moreover, we know that the greedy\nalgorithm has cost at most O(log(|TB|))OPT(TB)≤\nO(log(|TB|))OPT≤O(log\u0011)OPT. Thus we have the fol-\nlowing,P\ni∈TB\u0001\nic(A2)≤O(log\u0011)OPT. This completes the\nproof of Theorem 11.\nThe competitive ratio O(log(\u0011))approaches the worst-\ncase bound log(k)when\u0011=k. Here we give a stronger\nstatement to show our algorithm optimally uses the predic-\ntions. The proof is provided in the full version of this paper.\nTheorem 12. For online undirected Steiner tree with pre-\ndicted terminals, given any \u0011≥1, no online algorithm has\na competitive ratio better than \n(log(\u0011)).\nImproving the Performance of the Algorithm in Practice.\nWe describe a practical modiﬁcation of the algorithm. This\nmodiﬁcation ensures that the algorithm maintains its theo-\nretical bound, while improving the performance. The obser-\nvation is that the algorithm may purchase edges not needed\nfor feasibility. Some edges added by our algorithm are pur-\nchased based on predicted terminals and they will become\nuseless if these predicted terminals do not arrive. We can\nchoose not to buy these edges immediately. When tiarrives,\nthe edges in P/prime\niare not bought immediately if we buy ei. In-\nstead, the algorithm buys the edges the ﬁrst time a terminal\nuses them to connect to previous terminals.\nOnline Steiner Tree in Directed Graphs\nThis section considers online Steiner tree when the graph is\ndirected. The input is a directed graph G= (V;E), where\neach edgeehas costce≥0, a root vertex r∈Vand a\nterminal set T⊆Vthat arrives online. This paper assumes\nwithout loss of generality, that ce>1for any edge e. Addi-\ntionally the input graph is assumed to ensure that there exists\na directed path from root rto every vertex in V.\nThe terminals in Tarrive online. When a terminal v∈T\narrives the algorithm must buy some edges to ensure there is\na directed path from the root rtovin the subgraph induced\nby the bought edges. The goal is to minimize the total cost\nof the bought edges.\nIn directed graphs, the worst-case bound on the competi-\ntive ratio is \n(k)(Westbrook and Yan 1995). Our main result\nshows that we can break through this bound.Theorem 13. Given a predicted terminal set ^T, there exists\nan algorithm with competitive ratio at most O(logk+\u0011),\nwhere\u0011:=k−|T∩^T|.\nThe algorithm claimed in Theorem 13 is O(logk)-\nconsistent and O(k)-robust, meaning that the ratio is\nO(logk)if\u0011= 0 and is at most O(k)for any\u0011. The algo-\nrithm for directed graphs builds on the algorithm for undi-\nrected graphs. As before, there are two sets of edges A1;A2.\nThe setA1contains edges that are bought because a termi-\nnal arrives that was not predicted. As in the undirected case\nsuch these edges are bought using a greedy algorithm. The\nedges inA2are bought using a different algorithm over the\nundirected case.\nOnline Algorithm with Predicted Terminals in Directed\nGraphs: Initialize\u0015= 1to be a parameter, which is intu-\nitively a guess of the maximum connection cost of any ter-\nminal inT. Let ^T(\u0015) :={t∈^T|c(t;r)≤\u0015}be the set\nof predicted terminals that have a path to the root of cost at\nmost\u0015. Let MDST( ^T(\u0015))be the minimum directed Steiner\ntree of ^T(\u0015), which can be computed by an ofﬂine optimal\nalgorithm1.\nInitializeA1=∅andA2=∅. The edges that are bought\nwill beA=A1∪A2. Order the terminals such that tiarrives\nbeforeti+1and letTi={t1;t2;:::;t i}be the ﬁrstitermi-\nnals to arrive. Let \fi= max tj∈Tic(tj;r)be the maximum\ncost of connecting a terminal in Tidirectly to the root.\nConsider when a terminal ti∈Tarrives. If\fi> \u0015\nthen both increase \u0015by a factor 2 and update ^T(\u0015)and\nMDST( ^T(\u0015)). Next perform one of the following.\n• Ifti=∈^T(\u0015)then add the shortest path from titortoA1,\nbuying these edges.\n• Otherwise, add the unique path from titorin\nMDST( ^T(\u0015))toA2.\nOur goal is to show the following theorem.\nTheorem 14. The competitive ratio of the Algorithm for di-\nrected Steiner tree is O(logk+\u0011).\nBefore proving the theorem, we show a technical lemma.\nLemma 15. For any\u0015,c(MDST( ^T(\u0015)))≤OPT +\u0015\u0011.\nProof. The proof idea is to construct a feasible Steiner tree\nof^T(\u0015)whose value is at most OPT +\u0015\u0011. Then the in-\nequality will hold due to the optimality of MDST( ^T(\u0015)).\nThe feasible tree is constructed as follows: connect all ter-\nminals in ^T(\u0015)∩Tto the root in the same way as the opti-\nmal solution and add the shortest path from ttorfor each\nterminalt∈^T(\u0015)\\T. The total cost of the former part\nis at most OPT while the latter term incurs a cost of \u0015\u0011\nsincec(t;r)≤\u0015for any terminal t∈^T(\u0015). Thus, the total\ncost of this subgraph is at most OPT +\u0015\u0011, implying that\nc(MDST( ^T(\u0015)))≤OPT +\u0015\u0011.\n1Noting that this problem is NP-hard and it is known to be in-\napproximable within a O(logk)ratio unless P=NP (Dinur and\nSteurer 2014), we do not have efﬁcient optimal algorithms or ap-\nproximation algorithms in practice. Thus, the directed case is more\nfor theoretical interests.\n8748\n\nWe can now prove the main theorem. Due to space, the\ndetailed proof is omitted in this paper.\nExperimental Results\nThis section investigates the empirical performance of the\nproposed algorithm OAPT and IOAPT for the undirected\nSteiner tree problem. The goal is to answer the following\ntwo questions:\n• Robustness - How much prediction accuracy does the al-\ngorithms need to outperform the baseline algorithm em-\npirically?\n• Learnability - How many samples are required to em-\npirically learn predictions sufﬁcient for the algorithms to\nperform better than the baseline?\nThe baseline we compare against is the online greedy al-\ngorithm which is the best traditional online algorithm. We\ninvestigate the performance of both OAPT and IOAPT.\nSetup\nThe experiments2are conducted on a machine running\nUbuntu 18.04 with an i7-7800X CPU and 48 GB memory.\nExperiments are averaged over 10 runs. We consider two\ntypes of graphs.\nRandom Graphs. The number of nodes in a graph is set to\nbe 2,000 and 50,000 edges are selected uniformly at random.\nThe cost of each edge is an integer sampled uniformly from\n[1;1000] . To ensure the connectivity of graphs, we add all\nremaining edges to form a complete graph, given the edges\nhigh cost of 100,000.\nRoad Graphs. The road network of Bay Area is pro-\nvided by The 9th DIMACS Implementation Challenge3in\ngraph format where a node denotes a point in Bay Area\nand the cost of an edge is the road length between the two\nendpoints. The original data contains 321,270 nodes and\n400,086 edges. In the experiments, we employ the same\nsampling method as in (Moseley, Vassilvitskii, and Wang\n2021) to sample connected subgraphs from this large graph.\nBrieﬂy, we draw rectangles with a certain size on the road\nnetwork randomly and construct a subgraph from each rect-\nangle. The experiments employ 4 sampled subgraphs with\n23512±1135 nodes and 31835±1815 edges. These graphs\ngive the same trends, thus, we show one such graph in this\nsection and others appear in the full version of the paper.\nThe terminal set Tand the prediction ^Tare constructed\ndifferently depending on the experiments.\nRobustness to Accuracy\nThis experiment tests the performance of OAPT and IOAPT\nwhen the prediction accuracy varies. Recall that kis the\nnumber of terminals. We set k= 200 and2;000respectively\nfor random graphs and road graphs unless stated otherwise4.\n2The code is available at https://github.com/Chenyang-1995/\nOnline-Steiner-Tree\n3http://users.diag.uniroma1.it/challenge9/download.shtml\n4We also conduct experiments with different numbers of termi-\nnals. See this paper’s full version for more results.The setTofkterminals are sampled uniformly from the\nvertex setV. They arrive in random order.\nWe now construct the predictions. Let \u0015∈[0;1]be a pa-\nrameter corresponding to the prediction accuracy. First, we\nsample a node set ^T0withk\u0015nodes uniformly from the ter-\nminal setT. And then another node set ^T1withk(1−\u0015)\nnodes is sampled uniformly from the non-terminal nodes\nV\\T. Let ^T0∪^T1be the predicted terminal set ^T. No-\ntice that\u0015indicates the prediction accuracy. Thus, testing\nthe performance of algorithms with different \u0015’s answers the\nrobustness question. This experiment is in Fig. 2(a) and 3(a).\nLearning the Terminals\nHere we construct instances where the algorithm explicitly\nlearns the terminals. Each such instance will have a dis-\ntribution over terminal sets of size kand employ random\norder. We will sample straining instances of kterminals\nT1;T2;:::T s. The learning algorithm used to predict termi-\nnals is deﬁned as follows.\nThe Learning Algorithm. A nodevis predicted to be in\n^Twith probability f(v)=siff(v)> \u0012s , wheref(v)is the\nnumber of sampled sets in which node vappears and \u0012is a\nparameter in [0;1]. Note that the number of predicted termi-\nnals may not equal k.\nThere is a question on how to choose \u0012. This is done as\nfollows. We choose an instance Tifrom the training set at\nrandom and check which \u0012would give OAPT (IOAPT) the\nbest performance on this instance. We then use this \u0012for\nOAPT (IOAPT) on the online instance. For efﬁciency, we\nonly consider \u0012∈{0; 0:2;0:4;0:6;0:8;1}.\nDistribution for Random Graphs. Two distributions are\nconsidered for random graphs. The ﬁrst is a bad distribution\nwhere there is nothing to learn, the uniform distribution. In\nthis case, all terminals are drawn uniformly from V. The\nsecond is called a two-class distribution where there is a set\nof nodes to learn. Let Vhbe a small collection of nodes that\nwill be terminals with higher probability. Vhis set to 400\nnodes uniformly at random. Let k= 200 be the number of\nterminals. Half are drawn from Vhand half from V\\Vh.\nHere we hope the learning algorithm quickly learns Vh, and\nfurther, our algorithms can take advantage of the predictions.\nThe results appear in Fig. 2(b) and 2(c).\nDistribution for Road Graphs. This experiment is de-\nsigned to model the case where terminals can appear in geo-\ngraphical similar locations. The graph will be clustered and a\nspeciﬁed number of terminals will arrive per cluster follow-\ning a distribution over nodes in the cluster. Use rto denote\nthe radius of the graph. Given a parameter \u001b, partition all\nnodes into several clusters such that the radius of each clus-\nter is at most \u001br. The greedy clustering algorithm (Gonzalez\n1985) is used. We let \u001b= 0: 1in the experiments unless\nstate otherwise. The terminal set ^Tis obtained by picking\n⌊2000=x⌋random clusters and sampling xterminals uni-\nformly from each selected cluster. We let xbe 10 and 100.\nWhenx= 10 the distribution is harder to learn than when\nx= 100. See Fig. 3(b) and 3(c) for the results. Experiments\nvarying parameters appear in the full version of this paper.\n8749\n\n(a)\n (b)\n (c)\nFigure 2: The experimental results on random graphs. The ratio is the algorithm’s performance relative to the baseline. Fig. 2(a)\nshows the performance of algorithms over different \u0015’s, corresponding to the robustness experiment. Fig. 2(b) and Fig. 2(c) are,\nrespectively, the algorithms’ performance over the number of training instances on the uniform distribution and the two-class\ndistribution. Their corresponding prediction errors are present in this paper’s full version. Note that some of the x-axes are on\nlog-scale.\n(a)\n (b)\n (c)\nFigure 3: Results on road graphs. Fig. 3(a) shows the performance of algorithms over different \u0015’s. Fig. 3(b) and Fig. 3(c) are,\nrespectively, the algorithms’ performance and prediction error over different numbers of training instances when x= 10 and\n100. The corresponding prediction errors are present in this paper’s full version.\nEmpirical Discussion\nWe see the following trends.\n• Both Fig. 2(a) 3(a) show that the algorithms perform well\non different graphs even with modestly correct predic-\ntions. Once about 20% of the predictions are correct, the\nalgorithms perform better than the baseline.\n• Fig. 2(b) 3(b) show the algorithms are robust for difﬁ-\ncult distributions, which are sparse distributions where\nthere is effectively nothing to learn. The learning algo-\nrithm will quickly realize that predictions cause negative\neffects and then output very few predicted terminals (see\nthe prediction error ﬁgures in this paper’s full version for\nmore corroborating experiments). After tens of training\ninstances, the ratios become never worse than 1.01.\n• Fig. 2(c) 3(c) show the learning algorithm quickly learns\ngood distributions. Further, both online algorithms have\nstrong performance using the predictions. We conclude\nthat with a small number of training samples, the learning\nalgorithm is able to learn useful predictions sufﬁcient for\nthe online algorithms to outperform the baseline.\nThese experiments corroborate the theory. The algorithmsobtain much better performance than the baseline even with\nmodestly good predictions. If given very inaccurate predic-\ntions, the algorithms are barely worse than the baseline.\nMoreover, we see that only a small number of sample in-\nstances are needed for the algorithms to have competitive\nperformance when terminals arrive from a good distribution.\nConclusion\nOnline Steiner tree is one of the most fundamental online\nnetwork design problems. It is a special case or a sub-\nproblem of many online network design problems. Steiner\ntree captures the challenge of building networks online and,\nmoreover, Steiner tree algorithms are often used as build-\ning blocks or subroutines for more general problems. As\nthe community expands the learning augmented algorithms\narea into more general online network design problems, this\npaper provides models, and algorithmic and analysis tech-\nniques that can be leveraged for these problems.\n8750\n\nAcknowledgements\nChenyang Xu was supported in part by Science and Tech-\nnology Innovation 2030 –”The Next Generation of Artiﬁcial\nIntelligence” Major Project No.2018AAA0100902. Ben-\njamin Moseley was supported in part by NSF grants CCF-\n1824303, CCF-1845146, CCF-2121744, CCF-1733873 and\nCMMI-1938909. Benjamin Moseley was additionally sup-\nported in part by a Google Research Award, an Infor Re-\nsearch Award, and a Carnegie Bosch Junior Faculty Chair.\nWe thank Yuyan Wang for sharing their experimental data\nand thank the anonymous reviewers for their insightful com-\nments and suggestions.\nReferences\nAnand, K.; Ge, R.; and Panigrahi, D. 2020. Customizing\nML Predictions For Online Algorithms. ICML 2020.\nAngelopoulos, S. 2008. A Near-Tight Bound for the Online\nSteiner Tree Problem in Graphs of Bounded Asymmetry. In\nAlgorithms - ESA 2008, 16th Annual European Symposium,\nKarlsruhe, Germany, September 15-17, 2008. Proceedings ,\n76–87.\nAngelopoulos, S. 2009. Parameterized Analysis of Online\nSteiner Tree Problems. In Adaptive, Output Sensitive, On-\nline and Parameterized Algorithms, 19.04. - 24.04.2009.\nAntoniadis, A.; Coester, C.; Eli ´as, M.; Polak, A.; and Simon,\nB. 2020a. Online metric algorithms with untrusted predic-\ntions. CoRR, abs/2003.02144.\nAntoniadis, A.; Gouleakis, T.; Kleer, P.; and Kolev, P. 2020b.\nSecretary and Online Matching Problems with Machine\nLearned Advice. In Advances in Neural Information Pro-\ncessing Systems 33: Annual Conference on Neural Informa-\ntion Processing Systems 2020, NeurIPS 2020, December 6-\n12, 2020, virtual.\nBachhiesl, P.; Paulus, G.; Prossegger, M.; Werner, J.; and\nSt¨ogner, H. 2002. Cost Optimized Layout of Fibre Optic\nNetworks in the Access Net Domain. In Operations Re-\nsearch Proceedings 2002, Selected Papers of the Interna-\ntional Conference on Operations Research (SOR 2002), Kla-\ngenfurt, Austria, September 2-5, 2002, 247–252.\nBalcan, M.; Dick, T.; Sandholm, T.; and Vitercik, E. 2018.\nLearning to Branch. In Dy, J. G.; and Krause, A., eds., Pro-\nceedings of the 35th International Conference on Machine\nLearning, ICML 2018, Stockholmsm ¨assan, Stockholm, Swe-\nden, July 10-15, 2018, volume 80 of Proceedings of Machine\nLearning Research, 353–362. PMLR.\nBalcan, M.; Dick, T.; and White, C. 2018. Data-Driven Clus-\ntering via Parameterized Lloyd’s Families. In Bengio, S.;\nWallach, H. M.; Larochelle, H.; Grauman, K.; Cesa-Bianchi,\nN.; and Garnett, R., eds., Advances in Neural Information\nProcessing Systems 31: Annual Conference on Neural In-\nformation Processing Systems 2018, NeurIPS 2018, 3-8 De-\ncember 2018, Montr ´eal, Canada, 10664–10674.\nBamas, ´E.; Maggiori, A.; Rohwedder, L.; and Svensson, O.\n2020. Learning Augmented Energy Minimization via Speed\nScaling. In Larochelle, H.; Ranzato, M.; Hadsell, R.; Bal-\ncan, M.; and Lin, H., eds., Advances in Neural InformationProcessing Systems 33: Annual Conference on Neural Infor-\nmation Processing Systems 2020, NeurIPS 2020, December\n6-12, 2020, virtual.\nBamas, ´E.; Maggiori, A.; and Svensson, O. 2020. The\nPrimal-Dual method for Learning Augmented Algorithms.\nInAdvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Processing Sys-\ntems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\nBerman, P.; and Coulston, C. 1997. On-line algorithms for\nSteiner tree problems. Conference Proceedings of the An-\nnual ACM Symposium on Theory of Computing, 344–353.\nProceedings of the 1997 29th Annual ACM Symposium\non Theory of Computing ; Conference date: 04-05-1997\nThrough 06-05-1997.\nBhaskara, A.; Cutkosky, A.; Kumar, R.; and Purohit, M.\n2020. Online Learning with Imperfect Hints. CoRR,\nabs/2002.04726.\nByrka, J.; Grandoni, F.; Rothvoß, T.; and Sanit `a, L. 2010. An\nimproved LP-based approximation for steiner tree. In Pro-\nceedings of the 42nd ACM Symposium on Theory of Com-\nputing, STOC 2010, Cambridge, Massachusetts, USA, 5-8\nJune 2010, 583–592.\nChawla, S.; Gergatsouli, E.; Teng, Y .; Tzamos, C.; and\nZhang, R. 2020. Pandora’s Box with Correlations: Learn-\ning and Approximation. arXiv:1911.01632.\nChiang, M.; Lam, H.; Liu, Z.; and Poor, H. V . 2013. Why\nSteiner-tree type algorithms work for community detection.\nInProceedings of the Sixteenth International Conference on\nArtiﬁcial Intelligence and Statistics, AISTATS 2013, Scotts-\ndale, AZ, USA, April 29 - May 1, 2013, 187–195.\nDinur, I.; and Steurer, D. 2014. Analytical approach to paral-\nlel repetition. In Shmoys, D. B., ed., Symposium on Theory\nof Computing, STOC 2014, New York, NY, USA, May 31 -\nJune 03, 2014, 624–633. ACM.\nGollapudi, S.; and Panigrahi, D. 2019. Online Algorithms\nfor Rent-Or-Buy with Expert Advice. In Chaudhuri, K.;\nand Salakhutdinov, R., eds., Proceedings of the 36th In-\nternational Conference on Machine Learning, ICML 2019,\n9-15 June 2019, Long Beach, California, USA, volume 97\nofProceedings of Machine Learning Research, 2319–2327.\nPMLR.\nGonzalez, T. F. 1985. Clustering to Minimize the Maximum\nIntercluster Distance. Theor. Comput. Sci., 38: 293–306.\nGupta, A.; Hajiaghayi, M.; and Kumar, A. 2007. Stochas-\ntic Steiner Tree with Non-uniform Inﬂation. In Approxima-\ntion, Randomization, and Combinatorial Optimization. Al-\ngorithms and Techniques, 10th International Workshop, AP-\nPROX 2007, and 11th International Workshop, RANDOM\n2007, Princeton, NJ, USA, August 20-22, 2007, Proceed-\nings, 134–148.\nGupta, A.; and P ´al, M. 2005. Stochastic Steiner Trees With-\nout a Root. In Automata, Languages and Programming,\n32nd International Colloquium, ICALP 2005, Lisbon, Por-\ntugal, July 11-15, 2005, Proceedings, 1051–1063.\nGupta, R.; and Roughgarden, T. 2017. A PAC Approach\nto Application-Speciﬁc Algorithm Selection. SIAM J. Com-\nput., 46(3): 992–1017.\n8751\n\nIm, S.; Kumar, R.; Qaem, M. M.; and Purohit, M. 2021.\nNon-Clairvoyant Scheduling with Predictions. In Agrawal,\nK.; and Azar, Y ., eds., SPAA ’21: 33rd ACM Symposium on\nParallelism in Algorithms and Architectures, Virtual Event,\nUSA, 6-8 July, 2021, 285–294. ACM.\nImase, M.; and Waxman, B. M. 1991. Dynamic Steiner Tree\nProblem. SIAM J. Discret. Math., 4(3): 369–384.\nIndyk, P.; Mallmann-Trenn, F.; Mitrovic, S.; and Rubinfeld,\nR. 2020. Online Page Migration with ML Advice. CoRR,\nabs/2006.05028.\nJiang, Z.; Panigrahi, D.; and Sun, K. 2020. Online Al-\ngorithms for Weighted Paging with Predictions. In Czu-\nmaj, A.; Dawar, A.; and Merelli, E., eds., 47th Interna-\ntional Colloquium on Automata, Languages, and Program-\nming, ICALP 2020, July 8-11, 2020, Saarbr ¨ucken, Germany\n(Virtual Conference), volume 168 of LIPIcs, 69:1–69:18.\nSchloss Dagstuhl - Leibniz-Zentrum f ¨ur Informatik.\nKarp, R. M. 1972. Reducibility Among Combinatorial Prob-\nlems. In Proceedings of a symposium on the Complexity\nof Computer Computations, held March 20-22, 1972, at the\nIBM Thomas J. Watson Research Center, Yorktown Heights,\nNew York, USA, 85–103.\nKou, L. T.; Markowsky, G.; and Berman, L. 1981. A Fast\nAlgorithm for Steiner Trees. Acta Informatica, 15: 141–145.\nKraska, T.; Beutel, A.; Chi, E. H.; Dean, J.; and Polyzotis,\nN. 2018. The Case for Learned Index Structures. In Pro-\nceedings of the 2018 International Conference on Manage-\nment of Data, SIGMOD ’18, 489–504. New York, NY , USA:\nACM. ISBN 978-1-4503-4703-7.\nKurz, D.; Mutzel, P.; and Zey, B. 2012. Parameterized Al-\ngorithms for Stochastic Steiner Tree Problems. In Mathe-\nmatical and Engineering Methods in Computer Science, 8th\nInternational Doctoral Workshop, MEMICS 2012, Znojmo,\nCzech Republic, October 25-28, 2012, Revised Selected Pa-\npers, 143–154.\nLappas, T.; Terzi, E.; Gunopulos, D.; and Mannila, H. 2010.\nFinding effectors in social networks. In Proceedings of the\n16th ACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining, Washington, DC, USA,\nJuly 25-28, 2010, 1059–1068.\nLattanzi, S.; Lavastida, T.; Moseley, B.; and Vassilvitskii, S.\n2020. Online Scheduling via Learned Weights. In Chawla,\nS., ed., Proceedings of the 2020 ACM-SIAM Symposium on\nDiscrete Algorithms, SODA 2020, Salt Lake City, UT, USA,\nJanuary 5-8, 2020, 1859–1877. SIAM.\nLavastida, T.; Moseley, B.; Ravi, R.; and Xu, C. 2021.\nLearnable and Instance-Robust Predictions for Online\nMatching, Flows and Load Balancing. In Mutzel, P.; Pagh,\nR.; and Herman, G., eds., 29th Annual European Sympo-\nsium on Algorithms, ESA 2021, September 6-8, 2021, Lis-\nbon, Portugal (Virtual Conference), volume 204 of LIPIcs,\n59:1–59:17. Schloss Dagstuhl - Leibniz-Zentrum f ¨ur Infor-\nmatik.\nLeitner, M.; Ljubic, I.; Luipersbeck, M.; and Sinnl, M. 2018.\nDecomposition methods for the two-stage stochastic Steiner\ntree problem. Comput. Optim. Appl., 69(3): 713–752.Lykouris, T.; and Vassilvtiskii, S. 2018. Competitive\nCaching with Machine Learned Advice. In Dy, J.; and\nKrause, A., eds., Proceedings of the 35th International\nConference on Machine Learning, volume 80 of Proceed-\nings of Machine Learning Research, 3302–3311. Stock-\nholmsm ¨assan, Stockholm Sweden: PMLR.\nMoseley, B.; Vassilvitskii, S.; and Wang, Y . 2021. Hierarchi-\ncal Clustering in General Metric Spaces using Approximate\nNearest Neighbors. In The 24th International Conference\non Artiﬁcial Intelligence and Statistics, AISTATS 2021, April\n13-15, 2021, Virtual Event, 2440–2448.\nPurohit, M.; Svitkina, Z.; and Kumar, R. 2018. Improving\nOnline Algorithms via ML Predictions. In Advances in Neu-\nral Information Processing Systems 31: Annual Conference\non Neural Information Processing Systems 2018, NeurIPS\n2018, 3-8 December 2018, Montr ´eal, Canada., 9684–9693.\nRohatgi, D. 2020. Near-Optimal Bounds for Online Caching\nwith Machine Learned Advice. In Chawla, S., ed., Proceed-\nings of the 2020 ACM-SIAM Symposium on Discrete Algo-\nrithms, SODA 2020, Salt Lake City, UT, USA, January 5-8,\n2020, 1834–1845. SIAM.\nSadeghi, A.; and Fr ¨ohlich, H. 2013. Steiner tree methods\nfor optimal sub-network identiﬁcation: an empirical study.\nBMC Bioinform., 14: 144.\nSchrijver, A. 2003. Combinatorial optimization: polyhedra\nand efﬁciency, volume 24. Springer Science & Business Me-\ndia.\nTakakashi, H. 1980. An Approximate Solution for Steiner\nProblem in Graphs. Math. Japonica, 24(6): 573–577.\nWei, A. 2020. Better and Simpler Learning-Augmented On-\nline Caching. CoRR, abs/2005.13716.\nWestbrook, J. R.; and Yan, D. C. K. 1995. Linear Bounds for\nOn-Line Steiner Problems. Inf. Process. Lett., 55(2): 59–63.\nWu, Y .-F.; Widmayer, P.; and Wong, C.-K. 1986. A faster\napproximation algorithm for the Steiner problem in graphs.\nActa informatica, 23(2): 223–229.\n8752",
  "textLength": 45706
}