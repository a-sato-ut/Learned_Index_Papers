{
  "paperId": "d2ce1bb76757b3fea2902a8f85b5b98419df233d",
  "title": "Compressing (Multidimensional) Learned Bloom Filters",
  "pdfPath": "d2ce1bb76757b3fea2902a8f85b5b98419df233d.pdf",
  "text": "Compressing (Multidimensional) Learned Bloom\nFilters\nAngjela Davitkova\nTU Kaiserslautern (TUK)\nKaiserslautern, Germany\ndavitkova@cs.uni-kl.deDamjan Gjurovski\nTU Kaiserslautern (TUK)\nKaiserslautern, Germany\ngjurovski@cs.uni-kl.deSebastian Michel\nTU Kaiserslautern (TUK)\nKaiserslautern, Germany\nmichel@cs.uni-kl.de\nAbstract\nBloom ﬁlters are widely used data structures that compactly represent sets of\nelements. Querying a Bloom ﬁlter reveals if an element is not included in the\nunderlying set or is included with a certain error rate. This membership testing can\nbe modeled as a binary classiﬁcation problem and solved through deep learning\nmodels, leading to what is called learned Bloom ﬁlters. We have identiﬁed that\nthe beneﬁts of learned Bloom ﬁlters are apparent only when considering a vast\namount of data, and even then, there is a possibility to further reduce their memory\nconsumption. For that reason, we introduce a lossless input compression technique\nthat improves the memory consumption of the learned model while preserving\na comparable model accuracy. We evaluate our approach and show signiﬁcant\nmemory consumption improvements over learned Bloom ﬁlters.\n1 Introduction\nFor decades, relational database systems are the de facto standard for storing enterprise data, while\nonly recently there has been a true interest in introducing the beneﬁts of machine learning techniques\nfor relational databases. Inspired by the premise that each database component can be replaced or\nenhanced by a counterpart from the machine learning world, Kraska et al. [ 8] opened the ﬁeld of\ncombining machine learning with relational data structures. For instance, traditional one-dimensional\n(B-tree or hash index) or multidimensional indexes, can be replaced by regression or classiﬁcation\ndeep learning models, drastically reducing the space and the time needed for query answering.\nExistence indexes are data structures capable of answering the membership of an element in a given\nindexed set. The most prominent existence index is the Bloom ﬁlter [ 2], due to its space efﬁciency and\nquerying performance. Previous work suggests the replacement of Bloom ﬁlters with deep learning\nmodels for classiﬁcation [ 8]. A further idea [ 9] suggests a classiﬁcation model even in the presence\nof multidimensional data. Contrary to Bloom ﬁlters that need to index every value combination in\nthe multidimensional data for answering membership queries on subsets, learned multidimensional\nBloom ﬁlters can easily infer such pattern connections. However, unlike other traditional indexes\nwhich in most scenarios are slower and much larger than their learned parallel, it is difﬁcult to replace\nBloom ﬁlters, as they are already extremely compact and fast. To exhibit a real impact by their\nreplacement, the data has to be large, typically ranging in billions of records. Intuitively in the cases\nof multidimensional data, the number of combinations needed to be indexed by the Bloom ﬁlter often\ncreates such data. Still, the number of distinct values per dimension also increases the parameters of\nthe neural network and the embedding matrices, affecting the space beneﬁts of the learned model.\nIn this work, we suggest the usage of a compressed learned Bloom ﬁlter which exhibits the beneﬁts of\nthe learned multidimensional existence index while providing drastic space reduction, faster training\ntime, and comparable accuracy. We employ lossless compression of the categorical data which offers\nbeneﬁts over Bloom ﬁlters even when the underlying data is small.\nWorkshop on Databases and AI (DBAI) at NeurIPS 2021, Sydney, Australia.arXiv:2208.03029v1  [cs.DB]  5 Aug 2022\n\n2 Related work and background\n2.1 Related work\nKraska et al. [ 8] ﬁrst suggested the idea of replacing indexes with deep learning models. Since\nthis premise, a plethora of papers focus on improving traditional one-dimensional [ 4,6,7] and also\nmultidimensional indexes [ 3,5] with learned models. Since the idea of replacing a traditional Bloom\nﬁlter [ 2] with a classiﬁcation model [ 8], several improvements have been proposed. For instance,\nVaidya et al. [ 11] use multiple more accurate learned ﬁlters based on classiﬁcation score segments.\nMitzenmacher [ 10] proposes a new sandwiched learned Bloom ﬁlter, including two surrounding\nBloom ﬁlters for improved performance. Differently, Macke et al. [ 9] focus on extending the learned\nBloom ﬁlter and show the beneﬁts of having such an index for multidimensional data. Compressing\nthe input of deep learning models has also been investigated for learned cardinality estimators.\nNeuroCard [ 12] uses a variable byte compression of columns to improve the proposed cardinality\nestimator. In our experiments, we compare only with the multidimensional learned ﬁlter since ideas\nlike partitioning or sandwiching are orthogonal and can be used in combination with our approach.\n2.2 Background\nBloom ﬁlters are space-efﬁcient data structures, used to test the existence of an element in a given\ndataset. Their probabilistic nature enforces the guarantees of no false negatives and tunable false\npositive rates. More speciﬁcally, a Bloom ﬁlter is an mbit array (initially all bits are set to 0),\nrequiringhwell-deﬁned hash functions. Upon adding an element, each of the hhash functions maps\nthe element to a position in the mbit array and sets the bit to 1. Evidently, the adaptation of a Bloom\nﬁlter for multidimensional data would require indexing of all possible combinations of column values,\nto be able to accurately decide on the presence of subsets of values.\nLearned Bloom ﬁlters exploit the idea that classiﬁcation tasks resemble the behavior of Bloom\nﬁlters. The classiﬁcation model learns to identify the presence of the elements in the set, by learning\non positive samples drawn from the given dataset and negative samples, representing data not present\nin the given set. The learned model has a smaller memory consumption at the price of increased false\npositive rate and the introduction of false negatives. To solve the problem of false negatives, previous\nwork [ 8,9] suggests the use of a backup ﬁxup ﬁlter that stores the false negatives. The beneﬁts of\nthe learned Bloom ﬁlter are evident for larger dimensions because unlike the traditional Bloom ﬁlter\nwhich needs to contain all combinations of column-value pairs, the learned model can infer these\ninterconnections. The multidimensional learned Bloom ﬁlter [ 9] considersnstring tuples, each ﬁrst\nconverted into an embedding vector. The embedding vectors are concatenated and fed through dense\nlayer(s). Using the sigmoid activation, the output is converted to a logit suitable for presenting the\npresence and absence of terms. We extend on this idea, by improving space consumption.\n3 Compression for learned Bloom ﬁlters\n3.1 Modeling multidimensional data\nAs an example for multidimensional data, consider a dataset containing vehicle information for a car\nrental agency. For simplicity, let us consider that the dataset has only three columns, the name of\nthe car, the fuel type, and information on whether the car has been rented out. Intuitively, indexing\nmultidimensional data would lead to a more comprehensive Bloom ﬁlter since the ﬁlter needs to\nanswer membership queries involving subsets of the original records. As an example, considering the\nvehicles dataset described above, checking if there is an available car that runs on diesel can be done\nby querying for (?,diesel,true) where \" ?\" is a placeholder for any value. To answer this query, the\nBloom ﬁlter needs to index all subsets of column values for a particular entry, including the ones\nwhere the column values are not speciﬁed, i.e., queries of the type (?,fuel_type,rented_out) . Thus,\nthe size of the Bloom ﬁlter will be directly affected by the possible combinations of co-occurring\nattributes that need to be covered by the index. On the other hand, when considering learned Bloom\nﬁlters over multidimensional data, the model parameters are directly affected by the input dimensions,\ni.e., the distinct values of the columns. Hence, datasets in which columns have many distinctive\nvalues can result in higher memory consumption of the learned Bloom ﬁlter, although the space\nconsumption will be still lower compared to traditional Bloom ﬁlter implementations [8, 9].\n2\n\nColumnns = 2\n5144\n60000Scolumn 1\n20\n244\n... ...Scolumn 2\n244\n220... ...\nmax_vidsvd =nsmax_vid svdmod svd\n = 245Figure 1: Compressing a column into two subcolumns\n3.2 Lossless input compression\nThe number of inputs and their distinct values have a direct impact on the model parameters and,\nthus, the size of the model. One way to provide the inputs to the model is to use an embedding\nlayer on every column. To accomplish this, proper mapping of string data to an integer value is\nperformed for every column. However, the size of the embedding matrix will scale linearly with the\nnumber of unique values per column. Thus, even for a column with 105unique values, when using\na32-dimensional embedding, the embedding matrix would take around 12:8MB of space which is\nalready much larger than a normal Bloom ﬁlter. By applying our proposed input compression, we\naim at drastically reducing this space consumption.\nConsider a relation Rwherec1,:::,cnrepresent columns with v(c1),:::,v(cn)unique values. The\nmain idea is to split a column into several subcolumns, together having fewer dimensions than the\noriginal column, contributing to a smaller encoding of the input and, thus, a smaller model size. The\nnumber of subcolumns is chosen based on the number of distinct column values, with the goal of\nsmaller input dimensionality. For example, if the number of values for column ciisv(ci) = 10000 ,\ntwo subcolumns are sufﬁcient to efﬁciently compress the original column values whereas for a column\nwith10million unique values three or more subcolumns would be required to train the model. Our\ncompression is based on the observation that we can reduce the input dimensionality by dividing\nthe column values with a speciﬁc divisor. More speciﬁcally, to apply the encoding, we ﬁrst identify\nthe number of subcolumns nsthat a column should be split into. Then, we set as a divisor svdto\nbe thensthroot of the number of distinct values of the column, i.e., svd=\u0006nspmax _vid\u0007\n. For\ncompressing the column value x, we determine the quotient svqand reminder svrwhen dividing the\nvaluexwithsvd. Ifns> 2, we repeat the same procedure for x=svqandmax _vid=max _svq,\nat the end reaching nssubcolumns.\nAs an example, consider the column represented in Figure 1. As depicted, the number of distinct\nvalues for the column is max _vid= 60000 and we want to compress the value x= 5144 . If we\nwant to split the column into two subcolumns, i.e., ns= 2, then as the divisor we get svd= 245 by\ncalculating the squared root of max _vid. Thus, the value x= 5144 will be compressed in svq= 20\nandsvr= 244 . In this case, all column values are split into two subvalues having maximal values\nsvdandsvd\u00001. Through the compression, we reduce the number of dimensions needed to encode\nthe input from 60000 to489. When considering a 32-dimensional embedding, we reduce the size of\nthe embedding matrix from 7:8MB to approximately 0:06MB, which is a substantial space reduction.\nWe perform the compression over every column for which v(ci)is greater than a compression\nthreshold\u0012. Unlike previous learned multidimensional Bloom ﬁlters, which encode the structure of\nthe column through embeddings, we also allow a one-hot encoding in cases where the column has\nbeen already compressed to a smaller dimension and embedding matrices are no longer necessary.\nEvidently, the proposed input compression would affect not only the model size but also the model\naccuracy. More speciﬁcally, parameter ns, which determines the number of subcolumns a column\nneeds to be split into, allows a tradeoff between the model size and model accuracy. By increasing\nns, although the unique column values are decreased, the number of input columns is increased.\nConsequently, the learning of the model will be negatively affected since it would need to learn across\nmultiple columns with increased interconnection. Thus, we carefully set both nsand the compression\nthreshold\u0012for achieving an acceptable tradeoff.\n3\n\nTable 1: Comparison of C-LMBF, LMBF\n(both with 1 layer of 64 neurons) and BF\nAirplane\nMemory\nAccuracy MB NN params Input dim\n\u0012= 3000 0.95 0.53 33,006 5060\n\u0012= 5500 0.97 1.01 73,110 9933\n\u0012= 8000 0.98 2.35 186,713 23025\nLMBF 0.98 4.06 330,608 38728\nBF-0.1 1 6.10 — —\nDMV\nMemory\nAccuracy MB NN params Input dim\n\u0012= 100 0.98 0.36 5,447 892\n\u0012= 1000 0.98 0.47 19,564 3636\n\u0012= 2000 0.98 0.78 47,694 8097\nLMBF 0.98 1.97 147,351 17895\nBF-0.1 1 6.10 — —\n0.1250.250.5124\n32 64 128128x128 32 64 128128x128Memory (MB)\nNN SizeC-LMBF\nLMBF\nDMV Airplane Figure 2: Memory when varying NN size\n4 Experiments\nSetup&Datasets: For generating positive training data, we randomly sample from the data records\nand optionally replace some of the values with wildcards. For negative training data, we randomly\nselect non co-occurring combinations of values, optionally including a wildcard. If not mentioned\notherwise, columns are compressed into 2subcolumns. We implemented our proposed model in Keras,\nPython, and performed the experiments on NVidia GeForce RTX 2080 Ti GPU. The experiments were\nperformed using two real-world datasets, where we retrieve 100;000records, following the analysis\nof previous work [ 9]. The ﬁrst dataset (airplane) consists of ﬂight information, with 7columns having\nv(ci) = [6887;8021;8046;6537;2557;5017;1663] . The second dataset (DMV) consists of vehicle\nregistration data [ 1] which has 19columns, having less distinct values than the airplane dataset,\ni.e.,v(ci) = [5;10001;27;1627;27;1570;64;107;694;40;8;1509;346;966;794;102;3;3;2]. The\ndifferent distributions outline the beneﬁts and drawbacks of our compression approach ( C-LMBF ).\nWe compare with the original learned multidimensional ﬁlter ( LMBF ) and traditional Bloom ﬁlter\n(BF). For BF, we only use \u00195million unique subset combinations. Since the accuracy and FNR of\nC-LMBF and LMBF are almost identical, we do not show the ﬁxup ﬁlter memory in the experiments.\nResults: In Table 1, we show several measurements for C-LMBF with different compression\nboundaries\u0012. The different \u0012boundries result into [5;4;2]and[10;4;1]compressed columns, for\neach dataset respectively. We ﬁx the number of layers and dimensions for each of the models\nand train them until convergence. The embedding is set according to the input dimension size.\nThe compression reduces up to \u00197x and\u001920x of the input dimensionality, for the airplane and\nDMV dataset respectively, with a trade-off of a small accuracy reduction. The decrease in the input\ndimensions shows a drastic reduction of the memory of the model as well as the needed parameters\nfor training. Since the columns with the smaller number of values do not require compression, it is\nnot unusual that a smaller \u0012may introduce additional complexity, and thus produce worse accuracy,\nas most evident for the airplane dataset when \u0012= 3000 . In Figure 2, we show the impact of different\nneural network sizes on the memory consumption. We set \u0012= 5500 for the airplane dataset and\n\u0012= 100 for the DMV dataset. As expected, the C-LMBF has a constant reduction in size when\ncompared to LMBF. Furthermore, the increase in the neural network size causes a better or an\nequal accuracy (not shown). The number of subcolumns created ( ns) also impacts the model. The\nconsidered datasets have fewer unique values per column and dividing into subcolumns for ns> 2\nwould only increase the number of inputs and embedding matrices without a beneﬁcial reduction on\nthe input dimensions. Although not shown, larger values of nsare highly useful for many distinct\nvalues, e.g., considering knowledge graph data. For a carefully chosen \u0012andns, the compression\nalso causes faster training time, e.g., we have a 10–15seconds speedup when executing 1epoch.\n5 Conclusion\nWe introduced a memory efﬁcient learned Bloom ﬁlter by compressing the input parameters of\nthe model using a lossless column compression that splits the values of the input columns into a\npredeﬁned number of subcolumns. Through experiments, we showed drastic memory consumption\nimprovements while keeping comparable accuracy.\n4\n\nReferences\n[1]. State of new york. vehicle, snowmobile, and boat registrations. catalog.data.gov/\ndataset/vehicle-snowmobile-and-boat-registrations , . Accessed: 2021-09-15.\n[2]Burton H. Bloom. Space/time trade-offs in hash coding with allowable errors. Commun. ACM ,\n13(7):422–426, 1970.\n[3]Angjela Davitkova, Evica Milchevski, and Sebastian Michel. The ml-index: A multidimen-\nsional, learned index for point, range, and nearest-neighbor queries. In EDBT , pages 407–410.\nOpenProceedings.org, 2020.\n[4]Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li, Hantian Zhang,\nBadrish Chandramouli, Johannes Gehrke, Donald Kossmann, David B. Lomet, and Tim Kraska.\nALEX: an updatable adaptive learned index. In SIGMOD Conference , pages 969–984. ACM,\n2020.\n[5]Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. Tsunami: A learned multi-\ndimensional index for correlated data and skewed workloads. Proc. VLDB Endow. , 14(2):74–86,\n2020.\n[6]Paolo Ferragina and Giorgio Vinciguerra. The pgm-index: a fully-dynamic compressed learned\nindex with provable worst-case bounds. Proc. VLDB Endow. , 13(8):1162–1175, 2020.\n[7]Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper, Tim Kraska,\nand Thomas Neumann. Radixspline: a single-pass learned index. In aiDM@SIGMOD , pages\n5:1–5:5. ACM, 2020.\n[8]Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned\nindex structures. In SIGMOD Conference , pages 489–504. ACM, 2018.\n[9]Stephen Macke, Alex Beutel, Tim Kraska, M. Sathiamoorthy, D. Cheng, and Ed H. Chi. Lifting\nthe curse of multidimensional data with learned existence indexes. In ML for Systems@NeurIPS ,\n2018.\n[10] Michael Mitzenmacher. A model for learned bloom ﬁlters and optimizing by sandwiching. In\nNeurIPS , pages 462–471, 2018.\n[11] Kapil Vaidya, Eric Knorr, Tim Kraska, and Michael Mitzenmacher. Partitioned learned bloom\nﬁlter. CoRR , abs/2006.03176, 2020.\n[12] Zongheng Yang, Amog Kamsetty, Sifei Luan, Eric Liang, Yan Duan, Xi Chen, and Ion Stoica.\nNeurocard: One cardinality estimator for all tables. Proc. VLDB Endow. , 14(1):61–73, 2020.\n5",
  "textLength": 18504
}