{
  "paperId": "bd5c32f003c670366e5d62c59fe02e747a470647",
  "title": "Learning-based Support Estimation in Sublinear Time",
  "pdfPath": "bd5c32f003c670366e5d62c59fe02e747a470647.pdf",
  "text": "Published as a conference paper at ICLR 2021\nLEARNING -BASED SUPPORT ESTIMATION\nINSUBLINEAR TIME\nTalya Eden\u0003, Piotr Indyk, Shyam Narayanan, Ronitt Rubinfeld & Sandeep Silwal\nComputer Science and Artiﬁcial Intelligence Lab\nMassachusetts Institute of Technology\nCambridge, MA 02139, USA\nfteden,indyk,shyamsn,ronitt,silwal g@mit.edu\nTal Wagner\nMicrosoft Research\nRedmond, WA 98052, USA\ntalw@mit.edu\nABSTRACT\nWe consider the problem of estimating the number of distinct elements in a large\ndata set (or, equivalently, the support size of the distribution induced by the data\nset) from a random sample of its elements. The problem occurs in many applica-\ntions, including biology, genomics, computer systems and linguistics. A line of\nresearch spanning the last decade resulted in algorithms that estimate the support\nup to\u0006\"nfrom a sample of size O(log2(1=\")\u0001n=logn), wherenis the data\nset size. Unfortunately, this bound is known to be tight, limiting further improve-\nments to the complexity of this problem. In this paper we consider estimation\nalgorithms augmented with a machine-learning-based predictor that, given any el-\nement, returns an estimation of its frequency. We show that if the predictor is\ncorrect up to a constant approximation factor, then the sample complexity can be\nreduced signiﬁcantly, to\nlog(1=\")\u0001n1\u0000\u0002(1=log(1=\")):\nWe evaluate the proposed algorithms on a collection of data sets, using the neural-\nnetwork based estimators from Hsu et al, ICLR’19 as predictors. Our experiments\ndemonstrate substantial (up to 3x) improvements in the estimation accuracy com-\npared to the state of the art algorithm.\n1 I NTRODUCTION\nEstimating the support size of a distribution from random samples is a fundamental problem with\napplications in many domains. In biology, it is used to estimate the number of distinct species from\nexperiments (Fisher et al., 1943); in genomics to estimate the number of distinct protein encoding re-\ngions (Zou et al., 2016); in computer systems to approximate the number of distinct blocks on a disk\ndrive (Harnik et al., 2016), etc. The problem has also applications in linguistics, query optimization\nin databases, and other ﬁelds.\nBecause of its wide applicability, the problem has received plenty of attention in multiple ﬁelds1,\nincluding statistics and theoretical computer science, starting with the seminal works of Good and\nTuring Good (1953) and Fisher et al. (1943). A more recent line of research pursued over the last\ndecade (Raskhodnikova et al., 2009; Valiant & Valiant, 2011; 2013; Wu & Yang, 2019) focused on\nthe following formulation of the problem: given access to independent samples from a distribution\n\u0003Authors listed in alphabetical order\n1A partial bibliography from 2007 contains over 900 references. It is available at\nhttps://courses.cit.cornell.edu/jab18/bibliography.html .\n1arXiv:2106.08396v1  [cs.LG]  15 Jun 2021\n\nPublished as a conference paper at ICLR 2021\nPover a discrete domain f0;:::;n\u00001gwhose minimum non-zero mass2is at least 1=n, estimate\nthe support size of Pup to\u0006\"n. The state of the art estimator, due to Valiant & Valiant (2011); Wu\n& Yang (2019), solves this problem using only O(n=logn)samples (for a constant \"). Both papers\nalso show that this bound is tight.\nA more straightforward linear-time algorithm exists, which reports the number of distinct elements\nseen in a sample of size N=O(nlog\"\u00001)(which isO(n)for constant \"), without accounting\nfor the unseen items. This algorithm succeeds because each element iwith non-zero mass (and\nthus mass at least 1=n) appears in the sample with probability at least 1\u0000(1\u00001=n)N>1\u0000\";\nso in expectation, at most \"\u0001nelements with non-zero mass will not appear in the sample. Thus,\nin general, the number of samples required by the best possible algorithm (i.e., n=logn) is only\nlogarithmically smaller than the complexity of the straightforward linear-time algorithm.\nA natural approach to improve over this bound is to leverage the fact that in many applications, the\ninput distribution is not entirely unknown. Indeed, one can often obtain rough approximations of\nthe element frequencies by analyzing different but related distributions. For example, in genomics,\nfrequency estimates can be obtained from the frequencies of genome regions of different species; in\nlinguistics they can be inferred from the statistical properties of the language (e.g., long words are\nrare), or from a corpus of writings of a different but related author, etc. More generally, such esti-\nmates can be learned using modern machine learning techniques, given the true element frequencies\nin related data sets. The question then becomes whether one can utilize such predictors for use in\nsupport size estimation procedures in order to improve the estimation accuracy.\nOur results In this paper we initiate the study of such “learning-based” methods for support size\nestimation. Our contributions are both theoretical and empirical. On the theory side, we show\nthat given a “good enough” predictor of the distribution P, one can solve the problem using much\nfewer thann=lognsamples. Speciﬁcally, suppose that in the input distribution Pthe probability of\nelementiispi, and that we have access to a predictor \u0005(i)such that \u0005(i)\u0014pi\u0014b\u0001\u0005(i)for some\nconstant approximation factor b\u00151.3Then we give an algorithm that estimates the support size up\nto\u0006\"nusing only\nlog(1=\")\u0001n1\u0000\u0002(1=log(1=\"))\nsamples, assuming the approximation factor bis a constant (see Theorem 1 for a more detailed\nbound). This improves over the bound of Wu & Yang (2019) for any ﬁxed values of the accuracy\nparameter\"and predictor quality factor b. Furthermore, we show that this bound is almost tight.\nOur algorithm is presented in Algorithm 1. On a high level, it partitions the range of probability\nvalues into geometrically increasing intervals. We then use the predictor to assign the elements\nobserved in the sample to these intervals, and produce a Wu-Yang-like estimate within each interval.\nSpeciﬁcally, our estimator is based on Chebyshev polynomials (as in Valiant & Valiant (2011); Wu\n& Yang (2019)), but the ﬁner partitioning into intervals allows us to use polynomials with different,\ncarefully chosen parameters. This leads to signiﬁcantly improved sample complexity if the predictor\nis sufﬁciently accurate.\nOn the empirical side, we evaluate the proposed algorithms on a collection of real and synthetic data\nsets. For the real data sets (network trafﬁc data and AOL query log data) we use neural-network\nbased predictors from Hsu et al. (2019). Although those predictors do not always approximate the\ntrue distribution probabilities up to a small factor, our experiments nevertheless demonstrate that the\nnew algorithm offers substantial improvements (up to 3x reduction in relative error) in the estimation\naccuracy compared to the state of the art algorithm of Wu & Yang (2019).\n1.1 R ELATED WORK\nEstimating support size As described in the introduction, the problem has been studied exten-\nsively in statistics and theoretical computer science. The best known algorithm, due to Wu & Yang\n2This constraint is naturally satisﬁed e.g., if the distribution Pis an empirical distribution over a data set of\nnitems. In fact, in this case all probabilities are multiples of 1=nso the support size is equal to the number of\ndistinct elements in the data set.\n3Our results hold without change if we modify the assumption to r\u0001\u0005(i)\u0014pi\u0014r\u0001b\u0001\u0005(i), for any r >0.\nWe use r= 1for simplicity.\n2\n\nPublished as a conference paper at ICLR 2021\n(2019), uses O(log2(1=\")\u0001n=logn)samples. Because of the inherent limitations of the model that\nuses only random samples, Canonne & Rubinfeld (2014) considered an augmented model where\nan algorithm has access to the exact probability of any sampled item. The authors show that this\naugmentation is very powerful, reducing the sampling complexity to only O(1=\"2). More recently,\nOnak & Sun (2018) proved that Canonne and Rubinfeld’s algorithm works as long as the probabil-\nities accessed are accurate up to a (1\u0006\"\n3)-multiplicative factor. However, this algorithm strongly\nrelies on the probabilities being extremely accurate, and the predicted probabilities even being off\nby a small constant factor can cause the support size estimate to become massively incorrect. As a\nresult, their algorithm is not robust to mispredicted probabilities, as our experiments show.\nA different line of research studied streaming algorithms for estimating the number of distinct el-\nements. Such algorithms have access to the whole data set, but must read it in a single pass using\nlimited memory. The best known algorithms for this problem compute a (1 +\")-approximate esti-\nmation to the number of distinct elements using O(1=\"2+ logn)bits of storage (Kane et al., 2010).\nSee the discussion in that paper for a history of the problem and further references.\nLearning-based algorithms Over the last few years, there has been a growing interest in using\nmachine learning techniques to improve the performance of “classical” algorithms. This method-\nology found applications in similarity search (Wang et al., 2016; Sablayrolles et al., 2019; Dong\net al., 2020), graph optimization (Khalil et al., 2017; Balcan et al., 2018), data structures (Kraska\net al., 2018; Mitzenmacher, 2018), online algorithms (Lykouris & Vassilvitskii, 2018; Purohit et al.,\n2018), compressed sensing (Mousavi et al., 2015; Baldassarre et al., 2016; Bora et al., 2017) and\nstreaming algorithms (Hsu et al., 2019; Jiang et al., 2019). The last two papers are closest to our\nwork, as they solve various computational problems over data streams, including distinct elements\nestimation in Jiang et al. (2019) using frequency predictors. Furthermore, in our experiments we are\nusing the neural-network-based predictors developed in Hsu et al. (2019). However, our algorithm\noperates in a fundamentally different model, using a sublinear (in n) number of samples of the input,\nas opposed to accessing the full input via a linear scan. Thus, our algorithms run in sublinear time ,\nin contrast to streaming algorithms that use sublinear space .\nDistribution property testing This work can be seen more broadly in the context of testing prop-\nerties of distributions over large discrete domains. Such questions are studied at the crossroads\nof social networks, statistics, information theory, database algorithms, and machine learning al-\ngorithms. Examples of speciﬁc properties that have been extensively considered include testing\nwhether the distribution is uniform, Gaussian, high entropy, independent or monotone increasing\n(see e.g. Rubinfeld (2012); Canonne (2015); Goldreich (2017) for surveys on the topic).\n2 L EARNING -BASED ALGORITHM\n2.1 P RELIMINARIES\nProblem setting and notation. The support estimation problem is formally deﬁned as follows.\nWe are given sample access to an unknown distribution Pover a discrete domain of size n. For\nsimplicity, we identify the domain with [n] =f1;:::;ng. Letpidenote the probability of element\ni. LetS(P) =fi:pi>0gbe the support ofP. Our goal is to estimate the support size S=jS(P)j\nusing as few samples as possible. In particular, given \">0, the goal is to output an estimate ~Sthat\nsatisﬁes ~S2[S\u0000\"n;S +\"n].\nWe assume that the minimal non-zero mass of any element is at least 1=n, namely, that pi\u00151=nfor\neveryi2S(P). This is a standard promise in the support estimation problem (see, e.g., Raskhod-\nnikova et al. (2009); Valiant & Valiant (2011); Wu & Yang (2019)), and as mentioned earlier, it\nnaturally holds in the context of counting distinct elements, where piis deﬁned as the count of ele-\nmentiin the sample divided by n. Furthermore, a lower bound on the minimum non-zero probability\nis a necessary assumption without which no estimation algorithm is possible, even if the number of\nsamples is allowed to be an arbitrarily large function of n, i.e., not just sublinear algorithms. The\nreason is that there could be arbitrarily many elements with exceedingly small probabilities that\nwould never be observed. See for example the discussion in the supplementary Section 5 of Orlitsky\net al. (2016).\n3\n\nPublished as a conference paper at ICLR 2021\nIn the learning-based setting, we furthermore assume we have a predictor \u0005that can provide in-\nformation about pi. In our analysis, we will assume that \u0005(i)is a constant factor approximation\nof eachpi. In order to bound the running time of our algorithms, we assume we are given access\nto a ready-made predictor and (as in Canonne & Rubinfeld (2014)) that evaluating \u0005(i)takes unit\ntime. In our experiments, we use neural network based predictors from Hsu et al. (2019). In general,\npredictors need to be trained (or otherwise produced) in advance. This happens in a preprocessing\nstage, before the input distribution is given to the algorithm, and this stage is not accounted for in\nthe sublinear running time. We also note that training the predictor needs to be done only once for\nall future inputs (not once per input).\nThe Wu & Yang (2019) estimator. In the classical setting (without access to a predictor), Wu &\nYang (2019) gave a sample-optimal algorithm based on Chebyshev polynomials. We now describe\nit brieﬂy, as it forms the basis for our learning-based algorithm.\nSuppose we draw Nsamples, and let Nibe the number of times element iis observed. The output\nestimate of Wu & Yang (2019) is of the form\n~SWY=X\ni2[n](1 +f(Ni));\nwheref(Ni)is a correction term intended to compensate for the fact that some elements in the\nsupport do not appear in the sample at all. If pi= 0, then necessarily Ni= 0 (asicannot appear\nin the sample). Thus, choosing f(0) =\u00001ensures that unsupported elements contribute nothing\nto~SWY. On the other hand, if pi>logn\nN, then by standard concentration we have Ni>\n(logn)\nwith high probability; thus choosing f(Ni) = 0 for allNi>L= \n(logn)ensures that high-mass\nelements are only counted once in ~SWY. It remains to take care of elements iwithpi2[1\nn;logn\nN].\nBy a standard Poissonization trick, the expected additive error jS\u0000E[~SWY]jcan be bounded byP\ni2[n]jPL(pi)j, wherePLis the degree- Lpolynomial\nPL(x) =LX\nk=0E[N]k\nk!\u0001f(k)\u0001xk:\nTo make the error as small as possible, we would like to choose f(1);:::;f (L)so as to minimize\njPL(pi)jon the interval pi2[1\nn;logn\nN], under the constraint PL(0) =\u00001(which is equivalent\ntof(0) =\u00001). This is a well-known extremal problem, and its solution is given by Chebyshev\npolynomials, whose coefﬁcients have a known explicit formula. Indeed, Wu & Yang (2019) show\nthat choosing f(1);:::;f (L)such thatE[N]k\nk!f(k)are the coefﬁcients of an (appropriately shifted\nand scaled) Chebyshev polynomial leads to an optimal sample complexity of O(log2(1=\")\u0001n=logn).\n2.2 O URALGORITHM\nOur main result is a sample-optimal algorithm for estimating the support size of an unknown distri-\nbution, where we are given access to samples as well as approximations to the probabilities of the\nelements we sample. Our algorithm is presented in Algorithm 1. It partitions the interval [1\nn;logn\nN]\ninto geometrically increasing intervals f[bj\nn;bj+1\nn] :j= 0;1;:::g, wherebis a ﬁxed constant that\nwe refer to as the base parameter (in our proofs this parameter upper bounds the approximation\nfactor of the predictor, which is why we use the same letter to denote both; its setting in practice is\nstudied in detail in the next section). The predictor assigns the elements observed in the sample to\nintervals, and the algorithm computes a Chebyshev polynomial estimate within each interval. Since\nthe approximation quality of Chebyshev polynomials is governed by the ratio between the interval\nendpoints (as well as the polynomial degree), this assignment can be leveraged to get more accurate\nestimates, improving the overall sample complexity. Our main theoretical result is the following.\nTheorem 1. Letb>1be a ﬁxed constant. Suppose we have a predictor that given i2[n]sampled\nfrom the input distribution, outputs \u0005(i)such that \u0005(i)\u0014pi\u0014b\u0001\u0005(i). Then, for any \">n\u00001=2,\nif we setL=O(log(1=\"))and drawN\u0018Poisson\u0000\nL\u0001n1\u00001=L\u0001\nsamples, Algorithm 1 reports an\nestimate ~Sthat satisﬁes ~S2[S\u0000\"p\nnS;S +\"p\nnS]with probability at least 9=10. In other words,\n4\n\nPublished as a conference paper at ICLR 2021\nAlgorithm 1: Learning-Based Support Estimation\nInput: Number of samples N, domain size n, baseb, polynomial degree L, predictor \u0005\nOutput: Estimate ~Sof the support size\n1Partition [1\nn;1]into intervals Ij=h\nbj\nn;bj+1\nni\nforj= 0;:::; logbn\n2Leta0;a1;:::;aLbe the coefﬁcient of the Chebyshev polynomial PL(x)from Equation (1)\n(Note thatak= 0for allk>L )\n3DrawNrandom samples\n4Ni= # of times we see element iin samples\n5forevery interval Ijdo\n6 ifbj\nn\u00140:5 logn\nNthen\n7 ~Sj=P\ni2[n]:\u0005(i)2Ij\u0010\n1 +aNi\u0000n\nbj\u0001Ni\u0001Ni!\nNNi\u0011\n8 else\n9 ~Sj= #fi2[n] :Ni\u00151;\u0005(i)2Ijg\n10 end\n11end\n12return ~S=Plogbn\nj=0~Sj\nusing\nO\u0010\nlog (1=\")\u0001n1\u0000\u0002(1=log(1=\"))\u0011\nsamples, we can approximate the support size Sup to an additive error of \"p\nnS, with probability\nat least 9=10.\nNote that sample complexity of Wu & Yang (2019) (which is optimal without access to a predictor)\nis nearly linear in n, while Theorem 1 gives a bound that is polynomially smaller than nfor every\nﬁxed\">0. Also, note thatp\nnS\u0014n, so our estimate ~Sis also within \"\u0001nof the support size S.\nWe also prove a corresponding lower bound, proving that the above theorem is essentially tight.\nTheorem 2. Suppose we have access to a predictor that returns \u0005(i)such that \u0005(i)\u0014pi\u00142\u0001\u0005(i)\nfor all sampled i. Then any algorithm that estimates the support size up to an \"nadditive error with\nprobability at least 9=10needs \n(n1\u0000\u0002(1=log(1=\")))samples.\nWe prove Theorems 1 and 2 in Appendix A. We note that while our upper bound proof follows a\nsimilar approach to Wu & Yang (2019), our lower bound follows a combinatorial approach differing\nfrom their linear programming arguments.\nThe Chebyshev polynomial. For completeness, we explicitly write the polynomial coefﬁcients\nused by Algorithm 1. The standard Chebyshev polynomial of degree Lon the interval [\u00001;1]is\ngiven by4\nQL(x) = cos(L\u0001arccos(x)):\nFor Algorithm 1, we want a polynomial as follows:\nPL(x) =LX\nk=0akxksatisfyingPL(0) =\u00001andPL(x)2[\u0000\";\"]for all 1\u0014x\u0014b2:\nThis is achieved by shifting and scaling QL, namely, this polynomial can be written as\nPL(x) =\u0000QL\u0010\n2x\u0000(b2+1)\n(b2\u00001)\u0011\nQL\u0010\n\u0000b2+1\nb2\u00001\u0011; (1)\n4The fact this is indeed a polynomial is proven in standard textbooks, e.g., Timan et al. (1963).\n5\n\nPublished as a conference paper at ICLR 2021\nwhere\"equals\f\f\fQL\u0010\n\u0000b2+1\nb2\u00001\u0011\f\f\f\u00001\n, which decays as e\u0000\u0002(L)ifbis a constant. Thus it sufﬁces to\nchooseL=O(log(1=\"))in Theorem 1.\n2.3 A SSUMPTIONS ON THE PREDICTOR ’SACCURACY\nIn Theorems 1 and 2, we assume that the predictor \u0005is always correct within a constant factor for\neach element i. As we will discuss in the experimental section, the predictor that we use does not\nalways satisfy this property. Hence, perhaps other models of the accuracy of \u0005are better suited,\nsuch as a promised upper bound on TV(\u0005;P);i.e., the total variation distance between \u0005andP:\nHowever, it turns out that if we are promised that TV(\u0005;P)\u0014\";then the algorithm of Canonne\n& Rubinfeld (2014) can in fact approximate the support size of Pup to errorO(\")\u0001nusing only\nO(\"\u00002)samples. Conversely, if we are only promised that TV(\u0005;P)\u0014\rfor some\r\u001d\";the\nalgorithm of Wu & Yang (2019), which requires \u0002(n=logn)samples to approximate the support\nsize ofP, is optimal. We formally state and prove both of these results in Appendix A.3.\nOur experimental results demonstrate that our algorithm can perform better than both the algorithms\nof Canonne & Rubinfeld (2014) and Wu & Yang (2019). This suggests that our assumption (a point-\nwise approximation guarantee, but with an arbitrary constant multiplicative approximation factor),\neven if not fully accurate, is better suited for our application than a total variation distance bound.\n3 E XPERIMENTS\nIn this section we evaluate our algorithm empirically on real and synthetic data.\nDatasets. We use two real and one synthetic datasets:\n•AOL : 21 million search queries from 650 thousand users over 90 days. The queries are\nkeywords for the AOL search engine. Each day is treated as a separate input distribution.\nThe goal is to estimate the number of distinct keywords.\n•IP: Packets collected at a backbone link of a Tier1 ISP between Chicago and Seattle in 2016\nover 60 minutes.5Each packet is annotated with the sender IP address, and the goal is to\nestimate the number of distinct addresses. Each minute is treated as a separate distribution.\n•Zipﬁan : Synthetic dataset of samples drawn from a ﬁnite Zipﬁan distribution over\nf1;:::; 105gwith the probability of each element iproportional to i\u00000:5.\nThe AOL and IP datasets were used in Hsu et al. (2019), who trained a recurrent neural network\n(RNN) to predict the frequency of a given element for each of those datasets. We use their trained\nRNNs as predictors. For the Zipﬁan dataset we use the empirical counts of an independent sample as\npredictions. A more detailed account of each predictor is given later in this section. The properties\nof the datasets are summarized in Table 1.\nBaselines. We compare Algorithm 1 with two existing baselines:\n• The algorithm of Wu & Yang (2019), which is the state of the art for algorithms without\npredictor access. We abbreviate its name as WY .6\n• The algorithm of Canonne & Rubinfeld (2014), which is the state of the art for algorithms\nwith access to a perfect predictor. We abbreviate its name as CR.\nError measurement. We measure accuracy in terms of the relative error j1\u0000~S=Sj, whereSis\nthe true support size and ~Sis the estimate returned by the algorithm. We report median errors over\n50independent executions of each experiment, \u0006one standard deviation.\n5From CAIDA internet traces 2016, https://www.caida.org/data/monitors/\npassive-equinix-chicago.xml .\n6Apart from their tight analysis, Wu & Yang (2019) also report experimental results showing their algorithm\nis empirically superior to previous baselines.\n6\n\nPublished as a conference paper at ICLR 2021\nTable 1: Datasets used in our experiments. The listed values of n(total size) and support size (dis-\ntinct elements) for AOL/IP are per day/minute (respectively), approximated across all days/minutes.\nName Type #Distributions Predictor n Support size\nAOL Keywords 90 (days) RNN \u00184\u0001105\u00182\u0001105\nIP IP addresses 60 (minutes) RNN \u00183\u0001107\u0018106\nZipﬁan Synthetic 1 Empirical \u00182\u0001105105\nSummary of results. Our experiments show that on one hand, our algorithm can indeed leverage\nthe predictor to get signiﬁcantly improved accuracy compared to WY . On the other hand, our al-\ngorithm is robust to different predictors: while the CR algorithm performs extremely well on one\ndataset (AOL), it performs poorly on the other two (IP and Zipﬁan), whereas our algorithm is able\nto leverage the predictors in those cases too and obtain signiﬁcant improvement over both baselines.\n3.1 B ASE PARAMETER SELECTION\nAlgorithm 1 uses two parameters that need to be set: The polynomial degree L, and the base param-\neterb. The performance of the algorithm is not very sensitive to L, and for simplicity we use the\nsame setting as Wu & Yang (2019), L=b0:45 lognc. The setting of brequires more care.\nRecall thatbis a constant used as the ratio between the maximum and minimum endpoint of each\nintervalIjin Algorithm 1. There are two reasons why bcannot be chosen too small. One is that\nthe algorithm is designed to accommodate a predictor that provides a b-approximation of the true\nprobabilities, so larger bmakes the algorithm more robust to imperfect predictors. The other reason\nis that small bmeans using many small intervals, and thus a smaller number of samples assigned\nto each interval. This leads to higher noise in the Chebyshev polynomial estimators invoked within\neach interval (even if the assignment of elements to intervals is correct), and empirically impedes\nperformance. On the other hand, if we set bto be too large (resulting in one large interval the covers\nalmost the whole range of pi’s), we are essentially not using information from the predictor, and\nthus do not expect to improve over WY .\nTo resolve this issue, we introduce a sanity check in each interval Ij, whose goal is to rule out bases\nthat are too small. The sanity check passes if ~Sj2[0;1=lj], whereljis the left endpoint of Ij(i.e.,\nIj= [lj;rj]), and ~Sjis as deﬁned in Algorithm 1. The reasoning is as follows. On one hand, the\nChebyshev polynomial estimator which we use to compute ~Sjcan in fact return negative numbers,\nleading to failure modes with ~Sj<0. On the other hand, since all element probabilities in Ijare\nlower bounded by lj, it can contain at most 1=ljelements. Therefore, any estimate ~Sjof the number\nof elements in Ijwhich is outside [0;1=lj]is obviously incorrect.\nIn our implementation, we start by running Algorithm 1 with b= 2. If any interval fails the sanity\ncheck, we increment band repeat the algorithm (with the same set of samples). The ﬁnal base we\nuse is twice the minimal one such that all intervals pass the sanity check, where the ﬁnal doubling is\nto ensure we are indeed past the point where all checks succeed.\nThe effect of this base selection procedure on each of our datasets is depicted in Figures 1, 3, and 5.7\nFor a ﬁxed sample size, we plot the performance of our algorithm (dotted blue) as the base increases\ncompared to WY (solid orange, independent of base), as well as the fraction of intervals that failed\nthe sanity check (dashed green). The plots show that for very small bases, the sanity check fails on\nsome intervals, and the error is large. When the base is sufﬁciently large, all intervals pass the sanity\ncheck, and we see a sudden plunge in the error. Then, as the base continues to grow, our algorithm\ncontinues to perform well, but gradually degrades and converges to WY due to having a single\ndominating interval. This afﬁrms the reasoning above and justiﬁes our base selection procedure.\n7In those plots, for visualization, in order to compute the error whenever a sanity check fails in Ij, we\nreplace ~Sjwith a na ¨ıve estimate, deﬁned as the number of distinct sample elements assigned to Ij. Note\nthat our implementation never actually uses those na ¨ıve estimates, since it only uses bases that pass all sanity\nchecks.\n7\n\nPublished as a conference paper at ICLR 2021\n0 10 20 30 40 50 60\nBase0.00.20.40.60.81.01.2Error\nDataset: AOL Day 10, 10%\nOur Alg\nWY Alg\nFail %\nFigure 1: Error by base, AOL, sample size 10%\u0001n\n0 10 20 30 40 50\nSample size: % of n0.00.20.40.60.8Error\nDataset: AOL Day 10\nOur Alg\nWY Alg\nCR Alg Figure 2: Error per sample size, AOL\n0 200 400 600 800\nBase0.000.250.500.751.001.251.501.752.00Error\nDataset: IP Minute 59, 1%\nOur Alg\nWY Alg\nFail %\nFigure 3: Error by base, IP, sample size 1%\u0001n\n0 5 10 15 20 25\nSample size: % of n0.00.10.20.30.40.50.60.7Error\nDataset: IP Minute 59\nOur Alg\nWY Alg\nCR Alg Figure 4: Error per sample size, IP\n0 20 40 60 80 100 120\nBase0.00.20.40.60.81.01.2Error\nDataset: Zipfian, 5%\nOur Alg\nWY Alg\nFail %\nFigure 5: Error by base, Zipﬁan, sample size 5%n\n0 10 20 30 40 50\nSample size: % of n0.00.20.40.60.81.0Error\nDataset: Zipfian\nOur Alg\nWY Alg\nCR Alg Figure 6: Error per sample size, Zipﬁan\n3.2 R ESULTS\nAOL data. As the predictor, we use the RNN trained by Hsu et al. (2019) to predict the frequency\nof a given keyword. Their predictor is trained on the ﬁrst 5days and the 6th day is used for validation.\nThe results for day #10 are shown in Figure 2. (The results across all days are similar; see more\nbelow.) They show that our algorithm performs signiﬁcantly better than WY . Nonetheless, CR\n(which relies on access to a perfect predictor) achieves better performance on a much smaller sample\nsize. This is apparently due to highly speciﬁc traits of the predictor; as we will see presently, the\nperformance of CR is considerably degraded on the other datasets.\nIP data. Here too we use the trained RNN of Hsu et al. (2019). It is trained on the ﬁrst 7minutes\nand the 8th minute is used for validation. However, unlike AOL, Hsu et al. (2019) trained the RNN\nto predict the logof the frequency of the given IP address, rather than the frequency itself, due to\ntraining stability considerations. To use it as a predictor, we exponentiate the RNN output. This\ninevitably leads to less accurate predictions.\n8\n\nPublished as a conference paper at ICLR 2021\n10 20 30 40 50 60 70 80 90\nDay #0.00.10.20.30.40.5Error\nDataset: AOL, 5%\nOur Alg\nWY Alg\nCR Alg\n(a) Sample size: 5%\u0001n\n10 20 30 40 50 60 70 80 90\nDay #0.000.050.100.150.200.250.300.35Error\nDataset: AOL, 10%\nOur Alg\nWY Alg\nCR Alg (b) Sample size: 10%\u0001n\nFigure 7: Error across the different AOL days\n10 20 30 40 50 60\nMinute #0.10.20.30.40.50.60.7Error\nDataset: IP, 1%\nOur Alg\nWY Alg\nCR Alg\n(a) Sample size: 1%\u0001n\n10 20 30 40 50 60\nMinute #0.00.10.20.30.40.50.60.7Error\nDataset: IP, 2.5%\nOur Alg\nWY Alg\nCR Alg (b) Sample size: 2:5%\u0001n\nFigure 8: Error across the different IP minutes\nThe results for minute 59are shown in. Figure 4. (As in the AOL data, the results across all minutes\nare similar; see more below). Again we see a signiﬁcant advantage to our algorithm over WY for\nsmall sample sizes. Here, unlike the AOL dataset, CR does not produce good results.\nZipﬁan data. To form a predictor for this synthetic distribution, we drew a random sample of size\n10% ofn, and used the empirical count of each element in this ﬁxed sample as the prediction for its\nfrequency. If the predictor is queried for an element that did not appear in the sample, its predicted\nprobability is reported as the minimum 1=n. We use this ﬁxed predictor in all repetitions of the\nexperiment (which were run on fresh independent samples). The results are reported in Figure 6.\nAs with the IP data, our algorithm signiﬁcantly improves over WY for small sample sizes, and both\nalgorithms outperform CR by a large margin.\nAOL and IP results over time. Finally, we present accuracy results over the days/minutes of\nthe AOL/IP datasets (respectively). The purpose is to demonstrate that the performance of our\nalgorithm remains consistent over time, even when the data has moved away from the initial training\nperiod and the predictor may become ‘stale’. The results for AOL are shown in Figures 7a, 7b,\nyielding a median 2.2-fold and3.0-fold improvement over WY (for sample sizes 5%\u0001nand10%\u0001n,\nrespectively). The results for IP are shown in Figures 8a and 8b, yielding a median 1.7-fold and3.0-\nfold improvement over WY (for sample sizes 1%\u0001nand2:5%\u0001n, respectively). As before, CR\nperforms better than either algorithm on AOL, but fails by a large margin on IP.\nACKNOWLEDGMENTS\nThis research was supported in part by the NSF TRIPODS program (awards CCF-1740751 and\nDMS-2022448); NSF awards CCF-1535851, CCF-2006664 and IIS-1741137; Fintech@CSAIL;\nSimons Investigator Award; MIT-IBM Watson collaboration; Eric and Wendy Schmidt Fund for\n9\n\nPublished as a conference paper at ICLR 2021\nStrategic Innovation, Ben-Gurion University of the Negev; MIT Akamai Fellowship; and NSF Grad-\nuate Research Fellowship Program.\nThe authors would like to thank Justin Chen for helpful comments on a draft of this paper, as well\nas the anonymous reviewers.\nREFERENCES\nMaria-Florina Balcan, Travis Dick, Tuomas Sandholm, and Ellen Vitercik. Learning to branch. In\nInternational Conference on Machine Learning , pp. 353–362, 2018.\nLuca Baldassarre, Yen-Huan Li, Jonathan Scarlett, Baran G ¨ozc¨u, Ilija Bogunovic, and V olkan\nCevher. Learning-based compressive subsampling. IEEE Journal of Selected Topics in Signal\nProcessing , 10(4):809–822, 2016.\nAshish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. Compressed sensing using genera-\ntive models. In International Conference on Machine Learning , pp. 537–546, 2017.\nCl´ement Canonne and Ronitt Rubinfeld. Testing probability distributions underlying aggregated\ndata. In International Colloquium on Automata, Languages, and Programming , pp. 283–295.\nSpringer, 2014.\nCl´ement L. Canonne. A survey on distribution testing: Your data is big. but is it blue? Electron.\nColloquium Comput. Complex. , 22:63, 2015.\nYihe Dong, P. Indyk, Ilya P. Razenshteyn, and T. Wagner. Learning space partitions for nearest\nneighbor search. In ICLR , 2020.\nRonald A Fisher, A Steven Corbet, and Carrington B Williams. The relation between the number of\nspecies and the number of individuals in a random sample of an animal population. The Journal\nof Animal Ecology , pp. 42–58, 1943.\nOded Goldreich. Introduction to Property Testing . Cambridge University Press, 2017.\nIrving J Good. The population frequencies of species and the estimation of population parameters.\nBiometrika , 40(3-4):237–264, 1953.\nDanny Harnik, Ety Khaitzin, and Dmitry Sotnikov. Estimating unseen deduplication—from theory\nto practice. In 14thfUSENIXgConference on File and Storage Technologies ( fFASTg16), pp.\n277–290, 2016.\nChen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency estimation\nalgorithms. In International Conference on Learning Representations , 2019.\nTanqiu Jiang, Yi Li, Honghao Lin, Yisong Ruan, and David P Woodruff. Learning-augmented data\nstream algorithms. In International Conference on Learning Representations , 2019.\nDaniel M Kane, Jelani Nelson, and David P Woodruff. An optimal algorithm for the distinct ele-\nments problem. In Proceedings of the twenty-ninth ACM SIGMOD-SIGACT-SIGART symposium\non Principles of database systems , pp. 41–52, 2010.\nElias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial op-\ntimization algorithms over graphs. In Advances in Neural Information Processing Systems , pp.\n6348–6358, 2017.\nTim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned\nindex structures. In Proceedings of the 2018 International Conference on Management of Data ,\npp. 489–504, 2018.\nThodoris Lykouris and Sergei Vassilvitskii. Competitive caching with machine learned advice. In\nInternational Conference on Machine Learning , pp. 3302–3311, 2018.\nV . A. Markov. On functions of least deviation from zero in a given interval. St. Petersburg , 1892.\n10\n\nPublished as a conference paper at ICLR 2021\nMichael Mitzenmacher. A model for learned bloom ﬁlters and optimizing by sandwiching. In\nAdvances in Neural Information Processing Systems , pp. 464–473, 2018.\nAli Mousavi, Ankit B Patel, and Richard G Baraniuk. A deep learning approach to structured signal\nrecovery. In Communication, Control, and Computing (Allerton), 2015 53rd Annual Allerton\nConference on , pp. 1336–1343. IEEE, 2015.\nKrzysztof Onak and Xiaorui Sun. Probability–revealing samples. In International Conference on\nArtiﬁcial Intelligence and Statistics , volume 84 of Proceedings of Machine Learning Research ,\npp. 2018–2026. PMLR, 2018.\nAlon Orlitsky, Ananda Theertha Suresh, and Yihong Wu. Optimal prediction of the number\nof unseen species. Proceedings of the National Academy of Sciences , 113(47):13283–13288,\n2016. ISSN 0027-8424. doi: 10.1073/pnas.1607774113. URL https://www.pnas.org/\ncontent/113/47/13283 .\nManish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ml predictions.\nInAdvances in Neural Information Processing Systems , pp. 9661–9670, 2018.\nSofya Raskhodnikova, Dana Ron, Amir Shpilka, and Adam Smith. Strong lower bounds for approx-\nimating distribution support size and the distinct elements problem. SIAM Journal on Computing ,\n39(3):813–842, 2009.\nRonitt Rubinfeld. Taming big probability distributions. XRDS , 19(1):24–28, 2012.\nAlexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, and Herv ´e J´egou. Spreading vectors\nfor similarity search. In International Conference on Learning Representations , 2019. URL\nhttps://openreview.net/forum?id=SkGuG2R5tm .\nAF Timan, M Stark, IN Sneddon, and S Ulam. Theory of approximation of functions of a real\nvariable. 1963.\nGregory Valiant and Paul Valiant. Estimating the unseen: an n/log (n)-sample estimator for entropy\nand support size, shown optimal via new clts. In Proceedings of the forty-third annual ACM\nsymposium on Theory of computing , pp. 685–694, 2011.\nPaul Valiant and Gregory Valiant. Estimating the unseen: improved estimators for entropy and other\nproperties. In Advances in Neural Information Processing Systems , pp. 2157–2165, 2013.\nJun Wang, Wei Liu, Sanjiv Kumar, and Shih-Fu Chang. Learning to hash for indexing big data - a\nsurvey. Proceedings of the IEEE , 104(1):34–57, 2016.\nYihong Wu and Pengkun Yang. Chebyshev polynomials, moment matching, and optimal estimation\nof the unseen. The Annals of Statistics , 47(2):857–883, 2019.\nJames Zou, Gregory Valiant, Paul Valiant, Konrad Karczewski, Siu On Chan, Kaitlin Samocha,\nMonkol Lek, Shamil Sunyaev, Mark Daly, and Daniel G MacArthur. Quantifying unobserved\nprotein-coding variants in human populations provides a roadmap for large-scale sequencing\nprojects. Nature communications , 7(1):1–5, 2016.\n11\n\nPublished as a conference paper at ICLR 2021\nA O MITTED PROOFS\nA.1 A NALYSIS OF THE UPPER BOUND\nIn this subsection, we prove Theorem 1, which provides the upper bound for the sample complexity\nof Algorithm 1.\nProof of Theorem 1. Consider some i2[n]. The predictor \u0005gives us an estimate \u0005(i)2[pi;b\u0001pi]\nof the unknown true probability pi. Letjibe the integer for which \u0005(i)2[bji\nn;bji+1\nn]. We treatias\nassigned by the predictor to the interval Iji. Observe that as a consequence, we have\npi2\u0014bji\nn;bji+2\nn\u0015\n: (2)\nSupposeiis observedktimes in the sample. Deﬁne,\n~S(i) =8\n><\n>:1 +ak\u0000n\nbj\u0001k\u0001k!\nNkifbji\nn\u00140:5 logn\nN\n1 ifbji\nn>0:5 logn\nNandk\u00151\n0 ifbji\nn>0:5 logn\nNandk= 0\nTheak’s are the Chebyshev polynomial coefﬁcients as per Algorithm 1. Note that as we drew a total\nofPois(N)samples, the number of times each element iappears in the sample is distributed as\nNi=Pois(N\u0001pi)times, and the values Niare independent across different i’s (this is the standard\nPoissonization trick). Thus the ~S(i)’s are also independent. Moreover, note that the value ~Sjdeﬁned\nin Algorithm 1 satisﬁes ~Sj=P\ni:ji=j~S(i). Finally, by eq. (2) we have pi\u0001n\nbji2[1;b2]. Thus, if\nbji\nn\u00140:5 logn\nN;we compute the expectation of ~S(i)as (lettingj=jito ease notation),\nE[~S(i)] = 1 +LX\nk=0P(Ni=k)\u0001ak\u0010n\nbj\u0011k\n\u0001k!\nNk\n= 1 +LX\nk=0e\u0000Npi(Npi)k\nk!\u0001ak\u0010n\nbj\u0011k\n\u0001k!\nNk\n= 1 +e\u0000Npi\u0001LX\nk=0ak\u0010\npi\u0001n\nbj\u0011k\n= 1 +e\u0000Npi\u0001PL\u0010\npi\u0001n\nbj\u0011\n2[1\u0000\";1 +\"];\nsincepi\u0001n\nbj2[1;b2]and sincee\u0000Npi\u00141:However, ifbji\nn>0:5 logn\nN;thenE[~S(i)]equals the\nprobability that iis sampled, which is 1\u0000(1\u0000pi)N. But since pi>bji\nn>0:5 logn\nN;we have\nthat1\u0015E[~S(i)]>1\u0000\u0010\n1\u00000:5 logn\nN\u0011N\n>1\u0000e(0:5 logn=N)\u0001N= 1\u0000n\u00001=2:We note that if iis\nnever seen (which is possible even if pi6= 0), we do not know ji. However, in this case, ~S(i) = 0\nregardless of the value of ji. Therefore, we get an estimator ~S(i)such that ~S(i) = 0 ifpi= 0 and\nE[~S(i)]2[1\u0000\";1 +\"]otherwise, assuming \">n\u00001=2.\nNext we analyze the variance of ~S(i):Foriwithpi= 0,~S(i) = 0 always, so Var(~S(i)) = 0:\nOtherwise, ifbji\nn>0:5 logn\nN;then ~S(i)is always between 0and1, soVar(~S(i))\u00141:Finally, if\n12\n\nPublished as a conference paper at ICLR 2021\nbji\nn\u00140:5 logn\nN;thenVar(~S(i))\u0014E[(~S(i)\u00001)2]and we can write (again with j=ji)\nE[(~S(i)\u00001)2] =LX\nk=0P(Ni=k)\u0001\u0012\nak\u0010\npi\u0001n\nbj\u0011k\u00132\n=LX\nk=0e\u0000Npi(Npi)k\nk!\u0001\u0012\nak\u0010n\nbj\u0011k\n\u0001k!\nNk\u00132\n\u0014\u0012\nmax\n0\u0014k\u0014La2\nk\u0013\n\u0001LX\nk=0\u0010\npi\u0001n\nbj\u0011k\n\u0001\u0010n\nbj\u0011k\n\u0001k!\nNk\n\u0014\u0012\nmax\n0\u0014k\u0014La2\nk\u0013\n\u0001LX\nk=0\u0012b2\u0001k\u0001n\nN\u0013k\n; (3)\nwhere for the last inequality we noted that k!\u0014kk,pi\u0001n\nbj\u0014b2;andbj\u00151:\nNow, it is a well known consequence of the Markov Brothers’ inequality that all the coefﬁcients\nof the standard degree LChebyshev polynomial QL(x)are bounded by eO(L)(Markov, 1892).\nSincePL=PL\nk=0akxkis just\"\u0001Q\u0010\nb2+1\n2+x\u0001b2\u00001\n2\u0011\n;we have that for any ﬁxed b=O(1), the\ncoefﬁcientsakare all bounded by eO(L)as well. Therefore, for N=C\u0001b2\u0001L\u0001n1\u00001=Lfor some\nconstantC, we have that b2\u0001k\u0001n=N\u0014n1=L=C, so we can bound Equation (3) by\neO(L)\u0001LX\nk=0\u0012n1=L\nC\u0013k\n\u0014n\n(C0)L\nfor some other constant C0.\nIn summary, if pi6= 0;we have that E[~S(i)]2[1\u0000\";1 +\"]andVar(~S(i))\u0014\"2nif we choose C\nto be a sufﬁciently large constant, since \"=e\u0000\u0002(L)for a constant 1<b\u0014O(1). This is true even\nforbji\nn>0:5 logn\nNsince\"2n >1:However, we know that ~S=Plogbn\nj=0~Sj=Pn\ni=1~S(i);since by\nthe deﬁnition of ~Sj;~Sj=P\ni:ji=j~S(i):Therefore, since the estimators ~S(i)are independent, we\nhave that E[~S] =Pn\ni=1E[~S(i)]2(1\u0000\";1 +\")\u0001SandVar(~S) =P\ni:pi6=0Var(~S(i))\u0014\"2\u0001n\u0001S;\nwhere we use the independence of the ~S(i)’s. Therefore, since S\u0014n, with probability at least 0:9;\nj~S\u0000Sj=O(\")\u0001p\nn\u0001Sby Chebyshev’s inequality.\nRemark. We note that our analysis actually works (and becomes somewhat simpler) even if the\nalgorithm is modiﬁed to deﬁne ~Sj=P\ni2[n]:\u0005(i)2Ij\u0010\n1 +aNi\u0000n\nbj\u0001Ni\u0001Ni!\nNNi\u0011\nfor all intervals, as\nopposed to ~Sj= #fi2[n] :Ni\u00151;\u0005(i)2Ijgfor the intervals Ijwithbj\nn>0:5 logn\nN. However,\nbecause we can decrease the bias as well as the variance for these larger intervals, we modify the\nalgorithm accordingly. While this does not affect the theoretical guarantees, it demonstrated an\nimprovement in practice. This threshold was also used in Wu & Yang (2019) (the choice of leading\nconstant 0:5in the threshold term0:5 logn\nNis arbitrary, and for simplicity we have adopted the value\nthey used).\nA.2 A NALYSIS OF THE LOWER BOUND\nIn this subsection, we prove Theorem 2, which proves that the sample complexity of Algorithm 1 is\nessentially tight.\nProof of Theorem 2. In order to prove the lower bound, we shall deﬁne two distributions PandQ\nfor which the ﬁrst kmoments are matching, but their support size differs on at least \"nelements.\nFurthermore, both distributions will be supported on f0g[\u0002k\nn;k+1\nn;:::;2k\nn\u0003\n, so that a 2-factor ap-\nproximation predictor does not provide any useful information to an algorithm trying to distinguish\nthe two distributions.\n13\n\nPublished as a conference paper at ICLR 2021\nWe start with an overview of the deﬁnition of the two distributions and then explain how to formalize\nthe arguments, following the Poissonnization and rounding techniques detailed in Raskhodnikova\net al. (2009).\nLet\"=\u0010\nk\u00012k\u00001\u0001\u00002k\nk\u0001\u0011\u00001\nfor some integer k\u00151. Note that\"=e\u0000\u0002(k). In order to deﬁne the\ndistributions, we deﬁne k+ 1real numbers a0;:::;akasai=(\u00001)i\u0001(k\ni)\n2k\u00001\u0001(k+i)for everyi2f0;:::;kg.\nSuppose for now that nis a multiple of the least common multiple of 2k\u00001;k;:::; 2k, so thatai\u0001n\nis an integer for every i. We deﬁnePandQas follows:\n•The distribution P:For everyaisuch thatai>0, the support of Pcontainsai\u0001nelements\njthat have probability pj=ai\u0001k+i\nneach.\n•The distribution Q: For every aisuch thatai<0, the support of Qcontains\u0000ai\u0001n\nelementsjthat have probability qj=\u0000ai\u0001k+i\nneach.\nFirst we prove that PandQare valid distributions and that all of their non-zero probabilities are\ngreater than 1=n.\nClaim A.1. PandQas deﬁned above are distributions. Furthermore, their probability values are\neither 0or greater than 1=n.\nProof. The second part of the claim follows directly from the deﬁnition of PandQ. We continue\nto prove the ﬁrst part, that PandQare indeed distributions. It holds for Pthat\nX\naijai>0(nai)\u0001k+i\nn=kX\ni=0\nievenn\u00011\nk+i\u0001\u0012k\ni\u0013\n\u00011\n2k\u00001\u0001k+i\nn=1\n2k\u00001\u0001kX\ni=0\nieven\u0012k\ni\u0013\n= 1:\nSimilarly, for Q,\nX\nai<0(n(\u0000ai))\u0001k+i\nn=kX\ni=0\nioddn\u00011\nk+i\u0001\u0012k\ni\u0013\n\u00011\n2k\u00001\u0001k+i\nn=1\n2k\u00001\u0001kX\ni=0\niodd\u0012k\ni\u0013\n= 1:\nWe continue to prove that the ﬁrst kmoments of PandQare matching, and that their support size\ndiffers by\"n.\nClaim A.2. Leta1;:::;akbe deﬁned as above. Then\n•For anyr2f1;:::;kg, it holds thatPk\ni=0ai\u0001(k+i)r= 0.\n•Pk\ni=0ai=\".\nProof. By plugging the ai’s as deﬁned above,\nkX\ni=0ai\u0001(k+i)r=kX\ni=0(\u00001)i\u0000k\ni\u0001\n2k\u00001\u0001(k+i)\u0001(k+i)r=1\n2k\u00001kX\ni=0(\u00001)i\u0012k\ni\u0013\n\u0001(k+i)r\u00001:\nHence, letting r0=r\u00001, it sufﬁces to prove thatPk\ni=0(\u00001)i\u0000k\ni\u0001\n(k+i)r0= 0for all 0\u0014r0\u0014k\u00001:\nFor any ﬁxed k, note that since (k+i)r0is a degreer0polynomial in i,(k+i)r0can be written as a\nlinear combination of\u0000i\ns\u0001\nfor0\u0014s\u0014r0with coefﬁcients b0;:::;br0. Therefore, we would like to\nprove that:Pk\ni=0(\u00001)i\u0000k\ni\u0001Pr0\ns=0bs\u0000i\ns\u0001\n= 0. Fix somesinf0;:::;r0g: it sufﬁces to show that for\nany integerk,Pk\ni=0(\u00001)i\u0000k\ni\u0001\u0000i\ns\u0001\n= 0for all 0\u0014s\u0014k\u00001.\n14\n\nPublished as a conference paper at ICLR 2021\nSince\u0000k\ni\u0001\n\u0001\u0000i\ns\u0001\n=\u0000k\nk\u0000i;i\u0000s;s\u0001\n=\u0000k\ns\u0001\n\u0001\u0000k\u0000s\ni\u0000s\u0001\n;by settingk0=k\u0000sandi0=i\u0000s;we get\nkX\ni=0(\u00001)i\u0012k\ni\u0013\u0012i\ns\u0013\n=kX\ni=0(\u00001)i\u0012k\ns\u0013\n\u0001\u0012k\u0000s\ni\u0000s\u0013\n=\u0012k\ns\u0013\n\u0001k0X\ni0=0(\u00001)i0+s\u0012k0\ni0\u0013\n=\u0012k\ns\u0013\n\u0001(\u00001)s\u0001(1\u00001)k0= 0;\nwhere the last equality is since k0=k\u0000s\u00151:This concludes the proof of the ﬁrst item.\nWe continue to prove the second item in the claim:\nkX\ni=0ai=\": (4)\nRecall thatai=(\u00001)i\u0001(k\ni)\n2k\u00001\u0001(k+i), and that\"=\u0010\nk\u00012k\u00001\u0001\u00002k\nk\u0001\u0011\u00001\n, so plugging these into Equation (4)\nand multiplying both sides by 2k\u00001\u0001\u00002k\nk\u0001\n, this is equivalent to proving\n1\nk=kX\ni=0(\u00001)i\nk+i\u0001\u0012k\ni\u0013\u00122k\nk\u0013\n: (5)\nSince\u0000k\ni\u0001\n\u0001\u00002k\nk\u0001\n=\u00002k\nk;i;k\u0000i\u0001\n=\u00002k\nk+i\u0001\n\u0001\u0000k+i\nk\u0001\n;the right-hand side of Equation (5) equals\nkX\ni=0(\u00001)i\nk+i\u0001\u00122k\nk+i\u0013\u0012k+i\nk\u0013\n=kX\ni=0(\u00001)i\u0001\u00122k\nk+i\u0013\n\u0001\u0012k+i\u00001\nk\u00001\u0013\n\u00011\nk:\nMultiplying by k, it sufﬁces to show that\n1 =kX\ni=0(\u00001)i\u0001\u00122k\nk+i\u0013\n\u0001\u0012k+i\u00001\nk\u00001\u0013\n: (6)\nTo do this, let j=k+i. Then, note that\u0000k+i\u00001\nk\u00001\u0001\n=\u0000j\u00001\nk\u00001\u0001\n=(j\u00001)\u0001\u0001\u0001(j\u0000k+1)\n(k\u00001)!which is a degree\nk\u00001polynomial in j. For 1\u0014j\u0014k\u00001the polynomial equals 0, but forj= 0 the polynomial\nequals (\u00001)k\u00001:Therefore, the right hand side of Equation (6) equals\n2kX\nj=1(\u00001)j\u0000k\u0001\u00122k\nj\u0013\n\u0001(j\u00001)\u0001\u0001\u0001(j\u0000k+ 1)\n(k\u00001)!= 1+(\u00001)\u0000k\u00012kX\nj=0(\u00001)j\u0001\u00122k\nj\u0013\n\u0001(j\u00001)\u0001\u0001\u0001(j\u0000k+ 1)\n(k\u00001)!:\nAs proven in the ﬁrst part of this proof, for any P(j)of degree 0\u0014r\u00142k\u00001;P2k\nj=0(\u00001)j\u00002k\nj\u0001\nP(j) = 0:Therefore, the summation on the right hand side is 0, so this simpli-\nﬁes to 1, as required.\nThe following corollary follows directly from the deﬁnition of PandQand the previous claim.\nCorollary A.1. The following two items hold for PandQas deﬁned above.\n•E[Pr] =E[Qr]for allr2f1;:::;kg.\n•TV(P;Q) =\"n.\nThe above corollary states that indeed PandQas deﬁned above have matching moments for r= 1\ntok, and that they differ by \"nin their support size. This concludes the high level view of the\nconstruction of the distributions PandQ. In order to ﬁnalize the proof we rely on the standard\nPossionization and rounding techniques.\nFirst, by Theorem 5.3 in Raskhodnikova et al. (2009), any s-samples algorithm can be simulated by\nanO(s)-Poisson algorithm. Hence, we can assume that the algorithm takes Poi(s)samples, rather\nthan an arbitrary number s. Second, we can alleviate the assumption that the ai\u0001nvalues (similarly\n15\n\nPublished as a conference paper at ICLR 2021\n\u0000ai\u0001n) are integral for all i, by rounding down the value in case it is not integral, and choosing\nn\u0000Pk\ni=0dai\u0001neadditional elements in Pso that each has probability 1=n(and analogously for\nQ). By Claim 5.5 in Raskhodnikova et al. (2009) this process increases the number of values in\nPandQby at mostO(k2). Hence, the distributions are now well deﬁned, and we can rely on the\nfollowing theorem from Raskhodnikova et al. (2009).\nTheorem 3 (Corollary 5.7 in Raskhodnikova et al. (2009), restated.) .LetPandQbe random\nvariables over positive integers b1< ::: < b k\u00001, that have matching moments 1throughk\u00001.\nThen for any Poission- salgorithmAthat succeeds to distinguish PandQwith high probability,\ns= \n(n1\u00001=k).\nTherefore, plugging the value of \", we get ans= \n(n1\u0000\u0002(log(1=\")))lower bound as claimed.\nA.3 B OUNDS WHEN THE PREDICTOR IS ACCURATE UP TO LOWTOTAL VARIATION\nDISTANCE\nIn this section, we consider estimating support size when we are promised a bound on the total\nvariation distance, or `1distance, between the prediction oracle and the true distribution. First, we\nshow that if the predictor is accurate up to extremely low total variation distance, then the algorithm\nof Canonne & Rubinfeld (2014) is optimal.\nTheorem 4. Let\u0005represent our predictor, where for each sample we are given a random sample\ni\u0018P along with \u0005(i). If we deﬁne p(i) :=P(i\u0018P), then ifkP\u0000 \u0005k1=Pn\ni=1jp(i)\u0000\u0005(i)j\u0014\"\n2,\nthen usingO(\"\u00002)samples, the algorithm of Canonne and Rubinfeld can approximate the sample\nsize up to an \"\u0001nadditive error.\nProof. The algorithm of Canonne & Rubinfeld (2014) works as follows. For each sample (i;\u0005(i)),\nit computes 1=\u0005(i), and averages this over O(\"\u00002)samples.\nFrom now on, we also make the assumption that \u0005(i)\u00151=nfor alli. This is because we know\np(i)\u00151=nwheneveriis sampled, so the predictor \u00050= max(\u0005;1=n)is a strict improvement over\n\u0005for anyithat is sampled.\nIf we drawi\u0018P , then E[1\np(i)] =P\ni:p(i)6=0p(i)\u00011\np(i)=S, whereSis the support size. Also,\nE[1\n\u0005(i)\u00001\np(i)] =P\ni:p(i)6=0p(i)\u0001(1\n\u0005(i)\u00001\np(i)) =P[p(i)\n\u0005(i)\u00001];which, if \u0005(i)\u00151\nn;is bounded in\nabsolute value byPjp(i)\n\u0005(i)\u00001j\u0014n\u0001Pjp(i)\u0000\u0005(i)j\u0014\"\u0001n\n2. So,Ei\u0018P[1\n\u0005(i)]2[S\u0000\"\u0001n;S+\"\u0001n].\nSince we have made sure that \u0005(i)\u00151\nnwheneverxis sampled,Var(1\n\u0005(i))\u0014n2. Therefore, by\na basic application of Chebyshev’s inequality, an average of O(\"\u00002)samples of1\n\u0005(i)is sufﬁcient to\nestimateSup to a\"\u0001nadditive error.\nThe Canonne and Rubinfeld algorithm has been shown to be far less robust to the error of our\npredictor, even when given an equal number of samples as our main algorithm. This suggests that\nan\"-total variation distance bound assumption is a less suitable assumption in our context.\nConversely, if the predictor is only promised to be accurate up to not-as-low total variation distance,\nwe show that no algorithm can do signiﬁcantly better than Wu & Yang (2019).\nTheorem 5. Let\u0005represent our predictor, and recall that p(i) :=P(i\u0018P). Suppose the only\npromise we have on \u0005is thatkP\u0000 \u0005k1=Pn\ni=1jp(i)\u0000\u0005(i)j\u0014\rfor some\r\u0015C\", whereCis\nsome sufﬁciently large ﬁxed constant and \"\u00151\nn. Then, there does not exist an algorithm that can\nlearn the support size of Pup to an\"\u0001nadditive error using o(n=logn)samples.\nProof. Consider some unknown distribution Dover[n]that we want to learn the support size of,\nwith only samples and no frequency predictor. Moreover, assume that for all i2[n];D(i)\u00151=n\nwheneverD(i)>0, whereD(i) =P(i\u0018D). Now, letm=n=\r, and consider the following\ndistributionPon[m]. Forn<i\u0014m;letp(i) =1\nm=\r\nn;and for 1\u0014i\u0014n;letp(i) =\r\u0001D(i).\nLet\u0005be the uniform distribution on [n+ 1 :m], i.e., \u0005(i) = 0 ifi\u0014nand\u0005(i) =1\nm\u0000nif\ni\u0015n+ 1. It is clear thatPand\u0005are both distributions over [n](i.e., the sum of the probabilities\n16\n\nPublished as a conference paper at ICLR 2021\nequal 1), the minimum nonzero probability of Pis at least\r\nn=1\nm;and thatTV(P;\u0005)\u0014\r. Finally,\nthe support size of Pis preciselym\u0000nmore than the support size of D:\nAssume the theorem we are trying to prove is false. Then, there is an algorithm that, given k=\no(m\nlogm)random samples (i;\u0005(i))where eachi\u0018P, can determine the support size of Pup to an\n\"\u0001mfactor. Then, since the support size of Dis exactlym\u0000nless than the support size of D, one\ncan learn the support size of Dup to an\"\u0001m\u0014\"\n\r\u0001n\u0014n\nCadditive error using ksamples (i;\u0005(i))\nfori\u0018P.\nNow, using only \u0002(\r\u0001k)samples fromD, it is easy to simulate ksamples from (i;\u0005(i));where\ni\u0018P . This is because to generate a sample (i;\u0005(i));we ﬁrst decide if i > n ori\u0014n(the\nlatter occurring with probability \r). In the former case, we draw iuniformly at random and let\n\u0005(i) =1\nm\u0000n:In the latter case, we know that Pconditioned on i\u0014nhas the same distribution\nasD, so we just draw i\u0018D and\u0005(i) = 0 , since \u0005is supported only on [n+ 1 :m]. With very\nhigh probability (by a simple application of the Chernoff bound), we will not use more than \u0002(\r\u0001k)\nsamples fromD:\nOverall, this means that using only \u0002(\r\u0001k) =o\u0010\n\r\u0001m\nlogm\u0011\n=o\u0010\nn\nlogm\u0011\n=o\u0010\nn\nlogn\u0011\nsamples (since\n\r\u0015\"\u00151=n) fromD, we have learned the support size of Dup to ann\nCadditive error. This\nalgorithm works for an arbitrary Dand the predictor \u0005does not actually depend on D. Therefore,\nfor sufﬁciently large C, this violates the lower bound of Wu & Yang (2019), who show that one\ncannot learn the support size of Dup ton\nCadditive error using o\u0010\nn\nlogn\u0011\nsamples.\n17",
  "textLength": 51503
}