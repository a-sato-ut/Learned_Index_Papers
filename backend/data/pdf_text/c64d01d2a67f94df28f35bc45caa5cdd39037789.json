{
  "paperId": "c64d01d2a67f94df28f35bc45caa5cdd39037789",
  "title": "Shift-Table: A Low-latency Learned Index for Range Queries using Model Correction",
  "pdfPath": "c64d01d2a67f94df28f35bc45caa5cdd39037789.pdf",
  "text": "Shift-Table: A Low-latency Learned Index for Range Queries\nusing Model Correction\nAli Hadian\nImperial College LondonThomas Heinis\nImperial College London\nABSTRACT\nIndexing large-scale databases in main memory is still challeng-\ning today. Learned index structures â€” in which the core com-\nponents of classical indexes are replaced with machine learning\nmodels â€” have recently been suggested to significantly improve\nperformance for read-only range queries.\nHowever, a recent benchmark study shows that learned in-\ndexes only achieve limited performance improvements for real-\nworld data on modern hardware. More specifically, a learned\nmodel cannot learn the micro-level details and fluctuations of\ndata distributions thus resulting in poor accuracy; or it can fit to\nthe data distribution at the cost of training a big model whose\nparameters cannot fit into cache. As a consequence, querying a\nlearned index on real-world data takes a substantial number of\nmemory lookups, thereby degrading performance.\nIn this paper, we adopt a different approach for modeling a\ndata distribution that complements the model fitting approach\nof learned indexes. We propose Shift-Table , an algorithmic layer\nthat captures the micro-level data distribution and resolves the\nlocal biases of a learned model at the cost of at most one memory\nlookup. Our suggested model combines the low latency of lookup\ntables with learned indexes and enables low-latency processing of\nrange queries. Using Shift-Table, we achieve a speedup of 1.5X to\n2X on real-world datasets compared to trained and tuned learned\nindexes.\n1 INTRODUCTION\nTrends in new hardware play a significant role in the way we\ndesign high-performance systems. A recent technological trend is\nthe divergence of CPU and memory latencies, which encourages\ndecreasing random memory access at the cost of doing more\ncompute on cache-resident data [24, 40, 43].\nA particularly interesting family of methods exploiting the\nmemory/CPU latency gap are learned index structures. A learned\nindex uses machine learning instead of algorithmic data struc-\ntures to learn the patterns in data distribution and exploits the\ntrained model to carry out the operations supported by an al-\ngorithmic index, e.g., determining the location of records on\nphysical storage [ 7,12,17,23,24,28]. If the learned index man-\nages to build a model that is compact enough to fit in processor\ncache, then the results can ideally be fetched with a single access\nto main memory, hence outperforming algorithmic structures\nsuch as B-trees and hash tables.\nIn particular, learned index models have shown a great poten-\ntial for range queries, e.g., retrieving all records where the key is\nin a certain range ğ´<key<ğµ. To enable efficient retrieval of\nrange queries, range indexes keep the records physically sorted.\nTherefore, retrieving the range query is equivalent to finding\nthe first result and then sequentially scanning the records to\nÂ©2021 Copyright held by the owner/author(s). Published in Proceedings of the\n24th International Conference on Extending Database Technology (EDBT), March\n23-26, 2021, ISBN 978-3-89318-084-4 on OpenProceedings.org.\nDistribution of this paper is permitted under the terms of the Creative Commons\nlicense CC-by-nc-nd 4.0.retrieve the entire result set. Therefore, processing a range query\nğ´<key <ğµis equivalent to finding the first result, i.e., the\nsmallest key in the dataset that is greater than or equal to ğ´(sim-\nilar to lower_bound(A) in the C++ Library standard). A learned\nindex can be built by fitting a regression model to the cumulative\ndistribution function (CDF) of the key distribution. The learned\nCDF model can be used to determine the physical location where\nthe lower-bound of the query resides, i.e., pos(A)=ğ‘Ã—ğ¹ğœƒ(A)\nwhere N is the number of keys and ğ¹ğœƒis the learned CDF model\nwith model parameters ğœƒ.\nLearned indexes are very efficient for sequence-like data (e.g.,\nmachine-generated IDs), as well as synthetic data sampled from\nstatistical distributions. However, a recent study using the Search-\nOn-Sorted-Data benchmark (SOSD) [ 21] shows that for real-\nworld data distributions, a learned index has the same or even\npoorer performance compared to algorithmic indexes. For many\nreal-world data distributions, the CDF is too complex to be learned\nefficiently by a small cache-resident model. The data distribution\nof real-world data has \"too much information\" to be accurately\nrepresented by a small machine-learning model, while an accu-\nrate model is needed for an accurate prediction. One can of course\nuse smaller models that fit in memory with the cost of lower pre-\ndiction accuracy, but will end up in searching a larger set of\nrecords to find the actual result which consequently increases\nmemory lookups and degrades performance. Alternatively, a\nhigh accuracy can be achieved by training a bigger model, but\naccessing the model parameters incurs multiple cache misses\nand also increases memory lookups, reducing the margins for\nperformance improvement.\nIn this paper, we address the challenge of using learned models\non real-world data and illustrate how the micro-level details\n(e.g., local variance) of a cumulative distribution can dramatically\naffect the performance of a range index. We also argue that a\npure machine learning approach cannot shoulder the burden of\nlearning the fine-grained details of an empirical data distribution\nand demonstrate that not much improvement can be achieved\nby tuning the complexity or size thresholds of the models.\nWe suggest that by going beyond mere machine learning mod-\nels, the performance of a learned index architecture can be sig-\nnificantly improved using a complementary enhancement layer\nrather than over-emphasizing on the machine learning tasks. Our\nsuggested layer, called Shift-Table is an algorithmic solution that\nimproves the precision of a learned model and effectively acceler-\nates the search performance. Shift-Table, targets the micro-level\nbias of the model and significantly improves the accuracy, at the\ncost of only one memory lookup. The suggested layer is optional\nand applied after the prediction; it can hence be switched on or\noff without re-training the model.\nOur contributions can be summarized as follows:\nâ€¢We identify the problem of learning a range index for real-\nworld data, and illustrate the difficulty of learning from\nthis data.arXiv:2101.10457v1  [cs.DB]  25 Jan 2021\n\nâ€¢We suggest the Shift-Table approach for correcting a learned\nindex model, which complements a valid (monotonically\nincreasing) CDF model by correcting its error.\nâ€¢We show how, and in which circumstances, the suggested\nmethods can be used for best performance.\nâ€¢We suggest cost models that determine whether the Shift-\nTable layer can boost performance.\nâ€¢The experimental results show that our suggested method\ncan improve existing learned index structures and bring\nstable and almost-constant lookup time for real-world\ndata distributions. Our enhancement layer achieves up\nto 3X performance improvement over existing learned\nindexes. More interestingly, we show that for non-skewed\ndistributions, the Shift-Table layer is effective enough to\nhelp a dummy linear model outperform the state of the\nart learned indexes on real-world datasets\n2 MOTIVATION\n2.1 Lookup Cost for Learned Models\nIn modern hardware, the lookup times of in-memory range in-\ndexes and the binary search algorithm are mainly affected by\ntheir memory access pattern, most notably by how the algorithm\nuses the cache and the Last-Level-Cache (LLC) miss rate.\nProcessing a range query in a learned index has two stages:\n1)Prediction : Running the learned model to predict the location\nof the first result for the range query; and 2) Local search (also\nknown as last-mile search ): searching around the predicted loca-\ntion to find the actual location of the first result. Figure 1a shows\ncommon search methods for the local search. If the learned model\ncan determine a guaranteed range area around the predicted po-\nsition, one can perform binary search. Otherwise, exponential or\nlinear search should be used, starting from the predicted position.\nA cache miss in a learned index can occur in the first stage\nfor accessing the parameters of the model (if the model is too\nbig to fit in cache), or in stage two for the local search. Key in\nunderstanding the cost of a learned index is that local search is\ndone entirely over non-cached blocks of memory. A learned index\nbuilt over millions of records could predict the location of records\nwith an error of, say, 1000 records and yet achieve no performance\ngain over binary search algorithms or algorithmic indexes. This\nis because while the learned index fits the models in cache, its\nalgorithmic competitors also fit the frequently-accessed parts of\nthe data in cache, which limits the potential for improvement for\na learned index.\n2.2 Lookup Cost for Algorithmic Indexes\nClassical algorithms, such as binary search, can be seen as a hi-\nerarchy of [non-learned] models, which take the middle-point\nas its parameter and predicts (accurately) which direction the\nsearch should follow. Specifically for the first few steps of binary\nsearch where the middle-points usually reside in cache, the func-\ntionality of binary search is the same as a learned model from a\nperformance point of view.\nIn a pure binary search on the entire data, the first set of mem-\nory locations accessed by the algorithm (i.e., the median, quarters,\netc.) will already be in the CPU cache after a few lookups. There-\nfore, the major bottleneck in binary search is for the latter stages\nof search where the middle elements are not in cache, causing\nlast-level-cache (LLC) misses. Figure 1b shows a schematic illus-\ntration of how caching accelerates binary search.In basic implementations of binary search, the â€œhot keysâ€ are\ncached with their payload and nearby records in the same cache\nline, which wastes cache space. Binary search thus uses the cache\npoorly and there are more efficient algorithmic approaches whose\nperformance is not sensitive to data distributions.\nCache-optimized versions of binary search, e.g., a binary search\ntree such as FAST [ 20], a read-only search tree that co-locates\nthe hot keys but still follows the simple bisecting method of bi-\nnary search, are up to 3X faster than binary search [ 21]. This\nis because FAST keeps more hot keys in the cache and hence it\nneeds to scan a shorter range of records in the local search phase\n(cache-non-resident iterations of the search).\n2.3 Preliminary Experimental Analysis\nFor a tangible discussion and to elaborate on the real cost of a\nlearned model, we provide a micro-benchmark that measures\nthe cost of errors in a learned index. We use the experimental\nconfiguration used in the SOSD benchmark [ 21], i.e., searching\nover 200M records with 32-bit keys and 64-bit payloads. Figure 2a\nshows the lookup time of the second phase (local search) in a\nlearned model for different prediction errors. We include the\nlookup times for binary search, as well as FAST [ 20], over the\nwhole array of 200M keys.\nWe are interested to see that if the position predicted by a\nlearned index, say predicted_pos(ğ‘¥), has an error Î”, then how\nlong does it take in the local phase to find the correct record. Thus,\nfor each query ğ‘¥ğ‘–, we pre-compute the â€˜outputâ€™ of the learned\nindex with error Î”, i.e.,[predicted_pos(ğ‘¥ğ‘–)Â±Î”], and then run\nthe benchmark given {ğ‘¥ğ‘–,[predicted_pos(ğ‘¥ğ‘–)Â±Î”]}tuples.\nAs shown in Figure 2a, if the error of the model is more than\nâˆ¼300 records on average, then FAST outperforms the learned\nmodel (with linear or exponential local search). Even if the learned\nmodel can give a guaranteed range around the predicted point\nto guide the local search and enable binary search, FAST outper-\nforms it if the error exceeds 1000 records. The same trend can be\nseen for the LLC miss rates in Figure 2b.\nNote that this micro-benchmark over-estimates the maximum\nerror that the learned index can have because we only compare\nthe time of local search phase in a learned index with the total\nsearch time of FAST and binary search. Considering the time\ntaken to execute the model for predicting the location, a learned\nmodel needs to have a much lower error to compete with the\ngeneric, reliable, and distribution-independent algorithms such\nas binary search and FAST. For example, FAST takes 200 nanosec-\nonds to search a key in the entire 200M-key dataset. If a learned\nindex takes, say, 120 nanoseconds to run (for accessing model\nparameters and computing the prediction), then the local search\ncan take at most 80 nanoseconds so that the learned index can\noutperform FAST, which means that the prediction error ( Î”) must\nbe less than 16 records (based on Figure 2a).\nTuning the learned index for a balance of model size and ac-\ncuracy is a challenging task. Improving the local search time\nrequires using a more accurate model with a higher learning\ncapacity and more parameters. However, accessing such a big\nmodel typically incurs further cache misses during model exe-\ncution, and consequently the lookup time. Therefore, if the data\ndistribution cannot be learned efficiently with a small memory\nfootprint (fitting into cache), outperforming cache-efficient al-\ngorithmic indexes is very challenging. This is indeed the case\nfor most real-world datasets that cannot be modelled accurately\nwith a small-sized model.\n\nkeyModelpredictionActual positionÎ”(model-provided max error estimate)1234Exponential search Linear search Binary search\n56(a) Different \"last-mile\" search methods (performed after location prediction) in learned index. The locations predicted by the\nmodel depend on the query and are not known in advance. Since the last-mile search algorithms need to access different memory\nlocations for each query, they cannot exploit the processor cache and the search algorithm incurs multiple cache misses\n!ğ‘4!ğ‘2ğ‘!3ğ‘4L1-cache-resident locationsL2 / L3 -cache-resident locations Non-cached location accessed by binary search, causing LLC miss  \n(b) Schematic illustration of processor caching in binary search. The locations accessed by the very early stage of binary search,\nsuch as the min, max, and the midpoint, are frequently accessed and available in the L1 cache. Further steps of binary access\nlocations that are less frequently accessed and fit on lower levels of the memory hierarchy. Therefore, a deterministic search\nalgorithm like binary search enjoys a high cache hit rate\nFigure 1: Comparison of patterns in binary search (partially cached) and local search in learned indices (non-cached).\n2.4 Difficulty of Learning Real-world Data\nTo use a learned index in a production system, it is essential to\nidentify when learned indexes fail to achieve superior perfor-\nmance and what aspects of the data distribution contributes to\nthe performance of a learned index model. We realized that a\nmajor challenge in understanding learned indexes is that the\ncommon practices of performance evaluation for indexing al-\ngorithms are misleading for learned indexes. For example, it is\ncommon to use the uniform and skewed distributions (such as\nlog-normal) as arguably the two best- and worst-case extremes\nfor a search task [ 24]. However, for evaluating search over sorted\nread-only data, the difficulty of the task is determined by the\nunpredictability of the data, which is not necessarily a factor of\nskewness or shape parameter of the data distribution. As we will\nshow in this section, most statistical distributions are much easier\nto model than real-world data.\nDistributions that matter. An interesting observation from the\nSOSD benchmark results is that even for datasets that have the\nsame background distribution, e.g., both closely match a uniform\ndistribution, the performance of a learned model can vary signif-\nicantly, depending on the fine-grained details in the empirical\nCDFs. For example, consider Figures 3a and 3b, which repre-\nsent two CDFs that are both close to uniform. The uniform data\n(uden64 [21]) is comprised of dense integers that are synthetically\nsampled from a uniform distribution, and Facebook ( face64 [21])\nis a Facebook user ID dataset. While both datasets match closelywith the uniform distribution, face64 is significantly harder to\nmodel due to its fine-grained details in the CDF. The lookup time\nof learned indexes (both RMI and Radix-Splines) for face64 is\n6-7Ã—higher than that of uden64 (see Table 2) because there are\nmany micro-level details (unpredictability) in the CDF, hence a\nhuge model with a high learning capacity is needed to fit the CDF\naccurately. Using the RMI learned index, for example, the uden64\ndata is easily modelled with a simple line (two parameters) with\nnear-zero error, while the best architecture found by the SOSD\nbenchmark for modelling the face64 data is a hierarchy of two\nlinear models, a huge model (136MB), with an average error of\n202 records.\nGenerally speaking, real-world datasets are more difficult to\nlearn compared to synthetic ones and the learned index built\nover them is not significantly faster than the algorithmic rivals.\nThe main question remains what distinguishes a real-world data\nfrom a synthetic one? Consider the four distributions in Figure 3,\nwhere Figures 3a, 3c are synthetic (generated from uniform and\nlog-normal distributions), and Figures 3b, 3d are real-world data.\nThe mini-chart inside each CDF highlights the distribution in\na small sub-range, i.e., a â€œzoomed-inâ€ view of the CDF. For the\nsynthetic data, the CDF is very smooth in any short sub-range\nof the whole CDF. Synthetic data (such as uniform, normal and\nlog-normal) are built using a cumulative density function that\nis derivable, meaning that the at any small sub-range, the shape\nof the CDF is close to a straight line with a slope that is close\nto the derivation of the underlying CDF in that range. Such a\n\n101102103104105106\nError (search area)02004006008001000search time (ns)\nLinear\nBinary\nExponentialBinary w/o model\nFAST\nDRAM latency(a) Lookup time\n100101102103104105106\nError (search area)02468101214161820Cache misses\n(b) Cache misses\nFigure 2: Cost of local search in a learned index\nsmooth CDF has less information to be compressed into a model.\nFor example, a learned index model based on linear splines can\naccurately fit the whole CDF by fitting each part of the CDF to a\nline. Even for very skewed distributions, such as log-normal, the\ndata is so predictable that it can be easily fitted to simple, linear\nmodels.\nReal-world data, however, is much less predictable and has\na much higher level of complexity in its patterns. Even if an\nideal learning algorithm is used to model the real-world data, the\nmodel itself needs to be very big because the compressed version\nof the CDF (to be stored as a model) is still very big.\nThis explains why state-of-the-art learned indexes perform ex-\ntremely well for datasets that are synthetically generated from a\nstatistical distribution (such as uniform, normal, and log-normal),\nbut perform comparably poor for real-world data that even al-\nmost match (shapewise) with those synthetic distributions [ 21].\nOn real-world datasets, learned indexes have a high cache miss\nrate and lookup time, contrary to their primary goal of having\nfewer cache misses.\nUsing learned models is beneficial when they are 1) accurate\nenough to predict a position within the same cache line that con-\ntains the data point, otherwise the lookup time will be adversely\naffected due to multiple cache misses, and 2) compact enough to\nfit in cache and not to cause LLC misses. With this in mind, we\ncan argue that a pure machine-learning approach might fail to\nâ€œlearn the data perfectlyâ€ and â€œfit the model in cacheâ€ simultane-\nously, specifically in case of real-world datasets that contain a lot\nof underlying patterns like spikes and generally noise.\nAs a consequence, learned models are crucial to indexing but\nthey cannot shoulder the burden of indexing the data alone. We\nhence suggest an algorithmic layer that can mitigate the difficulty\n(a) uniform\n (b) Facebook\n(c) Lognormal\n (d) OSMC\nFigure 3: Example distributions with different complexi-\nties in micro and macro levels\nRecordskeyRecordskeySimple modelPDC (Correction)\nShift-TableModel\n(a) Learned index\nRecordskeyRecordskeySimple modelPDC (Correction)\nShift-TableModel (b) Model + Shift-Table\nFigure 4: Leveraging correction layers to a learned index\nof learning the data distribution. In this approach, the learned\nmodel is allowed to learn an semi-accurate, small model that\nlearns the holistic shape of the distribution, and the fine-tuned\nmodelling is provided by the algorithmic layers.\n2.5 Model Correction\nWhile learned index models are powerful tools for describing a\ndata distribution in a compact representation, merely focusing\non learning a highly-accurate model does not necessarily lead\nto a high-performance index. In this paper, we suggest a new\napproach for boosting existing learned models with additional\nlayers, specifically developed with hardware costs in mind.\nThe suggested helping layers add a small overhead when exe-\ncuting queries, but significantly reduce the overall lookup time\nof the learned index. The suggested layers are very powerful\nand consequently allow for using more lightweight models, yet\nideally avoid computationally-expensive algorithms for training.\nAs Figure 4 illustrates, in addition to the learned index model\nwe add a correction layer, an optional component, that can be\nadded to improve the performance. We explore the potential of\ncorrection layers in the next sections.\n3 SHIFT-TABLE\nA learned model predicts a relative position ğ¹ğœƒ(ğ‘¥)for a given\nqueryğ‘¥. To calculate the position of the result, the estimated\n\nrelative position is multiplied by the number of keys, and trun-\ncated to an integer (the index), hence the predicted position is\n[ğ‘ğ¹ğœƒ(ğ‘¥)]. The actual position of the record, however, is ğ‘ğ¹(ğ‘¥)\nwhereğ¹(ğ‘¥)is the empirical CDF of the data points, and ğ‘is\nthe data size. Therefore, the result is ğ‘ğ¹(ğ‘¥)âˆ’[ğ‘ğ¹ğœƒ(ğ‘¥)]records\nahead of the predicted position. We identify ğ‘ğ¹(ğ‘¥)âˆ’[ğ‘ğ¹ğœƒ(ğ‘¥)]as\nthedrift ofğ¹ğœƒat keyğ‘¥, which is the signed error of the prediction,\nas opposed to the absolute error.\nThe idea of the Shift-Table layer is to have a lookup table that\ncontains the drift values so that the drift of the prediction can\nbe corrected. Capturing the drift for every value of ğ‘¥requires an\nauxiliary index, which is not feasible. However, we can use the\noutput of the learned index model ( [ğ‘ğ¹ğœƒ(ğ‘¥)]), which is in the\nrange of[0,ğ‘], and we construct a mapping from each possible\noutput of the model, say ğ‘˜, to â€œhow far ahead is the actual record if\nmodel predicts kâ€™s recordâ€, so that we can correct the predictions\nusing this mapping. This means that for each prediction, we only\nneed an extra lookup of ğ‘˜in a fixed array of size ğ‘.\nTo build the Shift-Table layer, we first partition the keys ğ‘¥0,Â·Â·Â·,ğ‘¥ğ‘âˆ’1\nintoğ‘partitions. We define ğ‘ƒğ‘˜as the set of keys for which the\nmodel predicts ğ‘˜as the position:\nğ‘ƒğ‘˜={ğ‘¥|[ğ‘ğ¹ğœƒ(ğ‘¥)]=ğ‘˜} (1)\nEach of the indexed keys in ğ‘ƒğ‘˜has an index, say ğ‘ğ¹(ğ‘¥)and\na prediction ğ‘˜=[ğ‘ğ¹ğœƒ(ğ‘¥)]. For each partition, we extract two\nparameters that specify the range for local search, namely Î”ğ‘˜\nandğ¶ğ‘˜.Î”ğ‘˜is defined as:\nÎ”ğ‘˜=min(ğ‘ğ¹(ğ‘¥)âˆ’ğ‘˜])âˆ€ğ‘¥âˆˆğ‘ƒğ‘˜ (2)\nwhich indicates that if the predicted location is ğ‘˜, the search\nshould be started at point ğ‘˜+Î”ğ‘˜. Also,ğ¶ğ‘˜=|ğ‘ƒğ‘˜|is the cardinality\nofğ‘ƒğ‘˜, i.e., the number of indexed keys for which the prediction\npredicts the ğ‘˜â€™th record, in other words, the length of the area\nthat has to be searched in the local search phase.\nTo correct the prediction, we first compute the predicted posi-\ntionğ‘˜=[ğ‘ğ¹ğœƒ(ğ‘¥)], and then perform local search in the range\nof[ğ‘˜+Î”ğ‘˜,ğ‘˜+Î”ğ‘˜+ğ¶ğ¾âˆ’1].\nThe number of partitions depends on the range of the output of\nthe learned index, which should be 0,ğ‘). Therefore, The <Î”ğ‘˜,ğ¶ğ‘˜>\npairs: pairs are stored in a single array of size ğ‘, so that the\ncorrection can be done using a single lookup into the array of\npairs.\nA Shift-Table layer is depicted in Figure 5. The index contains\n100 elements in range [0,999]. The CDF model is a simple model:\nğ¹ğœƒ(ğ‘¥)=ğ‘¥/1000, hence the prediction is simply ğ‘˜=[ğ‘¥/10]. If the\nquery is 771, for example, the prediction of the model is ğ‘˜=77.\nThe correction information are Î”77=âˆ’41andğ¶77=2, which\nindicates that the result is -41 records ahead of the prediction,\nand the search area is of length 2. Therefore, the local search is\nperformed on the indexes of range [36,37].\nAlgorithm 1 shows how Shift-Table is used to accelerate query\nprocessing. The Shift-Table layer reduces the prediction error of\nthe model, but incurs an additional memory lookup.\n3.1 Querying non-indexed keys\nIf the query is on the indexed keys, the result is in range [ğ‘˜+\nÎ”ğ‘˜, ğ‘˜+Î”ğ‘˜+ğ¶ğ¾âˆ’1]. In Figure 5, for example, querying 771 and\n782 points to the correct range that contains the result. However,\nif the query is not among the indexed keys, then the query is\neither within the range, or in the position just after the range (at\ndata[ğ‘˜+Î”ğ‘˜+ğ¶ğ¾]. For example, in Figure 5, the record correspond-\ning to queries 778 and 781 is the same, though the aforementionedAlgorithm 1 Search with direct-mapped learned index\n1:procedure FIND_LOWER (ğ‘, model, Shift-Table)\n2: pos = model.predict(q)\n3: pos = Shift_Table.mapping[pos].startPoint\n4: range = Shift_Table.mapping[pos].range\n5: ifrange <linear_to_binary_threshold then\n6: pos = LinearSearch(start=data[pos],range)\n7: else\n8: pos = BinarySearch(start=data[pos],range)\n9: end if\n10: return pos\n11:end procedure\nmodel (ğ‘˜=[ğ‘/10]), maps 778 to range [36,37], and 781 to[38,39].\nIn both cases, however, the local search algorithm (either binary\nor linear search) within the range computes the correct position\nof the result (i.e., 38). Notably for ğ‘=778, a typical local search\nimplementation realizes that the query is greater than the largest\nvalue in range and returns the first index right after the range of\n[36,37], which is 38.\nAnother issue that can arise for non-indexed keys is when the\npredicted position ğ‘ƒğ‘˜has an empty partition that none of the\nindexed keys belongs to. In Figure 5, if the query is 15, then the\npredicted position is ğ‘˜=[15/10]=1, butğ‘ƒ1is empty because\nthe model does not predict position 1 for any of the indexed\nkeys. If the query is predicted to be in an empty partition, the\nresult is the first record in the next non-empty partition, e.g.,\nthe result of query=15 is record 3. To make the Shift-Table layer\nconsistent for the empty partitions, we put pseudo values for\nÎ”,ğ¶in the mapping layer such that they refer to the same range\nas the next existing partition. If ğ‘ƒğ‘˜âˆ…is an empty partition and\nğ‘ƒğ‘˜is the first non-empty partition after ğ‘ƒğ‘˜âˆ…, thenğ¶ğ‘˜âˆ…=ğ¶ğ‘˜and\nÎ”ğ‘˜âˆ…=Î”ğ‘˜+(ğ‘˜âˆ’ğ‘˜âˆ…). The pseudo Î”,ğ¶-values are depicted using\ndashed arrows in Figure 5.\n3.2 CDF and duplicate values\nIt should be noted that the empirical CDF function, i.e., ğ¹(ğ‘‹)=\nğ‘ƒ(ğ‘‹â‰¤ğ‘¥)does not exactly identify the result of a range query\non x. In this paper, we use the CDF (F(x)) notation as the index of\nthe result corresponding to ğ‘¥. We consider range queries of type\n(key<=query ), hence the CDF for a point ğ‘¥is the relative posi-\ntion of the first key in the indexed keys, as the range is scanned\ntowards the right. More precisely, we assume that ğ‘ğ¹(ğ‘¥0)=0\nandğ‘ğ¹(ğ‘¥ğ‘âˆ’1)=ğ‘âˆ’1(for the last key).\nA range learned index built for a specific comparison operator,\nsayğ‘¥â‰¤ğ‘, can be used for other operators ( â‰¥,>,, etc.) with a\nbrief left/right scan. However, if there are too many duplicates\nin the indexed data, then the the performance of the learned\nindex will be worse for queries that do not match the presumed\ndefinition of F(X). In such cases, it is more efficient to use the\nspecific definition of ğ¹(ğ‘¥)that reflects the position of the result of\nthe query in the most common type of constraint in the queries.\nFor example, if most of the queries are of type ğ‘¥>=ğ‘, thenğ¹(ğ‘¥)\nshould be defined such that ğ‘ğ¹(ğ‘¥)identifies the index of the last\nkey among the duplicate values.\n3.3 Building the Shift-Table layer\nAlgorithm 2 describes how the mapping of the Shift-Table layer\nis built. In the first stage, it computes the Î”,ğ¶values and updates\nfor the non-empty partitions, i.e., ğ‘ƒğ‘˜s for which at least one of\n\n01235...769770771782785â€¦8309990123...3536373839â€¦98990,32,11,10,1â€¦-41,1-41,2-40,2â€¦14,1...0,10123â€¦767778839901235...769770771782785â€¦8309990123...3536373839â€¦9899Records(only keys shown)Records(only keys shown)Correction layer...Figure 5: Shift-Table\nthe indexed keys is mapped to ğ‘˜. In the second stage, a backward\ntraversal is performed on the Shift-Table layer and the compute\nthe pseudo-values for the empty partitions (Algorithm 2, lines\n10â€“14). Starting from the last entry, a pseudo-partition has the\nsame count ( ğ¶) as the first non-empty partition on its right side,\nbut the shift Î”is adjusted so that they both point the the same\nregion for local search.\nThe computational complexity of building the Shift-Table layer\nisğ‘‚(ğ‘)Ã—ğ‘‚(ğ¹ğœƒ)to compute the drifts and updating the mapping,\nas it only traverses the data and the Shift-Table layer once. In\ncase that running the model is expensive, model executions can\nbe parallelized for faster execution.\nAlgorithm 2 Building the Shift-Table layer\n1:procedure Shift-Table_Build (model (ğ¹ğœƒ), data)\n2: Shift-Table = Array of tuples <Î”,ğ¶>, all set to zero\n3: for all xâˆˆdatado\n4:ğ‘ğ‘œğ‘ =ğ‘ğ¹(ğ‘¥) âŠ²Position of x (sec 3.2)\n5:ğ‘˜=[ğ‘ğ¹ğœƒ(data[i])]\n6: Î”=ğ‘ğ‘œğ‘ âˆ’ğ‘˜\n7: Shift_Table[k]. Î”= min(Shift_Table[ğ‘˜].Î”,Î”)\n8: Shift-Table[k].C += 1\n9: end for\n10: forğ‘˜â†ğ‘âˆ’1Â·Â·Â·0do\n11: ifShift_Table[k].C = 0 then âŠ²Empty partitions\n12: Shift_Table[k].C = Shift_Table[k-1].C\n13: Shift_Table[k]. Î”= Shift_Table[k-1]. Î”+1\n14: end if\n15: end for\n16: return Shift_Table\n17:end procedure\n3.4 Compressing the Shift-Table layer\nCorrecting the prediction of the model using the Shift-Table layer\ntakes a single DRAM lookup irrespective of the size of the index.\nHowever, it might be of interest to reduce the size of the layer.\nThe Shift-Table layer is an array of size N, containing <Î”,ğ¶>\ntuples. Further compression can be used to decrease the memory\nfootprint of the Shift-Table layer.\nOne approach is to keep a single parameter instead of the\n<Î”,ğ¶>tuples. In this regard, a predicted position ğ‘˜should be\nmapped to the key that is in the median point among the keys in\nğ‘ƒğ‘˜, which isÂ¯Î”ğ‘˜=\u0014\nÎ”ğ‘˜+ğ¶ğ‘˜\n2\u0015\n(3)\nTo correct using the Â¯Î”ğ‘˜values, the final position is computed\nasğ‘ğ‘œğ‘ =ğ‘˜+Â¯Î”ğ‘˜, which indicates where the search should be\nstarted without specifying the guaranteed range that should be\nsearched. Therefore, search algorithms that require the bound-\naries specified such as binary search cannot be used for local\nsearch. As discussed in section 2.4, linear or exponential search\ncan be used for local search without boundaries, but they are\nslightly slower if the error is considerable after the correction.\nA second approach that complements the first one, is to shrink\nthe size of the Shift-Table layer by merging nearby partitions.\nWe can extend the definition of P={ğ‘ƒ1,Â·Â·Â·,ğ‘ƒğ‘}to allow\npartitions that have a size of ğ‘€<ğ‘. We define ğ‘€partitions\nPğ‘€=\b\nğ‘ƒğ‘€\n1,Â·Â·Â·,ğ‘ƒğ‘€\nğ‘€\t\nwhere each partition is defined as:\nğ‘ƒğ‘€\nğ‘˜={ğ‘¥|[ğ‘€ğ¹ğœƒ(ğ‘¥)]=ğ‘˜} (4)\nSimilarly, Î”ğ‘€\nğ‘˜is the minimum \"move to the right\" shifts that\neach of the keys in ğ‘ƒğ‘€\nğ¾need:\nÎ”ğ‘€\nğ‘˜=min(ğ‘ğ¹(ğ‘¥)âˆ’[ğ‘ğ¹ğœƒ(ğ‘¥)])âˆ€ğ‘¥âˆˆğ‘ƒğ‘€\nğ‘˜(5)\nandğ¶ğ‘˜should be defined such that the boundary is valid for\nall keys inğ‘ƒğ‘€\nğ¾, which is:\nğ¶ğ‘€\nğ‘˜=max(ğ‘ğ¹(ğ‘¥)âˆ’([ğ‘ğ¹ğœƒ(ğ‘¥)]+Î”ğ‘€\nğ‘˜|              {z              }\nstart of the search window)) âˆ€ğ‘¥âˆˆğ‘ƒğ‘€\nğ‘˜(6)\nTo combine approaches to compact the Shift-Table layer, we\ncan use average drifts Â¯Î”ğ‘€\nğ‘˜instead of the <Î”ğ‘€\nğ‘˜,ğ¶ğ‘€\nğ‘˜>pairs:\nÂ¯Î”ğ‘€\nğ‘˜=ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°1\n|ğ‘ƒğ‘€\nğ‘˜|âˆ‘ï¸\nğ‘¥âˆˆğ‘ƒğ‘€\nğ‘˜(ğ‘ğ¹(ğ‘¥)âˆ’[ğ‘ğ¹ğœƒ(ğ‘¥)])ï£¹ï£ºï£ºï£ºï£ºï£ºï£»(7)\nand then use[ğ‘ğ¹ğœƒ(ğ‘¥)]+Â¯Î”ğ‘€\n[ğ‘€ğ¹ğœƒ(ğ‘¥)]as the corrected prediction.\nSuppose the same data as in Figure 5, but instead of a Shift-Table\nlayer of size N, we use only M=30 partitions. Table 1 shows how\na compact Shift-Table layer is built and used for correction, on\na portion of the index. We use the same model ( ğ¹ğœƒ=[ğ‘¥/1000]),\nhence the prediction is ğ‘ğ¹ğœƒ(ğ‘¥)=[0.1ğ‘¥], and the partition cor-\nresponding to a key is ğ‘ğ¹ğœƒ(ğ‘¥)=[0.03ğ‘¥]. All of the records\nfrom data[35..39] are assigned to the same partition ğ‘ƒ30\n23and their\npredictions are shifted 40 records backwards. Note that when\n\nğ‘€â‰ ğ‘, a partition does not specify a single point (or range) for\nall of the keys in the partition. Instead, the position of a key af-\nter correction depends on both ğ‘ğ¹ğœƒ(ğ‘¥)(prediction) and ğ‘€ğ¹ğœƒ(ğ‘¥)\n(partition number). For example, all keys belonging to ğ‘ƒ30\n23, i.e.,\ndata[35Â·Â·Â·39]have the same correction of Â¯Î”30\n23=âˆ’40, but their\nfinal predictions are different. Therefore, the correction error of\na compact Shift-Table layer is less than the number of elements\nin the partitions.\nTable 1: Illustration of Shift-Table with ğ‘€=30mapping\nentries on an index with ğ‘=100keys\nIndex 34 35 36 37 38 39 40 41\nkey (x) 752 769 770 771 782 785 820 830\nPredicted index= [0.1 x] 75 76 77 77 78 78 82 83\nError before correction -41 -41 -41 -40 -40 -39 -42 -42\nPartition (k) = [0.03 x] 22 23 24\nÂ¯Î”30\nğ‘˜-41 -40 -42\nPrediction after correction 34 36 37 37 38 38 40 41\nError after correction 0 1 1 0 0 -1 0 0\nThe drift of ğ‘ƒğ‘€\nğ‘˜, namely Â¯Î”ğ‘€\nğ‘˜is the index of the median key\namong the members of ğ‘ƒğ‘€\nğ‘˜. This means that if the key is predicted\nto be in the ğ‘˜â€™th partition (among the ğ‘€partitions), the local\nsearch is done around [ğ‘ğ¹ğœƒ(ğ‘¥)]+ Â¯Î”ğ‘€\nğ‘˜.\nUsing a Shift-Table layer of size ğ¾<ğ‘does not affect the\ncomplexity of building the layer, which is ğ‘‚(ğ‘)Ã—ğ‘‚(ğ¹ğœƒ)+ğ‘‚(ğ‘€).\nHowever, if the midpoint-values are used (correction without\nspecifying the boundary), it is possible to construct the map\nusing a sample of the indexed keys, which comes at the cost of\nthe accuracy. Using a sample of size ğ‘†<ğ‘, the layer can be built\ninğ‘‚(ğ‘†)Ã—ğ‘‚(ğ¹ğœƒ)+ğ‘‚(ğ¾)time.\nNonetheless, keep in mind that the Shift-Table layer is de-\nsigned for applications that favour latency to memory footprint,\nhence reducing the memory footprint of the Shift-Table layer\nby a large factor will limit its margin for improvement as the\nfine-grained details of the empirical CDF will be lost to some\nextent.\n3.5 Measuring the error\nSince the Shift-Table layer specifies a range for local search,\nthe notion of error is not trivial. However, we can use the es-\ntimates without range Â¯Î”), for which the correction picks the\nmedian value among the keys in the ğ‘ƒğ‘˜. The error for the keys\nin each partition isn\n[ğ¶ğ‘˜\n2],Â·Â·Â·,0,Â·Â·Â·,[ğ¶ğ‘˜\n2]o\nifğ¶ğ‘˜is odd, and\nn\n[ğ¶ğ‘˜\n2]âˆ’1,Â·Â·Â·,0,Â·Â·Â·,[ğ¶ğ‘˜\n2]o\nifğ¶ğ‘˜is even. The average error is\napproximately ğ¶ğ‘˜/4.\nIn a learned index without Shift-Table, the error is the distance\nbetweenğ¹(ğ‘¥)andğ¹ğœƒ(ğ‘¥). After correcting the model with the\nShift-Table, however, the error only depends on the ğ¶ğ‘˜values, i.e.,\na prediction error only occurs when [ğ¹ğœƒ(ğ‘¥)]predicts the same\nposition for multiple keys. Therefore, the local search range and\nthe error are combinations of multiple step functions over the\nğ‘ƒğ‘˜s withğ¶ğ‘˜>1.\nThe average error depends on the data distribution in the\nquery workload. If the queries are uniformly sampled from the\nkeys, then the average error is:\nÂ¯ğ‘’=1\n2ğ‘âˆ‘ï¸\nğ‘˜âˆˆPğ¶2\nğ‘˜(8)\n0.0 0.5 1.0\nkey 1e190.00.51.01.52.0position1e8pos estimated pos(a) Example data & model\n0.0 0.5 1.0 1.5 2.0\nposition 1e8101103105107prediction error (log-scale)\nModel Model + Shift-Table (b) Error\nFigure 6: Error correction using the Shift-Table layer\n3.6 Behaviour of the Shift-Table layer\nFigure 6 illustrates how the Shift-Table layer corrects the error of\na linear interpolation model on the OSMC data. While the model\nis too simple to capture the patterns in data, the Shift-Table layer\nalone is effective for correcting the predictions. While the average\nerror of the model is 28 million keys, Shift-Table reduces the error\nto only 129 keys.\nShift-Table corrects two types of error. First, when the model\nhas a considerable local bias, which means that ğ‘ğ¹(ğ‘¥)diverges\nsignificantly from ğ‘ğ¹ğœƒ(ğ‘¥)in a sub-range of the data distribution.\nThe second type of error is the fluctuations of the distribution\nbetween the nearby keys, for most of which the Shift-Table layer\nis very effective. The only type of error that can degrade the\nperformance of the Shift-Table layer is when there is a congestion\nof keys in a small sub-range of values, leading to many of the\nkeys being classified in a single layer, and hence having some\npartitions with high ğ¶ğ‘˜.\nThe behavior of the Shift-Table layer and its error estimate\nindicates that it can be effective in eliminating different types\nof errors that models have. One common type of error is the\nlocal bias in the model, i.e., when the error of the model, i.e.,\nğ‘ğ¹ğœƒ(ğ‘¥)âˆ’ğ‘ğ¹(ğ‘¥)has a considerable biasin some sub-ranges of\nthe distribution, meaning that the ğ¹andğ¹ğœƒdiverge at some point.\nThis happens when the model cannot capture the CDF in a local\nneighborhood. Table 2 shows that even if a single line is used as\na model, which has a huge bias in most areas of the distribution,\nthe Shift-Table layer can efficiently eliminate the huge bias of a\nfully linear model (a single line as a model), and reduces the error\nsignificantly such that the linear model outperforms all other\nalgorithms for the real-world datasets, as well as the uspr dataset\n(sparse uniformly-distributed integers) which has a significantly\nhigher variance than uniformly-distributed dense integers.\nAnother type of error that the Shift-Table layer eliminates is\nthe local variance in the data, which is the fluctuations of the\nvalues between nearby keys. This type of error is very common\nin real-world data. For example, the face, uspr, and uden datasets\nall follow a uniform distribution, but they have different local\nvariances, which is the amount of fluctuations in the nearby keys.\nThe uden dataset is very easy to model using the learned indexes\nand does not require a helping layer such as Shift-Table. The\nother two datasets, however, are very hard to model using the\nlearned index structures. This is because the Shift-Table model\ncan easily correct the fluctuations of values (different increments\nbetween each two points), as long as the model does not predict\na single record for a lot of nearby keys (resulting in a high ğ¶ğ‘˜\nvalue).\n\n3.7 Cost model of the Shift-Table layer\nThe accuracy of the model after correction with Shift-Table de-\npends on the cardinalities of the partitions ( ğ¶ğ‘–values). Ideally,\nif the records of each partition reside on a single cache line, the\nresults will be retrieved in a single memory lookup. The cost\nof local search, i.e., the mapping between the accuracy in each\npartition and the latency to do local search depends on the hard-\nware. As discussed in section 2.1, the latency of search for various\nranges can be measured by a micro-benchmark over non-cached\nregions with different sizes. Let ğ¿(ğ‘ )be the measured latency of\nnon-cached search over a range containing ğ‘ records. The latency\nfor looking up a key in a region of size ğ‘ isğ¿(ğ¶ğ‘˜). Assuming that\nthe queries have the same distribution as the data points, the\naverage lookup latency for the index is:\nLatency with Shift-Table = Latency (ğ¹ğœƒ)+1\nğ‘âˆ‘ï¸\nğ‘˜âˆˆPğ¶ğ‘˜ğ¿(ğ¶ğ‘˜)(9)\nThe cost model can also be used to estimate which of the\nlocal search algorithms should be used, by substituting in equa-\ntion 9 the local search cost of each local search algorithm, i.e.,\nğ¿(ğ‘ )mappings for linear, binary, and exponential search; and for\nand their different implementations. Branch-optimized binary\nsearch would be the natural choice if the Shift-Table model can\ndetermine the boundary (if using the Î”ğ‘˜,ğ¶ğ‘˜pairs), otherwise\neither linear or exponential search should be chosen based on\nthe latency estimate.\nTaking the cost of running the Shift-Table layer into account,\nwe should consider how much the correction improves the accu-\nracy of the learned index model and hence estimate the speedup.\nThe lookup time of the model without using the Shift-Table model\ncan be estimated once the Shift-Table model is built, without run-\nning a speedup benchmark. The model error for each key is\nÂ¯Î”ğ‘˜=Î”ğ‘˜+ğ¶ğ‘˜\n2, therefore the estimated runtime of the index\nwithout correction is:\nLatency without Shift-Table = Latency (ğ¹ğœƒ)+1\nğ‘âˆ‘ï¸\nğ‘˜âˆˆPğ¶ğ‘˜ğ¿(Â¯Î”ğ‘˜)\n(10)\n3.8 CDF model validity constraint\nThe correction layer requires the learned model to be a valid\nCDF function, i.e., ğ¹ğœƒ(ğ‘¥)should be monotonically increasing:\nğ‘¥ğ‘–>ğ‘¥ğ‘—âˆ’â†’ğ¹ğœƒ(ğ‘¥ğ‘–)>=ğ¹ğœƒ(ğ‘¥ğ‘—). Among our baselines, the RadixS-\nplines learned index always produces a valid (increasing) CDF,\nbut the RMI index does not always produce monotonically in-\ncreasing predictions. In RMI, for example, the CDF model might\ndecrease when using cubic models [ 29] or on the edge point\nbetween two models in the second-level. If ğ¹ğœƒ(ğ‘¥)is not mono-\ntonically increasing, then the correction layer could identify a\nrange that does not include the query result, because the values\nofğ‘¥for which the learned model predicts ğ‘˜â€™th record are not in\na contagious memory block.\nA learned index model that is non-monotonic can still use the\nShift-Table layer, as the output of the Shift-Table layer would still\npredict a position but it is not guaranteed that the position is in\nthe predicted range. Therefore, the local search algorithm should\ncheck if the query is in the predicted range and perform a search\noutside of the range. Another hack for non-monotonic model is\nto use the Â¯Î”midpoint-values instead of the Î”ğ‘˜,ğ¶ğ‘˜pairs, which\npredicts a location (instead of a range) to start the local search.If the Shift-Table layer uses the <Î”ğ‘€\nğ‘˜,ğ¶ğ‘€\nğ‘˜>pairs, it can deter-\nmine the range for local search and we can apply either linear or\nbinary search, depending on the error range. We do linear search\nif the range is smaller than a threshold (8 keys, in our experi-\nments), otherwise a binary search is performed. However, if it\nonly contains the average shift values ( Â¯Î”ğ‘˜, it predicts a position\nwithout specifying the boundaries that contain the record; hence\neither linear or exponential search can be performed depending\non the average error rate and performance objectives (average or\nworst-case latency).\n3.9 Tuning the system\nThe Shift-Table layer is optional and adds overhead to the search.\nTherefore, enabling Shift-Table is only worthwhile if it can even-\ntually accelerate the original learned index structure. An effective\nconfiguration of the index is a choice between 1) Using the model\nalone, 2) model + Shift-Table. Note that the Shift-Table layer is\noptional and can be deactivated with zero cost. The output of the\nmodel and the Shift-Table layer are of the same type and both\nrepresent a prediction of the records, hence if the Shift-Table\nlayer is disabled, we can easily use the model alone for prediction\nof the records.\nWhile tuning the system, the performance of each configu-\nration can be directly measured using performance tests, or by\nmeasuring the model error and then using the cost model of the\nShift-Table model on the bottom of the architecture (section 3.7).\nThe parameters of the architecture, i.e., the Shift-Table array\nsizeğ‘€and the parameters of the learned CDF model, can be\ntuned by computing the error estimate using Shift-Tableâ€™s cost\nmodel, or alternatively, by running a performance tests on the\nbuilt architecture. Our suggested default value for the Shift-Table\nlayer isğ‘€=ğ‘, because using a mapping layer that has the same\nnumber of entries as the keys will ensures that the layer can\nexhibit its ultimate effect to eliminate the signed error, and does\nnot have more latency compared to using smaller ğ‘€values.\nAn advantage of Shift-Table is that the learned model does\nnot need to be very accurate, as a correction will be applied\nanyway. Therefore, a more relaxed measure can be used instead\nof least-square error. In this paper, however, we do not learn the\nmodel w.r.t. the Shift-Table layer, for the sake of simplicity and\nto keep the Shift-Table layer detacheable (optional), preserving\nthe assumption that the Shift-Table layer can be disabled to free\nup memory space on run-time while the model can still be used.\nThe accuracy of the learned model also determines the size of\nthe entries of the Shift-Table layer. Each mapping entry should\nat most fit a Î”value of Î”ğ‘€ğ´ğ‘‹ , which is the maximum error of\nthe model. If, for example, the error is smaller than 216/2, then a\n16-bit integer ( short type) can be used.\n4 EVALUATION\nIn this section, we compare the performance of our proposed\nmethod with the SOSD benchmark1, which is a recent bench-\nmark for search on sorted data. The benchmark includes learned\nindexes, classical indexes, and no-index search algorithms.\nExperimental Setup. The algorithms are implemented in C++\nand compiled with GCC 9.1. The experiments are performed on\na system with 16 GB of memory and Intel Core i7-6700 (Skylake),\nwhich has four cores and is running at 3.4 GHz with 32 KB L1,\n256 KB L2, and 8 MB L3 caches. The operating system is Ubuntu\n18.04 with kernel version 4.15.0-65. In our setup, the LLC miss\n1https://github.com/learnedsystems/SOSD/tree/mlforsys19\n\npenalty measured by Intel Memory Latency Checker2is 36 ns,\nwhich is the minimum lookup time of an ideal index.\nNote that all data resides in main memory. The range index\nfinds the first indexed key that is equal to or bigger than the\nlookup key. Also, the keys on the physical layout are sorted\n(i.e., it is a clustered index), so that the entire result set of the\nrange query can be returned once the first key is found. Similar\nto [21,24], we only report the lookup time for the first result\nand do not include the scan times in our experiments because all\nindexes use the same layout for the data records.\nDatasets. For the sake of reproducibility, we used the same\ndatasets as in the SOSD benchmark, which contains four datasets\nsynthetically generated from known distributions and four real-\nworld ones. The synthetic data are generated from different distri-\nbutions, namely logn: lognormal distribution (0,2),norm : normal\ndistribution, uden : uniformly-generated dense integers, and uspr:\nuniformly-generated sparse integers. The real-world datasets\nareface: Facebook user IDs [ 40],amzn : book sale popularity\nfrom Amazon sales rank data3,osmc : uniform sample of Open-\nStreetMap locations4, and wiki: timestamps of edit actions on\nWikipedia articles5. All datasets contain 200M unsigned integers.\nImplementation details. Our experiments are based on the\nSOSD benchmark [ 21]. The baseline includes two learned indexes,\nnamely RadixSpline [ 32] (RS), which uses linear splines; and Re-\ncursive Model Index (RMI), which uses a hierarchy of models.\nNote that RMI has a choice of different models and SOSD [ 21]\nspecifically handpicked the best models for each of the datasets\nin the benchmark6. SOSD also includes no-index search algo-\nrithms such as binary search (BS), linear interpolation search (IS),\nand the recently suggested non-linear triple-point interpolation\n(TIP) [ 40]. We also compare against algorithmic index structures\nsuch as ART: Adaptive Radix Tree [ 25], FAST [ 20], RBS (Radix\nBinary Search): a two-stage algorithm in which a radix struc-\nture that maps a fixed-length key prefix to the range of all keys\nhaving that prefix and then a binary search is performed on the\nrange [ 21], and STX implementation of B+tree [ 1]. Finally, we\nincluded four On-the-fly search algorithms, namely BS: Binary\nsearch (STL implementation), TIP: three-point interpolation [ 40],\nInterpolation search, which is similar to binary search but uses\ninterpolated positions in each iteration, and IM: Interpolation as\na Model : a dummy model that interpolates the key between the\nminimum and maximum value of the keys and then performs\nexponential search around the predicted key.\nThe experiments use either 32- or 64-bit unsigned integer\nIDs for the key (depending on the dataset), and 64-bytes for the\npayload.\n4.1 The SOSD benchmark\nTo test the effectiveness of the suggested layers compared to\nlearned indexes, we use a simple interpolation model (IM), i.e.,\nğ¹ğœƒ(ğ‘¥)=(ğ‘¥âˆ’ğ‘šğ‘–ğ‘›ğ‘‰ğ‘ğ‘™)/(ğ‘šğ‘ğ‘¥ğ‘‰ğ‘ğ‘™âˆ’ğ‘šğ‘–ğ‘›ğ‘‰ğ‘ğ‘™). Such a dummy model\nis deliberately chosen to purely delegate the burden of data mod-\nelling to the correction layers.\n2https://software.intel.com/en-us/articles/intelr-memory-latency-checker\n3https://www.kaggle.com/ucffool/amazon-sales-rank-data-for-print-and-kindle-books\n4https://aws.amazon.com/public-datasets/osm\n5https://dumps.wikimedia.org\n6The architectures and parameters of the RMI models used for each dataset is spec-\nified at https://github.com/learnedsystems/SOSD/blob/mlforsys19/scripts/build_\nrmis.shThe Shift-Table layer has the same number of entries as the\nactual data, i.e., ğ‘€=ğ‘. We followed the tuning procedure dis-\ncussed in section 3.9: we start from the model (IM and RS) and\nconsequently evaluate IM+Shift-Table and RS+Shift-Table. The\ncost of running the Shift-Table layer is around 40ns, which pays\noff by reducing the prediction error and thus lookup time. There-\nfore, based on the cost model of the Shift-Table layer (Section 3.7)\nand the error-to-latency micro-benchmark (Figure 2a), we should\nnot add the Shift-Table layer if the error before adding the con-\nfiguration is less than a threshold (10 records), or 2) the error of\nthe index after adding the Shift-Table layer does not decrease\nby a factor of 10 (roughly equivalent to the 50-nanoseconds\nlatency the additional layer, according to the error-to-latency\nmicro-benchmark).\nTable 2 compares the lookup times (nanoseconds per lookup)\nof the baseline algorithms with our dummy interpolation model\n(IM), and the two corrected versions, i.e., IM+Shift-Table and\nRS+Shift-Table. Note that ART does not support data with dupli-\ncate keys, and FAST does not support 64-bit keys. Also, interpo-\nlation search (IS) takes too much time on some datasets, because\nthe execution time of interpolation search highly depends on the\nuniformity of data distribution, varying from O(loglogN) + O(1)\niterations on uniform distributions, to O(N) iterations for very\nskew ones [40].\nFor the synthetic datasets, the difficulty of the datasets for\nour dummy linear interpolation model varies from very easy\n(uden64) to extremely hard (logn64). While the Shift-Table layer\nsignificantly improves a dummy layer on non-uniform data dis-\ntributions, it cannot outperform the learned index models. This\nis not surprising, as all synthetic datasets (uniform, lognormal,\nand uniform) have a pattern derived from continuously differ-\nentiable density functions, hence the distribution is similar to\na straight line on smaller sub-ranges as we \"zoom in\" the data\ndistribution (e.g., see Figure 3c). Therefore, a learned index struc-\nture composed of linears at the bottom (including both RMI and\nRS) can effectively model the distribution using a very compact\nrepresentation.\nFor the real-world data, however, the fluctuations in data se-\nverely affect both RMI and RS learned indexes. The Shift-Table\nlayer, effectively corrects a highly inaccurate dummy IM model,\nsuch that it outperforms the RMI learned index by 1.5X to 2X on\nall datasets, while RS falls behind both. Keep in mind that RMI\nrequires to be tuned with the best architecture and parameters,\nwhile Shift-Table does not require a manual training process and\ncan even work with a simple model such as IM that is not trained,\nand yet deliver a lower latency.\nFigure 7 shows the average build times of the indexes, along\nwith the standard deviation bars indicating how the build time\nvaries for different distributions. Please note that the RMI imple-\nmentation used in the SOSD benchmark needs to be compiled for\nfaster retrievals, however we did not include RMIâ€™s extra over-\nhead for compiling the code and only reported the build time.\nIM+Shift-Table, the winner method latency-wise, also takes ei-\nther the same or even less build time than the competing learned\nindexes.\n4.2 Explaining the performance\nThe latencies reported in Table 2 present the fastest configuration\nfor each learned index. In this section, we present the details of the\ntuning process to see the optimum performance of each learned\nindex.\n\nTable 2: Comparison of lookup times (nanoseconds per lookup) with the SOSD benchmark. The red box indicates the base\nmodel (IM) and the enhanced versions.\nDatasetARTFASTRBSB+treeBSTIPISIMIM+ Shift-TableRMIRSRS+ Shift-Tablelogn32N/A230385375624551N/A138416673.983.9143.5norm32173197267390655671N/A147988.251.560.396.4uden3299.419623538965412632.338.667.538.147.872.3uspr32N/A19823039065429832142589.7141166153.5logn64238N/A622427674377N/A1075376132109151.0norm64214N/A317427672705N/A161588.651.761.893.2uden64104N/A25542867014234.840.467.439.847.971.8uspr64216N/A24442767332933847292.8145182154.6amzn32N/A2082433936585693228152499.5185236110.8face32179203238388654717792861103213310142.8amzn64N/AN/A28442867657835101575105189238119.3face64290N/A2574276719251257918149247344204.1osmc64N/AN/A4104286754617N/A1462194297339177.2wiki64N/AN/A2714376867675867168794.2172191124.1Algorithmic indexesOn-the-fly searchSyntheticReal-worldLearned indexes\nARTB+tree FAST RBS RMI RS\nRS+ShiftTable IM+ShiftTable101102103104Average index build time (ms)\nFigure 7: Build times (average time for all datasets)\nFor those indexes that have a parameter affecting the index\nsize (such as the branching factor in B+tree, and the number of\nradix bits in ART, RS, and RBS), the performance can be tuned\nby evaluating the latency for different index sizes.\nFigure 8 shows the latencies of the indexes for the face64 and\nosmc64 datasets, along with the average Log2 error, CPU instruc-\ntions, and L1/LLC cache misses. IM+Shift-Table and RS+Shift-\nTable achieve faster lookup times on both datasets. For most in-\ndexes, except RMI and RBS, the latency does not improve beyond\na certain optimum index size, after which the latency increases\nagain. RBS has a much larger latency than both [IM/RS]-Shift-\nTable indexes of the same size, and extrapolating the RMI laten-\ncies also suggest that if we could extend RMI size to 1400MB\n(equal to Shift-Tableâ€™s size), it could not achieve a game-changing\nperformance on either of the datasets. Note that we could not run\nRMI with larger models because RMI embeds the parameters into\nthe code, and the compile times for models larger than 400MB\nwere astonishingly high.Average Log2 errors indicate the average number of iterations\nin binary search for the last-mile search stage. Larger models\nresult in lower Log2 errors in all indexes and lead to faster last-\nmile search, however, once the model exceeds the LLC cache sizes,\ncache-miss rate increases (when running the model), and hence\nthe prediction time worsens. For RS, ART, and B+tree, the cache\nmisses and extra overhead of running the models increases either\nthe number of instruction, the cache misses, or both, enough\nto prevent the index from improving latency by increasing the\nfootprint.\n4.3 Layer size\nAs discussed in section 3.4, the Shift-Table layer can be com-\npressed by merging multiple entries, hence reducing its footprint.\nFigure 9 shows the effect of the Shift-Table layer size on lookup\ntime and prediction error. Shift-Table can operate in two modes:\nR-1: a full layer containing <Î”ğ‘€\nğ‘˜,ğ¶ğ‘€\nğ‘˜>pairs similar to Figure 5\nthat indicates the exact range for local search (hence enabling\nbinary search); and S-X: a compressed single-entry map similar\nto Table 1 containing one Â¯Î”ğ‘€\nğ‘˜entry perğ‘‹records. Thus, S-X\ncontainsğ‘€=ğ‘/ğ‘‹entries; and the memory footprint of S-1 is\nhalf the size of R-1.\nThe error of the S-1 Shift-Table is slightly more than that of\nR-1. This is due to the fact that S-1 is designed to draw boundaries\nfor binary search; hence it always points to the first record of\neach partition; while R-1 always points to the middle of the parti-\ntion and almost half the error of S-1. Performance-wise, however,\nS-1 always has the lowest latency, because its boundaries for the\nlast-mile search operation do not need to be discovered using\nadditional boundary-detection algorithms such as exponential\nsearch. As expected, compressing the Shift-Table by allocating\none entry per ğ‘‹records increases the error and hence degrades\n\n1051082004006008001000Lookup time (ns)\nLookup time , face64\n10510805101520Log2 error\nLog2 error , face64\n1051082Ã—1023Ã—1024Ã—102# CPU Instructions\nInstructions , face64\n10510820406080LLC cache misses\nL1-misses , face64\n1051080.10.20.30.4\nLLC-misses , face64\n104106108\nIndex size5001000Lookup time (ns)\nLookup time , osmc64\n105108\nIndex size01020Log2 error\nLog2 error , osmc64\n105108\nIndex size2Ã—1023Ã—1024Ã—102# CPU Instructions\nInstructions , osmc64\n105108\nIndex size255075100125LLC cache misses\nL1-misses , osmc64\n105108\nIndex size0.10.20.30.4\nLLC-misses , osmc64RS RMI ART B+tree RBS IM+ShiftTable RS+ShiftTableFigure 8: Analysis of the effect of index size on performance\namzn64 face32 logn32 norm64 osmc64 uden32 uspr32 wiki64102103Lookup time (ns)R 1\nS 1S 10\nS 100S 1000\nWithout Shift-Table\n(a) Latency\namzn64 face32 logn32 norm64 osmc64 uden32 uspr32 wiki64101103105107Avg error (records)\n(b) Error\nFigure 9: Analysis of the effect of Shift-Table layer size\nthe performance. This is due to the fact that with higher com-\npression ratios, the ability of Shift-Table to \"memorize\" the fine-\ngrained details of the data distribution degrades due to the loss\nof information after merging.\n5 RELATED WORK\nOn-the-fly search on sorted data A fundamental problem that\nis studied for decades is how to find a key among a sorted list\nof items. The classic approach is binary search and numerous\nextensions have been suggested to improve it for special cases,\nmost notably interpolation search [ 33] and exponential search [ 3].For data distributions that are close to uniform, interpolation-\nsearch is shown to be very effective [ 13,34,40]. Due to the\ngrowing gap between CPU power and memory latency in the past\ndecade, more advanced interpolation techniques such as three-\npoint interpolation are becoming viable on modern hardware [ 40].\nExponential search enables binary search over an unbounded list.\nExponential search is also extensively used in learned indexes\nwhen the key is more likely to be near a \"guessed\" location, but\na guaranteed boundary around the guessed point that contains\nthe data is not known [7, 24, 31].\nRange indexes An alternative to on-the-fly binary search\nover sorted data is to keep the data in an index structure. Nonethe-\nless, indexes that are built to answer range queries (such as B-\ntrees) are similar to the binary search in that they need to keep\nthe data sorted internally. Common index structures for range\nindex include skiplists, B+trees, and radix-trees. The B+-tree is\ncache-efficient, but requires pointer chasing, which incurs multi-\nple cache misses [ 14]. There has been a tremendous effort to make\nbinary search trees and B+-trees efficient on modern hardware.\nFor example, FAST [ 20] organizes tree elements efficiently to ex-\nploit modern hardware features such as the cache line and SIMD.\nAnother common solution is to use compression techniques on\nthe indexed keys, most notably as a radix-tree. Modern radix\ntrees exploit hardware-efficient heuristics for fitting a distribu-\ntion in memory (usually by building a heuristically-optimized\ncompressed trie), such as adaptive radix index (ART) [ 5,25], and\nSuccinct Range Filter (SuRF) [ 43]. Skiplist is specifically efficient\nfor concurrent updates workloads [39, 42].\nLearned index structures Learned range indexes [ 7,12,24,\n28,32] have recently been suggested as an alternative to range\nindexes. In this approach, a model is trained from the data with\nthe intent of capturing the data distribution and processing the\nqueries more efficiently. We refer to the paper by Kraska et\nal. [24], which introduced the idea of the learned index. In a\nlearned index, the CDF of the key distribution is learned by fit-\nting a model, and the learned model is subsequently used as a\nreplacement of the index (B+-trees or similar) for finding the\nlocation of the query results on the storage medium. Index learn-\ning frameworks such as the RMI model [ 24,29] can learn arbi-\ntrary models [ 29], although a further theoretical study [ 9] as\nwell as a recent experimental benchmark [ 21] have shown that\n\nsimple model like linear splines are very effective for datasets.\nSpline-based learned indexes include Piecewise Geometric Model\nindex (PGM-index) [ 11], Fiting-tree [ 12], Model-Assisted B-tree\n(MAB-tree) [ 18], Radix-Spline [ 22], Interpolation-friendly B-tree\n(IF-Btree) [ 17] and some others [ 28,38]. We refer to [ 10] for an\nextensive comparison of learned indexes. Recently, there has\nbeen numerous theoretical works [ 4,26,36,37] on learned in-\ndexes. Also, numerous efforts have been made to handle practi-\ncal challenges around using a learned index, including update-\nhandling [ 7,16] and designing a learned DBMS [ 23]. The idea of\nusing a model of the data to boost an existing algorithmic index\nhas been the center of focus in the past few years [ 14,16,18,35].\nIn the multivariate area, learning from a sample workload has\nalso shown interesting results [ 8,19,27,31]. Aside from the main\ntrend in learned indexes, which is on range indexing, machine\nlearning has also inspired other indexing and retrieval tasks. This\nincludes bloom filters [ 6,30], inverted indexes [ 41], computing\nlist intersections [ 2], and multidimensional indexing on datasets\nwith correlated attributes [15].\n6 CONCLUSION AND FUTURE WORK\nLearning and modeling data distributions via machine learn-\ning approaches is a great idea for managing and analyzing data\nmanagement systems. However, the approaches and objective\nfunctions that are common in machine learning problems are\nnot necessarily optimal choices when the ultimate target is per-\nformance improvement. Instead of pushing machine learning\nmodel algorithm to its limits for highly accurate modeling of\ndata distributions, it is more efficient if we only use ML models\nto approximate the high-level, generalizable \"patterns\" in data\ndistribution (the holistic shape), and handle the fluctuations and\nfine-grained details of the distribution using a more hardware-\nefficient approach, outperforms learned models as well as algo-\nrithmic index structures even if a simple or somewhat dummy\nmodel such as min/max linear interpolation is used. The Shift-\nTable layer is effective in learning almost all distributions even\nwithout using models that require training from data, and takes\nonly a single pass over the data points to build the layer. Our\nresults show that even a simple linear model equipped with the\nShift-Table enhancement layer outperforms trained and tuned\nlearned indexes by 1.5X to 2X on real-world datasets.\nOur current work only considers read-only workloads. We\nleave it as future work to adapt Shift-Table with workloads having\nupdates. One idea is to capture the drifts in data distribution using\nupdate-tracking segments [ 16], and use Fenwick trees to estimate\nand correct the drifts in both the model and the Shift-Table.\nREFERENCES\n[1] STX. B+Tree C++ Template Classes. http://panthema.net/2007/stx-btree.\n[2]Naiyong Ao, Fan Zhang, Di Wu, Douglas S Stones, Gang Wang, Xiaoguang\nLiu, Jing Liu, and Sheng Lin. 2011. Efficient parallel lists intersection and index\ncompression algorithms using graphics processing units. VLDB Endowment 4,\n8 (2011), 470â€“481.\n[3]Jon Louis Bentley and Andrew Chi-Chih Yao. 1976. An almost optimal algo-\nrithm for unbounded searching. Information processing letters 5 (1976).\n[4]Rasmus Bilgram and Per Hedegaard Nielsen. 2019. Cost Models for Learned\nIndex with Insertions . Technical Report. University of Aalborg.\n[5]Robert Binna, Eva Zangerle, Martin Pichl, GÃ¼nther Specht, and Viktor Leis.\n2018. HOT: a height optimized Trie index for main-memory database systems.\nInSIGMOD . 521â€“534.\n[6]Zhenwei Dai and Anshumali Shrivastava. 2019. Adaptive learned Bloom filter\n(Ada-BF): Efficient utilization of the classifier. arXiv:1910.09131 (2019).\n[7]Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\nDavid Lomet, and Tim Kraska. 2020. ALEX: An Updatable Adaptive Learned\nIndex. In SIGMOD . 969â€“984.[8]Mohamad Dolatshah, Ali Hadian, and Behrouz Minaei-Bidgoli. 2015. Ball*-\ntree: Efficient Spatial Indexing for Constrained Nearest-neighbor Search in\nMetric Spaces. arXiv:cs.DB/1511.00628\n[9]Paolo Ferragina, Fabrizio Lillo, and Giorgio Vinciguerra. 2020. Why are learned\nindexes so effective?. In ICML , Vol. 119. PMLR.\n[10] Paolo Ferragina and Giorgio Vinciguerra. 2020. Learned data structures. Recent\nTrends in Learning From Data (2020), 5â€“41.\n[11] Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-index: a fully-\ndynamic compressed learned index with provable worst-case bounds. VLDB\nEndowment 13, 8 (2020), 1162â€“1175.\n[12] Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and\nTim Kraska. 2019. FITing-Tree: A Data-aware Index Structure. In SIGMOD .\n1189â€“1206.\n[13] Goetz Graefe. 2006. B-tree indexes, interpolation search, and skew. In DaMoN .\n[14] Goetz Graefe and Harumi Kuno. 2011. Modern B-tree Techniques. Foundations\nand Trends in Databases 3, 4 (2011), 203â€“402.\n[15] Ali Hadian, Behzad Ghaffari, Taiyi Wang, and Thomas Heinis. 2021. COAX:\nCorrelation-Aware Indexing on Multidimensional Data with Soft Functional\nDependencies. arXiv:cs.DB/2006.16393\n[16] Ali Hadian and Thomas Heinis. 2019. Considerations for handling updates in\nlearned index structures. In AIDM .\n[17] Ali Hadian and Thomas Heinis. 2019. Interpolation-friendly B-trees: Bridging\nthe Gap Between Algorithmic and Learned Indexes. In EDBT .\n[18] Ali Hadian and Thomas Heinis. 2020. MADEX: Learning-augmented Algo-\nrithmic Index Structures. In AIDB .\n[19] Ali Hadian, Ankit Kumar, and Thomas Heinis. 2020. Hands-off Model Integra-\ntion in Spatial Index Structures. In AIDB .\n[20] Changkyu Kim, Jatin Chhugani, Nadathur Satish, Eric Sedlar, Anthony D\nNguyen, Tim Kaldewey, Victor W Lee, Scott A Brandt, and Pradeep Dubey.\n2010. FAST: fast architecture sensitive tree search on modern CPUs and GPUs.\nInSIGMOD . 339â€“350.\n[21] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons\nKemper, Tim Kraska, and Thomas Neumann. 2019. SOSD: A Benchmark for\nLearned Indexes. NeurIPS Workshop on Machine Learning for Systems (2019).\n[22] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons\nKemper, Tim Kraska, and Thomas Neumann. 2020. RadixSpline: a single-pass\nlearned index. In AIDM .\n[23] Tim Kraska, Mohammad Alizadeh, Alex Beutel, Ed H. Chi, Jialin Ding, Ani\nKristo, Guillaume Leclerc, Samuel Madden, Hongzi Mao, and Vikram Nathan.\n2019. SageDB: A Learned Database System. In CIDR .\n[24] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe case for learned index structures. In SIGMOD . 489â€“504.\n[25] Viktor Leis, Alfons Kemper, and Thomas Neumann. 2013. The adaptive radix\ntree: ARTful indexing for main-memory databases. In ICDE . 38â€“49.\n[26] Pengfei Li, Yu Hua, Pengfei Zuo, and Jingnan Jia. 2019. A Scalable Learned\nIndex Scheme in Storage Systems. arXiv:1905.06256 (2019).\n[27] Pengfei Li, Hua Lu, Qian Zheng, Long Yang, and Gang Pan. 2020. LISA: A\nLearned Index Structure for Spatial Data. In SIGMOD .\n[28] Anisa Llavesh, Utku Sirin, Robert West, and Anastasia Ailamaki. 2019. Ac-\ncelerating B+tree Search by Using Simple Machine Learning Techniques. In\nAIDB .\n[29] Ryan Marcus, Emily Zhang, and Tim Kraska. 2020. CDFShop: Exploring and\nOptimizing Learned Index Structures. In SIGMOD . 2789â€“2792.\n[30] Michael Mitzenmacher. 2018. A Model for Learned Bloom Filters and Related\nStructures. arXiv:1802.00884 (2018).\n[31] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020.\nLearning Multi-dimensional Indexes. In SIGMOD . 985â€“1000.\n[32] Thomas Neumann and Sebastian Michel. 2008. Smooth interpolating his-\ntograms with error guarantees. In BNCOD . Springer, 126â€“138.\n[33] W Wesley Peterson. 1957. Addressing for random-access storage. IBM journal\nof Research and Development 1, 2 (1957), 130â€“146.\n[34] CE Price. 1971. Table lookup techniques. Comput. Surveys 3, 2 (1971), 49â€“64.\n[35] Wenwen Qu, Xiaoling Wang, Jingdong Li, and Xin Li. 2019. Hybrid Indexes\nby Exploring Traditional B-Tree and Linear Regression. In WEBIST . 601â€“613.\n[36] Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, and HervÃ© JÃ©gou.\n2018. Deja Vu: an empirical evaluation of the memorization properties of\nConvNets. arXiv:1809.06396 (2018).\n[37] Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, and HervÃ© JÃ©gou.\n2019. Spreading vectors for similarity search. In ICLR .\n[38] Naufal Fikri Setiawan, Benjamin IP Rubinstein, and Renata Borovica-Gajic.\n2020. Function Interpolation for Learned Index Structures. In ADC . 68â€“80.\n[39] Stefan Sprenger, Steffen Zeuch, and Ulf Leser. 2016. Cache-sensitive skip list:\nEfficient range queries on modern cpus. In DaMoN . Springer, 1â€“17.\n[40] Peter Van Sandt, Yannis Chronis, and Jignesh M Patel. 2019. Efficiently Search-\ning In-Memory Sorted Arrays: Revenge of the Interpolation Search?. In SIG-\nMOD . 36â€“53.\n[41] Wenkun Xiang, Hao Zhang, Rui Cui, Xing Chu, Keqin Li, and Wei Zhou. 2018.\nPavo: A RNN-Based Learned Inverted Index, Supervised or Unsupervised?\nIEEE Access 7 (2018), 293â€“303.\n[42] Zhongle Xie, Qingchao Cai, HV Jagadish, Beng Chin Ooi, and Weng-Fai Wong.\n2017. Parallelizing skip lists for in-memory multi-core database systems. In\nICDE . IEEE, 119â€“122.\n[43] Huanchen Zhang, Hyeontaek Lim, Viktor Leis, David G Andersen, Michael\nKaminsky, Kimberly Keeton, and Andrew Pavlo. 2018. Surf: Practical range\nquery filtering with fast succinct tries. In SIGMOD . 323â€“336.",
  "textLength": 69454
}