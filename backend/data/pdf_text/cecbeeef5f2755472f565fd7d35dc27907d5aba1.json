{
  "paperId": "cecbeeef5f2755472f565fd7d35dc27907d5aba1",
  "title": "NeurBench: Benchmarking Learned Database Components with Data and Workload Drift Modeling",
  "pdfPath": "cecbeeef5f2755472f565fd7d35dc27907d5aba1.pdf",
  "text": "NeurBench: Benchmarking Learned Database Components with\nData and Workload Drift Modeling\nZhanhao Zhao1, Haotian Gao1, Naili Xing1, Lingze Zeng1, Meihui Zhang2,\nGang Chen3, Manuel Rigger1, Beng Chin Ooi1\n1National University of Singapore2Beijing Institute of Technology3Zhejiang University\n{zhzhao, gaohaotian, xingnl, lingze, rigger, ooibc}@comp.nus.edu.sg meihui_zhang@bit.edu.cn cg@zju.edu.cn\nABSTRACT\nLearned database components, which deeply integrate machine\nlearning into their design, have been extensively studied in recent\nyears. Given the dynamism of databases, where data and workloads\ncontinuously drift, it is crucial for learned database components\nto remain effective and efficient in the face of data and workload\ndrift. Adaptability, therefore, is a key factor in assessing their prac-\ntical applicability. However, existing benchmarks for learned data-\nbase components either overlook or oversimplify the treatment\nof data and workload drift, failing to evaluate learned database\ncomponents across a broad range of drift scenarios. This paper\npresents NeurBench , a new benchmark suite that applies measur-\nable and controllable data and workload drift to enable systematic\nperformance evaluations of learned database components. We quan-\ntify diverse types of drift by introducing a key concept called the\ndrift factor. Building on this formulation, we propose a drift-aware\ndata and workload generation framework that effectively simu-\nlates real-world drift while preserving inherent correlations. We\nemploy NeurBench to evaluate state-of-the-art learned query opti-\nmizers, learned indexes, and learned concurrency control within\na consistent experimental process, providing insights into their\nperformance under diverse data and workload drift scenarios.\n1 INTRODUCTION\nDatabase systems are increasingly embracing machine learning\n(ML) techniques in their design. Key database components, such\nas query optimizers, indexes, and concurrency control (CC), are\nbeing upgraded with ML models to boost performance. For ex-\nample, learned query optimizers [ 39,66] treat query optimization\nas an ML problem to obtain more efficient query plans. Learned\nindexes [ 13,23] replace traditional index structures with ML mod-\nels that predict the positions of query keys, enabling faster key\nsearch. Similarly, learned CC algorithms [ 53] predict optimal CC\nactions ( e.g., locking strategies and blocking timeouts) for different\noperations to improve transaction throughput.\nOne major challenge faced by learned database components is\nthe dynamic nature of databases, commonly referred to as data and\nworkload drift [24,25,61]. Specifically, the data stored in databases\nconstantly changes over time due to inserts, updates, and deletions,\nwhile workloads also evolve, with drift in query patterns [17] and\nquery/transaction arrival rates [ 34]. In contrast, ML models typ-\nically derive insights from static datasets, and this fundamental\ndifference between the paradigms of databases and ML can cause\nlearned database components to be ineffective and outdated in the\nface of data and workload drift. For instance, as data is updated\nover time, learned query optimizers may lose precision becausethe knowledge they initially learned for generating query plans\nbecomes obsolete. Similarly, learned CC algorithms may choose\nactions that were once optimal but no longer suit the current work-\nload with drifted transaction arrival rates.\nAchieving adaptive learned database components that remain\neffective regarding data and workload drift is crucial. Early ap-\nproaches [ 23,40,51] typically overlook the adaptability issue or sug-\ngest that it can only be addressed through complete model retrain-\ning. Consequently, when models become outdated, costly manual\nintervention and retraining are necessary, making them inefficient\nin dealing with continuous data and workload drift. More recently,\nimproving adaptability has been gaining significant attention in\nour community. For example, specific mechanisms [ 25,31,61,64]\nare proposed to automatically detect drift and trigger model re-\ntraining or fine-tuning for learned database components. Moreover,\nrecent research [ 52] utilizes advanced deep neural networks with\ninherently stronger generalizability to build more robust learned\ndatabase components [ 32]. In tandem with the advancement of ML\ntechniques, we envision continued efforts to enhance the adaptabil-\nity of learned database components.\nThe rapid developments in adaptive learned database compo-\nnents naturally raise a pertinent question: how well do existing\nlearned database components perform under data and workload\ndrift, and how do their key designs impact performance under differ-\nent drift scenarios? To be able to answer this question meaningfully,\nwe need comprehensive empirical evaluations and benchmarks that\nassess learned database components in the context of various data\nand workload drift. Thus far, several efforts have been made to eval-\nuate representative learned database components, such as learned\nquery optimizers [ 12,18,24,27] and learned indexes [ 48,59]. How-\never, their treatment of data and workload drift is rather simplistic\nand arbitrary. For example, data drift is simulated by directly delet-\ning half of a table [ 24,27], while workload drift is modeled by\nrandomly sampling queries from a predefined query set [ 61]. In\nparticular, there are two major limitations: 1) They lack effective\ncontrol over data and workload drift. Given that real-world drift\noccurs continuously and incurs various drift scenarios, this draw-\nback prevents in-depth evaluation of learned database components\nunder varying degrees of drift, and makes it challenging to identify\nthe exact threshold at which these components fail to remain ef-\nfective. 2) Their drift simulations typically overlook the inherent\ncorrelations in real-world data and workloads. For example, in an\ne-commerce database, attributes such as user demographics ( e.g.,\nage, location) are often correlated with purchasing behavior ( e.g.,\nproduct category, spending patterns). Evaluations under unreal-\nistic drift scenarios may not accurately reflect the componentsâ€™\nperformance in practical applications.arXiv:2503.13822v2  [cs.DB]  25 Mar 2025\n\nZhanhao Zhao, Haotian Gao, Naili Xing, Lingze Zeng, Meihui Zhang,\nGang Chen, Manuel Rigger, Beng Chin Ooi\nTo address these limitations, we design a comprehensive bench-\nmark suite that can simulate real-world data and workload drift,\nthereby enabling in-depth evaluations of different learned database\ncomponents under diverse drift scenarios. Accordingly, we have\nto fulfill two key requirements. First, it is essential to uniformly\nand effectively quantify data and workload drift. However, given\nthe various drift in databases, such as data drift, query pattern\ndrift, etc., consistently defining and measuring all drift scenarios is\nnon-trivial. Second, real-world data and workloads exhibit intricate\nrelationships among the attributes, and therefore, introducing spe-\ncific drift while preserving these correlations is challenging. The\ndrift simulation must effectively capture inherent correlations to\nensure that the drifted data and workloads remain representative\nof real-world scenarios.\nIn this paper, we present NeurBench , a novel benchmark suite\ndesigned to evaluate various learned database components under\ncontrollable data and workload drift. Unlike existing benchmarks,\nNeurBench standardizes the evaluation process by introducing sys-\ntematic drift factor modeling. In particular, we define a concept\ncalled the drift factor to quantify data and workload drift in a unified\nmanner. We model all types of drift as changes in underlying prob-\nabilistic distributions and measure drift as the distance between\nthese distributions. Leveraging this concept, NeurBench generates\ndata or workloads that exhibit a specified degree of drift from the\noriginal, offering fine-grained control over drift intensity. We then\nutilize the generated and original data and workloads to construct\nmeaningful drift scenarios, ensuring comparable evaluations across\ndiverse learned database components.\nWe propose a drift-aware data and workload generation frame-\nwork capable of introducing specific drift to the original data and\nworkloads while preserving their inherent correlation. Specifically,\nwe first utilize a base generative model to learn the underlying\ndistribution of the real data and workloads, and then train an in-\ndependent drifter module to guide the base model in generating\ndrifted data and workloads. The base generative model is built on\nthe Denoising Diffusion Probabilistic Model (DDPM) [ 19]. Based on\nthe proposed drift factor, we ensure the drifter effectively controls\nthe generation process to introduce targeted drift while maintaining\nfeature correlations. By decoupling the drifter from the base model,\nour framework generalizes across various types of drift, eliminating\nthe need to train a specialized model for each drift scenario.\nIn summary, we make the following contributions:\nâ€¢We introduce NeurBench , a practical benchmark suite that fa-\ncilitates the evaluation of learned database components under\ncontrollable data and workload drift.\nâ€¢We propose a unified concept called the drift factor to model\ndata and workload drift, enabling the systematic performance\nevaluation of database components under diverse drift scenarios.\nâ€¢We propose a drift-aware data and workload generation frame-\nwork that effectively synthesizes drifted data and workloads\naccording to a specified drift factor while preserving their inher-\nent correlations.\nâ€¢We conduct extensive experiments to demonstrate the effective-\nness of NeurBench in generating drifted data and workload, and\nuseNeurBench to evaluate start-of-the-art learned query op-\ntimizers, learned indexes, and learned CC. We report generaltrends for these learned database components w.r.t diverse data\nand workload drift, and based on the results, we effectively iden-\ntify the performance trade-off of different design choices under\nspecific drift scenarios.\nThe remainder of the paper is structured as follows. Section 2 pro-\nvides relevant background. Section 3 describes the overall design of\nNeurBench . Section 4 formally defines the drift factor and presents\nthe drift-aware data and workload generation framework in detail.\nSection 5 describes the benchmarking preparation. Section 6 shows\nthe experimental results. Section 7 discusses the related work, and\nSection 8 concludes.\n2 BACKGROUND\nIn this section, we provide the necessary background of learned\ndatabase components, drift in databases, and diffusion models.\n2.1 Learned Database Components\nOur study aims to be sufficiently general to systematically bench-\nmark various learned database components, such as learned query\noptimizers, learned indexes, etc. Since these components typically\nrely on ML models, we provide a general definition of learned\ndatabase components by mapping them to learnable ML functions:\nDefinition 1 (Learned Database Component) .Consider a database\nthat stores data Rand handles workload Q. A learned database\ncomponent can be formulated as an ML function ğ‘“(Â·;ğœƒ):Zâ†’Y,\nwhereğœƒdenotes the trainable parameters. The input space Zcon-\nsists of features derived from the data Rand workload Q, such as\ndata distributions and query patterns. The output space Yrepre-\nsents the predictions or decisions, such as estimated query plans,\nindex positions, or concurrency control actions. The parameters ğœƒ\nare trained by a stochastic learning algorithm A(e.g., Stochastic\nGradient Descent), which adjusts ğœƒto minimize a loss function\nL(ğ‘“(Z;ğœƒ),Y)representing the discrepancy between the predicted\noutput and the ground truth. The learning process can be formally\nexpressed as:\nğœƒ=A(Z,ğœƒğ‘Ÿ,L),\nwhereğœƒğ‘Ÿrepresents the initial random weights of the model, and\nAiteratively updates ğœƒbased on the gradients computed w.r.t. the\nloss functionL.\nA learned database component may continuously refine its model\nparameters ğœƒbased on feedback from actual query execution and\ndata/workload drift, ensuring its output Ymeets the following\nperformance objective:\nDefinition 2 (Performance Objective) .An optimal learned data-\nbase component is expected to maximize system performance ( e.g.,\nminimizing latency, execution cost, or resource consumption) over\na time window T=(ğ‘¡ğ‘ 1,ğ‘¡ğ‘ 2,...,ğ‘¡ğ‘ ğ‘)âŠ¤. Letğœ“(R(ğ‘–),Q(ğ‘–))denote\nthe performance metric at time ğ‘¡ğ‘ ğ‘–, where R(ğ‘–)andQ(ğ‘–)are the data\nand workload at time ğ‘¡ğ‘ ğ‘–, respectively, the performance objective is\nthen to find optimal model parameters ğœƒâˆ—, such that\nğœƒâˆ—=arg maxğœƒEğ‘¡ğ‘ ğ‘–âˆˆT[ğœ“(R(ğ‘–),Q(ğ‘–))],\nwhere E[Â·]denotes the expected performance.\n\nNeurBench: Benchmarking Learned Database Components with Data and Workload Drift Modeling\n2.2 Data and Workload Drift\nData stored in databases, as well as the workloads they handle,\nare subject to continuous drift. Data drift occurs due to operations\nincluding inserts, updates, deletes, and schema modifications. Work-\nload drift refers to variations in the queries and transactions pro-\ncessed by the database over time, which can be further categorized\ninto 1) Query pattern drift: changes in query structure, such as join\npatterns ( FROM clauses) and predicate conditions ( WHERE clauses);\n2) Arrival rate drift: fluctuations in the volume and concurrency of\nqueries and transactions.\nIntuitively, data and workload drift naturally occur over time\nin databases. In addition, ML models are typically trained on fixed\ndatasets, and their performance can degrade significantly when\ntested on data that deviate from the training distribution, i.e.,an is-\nsue commonly known as the distribution drift or out-of-distribution\nproblem [ 21,58]. As a result, to evaluate learned database compo-\nnents under drift scenarios, we model drift as temporal changes\nin data and workload distributions, ensuring a unified formula-\ntion that captures both data drift and workload drift. Drift may\noccur in the data, the workload, or both, depending on whether the\ndistribution of R,Q, or their joint distribution changes over time:\nDefinition 3 (Data and Workload Drift) .Consider a database\noperates over a time window Tconsisting of ğ‘time slots, i.e.,\nT=(ğ‘¡ğ‘ 1,ğ‘¡ğ‘ 2,...,ğ‘¡ğ‘ ğ‘)âŠ¤, whereğ‘â‰¥2. Let R(ğ‘–)andQ(ğ‘–)be random\nvariables representing the state of the data and the workload, re-\nspectively, at time ğ‘¡ğ‘ ğ‘–. Data and workload drift occurs when the\ndistributions of data Rand workload Qexhibit temporal changes\nacross the time window T. LetPR(ğ‘–),Q(ğ‘–)(r,q)represent the joint\nprobability density function of data Rand workload Qat timeğ‘¡ğ‘–.\nDrift happens if, for any two timestamps ğ‘–,ğ‘—âˆˆğ‘, whereğ‘–â‰ ğ‘—,\nPR(ğ‘–),Q(ğ‘–)(r,q)â‰ PR(ğ‘—),Q(ğ‘—)(r,q).\nConstructing realistic data and workload drift scenarios that\ncapture meaningful distribution drift is crucial for evaluating the\nadaptability of learned database components. We note that there\nare existing benchmarks [ 15,21] that enable the evaluation of ML\nmodels under distribution drift. However, they typically construct\nthe drift scenario by collecting real-world data exhibiting distribu-\ntion drift, which incurs two major limitations: First, they typically\nfocus on data drift while overlooking workload drift, making them\ninadequate in benchmarking learned database components under\nvarious types of drift in both data and workloads. Second, relying\non real-world data collection constrains the scope of drift scenarios\nthat can be tested, as it can only cover observed drift patterns. To\novercome these limitations, we opt to synthesize drifted data and\nworkload while maintaining realism. Specifically, we learn the un-\nderlying distributions and their drift from real data and workloads,\nand propose a generation framework that enables the controlled\nsynthesis of drifted data and workloads, thus ensuring comprehen-\nsive evaluations of learned database components under various\ndegrees and types of drift.\n2.3 Denoising Diffusion Probabilistic Model\nDiffusion models, e.g., Denoising Diffusion Probabilistic Models\n(DDPMs) [ 19], are powerful generative models that have been re-\ncently adopted to synthesize high-quality data, such as images [ 11],audio [ 35], and tabular data [ 22,36]. Inspired by the physical process\nof diffusion, where information ( e.g., particles, data) is gradually dis-\npersed over time, these models learn to reverse this process so that a\ntarget distribution can be generated from a given prior distribution\n(Gaussian distribution as in DDPM) by a series of Markov transi-\ntions. In particular, diffusion models operate in two key stages, i.e.,\nforward process and reverse process, with slight variations in the\ndetails depending on the specific model. For illustration purposes,\nwe use DDPM as a representative to formally explain these stages:\nDiffusion (Forward) Process. This process incrementally corrupts\nthe data by adding Gaussian noise in discrete timesteps, ultimately\ntransforming it into a simple noise distribution. Given a data sample\nx0âˆ¼ğ‘(x)from the true data distribution ğ‘(x), the forward process\nis formulated as a Markov chain:\nğ‘(xğ‘¡|xğ‘¡âˆ’1)=N(xğ‘¡;âˆšï¸\n1âˆ’ğ›½ğ‘¡xğ‘¡âˆ’1,ğ›½ğ‘¡I),\nwhereğ›½ğ‘¡represents the variance schedule controlling the noise\nadded at each step ğ‘¡. The data distribution after ğ‘‡timesteps,ğ‘(xğ‘‡),\napproximates an isotropic Gaussian distribution N(0,I).\nDenoising (Reverse) Process. This process reconstructs the tar-\nget data x0by iteratively denoising xğ‘¡in reverse timesteps, starting\nfrom xğ‘‡âˆ¼N( 0,I). The reverse process ğ‘(xğ‘¡âˆ’1|xğ‘¡)can be approxi-\nmated as a Gaussian distribution:\nğ‘(xğ‘¡âˆ’1|xğ‘¡)=N(xğ‘¡âˆ’1;ğ(xğ‘¡,ğ‘¡;ğœƒ),ğœ2\nğ‘¡I),\nwhere ğis learned parameters that estimate the mean of the reverse\nprocess, while ğœ2\nğ‘¡is derived from ğ›½ğ‘¡.\nGuided Diffusion. The reverse process can incorporate auxiliary\ninformation ğ‘, yielding a conditional reverse process ğ‘(xğ‘¡âˆ’1|xğ‘¡,ğ‘),\nwhere the conditioning variable ğ‘may represent factors such as\ncorrelation-preserving constraints, or contextual information [ 11,\n36]. By applying the Bayesâ€™ theorem, ğ‘(xğ‘¡âˆ’1|xğ‘¡,ğ‘)can be decom-\nposed as:\nğ‘(xğ‘¡âˆ’1|xğ‘¡,ğ‘)âˆğ‘(xğ‘¡âˆ’1|xğ‘¡)Pr(ğ‘|xğ‘¡âˆ’1,xğ‘¡).\nIn particular, ğ‘(xğ‘¡âˆ’1|xğ‘¡)should perform unconditional generation,\nwhile Pr(ğ‘|xğ‘¡âˆ’1,xğ‘¡)guides the reverse process to introduce the\nspecified condition ğ‘. Inspired by such guided diffusion models,\nwe treat the introduction of drift to data and workloads as a spe-\ncific condition, and propose a novel drift-aware data and workload\ngeneration framework based on diffusion models.\n3 OVERVIEW OF NeurBench\nIn this section, we introduce the system overview of NeurBench ,\nas shown in Figure 1. To systematically evaluate learned database\ncomponents under controllable data and workload drift, we design\nNeurBench with two key modules, a drift-aware data and workload\ngenerator , and a performance evaluator . After receiving the original\ndata and workloads from users, the generator synthesizes drifted\ndata and workloads, which are combined with the original ones\nas the drift scenarios so that the evaluator can then utilize it to\nconduct evaluations. We now provide more details on the generator\nand the evaluator, respectively.\nDrift-aware Data and Workload Generator. We allow users to\nspecify the desired drift level using a drift factorğ‘‘, whereğ‘‘âˆˆ[0,1].\nBy default, users can independently indicate drift factors for either\ndata or workload to achieve effective control over each. A drift\n\nZhanhao Zhao, Haotian Gao, Naili Xing, Lingze Zeng, Meihui Zhang,\nGang Chen, Manuel Rigger, Beng Chin Ooi\nReal Data and WorkloadsRelation\nDrifted Data and WorkloadsDrifter(Correlation-aware)Diï¬€user(DDPM-based)Required Drift: 0.1Diï¬€userDrifterDriftControllingDiï¬€userDenoiseDrifterDriftControlling+Drift-aware Data and Workload GeneratorLearned Database ComponentsTrain/InitializeLearned Query OptimizerLearned Concurrency ControlLearned IndexGenerateTrainUserAgeEduZMS30PhDY40PhD40XBal12K10K15KRelationUserAgeEduZâ€™MS25MSYâ€™35PhD35Xâ€™Bal8K5K10Kâ€¦+Denoiser1\n<latexit sha1_base64=\"Uk7SWPKRpnJep4S3LWCsgbFQlyY=\">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjDxRFoueiR68YiJBRNoyHaZwobtttndmpCG3+DFg8Z49Qd589+4QA8KvmSSl/dmMjMvTAXXxnW/ndLG5tb2Tnm3srd/cHhUPT7p6CRTDH2WiEQ9hlSj4BJ9w43Ax1QhjUOB3XByO/e7T6g0T+SDmaYYxHQkecQZNVby62rg1QfVmttwFyDrxCtIDQq0B9Wv/jBhWYzSMEG17nluaoKcKsOZwFmln2lMKZvQEfYslTRGHeSLY2fkwipDEiXKljRkof6eyGms9TQObWdMzVivenPxP6+Xmeg6yLlMM4OSLRdFmSAmIfPPyZArZEZMLaFMcXsrYWOqKDM2n4oNwVt9eZ10mg3PbXj3zVrrpoijDGdwDpfgwRW04A7a4AMDDs/wCm+OdF6cd+dj2VpyiplT+APn8we6cI30</latexit><latexit sha1_base64=\"Uk7SWPKRpnJep4S3LWCsgbFQlyY=\">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjDxRFoueiR68YiJBRNoyHaZwobtttndmpCG3+DFg8Z49Qd589+4QA8KvmSSl/dmMjMvTAXXxnW/ndLG5tb2Tnm3srd/cHhUPT7p6CRTDH2WiEQ9hlSj4BJ9w43Ax1QhjUOB3XByO/e7T6g0T+SDmaYYxHQkecQZNVby62rg1QfVmttwFyDrxCtIDQq0B9Wv/jBhWYzSMEG17nluaoKcKsOZwFmln2lMKZvQEfYslTRGHeSLY2fkwipDEiXKljRkof6eyGms9TQObWdMzVivenPxP6+Xmeg6yLlMM4OSLRdFmSAmIfPPyZArZEZMLaFMcXsrYWOqKDM2n4oNwVt9eZ10mg3PbXj3zVrrpoijDGdwDpfgwRW04A7a4AMDDs/wCm+OdF6cd+dj2VpyiplT+APn8we6cI30</latexit><latexit sha1_base64=\"Uk7SWPKRpnJep4S3LWCsgbFQlyY=\">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjDxRFoueiR68YiJBRNoyHaZwobtttndmpCG3+DFg8Z49Qd589+4QA8KvmSSl/dmMjMvTAXXxnW/ndLG5tb2Tnm3srd/cHhUPT7p6CRTDH2WiEQ9hlSj4BJ9w43Ax1QhjUOB3XByO/e7T6g0T+SDmaYYxHQkecQZNVby62rg1QfVmttwFyDrxCtIDQq0B9Wv/jBhWYzSMEG17nluaoKcKsOZwFmln2lMKZvQEfYslTRGHeSLY2fkwipDEiXKljRkof6eyGms9TQObWdMzVivenPxP6+Xmeg6yLlMM4OSLRdFmSAmIfPPyZArZEZMLaFMcXsrYWOqKDM2n4oNwVt9eZ10mg3PbXj3zVrrpoijDGdwDpfgwRW04A7a4AMDDs/wCm+OdF6cd+dj2VpyiplT+APn8we6cI30</latexit><latexit sha1_base64=\"Uk7SWPKRpnJep4S3LWCsgbFQlyY=\">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjDxRFoueiR68YiJBRNoyHaZwobtttndmpCG3+DFg8Z49Qd589+4QA8KvmSSl/dmMjMvTAXXxnW/ndLG5tb2Tnm3srd/cHhUPT7p6CRTDH2WiEQ9hlSj4BJ9w43Ax1QhjUOB3XByO/e7T6g0T+SDmaYYxHQkecQZNVby62rg1QfVmttwFyDrxCtIDQq0B9Wv/jBhWYzSMEG17nluaoKcKsOZwFmln2lMKZvQEfYslTRGHeSLY2fkwipDEiXKljRkof6eyGms9TQObWdMzVivenPxP6+Xmeg6yLlMM4OSLRdFmSAmIfPPyZArZEZMLaFMcXsrYWOqKDM2n4oNwVt9eZ10mg3PbXj3zVrrpoijDGdwDpfgwRW04A7a4AMDDs/wCm+OdF6cd+dj2VpyiplT+APn8we6cI30</latexit>r01\n<latexit sha1_base64=\"J8kByEr5oDs8td2Gkes5sHJMa1k=\">AAAB7XicbVA9TwJBEJ3DL8Qv1NJmIxityB2NlkQbS0wESeBC9pY9WNmPy+6eCbnwH2wsNMbW/2Pnv3GBKxR8ySQv781kZl6UcGas7397hbX1jc2t4nZpZ3dv/6B8eNQ2KtWEtojiSncibChnkrYss5x2Ek2xiDh9iMY3M//hiWrDlLy3k4SGAg8lixnB1kntqu4H59V+ueLX/DnQKglyUoEczX75qzdQJBVUWsKxMd3AT2yYYW0Z4XRa6qWGJpiM8ZB2HZVYUBNm82un6MwpAxQr7UpaNFd/T2RYGDMRkesU2I7MsjcT//O6qY2vwozJJLVUksWiOOXIKjR7HQ2YpsTyiSOYaOZuRWSENSbWBVRyIQTLL6+Sdr0W+LXgrl5pXOdxFOEETuECAriEBtxCE1pA4BGe4RXePOW9eO/ex6K14OUzx/AH3ucPG9SOJQ==</latexit><latexit sha1_base64=\"J8kByEr5oDs8td2Gkes5sHJMa1k=\">AAAB7XicbVA9TwJBEJ3DL8Qv1NJmIxityB2NlkQbS0wESeBC9pY9WNmPy+6eCbnwH2wsNMbW/2Pnv3GBKxR8ySQv781kZl6UcGas7397hbX1jc2t4nZpZ3dv/6B8eNQ2KtWEtojiSncibChnkrYss5x2Ek2xiDh9iMY3M//hiWrDlLy3k4SGAg8lixnB1kntqu4H59V+ueLX/DnQKglyUoEczX75qzdQJBVUWsKxMd3AT2yYYW0Z4XRa6qWGJpiM8ZB2HZVYUBNm82un6MwpAxQr7UpaNFd/T2RYGDMRkesU2I7MsjcT//O6qY2vwozJJLVUksWiOOXIKjR7HQ2YpsTyiSOYaOZuRWSENSbWBVRyIQTLL6+Sdr0W+LXgrl5pXOdxFOEETuECAriEBtxCE1pA4BGe4RXePOW9eO/ex6K14OUzx/AH3ucPG9SOJQ==</latexit><latexit sha1_base64=\"J8kByEr5oDs8td2Gkes5sHJMa1k=\">AAAB7XicbVA9TwJBEJ3DL8Qv1NJmIxityB2NlkQbS0wESeBC9pY9WNmPy+6eCbnwH2wsNMbW/2Pnv3GBKxR8ySQv781kZl6UcGas7397hbX1jc2t4nZpZ3dv/6B8eNQ2KtWEtojiSncibChnkrYss5x2Ek2xiDh9iMY3M//hiWrDlLy3k4SGAg8lixnB1kntqu4H59V+ueLX/DnQKglyUoEczX75qzdQJBVUWsKxMd3AT2yYYW0Z4XRa6qWGJpiM8ZB2HZVYUBNm82un6MwpAxQr7UpaNFd/T2RYGDMRkesU2I7MsjcT//O6qY2vwozJJLVUksWiOOXIKjR7HQ2YpsTyiSOYaOZuRWSENSbWBVRyIQTLL6+Sdr0W+LXgrl5pXOdxFOEETuECAriEBtxCE1pA4BGe4RXePOW9eO/ex6K14OUzx/AH3ucPG9SOJQ==</latexit><latexit sha1_base64=\"J8kByEr5oDs8td2Gkes5sHJMa1k=\">AAAB7XicbVA9TwJBEJ3DL8Qv1NJmIxityB2NlkQbS0wESeBC9pY9WNmPy+6eCbnwH2wsNMbW/2Pnv3GBKxR8ySQv781kZl6UcGas7397hbX1jc2t4nZpZ3dv/6B8eNQ2KtWEtojiSncibChnkrYss5x2Ek2xiDh9iMY3M//hiWrDlLy3k4SGAg8lixnB1kntqu4H59V+ueLX/DnQKglyUoEczX75qzdQJBVUWsKxMd3AT2yYYW0Z4XRa6qWGJpiM8ZB2HZVYUBNm82un6MwpAxQr7UpaNFd/T2RYGDMRkesU2I7MsjcT//O6qY2vwozJJLVUksWiOOXIKjR7HQ2YpsTyiSOYaOZuRWSENSbWBVRyIQTLL6+Sdr0W+LXgrl5pXOdxFOEETuECAriEBtxCE1pA4BGe4RXePOW9eO/ex6K14OUzx/AH3ucPG9SOJQ==</latexit>QueryPatternsq1\n<latexit sha1_base64=\"OmQa4K2DtTKjCnuZQMvb8Z5xODk=\">AAAB7HicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsaLYk2lph4QAIXsrfMwYa9vXN3z4QQfoONhcbY+oPs/DcucIWCL5nk5b2ZzMwLU8G1cd1vp7CxubW9U9wt7e0fHB6Vj09aOskUQ58lIlGdkGoUXKJvuBHYSRXSOBTYDse3c7/9hErzRD6YSYpBTIeSR5xRYyW/+tj3qv1yxa25C5B14uWkAjma/fJXb5CwLEZpmKBadz03NcGUKsOZwFmpl2lMKRvTIXYtlTRGHUwXx87IhVUGJEqULWnIQv09MaWx1pM4tJ0xNSO96s3F/7xuZqLrYMplmhmUbLkoygQxCZl/TgZcITNiYgllittbCRtRRZmx+ZRsCN7qy+ukVa95bs27r1caN3kcRTiDc7gED66gAXfQBB8YcHiGV3hzpPPivDsfy9aCk8+cwh84nz+46Y3z</latexit><latexit sha1_base64=\"OmQa4K2DtTKjCnuZQMvb8Z5xODk=\">AAAB7HicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsaLYk2lph4QAIXsrfMwYa9vXN3z4QQfoONhcbY+oPs/DcucIWCL5nk5b2ZzMwLU8G1cd1vp7CxubW9U9wt7e0fHB6Vj09aOskUQ58lIlGdkGoUXKJvuBHYSRXSOBTYDse3c7/9hErzRD6YSYpBTIeSR5xRYyW/+tj3qv1yxa25C5B14uWkAjma/fJXb5CwLEZpmKBadz03NcGUKsOZwFmpl2lMKRvTIXYtlTRGHUwXx87IhVUGJEqULWnIQv09MaWx1pM4tJ0xNSO96s3F/7xuZqLrYMplmhmUbLkoygQxCZl/TgZcITNiYgllittbCRtRRZmx+ZRsCN7qy+ukVa95bs27r1caN3kcRTiDc7gED66gAXfQBB8YcHiGV3hzpPPivDsfy9aCk8+cwh84nz+46Y3z</latexit><latexit sha1_base64=\"OmQa4K2DtTKjCnuZQMvb8Z5xODk=\">AAAB7HicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsaLYk2lph4QAIXsrfMwYa9vXN3z4QQfoONhcbY+oPs/DcucIWCL5nk5b2ZzMwLU8G1cd1vp7CxubW9U9wt7e0fHB6Vj09aOskUQ58lIlGdkGoUXKJvuBHYSRXSOBTYDse3c7/9hErzRD6YSYpBTIeSR5xRYyW/+tj3qv1yxa25C5B14uWkAjma/fJXb5CwLEZpmKBadz03NcGUKsOZwFmpl2lMKRvTIXYtlTRGHUwXx87IhVUGJEqULWnIQv09MaWx1pM4tJ0xNSO96s3F/7xuZqLrYMplmhmUbLkoygQxCZl/TgZcITNiYgllittbCRtRRZmx+ZRsCN7qy+ukVa95bs27r1caN3kcRTiDc7gED66gAXfQBB8YcHiGV3hzpPPivDsfy9aCk8+cwh84nz+46Y3z</latexit><latexit sha1_base64=\"OmQa4K2DtTKjCnuZQMvb8Z5xODk=\">AAAB7HicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsaLYk2lph4QAIXsrfMwYa9vXN3z4QQfoONhcbY+oPs/DcucIWCL5nk5b2ZzMwLU8G1cd1vp7CxubW9U9wt7e0fHB6Vj09aOskUQ58lIlGdkGoUXKJvuBHYSRXSOBTYDse3c7/9hErzRD6YSYpBTIeSR5xRYyW/+tj3qv1yxa25C5B14uWkAjma/fJXb5CwLEZpmKBadz03NcGUKsOZwFmpl2lMKRvTIXYtlTRGHUwXx87IhVUGJEqULWnIQv09MaWx1pM4tJ0xNSO96s3F/7xuZqLrYMplmhmUbLkoygQxCZl/TgZcITNiYgllittbCRtRRZmx+ZRsCN7qy+ukVa95bs27r1caN3kcRTiDc7gED66gAXfQBB8YcHiGV3hzpPPivDsfy9aCk8+cwh84nz+46Y3z</latexit>q2\n<latexit sha1_base64=\"OgVkHv7LHRvSd6mloNkjHPttUh0=\">AAAB7HicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsaLYk2lph4QAIXsrcssGFv79ydMyEXfoONhcbY+oPs/DcucIWCL5nk5b2ZzMwLEykMuu63U9jY3NreKe6W9vYPDo/KxyctE6eacZ/FMtadkBouheI+CpS8k2hOo1Dydji5nfvtJ66NiNUDThMeRHSkxFAwilbyq4/9erVfrrg1dwGyTrycVCBHs1/+6g1ilkZcIZPUmK7nJhhkVKNgks9KvdTwhLIJHfGupYpG3ATZ4tgZubDKgAxjbUshWai/JzIaGTONQtsZURybVW8u/ud1UxxeB5lQSYpcseWiYSoJxmT+ORkIzRnKqSWUaWFvJWxMNWVo8ynZELzVl9dJq17z3Jp3X680bvI4inAG53AJHlxBA+6gCT4wEPAMr/DmKOfFeXc+lq0FJ585hT9wPn8Aum6N9A==</latexit><latexit sha1_base64=\"OgVkHv7LHRvSd6mloNkjHPttUh0=\">AAAB7HicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsaLYk2lph4QAIXsrcssGFv79ydMyEXfoONhcbY+oPs/DcucIWCL5nk5b2ZzMwLEykMuu63U9jY3NreKe6W9vYPDo/KxyctE6eacZ/FMtadkBouheI+CpS8k2hOo1Dydji5nfvtJ66NiNUDThMeRHSkxFAwilbyq4/9erVfrrg1dwGyTrycVCBHs1/+6g1ilkZcIZPUmK7nJhhkVKNgks9KvdTwhLIJHfGupYpG3ATZ4tgZubDKgAxjbUshWai/JzIaGTONQtsZURybVW8u/ud1UxxeB5lQSYpcseWiYSoJxmT+ORkIzRnKqSWUaWFvJWxMNWVo8ynZELzVl9dJq17z3Jp3X680bvI4inAG53AJHlxBA+6gCT4wEPAMr/DmKOfFeXc+lq0FJ585hT9wPn8Aum6N9A==</latexit><latexit sha1_base64=\"OgVkHv7LHRvSd6mloNkjHPttUh0=\">AAAB7HicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsaLYk2lph4QAIXsrcssGFv79ydMyEXfoONhcbY+oPs/DcucIWCL5nk5b2ZzMwLEykMuu63U9jY3NreKe6W9vYPDo/KxyctE6eacZ/FMtadkBouheI+CpS8k2hOo1Dydji5nfvtJ66NiNUDThMeRHSkxFAwilbyq4/9erVfrrg1dwGyTrycVCBHs1/+6g1ilkZcIZPUmK7nJhhkVKNgks9KvdTwhLIJHfGupYpG3ATZ4tgZubDKgAxjbUshWai/JzIaGTONQtsZURybVW8u/ud1UxxeB5lQSYpcseWiYSoJxmT+ORkIzRnKqSWUaWFvJWxMNWVo8ynZELzVl9dJq17z3Jp3X680bvI4inAG53AJHlxBA+6gCT4wEPAMr/DmKOfFeXc+lq0FJ585hT9wPn8Aum6N9A==</latexit><latexit sha1_base64=\"OgVkHv7LHRvSd6mloNkjHPttUh0=\">AAAB7HicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsaLYk2lph4QAIXsrcssGFv79ydMyEXfoONhcbY+oPs/DcucIWCL5nk5b2ZzMwLEykMuu63U9jY3NreKe6W9vYPDo/KxyctE6eacZ/FMtadkBouheI+CpS8k2hOo1Dydji5nfvtJ66NiNUDThMeRHSkxFAwilbyq4/9erVfrrg1dwGyTrycVCBHs1/+6g1ilkZcIZPUmK7nJhhkVKNgks9KvdTwhLIJHfGupYpG3ATZ4tgZubDKgAxjbUshWai/JzIaGTONQtsZURybVW8u/ud1UxxeB5lQSYpcseWiYSoJxmT+ORkIzRnKqSWUaWFvJWxMNWVo8ynZELzVl9dJq17z3Jp3X680bvI4inAG53AJHlxBA+6gCT4wEPAMr/DmKOfFeXc+lq0FJ585hT9wPn8Aum6N9A==</latexit>AuditQueryPatternsPaymentX:50% | Y:50%AuditPaymentX:30% | Y:70%q01\n<latexit sha1_base64=\"4Z6DBewbH03XVdhu/Ypy07/uMdk=\">AAAB7XicbVA9T8MwEL2Ur1K+CowsFi2CqUq6wFjBwlgk+iG1UeW4Tmvq2MF2kKqo/4GFAYRY+T9s/BucNgO0POmkp/fudHcviDnTxnW/ncLa+sbmVnG7tLO7t39QPjxqa5koQltEcqm6AdaUM0FbhhlOu7GiOAo47QSTm8zvPFGlmRT3ZhpTP8IjwUJGsLFSu/o48M6rg3LFrblzoFXi5aQCOZqD8ld/KEkSUWEIx1r3PDc2foqVYYTTWamfaBpjMsEj2rNU4IhqP51fO0NnVhmiUCpbwqC5+nsixZHW0yiwnRE2Y73sZeJ/Xi8x4ZWfMhEnhgqyWBQmHBmJstfRkClKDJ9agoli9lZExlhhYmxAJRuCt/zyKmnXa55b8+7qlcZ1HkcRTuAULsCDS2jALTShBQQe4Ble4c2Rzovz7nwsWgtOPnMMf+B8/gAaTI4k</latexit><latexit sha1_base64=\"4Z6DBewbH03XVdhu/Ypy07/uMdk=\">AAAB7XicbVA9T8MwEL2Ur1K+CowsFi2CqUq6wFjBwlgk+iG1UeW4Tmvq2MF2kKqo/4GFAYRY+T9s/BucNgO0POmkp/fudHcviDnTxnW/ncLa+sbmVnG7tLO7t39QPjxqa5koQltEcqm6AdaUM0FbhhlOu7GiOAo47QSTm8zvPFGlmRT3ZhpTP8IjwUJGsLFSu/o48M6rg3LFrblzoFXi5aQCOZqD8ld/KEkSUWEIx1r3PDc2foqVYYTTWamfaBpjMsEj2rNU4IhqP51fO0NnVhmiUCpbwqC5+nsixZHW0yiwnRE2Y73sZeJ/Xi8x4ZWfMhEnhgqyWBQmHBmJstfRkClKDJ9agoli9lZExlhhYmxAJRuCt/zyKmnXa55b8+7qlcZ1HkcRTuAULsCDS2jALTShBQQe4Ble4c2Rzovz7nwsWgtOPnMMf+B8/gAaTI4k</latexit><latexit sha1_base64=\"4Z6DBewbH03XVdhu/Ypy07/uMdk=\">AAAB7XicbVA9T8MwEL2Ur1K+CowsFi2CqUq6wFjBwlgk+iG1UeW4Tmvq2MF2kKqo/4GFAYRY+T9s/BucNgO0POmkp/fudHcviDnTxnW/ncLa+sbmVnG7tLO7t39QPjxqa5koQltEcqm6AdaUM0FbhhlOu7GiOAo47QSTm8zvPFGlmRT3ZhpTP8IjwUJGsLFSu/o48M6rg3LFrblzoFXi5aQCOZqD8ld/KEkSUWEIx1r3PDc2foqVYYTTWamfaBpjMsEj2rNU4IhqP51fO0NnVhmiUCpbwqC5+nsixZHW0yiwnRE2Y73sZeJ/Xi8x4ZWfMhEnhgqyWBQmHBmJstfRkClKDJ9agoli9lZExlhhYmxAJRuCt/zyKmnXa55b8+7qlcZ1HkcRTuAULsCDS2jALTShBQQe4Ble4c2Rzovz7nwsWgtOPnMMf+B8/gAaTI4k</latexit><latexit sha1_base64=\"hP+6LrUf2d3tZaldqaQQvEKMXyw=\">AAAB2XicbZDNSgMxFIXv1L86Vq1rN8EiuCozbnQpuHFZwbZCO5RM5k4bmskMyR2hDH0BF25EfC93vo3pz0JbDwQ+zknIvSculLQUBN9ebWd3b/+gfugfNfzjk9Nmo2fz0gjsilzl5jnmFpXU2CVJCp8LgzyLFfbj6f0i77+gsTLXTzQrMMr4WMtUCk7O6oyaraAdLMW2IVxDC9YaNb+GSS7KDDUJxa0dhEFBUcUNSaFw7g9LiwUXUz7GgUPNM7RRtRxzzi6dk7A0N+5oYkv394uKZ9bOstjdzDhN7Ga2MP/LBiWlt1EldVESarH6KC0Vo5wtdmaJNChIzRxwYaSblYkJN1yQa8Z3HYSbG29D77odBu3wMYA6nMMFXEEIN3AHD9CBLghI4BXevYn35n2suqp569LO4I+8zx84xIo4</latexit><latexit sha1_base64=\"BDHVqgxYsCQzTle6EFsTk8OODiE=\">AAAB4nicbZBLSwMxFIXv1FetVatbN8FWdFVm3OhScOOygn1AO5RMeqeNzSRjkhHK0P/gxoUi/ih3/hvTx0JbDwQ+zknIvSdKBTfW97+9wsbm1vZOcbe0V94/OKwclVtGZZphkymhdCeiBgWX2LTcCuykGmkSCWxH49tZ3n5GbbiSD3aSYpjQoeQxZ9Q6q1V76gfntX6l6tf9ucg6BEuowlKNfuWrN1AsS1BaJqgx3cBPbZhTbTkTOC31MoMpZWM6xK5DSRM0YT6fdkrOnDMgsdLuSEvm7u8XOU2MmSSRu5lQOzKr2cz8L+tmNr4Ocy7TzKJki4/iTBCryGx1MuAamRUTB5Rp7mYlbEQ1ZdYVVHIlBKsrr0Prsh749eDehyKcwClcQABXcAN30IAmMHiEF3iDd095r97Hoq6Ct+ztGP7I+/wB+k+Mzg==</latexit><latexit sha1_base64=\"BDHVqgxYsCQzTle6EFsTk8OODiE=\">AAAB4nicbZBLSwMxFIXv1FetVatbN8FWdFVm3OhScOOygn1AO5RMeqeNzSRjkhHK0P/gxoUi/ih3/hvTx0JbDwQ+zknIvSdKBTfW97+9wsbm1vZOcbe0V94/OKwclVtGZZphkymhdCeiBgWX2LTcCuykGmkSCWxH49tZ3n5GbbiSD3aSYpjQoeQxZ9Q6q1V76gfntX6l6tf9ucg6BEuowlKNfuWrN1AsS1BaJqgx3cBPbZhTbTkTOC31MoMpZWM6xK5DSRM0YT6fdkrOnDMgsdLuSEvm7u8XOU2MmSSRu5lQOzKr2cz8L+tmNr4Ocy7TzKJki4/iTBCryGx1MuAamRUTB5Rp7mYlbEQ1ZdYVVHIlBKsrr0Prsh749eDehyKcwClcQABXcAN30IAmMHiEF3iDd095r97Hoq6Ct+ztGP7I+/wB+k+Mzg==</latexit><latexit sha1_base64=\"kQmPoEqie67yXEylViRwrnmSpiI=\">AAAB7XicbVC7TgMxEFzzDOEVoKSxSBBU0R0NlBE0lEEiDyk5RT7Hl5j47MP2IUWn/AMNBQjR8j90/A1OcgUkjLTSaGZXuzthIrixnveNVlbX1jc2C1vF7Z3dvf3SwWHTqFRT1qBKKN0OiWGCS9aw3ArWTjQjcShYKxzdTP3WE9OGK3lvxwkLYjKQPOKUWCc1K489/6zSK5W9qjcDXiZ+TsqQo94rfXX7iqYxk5YKYkzH9xIbZERbTgWbFLupYQmhIzJgHUcliZkJstm1E3zqlD6OlHYlLZ6pvycyEhszjkPXGRM7NIveVPzP66Q2ugoyLpPUMknni6JUYKvw9HXc55pRK8aOEKq5uxXTIdGEWhdQ0YXgL768TJoXVd+r+ndeuXadx1GAYziBc/DhEmpwC3VoAIUHeIZXeEMKvaB39DFvXUH5zBH8Afr8ARmsjiI=</latexit><latexit sha1_base64=\"4Z6DBewbH03XVdhu/Ypy07/uMdk=\">AAAB7XicbVA9T8MwEL2Ur1K+CowsFi2CqUq6wFjBwlgk+iG1UeW4Tmvq2MF2kKqo/4GFAYRY+T9s/BucNgO0POmkp/fudHcviDnTxnW/ncLa+sbmVnG7tLO7t39QPjxqa5koQltEcqm6AdaUM0FbhhlOu7GiOAo47QSTm8zvPFGlmRT3ZhpTP8IjwUJGsLFSu/o48M6rg3LFrblzoFXi5aQCOZqD8ld/KEkSUWEIx1r3PDc2foqVYYTTWamfaBpjMsEj2rNU4IhqP51fO0NnVhmiUCpbwqC5+nsixZHW0yiwnRE2Y73sZeJ/Xi8x4ZWfMhEnhgqyWBQmHBmJstfRkClKDJ9agoli9lZExlhhYmxAJRuCt/zyKmnXa55b8+7qlcZ1HkcRTuAULsCDS2jALTShBQQe4Ble4c2Rzovz7nwsWgtOPnMMf+B8/gAaTI4k</latexit><latexit sha1_base64=\"4Z6DBewbH03XVdhu/Ypy07/uMdk=\">AAAB7XicbVA9T8MwEL2Ur1K+CowsFi2CqUq6wFjBwlgk+iG1UeW4Tmvq2MF2kKqo/4GFAYRY+T9s/BucNgO0POmkp/fudHcviDnTxnW/ncLa+sbmVnG7tLO7t39QPjxqa5koQltEcqm6AdaUM0FbhhlOu7GiOAo47QSTm8zvPFGlmRT3ZhpTP8IjwUJGsLFSu/o48M6rg3LFrblzoFXi5aQCOZqD8ld/KEkSUWEIx1r3PDc2foqVYYTTWamfaBpjMsEj2rNU4IhqP51fO0NnVhmiUCpbwqC5+nsixZHW0yiwnRE2Y73sZeJ/Xi8x4ZWfMhEnhgqyWBQmHBmJstfRkClKDJ9agoli9lZExlhhYmxAJRuCt/zyKmnXa55b8+7qlcZ1HkcRTuAULsCDS2jALTShBQQe4Ble4c2Rzovz7nwsWgtOPnMMf+B8/gAaTI4k</latexit><latexit sha1_base64=\"4Z6DBewbH03XVdhu/Ypy07/uMdk=\">AAAB7XicbVA9T8MwEL2Ur1K+CowsFi2CqUq6wFjBwlgk+iG1UeW4Tmvq2MF2kKqo/4GFAYRY+T9s/BucNgO0POmkp/fudHcviDnTxnW/ncLa+sbmVnG7tLO7t39QPjxqa5koQltEcqm6AdaUM0FbhhlOu7GiOAo47QSTm8zvPFGlmRT3ZhpTP8IjwUJGsLFSu/o48M6rg3LFrblzoFXi5aQCOZqD8ld/KEkSUWEIx1r3PDc2foqVYYTTWamfaBpjMsEj2rNU4IhqP51fO0NnVhmiUCpbwqC5+nsixZHW0yiwnRE2Y73sZeJ/Xi8x4ZWfMhEnhgqyWBQmHBmJstfRkClKDJ9agoli9lZExlhhYmxAJRuCt/zyKmnXa55b8+7qlcZ1HkcRTuAULsCDS2jALTShBQQe4Ble4c2Rzovz7nwsWgtOPnMMf+B8/gAaTI4k</latexit><latexit sha1_base64=\"4Z6DBewbH03XVdhu/Ypy07/uMdk=\">AAAB7XicbVA9T8MwEL2Ur1K+CowsFi2CqUq6wFjBwlgk+iG1UeW4Tmvq2MF2kKqo/4GFAYRY+T9s/BucNgO0POmkp/fudHcviDnTxnW/ncLa+sbmVnG7tLO7t39QPjxqa5koQltEcqm6AdaUM0FbhhlOu7GiOAo47QSTm8zvPFGlmRT3ZhpTP8IjwUJGsLFSu/o48M6rg3LFrblzoFXi5aQCOZqD8ld/KEkSUWEIx1r3PDc2foqVYYTTWamfaBpjMsEj2rNU4IhqP51fO0NnVhmiUCpbwqC5+nsixZHW0yiwnRE2Y73sZeJ/Xi8x4ZWfMhEnhgqyWBQmHBmJstfRkClKDJ9agoli9lZExlhhYmxAJRuCt/zyKmnXa55b8+7qlcZ1HkcRTuAULsCDS2jALTShBQQe4Ble4c2Rzovz7nwsWgtOPnMMf+B8/gAaTI4k</latexit><latexit sha1_base64=\"4Z6DBewbH03XVdhu/Ypy07/uMdk=\">AAAB7XicbVA9T8MwEL2Ur1K+CowsFi2CqUq6wFjBwlgk+iG1UeW4Tmvq2MF2kKqo/4GFAYRY+T9s/BucNgO0POmkp/fudHcviDnTxnW/ncLa+sbmVnG7tLO7t39QPjxqa5koQltEcqm6AdaUM0FbhhlOu7GiOAo47QSTm8zvPFGlmRT3ZhpTP8IjwUJGsLFSu/o48M6rg3LFrblzoFXi5aQCOZqD8ld/KEkSUWEIx1r3PDc2foqVYYTTWamfaBpjMsEj2rNU4IhqP51fO0NnVhmiUCpbwqC5+nsixZHW0yiwnRE2Y73sZeJ/Xi8x4ZWfMhEnhgqyWBQmHBmJstfRkClKDJ9agoli9lZExlhhYmxAJRuCt/zyKmnXa55b8+7qlcZ1HkcRTuAULsCDS2jALTShBQQe4Ble4c2Rzovz7nwsWgtOPnMMf+B8/gAaTI4k</latexit><latexit sha1_base64=\"4Z6DBewbH03XVdhu/Ypy07/uMdk=\">AAAB7XicbVA9T8MwEL2Ur1K+CowsFi2CqUq6wFjBwlgk+iG1UeW4Tmvq2MF2kKqo/4GFAYRY+T9s/BucNgO0POmkp/fudHcviDnTxnW/ncLa+sbmVnG7tLO7t39QPjxqa5koQltEcqm6AdaUM0FbhhlOu7GiOAo47QSTm8zvPFGlmRT3ZhpTP8IjwUJGsLFSu/o48M6rg3LFrblzoFXi5aQCOZqD8ld/KEkSUWEIx1r3PDc2foqVYYTTWamfaBpjMsEj2rNU4IhqP51fO0NnVhmiUCpbwqC5+nsixZHW0yiwnRE2Y73sZeJ/Xi8x4ZWfMhEnhgqyWBQmHBmJstfRkClKDJ9agoli9lZExlhhYmxAJRuCt/zyKmnXa55b8+7qlcZ1HkcRTuAULsCDS2jALTShBQQe4Ble4c2Rzovz7nwsWgtOPnMMf+B8/gAaTI4k</latexit>q02\n<latexit sha1_base64=\"uZkT3MgGaZ4s/CUVw45w86TzwBM=\">AAAB7XicbVA9T8MwEL2Ur1K+CowsFi2CqUqywFjBwlgk+iG1UeW4Tmvq2MF2kKqo/4GFAYRY+T9s/BvcNgO0POmkp/fudHcvTDjTxnW/ncLa+sbmVnG7tLO7t39QPjxqaZkqQptEcqk6IdaUM0GbhhlOO4miOA45bYfjm5nffqJKMynuzSShQYyHgkWMYGOlVvWx759X++WKW3PnQKvEy0kFcjT65a/eQJI0psIQjrXuem5iggwrwwin01Iv1TTBZIyHtGupwDHVQTa/dorOrDJAkVS2hEFz9fdEhmOtJ3FoO2NsRnrZm4n/ed3URFdBxkSSGirIYlGUcmQkmr2OBkxRYvjEEkwUs7ciMsIKE2MDKtkQvOWXV0nLr3luzbvzK/XrPI4inMApXIAHl1CHW2hAEwg8wDO8wpsjnRfn3flYtBacfOYY/sD5/AEb0o4l</latexit><latexit sha1_base64=\"uZkT3MgGaZ4s/CUVw45w86TzwBM=\">AAAB7XicbVA9T8MwEL2Ur1K+CowsFi2CqUqywFjBwlgk+iG1UeW4Tmvq2MF2kKqo/4GFAYRY+T9s/BvcNgO0POmkp/fudHcvTDjTxnW/ncLa+sbmVnG7tLO7t39QPjxqaZkqQptEcqk6IdaUM0GbhhlOO4miOA45bYfjm5nffqJKMynuzSShQYyHgkWMYGOlVvWx759X++WKW3PnQKvEy0kFcjT65a/eQJI0psIQjrXuem5iggwrwwin01Iv1TTBZIyHtGupwDHVQTa/dorOrDJAkVS2hEFz9fdEhmOtJ3FoO2NsRnrZm4n/ed3URFdBxkSSGirIYlGUcmQkmr2OBkxRYvjEEkwUs7ciMsIKE2MDKtkQvOWXV0nLr3luzbvzK/XrPI4inMApXIAHl1CHW2hAEwg8wDO8wpsjnRfn3flYtBacfOYY/sD5/AEb0o4l</latexit><latexit sha1_base64=\"uZkT3MgGaZ4s/CUVw45w86TzwBM=\">AAAB7XicbVA9T8MwEL2Ur1K+CowsFi2CqUqywFjBwlgk+iG1UeW4Tmvq2MF2kKqo/4GFAYRY+T9s/BvcNgO0POmkp/fudHcvTDjTxnW/ncLa+sbmVnG7tLO7t39QPjxqaZkqQptEcqk6IdaUM0GbhhlOO4miOA45bYfjm5nffqJKMynuzSShQYyHgkWMYGOlVvWx759X++WKW3PnQKvEy0kFcjT65a/eQJI0psIQjrXuem5iggwrwwin01Iv1TTBZIyHtGupwDHVQTa/dorOrDJAkVS2hEFz9fdEhmOtJ3FoO2NsRnrZm4n/ed3URFdBxkSSGirIYlGUcmQkmr2OBkxRYvjEEkwUs7ciMsIKE2MDKtkQvOWXV0nLr3luzbvzK/XrPI4inMApXIAHl1CHW2hAEwg8wDO8wpsjnRfn3flYtBacfOYY/sD5/AEb0o4l</latexit><latexit sha1_base64=\"uZkT3MgGaZ4s/CUVw45w86TzwBM=\">AAAB7XicbVA9T8MwEL2Ur1K+CowsFi2CqUqywFjBwlgk+iG1UeW4Tmvq2MF2kKqo/4GFAYRY+T9s/BvcNgO0POmkp/fudHcvTDjTxnW/ncLa+sbmVnG7tLO7t39QPjxqaZkqQptEcqk6IdaUM0GbhhlOO4miOA45bYfjm5nffqJKMynuzSShQYyHgkWMYGOlVvWx759X++WKW3PnQKvEy0kFcjT65a/eQJI0psIQjrXuem5iggwrwwin01Iv1TTBZIyHtGupwDHVQTa/dorOrDJAkVS2hEFz9fdEhmOtJ3FoO2NsRnrZm4n/ed3URFdBxkSSGirIYlGUcmQkmr2OBkxRYvjEEkwUs7ciMsIKE2MDKtkQvOWXV0nLr3luzbvzK/XrPI4inMApXIAHl1CHW2hAEwg8wDO8wpsjnRfn3flYtBacfOYY/sD5/AEb0o4l</latexit>\n010020030040050000:0012:0023:59Queries/sq1'q2' 010020030040000:0012:0023:59Queries/sq1q2Drift ScenariosPerformance EvaluatorTest\nFigure 1: System Overview of NeurBench\nfactor ofğ‘‘=0 indicates no drift, while ğ‘‘=1 denotes that the entire\ndata or workload is fully drifted. Section 4.1 provides the formal\ndefinition of the drift factor.\nGiven a specified drift factor ğ‘‘as input, the idea is to drift the\ngiven original data (workload) to a target data (workload) that aligns\nwithğ‘‘, thereby enabling controllable drifted data and workload\ngeneration. In addition, we capture real-world drift patterns to\nensure that the generated data and workloads exhibit realistic drift\nbehaviors. To achieve this, we design the generator based on guided\ndiffusion, which consists of two components: 1) Diffuser , a DDPM-\nbased generative model that learns the underlying distribution\nof the original data and workloads; 2) Drifter , a drift-controlling\nmodule that guides the diffuser to yield a conditional denoising\nprocess. To synthesize drifted data and workloads, starting from a\nstandard normal distribution, we iteratively invoke the diffuser to\ndenoise the distribution, while leveraging the drifter to introduce\ndrift at each denoising step, gradually transforming the distribution\ninto the target distribution. Moreover, the drifter is explicitly trained\nto introduce drift patterns that align with real-world scenarios while\npreserving the inherent correlations within the data and workloads.\nSection 4.2 details the diffuser and drifter construction.\nPerformance Evaluator. To effectively assess the adaptability\nand robustness of learned database components, we evaluate their\nperformance across various drift scenarios. Given a drift factor ğ‘‘,\nwe use the generator to synthesize drifted data and workloads based\nonğ‘‘. We impose no constraints on drift scenario construction, and\nallow users to flexibly construct drift scenarios by selecting whether\nto use drifted or original data/workloads for training and testing.\nBy default, we train or initialize learned database components using\ndrifted data and workloads generated based on a specific ğ‘‘, and\nevaluate them on a fixed test set sampled from the original data\nand workloads. After repeating this process with different drift\nfactors, we can construct several drift scenarios and ensure the\ncomparability of results across varying drift scenarios.\nRunning Example. We now provide a running example to demon-\nstrate the use of NeurBench in generating drifted data and work-\nloads with a specified drift factor ğ‘‘=0.1. For data, let us consider an\noriginal user table r1with attributes including user name, age, edu-\ncation level (Edu), and balance (Bal). As shown in Figure 1, we train\na diffuser and a drifter for r1. The diffuser learns the underlying data\ndistribution, and therefore, it captures the inherent correlations,such as the tendency of older users with higher education levels to\nhave larger account balances. Further, the drifter introduces certain\ndrift while preserving correlations, e.g., as the average age slightly\ndecreases, the education level distribution drifts accordingly, and\nbalances are reduced overall. We then provide the drift factor ğ‘‘to\nthe drifter, allowing precise control over the intensity of the drift\nintroduced. Guided by the drifter, the diffuser generates the drifted\ntable râ€²\n1, ensuring that the original correlations are preserved while\naligning with the specified drift factor. For the workload generation,\nwe model the workload as a set of query patterns, such as q1for pay-\nment processing and q2for auditing, each following specific arrival\nrates, as shown in Figure 1. We then train a specialized diffuser\nand drifter to synthesize the workload with drifted queries and\ncorresponding arrival rates. For instance, after applying drift, q1\nmay change its focus to 30% of user X and 70% of user Y, indicating\nan increased demand for auditing processes.\nWithout loss of generality, we take an example of evaluating\nlearned query optimizers under data drift with two different drift\nfactors. We use the generator to derive two drifted tables from r1,\nnamely râ€²\n1andrâ€²â€²\n1, each corresponding to a specified drift factor.\nThe learned query optimizer is then trained separately on râ€²\n1andrâ€²â€²\n1\nand evaluated using a consistent test set derived from the original\ntable r1to ensure fair and comparable performance evaluations.\n4 CONTROLLED DRIFT GENERATION\nIn this section, we formalize the drift factor and introduce the drift-\naware data and workload generation framework in detail.\n4.1 Drift Factor Modeling\nWe aim to define a drift factor that can uniformly and effectively\ndescribe the differences in data and workloads over time, thus\nfacilitating the generation of new data or workloads from initial\nones with specific drift. In our problem setting, the drift factor\nmust satisfy two key properties: 1) It should accurately capture the\ndifferences between data or workloads while providing a bounded\nvalue range for users to specify the desired drift. 2) Its value should\nprovide a consistent representation for all types of drift.\nAccording to Definition 3, where drift is mapped to the underly-\ning distribution of data and workloads, we define the drift factor\nbased on the distance of distributions. To satisfy the properties\n\nNeurBench: Benchmarking Learned Database Components with Data and Workload Drift Modeling\nabove, we utilize the Jensen-Shannon (JS) divergence [38]. Specifi-\ncally, given a distribution Pand the drifted one Pâ€², their JS diver-\ngenceDğ½ğ‘†(Pâˆ¥Pâ€²)is defined as:\nDğ½ğ‘†(Pâˆ¥Pâ€²)=1\n2Dğ¾ğ¿(Pâˆ¥ğ‘€)+1\n2Dğ¾ğ¿(Pâ€²âˆ¥M),\nwhere M=1\n2(P+Pâ€²), andğ·ğ¾ğ¿is the Kullbackâ€“Leibler (KL) diver-\ngence, which is defined by:\nDğ¾ğ¿(Pâˆ¥Pâ€²)=âˆ‘ï¸\nğ‘¥P(ğ‘¥)logP(ğ‘¥)\nPâ€²(ğ‘¥).\nDğ½ğ‘†satisfiesDğ½ğ‘†(Pâˆ¥Pâ€²)=Dğ½ğ‘†(Pâ€²âˆ¥P), and is bounded within\n[0,log 2]. Normalizing it by log 2 , the drift factor ğ‘‘is defined as:\nğ‘‘(P,Pâ€²)=Dğ½ğ‘†(Pâˆ¥Pâ€²)\nlog 2, ğ‘‘âˆˆ[0,1].\nAs drift is measured based on the differences between distribu-\ntions, a critical challenge is effectively mapping data or workloads\nto their corresponding distributions, denoted as P. For data, we\nconsider a relational database Rcontaining multiple tables, each\ncomprising a set of attributes. To approximate the distribution PR,\nwe decompose the joint distribution into their marginal attribute\ndistributions, i.e.,PR(ğ´1,ğ´2,...,ğ´ğ‘š)â‰ˆÃğ‘š\nğ‘–=1P(ğ´ğ‘–), where P(ğ´ğ‘–)\nrepresents the distribution of the ğ‘–-th attribute. We would like to\nnote that measuring the distance between marginal distributions\nremains an effective approach to quantifying overall distributional\ndrift, regardless of underlying correlations, as our focus is solely on\ncapturing distributional differences. We exclude primary keys and\nattributes with unique constraints from the distribution computa-\ntion, as their values are inherently uniform or deterministic. For the\nremaining attributes, we employ specific strategies based on their\ndata types to approximate their distributions. Categorical attributes\n(e.g.,CHAR ,VARCHAR ) and discrete numerical values are handled\nby computing distributions over all observed values in the table.\nFor example, if a column contains values ğ´,ğµ,ğ¶, we calculate the\nprobability of each value occurring: Pr(ğ´),Pr(ğµ),Pr(ğ¶). Continu-\nous numerical attributes are approximated by discretizing values\ninto a fixed number of bins. The corresponding distribution is then\nestimated by computing the probability density of each bin. This\nbinning strategy is commonly employed in database histograms to\nestimate attribute distributions and query selectivity. Moreover, the\nnumber and range of bins are determined based on the observed\nvalue range to balance precision and computational efficiency. For\nexample, a numerical column with values in the range [0, 100] can\nbe divided into 10 bins, and the frequency of values in each bin\ndetermines the distribution.\nFor a workload consisting of ğ‘queries Q={q1,q2,..., qğ‘},\nwe define the workload distribution PQas the distribution over\nquery templates . Specific features, such as query patterns, query\narrival rates, etc., characterize each query template. To model query\npatterns and arrival rates in a unified manner, we define a workload\ntable with 3 attributes: Join Pattern, Predicate, and Query Interval.\nThe Join Pattern and Predicate columns capture the query pattern,\nrepresenting table join patterns ( e.g.,ğ‘Ÿ1âŠ² âŠ³ğ‘Ÿ2), and predicates ( e.g.,\nBal = â€˜10Kâ€™). The Query Interval column quantifies the arrival rate\nby recording the time gap between consecutive queries. We control\nthe distributions of the workload table to synthesize workloads with\nquery pattern drift, arrival rate drift, or both, while preserving theinherent correlations. For example, we can introduce a workload,\nwhere queries with a certain join pattern become more frequent,\nwhile simultaneously increasing the corresponding query arrival\nrates by reducing the query interval ( e.g., from 2.0s to 0.5s), thereby\ninducing drift in both query patterns and arrival rates. Note that\nthe workload table used here is for illustration purposes; it can be\nextended with additional attributes for more complex queries.\n4.2 Drift-aware Data and Workload Generation\nAfter defining the drift factor ğ‘‘, the next problem is to generate\ndrifted data and workloads that satisfy a specified ğ‘‘. To address\nthis, we first provide a formal problem definition and then explain\nhow to construct the drift-aware data and workload generator.\n4.2.1 Problem Definition. We define a database running in a time\nwindow Twithğ‘(ğ‘â‰¥2)time slots, i.e.,T=(ğ‘¡ğ‘ 1,ğ‘¡ğ‘ 2,...,ğ‘¡ğ‘ ğ‘)âŠ¤.\nRecall that we denote RandQas random variables of the data and\nworkload in the database, respectively, and we map them to their\nunderlying distributions P(ğ‘–)\nRandP(ğ‘–)\nQatğ‘¡ğ‘ ğ‘–. Formally, we define\nthe drift-aware data and workload generation below.\nDefinition 4 (Drift-aware Data and Workload Generation) .Given a\nseries of drift factors D=(ğ‘‘1,ğ‘‘2,...,ğ‘‘ğ‘)âŠ¤, where each drift factor ğ‘‘ğ‘–\nquantifies the degree of change in the distribution between adjacent\ntime slots(ğ‘¡ğ‘ ğ‘—,ğ‘¡ğ‘ ğ‘–). The distribution of either data or workload at\ntimeğ‘¡ğ‘ ğ‘—is obtained by transforming the distribution at time ğ‘¡ğ‘ ğ‘–\nwith a function ğ‘”, according to the drift factor ğ‘‘ğ‘–:\nP(ğ‘—)\nR=ğ‘”R(P(ğ‘–)\nR,ğ‘‘ğ‘–,C),P(ğ‘—)\nQ=ğ‘”Q(P(ğ‘–)\nQ,ğ‘‘ğ‘–,C),\nwhereCis the corresponding inherent correlations, and ğ‘”Z:Î©ZÃ—\n[0,1]Ã—Câ†’ Î©Zis the drift function applied on input space Z,\ntransforming its distribution on the same domain.\nGiven a distribution P(ğ‘–)and the drift factor ğ‘‘ğ‘–, the generated\nnew distribution P(ğ‘—)should 1) exhibit drift quantified by ğ‘‘from\nP(ğ‘–); and 2) preserve inherent correlations C. Mathematically, this\nis equivalent to finding a hypersurface in Î©Z, the domain of input\nspace Z(i.e., either data and workload), which satisfies the equation:\nğ‘‘(Pğ‘–,Pğ‘—)â‰ˆğ‘‘âˆ—subject toC(Pğ‘—)â‰ˆC( Pğ‘–). (1)\nHowever, there are infinite solutions to Equation 1, which makes it\ndifficult to interpret and control. To solve this, we limit the expres-\nsiveness of Pğ‘—through a DDPM-based generator.\n4.2.2 Drift-aware Data and Workload Generator Construction. We\nnow introduce the design of our proposed drift-aware data and\nworkload generator, which synthesizes drifted data and workloads\nwhile preserving inherent correlations. Below, we present the theo-\nretical foundations, describe the generatorâ€™s training mechanism,\nand illustrate the complete generation process.\nTheoretical Analysis. Given a desired drift factor ğ‘‘, our objective\nas outlined in Section 4.2.1 to generate the drifted data xğ‘‘ğ‘Ÿğ‘–ğ‘“ğ‘¡ can\nbe expressed as sampling from ğ‘(xğ‘¡âˆ’1|xğ‘¡,ğ‘‘), a conditional DDPM\nreverse distribution that incorporates the desired drift ğ‘‘. As outlined\nin Section 2.3, ğ‘(xğ‘¡âˆ’1|xğ‘¡,ğ‘‘)can be decomposed as:\nğ‘(xğ‘¡âˆ’1|xğ‘¡,ğ‘‘)âˆğ‘(xğ‘¡âˆ’1|xğ‘¡)Pr(ğ‘‘|xğ‘¡âˆ’1,xğ‘¡), (2)\nwhereğ‘(xğ‘¡âˆ’1|xğ‘¡)can be modeled by the denoising process of\nDDPM, i.e.,ğ‘(xğ‘¡âˆ’1|xğ‘¡)=N(xğ‘¡âˆ’1;ğ(xğ‘¡,ğ‘¡;ğœƒ),ğœ2\nğ‘¡I), and Pr(ğ‘‘|xğ‘¡âˆ’1,xğ‘¡)\n\nZhanhao Zhao, Haotian Gao, Naili Xing, Lingze Zeng, Meihui Zhang,\nGang Chen, Manuel Rigger, Beng Chin Ooi\nOriginal Data/ Workload+â€¦Diï¬€used DataEncoded Dataxt\n<latexit sha1_base64=\"HfebjsKgnXqJnWLbvsuYCnX+kbE=\">AAAB9XicbVDLTsJAFL3FF+ILdemmEUxckZaNLoluXGIijwQqmQ5TmDCdNjO3Kmn4DzcuNMat/+LOv3EKXSh4kklOzrk398zxY8E1Os63VVhb39jcKm6Xdnb39g/Kh0dtHSWKshaNRKS6PtFMcMlayFGwbqwYCX3BOv7kOvM7D0xpHsk7nMbMC8lI8oBTgka6r/ZDgmM/SJ9mA6wOyhWn5sxhrxI3JxXI0RyUv/rDiCYhk0gF0brnOjF6KVHIqWCzUj/RLCZ0QkasZ6gkIdNeOk89s8+MMrSDSJkn0Z6rvzdSEmo9DX0zmYXUy14m/uf1EgwuvZTLOEEm6eJQkAgbIzurwB5yxSiKqSGEKm6y2nRMFKFoiiqZEtzlL6+Sdr3mOjX3tl5pXOV1FOEETuEcXLiABtxAE1pAQcEzvMKb9Wi9WO/Wx2K0YOU7x/AH1ucPTVaSXQ==</latexit><latexit sha1_base64=\"HfebjsKgnXqJnWLbvsuYCnX+kbE=\">AAAB9XicbVDLTsJAFL3FF+ILdemmEUxckZaNLoluXGIijwQqmQ5TmDCdNjO3Kmn4DzcuNMat/+LOv3EKXSh4kklOzrk398zxY8E1Os63VVhb39jcKm6Xdnb39g/Kh0dtHSWKshaNRKS6PtFMcMlayFGwbqwYCX3BOv7kOvM7D0xpHsk7nMbMC8lI8oBTgka6r/ZDgmM/SJ9mA6wOyhWn5sxhrxI3JxXI0RyUv/rDiCYhk0gF0brnOjF6KVHIqWCzUj/RLCZ0QkasZ6gkIdNeOk89s8+MMrSDSJkn0Z6rvzdSEmo9DX0zmYXUy14m/uf1EgwuvZTLOEEm6eJQkAgbIzurwB5yxSiKqSGEKm6y2nRMFKFoiiqZEtzlL6+Sdr3mOjX3tl5pXOV1FOEETuEcXLiABtxAE1pAQcEzvMKb9Wi9WO/Wx2K0YOU7x/AH1ucPTVaSXQ==</latexit><latexit sha1_base64=\"HfebjsKgnXqJnWLbvsuYCnX+kbE=\">AAAB9XicbVDLTsJAFL3FF+ILdemmEUxckZaNLoluXGIijwQqmQ5TmDCdNjO3Kmn4DzcuNMat/+LOv3EKXSh4kklOzrk398zxY8E1Os63VVhb39jcKm6Xdnb39g/Kh0dtHSWKshaNRKS6PtFMcMlayFGwbqwYCX3BOv7kOvM7D0xpHsk7nMbMC8lI8oBTgka6r/ZDgmM/SJ9mA6wOyhWn5sxhrxI3JxXI0RyUv/rDiCYhk0gF0brnOjF6KVHIqWCzUj/RLCZ0QkasZ6gkIdNeOk89s8+MMrSDSJkn0Z6rvzdSEmo9DX0zmYXUy14m/uf1EgwuvZTLOEEm6eJQkAgbIzurwB5yxSiKqSGEKm6y2nRMFKFoiiqZEtzlL6+Sdr3mOjX3tl5pXOV1FOEETuEcXLiABtxAE1pAQcEzvMKb9Wi9WO/Wx2K0YOU7x/AH1ucPTVaSXQ==</latexit><latexit sha1_base64=\"HfebjsKgnXqJnWLbvsuYCnX+kbE=\">AAAB9XicbVDLTsJAFL3FF+ILdemmEUxckZaNLoluXGIijwQqmQ5TmDCdNjO3Kmn4DzcuNMat/+LOv3EKXSh4kklOzrk398zxY8E1Os63VVhb39jcKm6Xdnb39g/Kh0dtHSWKshaNRKS6PtFMcMlayFGwbqwYCX3BOv7kOvM7D0xpHsk7nMbMC8lI8oBTgka6r/ZDgmM/SJ9mA6wOyhWn5sxhrxI3JxXI0RyUv/rDiCYhk0gF0brnOjF6KVHIqWCzUj/RLCZ0QkasZ6gkIdNeOk89s8+MMrSDSJkn0Z6rvzdSEmo9DX0zmYXUy14m/uf1EgwuvZTLOEEm6eJQkAgbIzurwB5yxSiKqSGEKm6y2nRMFKFoiiqZEtzlL6+Sdr3mOjX3tl5pXOV1FOEETuEcXLiABtxAE1pAQcEzvMKb9Wi9WO/Wx2K0YOU7x/AH1ucPTVaSXQ==</latexit>x0\n<latexit sha1_base64=\"uB3irnKNb12ifd/yS98U6ieoKCU=\">AAAB9XicbVC7TsMwFL0pr1JeBUYWixaJqUq6wFjBwlgk+pDaUDmu01q1nch2gCrqf7AwgBAr/8LG3+C0GaDlSJaOzrlX9/gEMWfauO63U1hb39jcKm6Xdnb39g/Kh0dtHSWK0BaJeKS6AdaUM0lbhhlOu7GiWAScdoLJdeZ3HqjSLJJ3ZhpTX+CRZCEj2FjpvtoX2IyDMH2aDdzqoFxxa+4caJV4OalAjuag/NUfRiQRVBrCsdY9z42Nn2JlGOF0VuonmsaYTPCI9iyVWFDtp/PUM3RmlSEKI2WfNGiu/t5IsdB6KgI7mYXUy14m/uf1EhNe+imTcWKoJItDYcKRiVBWARoyRYnhU0swUcxmRWSMFSbGFlWyJXjLX14l7XrNc2vebb3SuMrrKMIJnMI5eHABDbiBJrSAgIJneIU359F5cd6dj8Vowcl3juEPnM8f5fOSGQ==</latexit><latexit sha1_base64=\"uB3irnKNb12ifd/yS98U6ieoKCU=\">AAAB9XicbVC7TsMwFL0pr1JeBUYWixaJqUq6wFjBwlgk+pDaUDmu01q1nch2gCrqf7AwgBAr/8LG3+C0GaDlSJaOzrlX9/gEMWfauO63U1hb39jcKm6Xdnb39g/Kh0dtHSWK0BaJeKS6AdaUM0lbhhlOu7GiWAScdoLJdeZ3HqjSLJJ3ZhpTX+CRZCEj2FjpvtoX2IyDMH2aDdzqoFxxa+4caJV4OalAjuag/NUfRiQRVBrCsdY9z42Nn2JlGOF0VuonmsaYTPCI9iyVWFDtp/PUM3RmlSEKI2WfNGiu/t5IsdB6KgI7mYXUy14m/uf1EhNe+imTcWKoJItDYcKRiVBWARoyRYnhU0swUcxmRWSMFSbGFlWyJXjLX14l7XrNc2vebb3SuMrrKMIJnMI5eHABDbiBJrSAgIJneIU359F5cd6dj8Vowcl3juEPnM8f5fOSGQ==</latexit><latexit sha1_base64=\"uB3irnKNb12ifd/yS98U6ieoKCU=\">AAAB9XicbVC7TsMwFL0pr1JeBUYWixaJqUq6wFjBwlgk+pDaUDmu01q1nch2gCrqf7AwgBAr/8LG3+C0GaDlSJaOzrlX9/gEMWfauO63U1hb39jcKm6Xdnb39g/Kh0dtHSWK0BaJeKS6AdaUM0lbhhlOu7GiWAScdoLJdeZ3HqjSLJJ3ZhpTX+CRZCEj2FjpvtoX2IyDMH2aDdzqoFxxa+4caJV4OalAjuag/NUfRiQRVBrCsdY9z42Nn2JlGOF0VuonmsaYTPCI9iyVWFDtp/PUM3RmlSEKI2WfNGiu/t5IsdB6KgI7mYXUy14m/uf1EhNe+imTcWKoJItDYcKRiVBWARoyRYnhU0swUcxmRWSMFSbGFlWyJXjLX14l7XrNc2vebb3SuMrrKMIJnMI5eHABDbiBJrSAgIJneIU359F5cd6dj8Vowcl3juEPnM8f5fOSGQ==</latexit><latexit sha1_base64=\"uB3irnKNb12ifd/yS98U6ieoKCU=\">AAAB9XicbVC7TsMwFL0pr1JeBUYWixaJqUq6wFjBwlgk+pDaUDmu01q1nch2gCrqf7AwgBAr/8LG3+C0GaDlSJaOzrlX9/gEMWfauO63U1hb39jcKm6Xdnb39g/Kh0dtHSWK0BaJeKS6AdaUM0lbhhlOu7GiWAScdoLJdeZ3HqjSLJJ3ZhpTX+CRZCEj2FjpvtoX2IyDMH2aDdzqoFxxa+4caJV4OalAjuag/NUfRiQRVBrCsdY9z42Nn2JlGOF0VuonmsaYTPCI9iyVWFDtp/PUM3RmlSEKI2WfNGiu/t5IsdB6KgI7mYXUy14m/uf1EhNe+imTcWKoJItDYcKRiVBWARoyRYnhU0swUcxmRWSMFSbGFlWyJXjLX14l7XrNc2vebb3SuMrrKMIJnMI5eHABDbiBJrSAgIJneIU359F5cd6dj8Vowcl3juEPnM8f5fOSGQ==</latexit>Added Noiseâœt\n<latexit sha1_base64=\"V/GU86qO18wYODbwfVC0XCbD0hs=\">AAAB83icbVBNTwIxEO3iF+IX6tFLI5h4Irtc9Ej04hETARN2Q7qlCw3dtmlnTciGv+HFg8Z49c94899YYA8KvmSSl/dmMjMv1oJb8P1vr7SxubW9U96t7O0fHB5Vj0+6VmWGsg5VQpnHmFgmuGQd4CDYozaMpLFgvXhyO/d7T8xYruQDTDWLUjKSPOGUgJPCesi05ULJAdQH1Zrf8BfA6yQoSA0VaA+qX+FQ0SxlEqgg1vYDX0OUEwOcCjarhJllmtAJGbG+o5KkzEb54uYZvnDKECfKuJKAF+rviZyk1k7T2HWmBMZ21ZuL/3n9DJLrKOdSZ8AkXS5KMoFB4XkAeMgNoyCmjhBquLsV0zExhIKLqeJCCFZfXifdZiPwG8F9s9a6KeIoozN0ji5RgK5QC92hNuogijR6Rq/ozcu8F+/d+1i2lrxi5hT9gff5A5nPkWE=</latexit><latexit sha1_base64=\"V/GU86qO18wYODbwfVC0XCbD0hs=\">AAAB83icbVBNTwIxEO3iF+IX6tFLI5h4Irtc9Ej04hETARN2Q7qlCw3dtmlnTciGv+HFg8Z49c94899YYA8KvmSSl/dmMjMv1oJb8P1vr7SxubW9U96t7O0fHB5Vj0+6VmWGsg5VQpnHmFgmuGQd4CDYozaMpLFgvXhyO/d7T8xYruQDTDWLUjKSPOGUgJPCesi05ULJAdQH1Zrf8BfA6yQoSA0VaA+qX+FQ0SxlEqgg1vYDX0OUEwOcCjarhJllmtAJGbG+o5KkzEb54uYZvnDKECfKuJKAF+rviZyk1k7T2HWmBMZ21ZuL/3n9DJLrKOdSZ8AkXS5KMoFB4XkAeMgNoyCmjhBquLsV0zExhIKLqeJCCFZfXifdZiPwG8F9s9a6KeIoozN0ji5RgK5QC92hNuogijR6Rq/ozcu8F+/d+1i2lrxi5hT9gff5A5nPkWE=</latexit><latexit sha1_base64=\"V/GU86qO18wYODbwfVC0XCbD0hs=\">AAAB83icbVBNTwIxEO3iF+IX6tFLI5h4Irtc9Ej04hETARN2Q7qlCw3dtmlnTciGv+HFg8Z49c94899YYA8KvmSSl/dmMjMv1oJb8P1vr7SxubW9U96t7O0fHB5Vj0+6VmWGsg5VQpnHmFgmuGQd4CDYozaMpLFgvXhyO/d7T8xYruQDTDWLUjKSPOGUgJPCesi05ULJAdQH1Zrf8BfA6yQoSA0VaA+qX+FQ0SxlEqgg1vYDX0OUEwOcCjarhJllmtAJGbG+o5KkzEb54uYZvnDKECfKuJKAF+rviZyk1k7T2HWmBMZ21ZuL/3n9DJLrKOdSZ8AkXS5KMoFB4XkAeMgNoyCmjhBquLsV0zExhIKLqeJCCFZfXifdZiPwG8F9s9a6KeIoozN0ji5RgK5QC92hNuogijR6Rq/ozcu8F+/d+1i2lrxi5hT9gff5A5nPkWE=</latexit><latexit sha1_base64=\"V/GU86qO18wYODbwfVC0XCbD0hs=\">AAAB83icbVBNTwIxEO3iF+IX6tFLI5h4Irtc9Ej04hETARN2Q7qlCw3dtmlnTciGv+HFg8Z49c94899YYA8KvmSSl/dmMjMv1oJb8P1vr7SxubW9U96t7O0fHB5Vj0+6VmWGsg5VQpnHmFgmuGQd4CDYozaMpLFgvXhyO/d7T8xYruQDTDWLUjKSPOGUgJPCesi05ULJAdQH1Zrf8BfA6yQoSA0VaA+qX+FQ0SxlEqgg1vYDX0OUEwOcCjarhJllmtAJGbG+o5KkzEb54uYZvnDKECfKuJKAF+rviZyk1k7T2HWmBMZ21ZuL/3n9DJLrKOdSZ8AkXS5KMoFB4XkAeMgNoyCmjhBquLsV0zExhIKLqeJCCFZfXifdZiPwG8F9s9a6KeIoozN0ji5RgK5QC92hNuogijR6Rq/ozcu8F+/d+1i2lrxi5hT9gff5A5nPkWE=</latexit>Fitted NoiseDiï¬€user LossËœâœt\n<latexit sha1_base64=\"MFXjOuceNSdKm/Sf+ApEM9ucxN8=\">AAAB/XicbVC5TsNAEF2HK4TLHB2NRYJEFdlpoIygoQwSOaTYitbrcbLK+tDuGClYEb9CQwFCtPwHHX/DJnEBCU8a6em9Gc3M81PBFdr2t1FaW9/Y3CpvV3Z29/YPzMOjjkoyyaDNEpHInk8VCB5DGzkK6KUSaOQL6Prjm5nffQCpeBLf4yQFL6LDmIecUdTSwDypuchFALkLqeJCSzitDcyqXbfnsFaJU5AqKdAamF9ukLAsghiZoEr1HTtFL6cSORMwrbiZgpSyMR1CX9OYRqC8fH791DrXSmCFidQVozVXf0/kNFJqEvm6M6I4UsveTPzP62cYXnk5j9MMIWaLRWEmLEysWRRWwCUwFBNNKJNc32qxEZWUoQ6sokNwll9eJZ1G3bHrzl2j2rwu4iiTU3JGLohDLkmT3JIWaRNGHskzeSVvxpPxYrwbH4vWklHMHJM/MD5/AJJblUg=</latexit><latexit sha1_base64=\"MFXjOuceNSdKm/Sf+ApEM9ucxN8=\">AAAB/XicbVC5TsNAEF2HK4TLHB2NRYJEFdlpoIygoQwSOaTYitbrcbLK+tDuGClYEb9CQwFCtPwHHX/DJnEBCU8a6em9Gc3M81PBFdr2t1FaW9/Y3CpvV3Z29/YPzMOjjkoyyaDNEpHInk8VCB5DGzkK6KUSaOQL6Prjm5nffQCpeBLf4yQFL6LDmIecUdTSwDypuchFALkLqeJCSzitDcyqXbfnsFaJU5AqKdAamF9ukLAsghiZoEr1HTtFL6cSORMwrbiZgpSyMR1CX9OYRqC8fH791DrXSmCFidQVozVXf0/kNFJqEvm6M6I4UsveTPzP62cYXnk5j9MMIWaLRWEmLEysWRRWwCUwFBNNKJNc32qxEZWUoQ6sokNwll9eJZ1G3bHrzl2j2rwu4iiTU3JGLohDLkmT3JIWaRNGHskzeSVvxpPxYrwbH4vWklHMHJM/MD5/AJJblUg=</latexit><latexit sha1_base64=\"MFXjOuceNSdKm/Sf+ApEM9ucxN8=\">AAAB/XicbVC5TsNAEF2HK4TLHB2NRYJEFdlpoIygoQwSOaTYitbrcbLK+tDuGClYEb9CQwFCtPwHHX/DJnEBCU8a6em9Gc3M81PBFdr2t1FaW9/Y3CpvV3Z29/YPzMOjjkoyyaDNEpHInk8VCB5DGzkK6KUSaOQL6Prjm5nffQCpeBLf4yQFL6LDmIecUdTSwDypuchFALkLqeJCSzitDcyqXbfnsFaJU5AqKdAamF9ukLAsghiZoEr1HTtFL6cSORMwrbiZgpSyMR1CX9OYRqC8fH791DrXSmCFidQVozVXf0/kNFJqEvm6M6I4UsveTPzP62cYXnk5j9MMIWaLRWEmLEysWRRWwCUwFBNNKJNc32qxEZWUoQ6sokNwll9eJZ1G3bHrzl2j2rwu4iiTU3JGLohDLkmT3JIWaRNGHskzeSVvxpPxYrwbH4vWklHMHJM/MD5/AJJblUg=</latexit><latexit sha1_base64=\"MFXjOuceNSdKm/Sf+ApEM9ucxN8=\">AAAB/XicbVC5TsNAEF2HK4TLHB2NRYJEFdlpoIygoQwSOaTYitbrcbLK+tDuGClYEb9CQwFCtPwHHX/DJnEBCU8a6em9Gc3M81PBFdr2t1FaW9/Y3CpvV3Z29/YPzMOjjkoyyaDNEpHInk8VCB5DGzkK6KUSaOQL6Prjm5nffQCpeBLf4yQFL6LDmIecUdTSwDypuchFALkLqeJCSzitDcyqXbfnsFaJU5AqKdAamF9ukLAsghiZoEr1HTtFL6cSORMwrbiZgpSyMR1CX9OYRqC8fH791DrXSmCFidQVozVXf0/kNFJqEvm6M6I4UsveTPzP62cYXnk5j9MMIWaLRWEmLEysWRRWwCUwFBNNKJNc32qxEZWUoQ6sokNwll9eJZ1G3bHrzl2j2rwu4iiTU3JGLohDLkmT3JIWaRNGHskzeSVvxpPxYrwbH4vWklHMHJM/MD5/AJJblUg=</latexit>Drifter LossDrift FactorFitted Drifted datad<latexit sha1_base64=\"FPG1C7oYrlV8KfgM3wYALcTsB4k=\">AAAB6nicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsaLYk2lhjlI4EL2dvbgw17e5fdORNC+Ak2Fhpj6y+y89+4wBUKvmSSl/dmMjMvSKUw6LrfTmFjc2t7p7hb2ts/ODwqH5+0TZJpxlsskYnuBtRwKRRvoUDJu6nmNA4k7wTj27nfeeLaiEQ94iTlfkyHSkSCUbTSQzWsDsoVt+YuQNaJl5MK5GgOyl/9MGFZzBUySY3peW6K/pRqFEzyWamfGZ5SNqZD3rNU0Zgbf7o4dUYurBKSKNG2FJKF+ntiSmNjJnFgO2OKI7PqzcX/vF6G0bU/FSrNkCu2XBRlkmBC5n+TUGjOUE4soUwLeythI6opQ5tOyYbgrb68Ttr1mufWvPt6pXGTx1GEMziHS/DgChpwB01oAYMhPMMrvDnSeXHenY9la8HJZ07hD5zPH37ujUI=</latexit><latexit sha1_base64=\"FPG1C7oYrlV8KfgM3wYALcTsB4k=\">AAAB6nicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsaLYk2lhjlI4EL2dvbgw17e5fdORNC+Ak2Fhpj6y+y89+4wBUKvmSSl/dmMjMvSKUw6LrfTmFjc2t7p7hb2ts/ODwqH5+0TZJpxlsskYnuBtRwKRRvoUDJu6nmNA4k7wTj27nfeeLaiEQ94iTlfkyHSkSCUbTSQzWsDsoVt+YuQNaJl5MK5GgOyl/9MGFZzBUySY3peW6K/pRqFEzyWamfGZ5SNqZD3rNU0Zgbf7o4dUYurBKSKNG2FJKF+ntiSmNjJnFgO2OKI7PqzcX/vF6G0bU/FSrNkCu2XBRlkmBC5n+TUGjOUE4soUwLeythI6opQ5tOyYbgrb68Ttr1mufWvPt6pXGTx1GEMziHS/DgChpwB01oAYMhPMMrvDnSeXHenY9la8HJZ07hD5zPH37ujUI=</latexit><latexit sha1_base64=\"FPG1C7oYrlV8KfgM3wYALcTsB4k=\">AAAB6nicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsaLYk2lhjlI4EL2dvbgw17e5fdORNC+Ak2Fhpj6y+y89+4wBUKvmSSl/dmMjMvSKUw6LrfTmFjc2t7p7hb2ts/ODwqH5+0TZJpxlsskYnuBtRwKRRvoUDJu6nmNA4k7wTj27nfeeLaiEQ94iTlfkyHSkSCUbTSQzWsDsoVt+YuQNaJl5MK5GgOyl/9MGFZzBUySY3peW6K/pRqFEzyWamfGZ5SNqZD3rNU0Zgbf7o4dUYurBKSKNG2FJKF+ntiSmNjJnFgO2OKI7PqzcX/vF6G0bU/FSrNkCu2XBRlkmBC5n+TUGjOUE4soUwLeythI6opQ5tOyYbgrb68Ttr1mufWvPt6pXGTx1GEMziHS/DgChpwB01oAYMhPMMrvDnSeXHenY9la8HJZ07hD5zPH37ujUI=</latexit><latexit sha1_base64=\"FPG1C7oYrlV8KfgM3wYALcTsB4k=\">AAAB6nicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsaLYk2lhjlI4EL2dvbgw17e5fdORNC+Ak2Fhpj6y+y89+4wBUKvmSSl/dmMjMvSKUw6LrfTmFjc2t7p7hb2ts/ODwqH5+0TZJpxlsskYnuBtRwKRRvoUDJu6nmNA4k7wTj27nfeeLaiEQ94iTlfkyHSkSCUbTSQzWsDsoVt+YuQNaJl5MK5GgOyl/9MGFZzBUySY3peW6K/pRqFEzyWamfGZ5SNqZD3rNU0Zgbf7o4dUYurBKSKNG2FJKF+ntiSmNjJnFgO2OKI7PqzcX/vF6G0bU/FSrNkCu2XBRlkmBC5n+TUGjOUE4soUwLeythI6opQ5tOyYbgrb68Ttr1mufWvPt6pXGTx1GEMziHS/DgChpwB01oAYMhPMMrvDnSeXHenY9la8HJZ07hD5zPH37ujUI=</latexit>Drifter Trainingdâ‡¤\n<latexit sha1_base64=\"oECCx57NRmezoEg9DivfG3Ym5rI=\">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjAxHkjLRY9ELx4xsUgClWy3W9iwu212tyak4Td48aAxXv1B3vw3LtCDgi+Z5OW9mczMC1POtHHdb6e0tr6xuVXeruzs7u0fVA+POjrJFKE+SXiiuiHWlDNJfcMMp91UUSxCTh/C8c3Mf3iiSrNE3ptJSgOBh5LFjGBjJb8ePV7UB9Wa23DnQKvEK0gNCrQH1a9+lJBMUGkIx1r3PDc1QY6VYYTTaaWfaZpiMsZD2rNUYkF1kM+PnaIzq0QoTpQtadBc/T2RY6H1RIS2U2Az0sveTPzP62UmvgpyJtPMUEkWi+KMI5Og2ecoYooSwyeWYKKYvRWREVaYGJtPxYbgLb+8SjrNhuc2vLtmrXVdxFGGEziFc/DgElpwC23wgQCDZ3iFN0c6L86787FoLTnFzDH8gfP5A5jljd4=</latexit><latexit sha1_base64=\"oECCx57NRmezoEg9DivfG3Ym5rI=\">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjAxHkjLRY9ELx4xsUgClWy3W9iwu212tyak4Td48aAxXv1B3vw3LtCDgi+Z5OW9mczMC1POtHHdb6e0tr6xuVXeruzs7u0fVA+POjrJFKE+SXiiuiHWlDNJfcMMp91UUSxCTh/C8c3Mf3iiSrNE3ptJSgOBh5LFjGBjJb8ePV7UB9Wa23DnQKvEK0gNCrQH1a9+lJBMUGkIx1r3PDc1QY6VYYTTaaWfaZpiMsZD2rNUYkF1kM+PnaIzq0QoTpQtadBc/T2RY6H1RIS2U2Az0sveTPzP62UmvgpyJtPMUEkWi+KMI5Og2ecoYooSwyeWYKKYvRWREVaYGJtPxYbgLb+8SjrNhuc2vLtmrXVdxFGGEziFc/DgElpwC23wgQCDZ3iFN0c6L86787FoLTnFzDH8gfP5A5jljd4=</latexit><latexit sha1_base64=\"oECCx57NRmezoEg9DivfG3Ym5rI=\">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjAxHkjLRY9ELx4xsUgClWy3W9iwu212tyak4Td48aAxXv1B3vw3LtCDgi+Z5OW9mczMC1POtHHdb6e0tr6xuVXeruzs7u0fVA+POjrJFKE+SXiiuiHWlDNJfcMMp91UUSxCTh/C8c3Mf3iiSrNE3ptJSgOBh5LFjGBjJb8ePV7UB9Wa23DnQKvEK0gNCrQH1a9+lJBMUGkIx1r3PDc1QY6VYYTTaaWfaZpiMsZD2rNUYkF1kM+PnaIzq0QoTpQtadBc/T2RY6H1RIS2U2Az0sveTPzP62UmvgpyJtPMUEkWi+KMI5Og2ecoYooSwyeWYKKYvRWREVaYGJtPxYbgLb+8SjrNhuc2vLtmrXVdxFGGEziFc/DgElpwC23wgQCDZ3iFN0c6L86787FoLTnFzDH8gfP5A5jljd4=</latexit><latexit sha1_base64=\"oECCx57NRmezoEg9DivfG3Ym5rI=\">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjAxHkjLRY9ELx4xsUgClWy3W9iwu212tyak4Td48aAxXv1B3vw3LtCDgi+Z5OW9mczMC1POtHHdb6e0tr6xuVXeruzs7u0fVA+POjrJFKE+SXiiuiHWlDNJfcMMp91UUSxCTh/C8c3Mf3iiSrNE3ptJSgOBh5LFjGBjJb8ePV7UB9Wa23DnQKvEK0gNCrQH1a9+lJBMUGkIx1r3PDc1QY6VYYTTaaWfaZpiMsZD2rNUYkF1kM+PnaIzq0QoTpQtadBc/T2RY6H1RIS2U2Az0sveTPzP62UmvgpyJtPMUEkWi+KMI5Og2ecoYooSwyeWYKKYvRWREVaYGJtPxYbgLb+8SjrNhuc2vLtmrXVdxFGGEziFc/DgElpwC23wgQCDZ3iFN0c6L86787FoLTnFzDH8gfP5A5jljd4=</latexit>Diffâœ“(xt,t)\n<latexit sha1_base64=\"7cPZZmr+VGccLI6vrH/OmRgdc/8=\">AAACB3icbVDLSsNAFJ3UV62vqEtBBluhgpSkG10WdeGygn1AE8JkOmmHTh7M3IgldOfGX3HjQhG3/oI7/8Zpm4VWD1w4nHMv997jJ4IrsKwvo7C0vLK6VlwvbWxube+Yu3ttFaeSshaNRSy7PlFM8Ii1gINg3UQyEvqCdfzR5dTv3DGpeBzdwjhhbkgGEQ84JaAlzzysXPEg8BwYMiBVJyQw9IPsfuLBKYaTimeWrZo1A/5L7JyUUY6mZ346/ZimIYuACqJUz7YScDMigVPBJiUnVSwhdEQGrKdpREKm3Gz2xwQfa6WPg1jqigDP1J8TGQmVGoe+7pweqha9qfif10shOHczHiUpsIjOFwWpwBDjaSi4zyWjIMaaECq5vhXTIZGEgo6upEOwF1/+S9r1mm3V7Jt6uXGRx1FEB+gIVZGNzlADXaMmaiGKHtATekGvxqPxbLwZ7/PWgpHP7KNfMD6+AbNdmIk=</latexit><latexit sha1_base64=\"7cPZZmr+VGccLI6vrH/OmRgdc/8=\">AAACB3icbVDLSsNAFJ3UV62vqEtBBluhgpSkG10WdeGygn1AE8JkOmmHTh7M3IgldOfGX3HjQhG3/oI7/8Zpm4VWD1w4nHMv997jJ4IrsKwvo7C0vLK6VlwvbWxube+Yu3ttFaeSshaNRSy7PlFM8Ii1gINg3UQyEvqCdfzR5dTv3DGpeBzdwjhhbkgGEQ84JaAlzzysXPEg8BwYMiBVJyQw9IPsfuLBKYaTimeWrZo1A/5L7JyUUY6mZ346/ZimIYuACqJUz7YScDMigVPBJiUnVSwhdEQGrKdpREKm3Gz2xwQfa6WPg1jqigDP1J8TGQmVGoe+7pweqha9qfif10shOHczHiUpsIjOFwWpwBDjaSi4zyWjIMaaECq5vhXTIZGEgo6upEOwF1/+S9r1mm3V7Jt6uXGRx1FEB+gIVZGNzlADXaMmaiGKHtATekGvxqPxbLwZ7/PWgpHP7KNfMD6+AbNdmIk=</latexit><latexit sha1_base64=\"7cPZZmr+VGccLI6vrH/OmRgdc/8=\">AAACB3icbVDLSsNAFJ3UV62vqEtBBluhgpSkG10WdeGygn1AE8JkOmmHTh7M3IgldOfGX3HjQhG3/oI7/8Zpm4VWD1w4nHMv997jJ4IrsKwvo7C0vLK6VlwvbWxube+Yu3ttFaeSshaNRSy7PlFM8Ii1gINg3UQyEvqCdfzR5dTv3DGpeBzdwjhhbkgGEQ84JaAlzzysXPEg8BwYMiBVJyQw9IPsfuLBKYaTimeWrZo1A/5L7JyUUY6mZ346/ZimIYuACqJUz7YScDMigVPBJiUnVSwhdEQGrKdpREKm3Gz2xwQfa6WPg1jqigDP1J8TGQmVGoe+7pweqha9qfif10shOHczHiUpsIjOFwWpwBDjaSi4zyWjIMaaECq5vhXTIZGEgo6upEOwF1/+S9r1mm3V7Jt6uXGRx1FEB+gIVZGNzlADXaMmaiGKHtATekGvxqPxbLwZ7/PWgpHP7KNfMD6+AbNdmIk=</latexit><latexit sha1_base64=\"7cPZZmr+VGccLI6vrH/OmRgdc/8=\">AAACB3icbVDLSsNAFJ3UV62vqEtBBluhgpSkG10WdeGygn1AE8JkOmmHTh7M3IgldOfGX3HjQhG3/oI7/8Zpm4VWD1w4nHMv997jJ4IrsKwvo7C0vLK6VlwvbWxube+Yu3ttFaeSshaNRSy7PlFM8Ii1gINg3UQyEvqCdfzR5dTv3DGpeBzdwjhhbkgGEQ84JaAlzzysXPEg8BwYMiBVJyQw9IPsfuLBKYaTimeWrZo1A/5L7JyUUY6mZ346/ZimIYuACqJUz7YScDMigVPBJiUnVSwhdEQGrKdpREKm3Gz2xwQfa6WPg1jqigDP1J8TGQmVGoe+7pweqha9qfif10shOHczHiUpsIjOFwWpwBDjaSi4zyWjIMaaECq5vhXTIZGEgo6upEOwF1/+S9r1mm3V7Jt6uXGRx1FEB+gIVZGNzlADXaMmaiGKHtATekGvxqPxbLwZ7/PWgpHP7KNfMD6+AbNdmIk=</latexit>Drift\u0000(xt,t ,d)\n<latexit sha1_base64=\"nCnlvnjNHtEsVIf65a4rQYoCYWE=\">AAACCXicbVC7TsMwFHXKq5RXgZHFokUqEqqSLjBWwMBYJPqQ2ihyHKe16jiRfYOooq4s/AoLAwix8gds/A3uY4CWI1s6Oude3XuPnwiuwba/rdzK6tr6Rn6zsLW9s7tX3D9o6ThVlDVpLGLV8YlmgkvWBA6CdRLFSOQL1vaHVxO/fc+U5rG8g1HC3Ij0JQ85JWAkr4jL14qH4PWSAa/0IgIDP8wexh6cYfOC07JXLNlVewq8TJw5KaE5Gl7xqxfENI2YBCqI1l3HTsDNiAJOBRsXeqlmCaFD0mddQyWJmHaz6SVjfGKUAIexMl8Cnqq/OzISaT2KfFM52VUvehPxP6+bQnjhZlwmKTBJZ4PCVGCI8SQWHHDFKIiRIYQqbnbFdEAUoWDCK5gQnMWTl0mrVnXsqnNbK9Uv53Hk0RE6RhXkoHNURzeogZqIokf0jF7Rm/VkvVjv1sesNGfNew7RH1ifP554mPg=</latexit><latexit sha1_base64=\"nCnlvnjNHtEsVIf65a4rQYoCYWE=\">AAACCXicbVC7TsMwFHXKq5RXgZHFokUqEqqSLjBWwMBYJPqQ2ihyHKe16jiRfYOooq4s/AoLAwix8gds/A3uY4CWI1s6Oude3XuPnwiuwba/rdzK6tr6Rn6zsLW9s7tX3D9o6ThVlDVpLGLV8YlmgkvWBA6CdRLFSOQL1vaHVxO/fc+U5rG8g1HC3Ij0JQ85JWAkr4jL14qH4PWSAa/0IgIDP8wexh6cYfOC07JXLNlVewq8TJw5KaE5Gl7xqxfENI2YBCqI1l3HTsDNiAJOBRsXeqlmCaFD0mddQyWJmHaz6SVjfGKUAIexMl8Cnqq/OzISaT2KfFM52VUvehPxP6+bQnjhZlwmKTBJZ4PCVGCI8SQWHHDFKIiRIYQqbnbFdEAUoWDCK5gQnMWTl0mrVnXsqnNbK9Uv53Hk0RE6RhXkoHNURzeogZqIokf0jF7Rm/VkvVjv1sesNGfNew7RH1ifP554mPg=</latexit><latexit sha1_base64=\"nCnlvnjNHtEsVIf65a4rQYoCYWE=\">AAACCXicbVC7TsMwFHXKq5RXgZHFokUqEqqSLjBWwMBYJPqQ2ihyHKe16jiRfYOooq4s/AoLAwix8gds/A3uY4CWI1s6Oude3XuPnwiuwba/rdzK6tr6Rn6zsLW9s7tX3D9o6ThVlDVpLGLV8YlmgkvWBA6CdRLFSOQL1vaHVxO/fc+U5rG8g1HC3Ij0JQ85JWAkr4jL14qH4PWSAa/0IgIDP8wexh6cYfOC07JXLNlVewq8TJw5KaE5Gl7xqxfENI2YBCqI1l3HTsDNiAJOBRsXeqlmCaFD0mddQyWJmHaz6SVjfGKUAIexMl8Cnqq/OzISaT2KfFM52VUvehPxP6+bQnjhZlwmKTBJZ4PCVGCI8SQWHHDFKIiRIYQqbnbFdEAUoWDCK5gQnMWTl0mrVnXsqnNbK9Uv53Hk0RE6RhXkoHNURzeogZqIokf0jF7Rm/VkvVjv1sesNGfNew7RH1ifP554mPg=</latexit><latexit sha1_base64=\"nCnlvnjNHtEsVIf65a4rQYoCYWE=\">AAACCXicbVC7TsMwFHXKq5RXgZHFokUqEqqSLjBWwMBYJPqQ2ihyHKe16jiRfYOooq4s/AoLAwix8gds/A3uY4CWI1s6Oude3XuPnwiuwba/rdzK6tr6Rn6zsLW9s7tX3D9o6ThVlDVpLGLV8YlmgkvWBA6CdRLFSOQL1vaHVxO/fc+U5rG8g1HC3Ij0JQ85JWAkr4jL14qH4PWSAa/0IgIDP8wexh6cYfOC07JXLNlVewq8TJw5KaE5Gl7xqxfENI2YBCqI1l3HTsDNiAJOBRsXeqlmCaFD0mddQyWJmHaz6SVjfGKUAIexMl8Cnqq/OzISaT2KfFM52VUvehPxP6+bQnjhZlwmKTBJZ4PCVGCI8SQWHHDFKIiRIYQqbnbFdEAUoWDCK5gQnMWTl0mrVnXsqnNbK9Uv53Hk0RE6RhXkoHNURzeogZqIokf0jF7Rm/VkvVjv1sesNGfNew7RH1ifP554mPg=</latexit>Diï¬€user Trainingxdrift\n<latexit sha1_base64=\"d/LQ/I0pLkZYMGo4omQyXF10xQw=\">AAAB/XicbVC7TsMwFL3hWcorPDYWixaJqUq6wFjBwlgk+pDaKHIcp7XqPGQ7iBJF/AoLAwix8h9s/A1OmwFajmTp6Jx7dY+Pl3AmlWV9Gyura+sbm5Wt6vbO7t6+eXDYlXEqCO2QmMei72FJOYtoRzHFaT8RFIcepz1vcl34vXsqJIujOzVNqBPiUcQCRrDSkmse14chVmMvyB5yN/MFC1Red82a1bBmQMvELkkNSrRd82voxyQNaaQIx1IObCtRToaFYoTTvDpMJU0wmeARHWga4ZBKJ5ulz9GZVnwUxEK/SKGZ+nsjw6GU09DTk0VUuegV4n/eIFXBpZOxKEkVjcj8UJBypGJUVIF8JihRfKoJJoLprIiMscBE6cKqugR78cvLpNts2FbDvm3WWldlHRU4gVM4BxsuoAU30IYOEHiEZ3iFN+PJeDHejY/56IpR7hzBHxifP8IzlWc=</latexit><latexit sha1_base64=\"d/LQ/I0pLkZYMGo4omQyXF10xQw=\">AAAB/XicbVC7TsMwFL3hWcorPDYWixaJqUq6wFjBwlgk+pDaKHIcp7XqPGQ7iBJF/AoLAwix8h9s/A1OmwFajmTp6Jx7dY+Pl3AmlWV9Gyura+sbm5Wt6vbO7t6+eXDYlXEqCO2QmMei72FJOYtoRzHFaT8RFIcepz1vcl34vXsqJIujOzVNqBPiUcQCRrDSkmse14chVmMvyB5yN/MFC1Red82a1bBmQMvELkkNSrRd82voxyQNaaQIx1IObCtRToaFYoTTvDpMJU0wmeARHWga4ZBKJ5ulz9GZVnwUxEK/SKGZ+nsjw6GU09DTk0VUuegV4n/eIFXBpZOxKEkVjcj8UJBypGJUVIF8JihRfKoJJoLprIiMscBE6cKqugR78cvLpNts2FbDvm3WWldlHRU4gVM4BxsuoAU30IYOEHiEZ3iFN+PJeDHejY/56IpR7hzBHxifP8IzlWc=</latexit><latexit sha1_base64=\"d/LQ/I0pLkZYMGo4omQyXF10xQw=\">AAAB/XicbVC7TsMwFL3hWcorPDYWixaJqUq6wFjBwlgk+pDaKHIcp7XqPGQ7iBJF/AoLAwix8h9s/A1OmwFajmTp6Jx7dY+Pl3AmlWV9Gyura+sbm5Wt6vbO7t6+eXDYlXEqCO2QmMei72FJOYtoRzHFaT8RFIcepz1vcl34vXsqJIujOzVNqBPiUcQCRrDSkmse14chVmMvyB5yN/MFC1Red82a1bBmQMvELkkNSrRd82voxyQNaaQIx1IObCtRToaFYoTTvDpMJU0wmeARHWga4ZBKJ5ulz9GZVnwUxEK/SKGZ+nsjw6GU09DTk0VUuegV4n/eIFXBpZOxKEkVjcj8UJBypGJUVIF8JihRfKoJJoLprIiMscBE6cKqugR78cvLpNts2FbDvm3WWldlHRU4gVM4BxsuoAU30IYOEHiEZ3iFN+PJeDHejY/56IpR7hzBHxifP8IzlWc=</latexit><latexit sha1_base64=\"d/LQ/I0pLkZYMGo4omQyXF10xQw=\">AAAB/XicbVC7TsMwFL3hWcorPDYWixaJqUq6wFjBwlgk+pDaKHIcp7XqPGQ7iBJF/AoLAwix8h9s/A1OmwFajmTp6Jx7dY+Pl3AmlWV9Gyura+sbm5Wt6vbO7t6+eXDYlXEqCO2QmMei72FJOYtoRzHFaT8RFIcepz1vcl34vXsqJIujOzVNqBPiUcQCRrDSkmse14chVmMvyB5yN/MFC1Red82a1bBmQMvELkkNSrRd82voxyQNaaQIx1IObCtRToaFYoTTvDpMJU0wmeARHWga4ZBKJ5ulz9GZVnwUxEK/SKGZ+nsjw6GU09DTk0VUuegV4n/eIFXBpZOxKEkVjcj8UJBypGJUVIF8JihRfKoJJoLprIiMscBE6cKqugR78cvLpNts2FbDvm3WWldlHRU4gVM4BxsuoAU30IYOEHiEZ3iFN+PJeDHejY/56IpR7hzBHxifP8IzlWc=</latexit>\nFigure 2: Diffuser and Drifter Training\nguides the process to introduce the specified drift ğ‘‘. Since the drifted\ndata xğ‘‘ğ‘Ÿğ‘–ğ‘“ğ‘¡ is the target output, Equation (2) can be rewritten as:\nğ‘(xğ‘¡âˆ’1|xğ‘¡,xğ‘‘ğ‘Ÿğ‘–ğ‘“ğ‘¡)âˆğ‘(xğ‘¡âˆ’1|xğ‘¡)Pr(xğ‘‘ğ‘Ÿğ‘–ğ‘“ğ‘¡|xğ‘¡âˆ’1,xğ‘¡). (3)\nNote that the mean of the Gaussian distribution, ğ(xğ‘¡,ğ‘¡;ğœƒ), deter-\nmines the trajectory of the denoising process [ 11,36]. We therefore\nleverage Pr(xğ‘‘ğ‘Ÿğ‘–ğ‘“ğ‘¡|xğ‘¡âˆ’1,xğ‘¡)to introduce a controlled drift to the\nmean ğ.\nTheorem 1. The controlled reverse distribution ğ‘(xğ‘¡âˆ’1|xğ‘¡,xğ‘‘ğ‘Ÿğ‘–ğ‘“ğ‘¡)\nis able to be approximated using Pr(xğ‘‘ğ‘Ÿğ‘–ğ‘“ğ‘¡|xğ‘¡)and its gradient at\nxğ‘¡, orâˆ‡xğ‘¡Pr(xğ‘‘ğ‘Ÿğ‘–ğ‘“ğ‘¡|xğ‘¡).\nProof. For Equation (3), the first part is approximated by a\nGaussian model, while the second part can be simplified using\nconditional independence, which transforms the equation to:\nğ‘(xğ‘¡âˆ’1|xğ‘¡,xğ‘‘ğ‘Ÿğ‘–ğ‘“ğ‘¡)â‰ˆğ‘§N(xğ‘¡;ğ,Î£)Pr(xğ‘‘ğ‘Ÿğ‘–ğ‘“ğ‘¡|xğ‘¡),\nwhereğ‘§is the normalizing factor. Next, as proven by Dickstein\net al. [ 47], we can perform Taylor expansion around ğto further\napproximate ğ‘(xğ‘¡âˆ’1|xğ‘¡,xğ‘‘ğ‘Ÿğ‘–ğ‘“ğ‘¡)so that a perturbed Gaussian distri-\nbution can be used to model it:\nlogğ‘(xğ‘¡âˆ’1|xğ‘¡,xğ‘‘ğ‘Ÿğ‘–ğ‘“ğ‘¡)â‰ˆlogN(xğ‘¡;ğ,Î£)+logPr(xğ‘‘ğ‘Ÿğ‘–ğ‘“ğ‘¡|xğ‘¡)\nâ‰ˆâˆ’1\n2(xğ‘¡âˆ’ğ)âŠ¤Î£âˆ’1(xğ‘¡âˆ’ğ)+(xğ‘¡âˆ’ğ)Â·ğ‘”+ğ¶\n=âˆ’1\n2(xğ‘¡âˆ’ğâˆ’Î£Â·ğ‘”)âŠ¤Î£âˆ’1(xğ‘¡âˆ’ğâˆ’Î£Â·ğ‘”)+ğ¶\n=logN(xğ‘¡;ğ+Î£Â·ğ‘”,Î£),\nwhereğ‘”=âˆ‡xğ‘¡Pr(xğ‘‘ğ‘Ÿğ‘–ğ‘“ğ‘¡|xğ‘¡), and the constant term ğ¶can be ig-\nnored. In other words, ğ‘(xğ‘¡âˆ’1|xğ‘¡,xğ‘‘ğ‘Ÿğ‘–ğ‘“ğ‘¡)can be approximated by\nN(xğ‘¡;ğ+Î£Â·âˆ‡xğ‘¡Pr(xğ‘‘ğ‘Ÿğ‘–ğ‘“ğ‘¡|xğ‘¡),Î£). â–¡\nConceptually, Theorem 1 ensures the generated data moves to-\nward the target drifted distribution. We therefore design a gen-\neration framework comprising two key components: the diffuser\nand the drifter. The diffuser learns the distribution of original data\nand workloads, while the drifter guides the generation process to\nintroduce specified drift.\nDiffuser and Drifter Training. We illustrate the training process\nof the diffuser and drifter in Figure 2. The diffuser serves as the\ncore generative model, learning to approximate the unconditional\ndistribution ğ‘(xğ‘¡âˆ’1|xğ‘¡). To achieve this, we train the diffuser to\npredict the mean ğ(xğ‘¡,ğ‘¡;ğœƒ)of the reverse denoising process, which\nis directly related to the noise added during the forward process. In\nparticular, the reverse process can be rewritten in terms of noise ğœ–ğ‘¡,\nwhere: ğ(xğ‘¡,ğ‘¡;ğœƒ)=1âˆšğ›¼ğ‘¡\u0000xğ‘¡âˆ’âˆš1âˆ’ğ›¼ğ‘¡Â·ğœ–ğ‘¡\u0001,andğ›¼ğ‘¡is a parameterderived from the variance schedule ğœğ‘¡. This formulation makes\nnoise reconstruction a central task in training, and thus, the diffuser\nis trained by minimizing the noise reconstruction error:\nLğ·ğ‘–ğ‘“ğ‘“=Eğ‘¡\u0002\nâˆ¥ğœ–ğ‘¡âˆ’ğ·ğ‘–ğ‘“ğ‘“(xğ‘¡,ğ‘¡)âˆ¥2\u0003\n,\nwhereğ·ğ‘–ğ‘“ğ‘“(xğ‘¡,ğ‘¡)is the noise prediction output of the diffuser at\ntimestepğ‘¡.\nThe drifter, represented as ğ·ğ‘Ÿğ‘–ğ‘“ğ‘¡(xğ‘¡,ğ‘¡,ğ‘‘), is then utilized to fit\nPr(xğ‘‘ğ‘Ÿğ‘–ğ‘“ğ‘¡|xğ‘¡)to generate xğ‘‘ğ‘Ÿğ‘–ğ‘“ğ‘¡ with the same size as the current\nnoisy data xğ‘¡, and satisfies the user-specified drift factor ğ‘‘. It is\ntrained using a composite loss function that balances the drift-\nintroduction accuracy and the preservation of original correlations:\nLğ·ğ‘Ÿğ‘–ğ‘“ğ‘¡ =Eğ‘¡,ğ‘‘âˆ—\u0002\nâˆ¥ğ‘‘(xğ‘‘ğ‘Ÿğ‘–ğ‘“ğ‘¡,xğ‘¡)âˆ’ğ‘‘âˆ—âˆ¥2\n2+ğœ†âˆ¥C(xğ‘‘ğ‘Ÿğ‘–ğ‘“ğ‘¡)âˆ’C( xğ‘¡)âˆ¥2\n2\u0003\n,\nwhere the first Mean Square Error (MSE) part ensures that the JS\ndivergence of the distributions between the drifted and the current-\nstep result closely matches any drift factor ğ‘‘âˆ—, and the second\nMSE part penalizes any deviation in the correlations of the two\ndistributions. The hyperparameter ğœ†balances their importance.\nGeneration Process. Synthesizing drifted data and workloads in-\nvolves iterative timesteps involving the diffuser and the drifter, as\nshown in Figure 1. After sampling an initial noise xğ‘‡âˆ¼N( 0,I),\nthe diffuser transforms xğ‘‡into drifted data Ëœx0overğ‘‡discrete\ntimesteps. At each step ğ‘¡, the drifter computes a gradient ğ‘”ğ‘¡=\nâˆ‡xğ‘¡Pr(xğ‘‘ğ‘Ÿğ‘–ğ‘“ğ‘¡|xğ‘¡)=âˆ‡xğ‘¡ğ·ğ‘Ÿğ‘–ğ‘“ğ‘¡(xğ‘¡,ğ‘¡,ğ‘‘), which adjusts the predicted\nmean Ëœğğ‘¡=ğ+Î£Â·ğ‘”ğ‘¡, and use the adjusted mean Ëœğğ‘¡to sample the next\nstate xğ‘¡âˆ’1. We repeat this process for timesteps ğ‘¡=ğ‘‡,ğ‘‡âˆ’1,..., 1,\ngradually refining xğ‘¡toward xğ‘‘ğ‘Ÿğ‘–ğ‘“ğ‘¡ . After decoding xğ‘‘ğ‘Ÿğ‘–ğ‘“ğ‘¡ , we ob-\ntain the drifted data or workload that reflects the desired drift while\nmaintaining the inherent correlations.\n4.2.3 Preserving Temporal Constraints. Real-world data rarely makes\nabrupt, arbitrary transitions; rather, it evolves in a manner consis-\ntent with observed temporal constraints. To ensure our generator\nrespects these constraints, we propose an agent-based selection\nmechanism that chooses an appropriate generator from a pool of\nsnapshot-specific models. In this way, any generated drifted data\nand workload follow plausible historical progressions rather than\nleaping to distributions that never occurred in practice. Specifi-\ncally, we first partition the historical timeline into snapshots, each\ncapturing data from a certain period or system phase (e.g., Snap-\nshot 1 for early behavior, Snapshot 2 for mid-range drift, and so\non). For each snapshot ğ‘†ğ‘–, we train a distinct generative model Gğ‘–\nthat learns the empirical distribution in that time segment. These\nsnapshot-specific models collectively form a library { G1,G2,...},\n\nNeurBench: Benchmarking Learned Database Components with Data and Workload Drift Modeling\nwhere eachGğ‘–is specialized to generate data or workload con-\nsistent with the distribution observed in its corresponding time\nwindow. We then introduce an agent to choose which model in\nthe library to invoke when asked to produce drifted data. Con-\ncretely, upon receiving a request specifying: 1) an original dataset\n(or baseline distribution P), 2) a desired drift factor ğ‘‘that quantifies\nhow far the new data should deviate from P, the agent measures\nhow closeğ‘‘is to each snapshotâ€™s drift range. For instance, we can\ncompute a distance metric dist(ğ‘‘,ğ›¼ğ‘–)between the requested drift\nğ‘‘and the average (or representative) drift ğ›¼ğ‘–of each snapshot ğ‘†ğ‘–.\nThe agent then selects the generator Gğ‘–âˆ—with the minimal distance:\nGğ‘–âˆ—=arg minGğ‘–dist(ğ‘‘, ğ›¼ğ‘–). By choosingGğ‘–âˆ—, the system ensures\nthat newly generated data aligns with a distribution known to have\noccurred naturally in the historical timeline, rather than fabricat-\ning an unobserved or infeasible distribution. This snapshot-driven\nmechanism implicitly preserves temporal constraint because any\nrequested drift level ğ‘‘is mapped to one of the valid distributions.\nIf a drift beyond all observed snapshots is requested (i.e., no model\nin {Gğ‘–} matches well), the agent can either: 1) reject the request as\nfalling outside real-world evolution patterns, or 2) apply an extrap-\nolation step, such as linear interpolation among multiple snapshot\nmodels, ensuring the resulting distribution is still anchored to plausi-\nble historical transitions. Because unseen or extreme values cannot\nbe arbitrarily generated, the possibility to produce data violating\nnatural progression is greatly reduced. In effect, the agent-based\nselection corroborates that a dataset with drift ğ‘‘is one that could\nappear either after or before Pin a genuine temporal sequence.\n5 BENCHMARKING PREPARATION\nIn this section, we first revisit the general frameworks and key\ndesign choices of three representative learned database component\ncategories, namely end-to-end learned query optimizers, updat-\nable learned indexes, and learned concurrency control. Then, we\ndescribe the implementation of NeurBench .\n5.1 Learned Database Components Revisited\n5.1.1 End-to-end Learned Query Optimizer. The general framework\nof end-to-end learned query optimizers is shown in Figure 3. Given\na query q, the optimizer first performs plan search to obtain a set\nof candidate query plans P(q)={p0,p1,...,pğ‘}, and then conducts\nplan selection to choose one or top-k plans from P(q)for real execu-\ntion. During the plan search and selection, it interacts with an ML\nmodel to assess the plan quality. Using the real execution statistics\nof chosen plans as training data, the model is periodically retrained\nor fine-tuned to adapt to data and workload drift. Consequently,\nthe performance of a learned query optimizer is determined by the\ndesign of its plan search, plan selection, and ML model.\nModel. There are two main types of models for assessing plan qual-\nity: 1) Regression model predicts the execution cost or latency Ë†ğ¿(p)\nof a given plan p, optionally providing a confidence score ğ¶(p)that\nreflects the modelâ€™s certainty, i.e.,ğ‘“ğ‘ğ‘œ(p)=(Ë†ğ¿(p),ğ¶(p))[39,40,63].\n2)Ranking model predicts a binary ranking value ğ‘Ÿfor two given\nplans p1andp2, whereğ‘Ÿ=1ifp1is expected to outperform p2, and\nğ‘Ÿ=0otherwise, i.e.,ğ‘“ğ‘ğ‘œ(p1,p2)=ğ‘Ÿ,ğ‘Ÿâˆˆ{0,1}[66]. We note that\nsince a query plan can be represented as a tree, these models are typ-\nically implemented with one of two structures: Tree-CNN [ 42] and\nBlack-box Plan SearchCost/LatencyRisk ModelRanking./<latexit sha1_base64=\"Mv19DTEDzAtqUvKGJhxV/bbeT7g=\">AAAB8HicbVDLTgJBEJz1ifhCPXqZCCaeyC4XPRK9eMREHgY2ZHaYhQnz2Mz0asiGr/DiQWO8+jne/BsH2IOClXRSqepOd1eUCG7B97+9tfWNza3twk5xd2//4LB0dNyyOjWUNakW2nQiYpngijWBg2CdxDAiI8Ha0fhm5rcfmbFcq3uYJCyUZKh4zCkBJz1UepF+As4q/VLZr/pz4FUS5KSMcjT6pa/eQNNUMgVUEGu7gZ9AmBEDnAo2LfZSyxJCx2TIuo4qIpkNs/nBU3zulAGOtXGlAM/V3xMZkdZOZOQ6JYGRXfZm4n9eN4X4Ksy4SlJgii4WxanAoPHsezzghlEQE0cINdzdiumIGELBZVR0IQTLL6+SVq0a+NXgrlauX+dxFNApOkMXKECXqI5uUQM1EUUSPaNX9OYZ78V79z4WrWtePnOC/sD7/AE1rZAA</latexit><latexit sha1_base64=\"Mv19DTEDzAtqUvKGJhxV/bbeT7g=\">AAAB8HicbVDLTgJBEJz1ifhCPXqZCCaeyC4XPRK9eMREHgY2ZHaYhQnz2Mz0asiGr/DiQWO8+jne/BsH2IOClXRSqepOd1eUCG7B97+9tfWNza3twk5xd2//4LB0dNyyOjWUNakW2nQiYpngijWBg2CdxDAiI8Ha0fhm5rcfmbFcq3uYJCyUZKh4zCkBJz1UepF+As4q/VLZr/pz4FUS5KSMcjT6pa/eQNNUMgVUEGu7gZ9AmBEDnAo2LfZSyxJCx2TIuo4qIpkNs/nBU3zulAGOtXGlAM/V3xMZkdZOZOQ6JYGRXfZm4n9eN4X4Ksy4SlJgii4WxanAoPHsezzghlEQE0cINdzdiumIGELBZVR0IQTLL6+SVq0a+NXgrlauX+dxFNApOkMXKECXqI5uUQM1EUUSPaNX9OYZ78V79z4WrWtePnOC/sD7/AE1rZAA</latexit><latexit sha1_base64=\"Mv19DTEDzAtqUvKGJhxV/bbeT7g=\">AAAB8HicbVDLTgJBEJz1ifhCPXqZCCaeyC4XPRK9eMREHgY2ZHaYhQnz2Mz0asiGr/DiQWO8+jne/BsH2IOClXRSqepOd1eUCG7B97+9tfWNza3twk5xd2//4LB0dNyyOjWUNakW2nQiYpngijWBg2CdxDAiI8Ha0fhm5rcfmbFcq3uYJCyUZKh4zCkBJz1UepF+As4q/VLZr/pz4FUS5KSMcjT6pa/eQNNUMgVUEGu7gZ9AmBEDnAo2LfZSyxJCx2TIuo4qIpkNs/nBU3zulAGOtXGlAM/V3xMZkdZOZOQ6JYGRXfZm4n9eN4X4Ksy4SlJgii4WxanAoPHsezzghlEQE0cINdzdiumIGELBZVR0IQTLL6+SVq0a+NXgrlauX+dxFNApOkMXKECXqI5uUQM1EUUSPaNX9OYZ78V79z4WrWtePnOC/sD7/AE1rZAA</latexit><latexit sha1_base64=\"Mv19DTEDzAtqUvKGJhxV/bbeT7g=\">AAAB8HicbVDLTgJBEJz1ifhCPXqZCCaeyC4XPRK9eMREHgY2ZHaYhQnz2Mz0asiGr/DiQWO8+jne/BsH2IOClXRSqepOd1eUCG7B97+9tfWNza3twk5xd2//4LB0dNyyOjWUNakW2nQiYpngijWBg2CdxDAiI8Ha0fhm5rcfmbFcq3uYJCyUZKh4zCkBJz1UepF+As4q/VLZr/pz4FUS5KSMcjT6pa/eQNNUMgVUEGu7gZ9AmBEDnAo2LfZSyxJCx2TIuo4qIpkNs/nBU3zulAGOtXGlAM/V3xMZkdZOZOQ6JYGRXfZm4n9eN4X4Ksy4SlJgii4WxanAoPHsezzghlEQE0cINdzdiumIGELBZVR0IQTLL6+SVq0a+NXgrlauX+dxFNApOkMXKECXqI5uUQM1EUUSPaNX9OYZ78V79z4WrWtePnOC/sD7/AE1rZAA</latexit>Chosen Plan./<latexit sha1_base64=\"Mv19DTEDzAtqUvKGJhxV/bbeT7g=\">AAAB8HicbVDLTgJBEJz1ifhCPXqZCCaeyC4XPRK9eMREHgY2ZHaYhQnz2Mz0asiGr/DiQWO8+jne/BsH2IOClXRSqepOd1eUCG7B97+9tfWNza3twk5xd2//4LB0dNyyOjWUNakW2nQiYpngijWBg2CdxDAiI8Ha0fhm5rcfmbFcq3uYJCyUZKh4zCkBJz1UepF+As4q/VLZr/pz4FUS5KSMcjT6pa/eQNNUMgVUEGu7gZ9AmBEDnAo2LfZSyxJCx2TIuo4qIpkNs/nBU3zulAGOtXGlAM/V3xMZkdZOZOQ6JYGRXfZm4n9eN4X4Ksy4SlJgii4WxanAoPHsezzghlEQE0cINdzdiumIGELBZVR0IQTLL6+SVq0a+NXgrlauX+dxFNApOkMXKECXqI5uUQM1EUUSPaNX9OYZ78V79z4WrWtePnOC/sD7/AE1rZAA</latexit><latexit sha1_base64=\"Mv19DTEDzAtqUvKGJhxV/bbeT7g=\">AAAB8HicbVDLTgJBEJz1ifhCPXqZCCaeyC4XPRK9eMREHgY2ZHaYhQnz2Mz0asiGr/DiQWO8+jne/BsH2IOClXRSqepOd1eUCG7B97+9tfWNza3twk5xd2//4LB0dNyyOjWUNakW2nQiYpngijWBg2CdxDAiI8Ha0fhm5rcfmbFcq3uYJCyUZKh4zCkBJz1UepF+As4q/VLZr/pz4FUS5KSMcjT6pa/eQNNUMgVUEGu7gZ9AmBEDnAo2LfZSyxJCx2TIuo4qIpkNs/nBU3zulAGOtXGlAM/V3xMZkdZOZOQ6JYGRXfZm4n9eN4X4Ksy4SlJgii4WxanAoPHsezzghlEQE0cINdzdiumIGELBZVR0IQTLL6+SVq0a+NXgrlauX+dxFNApOkMXKECXqI5uUQM1EUUSPaNX9OYZ78V79z4WrWtePnOC/sD7/AE1rZAA</latexit><latexit sha1_base64=\"Mv19DTEDzAtqUvKGJhxV/bbeT7g=\">AAAB8HicbVDLTgJBEJz1ifhCPXqZCCaeyC4XPRK9eMREHgY2ZHaYhQnz2Mz0asiGr/DiQWO8+jne/BsH2IOClXRSqepOd1eUCG7B97+9tfWNza3twk5xd2//4LB0dNyyOjWUNakW2nQiYpngijWBg2CdxDAiI8Ha0fhm5rcfmbFcq3uYJCyUZKh4zCkBJz1UepF+As4q/VLZr/pz4FUS5KSMcjT6pa/eQNNUMgVUEGu7gZ9AmBEDnAo2LfZSyxJCx2TIuo4qIpkNs/nBU3zulAGOtXGlAM/V3xMZkdZOZOQ6JYGRXfZm4n9eN4X4Ksy4SlJgii4WxanAoPHsezzghlEQE0cINdzdiumIGELBZVR0IQTLL6+SVq0a+NXgrlauX+dxFNApOkMXKECXqI5uUQM1EUUSPaNX9OYZ78V79z4WrWtePnOC/sD7/AE1rZAA</latexit><latexit sha1_base64=\"Mv19DTEDzAtqUvKGJhxV/bbeT7g=\">AAAB8HicbVDLTgJBEJz1ifhCPXqZCCaeyC4XPRK9eMREHgY2ZHaYhQnz2Mz0asiGr/DiQWO8+jne/BsH2IOClXRSqepOd1eUCG7B97+9tfWNza3twk5xd2//4LB0dNyyOjWUNakW2nQiYpngijWBg2CdxDAiI8Ha0fhm5rcfmbFcq3uYJCyUZKh4zCkBJz1UepF+As4q/VLZr/pz4FUS5KSMcjT6pa/eQNNUMgVUEGu7gZ9AmBEDnAo2LfZSyxJCx2TIuo4qIpkNs/nBU3zulAGOtXGlAM/V3xMZkdZOZOQ6JYGRXfZm4n9eN4X4Ksy4SlJgii4WxanAoPHsezzghlEQE0cINdzdiumIGELBZVR0IQTLL6+SVq0a+NXgrlauX+dxFNApOkMXKECXqI5uUQM1EUUSPaNX9OYZ78V79z4WrWtePnOC/sD7/AE1rZAA</latexit>./<latexit sha1_base64=\"Mv19DTEDzAtqUvKGJhxV/bbeT7g=\">AAAB8HicbVDLTgJBEJz1ifhCPXqZCCaeyC4XPRK9eMREHgY2ZHaYhQnz2Mz0asiGr/DiQWO8+jne/BsH2IOClXRSqepOd1eUCG7B97+9tfWNza3twk5xd2//4LB0dNyyOjWUNakW2nQiYpngijWBg2CdxDAiI8Ha0fhm5rcfmbFcq3uYJCyUZKh4zCkBJz1UepF+As4q/VLZr/pz4FUS5KSMcjT6pa/eQNNUMgVUEGu7gZ9AmBEDnAo2LfZSyxJCx2TIuo4qIpkNs/nBU3zulAGOtXGlAM/V3xMZkdZOZOQ6JYGRXfZm4n9eN4X4Ksy4SlJgii4WxanAoPHsezzghlEQE0cINdzdiumIGELBZVR0IQTLL6+SVq0a+NXgrlauX+dxFNApOkMXKECXqI5uUQM1EUUSPaNX9OYZ78V79z4WrWtePnOC/sD7/AE1rZAA</latexit><latexit sha1_base64=\"Mv19DTEDzAtqUvKGJhxV/bbeT7g=\">AAAB8HicbVDLTgJBEJz1ifhCPXqZCCaeyC4XPRK9eMREHgY2ZHaYhQnz2Mz0asiGr/DiQWO8+jne/BsH2IOClXRSqepOd1eUCG7B97+9tfWNza3twk5xd2//4LB0dNyyOjWUNakW2nQiYpngijWBg2CdxDAiI8Ha0fhm5rcfmbFcq3uYJCyUZKh4zCkBJz1UepF+As4q/VLZr/pz4FUS5KSMcjT6pa/eQNNUMgVUEGu7gZ9AmBEDnAo2LfZSyxJCx2TIuo4qIpkNs/nBU3zulAGOtXGlAM/V3xMZkdZOZOQ6JYGRXfZm4n9eN4X4Ksy4SlJgii4WxanAoPHsezzghlEQE0cINdzdiumIGELBZVR0IQTLL6+SVq0a+NXgrlauX+dxFNApOkMXKECXqI5uUQM1EUUSPaNX9OYZ78V79z4WrWtePnOC/sD7/AE1rZAA</latexit><latexit sha1_base64=\"Mv19DTEDzAtqUvKGJhxV/bbeT7g=\">AAAB8HicbVDLTgJBEJz1ifhCPXqZCCaeyC4XPRK9eMREHgY2ZHaYhQnz2Mz0asiGr/DiQWO8+jne/BsH2IOClXRSqepOd1eUCG7B97+9tfWNza3twk5xd2//4LB0dNyyOjWUNakW2nQiYpngijWBg2CdxDAiI8Ha0fhm5rcfmbFcq3uYJCyUZKh4zCkBJz1UepF+As4q/VLZr/pz4FUS5KSMcjT6pa/eQNNUMgVUEGu7gZ9AmBEDnAo2LfZSyxJCx2TIuo4qIpkNs/nBU3zulAGOtXGlAM/V3xMZkdZOZOQ6JYGRXfZm4n9eN4X4Ksy4SlJgii4WxanAoPHsezzghlEQE0cINdzdiumIGELBZVR0IQTLL6+SVq0a+NXgrlauX+dxFNApOkMXKECXqI5uUQM1EUUSPaNX9OYZ78V79z4WrWtePnOC/sD7/AE1rZAA</latexit><latexit sha1_base64=\"Mv19DTEDzAtqUvKGJhxV/bbeT7g=\">AAAB8HicbVDLTgJBEJz1ifhCPXqZCCaeyC4XPRK9eMREHgY2ZHaYhQnz2Mz0asiGr/DiQWO8+jne/BsH2IOClXRSqepOd1eUCG7B97+9tfWNza3twk5xd2//4LB0dNyyOjWUNakW2nQiYpngijWBg2CdxDAiI8Ha0fhm5rcfmbFcq3uYJCyUZKh4zCkBJz1UepF+As4q/VLZr/pz4FUS5KSMcjT6pa/eQNNUMgVUEGu7gZ9AmBEDnAo2LfZSyxJCx2TIuo4qIpkNs/nBU3zulAGOtXGlAM/V3xMZkdZOZOQ6JYGRXfZm4n9eN4X4Ksy4SlJgii4WxanAoPHsezzghlEQE0cINdzdiumIGELBZVR0IQTLL6+SVq0a+NXgrlauX+dxFNApOkMXKECXqI5uUQM1EUUSPaNX9OYZ78V79z4WrWtePnOC/sD7/AE1rZAA</latexit>Traditional Query OptimizerCandidate PlansPlan 1Plan Nâ€¦HintCardinalityWhite-box Plan SearchSub-PlansTablesR1R2R3./<latexit sha1_base64=\"Mv19DTEDzAtqUvKGJhxV/bbeT7g=\">AAAB8HicbVDLTgJBEJz1ifhCPXqZCCaeyC4XPRK9eMREHgY2ZHaYhQnz2Mz0asiGr/DiQWO8+jne/BsH2IOClXRSqepOd1eUCG7B97+9tfWNza3twk5xd2//4LB0dNyyOjWUNakW2nQiYpngijWBg2CdxDAiI8Ha0fhm5rcfmbFcq3uYJCyUZKh4zCkBJz1UepF+As4q/VLZr/pz4FUS5KSMcjT6pa/eQNNUMgVUEGu7gZ9AmBEDnAo2LfZSyxJCx2TIuo4qIpkNs/nBU3zulAGOtXGlAM/V3xMZkdZOZOQ6JYGRXfZm4n9eN4X4Ksy4SlJgii4WxanAoPHsezzghlEQE0cINdzdiumIGELBZVR0IQTLL6+SVq0a+NXgrlauX+dxFNApOkMXKECXqI5uUQM1EUUSPaNX9OYZ78V79z4WrWtePnOC/sD7/AE1rZAA</latexit><latexit sha1_base64=\"Mv19DTEDzAtqUvKGJhxV/bbeT7g=\">AAAB8HicbVDLTgJBEJz1ifhCPXqZCCaeyC4XPRK9eMREHgY2ZHaYhQnz2Mz0asiGr/DiQWO8+jne/BsH2IOClXRSqepOd1eUCG7B97+9tfWNza3twk5xd2//4LB0dNyyOjWUNakW2nQiYpngijWBg2CdxDAiI8Ha0fhm5rcfmbFcq3uYJCyUZKh4zCkBJz1UepF+As4q/VLZr/pz4FUS5KSMcjT6pa/eQNNUMgVUEGu7gZ9AmBEDnAo2LfZSyxJCx2TIuo4qIpkNs/nBU3zulAGOtXGlAM/V3xMZkdZOZOQ6JYGRXfZm4n9eN4X4Ksy4SlJgii4WxanAoPHsezzghlEQE0cINdzdiumIGELBZVR0IQTLL6+SVq0a+NXgrlauX+dxFNApOkMXKECXqI5uUQM1EUUSPaNX9OYZ78V79z4WrWtePnOC/sD7/AE1rZAA</latexit><latexit sha1_base64=\"Mv19DTEDzAtqUvKGJhxV/bbeT7g=\">AAAB8HicbVDLTgJBEJz1ifhCPXqZCCaeyC4XPRK9eMREHgY2ZHaYhQnz2Mz0asiGr/DiQWO8+jne/BsH2IOClXRSqepOd1eUCG7B97+9tfWNza3twk5xd2//4LB0dNyyOjWUNakW2nQiYpngijWBg2CdxDAiI8Ha0fhm5rcfmbFcq3uYJCyUZKh4zCkBJz1UepF+As4q/VLZr/pz4FUS5KSMcjT6pa/eQNNUMgVUEGu7gZ9AmBEDnAo2LfZSyxJCx2TIuo4qIpkNs/nBU3zulAGOtXGlAM/V3xMZkdZOZOQ6JYGRXfZm4n9eN4X4Ksy4SlJgii4WxanAoPHsezzghlEQE0cINdzdiumIGELBZVR0IQTLL6+SVq0a+NXgrlauX+dxFNApOkMXKECXqI5uUQM1EUUSPaNX9OYZ78V79z4WrWtePnOC/sD7/AE1rZAA</latexit><latexit sha1_base64=\"Mv19DTEDzAtqUvKGJhxV/bbeT7g=\">AAAB8HicbVDLTgJBEJz1ifhCPXqZCCaeyC4XPRK9eMREHgY2ZHaYhQnz2Mz0asiGr/DiQWO8+jne/BsH2IOClXRSqepOd1eUCG7B97+9tfWNza3twk5xd2//4LB0dNyyOjWUNakW2nQiYpngijWBg2CdxDAiI8Ha0fhm5rcfmbFcq3uYJCyUZKh4zCkBJz1UepF+As4q/VLZr/pz4FUS5KSMcjT6pa/eQNNUMgVUEGu7gZ9AmBEDnAo2LfZSyxJCx2TIuo4qIpkNs/nBU3zulAGOtXGlAM/V3xMZkdZOZOQ6JYGRXfZm4n9eN4X4Ksy4SlJgii4WxanAoPHsezzghlEQE0cINdzdiumIGELBZVR0IQTLL6+SVq0a+NXgrlauX+dxFNApOkMXKECXqI5uUQM1EUUSPaNX9OYZ78V79z4WrWtePnOC/sD7/AE1rZAA</latexit>./<latexit sha1_base64=\"Mv19DTEDzAtqUvKGJhxV/bbeT7g=\">AAAB8HicbVDLTgJBEJz1ifhCPXqZCCaeyC4XPRK9eMREHgY2ZHaYhQnz2Mz0asiGr/DiQWO8+jne/BsH2IOClXRSqepOd1eUCG7B97+9tfWNza3twk5xd2//4LB0dNyyOjWUNakW2nQiYpngijWBg2CdxDAiI8Ha0fhm5rcfmbFcq3uYJCyUZKh4zCkBJz1UepF+As4q/VLZr/pz4FUS5KSMcjT6pa/eQNNUMgVUEGu7gZ9AmBEDnAo2LfZSyxJCx2TIuo4qIpkNs/nBU3zulAGOtXGlAM/V3xMZkdZOZOQ6JYGRXfZm4n9eN4X4Ksy4SlJgii4WxanAoPHsezzghlEQE0cINdzdiumIGELBZVR0IQTLL6+SVq0a+NXgrlauX+dxFNApOkMXKECXqI5uUQM1EUUSPaNX9OYZ78V79z4WrWtePnOC/sD7/AE1rZAA</latexit><latexit sha1_base64=\"Mv19DTEDzAtqUvKGJhxV/bbeT7g=\">AAAB8HicbVDLTgJBEJz1ifhCPXqZCCaeyC4XPRK9eMREHgY2ZHaYhQnz2Mz0asiGr/DiQWO8+jne/BsH2IOClXRSqepOd1eUCG7B97+9tfWNza3twk5xd2//4LB0dNyyOjWUNakW2nQiYpngijWBg2CdxDAiI8Ha0fhm5rcfmbFcq3uYJCyUZKh4zCkBJz1UepF+As4q/VLZr/pz4FUS5KSMcjT6pa/eQNNUMgVUEGu7gZ9AmBEDnAo2LfZSyxJCx2TIuo4qIpkNs/nBU3zulAGOtXGlAM/V3xMZkdZOZOQ6JYGRXfZm4n9eN4X4Ksy4SlJgii4WxanAoPHsezzghlEQE0cINdzdiumIGELBZVR0IQTLL6+SVq0a+NXgrlauX+dxFNApOkMXKECXqI5uUQM1EUUSPaNX9OYZ78V79z4WrWtePnOC/sD7/AE1rZAA</latexit><latexit sha1_base64=\"Mv19DTEDzAtqUvKGJhxV/bbeT7g=\">AAAB8HicbVDLTgJBEJz1ifhCPXqZCCaeyC4XPRK9eMREHgY2ZHaYhQnz2Mz0asiGr/DiQWO8+jne/BsH2IOClXRSqepOd1eUCG7B97+9tfWNza3twk5xd2//4LB0dNyyOjWUNakW2nQiYpngijWBg2CdxDAiI8Ha0fhm5rcfmbFcq3uYJCyUZKh4zCkBJz1UepF+As4q/VLZr/pz4FUS5KSMcjT6pa/eQNNUMgVUEGu7gZ9AmBEDnAo2LfZSyxJCx2TIuo4qIpkNs/nBU3zulAGOtXGlAM/V3xMZkdZOZOQ6JYGRXfZm4n9eN4X4Ksy4SlJgii4WxanAoPHsezzghlEQE0cINdzdiumIGELBZVR0IQTLL6+SVq0a+NXgrlauX+dxFNApOkMXKECXqI5uUQM1EUUSPaNX9OYZ78V79z4WrWtePnOC/sD7/AE1rZAA</latexit><latexit sha1_base64=\"Mv19DTEDzAtqUvKGJhxV/bbeT7g=\">AAAB8HicbVDLTgJBEJz1ifhCPXqZCCaeyC4XPRK9eMREHgY2ZHaYhQnz2Mz0asiGr/DiQWO8+jne/BsH2IOClXRSqepOd1eUCG7B97+9tfWNza3twk5xd2//4LB0dNyyOjWUNakW2nQiYpngijWBg2CdxDAiI8Ha0fhm5rcfmbFcq3uYJCyUZKh4zCkBJz1UepF+As4q/VLZr/pz4FUS5KSMcjT6pa/eQNNUMgVUEGu7gZ9AmBEDnAo2LfZSyxJCx2TIuo4qIpkNs/nBU3zulAGOtXGlAM/V3xMZkdZOZOQ6JYGRXfZm4n9eN4X4Ksy4SlJgii4WxanAoPHsezzghlEQE0cINdzdiumIGELBZVR0IQTLL6+SVq0a+NXgrlauX+dxFNApOkMXKECXqI5uUQM1EUUSPaNX9OYZ78V79z4WrWtePnOC/sD7/AE1rZAA</latexit>./<latexit sha1_base64=\"Mv19DTEDzAtqUvKGJhxV/bbeT7g=\">AAAB8HicbVDLTgJBEJz1ifhCPXqZCCaeyC4XPRK9eMREHgY2ZHaYhQnz2Mz0asiGr/DiQWO8+jne/BsH2IOClXRSqepOd1eUCG7B97+9tfWNza3twk5xd2//4LB0dNyyOjWUNakW2nQiYpngijWBg2CdxDAiI8Ha0fhm5rcfmbFcq3uYJCyUZKh4zCkBJz1UepF+As4q/VLZr/pz4FUS5KSMcjT6pa/eQNNUMgVUEGu7gZ9AmBEDnAo2LfZSyxJCx2TIuo4qIpkNs/nBU3zulAGOtXGlAM/V3xMZkdZOZOQ6JYGRXfZm4n9eN4X4Ksy4SlJgii4WxanAoPHsezzghlEQE0cINdzdiumIGELBZVR0IQTLL6+SVq0a+NXgrlauX+dxFNApOkMXKECXqI5uUQM1EUUSPaNX9OYZ78V79z4WrWtePnOC/sD7/AE1rZAA</latexit><latexit sha1_base64=\"Mv19DTEDzAtqUvKGJhxV/bbeT7g=\">AAAB8HicbVDLTgJBEJz1ifhCPXqZCCaeyC4XPRK9eMREHgY2ZHaYhQnz2Mz0asiGr/DiQWO8+jne/BsH2IOClXRSqepOd1eUCG7B97+9tfWNza3twk5xd2//4LB0dNyyOjWUNakW2nQiYpngijWBg2CdxDAiI8Ha0fhm5rcfmbFcq3uYJCyUZKh4zCkBJz1UepF+As4q/VLZr/pz4FUS5KSMcjT6pa/eQNNUMgVUEGu7gZ9AmBEDnAo2LfZSyxJCx2TIuo4qIpkNs/nBU3zulAGOtXGlAM/V3xMZkdZOZOQ6JYGRXfZm4n9eN4X4Ksy4SlJgii4WxanAoPHsezzghlEQE0cINdzdiumIGELBZVR0IQTLL6+SVq0a+NXgrlauX+dxFNApOkMXKECXqI5uUQM1EUUSPaNX9OYZ78V79z4WrWtePnOC/sD7/AE1rZAA</latexit><latexit sha1_base64=\"Mv19DTEDzAtqUvKGJhxV/bbeT7g=\">AAAB8HicbVDLTgJBEJz1ifhCPXqZCCaeyC4XPRK9eMREHgY2ZHaYhQnz2Mz0asiGr/DiQWO8+jne/BsH2IOClXRSqepOd1eUCG7B97+9tfWNza3twk5xd2//4LB0dNyyOjWUNakW2nQiYpngijWBg2CdxDAiI8Ha0fhm5rcfmbFcq3uYJCyUZKh4zCkBJz1UepF+As4q/VLZr/pz4FUS5KSMcjT6pa/eQNNUMgVUEGu7gZ9AmBEDnAo2LfZSyxJCx2TIuo4qIpkNs/nBU3zulAGOtXGlAM/V3xMZkdZOZOQ6JYGRXfZm4n9eN4X4Ksy4SlJgii4WxanAoPHsezzghlEQE0cINdzdiumIGELBZVR0IQTLL6+SVq0a+NXgrlauX+dxFNApOkMXKECXqI5uUQM1EUUSPaNX9OYZ78V79z4WrWtePnOC/sD7/AE1rZAA</latexit><latexit sha1_base64=\"Mv19DTEDzAtqUvKGJhxV/bbeT7g=\">AAAB8HicbVDLTgJBEJz1ifhCPXqZCCaeyC4XPRK9eMREHgY2ZHaYhQnz2Mz0asiGr/DiQWO8+jne/BsH2IOClXRSqepOd1eUCG7B97+9tfWNza3twk5xd2//4LB0dNyyOjWUNakW2nQiYpngijWBg2CdxDAiI8Ha0fhm5rcfmbFcq3uYJCyUZKh4zCkBJz1UepF+As4q/VLZr/pz4FUS5KSMcjT6pa/eQNNUMgVUEGu7gZ9AmBEDnAo2LfZSyxJCx2TIuo4qIpkNs/nBU3zulAGOtXGlAM/V3xMZkdZOZOQ6JYGRXfZm4n9eN4X4Ksy4SlJgii4WxanAoPHsezzghlEQE0cINdzdiumIGELBZVR0IQTLL6+SVq0a+NXgrlauX+dxFNApOkMXKECXqI5uUQM1EUUSPaNX9OYZ78V79z4WrWtePnOC/sD7/AE1rZAA</latexit>Figure 3: End-to-end Learned Query Optimizers\nTree-LSTM [ 49]. Both structures are deep neural networks designed\nfor tree structures to capture bottom-up sequential information.\nPlan Search. There are two paradigms in plan search, which\ndiffer in whether they rely on traditional query optimizers: 1)\nBlack-box search provides auxiliary information to traditional query\noptimizers for candidate plan generation. Based on the type of infor-\nmation provided, it can be further categorized into: (a) Hint-based.\nIt supplies hints (parameters that control physical operators, such\nas join or scan types) to the traditional optimizer, guiding it to\ngenerate corresponding query plans [ 39,63]. Selecting proper hints\nis therefore important for the quality of the candidate plan set.\nEarly approaches [ 39] determine static hint sets in advance by\ndatabase experts with in-depth knowledge of query optimization.\nSome recent methods are proposed to dynamically select hints\nbased on search algorithms [ 1,63]. For example, HybridQO [ 63]\nuses a Monte Carlo tree search combined with a join order pre-\ndiction model to generate a set of potential join orders as hints.\nAutoSteer [ 1] automatically discovers hints that can impact a given\nquery through a greedy search of the hints supported by the data-\nbase. (b) Cardinality-based. It adjusts (by magnifying or reducing)\nestimated cardinalities multiple times to generate a variety of can-\ndidate plans [ 66]. 2) White-box search embeds the model directly\ninto the plan search process without relying on the traditional\noptimizers [ 40]. It starts with an empty or partial plan and itera-\ntively constructs a complete query plan until a predefined number\nof candidate plans is reached. At each step, multiple subplans are\ngenerated. After using an ML model to evaluate their quality, inef-\nfective subplans are pruned while the remaining ones are expanded\nin subsequent iterations.\nPlan Selection. Based on the cost/latency or ranking predicted\nby the model, the optimizer selects plans for execution from the\ncandidate set, following one of these strategies: 1) Best-only, which\nonly selects the plan with the lowest predicted cost/latency or\nhighest ranking. 2) Top-k, which selects the best plan along with\nthe nextğ‘˜-1 highest-ranked plans. These ğ‘˜-1 plans are executed\nasynchronously in the background to gather more real execution\ndata. Adjusting ğ‘˜controls the balance between exploiting the best-\nknown plan and exploring alternative plans.\n5.1.2 Updatable Learned Index. Given a search key k, the learned\nindexğ‘“ğ‘–ğ‘‘ğ‘¥predicts its position Ë†ğ‘ƒğ‘†(k)in the data table, i.e.,ğ‘“ğ‘–ğ‘‘ğ‘¥(k)=\nË†ğ‘ƒğ‘†(k). Learned indexes typically organize a set of ML models in a\nlayered structure, where each node contains a model for position\nprediction. To handle key insertions, the structure must be updat-\nable where nodes can be added or deleted dynamically. Therefore,\n\nZhanhao Zhao, Haotian Gao, Naili Xing, Lingze Zeng, Meihui Zhang,\nGang Chen, Manuel Rigger, Beng Chin Ooi\nIndex Structure45Inner NodeMData NodeM12M1M2M5M4MM\nFigure 4: Structures of Existing Learned Indexes\nthe design of the index structure and update mechanism are key\nfactors influencing the performance of learned indexes.\nIndex Structure. As shown in Figure 4, existing learned indexes\ncan be classified into two representative structures, based on whether\nthey have different node types: 1) Heterogeneous structure. It con-\nsists of two distinct types of nodes: inner nodes and leaf nodes.\nInner nodes do not store data, whereas leaf nodes store the actual\ndata. Given a query key, inner nodes predict the next step in the\nstructure, directing the query to the appropriate leaf node, where\na search is performed to locate the exact position of the key. 2)\nHomogeneous structure. Every node in this structure is responsible\nfor storing part of the data. Given a query key, each node predicts\nwhether the key falls within its stored data, or if the query should\nproceed to a corresponding child node.\nStructural Update. The key insertion can trigger index structural\nupdates. There are two main update mechanisms, which differ in\nhow they handle incoming keys: 1) In-place. Index nodes reserve\ngaps (empty slots), which allow new keys to be directly inserted\ninto the index structure. Given a new key, the index first locates\nan appropriate position for insertion. However, conflicts can occur\nwhen multiple keys are mapped to the same position, as the model\nwas trained without the new key, or if the predicted slot is already\noccupied. In such cases, conflict resolution is required, typically\nfollowing one of these two methods: (a) Shift method , sequentially\nmoving the existing key to the nearest available gap to create space\nat the target location for successful insertion [ 13]; (b) Chain method ,\ncreating a new node for the conflicting target position and inserting\nthe key into this new node [60]. 2) Delta-buffer. One or more dedi-\ncated buffers are created separately from the index structure. When\na new key arrives, it is first placed in the buffer and periodically\nmerged into the index structure. The buffer can be organized at two\ndifferent levels of granularity: (a) Tree-level buffer. A single buffer\nis maintained for the entire index [ 14]; (b) Node-level buffer. Each\nnode has its own dedicated buffer [50].\n5.1.3 Learned Concurrency Control (CC). Learned CC algorithm\ndynamically selects the most suitable CC actions based on the\ncurrent workload to optimize transaction performance. It takes a\ntransaction operation ğ‘œğ‘(e.g., read or write) and the current system\nconditionğ¶(ğ‘œğ‘)(e.g., contention levels) as inputs, and leverages\nan ML model ğ‘“ğ‘ğ‘to predict the most appropriate CC action for\nthe operation ğ‘œğ‘. These actions may include acquiring a shared\nlock, acquiring an exclusive lock, processing without locking, etc.\nFormally, let Abe the set of available CC actions, ğ‘“ğ‘ğ‘can be defined\nasğ‘“ğ‘ğ‘(ğ‘œğ‘,ğ¶(ğ‘œğ‘))=ğ‘, whereğ‘âˆˆA. As new operation types emerge\nor system conditions change, the learned CC algorithm requires re-\ntraining to adapt to workload drift. PolyJuice [ 53], a state-of-the-art\nlearned CC algorithm is proposed following this definition, whichTable 1: A Taxonomy of Evaluated Learned Components\nEnd-to-end\nLearned Query\nOptimizerMethod Model Plan Search Plan Selection\nHybirdQO [63] Regression Hint-based Top-k\nBao [39] Regression Hint-based Best-only\nNeo [40] Regression Write-box Best-only\nLero [66] Ranking Cardinality-based Top-k\nUpdatable\nLearned IndexMethod Structure Structural Update\nALEX [13] Heterogeneous In-place (Shift)\nLIPP [60] Homogeneous In-place (Chain)\nXIndex [50] Heterogeneous Delta-buffer (Node-level)\nPGM-Index [14] Heterogeneous Delta-buffer (Tree-level)\nLearned CC PolyJuice [53]\npredicts CC actions for each unique operation within a transac-\ntion. Furthermore, to adapt to workload drift, PolyJuice performs\nfull retraining based on historical workload traces and applies the\nupdated model to handle future workloads.\n5.2 Implementation of NeurBench\nWe build upon the diffusion model code from existing works [ 22,\n36] to implement the drift-aware data and workload generator\nofNeurBench . In particular, we implement the diffuser based on\na Multilayer Perceptron (MLP) with hidden layer dimensions of\n(512,1024,1024,512)and the drifter as a similar MLP with hidden\nlayer dimensions of (512,512). We use table formats to represent\nboth data and workloads, as specified in Section 4.1, and encode\nthe table using the analog bit encoding technique [ 36]. For each\ntable, we dynamically adjust the learning rate from 1ğ‘’âˆ’4to2ğ‘’âˆ’3to\nensure the convergence of the training process.\nNeurBench imposes no constraints on the choice of its evaluators\nand is flexible in supporting various implementations. Since no\nexisting database system seamlessly integrates all tested learned\ndatabase components, we use separate evaluators for each type of\nlearned component. We adopt existing learned database component\nevaluators [ 27,48,53] and extend them to support evaluation under\ndata and workload drift scenarios. The source code of NeurBench\nis available at [43].\n6 EVALUATION\nIn this section we evaluate the effectiveness of NeurBench in gen-\nerating drifted data and workloads, and then benchmark selected\nlearned query optimizers, learned indexes and learned CC.\n6.1 Experimental Setup\n6.1.1 Competitors. We compare the data generator of NeurBench\nwith RelDDPM, a state-of-the-art data synthesis framework, and\nuseNeurBench to evaluate representative learned database compo-\nnents as summarized in Table 1.\nRelDDPM [ 36]is a state-of-the-art DDPM-based data synthesis\nframework. We chose RelDDPM as a competitor because it also uses\nguided diffusion and achieves effective correlation-preserved data\ngeneration. Since RelDDPM does not natively support drift-aware\ndata generation, we extend RelDDPM by adjusting the strength of\nthe guidance in the denoising process to generate drifted data.\nRepresentative Learned Database Components. Based on the\nanalysis in Section 5.1, we carefully select learned database com-\nponents that cover most key design choices for our evaluation, as\n\nNeurBench: Benchmarking Learned Database Components with Data and Workload Drift Modeling\n 0 0.1 0.2 0.3 0.4\n 0.1  0.2  0.3  0.4  0.5Avg. Corr. Error\nDrift FactorRelDDPM NeurBench\n(a) IMDB ( Movie âŠ² âŠ³Director )\n 0 0.1 0.2 0.3 0.4\n 0.1  0.2  0.3  0.4  0.5Avg. Corr. Error\nDrift FactorRelDDPM NeurBench (b) IMDB ( Movie âŠ² âŠ³Actor )\n 0.1 0.2 0.3 0.4\n 0.1  0.2  0.3  0.4  0.5Avg. Corr. Error\nDrift FactorRelDDPM NeurBench (c) Facebook\n 0 0.1 0.2 0.3\n 0.1  0.2  0.3  0.4  0.5Avg. Corr. Error\nDrift FactorRelDDPM NeurBench (d) Shoppers\n 0 0.1 0.2 0.3 0.4 0.5 0.6\n 0.1  0.2  0.3  0.4  0.5Avg. Corr. Error\nDrift FactorRelDDPM NeurBench (e) WeatherAUS\nFigure 5: Performance on Generating Drifted Datasets with Varying Drift Factors\nlisted in Table 1. Due to space constraints, we opt not to conduct\nexhaustive evaluations of all existing learned database components.\n6.1.2 Datasets and Workloads. We construct an original dataset\nand workload, and use NeurBench to generate corresponding drifted\ndatasets and workloads for our experiments.\nOriginal Dataset and Workload. We use the Internet Movie Data-\nbase (IMDB) [ 20], a real-world dataset containing 21 tables with\nintricate inter-table and intra-table correlations. Based on IMDB,\nwe construct a combined workload to conduct our experiments. As\ndifferent learned database components may vary in their require-\nments of workloads, we accommodate three types of operations in\nthe workload, as listed below: 1) Complex queries. We include all 113\nqueries from JOB [ 28], a widely-used workload for query optimizer\nevaluation [ 5,27]. It is derived from 33 real query templates, cover-\ning complex query patterns that access 4 to 17 IMDB tables. 2) Range\nscans. Each scan operation queries a range [start_key, end_key] on\ntheIDattribute of the IMDB Name table, which contains 4M records.\nBy default, start_key is randomly selected from the entire table,\nand the scan length ( i.e., end_keyâˆ’start_key) is randomly selected\nbetween 1 and 100, following prior studies [ 48,60]. 3) Transactions.\nWe execute concurrent transactions on the IMDB Name table. By\ndefault, each transaction consists of 5 reads and 5 writes, following\nprevious works [ 8,17,53]. Each read or write is performed on a\nrandomly selected record.\nDrifted Datasets and Workloads. To evaluate learned database\ncomponents under various drift scenarios, we employ NeurBench\nto generate drift datasets and workloads from the original dataset\nand workload based on a given drift factor ğ‘‘. We collect historical\nIMDB datasets from [ 2] and feed them into the generator to preserve\ntemporal constraints. We denote no drift as ğ‘‘=0, indicating that the\noriginal dataset and workload are used. We apply drift factors of 0.1,\n0.3, and 0.5 (ğ‘‘âˆˆ{0.1,0.3,0.5}) to simulate mild, medium, and severe\ndrift. In particular, we use the following drifted data and workloads:\n1)Drifted IMDB, the dataset drifts from the original IMDB with ğ‘‘.\n2)Workload with join pattern drift, the workload with drift in query\njoin patterns according to ğ‘‘; 3)Workloads with predicate drift, the\nworkload with drift in query predicates; 4) Workloads with arrival\nrate drift, the workload with transaction arrival rate drift.\nWe also employ an extended version of CH-Benchmark, which\nincludes drifted workloads generated by NeurBench , to conduct\nour experiments.\nCH-Benchmark [ 7].CH-Benchmark is a hybrid benchmark com-\nbining TPC-C [ 9], a standard transactional benchmark, and TPC-\nH [10], a standard analytical benchmark. CH-Benchmark integrates\nthe tables from both benchmarks and consists of all TPC-C transac-\ntions and all TPC-H queries. Its transactional workload comprises\n5 types of transactions, with NewOrder and Payment accountingfor 45% and 43% respectively, and each of the other three trans-\naction types contributing 4%. In addition, its analytical workload\nincludes 22 analytical queries characterized by complex query pat-\nterns. We use NeurBench to generate transactional workloads with\narrival rate drift, as well as analytical workloads with mixed drift\n(including both join portion drift and predicate drift).\n6.1.3 Evaluation Metrics. We assess the effectiveness of data syn-\nthesis using average correlation error , which measures the average\ndeviation in correlations between the generated dataset and the\noriginal dataset across all attributes. Correlations are computed\nusing Pearsonâ€™s correlation coefficient [57].\nFor learned database components, we assess their performance\nusing execution time andthroughput , where execution time ( ğ¸ğ‘‹ğ¸)\nrefers to the time taken for a query, and throughput ( ğ‘‡ğ‘ƒğ‘†) mea-\nsures the number of queries/transactions processed per second. To\nquantify performance degradation under drift, we introduce two\nadditional metrics: 1) Performance Regression , which measures the\nrelative increase in execution time as (ğ¸ğ‘‹ğ¸ğ‘‘âˆ’ğ¸ğ‘‹ğ¸ğ‘‘=0)/ğ¸ğ‘‹ğ¸ğ‘‘=0,\nand 2) Throughput Drop , which captures the percentage decrease in\nthroughput as(1âˆ’ğ‘‡ğ‘ƒğ‘†ğ‘‘/ğ‘‡ğ‘ƒğ‘†ğ‘‘=0)Ã—100% . Both performance regres-\nsion and throughput drop indicate higher performance degradation\nwith larger values, and therefore, we use them to directly reflect\nthe impact of drift across different learned database components.\n6.1.4 Default Configuration. We conduct experiments on 2 servers\nequipped with an Intel(R) Xeon(R) W-1290P CPU@ 3.70GHz (10\ncores, 20 threads), 128 GB memory, and 3 NVIDIA GeForce RTX\n2080 Ti GPUs. All experiments are run within Docker contain-\ners (Ubuntu 22.04 with CUDA 11.8.0). We deploy NeurBench on\none machine as the client and the learned database components\non another machine to prevent local resource contention. Unless\notherwise specified, we use the default parameters stated in the\nadopted evaluators [27, 48, 53] for our experiments.\n6.2 Evaluation on Drift-aware Data and\nWorkload Generation Framework\nIn this set of experiments, we evaluate the effectiveness of our\nproposed drift-aware data and workload generation framework by\ncomparing NeurBench with RelDDPM.\n6.2.1 Experiments on generating drifted datasets, under varying\ndrift factors. We first assess the correlation error of drifted IMDB\ndatasets with the drift factor increasing from 0.1 to 0.5. Following\nRelDDPM, we join multiple IMDB tables and measure both inter-\ntable and intra-table correlations on the joined tables. Specifically,\nwe construct two joined tables: 1) Movie âŠ² âŠ³Director , where movies\nare linked to directors via director ID; and 2) Movie âŠ² âŠ³Actor , where\n\nZhanhao Zhao, Haotian Gao, Naili Xing, Lingze Zeng, Meihui Zhang,\nGang Chen, Manuel Rigger, Beng Chin Ooi\n 0 5 10 15 20\n 0  15  30d=0 d=0.1 d=0.3 d=0.5\n# of Join PatternProbability (%)\n  0  15  30d=0 d=0.1 d=0.3 d=0.5\n# of Join Pattern   0  15  30d=0 d=0.1 d=0.3 d=0.5\n# of Join Pattern   0  15  30d=0 d=0.1 d=0.3 d=0.5\n# of Join Pattern  \n(a) Generated Workloads with Join Pattern Drift\n 0 5 10 15 20\n 0  30  60d=0 d=0.1 d=0.3 d=0.5\n# of PredicateProbability (%)\n  0  30  60d=0 d=0.1 d=0.3 d=0.5\n# of Predicate   0  30  60d=0 d=0.1 d=0.3 d=0.5\n# of Predicate   0  30  60d=0 d=0.1 d=0.3 d=0.5\n# of Predicate  \n(b) Generated Workloads with Predicate Drift\n 0 20 40 60 80\n 0  10  20  30  40# of Predicate\n# of Join Patternd=0\n 0 20 40 60 80\n 0  10  20  30  40# of Predicate\n# of Join Patternd=0.5\n(c) Generated Workloads with Mixed Drift\nFigure 6: Performance on Generating Drifted Workloads\nwith Varying Drift Factors\nmovies are linked to actors via actor ID. We plot the average corre-\nlation error of these two joint tables in Figure 5(a) and Figure 5(b),\nrespectively. We can observe that NeurBench consistently achieves\nlower average correlation errors than RelDDPM across all drift\nfactors, with the correlation error increasing nearly linearly as the\ndrift factor grows. In contrast, RelDDPM fails to generate data for\ndrift factors greater than 0.4, and its correlation error fluctuates\nunpredictably, showing no clear relationship with the drift factor.\nThese results demonstrate that due to our proposed drift-aware\ndata generation framework, NeurBench can effectively synthesize\ndrifted data while preserving correlations in a controlled manner.\nWe then study the general applicability of NeurBench in syn-\nthesizing drifted data. To this end, we utilize NeurBench to gener-\nate drifted data for three additional real-world datasets, including\nFacebook [ 16], Shopper [ 46], and WeatherAUS [ 56]. As shown in\nFigure 5(c), Figure 5(d), and Figure 5(e), NeurBench consistently\nachieves lower average correlation error than RelDDPM across dif-\nferent datasets and drift factors. Moreover, NeurBench maintains a\ncontrolled increase in correlation error as the drift factor increases,\nwhereas RelDDPM exhibits erratic correlation errors and strug-\ngles to generate drifted data for high drift factors. We therefore\nconfirm the ability of NeurBench to generate data with controlled\ndrift while preserving correlation, demonstrating its effectiveness\nin generating drifted data across different datasets.\n6.2.2 Experiments on generating drifted workloads, under varying\ndrift factors. We now evaluate NeurBench in generating drifted\nworkloads, with drift factors ğ‘‘=0.1,ğ‘‘=0.3, andğ‘‘=0.5. We first assess\nthe effectiveness of generating drifted workloads with join pattern\ndrift by measuring the probability distribution of each join pattern,where probability refers to its proportion in the generated workload.\nIn this section, we use the complex queries indicated in Section 6.1.2\nas the original workload. The results in Figure 6(a) indicate that, as\nthe drift factor increases, the distribution becomes more skewed\ncompared to the original workload ( ğ‘‘=0). In addition, under larger\ndrift (ğ‘‘=0.5), some join patterns in the original workload may dis-\nappear in the drifted workload. Therefore, we demonstrate that\nNeurBench can effectively generate a drifted workload that can be\nused to simulate a realistic drift scenario where learned components\nmay encounter unseen query patterns. We then evaluate the per-\nformance of generating workloads with predicate drift. Figure 6(b)\nshows that predicate pattern distributions follow a consistent trend,\ni.e., higher drift factors result in more skewed distributions. These\nresults validate the effectiveness of NeurBench in synthesizing\ndrifted workloads across different types.\nWe also evaluate NeurBench â€™s ability to generate workloads\nwith mixed drift. Specifically, we generate workloads exhibiting\nboth join pattern drift and predicate drift with certain correlation,\nand visualize the distribution of queries across different join pat-\nterns and predicates in Figure 6(c). The results show that the query\ndistribution becomes more skewed as the drift factor increases, with\nfewer distinct categories appearing.\n6.3 Evaluation on Learned Query Optimizers\nHere we evaluate selected learned query optimizers, i.e., Bao [ 39],\nHybridQO [ 63], Lero [ 66], and Neo [ 40], under various data and\nworkload drift scenarios. We use the complex queries in the work-\nload specified in Section 6.1.2 to perform this set of experiments. In\nparticular, we construct a testing set of 33 queries from the original\n113 queries by ensuring all 33 query templates are included. Since\nall these learned optimizers are integrated with PostgreSQL, we also\ninclude PostgreSQL using its traditional optimizer as a reference\nbaseline. The datasets, workloads, and the corresponding training\nand evaluation processes are specified in the following sections.\n6.3.1 Experiments on the complex queries, under data drift with\nvarying drift factors. Given a drift factor ğ‘‘, we first train each learned\noptimizer with the drifted IMDB datasets corresponding to ğ‘‘and\nthe original workload, and then execute all the queries in the testing\nset on the original dataset. We plot the total execution time of these\nqueries in Figure 7(a), and use Figure 7(b) to show the corresponding\nperformance regression (defined in Section 6.1.3) under varying\ndrift factors. We report our observations below:\nO1: None of the evaluated learned optimizers outperform\nPostgreSQL under data drift. This is due to the fact that most\nlearned optimizers depend on the ML model for accurate query\nplan quality assessment. When data distributions drift, the models\ncan become less reliable, leading to suboptimal plan selection.\nO2: HybridQO, Neo, and Bao exhibit performance regression\nwith increased data drift, while the performance regression\nof Lero is negligible. This can be attributed to the fact that Hy-\nbridQO, Neo, and Bao rely on ML models that learn query plan\nexecution times from historical executions. Since execution time\nis susceptible to data distribution drift, their learned models can\nstruggle to generalize under drift. In contrast, Lero, which adopts\na ranking-based model, demonstrates better robustness to data\n\nNeurBench: Benchmarking Learned Database Components with Data and Workload Drift Modeling\n 10 100 1000 10000\n0 0.1 0.3 0.5Execution Time (s)\nDrift Factor\n(a) Execution Time - Data Drift\n 0 2 4 6 8 10 12 14\n0 0.1 0.3 0.5Perf. Regression\nDrift Factor (b) Regression - Data Drift\n 10 100 1000 10000\n0 0.1 0.3 0.5Execution Time (s)\nDrift Factor\n(c) Execution Time - Join Pattern Drift\n 0 0.5 1 1.5\n0 0.1 0.3 0.5Perf. Regression\nDrift Factor (d) Regression - Join Pattern Drift\n(e) Execution Time - Join Pattern Drift and Predicate Drift\nFigure 7: Performance of Learned Query Optimizers under\nData Drift, Join Pattern Drift, and Predicate Drift with Vary-\ning Drift Factors\ndrift, as the relative ranking can be more stable than exact exe-\ncution time predictions. Further, the stable performance of Lero\nalso stems from its cardinality-based plan search. While cardinality\nfor a certain query may change with drift, Lero may have already\nencountered similar plans with changed cardinality during training,\nthus enhancing its adaptability to data drift.\nO3: Bao experiences the highest performance regression un-\nder data drift, followed by HybridQO and then Neo. Bao relies\nentirely on best-only plan selection, whereas HybridQO explores\na top-k plan strategy. As a result, HybridQO may have learned\nsuboptimal plans for certain data distributions that become more\neffective under drifted data, making it more generalizable than Bao.\nUnlike Bao and HybridQO, Neo learns the cost of each subquery,\nwhich tends to be more stable under data drift since subquery cost\nestimation is less affected by global data distribution drift.\n6.3.2 Experiments on the complex queries, under workloads with\nvarious join pattern drift and predicate drift. We train each learned\noptimizer with the original data and the workloads with join pattern\ndrift corresponding to drift factor ğ‘‘, and then execute the queries in\nthe testing sets. We report the total execution time of these queries\nin Figure 7(c) and the corresponding performance regression under\nvarying drift factors in Figure 7(d). We can observe that:\nO4: Join pattern drift affects the performance of HybirdQO,\nNeo, and Bao more significantly than Lero. HybridQO and Bao\nbecome ineffective under higher join pattern drift because they\nboth rely on hints to generate query candidates, preventing them\n 0 10 20 30Total Execution Time (s)\n 10-210-1100101\nQ1Q2Q3Q4Q6Q10Q11Q12Q13Q14Q16Q17Q18Q19Q20Q21Execution Time (s)\nTPC-H QueriesBao (d=0) Bao (d=0.5)\n 0 10 20Total Execution Time (s)\n 10-210-110-1100\nQ2 Q3 Q4 Q9 Q10 Q12 Q13 Q14 Q16 Q17 Q19 Q22Execution Time (s)\nTPC-H QueriesLero (d=0) Lero (d=0.5)Figure 8: Performance Evaluation of Learned Query\nOptimizers using Drifted CH-Benchmark Workload\nfrom adapting to unseen join patterns and leading to inaccurate\nlatency predictions. Neo, which employs a white-box plan search\nwith a best-only plan selection strategy, is more susceptible to local\noptima and thus struggles to explore globally optimal plans under\njoin pattern drift. In contrast, as discussed in O2, Lero generates\ncandidate plans by varying cardinality estimations, enabling it to\nbetter tolerate join pattern drift.\nWe then fix the drift factor ğ‘‘to 0.5 and conduct experiments on\nworkloads with join pattern drift and predicate drift to investigate\nthe performance of learned query optimizers under different types\nof workload drift. Each learned optimizer is trained on the original\ndata with the drifted workload and then evaluated using the same\nset of testing queries. The results in Figure 7(e) show that:\nO5: HybridQO and Neo are more sensitive to join pattern drift\nthan Bao. HybridQO relies on join-order-based hints, making it\nmore susceptible to changes in join patterns, whereas Bao uses more\ngeneral hints that incorporate both join patterns and predicates,\nallowing it to adapt better to such drift. Further, as discussed in O3,\nNeo learns the cost of subqueries individually. While this makes\nit more stable under data drift, it increases its sensitivity to query\npattern drift, including changes in join patterns.\n6.3.3 Experiments on CH-Benchmark workload with mixed drift.\nWe now evaluate Bao and Lero using the CH-Benchmark workload\nspecified in Section 6.1.2. We exclude other learned query optimizers\nbecause their implementations have specific requirements on the\nquery format, and modifying their code to handle analytical queries\nin CH-Benchmark is cumbersome. For the evaluation, we generate\nworkloads exhibiting mixed drift (combining both join-pattern drift\nand predicate drift) with drift degree ğ‘‘=0.5. We then measure\nand compare the execution times for the original queries ( ğ‘‘=0)\nand the drifted queries ( ğ‘‘=0.5). Queries unsupported by Bao and\nLero are omitted from the analysis. The results, plotted in Figure 8,\nindicate that workload drift impacts the performance of both Bao\nand Lero, consistent with the trends observed in Figure 7(c).\nDiscussion: Based on our observations, we summarize the follow-\ning design trade-offs in learned query optimizers under data and\nworkload drift: 1) Black-box plan search generally achieves better\nperformance than white-box plan search (O1); 2) Cardinality-based\nplan search demonstrates greater generalizability across different\ndrift scenarios (O2, O4); 3) White-box plan search can be more\n\nZhanhao Zhao, Haotian Gao, Naili Xing, Lingze Zeng, Meihui Zhang,\nGang Chen, Manuel Rigger, Beng Chin Ooi\n 0 1 2 3 4\n0 0.1 0.3 0.5Throughput (106 ops/s)\nDrift Factor\n(a) Throughput - Range Scan\n 0 20 40 60 80 100\n0 0.1 0.3 0.5Throughput Drop (%)\nDrift Factor (b) Drop Ratio - Range Scan\n 100 1000\n0 0.1 0.3 0.5Memory Usage (MB)\nDrift Factor\n(c) Memory Usage\n00.20.40.60.81\n00.10.30.5 00.10.30.5Xindex AlexNorm. Runtime\nDrift FactorsSearch Others (d) Latency Breakdown - Range Scan\n 0.1 1 10 100\n0 0.1 0.3 0.5Point Select Scan Length:50 Scan Length:100Throughput (106 ops/s)\n 0 0.1 0.3 0.5Point Select Scan Length:50 Scan Length:100\nDrift Factor00.1 0.3 0.5Point Select Scan Length:50 Scan Length:100\n \n(e) Throughput - Varying the Scan Length (Number of Keys to Scan)\nFigure 9: Performance of Learned Indexes under Data Drift\nwith Varying Drift Factors\nresilient to data drift than black-box plan search, while top-k plan\nsearch enhances generalizability (O3); 4) Join-order-based hints\nare more susceptible to join pattern drift, whereas general hints\nprovide better adaptability to both join pattern and predicate drift\n(O5). Given these trade-offs, we envision a more flexible learned\nquery optimizer that dynamically selects the best-performing de-\nsign choices based on the current system conditions, balancing\nperformance and adaptability under data and workload drift.\n6.4 Evaluation on Learned Indexes\nWe now study the performance of selected learned indexes, i.e.,\nALEX [ 13], XIndex [ 50], PGM-Index [ 14], and LIPP [ 60], with vary-\ning data drift scenarios. Since all the evaluated learned indexes\nare in-memory, we include the in-memory implementation of two\ntraditional indexes, Adaptive Radix Tree (ART) [ 29] and B+-tree, as\nbaselines. We use the IMDB Name table, and build the index on the\nIDattribute as the key and the remaining attributes as values.\n6.4.1 Experiments on scan operations, under varying drift factors.\nGiven a drift factor ğ‘‘, we construct data drift scenarios to evaluate\nlearned index performance, following these steps: 1) Initialize the\nindex on the drifted table generated based on ğ‘‘; 2) Create a residual\nset containing records from the original table that are absent in the\ndrifted table; 3) Execute these insert operations to drift the table\nback toward the original table, and update the index correspond-\ningly. Afterward, we perform range scan operations (specified in\nSection 6.1.2) on the index with the default settings, unless other-\nwise specified. All these operations are run with a single thread,following prior works [ 13,60]. We plot the throughput of range\nscan operations in Figure 9(a) and use Figure 9(b) to show the corre-\nsponding throughput drop (defined in Section 6.1.3). We also record\nthe index size after completing the scan operations and present the\nresults in Figure 9(c). We can observe that:\nO1: ALEX achieves the highest scan throughput among all\nevaluated learned indexes across all drift factors. This can be\nattributed to the heterogeneous structure of ALEX, where data is\nstored exclusively in the leaf nodes, making its scan process closely\nresemble that of B+-tree. In ALEX, a range scan first locates the\nposition of the start_key by key search, and then sequentially scans\nthe following leaf nodes. In contrast, LIPP employs a homogeneous\nstructure that stores data within each node, preventing it from\nsupporting direct sequential scans. As a result, while ALEX can\ncomplete a range scan with a single key search, LIPP requires\nmultiple key searches, leading to higher overhead. Further, although\nXIndex and PGM-Index also adopt a heterogeneous structure, they\nmaintain separate buffers for handling insertions, which however\naffects their range scan performance.\nO2: The throughput of ALEX and LIPP decreases as drift\nincreases, with LIPP experiencing a more significant drop\nthan ALEX. This is expected because both ALEX and LIPP employ\nin-place update strategies, which lead to higher key search costs\nas drift increases, resulting in degraded range scan performance.\nLIPP in particular, incurs higher key search costs under larger drift\nbecause it requires more key searches per range scan than ALEX,\nas discussed in O1. To further analyze this behavior, we evaluate\nscan performance under different fixed scan lengths. Specifically,\nwe measure throughput with scan lengths of 1 (point select), 50,\nand 100, respectively. As shown in Figure 9(e), the throughput of\nall learned indexes decreases with larger scan length, while LIPP\nexhibits the steepest drop. Since a larger scan length triggers more\nkey searches, these results confirm that LIPP is more sensitive to\nkey search efficiency.\nO3: ALEX incurs a higher performance drop than XIndex\nwith increased drift. These observations indicate that the delta-\nbuffer strategy used in XIndex is more resilient to drift than the\nin-place update strategy used in ALEX. To verify this, we conduct a\nlatency breakdown analysis for range scan operations. The results in\nFigure 9(d) demonstrate that for ALEX, key search cost (denoted as\nâ€œSearchâ€) is the dominant bottleneck, and increases as drift increases.\nFurther, although the scan performance of XIndex can be affected\nby the additional overhead of buffer scanning (as discussed in O1),\nthis additional cost remains stable due to its fine-grained node-\nlevel buffer design, which further contributes to its relatively stable\nperformance under drift.\nO4: Only LIPPâ€™s size grows with larger drift, while PGM-\nIndex achieves the smallest index size. LIPP employs an in-\nplace update strategy with a chain method, which splits nodes when\nconflicts arise in the targeting node. As drift increases, more node\nsplits can potentially occur, causing LIPPâ€™s index size to expand.\nRetrieving data from a larger index also incurs higher key search\ncosts, which align with our observations in O2. Moreover, PGM-\nIndex maintains a global tree-structured buffer for the entire index,\nwhich avoids sparsity and minimizes storage overhead.\n\nNeurBench: Benchmarking Learned Database Components with Data and Workload Drift Modeling\n 0 5 10 15 20\nBalanced Point\nHeavyWrite\nHeavyScan\nHeavyThroughput (106 ops/s)\n(a) Throughput\n 0 400 800 1200\nBalanced Point\nHeavyWrite\nHeavyScan\nHeavyLatency (ns) (b) Latency\nFigure 10: Performance of Learned Indexes with Concurrent\nRead/Write Access\n6.4.2 Experiments on concurrent read/write access under mixed\nworkloads. We evaluate the performance of learned indexes un-\nder concurrent read and write workloads. Based on default oper-\nations specified in Section 6.1.2, we construct 4 mixed workloads:\n(1) Balanced workload, which contains 33% inserts to put keys into\nindexes, 33% point reads to randomly lookup a single key, and 33%\nrange scans to randomly read keys with default scan length. (2)\nPoint-heavy workload, which consists of 90% point reads, 5% writes,\nand 5% range scans. (3) Write-heavy workload, which contains 90%\nwrites, 5% point reads, and 5% range scans. (4) Scan-heavy workload,\nwhich consists of 90% range scans, 5% writes, and 5% point reads.\nEach experiment executes a total of 4M operations using 4 con-\ncurrent threads. We adopt the implementation of ART and B+-tree\nwith optimistic lock coupling (OLC) [ 30] as provided in [ 55], and\nuse the concurrent implementations of ALEX and LIPP as provided\nin [59], to conduct our experiments.\nWe report the throughput and latency across different mixed\nworkloads in Figure 10. We can observe that, under the balanced\nworkload, ALEX outperforms the other indexes due to its relatively\nstable performance across writes, point reads, and scans. Regarding\nthe point-heavy workload, LIPP archives the best performance\nbecause of its precise point-lookup technique. Learned indexes\ncannot outperform ART under the write-heavy workload, primarily\ndue to their inherent overhead for model updates and maintenance.\nIn addition, ALEX demonstrates superior performance under the\nscan-heavy workload for similar reasons as discussed in O1.\nDiscussion: We provide the following design trade-offs in learned\nindexes under data drift according to our observations: 1) A hetero-\ngeneous structure with an in-place insertion strategy achieves the\nbest range scan performance (O1); 2) A heterogeneous structure\nwith a fine-grained delta-buffer strategy can ensure stable range\nscan performance under drift (O3); 3) A homogeneous structure\nhandles point reads efficiently but cannot perform well for writes\nand range scans (O2); 4) A tree-level buffer helps minimize index\nsize (O4). Given that the different design choices are suitable for\nspecific drift scenarios and drift-tolerance requirements, we en-\nvision that designing a more flexible index structure capable of\ndynamically adapting to system conditions would be promising.\n6.5 Evaluation on Learned Concurrency Control\nWe now study the throughput of learned Concurrency Control (CC)\nwith varying workload (arrival rate) drift. Our evaluation includes\nstate-of-the-art learned CC, Polyjuice [ 53], with two traditional CC\nalgorithms, 2PL and OCC [ 62], and a hybrid CC algorithm, IC3 [ 54].\nWe use an initial arrival rate simulated with a single client thread,\n(a)(b)Figure 11: Throughput of Learned CC under Arrival Rate\nDrift with Varying Drift Factors (Default Workload)\n(a)(b)\nFigure 12: Throughput of Learned CC under Arrival Rate\nDrift with Varying Drift Factors (CH-Benchmark Workload)\nand apply the drift factors from 0.1 to 0.5, which gradually increases\ntransaction arrival rates. Polyjuice is first trained on the initial ar-\nrival rate and then retrained after each arrival rate drift, following\nthe retraining procedure outlined in [ 53]. We perform the default\ntransactions specified in Section 6.1.2. Figure 11(a) plots the trans-\naction throughput, while Figure 11(b) presents the corresponding\nnormalized throughput. The results show that, as drift increases,\nPolyjuice achieves higher throughput due to the increased arrival\nrate. However, Polyjuice requires more time for convergence to the\nmost efficient policy under higher drift. This can be attributed to\nthe fact that greater drift increases the complexity of learning an op-\ntimal policy due to limited prior knowledge. In addition, Polyjuiceâ€™s\nretraining process is time-consuming, which hinders its ability to\nadapt efficiently to continuous arrival rate drift.\nWe conduct additional experiments using the CH-Benchmark\nworkload under transaction arrival rate drift. Specifically, we run\nour evaluations on the transactional workloads from CH-Benchmark\nwith varying the drift factor across 0.1, 0.3, and 0.5. The transac-\ntion throughput results are presented in Figure 12. As shown in\nthe figure and consistent with observations in Figure 11, Polyjuice\nachieves higher throughput but requires longer convergence time\nto reach an optimal policy as the drift factor increases.\nDiscussion: A learned CC algorithm must understand transactional\ndependencies and actions to be taken when conflicts arise in its\nlearning. Fast adaptation is crucial for learned CC to be practical\nin real-world scenarios since transactions can be completed in\nmilliseconds or less. To validate this point, we perform additional\nexperiments to assess CCaaLF [ 44], a recently proposed learned CC\nalgorithm that enables fast adaptation under dynamic workloads.\nWe rerun our experiments on both the default workloads and CH-\nBenchmark workloads under arrival rate drift ( ğ‘‘=0.5), and plot the\nresults in Figure 13. As observed, CaaLF achieves higher throughput\nand faster convergence than Polyjuice. This improved adaptability\n\nZhanhao Zhao, Haotian Gao, Naili Xing, Lingze Zeng, Meihui Zhang,\nGang Chen, Manuel Rigger, Beng Chin Ooi\n 200 400 600\n0.5 1 1.5 2Throughput (103 Txns/s)\nElapse Time (h)PolyJuice CCaaLF\n(a) Default Workload\n 150 200 250\n0.5 1 1.5 2Throughput (103 Txns/s)\nElapse Time (h)PolyJuice CCaaLF (b) CH-Benchmark Workload\nFigure 13: CCaaLF vs PolyJuice (Arrival Rate Drift with Drift\nFactorğ‘‘=0.5)\nis mainly due to the fact that CCaaLF explicitly models transaction\ndependencies in action selection, enabling it to dynamically adjust\nits policy based on the current system state.\n7 RELATED WORK\nBenchmarks for Learned Database Components. Multiple\nbenchmarks have been devised for various learned database compo-\nnents, such as learned query optimizers [ 6,18], learned indexes [ 26,\n48], disk-oriented [ 26,37] and hardware-enhanced learned compo-\nnents [ 41,65],etc. Recent studies [ 39,67] also attempt to explore\nbenchmarking learned database components under drifted data or\nworkloads. However, their drift modeling remains simplistic, failing\nto capture diverse drift scenarios needed for a systematic perfor-\nmance assessment across different learned database components.\nNeurBench , in contrast, provides controllable and effective drifted\ndata and workload generation, enabling a more comprehensive\nperformance evaluation under diverse drift scenarios.\nBenchmarks with Distribution Drift. Several benchmarks have\nbeen developed to evaluate performance under distribution drift.\nDSB [ 12] a standard benchmark for decision-making applications,\nenables evaluations under distribution drift using TPC-DS. How-\never, it is restricted to exponential distributions and cannot quan-\ntitatively measure drift as NeurBench . Concept drift modeling [ 4,\n33,45] has gained traction in ML-related fields. However, they are\nnot directly applicable to database benchmarking, as they either\nfocus on non-tabular data or fail to consider the various types of\ndrift that occur in databases. Emerging dynamic benchmarks [ 3]\nshow promising potential. As a dynamic benchmark, NeurBench\nintroduces measurable and controllable data and workload drift\nbased on real drifts to enable systematic performance evaluations.\n8 CONCLUSIONS\nThis paper introduced NeurBench , a new benchmark suite designed\nto evaluate end-to-end learned DBMSs containing all learned com-\nponents under controllable data and workload drift. We defined the\ndrift factor to effectively quantify drift, and introduce a novel drift-\naware data and workload generation framework. Extensive experi-\nmental results show the effectiveness of NeurBench in drifted data\nand workload generation, and demonstrate that different learned\ndatabase components withstand and adapt to data and workload\ndrift to varying degrees, depending on their specific design choices.\nThrough in-depth analysis, we offer recommendations for enhanc-\ning the adaptability of learned database components.\nREFERENCES\n[1] Christoph Anneser, Nesime Tatbul, David E. Cohen, Zhenggang Xu, Prithviraj\nPandian, Nikolay Laptev, and Ryan Marcus. 2023. AutoSteer: Learned Query\nOptimization for Any SQL Database. Proc. VLDB Endow. 16, 12 (2023), 3515â€“3527.[2]Internet Archive. 2025. IMDB Archive . https://web.archive.org/web/\n20240301000000*/https://datasets.imdbws.com/\n[3] Lawrence Benson, Carsten Binnig, Jan-Micha Bodensohn, Federico Lorenzi, Jigao\nLuo, Danica Porobic, Tilmann Rabl, Anupam Sanghi, Russell Sears, Pinar TÃ¶zÃ¼n,\nand Tobias Ziegler. 2024. Surprise Benchmarking: The Why, What, and How. In\nDBTest@SIGMOD . ACM, 1â€“8.\n[4] Maximilian BÃ¶ther, Foteini Strati, Viktor Gsteiger, and Ana Klimovic. 2023. To-\nwards A Platform and Benchmark Suite for Model Training on Dynamic Datasets.\nInEuroMLSys@EuroSys . ACM, 8â€“17.\n[5] Tianyi Chen, Jun Gao, Hedui Chen, and Yaofeng Tu. 2023. LOGER: A Learned\nOptimizer towards Generating Efficient and Robust Query Execution Plans. Proc.\nVLDB Endow. 16, 7 (2023), 1777â€“1789.\n[6] Yannis Chronis, Yawen Wang, Yu Gan, Sami Abu-El-Haija, Chelsea Lin, Carsten\nBinnig, and Fatma Ã–zcan. 2024. CardBench: A Benchmark for Learned Cardinality\nEstimation in Relational Databases. CoRR abs/2408.16170 (2024).\n[7] Richard Cole, Florian Funke, Leo Giakoumakis, Wey Guy, Alfons Kemper, Stefan\nKrompass, Harumi Kuno, Raghunath Nambiar, Thomas Neumann, Meikel Poess,\net al.2011. The mixed workload CH-benCHmark. In Proceedings of the Fourth\nInternational Workshop on Testing Database Systems . 1â€“6.\n[8] Brian F. Cooper, Adam Silberstein, Erwin Tam, Raghu Ramakrishnan, and Russell\nSears. 2010. Benchmarking cloud serving systems with YCSB. In SoCC . ACM,\n143â€“154.\n[9] The Transaction Processing Council. 2024. TPC-C . http://www.tpc.org/tpcc/\n[10] The Transaction Processing Council. 2024. TPC-H . http://www.tpc.org/tpch/\n[11] Prafulla Dhariwal and Alexander Quinn Nichol. 2021. Diffusion Models Beat\nGANs on Image Synthesis. In NeurIPS . 8780â€“8794.\n[12] Bailu Ding, Surajit Chaudhuri, Johannes Gehrke, and Vivek R. Narasayya. 2021.\nDSB: A Decision Support Benchmark for Workload-Driven and Traditional\nDatabase Systems. Proc. VLDB Endow. 14, 13 (2021), 3376â€“3388.\n[13] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\nDavid B. Lomet, and Tim Kraska. 2020. ALEX: An Updatable Adaptive Learned\nIndex. In SIGMOD Conference . ACM, 969â€“984.\n[14] Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-index: a fully-dynamic\ncompressed learned index with provable worst-case bounds. Proc. VLDB Endow.\n13, 8 (2020), 1162â€“1175.\n[15] Josh Gardner, Zoran Popovic, and Ludwig Schmidt. 2023. Benchmarking Distri-\nbution Shift in Tabular Data with TableShift. In NeurIPS .\n[16] Minas Gjoka, Maciej Kurant, Carter T. Butts, and Athina Markopoulou. 2010.\nWalking in Facebook: A Case Study of Unbiased Sampling of OSNs. In INFOCOM .\nIEEE, 2498â€“2506.\n[17] Zhihan Guo, Kan Wu, Cong Yan, and Xiangyao Yu. 2021. Releasing Locks As\nEarly As You Can: Reducing Contention of Hotspots by Violating Two-Phase\nLocking. In SIGMOD Conference . ACM, 658â€“670.\n[18] Yuxing Han, Ziniu Wu, Peizhi Wu, Rong Zhu, Jingyi Yang, Liang Wei Tan,\nKai Zeng, Gao Cong, Yanzhao Qin, Andreas Pfadler, Zhengping Qian, Jingren\nZhou, Jiangneng Li, and Bin Cui. 2021. Cardinality Estimation in DBMS: A\nComprehensive Benchmark Evaluation. Proc. VLDB Endow. 15, 4 (2021), 752â€“\n765.\n[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising Diffusion Probabilistic\nModels. In NeurIPS .\n[20] IMDB. 2025. IMDB Dataset . https://www.imdb.com/\n[21] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin\nZhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas\nPhillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton Earn-\nshaw, Imran S. Haque, Sara M. Beery, Jure Leskovec, Anshul Kundaje, Emma\nPierson, Sergey Levine, Chelsea Finn, and Percy Liang. 2021. WILDS: A Bench-\nmark of in-the-Wild Distribution Shifts. In ICML (Proceedings of Machine Learning\nResearch, Vol. 139) . PMLR, 5637â€“5664.\n[22] Akim Kotelnikov, Dmitry Baranchuk, Ivan Rubachev, and Artem Babenko. 2023.\nTabDDPM: Modelling Tabular Data with Diffusion Models. In ICML (Proceedings\nof Machine Learning Research, Vol. 202) . PMLR, 17564â€“17579.\n[23] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In SIGMOD Conference . ACM, 489â€“504.\n[24] Meghdad Kurmanji, Eleni Triantafillou, and Peter Triantafillou. 2024. Machine\nUnlearning in Learned Databases: An Experimental Analysis. Proc. ACM Manag.\nData 2, 1 (2024), 49:1â€“49:26.\n[25] Meghdad Kurmanji and Peter Triantafillou. 2023. Detect, Distill and Update:\nLearned DB Systems Facing Out of Distribution Data. Proc. ACM Manag. Data 1,\n1 (2023), 33:1â€“33:27.\n[26] Hai Lan, Zhifeng Bao, J. Shane Culpepper, and Renata Borovica-Gajic. 2023.\nUpdatable Learned Indexes Meet Disk-Resident DBMS - From Evaluations to\nDesign Choices. Proc. ACM Manag. Data 1, 2 (2023), 139:1â€“139:22.\n[27] Claude Lehmann, Pavel Sulimov, and Kurt Stockinger. 2024. Is Your Learned\nQuery Optimizer Behaving As You Expect? A Machine Learning Perspective.\nProc. VLDB Endow. 17, 7 (2024), 1565â€“1577.\n\nNeurBench: Benchmarking Learned Database Components with Data and Workload Drift Modeling\n[28] Viktor Leis, Andrey Gubichev, Atanas Mirchev, Peter A. Boncz, Alfons Kemper,\nand Thomas Neumann. 2015. How Good Are Query Optimizers, Really? Proc.\nVLDB Endow. 9, 3 (2015), 204â€“215.\n[29] Viktor Leis, Alfons Kemper, and Thomas Neumann. 2013. The adaptive radix\ntree: ARTful indexing for main-memory databases. In ICDE . IEEE Computer\nSociety, 38â€“49.\n[30] Viktor Leis, Florian Scheibner, Alfons Kemper, and Thomas Neumann. 2016. The\nART of practical synchronization. In DaMoN . ACM, 3:1â€“3:8.\n[31] Beibin Li, Yao Lu, and Srikanth Kandula. 2022. Warper: Efficiently Adapting\nLearned Cardinality Estimators to Data and Workload Drifts. In SIGMOD Con-\nference . ACM, 1920â€“1933.\n[32] Pengfei Li, Wenqing Wei, Rong Zhu, Bolin Ding, Jingren Zhou, and Hua Lu. 2023.\nALECE: An Attention-based Learned Cardinality Estimator for SPJ Queries on\nDynamic Workloads. Proc. VLDB Endow. 17, 2 (2023), 197â€“210.\n[33] Wendi Li, Xiao Yang, Weiqing Liu, Yingce Xia, and Jiang Bian. 2022. DDG-DA:\nData Distribution Generation for Predictable Concept Drift Adaptation. In AAAI .\nAAAI Press, 4092â€“4100.\n[34] Yu-Shan Lin, Ching Tsai, Tz-Yu Lin, Yun-Sheng Chang, and Shan-Hung Wu.\n2021. Donâ€™t Look Back, Look into the Future: Prescient Data Partitioning and\nMigration for Deterministic Database Systems. In SIGMOD Conference . ACM,\n1156â€“1168.\n[35] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo P. Mandic,\nWenwu Wang, and Mark D. Plumbley. 2023. AudioLDM: Text-to-Audio Gener-\nation with Latent Diffusion Models. In ICML (Proceedings of Machine Learning\nResearch, Vol. 202) . PMLR, 21450â€“21474.\n[36] Tongyu Liu, Ju Fan, Nan Tang, Guoliang Li, and Xiaoyong Du. 2024. Controllable\nTabular Data Synthesis Using Diffusion Models. Proc. ACM Manag. Data 2, 1\n(2024), 28:1â€“28:29.\n[37] Chaohong Ma, Xiaohui Yu, Yifan Li, Xiaofeng Meng, and Aishan Maoliniyazi.\n2022. FILM: a Fully Learned Index for Larger-than-Memory Databases. Proc.\nVLDB Endow. 16, 3 (2022), 561â€“573.\n[38] Christopher D. Manning and Hinrich SchÃ¼tze. 2001. Foundations of statistical\nnatural language processing . MIT Press.\n[39] Ryan Marcus, Parimarjan Negi, Hongzi Mao, Nesime Tatbul, Mohammad Al-\nizadeh, and Tim Kraska. 2021. Bao: Making Learned Query Optimization Practical.\nInSIGMOD Conference . ACM, 1275â€“1288.\n[40] Ryan Marcus, Parimarjan Negi, Hongzi Mao, Chi Zhang, Mohammad Alizadeh,\nTim Kraska, Olga Papaemmanouil, and Nesime Tatbul. 2019. Neo: A Learned\nQuery Optimizer. Proc. VLDB Endow. 12, 11 (2019), 1705â€“1718.\n[41] Songsong Mo, Yile Chen, Hao Wang, Gao Cong, and Zhifeng Bao. 2023. Lemo: A\nCache-Enhanced Learned Optimizer for Concurrent Queries. Proc. ACM Manag.\nData 1, 4 (2023), 247:1â€“247:26.\n[42] Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. 2016. Convolutional Neural\nNetworks over Tree Structures for Programming Language Processing. In AAAI .\nAAAI Press, 1287â€“1293.\n[43] NeurBench. 2025. NeurBench Implementation . https://github.com/neurdb/\nneurbench\n[44] Hexiang Pan, Shaofeng Cai, Yeow Meng Chee, Tien Tuan Anh Dinh, Yuncheng\nWu, and Beng Chin Ooi. 2025. CCaaLF: Concurrency Control as a Learnable\nFunction. CoRR abs/2503.10036 (2025).\n[45] Piotr Porwik and Benjamin Mensah Dadzie. 2022. Detection of data drift in a\ntwo-dimensional stream using the Kolmogorov-Smirnov test. In KES (Procedia\nComputer Science, Vol. 207) . Elsevier, 168â€“175.\n[46] Cemal Okan Sakar, Suleyman Olcay Polat, Mete Katircioglu, and Yomi Kastro.\n2019. Real-time prediction of online shoppersâ€™ purchasing intention using multi-\nlayer perceptron and LSTM recurrent neural networks. Neural Comput. Appl. 31,\n10 (2019), 6893â€“6908.\n[47] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli.\n2015. Deep Unsupervised Learning using Nonequilibrium Thermodynamics. In\nProceedings of the 32nd International Conference on Machine Learning, ICML 2015,Lille, France, 6-11 July 2015 (JMLR Workshop and Conference Proceedings, Vol. 37) ,\nFrancis R. Bach and David M. Blei (Eds.). JMLR.org, 2256â€“2265.\n[48] Zhaoyan Sun, Xuanhe Zhou, and Guoliang Li. 2023. Learned Index: A Compre-\nhensive Experimental Evaluation. Proc. VLDB Endow. 16, 8 (2023), 1992â€“2004.\n[49] Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved\nSemantic Representations From Tree-Structured Long Short-Term Memory Net-\nworks. In ACL (1) . The Association for Computer Linguistics, 1556â€“1566.\n[50] Chuzhe Tang, Youyun Wang, Zhiyuan Dong, Gansen Hu, Zhaoguo Wang, Minjie\nWang, and Haibo Chen. 2020. XIndex: a scalable learned index for multicore\ndata storage. In PPoPP . ACM, 308â€“320.\n[51] Dixin Tang, Hao Jiang, and Aaron J. Elmore. 2017. Adaptive Concurrency Control:\nDespite the Looking Glass, One Concurrency Control Does Not Fit All. In CIDR .\nwww.cidrdb.org.\n[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In NIPS . 5998â€“6008.\n[53] Jia-Chen Wang, Ding Ding, Huan Wang, Conrad Christensen, Zhaoguo Wang,\nHaibo Chen, and Jinyang Li. 2021. Polyjuice: High-Performance Transactions\nvia Learned Concurrency Control. In OSDI . USENIX Association, 198â€“216.\n[54] Zhaoguo Wang, Shuai Mu, Yang Cui, Han Yi, Haibo Chen, and Jinyang Li. 2016.\nScaling Multicore Databases via Constrained Parallel Execution. In SIGMOD\nConference . ACM, 1643â€“1658.\n[55] Ziqi Wang, Andrew Pavlo, Hyeontaek Lim, Viktor Leis, Huanchen Zhang,\nMichael Kaminsky, and David G. Andersen. 2018. Building a Bw-Tree Takes\nMore Than Just Buzz Words. In SIGMOD Conference . ACM, 473â€“488.\n[56] WeatherAUS. 2025. WeatherAUS DataSet . https://www.kaggle.com/jsphyg/\nweather-dataset-rattle-package\n[57] Wikipedia. 2025. Pearson Correlation Coefficient . https://en.wikipedia.org/wiki/\nPearson_correlation_coefficient\n[58] Olivia Wiles, Sven Gowal, Florian Stimberg, Sylvestre-Alvise Rebuffi, Ira Ktena,\nKrishnamurthy Dvijotham, and Ali Taylan Cemgil. 2022. A Fine-Grained Analysis\non Distribution Shift. In ICLR . OpenReview.net.\n[59] Chaichon Wongkham, Baotong Lu, Chris Liu, Zhicong Zhong, Eric Lo, and\nTianzheng Wang. 2022. Are Updatable Learned Indexes Ready? Proc. VLDB\nEndow. 15, 11 (2022), 3004â€“3017.\n[60] Jiacheng Wu, Yong Zhang, Shimin Chen, Yu Chen, Jin Wang, and Chunxiao Xing.\n2021. Updatable Learned Index with Precise Positions. Proc. VLDB Endow. 14, 8\n(2021), 1276â€“1288.\n[61] Peizhi Wu and Zachary G. Ives. 2024. Modeling Shifting Workloads for Learned\nDatabase Systems. Proc. ACM Manag. Data 2, 1 (2024), 38:1â€“38:27.\n[62] Yu Xia, Xiangyao Yu, Matthew Butrovich, Andrew Pavlo, and Srinivas Devadas.\n2022. Litmus: Towards a Practical Database Management System with Verifiable\nACID Properties and Transaction Correctness. In SIGMOD Conference . ACM,\n1478â€“1492.\n[63] Xiang Yu, Chengliang Chai, Guoliang Li, and Jiabin Liu. 2022. Cost-based or\nLearning-based? A Hybrid Query Optimizer for Query Plan Selection. Proc.\nVLDB Endow. 15, 13 (2022), 3924â€“3936.\n[64] Shunkang Zhang, Ji Qi, Xin Yao, and AndrÃ© Brinkmann. 2024. Hyper: A High-\nPerformance and Memory-Efficient Learned Index via Hybrid Construction. Proc.\nACM Manag. Data 2, 3 (2024), 145.\n[65] Zhou Zhang, Zhaole Chu, Peiquan Jin, Yongping Luo, Xike Xie, Shouhong Wan,\nYun Luo, Xufei Wu, Peng Zou, Chunyang Zheng, Guoan Wu, and Andy Rudoff.\n2022. PLIN: A Persistent Learned Index for Non-Volatile Memory with High\nPerformance and Instant Recovery. Proc. VLDB Endow. 16, 2 (2022), 243â€“255.\n[66] Rong Zhu, Wei Chen, Bolin Ding, Xingguang Chen, Andreas Pfadler, Ziniu Wu,\nand Jingren Zhou. 2023. Lero: A Learning-to-Rank Query Optimizer. Proc. VLDB\nEndow. 16, 6 (2023), 1466â€“1479.\n[67] Rong Zhu, Lianggui Weng, Bolin Ding, and Jingren Zhou. 2024. Learned Query\nOptimizer: What is New and What is Next. In SIGMOD Conference Companion .\nACM, 561â€“569.",
  "textLength": 143421
}