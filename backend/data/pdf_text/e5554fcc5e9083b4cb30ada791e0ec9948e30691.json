{
  "paperId": "e5554fcc5e9083b4cb30ada791e0ec9948e30691",
  "title": "Standard versus uniform binary search and their variants in learned static indexing: The case of the searching on sorted data benchmarking software platform",
  "pdfPath": "e5554fcc5e9083b4cb30ada791e0ec9948e30691.pdf",
  "text": "Standard Vs Uniform Binary Search and Their Variants in Learned\nStatic Indexing: The Case of the Searching on Sorted Data\nBenchmarking Software Platform∗\nDomenico Amato1Giosu\u0013 e Lo Bosco1†Ra\u000baele Giancarlo1\n1Dipartimento di Matematica e Informatica\nUniversit\u0013 a degli Studi di Palermo, ITALY\nJuly 11, 2022\nAbstract\nLearned Indexes are a novel approach to search in a sorted table. A model is used to predict an\ninterval in which to search into and a Binary Search routine is used to \fnalize the search. They are quite\ne\u000bective. For the \fnal stage, usually, the lower bound routine of the Standard C++ library is used,\nalthough this is more of a natural choice rather than a requirement. However, recent studies, that do\nnot use Machine Learning predictions, indicate that other implementations of Binary Search or variants,\nnamely k-ary Search, are better suited to take advantage of the features o\u000bered by modern computer\narchitectures. With the use of the Searching on Sorted Sets SOSD Learned Indexing benchmarking\nsoftware, we investigate how to choose a Search routine for the \fnal stage of searching in a Learned\nIndex. Our results provide indications that better choices than the lower bound routine can be made.\nWe also highlight how such a choice may be dependent on the computer architecture that is to be used.\nOverall, our \fndings provide new and much-needed guidelines for the selection of the Search routine\nwithin the Learned Indexing framework.\n1 Introduction\nLearned Static Indexes, introduced by Kraska et al. [19] (but see also [6]), with follow-up in [19, 4, 5, 12, 11,\n10, 20, 15], are a novel approach to search in a sorted table, quite e\u000bective with respect to existing Procedures\nand Data Structures, e.g., B-trees [7], used in important application domains such as Data Bases [22] and\nSearch Engines [14]. With reference to Figure 1, a generic paradigm for Learned Searching in sorted sets\nconsists of a model, trained over the data in a sorted table. As described in Section 2, such a model may\nbe as simple as a straight line or more complex, with a tree-like structure, as the ones mentioned in Section\n3.2.2. It is used to make a prediction regarding where a query element may be in the sorted table. Then,\nthe search is limited to the interval so identi\fed and performed via Standard Binary Search. The use of this\nlatter routine is more of a natural choice rather than a requirement. In fact, the lower bound routine from\nthe standard C++ library is almost exclusively used.\nIn order to place our contributions on the proper ground, it is useful to recall that two major studies\n[14, 24] have recently investigated which Binary Search routines or variants are better suited to take advantage\n∗This research is funded in part by MIUR Project of National Relevance 2017WR7SHH \\Multicriteria Data Structures\nand Algorithms: from compressed to learned indexes, and beyond\". We also acknowledge an NVIDIA Higher Education\nand Research Grant (donation of a Titan V GPU). Additional support to RG has been granted by Project INdAM - GNCS\n\\Modellizazzione ed analisi di big knowledge graphs per la risoluzione di problemi in ambito medico e web\"\n†corresponding author, email: giosue.lobosco@unipa.it\n1arXiv:2201.01554v2  [cs.DS]  8 Jul 2022\n\nof modern computer architectures. Those experimental \fndings hold in the stand alone scenario, i.e. when\nno prediction to reduce the search interval is performed, and they provide useful indications on which routine\nto use in which circumstances. However, to what extent the recommendations coming out of those studies\nactually hold also for Learned Indexes has not been investigated. As a matter of fact, which version of Binary\nSearch to use, and when, is unresolved for Learned Indexes, relying on the natural choice mentioned earlier.\nWith the use of the SOSD [17] benchmarking software platform, we address such a question by exper-\nimenting with various Binary Search routines on both synthetic and real datasets, with executions on two\ndi\u000berent architectures, i.e., Intel I7/9 and Apple M1. On the one hand, our results further validate and\nextend the ones provided in [14, 24] for the stand alone scenario and, on the other, they provide novel indi-\ncations on when to use SOSD for Learned Indexing executions and with which Binary Search routine. Our\n\fndings are a signi\fcant advance with respect to the research performed on Learned indexes outlined above.\nFor completeness, we mention that our results hold for the static case of Learned Indexes, i.e., when no\ninsertions or deletions are allowed. For the dynamic case, Learned Indexing solutions exist [12, 9]. However,\nhow to phrase a research analogous to ours in that setting is open, to the best of our knowledge.\nIn order to make our experiments replicable, the software we developed or modi\fed is available at [1, 2],\nwhile the datasets are available at [3].\nQuery Element\nI ndex\nStructure {1\n5\n11\n14\n58\n59\n60\n97\n100\n101\nFigure 1: A General Paradigm of Learned Searching in a Sorted Set [21]. The model is trained on\nthe data in the table. Then, given a query element, it is used to predict the interval in the table where to\nsearch (included in brackets in the \fgure).\n2 A Simple View of Learned Searching in Sorted Sets\nConsider a sorted table Aofnkeys, taken from a universe U. It is well known that Sorted Table Search\ncan be phrased as the Predecessor Search Problem: for a given query element x, return the A[j] such that\nA[j]\u0014x < A [j+ 1]. Kraska et al. [19] have proposed an approach that transforms such a problem into a\nlearning-prediction one. With reference to Figure 1, the model learned from the data is used as a predictor\nof where a query element may be in the table. To \fx ideas, Binary Search is then performed only on the\ninterval returned by the model.\nWe now outline the basic technique that one can use to build a model for A. It relies on Linear Regression,\nwith Mean Square Error Minimization [13]. Consider the mapping of elements in the table to their relative\nposition within the table. Since such a function is reminiscent of the Cumulative Distribution Function over\nthe universe Uof elements from which the ones in the table are drawn, as pointed out by Marcus et al. [21]\nin their benchmarking study on Learned Indexes, we refer to it as CDF. With reference to the example in\nFigure 2, and assuming that one wants a linear model, i.e., F(x) =ax+b, Kraska et al. [19] note that\nthey can \ft a straight line to the CDF and then use it to predict where a point xmay fall in terms of rank\nand accounting also for approximation errors. More in general, in order to perform a query, the model is\nconsulted and an interval in which to search is returned. Then, to \fx ideas, Binary Search on that interval\nis performed. Di\u000berent models may use di\u000berent schemes to determine the required range, as outlined in\nSection 3.2.2. The reader interested in a rigorous presentation of those ideas can consult Marcus et al [21].\nFor this research, it is important to know how much of the table is discarded once the model makes a\nprediction on a query element. For instance, Binary Search, after the \frst test, discards 50% of the table.\n2\n\n(a)\n (b)\n (c)\ne\nFigure 2: The Process of Learning a Simple Model via Linear Regression. LetAbe\n[47;105;140;289;316;358;386;398;819;939]. (a) The CDF of A. In the diagram, the abscissa indicates\nthe value of an element in the table, while the ordinate is its rank. (b) The straight line F(x) =ax+b\nis obtained by determining aandbvia Linear Regression, with Mean Square Error Minimization. (c) The\nmaximum error \u000fone can incur in using Fis also important. In this case, it is \u000f= 3, i.e., accounting for\nrounding, it is the maximum distance between the rank of a point in the table and its rank as predicted by\nF. In this case, the interval to search into, for a given query element x, is given by [ F(x)\u0000\u000f;F(x) +\u000f].\nBecause of the diversity across models to determine the search interval, and in order to place all models on a\npar, we estimate the reduction factor of a model, i.e., the percentage of the table that is no longer considered\nfor searching after a prediction, empirically. That is, with the use of the model and over a batch of queries,\nwe determine the length of the interval to search into for each query. Based on it, it is immediate to compute\nthe reduction factor for that query. Then, we take the average of those reduction factors over the entire set\nof queries as the reduction factor of the model for the given table.\n3 Experimental Methodology\nOur experimental set-up follows closely the one outlined in the already mentioned benchmarking study by\nMarcus et al. [21] regarding Learned Indexes, with some variations. In particular, here we concentrate on\nthe study of how di\u000berent kinds of Binary and k-ary Searches can a\u000bect the performance of Learned Indexes.\nMoreover, following Amato et al [4, 5], we use datasets of varying sizes in order to understand how the data\nstructures perform on the di\u000berent levels of the internal memory hierarchy. In addition to that, we also use\ndatasets generated as in [14], in order to establish that the Binary Search routines we use behave consistently\nwith the \fndings in the mentioned paper. Details of the entire methodology are provided next.\n3.1 Computer Architectures and Compilers\nAll the experiments have been performed on two di\u000berent architectures, i.e, x86 and ARM, using three\ndi\u000berent CPUs: Intel i7-8700, Intel I9-10850, and Apple M1. In the following, the speci\fcations of the three\nsystems used are reported.\n•The i7-8700 works with a 3.2GHz clock and uses 64kb of L1 cache per core, 256kb of L2 cache per\ncore, and 12Mb of shared L3 cache. The amount of system memory is 32 Gbyte of DDR4. The OS is\nUbuntu LTS 20.04.\n•The I9-10850 works with a 3.6 GHz clock and uses 64kb of L1 cache per core, 256kb of L2 cache per\ncore, and 20Mb of shared L3 cache. The amount of system memory is 32 Gbyte of DDR4. The OS is\nUbuntu LTS 20.04.\n•The Apple M1 works with a 3.2Ghz clock and uses two levels of cache (L1 and L2). The amount of\nL1 memory cache depends on the cores, which are of two kinds. Namely, high-performance and high-\ne\u000eciency cores. Moreover, in the Apple M1 architecture, the L1 memory cache is divided into a part\nfor instructions and one for data. In our study, only one core is used, which is of the high-performance\n3\n\nkind. As consequence, the L1 cache size is 192KB for instruction and 128 KB for data. The L2 cache\nis 12 Mb. The amount of system memory is 8 Gbyte DDR4. The OS is macOS Monterey 12.3.1.\nThe adopted compiler is the same for all the operating systems we have used, i.e. GCC compiler with\noptimization \rag -O3. In order to better explain the results obtained, we use a hardware pro\fler on the\nIntel architecture, i.e. Intel Vtune , which makes possible the extraction of several performance parameters\nof interest. They are detailed in Section C.1 of the Appendix. The pro\fler is provided free of charge by Intel.\nSomewhat unfortunately, as far as the M1 architecture is concerned, we are not aware that an analogous\npro\fler is available. More in general, for the ARM architectures such as Neon, there is a pro\fler that may\ngive useful information ( Arm Forge Ultimate ), but it is not free of charge. Pro\flers such as gperf-tool-pro\fler\ndo not give useful information. Based on such a State of the Art, for this research, no pro\fling for the Apple\nM1 architecture is performed. In what follows, we report results on the Intel I7 and Apple M1 architectures,\nwhile, for conciseness, results on the Intel I9 are omitted when analogous to the I7.\n3.2 Algorithms, Code and Software Platforms\n3.2.1 Binary Search and Its Variants\nWe use the standard, i.e., textbook, Binary Search routine [18, 8], reported in Algorithm 1 and referred to as\nS-BS . We also use another version of the Binary Search strategy, presented in [18] (see also [14, 24]) under\nthe name Uniform. The corresponding code is provided in Algorithm 2 and it is referred to as U-BS . We\npoint out that in all the algorithms we present unless otherwise stated, we include prefetching instructions\nsince their use may be of advantage in terms of execution time [14]. However, whether or not to use them\nin our research is a fact that needs evaluation. The U-BS routine di\u000berentiates itself from the Standard\none because there is no test for exit within the main loop. Moreover, depending on the compiler and the\narchitecture, instruction 8 of Algorithm 2 may be translated into a predicated instruction, which is not a\nbranching instruction. Indeed, the main loop of the resulting assembly code reported in Code 1 has no\nbranches. This should be contrasted with the assembly code generated on the I7 for Algorithm 1, which is\nreported in Code 2 of the Appendix. Such a branch-freeness, as discussed in [14], results in better use of\nthe processor instruction pipeline.\nWe include in this study also the lower bound routine from the standard C++ library, which is equiv-\nalent to U-BS in terms of source code (see Algorithm 3 in the Appendix), since, in the SOSD software\nplatform, this routine is referred to as branchy Binary Search. More precisely, it is Uniform and Branchy.\nWe also take into consideration k-ary Search [23] routines, indicated as S-KS andU-KS (Algorithms 4\nand 5 in the Appendix) in their standard and uniform versions respectively, using k= 3 as recommended\nin the work by [24]. The reason is that k-ary Search is superior to U-BS andlower bound on modern\ncomputer architectures, according to results in [24].\nFinally, we include also the Eytzinger Layout Binary Search routine ( U-EL for short, see Algorithm 6\nin the Appendix) because, although not directly usable within the Leaned Indexing framework, it is a useful\nbaseline to compare against: it is superior to U-BS andS-BS [14]. In addition to that, it has not been\nincluded by Schulz et al. [24] in their benchmarking experiments.\n3.2.2 Index Model Classes in SOSD\nFrom the many models available in SOSD , we choose the ones that have been the most successful among\nthe ones benchmarked in [21]. That is, the Recursive Model Index ( RMI , for short) [19], the Radix Spline\n(RS, for short) [15] and the Piecewise Geometric Model ( PGM , for short) [12, 10]. For the convenience of\nthe reader, a brief outline of each of those indexes is provided in Section A.2 of the Appendix. We point\nout that we have modi\fed the SOSD library so that an implementation of a Learned Index can use one of\nU-BS ,S-BS ,U-KS orS-KS , for the \fnal search stage. It is to be noted that each of those models can\nerr in making a prediction. However, each of them has a mechanism to correct for such a mistake in order\nto return a valid interval in which to search into. The interested reader is referred to the original papers for\na description of those mechanisms, which are somewhat more complex than the one we have considered in\n4\n\nAlgorithm 1 C++ Implementation of Standard Binary Search with Prefetching. The version\nwithout prefetching is obtained by deleting lines 4-5.\n1:int StandardBinarySearch(int *A, int x, int left, int right) f\n2: while (left<right)f\n3: int m = (left + right) / 2\n4: builtin prefetch(&(data[lo + m / 2]), 0, 0);\n5: builtin prefetch(&(data[m + m / 2]), 0, 0);\n6: if(x<A[m]) rigth = m;\n7: else if( x>A[m]) left = m+1;\n8: else return m;\n9:g\n10: return right;\n11:g\nAlgorithm 2 C++ Implementation of Uniform Binary Search with Prefetching . The code is as\nin [14] (see also [18, 24]). The version without prefetching is obtained by deleteing lines 6-7.\n1:int prefetchUniformBinarySearch(int *A, int x, int left, int right) f\n2: const int *base = A;\n3: int n = right;\n4: while (n>1)f\n5: const int half = n / 2;\n6: builtin prefetch(base + half/2, 0, 0);\n7: builtin prefetch(base + half + half/2, 0, 0);\n8: base = (base[half] <x) ? &base[half] : base;\n9: n -= half;\n10:g\n11: return (*base <x) + base - A;\n12:g\n5\n\nTable 1: Branchy and Branch-free Assembly Code Production. The \frst row indicates the computer\narchitecture, while the \frst column the routines. For each entry, we report the kind of code produced by the\ncompiler, i.e., Branchy or Branch-free code.\nIntel I7 Intel I9 M1\nS-BS Branchy Branchy Branchy\nU-BS Branch-free Branch-free Branch-free\nlower bound Branchy Branchy Branch-free\nFigure 2. It is also worth recalling that those Models can only use Binary Search procedures with a sorted\ntable layout, i.e., U-EL cannot be used with those Models.\n3.3 Computer Architecture and Compilers: The Production of Branch-Free\nCode\nGiven the Binary Search routines and their variants described in Section 3.2.1, it is not clear that branch-\nfree assembly code is actually produced by the compiler. Therefore, we have inspected the assembly code\ngenerated by the compiler in each of the used architectures. The results are summarized in Table 1. For\nconciseness, we report only the branch-free assembly code in Section A.3 of the Appendix, in addition to\nCode 2 . The remaining code regarding Branchy Binary Search, rather lengthy, is available upon request.\nFurthermore, it is to be noted that the table does not report the case for the k-ary Searches, because\nthe program to extract the assembly code (Linux obj-dump) does not provide speci\fc details capable to\ndetermine the presence of predicated instructions.\nInterestingly, the lower bound routine is translated into branchy code on the Intel architectures and in\nbranch-free code on the Apple M1. Such a di\u000berence may be explained as follows. Although lower bound is\na Uniform routine, its inner loop makes explicit use of if-then-else (lines 7-11 of Algorithm 3 in the Appendix)\nrather than a conditional operator (line 8 in Algorithm 2). This accounts for the di\u000berence between the\ntwo assembly codes on the Intel architectures. As for the Apple M1, being an ARM architecture, it has an\nextensive set of predicated instructions [14], which apparently the compiler is able to use even in the presence\nof simple if-then-else constructs in the high-level code.\nCode 1 Assembly Code of Uniform Binary Search on the Intel I7 (Only Main Loop) . The\npredicated instruction is line 1485 (in bold). No prefetching is used.\n0000000000001460 <Z21Uniform Binary SearchPmmmm >:\n.\n.\n1473: 76 1d jbe 1492 <Z21Uniform Binary SearchPmmmm+0x32 >\n1475: 0f 1f 00 nopl (%rax)\n1478: 48 89 ca mov %rcx,%rdx\n147b: 48 d1 ea shr %rdx\n147e: 4c 8d 04 d0 lea (%rax,%rdx,8),%r8\n1482: 49 3b 30 cmp (%r8),%rsi\n1485: 49 0f 43 c0 cmovae %r8,%rax\n1489: 48 29 d1 sub %rdx,%rcx\n148c: 48 83 f9 01 cmp $0x1,%rcx\n1490: 77 e6 ja 1478 <Z21Uniform Binary SearchPmmmm+0x18 >\n.\n.\n6\n\n3.4 Datasets and Index Model Training\nWe have used two kinds of datasets. The \frst kind was generated as described in [14], i.e. we generated\n24 synthetic datasets with an increasing number nof elements from 24to 228, containing only odd 64-bit\nintegers in [1 ;2n+ 1]. For each of these, we generated a two million query \fle consisting of 1 million-odd\nelement present in the dataset of reference and 1 million even elements.\nThe second kind of datasets have origin from the carefully chosen ones in [21] (and therein referred to\nasamzn32 ,amzn64 ,face,osm,wiki). They have been derived from them in [4, 5], in order to \ft well\neach level of the main memory hierarchy with respect to the Intel I7 architecture. The essential point of the\nderivation is that, for each of the generated datasets, the CDF of the corresponding original dataset is well\napproximated. The details are as follows, where nis the number of elements in a table.\n\u000fFitting in L1 cache: cache size 64Kb. Therefore, we choose n= 3:7K. For each dataset, the table\ncorresponding to this type is denoted with the su\u000ex L1, e.g., amzn32-L1 , when needed.\n\u000fFitting in L2 cache: cache size 256Kb. Therefore, we choose n= 31:5K. For each dataset, the\ntable corresponding to this type is denoted with the su\u000ex L2, when needed.\n\u000fFitting in L3 cache: cache size 8Mb. Therefore, we choose n= 750K. For each dataset, the table\ncorresponding to this type is denoted with the su\u000ex L3, when needed.\n\u000fFitting in PC Main Memory: memory size 32Gb. Therefore, we choose n= 200M, i.e., the\nentire dataset. For each dataset, the table corresponding to this type is denoted with the su\u000ex L4.\nAs for query dataset generation, for each of the tables built as described above, we extract uniformly\nand at random (with replacement) from the Universe Ua total of two million elements, 50% of which are\npresent and 50% absent, in each table.\nFor the case of the Apple M1 architecture, we take into consideration only datasets \ftting at most into\nthe L3 cache of the Intel I7, because our system is equipped with 8 Gbyte of main memory. Indeed, when\nthe code used in this research is executed on the Apple M1 using as input each of the full datasets, the\nperformance degrades due to substantial swapping. As for model training, SOSD has ten prede\fned Models\neach for the PGM andRS. For the RMI Model family, following the Literature, we use CDFShop [21],\nwhich returns up to ten versions of an RMI for a given table. The selection process is heuristic and tries\nto choose good models in terms of query time that use little space. The interested reader can \fnd details in\n[21]\nAll the experiments involving the mentioned datasets are reported in full in the Appendix and in part\nhere. The query time that we report is an average taken on a batch of two million queries executed by\na search routine or a Learned Index. This is essential for Learned Indexes: a measure of a single query\nperformance would be unreliable [16], while the method we choose is compliant with the Literature [21].\nSuch a limitation makes it unreliable to measure some relevant performance parameters of a Learned Index,\nas for instance, for each query, the amount of time spent for prediction and the amount of time spent for\nsearching. In fact, to the best of our knowledge, none of the papers reporting on Learned Indexing provides\nsuch a breakdown. Rather they concentrate on the accuracy of a prediction. For completeness, and in terms\nof theoretic worst-case analysis, the prediction for the RMI s used here takes O(1) time and O(logn) time\nfor the PGM and the RS.\n4 Experiments: Searching in Constant Additional Space, With or\nWithout SOSD\nThe aim of this Section is to shed light on the consistency of our experimental setting with the current\nLiterature, i.e., Khough and Morin [14] and Schultz et al. [24]. However, the results reported here provide also\nuseful indications regarding the use of SOSD with Binary Search routines only. This scenario is meaningful\n7\n\nsince those routines require only constant additional space with respect to the table to be searched into,\nwhile the Learned Indexes may require additional space that depends on the parameters of the model. It\nmay be a small percentage or a really large one. The interested reader can \fnd a study in [4, 5]. The scenario\nwe consider is the one in which one can only use constant additional space with respect to the input table.\nTo the best of our knowledge, it is not clear whether SOSD is worth using and with which routine. As a\nfurther result, we get also indications on how to set up the search routines in SOSD , when Learned Indexing\nis to be used.\n4.1 Replication of The Experiments by Khough and Morin\nAccording to a study by Khough and Morin [14], modern processor architectures are best used with branch-\nfree Binary Search code. In order to assess to what extent those \fndings hold also in our experimental\nset-up, we have experimented with all the routines mentioned in Section 3.2.1, that have been executed as\nstand-alone C++ code ( as in [14]) and as code included into the highly engineered SOSD platform. We\nhave considered all the architectures mentioned in Section 3.1, the synthetic datasets described in Section\n3.4, with the inclusion also of the osm dataset for completeness. No prefetching is used here since the\nadvantage of its use is discussed separately within this Section.\nIn regard to the lower bound routine, which is the standard within SOSD and the associated bench-\nmarking of Learned Indexes, we have compared it with all the other routines, \fnding that it is inferior to\nall of them. The full set of experiments is available upon request and, for brevity, we only report some\ninteresting observations regarding this important routine as compared with S-BS andU-BS . On the Intel\nprocessors, despite being a Uniform Binary Search, its assembly code is branchy (code omitted for brevity\nand available upon request). The experiments reported in Figure 11 of the Appendix point to the fact that,\non those architectures, there is very little di\u000berence with S-BS . On the Apple M1 architecture, its assembly\ncode is branch-free (Code 5 in the Appendix) but, as shown in Figure 12 of that File, both S-BS andU-BS\nare better. For those reasons, we no longer consider lower bound in this study.\nFor the remaining routines, the results are reported in Figure 3 on synthetic datasets and in Figure 14\nof the Appendix for the osm dataset. We also anticipate that, from the experiments reported and discussed\nin this Section, there is no substantial di\u000berence in performance between S-KS andU-KS . For this reason,\nU-KS is not considered in the following Sections.\nStand-alone In this setting, on both the Intel and the ARM architectures, the results in [14] are con\frmed\n(see Figures 3(b) and (d) for synthetic datasets and Figures 14(b) and (d) of the Appendix for the osm\ndatasets). That is, Uniform branch-free is better than Standard branchy for datasets \ftting in the cache\nmemory. It is useful to recall that, for the ARM architecture, we use only datasets that \ft in the cache\nmemory. Moreover, there is also con\frmation of the \fndings in [24], stating that k-ary Search is better than\nboth of those routines for datasets \ftting in main memory. A new \fnding, overlooked both in [14] and in\n[24], is that U-EL is always better than k-ary Search, on both architectures we have considered.\nSOSD We discuss \frst the Intel architectures. In this setting, we \fnd the same results as in the stand-alone\ncase, with some notable di\u000berences: (a) for tables \ftting in the cache memory, e.g., of size in the interval\n[1;222], the gap between the performance of U-BS andS-BS is reduced and virtually unnoticeable for small\ntables, as shown by the two curves in Figure 3(a) and (b); (b) S-KS andU-KS are always better than\nS-BS andU-BS . In regard to the osm datasets, the same results are con\frmed (see Figure 14(a) and (c) of\nthe Appendix). Concerning the Apple M1 architecture, and in reference to Figure 3(c), we \fnd that S-BS ,\nU-BS andU-EL perform analogously, while S-KS andU-KS are always better.\nStand-alone Vs SOSD It is also of interest to assess whether there are di\u000berences in terms of query\nexecution time between the stand-alone and the SOSD case. With reference to Figure 13 of the Appendix\nfor the Intel I7, it is to be noted that the S-BS andU-BS routines perform better in their stand-alone\nversion with datasets \ftting the memory cache. Moreover, in regard to U-EL , the stand-alone version is\n8\n\nalways better than the SOSD counterpart. Finally, the S-KS andU-KS perform always better in the\nSOSD case.\nPro\fler Analysis In order to gain insights into the di\u000berences outlined above, we make use of the Intel\nVtune pro\fler on two synthetic datasets, i.e. a small table (size 210elements) and a large one (size 224\nelements). The results are reported in Tables 3 and 4 in the Appendix. Although the pro\fling does not give\na de\fnite indication regarding the superiority of one setting over the other, there are a few facts that are\nworth noting.\nConcerning the U-EL , all the pro\fler parameters get worse within SOSD , except for the Front-end one.\nIn particular, the Bad Speculation parameter (which indicates \"wrong prediction\" such as in the case of\nbranches), gets much worse in going from stand-alone toSOSD . Interestingly, the same thing happens with\nU-BS . On the other hand, such a parameter decreases for S-BS . Although no de\fnitive conclusion can be\ndrawn, those facts indicate that the optimizations within SOSD seem to be oriented towards branchy code.\n(a)\n2^4 2^6 2^8 2^10 2^12 2^14 2^16 2^18 2^20 2^22 2^24 2^26 2^28\nNumber of Elements in the T able01234567Mean Query Time (s)1e7\nS-BS\nU-BS\nU-EL\nS-KS\nU-KS (b)\n2^4 2^6 2^8 2^10 2^12 2^14 2^16 2^18 2^20 2^22 2^24 2^26 2^28\nNumber of Elements in the T able0.00.20.40.60.81.0Mean Query Time (s)1e6\nS-BS\nU-BS\nU-EL\nS-KS\nU-KS\n(c)\n2^4 2^6 2^8 2^10 2^12 2^14 2^16 2^18 2^20\nNumber of Elements in the T able012345678Mean Query Time (s)1e8\nS-BS\nU-BS\nU-EL\nS-KS\nU-KS (d)\n2^4 2^6 2^8 2^10 2^12 2^14 2^16 2^18 2^20\nNumber of Elements in the T able0.00.51.01.52.02.53.03.54.0Mean Query Time (s)1e7\nS-BS\nU-BS\nU-EL\nS-KS\nU-KS\nFigure 3: Mean Query Times of Search Methods on Synthetic Data. Figure (a) and (c) report\nresults using SOSD on the Intel I7 and the Apple M1, respectively. Figures (b) and (d) reports Stand-Alone\nimplementation for the same architectures. The abscissa reports number of elements in the input Table,\nwhile the ordinate the mean query time in seconds. The vertical lines indicate the size of each cache memory\nlevel.\n4.2 Pros and Cons of Prefetching\nAs pointed out in [14], explicit prefetching can improve performance by loading blocks of data into the cache\nmemory before they are accessed, avoiding processor stalls. However, this operation is expensive. Therefore,\n9\n\nthe advantage of using it must be carefully evaluated. To this end, using the same set-up of Section 4.1, we\nhave studied whether explicit prefetching can improve the performance of the search routines included in\nthis research.\nThe results are summarized in Figure 4 for the Intel I7 architecture and Figure 5 for the Apple M1. We\nreport only results for the execution within SOSD because in the stand-alone case they are analogous.\nThe \fndings of [14] for the use of explicit prefetching are con\frmed on the Intel I7 architectures, i.e. it\nis never useful for S-BS (Figure 4a), it is useful for U-BF only for the on big-sized datasets (Figure 4b),\nand it is always useful for U-EL (Figure 4c). In regard to the Apple M1 architecture, prefetching is not\nuseful across any search method. Possibly, this is due to the fact that we only use datasets \ftting the cache\nmemory.\nAs for the Intel architecture, in order to get insights into those \fndings, we have again pro\fled the code,\nas in the previous Section. The results are reported in Section C.2 of the Appendix. With reference to them,\nit is to be noted that the pro\fler parameters con\frm, for datasets larger than the cache memory, that the\nnumber of stalls due to data cache misses decreases with the use of explicit prefetching.\n(a)\n2^4 2^6 2^8 2^10 2^12 2^14 2^16 2^18 2^20 2^22 2^24 2^26 2^28\nNumber of Elements in the T able123456Mean Query Time (s)1e7\nS-BS with Prefetch\nS-BS (b)\n2^4 2^6 2^8 2^10 2^12 2^14 2^16 2^18 2^20 2^22 2^24 2^26 2^28\nNumber of Elements in the T able1234567Mean Query Time (s)1e7\nU-BS with Prefetch\nU-BS\n(c)\n2^4 2^6 2^8 2^10 2^12 2^14 2^16 2^18 2^20 2^22 2^24 2^26 2^28\nNumber of Elements in the T able0.20.40.60.81.01.21.41.61.8Mean Query Time (s)1e7\nU-EL with Prefetch\nU-EL\nFigure 4: Comparison between routines with and without explicit prefetching on SOSD using\nthe Intel I7 . The Figures show the comparison between routines with ad without explicit prefetching. In\nparticular, we report S-BS in Figure (a), U-BS in Figure (b) and U-EL in Figure (c). On the abscissa,\nwe report the number of elements in the table and, on the ordinates, the mean query time in seconds. The\nvertical lines indicate the size of each cache memory level.\n4.3 A Summary of Useful Indications\nBased on the experiments reported so far, we get some useful indications regarding the use of Binary Search\nroutines that account also for the SOSD platform.\n10\n\nTable 2: Worst-case Average Predicted Search Intervals Length for L4 datasets. For each of the\nlargest of the benchmark datasets and each model class, we have considered the ten model instances used in\nSOSD . For each, we have computed the average predicted interval length and its standard deviation over a\nquery dataset obtained as described in Section 3.4. Then, we have taken the maximum average, which we\nreport in the table for each dataset and model class, together with its standard deviation.\nRMI PGM RS\namzn32 3.44e5\u00061.18e6 4.10e3\u00062.37e3 2.71e2\u00061.58e2\namzn64 6.19e4\u00065.88e5 2.05e3\u00062.37e3 1.35e2\u00061.57e2\nface 8.59e4\u00064.19e5 4.10e3\u00062.37e3 5.31e2\u00063.08e2\nosm 5.26e6\u00062.73e6 2.05e3\u00062.37e3 3.65e2\u00064.23e2\nwiki 4.25e5\u00062.61e6 2.05e3\u00062.37e3 2.50e2\u00062.90e2\n•No Learned Indexing . This scenario indicated the constraint of constant additional space with\nrespect to the table to be searched into. The indication is to use U-EL as a stand alone routine, with\nprefetching on the Intel architectures and without it on the M1 architecture. For this latter, the table\nmust \ft in the cache memory, to avoid swapping.\n•Learned Indexing . With the exclusion of U-EL that cannot be used by the current models, the\nexperiments conducted so far provide the indication that prefetching is not really needed for routines\nthat complete the search of a Learned Index. As far as the Apple M1 architecture is concerned, the fact\nthat prefetching is of no advantage is evident, since we use only dataset \ftting in the cache memory.\nAs for the Intel architectures, the scenario is more complex. Indeed, S-BS never takes advantage of\nexplicit prefetching, while U-BS achieves a substantial improvement with its use for datasets larger\nthan the cache memory, i.e. datasets with a number of elements greater than about 4 :19e6 ( see Figure\n4(b) again). That is, the predicted interval of a model must be of length of at least 4 :19e6, in order\nto consider prefetching in the \fnal search routine of that model. Now, it is worth recalling that, for\nmodels that are e\u000bective, large reduction factors are expected that, in turn, correspond to small tables\nto be searched into. Unless the original table is really large or the model particularly bad, the case of\npredicted interval lengths where prefetching can be useful with U-BS seems unlikely, as the experiment\nreported in Table 2 seems to indicate. As evident from that table, only in the case of the worst RMI on\ntheosm-L4 dataset, there may be some marginal bene\ft in using prefetching with U-BS . Therefore,\nin what follows, explicit prefetching is not used.\n5 Experiments: Searching using Learned Indexes, With or With-\nout SOSD\nIn order to better describe the experimental work presented in this Section, it is useful to recall that SOSD\nhas been designed to provide an environment in which to evaluate the relative merits of existing and possibly\nfuture, Learned Index Models. Given a dataset, how models are trained is brie\ry described in Section 3.4.\nAlthough originally designed for benchmarking, SOSD can also be used to identify among the models it has\navailable for a given dataset, the best performing one. The list of those Models is reported in Table 7 of the\nAppendix. Here we focus on average query time, although space may be of importance also [4, 5]. For a\ngiven dataset, this amounts to up to thirty di\u000berent Models to choose from. The routine used for the \fnal\nsearch stage is, by default, the lower bound routine. Here we consider three search routines for the \fnal\nstage, i.e., U-BS, S-BS andS-KS . As mentioned earlier, the lower bound routine has been excluded since\nit is redundant with respect to the other ones. In summary, for a given dataset, one has up to ninety possible\nModel con\fgurations to choose from. Once such a Model con\fguration has been identi\fed, and in view of\nthe results reported in Section 4.1, it is not clear whether the actual deployment of the selected Model in an\n11\n\n2^4 2^6 2^8 2^10 2^12 2^14 2^16 2^18 2^20\nNumber of Elements in the T able0.00.51.01.52.02.53.03.54.0Mean Query Time (s)1e7\nS-BS with Prefetch\nS-BSFigure 5: Comparison between S-BS with and without explicit prefetching on SOSD using the\nApple M1 . The Figure shows the comparison between S-BS with ad without explicit prefetching. The\nresults for U-BS andU-EL are the same. On the abscissa, we report the number of elements in the table\nand, on the ordinates, the mean query time in seconds. The vertical lines indicate the size of each cache\nmemory level.\nApplication Domain, e.g., Data Bases [22] or Search Engines [14], it should be executed within the SOSD\nplatform or as a stand-alone software program. As a matter of fact, such a point has not been addressed in\nthe Literature. The main goal of this part of our experimental work is to investigate the mentioned aspects,\nwhich also provide useful indications on which search routine to use.\nSelecting a Best Model and Associated Search Routine via SOSD For each dataset described in\nSection 3.4, we consider up to ninety possible Model con\fgurations. Then, for each model class and search\nroutine so obtained, the Model with the best mean query time is chosen, based on its execution within\nSOSD . The results are reported in Figures 6(a) and (c) only for the osm dataset, while the remaining ones\nare reported in Figures 16-19 of the Appendix. From those Figures, we can extract the following \fndings\nregarding the search routines, for which we also provide a justi\fcation.\n•S-KS is the best . It is self-evident from the results reported in the mentioned Figures. Quite\nremarkably, they are consistent across all datasets, memory levels and architectures we have considered.\nThis is a novel \fnding in this area, since only the lower bound or the S-BS routine have been used\nfor the terminal stage of searching in a Learned Index, as well documented in experimental studies\nprior to this one (see [19] and [12]). It is worth noting that the results reported here are coherent with\nthe ones of the previous Section, in which we have evaluated S-KS as a generic Binary Search routine\nrather than as a terminal to a Learned Index. In order to explain such a coherence, we need to point\nout that the models resulted to be the best with S-KS provide a quite small predicted interval, on\naverage: for instance on the osm L4dataset and for the RMI model, the average predicted interval\nlength is 4.41e+2 with a standard deviation of 2.73e+3. The full data regarding this point is not\nshown and is available upon request. For small tables, as far as the Apple M1 architecture is concerned\nand when the search routines are performed within SOSD without a prediction phase, Figure 3(b)\nand Figure 14(b) of the Appendix report a wide margin in average query time between S-KS and the\n12\n\nother routines, which apparently turns out to be preserved also in the Learned Indexing framework\nexecuted within SOSD . As for the Intel architectures, to better highlight the performance of such\nroutines on small tables, we need to \\zoom-in\" in Figure 3(a) and Figure 14(a) of the Appendix. The\ncorresponding Figures are 15(a) and (b) in the Appendix. It is evident that also in this case S-KS is\nalways better than the other routines on small tables. It is worth noting that, compared to the results\non the Apple M1, the margin in average query time between S-KS and the other routines is smaller,\nwhich is re\rected in a smaller margin also in the Learned Indexing framework, executed within SOSD .\nA more re\fned analysis regarding such a coherence behaviour within SOSD would require individually\nmeasure prediction and search time for each query: as already mentioned in Section 3.4, this would\nyield unreliable results.\n•U-BS and S-BS . In contrast to the case of S-KS , the results for S-BS andU-BS indicate that there\nis no clear winner between those two procedures. Indeed, which procedures to pick seems to depend,\nfor both architectures, on the memory level in which the input table \fts, the input table itself and the\nModel we are using. Those latter results do not contradict the ones described in Section 4.1. In fact,\nas already mentioned in the discussion regarding S-KS , the best Models have a quite small average\npredicted interval length and, for small tables, the performance of S-BS andU-BS within SOSD is\nquite close.\n(a)\nRMI PGMRS020406080100120L1\nRMI PGMRS020406080100120L2\nRMI PGMRS020406080100120140160L3\nRMI PGMRS050100150200250L4S-BS\nU-BS\nS-KS (b)\nRMI PGMRS020406080100120L1\nRMI PGMRS020406080100120140L2\nRMI PGMRS020406080100120140160L3S-BS\nU-BS\nS-KS\nFigure 6: SOSD Mean Query Times of Best Learned Indexes on the osm Dataset . Figure (a)\nand (b) report results using SOSD on the Intel I7 and Apple M1, respectively. For each model class, we\nreport the mean query time of the best Learned Indexes adopting in their last stage the routines described\nin Section 3.2.1. In particular, the blue bar is S-BS , the orange bar is U-BS and the green bar is S-KS .\nConvenience of the Execution of the Best Model within SOSD or Stand-alone For each ex-\nperiment described above, we select the best performing Model. As anticipated, we execute each of those\nModels in a stand-alone setting, with the routines S-BS ,U-BS andS-KS as terminal for the search stage.\nThe results are reported in Figure 7. It is evident that the convenience of using SOSD with the selected\nModel rather than in a stand-alone setting is architecture-dependent. In particular, the experiments indicate\nthat it is advisable to use SOSD , with S-KS as a terminal for the search stage, on the Apple M1 architec-\nture. As for the Intel I7 architecture, we \fnd that, for the largest datasets (level L4), it is advisable to use\nSOSD , again with S-KS as a terminal for the search stage. On the other memory levels, the performance is\ndataset-dependent. In particular, SOSD is better than the stand-alone settings for face andwiki datasets\nand worse for the remaining ones. As far as S-BS andU-BS is concerned, in stand-alone setting U-BS\nseems to be the method of choice. This is in agreement with the result reported in Section 4.1 ( Stand-alone\nsetting).\n13\n\nAll of the above provides factual indications regarding the use of SOSD . As for their justi\fcation, a\npro\fler analysis would be required for the Apple M1 architecture. Unfortunately, as already stated, this is\nnot possible at this time with free software. As for the Intel architecture, the fact that SOSD is to be used\non large datasets, may be attributed to the design and use of SOSD : the vast majority of the experiments\nconducted with SOSD are on large datasets. Moreover, as pointed out in Section 4.1(Pro\fler Analysis), it\nseems to favour branchy code.\n(a)\namzn32 amzn64face osm wiki01020304050607080L1\namzn32 amzn64face osm wiki020406080L2\namzn32 amzn64face osm wiki020406080100120L3\namzn32 amzn64face osm wiki050100150200250300350400L4SOSD Best\nSOSD Best in SA + S-BS\nSOSD Best in SA  + U-BS\nSOSD Best in SA  + S-KS (b)\namzn32 amzn64face osm wiki0102030405060L1\namzn32 amzn64face osm wiki0102030405060L2\namzn32 amzn64face osm wiki020406080100L3SOSD Best\nSOSD Best in SA + S-BS\nSOSD Best in SA  + U-BS\nSOSD Best in SA  + S-KS\nFigure 7: Comparison Between the Best Model Indicated by SOSD and its Stand-alone coun-\nterpart . Figures (a) and (b) report results for the Intel I7 and the Apple M1, respectively. Each group\nbar is relative to a dataset, while on the ordinate we report the mean query times in nanoseconds. In each\ngroup, the blue bar indicates the best Model selected by SOSD (also named SOSD Best),while the next\nbars are the same Model when used in stand-alone con\fguration (indicated as SOSD Best in SA) together\nwithS-BS (orange bar), U-BS (green bar) and S-KS (red bar) as terminal stage routines.\n6 Conclusions and Future Directions\nThe main question we have addressed in this research is to provide indications on how the \fndings of Khough\nand Morin [14] and Schulz et al. [24], regarding the choice of which Binary Search routines or variants are\nto be used on modern computer architectures, can also be extended to the novel \feld of Learned Indexing,\nconsidering those routines for the \fnal search stage. So far, for that stage, only the lower bound routine\nhas been considered. A summary of our results that can be useful both to designers and users of Learned\nIndexes is the following.\n•When no additional space with respect to the input table can be a\u000borded, U-EL is the best choice\nboth in SOSD and stand-alone settings. This result con\frms the \fndings by Khough and Morin [14]\nand extends them since we consider also k-ary Search.\n•When Learned Indexing is to be used, for each model class considered in this research, SOSD returns\nthe best models with S-KS as the terminal search stage. This fact holds for both the hardware\narchitectures we have considered.\n•When the choice between SOSD and stand-alone settings needs to be made to allow deployment\nin an Application Domain, such as Databases [22] or Search Engines [14], several factors need to be\nconsidered.\n{On the Apple M1 architecture, SOSD withS-KS as the \fnal search stage is to be preferred.\n{On the Intel I7 architecture, the choice depends on datasets and memory levels. Indeed, for\ndatasets larger than cache memory, SOSD with again as \fnal search stage S-KS is to be preferred,\nwhile, for the ones \ftting the cache memory, such a choice is input data-dependent. In any case,\n14\n\nfor Learned Indexes to be executed in a stand-alone setting, U-BS as the \fnal search stage seems\nto be the more convenient choice.\nAmong the many open problems that the new area of Learned Indexing poses, we mention two that are\nrelevant to the research we have conducted. Although the Eyzinger Layout Binary Search is superior\nto the other routines we have considered, it cannot be used by the Index Models known so far. So, it\nwould be very interesting to devise new models that can use such an excellent layout in the \fnal search\nstage. The second problem is in relation to the extension of our study to Dynamic Learned Indexes,\ni.e., the PGM andAlex [9]. Although an extension to the PGM may be simple (such a Model uses\na Binary Search routine as the \fnal search stage, even in the Dynamic setting), it is not clear how to\nintervene in Alex , given the high level of engineering that has been deployed for the realization of that\nModel.\nReferences\n[1] https://github.com/globosco/A-modi\fed-SOSD-platform-for-benchmarking-Branchy-vs-Branch-free-\nsearch-algorithms.git.\n[2] https://github.com/globosco/Stand-Alone-Platform-for-benchmarking-Branchy-vs-Branch-free-search-\nalgorithms.\n[3] https://anonymous.4open.science/r/DatasetsAndSupplementaryMaterial-E534/README.md.\n[4] D. Amato, G. Lo Bosco, and R. Giancarlo. Learned sorted table search and static indexes in small\nmodel space. CoRR , abs/2107.09480, 2021.\n[5] D. Amato, G. Lo Bosco, and R. Giancarlo. Learned sorted table search and static indexes in small\nmodel space (Extended Abstract). In AIxIA 2021 { Advances in Arti\fcial Intelligence , Cham, 2022.\nAIxIA.\n[6] N. Ao, F. Zhang, D. Wu, D. S. Stones, G. Wang, X. Liu, J. Liu, and S. Lin. E\u000ecient parallel lists\nintersection and index compression algorithms using graphics processing units. Proc. VLDB Endow. ,\n4(8):470{481, May 2011.\n[7] D. Comer. Ubiquitous B-Tree. ACM Computing Surveys (CSUR) , 11(2):121{137, 1979.\n[8] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Introduction to Algorithms, Third Edition .\nThe MIT Press, 3rd edition, 2009.\n[9] J. Ding, U. F. Minhas, J. Yu, C. Wang, J. Do, Y. Li, H. Zhang, B. Chandramouli, J. Gehrke, D. Koss-\nmann, D. Lomet, and T. Kraska. Alex: An updatable adaptive learned index. In Proceedings of the\n2020 ACM SIGMOD International Conference on Management of Data , SIGMOD '20, page 969{984.\nACM, 2020.\n[10] P. Ferragina, F. Lillo, and G. Vinciguerra. On the performance of learned data structures. Theoretical\nComputer Science , 871:107{120, 2021.\n[11] P. Ferragina and G. Vinciguerra. Learned data structures. In Recent Trends in Learning From Data ,\npages 5{41. Springer International Publishing, 2020.\n[12] P. Ferragina and G. Vinciguerra. The PGM-index: a fully-dynamic compressed learned index with\nprovable worst-case bounds. PVLDB , 13(8):1162{1175, 2020.\n[13] D. Freedman. Statistical Models : Theory and Practice . Cambridge University Press, August 2005.\n15\n\n[14] Paul-Virak Khuong and Pat Morin. Array layouts for comparison-based searching. ACM Journal of\nExperimental Algorithmics , 22, may 2017.\n[15] A. Kipf, R. Marcus, A. van Renen, M. Stoian, A. Kemper, T. Kraska, and T. Neumann. Radixspline: A\nsingle-pass learned index. In Proceedings of the Third International Workshop on Exploiting Arti\fcial\nIntelligence Techniques for Data Management , aiDM '20, pages 1{5. ACM, 2020.\n[16] Andreas Kipf. Personal communication, 2021.\n[17] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper, Tim Kraska, and\nThomas Neumann. Sosd: A benchmark for learned indexes. NeurIPS Workshop on Machine Learning\nfor Systems , 2019.\n[18] D. E. Knuth. The Art of Computer Programming, Vol. 3 (Sorting and Searching) , volume 3. Addison-\nWesley Publishing Company, 1973.\n[19] T. Kraska, A. Beutel, E.H. Chi, J. Dean, and N. Polyzotis. The case for learned index structures. In\nProceedings of the 2018 International Conference on Management of Data , pages 489{504. ACM, 2018.\n[20] Marcel Maltry and Jens Dittrich. A critical analysis of recursive model indexes. Proceedings of the\nVLDB Endowment , 15(5):1079{1091, jan 2022.\n[21] R. Marcus, A. Kipf, A. van Renen, M. Stoian, S. Misra, A. Kemper, T. Neumann, and T. Kraska.\nBenchmarking learned indexes. Proc. VLDB Endow. , 14(1):1{13, sep 2020.\n[22] J. Rao and K. A Ross. Cache conscious indexing for decision-support in main memory. In Proceedings\nof the 25th International Conference on Very Large Data Bases , pages 78{89. ACM, 1999.\n[23] B. Schlegel, R. Gemulla, and W. Lehner. K-ary search on modern processors. In Proceedings of the\nFifth International Workshop on Data Management on New Hardware , DaMoN '09, page 52{60, New\nYork, NY, USA, 2009. ACM.\n[24] Lars-Christian Schulz, David Broneske, and Gunter Saake. An eight-dimensional systematic evalu-\nation of optimized search algorithms on modern processors. Proceedings of the VLDB Endoment ,\n11(11):1550{1562, jul 2018.\nA Appendix: Algorithms, Code and Software Platforms\nA.1 Binary Search and Its Variants - Additional Algorithms\nDetails about the routines mentioned in Section 3.2.1 are given here. In particular:\n•Lower bound Function . The routine is given in Algorithm 3.\n•K-ary Search . The routines are given in Algorithms 4-5.\n•Eytzinger Layout . The sorted table is now seen as stored in a virtual complete balanced Binary\nSearch Tree. Such a tree is laid out in Breadth-First Search order in an array. An example is provided\nin Fig. 8. Also, in this case, we adopt a Branch-free version with prefetching of the Binary Search\nprocedure corresponding to this layout. It is reported in Algorithm 6.\n16\n\nAlgorithm 3 lower bound Template .\n1:ForwardIterator lower bound (ForwardIterator \frst, ForwardIterator last, const T& val) f\n2:ForwardIterator it;\n3:iterator traits<ForwardIterator >::di\u000berence type count, step;\n4:count = distance(\frst,last);\n5:while (count >0)f\n6: it = \frst; step=count/2; advance (it,step);\n7: if (*it<val)f\n8: \frst=++it;\n9: count-=step+1;\n10:g\n11: else count=step;\n12:g\n13: return \frst;\n14:g\nAlgorithm 4 Implementation of Standard K-ary Search . The code is as in [24]\n1:int StandardKarySearch(int *arr, int x, int start, int end, int k) f\n2:int left = start, right = end;\n3:while (left<right)\n4:f\n5: int segLeft = left;\n6: int segRight = left + (right - left) / k;\n7: for (int i = 2; i <= k; ++i)\n8:f\n9: if (x<= arr[segRight]) break;\n10: segLeft = segRight + 1;\n11: segRight = left + (i * (right - left)) / k;\n12:g\n13: left = segLeft;\n14: right = segRight;\n15:g\n16: return left;\n17:g\n17\n\nAlgorithm 5 Implementation of Uniform K-ary Search . The code is as in [24]\n1:int UniformKarySearch(int *arr, int x, int start, int end, int k) f\n2:int left = start, right = end;\n3:while (left<right)f\n4: int segLeft = left;\n5: int segRight = left + (1 * (right - left)) / k;\n6: for (int i = 2; i <= k; ++i)\n7:f\n8: int nextSeparatorIndex = left + (i * (right - left)) / k;\n9: segLeft = x >arr[segRight] ? segRight + 1 : segLeft;\n10: segRight = x >arr[segRight] ? nextSeparatorIndex : segRight;\n11:g\n12: left = segLeft;\n13: right = segRight;\n14:g\n15: return left;\n16:g\nAlgorithm 6 Implementation of Uniform Binary Search with Eytzinger Layout and Prefetching .\nThe code is as in [14]. The version without prefetching is obtained by deleteing line 5.\n1:int UniformEytzingerLayout(int *A, int x, int left, int right) f\n2: int i = 0;\n3: int n = right;\n4: while (i<n)f\n5: builtin prefetch(A+(multiplier*i + o\u000bset));\n6: i = (x<= A[i]) ? (2*i + 1) : (2*i + 2);\n7:g\n8: int j = (i+1) >> builtin \u000bs(\u0018(i+1));\n9: return (j == 0) ? n : j-1;\n10:g\n18\n\n5\n79\n1113\n15 316\n1819\n2021\n25\n22 27\n20 16 9 215131925371115 18 22 27Figure 8: An example of Eyzinger layout of a table with 15 elements . See also [14].\nA.2 Index Model Classes in SOSD - Additional Details\nIn this Section details about the Learned Indexes mentioned in Section 3.2.2 are given. In particular:\n•RMI [19]. It is a multi-stage model. When searching for a given key and starting with the \frst stage,\na prediction at each stage identi\fes the model of the next stage to use for the next prediction. This\nprocess continues until a \fnal stage model is reached. This latter is used to predict the interval to\nsearch into. An example is helpful. Figure 9(a) describes a two-stage model. The \frst stage model\n(linear) is used to pick one of the stage 2 models (cubic). Finally, the selected model provides the \fnal\ninterval to search into.\n•RS[15]. It is a two-stage model. It uses user-de\fned approximation parameter \u000f. With reference to\nFigure 9(c), a spline curve approximating the CDF of the data is built. Then, the radix table is used\nto identify spline points to use to re\fne the search interval. Indeed, for the key in the Figure, the most\nsigni\fcant three bits are used to identify two spline points. Then, a Binary Search is performed on the\ntable of the spline points delimited by the points identi\fed earlier. Such a Binary Search identi\fes two\nspline points such that a simple interpolation guarantees a prediction with error \u000f.\n•PGM [12, 10].It is also a multi-stage model, built bottom-up and queried top down. It also uses a user-\nde\fned approximation parameter \u000f, that controls the prediction error at each stage. With reference\nto Figure 9(b), the table is subdivided into three pieces. A prediction in each piece can be done via\na linear model guaranteeing an error of \u000f. A new table is formed by selecting the minimum values in\neach of the three pieces. This new table is possibly again partitioned into pieces, in which a linear\nmodel can make a prediction within the given error. The process is iterated until only one linear model\nsu\u000eces, as in the case in the Figure. A query is processed via a series of predictions, starting at the\nroot of the tree.\n(a)\nLinear\nCubic1 2Cubic Cubicn....... (b)\n{\n{ 1\n5\n11\n14\n58\n59\n60\n97\n100\n101{1\n58\n97\n{{key:1 Model:f\nkey:58 Model:f\nkey:97 Model:fkey:58 Model:fkey:1 Model:f1\n2\n34\n5 (c)\n0123 4 5 6 7\nkey\nIndex\nFigure 9: Examples of (a) RMI (b) PGM (c) RS. See also [21]\n19\n\nA.3 Computer Architecture and Compilers: The Production of Branch-Free\nCode - Additional Code\nIn this Section, as indicated in Section 3.3, the assembly codes of the compiled routines are provided. In\nparticular:\n•The assembly code of Standard Binary Search on the Intel I7 is provided in Code 2.\n•The assembly codes of Uniform Binary Search on the Intel I9 and the Apple M1 are provided in Code\n3 and Code 4, respectively.\n•The assembly code of lower bound function on the Apple M1 is provided in Code 5.\n•The assembly codes of the other branchy methods are here omitted and available to the reader upon\nrequest.\nB Datasets and Index Model Training - Additional Considerations\nIn this Section, we report in Figure 10, the CDF of the datasets on L3 and L4 memory levels, described in\nSection 3.4. It can be noted that the curves of faceL3andfaceL4show a signi\fcant di\u000berence due to the\npresence of some outliers in the L4 memory level which makes the representation not very accurate.\n(a)\n0.0 2.5 5.0 7.5\n1e8011e6 amzn32\n0 2 4 6 8\n1e180500000amzn64\n0 2 4 6 8\n1e100500000face\n0.0 0.5 1.0\n1e190500000osm\n1.0 1.1 1.2\n1e90500000wiki (b)\n0 1 2 3\n1e9021e8 amzn32\n0 2 4 6 8\n1e18021e8 amzn64\n0.0 0.2 0.4 0.6 0.8 1.0\n1e19021e8 face\n0.0 0.5 1.0\n1e19021e8 osm\n1.00 1.05 1.10 1.15 1.20\n1e9021e8 wiki\nFigure 10: Datesets Empirical CDF . For each dataset, we report the value of the elements on the abscissa\nand their position on the ordinate. The curve so obtained is analogous to the Empirical CDF of the Universe\nfrom which the data come. In particular, Figure (a) is referred to the L3 memory level, while (b) to L4.\nC Experiments: Searching in Constant Additional Space, With\nor Without SOSD - Additional Results\nC.1 Replication of The Experiments by Khough and Morin\nWe provide here the details of the experiments mentioned in Section 4.1. In particular:\n•The comparison between the lower bound andS-BS implementations is reported in Figure 11 for\nthe case of the Intel I7 Architecture, and in Figure 12 for the Apple M1.\n•The comparison of each of the considered routines in their SOSD and stand-alone implementations is\nreported in Figure 13\n20\n\nCode 2 Assembly Code of Standard Binary Search on Intel I7 (Only Main Loop) . No prefetching\nis used.\n.0000000000001229 <ZL22Standard Binary SearchPmmii >:\n.\n.\n124d: 7f 6d jg 12bc <ZL22Standard Binary SearchPmmii+0x93 >\n124f: 8b 55 dc mov -0x24(%rbp),%edx\n1252: 8b 45 d8 mov -0x28(%rbp),%eax\n1255: 01 d0 add %edx,%eax\n1257: 89 c2 mov %eax,%edx\n1259: c1 ea 1f shr $0x1f,%edx\n125c: 01 d0 add %edx,%eax\n125e: d1 f8 sar %eax\n1260: 48 98 cltq\n1262: 48 89 45 f8 mov %rax,-0x8(%rbp)\n1266: 48 8b 45 f8 mov -0x8(%rbp),%rax\n126a: 48 8d 14 c5 00 00 00 lea 0x0(,%rax,8),%rdx\n1271: 00\n1272: 48 8b 45 e8 mov -0x18(%rbp),%rax\n1276: 48 01 d0 add %rdx,%rax\n1279: 48 8b 00 mov (%rax),%rax\n127c: 48 39 45 e0 cmp %rax,-0x20(%rbp)\n1280: 73 0c jae 128e <ZL22Standard Binary SearchPmmii+0x65 >\n1282: 48 8b 45 f8 mov -0x8(%rbp),%rax\n1286: 83 e8 01 sub $0x1,%eax\n1289: 89 45 d8 mov %eax,-0x28(%rbp)\n128c: eb b9 jmp 1247 <ZL22Standard Binary SearchPmmii+0x1e >\n128e: 48 8b 45 f8 mov -0x8(%rbp),%rax\n1292: 48 8d 14 c5 00 00 00 lea 0x0(,%rax,8),%rdx\n1299: 00\n129a: 48 8b 45 e8 mov -0x18(%rbp),%rax\n129e: 48 01 d0 add %rdx,%rax\n12a1: 48 8b 00 mov (%rax),%rax\n12a4: 48 39 45 e0 cmp %rax,-0x20(%rbp)\n12a8: 76 0c jbe 12b6 <ZL22Standard Binary SearchPmmii+0x8d >\n12aa: 48 8b 45 f8 mov -0x8(%rbp),%rax\n12ae: 83 c0 01 add $0x1,%eax\n12b1: 89 45 dc mov %eax,-0x24(%rbp)\n12b4: eb 91 jmp 1247 <ZL22Standard Binary SearchPmmii+0x1e >\n12b6: 48 8b 45 f8 mov -0x8(%rbp),%rax\n12ba: eb 05 jmp 12c1 <ZL22Standard Binary SearchPmmii+0x98 >\n12bc: 8b 45 d8 mov -0x28(%rbp),%eax\n.\n.\n21\n\nCode 3 Assembly Code of Uniform Binary Search on Intel I9 (Only Main Loop) . The predicated\ninstruction is line 1475 (in bold)\n0000000000001450 <Z21Uniform Binary SearchPmmmm >:\n.\n.\n1463: 76 1d jbe 1482 <Z21Uniform Binary SearchPmmmm+0x32 >\n1465: 0f 1f 00 nopl (%rax)\n1468: 48 89 ca mov %rcx,%rdx\n146b: 48 d1 ea shr %rdx\n146e: 4c 8d 04 d0 lea (%rax,%rdx,8),%r8\n1472: 49 3b 30 cmp (%r8),%rsi\n1475: 49 0f 43 c0 cmovae %r8,%rax\n1479: 48 29 d1 sub %rdx,%rcx\n147c: 48 83 f9 01 cmp $0x1,%rcx\n1480: 77 e6 ja 1468 <Z21Uniform Binary SearchPmmmm+0x18 >\n.\n.\nCode 4 Assembly Code of Uniform Binary Search on Apple Arm M1 (Only Main Loop) . The\npredicated instruction is line 10000371c (in bold)\n00000001000036f8 <Z21Uniform Binary SearchPyyyy >:\n.\n.\n100003708: 23 01 00 54 b.lo 0x10000372c <Z21Uniform Binary SearchPyyyy+0x34 >\n10000370c: 2a fd 41 d3 lsr x10, x9, #1\n100003710: 0b 0d 0a 8b add x11, x8, x10, lsl #3\n100003714: 6c 01 40 f9 ldr x12, [x11]\n100003718: 9f 01 01 eb cmp x12, x1\n10000371c: 08 81 8b 9a csel x8, x8, x11, hi\n100003720: 29 01 0a cb sub x9, x9, x10\n100003724: 3f 05 00 f1 cmp x9, #1\n100003728: 28 \u000b \u000b 54 b.hi 0x10000370c <Z21Uniform Binary SearchPyyyy+0x14 >\n.\n.\n22\n\nCode 5 Assembly Code of C++ lower bound Function on Apple Arm M1 (Only Main Loop) .\nThe predicated instruction are in lines 10000371c, 100003fa0 (in bold)\n1:0000000100003f68 <Z6searchRNSt3 16vectorIyNS 9allocatorIyEEEEyPmmm >:\n2:.\n3:.\n4:100003f7c: 60 01 00 54 b.eq 0x100003fa8 <Z6searchRNSt3 16vectorIyNS 9allocatorIyEEEEyPmmm+0x40 >\n5:100003f80: 29 fd 43 93 asr x9, x9, #3\n6:100003f84: 2a fd 41 d3 lsr x10, x9, #1\n7:100003f88: 0b 0d 0a 8b add x11, x8, x10, lsl #3\n8:100003f8c: 6c 85 40 f8 ldr x12, [x11], #8\n9:100003f90: ed 03 2a aa mvn x13, x10\n10: 100003f94: 29 01 0d 8b add x9, x9, x13\n11: 100003f98: 9f 01 01 eb cmp x12, x1\n12:100003f9c: 29 31 8a 9a csel x9, x9, x10, lo\n13:100003fa0: 68 31 88 9a csel x8, x11, x8, lo\n14:100003fa4: 09 \u000b \u000b b5 cbnz x9, 0x100003f84 <Z6searchRNSt3 16vectorIyNS 9allocatorIyEEEEyPmmm+0x1c >\n15: .\n16: .\n•The comparison among the remaining routines on the osm dataset is reported in Figure 14.\nPro\fler Analysis In order to look more closely into the results in Section 4.1, we make use of the pro\fler\non the routines to monitor the following parameters.\n•Percentage of Retiring operation . That is the percentage of the pipeline slots used to perform\nuseful work. It is divided into light operations, which are instructions that require at most one micro-\noperation, and heavy, which require two or more of such operations. A high percentage of this parameter\nimplies a very e\u000bective use of the CPU.\n•Percentage of Front-end operation . That is the percentage of pipeline slots where the processor\nis not able to fetch instructions to the Back-end part. This issue can be caused, for example, by stalls\ndue to instruction-cache misses.\n•Percentage of Back-end operation . That is the percentage of pipeline slots where the Back-end\npart is not able to deliver micro-operation for the retiring. This issue can be caused, for example, by\nstalls due to a lack of resources.\n•Percentage of Bad Speculation operation . That is the percentage of pipeline slots where a wrong\npipeline of instructions is loaded, due to a mispredicted branch.\nThe results are summarized in Tables 3 and 4.\nC.2 Pros and Cons of Prefetching\nThis Section presents the details of the pro\fler analysis indicated in Section 4.2. A deeper analysis with the\npro\fler, as just done in the previous Section, is reported in Tables 5-6. Such analysis allows us to see that\nthe utilisation of pipeline slots is relatively identical with or without explicit prefetching on a dataset that\nis small enough to be stored in cache memory, e.g. uniform with 210elements. On the other hand, it can be\nseen that if the dataset resides in central memory, e.g. uniform with 224elements, the percentage of stalls\nin the back-end component of the CPU is higher without the use of explicit prefetching. In particular, as\nshown in Table 6, 70% of these are due to a data cache miss problem that explicit prefetching manages to\navoid.\n23\n\n2^4 2^6 2^8 2^10 2^12 2^14 2^16 2^18 2^20 2^22 2^24 2^26 2^28\nNumber of Elements in the T able02468Mean Query Time (s)1e7\nlower_bound\nS-BSFigure 11: Mean Query times Comparison of the lower bound Function and S-BS on Synthetic\nDatasets . The Figure reports only the comparison of the routines executed within the SOSD platform on\nIntel I7 since the results with Standard C++ implementations are the same. On the abscissa, we report the\nnumber of elements in the table and, on the ordinates, the mean query time in seconds. The vertical lines\nindicate the size of each cache memory level.\nD Experiments: Searching using Learned Indexes, With or With-\nout SOSD - Additional Results\nWe report here the details of the experiments mentioned in Section 5. In particular, we provide the following.\n•We, report the result of Best Learned Indexes within SOSD other than osm in Figures 16-19.\n•The full list of best Models indicated by SOSD in Table 7.\n24\n\n(a)\n2^4 2^6 2^8 2^10 2^12 2^14 2^16 2^18 2^20\nNumber of Elements in the T able0.00.51.01.52.02.53.03.54.0Mean Query Time (s)1e7\nS-BS\nlower_bound (b)\n2^4 2^6 2^8 2^10 2^12 2^14 2^16 2^18 2^20\nNumber of Elements in the T able0.00.51.01.52.02.53.03.54.0Mean Query Time (s)1e7\nU-BS\nlower_bound\nFigure 12: Mean Query times Comparison of lower bound Function with S-BS and U-BS on\nSynthetic Datasets . Figures report only the comparison of the routines executed within the SOSD\nplatform on Apple M1 architecture, since the results with a Standard C++ implementation are the same.\nWe report the comparison between lower bound function and S-BS in Figure (a), while in Figure (b) the\ncomparison between lower bound function and U-BS . On the abscissa, we report the number of elements\nin the table and, on the ordinates, the mean query time in seconds. The vertical lines indicate the size of\neach cache memory level.\nS-BS U-BS U-EL S-KS\nType SOSD NO SOSD SOSD NO SOSD SOSD NO SOSD SOSD NO SOSD\nRetiring 34.2-95.3 18.7-95.7 38.1-100 36.8-100 17.0-100 23.8-100 33.5-88.1 25.6-32.4\nFront-end 22.6 16.5 15.8 8.9 25.2 37.5 21.8 42.3\nBack-end 28.5 21.9 41.1 51.3 29.7 25.5 28.2 15.1\nBad Spec. 14.6 43.0 5.0 3.0 28.2 13.2 16.5 17.1\nTable 3: Pro\fling of S-BS, U-BS, U-EL and S-KS on the Intel I7 . The results refer to a synthetic\ntable of 210elements. We consider S-KS only, since its performance is analogous to U-KS . The \frst row\nreports the methods and the second row, for each method, indicates the execution environment, i.e. with or\nwithout SOSD . The next rows represent hardware operation types. All the values are reported in percentage.\nIn particular, the second value on the Retiring row is the percentage of light operations.\nS-BS U-BS U-EL S-KS\nType SOSD NO SOSD SOSD NO SOSD SOSD NO SOSD SOSD NO SOSD\nRetiring 19.1-75.4 5.5-100 16.4-76.2 6.8-79.4 7.2-62.5 30.8-94.8 9.1-71.4 15-46.2\nFront-end 14.5 4.4 9.1 3 13.3 30.8 13.1 34.4\nBack-end 51.3 61.4 71.9 89.2 51.9 21.0 45.9 32.5\nBad Spec. 15.2 28.7 2.7 1 27.0 17.0 31.9 18.1\nTable 4: Pro\fling of S-BS, U-BS and S-KS on the Intel I7 . The results refer to a synthetic table of\n224elements. The legend is as in Table 3\n25\n\n(a)\n2^4 2^6 2^8 2^10 2^12 2^14 2^16 2^18 2^20 2^22 2^24 2^26 2^28024681e7\n S-BS\nSOSD\nNo SOSD (b)\n2^4 2^6 2^8 2^10 2^12 2^14 2^16 2^18 2^20 2^22 2^24 2^26 2^280.00.20.40.60.81.01e6\n U-BS\nSOSD\nNo SOSD\n(c)\n2^4 2^6 2^8 2^10 2^12 2^14 2^16 2^18 2^20 2^22 2^24 2^26 2^280.000.250.500.751.001.251.501.751e7\n U-EL\nSOSD\nNo SOSD (d)\n2^4 2^6 2^8 2^10 2^12 2^14 2^16 2^18 2^20 2^22 2^24 2^26 2^28012345671e7\n S-KS\nSOSD\nNo SOSD\n(e)\n2^4 2^6 2^8 2^10 2^12 2^14 2^16 2^18 2^20 2^22 2^24 2^26 2^28012345671e7\n U-KS\nSOSD\nNo SOSD\nFigure 13: Mean Query Times Comparison between SOSD and Stand-alone Implementations\nFor each method, i.e. S-BS(a), U-BS(b), U-EL(c), S-KS(d), U-KS(e), we report SOSD (blue curve) and\nstand-alone (orange curve) implementations for the case of Intel I7 architectures. The abscissa reports\nnumber of elements on the Table, while ordinate the mean query time in seconds.\n26\n\n(a)\nL1 L2 L3 L4\nNumber of Elements in the T able1.01.52.02.53.03.54.04.55.0Mean Query Time (s)1e7\nS-BS\nU-BS\nU-EL\nS-KS\nU-KS (b)\nL1 L2 L3 L4\nNumber of Elements in the T able0123456Mean Query Time (s)1e7\nS-BS\nU-BS\nU-EL\nS-KS\nU-KS\n(c)\nL1 L2 L3\nNumber of Elements in the T able45678Mean Query Time (s)1e8\nS-BS\nU-BS\nU-EL\nS-KS\nU-KS (d)\nL1 L2 L3\nNumber of Elements in the T able0.20.40.60.81.0Mean Query Time (s)1e7\nS-BS\nU-BS\nU-EL\nS-KS\nU-KS\nFigure 14: Mean Query times on osm Datasets on Intel I7 (a,b) and Apple M1 (c,d) Architec-\ntures . Figures (a,c) show only the comparison of the routines executed within the SOSD platform, while\nFigures (b,d) with a Standard C++ implementation. On the abscissa, we report the hierarchical memory\nlevel in which datasets \ft and, on the ordinates, the mean query time in seconds.\nTable 5: U-BS with or without Explicit Prefetching on Intel I7 Pipeline Usage Percentage The\n\frst column reports CPU pipeline operation types. For each of them, we report the percentage of slots used.\n210224\nType Prefetch No Prefetch Prefetch No Prefetch\nRetiring 38.4 39.2 24.4 19.7\nFront-end 16.2 16.3 8.2 6.8\nBack-end 40.4 39.4 64.4 71.2\nBad Spec. 5.1 5.1 2.7 2.3\nTable 6: U-BS with or without Explicit Prefetching on Intel I7 Back-end Usage Percentage\nThe \frst column reports the types of Back-end stalls. Memory indicates stalls occurred due to a data cache\nmiss error, while Core indicates stalls due to an overloaded execution unit. For each of them, we report the\npercentage of slots used.\nUniform 210Uniform 224\nType Prefetch No Prefetch Prefetch No Prefetch\nMemory 23.4 22.5 58.4 70.2\nCore 76.6 77.5 41.6 29.8\n27\n\n(a)\n2^4 2^6 2^8 2^10 2^12 2^14\nNumber of Elements in the T able345678Mean Query Time (s)1e8\nS-BS\nU-BS\nU-EL\nS-KS\nU-KS (b)\nL1 L2\nNumber of Elements in the T able1.001.051.101.151.201.251.301.35Mean Query Time (s)1e7\nS-BS\nU-BS\nU-EL\nS-KS\nU-KS\n(c)\n2^4 2^6 2^8 2^10 2^12 2^14\nNumber of Elements in the T able2.02.53.03.54.04.55.05.5Mean Query Time (s)1e8\nS-BS\nU-BS\nU-EL\nS-KS\nU-KS (d)\nL1 L2\nNumber of Elements in the T able45678Mean Query Time (s)1e8\nS-BS\nU-BS\nU-EL\nS-KS\nU-KS\nFigure 15: Zoom-in on Mean Query Time with SOSD . The Figures (a) and (b) reports a zoom-in of\n3(a) and of the Figure 14(a), respectively, while the Figures (c) and (d) reports a zoom-in of 3(c) and of the\nFigure 14(c), respectively. The legend is as in the corresponding Figures.\n(a)\nRMI PGMRS010203040506070L1\nRMI PGMRS010203040506070L2\nRMI PGMRS020406080100L3\nRMI PGMRS050100150200250300350400L4S-BS\nU-BS\nS-KS (b)\nRMI PGMRS0102030405060L1\nRMI PGMRS010203040506070L2\nRMI PGMRS020406080100L3S-BS\nU-BS\nS-KS\nFigure 16: Mean Query Times of Best Learned Indexes on amzn32 dataset . Figure (a) and (b)\nreport results using SOSD on the Intel I7 and Apple Arm M1, respectively. For each model class, we report\nthe mean query time of the best Learned Indexes adopting in their last stage the routines described in Section\n3.2.1. In particular, the blue bar is S-BS , the orange bar is U-BS and the green bar is S-KS .\n28\n\n(a)\nRMI PGMRS0102030405060L1\nRMI PGMRS010203040506070L2\nRMI PGMRS020406080100L3\nRMI PGMRS0255075100125150175200L4S-BS\nU-BS\nS-KS (b)\nRMI PGMRS010203040506070L1\nRMI PGMRS010203040506070L2\nRMI PGMRS020406080L3S-BS\nU-BS\nS-KS\nFigure 17: Mean Query Times of Best Learned Indexes on amzn64 dataset . The legend is as in\nFigure 16.\n(a)\nRMI PGMRS0102030405060L1\nRMI PGMRS010203040506070L2\nRMI PGMRS020406080100L3\nRMI PGMRS0100200300400500600L4S-BS\nU-BS\nS-KS (b)\nRMI PGMRS0102030405060L1\nRMI PGMRS0102030405060L2\nRMI PGMRS020406080100L3S-BS\nU-BS\nS-KS\nFigure 18: Mean Query Times of Best Learned Indexes on face dataset .The legend is as in Figure\n16.\n(a)\nRMI PGMRS010203040506070L1\nRMI PGMRS010203040506070L2\nRMI PGMRS020406080100L3\nRMI PGMRS050100150200L4S-BS\nU-BS\nS-KS (b)\nRMI PGMRS010203040506070L1\nRMI PGMRS010203040506070L2\nRMI PGMRS020406080L3S-BS\nU-BS\nS-KS\nFigure 19: Mean Query Times of Best Learned Indexes on wiki dataset . The legend is as in Figure\n16.\n29\n\nIntel I7\nL1 L2 L3 L4\namzn32 RMI RMI RMI RMI\namzn64 RMI RMI RMI RMI\nface PGM RMI RMI RMI\nosm RMI RMI RMI RMI\nwiki RMI RMI RMI RMI\nIntel I9\nL1 L2 L3 L4\namzn32 RMI RMI RMI RMI\namzn64 RMI RMI RMI RMI\nface RMI RMI RMI RMI\nosm RMI RMI RMI RMI\nwiki RMI RMI RMI RMI\nApple M1\nL1 L2 L3 L4\namzn32 RMI RMI RMI -\namzn64 RMI RMI RMI -\nface RS RS RS -\nosm RMI RMI RMI -\nwiki RMI RMI RMI -\nTable 7: SOSD Best Models . For each dataset and each memory level, the table reports the class of the\nbest performing model indicated by SOSD .\n30",
  "textLength": 71589
}