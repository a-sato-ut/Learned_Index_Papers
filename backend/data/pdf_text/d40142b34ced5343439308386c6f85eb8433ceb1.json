{
  "paperId": "d40142b34ced5343439308386c6f85eb8433ceb1",
  "title": "CARMI: A Cache-Aware Learned Index with a Cost-based Construction Algorithm",
  "pdfPath": "d40142b34ced5343439308386c6f85eb8433ceb1.pdf",
  "text": "CARMI: A Cache-Aware Learned Index with a Cost-based\nConstruction Algorithm\nJiaoyi Zhang\nTsinghua University\nBeijing, China\njy-zhang20@mails.tsinghua.edu.cnYihan Gao\nTsinghua University\nBeijing, China\ngaoyihan@mail.tsinghua.edu.cn\nABSTRACT\nLearned indexes, which use machine learning models to replace\ntraditional index structures, have shown promising results in recent\nstudies. However, existing learned indexes exhibit a performance\ngap between synthetic and real-world datasets, making them far\nfrom practical indexes.\nIn this paper, we identify that ignoring the importance of data\npartitioning during model training is the main reason for this prob-\nlem. Thus, we explicitly apply data partitioning to index construc-\ntion and propose a new efficient and updatable cache-aware RMI\nframework, called CARMI. Specifically, we introduce entropy as a\nmetric to quantify and characterize the effectiveness of data parti-\ntioning of tree nodes in learned indexes and propose a novel cost\nmodel, laying a new theoretical foundation for future research.\nThen, based on our novel cost model, CARMI can automatically de-\ntermine tree structures and model types under various datasets and\nworkloads by a hybrid construction algorithm without any manual\ntuning. Furthermore, since memory accesses limit the performance\nof RMIs, a new cache-aware design is also applied in CARMI, which\nmakes full use of the characteristics of the CPU cache to effectively\nreduce the number of memory accesses. Our experimental study\nshows that CARMI performs better than baselines, achieving an\naverage of 2.2Ã—/1.9Ã—speedup compared to B+ Tree/ALEX, while\nusing only about 0.77Ã—memory space of B+ Tree. On the SOSD plat-\nform, CARMI outperforms all baselines, with an average speedup\nof1.2Ã—over the nearest competitor RMI, which has been carefully\ntuned for each dataset in advance.\nPVLDB Reference Format:\nJiaoyi Zhang and Yihan Gao. CARMI: A Cache-Aware Learned Index with a\nCost-based Construction Algorithm. PVLDB, 14(1): XXX-XXX, 2020.\ndoi:XX.XX/XXX.XX\nPVLDB Artifact Availability:\nThe source code, data, and/or other artifacts have been made available at\nhttp://vldb.org/pvldb/format_vol14.html.\n1 INTRODUCTION\nAs an indispensable access method of database systems, indexes\nprovide fast data accesses by avoiding expensive table scans. Tradi-\ntional index structures are general-purpose, in the sense that they\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license. For any use beyond those covered by this license, obtain permission by\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 14, No. 1 ISSN 2150-8097.\ndoi:XX.XX/XXX.XXorganize data according to fixed rules without taking advantage of\nthe characteristics of underlying data distribution. Recently, Kraska\net al. [ 27] pioneered a line of research where indexes are constructed\nusing machine learning models. Specifically, they proposed a struc-\nture called the Recursive Model Index (RMI). In RMI, index nodes\nthemselves are ML models, and they are connected hierarchically.\nTo perform a lookup, we traverse the tree-like structure using the\nML model in each node to determine the child branch to continue.\nUpon arriving at a position in the data array (i.e., leaf nodes), we\nperform the â€œlast-mileâ€ search within a range to correct the model\nprediction error. [ 27] demonstrates that the RMI and learned in-\ndexes in general exhibit smaller memory consumption and superior\nsearch performance compared to B+ Trees.\nDespite the promising results shown in the latest research [ 16,\n18,19,25,27,38,49], few real database systems choose to adopt\nlearned indexes. An outstanding reason, as indicated in the SOSD\nbenchmark paper [ 24], is that the performance of learned indexes\n(represented by the RMIs) drops significantly when moving from\nsynthetic datasets to real-world datasets. The average latency of\nan index lookup is 2.92 Ã—larger on real-world datasets than that on\nthe same-sized synthetic ones, as shown in [24].\nWe argue that the main reason behind such a performance gap\nlies in the fact that existing RMI designs overlook the importance\nof data partitioning during model training. Many current RMIs,\nincluding the original proposal [ 27], typically emphasize on the\nâ€œmodel-fittingâ€ aspect during index construction. Since the fanout\nand depth of the index are predetermined (and manually tuned),\neach leaf node is assigned a corresponding subset of the data and\ntries to train a model to fit the cumulative distribution function\n(CDF) of the assigned data as accurately as possible.\nThe consequence of such a rigid data partitioning strategy con-\ntributes to the large performance gap between synthetic and real-\nworld datasets for RMIs. For synthetic datasets, the local data distri-\nbution at each leaf node is relatively â€œsmoothâ€ so that the expected\nprediction error is small, leading to a fast last-mile search. Real-\nworld datasets, however, are much â€œbumpierâ€, and the degree of\nâ€œbumpinessâ€ varies across data ranges. Such irregularity in data\ndistribution makes it difficult for simple models (e.g., linear re-\ngression) to achieve accurate predictions under predetermined or\nmanually-tuned data partitions. Larger prediction errors require\nmore memory accesses to correct and, therefore, hurt the index\nperformance. Prior work such as [ 16,19,53] has attempted to au-\ntomate partitioning during index construction. Their approaches,\nhowever, are heuristic-based and only work on a fixed model type,\ni.e., linear model.\nIn this paper, we propose to address the issue by explicitly incor-\nporating data partitioning into the RMI construction process. First,arXiv:2103.00858v4  [cs.DB]  19 May 2022\n\nwe propose a new cost model that considers the data partitioning\naspect for RMI training. The effectiveness of data partitioning is\nquantified using the entropy [ 21] over the number of data points in\neach partition. The intuition is that larger entropy indicates smaller\npartition sizes and a more even size distribution. Smaller partitions\nare preferable because they facilitate leaf-node training and pro-\nduce more accurate models, especially on non-linear datasets. In\naddition, smaller child partitions flatten the hierarchical structure\nof RMI, and thus reduce cache misses. Meanwhile, more even child\npartitions lead to a more balanced tree structure.\nBased on the new cost model, we formalize the index construc-\ntion problem as an optimization problem and solve it using an\nalgorithm combining the greedy and the dynamic programming\napproaches. Unlike CDFShop [ 38], where the node types for each\nlayer are determined ahead of time, our algorithm selects the best\nmodel and the best partition fanout for each node automatically at\nconstruction time without the need for recompilation.\nFinally, since memory accesses dominate the lookup perfor-\nmance for RMIs [36], we design the memory layout for each node\nin a cache-aware manner. Specifically, we require the size for every\nnode with a model to be exactly the cacheline size (i.e., 64 bytes) so\nthat each node visit (or model inference) incurs at most one cache\nmiss. For leaf nodes containing data points across multiple cache-\nlines, we adopt a two-level B+ Tree design, consisting of a 64-byte\nroot and multiple 256-byte data blocks. Such a cache-aware layout\ncan effectively reduce the number of memory accesses during the\nlast-mile search as in existing solutions [ 16,25,27,38], especially on\nreal-world datasets, with only a small space overhead. In addition,\nfixed-sized nodes facilitate memory prefetching where memory\naccesses are parallelized to further reduce the access latency.\nWe present our novel Cache- Aware RMI framework, called\nCARMI, that implements the new ideas introduced above with six\nexample tree node (model) types. Our experimental study shows\nthat CARMI outperforms all baselines on both our microbenchmark\nand SOSD benchmark. In our microbenchmark, CARMI achieves\nan average speedup of 2.2 Ã—(up to 4.2Ã—) and 1.9Ã—(up to 7.2Ã—) com-\npared to B+ Tree and ALEX, respectively, while only using about\n0.77Ã—memory space of B+ Tree. On the SOSD benchmark, CARMI\nachieves an average speedup of 2.5 Ã—(up to 3.0Ã—)/1.5Ã—(up to 2.3Ã—)\ncompared to B+ Tree/ALEX, respectively. Compared to its closest\ncompetitor RMI, which has been carefully tuned for each dataset\nin advance, CARMI is still 1.2 Ã—faster on average (up to 1.5 Ã—).\nWe make the following contributions:\nâ€¢We identify that the inflexibility of data partitioning for learned\nindexes is one of the key reasons why there is a large performance\ngap when applying them to synthetic and real-world datasets.\nâ€¢We propose a new cost model for RMI training, which uses en-\ntropy across partition sizes to measure the effect of data parti-\ntioning on index performance.\nâ€¢We formalize the index construction problem as an optimization\nproblem, and propose an algorithm to solve it efficiently and\nautomatically.\nâ€¢We propose CARMI, a novel RMI framework implementing the\nnew cost model and automatic node selection algorithm. CARMI\nalso uses a new memory layout that is more cache-friendly, espe-\ncially for the last-mile search.â€¢We conduct a series of experiments to demonstrate the superior\nperformance and robustness of our framework.\nThe remainder of this paper is outlined as follows: In Section 2,\nwe review the RMI framework and discuss the two ways of viewing\nRMI: model fitting vs. data partitioning. In Section 3, we derive a\ncost model for the entire index structure, and introduce entropy as a\nmetric for characterizing the node performance. Section 4 discusses\nthe cost-based hybrid index construction algorithm, which is used\nto choose different node settings flexibly to construct the optimal\nindex structure during runtime. In Section 5, we explain the cache-\naware designs of CARMI, including the new memory layout and\na prefetching mechanism. The experimental setup and results are\nshown in Section 6. We discuss the possible extension directions\nand future works in Section 7. Finally, we discuss related work in\nSection 8 and conclude in Section 9. Some implementation details,\nexperimental settings and proof of theorem can be found in the\nappendix.\n2 MOTIVATION AND CARMI\n2.1 RMI and Data Partitioning\nFigure 1 shows the structure of the Recursive Model Index (RMI), an\nML-based index framework. Each inner node in the RMI represents\nan order-preserving regression model: the model ğ‘“:ğ‘˜â†’ğ‘–ğ‘‘ğ‘¥\ntakes a key ğ‘˜as input, and outputs an integer ğ‘–ğ‘‘ğ‘¥âˆˆ{1,2,...,ğ‘},\nwhereğ‘is the number of child nodes of this inner node. The order-\npreserving property guarantees ğ‘˜1â‰¤ğ‘˜2â‡’ğ‘“(ğ‘˜1)â‰¤ğ‘“(ğ‘˜2)so that\nan RMI can answer range queries correctly. At the bottom layer,\nleaf nodes are trained using linear models to fit the underlying data\npoints. Searching the RMI given ğ‘˜proceeds as follows: starting\nfrom the root, we evaluate the model to determine which child\nnode to visit for the next step. This process is repeated until a leaf\nnode is reached. Finally, we perform a binary search (bounded by\nthe maximum error) to retrieve the matching records.\nkey\nroot model\nmodel 2.1\nmodel 2.2\nmodel 2.n\nâ€¦\nmodel 3.2\nmodel 3.1\nmodel 3.n\nmodel 3.1\nâ€¦\narray / gapped array\nInner Nodes\np = /* output of model */\nidx = p * childNumber\nkey\nthe pointer to the corresponding branch\ninner node\nBinary search among them\n (size is childNumber) \nâ€¦\ninput_key\nThe branch to the next level\nï¼ˆ\nkey[i] \nâ‰¥ input_key\nï¼‰\nkey[i-1] < input_key\narray\ngapped array\narray / \ngapped array\nLR inner node\nNN inner node\nBS inner node\nHistogram \ninner node\nLeaf Nodes\nkey\nroot model\nmodel 2.1\nmodel 2.2\nmodel 2.n\nâ€¦\nmodel 3.2\nmodel 3.1\nmodel 3.n\nmodel 3.1\nâ€¦\nInner Nodes\nLeaf Nodes\nleaf node\nleaf node\nleaf node\nleaf node\nâ€¦\np\n = /* output of \nmodel */\nnext branch\ninner node\nidx\n = p * \nchildNumber\nkey\nFigure 1: Learned Index\nExisting RMI designs [ 16,18,19,25,27,38,49] view the index\nconstruction problem from the perspective of model fitting. In this\nview, models are trained to minimize a loss function (e.g., squared\nloss function) to best fit the CDF of a given dataset. Specifically,\nthe root model targets at fitting the CDF of the entire dataset,\nwhile the leaf-node models try to fit their local distributions. Under\nsuch a problem setting, however, the index fanout and depth are\ntypically predetermined before model training, and thus wasting\nopportunities to improve model accuracies through a more flexible\ndata partitioning.\nApproaching RMI construction via a â€œmodel-fittingâ€ view can\nlead to a significant performance drop when shifting from synthetic\ndatasets to real-world datasets. Because real-world datasets are\noften non-smooth and non-linear, as shown in Section 6.3, simple\n2\n\nmodels such as linear regression are not able to fit well in certain\nCDF ranges if the partitioning is too coarse-grained. Inaccurate\npredictions from the models then require additional procedures\nsuch as large range scans to finish the â€œlast-mileâ€ search. These\nlarge-range searches span many cachelines and require multiple\nmemory accesses, resulting in performance degradation.\nIn this paper, we propose to look at RMI construction from a\nâ€œdata-partitioningâ€ view, where the models aim to partition a given\ndataset more evenly into smaller chunks to form a flatter and more\nbalanced tree. Moreover, larger fanouts (i.e., more child partitions)\nof the upper-level models are preferable in terms of the overall\nprediction accuracy because lower-level nodes now can work on\nsmaller local datasets to reduce their maximum model prediction\nerrors. The trade-off for more partitions, however, is the space over-\nhead due to more nodes and the performance hit due to potentially\nmore complex models. Therefore, we include the effectiveness of\ndata partitioning in our new cost model along with search time\nand space, and develop an algorithm to train an RMI automatically\nusing an objective function derived from the cost model.\nTo quantify the effectiveness of data partitioning, we propose\nto use entropy, an information-theoretic metric [ 21]. Suppose an\ninner node ğ‘€distributes a total of ğ‘›data points into ğ‘differ-\nent child nodes1, then the entropy ğ»(ğ‘€)is defined as: ğ»(ğ‘€)=\nâˆ’Ãğ‘\nğ‘–=1ğ‘ğ‘–log2ğ‘ğ‘–, whereğ‘ğ‘–=ğ‘›ğ‘–\nğ‘›andğ‘›ğ‘–is the number of data points\nallocated to the ğ‘–-th child node. As mentioned in the introduction,\nlarger entropies mean that datasets are divided more evenly into\nsmaller subsets, which is more desirable as discussed above. Further,\nthe entropy also helps establish the notion of local node efficiency,\nas described in Section 3.3.1, which combines the time and space\ncost of a single node and its dataset partitioning utility into a single\nmetric. With this metric, we can effectively compare models locally\nwithout global information, thus speeding up index construction.\n2.2 Overview of CARMI\nIn this paper, we extend the RMI framework and propose a re-\nfined general RMI framework that can automatically build suitable\nand updatable indexes for given datasets, called Cache-Aware RMI\n(CARMI). CARMI retains RMIâ€™s core idea of replacing traditional\nindexes with ML models and has similar procedures for querying\ndata points, but differs in specific designs. Specifically, in our new\nmemory layout (Â§ 5), all tree nodes are limited to 64 bytes and\nstored in a single array. Then, according to the independent char-\nacterization of each tree node (Â§ 3), CARMI can use different nodes\nto handle different sub-datasets and link them into an index tree.\nNodes in CARMI are explicitly classified into two categories:\ninner nodes and leaf nodes. Inner nodes are intermediate bridges\nbetween root and leaf nodes, using models to determine the next\nbranch. Leaf nodes are similar in concept to leaf nodes of B+ Tree,\nbut they only store data points conceptually by storing pointers to\ndata blocks containing actual data points.\nFor a given dataset, CARMI uses a hybrid construction algorithm\n(Â§ 4) to solve the index construction problem, whose optimization\nobjective is to minimize the weighted sum of time and space costs\n1Note that the number of child nodes for an inner node is part of the model configura-\ntions, and we need to determine its optimal value when constructing the index.(Â§ 3). With this algorithm, CARMI can automatically construct\nindexes with good performance at runtime.\nRoot(linear regression model)M0 (LR model)[0, 199]Data: ğ·={0,1,2,â€¦,199,200,202,204,â€¦,798}Find key: 592 M1[200, 398]L14[200, 230]â€¦M2[400, 598]L27[592, 598]M3[600, 798]L28[600, 630]L34[792, 798]â€¦Leaf nodes M1â€“M5Data blocks  L0â€“L34  (actually store data points)Inner nodes M0M4[0, 99]L0[0, 15]L6[96, 99]M5[100, 199]L7[100, 115]L13[196, 199]â€¦â€¦\nFigure 2: A Simple Example of CARMI\nThen, we illustrate the design of CARMI via a concrete example:\nExample 1. Consider a dataset in which there are 500 data points\nğ·={0,1,Â·Â·Â·199,200,202,Â·Â·Â·,798}. Then CARMI can be a four-\nlayer structure, as shown in Figure 2. The top layer is the root node\nwith a linear regression (LR) model ( ğ‘–ğ‘‘ğ‘¥=âŒŠ0.005Ã—ğ‘˜ğ‘’ğ‘¦âŒ‹), which\ndetermines the index of the next node for each given key. The first\n200 data points are managed by an LR inner node and its two child\nleaf nodes. The remaining 300 data points are directly managed by 3\nleaf nodes (ğ‘€1âˆ’ğ‘€3). Each leaf node is linked to 7 data blocks at the\nbottom and uses them to store data points.\nSuppose that we need to access the record with key value 592. We\nfirst access the root node and use its model to calculate the index\nof the next node ( ğ‘–ğ‘‘ğ‘¥=âŒŠ0.005Ã—592âŒ‹=2). After obtaining the\ncontent ofğ‘€2, we use its strategy to get the index of the data block\n(ğ¿27), and finally, search for the key value 592withinğ¿27. Since the\nroot node is frequently accessed, we assume that it is always in the\ncache memory. Therefore, for this example, we only need two random\nmemory accesses: the leaf node ğ‘€2and the data block ğ¿27, respectively.\n3 COST MODEL OF CARMI\nIn this section, we describe the new cost model of CARMI and its\napplication in the index construction problem in detail. Specifically,\nto quantify the performance of tree nodes in a standalone fashion\nand lay a solid foundation for index construction, we first charac-\nterize each tree node from three dimensions: time cost, space cost,\nand entropy, and use them as building blocks to derive a cost model\nfor the entire index structure. Since the analysis is performed from\na generic perspective, the CARMI framework can accommodate\nany new types of tree nodes as long as they can be represented ac-\ncordingly. Then, we use the cost model as an optimization objective\nto find the most suitable node design and tree structure at runtime.\nSome example tree node designs are briefly described in this paper,\ntogether with their performance characterization. These designs\nare all included in our open-source implementation of CARMI [ 1],\nand used in our experimental study in Section 6.\nThe rest of the section is organized as follows. We analyze the\ninner/leaf nodes from three perspectives in Section 3.1 and 3.2,\nrespectively. The cost model of the entire index and the formal\nformulation of the index construction problem is described in Sec-\ntion 3.3. Finally, in Section 3.4, we briefly talk about a few specific\ntree node designs that we have implemented.\n3.1 Inner Nodes\nThe main functionality of inner nodes is to determine which branch\nto go through, so that we can quickly map a given key to its cor-\nresponding leaf node. In the following, we discuss three separate\n3\n\nRoot (M1)M2M3M4M5Miâ€¦Mi+1Mi+2â€¦MjMKâ€¦Leaf nodesInner nodesTCost(Mi) : ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡ğ¶ğ‘ƒğ‘ˆ+ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡ğ‘ğ‘ğ‘ğ‘’ğ‘ ğ‘ SCost(Mi):  64ğµğ‘¦ğ‘¡ğ‘’ğ‘ H(Mi):    âˆ’âˆ‘ğ‘klogğ‘ğ‘˜TCostread(Mj):ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡!\"\"#$$+ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡%&'(+ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡)*+\",(TCostwrite(Mj): ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡!\"\"#$$+ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡%&'-+ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡)*+\",-SCost(Mj): 64ğµğ‘¦ğ‘¡ğ‘’ğ‘ + ğ‘†ğ‘ğ‘ğ‘ğ‘’ğ‘‘ğ‘ğ‘¡ğ‘ğ”¼ğ‘ğœ–ğ‘„[TCost(T, q)]=âˆ‘.!âˆˆ0ğ‘ƒğ‘€1ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡(ğ‘€1)SCost(T)=âˆ‘.!âˆˆ0ğ‘†ğ¶ğ‘œğ‘ ğ‘¡(ğ‘€1)P(Mi):#+341$15$5+6+7#.!#+3!**89#:1#$Root nodeFigure 3: Cost Model of CARMI\ndimensions for characterizing inner nodes: the time required for\npredicting the next branch, the space cost of the node, and the\ndegree of uniformity of data points partitioning.\n3.1.1 Time. The time cost of an inner node includes two parts: the\naccess time and the computation time, denoted as ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡ğ‘ğ‘ğ‘ğ‘’ğ‘ ğ‘  and\nğ‘‡ğ¶ğ‘œğ‘ ğ‘¡ğ¶ğ‘ƒğ‘ˆ, respectively. ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡ğ‘ğ‘ğ‘ğ‘’ğ‘ ğ‘  refers to the time to read the\nnode content, which is equal to the latency of the main memory\ndue to our cache-aware design in Section 5. ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡ğ¶ğ‘ƒğ‘ˆ is the time\nrequired for a model to compute the index of the next node, which\nonly depends on the model type. Then, the total time required\nfor an inner node ğ‘€to predict the next branch is: ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡(ğ‘€)=\nğ‘‡ğ¶ğ‘œğ‘ ğ‘¡ğ¶ğ‘ƒğ‘ˆ+ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡ğ‘ğ‘ğ‘ğ‘’ğ‘ ğ‘  .\n3.1.2 Space. The space cost of an inner node ğ‘€is the total amount\nof space in bytes, which is denoted as ğ‘†ğ¶ğ‘œğ‘ ğ‘¡(ğ‘€).\n3.1.3 Entropy. We use the entropy metric to characterize the ability\nof an inner node to partition a dataset evenly. The entropy of an\ninner nodeğ‘€is:ğ»(ğ‘€)=âˆ’Ãğ‘\nğ‘–=1ğ‘ğ‘–log2ğ‘ğ‘–(details in Section 2.1).\n3.1.4 Root Node. In CARMI, the root node is handled differently\ncompared to inner nodes: since the root node is always accessed\nduring lookup procedures, we can assume that it is always available\nin the cache memory. Because of this, ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡ğ‘ğ‘ğ‘ğ‘’ğ‘ ğ‘  of the root node\nis equal to the latency of cache memory.\n3.2 Leaf Nodes\nLeaf nodes are used to manage the actual data points and can also be\ncharacterized in terms of time and space cost. The third dimension\nis the capacity for storing data points (i.e., how many data points\ncan be stored in the leaf node).\nIn CARMI, we design a new type of leaf node similar to a two-\nlevel B+ tree node. Its root is 64 bytes and contains pointers to\nmultiple 256-byte data blocks that store data points. Then, three\ndimensions of leaf nodes are described as follows.\n3.2.1 Time. For leaf nodes, the time cost of two specific operations\nis analyzed: insert a new data point and lookup a data point2.\nSimilar to inner nodes, the time to access the node ( ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡ğ‘ğ‘ğ‘ğ‘’ğ‘ ğ‘  )\nis equal to the main memory access latency. Due to the new leaf\nnode, finding a data point requires first finding the index of the\ndata block, and then locating the data point within the block. Their\ntime costs are denoted as ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡ğ‘…\nğ¶ğ‘ƒğ‘ˆandğ‘‡ğ¶ğ‘œğ‘ ğ‘¡ğ‘…\nğµğ‘™ğ‘œğ‘ğ‘˜, respectively.\n2Deletion and update operations are not discussed since they are similar to the read\naccess operations (we adopt a lazy deletion approach).As for the insert operation, the content of both leaf node and\ndata blocks might need to be changed accordingly. To reflect this,\nthe time cost is denoted using a different superscript ( ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡ğ‘Š\nğ¶ğ‘ƒğ‘ˆ\nandğ‘‡ğ¶ğ‘œğ‘ ğ‘¡ğ‘Š\nğµğ‘™ğ‘œğ‘ğ‘˜).\nOverall, the time cost of a leaf node is modeled as:\nğ‘‡ğ¶ğ‘œğ‘ ğ‘¡ğ‘Ÿğ‘’ğ‘ğ‘‘(ğ‘€)=ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡ğ‘ğ‘ğ‘ğ‘’ğ‘ ğ‘ +ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡ğ‘…\nğ¶ğ‘ƒğ‘ˆ+ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡ğ‘…\nğµğ‘™ğ‘œğ‘ğ‘˜\nğ‘‡ğ¶ğ‘œğ‘ ğ‘¡ğ‘¤ğ‘Ÿğ‘–ğ‘¡ğ‘’(ğ‘€)=ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡ğ‘ğ‘ğ‘ğ‘’ğ‘ ğ‘ +ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡ğ‘Š\nğ¶ğ‘ƒğ‘ˆ+ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡ğ‘Š\nğµğ‘™ğ‘œğ‘ğ‘˜(1)\n3.2.2 Space. The space cost of a leaf node ğ‘€, consists of the bytes\nof metadata and data blocks. Then the total space cost is:\nğ‘†ğ¶ğ‘œğ‘ ğ‘¡(ğ‘€)=ğ‘†ğ¶ğ‘œğ‘ ğ‘¡ğ‘™ğ‘’ğ‘ğ‘“+ğ‘†ğ‘ğ‘ğ‘ğ‘’(ğ‘‘ğ‘ğ‘¡ğ‘) (2)\nwhereğ‘†ğ¶ğ‘œğ‘ ğ‘¡ğ‘™ğ‘’ğ‘ğ‘“ is 64 bytes, and ğ‘†ğ‘ğ‘ğ‘ğ‘’(ğ‘‘ğ‘ğ‘¡ğ‘)is the total amount\nof space occupied by the data blocks.\n3.2.3 Capacity of Leaf Nodes. The capacity of leaf nodes refers to\nthe total number of data points stored in a leaf node, and it depends\non both the total amount of space allocated for data points as well\nas the way these data points are arranged. For example, if we need\nto make room for subsequently inserted data points to reduce the\nlatency of the insert operation, then the capacity of the leaf node\nwill be reduced accordingly.\n3.3 The Optimization Problem\nBased on the above analysis, we can define a cost model for the\nentire index structure. The time cost of queries can be estimated\nby utilizing the above analysis results: For any query ğ‘and index\nstructureğ‘‡, let the traversal path of ğ‘inğ‘‡beğ‘€1(ğ‘Ÿğ‘œğ‘œğ‘¡)â†’ğ‘€2â†’\n...â†’ğ‘€ğ‘˜(ğ‘™ğ‘’ğ‘ğ‘“), then the time cost of ğ‘can be approximated as:\nğ‘‡ğ¶ğ‘œğ‘ ğ‘¡(ğ‘‡,ğ‘)=ğ‘˜âˆ‘ï¸\nğ‘–=1ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡(ğ‘€ğ‘–) (3)\nThe space cost of the index structure is simply the sum of the\nspace cost of all inner nodes and leaf nodes:\nğ‘†ğ¶ğ‘œğ‘ ğ‘¡(ğ‘‡)=âˆ‘ï¸\nğ‘€ğ‘–âˆˆğ‘‡ğ‘†ğ¶ğ‘œğ‘ ğ‘¡(ğ‘€ğ‘–) (4)\nNow we can formalize the problem of index construction as an\noptimization problem: We would like to find the optimal index\nstructure that minimizes the average time cost of each data lookup\noperation, under a certain space cost budget. Here the average time\ncost is evaluated with respect to a fixed query workload known\nin advance. The query workload can be constructed using recent\nhistory queries from users. In most cases, recent history queries\n4\n\nwill faithfully reflect the characteristics of queries we expect to see\nin the future. If there are no history queries available, then we can\nuse a uniform access workload instead, in which each data point is\naccessed exactly once.\nThe problem of finding an optimal tree structure is formulated\nas follows:\nProblem 1. Letğ‘„={ğ‘1,...,ğ‘ğ‘š}be a collection of queries, and\nğ·={ğ‘‘1,...,ğ‘‘ğ‘›}be the collection of keys to be maintained in the\nindex structure. Find the optimal index tree structure ğ‘‡such that\nEğ‘âˆˆğ‘„[ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡(ğ‘‡,ğ‘)]is minimized, under the constraint that the total\nspace cost of ğ‘‡does not exceed a fixed budget ğµ:ğ‘†ğ¶ğ‘œğ‘ ğ‘¡(ğ‘‡)â‰¤ğµ.\nProblem 1 is a constrained optimization problem, and a widely\nused technique for solving such problems is to use the Lagrange\nmultiplier method [ 10] to transform it into an unconstrained prob-\nlem. By utilizing this technique, Problem 1 can be transformed into\na roughly equivalent form with a linear combination of time and\nspace cost as objective, which is relatively easier to optimize:\nProblem 2. Letğ‘„={ğ‘1,...,ğ‘ğ‘š}be a collection of queries, ğ·=\n{ğ‘‘1,...,ğ‘‘ğ‘›}be the collection of keys to be maintained in the index\nstructure, and ğœ†be a positive constant parameter. Find the optimal\nindex tree structure ğ‘‡such that Eğ‘âˆˆğ‘„[ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡(ğ‘‡,ğ‘)]+ğœ†ğ‘†ğ¶ğ‘œğ‘ ğ‘¡(ğ‘‡)is\nminimized.\nProblem 2 suggests that we ultimately want to minimize a weighted\nsum of time and space cost of the index, and we will describe an\nalgorithm for solving it in Section 4.\nLetğ‘ƒ(ğ‘€ğ‘–)be the fraction of history queries passing through\na tree node ğ‘€ğ‘–. Then the expression Eğ‘âˆˆğ‘„[ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡(ğ‘‡,ğ‘)]can be\nrearranged into an alternative form, which is sometimes more con-\nvenient:\nEğ‘âˆˆğ‘„[ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡(ğ‘‡,ğ‘)]=âˆ‘ï¸\nğ‘€ğ‘–âˆˆğ‘‡ğ‘ƒ(ğ‘€ğ‘–)ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡(ğ‘€ğ‘–) (5)\nFinally, a summarization of CARMI cost model can be found in\nFigure 3 for fast reference.\n3.3.1 Theoretical Analysis. In the following analysis, we assume\nthe history queries to be a uniform access of the data points for\nsimplicity, in such a case the value of ğ‘ƒ(ğ‘€ğ‘–)is the same as the\ntotal fraction of data points in the subtree of ğ‘€ğ‘–. With this as-\nsumption, the following theorem establishes a connection betweenÃ\nğ‘–ğ‘ƒ(ğ‘€ğ‘–)ğ»(ğ‘€ğ‘–)and the total number of data points ğ‘›.\nTheorem 3.1. Letğ‘‡={ğ‘€1,...,ğ‘€ğ¾}be an index structure with\nğ¾nodes in total. ğ‘ƒ(ğ‘€ğ‘–)represents the ratio of data points in node ğ‘€ğ‘–\nrelative to the total number ğ‘›. Then we have:Ãğ¾\nğ‘–=1ğ‘ƒ(ğ‘€ğ‘–)ğ»(ğ‘€ğ‘–)=\nlog2ğ‘›, where for leaf nodes, ğ»(ğ‘€ğ‘–)is defined as log2(ğ¶ğ‘ğ‘ğ‘ğ‘ğ‘–ğ‘¡ğ‘¦(ğ‘€ğ‘–))\nandğ¶ğ‘ğ‘ğ‘ğ‘ğ‘–ğ‘¡ğ‘¦(ğ‘€ğ‘–)is the capacity of leaf nodes (Â§ 3.2.3).\nThe proof of this theorem can be found in the appendix. Essen-\ntially, Theorem 3.1 states that the weighted sum of the entropy of\nall tree nodes is always a constant value. In other words, entropy\ncharacterizes the â€œcontribution\" of each tree node to the index struc-\nture: the higher entropy each tree node contributes, the less overall\nnumber of tree nodes we need in the index structure.\nIt is also of interest to compare Theorem 3.1 with the optimiza-\ntion objective of Problem 2. We can rewrite the objective usingEquation 5:\nObjective =ğ¾âˆ‘ï¸\nğ‘–=1[ğ‘ƒ(ğ‘€ğ‘–)ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡(ğ‘€ğ‘–)+ğœ†ğ‘†ğ¶ğ‘œğ‘ ğ‘¡(ğ‘€ğ‘–)]\nComparing with Theorem 3.1, we see that intuitively each tree\nnodeğ‘€contributesğ‘ƒ(ğ‘€)ğ»(ğ‘€)entropy-wise to the index structure\nwhile incurring ğ‘ƒ(ğ‘€)ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡(ğ‘€)+ğœ†ğ‘†ğ¶ğ‘œğ‘ ğ‘¡(ğ‘€)cost to the overall\nobjective. Thus the ratio of these two terms can be used to quantify\nthe local efficiency of node ğ‘€:\ncost-ratio(M) =ğ‘ƒ(ğ‘€)ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡(ğ‘€)+ğœ†(ğ‘†ğ¶ğ‘œğ‘ ğ‘¡(ğ‘€))\nğ‘ƒ(ğ‘€)ğ»(ğ‘€)(6)\nGenerally, we want to minimize the cost-ratio of all tree nodes,\nespecially the ones with a large value of ğ‘ƒ(ğ‘€)ğ»(ğ‘€). For example,\nif all tree nodes have a cost-ratio less than ğ‘, then the optimization\nobjective would be bounded by ğ‘log2ğ‘›.\nTable 1: The Mechanism of Various Types of Nodes\nNode Mechanism\nLR Use a LR model to determine the corresponding branch\nP. LRSimilar to LR nodes, but use a piecewise linear regression\nmodel instead\nHist Determine the branch through a histogram lookup table\nBSUse binary search to determine the branch, similar to B+\nTree node\nCF Array A two-layer cache-friendly structure similar to a B+ Tree\nExt. ArrayOnly store meta data, and data points are stored in an\nexternal location\n3.4 Specific Implementation\nWe have implemented four types of inner nodes and two types\nof leaf nodes in CARMI. For inner nodes, they use either linear\nregression, piecewise linear regression, binary search or histogram\nmodels to predict the next branch. For leaf nodes, we implemented\ntwo different structures: cache-friendly array (CF array) and exter-\nnal array. CF array leaf nodes store data points compactly in the\ndata blocks in a sequential manner, and leaf node itself stores the\nminimum key values of data blocks. External array leaf nodes are\nused for primary index structures, where the original data points\nare already sorted according to the key value and stored in an ex-\nternal location. In such a case, we only need to store pointers to\nexternal locations in the leaf node.\nNote that the choice of inner/leaf nodes can be flexibly deter-\nmined at runtime and do not need to agree on a single one through-\nout the tree structure. Table 1 summarizes the mechanism and\ncharacteristics of each tree node type. The specific implementation\ndetails can be found in the appendix.\n3.4.1 Empirical Performance. In order to evaluate the empirical\nperformance of tree nodes, we have implemented a profiler program\nto evaluate the CPU computation time required for each node to\nobtain the position of the next branch and the time to obtain their\ncontents from memory. Table 2 shows the empirical performance\n5\n\nFigure 4: Entropy and cost-ratio, where each case is a node setting consisting of node type and number of children.\nof these node types based on our evaluation on an Ubuntu platform\nequipped with an AMD Ryzen 3700X 8-Core Processor and 32GB\nRAM.\nTable 2: The Empirical Values of Nodes in CARMI\nType NodeSpace Time (ns)\n(Bytes) Access CPU\nRootLR 20 8.29 3.25\nP. LR 76 11.24 18.38\nInnerLR\n64 80.095.2\nP. LR 22.8\nHist 14.1\nBS 27.3\nLeafCF (leaf node) 64 80.09 25.4\nCF (data block) 256 90.09 53.8\nExt. Array 64 80.09 117.1 âˆ’328.3\nWe also evaluate the entropy and the cost-ratio of inner nodes\non three synthetic datasets and one real-world dataset in Figure 4.\nDetailed descriptions of the datasets can be found in Section 6.1. We\nprovide the results of two different cases: (a) on two full synthetic\ndatasets: uniform and lognormal datasets, (b) on random consecu-\ntiveğ‘›data points in normal and OSMC datasets to show the results\nof local distributions. The number of key values in each case is\n256, 1024, 4096, 16384, and the number of child nodes is 8 and 256,\nrespectively.\n1) Analysis of Root Nodes. Since the root node is frequently\naccessed and always in the cache memory, we do not need to limit\nthe size of the root node, and can choose the node with a larger\nentropy to improve the overall performance. The LR node only\nneeds a small amount of time (3.25 ns) to calculate the position of\nthe next branch, while the P. LR node needs 18.38 ns.\n2) Analysis of Inner Nodes. Due to the cache-aware design, the\nspace cost of inner nodes is the same, but the time cost is different.\nThe LR node enjoys the minimum CPU time cost of 5.2 ns, while\nother nodes need more time to get the position of the next branch.\nNote that even for the most costly BS nodes (27.3 ns), the CPU\ncomputation time is still much less than the access time, confirming\nour earlier intuition that we should try to minimize the number of\nmemory accesses.\nThe utility of inner nodes on different datasets can be effectively\nreflected by entropy, and can be combined with time/space cost to\nhelp flexibly determine the appropriate nodes during construction.\nAs shown in Figure 4, LR nodes handle linear datasets well (e.g.,\nuniform dataset, local normal dataset), obtaining the largest entropyclose to log2ğ‘and lowest cost-ratio. However, ML nodes cannot\nachieve good utility with the same number of children as BS nodes\nsince the distribution of local OSMC dataset is highly non-linear. In\nthis case, BS nodes that can maintain the entropy of log2ğ‘are most\nsuitable. Overall, these results validate that the data partitioning\nview is valuable and that it makes sense to use different nodes to\nbuild indexes flexibly.\n3) Analysis of Leaf Nodes. The time cost of these two leaf nodes\nsuggests that they are suitable for different situations. Since LR\nmodels and binary searches are required to lookup data points,\nthe time cost of external leaf nodes varies greatly (117.1-328.3 ns)\ndepending on the capacity and data distribution. When search\nranges are small, this node only takes 197.19 ns to find the exact\nlocation, and thus, it is more suitable for linear datasets (e.g., YCSB\ndataset). CF leaf nodes can be widely used on various datasets,\nsince only two memory accesses are required to obtain data points\nregardless of data distributions.\n4 INDEX CONSTRUCTION ALGORITHM\nIn Section 3.3, we have shown that the optimal index tree structure\ncan be constructed by minimizing the weighted sum of the time\nand space cost of the index structure (see Problem 2). In this section,\nwe describe an algorithm for solving it.\nFirst, let us rearrange the optimization objective as follows:\nEğ‘âˆˆğ‘„[ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡(ğ‘‡,ğ‘)]+ğœ†ğ‘†ğ¶ğ‘œğ‘ ğ‘¡(ğ‘‡)=ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡(ğ‘Ÿğ‘œğ‘œğ‘¡)+ğœ†ğ‘†ğ¶ğ‘œğ‘ ğ‘¡(ğ‘Ÿğ‘œğ‘œğ‘¡)\n+ğ‘âˆ‘ï¸\nğ‘–=1[\f\fğ‘„ğ‘‡ğ‘–\f\f\n|ğ‘„|Eğ‘âˆˆğ‘„ğ‘‡ğ‘–[ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡(ğ‘‡ğ‘–,ğ‘)]+ğœ†ğ‘†ğ¶ğ‘œğ‘ ğ‘¡(ğ‘‡ğ‘–)] (7)\nwhereğ‘represents the number of child nodes of the root node, ğ‘‡ğ‘–is\nthe sub-index tree of the ğ‘–-th child node, and ğ‘„ğ‘‡ğ‘–is the collection of\nqueries that access ğ‘‡ğ‘–. Note that the number of child nodes is chosen\nfrom an exponentially increasing sequence to reduce training time,\nsuch as the powers of 2.\nAs we can see, the terms inside the square brackets have a very\nsimilar form compared to the original objective. In other words,\nonce the root node of this subtree is fixed, the original optimiza-\ntion problem can be broken down into several independent sub-\nproblems of similar form, and each sub-problem can be solved inde-\npendently. For each sub-problem, we have two different algorithms\nfor solving it:\nâ€¢Node selection algorithm: A greedy algorithm that only con-\nsiders local information to construct nodes, which is used when\nthe subtree manages a large dataset (e.g., the root node).\n6\n\nâ€¢Dynamic programming (DP) algorithm: This algorithm is\nguaranteed to find the optimal sub-structure but is slower, and\nwe only use it when the sub-dataset is small.\nThese two algorithms are used in combination in our construc-\ntion algorithm. Then, the overall workflow for constructing the\nindex structure for a given dataset is:\nâ€¢First, we choose a root node setting using the greedy node selec-\ntion algorithm.\nâ€¢Next, we use the setting obtained in the previous step to assign\nthe dataset to the child nodes of the root, and then construct a\nsub-index tree over each assigned sub-dataset.\nâ€¢Each child node chooses the appropriate algorithm to build the\nsub-index structure according to the size of sub-dataset.\nâ€¢Finally, all the sub-structures are merged together and linked to\nthe root node to form the complete index tree.\nThe rest of this section is organized as follows: The greedy node\nselection step is described in Section 4.1 and the DP algorithm is\ndescribed in Section 4.2. We provide a simple example to illustrate\nour construction algorithm in Section 4.3.\n4.1 Node Selection Algorithm\nFor nodes that manage a large fraction of the dataset, such as the\nroot node, we hope to quickly select a suitable node design using\nonly local information. Below, we develop a greedy node selection\nalgorithm to find the locally optimal solution without considering\nthe design of the lower-level nodes:\nâ€¢For the current node, the node selection algorithm enumerates\nvarious possible node types and considers a number of choices\nfor the number of child nodes. For each enumerated setting, we\ncalculate the time cost, space cost and entropy of the node.\nâ€¢Then, we calculate the cost ratio of each setting using Equation 6\nin Section 3.3, and select the setting with the minimum cost ratio\nfor the current node.\nâ€¢Using the setting (model and child number) obtained in the pre-\nvious step, we assign each data point to one of the child nodes of\nthe current node.\nâ€¢Finally, the sub-index tree of each child node is constructed recur-\nsively using either this algorithm or the dynamic programming\nalgorithm in the next section.\nThe pseudocode of the greedy node selection algorithm is shown\nin Algorithm 1. Note that we are not directly using the space cost\nof the node itself for computing the cost-ratio, but rather the total\nspace cost of the child nodes of node ğ‘€. The reason behind such\na design is that the original cost-ratio would always favor large\nnumber of child nodes (since the time/space cost of the node itself\nis fixed). This change allows the greedy algorithm to choose a more\nreasonable node setting.\n4.2 Dynamic Programming Algorithm\nAs shown in Equation 7, the original optimization problem can be\ndecomposed into similar sub-tasks once the root node is fixed. Based\non this intuition, we can use a dynamic programming algorithm to\nsolve this problem:\nâ€¢State: The state of each step is the dataset handled by the current\nnode, denoded as ğ·ğ‘™,ğ‘Ÿ, whereğ‘™andğ‘Ÿare the left and right indexAlgorithm 1 The Greedy Node Selection Procedure\nInput: keysğ¾[], access frequency ğ‘“[], query number ğ‘š;\nOutput: local optimal design ğ‘€\n1:ğ‘‚ğ‘ğ‘¡ğ‘–ğ‘šğ‘ğ‘™ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’â†âˆ\n2:ğ‘ƒâ†Ã|ğ‘“|\nğ‘—=1ğ‘“ğ‘—/ğ‘š\n3:forevery inner node design ğ‘€ğ‘–do\n4:ğ¾1,...,ğ¾ğ‘ğ‘–â†Partition(ğ¾,ğ‘€ğ‘–)\n5:ğ¶â„ğ‘–ğ‘™ğ‘‘ğ‘†ğ¶ğ‘œğ‘ ğ‘¡â†ğ‘ğ‘–Ã—64bytes\n6:ğ¶ğ‘œğ‘ ğ‘¡â†ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡(ğ‘€ğ‘–)+ğœ†Ã—ğ¶â„ğ‘–ğ‘™ğ‘‘ğ‘†ğ¶ğ‘œğ‘ ğ‘¡/ğ‘ƒ\n7:forğ‘—=1toğ‘ğ‘–do\n8:ğ‘ğ‘—â†\f\fğ¾ğ‘—\f\f/|ğ¾|\n9:end for\n10:ğ»(ğ‘€ğ‘–)â†âˆ’Ãğ‘ğ‘–\nğ‘—=1ğ‘ğ‘—log2ğ‘ğ‘—\n11: if(ğ¶ğ‘œğ‘ ğ‘¡/ğ»(ğ‘€ğ‘–))<ğ‘‚ğ‘ğ‘¡ğ‘–ğ‘šğ‘ğ‘™ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’ then\n12:ğ‘‚ğ‘ğ‘¡ğ‘–ğ‘šğ‘ğ‘™ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’â†ğ¶ğ‘œğ‘ ğ‘¡/ğ»(ğ‘€ğ‘–)\n13:ğ‘€â†ğ‘€ğ‘–\n14: end if\n15:end for\n16:returnğ‘€\nof the entire dataset. Let ğ‘„ğ‘™,ğ‘Ÿbe the collection of queries that\naccessğ·ğ‘™,ğ‘Ÿ, andğ‘‡ğ‘™,ğ‘Ÿbe the sub-index tree for ğ·ğ‘™,ğ‘Ÿ. Then the\ncorresponding entry ğ‘ğ‘œğ‘ ğ‘¡[ğ‘™,ğ‘Ÿ]in DP table is the value of the\nfollowing expression:\nğ‘ğ‘œğ‘ ğ‘¡[ğ‘™,ğ‘Ÿ]=min\nğ‘‡ğ‘™,ğ‘Ÿ{\f\fğ‘„ğ‘™,ğ‘Ÿ\f\f\n|ğ‘„|Eğ‘âˆˆğ‘„ğ‘™,ğ‘Ÿ[ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡(ğ‘‡ğ‘™,ğ‘Ÿ,ğ‘)]+ğœ†ğ‘†ğ¶ğ‘œğ‘ ğ‘¡(ğ‘‡ğ‘™,ğ‘Ÿ)}(8)\nâ€¢State transition equation: Letğ‘†be the collection of all possible\nsettings for the root node of sub-index tree, including the type of\nthe root node and the number of the child nodes. Then according\nto Equation 7 and 8, the state transition equation is:\nğ‘ğ‘œğ‘ ğ‘¡[ğ‘™,ğ‘Ÿ]=min\n(ğ‘€,ğ‘)âˆˆğ‘†{\f\fğ‘„ğ‘™,ğ‘Ÿ\f\f\n|ğ‘„|ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡(ğ‘€)+ğœ†ğ‘†ğ¶ğ‘œğ‘ ğ‘¡(ğ‘€)+ğ‘âˆ‘ï¸\nğ‘—=1ğ‘ğ‘œğ‘ ğ‘¡[ğ‘™ğ‘—,ğ‘Ÿğ‘—]}\nwhereğ‘€andğ‘are the node type and the number of child nodes\n(if the node is a leaf node, ğ‘is 0), respectively, ğ‘™ğ‘—andğ‘Ÿğ‘—are the\nleft and right index of the sub-dataset that belongs to the child\nnodeğ‘—. If the dataset is smaller than a certain threshold, then\nwe only consider the leaf node settings in the state transition\nequation.\nSpecifically, given a sub-dataset ğ·ğ‘™,ğ‘Ÿ, we need to enumerate all\nits possible settings for the root node, and compute the cost of each\nsetting separately. Then, the setting with minimum cost is selected\nto construct the current node, and the cost is stored to the entry\nğ‘ğ‘œğ‘ ğ‘¡[ğ‘™,ğ‘Ÿ]. For each setting, we first look up the time/space cost of the\nroot node from our cost model, and use the sub-dataset ğ·ğ‘™,ğ‘Ÿto train\nthe root node model ğ‘€. Then, we use the trained model ğ‘€to assign\nthe dataset to ğ‘child nodes and obtain their cost via a memorized\nsearch approach: for each subtask of computing ğ‘ğ‘œğ‘ ğ‘¡[ğ‘™ğ‘—,ğ‘Ÿğ‘—], the\nalgorithm first checks whether it has been solved before. Then\naccording to the check result, it either returns the minimum cost\ndirectly from the DP table or recursively calls the process of the\nDP algorithm. When the number of data points in the subtree\nis less than a pre-specified threshold, the algorithm will directly\nconstruct a leaf node as the current node. Otherwise, the algorithm\nconsiders two cases (leaf nodes or subtrees with inner nodes) and\n7\n\nchooses the optimal design according to the transition equation.\nThe pseudocode of the DP algorithm is shown in Algorithm 2.\nAlgorithm 2 DP Algorithm\nInput: keysğ¾[], queriesğ‘„[], left indexğ‘™, right index ğ‘Ÿ;\nOutput: optimal structure ğ‘‡ğ‘™,ğ‘Ÿ,|ğ‘„ğ‘™,ğ‘Ÿ|\n|ğ‘„|Eğ‘âˆˆğ‘„ğ‘™,ğ‘Ÿ[ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡(ğ‘‡ğ‘™,ğ‘Ÿ,ğ‘)]+\nğœ†(ğ‘†ğ¶ğ‘œğ‘ ğ‘¡(ğ‘‡ğ‘™,ğ‘Ÿ))\n1:ifsub-problem(ğ‘™,ğ‘Ÿ)have been solved before then\n2:return(ğ‘‡ğ‘™,ğ‘Ÿ,ğ‘ğ‘œğ‘ ğ‘¡[ğ‘™,ğ‘Ÿ])\n3:end if\n4:/*ğ‘˜ğ¿ğ‘’ğ‘ğ‘“ğ‘€ğ‘ğ‘¥ğ¶ğ‘ğ‘ğ‘ğ‘ğ‘–ğ‘¡ğ‘¦ must be larger than ğ‘˜ğ¿ğ‘’ğ‘ğ‘“ğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ */\n5:ifğ‘Ÿâˆ’ğ‘™+1<=ğ‘˜ğ¿ğ‘’ğ‘ğ‘“ğ‘€ğ‘ğ‘¥ğ¶ğ‘ğ‘ğ‘ğ‘ğ‘–ğ‘¡ğ‘¦ then\n6: Construct a leaf node ğ‘‡ğ‘™,ğ‘Ÿfromğ¾[ğ‘™,...,ğ‘Ÿ]andğ‘„[ğ‘™,...,ğ‘Ÿ].\n7:ğ‘‚ğ‘ğ‘¡ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’â†|ğ‘„ğ‘™,ğ‘Ÿ|\n|ğ‘„|Eğ‘âˆˆğ‘„ğ‘™,ğ‘Ÿ[ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡(ğ‘‡ğ‘™,ğ‘Ÿ,ğ‘)]+ğœ†(ğ‘†ğ¶ğ‘œğ‘ ğ‘¡(ğ‘‡ğ‘™,ğ‘Ÿ))\n8:end if\n9:ifğ‘Ÿâˆ’ğ‘™+1>ğ‘˜ğ¿ğ‘’ğ‘ğ‘“ğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ then\n10: forevery inner node design ğ‘€do\n11:({ğ‘™1,ğ‘Ÿ1},...,{ğ‘™ğ‘,ğ‘Ÿğ‘})â† Partition(ğ¾[ğ‘™,...,ğ‘Ÿ],ğ‘€)\n12: forğ‘—=1toğ‘do\n13:(ğ‘‡ğ‘™ğ‘—,ğ‘Ÿğ‘—,ğ‘ğ‘œğ‘ ğ‘¡[ğ‘™ğ‘—,ğ‘Ÿğ‘—])â† DP(ğ¾,ğ‘„,ğ‘™ğ‘—,ğ‘Ÿğ‘—)\n14: end for\n15:ğ‘…ğ‘œğ‘œğ‘¡ğ¶ğ‘œğ‘ ğ‘¡â†|ğ‘„ğ‘™,ğ‘Ÿ|\n|ğ‘„|ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡(ğ‘€)+ğœ†(ğ‘†ğ¶ğ‘œğ‘ ğ‘¡(ğ‘€))\n16: ifÃ\nğ‘—ğ‘ğ‘œğ‘ ğ‘¡[ğ‘™ğ‘—,ğ‘Ÿğ‘—]+ğ‘…ğ‘œğ‘œğ‘¡ğ¶ğ‘œğ‘ ğ‘¡ <ğ‘‚ğ‘ğ‘¡ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’ then\n17:ğ‘‚ğ‘ğ‘¡ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’â†Ã\nğ‘—ğ‘ğ‘œğ‘ ğ‘¡[ğ‘™ğ‘—,ğ‘Ÿğ‘—]+ğ‘…ğ‘œğ‘œğ‘¡ğ¶ğ‘œğ‘ ğ‘¡\n18: Construct an index structure {ğ‘‡ğ‘™ğ‘—,ğ‘Ÿğ‘—}fromğ‘‡ğ‘—andğ‘€\n19: end if\n20: end for\n21:end if\n22:return(ğ‘‡ğ‘™,ğ‘Ÿ,ğ‘‚ğ‘ğ‘¡ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’)\nAlthough in principle, the dynamic programming algorithm can\nbe used to construct the optimal index structure for the entire\ndataset, in practice, it is too slow to handle large datasets. Therefore,\nwe only use it to solve sub-problem that are small enough.\n4.3 A Simple Example\nHere we use the same example as before to demonstrate the entire\nindex construction process.\nLR Root NodeM0[0, 199]M1[200, 398]M2[400, 598]M3[600, 798]ğ·={0,1,2,â€¦,199,200,202,204,â€¦,798}Child numberLRP.  L R214.94347.721.788.417.7!ğ‘‡ğ‘¦ğ‘ğ‘’:ğ¿ğ‘…ğ¶â„ğ‘–ğ‘™ğ‘‘ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ:4Cost-ratio=!(#)%&'()#*+(,&'()(#))!(#)-(#)For child node M0 :[l, r]cost[0, 99]90[100, 199]90[0, 199]200Inner Node [0, 199]Cost:200Structure: Leaf0Cost: 90Possiblechoice:{ğ‘´â†’ğ‘³ğ‘¹,ğ’„=ğŸ}Cost =  ğ¶ğ‘œğ‘ ğ‘¡ğ‘Ÿğ‘œğ‘œğ‘¡+âˆ‘./01[ğ‘ğ‘œğ‘ ğ‘¡[ğ‘™.,ğ‘Ÿ.]]=(19.36+0.64) + 90 + 90 = 200Data: [0, 99]Look up cost[0, 99]Structure: Leaf1Cost: 90Data: [100, 199]cost[100, 199]\nFigure 5: Construct the Root/Inner Node\nExample 2. We demonstrate our construction algorithm using the\ndatasetğ·={0,1,Â·Â·Â·199,200,202,Â·Â·Â·,798}in Figure 5. First, we usethe greedy node selection algorithm to choose the setting of the root\nnode. We consider two node types: LR and P.LR, and compute their\ncost ratios with 2, 4, and 8 child nodes, respectively. Among them, the\nLR root node with 4 child nodes has the minimum cost ratio of 7.7.\nTherefore, we use it to construct the root node and assign the dataset\nto the 4 child nodes according to the trained LR model.\nWe use sub-datasets to construct the optimal sub-trees for 4 child\nnodes according to the DP algorithm. We need to enumerate all possible\nsettings for them. For example, for node ğ‘€0withğ·0,199, a possible\nsetting is that the type is the LR inner node and the number of child\nnodes is 2. After looking up its cost of 20, we use the trained LR\nnode to assign the dataset to the child nodes and obtain two smaller\nsub-datasets: ğ·0,99andğ·100,199. According to the state transition\nequation, the algorithm needs to compute the minimum cost for the\ntwo sub-datasets. Here we assume that their cost has already been\ncomputed previously, therefore, we only need to look up ğ‘ğ‘œğ‘ ğ‘¡[0,99]\nandğ‘ğ‘œğ‘ ğ‘¡[100,199]in the DP table (both are 90). Then, we can get the\noverall cost of 200 by adding up all three costs. After enumerating\neach possible choice, we use the setting with the minimum cost to\nconstructğ‘€0. Then, forğ‘€1âˆ’ğ‘€3, we repeat the above steps and select\na leaf node as their root node. Finally, the optimal index structure can\nbe obtained by linking these four subtrees with the root node.\n5 CACHE-AWARE DESIGN\nIn this section, we describe the cache-aware design of CARMI with\na new memory layout in detail, use some examples to explain the\nintuition and benefits of such a design, and also discuss the potential\nopportunities for using memory prefetching instructions to further\nspeed up queries.\nThe rest of this section is organized as follows: Section 5.1 dis-\ncusses the details of cache-aware design, and Section 5.2 describes\nour new memory layout and the basic lookup and insert operations.\nThe use of memory prefetch will be discussed in Section 5.3.\n5.1 Details of Cache-Aware Design\nB+ Trees with tree nodes occupying exactly a cache line size can\noutperform standard binary search trees [ 47]. Specifically, each\nmemory access always copies a fixed-size memory content (i.e.,\ncache line size, usually 64 bytes), then indexes that utilize the cache\nand do not â€œwaste\" any data retrieved into cache memory can\ngenerally outperform standard data structure by a large margin.\nIn CARMI, we employ the same design decision as in B+ Tree\nand enforce all tree nodes to have a fixed size of 64 bytes. To un-\nderstand the intuition behind such a design, we briefly analyze the\nperformance bottleneck of the lookup procedure in RMI. For an\nindex structure within the RMI framework (shown in Figure 1), the\nprocedure for accessing a data point is as follows:\nâ€¢We first access the content of the root node, and then use its\nmodel to choose a child node of the root.\nâ€¢Subsequently, in each layer, we visit a node chosen by the node in\nthe previous layer, and then use the model of this node to choose\none of its child nodes. We repeat this step until we have reached\na leaf node.\nIn the above procedure, for each tree node, we need at least one\nmemory access to get its content before performing the model com-\nputation. For simple models (e.g., linear models), the computation\n8\n\ntime is usually less than 20 ns, while a memory access takes about\n70-100 ns if the node content is not cached. Therefore, the time\nrequired for memory access would actually take up most of the\ntime we spent on data lookup. In other words, the performance\nbottleneck of RMI is memory access.\nBased on the above analysis, we hope to minimize the number of\nmemory accesses and fully utilize each access during data lookup.\nIn CARMI, we achieve them from the following two aspects.\nFirst, we enforce each node to have a size of exactly 64 bytes to\nalign the cache line. This design allows the node to be fetched with\nonly one memory access. Furthermore, the large node size allows\nit to store rich information, which helps to reduce the average tree\ndepth, thus reducing the number of needed memory accesses.\nTo further reduce the number of memory accesses in the last\nmile, we design a new leaf node called Cache-Friendly array leaf\nnode, which is conceptually similar to a two-layer B+ Tree node.\nThe first layer is a 64-byte root that stores pointers to several 256-\nbyte data blocks in the second layer and the minimum key values of\neach block. With such a design, we only need one memory access\nto obtain metadata to determine the next data block and narrow\ndown the last-mile search range to 256 bytes, effectively reducing\nmemory accesses on real-world datasets.\n5.2 Memory Layout\nFor the memory layout of CARMI, we have two main arrays, ğ‘‘ğ‘ğ‘¡ğ‘\nandğ‘›ğ‘œğ‘‘ğ‘’ , to assist in implementing our cache-aware design. These\ntwo arrays are used to store data points and tree nodes, respectively,\nas shown in Figure 6, with details as follows:\nâ€¢Data array: Theğ‘‘ğ‘ğ‘¡ğ‘ array is used to store data points in CARMI.\nIt is a large array containing many small data blocks, each of\nwhich has a fixed size, represented by a parameter ğ‘˜ğµğ‘™ğ‘œğ‘ğ‘˜ğ‘†ğ‘–ğ‘§ğ‘’\nwith a default value of 256. Data points are stored in these data\nblocks. In the ğ‘‘ğ‘ğ‘¡ğ‘ array, data blocks managed by the same leaf\nnode are stored in adjacent locations.\nâ€¢Node array: All tree nodes, including both inner and leaf nodes,\nare stored in the ğ‘›ğ‘œğ‘‘ğ‘’ array. Each tree node occupies a total of\n64 bytes: the first byte is always the node type identifier, and\nthe next three bytes are used to store the number of child nodes\n(the number of data blocks for leaf nodes). For inner nodes, the\nfollowing 4 bytes represent the starting index of the child nodes\nin theğ‘›ğ‘œğ‘‘ğ‘’ array. For leaf nodes, they store the starting index\nof data blocks in the ğ‘‘ğ‘ğ‘¡ğ‘ array instead. The remaining 56 bytes\nstore additional information depending on the tree node type.\nâ€¦â€¦â€¦â€¦datausedto store data blocksâ€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦M0M1M2Mkdata blocks in leaf node 0(thesize of each block is fixed)nodeusedto store  tree nodesa union structure of all types of nodes(64 bytes)4 bytes4 bytestypeinner: # of child nodesleaf: # of data blocksstart_index(the index of its first child node/data block)additional information56bytes\nFigure 6: Memory Layout\nWith the help of the above two arrays, we can perform operations\nsuch as lookup and insert. The way we use them in lookup and\ninsert operations is described below.5.2.1 Lookup Operation. When accessing a data point, we first use\nthe root node model to compute the index of the node in the next\nlayer. Next, we access the tree node according to the index value and\nuse its model to update the value of the index variable. This process\nis repeated iteratively until a leaf node is visited. Finally, we search\nwithin this leaf node to get the corresponding data record. The\npseudocode of the data access process can be found in Algorithm 3.\nAlgorithm 3 Lookup\nInput:ğ‘˜ğ‘’ğ‘¦\nOutput:ğ‘‘ğ‘ğ‘¡ğ‘\n1:ğ‘–ğ‘‘ğ‘¥â†ğ‘Ÿğ‘œğ‘œğ‘¡.ğ‘ƒğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡(ğ‘˜ğ‘’ğ‘¦)\n2:while true do\n3:ifğ‘›ğ‘œğ‘‘ğ‘’[ğ‘–ğ‘‘ğ‘¥].ğ‘¡ğ‘¦ğ‘ğ‘’ ==ğ‘–ğ‘›ğ‘›ğ‘’ğ‘Ÿ _ğ‘›ğ‘œğ‘‘ğ‘’ then\n4:ğ‘–ğ‘‘ğ‘¥â†ğ‘›ğ‘œğ‘‘ğ‘’[ğ‘–ğ‘‘ğ‘¥].ğ‘ƒğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡(ğ‘˜ğ‘’ğ‘¦)\n5:else\n6: /* access leaf node */\n7:ğ‘™â†ğ‘›ğ‘œğ‘‘ğ‘’[ğ‘–ğ‘‘ğ‘¥].ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ _ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥\n8:ğ‘â†ğ‘›ğ‘œğ‘‘ğ‘’[ğ‘–ğ‘‘ğ‘¥].ğ‘ƒğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡(ğ‘˜ğ‘’ğ‘¦)\n9:ğ‘Ÿğ‘’ğ‘ â†ğ‘†ğ‘’ğ‘ğ‘Ÿğ‘â„ğµğ‘™ğ‘œğ‘ğ‘˜(ğ‘‘ğ‘ğ‘¡ğ‘[ğ‘™+ğ‘],ğ‘˜ğ‘’ğ‘¦)\n10: returnğ‘‘ğ‘ğ‘¡ğ‘[ğ‘™+ğ‘].ğ‘ ğ‘™ğ‘œğ‘¡ğ‘ [ğ‘Ÿğ‘’ğ‘ ]\n11: end if\n12:end while\n5.2.2 Insert Operation. The basic process of the insert operation is\nsimilar to the lookup operation. After finding the correct data block\nfor insertion, we insert the data point into it. In addition, there are\ntwo mechanisms that can be initiated by the leaf node under certain\nsituations:\nâ€¢Expand: When a leaf node needs more space, it can initiate an\nexpand operation to get more data blocks. We first collect all the\ndata points stored in it, and then construct a new leaf node at a\nnew location with more space, as shown in Figure 7. The new\nleaf node then replaces the original one to complete the process.\nâ€¢Split: If we can no longer use a single leaf node to efficiently\nmanage all the data points, it needs to be split. The leaf node will\nbe replaced with a subtree consisting of a new inner node and\nseveral new leaf nodes, as shown in Figure 7. The number of leaf\nnodes in the subtree depends on the trained model of the new\ninner node.\nLeaf nodeL0L1L6Inner nodeL0L1â€¦â€¦Leaf node0Leaf node1Leaf nodekâ€¦â€¦L0L0L3â€¦â€¦L0L1L2L0L1L2L3Leaf nodeLeaf nodeSplitExpand\nFigure 7: Expand and Split Mechanism\n5.3 Prefetch\n5.3.1 Prefetch Mechanism. In some cases, when the key value dis-\ntribution in the dataset is very regular (e.g., uniform distribution),\nthe index of the data block can be directly predicted from the input\nkey value. In such cases, we can further reduce the data access\nlatency by utilizing memory prefetching. More specifically, we add\n9\n\nan additional prefetch model to the root node to predict the data\nblock where a given key value may be stored. Therefore, during\nthe access process, the predicted data block is prefetched at the\nroot node, which can be executed in parallel with other memory\naccesses in the normal process. If the prediction is correct, then the\ndata block will be available in the cache when we need to access\nit. In this way, for datasets with regular data distribution, we can\nspeed up the access to a certain extent. We demonstrate the prefetch\nmechanism via the following example:\n(4.1) 80.09 ns25(4.2)90.09ns1(7)48.5ns3CPUTimeAccessTime (normal process)CPU Time:(2) Calculate the idxof a leaf node(3)Predict the idxof a block (Prefetch)(5) Calculate the idxof a block(7) Get the data pointsAccessTime (prefetch)218.38ns320.87ns524.23ns8.29nsAccess Time:(1)Access root (cache)(4.1) Parallel access a leaf node (memory)(4.2)Parallel access a block (memory)  (Prefetch)(6.1) Access the data block (cache)(6.2) Access the data block (memory)\n(4.1) 80.09 ns51(7) 48.5 nsCPUTimeAccessTime220.38ns524.23ns8.29ns(6.2)90.09nsTime cost with prefetch: 205.65nsTime cost without prefetch: 272.47ns26.15.29ns\nFigure 8: Memory Prefetch in CARMI\nExample 3. We use the same settings as in Example 1 to show\nthe effect of prefetching. The time cost of accessing a data point in\nCARMI can be divided into two parts: CPU time and access time. In\nthis example, we need to access: the root node (8.29 ns), the leaf node\n(80.09 ns), and the data block (90.09ns). The CPU time consists of\nmodel calculations in the root node (20.38 ns), the leaf node (24.23 ns),\nand the search process (48.5 ns) in the data block. Note that the CPU\ncalculations must wait until the corresponding cache/memory access\nis completed before it can be processed. Therefore, without prefetching,\nCPU calculations and access operations are performed alternately and\ndo not overlap, which constitutes a total time cost of 272.47 ns.\nTo utilize the prefetch mechanism, we need to add an additional\n28.87 ns of CPU time to predict the index of the data block that needs to\nbe prefetched, so that we can then prefetch this data block in advance\nat the root node. The benefit of this prefetching step is that, when we\nactually need the data block later on in the process, it will be already\nin the cache and be retrieved very quickly, reducing the time cost of a\nmemory access. In general, a lookup operation with prefetching only\nrequires 205.65 ns in this example, which reduces the time by 67 ns\ncompared to the situation without prefetching.\n5.3.2 Prefetch Support. In order to support the prefetching design,\nwe need to make a few changes to the construction algorithm: we\nneed to add an additional model at the root node to predict the\nindex of the data block, and we also want to store data points in\ndata blocks according to the prefetch prediction model whenever\npossible.\nThis prefetch prediction model is used to predict the index of\nthe data block corresponding to each key value during the lookup\nprocedure, and we store data points in data blocks according to\nthe model during the construction of the index structure whenever\npossible.\nMore specifically, we use a piecewise linear regression model\nas the prefetch prediction model. The prefetch prediction model\nkeysizeâ‰¤2*Bstore keys as predictedâ€¦â€¦â€¦leafID(#of blocks allocated by the model)\nâ€¦ (3)1 (2)2 (2)blockIDâ€¦â€¦â€¦3 (2)â€¦â€¦1234ii+1i+2the size of each block is Bsizeâ‰¤2*Bstore keys as predictedsize â‰¤3*Bsize > 2*Breallocate a new set of data blocksâ€¦Figure 9: Prefetch Support\nrequires access to the raw output of the root model (leaf index\nbefore rounding down), and then use it as input to the piecewise\nlinear model to compute a block index. Note that by the time we\nstart to construct the prefetch prediction model, the root node has\nalready been determined by the greedy node selection algorithm.\nTherefore, the unrounded leaf node index can be directly obtained\nfrom the root node.\nIn the piecewise linear regression model, we force the slope\nand intercept of each segment to be integers, so that within each\nsegment, each leaf node is mapped to the same number of data\nblocks. For each leaf node, we first attempt to allocate its data blocks\naccording to the prefetch prediction model, and check whether the\ncurrent leaf node has enough capacity to store all assigned data\npoints. If the leaf node has enough capacity, then we simply store\nthe data points in these data blocks. Otherwise, we reallocate a new\nset of data blocks for the current leaf node at another irrelevant\nlocation. Figure 9 illustrates the process of data block allocation for\nsupporting the prefetch mechanism.\nWithin each leaf node, we try to store as many data points in\nthe predicted data block as possible, to increase the success rate of\nprefetch. This corresponds to another optimization problem within\nthe leaf node.\n5.3.3 The Prefetch Prediction Model. To learn the prefetch pre-\ndiction model, we also establish a cost model for it and optimize\naccordingly. Let ğ¿={ğ‘€1,...,ğ‘€ğ‘›}be the collection of leaf nodes\ndirectly under the root node, then the cost of index accesses on\nthese nodes can be approximated as:\nğ‘›âˆ‘ï¸\nğ‘–=1[\f\fğ‘„ğ‘€ğ‘–\f\f\n|ğ‘„|Eğ‘âˆˆğ‘„ğ‘€ğ‘–[ğ‘‡ğ¶ğ‘œğ‘ ğ‘¡(ğ‘€ğ‘–,ğ‘)]+ğœ†ğ‘†ğ¶ğ‘œğ‘ ğ‘¡(ğ‘€ğ‘–)] (9)\nwhereğ‘„ğ‘€ğ‘–is the collection of queries accessing ğ‘€ğ‘–.\nConsidering the effect of prefetching, the time cost of data blocks\nbecomes slightly different. a leaf node that supports prefetch (i.e.,\nallocated to its predicted location), the time cost of accessing data\nblocks is now equal to the latency of cache memory, and the value\nofğ‘‡ğ¶ğ‘œğ‘ ğ‘¡(ğ‘€ğ‘–,ğ‘)also changes accordingly. This downside is that\nnowğ‘†ğ¶ğ‘œğ‘ ğ‘¡(ğ‘€ğ‘–)can be slightly larger than needed.\nWe use a dynamic programming algorithm to optimize Equa-\ntion 9. The DP state ğ‘ğ‘œğ‘ ğ‘¡[ğ‘–,ğ‘Ÿ]is the total cost of first ğ‘Ÿleaf nodes\nwhen we use ğ‘–segments to model them. This is a standard 2D\ndynamic programming and we omit the algorithm details here.\n10\n\nuniformzipfianuniformzipfianuniformzipfianuniformzipfianCARMI1394.3114.385.11475.7129.292.111.4*93.21771.5287.5160.11812.6248.5166.4ALEX1474.3112.583.31476.7115.589.31474.397.61522.0421.9221.41522.0404.0228.1B-Tree2276.0482.6267.82276.0480.6268.12276.0269.72276.0479.9262.52276.0480.6261.5CARMI1509.9168.4154.52127.4190.3168.611.4*161.21772.5357.9246.01882.7316.9246.2ALEX1474.3164.1147.01476.7294.0262.91474.3317.31522.01109.1893.31522.01801.71777.6B-Tree2276.0500.3419.92276.0522.4421.92276.0401.22276.0503.5421.32276.0498.7432.9CARMI1509.4135.8102.12077.5155.0104.211.4*104.41771.9330.2172.51812.9278.3174.2ALEX1474.3116.289.21476.7121.795.41474.3234.71522.0651.7420.41522.0993.8829.0B-Tree2276.0488.9286.82276.0483.4292.52276.0280.42276.0483.1284.52276.0477.6280.1CARMI1509.9156.9117.32082.5162.8121.511.4*124.51772.2341.6179.51853.8286.2184.0ALEX1474.3130.4109.81476.7190.4180.11474.3285.01522.0854.7588.51522.01240.61092.4B-Tree2276.0490.1318.82276.0488.2321.42276.0311.22276.0487.3314.42276.0483.2314.5CARMI1509.4402.4256.42077.5434.3271.311.4*266.41771.9585.6305.31812.9600.9323.9ALEX1474.3375.0233.61476.7383.6240.31474.3359.31522.0935.3598.21522.01246.4990.5B-Tree2276.0704.5416.52276.0728.5414.72276.0393.12276.0696.4417.12276.0698.7415.3* Does not include the space of external array.time /nsread-onlywrite-heavyread-heavyrange scanwrite-partialFace datasetspace/MBtime /nsspace/MBtime /nsspace/MBtime/nsspace/MBtime /nsspace/MBworkloadsindexesuniform datasetlognormal datasetYCSB datasetOSMC datasetFigure 10: CARMI vs. Baselines: Time and Space Usage Comparison.\n6 EXPERIMENTS\nIn this section, we conduct experiments on various datasets and\nworkloads to evaluate the performance of CARMI and delve into\nCARMI from different aspects through several auxiliary experi-\nments.\n6.1 Experimental Setup\nWe conducted all the single-threaded experiments on an Ubuntu\nLinux machine equipped with an AMD Ryzen 3700X 8-Core Pro-\ncessor and 32GB RAM.\n6.1.1 Datasets. Seven datasets are used in our main experiments.\nTheir details are listed as follows:\nâ€¢synthetic dataset: 4synthetic datasets are generated from\n4different distributions: (a) lognormal :ğ‘™ğ‘œğ‘”(ğ‘˜ğ‘’ğ‘¦)âˆ¼N( 0,1);\n(b)uniform :ğ‘˜ğ‘’ğ‘¦âˆ¼U( 0,1); (c)normal :ğ‘˜ğ‘’ğ‘¦âˆ¼N( 0,1); (d)\nexponential :ğ‘˜ğ‘’ğ‘¦âˆ¼Exp(0.25). All these datasets are stored as\nkey-value pairs of < ğ‘‘ğ‘œğ‘¢ğ‘ğ‘™ğ‘’,ğ‘‘ğ‘œğ‘¢ğ‘ğ‘™ğ‘’ >. The size of each dataset is\n1GB, and the key values of data points are multipled by 108.\nâ€¢YCSB dataset : This dataset consists of 67,108,864 data records\ngenerated by YCSB benchmark [ 14]. We build an index on the\nğ‘Œğ¶ğ‘†ğµ _ğ¾ğ¸ğ‘Œ attribute, which obeys a uniform distribution.\nâ€¢OSMC dataset: This dataset is generated from a public dataset\nof Open Street Maps [ 2], which includes the latitude and longi-\ntude of points around the world. The number of data points in\nthis dataset is 67,108,864 (the size of each dataset is 1GB). This\ndataset is stored as key-value pairs of < ğ‘¢ğ‘–ğ‘›ğ‘¡64,ğ‘¢ğ‘–ğ‘›ğ‘¡ 64>.\nâ€¢Facebook dataset: This dataset is generated from the IDs of\nFacebook users [ 24,51], consisting of 67,108,864 data points.\nThis dataset is stored as key-value pairs of < ğ‘¢ğ‘–ğ‘›ğ‘¡64,ğ‘¢ğ‘–ğ‘›ğ‘¡ 64>.\nWe have also ported CARMI into the SOSD [ 24] platform to test\nits performance and the details are shown in Section 6.3.\n6.1.2 Evaluation Workloads. We use 5 different query workloads\nin our experiments. The first three workloads are similar to the\nworkloads in the YCSB benchmark [ 14]: (a) a write-heavy workload\nwith a mix of 50% reads and 50% inserts; (b) a read-heavy workload\nwith a mix of 95% reads and 5% inserts; (c) a read-only workload.\nIn addition, we include a write-partial workload in which insertoperations are concentrated in a small key value range. This work-\nload consists of 85% reads and 15% inserts, and the inserted data\npoints are concentrated between 60% and 90% of the sorted dataset.\nThe write-partial workload is intended to mimic some real-world\nscenarios in which the keys of new data points are contained in a\nsmall value range during each session (e.g., when using the current\ndate/time as key value). Finally, we include a range scan workload\nwith a mix of 95% range scan and 5% inserts as in [16].\nFor each workload, we exectue 100,000 operations and mea-\nsure the average time used by each operation. For all workloads,\nlookup keys are selected at random from the existing keys in the\nindex. We consider two access patterns for generating queries: a\nZipfian distribution (the normalized frequency of the ğ‘¥-th element\nis:ğ‘“(ğ‘¥)=1\nğ‘¥ğ›¼/Ãğ‘\nğ‘–=11\nğ‘–ğ›¼,ğ›¼=0.99as in [ 16]) and a uniform distri-\nbution. In workloads involving insert operations, read and insert\noperations are performed alternately in proportion. For example, in\nread-heavy workloads, we execute 19 lookups followed by 1 insert\noperation. For range scan workloads, the length of each range scan\nis uniformly sampled from [1, 100].\nFor the YCSB dataset, the queries are handled differently to\nsimulate real-world use cases: read queries are generated from a\nZipfian distribution as usual, but insert keys are monotonically\nincreasing to be consistent with the YCSB benchmark.\n6.1.3 Implementation and Baseline. We compare against the fol-\nlowing baselines: STX B+ Tree [ 3] and ALEX [ 16]. The node size of\nB+ Tree is 512 bytes, which is optimal for in-memory queries [ 55].\nAll other parameters use the default values in the source code. SOSD\nbenchmark [ 24] also includes some other indexes as baselines in\nSection 6.3. CARMI is open-sourced and can be found on Github [ 1].\nIn our experiments, we only tune one parameter ğœ†, which is sensi-\ntive to data distribution and dataset size, and all other parameters\ndo not need to be tuned and use the default values unless otherwise\nstated. More details can be found in the appendix.\n6.1.4 Training Queries for Index Construction in CARMI. In our\nexperiments, the training query workload for index construction\nis specified as a uniform read access over all data points, and a\nuniformly sampled subset of data points3to serve as key values for\n3The insert queries in the training queries of the write-partial workload are sampled\nbetween 60% and 90% of the overall dataset.\n11\n\ninsert queries (with the exception of YCSB dataset). The ratio of\nread/insert queries is the same as in the target query workload.\n6.2 General Efficiency Comparison\nIn this section, we evaluate the performance of CARMI and com-\npare it with the baselines. We consider 5 query workloads and 7\ndifferent datasets, as explained in Section 6.1. We consider two ac-\ncess patterns for datasets other than YCSB: uniform and Zipfian, as\nillustrated in Section 6.1.2. These result in a total of 65 possible con-\nfigurations. Figure 10 shows the results for 45 of them due to space\nlimitation, and the results for normal and exponential datasets are\nomitted since they are similar to those of the uniform dataset.\nIn general, the time efficiency of both learned indexes is signifi-\ncantly better than B+ Tree. Comparing CARMI with ALEX, which\nis also a learned index, we see that these two learned indexes per-\nform roughly the same over synthetic datasets. However, CARMI\nsignificantly outperforms on these real-world datasets, achieving\nan average speedup of 1.2 Ã—/2.2Ã—on read-only/read-write work-\nloads. This demonstrates the effectiveness of data partitioning and\ncache-aware design over real-world datasets, in which data location\nis much harder to predict.\n6.2.1 Read-only Workload. For read-only workloads, the lookup\nspeed of CARMI is about 1.6-4.2 Ã—faster than B+ Tree, and 1.2 Ã—\nfaster on average than ALEX. Meanwhile, CARMI uses only 0.7 Ã—\nof memory space compared to B+ Tree (excluding YCSB4dataset).\nIt is of interest to compare CARMI with B+ Tree, since their\ndifference is only on the upper level. The significant speed gain\nof CARMI is mainly due to the following reasons: (a) CARMI uses\nfast model prediction instead of binary search. (b) the larger fanout\nof the root nodes in CARMI reduces the depth of the index. Then,\nmost of the data points are managed by a leaf node that is directly\nunder the root node, making the index structure flatter. To verify\nthat our construction algorithm can build a flatter index, we also\ncalculated the average tree depth with respect to keys, where a\nsingle root node has a depth of 1. The average tree depth of CARMI\nis 2.1, while that of ALEX and B+ Tree are 2.5 and 7.3, respectively.\nComparing the two learned indexes, the main difference is in\nthe performance over OSMC/Face, in which CARMI achieves an\naverage speedup of 1.5 Ã—. The two datasets are real-world datasets\nwith highly non-linear local distribution (the CDFs can be found\nin Figure 11), which invalidates ALEXâ€™s strategy of storing data\npoints according to the predicted location, and causes the index to\ngrow deeper. Meanwhile, our hybrid construction algorithm can\nautomatically select suitable nodes to obtain better performance.\nSection 6.3 shows similar results on other real-world datasets as\nwell, making it clear that this is not a coincidence.\nIn summary, CARMI outperforms B+ Tree and can achieve simi-\nlar or better performance than ALEX under read-only workloads,\nand the gain is more evident on real-world datasets.\n4For the YCSB dataset, since CARMI uses the external leaf nodes, which only store a\npointer of the location of data points, the space cost of 11.4 MB only counts the space\nof the tree structure itself and does not include the space used by external data.6.2.2 Write-heavy Workload. CARMI achieves an average speedup\nratio of 2.2Ã—compared to B+ Tree and 3.0 Ã—compared to ALEX5on\nwrite-heavy workloads.\nIt is generally difficult for learned indexes to divide non-linear\ndatasets evenly, leading to a large gap between the numbers of\ndata points in each node. This results in additional expansion costs\nin ALEX during the insert operations. However, CARMI handles\nwell even under such scenarios, using only 0.42 Ã—time of ALEX\non lognormal/real-world datasets. As for space cost, CARMI uses\n1.2Ã—space compared to ALEX, and 0.8 Ã—compared to B+ Tree on\naverage.\nIn summary, CARMI has a better time efficiency and uses a\nsimilar amount of space compared to ALEX, and the gap is more\nsignificant on non-linear and real-world datasets.\n6.2.3 Other Read-Write Workloads. CARMI has a shorter access\ntime (2.04Ã—/2.03Ã—average speedup comapred to B+ Tree and ALEX\nrespectively) and use less space (0.79 Ã—on average compared to B+\nTree) for all other read-write workloads including range scan work-\nload. This shows that CARMI can better handle various workloads,\nespecially non-linear datasets.\n6.3 SOSD Results\nIn order to further test the performance of CARMI on real-world\ndatasets, we have integrated CARMI into the SOSD benchmark [ 24,\n36], a platform for testing learned index structures. We use all\nreal-world datasets in SOSD, each of which consists of 200 mil-\nlion unsigned 32-bit/64-bit integers: amzn is book sale popularity\ndata [ 4],face is the IDs of Facebook users [ 51],wiki is Wikipedia\narticle edit timestamps [ 5], and osmc is generated from a public\ndataset of Open Street Maps [ 2]. SOSD performs 10 million lookups\non each dataset, where the lookup keys are uniformly chosen from\nthe set of keys, and computes the average latency per lookup.\nThere are 10 baselines in SOSD and are categorized into four\ntypes: (a) three learned indexes : RMI, RS and ALEX; (b) three\ntraditional indexes : B+ Tree, FAST and ART; (c) three on-the-\nflyalgorithms that directly operate on a sorted array: BS, IS and\nTIP; (d) one auxiliary index that uses small auxiliary structures:\nRBS. Among them, FAST and ART do not support all the datasets\nas explained in SOSD and more details can be found in [ 24]. Ta-\nble 3 shows the average lookup time of indexes on all real-world\ndatasets in the SOSD benchmark, where the results with the short-\nest latency and those slightly slower (within 20ns) are bolded.\nWe also examine the distribution of datasets. As shown in Fig-\nure 11, a major difference between synthetic and real-world datasets\nis that synthetic datasets all have locally linear CDF, which is rare in\nreal-world datasets. The highly non-linear local CDFs of real-world\ndatasets make it difficult for prior solutions to predict the location of\nindividual data records accurately, leading to large-range searches\nin the last mile. In contrast, because CARMI is designed based on\nthe data partitioning view with a more cache-friendly leaf node\nlayout, it is less penalized by these highly non-linear parts and\nthus performs better in real-world datasets: CARMI can achieve an\naverage speedup of 1.21 Ã—even compared to a well-tuned RMI.\n5The insert of the YCSB protocol is to continuously insert new data points at the end\nof the dataset, and ALEX does not strictly follow this mode for insert in their paper.\nThus, our reported number is different compared to the values in their paper.\n12\n\nsynthetic datasetsreal-worlddatasetsFigure 11: The CDFs of Datasets in SOSD [24].\nTable 3: The Average Lookup Time (ns) on SOSD Platform.\nuint32 uint64\n(ns) amzn face amzn face wiki osmc\nCARMI 192.84 187.43 201.80 334.41 217.50 368.65\nRMI 265.43 274.53 266.54 334.54 222.43 402.32\nRS 280.03 362.54 296.61 436.63 218.43 412.39\nALEX 210.99 434.69 251.32 496.48 289.81 499.04\nRBS 325.43 312.39 385.96 334.73 335.75 529.06\nFAST 246.03 228.79 N/A N/A N/A N/A\nART N/A 182.36 N/A 391.76 N/A N/A\nB+Tree 529.43 524.54 601.23 592.43 608.42 599.43\nBS 1014.5 983.49 1015.1 961.93 1002.3 987.24\nTIP 731.97 880.12 750.89 1124.6 942.84 4773.8\nIS 3852.7 1007.7 4103.9 1494.8 6836.4 66474\nInterestingly, most learned indexes do not perform better than\ntraditional indexes on particular real-world datasets, as shown in Ta-\nble 3. Specifically, FAST takes only 237.41 ns on average on amzn32\nand face32, while RMI, RS, and ALEX take 1.14 Ã—/1.35Ã—/1.36Ã—aver-\nage lookup time, respectively. ART takes an average of 287.06 ns on\nface32 and face64, while RMI, RS, and ALEX take 1.06 Ã—/1.39Ã—/1.62Ã—,\nrespectively. Despite that, the lookup latency of CARMI is robust\nenough to be the shortest or very close to the shortest among all\nindexes for all datasets.\nWe also report the average space usage of each index in Table 4.\nThe space required by CARMI is comparable to that of traditional\nindexes with good performance, such as FAST and ART. Note that\nthe space cost of CARMI can be adjusted by users.\nTable 4: Average Space Usage (MB) on SOSD.\nCARMI RMI RS ALEX RBS\nuint32 3918.30 2471.65 2299.63 3336.74 2289.82\nuint64 5481.44 3143.31 3064.33 4430.83 3052.76\nFAST ART B+Tree BS TIP IS\nuint32 5120.00 4596.52 2657.31 2288.82 2288.82 2288.82\nuint64 N/A 5280.92 3540.52 3051.76 3051.76 3051.76\n6.4 Tradeoff between Time and Space\nOne additional advantage of our cost-based index construction\nalgorithm is that one can now flexibly choose the balance point\nbetween time and space cost. As shown in Figure 12, by tuning\nthe value of parameter ğœ†in Problem 2, one can reduce the memory\nusage of indexes at the cost of increased read access latency. The\ncurves of real-world datasets exhibit the characteristics of convex\nfunctions, but the optimal solution for each dataset depends on the\nactual data distributions. Therefore, we need to carefully tune ğœ†according to practical scenarios to find the optimal solution, so as\nto obtain a more desired tradeoff between time and space.\nFigure 12: The Tradeoff Between Time and Space Cost\n6.5 Cost of Construction\nThe construction of the index structure should only be performed\nperiodically when the database system has enough computation\nresources to spare (e.g., during night time when the frequency of\nthe incoming queries is low). As such, the time efficiency of the\nconstruction algorithm is not a major concern. Nevertheless, we\nstill want to have a basic understanding of its time cost.\nRecall that we have two algorithms (the greedy algorithm and\nthe DP algorithm) to construct a node. In order to achieve a balance\nbetween the construction time and the average lookup latency, we\ndefine a parameter ğ‘˜ğ·ğ‘ƒğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ in CARMI, and use the greedy\nalgorithm when the size of the sub-dataset is greater than the\nparameter. As shown in Figure 13, the construction of CARMI with\ndifferent values of ğ‘˜ğ·ğ‘ƒğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ can be finished within 0.3-3.2\nminutes, while B+ Tree and ALEX take 10s and 20s on average,\nrespectively. Although CARMI takes longer to build indexes than\nbaselines, it is generally acceptable for most practical scenarios.\nMoreover, the value of this parameter has a slight impact on the\naverage lookup latency, with the difference between a value of 96\nand 1024 being around 10 ns.\nTo conclude, in scenarios where construction time is more im-\nportant,ğ‘˜ğ·ğ‘ƒğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ can be tuned to reduce the construction\ntime. While in most practical scenarios, the default value can be\nused directly.\nB+ Tree ALEX CARMI_96 CARMI_256 CARMI_512 CARMI_768 CARMI_10240123Construction Time (min)\n200300400\nAverage Lookup Latency (ns)\nlognormal_build_time\nlognormal_latencyOSMC_build_time\nOSMC_latencyFace_build_time\nFace_latency\nFigure 13: Construction/Lookup Time of Indexes.\n13\n\nTable 5: Statistics on Different Root Fanouts.\nRootUniform Dataset Normal Dataset Lognormal Dataset OSMC Dataset\nTime # ofPercentageTime # ofPercentageTime # ofPercentageTime # ofPercentageFanout /ns Accesses /ns Accesses /ns Accesses /ns Accesses\n65536 301.64 6.82 5.14% 355.39 7.67 4.76% 372.72 8.73 3.32% 593.10 11.95 0.22%\n131072 280.58 5.72 7.28% 333.46 6.66 6.74% 337.73 7.69 4.68% 539.83 11.15 0.38%\n262144 254.16 4.62 10.36% 300.26 5.65 9.58% 308.69 6.64 6.62% 487.83 10.33 0.67%\n524288 212.34 3.52 14.66% 276.09 4.65 13.55% 283.86 5.58 9.35% 449.08 9.52 1.14%\n1048576 168.43 2.47 20.66% 232.51 3.68 19.05% 261.56 4.54 13.05% 410.11 8.69 1.89%\n2097152 152.71 1.53 28.64% 212.36 2.80 26.25% 230.68 3.53 17.99% 374.43 7.85 3.05%\n6.6 The Effect of Fanout\nIn order to show the effect of root fanout on average access time,\nwe use CARMI to simulate a two-layer static RMI with adjustable\nroot fanout. The root node, which is the linear regression model,\ndirectly manages the leaf nodes. Each leaf node is an external array\nleaf node. Table 5 shows the average access time and the average\nnumber of memory accesses required per query under different\nroot fanouts. We also count the percentage of data points that are\nstored in the same location as the location calculated by the linear\nmodel across all data points.\nAs demonstrated in Table 5, the average access time tends to be\nsmaller as the fanout increases. The gains are particularly evident\non non-linear datasets like the OSMC dataset. The benefits brought\nby a large fanout are mainly due to the following reasons: (a) It\nreduces the number of data points in each leaf node and thereby\navoids the occurrence of a wide range of binary searches, resulting\nin the reduced number of memory accesses. (b) It can simplify the\nsituation of lower-level nodes, so that more data points can have\ntheir actual storage positions to be consistent with the calculation\nresults of the linear model.\nIn general, a larger fanout of the root node can reduce the average\naccess time to some extent. Its contribution to the local efficiency\nof the root node is quantified in our entropy metric, which further\nvalidates our view of data partitioning.\n6.7 Performance Breakdown\nIn this section, we examine the performance improvements brought\nby each design.\n6.7.1 Detailed Performance Study. First, we investigate the con-\ntribution of each idea to the overall performance. We implement\n4 variants of CARMI to simulate indexes with different ideas and\nperform read-only workloads on 5 datasets, including 3 synthetic\ndatasets and 2 real-world datasets. A brief introduction of these\nvariants are as follows:\nâ€¢RMI : a two-layer RMI with an LR root node and 131072 external\nleaf nodes.\nâ€¢Greedy : a dynamic index constructed by the greedy algorithm.\nâ€¢Greedy cache-aware : a dynamic index equipped with the cache-\naware design and greedy algorithm.\nâ€¢CARMI : a complete CARMI equipped with all proposed ideas.\nAs shown in Figure 14, most of the advantages of the CARMI\nframework come from the data partitioning view, which enables\nthe greedy index to achieve an average speedup of 1.84 Ã—compared\nto RMI. Specifically, only the greedy algorithm is used to build the\nuniform normal lognormal osmc face640100200300400500time (ns)RMI\nGreedyGreedy cache-aware\nCARMIFigure 14: Latency of Four CARMIs.\nindex, so each node setting is determined by the entropy-based\nnotion of local efficiency. This means that nodes can have larger\nfanouts for smaller sub-datasets, reducing large-range searches in\nRMI. In addition, four inner nodes can be mixed to handle different\nsituations well to obtain better data partitioning effectiveness.\nNext, in the greedy cache-aware index, we replace leaf nodes\nwith CF array leaf nodes and cooperate with the prefetching mech-\nanism to examine the effect of our cache-aware design. Due to the\ninfluence of data distributions and different sub-datasets partitioned\nby the greedy algorithm, the cache-aware design shows different\neffects. Among them, the speedup for lognormal and face64 datasets\nis more significant. In summary, this index still obtains an aver-\nage speedup of 1.21 Ã—compared to the greedy index, which is the\ncontribution of cache-aware design.\nIf resources are sufficient, users can use the hybrid construction\nalgorithm consisting of the greedy algorithm and DP algorithm to\nbuild better indexes. According to the average access time when pa-\nrameterğ‘˜ğ·ğ‘ƒğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ is 1024, as shown in Figure 14, the complete\nCARMI can reduce the average access time by about 10ns.\n6.7.2 Model Characteristics. Next, we investigate the effect of us-\ning four inner nodes flexibly in the index. First, we conduct experi-\nments on the four inner nodes to show their respective characteris-\ntics. In these experiments, the root nodes are all piecewise linear\nnodes, but we only allow one kind of node to be used as inner nodes.\nIn addition, the results of CARMI are also shown in Table 6, where\nthe hybrid algorithm flexibly uses these four inner nodes.\nTable 6: Latency on Different Model Types.\nLatency /ns LR P. LR Hist BS CARMI\nUniform 136.8 143.5 141.3 145.8 136.6\nNormal 153.6 152.5 158.7 154.9 139.4\nLogn 157.5 155.0 155.8 153.8 139.9\nOSMC 415.8 405.3 413.32 374.9 293.4\nAs shown in Table 6, CARMI outperforms the indexes formed\nby using these nodes individually, indicating that the mixed-use of\n14\n\nthese four different types of models can effectively speed up the\nquery response. More specifically, on the one hand, LR nodes can\nhandle evenly distributed parts well, thus beating other models on\nthe uniform dataset. On the other hand, for the non-linear part,\nthe PLR and Hist nodes sacrifice some model calculation time in\nexchange for better data partitioning results. As for the extreme\ncase, BS node can handle it well. Although the model computation\ntakes more time, as shown in Table 2, it can partition the OSMC\ndataset more evenly, resulting in a shorter average access time.\nIn summary, it makes sense to combine different nodes together\nand can be easily achieved based on our fixed-size node design.\nCARMI benefits from our hybrid construction algorithm and cost\nmodel to select optimal node settings in different situations to get\nthe shortest average access time.\n6.7.3 Prefetching Mechanism. In order to evaluate the impact of\nthe prefetching mechanism, we conduct experiments with and\nwithout prefetching instructions on these four datasets and show\nthe time/space costs and the proportion of data blocks that are\nsuccessfully prefetched in Table 7. On these synthetic datasets, the\naverage success rate of the prefetching mechanism is 93.1%, and the\naverage access time is reduced by 63.1 ns. For real-world datasets,\nmost data points require at least two inner nodes to handle the data\ndistribution, so there is little gain in prefetching them at the root\nnode. As we discussed in Section 5.3, the prefetching mechanism\nrequires more space to better exert its effects.\nTable 7: Results With/Without Prefetch Mechanism.\nwith prefetch without prefetch\ntime/ns space/MB proportion time/ns space/MB\nUniform 114.0 1509.3 98.3% 185.8 1230.6\nNormal 124.6 1561.6 89.1% 184.4 1334.2\nLogn 126.2 2137.8 91.9% 183.7 1912.4\nOSMC 284.5 1557.86 2.5% 289.3 1533.4\n6.8 Robustness of CARMI\nIn this section, we simulate three groups of workload and data\ndistribution shifts to examine the robustness of CARMI.\nFigure 15: Robustness of Indexes in Different Situations.\n6.8.1 Workload Shift. First, we examine whether CARMI can cope\nwith read-only workload shifts. We construct indexes from evenly\ndistributed historical queries on OSMC/Face datasets, and query\nthem with 8 different access patterns: seven Zipfian distributions\n(ğ‘“(ğ‘¥)=1\nğ‘¥ğ›¼/Ãğ‘\nğ‘–=11\nğ‘–ğ›¼,ğ›¼âˆˆ{0.5,0.75,..., 1.5,2,3}) and a uniformdistribution as a baseline. The average access time of Zipfian indexes\ndecreases with the increase of ğ›¼and is smaller than that of the\nbaseline, as shown in Figure 15(a). The results show that CARMI is\nrobust and can handle read-only workload shifts well.\nThen, we construct indexes with historical queries under read-\nheavy and write-heavy workloads, respectively, and test them with\nwrite-heavy workloads. Although the indexes built on read-heavy\nworkloads require 1.1 Ã—the average time compared to indexes built\non write-heavy workloads, as shown in Figure 15(a), CARMI still\nexecutes queries quickly and is robust to workload shifts.\n6.8.2 Data Distribution Shift. Next, we demonstrate the robust-\nness of CARMI to data distribution shifts. We first use the uniform\ndataset with 67,108,864 key values to build indexes, and then per-\nform a write-heavy workload using 67,108,864 data points from\nOSMC dataset. Since inserted data points come from a different\ndataset, we can simulate the situation where the data distribution\ngradually changes from a uniform distribution to a non-linear dis-\ntribution. Figure 15(b) shows that although CARMI requires slightly\nmore average time than the index built on OSMC dataset, CARMI\ncan still maintain good performance in this situation. But we still\nrecommend that if data distributions change significantly, recon-\nstruction should be done in time for better performance when\nsufficient resources are available.\n6.9 Tree Structure\nTo understand the behavior of the index construction algorithm, we\nhave gathered statistics on the optimal indexes constructed by the\nalgorithm, and some of them are shown in Table 8. We follow the\nconvention that the depth of a trivial tree (with only root node) is\n1, and count depth on the leaf node level. The average is calculated\nwith respect to keys.\nTable 8: Statistics of Indexes Constructed by CARMI.\nread-only lognormal\nnode uniform normal write-heavy write-partial\nroot node LR P. LR P. LR P. LR\n# of children 1175653 1175653 1528348 1528348\n# of LR 2 835 4690 4697\n# of P. LR 0 4 8 8\n# of Hist 0 2 2 0\n# of BS 0 0 0 1\nArray 1175685 1320524 1784066 1767682\nMax Depth 3 3 4 4\nAvg Depth 2 2.03 2.07 2.07\nThe first two columns are statistics under a read-only workload.\nAs we can see, the algorithm constructs an average two-layer struc-\nture for uniform dataset under read-only workloads: the first layer\nis the LR root node and the second layer are the CF array leaf nodes.\nWhen performing a read query, we only need to access a single leaf\nnode and a single data block. Due to the distributional variation\ncaused by the sampling process, a small number of data points need\nan additional inner node.\n15\n\nFor other datasets, we can no longer construct a simple two-layer\nindex to handle the dataset as the data points are very concentrated\nin a specific range, which cannot be handled by a single leaf node.\nAs we can see, the root node setting changes depending on the data\ndistribution and CARMI chooses to use P. LR for all the other three\nscenarios. We also see that the algorithm generates new inner nodes\nto partition the data points further, and builds deeper subtrees in\nmore concentrated parts of the data points. Aside from the deep\npart, the algorithm maintains a shallow free structure in the other\nparts to guarantee that the average depth is small, achieving an\naverage depth of 2.03-2.07.\nAs we can see in Table 8, depending on the data distribution, the\nalgorithm will adaptively use the four types of inner nodes. For ex-\nample, in lognormal dataset under write-partial workload, CARMI\nconstructed 4697 LR nodes in the evenly distributed part, and 8P.\nLR nodes in the unevenly distributed part. In general, CARMI can ef-\nfectively handle different data distributions and diverse workloads,\nand construct a reasonable index structure accordingly. These re-\nsults also justify the decision of including multiple tree node types\nin CARMI, which enhances the flexibility of index structures.\n7 DISCUSSION AND FUTURE WORK\n7.1 CARMI in Disk/NVM\nIn our current implementation of CARMI, both inner nodes and\nleaf nodes are stored in memory. We can easily extend CARMI to\ninvolve disk operations. If we store data points on disk, we need\nto design a new type of leaf node that accesses a disk page instead\nof an in-memory data block. The corresponding time and space\ncosts of this new type of node should also reflect the latency of\ndisk operations. The current hybrid construction algorithm can be\ndirectly used without any change.\nMoreover, with the rapid development of non-volatile memory\n(NVM) [ 30], the capacity of memory can be vastly expanded while\nstill retaining decently high access speed. The difference in access\nlatency of heterogeneous storage devices can be characterized by\nmodifying the cost model of CARMI, and the construction algorithm\nwill be able to accommodate the new hardware properly.\n7.2 Other Data Types\nIn our implementation of CARMI, we only support numerical key\ntypes. It is possible to extend CARMI to handle other types as well.\nGenerally speaking, the existing RMI framework cannot readily\nhandle strings since the distribution of strings is hard to fit well\nby their current models. Our perspective based on data partition\ncan potentially address this difficulty. It is necessary to design new\nnodes for strings and make certain changes to the storage method.\nFor the hybrid construction algorithm and the overall cost model,\nwe need to make minor changes to them, but ideas and processes\nof the original algorithm can still be applied.\n7.3 Concurrent Operations\nThere have been many methods and implementations on concur-\nrency control of indexes, including B+ Tree [ 9,20,40,48]. The\noverall structure of CARMI is similar to B+ Tree, and it is also in\nthe form of a tree composed of multiple different nodes. Therefore,\nthe traditional concurrency control method can also be used onCARMI. We can use read/write locks to support concurrency. For\nlookup operations, we only need to obtain the read locks of the\nleaf node and its upper layer node when traversing the index. For\ninsert operations, we obtain the write locks on the nodes in the\naccess path. If the leaf node can handle the insert operation without\nthe need for split operation, the lock of the upper nodes can be\nreleased.\n8 RELATED WORK\nIn this section, we introduce some related works in the field of\ndatabase index.\nTraditional Indexes: Many researchers have optimized index\nstructures in the past decades and have proposed many indexes to\nachieve good performance, such as B-tree, B+ tree [ 7,8], T tree [ 31],\nbalanced B-tree [ 6] and red-black tree [ 12], etc. Since most indexes\nare stored in the main memory, CSS-tree [ 46] restricts node size to\nthe cache line size and eliminates child nodesâ€™ pointers to utilize\nthe cache and CSB+ tree [ 47] is then proposed to support updates.\nART [ 32] is an adaptive cardinality tree designed to reduce the\nnumber of cache misses. For further acceleration, pB+-tree [ 13]\nand FAST [ 22] use prefetching instructions and SIMD instructions,\nrespectively. Masstree [ 35] effectively handles possible binary keys\nof any length, including keys with long shared prefixes.\nLearned Indexes: Kraska et al. [ 27] propose a recursive model\nindex (RMI), which uses ML models instead of index structures rep-\nresented by B-tree. However, RMI does not support inserts. Fiting-\ntree [ 19] uses linear models to replace the leaf nodes of B-tree to\ncompress the index. PGM-index [ 18], a compressed learned index\nwith provable worst-case bounds, extends Fiting-tree and provides\nan optimal method to find the piecewise linear models. These two\nindexes support inserts by additional buffers, and the performance\nneeds to be improved. To support writes, ALEX [ 16] uses model-\nbased inserts and gapped array leaf nodes to make room for future\ninserts. However, ALEX requires large-range searches to accurately\nlocate data points, resulting in performance degradation on real-\nworld datasets. In this paper, we adopt a data partitioning view\nand use a cost-based hybrid construction algorithm to select suit-\nable node settings to build an updatable index. CARMI makes full\nuse of various types of nodes to handle different situations, thus\nmaintaining good performance on real-world datasets.\nCDFShop [ 38] uses Pareto analysis to find Pareto optimal config-\nuration for the static two-layer RMI. They use a parameter search\nstrategy, which is not suitable for dynamic structures due to the\nexponentially large number of configurations in the search space.\nOn the other hand, CARMI can automatically tune tree structures\nand model types at runtime through a new dynamic architecture\nand the cost-based algorithm.\nBesides, RadixSpline [ 25] focuses more on building indexes in\nsingle pass and PLEX [ 49] is built on RS and retains only one hyper-\nparameter for more convenient use. LIPP[ 53] uses entry types and\na conflict degree metric to decide tree node layout. Mitzenmacher\net al. [ 39] build a learning bloom filter that uses neural network\nmodels to predict whether keywords belong to a certain set. In\naddition, there are several other works that apply the idea of the\nlearned index to multi-dimensional indexes [ 17,41] and spatial\nquery processing [33, 43, 45, 52] to improve performance.\n16\n\nOther Applications of ML in Database: In other parts of the\ndatabase, many novel ideas that utilize ML techniques have also\nbeen proposed. Park et al. [ 44] applied ML techniques to approxi-\nmate query processing. They found that the answer to each query\nrevealed some degree of knowledge about the answer to another\nquery. Because their answers come from the same underlying distri-\nbution that produced the entire dataset. Therefore, it is possible to\nuse ML techniques to improve this knowledge and answer questions\nin a more organized manner instead of reading a lot of raw data.\nIn addition, several research teams [ 23,29,37,42,50,54] proposed\nthe use of reinforcement learning, CNN, DNN and other AI related\nmodels to predict cardinality and optimize queries.\nCumin et al. [ 15] proposed a rewriting technique of SQL query\nbased on ML. This technology can help data scientists formulate\nSQL queries and browse big data quickly and intuitively. At the\nsame time, the user input can be minimized without manual tuple\ndescription or marking.\nKraska et al. proposed a blueprint of a database that uses ML tech-\nniques in each part [ 26,28] to improve performance and discussed\nseveral ideas for designing new benchmarks [ 11] for learned sys-\ntems. ADCP [ 34], an active data collection platform, and HAL [ 34],\na novel active learning technique to gather data in the database, are\napplied to handle the problem of performance degradation of ML\nmodels in use.\n9 CONCLUSION\nThis paper conducts in-depth research on the basic framework of\nlearned indexes (RMI), and argues that the inflexibility of data par-\ntitioning is an important reason for the performance degradation\nof RMIs in practical scenarios. To address this issue, we propose to\nview RMI construction from a data partitioning view and propose\na general cache-aware RMI framework, called CARMI. Specifically,\nwe use the entropy metric to quantify the data partitioning effective-\nness and propose a new cost model to characterize the performance\nof individual tree nodes, which helps to design more robust indexes.\nFurthermore, CARMI is equipped with a new memory layout that\nis more cache-friendly and uses a hybrid algorithm to automatically\nconstruct index structures for different datasets and workloads\nwithout manual tuning. Experimental results show that CARMI\nhas an outstanding performance under a variety of datasets and\nworkloads, achieving an average of 2.2Ã—/1.9Ã—speedup compared\nto B+ Tree/ALEX, respectively, while using only 0.77Ã—the memory\nspace on average. This paper demonstrates that data partitioning is\nimportant for learned indexes during construction and that the new\ncost model can well characterize the performance of a single node,\nwhich is beneficial to improving the performance and flexibility of\nindexes. Finally, the CARMI framework is highly extensible and\nrobust, and can be applied to a wider range of scenarios if we design\ndifferent types of nodes for it.\nREFERENCES\n[1] [n.d.]. https://github.com/JiaoYiZhang/learned_index.\n[2] [n.d.]. https://registry.opendata.aws/osm/.\n[3] [n.d.]. https://panthema.net/2007/stx-btree/.\n[4][n.d.]. https://www.kaggle.com/ucffool/amazon-sales-rank-data-for-print-and-\nkindle-books.\n[5] [n.d.]. http://dumps.wikimedia.org.[6]Rudolf Bayer. 1972. Symmetric binary B-trees: Data structure and maintenance\nalgorithms. Acta informatica 1, 4 (1972), 290â€“306.\n[7]R Bayer and E McCreight. 1970. ORGANIZATION AND MAINTENANCE OF\nLARGE. (1970).\n[8]Rudolf Bayer and Edward McCreight. 2002. Organization and maintenance of\nlarge ordered indexes. In Software pioneers . Springer, 245â€“262.\n[9]Rudolf Bayer and Mario Schkolnick. 1977. Concurrency of operations on B-trees.\nActa informatica 9, 1 (1977), 1â€“21.\n[10] Brian Beavis and Ian Dobbs. 1990. Optimisation and stability theory for economic\nanalysis . Cambridge university press.\n[11] Laurent Bindschaedler, Andreas Kipf, Tim Kraska, Ryan Marcus, and Umar Farooq\nMinhas. 2021. Towards a Benchmark for Learned Systems. In 2021 IEEE 37th\nInternational Conference on Data Engineering Workshops (ICDEW) . IEEE, 127â€“133.\n[12] Joan Boyar and Kim S Larsen. 1994. Efficient rebalancing of chromatic search\ntrees. J. Comput. System Sci. 49, 3 (1994), 667â€“682.\n[13] Shimin Chen, Phillip B Gibbons, and Todd C Mowry. 2001. Improving index\nperformance through prefetching. ACM SIGMOD Record 30, 2 (2001), 235â€“246.\n[14] Brian F Cooper, Adam Silberstein, Erwin Tam, Raghu Ramakrishnan, and Russell\nSears. 2010. Benchmarking cloud serving systems with YCSB. In Proceedings of\nthe 1st ACM symposium on Cloud computing . 143â€“154.\n[15] Julien Cumin, Jean-Marc Petit, Vasile-Marian Scuturici, and Sabina Surdu. 2017.\nData exploration with sql using machine learning techniques.\n[16] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\net al.2020. ALEX: an updatable adaptive learned index. In Proceedings of the 2020\nACM SIGMOD International Conference on Management of Data . 969â€“984.\n[17] Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. 2020.\nTsunami: A learned multi-dimensional index for correlated data and skewed\nworkloads. arXiv preprint arXiv:2006.13282 (2020).\n[18] Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-index: a fully-dynamic\ncompressed learned index with provable worst-case bounds. Proceedings of the\nVLDB Endowment 13, 8 (2020), 1162â€“1175.\n[19] Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and Tim\nKraska. 2019. Fiting-tree: A data-aware index structure. In Proceedings of the 2019\nInternational Conference on Management of Data . 1189â€“1206.\n[20] Goetz Graefe and Harumi Kuno. 2011. Modern B-tree techniques. In 2011 IEEE\n27th International Conference on Data Engineering . IEEE, 1370â€“1373.\n[21] Robert M Gray. 2011. Entropy and information theory . Springer Science & Business\nMedia.\n[22] Changkyu Kim, Jatin Chhugani, Nadathur Satish, Eric Sedlar, Anthony D Nguyen,\nTim Kaldewey, Victor W Lee, Scott A Brandt, and Pradeep Dubey. 2010. FAST: fast\narchitecture sensitive tree search on modern CPUs and GPUs. In Proceedings of\nthe 2010 ACM SIGMOD International Conference on Management of data . 339â€“350.\n[23] Andreas Kipf, Thomas Kipf, Bernhard Radke, Viktor Leis, Peter Boncz, and\nAlfons Kemper. 2018. Learned cardinalities: Estimating correlated joins with\ndeep learning. arXiv preprint arXiv:1809.00677 (2018).\n[24] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2019. SOSD: A Benchmark for Learned\nIndexes. NeurIPS Workshop on Machine Learning for Systems (2019).\n[25] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2020. RadixSpline: a single-pass learned\nindex. In Proceedings of the Third International Workshop on Exploiting Artificial\nIntelligence Techniques for Data Management, aiDM@SIGMOD 2020, Portland,\nOregon, USA, June 19, 2020 . 5:1â€“5:5. https://doi.org/10.1145/3401071.3401659\n[26] Tim Kraska, Mohammad Alizadeh, Alex Beutel, Ed H Chi, Jialin Ding, Ani Kristo,\nGuillaume Leclerc, Samuel Madden, Hongzi Mao, and Vikram Nathan. 2019.\nSagedb: A learned database system. (2019).\n[27] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe case for learned index structures. In Proceedings of the 2018 International\nConference on Management of Data . 489â€“504.\n[28] Tim Kraska, Umar Farooq Minhas, Thomas Neumann, Olga Papaemmanouil,\nJignesh M Patel, Chris RÃ©, and Michael Stonebraker. 2021. ML-In-Databases:\nAssessment and Prognosis. IEEE Data Engineering Bulletin 44, 1 (2021), 3.\n[29] Sanjay Krishnan, Zongheng Yang, Ken Goldberg, Joseph Hellerstein, and Ion\nStoica. 2018. Learning to optimize join queries with deep reinforcement learning.\narXiv preprint arXiv:1808.03196 (2018).\n[30] Martijn HR Lankhorst, Bas WSMM Ketelaars, and Robertus AM Wolters. 2005.\nLow-cost and nanoscale non-volatile memory concept for future silicon chips.\nNature materials 4, 4 (2005), 347â€“352.\n[31] Tobin J Lehman and Michael J Carey. 1985. A study of index structures for\nmain memory database management systems . Technical Report. University of\nWisconsin-Madison Department of Computer Sciences.\n[32] Viktor Leis, Alfons Kemper, and Thomas Neumann. 2013. The adaptive radix tree:\nARTful indexing for main-memory databases. In 2013 IEEE 29th International\nConference on Data Engineering (ICDE) . IEEE, 38â€“49.\n[33] Pengfei Li, Hua Lu, Qian Zheng, Long Yang, and Gang Pan. 2020. LISA: A\nlearned index structure for spatial data. In Proceedings of the 2020 ACM SIGMOD\nInternational Conference on Management of Data . 2119â€“2133.\n17\n\n[34] Lin Ma, Bailu Ding, Sudipto Das, and Adith Swaminathan. 2020. Active learning\nfor ML enhanced database systems. In Proceedings of the 2020 ACM SIGMOD\nInternational Conference on Management of Data . 175â€“191.\n[35] Yandong Mao, Eddie Kohler, and Robert Tappan Morris. 2012. Cache craftiness\nfor fast multicore key-value storage. In Proceedings of the 7th ACM european\nconference on Computer Systems . 183â€“196.\n[36] Ryan Marcus, Andreas Kipf, Alexander van Renen, Mihail Stoian, Sanchit Misra,\nAlfons Kemper, Thomas Neumann, and Tim Kraska. 2020. Benchmarking Learned\nIndexes. Proc. VLDB Endow. 14, 1 (2020), 1â€“13.\n[37] Ryan Marcus and Olga Papaemmanouil. 2018. Towards a hands-free query\noptimizer through deep learning. arXiv preprint arXiv:1809.10212 (2018).\n[38] Ryan Marcus, Emily Zhang, and Tim Kraska. 2020. Cdfshop: Exploring and\noptimizing learned index structures. In Proceedings of the 2020 ACM SIGMOD\nInternational Conference on Management of Data . 2789â€“2792.\n[39] Michael Mitzenmacher. 2018. A model for learned bloom filters and related\nstructures. arXiv preprint arXiv:1802.00884 (2018).\n[40] C Mohan. 1989. ARIES/KVL: A key-value locking method for concurrency control\nof multiaction transactions operating on B-tree indexes . IBM Thomas J. Watson\nResearch Division.\n[41] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020. Learn-\ning multi-dimensional indexes. In Proceedings of the 2020 ACM SIGMOD Interna-\ntional Conference on Management of Data . 985â€“1000.\n[42] Jennifer Ortiz, Magdalena Balazinska, Johannes Gehrke, and S Sathiya Keerthi.\n2018. Learning state representations for query optimization with deep reinforce-\nment learning. In Proceedings of the Second Workshop on Data Management for\nEnd-To-End Machine Learning . 1â€“4.\n[43] Varun Pandey, Alexander van Renen, Andreas Kipf, Ibrahim Sabek, Jialin Ding,\nand Alfons Kemper. 2020. The case for learned spatial indexes. arXiv preprint\narXiv:2008.10349 (2020).\n[44] Yongjoo Park, Ahmad Shahab Tajik, Michael Cafarella, and Barzan Mozafari.\n2017. Database learning: Toward a database that becomes smarter every time.\nInProceedings of the 2017 ACM International Conference on Management of Data .\n587â€“602.\n[45] Jianzhong Qi, Guanli Liu, Christian S Jensen, and Lars Kulik. 2020. Effectively\nlearning spatial indices. Proceedings of the VLDB Endowment 13, 12 (2020), 2341â€“\n2354.\n[46] Jun Rao and Kenneth A. Ross. 1999. Cache Conscious Indexing for Decision-\nSupport in Main Memory. In Proceedings of the 25th International Conference\non Very Large Data Bases (VLDB â€™99) . Morgan Kaufmann Publishers Inc., San\nFrancisco, CA, USA, 78â€“89.\n[47] Jun Rao and Kenneth A Ross. 2000. Making B+-trees cache conscious in main\nmemory. In Proceedings of the 2000 ACM SIGMOD international conference on\nManagement of data . 475â€“486.\n[48] Venkathachary Srinivasan and Michael J Carey. 1991. Performance of B-tree\nconcurrency control algorithms. In Proceedings of the 1991 ACM SIGMOD inter-\nnational conference on Management of data . 416â€“425.\n[49] Mihail Stoian, Andreas Kipf, Ryan Marcus, and Tim Kraska. 2021. PLEX: Towards\nPractical Learned Indexing. arXiv preprint arXiv:2108.05117 (2021).\n[50] Ji Sun, Guoliang Li, and Nan Tang. 2021. Learned Cardinality Estimation for Simi-\nlarity Queries. In Proceedings of the 2021 International Conference on Management\nof Data . 1745â€“1757.\n[51] Peter Van Sandt, Yannis Chronis, and Jignesh M Patel. 2019. Efficiently Searching\nIn-Memory Sorted Arrays: Revenge of the Interpolation Search?. In Proceedings\nof the 2019 International Conference on Management of Data . 36â€“53.\n[52] Haixin Wang, Xiaoyi Fu, Jianliang Xu, and Hua Lu. 2019. Learned index for spatial\nqueries. In 2019 20th IEEE International Conference on Mobile Data Management\n(MDM) . IEEE, 569â€“574.\n[53] Jiacheng Wu, Yong Zhang, and Shimin Chen. 2021. Updatable Learned Index with\nPrecise Positions. Proceedings of the VLDB Endowment 14, 8 (2021), 1276â€“1288.\n[54] Peizhi Wu and Gao Cong. 2021. A Unified Deep Model of Learning from both Data\nand Queries for Cardinality Estimation. In Proceedings of the 2021 International\nConference on Management of Data . 2009â€“2022.\n[55] Huanchen Zhang, David G Andersen, Andrew Pavlo, Michael Kaminsky, Lin\nMa, and Rui Shen. 2016. Reducing the storage overhead of main-memory OLTP\ndatabases with hybrid indexes. In Proceedings of the 2016 International Conference\non Management of Data . 1567â€“1581.\nA DETAILS OF INNER NODES\nA.1 Linear Regression Node\nLinear regression node (LR node) is the type of inner node that uses\nlinear regression to determine the next branch. As mentioned in\nSection 5.1, the size of tree nodes must be 64 bytes. We only use a\nsimple linear model in this node to achieve the goal of predicting\nthe position as quickly as possible, and the remaining bytes areplaceholders. For a given input key, the next branch can be directly\nobtained by a few simple calculations and boundary processing.\nFigure 16 shows the memory layout and detailed computation steps\nof LR nodes for reference.\nLR node memory layout(64 bytes)typesize: # of child nodesstart_indexslopeinterceptminValuekey\nnode[idx].find(key)p = slopeÃ—(key -minValue) + interceptidx= start_index+ min(max(0, p), size -1)placeholder\nFigure 16: Details of LR Node\nA.2 Piecewise Linear Regression Node\nPiecewise linear regression node (P. LR node) uses a piecewise linear\nregression model to determine the next branch. More specifically,\nthe piecewise linear model contains ğ‘˜ğ¼ğ‘›ğ‘‘ğ‘’ğ‘¥ğ‘ğ‘¢ğ‘š+1segments, and\nwe store the coordinates of the segment end points in the P. LR\nnode. Due to the fixed number of segments, the training method\nof the PLR node, which is related to the segment point, is slightly\ndifferent from the least-squares method of the LR node. ğ‘˜ğ¼ğ‘›ğ‘‘ğ‘’ğ‘¥ğ‘ğ‘¢ğ‘š\nis calculated from the size of the key type to ensure that the total\nsize does not exceed 64 bytes, and placeholders fill the part less\nthan 64 bytes. For a 32-bit key, its value is 6, which means that there\nare a total of 8 linear models. When we traverse through a P. LR\nnode, we first find the segment to which the input key belongs, then\nuse the segment as a linear model to determine the next branch.\nThe memory layout and internal mechanism of a P. LR node are\ndemonstrated in Figure 17.\nP.  L R  n o d e  m e m o r y  l ay o u t(64 bytes)typesize: # of child nodesstart_indexkeys[kIndexNum+ 2]index [1â€¦ kIndexNum]a=indexidxâˆ’index[idxâˆ’1]keysidxâˆ’keys[idxâˆ’1]b=indexidxâˆ’aÃ—keys[idx]key\nidx=start_index+aÃ—key+b(index[0] = start_index,index[kIndexNum+1] = start_index+ size)node[idx].find(key)key = key -minValueidx= first i: keys[i]â‰¥keyminValueplaceholder\nFigure 17: Details of P. LR Node\nA.3 Binary Search Node\nBinary search node (BS node) uses a binary search procedure to\ndetermine the corresponding branch for the input key value. This\nis very similar to traversing through B+ Tree, the only difference\nis that the reference table in a BS node is determined during the\nindex construction procedure, and remains fixed afterward.\n18\n\nBS node memory layout(64 bytes)4 bytes4 bytestypesize: # of child nodesstart_indexkeys[56 / sizeof(Key)]56 bytes\nkey\nroot model\nmodel 2.1\nmodel 2.2\nmodel 2.n\nâ€¦\nmodel 3.2\nmodel 3.1\nmodel 3.n\nmodel 3.1\nâ€¦\narray / gapped array\nInner Nodes\np = /* output of model */\nidx = p * childNumber\nkey\nthe pointer to the corresponding branch\ninner node\nBinary search among them\n (size is childNumber) \nâ€¦\ninput_key\nThe branch to the next level\nï¼ˆ\nkey[i] \nâ‰¥ input_key\nï¼‰\nkey[i-1] < input_key\narray\ngapped array\narray / \ngapped array\nLR inner node\nNN inner node\nBS inner node\nHistogram \ninner node\nLeaf Nodes\nkey\nroot model\nmodel 2.1\nmodel 2.2\nmodel 2.n\nâ€¦\nmodel 3.2\nmodel 3.1\nmodel 3.n\nmodel 3.1\nâ€¦\nInner Nodes\nLeaf Nodes\nleaf node 1\nleaf node 1\nleaf node 1\nleaf node 1\nâ€¦\np\n = /* output of \nmodel */\nnext branch\ninner node\nidx\n = p * \nchildNumber\nkeyFigure 18: Details of BS Node\nEach BS node stores56\nğ‘ ğ‘–ğ‘§ğ‘’ğ‘œğ‘“(ğ‘˜ğ‘’ğ‘¦)key values internally, and di-\nvides the key range into56\nğ‘ ğ‘–ğ‘§ğ‘’ğ‘œğ‘“(ğ‘˜ğ‘’ğ‘¦)+1intervals. To determine\nwhich branch to go through, a binary search is performed among\nthe stored key values to locate the corresponding key value interval\ncovering the input key. Figure 18 shows the details of the BS nodes.\nA.4 Histogram Node\nHistogram nodes find the corresponding branch directly through a\nsimple one-step calculation and table look-up operation.\nâ€¦â€¦The index of the first bucket in each groupbaseusedto store indexes01/01/08 bitsoffsetusedto store  differenceğ‘–ğ‘‘ğ‘¥=ğ¢ğ§ğ­(ğ‘˜ğ‘’ğ‘¦âˆ’ğ‘šğ‘–ğ‘›ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’ğ‘‘ğ‘–ğ‘£ğ‘–ğ‘ ğ‘œğ‘Ÿ)keyHis node memory layout(64 bytes)typesize: # of child nodesstart_indexdivisorğ’ğ’ğ’…ğ’†ğ‘–ğ‘‘ğ‘¥.ğ‘“ğ‘–ğ‘›ğ‘‘(ğ‘˜ğ‘’ğ‘¦)minValuebase[kBaseLen] (unsigned char)offset[kBaseLen] (unsigned short)16 bitsplaceholder\nFigure 19: Details of Hist Node\nThe look-up table in Histogram node is stored very compactly\nusing two tables. Each byte in Base represents the index in child\nnodes of the first bit of the corresponding 16 bits in Offset . Each bit\nofOffset represents the difference (0 or 1) between the index of the\ncurrent bucket and the previous bucket, as shown in Figure 19. To\ndetermine the next branch, we only need to visit Base to get the\nbase index, and then count the number of bits in the Offset table,\nand finally add them together to get the index of the next branch.\nB DETAILS OF LEAF NODES\nB.1 CF Array Leaf Node\nIn cache-friendly array leaf nodes (CF node), the data points are\ncompactly stored in data blocks in a sequential manner. When\nsearching for data points in the CF array leaf nodes, we first search\nsequentially in the keywords of each block, and then search in the\nblock. When we need to insert a given data point, we must first\nfind the correct position, and then move all the data points by one\ncell to make room for the new data point. The memory layout and\ninternal mechanism of a CF leaf node are demonstrated in Figure 20.\nB.1.1 Insert Operation. In Section 5.2.2, we have introduced the\nprocedure of insert operations with two general mechanisms to\nmake room for the data points to be inserted. In the implementation\nCF node memory layout(64 bytes)4 bytes4 bytestype# of data blocksstart_indexpreviousLeaf4 bytesnextLeaf4 byteskeys[6]48 byteskeyblockIdx= start_index+ first i: keys[i] â‰¥keyidx= search in data[blockIdx]data point = data[blockIdx][idx]Figure 20: Details of CF Nodes\nof CARMI, we actually use three mechanisms in the CF leaf node:\nrebalance, expand, and split. Figure 21 shows the mechanisms of\nthem. Here we explain the rebalance mechanism that has not been\nintroduced.\nRebalanceL0# of Data: 4L1# of Data: 16L2# of Data: 16L3# of Data: 16Leaf nodeL0# of Data: 13L1# of Data: 13L2# of Data: 13L3# of Data: 13Leaf node\nFigure 21: Rebalance Mechanism\nWhen the data block to be inserted is full, but the next data\nblock is not full, we will insert the data point into the next block.\nIn order to keep all data points managed by this leaf node in order,\nthe inserted data point will be compared with the largest one in\nthe current block. The larger one is inserted into the next block,\nand the other is stored in the current block. If the next data block is\nalso saturated, the rebalance mechanism is triggered. The leaf node\ncollects all data points in its data blocks and reallocates them evenly\nto all these blocks. This mechanism can make the data distribution\nin data blocks more even, reducing a certain amount of time for\nsubsequent insert.\nB.2 External Array Leaf Node\nThe external array leaf node uses a simple linear model to predict\nthe location, and the pointer no longer points to the data array\nbut to the external location. The external array leaf node is used\nto create an index over a sorted array of data points (e.g., primary\nindexes in databases), and we use this type of node to create an\nindex for the YCSB dataset. Its access flow is shown in Figure 22.\nInevitably, the accuracy of the model prediction cannot reach\n100%, so we need a subsequent binary search step to find the\nexact position of the data record. For this purpose, we employ\nan additional parameter: the error ğœ€, whose value is determined\nduring the training process. For a given key value ğ‘¥, let us de-\nnote the actual location of the data point as ğ‘™ğ‘œğ‘(ğ‘¥), and the pre-\ndicted location of the data point as ğ‘ğ‘Ÿğ‘’ğ¼ğ‘‘ğ‘¥(ğ‘¥). Then, when look-\ning for the data point matching ğ‘¥, depending on the relationship\nbetween these two terms, there are essentially two cases: either\nğ‘™ğ‘œğ‘(ğ‘¥)âˆˆ[ğ‘ğ‘Ÿğ‘’ğ¼ğ‘‘ğ‘¥(ğ‘¥)âˆ’ğœ€\n2,ğ‘ğ‘Ÿğ‘’ğ¼ğ‘‘ğ‘¥(ğ‘¥)+ğœ€\n2], orğ‘™ğ‘œğ‘(ğ‘¥)is outside of this\nrange. The time cost of data access is also different for these two\ncases. In the former case, we need to perform a binary search over\nthe range of[ğ‘ğ‘Ÿğ‘’ğ¼ğ‘‘ğ‘¥(ğ‘¥)âˆ’ğœ€\n2,ğ‘ğ‘Ÿğ‘’ğ¼ğ‘‘ğ‘¥(ğ‘¥)+ğœ€\n2], while in the latter case\nthe range of the binary search is the entire array.\n19\n\nExternal leaf nodeâ€¦â€¦Binary search within the rangekeyidx= model prediction (LR model)\nReturnğ‘–ğ‘‘ğ‘¥âˆ’!\"ğ‘–ğ‘‘ğ‘¥ğ‘–ğ‘‘ğ‘¥+!\"Find data successfullyTrueFalseBinary search in the entire arrayFigure 22: Details of External Array Leaf Nodes\nB.2.1 The Optimal Value of Error Parameters. Essentially, we want\nto choose the error parameter ğœ€that minimizes the average time\ncost of data access. The method for finding its optimal value is given\nbelow. Let us denote for each data point its true position as ğ‘¦ğ‘–, the\npredicted value as ğ‘ğ‘–, and their difference ğ‘‘ğ‘–=ğ‘¦ğ‘–âˆ’ğ‘ğ‘–. With these\nnotations, we can compute the average time cost of data access as\nin Equation (10).\nğ´ğ‘£ğ‘”=1\nğ‘›[|{ğ‘–:|ğ‘‘ğ‘–|â‰¤ğœ€\n2}|Â·âŒŠlog2ğœ€âŒ‹+|{ğ‘–:|ğ‘‘ğ‘–|>ğœ€\n2}|Â·âŒŠlog2ğ‘›âŒ‹](10)\nThen, we only need to substitute different ğœ€values into Equa-\ntion (10) to find the minimum value of ğ´ğ‘£ğ‘”, where the range of ğœ€is\nfrom 0 to max|ğ‘ğ‘–âˆ’ğ‘¦ğ‘–|.\nğœ€=arg minğœ€|{ğ‘–:|ğ‘‘ğ‘–|â‰¤ğœ€\n2}|Â·âŒŠlog2ğœ€âŒ‹+|{ğ‘–:|ğ‘‘ğ‘–|>ğœ€\n2}|Â·âŒŠlog2ğ‘›âŒ‹\nC DETAILS OF ROOT NODES\nC.1 Linear Regression Root Node\nThe linear regression root node is a very simple root node consisting\nof a linear regression model. In addition to the basic members, only\ntwo linear model parameters need to be stored. We only need one\nmodel prediction and the boundary condition processing to get the\nindex of the next node. The details of LR root node is shown in\nFigure 23.\nLR root node memory layout4 bytes4 bytestypePlaceholderstart_indexchildNum4 bytesa8 bytesb8 byteskeyp = a Ã—(key â€“minValue) + bidx= start_index+ min(max(0, p), childNum-1)node[idx].find(key)minValuesizeof(Key)\nFigure 23: Details of LR Root Nodes\nC.2 Piecewise Linear Regression Root Node\nThe P. LR root node uses a piecewise linear regression model with\nup to five segments to allocate the dataset to the child nodes. It\ndiffers from the P. LR inner nodes only in the number of segments.\nWe use a dynamic programming algorithm similar to the prefetchmodel to find the optimal segmentation points and the number of\nsegments, thereby maximizing the entropy of the node. Since the\nroot node is in the cache, we do not limit its size here. For the P. LR\nroot node, we find the first breakpoint greater than or equal to the\nkey value, and then use the corresponding model parameters for\ncalculation and boundary processing, as shown in Figure 24.\nP.  L R  r o o t  n o d e  m e m o r y  l ay o u t4 bytes4 bytestypePlaceholderstart_indexchildNum4 bytespoint[1â€¦4]  (k, bound)24 byteskey\nnode[idx].find(key)theta [5] (a, b)40 bytespoint[0] = (-DBL_MAX, -1)key = key -minValuekey â‰¤point[4].kidx = first i: point[i].k â‰¥keyright = point[idx].boundleft = point[idx-1].bound+1idx= 5right = childNumâ€“1left = point[4].bound-1p = theta[idx-1].aÃ—key + theta[idx-1].bidx= start_index+ min(max(left, p), right)TrueFalse\nminValue4 bytes\nFigure 24: Details of P. LR Root Nodes\nD PARAMETER SETTINGS\nThis section describes the default parameter settings in our ex-\nperiments. The default value of the parameter ğœ†used to tradeoff\nbetween the time cost and space cost is 0.03, but we adjusted this\nparameter in the experiments according to the datasets and the\nneeds of the experiments. The size of each data block is represented\nasğ‘˜ğµğ‘™ğ‘œğ‘ğ‘˜ğ‘†ğ‘–ğ‘§ğ‘’ , and the default value is 256 bytes, and the maximum\ncapacity of external leaf nodes is 512. The parameter ğ‘˜ğ·ğ‘ƒğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘\nis used to switch between the dp and greedy algorithms and its\nvalue is 512. When a leaf node needs to perform a split operation, we\nneed to replace it with an inner node and ğ‘˜ğ·ğ‘–ğ‘£ğ‘–ğ‘‘ğ‘’ğ‘‘ğ‘ğ‘œğ‘‘ğ‘’ğ‘ğ‘¢ğ‘š leaf\nnodes with a default value of 16. The parameter ğ‘˜ğ¿ğ‘’ğ‘ğ‘“ğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘\nis90, which means that when the number of data points is less\nthan 90, the algorithm will directly construct a leaf node instead of\nchoosing a better one from leaf nodes and inner nodes, thus saving\na certain amount of space cost and construction time. Note that\nthis parameter must be smaller than ğ‘˜ğ¿ğ‘’ğ‘ğ‘“ğ‘€ğ‘ğ‘¥ğ¶ğ‘ğ‘ğ‘ğ‘ğ‘–ğ‘¡ğ‘¦ . Table 9\nlists these parameters with default values for a quick view.\nE PROOF OF THEOREM 3.1\nProof. W.l.o.g, let ğ‘€1,...,ğ‘€ğ¼be the set of inner nodes, and\nğ‘€ğ¼+1,...,ğ‘€ğ¾be the set of leaf nodes. Let ğ‘›ğ‘–be the number of data\npoints in node ğ‘€ğ‘–, andğ¶ğ‘–be the set of child nodes of ğ‘€ğ‘–. Then, we\ncan substitute the definition of ğ‘ƒ(ğ‘€ğ‘–)andğ»(ğ‘€ğ‘–)into Theorem 3.1:\nÃğ¾\nğ‘–=1ğ‘ƒ(ğ‘€ğ‘–)ğ»(ğ‘€ğ‘–)=âˆ’ğ¼âˆ‘ï¸\nğ‘–=1ğ‘›ğ‘–\nğ‘›âˆ‘ï¸\nğ‘—âˆˆğ¶ğ‘–ğ‘›ğ‘—\nğ‘›ğ‘–log2ğ‘›ğ‘—\nğ‘›ğ‘–+ğ¾âˆ‘ï¸\nğ‘–=ğ¼+1ğ‘›ğ‘–\nğ‘›log2ğ‘›ğ‘–\n=âˆ’ğ¼âˆ‘ï¸\nğ‘–=1âˆ‘ï¸\nğ‘—âˆˆğ¶ğ‘–ğ‘›ğ‘—\nğ‘›log2ğ‘›ğ‘—\nğ‘›ğ‘–+ğ¾âˆ‘ï¸\nğ‘–=ğ¼+1ğ‘›ğ‘–\nğ‘›log2ğ‘›ğ‘–\nFor each leaf node ğ‘€ğ‘™, let the traverse path from the root to\nğ‘€ğ‘™beğ‘…ğ‘œğ‘œğ‘¡â†’ğ‘€ğ‘–1â†’ğ‘€ğ‘–2â†’...â†’ğ‘€ğ‘–ğ‘˜â†’ğ‘€ğ‘™. We can define\nğ‘ƒğ‘–as the following set: ğ‘ƒğ‘–=\b\n(ğ‘…ğ‘œğ‘œğ‘¡,ğ‘€ğ‘–1),(ğ‘€ğ‘–1,ğ‘€ğ‘–2),...(ğ‘€ğ‘–ğ‘˜,ğ‘€ğ‘™)\t\n.\nThen, the double summation can be rearranged according to all the\n20\n\nTable 9: The Settings of Parameters\nParams Explanation Default Value\nğœ† several different values were used in experiments 0.03\nğ‘˜ğµğ‘™ğ‘œğ‘ğ‘˜ğ‘†ğ‘–ğ‘§ğ‘’ the fixed size of a data block 256\nğ‘˜ğ¿ğ‘’ğ‘ğ‘“ğ‘€ğ‘ğ‘¥ğ¶ğ‘ğ‘ğ‘ğ‘ğ‘–ğ‘¡ğ‘¦(1) the maximum capacity of CF nodes 96\n(2) the maximum capacity of external nodes 512\nğ‘˜ğ·ğ‘ƒğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ used to switch between DP and greedy algorithms 512\nğ‘˜ğ·ğ‘–ğ‘£ğ‘–ğ‘‘ğ‘’ğ‘‘ğ‘ğ‘œğ‘‘ğ‘’ğ‘ğ‘¢ğ‘š the number of new leaf nodes in split mechanism 16\nğ‘˜ğ¿ğ‘’ğ‘ğ‘“ğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ the size threshold in DP transition 90\npaths from the root node to the leaf nodes:\nâˆ’Ãğ¼\nğ‘–=1Ã\nğ‘—âˆˆğ¶ğ‘–ğ‘›ğ‘—\nğ‘›log2ğ‘›ğ‘—\nğ‘›ğ‘–=ğ¼âˆ‘ï¸\nğ‘–=1âˆ‘ï¸\nğ‘—âˆˆğ¶ğ‘–âˆ‘ï¸\nğ‘™:(ğ‘€ğ‘–,ğ‘€ğ‘—)âˆˆğ‘ƒğ‘™ğ‘›ğ‘™\nğ‘›log2ğ‘›ğ‘–\nğ‘›ğ‘—\nwhere we splitğ‘›ğ‘—\nğ‘›according to whether the ğ‘™-th leaf node is visited\nto obtainÃ\nğ‘™:(ğ‘€ğ‘–,ğ‘€ğ‘—)âˆˆğ‘ƒğ‘™ğ‘›ğ‘™\nğ‘›.\nThen we sort and rearrange them according to each path to the\nleaf node:\nâˆ’Ãğ¼\nğ‘–=1Ã\nğ‘—âˆˆğ¶ğ‘–ğ‘›ğ‘—\nğ‘›log2ğ‘›ğ‘—\nğ‘›ğ‘–=ğ¾âˆ‘ï¸\nğ‘™=ğ¼+1ğ‘›ğ‘™\nğ‘›âˆ‘ï¸\n(ğ‘€ğ‘–,ğ‘€ğ‘—)âˆˆğ‘ƒğ‘™log2ğ‘›ğ‘–\nğ‘›ğ‘—Finally, we merge the above equation with the leaf node part\nÃğ¾\nğ‘–=1ğ‘ƒ(ğ‘€ğ‘–)ğ»(ğ‘€ğ‘–)=ğ¾âˆ‘ï¸\nğ‘™=ğ¼+1ğ‘›ğ‘™\nğ‘›(âˆ‘ï¸\n(ğ‘€ğ‘–,ğ‘€ğ‘—)âˆˆğ‘ƒğ‘™log2ğ‘›ğ‘–\nğ‘›ğ‘—+log2ğ‘›ğ‘™)\nand simplify the equation in the path order:\nÃğ¾\nğ‘–=1ğ‘ƒ(ğ‘€ğ‘–)ğ»(ğ‘€ğ‘–)=ğ¾âˆ‘ï¸\nğ‘™=ğ¼+1ğ‘›ğ‘™\nğ‘›(log2ğ‘›\nğ‘›ğ‘™+log2ğ‘›ğ‘™)\n=ğ¾âˆ‘ï¸\nğ‘™=ğ¼+1ğ‘›ğ‘™\nğ‘›log2ğ‘›\n=log2ğ‘›\nwhereÃğ¾\nğ‘™=ğ¼+1ğ‘›ğ‘™is equal toğ‘›. â–¡\n21",
  "textLength": 117811
}