{
  "paperId": "f3d577444895a138333328b558acfbfa9fb6fe51",
  "title": "Updatable Learned Index with Precise Positions",
  "pdfPath": "f3d577444895a138333328b558acfbfa9fb6fe51.pdf",
  "text": "Updatable Learned Index with Precise Positions\nJiacheng Wu\nTsinghua University\nBeijing, China\nwu-jc18@mails.tsinghua.edu.cnYong Zhang*\nTsinghua University\nBeijing, China\nzhangyong05@tsinghua.edu.cnShimin Chen*\nChinese Academy of Sciences\nBeijing, China\nchensm@ict.ac.cn\nJin Wang\nUCLA\nLos Angeles, USA\njinwang@cs.ucla.eduYu Chen\nTsinghua University\nBeijing, China\ny-c19@mails.tsinghua.edu.cnChunxiao Xing\nTsinghua University\nBeijing, China\nxingcx@tsinghua.edu.cn\nABSTRACT\nIndex plays an essential role in modern database engines to accel-\nerate the query processing. The new paradigm of â€œlearned indexâ€\nhas significantly changed the way of designing index structures\nin DBMS. The key insight is that indexes could be regarded as\nlearned models that predict the position of a lookup key in the\ndataset. While such studies show promising results in both lookup\ntime and index size, they cannot efficiently support update oper-\nations. Although recent studies have proposed some preliminary\napproaches to support update, they are at the cost of scarifying the\nlookup performance as they suffer from the overheads brought by\nimprecise predictions in the leaf nodes.\nIn this paper, we propose LIPP , a brand new framework of learned\nindex to address such issues. Similar with state-of-the-art learned\nindex structures, LIPP is able to support all kinds of index oper-\nations, namely lookup query, range query, insert, delete, update\nand bulkload. Meanwhile, we overcome the limitations of previ-\nous studies by properly extending the tree structure when dealing\nwith update operations so as to eliminate the deviation of location\npredicted by the models in the leaf nodes. Moreover, we further\npropose a dynamic adjustment strategy to ensure that the height\nof the tree index is tightly bounded and provide comprehensive\ntheoretical analysis to illustrate it. We conduct an extensive set of\nexperiments on several real-life and synthetic datasets. The results\ndemonstrate that our method consistently outperforms state-of-the-\nart solutions, achieving by up to 4Ã—for a broader class of workloads\nwith different index operations.\nPVLDB Reference Format:\nJiacheng Wu, Yong Zhang, Shimin Chen, Yu Chen, Jin Wang, Chunxiao\nXing. Updatable Learned Index with Precise Positions. PVLDB, 14(8):\nXXX-XXX, 2021.\ndoi:10.14778/3457390.3457393\nPVLDB Artifact Availability:\nThe source code, data, and/or other artifacts have been made available at\nhttp://vldb.org/pvldb/format_vol14.html.\n*Yong Zhang and Shimin Chen are the corresponding authors.\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license. For any use beyond those covered by this license, obtain permission by\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 14, No. 8 ISSN 2150-8097.\ndoi:10.14778/3457390.34573931 INTRODUCTION\nTree indexes are essential components to support efficient data\naccess in modern database engines. Many different index structures\nhave been proposed to meet the requirement of various access\npatterns and workloads. The recent study on Learned Index [22]\nhas opened up a new way to construct the index for sorted data.\nGiven a dataset, Learned Index utilizes machine learning models\nto learn the data distribution and predict the position of a lookup\nkey in the dataset. It could be realized via supervised learning\ntechniques by using the Cumulative Distribution Function (CDF)\nof the dataset for training. Since the models might be inaccurate\nfor the predicted positions, the learned index needs to search the\nlookup key in a bounded range around the predicted positions.\nRecent comprehensive experimental studies [ 20,31] demonstrate\nthat learned indexes achieve significant advantages over traditional\nindex structures in terms of high performance and low memory\nfootprint.\nNevertheless, the original Learned Index [22] only supports\nlookup on read-only datasets and fails to handle update operations\nwhich are essential in index structures. To address this problem,\ntwo recent studies namely ALEX [11] and PGM [12] propose several\nstrategies to add support for updating the index. However, their\nsupport for updates is at the expense of extra search overhead for\nlookup operations. PGM uses the logarithmic method and thus\nneeds to find keys in a series of subtrees instead of a single one.\nWhatâ€™s more, ALEX even has unbounded â€œlast mileâ€œ search cost\nin the leaf nodes since it does not provide any threshold of errors\ncaused by the wrong predictions. As illustrated in the example\nshown in Figure 1, the lookup time of ALEX is dominated by the\nsearch in the leaf nodes, which in the worst case would need linear\nor binary search on the whole node. Consequently, they may suffer\nfrom the poor lookup performance. Moreover, the update opera-\ntions on these indexes also incur huge amounts of elements shifting.\nThese overheads are all brought by the imprecise predictions of\nlearned models.\nTo tackle with these issues brought by inaccurate predictions, we\npropose the Updatable Learned Index with Precise Positions ( LIPP ),\na brand new learned index to provide efficient support for a full\nset of index operations, namely lookup query, range query, insert,\nupdate, delete and bulkload. A distinct advantage of LIPP is that it\neliminates the â€œlast mileâ€ search in the leaf nodes, thereby bounding\nthe lookup cost to tree height and significantly improving index\nperformance. The key-to-position mapping is precise in LIPP . If\n\nmultiple keys are mapped into the same position, a new child node\nwill be created to hold the keys. To bound the height of the tree\nindex, we propose kernelized linear models that are able to evenly\ndistribute the mapping of newly inserted elements to positions, and\na light-weight adjustment strategy to keep the tree height bounded.\nWe also provide theoretical guarantee that the height of LIPP is\nbounded to ğ‘‚(logğ‘)withğ‘as the cardinality of dataset.\nMKey \nRoot \nNode \nM\nM MM\nM... \nM M\nM\nexponential \nsearch M\nposition M\nGapped/nobreakspace \nArray\n... \nFigure 1: Extreme Case for ALEX [11]\nWe conduct an extensive set of experiments on both real world\nand synthetic datasets with workloads of mixed operations. The\nexperimental results show that LIPP outperforms state-of-the-art\nlearned index structures by an obvious margin. Specifically, LIPP\nachieves 2.8Ã—and6.3Ã—better lookup performance than ALEX and\nPGM for read-only workload. In terms of write-only and read-write\nworkloads, LIPP outperforms ALEX by up to 2.9Ã—, and has similar\nindex size under most settings.\nThe rest of this paper is organized as follows: Section 2 intro-\nduces the background for learned index series. Section 3 describes\nthe structure and properties of LIPP . Section 4 displays the details\nof operations on LIPP . Section 5 provides theoretical analysis on\nthe performance of LIPP . Section 6 presents the experimental re-\nsults. Section 7 discusses issues related to concurrency and new\nhardware accelerators. Section 8 reviews the related work. Finally,\nthe conclusion is made in Section 9.\n2 PRELIMINARIES\n2.1 Tree-Based Index\nIn this paper, we aim at devising a learned index structure that can\nsupport all operations in traditional tree-based indexes. Given ğ‘¥\nandğ‘¦as keys and ğ‘£as value, an index ğ‘†supports the operations:\n(1)ğ‘šğ‘’ğ‘šğ‘ğ‘’ğ‘Ÿ(ğ‘¥)=TRUE ifğ‘¥âˆˆğ‘†, FALSE otherwise.\n(2)ğ‘™ğ‘œğ‘œğ‘˜ğ‘¢ğ‘(ğ‘¥)returns the element with key ğ‘¥âˆˆğ‘†(if any), NIL\notherwise.\n(3)ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’(ğ‘¥,ğ‘¦)returns the elements whose keys âˆˆ[ğ‘¥,ğ‘¦]\n(4)ğ‘–ğ‘›ğ‘ ğ‘’ğ‘Ÿğ‘¡(ğ‘¥,ğ‘£)inserts the element with key ğ‘¥and valueğ‘£toğ‘†.\n(5)ğ‘‘ğ‘’ğ‘™ğ‘’ğ‘¡ğ‘’(ğ‘¥)removes the element with key ğ‘¥fromğ‘†.\n(6)ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’(ğ‘¥,ğ‘›ğ‘£)is implemented with the ğ‘‘ğ‘’ğ‘™ğ‘’ğ‘¡ğ‘’(ğ‘¥)followed\nbyğ‘–ğ‘›ğ‘ ğ‘’ğ‘Ÿğ‘¡(ğ‘¥,ğ‘›ğ‘£)with a new key or value.\n(7)ğ‘ğ‘¢ğ‘™ğ‘˜ğ‘™ğ‘œğ‘ğ‘‘(ğ‘¥[ğ‘])is used in practice to index ğ‘elements at\ninitialization or for rebuilding index.\nIn this paper, we assume that the keys are unique. It is easy for\nindexes to support duplicate keys, e.g. maintaining a pointer to anoverflow list. A typical example of tree-based structure is B+Tree , a\ndynamic height-balanced tree. The overhead for lookup operations\nonB+Tree consists of traversing from root to leaf nodes and binary\nsearch in nodes. Besides, the node size of B+Tree is also limited.\nThe large node size will result in the huge cost of searching keys\ninside nodes.\n2.2 Learned Index\nGiven a key, Learned Index [22] maps it to the position in the\nsorted array of keys, which thus is considered as a trained model.\nLearned Index builds a hierarchy of models with fixed height called\nRecursive Model Index (RMI). In order to locate a key, the higher-\nlevel model predicts the model at the next level by learning the CDF\nof the dataset. And the leaf-level model outputs the final prediction\nfor the position of the key. Finally, Learned Index applies extra\nbinary search to correct the wrong predictions on the sorted array\nbased on the given error bound ğœ–. Due to the error bound, Learned\nIndex has much larger nodes that hold many more elements but\nwithout incurring drastically higher search cost compared with\nB+Tree . The larger node size of Learned Index results in fewer levels\nof index, which significantly saves the cost of index traversal. This is\nthe main reason why Learned Index outperforms B+Tree in lookup\noperations. However, Learned Index cannot support updates.\nSome recent studies aim at supporting updates. PGM [37] uses\nlinear models and separates the keys in different linear segments\nwith given error bound. Unlike Learned Index , each model or seg-\nment of PGM specifies the first key covered by the segment. In\nthis way, PGM recursively constructs index on the sorted keys of\nsegments in low level. To locate a key, PGM applies the similar tra-\nversal process as Learned Index except that the predicted position\nat each level is required to be corrected immediately. To support in-\nsertions, PGM employs the idea of LSM-tree [ 38]. Concretely, keys\nare separated into subsets with different sizes and PGM indexes are\nbuilt over those subsets. Each insertion of a key is required to find a\nseries of non-empty sets, merge them into a large subset and build\na new PGM index on the large one. Unfortunately, it decreases the\nlookup performance heavily since it has to search for a given key\nin all components with different sizes. As shown in Table 1, the\nlookup operations of PGM costğ‘‚(log2ğ‘).\nMeanwhile, ALEX separates elements in many data nodes shown\nin Figure 1. The lookup procedure of ALEX is similar to Learned\nIndex , i.e., recursively locating the models in the next level. How-\never, ALEX uses exponential search in the leaf node to locate the\ngiven key, which is due to that ALEX is not bounded by any pre-\ndiction error threshold. The insert procedure of ALEX first utilizes\nthe lookup procedure to locate a proper position for inserted key\nin the nodes and then tries to the insert the key into that position.\nALEX also shifts the elements to make the gap for the inserted\nkey when there is a key in that position. As shown in Table 1, the\nshifting procedure costs ğ‘‚(logğ‘š)on average due to the underlying\nlayout gapped array of each data node, but can have ğ‘‚(ğ‘š)cost in\nthe worst case, where ğ‘šis the max node size. Furthermore, ALEX\nperforms node expansion or split when a leaf node does not have\nenough free space for an insertion, but fails to provide upper bound\nof time complexity on the performance for insertions. As shown in\nTable 1, our solution LIPP avoids such problems and thus has lower\ncomplexity and average latency.\n\nTable 1: Summary of Complexity and Average Latency Comparisons among Different Indexes\nLIPP ALEX PGM Learned B+Tree\nLookupComplexity ğ‘‚(logğ‘)ğ‘‚(logğ‘+logğ‘š)1ğ‘‚(log2ğ‘)ğ‘‚(logğ‘)ğ‘‚(logğ‘)\nLatency624.23ns 68.92ns 151.53ns 139.09ns 237.94ns\nInsertComplexity ğ‘‚(log2ğ‘)ğ‘‚(log2ğ‘+logğ‘š)2ğ‘‚(log2ğ‘+logğ‘)3â€” ğ‘‚(logğ‘)\nLatency670.93ns 204.94ns 217.17ns â€” 1114.19ns\nSearch Range (Leaf) ğ‘‚(1)4ğ‘‚(ğ‘š) ğ‘‚(ğœ–)5ğ‘‚(ğœ–)ğ‘‚(ğ‘š)\nSearch Range (Non-Leaf) ğ‘‚(1) ğ‘‚(1) ğ‘‚(ğœ–) ğ‘‚(1)ğ‘‚(ğ‘š)\n1ğ‘šis the max number of slots in nodes.2ğ‘‚(log2ğ‘)+ğ‘‚(ğ‘š)for extreme cases.3Need existence checking to prevent duplicated\nkeys.4ğ‘‚(1)means no need to search in nodes.5ğœ–is the prediction error threshold.6The latency is conducted on YCSB .\n2.3 Monotonically Increasing Models\nAMonotonically Increasing function is one defined on ordered sets\nthat preserves the given order. Given a monotonically increasing\nmodelM, for any pair of keys ğ‘˜ğ‘–andğ‘˜ğ‘—, the property in Equation (1)\nholds:\nğ‘˜ğ‘–â‰¤ğ‘˜ğ‘—â†’M(ğ‘˜ğ‘–)â‰¤M(ğ‘˜ğ‘—) (1)\nIt is essential for the learned index structures to satisfy this\nproperty so as to support range queries. A range query first locates\nthe position of the start key, then scans forward until it reaches the\nend key. However, if the model is not monotonically increasing, it\nmay map the start key to a position after the predicted position of\nthe end key, causing incorrect results for the range query. In this\npaper, we follow previous studies [ 11,22] to employ linear models\nfor prediction, which satisfy the monotonically increasing property.\nIn our approach, the learned model Mis a kernelized linear\nfunction . We need to store the kernel function Gand two model\nparameters, i.e., the slope ğ´and the intercept ğ‘. Given a key ğ‘˜, the\nmodel computes the entry position in the node with an array of ğ¿\nentries as Equation (2):\nM(ğ‘˜)=ï£±ï£´ï£´ ï£²\nï£´ï£´ï£³0âŒŠğ´Â·G(ğ‘˜)+ğ‘âŒ‹<0\nğ¿âˆ’1âŒŠğ´Â·G(ğ‘˜)+ğ‘âŒ‹â‰¥ğ¿\nâŒŠğ´Â·G(ğ‘˜)+ğ‘âŒ‹ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’(2)\nThe only requirement of the kernel function is that it must be\nmonotonically increasing. Examples of kernel functions Ginclude\nthe exponential function eğ‘¥, the logarithm function ln(ğ‘¥), the linear\nfunctionğ‘¥, the quadratic function ğ‘¥2when keys are positive, and\neven polynomial functionsÃğ‘ğ‘–ğ‘¥ğ‘–, etc. In many real-world appli-\ncations, we find the linear function behaves well. Accordingly, we\nuse the linear function as the default kernel function, unless other-\nwise stated. However, if the distribution of a target data set is not\nregular, our approach allows users to take advantage of the prior\nknowledge of dataset and to specify a kernel function to improve\nthe performance of model. Due to the space limitation, we leave\nthe experiments related to the effect of specified kernel functions\nof our methods on synthetic dataset in Appendix [44] B.1.\n3 THE LIPP INDEX\n3.1 Overview\nThe core idea of LIPP is to avoid inaccurate predictions, i.e., all\npredictions made by models are exact. With precise positions,\nthe significant and inevitable overheads, including in-node search\nfor lookup, element shifting for insertion, can be eliminated. Never-\ntheless, to reach this goal we need to overcome the following two\nchallenges:Firstly, predictions for two different keys might coincide\nat one position . We call elements with such keys as conflicting\nelements. To resolve conflicts, ALEX shifts the elements to make a\ngap for the newly inserted ones, which causes inaccurate predic-\ntions. Instead, we preserve the precise predictions by replacing the\ncurrent element with a new node that accommodates these two\nconflicting elements (cf. Section 4.1 and 4.2).\nThe second challenge is derived from the first one: Simply cre-\nating new nodes for conflicting keys would cause the tree\nheight increasing without bound , thus hurting the performance\nfor both index lookup and insert operations. To deal with this chal-\nlenge, we propose a novel adjustment strategy which redistributes\nkeys in a subtree to control the height of the subtree. It can wisely\nselect the appropriate subtree and determine when and how to\nadjust the subtree to reduce the tree height (cf. Section 3.3 and 4.3).\nWe also provide theoretical guarantee for the tree height, along\nwith the complexity of lookup and insert operations (cf. Section 5).\nAs a result, we expect LIPP to be faster than both existing learned\nindex structures and traditional B+Tree s for all index operations,\nwhile the index size of LIPP is comparable to those of existing\nlearned index structures.\n3.2 Structure\nM\nM M\nM MM\n\u000f\u000f\u000f \u000f\u000f\u000f \u000f\u000f\u000f keyNODE\nDATA\nNULL\nFigure 2: Structure of LIPP\nThe overall structure of LIPP is shown in Figure 2. Each node\ncontains a modelM, an array of entries E, and a bit vector of entry\ntypes. There are three types of entries in a node:\n(1)NULL : The entry is an unused slot (gap). All entries are\ninitialized as NULL and used to store new keys. After an\ninsertion to a slot completes, the type of the entry is changed\ntoDATA .\n(2)DATA : The entry contains one element with its key and\npayload. When the payload is too large, we store a pointer\n(or an offset) to the payload in the entry.\n\n(3)NODE : The entry points to a child node in the next level,\nthus helping form a tree structures. When a new element is\ninserted into a DATA entry, a child node is created to hold the\ntwo conflicting entries. The current entry becomes NODE\nand its content is the pointer to the child node.\nThe size of all three types of entries is 16 bytes. A DATA entry\nconsists of an 8B key and an 8B payload or pointers to payload,\nwhile a NODE entry contains a child node pointer. The bit vector\nspecifies the type of each entry with two bits. For the ğ‘–ğ‘¡â„entry,\nthe2ğ‘–ğ‘¡â„bit indicates whether the entry is NULL or not, and the\n(2ğ‘–+1)ğ‘¡â„shows the entry type, i.e. DATA orNODE . Since the bit\nvector is a light-weighted structure, the node size could be bounded\nby a predefined hyper-parameter (e.g., 16MB).\nUnlike existing learned index structures, LIPP does not distin-\nguish leaf nodes (DataNode) from non-leaf nodes (InnerNode). In-\nstead, all nodes are treated equally. Entry types are used to guide the\nindex operations to choose different actions. Details are illustrated\nin Section 4.\nLIPP is a sorted index. In every node, the entries (either the DATA\nentry or entries in the subtree pointed to by the NODE entry) in\nthe array are sorted in key order. That is, keys in the left part of the\narray are less than keys in the right part whether they are in the\nnode directly or in subtrees. This property is achieved by simply\nrequiring all models in all nodes monotonically increase.\n3.3 Metrics for Evaluating the Learned Model\nIn this section, we start with finding a metric to evaluate the quality\nof a learned model for indexing.\nGiven a modelM, two keysğ‘˜ğ‘–andğ‘˜ğ‘—conflict if and only if\nM(ğ‘˜ğ‘–)==M(ğ‘˜ğ‘—). We propose the notion of conflict degree in\nDefinition 3.1 to capture the maximum number of conflicts at any\nposition in the entry array:\nDefinition 3.1. For a node with ğ¿entries and a learned model\nM(ğ‘˜), the conflict degree ğ‘‡Mof the node is:\nğ‘‡M=max\nğ‘™âˆˆ[0,ğ¿âˆ’1]|{ğ‘˜âˆˆK|M(ğ‘˜)==ğ‘™}| (3)\nwhereKis the set of keys contained in this node, and ğ‘™is a possible\nposition ranging from 0toğ¿âˆ’1.\nAccording to such a definition, the better the model is, the lower\nthe conflict degree it has. The conflict degree of the ideal model is 1.\nThe worst model maps all keys to the same position with a conflict\ndegree of|K|. Our goal is to find a model to achieve as low conflict\ndegree as possible.\nWe observe that there exists an upper bound for the minimum\nğ‘‡M, i.e.âˆƒM,ğ‘‡Mâ‰¤âŒˆğ‘\n3âŒ‰whereğ‘is the number of keys in K, i.e.\nğ‘=|K|. However, theâŒˆğ‘\n3âŒ‰may not be the tightest upper bound in\nmany cases. Thus, our goal is to find a best model M=ğ´G(ğ‘˜)+ğ‘\nwith the minimum conflict degree ğ‘‡M. We need to first identify\ncertain properties that the model Mshould satisfy under a given\nconflict degree ğ‘‡. TheMneeds to map keys to positions between\n0andğ¿âˆ’1. If a key is mapped to a position beyond the range, we\nset its position to either 0orğ¿âˆ’1. However, since the position 0or\nğ¿âˆ’1contains at most ğ‘‡elements, the values of parameters ğ´and\nğ‘should follow Condition (4).\u001ağ´Â·G(ğ‘˜ğ‘–)+ğ‘â‰¥1,âˆƒğ‘–â‰¤ğ‘‡\nğ´Â·G(ğ‘˜ğ‘âˆ’1âˆ’ğ‘—)+ğ‘<ğ¿âˆ’1,âˆƒğ‘—â‰¤ğ‘‡(4)Algorithm 1 :FMCD (K,ğ¿)\nInput :K: the collection of keys, ğ¿: the number of entries\nOutput :M: the model, ğ‘‡: the conflict degree\nbegin1\nğ‘–=0;ğ‘‡=1;ğ‘=|K|; 2\nğ‘ˆğ‘‡=G(ğ‘˜ğ‘âˆ’1âˆ’ğ‘‡)âˆ’G(ğ‘˜ğ‘‡)\nğ¿âˆ’2; 3\nwhileğ‘–â‰¤ğ‘âˆ’1âˆ’ğ‘‡do 4\nwhileğ‘–+ğ‘‡<ğ‘andG(ğ‘˜ğ‘–+ğ‘‡)âˆ’G(ğ‘˜ğ‘–)â‰¥ğ‘ˆdo 5\nğ‘–=ğ‘–+1; 6\nifğ‘–+ğ‘‡â‰¥ğ‘then 7\nbreak ; 8\nğ‘‡=ğ‘‡+1; 9\nğ‘ˆğ‘‡=G(ğ‘˜ğ‘âˆ’1âˆ’ğ‘‡)âˆ’G(ğ‘˜ğ‘‡)\nğ¿âˆ’2; 10\nM.ğ´=1\nğ‘ˆğ‘‡; 11\nM.ğ‘=ğ¿âˆ’(M.ğ´Â·(G(ğ‘˜ğ‘âˆ’1âˆ’ğ‘‡)+G(ğ‘˜ğ‘‡)))\n2; 12\nreturn{M,ğ‘‡}; 13\nend14\nSince the keys are sorted, the former formula means there exists a\nkeyğ‘˜ğ‘–in the firstğ‘‡elements ofKthat is not mapped to position 0. In\nother words, at most ğ‘‡elements go to position 0. The latter formula\ncan be derived in a similar way. Based on the above conditions, we\nfurther obtain the constraint on ğ´as Condition (5).\nğ´â‰¤max\nğ‘–,ğ‘—ğ¿âˆ’2\nG(ğ‘˜ğ‘âˆ’1âˆ’ğ‘—)âˆ’G(ğ‘˜ğ‘–)=ğ¿âˆ’2\nG(ğ‘˜ğ‘âˆ’1âˆ’ğ‘‡)âˆ’G(ğ‘˜ğ‘‡)(5)\nOnce the value of ğ´satisfies Condition (5), it is easy to find a\nvalueğ‘(e.g.,ğ‘=1âˆ’ğ´Â·G(ğ‘˜ğ‘‡)) to satisfy Condition (4). Therefore,\nCondition (5) and Condition (4) are essentially equivalent.\nBesides, according to the definition of ğ‘‡, any consecutive ğ‘‡+1ele-\nments should not conflict in the same position. Specifically, whether\ntwo keys conflict with each other can be checked by Lemma 3.2.\nLemma 3.2.âˆ€ğ‘˜,ğ‘˜â€², ifğ´â‰¥|(G(ğ‘˜)âˆ’G(ğ‘˜â€²))|âˆ’1,ğ‘˜andğ‘˜â€²will be\nmapped to different positions.\nProof.|ğ´Â·(G(ğ‘˜)âˆ’G(ğ‘˜â€²))|=|(ğ´Â·G(ğ‘˜)+ğ‘)âˆ’(ğ´Â·G(ğ‘˜â€²)+ğ‘)|â‰¥ 1.\nThenâŒŠğ´Â·G(ğ‘˜)+ğ‘âŒ‹â‰ âŒŠğ´Â·G(ğ‘˜â€²)+ğ‘âŒ‹. Thus,ğ‘˜andğ‘˜â€²are mapped\nto different positions. â–¡\nTherefore,ğ´â‰¥|(G(ğ‘˜ğ‘–)âˆ’G(ğ‘˜ğ‘–+ğ‘‡))|âˆ’1ensuresğ‘˜ğ‘–andğ‘˜ğ‘–+ğ‘‡do\nnot conflict. We can further have the Condition (6).\nğ´â‰¥ max\nğ‘–âˆˆ[0,ğ‘âˆ’1âˆ’ğ‘‡]1\nG(ğ‘˜ğ‘–+ğ‘‡)âˆ’G(ğ‘˜ğ‘–)(6)\nFinally, based on Condition (5)and(6), we conclude that the value\nofğ‘‡must follow the Condition (7).\nğ¿âˆ’2\nG(ğ‘˜ğ‘âˆ’1âˆ’ğ‘‡)âˆ’G(ğ‘˜ğ‘‡)â‰¥ max\nğ‘–âˆˆ[0,ğ‘âˆ’1âˆ’ğ‘‡]1\nG(ğ‘˜ğ‘–+ğ‘‡)âˆ’G(ğ‘˜ğ‘–)(7)\nSimilarly, the value of ğ‘should satisfy the Condition (8):\nğ¿âˆ’1âˆ’ğ´Â·G(ğ‘˜ğ‘âˆ’1âˆ’ğ‘‡)â‰¥ğ‘â‰¥1âˆ’ğ´Â·G(ğ‘˜ğ‘‡) (8)\n3.4 Efficient Algorithm for Deciding the Model\nAccording to the conclusion in Condition (7), the next step is to\ndesign an efficient algorithm to find the minimum ğ‘‡satisfying:\nâˆ€ğ‘–âˆˆ[0,ğ‘âˆ’1âˆ’ğ‘‡]:G(ğ‘˜ğ‘–+ğ‘‡)âˆ’G(ğ‘˜ğ‘–)â‰¥ğ‘ˆğ‘‡ (9)\nwhereğ‘ˆğ‘‡=G(ğ‘˜ğ‘âˆ’1âˆ’ğ‘‡)âˆ’G(ğ‘˜ğ‘‡)\nğ¿âˆ’2. Note that ğ‘ˆğ‘‡monotonically de-\ncreases asğ‘‡increases as the keys are in ascending order.\nThe naive algorithm to compute the minimum ğ‘‡is simply enu-\nmerating the value of ğ‘‡from 0toâŒˆğ‘\n3âŒ‰and checking the validity of\n\nğ‘‡by Condition (9). However, the time complexity is ğ‘‚(ğ‘2), which\nis too expensive for index operations.\nTo solve this problem, we propose the Fastest Minimum Con-\nflict Degree ( FMCD ) algorithm to achieve linear complexity for\ncomputing the minimum ğ‘‡and the corresponding model M.\nIn Algorithm 1, we begin by considering the minimum ğ‘‡as1\n(line 3) and setting ğ‘ˆğ‘‡asğ‘ˆ1. Then we traverse the sorted keys in K\nand check whether the current ğ‘‡satisfies the condition G(ğ‘˜ğ‘–+ğ‘‡)âˆ’\nG(ğ‘˜ğ‘–)â‰¥ğ‘ˆğ‘‡(line 5-6). If a key ğ‘˜ğ‘–breaks the condition, ğ‘‡is not\ngood. We increment the ğ‘‡, update the ğ‘ˆğ‘‡, and continue the check\nfrom the failed key ğ‘˜ğ‘–(line 9-10). Not until all keys are checked\ncan we jump out of the loop (line 7-8). At this moment, we get the\ncorresponding modelâ€™s parameters based on the computed conflict\ndegree (line 11-12).\nTheorem 3.3 shows the optimum of the FMCD algorithm:\nTheorem 3.3. Theğ‘‡0returned from Algorithm 1 is precisely the\nminimumğ‘‡satisfying Condition (9).\nCondition\tbreaks\tat\nCondition\tis\tsatisfied\tfor\tall\nFigure 3: Cases for Proof\nsketch. Figure 3 shows the relations of G(ğ‘˜ğ‘–+ğ‘‡),G(ğ‘˜ğ‘–), and\nğ‘ˆğ‘‡whenğ‘‡is changed to ğ‘‡0for the key ğ‘˜ğ‘—, which results in the\ncorrectness and the minimum of ğ‘‡0. See Appendix [ 44] A.1 for\ndetails. â–¡\nThe Complexity: Finally, we analyze the time and space complex-\nity of Algorithm 1. In fact, the algorithm visits a key only once if\nthe key passes the conditions in line 5. When the check fails, the\nalgorithm increments ğ‘‡in line 9 and visits the key again. Since\nğ‘‡<ğ‘‚(ğ‘), the number of additional checks due to failed checks is\nlimited toğ‘‚(ğ‘). Therefore, the time complexity of Algorithm 1 is\nğ‘‚(ğ‘+maxğ‘‡)=ğ‘‚(ğ‘). As for the space complexity, the algorithm\nneedsğ‘‚(1)space forğ‘ˆğ‘‡.\n4 OPERATIONS OF LIPP\nIn this section, we describe the procedures for index operations and\nthe algorithms to dynamically adjust the index structure.\n4.1 Lookup and Range Queries\nIndex read operations include looking up a single key and obtaining\na range of keys. We introduce these two types of operations for our\nproposed LIPP .Algorithm 2 :Lookup (T,ğ‘˜)\nInput :T: the LIPP index,ğ‘˜: the lookup key\nOutput :ğ‘–ğ‘ ğ¹ğ‘œğ‘¢ğ‘›ğ‘‘ : indicates whether ğ‘˜is found, ğ‘’: the\nentry containing ğ‘˜if found\nbegin1\nğ‘›â†the root node ofT; 2\nğ‘’â†ğ‘›.E[ğ‘›.M(ğ‘˜)]; 3\nwhileğ‘¡ğ‘¦ğ‘ğ‘’(ğ‘’)==NODE do 4\nğ‘›â†the node pointed to from entry ğ‘’; 5\nğ‘’â†ğ‘›.E[ğ‘›.M(ğ‘˜)]; 6\nifğ‘¡ğ‘¦ğ‘ğ‘’(ğ‘’)==DATA then 7\nğ‘˜â€²â†the key in entry ğ‘’; 8\nifğ‘˜==ğ‘˜â€²then 9\nreturnâŸ¨ğ‘‡ğ‘Ÿğ‘¢ğ‘’,ğ‘’âŸ©; 10\nreturnâŸ¨ğ¹ğ‘ğ‘™ğ‘ ğ‘’,ğ‘’âŸ©; 11\nend12\n4.1.1 Lookup Queries. To look up a key, we start at the root node\nof the index structure, and use the model of the current node to com-\npute the location of the given key in its entry array E. Depending\non the entry type, we have different actions.\nIf the type of the entry is NODE , we then follow the pointer to\nthe child node at the next level. Otherwise, we reach the lowest\nlevel of the traversal path. If it is a NULL entry, the key to lookup\ndoes not exist in our index. If it is a DATA entry, we should be\ncareful in this case and cannot directly return the current entry. We\ncheck whether the search key is the same as the key stored in the\nentry because different keys can be mapped to the same position.\nOnly when it is consistent can we return the current entry as the\nlookup result.\nThe above procedure is listed in Algorithm 2. Note that the\nreturned result contains the entry even when the key is not found.\nThis is used in the insert operation.\nIt is important to note that there is no need to use extra search\nsteps in each node in our lookup procedure. This is because the\nmodel computes the precise position for the key in each node.\nTherefore, the cost of the lookup query in our index is only ğ‘‚(â„),\nwhereâ„is the height of the index tree.\n4.1.2 Range Queries. A range query is used to find the elements\nwhose keys are in the specified range, which is an important op-\neration in database engines. Given the range [ğ‘¢,ğ‘£], we first find\nthe position of the start key ğ‘¢by performing the point lookup\nprocedure described above.\nSince the index is monotonic, we scan forward until reaching the\nend keyğ‘£. If we reach the end of the entry array containing the start\nkey before reaching the end key, we traverse back to the previous\nlevel and continue the scan. When reaching a NODE entry during\nthe scanning process, we follow the pointer to scan the associated\nchild node. We can further use the bit vector to quickly skip over\ngaps.\nOne concern is that during the scanning process, we may perform\nunnecessary comparisons between an entry key and the end key.\nWe address this concern by first locating the positions of the end\nkeyğ‘£in all levels and then simply visiting the elements up to the\ncomputed positions without key comparisons.\n\nAlgorithm 3 :Insert (T,ğ‘˜,ğ‘)\nInput :T: the LIPP index,ğ‘˜: the key,ğ‘: the payload\nbegin1\nEntryğ‘’â†Lookup (T,ğ‘˜).ğ‘’; 2\nifğ‘¡ğ‘¦ğ‘ğ‘’(ğ‘’)==NULL then 3\nğ‘¡ğ‘¦ğ‘ğ‘’(ğ‘’)â† DATA ; 4\nğ‘’â†âŸ¨ğ‘˜,ğ‘âŸ©; 5\nelse 6\nğ‘˜â€²â†the key in entry ğ‘’; 7\nğ‘›â†a new node trained on ğ‘˜,ğ‘˜â€²; 8\nğ‘¡ğ‘¦ğ‘ğ‘’(ğ‘’)â† NODE ; 9\nğ‘’â†the pointer to node ğ‘›; 10\nfornodeğ‘›â€²in the reversed traversal path do 11\nAdjust (T,ğ‘›â€²); 12\nend13\n4.2 Index Inserts\nApart from locating the keys, the index should handle insert opera-\ntions, while maintaining the strict order guarantee for the index. In\nour case, it is easy to achieve this requirement.\nIn the insert algorithm, the logic to reach the entry in the final\nlevel is the same as in the lookup algorithm described above. When\nthe entry returned by the lookup query is a gap, i.e. NULL , we\nsimply insert the new element into this gap. But when a conflict\nhappens, we need to replace the elements with a pointer to the new\nnode containing the new element as well as the original one. At\nthe same time, we change the entry type to NODE to indicate the\nexistence of the node.\nDuring the experiments, we find the creation of new nodes with\ntwo elements is a common operation. This can exert negative influ-\nence on the performance. To improve the performance, we construct\nour own memory pool for small node allocation and recycling.\nAfter finishing the insert operation, we follow the search tra-\nversal path in the opposite direction and judge whether the nodes\nshould trigger the adjustment to keep the index height bounded, as\nwill be detailed in Section 4.3.\n4.3 Adjustment Strategy on the Model\nIn this section, we describe the adjustment strategy to keep the tree\nheight bounded. Before doing the actual adjustment, we first update\nand check the statistics of the nodes in the traversal path after an\ninsert operation. We trigger the adjustment on a chosen node when\ncertain conditions are satisfied. The adjustment procedure is shown\nin Algorithm 4. To understand the adjustment strategy, we focus\non two core issues: When to adjust and How to adjust?\n4.3.1 When to adjust? InLIPP , we propose two main criteria, as\nshown in Algorithm 4, to decide whether to trigger adjustment on\na node:\nFirstly, the number of inserted elements in the subtree rooted at\nnodeğ‘›should be at least ğ›½times as large as the elements in the\nsubtree rooted at ğ‘›in the last adjustment . Concretely, the condition\ncan be formulated as the expansion ratioğ‘›.ğ‘’ğ‘™ğ‘’ğ‘šğ‘’ğ‘›ğ‘¡ _ğ‘›ğ‘¢ğ‘š\nğ‘›.ğ‘ğ‘¢ğ‘–ğ‘™ğ‘‘ _ğ‘›ğ‘¢ğ‘šâ‰¥ğ›½for\nnodeğ‘›, whereğ‘›.ğ‘ğ‘¢ğ‘–ğ‘™ğ‘‘ _ğ‘›ğ‘¢ğ‘š is the number of elements specified to\nğ‘›by the adjustment (line 13 in Algorithm 5) and ğ‘›.ğ‘’ğ‘™ğ‘’ğ‘šğ‘’ğ‘›ğ‘¡ _ğ‘›ğ‘¢ğ‘š\nis incremented for each insertion. ğ›½is set to 2by default. The\ndefault value is derived from the logarithm methods of PGM , i.e.,Algorithm 4 :Adjust (T,ğ‘›)\nInput :T: the LIPP index,ğ‘›: the node to adjust\nbegin1\nğ‘›.ğ‘’ğ‘™ğ‘’ğ‘šğ‘’ğ‘›ğ‘¡ _ğ‘›ğ‘¢ğ‘š=ğ‘›.ğ‘’ğ‘™ğ‘’ğ‘šğ‘’ğ‘›ğ‘¡ _ğ‘›ğ‘¢ğ‘š+1; 2\nifthe insertion conflicts then 3\nğ‘›.ğ‘ğ‘œğ‘›ğ‘“ğ‘™ğ‘–ğ‘ğ‘¡ _ğ‘›ğ‘¢ğ‘š=ğ‘›.ğ‘ğ‘œğ‘›ğ‘“ğ‘™ğ‘–ğ‘ğ‘¡ _ğ‘›ğ‘¢ğ‘š+1; 4\nifğ‘›.ğ‘’ğ‘™ğ‘’ğ‘šğ‘’ğ‘›ğ‘¡ _ğ‘›ğ‘¢ğ‘šâ‰¥ğ›½Â·ğ‘›.ğ‘ğ‘¢ğ‘–ğ‘™ğ‘‘ _ğ‘›ğ‘¢ğ‘šand 5\nğ‘›.ğ‘ğ‘œğ‘›ğ‘“ğ‘™ğ‘–ğ‘ğ‘¡ _ğ‘›ğ‘¢ğ‘š\nğ‘›.ğ‘’ğ‘™ğ‘’ğ‘šğ‘’ğ‘›ğ‘¡ _ğ‘›ğ‘¢ğ‘šâˆ’ğ‘›.ğ‘ğ‘¢ğ‘–ğ‘™ğ‘‘ _ğ‘›ğ‘¢ğ‘šâ‰¥ğ›¼then\nKâ† the collection of keys contained in the subtree 6\nrooted atğ‘›;\nğ‘›â€²â†BuildPartialTree (K); 7\nReplace node ğ‘›withğ‘›â€²inT; 8\nend9\nalways triggering the merge process for a series of indexes when\nthe inserted elements is twice as much as elements contained in\nthe largest index. This criterion effectively reduces the frequency\nof adjustments and leads to low adjustment complexity even in the\nworst cases (cf. Section 5).\nSecondly, the conflict ratio between the number of conflicts and\ninsertions in the subtree rooted at node ğ‘›exceeds a given threshold\nğ›¼, i.e.ğ‘›.ğ‘ğ‘œğ‘›ğ‘“ğ‘™ğ‘–ğ‘ğ‘¡ _ğ‘›ğ‘¢ğ‘š\nğ‘›.ğ‘’ğ‘™ğ‘’ğ‘šğ‘’ğ‘›ğ‘¡ _ğ‘›ğ‘¢ğ‘šâˆ’ğ‘›.ğ‘ğ‘¢ğ‘–ğ‘™ğ‘‘ _ğ‘›ğ‘¢ğ‘šâ‰¥ğ›¼. This is because conflicts\nresult in the creation of new small nodes and possibly increase the\ntree height. Consequently, conflicts tend to hurt the performance\nfor both lookup and insert operations. Thus, we should trigger\nadjustment if we have seen too many conflicts in the subtree rooted\natğ‘›. By default, we set the threshold ğ›¼=0.1to achieve a better\nbalanced tree. To further choose proper parameters, we conduct\nthe parameter analysis for ğ›¼andğ›½in Appendix [44] B.2.\nIn addition, we observe that adjustment on a node with either\ntoo many or too few elements downgrades the performance.\nFor the first observation, the extreme case is that when the root\nnode is triggered to adjust, all elements should participate in the\nadjustment, which may not be acceptable for a very large tree.\nTherefore, we should restrict the number of elements participating\nin the adjustment. When the node size is close to the predefined\nthreshold (e.g. 16MB) which means that the node has already con-\ntained enough entries and elements, the current node should be\nfixed and not participate the adjustment afterwards. As for the\nsecond observation, when the number of elements of a node is\nsmall, a few insertions will trigger adjustment in the early stage.\nTo eliminate such case, the nodes with less than 64elements are\nrequired not to trigger the adjustment.\n4.3.2 How to adjust? In Algorithm 4, if a node ğ‘›satisfies the above\nconditions, the adjustment is triggered. First of all, we collect all\nelements in the subtree rooted at node ğ‘›by sequential traversal\n(line 6). The keys of the collected elements are in order because the\ntree is sorted. Then, we build a partial tree structure on the elements\n(line 7). Finally, we update the pointer of the original node to point\nto the root of the new tree structure (line 8). Figure 4 illustrates\nthe node adjustment procedure. The main procedure of adjustment\nis to build a proper tree structure based on the collection of keys.\nUtilizing the properties of models and conflicts in Section 3.3, the\noptimal tree structure should consist of nodes with best models\nhaving the minimum conflict degree. The resulting models distrib-\nute the elements in the entry array to minimize conflicts, thereby\n\nM (15/24)\nM (4/7)\u000f\u000f\u000fkey NODE DATA NULL \nM (2/2) M (2/2)\u000f\u000f\u000fM (15/25)\nM (8/8)+1\n+1\n+1M (build_num/element_num) \n\u000f\u000f\u000f \u000f\u000f\u000f\n  .element_num â‰¥ 2 Â·   .build_num \n   (7+1)                     (4) \u0001\u0001 \u0001 \u0001 \u0001 /gid00034/gid00067/gid00073/gid00084/gid00082/gid00083 Figure 4: Node Adjustment\nreducing the number of elements assigned to the next level and\nlimiting the tree height.\nIn Algorithm 5, we use the FMCD algorithm as described in Sec-\ntion 3.4 to compute the best model for each node. Given a collection\nKof keys, we create a new node ğ‘›withğ¿entries, where ğ¿isğ›¿(e.g.,\nğ›¿=2) times of the number of keys to preserve enough gaps for\nfuture insertions after the adjustment (line 2). ğ¿is also bounded by\nthe pre-defined node size upper bound, i.e., ğ¿â‰¤16ğ‘€ğµ/16ğµ=1ğ‘€.\nIn fact, 16ğ‘€ğµis set from the max node size in ALEX , andğ¿=1ğ‘€in\nour experience always produces good results and does not need to\nbe tuned. Next, we obtain the model for the new node ğ‘›on the given\nkeys by FMCD (line 3). After that, we insert every key into the\nentry given by the model. When there is a conflict, i.e., more than\none element are mapped to a given position ğ‘™, we collect conflicting\nelements intoKğ‘™, recursively build the partial tree on Kğ‘™to create\na subtree, and set the entry at position ğ‘™to point to the child tree\n(line 9-12). For a non-conflicting element, we simply insert it to the\nentry predicted by the model (line 6-8). Finally, we initialize the\nbasic statistics for the new node, which will be used by the next\nadjustment (line 13-14). The returned node ğ‘›is the node to replace\nthe original node in LIPP .\nWe will analyze the impact of adjustment on lookup and insert\noperations in Section 5.\n4.4 More Operations\nBulkload: The bulkload operation follows the same procedure as\nthe partial tree building in algorithm 5. The returned result is the\nroot node.\nDelete: Delete is supported by looking up the entry of the element\nand marking the type of that entry as NULL . If a future lookup query\nfor this deleted key is executed, it just traverses to a NULL entry,\nwhich indicates non-existence of the key. It may be argued that\nnodes should be either contracted or merged after entry deletions.\nSince real-world datasets tend to increase over time, we believe\nretaining the gaps generated by deletions is beneficial to subsequent\ninsertions. Thus, for simplicity, we do not modify tree structures\nfor deletions.\nUpdate: There are two types of updates. One is to modify the key,\nand the other is only to modify the payload. The former is imple-\nmented by combining a delete operation with an insert operation,\nwhile the latter is supported by looking up the key and over-writing\nthe payload.Algorithm 5 :BuildPartialTree (K)\nInput :K: the collection of keys to adjust\nOutput :ğ‘›: the new root node of partial tree\nbegin1\nğ‘›â†new node with ğ¿=max(1ğ‘€,ğ›¿Â·|K| ) entries; 2\nğ‘›.Mâ† FMCD (K,ğ¿).M; 3\nforğ‘™âˆˆ[0,ğ¿]do 4\nKğ‘™={ğ‘˜âˆˆK|ğ‘›.M(ğ‘˜)==ğ‘™}; 5\nif|Kğ‘™|==1then 6\nğ‘¡ğ‘¦ğ‘ğ‘’(ğ‘›.E[ğ‘™])â† DATA ; 7\nğ‘›.E[ğ‘™]â†ğ‘˜âˆˆKğ‘™; 8\nelse if|Kğ‘™|>1then 9\nğ‘¡ğ‘¦ğ‘ğ‘’(ğ‘›.E[ğ‘™])â† NODE ; 10\nğ‘›â€²â†BuildPartialTree (Kğ‘™); 11\nğ‘›.E[ğ‘™]â† the pointer to node ğ‘›â€²; 12\nğ‘›.ğ‘’ğ‘™ğ‘’ğ‘šğ‘’ğ‘›ğ‘¡ _ğ‘›ğ‘¢ğ‘šâ†|K| ;ğ‘›.ğ‘ğ‘¢ğ‘–ğ‘™ğ‘‘ _ğ‘›ğ‘¢ğ‘šâ†|K| ; 13\nğ‘›.ğ‘ğ‘œğ‘›ğ‘“ğ‘™ğ‘–ğ‘ğ‘¡ _ğ‘›ğ‘¢ğ‘šâ†0; 14\nreturnğ‘›; 15\nend16\nOverflow Insert and Lookup: An overflow operation means that\na key exceeds the existing key space in the tree. For instance, the\nappend-only insert workload may cause consecutive overflow in-\nserts. In LIPP , the overflow insert and lookup can be optimized by\nmaintaining two temporary sorted buffers for the right-most and\nthe left-most entries and triggering adjustment when either buffer\nis full. Note that this design will not affect the normal insert and\nlookup operations because normal operations do not visit the two\nextra buffers. We use binary search in the buffers to improve search\nperformance for overflow operations.\n5 ANALYSIS\nIn this section, we provide complexity analysis for both lookup\nand insert operations, which is already shown in Table 1. Other\noperations can be analyzed in the similar way.\n5.1 Tree Height and Lookup Analysis\nInLIPP , the lookup traverses along a path from root to the final\nentry without extra search steps. Therefore, the lookup cost is only\nrelated to the height of the index structure.\nA tree index can be either built from bulkload operations, i.e., an\nadjustment on the root node, or result from a number of random\ninsertions. We first analyze the tree height for the former case:\nLetğ‘šbe the minimum fanout of an index, i.e., minâŒˆğ‘\nğ‘‡âŒ‰among all\nnodes. In fact, ğ‘šâ‰¥3sinceğ‘‡â‰¤âŒˆğ‘\n3âŒ‰, which indicates that at least\nâŒˆğ‘\nğ‘‡âŒ‰â‰¥3positions each contains up to ğ‘‡entries for any given ğ‘\nelements.\nTheorem 5.1. The height of a LIPP index withğ‘elements built\nfrom adjustment is at most ğ‘‚(logğ‘).\nProof. Even in the extreme case, for each level of the index\nstructure, there are at least ğ‘šbranches, each of which contains at\nmostğ‘\nğ‘šelements. Thus, the height of index is at most ğ‘‚(logğ‘šğ‘).\nâ–¡\n\nNow we pay more attention to the height of the index built from\nscratch in Theorem 5.2, which is a more common way for index\nconstruction in practice.\nTheorem 5.2. The height of a LIPP index withğ‘elements is at\nmostğ‘‚(logğ‘).\nsketch. The core idea is to find the recursive relation between\nthe number of elements existed in the parent and the child node.\nRefer to Appendix [44] A.2 for details. â–¡\nFinally, as the complexity of lookup operations depends only\non the tree height, it is obvious that the complexity of a lookup\noperation is ğ‘‚(logğ‘), even in the worst cases.\nIn the average case, it is obvious that LIPP has the complexity\nğ‘‚(logğ‘šğ‘) for lookup operations when the fanout of nodes is at\nleastğ‘š. Compared with PGM , which takes ğ‘‚(log2\nğ‘šğ‘), and ALEX ,\nwhich takes ğ‘‚(logğ‘šğ‘+log2ğ‘š),LIPP achieves the best complexity\nfor lookup operations.\n5.2 Insert Analysis\nThough the cost of a normal insertion also depends on the tree\nheight, we need to concentrate on the cost of adjustment, which\nincurs extra overhead for insertions. Since adjustment is not trig-\ngered by each insertion, it is reasonable to employ the amortized\nanalysis for insert operations.\nFirstly, we analyze the adjustment cost in the worst case.\nTheorem 5.3. The adjustment on a node with ğ‘elements costs at\nmostğ‘‚(ğ‘Â·logğ‘).\nProof. Withğ‘elements, obtaining the best model with the\nminimum conflict degree costs ğ‘‚(ğ‘). Meanwhile, the height for\nthe adjusted tree is ğ‘‚(logğ‘). In the worst case, each level would\ncostğ‘‚(ğ‘)to train models. Then the total cost would be at most\nğ‘‚(ğ‘Â·logğ‘). â–¡\nThen, for the LIPP index with size ğ‘built from the insertions,\nwe only pay amortized ğ‘‚(log2ğ‘)for each insertion:\nTheorem 5.4. The amortized cost for insert operations is at most\nğ‘‚(log2ğ‘).\nsketch. We use the accounting method [ 10] for the analysis.\nThe core idea is to save extra ğ‘‚(logğ‘)credits for each node along\nthe traversal path of insertions and to consume credits for the\nadjustment. Please refer to Appendix [44] A.3 for details. â–¡\nInsertions with amortized complexity ğ‘‚(log2ğ‘)forLIPP seems\nworse than B+Tree . But compared with other learned indexes, we\nstill achieve better complexity. In fact, ALEX costsğ‘‚(ğ‘š)for shifting\nthe elements in the extreme cases where ğ‘šis the average fan-out\nof a node. Besides, our performance is analyzed in the worse case\nrather than average case. In most cases, an insertion only needs to\ntraverse to the NULL entry and add the element, which costs only\nğ‘‚(logğ‘šğ‘). Moreover, experimental results in Section 6 show that\ninsertions behave well in practice.\n6 EVALUATION\n6.1 Experiment Setup\nDatasets: We evaluate our method using four popular benchmarks\nmainly used in [ 11], with the detailed statistics shown in Table 2: (1)\nLongitudes (LTD) consists of the longitudes of locations aroundTable 2: Statistics of Datasets\nLTD LLT LGN YCSB\nNum Keys 1B 200M 190M 200M\nKey Type double double int64 int64\nPayload Size 8B 8B 8B 80B\nTotal Size 16G 3.2G 3.04G 17.6G\nthe world from Open Street Maps1. (2)Longlat (LLT) consists of\ncompound keys which combine the longitudes and latitudes from\nOpen Street Maps by applying transformation to these pairs, whose\ndistribution is severely non-linear. (3) Lognormal (LGN ) is gener-\nated artificially according to the log-normal distribution2. Besides,\nthe values are multiplied by 109and rounded down to the nearest\nintegers. (4) YCSB is also artificially generated, representing the\nuser IDs from YCSB benchmark3. These values are uniformaly dis-\ntributed across int64 domain. No duplicated elements are contained\nin these datasets unless otherwise stated. We also randomly shuf-\nfle these four datasets to simulate the real-world scenarios, which\nfollows the settings mentioned in paper [11].\nBaselines: We compare LIPP with existing state-of-the-art base-\nlines: (1) Standard B+Tree , as implemented in the STX B+Tree [5]. It\nis the fastest implementation for all in-memory B+Tree structures\nover many common operations. (2) Adaptive Radix Tree( ART) [26],\na trie optimized for main memoring indexing and adapted to the\ndata. (3) ALEX [11], an in-memory, updateble learned index, which\nutilizes the gapped array to accommodate elements with exponen-\ntial search step. (4) PGM [12], a fully-dynamic compressed learned\nindex with provable worst-case bounds. (5) Learned Index [22],\nusing a two-level recursive model index with linear model at each\nnode and binary search steps which only support lookup operations.\n(6)BwTree [27], a general purpose, concurrent and lock-free B+-\nTree index. These source codes are publicly available. We download\nand run their codes following the guidelines specified in the papers.\nWorkloads: The primary metric is the average throughput for LIPP\ncompared with other methods. To demonstrate the performance\nover different index operations, we evaluate the throughput on\ndifferent types of workloads: (1) The Read-Only workload, which\nperforms lookup operations on the indexes built from bulkloading\n100M randomly selected keys. (2) The Read-Heavy workload, which\ncontains 33% inserts to put elements into indexes and 67% reads to\nrandomly lookup elements. (3) The Write-Heavy workload, which\ncontains 67% inserts and 33% lookups. (4) The Write-Only workload,\nwhich consists of only insert operations. The workloads (2) through\n(4) are tested on an empty index. Besides, the keys to lookup are\nselected randomly from the set of existing keys in the index, and\nthe keys to insert are randomly chosen from non-existed ones. We\nrun the workloads on different indexes within 100M operations for\nfive times and obtain their throughput of operations (either lookup\nor insert) finally.\nEnvironment: We implement LIPP in C++ and compile it with\nGCC 9.0.1 in O3 optimization mode. All experiments are conducted\non an Ubuntu 18.04 Linux machine with 4.0 GHz Intel Core i7 (4\ncores and 8 threads) and 32GB memory, only using a single thread.\n1https://registry.opendata.aws/osm\n2https://en.wikipedia.org/wiki/Log-normal_distribution\n3https://github.com/brianfrankcooper/YCSB.git\n\nLTDLLTLGNYCSB10203040Throughput  (million ops/sec )\n(a) Read-OnlyLTDLLTLGNYCSB5101520Throughput  (million ops/sec )\n(b) Read-HeavyLTDLLTLGNYCSB51015Throughput  (million ops/sec )\n(c) Write-HeavyLTDLLTLGNYCSB4812Throughput  (million ops/sec )\n(d) Write-OnlyLIPP B+ Tree ART ALEX PGM BwTree LearnedFigure 5: Throughput: Comparisons with State-of-the-art Methods\n6.2 Compare with State-of-the-art Methods\nNext we compare LIPP with state-of-the-art methods. The results\nare shown in Figure 5. To sum up, our approach achieves promising\nresults and consistently beats those state-of-the-art methods on\ndifferent types of workloads.\n6.2.1 Read-Only Workloads. For read-only workloads, LIPP achieves\nup to 2.8x, 6.3x, 15.3x, 9.8x throughput than ALEX ,PGM ,Learned In-\ndexandB+Tree , respectively. Compared with Learned Index ,ALEX\nandPGM ,LIPP is able to eliminate the in-node search step while\nkeeping the tree height bounded. Therefore, it incurs less compu-\ntation overhead to locate the correct position. Besides, for those\ntraditional indexes B+Tree andART,LIPP also has larger nodes to\naccommodate many more elements on one level, and thus has less\ntree height. Thus, the traversal path of LIPP is significantly shorter\nthan those of traditional indexes, resulting in better performance.\nBwTree behaves even much worse than B+Tree since it costs more\ntime on the infrastructure to allow concurrent operations.\nMeanwhile, on the LLT dataset, the throughput for LIPP is only\n1.47x than ALEX . But we see that ALEX andPGM even behave worse\nor similar than Learned Index on this dataset. The reason is that LLT\nis highly non-uniform, which makes it difficult for learned model\nto depict overall distributions and thus the tree height of indexes\nincreases. As a result, the traversal on tree structure dominates\nthe overall performance and the performance of all learned index\nstructures tends to be similar.\n6.2.2 Read-Write workloads. For read-write workloads, LIPP again\nachieves better performance than existed approaches with compa-\nrable index size. Since Learned Index does not support any update\noperations, we exclude it from the comparisons here. Figure 5 in-\ndicates that LIPP beats ALEX ,PGM ,B+Tree ,ART with 2.9x, 13.5x,\n5.4x, 3.7x higher throughput on performance.\nWe observe that the performance of all methods decreases with\nthe increasing portion of insert operations. The reason is that ad-\njustments are required to deal with changes in the index structure\ncaused by insert operations. Compared with the long distance shift-\ning of ALEX , the new node creation of LIPP is more light-weighted.\nSince the adjustment helps to bound the tree height, LIPP still has\nshort traversal path for insert operations. Therefore, LIPP better\nhandles the write operations than ALEX . Besides, PGM performs\neven worse in read-heavy workloads since PGM needsğ‘‚(logğ‘)\ntrees to support updating and each lookup operation should per-\nform searching in all these trees. Since the traversal cost is still\nthe main bottleneck for traditional indexes, LIPP obviously beats\nB+Tree andART.\nLTD LLT LGN YCSB1041061081010Index size (bytes)LIPP\nB+ Tree\nARTALEX\nPGM\nBwTree(a)Read-Heavy\nLTD LLT LGN YCSB1041061081010Index size (bytes)LIPP\nB+ Tree\nARTALEX\nPGM\nBwTree (b)Write-Heavy\nFigure 6: Index Size\n6.2.3 Index Size. Due to space limitation, we just display the in-\ndex size on read-heavy and write-heavy workloads in Figure 6. In\ngeneral, the index size of LIPP is not sensitive to the distribution of\ndatasets, and is comparable or less than that of ALEX andLearned\nIndex in read-write workload except for YCSB . The reason is that in\nYCSB , the model can fit the distribution almost perfectly and there-\nforeALEX can better trade the search performance for reducing the\nindex size by holding the inaccurate elements in wrong positions.\nAt the same time, LIPP needs extra small nodes in the lowest level\nto hold conflicting elements in local areas. Besides, we also observe\nthatPGM has larger index size because it utilizes the logarithmic\nmethod to support insert and therefore requires to store a series of\nindex trees.\nNevertheless, we point out that the space overhead for the raw\ndata will overshadow that of index. For example, the size of YCSB\nbenchmark is 17.6 GB while the index size is not larger than 500\nMB even for the worst method. Therefore, the index size will not\nbe the bottleneck of memory usage and the index overhead of LIPP\nis acceptable.\n6.2.4 Range Query. The range query operation can be implemented\nby a simple lookup operation on the start key and a scan for the\nfollowing elements. We restrict the number of keys for scanning\nto less than 100 for range queries, as mentioned in paper [11]. For\nread-only workload, the performance of range query is comparable\nwith ALEX andB+Tree as shown in Figure 7, just keeping its advan-\ntages up to 1.5x performance gain. The reason is that as scan time\nbegins to dominate the overall query time, the speedup of LIPP\non lookups becomes less apparent. Besides, LIPP needs to distin-\nguish the different entry types and apply different actions during\nthe scan, which also slightly influences the performance of range\nquery. It is also worth mentioning that ART fail to support range\ndue to its implementation. Since PGM needs to insert elements in\n\na series of indexes, it requires to scan all components to support\nthe range query in PGM . Thus the query time of PGM is orders of\nmagnitudes slower than LIPP and thus is ignored here.\nLTD LLTLGNYCSB1234Throughput  (million ops/sec)LIPP B+ Tree ALEX\nFigure 7: Range Query\nLTD LLT LGN YCSB204060Bulkload time (s)LIPP\nB+ TreeALEX\nART Figure 8: BulkLoad\n6.2.5 Bulkload. In Figure 8, we demonstrate the time to load 100M\nelements at once to build the index. Among all learned indexes,\nLIPP only requires less than half of the time to bulkload compared\nwith ALEX andPGM . Actually, LIPP only needs to create new nodes\nfor conflicting elements instead of shifting the elements to preserve\na gap for ALEX . Also, PGM builds its index in a bottom-up manner\nand results in more segments with specified error bound.\nIn addition, LIPP is only slightly slower than B+Tree on bulkload\nsince LIPP needs to create small nodes for conflicting elements.\nButLIPP can make up for its little slower bulkload time. Moreover,\nafter we further insert 100K records after bulkload, the performance\nof insert operation in B+Tree decreases sharply due to the aging\nproblem.\n6.3 Detailed Performance Study\nR/W LIPP ALEX PGM Learned B+Tree\nLTD 3.6/4.5 7.5/13.0 19.0/- 20.7/- 56.5/57.6\nLLT 4.4/6.8 9.2/17.4 19.0/- 36.0/- 56.8/57.8\nLGN 3.5/4.1 8.1/15.5 18.0/- 28.8/- 57.9/57.7\nYCSB 3.1/3.1 9.1/10.7 17.0/- 12.6/- 57.0/57.8\nTable 3: The Average Number of Memory Accesses\nNext we investigate how the contribution that each proposed\ntechnique makes to the overall performance. In Table 3, we dis-\nplay the average number of memory accesses for lookup and insert\noperations on read-only and write-only workloads, separately rep-\nresented by two values in each cell. For insert operations, we only\ncount the most common cases without structure modifications,\nwhich reveals the length of common paths accessed by insertions.\nBesides, since PGM uses logarithm methods to support insertion,\nits memory access is meaningless for single insert operation. ART\njust behaves similar to B+Tree . Thus we ignore them here.\nThe results in Table 3 show that LIPP has the significantly fewer\nmemory accesses compared with other baselines. In fact, the num-\nber of memory accesses is at least 3for the indexes with two or\nmore levels. That is, one access to get the entry in root model, one\naccess to extract the pointer and obtain the child model, and one\naccess to locate the entry of the lookup key.\nFor lookup operations, LIPP eliminates the â€œlast-mileâ€ search of\nALEX andLearned Index , removes redundant lookups in different\ncomponents of PGM and also reduces the traversal path over index\ntrees compared with B+Tree andART. Compared with LIPP ,ALEXrequires extra memory accesses to finish the exponential search to\ncorrect the mispredications in leaf nodes while PGM needs to find\nkeys in a series of subtrees and thus incurs more memory accesses.\nMoreover, since B+Tree index has many more levels, the lookup\noperations need to traverse the overall levels and figure out the\ninternal node by binary searching, which thus costs more overhead\nas shown in the table. All improvements are reflected by the small\nnumber of memory accesses. For insert operations, the adjustment\noperation of LIPP is much cheaper than shift in ALEX and split\ninB+Tree . In fact, ALEX aims to use a gapped array to reduce the\nnumber of shifting elements to ğ‘‚(logğ‘š)with high probabilities,\nbut still incurs huge costs for each insertion and introduces the\noverhead to maintain the structure of the gapped array.\nTo further display the internal properties of our method, we\nconduct the experiments of variance analysis on the write-heavy\nworkload. In one run, we collect the latency (execution time) of each\noperation, sort the latencies in the ascending order, and report 99-th\npercentile latency in Figure 9a. LIPP has relative small latency com-\npared with ALEX andB+Tree . This is due to that LIPP has fewer\nstructure modifications, which is shown in Table 3a. Therefore,\nmost of our operations are handled in the common cases without\nincurring too much overhead. Besides, LIPP utilizes the fewer mem-\nory accesses to finish these operations, which causes the lower\nlatency. As for ALEX , its structure modifications are more common\nthan LIPP [11]. While PGM uses logarithm methods to support\ninsertions, it will frequently incur merge process during insert and\nlookup operations, which causes the larger 99-th percentile latency.\nLTD LLT LGN YCSB0.51.01.52.02.53.099% Latency (1000 ns)LIPP\nB+ TreeART\nALEXPGM\n(a)99-th Percentile Latency\nLTD LLTLGNYCSB102103104105106Standard Deviation (ns)LIPP\nB+ TreeART\nALEXPGM (b)Standard Deviation\nFigure 9: Variance of Operations\nWe also report the standard deviation of the operation time\nin Figure 9b. Though LIPP seems not outstanding, the standard\ndeviation of our method is still comparable with those learned\nindexes, i.e., ALEX andPGM . Actually, the structure adjustment\nof the traditional indexes only involves a small amount of data.\nThus, the latency of each operation for B+Tree andART is relatively\nuniform. However, the structure modifications of learned indexes\ninvolve the larger number of keys and values and cost much more\ntime to finish, which causes the inevitable higher standard deviation.\nIn a word, the high standard deviation of our method is acceptable\nand reasonable.\nBesides, we report the number of adjustments, as well as the\nratio of time spent on adjustment to the overall running time. The\nresults in Table 3a illustrate the rarity of structure modifications\nonLTD,LGN andYCSB and the results in Table 3b show that the\ntime used in adjustments is relatively small compared with the\nrunning time, which thus is not the main bottleneck of LIPP . Be-\nsides, we find that a single adjustment may spend much more time\n\nthan an insert operation without adjustment triggered. However,\nit wonâ€™t be a severe case since the adjustment is rarity and more\nthan 99% of insert operations do not trigger adjustments. And in\nAppendix [ 44] B.3, the ratio of time spent on structure modifica-\ntions for ALEX and adjustments for LIPP are comparable, which\nalso alleviates the severity of the above problem.\n(a)# Adjustment\nRH WH WO\nLTD 27 32 32\nLLT 2448 10887 20594\nLGN 22 24 24\nYCSB 11 11 11(b)% Adjustment\nRH WH WO\nLTD 25% 33% 31%\nLLT 13% 17% 17%\nLGN 23% 26% 22%\nYCSB 23% 21% 17%\nTable 4: The Number and Time Ratio of Adjustments\nMoreover, we explain the behaviour of our method on LLT\ndataset. Since LLT is highly non-uniform and its distribution is\nroughly jagged, LIPP utilizes many more adjustments to adapt this\nirregular dataset. But the most of the adjustments only involve a\nvery small part of the data, which thus do not cost too much time.\nAs we can see in Table 3b, the time for adjustment caused on LLT\nis still small compared to the overall running time, whose ratio\nis only 13%, 17%, 17% on read-heavy, write-heavy and write-only\nworkloads.\n6.4 Scalability with Data Size\nHere we evaluate the scalability of LIPP with the increasing volume\nof data. We run both the read-heavy and write-heavy workloads on\ntheLTD dataset with the varying number of keys in the beginning,\ninstead of building the index from scratch.\n10%20%40% 80%\nInitial data size481216Throughput (million ops/sec)LIPP\nB+ TreeALEX\nART\n(a)Read-Heavy\n10%20%40% 80%\nInitial data size2468Throughput (million ops/sec)LIPP\nB+ TreeALEX\nART (b)Write-Heavy\nFigure 10: Data Scalability\nFigure 10 shows the results for both read-heavy and write-heavy\nworkloads. We see that LIPP beats ALEX andB+Tree with the in-\ncreasing data size. As the number of elements increases, the through-\nput of LIPP slows down, but in a low rate since the tree height is\nrestricted by the FMCD strategy in adjustment. We also point out\nthat when the data volume is 40% in Figure 10b, the abnormal in-\ncreasing of LIPP is caused by the different tree structure of indexes\nafter bulkload on different number of keys.\n6.5 The Effect of Adjustment Strategy\nTo show the effectiveness of our proposed algorithm for adjustment,\nwe compare LIPP using the default adjustment strategy ( FMCD )\nwith LIPP simply using Linear Regression ( LR) to obtain the model\nLTDLLTLGNYCSB5101520Throughput (million ops/sec)FMCD (Read-Heavy)\nLR (Read-Heavy)\nFMCD (Write-Heavy)\nLR (Write-Heavy)(a)ThroughputLR FMCD\nLTD 41567 57\nLLT 40852 3680\nLGN 1465 658\nYCSB 8 5\n(b)Conflict Degree minğ‘‡\nFigure 11: Effect of Adjustment Strategy\nparametersM.ğ´andM.ğ‘. Figure 11a shows the results, where\nFMCD consistently achieves better than LR, even on YCSB that is\ntotally uniform. FMCD distributes different keys to make as few\nconflicting elements as possible, while LRonly tries to approximate\nthe key and position with straight lines. Moreover, the low com-\nplexity of FMCD restricts the cost of reconstruction not higher\nthan the performance improved by this strategy, making FMCD\nmore competitive. To further demonstrate the dominance of our\nadjustment strategy, we apply LRandFMCD on1ğ‘€random keys\nfrom different datasets to get two models, respectively. We then\nobtain the conflict degree of these two models in Table 11b. The\nresults show that FMCD has many fewer conflict degrees than LR\nconsistently, which would have the fewer layers for all keys in\naverage and thus improve the overall performance. Actually, LRis\nsensitive to the elements, where a drift on the last point results in\nlarger slope and thus causes more conflicts. But FMCD strategy is\nmore robust to the distribution, and tries its best to limit the conflict\ndegree to a very small number.\n7 DISCUSSION\n7.1 Concurrent Operations\nAmong past years, researchers have extensively studied how to im-\nplement a concurrency control protocol in B+Tree or other trees [ 3,\n24,36]. In fact, LIPP can be considered as a tree and the adjustment\nonLIPP can be viewed as a type of structure modification, similar\nto the split or merge process in B+Tree . Thus, traditional ways to\nsupport concurrency can be used in our method. We could apply\nthe latch coupling [ 3] protocol on LIPP , which needs read/write\nlocks. For lookup operations, we repeatedly acquire the read lock\non child and then unlock parent along the traversal path. For insert\noperations, we traverse index and repeatedly obtain the write lock\non a child. Once the child is locked, we check if it is safe, i.e., this\nnode will not trigger the adjustment after inserted. If it is safe, we\nrelease locks on ancestors.\nBesides, LIPP â€™s unique advantage of precise positions reduces\nconcurrency contentions compared to B+Tree and other learned\nindexes. In B+Tree orALEX , insertions without structure modifica-\ntions may cause elements to shift in the leaf nodes. Consequently,\nlookup and insert operations in the same leaf node may still incur\ncontentions even though these two operations on different positions.\nHowever, LIPP effectively eliminates such unnecessary contentions\nsince insert operations with no adjustment do not affect lookup\noperations in other positions. Thus, we can remove mechanisms\nfor those contentions in concurrency control protocols.\n\n7.2 New Hardware Accelerators\nNew hardware accelerators, such as GPU/TPUs, will make our\nmethod even more valuable. At the same time, those new hardwares\nhave their own challenges, most importantly the high invocation\nlatency. It still requires 2-3 micro-seconds to invoke any operation\non them. However, with the expansion of video memory and the\ndevelopment of direct NVM access from GPU [ 35], it is reasonable\nto assume that probably all learned indexes will fit on the GPU.\nBesides, the integration of machine learning accelerators with the\nCPU is getting better and with techniques like batching requests,\nthe cost of invocation can be amortized, so that we do not believe\nthe invocation latency is a real obstacle.\nAnother challenge is under-utilization brought by the totally dif-\nferent computing models of new hardwares. We need to fully utilize\nthe thousands of available GPU cores, which requires eliminating or\nat least minimizing required communication between GPU threads\nand branch divergence within a SIMD instruction. Therefore, we\nmay need to change the underlying data layouts to avoid the con-\ntention as much as possible. Moreover, the synchronization and\ncommunication in GPU are far more heavy-weighted and com-\nplicated than that in CPU, which may require new concurrency\ncontrol protocols.\n8 RELATED WORK\nLearned Index With development of machine learning, a new\nfamily of index structures, such as A-Trees [ 13] and Learned In-\ndex[22], are introduced to learn the underlying data distribution to\nindex data items. This work inspires a series of work. FITing [14]\nreplaces the leaf nodes of a B+Tree with linear models to compress\nindex and maintain operationsâ€™ performance, which has restricted\nguarantees for prediction error. PGM [12] extends the idea from\nFITing , replaces the non-leaf node with linear models also and pro-\nvides an optimal way to obtain piece-wise linear model under their\nrequirements. Both of PGM andFITing do support insertions but\njust in naive and direct ways such as extra buffer and merge, which\nthus are not their main contributions. ALEX [11], indeed, supports\nthe insert operations by utilizing almost fixed non-leaf nodes as\nroute to and applying exponential search on the gapped array in\ntheir leaf nodes. However, these indexes just learn the approximate\nposition for elements and need extra search step to correct the\npredictions. Therefore, they introduce another set of space-time\ntrade-offs between prediction error and tree size. LIPP is totally dif-\nferent, which introduces new structure to completely eliminate the\ninaccuracy caused by learned model predictions. Besides, Nathan et\nal. [37] put attention on the multi-dimensional in-memory learned\nindex. Kipf et al. [ 21] focus on the constructions of learned index\nin single pass. Tang et al. [ 42] devise a scalable learned index for\nmulticore data storage. All of them are different from our scope.\nDatabase Index Meanwhile, various indexes are proposed over\nthe last decades. Traditionally, B+Tree [2] and its variants are origi-\nnally proposed for disk settings. However, realizing the importance\nof cache utilization for memory indexes, several cache conscious\nB+Trees [15] have entered the fields of researchersâ€™ vision. Among\nof them, Adaptive Radix Tree [ 26] integrates the B+Tree with Tries\nto reduce cache misses, while CSS-trees [ 40] restrict index node size\nto fit into CPU cache line and use arithmetic operations to elimi-\nnate the usage of pointers in index nodes. Instead, pB+-tree [ 8] useslarger index nodes but mainly relies on instructions to prefetch the\nnecessary information in advance. FAST [ 18] exploits the usage of\nSIMD instructions to further optimize searches within index nodes\nfor cache performance. Meanwhile, Blink-Tree [ 28] provides a con-\ncurrent tree restructuring mechanism for handling underflow nodes\nas well as overflow nodes. Except the series of B+Tree , the choices\nof index for databases also have many candidates for specific pur-\nposes, such as T-trees [ 25], red-black trees [ 7], LSM tree [ 38], and\nmore recently, HOT [ 6]. Besides, some researchers put their atten-\ntions on new hardware platforms, such as Multi-Core Chips [ 27],\nNon-Volatile Memory [ 9,29], Hardware Transactional Memory [ 41],\nSolid-State Disk [17], Graphic Processing Units [4, 19], etc.\nMachine Learning for Database Systems Machine Learning\nhas been proved its power in different database related fields. Pavlo\net al. [ 39] construct â€œself-drivingâ€ database systems which can auto-\nmatically optimize themselves without human intervention to cope\nwith complicated query workload and data characteristics. Mean-\nwhile, Ma et al. [ 30] follow the ideas and further design a query\nworkload forecasting framework in autonomous DBMS. Moreover,\nAken et al. [ 1] propose an automatically database tuning frame-\nwork boosted by machine learning techniques. More researchers\nalso put focus on combining learning techniques with traditional\nSQL optimization in different aspects. Marcus et al. [ 32â€“34] tackle\nthe problem of query optimization in database system. Meanwhile,\nWu et al. [ 43] also learn cardinality models for overlapping sub-\ngraph templates on the shared cloud workloads. Yang et al. [ 47]\nstudy the problem of selectivity estimation using deep learning\ntechniques, which approximate the joint data distribution without\nany independence assumptions. Also, Kristo et al. [ 23] introduce a\nnew type of distribution sort that leverages a learned model of the\nempirical CDF of the data. In addition, Zhang et al. [ 48] propose\na learned scheduler that leverages overlapping data reads among\nincoming queries and learns a scheduling strategy that improves\ncache hits. Besides, Yang et al. [ 46] adopt machine learning tech-\nniques for data partitioning to improve storage issues of database\nsystem. And Yang et al. [ 45] utilize a machine learning method\nto predict hot records in an LSM-tree storage engine and prefetch\nthem into the cache. Hsu et al. [ 16] devise new algorithms which\nlearn relevant patterns for data streams and use them to improve\nits frequency estimates.\n9 CONCLUSION AND FUTURE WORK\nIn this paper, we introduce LIPP , a brand new learned index struc-\nture to efficiently support a full set of index operations. We address\nthe bottleneck of previous learned index structures by precisely\npredicting the position of a search key and thus eliminating the\nlast-mile searches within leaf nodes. To this end, we provide a novel\nmetric to decide the layout of tree nodes and propose a dynamic\nadjustment strategy to tightly bound the height of the tree. Experi-\nmental results on popular datasets demonstrate the superiority of\nour proposed method on a variety of workloads.\nFor the future work, we plan to explore the ways to automatically\nand effectively select the monotonic functions to make LIPP more\nrobust to datasets with different distributions. Besides, it is also\ninteresting to investigate how to implement LIPP into real world\nrelational DBMS with concurrency control and how to adjust LIPP\nfor new hardwares.\n\nREFERENCES\n[1]Dana Van Aken, Andrew Pavlo, Geoffrey J. Gordon, and Bohan Zhang. 2017.\nAutomatic Database Management System Tuning Through Large-scale Machine\nLearning. In SIGMOD . 1009â€“1024.\n[2]Rudolf Bayer and Edward M. McCreight. 1972. Organization and Maintenance of\nLarge Ordered Indices. Acta Informatica 1 (1972), 173â€“189.\n[3]Rudolf Bayer and Mario Schkolnick. 1977. Concurrency of Operations on B-Trees.\nActa Informatica 9 (1977), 1â€“21.\n[4]Felix Beier and Kai-Uwe Sattler. 2017. GPU-GIST - a case of generalized database\nindexing on modern hardware. it Inf. Technol. 59, 3 (2017), 141.\n[5]Timo Bingmann. [n. d.]. STX B+ Tree . https://panthema.net/2007/stx-btree/,\nVersion 0.9.\n[6]Robert Binna, Eva Zangerle, Martin Pichl, GÃ¼nther Specht, and Viktor Leis. 2018.\nHOT: A Height Optimized Trie Index for Main-Memory Database Systems. In\nSIGMOD . 521â€“534.\n[7]Joan Boyar and Kim S. Larsen. 1992. Efficient Rebalancing of Chromatic Search\nTrees. In Algorithm Theory - SWAT , Otto Nurmi and Esko Ukkonen (Eds.), Vol. 621.\n151â€“164.\n[8]Shimin Chen, Phillip B. Gibbons, and Todd C. Mowry. 2001. Improving Index\nPerformance through Prefetching. In SIGMOD . 235â€“246.\n[9]Shimin Chen and Qin Jin. 2015. Persistent B+-Trees in Non-Volatile Main Memory.\nPVLDB 8, 7 (2015), 786â€“797.\n[10] Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein.\n2009. Introduction to Algorithms, 3rd Edition . MIT Press. http://mitpress.mit.\nedu/books/introduction-algorithms\n[11] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\nDavid B. Lomet, and Tim Kraska. 2020. ALEX: An Updatable Adaptive Learned\nIndex. In SIGMOD . 969â€“984.\n[12] Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-index: a fully-dynamic\ncompressed learned index with provable worst-case bounds. PVLDB 13, 8 (2020),\n1162â€“1175.\n[13] Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and\nTim Kraska. 2018. A-Tree: A Bounded Approximate Index Structure. CoRR\nabs/1801.10207 (2018).\n[14] Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and Tim\nKraska. 2019. FITing-Tree: A Data-aware Index Structure. In SIGMOD . 1189â€“1206.\n[15] Richard A. Hankins and Jignesh M. Patel. 2003. Effect of node size on the\nperformance of cache-conscious B+-trees. In SIGMETRICS . 283â€“294.\n[16] Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. 2019. Learning-Based\nFrequency Estimation Algorithms. In ICLR . OpenReview.net.\n[17] Martin V. JÃ¸rgensen, RenÃ© Bech Rasmussen, Simonas Saltenis, and Carsten SchjÃ¸n-\nning. 2011. FB-tree: a B+-tree for flash-based SSDs. In IDEAS . ACM, 34â€“42.\n[18] Changkyu Kim, Jatin Chhugani, Nadathur Satish, Eric Sedlar, Anthony D. Nguyen,\nTim Kaldewey, Victor W. Lee, Scott A. Brandt, and Pradeep Dubey. 2010. FAST:\nfast architecture sensitive tree search on modern CPUs and GPUs. In SIGMOD .\n339â€“350.\n[19] Mincheol Kim, Ling Liu, and Wonik Choi. 2018. A GPU-Aware Parallel Index\nfor Processing High-Dimensional Big Data. IEEE Trans. Computers 67, 10 (2018),\n1388â€“1402.\n[20] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2019. SOSD: A Benchmark for Learned\nIndexes. CoRR abs/1911.13014 (2019).\n[21] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2020. RadixSpline: a single-pass learned\nindex. In aiDM@SIGMOD 2020 . 5:1â€“5:5.\n[22] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In SIGMOD . 489â€“504.\n[23] Ani Kristo, Kapil Vaidya, Ugur Ã‡etintemel, Sanchit Misra, and Tim Kraska. 2020.\nThe Case for a Learned Sorting Algorithm. In SIGMOD . 1001â€“1016.\n[24] Philip L. Lehman and S. Bing Yao. 1981. Efficient Locking for Concurrent Opera-\ntions on B-Trees. ACM Trans. Database Syst. 6, 4 (1981), 650â€“670.\n[25] Tobin J. Lehman and Michael J. Carey. 1986. A Study of Index Structures for Main\nMemory Database Management Systems. In VLDB , Wesley W. Chu, Georges\nGardarin, Setsuo Ohsuga, and Yahiko Kambayashi (Eds.). 294â€“303.\n[26] Viktor Leis, Alfons Kemper, and Thomas Neumann. 2013. The adaptive radix\ntree: ARTful indexing for main-memory databases. In ICDE . 38â€“49.\n[27] Justin J. Levandoski, David B. Lomet, and Sudipta Sengupta. 2013. The Bw-Tree:\nA B-tree for new hardware platforms. In ICDE . IEEE Computer Society, 302â€“313.\n[28] Sungchae Lim, Joonseon Ahn, and Myoung-Ho Kim. 2003. A Concurrent B-Tree\nAlgorithm Using a Cooperative Locking Protocol. In BNCOD , Vol. 2712. 253â€“260.\n[29] Jihang Liu, Shimin Chen, and Lujun Wang. 2020. LB+-Trees: Optimizing Persistent\nIndex Performance on 3DXPoint Memory. PVLDB 13, 7 (2020), 1078â€“1090.\n[30] Lin Ma, Dana Van Aken, Ahmed Hefny, Gustavo Mezerhane, Andrew Pavlo, and\nGeoffrey J. Gordon. 2018. Query-based Workload Forecasting for Self-Driving\nDatabase Management Systems. In SIGMOD . 631â€“645.[31] Ryan Marcus, Andreas Kipf, Alexander van Renen, Mihail Stoian, Sanchit Misra,\nAlfons Kemper, Thomas Neumann, and Tim Kraska. 2020. Benchmarking Learned\nIndexes. CoRR abs/2006.12804 (2020).\n[32] Ryan Marcus and Olga Papaemmanouil. 2019. Towards a Hands-Free Query\nOptimizer through Deep Learning. In CIDR .\n[33] Ryan C. Marcus, Parimarjan Negi, Hongzi Mao, Chi Zhang, Mohammad Alizadeh,\nTim Kraska, Olga Papaemmanouil, and Nesime Tatbul. 2019. Neo: A Learned\nQuery Optimizer. PVLDB 12, 11 (2019), 1705â€“1718.\n[34] Ryan C. Marcus and Olga Papaemmanouil. 2019. Plan-Structured Deep Neural\nNetwork Models for Query Performance Prediction. PVLDB 12, 11 (2019), 1733â€“\n1746.\n[35] Pak Markthub, Mehmet E. Belviranli, Seyong Lee, Jeffrey S. Vetter, and Satoshi\nMatsuoka. 2018. DRAGON: breaking GPU memory capacity limits with direct\nNVM access. In SC. IEEE / ACM, 32:1â€“32:13.\n[36] C. Mohan. 1990. ARIES/KVL: A Key-Value Locking Method for Concurrency\nControl of Multiaction Transactions Operating on B-Tree Indexes. In PVLDB ,\nDennis McLeod, Ron Sacks-Davis, and Hans-JÃ¶rg Schek (Eds.). Morgan Kaufmann,\n392â€“405.\n[37] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020. Learn-\ning Multi-Dimensional Indexes. In SIGMOD . 985â€“1000.\n[38] Patrick E. Oâ€™Neil, Edward Cheng, Dieter Gawlick, and Elizabeth J. Oâ€™Neil. 1996.\nThe Log-Structured Merge-Tree (LSM-Tree). Acta Informatica 33, 4 (1996), 351â€“\n385.\n[39] Andrew Pavlo, Gustavo Angulo, Joy Arulraj, Haibin Lin, Jiexi Lin, Lin Ma,\nPrashanth Menon, Todd C. Mowry, Matthew Perron, Ian Quah, Siddharth San-\nturkar, Anthony Tomasic, Skye Toor, Dana Van Aken, Ziqi Wang, Yingjun Wu,\nRan Xian, and Tieying Zhang. 2017. Self-Driving Database Management Systems.\nInCIDR .\n[40] Jun Rao and Kenneth A. Ross. 1999. Cache Conscious Indexing for Decision-\nSupport in Main Memory. In VLDB , Malcolm P. Atkinson, Maria E. Orlowska,\nPatrick Valduriez, Stanley B. Zdonik, and Michael L. Brodie (Eds.). 78â€“89.\n[41] Dimitrios Siakavaras, Panagiotis Billis, Konstantinos Nikas, Georgios I. Goumas,\nand Nectarios Koziris. 2020. Efficient Concurrent Range Queries in B+-trees\nusing RCU-HTM. In SPAA , Christian Scheideler and Michael Spear (Eds.). ACM,\n571â€“573.\n[42] Chuzhe Tang, Youyun Wang, Zhiyuan Dong, Gansen Hu, Zhaoguo Wang, Minjie\nWang, and Haibo Chen. 2020. XIndex: a scalable learned index for multicore data\nstorage. In PPoPP . ACM, 308â€“320.\n[43] Chenggang Wu, Alekh Jindal, Saeed Amizadeh, Hiren Patel, Wangchao Le, Shi\nQiao, and Sriram Rao. 2018. Towards a Learning Optimizer for Shared Clouds.\nProc. VLDB Endow. 12, 3 (2018), 210â€“222.\n[44] Jiacheng Wu, Yong Zhang, Shimin Chen, Jin Wang, Yu Chen, and Chunxiao Xing.\n2021. Updatable Learned Index with Precise Positions. arXiv:cs.DB/2104.05520\n[45] Lei Yang, Hong Wu, Tieying Zhang, Xuntao Cheng, Feifei Li, Lei Zou, Yujie\nWang, Rongyao Chen, Jianying Wang, and Gui Huang. 2020. Leaper: A Learned\nPrefetcher for Cache Invalidation in LSM-tree based Storage Engines. PVLDB 13,\n11 (2020), 1976â€“1989.\n[46] Zongheng Yang, Badrish Chandramouli, Chi Wang, Johannes Gehrke, Yinan Li,\nUmar Farooq Minhas, Per-Ã…ke Larson, Donald Kossmann, and Rajeev Acharya.\n2020. Qd-tree: Learning Data Layouts for Big Data Analytics. In SIGMOD . 193â€“\n208.\n[47] Zongheng Yang, Eric Liang, Amog Kamsetty, Chenggang Wu, Yan Duan, Peter\nChen, Pieter Abbeel, Joseph M. Hellerstein, Sanjay Krishnan, and Ion Stoica. 2019.\nDeep Unsupervised Cardinality Estimation. PVLDB 13, 3 (2019), 279â€“292.\n[48] Chi Zhang, Ryan Marcus, Anat Kleiman, and Olga Papaemmanouil. 2020.\nBuffer Pool Aware Query Scheduling via Deep Reinforcement Learning. CoRR\nabs/2007.10568 (2020).\n\nA PROOFS\nA.1 Proof of Theorem 3.3\nThe Correctness of ğ‘‡0:We ensure that ğ‘‡0obtained in Algorithm 1\nsatisfies Equation (9). In Figure 3, we show the time point when ğ‘‡\nis changed to ğ‘‡0on the keyğ‘˜ğ‘—. We then proveâˆ€ğ‘–âˆˆ[0,ğ‘âˆ’1âˆ’ğ‘‡0],\nG(ğ‘˜ğ‘–+ğ‘‡0)âˆ’G(ğ‘˜ğ‘–)â‰¥ğ‘ˆğ‘‡0. Obviously, the constraint is established for\nanyğ‘–â‰¥ğ‘—sinceğ‘‡0has passed this check in line 5 of Algorithm 4. For\nğ‘–<ğ‘—, as shown in Figure 3, the algorithm has checked a ğ‘‡1<ğ‘‡0and\nğ‘‡1satisfiesG(ğ‘˜ğ‘–+ğ‘‡1)âˆ’G(ğ‘˜ğ‘–)â‰¥ğ‘ˆğ‘‡1. Because of the monotonicity\nof the kernel function and ğ‘ˆğ‘‡, we have:\nG(ğ‘˜ğ‘–+ğ‘‡0)âˆ’G(ğ‘˜ğ‘–)â‰¥G(ğ‘˜ğ‘–+ğ‘‡1)âˆ’G(ğ‘˜ğ‘–)â‰¥ğ‘ˆğ‘‡1â‰¥ğ‘ˆğ‘‡0(10)\nThus,ğ‘‡0is a correct solution for Condition (9). â–¡\nThe Minimum of ğ‘‡0:Suppose there exists a smaller ğ‘‡1<ğ‘‡0sat-\nisfying Condition (9). However, since ğ‘‡1is not the result returned\nfrom Algorithm 1, there must exist a time point when ğ‘‡=ğ‘‡1fails to\npass the check in line 5 and causes the increment of ğ‘‡in line 9. In Fig-\nure 3, we can find keys ğ‘˜ğ‘—andğ‘˜ğ‘—+ğ‘‡1whereG(ğ‘˜ğ‘—+ğ‘‡1)âˆ’G(ğ‘˜ğ‘—)<ğ‘ˆğ‘‡1,\nwhich means ğ‘‡1cannot be the solution of Condition (9). This is a\ncontradiction. â–¡\nA.2 Proof of Theorem 5.2\nAssume the latest adjustment is triggered when ğœ‡<ğ‘elements\nhave been inserted. Then we haveğ‘\nğ›½<ğœ‡<ğ‘. From theorem 5.1,\nthe maximum number of entries in a subtree of the index after this\nadjustment is at mostğœ‡\nğ‘š. Then there remain ğ‘âˆ’ğœ‡elements to\nbe inserted. Even if those remaining elements are inserted to one\nchild, the maximum elements contained in one node are limited to\nğ‘âˆ’ğœ‡+ğœ‡\nğ‘š=ğ‘âˆ’(ğ‘šâˆ’1)Â·ğœ‡\nğ‘š<(1âˆ’ğ‘šâˆ’1\nğ‘šÂ·ğ›½)Â·ğ‘. In other words, when\na node contains ğ‘elements, the maximum number of elements\nin its subtree is at most (ğ‘šÂ·ğ›½âˆ’(ğ‘šâˆ’1)\nğ‘šÂ·ğ›½)Â·ğ‘. Therefore, the height of\nthe index with ğ‘elements is at most ğ‘‚(log ğ‘šÂ·ğ›½\nğ‘šÂ·ğ›½âˆ’(ğ‘šâˆ’1)ğ‘), which is\nğ‘‚(logğ‘)sinceğ‘šÂ·ğ›½\nğ‘šÂ·ğ›½âˆ’(ğ‘šâˆ’1)>1whenğ‘šâ‰¥3andğ›½>1.â–¡\nA.3 Proof of Theorem 5.4\nWe use the accounting method [ 10] for this analysis. The core idea\nis to save extra ğ‘‚(logğ‘)credits for each node along the traversal\npath accessed by insertions and consume credits for the adjustment.\nWithout adjustment, the cost for each insertion is obviously\nğ‘‚(logğ‘)since we simply traverse to NULL orDATA entry and put\nelements on NULL entry or create new nodes on DATA entry. In fact,\nthe traversal path of insertions contains at most ğ‘‚(logğ‘)nodes. For\neach node in this traversal path, we storeğ›½\nğ›½âˆ’1Â·ğ‘‚(logğ‘)credits as\nshown in Figure 12. Therefore, we needğ›½\nğ›½âˆ’1Â·ğ‘‚(logğ‘)Â·ğ‘‚(logğ‘)=\nğ‘‚(log2ğ‘)credits in total for each insertion.\nNext, we display how to use these extra credits to pay the cost of\nthe adjustment. Once a node with ğœ‡elements is adjusted, then the\ncredit on this node is cleared to 0. Only after(ğ›½âˆ’1)Â·ğœ‡elements\nare inserted to this node can we adjust this node. And now we have\nalready collected(ğ›½âˆ’1)Â·ğœ‡Â·ğ›½\nğ›½âˆ’1Â·ğ‘‚(logğ‘)=ğ‘‚(ğ›½Â·ğœ‡Â·logğ‘)â‰¥\nğ‘‚(ğ›½Â·ğœ‡Â·log(ğ›½Â·ğœ‡)), which is enough to pay the current adjustment.\nTherefore, as shown in Figure 12, the adjustment triggered froma node simply consumes the credits already collected in this node,\nwithout consuming any extra cost.\nThus, based on the accounting methods, we prove that the cost\nforğ‘insertions is ğ‘‚(ğ‘Â·log2ğ‘), which contains the cost of ad-\njustment during those insertions, and the amortized cost for each\ninsert operation is ğ‘‚(log2ğ‘)at most. â–¡\n15/24\n4/76/8 5/9\n2/2 2/218logN\n8logN\n04logN\n6logN\n015/25\n8/86/8 5/920logN\n8logN 4logN\n0+1\n+1\n+1+2logN\n+2logN\n+2logN4BWFE\u0001DSFEJUT\u0001PO\u0001/PEF\u0001\nBSF\u0001QBJE\u0001GPS\u0001UIJT\u0001BEKVTUNFOUkey /PEFT\u0001JO\u0001UIF\u0001SFE\u0001QBUI\u0001BSF\u0001TUPSFE\u0001\n0\tMPH\u0001/\n\u0001DSFEJUT\u0001GPS\u0001UIJT\u0001JOTFSUJPO \nFigure 12: Amortized Analysis\nB MORE EXPERIMENTS\nB.1 Comparisons on Synthetic Datasets\nFigure 13 displays the results of our method with different kernel\nfunctions compared with other baselines on synthetic datasets. We\nuse different generating functions to construct synthetic datasets\nwith 100M keys. These generating functions include Square (pow2),\nCubic (pow3), Quartic (pow4), Logarithmic (log) and Exponential\n(exp) functions. Under these synthetic datasets, we compare the\nthroughput of LIPP with other baselines, as well as LIPP equipped\nwith specified kernel functions.\nIn Figure 13, LIPP equipped with the kernel function specifically\nchosen consistently performs better than the default one on differ-\nent workloads. In some extreme cases, the throughput of LIPP with\nthe specific kernel function is more than five times as that of the\ndefault setting, which is the significant performance improvement.\nOn these synthetic datasets, LIPP with the default linear kernel\nfunction triggers more conflicts and adjustments especially on the\ndense and irregular area of the distribution. In this case, the number\nof tree levels also increases and the overall performance for both\nlookup and insert operations degrades. However, using a kernel\nfunction consistent with the data distribution can distribute the\ndata more evenly, thereby reducing the element conflicts, the fre-\nquency of structure modifications and the number of levels in LIPP .\nTherefore, we provide the opportunity for users to inform LIPP the\ndistribution of datasets by specifying the kernel function, so as to\nobtain more performance improvements.\nBesides, the experimental results also indicate that our method\n(even only with the default kernel function) still outperforms the\n\npow2 pow3 pow4 log exp204060Throughput (million ops/sec)Specific Kernel\nDefault Kernel\nB+ TreeALEX\nPGM\nART(a) Read-Only\npow2 pow3 pow4 log exp10203040Throughput (million ops/sec)Specific Kernel\nDefault Kernel\nB+ TreeALEX\nPGM\nART (b) Read-Heavy\npow2 pow3 pow4 log exp10203040Throughput (million ops/sec)Specific Kernel\nDefault Kernel\nB+ TreeALEX\nPGM\nART (c) Write-Heavy\npow2 pow3 pow4 log exp10203040Throughput (million ops/sec)Specific Kernel\nDefault Kernel\nB+ TreeALEX\nPGM\nART (d) Write-Only\nFigure 13: Comparisons on Synthetic Datasets\n0.05 0.10 0.15 0.20\nÎ±51015Throughput (million ops/sec)RH WH WO\n(a)LTD\n0.05 0.10 0.15 0.20\nÎ±2468Throughput (million ops/sec)RH WH WO (b)LLT\n0.05 0.10 0.15 0.20\nÎ±51015Throughput (million ops/sec)RH WH WO (c)LGN\n0.05 0.10 0.15 0.20\nÎ±510152025Throughput (million ops/sec)RH WH WO (d)YCSB\nFigure 14: The Effect of ğ›¼\n1.5 2.0 2.5 3.0\nÎ²51015Throughput (million ops/sec)RH WH WO\n(a)LTD\n1.5 2.0 2.5 3.0\nÎ²2468Throughput (million ops/sec)RH WH WO (b)LLT\n1.5 2.0 2.5 3.0\nÎ²51015Throughput (million ops/sec)RH WH WO (c)LGN\n1.5 2.0 2.5 3.0\nÎ²510152025Throughput (million ops/sec)RH WH WO (d)YCSB\nFigure 15: The Effect of ğ›½\nstate-of-the-art methods on synthetic datasets with extremely non-\nlinear and non-uniform distribution, which verifies the effectiveness\nof our method. In fact, even on wired and irregular datasets, our\nadjustment strategy can quickly balance the index tree and make\nthe lookup and insert operations behave well in most cases. Besides,\nthe adjustments have lower complexity and often happen on a\nrelatively small collection of data, and thus do not incur too much\noverhead.\nOnly on the datasets generated by the exponential functions\ndoes LIPP with the default kernel function behave worse than ART.\nButLIPP is still better than other baselines. In fact, the exponential\ngenerating functions make the first part of the generated dataset\ndensely distributed, which causes repeated key collisions and ad-\njustments. Thus, LIPP on datasets generated by the exponential\nfunction behaves relatively worse. Moreover, the datasets generated\nby the exponential function make ALEX frequently shift element to\ngenerate gaps for inserted elements and split nodes to adapt to the\ndenser part of datasets.B.2 The Effect of Parameters\nTo choose the best values of parameters, we also give an analysis\nfor parameters in LIPP , i.e., the threshold for conflict ratio ğ›¼and\nthe threshold for expansion ratio ğ›½mentioned in Section 4.3.1. The\nresults in Figure 14 and 15 show that the performance of LIPP\nusing the default values for these parameters consistently is better\nthan the performance of LIPP using other values even on different\ndatasets and workloads, which proves the optimality of default\nvalues that LIPP chooses.\nThough the default values of ğ›¼andğ›½can bring some perfor-\nmance gains, we find that LIPP is not very sensitive to these param-\neters. In Figure 14, we find that there is at most 20% performance\ndegradation for ğ›¼=0.2compared with the case when ğ›¼=0.1. The\nsimilar trend of ğ›½is shown in Figure 15 as well. Therefore, these\nsituations indicate the stability of LIPP .\n\nLIPP ALEX B+Tree ART\nLTD 2.04 2.00 7.00 5.93\nLLT 2.99 2.23 7.00 6.20\nLGN 2.11 1.99 7.00 4.45\nYCSB 1.63 1.00 7.00 4.46\nTable 6: The Average Tree Height\nB.3 The Analysis for ALEX\nWe study the behaviour of ALEX to further demonstrate why ALEX\nbehaves better than LIPP . In Table 4a, we show the average number\nof memory accesses used in the exponential search (Search) for\nALEX on read-only workloads and in the elements shifting (Shift)\nto make gaps for ALEX on write-only workloads. The results show\nthat ALEX costs about 30% memory accesses on the exponential\nsearch in leaf nodes for lookup operations. As for insert operations,\nALEX costs more than 50% memory accesses in shifting elements\nand managing gapped array on most of datasets. However, those\nmemory accesses for ALEX are not necessary for LIPP . In fact, LIPP\nsaves the cost for those unnecessary memory accesses due to the\nprecise position properties, thus having better performance.\n(a)# Internal Memory Ac-\ncesses\nSearch Shift\nLTD 2.82 6.97\nLLT 2.15 11.95\nLGN 1.99 10.28\nYCSB 3.30 3.66(b)% Structure Modifications\n% Modification\nLTD 12.7%\nLLT 14.7%\nLGN 13.8%\nYCSB 14.0%\nTable 5: The Analysis for ALEXIn Table 4b, we show the ratio of time spent on structure modi-\nfications to the overall running time for ALEX only on write-only\nworkloads. The structure modifications for ALEX include four parts:\nExpand Scale, Expand Retrain, Split Sideways and Split Downwards.\nDetailed information can be found in paper [ 11]. Compared with\nTable 4, we find the ratios of time spent on the adjustments for LIPP\nand the structure modifications for ALEX are comparable, which\nmeans that the existing learned indexes still cost much time on\nthe rebuilding or structure modifications. Besides, since the overall\nrunning time of ALEX is much larger than that of LIPP , the time\nspent on the adjustments for LIPP is less than the time spent on\nthe structure modifications for ALEX on most datasets and work-\nloads, even though the ratio for LIPP is larger. Thus, compared with\nALEX , the cost of adjustments for LIPP is not the bottleneck and is\nacceptable for users.\nB.4 The Average Tree Height\nFinally, Table 6 shows the tree height of different indexes on the\nwrite-only workloads, i.e., after inserting all elements from scratch.\nThe tree height is averaged over keys. Since PGM uses logarithm\nmethods to support insertions and has different trees, we simply\nignore the tree height for PGM here. Learned Index does not support\ninsert operations and cannot run on the write-only workloads. Thus,\nwe also ignore the comparison with Learned Index .\nThe results of LIPP andALEX show that datasets which are\neasier to model result in lower tree height. Besides, the tree height\nofLIPP is comparable with that of ALEX , which verifies that the tree\nheight is not the essential factor for memory accesses and overall\nperformance of LIPP andALEX . Moreover, we find that the tree\nheight of LIPP is far less than that of B+Tree andART. Thus, when\ntraversing the index trees, LIPP reach out the leafs more quickly\nthan B+Tree andART, and has fewer memory accesses and better\noverall performance. Also, B+Tree andART need to search the keys\nin nodes of different layers, which incur more overheads.",
  "textLength": 86689
}