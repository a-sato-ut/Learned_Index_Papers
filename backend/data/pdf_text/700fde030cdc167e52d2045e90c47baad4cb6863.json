{
  "paperId": "700fde030cdc167e52d2045e90c47baad4cb6863",
  "title": "There is No Such Thing as an \"Index\"! or: The next 500 Indexing Papers",
  "pdfPath": "700fde030cdc167e52d2045e90c47baad4cb6863.pdf",
  "text": "There is No Such Thing as an â€œIndexâ€!\nor:\nThe next 500 Indexing Papers\nJens Dittrich\nSaarland Informatics Campus\njens.dittrich@bigdata.uni-\nsaarland.deJoris Nixâˆ—\nSaarland Informatics Campus\njoris.nix@bigdata.uni-saarland.deChristian SchÃ¶nâˆ—\nSaarland Informatics Campus\nchristian.schoen@uni-saarland.de\nABSTRACT\nIndex structures are a building block of query processing and com-\nputer science in general. Since the dawn of computer technology\nthere have been index structures. And since then, a myriad of index\nstructures are being invented and published each and every year.\nIn this paper we argue that the very idea of â€œinventing an indexâ€\nis a misleading concept in the first place. It is the analogue of â€œinvent-\ning a physical query planâ€. This paper is a paradigm shift in which\nwe propose to drop the idea to handcraft index structures (as done\nfor binary search trees over B-trees to any form of learned index)\naltogether. We present a new automatic index breeding framework\ncoined Genetic Generic Generation of Index Structures (GENE) . It\nis based on the observation that almost all index structures are\nassembled along three principal dimensions: (1) structural building\nblocks, e.g., a B-tree is assembled from two different structural node\ntypes (inner and leaf nodes), (2) a couple of invariants, e.g., for\na B-tree all paths have the same length, and (3) decisions on the\ninternal layout of nodes (row or column layout, etc.). We propose a\ngeneric indexing framework that can mimic many existing index\nstructures along those dimensions. Based on that framework we\npropose a generic genetic index generation algorithm that, given a\nworkload and an optimization goal, can automatically assemble and\nmutate, in other words â€˜breedâ€™ new index structure â€˜speciesâ€™. In our\nexperiments we follow multiple goals. We reexamine some good old\nwisdom from database technology. Given a specific workload, will\nGENE even breed an index that is equivalent to what our textbooks\nand papers currently recommend for such a workload? Or can we\ndo even more? Our initial results strongly indicate that generated\nindexes are the next step in designing index structures.\n1 INTRODUCTION\n1.1 Problem 1: Indexes are considered\nmonolithic entities\nWhen we database researchers talk about indexes, we use the term\nindex like referring to an entity of its own. But is that the case?\nLetâ€™s look at our good old B-tree: A B-tree index consists of two\ndifferent node types: inner nodes andleaves . Inner nodes keep point-\ners to other nodes. The main purpose of an inner node is to route\nincoming lookups to other nodes. In addition, a B-tree index al-\ngorithmically preserves a couple of invariants, e.g. all paths from\nthe root to a leaf have the same lengths, each node only has one\nparent node (i.e. nodes are structurally organized into a tree), and\nso forth. In addition, all nodes keep data in a specific layout (row or\nâˆ—Both authors contributed equally to this work.column layout, cache-and SIMD-efficient layouts, etc.) and define\nwhich search algorithm to use inside a node (binary search, interpo-\nlation, prediction, etc.). Since the publication of the original B-tree\npaper [ 6] almost 50 years ago, the physical organization of B-trees\nhas been improved in a zillion different ways, e.g. [30, 42, 43, 46].\nBut what concretely is the entity â€œthe indexâ€ in here? So far\nwe only defined two different node types pointing to each other,\nwe added a couple of constraints (fan-outs, tree-structure, concrete\nphysical organization of inner nodes and leaves). We may also add\nheuristics for invariant maintenance (split and merge). But, if we\nchange any aspect of this, do we receive a completely different\nindex? When is it just a variant of an existing index? And when\nis it a new index? For instance, if we change constraints to allow\nnodes to have more than one parent, would that be a completely\ndifferent index entity? Or is it just that one constraint that changes\n(with possible implications to other features of the index)?\nIn this paper, we will introduce the idea of logical and physical\nindexes. We will show that most existing indexes can be expressed\nas a specific configuration in a generic logical and physical indexing\nframework1including B-trees, radix-trees, learned indexes, and\neven extendible hashing. And those configurations can be combined\nalmost arbitrarily within the same configuration . This opens the book\nfor a myriad of hybrid â€œindexesâ€. For instance, in our framework,\none extreme of an index (say a single hash table) can smoothly be\nmorphed into another extreme (say a B-tree style index with all\nkinds of different layouts and search algorithms inside its nodes).\n1.2 Problem 2: Two completely different\nmethodologies to solve a similar problem\nIt is remarkable that there is quite a divide in databases when it\ncomes to designing efficient components of a database system like\nindex structures as opposed to designing query plans. For index\nstructures, the historic and state-of-the-art approach is to define\nsome performance goals, reason about complexities, design some-\nthing on a blackboard, and then implement it. Like that an index\n(much like any other system component) has to be designed from\nscratch and then implemented. Eventually, we receive a piece of\nsoftware that then (hopefully) serves the original purpose. In sharp\ncontrast to this, since the 70s and the seminal Selinger paper [ 48]\ndatabase researchers follow a completely different, and rather suc-\ncessful, design path when it comes to designing query plans: we\nautomatically assemble complex plans from logical and physical\noperators.\n1Note that we will not introduce this as a software framework as done in [ 11,23] but\nrather on a conceptual level.arXiv:2009.10669v2  [cs.DB]  16 Sep 2021\n\nSo why follow two completely different design approaches if at\nthe core these are similar problems? Once we are in the position\nto express an â€œindexâ€ as a configuration in a generic logical and\nphysical indexing framework, there is one question left: Why should\nwe configure indexes by hand anyways? Why should we handcraft\nwhich node type to use, which node-internal search algorithm to\nuse, which data layout, tree-levels to use, etc.?\nIf we have different components of an index which can be inter-\nchanged freely, plus options to play with, well, then we have an\noptimization problem!\nFor this reason, in this paper, we will propose a genetic algorithm\nthat, given a dataset and workload, will automatically determine a\nsuitable logical and physical index configuration.\n1.3 Problem Statement\nWe summarize the two principal problems discussed above into the\nfollowing problem statement that we will investigate in this work:\n(1)How can we generalize the most important index structures\ninto a common conceptual indexing framework?\n(2) How can we automatically breed index structures using (1).\n1.4 Contributions\nIn this paper we make the following contributions:\n(1) We introduce a generic index structure framework that makes\na clear difference between a logical and a physical indexing frame-\nwork. This is inspired by the split into logical and physical operators\nin relational and physical algebras/operators.\n(2)We present a genetic algorithm which allows us to automatically\ngenerate (breed) efficient index configurations (aka indexes).\n(3)We present an extensive experimental evaluation of our ap-\nproach demonstrating that we can both rediscover existing, previ-\nously handcrafted indexes as well as new types of hybrid indexes.\nThe paper is structured as follows: in Section 2, we introduce\nour logical generic indexing framework. After that, in Section 3, we\nintroduce our physical generic indexing framework. Both serve as\nthe basis for Section 4 where we introduce our index breeding ap-\nproach. Section 5 contrasts our approach to related work. Section 6\npresents our experimental evaluation. We will conclude and point\nout a couple of exciting future research directions in Section 7.\n2 GENERIC LOGICAL INDEXING\nFRAMEWORK\nIn this section we introduce our generic logical indexing framework.\nThe physical indexing framework is explained in Section 3.\nDescriptions of index structures tend to mix up logical ( what\nis done) and physical aspects ( how is that achieved). For instance,\nconsider the following sentence taken from a popular textbook:\nâ€œAsorted file, called the data file, is given another file,\ncalled the indexfile, consisting of key-pointer-pairs. A\nsearch keyK in the indexfile is associated with a pointer\nto adata- filerecord that has search keyK\".\n[Section â€œ13.1 Indexes on Sequential Filesâ€ in â€œDatabase\nSystems â€” The Complete Bookâ€ [21]]In this sentence the logical aspects of the index (black underlines,\ne.g. sorted, key, record) and the physical aspects of the index (red\nunderlines, e.g. file, pointer) are introduced at the same time and\nthus mix up both aspects in the same explanation. In a way this\nviolates physical data independence of the index structure. We want\nto clearly separate the logical and physical aspects of an index.\nBasic Definitions. Any expression ğœğ‘ƒ(ğ‘…)whereğ‘ƒis a predicate\ndefined on a relational schema [ğ‘…]:{[ğ´1:ğ·1,...,ğ´ğ‘›:ğ·ğ‘›]}, i.e.,\na functionğ‘ƒ:[ğ‘…]â†¦â†’{ true,false}, is called a query onğ‘…. The result\nof a query is ğœğ‘ƒ(ğ‘…)âŠ†ğ‘…. Given[ğ‘…]with an attribute ğ´ğ‘–with a\ncorresponding non-categorical one-dimensional domain ğ·ğ‘–, and\ntwo constants ğ‘™,â„âˆˆğ·ğ‘–,ğ‘™â‰¤â„,ğœğ‘™â‰¤ğ´ğ‘–â‰¤â„(ğ‘…)is arange query onğ‘…. It\nselects all tuples ğ‘¡=(ğ‘1,..,ğ‘ğ‘–,..,ğ‘ğ‘›)âˆˆğ‘…whereğ‘ğ‘–is contained in\nthe interval[ğ‘™;â„]. A range query with ğ‘™=â„is called a point query.\n2.1 Logical Nodes and Logical Indexes\nDefinition 2.1. Logical Node. A logical node is a tuple (p,RI,DT)\nwhere:\n(1) p :[ğ‘…]â†’ğ·is apartitioning function on the schema[ğ‘…]of\nthe dataset to index, (p may be undefined),\n(2)RI is the routing information . It is a function ğ‘…ğ¼:ğ·â†’P(ğ‘)\nwhereğ‘is a set of nodes and P(ğ‘)is the power set of ğ‘. In other\nwords, each element of ğ·(the target domain of p) is mapped to\na subset of the nodes in ğ‘. For each outcome of the partitioning\nfunction pwe can find a set of associated nodes or the empty set.\nNotice that the routing information does neither imply nor assume\na specific physical organization including a sort order on its entries\n(like in B-trees). RI may be undefined. In the following, we use\nnodes(RI) for the set of nodes mapped to by RI.\n(3)DT is the data . It is a set of tuples with relational schema [ğ‘…],\nDT may be empty2.\nâ€¦logical noderouting information RIdata DT{(2,A),(1,B)}â€¦â€¦partitioning function pp(t):=t.eÂ modÂ 5set of nodes Nâ€¦\n42031â€¦noderouting table RIdata DT{42, 9, 4, 8}â€¦â€¦â€¦partitioning function pp0(t):=t.aÂ modÂ 5set of nodes Nâ€¦old version:{4, 2, 0, 1}\nâ€¦\nFigure 1: An example of a logical node with a hash-style par-\ntitioning function, four mappings in the routing informa-\ntion RI, and two tuples in the data part.\nFigure 1 visualizes the principal structure of a logical node. The\npartitioning function ğ‘computesğ‘¡.ğ‘’mod 5 which yields a domain\nğ·={0,1,2,3,4}. Here, only a subset of ğ·is shown in the visu-\nalization of RI, i.e. 3 is not shown as it maps to the empty set. In\naddition, RI maps 2 and 0 to the same node. Moreover, the data part\nDT contains two tuples (2,ğ´)and(1,ğµ).\nDefinition 2.2. Complete Logical Index. Let ğ¿ğ‘be a set of logical\nnodes withâˆ€ğ‘›âˆˆğ¿ğ‘:ğ‘›ğ‘œğ‘‘ğ‘’ğ‘ (ğ‘›.ğ‘…ğ¼)âŠ†ğ¿ğ‘. Then the graph ğœ†=(ğ¿ğ‘)\nis called a complete logical index.\nIn other words, only if all routing information in the nodes of\nğ¿ğ‘points to nodes contained in ğ¿ğ‘, we callğ¿ğ‘a complete logical\n2In principle, DT could also be defined as a similar function as RI the difference being\nthat RI maps to nodes whereas DT maps to tuples. However, to simplify matters a bit,\nwe stick to a set definition at this point. Also note that the DT-fields can be used to\nvery naturally support buffer-tree-style indexes [ 3], bulkloading mechanisms [ 12] as\nwell as any form of recursive partitioning algorithm.\n2\n\nRIDT{}pt.e{(-;6), [6;11), [11;+)}âˆâˆ\nb-tree with ISAM\n{(1,B), (2,A)}{[6;+)}âˆâˆ’pRIDT\n{(7,B), (6,C)}{(-;6),[11;+)}âˆâˆâˆ’pRIDT\n{(11,C), (12,Z)}{-;11)}âˆâˆ’pRIDT(a)B-tree with ISAM: Here the partitioning function returns ğ‘¡.ğ‘’. The routing infor-\nmation maps ranges to nodes on the next level. This induces a B-tree-style partitioning.\nNotice that the common textbook explanation of B-trees showing ğ‘˜pivots andğ‘˜+1\npointers is already a specific physical implementation of this logical index. In addition,\nthis index contains entries on the leaf-level for backward and forward chaining of leaves\nas in ISAM.\nRMI\nRIDT{}p div 3t.e{0,2,3,4}\nRIDT{(1,B), (2,A)}p{}âˆ’\nRIDT{(7,B), (6,C)}p{}âˆ’\nRIDT{(11,C)}p{}âˆ’\nRIDT{(12,Z)}p{}âˆ’old:\n13â‹…t.e01234ï¬‚oor(p(t))pRIDT{}\n{(1,B), (2,A)}âˆ’p{}RIDT\n{}âˆ’p{}RIDT\n{(7,B), (6,C)}âˆ’p{}RIDT\n{(11,C)}âˆ’p{}RIDT\n{(12,Z)}âˆ’p{}RIDT\n(b)RMI: Here the partitioning function is a linear function ğ‘(ğ‘¡)=1\n3Â·ğ‘¡.ğ‘’+0that\nsqueezes the data into a smaller range ([0;12] â†’[0;4]). This is equivalent to a linear\nregression over the key space. RI groups the data into bins (corresponding to nodes on\nthe next level). However, ğ‘and RI can be set to use any form of regression method and\nfor any node independently.\nextensible hashing(2,A),(7,B),(1,B),(6,C), (12,Z),(11,C) (0010,A),(0111,B),(0001,B),(0110,C), (1100,Z),(1011,C) data:binary:(0010,A),(0111,B),(0001,B),(0110,C), (1100,Z),(1011,C) (0010,A), (0110,C) (0001,B)\n & 0x7t.e{001,010,011,100,110,111}\n{(0111,B) (1011,C)}\nlocal depth = 2\nlocal depth = 2\nlocal depth = 3\nlocal depth = 3\nglobal depth = 3\npRIDT{}âˆ’p{}RIDT\n{(0010,A), (0110,C)}âˆ’p{}RIDT\n{(0001,B)}âˆ’p{}RIDT\n{(1100,Z)}âˆ’p{}RIDT\n(c)extendible hashing: Here the partitioning function only considers a suffix of the\nlowest three bits (&0x7) of ğ‘¡.ğ‘’. This implies that it partitions exactly like an extendible\nhashing [ 16] directory with global depth of three. Note that there is no need to create\nentries for empty â€˜bucketsâ€˜.\nradix tree(1100,Z)\n & 0xCt.e{00,01,10,11}\n(0001,B)(0110,C)(1011,C))(0010,A)(0111,B)\n{01,10}pRIDT{}pRIDT{} & 0x3t.e\n{10,11}pRIDT{} & 0x3t.e\n{(1011,C)}âˆ’p{}RIDT\n{(1100,Z)}âˆ’p{}RIDT\n{(0001,B)}âˆ’p{}RIDT\n{(0010,A)}âˆ’p{}RIDT\n{(0110,C)}âˆ’p{}RIDT\n{(0111,B)}âˆ’p{}RIDT\n(d)radix tree: Here the partitioning functions partition the dataset on two adjacent\nbits each: the root-node partitions on the first two bits of the prefix, the next level on\nthe next two bits. This induces a radix-partitioning. Note that in this example the index\nis configured to keep at most one tuple per leaf. This can of course be configured. So\nalternatively, we could force a two-level tree just partitioning on the first two bits. The\nsecond level would then keep multiple entries in their DT-fields.\nFigure 2: The modeling power of our logical indexing frame-\nwork for traditional indexes. Four special cases of possi-\nble logical indexes for the running example. All examples\nmimic existing and handcrafted (physiological) index struc-\ntures.\nindex. At first, this definition sounds a bit trivial, but this definition\nmakes an important observation that is frequently overlooked: a\nlogical index is-agraph of logical nodes â€” and nothing else .\nRunning Example. Figure 2 illustrates the modeling power of our\nframework and shows four possible logical indexes for a running\nexample[ğ‘…]={[ğ‘’:int,ğ‘”:char]}.ğ‘…={(2,A),(7,B),(1,B),(6,C),\n(12,Z),(11,C)}. Notice that in all these examples the DT-fieldsare empty for internal nodes. The implications of allowing data\nin internal nodes are however considered future work and will\ntherefore not be investigated in this paper. Figure 3 demonstrates\nhow we can model arbitrary â€˜hybridâ€™ logical indexes.\nRMI-style indexextendible hashing-style indexradix-style indexB-tree-style index\nhybrid logical index\nt.e{(-;6), [6;11), [11;+)}âˆâˆ\nt.g{A, B}\n{(11,C)}\n{(12,Z)}\n{(0111,B)} \n{(0110,C)}\n{(2,A)}\n{(1,B)}\npDT{}RIRIDT{}pâˆ’pâˆ’pâˆ’p{}RI{}RI{}RI{}RI{}RIâˆ’pâˆ’pâˆ’p\n & 0x7t.e{110,111}RIDT{}pDT{}13â‹…t.e01234ï¬‚oor(D)RIp{}RIDTDTDTDTDTDT\nFigure 3: The modeling power of our logical indexing frame-\nwork for any form of â€˜hybridâ€™ index. The example combines\nproperties from four different traditional index structures.\nNotice that this is just one of countless possible examples:\nany node in this logical index may be exchanged by any\nother suitable logical node as long as the data in the index is\npartitioned in a way that all possible queries on the logical\nindex return the correct result set. On this abstraction level\nit is still undefined how data is represented in the different\nnodes and in particular in the RI-function and the DT-set\nandhow we search.\n2.2 Logical Queries\nDefinition 2.3. RQ: Result of a Range Query on a Logical Index.\nGiven a range query with predicate ğ‘ƒ:=ğ‘™â‰¤ğ´ğ‘–â‰¤â„, a logical index\nğœ†build upon a relation ğ‘…and a non-empty start node-set ğ‘†ğ‘âŠ†ğ¿ğ‘,\nthe result set of the range query is given by:\nRQ(ğ‘ƒ,ğ‘†ğ‘):=Ã˜\nğ‘›âˆˆğ‘†ğ‘\u0012\nğœğ‘ƒ(ğ‘›.ğ·ğ‘‡)\n|     {z     }\ndata inğ‘›âˆªRQ\u0010\nğ‘ƒ,Ã˜\nğ‘¡âˆˆğ‘…,ğ‘™â‰¤ğ‘¡.ğ´ğ‘–â‰¤â„ğ‘›.RI\u0000ğ‘›.ğ‘(ğ‘¡)\u0001\u0011\u0013\npRIDT{(1,B), (2,A)}âˆ’{}\npRIDT{(7,B), (6,C)}âˆ’{}\npRIDT{(11,C), (12,Z)}âˆ’{}logical index\nRIDT{}pt.e{(-;6), [6;11), [11;+)}âˆâˆspecify\nRIDT{}pt.e{(-;6), [6;11), [11;+)}âˆâˆDL: col, sortedSAlg: scan \npDL: col, unsortedRIDTSAlg: scan{(1,B), (2,A)}âˆ’{}\nspecify\nRIDT{}pt.e{(-;6), [6;11), [11;+)}âˆâˆDL: col, sortedSAlg: binS\npDL: col, unsortedRIDTSAlg: expS{(1,B), (2,A)}âˆ’{}\npDL: row, sortedRIDTSAlg: expS{(7,B), (6,C)}âˆ’{}\npDL: row, unsortedRIDTSAlg: hashS{(11,C), (12,Z)}âˆ’{}\nRIDT{}pt.e{(-;6), [6;11), [11;+)}âˆâˆDL: funSAlg: hashSspecifyphysical index\nphysical indexphysical index\npDL: col, unsortedRIDTSAlg: expS{(1,B), (2,A)}âˆ’{}\npDL: row, sortedRIDTSAlg: expS{(7,B), (6,C)}âˆ’{}\npDL: row, unsortedRIDTSAlg: hashS{(11,C), (12,Z)}âˆ’{}\npDL: row, sortedRIDTSAlg: expS{(7,B), (6,C)}âˆ’{}\npDL: row, unsortedRIDTSAlg: hashS{(11,C), (12,Z)}âˆ’{}\nFigure 4: The arrows show some possible transitions from a\nlogical to a physical index (we specify an algorithm and/or\na data layout). Notice that neither the partitioning tree nor\nthe assignment of data to nodes are changed in this process.\nNotice that the set semantics will implicitly remove duplicates\nwhich in a physical graph-structured index (possibly not obeying\nset semantics) may result from visiting nodes multiple times.\nAlso note that this query will recursively traverse the graph\nfor all qualifying nodes in the RI-fields. This is fine for a strictly\ntree-structured index, however, as soon as we do not have a tree-\nstructure anymore but a more general DAG, it may become possible\n3\n\nthat, given a set of start nodes ğ‘†ğ‘, certain nodes are reachable via\nmultiple paths. For a general graph, the implementing algorithm\nhas to be modified to not visit nodes multiple times.\nDefinition 2.4. Correctness of a Logical Index. Letğœ†=(ğ¿ğ‘)be a\ncomplete logical index. Let ğ‘†ğ‘be an arbitrary non-empty subset\nof start nodes: ğ‘†ğ‘âŠ†ğ¿ğ‘. Letğ·ğ‘‡ğœ†:=Ã\nğ‘›âˆˆğ¿ğ‘ğ‘›.ğ·ğ‘‡ be the data\ncontained in ğœ†. Letğœğ‘ƒ:=ğ‘™â‰¤ğ´ğ‘–â‰¤â„(ğ‘…)be a range query onğ‘…. If\nâˆ€ğ‘™,â„:ğœğ‘™â‰¤ğ´ğ‘–â‰¤â„(ğ·ğ‘‡ğœ†)=RQ(ğ‘ƒ,ğ‘†ğ‘),\nthenğœ†is called a correct logical index w.r.t ğ‘†ğ‘.\nNotice that the correctness of an index depends on whether\ndata is placed into the different DT-sets according to the properties\nof the different partitioning functions used at the various nodes.\nFurthermore, the start nodes ğ‘†ğ‘must be chosen such that all qual-\nifying data can be reached by the range query. For instance, in a\ntree-structured index picking the start node is trivial: we call it\nâ€˜the root nodeâ€™. In a general graph structure, which may even be\ndisconnected, things can become more complex, i.e. we might have\nmultiple â€˜root nodesâ€™, i.e. all nodes that cannot be reached from any\nother node of the index, or even no root nodes (in case of a cyclic\ngraph). This discussion is beyond the scope of this paper and there-\nfore in the following, we will only consider correct, DAG-structured\nindexes and assume that ğ‘†ğ‘is chosen accordingly.\n3 GENERIC PHYSICAL INDEXING\nFRAMEWORK\nAs we just have defined logical indexes (our counterparts to the\nlogical relational algebra operators), now, we can proceed to devise\nphysical indexes (our counterparts to physical operators).\nFor each logical node ,and for each of its RI and DT-part we even-\ntually have to specify how to realize it. We do this by making a\nphysical decision on the search algorithm (Section 3.1) and the\ndata layout to use for that set (Section 3.2). Or, we delegate those\ndecisions by using a nested index (Section 3.3).\nAny index where for all its nodes the data layouts and algorithms\nare sufficiently specified, is called a physical index .\n3.1 Specify Search Algorithm\nWe decide which search algorithm to use for searching (key/value)-\npairs in RI and/or DT. Note that all search algorithms stop once a\nqualifying key was found, i.e. we found the corresponding entry in\nRI or we have an exact key match in DT. The principal options are\nas follows: (1) scan: linear search through all entries, for each key\ncheck if it qualifies, (2) binS: binary search (3) intS: interpolation\nsearch, iteratively compute slope and intercept, i.e. a linear function,\nforleftandright key, predict key location pred and reduce search\narea to [left, pred] or (pred, right] respectively until keyqualifies.\n(4) expS: exponential search, start with the first entry, increase ex-\nponentğ‘–for key position specified by 2ğ‘–until keyis greater than the\nsearch value, use binary search (or any other suitable method) inside\nrange [ 2ğ‘–âˆ’1, end]. (5) hashS: chained hashing (or any other suitable\nhashing variant), use the underlying hash function to compute the\nlocation of the key(and its associated mapping). (6) linregS: linear\nregression (or any other form of approximation and/or learning),\ncompute slope and intercept, i.e. linear function, for all data points,compute error bounds, predict key location pred and use linear\nsearch (or any other suitable error correction method) inside [pred\n- lower error bound, pred + upper error bound]. (7) hybridS: any\nsuitable hybrid algorithm (i.e. a composite of the former options).\n3.2 Specify Data Layout\nWe decide which data layout to use for representing the data from RI\nand/or DT. To define a data layout, we have to specify the following:\n(1) col vs row: key/value-pairs are in row or col layout. (2) func:\nwe use a function to specify the RI and/or DT-mapping, thus we do\nnot need to represent pivots and/or data and therefore do not need\na data layout. As discussed in Definition 2.1 already, we assume\nthe DT-fields to be actual sets even though they could be modeled\nas a more general mapping as well. (3) unsorted vs sorted: the\nentries are (or are not) sorted by their key. (4) comp: the entries\nare compressed (and how exactly, i.e. which compression method).\n(5) hybridDL any suitable hybrid data layout (i.e. any composite of\nthe former options). Notice that some of these data layout decisions\ncannot be made independently from the search algorithms to use,\ne.g. binary search implies a sorted data layout. Figure 4 shows an\nexample of a logical index that by specifying the search algorithms\nand data layouts may be transformed into different physical indexes.\n3.3 Specify by Nested Logical or Physical Index\nWe make a decision to specify RI and DT by a nested physical index.\nNotice that this is not equivalent to the recursively reachable set of\nnodes pointed to by one particular RI. Nesting is about representing\nthe key/value-lookup search algorithms and data layout inside a\nnode by another index. For instance, consider a physical binary\nsearch tree (BST). If we use such BST to represent and search RI,\nwe basically have a nested physical index in our node. However,\nthis is just a special case, so in theory we can allow for arbitrary\nnested indexes at this point.\n4 GENETIC INDEX BREEDING\nAs we just have defined our logical and physical generic indexing\nframeworks, we proceed to present our genetic algorithm allowing\nus to automatically generate indexes. This is structured as follows:\n(1) Core algorithm (Section 4.1),\n(2) Initial population generation (Section 4.2),\n(3)The set of applicable mutations describing possible changes to\nindividual logical and physical index structures (Section 4.3), and\n(4)The fitness function used to measure the performance of indi-\nvidual physical index structures (Section 4.4).\nThe major challenge with a generic indexing framework pre-\nsented in Section 3 is the intractable search space. Therefore, we\nneed an optimization method that can cope with such a huge search\nspace. Notice that an intractable search space does not imply that\nwe cannot find a good solution. In fact, entire research communities\nwork on these kind of problems including: planning, reinforcement\nlearning, and genetic optimization. We decided to design our search\nalgorithm based on genetic optimization. Genetic optimization al-\ngorithms have been developed for more than 40 years [ 24], but\nrecently gained a lot of attention due to growing computational re-\nsources. They allow researchers to effectively explore larger search\nspaces. Recent surprising, and not widely-known, results include:\n4\n\nAlgorithm 1 Genetic Search Algorithm of GENE\n1:function InitPopulation (ğ·ğ‘†,ğ‘  init)\n2: Î =âˆ… âŠ²initialize population with empty set\n3: for(ğ‘–=0;ğ‘–<ğ‘ init;ğ‘–++) do âŠ²createğ‘ initinitial indexes\n4:ğœ‹=buildAndPopulateRandomIndex (ğ·ğ‘†) âŠ²build and populate index\n5: Î =Î âˆª{ğœ‹} âŠ²add index to population Î \n6: end for\n7: return Î  âŠ²return population Î \n8:end function\n9:function TournamentSelection (Î ,ğ‘ T,ğ‘Š)\n10:ğ‘‡=sample_subset(Î ,ğ‘ T) âŠ²draw random subset ğ‘‡âŠ†Î of sizeğ‘ T\n11:ğœ‹min=arg minğœ‹âˆˆğ‘‡ğ‘“(ğœ‹,ğ‘Š)âŠ²select fittest individual ğœ‹mininğ‘‡underğ‘Š\n12: Ëœğ‘¡=median_fitness(ğ‘‡) âŠ²compute median fitness of all ğœ‹âˆˆğ‘‡\n13: return(ğœ‹min,Ëœğ‘¡) âŠ²return fittest individual ğœ‹minand median fitness Ëœğ‘¡\n14:end function\n15:function GeneticSearch (ğ‘”max,ğ‘ init,ğ‘ max,ğ‘ Î ,ğ‘ ğ‘‡,ğ‘ ch,ğ·ğ‘†,ğ‘€ğ·,ğ‘ğ·,ğ‘Š )\n16: Î =InitPopulation(ğ‘ init,ğ·ğ‘†) âŠ²initialize population\n17: for(ğ‘–=0;ğ‘–<ğ‘”max;ğ‘–++) do âŠ²performğ‘Ÿmaxiterations/generations\n18:(ğœ‹min,Ëœğ‘¡)=TournamentSelection (Î ,ğ‘ ğ‘‡,ğ‘Š)âŠ²run tournament selection\n19: for(ğ‘—=0;ğ‘—<ğ‘ max;ğ‘—++) do âŠ²createğ‘ maxmutations\n20: ğ‘š=draw_mutation(ğ‘€ğ·) âŠ²draw from mutation distribution\n21: ğ‘›=draw_node\u0000ğ‘ğ·(ğœ‹min,ğ‘š)\u0001âŠ²draw from node distribution\n22: ğ‘â„=draw_phys\u0000ğ‘ƒğ·(ğ‘š,ğ‘›)\u0001âŠ²draw from phys distribution\n23: ğœ‹mut=ğ‘š(ğœ‹min,ğ‘›,ğ‘â„) âŠ²perform mutation\n24: ifğ‘“(ğœ‹mut,ğ‘Š)â‰¤Ëœğ‘¡then âŠ²addğœ‹muttoÎ if fitter than median Ëœğ‘¡\n25: if|Î |â‰¥ğ‘ Î then âŠ²if capacity exceeded\n26: Î =Î \\arg maxğœ‹âˆˆğ‘‡ğ‘“(ğœ‹,ğ‘Š)âŠ²remove unfittest individual\n27: end if\n28: Î =Î âˆª{ğœ‹mut} âŠ²add index to population\n29: end if\n30: end for\n31: end for\n32:ğœ‹min=arg minğœ‹âˆˆÎ ğ‘“(ğœ‹,ğ‘Š)âŠ²return fittest individual of final population\n33: returnğœ‹min\n34:end function\ngenetic algorithms can rediscover state-of-the-art machine learning\nalgorithms (!) [44]. Furthermore, they can devise yet unknown math-\nematical equations [ 9]. Genetic optimization tasks are very domain\nspecific as possible mutations and the performance measure depend\nheavily on the concrete task.\n4.1 Core Algorithm\nThe general design for our algorithm follows the principal of evo-\nlution which is known from nature: We start with the main func-\ntionGeneticSearch (line 15). We start by initializing a population\nof individuals (line 16), in our case a set of physical index struc-\nturesÎ :={ğœ‹|ğœ‹is a physical index}(see function InitPopulation ,\nline 1). To create the initial population, we build and populate ğ‘ init\nphysical index structures (line 4) and add them to the population Î \n(line 5). This build process is described in more detail in Section 4.2.\nNow, we enter the central iteration: we perform ğ‘”maxiterations\nin genetic search (lines 17â€“31). We start by tournament selection\n(line 18), see function TournamentSelection (line 9). We select a\nsample of size ğ‘ Tof the current population Î (line 10) from which\nwe select the fittest index ğœ‹min(line 11). We keep a trace of the\nfitness of physical indexes to never evaluate indexes multiple times.\nWe compute the median fitness Ëœğ‘¡of sampleğ‘‡(line 12) and return\nbothğœ‹minand Ëœğ‘¡(line 13) to the GeneticSearch function (line 18).\nThen, we enter the mutation loop (line 19). The core idea is to\ncomputeğ‘ maxâ‰¥1mutations for index ğœ‹min. We draw a random\nmutationğ‘šfrom a precomputed distribution of mutations ğ‘€ğ·\n(line 20). For the mutation ğ‘šwe draw a start node ğ‘›to be used\nfor this mutation (line 21) as well as a physical implementation ğ‘â„Symbol Meaning\nğœ† logical index\nğœ‹ physical index\nÎ  population\nğ‘ init initial size of the population\nğ‘ Î  maximum number of indexes in population\nğ‘”max number of generations\nğ‘ max number of mutations created and evaluated in a single iteration\nğ‘ T size of sample in tournament selection\nğ‘ ch maximum length of a mutation chain applied in one iteration\nğ·ğ‘† dataset\nğœ‹min best individual in tournament selection\nğœ‹mut mutated element\nËœğ‘¡ median fitness\nğ‘€ğ· probability distribution of mutations\nğ‘š a single mutation\nğ‘ğ·(ğœ‹,ğ‘š) probability distribution of nodes\nğ‘ƒğ·(ğ‘š,ğ‘) probability distribution of physical implementations\nğ‘Š workload of queries\nğ‘“(ğœ‹,ğ‘Š) fitness of a physical index\nTable 1: Symbols used.\n(line 22). The mutations and distributions are described in detail in\nSection 4.3. Then, we perform the actual mutation on ğœ‹min(line 23)\nand receive ğœ‹mut. We originally also experimented with applying\nchains of mutations (lines 20 and 23) but it did not show any bene-\nfits. We check, whether the mutated index ğœ‹muthas a better fitness\nthan the median Ëœğ‘¡(line 24). If it has a better fitness, we check if\nÎ exceeds its capacity of maximum allowed physical indexes ğ‘ Î \n(line 25). If that is the case, we remove the physical index with the\nworst fitness from Î (line 26). Then we add ğœ‹mutto the population\nÎ (line 28). Once the outer loop terminates, we determine the fittest\nindex from Î (line 32) and return it.\n4.2 Initial Population Generation\nWhat is a good start population Î for the genetic algorithm? In\nAlgorithm 1, function InitPopulation (line 1), we need to define an\ninitial population of individual index structures. There are several\npossible dimensions to consider. First, we can change the initial\nnumberğ‘ initof indexes in Î . This basically defines how diverse the\ninitial set of indexes may be. Second, we should determine how\nto actually build and populate the initial physical index with data\nfrom dataset DS (line 4). There are several options:\n(1)We start with a single physical node that does not contain data,\nmutate it, and only then insert the actual data. We experimented\nwith this approach initially but discarded it quickly due to its high\ntraining costs. Thus we do not support it in our algorithm anymore.\n(2)We start with a single physical node containing all data. For\ndata layout/search method we either randomly pick it or we pick\none that we believe works well for the given workload.\n(3)We use bottom-up bulkloading with the difference that for all\nnodes the search algorithms and data layouts are picked randomly.\nIn our current version we exclude hash nodes for inner nodes as we\nhave not defined a radix-partition search method on this data layout\nyet. We will integrate this in future versions of our optimization\nframework. The resulting tree is logically similar to a standard\nB-Tree, the physical nodes however differ considerably.\n(4)We start with a population containing a physical index that\nresembles a state-of-the-art hand-tuned index, i.e. we define the\nlogical index (including its partitioning functions) as well as the\nphysical nodes. Then we check whether we can still improve that\nindex through our genetic algorithm.\n5\n\nNotice that for options from (1) to (4) increasing, we postulate\nthat we take away load from GENE, using it increasingly as a refine-\nment tool: The more we start with something already representing\na very efficient (or fit, however fitness is defined) index, the more\nwe expect that only small mutations will be performed by GENE.\nAt least that is what we would believe. In fact, even if we (non-\nrandomly) specify an initial physical index to start with, recall, that\nGENE has all degrees of freedom to pick mutations, and may sur-\nprise us by taking unexpected turns and make different decisions.\n4.3 Mutations and their Distributions\nIn this section we introduce a suitable set of mutations and discuss\nhow they are used in our algorithm.\nMutation. In our framework, a mutation is a function ğ‘š:Indexâ†’\nIndex. A mutation takes a single index as input, mutates it, and\nreturns a modified index. By â€˜Indexâ€™ we mean, that either a logical\nindex (ğœ†)ora physical index ( ğœ‹) is given and a mutated index is\nreturned (ğœ†mutorğœ‹mut).ğœ†mutandğœ‹mutmust preserve the correct-\nness ofğœ†andğœ‹. This is inspired by rewrite rules in classical query\noptimization: there we also only consider rules that are guaranteed\nto not change the query result. We will only consider mutations\non tree-structured indexes. This is not a restriction of our generic\nframework but makes the following mutations a bit more digestible.\nMutation distributions. We use a probability distribution ğ‘€ğ·al-\nlowing us to assign different probabilities to the different mutations\n(line 20), e.g. we can give higher probabilities to certain mutations.\nGiven a mutation ğ‘šand a physical index ğœ‹minwe then draw from\na second distribution ğ‘ğ·(ğœ‹min,ğ‘š)to determine the nodes to use\nfor this mutation (line 21). Now, we draw from a third distribution\nğ‘ƒğ·(ğ‘š,ğ‘)to determine which physical implementation to use for\nthis mutation and node set. Setting probabilities to zero within\nthis distribution ğ‘ƒğ·(ğ‘š,ğ‘)excludes combinations of physical data\nlayout and search method which are invalid, e.g. binary search on\nunsorted data layouts. Note that these distributions can be created\nbased on microbenchmarks.\nFundamental Mutations. Our goal is to implement a minimal\nset of mutations that allow for breeding a huge variety of physical\nindex structures.\nM1Change data layout: Fromğ‘›, we randomly select either its RI-\norDT-part. Then we create a new physical node ğ‘›â€²with data layout\nğ‘›â€².ğ‘‘ğ‘™â‰ ğ‘›.ğ‘‘ğ‘™ drawn from ğ‘ƒğ·(ğ‘š,ğ‘)with the same data and routing\ninformation as ğ‘›:ğ‘›â€².ğ·ğ‘‡=ğ‘›.ğ·ğ‘‡âˆ§ğ‘›â€².ğ‘…ğ¼=ğ‘›.ğ‘…ğ¼. The options for data\nlayouts are described in Section 3.2. If ğ‘›contains child partitions,\nwe enforce the additional condition ğ‘›.ğ‘‘ğ‘™â€²â‰ hash , as our software\nframework does not (yet) support child partitions in nodes with a\nhash layout. In ğœ‹, we replace ğ‘›byğ‘›â€². Ifğ‘›â€².ğ‘ is incompatible with\nğ‘›â€².ğ‘‘ğ‘™, we draw a new method from ğ‘ƒğ·(ğ‘š,ğ‘)to ensure correctness.\nFigure 5(a) shows an example: the input node ğ‘›has a sorted column-\nlayout. In the index, we replace ğ‘›byğ‘›â€²which has a tree-layout.\nM2Change search method: Fromğ‘›, we randomly select either\nits RI- orDT-part. Given the existing search method ğ‘›.ğ‘ âˆˆğ‘†:=\n{scan,binS,intS,expS,linRegS,hashS}, we draw an ğ‘ â€²âˆˆğ‘†withğ‘ â€²â‰ \nğ‘ fromğ‘ƒğ·(ğ‘š,ğ‘). Then we create a new physical node ğ‘›â€²with the\nnew search method ğ‘ â€²with the same data and routing information\nasğ‘›:ğ‘›â€².ğ·ğ‘‡=ğ‘›.ğ·ğ‘‡âˆ§ğ‘›â€².ğ‘…ğ¼=ğ‘›.ğ‘…ğ¼. Figure 5(b) shows an example:\nthe input node ğ‘›uses a scan as search method. In the index, we\nreplaceğ‘›byğ‘›â€²which uses binary search.M3Merge sibling nodes horizontally: We set node ğ‘›parent :=ğ‘›\nwhose RI maps to at least one other node in ğœ‹, if not we abort\nthis mutation. From the set of nodes mapped to by ğ‘›parent we\nrandomly select a child node ğ‘›targetâˆˆnodes(ğ‘›parent .RI). We se-\nlect a non-empty subset ğ‘sourcesâŠ†nodes(ğ‘›parent .RI)of nodes\nto merge into ğ‘›target using the following restrictions: ğ‘›target âˆ‰\nğ‘sourcesâˆ§âˆ€ğ‘›âˆˆğ‘sourcesğ‘›.ğ‘=ğ‘›target.ğ‘. This implies that the source\ndomain of the routing information function ğ·is equal for all nodes\ninğ‘sourcesâˆª{ğ‘›target}. We then need to perform updates on two\nlevels of the index: The node ğ‘›target that we merge with and the\nparent node ğ‘›parent . We start by describing the updates to the node\nğ‘›target . First we update the data ğ‘›target .DT and set it to the union\nof all data within the merged nodes:\nğ‘›â€²\ntarget.DT=ğ‘›target.DTâˆªÃ˜\nğ‘›âˆˆğ‘sourcesğ‘›.DT.\nIn the following, we also update the routing information function\nğ‘›target.RI such that\nâˆ€ğ‘‘âˆˆğ·ğ‘›â€²\ntarget.ğ‘…ğ¼(ğ‘‘)=ğ‘›target.RI(ğ‘‘)âˆªÃ˜\nğ‘›âˆˆğ‘sourcesğ‘›.RI(ğ‘‘),\nwhereğ·is the common domain of the RIs in ğ‘sourcesâˆª{ğ‘›target}.\nThis ensures that our target node ğ‘›target now maps to all child\nnodes that any node ğ‘›âˆˆğ‘sources previously mapped to, i.e. we can\nstill reach all child nodes. For the parent node ğ‘›parent we have to\nupdate the routing information ğ‘›parent .RI such that\nâˆ€ğ‘‘âˆˆğ‘›ğ·parentâˆ€ğ‘›âˆˆğ‘sourcesğ‘›âˆˆğ‘›parent.ğ‘…ğ¼(ğ‘‘)\nâ‡’ğ‘›parent.ğ‘…ğ¼(ğ‘‘)={ğ‘›target}âˆªğ‘›parent.ğ‘…ğ¼(ğ‘‘)\\{ğ‘›}.\nIn other words: We remove all mappings to merged nodes ğ‘›âˆˆ\nğ‘sources and replace them with a new mapping to the node ğ‘›target .\nNotice that the merge operation performed in B-trees is essentially\njust a specialized version of this general merge mutation. In a B-tree\nthe number of merged nodes ğ‘˜is typically set to ğ‘˜=2and the nodes\nmust be directly neighboring due to the sorted key domain. For\nour actual implementation, we also restrict ourselves similarly to\nmerges where|ğ‘sources|=1. Merge operations with larger source-\nsets can easily be achieved by recursively executing the merge\noperation on the same node. Figure 5(c) shows an example: the set\nğ‘sources contains a single leaf that we want to merge into ğ‘›target .\nTo achieve this we first merge all data contained in ğ‘sources .DT into\nğ‘›â€²\ntarget.DT. Asğ‘sources .RI is empty, we do not have to do anything\nhere. Inğ‘›parent .RI, we need to remove the mapping to all nodes in\nğ‘sources , in this case the key-range [6; 11)âŠ‚ğ·must be changed\nto map toğ‘›â€²\ntarget. For this example this is equivalent to merging\nthe old entry(âˆ’âˆ ; 6)with[6; 11)into(âˆ’âˆ ; 11). Now, all nodes in\nğ‘sources can be removed from the index.\nM4Split child node horizontally into k nodes: This is the in-\nverse mutation of M 3. Figure 5(d) shows an example.\nM5Merge sibling nodes vertically: We set node ğ‘›parent :=ğ‘›\nwhose RI maps to at least one other node in ğœ‹, if not we abort this\nmutation. From the set of nodes mapped to by ğ‘›parent we randomly\nselect a child node ğ‘›childâˆˆnodes(ğ‘›parent .RI)using the following\nrestriction: ğ‘›child.ğ‘=ğ‘›parent.ğ‘. To merge ğ‘›child intoğ‘›parent , we\nthen need to perform the following updates: First we need to move\nall data inğ‘›child.DT to the parent node:\nğ‘›parent.DT=ğ‘›parent.DTâˆªğ‘›child.DT\n6\n\nRIDT{}pt.e{(-;6), [6;11), [11;+)}âˆâˆDL: col, sortedSAlg: scan \nM1n\nRIDT{}pt.e{(-;6), [6;11), [11;+)}âˆâˆDL: tree, sortedSAlg: scan nâ€™mutate M1 (a)M1Change node type: change data layout of RI.\nRIDT{}pt.e{(-;6), [6;11), [11;+)}âˆâˆDL: col, sortedSAlg: scan n\nRIDT{}pt.e{(-;6), [6;11), [11;+)}âˆâˆDL: col, sortedSAlg: binSnâ€™mutate M2 \nM2 (b)M2Change search method: change search of RI.\nRIDT{}pt.e{(-;6), [6;11), [11;+)}âˆâˆDL: col, sortedSAlg: scan nparentmutate M3 \npDL: row, sortedRIDTSAlg: expS{(7,B), (6,C)}âˆ’{}\npDL: row, unsortedRIDTSAlg: hashS{(11,C), (12,Z)}âˆ’{}\npDL: row, unsortedRIDTSAlg: hashS{(11,C), (12,Z)}âˆ’{}\npDL: col, unsortedRIDTSAlg: scan{(1,B), (2,A)}âˆ’{}\npDL: col, unsortedRIDTSAlg: scan{(1,B), (2,A), (7,B), (6,C)}âˆ’{}\nRIDTpt.e{(-;11), [11;+)}âˆâˆDL: col, sortedSAlg: scan {}nparentâ€™ ntargetntargetâ€™ Nsources\n(c)M3Merge nodes horizontally: merge left & middle child node.\nRIDT{}pt.e{(-;6), [6;11), [11;+)}âˆâˆDL: col, sortedSAlg: scan nparentmutate M4 \npDL: row, sortedRIDTSAlg: expS{(7,B), (6,C)}âˆ’{}\npDL: row, unsortedRIDTSAlg: hashS{(11,C), (12,Z)}âˆ’{}\npDL: row, sortedRIDTSAlg: expS{(7,B), (6,C)}âˆ’{}\npDL: row, unsortedRIDTSAlg: hashS{(11,C), (12,Z)}âˆ’{}\npDL: col, unsortedRIDTSAlg: scan{(2,A)}âˆ’{}\npDL: col, unsortedRIDTSAlg: scan{(1,B), (2,A)}âˆ’{}\npDL: col, unsortedRIDTSAlg: scan{(1,B)}âˆ’{}\nRIDTpt.e{(-;2), [2,6), [6;11), [11;+)}âˆâˆDL: col, sortedSAlg: scan {}nparentâ€™ (d)M4Split node horizontally: split left child node.\npDL: col, unsortedRIDTSAlg: scan{(1,B)}{(-;2), [2;6)}âˆDL: col, unsortedSAlg: scan\nRIDT{}pt.e{(-;6), [6;11), [11;+)}âˆâˆDL: col, sortedSAlg: scan nparentnparentâ€™ \nRIDTpt.e{(-;2), [2,6), [6;11), [11;+)}âˆâˆDL: col, sortedSAlg: scan DL: row, sortedSAlg: scan {(1,B)}mutate M5 \npDL: row, sortedRIDTSAlg: expS{(7,B), (6,C)}âˆ’{}\npDL: row, unsortedRIDTSAlg: hashS{(11,C), (12,Z)}âˆ’{}\npRIDT{}âˆ’{}\npDL: col, unsortedRIDTSAlg: scan{(2,A)}âˆ’{}\npRIDT{}âˆ’{}\npDL: col, unsortedRIDTSAlg: scan{(2,A)}âˆ’{}nchild\npDL: row, sortedRIDTSAlg: expS{(7,B), (6,C)}âˆ’{}\npDL: row, unsortedRIDTSAlg: hashS{(11,C), (12,Z)}âˆ’{}\nt.e\n(e)M5Merge nodes vertically: merge top-level nodeâ€™s left child.\nRIDT{}pt.e{(-;6), [6;11), [11;+)}âˆâˆDL: col, sortedSAlg: scan \npDL: row, sortedRIDTSAlg: expS{(7,B), (6,C)}âˆ’{}\npDL: row, unsortedRIDTSAlg: hashS{(11,C), (12,Z)}âˆ’{}\npDL: col, unsortedRIDTSAlg: scan{(1,B), (2,A)}âˆ’{}mutate M6 nparent\nRIDT{}pt.e{(-;6), [6;11), [11;+)}âˆâˆDL: col, sortedSAlg: scan \npDL: row, sortedRIDTSAlg: expS{(7,B), (6,C)}âˆ’{}\npDL: row, unsortedRIDTSAlg: hashS{(11,C), (12,Z)}âˆ’{}\npDL: col, unsortedRIDTSAlg: scan{(1,B), (2,A)}âˆ’{}nparent\npDL: col, unsortedRIDTSAlg: scan{}âˆ’{(-,6)}âˆ (f)M6Split node vertically: split left child.\nFigure 5: Performing the mutations described in Section 4.3 on actual physical indexes.\nIn the following we need to move potential child nodes ğ‘›â€²ofğ‘›child\nto the parent node ğ‘›parent :\nâˆ€ğ‘‘âˆˆğ·parentğ‘›childâˆˆğ‘›parent.RI(ğ‘‘)\nâ‡’ğ‘›parent.RI(ğ‘‘)=ğ‘›parent.RI(ğ‘‘)\\{ğ‘›child}âˆªğ‘›child.RI(ğ‘‘)\nwhereğ·parent is the domain of ğ‘›parent.RI. In other words: We re-\nmove all mappings to the merged node ğ‘›child and replace them with\nmappings to the child nodes of ğ‘›child. For our actual implementa-\ntion, we restrict ourselves to the merge of a single parent-child-pair\nduring a single mutation. Merge operations for longer chains of\nnodes can easily be achieved by recursively executing the merge\noperation on the same node. Figure 5(e) shows an example: We\nselect the root node as ğ‘›parent and its left child node as ğ‘›child which\nwe want to merge into the root node. To achieve this we first\nmerge all data contained in ğ‘›child.DT intoğ‘›parent .DT. Inğ‘›parent .RI,\nwe need to remove the mapping to ğ‘›child and replace them with\nmappings to the children of ğ‘›child. In this case, we remove the key-\nrange(âˆ’âˆ ; 6)âŠ‚ğ·and replace it with the corresponding entries of\nğ‘›child.RI. For this example this is equivalent to inserting the entries\n(âˆ’âˆ ; 2)and[2; 6)intoğ‘›parent .RI.\nM6Split child node vertically into k nodes: This is the inverse\noperation of M 5. Figure 5(f) shows an example.\n4.4 Fitness Function\nThe fitness function is used to measure the performance of a single\nphysical index and describes what to optimize by the genetic algo-\nrithm (either by minimizing or maximizing its value). Its definition\ncan be chosen freely depending on the optimization goal. We have\nchosen to optimize our index structures for the runtime given a spe-\ncific workload consisting of point and range queries. We therefore\ndefine the fitness function ğ‘“: Physical IndexÃ—Workloadâ†’â„to\nbe minimized in the following way: ğ‘“(ğœ‹,ğ‘Š)=ğ‘Ÿ(ğœ‹,ğ‘Š)ğ‘.ğœ‹denotes\nthe physical index (the individual) to evaluate, ğ‘Šis a sequence\nof queries and denotes the workload of the specific experiment.\nğ‘Ÿ(ğœ‹,ğ‘Š)ğ‘is the median runtime measured for this physical index\non the workload over ğ‘runs. The fitness function can also easilybe adapted to factor in other optimization goals like memory- or\nenergy-efficiency. Other interesting extensions include regulariza-\ntion, i.e. index complexity could be punished (similar to model\ncomplexity in ML). Furthermore we could punish or incentivize the\nfilling grade of leaves, e.g. if leaves are fully packed, this is bene-\nficial for read-optimized indexes but for inserts can quickly lead\nto structural modifications of the tree. However, if leaves are only\npartially filled, many inserts can be handled by leaf-local changes.\nAll these requirements can be modeled into the fitness function.\n5 RELATED WORK\nHandcrafted Indexes. Since the original B-tree-paper [ 6] in 1972,\nB-trees have become a workhorse in database systems. Since then\na myriad of B-tree-variants and -improvements have been pro-\nposed [ 30,42,43,46]. Other classes of handcrafted index structures\ninclude radix-trees like Judy-arrays [ 4] and its modern SIMDified\nincarnation ARTful [ 37]. Moreover, considerable work has been\ndone in the past years to better understand the performance of hash\ntables which are widely used in query processing [2, 45].\nLearned Indexes. The core task of a learned index [ 36] is to pro-\nvide an index on a densely packed, sorted array. The main idea\nis to manually define an (outer) B-tree-like structure, typically a\ntwo-level tree (coined RMI by the authors). Then, inside each node,\nrather than performing a binary search on the keys contained in\nthat node â€” as done in a textbook B-tree â€” a learned regression\nfunction is used to predict the position in the sorted array. Care has\nto be taken to avoid prediction errors. This is done through an error\ncorrection method: the prediction actually defines a range which\nmust be post-filtered through a different algorithm like binary or\ninterpolation search. The biggest advantage of a â€˜learned indexâ€™\nis that no space is required to store pivots in internal nodes thus\nallowing for high branching factors. Like our work, the original\nwork was a read-only index. It bulkloaded the index top-down, but\nas with any other B-tree like structure, bottom-up bulkloading up\nis also possible [ 32] and actually easier. Later on different proposals\nwere made to use different regression techniques [ 31] and support\n7\n\ninserts and deletes [ 14,17]. Also note that the RMIs make a couple\nof other assumptions that may not always hold in practice [ 10]. As\nillustrated in Figure 2(b) already, an RMI is just one special con-\nfiguration in GENE: an RMI is (1) a logical index: classical B-tree\n(however, fixed number of layers, balancing enforced, high fan-out),\n(2) a physical index: node internal search constrained to use some\nform of linear regression. In other words, an RMI handcrafts its\nlogical structure. Then, inside its nodes it uses a fixed physical\nregression method to learn a CDF. In contrast, we allow for opti-\nmizing the structure andthe search methods and data layouts used\ninside nodes. Thus, we fully embrace the orthogonality of learning\na model only inside a node vs optimizing the entire index structure .\nOur approach aims at optimizing the entire index structure not\nonly learning weights in a handcrafted structure.\nPeriodic Tables and Data Calculator. The work by Stratos Idreos\net.al. on semi-automatic data structure design is truly inspiring. In\ntheir vision paper [ 27] they aim at a complete dissection and classi-\nfication of the individual primitives used to design data structures.\nThey sketch the huge design space of indexes and conclude that\nmany quadrants in that space are still unexplored. They also phrase\nthe high-level vision to synthesize an index from a declarative\nspecification. Their main idea is to use a fine-grained learned cost\nmodels to be able to cost the physical individual index primitives\n(like scans, binary search, etc.). However, they go not further to\nshow how this can be achieved concretely. In addition, no split into\nlogical and physical indexes is given which is the key enabler in our\napproach. The follow-up work [ 28] is another vision paper which\ngoes into somewhat more detail in describing the problem space of\nthis endeavor and proposing a workbench like â€œâ€˜Data Alchemistâ€™\narchitectureâ€ which is a semi-automatic design tool. However, again\nno experiments and/or results are shown. Then, [ 29] explores a\nlarge set of physical index design primitives, benchmarks them,\nand uses the results to learn cost models for physical primitives.\nThis is used to build synthesized cost models for the expected cost\nof a combination of those physical primitives. The authors show\nseveral indexes where these cost estimates match the actual run-\ntimes very well. At the same time the paper emphasizes that many\nphysical design primitives and their cost models are missing includ-\ning compression, concurrency, updates, etc. In their most recent\nwork [ 25], they present the concept of design continuums, which\nunify different data structure designs by introducing common pa-\nrameters, rules, and domains necessary to describe the underlying\nindividuals. Using this design continuum, they show how to transi-\ntion between known data structures, exposing also hybrid designs,\nand how to extend the continuum by new designs. Their focus lies\non the semi-automated construction of these design continuums\nwhich are supposed to support researchers and engineers in finding\na close to optimal data structure for a given problem composed of\nworkload and hardware by using it as an inference engine.\nThere are four important differences to our work: we focus on\n(1) fully automatic index structure construction, (2) we provide a\nclear separation into logical and physical index components, (3) we\nbelieve that the index design space is simply too big for a practical\nsystem to be comprehensively modeled by (learned) cost models\none reason being that costs models of different physical primitives\nare often non-additive and hence not usable for an optimization\nprocess. (4) Optimization time is important but not as critical as instandard query optimization: recall that the creation of an index\nstructure is an offline process (in contrast to the creation of an index\ninstance at query time!). And therefore, it makes a lot of sense to\ndefine fitness via actual observed runtime measurements rather\nthan cost models whenever possible.\nGeneric Frameworks. A couple of generic indexing frameworks\nhave been proposed in the past, most notably GIST [ 23] and XXL [ 11].\nThose frameworks also aimed at generalizing presumably different\nindex structures into a common software framework. This in turn\nallowed architects to implement important database algorithms for\nthe generic index. The specialized indexes could then relatively\neasily be adapted to use the generic algorithms. Prominent exam-\nples include generic bulkloading [ 13] and concurrency control [ 33].\nThough that work was inspiring to us, we stress that in our paper\nwe argue on a conceptual level rather than an object-oriented-level.\nMoreover, we are primarily inspired by the analogue separation\ninto logical and relational operators without immediately specify-\ning how physical operators get implemented (ONC, vectorization,\nSIMD, whatever) or even how software interfaces need to be defined,\nas that is a tertiary concern.\nDQO. Recently, we proposed Deep Query Optimization [ 15]. The\ncore idea is to break operators into smaller components which can\nthen possibly be optimized using traditional query optimization\ntechnique. This paper is another inspiration of our work. However,\nthat work does not go into any detail on how such an idea can\nbe realized in the context of indexing. It neither details how tra-\nditional operators can be split nor how this can be turned into an\noptimization problem for automatic index creation. We fill that gap.\nIndex Selection. Index Selection [ 35,38] operates on a completely\ndifferent level as our approach. Instead of coming up with a con-\ncrete index structure, in index selection the goal is to determine a\nsuitable set of attributes to index in order to improve the runtime\nof a workload. In contrast, in our work we consider how to devise\nefficient index structures in the first place â€” which could then be\nleveraged in index selection algorithms.\nAdaptive Indexing. As index selection is NP-hard, an interesting\nstrategy is to not consider indexing a binary decision but rather\nallow indexes to become more and more fine-grained over time.\nThat is at the heart of adaptive indexing [ 26]. Several interesting\nproposals have been made in this space, see [ 47] for a survey. How-\never, all these indexes are still handcrafted indexes. In future work,\nwe are planning to revisit some of these techniques, as the DT-field\nof our logical nodes can be used to mimic many of those techniques.\nGenetic Algorithms. Genetic algorithms are a long known search\nmethod for an infeasible search space and have been used in our\ndatabase community for decades. Early work by Bennett et al. [ 7]\napplied a genetic algorithm to search for efficient plans in a query\noptimizer. Other papers used similar approaches to improve data-\nbase testing [ 5] or to perform index selection [ 20,34,40]. We are\nhowever not aware of papers tackling the problem of index creation\nusing a genetic algorithm and therefore try to further extend the\napplication area of these algorithms.\nDecoupling Logical and Physical Indexes. Early work on parti-\ntioning schemes was done by Hellerstein et al. [ 22]. They represent\ndata as a set of partitions where each partition is then (redundantly)\nmapped to at least one physical replica. In contrast to our work,\nthey do not consider partitioning trees as in our logical indexes and\n8\n\nthey also do not further detail how to physically implement each\npartition. In the field of structural indexing [ 1,19,41] introduce the\nidea to co-partition (or cluster) tuples in a relational schema using\ngraph partitioning. These graph partitions can then be exploited\nto answer structural queries which could be difficult to compute\nusing foreign key indexes only. Their work has a completely dif-\nferent goal: while we strive to create a single physical index, they\nstrive to create a graph partitioning which can then be mapped to\nsuitable existing indexes. Extending our logical index partitions to\ntheir graph co-partitions could be an interesting future extension\nto GENE. The GMAP project by Tsatalos et al. [ 49,50] is another in-\nteresting work in the area of physical data independence and index\ndesign. In contrast to their work, we focus on the clear difference\nbetween a logical and physical index and not the schema and a\nphysical index. Moreover, we automatically generate efficient index\nstructures, while their work only allows the choice of one concrete\nphysical index.\n6 EXPERIMENTAL EVALUATION\nIn our experiments, we first determine a suitable set of hyperparam-\neters for our genetic framework. Based on those hyperparameters,\nwe then carefully evaluate GENE. We highlight the cost for training\nand the ability to automatically reach a certain performance base-\nline. Finally, we show the capability of GENE to match and even\nbeat the performance of several state-of-the-art index structures.\nSystem. All experiments were executed on a machine with an\nAMD Ryzen Threadripper 1900X 8-Core processor with 32 GiB\nmemory on Linux. Our framework and the respective experiments\nare implemented in C++and compiled with Clang 8.0.1, -O3. All\nexperiments are run single-threaded and in main-memory.\nDatasets. We use three types of datasets. All datasets consist of\nunique 64-bit uint keys and a 64-bit payload. In the following, we\nrefer to the keys as data.keys . The payload represents the offset of\nthe corresponding key into a sorted array. Therefore, we refer to\nthe payload as data.offset . The datasets exhibit a variety of different\ncharacteristics like distribution ,density ,domain , and size. The first\ndataset unidense contains keys that are uniformly distributed in a\ndense domain. Concretely, ğ‘¢ğ‘›ğ‘–dense contains keys in the range [0, n)\nwhereğ‘›is the size of the dataset. The other two datasets, books and\nosm, represent real-world datasets with complex distributions and\nare taken from [ 31]. The datasets are sampled-down to our specific\ndata size by uniformly drawing elements without duplicates. We\nhave two main dataset sizes 100K and 100M, depending on the\nconcrete experiment. Table 2 gives an overview of the datasets.\nWorkloads. We use three classes of workloads: point, range, and\nmixed point and range query workloads. For the moment, all our\nworkloads are read-only, i.e. we do not consider insert ,delete , orup-\ndate statements. Note however, that our generic framework still sup-\nports insertions and deletions. In addition, update statements would\nnot alter the structure of the index so we could easily integrate\nthem into our framework. Table 3 summarizes the basic workload\ntypes. Point(data, idx min, idx max) represents a point query workload\nwhere the keys to lookup are taken from the keys in the dataset data\nby selecting indices in the subdomain [idx min, idx max)âŠ†[0,ğ‘›)with\na uniform distribution. Likewise, Range ğ‘ ğ‘’ğ‘™(data, idx min, idx max) de-\nscribes a range query workload consisting of pairs specifying the\nlower bound and upper bound of the query. The lower bound isDataset CDF Properties\nunidense\n0 1\nKey00.51Positionğ‘›:=# elements (100K, 100M)\n64-bit unique unsigned integers\nbooks\n0 1\nKey00.51Positionğ‘›:=# elements (100K, 1M, 10M, 100M)\n64-bit unique unsigned integer\nDataset taken from [31]\nosm\n0 1\nKey00.51Positionğ‘›:=# elements (100K, 100M)\n64-bit unique unsigned integers\nDataset taken from [31]\nTable 2: Datasets used in the experiments.\ndrawn with a uniform distribution in the index domain [idx min,\nidxmax- data.size * sel)âŠ†[0,ğ‘›)and the upper bound is set based\non the dataset size and the given selectivity sel. If the domain is\nnot explicitly specified, we assume it to cover the whole dataset.\nMix(data,ğ‘ƒ,ğ‘…) represents a mix of point and range queries with ğ‘ƒ\nandğ‘…being sets of point and range query workloads, respectively,\nbased on data. Note, that in contrast to the datasets, our workloads\nmay contain duplicates.\nWorkload Characteristics Parameters\nPoint(data, idx min, idx max)point queries in index\ndomain [idx min, idx max)\nwith uniform distribu-\ntion[idx min, idx max)âŠ†[0,ğ‘›)\nRangeğ‘ ğ‘’ğ‘™(data, idx min, idx max)range queries in index\ndomain [idx min, idx max)\nwith uniform distribu-\ntion and selectivity ğ‘ ğ‘’ğ‘™[idx min, idx max)âŠ†[0,ğ‘›)\nselâˆˆ[0,1]\nMix(data,ğ‘ƒ,ğ‘…)mix of point and range\nquery workloads with\nğ‘ƒandğ‘…being sets\nof respective workloads\nbased on datağ‘ƒ:={ğ‘|ğ‘is Point(data, idx min, idx max)}\nğ‘…:={ğ‘Ÿ|ğ‘Ÿis Rangeğ‘ ğ‘’ğ‘™(data, idx min, idx max)}\nTable 3: Workloads used in the experiments.\nAs already showcased in Sections 3 and 4, there is a huge search\nspace in designing physical index structures. Consequently, in our\nexperiments, we focus on the most important data layouts and\nsearch algorithms. We use the data layouts depicted in Table 4.\nAs search algorithms, we use scan ,binS ,intS,expS , and hashS\ndescribed in more detail in Section 3.1.\nData Layout Characteristics Implementation Detail\nsorted_colRI and DT have columnar lay-\nout for both keys and values.\nSorted according to keys.C++standard library container\nstd::vector<Key> and\nstd::vector<Value>\nhashDT represents hash table map-\nping keys to their values. RI\nempty.C++standard library container\nstd::unordered_map<Key,\nValue>\ntreeRI and DT represent tree data\nstructure mapping keys to\ntheir values. Sorted according\nto keys.C++standard library container\nstd::map<Key, Value>\nTable 4: Data layouts used in the experiments.\n6.1 Hyperparameter Tuning\nWe use ağ‘¢ğ‘›ğ‘–dense dataset of size 100K and vary five different pa-\nrameters within this experiment: (1) number of mutations per\ngeneration ( ğ‘ max):ğ‘ maxâˆˆ{10,50}, (2) maximum population size\n(ğ‘ Î ):ğ‘ Î âˆˆ{50,200,1000}, (3) tournament selection size ( ğ‘ ğ‘‡):ğ‘ ğ‘‡âˆˆ\n9\n\n0 500 1000 1500\ngeneration0.00.51.01.5median runtime [ms]baseline\nGENE(a)PQ,unidense\n0 500 1000\ngeneration020406080median runtime [ms]baseline\nGENE (b)RQ,unidense\n0 500 1000 1500\ngeneration01020304050median runtime [ms]baseline\nGENE (c)Mixed, unidense\n0 2000 4000\ngeneration0.00.20.40.60.81.0median runtime [ms]baseline\nGENE\n(d)PQ, books\n0 2000 4000 6000\ngeneration0255075100125median runtime [ms]baseline\nGENE (e)RQ, books\n0 2000 4000 6000\ngeneration0204060median runtime [ms]baseline\nGENE (f)Mixed, books\n0 2000 4000\ngeneration0.00.20.40.60.81.0relative improvement100K\n1M\n10M\n100M\n(g)Upscaling, PQ, books\n0 2000 4000 6000\ngeneration0.000.250.500.751.001.25relative improvement100K\n1M\n10M\n100M (h)Upscaling, RQ, books\n0 2000 4000 6000\ngeneration0.00.20.40.60.81.0relative improvement100K\n1M\n10M\n100M (i)Upscaling, Mixed,\nbooks\nFigure 6: Upper two rows (a-f): GENE approaching hand-\ncrafted baselines on three different workloads: A point\nquery only workload (PQ), a range query only workload\n(RQ) and a mixed workload consisting of 80% point and 20%\nrange queries. Bottom row (g-i): Relative improvement com-\npared to the initial index structure after upscaling to dataset\nsizes of 100K to 100M.\n{10%,50%,100% of population size}, (4) initial population size ( ğ‘ init):\nğ‘ initâˆˆ{10,50}, (5) population insertion criterion ( ğ‘): Instead of tak-\ning the median of the subset drawn during tournament selection,\nwe define a percentile ğ‘to be reached for a mutated individual\nto be inserted into the population: ğ‘âˆˆ{0%,50%,100%}. For the\n0% percentile, we always insert the mutated individual, for the\n100% percentile we only add it if it is better than the previous best\nindividual within the tournament selection subset.\nRankğ‘ maxğ‘ Î ğ‘ ğ‘‡ğ‘ initğ‘ median runtime [s] mean runtime [s]\n1 10 200 100% 50 0% 13.72 91.72\n2 10 1000 50% 50 50% 14.58 26.10\n3 10 1000 100% 10 50% 16.71 24.94\n4 10 1000 100% 50 0% 16.87 94.48\n5 10 1000 50% 10 50% 18.21 158.49\nTable 5: Best Genetic Search Configurations (over 5 runs)\nTable 5 shows the best configurations (based on the median of\nthe 5 runs executed per configuration). Given a total number of\nmutations we want to perform, we conclude that it is more beneficial\nto use a smaller number of mutations per generation combined\nwith a larger number of generations. As the population size has\na limited influence, we decided to keep it very small to reduce\nthe overhead to maintain the population. We therefore used the\nfollowing default parameters for the experiments in the following\nsections:ğ‘ max=10,ğ‘ Î =50,ğ‘ ğ‘‡=25,ğ‘ init=10andğ‘=50%.6.2 Rediscover Suitable Baseline Indexes\nIn this experiment, we will demonstrate that our genetic algorithm\nis capable of reproducing the performance of various baseline index\nstructures as known from textbooks. We consider two different\ndatasets:ğ‘¢ğ‘›ğ‘–dense andbooks of sizes 100K, 1M, 10M and 100M. We\ncombine each of those two datasets with three different workloads\ncontaining 10,000 queries each: Point(unidense), Range 0.001(unidense)\nand a Mix(uni dense ,ğ‘ƒ,ğ‘…) workload, with P := {Point (unidense)} and\nR := {Range 0.01(unidense)} consisting of 80% point and 20% range\nqueries. For each workload, we define a baseline within our generic\nframework of which we believe it has a decent performance: For\nthe point query only workload, we assume a simple hash table to\nperform best which is implemented as an index structure with a\nsingle node having the hash data layout. For the range query only\nand mixed workload, we assume a B-tree-like structure to offer a\ndecent performance. We initialize the tree to have 100 fully filled\nleaves, each containing 1,000 elements and a fan-out of 10 for the\ninternal nodes. Each node is configured to use the sorted_col\nlayout and binS . We configured GENE to allow nodes to contain up\nto 100,000 key-value-pairs or 100,000 child partitions (potentially\nleading to solutions consisting of a single node or solutions with\none node per element assembled under a single root node). In the\ninitial population trees were bulkloaded with 100 equally filled\nleaves and a fan-out of 10, but with randomized data layouts and\nsearch methods. Each experiment is conducted for 8000 generations.\nThe genetic search was run on the smallest sample size of 100K ele-\nments. Each time we found a new, best individual, we checked if the\nresults carry over to the larger datasets, i.e. we created new index\nstructures using the same routing information and data layouts and\nsearch methods as found by GENE (i.e. using the exact same index\nstructure), but bulkloaded them with the larger dataset, increasing\nleaf capacities if necessary. We then evaluated them using the exact\nsame workload as used in the genetic search.\nFigure 6 shows the results. Each plot in the first two lines com-\npares the performance of the baseline to the performance of the\ngenetic algorithm where we plotted the best individual of each gen-\neration. We plot the curves up to the point of the last improvement.\nAs we can clearly see, GENE rapidly approaches the baseline.\nThis is mostly due to the fact that GENE can rather easily improve\nby mutating very inefficient nodes in the beginning. After getting\nclose to the baseline, GENE only finds slight improvements, e.g. by\nchanging search algorithms within nodes, which are hardly visible\non the plot. The index structures found by GENE are very similar\nto the baselines: On the ğ‘¢ğ‘›ğ‘–dense dataset, GENE always returned a\nsingle node index structure. For the point query only workload, it\ncame up with a single hash node containing all entries, i.e. exactly\nthe baseline we defined beforehand. For the range query only as\nwell as mixed workload, GENE also reduced the index to a single\nnode, but with sorted_col data layout and intS search method.\nThis difference is due to the fact that range queries can not be\nexecuted efficiently on a hash node. This result is reasonable as a\nuniformly distributed, dense dataset can easily be modeled by an\narray with a linear model as search method. Considering the books\ndataset, the point query workload resulted in a tree with 68 nodes\nin total, 66 of them being leaves. All but one leaf are direct children\nof the sorted_col root node, the remaining leaf has a single tree\n10\n\nnode between itself and the root. With 48 nodes, the vast majority\nof the leaves has a hash data layout. The remaining leaves are of\nsorted_col (15) or tree data layout (3). The dominating search\nmethod for non- hash nodes is binS , with only 3 exceptions that\nuseexpS . The resulting index structure reminds of a partitioned\nhash map, indicating that GENE indeed approached the expected\nbaseline. For the range query workload, we obtained an index with\nsimilar size, having 44 nodes with sorted_col data layout in total,\n40 of them being leaves. The index has a height of three with the\nmajority of the leaves (38) situated at depth two and only two leaves\nbeing one level below. BinS is again the dominating search method\nfor the leaves, with four nodes using intS and two using expS\ninstead. The resulting index structure reminds of a shallow B-tree,\nindicating that GENE again approached the expected baseline. For\nthe final mixed workload, the results are similar to the range only\nworkload. We obtained an index of height three with 41 nodes in\ntotal (all with sorted_col data layout), 35 of them being leaves.\nThe majority of the leaves is at depth two, with three leaves being\none level above and one leaf being a level below. The dominating\nsearch method is again binS , with only 7 leaves using an intS\ninstead. As for the range query only workload, GENE approached a\nshallow B-tree like index to match the performance of the baseline.\nThe last line in Figure 6 shows the improvements of the scaled index\nstructures for the books dataset. Each line represents the relative\nimprovements compared to the best index structure of GENEâ€™s\ninitial population, upscaled to the indicated dataset sizes of 100K\n(the size on which the search was conducted) up to 100M. We can\nclearly see that an improvement in the solid line representing the\ntraining data nearly always results in a very similar improvement\nfor the upscaled index structures. The overall, relative improvement\nbecomes even bigger with increasing dataset size, indicating that is\nmost likely sufficient to run GENE on a sample of the data to obtain\na decent index structure, highly reducing the necessary search time.\nIf best possible performance is the ultimate goal, then GENE can\nagain be applied to the upscaled index structure resulting from the\nsample to perform further fine tuning.\nWe also experimented with an additional, mixed workload again\nconsisting of 10,000 queries with a 80% / 20% point to range query\nratio, based on the ğ‘¢ğ‘›ğ‘–dense dataset. However, this time we chose\nthe queries to be normally distributed around key 75,000 with a\nstandard deviation of 10,000, i.e. the queries were mainly focused\non the upper half of the key domain. Our GENE algorithm again\ndecided to shrink the initial index structures considerably, however\nit stopped after 3500 generations returning a tree with 4 levels and\n25 nodes in total, 17 of them being leaves. The nodes containing\nthe upper half of the key domain were again using the sorted_col\nlayout and either intS orbinS . The total runtimes of GENE heavily\ndepend on the concrete datasets and workloads. The fastest execu-\ntion forğ‘¢ğ‘›ğ‘–dense with point query only workload took less than 3\nminutes until the last improvement was found. The longest run on\nthe same dataset with range query only workload took about 122\nminutes. Performing the additional upscaling steps further influ-\nenced the runtimes, leading to execution times of up to 30 hours for\nthebooks dataset in combination with range query only workload.6.3 Optimized vs Heuristic Indexes\nIn this section, we will compare the performance of a GENE index\nwith representatives of different prevalent heuristic index types.\nTable 6 gives an overview of the different index types and respec-\ntive representatives. For the B+tree implementation we use the\ncommonly used TLX baseline implementation by Bingmann [ 8]. In\nparticular, we use the specialized B+tree template class btree_map\nimplementing STLâ€™s map container. The ART implementation is\ntaken from the SOSD benchmark [ 39] by Marcus et al. and con-\ncretely, we use the implementation ARTPrimaryLB that supports\nlower bound lookups. PGM [ 18] by Ferragina et al. provides multiple\nimplementations that support a variety of different functionalities\nlike insertion and deletion support or compression to reduce space\nusage. Since we are only interested in the lookup performance, we\nuse the default PGMIndex implementation. We purposely exclude\nhash tables since they do not support range queries efficiently.\nType Index Details\nTree B-tree TLX btree_map [8]\nRadix ART SOSD ARTPrimaryLB [39]\nLearned PGM PGM PGMIndex [18]\nTable 6: Overview of different index types and representa-\ntives of each category.\nWe conduct our performance evaluation on the three different\ndatasets, unidense,books , and osm, each with a size of ğ‘›=100M data\npoints. As for the workload, we are going to use a mixed workload\nconsisting of multiple point and range query workloads. Concretely,\nthe workload consists of 1M queries, divided in three point query\nworkloads and one range query workload: Mix(data, ğ‘ƒ,ğ‘…), withğ‘ƒ:=\n{Point(data, 0, 0.1Â·n), Point(data, 0.1Â·n, 0.85Â·n), Point(data, 0.85 Â·\nn, n)} andğ‘…:= {Range(data, 0.1Â·n, 0.85Â·n)}, where dataâˆˆ{uni dense ,\nbooks, osm}. With that, the queried key domain is essentially split\ninto three partitions at 10% and 85% of the data based on the different\nworkloads. The first partition [0,0.1Â·ğ‘›)exclusively receives point\nqueries representing 20% of the total workload size. The second\npartition[0.1Â·ğ‘›,0.85Â·ğ‘›)receives a mix of both, 10% point and 20%\nrange queries, and the third partition [0.85Â·ğ‘›,ğ‘›)50% point queries.\nFigure 7 illustrates the workload based on the osmdataset. Since\neach data point maps a key to its position in a sorted data array,\nrange queries can be translated to finding the position of the lower\nbound in the index and subsequently scanning the data array. This\nscan is independent of the underlying index type and can therefore\nbe neglected. Thus, a range query in our evaluation is equivalent\nto a lower bound lookup in the index. Our generic implementation\nallows us to easily replace specific parts of a physical index structure\nlike the data layout or search method. However, this leads to a non-\nnegligible performance overhead mainly due to repeated dynamic\ndispatches. To be competitive with the other baselines and state-of-\nthe-art index structures, we provide an additional implementation\nthat specifically contains the concrete physical index structures\nused in this experiment. Figure 9 shows the physical structure of\nour GENE index. Since the workload domain is split into three\npartitions with two exclusive point query regions, we bulkload our\nindex structure accordingly. The first and third partition are hash\nnodes while the second partition represents a B-tree-style index.\nThe root is a sorted array using binary search. We randomly shuffle\nthe workload before each execution to avoid caching effects.\n11\n\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\nKey 1e190.00.51.01.52.0#queries1e6\npoint\nrange\n0.00.20.40.60.81.0\nPositionCDF osm\npartitionFigure 7: Visualization of the experimental setup. The osm\ndataset is shown as CDF while the point and range queries\nare illustrated as a stacked histogram. The red vertical lines\nhighlight the partition borders.\npoc0200400600avg. lookup time [ns]uniform dense\nTLX B-tree\nART\nPGM\nGENE\npocbooks\npocosm\nFigure 8: Average index lookup time comparison between\nthree representative state-of-the-art index structures and\nour GENE index on three different datasets and the corre-\nsponding workload described in subsection 6.3. The small\nblack bars indicate the standard deviation of five runs,\nwhich is negligibly small.\nRIDT{}pDL: col, sortedSAlg: binS {[0,0.1â‹…n),[0.1â‹…n,0.85â‹…n),[0.85â‹…n,n)}data.offset\npDL: row, unsortedRIDTSAlg: hashS{â€¦}âˆ’{}\npDL: row, unsortedRIDTSAlg: hashS{â€¦}âˆ’{}B-tree-style\nIndexâ€¦\npDL: col, sortedRIDTSAlg: binS{}{â€¦}\npDL: col, sortedRIDTSAlg: binS{â€¦}{}\npDL: col, sortedRIDTSAlg: binS{â€¦}{}\nFigure 9: Physical index structure of the GENE index based\non the workload partitioning.\nFigure 8 shows the results of the index structures for different\ndatasets. We report the average index lookup time. Independent\nof the underlying dataset, the TLX B-tree requires around 700 ns\nand is not able to compete with the other indexes. On the uniform\ndense dataset, ART and PGM both achieve a lower lookup time\nthan GENE. However, for both, a uniform dense dataset is close\nto the optimal use case. For the two real-world skewed and sparsedatasets, our GENE index achieves a competitive or even faster\nlookup time than the other index structures of around 350 ns.\nWe are well aware that this is a very specific use case, however,\nit showcases that there are indeed scenarios where an optimized\nGENE index can outperform a state-of-the-art (heuristic) data struc-\nture. Expanding the covered design space by GENE, i.e. the available\ndata structures and search algorithms, and automatically finding\nthose scenarios is part of future work. In conclusion, our proof\nof concept emphasizes that there are use cases in which GENE is\nable to achieve a competitive or even superior performance than\nstate-of-the-art index structures and therefore, confirms its validity.\n7 CONCLUSION AND FUTURE WORK\nConclusions. This paper has opened the book for automatically\ngenerated index structures. We have proposed a powerful generic\nindexing framework on the logical and physical level analogue to\nlogical and physical operators in query processing and optimization.\nWe have shown that by clearly separating the logical and physical di-\nmensions of an index, a huge number of existing (physical) indexes\ncan be represented in our generic indexing framework. Furthermore,\nwe introduced Genetic Generic Generation of Indexes (GENE) . Given\na workload, GENE can come up with an efficient physical index\nstructure automatically. Our initial experimental results outlines\nthe potential and efficiency of our approach.\nFuture Work. This paper is obviously just a starting point of a\nmuch longer story. There are many possible exciting research di-\nrections ahead, including:\n(1)code-generation, similar to generating code for the most ef-\nficient physical plan found, generate code for the most efficient\nphysical index structure found,\n(2)The Index Farm : we plan to open source our framework: the goal\nis that people submit a workload on a web page and the framework\nemits suitable source code for an index structure,\n(3)runtime adaptivity: how to mutate structurally, this can also\nsimulate the adaptive indexing family of index structures,\n(4)updates: simple updates are trivial, i.e. if a value assigned to an\nexisting key is changed, no structural modifications required at this\npoint); the generic framwork already supports inserts and deletes\nbut adapting to insertion and deletion workloads is more complex\nand beyond the scope of this paper,\n(5)scalability: extend our scalability experiments to evaluate work-\nloads only on subtrees affected by mutations using cost functions to\nprioritize expensive partitions when drawing nodes for mutations\n(6) effects of non-empty DT-fields in internal nodes,\n(7) extend GENE to support more data layouts, search algorithms,\nand hardware acceleration (SIMD).\n12\n\nREFERENCES\n[1]Erik Agterdenbos, George H. L. Fletcher, Chee-Yong Chan, and Stijn Vansum-\nmeren. 2016. Empirical evaluation of guarded structural indexing. In Proceedings\nof the 19th International Conference on Extending Database Technology, EDBT .\n714â€“715. https://doi.org/10.5441/002/edbt.2016.101\n[2]Victor Alvarez, Stefan Richter, Xiao Chen, and Jens Dittrich. 2015. A comparison\nof adaptive radix trees and hash tables. In 31st IEEE International Conference\non Data Engineering, ICDE 2015, Seoul, South Korea, April 13-17, 2015 , Johannes\nGehrke, Wolfgang Lehner, Kyuseok Shim, Sang Kyun Cha, and Guy M. Lohman\n(Eds.). IEEE Computer Society, 1227â€“1238. https://doi.org/10.1109/ICDE.2015.\n7113370\n[3]Lars Arge. 1995. The Buffer Tree: A New Technique for Optimal I/O-Algorithms\n(Extended Abstract). In Algorithms and Data Structures, 4th International Work-\nshop, WADS â€™95, Kingston, Ontario, Canada, August 16-18, 1995, Proceedings\n(Lecture Notes in Computer Science) , Selim G. Akl, Frank K. H. A. Dehne, JÃ¶rg-\nRÃ¼diger Sack, and Nicola Santoro (Eds.), Vol. 955. Springer, 334â€“345. https:\n//doi.org/10.1007/3-540-60220-8_74\n[4]D. Baskins. 2004, (accessed September 10, 2020). Judy arrays . http://judy.\nsourceforge.net/\n[5]Hardik Bati, Leo Giakoumakis, Steve Herbert, and Aleksandras Surna. 2007.\nA genetic approach for random testing of database systems. In Proceedings of\nthe 33rd International Conference on Very Large Data Bases . 1243â€“1251. http:\n//www.vldb.org/conf/2007/papers/industrial/p1243-bati.pdf\n[6]Rudolf Bayer and Edward M. McCreight. 1972. Organization and Maintenance of\nLarge Ordered Indices. Acta Informatica 1 (1972), 173â€“189. https://doi.org/10.\n1007/BF00288683\n[7]Kristin P. Bennett, Michael C. Ferris, and Yannis E. Ioannidis. 1991. A Genetic\nAlgorithm for Database Query Optimization. In Proceedings of the 4th International\nConference on Genetic Algorithms . 400â€“407.\n[8]Timo Bingmann. 2018. TLX: Collection of Sophisticated C++ Data Structures,\nAlgorithms, and Miscellaneous Helpers. https://github.com/tlx/tlx, retrieved\nMay. 31, 2021.\n[9]Miles Cranmer, Alvaro Sanchez-Gonzalez, Peter Battaglia, Rui Xu, Kyle Cranmer,\nDavid Spergel, and Shirley Ho. 2020. Discovering Symbolic Models from Deep\nLearning with Inductive Biases. arXiv:2006.11287 [cs.LG]\n[10] Andrew Crotty. 2021. Hist-Tree: Those Who Ignore It Are Doomed to Learn. In\n11th Conference on Innovative Data Systems Research, CIDR 2021, Virtual Event,\nJanuary 11-15, 2021, Online Proceedings . www.cidrdb.org. http://cidrdb.org/\ncidr2021/papers/cidr2021_paper20.pdf\n[11] Jochen Van den Bercken, BjÃ¶rn Blohsfeld, Jens-Peter Dittrich, JÃ¼rgen KrÃ¤mer,\nTobias SchÃ¤fer, Martin Schneider, and Bernhard Seeger. 2001. XXL - A Library\nApproach to Supporting Efficient Implementations of Advanced Database Queries.\nInVLDB 2001, Proceedings of 27th International Conference on Very Large Data\nBases, September 11-14, 2001, Roma, Italy , Peter M. G. Apers, Paolo Atzeni, Stefano\nCeri, Stefano Paraboschi, Kotagiri Ramamohanarao, and Richard T. Snodgrass\n(Eds.). Morgan Kaufmann, 39â€“48. http://www.vldb.org/conf/2001/P039.pdf\n[12] Jochen Van den Bercken and Bernhard Seeger. 2001. An Evaluation of Generic\nBulk Loading Techniques. In VLDB 2001, Proceedings of 27th International Con-\nference on Very Large Data Bases, September 11-14, 2001, Roma, Italy , Peter M. G.\nApers, Paolo Atzeni, Stefano Ceri, Stefano Paraboschi, Kotagiri Ramamoha-\nnarao, and Richard T. Snodgrass (Eds.). Morgan Kaufmann, 461â€“470. http:\n//www.vldb.org/conf/2001/P461.pdf\n[13] Jochen Van den Bercken, Bernhard Seeger, and Peter Widmayer. 1997. A Generic\nApproach to Bulk Loading Multidimensional Index Structures. In VLDB .\n[14] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\nDavid B. Lomet, and Tim Kraska. 2020. ALEX: An Updatable Adaptive Learned\nIndex. In Proceedings of the 2020 International Conference on Management of\nData, SIGMOD Conference 2020, online conference [Portland, OR, USA], June 14-19,\n2020, David Maier, Rachel Pottinger, AnHai Doan, Wang-Chiew Tan, Abdussalam\nAlawini, and Hung Q. Ngo (Eds.). ACM, 969â€“984. https://doi.org/10.1145/3318464.\n3389711\n[15] Jens Dittrich and Joris Nix. 2020. The Case for Deep Query Optimisation. In\nCIDR 2020, 10th Conference on Innovative Data Systems Research, Amsterdam,\nThe Netherlands, January 12-15, 2020, Online Proceedings . www.cidrdb.org. http:\n//cidrdb.org/cidr2020/papers/p3-dittrich-cidr20.pdf\n[16] Ronald Fagin, JÃ¼rg Nievergelt, Nicholas Pippenger, and H. Raymond Strong. 1979.\nExtendible Hashing - A Fast Access Method for Dynamic Files. ACM Trans.\nDatabase Syst. 4, 3 (1979), 315â€“344. https://doi.org/10.1145/320083.320092\n[17] Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-index: a fully-dynamic\ncompressed learned index with provable worst-case bounds. Proc. VLDB Endow.\n13, 8 (2020), 1162â€“1175. http://www.vldb.org/pvldb/vol13/p1162-ferragina.pdf\n[18] Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-index: a fully-dynamic\ncompressed learned index with provable worst-case bounds. PVLDB 13, 8 (2020),\n1162â€“1175. https://doi.org/10.14778/3389133.3389135\n[19] George H. L. Fletcher, Dirk Van Gucht, Yuqing Wu, Marc Gyssens, Sofia Brenes,\nand Jan Paredaens. 2009. A methodology for coupling fragments of XPathwith structural indexes for XML documents. Inf. Syst. 34, 7 (2009), 657â€“670.\nhttps://doi.org/10.1016/j.is.2008.09.003\n[20] Farshad Fotouhi and Carlos E. Galarce. 1989. Genetic Algorithms and the Search\nfor Optimal Database Index Selection. In Computing in the 90â€™s, The First Great\nLakes Computer Science Conference (Lecture Notes in Computer Science) , Vol. 507.\n249â€“255. https://doi.org/10.1007/BFb0038500\n[21] Hector Garcia-Molina, Jeffrey D. Ullman, and Jennifer Widom. 2002. Database\nSystems - the Complete Book (International Edition). Pearson Education.\n[22] Joseph M. Hellerstein, Elias Koutsoupias, Daniel P. Miranker, Christos H. Papadim-\nitriou, and Vasilis Samoladas. 2002. On a model of indexability and its bounds for\nrange queries. J. ACM 49, 1 (2002), 35â€“55. https://doi.org/10.1145/505241.505244\n[23] Joseph M. Hellerstein, Jeffrey F. Naughton, and Avi Pfeffer. 1995. Generalized\nSearch Trees for Database Systems. In VLDBâ€™95, Proceedings of 21th International\nConference on Very Large Data Bases, September 11-15, 1995, Zurich, Switzerland ,\nUmeshwar Dayal, Peter M. D. Gray, and Shojiro Nishio (Eds.). Morgan Kaufmann,\n562â€“573. http://www.vldb.org/conf/1995/P562.PDF\n[24] John Henry Holland. 1975. Adaptation in natural and artificial systems: an in-\ntroductory analysis with applications to biology, control, and artificial intelligence .\nMIT press.\n[25] Stratos Idreos, Niv Dayan, Wilson Qin, Mali Akmanalp, Sophie Hilgard, Andrew\nRoss, James Lennon, Varun Jain, Harshita Gupta, David Li, and Zichen Zhu.\n2019. Design Continuums and the Path Toward Self-Designing Key-Value Stores\nthat Know and Learn. In CIDR 2019, 9th Biennial Conference on Innovative Data\nSystems Research, Asilomar, CA, USA, January 13-16, 2019, Online Proceedings .\nwww.cidrdb.org. http://cidrdb.org/cidr2019/papers/p143-idreos-cidr19.pdf\n[26] Stratos Idreos, Martin L. Kersten, and Stefan Manegold. 2007. Database Cracking.\nInCIDR 2007, Third Biennial Conference on Innovative Data Systems Research,\nAsilomar, CA, USA, January 7-10, 2007, Online Proceedings . www.cidrdb.org, 68â€“78.\nhttp://cidrdb.org/cidr2007/papers/cidr07p07.pdf\n[27] Stratos Idreos, Kostas Zoumpatianos, Manos Athanassoulis, Niv Dayan, Brian\nHentschel, Michael S. Kester, Demi Guo, Lukas M. Maas, Wilson Qin, Abdul\nWasay, and Yiyou Sun. 2018. The Periodic Table of Data Structures. IEEE Data\nEng. Bull. 41, 3 (2018), 64â€“75. http://sites.computer.org/debull/A18sept/p64.pdf\n[28] Stratos Idreos, Kostas Zoumpatianos, Subarna Chatterjee, Wilson Qin, Abdul\nWasay, Brian Hentschel, Mike S. Kester, Niv Dayan, Demi Guo, Minseo Kang,\nand Yiyou Sun. 2019. Learning Data Structure Alchemy. IEEE Data Eng. Bull. 42,\n2 (2019), 47â€“58. http://sites.computer.org/debull/A19june/p47.pdf\n[29] Stratos Idreos, Kostas Zoumpatianos, Brian Hentschel, Michael S. Kester, and\nDemi Guo. 2018. The Data Calculator: Data Structure Design and Cost Synthesis\nfrom First Principles and Learned Cost Models. In Proceedings of the 2018 Inter-\nnational Conference on Management of Data, SIGMOD Conference 2018, Houston,\nTX, USA, June 10-15, 2018 , Gautam Das, Christopher M. Jermaine, and Philip A.\nBernstein (Eds.). ACM, 535â€“550. https://doi.org/10.1145/3183713.3199671\n[30] Changkyu Kim, Jatin Chhugani, Nadathur Satish, Eric Sedlar, Anthony D. Nguyen,\nTim Kaldewey, Victor W. Lee, Scott A. Brandt, and Pradeep Dubey. 2010. FAST:\nfast architecture sensitive tree search on modern CPUs and GPUs. In Proceedings\nof the ACM SIGMOD International Conference on Management of Data, SIGMOD\n2010, Indianapolis, Indiana, USA, June 6-10, 2010 , Ahmed K. Elmagarmid and\nDivyakant Agrawal (Eds.). ACM, 339â€“350. https://doi.org/10.1145/1807167.\n1807206\n[31] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2019. SOSD: A Benchmark for Learned\nIndexes. CoRR abs/1911.13014 (2019). arXiv:1911.13014 http://arxiv.org/abs/1911.\n13014\n[32] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2020. RadixSpline: a single-pass learned\nindex. In Proceedings of the Third International Workshop on Exploiting Artificial\nIntelligence Techniques for Data Management, aiDM@SIGMOD 2020, Portland,\nOregon, USA, June 19, 2020 , Rajesh Bordawekar, Oded Shmueli, Nesime Tatbul,\nand Tin Kam Ho (Eds.). ACM, 5:1â€“5:5. https://doi.org/10.1145/3401071.3401659\n[33] Marcel Kornacker, C. Mohan, and Joseph M. Hellerstein. 1997. Concurrency\nand Recovery in Generalized Search Trees. In SIGMOD 1997, Proceedings ACM\nSIGMOD International Conference on Management of Data, May 13-15, 1997, Tucson,\nArizona, USA , Joan Peckham (Ed.). ACM Press, 62â€“72. https://doi.org/10.1145/\n253260.253272\n[34] Marcin Korytkowski, Marcin Gabryel, Robert Nowicki, and Rafal Scherer. 2004.\nGenetic Algorithm for Database Indexing. In Artificial Intelligence and Soft\nComputing - ICAISC (Lecture Notes in Computer Science) , Vol. 3070. 1142â€“1147.\nhttps://doi.org/10.1007/978-3-540-24844-6_179\n[35] Jan Kossmann, Stefan Halfpap, Marcel Jankrift, and Rainer Schlosser. 2020. Magic\nmirror in my hand, which is the best in the land? An Experimental Evaluation\nof Index Selection Algorithms. Proc. VLDB Endow. 13, 11 (2020), 2382â€“2395.\nhttp://www.vldb.org/pvldb/vol13/p2382-kossmann.pdf\n[36] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In Proceedings of the 2018 International\nConference on Management of Data, SIGMOD Conference 2018, Houston, TX, USA,\nJune 10-15, 2018 , Gautam Das, Christopher M. Jermaine, and Philip A. Bernstein\n(Eds.). ACM, 489â€“504. https://doi.org/10.1145/3183713.3196909\n13\n\n[37] Viktor Leis, Alfons Kemper, and Thomas Neumann. 2013. The adaptive radix\ntree: ARTful indexing for main-memory databases. In 29th IEEE International\nConference on Data Engineering, ICDE 2013, Brisbane, Australia, April 8-12, 2013 ,\nChristian S. Jensen, Christopher M. Jermaine, and Xiaofang Zhou (Eds.). IEEE\nComputer Society, 38â€“49. https://doi.org/10.1109/ICDE.2013.6544812\n[38] Vincent Y. Lum and Huei Ling. 1971. An Optimization Problem on the Selection\nof Secondary Keys. In Proceedings of the 1971 26th Annual Conference (ACM â€™71) .\nAssociation for Computing Machinery, New York, NY, USA, 349â€“356. https:\n//doi.org/10.1145/800184.810505\n[39] Ryan Marcus, Andreas Kipf, Alexander van Renen, Mihail Stoian, Sanchit Misra,\nAlfons Kemper, Thomas Neumann, and Tim Kraska. 2020. Benchmarking Learned\nIndexes. Proc. VLDB Endow. 14, 1 (2020), 1â€“13.\n[40] Priscilla Neuhaus, Julia Couto, Jonatas Wehrmann, Duncan Dubugras Alcoba\nRuiz, and Felipe Meneguzzi. 2019. GADIS: A Genetic Algorithm for Database\nIndex Selection (S). In The 31st International Conference on Software Engineering\nand Knowledge Engineering, SEKE . 39â€“54. https://doi.org/10.18293/SEKE2019-135\n[41] FranÃ§ois Picalausa, George H. L. Fletcher, Jan Hidders, and Stijn Vansummeren.\n2014. Principles of Guarded Structural Indexing. In Proc. 17th International\nConference on Database Theory (ICDT) . 245â€“256. https://doi.org/10.5441/002/icdt.\n2014.26\n[42] Jun Rao and Kenneth A. Ross. 1999. Cache Conscious Indexing for Decision-\nSupport in Main Memory. In VLDBâ€™99, Proceedings of 25th International Conference\non Very Large Data Bases, September 7-10, 1999, Edinburgh, Scotland, UK , Mal-\ncolm P. Atkinson, Maria E. Orlowska, Patrick Valduriez, Stanley B. Zdonik, and\nMichael L. Brodie (Eds.). Morgan Kaufmann, 78â€“89. http://www.vldb.org/conf/\n1999/P7.pdf\n[43] Jun Rao and Kenneth A. Ross. 2000. Making B+-Trees Cache Conscious in Main\nMemory. In Proceedings of the 2000 ACM SIGMOD International Conference onManagement of Data, May 16-18, 2000, Dallas, Texas, USA , Weidong Chen, Jeffrey F.\nNaughton, and Philip A. Bernstein (Eds.). ACM, 475â€“486. https://doi.org/10.\n1145/342009.335449\n[44] Esteban Real, Chen Liang, David R. So, and Quoc V. Le. 2020. AutoML-Zero:\nEvolving Machine Learning Algorithms From Scratch. arXiv:2003.03384 [cs.LG]\n[45] Stefan Richter, Victor Alvarez, and Jens Dittrich. 2015. A Seven-Dimensional\nAnalysis of Hashing Methods and its Implications on Query Processing. Proc.\nVLDB Endow. 9, 3 (2015), 96â€“107. https://doi.org/10.14778/2850583.2850585\n[46] Benjamin Schlegel, Rainer Gemulla, and Wolfgang Lehner. 2009. k-ary search\non modern processors. In Proceedings of the Fifth International Workshop on\nData Management on New Hardware, DaMoN 2009, Providence, Rhode Island,\nUSA, June 28, 2009 , Peter A. Boncz and Kenneth A. Ross (Eds.). ACM, 52â€“60.\nhttps://doi.org/10.1145/1565694.1565705\n[47] Felix Martin Schuhknecht, Alekh Jindal, and Jens Dittrich. 2013. The Uncracked\nPieces in Database Cracking. Proc. VLDB Endow. 7, 2 (2013), 97â€“108. https:\n//doi.org/10.14778/2732228.2732229\n[48] Patricia G. Selinger, Morton M. Astrahan, Donald D. Chamberlin, Raymond A.\nLorie, and Thomas G. Price. 1979. Access Path Selection in a Relational Database\nManagement System. In Proceedings of the 1979 ACM SIGMOD International\nConference on Management of Data, Boston, Massachusetts, USA, May 30 - June 1 ,\nPhilip A. Bernstein (Ed.). ACM, 23â€“34. https://doi.org/10.1145/582095.582099\n[49] Odysseas G. Tsatalos and Yannis E. Ioannidis. 1994. A Unified Framework for\nIndexing in Database Systems. In Database and Expert Systems Applications, 5th\nInternational Conference, DEXA (Lecture Notes in Computer Science) , Vol. 856.\n183â€“192. https://doi.org/10.1007/3-540-58435-8_183\n[50] Odysseas G. Tsatalos, Marvin H. Solomon, and Yannis E. Ioannidis. 1996. The\nGMAP: A Versatile Tool for Physical Data Independence. VLDB J. 5, 2 (1996),\n101â€“118. https://doi.org/10.1007/s007780050018\n14",
  "textLength": 90967
}