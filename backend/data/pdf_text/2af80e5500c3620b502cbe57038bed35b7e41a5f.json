{
  "paperId": "2af80e5500c3620b502cbe57038bed35b7e41a5f",
  "title": "bloomRF: On Performing Range-Queries in Bloom-Filters with Piecewise-Monotone Hash Functions and Prefix Hashing",
  "pdfPath": "2af80e5500c3620b502cbe57038bed35b7e41a5f.pdf",
  "text": "bloomRF: On Performing Range-Queries in Bloom-Filters with\nPiecewise-Monotone Hash Functions and Prefix Hashing\n[Extended Version]\nBernhard MÃ¶ÃŸner, Christian Riegger, Arthur Bernhardt, Ilia Petrov\nData Management Lab, Reutlingen University\n[firstname].[surname]@reutlingen-university.de\nABSTRACT\nWe introduce bloomRF as a unified method for approximate mem-\nbership testing that supports both point- and range-queries . As\na first core idea, bloomRF introduces novel prefix hashing to ef-\nficiently encode range information in the hash-code of the key\nitself. As a second key concept, bloomRF proposes novel piecewise-\nmonotone hash-functions that preserve local order and support fast\nrange-lookups with fewer memory accesses. bloomRF has near-\noptimal space complexity and constant query complexity. Although,\nbloomRF is designed for integer domains , it supports floating-points ,\nand can serve as a multi-attribute filter. The evaluation in RocksDB\nand in a standalone library shows that it is more efficient and out-\nperforms existing point-range-filters by up to 4 Ã—across a range of\nsettings and distributions, while keeping the false-positive rate low.\n1 INTRODUCTION\nModern data sets are large and grow at increasing rates [ 45]. To\nprocess them data-intensive systems perform massive scans that\nincur significant performance and resource consumption penalties.\nWhile indices may reduce the scan pressure, they are not always\neffective due to size or predicate selectivity concerns, or due to the\nhigh maintenance costs and workload compatibility. Filters are a\nclass of approximate data structures that may effectively comple-\nment the workhorse data structures to reduce scans. Bloom-Filters\n(BFs) [3], are prominent representatives of this class that are effi-\ncient and compact. They avoid false negatives, while false positives\nare possible, yet the false positive rate (FPR) can be controlled by\nparameters such as bits/key or the number of hash functions. If a BF\nreturns true, the search key may be present or not and the system\nneeds to verify that through expensive scans or index-lookups. BFs\nonly support point-lookups, i.e. is key 4711 not in the dataset.\nState-of-the-Art Overview. Many algorithms and systems ne-\ncessitate efficient range filtering for queries such as: are there keys\nbetween 42 and 4711 in the dataset. Classical Prefix BFs or Min/Max\nindices (fence pointers, ZoneMaps [ 34] in Neteeza or Block-Range\nIndex [38] in PostgreSQL) can perform range-filtering, but are im-\npractical for point queries and result in a higher FPR.\nRosetta [ 29], SuRF [ 49] and ARF [ 2] are some recent proposals\nthat can handle point- and range-lookups on a unified data struc-\nture and serve as point-range filters (PRF). ARF [ 2] and SuRF [ 49]\nutilize tries and thus partially materialize the index at the cost of\nextra space. Such techniques result in increased range-filter sizes,\nÂ©2022 Copyright held by the owner/author(s). The original version is to appear in\nproceedings of the EDBT 2023, ISBN 978-3-89318-088-2 on OpenProceedings.org.\nDistribution of this paper is permitted under the terms of the Creative Commons\nlicense CC-by-nc-nd 4.0.\n8 10 12 14 16 18 20 221632641041051061081091010\nRangeMedium Small Large\nRange Range\nRosettaBloomRFSuRF\nBits/KeyFigure 1: bloomRF is efficient, general and augments exist-\ning approaches. The color indicates the approach with the\nbest FPR for different number of keys ( 103-5Â·107), and nor-\nmal distribution (data/queries) in standalone settings.\nthat are reduced by trie-truncation or require tedious training/re-\noptimization. These yield an a posteriori/offline creation. Rosetta\n[29] takes a different approach, where each key is decomposed\ninto a set of prefixes according to a dyadic interval scheme and\nimplicit Segment-Trees [ 13]. The key-prefixes are maintained in\na hierarchical set of BFs, one for each prefix length. Fig. 1 shows\na holistic PRF positioning in the problem space according to their\nFPR, for different space budgets and query ranges. It is a flattened\nversion of Fig. 11.E, where we average the FPR for 103-5Â·107keys.\nProblem 1: Existing point-range-filters are designed either\nfor small or for large query ranges . Existing Point-Range-Filters\nare optimized for handling different query ranges sizes. While\nRosetta [ 29] excels at relatively small ranges [21âˆ’26], SuRF [ 49]\noffers outstanding FPR for mid- and large-ranges [ 237âˆ’238]. On the\none hand, as stated in [ 29] trie-truncation techniques, like the ones\nused in SuRF may lose effectiveness as short query ranges may fall\nin the scope of the truncated suffixes and thus have higher probabil-\nity of being detected non-empty. On the other hand, range-lookups\nin Rosetta have logarithmic (sometimes linear) complexity with\nrespect to the query range size. It may lose efficiency for longer\nranges as probing a hierarchical set of BFs, implies higher mem-\nory or CPU-costs. Moreover, it is not always possible to bound\nthe query-range size. While short-ranges seem reasonable for KV-\nstores, this does not apply to other systems or workloads. Besides,\ndatatypes also have an impact: for doubles a range of 1 can be 261\nin the bit representation.\nProblem 2: Existing point-range-filters are offline. Existing\nPRF (ARF [ 2], SuRF [ 49], or Rosetta [ 29]) employ powerful optimiza-\ntions, which require a priori the complete dataset and are therefore\nconstructed offline . Hence, PRF cannot serve range-queries, while\ndata is being simultaneously inserted. This limits PRF applicabilityarXiv:2207.04789v2  [cs.DB]  22 Jul 2022\n\nin the general case, i.e., when PRF are used standalone, when the\ndata is too large, is streamed, or is not available in advance, etc.\nThe issue can be mitigated by the way PRF are integrated in\nlarger systems. KV-stores use a main-memory delta area to absorb\nnew data. The PRF leverage that delta and get constructed only\nwhen it gets full and thus holds the complete PRF dataset. Searching\nthe main-memory delta is handled otherwise, e.g. through its orga-\nnization ( HashSkipLists orHashLinkLists in RocksDB). While\nthis approach is practicable in such systems, it: (a) is a property\nof the system integration, not of the PRF; (b) disregards the extra\nspace for the delta; and (c) is far from optimal in general settings.\nPrefix-BFs and Min/Max filters may be constructed online, but\nare inadequate for point-querying. Rosetta [ 29] may be used online\nper se, yet some of its optimizations require the dataset a priori .\nProblem 3: Existing Point-Range-Filters exhibit non-robust\nperformance across a variety of workload- and data- distri-\nbutions. Existing PRF are sensitive to data and workload skew. For\ninstance, Rosetta claims [ 29] to outperform SuRF by 2 Ã—on normally\ndistributed workloads in RocksDB [ 30] as the suffix-truncation tech-\nniques in SuRF yield more prefix-collisions for small ranges.\nBloom-Range-Filter ( bloomRF ).We introduce bloomRF as a uni-\nfied data structure, supporting approximate point- andrange- mem-\nbership tests that can substitute existing BFs. bloomRF operates on\nprefixes of keys. Firstly, bloomRF introduces novel prefix hashing\n(Fig. 2) to efficiently encode range information in the hash-code of\nthe key. This information is based on certain dyadic intervals to\nwhich the key prefixes correspond. Secondly, bloomRF proposes\nnovel piecewise-monotone hash-functions (PMHF) that preserve local\norder and support fast range-lookups with fewer memory accesses.\nPMHF place information for adjacent prefixes side by side in an\noverlapped bit-array such that this information can be queried with\na single word access. Insertions and point-lookups (Fig. 2.A) behave\nmuch like in a BF except that in bloomRF they operate on prefixes.\nRange-lookups (Fig. 2.B) follow a two-path algorithm, computing\nthe intervals along the prefix-paths for the left and the right key,\nand probe a tight interval-set. The area in between is probed auto-\nmatically. PMHF incur fewer memory accesses, e.g. for the query\n[42,43](Fig. 2.B),ğ»4uses a single access to probe both points.\nOurcontributions are: (a) bloomRF is a unified point-range-\nfilter that is online and can serve queries, while data is being si-\nmultaneously inserted. (b) bloomRF hasconstant query complexity ,\nindependent of the query range size, due to PMHF and its two-\npath range-lookup algorithm. bloomRF has a near-optimal space\nKey 42= 0x0000000000101010P1P2P3P4H1(P1)Prefixes:H2(P2)H3(P3)H4(P4)Hash-Funktions:BitArray01010010â€¦0101Prefix P1Prefix P2Prefix P3Prefix P4Prefix P1Prefix P2Prefix P3Prefix P4Â´01010010â€¦0101H1(P1)H2(P2)Insertion/LookupA\nRange-LookupBH3(P3)H4(P4Â´)H4(P4)Lookup [ 42, 43 ]Left Key PathRight Key Path\nFigure 2: (a) bloomRF relies on PMHF and prefix hashing. (b)\nRange lookups traverse two prefix paths, probing automati-\ncally the area in between (shaded).complexity , due to prefix hashing. (c) bloomRF can serve small-to-\nlarge query ranges and can handle different workload- and data-\ndistributions. It supports integers ,floating-point numbers , and can\nserve as a multi-attribute filter. (d) bloomRF outperforms all base-\nlines by up to 4Ã—across a wide range of settings. bloomRF is more\nefficient as it achieves better performance and FPR at lower bits/key.\nOutline. We continue with a brief background (Sect. 2), overview-\ning key terms. On their basis we introduce basic bloomRF â€™s prefix\nhashing and PMHF (Sect. 3.1, 3.2), and range-lookup algorithm\n(Sect. 4). We present the theoretical model of basic bloomRF and\ncompare it to the theoretical lower bound [ 20] in Sect. 5, 6. While\nbasic bloomRF is simple, tuning-free, and suitable for ranges ğ‘…â‰¤214,\nvarious optimizations (Sect. 7) are needed for larger queries. We\npresent the evaluation in Sect. 9 and conclude in Sect. 11.\n2 BACKGROUND\nWe now overview well-known BFs and dyadic intervals from the\nperspective of bloomRF and establish several key terms.\nBloom-Filters (BF) . Consider a set ğ‘‹={ğ‘¥1,ğ‘¥2,...,ğ‘¥ğ‘›}âŠ†ğ·ofğ‘›\nelements in a domain ğ·represented by ğ‘‘bits,|ğ·|=2ğ‘‘, e.g. d=16\nfor UINT16. We call the elements ğ‘¥âˆˆğ‘‹keys and arbitrary elements\nğ‘¦âˆˆğ·lookup keys. A BF[ 3] uses a bit-array of mbits with positions\nğ‘€={0,1,...,ğ‘šâˆ’1}andğ‘˜hash functions â„ğ‘–mappingğ·toğ‘€\n(i.e.â„ğ‘–:ğ·â†’ğ‘€,ğ‘–=ğ‘˜âˆ’1,..., 1,0). Noticeably, the hash functions\ntransform each lookup key ğ‘¦âˆˆğ·in acode of bit-array positions:\nğ‘ğ‘œğ‘‘ğ‘’(ğ‘¦)=\u0000â„ğ‘˜âˆ’1(ğ‘¦),â„ğ‘˜âˆ’2(ğ‘¦),...,â„ 0(ğ‘¦)\u0001. (1)\nInitially all bits in the bit-array are set to zero. To insert the set of\nkeysğ‘‹in a BF for each key ğ‘¥âˆˆğ‘‹the bits of code(ğ‘¥)are set to\none. A BF performs an approximate membership test to decide if a\nlookup keyğ‘¦âˆˆğ·is inğ‘‹, by checking, if all the bits of code(ğ‘¦)are\nset to one. This procedure may return positive results for elements\nğ‘¦âˆ‰ğ‘‹, called false-positives . The ratio between false-positives and\nnegatives is called false-positive rate .\nDyadic Intervals (DI). A DI is an interval whose boundaries are\naligned to powers of two. They can be organized in dyadic levels ,\nwhere an interval on level â„“spans 2â„“elements. For a domain rep-\nresented by ğ‘‘bits there are ğ‘‘+1dyadic levels â„“âˆˆ{0,1,...,ğ‘‘}.\nEach DIon levelâ„“+1is decomposed in two DIs on level â„“. Thus\nDIs form a complete binary tree. For example, for a domain ğ·of\nnon-negative integers with ğ‘‘=3bits there are ğ‘‘+1=4levels: on\nlevel 0 the DIs are the points [0,0],[1,1],...,[7,7]; on level 1 are\n[0,1],[2,3],. . .[6,7]; on level 2 are[0,3],[4,7]; and level 3 has just\n[0,7]. We show how bloomRF encodes DIs with code(ğ‘¦)in Sect. 3.\nPrefixes . Aprefix ofğ‘¦on levelâ„“is the sequence of the ğ‘‘âˆ’â„“most\nsignificant bits of ğ‘¦. These bits are accessed by a right shift by â„“bits\n(i.e.ğ‘¦>>â„“), discarding the â„“least significant bits. Thus for â„“>â„“â€²\nğ‘¦>>â„“=(ğ‘¦>>â„“â€²)>>(â„“âˆ’â„“â€²), (2)\ni.e., a prefix of ğ‘¦on levelâ„“is a prefix of a prefix of ğ‘¦on levelâ„“â€².\nNoticeably, prefixes are DIs . There is a one to one correspondence\nbetween prefixes on level â„“and DIs on level â„“, i.e., all lookup keys\nğ‘¦with an identical prefix on level â„“form a DI on level â„“. Consider,\nfor instance, a domain ğ·of non-negative integers represented by\nğ‘‘=3bits. The prefixes of a key ğ‘¦=5(bin 0b101) are1=0b1on\nlevel 2,2=0b10 on level 1and5=0b101 on level 0. The prefixes of\nğ‘¦=6=0b110 are0b1on level 2,0b11 on level 1and0b110 on level 0.\n\nThe prefixes of ğ‘¦=7=0b111 are0b1on level 2,0b11 on level 1and\n0b111 on level 0. Finally, the prefix 0b11 on level 1corresponds to\nthe DIğ¼=[6,7]on level 1. Indeed, exactly the keys 6and7share\nthe prefix 0b11 on level 1.\n3 BLOOM-RANGE-FILTER\nBased on the above concepts we now introduce the main aspects\nofbloomRF such as prefix hashing andPMHF .\n3.1 Prefix Hashing\nIn a BF a lookup key ğ‘¦corresponds to code(ğ‘¦)of bit-array positions.\nThus, we check if ğ‘¦is inğ‘‹by testing if the bits at code(ğ‘¦)are set.\nThe core idea of bloomRF is to encode range information in the\ncode(ğ‘¦)itself. To this end, we introduce code(ğ‘¦)ğ‘–asthe prefix of\ncode(ğ‘¦)on layerğ‘–. We define code(ğ‘¦)ğ‘–as an ordered sequence of\nthe firstğ‘˜âˆ’ğ‘–hash-functions of code(ğ‘¦):\ncode(ğ‘¦)ğ‘–=\u0000â„ğ‘˜âˆ’1(ğ‘¦), â„ğ‘˜âˆ’2(ğ‘¦),..., â„ğ‘–(ğ‘¦)\u0001. (3)\nThus the prefixes code(ğ‘¦)ğ‘–are sub-sequences of bit-array posi-\ntions of code(ğ‘¦). As an example we refer to Fig. 3.A, which will\nbe explained in detail below. Here the code of key 42 and prefixes\ncode(42)ğ‘–for all layers ğ‘–âˆˆ{3,2,1,0}are shown.\nWhen performing a lookup, our goal is to check prefixes of\nlookup keyğ‘¦by testing bits at prefixes of code(ğ‘¦). The issue at hand\nis that there are ğ‘‘+1dyadic levels, but code(ğ‘¦)comprisesğ‘˜hash-\nfunctions, making it impossible to encode each level. Therefore, we\nchoose to consider only certain levelsâ„“ğ‘˜âˆ’1â‰¥â„“ğ‘˜âˆ’2â‰¥...â‰¥â„“0.(Fig.\n3.A exemplifies equidistant levels.)\nOn this premise, we define prefix hashing as a key property of\nbloomRF . It mandates that for each layer ğ‘–âˆˆ{ğ‘˜âˆ’1,..., 1,0}a prefix\nofğ‘¦on dyadic level â„“ğ‘–corresponds to a prefix of code(ğ‘¦)on layerğ‘–,\ni.e., arbitrary lookup keys ğ‘¦,ğ‘¦â€²âˆˆğ·satisfy\nğ‘¦>>â„“ğ‘–=ğ‘¦â€²>>â„“ğ‘–â‡’ code(ğ‘¦)ğ‘–=code(ğ‘¦â€²)ğ‘–. (4)\nPrefix hashing allows using code(ğ‘¦)to test if DIs on level â„“ğ‘–\ninclude keys ğ‘¥âˆˆğ‘‹. Remember that such DIs are prefixes on level â„“ğ‘–.\nBy prefix hashing such DIs correspond to prefixes code(ğ‘¦)ğ‘–, which are\nchecked by testing if the bits at code(ğ‘¦)ğ‘–are set. This way, bloomRF\nimplicitly encodes range information in the code(ğ‘¦).\nTable 1: Most important symbols and abbreviations.\nğ·,|ğ·|,ğ‘‘ domain Dof size|ğ·|=2ğ‘‘elements, e.g., 216for UINT16\nğ‘¥,ğ‘‹,ğ‘› ğ‘¥âˆˆğ‘‹âŠ†ğ·- keys in the filter,|ğ‘‹|=ğ‘›- number of keys\nğ‘¦ ğ‘¦âˆˆğ·- lookup keys\nlevelâ„“ levelâ„“âˆˆ{ğ‘‘,..., 1,0}- defines prefixes/dyadic intervals of keys\nğ‘€,ğ‘š ğ‘€ ={0,1,...,ğ‘šâˆ’1}- bit-array positions, |ğ‘€|=ğ‘šbits\nâ„ğ‘–â„ğ‘–:ğ·â†’ğ‘€- hash function, ğ‘–âˆˆ{ğ‘˜âˆ’1,..., 1,0}\ncode(ğ‘¦) a sequence of bit array positions: code(ğ‘¦)=(â„ğ‘˜âˆ’1(ğ‘¦),...,â„ 0(ğ‘¦))\nlayerğ‘– layerğ‘–âˆˆ{ğ‘˜âˆ’1,..., 1,0}- defines prefixes of code(ğ‘¦)\ncode(ğ‘¦)ğ‘–prefix of code(ğ‘¦)on layerğ‘–:code(ğ‘¦)ğ‘–=(â„ğ‘˜âˆ’1(ğ‘¦),...,â„ ğ‘–(ğ‘¦))\nâ„“ğ‘–â„“ğ‘˜âˆ’1â‰¥...â‰¥â„“0- levelâ„“ğ‘–corresponds to layer ğ‘–\nÎ” distance between levels: â„“ğ‘–=ğ‘–Î”\nğ‘˜ ğ‘˜â‰ˆâŒˆğ‘‘/Î”âŒ‰number of hash functions â„ğ‘–. Considering ğ‘›and the satura-\ntion of levels ğ‘˜=âŒˆ(ğ‘‘âˆ’log2ğ‘›)/Î”âŒ‰.\nğ¼,|ğ¼|ğ¼is a lookup interval with |ğ¼|elements\nğ‘… upper bound for range-query size: |ğ¼|â‰¤ğ‘…\nPMHF Piecewise-Monotone Hash-Function ğ‘€ğ»ğ‘–on layerğ‘–\nword bit-array elements of size 2Î”âˆ’1bits that PMHF read/write\nDI; BF Dyadic Intervals; Bloom-Filters\nlayeri3210levelâ„“iâ„“3 = 12â„“2 = 8â„“1 = 4â„“0= 0ai2357bi29313741h3h2h1h0Key 4223191901234â€¦19â€¦2900110â€¦1â€¦0h3h2h1h04223191914142821215000020181017A\nBkeysprefix:  code(42)3 preifx: code(42)1prefix: code(42)0prefix: code(42)2hi(x)=( ai + bi (x >> â„“i) ) mod m\nBi t-arraycode(42)code(42)1 isa prefixofcode(42) on layer1, with\n43231904823262501234567891011121314151617181920212223242526272829001100001010000001111100000000bitarraypositions(2,3,19). Figure 3: Hash functions and codes of keys.\nArbitrary hash-functions can be used for prefix hashing: eq. (4)\nis satisfied, if hash-functions of code(ğ‘¦)ğ‘–only operate on prefixes\non levelâ‰¥â„“ğ‘–. Using (2) we achieve this by:\ncode(ğ‘¦)=\u0000â„ğ‘˜âˆ’1(ğ‘¦>>â„“ğ‘˜âˆ’1),...,â„ 1(ğ‘¦>>â„“1),â„0(ğ‘¦>>â„“0)\u0001\nFinally, we have to determine the levels â„“ğ‘–and the number of\nhash-functions ğ‘˜. A natural choice are equidistant levels. Thus we\ndefine a distance Î”between two adjacent levels and set â„“ğ‘–=ğ‘–Î”. Then\nthe number of hash-functions is given by âŒˆ(ğ‘‘+1)/Î”âŒ‰. Depending\non the number of keys top layers saturate (Sect. 7). We omit such\nlevels and therefore use ğ‘˜=âŒˆ(ğ‘‘âˆ’log2ğ‘›)/Î”âŒ‰hash-functions.\nIntroductory example. Consider a set ğ‘‹={42,1414,50000}(Fig.\n3.B) ofğ‘›=3keys in a domain ğ·withğ‘‘=16bits. We use 10\nbits/key and Î”=4, yielding a bit-array with ğ‘š=10|ğ‘‹|=30bits\nandğ‘˜=âŒˆ(ğ‘‘âˆ’log2ğ‘›)/Î”âŒ‰=4hash-functions. For hashing we use\nmultiplication with prime numbers ğ‘ğ‘–andğ‘ğ‘–, followed by modğ‘što\ndetermine a position in ğ‘€, i.e.,â„ğ‘–(ğ‘¥)=\u0000ğ‘ğ‘–+ğ‘ğ‘–Â·(ğ‘¥>>â„“ğ‘–)\u0001modğ‘š.\nFigure 3.A shows layers ğ‘–, levelsâ„“ğ‘–and values for the hash-\nfunctions. For example, key 42has a code(2,3,19,19)of positions\nin the bit-array. Inserting all keys of ğ‘‹leads to a bit-array where\nthe bits 2, 3, 8, 10, 17, 18, 19, 20 and 21 are set to one and all others\nare zero (Fig. 3.B). Since we choose Î”=4bit shifts by levels â„“ğ‘–=4ğ‘–\ncan be displayed in hexadecimal representation. For example, for\nkey 42 (hex 0x002A ) the prefix on level ğ‘™3=12is0x0, on levelğ‘™2=8\nis0x00, on levelğ‘™1=4is0x002 and on level ğ‘™0=0is0x002A .\nRemember that due to prefix hashing (eq. 4), a prefix of ğ‘¦on\na certain dyadic level â„“ğ‘–corresponds to a code(ğ‘¦)ğ‘–, i.e. a prefix of\ncode(ğ‘¦)on layerğ‘–. Thus, keys 42and43=0x002B have the same\nprefix on level â„“1=4,42>>â„“1=0x002=43>>â„“1and code(42)1=\n(2,3,19)=code(43)1as required by eq.(4).\n\nRecall also that prefix hashing allows us to use code(ğ‘¦)to test if\nDIs on level â„“ğ‘–include keys ğ‘¥âˆˆğ‘‹by testing the positions of code(ğ‘¦)ğ‘–,\nsince these DIs are in fact prefixes of ğ‘¦on levelâ„“ğ‘–corresponding to\ncode(ğ‘¦)ğ‘–. Allğ‘¦âˆˆğ¼=[32,47]=[0x0020,0x002F]have the same prefix\n0x002 on levelâ„“1and therefore the same prefix code(ğ‘¦)1=(2,3,19)\non layer 1 (Fig. 3.A,B). Thus we check positions (2,3,19)to test,\nif a keyğ‘¥âˆˆğ‘‹is included in ğ¼. In this example, the answer is\npositive, which is true since indeed 42âˆˆğ¼. All lookup keys in\n[48,63]=[0x0030,0x003F]have(2,3,26)as code prefix on layer 1\n(e.g. 48). Checking positions (2,3,26)results negative, since 26 is\nset to zero, and here indeed ğ‘‹âˆ©[48,63]=âˆ….\nA DIğ¼on levelâ„“,â„“ğ‘–<â„“<â„“ğ‘–+1, can be decomposed in up to 2Î”âˆ’1\nintervals on level â„“ğ‘–, thusğ¼can be tested via these DIs on level â„“ğ‘–.\nFor example ğ¼=[42,43]on level 1 can be checked by testing [42,42]\nand[43,43]on level 0. The corresponding prefixes code(ğ‘¦)ğ‘–only\ndiffer in the hash-function on layer 0. While 42and43are adjacent,\nthe positions of the hash-functions â„0(42)=19andâ„0(43)=0are\nnot. Clearly, a hash-function on layer ğ‘–does not preserve the order\nof the prefixes ğ‘¦>>â„“ğ‘–. We tackle this in Sect. 3.2.\nPrefix hashing is hierarchical . DIs are arranged hierarchically\nby inclusion. Prefixes are DIs and follow the same hierarchy â€“ eq.\n(2). The prefixes code(ğ‘¦)ğ‘–also inherit that hierarchy by (4), hence\nbloomRF uses hierarchical hashing. Thus, by testing key ğ‘¦âˆˆğ·, all\nDIs on levels â„“ğ‘–includingğ‘¦are automatically tested.\nFor example, when testing key ğ‘¦=43with code(ğ‘¦)=(2,3,19,0),\nthe following prefixes are checked: prefix (2,3,19)=code(43)1cor-\nresponding to DI[32,47], prefix(2,3)=code(43)2corresponding to\n[0,255]and prefix(2)=code(43)3corresponding to[0,4095].\nSpace Efficiency .bloomRF has a near-optimal space efficiency (Sect.\n6) since code(ğ‘¦)itself contains range information in terms of corre-\nsponding DIs. In particular, prefix hashing encodes the difference\nbetween any two consecutive prefixes of a key in a single position\nas a single bit . For example, the difference between prefixes 0x002\non level 4 and 0x002A on level 0 of key 42 is encoded in a single bit.\n3.2 Piecewise-Monotone Hash-Functions\nAlthough prefix hashing results in near-optimal space consumption\nthe order of prefixes ğ‘¦>>â„“ğ‘–is not preserved by hash-function â„ğ‘–,\nincreasing significantly the query time of intervals ğ¼on levelâ„“,â„“ğ‘–<\nâ„“<â„“ğ‘–+1. To this end, and as a second core idea, bloomRF introduces\npiecewise-monotone hash-functions (PMHF) that are locally order\npreserving and place corresponding bits side by side in the bit-array.\nThis allows checking all bits of DIs of ğ¼on levelâ„“ğ‘–with hash-function\nâ„ğ‘–, in a single memory access , yielding better performance.\nNoticeably, arbitrary hash-functions â„ğ‘–can be easily extended\nto satisfy this property and remain compute-efficient:\u0010\u0010\nâ„ğ‘–\u0000ğ‘¥>>(Î”âˆ’1)\u0001modğ‘š\n2Î”âˆ’1\u0011\n<<(Î”âˆ’1)\u0011\n+ğ‘¥&(2Î”âˆ’1)\nThe new hmust preserve the order of the least significant Î”âˆ’1\nbits of a prefix. Therefore, ğ‘¥is right-shifted by Î”âˆ’1bits, such that â„\nonly operates on the rest. The bit-array is accessed in words of size\n2Î”âˆ’1, thereforeğ‘šmust be a multiple of 2Î”âˆ’1. In fact, the bit-array\ncan be viewed as an array of ğ‘š/2Î”âˆ’1words. The modulo operation\ndetermines a position in this word-array. Finally a left-shift by Î”âˆ’1\nbits, yields the position of the word in the bit-array. To keep the\norder the least significant Î”âˆ’1bits are added to the position. These\nbits are extracted with a bitwise AND (&) with the mask 2Î”âˆ’1.\nMH3MH2MH1MH042162410214141629030500002827298keys43162410348162411801234567891011121314151617181920212223242526272829303100100000101000001000000010011110Figure 4: PMHF codes of keys (positions in bit- array)\nCombining with prefix hashing we get\nğ‘€ğ»ğ‘–(ğ‘¥)=\u0010\u0010\nâ„ğ‘–\u0000ğ‘¥>>(â„“ğ‘–+Î”âˆ’1)\u0001modğ‘š\n2Î”âˆ’1\u0011\n<<(Î”âˆ’1)\u0011\n+(ğ‘¥>>â„“ğ‘–)&(2Î”âˆ’1),\nwhich we call piecewise-monotone hash-functions .\nFor example, consider again the set ğ‘‹={42,1414,50000}(Fig.\n4) for a domain with ğ‘‘=16bits. Again, we use Î”=4andğ‘˜=4\nhash-functions. Here we set ğ‘š=32sinceğ‘šmust be a multiple\nof2Î”âˆ’1=8, thus we use approximately 10 bits per key. Again we\nuse the hash-functions â„ğ‘–(ğ‘¥)=ğ‘ğ‘–+ğ‘ğ‘–ğ‘¥as in the previous example.\nFigure 4 shows the codes of keys ğ‘¥âˆˆğ‘‹using corresponding PMHF.\nInserting all keys of ğ‘‹leads to a bit-array where the bits 0, 2, 8, 10,\n16, 24, 27, 28, 29 and 30 are set to one and all others are zero.\nTo test the DI[42,43]the codes(16,24,10,2)and(16,24,10,3)\nhave to be checked. Both have the same prefix 16, 24 and 10 on levels\n3 to 1 and the positions 2 and 3 on level 0 lie side by side. Thus on\nlevel 0 both can be tested with a single word access . The positions 2\nand 3 on level 0can be described by the bit-mask ğ‘=0b00110000 and\na word access on the first byte of the bit-array yields ğ‘¤=0b10100000 .\nThe bits at 16, 24 and 10 are set ğ‘&ğ‘¤â‰ 0, thus a positive answer.\nFor interval[44,47]all codes(16,24,10,4),. . . ,(16,24,10,7)have\nto be tested. They have the same prefix 16, 24 and 10 on levels 3 to\n1 and positions 4 to 7 on level 0 lie side by side and can be tested\nwith a single word access . Positions 4 to 7 on level 0correspond to\nthe bit-mask ğ‘=0b00001111 and as above ğ‘¤=0b10100000 . The bits\nat 16, 24 and 10 are set, but ğ‘&ğ‘¤=0, thus the negative answer.\nRandom Scatter. We now consider the scatter of PMHF as they\nshould preserve local order, but also distribute words randomly\nover the bit-array. We compare bloomRF against the standard BF\nin RocksDB. For a fair comparison we use 2M keys and 10 bits/key,\nfor which BFs have 10Â·ln2=6.93hash functions, floored to 6\nin RocksDB, as basic bloomRF with 64-bit words ( Î”=7) uses\nğ‘˜=âŒˆ(ğ‘‘âˆ’log2ğ‘›)/Î”âŒ‰=6PMHF. First, we investigate how well PMHF\nscatter words. To this end (Fig. 5.A), we measure how many times\nwords (x-axis) of different layers are overlaid in a bit-array ele-\nment for different data distributions. As the relative frequencies are\nmostly flat curves (the strong zipfian skew affects layers 2 and 3)\nwe conclude that PMHF scatter randomly at word granularity for\nnormal, zipfian and uniform data distributions. Second , we consider\nthe scatter/overlying of bits within words, by looking holistically at\nthe bit-array. To this end, we compare the length of 0-bit runs (Fig.\n5.B), as well as the bit-distance between two consecutive 0-bit runs\n(Fig. 5.C), for both BF and bloomRF and zipfian, normal, uniform\n\ndata distributions. The 0-bit runs are a relevant metric as they indi-\ncate bit areas that have never been set. Thus, significant differences\nwould indicate randomization issues. Clearly, both bit-arrays are in\nsimilar states. Intuition: bloomRF is not worse than BFs, with view\nof the scatter of words and their overlaying in bit array elements for\ncommon data distributions like zipfian, normal or uniform. PMHF\nrandomize words sufficiently. These insights are substantiated by\nthe relative point FPR of bloomRF vs BF in the evaluation (Fig. 10).\nDegenerate data distributions and PMHF . There are rare cases\nof degenerate data distributions, where PMHF may become ineffi-\ncient. The core observation is that certain bits of a key determine\nthe bit position in a word of the bit-array, since PMHF are piecewise\nmonotone. In basic bloomRF with distance among levels Î”=7, for\nexample, if all bits 0-5, 7-12, 14-19, . . . , ğ‘–Î”...(ğ‘–+1)Î”âˆ’2, . . . contain\nthe valueğœ†âˆˆ0,1,...,ğ‘¤ =2Î”âˆ’1âˆ’1, then every PMHF sets bit ğœ†in its\nword. A data distribution that generates such keys with high prob-\nability can be defined by counting the number of appearances of\nthese bits in a key ğ‘ğ‘¥=|{ğ‘–=0,1,...,ğ‘˜âˆ’1|((ğ‘¥>>ğ‘–Î”)&ğ‘¤)=ğœ†}|+1\n0%10%20%30%40%50%60%70%80%90%100%\n02468101214161820221357911131517192102468101214161820normaluniformzipfian10Layer 1Layer 2Layer 3Layer 4Layer 5Layer 6A\n00,511,522,5\n12345678910Uniform BloomUniform BloomRFNormal BloomNormal BloomRFZipf BloomZipf BloomRF\nBlength of 0-bit runs [bits]Frequency [10^6]\n00,20,40,60,811,21,4\n12345678910Uniform BloomUniform BloomRFNormal BloomNormal BloomRFZipf BloomZipf BloomRFCdistance to the next run of 0-bits [bits]Frequency [10^6]\nFigure 5: (a) Random scatter over bloomRF layers; (b), (c)\nComparison of the bit-array scatter to a BF.\nxâˆˆ{128, 129,130, 131}MH1(x) MH2(x)0â€¦01001010â€¦0101001010â€¦010â€¦4567891011â€¦14151617181920212223â€¦3031H1(x) = [ xâ€¢ 23 + 17 ] mod32H2(x) = [ xâ€¢ 101 + 5 ] mod32xâˆˆ{128, 129,130, 131}â€¦0â€¦00111100001000â€¦0000000067891011121314151617181925262728293031A\nBMH1(x) = 8â€¢[ H1( x >> 3 ) ] mod 4+[ x >> 0 ] mod 8MH2(x) = 8â€¢[ H2( x >> 7 ) ] mod 4 +[ x >> 4 ] mod 8CMHK(x )MH2(x )MH1(x )MH0(x )â€¦â€¦â€¦â€¦Layer KLayer 1Layer 0â€¦Figure 6: (a) Hashing in BFs is planar. (b) PMHF preserve par-\ntial order assisting range-querying, and are (c) hierarchical.\nand finally normalizing ğ‘ğ‘¥=ğ‘ğ‘¥/Ã\nğ‘¦ğ‘ğ‘¦.bloomRF can handle such\ncases. We can employ slightly different hash functions, which per-\nmute the bits in the word. For instance, on each layer, we can apply\nthe original PMHF on half of the keys, while the other half is\ntackled by a PMHF that writes the words in reverse order .\nVertical PMHF and error-correction . The hierarchical structure\nof PMHF allows a new interpretation of hashing in bloomRF , which\ncan also be transferred to BFs. DIs on a level â„“ğ‘–correspond to prefixes\ncode(ğ‘¦)ğ‘–. With PMHF only the hash-function on layer ğ‘–operates\nonğ‘¦>>â„“ğ‘–, all others only on prefixes of ğ‘¦>>â„“ğ‘–. Thus one hash-\nfunction is primarily responsible for level â„“ğ‘–, namelyğ‘€ğ»ğ‘–. Therefore\nhashing in bloomRF ishierarchical with a separate PMHF for each\nlayer (Fig. 6.C). The hash-functions on higher layers are used for\nerror-correction . In the example (Fig. 4) the DI ğ¼=[416,431]an level\nâ„“1=4is represented by the prefix (16,25,2). We haveğ¼âˆ©ğ‘‹=âˆ…,\nbut hash-function ğ‘€ğ»1, which is primarily responsible for layer 1,\nyields an error, since the bit at position 2 is set to 1. Hash-function\nğ‘€ğ»2checks bit 25 of the bit-array, which is zero. Thus, we get a\nnegative as the error of ğ‘€ğ»1is corrected by ğ‘€ğ»2. BFs can be viewed\nin the same way: Keys are represented by one hash-function while\nthe others are used for error-correction. Since hashing in BFs is\nplanar and not hierarchical (Fig. 6.A,B), none of the hash-functions\nis preferred for representing keys or error-correction.\n4bloomRF OPERATIONS\nWe now provide a detailed description of the main operations in\nbloomRF such as insertion, point- or range-queries.\nInsertion and Point-Lookup. To insert a key ğ‘¥âˆˆğ‘‹thecode(ğ‘¥)\nof bit-array positions is computed via piecewise-monotone hash-\nfunctionsğ‘€ğ»ğ‘–(ğ‘¥),ğ‘–=ğ‘˜âˆ’1,..., 1,0, and the corresponding bits in\nthe bit-array are set to one. To test, if a lookup key ğ‘¦âˆˆğ·is inğ‘‹,\nthecode(ğ‘¦)of bitarray positions is computed via PMHF ğ‘€ğ»ğ‘–(ğ‘¦),\nğ‘–=ğ‘˜âˆ’1,..., 1,0, and bloomRF checks if all corresponding bits in\nthe bit-array are set. For these operations bloomRF behaves like a\nregular BF, except that the hash-functions are replaced by PMHF.\nRange-Lookup. Range-queries in bloomRF are based on the de-\ncomposition of arbitrary lookup intervals ğ¼in DIs. Hierarchical\nprefix hashing allows testing all these intervals together in one\npass. Additional DIs covering ğ¼are automatically checked. Next, we\nexplain, which DIs are considered for an interval ğ¼. Upon that we\nelaborate on the algorithm that computes and tests all these DIs.\n\nDecomposition in DIs. For an arbitrary interval ğ¼the DIs to be\nconsidered are defined in a two-path algorithm, one for the left and\none for the right bound of ğ¼. Starting from the top level, ğ½ğ‘‘=ğ·is\na covering of ğ¼. We proceed recursively. Suppose ğ½â„“+1,â„“<ğ‘‘, is a\ncovering ofğ¼. Then we decompose ğ½â„“+1=ğ¾ğ‘™âˆªğ¾ğ‘Ÿin two DIs and set\nğ½â„“as the one covering ğ¼. Ifğ¼is not covered by a single DI the path\nof covering intervals splits in two, a left ğ½ğ‘™\nâ„“=ğ¾ğ‘™and a rightğ½ğ‘Ÿ\nâ„“=ğ¾ğ‘Ÿ.\nWe describe only DIs considered for the left path as the right\none is mirror-inverted. Suppose ğ½ğ‘™\nâ„“+1,â„“<ğ‘‘, is a covering of the left\nbound ofğ¼. We decompose ğ½ğ‘™\nâ„“+1=ğ¾ğ‘™âˆªğ¾ğ‘Ÿin two DIs. If ğ¾ğ‘™âˆ©ğ¼â‰ âˆ…, we\nknow thatğ¾ğ‘ŸâŠ†ğ¼, thusğ¼ğ‘™\nâ„“=ğ¾ğ‘Ÿbelongs to the decomposition of ğ¼in\nDIs andğ½ğ‘™\nâ„“=ğ¾ğ‘™is a covering of the left bound of ğ¼. Else, ifğ¾ğ‘Ÿâˆ©ğ¼â‰ ğ¾ğ‘Ÿ,\nthenğ¾ğ‘Ÿcovers the left bound of ğ¼, thus we set ğ½ğ‘™\nâ„“=ğ¾ğ‘Ÿ. Otherwise\nthe decomposition of the left side is complete and we set ğ¼ğ‘™\nâ„“=ğ¾ğ‘Ÿ. As\nexample we look at the considered DIs for ğ¼=[45,60],ğ‘‘=16(Fig. 7).\nFrom level 16 to 5 ğ¼is covered by single DIs. On the top levels left\nand right path coincide. On level 4 the paths split with a covering of\nğ¼by two DIs. On level 3 the first DI ğ¼ğ‘Ÿ\n3of the decomposition of ğ¼is\ncalculated. Finally, ğ¼=[45,45]âˆª[46,47]âˆª[48,55]âˆª[56,59]âˆª[60,60].\nNext, we map the above intervals onto the layers. A covering\nğ½ğ‘™\nâ„“ğ‘–+1on levelâ„“ğ‘–+1is split into several DIs on the levels â„“ğ‘–â‰¤â„“<â„“ğ‘–+Î”.\nThese can be represented by at most 2Î”DIs on level â„“ğ‘–. Some of\nthem are coverings, while others belong to the decomposition of\nğ¼. All DIs of the decomposition have to be tested. The covering\nshould be as tight as possible, thus we take the intersection of\nthe intervals ğ½ğ‘™\nâ„“, which isğ½ğ‘™\nâ„“ğ‘–. Using PMHF 2Î”âˆ’1DIs an level â„“ğ‘–\nlay side by side in the bit-array, thus all DIs to be tested can be\nchecked with at most two word-accesses. The same applies to the\nright path, thus checks require at most four word-accesses per\nlayer. In our example the decomposition of ğ¼=[45,60]results\nin intervals[45,47]=ğ¼ğ‘™\n0âˆªğ¼ğ‘™\n1,[48,55]=ğ¼ğ‘Ÿ\n3and[56,60]=ğ¼ğ‘Ÿ\n2âˆªğ¼ğ‘Ÿ\n0\nto be probed. Thereby, the coverings are automatically checked:\nğ½â„“4=ğ½16=[0,65535]on level 16, ğ½â„“3=ğ½12=[0,4095]on level\n12,ğ½â„“2=ğ½8=[0,255]on level 8 and on level 4 a covering with\nğ½ğ‘™\nâ„“1=ğ½ğ‘™\n4=[32,47]andğ½ğ‘Ÿ\nâ„“1=ğ½ğ‘Ÿ\n4=[48,63].\nDetailed algorithm. We now describe how bloomRF performs\nrange queries for arbitrary intervals ğ¼=[l_key,r_key] (Algorithm\n1). The main loop iterates over the layers, with ğ‘–being the current\nlayer, which ranges from the top ğ‘–=ğ‘˜âˆ’1(Line 2) down to the bottom\nğ‘–=0. On layerğ‘–several tests are performed using PMHF ğ‘€ğ»ğ‘–. The\nalgorithm checks coverings ( ğ½â„“ğ‘–) and intervals of the decomposition\nJd= J16=[0, 65535]J6=[0, 63]J5=[32, 63]J4l =[32, 47][48, 63]=J4rlevel16covering intervals -J J3l =[40, 47]â€¦\nJ2l =[44, 47][48, 55]= I3r[56, 63]=J3r[56, 59]=I2r[60, 63]=J2rJ1l =[44, 45]I1l =[46, 47][60, 61]=J1rI0l =[45, 45][60, 60]= I0rlevel6level5level4level3level2level1level0â€¦Phase 1Phase 2left key path (45)right key path (60)decomposition intervals -Iâ€¦â€¦â€¦\nFigure 7: Dyadic intervals for range-query of I=[45,60].Algorithm 1: bloomRF Range-Lookup\n1Function RangeLookup( l_key, r_key ):\n2 Letğ‘–â†ğ‘˜âˆ’1;\n3 Letchecksâ†init_checks (l_key, r_key, i);\n4 while checks â‰ âˆ…do\n5 Letnew_checksâ†âˆ… ;\n6 foreach checkâˆˆchecks do\n7 ifcheck.is_covering =truethen\n8 iffilter.bit_access(ğ‘€ğ»ğ‘–(check.l_key))=1then\n9 Expand check to layerğ‘–âˆ’1an append to\nnew_checks ;\n10 else\n11 Letğ‘â†bit_mask(ğ‘â„ğ‘’ğ‘ğ‘˜.ğ‘™ _ğ‘˜ğ‘’ğ‘¦, ğ‘â„ğ‘’ğ‘ğ‘˜.ğ‘Ÿ _ğ‘˜ğ‘’ğ‘¦);\n12 Letğ‘¤â†filter.word_access(ğ‘€ğ»ğ‘–(check.l_key));\n13 ifğ‘&ğ‘¤â‰ 0then\n14 returnğ‘¡ğ‘Ÿğ‘¢ğ‘’ ;\n15 Letğ‘–â†ğ‘–âˆ’1;\n16 Letchecksâ†new_checks ;\n17 return false;\nofğ¼(unions of intervals ğ¼â„“ğ‘–). The variable checks (L. 3) contains the\ndata for these tests: check.l_key ,check.r_key andcheck.is_covering .\nThe algorithm loops over the checks of layersğ‘–(L. 6). For a covering\n(L. 8), only a single bit must be tested. If this bit is set, the checks\nfor the underlying layer are computed. Otherwise this interval does\nnot contain any keys ğ‘¥âˆˆğ‘‹. As an early stop condition no further\nlayers have to be checked. To test an interval of the decomposition\nofğ¼(L. 11), the algorithm has to test several bits. We compute a\nbit-maskğ‘and since we use PMHF, all necessary bits are read in a\nsingle-word bit-array access. If a bitwise AND yields a value â‰ 0,\nthen the filter claims the existence of a key ğ‘¥âˆˆğ¼and returns a\npositive answer. Otherwise, if all intervals of the decomposition\nget excluded, checks gets empty, yielding a negative answer.\n5 THEORETICAL MODEL\nWe now analyze space and time complexity of bloomRF , and begin\nwith an FPR estimate for range-queries. As shown in Sect. 4, for an\ninterval I=[l_key,r_key] several DIs are considered. There are several\nspecial cases, depending on the position of ğ¼. All have in common\nthat they (phase1, Fig. 7) start with a sequence of ğ‘–1coverings by\nsingle DIsğ½â„“ğ‘—,ğ‘—=ğ‘˜,ğ‘˜âˆ’1,...,ğ‘˜âˆ’ğ‘–1+1, which then (phase 2) split up\ninğ‘–2coverings by two DIs ğ½ğ‘™\nâ„“ğ‘—âˆªğ½ğ‘Ÿ\nâ„“ğ‘—,ğ‘—=ğ‘˜âˆ’ğ‘–1,ğ‘˜âˆ’ğ‘–1âˆ’1,...,ğ‘˜âˆ’ğ‘–1âˆ’ğ‘–2+1.\nSince here all intervals are coverings only single bits have to be\nchecked. Let ğ‘be the probability that a bit in the bit-array is set\nto zero. A false positive can only occur, if all DIs of phase 1 yield\npositive and all DIs of left side of phase 2 yield positive, while on\nthe right side an arbitrary combination is possible, or vice versa.\n\nWe estimate the FPR ğœ–by eq. (5).\nğœ–â‰¤ ( 1âˆ’ğ‘)ğ‘–1+ğ‘–2(2ğ‘–2âˆ’1âˆ‘ï¸\nğ‘–=0\u0012ğ‘–2\nğ‘–\u0013\nğ‘ğ‘–2âˆ’ğ‘–(1âˆ’ğ‘)ğ‘–+(1âˆ’ğ‘)ğ‘–2)\n=2(1âˆ’ğ‘)ğ‘–1+ğ‘–2(ğ‘–2âˆ‘ï¸\nğ‘–=0\u0012ğ‘–2\nğ‘–\u0013\nğ‘ğ‘–2âˆ’ğ‘–(1âˆ’ğ‘)ğ‘–\n|                         {z                         }\n=1âˆ’(1âˆ’ğ‘)ğ‘–2)\nâ‰¤2(1âˆ’ğ‘)ğ‘–1+ğ‘–2 (5)\nThe DIs on level â„“ğ‘–=ğ‘–Î”have length 2ğ‘–Î”. Thus, an arbitrary\nintervalğ¼of length|ğ¼|â‰¤2ğ‘–Î”is covered by at most two DIs on level\nâ„“ğ‘–and therefore in phase 2 at least layer ğ‘–is reached, i.e. ğ‘–1+ğ‘–2â‰¥ğ‘˜âˆ’ğ‘–.\nThusğœ–â‰¤2(1âˆ’ğ‘)ğ‘˜âˆ’ğ‘–and therefore ğœ–â‰¤2(1âˆ’ğ‘)ğ‘˜âˆ’ğ‘™ğ‘œğ‘”2(|ğ¼|)/Î”.\nIt remains to estimate the probability ğ‘that a bit in the bit-\narray is set to zero. For BFs the assumption of perfect random\nhash-functions leads to a probability of 1/ğ‘šof bits being set and\ntherefore the standard estimate [3] yields\nğ‘=\u0012\n1âˆ’1\nğ‘š\u0013ğ‘˜ğ‘›\nâ‰ˆğ‘’âˆ’ğ‘˜ğ‘›\nğ‘š.\nWe model the influence of the data distribution on PMHF by in-\ntroducing a constant ğ¶, such thatğ‘=(1âˆ’ğ¶/ğ‘š)ğ‘˜ğ‘›â‰ˆğ‘’âˆ’ğ¶ğ‘˜ğ‘›/ğ‘š.\nOur experiments (PMHF random scatter, Fig. 5) suggest that ğ¶=1\nfor common distributions such as uniform, normal and zipfian.\nSummary: for range lookups with max. query range ğ‘…, such that\n|ğ¼|â‰¤ğ‘…=2ğ‘–Î”, and common distributions bloomRF has an FPR of\nğœ–â‰¤2\u0010\n1âˆ’ğ‘’âˆ’ğ‘˜ğ‘›\nğ‘š\u0011ğ‘˜âˆ’log2(ğ‘…)/Î”\n, (6)\nwhereğ‘˜=âŒˆ(ğ‘‘âˆ’log2ğ‘›)/Î”âŒ‰(Sect. 3.1).\nFor point-queries bloomRF behaves like a BF, except that ğ‘˜, the\nnumber of hash-functions, is not a free parameter. Thus for common\ndistributions the point FPR is ğœ–â‰ˆ(1âˆ’ğ‘’âˆ’ğ‘˜ğ‘›\nğ‘š)ğ‘˜.\nFortime complexity we consider the operations in Sect. 4. The\ninsertion of keys and point lookups requires evaluating of ğ‘˜hash-\nfunctions, thus both have constant time O(ğ‘˜). Range-queries are\nhandled by algorithm 1. There are two loops: The outer loop (Line 4)\niterates over the layers and the inner loop (Line 6) over the checks\non layerğ‘–. Since there are ğ‘˜layers and at most 4 word-accesses per\nlayer, range-queries require also constant time O(ğ‘˜). Notably, the\nquery-time is independent of the size of the query-interval ğ¼.\n6 COMPARISON: SPACE/TIME COMPLEXITY\nWith the theoretical model in place, we now compare bloomRF â€™s\nspace and time complexity to Rosetta â€™s model, and to the theoretical\nlower bounds for point[7] and range-queries[20].\nSpace complexity. We estimate the space ğ‘šneeded by bloomRF\nto achieve a given FPR ğœ–by solving eq. (6) for ğ‘š.\n[7] has shown that any structure which answers point-queries\nwith FPRğœ–, needs at least ğ‘šâ‰¥ğ‘›log2(1/ğœ–)space. [ 20] shows that\nany structure, answering range-queries of range-size ğ‘…with FPR\nğœ–, necessitates at least ğ‘šâ‰¥ğ‘›log2(ğ‘…1âˆ’ğ‘‚(ğœ–)/ğœ–)âˆ’ğ‘‚(ğ‘›)space. [ 20]\ngives a family of lower bounds with a free parameter, ğ›¾>1:\nğ‘šâ‰¥ğ‘›log2\u0012ğ‘…1âˆ’ğ›¾ğœ–\nğœ–\u0013\n+ğ‘›log2\u0010\n1âˆ’4ğ‘›ğ‘…\n2ğ‘‘\u0011 \u0010\n1âˆ’1\nğ›¾\u0011\nğ‘’.\n 4 6 8 10 12 14 16 18\n 0 0.005 0.01 0.015 0.02 0.025 0.03Bits per Key (m/n)(A) Point Queries(B) Range Queries of Size R\nFPRFPRBloomRFRosettaLower Bound\n 6 8 10 12 14 16 18 20 22 24 26\n 0 0.005 0.01 0.015 0.02 0.025 0.03Bits per Key (m/n)(A) Point Queries(B) Range Queries of Size R\nFPRFPRR = 16R = 32R = 64BPoint QueryRange Query ofSizeRAFigure 8: Comparison of bloomRF to Rosetta and the the-\noretical lower-bound [7, 20] for (a) point and (b) range\nlookups.\nThe lower bound is therefore the point-wise maximum of these\nbounds. We can determine ğ›¾as a function of ğœ–to achieve this\nmaximum, leading to a single curve for the lower bound (Fig. 8).\nFurthermore we compare with Rosetta [29], which has four vari-\nants for point-range filters and the variant (F) first-cut solution\nin analyzed in terms of space complexity. The first-cut solution\nuses a BF for each level of DIs, with FPR of ğœ–on the bottom level\nand1/(2âˆ’ğœ–)on all others. In [ 29] it is stated, that (F) achieves\nan FPR ofğœ–for range-queries of intervals up to length ğ‘…using\nğ‘šâ‰ˆlog2(ğ‘’)Â·ğ‘›log2(ğ‘…/ğœ–). Figure 8 shows the estimates for bloomRF ,\nlower bounds and Rosetta for point-queries (left) and range-queries\nof intervals of length ğ‘…=16,32,64(right) for a domain of ğ‘‘=64\nbit integers. For point-queries bloomRF andRosetta are close, but\nbloomRF always uses a little bit more memory except at one FPR.\nThe reason is, that for bloomRF the number of hash-functions is\ndetermined by the datatypeâ€™s domain size, ğ‘˜=âŒˆ(ğ‘‘âˆ’log2ğ‘›)/Î”âŒ‰, such\nthat the for BFs known optimal choice of ğ‘˜=ln(2)Â·ğ‘š/ğ‘›cannot be\nused. For range-queries the distance between Rosetta an the lower\nbound is given by a near-constant factor. bloomRF improves over\nRosetta , especially with larger ğ‘…, i.e., larger Î”, and gets closer to\nthe theoretical lower bound. The foundation for space savings in\nbloomRF isprefix hashing , yielding a near space-optimal PRF.\nTime-Complexity. Range queries in bloomRF are answered in\nconstant timeO(ğ‘˜), independent of the range size ğ‘….\nRosetta uses a BF for every dyadic level, but all levels except to\nlowest have larger FPRs, e.g. 1/(2âˆ’ğœ–)in the first-cut solution (F).\nTo improve FPR a process of doubting is applied. If a DI on level â„“\nyields a positive result, the two DIs on the level below are tested.\nIn the worst case, this may yield query-time linear in ğ‘…. According\nto [29], (F) has avg. query time O(log2(ğ‘…)/ğœƒ2)for intervalsâ‰¤ğ‘….\nTwo more variants also have log. avg. query-time O(log2(ğ‘…)/ğœƒâ€²2)\n[29]. An optimized variant (O), where as in (F) a BF is used for each\nlevel, but the FPRs ğœ–â„“on the levels are adjusted to how often inter-\nvals are queried, and a variable-level variant (V) similar to (O), but\nusing different weights, pushing more bits to lower levels, improv-\ning FPR of lower at cost of higher FPR of the middle and top levels.\nFinally a single-level variant (S) is suggested, where only a single\nlevel of Rosetta is used. Here range-queries are answered by testing\nevery element of an interval, yielding linear time [29].\nSpace efficiency, FPR and Query-range size. Rosetta is designed\nfor relatively small query ranges and KV-stores, and covers its\ndesign space well. Consider Rosetta (F) due to its logarithmic time\ncomplexity for longer ranges and its space requirement of ğ‘™ğ‘œğ‘”2(ğ‘’)Â·\nğ‘›ğ‘™ğ‘œğ‘” 2(ğ‘…/ğœ–)bits/key to achieve an FPR ğœ–for range-queries ğ‘…â‰ˆ2â„“\n\n[20,29]. For example, to achieve an FPR of 2%for ranges|ğ‘…|=26,\nRosetta uses 17 bits/key, yet for |ğ‘…|=210it already demands 22\nbits/key, while for|ğ‘…|=214it requires 28 bits/key. Given 17 bits/key,\nbasic bloomRF can handle ranges of |ğ‘…|=214with an FPR of 1.5%\nand low probe-latency, while with 22 bits/key basic bloomRF covers\n|ğ‘…|=221with 2.5% FPR, whereas with optimizations bloomRF can\nimprove on those significantly.\n7 OPTIMIZATIONS\nObservation. Basic bloomRF is simple, tuning-free, and can handle\nrange queries with ğ‘…â‰¤214with acceptable FPR and space budgets.\nHowever, the theoretical model also shows that further optimiza-\ntions for larger ğ‘…are needed. Next, we describe them.\nReplicated Hash-Functions. The number of hash-functions to\nquery DIsğ¼decreases with|ğ¼|. Larger DIs correspond to shorter\nprefixes, by prefix-hashing to shorter prefixes of the code and thus\nless hash-functions are used, weakening error-correction. To in-\ncrease the number of hash-functions on higher layers bloomRF uses\nreplicated hash-functions . They write replica of words of the original\nPMHFğ‘€ğ»ğ‘–but at different bitarray word-positions, preserving the\nlocal order defined by PMHF ğ‘€ğ»ğ‘–. Thus bloomRF hasğ‘Ÿğ‘–functions\nper layer (incl. ğ‘€ğ»ğ‘–), where typically ğ‘Ÿğ‘–=1for lower layers.\nVariable Distance Between Levels. While large Î”between lev-\nels work well on lower layers, as basic bloomRF shows, for higher\nlayers the exponentially increasing size of DIs is one reason for\nthe rapid increase of the FPR. Therefore we aim at smaller dis-\ntances on higher layers. To this end bloomRF uses a vector Î”=\n(Î”ğ‘˜âˆ’1,Î”ğ‘˜âˆ’2,...,Î”0)âˆˆNğ‘˜, defining level â„“ğ‘–corresponding to layer\nğ‘–byğ‘™ğ‘–=Ãğ‘–âˆ’1\nğ‘—=0Î”ğ‘—. Smaller distances on higher layers also increase\nthe number of hash-functions on higher layers.\nMemory Management. The relative frequency of DIs including\nkeys changes with the level. On low levels this frequency is nearly\nzero. Consider for example ğ‘›=50Â·106keys in a domain of integers\nwithğ‘‘=64. Level 0is nearly empty, since ğ‘›< <2ğ‘‘. But each\nincrement of the level halves the number of intervals. Thus in mid\nlevels more intervals are occupied and top levels saturate, depending\non the data distribution. Hash-functions used on saturated levels\nalmost always yield positive answers, such that these functions\ncan be omitted. In the above example, levels 39 to 64 (26 top levels)\nsaturate, given a uniform key distribution.\nThe next lower levels are more then 25% occupied, but their size\nis not much larger then ğ‘›. For example level 38 has size â‰ˆ1.34ğ‘›,\nlevel 37 has sizeâ‰ˆ2.68ğ‘›. Therefore a radical design decision is to\nuse one segment to store one level of DIs in an exact bitmap.\nFrom eq. (6) we see the FPR decreases step-wise with the level.\nFor better balance, we adjust the probability ğ‘, by separating the\nbit-array into ğ‘†=3memory segments, one for an exact layer,\none for the mid layers and one for the lower layers. More for-\nmallyğ‘š=ğ‘š1+ğ‘š2+...+ğ‘šğ‘†, where layer ğ‘–is assigned a segment\nğ‘—ğ‘–âˆˆ{1,2,...,ğ‘†}. The sizeğ‘š1is determined by the position of the\nexact layer. By increasing ğ‘š2, the segment for the mid layers, we\ncan improve ğ‘for these layers, thus improving the FPR an larger\nintervals, simultaneously reducing ğ‘š3and thus FPR on the lower\nlayers, especially for point-queries.\nSummary: To handle large query ranges bloomRF typically employs\nthe following strategy: (i) sparser bottom layers with large wordsizes (e.g. 64-bit) are packed together in one segment of the bit-array\nwith a single PMHF per layer; (ii) mid-layers with small word sizes\n(e.g. 8-bit or smaller) are stored in a separate and sparser segment\nwith replicating hash-functions besides the PMHFs to lower the\nerror-rates; (iii) a mid-upper layer is stored exactly in an exclusive\nsegment; (iv) the top layers are discarded as they saturate.\nExtended Model. We now describe a general bloomRF model to\nevaluate the FPR given the above optimizations. According to the\nfilter the DIs on each level â„“can be classified as: (a) empty ( ğ‘¡ğ‘›â„“);\nor are (b) non-empty and include a key ( ğ‘¡ğ‘â„“); or are (c) non-empty\nand do not include a key ( ğ‘“ğ‘â„“). Therefore, the FPR on level â„“is\nğ‘“ğ‘ğ‘Ÿâ„“=ğ‘“ğ‘â„“/(ğ‘“ğ‘â„“+ğ‘¡ğ‘›â„“).\nThe number of true positives ğ‘¡ğ‘â„“on each level can be derived\nfrom the distribution of the keys. For example, assuming uniform\ndistribution, the ğ‘›keys lie in approximately ğ‘›DIs on large enough\nlevels. Hence, the estimate: ğ‘¡ğ‘â„“=min(ğ‘›,2ğ‘‘âˆ’â„“). The numbers ğ‘“ğ‘â„“\nandğ‘¡ğ‘›â„“are estimated by recursion on the levels ğ‘™ğ‘–=Ãğ‘–âˆ’1\nğ‘—=0Î”ğ‘—corre-\nsponding to layers ğ‘–. We assume level â„“ğ‘˜is stored exactly, therefore\nğ‘“ğ‘â„“=0andğ‘¡ğ‘›â„“=2ğ‘‘âˆ’â„“âˆ’ğ‘¡ğ‘â„“,â„“=ğ‘‘,ğ‘‘âˆ’1,...,â„“ğ‘˜.\nSuppose we have computed ğ‘“ğ‘â„“ğ‘–andğ‘¡ğ‘›â„“ğ‘–corresponding to layer\nğ‘–. For the layer below, i.e., layer ğ‘–âˆ’1, we consider the levels â„“=â„“ğ‘–âˆ’\n1,â„“ğ‘–âˆ’2,...,â„“ğ‘–âˆ’1. A DI on a level splits in two DIs on the underlying\nlevel. Therefore, each DI ğ¼on levelâ„“ğ‘–includes 2â„“ğ‘–âˆ’â„“DIs on level â„“.\nIfğ¼is true negative, all 2â„“ğ‘–âˆ’â„“intervals are also true negatives. If ğ¼is\nfalse or true positive, then some of the 2â„“ğ‘–âˆ’â„“intervals can be false\npositive. Since ğ‘¡ğ‘â„“are true positive the number of potentially false\npositive intervals on level â„“isğ‘“ğ‘ğ‘ğ‘œğ‘¡\nâ„“=2â„“ğ‘–âˆ’â„“(ğ‘“ğ‘â„“ğ‘–+ğ‘¡ğ‘â„“ğ‘–)âˆ’ğ‘¡ğ‘â„“.\nFor these intervals the corresponding bits in segment ğ‘—ğ‘–âˆ’1of the\nbit-array will be probed. Let ğ‘be the probability that such a bit\nis set to zero. Analogous to section 5 we use the estimate ğ‘=(1âˆ’\nğ¶/ğ‘šğ‘—ğ‘–âˆ’1)ğ‘˜â€²Â·ğ‘›,whereğ¶models the influence of the data distribution\nand hereğ‘˜â€²=Ã\nğ‘—ğœˆ=ğ‘—ğ‘–âˆ’1ğ‘Ÿğœˆis the number of hash functions of segment\nğ‘—ğ‘–âˆ’1. For common distributions such as uniform, normal and zipfian\nwe can assume ğ¶=1(PMHF random scatter, Fig. 5).\nFor each potentially false positive DI on level â„“one or more\nbits will eventually be probed, depending on the number of hash-\nfunctionsğ‘Ÿğ‘–âˆ’1and layerğ‘–âˆ’1. Letğ‘â€²be the probability that such\na probe yields true, then ğ‘“ğ‘â„“=ğ‘â€²ğ‘“ğ‘ğ‘ğ‘œğ‘¡\nâ„“andğ‘¡ğ‘›â„“=2â„“ğ‘–âˆ’â„“ğ‘¡ğ‘›â„“ğ‘–+(1âˆ’\nğ‘â€²)ğ‘“ğ‘ğ‘ğ‘œğ‘¡\nâ„“. The probability ğ‘â€²can be computed by combinatorial\nformulas. For example for DIs on level â„“ğ‘–âˆ’1single bits a checked for\neach hash-function. Hence, ğ‘â€²=(1âˆ’ğ‘)ğ‘Ÿğ‘–âˆ’1. For DIs on level â„“ğ‘–âˆ’1+1\ntwo bits must be checked. For ğ‘Ÿğ‘–âˆ’1=1we getğ‘â€²=2ğ‘(1âˆ’ğ‘)+(1âˆ’ğ‘)2,\nforğ‘Ÿğ‘–âˆ’1=2we getğ‘â€²=2ğ‘2(1âˆ’ğ‘)2+4ğ‘(1âˆ’ğ‘)3+(1âˆ’ğ‘)4,etc.\nWe apply the FPR-model to our example in Section 3. The size of\nthe domain is|ğ·|=2ğ‘‘=16384 ,ğ‘‘=16, and we store ğ‘›=3keys. We\nassume Î”=4and thusğ‘˜=âŒˆ(ğ‘‘âˆ’log2ğ‘›)/Î”âŒ‰=4, orÎ”=(4,4,4,4).\nWe also assume one hash function per layer and a single shared seg-\nment, which is the bit-array with ğ‘š1=32bits. Levelâ„“4=ğ‘‘is the inter-\nval[0,16384], which is set when the first key is inserted. Thus, we as-\nsume it is stored exactly (it is a single bit, which is actually unused).\nIn our model we estimate ğ‘â‰ˆ0.683, where the relative frequency of\nbits set to 0 is 22/32â‰ˆ0.688. As estimate for the FPR on each level\nwe getğ‘“ğ‘ğ‘Ÿ=(0,0.95,0.78,0.53,0.32,0.27,..., 0.04,0.03,0.02,0.01).\nSo for point-queries we expect an FPR of 0.01 ( 1%) and for the\nintervals[0,32767],[32768,65535]an FPR of 0.95(95%).\n\nTuning Advisor. Given standard parameters like the number of\nkeysğ‘›, the memory budget ğ‘šand considering an (approx. max.)\nquery range size ğ‘…, the tuning advisor computes and selects an\nappropriate bloomRF configuration, comprising the parameters:\nvector Î”=(Î”ğ‘˜âˆ’1,Î”ğ‘˜âˆ’2,...,Î”0)âˆˆNğ‘˜, number of hash-functions ğ‘Ÿğ‘–\nand the assigned memory segment ğ‘—ğ‘–per layer, while using three\nsegments(ğ‘š1,ğ‘š2,ğ‘š3). Now we describe the procedure.\nFirst, we determine the exact level by means of a heuristic: its size\nshould beâ‰¤60%of the memory budget ğ‘š. Thus,â„“ğ‘’=min{â„“|2ğ‘‘âˆ’â„“<\n0.6ğ‘š}. The advisor examines multiple exact level candidates. For\nthe sake of simplicity, here we consider only: â„“ğ‘’andâ„“ğ‘’+1.\nThe position of the exact layer determines the vector Î”, the\nnumber of hash-functions and the assigned memory segments by\nthe following heuristics: For the lower layers we use Î”ğ‘–=7, which\nleads to a word-size of 64 bit and is as large as possible. The mid\nlayers are the transition region between lower layers and exact level.\nStarting from the lower layers we reduce Î”ğ‘–to match the exact\nlayer. As an example we consider ğ‘›=50Â·106keys with 14 bits/key\nin a domain with ğ‘‘=64bit. The lowest level with 2ğ‘‘âˆ’â„“<0.6ğ‘šis 36.\nFor the bottom levels we start with Î”ğ‘–=7and then reduce Î”ğ‘–to\nmatch 36. This results in a vector Î”=(2,2,4,7,7,7,7), which sums\nup to 36. We aim for as few replicated hash-functions as possible,\ntherefore we use only one hash-function per layer, and only on\nthe highest layer 2, e.g., ğ‘Ÿ=(2,1,1,1,1,1,1). The heuristic applied\nhere is: the closer we are to the exact layer, the higher the precision\nhas to be, and therefore we employ smaller Î”ğ‘–and use replicated\nhash-functions (but as few as possible). Finally memory segment\nğ‘š1=2ğ‘‘âˆ’ğ‘™is used for the exact, ğ‘š2for the middle and ğ‘š3for the\nbottom layers, e.g., ğ‘—=(2,2,2,3,3,3,3).\nSecond, with all other parameters defined by the above heuristics,\nfor a given exact level it remains to determine ğ‘š2, sinceğ‘š1+ğ‘š2+ğ‘š3=\nğ‘š. The goal is to minimize the FPR for range-queries of size up to\nğ‘…. Letğ‘“ğ‘ğ‘Ÿğ‘š=maxâŒŠlog2(ğ‘…)âŒ‹\nâ„“=0ğ‘“ğ‘ğ‘Ÿâ„“be the maximum FPR of DIs used\nfor rangesâ‰¤ğ‘…. Since the largest FPR-rates result from mid-top\nlevels (= large intervals), small intervals (= bottom levels) are under-\nprioritized. Thus we also consider ğ‘“ğ‘ğ‘Ÿğ‘=ğ‘“ğ‘ğ‘Ÿ0, i.e. point-query FPR.\nThe advisor makes a trade-off between lowering the range-query\nFPR (ğ‘“ğ‘ğ‘Ÿğ‘š) and the point-query FPR ( ğ‘“ğ‘ğ‘Ÿğ‘), as decreasing ğ‘“ğ‘ğ‘Ÿğ‘š\nmight imply higher ğ‘“ğ‘ğ‘Ÿğ‘. To this end, we define and minimize the\nweighted squared norm ğ‘“ğ‘ğ‘Ÿ2ğ‘¤=ğ‘“ğ‘ğ‘Ÿ2ğ‘š+ğ¶2ğ‘“ğ‘ğ‘Ÿ2ğ‘. It always holds\nğ‘“ğ‘ğ‘Ÿğ‘â‰¤ğ‘“ğ‘ğ‘Ÿğ‘š. As compensation we can increase ğ¶to weight point-\nqueries stronger. We determine all parameters for our exact level\ncandidatesâ„“ğ‘’andâ„“ğ‘’âˆ’1and select the configuration with min. ğ‘“ğ‘ğ‘Ÿğ‘¤.\nFinally, we select the configuration with minimum ğ‘“ğ‘ğ‘Ÿğ‘¤. The auto-\ntuning process is inexpensive, with computation times of ~8ms.\nFigure ??.C shows an example. For ğ‘›=50M keys, 16 bits/key and\nquery range|ğ‘…|=1010, the advisor examines â„“ğ‘’=27(red curve) and\nâ„“ğ‘’=28(blue curve). The minimum ğ‘“ğ‘ğ‘Ÿğ‘¤is marked on each curve\nand the blue one is chosen. Thus, we estimate an FPR of ~ 0.5%for\npoint-queries and ~ 3%for dyadic ranges up to size |ğ‘…|.\n8 DATATYPE SUPPORT\nVariable-length strings. The string support in bloomRF resem-\nbles SuRF-Hash [ 49] and considers the first seven characters in the\nseven most-significant bytes. In addition, for point queries it com-\nputes a one-byte hash-code of the rest of the string, including thelength, and places it in the least significant byte. This way bloomRF\nachieves a UINT64 representation of variable length-strings.\nFloating-Point Numbers. Floating-point numbers are represented\nwithğ‘bits for the mantissa ğœ‡,ğ‘Ÿbits for the exponent ğ‘’and one\nbit for the sign ğ‘ . For a bit combination ğ‘¥the represented value\nisğ‘“ğ‘™(ğ‘¥)=ğ‘ Â·ğœ‡Â·2ğ‘’. The bit combinations ğ‘¥are ordered as binary\nnumbers. Since floats are signed, this order is reversed for negative\nnumbers and is therefore lost. To this end, we use a map ğœ‘with\nğœ‘(ğ‘¥)=ğ‘¥+2ğ‘+ğ‘Ÿifğ‘¥ğ‘+ğ‘Ÿ=0andğœ‘(ğ‘¥)=ğ‘¥(bitwise inverse) other-\nwise, which is a monotone coding, i.e., ğœ‘(ğ‘¥)<ğœ‘(ğ‘¦)â‡”ğ‘“ğ‘™(ğ‘¥)<\nğ‘“ğ‘™(ğ‘¦). For all operations, we use ğœ‘(ğ‘¥)instead ofğ‘¥. To insertğ‘¥into\nbloomRF , we insertğœ‘(ğ‘¥). For a point-query of ğ‘¥we testğœ‘(ğ‘¥). For\na range-query[ğ‘¥,ğ‘¦], we perform a range-query with [ğœ‘(ğ‘¥),ğœ‘(ğ‘¦)].\nMulti-Attribute bloomRF .The ability to filter on multiple at-\ntributes simultaneously is necessary for complex operations in\ninteractive analytics, scientific packages, IoT and AI. bloomRF sup-\nports two-dimensional filtering with reduced precision. To this end\nweconcatenate the attribute-values and insert them in both combi-\nnations. For instance, bloomRF (A,B) will concatenate the values of\nAandB, and insert them as tuples <A,B>and<B,A>. The increased\nspace-requirements are lowered by reducing the precision of Aand\nB, e.g. to a 32-bit integer. As a result bloomRF can answer queries\nsuch as A<42ANDB=4711 ,A=42ANDB >4711 orA=42ANDB=4711 .\n9 EXPERIMENTAL EVALUATION\nIntegration in RocksDB [30]. bloomRF has been implemented\nin a standalone library and has been integrated in RocksDB v6.3.6\nthrough a standard filter policy. The policy is extended to pass\nquery-range information (lower/upper bounds) to the filter by\nmeans of slice structures . For persistence we implement our own\nser./deserialization mechanism, placing it as regular full filter block\nin each compaction-disabled SST file of a block-based table format .\nBaselines. Throughout the evaluation the following baselines are\nused: BFs, Prefix-BFs and fence pointers as well as state-of-the-art\npoint-range filters such as SuRF [ 43,44,49] and Rosetta [ 29]. We\nperform two types of experiments. First, system-level experiments,\nwhere all baselines are compared in RocksDB v6.3.6 to stress the\noverall effects in a real system. Second, standalone experiments are\nperformed to stress specific aspects in isolation.\nWorkloads. Throughout the evaluation we use a set of different\nworkloads. Firstly, we employ a derivative of YCSB [ 8] Workload E,\nwhich is range-scan intensive. The dataset comprises 50M 64-bit\ninteger keys, while the values are 512 bytes long. The data is uni-\nformly distributed, while the workloads are of normal, uniform and\nzipfian distributions. We issue 105queries of a single fixed range-\nsize that is specified in the respective experiments. All point- and\nrange-queries in this workload are empty (unless specified other-\nwise), which represents the worst-case . Depending on the workload,\nnon-empty queries may perform better, e.g. due to bloomRF â€™s early\nstop conditions. In fact, in a perfect system a perfect filter would\nincur minimal I/O, and thus the worst-case may overstate their\nimpact.\nRosetta and bloomRF rely on parameter tuning methods that\ncompute the proper filter-configurations, for given space budgets,\nnumber of keys and range sizes. SuRF, however, requires a suffix-\nlength parameter setting to tune itself to a space budget and trade\n\n00.010.020.030.040.050.06\n2166410^ 3 10^ 5 10^ 7 10^ 9 10^ 11(A1) Uniform\n2166410^ 3 10^ 5 10^ 7 10^ 9 10^ 11(B1) Normal\n2166410^ 3 10^ 5 10^ 7 10^ 9 10^ 1104812(C1) ZipfanFP R\nRange Size\nExecution time (s)Execution time (s)\nRange SizeRosetta (FPR)\nRosetta (Latency)SuRF (FPR)\nSuRF (Latency)BloomRF (FPR)\nBloomRF (Latency)\n26410^ 3 10^ 5 10^ 7 10^ 9 10^ 11141664256\n(D) Prefix-BF and Fence PointersPreï¬x Bloom Filters(Latency)\nFence Pointers (Latency)\n(A2) Point Query FPR\nRosetta SuRF BloomRF\n0.0000280.024596\n0.000184(B2) Point Query FPR\nRosetta SuRF BloomRF\n0.0000440.023688\n0.000176(C2) Point Query FPR\nRosetta SuRF BloomRF\n0.0000160.024792\n0.000240Figure 9: bloomRF has good performance for a variety of ranges and workload distributions in RocksDB (22 bits/key).\noff FPR, by selecting the appropriate variant. For some settings,\nwe were unable to select one, especially in RocksDB. Secondly, for\nthe floating point experiments we use a timeseries dataset from\nNASA[ 33]. Whereas for the multi-attribute experiments we utilize\na dataset from the Sloan Digital Sky Survey DR16 [42].\nExperimental Setup. The experimental server is equipped with\nan Intel E5-1620 3.50GHz, 32GB DDR4, and runs Ubuntu 16.04.\nExperiment 1: bloomRF is general-purpose and can handle\nvarious query ranges, from large to small. We begin by com-\nparing bloomRF against SuRF and Rosetta in RocksDB under con-\nditions favorable to all approaches. To this end, we employ a space\nbudget of 22 bits/key, 50M uniformly distributed keys and vary the\nquery range sizes and workload distributions (Fig. 9.A1, B1 and C1).\nIn terms of end-to-end probe latency ,bloomRF outperforms all\nbaselines, due to its two-path range-lookup and its CPU-efficient\nPMHF (Fig. 12.G). The sudden rise in bloomRF latency at|ğ‘…|=1011\nis due to approx. 1% non-empty ranges generated by the workload\ndriver because of the large interval size. Overall, bloomRF also\nhas the lowest FPRof all baselines. Rosetta is more accurate for\nvery short ranges (|ğ‘…|â‰¤8) as they hit its precise lower BF. Due to\nthe error-correcting effect of its PMHF bloomRF is more accurate\nthan Rosetta for small ranges of 16â‰¤|ğ‘…|â‰¤64, which must probe\nlarger area in its filters. The sudden fluctuations of Rosetta can be\nexplained with the switch between different variants. The good FPR\nofbloomRF forlarge ranges (e.g., 107â‰¤|ğ‘…|â‰¤1010) is due to the ability\nto probe more bits and the exact layer configurations. However,\nSuRFâ€™s LOUDS-encoding excels, for very large ranges (e.g.,|ğ‘…|=\n1011), while bloomRF still achieves an acceptable FPR of 0.0454, as it\nprobes larger areas of its mid-upper layers. Under the same settings,\nwe investigate the point-query FPR (Fig.9.A2, B2 and C2 shown as\nfigure-in-figure in Fig. 9). Rosetta exhibits the lowest point-query\nFPR due to its accurate bottom filter-layer. bloomRF needs more\nspace for its mid-upper layers yielding slightly higher FPR. SuRF\nhas the highest FPR due to its trie-truncation. All PRF outperform\nPrefix-BFs and fence pointers (Fig.9.D). Insight: bloomRF can handle\na broad set of query ranges and outperforms all baselines, under\nvarious workload distributions, addressing Problem 1 (Sect. 1).\nExperiment 2: bloomRF is efficient. We continue our compar-\nison, by varying the space budget in RocksDB (Fig.10). We start\nfrom the 22 bits/key (favorable for all approaches and used in the\nprevious experiment) and proceed to 10 bits/key, which is typical\nfor standard BFs. As we go, small (Fig. 10.A-C), medium (Fig. 10.D-F)andlarge (Fig. 10.G-I) range queries are performed. We use 50M\nkeys; data and workload are uniformly distributed.\nbloomRF outperforms all baselines. It remains competitive to\nRosetta for very small ranges and bigger space budgets ( â‰¥18\nbits/key). bloomRF also outperforms SuRF, except for very long\nranges (|ğ‘…|â‰¥1011). For point-lookups in RocksDB (Fig. 10, on the\nright) bloomRF is more accurate than the RocksDB BF due to the\nrandom scatter and the error-correction. For point-queries and 2M\nkeys, but in a standalone setting (Fig. 12.E1-E3) we compare all PRF,\nthe Cuckoo-Filter [ 16,17] and the BF from LevelDB [ 19]. We vary\nthe fingerprint sizes provided by the Cuckoo-Filter [ 16] and aim\nfor high occupancies (95%) to keep within the space budgets.\nIn terms of throughput bloomRF outperforms Rosetta 7% to 44% at\n22 and 10 bits/key, respectively. We elaborate by providing a detailed\nbreakdown of the probe-costs in RocksDB (Fig. 12.G). We use 22\nbits/key, 50M keys (2.06M per SST/filter), 105queries, uniform\nworkload/data distribution. bloomRF has the CPU- and total costs.\nInsight: Considering the performance and FPR at smaller space\nbudgets (Fig. 10,â‰¤18bits/key), we observe that bloomRF isefficient\nin terms of: (i) performance per bits/key; and (ii) FPR per bits/key.\nExperiment 3: bloomRF can handle skewed data distribu-\ntions. So far we only considered uniform data distributions. Now\nwe relax this assumption and investigate the impact of normal and\nzipfian data distributions in a standalone setting (Fig. 11). We also\nvary the number of keys ( 103..50M), the space budget, the query\nrange and the workload. The color of each point in Fig. 11 denotes\nthe best filter, while the symbol stands for the FPR difference to the\nsecond best filter or to bloomRF , in case it is not the best.\nWe observe that bloomRF can handle skewed data distributions\nacross various settings. For zipfian bloomRF is outperformed only\nin isolated cases. This is due to the underlying structure based\non bloom-techniques, where bits from bottom-mid layers can be\naccurately probed due to its vertical error-correction, while SuRF\nis truncating beyond a certain length. Rosetta (presumably its hash\nfunctions or its variable-level design) loses efficiency with |ğ‘…|â‰¥16.\nInsight: Fig. 11 depicts a holistic comparison among the PRF, on\nrelevant parts of the problem space. All three approaches bring\nsignificant advantages to the design space and augment each other.\nDue to its LOUDS-encoding, SuRF tends to be better for large ranges\n(108..1011), at higher space budgets with â‰¥14bits/key and more\nkeys. Rosetta tends to be better for very small query ranges with\nmore than 16 bits/key. bloomRF is generally applicable to various\n\nmemory budgets, different number of keys, and performs well for\ndifferent data distributions and workloads (Problem 3, Sec. 1).\nExperiment 4: bloomRF is online and concurrent insertions\nhave acceptable impact on its probe-performance at differ-\nent insert/probe ratios. We now quantify the online behavior, by\ninvestigating the impact of concurrent insertions on query perfor-\nmance and address Problem 2 (Sec. 1). To this end, we insert 50M,\nnot sorted or prepared, uniformly distributed keys with different\n(uniform) insert/lookup ratios (x-axis) in a standalone setting. In\nsingle-threaded settings (Fig. 12.A), the overall throughput increases\nwith higher insert/lookup ratios. Hence, the impact of insertions is\nacceptable. A deeper analysis in multi-threaded settings (Fig. 12.B)\nwith varying the number of concurrent lookup/insertion-threads\nshows that insertions have marginal impact on the lookup perfor-\nmance per thread. The overall insert-throughput increases with\nmore threads, although the throughput per insert-thread decreases.\nThis is not surprising as bloomRF is a parallel data structure.\nNext, we investigate the filter-construction costs (Fig. 12.C) on\nthe 50M, uniform dataset in RocksDB, where L0 comprises 25 SST\nfiles. We report the total creation and the serialization time (incl.\ntuning). bloomRF has the lowest creation time, due to its high\ninsertion performance. SuRF has relatively high overhead due to\nspace budget tuning and trie creation.\nExperiment 5: bloomRF can handle floats. Our floating-point\nnumbers dataset [ 33], contains positive and negative numbers. We\nexecute 1.8M range queries (standalone), of size 10âˆ’3. In absence of\nother baselines we only investigate bloomRF (Fig. 12.D). In absence\nof other baselines we only show that bloomRF achieves an avg. FPR\nof 0.18 for 10-22 bits/key and 4M lookups/s.Experiment 6: bloomRF can serve as multi-attribute filter.\nWe evaluate multi-attribute querying in bloomRF on a Sloan Digital\nSky Survey DR16 [ 42] dataset and extract the ObjectID and the Run\ncolumns. Their values roughly follow a normal distribution. In a\nstandalone setting, we compare a multi-attribute bloomRF(Run,\nObjectID) probed with Run<300 AND ObjectID=Const against two\nseparate filters bloomRF(Run) forRun<300andbloomRF(ObjectID)\nforObjectID=Const , combining the probe-results conjunctively.\nAs shown in (Fig. 12.F) bloomRF(Run,ObjectID) yields better\nFPR than the combined FPR of the two separate filter-lookups\nbloomRF(Run) andbloomRF(ObjectID) . This observation is surpris-\ning since the separate filters operate on 64-bit integers, while the\nmulti-attribute bloomRF reduces precision and operates on 32-bit in-\ntegers. The core intuition is that the FPR of bloomRF(Run,ObjectID)\ndepends on Z/Y, where YandZare the number of data points satisfy-\ningObjectID=Const andRun<300 AND ObectID=Const respectively.\n10 RELATED WORK\nBloom-Filters are well-known and with many variants [ 1,5,28,46]\ncovering different aspects: counting [ 4,18,41]; compressibility [ 31];\nSIMD vectorization [ 25,37]; partial deletes [ 40]; efficient hashing\n[15,23]; and data locality and novel hardware [ 6,14,25,27,39]. Re-\ncently, there have been numerous novel proposals [ 11,12,21,35,47],\nall of which are point-filters with different properties. Pioneered by\n[24,32], the concept of learned BFs, leads to interesting applications\n[22, 26, 48] and is a future direction for bloomRF .\nTheAdaptive Range Filter (ARF) [2] is one of the first approaches\nto describe the use of a simple form of dyadic numbering scheme\nto compute the covering intervals of a point. ARF, however, relies\n0.000.100.200.300.40\n10 12 14 16 18 20 22(G) Range 10^9\n10 12 14 16 18 20 22(H) Range 10^10\n10 12 14 16 18 20 2207142128 (I) Range 10^110.000.050.100.150.20\n(D) Range 10^4 (E) Range 10^5\n05101520(F) Range 10^60.000.100.200.300.40\n(A) Range 8 (B) Range 16\n07142128 (C) Range 32FPRSmall Ranges Medium Ranges Large Ranges\nExecution time (s)\nFPRRosetta(FPR)\nRosetta(Latency)SuRF(FPR)\nSuRF(Latency)BloomRF(FPR)\nBloomRF(Latency)Bloom Filter(FPR)\n07142128 (C) Range 32\n1E-051E-041E-031E-021E-01Point Query Uniform Workload\n1E-051E-041E-031E-021E-01Point Query Normal Workload\n1E-051E-041E-031E-021E-01\n10 12 14 16 18 20 22Point Query Zipï¬an Workload\nBits/Key\nFigure 10: bloomRF is efficient, with better performance for different space budgets and query ranges in RocksDB.\n\n1031041051061075*107\n81632-104105106-1081091010\n1031041051061075*107\n81632-104105106-1081091010\n101214161820221031041051061075*107\n10121416182022Zipï¬an WorkloadNormal WorkloadUniform Workload\nBits/KeyZipï¬an Key Distribution Normal Key Distribution Uniform Key Distribution\nRange\n1012141618202281632-104105106-1081091010\nSuRFBloomRFRosetta0.0001-0.0010.001-0.010.01-0.10.1-1.0Figure 11: bloomRF handles different data and workload distributions and remains efficient for different space budgets (10..22\nbits/key), query ranges (z-axis) and number of keys (y-axis). The color of each point represents the best filter, while the symbol\nstands for the relative FPR difference to the second best filter or to bloomRF , if not the best.\non a binary tree as a data structure and a powerful set of (learning)\noptimizations. Like bloomRF , ARF relies on the concept of covering\nthe whole domain of the datatype. SuRF [49] shows the full potential\nof trie-based filters (Fast Succinct Trie) with a powerful encoding\nscheme (LOUDS-Dense/Sparse). In bloomRF prefix hashing serves\nas an encoding scheme.\nRosetta [29], like bloomRF utilizes DIsand dyadic decomposi-\ntionfor point-range-filtering. The concept itself is applicable to a\nwider range of other applications such as stream processing and\nsummarization [ 9], hot/cold data separation techniques [ 10] or per-\nsistent sketches [ 36]. The Segment Trees employed by [ 9,29,36]\nhelp encoding interval information and mapping range-queries\ninto prefix-queries. bloomRF â€™s prefix hashing achieves near space-\noptimal and computationally efficient encoding interval. Another\nmajor difference to [ 9,29,36] is that bloomRF employs PMHF to\npreserve local order. They reduce the number of memory accesses\nwhen range querying and yields high range query performance.11 CONCLUSIONS\nWe introduce bloomRF as a unified PRF that extends BFs with\nrange-lookups. We propose novel prefix hashing to encode range\ninformation in the hash-code of the key, and novel PMHF for fast\nlookups and fewer memory accesses. We describe basic bloomRF\nthat is simple and tuning-free, and propose optimizations for han-\ndling larger ranges. bloomRF has near-optimal space- and constant\nquery-complexity and outperforms existing PRF by up to 4 Ã—.\nAcknowledgments. We thank the anonymous reviewers for the\nuseful comments and suggestions. We are deeply grateful to the\nauthors of [29] and [49] for providing the source code.\n\n63555473382184716501816989618282690188701.081190721.630189722.199183761860100020003000400050006000bloomRFRosettaSuRFbloomRFRosettaSuRFbloomRFRosettaSuRFbloomRFRosettaSuRFbloomRFRosettaSuRFbloomRFRosettaSuRFbloomRFRosettaSuRFbloomRFRosettaSuRFbloomRFRosettaSuRF12481632641001000Filter ProbeCPU cost (residual)DeserializationI/O Wait Time (Real)Executiontime breakdown [s](G)01224364810121416182022(C)Insert Time (s)Bits/KeyBloomRFSuRFRosetta\n0246810\n102030405060708090100(A)Million Ops/sPoint Query - Single ThreadedRange Query - Single Threaded0246\n012345678910012345678910\n(B)Number Lookup-ThreadsNumber Insert-Threads\nPercentage Lookups\nMillion Ops/sPoint Query Throughput/ThreadRange Query Throughput/ThreadPoint Insert Throughput/ThreadRange Insert Throughput/Thread\n0.00.20.40.60.81.0\n101214161820222402468\n(F)Bits/KeyFPR\nMillion Ops/sMulti-Attribute BloomRF(FPR)Multi-Attribute BloomRF(Throughput)Two Separate BloomRF(FPR)Two Separate BloomRF(Throughput)1E-051E-041E-031E-021E-0110121416182022Uniform WorkloadRosettaSuRFBloomRFBloom FilterCuckoo Filter1E-051E-041E-031E-021E-0110121416182022Uniform Workload10121416182022Normal Workload10121416182022Zipï¬an WorkloadFPRBits/Key(E.1)(E.2)(E.3)00.20.40.60.81\n10121416182022(D) Strings\nBits/KeyFPRBloomRFSuRF00.10.20.30.4\n1012141618202204812(E) Range 10^-3\nBits/KeyFPR\nMillion Ops/sBloomRF(FPR)BloomRF(Throughput)(D)Figure 12: (a, b) online behavior; (c) filter creation; (d) floats; (e) point-queries; (f) dual-attribute filter; (g) cost breakdown.\n\nREFERENCES\n[1]A. Abdennebi and K. Kaya. A bloom filter survey: Variants for different domain\napplications. arXiv, 2106.12189, 2021.\n[2]K. Alexiou, D. Kossmann, and P.-Ã…. Larson. Adaptive range filters for cold data:\nAvoiding trips to siberia. Proc. VLDB Endow. , 6(14), Sept. 2013.\n[3]B. H. Bloom. Space/time trade-offs in hash coding with allowable errors. Commun.\nACM , 13(7):422â€“426, 1970.\n[4]F. Bonomi, M. Mitzenmacher, R. Panigrahy, S. Singh, and G. Varghese. An\nimproved construction for counting bloom filters. In Proc. ESA , page 684â€“695,\n2006.\n[5]A. Broder and M. Mitzenmacher. Network Applications of Bloom Filters: A\nSurvey. In Internet Mathematics , volume 1, 2002.\n[6]M. Canim, G. A. Mihaila, B. Bhattacharjee, K. A. Ross, and C. A. Lang. Ssd\nbufferpool extensions for database systems. Proc. VLDB Endow. , 3(1â€“2):1435â€“1446,\nSept. 2010.\n[7]L. Carter, R. Floyd, J. Gill, G. Markowsky, and M. Wegman. Exact and approximate\nmembership testers. In Proc. STOC , page 59â€“65, 1978.\n[8]B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, and R. Sears. Benchmarking\ncloud serving systems with ycsb. In In Proc. SoCC , 2010.\n[9]G. Cormode and S. Muthukrishnan. An improved data stream summary: The\ncount-min sketch and its applications. J. Algorithms , 55(1), 2005.\n[10] G. Cormode and S. Muthukrishnan. Whatâ€™s hot and whatâ€™s not: Tracking most\nfrequent items dynamically. ACM Trans. Database Syst. , 30(1):249â€“278, Mar. 2005.\n[11] N. Dayan, M. Athanassoulis, and S. Idreos. Optimal bloom filters and adaptive\nmerging for lsm-trees. ACM Trans. Database Syst. , 43(4):16:1â€“16:48, 2018.\n[12] N. Dayan and M. Twitto. Chucky: A succinct cuckoo filter for lsm-tree. In Proc.\nSIGMOD , page 365â€“378, 2021.\n[13] M. de Berg, M. van Kreveld, M. Overmars, and O. Schwarzkopf. More geometric\ndata structures. In Computational Geometry: Algorithms and Applications , pages\n219â€“241. Springer Berlin Heidelberg, 2008.\n[14] B. Debnath, S. Sengupta, J. Li, D. J. Lilja, and D. H. Du. Bloomflash: Bloom filter\non flash-based storage. In Proc. ICDCS , pages 635â€“644, 2011.\n[15] P. C. Dillinger and P. Manolios. Bloom filters in probabilistic verification. In Proc.\nFMCAD , volume 3312, pages 367â€“381, 2004.\n[16] B. Fan, D. G. Andersen, and M. Kaminsky. Cuckoo-filter. Open-Source Implemen-\ntation. https://github.com/efficient/cuckoofilter, 2022.\n[17] B. Fan, D. G. Andersen, M. Kaminsky, and M. D. Mitzenmacher. Cuckoo filter:\nPractically better than bloom. In Proc. CoNEXT , page 75â€“88, 2014.\n[18] L. Fan, P. Cao, J. Almeida, and A. Broder. Summary cache: A scalable wide-area\nweb cache sharing protocol. IEEE/ACM Trans. Netw. , 8(3):281â€“293, June 2000.\n[19] S. Ghemawat and J. Dean. LevelDB. Open-Source Implementation. https://github.\ncom/google/leveldb, 2022.\n[20] M. Goswami, A. GrÃ¸nlund, K. G. Larsen, and R. Pagh. Approximate range\nemptiness in constant time and optimal space. SODA â€™15, page 769â€“775, USA,\n2015. Society for Industrial and Applied Mathematics.\n[21] G. Gupta, M. Yan, B. Coleman, B. Kille, R. A. L. Elworth, T. Medini, T. Treangen,\nand A. Shrivastava. Fast processing and querying of 170tb of genomics data via\na repeated and merged bloom filter (rambo). In Proc. SIGMOD , page 2226â€“2234,\n2021.\n[22] B. Hentschel, S. Idreos, and K. Deeds. Stacked filters: Learning to filter by structure.\nProc. VLDB Endow. , 14(4):600â€“612, 2020.\n[23] A. Kirsch and M. Mitzenmacher. Less hashing, same performance: Building a\nbetter bloom filter. Random Struct. Algorithms , 33(2):187â€“218, Sept. 2008.[24] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for learned\nindex structures. In Proc. SIGMOD , page 489â€“504, 2018.\n[25] H. Lang, T. Neumann, A. Kemper, and P. Boncz. Performance-optimal filtering:\nBloom overtakes cuckoo at high throughput. Proc. VLDB Endow. , 12(5):502â€“515,\nJan. 2019.\n[26] Q. Liu, L. Zheng, Y. Shen, and L. Chen. Stable learned bloom filters for data\nstreams. Proc. VLDB Endow. , 13(12):2355â€“2367, July 2020.\n[27] G. Lu, B. Debnath, and D. H. Du. A forest-structured bloom filter with flash\nmemory. In Proc. MSST , pages 1â€“6, 2011.\n[28] L. Luo, D. Guo, R. T. B. Ma, O. Rottenstreich, and X. Luo. Optimizing bloom filter:\nChallenges, solutions, and comparisons. IEEE Communications Surveys Tutorials ,\n21(2):1912â€“1949, 2019.\n[29] S. Luo, S. Chatterjee, R. Ketsetsidis, N. Dayan, W. Qin, and S. Idreos. Rosetta: A\nrobust space-time optimized range filter for key-value stores. SIGMOD, 2020.\n[30] Y. Matsunobu, S. Dong, and H. Lee. Myrocks: Lsm-tree database storage engine\nserving facebookâ€™s social graph. Proc. VLDB Endow. , 13(12):3217â€“3230, Aug. 2020.\n[31] M. Mitzenmacher. Compressed bloom filters. IEEE/ACM Transactions on Net-\nworking , 10(5):604â€“612, 2002.\n[32] M. Mitzenmacher. A model for learned bloom filters, and optimizing by sand-\nwiching. In Proc. NIPS , page 462â€“471, 2018.\n[33] NASA. Kepler labelled time series exoplanet dataset (Campaign 3). https://www.\nkaggle.com/keplersmachines/kepler-labelled-time-series-data, 2016.\n[34] Neteeza. ZoneMaps. https://www.ibm.com/docs/en/psfa/7.2.1?topic=\nstatistics-zone-maps, 2021.\n[35] P. Pandey, A. Conway, J. Durie, M. A. Bender, M. Farach-Colton, and R. Johnson.\nVector quotient filters: Overcoming the time/space trade-off in filter design. In\nProc. SIGMOD , page 1386â€“1399, 2021.\n[36] Y. Peng, J. Guo, F. Li, W. Qian, and A. Zhou. Persistent bloom filter: Membership\ntesting for the entire history. In Proc. SIGMOD , 2018.\n[37] O. Polychroniou and K. A. Ross. Vectorized bloom filters for advanced simd\nprocessors. In Proc. DaMoN , 2014.\n[38] Postgres. BRIN. https://www.postgresql.org/docs/14/brin-intro.html, 2021.\n[39] F. Putze, P. Sanders, and J. Singler. Cache-, hash-, and space-efficient bloom filters.\nACM J. Exp. Algorithmics , Jan. 2010.\n[40] C. E. Rothenberg, C. A. B. Macapuna, F. L. Verdi, and M. F. MagalhÃ£es. The\ndeletable bloom filter: A new member of the bloom family. Comm. Letters. ,\n14(6):557â€“559, June 2010.\n[41] O. Rottenstreich, Y. Kanizo, and I. Keslassy. The variable-increment counting\nbloom filter. In Proc. INFOCOM , pages 1880â€“1888, 2012.\n[42] Sloan Digital Sky Survey DR16. Server data with galaxies, stars and quasars.\nhttps://www.kaggle.com/muhakabartay/sloan-digital-sky-survey-dr16, 2019.\n[43] SuRF. Open source implementation. https://github.com/efficient/SuRF, 2020.\n[44] SuRF. Rocksdb integration. https://github.com/efficient/rocksdb, 2020.\n[45] A. Szalay and J. Gray. 2020 computing: Science in an exponential world. Nature ,\n2006.\n[46] S. Tarkoma, C. E. Rothenberg, and E. Lagerspetz. Theory and practice of bloom\nfilters for distributed systems. IEEE Communications Surveys Tutorials , 14(1):131â€“\n155, 2012.\n[47] D. Ting and R. Cole. Conditional cuckoo filters. In Proc. SIGMOD , 2021.\n[48] K. Vaidya, E. Knorr, T. Kraska, and M. Mitzenmacher. Partitioned learned bloom\nfilter. arXiv, 2006.03176, 2020.\n[49] H. Zhang, H. Lim, V. Leis, D. G. Andersen, M. Kaminsky, K. Keeton, and A. Pavlo.\nSurf: Practical range query filtering with fast succinct tries. Proc. SIGMOD, 2018.",
  "textLength": 74948
}