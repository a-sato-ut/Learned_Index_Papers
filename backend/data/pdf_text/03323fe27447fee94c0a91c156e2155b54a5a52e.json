{
  "paperId": "03323fe27447fee94c0a91c156e2155b54a5a52e",
  "title": "Cost Models for Big Data Query Processing: Learning, Retrofitting, and Our Findings",
  "pdfPath": "03323fe27447fee94c0a91c156e2155b54a5a52e.pdf",
  "text": "Cost Models for Big Data Query Processing: Learning,\nRetrofitting, and Our Findings\nTarique Siddiqui1,2, Alekh Jindal1, Shi Qiao1, Hiren Patel1, Wangchao Le1\n1Microsoft2University of Illinois, Urbana-Champaign\nABSTRACT\nQuery processing over big data is ubiquitous in modern\nclouds, where the system takes care of picking both the\nphysical query execution plans andthe resources needed to\nrun those plans, using a cost-based query optimizer. A good\ncost model, therefore, is akin to better resource efficiency\nand lower operational costs. Unfortunately, the production\nworkloads at Microsoft show that costs are very complex to\nmodel for big data systems. In this work, we investigate two\nkey questions: (i) can we learn accurate cost models for big\ndata systems, and (ii) can we integrate the learned models\nwithin the query optimizer. To answer these, we make three\ncore contributions. First, we exploit workload patterns to\nlearn a large number of individual cost models and combine\nthem to achieve high accuracy and coverage over a long pe-\nriod. Second, we propose extensions to Cascades framework\nto pick optimal resources, i.e, number of containers, during\nquery planning. And third, we integrate the learned cost\nmodels within the Cascade-style query optimizer of SCOPE\nat Microsoft. We evaluate the resulting system, Cleo , in a\nproduction environment using both production and TPC-H\nworkloads. Our results show that the learned cost models are\n2to3orders of magnitude more accurate, and 20×more cor-\nrelated with the actual runtimes, with a large majority ( 70%)\nof the plan changes leading to substantial improvements in\nlatency as well as resource usage.\nACM Reference Format:\nTarique Siddiqui1,2, Alekh Jindal1, Shi Qiao1, Hiren Patel1, Wangchao\nLe1 1Microsoft2University of Illinois, Urbana-Champaign . 2020.\nCost Models for Big Data Query Processing: Learning, Retrofitting,\nand Our Findings. In Proceedings of the 2020 ACM SIGMOD Inter-\nnational Conference on Management of Data (SIGMOD’20), June\n14–19, 2020, Portland, OR, USA. ACM, New York, NY, USA, 18 pages.\nhttps://doi.org/10.1145/3318464.3380584\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are not\nmade or distributed for profit or commercial advantage and that copies bear\nthis notice and the full citation on the first page. Copyrights for components\nof this work owned by others than ACM must be honored. Abstracting with\ncredit is permitted. To copy otherwise, or republish, to post on servers or to\nredistribute to lists, requires prior specific permission and/or a fee. Request\npermissions from permissions@acm.org.\nSIGMOD’20, June 14–19, 2020, Portland, OR, USA\n©2020 Association for Computing Machinery.\nACM ISBN 978-1-4503-6735-6/20/06. . . $15.00\nhttps://doi.org/10.1145/3318464.3380584\nDefault\tCost\tModel\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tManually\ttuned\tCost\tModel\twith\tActual\tCardinality\tFeedbackManually\ttuned\tCost\tModel\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tDefault\tCost\tModel\twith\tActual\tCardinality\tFeedback\na)\tPearson\tCorrelation\nModelPearson\tCorrelation\t0.040.100.090.14b)\tAccuracyUnder\tEstimationOver\tEstimationIdealFigure 1: Impact of manual tuning and cardinality feedback\non cost models in SCOPE\n1 INTRODUCTION\nThere is a renewed interest in cost-based query optimiza-\ntion in big data systems, particularly in modern cloud data\nservices (e.g., Athena [ 4], ADLA [ 40], BigSQL [ 22], and Big-\nQuery [ 20]) that are responsible for picking both the query\nexecution plans and the resources (e.g., number of containers)\nneeded to run those plans. Accurate cost models are therefore\ncrucial for generating efficient combination of plan and re-\nsources. Yet, the traditional wisdom from relational databases\nis that cost models are less important and fixing cardinalities\nautomatically fixes the cost estimation [ 31,33]. The question\nis whether this also holds for the new breed of big data sys-\ntems. To dig deeper, we analyzed one day’s worth of query\nlogs from the big data infrastructure (SCOPE [ 11,50]) at\nMicrosoft. We feed back the actual runtime cardinalities, i.e.,\nthe ideal estimates that any cardinality estimator, including\nlearned models [ 15,26,44,47] can achieve. Figure 1 com-\npares the ratio of cost estimates with the actual runtimes for\ntwo cost models in SCOPE: 1) a default cost model, and 2) a\nmanually-tuned cost model that is partially available for lim-\nited workloads. The vertical dashed-line at 100corresponds\nto an ideal situation where all cost estimates are equal to the\nactual runtimes. Thus, the closer a curve is to the dashed\nline, the more accurate it is.\nThe dotted lines in Figure 1(b) show that fixing cardinali-\nties reduces the over-estimation, but there is still a wide gap\nbetween the estimated and the actual costs, with the Pearson\ncorrelation being as low as 0.09. This is due to the complex-\nity of big data systems coupled with the variance in cloud\nenvironments [ 42], which makes cost modeling incredibly\ndifficult. Furthermore, any improvements in cost modelingarXiv:2002.12393v1  [cs.DB]  27 Feb 2020\n\nneed to be consistent across workloads and over time since\nperformance spikes are detrimental to the reliability expecta-\ntions of enterprise customers. Thus, accurate cost modeling\nis still a challenge in SCOPE like big data systems.\nIn this paper, we explore the following two questions:\n(1) Can we learn accurate, yet robust cost models for\nbig data systems? This is motivated by the presence of\nmassive workloads visible in modern cloud services that\ncan be harnessed to accurately model the runtime behavior\nof queries. This helps not only in dealing with the various\ncomplexities in the cloud, but also specializing or instance\noptimizing [35] to specific customers or workloads, which is\noften highly desirable. Additionally, in contrast to years of ex-\nperience needed to tune traditional optimizers, learned cost\nmodels are potentially easy to update at a regular frequency.\n(2) Can we effectively integrate learned cost models\nwithin the query optimizer? This stems from the obser-\nvation that while some prior works have considered learning\nmodels for predicting query execution times for a given phys-\nical plan in traditional databases [ 2,5,19,32], none of them\nhave integrated learned models within a query optimizer\nfor selecting physical plans. Moreover, in big data systems,\nresources (in particular the number of machines) play a sig-\nnificant role in cost estimation [ 46], making the integration\neven more challenging. Thus, we investigate the effects of\nlearned cost models on query plans by extending the SCOPE\nquery optimizer in a minimally invasive way for predicting\ncosts in aresource-aware manner . To the best of our knowl-\nedge, this is the first work to integrate learned cost models\nwithin an industry-strength query optimizer.\nOur key ideas are as follows. We note that the cloud work-\nloads are quite diverse in nature, i.e., there is no representa-\ntive workload to tune the query optimizer, and hence there\nis no single cost model that fits the entire workload, i.e.,\nno-one-size-fits-all . Therefore, we learn a large collection\nof smaller-sized cost models, one for each common subex-\npressions that are typically abundant in production query\nworkloads [ 23,24]. While this approach results in specialized\ncost models that are very accurate, the models do not cover\nthe entire workload: expressions that are not common across\nqueries do not have models. The other extreme is to learn\na cost model per operator, which covers the entire work-\nload but sacrifices the accuracy with very general models.\nThus, there is an accuracy-coverage trade-off that makes cost\nmodeling challenging . To address this, we define the notion\nof cost model robustness with three desired properties: (i)\nhigh accuracy, (ii) high coverage, and (iii) high retention, i.e.,\nstable performance for a long-time period before retraining.\nWe achieve these properties in two steps: First, we bridge\nthe accuracy-coverage gap by learning additional mutually\nenhancing models that improve the coverage as well as the\naccuracy. Then, we learn a combined model that automati-\ncally corrects and combines the predictions from multiple\nindividual models, providing accurate and stable predictions\nfor a sufficiently long window (e.g., more than 10 days).We implemented our ideas in a Cloud LEarning Optimizer\n(Cleo ) and integrated it within SCOPE. Cleo uses a feedback\nloop to periodically train and update the learned cost models\nwithin the Cascades-style top-down query planning [ 21]\nin SCOPE. We extend the optimizer to invoke the learned\nmodels, instead of the default cost models, to estimate the\ncost of candidate operators. However, in big data systems, the\ncost depends heavily on the resources used (e.g., number of\nmachines for each operator) by the optimizer [ 46]. Therefore,\nwe extend the Cascades framework to explore resources, and\npropose mechanisms to explore and derive optimal number\nof machines for each stage in a query plan. Moreover, instead\nof using handcrafted heuristics or assuming fixed resources,\nwe leverage the learned cost models to find optimal resources\nas part of query planning, thereby using learned models for\nproducing both runtime as well as resource-optimal plans .\nIn summary, our key contributions are as follows.\n(1)We motivate the cost estimation problem from produc-\ntion workloads at Microsoft, including prior attempts for\nmanually improving the cost model (Section 2).\n(2)We propose machine learning techniques to learn highly\naccurate cost models. Instead of building a generic cost model\nfor the entire workload, we learn a large collection of smaller\nspecialized models that are resource-aware and highly accu-\nrate in predicting the runtime costs (Section 3).\n(3)We describe the accuracy and coverage trade-off in learned\ncost models, show the two extremes, and propose additional\nmodels to bridge the gap. We combine the predictions from\nindividual models into a robust model that provides the best\nof both accuracy and coverage over a long period (Section 4).\n(4)We describe integrating Cleo within SCOPE, including\nperiodic training, feedback loop, model invocations during\noptimization, and novel extensions for finding the optimal\nresources for a query plan (Section 5).\n(5)Finally, we present a detailed evaluation of Cleo , using\nboth the production workloads and the TPC-H benchmark.\nOur results show that Cleo improves the correlation between\npredicted cost and actual runtimes from 0.1to0.75, the ac-\ncuracy by 2to3orders of magnitude, and the performance\nfor 70% of the changed plans (Section 6). In Section 6.7, we\nfurther describe practical techniques to address performance\nregressions in our production settings.\n2 MOTIVATION\nIn this section, we give an overview of SCOPE, its workload\nand query optimizer, and motivate the cost modeling problem\nfrom production workloads at Microsoft.\n2.1 Overview of SCOPE\nSCOPE [ 11,50] is the big data system used for internal data\nanalytics across the whole of Microsoft to analyze and im-\nprove its various products. It runs on a hyper scale infras-\ntructure consisting of hundreds of thousands of machines,\nrunning a massive workload of hundreds of thousands of jobs\nper day that process exabytes of data. SCOPE exposes a job\nservice interface where users submit their analytical queries\n\nFigure 2: 150instances of an hourly recurring job that\nextracts facts from a production clickstream.\nand the system takes care of automatically provisioning re-\nsources and running queries in a distributed environment.\nSCOPE query processor partitions data into smaller sub-\nsets and processes them in parallel. The number of machines\nrunning in parallel (i.e., degree of parallelism) depends on the\nnumber of partitions of the input. When no specific partition-\ning is required by upstream operators, certain physical op-\nerators (e.g., Extract andExchange (also called Shuffle )),\ndecide partition counts based on data statistics and heuristics.\nThe sequence of intermediate operators that operate over the\nsame set of input partitions are grouped into a stage — all\noperators in a stage run on the same set of machines. Except\nfor selected scenarios, Exchange operator is commonly used\nto re-partition data between two stages.\n2.2 Recurring Workloads\nSCOPE workloads primarily consist of recurring jobs . A\nrecurring job in SCOPE is used to provide periodic (e.g.,\nhourly, six-hourly, daily, etc.) analytical result for a specific\napplication functionality. Typically, a recurring job consists\nof a script template that accepts different input parameters\nsimilar to SQL modules. Each instance of the recurring job\nruns on different input data, parameters and have potentially\ndifferent statements. As a result, each instance is different\nin terms of input/output sizes, query execution plan, total\ncompute hour, end-to-end latency, etc. Figure 2 shows 150\ninstances of an hourly recurring job that extracts facts from a\nproduction clickstream. Over these 150instances, we can see\na big change in the total input size and the total execution\ntime, from 69,859GiB to 118,625GiB and from 40mins\nand 50seconds to 2hours and 21minutes respectively. Note\nthat a smaller portion of SCOPE workload is ad-hoc as well.\nFigure 3 shows our analysis from four of the production\nclusters. We can see that 7%−20%jobs are ad-hoc on a daily\nbasis, with the fraction varying over different clusters and\ndifferent days. However, compared to ad-hoc jobs, recurring\njobs represent long term business logic with critical value,\nand hence the focus of several prior research works [ 1,7–9,\n17,23–25,30,47] and also the primary focus for performance\nimprovement in this paper.\nAd-hoc Jobs (%)0.05.010.015.020.0\nDay1Day2Day3Cluster1Cluster2Cluster3Cluster4\n\u00001Figure 3: Illustrating ad-hoc jobs in SCOPE.\n2.3 Overview of SCOPE Optimizer\nSCOPE uses a cost-based optimizer based on the Cascades\nFramework [ 21] for generating the execution plan for a given\nquery. Cascades [ 21] transforms a logical plan using multiple\ntasks: (i) Optimize Groups, (ii) Optimize Expressions, (iii) Ex-\nplore Groups, and (iv) Explore Expressions, and (v) Optimize\nInputs. While the first four tasks search for candidate plans\nvia transformation rules, our focus in this work is essentially\non the Optimize Inputs tasks, where the cost of a physical\noperator in estimated. The cost of an operator is modeled to\ncapture its runtime latency , estimated using a combination\nof data statistics and hand-crafted heuristics developed over\nmany years. Cascades performs optimization in a top-down\nfashion, where physical operators higher in the plan are iden-\ntified first. The exclusive (or local) costs of physical operators\nare computed and combined with costs of children operators\nto estimate the total cost. In some cases, operators can have\narequired property (e.g., sorting, grouping) from its parent\nthat it must satisfy, as well as can have a derived property\nfrom its children operators. In this work, we optimize how\npartition counts are derived as it is a key factor in cost esti-\nmation for massively parallel data systems. Overall, our goal\nis to improve the cost estimates with minimal changes to\nthe Cascades framework. We next analyze the accuracy of\ncurrent cost models in SCOPE.\n2.4 Cost Model Accuracy\nThe solid red line in Figure 1 shows that the cost estimates\nfrom the default cost model range between an under-estimate\nof100×to an over-estimate of 1000×, with a Pearson cor-\nrelation of just 0.04. As mentioned in the introduction, this\nis because of the difficulty in modeling the highly complex\nbig data systems. Current cost models rely on hand-crafted\nheuristics that combine statistics (e.g., cardinality, average\nrow length) in complex ways to estimate each operator’s\nexecution time. These estimates are usually way off and get\nworse with constantly changing workloads and systems in\ncloud environments. Big data systems, like SCOPE, further\nsuffer from the widespread use of custom user code that ends\nup as black boxes in the cost models.\nOne could consider improving a cost model by consider-\ning newer hardware and software configurations, such as\nmachine SKUs, operator implementations, or workload char-\nacteristics. SCOPE team did attempt this path and put in\nsignificant efforts to improve their default cost model. This\n\nalternate cost model is available for SCOPE queries under a\nflag. We turned this flag on and compared the costs from the\nimproved model with the default one. Figure 1b shows the\nalternate model in solid blue line. We see that the correla-\ntion improves from 0.04to0.10and the ratio curve for the\nmanually improved cost model shifts a bit up, i.e., it reduces\nthe over-estimation. However, it still suffers from the wide\ngap between the estimated and actual costs, again indicating\nthat cost modeling is non-trivial in these environments.\nFinally, as discussed in the introduction and shown as\ndotted lines in Figure 1(b), fixing cardinalities to the perfect\nvalues, i.e., that best that any cardinality estimator [ 15,26,44,\n47] could achieve, does not fill the gap between the estimated\nand the actual costs in SCOPE-like systems.\n3 LEARNED COST MODELS\nIn this section, we describe how we can leverage the common\nsub-expressions abundant in big data systems to learn a large\nset of smaller-sized but highly accurate cost models.\nWe note that it is practically infeasible to learn a single\nglobal model that is equally effective for all operators. This is\nwhy even traditional query optimizers model each operator\nseparately.A single model is prone to errors because opera-\ntors can have very different performance behavior (e.g., hash\ngroup by versus merge join), and even the performance of\nsame operator can vary drastically depending on interactions\nwith underneath operators via pipelining, sharing of sorting\nand grouping properties, as well as the underlying software\nor hardware platform (or the cloud instance). In addition,\nbecause of the complexity, learning a single model requires a\nlarge number of features, that can be prohibitively expensive\nto extract and combine for every candidate operator during\nquery optimization.\n3.1 Specialized Cost Models\nAs described in Section 2.2, shared cloud environments often\nhave a large portion of recurring analytical queries (or jobs),\ni.e., the same business logic is applied to newer instances\nof the datasets that arrive at regular intervals (e.g., daily or\nweekly). Due to shared inputs, such recurring jobs often end\nup having one or more common subexpressions across them.\nFor instance, the SCOPE query processing system at Mi-\ncrosoft has more than 50%of jobs as recurring, with a large\nfraction of them appearing daily [ 25], and as high as 60%\nhaving common subexpressions between them [ 23,24,47].\nCommon subexpression patterns have also been reported in\nother production workloads, including Spark SQL queries\nin Microsoft’s HDInsight [ 41], SQL queries from risk con-\ntrol department at Ant Financial Services Group [ 51], and\niterative machine learning workflows [48].\nFigure 4 illustrates a common subexpression, consisting\nof a scan followed by a filter, between two queries. We ex-\nploit these common subexpressions by learning a large num-\nber of specialized models, one for each unique operator-sub-\ngraph template representing the subexpression. An operator-\nsubgraph template covers the root physical operator (e.g.,\nFilter) and all prior (descendants) operators (e.g., scan) from\nσ σγFigure 4: Illustrating common subexpressions.\nthe root operator of the subexpression. However, parameters\nand inputs in operator-subgraphs can vary over time , and are\nused as features for the model (along with other logical and\nphysical features) as discussed in Section 3.3.\nThe operator-subgraph templates essentially capture the\ncontext of root operator, i.e, learn the behavior of root physical\noperator conditioned on the operators beneath it in the query\nplan. This is helpful because of two reasons. First, the execu-\ntion time of an operator depends on whether it is running\nin a pipelined manner, or is blocked until the completion of\nunderneath operators. For example, the latency of a hash op-\nerator running on top of a filter operator is typically smaller\ncompared to when running over a sort operator. Similarly,\nthe grouping and sorting properties of operators beneath the\nroot operator can influence the latency of root operator [ 21].\nSecond, the estimation errors (e.g., of cardinality) grow\nquickly as we move up the query plan, with each intermedi-\nate operator building upon the errors of children operators.\nThe operator-subgraph models mitigates this issue partially\nsince the intermediate operators are fixed and the cost of\nroot operator depends only on the leaf level inputs. More-\nover, when the estimation errors are systematically off by\ncertain factors (e.g., 10x), the subgraph models can adjust\nthe weights such that the predictions are close to actual\n(discussed subsequently in Section 3.4). This is similar to\nadjustments learned explicitly in prior cardinality estimation\nwork [ 44]. These adjustments generalize well since recurring\njobs share similar schemas and the data distributions remain\nrelatively stable, even as the input sizes change over time.\nAccurate cardinality estimations are, however, still needed\nin cases where simple adjustment factors do not exist [47].\nNext, we discuss the learning settings, feature selection,\nand our choice of learning algorithm for operator-subgraphs.\nIn Section 5, we describe the training and integration of\nlearned models with the query optimizer.\n3.2 Learning Settings\nTarget variable. Given an operator-subgraph template, we\nlearn the exclusive cost of the root operator as our target. At\nevery intermediate operator, we predict the exclusive cost\nof the operator conditioned on the subgraph below it. The\nexclusive cost is then combined with the costs of the children\nsubgraphs to compute the total cost of the sub-graph, similar\nto how default cost models combine the costs.\nLoss function. As the loss function, we use mean-squared\nlog error between the predicted exclusive cost ( p) and actual\nexclusive latency ( a) of the operator:Í\nn(loд(p+1)−loд(a+1))2\nn,\n\nLoss Function Median Error\nMedian Absolute Error 246%\nMean Absolute Error 62%\nMean Squared Error 36%\nMean Squared-Log Error 14%\nTable 1: Median error using 5-fold CV over the production\nworkload for regression loss functions\nhere 1is added for mathematical convenience. Table 1 com-\npares the average median errors using 5-fold cross validation\n(CV) of mean-squared log error with other commonly used\nregression loss functions, using elastic net as the learning\nmodel (described subsequently Section 3.4). We note that not\ntaking the log transformation makes learning more sensitive\nto extremely large differences between actual and predicted\ncosts. However, large differences often occur when the job’s\nrunning time itself is long or even due to outlier samples\nbecause of machine or network failures (typical in big data\nsystems). We, therefore, minimize the relative error (since\nloд(p+1)−loд(a+1)=loд(p+1\na+1)), that reduces the penalty for\nlarge differences. Moreover, our chosen loss function helps\npenalize under-estimation more than over-estimation, since\nunder estimation can lead to under allocation of resources\nwhich is typically decided based on cost estimates. Finally,\nlog transformation implicitly ensures that the predicted costs\nare always positive.\n3.3 Feature Selection\nIt is expensive to extract and combine a large number of\nfeatures every time we predict the cost of an operator — a\ntypical query plan in big data systems can involve 10s of\nphysical operators, each of which can have multiple possible\ncandidates. Moreover, large feature sets require more number\nof training samples, while many operator-subgraph instances\ntypically have much fewer samples. Thus, we perform an\noffline analysis to identify a small set of useful features.\nFor selecting features, we start with a set of basic statistics\nthat are frequently used for estimating costs of operators\nin the default cost model. These include the cardinality, the\naverage row length, and the number of partitions. We con-\nsider three kinds of cardinalities: 1) base cardinality: the total\ninput cardinality of the leaf operators in the subgraph, 2) in-\nput cardinality: the total input cardinality from the children\noperators, and 3) output cardinality: the output cardinality of\nthe operator-subgraph. We also consider normalized inputs\n(ignoring dates and numbers) and parameters to the job that\ntypically vary over time for the recurring jobs. We ignore\nfeatures such as job name or cluster name, since they could\ninduce strong bias and make the model brittle to the smallest\nchange in names.\nWe further combine basic features to create additional\nderived features to capture the behavior of operator imple-\nmentations and other heuristics used in default cost models.\nWe start a large space of possible derivations by applying (i)\nlogarithms, square root, squares, and cubes of basic features,\n(ii) pairwise products among basic features and derived fea-\ntures listed in (i), and (iii) cardinality features divided by theFeature Description\nInput Cardinality (I) Total Input Cardinality from children operators\nBase Cardinality (B) Total Input Cardinality at the leaf operators\nOutput Cardinality (C) Output Cardinality from the current operator\nAverageRowLength (L) Length (in bytes) of each tuple\nNumber of Partitions (P) Number of partitions allocated to the operator\nInput (IN) Normalized Inputs (ignored dates, numbers)\nParameters (PM) Parameters\nTable 2: Basic Features\nCategory Features\nInput or Output data√\nI,√\nB, L*I, L*B, L*log(B), L*log(I), L*log(C)\nInput×Output B*C,I*C,B*log(C),I*log(C),log(I)*log(C),I*C, log(B)*log(C)\nInput or Output per\npartitionI/P, C/P, I*L/P, C*L/P,√\nI/P,√\nC/P,log(I)/P\nTable 3: Derived Features\nC\nsqrt(C)\nLog(B)*C\nB*log(C)\nB\nI*C\nI*log(C)\nI/P\nsqrt(I)\nL*log(B)\nB*C\nC/P\nsqrt(I)/P\nL\nL*log(I)\nL*log(C)\nI*C\nI*L/P\nL*B\nC*L/P\nL*I\nsqrt(C)/P\nP\nlog(I)/P\nI\nIN\nlog(B)*log(C)\nlog(I)*log(C)\nPM0.000.050.10Normalized Weights\nFigure 5: Features weights (Op-Subgraph model)\nnumber of partitions (i.e. machine). Given this set of can-\ndidate features, we use a variant of elastic net [ 53] model\nto select a subset of useful features that have at least one\nnon-zero weight over all subgraph models.\nTable 2 and Table 3 depict the selected basic and derived\nfeatures with non-zero weights. We group the derived fea-\ntures into three categories (i) input or output data, capturing\nthe amount of data read, or written, (ii) the product of in-\nput and output, covering the data processing and network\ncommunication aspects, and finally (iii) per-machine input\nor output, capturing the partition size.\nFurther, we analyze the influence of each feature. While\nthe influence of each feature varies over different subgraph\nmodels, Figure 5 shows the aggregated influence over all\nsubgraph models of each feature. Given a total of Knon zero\nfeatures and Nsubgraph models, with winas the weight of\nfeature iin model n, we measure the influence of feature i\nusing normalized weight nwi, asnwi=Í\nNabs(win)Í\nKÍ\nNabs(wkn).\n3.4 Choice of learning model\nFor learning costs over operator-subgraphs, we considered a\nnumber of variants of linear-regression, SVM, decision tree\nand their ensembles, as well as neural network models. On\n5-fold cross-validation over our production workload, the\nfollowing models give more accurate results compared to the\ndefault cost model: (i) Neural network. 3-layers, hidden layer\nsize = 30, solver = adam, activation = relu, l2 regularization =\n0.005, (ii) Decision tree: depth =15, (ii) Random forest number\nof trees = 20, depth = 5, (iii) FastTree Regression (a variant\nof Gradient Boosting Tree): number of trees = 20, depth = 5,\nand (iv) Elastic net: α=1.0, fit intercept=True, l1 ratio=0.5.\n\nWe observe that the strict structure of subgraph template\nhelps reduce the complexity, making the simpler models,\ne.g., linear- and decision tree-based regression models, per-\nform reasonably well with the chosen set of features. A large\nnumber of operator-subgraph templates have fewer train-\ning samples, e.g., more than half of the subgraph instances\nhave <30training samples for the workload described in\nSection 2. In addition, because of the variance in cloud en-\nvironments (e.g., workload fluctuations, machine failures,\netc.), training samples can have noise both in their features\n(e.g., inaccurate statistics estimates) and the class labels (i.e.,\nexecution times of past queries). Together, both these fac-\ntors lead to over-fitting, making complex models such as\nneural network as well as ensemble-based models such as\ngradient-boost perform worse.\nElastic net [53], aL1andL2regularized linear regres-\nsion model, on the other hand, is relatively less prone to\noverfitting. In many cases, the number of candidate features\n(ranging between 25to30) is as many as the number of\nsamples, while only a select few features are usually rele-\nvant for a given subgraph. The relevant features further tend\nto differ across subgraph instances. Elastic net helps per-\nform automatic feature selection , by selecting a few relevant\npredictors for each subgraph independently. Thus, we train\nall subgraphs with the same set of features, and let elastic\nnet select the relevant ones. Another advantage of elastic\nnet model is that it is intuitive and easily interpretable, like\nthe default cost models which are also weighted sums of a\nnumber of statistics. This is an important requirement for\neffective debugging and analysis of production jobs.\nTable 4 depicts the Pearson correlation and median error\nof the five machine learning models over the production\nworkload. We see that operator-subgraphs models trained\nusing elastic net can make sufficiently accurate cost predic-\ntions (14% median error), with a high correlation (more than\n0.92) with the actual runtime, a substantial improvement\nover the default cost model (median error of 258% and Pear-\nson correlation of 0.04). In addition, elastic net models are\nfast to invoke during query optimization, and have low stor-\nage footprint that indeed makes it feasible for us to learn\nspecialized models for each possible subgraph.\n4 ROBUSTNESS\nWe now discuss how we learn robust cost models . As defined\nin Section 1, robust cost models cover the entire workload\nwith high accuracy for a substantial time period before re-\nquiring retraining. In contrast to prior work on robust query\nprocessing [ 14,36,52] that either modify the plan during\nquery execution, or execute multiple plans simultaneously,\nwe leverage the massive cloud workloads to learn robust\nmodels offline and integrate them with the optimizer to gen-\nerate one robust plan with minimum runtime overhead. In\nthis section, we first explain the coverage and accuracy trade-\noff for the operator-subgraph model. Then, we discuss the\nother extreme, namely an operator model, and introduce ad-\nditional models to bridge the gap between the two extremes.Finally, we discuss how we combine predictions from indi-\nvidual models to achieve robustness.\n4.1 Accuracy-Coverage Tradeoff\nThe operator-subgraph model presented in Section 3 is highly\nspecialized. As a result, this model is likely to be highly\naccurate. Unfortunately, the operator-subgraph model does\nnot cover subgraphs that are not repeated in the training\ndataset, i.e., it has limited coverage. For example, over 1\nday of Microsoft production workloads, operator-subgraphs\nhave learned models for only 54% of the subgraphs. Note\nthat we create a learned model for a subgraph if it has at\nleast 5occurrences over the single day worth of training\ndata. Thus, it is difficult to predict costs for arbitrary query\nplans consisting of subgraphs never seen in training dataset.\nThe other extreme: Operator model. In contrast to the\noperator-subgraph model, we can learn a model for each\nphysical operator, similar to how traditional query optimiz-\ners model the cost. The operator models estimate the execu-\ntion latency of a query by composing the costs of individual\noperators in a hierarchical manner akin to how default cost\nmodels derive query costs. As a result, operator models can\npredict the cost of any query in the workload, including\nthose previously unseen in the training dataset. However,\nsimilar to traditional cost model, operator models also suf-\nfer from poor accuracy since the behavior of an operator\nchanges based on what operations appear below it. Further-\nmore, the estimation errors in the features or statistics at the\nlower level operators of a query plan are propagated to the\nupper level operators that significantly degrades the final\nprediction accuracy. On 5-fold cross-validation over 1day\nof Microsoft production workloads, operator models results\nin42% median error and 0.77Pearson correlation, which\nalthough better than the default cost model (258% median\nerror and 0.04 Pearson correlation), is relatively lower com-\npared to that of operator-subgraph models (14% median error\nand 0.92Pearson correlation). Thus, there is an accuracy and\ncoverage tradeoff when learning cost models, with operator-\nsubgraph and operator models being the two extremes of\nthis tradeoff.\n4.2 Bridging the Gap\nWe now present additional models that fall between the two\nextreme models in terms of the accuracy-coverage trade-off.\nOperator-input model. An improvement to per-operator\nis to learn a model for all jobs that share similar inputs. Sim-\nilar inputs also tend to have similar schema and similar data\ndistribution even as the size of the data changes over time,\nthus operator models learned over similar inputs often gen-\neralize over future job invocations. In particular, we create a\nmodel for each operator and input template combination. An\ninput template is a normalized input where we ignore the\ndates, numbers, and parts of names that change over time for\nthe same recurring input, thereby allowing grouping of jobs\nthat run on the same input schema over different sessions.\nFurther, to partially capture the context, we featurize the\n\nModel Correlation Median Error\nDefault 0.04 258%\nNeural Network 0.89 27%\nDecision Tree 0.91 19 %\nFast-Tree regression 0.90 20%\nRandom Forest 0.89 32%\nElastic net 0.92 14%\nTable 4: Correlation and error w.r.t. actual runtime for\nthe operator-subgraphs\nintermediate subgraph by introducing two additional fea-\ntures: 1) the number of logical operator in the subgraph (CL)\nand 2) the depth of the physical operator in the sub-graph\n(D). This helps in distinguishing subgraph instances that are\nextremely different from each other.\nOperator-subgraphApprox model. While operator-sub-\ngraph models exploit the overlapping and recurring nature\nof big data analytics, there is also a large number (about\n15-20%) of subgraphs that are similar but are not exactly\nthe same. To bridge this gap, we relax the notion of sub-\ngraph similarity, and learn one model for all subgraphs that\nhave the same inputs and the same approximate underlying\nsubgraph. We consider two subgraphs to be approximately\nsame if they have the same physical operator at the root, and\nconsist of the same frequency of each logical operator in the\nunderneath subgraph (ignoring the ordering between opera-\ntors). Thus, there are two relaxations: (1) we use frequency of\nlogical operators instead of physical operators (note that this\nis one of the additional features in Operator-input model) and\n(ii) we ignore the ordering between operators. This relaxed\nsimilarity criteria allows grouping of similar subgraphs with-\nout substantially reducing the coverage. Overall, operator-\nsubgraphApprox model is a hybrid of the operator-subgraph\nand operator-input models: it achieves much higher accuracy\ncompared to operator or operator-input models, and more\ncoverage compared to operator-subgraph model.\nTable 5 depicts the Pearson correlation, the median ac-\ncuracy using 5-fold cross-validation, as well as the cover-\nage of individual cost models using elastic net over pro-\nduction workloads. As we move from more specialized to\nmore generalized models (i.e., operator-subgraph to operator-\nsubgraphApprox to operator-input to operator), we see that\nthe model accuracy decreases while the coverage over the\nworkload increases. Figure 6 shows the feature weights\nfor each of the intermediate models. We see that while the\nweight for specialized models, like the operator-subgraph\nmodel (Figure 5), are concentrated on a few features, the\nweights for more generalized models, like the per-operator\nmodel, are more evenly distributed.\n4.3 The Combined Model\nGiven multiple learned models, with varying accuracy and\ncoverage, the strawman approach is to select a learned model\nin decreasing order of their accuracy over training data, start-\ning from operator-subgraphs to operator-subgraphApprox to\noperator-inputs to operators. However, as discussed earlier,\nthere are subgraph instances where more accurate models\nresult in poor performance over test data due to over-fitting.Model Correlation Median\nErrorCoverage\nDefault 0.04 258% 100%\nOp-Subgraph 0.92 14% 54%\nOp-Subgraph\nApprox0.89 16 % 76%\nOp-Input 0.85 18% 83%\nOperator 0.77 42% 100%\nCombined 0.84 19% 100%\nTable 5: Performance of learned models w.r.t to actual run-\ntimes\nModel Correlation Median Error\nDefault 0.04 258%\nNeural Network 0.79 31%\nDecision Tree 0.73 41 %\nFastTree Regression 0.84 19%\nRandom Forest 0.80 28%\nElastic net 0.68 64%\nTable 6: Correlation and errror w.r.t actual runtimes for the\nCombined Model\nTo illustrate, Figure 7 depicts the heat-map representation\nof the accuracy and coverage of different individual models,\nover more than 42Koperator instances from our production\nworkloads. Each point in the heat-map represents the error\nof the predicted cost with respect to the actual runtime: the\nmore green the point, the less the error. The empty region\nat the top of the chart depicts that the learned model does\nnot cover those subgraph instances. We can see that the pre-\ndictions are mostly accurate for operator-subgraph models,\nwhile Operator models have relatively more error (i.e, less\ngreen) than the operator-subgraph models. Operator-input\nhave more coverage with marginally more error (less green)\ncompared to operator-subgraphs. However, for regions b,\nd, and f, as marked on the left side of the figure, we notice\nthat operator-input performs better (more blue and green)\nthan operator-subgraph. This is because operators in those\nregions have much fewer training samples that results in\nover-fitting. Operator-input, on the other hand, has more\ntraining samples, and thus performs better. Thus, it is diffi-\ncult to decide a rule or threshold that always select the best\nperforming model for a given subgraph instance.\nLearning a meta-ensemble model. We introduce a meta-\nensemble model that uses the predictions from specialized\nmodels as meta features, along with the following extra fea-\ntures: (i) cardinalities (I, B, C), (ii) cardinalities per partition\n(I/P, B/P, C/P), and (iii) number of partitions (P) to output\na more accurate cost. Table 6 depicts the performance of\ndifferent machine learning models that we use as a meta-\nlearner on our production workloads. We see that the Fast-\nTree regression [ 16] results in the most accurate predictions.\nFastTree regression is a variant of the gradient boosted re-\ngression trees [ 18] that uses an efficient implementation of\nthe MART gradient boosting algorithm [ 37]. It builds a se-\nries of regression trees (estimators), with each successive\ntree fitting on the residual of trees that precede it. Using\n5-fold cross-validation, we find that the maximum of only\n20regression trees with mean-squared log error as the loss\nfunction and the sub-sampling rate of 0.9is sufficient for\n\nC\nsqrt(C)\nLog(B)*C\nB*log(C)\nB\nI*C\nI*log(C)\nI/P\nsqrt(I)\nL*log(B)\nB*C\nC/P\nsqrt(I)/P\nL\nL*log(I)\nL*log(C)\nI*C\nI*L/P\nL*B\nC*L/P\nL*I\nsqrt(C)/P\nP\nlog(I)/P\nI\nIN\nlog(I)*log(C)\nlog(B)*log(C)\nPM\nCL\nD0.000.050.10Normalized WeightsOperator-Subgraph-Approx Operator-Input OperatorFigure 6: Feature weights (all other models)\nOp-SubgraphabcdefOp-SubgraphApproxOp-InputOperatorCombinederrorno\tcoverageno\tcoverageno\tcoverage\nFigure 7: Heatmap of errors over 42Koperators from production jobs. Each point in the heatmap depicts the error\nof learned cost with respect to the actual runtime\noptimal performance. As depicted in Figure 7, using FastTree\nRegression as a meta-learner has three key advantages.\nFirst, FastTree regression can effectively characterize the\nspace where each model performs well. The regression trees\nrecursively split the space defined by the predictions from in-\ndividual models and features, creating fine-grained partitions\nsuch that prediction in each partition are highly accurate.\nSecond, FastTree regression performs better for opera-\ntor instances where individual models perform worse. For\nexample, some of the red dots found in outlier regions b,\nd and f of the individual model heat-maps are missing in\nthe combined model. This is because FastTree regression\nmakes use of sub-sampling to build each of the regression\ntrees in the ensemble, making it more resilient to overfitting\nand noise in execution times of prior queries. Similarly, for\nregion a where subgraph predictions are missing, FastTree\nregression creates an extremely large number of fine-grained\nsplits using extra features and operator predictions to give\neven better accuracy compared to Operator models.\nFinally, the combined model is naturally capable of cover-\ning all possible plans, since it uses Operator model as one of\nthe predictors. Further, as depicted in Figure 7, the combined\nmodel is comparable to that of specialized models, and al-\nmost always has better accuracy than Operator model. The\ncombined model is also flexible enough to incorporate addi-\ntional models or features, or replace one model with another.\nHowever, on also including the default cost model, it did not\nresult in any improvement on SCOPE. Next, we discuss how\nwe integrate the learned models within the query optimizer.\n5 OPTIMIZER INTEGRATION\nIn this section, we describe our two-fold integration of Cleo\nwithin SCOPE. First, we discuss our end-to-end feedback\nloop to learn cost models and generate predictions duringoptimization. Then, we discuss how we extend the optimizer\nfor supporting resource-exploration using learned models.\n5.1 Integrating Learned Cost Models\nThe integration of learned models with the query optimizer\ninvolves the following three components.\nInstrumentation and Logging. Big data systems, such as\nSCOPE, are already instrumented to collect logs of query\nplan statistics such as cardinalities, estimated costs, as well\nas runtime traces for analysis and debugging. For uniquely\nidentifying each operator and the subplan, query optimizers\nannotate each operator with a signature [8], a 64-bit hash\nvalue that can be recursively computed in a bottom-up fash-\nion by combining (i) the signatures of children operators,\n(ii) hash of current operator’s name, and (iii) hash of opera-\ntor’s logical properties. We extend the optimizer to compute\nthree more signatures, one for each individual sub-graph\nmode, and extract additional statistics (i.e., features) that\nwere missing for Cleo . Since all signatures can be computed\nsimultaneously in the same recursion, and that there are only\n25−30features (most of which were already extracted), the\nadditional overhead is minimal ( ≤10%), as we describe in\nSection 6. This overhead includes both the logging and the\nmodel lookup that we discuss subsequently.\nTraining and Feedback . Given the logs of past query runs,\nwe learn each of the four individual elastic net-based models\nindependently and in parallel using our SCOPE-based paral-\nlel model trainer. Experimentally, we found that a training\nwindow of two days and a training frequency of every ten\ndays results in acceptable accuracy and coverage (Section 6).\nWe then use the predictions from individual models on the\nnext day queries to learn the combined FastTree regression\nmodel. Since individual models can be trained in parallel, the\ntraining time is not much, e.g., it takes less than 45minutes\n\nIs\trequired\tpartition\tproperty?Partition\tExploration5.\t\tOptimize\t\t\tchildren\tgroupsOptimize\tchildren\tgroups7.\tIs\tpartition\toperator?Partition\t\t\t\t\t\tOptimizationDerive\tpartition\tcount\tfrom\tchildren10.\t\t\t\t\t\tLearned\tCost\tModel\tLookup11.\t\t\t\tCombine\twith\tchildren\tcost\tand\treturnyesno\nyesno2.\t\tSet\tpartition\tcount\tto\trequired12356789Update\tResource\tContext4\n1011(a) Resource-aware planning\nExtractSortExchangeReduceOutput\nStage\t1Stage\t2Partition\tCounts:\t\t\t2\t\t\t\t8\t\t\t\t16\t\t\t…\tOutput\tCosts\t\t\t\t\t\t\t\t\t90\t\t80\t\t70\t\t\t…Partition\tCounts:\t\t\t2\t\t\t\t8\t\t\t\t16\t\t\t…\tOutput\tCosts\t\t\t\t\t\t\t\t\t90\t\t80\t\t70\t\t\t…Reduce\tCosts\t\t\t\t\t\t\t200\t\t70\t\t30\t\t\t…Partition\tCounts:\t\t\t2\t\t\t\t8\t\t\t\t16\t\t\t…\tOutput\tCosts\t\t\t\t\t\t\t\t\t90\t\t80\t\t70\t\t\t…Reduce\tCosts\t\t\t\t\t\t\t\t200\t70\t\t30…Exchange\tCosts\t\t\t\t\t15\t\t\t20\t\t25…Resource\tContext1\n2Partition\tOptimization6\nPartition\tCounts:\t\t\t2\t\t\t\t8\t\t\t\t16\t\t\t…\tSort\tCosts\t\t\t\t\t\t\t\t\t\t\t\t\t\t10\t\t12\t\t\t14\t\t…Partition\tCounts:\t\t\t2\t\t\t\t8\t\t\t\t16\t\t\t…\tSort\tCosts\t\t\t\t\t\t\t\t\t\t\t\t\t\t10\t\t12\t\t\t14\t\t…Extract\tCosts\t\t\t\t\t\t\t\t\t40\t\t45\t\t\t50…345Partition\tExplorationPartition\tDerivationPartition\tOptimizationSET\tPartition\tCount=\t16\nSET\tPartition\tCount\t=\t2Resource\tContext\n(b) Example query plan\n0 20 40\nNumber of operators102104Model Lookups\nExhaustive\nAnalyticalGeom(s=0.5)\nGeom(s=5)(c) Model look-ups for partition\nexploration\nFigure 8: Integrating learned cost models with the query optimizer\nfor training 25Kmodels from over 50,000jobs using a clus-\nter of 200nodes. Once trained, we serialize the models and\nfeedback them to the optimizer. The models can be served\neither from a text file, using an additional compiler flag, or\nusing a web service that is backed by a SQL database.\nLook-up . All models relevant for a cluster are loaded upfront\nby the optimizer, into a hash map with keys as signatures of\nmodels, to avoid expensive lookup calls during optimization.\nWhen loaded simultaneously, all 25Kmodels together take\nabout 600MB of memory, which is within an acceptable\nrange. Finally, for cost estimation, we modify the Optimize\nInput phase of Cascade optimizer to invoke learned models.\nFigure 8a highlights the the key steps that we added in blue.\nEssentially, we replace the calls to the default cost-models\nwith the learned model invocations (Step 10in Figure 8a)\nto predict the exclusive cost of an operator, which is then\ncombined with costs of children operators, similar to how\ndefault cost models combine costs. Moreover, since there is\nan operator model and a combined model for every physi-\ncal operator, Cleo can cover all possible query plans, and\ncan even generate a plan unseen in training data. All the\nfeatures that learned models need are available during query\noptimization. However, we do not reuse the partition count\nderived by the default cost model, rather we try to find a\nmore optimal partition count (steps 3,4,9) as it drives the\nquery latency. We discuss the problem with existing partition\ncount selection and our solution below.\n5.2 Resource-aware Query Planning\nThe degree of parallelism (i.e., the number of machines or\ncontainers allocated for each operator) is a key factor in\ndetermining the runtime of queries in massively parallel\ndatabases [ 46], which implicitly depends on the partition\ncount. This makes partition count as an important feature in\ndetermining the cost of an operator (as noted in Figures 5–6).\nUnfortunately, in existing optimizers, the partition count\nis not explored for all operators, rather partitioning operators\n(e.g., Exchange for stage 2in Figure 8b) set the partition count\nfor the entire stage based on their local statistics [ 49]. The\nabove operators on the same stage simply derive the partitioncount set by the partitioning operator. For example, in stage\n2of Figure 8b, Exchange sets a partition count to 2as it\nresults in its smallest local cost (i.e., 15). The operators above\nExchange (i.e., Reduce and Output) derive the same partition\ncount, resulting in a total cost of 305for the entire stage.\nHowever, we can see that the partition count of 16results\nin a much lower overall cost of 125, though it’s not locally\noptimal for Exchange. Thus, not optimizing the partition count\nfor the entire stage results in a sub-optimal plan .\nTo address this, we explore the partition counts during\nquery planning, by making query planning resource-aware.\nFigures 8a and 8b illustrate our resource-aware query plan-\nning approach. We introduce the notion of a resource-context ,\nwithin the optimizer-context, for tracking costs of partitions\nacross operators in a stage. Furthermore, we add a partition\nexploration step, where each physical operator attaches a list\nof learned costs for different partition counts to the resource\ncontext (step 3 in Figure 8a). For example, in Figure 8b, the\nresource-context for stage 2 shows the learned cost for dif-\nferent partition counts for each of the three operators. On\nreaching the stage boundary, the partitioning operator Ex-\nchange performs partition optimization (step 9in Figure 8a)\nto set its local partition count to 16, that results in the lowest\ntotal cost of 125across for the stage. Thereafter, the higher\nlevel operators simply derive the selected partition count\n(line 8 in Figure 8a), like in the standard query planning, and\nestimate their local costs using learned models. Note that\nwhen a partition comes as required property [ 21] from up-\nstream operators, we set the partition count to the required\nvalue without any exploration (Figure 8a step 2).\nSCOPE currently does not allow varying other resources,\ntherefore we focus only on partition counts in this work.\nHowever, the resource-aware query planning with the three\nnew abstractions to Cascades framework, namely the resource-\ncontext, partition-exploration, and partition-optimization,\nis general enough to incorporate additional resources such\nas memory sizes, number of cores, VM instance types, and\nother infrastructure level decisions to jointly optimize for\nboth plan and resources. Moreover, our proposed extensions\n\nClusterDayTotal JobsRecurring JobsRecurring TemplatesTotal  Sub-Expr.Common Sub-Expr.Recurring Sub-Expr.Ad-hoc Sub-Expr.Cluster1Day164796524001766235460872874485484909186693Day262065500551688832847062633981468643182082Day368479557671814336572392946294493745217200Cluster2Day1529524945267412523173194896248853185680Day2367763316760441595925123827328504372609Day3404643642968672232402162408152278385538Cluster3Day1302772592962731337034103123123866667137Day2291202500259171292181101784921115563177Day3296672530662811326588102361123566867309Cluster4Day1145621314522775300454169726149251581Day2160001413226965199144083975381957698Day3186411704026065362944202824943466578Overall46379939782498395223815881758441835938881203282\n\u00001Figure 9: Workload consisting of 0.5million jobs from\n4different production clusters over 3days\ncan also be applied to other big data systems such as Spark,\nFlink, and Calcite that use variants of Cascades optimizers\nand follow a similar top-down query optimization as SCOPE.\nOur experiments in Section 6 show that the resource-\naware query planning not only generates better plans in\nterms of latency, but also leads to resource savings. However,\nthe challenge is that estimating the cost for every single par-\ntition count for each operator in the query plan can explode\nthe search space and make query planning infeasible. We\ndiscuss our approach to address this next.\n5.3 Efficient Resource Exploration\nWe now discuss two techniques for efficiently exploring the\npartition counts in Cleo , without exploding the search space.\nSampling-based approach. Instead of considering every\nsingle partition count, one option is to consider a uniform\nsample over the set of all possible containers for the tenant\nor the cluster. However, the relative change in partition is\nmore interesting when considering its influence on the cost,\ne.g., a change from 1to2partitions influences the cost more\nthan a change from 1200 to1210 partitions. Thus, we sam-\nple partition counts in a geometrically increasing sequence,\nwhere a sample xi+1is derived from previous sample xius-\ning:xi+1=⌈xi+xi/s⌉with x0=1,x1=2. Here, sis a\nskipping coefficient that decides the gap between successive\nsamples. A large sleads to a large number of samples and\nmore accurate predictions, but at the cost of higher model\nlook-ups and prediction time.\nAnalytical approach. We reuse the individual learned mod-\nels to directly model the relationship between the partition\ncount and the cost of an operator. The key insight here is that\nonly features where partition count is present are relevant\nfor partition exploration, while the rest of the features can\nbe considered constants since their values are fixed during\npartition exploration. Thus, we can express operator cost as\nfollows: cost∝(θ1∗I+θ2∗C+θ3∗I∗C)\nP+θc∗Pwhere I,C, and P\nrefer to input cardinality, cardinality and partition count re-\nspectively. During optimization, we know I,C,I∗C, therefore:\ncost∝θP\nP+θc∗P. Extending the above relationship acrossall operators (say nin number) in a stage, the relationship\ncan be modeled as: cost∝Ín\ni=1θPi\nP+Ín\ni=1θCi∗P\nThus, during partition exploration, each operator calcu-\nlatesθPandθCand adds them to resource-context, and the\npartitioning operator selects the optimal partition count by\noptimizing the above function. There are three possible sce-\nnarios: (i)Ín\ni=1θPiis positive whileÍn\ni=1θCiis negative: we\ncan have the maximum number of partitions for the stage\nsince there is no overhead of increasing the number of par-\ntitions, (ii)Ín\ni=1θPiis negative whileÍn\ni=1θCiis positive:\nwe set the partition count to minimum as increasing the\npartition count increases the cost, (iii)Ín\ni=1θPiandÍn\ni=1θCi\nare either both positive or both negative: we can derive the\noptimal partition count by differentiating the cost equation\nwith respect to P. Overall, for mphysical operator, and the\nmaximum possible partition count of Pmax, the analytical\nmodel makes 5·m·loдs+1\nsPmaxcost model look-ups. Figure 8c\nshows the number of model look-ups for sampling and an-\nalytical approaches as we increase the number of physical\noperators from 1to40in a plan. While the analytical model\nincurs a maximum of 200look-ups, the sampling approach\ncan incur several thousands depending on the skipping co-\nefficients. In section 6.5, we further analyze the accuracy\nof the sampling strategy with the analytical model on the\nproduction workload as we vary the sample size. Our results\nshow that the analytical model is at least 20×more efficient\nthan the sampling approach for achieving the same accuracy.\nThus, we use the analytical model as our default partition\nexploration strategy.\n6 EXPERIMENTS\nIn this section, we present an evaluation of our learned op-\ntimizer Cleo . For fairness, we feed the same statistics (e.g.,\ncardinality, average row length) to learned models that are\nused by the SCOPE default cost model. Our goals are five fold:\n(i) to compare the prediction accuracy of our learned cost\nmodels over all jobs as well as over only ad-hoc jobs across\nmultiple clusters, (ii) to test the coverage and accuracy of\nlearned cost models over varying test windows, (iii) to com-\npare the Cleo cost estimates with those from CardLearner,\n(iv) to explore why perfect cardinality estimates are not suffi-\ncient for query optimization, (iv) to evaluate the effectiveness\nof sampling strategies and the analytical approach proposed\nin Section 5.2 in finding the optimal resource (i.e., partition\ncount), and (v) to analyze the performance of plans produced\nbyCleo with those generated from the default optimizer\ninCleo using both the production workloads and the TPC-\nH benchmark (v) to understand the training and runtime\noverheads when using learned cost models.\nWorkload. As summarized in Figure 9, we consider a large\nworkload trace from 4different production clusters compris-\ning of 423virtual clusters, with each virtual cluster roughly\nrepresenting a business unit within Microsoft, and consist-\ning a total of≈0.5million jobs over 3days, that ran with a\ntotal processing time of ≈6million hours, and use a total of\n\nPercentage Change-6.0-3.00.03.06.09.012.0\nDay1-to-Day2Day2-Day3Total JobsRecurring JobsRecurring Templates\nPercentage Change-40.0-30.0-20.0-10.00.010.020.0\nDay1-to-Day2Day2-to-Day3Total JobsRecurring JobsRecurring Templates\nPercentage Change-7.0-4.7-2.30.02.34.77.0\nDay1-to-Day2Day2-to-Day3Total JobsRecurring JobsRecurring Templates\nPercentage Change-6.00.06.012.018.024.030.0\nDay1-to-Day2Day2-to-Day3Total JobsRecurring JobsRecurring Templates\n\u00001Figure 10: Illustrating workload changes over different clusters and different days.\n103\n102\n101\n100101102103\nEstimated/Actual0.000.250.500.751.00CDF of Operators\nOperator-Subgraph\n103\n102\n101\n100101102103\nEstimated/Actual\nOperator-Input\n103\n102\n101\n100101102103\nEstimated/Actual\nOperator\n103\n102\n101\n100101102103\nEstimated/Actual\nCombinedNeural Network (MLP) Decision Tree Regressor Random Forest FastTreeRegression Elastic Net Default\nFigure 11: Cross-validation results of ML algorithms for each learned model on Cluster 4 workload\nAll jobs Ad-hoc jobs\nCorrelation Median Error 95%tile Error Coverage Correlation Median Error 95%tile Error Coverage\nDefault 0.12 182% 12512% 100% 0.09 204% 17791% 100%\nOp-Subgraph 0.86 9% 56% 65% 0.81 14% 57% 36%\nOp-Subgraph Approx 0.85 12 % 71% 82% 0.80 16 % 79% 64%\nOp-Input 0.81 23% 90% 91% 0.77 26% 103% 79%\nOperator 0.76 33% 138% 100% 0.73 42% 186% 100%\nCombined 0.79 21% 112% 100% 0.73 29% 134% 100%\nTable 7: Breakdown of accuracy and coverage of each learned model for all jobs and ad-hoc jobs separately on Cluster1.\nCluster Default (all jobs) Learned (all jobs) Learned (ad-hoc jobs)\nCorrelation Median\nAccuracyCorrelation Median\nAccuracyCorrelation Median\nAccuracy\nCluster 1 0.12 182% 0.79 21% 0.73 29%\nCluster 2 0.08 256% 0.77 33% 0.75 40%\nCluster 3 0.15 165% 0.83 26% 0.81 38%\nCluster 4 0.05 153% 0.74 15% 0.72 26%\nTable 8: Pearsion Correlation and Median accuracy of de-\nfault and combined learned model over all jobs and ad-hoc\njobs on each cluster.\n≈1.4billion containers. The workload exhibits variations\nin terms of the load (e.g., more than 3×jobs on Cluster1\ncompared to Cluster4), as well as in terms of job properties\nsuch as average number of operators per job ( 50sin Cluster1\ncompared to 30sin Cluster4) and the average total processing\ntime of all containers per job (around 17hours in Cluster1\ncompared to around 5hours in Cluster4). The workload also\nvaries across days from a 30%decrease to a 20%increase of\ndifferent job characteristics on different clusters(Figure 10).\nFinally, the workload consists of a mix of both recurring and\nad-hoc jobs, with about 7%−20%ad-hoc jobs on different\nclusters and different days.\n6.1 Cross Validation of ML Models\nWe first compare default cost model with the five machine\nlearning algorithms discussed in Section 3.4. (i) Elastic net,\na regularized linear-regression model, (ii) DecisionTree Re-\ngressor, (iii) Random Forest, (iv) Gradient Boosting Tree, and\n(v) Multilayer Perceptron Regressor (MLP). Figure 11 (a tod) depicts the 5-fold cross-validation results for the ML al-\ngorithms for operator-subgraph, operator-input, operator,\nand combined models respectively. We skip the results for\noperator-subgraphApprox as it has similar results to that of\noperator-input. We observe that all algorithms result in bet-\nter accuracy compared to the default cost model for each of\nthe models. For operator-subgraph and operator-input mod-\nels, the performance of most of the algorithms (except the\nneural network) is highly accurate. This is because there are\na large number of specialized models, highly optimized for\nspecific sub-graph and input instances respectively. The accu-\nracy degrades as the heterogeneity of model increases from\noperator-subgraph to operator-input to operator. For individ-\nual models, the performances of elastic-net and decision-tree\nare similar or better than complex models such as neural\nnetwork and ensemble models. This is because the complex\nmodels are more prone to overfitting and noise as discussed\nin Section 3.4. For the combined model, the FastTree Regres-\nsion does better than other models because of its ability to\nbetter characterize the space where each model performs\nwell. The operator-subgraph and operator-input models have\nthe highest accuracy (at between 45% to 85% coverage), fol-\nlowed by the combined model (at 100% coverage) and then\nthe operator model (at 100% coverage).\n6.2 Accuracy\nNext, we first compare the accuracy and correlation of learned\nmodels to that of the default cost model for each of the clus-\nters. We use the elastic net model for individual learned\n\n103\n101\n101103\nEstimated/Actual0.000.250.500.751.00CDF of Operators\nCluster 1\n103\n101\n101103\nEstimated/Actual\nCluster 2\n103\n101\n101103\nEstimated/Actual\nCluster 3\n103\n101\n101103\nEstimated/Actual\nCluster 4Operator-SubgraphApprox Operator-Subgraph Operator-Input Operator Combined DefaultFigure 12: Accuracy results on all jobs (recurring + ad-hoc) over four different clusters\n103\n101\n101103\nEstimated/Actual0.000.250.500.751.00CDF of Operators\nCluster 1\n103\n101\n101103\nEstimated/Actual\nCluster 2\n103\n101\n101103\nEstimated/Actual\nCluster 3\n103\n101\n101103\nEstimated/Actual\nCluster 4Operator-SubgraphApprox Operator-Subgraph Operator-Input Operator Combined Default\nFigure 13: Accuracy results on only ad-hoc jobs over four different clusters.\nmodel and the Fast-Tree regression for the combined model.\nWe learn the models on day 1 and day 2, and predict on day\n3 of the workload as described in Section 5.1. Table 8 shows\nthe Pearson correlation and median accuracy of default cost\nmodel and the combined model for all jobs and only ad-hoc\njobs separately for each of the clusters on day 3. We further\nshow the breakdown of results for each of the individual\nmodels on cluster 1 jobs in Table 7. Figure 12 and Figure 13\nshow the CDF distribution for estimated vs actual ratio of\npredicted costs for each of the learned models over different\nclusters.\nAll jobs. We observe that learned models result between 8×\nto10×better accuracy and between 6×to14×better correla-\ntion compared to the default cost model across the 4clusters.\nFor operator-subgraph, the performance is highly accurate\n(9% median error and .86 correlation), but at lower coverage\nof 65%. This is because there are a large number of specialized\nmodels, highly optimized for specific sub-graph instances.\nAs the coverage increases from operator-subgraphApprox\nto operator-input to operator, the accuracy decreases. Over-\nall, the combined model is able to provide the best of both\nworlds, i.e., accuracy close to those of individual models and\n100% coverage like that of the operator model. The 50thand\nthe95thpercentile errors of the combined model are about\n10×and 1000×better than the default SCOPE cost model.\nThese results show that it is possible to learn highly accurate\ncost models from the query workload.\nOnly ad-hoc Jobs. Interestingly, the accuracy over ad-hoc\njobs drop slightly but it is still close to those over all jobs (Table 8\nand Table 7) . This is because: (i) ad-hoc jobs can still have\none or more similar subexpressions as other jobs (e.g., they\nmight be scanning and filtering the same input before doing\ncompletely new aggregates), which helps them to leveragethe subgraph models learned from other jobs. This can be\nseen in Table 7 — the sub-graph learned models still have\nsubstantial coverage of subexpression on ad-hoc jobs. For\nexample, the coverage of 64%for Op-Subgraph Approx model\nmeans that 64%of the sub-expressions on ad-hoc jobs had\nmatching Op-Subgraph Approx learned model. (ii) Both the\noperator and the combined models are learned on a per-\noperator basis, and have much lower error ( 42% and 29%)\nthan the default model ( 182% ). This is because the query\nprocessor, the operator implementations, etc. still remain the\nsame, and their behavior gets captured in the learned cost\nmodels. Thus, even if there is no matching sub-expression\nin ad-hoc jobs, the operator and the combined models still\nresult in better prediction accuracy.\n6.3 Robustness\nWe now look at the robustness (as defined in Section 1) of\nlearned models in terms of accuracy, coverage, and retention\nover a month long test window on cluster 1 workloads.\nCoverage over varying test window. Figure 14a depicts\nthe coverage of different subgraph models as we vary the\ntest window over a duration of 1 month. The coverage of\nper-operator and combined model is always 100% since there\nis one model for every physical operator. The coverage of\nper-subgraph models, strictest among all, is about 58%after\n2days, and decreases to 37%after 28days. Similarly, the cov-\nerage of per-subgraphApprox ranges between 75%and 60%.\nThe per-operator-input model, on the other hand remains\nstable between 78%and 84%.\nError and correlation over varying test window. Fig-\nures 14b and 14c depict the median and 95%ile error per-\ncentages respectively over a duration of one month. While\nthe median error percentage of learned models improves\n\n2 7 14 21 28\ndays0255075100Coverage (%)\na) CoverageDefault Op-Subgraph Op-Input Op-SubgraphApprox Op Combined\n2 7 14 21 28\ndays0204060Median Error %\n200\nb) Median Error\n2d 7d 14d 21d 28d\ndays0200400600800100095%tile Error %\n90000\nc) 95%tile Error\n2 7 14 21 28\ndays0.000.250.500.751.00Correlation\nd) Pearson CorrelationFigure 14: Coverage and accuracy with respect to default optimizer over 1 month\non default cost model predictions by 3-15x, the 95%ile er-\nror percentage is better by over three orders of magnitude.\nFor specialized learned models, the error increases slowly\nover the first two weeks and then grows much faster due to\ndecrease in the coverage. Moreover, the 95%ile error of the\nsubgraph models grows worse than their median error. Simi-\nlarly, in Figure 14d, we see that predicted costs from learned\nmodels are much more correlated with the actual runtimes,\nhaving high Pearson correlation (generally between 0.70and\n0.96), compared to default cost models that have a very small\ncorrelation (around 0.1). A high correlation over a duration\nmonth of 1month shows that learned models can better dis-\ncriminate between two candidate physical operators. Overall,\nbased on these results, we believe re-training every 10days\nshould be acceptable, with a median error of about 20%,95%\nerror of about 200% , and Pearson correlation of around 0.80.\nRobustness of the combined model . From Figure 14, we\nnote that the combined model (i) has 100% coverage, (ii)\nmatches the accuracy of best performing individual model\nat any time (visually illustrated in Figure 7), while having a\nhigh correlation ( >0.80) with actual runtimes, and (iii) gives\nrelatively stable performance with graceful degradation in\naccuracy over longer run. Together, these results show that\nthe combined model in Cleo is indeed robust.\n6.4 Impact of Cardinality\nComparison with Cardlearner. Next, we compare the ac-\ncuracy and the Pearson correlation of Cleo and the default\ncost model with CardLearner [ 47], a learning-based cardinal-\nity estimation that employs a Poisson regression model to\nimprove the cardinality estimates, but uses the default cost\nmodel to predict the cost. For comparison, we considered\njobs from a single virtual cluster from cluster 4, consisting of\nabout 900jobs. While Cleo and the default cost model use the\ncardinality estimates from the SCOPE optimizer, we addition-\nally consider a variant ( Cleo +CardLearner) where Cleo uses\nthe cardinality estimates from CardLearner. Overall, we ob-\nserve that the median error of default cost with CardLearner\n(211%) is better than that of default cost model alone (236%)\nbut much still worse than that of Cleo ’s (18%) and Cleo +\nCardLearner (13%). Figure 15 depicts the CDF of estimated\nand actual costs ratio, where we can see that by learning costs,\nCleo significantly reduces both the under-estimation as well\nas over-estimation, while CardLearner only marginally im-\nproves the accuracy for both default cost model and Cleo .\nSimilarly, the Pearson correlation of CardLearner’s estimates\n(.01) is much worse than that of Cleo ’s (0.84) and Cleo +CardLearner ( 0.86). These results are consistent with our\nfindings from Section 2 where we show that fixing the car-\ndinality estimates is not sufficient for filling the wide gap\nbetween the estimated and the actual costs in SCOPE-like\nsystems. Interestingly, we also observe that the Pearson cor-\nrelation of CardLearner is marginally less than that of the\ndefault cost model (0.04) inspite of better accuracy, which can\nhappen when a model makes both over and under estimation\nover different instances of the same operator.\nWhy cardinality alone is not sufficient? To understand\nwhy fixing cardinality alone is not enough, we performed\nthe following two micro-experiments on a subset of jobs,\nconsisting of about 200K subexpressions from cluster 4.\n1. The need for a large number of features. First, start-\ning with perfect cardinality estimates (both input and output)\nas only features, we fitted an elastic net model to find / tune\nthe best weights that lead to minimum cost estimation error\n(using the loss function described in Section 3.4). Then, we\nincrementally added more and more features and also com-\nbined feature with previously selected features, retraining\nthe model with every addition. Figure 18 shows the decrease\nin cost model error as we cumulatively add features from left\nto right. We use the same notations for features as defined\nin Table 2 and Table 3.\nWe make the following observations.\n(1)When using only perfect cardinalities, the median error\nis about 110% . However, when adding more features, we\nobserve that the error drops by more than half to about\n40%. This is because of the more complex environments and\nqueries in big data processing that are very hard to cost with\njust the cardinalities (as also discussed in Section 2.3).\n(2)Deriving additional statistics by transforming (e.g., sqrt,\nlog, squares) input and output cardinalities along with other\nfeatures helps in reducing the error. However, it is hard to\narrive at these transformations in hand coded cost models.\n(3)We also see that features such as parameter (PM), input\n(IN), and partitions (P) are quite important, leading to sharp\ndrops in error. While partition count is good indicator of\ndegree of parallelism (DOP) and hence the runtime of job;\nunfortunately query optimizers typically use a fixed partition\ncount for estimating cost as discussed in Section 5.2.\n(4)Finally, there is a pervasive use of unstructured data\n(with schema imposed at runtime) as well as custom user\ncode (e.g., UDFs) that embed arbitrary application logic in\nbig data applications. It is hard to come up with generic\nheuristics, using just the cardinality, that effectively model\nthe runtime behavior of all possible inputs and UDFs.\n\n103\n101\n101103\nEstimated/Actual0.00.51.0CDF of Operators\nCLEO\nDefaultCLEO+CardLearner\nDefault+CardLearnerFigure 15: Comparison with CardLearner\nC\nI\nL\nsqrt(C)\nPM\nP\nL*I\nC/P\nI/P\nL*B0.00.10.2Normalized \n WeightsSet 1 Set 2Figure 16: Hash join operator having\ndifferent weights over different sets\nof sub-expressions.\n21232527\nNumber of Samples406080Median Error (%)\nGeometric\nUniformRandom\nAnalyticalFigure 17: Partition Explo-\nration Accuracy vs. Efficiency\nC\nI\nL\nsqrt(C)\nP\nL*I\nIN\nPM\nC/P\nI/P\nL*B\nI*C\nB*C\nI*log(C)\nB/P\nsqrt(I)\nL*log(I)\nI*Sqrt(C)\nsqrt(I)/P\nL*log(B)\nL*log(C)\nlog(I)*C\nI*L/P\nC*L/P\nL*Sqrt(B)\nB*log(C)\nlog(I)/P\nLog(B)*C\nlog(I)*log(C)020406080100120Median Error\nFigure 18: Impact on median error as we cumulatively add\nfeatures from left to right. The first two features are perfect\noutput (C) and input (I) cardinalities.\n2. Varying optimal weights. We now compare the optimal\nweights of features of physical operators when they occur in\ndifferent kinds of sub-expressions. We consider the popular\nhash join operator and identified two sets of sub-expressions\nin the same sample dataset: (i) hash join appears on top of\ntwo scan operators, and (ii) hash join appears on top of two\nother join operators, which in turn read from two scans.\nFigure 16 shows the weights of the top 10features of\nthe hash join cost model for the two sets. We see that the\npartition count is more influential for set 2 compared to set 1.\nThis is because there is more network transfer of data in set 2\nthan in set 1 because of two extra joins. Setting the partition\ncount that leads to minimum network transfer is therefore\nimportant. On the other hand, for jobs in set 1, we notice that\nhash join typically sets the partition count to the same value\nas that of inputs, since that minimizes repartitioning. Thus,\npartition count is less important in set 1. To summarize, even\nwhen cardinality is present as a raw or derived feature, its\nrelative importance is instance specific (heavily influenced\nby the partitioning and the number of machines) and hard\nto capture in a static cost model.\n6.5 Efficacy of Partition Exploration\nIn this section, we explore the effectiveness of partition ex-\nploration strategies proposed in Section 5.2 in selecting the\npartition count that leads to lowest cost for learned models.\nWe used a subset of 200sub-expression instances from the\nproduction workload from cluster 1, and exhaustively probe\nthe learned models for all partition counts from 0to3000\n(the maximum capacity of machines on a virtual cluster) to\nfind the most optimal cost. Figure 17 depicts the median er-\nror in cost accuracy with respect to the optimal cost for (i)\nthree sampling strategies: random, uniform, and geometric\nas we vary the number of samples of partition counts (ii)\nthe analytical model (dotted blue line) that selects a single\npartition count.We note that the analytical model, although approximate,\ngives more accurate results compared to sampling-based ap-\nproaches until the sample size of about 15to20, thereby re-\nquiring much fewer model invocations. Further, for a sample\nsize between 4to20, we see that the geometrically increasing\nsampling strategy leads to more accurate results compared\nto uniform and random approaches. This is because it picks\nmore samples when the values are smaller, where costs tend\nto change more strongly compared to higher values. During\nquery optimization, each sample leads to five learned cost\nmodel predictions, four for individual models and one for the\ncombined model. Thus, for a typical plan in big data system\nthat consists of 10operators, the sampling approach requires\n20∗5∗10=1000 model invocations, while the analytical\napproach requires only 5∗10=50invocations for achieving\nthe same accuracy. This shows that the analytical approach is\npractically more effective when we consider both efficiency\nand accuracy together. Hence, Cleo uses the analytical ap-\nproach as the default partition exploration strategy.\n6.6 Performance\nWe split the performance evaluation of Cleo into three parts:\n(i) the runtime performance over production workloads, (ii) a\ncase-study on the TPC-H benchmark [ 38], explaining in de-\ntail the plan changes caused by the learned cost models,\nand (iii) the overheads incurred due to the learned models.\nFor these evaluations, we deployed a new version of the\nSCOPE runtime with Cleo optimizer (i.e., SCOPE+ Cleo ) on\nthe production clusters and compared the performance of\nthis runtime with the default production SCOPE runtime.\nWe used the analytical approach for partition exploration.\n6.6.1 Production workload. Since production resources\nare expensive, we selected a subset of the jobs, similar to prior\nwork on big data systems [ 47], as follows. We first recompiled\nall the jobs from a single virtual cluster from cluster 4 with\nCleo and found that 182(22%) out of 845jobs had plan\nchanges when partition exploration was turned off, while\n322(39%) had plan changes when we also applied partition\nexploration. For execution, we selected jobs that had at least\none change in physical operator implementations, e.g, for\naggregation (hash vs stream grouping), join (hash vs merge),\naddition or removal of grouping, sort or exchange operators.\nWe picked 17such jobs and executed them with and without\nCleo over the same production data, while redirecting the\noutput to a dummy location, similar to prior work [1].\nFigure 19a shows the end-to-end latency for each of the\njobs, compared to their latency when using the default cost\n\n1234567891011121314151617050100150200Latency (Mins)SCOPE(a) Changes in Latency\n12345678910111213141516170100200300400Total Process. Hrs (hrs)SCOPE + CLEO (b) Changes in Total Processing Time\n123456789101112131415161720\n10\n01020Overhead (%)Optimization overhead(c) Optimization Time Overhead\nFigure 19: Performance comparison on production jobs with changed plans\nmodel. We see that the learned cost models improve latency\nin 70% (12 jobs) cases, while they degrade latency in the re-\nmaining 30% cases. Overall, the average improvement across\nall jobs is 15.35%, while the cumulative latency of all jobs\nimproves by 21.3%. Interestingly, Cleo was able to improve\nthe end-to-end latency for 10out of 12jobs with less degree\nof parallelism (i.e., the number of partitions). This is in con-\ntrary to the typical strategy of scaling out the processing\nby increasing the degree of parallelism, which does not al-\nways help. Instead, resource-awareness plan selection can\nreveal more optimal combinations of plan and resources. To\nunderstand the impact on resource consumption, Figure 19b\nshows the total processing time (CPU hours). Learned cost\nmodels reduce the Overall, we see the total processing time\nreducing by 32.2% on average and 40.4%cumulatively across\nall 17 jobs— a significant operational cost saving in big data\ncomputing environments .\nThus, the learned cost models could reduce the overall\ncosts while still improving latencies in most cases. Below, we\ndig deeper into the plans changes using the TPC-H workload.\n6.6.2 TPC-H workload. We generated TPC-H dataset with\na scale factor of 1000 , i.e., total input of 1T B. We ran all 22\nqueries 10times, each time with randomly chosen different\nparameters, to generate the training dataset. We then trained\nour cost models on this workload and feedback the learned\nmodels to re-run all 22queries. We compare the performance\nwith- and without the feedback as depicted in Figure 20 . For\neach observation, we take the average of 3runs. Overall,\n6TPC-H queries had plan changes when using resource-\naware cost model. Out of these, 4changes (Q8, Q9, Q16, Q20)\nimprove latency as well as total processing time, 1 change\nimproves only the latency (Q11), and 1 change (Q17) leads\nto performance regression in both. We next discuss the key\nchanges observed in the plans.\n1. More optimal partitioning. In Q8, for Part (100 partitions)\nZpartKeyLineitem (200 partitions) and in Q9 for Part (100\npartitions) ZpartKey(Lineitem ZSupplier ) (250 partitions),\nthe default optimizer performs the merge join over 250 par-\ntitions, thereby re-partitioning both Part and the other join\ninput into 250partitions over the Partkey column. Learned\ncost models, on other hand, perform the merge join over 100\npartitions, which requires re-partitioning only the other join\ninputs and not the Part table. Furthermore, re-partitioning\n200or250partitions into 100partitions is cheaper, since it in-\nvolves partial merge [ 10], compared to re-partitioning them\ninto 250partitions. In Q16, for the final aggregation and top-kselection, both the learned and the default optimizer reparti-\ntion the 250partitions from the previous operator’s output\non the final aggregation key. While the default optimizer\nre-partitions into 128partitions, the learned cost models pick\n100partitions. The aggregation cost has almost negligible\nchange due to change in partition counts. However, reparti-\ntioning from 250to100turns out to be substantially faster\nthan re-partitioning from 250to128partitions.\n2. Skipping exchange (shuffe) operators. In Q8, for Part (100\npartitions) ZpartKeyLineitem (200 partitions), the learned\ncost model performs join over 100partitions and thus it\nskips the costly Exchange operator over the Part table. On\nthe other hand, the default optimizer creates two Exchange\noperator for partitioning each input into 250partitions.\n3. More optimal physical operator: For both Q8 and Q20,\nthe learned cost model performs join between Nations and\nSupplier using merge join instead of hash join (chosen by\ndefault optimizer). This results in an improvement of 10%\nto15%in the end-to-end latency, and 5%to8%in the total\nprocessing time (cpu hours).\n4. Regression due to partial aggregation. For Q17, the learned\ncost models add local aggregation before the global one to\nreduce data transfer. However, this degrades the latency by\n10%and total processing time by 25%as it does not help in\nreducing data. Currently, learned models do not learn from\ntheir own execution traces. We believe that doing so can\npotentially resolve some of the regressions.\n6.6.3 Training and Runtime Overheads. We now describe\nthe training and run-time overheads of Cleo . It takes less\nthan 1 hour to analyze and learn models for a cluster run-\nning about 800jobs a day, and less than 4 hours for training\nover 50K jobs instances at Microsoft. We use a parallel model\ntrainer that leverages SCOPE to train and validate models\nindependently and in parallel, which significantly speeds up\nthe training process. For a single cluster of about 800jobs,\nCleo learns about 23K models which when simultaneously\nloaded takes about 600MB of memory. About 80% of the\nmemory is taken by the individual models while the remain-\ning is used by the combined model. The additional memory\nusage is not an issue for big data environments, where an\noptimizer can typically have 100s of GBs of memory. Finally,\nwe saw between 5-10% increase in the optimization time for\nmost of the jobs when using learned cost models, which in-\nvolves the overhead of signature computation of sub-graph\nmodels, invoking learned models, as well as any changes in\nplan exploration strategy due to learned models. Figure 19c\n\nQ8 Q9 Q11 Q16 Q17 Q2030\n20\n10\n0102030% ImprovementLatency Total Processing TimeFigure 20: Performance change with SCOPE + CLEO\nfor TPC-H queries (higher is better)\ndepicts the overhead in the optimization time for each of\nthe17production jobs we executed. Since the optimization\ntime is often in orders of few hundreds of milliseconds, the\noverhead incurred here is negligible compared to the over-\nall compilation time (orders of seconds) of jobs in big data\nsystems as well as the potential gains we can achieve in the\nend-to-end latency (orders of 10s of minutes).\n6.7 Discussion\nWe see that it is possible to learn accurate yet robust cost\nmodels from cloud workloads. Given the complexities and\nvariance in the modern cloud environments, the model ac-\ncuracies in Cleo are not perfect. Yet, they offer two to three\norders of magnitude more accuracy and improvement in\ncorrelation from less than 0.1to greater than 0.7over the\ncurrent state-of-the-art. The combined meta model further\nhelps to achieve full workload coverage without sacrificing\naccuracy significantly. In fact, the combined model consis-\ntently retains the accuracy over a longer duration of time.\nWhile the accuracy and robustness gains from Cleo are ob-\nvious, the latency implications are more nuanced. These are\noften due to various plan explorations made to work with\nthe current cost models. For instance, SCOPE jobs tend to\nover-partition at the leaf levels and leverage the massive\nscale-out possible for improving latency of jobs.\nThere are several ways to address performance regres-\nsions in production workloads. One option is to revisit the\nsearch and pruning strategies for plan exploration [ 21] in\nthe light of newer learned cost models. For instance, one\nproblem we see is that current pruning strategies may some-\ntimes skip operators without invoking their learned models.\nAdditionally, we can also configure optimizer to not invoke\nlearned models for specific operators or jobs. Another im-\nprovement is to optimize a query twice (each taking only few\norders of milliseconds), with and without Cleo , and select\nthe plan with the better overall latency, as predicted using\nlearned models since they are highly accurate and correlated\nto the runtime latency. We can also monitor the performance\nof jobs in pre-production environment and isolate models\nthat lead to performance regression (or poor latency predic-\ntions), and discard them from the feedback. This is possible\nsince we do not learn a single global model in the first place.\nFurthermore, since learning cost models and feeding them\nback is a continuous process, regression causing models can\nself-correct by learning from future executions. Finally, re-\ngressions for a few queries is not really a problem for ad-hocworkloads, since majority of the queries improve their la-\ntency anyways and reducing the overall processing time (and\nhence operational costs) is generally more important.\nFinally, in this paper, we focused on the traditional use-\ncase of a cost model for picking the physical query plan.\nHowever, several other cost model use-cases are relevant in\ncloud environments, where accuracy of predicted costs is\ncrucial. Examples include performance prediction [ 39], allo-\ncating resources to queries [ 25], estimating task runtimes for\nscheduling [ 6], estimating the progress of a query especially\nin server-less query processors [ 29], and running what-if\nanalysis for physical design selection [ 12]. Exploring these\nwould be a part of future work. Examples include perfor-\nmance prediction [ 39], allocating resources to queries [ 25],\nestimating task runtimes for scheduling [ 6], estimating the\nprogress of a query especially in server-less query proces-\nsors [ 29], and running what-if analysis for physical design\nselection [ 12]. Exploring these would be a part of future\nwork.\n7 RELATED WORK\nMachine learning has been used for estimating the query exe-\ncution time of a given physical plan in centralized databases [ 2,\n19,32]. In particular, the operator and sub-plan-level mod-\nels in [ 2] share similarities with our operator and operator-\nsubgraph model. However, we discovered the coverage-accuracy\ngap between the two models to be substantially large. To\nbridge this gap, we proposed additional mutually enhancing\nmodels and then combined the predictions of these individual\nmodels to achieve the best of accuracy and coverage. There\nare other works on query progress indicators [ 13,34] that\nuse the run time statistics from the currently running query\nto tell how much percentage of the work has been completed\nfor the query. Our approach, in contrast, uses compile time\nfeatures to make the prediction before the execution starts.\nCardinality is a key input to cost estimation and several\nlearning and feedback-driven approaches [ 1,44,47]. How-\never, these works have either focused only on recurring or\nstrict subgraphs [ 1,47], or learn only the ratio between the\nactual and predicted cardinality [ 44] that can go wrong in\nmany situations, e.g., partial coverage results in erroneous es-\ntimates due to mixing of disparate models. Most importantly,\nas we discuss in Section 2, fixing cardinalities alone do not\nalways lead to accurate costs in big data systems. There are\nother factors such as resources (e.g., partitions) consumed,\noperator implementations (e.g., custom operators), and hard-\nware characteristics (e.g., parallel distributed environment)\nthat could determine cost. In contrast to cardinality models,\nCleo introduces novel learning techniques (e.g., multiple\nmodels, coupled with an ensemble) and extensions to opti-\nmizer to robustly model cost. That said, cardinality is still\nan important feature (see Figure 5), and is also key to de-\nciding partition counts, memory allocation at runtime, as\nwell as for speculative execution in the job manager. A more\ndetailed study on cardinality estimates in big data systems\nis an interesting avenue for future work.\n\nSeveral works find the optimal resources given a physical\nquery execution plan [ 3,39,45]. They either train a perfor-\nmance model or apply non-parametric Bayesian optimization\ntechniques with a few sample runs to find the optimal re-\nsource configuration. However, the optimal execution plan\nmay itself depend on the resources, and therefore in this\nwork, we jointly find the optimal cost and resources. Never-\ntheless, the ideas from resource optimization work can be\nleveraged in our system to reduce the search space, especially\nif we consider multiple hardware types.\nGenerating efficient combination of query plans and re-\nsources are also relevant to the new breed of serverless com-\nputing, where users are not required to provision resources\nexplicitly and they are billed based on usage [ 43]. For big\ndata queries, this means that the optimizer needs to accu-\nrately estimate the cost of queries for given resources and\nexplore different resource combinations so that users do not\nend up over-paying for their queries.\nFinally, several recent works apply machine learning tech-\nniques to improve different components of a data system [ 27,\n35]. The most prominent being learned indexes [ 28], which\noverfits a given stored data to a learned model that provides\nfaster lookup as well as smaller storage footprint. [ 35] takes\na more disruptive approach, where the vision is to replace\nthe traditional query optimizers with one built using neural\nnetworks. In contrast, our focus in this paper is on improv-\ning the cost-estimation of operators in big data systems and\nour goal is to integrate learned models into existing query\noptimizers in a minimally invasive manner.\n8 CONCLUSION\nAccurate cost prediction is critical for resource efficiency in\nbig data systems. At the same time, modeling query costs is\nincredibly hard in these systems. In this paper, we present\ntechniques to learn cost models from the massive cloud work-\nloads. We recognize that cloud workloads are highly hetero-\ngeneous in nature and no one model fits all . Instead, we lever-\nage the common subexpression patterns in the workload\nand learn specialized models for each pattern. We further\ndescribe the accuracy and coverage trade-off of these special-\nized models and present additional mutual enhancing models\nto bridge the gap. We combine the predictions from all of\nthese individual models into a robust model that provides the\nbest of both accuracy and coverage over a sufficiently long\nperiod of time. A key part of our contribution is to integrate\nthe learned cost models with existing query optimization\nframeworks. We present details on integration with SCOPE,\na Cascade style query optimizer, and show how the learned\nmodels could be used to efficiently find both the query and re-\nsource optimal plans. Overall, applying machine learning to\nsystems is an active area of research, and this paper presents\na practical approach for doing so deep within the core of a\nquery processing system.\nREFERENCES\n[1]S. Agarwal, S. Kandula, N. Bruno, M.-C. Wu, I. Stoica, and J. Zhou. Re-\noptimizing data-parallel computing. In Proceedings of the 9th USENIXconference on Networked Systems Design and Implementation , pages\n21–21. USENIX Association, 2012.\n[2]M. Akdere, U. Çetintemel, M. Riondato, E. Upfal, and S. B. Zdonik.\nLearning-based query performance modeling and prediction. In Data\nEngineering (ICDE), 2012 IEEE 28th International Conference on , pages\n390–401. IEEE, 2012.\n[3]O. Alipourfard, H. H. Liu, J. Chen, S. Venkataraman, M. Yu, and\nM. Zhang. Cherrypick: Adaptively unearthing the best cloud con-\nfigurations for big data analytics. In NSDI , volume 2, pages 4–2, 2017.\n[4] AWS Athena. https://aws.amazon.com/athena/.\n[5]F. R. Bach and M. I. Jordan. Kernel independent component analysis.\nJournal of machine learning research , 3(Jul):1–48, 2002.\n[6]E. Boutin, J. Ekanayake, W. Lin, B. Shi, J. Zhou, Z. Qian, M. Wu, and\nL. Zhou. Apollo: scalable and coordinated scheduling for cloud-scale\ncomputing. In 11th{USENIX}Symposium on Operating Systems Design\nand Implementation ( {OSDI}14), pages 285–300, 2014.\n[7]N. Bruno, S. Agarwal, S. Kandula, B. Shi, M. Wu, and J. Zhou. Recurring\njob optimization in scope. In SIGMOD , pages 805–806, 2012.\n[8]N. Bruno, S. Jain, and J. Zhou. Continuous cloud-scale query optimiza-\ntion and processing. Proceedings of the VLDB Endowment , 6(11):961–\n972, 2013.\n[9]N. Bruno, S. Jain, and J. Zhou. Recurring Job Optimization for Massively\nDistributed Query Processing. IEEE Data Eng. Bull. , 36(1):46–55, 2013.\n[10] N. Bruno, Y. Kwon, and M.-C. Wu. Advanced join strategies for large-\nscale distributed computation. Proceedings of the VLDB Endowment ,\n7(13):1484–1495, 2014.\n[11] R. Chaiken, B. Jenkins, P. Larson, B. Ramsey, D. Shakib, S. Weaver, and\nJ. Zhou. SCOPE: easy and efficient parallel processing of massive data\nsets. PVLDB , 1(2):1265–1276, 2008.\n[12] S. Chaudhuri and V. Narasayya. Self-tuning database systems: a decade\nof progress. In Proceedings of the 33rd international conference on Very\nlarge data bases , pages 3–14. VLDB Endowment, 2007.\n[13] S. Chaudhuri, V. Narasayya, and R. Ramamurthy. Estimating progress\nof execution for sql queries. In Proceedings of the 2004 ACM SIGMOD\ninternational conference on Management of data , pages 803–814. ACM,\n2004.\n[14] A. Dutt and J. R. Haritsa. Plan bouquets: A fragrant approach to robust\nquery processing. ACM Transactions on Database Systems (TODS) ,\n41(2):11, 2016.\n[15] A. Dutt, C. Wang, A. Nazi, S. Kandula, V. Narasayya, and S. Chaudhuri.\nSelectivity Estimation for Range Predicates Using Lightweight Models.\nPVLDB , 12(9):1044–1057, 2019.\n[16] FastTree. https://www.nuget.org/packages/Microsoft.ML.FastTree/.\n[17] A. D. Ferguson, P. Bodík, S. Kandula, E. Boutin, and R. Fonseca. Jockey:\nguaranteed job latency in data parallel clusters. In EuroSys , pages\n99–112, 2012.\n[18] J. H. Friedman. Stochastic gradient boosting. Computational statistics\n& data analysis , 38(4):367–378, 2002.\n[19] A. Ganapathi, H. Kuno, U. Dayal, J. L. Wiener, A. Fox, M. Jordan, and\nD. Patterson. Predicting multiple metrics for queries: Better decisions\nenabled by machine learning. In Data Engineering, 2009. ICDE’09. IEEE\n25th International Conference on , pages 592–603. IEEE, 2009.\n[20] Google BigQuery. https://cloud.google.com/bigquery.\n[21] G. Graefe. The Cascades framework for query optimization. IEEE Data\nEng. Bull. , 18(3):19–29, 1995.\n[22] IBM BigSQL. https://www.ibm.com/products/db2-big-sql.\n[23] A. Jindal, K. Karanasos, S. Rao, and H. Patel. Selecting Subexpressions\nto Materialize at Datacenter Scale. In VLDB , 2018.\n[24] A. Jindal, S. Qiao, H. Patel, Z. Yin, J. Di, M. Bag, M. Friedman, Y. Lin,\nK. Karanasos, and S. Rao. Computation Reuse in Analytics Job Service\nat Microsoft. In SIGMOD , 2018.\n[25] S. A. Jyothi, C. Curino, I. Menache, S. M. Narayanamurthy, A. Tumanov,\nJ. Yaniv, R. Mavlyutov, Í. Goiri, S. Krishnan, J. Kulkarni, et al. Morpheus:\nTowards automated slos for enterprise clusters. In 12th{USENIX}\nSymposium on Operating Systems Design and Implementation ( {OSDI}\n16), pages 117–134, 2016.\n\n[26] A. Kipf, T. Kipf, B. Radke, V. Leis, P. Boncz, and A. Kemper. Learned\ncardinalities: Estimating correlated joins with deep learning. CIDR,\n2019.\n[27] T. Kraska, M. Alizadeh, A. Beutel, E. Chi, J. Ding, A. Kristo, G. Leclerc,\nS. Madden, H. Mao, and V. Nathan. Sagedb: A learned database system.\nCIDR, 2019.\n[28] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case\nfor learned index structures. In Proceedings of the 2018 International\nConference on Management of Data , pages 489–504. ACM, 2018.\n[29] K. Lee, A. C. König, V. Narasayya, B. Ding, S. Chaudhuri, B. Ellwein,\nA. Eksarevskiy, M. Kohli, J. Wyant, P. Prakash, et al. Operator and\nquery progress estimation in microsoft sql server live query statistics.\nInProceedings of the 2016 International Conference on Management of\nData , pages 1753–1764. ACM, 2016.\n[30] C. Lei, Z. Zhuang, E. A. Rundensteiner, and M. Y. Eltabakh. Redoop\ninfrastructure for recurring big data queries. PVLDB , 7(13):1589–1592,\n2014.\n[31] V. Leis, A. Gubichev, A. Mirchev, P. Boncz, A. Kemper, and T. Neumann.\nHow good are query optimizers, really? Proceedings of the VLDB\nEndowment , 9(3):204–215, 2015.\n[32] J. Li, A. C. König, V. Narasayya, and S. Chaudhuri. Robust estimation\nof resource consumption for sql queries using statistical techniques.\nProceedings of the VLDB Endowment , 5(11):1555–1566, 2012.\n[33] G. Lohman. Is query optimization a âĂĲsolvedâĂİ problem. In Proc.\nWorkshop on Database Query Optimization , volume 13. Oregon Gradu-\nate Center Comp. Sci. Tech. Rep, 2014.\n[34] G. Luo, J. F. Naughton, C. J. Ellmann, and M. W. Watzke. Toward a\nprogress indicator for database queries. In Proceedings of the 2004\nACM SIGMOD international conference on Management of data , pages\n791–802. ACM, 2004.\n[35] R. Marcus, P. Negi, H. Mao, C. Zhang, M. Alizadeh, T. Kraska, O. Pa-\npaemmanouil, and N. Tatbul. Neo: A learned query optimizer. arXiv\npreprint arXiv:1904.03711 , 2019.\n[36] V. Markl, V. Raman, D. Simmen, G. Lohman, H. Pirahesh, and M. Cil-\nimdzic. Robust query processing through progressive optimization.\nInProceedings of the 2004 ACM SIGMOD international conference on\nManagement of data , pages 659–670. ACM, 2004.\n[37] MART. http://statweb.stanford.edu/ jhf/MART.html.\n[38] M. Poess and C. Floyd. New tpc benchmarks for decision support and\nweb commerce. ACM Sigmod Record , 29(4):64–71, 2000.\n[39] K. Rajan, D. Kakadia, C. Curino, and S. Krishnan. Perforator: eloquent\nperformance models for resource optimization. In Proceedings of the\nSeventh ACM Symposium on Cloud Computing , pages 415–427. ACM,\n2016.[40] R. Ramakrishnan, B. Sridharan, J. R. Douceur, P. Kasturi,\nB. Krishnamachari-Sampath, K. Krishnamoorthy, P. Li, M. Manu,\nS. Michaylov, R. Ramos, et al. Azure data lake store: a hyperscale\ndistributed file service for big data analytics. In Proceedings of the 2017\nACM International Conference on Management of Data , pages 51–63.\nACM, 2017.\n[41] A. Roy, A. Jindal, H. Patel, A. Gosalia, S. Krishnan, and C. Curino.\nSparkCruise: Handsfree Computation Reuse in Spark. PVLDB ,\n12(12):1850–1853, 2019.\n[42] J. Schad, J. Dittrich, and J.-A. Quiané-Ruiz. Runtime Measurements\nin the Cloud: Observing, Analyzing, and Reducing Variance. PVLDB ,\n3(1-2):460–471, 2010.\n[43] Cloud Programming Simplified: A Berkeley View on Serverless Com-\nputing. https://www2.eecs.berkeley.edu/Pubs/TechRpts/2019/EECS-\n2019-3.pdf.\n[44] M. Stillger, G. M. Lohman, V. Markl, and M. Kandil. Leo-db2’s learning\noptimizer. In VLDB , volume 1, pages 19–28, 2001.\n[45] S. Venkataraman and others. Ernest: Efficient performance prediction\nfor large-scale advanced analytics. In NSDI , pages 363–378, 2016.\n[46] L. Viswanathan, A. Jindal, and K. Karanasos. Query and Resource\nOptimization: Bridging the Gap. In ICDE , pages 1384–1387, 2018.\n[47] C. Wu, A. Jindal, S. Amizadeh, H. Patel, W. Le, S. Qiao, and S. Rao.\nTowards a Learning Optimizer for Shared Clouds. PVLDB , 12(3):210–\n222, 2018.\n[48] D. Xin, S. Macke, L. Ma, J. Liu, S. Song, and A. Parameswaran. HELIX:\nHolistic Optimization for Accelerating Iterative Machine Learning.\nPVLDB , 12(4):446–460, 2018.\n[49] Z. Yint, J. Sun, M. Li, J. Ekanayake, H. Lin, M. Friedman, J. A. Blakeley,\nC. Szyperski, and N. R. Devanur. Bubble execution: resource-aware\nreliable analytics at cloud scale. Proceedings of the VLDB Endowment ,\n11(7):746–758, 2018.\n[50] J. Zhou, N. Bruno, M.-C. Wu, P.-Å. Larson, R. Chaiken, and D. Shakib.\nSCOPE: parallel databases meet MapReduce. VLDB J. , 21(5):611–636,\n2012.\n[51] Q. Zhou, J. Arulraj, S. Navathe, W. Harris, and D. Xu. Automated\nVerification of Query Equivalence Using Satisfiability Modulo Theories.\nPVLDB , 12(11):1276–1288, 2019.\n[52] J. Zhu, N. Potti, S. Saurabh, and J. M. Patel. Looking ahead makes query\nplans robust: Making the initial case with in-memory star schema data\nwarehouse workloads. Proceedings of the VLDB Endowment , 10(8):889–\n900, 2017.\n[53] H. Zou and T. Hastie. Regularization and variable selection via the\nelastic net. Journal of the royal statistical society: series B (statistical\nmethodology) , 67(2):301–320, 2005.",
  "textLength": 100597
}