{
  "paperId": "5f074c7813e815c1f0fa5d210f36c31657d95f24",
  "title": "LES3: Learning-based exact set similarity search",
  "pdfPath": "5f074c7813e815c1f0fa5d210f36c31657d95f24.pdf",
  "text": "LES3: Learning-based Exact Set Similarity Search\nYifan Li\nYork University\nyifanli@eecs.yorku.caXiaohui Yu\nYork University\nxhyu@yorku.caNick Koudas\nUniversity of Toronto\nkoudas@cs.toronto.edu\nABSTRACT\nSet similarity search is a problem of central interest to a wide va-\nriety of applications such as data cleaning and web search. Past\napproaches on set similarity search utilize either heavy indexing\nstructures, incurring large search costs or indexes that produce\nlarge candidate sets. In this paper, we design a learning-based exact\nset similarity search approach, LES3. Our approach first partitions\nsets into groups, and then utilizes a light-weight bitmap-like index-\ning structure, called token-group matrix (TGM), to organize groups\nand prune out candidates given a query set. In order to optimize\npruning using the TGM, we analytically investigate the optimal par-\ntitioning strategy under certain distributional assumptions. Using\nthese results, we then design a learning-based partitioning approach\ncalled L2P and an associated data representation encoding, PTR,\nto identify the partitions. We conduct extensive experiments on\nreal and synthetic datasets to fully study LES3, establishing the\neffectiveness and superiority over other applicable approaches.\nPVLDB Reference Format:\nYifan Li, Xiaohui Yu, and Nick Koudas. LES3: Learning-based Exact Set\nSimilarity Search. PVLDB, 14(11): XXX-XXX, 2021.\ndoi:10.14778/3476249.3476263\nPVLDB Artifact Availability:\nThe source code, data, and/or other artifacts have been made available at\nhttps://github.com/AwesomeYifan/learning-based-set-sim-search.\n1 INTRODUCTION\nGiven a databaseDof sets each comprised of tokens (a token can\nbe an arbitrary string from a given alphabet Î£, a unique identifier\nfrom a known domain, etc.), a single query set ğ‘„(consisting of\ntokens from the same domain), and a similarity measure ğ‘†ğ‘–ğ‘š(âˆ—),\nthe problem of set similarity search is to identify fromDthose sets\nthat are within a user defined similarity threshold to the query ğ‘„\n(range query) or ğ‘˜sets that are the most similar to ğ‘„(ğ‘˜NN query).\nThis operation is essential to a wide spectrum of applications, such\nas data cleaning [ 27,66], data integration [ 18,23], query refine-\nment [ 57], and digital trace analysis [ 44]. For example, a common\ntask in data cleaning is to perform approximate string matching\nto identify near duplicates of a given query string. When strings\nare tokenized, the task of approximate string matching becomes\na set similarity search problem. Given its prevalent use, efficient\nset similarity search is of paramount importance. A brute-force\napproach to supporting set similarity search is to scan all the sets\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license. For any use beyond those covered by this license, obtain permission by\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 14, No. 11 ISSN 2150-8097.\ndoi:10.14778/3476249.3476263inDand evaluate ğ‘†ğ‘–ğ‘š(âˆ—)betweenğ‘„and each set inDto obtain\nthe results. When Dis large or such operations are carried out\nrepeatedly, however, its efficiency becomes a major concern.\nExisting proposals to improve the search performance adopt a\nfilter-and-verify framework: in the filter step, candidate sets are\ngenerated based on indexes on D, and the candidate sets are further\nexamined, computing the similarity between ğ‘„and each candidate\nset in the verify step. Depending on the indexes used in the filter\nstep, existing methods can be categorized into two groups: inverted\nindex-based and tree-based. Inverted index-based methods build\ninverted index on tokens and only fetch those sets containing (a\nsubset of) tokens present in the query set as candidates. Tree-based\nmethods [ 72,73] transform sets to scalars [ 72] or vectors [ 73] and\ninsert them into B+-trees or R-trees, which are then used at query\nprocessing time to quickly identify the candidate sets. As verifica-\ntion of a candidate set can be done very efficiently under almost\nall well-known set similarity measures (e.g., Jaccard, Dice, Cosine\nsimilarity) incurring a cost linear in the size of the set, optimiza-\ntion of the filter step is critical. Unfortunately, existing methods\neither utilize heavy-weight indexes that incur expensive storage\nconsumption and excessive scanning cost during filtering [ 73], or\nemploy indexes that are light-weight but with very limited pruning\nefficiency leading to an overly large candidate set [ 72]. Therefore,\nexisting approaches mostly do not solve the set similarity search\nproblem effectively. In fact for realistically low similarity thresholds\nor large result sizes, as we demonstrate in our experiments, the\nbrute-force approach may perform much better.\nIn this paper, we study the problem of set similarity search, and\npropose a new approach named LES3(short for Learning-based\nExact SetSimilarity Search) that strives to reduce the time needed\nfor filtering and increase the pruning efficiency of the index struc-\nture at the same time. At a high level, our approach also adopts a\nfilter-and-verify framework; however we advocate the partitioning\nof the sets inDinto non-overlapping groups for filtering. What\ndifferentiates our approach from existing methods is that instead of\nbuilding complex index structures that could become too expensive\nto utilize at run-time, we introduce a light-weight index structure\ncalled token-group matrix (TGM); this structure is essentially a col-\nlection of bit-maps, to organize all groups, yielding comparable or\nhigher pruning efficiency with only a fraction of storage cost and\nthus highly scalable. The TGM captures the association between to-\nkens and groups, and allows us to quickly compute an upper bound\non the similarity between the query set ğ‘„and any set in a given\ngroup. Such upper bounds can then be used for pruning unrelated\ngroups and directing search to the most promising groups.\nAs the search efficiency relies on the pruning efficiency of the\nTGM which in turn depends on how well the sets are partitioned,\nwe formulate the construction of TGM as an optimization problem,\nthat aims to identify the partitioning of sets that yields the highest\npruning efficiency. We first analytically model the base case inarXiv:2107.10417v1  [cs.DB]  22 Jul 2021\n\nwhich every token has the same probability of appearing in any\nset. Our developments reveal that the optimal partitioning has two\nproperties: balance and intra-group coherence. We then design a\ngeneral partitioning objective (ğºğ‘ƒğ‘‚ ) that strives to maximize the\npruning efficiency, taking both properties into consideration.\nWe showcase that the optimal partitioning is NP-Hard and ex-\nplore the use of algorithmic and machine learning-based methods to\nsolve the optimization problem. Recent works [ 22,38] have demon-\nstrated that machine learning techniques have solid performance\nin learning the cumulative distribution function (CDF) of real data\nsets and this property can be used in important data management\ntasks such as indexing [ 43] and sorting [ 39]. We establish that ma-\nchine learning techniques can also be utilized to produce superior\nsolutions to hard optimization problems, central to other important\nindexing tasks such as those in support of set similarity search.\nComplementary to existing works [ 22,43] that utilize models\nsuch as piece-wise linear regression to learn a CDF, we explore\nmodels that are much better a fit, proposing a unique ensemble\nlearning method suitable for progressive partitioning in our set-\nting. The main difficulty in solving the optimization problem is\nthat depending on D, the number of groups needed for effective\npruning can be large, so it is highly challenging to train a single\nnetwork that would place any given set into one of these groups. As\nsuch, we propose a new learning framework named L2P (short for\nLearning to Partition) to address this challenge. L2P trains a cascade\nof Siamese networks to hierarchically partition the database Dinto\nincreasingly finer groups until the desired number of groups is\nreached, resulting in 2ğ‘–groups at level ğ‘–. The loss function for the\nSiamese network is specifically designed to minimize the distances\nbetween sets in the same group. As the input of a Siamese network\nhas to be a vector, we devise a novel and efficient set representation\nmethod, path-table representation (PTR), that specifically caters to\nthe needs of our optimization problem and proves to be a better fit\nthan applicable embedding techniques. Although training ML mod-\nels is known to be time-consuming, as will be shown in Section 7,\nL2P yields better partitioning results with much shorter processing\ntime and only a small fraction of memory usage compared with\nother widely-adopted partitioning methods.\nWe fully develop the query processing algorithms for both range\nsearch andğ‘˜NN search based on the TGM, and conduct extensive\nexperiments on synthetic and real data sets to study the properties\nof our proposal and compare it against other applicable approaches.\nOur results demonstrate that both the proposed set representation\nmethod and the learning framework lead to much stronger pruning\nefficiency than competing methods. Overall, the proposed LES3\nmethod significantly outperforms the baseline methods in both\nmemory-based and disk-based settings.\nIn summary, we make the following main contributions.\nâ€¢We propose a learning-based approach, LES3, for exact set\nsimilarity search, which partitions the database into groups\nto facilitate filtering. Central to LES3is TGM, a light-weight\nyet highly effective index that provides stronger pruning\nefficiency with less cost than state-of-the-art indexes.\nâ€¢We formally analyze the partitioning of the database into\ngroups, casting it as an optimization problem and discussing\nits distinction from well-studied clustering problems.â€¢We devise a novel learning framework, L2P, to solve the par-\ntitioning optimization problem, which yields significantly\nbetter partitioning results while incurring a small fraction\nof processing time and space cost compared with traditional\nalgorithmic methods. L2P consists of a cascade of Siamese\nnetworks, an architecture that is able to effectively learn a\npartition of the dataset at different granularities, with up to\nthousands of groups at the finest level.\nâ€¢We develop a carefully designed method for set representa-\ntion, PTR, taking group separation into consideration. PTR\ntheoretically and experimentally facilitates the training of\nL2P. Compared with other embedding techniques, PTR is\norders of magnitude faster in computing set representations,\nand thus is more suitable for the target application where\nmillions or billions of sets are involved (see Section 7).\nâ€¢We experimentally study the performance of LES3, L2P, and\nPTR, varying parameters of interest, including the network\nstructure, number of groups and result size. We also examine\nthe scalability of LES3utilizing real world large datasets in\naddition to previously used set similarity benchmarks. The\nproposed methods significantly and consistently outperform\ncompeting methods across a large variety of settings, pro-\nviding up to 5 times faster query processing and requiring\nup to 90%less space in typical scenarios.\nThe rest of the paper is organized as follows. In Section 2, we\ndefine the terminologies to be used throughout the paper. Section\n3 introduces the index structure, TGM. In Section 4, we discuss\nhow the partitioning problem can be formulated as an optimization\nproblem. In Section 5, we propose a machine learning framework\nto solve the optimization problem. Section 6 presents the query\nprocessing algorithms for set similarity search. The experimental\nevaluation is presented in Section 7. Section 8 discusses related\nwork, and Section 9 concludes this paper.\n2 PRELIMINARIES\nAsetis an unordered collection of elements called tokens (we also\nconsider multiset which may contain duplicate tokens in the paper).\nWe useğ‘†to denote an arbitrary set and ğ‘¡an arbitrary token. The\ndatabaseDis a collection of sets, and all tokens form the token\nuniverseT. Two sets are considered similar if the overlap in their\ntokens exceeds a user-defined threshold. Usually, such overlap is\nnormalized to account for the size difference between sets. Exam-\nples of such similarity measures include Jaccard, Dice, and Cosine\nsimilarity. To make our discussion more concrete, we focus on Jac-\ncard similarity, and discuss how our approach can be applied to\nother similarity measures in Section 3.2. Next we give the formal\nproblem definitions.\nDefinition 2.1. kNN Search . Given the database of sets D, a set\nsimilarity measure ğ‘†ğ‘–ğ‘š(âˆ—), a query set1ğ‘„, and a result size ğ‘˜, find\na collectionRğ‘˜\nğ‘„âŠ†D s.t.|Rğ‘˜\nğ‘„|=ğ‘˜andâˆ€ğ‘†âˆˆRğ‘˜\nğ‘„,âˆ€ğ‘†â€²âˆˆDâˆ’Rğ‘˜\nğ‘„,\nğ‘†ğ‘–ğ‘š(ğ‘„,ğ‘†)â‰¥ğ‘†ğ‘–ğ‘š(ğ‘„,ğ‘†â€²).\n1Without loss of generality, we assume throughout the paper that a query set consists\nof tokens existing in Tonly. The case of query sets containing tokens not in Tcan be\nhandled similarly and is discussed in Section 3.1.\n\nDefinition 2.2. Range Search . Given the database of sets D, a set\nsimilarity measure ğ‘†ğ‘–ğ‘š(âˆ—), a query set ğ‘„, and a threshold ğ›¿, find a\ncollectionRğ›¿\nğ‘„âŠ†D s.t.âˆ€ğ‘†âˆˆRğ›¿\nğ‘„,ğ‘†ğ‘–ğ‘š(ğ‘„,ğ‘†)â‰¥ğ›¿, andâˆ€ğ‘†â€²âˆˆDâˆ’Rğ›¿\nğ‘„,\nğ‘†ğ‘–ğ‘š(ğ‘„,ğ‘†â€²)<ğ›¿.\nOur goal is to accelerate the process of identifying the result\ncollectionRğ‘˜\nğ‘„orRğ›¿\nğ‘„for the given ğ‘˜orğ›¿. In general, the query\nanswering process consists of a filtering step (choosing candidate\nsets) and a verification step (comparing candidate sets with the\nquery set). The cost of the verification step depends directly on\nthe pruning efficiency of the search process, which measures the\nproportion of sets in Dbeing pruned in the filtering step.\nDefinition 2.3. Pruning Efficiency (PE) . LetSğ‘„be the collec-\ntion of candidate sets for which the similarities to ğ‘„must be com-\nputed in the process of identifying Rğ‘˜\nğ‘„orRğ›¿\nğ‘„. Then the pruning\nefficiency of query processing, denoted as ğ‘ƒğ¸, is|D|âˆ’(|Sğ‘„|âˆ’ğ‘˜)\n|D|for\nğ‘˜NN query, or|D|âˆ’(|Sğ‘„|âˆ’|Rğ›¿\nğ‘„|)\n|D|for range query.\nClearlyğ‘ƒğ¸falls in the range[0,1]. All other things being equal,\na higherğ‘ƒğ¸leads to a lower verification cost. Our focus in this\npaper is therefore to design an approach for set similarity search\nthat enjoys high PE and low filtering and verification cost.\n3 TOKEN-GROUP MATRIX\nThe basic idea of our approach is to partition the sets in Dinto\nnon-overlapping groups and index them properly, so that the search\nspace can be pruned (i.e., certain groups can be quickly eliminated\nfrom further consideration) to speed up query processing. At the\nheart of our proposal is the token-group matrix (TGM), the index\nthat records the relationship between tokens and the groups result-\ning from partitioning. In this section, we present the index structure\nand discuss its applicability across different similarity measures.\n3.1 Index Structure\nAssume for now, that Dis already partitioned into ğ‘›non-overlapping\ngroups,G1,Â·Â·Â·Gğ‘›; we defer the discussion of the strategies for par-\ntitioning to the next section. The goals of the index are simplicity\n(so that it incurs little computational and storage overhead) and\neffectiveness (providing high pruning efficiency). To this end, the\nTGM,ğ‘€, with sizeğ‘›âˆ—|T| , is constructed in the following way:\nğ‘€[ğ‘”,ğ‘¡]=\u001a1,ifâˆƒğ‘†âˆˆGğ‘”s.t.ğ‘¡âˆˆğ‘†\n0,otherwise(1)\nwhereğ‘¡âˆˆ[1,|T|] andğ‘”âˆˆ[1,ğ‘›].\nAn example of TGM is given in Figure 1, where T={ğ´,ğµ,ğ¶,ğ·}\nand six sets are partitioned into two groups G0andG1.\nFigure 1: An example of TGM\nThe design of the TGM is based on the observation that when\ndeciding whether a group of sets is a candidate for a query set or\nnot, the only information needed is the number of common tokens\nthey share. Such information can be easily obtained by visitingsome elements in ğ‘€, and thus we can compute a similarity upper\nbound between a query set ğ‘„and a group of sets Gğ‘”, which are\nuseful in pruning the search space, as follows:\nğ‘ˆğµ(ğ‘„,Gğ‘”)=Ã\nğ‘¡âˆˆğ‘„ğ‘€[ğ‘”,ğ‘¡]\n|ğ‘„|(2)\nContinuing the example above, we assume that the query set is\n{ğ´}andG0andG1in Figure 1 are candidates. Then the similarity\nbound between the query set and G0isğ‘€[G0,ğ´]\n|{ğ´}|=1, and the upper\nbound forG1isğ‘€[G1,ğ´]\n|{ğ´}|=0.\nAlthough we assume ğ‘„contains tokens inTonly, the case where\nthis does not hold can be handled by letting ğ‘€[âˆ—,ğ‘¡â€²]=0forğ‘¡â€²âˆ‰T\nin Equation (2). No further changes are required.\nIn the query processing step, if the upper bound of group Gğ‘”\nexceeds a threshold (can be ğ›¿in range query, or the minimal ğ‘˜NN\nsimilarity found so far), we compare all sets in Gğ‘”with the query set.\nThe time complexity of computing the similarity bounds between\nthe query set and all groups of sets is ğ‘‚(ğ‘›|ğ‘„|). It in general costs\nmuch less than computing the similarity between the query set\nand each set inD, as the number of groups is usually orders of\nmagnitude smaller than |D|.\nIn terms of space consumption, each element in TGM is repre-\nsented by a bit, and TGM is essentially a bitmap index. It is evident\nthatğ‘€is usually a very sparse matrix as each set usually contains a\nvery small portion of the tokens from the universe. When necessary,\nmany existing compression techniques [ 58,59] can be employed to\nreduce the size of ğ‘€.\n3.2 Applicability\nAlthough Equation (2) is computed assuming Jaccard index as the\nsimilarity metric, TGM works with many other set similarity mea-\nsures as well, including measures that do not follow the triangle\ninequality, such as cosine similarity.\nTheorem 3.1. Forâˆ€ğ‘„,ğ‘†âŠ†T, letğ‘…=ğ‘„âˆ©ğ‘†. TGM is applicable to\nset similarity search tasks with measure ğ‘†ğ‘–ğ‘š(âˆ—)if\n(1)ğ‘†ğ‘–ğ‘š(ğ‘„,ğ‘…)â‰¥ğ‘†ğ‘–ğ‘š(ğ‘„,ğ‘†), and\n(2)âˆ€ğ‘…â€²âŠ‚ğ‘…,ğ‘†ğ‘–ğ‘š(ğ‘„,ğ‘…)â‰¥ğ‘†ğ‘–ğ‘š(ğ‘„,ğ‘…â€²)\nProof. We prove that for an arbitrary query set ğ‘„and an arbi-\ntrary groupGğ‘”, we can compute a similarity upper bound ğ‘ˆğµ(ğ‘„,Gğ‘”)\nwith TGM using Equation (2) such that âˆ€ğ‘†âˆˆGğ‘”,ğ‘ˆğµ(ğ‘„,Gğ‘”) â‰¥\nğ‘†ğ‘–ğ‘š(ğ‘„,ğ‘†). Letğ‘…={ğ‘¡|ğ‘¡âˆˆğ‘„âˆ§âˆƒğ‘†âˆˆ Gğ‘”,ğ‘¡âˆˆğ‘†}, then we know\nâˆ€ğ‘†âˆˆGğ‘”,ğ‘„âˆ©ğ‘†âŠ†ğ‘…. Ifğ‘„âˆ©ğ‘†=ğ‘…, then clearly ğ‘†ğ‘–ğ‘š(ğ‘„,ğ‘…)â‰¥ğ‘†ğ‘–ğ‘š(ğ‘„,ğ‘†);\nifğ‘„âˆ©ğ‘†=ğ‘…â€²âŠ‚ğ‘…, thenğ‘†ğ‘–ğ‘š(ğ‘„,ğ‘…)â‰¥ğ‘†ğ‘–ğ‘š(ğ‘„,ğ‘…â€²)â‰¥ğ‘†ğ‘–ğ‘š(ğ‘„,ğ‘†). In\neither case,ğ‘†ğ‘–ğ‘š(ğ‘„,ğ‘…)upper bounds ğ‘†ğ‘–ğ‘š(ğ‘„,ğ‘†), and thus we can use\nğ‘†ğ‘–ğ‘š(ğ‘„,ğ‘…)asğ‘ˆğµ(ğ‘„,Gğ‘”). Since it is possible that ğ‘…=ğ‘†, in which\ncaseğ‘†ğ‘–ğ‘š(ğ‘„,ğ‘…)=ğ‘†ğ‘–ğ‘š(ğ‘„,ğ‘†), the bound ğ‘ˆğµ(ğ‘„,Gğ‘”)is tight, even in\nmultiset settings. â–¡\nFor example, let ğ‘„={ğ‘¡1,ğ‘¡2,ğ‘¡3}andğ‘„âˆ©ğ‘†={ğ‘¡1,ğ‘¡2}. Then with\nJaccard similarity, the set with the maximal similarity to ğ‘„is{ğ‘¡1,ğ‘¡2}\nand the upper bound is2\n3; with cosine similarity, the set with the\nmaximal similarity to ğ‘„is also{ğ‘¡1,ğ‘¡2}, but the upper bound is\n2âˆš\n3âˆ—2â‰ˆ0.82. Note that although most similarity measures satisfy\nthe TGM Applicability Property, some exceptions do exist. One\n\nsuch example is the learned metric [ 37] which takes two samples\n(e.g., images) as the input and predicts their similarity.\nIn what follows, we call the two properties listed in Theorem\n3.1 the TGM Applicability Property . Note that the token universe T\ndoes not need to be static. We will discuss how to adapt TGM to\ndeal with cases where Tis dynamically changing in Section 6.\n4 OPTIMIZING PARTITIONING\nWe analyze how to optimize partitioning to provide higher pruning\nefficiency. We discuss desired properties of the partitioning, and\ndevelop the objective function for the partitioning optimization\nproblem that will guide the development of effective partition-\ning strategies. To make our formal analysis tractable, we make\nassumptions regarding the token distribution; nonetheless, as will\nbe demonstrated by our experimental results in Section 7, the opti-\nmization objectives and strategies thus developed are also expected\nto perform well when the assumptions do not hold.\n4.1 The Case of Uniform Token Distribution\nWe formally analyze the effect of partitioning on pruning efficiency\nwhen the following assumption on token distribution holds.\nDefinition 4.1. Uniform Token Distribution Assumption . The\nprobabilities that different tokens belong to an arbitrary set are\nidentical and independent. More specifically, âˆ€ğ‘¡ğ‘–,ğ‘¡ğ‘—âˆˆT,âˆ€ğ‘†âˆˆD,\nğ‘ƒ(ğ‘¡ğ‘–âˆˆğ‘ )=ğ‘ƒ(ğ‘¡ğ‘—âˆˆğ‘†), andğ‘ƒ(ğ‘¡ğ‘–âˆˆğ‘†|ğ‘¡ğ‘—âˆˆğ‘ )=ğ‘ƒ(ğ‘¡ğ‘–âˆˆğ‘†|ğ‘¡ğ‘—âˆ‰ğ‘†).\nFor an arbitrary query ğ‘„, the expected pruning efficiency can be\ncomputed as follows:\nğ¸[ğ‘ƒğ¸]=ğ‘›âˆ‘ï¸\nğ‘”=1|Gğ‘”|(1âˆ’ğ‘ˆğµ(ğ‘„,Gğ‘”)) (3)\nGiven the way the TGM is constructed, we rewrite Equation (2)\nin the following way to ease subsequent discussion:\nğ‘ˆğµ(ğ‘„,Gğ‘”)=Ã\nğ‘¡âˆˆğ‘„ğ‘€[ğ‘¡,ğ‘”]\n|ğ‘„|=|ğºğ‘†ğ‘”âˆ©ğ‘„|\n|ğ‘„|, ğºğ‘†ğ‘”=Ã˜\nğ‘†âˆˆGğ‘”ğ‘† (4)\nAccordingly, we rewrite Equation (3) as follows:\nğ¸[ğ‘ƒğ¸]=ğ‘›âˆ‘ï¸\nğ‘”=1|Gğ‘”|(1âˆ’|ğºğ‘†ğ‘”âˆ©ğ‘„|\n|ğ‘„|) (5)\nAs we assume ğ‘„follows the same distribution as D,ğ¸[ğ‘ƒğ¸]over\nall possibleğ‘„can be estimated by the following equation:\nÃ\nğ‘„âˆˆDÃğ‘›\nğ‘”=1|Gğ‘”|(1âˆ’|ğºğ‘†ğ‘”âˆ©ğ‘„|\n|ğ‘„|)\n|D|(6)\nSince|D|is a constant, we keep the nominator of Equation (6)\nonly, and adjust the order as follows:\nğ‘›âˆ‘ï¸\nğ‘”=1|Gğ‘”|âˆ‘ï¸\nğ‘„âˆˆD(1âˆ’|ğºğ‘†ğ‘”âˆ©ğ‘„|\n|ğ‘„|) (7)\nTo ease following analysis, we define term ğ¹in Equation (8),\nand claim that maximizing Equation (7) (and thus maximizing the\npruning efficiency) is equivalent to minimizing ğ¹:\nğ¹=ğ‘›âˆ‘ï¸\nğ‘”=1|Gğ‘”|âˆ‘ï¸\nğ‘„âˆˆD|ğºğ‘†ğ‘”âˆ©ğ‘„|\n|ğ‘„|(8)\nWe derive several properties regarding the partitioning from\nEquation (8) so as to design practical partitioning algorithms.Theorem 4.2. In a database that satisfies the uniform token dis-\ntribution assumption, the partitioning that minimizes Equation (8)\nproduces groups with equal size (or differ by at most 1).\nProof. We consider the special case where Dis partitioned into\ntwo groupsG1andG2, and|G1| â‰¤ |G 2|. Theğ¹value of such a\npartitioning is:\nğ¹=ğ¹1+ğ¹2=|G1|âˆ‘ï¸\nğ‘„âˆˆD|ğºğ‘†1âˆ©ğ‘„|\n|ğ‘„|+|G 2|âˆ‘ï¸\nğ‘„âˆˆD|ğºğ‘†2âˆ©ğ‘„|\n|ğ‘„|(9)\nNext we move a set ğ‘†fromG1toG2and prove that such move-\nment increases the ğ¹value. We know that if ğ‘†is moved fromG1to\nG2,ğ¹1would decrease and ğ¹2would increase. And since |G1|â‰¤|G 2|,\nequivalently we can prove that |Gğ‘–|Ã\nğ‘„âˆˆD|ğºğ‘†ğ‘–âˆ©ğ‘„|\n|ğ‘„|grows super-\nlinearly with respect to |Gğ‘–|, orÃ\nğ‘„âˆˆD|ğºğ‘†ğ‘–âˆ©ğ‘„|\n|ğ‘„|grows with|Gğ‘–|.\nGiven the construction of ğºğ‘†ğ‘–in Equation (4), this is evidently true.\nTherefore, the ğ¹value increases after the movement of ğ‘†.\nThe above discussion can be naturally extended to multi-groups\nby moving one set from a small group to a large group each time,\nwith theğ¹value increasing and the pruning efficiency decreasing\nduring the process. In conclusion, balanced partitioning results\nyield the highest pruning efficiency. â–¡\nEven though the optimal partitioning is expected to produce\ngroups with almost equal sizes, evidently balance is not the only\ndesired property, according to Equation (8). We temporarily omit\nthe|Gğ‘”|in Equation (8) and discuss other properties the partitioning\nmust satisfy in order to provide higher pruning efficiency.\nTheorem 4.3. In a database that satisfies the uniform token dis-\ntribution assumption, the partitioning that minimizes the following\nobjective provides the highest pruning efficiency:\nğ‘›âˆ‘ï¸\nğ‘”=1|Ã˜\nğ‘†âˆˆGğ‘”ğ‘†| (10)\nProof. Given the assumption that all groups are balanced, min-\nimizing Equation (8) is equivalent to minimizing\nğ‘›âˆ‘ï¸\nğ‘”=1âˆ‘ï¸\nğ‘„âˆˆD|ğºğ‘†ğ‘”âˆ©ğ‘„|\n|ğ‘„|(11)\nSinceğ‘„follows the uniform token distribution as well, which\nmeans that all tokens appear in ğ‘„with the same probability, Equa-\ntion (11) is proportional to the following equation:\nğ‘›âˆ‘ï¸\nğ‘”=1|ğºğ‘†ğ‘”âˆ©T|=ğ‘›âˆ‘ï¸\nğ‘”=1|ğºğ‘†ğ‘”|=ğ‘›âˆ‘ï¸\nğ‘”=1|Ã˜\nğ‘†âˆˆGğ‘”ğ‘†|, (12)\nwhereTdenotes the token universe.\nThus, we can maximize PE by minimizing Equation (10). â–¡\nIn summary, we have the following two desired properties re-\ngarding the partitioning of database D.\nâ€¢Property 1: Groups are balanced;\nâ€¢Property 2:ğ‘ˆ=Ãğ‘›\nğ‘”=1|Ã\nğ‘†âˆˆGğ‘”ğ‘†|is minimized.\n\n(a) Groups produced by ğºğ‘ƒğ‘‚\n(b) Groups produced by ğ‘˜-medians\nFigure 2: Comparison of different partitioning results\n4.2 The General Case\nThe analysis in the preceding section depends on the uniform token\ndistribution assumption. In real-life datasets, this assumption does\nnot hold. However, following the same methodology to derive a\nformal treatment of an arbitrary set/token distribution would be\nchallenging as a realistic mathematical model of arbitrary set/token\ndistributions would be hard to justify. Although the two proper-\nties identified above may not be true for optimal partitioning in\nthe general case, we draw inspirations from them and propose a\nheuristic objective function that strives to maximize PE.\nIn essence Property 2 directs that the more similar (in terms of\ntoken composition) the sets are within a group, the better. We thus\ndesign a general partitioning objective ( ğºğ‘ƒğ‘‚ )we wish to minimize\nreflecting this property:\nğºğ‘ƒğ‘‚=ğ‘›âˆ‘ï¸\nğ‘”=1âˆ‘ï¸\nğ‘†ğ‘¥âˆˆGğ‘”âˆ‘ï¸\nğ‘†ğ‘¦âˆˆGğ‘”(1âˆ’ğ‘†ğ‘–ğ‘š(ğ‘†ğ‘¥,ğ‘†ğ‘¦)), (13)\nwhereğ‘†ğ‘–ğ‘š(âˆ—)can be any measures discussed in Section 3.2.\nIntuitively,ğºğ‘ƒğ‘‚ aims to minimize the sum of the intra-group\npair-wise distances, where distance is defined as 1âˆ’ğ‘†ğ‘–ğ‘š(âˆ—). This\nis similar to Property 2. As an example, consider two groups Gğ‘–\nandGğ‘—with|Gğ‘–|=|Gğ‘—|. Assume that ğ‘†ğ‘–ğ‘š(âˆ—)is Jaccard similarity,\nand we are to place a new set ğ‘†into one of the two groups. Then,\nifÃ\nğ‘†ğ‘–âˆˆGğ‘–(1âˆ’ğ‘†ğ‘–ğ‘š(ğ‘†,ğ‘†ğ‘–))<Ã\nğ‘†ğ‘—âˆˆGğ‘—(1âˆ’ğ‘†ğ‘–ğ‘š(ğ‘†,ğ‘†ğ‘—)), that would\nmeanğ‘†shares more common tokens with sets in group Gğ‘–, and\nthus inserting ğ‘†into groupGğ‘–helps to minimize ğ‘ˆ.\nHowever, considering only Property 2 results in highly skewed\npartitioning results, as placing all sets in the same group provides\nthe minimal ğ‘ˆ(which equals to|T|). Evidently, Property 1 is used to\nprevent such skewed partitioning in the uniform case. Luckily, ğºğ‘ƒğ‘‚\nenjoys a similar functionality: placing all sets in the same group\nprovidesğºğ‘ƒğ‘‚=Ã\nğ‘†ğ‘¥,ğ‘†ğ‘¦âˆˆD(1âˆ’ğ‘†ğ‘–ğ‘š(ğ‘†ğ‘¥,ğ‘†ğ‘¦)), which is the maximal\npossibleğºğ‘ƒğ‘‚ , and thus such a partitioning is never the optimal\nin terms ofğºğ‘ƒğ‘‚ . Thus, the design of ğºğ‘ƒğ‘‚ implicitly incorporates\nboth Property 1 and Property 2.\nIn order to better appreciate the distinctive value of the proposed\npartitioning objective, we compare ğºğ‘ƒğ‘‚ withğ‘˜-medians, perhaps\nthe most popular clustering technique, and show by an example\nhow optimizing ğºğ‘ƒğ‘‚ leads to better results. We use a database of\n21 sets, and the partitioning results based on different clustering\nobjectives are given in Figure 2. Each set is represented by a point in\nthe plot, and to better visualize the results we replace (1âˆ’ğ‘†ğ‘–ğ‘š(âˆ—))\nwith Euclidean distance.\nAssume that the query is to identify the nearest neighbors of\nall 21 points. According to the search strategy of LES3given in\nSection 3.1, all points in the same group are candidates of eachother. Therefore, with the clustering results given in Figure 2(b),\nthe total number of distance calculations is 20âˆ—20+1âˆ—1=401,\nwhile with the partitioning results in Figure 2(a) the number is\n13âˆ—13+8âˆ—8=233. Clearly, the results based on Equation (13)\nhave better pruning efficiency.\nTheorem 4.4. Given a database of sets Dminimizingğºğ‘ƒğ‘‚ onD\nis NP-complete.\nProof. We give a brief proof by showing that minimizing ğºğ‘ƒğ‘‚\nis essentially a 0-1 integer linear programming problem, which has\nbeen shown to be NP-complete [ 10]. More specifically, minimizing\nğºğ‘ƒğ‘‚ is equivalent to solving the following optimization problem:\nmaximize e|D|Â·[AÂ·AâŠºâŠ™D]Â·eâŠº\n|D|\nsubject to eğ‘›Â·AâŠº=e|D|(14)\nwhere Ais a|D|Ã—ğ‘›matrix and A[ğ‘¥,ğ‘”]=1if setğ‘†ğ‘¥belongs to\ngroupGğ‘”andA[ğ‘¥,ğ‘”]=0otherwise, and Dof size|D|Ã—|D| denotes\nthe distance matrix where D[ğ‘¥,ğ‘¦]=1âˆ’ğ‘†ğ‘–ğ‘š(ğ‘†ğ‘¥,ğ‘†ğ‘¦), and eğ‘–is a\nrow vector of length ğ‘–filled with ones. The goal is to find the A\nwhich satisfies the constraint and maximizes the objective.\nThe intuition behind Equation (14) can be described as follows:\nAÂ·AâŠºis a|D|Ã—|D| matrix such that the value at position [ğ‘¥,ğ‘¦]\nis 1 ifğ‘†ğ‘¥andğ‘†ğ‘¦belong to the same group, and 0 otherwise. The\nelement-wise product between AÂ·AâŠºandDmasks out those pair-\nwise distances between sets belonging to different groups, and e|D|Â·\n[AÂ·AâŠºâŠ™D]Â·eâŠº\n|D|sums the remaining distances, which is the same\nobjective as ğºğ‘ƒğ‘‚ . The constraint eğ‘›Â·AâŠº=e|D|guarantees that\neach set belongs to one and only one group. Therefore, minimizing\nğºğ‘ƒğ‘‚ is equivalent to solving Equation (14), which completes the\nproof. â–¡\n4.3 Algorithmic Approaches\nIn this section we propose several algorithmic approaches based on\nexisting applicable clustering methods, which are expected to yield\ngroups with low ğºğ‘ƒğ‘‚ values. More specifically, we design a graph\ncut-based approach (PAR-G), a centroid-based approach (PAR-C),\nand a hierarchical approach (PAR-H).\n4.3.1 Graph cut-based method (PAR-G). Whenğ‘˜orğ›¿is fixed, it\nis possible to build an index structure specifically optimized for\nthe workload. Dong et al. [19] propose a graph cut-based solution\nfor (approximate) nearest neighbor search in Rğ‘‘space by linking\neach point to its neighbors and partitioning the resulting graph into\nbalanced subgraphs with the number of edges crossing different\nsubgraphs minimized. Such a partitioning is shown to yield high\npruning efficiency. Inspired by their approach, we design PAR-G,\nwhich takes ğ‘˜orğ›¿as one of its inputs, as follows:\n(1)Similarity graph construction. For a givenğ‘˜inğ‘˜NN query,\nconstruct the similarity graph, ğºD, ofD, such thatâˆ€ğ‘†ğ‘¥âˆˆD,\nthere exists a corresponding vertex ğ‘‰ğ‘¥inğºD, andâˆ€ğ‘†ğ‘¦âˆˆD,\nifğ‘†ğ‘¦is one of the ğ‘˜nearest neighbors of ğ‘†ğ‘¥, there is an edge\nbetweenğ‘‰ğ‘¥andğ‘‰ğ‘¦inğºD. For a given ğ›¿in range query,\nthere is an edge between ğ‘‰ğ‘¥andğ‘‰ğ‘¦ifğ‘†ğ‘–ğ‘š(ğ‘†ğ‘¥,ğ‘†ğ‘¦)â‰¥ğ›¿.\n(2)Graph cut. PartitionğºDintoğ‘›balanced subgraphs while\nminimizing the number of edges crossing different subgraphs.\nThis can be done with existing graph partitioners [24, 34].\n\n4.3.2 Centroid-based method (PAR-C). Centroid-based methods\n[29] are iterative algorithms which at each iteration relocate an\nelements into a different cluster if such relocation improves the\noverall objective function. For our case, let ğœ™(G)=Ã\nğ‘†ğ‘¥,ğ‘†ğ‘¦âˆˆG(1âˆ’\nğ‘†ğ‘–ğ‘š(ğ‘†ğ‘¥,ğ‘†ğ‘¦))be the sum of all pair-wise distances2in groupG,\nğ‘†âˆˆGğ‘–an arbitrary set, and Î”(ğ‘†,Gğ‘–,Gğ‘—)=ğœ™(Gğ‘–\\ğ‘†)+ğœ™(Gğ‘—âˆªğ‘†)âˆ’\nğœ™(Gğ‘–)âˆ’ğœ™(Gğ‘—)the decrease of ğºğ‘ƒğ‘‚ after moving ğ‘†fromGğ‘–toGğ‘—\n(ğ‘–,ğ‘—âˆˆ[1,ğ‘›]). To be more specific, our method works as follows:\n(1)Initialization. Randomly partition Dintoğ‘›groups;\n(2)Relocation. For eachğ‘†âˆˆD, supposeğ‘†âˆˆGğ‘–. Find groupGâˆ—\nğ‘—\nsuch that Î”(ğ‘†,Gğ‘–,Gâˆ—\nğ‘—)=maxğ‘†,Gğ‘–,Gğ‘—Î”(ğ‘†,Gğ‘–,Gğ‘—)(denoted as\nâ€œthe best groupâ€). If Î”(ğ‘†,Gğ‘–,Gâˆ—\nğ‘—)>0, relocateğ‘†fromGğ‘–to\nGâˆ—\nğ‘—. Repeat this step until no sets are relocated in an iteration.\n(3)Simplification. Considering the data size we deal with (see\nSection 7), finding â€œthe best groupâ€ at each iteration would be\ntoo expensive. Therefore, we adopt the â€œfirst-improvementâ€\nvariant [ 63] of the algorithm, i.e., pick the first group Gğ‘—\nwithÎ”(ğ‘†,Gğ‘–,Gğ‘—)>0rather than the best group.\n4.3.3 Divisive clustering method (PAR-D). Divisive clustering meth-\nods [ 35] start from the single cluster containing all elements and\nrepeatedly split clusters until a desired number of clusters is reached.\nWe reuseğœ™(G)introduced in Section 4.3.2 and use ğ‘–ğ‘‘ğ‘£_ğ‘‘(ğ‘†)to de-\nnote the sum of distances between ğ‘†and all other sets in the same\ngroup asğ‘†. PAR-D works as follows:\n(1)Initialization. TakeDas the initial group.\n(2)Splitting. Find groupGâˆ—=arg maxGğ‘–âˆˆ{G 1,G2,Â·Â·Â·}ğœ™(Gğ‘–),\nwhere{G1,G2,Â·Â·Â·} denotes all current groups. Find set\nğ‘†âˆ—=arg maxğ‘†âˆˆGâˆ—ğ‘–ğ‘‘ğ‘£_ğ‘‘(ğ‘†). Create a new group Gğ‘›ğ‘’ğ‘¤=\n{ğ‘†âˆ—}. For all other sets ğ‘†â€²âˆˆGâˆ—, moveğ‘†â€²toGğ‘›ğ‘’ğ‘¤if such\nmovement reduces the overall ğºğ‘ƒğ‘‚ . Repeat this step until\nthere areğ‘›groups.\n(3)Simplification. Considering the data size we deal with, in-\nstead of finding ğ‘†âˆ—, we choose a random set in Gâˆ—to initialize\nGğ‘›ğ‘’ğ‘¤, which is commonly adopted for group splitting [26].\n4.3.4 Agglomerative clustering method (PAR-A). Agglomerative\nclustering [ 55] works in a bottom-up fashion by initially treating\neach element as a cluster and repeatedly merging clusters until a\ndesired number of clusters is reached. We reuse ğœ™(G)introduced in\nSection 4.3.2 to denote the sum of all pair-wise distances in group\nG. PAR-A works as follows:\n(1)Initialization. Create groupGğ‘–={ğ‘†ğ‘–}for eachğ‘†ğ‘–âˆˆD.\n(2)Merging. Find groupsGâˆ—\n1,Gâˆ—\n2=arg minGğ‘–,Gğ‘—âˆˆ{G 1,G2,Â·Â·Â·}\nğœ™(Gğ‘–âˆªGğ‘—), where{G1,G2,Â·Â·Â·} denotes all current groups\nandğ‘–â‰ ğ‘—. Create a new group Gğ‘›ğ‘’ğ‘¤=Gâˆ—\n1âˆªGâˆ—\n2and remove\ngroupsGâˆ—\n1andGâˆ—\n2. Repeat this step until there are ğ‘›groups.\n(3)Simplification. Considering the data size we deal with, we\nadopt the heuristic that merging smaller groups (groups with\nsmaller number of sets) usually results in smaller values of\nğœ™(Gğ‘–âˆªGğ‘—)and restrict thatGâˆ—\n1is the smallest group (break-\ning ties randomly), and thus only Gâˆ—\n2needs to be identified\nin each iteration.\n2Note that repetitively calculating ğœ™(G) during the partitioning process is computa-\ntional prohibitive, and thus we approximate ğœ™(G) with randomly selected sets in G\nin the experiment (Section 7.4).As we will demonstrate in our experimental study in Section 7,\nthese heuristic approaches do not provide satisfactory performance.\nThe structure of the ğºğ‘ƒğ‘‚ problem objective does not resemble\nthose targeted by well-studied clustering algorithms. In the next\nsection, we explore the use of ML to perform such a partitioning.\n5 L2P: LEARN TO PARTITION SETS INTO\nGROUPS\nAs pointed out by Bengio et al. [ 5], a machine learning approach to\ncombinatorial optimization problems with well-defined objective\nfunctions, such as the Travelling Salesman Problem, has proven to\nbe more promising than classical optimization methods with hand-\nwired rules in many scenarios, for the reason that it adapts solutions\nto the data and thus can uncover patterns in the specific problem\ninstance as opposed to solving a general problem for every instance.\nIt is widely agreed [ 4,64] that ML-based methods are especially\nvaluable in cases where expert knowledge of the problem domain\nmay not be sufficient and some algorithmic decisions may not give\nsatisfactory results. Our goal in this section, therefore, is to develop\na machine learning method to optimize ğºğ‘ƒğ‘‚ .\n5.1 Siamese Networks\nConsidering that the goal of optimizing ğºğ‘ƒğ‘‚ is to maximize the\noverall intra-group similarity, we adopt Siamese networks [ 8,20]\nto solve the partitioning task. Siamese networks have been suc-\ncessfully utilized in deep metric learning tasks [ 50,52] in computer\nvision, capturing both intra-class similarity and inter-class discrim-\nination in many challenging tasks including face recognition.\nWe design a Siamese network as shown in Figure 3 to learn the\noptimal partitioning. It consists of a pair of twin networks sharing\nthe same set of parameters working in tandem on two inputs and\ngenerate two comparable outputs.\nFigure 3: Siamese network\nWe useğ‘…ğ‘’ğ‘(ğ‘†ğ‘¥)andğ‘…ğ‘’ğ‘(ğ‘†ğ‘¦)to denote the vector representa-\ntions of two sets ğ‘†ğ‘¥andğ‘†ğ‘¦, a pair of inputs to the twin networks,\nand useğº(ğ‘†ğ‘¥)andğº(ğ‘†ğ‘¦)to represent their respective group as-\nsignment indicted by the outputs of the twin networks respectively.\nFollowing Equation (13) we define the loss function of the Siamese\nnetwork as follows:\nğ‘™ğ‘œğ‘ ğ‘ (ğ‘†ğ‘¥,ğ‘†ğ‘¦)=\u001a(1âˆ’ğ‘†ğ‘–ğ‘š(ğ‘†ğ‘¥,ğ‘†ğ‘¦)),ifğº(ğ‘†ğ‘¥)=ğº(ğ‘†ğ‘¦)\n0, otherwise(15)\nEquation (15) minimizes the intra-group dissimilarities by sum-\nming(1âˆ’ğ‘ ğ‘–ğ‘š(ğ‘†ğ‘¥,ğ‘†ğ‘¦))as the losses, and penalizes imbalanced\ngroups by counting pairwise dissimilarities only between sets in\nthe same group. We use an example to illustrate how Equation (15)\npenalizes imbalanced partitioning. Suppose there are ğ‘sets and\ndissimilarity between any pair of sets is the same at ğ‘‘. The task\n\nis to partition these sets into 2 groups, containing ğ‘1andğ‘2sets\nrespectively ( ğ‘1+ğ‘2=ğ‘). Then the overall loss isğ‘1(ğ‘1âˆ’1)ğ‘‘\n2+\nğ‘2(ğ‘2âˆ’1)ğ‘‘\n2=ğ‘‘\n2[ğ‘1(ğ‘1âˆ’1)+ğ‘2(ğ‘2âˆ’1)]=ğ‘‘\n2(ğ‘2\n1+ğ‘2\n2âˆ’ğ‘)â‰¥\nğ‘‘\n2((ğ‘1+ğ‘2)2\n2âˆ’ğ‘)=ğ‘‘\n2(ğ‘2\n2âˆ’ğ‘), and the bound is tight when\nğ‘1=ğ‘2. Therefore, Equation (15) favors balanced partitioning.\nBy training the Siamese network with sufficient samples drawn\nfromD, theoretically we can minimize the overall distances be-\ntween all pairs of sets which belong to the same group, and thus\nthe Siamese network is expected to give the partitioning result\nin whichğºğ‘ƒğ‘‚ is minimized. Practically, however, we expect to\nachieve near-optimal partitioning only as the network is essentially\nperforming local search.\n5.2 Framework\nAlthough using Siamese networks to solve the optimization problem\nis a promising approach, training such networks turns out to be\ndifficult for the following reasons:\n(1)When dealing with real world data, we may need to partition\nsets into thousands of groups. Therefore, for an input set ğ‘†ğ‘¥,\nthe network needs to make prediction on which group ğ‘†ğ‘¥\nbelongs to, among a collection of thousands of groups. It is\nwell known [ 25] that training networks to tackle prediction\nproblems involving thousands or more labels is challenging.\n(2)What makes this task even more difficult is that unlike a clas-\nsification problem, the label for each input (i.e., the optimal\ngroup) in this optimization problem is unknown, i.e., there is\nno ground truth regarding the labels/groups available. The\nonly information we have is the loss if the two input sets are\nassigned into the same group. This makes the problem even\nmore challenging than typical classification problems.\nThe inherent difficulty of utilizing Siamese networks for this\nproblem is the dimensionality (i.e., degrees of freedom) of the out-\nput space. In response to this challenge, we propose a learning\nframework consisting of a cascade of Siamese models, which parti-\ntions the database in a hierarchical fashion. Each Siamese network\nin the framework is responsible for partitioning a group of sets into\ntwo sub-groups. The framework is illustrated in Figure 4.\nFigure 4: Cascade framework\nAt Level 0 of the framework, we train a Siamese network which\ntakes each set in the entire database Dand assigns it into one of two\ngroups,G1andG2, based on the loss function given in Equation (15).\nThen at Level 1, we train two Siamese networks working on G1\nandG2respectively in the same fashion. Thus, they partition the\nentire database into four groups. We continue adding more levels\nto the cascade framework until all groups are small enough or a\npre-defined threshold on the number of levels is reached. Sinceeach model in the cascade is specialized to partition a group of sets\ninto only two sub-groups the resulting classification problem can\nbe solved effectively.\nThe architecture of the cascade models motivates the use of a\nhierarchical indexing structure, which we call Hierarchical TGM (or\nHTGM). More specifically, assuming the level of the cascade frame-\nwork isğ‘™, and 0â‰¤ğ‘–<ğ‘—<ğ‘™, we construct TGM ğ‘–and TGMğ‘—based on\nthe partitioning results at level ğ‘–and levelğ‘—respectively. Suppose\na group at level ğ‘–, sayGğ‘”, is partitioned into several sub-groups\nat levelğ‘—, saySG 1,Â·Â·Â·,SGğ‘š. If for a query ğ‘„, groupGğ‘”can be\npruned by checking TGM ğ‘–, then all verification operations involv-\ning groupsSG 1,Â·Â·Â·,SGğ‘šcan be eliminated. The construction can\nbe easily generalized to HTGM with â„(â„>1) levels.\n5.3 PTR: a Set Representation Method\nA Siamese network accepts vectors as input and thus the sets in D\ncannot be directly fed into the network. As a result we have to build\na vector representation for each set. Considering the time and space\ncomplexity of existing embedding methods such as Principal Com-\nponent Analysis (PCA) or Multidimensional Scaling (MDS), they\ncan hardly be applied to the target setting introduced in Section\n7 where millions or billions of sets are involved (see comparison\nregarding embedding cost in Section 7.3). Besides, different from\nthe objectives of these general-purpose embedding methods such\nas maximizing variance or preserving distance, our concern is to\nmake sets containing different tokens more separable to benefit\nthe training of the Siamese network. Intuitively, the representa-\ntions that ease the training of the models are expected to bear the\nfollowing property:\nDefinition 5.1. Set Separation-Friendly Property .âˆ€ğ‘¡âˆˆT, let\nGğ‘¡be the collection of sets containing ğ‘¡, andGÂ¬ğ‘¡be the collection\nsets not containing ğ‘¡, thenGğ‘¡andGÂ¬ğ‘¡should to be easily separable\nin the representation space.\nNext we discuss how to construct such representations. As the\nfirst step, we organize all tokens with a balanced binary tree such\nthat tokens appear in leaf nodes and each leaf contains only one\ntoken. The height of the tree is thus â„=âŒˆlog2|T|âŒ‰. We mark the\nedge from a node to its left child with 1 and the edge to its right\nchild with 0. An example of such a tree is depicted in Figure 5.\nFigure 5: Tokens organized with a balanced tree\nWe useğ‘ğ‘ğ‘¡â„ğ‘¡to denote the path from the root to an arbitrary\ntokenğ‘¡. Since each leaf contains only one token, no two tokens\nshare the same path. We build a path table (PT) of all tokens defined\nas follows:\nPT[ğ‘¡,ğ‘–]=\u001ağ‘ğ‘ğ‘¡â„ğ‘¡[ğ‘–], ifğ‘–âˆˆ[1,â„]\n1âˆ’ğ‘ğ‘ğ‘¡â„ğ‘¡[ğ‘–],ifğ‘–âˆˆ[â„+1,2â„](16)\nAn example of PT is provided in Table 1.\nWe propose a method called PTR ( PathTableRepresentation) to\nbuild a representation for a given set ğ‘†as follows:\nğ‘…ğ‘’ğ‘(ğ‘†)[ğ‘–]=âˆ‘ï¸\nğ‘¡âˆˆğ‘†ğ‘ƒğ‘‡[ğ‘¡,ğ‘–], ğ‘–âˆˆ[1,2â„] (17)\n\nTable 1: An example of path table (PT)\nPosition 1234\nA 1100\nB 1001\nC 0110\nD 0011\nFigure 6: Separating sets\nIn the above example, the representation of {ğ´,ğµ,ğ¶}is[2,2,1,1]\nand the representation of {ğµ,ğ·}is[1,0,1,2]. The second half of\nthe path table (Positions 3 and 4) helps to reduce the chance that\ndifferent sets have common representations. For example, if only\nthe first half is used, then the representations of {ğ´},{ğµ,ğ¶},{ğ´,ğ·},\n{ğµ,ğ¶,ğ·}would all be[1,1]. We compare the set representations\nconstructed on the full vs. half path tables in Section 7.3.\nPTR also naturally differentiates multisets containing the same\ncollection of tokens but with different number of occurrences. For\nexample,ğ‘…ğ‘’ğ‘({ğ´})=[1,1,0,0]whileğ‘…ğ‘’ğ‘({ğ´,ğ´})=[2,2,0,0].\nThe basic idea of the representation is to map the sets into a\nnew space, in a way that determining collections of sets containing\nspecific tokens can be easily performed. More specifically, set ğ‘†is\nplaced in the representation space based on the presence or absence\nof all tokens in ğ‘†, and consequently, given a collection of tokens Tğ‘,\nwe can quickly locate all sets containing Tğ‘. This evidently yields\nthe set separation-friendly property. To better illustrate this, we\nreuse the path table in Table 1 and show that sets containing ğµcan\nbe separated from other sets. For better visualization, we project\nthe representation space onto the first two dimensions (Positions\n1 and 2), and keep tokens ğµ,ğ¶, andğ·only in Figure 6. Clearly all\nsets containing token ğµfall into the striped area, defined by the\naxis aligned hyper-plane in the representation space passing from\npoint(1,0)(corresponding to {ğµ}). Similarly all sets containing\nbothğµandğ¶are located at the intersection of the axis aligned\nhyper-planes passing from (1,0)and(0,1)(corresponding to{ğ¶}).\nThat way separating sets in the representation space based on token\nmembership is conducted by determining hyper-plane intersections.\nWe will demonstrate that such a representation is easier to learn\nand yields effective partitions in Section 7.\n6 DEALING WITH UPDATES\nOur discussions so far have assumed that the database Dand the\ntoken universe are fixed. In some cases, however, new sets may\nbe added to the database after the index is built, and previously\nunseen tokens may appear. We therefore study how updates can\nbe handled, with a focus on TGM, as HTGM can be updated level\nby level in the same way.\nWe first discuss the case where new sets are added but the token\nuniverse remains the same. Given a new set ğ‘†, we addğ‘†into thegroupGğ‘”if the similarity upper bound between Gğ‘”andğ‘†is the\nhighest among all groups. When there exist multiple groups with\nthe same highest ğ‘ˆğµ, we insertğ‘†into the group with the minimal\nnumber of sets, in line with the optimization target discussed in\nSection 4. After insertion, we update the TGM accordingly, i.e., for\nall tokensğ‘¡âˆˆğ‘†, we setğ‘€[ğ‘”,ğ‘¡]=1.\nWe now demonstrate how our approach naturally handles previ-\nously unseen tokens. This is an important feature of our solution as\nit is the first to deal with dynamic tokens. All previous attempts to\na solution of this problem assumed a fixed token universe [ 72,73].\nLetğ‘†be a set containing one or more new tokens. We insert ğ‘†into\nthe database in the following two steps:\n(1)Letğ‘ƒğ‘†=ğ‘†âˆ©T be all tokens in ğ‘†that have been seen\npreviously. We find the group with the highest similarity\nupper bound to ğ‘ƒğ‘†, denoted byGğ‘”. Ifğ‘ƒğ‘†=âˆ…, thenGğ‘”is\nsimply the group with the minimal number of sets. ğ‘†is\ninserted toGğ‘”.\n(2)For any token ğ‘¡ğ‘›ğ‘’ğ‘¤âˆˆğ‘†\\ğ‘ƒğ‘†, add a row in ğ‘€corresponding\ntoğ‘¡ğ‘›ğ‘’ğ‘¤. For all tokens ğ‘¡âˆˆğ‘†, setğ‘€[ğ‘”,ğ‘¡]=1.\nAlthough the partitioning in Section 4 is optimized based on existing\nsets and tokens, inserting new sets and tokens will not severely\nimpact the performance of the approach, as we demonstrate in\nSection 7.8.\n7 EXPERIMENTS\nIn this section, we present a thorough experimental evaluation of\nour approach varying parameters of interest, comparing LES3and\nits important components, L2P and PTR, with competing methods.\n7.1 Settings\nEnvironment. We run the experiments on a machine with an In-\ntel(R) Core i7-6700 CPU, 16GB memory and a 500GB, 5400 RPM\nHDD (roughly 80MB/s data read rate). We use HDD for fair com-\nparison as other disk-based methods require no random access of\nthe data (see Section 7.6). However one could expect better perfor-\nmance of LES3when running on SDD as it incurs random access\nof the data by skipping some groups, especially when the number\nof groups is large.\nImplementation. L2P is implemented with PyTorch, embed-\nding methods in Section 7.3 and partitioning methods in Section\n7.4 are implemented with Python, and TGM and the set similar-\nity search baselines are implemented in C++ and compiled using\nGCC 9.3 with -O3 flag. TGM is compressed by Roaring [ 41], a well-\nperformed bitmap compression technique.\nDatasets. KOSARAK [ 2], LIVEJ [ 49], DBLP [ 1], and AOL [ 53]\nare three popular datasets used for set similarity search problems\nand we adapt them for this reason. We also include a social network\ndataset from Friendster[ 70] (denoted by FS), where each user is\ntreated as a set with his/her friends being the tokens; and a dataset\nfrom PubMed Central journal literature [ 33] (denoted by PMC),\nwhere each sentence is treated as a set with the words being the\ntokens3. Table 2 presents a summary of the statistics on these\ndatasets. Considering the size of FS and PMC, we utilize them for\ndisk-based evaluation in Section 7.6 to examine the scalability of\nLES3.\n3with basic data cleaning operations such as stop-words removal.\n\nTable 2: Dataset statistics\nDataset|D|Set size|T|Max Min Avg\nKOSARAK 990,002 2,498 1 8.1 41,270\nLIVEJ 3,201,202 300 1 35.1 7,489,073\nDBLP 5,875,251 462 2 8.7 3,720,067\nAOL 10,154,742 245 1 3.0 3,849,555\nFS 65,608,366 3,615 1 27.5 65,608,366\nPMC 787,220,474 2,597 1 8.8 22,923,401\nEvaluation. Following previous studies [ 14,46,73], we adopt\nJaccard similarity as the metric in our experimental evaluation. We\nstress however that any similarity measures satisfying the TGM\napplicability property introduced in Section 3.2 can be adopted\nin our framework with highly similar results as those reported\nbelow. For each experiment, we randomly select 10K sets in the\ncorresponding dataset as the queries and report the average search\ntime. Unless otherwise specified, the indexing structure (TGM) and\nthe data are memory-resident. We conduct disk-based evaluation\nin Section 7.6. We compare TGM with HTGM in Section 7.7. We\nselectğ‘›(number of groups) for each dataset which results in the\nshortest query latency. The influence of ğ‘›is studied in Section 7.5.\nNetwork and Loss Function. We consider Multi-Layer Per-\nceptron (two hidden layers, each consisting of eight neurons) and\nSigmoid activation function for L2P training in the experiment and\nleave the investigation of other networks as future work. Clearly\nthe network has one neuron at the output layer. Let ğ‘‚ğ‘¥be the out-\nput on input set ğ‘†ğ‘¥. Ifğ‘‚ğ‘¥<0.5, thenğ‘†ğ‘¥belongs to the first group;\nifğ‘‚ğ‘¥â‰¥0.5, thenğ‘†ğ‘¥belongs to the second group.\nThe loss function given in Equation (15) clearly describes the\nlearning objective. However, it is difficult to train the network with\nthat loss function as its gradient is 0 for most outputs (to be exact,\nthe gradient is(1âˆ’ğ‘ ğ‘–ğ‘š(ğ‘†ğ‘¥,ğ‘†ğ‘¦))whenğ‘‚ğ‘¥=ğ‘‚ğ‘¦=0.5, and is 0\nelsewhere). For efficient training, we use the following surrogate\nloss function, which leads to the same global optimum as Equation\n(15) while introducing non-zero gradients:\nğ‘™ğ‘œğ‘ ğ‘ â€²(ğ‘†ğ‘¥,ğ‘†ğ‘¦)=\u001ağ‘Š(ğ‘‚ğ‘¥,ğ‘‚ğ‘¦)(1âˆ’ğ‘†ğ‘–ğ‘š(ğ‘†ğ‘¥,ğ‘†ğ‘¦)),ifğ‘‰(ğ‘‚ğ‘¥,ğ‘‚ğ‘¦);\n0, otherwise;(18)\nwhereğ‘Š(ğ‘‚ğ‘¥,ğ‘‚ğ‘¦)=(0.5âˆ’|ğ‘‚ğ‘¥âˆ’ğ‘‚ğ‘¦|), andğ‘‰(ğ‘‚ğ‘¥,ğ‘‚ğ‘¦)=[(ğ‘‚ğ‘¥â‰¥\n0.5âˆ§ğ‘‚ğ‘¦â‰¥0.5)âˆ¨(ğ‘‚ğ‘¥<0.5âˆ§ğ‘‚ğ‘¦<0.5)].\nInitialization. Models at the first few levels of the Cascade\nframework deal with a large number of sets and incur long training\ntime. To improve the training efficiency, we first sort all sets based\non the minimal token contained in each set, and then partition all\nsets into 128 groups such that each group contains consecutive\n|D|/ 128sets, inspired by the idea of imposing sequential constraint\nto clustering tasks [ 62]. Since we always build TGM on the parti-\ntioning results at level 10 or higher which may contain thousands\nof groups, such initialization has minor impact on the performance\nbut greatly reduces the training time. Note that initialization is not\nperformed for the sampled dataset used in Section 7.3 due to its\nsmall size.\nTraining. For the Siamese network partitioning an arbitrary\ngroup, we randomly select 40,000 pairs of sets in the group to\ngenerate training samples, relatively small compared to the data\nsize. It is observed that further increasing the number of training\nsamples do not significantly improve the pruning efficiency of thepartitioning results. We stop partitioning a group if it contains less\nthan 50 sets, and thus the number of groups at level ğ‘–may be less\nthan 2ğ‘–. The batch size is set to 256, the number of epochs is set to\n3 (except for Section 7.2 which reports the learning curves), and\nAdam is used as the optimizer. The same sampling-and-training\nprocedure is repeated for each model in the cascade framework,\nstarting from level 0.\n7.2 Model Convergence and Training Cost\nIn this section we report the learning curves and the training costs.\nWe observe that different models in the cascade framework intro-\nduced in Section 5.2 yield similar learning curves, and thus we\npresent the training loss of a random model at level 0 for each\ndataset (note that there are 128 models at level 0, see Section 7.1,\nparagraph Initialization). The training losses and costs are presented\nin Figure 7.\nFigure 7: Training losses and costs\nAs is clear from Figure 7(a), on all datasets used in the experiment,\nthe training loss decreases rapidly and the model converges after\napproximately two epochs, attesting to the efficiency of the model\ntraining process. Also, as can be observed from Figure 7(b), the\ntraining cost grows linearly with respect to the number of groups,\nmaking LES3scalable for large datasets. Besides, models at the same\nlevel of the cascade framework can be trained in parallel to further\nreduce the training cost, which is an interesting direction for future\ninvestigation.\n7.3 PTR vs. Set Representation Techniques\nWe compare PTR with other applicable set representation tech-\nniques. More specifically, we choose PCA [ 32], a widely-used linear\nembedding method, MDS [ 12], a representative non-linear embed-\nding approach, and Binary Encoding [ 28], an efficient categorical\ndata embedding technique. We also include the variant of PTR con-\nstructed on the first half of the path table (see Section 5.3), denoted\nby PTR-half. Considering the complexity of PCA and MDS, we con-\nduct experiments on sampled KOSARAK (sample ratio of 5%). We\nreport the representation construction time of each method and the\nquery answering time using the resulting partitioning results for\nğ‘˜NN query (ğ‘˜=10) and range query ( ğ›¿=0.7) in Figure 8; similar\ntrends are observed on other datasets and queries.\nFigure 8: Comparison of representation techniques\n\nAs can be observed from Figure 8, compared with PCA and MDS,\nPTR incurs much lower embedding time (10 to 20,000 times faster)\nwhile results in similar search time; compared with Binary Encoding\nand PTR-half, PTR leads to faster query answering with comparable\nembedding cost. Binary Encoding assigns unique representations\nto different sets without considering set characteristics (e.g., tokens\ncontained therein), and thus can hardly achieve any Set Separation-\nFriendly Property. PTR-half, as discussed in Section 5.3, suffers from\nthe risk that different sets may have common representations, and\nconsequently these (dissimilar) sets are partitioned into the same\ngroup as they are not separable in the representation space, and\nthe resulting search time thus is slightly longer than that of PTR.\nThe major advantage of PTR is that it integrates the Set Separation-\nFriendly Property introduced in Section 5.3 into set representations\nby allowing sets consisting of different tokens to be easily separable\nby axis-aligned hyper-planes in the embedding space, and thus\neases the training of the downstream Siamese networks.\n7.4 L2P vs. Algorithmic Approaches\nWe compare the learning-based partitioning approach, L2P, to the\nalgorithmic methods introduced in Section 4.3, namely the graph\ncut-based method (PAR-G), centroid-based method (PAR-C), di-\nvisive clustering method (PAR-D), and agglomerative clustering\nmethod (PAR-A), in terms of partitioning cost, including time cost\nand space cost, and query answering time.\nFor PAR-G, we adopt PaToH [ 9], a graph partitioning tool known\nto be efficient and performing well, to cut the graph. We report\nthe cost of different methods in partitioning KOSARAK into 1024\ngroups and the query answering time for ğ‘˜NN withğ‘˜=10in\nFigure 9; similar trends are observed on other datasets and queries.\nNote that the partitioning time of L2P includes model training\ntime and inference time (the time required to assign a set into a\ngroup), and the partitioning time of PAR-G consists of the ğ‘˜NN\ngraph construction time and the graph cut time. PAR-G is specially\noptimized for ğ‘˜=10and the construction of its ğ‘˜NN graph is\naccelerated by LES3.\nFigure 9: Comparison of partitioning methods\nAs depicted in Figure 9, L2P provides the fastest search while\nonly incurs a small fraction of partitioning time and space cost\ncompared to competitors (saving 80%partitioning time and 99%\nspace compared with PAR-G). The reason why L2P incurs less\npartitioning time and space overhead is that, as described in Section\n5.1 and Section 7.1, by training the models on a small portion of data,\nL2P is better positioned to approach the optimal partitioning where\ntheğºğ‘ƒğ‘‚ is minimized, while other techniques work on the entire\ndataset and require (sometimes repetitively) computing the ğºğ‘ƒğ‘‚ of\narbitrary groups (or pairs) of sets. Besides, only model parameters\nand the training samples in a mini batch have to be saved in memory\nfor L2P, with minimal storage overhead, while other techniquesrequire materializing a large amount of intermediate partitioning\nresults (and the entire ğ‘˜NN graph for PAR-G) in memory, incurring\nprohibitive space consumption.\nBy directly optimizing the ğºğ‘ƒğ‘‚ which integrates the two de-\nsired properties of a partitioning with higher pruning efficiency,\nL2P is able to outperform PAR-G, the objective of which is minimiz-\ning the number of edges in the similarity graph crossing different\nsub-graphs rather than ğºğ‘ƒğ‘‚ . PAR-C, PAR-D, and PAR-A, although\nalso aim to optimize the ğºğ‘ƒğ‘‚ , suffer from severe local optimality\nproblems: a set is moved to a group only if such movement reduces\nthe overallğºğ‘ƒğ‘‚ , while in many cases movements temporarily in-\ncreasing the ğºğ‘ƒğ‘‚ must be allowed to determine a global optimum.\nFor example, let ğ‘†ğ‘–âˆˆGğ‘–,ğ‘†ğ‘—âˆˆGğ‘—be two sets. Assume that moving\nğ‘†ğ‘–toGğ‘—and moving ğ‘†ğ‘—toGğ‘–when individually carried out both\nincrease the ğºğ‘ƒğ‘‚ , and consequently ğ‘†ğ‘–remains inGğ‘—andğ‘†ğ‘—in\nGğ‘–. However, swapping ğ‘†ğ‘–andğ‘†ğ‘—may reduce the overall ğºğ‘ƒğ‘‚ and\nthus leads to better partitioning. Such swapping cannot be achieved\nbased on the strategy followed by PAR-C and PAR-D. Similarly, the\nstrategy of PAR-A does not allow the merge of groups temporarily\nincreasing the overall ğºğ‘ƒğ‘‚ , which however may be necessary in\nidentifying a global optimum.\n7.5 Sensitivity to Number of Groups and ğ‘˜\nWe test the performance of LES3in terms of query processing time\nonğ‘˜NN queries, varying the number of groups ğ‘›and the result size\nğ‘˜. The results are presented in Figure 10.\nFigure 10: Sensitivity to the number of groups and result size\nIncreasingğ‘›accelerates query answering, regardless of the result\nsize. This is because with more groups, as indicated by Equation (13),\nthe overall pruning efficiency of LES3can be improved, meaning\nfewer candidates have to be checked. Increasing ğ‘›benefits search\ntime up to a point. In particular we observe a diminishing return\nbehavior with respect to search performance as ğ‘›increases fur-\nther. The reason is that, with a sufficiently large number of groups,\nsets are already well-separated, and further increasing ğ‘›brings\nno significant change to the pruning efficiency but incurs higher\nindex (TGM) scan cost. Moreover, search time increases for larger ğ‘˜,\nwhich is consistent with our analysis in Equation ( ??), as in general\na largerğ‘˜inğ‘˜NN search is analogous to a smaller ğ›¿in range search\nand thus the pruning efficiency is lower.\nWhile determining the optimal number of groups for partitioning\nis a known NP-hard problem [ 31], we empirically observe from the\nexperiments that setting the number of groups at approximately\n0.5%|D|leads to the lowest search time, where |D|is the number\nof sets in the corresponding dataset.\n7.6 LES3vs. Set Similarity Search Baselines\nIn this section, we compare LES3with existing set similarity search\napproaches to answering ğ‘˜NN and range queries in memory-based\n\nFigure 11: Index size and construction time\nand disk-based settings respectively. Among tree-based set similar-\nity search approaches to date, DualTrans [ 73] provides the fastest\nquery processing. For inverted index-based methods, we adopt\nthe method proposed in [ 67] (denoted by InvIdx), which yields the\nstate-of-the-art performance for set similarity join tasks. Note that\nwe exclude methods requiring index construction during query\ntime [ 13,14,69] as the index construction cost is much higher than\nthe query cost. Since inverted index-based methods are designed\nfor range queries and do not naturally support ğ‘˜NN queries, we\nmodify the query answering algorithm of InvIdx for ğ‘˜NN queries\nas follows. (1) Given a query set ğ‘„and a result size ğ‘˜, start with\nthresholdğ›¿=1.0and use InvIdx to find candidate sets from D\nwhose similarity with ğ‘„exceedsğ›¿, denoted byC. (2) Identify the\ntemporaryğ‘˜NN results fromC, denoted byRğ‘˜. If the minimal simi-\nlarity between any set in Rğ‘˜andğ‘„exceedsğ›¿, terminate. Otherwise,\ndecreaseğ›¿byğ‘§, use InvIdx to find candidates sets with the new ğ›¿,\nupdateCaccordingly, and repeat the step. (3) Upon termination,\nRğ‘˜is guaranteed to be the ğ‘˜NN toğ‘„, as the similarity between any\nsets inD\\C andğ‘„does not exceed the current ğ›¿. The value of ğ‘§\nis tuned for faster query answering.\nIn addition, we also include a brute-force approach, i.e., comput-\ning the similarity between the query set and all other sets to derive\nthe results, for completeness of comparison.\nIn Figure 11, we show the index size and index construction\ntime for all methods. It is clear that the indexing structure of LES3,\nnamely TGM, is much more lightweight, requiring up to 90%less\nspace than DualTrans and InvIdx. The major time cost of con-\nstructing TGM comes from the model training, which however is\na preprocessing step incurring only a one-time cost and can be\nfurther reduced as discussed in Section 7.2.\nIn Figure 12 we compare the performance of the four methods in\na memory-based setting. We observe that LES3outperforms com-\npetitors for both ğ‘˜NN queries and range queries, accelerating the\nquery answering by 2 to 20 times. DualTrans incurs longer search\ntime as it uses an R-tree to organize all sets, with each set being\nrepresented with a ğ‘‘-dimensional vector ( ğ‘‘can be tuned for faster\npruning). When the value of ğ‘‘is small, sets containing different\ntokens cannot be clearly separated based on their representations,\nwhile when the value of ğ‘‘is large, using R-tree to organize the\nvectors incurs high overlap between the bounding boxes of nodes\non the R-tree, as previous research indicates [ 30]. Besides, scan-\nning the R-tree is expensive, which is not worthwhile considering\nthat set similarity (e.g., Jaccard similarity) can usually be computed\nefficiently. While InvIdx provides comparable performance with\nLES3for range queries with large ğ›¿, it incurs greater search latency\nforğ‘˜NN queries, especially when the average set size is large (e.g.,\non KOSARAK and LIVEJ). The reason is that, with InvIdx filteringoperations need to be repeated for each candidate set (or multiple\ncandidates with some common characteristics), and larger set size\nandğ‘˜NN queries both enlarge the number of candidates, leading to\nsub-par query performance.\nIn contrast, we use TGM to compute the upper bounds between\na query set and a group of sets; obtaining all bounds requires only\nğ‘‚(|ğ‘†|âˆ—|G|) time, which is relatively cheap. Although the search\ntime of LES3increases for range query as ğ›¿decreases, LES3provide\nmuch faster query answering under a wide range of ğ›¿.\nWe compare the performance of the four methods in the disk-\nbased setting in Figure 13. Note that for DualTrans and InvIdx,\nonly the part of the index that is necessary to the query answering,\nsuch as R-nodes on the search path and inverted indexes related\nto the query set, is retrieved into memory to reduce I/O cost. We\nobserve that LES3generally provides faster search compared with\ncompetitors, accelerating the query answering by 2 to 10 times. The\nreasons why LES3incurs lower search time are: (1) Sets sharing\nno or very few common tokens with the query set can be easily\npruned without being retrieved into memory; and (2) Since sets in\nthe same group are checked jointly during the searching process;\nmaterializing a group of sets continuously on disk minimizes the\ndata transfer delay. DualTrans and InvIdx, on the contrary, incur\nlonger search latency and are outperformed by the Brute-force\nmethod for a wide range of ğ‘˜andğ›¿. Besides the drawbacks dis-\ncussed above in the memory-based setting, the search strategies of\nDualTrans and InvIdx incur repetitive retrieval of data with random\ndisk access, which results in higher I/O cost (more pages retrieved,\nhigher seek and rotation overhead, etc.), making them less efficient\nin the disk-based setting.\n7.7 TGM vs. HTGM\nWe evaluate the performance of TGM and HTGM to determine\nwhether building a hierarchical index pays off. Intuitively, whether\nit benefits the query processing largely depends on the similarity\ndistribution. For example, in cases where very few sets share com-\nmon tokens, one can prune a large number of candidates using the\nmatrices at the first few levels of HTGM, avoiding scanning the\nlarger matrices at finer levels. However, in cases where most sets\nare similar, the small matrices at the first few levels of HTGM may\nprovide no pruning efficiency at all. We assume that the similarity\nbetween sets inDcan be modeled by a power-law distribution\nğ‘ƒ[ğ‘ ğ‘–ğ‘š=ğ‘£]âˆ¼ğ‘£âˆ’ğ›¼, whereğ‘ƒ[ğ‘ ğ‘–ğ‘š=ğ‘£]denotes the probability that\nthe similarity between any two sets is ğ‘£,ğ‘£âˆˆ[0,1],ğ›¼âˆˆ[1,âˆ). We\ngenerate multiple synthetic databases consisting of 20,000 sets and\n20,000 tokens each, by varying the value of ğ›¼. We train a cascade\nmodel with 9 levels (including level 0). We use the partitioning\nresults at level 8 (256 groups) to build the TGM, and use the parti-\ntioning results at level 5 (32 groups) and level 8 to build the HTGM.\nWe compare HTGM and TGM from two aspects. First, the index\naccess cost, measured by the number of columns in the HTGM\nor TGM that are checked when processing the query. Second, the\ncomputational cost, measured by the number of similarity calcu-\nlations. We measure the ratio of cost between HTGM and TGM,\nand the results are shown in Figure 14. It is evident that HTGM\noutperforms TGM when the value of ğ›¼is large, i.e., most sets are\ndissimilar. This is in line with the discussions in Section 7.7.\n\nFigure 12: Comparison to baselines in memory-based settings for range queries (left) and ğ‘˜NN queries (right)\nFigure 13: Comparison to baselines in disk-based settings for\nrange queries (left) and ğ‘˜NN queries (right)\nFigure 14: TGM vs. HTGM\n Figure 15: Handling updates\n7.8 Handling Updates\nWe evaluate the performance of the proposed approach under up-\ndates. Two cases are considered: (1) closed universe , meaning the\nnew sets to be inserted contain only tokens from the original data-\nbase, and (2) open universe , where the new sets may contain previ-\nously unseen tokens. Let Dbe the original database, Dğ‘ğ‘™ğ‘œğ‘ ğ‘’ğ‘‘be the\ncollection of new sets to be inserted under a closed universe, and\nDğ‘œğ‘ğ‘’ğ‘›be the collection of new sets to be inserted under an open\nuniverses. For the experiment, we set insertion ratio ( |Dğ‘ğ‘™ğ‘œğ‘ ğ‘’ğ‘‘|/|D|\nand|Dğ‘œğ‘ğ‘’ğ‘›|/|D| ) in range[0,1], and half of the tokens in Dğ‘œğ‘ğ‘’ğ‘›\nare fromDand half are new. We compute the decrease in prun-\ning efficiency after insertion compared to obtaining a partitioning\nfrom scratch (namely running L2P) on DâˆªDğ‘ğ‘™ğ‘œğ‘ ğ‘’ğ‘‘orDâˆªDğ‘œğ‘ğ‘’ğ‘›\n(referred to as re-build ). We give the results for ğ‘˜NN query with\nğ‘˜=10on KOSARAK in Figure 15; the experiments on the other\ndatasets show similar trends.\nFigure 15 depicts the percentage of pe reduction compared to\nre-build. The pruning efficiency decreases slightly as more new sets\nare inserted into the database. Insertions under an open universe\nhave a higher impact on performance. The reason is that the tokens\nfrom the same universe mainly follow a similar distribution and\nthe partition results obtained on the original data are still sufficient,\nwhile there is no prior knowledge regarding the distribution of new\ntokens. We observe, however, that the overall pruning efficiency\nis resistant to insertions (experiencing a decrease by at most 8%),\nwhich attests to the robustness of the proposed approach.\n8 RELATED WORK\nThe problem of processing set similarity queries, including set sim-\nilarity search [ 36,72,73] and set similarity joins [ 14,15,21,46,67],\nhas attracted remarkable research interest recently. Zhang et al.\n[72,73] propose to transform sets into scalars or vectors with the\nrelative distance between sets preserved, and organize the trans-\nformed sets with B+-trees or R-trees, which facilitate the use oftree-based branch-and-bound algorithms for similarity search. The\nmajor drawback of their work is that, as shown in the experiments,\nthe tree structure can easily grow larger than the original data\nand thus using the index for filtering incurs a significant cost, es-\npecially when the index and the data are stored externally. Most\nprior research in the area of set similarity join focuses on threshold-\njoin queries and follows the filter-and-verify framework. In the\nfilter step, existing methods mainly adopt (1) prefix-based filters\n[7,65,69], based on the observation that if the similarity between\ntwo sets exceeds ğ›¿, then they must share common token(s) in their\nprefixes of length ğ‘š; and (2) partition-based filters [ 3,13,14,68],\nwhich partition a set into several subsets so that two sets are similar\nonly if they share a common subset. Set similarity queries in dis-\ntributed environments [ 11,47] and approximate queries [ 60,61] are\nbeyond the scope of this paper but represent promising directions\nfor further investigation.\nIndexing is an important and well-studied problem in data man-\nagement and recent works have utilized machine learning to learn a\nCDF or to partition the data space for traditional database indexing\n[17,19,40,42,43,51,56,71]. In this paper, we complement recent\nwork by studying the applicability of machine learning techniques\nto assist index construction for set similarity search problems. Our\nresults show that the proposed methods offer vast advantages over\ntraditional techniques.\nEmbedding sets and other entities consisting of discrete elements\nhas been well-studied. The most natural way to represent such data\ntypes isğ‘›-hot encoding, but the resulting vectors are often very\nlong and sparse. Dimensionality reduction techniques are used to\ncompress the encoding vectors with different focuses: maximiz-\ning variances [ 54], preserving distances [ 6], solving the crowding\nproblem [ 45], etc. Recent advances in document embedding, e.g.,\nword2vec [ 48], BERT [ 16], also provide new perspectives to con-\nstruct representations of sets. Compared to these methods, the PTR\nproposed in Section 5.3 utilizes a very efficient method to produce\nrelatively short representations and is optimized for the specific\nproblem at hand.\n9 CONCLUSIONS\nIn this paper, we have studied the problem of exact set similarity\nsearch , and designed LES3, a filter-and-verify approach for effi-\ncient query processing. Central to our proposal is TGM, a simple\nyet effective structure that strikes a balance between index access\ncost and effectiveness in pruning candidate sets. We have revealed\nthe desired properties of optimal partitioning in terms of pruning\nefficiency under the uniform token distribution assumption. We de-\nvelop a learning-based approach, L2P, utilizing a cascade of Siamese\nnetworks to identify partitions. A novel set representation method,\nPTR, is developed to cater to the requirements of network train-\ning. The experimental results have demonstrated the superiority of\nLES3over other applicable approaches.\n\nREFERENCES\n[1] DBLP. http://dblp.uni-trier.de/\n[2] KOSARAK. http://fimi.uantwerpen.be/data/\n[3]Arvind Arasu, Venkatesh Ganti, and Raghav Kaushik. 2006. Efficient exact set-\nsimilarity joins. In Proceedings of the 32nd international conference on Very large\ndata bases . VLDB Endowment, 918â€“929.\n[4]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural ma-\nchine translation by jointly learning to align and translate. arXiv preprint\narXiv:1409.0473 (2014).\n[5]Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. 2020. Machine learning for\ncombinatorial optimization: A methodological tour dâ€™horizon. European Journal\nof Operational Research (2020).\n[6]Ingwer Borg and Patrick Groenen. 2003. Modern multidimensional scaling:\nTheory and applications. Journal of Educational Measurement 40, 3 (2003), 277â€“\n280.\n[7]Panagiotis Bouros, Shen Ge, and Nikos Mamoulis. 2012. Spatio-textual similarity\njoins. Proceedings of the VLDB Endowment 6, 1 (2012), 1â€“12.\n[8]Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard SÃ¤ckinger, and Roopak Shah.\n1994. Signature verification using a\" siamese\" time delay neural network. In\nAdvances in neural information processing systems . 737â€“744.\n[9]Ãœmit V Ã‡atalyÃ¼rek and Cevdet Aykanat. 2011. Patoh (partitioning tool for\nhypergraphs). In Encyclopedia of Parallel Computing . Springer, 1479â€“1487.\n[10] Stephen A Cook. 1971. The complexity of theorem-proving procedures. In\nProceedings of the third annual ACM symposium on Theory of computing . 151â€“\n158.\n[11] Akash Das Sarma, Yeye He, and Surajit Chaudhuri. 2014. Clusterjoin: A similarity\njoins framework using map-reduce. Proceedings of the VLDB Endowment 7, 12\n(2014), 1059â€“1070.\n[12] Vin De Silva and Joshua B Tenenbaum. 2004. Sparse multidimensional scaling\nusing landmark points . Technical Report. Technical report, Stanford University.\n[13] Dong Deng, Guoliang Li, He Wen, and Jianhua Feng. 2015. An efficient partition\nbased method for exact set similarity joins. Proceedings of the VLDB Endowment\n9, 4 (2015), 360â€“371.\n[14] Dong Deng, Yufei Tao, and Guoliang Li. 2018. Overlap set similarity joins with\ntheoretical guarantees. In Proceedings of the 2018 International Conference on\nManagement of Data . 905â€“920.\n[15] Dong Deng, Chengcheng Yang, Shuo Shang, Fan Zhu, Li Liu, and Ling Shao. 2019.\nLCJoin: set containment join via list crosscutting. In 2019 IEEE 35th International\nConference on Data Engineering (ICDE) . IEEE, 362â€“373.\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding. arXiv\npreprint arXiv:1810.04805 (2018).\n[17] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\net al.2020. ALEX: an updatable adaptive learned index. In Proceedings of the 2020\nACM SIGMOD International Conference on Management of Data . 969â€“984.\n[18] Xin Luna Dong and Theodoros Rekatsinas. 2018. Data integration and machine\nlearning: A natural synergy. In Proceedings of the 2018 International Conference\non Management of Data . 1645â€“1650.\n[19] Yihe Dong, Piotr Indyk, Ilya Razenshteyn, and Tal Wagner. 2020. Learning Space\nPartitions for Nearest Neighbor Search. In International Conference on Learning\nRepresentations . https://openreview.net/forum?id=rkenmREFDr\n[20] Jingfan Fan, Xiaohuan Cao, Zhong Xue, Pew-Thian Yap, and Dinggang Shen. 2018.\nAdversarial similarity network for evaluating image alignment in deep learning\nbased registration. In International Conference on Medical Image Computing and\nComputer-Assisted Intervention . Springer, 739â€“746.\n[21] Raul Castro Fernandez, Jisoo Min, Demitri Nava, and Samuel Madden. 2019.\nLazo: A cardinality-based method for coupled estimation of jaccard similarity\nand containment. In 2019 IEEE 35th International Conference on Data Engineering\n(ICDE) . IEEE, 1190â€“1201.\n[22] Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and Tim\nKraska. 2019. Fiting-tree: A data-aware index structure. In Proceedings of the 2019\nInternational Conference on Management of Data . 1189â€“1206.\n[23] Chang Ge, Yinan Li, Eric Eilebrecht, Badrish Chandramouli, and Donald Koss-\nmann. 2019. Speculative distributed CSV data parsing for big data analytics. In\nProceedings of the 2019 International Conference on Management of Data . 883â€“899.\n[24] Lars GottesbÃ¼ren, Michael Hamann, Sebastian Schlag, and Dorothea Wagner. 2020.\nAdvanced Flow-Based Multilevel Hypergraph Partitioning. In 18th International\nSymposium on Experimental Algorithms, SEA 2020, June 16-18, 2020, Catania, Italy\n(LIPIcs) , Simone Faro and Domenico Cantone (Eds.), Vol. 160. Schloss Dagstuhl -\nLeibniz-Zentrum fÃ¼r Informatik, 11:1â€“11:15. https://doi.org/10.4230/LIPIcs.SEA.\n2020.11\n[25] Maya R Gupta, Samy Bengio, and Jason Weston. 2014. Training highly multiclass\nclassifiers. The Journal of Machine Learning Research 15, 1 (2014), 1461â€“1492.\n[26] Antonin Guttman. 1984. R-trees: A dynamic index structure for spatial searching.\nInProceedings of the 1984 ACM SIGMOD international conference on Management\nof data . 47â€“57.[27] Marios Hadjieleftheriou, Xiaohui Yu, Nick Koudas, and Divesh Srivastava. 2008.\nHashed samples: selectivity estimators for set similarity selection queries. Pro-\nceedings of the VLDB Endowment 1, 1 (2008), 201â€“212.\n[28] Jiawei Han, Micheline Kamber, and Jian Pei. 2011. Data mining concepts and\ntechniques third edition. The Morgan Kaufmann Series in Data Management\nSystems 5, 4 (2011), 83â€“124.\n[29] John A Hartigan and Manchek A Wong. 1979. Algorithm AS 136: A k-means\nclustering algorithm. Journal of the Royal Statistical Society. Series C (Applied\nStatistics) 28, 1 (1979), 100â€“108.\n[30] Piotr Indyk and Rajeev Motwani. 1998. Approximate nearest neighbors: towards\nremoving the curse of dimensionality. In Proceedings of the thirtieth annual ACM\nsymposium on Theory of computing . 604â€“613.\n[31] Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An\nintroduction to statistical learning . Vol. 112. Springer.\n[32] Ian T Jolliffe and Jorge Cadima. 2016. Principal component analysis: a review\nand recent developments. Philosophical Transactions of the Royal Society A:\nMathematical, Physical and Engineering Sciences 374, 2065 (2016), 20150202.\n[33] PubMed Central Journal. .. https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/.\n[34] George Karypis, Rajat Aggarwal, Vipin Kumar, and Shashi Shekhar. 1999. Multi-\nlevel hypergraph partitioning: applications in VLSI domain. IEEE Transactions\non Very Large Scale Integration (VLSI) Systems 7, 1 (1999), 69â€“79.\n[35] Leonard Kaufman and Peter J Rousseeuw. 2009. Finding groups in data: an\nintroduction to cluster analysis . Vol. 344. John Wiley & Sons.\n[36] Jongik Kim and Hongrae Lee. 2012. Efficient exact similarity searches using\nmultiple token orderings. In 2012 IEEE 28th International Conference on Data\nEngineering . IEEE, 822â€“833.\n[37] Sungyeon Kim, Minkyo Seo, Ivan Laptev, Minsu Cho, and Suha Kwak. 2019. Deep\nmetric learning beyond binary supervision. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition . 2288â€“2297.\n[38] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe case for learned index structures. In Proceedings of the 2018 International\nConference on Management of Data . 489â€“504.\n[39] Ani Kristo, Kapil Vaidya, Ugur Ã‡etintemel, Sanchit Misra, and Tim Kraska. 2020.\nThe Case for a Learned Sorting Algorithm. In Proceedings of the 2020 ACM SIG-\nMOD International Conference on Management of Data . 1001â€“1016.\n[40] Harald Lang, Alexander Beischl, Viktor Leis, Peter Boncz, Thomas Neumann,\nand Alfons Kemper. 2020. Tree-Encoded Bitmaps. In Proceedings of the 2020 ACM\nSIGMOD International Conference on Management of Data . 937â€“967.\n[41] Daniel Lemire, Owen Kaser, Nathan Kurz, Luca Deri, Chris Oâ€™Hara, FranÃ§ois\nSaint-Jacques, and Gregory Ssi-Yan-Kai. 2018. Roaring bitmaps: Implementation\nof an optimized software library. Software: Practice and Experience 48, 4 (2018),\n867â€“895.\n[42] Linwei Li, Kai Zhang, Jiading Guo, Wen He, Zhenying He, Yinan Jing, Weili Han,\nand X Sean Wang. 2020. BinDex: A Two-Layered Index for Fast and Robust Scans.\nInProceedings of the 2020 ACM SIGMOD International Conference on Management\nof Data . 909â€“923.\n[43] Pengfei Li, Hua Lu, Qian Zheng, Long Yang, and Gang Pan. 2020. LISA: A\nLearned Index Structure for Spatial Data. In Proceedings of the 2020 ACM SIGMOD\nInternational Conference on Management of Data . 2119â€“2133.\n[44] Yifan Li, Xiaohui Yu, and Nick Koudas. 2019. Top-k queries over digital traces. In\nProceedings of the 2019 International Conference on Management of Data . 954â€“971.\n[45] Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.\nJournal of machine learning research 9, Nov (2008), 2579â€“2605.\n[46] Willi Mann, Nikolaus Augsten, and Panagiotis Bouros. 2016. An empirical evalu-\nation of set similarity join techniques. Proceedings of the VLDB Endowment 9, 9\n(2016), 636â€“647.\n[47] Ahmed Metwally and Christos Faloutsos. 2012. V-smart-join: A scalable mapre-\nduce framework for all-pair similarity joins of multisets and vectors. arXiv\npreprint arXiv:1204.6077 (2012).\n[48] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient\nestimation of word representations in vector space. arXiv preprint arXiv:1301.3781\n(2013).\n[49] Alan Mislove, Massimiliano Marcon, Krishna P. Gummadi, Peter Druschel, and\nBobby Bhattacharjee. 2007. Measurement and Analysis of Online Social Networks.\nInProceedings of the 5th ACM/Usenix Internet Measurement Conference (IMCâ€™07) .\nSan Diego, CA.\n[50] Jonas Mueller and Aditya Thyagarajan. 2016. Siamese recurrent architectures for\nlearning sentence similarity. In thirtieth AAAI conference on artificial intelligence .\n[51] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020. Learn-\ning Multi-dimensional Indexes. In Proceedings of the 2020 ACM SIGMOD Interna-\ntional Conference on Management of Data . 985â€“1000.\n[52] Paul Neculoiu, Maarten Versteegh, and Mihai Rotaru. 2016. Learning text sim-\nilarity with siamese recurrent networks. In Proceedings of the 1st Workshop on\nRepresentation Learning for NLP . 148â€“157.\n[53] Greg Pass, Abdur Chowdhury, and Cayley Torgeson. 2006. A picture of search.\nInProceedings of the 1st international conference on Scalable information systems .\n1â€“es.\n[54] Karl Pearson. 1901. LIII. On lines and planes of closest fit to systems of points in\nspace. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of\n\nScience 2, 11 (1901), 559â€“572.\n[55] Lior Rokach and Oded Maimon. 2005. Clustering methods. In Data mining and\nknowledge discovery handbook . Springer, 321â€“352.\n[56] Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, and HervÃ© JÃ©gou. 2019.\nSpreading vectors for similarity search. In 7th International Conference on Learning\nRepresentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net.\nhttps://openreview.net/forum?id=SkGuG2R5tm\n[57] Mehran Sahami and Timothy D Heilman. 2006. A web-based kernel function\nfor measuring the similarity of short text snippets. In Proceedings of the 15th\ninternational conference on World Wide Web . 377â€“386.\n[58] David Salomon. 2004. Data compression: the complete reference . Springer Science\n& Business Media.\n[59] David Salomon and Giovanni Motta. 2010. Handbook of data compression . Springer\nScience & Business Media.\n[60] Venu Satuluri and Srinivasan Parthasarathy. 2011. Bayesian locality sensitive\nhashing for fast similarity search. arXiv preprint arXiv:1110.1328 (2011).\n[61] Sebastian Schelter and JÃ©rÃ´me Kunegis. 2016. Tracking the trackers: A large-scale\nanalysis of embedded web trackers. In Tenth International AAAI Conference on\nWeb and Social Media .\n[62] Tibor Szkaliczki. 2016. clustering. sc. dp: Optimal clustering with sequential\nconstraint by using dynamic programming. R JOURNAL 8, 1 (2016), 318â€“327.\n[63] Matus Telgarsky and Andrea Vattani. 2010. Hartiganâ€™s method: k-means cluster-\ning without voronoi. In Proceedings of the Thirteenth International Conference on\nArtificial Intelligence and Statistics . 820â€“827.\n[64] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In\nAdvances in neural information processing systems . 2692â€“2700.\n[65] Jiannan Wang, Guoliang Li, and Jianhua Feng. 2012. Can we beat the prefix\nfiltering? An adaptive framework for similarity join and search. In Proceedings ofthe 2012 ACM SIGMOD International Conference on Management of Data . 85â€“96.\n[66] Pei Wang and Yeye He. 2019. Uni-Detect: A Unified Approach to Automated\nError Detection in Tables. In Proceedings of the 2019 International Conference on\nManagement of Data . 811â€“828.\n[67] Xubo Wang, Lu Qin, Xuemin Lin, Ying Zhang, and Lijun Chang. 2019. Leveraging\nset relations in exact and dynamic set similarity join. The VLDB Journal 28, 2\n(2019), 267â€“292.\n[68] Chuan Xiao, Wei Wang, Xuemin Lin, and Haichuan Shang. 2009. Top-k set\nsimilarity joins. , 916â€“927 pages.\n[69] Chuan Xiao, Wei Wang, Xuemin Lin, Jeffrey Xu Yu, and Guoren Wang. 2011.\nEfficient similarity joins for near-duplicate detection. ACM Transactions on\nDatabase Systems (TODS) 36, 3 (2011), 1â€“41.\n[70] Jaewon Yang and Jure Leskovec. 2015. Defining and evaluating network commu-\nnities based on ground-truth. Knowledge and Information Systems 42, 1 (2015),\n181â€“213.\n[71] Cong Yue, Zhongle Xie, Meihui Zhang, Gang Chen, Beng Chin Ooi, Sheng Wang,\nand Xiaokui Xiao. 2020. Analysis of Indexing Structures for Immutable Data. In\nProceedings of the 2020 ACM SIGMOD International Conference on Management of\nData . 925â€“935.\n[72] Yong Zhang, Xiuxing Li, Jin Wang, Ying Zhang, Chunxiao Xing, and Xiaojie\nYuan. 2017. An efficient framework for exact set similarity search using tree\nstructure indexes. In 2017 IEEE 33rd International Conference on Data Engineering\n(ICDE) . IEEE, 759â€“770.\n[73] Yong Zhang, Jiacheng Wu, Jin Wang, and Chunxiao Xing. 2020. A Transformation-\nBased Framework for KNN Set Similarity Search. IEEE Trans. Knowl. Data Eng.\n32, 3 (2020), 409â€“423.",
  "textLength": 82968
}