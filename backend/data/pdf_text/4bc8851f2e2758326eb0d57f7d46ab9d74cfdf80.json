{
  "paperId": "4bc8851f2e2758326eb0d57f7d46ab9d74cfdf80",
  "title": "How BPE Affects Memorization in Transformers",
  "pdfPath": "4bc8851f2e2758326eb0d57f7d46ab9d74cfdf80.pdf",
  "text": "Preprint. Work in progress.\nHOWBPE A FFECTS MEMORIZATION IN\nTRANSFORMERS\nEugene Kharitonov\nFacebook AI\nkharitonov@fb.comMarco Baroni\nFacebook AI & ICREA\nmbaroni@fb.comDieuwke Hupkes\nFacebook AI\ndieuwkehupkes@fb.com\nABSTRACT\nTraining data memorization in NLP can both be beneﬁcial (e.g., closed-book QA)\nand undesirable (personal data extraction). In any case, successful model training\nrequires a non-trivial amount of memorization to store word spellings, various\nlinguistic idiosyncrasies and common knowledge. However, little is known about\nwhat affects the memorization behavior of NLP models, as the ﬁeld tends to focus\non the equally important question of generalization.\nIn this work, we demonstrate that the size of the subword vocabulary learned by\nByte-Pair Encoding (BPE) greatly affects both ability and tendency of standard\nTransformer models to memorize training data, even when we control for the\nnumber of learned parameters. We ﬁnd that with a large subword vocabulary size,\nTransformer models ﬁt random mappings more easily and are more vulnerable\nto membership inference attacks. Similarly, given a prompt, Transformer-based\nlanguage models with large subword vocabularies reproduce the training data more\noften. We conjecture this effect is caused by reduction in the sequences’ length that\nhappens as the BPE vocabulary grows. Our ﬁndings can allow a more informed\nchoice of hyper-parameters, that is better tailored for a particular use-case.\n1 I NTRODUCTION\nThe Transformer architecture (Vaswani et al., 2017) became the backbone of the state-of-the-art\nmodels in a variety of tasks (Liu et al., 2019; Raffel et al., 2019; Adiwardana et al., 2020; Brown et al.,\n2020). This spurred a signiﬁcant interest in better understanding inner workings of these models (Vig\nand Belinkov, 2019; Clark et al., 2019; Kharitonov and Chaabouni, 2020; Hahn, 2020; Movva and\nZhao, 2020; Chaabouni et al., 2021; Merrill et al., 2021; Sinha et al., 2021). Most of these works\nhave focussed speciﬁcally on how models generalize and capture structure across samples that are\nsimilar. For instance, Vig and Belinkov (2019) focussed on how attention align with speciﬁc syntactic\ndependency relations, Hupkes et al. (2020) considered if Transformers generalize compositionally\nand Kharitonov and Chaabouni (2020) studied how different models generalize from very few data.\nIn contrast to these studies, we focus on factors that control the training data memorization behavior\nof Transformers, which we believe to be important for several reasons.\nFirst, large Transformer models are increasingly often used as a storage , for instance, as a general-\npurpose knowledge base or as a closed-book question-answering system (Petroni et al., 2019; Roberts\net al., 2020; Lewis et al., 2020). Clearly, the ability to memorize factual knowledge from the training\ndata is crucial for such applications. There are even Transformers models that are explicitly endowed\nwith an external training data memorization mechanism (Khandelwal et al., 2020; 2021; He et al.,\n2021), demonstrating that further boosting their memorization abilities is beneﬁcial.\nSecond, in contrast, the same ability to memorize can become undesirable and lead to a leakage of\npersonal data from trained models (Carlini et al., 2020; Thakkar et al., 2021). A better understanding\nof the phenomenon is thus instrumental both to enable better memorization when it is needed and to\navoid it when not.\nThird, while generalization and memorization are often thought of as competing modes of ﬁtting data,\ntraining effective models in real tasks requires a non-trivial combination of the two. For instance,\nsuccessful language models need to generalize to be able to deal with never-seen-before sentences,\n1arXiv:2110.02782v2  [cs.CL]  2 Dec 2021\n\nPreprint. Work in progress.\nbut they also need to memorize the spelling of words, the non-compositional meaning of idioms,\nidiosyncrasies of languages, common knowledge, etc (see, e.g. Dankers et al., 2021).1\nDespite this apparent importance, there is very little research into memorization in Transformers\nand in NLP models in general, and we have only superﬁcial understanding of what factors affect\nthis behavior. Intuitively, the number of parameters, data augmentation, and regularization are\nlikely to affect how successful models are in memorization (Zhang et al., 2016; Sablayrolles et al.,\n2018). In this work, we primarily focus on the inﬂuence of a less obvious yet important factor: we\nstudy how the selection of modelling units affects memorization. Typically, the same data can be\nrepresented on various levels: raw bytes and their groups, individual characters, subword units, and\nentire words. A very common approach is to learn subword-level vocabulary with Byte-Pair Encoding\n(BPE) (Sennrich et al., 2015) or similar methods (e.g., Devlin et al., 2018; Kudo and Richardson,\n2018; Provilkov et al., 2019). In spite of ubiquitous use of these methods, to the best of our knowledge,\nthere is no clear understanding of how the number of subwords or BPE operations should be chosen\nand how this affect behavior of a model. We expect that BPE-like segmentation might play a crucial\nrole in memorization, as it controls the trade-off between the number of primitives a model will have\nto operate with and the lengths of sequences it must represent.\nIn this work, to characterize a model’s behavior, we measure three “facets” of training data memo-\nrization. First, as a proxy for the memorization capacity of a model, we use its ability to ﬁt random,\nnon-systematic mappings. Next, we study the preference for memorization when generalization is\npossible. For that, we study how easy it is to accurately tell if a particular example was used in the\nmodel’s training data via a membership inference attack (Shokri et al., 2017). Finally, we examine\nhow easy it is to recover training data from a trained language model. We experiment with three\nTransformer architectures: causal & masked language models, and encoder-based classiﬁers.\nOur main experimental ﬁnding is that, across all architectures and tasks, the choice of modeling units\nstrongly affects the memorization behavior of the models, with large-cardinality BPE vocabularies\ngreatly facilitating memorization . This observation holds even when we control for the number of\ntrainable parameters.\nAfter establishing this fact, we look deeper into the causes of the phenomenon we observe. We\nexamine three candidate causes which are principal (side-)effects of applying BPE: (i) removing\nredundancy in the data (due to compression), (ii) increase in the number of the unique units used to\nrepresent the data, or (iii) reducing the length of the training sequences. By ﬁnding a similar effect\nwith incompressible randomly generated data we can rule out the ﬁrst possibility. Next, we artiﬁcially\ndouble the vocabulary size by introducing “synonym” tokens and observe that the vocabulary growth,\nin isolation, leads to a different memorization pattern. Thus, by exclusion, we conjecture that reducing\nutterance length is, at least, a very important factor of memorization.2\n2 S TUDYING MEMORIZATION –THE TASKS\nTo quantify the memorization capabilities and preferences of NLP models, we use three different\nsetups, with which we aim to cover different facets of what one can call training data memorization.\n2.1 L EARNING MAPPINGS WITH RANDOM LABELS\nFirstly, we consider a task of learning non-systematic mappings, where labels are independent from\ninputs (Zhang et al., 2016). To achieve accuracy above chance, the model has to “store” the training\nexample in some way, thus we assume that a higher training accuracy implies increased training data\nmemorization ability.\nTo experiment with realistic natural input data, we consider the Stanford Natural Language Inference\ndataset (SNLI, Bowman et al., 2015). In this dataset, each example is a pair of two sentences, one\nrepresenting a premise (“A boy is jumping on skateboard in the middle of a red bridge.”) and the\n1An interesting example are the language models in (Lakhotia et al., 2021; Kharitonov et al., 2021), which are\ntrained on sub-phonemic acoustic units without word boundaries, but conﬁdently processes a large vocabulary\nof English words.\n2Another potential cause that we consider is the changes in the relative frequencies of tokens that BPE brings\nalong. In Appendix D we investigate and rule out this hypothesis.\n2\n\nPreprint. Work in progress.\nother representing a hypothesis (“The boy does a skateboarding trick.”). Each example is assigned a\nlabel that denotes if the hypothesis entails the premise, contradicts it, or neither contradict nor entails\n(neutral). We represent examples in a concatenated form with a separator token (::) between a premise\nand a hypothesis (“A boy is jumping on skateboard in the middle of a red bridge. :: The boy does a\nskateboarding trick.”). For uniformity with other experiments, we transform the dataset into a binary\nclassiﬁcation task by ﬁltering out all examples with neutral labels.3After this ﬁltering, 367,388\nexamples remain. We replace original labels with randomly sampled ones (-1 / +1, equiprobably),\nand we measure memorization by measuring the (training) accuracy of the models on this data.\n2.2 M EMBERSHIP INFERENCE\nWhile the task of memorizing random labels can tell us how much a model can remember, it doesn’t\nallow us to test how much or what a model memorizes when it is trained on tasks which also admit\n(or require) generalization. As this is typically the case with natural data, we thus need a different\nstrategy to assess memorization in more realistic scenarios. To evaluate memorization in such cases,\nwe resort to measuring models’ vulnerability to membership inference attacks (Shokri et al., 2017).\nIndeed, if it is “easy” to accurately tell if a particular example was used for training, we assume\nit is actually “stored” in the weights of the model in some form, rather than being inferred from a\nmore general rule or pattern (which would lead to high scores also for examples that were not in the\ntraining data, but that are likely given that data).\nMore formally, suppose we have a model f\u0012that was trained on a subset D0of a large set of examples\nD,(D0\u001aD )withD0obtained by sampling examples independently from Dwith some probability \u0015.\nThe goal of membership inference is to ﬁgure out, given f\u0012, whether a particular example (xi;yi)2D\nwas included in the training data D0.\nWe implement a simple membership inference attack protocol by Yeom et al. (2018). Given a model\nf\u0012parameterized by \u0012, we calculate its loss on a data point l(f\u0012(xi);yi)and compare to a threshold \u001c:\nif it is below the threshold, the data point belongs to the training data. By controlling \u001cwe can control\nthe trade-off between precision and recall of the attack. To avoid the dependency on this parameter\nand to represent the entire space of possible trade-offs, we use the AUC metric. After training a\nmodel, we measure the AUC of the above rule that separates training and hold-out examples.\nIn this set of experiments, we again use the SNLI dataset. However, in this experiment we use the true\nlabels of the dataset, rather than using the random labels of the previous setup, allowing us to consider\nboth generalization and memorization. To make the prior probability of an example belonging to the\ntraining dataset equal to1\n2, at training time we use only a half of the original’s dataset training data\n(367,388 examples remaining after ﬁltering), with the second half playing the role of the hold-out.\n2.3 T RAINING DATA RECOVERY\nLastly, we study the memorization capabilities of Transformer models in a setup that is closer to\nnatural large-scale tasks. In particular, we focus on the – interesting for memorization – domain\nof question-answering, and we consider how well Transformer language models can reproduce\nexact-match answers to questions present in the training data.\nFor this experiment, we use the L1 subset of the large-scale Probably Asked Questions (PAQ)\ndataset (Lewis et al., 2021), which contains 14M queries with candidate answers. As with SNLI,\nwe transform this dataset so that it is suitable for training an LM by concatenating queries and their\nrespective candidate answers, separated by a special token (::). For instance, a training example might\nbe “where is the capital of argentina located :: buenos aires”. Whenever PAQ provided more than one\nanswer, we used the ﬁrst. We lower-cased questions and answers. At test-time, we prompt the trained\nLM by a query followed with the separator token and check whether the trained LM reproduces the\ncorrect answer within top-1 or top-5 results returned by beam search (beam size 5). To speed up the\nevaluation, we probe a ﬁxed random sample of 4M questions.\n3In the current experiment, this is not strictly necessary, since we map the input examples to random labels.\nFiltering the data becomes important in the next experiments, in which we instead use true labels.\n3\n\nPreprint. Work in progress.\n3 M ODELS AND HYPERPARAMETERS\nWe consider three standard Transformer-based architectures: a (causal) language model (LM), a\nmasked language model (MLM), and a sequence encoder (Encoder). In our ﬁrst two setups (random\nlabel memorization and membership inference), we study all three architecture variants. In the\nexperiments on question-answer-recovery, we only study the LM architecture, as those experiments\nrequire the ability to sample from the model efﬁciently.\n3.1 BPE SETTINGS\nThe core question that we ask in this paper is how the choice of modeling units affects the memoriza-\ntion behaviour of state-of-the-art Transformer models. To answer this question, for every experiment\nwe create various versions of the involved datasets that differ in the number of subwords, by varying\nthe parameters of the BPE process used to create subwords. In particular, for the SNLI dataset (used in\nrandom label memorization and membership inference), we apply BPE with (0:5;1;5;10;20)\u0002103\nsteps, resulting in vocabulary sizes of 611, 1097, 4943, 9574, and 18336, respectively. For the larger\nPAQ dataset, used in the recovery experiment, we get different versions of the dataset by running\nBPE for (0:5;1;5;10;15;20)\u0002103steps, obtaining vocabulary sizes of 1280, 1784, 5784, 10784,\n15784, and 20776, respectively.\n3.2 C ONTROLLING THE NUMBER OF LEARNED PARAMETERS\nIn our experiments, we vary the size of the subword vocabulary used to represent the data. In turn,\nthe number of the learned parameters that are present in the embedding layer changes, too. It is not\nunreasonable to expect that the memorization capabilities of a model are impacted by this growth of\nthe number of the learned parameters. To avoid this confounding factor, we complement our study\nwith experiments where we control for the change in the number of embedding parameters. To do\nso, we replace the embedding layer by a combination of an embedding and a fully-connected layer.\nThis way, we can change the dimensionality of the input & output token embeddings and control the\nnumber of the learned parameters while maintaining the rest of the model isolated from any changes.\nThis is a standard architecture variant in fairseq (Ott et al., 2019). We report the used embedding\nsizes and the resulting numbers of parameters in Appendix.\n3.3 C ASTING MODELS AS CLASSIFIERS\nSome of the tasks introduced above require that the studied models are binary classiﬁers (e.g., learning\nnon-systematic mappings § 2.1). Here we discuss how we turn our considered architectures into\nclassiﬁers. Encoder takes the input sequence of length land embeds it into a continuous representation\nRe\u0002l, whereein the embedding size. We take the embedding of the eos token and linearly map it\ninto logits of the labels ( f\u00001;+1g). Encoder is trained to minimize the cross-entropy error of the\ntarget label.\nTo use LM as a classiﬁer, we associate new tokens (which never occur in the training data) with the\ntarget labels and append them to the input strings, after the eos tokens. We train the model using the\nstandard teacher-forcing procedure. This way the accuracy of the classiﬁer equates to the accuracy\nof the predicting last token in a sequence. While this is a non-standard way of training classiﬁers,\nit (i) reﬂects some of the interesting cases where language models are used as universal zero-shot\nlearners (Brown et al., 2020), (ii) allows us to compare LM’s memorization capabilities with other\narchitectures directly.\nTo use MLM as a classiﬁer, we follow a similar approach. We extend an input string by a token that\nspeciﬁes the label. At training-time, this token and a random part of the other tokens are masked. The\nmodel is trained to recover all the masked tokens. The task of recovering a masked label resembles\nhow mask-ﬁlling is used in knowledge-intensive tasks (Petroni et al., 2019).\n3.4 M ODEL AND TRAINING DETAILS\nWe use similar Transformer models across experimental setups, with only slight differences, that\nwe describe below. In all experiments, we use the Adam optimizer (Kingma and Ba, 2014). We\n4\n\nPreprint. Work in progress.\n10 11 12 13 14\nlog2 |V|0.860.880.900.920.940.960.981.00Train accuracy\nEncoder\nLM\nMLM\n(a) Vanilla Transformer models.\n10 11 12 13 14\nlog2 |V|0.800.850.900.951.00Train accuracy\nEncoder\nLM\nMLM (b) Models with the number of parameters ﬁxed.\nFigure 1: Training accuracy for ﬁtting random labels on SNLI. Shaded area represents \u00061SEM\n(Standard Error of the Mean).\nuse half-precision ﬂoats throughout all experiments. We implemented all three architectures using\nPytorch (Paszke et al., 2019).\nMemorizing random labels The models are trained with learning rate 5e-4. The learning rate\nis linearly warmed-up for the ﬁrst 500 updates and then decreased under the square-root schedule.\nAs in this experiment we are interested in the ability to overﬁt training data, we train the models\nfor 500 epochs. We use batches of 512 examples. If a model achieves training accuracy above\nor equal to 0.999, the training is stopped earlier. For LM and MLM are implemented as 4-layer,\n4-head Transformers with embedding size 512 and FFN layer dimensionality of 1024. In preliminary\nexperiments, we found that a 4-layer Encoder model quickly achieves 100% label prediction accuracy\n(on train), irrespective of the number of subwords. Hence, in this experiment we used a 1-layer\nversion for that model type with embedding and FFN sizes of 256 & 512. When training MLM, each\ntoken is masked with probability of 0.2.\nMembership inference We use similar architectures and hyperparmeters as above for LM and\nMLM. Encoder has 4 layers, embedding size of 512 and FFN dimensionality of 1024, same as\n(M)LM. In this experiment we train for 100 epochs.\nQuestion Answering In this set of experiments, we use the fairseq (Ott et al., 2019) imple-\nmentation of Transformer LM. We study two variants of the architecture: base and large. The base\narchitecture has 6 layers with 8 attention heads, embedding dimensionality of 512 and FFN dimen-\nsionality of 2048. The large architecture has 12 layers, 16 attention heads, embdedding dimension of\n1024 and FFN dimension of 2048. Both variants have dropout probabilities of 0.1. In this experiment\nwe are only interested in training data memorization, hence we allow all models to overﬁt and do not\napply any early stopping. We stop training after a ﬁxed budget of 70 epochs. We use a learning rate\nof 5e-4, inverse-sqrt learning rate schedule and a linear warm-up for 4000 updates.\n4 BPE INFLUENCES TRAINING DATA MEMORIZATION\nIn this section, we discuss our ﬁrst main ﬁnding: that the number of BPE merges inﬂuences the\nextent to which a model memorizes training data. This ﬁnding is persistent across the three different\nexperimental setups we described earlier: random label memorization, membership inference and\nquestion-answer recovery. In all these experiments, we systematically control the number of merges,\nwhich is roughly the same as the vocabulary size. We also run complementary experiments where we\nﬁx the number of learned parameters to stay roughly the same for each vocabulary size (see §3.2).\n4.1 M EMORIZING RANDOM LABELS\nIn Figure 1, we report the accuracy on ﬁtting the training data with random labels as a function of\nbase-2 logarithm of the vocabulary size jVj. In Figure 1a we report results for “vanilla” architectures\n(without an additional FFN layer that allows to control for the number of learned parameters) and in\nFigure 1b we provide results for the models with the number of parameters ﬁxed. We see that for all\n5\n\nPreprint. Work in progress.\n10 11 12 13 14\nlog2|V|0.60.70.80.91.0AUC\nEncoder\nLM\nMLM\n(a) AUC, vanilla models.\n10 11 12 13 14\nlog2|V|0.60.70.80.91.0AUC\nEncoder\nLM\nMLM (b) AUC, models with the number of parameters ﬁxed.\n10 11 12 13 14\nlog2|V|0.780.800.820.840.860.880.90Test accuracy\nEncoder\nLM\nMLM\n(c) Test accuracy, vanilla models.\n10 11 12 13 14\nlog2|V|0.780.800.820.840.860.880.90Test accuracy\nEncoder\nLM\nMLM(d) Test accuracy, models with ﬁxed number of params.\nFigure 2: Membership inference attack, SNLI dataset. Shaded area represents \u00061SEM.\nthree models, LM, MLM, and Encoder, the dependency is very consistent: as the vocabulary size\ngrows, the models become universally more successful in ﬁtting random labels. Encoder ﬁts around\n87% of the random labels with the subword vocabulary size of 611 and has a training accuracy of\nmore than 98% when the vocabulary size is increased to 18,336. LM and MLM – that have more\nlayers than Encoder – start off with a slightly higher accuracy (94% and 97%, respectively), their\naccuracy quickly climbs up to 100% when increasing the number of subwords. This tendency persists\nwhen controlling for the number of learned parameters (Figure 1b).\n4.2 M EMBERSHIP INFERENCE\nIn Figure 2 we report the AUC for our membership inference experiments for all three architectures\n(recall that these experiments used the same SNLI data as the previous experiment, except with\nwith true instead of random labels). Mirroring the results of our previous experiment, we observe a\nmonotonic dependency of the membership attack success rate in function of the size of the vocabulary\nused: for all architectures, larger vocabulary granularity implies more memorisation.4\nIn Figures 2c & 2d, we report the accuracy on the same exact models on the hold-out data. We see\nthat all models achieve decent generalization, with accuracy above 0.78. From Figure 2c we see that\nas the vocabulary size grows, the test accuracy of (M)LM has distinct regions of growth. We believe\nthis indicates two important points: (i) generalization is not directly at odds with memorization, and\n(ii) there is a level of granularity that allows better memorization andbetter generalization.\n4.3 Q UESTION ANSWER RECOVERY\nIn Figure 3 we report how the top-1 (Figure 3a) and top-5 (Figure 3b) accuracies change in function\nof the logarithm of the vocabulary size. We report results for Transformer-base and Transformer-large\nmodiﬁcations, alongside with the parameter-controlled variants. We ﬁrstly observe that the model\nsize has a large impact on memorization: LM-large outperforms LM-base in all cases. Next, we see\nthat both models conﬁrm the pattern we observed in the previous experiments: vocabulary growth\nconsistently leads to a growth in the memorization accuracy.\n4In Appendix E we extend these ﬁndings to Transformer seq2seq architectures in a machine translation task.\n6\n\nPreprint. Work in progress.\n11 12 13 14 15\nlog2|V|0.040.060.080.100.120.14Top-1 accuracy\nBase\nLarge\nBase-control\nLarge-control\n(a) Top-1 accuracy.\n11 12 13 14 15\nlog2|V|0.100.150.200.250.30Top-5 accuracy\nBase\nLarge\nBase-control\nLarge-control (b) Top-5 accuracy.\nFigure 3: Training data recovery: top-1 and top-5 accuracy on extracting the correct answer when\nprompted with a query.\n3 4 5 6 7 8 9 10 11\nlog2 |V|0.50.60.70.80.91.0Train accuracy\nEncoder\nLM\nMLM\n(a) Random strings.\n9.5 10.0 10.5 11.0 11.5 12.0 12.5 13.0\nlog2|V|0.750.800.850.900.951.00Train acc\nBPE\nDuplicated vocab (b) SNLI. BPE vs. Duplicated vocab.\nFigure 4: Analysing potential causes of the increased memorization: ﬁtting random labels on random\nstrings (left) and SNLI (right) datasets. Everywhere shaded area represents \u00061SEM.\nFocusing on the models with the number of parameters ﬁxed, we see that even when correcting for\nthe total number of embedding parameters, the size of the learned vocabulary greatly affects the\nability of recovering training data.\n5 L OOKING FOR EXPLANATIONS\nIn §4 we established that larger vocabularies, learned by BPE, lead to stronger memorization in\nTransformer models, even when we control for the number of the learned parameters. In the current\nsection, we focus on why this might be the case, and further investigate this observed effect.\nWe hypothesise that the primary cause of the observed behavior is that with the growth of the BPE\nvocabulary the input sequences become shorter ( length hypothesis ). In turn, shorter sequences are\neasier to memorize as the work of attention is simpliﬁed. Indeed, it is known that learning complex\nattention patterns is hard and often multiple attention heads turn out to be useless, which potentially\nlimits the ability to memorize complex “explanations” of the data (V oita et al., 2019; Michel et al.,\n2019). At the same time, shorter sequences would shift the responsibility for memorization onto FFN\nlayers, which are known to memorize well (Zhang et al., 2016).\nHowever, there could be two alternative explanations at play. Being a compression algorithm\noriginally, BPE compresses the data and it could be easier to memorize data without redundancies\n(redundancy hypothesis ). Second, BPE increases the vocabulary size and it is possible that each\nindividual token becomes more predictive of the label ( vocabulary size hypothesis ). As an illustration,\nin the limit case of the vocabulary growth, every sequence might have a unique token.5In this case,\nthere is a one-to-one mapping between those tokens and labels (answers) that will, again, simplify\nthe work of self-attention. In a series of experiments below, we contrast out these three hypotheses.\n5Generally, BPE stops before that, as it is does not cross word boundaries.\n7\n\nPreprint. Work in progress.\n5.1 E FFECT PERSISTS ON INCOMPRESSIBLE DATA\nTo investigate the redundancy hypothesis, we experiment with learning on randomly generated\nincompressible data and examine whether a similar behavior persists. We generate random data by\nenumerating all 2lbinary (V=f0;1g)sequences of length land randomly assign each sequence to\none of two classes, f\u00001;+1g. To study how the chosen level of representation affects the models’\nmemorization capacity, we apply BPE with {0, 4, 8, 16, 32, 64, 128, 256} steps on the generated data.\nThis allows us to start from a binary dataset with sufﬁciently large sequences and produce a sequence\nof datasets of the same storage complexity (in bits), but with varied vocabulary sizes.\nIn Figure 4 we report the accuracy on ﬁtting the training data with random labels as a function of the\nlogarithm of the vocabulary size. Again, we observe that for all three architectures the dependency\nis very consistent: as the vocabulary size grows, the models become universally more successful in\nﬁtting random labels.6We conclude that the increased memorization success is thus not caused by\nBPE compressing-out redundancies in more natural data.\n5.2 V OCABULARY SIZE GROWTH DOES NOT EXPLAIN BETTER MEMORIZATION\nTo investigate the vocabulary size hypothesis we set up an experiment in which we increase the\nvocabulary size, while keeping sequence lengths constant. We start from our earlier random label\nmemorization setup with SNLI data, using the vocabulary learned with 500 BPE operations. To\nincrease the number of tokens in this vocabulary, we randomly replace each unique token’s occurrence\nwith either of two new tokens, with equal probability, roughly doubling the vocabulary size. For\ninstance, say that the current vocabulary has a token “cat”. We then iterate over the corpus and\nreplace half of its occurrences with “cat1” and the other half with “cat2” (making sure that those do\nnot happen in the prior vocabulary). Thus, if the vocabulary growth alone can explain the observed\neffect, we will see that the datasets with “duplicated” tokens would also be easier to memorize.\nWe report results of this experiment using the LM model in Figure 4b, contrasting it to the BPE-\ninduced memorization results. We see that the resulting curves exhibit considerably different patterns.\nThe line that corresponds to the doubling procedure has a sharp fall in the beginning. Later, as the\nvocabulary size grows, in accordance with in our thought experiment, tokens become increasingly\nmore unique to sequences which boosts the train accuracy. In contrast, the line obtained by growing\nthe vocabulary via BPE shows a monotonic, consistent growth for all vocabulary sizes. From that we\ncan conclude that the observed phenomenon cannot be explained solely by the vocabulary growth.\nTo summarize, the above experiments allowed us to rule out the alternative explanations (redundancy\nand vocabulary growth hypotheses) and we conclude that reduction of the sequence length is the\nprimary factor of the observed memorization effect .7\n6 D ISCUSSION\nWhile the generalization abilities of state-of-the-art models in NLP have been quite extensively\nstudied in the recent past (Finegan-Dollak et al., 2018; Hupkes et al., 2018; Lake and Baroni, 2018;\nKeysers et al., 2019; Korrel et al., 2019; Mul and Zuidema, 2019; Raunak et al., 2019; Saxton et al.,\n2019; Kim and Linzen, 2020; Dankers et al., 2021; Vig and Belinkov, 2019; Dubois et al., 2020;\nHupkes et al., 2020; Kharitonov and Chaabouni, 2020) comparatively little is known about what\nfactors impact how such models memorize .\nFor modelling natural data, however, both generalization and memorization are relevant properties.\nIn some cases, because memorization is harmful – for instance when personal data leak from a\ntrained model. In other cases, memorization instead is necessary for accurate performance – for\ninstance to recall the spelling of words or the meanings of idioms. In this work, we therefore focus\nspeciﬁcally on memorization, considering in particular the impact of the choice of modelling unit,\nwhich recently became particularly relevant given that all current SOTA NLP architectures are trained\nwith an experimentally tuned number of subwords .\n6In this experiment, for small vocabulary sizes, the total number of learned parameters is nearly constant.\n7A ﬂip side of this conjecture is that a more expressive attention improves memorization. In Appendix we\nshow this to be the case: increasing the number of attention heads improve random sequence memorization.\n8\n\nPreprint. Work in progress.\nWe studied memorization behavior of three types of Transformer models (causal and masked lan-\nguage models, and an Encoder-based classiﬁer). We looked at memorization from three different\nperspectives: the ability to memorize random mappings, vulnerability to membership inference\nattacks, and ability to directly reproduce training data on a prompt. We have observed a strong\nand monotonic inﬂuence of the BPE vocabulary size on how successful Transformer models are at\nmemorizing training data. With higher granularity vocabulary (i.e., more BPE merge operations),\nTransformers (a) canmemorize random mappings better, hence they have higher memory capacity,\nand (b) do memorize more of training data in realistic tasks.\nWe considered several explanations for this strong trend. Is the increase perhaps due to the increase\nin the number of trainable parameters ? Often, the embedding layer has a noticeable share of the\ntrained parameters and this number grows with the size of the vocabulary. We show that while this\ngrowth does play a role, the effect persists when we ensure that the number of trained parameters\nremains constant when the granularity of the vocabulary grows.\nNext, we conjecture that the phenomenon is caused by the reduction in the sequence length that\ncorrelates with using larger-cardinality BPE vocabularies. It can simplify memorization, as the\nlatter will no longer require learning complex attention patterns “explaining” the data and offset\ncomplexity onto fully-connected layers, which are known to be good in memorization (Zhang et al.,\n2016). In contrast, learning useful attention patterns is hard and often multiple heads turn out to be\nuseless (V oita et al., 2019; Michel et al., 2019). Hence, it is possible that self-attention serves as a\nbottleneck that limits the memorization capacity.\nThere are, however, two alternative explanations of the observed trend. It can happen that compressed\ndata with less redundancy is easier to memorize ( redundancy hypothesis). However, our experiment\non random artiﬁcial data (Section 4.1) indicates that the reported effect holds even when the data is\nnot compressible in the information-theoretic sense. Further, can the growth of the vocabulary size\nexplain the behavior? It does not seem so: our experiment with artiﬁcially duplicating tokens shows\nthat this only starts to improve memorization with relatively large vocabularies and is detrimental for\nmemory at the beginning. After ruling out the two alternative possibilities, we are left with our initial\nhypothesis that using higher-cardinality BPE subword vocabularies implies having to manage shorter\nsentences on average, which in turn enable easier memorization.\nWith our work, we contribute to the important question of what impacts memorization behaviour in\nTransformer models. We believe that our ﬁndings, ﬁrstly, have an important practical implication.\nOur experiments provide a clear demonstration that both in artiﬁcial and natural data, the simple\nchoice of the number of subwords used has a large impact on the extent to which models memorize\ntraining data, a fact that is not at all obvious when merely looking at performance on i.i.d. train/test\nsplits. This therefore calls for a more careful consideration of how many subwords to use for a\nparticular task, a decision that, in practical scenarios, is rarely thoroughly motivated or investigated.\nIn cases where a lot of memorization is desirable (e.g., QA-tasks) or undesirable (e.g., public models\ntrained on medical data), the number of subwords could provide an important factor in tuning the\namount of memorization that happens inside a model. Our ﬁndings can provide guidance for tuning\nthe learned data structures (e.g., learned Bloom ﬁlter-like existence indexes (Kraska et al., 2017))\nand compression algorithms that train a Transformer model on the data-to-be-compressed as a\ncompression step (Izacard et al., 2019; Bellard, 2021). If increasing the subword vocabulary size is\nnot possible, boosting expressiveness of the attention can provide another solution (Appendix A).\nSecond, our work provides an interesting perspective about the relationship between memorization\nand generalization. As we already mentioned before, memorization and generalization are often\nthought of as competing modes of ﬁtting data. Even in work that explicitly acknowledges that both\nare required to model natural language (e.g. Dankers et al., 2021), they are still seen as skills that\nare required in different situations: some examples, or phrases require memorization, while others\nrequire generalization. However, our experiments with SNLI data show the relationship between\nmemorization and generalization are not directly at odds with each other, there relationship is more\ncomplex than that: there appears to be a level of granularity that allows better memorization and\ngeneralization. This experiment therefore begs the question: what is the actual relationship between\nmemorization and generalization in neural networks? To what extent does one help the other? Is\nthis related to the age-old processing vs storage question in human language processing (Jackendoff,\n2007; Pinker, 1999; Pinker and Prince, 1988; Clahsen, 1999; Rumelhart and McClelland, 1986)?\n9\n\nPreprint. Work in progress.\nREFERENCES\nDaniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan,\nZi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et al. Towards a human-like open-\ndomain chatbot. arXiv preprint arXiv:2001.09977 , 2020.\nFabrice Bellard. Nncp v2: Lossless data compression with transformer. 2021. URL https:\n//bellard.org/nncp/nncp_v2.1.pdf .\nSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated\ncorpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) . Association for Computational Linguistics,\n2015.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165 , 2020.\nNicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data\nfrom large language models. arXiv preprint arXiv:2012.07805 , 2020.\nRahma Chaabouni, Roberto Dessì, and Eugene Kharitonov. Can transformers jump around right in\nnatural language? assessing performance transfer from scan. arXiv preprint arXiv:2107.01366 ,\n2021.\nHarald Clahsen. Lexical entries and rules of language: A multidisciplinary study of German\ninﬂection. Behavioral and Brain Sciences , 22(6):991–1013, December 1999. ISSN 0140-525X,\n1469-1825. doi: 10.1017/S0140525X99002228. URL https://www.cambridge.org/\ncore/product/identifier/S0140525X99002228/type/journal_article .\nKevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. What does bert look at?\nan analysis of bert’s attention. arXiv preprint arXiv:1906.04341 , 2019.\nVerna Dankers, Elia Bruni, and Dieuwke Hupkes. The paradox of the compositionality of natural\nlanguage: a neural machine translation case study. CoRR , abs/2108.05885, 2021. URL https:\n//arxiv.org/abs/2108.05885 .\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.\nYann Dubois, Gautier Dagan, Dieuwke Hupkes, and Elia Bruni. Location Attention for Extrapo-\nlation to Longer Sequences. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics , pages 403–413, Online, July 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.acl-main.39. URL https://aclanthology.org/2020.\nacl-main.39 .\nCatherine Finegan-Dollak, Jonathan K Kummerfeld, Li Zhang, Karthik Ramanathan, Sesh Sadasivam,\nRui Zhang, and Dragomir Radev. Improving text-to-SQL evaluation methodology. In Proceedings\nof the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers) , pages 351–360, 2018. URL https://aclanthology.org/P18-1033/ .\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, San-\njana Krishnan, Marc’Aurelio Ranzato, Francisco Guzman, and Angela Fan. The ﬂores-101\nevaluation benchmark for low-resource and multilingual machine translation. arXiv preprint\narXiv:2106.03193 , 2021.\nMichael Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of\nthe Association for Computational Linguistics , 8:156–171, 2020.\nJunxian He, Graham Neubig, and Taylor Berg-Kirkpatrick. Efﬁcient nearest neighbor language\nmodels. arXiv preprint arXiv:2109.04212 , 2021.\n10\n\nPreprint. Work in progress.\nDieuwke Hupkes, Sara Veldhoen, and Willem Zuidema. Visualisation and ‘diagnostic classiﬁers’\nreveal how recurrent and recursive neural networks process hierarchical structure. Journal of\nArtiﬁcial Intelligence Research , 61:907–926, 2018.\nDieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia Bruni. Compositionality decomposed: how\ndo neural networks generalise? Journal of Artiﬁcial Intelligence Research , 67:757–795, 2020.\nGautier Izacard, Armand Joulin, and Edouard Grave. Lossless data compression with transformer.\n2019. URL https://openreview.net/pdf?id=Hygi7xStvS .\nRay Jackendoff. A parallel architecture perspective on language processing. Brain research , 1146:\n2–22, 2007.\nDaniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashu-\nbin, Nikola Momchev, Danila Sinopalnikov, Lukasz Staﬁniak, Tibor Tihon, et al. Measuring\ncompositional generalization: A comprehensive method on realistic data. In International Con-\nference on Learning Representations , 2019. URL https://openreview.net/pdf?id=\nSygcCnNKwr .\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization\nthrough memorization: Nearest neighbor language models. In International Conference on Learn-\ning Representations , 2020. URL https://openreview.net/forum?id=HklBjCEKvH .\nUrvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Nearest\nneighbor machine translation. In International Conference on Learning Representations , 2021.\nURLhttps://openreview.net/forum?id=7wCBOfJ8hJM .\nEugene Kharitonov and Rahma Chaabouni. What they do when in doubt: a study of inductive biases\nin seq2seq learners. arXiv preprint arXiv:2006.14953 , 2020.\nEugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi, Jade Copet, Kushal Lakhotia, Tu-Anh\nNguyen, Morgane Rivière, Abdelrahman Mohamed, Emmanuel Dupoux, et al. Text-free prosody-\naware generative spoken language modeling. arXiv preprint arXiv:2109.03264 , 2021.\nNajoung Kim and Tal Linzen. COGS: a compositional generalization challenge based on semantic\ninterpretation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP) , pages 9087–9105, 2020. URL https://aclanthology.org/2020.\nemnlp-main.731/ .\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.\nKris Korrel, Dieuwke Hupkes, Verna Dankers, and Elia Bruni. Transcoding compositionally: Us-\ning attention to ﬁnd more generalizable solutions. In Proceedings of the 2019 ACL Workshop\nBlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pages 1–11, Florence, Italy,\nAugust 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-4801. URL\nhttps://aclanthology.org/W19-4801 .\nTim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned index\nstructures. arXiv preprint arXiv:1712.01208 , 2017.\nTaku Kudo and John Richardson. SentencePiece: A simple and language independent subword\ntokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing: System Demonstrations , 2018.\nBrenden Lake and Marco Baroni. Generalization without systematicity: On the compositional skills\nof sequence-to-sequence recurrent networks. In proceedings of the 35th International Conference\non Machine Learning (ICML) , pages 4487–4499, 2018.\nKushal Lakhotia, Evgeny Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte,\nTu-Anh Nguyen, Jade Copet, Alexei Baevski, Adelrahman Mohamed, and Emmanuel Dupoux.\nGenerative spoken language modeling from raw audio. arXiv preprint arXiv:2102.01192 , 2021.\n11\n\nPreprint. Work in progress.\nPatrick Lewis, Pontus Stenetorp, and Sebastian Riedel. Question and answer test-train overlap in\nopen-domain question answering datasets. arXiv preprint arXiv:2008.02637 , 2020.\nPatrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale Minervini, Heinrich Küttler, Aleksandra Piktus,\nPontus Stenetorp, and Sebastian Riedel. PAQ: 65 million probably-asked questions and what you\ncan do with them. arXiv preprint arXiv:2102.07033 , 2021.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692 , 2019.\nWilliam Merrill, Vivek Ramanujan, Yoav Goldberg, Roy Schwartz, and Noah Smith. Effects of\nparameter norm growth during transformer training: Inductive bias from gradient descent, 2021.\nPaul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? arXiv\npreprint arXiv:1905.10650 , 2019.\nRajiv Movva and Jason Y Zhao. Dissecting lottery ticket transformers: Structural and behavioral\nstudy of sparse neural machine translation. arXiv preprint arXiv:2009.13270 , 2020.\nMathijs Mul and Willem Zuidema. Siamese recurrent networks learn ﬁrst-order logic reasoning\nand exhibit zero-shot compositional generalization. In CoRR, abs/1906.00180 , 2019. URL\nhttps://arxiv.org/pdf/1906.00180.pdf .\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,\nand Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of\nNAACL-HLT 2019: Demonstrations , 2019.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings of the 40th annual meeting of the Association\nfor Computational Linguistics , pages 311–318, 2002.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep\nlearning library. In Advances in Neural Information Processing Systems 32 . 2019.\nFabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller,\nand Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066 ,\n2019.\nSteven Pinker. Words and rules: The ingredients of language. 1999.\nSteven Pinker and Alan Prince. On language and connectionism: Analysis of a parallel distributed\nprocessing model of language acquisition. Cognition , 28(1):73–193, 1988. ISSN 0010-0277.\ndoi: https://doi.org/10.1016/0010-0277(88)90032-7. URL https://www.sciencedirect.\ncom/science/article/pii/0010027788900327 .\nIvan Provilkov, Dmitrii Emelianenko, and Elena V oita. Bpe-dropout: Simple and effective subword\nregularization. arXiv preprint arXiv:1910.13267 , 2019.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683 , 2019.\nVikas Raunak, Vaibhav Kumar, Florian Metze, and Jaimie Callan. On compositionality in neural\nmachine translation. In NeurIPS 2019 Context and Compositionality in Biological and Artiﬁ-\ncial Neural Systems Workshop , 2019. URL https://vaibhav4595.github.io/files/\ncompo.pdf .\nAdam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the\nparameters of a language model? arXiv preprint arXiv:2002.08910 , 2020.\n12\n\nPreprint. Work in progress.\nD E Rumelhart and J McClelland. On Learning the Past Tenses of English Verbs. In Parallel\ndistributed processing: Explorations in the microstructure of cognition , pages 216–271. MIT Press,\nCambridge, MA, 1986. URL https://apps.dtic.mil/sti/pdfs/ADA164233.pdf .\nAlexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, and Hervé Jégou. Deja vu: an empirical\nevaluation of the memorization properties of convnets. arXiv preprint arXiv:1809.06396 , 2018.\nDavid Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical\nreasoning abilities of neural models. In Proceedings of the 7th International Conference on\nLearning Representations (ICLR) , 2019.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909 , 2015.\nReza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks\nagainst machine learning models. In 2017 IEEE Symposium on Security and Privacy (SP) , pages\n3–18. IEEE, 2017.\nKoustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, Adina Williams, and Douwe Kiela.\nMasked language modeling and the distributional hypothesis: Order word matters pre-training for\nlittle. CoRR , abs/2104.06644, 2021. URL https://arxiv.org/abs/2104.06644 .\nOm Dipakbhai Thakkar, Swaroop Ramaswamy, Rajiv Mathews, and Francoise Beaufays. Under-\nstanding unintended memorization in language models under federated learning. In Proceedings\nof the Third Workshop on Privacy in Natural Language Processing , 2021.\nJörg Tiedemann. The tatoeba translation challenge–realistic data sets for low resource and multilingual\nmt.arXiv preprint arXiv:2010.06354 , 2020.\nJörg Tiedemann and Santhosh Thottingal. OPUS-MT – building open translation services for the\nworld. In Proceedings of the 22nd Annual Conference of the European Association for Machine\nTranslation , pages 479–480, Lisboa, Portugal, November 2020. European Association for Machine\nTranslation. URL https://aclanthology.org/2020.eamt-1.61 .\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762 , 2017.\nJesse Vig and Yonatan Belinkov. Analyzing the structure of attention in a transformer language model.\nInProceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP , pages 63–76, Florence, Italy, August 2019. Association for Computational Lin-\nguistics. doi: 10.18653/v1/W19-4808. URL https://aclanthology.org/W19-4808 .\nElena V oita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head\nself-attention: Specialized heads do the heavy lifting, the rest can be pruned. arXiv preprint\narXiv:1905.09418 , 2019.\nSamuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machine learning:\nAnalyzing the connection to overﬁtting. In 2018 IEEE 31st Computer Security Foundations\nSymposium (CSF) , pages 268–282. IEEE, 2018.\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding\ndeep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530 , 2016.\nGeorge Zipf. Human Behavior and the Principle of Least Effort . Addison-Wesley, Boston, MA,\n1949.\n13\n\nPreprint. Work in progress.\nA N UMBER OF HEADS\nIn our analysis in §5, we conjecture that the observed effect is caused by the fact that large-cardinality\nBPE vocabulary reduces pressure on attention to learn complex patterns due to having less tokens to\nattend to. This raises the next question: if we increase the power of attention (e.g., by growing number\nof heads) would the memorization be improved? To investigate that, we repeated our random-string\nmemorization experiment. This time, we have ﬁxed the BPE vocabulary size (8 merges) and varied\nthe number of heads used in each layer as f1;2;4;8;16gand measured the training accuracy.\nOur results in Figure 5 conﬁrm our expectations: more heads allow more successful memorization.\nIndeed it seems that the attention serves as a representation bottleneck that does not allow to easily\nmodel complex interactions between tokens and, in turn, limits the memorization. What we observe\nin this paper is that, when needed, this limitation can be alleviated by (a) increasing granularity of\nthe units that represent the data (e.g., by running BPE), or (b) improving representation power of the\nself-attention mechanism.\nB H YPERPARAMETERS USED IN EXPERIMENTS\nB.1 R ANDOM LABELS , SNLI\nIn all experiments we use ﬁxed (sinusoidal) positional embeddings. We use starting learning rate of\n5e-4 that is linearly warmed-up for 500 epochs and then decayed under the inverse-sqrt rule. We use\nhalf-precision ﬂoats to represent the weights. Batch size is 512. In these experiments, we disabled\ndropout. LM & MLM models have 4 layers while Encoder has 1. For each BPE vocabulary size, we\nrepeated our experiments with 3 different random seeds. The attention layers have 4 heads. Hidden\nand embedding layers have dimensionalities of 512 and 256. Depending on the experiment, we used\n1 or 4 GPUs to train each model, maintaining the overall batch size constant.\nB.2 M EMBERSHIP INFERENCE\nFor (M) LM we used the same parameters as in § B.1. In this experiment, Encoder has 4 layers,\nembedding size of 512 and FFN size of 1024. We trained for 100 epochs.\nB.3 QA\nWe used the standard Transformer LM models of fairseq that are speciﬁed by the\n--arch=transformer_lm_big (large) and --arch=transformer_lm (base).\nWe use ﬁxed (sinusoidal) positional embeddings. The base architecture has 6 layers with 8 attention\nheads, embedding dimensionality of 512 and FFN dimensionality of 2048. The large architecture\nhas 12 layers, 16 attention heads, embdedding dimension of 1024 and FFN dimension of 2048. Both\n2 4 6 8 10 12 14 16\nnum. heads0.50.60.70.80.91.0Train accuracy\nEncoder\nLM\nMLM\nFigure 5: Random label memorization accuracy vs. number of heads.\n14\n\nPreprint. Work in progress.\nBPE steps token embedding size num. parameters\n500 4400 18,311,749\n1K 3050 18,239,335\n5K 900 18,238,681\n10K 480 18,105,192\n20K 260 18,231,474\nTable 1: LM: input embedding size and number of learned parameters for the control models.\nBPE steps token embedding size num. parameters\n500 4400 18,316,149\n1K 3050 18,242,385\n5K 900 18,239,581\n10K 480 18,105,672\n20K 260 18,231,734\nTable 2: MLM: input embedding size and number of learned parameters for the control models.\nvariants have dropout probabilities of 0.1. All models are trained for 70 epochs. We use a learning\nrate of 5e-4, inverse-sqrt learning rate schedule and a linear warm-up for 4000 updates. Again, we\nused half-precision ﬂoats to represent models’ weights. The input and output embedding matrices are\nshared in the models. Each GPU’s batch contains up to 3072 tokens and accumulate gradients from 8\nbatches before running an update. The models were trained on 8 GPUs each.\nB.4 R ANDOM SEQUENCES\nAs in § B.1.\nB.5 D UPLICATED TOKENS EXPERIMENT\nAs in § B.1.\nB.6 C ONTROL MODELS\nIn experiments where we control the number of learned parameters, input token embedding size is\nno longer attached to the embedding size within the Transformer model (which remains identical to\nthe vanilla, non-ctontrolled model). In Tables 1-6 we report the embedding sizes and the number of\nlearned parameters for control models we used in experiments.\nC T RAINING DATA EXTRACTION WITH SAMPLING\nIn this Section we investigate whether the relation between the success rate in the training data\nextraction experiments and the BPE-learned vocabulary size, reported in Section 4.3 and Figure 3, is\nindependent from the way we generate continuations of a prompt.\nWe re-evaluate the training data extraction performance for the large models used in Section 4.3.\nWe follow exactly the same protocol as in Section 4.3 with one exception: instead of using beam\nsearch, we sequentially sample tokens from the language model (i.e., we run ancestral sampling)\nat temperature 1.0. For each prompt, we sample 20 candidates and measure how often the ground-\ntruth continuation is among those. We report our ﬁndings in Figure 6. From Figure 6 we see\nthat the reported relation has the same form as before: larger BPE vocabulary sizes lead to better\nmemorization. Overall, we conclude that our ﬁndings are likely to be independent from the speciﬁc\ntraining data extraction method used.\n15\n\nPreprint. Work in progress.\nBPE steps token embedding size num. parameters\n500 1125 1,792,731\n1K 785 1,791,741\n5K 231 1,787,673\n10K 125 1,788,106\n20K 67 1,790,056\nTable 3: Encoder: input embedding size and number of learned parameters for the control models (1\nlayer).\nBPE steps token embedding size num. parameters\n500 3000 13,322,138\n1K 2300 13,294,038\n5K 820 13,305,718\n10K 460 13,287,138\n20K 220 12,670,778\nTable 4: Encoder: input embedding size and number of learned parameters for the control models (4\nlayer).\nD C ANCHANGES IN FREQUENCY DISTRIBUTION CAUSE THE OBSERVED\nEFFECT ?\nIn the main text, we considered three factors that accompany growing BPE vocabulary sizes: reduction\nin sequence length, increased number of tokens, and decrease in the redundancy. Another potential\nconfounding behavior is that BPE affects the (relative) frequency of the tokens used for modelling\nthe data.\nTo showcase this, we run the following experiment. We take BPE vocabularies learned for the SNLI\ndataset with the number of merges varied in f100;400;1000;5000gand, for each token, measure\nits frequency in the corpus. Next, we sort the tokens in the order of decreasing frequency and plot\ntoken frequency in function of its order after sorting. We can expect that with the increased number\nof merges, BPE-learned subwords will start approximating words more closely and the frequency\ndistribution of tokens will approach that of words. In turn, the latter follows an extremely skewed\nZipf distribution (Zipf, 1949). Figure 7a supports this prediction: increasing the number of merges\nforces the token distribution to become more peaky.\n11 12 13 14 15\nlog2|V|0.120.130.140.150.160.170.180.190.20Top-20 accuracy\nLarge\nFigure 6: Training data recovery: top-20 accuracy on extracting the correct answer when prompted\nwith a query (sampling).\n16\n\nPreprint. Work in progress.\nBPE steps token embedding size num. parameters\n500 3000 162,139,136\n1K 2900 162,269,536\n5K 1420 162,278,176\n10K 860 162,192,256\n15K 620 162,212,576\n20K 480 162,112,256\n30K 335 162,150,096\nTable 5: LM-Large, QA experiment: input embedding size and number of learned parameters for the\ncontrol models.\nBPE steps token embedding size num. parameters\n500 2400 24,444,928\n1K 1965 24,433,048\n5K 811 24,443,424\n10K 468 24,441,472\n15K 328 24,428,352\n20K 253 24,430,728\n30K 174 24,447,136\nTable 6: LM-Base, QA experiment: input embedding size and number of learned parameters for the\ncontrol models.\nCan the increased skeweness of the token distribution explain the increased memorization? To rule\nthis possibility out, we run an experiment where the token distribution is ﬁxed to be uniform, but the\nsequence length is reduced.\nTo achieve this, we start from the same dataset of random strings as in Section 5.1. In this dataset,\nthe initial distribution of 0s and 1s is uniform as we enumerate allbinary strings of a ﬁxed length.\nNext, we replace BPE with an “idealized” subword learning procedure which maintains the uniform\ndistribution of tokens but reduces the lengths of the strings. The procedure is simple: we ﬁrstly group\nall possible pairs of 0s and 1s into new tokens (00, 01, 10, 11) and use them to encode the initial\nstrings (e.g., “0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1” becomes “00 00 00 00 00 00 00 01”). We recursively\nrepeat this procedure, obtaining a sequence of views of the same dataset, at each step growing the\nvocabulary size quadratically (2, 4, 16, and 256) and reducing the string length by two. In this process,\nall tokens remain uniformly distributed.\nUsing the obtained series of datasets, we repeat the random label memorization experiment (Sec-\ntion 5.1) and report the results obtained in Figure 7b. We observe that the reported effect persists\neven in the case when the relative frequencies of tokens are not changed and ﬁxed to be uniform, thus\ndisproving the hypothesis that the observed effect is due to frequency and not token length.\n17\n\nPreprint. Work in progress.\n0 20 40 60 80 100\ntoken rank0100200300400500600Frequency, 103BPE 100\nBPE 500\nBPE 1000\nBPE 5000\n(a) Token frequency in function of its frequency rank (position in the list of tokens\nwhen sorted in the decreasing frequency order) for BPE vocabularies of different\nsize. SNLI dataset.\n1 2 3 4 5 6 7 8\nlog2|V| 0.50.60.70.80.91.0Train accuracy\nEncoder\nLM\nMLM\n(b) Accuracy on ﬁtting random labels on random strings. The vocabulary size is\ncontrolled by a procedure that maintains uniform distribution of the tokens (see the\ntext).\nFigure 7: Investigating the relative frequency hypothesis.\n18\n\nPreprint. Work in progress.\nE M EMBERSHIP INFERENCE ON MACHINE TRANSLATION MODELS\nIn this Section we demonstrate that our ﬁndings also transfer to Machine Translation, where Trans-\nformer seq2seq architectures are widely used. Following our earlier experiments, we ﬁx training,\nvalidation, and test data but represent it using BPE-learned vocabularies of increasingly large size.\nWe train Transformer seq2seq models (Vaswani et al., 2017) on these different “views” of the data.\nNext, we run membership inference attacks, as described in Section 4.2: we calculate the negative\nlog-likelihood loss on training and on hold-out examples and then measure AUC of a classiﬁer that\nseparates train vs. hold-out based on this loss. We consider that a higher success of the membership\ninference attack (i.e., higher AUC) is indicative of increased memorization.\nIn § E.1 and E.2 we provide details on the task and model. In § E.3 we report our ﬁndings.\nE.1 M ODEL DETAILS\nAll models we train are English-Dutch Transformer-base machine translation models, con-\nsisting of 6 self-attention layers and 8 attention heads per layer in the encoder and decoder, as\nimplemented in fairseq (Ott et al., 2019). Encoder and decoder both have an embedding size of 512\nand a forward layer dimonsionality of 2048. We follow the standard settings suggested by fairseq.8\nThat is, we use Adam as optimizer with \f-values (0.9, 0.98). We start from an initial learning rate\nof 1e-07, increase it to 0.0005 during 4000 warmup updates, and then decay it using an inverse\nsquare root learning rate schedule. We share all embeddings between encoder and decoder, we use a\nclip-norm of 0.0, dropout of 0.3, weight-decay of 0.0001, and label-smoothing of 0.1. The maximum\nnumber of tokens in a single GPU’s batch is 3584. We train using 32 GPUs and aggregate gradients\nfrom 16 batches before an update. For early stopping, we evaluate BLEU (Papineni et al., 2002)\nscore on the validation set using a beam size of 5. We set the patience parameter to 10. For any other\nhyperparameters, we use the fairseq defaults.\nE.2 D ATA DETAILS\nWe train our model on 8M sentence pairs of the MT corpus OPUS (Tiedemann and Thottingal, 2020;\nTiedemann, 2020).9For tokenization, we use the tokenization script10from the SMT library Moses.11\nFor early stopping, we use the quality-controlled dev set of the FLORES -101 corpus (Goyal et al.,\n2021).12In our membership inference experiments, as a hold-out, we use a disjoint sample of 8M\nsentences from the OPUS corpus.\nWe iterate the number of merges done by the BPE learning process in f10;20;40;80g\u0001103. Learning\nBPE vocabularies was done separately on English and Dutch parts of the training data. For simplicity,\nwe only train models on variants where English and Dutch vocabularies are obtained with the same\nnumber of merges.\nE.3 R ESULTS\nWe ﬁrstly veriﬁed that the models achieved reasonable performance. This turned out to be the case:\nthe models trained with BPE-10k, BPE-20k, BPE-40k, and BPE-80k have validation BLEU scores\n24.52, 24.95, 24.81, 25.12, respectively. These numbers are in line with what was reported in the\nliterature (Dankers et al., 2021). Before stopping, these models trained for 100, 80, 71, and 65 epochs,\nrespectively, with smaller-vocabulary models training longer, thus having higher chance to memorize\ntraining data.\n8https://github.com/pytorch/fairseq/tree/main/examples/scaling_nmt\n9https://github.com/Helsinki-NLP/Tatoeba-Challenge/blob/master/data/\nREADME-v2020-07-28.md\n10https://github.com/moses-smt/mosesdecoder/blob/master/scripts/\ntokenizer/tokenizer.perl\n11https://github.com/moses-smt/mosesdecoder\n12Can be downloaded from https://dl.fbaipublicfiles.com/flores101/dataset/\nflores101_dataset.tar.gz .\n19\n\nPreprint. Work in progress.\n14.5 15.0 15.5 16.0 16.5 17.0\nlog2{|Vnl|+|Ven|}0.5100.5120.5140.516AUC\nFigure 8: Membership inference attack on an NMT model.\nNext, we report the results of the membership inference experiment in Figure 8. We see that also in a\nnatural setup, growing the size of the learned vocabulary size makes it easier to predict whether a\nparticular example was used in the training of a Transformer seq2seq model. We conclude that our\nﬁndings in the main text are likely to hold for Transformer seq2seq models in machine translation\ntasks.\nWe also want to highlight that larger-vocabulary models achieved higher validation BLEU scores and\nare more susceptible to membership inference attacks. This resonates with our ﬁndings in the main\ntext and, we believe, is an interesting direction for further inquiry.\n20",
  "textLength": 64787
}