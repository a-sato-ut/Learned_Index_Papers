{
  "paperId": "0c917fef48a23f02c701d21b0bb0e20754316e60",
  "title": "Learning Key-Value Store Design",
  "pdfPath": "0c917fef48a23f02c701d21b0bb0e20754316e60.pdf",
  "text": "Learning Key-Value Store Design\nStratos Idreos, Niv Dayan, Wilson Qin, Mali Akmanalp, Sophie Hilgard, Andrew Ross,\nJames Lennon, Varun Jain, Harshita Gupta, David Li, Zichen Zhu\nHarvard University\nABSTRACT\nWe introduce the concept of design continuums for the data\nlayout of key-value stores. A design continuum uni\fes major\ndistinct data structure designs under the same model. The\ncritical insight and potential long-term impact is that such\nunifying models 1) render what we consider up to now as\nfundamentally di\u000berent data structures to be seen as\\views\"\nof the very same overall design space, and 2) allow \\seeing\"\nnew data structure designs with performance properties that\nare not feasible by existing designs. The core intuition be-\nhind the construction of design continuums is that all data\nstructures arise from the very same set of fundamental de-\nsign principles, i.e., a small set of data layout design con-\ncepts out of which we can synthesize any design that exists\nin the literature as well as new ones. We show how to con-\nstruct, evaluate, and expand, design continuums and we also\npresent the \frst continuum that uni\fes major data structure\ndesigns, i.e., B+tree, B\u000ftree, LSM-tree, and LSH-table.\nThe practical bene\ft of a design continuum is that it cre-\nates a fast inference engine for the design of data structures.\nFor example, we can predict near instantly how a speci\fc de-\nsign change in the underlying storage of a data system would\na\u000bect performance, or reversely what would be the optimal\ndata structure (from a given set of designs) given workload\ncharacteristics and a memory budget. In turn, these prop-\nerties allow us to envision a new class of self-designing key-\nvalue stores with a substantially improved ability to adapt\nto workload and hardware changes by transitioning between\ndrastically di\u000berent data structure designs to assume a di-\nverse set of performance properties at will.\n1. A VAST DESIGN SPACE\nKey-value stores are everywhere , providing the stor-\nage backbone for an ever-growing number of diverse appli-\ncations. The scenarios range from graph processing in social\nmedia [11, 18], to event log processing in cybersecurity [19],\napplication data caching [71], NoSQL stores [78], \rash trans-\nlation layer design [26], time-series management [51, 52], and\nonline transaction processing [31]. In addition, key-value\nstores increasingly have become an attractive solution as\nThis is an extended version of a paper presented at CIDR 2019, the 9th\nBiennial Conference on Innovative Data Systems.\nReadMemoryUpdate\nPerformanceTrade-offs\nData StructuresKey-Value StoresDatabases \nAccess PatternsHardwareCloud costsKVKVKV…TableTableLSMHashB-TreeMachine GraphStore\nDataLearningFigure 1: From performance trade-o\u000bs to data structures,\nkey-value stores and rich applications.\nembedded systems in complex data-intensive applications,\nmachine learning pipelines, and larger systems that support\nmore complex data models. For example, key-value stores\nare utilized in SQL systems, e.g., FoundationDB [9] is a core\npart of Snow\rake [23], while MyRocks integrates RockDB\nin MySQL as its backend storage.\nThere is no Perfect Design . As shown in Figure 1, at\nits core a key-value store implements a data structure that\nstores key-value pairs. Each data structure design achieves\na speci\fc balance regarding the fundamental trade-o\u000bs of\nread, update, and memory ampli\fcation [13]. For example,\nread ampli\fcation is de\fned as \\how much more data do\nwe have to read for every key we are looking for on top\nof accessing this particular key\". There exists no perfect\ndata structure that minimizes all three performance trade-\no\u000bs [13, 44]. For example, if we add a log to support e\u000ecient\nout of place writes, we sacri\fce memory/space cost as we\nmay have duplicate entries, and read cost as future queries\nhave to search both the core data structure and the log.\nIn turn, this means that there exists no perfect key-value\nstore that covers diverse performance requirements. Every\ndesign is a compromise. But then how do we know which\ndesign is best for an application, e.g., for speci\fc data, ac-\ncess patterns, hardware used, or even a maximum \fnancial\nbudget on the cloud? And do we have enough designs and\nsystems to cover the needs of emerging and ever-changing\ndata-driven applications? This is the problem we study in\nthis paper and envision a research path that makes it easier\nto create custom data structure designs that match the needsarXiv:1907.05443v1  [cs.DB]  11 Jul 2019\n\nof new applications, hardware, and cloud pricing schemes.\nThe Big Three. As of 2018, there are three predomi-\nnant data structure designs for key-value stores to organize\ndata. To give an idea of the diverse design goals and per-\nformance balances they provide, we go brie\ry through their\ncore design characteristics. The \frst one is the B+tree [15].\nThe prototypical B+tree design consists of a leaf level of in-\ndependent nodes with sorted key-value pairs (typically mul-\ntiple storage blocks each) and an index (logarithmic at the\nnumber of leaf nodes) which consists of nodes of fractional\ncascading fence pointers with a large fanout. For example,\nB+tree is the backbone design of the BerkeleyDB key-value\nstore [73], now owned by Oracle, and the backbone of the\nWiredTiger key-value store [94], now used as the primary\nstorage engine in MongoDB [72]. FoundationDB [9] also re-\nlies on a B+tree. Overall, B+tree achieves a good balance\nbetween read and write performance with a reasonable mem-\nory overhead that is primarily due to its \fll factor in each\nnode (typically 50%) and the auxiliary internal index nodes.\nIn the early 2000s, a new wave of applications emerged\nrequiring faster writes, while still giving good read perfor-\nmance. At the same time, the advent of \rash-based SSDs\nhas made write I/Os 1-2 orders of magnitude costlier than\nread I/Os [1]. These workload and hardware trends led\nto two data structure design decisions for key-value stores:\n1) bu\u000bering new data in memory, batching writes in sec-\nondary storage, and 2) avoiding global order maintenance.\nThis class of designs was pioneered by the Log-Structured\nMerge Tree (LSM-tree) [74] which partitions data tem-\nporally in a series of increasingly larger levels. Each key-\nvalue entry enters at the very top level (the in-memory write\nbu\u000ber) and is sort-merged at lower levels as more data ar-\nrives. In-memory structures such as Bloom \flters, fence\npointers and Tries help \flter queries to avoid disk I/O [24,\n99]. This design has been adopted in numerous industrial\nsettings including LevelDB [35] and BigTable [21] at Google,\nRocksDB [32] at Facebook, Cassandra [60], HBase [38] and\nAccumulo [8] at Apache, Voldemort [65] at LinkedIn, Dy-\nnamo [29] at Amazon, WiredTiger [94] at MongoDB, and\nbLSM [84] and cLSM [34] at Yahoo, and more designs in re-\nsearch such as SlimDB [79], WiscKey [68], Monkey [24, 25],\nDostoevsky [27], and LSM-bush [28]. Relational databases\nsuch as MySQL and SQLite4 support this design too by\nmapping primary keys to rows as values. Overall, LSM-\ntree-based designs achieve better writes than B+tree-based\ndesigns but they typically give up some read performance\n(e.g., for short-range queries) given that we have to look for\ndata through multiple levels, and they also give up some\nmemory ampli\fcation to hold enough in-memory \flters to\nsupport e\u000ecient point queries. Crucial design knobs, such\nas \fll factor for B+tree and size ratio for LSM-tree, de\fne\nthe space ampli\fcation relationship among the two designs.\nMore recently, a third design emerged for applications that\nrequire even faster ingestion rates. The primary data struc-\nture design decision was to drop order maintenance. Data\naccumulates in an in-memory write bu\u000ber. Once full, it is\npushed to secondary storage as yet another node of an ever-\ngrowing single level log. An in-memory index, e.g., a hash ta-\nble, allows locating any key-value pair easily while the log is\nperiodically merged to enforce an upper bound on the num-\nber of obsolete entries. This Log-Structured Hash-table\n(LSH-table) is employed by BitCask [86] at Riak, Sparkey\n[88] at Spotify, FASTER [20] at Microsoft, and many moresystems in research [80, 64, 2]. Overall, LSH-table achieves\nexcellent write performance, but it sacri\fces read perfor-\nmance (for range queries), while the memory footprint is\nalso typically higher since now all keys need to be indexed\nin-memory to minimize I/O needs per key.\nThe Practical Problem. While key-value stores con-\ntinue to be adopted by an ever-growing set of applications,\neach application has to choose among the existing designs\nwhich may or may not be close to the ideal performance\nthat could be achieved for the speci\fc characteristics of the\napplication. This is a problem for several increasingly press-\ning reasons. First, new applications appear many of which\nintroduce new workload patterns that were not typical be-\nfore. Second, existing applications keep rede\fning their ser-\nvices and features which a\u000bects their workload patterns di-\nrectly and in many cases renders the existing underlying\nstorage decisions sub-optimal or even bad. Third, hardware\nkeeps changing which a\u000bects the CPU/bandwidth/latency\nbalance. Across all those cases, achieving maximum perfor-\nmance requires low-level storage design changes. This boils\ndown to the one size does not \ft all problem, which holds\nfor overall system design [90] and for the storage layer [13].\nEspecially in today's cloud-based world, even designs\nslightly sub-optimal by 1% translate to a massive loss in\nenergy utilization and thus costs [57], even if the perfor-\nmance di\u000berence is not directly felt by the end users. This\nimplies two trends. First, getting as close to the optimal\ndesign is critical. Second, the way a data structure design\ntranslates to cost needs to be embedded in the design pro-\ncess as it is not necessarily about achieving maximum query\nthroughput, but typically a more holistic view of the de-\nsign is needed, including the memory footprint. Besides,\nthe cost policy varies from one cloud provider to the next,\nand even for the same provider it may vary over time. For\nexample, Amazon AWS charges based on CPU and memory\nfor computation resources, and based on volume size, re-\nserved throughput, and I/O performed for networked stor-\nage. Google Cloud Platform, while charging similarly for\ncomputation, only charges based on volume size for net-\nworked storage. This implies that the optimal data structure\n1) is di\u000berent for di\u000berent cloud providers where the key-\nvalue store is expected to run, and 2) can vary over time\nfor the same cloud provider even if the application itself and\nunderlying hardware stay the same.\nThe Research Challenge. The long-term challenge is\nwhether we can easily or even automatically \fnd the op-\ntimal storage design for a given problem. This has been\nrecognized as an open problem since the early days of com-\nputer science. In his seminal 1978 paper, Robert Tarjan\nincludes this problem in his list of the \fve major challenges\nfor the future (which also included PVsNP) [91]: \\Is there\na calculus of data structures by which one can choose the\nappropriate data representation and techniques for a given\nproblem?\" . We propose that a signi\fcant step toward a so-\nlution includes dealing with the following two challenges:\n1)Can we know all possible data structure designs?\n2)Can we compute the performance of any design?\nToward an Answer to Challenge 1. We made a step\ntoward the \frst challenge by introducing the design space\nof data structures supporting the key-value model [45]. The\ndesign space is de\fned by all designs that can be described\n\ndesign space\nfanoutﬁlter bitsbuﬀer size  merge policy…Design Primitives\ndata structuresdesign continuum\nLSMB-treeTGCFYTKVGOGOQT[&CVC\u0002UVTWEVWTG\u0002FGUKIPU\u0002CTG\u0002FGTKXGF\u0002CU\u0002EQODKPCVKQPU\u0002QH\u0002HWPFCOGPVCN\u0002FGUKIP\u0002RTKOKVKXGUFigure 2: From data layout design principles to the de-\nsign space of possible data structures, where design continu-\nums can be observed to help navigate performance trade-o\u000bs\nacross diverse data structure designs.\nas combinations and tunings of the \\\frst principles of data\nlayout design\". A \frst principle is a fundamental design con-\ncept that cannot be broken into additional concepts, e.g.,\nfence pointers, links, and temporal partitioning. The intu-\nition is that, over the past several decades, researchers have\ninvented numerous fundamental design concepts such that\na plethora of new valid designs with interesting properties\ncan be synthesized out of those [45].\nAs an analogy consider the periodic table of elements\nin chemistry; it sketched the design space of existing ele-\nments based on their fundamental components, and allowed\nresearchers to predict the existence of unknown, at the time,\nelements and their properties, purely by the structure of the\ndesign space. In the same way, we created the periodic\ntable of data structures [44] which describes more data\nstructure designs than stars on the sky and can be used as\na design and new data structure discovery guide.\nNaturally, a design space does not necessarily describe\\all\npossible data structures\"; a new design concept may be in-\nvented and cause an exponential increase in the number of\npossible designs. However, after 50 years of computer sci-\nence research, the chances of inventing a fundamentally new\ndesign concept have decreased exponentially; many exciting\ninnovations, in fact, come from utilizing a design concept\nthat, while known, it was not explored in a given context\nbefore and thus it revolutionizes how to think about a prob-\nlem. Using Bloom \flters as a way to \flter accesses in storage\nand remote machines, scheduling indexing construction ac-\ntions lazily [41], using ML models to guide data access [59],\nstorage [50] and other system components [58], can all be\nthought of as such examples. Design spaces that cover large\nfundamental sets of concepts can help accelerate progress\nwith \fguring out new promising directions, and when new\nconcepts are invented they can help with \fguring out the\nnew possible derivative designs.\nToward an Answer to Challenge 2. The next piece\nof the puzzle is to investigate if we can make it easy to com-\npute the performance properties of any given data structure\ndesign. With the Data Calculator we introduced the idea of\nlearned cost models [45] which allow learning the costs\nof fundamental access patterns (random access, scan, sorted\nsearch) out of which we can synthesize the costs of complex\nalgorithms for a given data structure design. These costs\ncan, in turn, be used by machine learning algorithms that\niterate over machine generated data structure speci\fcationsto label designs, and to compute rewards, deciding which de-\nsign speci\fcation to try out next. Early results using genetic\nalgorithms show the strong potential of such approaches [42].\nHowever, there is still an essential missing link; given the fact\nthat the design space size is exponential in the number of\ndesign principles (and that it will likely only expand over\ntime), such solutions cannot \fnd optimal designs in feasible\ntime, at least not with any guarantee, leaving valuable per-\nformance behind [57]. This is the new problem we attack in\nthis paper: Can we develop fast search algorithms that au-\ntomatically or interactively help researchers and engineers\n\fnd a close to optimal data structure design for a key-value\nstore given a target workload and hardware environment?\nDesign Continuums. Like when designing any algo-\nrithm, the key ingredient is to induce domain-speci\fc knowl-\nedge. Our insight is that there exist\\design continuums\"em-\nbedded in the design space of data structures. An intuitive\nway to think of design continuums is as a performance hy-\nperplane that connects a speci\fc subset of data structures\ndesigns. Design continuums are e\u000bectively a projection of\nthe design space, a \\pocket\" of designs where we can iden-\ntify unifying properties among its members. Figure 2 gives\nan abstract view of this intuition; it depicts the design space\nof data structures where numerous possible designs can be\nidenti\fed, each one being derived as a combination of a small\nset of fundamental design primitives and performance con-\ntinuums can be identi\fed for subsets of those structures.\n1. We introduce design continuums as subspaces of the\ndesign space which connect more than one design. A\ndesign continuum has the crucial property that it cre-\nates a continuous performance tradeo\u000b for fundamen-\ntal performance metrics such as updates, inserts, point\nreads, long-range and short-range scans, etc.\n2. We show how to construct continuums using few design\nknobs. For every metric it is possible to produce a\nclosed-form formula to quickly compute the optimal\ndesign. Thus, design continuums enable us to know\nthe best key-value store design for a given workload\nand hardware.\n3. We present a design continuum that connects major\nclasses of modern key-value stores including LSM-tree,\nB\u000ftree, and B+tree.\n4. We show that for certain design decisions key-value\nstores should still rely on learning as it is hard (per-\nhaps even impossible) to capture them in a continuum.\n5. We present the vision of self-designing key-value stores,\nwhich morph across designs that are now considered as\nfundamentally di\u000berent.\nInspiration. Our work is inspired by numerous e\u000borts\nthat also use \frst principles and clean abstractions to un-\nderstand a complex design space. John Ousterhout's project\nMagic allows for quick veri\fcation of transistor designs so\nthat engineers can easily test multiple designs synthesized\nby basic concepts [75]. Leland Wilkinson's \\grammar of\ngraphics\" provides structure and formulation on the massive\nuniverse of possible graphics [93]. Timothy G. Mattson's\nwork creates a language of design patterns for parallel algo-\nrithms [70]. Mike Franklin's Ph.D. thesis explores the pos-\nsible client-server architecture designs using caching based\n\nreplication as the main design primitive [33]. Joe Heller-\nstein's work on Generalized Search Trees makes it easy to\ndesign and test new data structures by providing templates\nwhich expose only a few options where designs need to dif-\nfer [39, 6, 7, 54, 53, 55, 56]. S. Bing Yao's [98] and Stefan\nManegold's [69] work on generalized hardware conscious cost\nmodels showed that it is possible to synthesize the costs of\ncomplex operations from basic access patterns. Work on\ndata representation synthesis in programming languages en-\nables synthesis of representations out of small sets of (3-5)\nexisting data structures [81, 82, 22, 87, 85, 36, 37, 67, 89].\n2. DESIGN CONTINUUMS\nWe now describe how to construct a design continuum.\n2.1 From B+tree to LSM-tree\nWe \frst give an example of a design continuum that con-\nnects diverse designs including Tiered LSM-tree [46, 24, 60],\nLazy Leveled LSM-tree [27], Leveled LSM-tree [74, 24, 32,\n35], COLA [16, 48], FD-tree [63], B\u000ftree [17, 10, 16, 47, 48,\n76], and B+tree [14]. The design continuum can be thought\nof as a super-structure that encapsulates all those designs.\nThis super-structure consists of Llevels where the larger Y\nlevels are cold and the smaller L\u0000Ylevels are hot. Hot\nlevels use in-memory fence pointers and Bloom \flters to fa-\ncilitate lookups, whereas cold levels apply fractional cascad-\ning to connect runs in storage. Each level contains one or\nmore runs, and each run is divided into one or more con-\ntiguous nodes. There is a write bu\u000ber in memory to ingest\napplication updates and \rush to Level 1 when it \flls up.\nThis overall abstraction allows instantiating any of the data\nstructure designs in the continuum. Figure 3 formalizes the\ncontinuum and the super-structure is shown at the bottom\nleft. For reference, we provide several examples of super-\nstructure instantiations in Appendix A and Figure 12.\nEnvironmental Parameters. The upper right table in\nFigure 3 opens with a number of environmental parameters\nsuch as dataset size, main memory budget, etc. which are\ninherent to the application and context for which we want\nto design a key-value store.\nDesign Parameters. The upper right table in Figure 3\nfurther consists of \fve continuous design knobs which have\nbeen chosen as the smallest set of movable design abstrac-\ntions that we could \fnd to allow di\u000berentiating among the\ntarget designs in the continuum. The \frst knob is the growth\nfactorTbetween the capacities of adjacent levels of the\nstructure (e.g., \\fanout\" for B+tree or \\size ratio\" for LSM-\ntree). This knob allows us to control the super-structure's\ndepth. The second knob is the hot merge threshold K, which\nis de\fned as the maximum number of independent sorted\npartitions (i.e., runs) at each of Levels 1 to L\u0000Y\u00001 (i.e.,\nall hot levels but the largest) before we trigger merging. The\nlower we set K, the more greedy merge operations become\nto enforce fewer sorted runs at each of these hot levels. Sim-\nilarly, the third knob is the cold merge threshold Zand is\nde\fned as the maximum number of runs at each of Levels\nL\u0000YtoL(i.e., the largest hot level and all cold levels)\nbefore we trigger merging. The node sizeDis the maximal\nsize of a contiguous data region (e.g., a \\node\" in a B+tree\nor \\SSTable\" in an LSM-tree) within a run. Finally, the\nfence and \flters memory budget MFcontrols the amount\nof the overall memory that is allocated for in-memory fence\npointers and Bloom \flters.Setting the domain of each parameter is a critical part\nof crafting a design continuum so we can reach the target\ndesigns and correct hybrid designs. Figure 3 describes how\neach design parameter in the continuum may be varied. For\nexample, we set the maximum value for the size ratio Tto\nbe the block size B. This ensures that when fractional cas-\ncading is used at the cold levels, a parent block has enough\nspace to store pointers to all of its children. As another ex-\nample, we observe that a level can have at most T\u00001 runs\nbefore it runs out of capacity and so based on this observa-\ntion we set the maximum values of KandZto beT\u00001.\nDesign Rules: Forming the Super-structure. The\ncontinuum contains a set of design rules, shown on the up-\nper right part of Figure 3. These rules enable instantiating\nspeci\fc designs by deterministically deriving key design as-\npects. Below we describe the design rules in detail.\nExponentially Increasing Level Capacities. The levels' ca-\npacities grow exponentially by a factor of Tstarting with\nthe write bu\u000ber's capacity. As a result, the overall number\nof levelsLgrows logarithmically with the data size.\nFence Pointers & Bloom Filters. Our design allocates\nmemory for fence pointers and Bloom \flters from smaller to\nlarger levels based on the memory budget assigned by the\nknobMF. Speci\fcally, we \frst compute Qas the number\nof levels for which there is not enough memory for having\nboth Bloom \flters and fence pointers. We then assign the\nmemory budget MFfor fence pointers to as many levels as\nthere is enough memory for. This is shown by the Equa-\ntion for the fence pointers budget MFPin Figure 3. The\nremaining portion of MFafter fence pointers is assigned to\na Bloom \flters memory budget MBF. This can also be done\nin the reverse way when one designs a structure, i.e., we can\nde\fne the desired write bu\u000ber budget \frst and then give the\nremaining from the total memory budget to \flters and fence\npointers.\nOptimal Bloom Filter Allocation Across Levels. The con-\ntinuum assigns exponentially decreasing false positive rates\n(FPRs) to Bloom \flters at smaller levels, as this approach\nwas shown to minimize the sum of their false positive rates\nand thereby minimize point read cost [24]. In Figure 3, we\nexpress the FPR assigned to Level iaspiand give corre-\nsponding equations for how to set pioptimally with respect\nto the di\u000berent design knobs.\nHot vs. Cold Levels. Figure 3 further shows how to com-\npute the number of cold levels Yfor which there is not suf-\n\fcient memory for Bloom \flters. The derivation for Yis in\nterms of a known threshold Xfor when to drop a \flter for\na level and instead use that memory for \flters at smaller\nlevels to improve performance [27]. Note that Yis either\nequal toQor larger than it by at most one. We derive\nMFHIas the amount of memory above which all levels are\nhot (i.e.,Y= 0). We also set a minimum memory require-\nmentMFLOonMFto ensure that there is always enough\nmemory for fence pointers to point to Level 1.\nFractional Cascading for Cold Levels. We use fractional\ncascading to connect data at cold levels to the structure,\nto address the issue of not having enough memory to point\nto them using in-memory fence pointers. For every block\nwithin a run at a cold level, we embed a \\cascading\" fence\npointer within the next younger run along with the smallest\nkey in the target block. This allows us to traverse cold levels\nwith one I/O for each run by following the corresponding\ncascading fence pointers to reach the target key range.\n\nEnvironment Parameters Design ParametersTerm Name Description Min.\nValueMax.\nValueUnits\nB Block Size # data entries that ￿t in a storage block. Entries\nM Memory Total main memory budget. B·E+\nF·T·MB\nE·BN ·EBits\nN Dataset Size # total data entries in the dataset. Entries\nE Entry Size Size of an entry. Bits\nF Key Size Size of a key, also used to approximate\nsize of a fence (fence key and pointer).Bits\ns Avg. Selectivity Average selectivity of a long range query. Entries\nT Growth Factor Capacity ratio between adjacent levels. 2 B Ratio\nK Hot Merge\nThresholdMaximum # runs per hot level. 1T\u00001Runs\nZ Cold Merge\nThresholdMaximum # runs per cold level. 1T\u00001Runs\nD Max. Node Size Maximum size of a node; de￿nes a con-\ntiguous data region.1N\nBBlocks\nMFFence & Filter\nMemory Budget# bits of main memory budgeted to fence\npointers and ￿lters.F·T·MB\nE·BM BitsDerived Term Expression Units\nL(# total levels) dlogT(N ·E\nMB·T\u00001\nT)e Levels\nX(Filters\nMemory Threshold)1\nln(2)2·(ln(T)\nT\u00001+ln(K)\u0000ln(Z)\nT) Bits per\nEntry\nMFHI(MFThreshold:\nHot Levels Saturation)N ·(X\nT+F\nB·T\nT\u00001) Bits\nMFLO(MFThreshold:\nCold Levels Saturation)MB·F·T\nE·BBits\nQ(# Cascading Levels)8>><>>:0 ifMF\u0000MFHI\ndlogTN\nMF·(X\nT+F\nB·T\nT\u00001)eifMFLO<MF<MFHI\nL\u00001 ifMF=MFLOLevels\nMB(Bu￿er Memory\nBudget)B ·E+(M\u0000MF) Bits\nMFP(Fence Pointer\nMemory Budget)TL\u0000Q\u00001\nT\u00001·MB·F\nE·B·T Bits\nMBF(Filter Memory\nBudget)MF\u0000MFP Bits\nY(# Cold Levels)8>><>>:0 ifMBF\u0000X ·N\ndlogTN ·X\nMFe if (X ·N)/TL<MBF<X ·N\nL ifMBF(X ·N)/TLLevels\np(Sum of BF False\nPositive Rates)e\u0000MBF\nN·ln(2)2·TY·ZT\u00001\nT ·K1\nT ·TT\nT\u00001\nT\u00001\npi(BF False Positive\nRate at Level i)8>><>>:1 ifi>L\u0000Y\np\nZ·T\u00001\nTifi=L\u0000Y\np\nK·T\u00001\nT·1\nTL\u0000Y\u0000i ifi<L\u0000Y\nDerived Design RulesSuper-Structure\n…BuﬀerBloom\nFiltersFence \nPointers\nMemoryStorage\nFP BF\nFP\nFPBFBFBFBF …\n… …\n……\n…node\nrun boundary\nfence pointerstorage block\nFP\nFP\n…L-Y hot levels\nY cold levelsFP BF\n…oldest hot run contains\ncascading fence pointersOperation Cost Expression (I/O)\nUpdate1\nB·(T\u00001\nK+1·(L\u0000Y\u00001)+T\nZ·(Y+1))\nZero Result Lookup p+Y ·Z\nSingle Result Lookup 1+p+Y ·Z\u0000pL·Z+1\n2\nShort Scan K ·(L\u0000Y\u00001)+Z ·(Y+1)\nLong Scans·Z\nBGeneral Cost ModelFigure 3: An example of a design continuum: connecting complex designs with few continuous parameters.\nActive vs. Static Runs. Each level consists of one active\nrun and a number of static runs . Incoming data into a\nlevel gets merged into the active run. When the active run\nreaches a fraction of T=Kof the levels' capacity for Levels 1\ntoL\u0000Y\u00001orT=Zfor LevelsL\u0000YtoL, it becomes a static\nrun and a new empty active run is initialized.\nGranular Rolling Merging. When a level reaches capacity,\na merge operation needs to take place to free up space. We\nperform a merge by \frst picking an eviction key1. Since\neach run is sorted across its constituent nodes, there is at\nmost one node in each of the static runs at the level that\nintersects with the eviction key. We add these nodes into an\neviction set and merge them into the active run in the next\nlarger level. Hence, the merge granularity is controlled by\nthe maximum node size D, and merge operations rollacross\n1In practice, an eviction key implies a range of eviction keys\nwith a start key and end key, because across runs the notion\nof temporal sequentiality for entries with the same key must\nbe maintained. The strategy for picking the eviction key\nmay be as simple as round robin, though more sophisticated\nstrategies to minimize key overlap with the active run in the\nnext level are possible so as to minimize merge overheads\n[92].static runs and eventually empty them out.\nFractional Cascading Maintenance. As merge operations\ntake place at cold levels, cascading fence pointers must be\nmaintained to keep runs connected. As an active run grad-\nually \flls up, we must embed cascading fence pointers from\nwithin the active run at the next smaller level. We must\nalso create cascading fence pointers from a new active run\ninto the next older static run at each level. To manage this,\nwhenever we create a new run, we also create a block in-\ndexin storage to correspond to the fences for this new run.\nWhenever we need to embed pointers into a Run ifrom\nsome new Run jas Runjis being created, we include the\nblock index for Run iin the sort-merge operation used to\ncreate Run jto embed the cascading fence pointers within\nthe constituent nodes of that run.\nUni\fed Cost Model. A design continuum includes a\ncost model with a closed-form equation for each one of the\ncore performance metrics. The bottom right part of Figure\n3 depicts these models for our example continuum. These\ncost models measure the worst-case number of I/Os issued\nfor each of the operation types, the reason being that I/O\nis typically the performance bottleneck for key-value stores\n\nParameters MetricsTermsDesignsTiered LSM-\nTree [60,\n24, 25, 46]Lazy Leveled\nLSM-Tree\n[27, 25]Leveled LSM-\nTree [35,\n32, 24, 25]COLA [16] FD-Tree [63] B\u0000Tree [17, 16,\n47, 76, 10, 48]B+Tree [14]\nT (Growth\nFactor)[2,B] [2,B] [2,B] 2 [2,B] [2,B] B\nK (Hot Merge\nThreshold)T\u00001 T\u00001 1 1 1 1 1\nZ (Cold Merge\nThreshold)T\u00001 1 1 1 1 1 1\nD (Max.\nNode Size)[1,N\nB] [1,N\nB] [1,N\nB]N\nBN\nB1 1\nMF(Fence &\nFilter Mem.)N ·(F\nB+10) N ·(F\nB+10) N ·(F\nB+10)F·T·MB\nE·BF·T·MB\nE·BF·T·MB\nE·BF·T·MB\nE·B\nUpdate O(L\nB) O(1\nB·(T+L)) O(T\nB·L) O(L\nB) O(T\nB·L) O(T\nB·L) O(L)\nZero Result\nLookupO(T·e\u0000MBF\nN ) O(e\u0000MBF\nN ) O(e\u0000MBF\nN ) O(L) O(L) O(L) O(L)\nExisting\nLookupO(1+T·e\u0000MBF\nN ) O(1) O(1) O(L) O(L) O(L) O(L)\nShort Scan O(L·T) O(1+T·(L\u00001)) O(L) O(L) O(L) O(L) O(L)\nLong Scan O(T·s\nB) O(s\nB) O(s\nB) O(s\nB) O(s\nB) O(s\nB) O(s\nB)Figure 4: Instances of the design continuum and examples of their derived cost metrics.\nthat store a larger amount of data than can \ft in memory.2\nFor example, the cost for point reads is derived by adding\nthe expected number of I/Os due to false positives across\nthe hot levels (given by the Equation for p, the sum of the\nFPRs [27]) to the number of runs at the cold levels, since\nwith fractional cascading we perform 1 I/O for each run.\nAs another example, the cost for writes is derived by ob-\nserving that an application update gets copied on average\nO(T=K) times at each of the hot levels (except the largest)\nandO(T=Z) times at the largest hot level and at each of the\ncold levels. We amortize by adding these costs and dividing\nby the block size Bas a single write I/O copies Bentries\nfrom the original runs to the resulting run.\nWhile our models in this work are expressed in terms of\nasymptotic notations, we have shown in earlier work that\nsuch models can be captured more precisely to reliably pre-\ndict worst-case performance [24, 27]. A central advantage of\nhaving a set of closed-form set of models is that they allow\nus to see how the di\u000berent knobs interplay to impact per-\nformance, and they reveal the trade-o\u000bs that the di\u000berent\nknobs control.\nOverall, the choice of the design parameters and the deriva-\ntion rules represent the infusion of expert design knowledge\nin order to create a navigable design continuum. Speci\f-\ncally, fewer design parameters (for the same target de-\nsigns) lead to a cleaner abstraction which in turn makes it\neasier to come up with algorithms that automatically \fnd\nthe optimal design (to be discussed later on). We minimize\n2Future work can also try to generate in-memory design\ncontinuums where we believe learned cost models that help\nsynthesize the cost of arbitrary data structure designs can\nbe a good start [45].the number of design parameters in two ways: 1) by adding\ndeterministic design rules which encapsulate expert knowl-\nedge about what is a good design, and 2) by collapsing more\nthan one interconnected design decisions to a single design\nparameter. For example, we used a single parameter for\nthe memory budget of Bloom \flters and fence pointers as\nwe assume their co-existence since fence pointers accelerate\nboth point lookups and range queries, while Bloom \flters\naccelerate point lookups exclusively. We specify a design\nstrategy that progressively adds more point query bene\ft as\nthe memory for fences and \flters increases by prioritizing\nfence budget before \flter budget at each level, accordingly\njointly budgeting fences and \flters together as one design\nknob.\nDesign Instances. Figure 4 depicts several known in-\nstances of data structure designs as they are derived from\nthe continuum. In particular, the top part of Figure 4 shows\nthe values for the design knobs that derive each speci\fc de-\nsign, and the bottom part shows how their costs can indeed\nbe derived from the generalized cost model of the contin-\nuum. Additional example diagrams of super-structure in-\nstantiations are provided in Appendix A Figure\nFor example, a B+tree is instantiated by (1) setting the\nmaximum node size Dto be one block3, (2) setting KandZ\nto 1 so that all nodes within a level are globally sorted, (3)\nsettingMFto the minimum amount of memory so that Lev-\nels 1 to L get traversed using fractional cascading without\nthe utilization of Bloom \flters or in-memory fence pointers,\nand (4) setting the growth factor to be equal to the block\nsize. By plugging the values of these knobs into the cost ex-\n3Node size can be set to whatever we want the B+tree node\nsize to be - we use D= 1 block here as an example only.\n\ndesign spaceLSMbLSMdesignparametersdesign continuum\n    Modellingperformance continuum\npoint readDelete\nPutpareto curvesB-TreeBeTree\ngrowth factorﬁlters memory node sizelevelsFGUKIP\u0002RCTCOGVGTUEQODKPCVQTKCN\u0002URCEG\u0002QH\u0002FGTKXGF\u0002FGUKIPUdesign rulesHKNVGT\u0002RQUUKDNG\u0002FGUKIPUbound parametersFigure 5: Constructing a design continuum: from design parameters to a performance hyperplane.\npressions, the well-known write and read costs for a B+tree\nofO(L) I/Os immediately follow.\nAs a second example, a leveled LSM-tree design is instan-\ntiated by (1) setting KandZto 1 so that there is at most\none run at each level, and (2) assigning enough memory\nto the knob MFto enable fence pointers and Bloom \flters\n(with on average 10 bits per entry in the table) for all lev-\nels. We leave the knobs DandTas variables in this case as\nthey are indeed used by modern leveled LSM-tree designs to\nstrike di\u000berent trade-o\u000bs. By plugging in the values for the\ndesign knobs into the cost models, we immediately obtain\nthe well-known costs for a leveled LSM-tree. For example,\nwrite cost simpli\fes to O(T\u0001L\nB) as every entry gets copied\nacrossO(L) levels and on average O(T) times within each\nlevel.\nConstruction Summary. Figure 5 summarizes the pro-\ncess of constructing a design continuum. We start by select-\ning a set of data structures. Then we select the minimum\nset of design knobs that can instantiate these designs and\nwe impose design rules and domain restrictions to restrict\nthe population of the continuum to only the best designs\nwith respect to our target cost criteria. Finally, we derive\nthe generalized cost models.\nDe\fnition of Continuum. We can now revisit the ex-\nact de\fnition of the continuum. A design continuum con-\nnects previously distinct and seemingly fundamentally dif-\nferent data structure designs. The construction process does\nnot necessarily result in continuous knobs in the mathemat-\nical sense (most of the design knobs have integer values).\nHowever, from a design point of view a continuum opens\nthe subspace in between previously unconnected designs; it\nallows us to connect those discrete designs in \fne grained\nsteps, and this is exactly what we refer to as the \\design\ncontinuum\". The reason that this is critical is that it allows\nus to 1) \\see\" designs that we did not know before, derived\nas combinations of those \fne-grained design options, and\n2) build techniques that smoothly transition across discrete\ndesigns by using those intermediate states.\n2.2 Interactive Design\nThe generalized cost models enable us to navigate the con-\ntinuum, i.e., interactively design a data structure for a key-\nvalue store with the optimal con\fguration for a particular\napplication as well as to react to changes in the environ-\nment, or workload. We formalize the navigation process by\nintroducing Equation 1 to model the average operation cost\n\u0012through the costs of zero-result point lookups R, non-zero-\nresult point lookups V, short range lookups Q, long range\nlookupsC, and updates W(the coe\u000ecients depict the pro-\nportions of each in the workload).\u0012= (r\u0001R+v\u0001V+q\u0001Q+c\u0001C+w\u0001W) (1)\nTo design a data structure using Equation 1, we \frst iden-\ntify the bottleneck as the highest additive term as well as\nwhich knobs in Figure 3 can be tweaked to alleviate it. We\nthen tweak the knob in one direction until we reach its\nboundary or until \u0012reaches the minimum with respect to\nthat parameter. We then repeat this process with other pa-\nrameters as well as with other bottlenecks that can emerge\nduring the process. This allows us to converge to the opti-\nmal con\fguration without backtracking, which allows us to\nadjust to a variety of application scenarios reliably. For ex-\nample, consider an application with a workload consisting of\npoint lookups and updates and an initial con\fguration of a\nlazy-leveled LSM-tree with T= 10,K=T\u00001,Z= 1,D= 64,\nMBset to 2 MB, and Mfset toN\u0001(F=B + 10) , meaning we\nhave memory for all the fence pointers and in addition 10\nbits per entry for Bloom \flters. We can now use the cost\nmodels to react to di\u000berent scenarios.\nScenario 1: Updates Increasing. Suppose that the propor-\ntion of updates increases, as is the case for many applica-\ntions [84]. To handle this, we \frst increase Zuntil we reach\nthe minimum value for \u0012or until we reach the maximum\nvalue ofZ. If we reach the maximum value of Z, the next\npromising parameter to tweak is the size ratio T, which we\ncan increase in order to decrease the number of levels across\nwhich entries get merged. Again, we increase Tuntil we hit\nits maximum value or reach a minimum value for \u0012.\nScenario 2: Range Lookups. Suppose that the application\nchanges such that short-range lookups appear in the work-\nload. To optimize for them, we \frst decrease Kto restrict\nthe number of runs that lookups need to access across Levels\n1 toL\u00001. If we reach the minimum value of Kand short-\nrange lookups remain the bottleneck, we can now increase T\nto decrease the overall number of levels thereby decreasing\nthe number of runs further.\nScenario 3: Data Size Growing. Suppose that the size of\nthe data is growing, yet most of the lookups are targeting the\nyoungestNyoungest entries, and we do not have the resources\nto continue scaling main memory in proportion to the overall\ndata sizeN. In such a case, we can \fx MftoNyoungest\u0001\n(F=B + 10) to ensure memory is invested to provide fast\nlookups for the hot working set while minimizing memory\noverhead of less frequently requested data by maintaining\ncold levels with fractional cascading.\nE\u000bectively the above process shows how to quickly and\nreliably go from a high-level workload requirement to a low-\nlevel data structure design con\fguration at interactive times\nusing the performance continuum.\nAuto-Design. It is possible to take the navigation pro-\ncess one step further to create algorithms that iterate over\n\nthe continuum and independently \fnd the best con\fgura-\ntion. The goal is to \fnd the best values for T,K,Z,D, and\nthe best possible division of a memory budget between MF\nandMB. While iterating over every single con\fguration\nwould be intractable as it would require traversing every\npermutation of the parameters, we can leverage the man-\nner in which we constructed the continuum to signi\fcantly\nprune the search space. For example, in Monkey [24], when\nstudying a design continuum that contained only a limited\nset of LSM-tree variants we observed that two of the knobs\nhave a logarithmic impact on \u0012, particularly the size ratio T\nand the memory allocation between MbandMf. For such\nknobs, it is only meaningful to examine a logarithmic num-\nber of values that are exponentially increasing, and so their\nmultiplicative contribution to the overall search time is log-\narithmic in their domain. While the continuum we showed\nhere is richer, by adding B-tree variants, this does not add\nsigni\fcant complexity in terms of auto-design. The decision\nto use cascading fence pointers or in-memory fence point-\ners completely hinges on the allocation of memory between\nMFandMB, while the node size Dadds one multiplicative\nlogarithmic term in the size of its domain.\n2.3 Success Criteria\nWe now outline the ideal success criteria that should guide\nthe construction of elegant and practically useful design con-\ntinuums in a principled approach.\nFunctionally Intact. All possible designs that can be\nassumed by a continuum should be able to correctly sup-\nport all operation types (e.g., writes, point reads, etc.). In\nother words, a design continuum should only a\u000bect the per-\nformance properties of the di\u000berent operations rather than\nthe results that they return.\nPareto-Optimal. All designs that can be expressed should\nbe Pareto-optimal with respect to the cost metrics and work-\nloads desired. This means that there should be no two de-\nsigns such that one of them is better than the other on one\nor more of the performance metrics while being equal on\nall the others. The goal of only supporting Pareto-optimal\ndesigns is to shrink the size of the design space to the min-\nimum essential set of knobs that allow to control and nav-\nigate across only the best possible known trade-o\u000bs, while\neliminating inferior designs from the space.\nBijective. A design continuum should be a bijective (one-\nto-one) mapping from the domain of design knobs to the\nco-domain of performance and memory trade-o\u000bs. As with\nPareto-Optimality, the goal with bijectivity is to shrink a\ndesign continuum to the minimal set of design knobs such\nthat no two designs that are equivalent in terms of perfor-\nmance can be expressed as di\u000berent knob con\fgurations. If\nthere are multiple designs that map onto the same trade-o\u000b,\nit is a sign that the model is either too large and can be\ncollapsed onto fewer knobs, or that there are core metrics\nthat we did not yet formalize, and that we should.\nDiverse. A design continuum should enable a diverse\nset of performance properties. For Pareto-Optimal and bi-\njective continuums, trade-o\u000b diversity can be measured and\ncompared across di\u000berent continuums as the product of the\ndomains of all the design knobs, as each unique con\fguration\nleads to a di\u000berent unique and Pareto-optimal trade-o\u000b.\nNavigable. The time complexity required for navigat-\ning the continuum to converge onto the optimal (or even\nnear-optimal) design should be tractable. With the Monkeycontinuum, for example, we showed that it takes O(logT(N))\niterations to \fnd the optimal design [24], and for Dosto-\nevsky, which includes more knobs and richer trade-o\u000bs, we\nshowed that it takes O(logT(N)3)iterations [27]. Measuring\nnavigability complexity in this way allows system designers\nfrom the onset to strike a balance between the diversity vs.\nthe navigability of a continuum.\nLayered. By construction, a design continuum has to\nstrike a trade-o\u000b between diversity and navigability. The\nmore diverse a continuum becomes through the introduc-\ntion of new knobs to assume new designs and trade-o\u000bs,\nthe longer it takes to navigate it to optimize for di\u000berent\nworkloads. With that in mind, however, we observe that de-\nsign continuums may be constructed in layers, each of which\nbuilds on top of the others. Through layered design, di\u000ber-\nent applications may use the same continuum but choose\nthe most appropriate layer to navigate and optimize perfor-\nmance across. For example, the design continuum in Dos-\ntoevsky [24] is layered on top of Monkey [27] by adding two\nnew knobs, KandZ, to enable intermediate designs be-\ntween tiering, leveling and lazy leveling. While Dostoevsky\nrequiresO(logT(N)3)iterations to navigate the possible de-\nsigns, an alternative is to leverage layering to restrict the\nknobsKandZto both always be either 1 or T\u00001 (i.e.,\nto enable only leveling and tiering) in order to project the\nMonkey continuum and thereby reduce navigation time to\nO(logT(N)). In this way, layered design enables continuum\nexpansion with no regret : we can continue to include new de-\nsigns in a continuum to enable new structures and trade-o\u000bs,\nall without imposing an ever-increasing navigation penalty\non applications that need only some of the possible designs.\n2.4 Expanding a Continuum:\nA Case-Study with LSH-table\nWe now demonstrate how to expand the continuum with\na goal of adding a particular design to include certain per-\nformance trade-o\u000bs. The goal is to highlight the design con-\ntinuum construction process and principles.\nOur existing continuum does not support the LSH-table\ndata structure used in many key-value stores such as BitCask\n[86], FASTER [20], and others [2, 64, 80, 88, 95]. LSH-\ntable achieves a high write throughout by logging entries in\nstorage, and it achieves fast point reads by using a hash table\nin memory to map every key to the corresponding entry in\nthe log. In particular, LSH-table supports writes in O(1=B)\nI/O, point reads in O(1) I/O, range reads in O(N)I/O, and\nit requires O(F\u0001N)bits of main memory to store all keys\nin the hash table. As a result, it is suitable for write-heavy\napplication with ample memory, and no range reads.\nWe outline the process of expanding our continuum in\nthree steps: bridging, patching, and costing .\nBridging. Bridging entails identifying the least number\nof new movable design abstractions to introduce to a con-\ntinuum to assume a new design. This process involves three\noptions: 1) introducing new design rules, 2) expanding the\ndomains of existing knobs, and 3) adding new design knobs.\nBridging increases the diversity of a design continuum,\nthough it risks compromising the other success metrics. De-\nsigners of continuums should experiment with the three steps\nabove in this particular order to minimize the chance of that\nhappening. With respect to LSH-table, we need two new ab-\nstractions: one to allow assuming a log in storage, and one\nto allow assuming a hash table in memory.\n\nSuper-Structure\n…Buﬀer FiltersFence \nPointers\nMemoryStorage\nFP\nFP\nFP …\n…\n……\n…node\nrun boundary\nfence pointerstorage block\nFP\nFP\n…L-Y hot levels\nY cold levelsFP\n…FiltersFilters\nBFBloom \nFilter\nFilter Choice by Mem. BudgetHash \nTableor\n(Fewer Bits per Key) (Full Key Size)…oldest hot run contains\ncascading fence pointersOperation Cost Expression (I/O)\nUpdate1\nB·(T\u00001\nK+1·(L\u0000Y\u00001)+T\nZ·(Y+1))\nZero Result Lookup p+Y·(Z+T\nB)\nSingle Result Lookup 1+p+Y·(Z+T\nB)\u0000pL·Z+1\n2\nShort Scan O(K·(L\u0000Y\u00001)+(Y+1)·(Z+T\nB))\nLong Scan O(s·Z\nB)General Cost ModelFigure 6: Extending the design continuum to support Log Structured Hash table designs.\nTo assume a log in storage, our insight is that with a tiered\nLSM-tree design, setting the size ratio to increase with re-\nspect to the number of runs at Level 1 (i.e., T=(N\u0001E)=MB)\ncauses Level 1 to never run out of capacity. This e\u000bectively\ncreates a log in storage as merge operations never take place.\nOur current design continuum, however, restricts the size ra-\ntio to be at most B. To support a log, we expand the domain\nof the size ratio with a new maximum value of (N\u0001E)=MB.\nTo assume a hash table in memory, recall that our contin-\nuum assigns more bits per entry for Bloom \flters at smaller\nlevels. Our insight is that when the number of bits per entry\nassigned to given level exceeds the average key size F, it is\nalways bene\fcial to replace the Bloom \flters at that level\nwith an in-memory hash table that contains all keys at the\nlevel. The reason is that a hash table takes as much memory\nas the Bloom \flters would, yet it is more precise as it does\nnot allow false positives at all. We therefore introduce a new\ndesign rule whereby levels with enough memory to store all\nkeys use a hash table while levels with insu\u000ecient memory\nuse Bloom \flters4. With these two new additions to the\ncontinuum, we can now set the size ratio to(N\u0001E)=MBandK\nandZtoT\u00001while procuring at least F\u0001Nbits of memory\nto our system to assume LSH-table5. Figure 6 shows the\nnew super-structure of the continuum while Figure 7 shows\nhow LSH-table can be derived.\nAn important point is that we managed to bridge LSH-\ntable with our continuum without introducing new design\nknobs. As a rule of thumb, introducing new knobs for bridg-\ning should be a last resort as the additional degrees of free-\ndom increase the time complexity of navigation. Our case-\nstudy here, however, demonstrates that even data structures\nthat seem very di\u000berent at the onset can be bridged by \fnd-\ning the right small set of movable abstractions.\nPatching. Since the bridging process introduces many\nnew intermediate designs, we follow it with a patching pro-\ncess to ensure that all of the new designs are functionally\nintact (i.e., that they can correctly support all needed types\nof queries). Patching involves either introducing new design\nrules to \fx broken designs or adding constraints on the do-\nmains of some of the knobs to eliminate broken designs from\nthe continuum. To ensure that the expanded continuum is\nlayered (i.e., that it contains all designs from the contin-\nuum that we started out with), any new design rules or con-\n4Future work toward an even more navigable continuum can\nattempt to generalize a Bloom \flter and a hash table into\none uni\fed model with continuous knobs that allows to grad-\nually morph between these structures based on the amount\nof main memory available.\n5More precisely, F\u0001N\u0001(1 +1\nB) bits of memory are needed\nto support both the hash table and fence pointers.\nParameters MetricsTermsDesignsLog LSH Table\n[86, 20, 88,\n80, 64, 2, 95]Sorted Array\nT (Growth\nFactor)N·E\nMBN·E\nMBN·E\nMB\nK (Hot Merge\nThreshold)T\u00001 T\u00001 1\nZ (Cold Merge\nThreshold)T\u00001 T\u00001 1\nD (Max.\nNode Size)N\nBN\nBN\nB\nMF(Fence &\nFilter Mem.)N·F\nBN ·F·(1+1\nB)N·F\nB\nUpdate O(1\nB) O(1\nB) O(N·E\nMB·B)\nZero Result\nLookupO(N·E\nMB) O(0) O(1)\nSingle Result\nLookupO(N·E\nMB) O(1) O(1)\nShort\nRange ScanO(N·E\nMB) O(N·E\nMB) O(1)\nLong\nRange ScanO(N·E\nMB·s\nB) O(N·E\nMB·s\nB) O(s\nB)Figure 7: Instances of the extended design continuum and\nexamples of their derived cost metrics.\nstraints introduced by patching should only a\u000bect new parts\nof the continuum. Let us illustrate an example of patching\nwith the expanded continuum.\nThe problem that we identify arises when fractional cas-\ncading is used between two cold Levels iandi+ 1 while the\nsize ratioTis set to be greater than B. In this case, there\nis not enough space inside each block at Level ito store all\npointers to its children blocks (i.e., ones with an overlap-\nping key range) at Level i+ 1. The reason is that a block\ncontainsBslots for pointers, and so a block at Level ihas\na greater number of children Tthan the number of pointer\nslots available. Worse, if the node size Dis set to be small\n(in particular, when D <T=B), some of the blocks at Level\ni+ 1 will neither be pointed to from Level inor exist within\na node whereon at least one other block is pointed to from\nLeveli. As a result, such nodes at Level i+1 would leakout\nof the data structure, and so the data on these blocks would\nbe lost. To prevent leakage, we introduce a design rule that\nwhenD <T=BandB <T , the setting at which leakage can\noccur, we add sibling pointers to reconnect nodes that have\nleaked. We introduce a rule that the parent block's point-\n\ners are spatially evenly distributed across its children (every\n(T=(B\u0001D))thnode at Level i+ 1 is pointed to from a block at\nleveli) to ensure that all sibling chains of nodes within Level\ni+ 1 have an equal length. As these new rules only apply to\nnew parts of our continuum (i.e., when T >B ), they do not\nviolate layering.\nCosting. The \fnal step is to generalize the continuum's\ncost model to account for all new designs. This requires\neither extending the cost equations and/or proving that the\nexisting equations still hold for the new designs. Let us\nillustrate two examples. First, we extend the cost model\nwith respect to the patch introduced above. In particular,\nthe lookup costs need to account for having to traverse a\nchain of sibling nodes at each of the cold levels when T >B .\nAs the length of each chain isT=Bblocks, we extend the\ncost equations for point lookups and short-range lookups\nwith additionalT=BI/Os per each of the Ycold levels. The\nextended cost equations are shown in Figure 6.\nIn the derivation below, we start with general cost ex-\npression for point lookups in Figure 6 and show how the\nexpected point lookup cost for LSH-table is indeed derived\ncorrectly. In Step 2, we plug inN=BforTandZto assume a\nlog in storage. In Step 3, we set the number of cold levels to\nzero as Level 1 in our continuum by construction is always\nhot and in this case, there is only one level (i.e., L= 1), and\nthusYmust be zero. In Step 4, we plug in the key size Ffor\nthe number of bits per entry for the Bloom \flters, since with\nLSH-table there is enough space to store all keys in memory.\nIn Step 5, we reason that the key size Fmust comprise on\naverage at least log(N) bits to represent all unique keys. In\nStep 6, we simplify and omit small constants to arrive at a\ncost ofO(1) I/O per point lookup.\n2O(1 +Z\u0001e\u0000(MBF=N)\u0001TY+Y\u0001(Z+T=B))\n2O(1 + N=B\u0001e\u0000(MBF=N)\u0001(N=B)Y+Y\u0001(N=B+N=B2)) (2)\n2O(1 + N=B\u0001e\u0000(MBF=N)) (3)\n2O(1 + N=B\u0001e\u0000F) (4)\n2O(1 + N=B\u0001e\u0000log2(N)) (5)\n2O(1) (6)\n2.5 Elegance Vs. Performance:\nTo Expand or Not to Expand?\nAs new data structures continue to get invented and opti-\nmized, the question arises of when it is desirable to expand a\ndesign continuum to include a new design. We show through\nan example that the answer is not always clear cut.\nIn an e\u000bort to make B-trees more write-optimized for \rash\ndevices, several recent B-tree designs bu\u000ber updates in mem-\nory and later \rush them to a log in storage in their arrival\norder. They further use an in-memory indirection table to\nmap each logical B-tree node to the locations in the log that\ncontain entries belonging to that given node. This design\ncan improve on update cost relative to a regular B-tree by\n\rushing multiple updates that target potentially di\u000berent\nnodes with a single sequential write. The trade-o\u000b is that\nduring reads, multiple I/Os need to be issued to the log\nfor every logical B-tree node that gets traversed in order to\nfetch its contents. To bound the number of I/Os to the log,\na compaction process takes place once a logical node spans\noverCblocks in the log, where Cis a tunable parameter.\n051015202530354045reads (I/O)\n0.0 0.5 1.0 1.5 2.0\nwrite cost (amortized weighted I/O)LSB-tree (point & short range reads)\nLeveled LSM-tree short range reads\nLeveled LSM-tree point readsFigure 8: Leveled LSM-tree dominates LSB-tree for most of\ntheir respective continuums.\nOverall, this design leads to a point and range read cost of\nO(C\u0001logB(N))I/Os. On the other hand, update cost con-\nsists ofO(C\u0001logB(N))read I/Os to \fnd the target leaf node\nand an additional amortized O(1=C)write I/Os to account\nfor the overheads of compaction. The memory footprint for\nthe mapping table is O((C\u0001N\u0001F)=B)bits. We refer to this de-\nsign as log-structured B-tree (LSB-tree). Would we bene\ft\nfrom including LSB-tree in our continuum?\nTo approach an answer to this question, we analytically\ncompare LSB-tree against designs within our continuum to\ngauge the amount by which LSB-tree would allow us to\nachieve better trade-o\u000bs with respect to our continuum's\ncost metrics. We demonstrate this process in Figure 8, which\nplots point and range read costs against write cost for both\nLSB-tree and Leveled LSM-tree, a representative part of our\ncontinuum. To model write cost for LSB-tree, we computed\na weighted cost of O(C\u0001logB(N))read I/Os to traverse the\ntree,O(1=C)write I/Os to account for compaction overheads,\nand we discounted the cost of a read I/O relative to a write\nI/O by a factor of 20 to account for read/write cost asymme-\ntries on \rash devices. We generated the curve for LSB-tree\nby varying the compaction factor Cfrom 1 to 9, and the\ncurves for the LSM-tree by varying the size ratio Tfrom 2 to\n10. To enable an apples-to-apples comparison whereby both\nLSB-tree and the LSM-tree have the same memory budget,\nwe assigned however much main memory LSB-tree requires\nfor its mapping table to the LSM-tree's fence pointers and\nBloom \flters. Overall, the \fgure serves as a \frst approx-\nimation for the trade-o\u000bs that LSB-tree would allow us to\nachieve relative to our continuum.\nFigure 8 reveals that point read cost for the LSM-tree is\nmuch lower than for LSB-tree. The reason is that when\nthe same amount of memory required by LSB-tree's mem-\nory budget is used for the LSM-tree's fence pointers and\nBloom \flters, hardly any false positives take place and so\nthe LSM-tree can answer most point reads with just one\nI/O. Secondly, we observe that as we increase LSB-tree's\ncompaction factor C, write cost initially decreases but then\nstarts degrading rapidly. The reason is that as Cgrows,\nmore reads I/Os are required by application writes to tra-\nverse the tree to identify the target leaf node for the write.\nOn the other hand, for range reads there is a point at which\nLSB-tree dominates the LSM-tree as fewer blocks need to\nbe accessed when Cis small.\nElegance and Navigability versus Absolute Perfor-\nmance. By weighing the advantages of LSB-tree against\nthe complexity of including it (i.e., adding movable abstrac-\n\ntions to assume indirection and node compactions), one can\ndecide to leave LSB-tree out of the continuum. This is be-\ncause its design principles are fundamentally di\u000berent than\nwhat we had included and so substantial changes would be\nneeded that would complicate the continuum's construction\nand navigability. On the other hand, when we did the expan-\nsion for LSH-table, even though, it seemed initially that this\nwas a fundamentally di\u000berent design, this was not the case:\nLSH-table is synthesized from the same design principles we\nalready had in the continuum, and so we could achieve the\nexpansion in an elegant way at no extra complexity and with\na net bene\ft of including the new performance trade-o\u000bs.\nAt the other extreme, one may decide to include LSB-tree\nbecause the additional performance trade-o\u000bs outweigh the\ncomplexity for a given set of desired applications. We did\nthis analysis to make the point of elegance and navigabil-\nity versus absolute performance. However, we considered a\nlimited set of performance metrics, i.e., worst-case I/O per-\nformance for writes, point reads and range reads. Most of\nthe work on LSB-tree-like design has been in the context\nof enabling better concurrency control [62] and leveraging\nworkload skew to reduce compaction frequency and over-\nheads [96]. Future expansion of the design space and contin-\nuums should include such metrics and these considerations\ndescribed above for the speci\fc example will be di\u000berent. In\nthis way, the decision of whether to expand or not to expand\na continuum is a continual process, for which the outcome\nmay change over time as di\u000berent cost metrics change in\ntheir level of importance given target applications.\n3. WHY NOT MUTUALLY EXCLUSIVE\nDESIGN COMPONENTS?\nMany modern key-value stores are composed of mutually\nexclusive sets of swappable data layout designs to provide di-\nverse performance properties. For example, WiredTiger sup-\nports separate B-tree and LSM-tree implementations to op-\ntimize more for reads or writes, respectively, while RocksDB\n\fles support either a sorted strings layout or a hash table\nlayout to optimize more for range reads or point reads, re-\nspectively. A valid question is how does this compare to the\ndesign continuum in general? And in practice how does it\ncompare to the vision of self-designing key-value stores?\nAny exposure of data layout design knobs is similar in\nspirit and goals to the continuum but how it is done exactly\nis the key. Mutually exclusive design components can be in\npractice a tremendously useful tool to allow a single system\nto be tuned for a broader range of applications than we\nwould have been able to do without this feature. However,\nit is not a general solution and leads to three fundamental\nproblems.\n1) Expensive Transitions. Predicting the optimal set\nof design components for a given application before deploy-\nment is hard as the workload may not be known precisely. As\na result, components may need to be continually reshu\u000fed\nduring runtime. Changing among large components during\nruntime is disruptive as it often requires rewriting all data.\nIn practice, the overheads associated with swapping compo-\nnents often force practitioners to commit to a suboptimal\ndesign from the onset for a given application scenario.\n2) Sparse Mapping to Performance Properties. An\neven deeper problem is that mutually exclusive design com-\nponents tend to have polar opposite performance properties(e.g., hash table vs. sorted array). Swapping between two\ncomponents to optimize for one operation type (e.g. point\nreads) may degrade a di\u000berent cost metric (e.g. range reads)\nby so much that it would o\u000bset the gain in the \frst metric\nand lead to poorer performance overall. In other words,\noptimizing by shu\u000fing components carries a risk of over-\nshooting the target performance properties and hitting the\npoint of diminishing returns. A useful way of thinking about\nthis problem is that mutually exclusive design components\nmap sparsely onto the space of possible performance proper-\nties. The problem is that, with large components, there are\nno intermediate designs that allow to navigate performance\nproperties in smaller steps.\n3) Intractable Modeling. Even analytically, it quickly\nbecomes intractable to reason about the tens to hundreds\nof tuning parameters in modern key-value stores and how\nthey interact with all the di\u000berent mutually exclusive design\ncomponents to lead to di\u000berent performance properties. An\nentirely new performance model is often needed for each\npermutation of design components, and that the number of\npossible permutations increases exponentially with respect\nto the number of components available. Creating such an\nextensive set of models and trying to optimize across them\nquickly becomes intractable. This problem gets worse as\nsystems mature and more components get added and it boils\ndown to manual tuning by experts.\nThe Design Continuum Spirit. Our work helps with\nthis problem by formalizing this data layout design space\nso that educated decisions can be made easily and quickly,\nsometimes even automatically. Design continuums deal with\neven more knobs than what existing systems expose because\nthey try to capture the fundamental design principles of de-\nsign which by de\fnition are more \fne-grained concepts. For\nexample, a sorted array is already a full data structure that\ncan be synthesized out of many smaller decisions. However,\nthe key is that design continuums know how to navigate\nthose \fne-grained concepts and eventually expose to design-\ners a much smaller set of knobs and a way to argue directly at\nthe performance tradeo\u000b level. The key lies in constructing\ndesign continuums and key-value store systems via uni\fed\nmodels and implementations with continuous data layout\ndesign knobs rather than swappable components.\nFor example, the advantage of supporting LSH-table by\ncontinuum expansion rather than as an independent swap-\npable component is that the bridging process adds new in-\ntermediate designs into the continuum with appealing trade-\no\u000bs in-between. The new continuum allows us to gradually\ntransition from a tiered LSM-tree into LSH-table by increas-\ning the size ratio in small increments to optimize more for\nwrites at the expense of range reads and avoid overshooting\nthe optimal performance properties when tuning.\n4. ENHANCING CREATIVITY\nBeyond the ability to assume existing designs, a contin-\nuum can also assist in identifying new data structure designs\nthat were unknown before, but they are naturally derived\nfrom the continuum's design parameters and rules.\nFor example, the design continuum we presented in this\npaper allows us to synthesize two new subspaces of hybrid\ndesigns, which we depict in Figure 9. The \frst new subspace\nextends the B\u000ftree curve to be more write-optimized by in-\ncreasingZandKto gather multiple linked nodes at a level\nbefore merging them. The second subspace connects B\u000ftree\n\nupdate cost (I/O)O1𝐵∙log2𝑁𝐵∙𝐷Olog𝐵𝑁𝐵∙𝐷single result lookup cost (I/O)\nO1O1Olog2𝑁𝐵∙𝐷Olog𝐵𝑁𝐵∙𝐷B+-treeBe-tree with 𝑀𝐹=0LSM-tree with 𝑀𝐹=𝑁∙(𝐹𝐵+10)new designs\nO𝐵∙log𝐵𝑁𝐵∙𝐷O1+𝐵∙𝑒2𝑀𝐵𝐹/𝑁\nO1𝐵∙log𝐵𝑁𝐵∙𝐷Figure 9: Visualizing the performance continuum.\nwith LSM-tree designs, allowing \frst to optimize for writes\nand lookups at hot levels by using Bloom \flters and fence\npointers, and second to minimize memory investment at cold\nlevels by using fractional cascading instead. Thus, we turn\nthe design space into a multi-dimensional space whereon ev-\nery point maps onto a unique position along a hyperplane of\nPareto-optimal performance trade-o\u000bs (as opposed to hav-\ning to choose between drastically di\u000berent designs only).\nIn addition, as the knobs in a bijective continuum are di-\nmensions that interact to yield unique designs, expanding\nany knob's domain or adding new knobs during the bridg-\ning process can in fact enrich a continuum with new, good\ndesigns that were not a part of the original motivation for\nexpansion. Such examples are present in our expanded con-\ntinuum where our original goal was to include LSH-table.\nFor example, \fxing KandZto 1 and increasing the size\nratio beyond BtowardsN=(D\u0001P)allows us to gradually tran-\nsition from a leveled LSM-tree into a sorted array (as even-\ntually there is only one level). This design was not possible\nbefore, and it is bene\fcial for workloads with many range\nreads. In this way, the bridging process makes a continuum\nincreasingly rich and powerful.\nWhat is a new Data Structure? There are a number\nof open questions this work touches on. And some of these\nquestions become even philosophical. For example, if all\ndata structures can be described as combinations of a small\nset of design principles, then what constitutes a new data\nstructure design? Given the vastness of the design space, we\nthink that the discovery of any combination of design princi-\nples that brings new and interesting performance properties\nclassi\fes as a new data structure. Historically, this has been\nthe factor of recognizing new designs as worthy and inter-\nesting even if seemingly \\small\" di\u000berences separated them.\nFor example, while an LSM-tree can simply be seen as a\nsequence of unmerged B-trees, the performance properties\nit brings are so drastically di\u000berent that it has become its\nown category of study and whole systems are built around\nits basic design principles.\n5. THE PATH TO SELF-DESIGN\nKnowing which design is the best for a workload opens\nthe opportunity for systems that can adapt on-the-\ry. While\nadaptivity has been studied in several forms including adapt-\ning storage to queries [41, 4, 12, 49, 43, 30, 40, 66], the\nnew opportunity is morphing among what is typically con-\nsidered as fundamentally di\u000berent designs, e.g., from an\nFigure 10: Potential bene\ft of on-the-\ry transitions between\nB+tree and LSM-tree.\nLSM-tree to a B+tree, which can allow systems to grace-\nfully adapt to a larger array of diverse workload patterns.\nDesign continuums bring such a vision a small step closer be-\ncause of two reasons: 1) they allow quickly computing the\nbest data structure design (out of a set of possible designs),\nand 2) by knowing intermediate data structure designs that\ncan be used as transition points in-between \\distant\" designs\n(among which it would otherwise be too expensive to tran-\nsition).\nThere are (at least) three challenges on the way to such\nself-designing systems: a) designing algorithms to physically\ntransition among any two designs, b) automatically materi-\nalizing the needed code to utilize diverse designs, and c) re-\nsolving fundamental system design knobs beyond layout de-\ncisions that are hard to encapsulate in a continuum. Below\nwe brie\ry touch on these research challenges, and we show\nhints that they are likely possible to be resolved.\nTransitions. As in all adaptive studies, we need to con-\nsider the cost of a transition. The new challenge here is\ntransitioning among fundamentally di\u000berent designs. For\nexample, assume a transition between a Leveled LSM-tree\nand B+tree. Even though at a \frst glance these designs\nare vastly di\u000berent, the design continuum helps us see pos-\nsible e\u000ecient transitions; The di\u000berence in the speci\fcation\nof each structure on the design continuum indicates what\nwe need to morph from one to the other. Speci\fcally, be-\ntween an LSM-tree and B+tree, merging and fence pointers\ncharacterize the main design di\u000berences and so the transi-\ntion policies should depend on these design principles. For\nexample, one way to do such a transition is to wait until\nthe LSM-tree is in a state where all of the entries are at the\nbottom level and then build the B+tree o\u000b of that level so\nthat we don't have to copy the data (similar to how we build\nthe internal index of B+tree when we do bulk loading). Ef-\nfectively waiting until merging is not a di\u000berence between\nthe source and target design. A second option is to preemp-\ntively merge the levels of the LSM-tree into a single level so\nwe can build the B+tree o\u000b of that level without waiting for\na natural merge to arrive. A third option is a compromise\nbetween the two: we can use the bottom level of the LSM-\ntree as the leaf layer of the B+tree (avoiding copying the\ndata) and then insert entries from the smaller levels of the\nLSM-tree into the B+tree one by one. We discuss examples\nof transition algorithms in detail in Appendix B.1.\nThe opposite transition, from a B+tree to an LSM-tree,\n\n1PointLookup (searchKey )\n2 ifMB> E then\n3 entry := buffer .\fnd(searchKey );\n4 ifentry then\n5 return entry ;\n// Pointer for direct block access. Set to root.\n6 blockToCheck :=levels [0].runs[0].nodes[0];\n7 fori 0toLdo\n// Check each level's runs from recent to oldest.\n8 forj 0tolevels [i].runs.count do\n/* Prune search using bloom filters and fences\nwhen available. */\n9 ifi <(L\u0000Y)// At hot levels.\n10 then\n11 keyCouldExist :=\nfilters [i][j].checkExists( searchKey );\n12 if!keyCouldExist then\n13 continue ;\n14 else\n15 blockToCheck :=\nfences [i][j].\fnd( searchKey );\n/* For oldest hot run, and all cold runs, if no\nentry is returned, then the search continues\nusing a pointer into the next oldest run. */\n16 entry ,blockToCheck :=\nblockToCheck.\fnd( searchKey );\n17 ifentry then\n18 return entry ;\n19 return keyDoesNotExist ;\nAlgorithm 1: Lookup algorithm template for any design.\nis also possible with the reverse problem that the scattered\nleaves of the B+tree need to represent a contiguous run in an\nLSM-tree. To avoid a full write we can trick virtual memory\nto see these pages as contiguous [83]. The very \frst time\nthe new LSM-tree does a full merge, the state goes back to\nphysically contiguous runs. We explore the implementation-\nlevel concerns of keeping the data layouts of such designs\ninteroperable in Appendix B.2.\nFigure 10 depicts the potential of transitions. During the\n\frst 2000 queries, the workload is short-range scan heavy\nand thus favors B+tree. During the next 2000 queries, the\nworkload becomes write heavy, favoring LSM-Trees. While\npure LSM-tree and pure B-tree designs fail to achieve glob-\nally good performance, when using transitions, we can stay\nclose to the optimal performance across the whole workload.\nThe \fgure captures the I/O behavior of these data structure\ndesigns and the transitions (in number of blocks). Overall,\nit is possible to do transitions at a smaller cost than reading\nand writing all data even if we transition among fundamen-\ntally di\u000berent structures. The future path for the realization\nof this vision points to a transition algebra .\nCode Generation. Tailored storage requires tailored\ncode to get maximum performance [4]. The continuum pro-\nvides the means towards such a path; since there exists a\nunifying model that describes the diverse designs in the con-\ntinuum, this means that we can write a single generalized\nalgorithm for each operation othat can instantiate the in-\ndividual algorithm for operation ofor each possible designs.\nFor example, Algorithm 1 depicts such a generalized algo-\nrithm for the point lookup operation for the design contin-\nuum we presented in this paper.\nLearning to go Beyond the Continuum. We expect\nthat there will likely be critical design knobs that are very\nhard or even impossible to include in a well-constructed de-\nsign continuum. The path forward is to combine machine\nlearning with the design continuum. Machine learning is\nincreasingly used to tune exposed tuning knobs in systems\nFigure 11: Navigating memory allocation by learning.\n[3, 77]. The new opportunity here is the native combina-\ntion of such techniques with the system design itself. For\nexample, consider the critical decision of how much memory\nresources to allocate to the cache. What is hard about this\ndecision is that it interacts in multiple ways with numer-\nous other memory allocations that uniquely characterize a\ndesign (speci\fcally the write bu\u000ber, the bloom \flters, and\nthe fence pointers in our design continuum) but it is also\nhighly sensitive to the workload. However, we can use the\ngeneralized cost formulas of the continuum to derive formu-\nlas for the expected I/O savings if we increase the memory\nin any memory component. We can then use these esti-\nmates to implement a discrete form of stochastic gradient\ndescent (SGD). Figure 11 shows an example of our results\nfor a skewed workload where we tested two instances of our\ncontinuum, the Monkey design [24] which optimizes bloom\n\flter allocation and a Leveled LSM-tree design with \fxed\nfalse positive rates across all bloom \flters.\nWe evaluate three gradients (pertaining to cache, write\nbu\u000ber and bloom \flter budget) at every grid point along the\nsimplex of simulated LSM-trees with constant total mem-\nory. We then overlay an arrow on top of the disk access\ncontour plot pointing from the lowest gradient component\nto the highest gradient component (we move 8 bytes from\none component to the other every time). Finally, for each\ngrid location, the process follows the arrows until we either\nreach the edge of the simplex or a previously visited point.\nWe then plot partially opaque orange dots at the I/O min-\nima for 3-way memory tunings, predicted by the SGD ap-\nproach beginning at each simplex starting point. The set of\nvisible orange dots represents the most frequently predicted\nI/O minima by the overall SGD process, showing there is\ngeneral clustering of predicted minima. The yellow dot rep-\nresents a global I/O minimum found experimentally. Tests\nwith numerous other workloads also indicate that although\nas expected the overall optimization problem is sometimes\nnon-convex, we can usually reach a point close to the opti-\nmum. The details of this methodology for predicting optima\nfor memory tunings using an SGD-based approach are out-\nlined in Appendix C. The net result is that design continu-\nums can be blended with ML approaches to co-design a tai-\nlored system that both knows how to navigate a vast space\nof the design space and learns when needed to navigate\ndesign options that are hard to deterministically formulate\nhow they will interact with the rest of the design.\n6. NEXT STEPS\nResearch on data structures has focused on identifying\nthe fundamentally best performance trade-o\u000bs. We envision\na complementary line of future research to construct and\nimprove on design continuums. The overarching goal is to\n\rexibly harness our maturing knowledge of data structures\nto build more robust, diverse and navigable systems. Future\nsteps include the construction of more and larger continu-\n\nums, and especially the investigation of broader classes of\ndata structure design, including graphs, spatial data, com-\npression, replication as well as crucially more performance\nmetrics such as concurrency, and adaptivity. The most chal-\nlenging next step is whether the construction of design con-\ntinuums itself can be (semi-) automated.\n7. ACKNOWLEDGMENTS\nMark Callaghan has provided the authors with feedback\nand insights on key-value store design and industry prac-\ntices repeatedly over the past few years and speci\fcally for\nthis paper. This work is supported by the National Science\nFoundation under grant IIS-1452595.\n8. REFERENCES\n[1] N. Agrawal, V. Prabhakaran, T. Wobber, J. D. Davis, M. Manasse,\nand R. Panigrahy. Design Tradeo\u000bs for SSD Performance. ATC,\n2008.\n[2] J.-S. Ahn, C. Seo, R. Mayuram, R. Yaseen, J.-S. Kim, and\nS. Maeng. ForestDB: A Fast Key-Value Storage System for\nVariable-Length String Keys. TC, 65(3):902{915, 2016.\n[3] D. V. Aken, A. Pavlo, G. J. Gordon, and B. Zhang. Automatic\nDatabase Management System Tuning Through Large-scale Machine\nLearning. SIGMOD , 2017.\n[4] I. Alagiannis, S. Idreos, and A. Ailamaki. H2O: A Hands-free\nAdaptive Store. SIGMOD , 2014.\n[5] D. J. Aldous. Exchangeability and related topics. pages 1{198, 1985.\n[6] P. M. Aoki. Generalizing \"Search\" in Generalized Search Trees\n(Extended Abstract). ICDE , 1998.\n[7] P. M. Aoki. How to Avoid Building DataBlades That Know the\nValue of Everything and the Cost of Nothing. SSDBM , 1999.\n[8] Apache. Accumulo. https://accumulo.apache.org/ .\n[9] Apple. Foundationdb. https://github.com/apple/foundationdb , 2018.\n[10] L. Arge. The Bu\u000ber Tree: A Technique for Designing Batched\nExternal Data Structures. Algorithmica , 37(1):1{24, 2003.\n[11] T. G. Armstrong, V. Ponnekanti, D. Borthakur, and M. Callaghan.\nLinkBench: a Database Benchmark Based on the Facebook Social\nGraph. SIGMOD , 2013.\n[12] J. Arulraj, A. Pavlo, and P. Menon. Bridging the Archipelago\nbetween Row-Stores and Column-Stores for Hybrid Workloads.\nSIGMOD , 2016.\n[13] M. Athanassoulis, M. S. Kester, L. M. Maas, R. Stoica, S. Idreos,\nA. Ailamaki, and M. Callaghan. Designing Access Methods: The\nRUM Conjecture. EDBT , 2016.\n[14] R. Bayer and E. M. McCreight. Organization and Maintenance of\nLarge Ordered Indexes. Proceedings of the ACM SIGFIDET\nWorkshop on Data Description and Access , 1970.\n[15] R. Bayer and E. M. McCreight. Organization and Maintenance of\nLarge Ordered Indices. Acta Informatica , 1(3):173{189, 1972.\n[16] M. A. Bender, M. Farach-Colton, J. T. Fineman, Y. R. Fogel, B. C.\nKuszmaul, and J. Nelson. Cache-Oblivious Streaming B-trees.\nSPAA , 2007.\n[17] G. S. Brodal and R. Fagerberg. Lower Bounds for External Memory\nDictionaries. SODA , 2003.\n[18] Y. Bu, V. R. Borkar, J. Jia, M. J. Carey, and T. Condie. Pregelix:\nBig(ger) Graph Analytics on a Data\row Engine. PVLDB ,\n8(2):161{172, 2014.\n[19] Z. Cao, S. Chen, F. Li, M. Wang, and X. S. Wang. LogKV:\nExploiting Key-Value Stores for Log Processing. CIDR , 2013.\n[20] B. Chandramouli, G. Prasaad, D. Kossmann, J. J. Levandoski,\nJ. Hunter, and M. Barnett. FASTER: A Concurrent Key-Value Store\nwith In-Place Updates. SIGMOD , 2018.\n[21] F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A. Wallach,\nM. Burrows, T. Chandra, A. Fikes, and R. E. Gruber. Bigtable: A\nDistributed Storage System for Structured Data. OSDI , 2006.\n[22] D. Cohen and N. Campbell. Automating Relational Operations on\nData Structures. IEEE Software , 10(3):53{60, 1993.\n[23] B. Dageville, T. Cruanes, M. Zukowski, V. Antonov, A. Avanes,\nJ. Bock, J. Claybaugh, D. Engovatov, M. Hentschel, J. Huang,\nA. W. Lee, A. Motivala, A. Q. Munir, S. Pelley, P. Povinec,\nG. Rahn, S. Triantafyllis, and P. Unterbrunner. The Snow\rake\nElastic Data Warehouse. SIGMOD , 2016.\n[24] N. Dayan, M. Athanassoulis, and S. Idreos. Monkey: Optimal\nNavigable Key-Value Store. SIGMOD , 2017.\n[25] N. Dayan, M. Athanassoulis, and S. Idreos. Optimal Bloom Filters\nand Adaptive Merging for LSM-Trees. TODS , (to appear, 2018.\n[26] N. Dayan, P. Bonnet, and S. Idreos. GeckoFTL: Scalable Flash\nTranslation Techniques For Very Large Flash Devices. SIGMOD ,\n2016.\n[27] N. Dayan and S. Idreos. Dostoevsky: Better Space-Time Trade-O\u000bs\nfor LSM-Tree Based Key-Value Stores via Adaptive Removal of\nSuper\ruous Merging. SIGMOD , 2018.[28] N. Dayan and S. Idreos. The log-structured merge-bush & the wacky\ncontinuum. In SIGMOD , 2019.\n[29] G. DeCandia, D. Hastorun, M. Jampani, G. Kakulapati,\nA. Lakshman, A. Pilchin, S. Sivasubramanian, P. Vosshall, and\nW. Vogels. Dynamo: Amazon's Highly Available Key-value Store.\nSIGOPS Op. Sys. Rev. , 41(6):205{220, 2007.\n[30] J. Dittrich and A. Jindal. Towards a One Size Fits All Database\nArchitecture. CIDR , 2011.\n[31] S. Dong, M. Callaghan, L. Galanis, D. Borthakur, T. Savor, and\nM. Strum. Optimizing Space Ampli\fcation in RocksDB. CIDR , 2017.\n[32] Facebook. RocksDB. https://github.com/facebook/rocksdb .\n[33] M. J. Franklin. Caching and Memory Management in Client-Server\nDatabase Systems . PhD thesis, University of Wisconsin-Madison,\n1993.\n[34] G. Golan-Gueta, E. Bortnikov, E. Hillel, and I. Keidar. Scaling\nConcurrent Log-Structured Data Stores. EuroSys , 2015.\n[35] Google. LevelDB. https://github.com/google/leveldb/ .\n[36] P. Hawkins, A. Aiken, K. Fisher, M. C. Rinard, and M. Sagiv. Data\nRepresentation Synthesis. PLDI , 2011.\n[37] P. Hawkins, A. Aiken, K. Fisher, M. C. Rinard, and M. Sagiv.\nConcurrent data representation synthesis. PLDI , 2012.\n[38] HBase. Online reference. http://hbase.apache.org/ , 2013.\n[39] J. M. Hellerstein, J. F. Naughton, and A. Pfe\u000ber. Generalized\nSearch Trees for Database Systems. VLDB , 1995.\n[40] S. Idreos, I. Alagiannis, R. Johnson, and A. Ailamaki. Here are my\nData Files. Here are my Queries. Where are my Results? CIDR ,\n2011.\n[41] S. Idreos, M. L. Kersten, and S. Manegold. Database Cracking.\nCIDR , 2007.\n[42] S. Idreos, L. M. Maas, and M. S. Kester. Evolutionary Data\nSystems. CoRR , abs/1706.0, 2017.\n[43] S. Idreos, S. Manegold, H. Kuno, and G. Graefe. Merging What's\nCracked, Cracking What's Merged: Adaptive Indexing in\nMain-Memory Column-Stores. PVLDB , 4(9):586{597, 2011.\n[44] S. Idreos, K. Zoumpatianos, M. Athanassoulis, N. Dayan,\nB. Hentschel, M. S. Kester, D. Guo, L. M. Maas, W. Qin, A. Wasay,\nand Y. Sun. The Periodic Table of Data Structures. IEEE DEBULL ,\n41(3):64{75, 2018.\n[45] S. Idreos, K. Zoumpatianos, B. Hentschel, M. S. Kester, and D. Guo.\nThe Data Calculator: Data Structure Design and Cost Synthesis\nfrom First Principles and Learned Cost Models. SIGMOD , 2018.\n[46] H. V. Jagadish, P. P. S. Narayan, S. Seshadri, S. Sudarshan, and\nR. Kanneganti. Incremental Organization for Data Recording and\nWarehousing. VLDB , 1997.\n[47] W. Jannen, J. Yuan, Y. Zhan, A. Akshintala, J. Esmet, Y. Jiao,\nA. Mittal, P. Pandey, P. Reddy, L. Walsh, M. A. Bender,\nM. Farach-Colton, R. Johnson, B. C. Kuszmaul, and D. E. Porter.\nBetrFS: A Right-optimized Write-optimized File System. FAST ,\n2015.\n[48] C. Jermaine, E. Omiecinski, and W. G. Yee. The Partitioned\nExponential File for Database Storage Management. VLDBJ ,\n16(4):417{437, 2007.\n[49] O. Kennedy and L. Ziarek. Just-In-Time Data Structures. CIDR ,\n2015.\n[50] M. L. Kersten and L. Sidirourgos. A database system with amnesia.\nInCIDR , 2017.\n[51] H. Kondylakis, N. Dayan, K. Zoumpatianos, and T. Palpanas.\nCoconut: A scalable bottom-up approach for building data series\nindexes. VLDB , 11(6):677{690, 2018.\n[52] H. Kondylakis, N. Dayan, K. Zoumpatianos, and T. Palpanas.\nCoconut palm: Static and streaming data series exploration now in\nyour palm. In SIGMOD , 2019.\n[53] M. Kornacker. High-Performance Extensible Indexing. VLDB , 1999.\n[54] M. Kornacker, C. Mohan, and J. M. Hellerstein. Concurrency and\nRecovery in Generalized Search Trees. SIGMOD , 1997.\n[55] M. Kornacker, M. A. Shah, and J. M. Hellerstein. amdb: An Access\nMethod Debugging Tool. SIGMOD , 1998.\n[56] M. Kornacker, M. A. Shah, and J. M. Hellerstein. Amdb: A Design\nTool for Access Methods. IEEE DEBULL , 26(2):3{11, 2003.\n[57] D. Kossman. Systems Research - Fueling Future Disruptions. In\nKeynote talk at the Microsoft Research Faculty Summit , Redmond,\nWA, USA, aug 2018.\n[58] T. Kraska, M. Alizadeh, A. Beutel, E. Chi, A. Kristo, G. Leclerc,\nS. Madden, H. Mao, and V. Nathan. Sagedb: A learned database\nsystem. In CIDR , 2019.\n[59] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The\nCase for Learned Index Structures. SIGMOD , 2018.\n[60] A. Lakshman and P. Malik. Cassandra - A Decentralized Structured\nStorage System. SIGOPS Op. Sys. Rev. , 44(2):35{40, 2010.\n[61] V. Leis, A. Kemper, and T. Neumann. The Adaptive Radix Tree:\nARTful Indexing for Main-Memory Databases. ICDE , 2013.\n[62] J. J. Levandoski, D. B. Lomet, and S. Sengupta. The Bw-Tree: A\nB-tree for New Hardware Platforms. ICDE , 2013.\n[63] Y. Li, B. He, J. Yang, Q. Luo, K. Yi, and R. J. Yang. Tree Indexing\non Solid State Drives. PVLDB , 3(1-2):1195{1206, 2010.\n[64] H. Lim, D. Han, D. G. Andersen, and M. Kaminsky. MICA: A\nHolistic Approach to Fast In-Memory Key-Value Storage. NSDI ,\n2014.\n[65] LinkedIn. Voldemort. http://www.project-voldemort.com .\n\n[66] Z. Liu and S. Idreos. Main Memory Adaptive Denormalization.\nSIGMOD , 2016.\n[67] C. Loncaric, E. Torlak, and M. D. Ernst. Fast Synthesis of Fast\nCollections. PLDI , 2016.\n[68] L. Lu, T. S. Pillai, A. C. Arpaci-Dusseau, and R. H.\nArpaci-Dusseau. WiscKey: Separating Keys from Values in\nSSD-conscious Storage. FAST , 2016.\n[69] S. Manegold, P. A. Boncz, and M. L. Kersten. Generic Database\nCost Models for Hierarchical Memory Systems. VLDB , 2002.\n[70] T. Mattson, B. Sanders, and B. Massingill. Patterns for Parallel\nProgramming . Addison-Wesley Professional, 2004.\n[71] Memcached. Reference. http://memcached.org/ .\n[72] MongoDB. Online reference. http://www.mongodb.com/ .\n[73] M. A. Olson, K. Bostic, and M. I. Seltzer. Berkeley DB. ATC, 1999.\n[74] P. E. O'Neil, E. Cheng, D. Gawlick, and E. J. O'Neil. The\nlog-structured merge-tree (LSM-tree). Acta Informatica ,\n33(4):351{385, 1996.\n[75] J. K. Ousterhout, G. T. Hamachi, R. N. Mayo, W. S. Scott, and\nG. S. Taylor. Magic: A VLSI Layout System. DAC, 1984.\n[76] A. Papagiannis, G. Saloustros, P. Gonz\u0013 alez-F\u0013 erez, and A. Bilas.\nTucana: Design and Implementation of a Fast and E\u000ecient Scale-up\nKey-value Store. ATC, 2016.\n[77] A. Pavlo, G. Angulo, J. Arulraj, H. Lin, J. Lin, L. Ma, P. Menon,\nT. C. Mowry, M. Perron, I. Quah, S. Santurkar, A. Tomasic,\nS. Toor, D. V. Aken, Z. Wang, Y. Wu, R. Xian, and T. Zhang.\nSelf-Driving Database Management Systems. CIDR , 2017.\n[78] Redis. Online reference. http://redis.io/ .\n[79] K. Ren, Q. Zheng, J. Arulraj, and G. Gibson. SlimDB: A\nSpace-E\u000ecient Key-Value Storage Engine For Semi-Sorted Data.\nPVLDB , 10(13):2037{2048, 2017.\n[80] S. M. Rumble, A. Kejriwal, and J. K. Ousterhout. Log-structured\nmemory for DRAM-based storage. FAST , 2014.\n[81] E. Schonberg, J. T. Schwartz, and M. Sharir. Automatic data\nstructure selection in setl. In Proceedings of the 6th ACM\nSIGACT-SIGPLAN Symposium on Principles of Programming\nLanguages , pages 197{210, 1979.\n[82] E. Schonberg, J. T. Schwartz, and M. Sharir. An automatic\ntechnique for selection of data representations in setl programs.\nACM Trans. Program. Lang. Syst. , 3(2):126{143, apr 1981.\n[83] F. M. Schuhknecht, A. Jindal, and J. Dittrich. An experimental\nevaluation and analysis of database cracking. VLDBJ , 25(1):27{52,\n2016.\n[84] R. Sears and R. Ramakrishnan. bLSM: A General Purpose Log\nStructured Merge Tree. SIGMOD , 2012.\n[85] O. Shacham, M. T. Vechev, and E. Yahav. Chameleon: Adaptive\nSelection of Collections. PLDI , 2009.\n[86] J. Sheehy and D. Smith. Bitcask: A Log-Structured Hash Table for\nFast Key/Value Data. Basho White Paper , 2010.\n[87] Y. Smaragdakis and D. S. Batory. DiSTiL: A Transformation\nLibrary for Data Structures. DSL, 1997.\n[88] Spotify. Sparkey. https://github.com/spotify/sparkey , 2014.\n[89] M. J. Steindorfer and J. J. Vinju. Towards a Software Product Line\nof Trie-Based Collections. GPCE , 2016.\n[90] M. Stonebraker and U. Cetintemel. \"One Size Fits All\": An Idea\nWhose Time Has Come and Gone. ICDE , 2005.\n[91] R. E. Tarjan. Complexity of Combinatorial Algorithms. SIAM\nReview , 20(3):457{491, 1978.\n[92] R. Thonangi and J. Yang. On Log-Structured Merge for Solid-State\nDrives. ICDE , 2017.\n[93] L. Wilkinson. The Grammar of Graphics . Springer-Verlag, 2005.\n[94] WiredTiger. Source Code. https://github.com/wiredtiger/wiredtiger .\n[95] C. Wu, J. M. Faleiro, Y. Lin, and J. M. Hellerstein. Anna: A kvs for\nany scale. pages 401{412, 2018.\n[96] C.-H. Wu, T.-W. Kuo, and L.-P. Chang. An e\u000ecient B-tree layer\nimplementation for \rash-memory storage systems. TECS , 6(3), 2007.\n[97] Y. Xu, E. Frachtenberg, S. Jiang, and M. Paleczny. Characterizing\nfacebook's memcached workload. IEEE Internet Computing ,\n18(2):41{49, 2014.\n[98] S. B. Yao. An Attribute Based Model for Database Access Cost\nAnalysis. TODS , 2(1):45{67, 1977.\n[99] H. Zhang, H. Lim, V. Leis, D. G. Andersen, M. Kaminsky,\nK. Keeton, and A. Pavlo. SuRF: Practical Range Query Filtering\nwith Fast Succinct Tries. SIGMOD , 2018.\nAPPENDIX\nA. SUPER-STRUCTURE INSTANCES\nWe discuss four instantiations of the design continuum\nin this section. Lazy-Leveled LSM-tree, Leveled LSM-tree,\nB\u000ftree, and LSH-table instances are shown in Figure 12.\nwhile one instance (LSH-table) is expressable in the exten-\nsion of the design continuum. In this section, we explain\nhow these four example instantiations are formed.A.1 LSH-table\nLSH-table is a structure with a single hot level. This hot\nlevel is the log portion of the structure, and has growth\nfactorT=N\u0001E\nMB, meaning that the single level log can grow\nup to a capacity sized to \ft up to all of the dataset in the \frst\nlevel of the data structure. The typical compaction behavior\nthat LSH-table requires as its managed dataset grows can\nbe described by the two merge thresholds de\fned by the\ndesign continuum. The LSH-table will reach compaction\ncapacity when the number of runs exceeds T\u00001, indicated\nby the cold merge threshold Z=T\u00001, which speci\fes\nthat the single hot level will prolong merge for as long as\npossible. The hot merge threshold Kis only provided for\ncompleteness for single level structures like the LSH-table; it\nis of note to mention that for the design continuum the hot\nmerge threshold parameter does not apply to the ultimate\nhot level, rather the ultimate hot level is governed by Z\nthe cold merge threshold. The LSH-table node size could\ngrow up to N=B , depending on how compaction works on\nthe log part. For instance, a stream of Nsequential inserts\nall compacted at once would compact to a single contiguous\nsection ofN=B pages which is analagous to a max node size\nDofN=B pages.\nTwo main memory navigation features of LSH-table de-\nsign di\u000ber from that of other instantiations of the design\ncontinuum: \frst, a hash table is used as a \flter and requires\non order of one key size ( F) for each data entry, and second,\nto support range scan pruning on every page worth of data\n(everyBentries) in the log, a fence of order size Fmust be\nmaintained in memory.\nNote that in the extended design continuum a hash table\nis allowed to take the place of what was once the Bloom\n\flter budget as de\fned by the original design continuum.\nImplementations of LSH-table which do not need to han-\ndle range scans, often omit the fence maintenance feature,\nthereby keeping MFPe\u000bectively zero, and allocating all of\nMFto the hash table. We illustrate an LSH-table example\nin the top left of Figure 12; with a single large hash-table\nand fence pointers enabled.\nA.2 Leveled LSM-tree\nLeveled LSM-tree is a structure with multiple hot levels,\nwith a growth factor of T; each level of the LSM-tree is T\ntimes larger in capacity than the previous level. All levels\nbut the ultimate hot level, are controlled by the hot merge\nthresholdK, which is set to 1, allowing no more than a sin-\ngle run at every hot level. The ultimate level is controlled\nby cold merge threshold Z, which is also set to 1 for com-\npleteness. Leveling corresponds to maximizing the amount\nof merge greediness, bounding the number of runs at each\nlevel, and reducing space ampli\fcation, bene\fting short and\nlong scans at the expense of merging updates potentially\nmultiple times at each level. Figure 12 bottom left, illus-\ntrates a Leveled LSM-tree that uses fence and \flter memory\nbudgetMFamounting toF\nBbits per entry for fences and 10\nbits per entry for Bloom \flter.\nA.3 Lazy-Leveled LSM-tree\nLazy-Leveled LSM-tree is a structure with multiple hot\nlevels, with a growth factor of T; each level of the LSM-tree\nisTtimes larger in capacity than the previous level. All\nlevels but the ultimate hot level are controlled by the hot\nmerge threshold K, which is set to T\u00001, allowing up to\n\nT\u00001 runs to build up at each of these levels and maximally\nrelaxes the number of times merge occurs (in comparison to\na Leveled LSM-tree). The ultimate hot level is controlled\nby the cold merge threshold Z, and only has one large run,\nthereby bounding space ampli\fcation caused from relaxing\nhot merge threshold Kat the \frst L\u00001 levels of the tree.\nFigure 12 top right illustrates a Lazy-Leveled LSM-tree that\nuses fence and \flter memory budget MFamounting toF\nB\nbits per entry for fences and 10 bits per entry for Bloom\n\flter.\nA.4 B\u000ftree\nB\u000ftree is a structure with only one hot level and the rest\ncold levels. The single hot level is analagous to the \frst\nlevel of a tree below the root node. The in-memory fence\npointers only exist for the \frst level. The fence pointers and\nwrite bu\u000ber are analgous to the child pointers and contents\ntypically held in a B-tree's root node. From a performance\nstandpoint, the B\u000ftree instantiation behaves as if it has a\nroot node pinned in memory. The tree itself has a fanout\ndescribed by the growth factor T, which varies dynamically\nbetween 2 to Bdepending on how full the nodes are. The\nhot merge threshold Kis de\fned for completeness, the cold\nmerge threshold Zgoverns all levels (including the single hot\nlevel) and is set at 1, describing that each level of the in-\nstantiation is one run. This allows point lookups to navigate\nthe structure only by examining one node at each level, and\nfollowing the embedded fence pointers to reach cold levels of\nthe tree. The batch update mechanism is cascading, as each\nlevel becomes full, the entries at that level must be \rushed\nand merged with the overlapping nodes of the next largest\nlevel, while the embedded fences of each level that point to\nthe next are updated to re\rect the merges that have taken\nplace. By setting the maximum node size Dto 1, the instan-\ntiation describes the smallest unit of merge as a block size.\nFigure 12 bottom right illustrates a B\u000ftree instantiation.\nB. DESIGN TRANSITIONS\nIn this section we discuss in further detail a preliminary\nexploration of what is required for facilitating online transi-\ntions between data structure designs on the design contin-\nuum.\nB.1 Transition Algorithms\nAs an example we outline possible transition algorithms\nbetween an LSM-tree to a B+tree. We give two di\u000berent\nstrategies and then we discuss how to chose between them\nand how to holistically design and schedule transitions.\nSort-Merge Transition. In an LSM-tree, the data is or-\nganized into several sorted runs with some obsolete entries\nthat result in space ampli\fcation. In order to condense this\ndata into the leaf level of a B+tree, we must merge all of\nthese runs into a dense, sorted array that becomes the leaf\nlevel. In this approach, we force a sort-merge of all the runs\nin the leveled LSM-tree structure. This has a performance\ncost of reading in all the runs' pages, and then writing each\nof the sorted pages to disk. This method has optimal space\nampli\fcation properties, since the resulting array is com-\npact. Moreover, since we would usually switch to a B+tree\nwhen the workload is more read-heavy than write-heavy, a\ncompact array would be better (fuller B+tree leaves and\ntherefore fewer disk pages of data to scan).\nBatch Inserts Transition. In some cases (e.g. whenthere is relatively little data in the levels above the bottom of\nthe LSM-tree), we may be able to avoid reading and writing\nmost of the data. To achieve a more lightweight transition\nin these cases, we can take the bottommost (largest) run of\nthe LSM-tree and treat it as the leaf level of a B+tree. We\nthen insert the entries from the upper levels (levels above\nthe bottom) of the LSM-tree into this B+tree. By perform-\ning these inserts in sorted order, we can \\batch insert\" the\nentries into the B+tree. In particular, consider a node Nof\nthe leaf layer for which there are entries in the upper lev-\nels to be inserted into N. We load that node into memory.\nThen, we iterate through the elements in the upper levels\ncorresponding to the range of N- call this set of elements R.\nAs we do so, we merge the elements of NandR, making new\nleaf-level nodes in memory. These new nodes are \rushed to\ndisk as they are made. As an example, if the set of elements\nRto be inserted into node Nconsists of just one element,\nthen the above process is equivalent to a B+tree node split.\nChoosing the Optimal Transition. Compared to the\nsort-merge approach, the batch inserts method may be more\ne\u000ecient because we may not insert entries into every node\nof the B+tree, saving reading and writing costs of certain\nnodes. On the other hand, it has worse space ampli\fcation\nproperties because some pages may be partially empty; this\ncan result in more disk IO. We show that, in both average\nand worst case analysis, the batch inserts approach super-\nsedes the sort-merge approach when the size of the upper\nruns is below a certain threshold. To illustrate this point,\nwe present the results for a simple worst case analysis of a\nbasic LSM-tree with node of size 1 page.\nAssume an LSM-tree with Llevels where xiis number\nof entries in the ith level and dis the size of each entry in\nbytes and a machine where pis the page size in bytes and\n\u001eis the ratio of a disk write to a disk read. The cost of\na sort merge transition is CSM=\u0010PL\ni=1l\nd\u0001xi\npm\u0011\n(1 +\u001e).\nThe worst-case cost of a batch insert transition is CBI= l\nd\u0001xL\npm\n+PL\u00001\ni=1xi\u0010\nd\np+ 1 + 2\u001e\u0011\n. Letx\u0000Lbe the number\nof entries in all levels above L; we choose to do a batch\ninsert transition only when CBI< CSM, in other terms\nwhenx\u0000L\nxL<d\u001e\np+(2p\u0000d)\u001e. See Figure 2 as a visualization of\nthis tradeo\u000b: Using our cost model, we show that as the\nratiox\u0000L\nxLdecreases, the cost of the batch insert method\ndecreases while the sort-merge method is una\u000bected. This\nis because as xLbecomes relatively larger, the LSM-tree is\nmore \"bottom-heavy,\" and we can potentially use fewer disk\noperations when executing a batch insert transition. In this\ncase,x\u0000L\nxL= 0:2 is our threshold point, below which it is\ncheaper to use a batch insert transition.\nGradual Transition. In many situations a sudden spike\nin query latency would be undesirable. Transitions should\nbe lightweight but also we should be able to control the\ntransition cost with respect to the current system status.\nWe propose a robust, gradual transition algorithm that al-\nlows us to transition from an LSM-tree to a B+tree over\nthe course of an arbitrary number of steps. This algorithm\nexhibits a tradeo\u000b between total transition cost and query\nlatency during the transition phase. At each step, we take\ntheksmallest pages of data from the LSM-tree and insert\nthese pages into the B+tree at the leaf level, updating the in-\ntermediate in-memory nodes accordingly. We keep track of\na \"threshold value\" \u0012, which is the highest key value from our\nLSM-tree that we have inserted into our B+tree. Thus, each\n\nFPFPBufferFiltersFence PointersMemoryStorageHashTablenoderun boundaryfence pointerstorage block1 hot level\nLSH-TableBufferFiltersFence PointersFPBFFPFPBFBFBFBFnoderun boundaryfence pointerstorage block\nFPFPL hot levelsFPBF…FPFPBFBFFPFP……………BFBFBFBFBFBFLazy Leveled LSM-TreeFPFPFPFP…FPFP\n…BufferFiltersFence PointersFP……noderun boundaryfence pointerstorage block\n…1 hot levelL-1 cold levelsFP\nBε TreeBufferFiltersFence PointersFPBFFPFPBFBFBFBFnoderun boundaryfence pointerstorage block\nFPFPL hot levelsFPBF\nFPFPBFBFFPFP…………BFBFBFBFBFBFLeveled LSM-TreeFPFPFPFP……\nMemoryStorage………\n…Figure 12: Various instantiations of the super-structure from the design continuum.\nFigure 13: Tradeo\u000b Between Merge Sort (Upper Right) and\nBatch Insert (Lower Right)\nstep will only require us to append kpages to the leaf level\nof the B+tree since we know that the key values contained in\nthese pages will all be greater than \u0012at the previous step of\nthis algorithm. After each step we update \u0012. We sort-merge\nthe \frstkpages' worth of data to generate the values to add\nto our B+tree, and we can do this e\u000eciently by generating\nin memory one page at a time and then writing that page to\ndisk. This allows us to only have to read and write to disk\neach entry in the LSM-tree once. To handle reads and writes\nwhile the transition is underway, we check if the query key\nis greater than or less than \u0012, and this tells us whether to\nexecute the query on the LSM-tree, the B+tree, or possibly\nboth in the case of range queries. The total cost of the tran-\nsition will include the cost of reading and writing data in\nthe original LSM tree, the cost of reading and writing any\nnew data added to the LSM tree during the transition, and\nthe opportunity cost of queries during the transition.\nB+tree to LSM-tree Transitions. A naive transition\nfrom a B+tree to an LSM-tree could take one of two ap-\nproaches: we could insert the leftmost kpages of our B+tree\ninto our LSM-tree over ksteps.The second approach, which we detail further in the pro-\nceeding sections, would allow us to treat the B+tree as the\nbottom-most level of the LSM tree without reading its con-\ntents from disk. The naive approach to this would requires\nre-writing all the non-contiguous leaves of the B+tree into\na contiguous block of memory, since B+trees leaves are not\nguaranteed to be contiguous while LSM runs aretradition-\nally required to be contiguous. This operation would depend\non a variable x: The \"degree of fragmentation\" given by the\nratio of the number of contiguous chunks in the data layer to\nNE, the total bits of data in the data layer. The cost of this\nnaive transition would be linear in the number of pages to\nread and write, namely \u000bNEx +\u000bNE (1\u0000x) +NE, where\n\u000bis an ampli\fcation factor to account for the amount of\nempty space in the B+tree leaves (since B+tree leaves are\nusually not completely full; i.e.\u000bN\nBpages will contain N\ndata entries).\nThis presents a clear opportunity for optimization. Since\nthe B+tree's leaves are already internally sorted and in sorted\norder, the data would simply be copied over and no sorted\nstep takes place, unlike in the LSM - >B+tree transition,\nwhere rewrites were necessary .\nB.2 Supporting Data Layout Interoperability\nAs a self-designing system undergoes design transition,\nthe architecture must employ techniques to support seam-\nless interoperabilty between data layouts as the transition\nis underway; it is crucial that as many components of the\ndata layout from the previous design can be reused in the\nnew design to save work for the system. Here we propose an\nimplementation-level abstraction for storage layer manage-\nment with the objective to mitigate the cost of data rewrite\nduring a B+tree to LSM-tree transition.\nMapping Page IDs to Disk Locations. To avoid sim-\n\nply copying the data into a contiguous memory region, we\npropose an abstraction layer to allow LSM trees to work\nwith non-contiguously stored runs and levels. This abstrac-\ntion layer would \\deceive\" the LSM tree into believing that\nit's using contiguous memory regions, and so would require\nno change to the logic of the LSM tree, and would avoid\nrewriting any disk data. Instead, only memory indexing\nstructures would change.\nThis abstraction would cause the LSM tree to manage its\nreferences to data in terms of page IDs instead of in terms\nof addresses to disk pages. LSM trees require contiguously\nstored runs due to their fence pointer architecture. Each run\nof the LSM tree is indexed into via a set of fences that indi-\ncate which value ranges are stored in which pages. In order\nto be space e\u000ecient, this indexing system stores only one\npointer (the pointer to the \frst page of data in the run), has\none \"fence\" for each page which indicates the largest value\nstored in that page. These fences are iterated through to\n\fnd the poge that contains the value(s) of interest, and the\nappropriate page is then accessed using the single pointer\nand an o\u000bset. It is this single-pointer and o\u000bset mecha-\nnism that would break down if the pages that make up a\nrun of an LSM tree were not stored contiguously. Page IDs\nsolve this problem by providing a mapping from contiguous\nnumbers (page IDs) to actual disk page addresses, which no\nlonger need to be contiguous.\nTherefore, the LSM tree's implementation would sense lit-\ntle di\u000berence between referencing disk regions and page IDs.\nThe page IDs would be redirected through our abstraction\nlayer to the appropriate disk pages.\nSince the abstraction is only useful when a B+tree is being\nconverted to an LSM tree, and we expect all the B+tree's\ndata to reside in the bottom layer of the newly formed LSM\ntree, our abstraction is only necessary for the last level of the\nLSM tree, until the \frst merge occurs. As the LSM tree is\nupdated, upper levels can be stored contiguously on physical\npages, which will be a faster writing approach anyway.\nAdditionally, once the time for the \frst merge operation\narises, we will merge the penultimate layer with the last\nlayer (the one behind the abstraction) and write the new\nlast layer to disk contiguously. Since the last layer is now\ncontiguous, our data structure is not necessary.\nLightweight Indirection Mapping. The most obvi-\nous approach to map page IDs to disk locations is to use a\nhash table that stores page IDs as keys and disk locations\nas values.\nThis structure, however, would require memory on the\norder ofO(N). Additionally, a one-to-one mapping would\nnot be necessary for the majority of pages in our data layer\nsince many of the pages that comprise the data layer may\nbe stored contiguously, since if we convert an LSM Tree to\na B+tree, then back, we observe that the only case where\nthe contiguous runs of the LSM tree are broken is in places\nwhere new data pages are inserted. If we bound the num-\nber of insertions while in the B+tree data structure, we can\nmaintain fairly contiguous regions of data. We can therefore\nde\fne a compression scheme for our mapping data structure.\nInstead of mapping page IDs to disk locations, we will map\ndata \"regions\" to disk locations. For instance, if page IDs 2\nto 1002 are stored contiguously, instead of being represented\nby 1000 mappings from page IDs to physical pages, the re-\ngion would be represented by a single entry containing the\nstarting page ID and the disk location of the \frst page. Therest of the pages may be accessed using o\u000bset arithmetic.\nThis data structure would require memory on the order of\nO(xN), wherexis the fragmentation percentage of the data\nlayer.\nTherefore, in order to redirect page IDs to disk locations,\nwe will make use of a miniature B+tree like structure. Since\nthe keys on a B+tree are sorted, we can easily search the\nB+tree for a key that is not currently mapped by the B+tree,\nand access (with an o\u000bset) he value associated with the\nlargest key less than it. Hence, rather than storing a map-\nping for every mapped page, we can simply store the begin-\nning Page ID for a run of contiguous pages as our key, and\nthe corresponding beginning pointer for our physical page\nas the value.\nSide-E\u000bects for LSM Tree Performance. In consid-\nering the performance implications of using this abstraction,\nthe key insight is that we must now complicate our earlier\nalgorithms, which costed in terms of disk I/Os, and instead\ncost in terms of random disk I/Os and contiguous disk I/Os.\nWe introduce three new variables:\n\u000fc: Average cost of continuous read\n\u000fr: Average cost of random read\n\u000fw: Average cost of continuous write\nWe arrive at the following conclusions about the use of\nour storage/compute abstraction:\n\u000fUse of our API causes 100% reduction in the disk I/O\ncost of transitioning from a B+tree to an LSM Tree,\nsince our new transition only requires memory opera-\ntions, but no disk I/Os.\n\u000fUntil the \frst merge occurs, range queries take signif-\nicantly longer than they would in a regular LSM tree\nthat does not use our API. Speci\fcally, the degradation\nin performance is equal to\n\u000bNEs (xr+xc\u0000c) +scNE (T+ 1)\n(T\u00001)(7)\n\u000fThe cost for the \frst merge di\u000bers between our API\nand the standard algorithm by\nx(NEr\u0000c) (8)\nThis term is positive since r>c andN\u00151.\nThe decrease in performance for a range query in an LSM\ntree is not a serious concern for our intended use case; since\nour hybrid data structure would only switch to a B+tree\nin the case of a write-heavy workload, in most cases, the\nadditional cost of the few range queries that occur prior to\nthe \frst merge will not outweigh the bene\fts of the faster\ntransition.\nC. BEYOND DESIGN CONTINUUMS\nBringing additional knobs into the fold with an existing\ndesign continuum adds additional challenges for reasoning\nabout the performance tradeo\u000bs of designs.\nComplex parameter spaces: In many cases the knobs\nmight display a high level of sensitivity to the values of other\nknobs. This kind of complex interaction can make the man-\nual construction of a proper design continuum intractable.\nConsider the critical design decision of how much mem-\nory to allocate to a cache in a LSM-tree. Since we have\na \fxed amount of main memory budget, the cache budget\n\nmust interact in multiple ways with other memory allocation\nbudgets.\nIn order to increase the size of the cache, we must strate-\ngize the best way to accordingly distribute the reduction\nin budget between the two other components that consume\nmain memory budget: the write bu\u000ber and Bloom \flter.\nFor instance, if we choose to take away from the size of the\nwrite bu\u000ber, we might assume that more queries will end up\ntouching more levels of the LSM-tree. Is this penalty o\u000bset\nby having a larger cache? This depends on the hit rate of\nthe cache, which ultimately depends on the query workload.\nIf we have a workload consisting of a small working set of\nkeys dominating the majority of queries, then perhaps the\nresulting improvement from cache hits will produce an over-\nall improvement in performance. Alternatively, if we have\na workload where the hot working set of keys is larger than\nthe cache or even main memory, then we might \fnd that\nthe strategy of maximizing cache size turns into a bad one\n- e.g. a relatively small proportion of queries that cause a\ncache miss and hit disk might be enough to sink overall per-\nformance because by reducing the space allocated to Bloom\n\flters we've increased their inaccuracy with higher false pos-\nitive rate (FPRs), meaning that each run is more likely to\nincur the fullcost of a random I/O probe, rather than a\nlower protected cost from e\u000bective Bloom \flters.\nIf we had instead decided in the beginning to remove more\nmemory from the Bloom \flters rather than from the write\nbu\u000ber, we'd encounter a di\u000berent set of dilemmas in the\ndecision making process. On top of this, would increasing\nthe size of the write bu\u000ber be a good decision in the \frst\nplace? We would need yet another set of criteria to be able\nto make that determination.\nDiscretizing parameter spaces: We might have design\nknobs that are not smooth and continuous, but are made of\nmany discrete options. Moreover, even continuous design\nknobs may need to be discretized in practice, to be naviga-\nble.\nComplex workloads: In real world scenarios we might\ne.g. \fnd that the hot working set of keys is not static but\nchanges over time, a\u000becting performance with it [97].\nFor only a few parameters, we can end up with a situation\nwhere even hand-wavy intuition on tradeo\u000bs becomes com-\nplicated. Knowing the exact parameters or the exact tipping\npoints in which certain design strategies become better or\nworse is yet harder. At each turn, we encounter more sce-\nnarios where the prevailing strategy is highly dependent on\nthe co-interaction of other design knobs and the nuances of\nthe workload.\nC.1 A Solution: Stochastic Gradient Descent\nWe see a path forward by combining machine learning\nideas with design continuum knowledge to create solutions\nthat approach the optimal for a broader set of design de-\ncisions. The problem, to us, seems analogous to training a\nmachine learning model:\nThe cost function that we're minimizing is the data\naccess cost expressed in random I/O's to storage, derived\nfrom the design continuum.\nThe goal is to minimize the cost function, i.e. \fnd the\nparameters that provide the lowest I/O cost for the given\nset of queries.\nThe parameters are the cache size, the write bu\u000ber size\nand the bloom \flter size.The parameter space is only a subset of 3D space,\nbecause total memory is constrained. For example, if we\nwanted to increase both the cache size and the write bu\u000ber\nsize we would have to decrease the bloom \flter size.\nThe gradient functions are the estimated I/O savings\nif we increase the memory by N bits for any single com-\nponent. Following the gradients, one would expect to end\nup in a minima. While deriving these analytically is di\u000e-\ncult, we come up with reasonable estimates using the design\ncontinuum cost formulas.\nThe dataset we're training the model on is a key-value\nworkload, i.e. a set of queries. We automatically generate\nours from probabilistic models, but one could also use traces\nof real systems, or perhaps a combination of both.\nC.2 Stochastic Workloads\nTo avoid over\ftting to a particular set of queries, we de\fne\nsome workload classes as con\fgurable probabilistic models.\nWe can then generate an ordered sequence of point reads\nor updates (i.e. a workload) from any of these. We include\nmore complex, time-varying workloads (inspired by [97] and\n[11]) that attempt to mimic realistic settings.\nFor all the workloads, when we draw a particular key for\nthe \frst time we will insert it into the database as a write,\nand subsequently we will either look it up or update it with\nprobability w.\nUniform queries will be drawn uniformly from a set of\nKkeys. This is often one in which the cache is unhelpful,\nbut in practice may be unrealistic. Nevertheless, this is the\nscenario that many analyses assume.\nRound-Robin queries are drawn deterministically using\nki= (imodK), i.e. we iteratively draw each key in se-\nquence, then repeat. This represents another pathological\nscenario for a cache: a key that has been recently written or\nread is actually a contraindication we will access it again.\n80-20 queries (considered in [24]) are drawn such that\n20% of the most recently inserted keys constitute 80% of\nthe lookups. This is a simple model to observe the e\u000bects of\nskew.\nZipf queries are distributed according to a zeta distribu-\ntion, with a parameter swhich describes the skewness. Zipf-\ndistributed queries are considered in [61] as another simple\nproxy for realistically skewed queries.\nDiscover-Decay queries are distributed according to the\nfollowing stochastic process, inspired by the Chinese Restau-\nrant process [5] but with time decay: with every passing time\nstep, we draw a number of reads nr, writesnw, and updates\nnuassuming queries arrive according to Poisson processes\nwith con\fgurable rates:\nnr\u0018Pois(\u0015r)\nnw\u0018Pois(\u0015w)\nnu\u0018Pois(\u0015u)\nOnce we've drawn our nwnew keyski, we assign them an\ninitial popularity\n\u0012i\u0018Beta(a\u0012;b\u0012)\nwith a random decay rate\n\ri\u0018Beta(a\r;b\r);\nwhich is the factor by which they exponentially decay each\nsubsequent time step. At any time t, the popularity of\n\neach key is given by p(ki;t)/\u0012i\rt\u0000ti\ni, wheretiis when\nthe key was inserted. We use these time-dependent popu-\nlarities to draw each of our nrreads andnuupdates from\nMult(fp(ki;t)g).\nPeriodic Decay workloads are a simple modi\fcation of\nthe Discover-Decay model where p(ki;t) now depends not\nonly on the decay rate \ribut also on a periodic function\nof the key's age t\u0000ti. To mimic the combination of ex-\nponential decay and sharp periodic peaking we see in [97],\nwe multiply \u0012i\rt\u0000ti\niby an inverse cycloid function with pe-\nriodT, clamped from 0 to 1, and taken to a con\fgurable\npower (to make the cusps sharper or duller) that we call the\ncycloid's cuspity .\nExamples of these workloads are illustrated in Figure 14.\nThe \frst row contains simple workloads where the distri-\nbution of key popularities does not change over time, and\nwhere the read/write ratio is a uniform probability. The\nsecond row contains Discover-Decay workloads, consisting of\noperations that insert/read/update keys according to Pois-\nson processes and simulate popularity decays over time. The\nthird row is a modi\fed version of Discover-Decay that adds\na periodic signal to the decaying popularity with a con\fg-\nurable period and cusp sharpness. Blue dots represent reads\nand green dots represent writes (inserts or updates).\nC.3 Modeling\nWe derive equations for the number of storage accesses\nsaved by adding N bits more space to a component. We store\nsimple O(1) space statistics (e.g. number of queries, number\nof accesses for each bloom \flter, number of times a key was\nfound in a level) to get easy to compute and reasonable\nestimates for these. These loosely follow the general form:\n\u0001accesses =\\hitrate\u0001\u0001entries\u0001\\misscost\nMore speci\fcally:\n\u0001cache =\\lastslothits\u0001\u0001entries\u0001 \\avgcostofmiss\n\u0001buffer =X\nl2L\\accessesl\u0001dM\u0001levelratio\nMlevell\u0001 \\avgcostofmissl\n\u0001bloom =X\nl2L\\accessesl\u0001alloc(l;m+dM)\u0001 \\ \u0001falseposratel\nFor full derivations, refer to Section C.5.\nC.4 Gradient Descent\nResults for basic workloads can be seen in Figure 15. To\ndisplay our results, we introduce a speci\fc plot that shows\nthe workload, our estimated gradients at each point, our es-\ntimated best con\fgurations, the actual performance across\nthe whole parameter space and the actual optimal con\fgu-\nration.\nThe triangle shows the parameter space. Each coordi-\nnate within the space represents an LSM-tree with a par-\nticular combination of these parameters. Perpendicular dis-\ntance from the edge opposite a corner represents the value\nfor that corner's parameter, e.g. a coordinate in the very\ncenter represents an equal allocation of all three parameters\nand a coordinate at a corner represents allocating everything\nto only that parameter.The arrows represent our estimated gradient around that\ncoordinate i.e. given the current set of parameters, which\ndirection our I/O savings model says you should move to get\na better total I/O cost.\nThe orange dots signify the parameters we predict will\nhave the minimum I/O cost. One can start the gradient\ndescent process from any given initial point. We test our\nmethod with every possible initial point to ensure that it\nconsistently yields good results, which is why there is more\nthan one prediction, and why the dots have varying opacity\nof orange color. Darker orange points indicate parameter\ncon\fgurations that were more frequently the resulting pre-\ndictions, while fainter orange points were less frequently the\nresulting predictions.\nThe blue-red shading represents actual I/O cost for\neach parameter combination, generated by exhaustively run-\nning queries from the workload on an LSM-tree simulator.\nThe yellow dot represents the lowest actual minimum\nI/O cost determined experimentally, and therefore the opti-\nmal allocation of parameters - our target.\nThe gradient descent process works as follows:\n\u000fIgnoring the shading, start at any random initial pa-\nrameters. Follow the arrows until you either hit the\nedge of the parameter space or hit a point where there\nare no outward arrows from your current location.\n\u000fTo evaluate our results, one can look at the plot and\nshould check to see that the predicted minima (orange\ndots) are nearby the actual minimum (yellow dot), or\nfailing that, on a grid coordinate with similar IO cost\n(judged by the blue-red shading).\nResults for basic workloads can be seen in Figure 15.\nFor each workload, we provide results for both the even\nBloom \flter allocation and the Monkey Bloom \flter allo-\ncation schemes.\nThe uniform workload provides a baseline workload to\ncompare other results to. The round-robin workload pro-\nvides an example of a canonical workload that thrashes the\ncache to the point where it is useless, and indeed our rec-\nommendation is to allocate no space to the cache. Discon-\ntinuities in the number of levels as we vary the bu\u000ber size\nmakes the optimization non-convex, but Monkey improves\nboth absolute performance and convexity.\nThe results for Zipf workloads in Figure 16 look quite\ndi\u000berent. At high skewness s, we \fnd that Bloom \flters are\nless useful and it is better to allocate more memory to the\nbu\u000ber. At low skewness, the best con\fguration is a mixture\nof mostly Bloom \flter and cache memory with a relatively\nsmall write bu\u000ber.\nThis e\u000bect may be due to the fact for highly skewed work-\nloads, we obtain better savings for the small hot set of keys\nby using the cache (for reads) and the write bu\u000ber (for\nwrites, and also as kind of auxiliary cache). For less skewed\nworkloads, we are more likely to request unpopular keys\nwhich may be buried deep in the tree and impose a higher\nIO cost. To counteract this, we need better Bloom \flters.\nFinally, for the Discover-Decay and Periodic Decay work-\nloads in Figures 17 and 18, we \fnd that our gradients capture\nthe behavior we noted near the beginning of this Appendix\nsection. For lower e\u000bective numbers of popular keys (but\nhigh temporal locality), we tend to end up allocating most\nof our memory to the bu\u000ber and none to the Bloom \flters,\nbut as our \\working set\" expands, we are pushed closer to\n\nFigure 14: Diverse set of workloads used for benchmarking.\nthe center of the graph. In the bottom row of Figure 18, gra-\ndient descent is drawn into two distinct modes based on the\nstarting location, suggesting that our gradient estimations\nare high-resolution enough to capture the nonconvexity. In\ngeneral, there are many situations in which increasing either\nthe write bu\u000ber or the Bloom \flter will reduce I/Os, so we\nshould expect multiple locally optimal allocation strategies\nto exist.\nC.5 Modeling\nWe \frst consider the case of a uniform query distribution\nand then show how the formulation can be generalized to\nany distribution with an empirical trace.\nC.6 Uniform query distribution\nAssuming we have\n\u000fNitems in total DB\n\u000fEsize of an entry in bits\n\u000fMtotal memory\n\u000fMcmemory allocated to cache\n\u000fMbmemory allocated to write bu\u000ber\n\u000fBentries that \ft in a disk page\n\u000fPbsize of the write bu\u000ber in pages, i.e.Mb\nBE\n\u000fTratio between levels of LSM-tree such that\n\u000fL1 =T\u0003Pb\u0003B,L2 =T2\u0003Pb\u0003B, and so on,\nthen we can solve for Lthe total number of levels required\nto store all the data:Pb\u0003B\u00031\u0000TL\n1\u0000T=N\nL=dlogT\u0012N(T\u00001)\nBPb+ 1\u0013\ne\nThe average cost of a write remains the same as for the\nbasic LSM-tree case:\nwrite cost = logTN\nPbB\nThe average cost of a read must be considered probabilis-\ntically over all possible locations of the read item, in this\ncase assuming a uniformly random distribution of reads:\n\u000fProbability that read is in write bu\u000ber = p(MT) =\nPb\u0003B\nN\n\u000fProbability that read is in cache = p(cache) =Mc=E\nN\n\u000fProbability that read is in L1 but not in cache = p(L1)\n=Pb\u0003B\u0003T\u0000Pb\u0003B\u0003T\nN\u0000Pb\u0003B\u0003Mc=E\nN\nwhere the numerator is the number of items Pb\u0003B\u0003T\nthat are in the \frst level minus the proportion of items from\nthat level that are probabilistically in the cache already:\nPb\u0003B\u0003T\nN\u0000Pb\u0003B\u0003Mc=E\n\nFigure 15: Uniform and Round-Robin simulation results\noverlaid with gradient estimates.\nFigure 16: Zipf simulation results overlaid with gradient es-\ntimates for s= 1:1 (lightly skewed) and s= 1:5 (highly\nskewed).\nand \fnally where the N\u0000Pb\u0003Bcomes from the fact that\nitems already in bu\u000ber (L0) are not allowed to occupy the\ncache.\nFigure 17: Discover-Decay simulation results overlaid with\ngradient estimates for lightly skewed and highly skewed.\nFigure 18: Periodic Decay simulation results overlaid with\ngradient estimates for variations in periodicity (T) and skew\n(Beta distribution parameters).\nTherefore, given a uniform query distribution, the full ex-\n\npected cost in disk reads of a read is\nE[Cuniform ] =p(MT)\u00030 +p(cache)\u00030 +LX\ni=1p(Li)\u0003i\n=LX\ni=1Pb\u0003B\u0003Ti\u0000Pb\u0003B\u0003Ti\nN\u0000Pb\u0003B\u0003Mc=E\nN\u0003i\nC.7 Bloom Filters\nThe previous analysis hasn't yet accounted for the pres-\nence of Bloom \flters, which reduce the likelihood we will\nunnecessarily access a larger level. For a Bloom \flter of\nkbits withhindependent hash functions h1;h2;:::hh, the\nprobability that a given bit is still set to 0 after inserting n\nkeys is\n(1\u00001\nk)n\u0003h\nThen the probability of a false positive is\n(1\u0000(1\u00001\nk)n\u0003h)h\u0019(1\u0000e\u0000hn=k)h\nWe can minimize this over hto \fnd the optimal number of\nhash functions, which is h= ln(2)\u0003k\nn. Assuming that this is\nthe number of hash functions hwe will use, the probability\nof a false positive as a function of the number of bits is then\n(1\u0000e\u0000ln(2)\u0003k=n\u0003n=k)ln(2)\u0003k\nn= (1\n2)ln(2)\u0003k\nn\u0019(:6185)k\nn\nFor an item in any level Liof the LSM-tree with i\u00152\nwe can reduce the expected cost of accessing that item from\nLiby the number of Bloom \flter negatives at any level j <i .\nThen the expected cost of accessing an item at Liis\ni\u00001X\nj=1p(fpj)\u00031 + 1\nWherep(fpj) is the probability of a false positive for that\nkey at level jand 1 is the cost of actually accessing the item\nat leveliassuming fence pointers that lead us to the correct\npage.\nC.8 Cost with Bloom Filters - Base Case\nAssuming a random distribution of reads, we now consider\nalso the probability that a Bloom \flter allows us to ignore\na level:\nExpected cost of read for an item in the tree =\np(mt)\u00030 +p(cache ) + 0 +LX\ni=1p(Li)\u0003i\u00001X\nj=1p(fpj)\nExpected cost for a null result read =PL\nj=1p(fpj)\nGiven a total memory allocation M, the total number of\nbits we can allocate to Bloom \flters is M\u0000Mc=PL\ni=1mi\nThen the total formula for the expected cost of a read in the\ntree is:E[c] =PL\ni=1B\u0003P\u0003Ti\u0000Pb\u0003B\u0003Ti\nN\u0000Pb\u0003B\u0003Mc=E\nN\n\u0001\" i\u00001X\nj=1(:6185)mj\nPb\u0003B\u0003Tj!\n+ 1#\n(9)\nWhereas with a given percentage of null reads in the work-\nloadpnull:\nE[c] = (1\u0000pnull)PL\ni=1Pb\u0003B\u0003Ti\u0000Pb\u0003B\u0003Ti\nN\u0000Pb\u0003B\u0003Mc=E\nN\n\u0001\" i\u00001X\nj=1(:6185)mj\nPb\u0003B\u0003Tj!\n+ 1#\n+pnullLX\nj=1p(fpj) (10)\nE[c] =PL\ni=1(1\u0000pnull)Pb\u0003B\u0003Ti\u0000Pb\u0003B\u0003Ti\nN\u0000Pb\u0003B\u0003Mc=E\nN\n\u0001\" i\u00001X\nj=1(:6185)mj\nPb\u0003B\u0003Tj!\n+ 1#\n+pnull\u0001p(fpi) (11)\nC.9 Cost Gradients w. Bloom Filters - Gener-\nalized Distribution\nC.9.1 Cache Gradient\nNote that in the above, the workload speci\fc factors are\nthe probability that a read is at any given level and the re-\nlated probability that any given item from a level is already\nin the cache. To compute an empirical estimation of the\nprobability that any given item is in a level but not already\nin the cache, we can simply keep statistics on the total num-\nber of times a key was found in that level divided by the\ntotal number of (non-null) read queries executed. Then we\ncan consider the following simpli\fcation:\nE[c] =PL\ni=1(1\u0000pnull)h\np(Li)\u0000p(Li)\n(N\u0000PbB)\u0003Mc=Ei\n\u0001\" i\u00001X\nj=1(:6185)mj\nPb\u0003B\u0003Tj!\n+ 1#\n+pnull\u0001p(fpi) (12)\nTaking the derivative with respect to the number of entries\nin the cache, Mc=E, we get:\nLX\ni=1\u0000(1\u0000pnull)p(Li)=(N\u0000PbB)\u0001\" i\u00001X\nj=1(:6185)mj\nPb\u0003B\u0003Tj!\n+ 1#\nWhich is just the average cost of a read throughout the\ntree. Then, to keep statistics on how valuable we expect the\ncache to be, we maintain statistics on the average cost of\nevery read performed in the window of interest.\nC.9.2 Bloom Filter Gradients\nBecause the memory allocation problem is discrete any-\nway, we consider the value of the Bloom \flters as a \fnite\ndi\u000berence, that is the approximate value of any marginal\nbloom \flter bit at level kwill beE[cjmk+ 1]\u0000E[cjmk]. In\nthis computation, all terms in the sums drop out except for\nthose concerning mj, and we are left with:\n\nPL\ni=k(1\u0000pnull)h\np(Li)\u0000p(Li)\n(N\u0000PbB)\u0003Mc=Ei\n\u0001\u001a\u0014\u0012\n(:6185)mk+1\nPb\u0003B\u0003Tj\u0013\n+ 1\u0015\n\u0000\u0014\u0012\n(:6185)mk\nPb\u0003B\u0003Tj\u0013\n+ 1\u0015\u001b\n+pnull\u0012\n(:6185)mk+1\nPb\u0003B\u0003Tj\u0000(:6185)mk\nPb\u0003B\u0003Tj\u0013\n(13)\nRearranging terms, we get:\nPL\ni=kh\n(1\u0000pnull)h\np(Li)\u0000p(Li)\n(N\u0000PbB)\u0003Mc=Ei\n+pnulli\n\u0001\u0012\n(:6185)mk+1\nPb\u0003B\u0003Tj\u0000(:6185)mk\nPb\u0003B\u0003Tj\u0013\n(14)\nWhere this is exactly the number of times the given bloom\n\flter is accessed times the di\u000berence in the theoretical false\npositive rates given memory allocations mjandmj+ 1.\nThen, to keep statistics on how valuable we expect any given\nBloom \flter to be, we maintain statistics on the number of\ntimes every Bloom \flter was accessed in the window of in-\nterest.\nC.9.3 Write Buffer Gradient: Gets\nTo estimate the additional value of any marginal memory\nin the write bu\u000ber with respect to reads, we must make a\nnumber of simpli\fcations, as Pb, the number of pages in the\nwrite bu\u000ber, factors into every term in this equation. Fur-\nther, the interaction between Pband most of the terms is\nnot available in closed form, in general. Rather, the crit-\nical termsP(Li) we are empirically estimating. Then, for\nreasonably large values of NandPb, we will assume that\nthe Bloom \flter false positive rate stays approximately the\nsame, as does the value of the cache. Then, we consider only\nthe change in I/Os occurring from the altered probability of\nany given element occurring in any level as a result of more\nelements being in the write bu\u000ber. We can provide a simple\nestimate of this by assuming that any items we add to the\nwrite bu\u000ber would have otherwise occurred in L1, and in\nthe resulting cascade, Titimes that number of items will be\nmoved up into each level Lifrom the level below.\nThen, an appropriate estimate of how useful any addi-\ntional space of memory in the write bu\u000ber is for reads is\nsimply the resulting change in p(Li) for each level (that is,\nthe number of hits we expect to see on the newly added\nelements)\u0003fpifor any level i6= 0, as the original cost of ac-\ncessing that element wasPi\nj=1fpj+ 1, and the new cost of\naccessing isPi\u00001\nj=1fpj, the di\u000berence between which is just\nfpi. Fori= 0, the write bu\u000ber itself, the expected savings\nper hits is exactly 1, as the item will be moved from having\nan access cost of 1 to 0. To estimate how many additional\ntimes L1 would be accessed if we instead allocated the \f-\nnal portion of the write bu\u000ber to L1, we keep statistics on\nhow often the \fnal spots of the write bu\u000ber were accessed\nin a read. In practice, these spots are accessed only very\ninfrequently, as the write bu\u000ber is accessed only a hand-\nful of times at this stage before being \rushed. This statistic\nmight be more helpful on a system with constant compaction\nrather than a full level \rush. For the rest of the levels, we\nsimply assume the same hit rate per key as measured over\nthe existing keys on any level and multiply by the number\nof elements we will be adding to calculate the expected ac-\ncesses to the new keys on each level. We then multiply bythe empirical rate of bloom \flter false positives on the level.\nC.9.4 Write Buffer Gradient: Puts\nFor the write bu\u000ber, we must additionally consider the\nsaved I/Os for the update/insert operations.\nwrite cost = logTN\nPbB\nTaking the derivative with respect to PbB, the number of\nitems in the bu\u000ber, we get1\nPbBIn discrete terms, this eval-\nuates to logTPbB\nPbB+1.\nUnfortunately, this simpli\fcation only works if we can as-\nsume that memory is being allocated in page-size chunks and\nthat the workload has no duplicates. In practice, the number\nof I/Os associated with reading and writing throughout the\nmerging process is a stepwise function that depends on page\nsize, as reading or writing one element from or to a page has\nthe same I/O cost as reading or writing a full page. To sim-\nplify our analysis of the page size write savings, we consider\nonly a ratio of T= 2, and we begin by addressing the case\nwth no duplicates.\nWith no duplicates, the \fnal number of elements at any\nlevel of the tree is a deterministic function of the number of\nelements inserted as well as the level sizes. Then considering\nthe empirical number of items inserted into the bu\u000ber as\nwell as the size of the original bu\u000ber, we can solve for the\ntheoretical \fnal structure of an alternate LSM-tree that had\na bu\u000ber of size PbB+ 1.\nAdditionally, given the number of elements on any given\nlevel, no duplicates, and an original bu\u000ber size PbB+ 1, we\nknow the number of times each Ti\u0003(PbB+ 1)-size chunk on\neach level will have been read and written given the current\nfullness of the level. We can then multiply these numbers\nof known chunk reads and writes by the ceiling of the size\nof those possible chunks (which, with ratio T= 2 will be\nTi\u0003(PbB+ 1) andTi\u0003(PbB+ 1)\u00032) divided by pagesize,\nB. This gives us a more realistic number in which additions\nof less than a pagesize of memory are not helpful in I/O\nsavings.\nComparing the read and write costs of this theoretical tree\nto the empirical reads and writes accesses of the existing tree\ngives us an expected I/O savings related to updates for the\nlarger tree.\nWe consider additionally the fact that I/O savings are in\ngeneral lessened by the number of duplicates inserted, as\nduplicates will not be merged across the full depth of the\ntree. To take this into account we also keep a statistic for\nthe total number of duplicates merged over the window of\ninterest per level and use this to calculate the percentage\nof duplicates removed relative to total keys at each level.\nThis factors in in several places. First, when computing the\ntheoretical allocation of keys in the \fnal tree, we consider\nthe total number of items that would have come in to the\nbu\u000ber from the empirical count and adjust this at each level\nby the percentage that are expected to have been removed as\nduplicates. Further, when computing read and write I/Os\nduring merging, we expect that number of items written\nwhen the level is already half full should be decreased by the\nexpected number of duplicates removed among the two sets\nof keys. Again, the resulting I/O savings will be stepwise in\npagesize. In particular, if the original size of the array would\nhave only been slightly into the \fnal page, it will take very\nfew duplicates to reduce the I/O count by 1, whereas if all\n\npages would have been full, it will take a full page's worth\nof duplicate removals to improve I/Os. The same savings\nwill be experienced again when these items are read to be\nmerged into the lower level.\nThe correct way to handle the duplicates requires some-\nwhat more consideration, but the only statistics we are cur-\nrently using The are the empirical number of update queries\nand the empirical number of duplicates found and removed\non each level over the window of interest.\nC.10 Estimating Statistics with O(1)Memory\nCache: to estimate the number of storage accesses we\nwill save by adding dMextra bits of memory to the cache,\nwe let consider dMas a number of extra entries in the cache.\nThat is, we calculate the savings from having dM=E extra\ncache entries available. As mentioned above, the relevant\nstatistic here is the average cost of a read in the database.\nTo calculate this, we collect statistics on the total number of\nstorage accesses and total number of queries. The expected\ncost per query is then the number of disk accesses over the\nwindow divided by the total number of queries. To approxi-\nmate the probability of the item being in the cache times the\nnumber of queries, we maintain a statistic for the number of\ntimes the last cache slot was accessed during the window of\ninterest and make the assumption that the number of hits on\nthe next marginal slot(s) would be approximately the same.\nThen we can calculate the \fnal expected I/O savings as\ndM=E\u0003E[hits]\u0003E[cost=query ]\nBloom Filters: To estimate the number of storage ac-\ncesses we will save by adding dM extra bits of memory\nto the Bloom \flters, we \frst decide how to allocate that\nM0\nbloom =Mbloom +dMbits using Monkey or the baseline\nallocation, giving us miandm0\nibits per Bloom \flter on\neach level. At each level i, for both miandm0\ni, we up-\ndate rolling averages of the theoretical false positive rate\n^fpi=Eh\n0:6185mi\nnii\nand ^fp0\ni=E\u0014\n0:6185m0\ni\nni\u0015\nevery time\nthe Bloom \flter is queried (where niis constantly changing\nbased on insertions and \rushes of the \flter). These statis-\ntics (individual \roats) give us an estimate of the aggregate\nfalse positive rate at miandm0\nirobust to changing level\nfullness. Finally, we keep a counter ni;bloom false of the num-\nber of times requested items are notin bloom \flter i. This\ncounter is incremented either when the bloom \flter returns\nfalse (which we know immediately) or returns a false positive\n(which we can record after fruitlessly searching the level).\nThis counter allows us to estimate storage accesses resulting\nfrom our current or altered false positive rates. The \fnal\nsavings is therefore\nSavings(M0\nbloom ) =X\ni(^fp0\ni\u0000^fpi)\u0003ni;bloom false;\nand only requires keeping two \roats and one integer. Note\nthat in our simulation, for \rexibility, we keep a histogram\nofnivalues at each bloom \flter request to avoid needing to\npredetermine m0\ni, but in a practical implementation this is\nunnecessary.\nNote that because we can obtain these estimates on a\nlevel-by-level basis, we can investigate whether reallocat-\ning memory from one Bloom \flter to another, empirically,\nshould reduce I/Os. Validating the results of Monkey [24],in Figure 19 we \fnd that for the baseline allocation, moving\nbits does improve performance, but for Monkey, it does not,\nregardless of workload.\nFigure 19: Estimated change in I/Os when moving bits from\none Bloom \flter to another (keeping total bloom \flter mem-\nory constant). Regardless of workload, changes in I/Os for\nMonkey are all less than 1, indicating its optimality.\nBu\u000ber: To estimate the number of storage accesses we\nwill save in reads by adding dMextra bits of memory to the\nbu\u000ber, we use statistics maintained on the total bloom \flter\naccesses per level, Bloom \flter false positives per level, and\nhits per level. We estimate the expected additional number\nof hits on any given level as the original hits times the new\ntheoretical size divided by the actual original size. That is,\nthe number of extra hits is equal to\nnewhitsi=hitsi\u0003sizei+dM\u0003Ti\nsizei\nFor each expected hit, we have an I/O savings equal to the\nfalse positive rate on the bloom \flter of that level, as de-\nscribed in the previous section. To calculate this for a level\ni, we use\nE[savings=hit ]I=falsepositivesi\nbloomaccessesi\nThen the total number of I/Os saved should be\nLX\ni=0newhitsi\u0003E[savings=hit ]i\nwhere for level 0, the write bu\u000ber, the E[savings=hit ] = 1,\nas the access cost at L1 is always exactly 1 and the access\ncost at the write bu\u000ber is always 0.\nTo estimate the number of storage accesses we will save\nin writes/updates by adding dMextra bits of memory to\nthe write bu\u000ber, we maintain statistics on total number of\nentries that passed through any given level, number of du-\nplicates removed at any given level, and number of entries\nin any given level at the end of the period. For a workload\nwithout duplicates, we can simply use these statistics to de-\nterministically calculate the \fnal allocation and number of\nread and write I/Os that would have occurred throughout\n\nthe process for a second tree with write bu\u000ber size + dM, cal-\nculating every batch of read and write merges and summing\nover the number of pages that would have been involved.\nFor the original tree we can either use statistics on empirical\nI/Os during the merging process or use the same determin-\nistic formula to calculate what they would have been. The\nexpected saved I/Os then is simply\ncosttree\u0000costtree+dM\nWhen we consider duplicates, the estimate becomes much\nmore noisy. To consider the e\u000bect of duplicates on reduc-\ning the total number of pages read and written during the\nmerging process, we reduce the number of entries that pass\nthrough each level of our theoretical larger tree by the per-\ncentage of duplicates removed at each level, calculated as\nduplicates removedi\ntotalentriesi\nThis then changes the \fnal level structure of the estimated\ntree. We also consider that duplicates should reduce the to-\ntal number of entries written and then read after two seg-\nments are merged together. Then for those read and write\ncomponents that occur on an already half-\flled level, we\nreduce the number of elements by multiplying by\n1\u0000duplicates removedi\ntotalentriesi\nThis will reduce the total I/Os by number of page reads it\nmakes unnecessary. With this adjusted cost for the larger\ntree, we again calculate the expected saved I/Os as the esti-\nmated I/Os of the hypothetical larger tree subtracted from\nthe empirical or theoretical I/Os of the existing tree.\nC.11 Gradient Validation\nTo con\frm that our estimates are reasonable, we ran 250\nsimulations for three separate workloads and compared our\nestimates of each gradient to the actual savings for a sepa-\nrate tree with 8 bytes of extra memory in the corresponding\nLSM component (against which we ran the same workload).\nResults can be seen in Figure 20.\nThere is a large amount of variance in the simulated re-\nsults, both because of randomness in the separate instanti-\nations of the workload and randomness in the execution of\nits queries, but for the most part, our estimates of the aver-\nage savings are both precise and accurate. There is a slight\ndeviation for the uniform bu\u000ber savings calculation, but the\nvariance is so high that it does not appear to be signi\fcant.\nThe fact that our estimates of the expected I/O savings\nare so precise across workloads gives us con\fdence \frst that\nour simulation and modeling are correct, and second that\nthey will generalize to more complex, real-world workloads\nwith more queries and keys.\n\nFigure 20: Light-footprint statistical estimations of the gradient vs. simulated results for cache, Bloom \flters, and the write\nbu\u000ber on three distinct workloads.",
  "textLength": 142387
}