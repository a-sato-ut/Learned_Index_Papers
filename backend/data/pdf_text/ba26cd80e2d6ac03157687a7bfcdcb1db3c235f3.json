{
  "paperId": "ba26cd80e2d6ac03157687a7bfcdcb1db3c235f3",
  "title": "FlexSketch: Estimation of Probability Density for Stationary and Non-Stationary Data Streams",
  "pdfPath": "ba26cd80e2d6ac03157687a7bfcdcb1db3c235f3.pdf",
  "text": "sensors\nArticle\nFlexSketch: Estimation of Probability Density for Stationary\nand Non-Stationary Data Streams\nNamuk Park and Songkuk Kim *\n/gid00030/gid00035/gid00032/gid00030/gid00038/gid00001/gid00033/gid00042/gid00045 /gid00001\n/gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00046\nCitation: Park, N.; Kim, S.\nFlexSketch: Estimation of Probability\nDensity for Stationary and\nNon-Stationary Data Streams. Sensors\n2021 ,21, 1080. https://doi.org/\n10.3390/s21041080\nAcademic Editor: Andrea Facchinetti\nReceived: 30 December 2020\nAccepted: 28 January 2021\nPublished: 4 February 2021\nPublisher’s Note: MDPI stays neu-\ntral with regard to jurisdictional clai-\nms in published maps and institutio-\nnal afﬁliations.\nCopyright: © 2021 by the authors. Li-\ncensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and con-\nditions of the Creative Commons At-\ntribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).School of Integrated Technology, Yonsei University, Incheon 21983, Korea; namuk.park@yonsei.ac.kr\n*Correspondence: songkuk@yonsei.ac.kr\nAbstract: Efﬁcient and accurate estimation of the probability distribution of a data stream is an\nimportant problem in many sensor systems. It is especially challenging when the data stream is non-\nstationary, i.e., its probability distribution changes over time. Statistical models for non-stationary\ndata streams demand agile adaptation for concept drift while tolerating temporal ﬂuctuations. To this\nend, a statistical model needs to forget old data samples and to detect concept drift swiftly. In\nthis paper, we propose FlexSketch , an online probability density estimation algorithm for data\nstreams. Our algorithm uses an ensemble of histograms, each of which represents a different length\nof data history. FlexSketch updates each histogram for a new data sample and generates probability\ndistribution by combining the ensemble of histograms while monitoring discrepancy between recent\ndata and existing models periodically. When it detects concept drift, a new histogram is added to the\nensemble and the oldest histogram is removed. This allows us to estimate the probability density\nfunction with high update speed and high accuracy using only limited memory. Experimental results\ndemonstrate that our algorithm shows improved speed and accuracy compared to existing methods\nfor both stationary and non-stationary data streams.\nKeywords: probability density estimation; streaming data; sensor system\n1. Introduction\nEstimating the probability density function (PDF) of a random variable based on a\nstream of data samples from sensors is a fundamental problem arising in a broad range\nof areas such as machine learning [ 1], data structures [ 2], and systems [ 3]. There are two\nrecent challenges in this problem.\nFirst, the characteristics of a data stream often change. This might be due to the\naccidental ﬂuctuation caused by an insufﬁcient number of samples. In this case, a sufﬁcient\namount of data could reduce the ﬂuctuation gradually. In some other cases, the data stream\nitself is non-stationary. In other words, the probability distribution of a random variable\nover the data stream, called concept, changes over time, which is called concept drift [ 4].\nConcept drift occurs in many types of data such as temporal sensor data [ 5], video [ 6],\nand spatiotemporal data [ 7]. A static model constructed with the assumption of stationarity\nof the data stream may lead to an erroneous conclusion under the presence of concept drift.\nTherefore, there is a need for a method that can estimate PDFs adaptively according to\nconcept drift for practical applications.\nSecond, real-world applications need to be able to handle increasing amounts of\ndata and high-speed data streams while keeping low latency. Therefore, the demand\nfor an online algorithm to estimate PDFs with high speed and high accuracy using only\na small amount of memory is ever-increasing. While there exist online PDF estimation\nalgorithms in literature [ 8–11], they usually cannot update probability models at high\nspeed. Furthermore, they cannot adapt well to various types of data streams including\nthose with concept drift.\nIn order to deal with these challenges, we propose FlexSketch , which is an online\nprobability density estimation algorithm that achieves high update speed and high accu-\nSensors 2021 ,21, 1080. https://doi.org/10.3390/s21041080 https://www.mdpi.com/journal/sensors\n\nSensors 2021 ,21, 1080 2 of 19\nracy with only a small amount of memory for both stationary and non-stationary data\nstreams. As shown in Figure 1, FlexSketch estimates the PDF by using an ensemble structure\ncomposed of several statistical models. In particular, we exploit histogram for the statistical\nmodels, which allows fast and low-memory operations for data streams. Each histogram\nrepresents statistics of a different length of data history. FlexSketch updates each histogram\nfor a new data sample and builds a new histogram when it detects the concept drift of\ninput data. By decoupling updating each statistical model from amending the composition\nof the ensemble, FlexSketch achieves high accuracy both for stationary and non-stationary\ndata streams.\nS\n⋮\nMμ-1\nMμxt\noutputnew MMAJORUPDATE\nPROBABILITYdata \nstreamMINORUPDATE\nM1Q xt-1 j x1\nFigure 1. Overview of the proposed FlexSketch .\nTo adapt non-stationary data streams in an efﬁcient way, a single operation updating\nthe estimated PDF according to the data stream is divided into two elementary opera-\ntions: “fast and minor update operation” ( MINOR UPDATE ) and “slow and major update\noperation” ( MAJOR UPDATE ). The type of operation to be performed varies depending\non the characteristics of the data stream. When minor changes occur in a data stream,\nFlexSketch updates the PDF for the data stream at high speed by using MINOR UPDATE .\nThis operation simply updates each model in FlexSketch . In contrast, when major changes\noccur in the data stream, FlexSketch updates the PDF by using MAJOR UPDATE , which builds\na new model including the recent data stream and adds it to FlexSketch . Finally, FlexS-\nketch has multiple versions of a model, ranging from a version representing only recent\ndata to a version representing both recent and old data. FlexSketch constructs the PDF by\nlinear combination of these coarse models.\nFlexSketch dynamically decides when to forget old data and to build a new statistical\nmodel by measuring divergence between the current model and recently sampled data.\nThis allows FlexSketch to stay stable when the concept of the data stream does not change,\nand to tolerate temporal out-linear data. In addition, FlexSketch achieves agile adaptation\nto sudden or incremental concept drift since MAJOR UPDATE integrates a newly built\nhistogram, which amends the statistical model. Though the histogram is a compact\ndata structure and easy to maintain, it may provide coarse information about probability\ndistribution. FlexSketch alleviates this problem by incorporating an ensemble of histograms.\nThe current implementation of FlexSketch only supports one-dimensional data. There are\nmany applications relying on statistical modeling of one-dimensional data streams such\nas online anomaly detection [ 5,12], fault-detection [ 13] and DDoS detection [ 14], which are\npotential areas where FlexSketch is applicable. While many online sensor-based applications\nhandle one-dimensional data, other applications need to deal with multi-dimensional data\nstreams. Since the limited dimension of FlexSketch circumscribes the application of FlexSketch\nin some areas such as machine learning for high dimensional data [ 15] and anomaly detection\nbased on multivariate data [ 16],FlexSketch needs to be expanded to handle multi-dimensional\ndata for wider applications.\n\nSensors 2021 ,21, 1080 3 of 19\nThe experimental results demonstrate that FlexSketch updates PDFs for data streams\nup to 16\u0002faster than the alternatives while using only a limited amount of memory.\nMoreover, FlexSketch adapts well to various types of concept drift, and it is more accurate\nthan the alternatives.\nThe main contributions of FlexSketch are as follows:\n• We propose a new method to estimate probability distribution for data streams with\nconcept drift.\n• FlexSketch decouples adapting to concept drift from adjusting the statistical model for\nstationary data by incorporating two separate operations.\n• FlexSketch achieves low computational overhead and high throughput, which are\ncritical for processing of stream data, using an ensemble of compact histograms.\nThe remainder of the paper is organized as follows. Section 2 brieﬂy surveys the\nrelated work. In Section 3, the proposed FlexSketch is described in detail. Section 4 presents\nextensive experimental results. Finally, the conclusions are given in Section 5.\n2. Related Work\nThere are multiple research topics to deal with data streams with concept drift [ 17].\nSupervised classiﬁcation of data streams with concept drift is also studied extensively,\ne.g., [ 18–21]. The core parts of these researches are how to detect concept drift, how to forget\nold data, and how to rebuild a new statistical model. Some methods [ 9,22] gradually update\nstatistical model without explicit detection of concept drift. Other studies attempt to detect\nconcept drift in batch-based methods [ 23–26] and online methods [ 27–29]. While [ 30,31]\nrely on process control, FlexSketch detects concept drift using a multiple-window-based\nmethod like [ 32]. Researches also focused on how to measure difference in distribution\nbetween recent data and old data. The common methods are based on entropy or KL-\ndivergence [ 33–35]. We introduce an error based metric to detect concept drift in Section 3.1 .\nTo deal with non-stationary data, statistical models should be able to forget old data\nor to depreciate their contribution. Some methods [ 9,36,37] decay the importance of old\ndata linearly or exponentially. This approach is good for tolerating temporal ﬂuctuations.\nSince gradual decaying is slow to adapt to sudden concept drift, [ 38,39] use sliding win-\ndow mechanisms to keep some recent data and to discard old data. FlexSketch deploys\nboth gradual decaying and abrupt forgetting. When the input data is stationary, FlexS-\nketch depreciates old data exponentially. However, when FlexSketch detects concept drift,\nit discards the oldest histogram and incorporates a new histogram, which allows FlexSketch\nagile adaptation.\nThere exist many kinds of density estimation algorithms for data streams. Tradition-\nally, kernel density estimation is performed as a batch-processing algorithm for density\nestimation of a dataset. There are some algorithms [ 9–11] generalized to online processing\nfor adaptive density estimation. They usually constitute a Gaussian mixture model by\nassigning a Gaussian kernel to newly added data and thereafter merge kernels based on\ncertain rules. Particularly, the online kernel density estimation (denoted by oKDE in this\npaper) [ 9] can adapt to concept drift by enabling to forget past data. However, this method\nis slow in updating the estimated probability density due to the requirement of relatively\nmassive calculation. In contrast, we focus on developing an efﬁcient mechanism that\ncan update the probability density adaptively by exploiting updating operations having\ndifferent levels of computational complexity.\nThere also exist methods to estimate the distribution of a data stream based on his-\ntograms, including the streaming parallel decision tree (denoted by SPDT in this paper) [ 8],\nvariations of the V-optimal histogram algorithm [ 40,41] and quantile summarization algo-\nrithms [ 42–45]. However, they cannot forget the contribution of the past data and adapt to\nvarious types of concept drift. To solve this problem, model maintenance strategies using\nﬁxed and variable size sliding windows [ 46–48] for histograms can be used. [ 7] proposes\na histogram-based sketch mechanism with gradual forgetting. However, it is unknown\nwhether they guarantee satisfactory performance when different types of concept drift\n\nSensors 2021 ,21, 1080 4 of 19\noccurs. On the other hand, FlexSketch efﬁciently updates the statistical model for both high\naccuracy and high efﬁciency by amending the composition of the ensemble.\nThere are some studies using ensemble methods, e.g., [ 49–51]. While some meth-\nods [ 33,34] use hierarchical structure, FlexSketch uses ﬂat combination of compact and\nsimple data structure like [ 19,52]. Most of previous works focus on improving accuracy\nof supervised classiﬁcation for data streams with concept drift. On the other hand, FlexS-\nketch uses the ensemble technique to improve the update speed of density estimation.\n3. Proposed Method\nThe goal of our method is to estimate the PDF of stationary (i.e., without concept drift)\nand non-stationary (i.e., with concept drift) data stream at high speed and high accuracy\nwhile using a small amount of memory. Here, the meaning of accurately estimating the\nPDF for a stationary data stream is straightforward. On the other hand, for a non-stationary\ncase, it is not simple and has several aspects. We consider the accuracy of density estimation\nfor a non-stationary data stream from three points of view. First, the estimated PDF should\nforget old concepts quickly after concept drift occurs. Second, the estimated PDF should\nadapt to the latest concept as soon as a concept drift occurs. Third, the estimated PDF\nshould remain stable even if an accidental outlier occurs in the data stream.\nTo fulﬁl these requirements, the proposed method is built on the following ideas.\n(a) We choose a histogram as the statistical model. When there are only minor changes\nin the data stream, a histogram is a suitable model since it can be updated at a\nhigh speed.\n(b) Our method uses an ensemble data structure consisting of several histograms.\nThe ensemble structure can compensate for inaccuracy of a histogram.\n(c) We design two adaptation techniques. If the data stream is stationary or there are\nonly minor changes in it, FlexSketch updates the models, i.e., histograms. On the\nother hand, if there are major changes in the data stream, updating the models may\nnot guarantee sufﬁcient accuracy. To address this issue, we generate a new model\nthat represents the changed data stream and adds it to the data structure.\nLetSdenote the data structure of the FlexSketch framework. Sconsists of a recent\ndata stream and multiple versions of a statistical model as follows:\nS\u0010tQ,M1, . . . ,MNM,n1, . . . , nNMu (1)\nwhere Qis the buffer for the recent dataset given through the input data stream, Miis the\nith histogram, NM¥2is the total number of histograms, and niis the number of data used\nto update Mi. Since we build a new histogram when a major change in the data stream is\ndetected, the histograms in Sare created at different times. As a convention, M1is the most\nrecently added one and MNMis the oldest one. This means that Mi\u00001is older than Mi\nand thus undergoes more updates. Therefore, ni\u00001¡ni. Each histogram is a set of disjoint\nintervals called bins ( Ij) and the frequency count ( mj) for each bin, where j\u00101, . . . , NB:\nM\u0010tI1, . . . , INB,m1, . . . , mNBu. (2)\nTwo important operations of FlexSketch are (a) the update operation for the data\nstream online and (b) the query operation to obtain the probability for a certain data,\nwhich are explained below.\n3.1. Update Operation\nAlgorithm 1 summarizes the operation to update Swith a given data sub-stream\nXwhose number of data is |X|. First, Sis updated at a minor level using operation\nMINOR UPDATE and Xis appended in the buffer. If the size of the buffer Qexceeds a\nthreshold NQ, i.e., if only the minor update has been performed with a certain number\nof data, the adequacy of the most recently added model M1is examined by operation\n\nSensors 2021 ,21, 1080 5 of 19\nDIAGNOSE . If a large discrepancy is found from the operation between the recent data\nstream stored in QandM1, the major update operation ( MAJOR UPDATE ) is performed\nand the buffer is cleared.\nAlgorithm 1: Update operation.\nInput: S\u0010tQ,M1, . . . ,MNM,n1, . . . , nNMu, sub-stream: X, size limit of Q:NQ, number of models: NM,\nDIAGNOSE threshold: g\nOutput: Updated S\n1SÐMINOR UPDATEpS,Xq\n2QÐQYX\n3if|Q|¡NQthen\n4 ifDIAGNOSEpM1,Qq¥gthen\n5 SÐMAJOR UPDATEpS,Qq\n6 end\n7 QÐ?\n8end\n9return S\n10function MinorUpdate( S, X):\n11 foriÐ1toNM\u00011do\n12 MiÐSINGLE UPDATEpMi,Xq\n13 niÐni\u0000|X|\n14 end\n15SÐtQ,M1, . . . ,MNM,n1, . . . , nNMu\n16 return S\n17function MajorUpdate( S, X):\n18M1ÐBUILDpS,Xq\n19 n1Ð|X|\n20SÐtQ,M1,M1, . . . ,MNM\u00011,n1,n1, . . . , nNM\u00011u\n21 return S\nThe MINOR UPDATE operation causes only minor changes of S. The steps of the\noperation are shown in lines 10 to 16 of Algorithm 1. Each histogram in Sexcept the oldest\none (MNM) is updated by operation SingleUpdate. This operation consists of two steps.\nFirst, it searches the bin Ijwhose interval contains each data in X,x. For fast search, we use\nthe red-black tree method. Second, it increases the count of Ij,mj, by 1. If there is no bin for\nx,xis ignored. This causes a discrepancy between the histogram and X, which is resolved\nby the M AJOR UPDATE operation.\nThe DIAGNOSE operation measures the amount of discrepancy between the most\nrecent model M1and the data stream in Q. If the output of the operation is larger than a\nthreshold g, we consider that concept drift occurs and the MAJOR UPDATE operation needs\nto be performed.\nThere are some requirements for the DIAGNOSE operation. First, its result should be\ninvariant under scaling transformation of the data, so that the threshold is independent of\nthe scale of the data. Second, the result of DIAGNOSE should be stable even when only a\nsmall number of data are given; otherwise, MAJOR UPDATE is performed too frequently,\nwhich results in increased computational complexity, and the PDF estimation becomes\ninaccurate. Third, the result of DIAGNOSE must be ﬁnite even if the input dataset is not\nincluded in the domain of the histogram; otherwise, the result will diverge whenever\naccidental outliers deviate from the domain.\nTo design a DIAGNOSE operation satisfying these requirements, we ﬁrst deﬁne the\nerror function Dpxqbetween XandMas the absolute difference between the cumulative\n\nSensors 2021 ,21, 1080 6 of 19\ndistribution function (CDF) of M,CDF Mpxq, and the empirical distribution function (EDF)\nforX, EDF Xpxq(Figure 2a):\nDpxq\u0010| CDF Mpxq\u0001EDF Xpxq|. (3)\nThen, a representative value of the error function serves as the output of the DIAGNOSE\noperation. We consider two options, i.e., the maximum value given by\nq#\u0010max\nxDpxq (4)\nand the mean value given by\ns#\u0010»1\n0D\u0004ppDqdD (5)\nwhere ppDqis the PDF for D.q#ors#may be used as the result of DIAGNOSE directly.\nNote that since Dpxqis the difference between two probability distributions, its value can\ntake only between 0 and 1, which is the range of the integration. However, we note that\nthe range of q#ands#, which is between 0 and 1, is too narrow for practical use. Thus,\nfor convenience, we scale them to obtain the ﬁnal output of D IAGNOSE as follows:\nd\u0010#\n1\u0001#(6)\nNote that d\u00100for#\u00100,d\u0014#for small #(i.e.,#!1), and d\u00108 for#\u00101. Therefore,\nthe output of D IAGNOSE ranges from 0 to 8through this scaling.\n2\n 1\n 0 1 2\nx0.00.20.40.60.81.0CDFCDF\nEDFX\n(a)\n0.0 0.1 0.2 0.3 0.4 0.5\n0.00.10.20.30.4p()\nmaxmean (b)\nFigure 2. Example of the error function. ( a) The error function is given as the difference (solid line) between the cumulative\ndistribution function (CDF) of a model (dashed line) and the empirical distribution function (EDF) of a sub-stream (dash-dot\nline). ( b) The probability density function (PDF) for the value of the error function.\nThe MAJOR UPDATE operation is shown in lines 18 to 21 in Algorithm 1. It ﬁrst builds\na new histogram M1with Xthrough operation BUILD in order to accommodate a big\nchange (i.e., concept drift) in the data stream. Then, the new histogram is enqueued to Sas\nthe ﬁrst model of the ensemble and the oldest one ( MNM) is dequeued from S.\nThe BUILD operation is shown in Algorithm 2. It basically creates a new histogram that\ncovers the data ranges of both the existing histograms in Sand the recent data, so that the\nnew histogram can account for the characteristics of the recent data. First, the boundaries\nof the bins of the new histogram are obtained so as to uniformly split the range of the\ncombined CDF (CDF of Sand EDF of X) (lines 2 to 4 of Algorithm 2). Then, the EDF of X\nis used to obtain the count in each bin (lines 5 to 8 of Algorithm 2).\n\nSensors 2021 ,21, 1080 7 of 19\nAlgorithm 2: Build operation.\nInput: S\u0010tQ,M1, . . . ,MNM,n1, . . . , nNMu, sub-stream: X, number of bins: NB\nOutput: new histogram: M1\n1CDFÐpCDF S\u0000EDF Xq{2\n2forjÐ1toNB\u00001do\n3 zjÐCDF\u00011pCDFUp0,1qpj\nNB\u00002qq\n4end\n5forjÐ1toNB\u00001do\n6 IjÐrzj,zj\u00001q\n7 mjÐ|X|\u0004pEDF Xpzj\u00001q\u0001EDF Xpzjqq\n8end\n9M1ÐtI1, . . . , INB,m1, . . . , mNBu\n10return M1\n3.2. Query Operation\nThe query operation is to obtain the probability of a certain input data using Sthat\nhas been established using the past data through the update operations explained above.\nAs mentioned before, we employ an ensemble approach for this using the histograms\ncontained in S. In other words, the probability of a given data xis calculated from Sas a\nlinear combination of the PDFs represented by the histograms Mi, i.e.,\npSpxq\u0010NM¸\ni\u00101ai\u0004pMipxq, (7)\nwhere pMipxqis the probability of xfrom histogram Miandaiis the weight of Mi.\nThe former is given by the proportion of the data count for the bin to which xbelongs, i.e.,\npMpxq\u0010$\n&\n%mj\n|Ij|\u0004°NB\nk\u00101mk,xPIj,\n0, otherwise,(8)\nwhere|Ij|is the size of an interval and we omit the subscript ifor simplicity. The weight ai\nis determined in a way that a newer histogram receives a higher weight. Then, the ﬁnal\nprobability (7) depends more on the histograms that have been built more recently. For this,\nwe use that an older histogram has been updated with more data, i.e., ni\u00001¡ni. Speciﬁcally,\nthe weight aiis set to be negatively proportional to ni, where the exponential function is\nused to ensure the weight value remains positive and, at the same time, to forget an old\nconcept exponentially:\nai\u0010expp\u0001ni\u00011l{NQq\n°NM\nk\u00101expp\u0001nk\u00011l{NQq, (9)\nwith n0\u00100. Here, lis a hyperparameter. Note that°NM\ni\u00101ai\u00101due to the normalization\nand 1¡ai¡ai\u00001¡0 because ni ni\u00001.\nTo see how this works, let us consider the situation where concept drift occurs con-\ntinuously so that we can assume that n\u0010ni\u00001\u0001nianda\u0010ai\u00001{ai\u0010expp\u0001nl{NQqfor\nalli. And, let a PDF of concept Cachanging over time be pCafor positive integer awith\nC1being the latest one. Then, pM1\u0010pC1,pM2\u0010pC1\u0000pC2, . . . , pMNM\u0010pC2\u0000. . .since\nMINOR UPDATE does not update the oldest model. Then, pS9p1\u0000a\u0000\u0004\u0004\u0004\u0000 aNM\u00011qpC1\u0000\npa\u0000\u0004\u0004\u0004\u0000 aNMqpC2\u0000\u0004\u0004\u0004 holds. In other words, the contribution of the concept decreases at\na rate of a. This means that FlexSketch forgets an old concept exponentially.\n\nSensors 2021 ,21, 1080 8 of 19\n4. Experiments\nWe evaluate the computation time, accuracy, and memory usage of the proposed\nmethod for various types of stationary and non-stationary data streams. In particular,\nwe compare our method with the two representative existing density estimation algorithms,\noKDE [9] and SPDT [8].\n4.1. Datasets\nStationary datasets: We consider three distributions. The ﬁrst is a standard normal\ndistribution, Np0, 1q, which appears frequently. The second is a bimodal distribution,\n1{2Np\u00012, 1q\u00001{2Np2, 1q, which is used to test if a density estimation algorithm can recognize\nmultiple modes. The third is a log-normal distribution, lnNp0, 1q, which is used to test\nif an algorithm can estimate a long-tailed distribution. For all cases, one million data are\nrandomly generated to follow the distributions.\nNon-stationary datasets: For non-stationary datasets, we consider three types of\nconcept drift as follows. For each case, we use one million data randomly sampled from\nthe distribution.\n(a) Sudden concept drift is deﬁned as the case where the distribution of the data stream\nchanges suddenly. It is to test how well a density estimation algorithm forgets\nold concepts after concept drift occurs. The underlying distribution is a normal\ndistribution whose mean value changes abruptly, i.e., N\u0000\nxptq, 1\b\n, where xptq\u00100\nfort t1and xptq\u0010x1fort¥t1. We consider t1\u0010300and x1\u00105as shown in\nFigure 3a.\n(b) Incremental concept drift is deﬁned as the case where the distribution of the data\nstream changes incrementally. It is to test how well a density estimation algorithm\nadapts to the latest concept. The underlying distribution is a normal distribution\nwhose mean value moves at a constant speed, i.e., N\u0000\nxptq, 1\b\n, where xptq\u00100for\nt¤t1and xptq\u0010v1\u0004pt\u0001t1qfort¡t1. We set t1\u0010300andv1\u00100.01, as shown in\nFigure 3b.\n(c) Blip concept drift is deﬁned as the case where the distribution of data stream\nsuddenly changes and returns to the original state in a short time. It is to test how\nwell the estimated PDF remains stable even if an outlier occurs. The underlying\ndistribution is a normal distribution whose mean value changes suddenly and\nreturns, i.e., N\u0000\nxptq, 1\b\n, where xptq\u0010x1fort1 t¤t1\u0000teandxptq\u00100otherwise.\nteis the duration of blip concept drift, which is set to te\u00103. We also set x1\u00105and\nt1\u0010300, as shown in Figure 3c.\n260 280 300 320 340\nTimestamp (t)012345x(t)\n(a)\n260 280 300 320 340\nTimestamp (t)0.00.10.20.30.40.5x(t) (b)\n260 280 300 320 340\nTimestamp (t)012345x(t) (c)\nFigure 3. Changes in mean of distributions for data streams where concept drift occurs. ( a) Sudden concept drift.\n(b) Incremental concept drift. ( c) Blip concept drift.\n\nSensors 2021 ,21, 1080 9 of 19\n4.2. Implementation\nThe parameters of FlexSketch are selected such that it has similar accuracy to oKDE and\nSPDT for the stationary data streams as follows: NM\u00103,NQ\u001030,l\u00102.5, and g\u00100.4.\nFlexSketch is implemented in Scala, which is publicly available at GitHub [ 53]. For oKDE , we\nuse the JAVA implementation available in [ 54]. For SPDT , we use the Scala implementation\nin [55]. Note that the accuracy of SPDT decreases when concept drift occurs since SPDT\nstores the entire frequencies of the data stream. To address this issue, we modify SPDT\nby using a sliding window, which is referred to as SPDTw . The window size is set to\n100 (this value was tuned such that SPDTw would exhibit similar accuracy to SPDT for\nstationary data. Increases in window size favor accuracy of stationary (or slowly changing)\ndata streams to sudden concept drift, which reductions cause the inverse. Therefore, we\ncalibrated these comparative methods for equitable results), for which SPDTw shows\nsimilar accuracy to FlexSketch for non-stationary data streams.\nWe perform all experiments on a machine with 4-core Intel CPU i7-7700K @ 4.2 GHz\nand 16 GB memory. The experiments run on a single thread. The version of Scala is 2.12.5\nand the version of Java is 1.8.0.\n4.3. Performance Metrics\nThroughput We evaluate the computation times of the update and query operations\nofFlexSketch in million operations per second (Mops), which indicates the number of times\nper second our benchmark operation can be executed. There is a performance degradation\nin JVM in the ﬁrst few iterations. Thus, we start to record the throughput after 20 iterations\nto warm up. Then, we record the mean value of the throughputs for the subsequent 30\niterations to minimize accidental deviations.\nError We measure the discrepancy between the estimated PDF and the ground truth\ndistribution. We adopt the scaled mean average error (scaled MAE) of CDF, which is\ndeﬁned in (5) with scaling in (6), i.e., d\u0010s#L\np1\u0001s#q.\nAdaptability When concept drift occurs, the PDF estimated by a density estimation\nalgorithm changes over time, so does the error. Thus, the mean of the error for a given\ntime interval is not a sufﬁcient metric for the accuracy of the algorithm for non-stationary\ndata streams. Instead, we measure the adaptability of the algorithm using how the error\nchanges over time. For this, we introduce a damped harmonic oscillator model in classical\nmechanics (e.g., a vibrating mass connected to a spring under damping) to represent\nthe change in the error of the density estimation. In other words, the stability against\noutliers is equivalent to the resistance force (or frictional force) and the UPDATE operation\nis equivalent to the restoring force. The density estimation algorithm tries to make the\nerror smaller as the error increases and to keep the error unchanged as the error suddenly\nincreases. Then, the governing equation for the time-dependent error dptqcan be written as:\n\u0000:dptq\u0000 :d0ptq\b\n\u0000c\u0004\u00009dptq\u0000 9d0ptq\b\nloooooooomoooooooon\nresistance force\u0000 k\u0004dptqloomoon\nrestoring force\u00100 (10)\nwhere d0ptqis the error between before and after the data distribution changes, 9dand :d\nare the ﬁrst- and second-order time derivatives of d, respectively, and kand care model\ncoefﬁcients. kandcare determined by ﬁtting the observed values of dptqandd0ptqto the\nmodel (10) under the assumption of over-damped oscillation (i.e., pc{2q2¡k). The solution\nis given by\ndptq\u0010A1e\u0001pc{2\u0000?\nc2{4\u0001kqtlooooooooooomooooooooooon\nshort-lived term\u0000A2e\u0001pc{2\u0001?\nc2{4\u0001kqtlooooooooooomooooooooooon\nlong-lived term(11)\nwhere A1andA2are constants. Based on the ﬁtted model, the following four performance\nmetrics are derived.\n\nSensors 2021 ,21, 1080 10 of 19\n(a) Half-life: In order to measure the adaptability of an algorithm under sudden concept\ndrift, we measure the time taken until the error at the time of concept drift is reduced\nby a half, which is denoted as half-life:\nt1{2\u0010d\u00011\u0000\n1{2\u0004dpt1q\b\n. (12)\nThis metric basically measures how quickly an old concept is forgotten in the\nshort term.\n(b) Lifetime: Similarly, we also quantify how long the contribution of the past data\nstays, or equivalently, how quickly the old concept is forgotten in the long term.\nThe lifetime is deﬁned as the time required for a long-lived term in (11) to reduce to\n1{etimes its initial value, which is given by\nt\u0010\u0002\nc{2\u0001b\nc2{4\u0001k\n\u00011\n. (13)\n(c) Lag: The lag measures how well the estimated PDF adapts to the data stream under\nincremental concept drift. It is deﬁned as the absolute ratio of dand the derivative\nofd0attÑ8 , which can be obtained by\n\u0007\u0007\u0007\u0007\u0007dp8q\n9d0p8q\u0007\u0007\u0007\u0007\u0007\u0010c\nk. (14)\nIf an algorithm does not adapt well to the concept drift, the accumulated error\nmakes the algorithm lag behind more and more.\n(d) Instability The instability measures how fast the estimated PDF moves for a short\nduration when blip concept drift occurs. It is deﬁned as the velocity of the error,\nwhich can be approximated as\ns\u00109dpt1q\u0014dpt1\u0000teq\u0001dpt1q\nte. (15)\nMemory Usage The PDF estimated using the density estimation algorithm continues\nto use memory. After the estimation, this result or its changing history is recorded in\nthe disk if necessary. Therefore, we record only the memory usage of the estimated PDF,\nbut not the whole memory usage consumed by the U PDATE or Q UERY operation.\n4.4. Throughput\nWe compare the throughput performance of the existing and our methods for the\ntwo key operations, i.e., updating the estimated density and producing the probability\nfor a given data, which correspond to the UPDATE and QUERY operations in our method,\nrespectively.\nUPDATE Figure 4a shows the throughputs of the update operation of different density\nestimation algorithms for different types of data streams. The throughput of FlexSketch is\n1.1 Mops, which is 16 \u0002, 16\u0002, and 1800\u0002higher than that of oKDE ,SPDT , and SPDTw ,\nrespectively , for the stationary data streams. For the non-stationary data streams, the through-\nput of FlexSketch is 0.37 Mops, which is 5.8 \u0002, 5.7\u0002, and 570\u0002higher than that of oKDE ,\nSPDT , and SPDTw , respectively . For the mixture data streams, the throughput of FlexSketch\nis 0.61 Mops, which is 9.7 \u0002, 9.3\u0002, and 1000\u0002higher than that of oKDE ,SPDT , and SPDTw ,\nrespectively . We also perform the one-sample Wilcoxon signed-rank test under the hypothesis\nthat the median of the throughput differences between the proposed method and the existing\nmethods is zero, which confirms the significance of the differences ( p 0.005 ). This result\ndemonstrates that the additional computation time to manage multiple models is signifi-\ncantly smaller than the computation time to represent the data stream elaborately . This effect\nbecomes more prominent when major concept drift does not occur. However, it shows a\nnoticeable improvement even for a data stream with frequent major concept drift.\n\nSensors 2021 ,21, 1080 11 of 19\nStationary Non-stationary Mixture0.00.20.40.60.81.01.2Throughput (Mops)1.1\n0.370.61oKDE\nSPDT\nSPDTw\nFlexSketch\n(a) Update operation\n012345Throughput (Mops)3.5\n0.45 0.44 0.47oKDE\nSPDT\nSPDTw\nFlextSketch\nFlextSketch +cache.41 (b) Query operation\nFigure 4. Throughput performance of the update and query operation (a higher value is better).\nQUERY Figure 4b shows the throughputs of different density estimation algorithms\nfor the query operation. The throughput of FlexSketch is 0.47 Mops, which is similar to that\nofSPDT andSPDTw and smaller than that of oKDE . As shown in (7), FlexSketch linearly\ncombines multiple models for each query. In order to improve the querying speed, we can\nadd a caching algorithm, although it consumes 20–30% more memory. The throughput\nofFlexSketch is signiﬁcantly improved with cache up to 4.1 Mops, which is 1.2 \u0002, 9.2\u0002,\nand 9.4\u0002higher than that of oKDE ,SPDT , and SPDTw , respectively.\n4.5. Accuracy (Error and Adaptability)\nWe compare the accuracy of FlexSketch with that of the alternatives by measuring\nerrors for the stationary and non-stationary data streams.\n4.5.1. Stationary Case\nFigure 5 compares the estimation error of each algorithm after performing the update\noperation for three different stationary data streams. The error of FlexSketch for the\nnormal distribution is 0.012, which is 0.75 \u0002, 1.2\u0002, and 3.2\u0002less than those of oKDE ,SPDT ,\nand SPDTw , respectively. It is intuitive that oKDE records the lowest error because it\nestimates the distribution by using a mixture of Gaussian distributions. SPDTw is less\naccurate than FlexSketch since the number of data used for update by SPDTw is limited to a\nﬁxed size within its window (note that the parameters of SPDTw are deliberately selected\nso as to have similar accuracy as FlexSketch when concept drift occurs, as mentioned in\nSection 4.2).\nNormal Bimodal Log-normal0.000.010.020.030.040.05Error ( )\n0.00910.0180.034\n0.0140.0220.0450.050\n0.0120.0180.025oKDE \nSPDT   SPDT   w \nFlexSketch\n 0.039  \nFigure 5. Estimation error ( d) for stationary data streams (a smaller value is better).\nThe error of FlexSketch for the bimodal distribution is 0.018, which is 0.98 \u0002, 1.2\u0002,\nand 2.5\u0002smaller than those of oKDE ,SPDT , and SPDTw , respectively. oKDE using a\n\nSensors 2021 ,21, 1080 12 of 19\nGaussian kernel shows the best result, as in the case of the normal distribution. And,\nthe performance of FlexSketch is equal to that of oKDE within a margin of error. This indi-\ncates that the BUILD operation successfully constructs a new model that recognizes different\nmodes well. Again, SPDTw is less accurate than FlexSketch for the aforementioned reason.\nThe error of FlexSketch for the log-normal distribution is 0.025, which is 1.4 \u0002, 0.93\u0002,\nand 2.0\u0002smaller than those of oKDE ,SPDT , and SPDTw , respectively. Contrary to the\nresults for the normal and bimodal distributions, FlexSketch andSPDT , which have high\ndegrees of freedom, show smaller errors than oKDE .\n4.5.2. Non-Stationary Case\nFigure 6 shows the error and adaptability performance of different methods under\nsudden concept drift, i.e., the errors over time between the PDFs estimated using different\nalgorithms and the underlying distribution of the data stream in Figure 6a, and the half-life\nand lifetime in Figure 6b. When the concept drift occurs at t\u0010300, the errors jump to\n1.0 or higher for all methods. As soon as the PDFs adapts to the new concept, the PDF\nforgets the old concept and the errors slowly fall to zero. In the short term, the error of\noKDE decreases more quickly compared to SPDTw andFlexSketch , resulting in the shortest\nhalf-life by oKDE . However, oKDE shows the longest lifetime, indicating that it is affected\nby the old concept for a long time. In the long term, FlexSketch shows the smallest error in\nFigure 6a and also the shortest lifetime in Figure 6b.\n300 4000.000.250.500.751.001.25Error ( )\n700 8000.020.040.060.080.10\noKDE\nSPDTw\nFlexSketch\nTimestamp (t )0.00\n(a)\n020406080Half-life (t 1/2)\n33545oKDE \nSPDTw  \nFlexSketch\n050100150200\nLifetime ( )\n190\n4136 (b)\nFigure 6. Error and adaptability performance under sudden concept drift. ( a) Change of the error over time. The period\nbetween about 400 and 650 is omitted for better visualization, and the error after t\u0010650is magniﬁed with the scale at the\nright side. ( b) Half-life (left side) and lifetime (right side). A smaller value is better.\nWe also measure the accuracy of the three methods over data generated by the MOA\nframework [ 56] for sudden concept change. Function 2 and 3 of the SEA generator [ 57]\nis used with a narrow drift-window (100 samples) to produce data streams and the error\nis measured against the mean of each cluster at the 2000 sample point after the drift.\nFlexSketch, SPDTw and oKDE exhibit errors of 0.27, 0.21 and 0.57, respectively. Though the\noverall trend is similar, FlexSketch shows a slightly higher error than SPDTw since the\nwindow size of SPDTw is small enough to evade from the effect of old data.\nIn Figure 7, the performance of different methods under incremental drift is shown.\nThe error of FlexSketch is saturated at 0.11, while those of SPDTw andoKDE are saturated\nat 0.14 and 0.58, respectively, as shown in Figure 7a. In addition, Figure 7b shows that\nFlexSketch has the smallest lag (5.3 \u0002and 1.2\u0002smaller than those of oKDE andSPDTw ,\nrespectively). This demonstrates that FlexSketch can not only speed up computation,\nbut also adapt more accurately to the changes in the data stream. This is also consistent\nwith the observation for sudden concept drift that FlexSketch forgets the past concept faster\nthan oKDE in the long term.\n\nSensors 2021 ,21, 1080 13 of 19\n300 400 500 600 700\nTimestamp (t)0.00.20.40.6Error ( )\noKDE\nSPDTw\nFlexSketch\n(a)\n 0102030Lag (c/k)29.0\n6.85.5oKDE\nSPDTw\nFlexSketch (b)\nFigure 7. Error and adaptability performance under incremental concept drift. ( a) Change of the error over time. ( b) Lag (a\nsmaller value is better).\nFigure 8a shows the errors of the three algorithms for the case of blip concept drift.\nThe errors of FlexSketch andSPDTw increase only by 0.0021 and 0.0050, respectively, due to\nthe blip concept drift, whereas oKDE shows a much larger increase of the error (up to 0.84).\nThe comparison of the instability metric in Figure 8b also conﬁrms that FlexSketch shows\nsigniﬁcantly improved performance, especially compared to oKDE .\n298 300 302 304 306\nTimestamp (t)0.00.20.40.60.8Error ( )\noKDE \nSPDTw  \nFlexSketch\n(a)\n0.00.10.20.3Instability ( )\n0.28\n0.0017  0.00071oKDE\nSPDTw\nFlexSketch (b)\nFigure 8. Error and adaptability performance under blip concept drift. ( a) Change of the error over time. ( b) Instability (a\nsmaller value is better).\n4.6. Memory Usage\nFigure 9 compares the amount of memory used in the data structures of different\ndensity estimation algorithms for the stationary data stream. Our FlexSketch consumes\n6.2 kbytes of memory, which is 1.3 \u0002, 1.4\u0002and 0.68\u0002more than that of oKDE ,SPDT ,\nandSPDTw , respectively. Since we set NM\u00103, one could expect that FlexSketch requires\n3\u0002more memory usage than the others. However, the increased amount of memory con-\nsumption is much less than such an expectation by using efﬁcient histogram computation.\n\nSensors 2021 ,21, 1080 14 of 19\n0246810Memory (KB)4.64.39.1\n6.2oKDE\nSPDT\nSPDTw\nFlexSketch\nFigure 9. Comparison of memory usage.\n4.7. Effects of Parameters\nWe investigate the effects of the algorithm parameters, i.e., NM,NQ,l, and g, on the\nperformance in terms of throughput of the UPDATE operation, error, and memory usage\nfor the stationary data stream and the non-stationary data stream with incremental concept\ndrift. The ranges of the parameters are as follows: 2 to 10 for NM, 10 to 150 for NQ, 0.2 to 3.0\nforl, and 0.01 to 2.3 for g. Experimental results with the combinations of these parameter\nvalues are analyzed below.\n4.7.1. Stationary Case\nFigure 10a is a three-dimensional representation of the throughput, error, and memory\nusage for the stationary data stream as the parameters are changed. Many of the data points\nare located in the upper left side, indicating that FlexSketch achieves high throughputs and\nlow errors over various combinations of the parameter values.\nFigure 11 presents the effect of each parameter separately by increasing one of the four\nparameters while the others remain ﬁxed. The following observations can be made. First,\nasNMincreases, the throughput tends to decrease because of increased computational\ncomplexity for more histograms, while the error does not change (Figure 11a). Second,\nincreasing NQandgresults in decreased errors and increased throughputs in Figure 11b,d ,\nrespectively. The improved throughput is because using a larger Q, or increasing the thresh-\noldgallows the computationally intensive MAJOR UPDATE operation to be performed less\nfrequently. Since the MAJOR UPDATE operation adds a model representing the latest data to\nthe data structure, performing less MAJOR UPDATE operations reduces the dependence on\nthe latest data, which improves the accuracy for the stationary data stream. Third, the value\nofldoes not affect much on the performance (Figure 11c).\n4.7.2. Non-Stationary Case\nFigure 10b shows the throughput, error, and memory usage for the non-stationary\ndata stream with incremental concept drift as the parameters are changed. Depending on\nthe values of the parameters, the performance of FlexSketch may become degraded (i.e.,\nlarger errors, lower throughput, or larger memory consumption).\nFigure 12 shows how each parameter affects to the performance, from which we\ndraw the following observations. First, as shown in Figure 12a, increasing NMresults in\nlowering the throughput without changing the error much, which is due to the increased\nnumber of histograms as in the case of the stationary data stream. Second, when NQorg\nincreases, a trade-off relationship is observed, i.e., the throughput increases but the error\nalso increases (Figure 12b,d). Suppressing the MAJOR UPDATE operation with increased\nNQorgimproves the throughput, but prevents FlexSketch from accurately adapting to the\nconcept drift. Third, by increasing l, the contribution of past data is reduced and thus the\nerror can be reduced.\n\nSensors 2021 ,21, 1080 15 of 19\n0.0 0.2 0.4 0.6\nError ()\n0.00.20.40.60.81.01.21.4Throughput (Mops)Better\n04681020\nMemory (KB)\n(a)\n0.0 0.2 0.4 0.6\nError ()\n0.00.20.40.60.81.01.21.4Throughput (Mops)\n04681020\nMemory (KB) (b)\nFigure 10. Performance of FlexSketch for various combinations of parameters’ values. ( a) Stationary\ndata stream. ( b) Non-stationary data stream.\n0.0 0.2 0.4 0.6\nError ()\n0.00.20.40.60.81.01.21.4Throughput (Mops)\n04681020\nMemory (KB)\n(a)\n0.0 0.2 0.4 0.6\nError ()\n0.00.20.40.60.81.01.21.4Throughput (Mops)\n04681020\nMemory (KB) (b)\n0.0 0.2 0.4 0.6\nError ()\n0.00.20.40.60.81.01.21.4Throughput (Mops)\n04681020\nMemory (KB)\n(c)\n0.0 0.2 0.4 0.6\nError ()\n0.00.20.40.60.81.01.21.4Throughput (Mops)\n04681020\nMemory (KB) (d)\nFigure 11. Trajectories (represented by red arrows) of the performance of FlexSketch for the stationary data stream as the\nvalue of each parameter increases ( a)NM(b)NQ(c)l(d)g.\n\nSensors 2021 ,21, 1080 16 of 19\n0.0 0.2 0.4 0.6\nError ()\n0.00.20.40.60.81.01.21.4Throughput (Mops)\n04681020\nMemory (KB)\n(a)\n0.0 0.2 0.4 0.6\nError ()\n0.00.20.40.60.81.01.21.4Throughput (Mops)\n04681020\nMemory (KB) (b)\n0.0 0.2 0.4 0.6\nError ()\n0.00.20.40.60.81.01.21.4Throughput (Mops)\n04681020\nMemory (KB)\n(c)\n0.0 0.2 0.4 0.6\nError ()\n0.00.20.40.60.81.01.21.4Throughput (Mops)\n04681020\nMemory (KB) (d)\nFigure 12. Trajectories (represented by red arrows) of the performance of FlexSketch for the non-stationary data stream with\nincremental concept drift as the value of each parameter increases. ( a)NM(b)NQ(c)l(d)g.\n5. Conclusions\nIn this paper, we have proposed the FlexSketch framework, which is an online algo-\nrithm based on an ensemble of histograms and consists of three operations: MINOR UPDATE ,\nMAJOR UPDATE , and DIAGNOSE . Since it dynamically determines when to forget old data\nby observing divergence, it estimates probability distributions stably for stationary data\nstreams without invoking the MAJOR UPDATE operation. FlexSketch adapts to concept drift\nswiftly for non-stationary data streams by updating underlying model rapidly using MA-\nJORUPDATE . As shown in Section 4.5.2, FlexSketch estimates probability distribution with\nhigh accuracy for data streams with sudden and incremental concept drift. Because FlexS-\nketch utilizse simple histogram as the elemental data structure, it achieves high through-\nput update and query operations using only limited memory. The experimental results\ndemonstrated the advantages of the method we propose in this paper. FlexSketch exhibits\nsigniﬁcantly improved speed compared to its alternatives. Moreover, FlexSketch adapts\n\nSensors 2021 ,21, 1080 17 of 19\nwell to various non-stationary data streams while maintaining stability over temporal\nﬂuctuations. Nevertheless, FlexSketch has a disadvantage since it has multiple parameters.\nAs discussed in Section 4.7, FlexSketch exposes some changes in throughput and accuracy\naccording to parameters, which could be a burden of design choice in domain speciﬁc\napplications. While FlexSketch exhibits preferable characteristics, it needs to be extended to\novercome the current limitation of supporting one-dimensional data only, which could be\na drawback for some applications. In our future work, we plan to extend our method for\nmulti-dimensional data streams. Because histogram is a simple and efﬁcient underlying\ndata structure for ensemble methods as shown in this paper, we will try to incorporate\nmulti-dimensional histogram [ 58,59] to accommodate multi-dimensional data. In addi-\ntion, we will explore applications that utilize probability estimation as a core building\nblock. Drifting data stream classiﬁcation [ 35] and anomaly detection in non-stationary data\nstream [60] would be good candidates to deploy FlexSketch for practical applications.\nAuthor Contributions: Data curation, N.P .; Funding acquisition, S.K.; Investigation, S.K.; Project\nadministration, S.K.; Software, N.P .; Writing—original draft, N.P .; Writing—review and editing, S.K.\nAll authors have read and agreed to the published version of the manuscript.\nFunding: This research was supported by the Samsung Science and Technology Foundation under\nProject Number SSTF-BA1501-52 and Samsung Research Funding & Incubation Center of Samsung\nElectronics under Project Number SRFC-IT1801-10.\nConﬂicts of Interest: The authors declare no conﬂict of interest.\nReferences\n1. Kraska, T.; Beutel, A.; Chi, E.H.; Dean, J.; Polyzotis, N. The case for learned index structures. In Proceedings of the International\nConference on Management of Data, Houston, TX, USA, 10–15 June 2018; pp. 489–504.\n2. Ustinova, E.; Lempitsky, V . Learning deep embeddings with histogram loss. In Proceedings of the Advances in Neural\nInformation Processing Systems, Barcelona, Spain, 5–10 December 2016; pp. 4170–4178.\n3. Geng, Y.; Liu, S.; Yin, Z.; Naik, A.; Prabhakar, B.; Rosenblum, M.; Vahdat, A. Exploiting a natural network effect for scalable, ﬁne-\ngrained clock synchronization. In Proceedings of the USENIX Symposium on Networked Systems Design and Implementation,\nRenton, WA, USA, 9–11 April 2018; pp. 81–94.\n4. Webb, G.I.; Hyde, R.; Cao, H.; Nguyen, H.L.; Petitjean, F. Characterizing concept drift. Data Min. Knowl. Discov. 2016 ,30, 964–994.\n[CrossRef]\n5. Ahmad, S.; Lavin, A.; Purdy, S.; Agha, Z. Unsupervised real-time anomaly detection for streaming data. Neurocomputing 2017 ,\n262, 134–147. [CrossRef]\n6. Cheng, K.W.; Chen, Y.T.; Fang, W.H. Video anomaly detection and localization using hierarchical feature representation and\nGaussian process regression. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Boston, MA,\nUSA, 7–12 June 2015; pp. 2909–2917.\n7. Yang, D.; Li, B.; Rettig, L.; Cudré-Mauroux, P . HistoSketch: Fast similarity-preserving sketching of streaming histograms with\nconcept drift. In Proceedings of the IEEE International Conference on Data Mining, New Orleans, LA, USA, 18–21 November\n2017; pp. 545–554.\n8. Ben-Haim, Y.; Tom-Tov, E. A streaming parallel decision tree algorithm. J. Mach. Learn. Res. 2010 ,11, 849–872.\n9. Kristan, M.; Leonardis, A.; Skoˇ caj, D. Multivariate online kernel density estimation with Gaussian kernels. Pattern Recognit. 2011 ,\n44, 2630–2642. [CrossRef]\n10. Heinz, C.; Seeger, B. Towards kernel density estimation over streaming data. In Proceedings of the International Conference on\nManagement of Data, Chicago, IL, USA, 27–29 June 2006; pp. 1–12.\n11. Qahtan, A.A.; Wang, S.; Zhang, X. KDE-Track: An efﬁcient dynamic density estimator for data streams. IEEE Trans. Knowl.\nData Eng. 2017 ,29, 642–655. [CrossRef]\n12. Hill, D.J.; Minsker, B.S. Anomaly detection in streaming environmental sensor data: A data-driven modeling approach.\nEnviron. Model. Softw. 2010 ,25, 1014–1022. [CrossRef]\n13. Wu, C.; Jiang, P .; Ding, C.; Feng, F.; Chen, T. Intelligent fault diagnosis of rotating machinery based on one-dimensional\nconvolutional neural network. Comput. Ind. 2019 ,108, 53–61. [CrossRef]\n14. Wang, J.; Yang, X.; Long, K. A new relative entropy based app-DDoS detection method. In Proceedings of the IEEE Symposium\non Computers and Communications, Riccione, Italy, 22–25 June 2010; pp. 966–968.\n15. Wilson, A.G.; Gilboa, E.; Nehorai, A.; Cunningham, J.P . Fast kernel learning for multidimensional pattern extrapolation.\nInProceedings of the 27th International Conference on Neural Information Processing Systems (NIPS’14)—Volume 2 ; MIT Press:\nCambridge, MA, USA, 2014; pp. 3626–3634.\n\nSensors 2021 ,21, 1080 18 of 19\n16. Pham, D.S.; Venkatesh, S.; Lazarescu, M.; Budhaditya, S. Anomaly detection in large-scale data stream networks. Data Min.\nKnowl. Discov. 2014 ,28, 145–189. [CrossRef]\n17. Gama, J.A.; Žliobaitundeﬁned, I.; Bifet, A.; Pechenizkiy, M.; Bouchachia, A. A survey on concept drift adaptation. ACM Comput.\nSurv. 2014 ,46, 1–37. [CrossRef]\n18. Bifet, A.; Holmes, G.; Pfahringer, B.; Kirkby, R.; Gavaldà, R. New ensemble methods for evolving data streams. In Proceedings\nof the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Paris, France, 28 June–1 July 2009;\npp. 139–148.\n19. Bifet, A.; Holmes, G.; Pfahringer, B. Leveraging bagging for evolving data streams. In Proceedings of the Joint European\nConference on Machine Learning and Knowledge Discovery in Databases, Barcelona, Spain, 19–23 September 2010; pp. 135–150.\n20. Gomes, H.M.; Bifet, A.; Read, J.; Barddal, J.P .; Enembreck, F.; Pfharinger, B.; Holmes, G.; Abdessalem, T. Adaptive random forests\nfor evolving data stream classiﬁcation. Mach. Learn. 2017 ,106, 1469–1495. [CrossRef]\n21. Cano, A.; Krawczyk, B. Kappa updated ensemble for drifting data stream mining. Mach. Learn. 2020 ,109, 175–218. [CrossRef]\n22. Klinkenberg, R.; Joachims, T. Detecting concept drift with support vector machines. In Proceedings of the International\nConference on Machine Learning, Stanford, CA, USA, 29 June–2 July 2000; pp. 487–494.\n23. Li, B.; Wang, Y.J.; Yang, D.S.; Li, Y.M.; Ma, X.K. FAAD: An unsupervised fast and accurate anomaly detection method for a\nmulti-dimensional sequence over data stream. Front. Inf. Technol. Electron. Eng. 2019 ,20, 388–404. [CrossRef]\n24. Bashir, S.; Petrovski, A.; Doolan, D. A framework for unsupervised change detection in activity recognition. Int. J. Pervasive\nComput. Commun. 2017 ,13, 157–175. [CrossRef]\n25. Sethi, T.; Kantardzic, M. Handling adversarial concept drift in streaming data. Expert Syst. Appl. 2018 ,97, 18–40. [CrossRef]\n26. Costa, A.F.J.; Albuquerque, R.A.S.; dos Santos, E.M. A drift detection method based on active learning. In Proceedings of the\nInternational Joint Conference on Neural Networks, Rio de Janeiro, Brazil, 8–13 July 2018; pp. 1–8.\n27. Koh, Y.S. CD-TDS: Change detection in transactional data streams for frequent pattern mining. In Proceedings of the International\nJoint Conference on Neural Networks, Vancouver, BC, Canada, 24–29 July 2016; pp. 1554–1561.\n28. De Mello, R.; Vaz, Y.; Grossi, C.; Bifet, A. On learning guarantees to unsupervised concept drift detection on data streams.\nExpert Syst. Appl. 2019 ,117, 90–102. [CrossRef]\n29. Pinagé, F.; dos Santos, E.M.; Gama, J. A drift detection method based on dynamic classiﬁer selection. Data Min. Knowl. Discov.\n2019 ,34, 50–74. [CrossRef]\n30. Bouchachia, A. Fuzzy classiﬁcation in dynamic environments. Soft Comput. 2011 ,15, 1009–1022. [CrossRef]\n31. Gomes, J.A.B.; Menasalvas, E.; Sousa, P .A.C. Learning recurring concepts from data streams with a context-aware ensem-\nble. In Proceedings of the 2011 ACM Symposium on Applied Computing (SAC’11), Taichung, Taiwan , 21–24 March 2011;\nAssociation for Computing Machinery: New York, NY, USA, 2011; pp. 994–999.\n32. Adä, I.; Berthold, M.R. EVE: A framework for event detection. Evol. Syst. 2013 ,4, 61–70. [CrossRef]\n33. Vorburger, P .; Bernstein, A. Entropy-based concept shift detection. In Proceedings of the International Conference on Data\nMining (ICDM’06), Hong Kong, China, 18–22 December 2006; pp. 1113–1118.\n34. Gözüaçık, O.; Büyükçakır, A.; Bonab, H.; Can, F. Unsupervised concept drift detection with a discriminative classiﬁer. In Pro-\nceedings of the 28th ACM International Conference on Information and Knowledge Management (CIKM’19), Beijing, China,\n3–7 November 2019; Association for Computing Machinery: New York, NY, USA, 2019; pp. 2365–2368.\n35. Wang, X.; Kang, Q.; An, J.; Zhou, M. Drifted Twitter spam classiﬁcation using multiscale detection test on K-L divergence.\nIEEE Access 2019 ,7, 108384–108394. [CrossRef]\n36. Prabhu, S.S.; Runger, G.C. Designing a multivariate EWMA control chart. J. Qual. Technol. 1997 ,29, 8–15. [CrossRef]\n37. Koren, Y. Collaborative ﬁltering with temporal dynamics. In Proceedings of the 15th ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, Paris, France, 28 June–1 July 2009; pp. 447–456.\n38. Pechenizkiy, M.; Bakker, J.; Žliobait ˙e, I.; Ivannikov, A.; Kärkkäinen, T. Online mass ﬂow prediction in CFB boilers with explicit\ndetection of sudden concept drift. ACM SIGKDD Explor. Newsl. 2010 ,11, 109–116. [CrossRef]\n39. Forman, G. Tackling concept drift by temporal inductive transfer. In Proceedings of the 29th ACM Conference on Research and\nDevelopment in Information Retrieval, Seattle, WA, USA, 6–11 August 2006, pp. 252–259.\n40. Gilbert, A.C.; Guha, S.; Indyk, P .; Kotidis, Y.; Muthukrishnan, S.; Strauss, M.J. Fast, small-space algorithms for approximate\nhistogram maintenance. In Proceedings of the Annual ACM Symposium on Theory of Computing, Montreal, QC, Canada,\n19–21 May 2002; pp. 389–398.\n41. Guha, S.; Koudas, N.; Shim, K. Approximation and streaming algorithms for histogram construction problems. ACM Trans.\nDatabase Syst. 2006 ,31, 396–438. [CrossRef]\n42. Greenwald, M.; Khanna, S.; Greenwald, M.; Khanna, S. Space-efﬁcient online computation of quantile summaries. ACM\nSIGMOD Rec. 2001 ,30, 58–66. [CrossRef]\n43. Shrivastava, N.; Buragohain, C.; Agrawal, D.; Suri, S. Medians and beyond: New aggregation techniques for sensor networks.\nIn Proceedings of the International Conference on Embedded Network Sensor Systems, Baltimore, MD, USA, 3–5 November\n2004; pp. 239–249.\n44. Cormode, G.; Korn, F.; Muthukrishnan, S.; Srivastava, D. Effective computation of biased quantiles over data streams. In Pro-\nceedings of the International Conference on Data Engineering, Tokoyo, Japan, 5–8 April 2005; pp. 1–12.\n\nSensors 2021 ,21, 1080 19 of 19\n45. Singh, S.A.; Srivastava, D.; Tirthapura, S. Estimating quantiles from the union of historical and streaming data. In Proceedings of\nthe VLDB Endowment, New Delhi, India, 9 May 2016; Volume 10, pp. 433–444.\n46. Datar, M.; Gionis, A.; Indyk, P .; Motwani, R. Maintaining stream statistics over sliding windows. SIAM J. Comput. 2002 ,\n31, 1794–1813. [CrossRef]\n47. Kuncheva, L.I.; Žliobait ˙e, I. On the window size for classiﬁcation in changing environments. Intell. Data Anal. 2009 ,13, 861–872.\n[CrossRef]\n48. Deypir, M.; Sadreddini, M.H.; Hashemi, S. Towards a variable size sliding window model for frequent itemset mining over data\nstreams. Comput. Ind. Eng. 2012 ,63, 161–172. [CrossRef]\n49. Kolter, J.Z.; Maloof, M.A. Dynamic weighted majority: An ensemble method for drifting concepts. J. Mach. Learn. Res. 2007 ,\n8, 2755–2790.\n50. Elwell, R.; Polikar, R. Incremental learning of concept drift in nonstationary environments. IEEE Trans. Neural Netw. 2011 ,\n22, 1517–1531. [CrossRef] [PubMed]\n51. Gomes, H.M.; Barddal, J.P .; Enembreck, F.; Bifet, A. A survey on ensemble learning for data stream classiﬁcation. ACM Comput.\nSurv. 2017 ,50, 1–36. [CrossRef]\n52. Oza, N.C. Online bagging and boosting. In Proceedings of the IEEE International Conference on Systems, Man and Cybernetics,\nWaikoloa, HI, USA, 12 October 2005; Volume 3, pp. 2340–2345.\n53. Source Codes of FlexSketch , October 2020. Available online: https://xxxnell.github.io/ﬂex/docs/core/sketch.html (accessed on\n4 February 2021).\n54. Source Codes of Online Kernel Density Estimation. June 2017. Available online: https://github.com/joluet/okde-java (accessed\non 4 February 2021).\n55. Source Codes of Streaming Parallel Decision Tree. June 2017. Available online: https://github.com/soundcloud/spdt (accessed\non 4 February 2021).\n56. Bifet, A.; Holmes, G.; Kirkby, R.; Pfahringer, B. MOA: Massive online analysis. J. Mach. Learn. Res. 2010 ,11, 1601–1604.\n57. Street, N.; Kim, Y. A streaming ensemble algorithm (SEA) for large-scale classiﬁcation. In Proceedings of the ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, 26–29 August 2001; pp. 377–382.\n58. Thaper, N.; Guha, S.; Indyk, P .; Koudas, N. Dynamic multidimensional histograms. In Proceedings of the 2002 ACM SIGMOD\nInternational Conference on Management of Data (SIGMOD ’02), Madison, WI, USA, 4–6 June 2002; Association for Computing\nMachinery: New York, NY, USA, 2002; pp. 428–439.\n59. Diakonikolas, I.; Kane, D.M.; Peebles, J. Testing identity of multidimensional histograms. In Proceedings of the Conference on\nLearning Theory (PMLR), Phoenix, AZ, USA, 25–28 June 2019; pp. 1107–1131.\n60. Jordaney, R.; Sharad, K.; Dash, S.K.; Wang, Z.; Papini, D.; Nouretdinov, I.; Cavallaro, L. Transcend: Detecting concept drift in\nmalware classiﬁcation models. In Proceedings of the 26th USENIX Security Symposium (USENIX Security 17), Vancouver, BC,\nCanada, 16–18 August 2017; USENIX Association: Vancouver, BC, Canada, 2017; pp. 625–642.",
  "textLength": 59116
}