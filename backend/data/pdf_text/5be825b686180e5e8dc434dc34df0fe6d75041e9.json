{
  "paperId": "5be825b686180e5e8dc434dc34df0fe6d75041e9",
  "title": "Learning-Augmented Search Data Structures",
  "pdfPath": "5be825b686180e5e8dc434dc34df0fe6d75041e9.pdf",
  "text": "Learning-Augmented Search Data Structures\nChunkai Fu∗Brandon G. Nguyen∗Jung Hoon Seo∗\nRyan Zesch∗Samson Zhou†\nMarch 10, 2025\nAbstract\nWe study the integration of machine learning advice to improve upon traditional data\nstructure designed for efficient search queries. Although there has been recent effort in improving\nthe performance of binary search trees using machine learning advice, e.g., Lin et. al. (ICML\n2022), the resulting constructions nevertheless suffer from inherent weaknesses of binary search\ntrees, such as complexity of maintaining balance across multiple updates and the inability to\nhandle partially-ordered or high-dimensional datasets. For these reasons, we focus on skip lists\nand KD trees in this work. Given access to a possibly erroneous oracle that outputs estimated\nfractional frequencies for search queries on a set of items, we construct skip lists and KD trees\nthat provably provides the optimal expected search time, within nearly a factor of two. In fact,\nour learning-augmented skip lists and KD trees are still optimal up to a constant factor, even if\nthe oracle is only accurate within a constant factor. We also demonstrate robustness by showing\nthat our data structures achieves an expected search time that is within a constant factor of\nan oblivious skip list/KD tree construction even when the predictions are arbitrarily incorrect.\nFinally, we empirically show that our learning-augmented search data structures outperforms\ntheir corresponding traditional analogs on both synthetic and real-world datasets.\n1 Introduction\nAs efficient data management has become increasingly crucial, the integration of machine learn-\ning (ML) has significantly improved the design and performance of traditional algorithms for\nmany big data applications. [ KBC+18] first showed that ML could be incorporated to create\ndata structures that support faster look-up operations while also saving an order-of-magnitude of\nmemory compared to optimized data structures oblivious to such ML heuristics. Subsequently,\nlearning-augmented algorithms [MV20 ] have been shown to achieve provable worst-case guarantees\nbeyond the limitations of oblivious algorithms for a wide range of settings. For example, ML\npredictions have been utilized to achieve more efficient data structures [ Mit18 ,LLW22 ], algorithms\nwith faster runtimes [ DIL+21,CSVZ22 ,DMVW23 ], mechanisms with better accuracy-privacy\ntradeoffs [ KADV23 ], online algorithms with better performance than information-theoretic lim-\nits [PSK18 ,GP19 ,LLMV20 ,WLW20 ,WZ20 ,BMS20 ,AGP20 ,ACL+21,AGKP21 ,IKQP21 ,LV21 ,\nACI22 ,AGKP22 ,APT22 ,GLS+22,KBTV22 ,JLL+22,SGM22 ,AGKK23 ,ACE+23,SLLA23 ],\n∗Equal contribution, Texas A&M University, chunkai369@gmail.com ,bgn@tamu.edu ,j.seo0917@gmail.com ,\nrzesch@tamu.edu\n†Texas A&M University, samsonzhou@gmail.com\n1arXiv:2402.10457v2  [cs.DS]  7 Mar 2025\n\nstreaming algorithms with better accuracy-space tradeoffs [ HIKV19 ,IVY19 ,JLL+20,CIW22 ,\nCEI+22, LLL+23], and polynomial-time algorithms beyond hardness-of-approximation limits, e.g.,\nNP-hardness [EFS+22, NCN23, CLR+24].\nIn this paper, we focus on the consolidation of ML advice to improve data structures for the\nfundamental problem of searching for elements among a large dataset. For this purpose, tree-based\nstructures stand out as a popular choice among other structures, particularly for their logarithmic\naverage performance. However, these structures often have weaknesses for specific use cases that\nmake them sub-optimal for various applications, which we now discuss.\nSkip lists. One weakness of tree-based structures is that they need to be balanced for optimal\nperformance, and thus their effectiveness is often closely tied to the order of element insertions.\nFor example, the motivation of [ LLW22 ] to study learning-augmented binary search trees noted\nthat although previous results already characterized the statically optimal tree if the underlying\ndistribution is known [ Knu71 ,Meh77 ], these methods do not handle dynamic insertion operations.\nIn contrast, skip lists, introduced by [ Pug90a ], maintain balance probabilistically, offering a simpler\nimplementation while delivering substantial speed enhancements [ Pug90b ]. Skip lists are generally\nbuilt iteratively in levels. The bottom levels of the skip list is an ordinary-linked list, in which the\nitems of the dataset are organized in order. Each higher level serves to accelerate the search for the\nlower levels, by storing only a subset of the items in the lower levels, also as an ordered link list.\nTraditional skip lists are built by promoting each item in a level to a higher level randomly with a\nfixed probability p∈(0,1).\nQuerying for a target element begins at the first element in the highest level and continues by\nsearching along the linked list in the highest level until finding an item whose value is at least that\nof the target element. If the found item is greater than the target element, the process is repeated\nafter returning to the previous element and dropping to a lower list. It can be shown that the\nexpected number of steps in the search is O\u0010\n1\nplog1/pn\u0011\nso that pserves as a trade-off parameter\nbetween the search time and the storage costs.\nIn many modern applications, skip lists are used because of their excellent search runtime and\ntheir space efficiency. Skip lists are often preferred over binary search trees due to their simplicity of\nimplementation, their support for efficient range query, and their amenability to concurrent processes\n[SL00 ,LJ13 ], high efficiency for dynamic datasets [ GZ08 ,PT10 ], network routing [ HPJ03 ,ASS20 ],\nand real-time analytics [ BBB+20,ZCL+23]. Thus while binary search trees have been a long-\nstanding choice for querying ordered elements, skip lists offer a simpler, more efficient, and in some\ncases, necessary alternative.\nKD trees. Another weakness of tree-based data structures is that they generally require\nthe data to obey an absolute ordering. However, in many cases, e.g., geometric applications or\nmultidimensional data, the input points can only be partially ordered. Thus in 1975, KD trees,\nwhich stand for k-dimensional trees, were proposed as a more efficient alternative to binary search\ntrees for searching in higher-dimensional spaces in procedures such as nearest neighbor search or\nray tracing for applications in computational geometry or computer vision. A KD tree works by\npicking a data point and splitting along some spatial dimension to partition the space. This process\nis repeated until every data point is included in the tree, creating a hierarchical tree structure that\nenables quick access to specific data points or ranges within the dataset.\nSkewed distributions. Traditional search data structures treat each element equally when\npromoting the elements to higher levels. This balancing behavior facilitates good performance in\nexpectation when a query to the skip list is equally likely to be any dataset element. On the other\n2\n\nhand, this behavior may limit the performance of the data structure when the incoming queries are\nfrom an unbalanced probability distribution.\nReal-world applications can feature a diverse range of distribution patterns. One particularly\ncommon distribution is the Zipfian distribution, which is a probability distribution that is a discrete\ncounterpart of the continuous Pareto distribution, and is characterized by the principle that a small\nnumber of events occur very frequently, while a large number of events occur rarely.\nIn a Zipfian distribution, the frequency of an event N(k;α, N) is inversely proportional to its\nrank k, raised to the power of α(where αis a positive parameter), in a dataset of Nelements.\nIn particular, we have N(k;α, N) =1/kα\nPN\nn=1(1/nα). The value of αdetermines the steepness of the\ndistribution so that a smaller αvalue, i.e., closer to 0, makes it more uniform, while a larger α\nincreases skewness.\nZipfian distributions provide a simple means for understanding phenomena in various fields involv-\ning rank and frequency, ranging from linguistics to economics, and from urban studies to information\ntechnology. Indeed, they appear in many applications such as word frequencies in natural lan-\nguage [ WW16 ,BHZ18 ], city populations [ Gab99 ,VA15 ], biological cellular distributions [ LVM+23],\nincome distribution [San15], etc.\nUnfortunately, although Zipfian distributions are common in practice, their properties are\ngenerally not leveraged by traditional search data structures, which are oblivious to any information\nabout the query distributions. To improve this performance bottleneck, we propose the augmentation\nof traditional skip lists and KD trees with “learned” advice, which (possibly erroneously) informs\nthe data structure in advance about some useful statistics on the incoming queries. Although we\nmodel the data structure as having oracle access to the advice, in practice, such advice can often\neasily be acquired from machine learning heuristics trained for these statistics.\n1.1 Our Contributions\nWe propose the incorporation of ML advice into the design of skip lists and KD trees to improve\nupon traditional data structure design. For ease of discussion in this section, we assume the\nitems that may appear either in the data set or the query set can be associated with an integer\nin [N] :={1, . . . , N }, which also in the case of high-dimensional data, may be associated with a\nk-dimensional point. We allow the algorithm access to a possibly erroneous oracle that, for each\ni∈[N], outputs a quantity pi, which should be interpreted as an estimation for the proportion of\nsearch queries that will be made to the data structure for the item i. Hence, for each i∈[N], we\nassume that pi∈[0,1] and p1+. . .+pn= 1. Note that these constraints can be easily enforced\nupon the oracle as a pre-processing step prior to designing the skip list or KD tree data structure.\nWe also assume that the oracle is readily accessible so that there is no cost for each interaction with\nthe oracle. Consequently, we assume the algorithm has access to the predicted frequency piby the\noracle for all i∈[N]. On the other hand, we view a sequence of queries as defining a probability\ndistribution over the set of queries, so that fiis the true proportion of queries to item i, for each\ni∈[N]. Although fiis the ground truth, our algorithms only have access to pi, which may or may\nnot accurately capture fi.\nConsistency for accurate oracles. We introduce construction for a learning-augmented skip\nlist and KD trees, which gives expected search time at most 2 C+ 2Pn\ni=1fi·min\u0010\nlog1\npi,logn\u0011\n,\nfor some constant C >0. On the other hand, we show that any skip list or KD tree construction\nrequires an expected search time of at least the entropy H(f) of the probability vector f. We recall\n3\n\nthat the entropy H(f) is defined as H(f) =Pn\ni=1fi·log1\nfi.\nThus, our results indicate that within nearly a factor of two, our learning-augmented search data\nstructures are optimal for any distribution of queries, provided that the oracle is perfectly accurate.\nMoreover, even if the oracle on each estimated probability piis only accurate up to a constant\nfactor, then our learning-augmented search data structures are still optimal, up to a constant factor.\nImplications to Zipfian distributions. We describe the implications of our results to queries\nthat follow a Zipfian distribution; analogous results hold for other skewed distributions, e.g., the\ngeometric distribution. It is known that if the r-th most common query/item has proportionz\nrsfor\nsome s >1, then the entropy of the corresponding probability vector is a constant. Consequently, if\nthe set of queries follows a Zipfian distribution and the oracle is approximately accurate within a\nconstant factor, then the expected search time for an item by our search data structures is only a\nconstant, independent of the total number of items, i.e., O(1). By comparison, a traditional skip\nlist or KD tree will have expected search time O(logn).\nRobustness to erroneous oracles. So far, our discussions have centered around an oracle\nthat either produces estimated probabilities pisuch that pi=fiorpiis within a constant factor of\nfi. However, in some cases, the machine learning algorithm serving as the oracle can be completely\nwrong. In particular, a model that is trained on a dataset before a distribution change, e.g., seasonal\ntrends or other temporal shifts, can produce wildly inaccurate predictions. We show that our search\ndata structures are robust to erroneous oracles. Specifically, we show that our algorithms achieve an\nexpected search time that is within a constant factor of an oblivious skip list or KD tree construction\nwhen the predictions are incorrect. Therefore, our data structure achieves both consistency, i.e.,\ngood algorithmic performance when the oracle is accurate, and robustness, i.e., standard algorithmic\nperformance when the oracle is inaccurate.\nEmpirical evaluations. Finally, we analyze our learning-augmented search data structures\nlist on both synthetic and real-world datasets. Firstly, we compare the performance of traditional\nskip lists with our learning-augmented skip lists on synthetically generated data following Zipfian\ndistributions with various tail parameters. The dataset is created using four distinct αvalues\nranging from 1.01 to 2, along with a uniform dataset. During the assessment, we query a specified\nnumber of nitems selectively chosen based on their frequency weights. Our results match our theory,\nshowing that learning-augmented skip lists have faster query times, with an average speed-up factor\nranging from 1.33 up to 7.76, depending on the different skewness parameters.\nWe then consider various datasets for internet traffic data, collected by AOL and by CAIDA,\nobserved over various durations. For each dataset, we split the overall observation time into an\nearly period, which serves as the training set for the oracle, and a later period, which serves as the\nquery set for the skip list. The oracle trained using the IP addresses in the early periods outputs\nthe probability of the appearance of a given node, and then the position of each node is determined.\nOur learning-augmented skip list outperforms traditional skip lists with an average speed-up\nfactor of 1.45 for the AOL dataset and 1.63 for the CAIDA dataset. Moreover, the insertion time of\nour learning-augmented skip list is comparable with that of traditional skip lists on both synthetic\nand real-world datasets. We also observe that our history-based oracle demonstrates good robustness\nagainst temporal change, with little shift in the dominant element set. The adopted datasets show\nthat the set of the top frequent elements does not change much across the time intervals in the\ndatasets used herein.\nWe similarly perform evaluations on our learning-augmented KD tree data structure. We evaluate\nour KD tree data structure on Zipfian distributions with various tail parameters, and provide a\n4\n\nheatmap of average lookup times for elements. We find that for a large variety of Zipfian parameters,\nourt method is able to provides large improvements over traditional KD trees. We perform a similar\nexperiment under Zipfian distributions with added noise, and find our data structure still provides\nconsiderable improvements in query time.\nWe additionally evaluate our method on point cloud samples taken from a 3D model. We bin\nthese samples in space, and create our learning-augmented KD tree on these binned samples. When\nquerying this tree with new binned point samples, we find a decrease in average query time as\ncompared to a traditional KD tree under the same conditions. In addition to this experiment,\nwe provide results on real world datasets of n-grams and neuron activation, and similarly find\nimprovements over traditional KD trees.\nConcurrent and independent work. We mention that concurrent and independent of\nour work, [ ZKH24 ] used similar techniques to achieve the same guarantees on the performance of\nlearning-augmented skip lists that are robust to erroneous predictions. However, they do not show\noptimality for their learning-augmented skip lists and arguably perform less exhaustive empirical\nevaluations. They also do not consider KD trees at all, which forms a significant portion of our\ncontribution, both theoretically and empirically.\nComparison to [ LLW22 ].Our work was largely inspired by [ LLW22 ], who observed that\nclassical literature characterizing statically optimal binary search trees [ Knu71 ,Meh77 ] no longer\napply in the dynamic setting, as elements arrive iteratively over time. Thus, they designed\nthe construction of dynamic learning-augmented binary search trees (BSTs). Their analysis for\nthe expected search time utilized the notion of pivots within their trees and thus were somewhat\nspecialized to BSTs. Therefore, [ LLW22 ] explicitly listed skip trees and advanced tree data structures\nas interesting open directions. Qualitatively, our results are similar to [ LLW22 ], as are those of\n[ZKH24 ]. This is not quite altogether surprising because the main difference between these data\nstructures is not necessarily the search time, but the either the ease of construction in the setting of\nskip lists, or the ability to handle multi-dimensional data in the setting of KD trees.\n2 Learning-Augmented Skip Lists\nIn this section, we describe our construction for a learning-augmented skip list and show various\nconsistency properties of the data structure. In particular, we show that up to a factor of two, our\nalgorithm is optimal, given a perfect oracle. More realistically, if the oracle provides a constant-factor\napproximation to the probabilities of each element, our algorithm is still optimal up to a constant\nfactor.\nWe first describe our learning-augmented skip list, which utilizes predictions pifor each item\ni∈[n], from an oracle. Similar to a traditional skip list, the bottom level of our skip list is an\nordinary-linked list that contains the sorted items of the dataset. As before, the purpose of each\nhigher level is to accelerate the search for an item, but the process for promoting an item from a\nlower level to a higher level now utilizes the predictions. Whereas traditional skip lists promote\neach item in a level to a higher level randomly with a fixed probability p∈(0,1), we automatically\npromote the item ito a level ℓif its predicted frequency pisatisfies pi≥2ℓ−1\nn. Otherwise, we\npromote the item with probability1\n2. This adaptation ensures that items with high predicted query\nfrequencies will be promoted to higher levels of the skip list and thus be more likely to be found\nquickly.\nIt is worth addressing a number of other natural approaches and their shortcomings. For example,\n5\n\none natural approach would be to use the “median” frequency across the items as a threshold to\npromote elements to higher levels. However, this promotion scheme is not ideal because computing\nthe median frequency at each time would either require an additional data structure for fast update\ntime or increase the insertion time. A potential approach to resolve this issue would be to use a\nseparate threshold probability is set for each level so that only nodes with a probability higher than\nthe corresponding threshold are promoted to the next level. However, this approach seems to result\nin an unnecessarily large number of created levels if some item appears with small probability, e.g.,\n1\n2n. We can thus first filters out the low-frequency elements and place them remain on the bottom\nlevel of the skip list and then proceed with using a separate threshold probability for each level.\nUnfortunately, this approach utterly fails to even match the search time performance of oblivious\nskip lists when the distribution is uniform, because all items will be in the same level, resulting in\nan expected search time of Ω( n). Hence, we ensure that each element still has a chance of being\npromoted to higher levels even when their probability is less than the corresponding threshold.\nWe again emphasize that due to the dynamic nature of the updates, existing results on statically\noptimal binary search trees [ Knu71 ,Meh77 ] do not apply, as observed by [ LLW22 ]. We give the full\ndetails in Algorithm 1. For the sake of presentation, we focus on the setting where the queries are\nmade to items in the dataset. However, we remark that our results generalize to the setting where\nqueries can be made on the search space rather than the items in the dataset, provided the oracle is\nalso appropriately adjusted to estimate the query distribution, using the approach we describe in\nSection 3.\nAlgorithm 1 Learning-augmented skip list\nRequire: Predicted frequencies p1, . . . , p nfor each item in [ n]\nEnsure: Learning-augmented skip list\n1:Insert all items at level 0\n2:foreach ℓdo\n3: ifthere are no items at level ℓ−1then\n4: return the skip list\n5: else\n6: foreach i∈[n]do\n7: ifpredicted frequency pi≥2ℓ−1\nnthen\n8: Insert iinto level ℓ\n9: else if iis in level ℓ−1then\n10: Insert iinto level ℓwith probability1\n2\nWe first show an upper bound on the expected search time of our learning-augmented skip-list.\nTheorem 2.1. For each i∈[n], letfiandpibe the proportion of true and predicted queries to item\ni. Then with probability at least 0.99over the randomness of the construction of the skip list, the\nexpected search time over the choice of queries at most 20 + 2Pn\ni=1fi·min\u0010\nlog1\npi,logn\u0011\n.\nTo achieve Theorem 2.1, we first show that each item i∈[n] must be contained at some level\nmax (0 ,1 +⌊log(npi)⌋), depending on the predicted frequency piof the item. We also show that\nwith high probability, the total number of levels in the skip list is at most O(logn). This allows us\nto upper bound the expected search time for item iby at most 2 C+ 2min\u0010\nlog1\npi,logn\u0011\n. We can\n6\n\nthen analyze the expected search time across the true probability distribution fi. Putting these\nsteps together, we obtain Theorem 2.1.\nWe also prove a lower bound on the expected search time of an item drawn from a probability\ndistribution fforanyskip list.\nTheorem 2.2. Given a random variable X∈[n]so that X=iwith probability fi, letT(X)denote\nthe search time for Xin a skip list. Then E[T(x)]≥H(f), where H(f)is the entropy of f.\nTheorem 2.2 uses standard entropy arguments that have been previously used to lower bound\nthe optimal constructions of data structures such as Huffman codes. We next upper bound the\nentropy of a probability vector that satisfies a Zipfian distribution with parameter s.\nLemma 2.3. Lets, z > 0be fixed constants and let fbe a frequency vector such that fi=z\nisfor all\ni∈[n]. Ifs >1, then H(f) =O(1)and otherwise if s≤1, then H(f)≤logn.\nBy Theorem 2.1 and Lemma 2.3, we thus have the following corollary for the expected search\ntime of our learning-augmented skip list on a set of search queries that follows a Zipfian distribution.\nCorollary 2.4. With high probability, the expected search time on a set of queries that follows a\nZipfian distribution with exponent sis at most O(1)fors >1andO(logn)fors≤1.\nNext, we show that our learning-augmented skip list construction is robust to somewhat\ninaccurate oracles. Let fbe the true-scaled frequency vector so that for each i∈[n],fiis the\nprobability that a random query corresponds to i. Let pbe the predicted frequency vector, so\nthat for each i∈[n],piis the predicted probability that a random query corresponds to i. For\nα, β∈(0,1), we call an oracle ( α, β)-noisy if for all i∈[n], we have pi≥α·fi−β. Then we have\nthe following guarantees for an ( α, β)-noisy oracle:\nLemma 2.5. Letαbe a constant and β <α\n4n. A learning-augmented skip list with a set of\n(α, β)-noisy predictions has performance that matches that of a learning-augmented learned with a\nperfect oracle, up to an additive constant.\nTo achieve Lemma 2.5, we parameterize our analysis in Theorem 2.1. Due to the guarantees of\nthe (α, β)-noisy oracles, we can write pi≥α\n2·fi, which allows us to express the search time log1\npiin\nterms of the true entropy of the distribution and a small additive constant that stems from log1\nα. In\nfact, we remark that even when the predictions are arbitrarily inaccurate, our learning-augmented\nskip list still has expected query time O(logn), since the total number of levels is at most O(logn)\nwith high probability. Since the expected query list of an oblivious skip list is also O(logn), then\nthe expected query time of our learning-augmented skip list is within a constant multiplicative\nfactor, even with arbitrarily poor predictions.\n3 Learning-Augmented KD Trees\nIn this section we present details on our novel approach to KD tree construction. First, we present\nthe algorithm that constructs a learning-augmented KD tree. We focus on the setting where queries\ncan be made on the search space rather than the items in the dataset, which is much more interesting\nfor high-dimensional datasets, since even building a balanced tree on the search space could result\n7\n\nin prohibitively high query time, as the height of the tree would already be at least the dimension d.\nNevertheless, assuming that we have a query probability prediction pifor element iof our dataset,\nthe intuition of our method is straightforward. Whereas a learning-augmented binary search tree\nwould attempt to find a value such that the probability of a query being on either branch of the\ntree is balanced, high-dimensional datasets do not have an absolute ordering.\nThus, instead of relying on standard techniques to determine the splitting point of our dataset,\nwe find a specific dimension in which there exists a balanced split such that the probability of a\nquery being on either branch of the tree is balanced. However, there can still be high frequency\nqueries that are not in the dataset, which can cause significantly high query time if not optimized.\nHence, we also add to the tree construction high frequency queries that are not data points, in order\nto reject these negative queries more quickly.\nAlgorithm 2 Learning-augmented KD tree construction\n1:function BuildNode (x)\n2: T← ∅\n3: if|x|= 1then\n4: T=x\n5: return T\n6: best← ∅\n7: foreach dimension iofxdo\n8: foreach element xdo\n9: Compute the probability of points to the left of xon axis i\n10: If the probability is closer to 0 .5 than best, update best\n11: T.axis =best.axis\n12: T.left =BuildNode (x:x[axis]≤best.value )\n13: T.right =BuildNode (x:x[axis]>best.value )\n14: return T\nAlgorithm 3 Learning-augmented KD tree\n1:function Build (dataset ,queries )\n2: dataset f← {x∈dataset |x.prob >1\nn2}\n3: queriesf← {x∈queries |x.prob >1\nn2}\n4: T←BuildNode (dataset f∪queriesf)\n5: Insert {x∈dataset |x.prob≤1\nn2}intoTusing standard balanced KD tree construction\n6: return T\nWe prove the following guarantees on the performance of our learning-augmented KD tree, first\nassuming that our oracle is perfect.\nTheorem 3.1. Suppose [∆]dis the space of possible input points and queries. Let N= ∆dand\npibe the probability that a random query is made to i∈[N], given the natural mapping between\n[N]and[∆]d. Let p= (p1, . . . , p N)∈RNbe the probability vector and H(p)be its entropy. Then\ngiven a set of ninput points, the expected query time for the tree Tcreated by Algorithm 3 is\nO(min( H(p),logn)).\n8\n\nThe analysis of Theorem 3.1 corresponding to our learning-augmented KD tree follows from a\nsimilar structure as the proof of Theorem 2.1. However, the crucial difference is that the universe\nsize is now N, which is exponential in d. Thus constructions that consider distributions over all of\n[N] may suffer O(logN)=O(dlog ∆) query time, which can be prohibitively expensive for large\nd, e.g., high-dimensional data. Hence, our algorithm requires a bit more care in the truncation of\nqueries with low probability and instead, we build a balanced KD tree for any item with less than\n1\nn2probability of being queried, so that each of their query times is at most O(logn). We further\nremark this implies robustness of our data structure to arbitrarily poor predictions, by a similar\nargument as in Section 2.\nWe next prove a lower bound on the expected search time of an item drawn from a probability\ndistribution fforanyKD tree.\nTheorem 3.2. Given a random variable X∈[n]so that X=iwith probability fi, letD(X)denote\nthe depth for Xin a learning-augmented KD tree. Then E[D(X)]≥H(f), where H(f)is the\nentropy of f.\nBy Theorem 3.1 and Lemma 2.3, we thus have the following corollary for the expected query\ntime on our learning-augmented KD tree on a set of search queries that follows a Zipfian distribution.\nCorollary 3.3. With high probability, the expected query time on a set of queries that follows a\nZipfian distribution with exponent sis at most O(1)fors >1andO(logn)fors≤1.\nFinally, we show near-optimality when given imperfect predictions from a ( α, β)-noisy oracle:\nLemma 3.4. Letαbe a constant and let β≤α\nn2. Then the query time for our learning-augmented\nKD tree with (α, β)-noisy prediction matches the performance of a learning-augmented KD tree\nconstructed using a perfect oracle up to an additive constant.\n4 Empirical Evaluations\nIn this section, we describe a number of empirical evaluations demonstrating the efficiency of our\nlearning-augmented search data structures on both synthetic and real-world datasets. We provide\nadditional experiments in Appendix D.\nSkip lists on CAIDA dataset. In the CAIDA datasets [ CAI16 ], the receiver IP addresses\nfrom one minute of the internet flow data are extracted for testing, which contains over 650k unique\nIP addresses of the 30 million queries. Given that the log-log plot of the frequency of all nodes in\nthe CAIDA datasets follows approximately a straight line in Figure 1, the CAIDA datasets can be\napproximately characterized by an αfactor of 1.37. The insertion time is similar between classic and\naugmented skip lists, while Figure 2 shows that query time is almost halved when using the learning\naugmented skip lists at different query sizes. These results assume that the predicted frequency\nof all items in the query stream is accurate, i.e., the probability vector that is used to build the\nskip list matches exactly the query stream. The speed-up between the query times of the largest\nlearning-augmented and the oblivious skip lists in Figure 2 is roughly 1 .86×, which is surprisingly\nand perhaps coincidentally close to our theoretical speed-up of roughly 1 .81×on a Zipfian dataset\nwith exponent 1 .37.\nNext, we demonstrate that our proposed algorithm still manages to outperform the classic skip\nlist even when temporal change exists in the probability vector by comparing the query time for the\n9\n\n(a) CAIDA data distribution\n (b) Zipfian fit ( α= 1.37)\nFigure 1: CAIDA datasets distribution characterization in Figure 1a. The nearly straight-fitted\ncurve in Figure 1b implies that a Zipfian distribution with α= 1.37 is a good fit to the CAIDA\ndataset distribution.\n(a) Insert time on CAIDA\n (b) Query time on CAIDA\nFigure 2: Comparison of insertion and query time on CAIDA for classic and learning-augmented skip\nlists. This figure compares the insertion and query times under varying numbers of top frequently\naccessed unique IPs between classic and augmented implementations. The horizontal axis in the\ntwo subfigures depicts the same scheme of IP selection, represented in two different ways, e.g., the\ntop 29.9 million queries contain 665210 unique IPs, the next 29.5 million queries comprise 296384\nunique IPs, etc.\nsame set of query elements with different probability vectors being used to guide the building of\nthe structure. For the skip list augmented by a noisy probability vector, the probability vector of\nelements during a period of T1 is used as the predicted frequencies. The skip list being augmented\nby this probability vector has its own set of elements to be organized into the target skip list.\nSuppose the historic data from T1 contains a set of elements S1, and some future query stream\ncontains a set of elements S2. For each element in our target set S2, if the element is present in S1,\nthen the occurrence probability of this element from S1 will be used to build S2; otherwise, if the\nelement has not shown up during T1 (i.e., in S1), then we assume its probability to be 0. After this,\nthe probability vector is normalized to sum to 1, resulting in a predicted probability vector to be\nused to build a skip list based on the historic element frequency. Since there is temporal changes in\n10\n\nthe frequency of elements being queried, the predicted probability vector will show a discrepancy\nwith the true probability vector. The results are presented in Figure 3a and we show that even\nwhen the prediction is not perfect, the augmented skip list still performs better than a conventional\nskip list.\n(a) Robustness test on CAIDA datasets\n (b) Oracle credibility on CAIDA datasets\nFigure 3: Robustness of our learning-augmented skip list to erroneous oracles. In Figure 3b, the\nlabels on the axis indicate the time stamp that the internet trace data is collected, e.g., 130100\nmeans the collection starts at 13:01:00 and lasts for 1 minute.\nFigure 3a shows that the skip list with perfect learning shows the best performance, while the\nskip list augmented with noisy learning performs very close to the scenario with perfect predictions.\nMoreover, the closer the test data is to the reference data chronologically, the closer the noisy-\naugmented skip list will perform to the perfect learning skip list. The CAIDA datasets used in this\nstudy contain 12 minutes of internet flow data, which totals around 444 million queries. The indices\non the x-axis in Figure 3a means:\n•102: the first 10 minutes of data are used to create the reference (i.e., oracle) and the last 2\nminutes are used to build and test the total query time using the former as reference.\n•22: the 9th and 10th minutes data is used as reference and the last 2 minutes are used for\ntesting.\n•33: the 1st, 2nd and 3rd minutes of data are used to create reference and the 4th, 5th and\n6th minutes of data are used for testing.\n•66: the first 6 minutes are used to create the reference and the last 6 minutes are used for\ntesting.\nFurther analysis of the temporal change of item frequency shows the reason behind the good\nperformance of the history-based oracle. Figure 3b shows the change of intersection index between\nany 2 given minutes among the 12 minutes of CAIDA data. The intersection index is defined as\nthe ratio of the number of shared queries to the total number of queries of any given 2 minutes of\nqueries. Figure 3b shows that the number of intersects queries has decreased by about 6% after 12\nminutes, which indicates that the probability of the majority of the elements will be predicted with\ngood accuracy, resulting in good oracle performance.\n11\n\nFigure 4: Query time comparison for standard and learning-augmented KD trees with various noise.\nKD trees on synthetic datasets. KD Trees are commonly used in the field of computer\ngraphics, with applications in collision detection, ray-tracing, and reconstruction. We first generate\ndatasets of 212points in 3-dimensional space, with frequencies given by a fixed Zipfian distribution\nwith parameters a= 5, b= 2 – parameters at which our method greatly outperforms a standard KD\ntree. In order to simulate constructing the tree on noisy data, we multiply the ground truth query\nprobabilities by numbers sampled uniformly from 1 to M, and then add numbers uniformly sampled\nfrom 0 to A, before renormalizing to form a valid probability distribution. We query the tree 214\ntimes, with point queries selected by the ground truth Zipfian distribution. We repeat this process\n32 times, and report the median of the average query depth across all runs in Figure 4. We find\nthat our method continues to outperform traditional KD trees under moderate amounts of noise,\nand at worst, performs on-par with a traditional KD tree.\nNext, we generate datasets of 212points in 3-dimensional space, with frequencies given by a\nZipfian distribution with parameters a, b. In the left plot, we assign these Zipfian weights randomly.\nIn the right plot, however, we assign Zipfian weights with ranks decreasing with the distance to\nsome random data point. We then query the tree 214times, with point queries selected by the same\nZipfian distribution. We repeat this process 32 times, and report the median of the average query\ndepth across all runs. We find that, when points frequencies are distributed smoothly over space,\nour method’s performance increases on less skew distributions, as seen in this Figure 5.\nKD trees on 3D point-cloud datasets. Finally, we evaluate our method on point cloud data\ngenerated from the Stanford Lucy mesh [ Sta96 ], with dimensions ∼1000×500×1500. We first\nuniformly sample 222points along the mesh surface, and bin points with resolution 10, and assign\nlookup frequencies by the number of bin occupants. This results in 32k bins. Note, the resulting\nfrequency distribution for binned cells is not highly skewed.\nWe then generate a new set of 216surface samples on the mesh, binning them and assigning\nfrequencies in the same way. When looking up with the new samples, our method yields an average\nquery depth of 15.1, while a traditional KD tree yields an average lookup depth of 17.6.\n12\n\nFigure 5: Comparison of query time on learning-augmented KD trees with and without smooth\nspatial distribution across various Zipfian parameters\nAcknowledgements\nSamson Zhou is supported in part by NSF CCF-2335411. The work was conducted in part while\nSamson Zhou was visiting the Simons Institute for the Theory of Computing as part of the Sublinear\nAlgorithms program.\nReferences\n[ACC+11] Nir Ailon, Bernard Chazelle, Kenneth L. Clarkson, Ding Liu, Wolfgang Mulzer, and\nC. Seshadhri. Self-improving algorithms. SIAM J. Comput. , 40(2):350–375, 2011. 20\n[ACE+23] Antonios Antoniadis, Christian Coester, Marek Eli´ as, Adam Polak, and Bertrand\nSimon. Online metric algorithms with untrusted predictions. ACM Trans. Algorithms ,\n19(2):19:1–19:34, 2023. 1\n[ACI22] Anders Aamand, Justin Y. Chen, and Piotr Indyk. (optimal) online bipartite matching\nwith degree information. In Advances in Neural Information Processing Systems 35:\nAnnual Conference on Neural Information Processing Systems, NeurIPS , 2022. 1\n[ACL14] Laurence Aitchison, Nicola Corradi, and Peter E Latham. Zipf’s law arises naturally\nin structured, high-dimensional data. arXiv preprint arXiv:1407.7135 , 2014. 33\n[ACL+21] Matteo Almanza, Flavio Chierichetti, Silvio Lattanzi, Alessandro Panconesi, and\nGiuseppe Re. Online facility location with multiple advice. In Advances in Neural Infor-\nmation Processing Systems 34: Annual Conference on Neural Information Processing\nSystems, NeurIPS , pages 4661–4673, 2021. 1\n[AGKK23] Antonios Antoniadis, Themis Gouleakis, Pieter Kleer, and Pavel Kolev. Secretary\nand online matching problems with machine learned advice. Discret. Optim. , 48(Part\n2):100778, 2023. 1, 19\n13\n\n[AGKP21] Keerti Anand, Rong Ge, Amit Kumar, and Debmalya Panigrahi. A regression approach\nto learning-augmented online algorithms. In Advances in Neural Information Processing\nSystems 34: Annual Conference on Neural Information Processing Systems, NeurIPS ,\npages 30504–30517, 2021. 1\n[AGKP22] Keerti Anand, Rong Ge, Amit Kumar, and Debmalya Panigrahi. Online algorithms\nwith multiple predictions. In International Conference on Machine Learning, ICML ,\npages 582–598, 2022. 1\n[AGP20] Keerti Anand, Rong Ge, and Debmalya Panigrahi. Customizing ML predictions for\nonline algorithms. In Proceedings of the 37th International Conference on Machine\nLearning, ICML , pages 303–313, 2020. 1\n[AM78] Brian Allen and J. Ian Munro. Self-organizing binary search trees. J. ACM , 25(4):526–\n535, 1978. 20\n[APT22] Yossi Azar, Debmalya Panigrahi, and Noam Touitou. Online graph algorithms with\npredictions. In Proceedings of the 2022 ACM-SIAM Symposium on Discrete Algorithms,\nSODA , pages 35–66, 2022. 1\n[ASS20] Chen Avin, Iosif Salem, and Stefan Schmid. Working set theorems for routing in self-\nadjusting skip list networks. In IEEE INFOCOM 2020-IEEE Conference on Computer\nCommunications , pages 2175–2184. IEEE, 2020. 2\n[BBB+20] Dmitry Basin, Edward Bortnikov, Anastasia Braginsky, Guy Golan-Gueta, Eshcar\nHillel, Idit Keidar, and Moshe Sulamy. Kiwi: A key-value map for scalable real-time\nanalytics. ACM Transactions on Parallel Computing (TOPC) , 7(3):1–28, 2020. 2\n[BHZ18] Jeremiah Blocki, Benjamin Harsha, and Samson Zhou. On the economics of offline\npassword cracking. In IEEE Symposium on Security and Privacy, SP, Proceedings ,\npages 853–871. IEEE Computer Society, 2018. 3\n[BMS20] ´Etienne Bamas, Andreas Maggiori, and Ola Svensson. The primal-dual method for\nlearning augmented algorithms. In Advances in Neural Information Processing Systems\n33: Annual Conference on Neural Information Processing Systems, NeurIPS , 2020. 1\n[CAI16] CAIDA. The caida ucsd anonymized internet traces. https://www.caida.org/\ncatalog/datasets/passive_dataset , 2016. 9\n[CCC+23] Xinyuan Cao, Jingbang Chen, Li Chen, Chris Lambert, Richard Peng, and Daniel\nSleator. Learning-augmented b-trees, 2023. 19\n[CD07] Lawrence Cayton and Sanjoy Dasgupta. A learning framework for nearest neighbor\nsearch. In Advances in Neural Information Processing Systems 20, Proceedings of the\nTwenty-First Annual Conference on Neural Information Processing Systems , pages\n233–240, 2007. 20\n[CEI+22] Justin Y. Chen, Talya Eden, Piotr Indyk, Honghao Lin, Shyam Narayanan, Ronitt\nRubinfeld, Sandeep Silwal, Tal Wagner, David P. Woodruff, and Michael Zhang.\nTriangle and four cycle counting with predictions in graph streams. In The Tenth\nInternational Conference on Learning Representations, ICLR , 2022. 2\n14\n\n[CFLM02] Valentina Ciriani, Paolo Ferragina, Fabrizio Luccio, and S. Muthukrishnan. Static opti-\nmality theorem for external memory string access. In 43rd Symposium on Foundations\nof Computer Science (FOCS, Proceedings , pages 219–227. IEEE Computer Society,\n2002. 20\n[CIW22] Justin Y. Chen, Piotr Indyk, and Tal Wagner. Streaming algorithms for support-aware\nhistograms. In International Conference on Machine Learning, ICML , pages 3184–3203,\n2022. 2\n[CLR+24] Karthik C. S., Euiwoong Lee, Yuval Rabani, Chris Schwiegelshohn, and Samson Zhou.\nOn approximability of ℓ2\n2min-sum clustering. CoRR , abs/2412.03332, 2024. 2\n[Cov99] Thomas M Cover. Elements of information theory . John Wiley & Sons, 1999. 23\n[CSVZ22] Justin Y. Chen, Sandeep Silwal, Ali Vakilian, and Fred Zhang. Faster fundamental\ngraph algorithms via learned predictions. In International Conference on Machine\nLearning, ICML , pages 3583–3602, 2022. 1\n[DIL+21] Michael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvit-\nskii. Faster matchings via learned duals. In Advances in Neural Information Processing\nSystems 34: Annual Conference on Neural Information Processing Systems, NeurIPS ,\npages 10393–10406, 2021. 1\n[DMVW23] Sami Davies, Benjamin Moseley, Sergei Vassilvitskii, and Yuyan Wang. Predictive flows\nfor faster ford-fulkerson. In International Conference on Machine Learning, ICML ,\nvolume 202, pages 7231–7248, 2023. 1\n[EFS+22] Jon C. Ergun, Zhili Feng, Sandeep Silwal, David P. Woodruff, and Samson Zhou.\nLearning-augmented k-means clustering. In The Tenth International Conference on\nLearning Representations, ICLR , 2022. 2, 20\n[Gab99] Xavier Gabaix. Zipf’s law for cities: an explanation. The Quarterly journal of economics ,\n114(3):739–767, 1999. 3\n[GLS+22] Elena Grigorescu, Young-San Lin, Sandeep Silwal, Maoyuan Song, and Samson Zhou.\nLearning-augmented algorithms for online linear and semidefinite programming. In\nAdvances in Neural Information Processing Systems 35: Annual Conference on Neural\nInformation Processing Systems, NeurIPS , 2022. 1\n[Goo12] Google. Google ngram viewer. http://books.google.com/ngrams/datasets, 2012. 33\n[Goo22] Google books n-gram frequency lists. https://github.com/orgtre/\ngoogle-books-ngram-frequency , 2022. 33\n[GP06] C. Torgeson G. Pass, A. Chowdhury. 500k user session collection. https://www.kaggle.\ncom/datasets/dineshydv/aol-user-session-collection-500k , 2006. 30\n[GP19] Sreenivas Gollapudi and Debmalya Panigrahi. Online algorithms for rent-or-buy with\nexpert advice. In Proceedings of the 36th International Conference on Machine Learning,\nICML , pages 2319–2327, 2019. 1\n15\n\n[GZ08] Tingjian Ge and Stan Zdonik. A skip-list approach for efficiently processing forecasting\nqueries. Proceedings of the VLDB Endowment , 1(1):984–995, 2008. 2\n[HIKV19] Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency\nestimation algorithms. In 7th International Conference on Learning Representations,\nICLR , 2019. 2, 19\n[HPJ03] Yih-Chun Hu, Adrian Perrig, and David B Johnson. Efficient security mechanisms for\nrouting protocolsa. In Ndss, 2003. 2\n[Huf52] David A Huffman. A method for the construction of minimum-redundancy codes.\nProceedings of the IRE , 40(9):1098–1101, 1952. 22\n[IKQP21] Sungjin Im, Ravi Kumar, Mahshid Montazer Qaem, and Manish Purohit. Online\nknapsack with frequency predictions. In Advances in Neural Information Processing\nSystems 34: Annual Conference on Neural Information Processing Systems, NeurIPS ,\npages 2733–2743, 2021. 1\n[ISZ21] Zachary Izzo, Sandeep Silwal, and Samson Zhou. Dimensionality reduction for wasser-\nstein barycenter. In Advances in Neural Information Processing Systems 34: Annual\nConference on Neural Information Processing Systems 2021, NeurIPS , pages 15582–\n15594, 2021. 20\n[IVY19] Piotr Indyk, Ali Vakilian, and Yang Yuan. Learning-based low-rank approximations. In\nAdvances in Neural Information Processing Systems 32: Annual Conference on Neural\nInformation Processing Systems 2019, NeurIPS , pages 7400–7410, 2019. 2\n[JLL+20] Tanqiu Jiang, Yi Li, Honghao Lin, Yisong Ruan, and David P. Woodruff. Learning-\naugmented data stream algorithms. In 8th International Conference on Learning\nRepresentations, ICLR , 2020. 2\n[JLL+22] Shaofeng H.-C. Jiang, Erzhi Liu, You Lyu, Zhihao Gavin Tang, and Yubo Zhang.\nOnline facility location with predictions. In The Tenth International Conference on\nLearning Representations, ICLR , 2022. 1\n[KADV23] Mikhail Khodak, Kareem Amin, Travis Dick, and Sergei Vassilvitskii. Learning-\naugmented private algorithms for multiple quantile release. In International Conference\non Machine Learning, ICML 2023 , pages 16344–16376, 2023. 1\n[KBC+18] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. The case\nfor learned index structures. In Proceedings of the 2018 International Conference on\nManagement of Data, SIGMOD Conference , pages 489–504, 2018. 1, 19\n[KBTV22] Misha Khodak, Maria-Florina Balcan, Ameet Talwalkar, and Sergei Vassilvitskii. Learn-\ning predictions for algorithms with predictions. In Advances in Neural Information\nProcessing Systems 35: Annual Conference on Neural Information Processing Systems,\nNeurIPS , 2022. 1\n16\n\n[KLR96] Marek Karpinski, Lawrence L. Larmore, and Wojciech Rytter. Sequential and parallel\nsubquadratic work algorithms for constructing approximately optimal binary search\ntrees. In Proceedings of the Seventh Annual ACM-SIAM Symposium on Discrete\nAlgorithms , pages 36–41, 1996. 19\n[Knu71] Donald E. Knuth. Optimum binary search trees. Acta Informatica , 1:14–25, 1971. 2, 5,\n6, 19\n[LJ13] Jonatan Lind´ en and Bengt Jonsson. A skiplist-based concurrent priority queue with\nminimal memory contention. In Principles of Distributed Systems: 17th International\nConference, OPODIS 2013, Nice, France, December 16-18, 2013. Proceedings 17 , pages\n206–220. Springer, 2013. 2\n[LLL+23] Yi Li, Honghao Lin, Simin Liu, Ali Vakilian, and David P. Woodruff. Learning\nthe positions in countsketch. In The Eleventh International Conference on Learning\nRepresentations, ICLR , 2023. 2\n[LLMV20] Silvio Lattanzi, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Online\nscheduling via learned weights. In Proceedings of the 2020 ACM-SIAM Symposium on\nDiscrete Algorithms, SODA , pages 1859–1877, 2020. 1\n[LLW22] Honghao Lin, Tian Luo, and David P. Woodruff. Learning augmented binary search\ntrees. In International Conference on Machine Learning, ICML , pages 13431–13440,\n2022. 1, 2, 5, 6, 19\n[LV21] Thodoris Lykouris and Sergei Vassilvitskii. Competitive caching with machine learned\nadvice. J. ACM , 68(4):24:1–24:25, 2021. 1, 19\n[LVM+23] Silvia Lazzardi, Filippo Valle, Andrea Mazzolini, Antonio Scialdone, Michele Caselle,\nand Matteo Osella. Emergent statistical laws in single-cell transcriptomic data. Physical\nReview E , 107(4):044403, 2023. 3\n[Meh77] Kurt Mehlhorn. A best possible bound for the weighted path length of binary search\ntrees. SIAM J. Comput. , 6(2):235–239, 1977. 2, 5, 6, 19, 20\n[Mit18] Michael Mitzenmacher. A model for learned bloom filters and optimizing by sandwiching.\nInAdvances in Neural Information Processing Systems 31: Annual Conference on\nNeural Information Processing Systems, NeurIPS , pages 462–471, 2018. 1, 19\n[MV20] Michael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. In Tim\nRoughgarden, editor, Beyond the Worst-Case Analysis of Algorithms , pages 646–662.\nCambridge University Press, 2020. 1\n[NCN23] Thy Dinh Nguyen, Anamay Chaturvedi, and Huy L. Nguyen. Improved learning-\naugmented algorithms for k-means and k-medians clustering. In The Eleventh Interna-\ntional Conference on Learning Representations, ICLR , 2023. 2\n[PSK18] Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via\nML predictions. In Advances in Neural Information Processing Systems 31: Annual\nConference on Neural Information Processing Systems 2018, NeurIPS , pages 9684–9693,\n2018. 1, 19\n17\n\n[PT10] Jonathan J Pittard and Alan L Tharp. Simplified self-adapting skip lists. In Inter-\nnational Conference on Intelligent Data Engineering and Automated Learning , pages\n126–136. Springer, 2010. 2\n[Pug90a] William Pugh. Concurrent maintenance of skip lists . University of Maryland at College\nPark, 1990. 2\n[Pug90b] William Pugh. Skip lists: A probabilistic alternative to balanced trees. Commun. ACM ,\n33(6):668–676, jun 1990. 2\n[San15] Agnar Sandmo. The principal problem in political economy: income distribution in\nthe history of economic thought. In Handbook of income distribution , volume 2, pages\n3–65. Elsevier, 2015. 3\n[SGM22] Ziv Scully, Isaac Grosof, and Michael Mitzenmacher. Uniform bounds for scheduling\nwith job size estimates. In 13th Innovations in Theoretical Computer Science Conference,\nITCS , pages 114:1–114:30, 2022. 1\n[Sha01] Claude Elwood Shannon. A mathematical theory of communication. ACM SIGMOBILE\nmobile computing and communications review , 5(1):3–55, 2001. 22\n[SL00] Nir Shavit and Itay Lotan. Skiplist-based concurrent priority queues. In Proceedings\n14th International Parallel and Distributed Processing Symposium. IPDPS 2000 , pages\n263–268. IEEE, 2000. 2\n[SLLA23] Yongho Shin, Changyeol Lee, Gukryeol Lee, and Hyung-Chan An. Improved learning-\naugmented algorithms for the multi-option ski rental problem via best-possible com-\npetitive analysis. In International Conference on Machine Learning, ICML , pages\n31539–31561, 2023. 1\n[Sta96] The stanford 3d scanning repository, 1996. 12\n[VA15] Nikolay K Vitanov and Marcel Ausloos. Test of two hypotheses explaining the size of\npopulations in a system of cities. Journal of Applied Statistics , 42(12):2686–2693, 2015.\n3\n[WLW20] Shufan Wang, Jian Li, and Shiqiang Wang. Online algorithms for multi-shop ski rental\nwith machine learned advice. In Advances in Neural Information Processing Systems\n33: Annual Conference on Neural Information Processing Systems, NeurIPS , 2020. 1\n[WW16] Ding Wang and Ping Wang. On the implications of zipf’s law in passwords. In\nComputer Security - ESORICS 2016 - 21st European Symposium on Research in\nComputer Security, Proceedings, Part I , volume 9878 of Lecture Notes in Computer\nScience , pages 111–131. Springer, 2016. 3\n[WZ20] Alexander Wei and Fred Zhang. Optimal robustness-consistency trade-offs for learning-\naugmented online algorithms. In Advances in Neural Information Processing Systems\n33: Annual Conference on Neural Information Processing Systems, NeurIPS , 2020. 1\n[Yao82] F Frances Yao. Speed-up in dynamic programming. SIAM Journal on Algebraic\nDiscrete Methods , 3(4):532–540, 1982. 19\n18\n\n[ZCL+23] Xuanhe Zhou, Cheng Chen, Kunyi Li, Bingsheng He, Mian Lu, Qiaosheng Liu, Wei\nHuang, Guoliang Li, Zhao Zheng, and Yuqiang Chen. Febench: A benchmark for\nreal-time relational data feature extraction. Proceedings of the VLDB Endowment ,\n16(12):3597–3609, 2023. 2\n[ZKH24] Ali Zeynali, Shahin Kamali, and Mohammad Hajiesmaili. Robust learning-augmented\ndictionaries. CoRR , abs/2402.09687, 2024. 5, 19\nA Additional Related Works\nIn this section, we discuss a number of related works in addition to those mentioned in Section 1. This\npaper builds upon the increasing body of research in learning-augmented algorithms, data-driven\nalgorithms, and algorithms with predictions. For example, learning-augmented algorithms have\nbeen applied to a number of problems in the online setting, where the input arrives sequentially\nand the goal is to achieve algorithmic performance competitive with the best solution in hindsight,\ni.e., an algorithm that has the complete input on hand. Among the applications in the online\nmodel, learning-augmented algorithms have been developed for ski rental problem and job schedul-\ning [PSK18 ], caching [ LV21 ], and matching [ AGKK23 ]. Learning-augmented algorithms have also\nbeen used to improve the performance of specific data structures such as Bloom filters [ Mit18 ],\nindex structures [ KBC+18], CountMin and CountSketch [ HIKV19 ]. Specifically, [ KBC+18] proposes\nsubstituting B-Trees (or other index structures) with trained models for querying databases. In\ntheir approach, rather than traversing the B-Tree to locate a record, they use a neural network to\ndirectly identify its position. Our work differs in that we retain the desired data structures, i.e., skip\nlists and kd trees, and focus on optimizing their structures to enable faster queries, which allows\nus to continue supporting standard operations specific to the data structures such as traversal,\norder statistics, merging, and joining, among others. Our work uses the frequency estimation oracle\ntrained in [ HIKV19 ] on the AOL search query dataset and the CAIDA IP traffic monitoring dataset.\nPerhaps the works most closely related to ours in the area of learning-augmented algorithms are\nthose of [ LLW22 ,CCC+23,ZKH24 ]. [LLW22 ] noted that traditional theory on statically optimal\nbinary search trees [ Knu71 ,Meh77 ] is no longer applicable in dynamic settings, where elements\nare added incrementally over time. Hence, they developed learning-augmented binary search trees\n(BSTs) and showed that their expected search time is near-optimal. [ CCC+23] then extended these\ntechniques to general search trees, allowing for nodes with more than two children. [ CCC+23] also\nstudied the setting where the predictions may be updated, while ultimately still utilizing a data\nstructure that requires rebalancing as data is dynamically changing. [ ZKH24 ] also consider the\nperformance of learning-augmented skip lists that are robust to erroneous predictions; we elaborate\nmore on the differences from [ ZKH24 ] in Section 1.1. We also note that none of these works consider\nKD trees at all, which is an important data structure with applications in computer vision and\ncomputational geometry, thus forming a basis of our work. For a more comprehensive source\nof related works in learning-augmented algorithms, see https://algorithms-with-predictions.\ngithub.io/ .\nBeyond the context of learning-augmented algorithms, there is a large body of works that study\ndesign of data structures that are optimal for their inputs. For example, while standard binary search\ntrees use O(logn)query time, optimal static trees can be constructed using dynamic programming\nor efficient greedy algorithms [ Meh77 ,Yao82 ,KLR96 ], given access frequencies. However, the\n19\n\ncomputational cost of these methods often exceeds the cost of directly querying the tree. As a result,\na key objective is to construct a tree whose cost is within a constant factor of the entropy of the\ndata. Several approaches have achieved this either for worst-case data [ Meh77 ] or when the input\nfollows particular distributions [AM78].\nMore recent works have considered using results from learning theory to estimate the query\nfrequencies, rather than assuming explicit access to their values. For example, [ CD07 ] studied\nhow to obtain such an oracle for learning-augmented data structures. In particular, they study\ngeneralization bounds in the context of learning theory, analyzing the number of samples from\nan underlying distribution necessary to produce an oracle with a small error rate. On the other\nhand, [ ACC+11] studied algorithms for sorting and clustering that can improve their expected\nperformance given access to multiple instances sampled from a fixed distribution. Although the\nhigh-level goal of improving algorithmic performance using auxiliary information is the same as\nours, the specifics of the paper seem quite different than ours, as the paper focuses on techniques for\nsorting and clustering. Similarly, [ CFLM02 ] considers self-adjusting data structures, including skip\nlists, which can dynamically change as the sequence of queries arrive. However, their methods are\ncatered specifically to the setting where there is access to the queries, whereas our data structures\nmust be constructed without such access and must therefore be able to handle erroneous predictions.\nFinally, we remark that utilizing techniques for learning theory, there are standard results about\nthe learnability of oracles for the purposes of learning-augmented algorithms [EFS+22, ISZ21].\nB Missing Proofs from Section 2\nIn this section, we give the missing proofs from Section 2.\nB.1 Expected Search Time\nWe first show that each item is promoted to a higher level with probability at least1\n2.\nLemma B.1. For each item i∈[n]at level ℓ, the probability that iis in level ℓ+ 1is at least1\n2.\nProof. Note that if pi≥2ℓ\nn, then iwill be placed in level ℓ+ 1. Otherwise, conditioned on the\nitem i∈[n] being at level ℓ, then Algorithm 1 places iat level ℓ+ 1 with probability1\n2. Thus, the\nprobability that iis in level ℓ+ 1 is at least1\n2.\nWe next upper bound the expected search time for any item at any fixed level, where the randomness\nis over the construction of the skip list.\nLemma B.2. In expectation, the search time for item i∈[n]at level ℓis at most 2.\nProof. Suppose item i∈[n] is in level ℓ. Let Sℓ\n<i⊆[n] be the subset of items in level ℓthat are less\nthan i. Note that by Lemma B.1, each item of Sℓ\n≤iis promoted to level ℓ+ 1 with probability at\nleast1\n2. Thus, the search time for item iat level ℓistif and only if the previous titems in Sℓ\n≤i\nwere all not promoted, which can only happen with probability at most1\n2t. Hence, the expected\nsearch time Tfor item i∈[n] at level ℓis at most\nE[T]≤1·1\n2+ 2·1\n22+. . .+n·1\n2n≤∞X\nt=1t\n2t≤2.\n20\n\nWe now show that each item imust be contained at some level depending on the predicted\nfrequency piof the item.\nLemma B.3. Each item iis included in level max (0 ,1 +⌊log(npi)⌋).\nProof. First, observe that all items are inserted at level 0. Next, note that Algorithm 1 inserts item\niinto level ℓifpi≥2ℓ−1\nnor equivalently log(npi)≥ℓ−1. Thus, each item iis included in level\nmax (0 ,1 +⌊log(npi)⌋).\nWe next analyze the expected search time for each item i.\nLemma B.4. Suppose the total number of levels is at most C+lognfor some constant C > 0.\nThen the expected search time for item iis at most 2C+ 2 min\u0010\nlog1\npi,logn\u0011\n.\nProof. By Lemma B.3, item iis included in level max (0 ,1 +⌊log(npi)⌋). By Lemma B.2 the\nexpected search time at each level is at most 2. Thus, in expectation, the total search time is at\nmost 2( C+ log n−max (0 ,1 +⌊log(npi)⌋))≤2C+ 2 min\u0010\nlog1\npi,logn\u0011\n.\nFinally, we analyze the expected search time across the true probability distribution fi.\nLemma B.5. Suppose the total number of levels is at most C+lognfor some constant C > 0.\nFor each i∈[n], letfibe the proportion of queries to item i. Then the expected search time at most\n2C+ 2Pn\ni=1fi·min\u0010\nlog1\npi,logn\u0011\n.\nProof. For each query, the probability that the query is item iisfi. Conditioned on the total\nnumber of levels being at most C+logn, then by Lemma B.4, the expected search time for item i\nis at most 2 C+ 2 max\u0010\nlog1\npi,logn\u0011\n. Thus, the expected search time at most\n2C(f1+. . .+fn) + 2f1min\u0012\nlog1\np1,logn\u0013\n+. . .+ 2fnmin\u0012\nlog1\npn,logn\u0013\n= 2C+ 2nX\ni=1fimin\u0012\nlog1\npi,logn\u0013\n.\nWe now show that with high probability, the total number of levels in the skip list is at most\nO(logn).\nLemma B.6. With probability at least 0.99, the total number of levels in the skip list is at most\n10 + log n.\nProof. For each level ℓ, letnℓbe the number of items i∈[n] that are deterministically promoted to\nexactly level ℓ, i.e., pi∈h\n2ℓ−1\nn,2ℓ\nn\u0011\n. Note that for each fixed i∈[n], the highest level it remains is a\ngeometric random variable with parameter1\n2, beyond the highest level at which it is deterministically\nplaced. This is because the item is promoted to each higher level with probability1\n2. Hence with\nprobability 1 −1\n2k,iis not placed at least klevels above its highest deterministic placement.\nTherefore, the probability that an item at level ℓis placed at level 10 + lognis at most2ℓ\n1024n. Since\n21\n\nno fixed iwill have predicted frequency more than 1, then no item will be deterministically placed\nat level 2 + logn. Hence by a union bound over all ℓ∈[2 +logn], the probability that an item is\nplaced at level 10 + log nis at most\n2+log nX\nℓ=0nℓ·2ℓ\n1024n.\nOn the other hand, we havePn\ni=1pi= 1, so that\n2+log nX\nℓ=0nℓ·2ℓ≤2n.\nTherefore, with probability at least 0 .99, the total number of levels in the skip list is at most\n10 + log n.\nThus, putting together Lemma B.5 and Lemma B.6, we get:\nTheorem 2.1. For each i∈[n], letfiandpibe the proportion of true and predicted queries to item\ni. Then with probability at least 0.99over the randomness of the construction of the skip list, the\nexpected search time over the choice of queries at most 20 + 2Pn\ni=1fi·min\u0010\nlog1\npi,logn\u0011\n.\nB.2 Near-Optimality\nWe first recall the construction of a Huffman code, a type of variable-length code that is often used\nfor data compression. The encoding for a Huffman is known to be an optimal prefix code and can\nbe represented by a binary tree, which we call the Huffman tree [Huf52].\nTo construct a Huffman code, we first create a min-heap priority queue that initially contains all\nthe leaf nodes sorted by their frequencies, so that the least frequent items have the highest priority.\nThe algorithm then iteratively removes the two nodes with the lowest frequencies from the priority\nqueue, which become the left and right children of a new internal node that is created to represent\nthe sum of the frequencies of the two nodes. This internal node is then added back to the priority\nqueue. This process is continued until there only remains a single node left in the priority queue,\nwhich is then the root of the Huffman tree.\nA binary code is then assigned to the paths from the root to each leaf node in the Huffman\ntree, so that each movement along a left edge in the tree corresponds to appending a 0 to the\ncodeword, and each movement along a right edge in the tree corresponds to appending a 1 to the\ncodeword. Thus, the resulting binary code for each item is the path from the root to the leaf node\ncorresponding to the item.\nHuffman coding is a type of symbol-by-symbol coding, where each individual item is separately\nencoded, as opposed to alternatives such as run-length encoding. It is known that Huffman coding\nis optimal among symbol-by-symbol coding with a known input probability distribution [ Huf52 ]\nand moreover, by Shannon’s source coding theorem, that the entropy of the probability distribution\nis an upper bound on the expected length of a codeword of a symbol-by-symbol coding:\nTheorem B.7 (Shannon’s source coding theorem) .[Sha01 ] Given a random variable X∈[n]\nso that X=iwith probability fi, letL(X)denote the length of the codeword assigned to Xby a\nHuffman code. Then E[L(x)]≥H(f), where H(f)is the entropy of f.\n22\n\nWe now prove our lower bound on the expected search time of an item drawn from a probability\ndistribution f.\nTheorem 2.2. Given a random variable X∈[n]so that X=iwith probability fi, letT(X)denote\nthe search time for Xin a skip list. Then E[T(x)]≥H(f), where H(f)is the entropy of f.\nProof. LetLbe a skip list. We build a symbol-by-symbol encoding using the search process in L.\nWe begin at the top level. At each step, we either terminate, move to the next item at the current\nlevel, or move down to a lower level. Similar to the Huffman coding, we append a 0 to the codeword\nwhen we move down to a lower level, and we append a 1 to the codeword when we move to the\nnext item at the current level. Now, the search time for an item xinLcorresponds to the length\nof the codeword of xin the symbol-by-symbol encoding. By Theorem B.7 and the optimality of\nHuffman codes among symbol-by-symbol encodings, we have that E[T(x)]≥H(f), where fis the\nprobability distribution vector of x.\nB.3 Zipfian Distribution\nIn this section, we briefly describe the implications of our data structure to Zipfian distributions.\nWe first recall the following entropy upper bound for a probability distribution with support at\nmost n.\nTheorem B.8. [Cov99 ] Let fbe a probability distribution on a support of size [n]. Then H(f)≤\nlogn.\nWe can then upper bound the entropy of a probability vector that satisfies a Zipfian distribution\nwith parameter s.\nLemma 2.3. Lets, z > 0be fixed constants and let fbe a frequency vector such that fi=z\nisfor all\ni∈[n]. Ifs >1, then H(f) =O(1)and otherwise if s≤1, then H(f)≤logn.\nProof. Since fis a probability distribution on the support of size [ n], then by Theorem B.8, we\nhave that H(f)≤logn. Thus, it remains to consider the case where s >1. Since z≤1, we have\nh(f) =nX\ni=1z\nislogis\nz\n≤snX\ni=1logi\nis.\nNote that there exists an integer γ >0 such that for i > γ , we havelogi\nis<1\ni(s+1)/2. Since s >1,\nthens+1\n2>1 and thus\nnX\ni=γ1\ni(s+1)/2≤∞X\ni=11\ni(s+1)/2=O(1).\nHence,\nh(f)≤sγ−1X\ni=1logi\nis+s∞X\nγlogi\nis=O(1).\n23\n\nBy Theorem 2.1 and Lemma 2.3, we have the following statement about the performance of our\nlearning-augmented skip list on a set of search queries that follows a Zipfian distribution.\nCorollary 2.4. With high probability, the expected search time on a set of queries that follows a\nZipfian distribution with exponent sis at most O(1)fors >1andO(logn)fors≤1.\nB.4 Noisy Robustness\nIn this section, we show that our learning-augmented skip list construction is robust to somewhat\ninaccurate oracles. Let fbe the true-scaled frequency vector so that for each i∈[n],fiis the\nprobability that a random query corresponds to i. Let pbe the predicted frequency vector, so\nthat for each i∈[n],piis the predicted probability that a random query corresponds to i. For\nα, β∈(0,1), we call an oracle ( α, β)-noisy if for all i∈[n], we have pi≥α·fi−β.\nLemma 2.5. Letαbe a constant and β <α\n4n. A learning-augmented skip list with a set of\n(α, β)-noisy predictions has performance that matches that of a learning-augmented learned with a\nperfect oracle, up to an additive constant.\nProof. Suppose the total number of levels is at most C+lognfor some constant C >0. Note that\nthis occurs with a high probability for a learning-augmented skip list with a set of ( α, β)-noisy\npredictions. For each i∈[n], let fibe the proportion of queries to item iand let pibe the predicted\nproportion of queries to item i. By Lemma B.5, the expected search time at most\n2C+ 2nX\ni=1fi·min\u0012\nlog1\npi,logn\u0013\n.\nSince the oracle is ( α, β)-noisy then we have pi≥α·fi−βfor all i∈[n].\nWe first note that in the expected search time for iis proportional to min\u0010\nlog1\nfi,logn\u0011\n. Thus,\nfor expected search time for item i, it suffices to assume fi>1\n2nfor all i.\nObserve that for fi>1\n2nandβ <α\n4n, then pi≥α·fi−βimplies\npi≥α·fi−β≥α·fi−α\n4n≥α\n2·fi.\nHence, we have1\npi≤2\nα·1\nfiso that the expected search time for item iis at most\n2C+ 2·min\u0012\nlog1\nfi+ log2\nα,logn\u0013\n.\nTherefore, the expected search time is at most\n2C+ 2nX\ni=1fi·min\u0012\nlog1\npi,logn\u0013\n≤2C+ 2nX\ni=1\u0012\nfi·min\u0012\nlog1\nfi,logn\u0013\n+fi·log2\nα\u0013\n≤2C+ 2 log2\nα+ 2nX\ni=1fi·min\u0012\nlog1\nfi,logn\u0013\n.\nSince the perfect oracle would achieve runtime 2 C+ 2Pn\ni=1fi·min\u0010\nlog1\nfi,logn\u0011\n, then it follows\nthat a learning-augmented skip list with a set of ( α, β)-noisy predictions has performance that\nmatches that of a learning-augmented learned with a perfect oracle, up to an additive constant.\n24\n\nC Missing Proofs from Section 3\nFirst, we will show that the expected depth of a given query depends on the probability of that\nquery, and that high frequency queries must be found close to the root of our tree.\nLemma C.1. Suppose [∆]dis the space of possible input points and queries. Let N= ∆dandpibe\nthe probability that a random query is made to i∈[N], given the natural mapping between [N]and\n[∆]d. Then the level at which iresides in the tree is at most O\u0010\nlog1\npi\u0011\n.\nProof. First, consider only the high-frequency query points and data points for which we base our\nconstruction off.\nIn constructing the learning-augmented KD tree, we balance the contents of the children nodes\nsuch that a query to that node has a probability of1\n2of belonging to each of the children. Therefore,\nat a depth of d, the probability of belonging to either child is1\n2d. In particular, a query iwith\nprobability piat a depth dmust satisfy pi>1\n2d. Thus, we have that the depth of iisO\u0010\nlog1\npi\u0011\n, as\ndesired.\nSince the lowest probability of a high-frequency data point is1\nn2, this tree must have a depth of\nat most 2 log n.\nNow, consider a low-frequency data point, which we add to the bottom of the tree. By\nconstruction, the learned point of our tree has depth at most 2 logn. Then, when inserting the\nadditional data points as a balanced KD tree, we can accumulate at most an additional depth of\nlogn. Note, p <1\nn2implies logn <log1\np. Thus, this low-frequency data point will have a depth of\nat most 3 log n=O\u0010\nlog1\np\u0011\n, as desired.\nSimilarly, if iis not a data point and is low frequency, we achieve the same bound of O\u0010\nlog1\np\u0011\n.\nIn this case, we simply terminate at a leaf node and determine that the desired query is not in the\ndataset.\nIn summary, any query which has high frequency can be found in O\u0010\nlog1\np\u0011\ntime. Low-frequency\ndata points can similarly be found in O\u0010\nlog1\np\u0011\ntime, and low frequency queries can be determined\nto not exist in O\u0010\nlog1\np\u0011\ntime.\nLemma C.2. Suppose [∆]dis the space of possible input points and queries. Let N= ∆dandpibe\nthe probability that a random query is made to i∈[N], given the natural mapping between [N]and\n[∆]d. Then the level at which iresides in the tree is at most O(logn).\nProof. This follows directly from the analysis in Llemma C.1.\nNow, we have demonstrated the the depth of a given query point iis bounded by both O(logn)\nandO\u0010\nlog1\npi\u0011\n. Using this fact, we will now show that the expected query time of our algorithm is\nbounded by both the entropy of the dataset H(p) in addition to log n.\nWe now analyze the performance of our learning-augmented KD tree.\nTheorem 3.1. Suppose [∆]dis the space of possible input points and queries. Let N= ∆dand\npibe the probability that a random query is made to i∈[N], given the natural mapping between\n[N]and[∆]d. Let p= (p1, . . . , p N)∈RNbe the probability vector and H(p)be its entropy. Then\n25\n\ngiven a set of ninput points, the expected query time for the tree Tcreated by Algorithm 3 is\nO(min( H(p),logn)).\nProof. Following C.1, the points iin [∆]dwith non-negligible probability pi≥1\nn2are guaranteed to\nexist in the learning-augmented KD tree Twith depth at most log1\npi. For points in the dataset [ n]\nwith negligible probability, they exist in the tree and have depth in O(logn). For all other points\nnot contained in the KD tree, the query will terminate at a depth of O(logn).\nFor any point iin [∆]d, the depth that a query to iwill terminate in the tree is\nDepth( i) =O\u0012\nmin\u0012\nlog1\npi,logn\u0013\u0013\n. (1)\nThen, the expected search time Tis the expected depth of a given point,\nE[T] =X\ni∈[∆]dpi·Depth( i) =X\ni∈[∆]dpi· O\u0012\nmin\u0012\nlog1\npi,logn\u0013\u0013\n=O(min( H(p),logn)).(2)\nThus, we have shown that the expected query time of our algorithm is bounded by the entropy\nof the dataset. In particular, when the dataset has a highly skew distribution, H(p) can be far less\nthan log n.\nC.1 Near-Optimality\nNear-optimality of our learning-augmented KD trees uses a similar argument to the proof of the\nnear-optimality of our learning-augmneted skip lists. In particular, we again utilize Shannon’s\nsource coding theorem from Theorem B.7. We then have the following:\nTheorem 3.2. Given a random variable X∈[n]so that X=iwith probability fi, letD(X)denote\nthe depth for Xin a learning-augmented KD tree. Then E[D(X)]≥H(f), where H(f)is the\nentropy of f.\nProof. In a learning-augmented KD tree, the search path to an element ican be encoded as a 0 −1\ncodeword, with entries indicating whether the lower or upper branch is taken at each node traversal.\nMoreover, the length of this codeword in the symbol-by-symbol encoding corresponds to the depth\nof element i. Then, by Theorem B.7 and the optimality of Huffman codes in symbol-by-symbol\nencodings, we have that E[D(X)]≥H(f), as desired.\nC.2 Noisy Robustness\nIn this section, we analyze the performance of the learning-augmented KD tree under noisy data\nconditions.\nPreviously, we analyzed the performance of the learning-augmented KD-tree assuming access to\na perfect prediction oracle.\nNow we analyze the performance with a noisy oracle. That is, for each i∈[∆]dthere is a\ntrue-scaled frequency fithat the point will be queried and that piis a prediction made by the noisy\noracle.\nFirst, we analyze the multiplicative robustness of the algorithm. In this case, the oracle predicts\nfiup to some multiplicative constant α∈R+such that fi=α pi.\n26\n\nLemma C.3. Suppose [∆]dis the space of possible input points and queries. Let N= ∆dandpibe\nthe probability that a random query is made to i∈[N]during tree construction. Suppose during\nruntime that the true probability of querying iisfi=α pifor some α∈R+. Then the level at which\niresides in the tree is at most O\u0010\nlog1\nfi+ log1\nα\u0011\n.\nProof. Ifα≤1, this is immediate. If this is the case, in construction we expected ito be queried\nmore often than it actually is, so our construction placed the point ihigher in the tree than is\nnecessary. Thus, the depth is at most the previously shown1\npi≤1\nfi.\nNow, suppose α >1. In this case, we must have placed ideeper in the tree than we should have,\nas our construction frequency is less than the true query frequency. Then, as in C.1, the depth of i\nisO\u0010\nlog1\npi\u0011\n.Now, we have that\nO\u0012\nlog1\npi\u0013\n=O\u0012\nlogα\nfi\u0013\n=O\u0012\nlog1\nfi+ log α\u0013\n. (3)\nHaving shown multiplicative robustness of the learning-augmented KD tree we next analyze the\nadditive-multiplicative robustness of the method. An oracle is ( α, β)-noisy if the prediction satisfies\npi≥α fi−βfor constants α, β∈(0,1).\nLemma 3.4. Letαbe a constant and let β≤α\nn2. Then the query time for our learning-augmented\nKD tree with (α, β)-noisy prediction matches the performance of a learning-augmented KD tree\nconstructed using a perfect oracle up to an additive constant.\nProof. From Lemma C.1 we have that the depth of a point iin the learning-augmented KD tree is\nO\u0010\n1\npi\u0011\nwhere piis the predicted frequency of i.\nThen, with an ( α, β)-noisy oracle we have that the prediction is bounded from below by\npi≥α fi−β. As iis a point in the tree, we can assume that the true-scaled frequency fihas a\nlower-bound of 1 /n2.\nBy choosing β≤α/n2we ensure that the predicted piis always nonnegative. Then, let\nβ=α/2n2\npi≥α fi−β≥α fi−α\n2n2≥α\n2fi (4)\nThen, applying Lemma C.1 again\nO\u0012\nlog1\npi\u0013\n=O\u0012\nlog2\nαfi\u0013\n=O\u0012\nlog1\nfi+ log1\nα\u0013\n. (5)\nD Additional Empirical Evaluations\nD.1 Skip Lists\nIn this section, we perform empirical evaluations comparing the performance of our learning-\naugmented skip list to that of traditional skip lists, on both synthetic and real-world datasets.\n27\n\nFirstly, we compare the performance of traditional skip lists with our learning-augmented skip lists\non synthetically generated data following Zipfian distributions. The proposed learning-augmented\nskip lists are evaluated empirically with both synthetic datasets and real-world internet flow datasets\nfrom the Center for Applied Internet Data Analysis (CAIDA) and AOL. In the synthetic datasets,\na diverse range of element distributions, which are characterized by the skewness of the datasets,\nare evaluated to assess the effectiveness of the learning augmentation. In the CAIDA datasets, the\nαfactor is calculated to reflect the skewness of the data distribution.\nThe metrics of performance evaluations include insertion time and query time, representing\nthe total time it takes to insert all elements in the query stream and the time it takes to find all\nelements in the query stream using the data structure, respectively.\nThe computer used for benchmarking is a Lenovo Thinkpad P15 with an intel core i7-11800H@2.3GHz,\n64GB RAM, and 1TB of Solid State Drive. The tests were conducted in a Ubuntu 22.04.3 LTS OS.\nGNOME version 42.9.\nD.1.1 Synthetic Datasets\nIn the synthetic datasets, both the classic and augmented skip lists are tested against different\nelement counts and αvalues. In terms of the distribution of the synthetic datasets, the uniform\ndistribution and a Zipfian distribution of αbetween 1.01 and 2 with query counts up to 4 million\nare evaluated. It is worth noting that the number of unique element queries could vary for the same\nquery count at different αvalues in the Zipfian distribution, which may affect the insertion time.\nTable 1: Speed up factor of augmented skip list over classic skip list under different synthetic\ndistributions\nDistributionQuery size of synthetic data (unit: thousand)\n0.5 10 100 500 1000 1500 2000 2500 3000 3500 4000 Average\nuniform 3.02 0.84 1.01 1.05 1.11 1.14 1.17 1.21 1.22 1.42 1.4 1.33\nα=1.01 3.63 2.6 1.04 1.24 1.03 1.21 1.2 1.14 1.3 1.18 1.3 1.53\nα=1.25 3.28 3.74 5.87 2.89 2.47 3.21 2.95 3.34 3.55 3.16 3.12 3.42\nα=1.5 2.42 8.97 6.93 6.54 7.99 5.83 4.65 3.8 4.92 5.34 5.93 5.76\nα=1.75 12.43 10.4 5.76 9.78 6.76 7.13 7.31 7.09 6.63 5.07 6.98 7.76\nα=2 8.19 2.5 5.56 10.1 4.47 3.91 7.26 5.33 9.29 7.65 5.55 6.35\nTable 2: Node count for each distribution configuration in the 4 million dataset\nαUnique node count\n1.01 2886467\n1.25 259892\n1.75 8386\n2 2796\nTable 1 shows the speed-up factor, defined as the time taken by the augmented skip list over\nthe classic skip list for the same query stream. We can observe a progressive improvement in the\nperformance of our augmented skip lists as the dataset skewness increases. It also suggests that our\naugmented skip list will perform at least as good as the traditional skip list and will outperform a\ntraditional skip list by a factor of up to 7 times depending on the skewness of the datasets.\n28\n\nFigure 6 shows that the insertion time decreases with more skewed datasets for the same size of\nthe query stream. This is attributed to the reduced number of nodes in the datasets, as shown in\nTable 2.\nThe query time of augmented skip lists is also reduced greatly compared to the classic skip lists\nas shown in Figure 7.\n(a) Uniform Distribution\n (b)α= 1.25\n(c)α= 1.5\n (d)α= 2\nFigure 6: Insertion time for synthetic datasets with a uniform distribution and under different α\nvalues of the Zipfian distribution for both classic and augmented skip lists. This figure illustrates the\ninsertion time on the synthetic data for both the uniform distribution and the Zipfian distribution\nat different αvalues. Generally, higher skewness of the datasets results in less insertion time when\nusing the augmented structure. The decrease in insertion time is proportional to the increase in the\nαvalue, as a higher αvalue leads to a reduction in the number of unique nodes, as illustrated in\nTable 2.\nIn addition, we conduct experiments to compare the performance of standard binary search trees\nand standard skip lists. In particular, we generate datasets of size n∈ {5000,10000 ,15000 ,20000 ,25000 ,30000 ,35000 ,40000 ,45000 ,50000 ,}.\nFor each fixed value of n, each element of the dataset is generated uniformly at random in [2 n], i.e.,\nuniformly at random from {1,2, . . . , 2n}. Because the dataset is generated uniformly at random,\nthen learning-augmented data structures will perform similar to oblivious data structures. We\nmeasure the construction time of the data structures, based on the input dataset. Our results\ndemonstrate that as expected, skip lists perform significantly better than balanced binary search\ntrees across all values of n, due to the latter’s necessity of constantly rebalancing the data structure.\nIn fact, skip lists performed almost 4 ×better than BSTs in some cases, e.g., n= 20000. We\nillustrate our results in Figure 8.\n29\n\n(a) Uniform Distribution\n (b)α= 1.25\n(c)α= 1.5\n (d)α= 2\nFigure 7: Query time for synthetic datasets with a uniform distribution and under different αvalues\nof the Zipfian distribution for both classic and augmented skip lists. This figure compares the query\ntime of the classic and augmented skip lists for both uniform distribution and Zipfian distribution at\nvarious αvalues. Similar to insertion, the query time is significantly reduced under different query\nsizes with the implemented augmentation. The performance enhancement is especially pronounced\nfor the high skewness of the dataset.\nD.1.2 AOL Dataset\nThe AOL dataset [ GP06 ] features around 20M web queries collected from 650k users over three\nmonths. The distribution of the queries is shown in Figure 9. The AOL dataset is a less skewed\ndataset than CAIDA with an alpha value of 0.75.\nFigure 9 shows the distribution of the AOL queries with an estimated alpha value of 0.75. The\nAOL dataset resembles more to a slightly skewed uniform distribution with very few highly frequent\nitems, which accounts for a lower improvement as in the case of AOL shown in Figure 10. The\ntotal number of queries for items with higher than 1000 frequency accounts for only 5% of the total\nnumber of queries for the AOL datasets. The learning-augmented skip list still outperforms the\ntraditional skip list on this slightly skewed dataset. This result is also in line with the results from\nthe synthetic data shown in Table 1 where lower alpha values have resulted in a lower speedup\nfactor.\nD.2 KD Trees\nFor KD Trees, first describe our methodology for evaluating our data structure on synthetic data.\nWe then describe our empirical evaluations on real-world datasets.\nThe computer used for KD tree benchmarking is a desktop machine with an Intel Core i9-\n30\n\nFigure 8: BST vs skip list construction times\n14900KF@ 3200MHz, with 64GB RAM, and 2TB of Solid State Drive. The tests were conducted in\nWindows 10 Enterprise, version 10.0.19045 Build 19045.\nD.2.1 Synthetic Datasets with Perfect Knowledge\nFirst, consider a dataset with a Zipfian distribution. In order to construct this dataset, we first\nselect nunique data points in [∆]duniformly. We then generate Zipfian frequencies, fi≈1\n(i+b)a,\nand randomly pair the frequencies to the data points to serve as both the construction and query\nfrequencies. We construct either a traditional KD tree, or our learned KD tree on this dataset.\nThen, we evaluate the performance of querying by sampling the known datapoints with probabilities\ngiven by their Zipfian probabilities.\nIn Table 3, we use a Zipfian distribution with parameters a= 1 and b= 2.7. This data\ndemonstrates that our method outperforms a traditional KD tree, given that the Zipfian distribution\nhas the given parameters. In Figure 11, we vary the Zipfian parameters of our dataset. As expected,\nour learned KD tree performance increases with how skew the distribution is. Moreover, we find\nthat our method outperforms the traditional KD tree on all tested Zipfian distributions.\nD.2.2 Synthetic Datasets with Noisy Knowledge\nIn reality, it is rarely the case that we have perfect knowledge in constructing a model. In order to\nevaluate the performance of our method on noisy data, we create a synthetic dataset with noisy\ntraining information.\n31\n\n(a) AOL data distribution\n (b) Zipfian fit ( α= 0.75)\nFigure 9: AOL datasets distribution characterization. This figure illustrates how the αvalue of 0.75\nis obtained for the AOL dataset. The AOL dataset shows a much smaller αvalue compared to the\nCAIDA dataset so AOL almost resembles a uniform distribution despite very few high-frequency\nnodes. This also explains why the performance of the augmented skip list is close to the classic\nimplementation.\n(a) Insert time on AOL\n (b) Query time on AOL\nFigure 10: Insertion and query time on AOL of classic and augmented skip lists\nAs before, we generate world points and assign them Zipfian weights. When constructing the\ntree, each weight us updated to be Mpi+A, where MandAare drawn from uniform distributions.\nThis new noisy distribution is normalized in order to form a valid probability distribution. Then,\npoints are queried many times with their ground truth Zipfian probabilities. For the fixed Zipfian\ndistribution with a= 5, b= 2, we plot the effects of different ranges of MandAto demonstrate the\neffect noise has in Fig. 4. This figure demonstrates that, even with moderate amounts of noise, our\nmethod still outperforms a traditional KD tree. Moreover, our method still remains on par with the\ntraditional KD tree when significant noise is present, due to our robustness guarantees.\nD.2.3 Real-World Datasets\nIn addition to evaluating our method on synthetic data, we also evaluate results on real-world data.\n32\n\ntype dim const time avg query(s) avg query depth\ntraditional 1 1.267E-01 2.645E-05 1.330E+01\nlearned 1 2.387E-01 2.181E-05 1.086E+01\ntraditional 2 1.266E-01 2.709E-05 1.341E+01\nlearned 2 3.232E-01 2.221E-05 1.086E+01\ntraditional 3 1.253E-01 2.700E-05 1.334E+01\nlearned 3 3.981E-01 2.264E-05 1.097E+01\ntraditional 4 1.185E-01 2.724E-05 1.336E+01\nlearned 4 4.549E-01 2.256E-05 1.089E+01\ntraditional 5 1.300E-01 2.743E-05 1.336E+01\nlearned 5 5.286E-01 2.296E-05 1.096E+01\ntraditional 10 1.277E-01 2.896E-05 1.340E+01\nlearned 10 8.642E-01 2.403E-05 1.091E+01\ntraditional 20 1.295E-01 3.121E-05 1.344E+01\nlearned 20 1.543E+00 2.564E-05 1.085E+01\ntraditional 40 1.361E-01 3.558E-05 1.340E+01\nlearned 40 2.911E+00 2.902E-05 1.078E+01\nTable 3: We construct and query KD trees with our method and with a traditional KD tree on\nsynthetic datasets of various dimensionality. We construct our tree on 10k points, and query 1M\ntimes. We find that, independent of the data dimensionality, our method produces lower average\nquery depths.\nFirst, we consider n-grams in various languages. We test on a pre-processed subset of the Google\nN-Gram dataset [Goo22, Goo12].\nIn order to evaluate our method, we convert an n-gram to a vector in Znwith each entry indexing\nthe words in the n-gram. We construct the learning-augmented and traditional KD trees, and show\nthe performance of lookup with queries weighted by their ground truth frequency in Table 4. In all\ncases, we find that the learning-augmented KD tree outperforms the traditional KD tree at average\nlookup depth.\nAdditionally, we test our method on a dataset of neuron activity, as provided by [ ACL14 ], which\nhas shown to be Zipfian. This dataset consists of vectors in {0,1}30, indicating which of 30 cells fire\nat agiven time. As in their work, we bin in 20ms increments when constructing these vectors. We\nsimilarly ignore the time of observations, and build our learning-augmented KD tree with frequencies\ngiven by the rate of appearance of vectors. We find that, when querying with probabilities equivalent\nto the training distribution, a traditional KD tree has an average query depth of 23.7. Using our\nlearning-augmented KD tree, however, we are able to achieve an average query depth of 14.9.\n33\n\nFigure 11: We generate datasets of 212points in 3-dimentional space, with frequencies given by\na Zipfian distribution with parameters a, b. We then query the tree 214times, with point queries\nselected by the same Zipfian distribution. We repeat this process 32 times, and report the median\nof the average query depth across all runs. We find that our method is able to outperform the\ntraditional KD tree method across all tested Zipfian distributions. Moreover, our performances\nincreases as aincreases and bdecreases, making the Zipfian distribution most skew. Notably, when\na= 0, all points have uniform weights, at which point our method performs equivalently to the\ntraditional KD tree.\n34\n\nDataset Traditional Avg Query Depth Learned Avg Query Depth\n2grams chinese simplified 14.8388 12.1144\n2grams english-fiction 14.216 11.6232\n2grams english 14.7253 11.6647\n2grams french 14.5123 11.5999\n2grams german 13.9473 11.7066\n2grams hebrew 13.5322 12.0007\n2grams italian 14.5231 11.8027\n2grams russian 14.5769 11.8082\n2grams spanish 15.1763 11.6582\n3grams chinese simplified 12.343 11.4211\n3grams english-fiction 13.5264 11.3183\n3grams english 15.002 11.3632\n3grams french 14.9235 11.3529\n3grams german 14.129 11.4285\n3grams hebrew 13.0887 9.6663\n3grams italian 13.3839 11.3616\n3grams russian 15.2475 11.1415\n3grams spanish 15.3855 11.3635\n4grams chinese simplified 11.5823 9.8661\n4grams english-fiction 12.5141 9.7589\n4grams english 13.4464 9.7793\n4grams french 12.6383 9.835\n4grams german 12.3911 9.8765\n4grams hebrew 9.3198 7.4493\n4grams italian 11.3456 9.6834\n4grams russian 13.2406 9.6274\n4grams spanish 12.9712 9.8191\n5grams chinese simplified 11.5373 9.8982\n5grams english-fiction 12.8027 9.8069\n5grams english 12.5607 9.5967\n5grams french 12.402 9.835\n5grams german 11.6912 9.8432\n5grams hebrew 7.097 6.2143\n5grams italian 11.3061 9.7771\n5grams russian 12.885 9.6805\n5grams spanish 12.111 9.8079\nTable 4: We construct traditional and learning-augmented KD trees for n-grams for various languages,\nand of various lengths n. Our method outperforms a traditional KD tree in all cases.\n35",
  "textLength": 90367
}