{
  "paperId": "fac67330a538c6b72b32d18625a8ddc6db48aa19",
  "title": "DLB: Deep Learning Based Load Balancing",
  "pdfPath": "fac67330a538c6b72b32d18625a8ddc6db48aa19.pdf",
  "text": "DLB: Deep Learning Based Load Balancing\nXiaoke Zhux\u0003, Qi Zhangy\u0003, Taining Chengx, Ling Liuz, WeiZhoux\u0003\u0003and Jing Hex\u0003\u0003\nYunnan Universityx, IBM Thomas J. Watson Researchy, Georgia Institute of Technologyz\nAbstract —In this paper, we introduce DLB, a Deep Learning\nbased load Balancing mechanism, to effectively address the data\nskew problem. The key idea of DLB is to replace hash functions\nin the load balancing mechanisms with deep learning models,\nwhich are trained to be able to map different distributions\nof workloads and data to the servers in a uniformed manner.\nWe implemented DLB and deployed it on a practical Cloud\nenvironment using CloudSim. Experimental results using both\nsynthetic and real-world data sets show that compared with\ntraditional hash function based load balancing methods, DLB\nis able to achieve more balanced mappings, especially when the\nworkload is highly skewed.\nIndex Terms —load balancing, consistent hashing, neural\nnetworks, cloudsim\nI. I NTRODUCTION\nWith the development of Cloud computing, companies are\nbecoming increasingly interested in migrating their services\nand data to Cloud platforms, such as AWS [1], IBM\nCloud [2], Google Cloud [3], and Microsoft Azure [4],\non which the effectiveness of load balancing is of great\nimportance. Maintaining a balanced workloads beneﬁts the\ncloud service provider by not only increasing the utilization\nof their resources, but also improving the quality of services.\nCurrently, hash function based load balancing mechanisms are\nthe dominant design, in which hash function based approaches\nare used to determine which server a workload needs to be\nassigned to.\nA hash function is able to generate balanced results when\nthe input data is uniformly distributed. However, the real-world\ndata sets often exhibit remarkable skew. For instance, analysis\nof air trafﬁcs and online human behavior data sets [5], [6]\nrevealed that such data usually follows different power law\ndistributions. When the input data is skewed, the output of\nthe hash function will also be skewed. Therefore, using a\nhash function in a load balancing mechanism can result in\nunbalanced workloads assignment when data skew exists in\nthe input. Even worse, such unbalanced workloads could\nseriously harm the performance of applications and services\nrunning on distributed platforms. Elaheh Gavagsaz and et al.\n[7] demonstrated that traditional join algorithms based on\nMapReduce are not efﬁcient when working with skew data,\nJoanna Berlinska and et al. [8] also revealed that the uneven\ndistribution of the keys might cause imbalance computation\nThis work was supported in part by the National Natural Science Foundation\nof China under Grant 61762089, Grant 61663047, Grant 61863036, Grant\n61762092 and Open Foundation of Key Laboratory in Software Engineering\nof Yunnan Province under Grand 2020SE310\u0003made equal contribution to this work.\u0003\u0003to whom correspondence should address fzwei,hejingg@ynu.edu.cn\n/s48 /s53 /s49/s48 /s49/s53 /s50/s48 /s50/s53 /s51/s48 /s51/s53/s50/s56/s48/s51/s48/s48/s51/s50/s48/s51/s52/s48/s51/s54/s48/s80/s114/s111/s99/s101/s115/s115/s101/s100/s32/s68/s97/s116/s97/s32/s40/s98/s97/s108/s108/s115/s46/s41\n/s66/s105/s110/s32/s73/s68/s32/s66/s75/s68/s82/s32/s72/s65/s83/s72\n/s32/s77/s85/s82/s77/s85/s82/s32/s72/s65/s83/s72\n/s32/s80/s89/s84/s72/s79/s78/s32/s72/s65/s83/s72Fig. 1: Data skew for different hash functions\ncompletion time among different MapReduce tasks, which\neventually prolonged execution of the whole MapReduce job.\nThere are several reasons why hash functions do not\nperform well on skewed data sets. First, the hash function\nwas originally designed to perform fast index [9] (i.e.,\nindexing with O(1) time complexity), compression [10]\n(i.e., compressing a large input in a deterministic way),\ncryptography [11] (i.e., irreversible mapping from inputs\nto outputs) and etc., thus dealing with skewed data was\nnot considered as one of its primary design goals. Second,\nalthough there have been efforts, such as BKDR hash [12],\nMURMUR hash [13], and Python hash [14], to enhance\nthe hash functions to better handle the skewed data, their\neffectiveness are not satisfying. As shown in Figure 1, we\nmanually generate a skew data set under normal distribution\nas the inputs and use the above mentioned three hash functions\nto map these inputs to 32 bins. After that, the bins are sorted\nby the number of inputs assigned to it. Ideally, the lines in this\nﬁgure should be ﬂat, which means different bins are taking a\nsimilar amount of inputs if the hash functions are able to map\nthe input to a uniformed distribution. However, the lines in the\nﬁgure are all in an increasing trend. The numbers of inputs\nbeing assigned to different bins vary from 280 to 360, which\nare signiﬁcantly unbalanced.\nThe availability of big data and the rapid advance of\nAI techniques provide unique opportunities to rethink the\ndesign of load balancing mechanisms by making them perform\nbetter on skew data. The key idea is as follows: instead\nof using a hash function, learned models can be applied to\ndetermine where the inputs should be mapped to the hash\ncircle. Although model training is required beforehand, this\napproach is practical and has several advantages compared\nto traditional hash based methods. First, the capability and\naffordability of collecting large amount of data nowadaysarXiv:1910.08494v4  [cs.DC]  12 Sep 2021\n\nprovides the potential to train data distribution aware models\nfor load balancing mechanisms. Second, the distribution of\ndata collected by a speciﬁc company or organization for a\ngiven task is usually consistent, which is demonstrated by\nanalysis results from [5], [6], [15]. This shows the feasibility\nof using historical information to deal with future workloads.\nThird, when appropriately trained, the output of a learned\nmodel can be uniformly distributed even when the input data\nset is highly skewed.\nIn this paper, we propose DLB, which uses deep learning\nmodels to effectively address the data skew problem in\nexisting load balancing mechanisms. Researchers [16]–[19]\nhave explored the possibility of partially replacing existing\ndata structures and algorithms with deep learning models.\nFor example, Tim Kraska and et al. [16] introduced the\nhash model index, which reduces the total number of hash\nconﬂicts over map data set by learning a CDF(Cumulative\ndistribution function) at a reasonable cost. However, there are\nstill remaining challenges to leverage deep learning models\nto improve the effectiveness of load balancing mechanisms.\nOn the one hand, how to design a neural network that\ncan converge quickly during the training while also being\nable to effectively mapping large volumes of inputs to a\nuniformly distributed space. On the other hand, how to balance\nbetween the complexity and the expressiveness of the model.\nConcretely speaking, a simple neural network can be easily\ntrained, but it will not be able to map large amount of inputs\ninto a uniformly distributed space without incurring signiﬁcant\nconﬂicts. While a complex model can reduce the mapping\nconﬂicts, but it cannot be trained easily due to gradient\ndissipation and explosion problems.\nIn order to solve these challenges, DLB is designed in a way\nthat, instead of using a single end-to-end model, it organizes\na set of models into a hierarchical architecture. In such an\narchitecture, the models are organized in different connected\nlayers. For a speciﬁc input, it will go through one model in\neach layer, while the model in the previous layer speciﬁes\nwhich model in the next layer needs to be invoked. The ﬁnal\noutput will be the position on the hash circle for this input.\nSince the distribution of input data is not guaranteed to stay\nthe same, DLB also continuously monitors the actual load\ndistribution of all the servers to make sure no server becomes\na hotspot. Compared with traditional hash function based load\nbalancing mechanisms, such as Consistent Hashing [20] and\nConsistent Hash with Bound Load [21], DLB is able to map\nthe input data sets to a uniformly distributed space even when\nthey are highly skewed. In addition, compared with a single\nbut complex end-to-end model, a hierarchical design makes\neach model converge more quickly during the training.\nThe main contributions of this paper are as follows:\n\u000fWe designed DLB, a deep learning based load balancing\nmechanism which solves the data skew problem by\nreplacing the hash function with deep learning models.\n\u000fWe implemented DLB and deployed it in a practical\nenvironment using CloudSim [22], which enablesmodeling and simulation of real Cloud computing\nsystems and application provisioning environments.\n\u000fThe effectiveness of DLB is measured by using both\nsynthetic and real-world data sets under different\ndistributions.\nII. R ELATED WORK\nLoad balancing mechanisms are widely used in distributed\ncomputing environment to balance the workloads among\ndifferent servers, and the effectiveness of such mechanisms is\ncritical to the overall performance and service quality of the\ndistributed platforms. Therefore, how to design an effective\nload balancing mechanism has attracted the interest of many\nresearchers. In this section, we introduce related researches in\nthis area, while at the same time, we also discuss the existing\nefforts on trying to use neural network based learned data\nstructures to improve the performance of traditional systems.\nHashing based load balancing. As one of the mainstream\nload balancing mechanisms, Consistent Hashing(CH) [23]\nproposed by Karger and et al. has been widely adopted. Ideally,\nby using a randomized hash function, both balls and bins can\nbe assigned to the hash circle in a uniformed way, so that\ndifferent bins will be able to hold the similar number of balls.\nHowever, it is usually not the case in the real-world due to the\nexistence of data skew in the inputs. There have been many\nefforts to address this issue [24]–[27]. For example, David\nR. Karger and el al. [20] tried to enable CH to generate more\nbalanced results by using virtual bins which are replicas of real\nbins in hash space, and one real host can be correspondent\nto several virtual bins. The authors showed that the overall\nload balancing performance could be improved accordingly.\nTo further address the data skew problem in load balancing,\nJohan Lamping and Eric Veach proposed jump Consistent\nHashing [25], which works by computing when its output\nchanges as the number of bins varies. In this approach, the\nhash value of a ball is not randomly generated, but acquired\naccording to the probability determined by the number of\nexisting bins. Also, whenever a new ball is added, the hash\nvalue of the existing balls needs to be recomputed according to\nthe a predeﬁned probability. Roberto Grossi and et al. designed\nRound-Hashing [26]. Thaler and Ravishankar proposed [24]\nRendezvous hashing algorithm, for a given ball qandnbins,\nit applies a hash function to the all the pairs fq;pig, in which\ni2f1:::ng, and assigns the ball to the bin that can lead to\nlargest hashing result.\nNeural network based learned data structures. This\nthread of research explores the potential of utilizing the\nneural network based learned data structures to improve the\nperformance of traditional systems [16], [17], [19], [28].\nAmong them, Tim Kraska and et al. proposed learned B-tree,\nlearned hash, and learned bloom ﬁlter structure [16] to improve\nthe indexing performance of traditional structure by learning\nthe distribution from the historical data. Xiang and et al. [17]\nproposed a LSTM-based inverted index structure.\nDifferent from the above-mentioned researches, instead\nof improving the effectiveness of traditional hash functions,\n\nour DLB approach takes the advantages of both Consistent\nHashing and deep neural network. In DLB, a deep learning\nmodel based on the historical data is trained, and then used to\nmap the newly coming data to a uniformly distributed space\neven when such data is skewed.\nIII. DLB: D EEP LEARNING BASED LOAD BALANCING\nIn this section, we discuss the details of DLB design. The\ngoal of load balancing is to uniformly distribute different\nworkloads on multiple servers so that no server will become\nthe hotspot. A hash function, being the core building block\nin most of the existing load balancing mechanisms, can be\nconsidered as a black box that takes an input and maps it\nto a position on the hash circle. Therefore, we propose a\nDeep L earning based load B alancing mechanism named DLB,\nwhich replaces the hash functions with deep learning models\nto fulﬁll the same mapping task. We observe that this approach\nis able to work well on skew data and provide more balanced\nworkload distribution compared with traditional hash function\nbased load balancing mechanisms.\nA. Design\n1) Hierarchical models: As discussed earlier, a hash\nfunction in a load balancing mechanism can be replaced by a\ndeep learning model, and a well trained model can generate\nuniformly distributed outputs even when the inputs are highly\nskewed. A natural question to ask is which model should be\nused. In load balancing mechanisms such as CH, the inputs\nare usually mapped into a large space (e.g., 232) to avoid\nconﬂicts. If we consider each slot in this output space as a\nclass, what the model needs to achieve is actually classifying\neach input into one of these different classes. This actually\nturns the mapping task into a classiﬁcation task. Since the\nspace of this classiﬁcation so large, training a single model\nfor such a task will be really difﬁcult.\nTherefore, we propose an architecture of hierarchical\nmodels in DLB to address this problem. As shown in Figure\n2, instead of using one single model, multiple models are\ninvolved to solve this classiﬁcation problem. The models are\norganized into a hierarchical structure with different connected\nlayers. To ﬁnd out the position that an input should be mapped\nto on the hash circle, each input will need to go through a\nmodel in each layer. The whole mapping procedure can be\ndivided into 4 stages, and details of each stage are described\nas follows:\nInput stage. Various formatted features can be observed as\nthe inputs of load balancing mechanisms when a hash function\nis used, being it an ID string of a user or a MD5 value of a\nﬁle. However, these features need to be converted so that they\ncan be consumed by a neural network model. Therefore, the\ngoal of this stage is to pre-process the input data, such as\nconverting strings or numerical data into vectors, so that they\ncan be directly used as inputs of a neural network model.\nDisperse stage. The main strategy used in this stage\nisdivide and conquer . Concretely speaking, the disperse\nstage consists of multiple models which are organized in ahierarchical architecture(i.e., a tree structure). All the models\nin this architecture work collaboratively to ﬁgure out which\nposition on the hash circle a given input should be placed.\nSince the space of the ﬁnal hash circle is usually very large, the\nmotivation of this design is to divide a complex classiﬁcation\ntask, which is supposed to deal with a large output space,\ninto multiple smaller tasks. In this way, the original complex\nclassiﬁcation problem can be conquered more effectively by\nsolving these smaller problems. In other words, with such a\nhierarchical architecture, each model only needs to tackle a\nmuch simpler classiﬁcation problem with only a subset of the\nwhole output space. As shown in Figure 2, the models in this\nstage are split into multiple layers. The root model(i.e. the\none on the left most in Figure 2) takes the input data set and\ndetermines which model in the next layer needs to be invoked,\nwhile the models in the other layers of the disperse stage go\nthrough the same procedure using their corresponding assigned\ninput.\nMapping stage. Models involved in this stage are located in\nthe last layer of the hierarchical architecture. Different from\nthe models in the disperse stage that select which model in\nthe next layer needs to be used, each model in the mapping\nstage is responsible for generating the position of a given input\nin its sub-circle. Different models in the mapping stage are\ncorrespondent to different sub-circles, which are actually areas\non the hash circle, while they collectively cover the whole hash\ncircle.\nJoin stage. Since the output of each model in the mapping\nstage is a position on each model’s own sub-circle, another\nlayer is needed to translate such a local position on a sub-circle\ninto a global position on the hash circle. In order to create\nthe continue hash circle, sub-circle of different mapping\nstage models are connected sequentially, thus the ﬁnal global\npositionPosqiof inputqion the hash circle is established by\nEq 1.\nPosqi=\u0016i+ (modelid\u00001)t (1)\nmodelidis the ID of the model in the mapping stage\nstarting from 1.\n2) Server Management: In traditional load balancing\nmechanisms such as CH, both workloads and servers are\nmapped to the hash circle by hash functions, and a workload\nis assigned to its clockwise closest server. In DLB, a deep\nlearning model is used to map a workload to a position on\nthe hash circle, while a deterministic approach is used to map\nthe servers. Although server mapping can also be done by\nusing learned models, it is not necessary since the number\nof servers is usually much smaller than that of the workloads\nand a deterministic approach is good enough to evenly map\nthe servers to the hash circle.\nSimilar to traditional load balancing mechanisms, DLB\nassigns a workload to a server in a clockwise manner. Since\nDLB is able to uniformly map the workloads to the hash circle,\nusing a deterministic server mapping approach can achieve\nwell balanced workload distribution. Concretely speaking,\nwhen a new server is added, DLB will add the server to\n\nInput Stage Join Stage Disperse Stage\nModel 1.1Mapping Stage\nPosKeys\nIP Addresses\nFile ID\nObjects\n…local-pos232-1\n…\n……\n……\n……Split layer 1…Split layer 2…\nModel 2.1\nModel 2.2\nModel 2.3Model (ɸ+1).1\n… Model (ɸ+1).2\nModel (ɸ+1).3\n……Fig. 2: The hierarchical model architecture in DLB\na place such that this server can evenly divided the largest\nsub-circle on the hash circle. When an existing server becomes\nunavailable, the workloads on this server will be reassigned to\nits clockwise next server. Similar to Consistent Hashing with\nbounded load(CHBL), each server in DLB has a load threshold\n\u000f. A new workload can be assigned to a server only if the load\nof this server will not exceed \u000f. Otherwise, other servers need\nto be considered.\nB. Training\nIn this sub-section, we discuss the considerations of how\nto train the hierarchical models mentioned above from two\naspects.\nFirst, how to label the training data. Given historical cluster\naccess data, to make sure that the models will not generate\nskew output when the training data is skewed. Meanwhile,\nSince the hierarchical architecture includes multiple models\nin different layers, for each input, it needs to be labeled for\neach model. We discuss the labeling process for DLB from two\naspects: creating labels used in the mapping stage models as\nwell as in the disperse stage models. The difference between\nthese two types of labels is that the former one represents\npositions on a hash circle, while the latter one is correspondent\nto the ID of the model in the next layer.\nThe method to create labels for DLB is depicted in\nAlgorithm 1. tag\u001e(ki)generates the label of kiin layer\u001e.\nA formal description of tag\u001e(ki)is shown in Eq 2, in which\nC\u001erepresents the the number of models in the \u001eth layer.\ntag\u001e(ki) =j j2[1 :C\u001e+1] (2)\nsubject to:T\nC\u001e+1\u0003(j)\u0014labelm\ni\u0014T\nC\u001e+1\u0003(j+ 1)\nSecond, we also describe what loss function is used in the\ntraining process. The loss value used in the training is deﬁned\nasLoss =P\n\u001e2(1;\b+1)P\nn2(1;m\u001e)(On\n\u001e\u0000labeln\n\u001e)2, which is\nthe sum of the loss value of each model in the hierarchical\narchitecture. We refer the output of n-th model in the \u001e-layer\nto asOn\n\u001e,labeln\n\u001eto represent the corresponding labels, while\nm\u001eas the number of models in the \u001eth layer.Algorithm 1 Labeling DLB training data\nInput:K- key list of balls\nInput:T- number of positions on the hash circle\nInput: \b- number of layers in disperse stage\nInput:labelsi- labels for the models in the ith layer\nInput:indexof (ki;K)- index of the element kiin listK\nInput:tag\u001e(ki)- label ofkifor the model in \u001eth layer.\nInitialize:Labelsi fg (i2(1;\b + 1))\n1:Ks Sort(K)\n2:forkiinKdo//Create labels for models in mapping stage\n3:label indexof (ki;K)\u0003(T=sizeof (K))\n4:labels\b+1 Labels\b+1[label\n5:end for\n6:for\u001ein\bdo//Create labels for models in disperse stage\n7: forkiinKdo\n8:label tag\u001e(ki)\n9:labels\u001e Labels\u001e[label\n10: end for\n11:end for\n12:returnflabels1;:::;labels\b+1g\nIV. E VALUATION\nIn this section, we compare the effectiveness of load\nbalancing between DLB and the following widely used and\nclassical load balancing approaches:\n\u000fConsistent Hashing(CH). CH hashes the balls and bins\ninto a unit circle, and uses the hash values to create a\ncircular order of balls and bins. The palcement decision\nare all based on the relatevie location among balls and\nbins.\n\u000fConsistent Hashing with Bounded Load(CHBL). CHBL\nis similar to CH, except that it uses a parameter to try\nto keep the balls uniformly distributed among different\nbins.\nIn our design each sub model of DLB has 3 fully connected\nlayer, and each layer has 8, 32, 64 neuros respectively. We\nuse Adam [29] with learning rate of 0:01to train all the\nsub models. For CH and CHBL, we also combine them with\ndifferent hash implementations, such as BKDR Hash [12],\nPython Hash [14], and Murmur Hash [13].\n\n(a) Log-normal distribution\n(b) Normal distribution\n(c) Uniform distribution\nFig. 3: Compare the effectiveness of different load balancing mechanisms in a practical Cloud environment created by CloudSim.\nA. Setup\nIn this subsection, we describe the experimental setup,\nincluding the hardware and software environment, as well as\nthe data sets and metrics used throughout the measurements.\n1) Environment: The experiments are carried on a machine\nwith 64GB main memory and one 2.6GHZ Intel(R) i7\nprocessors. Each test is run 10 times and the median of the\nresults are shown in this section.\n2) Data sets: In order to measure how different\ndistributions of the input data set can affect the effectiveness\nof DLB, the synthetic data sets used in Section IV-B are\ngenerated under three most commonly observed distributions:\nuniform distribution, normal distribution, and log-normal\ndistribution. Each distribution has two data sets, one for\ntesting and the other for training. Each data set consists of\n20;000;000 balls while each ball is a double-precision digit\nwhich represents a key of client workload in load balancing\nscenario. A 4TB data set collected from a radio monitoring\ncenter is also used in our experiments. This data set consists\nof100;000records, and each record has 13 features.\n3) Measurement metrics: The effectiveness of a load\nbalancing mechanism is measured by the standard deviation\namong the load of different bins on the hash circle(std).\nTherefore, the smaller the std value is, the more effective\nthe load balancing mechanism is. Formally, the stdcan be\ncalculated as std=qPn\nj=1(loadj\u0000m\nn)2\nn, in which,loadjrefers\nto the number of balls assigned to bin j, whilemandnare\nthe total number of balls and bins respectively.B. Analysis\n1) CloudSim based evaluation: In this set of experiments,\nwe deploy DLB, CH, and CHBL on a practical Cloud\nenvironment created by CloudSim [22], and compare their\neffectiveness to balance the workloads among a number of\nservers. In the simulated Cloud environment, 64 machines\nare hosted within a single data center. Each machine has 4\nthreads, 1 GB memory, 10 GB of storage, and 1Gbps network\nbandwidth. On the client side, 8192 workloads(cloudlets)\nwith different distributions are created. The distribution is\nbased on the ID of each workload, which is also the key\nused for mapping. Each server can run at most 4 workloads\nsimultaneously. When wait list of a server is full and an\nadditional workload is assigned, this workload needs to be\nloaded by the next spare server in the hash circle. All the\nworkloads are submitted to the data center in one batch, and\neach workload takes 20 seconds CPU time to ﬁnish.\nFig 3 shows the actual ﬁnishing time of each workload when\ndifferent load balancing mechanisms are used. The x-axis\nrepresents the ID of each workload while the y-axis shows the\nactual ﬁnishing time. It can be observed that, no matter what\nthe workload distribution is, the heights of lines in ﬁgures\ncorresponding to DLB is much lower and smoother. This\nmeans that, when DLB is used, workloads can ﬁnish in a\nshorter and more balanced time. While in CH and CHBL\ncases, some workloads takes much longer time to ﬁnish than\nthe other ones. This is due to the imbalanced assignment of\nworkloads, which causes some servers to become the hotspots.\nTherefore, it takes much longer for workloads on these servers\n\n/s67/s72/s32/s40/s80/s89/s84/s72/s79/s78/s58/s58/s72/s65/s83/s72/s41/s67/s72/s32/s40/s66/s75/s68/s82/s32/s72/s65/s83/s72/s41\n/s67/s72/s32/s40/s77/s85/s82/s77/s85/s82/s32/s72/s65/s83/s72/s41\n/s67/s72/s66/s76/s32/s40/s80/s89/s84/s72/s79/s78/s58/s58/s72/s65/s83/s72/s41/s67/s72/s66/s76/s32/s40/s66/s75/s68/s82/s32/s72/s65/s83/s72/s41\n/s67/s72/s66/s76/s32/s40/s77/s85/s82/s77/s85/s82/s32/s72/s65/s83/s72/s41/s68/s76/s66/s48/s49/s50/s51/s52/s53/s54/s55/s56/s83/s116/s97/s110/s100/s97/s114/s100/s32/s100/s101/s118/s105/s97/s116/s105/s111/s110/s32/s111/s102/s32/s98/s105/s110/s32/s108/s111/s97/s100/s40/s120/s49/s48/s53\n/s41(a) Log-normal\n/s67/s72/s32/s40/s66/s75/s68/s82/s32/s72/s65/s83/s72/s41\n/s67/s72/s32/s40/s77/s85/s82/s77/s85/s82/s32/s72/s65/s83/s72/s41\n/s67/s72/s66/s76/s32/s40/s80/s89/s84/s72/s79/s78/s58/s58/s72/s65/s83/s72/s41/s67/s72/s66/s76/s32/s40/s66/s75/s68/s82/s32/s72/s65/s83/s72/s41\n/s67/s72/s66/s76/s32/s40/s77/s85/s82/s77/s85/s82/s32/s72/s65/s83/s72/s41/s68/s76/s66/s48/s49/s50/s51/s52/s53/s54/s55/s56/s83/s116/s97/s110/s100/s97/s114/s100/s32/s100/s101/s118/s105/s97/s116/s105/s111/s110/s32/s111/s102/s32/s98/s105/s110/s32/s108/s111/s97/s100/s40/s120/s49/s48/s53\n/s41 (b) Normal\n/s67/s72/s32/s40/s80/s89/s84/s72/s79/s78/s58/s58/s72/s65/s83/s72/s41/s67/s72/s32/s40/s66/s75/s68/s82/s32/s72/s65/s83/s72/s41\n/s67/s72/s32/s40/s77/s85/s82/s77/s85/s82/s32/s72/s65/s83/s72/s41\n/s67/s72/s66/s76/s32/s40/s80/s89/s84/s72/s79/s78/s32/s72/s65/s83/s72/s41/s67/s72/s66/s76/s32/s40/s66/s75/s68/s82/s32/s72/s65/s83/s72/s41\n/s67/s72/s66/s76/s32/s40/s77/s85/s82/s77/s85/s82/s32/s72/s65/s83/s72/s41/s68/s76/s66/s48/s50/s52/s54/s56/s49/s48/s83/s116/s97/s110/s100/s97/s114/s100/s32/s100/s101/s118/s105/s97/s116/s105/s111/s110/s32/s111/s102/s32/s98/s105/s110/s32/s108/s111/s97/s100/s40/s120/s49/s48/s53\n/s41\n(c) Uniform\n/s67/s72/s32/s40/s80/s89/s84/s72/s79/s78/s58/s58/s72/s65/s83/s72/s41/s67/s72/s32/s40/s66/s75/s68/s82/s32/s72/s65/s83/s72/s41\n/s67/s72/s32/s40/s77/s85/s82/s77/s85/s82/s32/s72/s65/s83/s72/s41\n/s67/s72/s66/s76/s32/s40/s80/s89/s84/s72/s79/s78/s58/s58/s72/s65/s83/s72/s41/s67/s72/s66/s76/s32/s40/s66/s75/s68/s82/s32/s72/s65/s83/s72/s41\n/s67/s72/s66/s76/s32/s40/s77/s85/s82/s77/s85/s82/s32/s72/s65/s83/s72/s41/s68/s76/s66/s48/s49/s50/s51/s52/s53/s83/s116/s97/s110/s100/s97/s114/s100/s32/s100/s101/s118/s105/s97/s116/s105/s111/s110/s32/s111/s102/s32/s98/s105/s110/s32/s108/s111/s97/s100/s40/s120/s49/s48/s50\n/s41 (d) Radio monitoring data\nFig. 4: Standard deviation of bin load using different load\nbalancing mechanisms and distributions of input data sets\nto ﬁnish. Considering the exact running time of the longest\njob (i.e., max job duration) in each scenario, DLB is able to\nreduce such time by 61:3%over CH and 52% over CHBL\nunder log-normal distribution, 33% over both CH and CHBL\nunder normal distribution, 60:7%and53:68% when compared\nwith CH and CHBL respectively under uniformed distribution.\n2) Load Balancing: Figure 4 compares the stdof DLB\nwith CH and CHBL based methods. The std is used to\nreﬂect how much an actual distribution of the balls on the\nhash circle deviates from an idea uniform distribution. For\neach experiment, we collect the average result as well as the\ndistribution of 10 runs and plot them in Figure 4. It can be\nobserved that compared with other methods, DLB has the\nlowest value of stdregardless of the data distributions. For\nexample, when the real-world data set is used, average std\nvalue of DLB for the 10 runs is 78, while that of other\nmethods, such as CH(wiht Python Hash) and CH(with BKDR\nHash) are 337 and 299 respectively, which are 3.32x and 2.83x\nlarger than that of DLB.\nV. C ONCLUSIONS\nExisting hash function based load balancing mechanisms\ncannot perform well when the input data is skewed. In\nthis paper, we proposed DLB, a D eep L earning based load\nBalancing mechanism, to address this problem. DLB replaces\nthe hash functions in traditional load balancing mechanisms\nwith deep learning models. Given the constant time of\nmodel inferencing, using a learned model does not introduce\nadditional runtime overhead compared with using a hashfunction. We implemented DLB and deployed it on a practical\nCloud environment using CloudSim. Experimental results\nshow that, compared to traditional hash function based load\nbalancing mechanisms, DLB is able to achieve more balanced\nand stable results even when the input data is skewed.\nREFERENCES\n[1] EC Amazon. Amazon web services. Available in: http://aws. amazon.\ncom/es/ec2/(November 2012) , page 39, 2015.\n[2] Ibm cloud. https://www.ibm.com/cloud,\n[3] Google cloud. https://cloud.google.com/.\n[4] Microsoft azure. https://azure.microsoft.com/en-us/.\n[5] Bin Jiang and Tao Jia. Exploring human mobility patterns based on\nlocation information of us ﬂights. arXiv preprint arXiv:1104.4578 , 2011.\n[6] Filippo Radicchi. Human activity in the web. Physical Review E ,\n80(2):026118, 2009.\n[7] Elaheh Gavagsaz, Ali Rezaee, and Hamid Haj Seyyed Javadi. Load\nbalancing in join algorithms for skewed data in mapreduce systems.\nThe Journal of Supercomputing , 75(1):228–254, 2019.\n[8] Joanna Berlinska and Maciej Drozdowski. Comparing load-balancing\nalgorithms for mapreduce under zipﬁan data skews. Parallel Computing ,\n72:14–28, 2018.\n[9] ThomasH.Cormen. . . [etal. Introduction to algorithms . 2002.\n[10] Shoichi Hirose and et al.\n[11] Michael Coles and Rodney Landrum. Asymmetric Encryption . 2009.\n[12] Arash Partow. Bkdr hash in ”the general hash functions library”, 2020.\n[13] Austin Appleby. Murmurhash3 on github”, 2020.\n[14] Google Inc. The python standard library, 2020.\n[15] Dirk Brockmann, Lars Hufnagel, and Theo Geisel. The scaling laws of\nhuman travel. Nature , 439(7075):462, 2006.\n[16] Tim Kraska et al. The case for learned index structures. In Proceedings\nof the 2018 International Conference on Management of Data, SIGMOD\nConference 2018, Houston, TX, USA, June 10-15, 2018 , 2018.\n[17] Wenkun Xiang and et al. Pavo: A rnn-based learned inverted index,\nsupervised or unsupervised? IEEE Access , 7:293–303, 2019.\n[18] Tim Kraska and et al. Sagedb: A learned database system. In CIDR\n2019, 9th Biennial Conference on Innovative Data Systems Research,\nAsilomar, CA, USA, January 13-16, 2019, Online Proceedings , 2019.\n[19] Alex Galakatos and et al. Fiting-tree: A data-aware index structure. In\nProceedings of the 2019 International Conference on Management of\nData, SIGMOD Conference 2019, Amsterdam, The Netherlands, June\n30 - July 5, 2019. , pages 1189–1206, 2019.\n[20] David R. Karger and et al. Consistent hashing and random trees:\nDistributed caching protocols for relieving hot spots on the world wide\nweb. In Proceedings of the Twenty-Ninth Annual ACM Symposium on\nthe Theory of Computing, El Paso, Texas, USA, May 4-6, 1997 , pages\n654–663, 1997.\n[21] Vahab S. Mirrokni and et al. Consistent hashing with bounded loads.\nInProceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on\nDiscrete Algorithms, SODA 2018, New Orleans, LA, USA, January 7-10,\n2018 , pages 587–604, 2018.\n[22] Rodrigo N Calheiros and et al. Cloudsim: a toolkit for modeling and\nsimulation of cloud computing environments and evaluation of resource\nprovisioning algorithms. Software: Practice and experience , 41(1), 2011.\n[23] David R. Karger and Matthias Ruhl. Simple efﬁcient load-balancing\nalgorithms for peer-to-peer systems. Theory Comput. Syst. , 39(6).\n[24] David Thaler and Chinya V . Ravishankar. Using name-based mappings\nto increase hit rates. IEEE/ACM Trans. Netw. , 6(1):1–14, 1998.\n[25] John Lamping and Eric Veach. A fast, minimal memory, consistent hash\nalgorithm. CoRR , abs/1406.2294, 2014.\n[26] Roberto Grossi and Luca Versari. Round-hashing for data storage:\nDistributed servers and external-memory tables. In 26th Annual\nEuropean Symposium on Algorithms, ESA 2018, August 20-22, 2018,\nHelsinki, Finland , pages 43:1–43:14, 2018.\n[27] Wei Wang and Chinya V . Ravishankar. Hash-based virtual hierarchies\nfor scalable location service in mobile ad-hoc networks. MONET ,\n14(5):625–637, 2009.\n[28] Xiaoke Zhu, Taining Cheng, Qi Zhang, Ling Liu, Jing He, Shaowen Yao,\nand Wei Zhou. Nn-sort: Neural network based data distribution-aware\nsorting. arXiv preprint arXiv:1907.08817 , 2019.\n[29] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980 , 2014.",
  "textLength": 33272
}