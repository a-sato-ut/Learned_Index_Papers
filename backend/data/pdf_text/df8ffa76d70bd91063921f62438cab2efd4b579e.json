{
  "paperId": "df8ffa76d70bd91063921f62438cab2efd4b579e",
  "title": "Accelerating Spatio-Textual Queries with Learned Indices",
  "pdfPath": "df8ffa76d70bd91063921f62438cab2efd4b579e.pdf",
  "text": "ACCELERATING SPATIO -TEXTUAL QUERIES WITH LEARNED\nINDICES\nGeorgios Chatzigeorgakidis1, Kostas Patroumpas2, Dimitrios Skoutas3, and Spiros Athanasiou4\n1IMSI, Athena R.C., Greece, email: gchatzi@athenarc.gr\n2IMSI, Athena R.C., Greece, email: kpatro@athenarc.gr\n3IMSI, Athena R.C., Greece, email: dskoutas@athenarc.gr\n4IMSI, Athena R.C., Greece, email: spathan@athenarc.gr\nABSTRACT\nEfficiently computing spatio-textual queries has become increasingly important in various applications\nthat need to quickly retrieve geolocated entities associated with textual information, such as in\nlocation-based services and social networks. To accelerate such queries, several works have proposed\ncombining spatial and textual indices into hybrid index structures. Recently, the novel idea of\nreplacing traditional indices with ML models has attracted a lot of attention. This includes works\non learned spatial indices, where the main challenge is to address the lack of a total ordering among\nobjects in a multidimensional space. In this work, we investigate how to extend this novel type\nof index design to the case of spatio-textual data. We study different design choices, based on\neither loose or tight coupling between the spatial and textual part, as well as a hybrid index that\ncombines a traditional and a learned component. We also perform an experimental evaluation using\nseveral real-world datasets to assess the potential benefits of using a learned index for evaluating\nspatio-textual queries.\n1 Introduction\nThe ubiquity of mobile devices, along with the proliferation of location-based services and geosocial networks, has\nset off an unprecedented generation of geotagged content. Efficiently querying and exploring spatio-textual data has\nbecome increasingly important for various applications. Typically, query execution must be very fast in order to enable\ninteractive response times. To address this need, several spatio-textual indices have been proposed over the past decade.\nThese are hybrid structures that combine a spatial index, such as grid or R-tree, with a textual part, such as bitmap or\ninverted file, to allow pruning the search space using both criteria.\nRecently, the novel idea of thinking of indices as ML models has been proposed [ 14]. According to it, an index can be\nformulated as a function fthat maps an input key to a set of candidate objects (e.g., stored in a page on disk). Naturally,\nthis has attracted a lot of interest, as it opens up a fundamentally new approach to the decades old problem of index\ndesign. Initial works focused on designing learned indices for one-dimensional data, leveraging the total ordering that\nexists among such objects.\nFollowing this novel line of research, learned spatial indices have been recently introduced [ 8,18], extending the\nconcept of learned indices from 1-dimensional data to two or more dimensions. Coping with more than one dimension\nraises additional challenges, most importantly the lack of a total ordering among objects. This is typically addressed\nby employing a space-filling curve (SFC) to impose an ordering. However, the higher the dimensionality, the less\nthe accuracy of the resulting ordering, which also means a higher error for the ML model that tries to learn the data\ndistribution based on it. Consequently, learned indices over multidimensional data typically focus on a low number of\ndimensions.\nMany spatial objects, such as Points of Interest or geotagged posts in social media, are described by some textual\ninformation, which is or can be represented as a set of keywords . Spatio-textual queries allow retrieving such objects\nbased on both spatial and textual criteria. Perhaps the two most common types of such queries are Boolean Window\nQueries (BWQ) and Boolean k-nearest neighbor Queries (BkQ). A BWQ retrieves objects located within a rectangular\nspatial region that match all query keywords. A B kQ retrieves the knearest objects to the query location that also match\nall given keywords.arXiv:2312.09864v1  [cs.DS]  15 Dec 2023\n\n(a) A Boolean Window Query.\n (b) A Boolean kNN Query.\nFigure 1: Boolean spatio-textual queries.\nFigure 1 illustrates an example involving ten spatio-textual objects. Each object oiis associated with a location and a\nset of keywords. Figure 1a shows a BWQ Q= (W,⟨pizza, bar ⟩) to fetch all objects inside a rectangular region W\n(shown in blue) that contain the keywords pizza andbar. Figure 1b shows a B kQQ= (q,⟨pizza, bar ⟩, 1), which seeks\nthe closest object to location qtagged with pizza andbar. In both queries, the result is object o2.\nTo accelerate such queries, several spatio-textual indices have been proposed over the past decade. These are hybrid\nstructures that combine a spatial access method (e.g., grid, R-tree, quadtree, k-d tree) with a textual one (e.g., bitmap,\ninverted file). Proposed variants differ on the type of spatial and textual index they are based on, the type of coupling\n(i.e., loose vstight), and the type of queries they support. A typical example is the IR2-Tree [ 11], which extends the\nwidely used spatial index R∗-Tree [ 2] by augmenting its nodes with textual information in the form of signatures [ 10].\nA question that naturally arises is whether and how the evaluation of spatio-textual queries can benefit from the use\nof ML models, as has been suggested for one-dimensional or two-dimensional data. In this work, we investigate\nthis question by designing and evaluating hybrid spatio-textual indices that employ a learned component. Although\nsome suggestions for learned indices over strings have been proposed, their benefits are not clear [ 14]. Thus, for\nthe learned part, we focus on the spatial component, in particular using the current state-of-the-art learned spatial\nindex RSMI [ 8]. RSMI has been shown to be able to speed up spatial queries by an order of magnitude compared to\ntraditional query processing based on R-trees. Motivated by this, we investigate how to extend RSMI towards a learned\nhybrid spatio-textual index , and we study the implications of the learned part and the textual part to the performance\nof the resulting index structure. To the best of our knowledge, this is the first work to employ learned indices over\nspatio-textual data.\nThe contributions of our work can be summarized as follows:\n• We derive a baseline learned spatio-textual index by loosely coupling RSMI with an inverted index to enable\ntextual pruning on the leaf nodes.\n•We suggest a tightly coupled index that augments RSMI by associating both internal and leaf models with\ntextual information in the form of bitmaps.\n• We design a hybrid index that combines both a learned part (RSMI) and a traditional part (IR2-Tree).\n•We present an alternative partitioning strategy that takes into account the textual information besides the spatial\none.\n•We empirically evaluate the performance of proposed learned index variants against real-world datasets and\nalso compare them with traditional indices to identify potential benefits.\nThe remainder of this paper is organized as follows: Section 2 reviews related work. In Section 3 we formulate the\nproblem. The methods under examination are presented in Section 4, followed by an extensive experimental evaluation\nin Section 5. Finally, Section 6 concludes the paper.\n2 Background and Related Work\nIn this section, we briefly review related work on spatio-textual indices and learned indices.\n2\n\nSpatio-Textual Indices. Spatio-textual indices are hybrid index structures that combine a spatial and a textual\ncomponent so that pruning on both criteria can be performed during query processing. Several such indices have been\nproposed in the literature (see surveys in [ 7,4,6]). The spatial indexing part can be based on R-trees (e.g., IF-R∗-Tree\nand R∗-Tree-IF [ 27], KR∗-Tree [ 12], IR2-Tree [ 11], IR-Tree [ 23], SKI [ 3], S2I [ 20], WIBR-Tree [ 24]), Quad-tree (e.g.,\nI3[26], ILQ [ 25]), grid (e.g., ST and TS [ 21], SKIF [ 13]) or Space-filling curve (e.g., SF2I [ 5], SFC-QUAD [ 7]). For\nthe textual part, typically a signature (e.g., a bitmap) or an inverted file is used to indicate the presence of keywords in\nthe objects being indexed. In our approach, we employ the IR2-tree [ 11], which is a representative spatio-textual index\nthat can answer both Boolean window (BWQ) and kNN (B kQ) queries. It is based on the R∗-Tree [ 2], where each node\nis enhanced with a signature indicating which keywords exist in the objects corresponding to that node.\nLearned Indices. Recently, the novel idea of learned indices has been proposed [ 14]. Such indices rely on ML models\nto predict the physical location of an object in memory or on disk. The key idea is to use a machine learning model,\nwhich may range from a deep neural network to a simple regression model, to learn the cumulative distribution function\n(CDF) of object keys in a sorted array. Thus, most of these indices are proposed for 1-dimensional data, where a natural\nordering of the objects exists [16].\nNevertheless, several multidimensional learned indices have also been proposed to handle data of two or more\ndimensions, overcoming the lack of an inherent total ordering [ 1,17,9]. These often rely on a space-filling curve to\nmap the data to a single dimension, so that a total ordering can be imposed. For instance, the z-order model (ZM)\nindex [ 22] combines the z-order space-filling curve with a model to learn the distribution of the data. LISA [ 15] is a\nlearned index structure for spatial data, which consists of a function that maps spatial keys into 1-D values, a function\nthat partitions the mapped space, and a series of local models that organize partitions into pages. In this paper, we use\nthe Recursive Spatial Model Index (RSMI) [ 18] as the basis for designing a learned spatio-textual index, hence we\ndescribe RSMI in more detail below.\nRSMI [ 18] relies on a z-order space-filling curve to map the 2-D spatial points to a single dimension, where the resulting\nCDF of the data is learned. To avoid large gaps in this CDF, it first transforms the original point coordinates to a rank\nspace, following the same idea that has been proposed as an R-Tree packing strategy [ 19]. Hence, the z-order curve is\napplied to the rank space instead of the original one. Then, RSMI employs a feed forward neural network to learn the\nresulting CDF. The maximum error ϵof the model is also calculated and is used at query time to define a search range\naround the predicted location so that results are not missed.\nWhen indexing a large collection of spatial objects, it is often not feasible for a single model to learn the entire CDF\nwith sufficient accuracy. To overcome this, RSMI recursively partitions the space and creates a hierarchy of models, so\nthat each model only learns a fraction of the entire CDF. The partitioning is a non-regular grid that follows the data\ndistribution, assuming a maximum allowed partition size S. At each level of the hierarchy, the partitions are ordered\nbased on a z-order curve calculated over their centroids. For each partition, a separate model is trained. At query time,\neach model predicts which of its children nodes should be visited next, thus traversing the hierarchy from the root to the\nleaf nodes. At the bottom level, each leaf model predicts a range of blocks to be searched, also taking into account the\nmodel error ϵ.\n3 Problem Definition\nIn this section, we introduce the main definitions and notions used throughout the paper. The basic notations are listed\nin Table 1.\nAssume a collection Dof spatio-textual objects, which are formally defined as follows.\nDefinition 1 (Spatio-Textual Object) A spatio-textual object ois represented by a pair (o.s, o.T ), where o.sis a point\nin a 2-dimensional Euclidean space and o.Tis a set of keywords.\nWe consider the following two basic types of spatio-textual queries, which are commonly studied in the literature [ 7,4,6].\nDefinition 2 (Boolean Window Query (BWQ)) A boolean window spatio-textual query is defined as BWQ =\n(W,T), where Wis a rectangular spatial area (window), and Tis a set of keywords. Given a collection Dof\nspatio-textual objects, the result set RofBWQ comprises all objects that lie within Wand contain all the keywords in\nT, i.e., R={o∈ D | within (o.s,W)∧ T ⊆ o.T}.\nIf the sequel, for purposes of presentation, we interchangeably denote a spatial window Wwith the pair (ql, qh)of its\nlower left and upper right corners.\n3\n\nTable 1: Basic notations\nD dataset of spatio-textual objects\no.s location of spatio-textual object o∈ D\no.T set of keywords associated to spatio-textual object o∈ D\nT set of query keywords\nW spatial query rectangle (window) in BWQ\nql, qhlower left and upper right corner of window Win BWQ\nmbr Minimum Bounding Rectangle over the input location(s)\nk number of nearest qualifying objects to retrieve in B kQ\nℓ side length of window used in evaluating B kQ\nR set of query results\nS maximum size (number of contained objects) per partition\nB block of data in a leaf node of RSMI\nϵ maximum error of the model\nAlgorithm 1: Index Traversal\n1Procedure BWQTraversal (N, Q, R )\n2 M←N.model\n3 ifNisleaf then\n4 foreach B∈[M(Q.q l)−M.ϵ l, M(Q.q h) +M.ϵ h]do\n5 ifintersects (B.mbr, Q. W)then\n6 R←CheckLeafNode (B, Q, R )\n7 else\n8 foreach nodeN′∈[M(Q.q l), M(Q.q h)]do\n9 ifCheckInnerNode (N′, Q)then\n10 BWQTraversal (N′, Q, R )\n11 return R\nDefinition 3 (Boolean kNN Query (B kQ)) A boolean kNN spatio-textual query is defined as BkQ = (q,T, k), where\nqis a point location, Tis a set of keywords, and kis the number of objects to be returned. Given a collection Dof\nspatio-textual objects, the result set RofBkQ comprises kobjects that contain all the keywords in Tand are most\nclosely located to q, i.e., (i) R⊆ D, (ii)|R|=k, (iii)∀o∈R,T ⊆o.T, (iv)∀o̸∈R,∃o′∈R|d(q.o.s )≥d(q, o′.s),\nwhere drefers to Euclidean distance.\nOur goal in this paper is to design learned spatio-textual indices that can accelerate the evaluation of BWQ andBkQ\nqueries compared to traditional indexing schemes.\n4 Index Design\nInspired by traditional spatio-textual indices, which combine a spatial part and a textual part, our approach is to enhance\na learned spatial index with keyword information. As a basis, we use RSMI [ 18] (see Section 2). We first provide\nan overview of our approach, and then we describe three indices based on different design choices, namely a loosely\ncoupled, a tightly coupled, and a hybrid scheme.\n4.1 Overview\nThe backbone of our proposed indices is RSMI, which hierarchically partitions the space and builds a respective\nhierarchy of ML models. At each level, partitions are created. Each partition is assigned an id based on z-order, and\nis associated with a model that directs the search to its children. An illustrative example is depicted in Figure 2 (the\nbitmaps next to each model Mi,jare explained later). There are four partitions at the top level. The root model is M0,0,\nwhich points to the four models M1,0, ..., M 1,3at the next level. Models M2,0, ..., M 2,3are leaf models. Each model is\nalso associated with the MBR of the respective partition. This is used during query evaluation to avoid false positives\nthat may occur due to prediction errors by the models, i.e., to prune subtrees whose MBR does not intersect with the\nspecified query window.\n4\n\npizza coffee fish winery burger bar \n1 1 1 1 1 1\n 3 M0,0 pizza coffee fish winery burger bar \n1 1 0 0 0 0\n1 0 0 1 0 0\n0 0 0 0 1 1\n1 1 1 1 1 1\npizza coffee fish winery burger bar \n0 1 1 1 1 0\n1 0 0 1 0 0\n0 1 0 0 0 1\n0 0 1 0 1 1 1  2  4 \n(a) (b) (c)M2,2 \nM2,0 M2,3 \n M 2,1 M1,3 \nM1,1 M1,2 \nM1,0 Figure 2: The RSMI-BM spatio-textual index.\nTo answer a Boolean Window Query Q, we traverse the index as in Algorithm 1. Procedure BWQTraversal is recursive,\nstarting from the root node. Assume the model Mof a visited node N. Given the lower left and upper right corners ( ql,\nqh) of the query window W, model Mpredicts the range of the children nodes (or blocks, in the case of leaf models)\nacross the z-order curve that should be checked. For inner nodes (Lines 7-10), process CheckInnerNode takes into\naccount any available information (MBR, bitmaps) per node and determines whether to recursively descend its subtree,\nas detailed later for each of the proposed schemes. To reduce the number of total checks, note that RSMI does not\nconsider the model errors at inner nodes. For each block in a visited leaf node (Lines 3-6), if its MBR intersects the\nquery window, we proceed to check its contents according to the keyword indexing scheme applied by each of our\nproposed methods. At the leaf level, the model errors ϵl, ϵhare taken into account. The process terminates once there\nare no more leaves to search and the set Rof results is returned.\nTo answer a Boolean kNN Query (BkQ), we follow the same approach as in RSMI. This involves executing a series of\nBWQ with increasingly larger window size until at least kobjects are found. As a heuristic, the side length of the initial\nwindow is set to ℓ=p\nk/|D|(assuming that original coordinates have been normalized to a unit space). Then, this\nlength is increased by a factor of 2 at each subsequent window query. In all proposed indexing schemes, the applied\nquery is a BWQ query, i.e., only objects containing the query keywords are qualifying for the top- kresults.\n4.2 Loosely Coupled Scheme\nThis approach loosely combines a spatial and a textual indexing component. Its advantage is simplicity and modularity,\nallowing existing implementations to be reused. The downside is lower pruning capacity, since each component only\nallows pruning with the respective criterion.\nIndex Structure. In this scheme, we combine RSMI with inverted files (IF) attached to its leaf nodes. We refer to\nthe resulting index as RSMI-IF . Given a collection Dof spatio-textual objects, RSMI is first built based on their point\nlocations. Recall that each leaf model points to a series of blocks. RSMI-IF constructs an inverted index for each such\nblock. Hence, at query time, instead of checking all objects in each block, only those that contain a query keyword can\nbe accessed. Essentially, this is analogous to the R∗-Tree-IF [ 27] spatio-textual index, where we use the learned spatial\nindex RSMI in place of the R∗-Tree.\nQuery Processing. Following the loosely-coupled nature of the RSMI-IF index, evaluation of a Boolean window query\nBWQ = (W,T)comprises two main stages. The first stage is essentially a spatial window query over RSMI using\nthe specified window W. Inner nodes with MBRs not intersecting Wcan be safely pruned (Algorithm 2, process\nCheckInnerNode ). However, instead of returning all objects within W, the process terminates a step earlier, specifically\nas soon as the leaf models identify the candidate blocks where these points may be contained. The second stage is\nhandled by process CheckLeafNode in Algorithm 2. For each such block in a leaf, its associated inverted file IFis used\nto retrieve the inverted lists of objects containing the query keywords. The intersection of these lists produces a set of\ncandidate objects that contain all the query keywords. Finally, the location of each candidate object ois checked to\nverify that it is within W. This processing method is also employed to answer B kQ queries, as noted in Section 4.1.\n4.3 Tightly Coupled Scheme\nIn this setting, the proposed index tightly combines a spatial and a textual component, so that pruning with both criteria\ncan be performed throughout all levels of the hierarchy. Next, we introduce RSMI-BM , an index that augments RSMI\nwith the use of bitmaps to encode the presence of keywords in the objects.\n5\n\nAlgorithm 2: RSMI-IF query processing\n1Procedure CheckInnerNode (N, Q)\n2 ifintersects( N.mbr, Q. W)then return true\n3 else return false\n4Procedure CheckLeafNode (B, Q, R )\n5 cands ←search (B.IF, Q. T)\n6 foreach object o∈cands do\n7 ifwithin( o.s, Q. W)then R←R∪ {o}\n8 return R\nAlgorithm 3: RSMI-BM query processing\n1Procedure CheckInnerNode (N, Q)\n2 ifintersects( N.mbr, Q. W)∧BitmapMatch( N.bm, bm (Q.T))then return true\n3 else return false\n4Procedure CheckLeafNode (B, Q, R )\n5 foreach object o∈Bdo\n6 ifwithin( o.s, Q. W)∧BitmapMatch( bm(o.T), bm(Q.T))then R←R∪ {o}\n7 return R\npizza coffee fish winery burger bar \n1 1 1 1 0 1\n1 1 1 1 1 0o9\no1{pizza,bar}{fish}\n{burger}{winery}\n{bar}{coffee}\n{fish}\n{pizza}{burger}o2o6\no5o7\no3o4o8\no10 {coffee,bar}\n(a) Partitioning on x-axis.\npizza coffee fish winery burger bar \n0 1 1 0 1 1\n1 0 0 1 0 1o9\no1{pizza,bar}{fish}\n{burger}{winery}\n{bar}{coffee}\n{fish}\n{pizza}{burger}o2o6\no5o7\no3o4o8\no10 {coffee,bar} (b) Partitioning on y-axis.\no9\no1{pizza,bar}{fish}\n{burger}{winery}\n{bar}{coffee}\n{fish}\n{pizza}{burger}o2o6\no5o7\no3o4o8\no10 {coffee,bar} (c) Final partitioning.\nFigure 3: The alternative partitioning method.\nIndex Structure. RSMI-BM follows the same base structure as RSMI, i.e., it consists of a hierarchy of models.\nHowever, each model is now additionally associated with a bitmap, where each bit corresponds to a keyword. The\nbit corresponding to keyword tis set to 1, if there exists an object in the subtree of that model that contains t, or 0\notherwise. To construct the index, the RSMI index is built first, based on the locations of the objects. Then, its nodes are\ntraversed bottom-up, keeping track of the encountered keywords, which are used to set the respective bitmap for each\nmodel. An RSMI-BM index with three levels is illustrated in Figure 2, corresponding to the collection of spatio-textual\nobjects depicted in Figure 1.\nQuery Processing. Evaluating a Boolean window query BWQ again follows the paradigm shown in Algorithm 1. The\nmodel hierarchy of the RSMI component is traversed from the root to the leaves, visiting the nodes indicated by each\nmodel encountered at each level. Checking the given spatial window Wagainst node MBRs is also used for pruning, as\nin RSMI-IF. The difference now is that before using a model to make a prediction, the algorithm also checks whether\nthe bitmap bmassociated to the corresponding leaf or inner node N(Algorithm 3) matches the bitmap of the query\nkeywords. If bmhas a bit set to 0 for any of the query keywords, then that model is skipped, i.e., its corresponding\nsubtree is pruned.\nAlternative Partitioning. Recall that RSMI partitions the data based on a non-regular grid adapted to the spatial\ndistribution of the objects. This involves interchangeably splitting the xandyaxes into equi-sized partitions. Having\n6\n\nM0,0 \nbm 0,0 \nM1,0 bm 1,0 M1,1 bm 1,1 M1,2 bm 1,2 M1,3 bm 1,3 \nM2,0 bm 2,0 M2,1 bm 2,1 M2,2 bm 2,2 M2,3 bm 2,3 \nIR 2,2,0 bm 2,2,0 IR 2,2,1 bm 2,2,1 ………\n…… … …Figure 4: The hybrid RSMI-BM-IR2spatio-textual index.\nbalanced partitions facilitates model training. Nevertheless, RSMI completely disregards the presence of keywords in\nthe objects. Next, we describe a modified partitioning scheme that also takes the keywords into consideration. The\ngoal is to produce partitions with more diverse bitmaps, which will increase the pruning capacity based on the textual\ninformation. We use the Hamming distance Hto measure the difference between two bitmaps. Let bmqdenote the\nbitmap corresponding to the set of query keywords Q.T, andbmi,bmjthe bitmaps of two nodes Ni,Nj, respectively,\nat a given level of the hierarchy. Intuitively, the higher the distance H(bmi, bmj), the less likely it is that both bmiand\nbmjmatch with bmq, i.e., that both subtrees rooted at NiandNjneed to be traversed.\nBased on this, our modified partitioning scheme works as follows. At each iteration, we perform two splits, one on the\nxaxis and another one on the yaxis, each producing two equi-sized partitions. We choose the order of these splits\nbased on the above criterion. Let bmx\n1andbmx\n2(resp., bmy\n1andbmy\n2) denote the bitmaps of the resulting partitions by\nsplitting on the x(resp., y) axis. If H(bmx\n1, bmx\n2)> H (bmy\n1, bmy\n2), we first split on the xaxis, otherwise we first split\nony. The process is repeated recursively until no partition exceeds the maximum allowed size S.\nFigure 3 illustrates an example. Performing a split on the xaxis (Figure 3a) yields two bitmaps with Hamming distance\n2. Instead, a split on the yaxis (Figure 3b) yields two bitmaps with Hamming distance 5. Thus, we choose to split on\ntheyaxis first, resulting on the partitions shown in Figure 3c.\n4.4 Hybrid Scheme\nIn the context of learned indices, hybrid index designs have been suggested, which may employ different types of\nmodels, or even non-learned indices, at different levels of the hierarchy [ 14]. Following this idea, we present the\nRSMI-BM-IR2index, which combines a learned and a non-learned part.\nIndex Structure. In RSMI-BM-IR2, the top levels in the hierarchy consist of an RSMI-BM index, as described in the\nprevious section. The difference is that here we set a much larger maximum allowed partition size S, which reduces the\ndepth of the model hierarchy and increases the number of objects contained in each leaf node. Then, for each leaf node\nof RSMI-BM, we index the set of its objects using an IR2-tree, which is a traditional spatio-textual index (see Section 2).\nThe structure of RSMI-BM-IR2is illustrated in Figure 4. Starting from the root, there is first a hierarchy of models,\nwith 3 levels in this example. Then, each leaf model (e.g., M2,3) points to a series of IR2-trees (i.e., IR2,2,0,IR2,2,1,\netc.). Each of these is again associated with a respective bitmap, which is checked before starting a tree traversal.\nQuery Processing. Processing a BWQ using RSMI-BM-IR2proceeds similarly to the RSMI-BM until a leaf is reached.\nThen, as shown in Algorithm 4, the same search query is issued against the IR2-tree attached under each such leaf in\nRSMI-BM. The union of these search results provides the final answer.\n5 Experimental Evaluation\nIn this section, we present an experimental evaluation using four real-world datasets. We compare the different index\nvariants described in Section 4, as well as two traditional spatio-textual indices.\n7\n\nAlgorithm 4: RSMI-BM-IR2query processing\n1Procedure CheckLeafNode (B, Q, R )\n2 R←search (B.IR2, Q)\n3 return R\nTable 2: Dataset characteristics.\nDataset Objects Area Size Distinct kwds Kwds per obj.\nFoursquare 94,158 5,053,518 mi2412 5.63\nOSM USA 1,228,616 3,975,720 mi2204 1.49\nOSM Europe 3,451,588 6,683,510 mi2255 1.71\nTwitter 10,000,000 6,301,967 mi2313 2.19\n5.1 Experimental Setup\n5.1.1 Datasets\nWe use four real-world datasets with different characteristics, as shown in Table 2. The datasets have been preprocessed\nto only keep frequent keywords.\nFoursquare . This contains 94,158 Points of Interest (POIs) extracted from Foursquare in the United States1, with a\ntotal of 412 distinct keywords and an average of 5.63 keywords per object.\nOSM USA . This contains 1,228,616 spatial entities extracted from OpenStreetMap (OSM) across the United States2.\nEach entity is represented by its centroid (lon/lat coordinates). There are 204 distinct keywords, with an average of 1.49\nkeywords per object.\nOSM Europe . This dataset contains 3,451,588 spatial entities extracted from OSM across Europe3. Each entity is again\nrepresented by its centroid (lon/lat coordinates). There are 255 distinct keywords, with an average of 1.71 keywords per\nentity.\nTwitter . This contains 10 million geo-tagged tweets in the United States4. There are 313 distinct keywords, with an\naverage of 2.19 keywords per tweet.\n5.1.2 Competitors\nWe compare the indexing schemes listed below.\nR∗-Tree-IF . A traditional spatio-textual index [27] that loosely combines the R∗-tree with inverted files.\nIR2-tree . A traditional spatio-textual index [ 11] that tightly combines the R∗-tree with signatures encoding the textual\ninformation. As signatures, we use bitmaps with length equal to the number of distinct keywords per dataset.\nRSMI-IF . The index presented in Section 4.2, which loosely combines RSMI with inverted files.\nRSMI-BM . The index presented in Section 4.3, which tightly combines RSMI with bitmaps.\nRSMI-BM∗. The variant of RSMI-BM using the modified partitioning strategy in Section 4.3.\nRSMI-BM-IR2. The index presented in Section 4.4, which combines RSMI-BM with the non-learned IR2-tree.\nAll methods were implemented in GNU C++. All indices are held in memory. The experiments were executed using\ngcc version 9.4.0 on a server with AMD Ryzen Threadripper 3960X 24-Core processor and 256 GB RAM running\nUbuntu 20.04.1 LTS.\n1https://star.cs.ucr.edu/?yin/foursquare#center=39.00,-98.00&zoom=4\n2http://download.slipo.eu/results/osm-to-csv/all/north-america/us/\n3http://download.slipo.eu/results/osm-to-csv/all/europe/\n4https://star.cs.ucr.edu/?Tweets#center=39.00,-98.00&zoom=4\n8\n\nTable 3: Query parameters and their ranges.\nParameter Value\nWindow side length ℓin BWQ queries 0.01, 0.05, 0.1, 0.15, 0.2\nCount kof results in B kQ queries 5,10, 20, 50, 100\nNumber of query keywords T 1, 2, 3, 4, 5\nR*Tree-IF\nIR2-TreeRSMI-IF\nRSMI-BMRSMI-BM*\nRSMI-BM-IR2\nFoursquare\nOSM EuropeOSM USAT itter\nDataset10−610−2102106Build Time (sec)\n(a) Index build time.\nFoursquare\nOSM EuropeOSM USAT witter\nDataset10−610−2102106Size (MB) (b) Index size.\nFigure 5: Index build time and size.\n5.1.3 Index Parameters\nWe conducted some preliminary tuning tests, and we have set the parameters of each index as described next. The\nblock size for RSMI-BM and RSMI-BM∗was set to |B|= 100 . Similarly, for the IR2-Tree, we set the minimum and\nmaximum node capacity to m= 50 andM= 100 , respectively. For both R∗-Tree-IF and RSMI-IF, we set these values\nto|B|= 1000 ,m= 500 , and M= 1000 , to benefit from the inverted indices in the leaves. For RSMI-BM-IR2, the\nblock size was set to |B|= 1000 , while using m= 10 andM= 20 for the IR2-trees constructed in the leaves in order\nto increase the tree height. For RSMI-IF, RSMI-BM, RSMI-BM∗and RSMI-BM-IR2the maximum partition size was\nset to S= 0.1· |D| for each dataset.\nThe ML models used in the RSMI-based indices are multi-layer perceptrons (MLP) and their configuration is the same\nas in the original RSMI paper [ 18]. Specifically, the hidden layer size is set to the sum of the number of dimensions and\nthe number of output classes divided by two. For the hidden layer, we use the sigmoid activation function. The models\nare trained level by level, starting from the root, with the learning rate set to 0.01 and the number of epochs to 500.\n5.1.4 Query Workload\nThe query parameters are listed in Table 3, with default values shown in bold. Each experiment is performed using a\nrandomly selected workload of 1,000 objects as queries from each dataset, reporting the average response time. For\nBWQ, we construct the query windows as squares having as center the query location and specifying a varying side\nlength ℓas listed in Table 3. The window side length ℓis expressed as a percentage of the whole dataset area. For tests\ninvolving a varying number of keywords T, we isolate the spatio-textual objects containing at least |T |keywords in\neach case, and randomly select 1,000 objects as the respective query workload. To assess the quality of query results,\nwe measure the average recall over the query workload. For B kQ queries, we also calculate the distance of the returned\nkNN from the query point and we measure its deviation (%) from the distance of the exact kNN.\n5.2 Index Construction\nFigure 5 depicts the build time and memory footprint required by each index per dataset. It is noticeable that traditional\nindices (i.e., IR2-Tree and R*-Tree-IF) incur a significantly less construction cost, built almost an order of magnitude\nfaster compared to the RSMI-based indices (Figure 5a). This is expected, since all proposed RSMI-based indices must\nundergo multiple expensive multi-epoch training tasks during construction; the more the models are trained, the more\nthe construction cost.\nBesides, traditional indices consume less memory (Figure 5b). The increased memory footprint of the RSMI-based\nindices should be attributed to the ML models that must also be kept in memory and the large number of bitmaps stored\nper leaf node and block along with the raw data (point coordinates and keywords). Furthermore, the RSMI-based\n9\n\nindices by default must keep for each point the mapped coordinates to the rank space and the space-filling curve value,\nthus requiring extra memory.\n5.3 BWQ Performance Results\nR*Tree-IF IR2-Tree RSMI-IF RSMI-BM RSMI-BM*RSMI-BM-IR2\n0.01 0.05 0.1 0.15 0.2\nr01Execution T ime (msec)\n(a) Foursquare Dataset\n0.01 0.05 0.1 0.15 0.2\nr0510Execution T ime (msec) (b) OSM USA Dataset\n0.01 0.05 0.1 0.15 0.2\nr050Execution T ime (msec) (c) OSM Europe Dataset\n0.01 0.05 0.1 0.15 0.2\nr050Execution T ime (msec) (d) Twitter Dataset\nFigure 6: BWQ response time with varying window size.\nVarying Window Size Figure 6 illustrates performance of BWQ for various window sizes. Unsurprisingly, RSMI-IF\nis significantly slow, an order of magnitude worse than its competitors, as it does not take into account the query\nkeywords while traversing the RSMI. It also has to perform multiple additional searches in inverted files of the leaves\nfollowed by expensive set intersection operations. The fastest scheme in all cases is RSMI-BM-IR2, which manages to\ncope well, taking advantage of RSMI’s quick node traversal. IR2-Tree comes in a close second in most cases, while all\nother indices are less efficient. Interestingly, though, the R*-Tree-IF manages to outperform IR2-Tree for larger window\nsizes against the Twitter dataset, despite being the slowest in all other cases. This is possibly due to the larger node\ncapacities along with efficient pruning on the spatial domain, requiring only few interactions with the inverted files at\nthe leaf level.\nThe corresponding recall results are shown in Figure 7. Noticeably, recall values for the RSMI-based methods on OSM\ndata are all close to 1. Performance of RSMI-BM∗against the Foursquare data is poor, especially for smaller window\nsizes, possibly because the ML models are struggling to learn the calculated partitions. Interestingly, for the Twitter\ndataset, RSMI-BM achieves the highest recall (close to 1) for all window sizes, possibly due to a partitioning that was\neasier to learn. In general, we noticed that RSMI-based indices with multiple levels tend to have many false negatives,\nsince inner node traversal does not take into account the model errors.\nVarying Number of Query Keywords Figure 8 plots BWQ response times for a varying number |T |of keywords\nper query. As previously noted, RSMI-IF is the slowest scheme. Since objects with many keywords are rather rare\n(see Table 2), increasing the number of keywords decreases the selectivity in all methods and performance is either\nimproved, or remains stable. In general, RSMI-BM-IR2is again the fastest; however, it is outperformed by RSMI-BM\nand RSMI-BM∗for|T |>3, except for the small Foursquare dataset.\nRegarding accuracy, note that recall is almost equal to 1 in most cases (Figure 9). The only exception is RSMI-BM∗,\nwhich offers less accurate results in some settings over the Foursquare and Twitter datasets, possibly because it is\nharder to learn the derived partitions. Nevertheless, for queries with |T |>4keywords over the Twitter dataset, the\nRSMI-BM∗approach is both the fastest and achieves a very high recall close to 1.\n5.4 B kQ Performance Results\nVarying kFigure 10 illustrates B kQ performance for a varying number kof returned results. Note that R∗-Tree-IF\ndoes not support such queries, hence it is not included in this experiment. In contrast to BWQ performance results,\nthe RSMI-BM-IR2approach is now the fastest only over the Foursquare and Twitter datasets. For the rather small\nFoursquare dataset, the IR2-Tree scales better for larger k, overtaking RSMI-BM-IR2. However, this is not the case over\nthe OSM datasets, where RSMI-BM and RSMI-BM∗are almost on par (RSMI-BM∗is slightly faster), and generally\nfaster than their competitors. While RSMI-BM-IR2is faster than IR2-Tree over the OSM Europe data, it struggles\nover OSM USA data. The most interesting observation, though, concerns the larger Twitter dataset, where IR2-Tree\nis significantly slower than the RSMI-based indices. In this case, these latter learned schemes manage to detect the\nrequested knearest neighbors without issuing many window queries.\nFigure 11 depicts the distance deviation between exact and RSMI-based k-th nearest neighbors from the query point.\nIntuitively, this indicates the quality of the RSMI-based results by considering the distance difference from the farthest\n10\n\nRSMI-IF RSMI-BM RSMI-BM*RSMI-BM-IR2\n0.01 0.05 0.1 0.15 0.2\nr0.00.51.0Recall(a) Foursquare Dataset\n0.01 0.05 0.1 0.15 0.2\nr0.00.51.0Recall (b) OSM USA Dataset\n0.01 0.05 0.1 0.15 0.2\nr0.00.51.0Recall (c) OSM Europe Dataset\n0.01 0.05 0.1 0.15 0.2\nr0.00.51.0Recall (d) Twitter Dataset\nFigure 7: Recall of BWQ results for varying window sizes.\nR*Tree-IF IR2-Tree RSMI-IF RSMI-BM RSMI-BM*RSMI-BM-IR2\n1 2 3 4 5\n# Query Keywords01Execution T ime (msec)\n(a) Foursquare Dataset\n1 2 3 4 5\n# Query Keywords110Execution T ime (msec) (b) OSM USA Dataset\n1 2 3 4 5\n# Query Keywords110100Execution T ime (msec) (c) OSM Europe Dataset\n1 2 3 4 5\n# Query Keywords10100Execution T ime (msec) (d) Twitter Dataset\nFigure 8: BWQ response time with varying number of query keywords T.\nRSMI-IF RSMI-BM RSMI-BM*RSMI-BM-IR2\n1 2 3 4 5\n# Query Keywords0.00.51.0Recall\n(a) Foursquare Dataset\n1 2 3 4 5\n# Query Keywords0.00.51.0Recall (b) OSM USA Dataset\n1 2 3 4 5\n# Query Keywords0.00.51.0Recall (c) OSM Europe Dataset\n1 2 3 4 5\n# Query Keywords0.00.51.0Recall (d) Twitter Dataset\nFigure 9: Recall of BWQ results for varying number of query keywords T.\nIR2-Tree RSMI-IF RSMI-BM RSMI-BM*RSMI-BM-IR2\n5 10 20 50 100\nk0.00.5Execution T ime (msec)\n(a) Foursquare Dataset\n5 10 20 50 100\nk0.00.51.0Execution T ime (msec) (b) OSM USA Dataset\n5 10 20 50 100\nk012Execution T ime (msec) (c) OSM Europe Dataset\n5 10 20 50 100\nk050Execution T ime (msec) (d) Twitter Dataset\nFigure 10: B kQ response time for varying k.\nresult. The loosely coupled RSMI-IF has poor accuracy, with distance deviations reaching 75% in some cases. RSMI-\nBM∗and RSMI-BM manage to cope reasonably well, with a deviation of 20-40% in most cases. The best performance\nis achieved by RSMI-BM-IR2, which yields an almost zero deviation in most cases. This observation is also confirmed\nby the corresponding recall values (Figure 12), with RSMI-BM-IR2achieving a recall close to 1 in all cases. Now, the\nsuperiority of RSMI-BM-IR2is more evident; by taking advantage of the fast RSMI-based inner node traversal and\nthe accurate query results of IR2-Tree in the leaves, it manages to achieve very low response times in all cases, while\nmaintaining a recall approximately equal to 1.\nVarying Number of Query Keywords Figure 13 illustrates B kQ performance for a varying number |T |of keywords\nper query. Increasing the number of keywords does not improve the response time, since queries with more keywords\n11\n\nRSMI-IF RSMI-BM RSMI-BM*RSMI-BM-IR2\n5 10 20 50 100\nk0204060Deviation (%)(a) Foursquare Dataset\n5 10 20 50 100\nk0204060Deviation (%) (b) OSM USA Dataset\n5 10 20 50 100\nk0255075Deviation (%) (c) OSM Europe Dataset\n5 10 20 50 100\nk0204060Deviation (%) (d) Twitter Dataset\nFigure 11: Distance deviation of k-th NN in B kQ results for varying k.\nRSMI-IF RSMI-BM RSMI-BM*RSMI-BM-IR2\n5 10 20 50 100\nk0.00.51.0Recall\n(a) Foursquare Dataset\n5 10 20 50 100\nk0.00.51.0Recall (b) OSM USA Dataset\n5 10 20 50 100\nk0.00.51.0Recall (c) OSM Europe Dataset\n5 10 20 50 100\nk0.00.51.0Recall (d) Twitter Dataset\nFigure 12: Recall of B kQ results for varying k.\nare rather rare and more searches are needed to fetch all qualifying kneighbors. RSMI-BM-IR2is again the fastest\nin most cases. Interestingly, IR2-Tree does not scale well; it is the fastest approach for |T |= 1(or |T |= 2over the\nFoursquare dataset), but it gets significantly slower with more query keywords. RSMI-based indices in general seem to\ncope well for any number of query keywords.\nFigure 14 shows the corresponding distance deviation of the retrieved k-th nearest neighbor from the exact one. As\nbefore, RSMI-BM-IR2manages to achieve a deviation close to 0%. RSMI-BM and RSMI-BM∗have a similar behavior\nin most cases, except for the Foursquare dataset, where RSMI-BM∗yields slightly better results. The most interesting\nfindings concern the Twitter dataset, and are also confirmed by the corresponding recall results in Figure 15. Observe\nthat, increasing the number of query keywords tends to improve the recall (and reduce the deviation as shown in\nFigure 14d) in all RSMI-based indices over Twitter data. This is possibly because fewer objects have many keywords,\nwhich can result in less false positives. The only differentiation is RSMI-BM-IR2, yielding recall values very close to 1\nin all cases. Since RSMI-based processing of B kQ employs windows (i.e., BWQ) of side length ℓ(instead of circles of\nradius ℓ), there is an interesting observation. The result set may contain objects that may be at a distance up toℓ√\n2\n2from the query point (close to a corner of the window). However, there may still exist objects with a smaller distance\nthat may be outside of the applied window; these will be missed. This explains the lower recall rates for B kQ queries\nusing RSMI-based schemes.\nIR2-Tree RSMI-IF RSMI-BM RSMI-BM*RSMI-BM-IR2\n1 2 3 4 5\n# Query Keywords0.11.0Execution T ime (msec)\n(a) Foursquare Dataset\n1 2 3 4 5\n# Query Keywords0.11.0Execution T ime (msec) (b) OSM USA Dataset\n1 2 3 4 5\n# Query Keywords0.110.0Execution T ime (msec) (c) OSM Europe Dataset\n1 2 3 4 5\n# Query Keywords1.0100.0Execution T ime (msec) (d) Twitter Dataset\nFigure 13: B kQ response time with varying number of query keywords T.\n6 Conclusions\nIn this paper, we examined the case of answering Boolean spatio-textual queries by extending RSMI, a state-of-the-art\nspatial learned index. We presented three different indexing schemes: a loosely coupled RSMI with inverted indices,\n12\n\nRSMI-IF RSMI-BM RSMI-BM*RSMI-BM-IR2\n5 10 20 50 100\nk02040Deviation (%)(a) Foursquare Dataset\n5 10 20 50 100\nk050100Deviation (%) (b) OSM USA Dataset\n5 10 20 50 100\nk02040Deviation (%) (c) OSM Europe Dataset\n5 10 20 50 100\nk050100Deviation (%) (d) Twitter Dataset\nFigure 14: Distance deviation of k-th NN in B kQ results for varying number of query keywords T.\nRSMI-IF RSMI-BM RSMI-BM*RSMI-BM-IR2\n1 2 3 4 5\n# Query Keywords0.00.51.0Recall\n(a) Foursquare Dataset\n1 2 3 4 5\n# Query Keywords0.00.51.0Recall (b) OSM USA Dataset\n1 2 3 4 5\n# Query Keywords0.00.51.0Recall (c) OSM Europe Dataset\n1 2 3 4 5\n# Query Keywords0.00.51.0Recall (d) Twitter Dataset\nFigure 15: Recall of B kQ results for varying number of query keywords T.\na tightly coupled scheme that augments RSMI nodes with keyword bitmaps, and a hybrid approach that attaches\na traditional state-of-the-art spatio-textual index under each leaf node of RSMI. We also examined an alternative\npartitioning strategy that takes the textual part of the objects under consideration. Our extensive experimental evaluation\nindicated that, in general, the hybrid approach is faster and yields very accurate results. In the future, we plan to\ninvestigate ways for integrating the textual information in the learning process of the indices, to further improve query\nresponse times, as well as to support index updates.\nAcknowledgments\nThis work was supported by the EU H2020 project SmartDataLake (825041) and the EU H2020 project OpertusMundi\n(870228).\nReferences\n[1]A. Al-Mamun, H. Wu, and W. G. Aref. A tutorial on learned multi-dimensional indexes. In SIGSPATIAL , pages\n1–4, 2020.\n[2]N. Beckmann, H.-P. Kriegel, R. Schneider, and B. Seeger. The R*-tree: An efficient and robust access method for\npoints and rectangles. In SIGMOD , pages 322–331, 1990.\n[3]A. Cary, O. Wolfson, and N. Rishe. Efficient and scalable method for processing top-k spatial boolean queries. In\nSSDBM , pages 87–95, 2010.\n[4]L. Chen, G. Cong, C. S. Jensen, and D. Wu. Spatial keyword query processing: An experimental evaluation.\nPVLDB , 6(3):217–228, 2013.\n[5]Y . Chen, T. Suel, and A. Markowetz. Efficient query processing in geographic web search engines. In SIGMOD ,\npages 277–288, 2006.\n[6]Z. Chen, L. Chen, G. Cong, and C. S. Jensen. Location- and keyword-based querying of geo-textual data: a survey.\nVLDB J. , 30(4):603–640, 2021.\n[7]M. Christoforaki, J. He, C. Dimopoulos, A. Markowetz, and T. Suel. Text vs. space: efficient geo-search query\nprocessing. In CIKM , pages 423–432, 2011.\n[8]A. Davitkova, E. Milchevski, and S. Michel. The ML-index: A multidimensional, learned index for point, range,\nand nearest-neighbor queries. In EDBT , pages 407–410, 2020.\n13\n\n[9]J. Ding, V . Nathan, M. Alizadeh, and T. Kraska. Tsunami: A learned multi-dimensional index for correlated data\nand skewed workloads. PVLDB , 14(2):74–86, 2020.\n[10] C. Faloutsos and S. Christodoulakis. Signature files: An access method for documents and its analytical\nperformance evaluation. ACM Transactions on Information Systems (TOIS) , 2(4):267–288, 1984.\n[11] I. D. Felipe, V . Hristidis, and N. Rishe. Keyword search on spatial databases. In ICDE , pages 656–665, 2008.\n[12] R. Hariharan, B. Hore, C. Li, and S. Mehrotra. Processing spatial-keyword (SK) queries in geographic information\nretrieval (GIR) systems. In SSDBM , page 16, 2007.\n[13] A. Khodaei, C. Shahabi, and C. Li. Hybrid indexing and seamless ranking of spatial and textual features of web\ndocuments. In DEXA , volume 6261, pages 450–466, 2010.\n[14] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for learned index structures. In SIGMOD ,\npages 489–504, 2018.\n[15] P. Li, H. Lu, Q. Zheng, L. Yang, and G. Pan. LISA: A learned index structure for spatial data. In SIGMOD , pages\n2119–2133, 2020.\n[16] R. Marcus, A. Kipf, A. van Renen, M. Stoian, S. Misra, A. Kemper, T. Neumann, and T. Kraska. Benchmarking\nlearned indexes. PVLDB , 14(1):1–13, 2020.\n[17] V . Nathan, J. Ding, M. Alizadeh, and T. Kraska. Learning multi-dimensional indexes. In SIGMOD , pages\n985–1000, 2020.\n[18] J. Qi, G. Liu, C. S. Jensen, and L. Kulik. Effectively learning spatial indices. PVLDB , 13(12):2341–2354, 2020.\n[19] J. Qi, Y . Tao, Y . Chang, and R. Zhang. Theoretically optimal and empirically efficient r-trees with strong\nparallelizability. PVLDB , 11(5):621–634, 2018.\n[20] J. B. Rocha-Junior, O. Gkorgkas, S. Jonassen, and K. Nørvåg. Efficient processing of top-k spatial keyword\nqueries. In SSTD , pages 205–222, 2011.\n[21] S. Vaid, C. B. Jones, H. Joho, and M. Sanderson. Spatio-textual indexing for geographical search on the web. In\nSSTD , volume 3633, pages 218–235, 2005.\n[22] H. Wang, X. Fu, J. Xu, and H. Lu. Learned index for spatial queries. In MDM , pages 569–574, 2019.\n[23] D. Wu, G. Cong, and C. S. Jensen. A framework for efficient spatial web object retrieval. VLDB J. , 21(6):797–822,\n2012.\n[24] D. Wu, M. L. Yiu, G. Cong, and C. S. Jensen. Joint top-k spatial keyword query processing. IEEE Trans. Knowl.\nData Eng. , 24(10):1889–1903, 2012.\n[25] C. Zhang, Y . Zhang, W. Zhang, and X. Lin. Inverted linear quadtree: Efficient top k spatial keyword search. IEEE\nTrans. Knowl. Data Eng. , 28(7):1706–1721, 2016.\n[26] D. Zhang, K. Tan, and A. K. H. Tung. Scalable top-k spatial keyword search. In EDBT , pages 359–370, 2013.\n[27] Y . Zhou, X. Xie, C. Wang, Y . Gong, and W. Ma. Hybrid index structures for location-based web search. In CIKM ,\npages 155–162, 2005.\n14",
  "textLength": 47452
}