{
  "paperId": "364275dc427992889b0a601e827150d9f9cab7da",
  "title": "Daisy Bloom Filters",
  "pdfPath": "364275dc427992889b0a601e827150d9f9cab7da.pdf",
  "text": "Daisy Bloom Filters\nIoana O. Bercea /envel⌢pe\nKTH Royal Institute of Technology, Stockholm, Sweden\nJakob Bæk Tejs Houen /envel⌢pe\nBARC, University of Copenhagen, Denmark\nRasmus Pagh /envel⌢pe\nBARC, University of Copenhagen, Denmark\nAbstract\nA filter is a widely used data structure for storing an approximation of a given set Sof elements\nfrom some universe U(a countable set). It represents a superset S′⊇Sthat is “close to S” in the\nsense that for x̸∈S, the probability that x∈S′is bounded by some ε >0. The advantage of using\na Bloom filter, when some false positives are acceptable, is that the space usage becomes smaller\nthan what is required to store Sexactly.\nThough filters are well-understood from a worst-case perspective, it is clear that state-of-the-art\nconstructions may not be close to optimal for particular distributions of data and queries. Suppose,\nfor instance, that some elements are in Swith probability close to 1. Then it would make sense\nto always include them in S′, saving space by not having to represent these elements in the filter.\nQuestions like this have been raised in the context of Weighted Bloom filters (Bruck, Gao and Jiang,\nISIT 2006) and Bloom filter implementations that make use of access to learned components (Vaidya,\nKnorr, Mitzenmacher, and Krask, ICLR 2021).\nIn this paper, we present a lower bound for the expected space that such a filter requires. We\nalso show that the lower bound is asymptotically tight by exhibiting a filter construction that\nexecutes queries and insertions in worst-case constant time, and has a false positive rate at most ε\nwith high probability over input sets drawn from a product distribution. We also present a Bloom\nfilter alternative, which we call the Daisy Bloom filter , that executes operations faster and uses\nsignificantly less space than the standard Bloom filter.\n2012 ACM Subject Classification Theory of computation →Data structures design and analysis\nKeywords and phrases Bloom filters, input distribution, learned data structures\nFunding Supported by grant 16582, Basic Algorithms Research Copenhagen (BARC), from the\nVILLUM Foundation.arXiv:2205.14894v2  [cs.DS]  16 Jun 2024\n\nI.O. Bercea, J.B.T. Houen, R. Pagh 1\n1 Introduction\nThis paper shows asymptotically matching upper and lower bounds for the space of an\noptimal (Bloom) filter when the input and queries come from specific distributions. For a\nsetSof keys (the input set), a filter on Swith parameter ε∈(0,1)is a data structure that\nanswers membership queries of the form “is x in S?” with a one-sided error: if x∈S, then the\nfilter always answers YES, otherwise it makes a mistake (i.e., a false positive) with probability\nat mostε. The Bloom filter [ 8] is the most widely known such filter, although more efficient\nconstructions are known [ 3,24,2,5,6,19,33,40,41,44]. Filters are also intimately related\nto dictionaries (or hash tables), the latter of which always answer membership queries exactly.\nWhen errors can be tolerated, (Bloom) filters are much better than dictionaries at\nencoding the input set: they require Θ(nlog(1/ε))bits to represent a set of size n, versus\nthe≥nlog(u/n)bits that a dictionary would require (here, uis the size of the universe). As\nsuch, filters are often used in conjunction with dictionaries to speed up negative queries. In\nparticular, filters are often stored in a fast but small memory and are used to “filter out” a\nmajority of negative queries to a dictionary (which might reside in big but slow memory).\nBecause of this, they have proved to be extremely popular in practice and research on them\ncontinues to this day, both in the direction of practical implementations [ 43,23,18] and on\nthe theoretical front [ 33,5,4]. For instance, recent advances in filter design have included\nmaking them dynamic, resizeable and lowering the overall space that they require.\nThe filter encoding. In this paper, we ask ourselves what should optimal filters look like\nwhen they encode sets that come from a specific distribution. While this question has been\nresolved for exact encodings (i.e., entropy), no similar concepts are known for filter encodings.\nIndeed, considering input distributions raises several technical questions. For instance, it is\nnot even clear how to define the concept of approximate membership with respect to a set\ndrawn from a distribution. Should we assume that the input set is given to us in full before\nwe build our filter and allocate memory? Moreover, we would like to obtain designs that are\nnever worse than filters with no knowledge of the input distribution, both in space allocated\nand time required to perform every operation. Should we then require that the false positive\nguarantee hold for every possible input set or just on average over the input distribution?\nWe also study optimality when additionally, we have access to a distribution over queries.\nThis is especially important for applications in which the performance of the filter is measured\nover a sequence of queries, rather than for each query separately [ 25,10]. At the extreme end\nof this one can consider adversarial settings, in which an adversary forces the filter to incur\nmany false positives (which can cause a delay in the system by forcing the filter to repeatedly\naccess the slow dictionary). In these settings, defining what it means for the filter to behave\nefficiently can be a challenge and several definitions have been considered [ 39,37,2,38]. For\nus, the challenge is to use the query distribution to obtain gains, while making sure that the\nfilter does not on average exhibit more false positives than usual. This is natural when each\nfalse positive has the same cost, independent of the query element.\nTo this end, we consider a natural generative model of input sets and queries. Specifically,\nwe letPandQdenote two distributions over the universe Uof keys and let px(andqx,\nrespectively) denote the probability that a specific key x∈Uis sampled fromP(andQ,\nrespectively). The input set Sis generated by nindependent draws (with replacement) from\nPand we letPndenote this product distribution.1\n1We do not consider multiplicities although our design can be made to handle them by using techniques\n\n2 Daisy Bloom Filters\nWe then define approximate membership for a fixed set Sto mean that the average\nfalse positive probability over Qis at mostε. Specifically, let Fdenote the filter and let\nF(S,x)∈{YES,NO}denote the answer that Freturns when queried on an element x∈U,\nafter having been given S⊆Uas input. Then we propose the following definition:\n▶Definition 1. For anyεwith 0<ε< 1, we say thatFis a(Q,ε)-filter forSif it satisfies\nthe following conditions:\n1.No false negatives: For all x∈S, we have that Pr [F(S,x) =YES] = 1.\n2.Bounded false positive rate:/summationtext\nx∈U\\Sqx·Pr [F(S,x) =YES]≤ε\nWe note a detail in the above definition that has important technical consequences and\nthat is, the false positive rate is not computed with respect to the input distribution (i.e.,\nthe probability of a false positive only depends on the internal randomness of the filter and\nnot the random process of drawing the input set). As a consequence, we can argue about\nfilter designs that work over all input sets except some that occur very rarely under Pn.\nThis is stronger than saying that Fworks only on average over Pn. Moreover, we also want\ndesigns that do not require knowing the specific realization of the input set in advance. Our\ndependency onPnshows up in the space requirements of the filter.\nAccess toPandQ.For simplicity, we consider filter designs that have oracle access to\nPandQ: upon seeing a key x, we also get pxandqx. We assume that this is done in\nconstant time and do not account for the size of the oracle when we bound the size of the\nfilter. Critics of this model have argued that assuming oracle access to a distribution over\nthe universe is too strong of an assumption. Indeed, this is a valid concern, since we are\ntalking about a data structure that is meant to save space over a dictionary. We try to\nalleviate this concern in several ways. On one hand, our construction can tolerate mistakes.\nIn particular, our designs are robust even if we have a constant factor approximation for px\nandqx, in the sense in which the space increases only by O(n)bits and the time to perform\neach operation by an added constant. The assumption of access to such approximate oracles\nis standard [ 12,22] and can be based on samples of historical information, on frequency\nestimators such as Count-Min [ 16] or Count-Sketch [ 15], or on machine learning models (see\nfor instance, the neural-net based frequency predictor of Hsu et al. [ 31]). This view is indeed\npart of an emerging body of work on algorithms with predictions, to which the data structure\nperspective is just beginning to contribute [36, 13, 29, 28, 27, 26, 47, 17, 35].\nOn the other hand, empirical studies have shown that significant gains are possible even\nwhen using off-the-shelf, “simplistic” learned components such as random forest classifiers.\nIn particular, the Partitioned Learned Bloom Filter [ 47] and the Adaptive Learned Bloom\nFilter [17] consider settings in which the size of the learned component is comparable to the\nsize of the filter itself (rather than proportional to the size of the universe), and compare the\ntraditional Bloom filter design [ 8] with a learned design whose space includes the random\nforest classifier. In one experiment with a universe of ≈138,000keys and a classifier of\n136Kb, [17] show that, within the range 150-300Kb, there is a 98%decrease in false positive\nrate compared to the original Bloom filter. This continues to hold for larger universe ( ≈\n450,000keys) with total allocated space between 200Kb and 1000Kb. A discussion of how\nour current (theoretical) design compares to the ones in [ 17] and [47] can be found in Sec. 1.2.\nfrom counting filters [40, 9, 42, 6].\n\nI.O. Bercea, J.B.T. Houen, R. Pagh 3\nFinally, strictly speaking, our designs do not necessarily rely on knowing pxandqxfor\nevery element inserted or queried. As we will see in the next section, our designs depend\nrather on knowing which subset of the universe a key xbelongs to. This corresponds to\na partitioning of the universe that mainly depends on the ratio qx/px, rather than the\nindividual values of pxandqx(with the exception of values of pxandqxthat are very small,\ne.g., smaller than 1/n). This can conceivably lead to even smaller oracles that just output\nthe partition to which an element belongs. We also do not need to query the entire universe\nin order to set the internal parameters of the filter, in contrast to [17, 47].\nWeighted Bloom filters. The design that we propose starts by gathering information about\nthe input and query distributions, using polylog (n)samples.2This information is used to\nestimate the internal parameters of the filter which are then used to allocate space for the\nfilter and implement the query and insert operations. Thus, the most important aspect of\nour design is in setting the aforementioned internal parameters.\nAs a baseline for comparison, we can consider the classic Bloom filter design which\nallocates an array of ≈1.44·nlog(1/ε)bits and hashes every key to log(1/ε)locations in the\narray. Upon insertion, the corresponding bits are set to 1 and a query returns a YES if and\nonly if all locations are set to 1. The more locations we hash into, the lower the probability\nthat we make a mistake. Thus, a natural approach for our problem would be to vary the\nnumber of hashed locations of xbased onpxandqx. Indeed, this is the question investigated\nby Bruck, Gao and Jiang [ 11] in their Weighted Bloom filter design. More precisely, let kx\ndenote the number of locations that key xis hashed to. Then [ 11] investigates the optimal\nchoice of the parameters kxthat limits the false positive rate in expectation over both the\ninput and the query distribution. Their approach follows the original Bloom filter analysis\nand casts the problem as an unconstrained optimization problem in which kxis allowed to\nbe any real number (including negative). For more details, we refer the reader to Sec. 1.2\nThis formulation and the fact that their false positive rate is taken as an average over Pn\nleads to situations in which kxcan be made arbitrarily large and, with high probability, the\nfilter is filled with 1s and has a high false positive probability (for instance, when a key is\nqueried very rarely). To avoid such situations, as we shall see next, optimal choices for kx\nexhibit some rather counter-intuitive trade-offs between pxandqx.\n1.1 Our Contributions\nWe start by discussing near-optimal choices for kxfor a Weighted Bloom filter that is a\n(Q,ε)-filter for sets drawn from Pn. While this filter is not the most efficient of the filters we\nconstruct, reasoning through it helps us present our parametrizations and addresses the fact\nthat Bloom filters remain well-liked in practice [34]. Specifically, we define kxas follows:3\nkx≜\n\n0 if orpx>1/norqx≤εpx,\nlog(1/ε·qx/px)ifεpx<qx≤min{px,ε/n},\nlog(1/ε) ifqx>pxandpx≤ε/n,\nlog(1/(npx))ifqx>ε/nandε/n<px≤1/n.\n2Elements that are inserted in the set during that time can be stored in a small dictionary that only\nrequires polylog (n)bits, see Sec. 4.3.\n3Throughout the paper, we employ logxto denote log2xandlnxto denote logex.\n\n4 Daisy Bloom Filters\nε/n\nε/n\n1/n\n1\n1\nqx\npx\nlog\u00121\nε\u0013\nlog\u00121\nnpx\u0013\n0\nlog\u0012qx\nεpx\u0013\nU3\nU4\nU1\nU2\nU0\n0\nFigure 1 A schematic visualization of the different regimes for kx.\nThe first case covers the situation in which xis very likely to be included in the set or\nis queried very rarely (relative to px). Intuitively, it makes sense in these cases to always\nsayYESwhen queried. Thus, we set kx= 0and store no information about these keys.\nConversely, the third case considers the case in which xis queried so often (relative to px)\nthat we need to explicitly keep the false positive probability below ε, which is achieved by\nsettingkx=log(1/ε). This is the largest number of hash functions we employ for any key, so\nin this sense, we are never worse than the classical Bloom filter. The second case interpolates\nsmoothly between the first and third cases for elements that are rarely (but not very rarely)\nqueried (compared to how likely they are to be inserted). Finally, the fourth case interpolates\nbetween the first and third case for elements that are not too rarely queried, in which case\nthe precise query probability does not matter. See Fig. 1 for a visualization of these regimes.\nTo further make sense of these regimes, we consider the case of uniform queries, i.e.,\nqx= 1/u, and assume that ε > n/u, a standard assumption in filter design (otherwise,\nthe filter would essentially have to answer correctly on all queries and the lower bound of\nnlog2(1/ε)−O(1)would not hold [ 14,19]). Then in the two extremes, we would set kx= 0for\nelements with px≥1/(uε)(first case) and kx=log(1/ε)whenpx≤1/u(third case). Keys\nwithpxbetween the two cases would exhibit the smooth interpolation kx=log(1/(uε)·1/px),\ncorresponding to the intuition that the more likely an element is to be inserted, the less\ninformation we should store about it (i.e., smaller kx).\nThe lower bound. Given the above parameters, we then define the quantity\nLB(Pn,Q,ε)≜/summationtext\nx∈Upxkx\nand show that, perhaps surprisingly, it gives a lower bound for the expected space that any\n(Q,ε)-filter requires when the input set is drawn from Pn:\n▶Theorem 2 (Lower bound - simplified) .LetAbe an algorithm and assume that for any\ninput setS⊆Uwith|S|≤n,A(S)is a(Q,ε)-filter forS. Then the expected size of A(S)\nmust satisfy\nEPn,A[|A(S)|]≥LB(Pn,Q,ε)−1−6n,\nwhereSis sampled with respect to Pnand the queries are sampled with respect to Q.\n\nI.O. Bercea, J.B.T. Houen, R. Pagh 5\nPrevious approaches for filter lower bounds show that there exists a set S⊆Uof sizen\nfor which the filter needs to use nlog2(1/ε)−O(1)bits [14,19]. This type of lower bound is\nstill true in our model but it does not necessarily say anything meaningful, since the bad\nsetScould be sampled in Pnwith a negligible probability. Indeed, if we were to ignore the\ninput distribution, then we would not be able to beat the worst input distribution and, in\nparticular, we would need to use at least supPLB(Pn,Q,ε) =LB(Qn,Q,ε) =nlog(1/ε)bits\nin expectation, where Qndenotes a distribution over nindependent draws from Q.\nIn our model, it is therefore more natural to lower bound the expected size of the filter\nover the randomness of the input set. Finally, we remark that the full lower bound we prove is\nslightly stronger in that it holds for all but an unlikely collection of possible input sets, i.e. we\nonly require that A(S)is a(Q,ε)-filter forS∈TwhereT ⊆P(U)andPrPn[S̸∈T]≤1\nlogu\n(see Thm. 6).\nThe space-efficient filter. We also show a filter design that asymptotically matches our\nspace lower bound and executes operations in constant time in the worst case:\n▶Theorem 3 (Space-efficient filter - simplified) .Given 0<ε< 1, there is a (Q,ε)-filter with\nthe following guarantees:\nit is a (Q,ε)-filter with high probability over sets drawn from Pn, if/summationtext\nx∈Upxqx≤ε/n,\nqueries and insertions take constant time in the worst case,\nthe space it requires is (1 +on(1))·LB(Pn,Q,ε) +O(n)bits.\nThe construction uses the kxvalues from above in conjunction with the fingerprinting\ntechnique of Carter et al. [ 14] to obtain results that are comparable to state-of-the-art\n(classic) filter implementations that execute all operations (queries and insertions) in worst\ncase constant time, and are space efficient, in the sense in which they require (1 +on(1))·\nnlog(1/ε) +O(n)bits [1,5,6,4]. The condition that/summationtext\nx∈Upxqx≤ε/ncan be seen as a\ngeneralization of the standard filter assumption that ε≥n/u.\nThe Daisy Bloom filter. For completeness, we also present our variant of the Weighted\nBloom filter, which we call the Daisy Bloom filter :4\n▶Theorem 4 (Daisy Bloom filter - simplified) .Given 0<ε< 1, the Daisy Bloom filter has\nthe following guarantees:\nit is a (Q,ε)-filter with high probability over sets drawn from Pn, if/summationtext\nx∈Upxqx≤ε/n,\nqueries and insertions take at most ⌈log2(1/ε)⌉time in the worst case,\nthe space it requires is log(e)·LB(Pn,Q,ε) +O(n)bits.\nIn contrast to the weighted Bloom filters of Bruck et al. [ 11], the Daisy Bloom filter\nexecutes operations in time that is at most ⌈log2(1/ε)⌉in the worst case (versus arbitrarily\nlarge) and achieves a false positive rate of at most εwith high probability over the input\nset (and not just on average). We also depart in our analysis from their unconstrained\noptimization approach (to setting kx) and instead use Bernstein’s inequality to argue that,\nif the length of the array is set to log(e)·LB(Pn,Q,ε) +O(n)bits, then whp, at most half of\nthe entries in the array will be set to 1(see Sec. 5 for more details).\n4The daisy is one of our favorite flowers, especially when in full bloom, and is also a subsequence of\n“dynamicstrechy” which describes the key properties of our data structure. It is also the nickname\nof the Danish queen, whose residence is not far from the place where this work was conceived. Daisy\nBloom filters are not related to any celebrities.\n\n6 Daisy Bloom Filters\n1.2 Related Work\nFilters have been studied extensively in the literature [ 2,5,6,14,19,33,40,41,44], with\nBloom filters perhaps the most widely employed variants in practice [ 34]. Learning-based\napproaches to classic algorithm design have recently attracted a great deal of attention, see\ne.g. [20,30,31,32,45]. For a comprehensive survey on learned data structures, we refer the\nreader to Ferragina and Vinciguerra [28].\nWeighted Bloom Filters\nGiven information about the probability of inserting and querying each element, Bruck, Gao\nand Jiang [ 11] set out to find an optimal choice of the parameters kxthat limit the false\npositive rate (in expectation over both the input and the query distribution). The approach\nis to solve an unconstrained optimization problem where the variables kxcan be any real\nnumber. In a post-processing step each kxis rounded to the nearest non-negative integer.\nUnfortunately, this process does not lead to an optimal choice of parameters, and in fact,\ndoes not guarantee a non-trivial false positive rate. The issue is that the solution to the\nunconstrained problem may have many negative values of kx, so even though the weighted\nsum/summationtext\nxpxkxis bounded, the post-processed sum/summationtext\nxpxmax(kx,0)can be arbitrarily large.\nIn particular, this is the case if at least one element is queried very rarely. This means that\nthe weighted Bloom filter may consist only of 1s with high probability, resulting in a false\npositive probability of 1.\nThe above issue was noted by Wang, Ji, Dang, Zheng and Zhao [ 48] who attempt to\ncorrect the values for kx, but their analysis still suffers from the same, more fundamental,\nproblem: the existence of a very rare query element drives the false positive rate to 1. Wang\net al. [48] also show an information-theoretical “approximate lower bound” on the number\nof bits needed for a weighted Bloom filter with given distributions PandQ. The sense in\nwhich the lower bound is approximate is not made precise, and the lower bound is certainly\nnot tight (for example, it can be negative).\nPartitioned Learned Bloom Filters\nThere are several learned Bloom filter designs that assume that the filter has access to a\nlearned model of the input set [ 32,36,17,47]. The model is given a fixed input set Sand a\nrepresentative sample of elements in U\\S( the query distribution is not specified). Given a\nquery element x, the model returns a scores(x)∈[0,1], which can be intuitively thought\nof as the model’s belief that x∈S. Based on this score, Vaidya, Knorr, Mitzenmacher and\nKraska [47] choose a fixed number of kthresholds, partition the elements according to these\nthresholds, and build separate Bloom filters for each set of the partition. For fixed threshold\nvalues, they then formulate the optimization problem of setting the false positive rates fi\nsuch that the total space of the data structure is minimized and the overall false positive\nrate is at most a given F.\nAs noted by Ferragina and Vinciguerra [ 28], a significant drawback in these constructions\nis that the guarantees they provide depend significantly on the query set given as input to\nthe machine learning component and in particular, the set being representative for the whole\nquery distribution. We avoid this issue by making the dependencies on qxexplicit and by\nbounding the average false positive probability even when just one element is queried. In\naddition, our data structure does not need to know the set Sin advance (and hence, training\ncan be done just once, in a pre-processing phase), employs only one data structure, and our\nguarantees are robust to approximate values for pxandqx.\n\nI.O. Bercea, J.B.T. Houen, R. Pagh 7\n1.3 Paper Organization\nAfter some preliminaries, Sec. 3 shows our lower bound on the space usage. In Sec. 4,\nwe discuss a space-efficient filter with constant time worst-case operations. Finally, Sec. 5\npresents the analysis of the Daisy Bloom filter.\n2 Preliminaries\nFor clarity, throughout the paper, we will distinguish between probabilities over the random-\nness of the input set, denoted by PrPn[·], and probabilities over the internal randomness of\nthe filter, denoted by PrA[·]. Joint probabilities are denoted by PrPn,A[·]. For the analysis,\nit will also make sense to partition the universe Uinto the following 5parts:\nU0≜{x∈U|qx≤εpx},\nU1≜{x∈U|qx>εpxandpx>1/n},\nU2≜{x∈U|εpx<qx≤min{px,ε/n}},\nU3≜{x∈U|qx>pxandε/n≥px},\nU4≜{x∈U|qx>ε/nandε/n<px≤1/n}.\nThe high probability guarantees we obtain increase with LB(Pn,Q,ε). Therefore, such\nbounds are meaningful for distributions in which the optimal size LB(Pn,Q,ε)of a filter is\nnot too small. Similarly, we can assume that the size of the universe is polynomial in n, and\nsolog(1/ε) =O(logn)in the standard case in which ε>n/|U|. Therefore, while in general\nLB(Pn,Q,ε)can be much smaller than nlog2(1/ε), we do require some mild dependency on\nnfor the high probability bounds to be meaningful. Finally, we recall the following classic\nresult in data compression:\n▶Theorem 5 (Kraft’s inequality [ 46]).For any instantaneous code (prefix code) over an\nalphabet of size D, the codeword lengths ℓ1,ℓ2,...,ℓmmust satisfy the inequality\n/summationtext\niD−ℓi≤1.\nConversely, given a set of codeword lengths that satisfy this inequality, there exists an\ninstantaneous code with these word lengths.\n3 The Lower Bound\nThe goal of this section is to prove the lower bound from Thm. 2. As discussed, we prove a\nslightly stronger statement where we allow our algorithm to not produce a (Q,ε)-filter for\nsome input sets as long as the probability of sampling them is low. Formally, we show that:\n▶Theorem 6. LetT ⊆P(U)be given such that PrPn[S̸∈T]≤1\nlogu. IfAis an algorithm\nsuch that for all S∈T,A(S)is a(Q,ε)-filter forS. Then the expected size of A(S)must\nsatisfy\nEPn,A[|A(S)|]≥LB(Pn,Q,ε)−1−6n,\nwhereSis sampled with respect to Pn.\n\n8 Daisy Bloom Filters\nProof.Each instanceIof the data structure corresponds to a subset UI⊂Uon which the\ndata structure answers YES. We denote the number of bits needed by such an instance by\n|I|. For any set S∈T, we have thatI=A(S)satisfies that S⊆UIand\nEA\n/summationdisplay\nx∈UI\\Sqx\n≤ε.\nThe goal is to prove that\nEPn,A[|A(S)|]≥n·/parenleftigg/summationdisplay\nx∈U2pxlog/parenleftbigg1\nε·qx\npx/parenrightbigg\n+/summationdisplay\nx∈U3pxlog1\nε+/summationdisplay\nx∈U4pxlog1\nnpx/parenrightigg\n−1−6n.\nWe will lower bound EPn,A[|A(S)|]by using it to encode an ordered sequence of n\nelements drawn according to Pn. Specifically, for any ordered sequence of nelements ˆS∈Un,\nwe letS⊆Ube the set of distinct elements and let I=A(S)as above. We first note that\nto encode ˆS∼Pn, in expectation, we need at least the entropy number of bits, i.e.,\nn/summationdisplay\nx∈Upxlog1\npx. (1)\nNow our encoding using Iwill depend on whether S∈Tor not. First, we will use 1 bit\nto describe whether S∈Tor not. For (xi)i∈[n]∈ˆS, we will denote bito be the number\nbits to encode xi. IfS̸∈Tthen for all i∈[n]we encodexiusingbi=⌈log(1/pxi)⌉bits. If\nS∈Tthen for all i∈[n]we encodexidepending on which subset if Uit belongs to:\n1.Ifxi∈U0∪U1, we encode xiusingbi=⌈log(4/pxi)⌉bits.\n2.Ifxi∈U2, we encode xiusingbi=/ceilingleftbigg\nlog/parenleftbigg\n4/summationtext\ny∈UI∩U2qy\nqxi/parenrightbigg/ceilingrightbigg\nbits.\n3.Ifxi∈U3, we encode xiusingbi=/ceilingleftbigg\nlog/parenleftbigg\n4/summationtext\ny∈UI∩U3py\npxi/parenrightbigg/ceilingrightbigg\nbits.\n4.Ifxi∈U4, we encode xiusingbi=⌈log (4|UI∩U4|)⌉bits.\nIt is clear from the construction that we satisfy the requirement for Thm.5 thus there exists\nsuch an encoding. Now we will bound the expectation of the size of this encoding:\nEPn,A\n|A(S)|+ 1 +/summationdisplay\ni∈[n]bi\n=EPn,A[|A(S)|] + 1 + EPn,A\n/summationdisplay\ni∈[n]bi\n.\nWe will write EPn,A/bracketleftig/summationtext\ni∈[n]bi/bracketrightig\n=EPn,A/bracketleftig\n[S∈T]/summationtext\ni∈[n]bi/bracketrightig\n+EPn,A/bracketleftig\n[S̸∈T]/summationtext\ni∈[n]bi/bracketrightig\n, and\nbound each term separately.\nWe start by bounding EPn,A/bracketleftig\n[S̸∈T]/summationtext\ni∈[n]bi/bracketrightig\n.\nEPn,A\n[S̸∈T]/summationdisplay\ni∈[n]bi\n=EPn\n[S̸∈T]/summationdisplay\ni∈[n]⌈log(1/pxi)⌉\n\n≤PrPn[S̸∈T]n+EPn\n[S̸∈T] log\n/productdisplay\ni∈[n]1/pxi\n\n\n= PrPn[S̸∈T]n+/summationdisplay\nˆs∈Un[ˆs∈T] PrPn/bracketleftig\nˆS= ˆs/bracketrightig\nlog1\nPrPn/bracketleftig\nˆS= ˆs/bracketrightig\n\nI.O. Bercea, J.B.T. Houen, R. Pagh 9\nNow using Jensen’s inequality we get that\n/summationdisplay\nˆs∈Un[ˆs∈T] PrPn/bracketleftig\nˆS= ˆs/bracketrightig\nlog1\nPrPn/bracketleftig\nˆS= ˆs/bracketrightig≤PrPn[S̸∈T] log/parenleftig\n1\nPrPn[S̸∈T]un/parenrightig\n.\nPutting this together with the fact that PrPn[S̸∈T]≤1\nlogu, we get that,\nEPn,A\n[S̸∈T]/summationdisplay\ni∈[n]bi\n≤2n.\nNow we bound EPn,A/bracketleftig\n[S∈T]/summationtext\ni∈[n]bi/bracketrightig\n=/summationtext\ni∈[n]EPn,A[[S∈T]bi]. We will bound\nEPn,A[[S∈T]bi]depending on which subset of Uthatxibelongs to.\nIfxi∈U0∪U1, then we have that EPn,A[[S∈T]bi]≤⌈log(4/pxi)⌉≤3 + log(1/pxi).\nIfxi∈U2, defineZ2=UI∩U2. Then\nEPn,A[[S∈T]bi]≤3 +EPn,A/bracketleftbigg\n[S∈T] log/parenleftbigg/summationtext\ny∈Z2qy\nqxi/parenrightbigg/bracketrightbigg\n.\nWe know that/summationtext\ny∈S∩U2qy≤εsinceqy≤ε/nfor ally∈U2and|S|≤n. We also know that\nEA/bracketleftig/summationtext\nx∈Z2\\Sqx/bracketrightig\n≤εforS∈T. Now using Jensen’s inequality we get that\nEPn,A/bracketleftbigg\n[S∈T] log/parenleftbigg/summationtext\ny∈Z2qy\nqxi/parenrightbigg/bracketrightbigg\n=EPn/bracketleftbigg\n[S∈T]EA/bracketleftbigg\nlog/parenleftbigg/summationtext\ny∈Z2qy\nqxi/parenrightbigg/bracketrightbigg/bracketrightbigg\n≤EPn\n[S∈T] log\nEA/bracketleftig/summationtext\ny∈Z2qy/bracketrightig\nqxi\n\n\n≤EPn/bracketleftig\n[S∈T] log/parenleftig\n2ε\nqxi/parenrightig/bracketrightig\n≤1 +EPn/bracketleftig\nlog/parenleftig\nε\nqxi/parenrightig/bracketrightig\n.\nIfxi∈U3, defineZ3=UI∩U3. Then\nEPn,A[[S∈T]bi]≤3 +EPn,A/bracketleftbigg\n[S∈T] log/parenleftbigg/summationtext\ny∈Z3py\npxi/parenrightbigg/bracketrightbigg\n.\nWe know that/summationtext\ny∈S∩U3py≤εsincepy≤ε/nfor ally∈U3and|S|≤n. We also know that\nEA/bracketleftig/summationtext\nx∈Z3\\Spx/bracketrightig\n≤EA/bracketleftig/summationtext\nx∈Z3\\Sqx/bracketrightig\n≤εforS∈T. Using Jensen’s inequality we get that,\nEPn,A/bracketleftbigg\n[S∈T] log/parenleftbigg/summationtext\ny∈Z3py\npxi/parenrightbigg/bracketrightbigg\n=EPn/bracketleftbigg\n[S∈T]EA/bracketleftbigg\nlog/parenleftbigg/summationtext\ny∈Z3py\npxi/parenrightbigg/bracketrightbigg/bracketrightbigg\n≤EPn\n[S∈T] log\nEA/bracketleftig/summationtext\ny∈Z3py/bracketrightig\npxi\n\n\n≤EPn/bracketleftig\n[S∈T] log/parenleftig\n2ε\npxi/parenrightig/bracketrightig\n≤1 +EPn/bracketleftig\nlog/parenleftig\nε\npxi/parenrightig/bracketrightig\n.\nIfxi∈U4, defineZ4=UI∩U4. Then EPn,A[[S∈T]bi]≤3 +EPn,A[[S∈T] log (|Z4|))].\nWe know that|Z4|=|Z4∩S|+|Z4\\S|≤n+n\nε/summationtext\nx∈Z4\\Sqxsinceqy>ε/nfor ally∈U4\nand|S|≤n. Using Jensen’s inequality we get that, for Z′\n4=Z4\\S:\n\n10 Daisy Bloom Filters\nEPn,A\n[S∈T] log\nn+n\nε/summationdisplay\nx∈Z′\n4qx\n\n=EPn\n[S∈T]EA\nlog\nn+n\nε/summationdisplay\nx∈Z′\n4qx\n\n\n\n≤EPn\n[S∈T] log\nEA\nn+n\nε/summationdisplay\nx∈Z′\n4qx\n\n\n\n≤EPn[[S∈T] log (2n)]≤1 + log(n).\nCombining it all we get an encoding that in expectation uses at most\nEPn,A[|A(S)|] + 1 + 6n+\n/summationdisplay\nx∈(U0∪U1)pxlog(1/px) +/summationdisplay\nx∈U2pxlog(ε/qx) +/summationdisplay\nx∈U3pxlog(ε/px) +/summationdisplay\nx∈U4pxlogn.\nbits to encode ˆS. Comparing this with Eq.1 we get that,\nEPn,A[|A(S)|]≥n·/parenleftigg/summationdisplay\nx∈U2pxlog/parenleftbigg1\nε·qx\npx/parenrightbigg\n+/summationdisplay\nx∈U3pxlog1\nε+/summationdisplay\nx∈U4px1\nnpx/parenrightigg\n−1−6n.\nThis proves the claim. ◀\n4 Space-Efficient Filter\nIn this section, we show how one can use the kxvalues proposed to design a space-efficient\nQ-filter with worst-case constant time operations. Formally, we show that:\n▶Theorem 7. Assume thatPnandQsatisfyn/summationtext\nx∈Upxqx≤ε. Then there exists a filter\nwith the following guarantees:\nthere existsT ⊆P(U)where a set S∈Twith high probability over the randomness of\nPn, such that the filter is a (Q,ε)-filter for any S∈T,\nthe filter uses (1 +on(1))·LB(Pn,Q,ε) +O(n)bits,\nthe filter executes queries and insertions in worst case constant time and,\nthe filter does not fail with high probability over its internal randomness.\n4.1 Construction\nForj∈{1,...,⌈log(1/ε)⌉}, we letU(j)≜{x∈U|⌈kx⌉=j}denote the set of elements that\nhash tojlocations in the Daisy Bloom filter and Pj≜/summationtext\nx∈U(j)pxdenote the probability\nthat we select an element from U(j)in one sample from P. Thennj≜n·Pjdenotes the\naverage number of elements from U(j)that we expect to see in the input set. We distinguish\nbetween the sets/braceleftbig\nU(j)/bracerightbig\ndepending on their corresponding nj. Specifically, we say that U(j)\nis arare class ifnj<n/ logcn, for some constant c>2, and otherwise we say that U(j)is a\nfrequent class . We further define Ur⊆Uto be the set of all elements that are in a rare class,\ni.e.,U(j)⊆Uif and only ifU(j)is a rare class.\nNow letF(ε,n)be a (standard) filter implementation for at most nelements with false\npositive probability at most ε. We focus on implementations that execute all operations\n(queries and insertions) in worst-case constant time, and are space efficient: they require\n(1 +on(1))·nlog(1/ε) +O(n)bits [1,5,6,4]. We employ⌈log(1/ε)⌉+ 1instantiations ofF,\nwhich we denote by F1,...,F⌈log(1/ε)⌉andFr. They are parametrized as follows: for j∈\n{1,...,⌈log(1/ε)⌉}, we further define Nj≜(1+1/logn)·njand instantiateFj=F(2−j,Nj).\nWe instantiateFrasFr=F(F,Nr), whereNr≜Θ(n/logc−1n).\n\nI.O. Bercea, J.B.T. Houen, R. Pagh 11\nOperations. We distinguish between elements that are in a frequent class and elements that\nare in a rare class. If an element is in a frequent class U(j), then operations are forwarded to\nthe corresponding filter Fj. Otherwise, the operation is forwarded to Fr. Since all the filters\nwe employ perform operations in constant time in the worst case, the same holds for our\nconstruction5.\nSpace. We now bound the total number of bits that F1,...,F⌈log(1/ε)⌉andFrrequire:\n▶Lemma 8. The above filter requires (1 +on(1))·LB(Pn,Q,ε) +O(n)bits.\nProof.RecallthatF1,...,F⌈log(1/ε)⌉andFrareinstantiationsofafilter Fwhichrequires (1+\nf(n))·nlog(1/ε)+O(n)bits for a set of nelements and false positive probability ε, where the\nfunctionf(n)satisfiesf(n) =on(1)[1,5,6,4]. For simplicity, we choose the implementation\nin [5], wheref(n) = Θ( log logn/√logn). Consequently, for j∈{1,...,⌈log(1/ε)⌉}, the\nspace ofFjis:\n(1 +f(Nj))·Njlog(1/2−j) +O(Nj) = (1 +f(Nj))·Nj·j+O(Nj)\nbits. SinceFjis instantiated only for frequent classes, it follows that n≥Nj≥n/logcnand\nhence,f(Nj) = Θ(f(n))for alljwithU(j)a frequent class. Furthermore, by definition, we\nhave thatj≤kx+1for allx∈U(j)andNj= (1+1/logn)·nj= (1+1/logn)·n/summationtext\nx∈U(j)px.\nTherefore, the space that Fjrequires can be upper bounded by\n(1 + Θ(f(n)))·n/summationdisplay\nx∈U(j)pxkx+O(Nj).\nSince/summationtext\njNj= (1 + 1/logn)·nwe get that, in the worst case in which all the classes are\nfrequent, the filters F1,...,F⌈log(1/ε)⌉require\n(1 +on(1))·n/summationdisplay\nx∈Upxkx+O(n) = (1 +on(1))·LB(Pn,Q,ε) +O(n)\nbits. The space of the final filter Fris upper bounded by Θ(n/logc−1n)·log(1/ε) =\nΘ(n/logc−2n) =o(n)bits for any constant c>2. The claim follows. ◀\n4.2 Analysis\nIn this section, we show that the filter described above does not fail whp and that it achieves\na false positive probability of at most 3εwith respect toQ. In our construction, there are\ntwo sources of failure: when the number of elements which we insert into each filter exceeds\nthe maximum capacity of the filter, and when the filters themselves fail as a consequence\nof their internal randomness. In the latter case, we note that the failure probability of\nF(ε,n)is guaranteed to be at most 1/poly(n), where the degree of the polynomial is a\nconstant of our choosing [ 1,5,6,4]. Since all of the instantiations we employ have maximum\ncapacities which are Ω(n/polylog (n)), we conclude that each of these separate instantiations\nalso fails with probability at most 1/poly(n), and therefore, by a union bound over the\n⌈log(1/ε)⌉+ 1 =O(logn)instantiations, we get that some filter fails with probability at\nmost 1/poly(n). We now show that the maximum capacities we set for each filter suffice.\n5We note here that it is possible to combine the filters F1, . . . ,F⌈log(1/ε)⌉into one single data structure.\nOne could use, for example, the balls-into-bins implementation in [ 5], where elements are randomly\nassigned to one of n/Θ(logn/(log(1/ε)))buckets and the buckets explicitly store random strings of\nlength log(1/ε)associated with the elements that hash into them. Combining F1, . . . ,F⌈log(1/ε)⌉would\nthen entail “superimposing” their buckets.\n\n12 Daisy Bloom Filters\n▶Lemma 9. Whp, at most Nr= Θ(n/logc−1n)elements fromUrare sampled in the input\nset.\nProof.LetU(j)be a rare class and let Xjdenote the number of elements from U(j)that we\nsample in the input set. By definition, the expected number of elements we see from U(j)\nsatisfies EP[Xj]=nj<n/ logcn, for some constant c>2. By Chernoff bound, we therefore\nget that:\nPr [Xj>6n/logcn]≤2−6n/logcn.\nThere are at most ⌈log(1/ε)⌉=O(logn)possible rare classes, and so, by the union bound, the\nnumber of elements from Urthat we sample in the input set is at most Nr= Θ(n/logc−1n)\nwhp. ◀\nWe now focus on sampling elements from a frequent class:\n▶Lemma 10. LetU(j)be a frequent class. Then, whp, at most Njelements fromU(j)are\nsampled in the input set.\nProof.LetXjdenote the number of elements from U(j)that we sample in the input set and\nnote that EP[Xj] =nj≥n/logcn. By Chernoff:\nPr [Xj>Nj] = Pr [Xj>(1 + 1/logn)·nj]≤exp(−Θ(n/logc−2n))≤1/polyn.\nThis concludes our proof. ◀\nFinally, we bound the false positive rate of the filter:\n▶Lemma 11. Assume thatPnandQsatisfyn/summationtext\nx∈Upxqx≤εand that the input set Sdoes\nnot makeF1,...,F⌈log(1/ε)⌉andFrfail. Then the filter described is a (Q,3ε)-filter onS.\nProof.Fix an input set Sand denote by A′(S,x)the output of the filter when queried for an\nelementx. We are interested in bounding Pr [A′(S,x) =YES]for an element x /∈S. Ifx∈Ur,\nthen we forward the query operation to Fr, which guarantees that Pr [A′(S,x) =YES]≤ε.\nTherefore:/summationdisplay\nx∈Urqx·Pr [A′(S,x) =YES]≤/summationdisplay\nx∈Urqx·ε,\nOtherwise, if x /∈Ur, the query is forwarded to Fj, wherej=⌈kx⌉. In this case,\nPr [A′(S,x) =YES]≤2−j≤2−kxand we get that\n/summationdisplay\nx∈U0∪U2\\Urqx·Pr [A′(S,x) =YES]≤/summationdisplay\nx∈U0\\Urqx+/summationdisplay\nx∈U2\\Urpxε≤/summationdisplay\nx∈U0∪U2\\Urpx·ε,\nand similarly,\n/summationdisplay\nx∈U1∪U4\\Urqx·Pr [A′(S,x) =YES]≤/summationdisplay\nx∈U1\\Urqx+/summationdisplay\nx∈U4\\Urnpxqx≤/summationdisplay\nx∈U1∪U4\\Urnpxqx.\nFinally, we have that\n/summationdisplay\nx∈U3\\Urqx·Pr [A′(S,x) =YES]≤/summationdisplay\nx∈U3\\Urqx·ε.\nAdding all of these quantities, we obtain the claim. ◀\n\nI.O. Bercea, J.B.T. Houen, R. Pagh 13\n4.3 Remarks\nThe filter construction assumes that we know, in advance, whether a class U(j)is frequent\nand, if so, what is the value of its corresponding Pj=/summationtext\nx∈U(j)px. This is because we employ\nfixed capacity filters which require us to provide an upper bound on the cardinality of the\ninput setS∩U(j)at all points in time. We note that this assumption can be alleviated in\ntwo ways: on one hand, one can employ filters that do not require us to know the size of the\ninput set in advance [ 41,7]. This would incur an additional Θ(nlog logn)bits in the space\nconsumption of our filter (operations would remain constant time worst case).\nOn the other hand, one can estimate Pjfor all frequent classes U(j)if we are allowed\nto take polylog (n)samples fromPbefore constructing the filter. Specifically, fix j∈\n{1,...,⌈log(1/ε)⌉}and takeℓ=O(log2cn)samples fromP. DefineZjto be the number of\nsampled elements that are in U(j). Then, ifU(j)is indeed frequent, by the standard Chernoff\nbound we get that, whp, Zj>logcnelements and Zj/ℓis an unbiased estimator for Pjwith\nthe guarantee that Pj= (1±O(1/log(c−1)/2n))·Zj/ℓwhp. Note that we can tolerate such\nan estimate since we set the maximum capacity of each filter to be Nj= (1+1/logn)·nPj. A\nsimilar argument can be used for estimating the lower bound LB(Pn,Q,ε) =n·/summationtext\nx∈Upx·kx\nwhp. Specifically, by the definition of Pj, we have that\n/summationdisplay\nx∈Upx·⌈kx⌉=⌈log(1/ε)⌉/summationdisplay\nj=1j·Pj.\nIfU(j)is a frequent class, then by the above argument we have an estimate of its Pj.\nOtherwise, we know that Pj<1/logcnand, sincej≤⌈log(1/ε)⌉=O(logn), get that\n/summationdisplay\njs.t.U(j)∈Urj·Pj≤/summationdisplay\njs.t.U(j)∈Ur⌈log(1/ε)⌉·1/logcn≤1/logc−2n,\nwhich contributes a o(n)term to the lower bound.\n5 The Daisy Bloom Filter Analysis\nIn this section, we analyse the behaviour of the Daisy Bloom filter with the values kx\ndenoting the number of hash functions that we use to hash xinto the array. Let Xidenote\nthe number of hash functions that are employed when we sample in the ithround, i.e.,\nXi=kxwith probability px. ThenX=/summationtext\niXidenotes the number of locations that are\nset in the Bloom filter (where the same location might be set multiple times). Moreover,\nEPn[X]=n·/summationtext\nx∈Upxkx=LB(Pn,Q,ε), since the{Xi}iare identically distributed. We\nthen set the length mof the Daisy Bloom filter array to\nm≜EPn[X]/ln 2.\nThe remainder of this section is dedicated to proving the following statement:\n▶Theorem 12. Assume thatPnandQsatisfyn/summationtext\nx∈Upxqx≤ε. Then there exists T ⊆P(U)\nsuch thatS∈Twith high probability over the randomness of Pn, and for all S∈Tthe Daisy\nBloom Filter is a (Q,ε)-filter forS. The Daisy Bloom filter uses log(e)·LB(Pn,Q,ε) +O(n)\nbits and executes all operations in at most ⌈log(1/ε)⌉time in the worst case.\nThe sets inTare the sets for which X≈EPn[X]=LB(Pn,Q,ε). The reason we constrain\nourselves to these sets, is that if X≫LB(Pn,Q,ε)then most bits will be set to 1 which will\nmake the false positive rate large. We will bound the probability that X≫LB(Pn,Q,ε)by\nusing Bernstein’s inequality and here, the following observation becomes crucial:\n\n14 Daisy Bloom Filters\n▶Observation 13. For everyx∈U,kx≤log(1/ε).\nProof.Forx∈U0∪U1, we have that kx= 0which is clearly less than log(1/ε). Forx∈U3\nwe have that kx=log(1/ε)and again the statement holds trivially. For x∈U2, we have that\nqx≤pxand sokx=log(1/ε·qx/px)≤log(1/ε). Forx∈U4, we have that px>F/nand so\nkx= log(1/(npx))<log(1/ε). ◀\nWe are now ready to prove that the random variable Xis concentrated around its\nexpectation.\n▶Lemma 14. For anyδ>0,\nPrPn[X > (1 +τ)·EPn[X]]≤exp/parenleftbigg\n−τ2ln 2\n2(1 +τ/3)·m\nlog(1/ε)/parenrightbigg\nProof.The random variables {Xi}iare independent and Xi≤b≜log(1/ε)for alliby\nObs. 13. We apply Bernstein’s inequality [21]:\nPrPn[X−EPn[X]>t]≤exp/parenleftbigg\n−t2/2\nnVarPn[X1] +bt/3/parenrightbigg\n.\nNote that VarPn[Xi]≤EPn/bracketleftbig\nX2\ni/bracketrightbig\n≤b·EPn[Xi]. Settingt=τ·EPn[X]=τn·EPn[X1], we\nget that\nPrPn[X > (1 +τ)EPn[X]]≤exp/parenleftbigg\n−τ2\n2·n2(EPn[X1])2\nnb·EPn[X1] +τ/3·nb·EPn[X1]/parenrightbigg\n= exp/parenleftbigg\n−τ2\n2(1 +τ/3)·nEPn[X1]\nb/parenrightbigg\n.\nThe claim follows by noticing that nEPn[X1] =mln 2. ◀\nWe can now prove that as long as X≤(1 + 1/(2log(1/ε)))·EPn[X], the Daisy Bloom\nfilter is a (Q,6ε)-filter forS. We consider the fraction ρof entries in the array that are set\nto0after we have inserted the elements of the set. We then show that ρis close to 1/2with\nhigh probability over the input set and the randomness of the hash functions. Conditioned\non this, we then have that the probability that we make a mistake for xis at most 2−kx+1.\nThe false positive rate is then derived similarly to that of Lemma 11.\n▶Lemma 15. Assume thatPnandQsatisfyn/summationtext\nx∈Upxqx≤ε, and that X≤(1 +\n1/(2 log(1/ε)))EPn[X]. Then, whp, the Daisy Bloom filter is a (Q,6ε)-filter onS.\nProof.Letρ∈[0,1]denote the fraction of entries in the Daisy Bloom filter that are set to 0\nafter we have inserted the elements of the set. Recall that the random variable Xdenotes\nthe total number of entries that are set in the Bloom filter, including multiplicities. In the\nworst case, all the entries to the filter are distinct, and we have Xindependent chances to\nset a specific bit to 1. Therefore\nEh[ρ|X]≥/parenleftbigg\n1−1\nm/parenrightbiggX\n≈e−X/m= 2−X/EPn[X].\nMoreover, by applying a Chernoff bound for negatively associated random variables, we\nhave that for any 0<γ < 1,\n\nI.O. Bercea, J.B.T. Houen, R. Pagh 15\nPrA/bracketleftigg\nρ≤(1−γ)·/parenleftbigg\n1−1\nm/parenrightbiggX/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleX/bracketrightigg\n≤exp/parenleftigg\n−m/parenleftbigg\n1−1\nm/parenrightbiggX\n·γ2/2/parenrightigg\n(2)\nWe now let Bδdenote the event that/parenleftbig\n1−1\nm/parenrightbigX>(1−δ)·1\n2andBγthe event that\nρ>(1−γ)/parenleftbig\n1−1\nm/parenrightbigX. We then choose 0<δ< 1and0<γ < 1such thatBδandBδ′imply\nthat\nρ≥1−21/log(1/ε)·1\n2.\nMoreover, for our choices of δandγ, we have that both BδandBγ|Bδoccur with high\nprobability.6We refer the reader to App.A for δandγ. Conditioned on BδandBγ, we get\nthat, for an x /∈S, sincekx≤log(1/ε),\nPrA[A(S,x) =YES|Bδ∧Bγ] = (1−ρ)kx≤2kx/b·2−kx≤2·2−kx\nWe bound the false positive rate on each partition. For x∈U0, i.e., withqx≤Fpxand\nkx= 0, we can upper bound the false positive rate as such\n/summationdisplay\nx∈U0qx·Pr [A(S,x) =YES|Bδ∧Bγ]≤/summationdisplay\nx∈U0qx≤/summationdisplay\nx∈U0px·ε.\nForx∈U1withpx>1/nandkx= 0, we have the following\n/summationdisplay\nx∈U1qx·Pr [A(S,x) =YES|Bδ∧Bγ]≤/summationdisplay\nx∈U1qx<n/summationdisplay\nx∈U1pxqx.\nForx∈U2withkx= log(1/ε·qx/px), we have the following\n/summationdisplay\nx∈U2qx·Pr [A(S,x) =YES|Bδ∧Bγ]≤/summationdisplay\nx∈U2qx·2·2−kx=/summationdisplay\nx∈U2qx·2·px/qx·ε\n=/summationdisplay\nx∈U2px·2ε.\nForx∈U3withkx= log(1/ε),\n/summationdisplay\nx∈U3qx·Pr [A(S,x) =YES|Bδ∧Bγ]≤/summationdisplay\nx∈U3qx·2ε≤2ε.\nForx∈U4withkx= log(1/(npx)),\n/summationdisplay\nx∈U4qx·Pr [A(S,x) =YES|Bδ∧Bγ]≤/summationdisplay\nx∈U4qx·2npx= 2n/summationdisplay\nx∈U4pxqx.\nFor the overall false positive rate, note that the total false positive rate in U0andU2is\nat most 2ε. Similarly for the false positive rate in U3. For the remaining partitions U1and\nU4, we have that it is at most\n2n/summationdisplay\nx∈U1∪U4pxqx.\nFrom our assumption, this later term is at most 2εas well.\n◀\nCombining the above with Lemma 14 we get that with probability 1−exp/parenleftig\n−m\nΘ(log3(1/ε))/parenrightig\nover the randomness of the input set, the Daisy Bloom filter is a (Q,6ε)-filter forS. This is\nexactly the statement of Thm. 12.\n6We implicitly assume here that 21/log(1/ε)·≤2, i.e., ε≤1/2. Notice that this does not affect the overall\nresult, since the false positive rate we obtain is 5·ε.\n\n16 Daisy Bloom Filters\nReferences\n1Yuriy Arbitman, Moni Naor, and Gil Segev. Backyard cuckoo hashing: Constant worst-\ncase operations with a succinct representation. In 2010 IEEE 51st Annual Symposium on\nFoundations of Computer Science , pages 787–796. IEEE, 2010. See also arXiv:0912.5424v3.\n2Michael A. Bender, Martin Farach-Colton, Mayank Goswami, Rob Johnson, Samuel McCauley,\nand Shikha Singh. Bloom filters, adaptivity, and the dictionary problem. In 59th IEEE Annual\nSymposium on Foundations of Computer Science, FOCS 2018, Paris, France, October 7-9,\n2018, pages 182–193, 2018. doi:10.1109/FOCS.2018.00026 .\n3Michael A Bender, Martin Farach-Colton, Rob Johnson, Bradley C Kuszmaul, Dzejla Med-\njedovic, Pablo Montes, Pradeep Shetty, Richard P Spillane, and Erez Zadok. Don’t thrash:\nHow to cache your hash on flash. In 3rd Workshop on Hot Topics in Storage and File Systems\n(HotStorage 11) , 2011.\n4Michael A. Bender, Martin Farach-Colton, John Kuszmaul, William Kuszmaul, and Mingmou\nLiu. On the optimal time/space tradeoff for hash tables. In Stefano Leonardi and Anupam\nGupta, editors, STOC ’22: 54th Annual ACM SIGACT Symposium on Theory of Computing,\nRome, Italy, June 20 - 24, 2022 , pages 1284–1297. ACM, 2022. doi:10.1145/3519935.\n3519969.\n5Ioana O. Bercea and Guy Even. A dynamic space-efficient filter with constant time operations.\nIn Susanne Albers, editor, 17th Scandinavian Symposium and Workshops on Algorithm Theory,\nSWAT 2020, June 22-24, 2020, Tórshavn, Faroe Islands , volume 162 of LIPIcs, pages 11:1–\n11:17. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2020. doi:10.4230/LIPIcs.SWAT.\n2020.11.\n6Ioana O. Bercea and Guy Even. Dynamic dictionaries for multisets and counting filters with\nconstant time operations. In Anna Lubiw and Mohammad R. Salavatipour, editors, Algorithms\nand Data Structures - 17th International Symposium, WADS 2021, Virtual Event, August\n9-11, 2021, Proceedings , volume 12808 of Lecture Notes in Computer Science , pages 144–157.\nSpringer, 2021. doi:10.1007/978-3-030-83508-8\\_11 .\n7Ioana Oriana Bercea and Guy Even. An extendable data structure for incremental stable\nperfect hashing. In Stefano Leonardi and Anupam Gupta, editors, STOC ’22: 54th Annual\nACM SIGACT Symposium on Theory of Computing, Rome, Italy, June 20 - 24, 2022 , pages\n1298–1310. ACM, 2022. doi:10.1145/3519935.3520070 .\n8Burton H. Bloom. Space/time trade-offs in hash coding with allowable errors. Commun. ACM ,\n13(7):422–426, 1970. doi:10.1145/362686.362692 .\n9Flavio Bonomi, Michael Mitzenmacher, Rina Panigrahy, Sushil Singh, and George Varghese.\nAn improved construction for counting bloom filters. In Algorithms–ESA 2006: 14th Annual\nEuropean Symposium, Zurich, Switzerland, September 11-13, 2006. Proceedings 14 , pages\n684–695. Springer, 2006.\n10Andrei Z. Broder and Michael Mitzenmacher. Survey: Network applications of bloom filters:\nA survey. Internet Math. , 1(4):485–509, 2003. doi:10.1080/15427951.2004.10129096 .\n11JehoshuaBruck, JieGao, andAnxiaoJiang. Weightedbloomfilter. In International Symposium\non Information Theory (ISIT) , pages 2304–2308. IEEE, 2006.\n12Clément Canonne and Ronitt Rubinfeld. Testing probability distributions underlying aggreg-\nated data. In International Colloquium on Automata, Languages, and Programming , pages\n283–295. Springer, 2014.\n13Xinyuan Cao, Jingbang Chen, Li Chen, Chris Lambert, Richard Peng, and Daniel Sleator.\nLearning-augmented b-trees, 2023. arXiv:2211.09251 .\n14Larry Carter, Robert W. Floyd, John Gill, George Markowsky, and Mark N. Wegman. Exact\nand approximate membership testers. In Richard J. Lipton, Walter A. Burkhard, Walter J.\nSavitch, Emily P. Friedman, and Alfred V. Aho, editors, Proceedings of the 10th Annual ACM\nSymposium on Theory of Computing, May 1-3, 1978, San Diego, California, USA , pages 59–65.\nACM, 1978.\n\nI.O. Bercea, J.B.T. Houen, R. Pagh 17\n15Moses Charikar, Kevin Chen, and Martin Farach-Colton. Finding frequent items in data\nstreams. In International Colloquium on Automata, Languages, and Programming , pages\n693–703. Springer, 2002.\n16Graham Cormode and Shan Muthukrishnan. An improved data stream summary: the\ncount-min sketch and its applications. Journal of Algorithms , 55(1):58–75, 2005.\n17Zhenwei Dai and Anshumali Shrivastava. Adaptive learned bloom filter (ada-bf): Efficient\nutilization of the classifier with application to real-time information filtering on the web. In\nHugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien\nLin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on\nNeural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual ,\n2020.\n18Niv Dayan, Ioana O. Bercea, Pedro Reviriego, and Rasmus Pagh. Infinifilter: Expanding\nfilters to infinity and beyond. Proc. ACM Manag. Data , 1(2):140:1–140:27, 2023. URL:\nhttps://doi.org/10.1145/3589285 .\n19Martin Dietzfelbinger and Rasmus Pagh. Succinct data structures for retrieval and approximate\nmembership. In International Colloquium on Automata, Languages, and Programming , pages\n385–396. Springer, 2008.\n20Yihe Dong, Piotr Indyk, Ilya Razenshteyn, and Tal Wagner. Learning space partitions for\nnearest neighbor search. In International Conference on Learning Representations (ICLR) ,\n2020.\n21Devdatt P. Dubhashi and Alessandro Panconesi. Concentration of Measure for the Analysis\nof Randomized Algorithms . Cambridge University Press, USA, 2012.\n22Talya Eden, Piotr Indyk, Shyam Narayanan, Ronitt Rubinfeld, Sandeep Silwal, and Tal\nWagner. Learning-based support estimation in sublinear time. In International Conference on\nLearning Representations , 2020.\n23Tomer Even, Guy Even, and Adam Morrison. Prefix filter: Practically and theoretically better\nthan bloom. Proc. VLDB Endow. , 15(7):1311–1323, 2022.\n24Bin Fan, Dave G Andersen, Michael Kaminsky, and Michael D Mitzenmacher. Cuckoo filter:\nPractically better than bloom. In Proceedings of the 10th ACM International on Conference\non emerging Networking Experiments and Technologies , pages 75–88, 2014.\n25Li Fan, Pei Cao, Jussara Almeida, and Andrei Z Broder. Summary cache: a scalable wide-area\nweb cache sharing protocol. IEEE/ACM transactions on networking , 8(3):281–293, 2000.\n26Paolo Ferragina, Hans-Peter Lehmann, Peter Sanders, and Giorgio Vinciguerra. Learned\nmonotone minimal perfect hashing. In Inge Li Gørtz, Martin Farach-Colton, Simon J. Puglisi,\nand Grzegorz Herman, editors, 31st Annual European Symposium on Algorithms, ESA 2023,\nSeptember 4-6, 2023, Amsterdam, The Netherlands , volume 274 of LIPIcs, pages 46:1–46:17.\nSchloss Dagstuhl - Leibniz-Zentrum für Informatik, 2023. URL: https://doi.org/10.4230/\nLIPIcs.ESA.2023.46 ,doi:10.4230/LIPICS.ESA.2023.46 .\n27Paolo Ferragina, Fabrizio Lillo, and Giorgio Vinciguerra. Why are learned indexes so effective?\nIn Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference\non Machine Learning , volume 119 of Proceedings of Machine Learning Research . PMLR, 2020.\n28Paolo Ferragina and Giorgio Vinciguerra. Learned data structures. In Luca Oneto, Nicolò\nNavarin, Alessandro Sperduti, and Davide Anguita, editors, Recent Trends in Learning\nFrom Data - Tutorials from the INNS Big Data and Deep Learning Conference (INNSBDDL\n2019), volume 896 of Studies in Computational Intelligence , pages 5–41. Springer, 2019.\ndoi:10.1007/978-3-030-43883-8\\_2 .\n29Paolo Ferragina and Giorgio Vinciguerra. The pgm-index: a fully-dynamic compressed learned\nindex with provable worst-case bounds. Proceedings of the VLDB Endowment , 13(8):1162–1175,\n2020.\n30Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and Tim Kraska.\nFiting-tree: A data-aware index structure. In Proceedings International Conference on\nManagement of Data (SIGMOD) , pages 1189–1206, 2019.\n\n18 Daisy Bloom Filters\n31Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency estimation\nalgorithms. In International Conference on Learning Representations , 2019.\n32Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned\nindex structures. In Gautam Das, Christopher M. Jermaine, and Philip A. Bernstein, editors,\nProceedings of the 2018 International Conference on Management of Data (SIGMOD) , pages\n489–504. ACM, 2018.\n33Mingmou Liu, Yitong Yin, and Huacheng Yu. Succinct filters for sets of unknown sizes. arXiv\npreprint arXiv:2004.12465 , 2020.\n34Lailong Luo, Deke Guo, Richard T. B. Ma, Ori Rottenstreich, and Xueshan Luo. Optimiz-\ning bloom filter: Challenges, solutions, and comparisons. IEEE Commun. Surv. Tutorials ,\n21(2):1912–1949, 2019. doi:10.1109/COMST.2018.2889329 .\n35Samuel McCauley, Benjamin Moseley, Aidin Niaparast, and Shikha Singh. Online list labeling\nwith predictions, 2023. arXiv:2305.10536 .\n36Michael Mitzenmacher. A model for learned bloom filters and optimizing by sandwiching. In\nAdvances in Neural Information Processing Systems , volume 31. Curran Associates, Inc., 2018.\n37Michael Mitzenmacher, Salvatore Pontarelli, and Pedro Reviriego. Adaptive cuckoo filters.\nIn2018 Proceedings of the Twentieth Workshop on Algorithm Engineering and Experiments\n(ALENEX) , pages 36–47. SIAM, 2018.\n38Moni Naor and Noa Oved. Bet-or-pass: Adversarially robust bloom filters. In Theory of\nCryptography Conference , pages 777–808. Springer, 2022.\n39Moni Naor and Eylon Yogev. Bloom filters in adversarial environments. In Annual Cryptology\nConference , pages 565–584. Springer, 2015.\n40Anna Pagh, Rasmus Pagh, and S. Srinivasa Rao. An optimal Bloom filter replacement. In\nSODA, pages 823–829. SIAM, 2005.\n41Rasmus Pagh, Gil Segev, and Udi Wieder. How to approximate a set without knowing its\nsize in advance. In 2013 IEEE 54th Annual Symposium on Foundations of Computer Science ,\npages 80–89. IEEE, 2013.\n42Prashant Pandey, Michael A Bender, Rob Johnson, and Rob Patro. A general-purpose counting\nfilter: Making every bit count. In Proceedings of the 2017 ACM international conference on\nManagement of Data , pages 775–787, 2017.\n43Prashant Pandey, Alex Conway, Joe Durie, Michael A. Bender, Martin Farach-Colton, and\nRob Johnson. Vector quotient filters: Overcoming the time/space trade-off in filter design. In\nSIGMOD ’21: International Conference on Management of Data, Virtual Event, China, June\n20-25, 2021 , pages 1386–1399. ACM, 2021. doi:10.1145/3448016.3452841 .\n44Ely Porat. An optimal Bloom filter replacement based on matrix solving. In International\nComputer Science Symposium in Russia , pages 263–273. Springer, 2009.\n45Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ml\npredictions. Advances in Neural Information Processing Systems (NeurIPS) , 31, 2018.\n46MTCAJ Thomas and A Thomas Joy. Elements of information theory . Wiley-Interscience,\n2006.\n47Kapil Vaidya, Eric Knorr, Michael Mitzenmacher, and Tim Kraska. Partitioned learned bloom\nfilters. In 9th International Conference on Learning Representations (ICLR) . OpenReview.net,\n2021.\n48Xiujun Wang, Yusheng Ji, Zhe Dang, Xiao Zheng, and Baohua Zhao. Improved weighted\nbloom filter and space lower bound analysis of algorithms for approximated membership\nquerying. In Database Systems for Advanced Applications (DASFAA) , volume 9050 of Lecture\nNotes in Computer Science , pages 346–362. Springer, 2015.\nA Choices for δand γin the proof of Lemma 15\nIn the proof of Lemma 15, we define the event Bδto be when/parenleftbig\n1−1\nm/parenrightbigX>(1−δ)·1\n2and\nthe eventBγto be when ρ>(1−γ)/parenleftbig\n1−1\nm/parenrightbigX. We then claim that we can choose values\n\nI.O. Bercea, J.B.T. Houen, R. Pagh 19\n0<δ< 1and0<γ < 1such thatBδandBδ′occur with high probability and imply that\nρ≥1−21/log(1/ε)·1\n2.\nIn this section, we show such possible values for δandγand bound the probability of Bδand\nBγ|Bδ. Specifically, welet b=log(1/ε)anddefineδ≜21/(2b)−1andγ≜(1−2−1/(2b))·21/b−1.\nThenBδ∧Bγimply that\nρ>(1−γ)·(1−δ)1\n2= (1−γ)·/parenleftig\n2−21/(2b)/parenrightig1\n2= (1−γ)·/parenleftig\n1−21/(2b)−1/parenrightig\n= (1−γ)·/parenleftig\n1−2−1/(2b)·21/b−1/parenrightig\n≥1−(γ/21/b−1+ 2−1/(2b))·21/b−1= 1−21/b−1.\nWe now show that BδandBγ|Bδoccur with high probability.\nLetτ= 1/(2b) = 1/(2log(1/ε))and note that if X≤(1 +τ)·EPn[X], we get that for\nm≥(2b)4:\n/parenleftbigg\n1−1\nm/parenrightbiggX\n≥/parenleftbigg/parenleftbigg\n1−1\nm/parenrightbigg\n·1\ne/parenrightbiggX/m\n≥/parenleftbigg\n1−1\nm/parenrightbigg(1+τ) ln 2\n·2−1−τ\n≥/parenleftbigg\n1−1\nm/parenrightbigg2\n·2−1−τ≥/parenleftbigg\n1−2\nm/parenrightbigg\n·2−1−τ≥/parenleftbigg\n1−2\n(2b)4/parenrightbigg\n·2−1−τ\n≥(1−δ)·2τ·2−1−τ= (1−δ)1\n2,\nwhere the inequality 1−2/(2b)4≥(1−δ)·2τ= (2−21/(2b))·21/(2b)follows from the fact that\n1−2/x4>(2−21/x)·21/xforx= 2b>2. In other words, we have that X≤(1+τ)·EPn[X]\nimpliesBδ, and since we assume that X≤(1 +τ)·EPn[X]holds then Bδis also true. For\nbounding the probability of Bγ|Bδ, we have that:\nPrA[¬Bγ|Bδ]≤exp/parenleftbig\n−m(1−δ)γ2/4/parenrightbig\n≤exp/parenleftbig\n−m(1−δ)γ2/4/parenrightbig\n≤exp/parenleftbig\n−Θ(γ2m)/parenrightbig\n,(3)\nwhere 1−δ= 2−21/2b≥2−√\n2sinceb>1. We then get that\n2γ= (1−2−1/(2b))·21/b>1/(2b)2,\nsince (1−2−1/x)·22/x>1/x2for allx= 2b>0. Finally, we have that:\nPrA[Bγ|Bδ]≥1−exp/parenleftbigg\n−m\nΘ(log4(1/ε))/parenrightbigg\n. (4)",
  "textLength": 59217
}