{
  "paperId": "35c5ae25b6927c0d370cef6600e35fbbe4073fd0",
  "title": "NeuroCard",
  "pdfPath": "35c5ae25b6927c0d370cef6600e35fbbe4073fd0.pdf",
  "text": "NeuroCard: One Cardinality Estimator for All Tables\nZongheng Yang\nUC Berkeley\nzongheng@berkeley.eduAmog Kamsettyâˆ—\nUC Berkeley\namogkamsetty@berkeley.eduSifei Luanâˆ—\nUC Berkeley\nlsf@berkeley.edu\nEric Liang\nUC Berkeley\nericliang@berkeley.eduYan Duan\nXi Chen\nCovariant\n{rocky,peter}@covariant.aiIon Stoica\nUC Berkeley\nistoica@berkeley.edu\nABSTRACT\nQuery optimizers rely on accurate cardinality estimates to pro-\nduce good execution plans. Despite decades of research, existing\ncardinality estimators are inaccurate for complex queries, due to\nmaking lossy modeling assumptions and not capturing inter-table\ncorrelations. In this work, we show that it is possible to learn the cor-\nrelations across all tables in a database without any independence\nassumptions. We present NeuroCard, a join cardinality estimator\nthat builds a single neural density estimator over an entire database.\nLeveraging join sampling and modern deep autoregressive models,\nNeuroCard makes no inter-table or inter-column independence as-\nsumptions in its probabilistic modeling. NeuroCard achieves orders\nof magnitude higher accuracy than the best prior methods (a new\nstate-of-the-art result of 8.5Ã—maximum error on JOB-light ), scales\nto dozens of tables, while being compact in space (several MBs) and\nefficient to construct or update (seconds to minutes).\nPVLDB Reference Format:\nZongheng Yang, Amog Kamsetty, Sifei Luan, Eric Liang, Yan Duan, Xi\nChen, and Ion Stoica. NeuroCard: One Cardinality Estimator for All Tables.\nPVLDB, 14(1): 61 - 73, 2021.\ndoi:10.14778/3421424.3421432\nPVLDB Artifact Availability:\nThe source code, data, and/or other artifacts have been made available at\nhttps://github.com/neurocard.\n1 INTRODUCTION\nQuery optimizers translate queries into executable plans with the\nbest estimated performance. They are critical not only for relational\ndatabases, but also for modern analytics engines, such as Spark [ 1]\nand Presto [ 36]. Among various techniques, cardinality estimation\noften plays a larger role than the cost model or the plan search\nspace in producing high-quality query plans [ 19]. Unfortunately,\ncardinality estimation is a notoriously difficult problem, where the\naccuracy may drop exponentially as the query complexity (e.g., the\nnumber of joins) increases [21].\nâˆ—Equal contribution.\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license. For any use beyond those covered by this license, obtain permission by\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 14, No. 1 ISSN 2150-8097.\ndoi:10.14778/3421424.3421432\nNeuroCardCardinality?50Cardinality?1000JoinedNot joinedP(all tables)\n<latexit sha1_base64=\"quhJtApYKx4j0ILn8og/98pKl/w=\">AAAChXicbVFdSxtBFJ1sbdXth7H65svQUIiQht0ibd8qWNAHHyI2akmWcHdyNw6ZnV1m7pbEJf/F1/qP/DfOxggm9sLA4Zz7MffcOFfSUhDc17xXa6/frG9s+m/fvf+wVd/+eGGzwgjsikxl5ioGi0pq7JIkhVe5QUhjhZfx+KjSL/+isTLTv2maY5TCSMtECiBHDeq7nWafcEIlKMUJXJmd7Q/qjaAdzIO/BOECNNgiOoPt2p/+MBNFipqEAmt7YZBTVIIhKRTO/H5hMQcxhhH2HNSQoo3K+fdn/LNjhjzJjHua+Jx9XlFCau00jV1mCnRtV7WK/J/WKyj5EZVS5wWhFo+DksLtmfHKCz6UBgWpqQMgjHR/5eIaDAhyjvn+0px59xzF0i7lpNBSZENcYRVNyIAjLVIKUld7lcfSWXwO2j7xrmElNH/JkSTbOnUn0a1jgzjef5bsjhGuWv8SXHxthw6fHTQO+eIsG2yPfWJNFrLv7JCdsA7rMsFu2C37x+68de+Ld+B9e0z1aouaHbYU3s8HECXGQA==</latexit><latexit sha1_base64=\"quhJtApYKx4j0ILn8og/98pKl/w=\">AAAChXicbVFdSxtBFJ1sbdXth7H65svQUIiQht0ibd8qWNAHHyI2akmWcHdyNw6ZnV1m7pbEJf/F1/qP/DfOxggm9sLA4Zz7MffcOFfSUhDc17xXa6/frG9s+m/fvf+wVd/+eGGzwgjsikxl5ioGi0pq7JIkhVe5QUhjhZfx+KjSL/+isTLTv2maY5TCSMtECiBHDeq7nWafcEIlKMUJXJmd7Q/qjaAdzIO/BOECNNgiOoPt2p/+MBNFipqEAmt7YZBTVIIhKRTO/H5hMQcxhhH2HNSQoo3K+fdn/LNjhjzJjHua+Jx9XlFCau00jV1mCnRtV7WK/J/WKyj5EZVS5wWhFo+DksLtmfHKCz6UBgWpqQMgjHR/5eIaDAhyjvn+0px59xzF0i7lpNBSZENcYRVNyIAjLVIKUld7lcfSWXwO2j7xrmElNH/JkSTbOnUn0a1jgzjef5bsjhGuWv8SXHxthw6fHTQO+eIsG2yPfWJNFrLv7JCdsA7rMsFu2C37x+68de+Ld+B9e0z1aouaHbYU3s8HECXGQA==</latexit><latexit sha1_base64=\"quhJtApYKx4j0ILn8og/98pKl/w=\">AAAChXicbVFdSxtBFJ1sbdXth7H65svQUIiQht0ibd8qWNAHHyI2akmWcHdyNw6ZnV1m7pbEJf/F1/qP/DfOxggm9sLA4Zz7MffcOFfSUhDc17xXa6/frG9s+m/fvf+wVd/+eGGzwgjsikxl5ioGi0pq7JIkhVe5QUhjhZfx+KjSL/+isTLTv2maY5TCSMtECiBHDeq7nWafcEIlKMUJXJmd7Q/qjaAdzIO/BOECNNgiOoPt2p/+MBNFipqEAmt7YZBTVIIhKRTO/H5hMQcxhhH2HNSQoo3K+fdn/LNjhjzJjHua+Jx9XlFCau00jV1mCnRtV7WK/J/WKyj5EZVS5wWhFo+DksLtmfHKCz6UBgWpqQMgjHR/5eIaDAhyjvn+0px59xzF0i7lpNBSZENcYRVNyIAjLVIKUld7lcfSWXwO2j7xrmElNH/JkSTbOnUn0a1jgzjef5bsjhGuWv8SXHxthw6fHTQO+eIsG2yPfWJNFrLv7JCdsA7rMsFu2C37x+68de+Ld+B9e0z1aouaHbYU3s8HECXGQA==</latexit><latexit sha1_base64=\"quhJtApYKx4j0ILn8og/98pKl/w=\">AAAChXicbVFdSxtBFJ1sbdXth7H65svQUIiQht0ibd8qWNAHHyI2akmWcHdyNw6ZnV1m7pbEJf/F1/qP/DfOxggm9sLA4Zz7MffcOFfSUhDc17xXa6/frG9s+m/fvf+wVd/+eGGzwgjsikxl5ioGi0pq7JIkhVe5QUhjhZfx+KjSL/+isTLTv2maY5TCSMtECiBHDeq7nWafcEIlKMUJXJmd7Q/qjaAdzIO/BOECNNgiOoPt2p/+MBNFipqEAmt7YZBTVIIhKRTO/H5hMQcxhhH2HNSQoo3K+fdn/LNjhjzJjHua+Jx9XlFCau00jV1mCnRtV7WK/J/WKyj5EZVS5wWhFo+DksLtmfHKCz6UBgWpqQMgjHR/5eIaDAhyjvn+0px59xzF0i7lpNBSZENcYRVNyIAjLVIKUld7lcfSWXwO2j7xrmElNH/JkSTbOnUn0a1jgzjef5bsjhGuWv8SXHxthw6fHTQO+eIsG2yPfWJNFrLv7JCdsA7rMsFu2C37x+68de+Ld+B9e0z1aouaHbYU3s8HECXGQA==</latexit>learned in a single deep autoregressive modelT ableFigure 1: NeuroCard uses a single probabilistic model, which learns\nall possible correlations among all tables in a database, to estimate\njoin queries on any subset of tables.\nAt a high level, there are two approaches to cardinality estima-\ntion: query-driven anddata-driven . Query-driven estimators typ-\nically rely on supervised learning to learn a function mapping\n(featurized) queries to predicted cardinalities. They implicitly as-\nsume queries from a production workload are â€œsimilarâ€ to training\nqueriesâ€”namely, training and test sets of queries are drawn from\nthe same underlying distribution. This assumption can be violated\nwhen, for example, users issue unexpected types of queries.\nIn contrast, data-driven estimators approximate the data distri-\nbution of a tableâ€”a function mapping each tuple to its probability\nof occurrence in the tableâ€”instead of training on â€œrepresentativeâ€\nqueries. A simple method to approximate the data distribution is\na histogram. In theory, once we estimate the distribution of each\ntable in a schema, we can estimate the output cardinality of any\nquery. While this approach is more general, it suffers from two\ndrawbacks: (1) lossy modeling assumptions (e.g., assume the tablesâ€™\ndistributions are independent), and (2) low precision (e.g., a limited\nnumber of histogram bins). Fortunately, recent advances in ma-\nchine learning have alleviated both drawbacks. Unlike previous\ndensity estimators, deep autoregressive (AR) models [4,6,32,33,42]\ncan learn complex high-dimensional data distributions without in-\ndependence assumptions, achieving state-of-the-art results in both\nprecision and expressiveness. This has resulted in new data-driven\ncardinality estimators based on deep AR models [48].\nHowever, despite their promise, deep autoregressive model-based\ncardinality estimators are limited to handling single tables. There\nare three challenges that make this approach ineffective for joins :\nâ€¢High training cost: To learn the distribution of a join, any data-\ndriven estimator needs to see actual tuples from the join result.\nUnfortunately, for all but the smallest scale, it is expensive, and\nsometimes infeasible, to precompute the join.\nâ€¢Lack of generality: The AR approach builds a probabilistic\nmodel for each join, e.g., ğ‘‡=ğ‘‡1âŠ²âŠ³ğ‘‡2âŠ²âŠ³ğ‘‡3, that it estimates.arXiv:2006.08109v2  [cs.DB]  2 Nov 2020\n\nHowever, the model for ğ‘‡cannot be directly used to estimate a\njoin on a subset of ğ‘‡, e.g.,ğ‘‡2âŠ²âŠ³ğœ(ğ‘‡3). Of course, one could train\na model for every possible join. This can be prohibitive, as the\nnumber of possible joins is exponential in the number of tables.\nâ€¢Large model size: The complexity of the learned AR model\ngrows with the cardinality of the dataset. As joins tend to in-\nvolve columns with high cardinalities, an AR model built on a\njoin may incur a prohibitively large size.\nIn this work, we propose NeuroCard, a learning-based join car-\ndinality estimator that directly learns from data to overcome these\nchallenges. NeuroCardâ€™s distinctive feature is the ability to capture\nthe correlations across multiple joins in a single deep AR model,\nwithout any independence assumptions (Figure 1). Once trained,\nthis model can handle all queries issuable to the schema, regard-\nless of what subset of tables is involved. We address the above\nchallenges using the following key ingredients.\nTo reduce training cost, NeuroCard samples from a join, instead\nof computing the join fully (Â§4). The key property of such a sample is\nto capture the joinâ€™s distribution: if a key is more frequent in the join\nresult, it should be more frequent in the sample as well. To meet this\nrequirement, we precompute the correct sampling weights for each\nkey. While the worst-case cost of computing the join is exponential\nin the number of tables, computing the sampling weights is done\nin time linear with the data size by dynamic programming.\nTo achieve generality, NeuroCard needs to train a single model to\nanswer queries on any subset of tables (Â§6). The basic idea behind\nour solution is to train the AR model on samples from the full\nouter join of all tables. The full join contains the values from all\nthe base tables, so it has sufficient information to answer a query\ntouching any subset of tables. At inference time, if a table in the\nschema is not present in a join query, we need to account for any\npotential fanout effect. Consider an AR model trained on samples\nfrom the full join ğ‘‡=ğ‘‡1âŠ²âŠ³ğ‘‡2, and a query ğœ(ğ‘‡1)whose cardinality\nwe want to estimate. If the join key of ğ‘‡2is the foreign key of\nğ‘‡1, then a tuple of ğ‘‡1may appear multiple times inğ‘‡. NeuroCard\nlearns the probabilities of these â€œduplicatedâ€ tuples and additional\nbookkeeping information, which enables us to account for fanouts.\nFinally, to scale to large-cardinality columns while avoiding pro-\nhibitively large models, NeuroCard employs lossless column factor-\nization (Â§5). An AR model stores one embedding vector per distinct\nvalue, so it could quickly blow up in size for columns with large\nnumbers of distinct values, e.g., 100,000s or more. With factoriza-\ntion, a column is decomposed into several subcolumns, each taking\na chunk of bits from the binary representation of the original col-\numn values. For instance, a 32-bit ID column idcan be decomposed\ninto(id0,..., id3)with the first subcolumn corresponding to the\nfirst 8 bits, and so on. We then train the autoregressive model on\nthese lower-cardinality subcolumns instead of the full columns.\nBy combining these ingredients, NeuroCard achieves state-of-\nthe-art estimation accuracy, including in the challenging tail quan-\ntiles. On the popular JOB-light benchmark, a schema that contains\n6 tables and basic filters, NeuroCard achieves a maximum Q-error\nof8.5Ã—using 4MB. This corresponds to a 4.6Ã—improvement over\nthe previous state of the art. We created a more difficult benchmark,\nJOB-light-ranges , with a larger variety of content columns and\nrange filters. On this benchmark, NeuroCard achieves up to 15â€“34 Ã—\nStorage \nStorage \nT ables \nT1 Tn. . .Join Sampler Autoregressive \nCore \nIndexes Unbiased \nSampler \nJoin Count T ables \nindexed \nlookup base \ntuples \nJoin \nSchema prepare Learned \nDistribution \npğœƒ(all tables) \ntuples from join Inference \nAlgorithms Figure 2: Overview of NeuroCard. The Join Sampler (Â§4) provides\ncorrect training data (sampled tuples from join) by using unbi-\nased join counts. Sampled tuples are streamed to an autoregressive\nmodel for maximum likelihood training (Â§3). Inference algorithms\n(Â§6) use the learned distribution to estimate query cardinalities.\nhigher accuracy than previous solutions, including DeepDB [ 12],\nMSCN [ 15], and IBJS [ 20]. Lastly, to test NeuroCardâ€™s ability to\nhandle a more complex join schema, we created JOB-M which has\n16 tables and multi-key joins. NeuroCard scales well to this bench-\nmark, offering 10Ã—higher accuracy than conventional approaches\nwhile maintaining a low model size ( 27MB, covering 16 tables).\nIn summary, this paper makes the following contributions:\nâ€¢We design and implement NeuroCard, the first learned data-\ndriven cardinality estimator that learns across joins in a schema\nwithout any independence assumptions. All in-schema correla-\ntions among the tables are captured by a single autoregressive\nmodel, which can estimate any query on any subset of tables.\nâ€¢NeuroCard learns the correct distribution of a join without actu-\nally computing that join. Instead, the model is trained on uniform\nand independent samples of the join of all tables in a schema.\nâ€¢We propose lossless column factorization (Â§5), a technique that\nsignificantly reduces the size of the autoregressive model, making\nits use practical for high-cardinality columns.\nâ€¢Compared to the best prior methods, NeuroCard significantly im-\nproves the state-of-the-art accuracy on the JOB-light benchmark.\nWe further propose two new benchmarks, JOB-light-ranges and\nJOB-M , and show that both are much more challenging and thus\nbetter gauges of estimator quality (Â§7).\nTo invite further research, NeuroCard and the benchmarks used in\nthis paper are open source at https://github.com/neurocard.\n2 OVERVIEW OF NEUROCARD\nConsider a set of tables, ğ‘‡1,...,ğ‘‡ğ‘. We define their join schema\nas the graph of join relationships, where vertices are tables, and\neach edge connects two joinable tables. A query is a subgraph of\nthe overall schema. If a query joins a table multiple times, our\nframework duplicates that table in the schema. We assume the\nschema and queries submitted to the estimator are acyclic (Â§4.2\ndiscusses relaxations), so they can be viewed as trees.\nNext, we present an overview of NeuroCard as a sequence of\ngoals and solutions to achieve these goals.\n\n2.1 Goals and Solutions\nGoal: A single estimator . Our goal is building a single cardinality\nestimator for the entire join schema. For example, assuming the\nschema has three tables, the estimator should handle joins on any\nsubset of tables, e.g., ğœ(ğ‘‡2),ğ‘‡1âŠ²âŠ³ğ‘‡3, orğ‘‡1âŠ²âŠ³ğ‘‡2âŠ²âŠ³ğœ(ğ‘‡3).\nHaving a single estimator has two key benefits: simplicity and\naccuracy. Having multiple estimatorsâ€”each covering a specific join\ntemplate (a table subset)â€”does not scale for a large number of tables,\nas the number of possible join templates increases exponentially.\nIn addition, it is easier for a DBMS to operationalize a single es-\ntimator rather than many estimators. Most importantly, having\nmultiple estimators can hurt accuracy. This is because estimating\nthe cardinality of a query on a table subset not covered by any\nsingle estimator, but by multiple estimators, requires some form\nof independence assumption to combine these estimators. If the\nindependence assumption does not hold, the accuracy will suffer.\nSolution: We build a single cardinality estimator that learns\nthe distribution of the full outer join of all tables in the schema\n(henceforth, full join). For example, for a three-table schema, we\nlearnğ‘(ğ‘‡1âŠ²âŠ³ğ‘‡2âŠ²âŠ³ğ‘‡3). Note that using the inner join instead of the\nfull join would not work. Indeed, the inner join ğ‘‡1âŠ²âŠ³ğ‘‡2âŠ²âŠ³ğ‘‡3is the\nintersection of the three tables. If a query uses only ğ‘‡1orğ‘‡1âŠ²âŠ³ğ‘‡3,\ntheir tuples may not be fully contained in this intersection, and\nthus the estimator would have insufficient information to answer\nthese queries.\nGoal: Efficient sampling of the full join . A data-driven estima-\ntor learns a distribution by reading representative tuples from that\ndistribution. To learn the distribution of the full join, a straightfor-\nward approach is to compute it and then uniformly draw random\nsamples from the result. Unfortunately, even on a small 6-table\nschema (the JOB-light workload), the full join contains two trillion\n(2Â·1012) tuples, making it infeasible to compute in practice.\nSolution: We perform uniform sampling over the full join with-\noutmaterializing it. Specifically, we ensure that any tuple in the full\njoinğ½(a multiset) is sampled with same probability, 1/|ğ½|. To achieve\nthis, we leverage a state-of-the-art join sampling algorithm [ 50]\n(Â§4). We first precompute join count tables that map each tableâ€™s join\nkeys to their correct sampling weights with respect to the full join.\nThen, we sample the keys using these counts as weights. Given a\nsampled key, we construct the full tuple by looking up the remain-\ning columns via indexes1from all tables, and then concatenating\nthem. This way, we only need to materialize the join counts as\nopposed to the full join. Using dynamic programming, computing\nthe join counts takes time linear in the size of the database, and is\nquite fast in practice (e.g., 13 seconds for 6 tables in JOB-light , and\n4 minutes for 16 tables in JOB-M ).\nGoal: Support any subset of tables . Although the full outer join\ncontains all information of the tables, we need to take care when a\nquery involves just a subset of the tables. Consider:\nğ‘‡1.id:[1,2]ğ‘‡2.id:[1,1] âˆ’â†’ğ‘‡1âŠ²âŠ³ğ‘‡2:[(1,1),(1,1),(2,âˆ…)]\nQuery:ğœid=1(ğ‘‡1)\n1Like prior work on join sampling [ 20,22], we assume base tables have an index built\nfor each join key. This impacts the efficiency but not correctness of the design.The correct selectivity is1\n2(1 row). However, in the full join dis-\ntribution,ğ‘ƒ(ğ‘‡1.id=1)=2\n3(2 rows). This is because we have not\naccounted for the fanout produced by the missing table, ğ‘‡2.\nSolution: Handle schema subsetting : If a query does not include\na table, we downscale the estimate by the fanout introduced by that\ntable. In essence, since the learned probability space is the full join,\nwe must downscale appropriately when a query touches a subset\nand expects the returned selectivity to refer to that subset.\nGoal: Accurate density estimation . The final ingredient to achieve\nour goal is an accurate and compact density estimator.\nSolution: We leverage deep autoregressive (AR) models to imple-\nment our density estimator. This family of neural density estimators\nhave been successfully employed on high-dimensional data types\nsuch as image [ 33], audio [ 42], and text [ 32]. Recently, Naru [ 48]\nhas leveraged deep AR models to achieve state-of-the-art accuracy\nresults on estimating the cardinalities of single-table queries, while\nlearning the correlations among all columns without independence\nassumptions. We apply Naru to learn the distribution of the full\njoin, and optimize its construction and inference for our setting.\n2.2 Putting It All Together\nFigure 2 shows the high-level architecture of NeuroCard.\nBuilding the estimator consists of two stages. First, we prepare\nthe join sampler by building or loading existing single-table indexes\non join keys and computing the join count tables for the specified\njoin schema (Â§4). Second, we train the deep AR model by repeatedly\nrequesting batches of sampled tuples from the sampler, usually 2K\ntuples at a time. The sampler fulfills this request in the background,\npotentially using multiple sampling threads.\nOnce the estimator is built, it is ready to compute the cardinality\nestimates for given queries. For each query, we use probabilistic\ninference algorithms (Â§6) to compute the cardinality estimate by\n(1) performing Monte Carlo integration on the learned AR model,\nand (2) handling schema subsetting. A single estimator can handle\nqueries joining any subset of tables, with arbitrary range selections.\n3 CONSTRUCTING NEUROCARD\nIn this section, we present the background of the techniques used\nto implement NeuroCard.\n3.1 Probabilistic Modeling of Tables\nConsider a table ğ‘‡with column domains {ğ´1,...,ğ´ğ‘›}. This table\ninduces a discrete joint data distribution , defined as the probability\nof occurrence of each tuple ( ğ‘“(Â·)denotes number of occurrences):\nğ‘(ğ‘1,...,ğ‘ğ‘›)=ğ‘“(ğ‘1,...,ğ‘ğ‘›)/|ğ‘‡|.\nTheğ‘›-dimensional data distribution (the joint )ğ‘(Â·)allows us to\ncompute a queryâ€™s cardinality as follows. Define a query ğ‘„asğœ:\nğ´1Ã—Â·Â·Â·Ã—ğ´ğ‘›â†’{0,1}. Then, the selectivity â€”the fraction of records\nthat satisfy the queryâ€”can be computed as a probability: ğ‘ƒ(ğ‘„)=Ã\nğ‘1âˆˆğ´1Â·Â·Â·Ã\nğ‘ğ‘›âˆˆğ´ğ‘›ğœ(ğ‘1,...,ğ‘ğ‘›)Â·ğ‘(ğ‘1,...,ğ‘ğ‘›). The cardinality is\nobtained by multiplying it with the row count: |ğ‘„|=ğ‘ƒ(ğ‘„)Â·|ğ‘‡|.\nData-driven cardinality estimators can be grouped along two\naxes: (1) joint factorization, and (2) the density estimator used.\n\nJoint factorization , or the modeling assumption, determines\nhow precisely data distribution ğ‘is factored. Any modeling assump-\ntion risks losing information about correlations across columns,\nwhich ultimately leads to a loss in accuracy. For example, the widely\nused ID histogram technique assumes the columns are independent.\nAs a result, it factors ğ‘into a set 1D marginals, ğ‘â‰ˆÃğ‘›\nğ‘–=1ğ‘(ğ´ğ‘–),\nwhich can lead to large inaccuracies when the columnsâ€™ values\nare strongly correlated. Similarly, other data-driven cardinality es-\ntimators such as graphical models [ 3,7,8,40,41] either assume\nconditional independence or partial independence among columns.\nOne exception is the autoregressive (product-rule) factorization,\nğ‘=ğ‘›Ã–\nğ‘–=1ğ‘(ğ´ğ‘–|ğ´<ğ‘–), (1)\nwhich precisely expresses the overall joint distribution as the prod-\nuct of theğ‘›conditional distributions.\nThe density estimator determines how precisely the afore-\nmentioned factors are actually approximated. The most accurate\nâ€œestimatorâ€ would be recording these factors exactly in a hash table.\nUnfortunately, this leads to enormous construction and inference\ncosts (e.g., in the case of ğ‘(ğ´ğ‘›|ğ´1:ğ‘›âˆ’1)). At the other end, the 1D his-\ntogram has low costs, but this comes at the expense of low precision,\nas it makes no distinction between the values falling in the same\nbin. Over the years, a plethora of solutions have been proposed, in-\ncluding kernel density estimators and Bayesian networks. Recently,\ndeep autoregressive (AR) models [4,32,33] have emerged as the\ndensity estimator of choice. Deep AR models compute {ğ‘(ğ´ğ‘–|ğ´<ğ‘–)}\nwithout explicitly materializing them by learning the ğ‘›conditional\ndistributions in compact neural networks. Deep AR models achieve\nstate-of-the-art precision, and, for the first time, provide a tractable\nsolution for implementing the autoregressive factorization.\n3.2 Naru: Deep Autoregressive Models as\nCardinality Estimators\nNeuroCard builds on Naru, a state-of-the-art cardinality estimator\nthat fully captures the correlations among all columns of a single\ntable using a deep AR model. Next, we present an overview of Naru\nand discuss how NeuroCard leverages it.\nConstruction. Given tableğ‘‡, an AR model ğœƒtakes a tuple ğ’™âˆˆğ‘‡as\ninput, and predicts conditional probability distributions, {ğ‘ğœƒ(ğ‘‹ğ‘–|ğ’™<ğ‘–)},\neach of which is an 1D distribution over the ğ‘–-th column (condi-\ntioned on all prior column values of ğ’™). The likelihood of the input\ntuple is then predicted as ğ‘ğœƒ(ğ’™)=Ãğ‘›\nğ‘–=1ğ‘ğœƒ(ğ‘‹ğ‘–=ğ’™ğ‘–|ğ’™<ğ‘–). Any deep\nAR architecture can instantiate this framework, e.g., ResMADE [ 4]\nor Transformer [ 43]. Training aims to approximate the data distribu-\ntionğ‘usingğ‘ğœƒ, by minimizing the KL divergence [ 26],ğ·ğ¾ğ¿(ğ‘||ğ‘ğœƒ).\nThis is achieved by maximum likelihood estimation (MLE) and\ngradient ascent to maximize the predicted (log-)likelihood of data:\nSample i.i.d. ğ’™âˆ¼ğ‘ (2)\nTake gradient steps to maximize logğ‘ğœƒ(ğ’™) (3)\nIn our setting, we define ğ‘‡as the full outer join of all tables within\na schema. Consequently, the deep AR model learns the correlations\nacross all tables. Next, we need to sample tuples with probabilities\nprescribed by ğ‘. Otherwise, ğ‘ğœƒwould approximate an incorrect,\nResidual Block MaskedLinearMaskedLinearx1xnPer-colEmbedInputMaskingP!(Xi|x<i)Figure 3: Default architecture of the autoregressive model.\nbiased distribution. To achieve this, we use a sampler that emits\nsimple random samples from the full join ğ‘‡(Â§4).\nEstimating query cardinalities. Once constructed, the Naru es-\ntimator estimates the cardinality of a given query. A query is rep-\nresented as a hyper-rectangle: each column ğ‘‹ğ‘–with domain ğ´ğ‘–is\nconstrained to take on values in a valid region ğ‘…ğ‘–âŠ†ğ´ğ‘–:\nQuery:âˆ§{ğ‘‹ğ‘–âˆˆğ‘…ğ‘–} (4)\nNext, Naru estimates the probability of the query (an event) using\na Monte Carlo integration algorithm, progressive sampling :\nProgressiveSampling( {ğ‘‹ğ‘–âˆˆğ‘…ğ‘–}):ğ‘ğœƒ(âˆ§{ğ‘‹ğ‘–âˆˆğ‘…ğ‘–})Â·|ğ‘‡|(5)\nIt works by drawing imaginary, in-region tuples from the modelâ€™s\nlearned distributions. Specifically, it draws the first dimension of\nthe sample as ğ‘¥1âˆ¼ğ‘ğœƒ(ğ‘‹1|ğ‘‹1âˆˆğ‘…1), the second dimension of the\nsample asğ‘¥2âˆ¼ğ‘ğœƒ(ğ‘‹2|ğ‘‹2âˆˆğ‘…2;ğ‘¥1), and so on. The likelihoods of the\nsamples are importance-weighted. This procedure also efficiently\nsupports omitted columns, i.e., wildcards of the form ğ‘‹ğ‘–âˆˆâˆ—.\nNeuroCardâ€™s inference invokes progressive sampling to estimate\ncardinalities, but extends it in two ways. First, we apply the column\nfactorization optimization (Â§5), which potentially changes a ğ‘‹ğ‘–+ğ‘–â€™s\nvalid region, ğ‘…ğ‘–+1, based on the value drawn from ğ‘‹ğ‘–. Second, we\nadd support for schema subsetting (Â§6), by downscaling selectivity\nğ‘ğœƒ(âˆ§{ğ‘‹ğ‘–âˆˆğ‘…ğ‘–})by the corresponding fanout.\n3.3 Join Problem Formulation\nA join schema induces the full outer join of all tables in the schema,\nğ‘‡=ğ‘‡1âŠ²âŠ³Â·Â·Â·âŠ²âŠ³ğ‘‡ğ‘. Our goal is to build a fully autoregressive\nprobabilistic model on the full join consisting of all tablesâ€™ columns:\nModel:ğ‘ğœƒ(ğ‘‡)â‰¡ğ‘ğœƒ(ğ‘‡1.col1,ğ‘‡1.col2,...,ğ‘‡ğ‘.colğ‘˜) (6)\nWe can then use the probabilistic model to estimate the cardinalities\nof join queries on any subset of tables in the schema.\nSupported joins. NeuroCard supports acyclic join schemas and\nqueries containing multi-way, multi-key equi-joins (Â§4.2 discusses\nhow to relax the acyclic requirement). The schema should capture\nthe most common joins. For joins not captured in the schema,\ntheir cardinalities can be estimated by first obtaining single-table\nestimates using NeuroCard, then combining the estimates using\nclassical heuristics [ 21]. This allows uncommon cases to be handled\nunder the same framework, albeit at the cost of lower accuracy.\nSupported filters. NeuroCard supports equality and range filters\non discrete or numerical columns. These include arithmetic com-\nparison operators ( <,>,â‰¤,â‰¥,=) and IN. More complex filters can\nalso be expressed using the valid region encoding, mentioned in\nthe previous section. Arbitrary forms of AND /ORcan be handled\nvia the inclusion-exclusion principle.\n\n3.4 Model architecture\nNeuroCard uses a standard AR architecture, ResMADE [ 4], which\nis also employed by Naru; see Figure 3. Input tuples are represented\nas discrete, dictionary-encoded IDs, (ğ‘¥1,...,ğ‘¥ğ‘›), and embedded\nby per-column embedding matrices. The concatenated embedded\nvector is fed to a series of residual blocks, each consisting of two\nmasked linear layers (they are masked to ensure the autoregressive\nproperty). The output layer produces logits {logğ‘ğœƒ(ğ‘‹ğ‘–|ğ’™<ğ‘–)}by\ndotting the last layerâ€™s output with the embedding matrices. Next,\nwe compute a cross-entropy loss on the logits and perform back-\npropagation. We turn on Naruâ€™s wildcard skipping optimization,\nwhich randomly masks inputs to train special marginalization to-\nkens that aid infer-time estimation (i.e., using these tokens to skip\nsampling any wildcards in a query).\nMasked multi-layer perceptrons such as ResMADE strike a good\nbalance between efficiency and accuracy. NeuroCard can use any\nadvanced AR architectures, if desired. In Â§7, we also instantiate\nNeuroCard with an advanced architecture (the Transformer [ 43]).\n4 SAMPLING FROM JOINS\nA key challenge in NeuroCard is computing an unbiased sam-\nple of the full join (Â§2.1) to ensure that the learned distribution\nfaithfully approximates the full join distribution. Namely, every\ntuple in the full join ğ½(a multiset) must be sampled equally likely\nwith probability 1/|ğ½|. The samples should also be i.i.d., as required\nby Equation 2. NeuroCard meets these requirements by using a\nsampler that produces simple random samples with replacement .\n4.1 Algorithm\nA tuple in the full join contains join key columns andcontent columns .\nOur sampler exploits this decomposition. The first step of the sam-\npler is to precompute join count tables , which are per-table statistics\nthat reflect the occurrence counts of the join keys in the full join.\nThe sampler then samples the join keys, table-by-table, with oc-\ncurrence probabilities proportional to their join counts. Lastly, it\nselects content columns from the base tables by looking up the\ndrawn join keys. This completes a batch of sample, which is sent\nto the model for training, and the procedure repeats on demand.\nComputing join counts. Zhao et al. [50] provide an efficient algo-\nrithmic framework of join sampling that produces simple random\nsamples from general multi-key joins. NeuroCard implements the\nExact Weight algorithm from Zhao et al. , adapted to full outer joins.\nWe illustrate the algorithm on a join schema (a tree) consisting\nof tablesğ‘‡1,...,ğ‘‡ğ‘. For exposition, assume they only involve join\nkeys (content columns are gathered later). Let ğ‘‡1be the root table.\nThe join count of a tuple ğ‘¡âˆˆğ‘‡ğ‘–is the total number of tuples in\nthe full outer join of all of ğ‘‡ğ‘–â€™s descendants that joins with ğ‘¡. It is\nrecursively defined as:\nğ‘¤ğ‘–(ğ‘¡)=Ã–\nğ‘‡ğ‘—âˆˆChildren(ğ‘‡ğ‘–)âˆ‘ï¸\nğ‘¡â€²âˆˆğ‘¡â‹Šğ‘‡ğ‘—ğ‘¤ğ‘—(ğ‘¡â€²) âˆ€ğ‘–,âˆ€ğ‘¡âˆˆğ‘‡ğ‘–(7)\nwhereğ‘¡â‹Šğ‘‡ğ‘—denotes all tuples in ğ‘‡ğ‘—that join with ğ‘¡. For a leaf\ntable with no descendants, ğ‘¤ğ‘–(Â·)is defined as 1. At the root table ğ‘‡1,\nğ‘¤1(ğ‘¡)represents the count of all ğ‘¡âˆˆğ‘‡1in the entire full outer join.\nThe join counts of each table are computed by aggregating over\nthe join counts of all of its child tables, and can thus be computedrecursively in a bottom-up fashion. Using dynamic programming,\nthe time complexity is linear in the number of tuples in all tables,\nğ‘‚(|ğ‘‡1|+Â·Â·Â·+|ğ‘‡ğ‘|).\nSampling. Once the join counts are computed, the sampler pro-\nduces a sample by traversing the join tree in a top-down fashion. It\nstarts by drawing a sample ğ‘¡1from the root table ğ‘‡1using weights\n{ğ‘¤1(ğ‘¡):ğ‘¡âˆˆğ‘‡1}(i.e., with probabilities {ğ‘¤1(ğ‘¡)/Ã\nğ‘¡â€²âˆˆğ‘‡1ğ‘¤1(ğ‘¡â€²)}).\nIt then samples through all descendants of ğ‘‡1in the breadth-first\norder. At a child table, say ğ‘‡2, it samples ğ‘¡2fromğ‘¡1â‹Šğ‘‡2(all tuples\ninğ‘‡2that join with ğ‘¡1) using weights{ğ‘¤2(ğ‘¡):ğ‘¡âˆˆğ‘¡1â‹Šğ‘‡2}. The\nprocedure continues recursively until all tables are visited, and thus\nproduces a sample (ğ‘¡1,Â·Â·Â·,ğ‘¡ğ‘), eachğ‘¡ğ‘–being a tuple of join keys\nfrom the respective table.\nExample. Consider the schema in Figure 4a. Figure 4b shows the\ncomputed join counts. The leaf table ğ¶has a count of 1 for every\ntuple. Inğµ, since(2,ğ‘)can join with two tuples in ğ¶, its join count\nis2=1+1. Similar propagation happens for ğ´.ğ‘¥=2which gets\na count of 3=1+2. Physically, we store the join counts indexed\nby join keys (e.g., for ğ¶, only one mapping ğ‘â†’1is kept). For\nsampling, suppose ğ´.ğ‘¥=2is first sampled. It has two matches in ğµ\nwith weights 1 and 2, so the second match, (2,ğ‘), has an inclusion\nprobability of 2/3.\nNULL handling. To support full outer joins, we handle NULL keys\nas follows. We add a virtual âˆ…tuple (which denotes NULL ) to each\ntableğ‘‡ğ‘–, and make it join with all normal ğ‘¡âˆˆğ‘‡ğ‘—that have no\nmatches in ğ‘‡ğ‘–, whereğ‘‡ğ‘—âˆˆChildren(ğ‘‡ğ‘–). Similarly, any normal\nğ‘¡âˆˆParent(ğ‘‡ğ‘–)that has no match in ğ‘‡ğ‘–joins withğ‘‡ğ‘–â€™sâˆ…. All-NULL is\ninvalid. Propagation proceeds as before; Figure 4b shows examples.\nConstructing complete sample tuples. In the prior example,\nsupposeâŸ¨2; 2,ğ‘;ğ‘âŸ©is drawn. We gather the content columns of ğ´\nby looking up ğ´.ğ‘¥=2and similarly for(ğµ.ğ‘¥,ğµ.ğ‘¦)=(2,ğ‘)2and\nğ¶.ğ‘¦=ğ‘. On multiple matches, we pick a row uniformly at random.\nTheir concatenation represents a sampled tuple from the full join.\nComputing the size of the full join (normalizing constant).\nRecall from Â§3.2 that the row count |ğ½|(the normalizing constant in\nprobabilistic terms) is required to convert selectivities into cardinali-\nties. With join counts it can be computed exactly: |ğ½|=Ã\nğ‘¡âˆˆğ‘‡1ğ‘¤1(ğ‘¡).\nParallel sampling. Finally, the sampling procedure is embarrass-\ningly parallel: after the join count tables {ğ‘¤ğ‘–(Â·)}are produced, par-\nallel threads can be launched to read the join counts and produce\nsamples. Computation of the join count tables is also parallelizable,\nalthough it is an one-time effort. Sampling correctness is preserved\neven in the presence of parallelism due to the i.i.d. property.\n4.2 Comparison with other samplers\nOur key requirements of uniform and i.i.d. samples from the full\njoin render many related sampling algorithms unsuitable. If either\nproperty is not satisfied, the sampling distribution would be bi-\nased and thus compromise the quality of the learned AR model.\nAs examples, Index-based Join Sampling (IBJS) [ 20] is neither uni-\nform nor independent; Wander Join [ 22] produces independent\nbut non-uniform samples. Both approaches do produce unbiased\n2Either intersect two matching lists from both columnsâ€™ index lookups, or do a single\nlookup if a composite index is available.\n\nğ´ğµğ¶ğ‘¥ğ‘¦\nğ´.ğ‘¥\n1\n2ğµ.ğ‘¥ ğµ.ğ‘¦\n1 a\n2 b\n2 cğ¶.ğ‘¦\nc\nc\nd\n(a) Schema and base tablesğ´.ğ‘¥ğµ.{ğ‘¥,ğ‘¦}ğ¶.ğ‘¦\n11 1, a1c1\n23 2, b1c1\nâˆ…1 2, c2d1\nâˆ…1âˆ…1\n(b) Join countsğ´.ğ‘¥ ğµ.ğ‘¥Fğµ.ğ‘¥ğµ.ğ‘¦ ğ¶.ğ‘¦Fğ¶.ğ‘¦ 1ğ´1ğµ1ğ¶\n1 1 1 a âˆ… 1 1 1 0\n2 2 2 bâˆ… 1 1 1 0\n2 2 2 c c 2 1 1 1\n2 2 2 c c 2 1 1 1\nâˆ… âˆ… 1âˆ… d 1 0 0 1\n(c) Full outer join, with virtual columns in blue-- In full join, |A.x=2|=3.\n-- Q1. True answer is 2.\nSELECT COUNT( *)\nFROM A JOIN B ON x\nJOIN C ON y\nWHERE A.x = 2;\n-- Q2. True answer is 1.\nSELECT COUNT( *)\nFROM A WHERE A.x = 2;\n(d) Schema subsetting\nFigure 4: End-to-end example. (a) A join schema of three tables and their join key columns. Content columns are omitted. (b) Join counts\n(blue) enable uniform sampling of the full outer join and are computed in linear time by dynamic programming. Here, edges connect join\npartners. (c) Learning target: the full outer join of the schema, with virtual columns in blue. We show the fanoutsF, the number of times a\njoin key value appears in the corresponding base table, for keys ğµ.ğ‘¥andğ¶.ğ‘¦. The fanouts for ğ´.ğ‘¥andğµ.ğ‘¦are all 1and omitted. Each indicator\n1ğ‘‡denotes whether a tuple has a match in table ğ‘‡.(d) Examples of schema subsetting, i.e., queries that touch a subset of the full join (Â§6).\nColumnDomain: 1061,000,0001Binary representation1111010000  1001000000             0000000000  0000000001 Subcol 19760Subcol 25761Domain: â‰¤ 2NChunk every N=10 bits    \nFigure 5: Lossless column factorization (Â§5).\nestimators for counts or other aggregate statistics, but are not de-\nsigned to return uniform join samples. Reservoir sampling, a well-\nknown technique, draws samples without replacement (thus, non-\nindependent) and requires a full scan over the full join, which is not\nscalable. Lastly, the Exact Weight algorithm NeuroCard implements\nis among the most efficient in Zhao et al. [50]. They provide addi-\ntional extensions to support general, potentially cyclic joins (e.g.,\na cycle can be broken ), which NeuroCard can leverage to broaden\nour formulation (Â§3.3).\n5 LOSSLESS COLUMN FACTORIZATION\nA key challenge of using an autoregressive model for high-cardinality\ndata is that the size of the model parameters can scale linearly with\nthe numbers of distinct values in the columns. In the model archi-\ntecture we use (Â§3.4), each column (any data type; categorical or\nnumerical) is first dictionary-encoded into integer token IDs. Then\na per-column embedding layer is applied on these token IDs. The\nsize of the trainable embedding matrix (essentially, a hash table) for\neach column ğ¶scales linearly with |ğ¶|, i.e., the number of distinct\nvalues in the domain. Even a moderately sized column with up\nto106distinct values, therefore, easily takes up 128MBof space,\nassuming 32-dimensional embeddings are used.\nTo handle high-cardinality columns efficiently, we propose an\noptimization that we call lossless column factorization . This opti-\nmization is inspired by the popular use of â€œsubword unitsâ€ [ 35] in\nmodern natural language processing, and also shares characteristics\nwith â€œbit slicingâ€ in the indexing literature [ 28]. Different from sub-\nword units, column factorization does not use a statistical algorithm\nsuch as byte pair encoding to determine what subwords to use (a\npotential optimization). Different from bit slicing, we slice a value\ninto groups of bits and convert them back into base-10 integers.\nFigure 5 illustrates the idea on a simple example. Suppose a\ncolumn (any datatype) has a domain size of |ğ¶|=106. Naively\nsupporting this column would require allocating |ğ¶|Â·â„floats asits embedding matrix, where â„is the embedding dimension. In-\nstead, NeuroCard factorizes each value on-the-fly during training:\nwe convert an original-space value into its binary representation,\nthen slice off every ğ‘bits, the factorization bits hyperparameter.\nEach sliced off portion becomes a subcolumn , now in base-10 inte-\nger representation. These subcolumns are now treated as regular\ncolumns to learn over by the autoregressive model. Crucially, a\nmuch smaller embedding matrix is now needed for each subcolumn\ncontaining at most 2ğ‘Â·â„floats. In this example, we can reduce\n128MB to 250KBâ€”a more than 500Ã—space reduction.\nModel size vs. statistical efficiency. Choosing the factorization\nbitsğ‘enables a tradeoff between model size vs. statistical efficiency.\nBy decreasing ğ‘, we have more subcolumns, each with a smaller\ndomain, but learning across more variables becomes harder. In\ntheory, by using autoregressive modeling no information is lost in\nthis translation, so the precision of the learned distributions is not\naffected. In practice, we observed that lower factorization bits, i.e.,\nslicing into more subcolumns, generally underperform higher ones\nthat use more space, but not by a significant margin (Â§7.5). We thus\nset the factorization bits ğ‘based on a space usage budget.\nLossless = factorization + autoregressive modeling. With fac-\ntorization, a column is factorized into multiple subcolumns, which\nare then fed into a downstream density estimator. However, if a\ndensity estimator with independence assumptions, e.g., 1D his-\ntograms, is used, then this whole process is lossy . By modeling\nğ‘(subcol 1,subcol 2)â‰ˆğ‘(subcol 1)ğ‘(subcol 2), histograms would fail\nto capture any potential correlation between the two subcolumns.\nIn other words, other estimators could read in subcolumn values\nand potentially reduce space usage, but their inherent quality and\nassumptions determine how much information is learned about the\nsubcolumns, and about their correlations with other columns. By us-\ning autoregressive modeling, NeuroCard forces the AR model to ex-\nplicitly capture such correlation, namely (ignoring other columns):\nğ‘(col)â‰¡ğ‘(subcol 1,subcol 2)=ğ‘(subcol 1)ğ‘(subcol 2|subcol 1),\nwhich has no inherent loss of information. Hence, we call the unique\ncombination of factorization and autoregressive modeling lossless .\nFilters on subcolumns. During probabilistic inference, a filter on\nan original column needs to be translated into equivalent filters\non subcolumns. Recall from Â§3.2 that the probabilistic inference\nprocedure draws samples that lie inside the queried region. We\n\nmodify that procedure to handle subcolumns by respecting each\nfilterâ€™s semantics. Going back to our example, consider the filter\ncol<1,000,000. The filter for the high-bits subcol 1isrelaxed to\nâ‰¤976(note the less-equal). The inference procedure would draw\nasubcol 1value in this range, based on which the low-bits filter is\nrelaxed appropriately. If the drawn subcol 1is976, then the filter on\nsubcol 2is set to â€œ <576â€; otherwise, the high-bits already satisfy\nthe original filter so a wildcard is placed on the low-bits subcolumn.\nThis is reminiscent of processing range predicates on bit-sliced\nindexes [ 28]; NeuroCard applies these processing logic in the new\ncontext of probabilistic inference for autoregressive models.\n6 QUERYING NEUROCARD\nOnce built, the autoregressive model summarizes the entire full\nouter join. The challenge with querying this probabilistic model\nfor a selectivity estimate is that the query may restrict the space it\ntouches to a subset of the full join â€”a phenomenon we term schema\nsubsetting . Since the selectivity estimate returned by the model as-\nsumes the probability space to be the full outer join, rather than the\nquery-specific restricted space, the estimate should be downscaled\nappropriately during probabilistic inference.\nNeuroCardâ€™s inference algorithms combine two building blocks.\nFirst, Naru [ 48] introduced progressive sampling , a Monte Carlo\nalgorithm that integrates over an autoregressive model to produce\nselectivity estimates. We invoke this routine (i.e., Equation 5) on the\ntrained autoregressive model with changes outlined in this section.\nSecond, Hilprecht et al. [12] have proposed inference algorithms to\nquery a sum-product network trained on a full outer join. We state\ntheir algorithms below and discuss how to adapt these algorithms\ninto our framework, thereby generalizing them to a new type of\nprobabilistic model.\nBasic case: no table omitted. The simplest case of schema subset-\nting is an inner join query on all tables. Consider the example data\nin Figure 4a and an inner join query Q1 in Figure 4d. The query,\nğœğ´.ğ‘¥=2(ğ´âŠ²âŠ³ğ‘¥ğµâŠ²âŠ³ğ‘¦ğ¶), restricts the probability space from the full\njoin to the inner join. Naively querying the model for |ğ´.ğ‘¥=2|\nwould return a cardinality of |ğ½|Â·(3/5)=3rows, as 3 out of 5 rows\nin the full join ğ½(Figure 4c) satisfy the filter. However, the correct\nrow count for this query is 2(two rows in the inner join; both pass\nthe filter). Left/right outer joins can also exhibit this behavior.\nTo correct for this, Hilprecht et al. propose a simple solution\nby adding an indicator column per table into the full join. A binary\ncolumn 1ğ‘‡is added for each table ğ‘‡, with value 1if a tuple (in the\nfull join) has a non-trivial join partner with table ğ‘‡, and 0otherwise.\nNeuroCard adopts this solution as follows. First, during train-\ning, the sampler is tasked with appending these virtual indicator\ncolumns on-the-fly to sampled tuples. Recall that each sampled\ntuple is formed by querying base-table indexes with sampled join\nkeys. If a table ğ‘‡contains a join key, we set that sampled tupleâ€™s\n1ğ‘‡to1, and 0otherwise (see Figure 4c). The autoregressive model\ntreats these indicator columns as regular columns to be learned.\nSecond, during inference, NeuroCard adds equality constraints\non the indicator columns, based on what tables are present in the\nquery. The progressive sampling routine (Equation 5) not only gets\nthe usual filter conditions, {ğ‘‹ğ‘–âˆˆğ‘…ğ‘–}, but also{1ğ‘‡=1}for anytableğ‘‡that appears in the inner-join query graph3. In summary,\nfor the no-omission case, the routine now estimates the probability:\nğ‘ƒ({ğ‘‹ğ‘–âˆˆğ‘…ğ‘–}âˆ§{1ğ‘‡=1 :for all table ğ‘‡}) (8)\nExample. Coming back to the example query Q1, ğœğ´.ğ‘¥=2(ğ´âŠ²âŠ³ğ‘¥\nğµâŠ²âŠ³ğ‘¦ğ¶), we compute the selectivity under the full join as ğ‘ƒ(ğ´.ğ‘¥=\n2âˆ§1ğ´=1ğµ=1ğ¶=1). Reading from Figure 4c, this probability is\n2/5, so the cardinality is correctly computed as 5Â·(2/5)=2rows.\nOmitting tables and fanout scaling. The less straightforward\ncase is if a query omits , i.e., does not join, certain tables. Consider Q2\nin Figure 4d: ğœğ´.ğ‘¥=2(ğ´). When restricting the scope to table ğ´, the\nrow count of ğ´.ğ‘¥=2is 1, different from|ğ½|Â·ğ‘ƒ(ğ´.ğ‘¥=2âˆ§1ğ´=1)=3\nrows. The fundamental reason this happens is because the operation\nof a full join has fanned out tuples from base tables. To correctly\ndownscale, Hilprecht et al. propose recording a per-join fanout\ncolumn. We adapt this solution in NeuroCard4.\nSpecifically, for each join key column ğ‘‡.ğ‘˜, we insert into the full\njoin a virtual fanout column, Fğ‘‡.ğ‘˜, defined as the number of times\neach value appears in ğ‘‡.ğ‘˜. For example, 2appears twice in ğµ.ğ‘¥, so\nits fanout isFğµ.ğ‘¥(2)=2; see Figures 4a and 4c. Again, we task the\njoin sampler with adding these fanout values on-the-fly to each\nbatch of sampled tuples. The inclusion of fanouts is piggybacked\nonto the index lookup path (querying the size of each lookup result\nlist), which adds negligible overheads.\nOn the inference side, Hilprecht et al. showed that the correct\ncardinality with omitted tables can be computed via fanout scaling :\nCardinality(query Q) =|ğ½|Â·ğ‘ƒ({ğ‘‹ğ‘–âˆˆğ‘…ğ‘–}subsetted to query Q )\n=|ğ½|Â·E\nğ‘‹âˆ¼ğ½\u00141{ğ‘‹ğ‘–âˆˆğ‘…ğ‘–}Â·Ã\nğ‘‡âˆˆğ‘„1ğ‘‡Ã\nğ‘…âˆ‰ğ‘„Fğ‘….key\u0015\n.\n(9)\nIn essence, the numerator handles the basic case above, while the\ndenominator counts the total number of times omitted tables {ğ‘…âˆ‰\nğ‘„}have fanned out each tuple in query Q. It loops through each\nomitted table ğ‘…, finds its unique join key ğ‘….keythat connects to Q in\nthe schema (discussed in detail below), and looks up the associated\nfanout valueFğ‘….key. We incorporate this scaling as follows. Since\nthe fanout columns are learned by the model, we modify progressive\nsampling to draw a concrete value for each relevant Fğ‘….keyper\nprogressive sample, compute the product of these fanouts, and\ndivide the progressive sampleâ€™s estimated likelihood by this product.\nExample. Coming back to Q2, ğœğ´.ğ‘¥=2(ğ´), the constraints are {ğ´.ğ‘¥=\n2,1ğ´=1}. Reading from Figure 4c, three rows satisfy the con-\nstraints and the relevant downscaling keys are ğµ.ğ‘¥andğ¶.ğ‘¦. Thus\nthe expectation expands as:1\n5Â·(1\n2Â·1+1\n2Â·2+1\n2Â·2)=1\n5. Multiplying\nwith|ğ½|=5arrives at the correct cardinality of 1row.\nHandling fanout scaling for multi-key joins. Our formulation\nof fanout scaling supports multi-key joins, e.g., both ğ‘¥andğ‘¦keys\nin the example schema ğ´.ğ‘¥=ğµ.ğ‘¥âˆ§ğµ.ğ‘¦=ğ¶.ğ‘¦(Figure 4a). The\nchallenge of fanout scaling in this case is determining the set of\nomitted keys to downscale. Let ğ‘‰be the set of all tables. Let ğ‘„be\n3The indicator columns can also be constrained appropriately for left or right joins.\n4Our definition differs slightly from Hilprecht et al. . In that work, each fanout column\nis bound to a PK-FK join and stores the frequency of a value in the FK. Our treatment\nbinds a fanout to each join key, regardless of PK/FK, and is defined as the frequency\neach value appears in that key column itself. This removes their assumption of PK-FK\njoins and supports general equi-joins where both join keys can have duplicate values.\n\n10âˆ’1010âˆ’810âˆ’610âˆ’410âˆ’2100\nQuery Selectivity [log scale]0%25%50%75%100%% of QueriesJOB-M JOB-light-ranges JOB-lightFigure 6: Distribution of query selectivity (Â§7.1).\nthe set of tables joined in a query, and the complement ğ‘‚=ğ‘‰\\ğ‘„\nthe omitted tables. Pick any table ğ‘‡âˆˆğ‘„. There exists a unique path\nfrom each omitted ğ‘‡ğ‘‚âˆˆğ‘‚toğ‘‡, because the join schema graph is a\ntree (acyclic, connected). The join key attached to the edge incident\ntoğ‘‡ğ‘‚on this path is the unique join key for table ğ‘‡ğ‘‚to downscale.\nHence, the fanout downscaling factor in Equation 9 is well-defined.\nGoing back to example Q2 where only ğ´is queried, when consid-\nering the omitted table ğµwhich has two join keys ( ğµ.ğ‘¥,ğµ.ğ‘¦), we see\nthatğµ.ğ‘¥is the unique fanout key since it lies on the path ğ´â†â†’ğµ.\nSummary of schema subsetting. To recap, NeuroCardâ€™s prob-\nabilistic inference leverages the progressive sampling algorithm\nfrom Naru and the idea of additional columns from Hilprecht et\nal.that we term virtual columns . Our join sampler is modified to\nlogically insert into the full join two types of virtual columns, the\nindicators and the fanouts. Both are treated as regular columns to\nbe learned over by the density model, and both are used during\nprogressive sampling to handle various cases of schema subsetting.\nOrdering virtual columns in the autoregressive factorization.\nThe autoregressive model requires some fixed ordering of columns\nin its factorization (Â§3.2). Naru has shown that different orderings\nmay have different performance in the tail error but not in the lower\nerror quantiles. We adopt the same practice as Naru in using an\narbitrary ordering for the content columns. For the virtual columns\nintroduced above, we place them after all the content columns, with\nindicators before fanouts. The intuition here is to ensure that (1)\nthe conditional distributions involving content columns do not get\nconfused by the presence of virtual columns, and (2) when sam-\npling fanouts, placing them at the end allows for prediction using a\nmaximum amount of prior information.\nIn our early benchmarks this choice performed better than if\nvirtual columns were placed early in the ordering. We also experi-\nmented with multi-order training [6] in the autoregressive model,\nbut did not see noticeably better performance. Thus, we opt for a\nsimple treatment and leave such optimizations to future work.\n7 EVALUATION\nWe evaluate NeuroCard on accuracy and efficiency and compare it\nwith state-of-the-art cardinality estimators. The key takeaways are:\nâ€¢NeuroCard outperforms the best prior methods by 4â€“34 Ã—\nin accuracy (Â§7.3). On the popular JOB-light benchmark, Neu-\nroCard achieves a maximum error of 8.5Ã—using 4MB.\nâ€¢NeuroCard scales well to more complex queries (Â§7.3). On\nthe two new benchmarks JOB-light-ranges (more difficult range\nfilters) and JOB-M (more tables in schema), NeuroCard achieves\norders of magnitude higher accuracy than prior approaches.Table 1: Workloads used in evaluation. Tables : number of base tables.\nRows, Cols, Dom. : row count, column count, and maximum column\ndomain size of the full outer join of each schema. Feature character-\nizes each workloadâ€™s queries. Rows in full join: 2Â·1012; 2Â·1012; 1013.\nWorkload Tables Rows Cols Dom. Feature\nJOB-light 6 2Â·10128 235K single-key joins\nJOB-light-ranges 6 2Â·101213 134K+complex filters\nJOB-M 16 101316 2.7M+multi-key joins\nâ€¢NeuroCard is efficient to construct and query (Â§7.4). A few\nmillion tuples, learned in less than 5 minutes, suffice for it to\nreach best-in-class accuracy.\nâ€¢We study the relative importance of each component of\nNeuroCard (Â§7.5). Out of all factors, learning the correlations\nacross all tables and performing unbiased join sampling prove\nthe most impactful.\n7.1 Experimental Setup\nWorkloads (Table 1). We adopt the real-world IMDB dataset and\nschema to test cardinality estimation accuracy. Prior work [ 19,21]\nreported that correlations abound in this dataset and established it\nto be a good testbed for cardinality estimators. We test the following\nquery workloads on IMDB:\nâ€¢JOB-light : a 70-query benchmark used by many recent cardi-\nnality estimator proposals [ 12,15,38]. The schema contains 6\ntables, title (primary), cast_info ,movie_companies ,movie_info ,\nmovie_keyword ,movie_info_idx and is a typical star schemaâ€”\nevery non-primary table only joins with title ontitle.id . The full\nouter join contains 2Â·1012tuples. Each query joins between 2\nto 5 tables, with only equality filters except for range filters on\ntitle.production_year .\nâ€¢JOB-light-ranges : we synthesized this second benchmark con-\ntaining 1000 queries derived from JOB-light by enriching filter\nvariety. We generate the 1000 queries uniformly distributed to\neach join graph of JOB-light (18 in total), as follows. For each\njoin graph, using our sampler we draw a tuple from the inner\njoin result. We use the non-null column values of this tuple\nas filter literals, and randomly place 3â€“6 comparison operators\nassociated with these literals, based on whether each column\ncan support range (draw one of {â‰¤,â‰¥,=}) or equality filters ( =).\nOverall, this generator (1) follows the data distribution and guar-\nantees non-empty results, and (2) includes more filters, in variety\nand in quantity, than JOB-light . An example 3-table query is:\nmcâŠ²âŠ³ğœinfo_type_id=99(mi_idx)âŠ²âŠ³ğœepisode_nrâ‰¤4âˆ§phonetic_codeâ‰¥â€™N612â€™(ğ‘¡),\nwhere t.idis joined with other tablesâ€™ movie_id .\nâ€¢JOB-M : this last benchmark contains 16 tables in IMDB and in-\nvolves multiple join keys. For instance, the table movie_companies\nis joined not only with title onmovie_id , but also with com-\npany_name oncompany_id , and with company_type oncom-\npany_type_id , etc. We adapt the 113 JOB queries [ 19] by allowing\neach table to appear at most once per query and removing logical\ndisjunctions (e.g., A.x=1âˆ¨B.y=1 ). Each query joins 2â€“11 tables.\nWe use JOB-M to test NeuroCardâ€™s scalability as its full join is\n5Ã—larger and has more dimensions than the above (see Table 1).\nThe benchmarks are available at https://github.com/neurocard.\n\nMetric. We report the usual Q-error distribution of each work-\nload, where the Q-error of a query is the multiplicative factor an\nestimated cardinality deviates from the queryâ€™s true cardinality:\nQ-error(query) :=max\u0010card actual\ncard estimate,card estimate\ncard actual\u0011\n. Both actual and es-\ntimated cardinalities are lower bounded by 1, so the minimum\nattainable Q-error is 1Ã—. As reported in prior work [ 48], reducing\nhigh-quantile errors is much more challenging than mean or me-\ndian; thus, we report the quantiles ğ‘100,ğ‘99,ğ‘95, and the median.\nFor timing experiments, we report latency/throughput using an\nAWS EC2 VM with a NVIDIA V100 GPU and 32 vCPUs.\nBenchmark characteristics. Figure 6 plots the distributions of\nselectivities of these workloads, where we calculate each queryâ€™s\nselectivity as card actual/card inner (denominator is the row count of\nthe query join graphâ€”an inner joinâ€”without filters). The selectivity\nspectrums of our two benchmarks ( JOB-light-ranges andJOB-M )\nare much wider than JOB-light due to higher filter variety. The\nmedian selectivity is more than 100Ã—lower, while at the low tail\nthe minimum selectivities are 1000Ã—lower.\n7.2 Compared Approaches\nWe compare against several prevalent families of estimators. In each\nfamily, we aim to choose a state-of-the-art representative. Related\nWork (Â§8) includes a more complete discussion on all families and\ntheir representative methods.\nSupervised query-driven estimators. We use MSCN [ 15] as a\nrecent representative from this family. It takes in a featurized query,\nruns the query filters on pre-materialized samples of the base tables,\nthen use these bitmaps as additional network inputs, and predicts\na final cardinality. For JOB-light , we used the training queries and\nsample bitmaps provided in the authorsâ€™ source code [ 16]. For JOB-\nlight-ranges , due to new columns, we generated 10K new training\nqueriesâ€”generating and executing them to obtain true cardinality\nlabels took 3.2 hoursâ€”and used a bitmap size of 2K to match the\nsize of other estimators in this benchmark. For JOB-light , we also\ncite the best numbers obtained by Sun and Li [ 38], termed E2E,\nwhich is a deep supervised net with more effective building blocks\n(e.g., pooling, LSTM) than MSCN.\nUnsupervised data-driven estimators. We use DeepDB [ 12] as\na recent technique in this family. It uses a non-neural sum-product\nnetwork [ 29] as the density estimator for each table subset chosen\nby correlation tests. Conditional independence is assumed across\nsubsets. In contrast, NeuroCard uses a neural autoregressive model\nto build a single learned estimator over all tables in a schema. We\nuse two recommended configurations from DeepDB: a base version\nthat learns up to 2-table joins, and a larger version that additionally\nbuilds 3-table models. These correspond to their storage-optimized\nand the standard setups, respectively.\nWe found that the DeepDB source code [ 13] did not support\nrange queries on categorical string columns out-of-the-box. Since\nJOB-light-ranges contains such queries, we perform data and query\nrewriting for this baseline, by dictionary-encoding the string values\ninto integers. Reported results are with this optimization enabled.\nJoin sampling. We implement the Index-based Join Sampling\nmethod (IBJS) [ 20], using 10,000 as the maximum sample size. ATable 2: JOB-light , estimation errors. Lowest errors are bolded.\nEstimator Size Median 95th 99th Max\nPostgres 70KB 7.97 797 3Â·103103\nIBJS â€“ 1.48 103103104\nMSCN 2.7MB 3.01 136 1Â·103103\nE2E (quoting [38]) N/A 3.51 139 244 272\nDeepDB 3.7MB 1.32 4.90 33.7 72.0\nDeepDB-large 32MB 1.19 4.66 35.0 39.5\nNeuroCard 3.8MB 1.57 5.91 8.48 8.51\nqueryâ€™s cardinality is estimated by taking a sample from the queryâ€™s\njoin graph and executing per-table filters on-the-fly.\nReal DBMS. We use Postgres v12, which performs cardinality\nestimation using 1D histograms and heuristics to combine them.\nOther baselines. The methods chosen above have been compared\nto other estimators in prior studies. Naru [ 48] has shown that estima-\ntors based on classical density modeling (KDE; Bayesian networks;\nthe MaxDiff n-dimensional histogram) or random sampling signifi-\ncantly lag behind deep autoregressive models. DeepDB [ 12] also\nshows that it significantly outperforms wavelets [ 2]. We therefore\ndo not compare to these methods.\nNeuroCard. We implement NeuroCard on top of the Naru source\ncode [ 27]. We use ResMADE by default. For complex benchmarks\nwe also use the Transformer (Â§3.4), which is suffixed with -large .\n7.3 Estimation Accuracy\n7.3.1 JOB-light. Table 2 reports each estimatorâ€™s accuracy on the\n70JOB-light queries. Overall, NeuroCard exhibits high accuracy\nacross the spectrum. It sets a new state-of-the-art maximum\nerror at 8.5Ã—using 3.8MBof parameters. This represents an >8Ã—\nimprovement over the best prior method when controlling for size.\nWe now discuss a few observations. Not surprisingly, Postgres\nhas the most inaccurate medianâ€”indicating a systematic mismatch\nbetween the approximated distribution and dataâ€”due to its use of\ncoarse-grained density models (histograms) and heuristics. IBJS\nfares better at the median, but falls off sharply at tail, because sam-\nples of a practical size have a small chance to hit low-density queries\nin a large joint space. Both MSCN and E2E are deep supervised\nregressors which show marked improvements over prior methods.\nHowever, their median and 95th errors are quite similar and have\nsizable gaps from the two data-driven estimators.\nNeuroCard vs. DeepDB shows interesting trends. NeuroCard\nis up to 4â€“8Ã—better at tail (99th, max), and DeepDB is slightly\nbetter at lower quantiles. NeuroCard is more robust at tail due to\n(1) a markedly better density model (neural autoregressive vs. non-\nneural sum-product networks that use inter-column independence\nassumptions), and (2) learning all possible correlations among the\ncolumns of all 6 tables, whereas DeepDB assumes (conditional)\nindependence across several table subsets. DeepDB-large, being\n8.4Ã—bigger and trained on 7.7Ã—more (54M) tuples, still trails Neu-\nroCard at tail by more than 4Ã—. NeuroCard slightly trails at the\nlower quantiles (â€œeasyâ€ queries with high true density) likely due\nto the mode-covering behavior of KL-divergence minimization [ 9].\n\nTable 3: JOB-light-ranges , estimation errors. Lowest errors bolded.\nEstimator Size Median 95th 99th Max\nPostgres 70KB 13.8 2Â·1032Â·1045Â·106\nIBJS â€“ 10.1 4Â·104106108\nMSCN 4.5MB 4.53 397 6Â·1032Â·104\nDeepDB 4.4MB 3.40 537 8Â·1032Â·105\nDeepDB-large 21MB 2.00 91.7 2Â·1034Â·104\nNeuroCard 4.1MB 1.87 57.1 375 8169\nNeuroCard-large 21MB 1.40 35.1 232 1029\nTable 4: JOB-M , estimation errors. Lowest errors are bolded.\nEstimator Size Median 95th 99th Max\nPostgres 120KB 174 1Â·1048Â·1041Â·105\nIBJS â€“ 61.1 3Â·1054Â·1064Â·106\nNeuroCard 27.3MB 2.84 404 1327 2Â·104\nNeuroCard-large 409MB 1.96 26.4 304 874\n7.3.2 JOB-light-ranges. This 1000-query benchmark adds equal-\nity/range filters on more content columns, using the same join\ntemplates as JOB-light (which has range filters on one column\nonly). Results are shown in Table 3.\nNeuroCard achieves the best accuracy across all error quan-\ntiles, and improves on the best prior methods by up to 15âˆ’34Ã—.\nIt is also the only estimator with <2Ã—median and 3-digit 99%-tile\nerrors. Overall, all estimators produce less accurate cardinalities,\nthough the drops are of varying degrees. Compared with MSCN,\nNeuroCard improves by 2Ã—at median, 7Ã—at 95th, 15Ã—at 99th, and\n2Ã—at max. Compared with DeepDB, NeuroCard improves the four\nquantiles by 2Ã—,9Ã—,21Ã—, and 23Ã—, respectively. Comparing the\nenlarged versions of the two estimators (suffixed with -large ), the\naccuracy gains become 1.4Ã—,2.6Ã—,9.6Ã—and34Ã—, respectively.\nNeuroCardâ€™s improvements over baselines significantly widen\nin this benchmark, due to prior approaches failing to capture the\nmore complex inter-column correlations being tested.\n7.3.3 JOB-M. This final benchmark tests NeuroCardâ€™s ability to\nscale to a much larger and more complex join schema. Different\nfrom the JOB-light schema, JOB-M contains 16 tables, with each\nquery joining 2â€“11 tables on multiple join keys (in addition to\nmovie_id only in JOB-light ). For baselines, we only include Postgres\nand IBJS, because MSCNâ€™s query encoding does not support the\ncomplex filters in this benchmark and DeepDB ran out of memory\non this 16-table dataset due to high-cardinality categorical columns.\nResults in Table 4 show that NeuroCardâ€™s accuracy remains\nhigh on this complex schema . Postgres produces large errors,\nand IBJS also struggles, due to many intermediate samples becoming\nempty as the number of joins grows. NeuroCard overcomes this\nchallenge and offers more than 10Ã—better accuracy across the\nboard. In terms of space efficiency, since the model needs to be\ntrained on the full outer join of 16 tables and the maximum domain\nsize exceeds 2 million, a vanilla NeuroCard would require 900MB\nin model size. With column factorization (Â§5), the model size is\nreduced to 27MBâ€”less than 1% of the total size of all tables. We also\npresent a large model NeuroCard-large to demonstrate scalability.\n1234567\nPer 1M tuples100101102103104Q-error, p99\nJOB-light-ranges JOB-light(a)Accuracy vs. Tuples Trained\n1 2 4 8 16\nNum Sampling Threads010K20K30K40KTuples/secondJOB-light-ranges JOB-light (b)Training Throughput\n0 1000 2000\nRuntime (sec)MSCN\nDeepDB\nNeuroCard3min\n8min\n5min38min\n7min\n3minJOB-light-ranges JOB-light\n(c)Training Comparison\n1 510 50100\nLatency (ms) [log scale]0%50%100%% of QueriesMSCN NeuroCard DeepDB (d)Inference Comparison\nFigure 7: Statistical and physical efficiency of NeuroCard.\n7.4 Efficiency\nHaving established that NeuroCard achieves the best accuracy, we\nnow study the statistical and physical efficiency of NeuroCard.\nHow many tuples are required for good accuracy? Figure 7a\nplots accuracy (p99 on JOB-light andJOB-light-ranges ) vs. number\nof tuples trained. About 2â€“3M tuples are sufficient for NeuroCard to\nachieve best-in-class accuracy (compare with Tables 2 and 3). Us-\ning more samples helps, but eventually yields diminishing returns.\nReaching high accuracy using a total of âˆ¼107samples out of a pop-\nulation of 1012data points (i.e., only 0.001% of the data)â€”many\nqueries would inevitably touch unseen data pointsâ€”shows that\nNeuroCard generalizes well and is statistically efficient .\nHow does sampling affect training throughput? Figure 7b plots\nthe training throughput, in tuples per second, vs. the number of\nsampling threads used to provide training data. Four threads suf-\nfice to saturate the GPU used for training. At lower thread counts,\nthe device spends more time waiting for training data than do-\ning computation. With a peak throughput of âˆ¼40K tuples/second,\nNeuroCard can finish training on 3M tuples in about 1.25 minutes.\nWall-clock training time comparison. Figure 7c compares the\nwall-clock time used for training the MSCN, DeepDB, and Neuro-\nCard configurations reported in Tables 2 and 3. MSCN requires a\nseparate phase of executing training queries to collect true cardinal-\nities, which takes much longer (3.2 hours for 10K queries) than just\nthe training time shown here. DeepDB runs on parallel CPUs and is\nquite efficient. NeuroCard starts training/on-the-fly sampling after\ncalculating the join count tables, which takes 13 seconds for both\ndatasets. Its construction is efficient due to parallel sampling and\naccelerated GPU computation.\nWall-clock inference time comparison. Lastly, Figure 7d plots\nthe latency CDF of the learning approaches for 1000 JOB-light-\nranges queries. As before, we use the base configurations reported\nin the accuracy Tables. MSCN and NeuroCard run on GPU while\nDeepDB runs on CPU; all three approaches are implemented in\nPython. MSCN is fastest because its lightweight network has fewer\n\nTable 5: Ablation studies: varying primary components of Neuro-\nCard. Unlisted values are identical to the Base configuration. We\nshow the impact of the sampler (A), column factorization bits (B),\nautoregressive model size (C), inter-table correlations learned (D),\nand whether to use an autoregressive model at all (E) on the 50%\nand 95%-tile errors of JOB-light-ranges .\nSamplerFact.\nBitsğ‘‘ff;ğ‘‘embCorrelations\nLearnedp50 p95\nBase\n(4.1 MB)unbiased 14 128; 16all tables in one\nAR1.9 57.1\n(A) biased 33 3270\n(B)10\n(2.2 MB)2.2 173\n12\n(2.6 MB)2.0 168\nNone\n(12 MB)1.6 62.7\n(C)128; 64\n(23 MB)1.5 44.0\n1024; 16\n(31 MB)1.7 64.0\n(D) one AR per table 40 9Â·104\n(E) No model; uniform join samples only 4.0 2Â·105\ncalculations involved. DeepDBâ€™s latencies span a wide spectrum,\nfromâˆ¼1msfor queries with low complexity (numbers of joins and\nfilters involved) toâˆ¼100msfor queries with the highest complexity.\nNeuroCardâ€™s latencies are more predictable, with 17msat median\nand12msat minimum: this is due to the higher number of floating\npoint operations involved in the neural autoregressive model. All\napproaches can be sped up by engineering efforts (e.g., if run in\na native language). For NeuroCard, model compression or weight\nquantization can also reduce the computational cost.\n7.5 Dissecting NeuroCard\nTo gain insights, we now evaluate the relative importance of pri-\nmary components of NeuroCard, by varying them and measuring\nthe change in estimation accuracy on JOB-light-ranges . We use the\nsmaller NeuroCard in Table 3 as the Base configuration, and ablate\neach component in isolation. Table 5 presents the results.\nIn (A), using IBJS adapted for full joins5as a biased sampler\nsignificantly decreases the learned estimatorâ€™s accuracy. The large\nincrease in the median error implies a systematic distribution mis-\nmatch. Overall, this design choice is the second most important.\nRows in group (B) vary the column factorization granularity.\nUsing smaller bits results in more subcolumns and yields a small\ndrop in accuracy. Disabling factorization uses the most space and\nappears to perform the best.\nGroup (C) varies the size of the autoregressive model, by chang-\ning the dimension of the feedforward linear layers ( ğ‘‘ff) or the em-\nbeddings (ğ‘‘emb). An enlarged embedding proves markedly more\nuseful than enlarged linear layers, likely because each tokenâ€™s cap-\ntured semantics becomes more finetuned during optimization.\n5The fact table title is ordered at front and a large intermediate size of 106is used.\n1 2 3 4 5101102103104Q-error, p95\n1 2 3 4 5101102Q-error, p50\nFast update (âˆ¼3sec)\nRetrain (âˆ¼3min)Stale\nPostgres, re-ANALYZE\n0.0 0.2 0.4 0.6 0.8 1.0\nNumber of Ingested Partitions0.00.51.0Figure 8: Updating NeuroCard, fast and slow. JOB-light . Errors (p95,\np50) of each strategy are averaged from 10 runs. Postgres is also run\nas comparison, whose statistics are updated ( 1âˆ¼2sec.) on each ingest.\nIn group (D) we vary the correlation learned by NeuroCard.\nWhile all configurations above learn the distribution of all tables in\na single modelâ€”capturing all possible correlations among themâ€”\nhere we build one model (same architecture as Base) per table.\nQueries that join across tables are estimated by combining individ-\nual modelsâ€™ estimates via independence. Without modeling inter-\ntable correlations, this variant yields the lowest accuracy.\nFinally, group (E) ablates away the AR model altogether. We\ntest uniform join samples as a standalone estimator: it uses our\nsampler (Â§4) to draw 104simple random samples (actual tuples\nin the database) from each queryâ€™s join graph. While the median\nerror is reasonable, it is 104Ã—less accurate than an autoregressive\nmodel at tail as many queries have no sample hits. The AR model\nis more statistically efficient than sampling, because it provides\naccess to conditional probability distributionsâ€”these conditional\ncontributions enable an efficient probabilistic inference procedure,\ni.e., progressive sampling, which cannot be used otherwise.\nTuning guide. Groups (B) and (C) show that NeuroCard is not\noverly sensitive to hyperparameters. For new datasets, we recom-\nmend starting with the Base configuration and increasing sizing\nas much as possible up to a size budget. The recommended prece-\ndence is: factorization bits; ğ‘‘emb;ğ‘‘ffand the number of layers. The\nnumber of training tuples can be set by early-stopping or a time\nbudget; Â§7.4â€™s results suggest starting with a few to 10+million.\n7.6 Update Strategies\nNeuroCard handles new data by either retraining, or taking addi-\ntional gradient steps, i.e., incremental training. To test both strate-\ngies, we simulate the practice of time-ordered partition appends :\ntable title is range-partitioned on a year column into 5 partitions.\nEach partition defines a distinct snapshot of the entire database\nand the full join, so running the same set of queries at different\npartition count yields 5 sets of true cardinalities. We compare three\nupdate strategies, all of which are trained fully for 7M tuples after\nthe first ingest: (1) stale, trained once on the first snapshot and\nnever updated, (2) fast update , incrementally updated after each\nnew ingest on 1% of original samples (70K), and (3) retrain , using\n100% of original samples (7M) after each ingest. We also show the\nlatency required to perform additional gradient steps.\nResults are shown in Figure 8. Without update, the stale Neu-\nroCard significantly degrades in accuracy, which is expected as\neach partition adds a significant amount of new information. A\nfast updated NeuroCard recovers most of the accuracy, incurring a\n\nminimal overhead. Even fully retraining only requires a few min-\nutes and yields the highest accuracy. Both the statistical efficiency\n(number of tuples needed vs. accuracy) and the physical efficiency\nof NeuroCard contribute to these highly practical update strategies.\n8 RELATED WORK\nUnsupervised data-driven cardinality estimators. This family\napproximates the data distribution and dates back to System Râ€™s\nuse of 1D histograms [ 34]. The quality of the density model used\nhas seen steady improvements throughout the years:\nClassical methods. Multidimensional histograms [ 10,25,30,\n31] are more precise than 1D histograms by capturing inter-column\ncorrelations. Starting from early 2000s, graphical models were pro-\nposed for either single-table or join cardinality estimation [ 3,8,40].\nThese density models tradeoff precision for efficiency by assuming\nconditional or partial independence, and require expensive struc-\nture learning (finding the best model structure given a dataset).\nSum-product networks. SPNs, a tree-structured density esti-\nmator, were proposed about 10 years ago [ 29]. Each leaf is a coarse\nhistogram of a slice of an attribute, and each intermediate layer\nuses eitherÃ—and+to combine children information. Due to their\nheuristics (e.g., inter-slice independence), SPNs have limited expres-\nsiveness : there exists simple distributions that cannot be efficiently\ncaptured by SPNs of any depth [ 24]. DeepDB [ 12] is a recent cardi-\nnality estimator that uses SPNs. NeuroCard is similar to DeepDB\nin the following aspects. (S1)Both works use the formulation of\nlearning the full outer join of multiple tables. (S2)Our â€œschema\nsubsettingâ€ capability builds on their querying algorithms.\nNeuroCard differs from DeepDB in the following. (D1) Modern\ndensity model: NeuroCardâ€™s choice of a deep autoregressive model\nis a universal function approximator hence fundamentally more\nexpressive. Unlike SPNs, no independence assumption is made in\nthe modeling. (D2) Correlations learned: NeuroCard argues for cap-\nturing as much correlation as possible across tables, and proposes\nlearning the full outer join of all tables of a schema. DeepDB, due to\nlimited expressiveness, learns multiple SPNs, each on a table subset\n(âˆ¼1â€“3 tables) chosen by correlation tests. Conditional independence\nis assumed across table subsets. (D3) Correct sampling: NeuroCard\nidentifies the key requirement of sampling from the data distribu-\ntion of joins in an unbiased fashion. In contrast, DeepDB obtains\njoin tuples either from full computation or IBJS which samples\nfrom a biased distribution. Due to these differences, NeuroCard\noutperforms DeepDB by up to 34Ã—in accuracy (Â§7).\nDeep autoregressive models. A breakthrough in density es-\ntimation, deep AR models are the current state-of-the-art density\nmodels from the ML community [ 4,6,32,43]. They tractably learn\ncomplex, high-dimensional distributions in a neural net, captur-\ning all possible correlations among attributes. Distinctively, AR\nmodels provide access to all conditional distributions among input\nattributes. Naru [ 48] is a single-table cardinality estimator that uses\na deep AR model. By accessing conditional distributions, Naru pro-\nposes efficient algorithms to integrate over an AR model, thereby\nproducing selectivity estimates. NeuroCard builds on single-table\nNaru and overcomes the unique challenges (Â§2) to support joins.Supervised query-driven cardinality estimators. Leveraging\npast or collected queries to improve estimates dates back to LEO [ 37].\nInterest in this approach has seen a resurgence partly due to an\nabundance of query logs [ 44] or better function approximators\n(neural networks) [ 15,38] that map featurized queries to predicted\ncardinalities. Hybrid methods that leverage query feedback to im-\nprove density modeling have also been explored, e.g., KDE [ 11,14]\nand mixture of uniforms [ 49]. Supervised estimators can easily\nleverage query feedback, handle complex predicates (e.g., UDFs),\nand are usually more lightweight [ 5]. NeuroCard has demonstrated\nsuperior estimation accuracy to representatives in this family, while\nbeing fundamentally more robust since it is not affected by out-of-\ndistribution queries. Complex predicates can also be handled by\nexecuting on tuples sampled from NeuroCardâ€™s learned distribution.\nJoin sampling. Extensive research has studied join sampling, a\nfundamental problem in databases. NeuroCard leverages a state-\nof-the-art join sampler to obtain training tuples representative of\na join. NeuroCard adopts the linear-time Exact Weight algorithm\nfrom Zhao et al. [50], which is among the top-performing samplers\nthey study. This algorithm provides uniform and independent sam-\nples, just as NeuroCard requires. NeuroCard may further leverage\ntheir extensions to support cyclic join schemas. While IBJS [ 20]\nand Wander Join [ 22] provide unbiased estimators for counts and\naggregates, they do not provide uniform samples of a join and thus\nare unsuitable for collecting training data. Lastly, we show that it\nis advantageous to layer a modern density model on join samples.\nLearned database components. A great deal of work has recently\napplied either classical ML or modern deep learning to various\ndatabase components, e.g., indexing [ 17], data layout [ 47], and\nquery optimization [ 18,23,39]. NeuroCard can be seen as a versatile\ncorethat can benefit any query engine, learned or not learned. Being\nable to model inter-table and inter-column correlations without\nany independence assumptions, NeuroCardâ€™s use may go beyond\nquery optimization to other tasks that require an understanding of\ntables and attributes (e.g., data imputation [45] or indexing [46]).\n9 CONCLUSION\nNeuroCard is built on a simple idea: learn the correlations across all\ntables in a database without making any independence assumptions.\nNeuroCard applies established techniques from join sampling and\ndeep self-supervised learning to cardinality estimation, a funda-\nmental problem in query optimization. It learns from dataâ€”just\nlike classical data-driven estimatorsâ€”but captures all possible inter-\ntable correlations in a probabilistic model: ğ‘ğœƒ(all tables). To our\nknowledge, NeuroCard is the first cardinality estimator to achieve\nassumption-free probabilistic modeling of more than a dozen tables.\nNeuroCard achieves state-of-the-art accuracy for join cardinality\nestimation (4â€“34Ã—better than prior methods) using a single per-\nschema model that is both compact and efficient to learn.\nACKNOWLEDGMENTS\nWe thank Joe Hellerstein for fruitful discussions and guidance, and\nMichael Whittaker, Richard Liaw, and Chenggang Wu for their\ninsightful comments on this paper.\n\nREFERENCES\n[1]Michael Armburst, Reynold S. Xin, Cheng Lian, Yin Huai, Davies Liu, Joseph K.\nBradley, Xiangrui Meng, Tomer Kaftan, Michael J. Franklin, Ali Ghodsi, and\nMatei Zaharia. 2015. Spark SQL: Relational Data Processing in Spark. In Pro-\nceedings of the 2015 ACM SIGMOD International Conference on Management of\nData (Melbourne, Victoria, Australia) (SIGMOD â€™15) . ACM, New York, NY, USA,\n1383â€“1394.\n[2]Kaushik Chakrabarti, Minos Garofalakis, Rajeev Rastogi, and Kyuseok Shim. 2001.\nApproximate query processing using wavelets. The VLDB Journal 10, 2-3 (2001),\n199â€“223.\n[3]Amol Deshpande, Minos Garofalakis, and Rajeev Rastogi. 2001. Independence is\ngood: Dependency-based histogram synopses for high-dimensional data. ACM\nSIGMOD Record 30, 2 (2001), 199â€“210.\n[4]Conor Durkan and Charlie Nash. 2019. Autoregressive Energy Machines. In\nProceedings of the 36th International Conference on Machine Learning (Proceedings\nof Machine Learning Research) , Kamalika Chaudhuri and Ruslan Salakhutdinov\n(Eds.), Vol. 97. PMLR, Long Beach, California, USA, 1735â€“1744.\n[5]Anshuman Dutt, Chi Wang, Azade Nazi, Srikanth Kandula, Vivek Narasayya,\nand Surajit Chaudhuri. 2019. Selectivity estimation for range predicates using\nlightweight models. Proceedings of the VLDB Endowment 12, 9 (2019), 1044â€“1057.\n[6]Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. 2015. MADE:\nMasked autoencoder for distribution estimation. In International Conference on\nMachine Learning . 881â€“889.\n[7]Lise Getoor, Nir Friedman, Daphne Koller, and Benjamin Taskar. 2001. Learning\nprobabilistic models of relational structure. In ICML , Vol. 1. 170â€“177.\n[8]Lise Getoor, Benjamin Taskar, and Daphne Koller. 2001. Selectivity estimation\nusing probabilistic models. In ACM SIGMOD Record , Vol. 30. ACM, 461â€“472.\n[9]Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep learning . MIT\npress.\n[10] Dimitrios Gunopulos, George Kollios, Vassilis J Tsotras, and Carlotta Domeni-\nconi. 2005. Selectivity estimators for multidimensional range queries over real\nattributes. The VLDB Journal 14, 2 (2005), 137â€“154.\n[11] Max Heimel, Martin Kiefer, and Volker Markl. 2015. Self-Tuning, GPU-Accelerated\nKernel Density Models for Multidimensional Selectivity Estimation. In Proceed-\nings of the 2015 ACM SIGMOD International Conference on Management of Data\n(SIGMOD â€™15) . ACM, New York, NY, USA, 1477â€“1492.\n[12] Benjamin Hilprecht, Andreas Schmidt, Moritz Kulessa, Alejandro Molina, Kristian\nKersting, and Carsten Binnig. 2020. DeepDB: Learn from Data, not from Queries!\nProceedings of the VLDB Endowment 13, 7 (2020), 992â€“1005.\n[13] Hilprecht et al. 2020. Github repository, deepdb-public. github.com/\nDataManagementLab/deepdb-public. [Online; accessed April, 2020].\n[14] Martin Kiefer, Max Heimel, Sebastian BreÃŸ, and Volker Markl. 2017. Estimating\njoin selectivities using bandwidth-optimized kernel density models. Proceedings\nof the VLDB Endowment 10, 13 (2017), 2085â€“2096.\n[15] Andreas Kipf, Thomas Kipf, Bernhard Radke, Viktor Leis, Peter A. Boncz, and\nAlfons Kemper. 2019. Learned Cardinalities: Estimating Correlated Joins with\nDeep Learning. In CIDR 2019, 9th Biennial Conference on Innovative Data Systems\nResearch, Asilomar, CA, USA, January 13-16, 2019, Online Proceedings .\n[16] Kipf et al. 2019. Github repository, learnedcardinalities. github.com/andreaskipf/\nlearnedcardinalities. [Online; accessed April, 2020].\n[17] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe case for learned index structures. In Proceedings of the 2018 International\nConference on Management of Data . ACM, 489â€“504.\n[18] Sanjay Krishnan, Zongheng Yang, Ken Goldberg, Joseph Hellerstein, and Ion\nStoica. 2018. Learning to optimize join queries with deep reinforcement learning.\narXiv preprint arXiv:1808.03196 (2018).\n[19] Viktor Leis, Andrey Gubichev, Atanas Mirchev, Peter Boncz, Alfons Kemper, and\nThomas Neumann. 2015. How good are query optimizers, really? Proceedings of\nthe VLDB Endowment 9, 3 (2015), 204â€“215.\n[20] Viktor Leis, Bernhard Radke, Andrey Gubichev, Alfons Kemper, and Thomas\nNeumann. 2017. Cardinality Estimation Done Right: Index-Based Join Sampling..\nInCIDR .\n[21] Viktor Leis, Bernhard Radke, Andrey Gubichev, Atanas Mirchev, Peter Boncz,\nAlfons Kemper, and Thomas Neumann. 2018. Query optimization through the\nlooking glass, and what we found running the join order benchmark. The VLDB\nJournal (2018), 1â€“26.\n[22] Feifei Li, Bin Wu, Ke Yi, and Zhuoyue Zhao. 2016. Wander join: Online aggre-\ngation via random walks. In Proceedings of the 2016 International Conference on\nManagement of Data . 615â€“629.\n[23] Ryan Marcus, Parimarjan Negi, Hongzi Mao, Chi Zhang, Mohammad Alizadeh,\nTim Kraska, Olga Papaemmanouil, and Nesime Tatbul. 2019. Neo: A Learned\nQuery Optimizer. PVLDB 12, 11 (2019), 1705â€“1718.\n[24] James Martens and Venkatesh Medabalimi. 2014. On the expressive efficiency of\nsum product networks. arXiv preprint arXiv:1411.7717 (2014).\n[25] M Muralikrishna and David J DeWitt. 1988. Equi-depth multidimensional his-\ntograms. In ACM SIGMOD Record , Vol. 17. ACM, 28â€“36.\n[26] Kevin P Murphy. 2012. Machine learning: a probabilistic perspective . MIT press.[27] Neural Relation Understanding (Naru). 2020. Github repository, naru. github.\ncom/naru-project/naru. [Online; accessed April, 2020].\n[28] Patrick Oâ€™Neil and Dallan Quass. 1997. Improved query performance with\nvariant indexes. In Proceedings of the 1997 ACM SIGMOD international conference\non Management of data . 38â€“49.\n[29] Hoifung Poon and Pedro Domingos. 2011. Sum-product networks: A new deep\narchitecture. In 2011 IEEE International Conference on Computer Vision Workshops\n(ICCV Workshops) . IEEE, 689â€“690.\n[30] Viswanath Poosala, Peter J. Haas, Yannis E. Ioannidis, and Eugene J. Shekita.\n1996. Improved Histograms for Selectivity Estimation of Range Predicates. In\nProceedings of the 1996 ACM SIGMOD International Conference on Management\nof Data (Montreal, Quebec, Canada) (SIGMOD â€™96) . ACM, New York, NY, USA,\n294â€“305.\n[31] Viswanath Poosala and Yannis E Ioannidis. 1997. Selectivity estimation without\nthe attribute value independence assumption. In VLDB , Vol. 97. 486â€“495.\n[32] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. 2019. Language models are unsupervised multitask learners. URL\nhttps://openai.com/blog/better-language-models (2019).\n[33] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. 2017. Pixel-\nCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood\nand Other Modifications. In 5th International Conference on Learning Representa-\ntions, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings .\n[34] P Griffiths Selinger, Morton M Astrahan, Donald D Chamberlin, Raymond A\nLorie, and Thomas G Price. 1979. Access path selection in a relational database\nmanagement system. In Proceedings of the 1979 ACM SIGMOD international\nconference on Management of data . ACM, 23â€“34.\n[35] Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine\nTranslation of Rare Words with Subword Units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers) .\nAssociation for Computational Linguistics, Berlin, Germany, 1715â€“1725.\n[36] R. Sethi, M. Traverso, D. Sundstrom, D. Phillips, W. Xie, Y. Sun, N. Yegitbasi, H.\nJin, E. Hwang, N. Shingte, and C. Berner. 2019. Presto: SQL on Everything. In\n2019 IEEE 35th International Conference on Data Engineering (ICDE) . 1802â€“1813.\n[37] Michael Stillger, Guy M Lohman, Volker Markl, and Mokhtar Kandil. 2001. LEO-\nDB2â€™s learning optimizer. In VLDB , Vol. 1. 19â€“28.\n[38] Ji Sun and Guoliang Li. 2019. An end-to-end learning-based cost estimator.\nProceedings of the VLDB Endowment 13, 3 (2019), 307â€“319.\n[39] Immanuel Trummer, Junxiong Wang, Deepak Maram, Samuel Moseley, Saehan\nJo, and Joseph Antonakakis. 2019. SkinnerDB: Regret-Bounded Query Evaluation\nvia Reinforcement Learning. In Proceedings of the 2019 International Conference\non Management of Data (SIGMOD â€™19) . ACM, New York, NY, USA, 1153â€“1170.\n[40] Kostas Tzoumas, Amol Deshpande, and Christian S Jensen. 2011. Lightweight\ngraphical models for selectivity estimation without independence assumptions.\nProceedings of the VLDB Endowment 4, 11 (2011), 852â€“863.\n[41] Kostas Tzoumas, Amol Deshpande, and Christian S Jensen. 2013. Efficiently\nadapting graphical models for selectivity estimation. The VLDB Journal 22, 1\n(2013), 3â€“27.\n[42] Aaron Van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol\nVinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu.\n2016. WaveNet: A generative model for raw audio. arXiv preprint arXiv:1609.03499\n(2016).\n[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information processing systems . 5998â€“6008.\n[44] Chenggang Wu, Alekh Jindal, Saeed Amizadeh, Hiren Patel, Wangchao Le, Shi\nQiao, and Sriram Rao. 2018. Towards a learning optimizer for shared clouds.\nProceedings of the VLDB Endowment 12, 3 (2018), 210â€“222.\n[45] Richard Wu, Aoqian Zhang, Ihab Ilyas, and Theodoros Rekatsinas. 2020.\nAttention-based Learning for Missing Data Imputation in HoloClean. Proceedings\nof Machine Learning and Systems (2020), 307â€“325.\n[46] Yingjun Wu, Jia Yu, Yuanyuan Tian, Richard Sidle, and Ronald Barber. 2019.\nDesigning succinct secondary indexing mechanism by exploiting column cor-\nrelations. In Proceedings of the 2019 International Conference on Management of\nData . 1223â€“1240.\n[47] Zongheng Yang, Badrish Chandramouli, Chi Wang, Johannes Gehrke, Yinan Li,\nUmar Farooq Minhas, Per-Ã…ke Larson, Donald Kossmann, and Rajeev Acharya.\n2020. Qd-tree: Learning Data Layouts for Big Data Analytics. In Proceedings of\nthe 2020 International Conference on Management of Data (SIGMOD â€™20) .\n[48] Zongheng Yang, Eric Liang, Amog Kamsetty, Chenggang Wu, Yan Duan, Xi Chen,\nPieter Abbeel, Joseph M Hellerstein, Sanjay Krishnan, and Ion Stoica. 2019. Deep\nUnsupervised Cardinality Estimation. Proceedings of the VLDB Endowment 13, 3\n(2019), 279â€“292.\n[49] Barzan Mozafari Yongjoo Park, Shucheng Zhong. 2020. QuickSel: Quick Selectiv-\nity Learning with Mixture Models. SIGMOD (2020).\n[50] Zhuoyue Zhao, Robert Christensen, Feifei Li, Xiao Hu, and Ke Yi. 2018. Random\nsampling over joins revisited. In Proceedings of the 2018 International Conference\non Management of Data . 1525â€“1539.",
  "textLength": 86426
}