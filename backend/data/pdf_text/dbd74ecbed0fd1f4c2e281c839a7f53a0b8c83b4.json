{
  "paperId": "dbd74ecbed0fd1f4c2e281c839a7f53a0b8c83b4",
  "title": "LearnedKV: Integrating LSM and Learned Index for Superior Performance on Storage",
  "pdfPath": "dbd74ecbed0fd1f4c2e281c839a7f53a0b8c83b4.pdf",
  "text": "LearnedKV: Integrating LSM and Learned Index for Superior Performance on\nStorage\nWenlong Wang\nUniversity of Minnesota, Twin CitiesDavid Hung-Chang Du\nUniversity of Minnesota, Twin Cities\nAbstract\nWe present LearnedKV , a novel tiered key-value store that\nseamlessly integrates a Log-Structured Merge (LSM) tree\nwith a Learned Index to achieve superior read and write per-\nformance on storage systems. While existing approaches use\nlearned indexes primarily as auxiliary components within\nLSM trees, LearnedKV employs a two-tier design where the\nLSM tree handles recent write operations while a separate\nLearned Index accelerates read performance. Our design in-\ncludes a non-blocking conversion mechanism that efficiently\ntransforms LSM data into a Learned Index during garbage\ncollection, maintaining high performance without interrupt-\ning operations. LearnedKV dramatically reduces LSM size\nthrough this tiered approach, leading to significant perfor-\nmance gains in both reads and writes. Extensive evaluations\nacross diverse workloads show that LearnedKV outperforms\nstate-of-the-art LSM-based solutions by up to 4.32x for read\noperations and 1.43x for writes. The system demonstrates\nrobust performance across different data distributions, access\npatterns, and storage media including both SSDs and HDDs.\n1 Introduction\nUnstructured data is projected to constitute over 80% of all\ndata collected globally by 2025 [17], expanding at an annual\nrate of 55-65%. Traditional relational databases struggle with\nsuch data’s variability and complexity, leading to the rise of\nKey-Value (KV) stores [11, 12, 15, 19, 21, 39, 45, 49, 50]. KV\nstores organize data as key-value pairs, providing schema-\nfree flexibility for diverse applications like caching, session\nstorage, and large-scale data processing.\nThe Learned Index [29] has transformed indexing systems\nby learning key distribution patterns to predict data locations\nwithin a tolerable range. Unlike B+-Trees, it eliminates ex-\nplicit mappings, reducing pointer traversals and index space\nwhile improving read performance. However, write/update\nand insert operations remain challenging as they can inval-\nidate learned models by shifting key distributions. Whileupdatable variants exist [20, 46], they struggle to match the\nperformance of traditional indexes in larger-than-memory\nsystems [30].\nInitially positioned as an in-memory index [20, 22, 23, 28,\n29, 32, 43, 46], the Learned Index faced significant challenges\nin persistence and recovery. System crashes would require\ncomplete index rebuilding through full data scans, making\nstorage adaptation essential for practical deployments.\nThe Log-Structured Merge-Tree (LSM-Tree) [37] rep-\nresents a crucial storage-centric indexing approach. It ex-\ncels in write-intensive applications by organizing data into\nmultiple layers and converting small random writes into\nlarger sequential operations through periodic merging called\ncompactions. This design has spawned numerous variants\n[3,4,14,24,25,31,41,45,47], with prominent implementations\nin production systems like Meta’s RocksDB [4], Google’s\nLevelDB [3], Amazon’s DynamoDB [7], and Apache HBase\n[8].\nRecent studies have explored LSM and Learned Index com-\nbinations [9, 16, 35]. However, these approaches merely use\nthe Learned Index as a supplementary path for SST files, over-\nlooking its potential to replace traditional indexing mech-\nanisms in a tiered structure that could fully leverage the\nstrengths of both designs.\nWe analyze key trade-offs between LSM and Learned In-\ndex structures: LSM provides efficient writes but suffers from\nincreasing I/O amplification due to compactions and less read\nperformance due to the required search in multiple layers as\nthe data grows, while Learned Index enables fast reads with\ncompact space usage but requires sorted, preferably static\ndatasets and struggles with updates. To leverage these com-\nplementary strengths, we propose a tiered architecture where\nLSM handles write operations and Learned Index accelerates\nread performance. While ideally static data would remain in\nthe Learned Index and modifications would route to LSM,\npredicting update patterns is impractical. We therefore intro-\nduce a data migration mechanism that moves entries between\nthe two indexes to optimize the overall system performance.\nIn many KV store applications, small keys are associatedarXiv:2406.18892v2  [cs.DB]  11 Apr 2025\n\nwith significantly larger values, ranging from 100B to over\n10KB [10, 11, 13, 18, 36]. \"KV Separation\" [36] mitigates\nwrite amplification by storing complete key-value pairs in\nan append-only Value Log, while maintaining only keys and\npointers in the LSM tree. For such systems, Garbage Collec-\ntion (GC) helps maintain reasonable storage utilization by\nreclaiming space from outdated or deleted pairs.\nWhen storage space is constrained, GC triggers more fre-\nquently. During GC, we observed that in-memory key sorting\nincurs negligible overhead compared to data migration costs.\nThis efficiency enables appending key-value pairs in sorted\norder, creating an ideal dataset for learned model construction.\nSince log entries remain static between GC cycles, the dataset\nprovides the stability that is necessary for effective learned in-\ndexes. Consequently, converting the LSM to a Learned Index\nduring GC becomes both feasible and efficient, significantly\naccelerating reads with minimal performance impact.\nIn this paper, we introduce LearnedKV , an efficient tiered\nkey-value store that integrates LSM and Learned Index for\nsuperior storage performance. Through thoughtful design,\nthis tiered architecture leverages LSM’s high update through-\nput and Learned Index’s read efficiency in a complementary\nmanner.\nOur key contributions are:\n•We identify that GC-appended valid key-value pairs\nin KV separation designs provide an ideal dataset\nfor Learned Index construction, addressing its storage-\nrelated limitations.\n•We propose LearnedKV , a novel tiered design where\nLearned Index directly references key-value pairs, reduc-\ning LSM size and I/O overhead while improving read\nperformance.\n•We develop a non-blocking conversion mechanism that\nbuilds the Learned Index during GC while maintaining\nsystem availability.\n•Our evaluations show that LearnedKV outperforms ex-\nisting LSM-based solutions by up to 332\nThe rest of this paper is organized as follows: Section 2\nprovides background, Section 3 presents our motivation and\nchallenges, Section 4 details LearnedKV’s design and imple-\nmentation, Section 5 presents our experimental evaluation,\nSection 6 discusses future work and concludes the paper.\n2 Background\n2.1 LSM and RocksDB\nThe Log-Structured Merge-Tree (LSM-Tree) [37], introduced\nin 1996, is widely adopted for persistent key-value storage.\nIt efficiently transforms small random writes into large, se-\nquential updates, beneficial for accessing SSDs and HDDs.And there are many researchers have explored methods to\nsupport large-scale key-value stores using the LSM struc-\nture [14, 24, 25, 31, 47]. RocksDB [4], developed by Meta,\noptimizes the LSM structure for fast storage media and write-\nintensive workloads.\nAs shown in Figure 1, RocksDB maintains in-memory\nMemtables and on-storage Write-Ahead Logs (WAL) and\nSST files. New writes enter a mutable Memtable, which, when\nfull, becomes immutable and flushes to storage as an SST file.\nSST files are organized in levels ( L0toLn), with higher lev-\nels containing fewer files and recent updates and insertions.\nA background compaction process periodically merges files\nfrom a adjacent higher level to a lower level, removing redun-\ndant and outdated entries.\nRead operations search through Memtables and SST files\nacross levels. While L0may require multiple file searches due\nto overlapping key ranges, lower levels guarantee at most one\nfile search per level. RocksDB employs both file-level index-\ning and Bloom filters to minimize unnecessary I/O operations.\nHowever, the read performance is likely to be reduced with\nmore data stored in more levels.\nMemtable\nSST\nSST\nSST...\n...DRAM\nSSDW rite\nW ALImmutable\nMemtable\nL0\nL1\nLn...\nFigure 1: RocksDB architecture. In this figure and following\nsections, L0represents the highest level and Lnrepresents the\nlowest level.\n2.2 KV Separation\nReal-world key-value workloads often feature small keys (8-\n16 bytes) paired with large values (100B to 10KB+) [10, 11,\n13, 18, 36]. KV separation techniques [33, 36, 42] address the\nresulting I/O overhead by storing values in a separate append-\nonly log while keeping only keys and pointers in the LSM\ntree.\nWiscKey [36] pioneered this approach, introducing garbage\ncollection (GC) to reclaim space from outdated entries.\nHashKV [33] and FenceKV [42] further improved efficiency\nby partitioning the key space across multiple logs. The GC\nprocess is crucial for managing storage capacity, reducing\nspace overhead, and maintaining system performance through\nefficient I/O management.\n2\n\n2.3 In-memory Learned Index\nThe Learned Index [29] introduced a novel approach to index-\ning by employing machine learning to predict key locations\nin sorted datasets. Initially developed for in-memory envi-\nronments [20, 22, 23, 28, 29, 32, 43, 46], it replaces explicit\nkey-to-location mappings with lightweight models trained\nusing methods like Linear Regression and Neural Networks.\nDue to complex real-world key distributions, perfect pre-\ndictions are likely unfeasible, requiring local searches around\npredicted locations to identify the target key. To bound worst-\ncase performance, designs incorporate error bounds [28, 29]\nand hierarchical models that partition key ranges into smaller,\nmore manageable segments. These approaches have demon-\nstrated 1.8x to 3.2x read performance improvements over\ntraditional indexes [29].\n2.4 On-storage Learned Index with LSM\nAs datasets grow beyond DRAM capacity, adapting Learned\nIndexes to SSDs has become crucial. While most of the pure\nLearned Index are yet to consistently outperform traditional\nB+-tree indexes [30], some works are trying to integrate the\nLearned Indexes with LSM trees for large-scale key-value\nstores [9, 16, 34, 35, 44]. Bourbon [16] uses piecewise linear\nregression to optimize SST file lookups, while Google’s ap-\nproach [9] implements model-guided data placement. Triden-\ntKV [35] adapts model training based on workload patterns.\nMemtable\nSST...\n...1Locate File\n4Load Chunk5Local Search\nValue Log<Key , Value>\n6Load KVDRAM\nSSD\nL0\nL1\nLn...2Load BF3\nModel\n<Key , Offset>Search: Key\nModel Lookup\nFigure 2: Bourbon lookup process\nOne representative of them is Bourbon [16], whose detailed\narchitecture and lookup process are shown in Figure 2. When\nsearching for a target key, it firstly locates a candidate file and\nreads its Bloom Filter as traditional LevelDB/RocksDB does.\nIf the Bloom Filter indicates the key’s potential presence, it\nthen queries the corresponding learned model of the SST to\npredict the location of the data chunk that should be loaded.\nThen it loads the data chunk and performs a local search to\nfind the target key together with its offset in the Value Log.\nAt last, it loads the target key-value pair from the Value Log.\nThe Bourbon, together with some other Learned Index-\nLSM combination, primarily treat the Learned Index as a\nsupplementary component for SST file optimization rather\nthan fundamentally redesigning the index structure. As shownin Figure 2, Bourbon still relies on LSM procedures for file lo-\ncation while using learned models only for intra-file searches.\nThis pattern of limited integration is common across existing\nsolutions, suggesting opportunities for more comprehensive\narchitectural changes.\n3 Motivations and Challenges\n3.1 Reducing LSM Size with Learned Index\nfor Enhanced Performance\nDespite there are some research work regarding the on-storage\nLearned Index explored recently [9, 16, 35], there are still\nseveral limitations inherent to these designs, which guide us\ntoward our proposed architecture: (1) Current approaches\ntypically bind Learned Index models to specific SST files,\nnecessitating frequent rebuilds during LSM compaction and\nresulting in redundant I/O operations. (2) Moreover, these\ndesigns often underutilize the Learned Index’s potential as\na standalone, compact, and efficient indexing structure, with\nits size being up to two orders of magnitude smaller than\ntraditional indexes when indexing the same amount of data\n[29].\nAs data volumes expand, LSMs must manage increasingly\nlarge-scale data within acceptable latencies. LSM storage\nusage often grows faster than the rate of raw key-value pair in-\ncrease because there may be multiple versions of the same key\nwithin different levels. Despite compaction mechanisms and\nlevel additions, LSM size inevitably expands, impacting both\nread and write performance: Read performance degrades with\nincreasing tree depth, as worst-case lookups may traverse to\nthe bottom level, checking key availability at each level. Write\nperformance suffers from increased compaction frequency\nand cost due to more SST files, affecting write throughput as\ncompaction overhead is amortized across operations.\n1,000,000 5,000,000 10,000,000\nInitial Data Size0100020003000400050006000Time Cost (ms)\n 2,0395,1756,443Read Time Cost\n1,000,000 5,000,000 10,000,000\nInitial Data Size010002000300040005000Time Cost (ms)\n3,8014,0975,492Write Time CostLSM Size Performance Comparison\nFigure 3: Read and write time cost of the RocksDB across\ndifferent stages\nTo quantify these effects, we conducted experiments using\nRocksDB [4] with initial data sizes of 1, 5, and 10 million keys,\nmeasuring time costs for 1 million read and write operations\neach. In the experiment, we won’t trigger any GC but will\n3\n\nenable background compaction as configured by the default\nsettings. Figure 3 illustrates the performance degradation as\nLSM size increases, with read time cost rising by 216% and\nwrite time cost increasing by 44.5% when the dataset size\nincreases from 1 to 10 million keys.\nWhile existing studies on accelerating LSMs with on-\nstorage Learned Indexes [9, 16, 35] have focused primarily\non replacing SST file index blocks, they overlook opportu-\nnities for LSM size reduction. This gap motivates our tiered\ndesign that leverages Learned Indexes to reduce LSM size,\npotentially improving both write throughput (through smaller\nLSMs) and read throughput (via optimized Learned Indexes),\nespecially for larger datasets.\n3.2 Leveraging Complementary Strengths of\nLearned Index and LSM\nAs discussed in Section 2, Learned Indexes demonstrate su-\nperior read performance over traditional B+-Trees, showing\n1.8x to 3.2x improvements for static in-memory datasets [29].\nHowever, their performance in SSD environments and update\nefficiency remain limited [30]. Conversely, LSMs excel in\nhandling update-intensive workloads on SSDs due to their\nability to transform small random writes into large sequential\noperations [4]. This complementary nature motivates our hy-\nbrid design that combines Learned Indexes’ fast lookups with\nLSMs’ efficient update handling.\nUnlike existing solutions [9, 16, 35], we propose a tiered\nindex where LSM absorbs random writes while Learned Index\naccelerates lookups. This design offers several advantages:\n•Lookup performance : Our system queries LSM first, then\nLearned Index if necessary, or accesses both simultane-\nously in multi-threaded scenarios. Based on Bourbon’s\n\"level learning\" concept [16], the Learned Index can\nspeed up read queries by up to 92%. The overhead from\nLSM failure lookups remains minimal due to reduced\nLSM size.\n•Reduced LSM size : Learned Index’s partial dataset index-\ning reduces LSM size and compaction costs. Addition-\nally, building the Learned Index during GC eliminates\npost-GC re-insertion overhead and cache pollution.\n•Write efficiency : Support writes with pure LSM-level\nlatency through Memtable handling.\nThe elegance of our approach lies in its simplicity and\neffectiveness - by fully decoupling these components, each in-\ndexing structure can operate in its optimal domain. Therefore,\nthis design can potentially outperform current state-of-the-art\nsolutions in both read and write operations while maintaining\nrobust worst-case performance.3.3 Read-Optimized Indexing for Post-GC\nStatic Data\nvlogLSM\n\"static\"\nvlogLSMLSM\n\"static\"\nvlogLSM IndexTraditional approach\nNew approachGC\nIndexGCWrite\nWritevlog\nvlog\nFigure 4: Using read-optimized index (Learned Index) to\nindex the \"static\" data\nAs shown in the upper part of Figure 4, traditional KV-\nseparated LSM systems periodically trigger GC to compact\nboth the LSM and vlog by removing duplicates. Then, sub-\nsequent writes append new key-value pairs to the vlog after\nthe compacted section (yellow part in the figure), creating a\n\"static\" region that only serves reads without further modifica-\ntions. We call this region \"static\" because after GC processes\nthe data, these key-value pairs remain unchanged until the\nnext GC cycle—they are not updated in place but instead, any\nmodifications result in new entries appended at the end of the\nvlog. This stability makes the region ideal for read-focused\noptimization since its data distribution and location remain\nfixed. This observation reveals a significant opportunity: the\npost-GC static data could benefit from a different indexing\nstructure than LSM.\nLSM trees excel at write performance through buffering\nand compaction but introduce overhead for read operations,\nparticularly as tree depth increases. For static data that no\nlonger changes, this write optimization becomes unneces-\nsary overhead. In contrast, specialized read-optimized indexes\nlike Learned Indexes can provide significantly faster lookups\nwhen operating on stable data distributions. The top portion of\nFigure 4 illustrates the traditional approach where LSM con-\ntinues to index all data, including the static region. The bottom\nportion shows our proposed approach, where we strategically\napply a read-optimized index (specifically a Learned Index)\nto the static data while maintaining LSM only for the active\nportion that receives new writes.\nThis hybrid approach capitalizes on the complementary\nstrengths of both indexing structures: LSM efficiently handles\nwrite-intensive workloads with its batching and compaction\nmechanisms, while the Learned Index provides superior read\nperformance for the stable data patterns in the static region.\nBy applying the appropriate index to each data region based\non its access pattern, we can optimize both read and write\nperformance without compromising either.\n4\n\n3.4 Challenges\nThe proposed tiered index architecture presents several signif-\nicant technical challenges. As the LSM component continu-\nously absorbs random writes while the Learned Index remains\nstatic, the Learned Index gradually becomes outdated, dimin-\nishing its performance advantages over time. This necessitates\nan efficient mechanism to periodically migrate data from the\nLSM to the Learned Index while minimizing overhead.\nFurthermore, a robust KV store must maintain uninter-\nrupted operation during internal structure modifications, re-\nquiring a conversion mechanism that preserves acceptable\nrequest latency without sacrificing data consistency or dura-\nbility. With data distributed across two indexing structures,\nefficient query routing becomes essential—the system must\nefficiently determine which index to query first and how to\nhandle potential duplication when the same key exists in both\nindexes.\nThese challenges highlight the complexity of effectively\nintegrating two fundamentally different indexing paradigms.\nIn the following section, we describe how LearnedKV ad-\ndresses these challenges through its novel architecture and\nconversion mechanisms that efficiently transform LSM data\ninto the Learned Index during garbage collection processes\nwhile maintaining continuous system operation.\n4 LearnedKV\n4.1 Architecture and Basic Operations\nWe propose LearnedKV , an efficient tiered key-value store\ncombining an LSM and a Learned Index for high-performance\nread and write operations on SSDs. Figure 5 illustrates the\noverall architecture and basic operations.\nLearnedKV comprises three main components: an LSM\nTree, a Learned Index, and two Value Logs. The \"Value Log\"\nis append-only and indexed by the LSM, while the \"Static\nValue Log\" stores key-value pairs indexed by the Learned\nIndex and remains static until a GC process occurs. For clari-\nfication, unless specified, the \"Value Log\" will be referred to\nas the one indexed by the LSM in our later discussion.\nThe Value Log, following common practice, is fixed-size\nand accumulates both valid and invalid key-value (KV) pairs\nuntil a GC process is triggered. Valid KV pairs are newly\ninserted or never updated entries. When a new KV pair with\nan existing key is appended, the old entry becomes logically\n\"invalid\". Both LSM and Learned Index store key-offset pairs,\nwhere the offset indicates the KV pair’s location in the respec-\ntive Value Log.\nDuring a write operation, the key-value pair is first ap-\npended to the Value Log, and its position is recorded as\nthe \"offset\". The < key ,o f f set > pair is then inserted into the\nMemtable. When the Memtable reaches capacity, it is flushed\nto storage and compacted into the LSM tree, following a\nMemtable\nLearned IndexDRAM\nSSDLSMwrite:\n<Key , Value>\n<Key , Offset>\n<Key ,Value>\nFlush\nK\nVK\nV... ... Value Log ... ... Valid KV\nInvalid KV\nEmpty slotOffset... ... ...\nStatic Value LogK\nV(a)Write operation in LearnedKV\nMemtable\nLearned IndexDRAM\nSSD LSMSear ch: Key\nIf not found\nIf not found\nRead Offset Read Offset<Key , Value>\nK\nVK\nV... ...\nValue Log... ... Valid KV\nInvalid KV\nEmpty slot... ... ...\nStatic Value LogK\nV\n(b)Read operation in LearnedKV\nFigure 5: LearnedKV Architecture\nprocess similar to RocksDB.\nK\nO... ... Key List ...\nRecord the offset of KV  pair  in the Static Value Log\nK\nVStatic Value LogK\nOK\nO<s, a, b> <s, a, b> <s, a, b> ... ModelsLearned Index\nFigure 6: Learned Index Architecture\nFor point search requests, we first query the in-memory\nMemtable, returning the result immediately if found. If not,\nwe search the on-storage LSM, followed by the Learned Index\nif necessary. Figure 6 illustrates the Learned Index structure,\ncomprising a list of models and a \"Key List\" file. Following\nthe trend of prior studies, we employ Linear Models in our\nLearned Index. Each model is defined as a starting key (s),\nslope (a), and intercept (b). To search, we use binary search\nto find the largest starting key (s_i) not exceeding the target\nkey, then apply the linear model y = a*s_i + b to predict the\noffset in the \"Key List\" file. This file contains sorted Key-\nOffset pairs, where \"Offset\" indicates the KV pair’s position\n5\n\nin the Static Value Log. The predicted Key-Offset pair and\nsurrounding pairs within the error range are loaded into mem-\nory. Once identified, the offset is used to retrieve the full KV\npair from the Static Value Log. Similarly, if a key is found in\nthe LSM, its offset is used to load the KV pair from the Value\nLog. When a key exists in both the LSM and Learned Index,\nwe prioritize the LSM version, as our mechanism ensures\nLSM-resident keys are more recent and thus valid.\nIt’s worth noting that while our design significantly re-\nduces LSM size—often to a point where it could theoretically\nfit entirely in memory—we deliberately maintain the LSM\ntree on persistent storage rather than keeping it exclusively\nin DRAM. This design choice ensures durability and crash\nrecovery capabilities, as the LSM component contains the\nmost recent updates to the key-value store. This persistence\nstrategy maintains the robustness of traditional LSM-based\nsystems while leveraging our tiered approach for superior\nperformance, striking an optimal balance between durability\nand efficiency.\n4.2 GC-triggered Conversion from LSM to\nLearned Index\nA key contribution of our work is the ability to efficiently\nconvert the entire LSM into a Learned Index during garbage\ncollection (GC) while maintaining uninterrupted system op-\neration. This process addresses the challenge of building a\nLearned Index, which typically requires a sorted, static dataset\nwhile ensuring continuous service to user requests.\nAs shown in Figure 7, our GC and conversion process,\ninspired by HashKV [33] and FenceKV [42], operates in\nfour phases. Initially, the system runs normally using LSM,\nLearned Index, Value Logs on SSD, and an active Memtable.\nUpon GC initiation, we create a new LSM tree and Value Log\nfor future writes, freeze the old Memtable, and collect valid\nkey-value pairs from the old Value Log. We then sort these\nvalid pairs in memory ( O(nlogn )time complexity), migrate\nthem to a new Static Value Log, and construct a new Learned\nIndex. This in-memory sorting incurs negligible costs com-\npared to storage access latencies, requiring less than 80MB\nof temporary memory for our 10M-entry benchmark, which\nis released immediately after construction, making it feasible\neven on memory-constrained systems.\nOur non-blocking GC process is designed to handle com-\nplete dataset compaction without imposing strict time con-\nstraints. Unlike traditional approaches where GC must com-\nplete quickly to minimize service disruptions, LearnedKV’s\narchitecture supports concurrent user operations throughout\nthe entire GC cycle. By creating separate data structures for\nincoming writes while processing the old ones, we enable\ncontinuous service without performance degradation. This\napproach allows thorough processing of the entire dataset for\noptimal space reclamation and index construction without\ntime pressure, leading to better storage utilization and moreefficient indexing. This capability represents a significant\nadvantage over systems that must compromise between GC\ncompleteness and operation latency.\nTo be noticed that, only the < key ,o f f set > pairs will be\nstored and sorted in the memory, the large-sized key-value\npairs are directly rewritten to the new static log. This approach\nis memory-efficient, requiring only megabytes of memory\nsince the key-offset pairs are typically much smaller than the\nfull key-value pairs. The Key List layer maintains logical\nsorting of key-value pairs while allowing physical storage to\nremain optimized for I/O performance.\nThis approach ensures continuous operation during GC\nwhile efficiently managing data and index structures. Write\noperations remain uninterrupted due to the separate new LSM\nand Value Log. For reads during GC, we query both old and\nnew structures, with performance impact mitigated by the\nLearned Index’s optimization for read operations. The sorted\nvalid key-value pairs in the new Static Value Log, along with\na \"Key List\" file containing sorted keys and their locations,\nprovide an ideal static, sorted dataset for building the Learned\nIndex. This file is recreated during each GC cycle and remains\nunmodified between cycles. The resulting Learned Index,\ncontaining all valid keys from the LSM in a more compact and\nread-efficient format, replaces the old LSM tree. We reclaim\nthe space occupied by the old LSM and value log, creating a\nnew LSM for incoming writes.\nThis combined GC and conversion process effectively ad-\ndresses both the challenge of efficient data conversion from\nLSM to Learned Index and the need for non-blocking sup-\nport during conversion, ensuring system responsiveness and\nperformance optimization throughout the process.\n4.3 Learned Index: Greedy-PLR+\nWhile our design can accommodate various Learned Index\nmodels (e.g., RS [28], RMI [29], ALEX [20]), we priori-\ntize minimal overhead in model building and querying, with-\nout requiring in-place update capabilities. Following Bour-\nbon [16], we adopt Greedy-PLR (Piece-wise Linear Repre-\nsentation) [48] as our baseline statistical model and leave the\nexploration of the best-fit model for future research.\nOur variant, Greedy-PLR+, focuses on page distance rather\nthan absolute location distance, better suiting block device\ncharacteristics. Greedy-PLR+ builds piece-wise linear seg-\nments from sorted key-location pairs with O(n)complexity.\nStarting with an initial segment from the first two points, it\nprogressively adds points within a defined page-number error\nbound or creates new segments when the bound is exceeded.\nThis produces a compact model list, each entry containing\n(start_key, slope, intercept).\nOur query processing uses two files: a sorted \"Key List\"\ncontaining (key, offset) pairs and cached \"Models\" in memory\nstoring line segments. Given the Models’ extremely com-\npact size - our evaluation (Table 1) shows it occupies only\n6\n\nMemtable\nValue LogDRAM\nSSD\nLearned IndexLSM\n(a)<Key , Value>\n<Key , Offset>\n<Key ,Value>\nStatic\nValue LogImmutable\nMemtable\nValue\nLog\nLearned IndexLSMValid\nKeysMemtable\n(new)\nValue Log\n(new)LSM (new)DRAM\nSSD\n(b)<Key , Value>\n<Key , Offset>\n<Key ,Value>\nStatic V alue\nLogStatic V alue\nLog (new)DRAM\nLearned Index\n(new)Memtable\nValue Log\n(new)LSM (new) SSD\n(c)<Key , Value>\n<Key , Offset><Key ,Value>\nStatic V alue\nLog (new)DRAM\nLearned Index\n(new)Memtable\nValue Log\n(new)LSM (new) SSD\n(d)<Key , Value>\n<Key , Offset> <Key , Value>\nStatic V alue\nLog (new)Figure 7: Non-blocking Garbage Collection and Conversion. The dashed box means the file is newly created in the current\nstep.\nkilobytes, about 1/1000th the size of other components - the\nmemory overhead is negligible. Queries first search the in-\nmemory Models to locate the appropriate segment, predict the\nkey’s position, and load a bounded data chunk from the Key\nList. The matched offset is then used to retrieve the key-value\npair from the Value Log, minimizing overall I/O operations.\n4.4 Range Scan\nRange scan support is essential for modern, large-scale KV\nstores. LearnedKV’s tiered architecture necessitates efficient\nscanning across both the LSM tree and Learned Index to en-\nsure comprehensive data retrieval. Our LSM implementation,\nRocksDB [4], provides a built-in iterator API for range scans,\nenabling sorted sequential access to key-value pairs within a\nspecified range. RocksDB maintains individual iterators for\neach Memtable and SST file, managed by a \"MergingIterator\"\nthat exposes them as a sorted stream.\nFor the Learned Index, keys are stored in the sorted \"Key\nList\" file. During a range scan, LearnedKV predicts the loca-\ntions of start_key and end_key using the models, loads the\nsurrounding data chunks, and determines their exact positions.\nIntermediate chunks are then loaded to identify keys within\nthe scan range.\nTo merge the sorted key lists from the LSM and Learned\nIndex, we iterate through both, adding the smaller key to the\nresult list. For duplicate keys, we prioritize the LSM version\nas it contains the most recent updates. This process continues\nuntil both lists are exhausted. Finally, we retrieve the corre-\nsponding key-value pairs from the Value Log/Static Value\nLog within the specified range and return them to users.\nOur experiments in Section 5.1 show that LearnedKV’s\nrange scan operations outperform pure RocksDB [4] by up\nto 4x for the same key range. This improvement can be con-\ntributed from two key factors:\nFirst, LearnedKV reduces the time required for level-by-\nlevel key searches. While RocksDB’s iterator presents keys in\nsorted order, they remain physically distributed across differ-\nent levels and files. Maintaining a \"MergingIterator\" across\nthese disparate locations incurs significant overhead com-pared to the Learned Index’s direct, consecutive data chunk\nreads.\nSecond, key-value pairs in the Learned Index are stored\ncontiguously in the Value Log. During the GC process (Sec-\ntion 4.2), valid key-value pairs are appended to the Value Log\nin sorted order and used to construct the Learned Index. This\nsorted structure in both the Key List and Value Log remains\nintact until the next Learned Index construction, enabling effi-\ncient range scans. Conversely, the LSM’s Value Log appends\nkey-value pairs based on request timing, not key order, po-\ntentially scattering related pairs across the file and leading to\nless efficient random reads and page fragmentation.\n4.5 Optimizations\nWhile our general design is effective, we propose additional\noptimizations to enhance performance in specific scenarios.\nThese optimizations are not essential concepts of our design\nbut can offer benefits in certain circumstances.\n4.5.1 Range-query Assisted Conversion\nOur standard approach triggers conversion during GC, incur-\nring minimal extra cost for building the Learned Index and\navoiding LSM re-insertion. However, in over-provisioned stor-\nage scenarios or infrequent updates where GC is infrequent,\nthis may delay conversion and limit the Learned Index’s bene-\nfits. To address this, we introduce the \"Range-query Assisted\nConversion\" algorithm for proactive conversion.\nThis proactive conversion is triggered when the LSM’s size\nor level count reaches a predefined threshold, typically when\nLSM performance begins to degrade. We then perform a full\nrange query across the LSM and Learned Index, merging\nand sorting key-value pairs from different SST files. Treating\nthe Learned Index as an additional bottom level allows for\njoint merging and sorting. Using the sorted keys and their\nValue Log offsets, we construct a new Learned Index while a\nnew LSM and Value Log absorb write operations, ensuring\nuninterrupted operation.\n7\n\nThis approach is lightweight, avoiding key-value pair mi-\ngration and allowing user-triggered conversion. However, it\nincurs some overhead for index building without improving\nspace utilization.\n4.5.2 In-memory Bloom Filter\nOur experiments reveal that even with highly skewed work-\nloads (Zipfian distribution), about one-tenth of read queries\naccess the learned index. If the concurrent probing is disabled,\nour tiered design necessitates an initial LSM check for all\nqueries, as recent keys reside there. This level-by-level LSM\ntraversal, though mitigated by RocksDB’s techniques, can ac-\ncumulate some overhead from false read attempts. To address\nthis, we implement a lightweight in-memory Bloom filter.\nWe maintain this Bloom filter for keys in the LSM tree,\nmarking representative bits for each new key added to the\nMemtable. Read requests first consult this filter. If the target\nkey’s bits are not fully marked, we skip the LSM and query\nthe Learned Index directly. Otherwise, we query the LSM,\naccepting a false positive rate under 5% in our experiments.\n5 Evaluation\nTo evaluate our LearnedKV , we compare our design with mul-\ntiple state-of-the-art KV stores in various test environments.\nFirst, in order to show the effectiveness of our tiered design,\nwe construct a micro-benchmark experiment to compare the\nperformance of the LearnedKV with its baseline RocksDB [4]\nthrough YCSB workloads [6] and SOSD datasets [27]. Then\nwe broaden our comparison to various situations, including\ndifferent workload distribution, various dataset size and a\nrange scan experiment. Then, we further include several other\nstate-of-the-art KV stores, Bourbon [16], HashKV [33], B+-\nTree [5], and some existing on-storage Learned Indexes into\nthe comparison. Finally, we drill down our analysis to our\ndesign choice and multiple parameter settings.\nImplementation. Given that modern workloads typically\nemploy large-sized values [36], we designed our KV store\naround the KV separation concept. We implemented KV sep-\naration for all baselines that did not originally include it. Fol-\nlowing prior work [33], we over-provisioned storage space by\n30% of the key-value pair size, ensuring comparable space al-\nlocation across all systems. For fair comparison, we equipped\nall systems with LearnedKV’s GC mechanism, differing only\nin the indexing approach, while maintaining equivalent mem-\nory consumption. We optimized hyper-parameters for each\nindex design and disabled concurrent probing of the Learned\nIndex during LSM searches in LearnedKV .\nEvaluation Setup. We conduct our experiments on a ma-\nchine with an AMD 64-Core Processor and a SAMSUNG\nSATA SSD of 447GB on Chameleon testbed [26]. For compu-\ntational convenience, unless otherwise specified, the key size\nand value size are set to 8 bytes and 1016 bytes, respectively,making each key-value pair 1 KB. The pointer/offset size in\nour testbed is 8 bytes.\nWorkload. In our experiments, we divide the performance\ntest into four phases: P0, P1, P2, and P3. In the first phase\nP0, we load 10M unique key-value pairs into the KV store.\nIn each subsequent phase (P1, P2, and P3), we perform 10M\nread or update operations. Given our 1KB key-value pair\nsize, the total data volume processed across these four phases\namounts to 40GB (10M operations ×1KB per operation ×4\nphases). The number of read or update operations is based\non the pre-configured read/write ratio of the workload, with\nrequests following a Zipfian distribution by default. Unless\notherwise specified, we collect the read and write throughput\nfrom the last phase to mitigate the performance impact of the\nGC process.\nFor experiments that do not involve any update or insert\noperations, we perform a GC once the loading phase (P0)\nis completed. This is because the loading phase only writes\nkeys into the LSM, and the GC process will not be passively\ntriggered by read-only workloads. Therefore, we manually\ntrigger a GC alongside the conversion from LSM to Learned\nIndex. This procedure is applied uniformly across all schemes,\nallowing us to highlight the performance differences with and\nwithout our Learned Index design more clearly.\nIn this section, we present the evaluation of LearnedKV by\nanswering the following questions:\n•How does the tiered index design benefit the perfor-\nmance?\n•How does the LearnedKV compare with other state-of-\nthe-art KV Stores?\n•What is the performance of the LearnedKV under various\nworkloads and various key distributions?\n•How does the performance get affected by different con-\nfigurations (e.g. storage over-provision ratio)?\n•How much does the performance benefit from our opti-\nmizations (e.g. In-memory Bloom Filter)?\n5.1 Overall Performance Comparison\nExperiment 1: Effect of Learned Index. We evaluate the im-\npact of the Learned Index using a read-write balanced (50%\nread and 50% write) YCSB workload with a Zipfian dis-\ntribution (s=0.99). The experiment involves loading 10M\nkey-value pairs followed by three operational phases, each\nconsisting of 5M read and 5M update operations.\nFigure 8 shows the overall throughput comparison\namong LearnedKV , and RocksDB+ over different phases.\n\"RocksDB+\" is the modified implementation of RocksDB\nto which we adapted our key-value separation and GC\nmechanism. The only difference between LearnedKV and\nRocksDB+ is that LearnedKV has a tiered layer of Learned\n8\n\nP0 P1 P2 P3020406080Write Throughput (KOPS)87.24\n34.25\n26.98 25.8578.94\n28.69\n21.01 19.82Write Throughput by Phase\nLearnedKV\nRocksDB\nP1 P2 P3\nPhases01020304050Read Throughput (KOPS)16.8049.38 48.79\n12.5114.76 15.65Read Throughput by Phase\nLearnedKV\nRocksDBFigure 8: Throughput Comparison between LearnedKV\nand RocksDB. P0 is load phase; P1,P2,P3 consist of read\nand update. The learned Index is built in the middle of P1.\nGC happens in P1,P2,P3)\nIndex while RocksDB dose not. In this way, we can clearly\nsee the effectiveness of our key design concept.\nFrom the results, we can see that, in Phase 0 (loading),\nboth systems perform similarly as the Learned Index is not\nyet built. During P1, LearnedKV begins to show advantages,\nbecause the Learned Index construction is triggered in the\nmiddle of P1. The performance gap gets wider in P2 and P3,\nwhere LearnedKV fully leverages its tiered index structure.\nLearnedKV outperforms RocksDB+ by up to 1.30x in write\nthroughput and 3.35x in read throughput during these phases,\nclearly demonstrating the effectiveness of the Learned Index.\nTable 1 illustrates the storage space usage at the end of P3.\nWhile Key-Value Space consumption remains constant due to\nidentical workloads and GC policies, LearnedKV significantly\nreduces the indexing size (2.98x). This improvement is largely\ncontributed from the space amplification within the LSM.\nExperiment 2: Read-Write Ratio. To comprehensively\nassess the impact of our design, we extend Experiment 1 to\nevaluate performance across different read/write ratios. We\nexamine four scenarios of different read:write ratios: read-\nheavy (7:3), read-write-balanced (5:5), write-heavy (3:7), and\nwrite-only workloads. The experiment involves loading key-\nvalue pairs followed by three operational phases with the\nspecified ratios, focusing on updates to existing keys. We\nanalyze the last phase to minimize GC process impact and\nexclude read-only workloads as they don’t trigger GC, which,\nin such cases, cannot show the key design of LearnedKV .\nFigure 9 demonstrates that LearnedKV can consistently im-\nproves operation throughput across all scenarios, with gainsLearnedKV RocksDB\nKey-Ptr Space\nLSM 4.2MB 242MB\nkey_array 77MB -\nmodel 96KB -\nTotal 81.3MB 242MB\nKey-Value Space\nvlog 9.6GB 9.6GB\nTable 1: Comparison of storage sizes for LearnedKV and\nRocksDB.\n7:3 5:5 3:7 Write-only\nRead:Write Ratio01020304050607080Throughput (KOPS) 17.925.8 24.932.377.4\n48.8\n26.4\n12.719.8 19.932.1\n17.915.7\n12.7LearnedKV Write\nLearnedKV Read\nRocksDB Write\nRocksDB Read\nFigure 9: Performance of LearnedKV vs. RocksDB for\nDifferent Read: Write Ratios\nranging up to 1.41x in write throughput and 4.32x in read\nthroughput. This stability in read and write performance en-\nhancement across various workloads underscores the robust-\nness of LearnedKV’s design in handling diverse operational\npatterns.\nExperiment 3: SOSD dataset. We also evaluate our de-\nsign on real-world complex datasets collected by SOSD [27],\nwhich is designed for Learned Index. We used four of their\ndatasets: Amazon book sale popularity (books) [1], unsam-\npled Facebook user IDs (fb) [40], uniformly sampled Open-\nStreetMap locations (osm) [38], and Wikipedia article edit\ntimestamps (wiki) [2]. As shown in Table 2, LearnedKV\ndemonstrates consistent performance advantages across all\ndatasets. For write operations, LearnedKV achieves improve-\nments ranging from 1.18x to 1.25x over RocksDB+. The\nread performance improvements are even more substantial,\nwith LearnedKV outperforming RocksDB+ by 2.69x to 3.21x.\nMost notably, on the Facebook user ID dataset, LearnedKV\nachieves a 3.21x read throughput improvement while main-\ntaining 1.24x better write performance, demonstrating the\nsystem’s ability to handle real-world data distributions effec-\ntively.\nExperiment 4: Workload Distribution. Table 3 demon-\nstrates LearnedKV’s consistent superiority over RocksDB+\nacross various workload distributions. For write operations,\nLearnedKV shows improvements of 1.04x to 1.30x. Read\n9\n\nDataset Write (KOPS) Read (KOPS)\nSize LearnedKV RocksDB+ LearnedKV RocksDB+\nbooks 24.31 (1.18x) 20.53 50.81 (2.72x) 18.65\nfb 25.34 (1.24x) 20.36 54.38 (3.21x) 16.92\nosm 24.88 (1.20x) 20.70 52.52 (3.17x) 16.55\nwiki 43.83 (1.25x) 35.18 54.29 (2.69x) 20.21\nTable 2: Performance comparison of LearnedKV and\nRocksDB+ under different key space distributions. KOPS:\nThousand Operations Per Second\nperformance advantages are even more significant, especially\nin skewed distributions. Under Zipfian distributions (param-\neters 0.99 and 0.9), LearnedKV achieves impressive 3.12x\nand 2.95x read throughput improvements respectively. Even\nwith uniform distribution, LearnedKV maintains a 2.51x read\nperformance advantage. These results highlight LearnedKV’s\nrobust performance across diverse workload patterns, particu-\nlarly benefiting read-heavy workloads in skewed distributions\ncommon in real-world scenarios.\nDataset Write (KOPS) Read (KOPS)\nSize LearnedKV RocksDB+ LearnedKV RocksDB+\nZipfian(0.99) 25.85 (1.30x) 19.82 48.79 (3.12x) 15.65\nZipfian(0.9) 40.29 (1.04x) 38.58 50.79 (2.95x) 17.24\nUniorm 35.36 (1.08x) 32.80 11.14 (2.51x) 4.43\nTable 3: Performance comparison of LearnedKV and\nRocksDB+ under different workload distributions. KOPS:\nThousand Operations Per Second\nExperiment 5: Dataset size. Table 4 shows LearnedKV’s\nperformance across dataset sizes from 500K to 10M key-value\npairs. LearnedKV consistently outperforms RocksDB+, with\nwrite improvements of 1.08x to 1.35x. Read performance\nadvantages grow significantly with dataset size, from 1.27x\nat 500K to 3.12x at 10M entries. Even with 10M entries,\nLearnedKV maintains substantial advantages in both writes\n(25.85 KOPS, 1.30x improvement) and reads (48.79 KOPS,\n3.12x improvement), demonstrating effective scalability par-\nticularly for read operations.\nDataset Write (KOPS) Read (KOPS)\nSize LearnedKV RocksDB+ LearnedKV RocksDB+\n500K 54.96 (1.08x) 50.56 97.09 (1.27x) 76.48\n1M 38.00 (1.14x) 33.40 85.54 (1.90x) 44.94\n3M 31.74 (1.35x) 23.57 62.38 (2.95x) 21.15\n6M 26.92 (1.24x) 21.69 51.61 (2.90x) 17.80\n10M 25.85 (1.30x) 19.82 48.79 (3.12x) 15.65\n30M 22.24 (1.12x) 19.77 23.01 (1.87x) 12.33\nTable 4: Performance comparison of LearnedKV and\nRocksDB+ with different dataset sizes. KOPS: Thousand\nOperations Per Second\nExperiment 6: Range Scan. We also compare our range\nscan performance with the state-of-the-art RocksDB. Similar\nto previous experiments, we first load 1,000,000 key-valuepairs into the KV store and perform 1,000 scan operations\nover the database. For the range scan, we did not pre-set the\nnumber of key-value pairs to be involved, as these numbers\nare impractical to pre-determine in real-world scenarios. In-\nstead, we configured a ScanRange , and all KV stores were\nused to read all the key-value pairs within this range. To en-\nsure a fair comparison, we modified the string keys used in\nRocksDB so that it could maintain the same order as integers.\nThis ensured that the keys involved in each competitor were\nidentical. Based on the experiment logs, each scan request\nin this experiment reads about 500KB of key-value pairs on\naverage.\nAs shown in figure 10, our setup consists of three parts,\nusing the same loading phase but with different workloads.\n\"Scan without update\" workloads consist solely of scan oper-\nations. In LearnedKV , after the GC process at the end of the\nloading phase, all valid key-value pairs are re-grouped and\nmigrated to the Learned Index. Because there are no insert\nor update requests afterward, the LSM will not absorb any\nnew keys, making the Learned Index the only active compo-\nnent in this tiered storage. Thus, this comparison effectively\nmeasures the performance difference between the Learned\nIndex and RocksDB. In this scenario, LearnedKV achieves\nup to 2.02x greater performance in range scans compared to\nRocksDB. However, such scenarios are rare. Therefore, we\nalso conducted experiments on \"Scan with Update\" work-\nloads. We included two sets of such workloads: \"Set 1\" con-\ntains 0.5M updates before the scan requests, while \"Set 2\"\nincludes 1M update requests before the scan. These work-\nloads make the LSM absorb some keys after the GC process,\nrequiring LearnedKV to scan both the LSM and the Learned\nIndex to obtain correct results. As expected, there is a 21%\nperformance drop compared to the pure scan workload, but it\nstill outperforms the state-of-the-art by 1.76x.\nScan Without Update Scan With Update Set 1 Scan With Update Set 20.00.20.40.60.81.01.2Scan Throughput (KOPS)1.23\n0.971.04\n0.610.58 0.59LearnedKV\nRocksDB\nFigure 10: Performance Comparison of Range Scan\nExperiment 7: Other KV Stores. We evaluated\nLearnedKV against several KV stores optimized for on-\nstorage performance: Bourbon [16], HashKV [33], disk-\nresident B+-Tree [5], and hybrid learned indexes [51]. Since\nmost of the works are adapted from LevelDB, for a fair com-\nparison, we also implemented a LevelDB-based version of\n10\n\nLearnedKV and adapted B+-Tree and hybrid indexes with key-\nvalue separation and garbage collection mechanisms similar\nto LearnedKV . The B+-Tree was configured with leaf nodes\nand two levels of inner nodes on storage to match memory con-\nsumption. For hybrid indexes, we used \"Hybrid_PGM_Disk\"\nand \"Hybrid_Leco_Disk\" that are proposed as \"all-in-one\"\nhybrid index from [51]. We also fine-tuned all the KV-stores\nsuch that all memory consumption will be similar.\nA:write-heavy B:read-heavy C:read-only\nYCSB Workload020406080100120140Throughput (KOPS)93140\n8683\n74\n64\n5975\n65\n35\n17\n835\n17\n7111917LearnedKV\nHashKV\nBourbonHybrid_PGM_Disk\nHybrid_Leco_Disk\nB+-Tree\nFigure 11: KV Store Throughput Comparison for YCSB\nWorkloads. Three YCSB workloads: A (write-heavy, w:50%,\nr:50%), B (read-heavy, w:5%, r:95%), and C (read-only).\nFigure 11 compares throughput across these KV stores un-\nder YCSB workloads A (write-heavy), B (read-heavy), and\nC (read-only). In this experiment, we use the benchmark\nwith 1M entries because when the data size is larger, some\nexisting solutions will become extremely slow (> 2 hours\nfor each datapoint). LearnedKV consistently outperforms\nall systems across workload types. In the write-heavy work-\nload, LearnedKV surpasses HashKV by 12% and Bourbon by\n58%. This performance advantage becomes more pronounced\nin read-heavy workloads, with LearnedKV outperforming\nHashKV by 89% and Bourbon by 87%. For the read-only\nscenario, LearnedKV maintains a 34% and 32% lead over\nHashKV and Bourbon, respectively. To be noticed that, one\nof the important reason of the extreme well performance of\nLearnedKV on read-heavy workloads, based on our analysis,\nis that the Learned Index help LSM reduce its size so that\nmany of the read requests can be efficiently processed by both\nLSM and Learned Index.\nLearnedKV’s advantages are even more pronounced\nagainst on-storage learned indexes and B+-Trees. In write-\nheavy workloads, it outperforms them by 2.66x. This perfor-\nmance gap widens dramatically in read-intensive workloads,\nwith LearnedKV achieving up to 8.24x higher throughput in\nread-heavy scenarios.\nWe also evaluated LearnedKV against state-of-the-art on-\nstorage learned indexes from [51] under direct I/O, using\nRocksDB as our base LSM implementation. (Direct I/O is not\nsupported in LevelDB and its variants mentioned above.) As\nshown in Table 5, LearnedKV can still significantly outper-\nform both Hybrid_PGM_Disk and Hybrid_Leco_Disk acrossYCSB Throughput (KOPS)\nworkload LearnedKV Hybrid_PGM_Disk Hybrid_Leco_Disk\nA 48.52 34.62 35.37\nB 90.78 17.16 16.96\nC 60.80 7.66 7.43\nTable 5: Performance comparison of LearnedKV and on\nstorage Learned Index with direct I/O KOPS: Thousand\nOperations Per Second\nall tested YCSB workloads. Most notably, for read-heavy\nworkload B, LearnedKV achieves 5.3x higher throughput,\nwhile for read-only workload C, the performance gap widens\nto 7.9x.\n5.2 Drill-down Analysis\nTo fully investigate what is happening inside the LearnedKV ,\nwe further developed some drill-down analysis on its internal\nbehavior.\nExperiment 8: Latency & Time breakdown. Table 6 and\nFigure 12 provide a detailed time breakdown of LearnedKV\nread operations on Phase 1. Each operation consists of three\ncomponents: RocksDB querying, Learned Index querying,\nand KV pair loading from the vlog.\nAmong approximately 5M read operations, 12.6% are pro-\ncessed by the Learned Index with an average time of 25.94\nµs, either through LSM read failures or by skipping LSM en-\ntirely. Despite using an in-memory Bloom Filter, over 95%\nof read requests still access RocksDB, a result of the skewed\nworkload favoring frequently updated keys.\nLearnedKV handles read queries in two scenarios: (1)\n\"LearnedKV_key_in_R\", where LSM successfully locates\nand returns the key, and (2) \"LearnedKV_key_in_LI\", where\neither LSM fails to find the key or the Bloom Filter indi-\ncates the key’s absence in LSM, triggering a Learned Index\nquery. Figure 12 shows that Learned Index queries achieve\n67% lower latency compared to the baseline, despite re-\nquiring both indexes, due to efficient LSM failure detection\nand fast Learned Index reads. Even for LSM-resident keys,\nLearnedKV delivers 23% lower latency, benefiting from its\nreduced LSM size compared to RocksDB+.\nOperation Number\nWrite 5,001,245\nRead 4,998,755\nRead Operation Breakdown\nRead through LI 538,347\nRead through RD 4,763,353\nLoad from vlog 4,998,755\nTable 6: Summary of operations and their counts. LI:\nLearned Index; RD: RocksDB\n11\n\nLearnedKV_key_in_LI LearnedKV_key_in_R RocksDB+01020304050607080Time (microseconds)25.9460.9078.63 query_R\nquery_LI\nload_KVFigure 12: Time breakdown of read operations over\nLearnedKV and RocksDB+\nExperiment 9: Over-provisioning ratio. Table 7 com-\npares LearnedKV and RocksDB+ performance across vari-\nous over-provisioning ratios. LearnedKV consistently out-\nperforms RocksDB+ in both write (1.14x to 1.29x im-\nprovement) and read operations (1.39x to 1.65x improve-\nment). Write performance for both systems improves with\nhigher over-provisioning, with LearnedKV peaking at 59.57\nKOPS at 50% over-provisioning. Read performance in\nLearnedKV shows more variation (69.77 to 76.34 KOPS)\ncompared to RocksDB+’s stability. These results demonstrate\nLearnedKV’s superior performance, particularly in read oper-\nations, while highlighting the trade-offs in over-provisioning\nratio selection, as it differentially affects read and write per-\nformance.\nOver- Write (KOPS) Read (KOPS)\nprov. (%) LearnedKV RocksDB+ LearnedKV RocksDB+\n10 11.99 (1.15x) 10.41 37.23 (3.01x) 12,37\n20 26.84 (1.42x) 18.94 46.36 (3.06x) 15.16\n30 24.71 (1.29x) 19.19 49.48 (3.11x) 15.91\n40 36.60 (1.22x) 29.88 46.95 (2.95x) 15.90\n50 36.35 (1.30x) 27.86 40.18 (2.58x) 15.59\nTable 7: Performance comparison of LearnedKV and\nRocksDB+ with different over-provisioning ratios KOPS:\nThousand Operations Per Second\nExperiment 10: In-memory Bloom Filter. Figure 13\nshows the performance comparison related to the in-memory\nBloom Filter. After applying the Bloom Filter, write through-\nput decreases by 5.65% due to maintenance overhead for new\nkey insertions. The read throughput improves by 9.60%, pro-\nviding a modest performance gain. This limited improvement\nis not surprising since, according to our previous experiments\n6, the Bloom Filter only affects about 0.235M among 5M\noperations by directing them directly to the Learned Index.\nHowever, this presents an interesting trade-off: a small write\nperformance penalty for potentially significant read gains in\nread-heavy workloads, making the Bloom Filter a valuableoptimization option depending on workload characteristics.\nRead Throughput Write Throughput01020304050Throughput (KOPS)9.60%\n-5.65%With Bloom Filter\nWithout Bloom Filter\nFigure 13: Comparison between throughputs with and\nwithout Bloom Filter\n5.3 Performance in Memory\nTo evaluate the in-memory performance characteristics of\nboth systems, we conducted experiments with datasets that\nfit entirely in DRAM. Figure 14 illustrates the through-\nput comparison between LearnedKV and RocksDB across\nvarious read:write ratios. LearnedKV consistently outper-\nforms RocksDB by up to 1.26x. This demonstrates that\nLearnedKV’s architectural benefits extend beyond storage-\nbound scenarios to in-memory operations, where the tiered\nindex design effectively reduces computational overhead and\nimproves cache efficiency.\n7:3 5:5 3:7 Write-only\nRead:Write Ratio020406080100120Throughput (KOPS)33.249.0 47.260.8130.3\n117.8\n105.9\n26.439.7 40.651.7115.8\n107.1\n93.7LearnedKV Write\nLearnedKV Read\nRocksDB Write\nRocksDB Read\nFigure 14: Performance of LearnedKV vs. RocksDB for\nDifferent Read-Write Ratios in Memory\n5.4 Performance on HDD\nTo evaluate LearnedKV’s adaptability across diverse storage\nenvironments, we extended our experiments to HDD-based\nsystems using a 12-core Intel Xeon E5-2620 v3 2.40GHz CPU\nwith a 931GB Seagate Constellation ES.3 HDD, and we used\n1M-key datasets due to the slow performance of HDD. Figure\n15 compares LearnedKV’s performance against RocksDB\n12\n\nfor various read-write ratios on this HDD setup. LearnedKV\ndemonstrates more pronounced advantages on HDDs com-\npared to SSDs, consistently outperforming RocksDB across\nall scenarios with throughput improvements of 1.22x to 1.43x.\nNotably, LearnedKV shows significant gains in write-heavy\nand write-only workloads, which are typical bottlenecks for\nHDD-based systems. The substantial write throughput im-\nprovement (up to 1.43x) highlights LearnedKV’s effective-\nness in mitigating mechanical storage limitations. These re-\nsults underscore LearnedKV’s versatility and its capacity to\noptimize performance across both SSD and HDD infrastruc-\ntures, effectively addressing the challenges posed by mechan-\nical storage systems.\n7:3 5:5 3:7 Write-only\nRead:Write Ratio0246810121416Throughput (KOPS)6.4 6.3 6.66.916.4\n15.5\n14.2\n5.0\n4.45.45.012.8\n11.9\n10.7LearnedKV Write\nLearnedKV Read\nRocksDB Write\nRocksDB Read\nFigure 15: Performance of LearnedKV vs. RocksDB for\nDifferent Read-Write Ratios on HDD\n6 Future Work & Conclusion\nThe choice of the Learned Index model presents an important\npart of future research. While our implementation uses piece-\nwise linear representation for its simplicity and efficiency,\nalternative learned index structures could potentially offer\ndifferent performance trade-offs for on-storage indexing. This\nrepresents a rich area for future exploration.\nIn this paper, we introduced LearnedKV , a novel approach\nto key-value store design through its tiered architecture that\nfully decouples the LSM tree from the Learned Index. By hav-\ning the LSM tree handle write operations while the Learned\nIndex accelerates reads, the system achieves superior perfor-\nmance compared to traditional approaches that treat learned\nindexes as auxiliary components. Our non-blocking conver-\nsion mechanism efficiently migrates data during garbage col-\nlection, enabling smooth operation without sacrificing perfor-\nmance. This design significantly reduces LSM size, which\nleads to improvements in both read and write operations.\nExtensive experimental evaluation demonstrates that\nLearnedKV consistently outperforms state-of-the-art LSM-\nbased solutions across diverse workloads and environments,\nachieving up to 4.32x faster reads and 1.43x faster writes.\nThe system maintains these performance advantages acrossdifferent data distributions, workload patterns, and storage\nmedia, confirming the robustness and broad applicability of\nour approach.\nReferences\n[1]Amazon sales rank data for print and kindle\nbooks. https://www.kaggle.com/ucffool/\namazon-sales-rank-data-for-print-and-kindle-books .\n[2] Wikimedia downloads. http://dumps.wikimedia.org.\n[3]Leveldb: a fast key-value storage library written at google that provides\nan ordered mapping from string keys to string values, 2023. https:\n//github.com/google/leveldb .\n[4]Rocksdb: A persistent key-value store for flash and ram storage, 2023.\nhttps://rocksdb.org/ .\n[5] Stx b+, 2023. https://panthema.net/2007/stx-btree/ .\n[6]Yahoo! cloud serving benchmark, 2023. https://github.com/\nbrianfrankcooper/YCSB .\n[7]Amazon dynamodb, 2024. https://aws.amazon.com/dynamodb// .\n[8] Apache hbase, 2024. https://hbase.apache.org/ .\n[9]ABU-LIBDEH , H., A LTINBUKEN , D., B EUTEL , A., C HI, E. H.,\nDOSHI , L., K RASKA , T., L I, X., L Y, A., AND OLSTON , C. Learned\nindexes for a google-scale disk-based database. ArXiv abs/2012.12501\n(2020).\n[10] AHN, J.-S., S EO, C., M AYURAM , R., Y ASEEN , R., K IM, J.-S., AND\nMAENG , S. R. Forestdb: A fast key-value storage system for variable-\nlength string keys. IEEE Transactions on Computers 65 (2016), 902–\n915.\n[11] ATIKOGLU , B., X U, Y., F RACHTENBERG , E., J IANG , S., AND\nPALECZNY , M. Workload analysis of a large-scale key-value store. In\nMeasurement and Modeling of Computer Systems (2012).\n[12] CAO, Z., D ONG, H., W EI, Y., L IU, S., AND DU, D. H.-C. Is-hbase:\nAn in-storage computing optimized hbase with i/o offloading and self-\nadaptive caching in compute-storage disaggregated infrastructure. ACM\nTransactions on Storage (TOS) 18 (2022), 1 – 42.\n[13] CAO, Z., D ONG, S., V EMURI , S., AND DU, D. H.-C. Characterizing,\nmodeling, and benchmarking rocksdb key-value workloads at facebook.\nInUSENIX Conference on File and Storage Technologies (2020).\n[14] CHEN, H., R UAN, C., L I, C., M A, X., AND XU, Y.Spandb: A fast,\ncost-effective lsm-tree based kv store on hybrid storage. In USENIX\nConference on File and Storage Technologies (2021).\n[15] COOPER , B. F., S ILBERSTEIN , A., T AM, E., R AMAKRISHNAN , R.,\nAND SEARS , R. Benchmarking cloud serving systems with ycsb. In\nACM Symposium on Cloud Computing (2010).\n[16] DAI, Y., X U, Y., G ANESAN , A., A LAGAPPAN , R., K ROTH , B.,\nARPACI -DUSSEAU , A. C., AND ARPACI -DUSSEAU , R. H. From\nwisckey to bourbon: A learned index for log-structured merge trees. In\nUSENIX Symposium on Operating Systems Design and Implementation\n(2020).\n[17] DAVIS , D. Ai unleashes the power of unstruc-\ntured data. https://www.cio.com/article/220347/\nai-unleashes-the-power-of-unstructured-data.html .\n[18] DEBNATH , B. K., S ENGUPTA , S., AND LI, J.Flashstore: High through-\nput persistent key-value store. Proc. VLDB Endow. 3 (2010), 1414–\n1425.\n[19] DECANDIA , G., H ASTORUN , D., J AMPANI , M., K AKULAPATI , G.,\nLAKSHMAN , A., P ILCHIN , A., S IVASUBRAMANIAN , S., V OSSHALL ,\nP.,AND VOGELS , W. Dynamo: amazon’s highly available key-value\nstore. In Symposium on Operating Systems Principles (2007).\n13\n\n[20] DING, J., M INHAS , U. F., Z HANG , H., L I, Y., W ANG , C., C HAN -\nDRAMOULI , B., G EHRKE , J., K OSSMANN , D., AND LOMET , D. B.\nAlex: An updatable adaptive learned index. Proceedings of the 2020\nACM SIGMOD International Conference on Management of Data\n(2019).\n[21] ELDAKIKY , H., AND DU, D. H.-C. Transkv: A networking support\nfor transaction processing in distributed key-value stores. 2021 IEEE\nSeventh International Conference on Big Data Computing Service and\nApplications (BigDataService) (2021), 41–49.\n[22] FERRAGINA , P., AND VINCIGUERRA , G.The pgm-index. Proceedings\nof the VLDB Endowment 13 (2019), 1162 – 1175.\n[23] GALAKATOS , A., M ARKOVITCH , M., B INNIG , C., F ONSECA , R.,\nAND KRASKA , T.Fiting-tree: A data-aware index structure. Proceed-\nings of the 2019 International Conference on Management of Data\n(2018).\n[24] KAIYRAKHMET , O., L EE, S. Y., N AM, B., N OH, S. H., AND RI CHOI,\nY.Slm-db: Single-level key-value store with persistent memory. In\nUSENIX Conference on File and Storage Technologies (2019).\n[25] KANG , Y., P ITCHUMANI , R., M ISHRA , P., K EE, Y.-S., L ONDONO ,\nF., O H, S., L EE, J., AND LEE, D. D. G. Towards building a high-\nperformance, scale-in key-value storage system. Proceedings of the\n12th ACM International Conference on Systems and Storage (2019).\n[26] KEAHEY , K., A NDERSON , J., Z HEN, Z., R ITEAU , P., R UTH, P.,\nSTANZIONE , D., C EVIK , M., C OLLERAN , J., G UNAWI , H. S., H AM-\nMOCK , C., M AMBRETTI , J., B ARNES , A., H ALBACH , F., R OCHA ,\nA., AND STUBBS , J. Lessons learned from the chameleon testbed.\nInProceedings of the 2020 USENIX Annual Technical Conference\n(USENIX ATC ’20) . USENIX Association, July 2020.\n[27] KIPF, A., M ARCUS , R., VAN RENEN , A., S TOIAN , M., K EMPER , A.,\nKRASKA , T., AND NEUMANN , T. Sosd: A benchmark for learned\nindexes. ArXiv abs/1911.13014 (2019).\n[28] KIPF, A., M ARCUS , R., VAN RENEN , A., S TOIAN , M., K EMPER , A.,\nKRASKA , T., AND NEUMANN , T.Radixspline: a single-pass learned\nindex. Proceedings of the Third International Workshop on Exploiting\nArtificial Intelligence Techniques for Data Management (2020).\n[29] KRASKA , T., B EUTEL , A., C HI, E. H., D EAN, J., AND POLYZOTIS ,\nN.The case for learned index structures. Proceedings of the 2018\nInternational Conference on Management of Data (2017).\n[30] LAN, H., B AO, Z., C ULPEPPER , J. S., AND BOROVICA -GAJIC , R.\nUpdatable learned indexes meet disk-resident dbms - from evaluations\nto design choices. Proceedings of the ACM on Management of Data 1\n(2023), 1 – 22.\n[31] LEPERS , B., B ALMAU , O., G UPTA , K., AND ZWAENEPOEL , W. Kvell:\nthe design and implementation of a fast persistent key-value store. Pro-\nceedings of the 27th ACM Symposium on Operating Systems Principles\n(2019).\n[32] LI, P., H UA, Y., J IA, J., AND ZUO, P.Finedex: A fine-grained learned\nindex scheme for scalable and concurrent memory systems. Proc.\nVLDB Endow. 15 (2021), 321–334.\n[33] LI, Y., C HAN, H. H. W., L EE, P. P. C., AND XU, Y.Enabling efficient\nupdates in kv storage via hashing. ACM Transactions on Storage (TOS)\n15(2018), 1 – 29.\n[34] LIU, M., G U, J., AND ZHAO, T.Lckv: Learner-cleaner optimized adap-\ntive key-value separated lsm-tree store. 2024 IEEE 42nd International\nConference on Computer Design (ICCD) (2024), 479–482.\n[35] LU, K., Z HAO, N., GUANG WAN, J., F EI, C., Z HAO, W., AND DENG,\nT.Tridentkv: A read-optimized lsm-tree based kv store via adaptive in-\ndexing and space-efficient partitioning. IEEE Transactions on Parallel\nand Distributed Systems PP (2021), 1–1.\n[36] LU, L., P ILLAI , T. S., A RPACI -DUSSEAU , A. C., AND ARPACI -\nDUSSEAU , R. H. Wisckey: Separating keys from values in ssd-\nconscious storage. In USENIX Conference on File and Storage Tech-\nnologies (2016).[37] O’N EIL, P. E., C HENG , E. Y. C., G AWLICK , D., AND O’N EIL, E. J.\nThe log-structured merge-tree (lsm-tree). Acta Informatica 33 (1996),\n351–385.\n[38] PANDEY , V., K IPF, A., N EUMANN , T., AND KEMPER , A. How good\nare modern spatial analytics systems? Proc. VLDB Endow. 11 (2018),\n1661–1673.\n[39] RUMBAUGH , D. B., X IE, D., AND ZHAO, Z. Towards systematic\nindex dynamization. Proc. VLDB Endow. 17 (2024), 2867–2879.\n[40] SANDT , P. V., C HRONIS , Y., AND PATEL , J. M. Efficiently searching\nin-memory sorted arrays: Revenge of the interpolation search? Pro-\nceedings of the 2019 International Conference on Management of Data\n(2019).\n[41] SARKAR , S., P APON , T. I., S TARATZIS , D., AND ATHANASSOULIS ,\nM.Lethe: A tunable delete-aware lsm engine (updated version). ArXiv\nabs/2006.04777 (2020).\n[42] TANG, C., GUANG WAN, J., AND XIE, C.Fencekv: Enabling efficient\nrange query for key-value separation. IEEE Transactions on Parallel\nand Distributed Systems 33 (2022), 3375–3386.\n[43] TANG , C., W ANG , Y., D ONG , Z., H U, G., W ANG , Z., W ANG , M.,\nAND CHEN, H.Xindex: a scalable learned index for multicore data stor-\nage. Proceedings of the 25th ACM SIGPLAN Symposium on Principles\nand Practice of Parallel Programming (2020).\n[44] WANG, Y., Y UAN, J., W U, S., L IU, H., C HEN, J., M A, C., AND QIN,\nJ.Leaderkv: Improving read performance of kv stores via learned index\nand decoupled kv table. 2024 IEEE 40th International Conference on\nData Engineering (ICDE) (2024), 29–41.\n[45] WU, F., Y ANG , M.-H., Z HANG , B., AND DU, D. H.-C. Ac-key:\nAdaptive caching for lsm-based key-value stores. In USENIX Annual\nTechnical Conference (2020).\n[46] WU, J., Z HANG , Y., C HEN, S., W ANG, J., C HEN, Y., AND XING, C.\nUpdatable learned index with precise positions. ArXiv abs/2104.05520\n(2021).\n[47] WU, X., X U, Y., S HAO, Z., AND JIANG , S.Lsm-trie: An lsm-tree-\nbased ultra-large key-value store for small data items. In USENIX\nAnnual Technical Conference (2015).\n[48] XIE, Q., P ANG , C., Z HOU , X., Z HANG , X., AND DENG, K. Maxi-\nmum error-bounded piecewise linear representation for online stream\napproximation. The VLDB Journal 23 (2014), 915–937.\n[49] ZHANG , B., AND DU, D. H.-C. Nvlsm: A persistent memory key-\nvalue store using log-structured merge tree with accumulative com-\npaction. ACM Trans. Storage 17 (2021), 23:1–23:26.\n[50] ZHANG , B., G ONG , H., AND DU, D. H.-C. Pmdb: A range-based\nkey-value store on hybrid nvm-storage systems. IEEE Transactions on\nComputers 72 (2023), 1274–1285.\n[51] ZHANG , J., S U, K., AND ZHANG , H. Making in-memory learned\nindexes efficient on disk. Proceedings of the ACM on Management of\nData (2024).\n14",
  "textLength": 67619
}