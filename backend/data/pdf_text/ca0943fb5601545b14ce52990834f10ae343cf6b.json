{
  "paperId": "ca0943fb5601545b14ce52990834f10ae343cf6b",
  "title": "Fast Construction of Partitioned Learned Bloom Filter with Theoretical Guarantees",
  "pdfPath": "ca0943fb5601545b14ce52990834f10ae343cf6b.pdf",
  "text": "Fast Construction of Partitioned Learned Bloom Filter\nwith Theoretical Guarantees\nAtsuki Sato asato@hal.t.u-tokyo.ac.jp\nYusuke Matsui matsui@hal.t.u-tokyo.ac.jp\nDepartment of Information and Communication Engineering\nGraduate School of Information Science and Technology\nThe University of Tokyo, Japan\nAbstract\nBloom filter is a widely used classic data structure for approximate membership queries.\nLearned Bloom filters improve memory efficiency by leveraging machine learning, with\nthe partitioned learned Bloom filter (PLBF) being among the most memory-efficient vari-\nants. However, PLBF suffers from high computational complexity during construction,\nspecifically O(N3k), where Nandkare hyperparameters. In this paper, we propose three\nmethods‚Äîfast PLBF, fast PLBF++, and fast PLBF#‚Äîthat reduce the construction com-\nplexity to O(N2k),O(NklogN), and O(Nklogk), respectively. Fast PLBF preserves the\noriginal PLBF structure and memory efficiency. Although fast PLBF++ and fast PLBF#\nmay have different structures, we theoretically prove they are equivalent to PLBF under\nideal data distribution. Furthermore, we theoretically bound the difference in memory\nefficiency between PLBF and fast PLBF++ for non-ideal scenarios. Experiments on real-\nworld datasets demonstrate that fast PLBF, fast PLBF++, and fast PLBF# are up to 233,\n761, and 778 times faster to construct than original PLBF, respectively. Additionally, fast\nPLBF maintains the same data structure as PLBF, and fast PLBF++ and fast PLBF#\nachieve nearly identical memory efficiency.\nKeywords: learned Bloom filter, learned index, membership query, optimization, dy-\nnamic programming\n1 Introduction\nBloom filter is a memory-efficient probabilistic data structure for approximate membership\nqueries. While Bloom filters produce false positives due to hash collisions, they do not\nproduce false negatives. Because of this favorable property, they are widely used in various\nmemory-constrained applications such as networks, databases, etc. There is a trade-off\nbetween the size of the Bloom filter and the false positive rate (FPR); the lower the FPR,\nthe larger the filter needed. According to Pagh et al. (2005), a data structure that stores a\nset of nelements with an FPR of Fneeds at least nlog2(1/F) bits of memory. However, in\npractical applications, the data set and/or query has a certain structure, and thus we may be\nable to achieve better efficiency than this theoretical limit. Innovatively, Kraska et al. (2018)\nintegrate a machine learning model with a Bloom filter and use this model as a prefilter,\nfurther reducing the space requirement. Such a Bloom filter is called a learned Bloom\nfilter (LBF) and has been actively studied. In particular, partitioned learned Bloom filter\n(PLBF) (Vaidya et al., 2021) solves an optimization problem using dynamic programming\n1arXiv:2410.13278v1  [cs.DS]  17 Oct 2024\n\nSato and Matsui\nMethod Construction Time Memory-Efficiency Comparison with PLBF\nPLBF O(N3k) -\nFast PLBF O(N2k) Always identical\nFast PLBF++ O(NklogN+Nk2) Identical under ideal conditions\n‚Üí O(NklogN) + Theoretical bound under non-ideal conditions\nFast PLBF# O(Nklogk) Identical under ideal conditions\nTable 1: Comparison of PLBF (Vaidya et al., 2021) and our proposed methods. The second\nand third rows show the results of our previously published conference paper (Sato\nand Matsui, 2023). The fourth and fifth rows show the new contributions made\nin this paper.\nto find the optimal configuration of the prefilter and the backup filters, which allows almost\nfull utilization of the machine learning performance.\nHowever, PLBF requires a lot of time for its construction. This is because O(N3k) of\ncomputation is required to solve the optimization problem, where Nandkare hyperpa-\nrameters of PLBF. As Nandkincrease, the PLBF becomes more memory-efficient but the\nconstruction time becomes unrealistically long.\nWe propose three methods to speed up the construction of PLBF while maintaining\ntheir excellent memory efficiency: fast PLBF, fast PLBF++, and fast PLBF#. The com-\nputational complexity for solving the optimization problem of fast PLBF, fast PLBF++,\nand fast PLBF# is O(N2k),O(NklogN), and O(Nklogk), respectively. Fast PLBF has\nthe same data structure as PLBF, that is, it achieves the same memory efficiency as PLBF.\nAlthough fast PLBF++ and fast PLBF# do not necessarily have the same data structure\nas PLBF, we theoretically guarantee that they have the same data structures as PLBF\nunder an intuitive and ideal condition on the distribution. Furthermore, we theoretically\nbound the difference of memory efficiency between (fast) PLBF and fast PLBF++/#.\nWe evaluate our methods using real-world data sets and artificial data sets. The results\nshow that (i) fast PLBF, fast PLBF++, and fast PLBF# can be constructed at most 233,\n761, and 778 times faster than PLBF, (ii) fast PLBF achieves exactly the same memory\nefficiency as PLBF, and (iii) fast PLBF++ and fast PLBF# achieve almost the same\nmemory efficiency as PLBF, and the difference is within limits guaranteed by our theorem.\nThis study builds on a previous shorter conference paper (Sato and Matsui, 2023). We\nsignificantly extend this work and make several further contributions, as summarized in Ta-\nble 1. First, we have improved the worst-case computational complexity of fast PLBF++\nfromO(NklogN+Nk2) toO(NklogN) by carefully observing the optimization problem\nand modifying parts of the algorithm. Second, we have extended the theoretical guarantee\nof the memory efficiency of fast PLBF++. Previously, memory efficiency was only guar-\nanteed under ideal circumstances. However, it is now guaranteed even under non-ideal\nconditions. Third, we have proposed and given a theoretical guarantee for fast PLBF#,\nwhich has an even smaller computational complexity than fast PLBF++. Furthermore, we\nhave confirmed that the theoretical guarantee fits well with the experimental results.\n2\n\nFast Construction of PLBF with Guarantees\nThe remainder of this paper is organized as follows. In Section 2, we present the related\nwork. In Section 3, we give preliminaries consisting of two parts. First, we describe PLBF,\nincluding its architecture, the optimization problem it addresses, and its solution. Next, we\nintroduce the matrix problem and the related algorithms used in the core of our acceleration.\nIn Section 4, we propose our three methods for fast PLBF construction. In Section 4.1, we\npropose fast PLBF and give a proof that it has exactly the same memory efficiency as PLBF.\nIn Section 4.2 and Section 4.3, we propose fast PLBF++ and fast PLBF#, respectively,\nand give theoretical guarantees for the difference in memory efficiency between them and\nPLBF. In Section 5, we present extensive experiments that demonstrate these methods‚Äô\neffectiveness and our theorems‚Äô validity.\n2 Related Work\nOur work is in the context of LBF, which uses machine learning to improve the performance\nof the Bloom filter. We first introduce the Bloom filter and its variants in Section 2.1, and\nthen present related work on LBF in Section 2.2.\n2.1 Bloom Filter and Its Variants\nThe Bloom filter (Bloom, 1970) is a traditional data structure that answers approximate\nmembership queries. An approximate membership query is a query that determines whether\na setScontains an element q, while allowing false positives with some probability. Bloom\nfilters are often used in contexts such as networks (Broder and Mitzenmacher, 2004; Tarkoma\net al., 2011; Geravand and Ahmadi, 2013) and databases (Chang et al., 2008; Goodrich and\nMitzenmacher, 2011; Lu et al., 2012), where memory constraints are tight and a small rate\nof false positives is tolerable.\nThe Bloom filter is a data structure consisting of an m-bit bit array, b, and kindependent\nhash functions, h1, h2, . . . , h k, each of which maps an input to an integer value between 1\nandm. To set up a Bloom filter, all bits of bare initially set to 0, and then for each element\nx(‚àà S) and for each i(‚àà {1,2, . . . , k }), set the bit bhi(x)to 1. To check whether an element\nqis in the set S, the Bloom filter checks the bits at positions h1(q), h2(q), . . . , h k(q) in the\nbit array b. If any of these bits is 0, then qis definitely not in the set S(no false negatives\noccur). If all of these bits are 1, the Bloom filter indicates that qis in the set S. However,\ndue to the possibility of hash collisions, this may not be correct, leading to a false positive.\nThe number of bits the Bloom filter requires to achieve an FPR of F(‚àà(0,1)) is\n(log2e)¬∑ |S|log21\nF. (1)\nIt is known that the lower bound for the number of bits required for a given FPR of Fis\n|S|log2(1/F), so this means that a Bloom filter uses log2e‚âà1.44 times the number of bits\nof the theoretical lower bound.\nSeveral variants of Bloom filters have been proposed. One of the most representative\nvariants is the Cuckoo filter (Fan et al., 2014). The Cuckoo filter is more memory-efficient\nthan the original Bloom filter and supports dynamic addition and removal of elements. In\naddition, various derivatives such as Vacuum filter (Wang and Zhou, 2019), Xor filter (Graf\nand Lemire, 2020), and Ribbon filter (Dillinger and Walzer, 2021) have been proposed with\n3\n\nSato and Matsui\nùë•\nLearned Modelùë†ùë•‚àà[0,1]ÔºàLikelihood of ùë•‚ààùëÜÔºâ\nBF!\nBF\"\nBF#\nùë†ùë•‚àà[0,ùë°!)FPR: ùëì*FPR: ùëì+FPR: ùëì,......ùë†ùë•‚àà[ùë°!,ùë°\")\nùë†ùë•‚àà[ùë°#$!,1]\nFigure 1: PLBF partitions the score space into kregions and assigns backup Bloom filters\nwith different FPRs to each region.\nthe aim of achieving memory efficiency close to the theoretical lower bound and high query\nspeeds. A variant of the Bloom filter with this lower bound has also been proposed (Pagh\net al., 2005), but it is so complex that it is difficult to implement in practice.\n2.2 Learned Bloom Filter\nBloom filter and B-tree (and their variants) are commonly employed as indexing struc-\ntures in database systems. Kraska et al. (2018) showed that combining these index data\nstructures with machine learning models has the potential to achieve memory savings, and\nthey named these new index data structures ‚Äúlearned indexes.‚Äù The main idea behind the\nlearned index is based on the following two points: (i) classical index data structures are\nessentially ‚Äúmodels‚Äù that make some kind of prediction, and (ii) machine learning models\nhave the potential to exploit the structural properties of data and queries more efficiently\nthan conventional index data structures. In fact, Kraska et al. showed that by replacing\nB-trees with machine learning models, it is possible to achieve query response speeds 1.5 to\n3 times faster than B-trees while saving nearly 100 times less memory experimentally. This\nplain learning-augmented B-tree did not support insertion or deletion, and was limited to\nthe case of an one-dimensional array. However, inspired by the research of Kraska et al.,\nvarious proposals have been made, including those that support insertion and deletion (Fer-\nragina and Vinciguerra, 2020; Ding et al., 2020; Dai et al., 2020; Zhang et al., 2022) and\nthose that extend to multi-dimensional cases (Nathan et al., 2020; Ding et al., 2020; Li\net al., 2020; Gu et al., 2023). In addition, there is a lot of research that integrates machine\nlearning with other data structures, such as count-min sketches (Hsu et al., 2019; Zhang\net al., 2020) and binary search trees (Lin et al., 2022).\nLBF is a type of learned index that extends a Bloom filter with machine learning. The\noriginal LBF proposed by Kraska et al. (2018) consists of a single machine learning model\nand a single backup Bloom filter. The machine learning model is trained to solve the\nbinary classification task of determining whether an input element is in the set Sor not.\nThis training is done using the set Sand the set Q, where Qconsists of elements that are\nnot in the set S. In many real-world applications, Qcan be obtained from past queries that\nare not in S. When this LBF responds to a query, if the machine learning model predicts\n4\n\nFast Construction of PLBF with Guarantees\nBF!\nBF\"\nBF#FPR: ùëì!FPR: ùëì\"FPR: ùëì#...\n0.0050.0020.008‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶0.10.2\n0.10.030.02‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶0.0030.001ùëî$‚Ñé$123ùëÅùëñùê∫!ùê∫\"ùê∫#ùêª!ùêª\"ùêª#ùë°!=0ùë°\"ùë°#=1ùë°$ùë°#%\"\nFigure 2: PLBF divides the score space into Nsegments and then clusters the Nsegments\nintokregions. PLBF uses dynamic programming to find the optimal way to\ncluster segments into regions.\nthat the query is contained in the set, the LBF immediately responds that the query is\ncontained in the set. Otherwise, the backup Bloom filter is consulted, and its result is used\nas the final answer. By using this hybrid approach, the LBF retains the important property\nof Bloom filters of having no false negatives, while taking advantage of the machine learning\nmodel‚Äôs ability to efficiently capture the structure of SandQ.\n3 Preliminaries\nThis chapter gives the preliminaries. First, we give a detailed explanation of PLBF, one\nof the state-of-the-art LBFs (Section 3.1). Next, we introduce the matrix problem and the\nclasses of matrices related to the problem (Section 3.2).\n3.1 Partitioned Learned Bloom Filter\nFirst, we define terms to describe PLBF (Vaidya et al., 2021) and explain its design. Let\nSbe a set of elements for which the Bloom filter is to be built, and let Qbe the set of\nelements not included in Sthat is used when constructing PLBF ( S‚à©Q =‚àÖ). The elements\nincluded in Sare called keys, and those not included are called non-keys. Qis employed\nto approximate the distribution of non-key queries. To build PLBF, a machine learning\nmodel is trained to predict whether a given element xis included in the set SorQ. For\na given element x, the machine learning model outputs a score s(x) (‚àà[0,1]). The score\ns(x) indicates ‚Äúhow likely is xto be included in the set S.‚Äù PLBF partitions the score\nspace [0 ,1] into kregions and assigns backup Bloom filters with different FPRs to each\nregion (Figure 1). Given a target overall FPR, F(‚àà(0,1)), PLBF optimizes t(‚ààRk+1)\nandf(‚ààRk) to minimize the total memory usage. Here, tis a vector of thresholds for\n5\n\nSato and Matsui\nAlgorithm 1 PLBF (Vaidya et al., 2021)\nInput:\ng‚ààRN: probabilities that the keys are contained in each segment\nh‚ààRN: probabilities that the non-keys are contained in each segment\nF‚àà(0,1) : target overall FPR\nk‚ààN: number of regions\nOutput:\nt‚ààRk+1: threshold boundaries of each region\nf‚ààRk: FPRs of each region\nAlgorithm:\nThresMaxDivDP (g,h, j, k) :\nconstructs DPj\nKLand calculates the optimal thresholds by tracing the transitions\nbackward from DPj\nKL[j‚àí1][k‚àí1]\nOptimalFPR (g,h,t, F, k) :\nreturns the optimal FPRs for each region under the given thresholds\nSpaceUsed (g,h,t,f) :\nreturns the total size of the backup Bloom filters for the given thresholds and FPRs\nMinSpaceUsed ‚Üê ‚àû\ntbest‚ÜêNone\nfbest‚ÜêNone\nforj=k, k+ 1, . . . , N\nt‚ÜêThresMaxDivDP (g,h, j, k)\nf‚ÜêOptimalFPR (g,h,t, F, k)\nifMinSpaceUsed >SpaceUsed (g,h,t,f)then\nMinSpaceUsed ‚ÜêSpaceUsed (g,h,t,f)\ntbest‚Üêt\nfbest‚Üêf\nreturn tbest,fbest\npartitioning the score space into kregions, and fis a vector of FPRs for each region,\nsatisfying t0= 0< t1< t2<¬∑¬∑¬∑< tk= 1 and 0 < fi‚â§1 (i= 1,2, . . . , k ).\nNext, we explain the optimization problem and its solution designed by PLBF. PLBF\nfinds values of tandfthat minimize the total memory usage of the backup Bloom filter,\nunder the condition that the expected value of the overall FPR is F. Here, we define\nG(‚ààRk) and H(‚ààRk) as follows:\nGi= Pr[ ti‚â§s(x)< ti+1|x‚àà S], H i= Pr[ ti‚â§s(x)< ti+1|x‚àà Q]. (2)\nGiandHirepresent the probability that the score of a key and non-key, respectively, falls\nwithin the i-th region. Once tis determined, GandHcan be obtained using S,Q,\nand the learned model. The expected value of the overall FPR isPk\ni=1Hifi. The total\nmemory usage of the backup Bloom filter is given byPk\ni=1c|S|Gilog2(1/fi), where cis\na constant determined by the type of backup Bloom filters. Therefore, the optimization\n6\n\nFast Construction of PLBF with Guarantees\nAlgorithm 2 OptimalFPR (Algorithm 1 by Vaidya et al. (2021))\nInput:\nG‚ààRk: probabilities that the keys are contained in each region\nH‚ààRk: probabilities that the non-keys are contained in each region\nF‚àà(0,1) : target overall FPR\nk‚ààN: number of regions\nOutput:\nf‚ààRk: FPRs of each region\nfori= 1,2, . . . , k\nfi‚ÜêGiF/H i\nwhile ‚àÉi:fi>1\nGf=1‚Üê0\nHf=1‚Üê0\nfori= 1,2, . . . , k\niffi‚â•1then\nfi‚Üê1\nGf=1‚ÜêGf=1+Gi\nHf=1‚ÜêHf=1+Hi\nfori= 1,2, . . . , k\niffi<1then\nfi‚Üê((F‚àíHf=1)Gi)/((1‚àíGf=1)Hi)\nreturn f\nproblem designed by PLBF is expressed as follows:\nminimize\nf,tkX\ni=1c|S|Gilog2\u00121\nfi\u0013\nsubject toÔ£±\nÔ£¥Ô£¥Ô£¥Ô£¥Ô£≤\nÔ£¥Ô£¥Ô£¥Ô£¥Ô£≥kX\ni=1Hifi‚â§F\nt0= 0< t1< t2<¬∑¬∑¬∑< tk= 1\nfi‚â§1 ( i= 1,2, . . . , k ).(3)\nThe original PLBF paper solved the problem by ignoring the condition fi‚â§1. However,\nby introducing the set of indices If=1, where fi= 1, we can obtain a more general form of\nfas follows (see the appendix for details):\nfi=Ô£±\nÔ£¥Ô£≤\nÔ£¥Ô£≥1 ( i‚àà If=1)\n(F‚àíHf=1)Gi\n(1‚àíGf=1)Hi(i /‚àà If=1),(4)\nwhere\nGf=1=X\ni‚ààIf=1Gi, H f=1=X\ni‚ààIf=1Hi. (5)\n7\n\nSato and Matsui\nThen, the total backup Bloom filter memory usage, that is, the objective function of Equa-\ntion (3), is\nc|S|(1‚àíGf=1) log2\u00121‚àíGf=1\nF‚àíHf=1\u0013\n‚àíc|S|X\ni/‚ààIf=1Gilog2\u0012Gi\nHi\u0013\n. (6)\nNext, we explain how PLBF finds the optimal tandf. PLBF divides the score space\n[0,1] into N(> k) segments and then finds the optimal tandfusing DP. Deciding how to\ncluster Nsegments into kconsecutive regions corresponds to determining the threshold t\n(Figure 2). We denote the probabilities that the key and non-key scores are contained in the\ni-thsegment bygiandhi, respectively. Then, the probabilities that the key and non-key\nscores are contained in the i-thregion , that is, GiandHi, are then calculated by summing\nthegandhvalues of the segments within each region (for example, g1+g2+g3=G1in\nFigure 2).\nPLBF makes the following assumption to find the optimal thresholds t.\nAssumption 1 For optimal tandf,If=1=‚àÖorIf=1={k}.\nIn other words, PLBF assumes that there is at most one region for which f= 1, and if there\nis one, it is the region with the highest score output by the machine learning model. Using\nthis assumption, PLBF finds the optimal solution by trying all possible threshold values\ntk‚àí1. That is, PLBF finds the thresholds tthat minimize memory usage by solving the\nfollowing problem for each j=k, k+ 1, . . . , N ; we find a way to cluster the 1st to ( j‚àí1)-th\nsegments into k‚àí1 regions that maximizes\nk‚àí1X\ni=1Gilog2\u0012Gi\nHi\u0013\n. (7)\nPLBF solves this problem by building a j√ókDP table DPj\nKL[p][q] (p= 0,1, . . . , j ‚àí1\nandq= 0,1, . . . , k ‚àí1) for each j=k, k+1, . . . , N ; DPj\nKL[p][q] denotes the maximum value\nofPq\ni=1Gilog2(Gi/Hi) that can be obtained by clustering the 1st to p-th segments into q\nregions. To construct PLBF, one must find a clustering method that achieves DPj\nKL[j‚àí\n1][k‚àí1]. DPj\nKLcan be computed recursively as follows:\nDPj\nKL[p][q] (8)\n=Ô£±\nÔ£¥Ô£¥Ô£≤\nÔ£¥Ô£¥Ô£≥0 ( p= 0‚àßq= 0)\n‚àí‚àû ((p= 0‚àßq >0)‚à®(p >0‚àßq= 0))\nmax\ni=1,2,...,p\u0010\nDPj\nKL[i‚àí1][q‚àí1] +dKL(i, p)\u0011\n(else) ,(9)\nwhere the function dKL(il, ir) is the following function defined for integers ilandirsatisfying\n1‚â§il‚â§ir‚â§N:\ndKL(il, ir) =Ô£´\nÔ£≠irX\ni=ilgiÔ£∂\nÔ£∏log2 Pir\ni=ilgiPir\ni=ilhi!\n. (10)\n8\n\nFast Construction of PLBF with Guarantees\n72415426535897932341max\n7694\nFigure 3: Example of a\nmatrix problem\nfor a monotone\nmatrix .\n2482417(a)\n2482417 (b)\n329248241734171 (c)\nFigure 4: Monotone maxima. (a) exhaustive search on the\nmiddle row, and (b) narrowing the search area\n(c) are repeated recursively.\nThe time complexity to construct this DP table is O(j2k). Then, by tracing the recorded\ntransitions backward from DPj\nKL[j‚àí1][k‚àí1], we obtain the best clustering with a time\ncomplexity of O(k). As the DP table is constructed for each j=k, k+ 1, . . . , N , the overall\ncomplexity is O(N3k). We can divide the score space more finely with a larger Nand thus\nobtain a tcloser to the optimum. However, the time complexity increases rapidly with\nincreasing N.\nWe show the PLBF algorithm in Algorithm 1, and the OptimalFPR algorithm, which\nis used by PLBF, in Algorithm 2. For details of SpaceUsed , please refer to Equation\n(2) in the original PLBF paper (Vaidya et al., 2021). The worst case time complexity of\nOptimalFPR isO(k2) because this algorithm repeatedly solves the relaxation problem up\ntoktimes with a complexity of O(k) until there are no more is such that fi>1. The time\ncomplexity of SpaceUsed isO(k). As the DP table is constructed with a time complexity\nofO(j2k) for each j=k, k+ 1, . . . , N , the overall complexity is O(N3k).\n3.2 Matrix Problem\nHere, we define the matrix problem ,monotone matrix , and totally monotone matrix follow-\ning Aggarwal et al. (1987).\nDefinition 2 ( matrix problem )LetBbe an n√ómreal matrix, and let J:{1,2, . . . , n } ‚Üí\n{1,2, . . . , m }be a function defined on B. For each row index i,J(i)denotes the smallest\ncolumn index jsuch that Bi,jis the maximum value in the i-th row of B. Finding the value\nofJ(i)for all i(‚àà {1,2, . . . , n })is called a matrix problem.\nDefinition 3 ( monotone matrix )Ann√ómreal matrix Bis called a monotone matrix\nifJ(i)‚â§J(i‚Ä≤)for any iandi‚Ä≤that satisfy 1‚â§i < i‚Ä≤‚â§n.\nDefinition 4 ( totally monotone matrix )Ann√ómreal matrix Bis called a totally\nmonotone matrix if every submatrix of Bis monotone.\nIt is known that the definition of totally monotone matrix is equivalent to the following:\nall 2√ó2 sub-matrices are monotone, that is, for any 1 ‚â§i < i‚Ä≤‚â§nand 1 ‚â§j < j‚Ä≤‚â§m,\nAi,j< A i,j‚Ä≤‚áíAi‚Ä≤,j< A i‚Ä≤,j‚Ä≤holds.\n9\n\nSato and Matsui\nAn example of a matrix problem for a monotone matrix is shown in Figure 3. Solving\nthematrix problem for a general n√ómmatrix requires O(nm) time complexity because all\nmatrix values must be checked. Meanwhile, if the matrix is known to be a monotone matrix ,\nthematrix problem for this matrix can be solved with time complexity of O(n+mlogn)\nusing the monotone maxima algorithm (Aggarwal et al., 1987). The exhaustive search for\nthe middle row and the refinement of the search range are repeated recursively (Figure 4).\nIn addition, the SMAWK algorithm can solve matrix problem inO(n+m) for a totally\nmonotone matrix . This algorithm efficiently solves matrix problem by recursively reduc-\ning the number of rows and columns to be considered, using the properties of completely\nmonotonic matrices to maintain the necessary elements.\n4 Methods\nIn this section, we propose three methods to speed up the construction of PLBF. First, we\nshow that the computation of building the DP table O(N) times in PLBF is redundant,\nand propose a method called fast PLBF that builds the DP table only one time and reuses\nit efficiently (Section 4.1). The computational complexity of building PLBF is O(N3k), but\nfast PLBF is O(N2k). We then propose fast PLBF++ (Section 4.2). In this method, we\nreduce the construction time to O(NklogN) by (i) replacing OptimalFPR with a method\nthat has a smaller worst-case computational complexity, and (ii) using the monotone max-\nima algorithm to speed up the construction of DP table. Finally, we propose fast PLBF#\n(Section 4.3). This algorithm reduces the construction time to O(Nklogk) by using the\nSMAWK algorithm instead of the monotone maxima. Although fast PLBF++ and fast\nPLBF# use approximations and do not necessarily achieve the same data structure as\nPLBF, we guarantee that, under an ideal condition, fast PLBF++ and fast PLBF# will\nhave the same structure as PLBF. Additionally, we provide a theoretical guarantee for fast\nPLBF++ even in non-ideal conditions.\n4.1 Fast PLBF\nWe propose fast PLBF, which constructs the same data structure as PLBF more quickly\nthan PLBF by omitting the redundant construction of DP tables. Fast PLBF uses the same\ndesign as PLBF and finds the optimal tandfto minimize memory usage.\nPLBF constructs a DP table for each j=k, k+ 1, . . . , N . We found that this computa-\ntion is redundant and that we can also use the last DP table DPN\nKLforj=k, k+1, . . . , N ‚àí1.\nThis is because the maximum value ofPk‚àí1\ni=1Gilog2(Gi/Hi) when clustering the 1st to\n(j‚àí1)-th segments into k‚àí1 regions is equal to DPN\nKL[j‚àí1][k‚àí1]. We can obtain the best\nclustering by tracing the transitions backward from DPN\nKL[j‚àí1][k‚àí1]. The time complexity\nof tracing the transitions is O(k), which is faster than constructing the DP table.\nThe pseudo-code for fast PLBF construction is provided in Algorithm 3. The time\ncomplexity of building DPN\nKLisO(N2k), and the worst-case complexity of subsequent com-\nputations is O(Nk2). Because N > k , the total complexity is O(N2k), which is faster than\nO(N3k) for PLBF, although fast PLBF constructs the same data structure as PLBF.\n10\n\nFast Construction of PLBF with Guarantees\nAlgorithm 3 Fast PLBF\nAlgorithm:\nMaxDivDP (g,h, N, k ) :\nconstructs DPN\nKL\nThresMaxDiv (DPN\nKL, j, k) :\ntraces the transitions backward from DPN\nKL[j‚àí1][k‚àí1] and finds the optimal thresh-\nolds\nMinSpaceUsed ‚Üê ‚àû\ntbest‚ÜêNone\nfbest‚ÜêNone\nDPN\nKL‚ÜêMaxDivDP (g,h, N, k )\nforj=k, k+ 1, . . . , N\nt‚ÜêThresMaxDiv (DPN\nKL, j, k)\nf‚ÜêOptimalFPR (g,h,t, F, k)\nifMinSpaceUsed >SpaceUsed (g,h,t,f)then\nMinSpaceUsed ‚ÜêSpaceUsed (g,h,t,f)\ntbest‚Üêt\nfbest‚Üêf\nreturn tbest,fbest\n4.2 Fast PLBF++\nWe propose fast PLBF++, which can be constructed even faster than fast PLBF. The\nacceleration of fast PLBF++ consists of two components. First, fast PLBF++ improves\nthe worst-case computational complexity of OptimalFPR , the algorithm for finding the\noptimal FPR for each backup Bloom filter (Section 4.2.1) Second, fast PLBF++ accelerates\nthe construction of the DP table DPN\nKL, that is, MaxDivDP , by taking advantage of a\ncharacteristic that DP tables often have (Section 4.2.2). We provide theoretical guarantees\non the memory efficiency of fast PLBF++ under both ideal conditions (Section 4.2.3) and\nnon-ideal conditions (Section 4.2.4).\n4.2.1 Speed up OptimalFPR\nWe improve the worst-case complexity of PLBF‚Äôs OptimalFPR fromO(k2) toO(klogk).\nThe original OptimalFPR algorithm repeatedly solves the relaxation problem, where the\ncondition fi‚â§1 is ignored, until there are no more isatisfying fi>1. Since each iteration\ntakes a complexity of O(k) and the repetition is at most ktimes, the worst-case complexity\nof the original OptimalFPR algorithm is O(k2). However, by carefully analyzing the\noptimization problem, we can improve the worst-case complexity to O(klogk).\nFirst, we prove the following theorem.\nTheorem 5 There exists an optimal solution fto the optimization problem (Equation 3)\nthat satisfies\nGi\nHi<Gj\nHj(11)\n11\n\nSato and Matsui\nfor all i, jsuch that fi<1, fj= 1. (Here, when Hi= 0,Gi/Hi=‚àû, which is greater\nthan any finite value.)\nProof First, in the case ofP\ni‚àà{j|Gj>0}Hi‚â§F, one of the optimal solution fis\nfi=(\n0 (Gi= 0)\n1 (Gi>0),(12)\nand it is clear that the claim holds.\nOtherwise, in the case ofP\ni‚àà{j|Gj>0}Hi> F, from Equation (71), Equation (72), and\nEquation (73), there exists an optimal solution fand a threshold T(‚ààR) that satisfy\nHi= 0‚áífi= 1, (13)\nHi>0‚àßGi\nHi‚â•T‚áífi= 1, (14)\nHi>0‚àßGi\nHi< T‚áífi<1. (15)\nTherefore, it is clear that the claim holds.\nThis theorem indicates that only some iwith the highest Gi/Hiare included in the set\nIf=1. As a result, there are at most kpotential candidate sets If=1that could form the\noptimal solution.\nTo find the optimal set, we first sort the values of Gi/Hiin descending order. This\nsorting operation has a worst-case time complexity of O(klogk). Once sorted, we evaluate\nthe objective function for each candidate set If=1sequentially. To do this efficiently, we\nmaintain and update the following values as we examine each candidate set: Gf=1(=P\ni‚ààIf=1Gi),Hf=1(=P\ni‚ààIf=1Hi), andP\ni/‚ààIf=1Gilog2(Gi/Hi). This allows us to evaluate\nEquation (6) in O(1) time for each candidate set. Therefore, the total worst-case complexity\nisO(klogk).\n4.2.2 Speed up MaxDivDP\nFast PLBF++ speed up MaxDivDP based on the following intuition; it is more likely that\nwhen the number of regions is fixed at qand the number of segments is increased by 1, the\noptimal tq‚àí1remains unchanged or increases, as in Figure 5(a), than that tq‚àí1decreases,\nas in Figure 5(b). Fast PLBF++ takes advantage of this characteristic to construct the DP\ntable with less complexity O(NklogN).\nFirst, we define the terms to describe fast PLBF++. For simplicity, DPN\nKLis denoted\nas DP KLin this section. The ( N‚àí1)√ó(N‚àí1) matrix Ais defined as follows:\nAp,i=(\nDPKL[i‚àí1][q‚àí1] +dKL(i, p) (i= 1,2, . . . , p )\n‚àí‚àû (else) .(16)\nThen, from the definition of DP KL,\nDPKL[p][q] = max\ni=1,2,...,N‚àí1Ap,i. (17)\n12\n\nFast Construction of PLBF with Guarantees\nùëù+1ùë°!\"#ùëñ12ùëùùëî$‚Ñé$ùë°!\"#ùëñ12ùëùùëî$‚Ñé$\n(a)\nùëî!‚Ñé!\n‚Ñé!\nùëù+1ùë°\"#$ùëñ12ùëùùë°\"#$ùëñ12ùëùùëî! (b)\nFigure 5: When the number of regions is fixed at qand the number of segments is increased\nby 1, the optimal tq‚àí1remains unchanged or increases if Ais amonotone matrix\n(a). When Ais not a monotone matrix (b), the optimal tq‚àí1may decrease.\nùëñ123‚Ä¶DP!\"ùëñ‚àí1[ùëû‚àí1]\nDP!\"ùëù[ùëû]ùëù123‚Ä¶\n‚àí‚àû‚àí‚àû‚àí‚àû‚àí‚àû‚àí‚àû‚àí‚àû‚àí‚àû‚àí‚àû‚àí‚àû‚àí‚àûùê¥=maxùëÅ‚àí1\nùëÅ‚àí1\nFigure 6: Computing DP KL[p][q] (p= 1,2, . . . , N ‚àí1) from DP KL[p][q‚àí1] (p= 0,1, . . . , N ‚àí\n2) via the matrix A. The computation is the same as solving the matrix problem\nfor matrix A. When the score distribution is ideal,Ais amonotone matrix .\nThe matrix Arepresents the intermediate calculations involved in determining DP KL[p][q]\nfrom DP KL[p][q‚àí1] (Figure 6). When the intuition that tq‚àí1does not decrease, as discussed\nat the beginning of this section, holds, Ais amonotone matrix .\nWhile fast PLBF checks almost all elements of the matrix Aand thus takes O(N2) com-\nputations, fast PLBF++ uses the monotone maxima algorithm to obtain an approximation\nof the maximum value of each row of the matrix AinO(NlogN) computations. This is\nmuch faster than PLBF and fast PLBF, but it involves certain approximations, so it does\nnot necessarily achieve the same optimal memory efficiency as those methods.\n4.2.3 Theoretical Guarantees Under Ideal Conditions\nHere, we demonstrate that the memory efficiency of fast PLBF++ is equivalent to that\nof PLBF under ideal conditions. The acceleration of fast PLBF++ is attributed to two\n13\n\nSato and Matsui\nfactors. The first factor, speed up OptimalFPR , involves no approximation, so we focus\non proving that there is no degradation due to approximation in the second factor, speed\nupMaxDivDP , under ideal conditions.\nFirst, we define an ideal score distribution as follows.\nDefinition 6 A score distribution is called ideal if\ng1\nh1‚â§g2\nh2‚â§ ¬∑¬∑¬∑ ‚â§gN\nhN. (18)\nAnideal score distribution implies that the probability of x(‚àà S) and the score s(x) are\nideally well correlated. In other words, an ideal score distribution means that the machine\nlearning model learns the distribution ideally.\nFast PLBF++ does not necessarily have the same data structure as PLBF because Ais\nnot necessarily a monotone matrix . However, as the following theorem shows, we can prove\nthatAis amonotone matrix and thus fast PLBF++ has the same data structure as (fast)\nPLBF under certain conditions.\nFirst, the following lemma holds.\nLemma 7 If the score distribution is ideal, then for all 1‚â§i < i‚Ä≤‚â§p < p‚Ä≤‚â§N, it holds\nthat\u0000\ndKL(i, p) +dKL(i‚Ä≤, p‚Ä≤)\u0001\n‚àí\u0000\ndKL(i, p‚Ä≤) +dKL(i‚Ä≤, p)\u0001\n‚â•0. (19)\nProof We define u1,u2,v1,v2,x, and yas follows:\nu1:=pX\nj=i‚Ä≤gj, u 2:=pX\nj=igj, x:=p‚Ä≤X\nj=p+1gj,\nv1:=pX\nj=i‚Ä≤hj, v 2:=pX\nj=ihj, y:=p‚Ä≤X\nj=p+1hj.(20)\nHere, we define the left-hand side of Equation (19) as the function D(x, y), that is,\nD(x, y) (21)\n:=\u0000\ndKL(i, p) +dKL(i‚Ä≤, p‚Ä≤)\u0001\n‚àí\u0000\ndKL(i, p‚Ä≤) +dKL(i‚Ä≤, p)\u0001\n(22)\n=\u0012\nu2log2\u0012u2\nv2\u0013\n+ (u1+x) log2\u0012u1+x\nv1+y\u0013\u0013\n‚àí (23)\n\u0012\n(u2+x) log2\u0012u2+x\nv2+y\u0013\n+u1log2\u0012u1\nv1\u0013\u0013\n. (24)\nSince the score distribution is ideal, we have ( u2+x)/(v2+y)‚â§(u1+x)/(v1+y).\nTherefore,\n‚àÇD\n‚àÇx= log2\u0012u1+x\nv1+y\u0013\n‚àílog2\u0012u2+x\nv2+y\u0013\n‚â•0, (25)\n‚àÇD\n‚àÇy=\u0012\n‚àíu1+x\nv1+y+u2+x\nv2+y\u0013\nlog2(e)‚â§0. (26)\n14\n\nFast Construction of PLBF with Guarantees\nSince the score distribution is ideal, we also have u1/v1‚â§x/y. Thus,\nD(x, y)‚â• inf\n0<x,0<y,u1\nv1‚â§x\nyD(x, y) (27)\n‚â• inf\n0<x,0<y,u1\nv1=x\nyD(x, y). (28)\nWhenu1\nv1=x\ny, we can introduce a positive variable zsuch that x=zu1andy=zv1.\nSubstituting this into the expression, we have\nD(x, y)‚â•inf\nz>0D(zu1, zv1) (29)\n= inf\nz>0\u0012\nzu1log2\u0012zu1\nzv1\u0013\n+u2log2\u0012u2\nv2\u0013\n‚àí(zu1+u2) log2\u0012zu1+u1\nzv1+v1\u0013\u0013\n.(30)\nFinally, by applying Jensen‚Äôs inequality (Jensen, 1906), we obtain D(x, y)‚â•0.\nUsing Lemma 7, we can prove the following theorem, which demonstrates that the\nmemory efficiency of fast PLBF++ is equivalent to that of PLBF when the score distribution\nisideal.\nTheorem 8 If the score distribution is ideal, Ais a totally monotone matrix.\nProof\nFor any i, i‚Ä≤, p, p‚Ä≤satisfying 1 ‚â§i < i‚Ä≤‚â§p < p‚Ä≤‚â§N‚àí1,\n\u0000\nAp,i+Ap‚Ä≤,i‚Ä≤\u0001\n‚àí\u0000\nAp‚Ä≤,i+Ap,i‚Ä≤\u0001\n(31)\n=\u0000\n(DP KL[i‚àí1][q‚àí1] +dKL(i, p)) +\u0000\nDPKL[i‚Ä≤‚àí1][q‚àí1] +dKL(i‚Ä≤, p‚Ä≤)\u0001\u0001\n‚àí (32)\u0000\u0000\nDPKL[i‚àí1][q‚àí1] +dKL(i, p‚Ä≤)\u0001\n+\u0000\nDPKL[i‚Ä≤‚àí1][q‚àí1] +dKL(i‚Ä≤, p)\u0001\u0001\n(33)\n=\u0000\ndKL(i, p) +dKL(i‚Ä≤, p‚Ä≤)\u0001\n‚àí\u0000\ndKL(i, p‚Ä≤) +dKL(i‚Ä≤, p)\u0001\n(34)\n‚â•0. (35)\nThe last inequality is from the Lemma 7. Thus, Ap,i< A p,i‚Ä≤‚áíAp‚Ä≤,i< A p‚Ä≤,i‚Ä≤holds. Conse-\nquently, it has been demonstrated that every 2 √ó2 submatrix is monotone, thereby proving\nthatAis atotally monotone matrix .\n4.2.4 Theoretical Guarantees Under Non-Ideal Conditions\nHere, we provide a theoretical guarantee for cases where the distribution is not ideal. Specifi-\ncally, we offer an upper bound on the difference in DP KLbetween solving the matrix problem\ngreedily and solving it efficiently using monotone maxima. For simplicity, we will assume\nthat N= 2M(M‚ààN). In this case, when selecting the ‚Äúmiddle row‚Äù in the monotone\nmaxima algorithm, the number of rows to be considered will always be odd, allowing us\nto choose the exact middle row. For example, in Figure 4, the number of rows is 7, corre-\nsponding to the case of N= 8 = 23.\n15\n\nSato and Matsui\nFirst, we define the function ‚àÜ : {0,1, . . . , N } √ó { 0,1, . . . , N } ‚ÜíRas follows:\n‚àÜ(p, p‚Ä≤) (36)\n=Ô£±\nÔ£≤\nÔ£≥max\n1‚â§i‚â§i‚Ä≤‚â§p‚àí\u0000\ndKL(i, p) +dKL(i‚Ä≤, p‚Ä≤)\u0001\n+\u0000\ndKL(i, p‚Ä≤) +dKL(i‚Ä≤, p)\u0001\n(0< p < p‚Ä≤< N)\n0 (else) .(37)\nFrom Lemma 7, when the distribution is ideal, ‚àÜ(p, p‚Ä≤)‚â§0 for any p, p‚Ä≤. Furthermore, we\ndefine Œ¥:{0,1, . . . , N } ‚ÜíRrecursively as follows:\nŒ¥(p) =(\n0 ( p= 0, N)\nmax( Œ¥(l) + ‚àÜ( l, p),0, Œ¥(r) + ‚àÜ( p, r)) (else) .(38)\nIn this definition, lis the largest row index smaller than pthat has been processed before\nthep-th row by the monotone maxima algorithm (if no such row exists, we set l= 0).\nSimilarly, ris the smallest row index greater than pthat has been processed before the p-th\nrow by the monotone maxima algorithm (if no such row exists, we set r=N). For example,\nwhen N= 8, as shown in Figure 4, we have l= 2 and r= 4 for p= 3, l= 0 and r= 8 for\np= 4. Since N= 2M(M‚ààN), we have l=p‚àí2mandr=p+ 2m, where mis the largest\nnon-negative integer such that pis divisible by 2m. Then, we define Œ¥maxas follows:\nŒ¥max= max\np=1,2,...,N‚àí1Œ¥(p). (39)\nŒ¥maxis a value that depends on the training data and N. In particular, when the distribution\nisideal,Œ¥(p) = 0 for all p, and therefore, Œ¥max= 0.\nHere, the following theorem holds.\nTheorem 9 LetDPtruebe the DPKLin the case of (fast) PLBF, that is, when calculating\ngreedily without using monotone maxima. Furthermore, let DPmmbe the DPKLin the\ncase of fast PLBF++, that is, when calculating with monotone maxima. Then, for all\np= 1,2, . . . , N ‚àí1andq= 1,2, . . . , k ‚àí1,\n0‚â§DPtrue[p][q]‚àíDPmm[p][q]‚â§qŒ¥max. (40)\nProof DPtrue[p][q] is a DP table derived from the exact calculation of all transitions,\nwhereas DP mm[p][q] is a DP table derived from calculations where some transitions are\nomitted. Therefore, it is evident that 0 ‚â§DPtrue[p][q]‚àíDPmm[p][q]. We will now prove the\nright-hand side of the inequality.\nAs the definition in Theorem 2, let J(p) be the smallest column index jsuch that Ap,j\nis the maximum value in the p-th row of A. Additionally, let ÀÜJ(p) be the column index j\nsuch that the monotone maximum algorithm considers Ap,jto be the maximum value in\nthep-th row of A.J(p) and ÀÜJ(p) always satisfy Ap,J(p)‚àíAp,ÀÜJ(p)‚â•0.\nFirst, we prove recursively that Ap,J(p)‚àíAp,ÀÜJ(p)‚â§Œ¥(p) for all p= 1,2, . . . , N ‚àí1. We\nintroduce virtual 0-th and N-th row to the matrix A, and we assume J(0) = ÀÜJ(0) = 0,\nJ(N) = ÀÜJ(N) =N‚àí1, and A0,J(0)‚àíA0,ÀÜJ(0)=AN,J(N)‚àíAN,ÀÜJ(N)= 0. Now, assume\nthat for all q(‚àà {0,1, . . . , N }) that is a multiple of 2m+1,Aq,J(q)‚àíAq,ÀÜJ(q)‚â§Œ¥(q) holds.\n16\n\nFast Construction of PLBF with Guarantees\nThis assumption is clearly true when m+ 1 = Mbecause N= 2Mand we introduced the\nvirtual 0-th and N-th row. Then, we can prove that for any p(‚àà {0,1, . . . , N }) that is a\nmultiple of 2m,Ap,J(p)‚àíAp,ÀÜJ(p)‚â§Œ¥(p) holds. In the following, let pbe a number that is\na multiple of 2m, but not a multiple of 2m+1. In this case, l=p‚àí2mandr=p+ 2m,\nwhere the range of indices to be searched by the monotone maximum algorithm in the p-th\nrow is determined by the search result of the l-th and r-th rows. We denote the minimum\nand maximum values of this range of indices as L(p) and R(p), which can be expressed as\nfollows:\nL(p) =(\n1 ( l= 0)\nÀÜJ(l) (else), R (p) =(\nN‚àí1 (r=N)\nÀÜJ(r) (else) .(41)\nFirst, from the definition of ÀÜJ(p),\nmax\nL(p)‚â§j‚â§R(p)Ap,j‚àíAp,ÀÜJ(p)=Ap,ÀÜJ(p)‚àíAp,ÀÜJ(p)= 0. (42)\nNext, when R(p)< p,\nmax\nR(p)<j‚â§pAp,j‚àíAp,ÀÜJ(p)‚â§max\nR(p)<j‚â§pAp,j‚àíAp,ÀÜJ(r)(43)\n‚â§max\nR(p)<j‚â§pAr,j‚àíAr,ÀÜJ(r)+ ‚àÜ( p, r) (44)\n‚â§Ar,J(r)‚àíAr,ÀÜJ(r)+ ‚àÜ( p, r) (45)\n‚â§Œ¥(r) + ‚àÜ( p, r). (46)\nHere, the first and third inequality is due to Ap,ÀÜJ(p)= max ÀÜJ(l)‚â§j‚â§ÀÜJ(r)Ap,j‚â•Ap,ÀÜJ(r)and\nAr,J(r)= max 1‚â§j‚â§rAr,j‚â•Ar,J(r). The fourth inequality is based on the assumptions of\ninduction (note that ris multiple of 2m+1). The second inequality can be shown as follows,\nusing the definition of Aand ‚àÜ:\nmax\nÀÜJ(r)<j‚â§pAp,j‚àíAp,ÀÜJ(r)(47)\n= max\nÀÜJ(r)<j‚â§p(DP KL[j‚àí1][q‚àí1] +dKL(j, p))‚àí (48)\n\u0010\nDPKL[ÀÜJ(r)‚àí1][q‚àí1] +dKL(ÀÜJ(r), p)\u0011\n(49)\n‚â§max\nÀÜJ(r)<j‚â§p(DP KL[j‚àí1][q‚àí1] +dKL(j, r))‚àí (50)\n\u0010\nDPKL[ÀÜJ(r)‚àí1][q‚àí1] +dKL(ÀÜJ(r), r)\u0011\n+ ‚àÜ( p, r) (51)\n= max\nÀÜJ(r)<j‚â§pAr,j‚àíAr,ÀÜJ(r)+ ‚àÜ( p, r). (52)\nIn the same way, we have\nmax\n1‚â§j<L(p)Ap,j‚àíAp,ÀÜJ(p)‚â§Œ¥(l) + ‚àÜ( l, p). (53)\n17\n\nSato and Matsui\nTherefore, from Equation (53), Equation (42), and Equation (46),\nAp,J(p)‚àíAp,ÀÜJ(p)= max\n1‚â§j‚â§pAp,j‚àíAp,ÀÜJ(p)(54)\n‚â§max ( Œ¥(l) + ‚àÜ( l, p),0, Œ¥(r) + ‚àÜ( p, r)) (55)\n=Œ¥(p). (56)\nThus, we have Ap,J(p)‚àíAp,ÀÜJ(p)for all p(‚àà {1,2, . . . , N ‚àí1}). This means that a deviation\nup to Œ¥maxis produced in row to row calculation in DP table. Therefore, it has been shown\nthat the maximum error in DP KL[p][q] isqŒ¥max.\nSince Œ¥max= 0 when the distribution is ideal, the DP table generated in the fast PLBF++\nconstruction is exactly the same as the DP table generated in the fast PLBF construction.\nThe conclusion from Theorem 8 covers only ideal situations. However, Theorem 9 theo-\nretically bounds the difference even in non-ideal situations by using Œ¥max, a value that, in\na sense, indicates the deviation from the ideal distribution. The URL and malware data\nsets we used for our experiment have Œ¥max= 0.028 and Œ¥max= 0.010, respectively, when\nN= 1024. Considering k‚àº10, which is a typical setting, the difference in the DP table\nvalue is approximately up to 0 .25. IfIf=1=‚àÖ, this suggests that the difference in memory\nusage between fast PLBF and fast PLBF++ is at most 0 .25√ólog2(e) = 0 .36 bit per element.\nAdditionally, it implies that for the same memory usage, the difference in the FPR is at\nmost 20.25= 1.18 times.\n4.3 Fast PLBF#\nFast PLBF# uses the SMAWK algorithm instead of the monotone maxima. In addition, as\nwith fast PLBF++, it replaces OptimalFPR withFastOptimalFPR , which has better\nworst-case computational complexity. Therefore, the DP using the SMAWK algorithm takes\nO(Nk) time, and for each of the j=k, k+ 1, . . . , N , determining the optimal FPR takes\nO(klogk) operations, so the overall time taken is O(Nk) +O(Nklogk) =O(Nklogk).\nThe SMAWK algorithm is an algorithm that solves the problem of finding the maxi-\nmum value of each row of a matrix. While monotone maxima assumes that the matrix is\namonotone matrix , the SMAWK algorithm assumes that the matrix is a totally monotone\nmatrix . The totally monotone assumption is stronger than the monotone assumption, that\nis, every totally monotone matrix is amonotone matrix . Therefore, the SMAWK algorithm\nis an algorithm that relies on stronger assumptions than those of monotone maxima. Al-\nthough the SMAWK algorithm is more complicated than monotone maxima, it has less\ncomputational complexity.\nFrom the proof of Theorem 8, Ais atotally monotone matrix when the distribution\nisideal. Therefore, although the SMAWK algorithm relies on stronger assumptions than\nmonotone maxima, it satisfies the assumptions of both algorithms under the condition that\nthe distribution is ideal. As we will see in the next section, fast PLBF# achieves almost\nthe same memory efficiency as (fast) PLBF, just as fast PLBF++ does.\n18\n\nFast Construction of PLBF with Guarantees\n0.0 0.2 0.4 0.6 0.8 1.0\nScore103104105CountMalicious (key)\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104105CountBenign (non-key)\n(a) Malicious URLs Data Set\n0.0 0.2 0.4 0.6 0.8 1.0\nScore103104105CountMalicious (key)\n0.0 0.2 0.4 0.6 0.8 1.0\nScore103104105CountBenign (non-key) (b) EMBER Data Set\nFigure 7: Histograms of the score distributions of keys and non-keys.\n0 200 400 600 800 1000\ni101\n100101102gi/hi\n(a) Malicious URLs Data Set\n0 200 400 600 800 1000\ni103\n102\n101\n100101102103gi/hi (b) EMBER Data Set\nFigure 8: Ratio of keys to non-keys.\n5 Experimetns\nThis section evaluates the experimental performance of fast PLBF, fast PLBF++, and\nfast PLBF#. We compared the performances of our proposed methods with four baselines:\nBloom filter (Bloom, 1970), Ada-BF (Dai and Shrivastava, 2020), sandwiched LBF (Mitzen-\nmacher, 2018), and PLBF (Vaidya et al., 2021). Similar to PLBF, Ada-BF is an LBF that\npartitions the score space into several regions and assigns different FPRs to each region.\nHowever, Ada-BF relies heavily on heuristics for clustering and assigning FPRs. Sand-\nwiched LBF is a LBF that ‚Äúsandwiches‚Äù a machine learning model with two Bloom filters.\nThis achieves better memory efficiency than the original LBF by optimizing the size of two\nBloom filters.\nTo facilitate the comparison of different methods or hyperparameters results, we have\nslightly modified the original PLBF framework. The original PLBF was designed to mini-\nmize memory usage under the condition of a given FPR. However, this approach makes it\ndifficult to compare the results of different methods or hyperparameters. This is because\nboth the FPR at test time and the memory usage vary depending on the method and hy-\nperparameters, which often makes it difficult to determine the superiority of the results.\nTherefore, in our experiments, we used a framework where the expected FPR is minimized\nunder the condition of memory usage. This makes it easy to obtain two results with the\nsame memory usage and compare them by the FPR at test time. See the appendix for\nmore information on how this framework modification will change the construction method\nof PLBFs.\nWe evaluated the algorithms using the following two data sets.\n19\n\nSato and Matsui\nInsert keys to Bloom filter Train machine learning model Find optimal parameters\n100150200\nBloom filter Ada-BF Sandwiched\nLBFPLBF Fast PLBF\n(Proposed)   Fast PLBF++\n   (Proposed)    Fast PLBF#\n    (Proposed)0.02.55.07.5Construction Time [s]\n(a) Malicious URLs Data Set\n100150200250\nBloom filter Ada-BF Sandwiched\nLBFPLBF Fast PLBF\n(Proposed)   Fast PLBF++\n   (Proposed)    Fast PLBF#\n    (Proposed)0510Construction Time [s] (b) EMBER Data Set\nFigure 9: Construction time.\n‚Ä¢Malicious URLs Data Set : As in previous papers (Dai and Shrivastava, 2020;\nVaidya et al., 2021), we used Malicious URLs data set (Siddhartha, 2021). The URLs\ndata set comprises 223,088 malicious and 428,118 benign URLs. We extracted 20\nlexical features such as URL length, use of shortening, number of special characters,\netc. We used all malicious URLs and 342,482 (80%) benign URLs as the training set,\nand the remaining benign URLs as the test set.\n‚Ä¢EMBER Data Set : We used the EMBER data set (Anderson and Roth, 2018) as\nin the PLBF research. The data set consists of 300,000 malicious and 400,000 benign\nfiles, along with the features of each file. We used all malicious files and 300,000 (75%)\nbenign files as the train set and the remaining benign files as the test set.\nWhile any model can be used for the classifier, we used LightGBM (Ke et al., 2017)\nbecause of its fast training and inference, memory efficiency, and accuracy. The sizes of\nthe machine learning model for the URLs and EMBER data sets are 312 Kb and 1.19 Mb,\nrespectively. The training time of the machine learning model for the URLs and EMBER\ndata sets is 1.09 and 2.71 seconds, respectively. The memory usage of LBF is the memory\nusage of the backup Bloom filters plus the size of the machine learning model. Figure 7\nshows a histogram of the score distributions for keys and non-keys in each data set. We\ncan see that the frequency of keys increases and that of non-keys decreases as the score\nincreases. In addition, Figure 8 plots gi/hi(i= 1,2, . . . , N ) when N= 1,000. We can see\nthat gi/hitends to increase as iincreases, but the increase is not monotonic; that is, the\nscore distribution is not ideal.\n5.1 Construction Time\nWe compared the construction times of our proposed methods with those of existing meth-\nods. Following the experiments in the original PLBF paper, hyperparameters for PLBFs\nwere set to N= 1,000 and k= 5.\nFigure 9 shows the construction time for each method. The construction time for LBFs\nincludes not only the time to insert keys into the Bloom filters but also the time to train\n20\n\nFast Construction of PLBF with Guarantees\nBloom filter\nFast PLBF (Proposed)Ada-BF\nFast PLBF++ (Proposed)Sandwiched LBF\nFast PLBF# (Proposed)PLBF\n0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0\nTotal Memory Usage [Mbit]  103\n102\n101\nFalse Positive Rate\n(a) Malicious URLs Data Set\n1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0\nTotal Memory Usage [Mbit]  103\n102\n101\nFalse Positive Rate\n (b) EMBER Data Set\nFigure 10: Trade-off between memory usage and FPR.\nthe machine learning model and the time to compute the optimal parameters ( tandfin\nthe case of PLBF).\nAda-BF and sandwiched LBF use heuristics to find the optimal parameters, so they\nhave shorter construction times than PLBF. However, these methods demonstrate lower\naccuracy, as we will demonstrate in the following section. PLBF has better accuracy but\ntakes more than 3 minutes to find the optimal tandf. On the other hand, our proposed\nmethods take less than 2 seconds. Specifically, fast PLBF constructs 50.8 and 34.3 times\nfaster than PLBF for the URLs and EMBER data sets, respectively. Fast PLBF++ and\nfast PLBF# exhibit even greater speed improvements; fast PLBF++ is 63.1 and 39.3 times\nfaster, and fast PLBF# is 67.2 and 40.8 times faster than PLBF for the URLs and EMBER\ndata sets, respectively. These construction times are comparable to those of sandwiched\nLBF, which relies heavily on heuristics and, as we will see in the next section, is much less\naccurate than PLBF.\n5.2 Memory Usage and FPR\nWe compared the trade-off between memory usage and FPR for the proposed methods\nwith Bloom filter, Ada-BF, sandwiched LBF, and PLBF. Following the experiments in the\noriginal PLBF paper, hyperparameters for PLBFs were always set to N= 1,000 and k= 5.\nFigure 10 shows each method‚Äôs trade-off between memory usage and FPR. PLBF, fast\nPLBF, fast PLBF++, and fast PLBF# have better Pareto curves than the other methods\nfor all data sets. Fast PLBF constructs the same data structure as PLBF in all cases, so it\nalways has exactly the same accuracy as PLBF. In the hyperparameter settings here, fast\nPLBF++ and fast PLBF# also have the same data structure as PLBF in all cases and,\ntherefore, have exactly the same accuracy as PLBF.\n5.3 Ablation Study for Hyper-Parameters\nThe parameters of the PLBFs are total memory size, N, and k. The total memory size is\nspecified by the user, and Nandkare hyperparameters that are determined by balancing\nconstruction time and accuracy. In the previous sections, we set Nto 1,000 and kto 5,\n21\n\nSato and Matsui\nPLBF Fast PLBF (Proposed) Fast PLBF++ (Proposed) Fast PLBF# (Proposed)\n(a) Malicious URLs Data Set\n (b) EMBER Data Set\nFigure 11: Ablation study for hyper-parameter N.\nPLBF Fast PLBF (Proposed) Fast PLBF++ (Proposed) Fast PLBF# (Proposed)\n(a) Malicious URLs Data Set\n (b) EMBER Data Set\nFigure 12: Ablation study for hyper-parameter k.\nfollowing the original paper on PLBF. However, in this section, we perform ablation studies\nfor these hyperparameters to confirm that our proposed methods can construct accurate\ndata structures quickly, no matter what hyperparameter settings are used. We also confirm\nthat the accuracy tends to be better and the construction time increases as Nandkare\nincreased, and that the proposed methods show a much slower increase in their construction\ntime compared to PLBF.\nFigure 11 shows the FPR and construction time with various Nwhile the memory usage\nof the backup Bloom filters is fixed at 500 Kb and kis fixed at 5. For all four PLBFs, the\nFPR tends to decrease as Nincreases. Note that this is the FPR on test data, so it does\nnot necessarily decrease monotonically. Also, as Nincreases, the PLBF construction time\nincreases rapidly, but the fast PLBF construction time increases much more slowly than\nthat, and for fast PLBF++ and fast PLBF#, the construction time changes little. This\nis because the construction time of PLBF is asymptotically proportional to N3, while that\nof fast PLBF, fast PLBF++, and fast PLBF# is proportional to N2,NlogN, and N,\nrespectively. The experimental results show that the three proposed methods can achieve\nhigh accuracy without significantly changing the construction time with large N.\nFigure 12 shows the construction time and FPR with various kwhile the backup Bloom\nfilter memory usage is fixed at 500 Kb and Nis fixed at 1,000. For all four PLBFs, the\nFPR tends to decrease as kincreases. For the EMBER data set, the FPR stops decreasing\nat about k= 20, while for the URLs data set, it continues to decrease even at about\n22\n\nFast Construction of PLBF with Guarantees\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104CountPositive\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104CountNegative\n(a) 0 swaps\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104CountPositive\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104CountNegative (b) 10 swaps\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104CountPositive\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104CountNegative (c) 102swaps\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104CountPositive\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104CountNegative\n(d) 103swaps\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104CountPositive\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104CountNegative (e) 104swaps\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104CountPositive\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104CountNegative (f) 105swaps\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104CountPositive\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104CountNegative\n(g) 106swaps\n0.0 0.2 0.4 0.6 0.8 1.0\nScore103104CountPositive\n0.0 0.2 0.4 0.6 0.8 1.0\nScore103104CountNegative (h) 107swaps\n0.0 0.2 0.4 0.6 0.8 1.0\nScore103104CountPositive\n0.0 0.2 0.4 0.6 0.8 1.0\nScore103104CountNegative (i) 108swaps\nFigure 13: Score distribution histograms of artificial data sets (seed 0).\nk= 500. Just as in the case of experiments with varying N, this decrease is not necessarily\nmonotonic because this is the FPR on test data. Fast PLBF always has the same accuracy\nas PLBF, but fast PLBF++ and fast PLBF# have slightly different accuracy from PLBF;\nfast PLBF++ exhibits up to 1.022 and 1.007 times the FPR of PLBF, and fast PLBF#\nexhibits up to 1.007 and 1.015 times FPR of PLBF for the URLs and EMBER data sets,\nrespectively. In addition, the construction times of all four PLBFs increase proportionally\ntok, but fast PLBF has a much shorter construction time than PLBF, and fast PLBF++\nand fast PLBF# have an even shorter construction time than fast PLBF. When k= 50,\nfast PLBF constructs 233 and 199 times faster, fast PLBF++ constructs 761 and 500 times\nfaster, and fast PLBF# constructs 778 and 507 times faster than PLBF for the URLs and\nEMBER data sets, respectively. The experimental results indicate that by increasing k,\nthe three proposed methods can achieve high accuracy without significantly affecting the\nconstruction time.\n5.4 Robustness of Fast PLBF++ and Fast PLBF# to Non-Ideal Distributions\nFast PLBF++ and fast PLBF# are not theoretically guaranteed to be as accurate as PLBF,\nexcept when the score distribution is ideal. Therefore, we evaluated the accuracy of fast\nPLBF++ by creating a variety of artificial data sets, ranging from data with monotonicity\nto data with little monotonicity. The results show that the smaller the monotonicity of the\nscore distribution, the larger the difference in accuracy between fast PLBF++ and PLBF.\nWe also observed that in most cases, the FPR for fast PLBF++ is within 1.1 times that\nof PLBF, but in cases where there is little monotonicity in the score distribution, the FPR\nof fast PLBF++ and fast PLBF# can be up to 1.85 times and 1.83 times that of PLBF,\nrespectively.\n23\n\nSato and Matsui\n0 200 400 600 800 1000\ni103\n102\n101\n100101102103gi/hi\n(a) 0 swaps\n0 200 400 600 800 1000\ni103\n102\n101\n100101102103gi/hi (b) 10 swaps\n0 200 400 600 800 1000\ni103\n102\n101\n100101102103gi/hi (c) 102swaps\n0 200 400 600 800 1000\ni103\n102\n101\n100101102103gi/hi\n(d) 103swaps\n0 200 400 600 800 1000\ni103\n102\n101\n100101102103gi/hi (e) 104swaps\n0 200 400 600 800 1000\ni103\n102\n101\n100101102103gi/hi (f) 105swaps\n0 200 400 600 800 1000\ni103\n102\n101\n100101102103gi/hi\n(g) 106swaps\n0 200 400 600 800 1000\ni103\n102\n101\n100101102103gi/hi (h) 107swaps\n0 200 400 600 800 1000\ni103\n102\n101\n100101102103gi/hi (i) 108swaps\nFigure 14: Ratio of key to non-key of artificial data sets (seed 0).\nHere, we explain the process of creating an artificial data set, which consists of two\nsteps. First, as in the original PLBF paper (Vaidya et al., 2021), the key and non-key score\ndistribution is generated using the Zipfian distribution. Figure 13(a) shows a histogram of\nthe distribution of key and non-key scores at this time, and Figure 14(a) shows gi/hi(i=\n1,2, . . . , N ) when N= 1,000. This score distribution is ideal. Next, we perform swaps to\nadd non-monotonicity to the score distribution. Swap refers to changing the scores of the\nelements in the two adjacent segments so that the number of keys and non-keys in the two\nsegments are swapped. Namely, an integer iis randomly selected from {1,2, . . . , N ‚àí1},\nand the scores of elements in the i-th segment are changed so that they are included in\nthe (i+ 1)-th segment, and the scores of elements in the ( i+ 1)-th segment are changed to\ninclude them in the i-th segment. Figures 13(b) to 13(i) and Figures 14(b) to 14(i) show the\nhistograms of the score distribution and gi/hi(i= 1,2, . . . , N ) for 10 ,102, . . . , 108swaps,\nrespectively. It can be seen that as the number of swaps increases, the score distribution\nbecomes more non-monotonic. For each case of the number of swaps, 10 different data sets\nwere created using 10 different seeds.\nFigure 15 shows the accuracy of each method for each number of swaps, with the seed\nset to 0. Hyperparameters for PLBFs are set to N= 1,000 and k= 5. It can be seen that\nfast PLBF and fast PLBF++ achieve better Pareto curves than the other methods for all\ndata sets. It can also be seen that the higher the number of swaps, the more often there is\na difference between the accuracy of fast PLBF++/# and fast PLBF.\nFigure 16 shows the difference in accuracy between fast PLBF++/# and fast PLBF\nfor each swap count. Here, the ‚Äúrelative false positive rate‚Äù is the false positive rate of\nfast PLBF++/# divided by that of PLBF constructed with the same hyperparameters\nand conditions. We created 10 different data sets for each swap count and conducted 6\n24\n\nFast Construction of PLBF with Guarantees\nBloom filter\nFast PLBF (Proposed)Ada-BF\nFast PLBF++ (Proposed)Sandwiched LBF\nFast PLBF# (Proposed)\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\nTotal Memory Usage [Mbit]  104\n103\n102\n101\nFalse Positive Rate\n(a) 0 swaps\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\nTotal Memory Usage [Mbit]  104\n103\n102\n101\nFalse Positive Rate\n (b) 10 swaps\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\nTotal Memory Usage [Mbit]  104\n103\n102\n101\nFalse Positive Rate\n (c) 102swaps\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\nTotal Memory Usage [Mbit]  105\n104\n103\n102\n101\nFalse Positive Rate\n(d) 103swaps\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\nTotal Memory Usage [Mbit]  104\n103\n102\n101\nFalse Positive Rate\n (e) 104swaps\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\nTotal Memory Usage [Mbit]  104\n103\n102\n101\nFalse Positive Rate\n (f) 105swaps\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\nTotal Memory Usage [Mbit]  104\n103\n102\n101\nFalse Positive Rate\n(g) 106swaps\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\nTotal Memory Usage [Mbit]  104\n103\n102\n101\nFalse Positive Rate\n (h) 107swaps\n0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6\nTotal Memory Usage [Mbit]  104\n103\n102\n101\nFalse Positive Rate\n (i) 108swaps\nFigure 15: Trade-off between memory usage and FPR for artificial data sets (seed 0). The\nhyperparameters for PLBFs are N= 1,000 and k= 5.\n0101102103104105106107108\nNumber of Swaps1.01.21.41.61.8Relative False Positive Rate\n(a) Fast PLBF++\n0101102103104105106107108\nNumber of Swaps1.01.21.41.61.8Relative False Positive Rate\n (b) Fast PLBF#\nFigure 16: Distribution of the ‚Äúrelative false positive rate‚Äù of fast PLBF++ and fast\nPLBF# for each swap count. The ‚Äúrelative false positive rate‚Äù is the false posi-\ntive rate of fast PLBF++ and fast PLBF# divided by that of PLBF constructed\nwith the same hyperparameters and conditions.\n25\n\nSato and Matsui\nexperiments for each data set and method with a memory usage of 0.25Mb, 0.5Mb, 0.75Mb,\n1.0Mb, 1.25Mb, and 1.5Mb. Namely, we compared the false positive rates of fast PLBF\nand fast PLBF++/# under 60 conditions for each swap count. The result shows that fast\nPLBF++ consistently achieves the same accuracy as PLBF in a total of 240 experiments\nwhere the number of swaps is 103or less. Similarly, fast PLBF# consistently achieves the\nsame accuracy as PLBF in a total of 180 experiments where the number of swaps is 102or\nless. Furthermore, in the experiments where the number of swaps is 107or less, the relative\nfalse positive rate is generally less than 1.1, with the exception of 14 cases for fast PLBF++\nand 7 cases for fast PLBF#. However, in the cases of 108swap counts, where there is\nalmost no monotonicity in the score distribution, the false positive rate for fast PLBF++\nand fast PLBF# is up to 1.85 times and 1.83 times that for PLBF, respectively.\n6 Conclusion\nPLBF is an outstanding LBF that can effectively leverage the distribution of the set and\nqueries captured by a machine learning model. However, PLBF is computationally expen-\nsive to construct. We proposed fast PLBF and fast PLBF++ to solve this problem. Fast\nPLBF is superior to PLBF because fast PLBF constructs exactly the same data structure as\nPLBF but does so faster. Fast PLBF++ and fast PLBF# are even faster than fast PLBF\nand achieve almost the same accuracy as PLBF and fast PLBF. These proposed methods\nhave greatly expanded the range of applications of PLBF.\n26\n\nFast Construction of PLBF with Guarantees\nAcknowledgments and Disclosure of Funding\nThis work was supported by JST AIP Acceleration Research JPMJCR23U2, Japan. We\nappreciate the valuable feedback from the anonymous reviewers of NeurIPS, which has\nimproved the quality of our paper and led to the development of new methods and analyses.\nReferences\nAlok Aggarwal, Maria M Klawe, Shlomo Moran, Peter Shor, and Robert Wilber. Geometric\napplications of a matrix-searching algorithm. Algorithmica , 2(1):195‚Äì208, 1987.\nHyrum S Anderson and Phil Roth. Ember: An open dataset for training static pe malware\nmachine learning models. arXiv:1804.04637 , 2018.\nBurton H Bloom. Space/time trade-offs in hash coding with allowable errors. Communica-\ntions of the ACM , 13(7):422‚Äì426, 1970.\nAndrei Broder and Michael Mitzenmacher. Network applications of bloom filters: A survey.\nInternet Mathematics , 1(4):485‚Äì509, 2004.\nFay Chang, Jeffrey Dean, Sanjay Ghemawat, Wilson C Hsieh, Deborah A Wallach, Mike\nBurrows, Tushar Chandra, Andrew Fikes, and Robert E Gruber. Bigtable: A distributed\nstorage system for structured data. ACM Transactions on Computer Systems , 26(2):\n1‚Äì26, 2008.\nYifan Dai, Yien Xu, Aishwarya Ganesan, Ramnatthan Alagappan, Brian Kroth, Andrea\nArpaci-Dusseau, and Remzi Arpaci-Dusseau. From wisckey to bourbon: A learned index\nfor log-structured merge trees. In USENIX Symposium on Operating Systems Design and\nImplementation (OSDI) , 2020.\nZhenwei Dai and Anshumali Shrivastava. Adaptive learned bloom filter (ada-bf): Efficient\nutilization of the classifier with application to real-time information filtering on the web.\nInAdvances in Neural Information Processing Systems (NeurIPS) , 2020.\nPeter C. Dillinger and Stefan Walzer. Ribbon filter: Practically smaller than bloom and\nxor. arXiv:2103.02515 , 2021.\nJialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li, Hantian\nZhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann, et al. Alex: An\nupdatable adaptive learned index. In ACM SIGMOD International Conference on Man-\nagement of Data (SIGMOD) , 2020.\nBin Fan, Dave G Andersen, Michael Kaminsky, and Michael D Mitzenmacher. Cuckoo\nfilter: Practically better than bloom. In ACM International Conference on Emerging\nNetworking Experiments and Technologies (CoNEXT) , 2014.\nPaolo Ferragina and Giorgio Vinciguerra. The pgm-index: A fully-dynamic compressed\nlearned index with provable worst-case bounds. In International Conference on Very\nLarge Data Bases (VLDB) , 2020.\n27\n\nSato and Matsui\nShahabeddin Geravand and Mahmood Ahmadi. Bloom filter applications in network secu-\nrity: A state-of-the-art survey. Computer Networks , 57(18):4047‚Äì4064, 2013.\nMichael T Goodrich and Michael Mitzenmacher. Invertible bloom lookup tables. In Allerton\nConference on Communication, Control, and Computing (Allerton) , 2011.\nThomas Mueller Graf and Daniel Lemire. Xor filters: Faster and smaller than bloom and\ncuckoo filters. Journal of Experimental Algorithmics , 25:1‚Äì16, 2020.\nTu Gu, Kaiyu Feng, Gao Cong, Cheng Long, Zheng Wang, and Sheng Wang. The rlr-\ntree: a reinforcement learning based r-tree for spatial data. Proceedings of the ACM on\nManagement of Data , 1(1):1‚Äì26, 2023.\nChen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency es-\ntimation algorithms. In International Conference on Learning Representations (ICLR) ,\n2019.\nJohan Ludwig William Valdemar Jensen. Sur les fonctions convexes et les in¬¥ egalit¬¥ es entre\nles valeurs moyennes. Acta Mathematica , 30(1):175‚Äì193, 1906.\nGuolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and\nTie-Yan Liu. Lightgbm: A highly efficient gradient boosting decision tree. In Advances\nin Neural Information Processing Systems (NeurIPS) , 2017.\nTim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for\nlearned index structures. In ACM SIGMOD International Conference on Management\nof Data (SIGMOD) , 2018.\nPengfei Li, Hua Lu, Qian Zheng, Long Yang, and Gang Pan. Lisa: A learned index structure\nfor spatial data. In ACM SIGMOD International Conference on Management of Data\n(SIGMOD) , 2020.\nHonghao Lin, Tian Luo, and David Woodruff. Learning augmented binary search trees. In\nInternational Conference on Machine Learning (ICML) , 2022.\nGuanlin Lu, Young Jin Nam, and David HC Du. Bloomstore: Bloom-filter based memory-\nefficient key-value store for indexing of data deduplication on flash. In IEEE Symposium\non Mass Storage Systems and Technologies (MSST) , 2012.\nMichael Mitzenmacher. A model for learned bloom filters and optimizing by sandwiching.\nInAdvances in Neural Information Processing Systems (NeurIPS) , 2018.\nVikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. Learning multi-\ndimensional indexes. In ACM SIGMOD International Conference on Management of\nData (SIGMOD) , 2020.\nAnna Pagh, Rasmus Pagh, and S Srinivasa Rao. An optimal bloom filter replacement. In\nACM-SIAM Symposium on Discrete Algorithms (SODA) , 2005.\nAtsuki Sato and Yusuke Matsui. Fast partitioned learned bloom filter. In Advances in\nNeural Information Processing Systems (NeurIPS) , 2023.\n28\n\nFast Construction of PLBF with Guarantees\nManu Siddhartha. Malicious urls dataset. https://www.kaggle.com/datasets/\nsid321axn/malicious-urls-dataset , 2021. [Online; accessed 22-December-2022].\nSasu Tarkoma, Christian Esteve Rothenberg, and Eemil Lagerspetz. Theory and practice\nof bloom filters for distributed systems. IEEE Communications Surveys & Tutorials , 14\n(1):131‚Äì155, 2011.\nKapil Vaidya, Eric Knorr, Michael Mitzenmacher, and Tim Kraska. Partitioned learned\nbloom filters. In International Conference on Learning Representations (ICLR) , 2021.\nMinmei Wang and Mingxun Zhou. Vacuum filters: More space-efficient and faster replace-\nment for bloom and cuckoo filters. In International Conference on Very Large Data Bases\n(VLDB) , 2019.\nMeifan Zhang, Hongzhi Wang, Jianzhong Li, and Hong Gao. Learned sketches for frequency\nestimation. Information Sciences , 507:365‚Äì385, 2020.\nZhou Zhang, Zhaole Chu, Peiquan Jin, Yongping Luo, Xike Xie, Shouhong Wan, Yun Luo,\nXufei Wu, Peng Zou, Chunyang Zheng, et al. Plin: A persistent learned index for non-\nvolatile memory with high performance and instant recovery. In International Conference\non Very Large Data Bases (VLDB) , 2022.\n29\n\nSato and Matsui\nAppendix A. Solution of the Optimization Problems\nIn the original PLBF paper (Vaidya et al., 2021), the analysis with mathematical expres-\nsions was conducted only for the relaxed problem, while no analysis utilizing mathematical\nexpressions was performed for the general problem. Consequently, it was unclear which\ncalculations were redundant, leading to the repetition of constructing similar DP tables.\nHence, this appendix provides a comprehensive analysis of the general problem. It presents\nthe optimal solution and the corresponding value of the objective function using mathemat-\nical expressions. This analysis uncovers redundant calculations, enabling the derivation of\nfast PLBF.\nThe optimization problem designed by PLBF (Vaidya et al., 2021) can be written as\nfollows:\nminimize\nf,tkX\ni=1c|S|Gilog2\u00121\nfi\u0013\nsubject tokX\ni=1Hifi‚â§F\nt0= 0< t1< t2<¬∑¬∑¬∑< tk= 1\nfi‚â§1 ( i= 1,2, . . . , k ).(57)\nThe objective function represents the total memory usage of the backup Bloom filters. The\nfirst constraint equation represents the condition that the expected overall FPR is below F.\nc‚â•1 is a constant determined by the type of backup Bloom filters. GandHare determined\nbyt. Once tis fixed, we can solve this optimization problem to find the optimal fand the\nminimum value of the objective function. In the original PLBF paper (Vaidya et al., 2021),\nthe condition fi‚â§1 (i= 1,2, . . . , k ) was relaxed; however, here we conduct the analysis\nwithout relaxation.\nFirst, in the case ofP\ni‚àà{j|Gj>0}Hi‚â§F, the minimum value of the objective function\nis clearly 0. This is because if we set\nfi=(\n0 (Gi= 0)\n1 (Gi>0),(58)\nthe conditions of the optimization problem are satisfied, and the value of the objective\nfunction (which is always non-negative) becomes 0. In addition, even assuming that Hi=\n0‚áífi= 1, it has no effect on the solution to the optimization problem. This is because\neven if there is an ithat satisfies Hi= 0‚àßfi<1, the fthat can be obtained by changing\nfito 1 satisfies the conditions of the optimization problem and has the same or a smaller\nvalue for the objective function. Therefore, in the following, we will consider the case ofP\ni‚àà{j|Gj>0}Hi> F, and assume that Hi= 0‚áífi= 1.\nThe Lagrange function is defined using the Lagrange multipliers ¬µ‚ààRkandŒΩ‚ààR, as\nfollows:\nL(f,¬µ, ŒΩ) =kX\ni=1c|S|Gilog2\u00121\nfi\u0013\n+kX\ni=1¬µi(fi‚àí1) +ŒΩ kX\ni=1Hifi‚àíF!\n. (59)\n30\n\nFast Construction of PLBF with Guarantees\nFrom the KKT condition, there exist ¬Ø¬µand ¬ØŒΩin the local optimal solution ¬Øfof this\noptimization problem, and the following holds:\n‚àÇL\n‚àÇfi(¬Øf,¬Ø¬µ,¬ØŒΩ) = 0 ( i= 1,2, . . . , k ) (60)\n¬Øfi‚àí1‚â§0,¬Ø¬µi‚â•0,¬Ø¬µi(¬Øfi‚àí1) = 0 ( i= 1,2, . . . , k ) (61)\nkX\ni=1Hi¬Øfi‚àíF‚â§0,¬ØŒΩ‚â•0,¬ØŒΩ kX\ni=1Hi¬Øfi‚àíF!\n= 0. (62)\nBy introducing If=1={i|¬Øfi= 1}, Equations (60) to (62) can be organized as follows:\nc|S|Gi= ¬Ø¬µi+ ¬ØŒΩHi,¬Ø¬µi‚â•0,¬Øfi= 1 ( i‚àà If=1), (63)\nc|S|Gi=¬Øfi¬ØŒΩHi,¬Ø¬µi= 0,¬Øfi<1 ( i /‚àà If=1), (64)\nX\ni‚ààIf=1Hi+X\ni/‚ààIf=1Hi¬Øfi‚àíF‚â§0,¬ØŒΩ‚â•0,¬ØŒΩÔ£´\nÔ£≠X\ni‚ààIf=1Hi+X\ni/‚ààIf=1Hi¬Øfi‚àíFÔ£∂\nÔ£∏= 0.(65)\nHere, we can prove ¬Ø ŒΩ >0 by contradiction. Assuming ¬Ø ŒΩ= 0, Equation (64) implies that\n¬Øfi<1‚áíGi= 0; that is, Gi>0‚áí¬Øfi= 1. However, this does not satisfy the conditions\nof the optimization problem, asPk\ni=1Hi¬Øfi‚â•P\ni‚àà{j|Gj>0}Hi¬Øfi=P\ni‚àà{j|Gj>0}Hi> F.\nTherefore, we have ¬Ø ŒΩ >0.\nNoting that we are assuming Hi= 0‚áífi= 1, we obtain the following from equations\n¬ØŒΩ >0 and Equation (64):\n¬Øfi=c|S|\n¬ØŒΩGi\nHi(i /‚àà If=1). (66)\nFrom Equations (65) and (66) and ¬Ø ŒΩ >0, we obtain\nc|S|\n¬ØŒΩ=F‚àíP\ni‚ààIf=1Hi\n1‚àíP\ni‚ààIf=1Gi, (67)\nwhere Gf=1andHf=1are defined in Equation (5). Thus, we get\n¬Øfi=Ô£±\nÔ£¥Ô£≤\nÔ£¥Ô£≥1 ( i‚àà If=1)\n(F‚àíHf=1)Gi\n(1‚àíGf=1)Hi(i /‚àà If=1).(68)\nSubstituting this into the objective function of the optimization problem (Equation 3), we\nobtain\nkX\ni=1c|S|Gilog2\u00121\n¬Øfi\u0013\n=X\ni/‚ààIf=1‚àíc|S|Gilog2 F‚àíP\nj‚ààIf=1Hj\n1‚àíP\nj‚ààIf=1GjGi\nHi!\n(69)\n=c|S|(1‚àíGf=1) log2\u00121‚àíGf=1\nF‚àíHf=1\u0013\n‚àíc|S|X\ni/‚ààIf=1Gilog2\u0012Gi\nHi\u0013\n.\n(70)\n31\n\nSato and Matsui\nWe can also obtain the conditions of If=1. First, from the assumption,\nHi= 0‚áíi‚àà If=1. (71)\nUsing ¬Ø ŒΩ >0, c > 0,|S|>0, Equation (63) and Equation (67) derives\ni‚àà If=1‚àßHi>0‚áíGi\nHi‚â•¬ØŒΩ\nc|S|=1‚àíGf=1\nF‚àíHf=1, (72)\nand Equation (64) and Equation (67) derives\ni /‚àà If=1‚áíGi\nHi<¬ØŒΩ\nc|S|=1‚àíGf=1\nF‚àíHf=1. (73)\nAppendix B. Modification of the PLBF Framework\nIn this appendix, we describe the framework modifications we made to PLBF in our exper-\niments. In the original PLBF paper (Vaidya et al., 2021), the optimization problem was\ndesigned to minimize the amount of memory usage under a given target FPR (Equation 3).\nHowever, this framework makes it difficult to compare the results of different methods\nand hyperparameters. Therefore, we designed the following optimization problem, which\nminimizes the expected FPR under a given memory usage condition:\nminimize\nf,tkX\ni=1Hifi\nsubject tokX\ni=1c|S|Gilog2\u00121\nfi\u0013\n‚â§M\nt0= 0< t1< t2<¬∑¬∑¬∑< tk= 1\nfi‚â§1 ( i= 1,2, . . . , k ),(74)\nwhere Mis a parameter that is set by the user to determine the upper bound of memory\nusage.\nIntroducing If=1for analysis as we have done in Appendix A, it follows that the optimal\nFPRs ¬Øfis\n¬Øfi=Ô£±\nÔ£≤\nÔ£≥1 ( i‚àà If=1)\n2‚àíŒ≤Gi\nHi(i /‚àà If=1), (75)\nwhere\nŒ≤=M\nc|S|(1‚àíGf=1)+1\n1‚àíGf=1X\ni/‚ààIf=1Gilog2\u0012Gi\nHi\u0013\n. (76)\nWhen f=¬Øf, the expected FPR, that is, the objective function of Equation (74) is\nHf=1+ 2‚àíŒ≤(1‚àíGf=1). (77)\nTherefore, as in the original framework, we can find the best tandfby finding a way to\ncluster the 1st to ( j‚àí1)-th segments into k‚àí1 regions that maximizesPk‚àí1\ni=1Gilog2(Gi/Hi)\nfor each j=k, k+1, . . . , N . We can find it using the same DP algorithm and its acceleration\nmethods as in the original framework.\n32",
  "textLength": 70607
}