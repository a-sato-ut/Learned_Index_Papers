{
  "paperId": "a11cf091955f78e594acb8e18d01d4e9537ca99c",
  "title": "Online Sketch-based Query Optimization",
  "pdfPath": "a11cf091955f78e594acb8e18d01d4e9537ca99c.pdf",
  "text": "Online Sketch-based Query Optimization\nYesdaulet Izenov, Asoke Datta, Florin Rusu, Jun Hyung Shin\nfyizenov,adatta2,frusu,jshin33 g@ucmerced.edu\nUniversity of California Merced\nFebruary 2021\nAbstract\nCost-based query optimization remains a critical task in relational databases even after decades of research and\nindustrial development. Query optimizers rely on a large range of statistical synopses â€“ including attribute-level\nhistograms and table-level samples â€“ for accurate cardinality estimation. As the complexity of selection predicates\nand the number of join predicates increase, two problems arise. First, statistics cannot be incrementally composed\nto effectively estimate the cost of the sub-plans generated in plan enumeration. Second, small errors are propagated\nexponentially through join operators, which can lead to severely sub-optimal plans.\nIn this paper, we introduce COMPASS, a novel query optimization paradigm for in-memory databases based on\na single type of statisticsâ€”Fast-AGMS sketches. In COMPASS, query optimization and execution are intertwined.\nSelection predicates and sketch updates are pushed-down and evaluated online during query optimization. This allows\nFast-AGMS sketches to be computed only over the relevant tuplesâ€”which enhances cardinality estimation accuracy.\nPlan enumeration is performed over the query join graph by incrementally composing attribute-level sketchesâ€”not\nby building a separate sketch for every sub-plan.\nWe prototype COMPASS in MapD â€“ an open-source parallel database â€“ and perform extensive experiments over\nthe complete JOB benchmark. The results prove that COMPASS generates better execution plans â€“ both in terms of\ncardinality and runtime â€“ compared to four other database systems. Overall, COMPASS achieves a speedup ranging\nfrom 1.35X to 11.28X in cumulative query execution time over the considered competitors.\n1 INTRODUCTION\nConsider query 6a from the JOB benchmark [30]:\nSELECT MIN(k.keyword), MIN(n.name), MIN(t.title)\nFROM cast info ci, keyword k, movie keyword mk, name n, title t\nWHERE\n.selection predicates\nk.keyword = â€™marvel-cinematic-universeâ€™ AND\nn.name LIKE â€™%Downey%Robert%â€™ AND t.production year > 2010 AND\n.join predicates\nk.id = mk.keyword idAND t.id = mk.movie idAND t.id = ci.movie idAND\nci.movie id = mk.movie idAND n.id = ci.person id\nThe query has 3 selection predicates â€“ point, subset, and range â€“ and joins 5 tables with 5 join predicatesâ€”there is a\ntriangle subquery between tables t,mk, and ci. The corresponding join graph is depicted in Figure 1. For each join,\nthe graph contains a named edge e1â€“e5that connects the tables involved in the join predicate. For example, edge e1\nrepresents the join predicate k.id = mk.keyword id.\nFigure 1 also includes the execution plans together with their cost â€“ the total cardinality of the intermediate results\nâ€“ for COMPASS and the four other databases considered in the paper. Although all the plans are left-deep trees, their\ncost ranges from 1;249to215millions tuples. This is entirely due to the statistics used for cardinality estimation.\nMapD [76] does not use any statistics, thus its cost is orders of magnitude higher. The plan is determined by sorting\n1arXiv:2102.02440v1  [cs.DB]  4 Feb 2021\n\nğœğ‘›\nâ‹ˆ\nkmkci nâ‹ˆâ‹ˆâ‹ˆ\n(14)(1242)(6)(6)\nğœğ‘˜tğœğ‘¡COMP ASS\nCost =1262â‹ˆ\ntmkci nâ‹ˆâ‹ˆ\n(17ğ‘€)(1194)(6)\nğœğ‘¡ğœğ‘› kğœğ‘˜\n(300ğ¾)MonetDB\nâ‹ˆ\nCost â‰ˆ17ğŒâ‹ˆ\nmkğœğ‘› tâ‹ˆâ‹ˆ\n(215ğ‘€)(10ğ¾)(1194)(6)\nciğœğ‘¡ kğœğ‘˜\nnâ‹ˆMapD\nâ‹ˆ\nkmkğœğ‘¡\ntâ‹ˆâ‹ˆ\n(14)(11)(1224)(6)\nğœğ‘˜ci nğœğ‘›\nâ‹ˆPostgr eSQL,\nDBMS A\nCost =1249 Cost â‰ˆ215ğŒcast_info\nğœğ¢id\nperson_ide5id\nmovie_ide3\nmovie_idmovie_id\ne4title\nğ­name\nğ§keyword\nğ¤\nmovie_keyword\nğ¦ğ¤id\nmovie_ide2id\nkeyword_ide1Figure 1: Join graph and corresponding execution plans for query JOB 6a. The numbers represent cardinality.\nthe tables in decreasing order of their sizeâ€”number of tuples. MonetDB [77] has a rule-based optimizer with mini-\nmum support for statistics [19] which generates a better plan. The reason why both of these systems have primitive\noptimizers is because they are relatively â€œyoungâ€ and are targeted at modern architectures. They try to compensate\nbad plans with highly-optimized execution engines that make use of extensive in-memory processing supported by\nmassive multithread parallelism and vectorized instructions. However, this approach is clearly limited.\nPostgreSQL [78] and the industrial-grade DBMS A â€“ name anonymized for legal reasons â€“ are â€œmatureâ€ databases\nwith advanced query optimizers. In order to ï¬nd the much better plan, they use a large variety of statistics. Histograms,\nmost frequent values, and number of distincts are used to estimate the selectivity of the point predicate on attribute\nk.keyword and of the range predicate on t.production year. The subset LIKE predicate on n.name is estimated with\ntable-level samples. Estimating join cardinality requires correlated statistics on the join attributes. While such statistics\nexist, e.g., correlated samples [22, 62, 29], they require the existence of indexes on every join attribute combination,\nwhich severely limits their applicability in the case of multi-way joins. As a result, even advanced optimizers rely on\ncrude formulas that assume uniformity, inclusion, and independenceâ€”which are likely to produce highly sub-optimal\nexecution plans [28]. Since implementing and maintaining these many statistics requires considerable effort, it is\ncompletely understandable that only mature systems implement them.\nProblem. We investigate how to design a lightweight â€“ yet effective â€“ query optimizer for modern in-memory\ndatabases. We have two design principles. First, we aim to capitalize on the highly-parallel execution engine in the\nquery optimization process. Since query execution is already fast, it is challenging to minimize the overhead incurred\nby the additional optimization. Second, the type and number of synopses included in the optimizer has to be minimal.\nOur goal is to employ a single type of synopsis built exclusively for single-attributes and without the requirement of\nadditional data structures such as indexes. The challenge is to design a composable â€“ and consistent â€“ synopsis that\nprovides incremental cardinality estimates for the sub-plans generated in plan enumeration.\nCOMPASS query optimizer. We introduce the online sketch-based COMPASS query optimizer. Fast-AGMS\nsketches [7] are the only statistics present in COMPASS. These sketches are a type of correlated synopses for join\ncardinality estimation [47, 49] that use small space, can be computed efï¬ciently in a single scan over the data, are\nlinearly composable, and â€“ more importantly â€“ have statistically high accuracy. These properties allow for Fast-\nAGMS sketches to be computed online in COMPASS by leveraging the optimized parallel execution engine in modern\ndatabases. This is realized by decomposing query processing into two stages performed before and after the optimiza-\ntion. In the ï¬rst stage, selection predicates are pushed-down and Fast-AGMS sketches are built concurrently only\nover the relevant tuples. Sketches are built for each two-way join independentlyâ€”not for every combination of tables.\nIn the query optimization stage, plan enumeration is performed over the join graph by incrementally composing the\ncorresponding two-way join sketches in order to estimate the cardinality of multi-way joins. The optimal join ordering\nis ï¬nally passed to the execution engine to ï¬nalize the query. As shown in Figure 1, COMPASS identiï¬es a plan as\ngood as PostgreSQL and DBMS A, while relying exclusively on a single synopsisâ€”Fast-AGMS sketches. In addition\nto the novel query optimization paradigm, we make the following technical contributions:\nâ€¢ We present a systematic approach of using sketches for join cardinality estimation in a query optimizer. This includes\ntwo-way and multi-way joins. We do this for two types of sketchesâ€”AGMS [1] and Fast-AGMS [7].\n2\n\nâ€¢ We introduce two novel strategies to extend Fast-AGMS sketches to multi-way join cardinality estimation. The\nï¬rst strategy â€“ sketch partitioning â€“ is a theoretically sound estimator for a given multi-way join. Since it does not\nsupport composition, sketch partitioning is not scalable for join order enumeration. The second strategy â€“ sketch\nmerging â€“ addresses scalability by incrementally creating multi-way sketches from two-way sketches. Although\nthis is done heuristically for a certain multi-way join taken separately, all the multi-way joins with a given size are\nequally impacted. This property guarantees estimation consistency in plan enumeration.\nâ€¢ We prototype COMPASS in MapD and perform extensive experiments over the complete JOB benchmarkâ€”113\nqueries. The results prove the reduced overhead COMPASS incurs â€“ below 500 milliseconds â€“ while generating\nsimilar or better execution plans compared to the four databases systems included in Figure 1. COMPASS outper-\nforms the other databases both in terms of the number of queries it obtains the best result on, as well as on the\ncumulative workload execution time.\nOutline. The paper is organized as follows. Background information on cost-based query optimization and sketches\nis given in Section 2. A high-level overview of COMPASS is presented in Section 3, followed by the technical details\nof sketch-based cardinality estimation in Section 4. The novel Fast-AGMS sketches for multi-way joins are introduced\nin Section 5. In Section 6, we show how the sketches are integrated in a typical enumeration algorithm. The empirical\nevaluation of COMPASS is detailed in Section 7. We discuss related work in Section 8 and conclude with future work\ndirections in Section 9.\n2 PRELIMINARIES\nCost-based query optimization. The query optimization problem [67, 30, 28, 6] consists in ï¬nding the best exe-\ncution plan â€“ which typically corresponds to the one with the fastest execution time â€“ for a given query. The search\nspace is deï¬ned over all the valid plans â€“ combinations of relational algebra operators â€“ which can answer the query\ncorrectly. The number of potential plans is exponentially factorial in the number of tables. Thus, inspecting all of\nthem is not practical for a large number of tables. Plan enumeration is the procedure that deï¬nes the plans in the\nsearch space. Since the execution time of a plan cannot be determined without running it â€“ which defeats the purpose\nâ€“ alternative cost functions are deï¬ned. The most common cost function is the total size â€“ or cardinality â€“ of the inter-\nmediate results produced by all the operators in the plan . This function captures the correlation between the amount\nof accessed data and execution timeâ€”which is true in general. Computing the cardinality of a relational algebra op-\nerator is itself a difï¬cult problem and requires knowledge about the data on which the operator is performed. This\nknowledge is captured by incomplete statistics â€“ or synopses â€“ about the data. Different classes of statistics [8] are\nuseful for different relational operators. For example, attribute histograms and number of distinct values are optimal\nfor selection predicates, while correlated samples are better for join predicates. With statistics, the cardinality can\nonly be estimatedâ€”it is not exact. While accurate for simple predicates over a small number of attributes, cardinality\nestimation becomes harder for correlated predicates and multi-way joins. This is not necessarily a problem if all the\nplans are equally impacted. However, estimation errors vary widely across sub-plans and this can potentially lead to\na highly suboptimal plan. The COMPASS query optimizer includes solutions both for effective plan enumeration as\nwell as incremental cardinality estimation for the enumerated sub-plans.\nParallel in-memory databases. Database systems for modern computing architectures rely on extensive in-memory\nprocessing supported by massive multithread parallelism and vectorized instructions. GPUs represent the pinnacle of\nsuch architectures, harboring thousands of SMT threads which execute tens of vectorized SIMD instructions simulta-\nneously. MapD, Ocelot [70], CoGaDB [74], Kinetica [75], and Brytlyt [72] are a few examples of modern in-memory\ndatabases with GPU support. They provide relational algebra operators and pipelines for GPU architectures [16, 4, 13]\nthat optimize memory access and bandwidth. This results in considerable performance improvement for certain classes\nof queries. However, these databases provide only primitive rule-based query optimizationâ€”if at all. This limits dras-\ntically their applicability to general workloads. In COMPASS, we leverage the optimized execution engine of MapD\nto build a lightweight â€“ yet accurate and general â€“ query optimizer based on a single type of synopsis.\n3\n\nSketches. Sketch synopses [8] summarize the tuples of a relation as a set of random values. This is accomplished by\nprojecting the domain of the relation on a signiï¬cantly smaller domain using random functions or seeds. In the case\nof join attributes, correlation between attributes is maintained by using the same random function. While sketches\ncompute only approximate results with probabilistic guarantees, they satisfy several major requirements of a query\noptimizer for in-memory databasesâ€”single-pass computation, small space, fast update and query time, and linearity:\nâ€¢ A sketch is built by streaming over the input data and considers each tuple at most once.\nâ€¢ A basic sketch is composed of a single counter and one or more random seedsâ€”a few bytes. In order to improve\naccuracy, a standard method is to use multiple independent basic sketch instances. The number of instances is\nderived from the desired accuracy and conï¬dence levels. In practice, very good accuracy can be achieved with\nsketches having size in kilobytes.\nâ€¢ The update of a sketch with a new tuple consists in generating one or more random numbers and adding them to the\nsketch counter. The answer to a query involves simple arithmetic operations on the sketch. In the case of multiple\nsketches, both the update and query are applied to all the instances. Overall, update and query time are linearly\nproportional with the sketch size.\nâ€¢ A sketch can be computed by partitioning the input relation into multiple parts, building a sketch for every part,\nand then merging the partial sketches. This mergeable property makes sketches amenable for parallel processing on\nmodern hardware and can result in linear speedups in update and query time [44, 53].\nWhile previous work addresses how to apply sketches to certain cardinality estimation problems that occur in query\noptimization, COMPASS is a complete query optimizer based exclusively on sketches. In addition to cardinality\nestimation, we show how to integrate the sketch estimations in plan enumeration. We are not aware of any work that\nintegrates sketches effectively with plan enumeration. This is the main reason why sketches have not been integrated\nin a query optimizer before. COMPASS solves this problem.\n3 COMPASS: ONLINE SKETCH-BASED QUERY OPTIMIZATION\nIn this section, we provide a high-level description of the COMPASS query optimization paradigm, while the technical\ndetails of cardinality estimation, join ordering, and plan enumeration are presented in Section 4, 5, and 6, respectively.\nWorkï¬‚ow. The workï¬‚ow performed by the COMPASS query optimizer is depicted in Figure 2. It consists of a two-\nstep process that requires interaction with the query processor. First, the optimizer extracts the selection predicates\nand join attributes for every table. A sketch is built for every join attribute while performing the selection query\non the base table, and only over the tuples that satisfy the predicate. Figure 2 shows the procedure for table title\nwhich has a range predicate and two join conditionsâ€”although both join predicates involve the same attribute t.id,\ntwo independent sketches have to be built. COMPASS leverages the high-parallelism of in-memory databases and the\nmergeable property of sketches to execute this process with minimal overhead. Two additional optimizations can be\napplied to further reduce the overhead. Sketches for join attributes from tables without selection predicates can be\nbuilt ofï¬‚ine and plugged-in directly. Sketches can be built only over a sample [46], which, however, incurs a decrease\nin accuracy. In the second step of the workï¬‚ow, plan enumeration is performed by estimating the cardinality of all\nthe sub-plans using the sketches built in the ï¬rst step. This is possible only because the attribute-level sketches we\ndesign are incrementally composable. Otherwise, separate sketches have to be built for every enumerated sub-plan. In\nour example, there are two sketches on attribute t.id, one for join e2and one for join e3in the join graph (Figure 1).\nThe sketch for e2is included in all the sub-plans that contain this join attributeâ€”similar for e3. In a sub-plan that\nincludes both e2ande3, these two sketches are ï¬rst merged and then used in estimation as before. This process is\nperformed incrementally during plan enumeration. Finally, the optimal plan is submitted for execution together with\nany materialized intermediates from step one.\nPartitioned query execution. As shown in Figure 2, COMPASS intertwines query optimization and evaluation by\npartitioning execution into push-down selection (step 1) and join computation (step 3). Query optimization, i.e., join\nordering plan enumeration, is performed in-between these two stages. Since plan enumeration and join computation\nare standard, we focus on push-down selection, where online sketch building is performed. Push-down selection\n4\n\nSELECT  COUNT(*)\nFROM  title AS t\nWHERE  t.production_year > 2010ğğ®ğ¬ğ¡-ğƒğ¨ğ°ğ§ ğ’ğğ¥ğğœğ­ğ¢ğ¨ğ§      \n          MIN(k.keyword), MIN(n.name), MIN(t.title)\n      \n          cast_info AS ,\n          keyword AS ,\n          movie_keyword AS ,\n          name AS ,\n          title AS \n      \n         k.keyword = 'marvel-cinematic-universe' \n         n.name LIKE '%Downey%Robert%' \n         t.production_year > 2010  \n         k.id = mk.keyword_id \n         t.id = mk.movie_id \n         t.id = ci.movie_id \n         ci.movie_id = mk.movie_id \n         n.id = ci.person_idğ’ğ„ğ‹ğ„ğ‚ğ“\nğ…ğ‘ğğŒ\nğœğ¢\nğ¤\nğ¦ğ¤\nğ§\nğ­\nğ–ğ‡ğ„ğ‘ğ„\nğ€ğğƒ\nğ€ğğƒ\nğ€ğğƒ\nğ€ğğƒ\nğ€ğğƒ\nğ€ğğƒ\nğ€ğğƒSketch Build\nt.id (e2)\nt.id (e3)exact cardinalities\nsketches\nmaterialized intermediatesQueries on base tables\nOptimal planğğ¥ğšğ§ ğ„ğ§ğ®ğ¦ğğ«ğšğ­ğ¢ğ¨ğ§\nğ‘›\nâ‹ˆ\nğ‘šğ‘˜ğ‘ğ‘–â‹ˆâ‹ˆâ‹ˆ\nğ‘˜ğ‘¡QUER Y PROCESSOR\nğœğ¢ğ­ ğ§ ğ¤\nğ¦ğ¤ğ‚ğšğ«ğğ¢ğ§ğšğ¥ğ¢ğ­ğ² ğ„ğ¬ğ­ğ¢ğ¦ğšğ­ğ¢ğ¨ğ§1\n3COMP ASS QUER Y OPTIMIZER\n2Figure 2: COMPASS workï¬‚ow: online sketch-based query optimization for in-memory databases.\ncomputes the exact selectivity cardinalities for all the base tables that have selections. This is similar to the ESC\napproach introduced in [51]. However, in addition to predicate evaluation, COMPASS also builds sketches for every\njoin attribute in the table by piggybacking on the same traversalâ€”sketch building is performed during the selection.\nNotice that this works both for sequential and index scans. It is important to emphasize that only the tuples that satisfy\nthe predicate are included in the sketch, which increases their accuracy signiï¬cantly. Moreover, the sketch update\noverhead is kept to the minimum necessary. While the exact cardinalities and sketches are always materialized due to\ntheir reduced size and role in optimization, the decision to materialize the selection output â€“ the intermediate result\nâ€“ depends on its size. COMPASS follows the same approach as in [51]. If the intermediate size is smaller than a\nthreshold, it is materialized. Otherwise, it is not, since the space reduction does not compensate for the access time\nreduction. Notice, though, that, even when intermediates are not materialized, sketches still contain only the relevant\ntuples for join cardinality estimation.\nWhile the idea of partitioned query execution for XML processing is introduced in ROX [22], the COMPASS\napproach is different in several aspects. First, similar to adaptive query processing [9], COMPASS works for relational\ndata and operators. However, COMPASS does not change the plan while the query is executing. This is not necessary\nbecause the sketch-based optimization strategy ï¬nds better plans in the ï¬rst place. ROX can decompose a join graph\ninto an arbitrary number of stages, each of which requiring materialization. COMPASS, on the other hand, splits\nexecution in exactly two stages and intermediate result materialization is only optional. The reason ROX requires\nmaterialization is because it uses chain sampling to estimate cardinalities. In order to provide acceptable accuracy,\nsamples have to be extracted from the most recent intermediate resultsâ€”not the base tables. Moreover, ROX chain\nsampling requires indexes on all the join attributes to guarantee a minimum sample size. This is a stringent constraint\nhardly satisï¬ed in most real-world databases. Sketches, on the other hand, do not impose any constraints. Lastly, due\nto its incremental greedy exploration of the join order space, ROX considers only a limited number of plansâ€”possibly\nsub-optimal. In COMPASS, plan enumeration is performed at once after push-down selection and can cover any\nportion of the join space. This can be achieved with the base table sketches which can be composed without the risk\nto become emptyâ€”the case for chain sampling.\nPlan enumeration. The join attribute-level sketches computed during push-down selection can be composed to\nestimate the cardinality of any valid join order â€“ excluding cross products â€“ generated during plan enumeration.\nIn most cases, cross products are ignored by join enumeration algorithms anyway [29]. As shown in Section 5,\nsketch composition consists of two stages. First, the sketches of all the relevant join attributes in a table are merged\ntogether. An attribute is relevant for a partial join order if its join is part of the order. Second, the sketches across\ntables are combined to estimate the cardinality of the join order. Since the overall composition consists only of\narithmetic operations, sketches can be integrated into any enumeration algorithmâ€”exhaustive, bushy, or left-deep.\nEssentially, sketches can readily replace the standard join cardinality estimation formula based on table and join\nattribute distinct cardinality [14]. However, since sketches capture the correlation between join attributes and do not\nmake the independence and containment assumptions, their accuracy is expected to be better.\n5\n\nSketches vs. other synopses. The decision to exclusively use sketches in COMPASS may seem questionable given\nthat sketches are designed for speciï¬c stream processing tasks, while traditional databases support generic batch-\noriented execution. To put it differently, there is a speciï¬c sketch for every streaming query, while synopses are for the\nentire database. To achieve generality, COMPASS has to build a set of sketches for every queryâ€”except base tables\nwithout predicates. However, this is done concurrently with push-down selection and is highly-parallel, resulting\nin low overhead (Section 7). As a result, sketches do not require any maintenance under modiï¬cation operations\nsince they are built on the current data. This is not possible for any of the other database synopses. The beneï¬t of\nhaving query-speciï¬c synopses is also exploited in [29], where index-based join sampling â€“ a variation of ROX chain\nsampling [22] â€“ is introduced. Index-based join sampling is performed during the plan enumeration of every query\nunder the corresponding selection predicates. Since the sample size â€“ both minimum and maximum â€“ is carefully\ncontrolled, index-based join sampling has improved memory usage and accuracy because it avoids empty results.\nCompared to sketches, though, this sampling strategy has two serious shortcomings. First, it requires the existence\nof an index and complete frequency distribution on every join attribute. Sketches require nothing beyond the data.\nSecond, the estimation of every join cardinality requires separate sampling from each of the involved tables. Since\nthis process is time-consuming, plan enumeration is performed bottom-up â€“ or breadth-ï¬rst â€“ in a limited time budget.\nSketches can be composed incrementally in any order, without the need to access the data.\nThe other types of synopses â€“ histograms and distinct cardinality â€“ are not query-speciï¬c. Thus, they do not incur\nany creation overhead during optimization. To estimate join cardinality, the attribute-level instances of these synopses\nare composed by simple arithmetic operations [50, 14]. However, due to the strong assumptions â€“ uniformity, inde-\npendence, inclusion, ad-hoc constants â€“ made by these operations, the estimates can be highly-inaccurate. Sketches\ndo not make any of these assumptions because they capture correlations by design.\nâŠ•(ğ‘¡.ğ‘–ğ‘‘=70)âŠ™ ğœ‰ğ‘’2\n1,2\n(ğ‘¡.ğ‘–ğ‘‘=70)âŠ™ ğœ‰ğ‘’3\n1,2\n(ğ‘¡.ğ‘ğ‘Ÿğ‘œğ‘‘ğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘›_ğ‘¦ğ‘’ğ‘ğ‘Ÿ>2010) ğœ‰ğ‘3\n1,2\ne1 e2\ne4e3 e5(ğ‘˜.ğ‘–ğ‘‘=95)âŠ™ ğœ‰ğ‘’1\n2,1\n(ğ‘˜.ğ‘˜ğ‘’ğ‘¦ğ‘¤ğ‘œğ‘Ÿğ‘‘=ğ¦ğœ) ğœ‰ğ‘1\n2,1â€²ğ®â€²\nâŠ• LIKE (ğ‘›.ğ‘–ğ‘‘=27)âŠ™ ğœ‰ğ‘’5\n2,ğ‘\n(ğ‘›.ğ‘›ğ‘ğ‘šğ‘’ ğœ‰ğ‘2\n2,ğ‘%ğƒ%ğ‘ )â€²%â€²\nâŠ•\n(ğ‘šğ‘˜.ğ‘˜ğ‘’ğ‘¦ğ‘¤ğ‘œğ‘Ÿğ‘‘_ğ‘–ğ‘‘=20)âŠ™ ğœ‰ğ‘’1\nğ‘Ÿ,2\n(ğ‘šğ‘˜.ğ‘šğ‘œğ‘£ğ‘–ğ‘’_ğ‘–ğ‘‘=7)âŠ™ ğœ‰ğ‘’2\nğ‘Ÿ,2\n(ğ‘šğ‘˜.ğ‘šğ‘œğ‘£ğ‘–ğ‘’_ğ‘–ğ‘‘=7) ğœ‰ğ‘’4\nğ‘Ÿ,2(ğ‘ğ‘–.ğ‘šğ‘œğ‘£ğ‘–ğ‘’_ğ‘–ğ‘‘=20)âŠ™ ğœ‰ğ‘’3\nğ‘Ÿ,1\n(ğ‘ğ‘–.ğ‘šğ‘œğ‘£ğ‘–ğ‘’_ğ‘–ğ‘‘=20)âŠ™ ğœ‰ğ‘’5\nğ‘Ÿ,1\n(ğ‘ğ‘–.ğ‘ğ‘’ğ‘Ÿğ‘ ğ‘œğ‘›_ğ‘–ğ‘‘=22) ğœ‰ğ‘’4\nğ‘Ÿ,1â›\nââœ\nâœ\nâœ\nâœ\nâœğ‘ ğ‘˜1,1\nğ¬ğ¤2,1\nâ‹®\nğ‘ ğ‘˜ğ‘Ÿ,1ğ‘ ğ‘˜1,2\nğ‘ ğ‘˜2,2\nâ‹®\nğ‘ ğ‘˜ğ‘Ÿ,2â‹¯\nâ‹¯\nâ‹±\nâ‹¯ğ‘ ğ‘˜1,ğ‘\nğ‘ ğ‘˜2,ğ‘\nâ‹®\nğ‘ ğ‘˜ğ‘Ÿ,ğ‘â\nâ âŸ\nâŸ\nâŸ\nâŸ\nâŸ\nğ‘ŸÃ—ğ‘ğ¬ğ¤ğ¤â›\nââœ\nâœ\nâœ\nâœ\nâœğ‘ ğ‘˜1,1\nğ‘ ğ‘˜2,1\nâ‹®\nğ‘ ğ‘˜ğ‘Ÿ,1ğ¬ğ¤1,2\nğ‘ ğ‘˜2,2\nâ‹®\nğ‘ ğ‘˜ğ‘Ÿ,2â‹¯\nâ‹¯\nâ‹±\nâ‹¯ğ‘ ğ‘˜1,ğ‘\nğ‘ ğ‘˜2,ğ‘\nâ‹®\nğ‘ ğ‘˜ğ‘Ÿ,ğ‘â\nâ âŸ\nâŸ\nâŸ\nâŸ\nâŸ\nğ‘ŸÃ—ğ‘ğ¬ğ¤ğ­â›\nââœ\nâœ\nâœ\nâœ\nâœğ‘ ğ‘˜1,1\nğ‘ ğ‘˜2,1\nâ‹®\nğ‘ ğ‘˜ğ‘Ÿ,1ğ‘ ğ‘˜1,2\nğ‘ ğ‘˜2,2\nâ‹®\nğ‘ ğ‘˜ğ‘Ÿ,2â‹¯\nâ‹¯\nâ‹±\nâ‹¯ğ‘ ğ‘˜1,ğ‘\nğ¬ğ¤2,ğ›\nâ‹®\nğ‘ ğ‘˜ğ‘Ÿ,ğ‘â\nâ âŸ\nâŸ\nâŸ\nâŸ\nâŸ\nğ‘ŸÃ—ğ‘ğ¬ğ¤ğ§\nâ›\nââœ\nâœ\nâœ\nâœ\nâœğ‘ ğ‘˜1,1\nğ‘ ğ‘˜2,1\nâ‹®\nğ‘ ğ‘˜ğ‘Ÿ,1ğ‘ ğ‘˜1,2\nğ‘ ğ‘˜2,2\nâ‹®\nğ¬ğ¤ğ«,2â‹¯\nâ‹¯\nâ‹±\nâ‹¯ğ‘ ğ‘˜1,ğ‘\nğ‘ ğ‘˜2,ğ‘\nâ‹®\nğ‘ ğ‘˜ğ‘Ÿ,ğ‘â\nâ âŸ\nâŸ\nâŸ\nâŸ\nâŸ\nğ‘ŸÃ—ğ‘ğ¬ğ¤ğ¦ğ¤â›\nââœ\nâœ\nâœ\nâœ\nâœğ‘ ğ‘˜1,1\nğ‘ ğ‘˜2,1\nâ‹®\nğ¬ğ¤ğ«,1ğ‘ ğ‘˜1,2\nğ‘ ğ‘˜2,2\nâ‹®\nğ‘ ğ‘˜ğ‘Ÿ,2â‹¯\nâ‹¯\nâ‹±\nâ‹¯ğ‘ ğ‘˜1,ğ‘\nğ‘ ğ‘˜2,ğ‘\nâ‹®\nğ‘ ğ‘˜ğ‘Ÿ,ğ‘â\nâ âŸ\nâŸ\nâŸ\nâŸ\nâŸ\nğ‘ŸÃ—ğ‘ğ¬ğ¤ğœğ¢â›\nââœ\nâœ\nâœ\nâœ\nâœAvg{ğ‘  â‹…ğ‘  â‹…ğ‘  â‹…ğ‘  â‹…ğ‘  ğ‘˜ğ‘˜\n1,ğ‘–ğ‘˜ğ‘¡\n1,ğ‘–ğ‘˜ğ‘›\n1,ğ‘–ğ‘˜ğ‘šğ‘˜\n1,ğ‘–ğ‘˜ğ‘ğ‘–\n1,ğ‘–}ğ‘\nğ‘–=1\nAvg{ğ‘  â‹…ğ‘  â‹…ğ‘  â‹…ğ‘  â‹…ğ‘  ğ‘˜ğ‘˜\n2,ğ‘–ğ‘˜ğ‘¡\n2,ğ‘–ğ‘˜ğ‘›\n2,ğ‘–ğ‘˜ğ‘šğ‘˜\n2,ğ‘–ğ‘˜ğ‘ğ‘–\n2,ğ‘–}ğ‘\nğ‘–=1\nâ‹®\nAvg{ğ‘ â‹…ğ‘ â‹…ğ‘ â‹…ğ‘  â‹…ğ‘  ğ‘˜ğ‘˜\nğ‘Ÿ,ğ‘–ğ‘˜ğ‘¡\nğ‘Ÿ,ğ‘–ğ‘˜ğ‘›\nğ‘Ÿ,ğ‘–ğ‘˜ğ‘šğ‘˜\nğ‘Ÿ,ğ‘–ğ‘˜ğ‘ğ‘–\nğ‘Ÿ,ğ‘–}ğ‘\nğ‘–=1â\nâ âŸ\nâŸ\nâŸ\nâŸ\nâŸ\nğ‘ŸÃ—1Median\nâŠ• âŠ•\nFigure 3: Cardinality estimation for query JOB 6a with AGMS sketches.\n6\n\n4 SKETCH CARDINALITY ESTIMATION\nIn this section, we present how the class of AGMS sketches are applied for estimating the cardinality of complex\nqueries involving selection predicates and multi-way joins. We organize the presentation around the original AGMS\nsketches [1] which have known solutions to these problems. However, AGMS sketches are too inefï¬cient to be\naccurate and cannot be integrated in query plan enumeration. This leads us to the Fast-AGMS sketches [7] which are\nasymptotically more efï¬cient and have been shown to be statistically more accurate [47, 49]. However, Fast-AGMS\nsketches are limited to estimating two-way join cardinality. Our main contributions are to extend Fast-AGMS sketches\nto multi-way joins and to effectively integrate them in query plan enumeration.\n4.1 AGMS Sketches\nThe basic AGMS sketch of an attribute from a relation consists of a single random value skthat summarizes the values\nof the attribute across all the tuples in the relation. For example, all the values of attribute idfrom table keyword can\nbe summarized by a sketch sk(k:id)computed as sk(k.id) =P\nt2k\u0018(t.id), where\u0018is a family off+1;\u00001grandom\nvariables that are 4-wise independent. Essentially, a random value of either +1or\u00001is associated to each point in\nthe domain of attribute k.id. Then, the corresponding random value is added to the sketch sk(k:id)â€“ initialized to 0\nâ€“ for each tuple tin table keyword . Intuitively, the more frequent a value is, the more is â€œpullingâ€ the sketch to its\nfrequency. Since all the tuples are combined in the same sketch sk(k:id), they are conï¬‚icting and the output can be\nfar away from the frequency of each single tuple. This is where the 4-wise independence property of \u0018is important.\nIt guarantees that for any group of at most 4 different values of attribute k.id, the product of their corresponding \u0018\nvalues is 0on expectationâ€”they cancel out. This, in turn, allows for each individual attribute value frequency to be\nunbiasedly estimated by multiplying the sketch with the corresponding \u0018random value. For example, the frequency\nofk:id= 5is estimated by the product sk(k:id)\u0001\u0018(5).\n4.1.1 Two-Way Join Cardinality Estimation\nConsider the join e1between tables keyword andmovie keyword with predicate k.id = mk.keyword id(Figure 1). The\ncardinality of this join operator can be estimated with two AGMS sketches sk(k:id)andsk(mk:keyword id)built\non the join attributes. The requirement is that these sketches share the same family \u0018of random variablesâ€” \u0018e1is\nassociated with edge e1.\u0018e1guarantees that join keys with the same value are assigned the same f+1;\u00001grandom\nvalueâ€”they are correlated. The basic AGMS estimator is the product of sk(k:id)andsk(mk:keyword id):\nEst(je1j) =sk(k.id)\u0001sk(mk.keyword id)=X\nx2kX\ny2mk\u0018e1(x.id)\u0001\u0018e1(y.keyword id)\nDue to the 4-wise independence property of \u0018e1, this estimator is unbiasedâ€”its expectation equals the true je1jcar-\ndinality. However, its variance is highâ€”it has poor accuracy. This is expected since a full table with any number of\ntuples is summarized as a single number. The standard technique to improve accuracy is to build multiple independent\nbasic sketch estimators. This is achieved by using independent families of random variables \u0018e1. It is theoretically\nproven that, in order to obtain an estimator with relative error at most \u000fwith conï¬dence \u000e,O\u0000\n1=\u000f2log (1=\u000e)\u0001\nbasic\nsketches are necessary. As shown in Figure 3, they are grouped into a matrix of rrows andbcolumns. Then, the\nï¬nal AGMS estimator is obtained by averaging the binstances in each row and taking the median over the resulting r\naverages. In summary, an AGMS sketch has \n(r\u0001b)update and query time, and its space usage is also \n(r\u0001b). This\nassumes that the random number generators \u0018have small seeds and produce their values fastâ€”aspects that require\ncareful implementation.\n4.1.2 Multi-Way Join Cardinality Estimation\nWe show how to extend AGMS sketches to multi-way join cardinality estimation. For this we add the join e2between\nmovie keyword andtitle toe1and aim to estimate the cardinality of this 3-table query. Following the approach\nfor two-way joins, a family of sketches is built for edge e2on attributes mk.movie idandt.id, respectively. These\nsketches share their own family \u0018e2of random variables. Since two attributes from mkâ€“keyword idandmovie idâ€“\n7\n\nparticipate in join operators with other tables, we have to preserve their tuple connection. This is achieved by creating\na single composed sketch sk(mk:kid;mk:mid)instead of separate sketches for each attribute [11]. The value of\nsk(mk:kid;mk:mid)is computed as:\nsk(mk.k id,mk.m id)=X\nt2mk\u0018e1(t.kid)\u0001\u0018e2(t.mid)\nwhere the product of the two random variables is added to the sketch. The cardinality estimator is deï¬ned as the\nproduct of three sketches in this case:\nEst(je1[e2j) =sk(k.id)\u0001sk(mk.k id,mk.m id)\u0001sk(t.id) =X\nx2kX\ny2mkX\nz2t\u0018e1(x.id)\u0001\u0018e1(y.kid)\u0001\u0018e2(y.mid)\u0001\u0018e2(z.id)\nAs long as the families \u0018e1and\u0018e2are independent, this estimator is unbiased. However, its variance can be exponen-\ntially worse than that of the two-way join estimatorâ€”which makes sense, given the additional degree of randomness.\nThus, to achieve the same accuracy, a considerably larger number of basic sketches are required.\nThis strategy can be generalized to complex queries involving any number of tables and join predicates. A sketch\nis built for every table. Independent random families \u0018are used for every join predicate. The sketch corresponding\nto a table is updated with the product of all the \u0018families incident to it, applied to the corresponding join attribute.\nIn the case of our example query JOB 6a with 5 tables and 5 join predicates (Figure 3), there are 5 sketches and 5\nfamilies\u0018. The sketch skmkfor table mkis updated with the product \u0018e1(kid)\u0001\u0018e2(mid)\u0001\u0018e4(mid)which includes\na factor for each of the three join predicates. The unbiased cardinality estimator is the product of the 5 sketches\nskk\u0001skmk\u0001skt\u0001skci\u0001skn. For the same number of basic sketches r\u0001bas in the case of the je1jjoin, the accuracy of the\nje1[e2[e3[e4[e5jjoin can be exponentially worse.\n4.1.3 Selection Cardinality Estimation\nQuery JOB 6a contains 3 selection predicatesâ€”point on k, subset on n, and range on t. These have to be accounted for\nwhen estimating the overall query cardinality. AGMS sketches can handle selection predicates as long as the domain\nof the attribute is discreteâ€”which is the case for the ï¬xed-size data types in databases. The idea is to express the\nselection as a join predicate between the table and the domain of the selection attribute [45, 48]. Following the two-\nway join approach, a sketch is built on the selection attribute over all the tuples in the table. The sketch over the domain\nâ€“ which shares the same random family \u0018â€“ summarizes the values in the domain which satisfy the predicate by adding\nan entry for each of them to the sketchâ€”for a point predicate, the sketch includes only the \u0018value corresponding to\nthe constant in the predicate; for a range, the \u0018values for all points in the range; for a subset, the \u0018values for the\npoints in the subset. As long as the number of points is small, these sketches can be computed fast. Moreover, even\nfor ranges, there is a speciï¬c fast range-summable random family \u0018for which the sketch can be computed in constant\ntime, independent of the range size [45, 48]. In the JOB 6a query depicted in Figure 3, the sketch update procedure for\ntables with predicates includes an additional factor corresponding to the selection attribute. For example, the sketch\nskkfor table keyword is updated with the product \u0018e1(id)\u0001\u0018p1(keyword ). Overall, 8 families \u0018and 8 sketches are\nrequiredâ€”the sketches over the domain of the selection attributes are not included in Figure 3. The ï¬nal estimator is\nthe product of these 8 sketches. Since this estimator is a multi-way join with a larger number of sketches, its accuracy\nbecomes worse than that of the join sketches only.\n4.1.4 Why AGMS Sketches Are Not Practical for Query Optimization?\nAs shown, AGMS sketches can be theoretically used to estimate the cardinality of arbitrary complex queries with\njoin and selection predicates. While all the sketches for a table can be built in a single scan, since the update\ntime per AGMS sketch is linear in the sketch size, updating an exponential number of sketches becomes domi-\nnant. Moreover, the space requirement for all the sketches is also a problem. These scalability issues hinder the\napplication of AGMS sketches to join order enumeration. However, AGMS sketches suffer from a more serious\nproblem in query optimizationâ€”they cannot be incrementally composed. What this means is that a sketch used to\n8\n\nestimate a two-way join between two relations cannot be used to estimate a three-way join that includes another re-\nlation. The addition of join e2toe1in our example illustrates this well. It is not possible to compute the sketch\nsk(mk:kid;mk:mid)from sketch sk(mk:keyword id). It is not even possible to compute sk(mk:kid;mk:mid)\nfromsk(mk:keyword id)andsk(mk:movieid). The reason is the order of multiplication and addition. The other\ndirection â€“ use sk(mk:kid;mk:mid)instead ofsk(mk:keyword id)orsk(mk:movieid)â€“ is also not possi-\nble. Thus, in order to support plan enumeration, a separate sketch has to be built for every combination of the join\nattributesâ€”which is an exponential number. For example, 7 sketches have to be built for both tables mkandciwhich\nparticipate in 3 join predicates. If we include the attributes that can appear in selection predicates, the number of\nsketches that has to be built for a table can become exponential in the number of attributes in the table. While work-\nload information can be used to reduce this number, there is little that can be done for tables that join with several\nother tables on different attributes. Practically, AGMS sketches cannot achieve the goal of having synopses only for\nsingle attributes.\nğœ(ğ‘¡.year)t.id\nğ‘¤1\nğ‘¤2\nğ‘¤ğ‘¡k.id k.keyword\nğ‘¥1 xk1\nğ‘¥2 xk2\nâ€¦ â€¦\nğ‘¥ğ‘˜ xkğ‘˜\nmk.keyword_id mk.movie_id\nğ‘¦1 ğ‘§1\nğ‘¦2 ğ‘§2\nâ€¦ â€¦\nğ‘¦ğ‘š ğ‘§ğ‘šğœ(ğ‘˜.keyword)â›\nââœâœ\nâœâœ\nâœâ„ğ‘’1\n1\nâ„ğ‘’1\n2\nâ‹®\nâ„ğ‘’1ğ‘Ÿâ\nâ âŸâŸ\nâŸâŸ\nâŸ\nğ‘ŸÃ—1\nâ›\nââœâœ\nâœâœ\nâœâ„ğ‘’2\n1\nâ„ğ‘’2\n2\nâ‹®\nâ„ğ‘’2ğ‘Ÿâ\nâ âŸâŸ\nâŸâŸ\nâŸ\nğ‘ŸÃ—1k.id\nğ‘¥2\nğ‘¥ğ‘˜â›\nââœâœ\nâœâœ\nâœğœ‰ğ‘’1\n1\nğœ‰ğ‘’1\n2\nâ‹®\nğœ‰ğ‘’1ğ‘Ÿâ\nâ âŸâŸ\nâŸâŸ\nâŸ\nğ‘ŸÃ—1\nâ›\nââœâœ\nâœâœ\nâœğœ‰ğ‘’2\n1\nğœ‰ğ‘’2\n2\nâ‹®\nğœ‰ğ‘’2ğ‘Ÿâ\nâ âŸâŸ\nâŸâŸ\nâŸ\nğ‘ŸÃ—1t.id t.year\nğ‘¤1 ğ‘¤ğ‘¦1\nğ‘¤2 ğ‘¤ğ‘¦2\nâ€¦ â€¦\nğ‘¤ğ‘¡ ğ‘¤ğ‘¦ğ‘¡â›\nââœ\nâœ\nâœ\nâœ\nâœğ‘1,1\n+ (ğ‘¥) ğ‘2,(ğ‘¥) â„ğ‘’1\n2ğœ‰ğ‘’1\n2\nâ‹®\nğ‘ğ‘Ÿ,1ğ‘1,2\nğ‘2,2\nâ‹®\nğ‘ğ‘Ÿ,2â‹¯\nâ‹¯\nâ‹±\nâ‹¯+ (ğ‘¥) ğ‘1,(ğ‘¥) â„ğ‘’1\n1ğœ‰ğ‘’1\n1\nğ‘2,ğ‘\nâ‹®\n+ (ğ‘¥) ğ‘ğ‘Ÿ,(ğ‘¥) â„ğ‘’1\nğ‘Ÿğœ‰ğ‘’1ğ‘Ÿâ\nâ âŸ\nâŸ\nâŸ\nâŸ\nâŸ\nğ‘ŸÃ—ğ‘ğ¬ğ¤ğ¤.ğ¢ğ\nâ›\nââœ\nâœ\nâœ\nâœ\nâœğ‘1,1\n+ (ğ‘¦) ğ‘2,(ğ‘¦) â„ğ‘’1\n2ğœ‰ğ‘’1\n2\nâ‹®\nğ‘ğ‘Ÿ,1+ (ğ‘¦) ğ‘1,(ğ‘¦) â„ğ‘’1\n1ğœ‰ğ‘’1\n1\nğ‘2,2\nâ‹®\n+ (ğ‘¦) ğ‘ğ‘Ÿ,(ğ‘¦) â„ğ‘’1\nğ‘Ÿğœ‰ğ‘’1ğ‘Ÿâ‹¯\nâ‹¯\nâ‹±\nâ‹¯ğ‘1,ğ‘\nğ‘2,ğ‘\nâ‹®\nğ‘ğ‘Ÿ,ğ‘â\nâ âŸ\nâŸ\nâŸ\nâŸ\nâŸ\nğ‘ŸÃ—ğ‘ğ¬ğ¤ğ¦ğ¤.ğ¤ğğ²ğ°ğ¨ğ«ğ_ğ¢ğ\nâ›\nââœ\nâœ\nâœ\nâœ\nâœğ‘1,1\n+ (ğ‘§) ğ‘2,(ğ‘§) â„ğ‘’2\n2ğœ‰ğ‘’2\n2\nâ‹®\nğ‘ğ‘Ÿ,1ğ‘1,2\nğ‘2,2\nâ‹®\nğ‘ğ‘Ÿ,2â‹¯\nâ‹¯\nâ‹±\nâ‹¯+ (ğ‘§) ğ‘1,(ğ‘§) â„ğ‘’2\n1ğœ‰ğ‘’2\n1\nğ‘2,ğ‘\nâ‹®\n+ (ğ‘§) ğ‘ğ‘Ÿ,(ğ‘§) â„ğ‘’2\nğ‘Ÿğœ‰ğ‘’2ğ‘Ÿâ\nâ âŸ\nâŸ\nâŸ\nâŸ\nâŸ\nğ‘ŸÃ—ğ‘ğ¬ğ¤ğ¦ğ¤.ğ¦ğ¨ğ¯ğ¢ğ_ğ¢ğâŠ•\nâ›\nââœ\nâœ\nâœ\nâœ\nâœğ‘1,1\n+ (ğ‘¤) ğ‘2,(ğ‘¤) â„ğ‘’2\n2ğœ‰ğ‘’2\n2\nâ‹®\nğ‘ğ‘Ÿ,1+ (ğ‘¤) ğ‘1,(ğ‘¤) â„ğ‘’2\n1ğœ‰ğ‘’2\n1\nğ‘2,2\nâ‹®\n+ (ğ‘¤) ğ‘ğ‘Ÿ,(ğ‘¤) â„ğ‘’2\nğ‘Ÿğœ‰ğ‘’2ğ‘Ÿâ‹¯\nâ‹¯\nâ‹±\nâ‹¯ğ‘1,ğ‘\nğ‘2,ğ‘\nâ‹®\nğ‘ğ‘Ÿ,ğ‘â\nâ âŸ\nâŸ\nâŸ\nâŸ\nâŸ\nğ‘ŸÃ—ğ‘ğ¬ğ¤ğ­.ğ¢ğ\nâŠ•âŠ•Push-Down Selection Fast-AGMS Sketch\nâ›\nââœ\nâœ\nâœ\nâœ\nâœğ‘  â‹…(ğ‘  âŠ—ğ‘  )â‹…ğ‘  âˆ‘ğ‘\nğ‘–=1ğ‘˜ğ‘˜.ğ‘–ğ‘‘\n1,ğ‘–ğ‘˜ğ‘šğ‘˜.ğ‘˜_ğ‘–ğ‘‘\n1,ğ‘–ğ‘˜ğ‘šğ‘˜.ğ‘š_ğ‘–ğ‘‘\n1,ğ‘–ğ‘˜ğ‘¡\n1,ğ‘–\nğ‘  â‹…(ğ‘  âŠ—ğ‘  )â‹…ğ‘  âˆ‘ğ‘\nğ‘–=1ğ‘˜ğ‘˜.ğ‘–ğ‘‘\n2,ğ‘–ğ‘˜ğ‘šğ‘˜.ğ‘˜_ğ‘–ğ‘‘\n2,ğ‘–ğ‘˜ğ‘šğ‘˜.ğ‘š_ğ‘–ğ‘‘\n2,ğ‘–ğ‘˜ğ‘¡\n2,ğ‘–\nâ‹®\nğ‘  â‹…(ğ‘  âŠ—ğ‘  )â‹…ğ‘  âˆ‘ğ‘\nğ‘–=1ğ‘˜ğ‘˜.ğ‘–ğ‘‘\nğ‘Ÿ,ğ‘– ğ‘˜ğ‘šğ‘˜.ğ‘˜_ğ‘–ğ‘‘\nğ‘Ÿ,ğ‘– ğ‘˜ğ‘šğ‘˜.ğ‘š_ğ‘–ğ‘‘\nğ‘Ÿ,ğ‘– ğ‘˜ğ‘¡\nğ‘Ÿ,ğ‘–â\nâ âŸ\nâŸ\nâŸ\nâŸ\nâŸ\nğ‘ŸÃ—1MedianâŠ•\nFigure 4: Cardinality estimation for query JOB 6a with Fast-AGMS sketches.\n4.2 Fast-AGMS Sketches\nFast-AGMS sketches preserve the (r\u0002b)matrix structure of AGMS sketches. However, they deï¬ne a complete row\nofbcounters as a basic sketch element (Figure 4). Only one of these counters is updated for every tuple, thus, a factor\nbreduction in update time is obtained. The updated counter is chosen by a random hash function hassociated with the\nrow. The purpose of his to spread tuples with different values as evenly as possibleâ€”tuples with the same key still end\nup in the same bucket. On average, a factor bless tuples collide on the same counter, which preserves the frequency of\neach of them better. Since a full row is a sketch element, a single \u0018family of random variables is associated with every\nrow. Thus, a Fast-AGMS sketch with rrows requires only rhash and\u0018random functions. The value of a counter jis\nsk(k.id)j=P\nt2k;h(t.id)=j\u0018(t.id).\n9\n\n4.2.1 Two-Way Join Cardinality Estimation\nIn order to estimate join cardinality, the same principle appliesâ€”Fast-AGMS sketches are built over the join attributes\nusing the same random functions hand\u0018. The hash function hlands identical keys to the same bucket, while \u0018gives\nthe same sign. The unbiased estimator for a basic sketch sums up the product of the corresponding buckets:\nEst(je1j) =bX\nj=1sk(k.id)j\u0001sk(mk.keyword id)j\nSummation is necessary because hpartitions the tuples. As for AGMS sketches, the ï¬nal estimate is obtained by taking\nthe median of the rindependent basic sketches. Although the accuracy of Fast-AGMS sketches is asymptotically\nequal to that of AGMS sketches [7] in the worst case, it has been shown statistically that Fast-AGMS sketches have\nconsiderably better accuracy than any other sketching technique on average [47]. The combined accuracy and fast\nupdate time make Fast-AGMS sketches suitable for query optimization.\n4.2.2 Why Fast-AGMS Sketches Are Not Applicable to Query Optimization?\nAs far as we know, there is no work that extends Fast-AGMS sketches to multi-way join estimation. The main problem\nis the requirement to have independent hash functions he1andhe2for the two join attributes. These functions allocate\nthe attributes to different buckets, which means that the tuple is added to the sketch twice. Moreover, the relationship\nbetween attributes is lost. Since sketch-based selectivity estimation is also reduced to a join between the selection\nattribute and its domain, this implies that Fast-AGMS sketches cannot be used to estimate the cardinality of two-way\njoins with predicates. In fact, computing optimally the Fast-AGMS sketch of the domain of a range predicate does\nnot have a solution. This is because there is no order relationship between the hash values of adjacent points in the\ndomain. Due to these limitations, Fast-AGMS sketches have not been used in query optimization before. COMPASS\nintroduces Fast-AGMS extensions for multi-way joins and solves the selectivity issue by pushing-down predicates\nduring query optimization, and adding only the relevant tuples to the sketch.\n5 FAST-AGMS SKETCH MULTI-WAY JOIN ESTIMATION\nWe present two strategies to extend Fast-AGMS sketches to multi-way join cardinality estimation. The ï¬rst strategy\nâ€“ sketch partitioning â€“ is a theoretically sound estimator for a given multi-way join. Its limitation is that it cannot\nbe composed/decomposed, thus, it is not scalable for plan enumeration. The second strategy â€“ sketch merging â€“ ad-\ndresses the scalability issue by incrementally creating multi-way sketches from two-way sketches. Although this is\ndone heuristically for a certain multi-way join taken separately, all the multi-way joins with a given size are equally im-\npacted. We show empirically that this property is a good surrogate for accuracy â€“ which is much harder to consistently\nachieve â€“ in join order enumeration.\n5.1 Sketch Partitioning\nThe idea of sketch partitioning is to reorganize the bbuckets of the elementary sketch into a (b1\u0002b2)2-D matrixâ€”\nas done in [5] for frequency sketches. he1hashes a tuple mk(kid;mid)to one of the b1rows, while he2hashes\nto one of the b2columns. Then, only the counter at indices\u0002\nhe1(kid);he2(mid)\u0003\nis updated with the product\n\u0018e1(kid)\u0001\u0018e2(mid). This process is depicted in Figure 5. For example, tuple (6,3) in mkadds 1 to the counter [2,1].\nhe1guarantees that all the tuples with kid= 6are hashed to row 2, while he2sends tuples with mid= 3to column\n1. Conï¬‚icts happen only when the output of both hash functions is identical. Given the quadratic number of buckets\ncompared to the sketch for a single attribute â€“ while the number of tuples is the same â€“ conï¬‚icts are less frequent. The\ncardinality estimate for the 3-table join k ./mk ./t is obtained by summing up all the entries in the matrix resulted\nafter the scalar multiplication between sk(k:id)and every row in skpart(mk), followed by the scalar multiplication\nbetween the transpose of sk(t:id)and every column in skpart(mk). This can be written as:\nEst(je1[e2j) =X\n0\u0014i<b1X\n0\u0014j<b 2skk[i]\u0001skpart(mk)[i;j]\u0001skt[j]\n10\n\n\u0000\u0000\u0000\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u00000 1 2 3\nÂ 1 Â 2 Â 0 -1mk.keyword_id\nmk.movie_id\n0 1 2 3\n-2 Â 1 Â 1 Â 0 \u0000\u0000\u0000\u00001\n0 1 2 3\nÂ 3 Â 2 -1 Â 0 \n0 1 2 3\n-2 Â 3 Â 0 -1\u0000\u0000\u0000\u000021114489262\n1275769139Â 0 Â 1 Â 3 Â 4 Â 5 Â 8 Â 9 13\nÂ 1 Â 2 Â 3 Â 4 Â 5 Â 6 k.id\nt.id\n(k.id)\n(t.id)(mk.keyword_id)\n(mk.movie_id)Â \n(\u0000 âŠ•\u0000 ) \u0000\u0000\u00001 \u0000\u0000\u00002(\u0000)=\u0000%4 â„\u00001\n(\u0000)=(\u0000+2)%4 â„\u000020 1 2 3\n0-1Â 1Â 0-1\n1-1Â 1Â 02\n20Â 1Â 00\n30Â 0Â 00mk.keyword_idmk.movie_id({1,4,5,6,8,13})=+1 \u0000\u00001\n({0,2,3,9})=âˆ’1 \u0000\u00001\n({1,3,4,7})=+1 \u0000\u00002\n({2,5,6,9})=âˆ’1 \u0000\u00002Sketch Partitioning Sketch Merging\n\u0000 (\u0000\u0000) \u0000\u0000\u0000\u0000\u00000 1 2 3\n0-2Â 3Â 0-1\n12Â 2Â 0-1\n2-1-1Â 0-1\n30Â 0Â 00mk.keyword_idmk.movie_idFigure 5: Fast-AGMS sketches for multi-way join cardinality estimation on query JOB 6a.\nIt can be shown theoretically that this estimator is unbiased following the same proof as for AGMS sketches in [11].\nMoreover, given the larger size of sketch skpart(mk), its accuracy is expected to be better. This procedure can be\ngeneralized to any number of join attributes by partitioning â€“ or replicating â€“ binto the corresponding number of\ndimensions. For example, a table with 3 joins has a 3-D tensor as its sketch, with one dimension for every join\nattribute. Thus, there is a polynomial factor increase in the size of the sketch and the estimate computation. This has to\nbe carefully accounted for in the overall memory budget since the likelihood of conï¬‚icts varies with the dimensionality\nof the sketch tensor. The constraint to have the same number of buckets for a join predicate, e.g., sk(k:id)has as many\nbuckets as the number of rows in skpart(mk), makes memory allocation among sketches more complicated than for\nthe 1-D AGMS sketch vectors.\nPartitioned Fast-AGMS sketches are not scalable for join order enumeration. This is because separate sketches are\nrequired for every join. For example, in Figure 5, the 2-D sketch skpart(mk)is used for the 3-way join k./mk./t ,\nwhile the 1-D sketches skmk1andskmk2are used for the 2-way joins k ./ mk andt ./ mk , respectively. Building\nand storing these many sketches is impractical in query optimization. One alternative is to build only the sketches for\nup to k-way joins and use other methods to estimate higher-order join cardinality. This strategy is applied for run-time\njoin samples in [29]. The drawback is that other statistics are required for the higher-order joins and the interaction\nbetween the estimates produced by these statistics and the sketch estimates has to be carefully controlled.\nOur goal is to exclusively use sketches. Intuitively, we want to be able to either generate the 2-D sketch from the\n1-D sketches or extract the 1-D sketches from the 2-D sketch. Unfortunately, none of these have a clear solution for\nFast-AGMS sketches. The composition of skmk1andskmk2requires to determine how to combine all the pairs of\nbuckets in the 1-D sketches in order to compute the quadratic number of entries in the 2-D sketch. Since the identity\nof tuples is lost when they are inserted in the 1-D sketch, it is not possible to recreate the tuple and determine its\ncorresponding 2-D bucket. Moreover, due to conï¬‚icts in the \u0018random functions, we do not even know how many\ntuples belong to a 1-D bucket. For example, bucket 1 in skmk1is 2 even though 4 tuples are hashed to it. The\nextraction of a 1-D sketch from the 2-D sketch also does not work because of the \u0018variables. Speciï¬cally, the update\nby the product \u0018e1\u0001\u0018e2makes it impossible to retrieve the value of a 1-D bucket by summing up the corresponding\n2-D buckets. For example, the value of bucket 0 in skmk1is not the sum of the buckets in row 0 of sketch skpart(mk).\nThis property is true only for hash-based sketches [5].\n5.2 Sketch Merging\nWe introduce the sketch merging heuristic as a lightweight method to compose two-way join Fast-AGMS sketches in\norder to estimate the cardinality of multi-way joins. The procedure works as follows. We build sketches for every\ntwo-way join predicate independently, as shown in Figure 5. The number of sketches corresponding to a table is equal\n11\n\nto the number of joins it participates in. For example, tables kandthave one sketch, while mkhas two sketches.\nWe estimate any join combination generated during plan enumeration using only these sketches. The two-way joins\nk ./ mk andt ./ mk are estimated optimally with the sketch pairs (skk;skmk1)and(skt;skmk2), respectively.\nThese are the most accurate sketch estimates we can get. For the 3-way join k ./ mk ./ t , we create a merged\nsketchskmerge (mk) =skmk1\bskmk2from the two 2-way join sketches on demand during plan enumeration. This\nmerged sketch approximates the partitioned sketch skpart(mk)computed with the same random functions, without\naccessing the tuples. A bucket [i;j]inskmerge is set to the value having the minimum absolute magnitude among the\ncorresponding [i]and[j]buckets in the two basic sketches:\nskmerge[i;j] =\u001askmk1[i];ifjskmk1[i]j\u0014jskmk2[j]j\nskmk2[j];ifjskmk1[i]j>jskmk2[j]j\nFor the example in Figure 5, bucket [0;0]is set to\u00002becausej3j>j\u00002j, while bucket [0;2]to0becausej0j<j3j. The\nreason for this merge procedure is multifolded. The interaction between the random functions \u0018is considered â€“ albeit\nnot through a direct multiplication â€“ by preserving the sign of the value in the basic sketch. The absolute magnitude\ncorresponds to the maximum number of tuples with a given join key that are hashed to the bucketâ€”assuming no\nconï¬‚icts. These tuples are partitioned across the buckets of the other join key. The minimum is chosen because this is\nthe maximum number of tuples that can have identical values for both join keys when considered together. However,\nthis is an overestimate because the exact tuple pairing is lost. This can be seen when comparing the magnitude of the\nvalues in the two 2-D sketches in Figure 5. In fact, sketch merging is likely to always overestimate join cardinality.\nThe only caveat is the interaction between the \u0018functions.\nSketch merging can be generalized to any number of joins by applying the procedure iteratively. Moreover, (n+1)-\nD sketches can be derived incrementally from n-D sketches in a single stepâ€”without the need to always start from the\nbasic sketches. This property can be exploited to speed up the computation and reduce memory usage in bottom-up\nplan enumeration since only the highest dimensional sketches have to be maintained. An even more important property\nof sketch merging is that it is consistent in how it handles the multi-way joins with the same number of predicates.\nSpeciï¬cally, all these joins rely on the same basic sketches and the same assumptions for merging. Thus, it is likely\nthat these estimates exhibit similar accuracy behaviorâ€”same type of errors for equal join size.\n1 2 3 4 5 6 7 8 910\nnumber of joins106\n105\n104\n103\n102\n101\n100101102 underestimate [ratio (log)] overestimate \n95th percentile\n75th percentile\nmedian\n25th percentile\n5th percentile\n(a) estimate/true\n1 2 3 4 5 6 7 8 910\nnumber of joins010203040normalized L1 permutation distance (b) L1-distance\nFigure 6: Accuracy ratio (a) and L1-distance between the estimated sketch permutation and the correct join order (b).\n12\n\nIn order to verify this claim, we depict the accuracy of sketch merging for the JOB queries in Figure 6. We use\ntwo measures to quantify accuracy. The ï¬rst is the ratio between the sketch estimate and the true cardinality for all the\nenumerated sub-plans having at most ten joins (Figure 6a). We observe that the median ratio is within a factor of 10\nfor up to six joins, which is better than any previous practical results [29]. For a larger number of joins, sketch merging\ngenerates underestimates systematically. In previous results [30], this behavior occurs starting from 3-way joins.\n2-join mk./kci./nmk./tci./tmk./ci\nTrue cardinality 14 486 0.3M 6M 215M\nSketch merging 3K 7.5K 6.4M 14M 397M\n3-join mk./k./t t./ci./n mk./ci./k mk./ci./n mk./ci./t\nTrue cardinality 11 61 1242 10K 17M\nSketch merging 6.7K 35K 0.7M 1.5M 8.7B\n4-join mk./ci./n./k mk./ci./n./t mk./ci./k./t\nTrue cardinality 6 1194 1224\nSketch merging 198 18M 6.5M\nTable 1: 2-, 3-, and 4-way join L1 permutation distance for JOB 6a.\nThe second measure is the normalized L1 distance [71] between the permutation generated by sketch merging and\nthe correct join order. Given nsub-plans of the same size, the correct order Cis obtained by sorting them in increasing\norder of their cardinality. The permutation corresponding to sketch merging Sis obtained by sorting the sub-plans\nbased on the sketch estimates. The L1 distance is deï¬ned asPn\ni=1jSi\u0000Cij, the sum of the differences between the\nposition in the permutation and the correct order. For example, the L1 distance for the 4-way joins in query JOB 6a\n(Table 1) is 0 + 1 + 1 = 2 . The normalized L1 distance â€“ we divide the distance by the number of sub-plans in the\nquery â€“ is depicted in Figure 6b. The closer the distance is to zero, the more similar is the permutation to the correct\norder. For reference, we plot the line corresponding to the maximum L1 distance. The join orders generated by sketch\nmerging have an L1 distance that is signiï¬cantly below the maximum. In particular, for 2-way join sub-plans, the\ndistance is almost zero, while for sub-plans with more joins, the distance is constantly below 10. This conï¬rms that\nsketch merging selects orders that are close to optimal most of the time.\n6 FAST-AGMS SKETCH JOIN ORDER ENUMERATION\nIn this section, we present how merged Fast-AGMS sketches are integrated into a novel plan enumeration algorithm\nwe introduce in this work. It is important to emphasize that â€“ due to the proposed generalization to multi-way joins\nâ€“ Fast-AGMS sketches can be embedded into any enumeration algorithm. According to the investigation of different\njoin order enumeration algorithms performed in [30, 28], plan enumeration is not the most critical component of a\nquery optimizerâ€”it has a relatively small impact on plan quality compared to cardinality estimation accuracy. This\nconï¬rms the approach taken by many query optimizers [78, 77] that consider only a plan subset â€”left- or right-deep\nplans, which are a permutation of the query tables. We follow a similar approach and introduce an heuristic plan\nenumeration algorithm that allows us to explore the overall impact of merged Fast-AGMS sketches. Although this\nalgorithm borrows ideas from previous work [52], we argue that it is original in the presented form.\nCOMPASS uses the join graph (Figure 1) in plan enumeration. This guarantees that only valid join order plans\nare considered and cross products are ignored. Plan enumeration becomes a graph traversal problem. We design\na depth-ï¬rst search (DFS) traversal algorithm (Algorithm 1) that enumerates left-deep plans following the edges in\nthe join graph. This is achieved by considering all the vertices as the source of DFS and backtracking whenever a\ncomplete plan is reached (line 26). The number of plans explored from a source vertex is controlled by a user-deï¬ned\nparametermaxplans (line 25) that plays a similar role to the timeout used to conï¬ne the plan search space in [29].\nHowever, our algorithm does not require bottom-up plan enumeration since sketches can be merged and combined\nin any order. The maxplans parameter also allows the plan search to restart from other sources in the presence of\nhigh-degree vertices and cyclesâ€”instead of getting locked on the initial selections. On a continuum spectrum that\n13\n\nAlgorithm 1 Fast-AGMS Sketch Join Order Enumeration\n1:LetG= (V;E)be the join graph\n2:LetCbe the join order set, initially ;\n3:Letpbe the number of complete plans, initially 0\n4:Letmincostbe the minimum cost, initially 1\n5:Letmax plans be the maximum threshold of plans\nenumerated from a source vertex\n6:procedure PLAN ENUMERATION (G)\n7: LetSbe the set of vertices Vsorted in in-\ncreasing order of function fthat combines table\ncardinality with the degree of vinG:f(v) =\n\u000b\u0001cardinality (v)\nmaxfcardinality (u);8u2Vg+\f\u0001deg(v)\nmaxfdeg(u);8u2Vg,\nwhere\u000band\fare user-deï¬ned constants\n8: foreach vertexv2Sdo\n9:p 0\n10: DFS T RAVERSAL (G,fvg,0)\n11: end for\n12:end procedure\n13:procedure DFS T RAVERSAL (G,C,cost)\n14: curr est FAST-AGMS E STIMATE (C)\n15: cost cost+curr est\n16:.Early pruning\n17: ifcost>mincostthen return18:.Evaluate complete plan\n19: ifjCj=jVjthen\n20: ifcost<mincostthen\n21: mincost cost\n22: optpath C\n23: end if\n24:p p+ 1\n25: ifp=max plans then abort\n26: return\n27: end if\n28:.Recursive enumeration in increasing order of\nthe join cardinality estimates\n29: LetLbe the set of vertices v =2Cthat are adjacent\nto a vertexu2C\n30: foreach vertexv2Ldo\n31:e[v] FAST-AGMS E STIMATE (C[fvg)\n32: end for\n33: LetL0be setLsorted in increasing order of e[v]\n34: foreach vertexv2L0do\n35: DFS T RAVERSAL (G,C[fvg,cost)\n36: end for\n37:end procedure\nhas left-deep greedy search at one extreme and exhaustive enumeration at the other [14], the proposed algorithm\ncan be conï¬gured anywhere in-between by controlling the value of parameter maxplans . When a single plan is\nenumerated from every vertex, we obtain left-deep ordering. This is achieved by selecting the vertex vthat has the\nsmallest cardinality when appended to the current plan in line 35. When all the plans are enumerated from every\nvertex â€“ set maxplans =1â€“ we obtain exhaustive enumeration. The tradeoff between these two alternatives is\nevidentâ€”number of explored plans vs. enumeration overhead. Since sketch merging and estimation are fast operations\namenable to parallelization, the overhead is smallâ€”see the experiments in Section 7. Thus, maxplans values larger\nthan1are amenableâ€” maxplans is set by default to 10. This allows for a more comprehensive exploration of the\njoin order space compared to alternative synopses that incur higher overhead [22, 29].\nWe design two heuristics that control the order in which the plan space is explored and the depth of exploring\nsub-optimal plans. First, we sort the vertices according to a normalized cost function f(v)that combines vertex\ncardinality and the number of join predicates the vertex participates in (line 7). The conï¬gurable weights \u000band\f\ncontrol the relative importance of these factors. They are set by default to 0:5, which assigns equal weight to each\nfactor. DFS Traversal is invoked from the source vertices in increasing order of cost f(v). The intuition is to generate\nsub-plans with small cardinalities and limited orders as early as possible in the enumerationâ€”get the left-deep plan\ncorresponding to maxplans = 1ï¬rst. The second heuristic is early pruning of sub-optimal plans (line 17). Whenever\nthe cost of a sub-plan exceeds the minimum cost, we backtrack to a sub-plan that can still become optimal.\nFast-AGMS sketch cardinality estimation is invoked for every enumerated sub-plan (line 14) and to decide the\norder in which vertices are explored (line 31)â€”while the call on line 14 can be eliminated, we keep it for clarity.\nTheFast-AGMS Estimate function performs sketch merging and estimation only for the joins included in the sub-\nplan. In order to avoid recomputation, the estimates are cached. Alternatively, the merged sketches corresponding\nto join subsets a table is involved in and the product of the basic sketches corresponding to a sub-plan can also\nbe cached. They provide different levels of reuse and computation to generate the estimate. Caching the estimate\nprovides the least reuse, while caching merged sketches and products allows for incremental estimate evaluation. In\n14\n\nour implementation, we settle for a combined solution in which all the estimates, and merged sketches and products\nof up to three sketches are cached. This insures that the estimate corresponding to any sub-plan is computed only once\nand allows for incremental extension of the core sub-plans. It is important to emphasize that the input to Fast-AGMS\nEstimate is always represented only by the sketches corresponding to the two-way joins.\nWe illustrate how the plan enumeration algorithm works for query JOB 6a based on the join graph in Figure 1\nand the join cardinality estimates in Table 1. The ï¬ve vertices are sorted in the order n\u0000k\u0000t\u0000mk\u0000cibased\non function f(v)applied on the tables resulted after selection push-down. mkandciare the last two because they\nhave the largest degree and cardinalityâ€”and have no selection predicates. kandncome before tbecause they have\na smaller degree. Although nhas an order of magnitude more tuples than k, the predicate on nis very selective and\noutputs a smaller cardinality. The degree being the same, nis ï¬rst in the order. Thus, DFS Traversal is performed\nwithnas the ï¬rst source. The ï¬rst enumerated plan is n\u0000ci\u0000t\u0000mk\u0000kand its estimated cost is \u001918M. While a\nleft-deep plan search ï¬nishes the enumeration at this point, our algorithm backtracks and explores alternative plansâ€”\nwe setmaxplans = 2in this case. The other plan enumerated from nisn\u0000ci\u0000mk\u0000k\u0000twhich has an estimated\ncost of\u00191:5M. This is the optimal plan from n. Since the number pof explored plans from nreaches the value of\nmaxplans , in the next step, DFS Traversal is performed from k. The ï¬rst plan enumerated from kisk\u0000mk\u0000t\u0000ci\nwhich is pruned early because its partial cost of \u00196:5Mis larger than the complete minimum cost of \u00191:5M. The\nnext plank\u0000mk\u0000ci\u0000n\u0000thas an estimated cost of only \u00190:7Mand becomes the optimal plan up to this pointâ€”in\nfact, it is the optimal plan identiï¬ed by COMPASS (Figure 1). The third â€“ and ï¬nal â€“ plan considered from kis\nk\u0000mk\u0000ci\u0000t. This plan is pruned early. Notice that the enumeration terminates without reaching the maximum\nnumber of allowed plans. The enumeration starting from tdoes not proceed beyond its immediate neighbors because\nof the large cardinality estimates. The only complete plan enumerated from mkismk\u0000k\u0000ci\u0000n\u0000twhich has the\nsame cost as the minimal cost planâ€”they are equivalent. All the plans starting from ciare pruned early. The left-deep\nplan identiï¬ed by PostgreSQL and DBMS A is k\u0000mk\u0000t\u0000ci\u0000n(Figure 1). Although this plan has a slightly lower\ncost, it is not enumerated by our algorithm. The reason is the large sketch estimate for the join k ./ mk ./ t ./ ci\nwhich stops the enumeration early. While this estimate is inaccurate, the more thorough plan space exploration allows\nour algorithm to identify an alternative plan with a cost almost identical. We point out that if we apply the same\ngreedy strategy as in left-deep plan search using the sketch estimates in Table 1, we would get the same optimal plan\nas PostgreSQL and DBMS A. This is because, once we reach the sub-plan k\u0000mk\u0000t, the only alternative is to choose\nciâ€”there is no backtracking. Thus, left-deep search identiï¬es the plan k\u0000mk\u0000t\u0000ci\u0000nby chance rather than by\nconsidering estimates for four-way joinsâ€”known to be unreliable for any type of synopses, not only sketches.\n7 EMPIRICAL EV ALUATION\nWe perform an extensive experimental study over the complete JOB benchmark [30] in order to evaluate the perfor-\nmance of COMPASS and compare it against four other database query optimizers (Figure 1). While our main goal\nis to determine whether COMPASS is a complete optimizer â€“ which requires an effective integration of cardinality\nestimation in plan enumeration â€“ we also perform a detailed comparison between Fast-AGMS sketches and several\nstate-of-the-art methods for multi-way join cardinality estimation. Moreover, we assess the impact of the proposed\nplan enumeration algorithm. To this end, our evaluation investigates the following questions:\nâ€¢ What is the quality of the query execution plans generated by COMPASS? We measure plan quality as the total car-\ndinality of the intermediate results since this is independent from speciï¬c execution engine optimizations. Moreover,\nlogical optimizers use cardinality information as the main criterion to rank plans.\nâ€¢ What is the execution time â€“ or runtime â€“ for the COMPASS plans? Since this is highly dependent on the underlying\nquery processing engine, we execute the plans in MapD, PostgreSQL and DBMS A. This allows us to identify the\ncorrelation â€“ if there is one â€“ between plan quality and execution time.\nâ€¢ What is the overall JOB workload runtime? While individual queries allow for localized analysis, the workload\nexecution time measures the reliability of COMPASS. However, due to the high variance in JOB query complexity,\nthis measure alone is not an absolute indicator of the quality of an optimizer.\nâ€¢ How does COMPASS compare against the pessimistic optimizers that minimize upper bound cardinality, i.e., over-\nestimates? Since the highly-optimized pessimistic plans are shown to be considerably faster than the default Post-\ngreSQL plans [5, 18], we are interested where COMPASS stands on this scale.\n15\n\nâ€¢ What is the optimization overhead incurred by sketch merging in plan enumeration? While signiï¬cantly improving\nupon sketch partitioning, it is not clear if online sketch merging during push-down selection is fast enough to\nbe practical. We deem COMPASS to be a practical optimizer if it manages to consistently outperform the other\ndatabases and also incurs a reduced overhead.\nâ€¢ How does the accuracy of Fast-AGMS sketch merging compare against state-of-the-art methods for multi-way join\ncardinality estimation? Previous studies [56, 23] include only AGMS sketches, which are known to be considerably\nworse than Fast-AGMS for two-way join estimation [47, 49].\nâ€¢ How does the plan enumeration algorithm driven by sketch merging compare against standard algorithms? Does\nthe larger search space improve the plan quality compared to the greedy left-deep enumeration? Alternatively, how\nclose (far) is the proposed algorithm to exhaustive enumeration?\n7.1 Experimental Setup\nImplementation. We implement COMPASS in MapD (version 3.6.1) [76]. The source code is publicly available\nin Github [65]. MapD has a highly-parallel GPU-accelerated query execution engine. Relational operators are com-\npiled into CUDA kernels that are executed concurrently across the SIMD GPU architecture. In order to reduce data\nmovement, MapD coalesces multiple relational operators into a single CUDA kernel. For joins, this corresponds to a\nworst-case optimal join algorithm [41]. The MapD query optimizer, however, is not as sophisticated as its execution\nengine. It relies on the Calcite SQL compiler [73] to get a lexicographic â€“ in the order in which the query is written\nâ€“ query execution plan. The join order is computed based on a primitive heuristic that sorts the tables in decreasing\norder of their cardinality. Moreover, selection predicates are not considered in the optimization. COMPASS brings a\nprincipled cost-based optimization procedure to the MapD query optimizer.\nThe COMPASS implementation consists of two modulesâ€”a scan operator that integrates Fast-AGMS sketch con-\nstruction with push-down selection and a lightweight join order enumeration algorithm. For sketch construction, we\nadapt a publicly available two-way join Fast-AGMS sketch implementation [69] to the MapD CUDA kernel API. This\nrequires parallelizing both the update and the estimation functions. The scan operator ï¬lters only the relevant tuples\nto be passed to the sketch update. Since separate sketch instances are created for every GPU block warp, this requires\nan additional merge stageâ€”currently performed on the CPU. The sketches used throughout the experiments have 11\nrows of 1023 buckets, for a total of roughly \u001911K integers. Assuming 4-byte integers, the memory usage of a sketch\nis\u001945KB. The largest query has 28 joins. With two sketches per join, the maximum memory usage for a query is \u0019\n2.5MB ( 2\u000128\u000145), which is quite small. Depending on the parallelization approach, there can be a sketch instance\non every GPU block warp. In our case, the NVIDIA Tesla K80 has 26 block warps, resulting in a total of \u001965MB\nmemory usage for the most complex JOB query. The COMPASS plan enumeration algorithm depicted in Algorithm 1\nreplaces the primitive sorting heuristic from MapD. It determines the optimal plan based on the sketches computed\nby the scan operator and the conï¬gurable enumeration logic. Greedy join enumeration is the default algorithm used\nthroughout experiments.\nDatabase systems & hardware. The other three databases we use in addition to MapD are PostgreSQL (v.11.5),\nMonetDB (v.11.33.11), and the commercial DBMS A. PostgreSQL and DBMS A are used as the common ground in\nall the experiments because of their extensibility. Both of them allow us to inject and execute the join orders computed\nby the other databasesâ€”the CROSS JOIN statement in PostgreSQL and the hints in DBMS A, respectively. We\nconï¬gure PostgreSQL with 2GB memory per operator, 32GB buffer cache size, and we force the optimizer to use\ndynamic programming in plan enumeration for queries with no more than 18 join predicates. These settings follow\nprior art [30]. We use an optimized docker image publicly available for DBMS A, while for MonetDB we keep the\ndefault conï¬guration. All the systems run on a Ubuntu 16.04 LTS machine with 56 CPU cores (Intel Xeon E5-2660),\n256GB RAM, HDD storage, and an NVIDIA Tesla K80 GPU.\nDataset and query workload. We perform the experiments on the IMDB dataset [63] which has been used exten-\nsively to evaluate query optimizers [28] and has become a de-facto standard. The JOB benchmark [68] deï¬nes 113\nqueries â€“ grouped into 33 families â€“ over the IMDB dataset. These queries vary signiï¬cantly in their complexity,\nwith the simplest one having 4 joins and the most complex one having 28 joins. This variability manifests itself in\n16\n\n0.010.11101001,00010,0000.010.11101001,00010,0000.010.11101001,00010,000\n0.010.11101001,00010,000MapD\nMonetDB\nPgSQL\nDBMS A\n4-9 10-19 20-28\nNumber of join predicatesCardinality [log]Figure 7: Cardinality (in PostgreSQL) as a normalized ratio to COMPASS.\n17\n\nexecution times that are highly-different. To compensate for this, we split the queries into three groups and examine\neach group separately. These groups are based on the number of joins in the query: group1 contains queries with 4-9\njoins; group2, 10-19 joins; and group3, 20-28 joins. We organize the results according to these groups.\nMethodology. To quantitatively assess the quality of a join order plan, we use two metricsâ€”intermediate result\ncardinality and query execution time. The total cardinality of the intermediate results quantiï¬es how many tuples are\nproduced by all the joins in the plan. The lower this number is, the better the plan. This is the primary metric used\nin logical query optimization to estimate the cost of a plan. However, the actual execution time depends on speciï¬c\nquery processing optimizations. Thus, the execution time is not entirely correlated with the cardinality.\nIn order to fairly evaluate the join orders produced by every database, we use both PostgreSQL and DBMS A as\ncommon ground. First, we run the queries in each database and collect their join plans. Then, we inject these plans\ninto PostgreSQL and DBMS A, respectively, and measure their runtime. Moreover, we execute all the subqueries in\nthe plans to compute the intermediate cardinality. Notice that every system generates its plan independently based on\nits own algorithm and statistics. PostgreSQL and DBMS A serve as common execution engines for all the plans. This\nprocedure allows for a holistic comparison of the query optimizersâ€”independent of the execution engine.\n7.2 Results\nWe present the results of our extensive experimental evaluation, organized based on the investigation questions deï¬ned\nat the beginning of this section. The answers to the questions are summarized after the presentation of the results.\n7.2.1 Query-level Analysis\nIn this experiment, we compare COMPASS against every other system for each query in the JOB benchmark. We\nmeasure both the intermediate result cardinality, as well as the execution timeâ€”taken as the median value over 9\nruns. The execution plans are obtained by performing the query in each system. These are subsequently injected in\nPostgreSQL and DBMS A, and executed on the same execution engine. The cardinalities are generated by executing\nall the subqueries in the plan in the corresponding orderâ€”which is done in PostgreSQL. This information is extracted\nfrom the individual plans. Figure 7 and 8 depict the results normalized to COMPASS. All the values are divided by the\nCOMPASS resultsâ€”represented as a horizontal dotted line at position 1 on the y-axis. A point below this line means\nthat the other system has a better result, otherwise, COMPASS performs better. The results are grouped by the number\nof joins in the JOB queries (x-axis) and separated by two dotted vertical lines.\nMapD. MapD consistently produces execution plans that have cardinality two orders of magnitude or larger than\nCOMPASS. With a few exceptions, all MapD plans are worse. There is one such query â€“ the discontinuity going to\nzero in the ï¬gure â€“ that indeed has cardinality zero and MapD correctly detects it. However, this is only a matter\nof chance because the ï¬rst join in the plan â€“ between the largest tables in the query â€“ does not produce any results.\nThe reason for this poor plan quality is the lack of statistics in the MapD query optimizer. Decisions are taken solely\nbased on the full table cardinalityâ€”the number of tuples before any selection predicate. Therefore, the resulting plans\nare highly sub-optimal. While runtime follows cardinality â€“ with many results 100X slower than COMPASS â€“ the\ncorrelation between the two is not complete. There are several queries for which the MapD cardinality is considerably\nworse, while the execution time is similar or better than COMPASS. This is the case for some of the complex queries\nwith 20 or more joins executed in PostgreSQL. In this situation, MapD chooses a large well-connected table early in\nthe plan. This allows it to check many join predicates at the beginning and prune a large number of tuples. On the\nother hand, COMPASS â€“ and the other systems â€“ start from small tables on the periphery of the join graph and make\ntheir way to the highly-connected tables in the center. This strategy produces many staged intermediate results that\nincrease the runtime. While the runtime trend across PgSQL and DBMS A is similar, we observe that queries with 20\nor more joins are handled better by DBMS A, while queries with less than 20 joins are faster in PgSQL. This is an\nindication that DBMS A is better optimized for complex queries.\n18\n\n0.11101000.11101000.1110100\n0.1110100Runtime [log]MapD\nMonetDB\nPgSQL\nDBMS A\n4-9 10-19 20-28\nNumber of join predicatesPgSQLDBMS AFigure 8: Runtime (in PosgreSQL and DBMS A) as a normalized ratio to COMPASS.\n19\n\nMonetDB. The trend of the cardinality results in MonetDB follows the one in MapD. While the majority of the\nresults are worse than COMPASS, the ratio is smaller than for MapD. This improvement is due to the more advanced\nrule-based MonetDB query optimizer with limited statistics support. However, compared to the full sketch-based\nCOMPASS, the MonetDB cardinalities are considerably worseâ€”many times an order of magnitude or more. Interest-\ningly enough, though, the corresponding query runtimes fare much better than predicted by the cardinality. With few\nexceptions, they are always within a factor of 10 â€“ more often less â€“ off of COMPASS. Moreover, they are independent\nof query complexity and do not exhibit spikes. Overall, the MonetDB runtimes are the most consistent with COM-\nPASS across both PgSQL and DBMS A. This is because the MonetDB query optimizer ï¬nds plans that are executed\nsimilarly to COMPASSâ€”albeit they have higher cardinality.\nPostgreSQL. The cardinality results for PostgreSQL â€“ PgSQL in the ï¬gure â€“ are the closest to COMPASS among\nall the systems. This is entirely due to the advanced statistics the PostgreSQL optimizer employs. While mildly better\nthan COMPASS for several queries, PostgreSQL still exhibits spikes that go beyond a factor of 1000X. The reason is\nthe failure to detect correlations between join attributes. Since the plans are optimized for the PostgreSQL execution\nengine in this case, we expect the runtimes to be optimal. This is indeed the case for queries with less than 20 joins.\nHowever, for 20 or more joins, the PostgreSQL runtime is considerably worse compared to COMPASS. This is where\nthe PostgreSQL optimizer drops dynamic programming in plan search. With a few exceptions where there are dramatic\nspikes that go beyond 100X, the PgSQL plans executed in DBMS A perform as well as or better than in PostgreSQL\nitself. This is especially true for the complex queries having 20 or more joins. Overall, COMPASS generates more\nstable plans than PostgreSQL. Although not speciï¬cally optimized for it, PostgreSQL executes them as fast â€“ or faster\nâ€“ than its own plans.\nDBMS A. The commercial DBMS A produces plans that have consistently higher cardinality than COMPASS across\nall the JOB queries. This clearly shows that the employed statistics do a poor job at estimating the join cardinality.\nHowever, when executed in PostgreSQL, these plans have unexpectedly good runtimesâ€”except for queries with more\nthan 20 joins. This is likely due to the more complex cost function that considers other parameters beyond cardinality\nin determining the optimal plan. Interestingly enough, when executing its own plans, DBMS A does not fare better\nthan PostgreSQL, except for the complex queries with more than 20 joins. In fact, DBMS A has worse runtime for\nqueries with 10 to 20 joins. The runtimes of DBMS A and COMPASS are close to each other and always within a\nfactor of 10X. This conï¬rms that the COMPASS plans are also optimal for DBMS A.\nMapDMonetDBPgSQLDBMS ACOMPASS020406080\n71939\n2163Total number of queries won\n(a) Cardinality comparison\n4-9 10-19 20-28010203040\n1733\n13MapDMonetDBPgSQLDBMS ACOMPASS\nJoinsNumber of queries won (b) Cardinality vs number of joins\nFigure 9: Distribution of winning queries across the databases in terms of intermediate cardinality. The databases with\nthe lowest cardinality are the winners. The total (149) is larger than 113 â€“ the number of queries in JOB â€“ because\nthere are multiple queries for which more than one database is the winner.\n7.2.2 Aggregated Workload Statistics\nWe aggregate the query-level results (Figure 7 and 8) in order to obtain an overall view of the relative performance\nof the compared systems. These aggregated results are depicted in Figure 9 and 10, respectively. They give the total\n20\n\nnumber of queries for which a database performs the best, as well as the distribution as a function of the number of\njoins in the query. In the case of cardinality, a database is counted if it achieves the minimum cardinality among all the\ndatabases. For runtime, a database is counted if it comes within 10% of the fastest runtimeâ€”computed as the median\nof 9 runs. This bound compensates for variations in the environment.\nMapDMonetDBPgSQLDBMS ACOMPASS020406080100\n1435566382\n1953757481Total number of queries won\nPgSQL\nDBMS A\n(a) Execution time comparison\n4-9(Pg)4-9(A)10-19(Pg)10-19(A)20-28(Pg)20-28(A)01020304037\n273442\n1112MapD\nMonetDB\nPgSQL\nDBMS A\nCOMPASS\nJoinsNumber of queries won (b) Execution time vs number of joins\nFigure 10: Distribution of winning queries across the databases in terms of execution time when the optimal plans are\nplugged-in and executed in PostgreSQL and DBMS A. We obtain the optimal plan for every database from its query\noptimizer and execute it in PostgreSQL and DBMS A. All the databases within 10% of the fastest execution time are\nconsidered as winners. Thus, the total (250) is larger than 113â€”the number of queries in JOB.\nBased on Figure 9a, COMPASS achieves the plan with the minimum cardinality for 63 out of the 113 JOB queries.\nThis represents approximately 56% of the workload. PostgreSQL (PgSQL) comes in second place with 39 queries.\nThe other three databases obtain the best cardinality in less than 20% of the queries each, with MapD winning only 7\nqueries. The careful reader notices that the sum of the winning queries is larger than 113. This is because there are\nqueries for which two or more systems achieve the same best cardinalityâ€”case in which we count each of them. The\ndistribution of the winning queries in terms of the number of joins is depicted in Figure 9b. While for the simpler\nqueries with less than 10 joins all the systems perform similarly, COMPASS clearly dominates the others when the\ncomplexity increases. PostgreSQL is the only other database that performs sufï¬ciently well, however, only for queries\nwith a moderate number of joins. These results prove the beneï¬t of using statistics in query optimization, especially\nfor complicated queries. While the PostgreSQL statistics perform well for simple to moderate queries, COMPASS\nsketches are less sensitive to the number of joins in the queryâ€”they provide more consistent estimates. Moreover,\nCOMPASS is not heavily impacted by the greedy join enumeration algorithm. When PostgreSQL switches from\ndynamic programming â€“ more than 18 joins â€“ it fails to ï¬nd any best plan.\nThe aggregated runtime results in PgSQL and DBMS A are depicted in Figure 10a and 10b. They follow closely\nthe corresponding cardinality resultsâ€”with one exception. The runtime for the commercial DBMS A is much better\nthan its cardinalities anticipateâ€”DBMS A has the best runtime for 63 and 74 queries, while its cardinality is best\nonly for 21 queries. The reasons are outlined when the individual query results are discussed. Additionally, DBMS\nA beneï¬ts from the bound on runtime since it often comes within the fastest system. Overall, COMPASS achieves\nthe fastest runtime for 82 (PgSQL) and 81 (DBMS A) out of the 113 JOB queries â€“ 72% of the workload â€“ which is\nmore than any other database. This proves the superiority of the identiï¬ed plans and conï¬rms the correlation between\ncardinality and runtime. The correlation manifests more clearly for queries with a larger number of joins because of\nthe higher runtime, which makes ties more unlikely. Moreover, the correlation is stronger for PgSQL than for DBMS\nA since the number of winning queries is higher in DBMS A for all systems except COMPASS. A careful reader\nobserves that the runtime results are higher than the cardinality results for all the systemsâ€”and larger than 113 when\nsummed up. This is because it is more common to have close-enough runtimes than it is to have the same cardinalityâ€”\nmultiple counting is more frequent. Based on these results, we conclude that COMPASS is the optimizer with the most\nconsistent and resilient plans on the JOB benchmark.\n21\n\n7.2.3 Total Workload Runtime\nThe runtimes for the complete JOB workload execution in PostgreSQL and DBMS A using the plans generated by\neach database are included in Table 2. Given the high variance among queries, these numbers have to be taken with\na grain of salt since they may be dominated by a few complex queries with a large number of joins. Nonetheless, we\nfollow prior art [5, 54] and include them together with the aggregated workload statistics. As expected, COMPASS\nhas the overall fastest runtime. Somewhat unexpectedly, MonetDB comes in second for the PgSQL execution with a\nruntime that is almost twice as large as that of COMPASS. The reason is because MonetDB does not fail dramatically\nfor any of the JOB queries. While it performs consistently slower, it never derails on heavily sub-optimal plans. The\nruntime for PgSQL and DBMS A in PgSQL is dominated by the long-running queries with 20 or more joins, which\npull the total time to more than 8X and 5X that of COMPASS. These outliers are sufï¬cient to skew the overall runtime.\nIn the case of MapD, there are 30 queries that do not ï¬nish execution even after a timeout of 20 minutes per query.\nThus, the very large runtime. When the workload is executed in DBMS A, all systems except DBMS A incur an\nincrease in runtime. The increase is most signiï¬cant for COMPASS as it stands at 50% more than in PgSQL. On the\nother hand, DBMS A has a reduction of more than 50% of its PgSQL runtime. Nonetheless, COMPASS still has the\noverall fastest runtime, which is 35% faster than DBMS A.\nDatabaseRuntime (minutes) Ratio to COMPASS\nPgSQL DBMS A PgSQL DBMS A\nMapD >300>300 >23 >13\nMonetDB 27.52 35.71 2.19 1.65\nPgSQL 103.00 244.31 8.20 11.28\nDBMS A 70.72 29.22 5.63 1.35\nCOMPASS 12.56 21.66 1.00 1.00\nTable 2: JOB benchmark runtime in PgSQL and DBMS A.\n7.2.4 Comparison with Pessimistic Plans\nWe compare the plans produced by COMPASS against the pessimistic plans generated in [18]. The pessimistic plans\nare determined by minimizing the worst case cardinality estimates. Thus, they always produce over-estimates of the\ntrue cardinality. This is in contrast to COMPASS, which generates both over- and under-estimates. The pessimistic\nplans for all the JOB queries in PostgreSQL are available at [64]. They are generated by rewriting the SQL statements\nsuch that selection predicates and one-to-many â€“ key/foreign-key â€“ joins are evaluated before the many-to-many joins.\nMoreover, ordering is performed separately for one-to-many and many-to-many joins. This partitioning of the search\nspace results in a massive reduction of the number of considered join orders. A similar idea is employed in [5], where\nonly at most 2-D partitioned sketches are built.\nThe comparison between the runtime of the COMPASS plans and that of the pessimistic plans executed in Post-\ngreSQL is depicted in Figure 11. The results are normalized to the runtime of the COMPASS plans. We observe\nthat the difference between these plans is smaller than for the other systemsâ€”an indication that the plans have closer\nruntime. There are queries for which COMPASS generates faster plans and queries for which the pessimistic plans are\nbetter. Overall, there is a slight advantage for COMPASS since the curve is above the horizontal 1-axis more often.\nMoreover, the gap is higher for COMPASS, reaching a factor of almost 10X for certain queries. In terms of number of\njoins, the best plans are almost evenly distributed among the two methods. Table 3 summarizes the individual query\nresults from Figure 11. The timing results are higher than previously published [18] because no indexes are deï¬ned\nover the key attributes. COMPASS achieves a slightly better performance both in the number of queries won â€“ 88 vs.\n83 â€“ as well as in the cumulative runtimeâ€”COMPASS is faster by approximately 25 seconds. The main reason for the\nbetter performance of pessimistic plans â€“ compared to other systems â€“ is the separate optimization of the joins. The\npartition of the join graph based on the many-to-many joins and their independent ordering reduces the multi-way join\nestimation error signiï¬cantly. The separate evaluation of the one-to-many joins in every partition reduces the error\nfurther. Moreover, the estimation for these simpler joins is more accurate. While partitioning the join order space\n22\n\n0.1110 Runtime [log]\n4-9 10-19 20-28\nNumber of join predicatesFigure 11: Runtime of pessimistic plans (in PosgreSQL) as a normalized ratio to COMPASS.\nreduces estimation complexity, it also ignores orderings that can result in better plans. For example, the cardinality\nof a many-to-many join can be smaller than that of a one-to-many join. Pessimistic plans ignore these interleaved\norders. Given the holistic approach that considers the complete join graph, the COMPASS results are quite impressive\ngiven the size of the multi-way joins. This is possible because of the good accuracy â€“ and consistency â€“ of the merged\nFast-AGMS sketches. We conjecture that COMPASS can be improved by adopting a similar tiered approach to join\nordering. We plan to explore this idea in future work.\nQuery plans Queries won Runtime (seconds) Ratio to COMPASS\nPessimistic 83 777.10 1.03\nCOMPASS 88 753.86 1.00\nTable 3: JOB benchmark execution for pessimistic [5, 18] and COMPASS plans in PostgreSQL.\n1E+11E+21E+31E+41E+5\nMapD\nQuery execution\nCOMPASSRuntime (ms) [log]\n4-9 10-19 20-28\nNumber of join predicates\nFigure 12: Query runtime in MapD with the COMPASS query optimizer.\n7.2.5 Runtime in MapD\nIn this experiment, we evaluate the impact COMPASS has on the MapD database. For this, we replace the default\nMapD query optimizer with COMPASS and execute the JOB benchmark in both scenarios. We measure the end-to-end\n23\n\n0100200300400500Plan enumeration Cardinality estimationOptimization overhead (ms)\n4-9 10-19 20-28Number of join predicates(a) Optimization overhead on GPU.\n0%5%10%15%20% Plan enumeration Cardinality estimationOptimization overhead (%)\n4-9 10-19 20-28Number of join predicates\n(b) Optimization overhead on GPU as a percentage from the total runtime.\n110100100010000\nGPUCPUOverhead (ms) [log]\n4-9 10-19 20-28\nNumber of join predicates\n(c) Optimization overhead on GPU and CPU.\nFigure 13: The overhead of the COMPASS optimizer implemented in MapD.\n24\n\nquery runtime, as well as only the query execution time without optimizationâ€”these are the same in MapD. We report\nthe median over 9 runs. Figure 12 depicts the results for every query. We observe that MapD outperforms COMPASS\nfor simple and some moderate queries. However, the differences are not signiï¬cant, as opposed to the difference for\nmore complex queries. This may be surprising given the primitive MapD query optimizer. However, its execution\nengine is quite different from PostgreSQL. It is highly-optimized for parallel in-memory processing. This alleviates\nthe need for careful optimization on simple queries. For more complicated queries, though, sketch-based optimization\npays off as COMPASS ï¬nds considerably better plans. In fact, MapD fails on 8 queries and times out after 30 minutes\non 8 other queries. COMPASS ï¬nishes all the queries and is faster than MapD for 74 of them, which represents 65%\nof the workload. The total runtime for the 97 queries MapD successfully runs is included in Table 4. COMPASS\nhas a runtime of 6.21 minutes to MapDâ€™s 47.64â€”which is a net speedup of 7.67X. This proves both that sketches\ncan be effectively computed at runtime, as well as their beneï¬t to generate better query plans, which result in faster\nexecution. The last point is clear when we compare only the execution time, without optimization overheadâ€”less than\nten COMPASS plans have execution time larger than MapD.\nDatabase Queries won Runtime (minutes) Ratio to COMPASS\nMapD 42 47.64 7.67\nCOMPASS 74 6.21 1.00\nTable 4: JOB benchmark execution in MapD.\n7.2.6 COMPASS Overhead\nWe measure the optimization overhead of building Fast-AGMS sketches, as well as that of sketch-based plan enumer-\nation, for the COMPASS MapD implementation. Sketch building can be performed either on GPU or CPU, while\nmerging and plan enumeration are performed on CPU. The results are depicted in Figure 13. Figure 13a and 13b show\nthe absolute and relative overhead, respectively, for the GPU execution. The overhead in queries with up to 9 joins\nis at most 15%â€”or at most 420 ms. At a ï¬rst glance, the overhead may seem signiï¬cant. However, this time is not\nspent in vain since the plans selected by COMPASS are quite fast even with the overhead included. For these simpler\nqueries, there is not signiï¬cant difference between plans. Thus, a primitive optimizer as in MapD is sufï¬cient. The\noverhead for the rest of the workload is at most 17%â€”or 500 ms. This overhead becomes negligible in the overall\nexecution time. As a result, COMPASS largely outperforms the other four databases. Figure 13c shows that â€“ as\nexpected â€“ sketch building is more efï¬cient on GPU than on CPU due to the higher degree of parallelism. In both\ncases, the optimization overhead increases with the number of joins in the query. For GPU, the overhead is in the\norder of hundreds of milliseconds (ms), with a maximum of around 500 ms for certain complex queries. For CPU, the\noverhead is always below 5 seconds, which is relatively small for queries that take minutes to run. Given that this is\nonly a prototype, we believe that the sketch overhead can be further reduced with more optimized code.\n7.2.7 Comparison with State-of-the-art Synopses for Join Cardinality Estimation\nWe compare COMPASS against seven methods for join cardinality estimation following the study presented in [23].\nThese methods are PostgreSQL, AGMS sketches [2, 1], random table samples (TS), correlated samples [56], join\nsamples (JS), KDE with table samples (TS+KDE), and KDE with join samples (JS+KDE) [23]. We perform all the\nexperiments for join cardinality estimation on three types of JOB queries over at most ï¬ve tables â€“ the simplest in the\nbenchmark â€“ as presented in [23]. We use the publicly available code, workload, and data from [66]. The results\nare depicted in Figure 14. We observe that the COMPASS accuracy matches that of the best estimators closely for\nall the queries. The only two estimators that always outperform COMPASS are based on join samples (Join Sample\nand JS+KDE). This type of estimators require indexes on all possible join attribute combinations, thus, they have a\nhigh set-up and maintenance cost. Moreover, the KDE models are trained on query samples with the same set of\nselection and join predicates, i.e., same type of training queries with different constant values. In addition, JS+KDE\nrequires training for every join size. Thus, it is not clear what is the behavior of the KDE estimators on different types\nof queriesâ€”the sub-queries enumerated by the optimizer, in particular. Notice that the results also include sketches\n25\n\n(AGMS). However, these are the AGMS sketches, not the Fast-AGMS sketches on which COMPASS is built upon.\nGiven the detailed sketch comparison in [47, 49], the difference between the two is expected. Since we perform a\nthorough query plan evaluation with PostgreSQL (Postgres), the comparison in terms of accuracy is interesting. While\nCOMPASS and Postgres have very similar accuracy â€“ with an advantage for COMPASS â€“ our holistic results prove\nthat COMPASS generates better query plans both in terms of quality and execution time. This proves that other factors\nbeyond single query accuracy â€“ such as sub-plan enumeration and estimator composition â€“ have to be considered by the\noptimizer. Overall, the comparison in terms of accuracy is limited to a series of relatively simple hand-picked queries\nwith at most four joinsâ€”nothing close to the full JOB benchmark. While useful, it fails to conï¬rm the practicality of\nthe considered approaches in a complete query optimizer, which COMPASS does.\nIMDB Q1 Uniform\n1e+001e+011e+021e+031e+041e+051e+061e+07Q-error (log scale) IMDB Q2 Uniform\n IMDB Q3 Uniform\nIMDB Q1 Distinct\n1e+001e+011e+021e+031e+041e+051e+061e+07\nPostgresAGMS\nTable Sample (TS) Correlated SampleJoin Sample (JS)TS+KDE JS+KDECOMPASSQ-error (log scale) IMDB Q2 Distinct\nPostgresAGMS\nTable Sample (TS) Correlated SampleJoin Sample (JS)TS+KDE JS+KDECOMPASS IMDB Q3 Distinct\nPostgresAGMS\nTable Sample (TS) Correlated SampleJoin Sample (JS)TS+KDE JS+KDECOMPASS\nFigure 14: Comparison with state-of-the-art techniques for cardinality estimation.\n7.2.8 Plan Enumeration Analysis\nWe assess the performance of the join order enumeration algorithm by varying the search space. We set four different\ndimensions in the left-deep search spaceâ€“ greedy ,full-greedy ,limit-10 , and exhaustive . The greedy solution traverses\nthe join graph from a single source and greedily adds nodes relying on the sketch estimations. The source node is\nchosen as the smallest table from the two-way join that has the smallest cardinality estimation. full-greedy executes\nthegreedy traversal from every node in the join graph. The join order with the smallest overall estimation is selected.\nlimit-10 enhances full-greedy with backtracking. Rather than stopping after the ï¬rst greedy plan is generated, limit-10\ntraverses additional paths in the join graph until the ï¬rst 10 join orders are generated, i.e., maxplans is set to 10\nin Algorithm 1. Lastly, exhaustive traverses the entire search space to ï¬nd the optimal plan, i.e., maxplans is set\nto1in Algorithm 1. Sketch merging and estimation are arithmetic operations amenable to parallelization. Thus,\nthe traversal algorithm quickly explores even the large spaces enumerated by exhaustive . To further speed up the\nenumeration phase, two practical heuristics are applied in Algorithm 1 (see Section 6).\n26\n\nWe collect the join order corresponding to every approach for all the JOB queries and compare their intermediate\ncardinality and runtime. The results are depicted in Figure 15. Intuitively, we expect that the larger the search space\nis, the higher the odds to ï¬nd a better join order plan. However, as discussed in [30, 28], the plan enumeration\nalgorithm has a relatively small impact on plan quality. Our results conï¬rm this hypothesis. We observe that no\nparticular approach is signiï¬cantly better for the proposed sketch-driven traversal algorithm. In larger search spaces,\nsub-optimal plans are pruned by the early stopping criteria since the overall cardinality estimates exceed the current\nminimum cost. The plan search space is not only limited by the join graph. The search space may shrink because of\nthe early stopping criteria that compare the overall cardinality estimates for the sub-queries. For example, it is possible\nthe execution plans for greedy andfull-greedy are the same, although full-greedy searches from each node separately.\nThe reason is that all the other execution plans selected by full-greedy may have larger overall cost. In fact, even the\nsub-queries may have already larger cost than the plan selected by greedy solutionâ€”thus the enumeration algorithm\nbacktracks. As a result, there is no particular preference for the search space size since the cardinality differences are\nnot signiï¬cant in Figure 15 (upper part of the ï¬gure). Although there are some outliers â€“ spikes in the ï¬gure â€“ they\nare mostly selected by full-greedy ,limit-10 , and exhaustive . This behavior is caused by the over- and under-estimates.\nAlso, there is no particular trend when comparing the runtime corresponding to the different plansâ€”except for a few\noutliers in exhaustive . Moreover, there is no correlation between cardinality and runtime results for the outliers. We\nbelieve the plan differences chosen by these four different solutions are not signiï¬cantly different in order to notice\nchanges in terms of the runtime. The results conï¬rm that the plan chosen by greedy is not necessarily slow and,\nthus, we conclude that COMPASSS is not heavily impacted by the join enumeration algorithm. In fact, COMPASS\noutperforms the other systems, especially for complex queries with a large number of joins.\n0.1110Runtime [log]0.010.11101001000\ngreedy\nfull-greedy\nlimit-10\nexhaustiveCardinality [log]\n4-9 10-19 20-28\nNumber of join predicates\nFigure 15: The effect of plan enumeration algorithms on the cardinality and runtime of the selected plan. Values are\nnormalized with respect to the cardinality and runtime of the plan determined by the greedy algorithm. Values above\nthe â€œ1â€ horizontal line are worse while values below are better.\n27\n\n7.3 Summary\nBased on the presented results, we can answer the questions raised at the beginning of the experimental section:\nâ€¢ COMPASS generates query plans with the lowest cardinality among all the considered systems for 56% of the\nqueries in the workload. This percentage increases to 65% for complicated queries with 10 or more joins.\nâ€¢ The better plans identiï¬ed by COMPASS translate into faster query runtimes in PostgreSQL, DBMS A, and MapD.\nOut of the 113 JOB queries, COMPASS achieves the fastest runtime for more than 80 in PostgreSQL and DBMS A,\nand 74 in MapD. This conï¬rms the correlation between cardinality and runtime. DBMS A is the only database that\ndoes not satisfy this correlation, which can be problematic for a user.\nâ€¢ COMPASS and MonetDB are the only databases that perform all the JOB queries without serious hiccups both in\nPostgreSQL and DBMS A. The other systems have several queries for which the runtime â€œexplodesâ€. This results in\nsigniï¬cantly higher workload runtime. On the PostgreSQL engine, COMPASS outperforms MonetDB by a factor of\n2.19X, while on DBMS A by 1.65X. DBMS A optimizes queries speciï¬cally for its engine, resulting in a signiï¬cant\nreduction in runtime compared to PostgreSQL. However, COMPASS is faster by a factor of 1.35X. Moreover,\nCOMPASS is at least 7.67X faster than MapD.\nâ€¢ Even though COMPASS treats the complete join graph in ordering â€“ which results in a much larger search space â€“\nit manages to outperform the pessimistic optimizersâ€”which perform join ordering in a tiered approach. While only\n3% reduction in the workload runtime, this improvement is signiï¬cant because COMPASS achieves a faster runtime\nand higher speedup for more queries.\nâ€¢ The overhead incurred by the COMPASS optimizer in MapD is less than 500 milliseconds on GPU and less than 5\nseconds on CPU. While this may be too large for simple queries, it results in faster execution for more than 91% of\nthe queries. We plan to optimize our implementation in the future.\nâ€¢ The accuracy achieved by COMPASS matches â€“ and often surpasses â€“ that of most of the state-of-the-art methods for\njoin cardinality estimation. The only methods that have better accuracy are based on join samples, which have never\nbeen fully-integrated in a query optimizer because of their complexity. If we consider only methods implemented in\nexistent query optimizers, COMPASS has better overall accuracy.\nâ€¢ The COMPASS plan enumeration algorithm can be customized to perform the search for the optimal join order from\na limited greedy to an exhaustive left-deep tree. However, the difference between these alternatives is not signiï¬cant\nenough to compensate for their gap in overhead. Thus, the fast greedy join order enumeration is sufï¬cient to achieve\nthe optimal COMPASS plans.\n8 RELATED WORK\nCardinality estimation. While exhaustive surveys on query optimization [6, 67] argue that each component is im-\nportant in ï¬nding the optimal plan, Leis et al. [28, 30] show experimentally that cardinality estimation is the most\ndominant component in query optimization. However, consistency in estimations is more important than high ac-\ncuracy only for a limited number of instances. There are four mainstream cardinality estimation approaches in the\nliteratureâ€”histograms, sampling techniques, sketches, and, more recently, machine learning models. While his-\ntograms can provide accurate selectivity estimation for a single attribute in a relation [20], it is difï¬cult for them\nto capture correlations between cross-join attributes [43], thus reducing their applicability to joins. Unlike histograms,\nsampling techniques [29, 40] can detect arbitrary correlations for common values. However, samples are sensitive\nto skewed and sparse data when few tuples are selected by a query [58]. As the query optimizer estimates a large\nnumber of joins, the cardinality drops quickly, causing wrong estimates for intermediate results. Estimating the car-\ndinality of multi-way joins with AGMS sketches is introduced in [10, 11], while a statistical analysis of two-way\njoin sketch-based techniques is performed by Rusu and Dobra [47, 49]. Their results show that Fast-AGMS sketches\nare clearly superior to other sketches. In this work, we extend Fast-AGMS sketches to capture all the join attributes\ninvolved in a given query within a single sketch and efï¬ciently estimate multi-way joins. Vengerov et al. [56] present\nan extension to AGMS sketches that captures selection predicates, while Cai et al. [5] introduce bound sketches that\nprovide theoretical upper bounds for cardinality estimation. The problem with these approaches is that the online\nsketch building process is not scalable. Hertzschuch et al. [18] maintain the pessimistic property for cardinality esti-\nmation, while replacing sketches with a simple formula based on statistics already available to the PostgreSQL query\n28\n\noptimizer. This eliminates the sketch overhead, while preserving the quality of the pessimistic plansâ€”as long as the\noptimizer statistics estimate predicate selectivity accurately. Kernel density models for cardinality estimation (KDE)\nare introduced in [17, 23]. They are built on samples extracted either from the base tables or the join. While their\naccuracy is shown to be superior to any other method on JOB queries over at most ï¬ve tables â€“ the simplest in the\nbenchmark â€“ it is not clear how to generalize and fully integrate KDE models in plan enumeration. Speciï¬cally, the\nKDE implementation [66] builds a separate estimator for every query. No details are provided on how to apply the\nestimator to query sub-plans derived from the main query, which is the centerpiece of plan enumeration.\nQuery reoptimization. In order to overcome the inherent mis-estimations in the query optimizer, Adaptive Query\nProcessing [9] allows the query processor to modify the optimal query plan computed by the optimizer in case of\nlarge deviations from the true cardinality values detected at runtime. The Mid-Query Re-Optimizer [21], ROX [22],\nand SkinnerDB [54] re-run the query optimizer at runtime in the case of large differences between estimations and\nthe true cardinalities. Wu et al. [59] apply online sampling to correct the errors in the plans generated by the query\noptimizer. These approaches use the output of the query executor and sampling techniques to re-estimate the car-\ndinalities based on already computed intermediate join outputs and change the query plan whenever the estimated\nvalues deviate signiï¬cantly. In the self-adaptable LEO optimizer [36], the query engine monitors and uses the feed-\nback from the execution engine in order to adjust the histogram-based synopses for better performance in subsequent\nqueries. Eddies [3] process batches of tuples by following dynamic routing policies during query execution. Unlike\nthese systems, COMPASS performs query optimization as a single stage, while query execution is partitioned into two\nphasesâ€”before and after the optimization. As in query reoptimization, COMPASS uses the intermediates â€“ sketches\nâ€“ produced by the ï¬rst phase of execution. However, this process is performed only once, thus its overhead is smaller\ncompared to continuous reoptimization.\nMachine learning for query optimization. Using machine learning techniques and deep neural networks is a recent\ntrend in query optimization. Join order enumeration [34, 27, 33], cardinality estimation [32, 31, 24, 25, 57, 42], selec-\ntivity estimation [61, 15, 12], and index structures [26] have been active research directions. Regarding the cardinality\nestimation problem, Malik et al. [32] propose to train neural network models based on cardinality distributions for\na separate class of similar queries and estimate overall query cardinalities. Yang et al. [60] utilize neural networks\nto learn a function to estimate cardinalities of queries with range selection predicates. Kipf et al. [24] use multi-set\nconvolutional neural networks in order to model join and selection predicates, and capture join correlations. Woltmann\net al. [57] propose to train neural network models to estimate cardinalities in equi-joins. Marcus et al. [34] use rein-\nforcement learning in order to efï¬ciently explore the search space and ï¬nd optimal join order plans. Different from\nthese approaches, COMPASS uses traditional randomized algorithms to estimate cardinality. Moreover, COMPASS is\nfully-integrated into a database engine, which is often not the case for these machine learning solutions.\nPlan enumeration. In plan enumeration, multiple semantically equivalent plans are explored in order to identify the\noptimal execution plan. Different exhaustive [55, 39] and heuristic-based [52] algorithms have been proposed. They\nconsider different tree shapes â€“ such as left-deep and bushy trees â€“ in the search space. Leis et al. [28, 30] evaluate the\ninï¬‚uence of several plan enumeration algorithms and the impact of considering bushy trees. Several recent approaches\nhave been proposed to optimize the plan enumeration phase by using GPUs [37, 38] and deep reinforcement learning\nmodels [34, 35]. In this work, we propose an adaptive graph traversal algorithm that efï¬ciently explores the search\nspace. This algorithm can be conï¬gured to cover plan enumeration from greedy to exhaustive search.\n9 CONCLUSIONS\nWe introduce the online sketch-based COMPASS query optimizer, which uses exclusively Fast-AGMS sketches for\ncardinality estimation and plan enumeration. Fast-AGMS sketches are computed online by leveraging the optimized\nparallel execution engine in modern databases. Selection predicates and sketch updates are pushed-down and eval-\nuated online during query optimization. Plan enumeration is performed over the query join graph by incrementally\ncomposing the corresponding sketches. We prototype COMPASS in MapD and perform extensive experiments over\n29\n\nthe complete JOB benchmark. The results prove the reduced overhead COMPASS incurs, while generating better\nexecution plans than four other database systemsâ€”COMPASS outperforms four other databases on all the considered\nmetrics over the JOB benchmark. In future work, we plan to investigate alternative merging strategies for Fast-AGMS\nsketches in order to support multi-way joins. SIMD-optimized sketch algorithms â€“ for CPU and GPU â€“ with lower\noverhead and alternative plan enumeration strategies are other directions we plan to pursue.\nAcknowledgments. This work is supported by NSF award number 2008815 and by a U.S. Department of Energy\nEarly Career Award (DOE Career). The authors want to thank Alex Suhan â€“ one of the MapD architects â€“ for explain-\ning the internals of the MapD execution engine, as well as Hung Ngo and Mahmoud Abo Khamis from relationalAI\nfor the discussions on query optimization. Lastly, the authors acknowledge the insightful comments made by the\nSIGMOD 2021 anonymous reviewers that helped improve the quality of the paper.\nReferences\n[1] N. Alon, P. B. Gibbons, Y . Matias, and M. Szegedy. Tracking Join and Self-Join Sizes in Limited Storage. In\nPODS 1999 , pages 10â€“20.\n[2] N. Alon, Y . Matias, and M. Szegedy. The Space Complexity of Approximating the Frequency Moments. In\nSTOC 1996 , pages 20â€“29.\n[3] R. Avnur and J. M. Hellerstein. Eddies: Continuously Adaptive Query Processing. In SIGMOD 2000 , pages\n261â€“272.\n[4] S. BreÃŸ, M. Heimel, N. Siegmund, L. Bellatreche, and G. Saake. GPU-accelerated Database Systems: Survey\nand Open Challenges. TLKDS , pages 1â€“35, 2014.\n[5] W. Cai, M. Balazinska, and D. Suciu. Pessimistic Cardinality Estimation: Tighter Upper Bounds for Intermediate\nJoin Cardinalities. In SIGMOD 2019 , pages 18â€“35.\n[6] S. Chaudhuri. An Overview of Query Optimization in Relational Systems. In PODS 1998 , pages 34â€“43.\n[7] G. Cormode and M. Garofalakis. Sketching Streams Through the Net: Distributed Approximate Query Tracking.\nInVLDB 2005 , pages 13â€“24.\n[8] G. Cormode, M. Garofalakis, P. J. Haas, and C. Jermaine. Synopses for Massive Data: Samples, Histograms,\nWavelets, Sketches. Foundation and Trends in Databases , 4:1â€“294, 2012.\n[9] A. Deshpande, Z. Ives, and V . Raman. Adaptive Query Processing. Foundations and Trends in Databases ,\n1(1):1â€“140, 2007.\n[10] A. Dobra, M. Garofalakis, J. Gehrke, and R. Rastogi. Processing Complex Aggregate Queries over Data Streams.\nInSIGMOD 2002 , pages 61â€“72.\n[11] A. Dobra, M. Garofalakis, J. Gehrke, and R. Rastogi. Sketch-Based Multi-query Processing over Data Streams.\nInEDBT 2004 , pages 551â€“568.\n[12] A. Dutt, C. Wang, A. Nazi, S. Kandula, V . Narasayya, and S. Chaudhuri. Selectivity Estimation for Range\nPredicates Using Lightweight Models. PVLDB , 12(9):1044â€“1057, 2019.\n[13] H. Funke, S. BreÃŸ, S. Noll, V . Markl, and J. Teubner. Pipelined Query Processing in Coprocessor Environments.\nInSIGMOD 2018 , pages 1603â€“1618.\n[14] H. Garcia-Molina, J. D. Ullman, and J. Widom. Database Systems: The Complete Book . Prentice Hall, 2008.\n[15] S. Hasan, S. Thirumuruganathan, J. Augustine, N. Koudas, and G. Das. Multi-Attribute Selectivity Estimation\nUsing Deep Learning. CoRR , arXiv:1903.09999v2, 2019.\n30\n\n[16] B. He, M. Lu, K. Yang, R. Fang, N. K. Govindaraju, Q. Luo, and P. V . Sander. Relational Query Coprocessing\non Graphics Processors. TODS , 34(4):1â€“39, 2009.\n[17] M. Heimel, M. Kiefer, and V . Markl. Self-Tuning, GPU-Accelerated Kernel Density Models for Multidimen-\nsional Selectivity Estimation. In SIGMOD 2015 , pages 1477â€“1492.\n[18] A. Hertzschuch, C. Hartmann, D. Habich, and W. Lehner. Simplicity Done Right for Join Ordering. In CIDR\n2021 .\n[19] S. Idreos, F. Groffen, N. Nes, S. Manegold, S. Mullender, and M. Kersten. MonetDB: Two Decades of Research\nin Column-oriented Database Architectures. IEEE Data Engineering Bulletin , 35(1):40â€“45, 2012.\n[20] Y . E. Ioannidis and S. Christodoulakis. On the Propagation of Errors in the Size of Join Results. SIGMOD\nRecord , 20(2):268â€“277, 1991.\n[21] N. Kabra and D. J. DeWitt. Efï¬cient Mid-query Re-optimization of Sub-optimal Query Execution Plans. In\nSIGMOD 1998 , pages 106â€“117.\n[22] A. R. Kader, P. Boncz, S. Manegold, and M. van Keulen. ROX: Run-time Optimization of XQueries. In SIGMOD\n2009 , pages 615â€“626.\n[23] M. Kiefer, M. Heimel, S. BreÃŸ, and V . Markl. Estimating Join Selectivities using Bandwidth-Optimized Kernel\nDensity Models. PVLDB , 10(13):2085â€“2096, 2017.\n[24] A. Kipf, T. Kipf, B. Radke, V . Leis, P. Boncz, and A. Kemper. Learned Cardinalities: Estimating Correlated\nJoins with Deep Learning. In CIDR 2019 .\n[25] A. Kipf, D. V orona, J. Muller, T. Kipf, B. Radke, V . Leis, P. Boncz, T. Neumann, and A. Kemper. Estimating\nCardinalities with Deep Sketches. CoRR , arXiv:1904.08223v1, 2019.\n[26] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The Case for Learned Index Structures. In SIGMOD\n2018 , pages 489â€“504.\n[27] S. Krishnan, Z. Yang, K. Goldberg, J. Hellerstein, and I. Stoica. Learning to Optimize Join Queries With Deep\nReinforcement Learning. CoRR , arXiv:1808.03196v2, 2018.\n[28] V . Leis, A. Gubichev, A. Mirchev, P. Boncz, A. Kemper, and T. Neumann. How Good Are Query Optimizers,\nReally? PVLDB , 9(3):204â€“215, 2015.\n[29] V . Leis, B. Radke, A. Gubichev, A. Kemper, and T. Neumann. Cardinality Estimation Done Right: Index-Based\nJoin Sampling. In CIDR 2017 .\n[30] V . Leis, B. Radke, A. Gubichev, A. Mirchev, P. Boncz, A. Kemper, and T. Neumann. Query Optimization\nThrough the Looking Glass, and What We Found Running the Join Order Benchmark. VLDB Journal , 27:643â€“\n668, 2018.\n[31] H. Liu, M. Xu, Z. Yu, V . Corvinelli, and C. Zuzarte. Cardinality Estimation Using Neural Networks. In CASCON\n2015 , pages 53â€“59.\n[32] T. Malik, R. C. Burns, and N. V . Chawla. A Black-Box Approach to Query Cardinality Estimation. In CIDR\n2007 .\n[33] R. Marcus, P. Negi, H. Mao, C. Zhang, M. Alizadeh, T. Kraska, O. Papaemmanouil, and N. Tatbul. Neo: A\nLearned Query Optimizer. VLDB Journal , 12(11), 2019.\n[34] R. Marcus and O. Papaemmanouil. Deep Reinforcement Learning for Join Order Enumeration. In aiDM 2018 .\n[35] R. Marcus and O. Papaemmanouil. Towards a Hands-Free Query Optimizer through Deep Learning. CoRR ,\narXiv:1809.10212v2, 2018.\n31\n\n[36] V . Markl, G. M. Lohman, and V . Raman. LEO: An Autonomic Query Optimizer for DB2. IBM Systems Journal ,\n42(1):98â€“106, 2003.\n[37] A. Meister. GPU-Accelerated Join-Order Optimization. In PhD Workshop @ VLDB 2015 .\n[38] A. Meister and G. Saake. Challenges for a GPU-Accelerated Dynamic Programming Approach for Join-Order\nOptimization. In GvD 2016 .\n[39] G. Moerkotte and T. Neumann. Analysis of Two Existing and One New Dynamic Programming Algorithm for\nthe Generation of Optimal Bushy Join Trees Without Cross Products. In VLDB 2006 , pages 930â€“941.\n[40] M. Muller, G. Moerkotte, and O. Kolb. Improved Selectivity Estimation by Combining Knowledge from Sam-\npling and Synopses. PVLDB , 9(11):1016â€“1028, 2018.\n[41] H. Q. Ngo, E. Porat, C. R Â´e, and A. Rudra. Worst-Case Optimal Join Algorithms. In PODS 2012 , pages 37â€“48.\n[42] J. Ortiz, M. Balazinska, J. Gehrke, and S. Sathiya Keerthi. An Empirical Analysis of Deep Learning for Cardi-\nnality Estimation. CoRR , arXiv:1905.06425v2, 2019.\n[43] V . Poosala and Y . E. Ioannidis. Selectivity Estimation Without the Attribute Value Independence Assumption. In\nVLDB 1997 , pages 486â€“495.\n[44] P. Roy, A. Khan, and G. Alonso. Augmented Sketch: Faster and More Accurate Stream Processing. In SIGMOD\n2016 , pages 1449â€“1463.\n[45] F. Rusu and A. Dobra. Fast Range-Summable Random Variables for Efï¬cient Aggregate Estimation. In SIGMOD\n2006 , pages 193â€“204.\n[46] F. Rusu and A. Dobra. Sketching Sampled Data Streams. In ICDE 2009 , pages 381â€“392.\n[47] F. Rusu and A. Dobra. Statistical Analysis of Sketch Estimators. In SIGMOD 2007 , pages 187â€“198.\n[48] F. Rusu and A. Dobra. Pseudo-Random Number Generation for Sketch-Based Estimations. TODS , 32(2), 2007.\n[49] F. Rusu and A. Dobra. Sketches for Size of Join Estimation. TODS , 33(15), 2008.\n[50] P. G. Selinger, M. M. Astrahan, D. D. Chamberlain, R. A. Lorie, and T. G. Price. Access Path Selection in a\nRelational Database Management System. In SIGMOD 1979 , pages 23â€“34.\n[51] J. H. Shin, F. Rusu, and A. Suhan. Exact Selectivity Computation for Modern In-Memory Database Query\nOptimization. CoRR , arXiv:1901.01488v1, 2019.\n[52] M. Steinbrunn, G. Moerkotte, and A. Kemper. Heuristic and Randomized Optimization for the Join Ordering\nProblem. VLDB Journal , 6(3):191â€“208, 1997.\n[53] C. Stylianopoulos, I. Walulya, M. Almgren, O. Landsiedel, and M. Papatriantaï¬lou. Delegation Sketch: A\nParallel Design with Support for Fast and Accurate Concurrent Operations. In EuroSys 2020 .\n[54] I. Trummer, J. Wang, D. Maram, S. Moseley, S. Jo, and J. Antonakakis. SkinnerDB: Regret-Bounded Query\nEvaluation via Reinforcement Learning. In SIGMOD 2019 , pages 1153â€“1170.\n[55] B. Vance and D. Maier. Rapid Bushy Join-order Optimization with Cartesian Products. SIGMOD Record ,\n25(2):35â€“46, 1996.\n[56] D. Vengerov, A. C. Menck, M. Zait, and S. P. Chakkappen. Join Size Estimation Subject to Filter Condition.\nPVLDB , 8(12):1530â€“1541, 2015.\n[57] L. Woltmann, C. Hartmann, M. Thiele, D. Habich, and W. Lehner. Cardinality Estimation with Local Deep\nLearning Models. In aiDM 2019 , pages 1â€“8.\n32\n\n[58] W. Wu. Sampling-Based Cardinality Estimation Algorithms: A Survey and An Empirical Evaluation, 2012.\n[59] W. Wu, J. F. Naughton, and H. Singh. Sampling-Based Query Re-Optimization. In SIGMOD 2016 , pages\n1721â€“1736.\n[60] T. Yang, L. Liu, Y . Yan, M. Shahzad, Y . Shen, X. Li, B. Cui, and G. Xie. SF-sketch: A Two-stage Sketch for\nData Streams. CoRR , arXiv:1701.04148v3, 2017.\n[61] Z. Yang, E. Liang, A. Kamsetty, C. Wu, Y . Duan, X. Chen, P. Abbeel, J. M. Hellerstein, S. Krishnan, and I. Stoica.\nSelectivity Estimation with Deep Likelihood Models. CoRR , arXiv:1905.04278v2, 2019.\n[62] F. Yu, W. Hou, C. Luo, D. Che, and M. Zhu. CS2: A New Database Synopsis for Query Estimation. In SIGMOD\n2013 .\n[63] P. Boncz. The IMDB Dataset. http://homepages.cwi.nl/ Ëœboncz/job/imdb.tgz .\n[64] A. Hertzschuch. SimplicityDoneRight. https://github.com/axhertz/SimplicityDoneRight .\n[65] Y . Izenov. The COMPASS Query Optimizer. https://github.com/yizenov/compass_query_\noptimizer .\n[66] M. Kiefer. join-kde. https://github.com/martinkiefer/join-kde .\n[67] G. Lohman. Is Query Optimization a Solved Problem? https://wp.sigmod.org/?p=1075 , 2014.\n[68] G. Rahn. Join Order Benchmark (JOB). https://github.com/gregrahn/\njoin-order-benchmark .\n[69] F. Rusu. Sketches for Size of Join Estimation. https://faculty.ucmerced.edu/frusu/Projects/\nSketches .\n[70] M. Saecker. MonetDB Ocelot. https://bitbucket.org/msaecker/monetdb-opencl/src/\nsimple_mem_manager/ .\n[71] StackExchange. Distance Between Two Permutations? https://math.stackexchange.com/\nquestions/2492954/distance-between-two-permutations .\n[72] Brytlyt. https://www.brytlyt.com/gpu-accelerated-database/# .\n[73] Apache Calcite. https://calcite.apache.org .\n[74] CoGaDB. http://cogadb.cs.tu-dortmund.de/wordpress/download/ .\n[75] Kinetica. https://www.kinetica.com/products/gpu-accelerated-database/ .\n[76] MapD. www.omnisci.com .\n[77] MonetDB. www.monetdb.org .\n[78] PostgreSQL. www.postgresql.org .\n33",
  "textLength": 118148
}