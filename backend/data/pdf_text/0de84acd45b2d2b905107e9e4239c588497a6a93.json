{
  "paperId": "0de84acd45b2d2b905107e9e4239c588497a6a93",
  "title": "Multiple Set Matching and Pre-Filtering with Bloom Multifilters",
  "pdfPath": "0de84acd45b2d2b905107e9e4239c588497a6a93.pdf",
  "text": "1\nMultiple Set Matching and Pre-Filtering with\nBloom Multiﬁlters\nFrancesco Concas, Pengfei Xu, Mohammad A. Hoque, Jiaheng Lu, and Sasu Tarkoma\nAbstract —Bloom ﬁlter is a space-efﬁcient probabilistic data structure for checking elements’ membership in a set. Given multiple sets,\nhowever, a standard Bloom ﬁlter is not sufﬁcient when looking for the items to which an element or a set of input elements belong to. In\nthis article, we solve multiple set matching problem by proposing two efﬁcient Bloom Multiﬁlters called Bloom Matrix and Bloom Vector.\nBoth of them are space efﬁcient and answer queries with a set of identiﬁers for multiple set matching problems. We show that the\nspace efﬁciency can be optimized further according to the distribution of labels among multiple sets: Uniform and Zipf. While both of\nthem are space efﬁcient, Bloom Vector can efﬁciently exploit Zipf distribution of data for further space reduction. Our results also\nhighlight that basic A DDand L OOKUP operations on Bloom Matrix are faster than on Bloom Vector. However, Bloom Matrix does not\nmeet the theoretical false positive rate of less than 10\u00002for L OOKUP operations if the represented data or the labels are not uniformly\ndistributed among the multiple sets. Consequently, we introduce Bloom Test which uses Bloom Matrix as the pre-ﬁlter structure to\ndetermine which structure is suitable for improved performance with an arbitrary input dataset.\nF\n1 I NTRODUCTION\nModern popular Internet services, including Google search,\nYahoo directory, and web-based storage services, rely on\nefﬁcient data matching [1]. They have custom techniques\nfor providing scalable, fault-tolerant and low-cost services\n[2], [3], [4]. Fast matching of arbitrary identiﬁers to speciﬁc\nvalues is a fundamental requirement of these applications\nin which data objects are organized using unique local or\nglobal identiﬁers, usually called labels [5]. In a typical usage\nscenario, each label maps to a set of values [6]. For example,\nin order to provide low latency for accessing data, such data\nis cached across different regions; given a set of contents,\nthe application needs to ﬁnd to which proxy servers they\nare mapped to.\nA popular probabilistic data structure called Bloom Filter\nserves a similar purpose, namely to answer whether a\nlabel belongs to a particular set. The standard Bloom Filter\nrepresents a set of labels, using a number nof bits. Although\nit is a very space efﬁcient structure, it can only answer\nwhether a label lbelongs to a set e, with a probability of\nfalse positives. There are many extensions of the Bloom\nFilter, such as space-code [7], spectral [8], and Shift Bloom\nFilter [9]. These structures represent a multiset, where a\nlabel can exist multiple times. Therefore, they can answer\nmultiplicity queries, for example, how many times a label\nexists in a multiset.\nLetL=fl1; :::; ljLjgbe a set of labels and E=\nfe1; :::; e Ngbe a set of items. We are interested in represent-\ning the function f:L!P (E), whereP(E)is the power\nset of E. Nevertheless, all the mentioned extensions are not\nable to locate multiple sets when ﬁnding a single or multiple\nlabels. In this study, we present two data structures, namely\nBloom Matrix and Bloom Vector, that can represent such re-\nlations for multiple sets where each set contains a number of\nunique elements. While both structures use multiple Bloom\nFilters to represent multiple sets, Bloom Matrix contains the\nBloom Filters of equal size, whereas the Bloom Filters inBloom Vector can be of different sizes.\nThese new data structures associate multiple sets and\nsupport queries with single or multiple labels through an\ninverted indexing solution using multiple Bloom Filters,\nwhile providing a reasonable accuracy. The most efﬁcient\nLOOKUP (l)operation on them returns a set or list of items,\ni.e.,S2P(E), rather than a simple true or false answer.\n1.1 Example applications\nThe usage of Bloom Filters in networking is widespread.\nThey are suitable to summarize the contents of a P2P\nnetwork for supporting collaborative operations [10]. They\nalso have been applied to enhance probabilistic algorithms\nfor locating resources [11], for taking routing decisions [12],\nand for trafﬁc monitoring [13]. In this subsection, we discuss\nsome applications in which Bloom Multiﬁlters could be\nuseful: load balancing, web caching, and document search.\nLoad balancing. Load balancing is used to optimally dis-\ntribute workloads among multiple computing resources, in\norder to maximize the performance of the whole system [3],\n[4], [14], [15].\nIf a cluster of servers provides multiple services, with\nthose services distributed across different servers, the ser-\nvices can be represented as the labels Land the servers can\nbe represented as the items E, and one of the proposed\nBMFs can be used to map the services to the servers that\nprovide them.\nWeb caching. Much of the modern Internet is composed\nof proxy servers. This allows the workload to be split, also\nproviding redundancy and an extra level of security due to\nthe provider being hidden by the proxy servers. Whenever a\nclient needs to access a service from the provider, the request\nfrom the client is handled by one of the proxy caches.\nIn a proxy cache, Internet contents tend to be small\ncompared to the number of requests. If we represent the\ncontents as labels Land the proxy servers spread across asarXiv:1901.01825v2  [cs.DS]  11 Jan 2019\n\n2\nthe set of items E, we could apply our Bloom Multiﬁlters to\nthe proxy caches for a service.\nDocument Search. Document search with keywords is an-\nother important use case. In a search engine, such as Solr1,\nthe documents go through multiple transformations while\nbeing added, and a series of tokens are generated from\ndifferent ﬁelds of the documents. These tokens are added\nto the index. In this way, Solr achieves faster document\nretrieval. However, indexing different ﬁelds increases the\nsize of the index and slows down the search.\nThe proposed Bloom Multiﬁlter structures can be ap-\nplied as an alternate solution to represent large text corpora.\nFor example, the Wikipedia corpus can be represented as\nthe set of all pages or items, i.e., E. The unique words in a\npage can be the set of labels, L, which can be represented\nas a Bloom Filter for a document. These Bloom Filters could\nalso be encoded as Solr indices rather than the actual words.\nA search query would returns a list of page identiﬁers.\n1.2 Contributions\nIn this article, we present two new Bloom Multiﬁlters\n(BMFs) for fast space-efﬁcient matching of arbitrary iden-\ntiﬁers to sets, at the cost of introducing a false positive\nrate, similarly to Bloom Filters. Our contributions are the\nfollowing:\n\u000fWe introduce two new Bloom Multiﬁlters, namely Bloom\nMatrix and Bloom Vector, to solve the labels-to-sets match-\ning problem. These data structures are inspired by stand-\nard Bloom Filter and some of its extensions (see Section 8),\nin particular, the Bloomier ﬁlter [16] and the Bloom Mul-\ntiﬁlter [17]. Bloomier ﬁlter encodes only one set without\nfalse positives, whereas our Bloom Matrix and Vector en-\ncode multiple sets and can have false positives. Compared\nto the other Bloom Multiﬁlter [17], our Multiﬁlters return\nthe set of items or their identiﬁers instead of a simple true\nor false answer for a L OOKUP operation.\n\u000fWe present theoretical analysis and demonstrate that they\nadhere basic Bloom Filter operations. We evaluate the\nperformance of these structures with different conﬁgur-\nation parameters and the distribution of the labels in\nthe presence of synthetic and 20 Newsgroups corpus test\ndataset [18]. Our results also highlight that basic A DD\nand L OOKUP operations on Bloom Matrix are faster than\non Bloom Vector. While both of them are space efﬁcient,\nBloom Vector can efﬁciently exploit Zipf distribution of\ndata for further space reduction.\n\u000fWe also evaluate their performance according to the de-\nsired false positive rate with a dataset of unknown distri-\nbution of labels. We demonstrate that Bloom Matrix can be\nused to determine whether such dataset follows Uniform\ndistribution or not. We introduce Bloom Test to determine\nthe appropriate Multiﬁlter according to a desired false\npositive rate lower than 10\u00002. In other words, Bloom\nMatrix can be used as a pre-ﬁlter structure to model the\ninput data with multiple sets.\nWe organize the rest of the article as follows. In Section 2,\nwe introduce the readers to standard Bloom Filter and its\nproperties. In Section 3, we describe the problem. Section 4\n1. http://www.solrtutorial.com/basic-solr-concepts.htmland Section 5 present the deﬁnitions and theoretical analysis\nof the Bloom Matrix and Vector. In Section 6, we investigate\nthe performance of our Bloom Multiﬁlters and introduce\nBloom Test in Section 7. We outline the related works in\nSection 8 before concluding the paper.\n2 P RELIMINARIES\nNotations Implications\nl; L a label and a set of labels\ne; E an item and set of items\nh; H a hash function and a set of hash functions\nM the range of hash neighborhoods\nN the total number of items\nm the number of bits in one Bloom Filter, calculated\nfrom Eq. 2a\nk the number of hash functions in one Bloom Filter,\ncalculated from Eq. 2b\nn the number of items inserted to the structure spe-\nciﬁed in the context\nTable 1: Notations used for Bloom Multiﬁlters.\nBloom Filter [19] is a probabilistic data structure which\nrepresents a set so that the set will occupy much less\nmemory space than it normally would when represented\nwith conventional methods. This comes at the cost of intro-\nducing a False Positive (FP) rate. False Negatives (FNs), on\nthe other hand, are not allowed. In this section, we outline\nthe Bloom Filter and other variations of Bloom Filter. Table 1\nsummarizes the notations used to describe Bloom Filter and\nBloom Multiﬁlters in this article.\n2.1 Bloom Filter\nDeﬁnition. LetUbe a set of all the items that we can\npossibly store, and let E\u0012Ube the set that we wish\nto represent. We are interested in encoding the function\nf:U!f0;1gdeﬁned as:\nf(x) =(\n1ifx2E\n0otherwise\nThe Bloom Filter is deﬁned as a pair (B; H ), where B\nis a bitset of size mandH=fh1(x); :::; h k(x)gis a set\nof hash functions, each having image [0; m\u00001]. There are\ntwo operations on Bloom Filter: A DD(x)and L OOKUP (x). A\nremove or delete operation on Bloom Filter would introduce\na chance of FN.\nThe set of distinct values returned by all the hash func-\ntions for an input label xis called its hash neighborhood ; we\ndeﬁne it as H(x)with the abuse of notation.\nAdd. When a Bloom Filter is created, all the bits in bitset B\nare initialized to 0. Whenever we add a label or element x,\nwe set to 1 each B[i]for each i2H(x).\nLookup. To test the membership of an element x, we have\nto check whether all of the bits B[i]for each i2H(x)are\nset to 1. If it is true, then the element is probably in the set;\notherwise, it is deﬁnitely not in the set.\nFalse positive rate. FPs occur whenever we look for an\nelement xwhich is not in the set, and the L OOKUP function\nreturns true. Such function returns true whenever all the\nbits are having as indices the neighborhood of xis set to 1.\n\n3\nThis implies that the more bits are set to 1, the higher is the\nFP rate. The number of hash functions also inﬂuences the\nFP rate. A higher number of hash functions decreases the\nchance of collisions between two different elements. There-\nfore, the choice of an optimal number of hash functions is a\ncompromise.\nBose et al. [20] have shown that the probability pof false\npositives in a Bloom Filter of size m,khash functions, and\nhave added nelements is:\np= \u00020\n@\"\n1\u0000\u0012\n1\u00001\nm\u0013kn#k1\nA= \u0002\u0012\u0010\n1\u0000e\u0000kn=m\u0011k\u0013\n(1)\nIf we know a priori the number of elements that we\nare going to insert in a Bloom Filter, we can choose its\nparameters so that the Bloom Filter will have a probability\nof false positives around a certain value p. We derive from\nEquation 1:\nm=\u0000nlnp\nln22(2a)\nk= ln 2\u0001m\nn=\u0000log2p (2b)\n3 P ROBLEM DEFINITION\nStandard Bloom Filter and it’s extensions can encode only\none set and can answer whether a label belongs to a set\nor not. In this article, we extend standard Bloom Filter not\nonly to encode multiple sets and to efﬁciently check the\nmembership of an element in all the sets but also to answer\nto which of the sets such element belongs to, i.e. a set of\nidentiﬁers. We discuss the related works in Section 8.\n3.1 Deﬁnition\nLetL=fl1; :::; ljLjgbe a set of labels and E=fe1; :::; e Ng\nbe a set of items. We are interested in representing the\nfunction f:L!P (E), whereP(E)is the power set of\nE.\nThe most straightforward approach is to use a Bloom\nFilter to store the labels associated with each item. We\nalso show another approach that has both advantages and\ndisadvantages compared with the former. The idea of the\nlatter approach is to represent the function fsimilarly to\na Bloom Filter. However, instead of using single bits to\nencode the elements, we use bitsets in which we store binary\nrepresentations of the element s2P(E)to which the labels\nmap to. We obtain these representations with two functions;\nENCODE and D ECODE .\n3.2 Encode and Decode\nLet\u0005be an ordering on E. We introduce E NCODE (\u0005; S),\nwhich returns a binary representation V=fv1; :::; v NgofS\ngiven an ordering \u0005ofEand a set of items S2P(E), such\nthat:\nvi=(\n1if\u0005(i)2S\n0otherwise :\nWe also introduce D ECODE (\u0005; V), which returns Sgiven an\nordering \u0005ofEand a binary representation Vof a set of\nitems S2P(E).ENCODE and D ECODE essentially associate each element\nto a bit in a binary representation, as we can see in the\nfollowing Example 3.1:\nExample 3.1. LetE=fe1; e2; e3gand\u0005being an ordering\nonE, which in this case orders the element as written in the\ndeﬁnition of E. We have:\nENCODE (\u0005;fe1; e3g) =f1;0;1g\nDECODE (\u0005;011) =fe2; e3g\nAnalysis of Encode and Decode. The E NCODE (\u0005; S)func-\ntion needs to initialize a bitset Vto 0 and to set some bits\nto 1, in order to return the encoded value of S. The space\ncomplexity is, therefore, \u0002(N). For each element in S, we\nneed to set V[i] 1, where iis the index of Sin the\nordering \u0005. Since we are using \u0005, the time complexity is\nO(jSj).\nThe D ECODE (\u0005; V)function needs to create a set from\nthe bitset V, which can be at most N. Therefore it takes\nO(N)space. For each bit viset to 1 in V, the function fetches\nthe element at position iin the array and adds it to the set.\nIf we need to return an ordered set, it takes O(jVjlogjVj)\ntime; otherwise it takes O(jVj)time. In the rest of this thesis,\nwe assume that we do not need to return an ordered set.\n4 B LOOM MATRIX\nIn this section, we introduce Bloom Matrix as our ﬁrst effort\nto solve the membership checking problem with multiple\nsets. Precisely, it consists of multiple columns of bitsets, in\nwhich each column represents an item, and values of its bits\nare determined by associated labels, e.g., the set of unique\nwords (as labels) in a document (as an item). In other words,\nunlike Bloom Filter, each bit in the Filter is replaced by\nanother bitset of a ﬁxed length, hence the name is Bloom\nMatrix.\n4.1 Deﬁnition\nWe deﬁne a Bloom matrix as a triplet (G;\u0005; H), whereGis\na binary matrix of size m\u0002N,\u0005represent an ordering on\nthe set Eas previously deﬁned, and H=fh1(x); :::; h k(x)g\nis a set of hash functions, each having image [0; m\u00001]. If\nusing MurmurHash, we can replace Hwith a number of\nhash functions k, and use as seeds for MurmurHash the\nrange [1; k]. In the rest of the thesis we replace Hwithk.\n4.2 Operations\nAdd.Gis initialized with all its bits set to 0. In order to add\na label lto a Bloom Matrix F, we add the value returned\nby E NCODE (\u0005; f(l))to the rows in the bit matrix Ghaving\nthe indices equal to the hash neighborhood of l, using the\nbitwise OR operator. The add operation can be formally\ndeﬁned as:\nF:ADD(l) :=G[hi(l);] G[hi(l);]_ENCODE (\u0005; f(l))\nfor1\u0014i\u0014k\nThe steps are illustrated in Alg. 1. Suppose that we want\nto add some label lto the Bloom Matrix. We obtain a bitset,\nV, from E NCODE (\u0005; f(l))in which the bits set to 1 are at\n\n4\n  1   1  \n1 1 1   1\n         \n1 1     1\n  1 1 1 1Document ids: NHash value ranges: MH(l 1)\nH(l 2)\nH(l 3)0 1 2 3 4 \n0 \n1 \n2 \n3 \n4 \n5 \n6 \n7 \nEncode(Π, f(l 1)Encode(Π, f(l 2)Encode(Π, f(l 3)         \n                  Doc id\nordering:\n0: e 1\n1: e 2\n2: e 3\n3: e 4\n4: e5\nFigure 1: Example of a Bloom matrix G[m; N ]obtained from\nExample 4.1.\nthe positions V=fv1; :::; vjVjg, which represent columns\nof the Bloom matrix. We next obtain a set of indices H=\nfh1(l); :::; h k(l)gfrom the hash functions, which represent\nrows of the Bloom Matrix. Therefore, the A DD(l)function is\ngoing to set to 1 all the bits whose indices are given by the\nCartesian product V\u0002H.\nLookup. In order to ﬁnd out which subset of P(E)is\nlabelled with l, we use the D ECODE function on the bitset\nresulting from the bitwise AND operation on the rows in G\nhaving the indices equal to the hash neighborhood of l:\nF:LOOKUP (l) := DECODE0\n@\u0005;^\n1\u0014i\u0014kG[hi(l);]1\nA\nMultiple labels lookup. The lookup operation is illustrated\nin Alg. 2. Similarly, we can lookup for multiple labels by\ncomputing the hash neighborhood of all the labels, the rest\nof the lookup algorithm is identical to the algorithm for\nlooking up a single label.\nAlgorithm 1 Bloom Matrix: Add Operation\n1:procedure ADD(l)\n2: V ENCODE (\u0005; f(l))\n3: H GETNEIGHBOURHOOD (l; k; m )\n4: fori Hdo\n5: G[i;] G[i;]_V\n6: end for\n7:end procedure\nAlgorithm 2 Bloom Matrix Lookup\n1:procedure LOOKUP (l)\n2: H GETNEIGHBOURHOOD (l; k; m )\n3: LetVbe an empty bitset\n4: fori Hdo\n5: V V^G[i;]\n6: end for\n7: return DECODE (\u0005; V)\n8:end procedureExample 4.1. Given 3 labels L=fl1; l2; l3g, 5 items\nE=fe1;\u0001\u0001\u0001; e5g, an ordering \u0005that sort items in in-\nput order, and two hash functions H=fh1(x); h2(x)g\nreturns [0;7]such that H(l1) =f0;7g,H(l2) =f2;4gand\nH(l3) =f2;7g. The labels can be assumed as the words and\nitems can be assumed as the documents.\nWe now represent f(l1) =fe2; e4g,f(l2) =fe1; e2; e5g\nandf(l3) =fe3; e5gwith a Bloom Matrix. As showing\nin Figure 1, when adding l1, we determine target rows\nG[0;]andG[7;]according to H(l1) =f0;7g, then run\nENCODE function to ﬁnd two columns G[;2]andG[;4]\nsince E NCODE returnsf0;1;0;1;0;0g. We ﬁnally set four\nbits to 1:G[0;2],G[7;2],G[0;4]andG[7;4].l2andl3can\nbe added by the same manner.\nWhen preforming L OOKUP (l1), we do hash operation\nH(l1) =f0;7gand then AND two rows: G[0;]andG[7;].\nThis operation returns f0;1;0;1;0;0g. Therefore, D ECODE\nreturnsfe2; e4g. Noteworthy, L OOKUP (l3)perform AND\noperation on G[2;]andG[7;]which outputsfe2; e3; e5g,\nwith a FP e2.\nUpdate. Bloom Matrix is a ﬁxed structure for the sets. It is\nnot possible to update the Matrix with the labels of a new\nitem and it requires reconstruction of the Matrix. However,\nit possible to add a label to an existing item at a cost of an\nincreasing FP rate.\n4.3 False Positive Rate\nLemma 4.1. Given a set of labels L=fl1;\u0001\u0001\u0001; ljLjgwhere\neach lis associated with some items e. The total false\npositive rate of a Bloom Matrix when performing multiple\nlabel lookup on L, L OOKUP (L)is\nFPR=P\nl2LP\ne2Enf(l)(1\u0000(1\u00001\nm)nk)k\njLj(3)\nwhere kis the number of hash functions mis the range of\nhash neighborhoods, nis the number of items added to the\nBloom Matrix, and Eis the universe of all items.\nProof. Given khash functions, Bloom Matrix’s L OOKUP op-\neration performs AND operation on krows, which indexes\ndetermined by the value of hash neighborhoods. Therefore,\na false positive in this case is when the bit at column c, i.e.,\nbits of a document id in Figure 1, of all krows are set to 1,\nwhere cis a positive integer.\nWhen executing A DDoperation for a label l, a hash\nfunction sets bits uniformly to one of mrows for each item\ne2f(l). Therefore, for any single bit in Bloom Matrix, the\nprobability that it is not set to 1bykhash functions during\none A DDoperation is\n(1\u00001\nm)k(4)\nAssume that A DDoperation is performed on n=jf(l)j\nitems, i.e., we have added a label nitems, then probability\nthat the bit is still 0is\n(1\u00001\nm)nk\nIn contrast, the probability that the bit is set to 1is\n1\u0000(1\u00001\nm)nk(5)\n\n5\nNow assume that there is a false positive item efas-\nsigned to a label lfor the L OOKUP operation. Then, there\narekbits at a single column, whose index corresponding to\nef, and krows, whose index determined by hash functions,\nare set to 1. This happens with probability\n(1\u0000(1\u00001\nm)nk)k(6)\nIn other words, when we perform L OOKUP operation\nfor a label l, each returned items has a probability equals\nto Equation 6 for being a false positive. Since efis the\nmember of e2Enf(l), then any member of Enf(l)has\nthe probability of being false positive, i.e.,\nX\ne2Enf(l)(1\u0000(1\u00001\nm)nk)k: (7)\nFurthermore, for multiple label lookup in L, the overall\nfalse positive rate for looking up all labels l2Lcan be\nobtained by summing up the probabilities for all items not\nin any f(l), and the average false positive rate is,\nFPR=P\nl2LP\ne2Enf(l)(1\u0000(1\u00001\nm)nk)k\njLj(8)\n4.4 Complexity\nLemma 4.2. Bloom Matrix has a space complexity \u0002(mN),\nADDtime complexity O(Nlogjej)and L OOKUP complexity\n\u0002(kN), where mis the size of hash neighborhood, kis the\nnumber of hash functions, Nis the total number of items\nandjejis the number of input items in an A DDoperation.\nProof. Space complexity: Bloom Matrix stores three com-\nponents: (i) the bitset that has size m\u0002N, (ii) The total\nordering \u0005, which has size 2N, required by E NCODE and\nDECODE functions, and (iii) khash functions. The space\ncost is therefore \u0002(mN) + 2\u0002( N) + \u0002( k) = \u0002( mN)when\nk\u001cmN .\nTime complexity: an A DD(l; e)operation on the Bloom\nMatrix computes the neighborhood of land executes\nENCODE (\u0005; e)to obtain a bit sequence V, then updates the\nmatrix according to V. The E NCODE takes \u0002(jej)time, while\nthe hash operations take \u0002(k)time in total. Then, insertion\nin the matrix takes \u0002(N)time because there are exactly k\nbits in each column altered (with a constant operate time),\nwhile there are at most Ncolumns. The time of A DDis\ntherefore \u0002(k) + \u0002(jej) + \u0002( k) = \u0002(jej+k).\nA L OOKUP (l)operation computes the neighborhood of l\nand executes D ECODE (\u0005; V)on the Vobtained by bitwise\nAND operation on the krows, each has at most Nbits. The\ntime cost is \u0002(N) + \u0002( kN) = \u0002( kN).\n4.5 Sparse Bloom Matrix\nBloom Matrix can be sparse , where some or the bits are zero.\nThis allows us to use a sparse storage method, to make its\nspace cost less than m\u0002N. Furthermore, it is possible to\nfurther reduce the space cost by carefully selecting the total\nordering so that sparse rows of the Bloom Matrix, have more\nzeros at the end, because trailing zeros can be spared by\nsparse vectors to save space. We name a Bloom Matrix with\nsuch ordering a Sparse Bloom Matrix . We use the following\n1   1    \n1 1   1 1\n         \n1 1     1\n1 1 1 1  N\nM0 1 2 3 4 \n0 \n1 \n2 \n3 \n4 \n5 \n6 \n7          \n                  Doc id\nordering:\n0: e 2\n1: e 5\n2: e 4\n3: e 3\n4: e 1Figure 2: A Sparse Bloom Matrix constructed from Figure 1\nby modifying the total ordering. The space cost reduces from\n19 to 17 bits.\nexample to illustrate the efﬁciency of Sparse Bloom Matrix\nover simple Bloom Matrix:\nExample 4.2. Take Example 4.1 as an example. The Bloom\nMatrix in Figure 1 spares 19 bits (cells with grey back-\nground) according to the ordering in Example 4.1. In con-\ntrast, if the ordering is replaced by \u00050such that items are\nordered byfe2; e5; e4; e3; e1g, the new Sparse Bloom Matrix\ncan be constructed as in Figure 2, which spares 2 more bits\nthanks to more zeros at the end of each vector.\nThe construct of a Sparse Bloom Matrix is straightfor-\nward: one can choose a total ordering \u0005that sorts items in\nsetEin decreasing order of the number of assigned labels.\nThis maximizes the probability of having more zeros at the\nend when using E NCODE . More formally, let C(e)be the\nnumber of labels assigned to an item e. We have to deﬁne\nthe total ordering \u0005so that \u0005(ei)>\u0005(ej)iffC(ei)\u0014C(ej).\nLater in Section 6, we will see that such ordering can archive\nin average 20% reduction for space occupation.\nSBM Complexity. In the case of a Sparse Bloom Matrix, the\nADDand L OOKUP operations are the same. However, the\ninitialization operation changes, because we need to com-\npute the ordering \u0005. The speed of this operation depends\non how the dataset is represented. If we represent it as\nan array ofjLjitems, each item representing a l2Land\ncontaining a set of items of E, we need to scan the whole\nstructure keeping a counter for each e2E. The time would\nbe therefore \u0002(jLjN). If we represent it as an array of N\nitems, each item representing a e2Eand containing a set\nof labels of L, and if each set has a precomputed size, the\ntime would be \u0002(N).\n5 B LOOM VECTOR\nAs we have seen in Section 2, if we know a priori the number\nof items to be inserted in a Bloom Filter, we can choose\nits parameters so that the probability of FP remains around\na chosen value. Unlike Bloom Matrix, we aim for a new\ndata structure with multiple variable length Bloom Filters,\nsigniﬁcantly reducing the memory overhead. We call this\ndata structure Bloom Vector. Therefore, we can expect Bloom\nVector to acquire much less space by FP rate than Bloom\nMatrix. We will see this in practice in Section 6.\n5.1 Deﬁnition\nWe deﬁne a Bloom Vector as a tuple (G;\u0005), where Gis\nanassociative array of size N=jEjin which each item\n\n6\ncorresponds to a Bloom Filter, and \u0005represents an ordering\non the set Eas previously deﬁned. As already stated, the\nBloom Filters in Gcan have different sizes and different\nhash functions from each other.\n5.2 Operations\nLet us now deﬁne the operations on the Bloom Vector, which\nare based on the operations on Bloom Filter.\nAlgorithm 3 Bloom Vector: Add Operation\n1:procedure ADD(l,e)\n2: V ENCODE (\u0005; e)\n3: I V:toList\n4: fori Ido\n5: G[i]:ADD(l)\n6: end for\n7:end procedure\nAdd. In order to add a label lto the subset ofP(E)given by\nf(l), we compute E NCODE (\u0005; f(l)). Let us call Ithe indices\nof the bits set to 1 in E NCODE (\u0005; f(l)). The add function on\nthe Bloom vector F, is deﬁned as:\nF:ADD(l) :=G[i]:ADD(l)8i2I\nThe add operation on Bloom Vector is illustrated in\nAlg. 3. Suppose that we add the label lto a Bloom Vector.\nThe A DD(l)operation executes only on the rows V=\nfv1; :::; vjVjgcorresponding to the bits set to 1 in the value\ngiven by E NCODE (\u0005; f(l)). The bits to be set to 1 in each of\nthose rows, however, are now determined by a new set of\nhash functions H(l; m) =fh1(l; m); :::; h k(l; m)g, where m\nbe the number of bits of the corresponding row.\nNoteworthy, Bloom Vector does not require having the\nsame hash functions for each row (i.e., for each Bloom\nFilter), because either A DDor L OOKUP uses bits only a\nspeciﬁed row decided by E NCODE . However, considering\nthe maintainability and the difﬁculty of implementation,\none often like to use the same hash functions for all Bloom\nFilters. To achieve this goal, one needs to make an extension\nto the hash functions Hbecause it becomes it returns hash\nneighborhoods in a speciﬁc range, which becomes infeasible\nin a Bloom Vector where all Bloom Filters can have different\nlengths. As a solution, we can attach the maximal valid\ninteger as a parameter of hash function, so that H(l; m) =\nfh1(l; m);\u0001\u0001\u0001; hk(l; m)greturns kthe hash neighborhoods\nwithin the range [0; mk). Then, during A DDoperation, we\nassign each mas the number of bits in each row to ensure\nthe output of H(l; m)can always be mapped to a valid bit.\nIn practice, this new function can be implemented easily\nby regulating the output of a hash algorithm h(l)using its\nmaximum possible value, e.g., h(l; m) =bh(l)=232\u0002mc\nwhen h(l)is an 32-bit MurmurHash.\nLookup. Alg. 4 describes the lookup operation on Bloom\nVectors. Similarly, in order to ﬁnd out which subset of P(E)\nis labelled with l, letVbe a bitset deﬁned, for 1\u0014i\u0014Nas:\nV[i] :=(\n1ifG[i]:LOOKUP (l)\n0otherwiseAlgorithm 4 Bloom Vector: Lookup Operation\n1:procedure LOOKUP (l,e)\n2: LetVbe an empty bitset\n3: fori [0; N)do\n4: ifG[i]:LOOKUP (l)then\n5: V[i] 1\n6: end if\n7: end for\n8: return DECODE (\u0005; V)\n9:end procedure\nThe L OOKUP operation is deﬁned as:\nF:LOOKUP (l) := DECODE (\u0005; V)\n    1     1\n    1         1\n               \n    1       1 1Hash range: MDoc ids: NEncode(Π, f(l 1))\nEncode(Π, f(l 2))0 \n1 \n2 \n3 \n4 0 1 2 3 4 5 6 7 \nH(l 1)H(l 2)    1       1  Doc id\nordering:\n0: e1\n1: e2\n2: e3\n3: e4\n4: e5\nFigure 3: Example of a Bloom vector G[N;], obtained from\nExample 5.1. The size of each Bloom Filter in the vector can\nbe different.\nExample 5.1. Given 2 labels L=fl1; l2g, 5 items E=\nfe1;\u0001\u0001\u0001; e5g, an ordering \u0005that sort items in input order,\nand two hash functions H=fh1; h2greturning integers\nsuch that H(l1;6) =f2;5g,H(l1;8) =f2;6g,H(l2;6) =\nf2;5g, and H(l2;8) =f2;7g. The labels can be assumed as\nthe words and items can be assumed as the documents.\nWe now represent f(l1) =fe1; e2; e5gandf(l2) =\nfe3; e5gwith a Bloom Vector. As showing in Figure 3,\nwhen adding l1, ENCODE functions returns f1;1;0;0;1g\nand hence we have three target rows G[1;],G[2;]and\nG[5;]. Then, we perform a hash operation by having row\nlength as input: H(l1;6) =f2;5gandH(l1;8) =f2;6gto\nobtain Columns 2 and 5 for Row 0, and Columns 2 and 6 for\nRows 1 and 4. Finally, we need to set six bits: G[0;2],G[0;5],\nG[1;2],G[1;6],G[4;2], andG[4;6].l3can be added to the\nBloom Vector by the same manner.\nWhen preforming L OOKUP (l2), we ﬁrst build a empty\nbitset Awith length N. We scan each row by checking the\nbit at position determined by the hash function. Speciﬁcally,\nfor the ﬁrst row, we get H(l2;6) =f2;5gto check the bits at\nColumns 2 and 5. Since both bits are 1, we mark the 1st bit\nofAas1. For other rows, we get H(l2;8) =f2;7gto check\nColumns 2 and 7, and thereafter marks A’s 3rd and 5th bits\nas1. Finally, we run D ECODE onAand getfe1; e3; e5g, in\nwhich e1is a false positive.\nMultiple label lookup. We can look up which items contain\nmultiple labels by using the same algorithm for a single\nlookup, using in each Bloom Filter, the function for looking\nup multiple labels.\n\n7\n5.3 False positive rate\nLemma 5.1. Given a set of labels L=fl1;\u0001\u0001\u0001; ljLjgwhere\neach lis associated with some items e. The average false\npositive rate of a Bloom Vector, when performing L OOKUP\nfor all the labels in L, is\nFPR=P\nl2LP\ni2I(l)\u0014\n1\u0000\u0010\n1\u00001\nmi\u0011kini\u0015ki\njLj(9)\nwhere I(l)returns a set of row indices corresponding to\npositions of 1’s in the output of E NCODE (l),ki,miandni\nare the number of hash functions, the number of bits, and\nthe number of labels added to i-th Bloom Filter, respectively.\nProof. The L OOKUP operation on a Bloom Vector goes\nthrough each row to check whether the bits at columns\ngiven by khash functions are all 1. Therefore, a false positive\nin this case is when all kbits are being set to 1when adding\nother items into the structure.\nRecall Equation 5. Given an arbitrary Bloom Filter with\nmbits, khash functions, and nadded labels, its false\npositive rate is\n(1\u0000(1\u00001\nm)nk)k(10)\nThen, let miandkbe the parameters of i-th Bloom Filter\nin a Bloom Vector, and let nibe the number of labels that the\ni-th Bloom Filter contains. We can use Equation 10 to derive\nthe total expected FP rate when looking up a label l:\nFPR=X\ni2I(l)\"\n1\u0000\u0012\n1\u00001\nmi\u0013kini#ki\n(11)\nwhere I(l)returns the index of Bloom Filters changed when\nadding l. Formally, it returns a set of row indices corres-\nponding to positions of 1’s in the output of E NCODE (l).\nFinally, given multiple labels Lfor looking up, the aver-\nage FP rate is\nFPR=P\nl2LP\ni2I(l)\u0014\n1\u0000\u0010\n1\u00001\nmi\u0011kini\u0015ki\njLj(12)\n5.4 Complexity\nCompared with Bloom Matrices, Bloom Vectors cost less\nspace but perform slower lookups due to the traversal of\nall contained Bloom Vectors.\nLemma 5.2. Bloom Vector has a space complexity O(mN),\nADDtime complexity O(jVjk)and L OOKUP complexity\n\u0002(kN), where mis the max size of hash neighborhood\namong all rows, kis the max number of hash functions for\neach rows, Nis the total number of items.\nProof. Space: a Bloom Vector needs to store three compon-\nents: (i) m\u0002Nin the worst case when all Nrows have an\nequal length m, (ii) The total ordering \u0005, which uses 2N\nspace, and (iii) kNhash functions. Therefore, the space cost\nisO(mN)+2\u0002( N)+O(kN) =O(mN)when k\u001cm. Note\nthat this bound is not tight because different rows in the\nBloom Vector can have different lengths, and hash functionscan be reused for more than one rows if they have the same\nlength.\nTime complexity: the A DD(l; e)operation needs to com-\npute the ordering Vby executing E NCODE (\u0005; e), and then\nperform A DDoperation to each row corresponding to V.\nEach A DDrequires O(k)time for hash functions. Therefore,\nthe total time is O(jVj)\u0001O(k) =O(jVjk)time, since time\nfor updating one bit is negligible.\nA L OOKUP (l)operation needs to go through all rows.\nFor each row, it needs to calculate O(k)hash neighborhoods.\nTherefore, The total time is N\u0001O(k) =O(kN).\nUpdate. Unlike Bloom Matrix, each Bloom Filter in a Bloom\nVector has its own parameters. The A DDoperation is per-\nformed on an individual ﬁlter. Therefore, it is possible to\nadd new items incrementally to a vector and so the cor-\nresponding labels. Updating an already existing Filter in a\nvector increases the FP rate.\n6 P ERFORMANCE EVALUATION\nWe chose Scala as the implementation language. We adapted\nthe operations of both Bloom Multiﬁlters to Scala, using aux-\niliary functions and taking advantage of the Map-Reduce\nparadigm. We evaluated the performance of our Bloom Mul-\ntiﬁlters with three different datasets. In this section, we ﬁrst\ndemonstrate the performance with synthetic datasets and\nthen with a small real dataset used in various researches.\nThe experiments were conducted on a machine with a quad-\ncore processor at 2.3 GHz with eight logical processors, 16\nGB of RAM at 1.6 GHz, and a 512 GB SSD.\n6.1 Dataset Generation\nIn the ﬁrst set of experiments, we used artiﬁcial datasets.\nThis is useful to experiment with the behavior of Bloom\nMultiﬁlters with different data distribution types. We imple-\nmented two functions. One function generates data having\nuniform distributions, and the other generates Zipf distri-\nbutions. It is intuitive that Bloom Matrix is suitable for a\ndataset with Uniform distribution, as every Bloom Filter in\nthe matrix are of equal size and so input sets. On the other\nhand, Bloom Vector is suitable for Zipf distribution, as the\nsize of every Bloom Filter can be different depending on\nthe size of the input sets. In Section 7, we verify this with\nanother set of experiments with a real dataset of unknown\ndistribution.\nName Number of labels File size\nUniform\u00182500000 12.2 MB\nZipf\u001830000 171 kB\nTable 2: Sizes of data sets used in the experiments.\nUniform. The algorithm that generates uniform distribu-\ntions, given E,L, and a probability p, for each e2E, for\neachl2L, decides with a probability of pwhether to assign\nsuch label ltoeor not. We used p= 0:5.\nZipf. The algorithm that generates Zipf distributions, given\nE,Land a real number s, generates the ﬁrst jEjZipf rank\nnumbers with exponent value sandN=jEj, following the\nequation:\nf(k;s; n) =1=ks\nPN\ni=1(1=ns)(13)\n\n8\n102\n103\n104\n105510152000:51\nm\nkFP Ratem=f100;\u0001\u0001\u0001;143800g\nUniform\n101\n102\n103\n104510152000:51\nm\nkFP Ratem=f10;\u0001\u0001\u0001;43100g\nZipf\nFigure 4: The performance of Bloom Multiﬁlters with respect\nto the number of hash functions, the size of the Bloom\nMultiﬁlters, and the distribution of labels. All the structures,\nBM, SBM, and BV, have similar characteristics.\nwhere s= 0:8. Then, for each ek2E, for each l2L, the\nalgorithm decides with a probability equal to the rank k\nwhether to assign such label ltoeor not.\nTable 2 illustrates the properties of the synthetic datasets\ngenerated by the above methods, with jEj= 500 for both\ndatasets. Both functions save the generated data into CSV\nﬁles. Each row of the ﬁle contains the name of an element\nas the ﬁrst value and the names of all the labels assigned to\nit following in the same row. This is similar to a row having\na document id and the unique words in the document.\n6.2 Bloom Multiﬁlters comparison\nWe evaluate and compare the performance of our Bloom\nMultiﬁlters, Bloom Matrix (BM), Spare Bloom Matrix (SBM),\nand Bloom Vector (BV), with above two datasets. We ﬁrst\ninvestigate their FP rates with respect to the number of hash\nfunctions, k, and their sizes, m. Therefore, we construct the\nBloom Multiﬁlters with different combinations of mandk.\nNext, we compare their performance for memory overhead,\nADD, and L OOKUP operations with respect to various FP\nrates.10\u0000710\u0000410\u0000110\u0000310\u0000210\u00001100\nFP RateAvg. Add Time (ms)Uniform\nBM\nSBM\nBV\n10\u0000510010\u0000410\u0000310\u0000210\u00001100\nFP RateZipf\nFigure 5: Time required to execute an A DDoperation or to\nadd a label to Bloom Multiﬁlters by FP rate, using optimal m\nandkfor each point.\n10\u0000610\u0000310051015\nFP RateMemory Overhead (MB)Uniform\nBM\nSBM\nBV\n10\u0000510010\u0000310\u0000210\u00001100101\nFP RateZipf\nFigure 6: Memory usage by FP rate, using optimal mand\nkfor each point. The horizontal dashed line represents\nmemory usage by conventional data structures representing\nthe data.\nFalse Positive Rates. As we can see in Figure 4, the FP\nrate decreases as kincreases, however, increases again after\na certain number of hash functions, as expected from our\ntheoretical analysis in the earlier sections. All the Bloom\nMultiﬁlters perform the same.\nThe FP rates of BM and SBM also decrease as min-\ncreases for both Uniform and Zipf distributions as shown\nin Figure 4. With Zipf distribution, they can achieve lower\nFP rates with smaller sizes. It is difﬁcult to demonstrate\nsuch relations for a Bloom Vector when the size of each\nBloom Filter is different in the vector. Nevertheless, if all\nthe vectors are of equal size and have the same number of\nhash functions, then Bloom Vector also depicts exactly the\nsame performance. In this case, for every combination of m\nandk, all the Bloom Filters in the Vectors are of equal size.\nADDOperation Times Vs FP Rates. In Figure 5, we notice\nthat the A DDoperation takes near a linear time on Bloom\nMatrix with Uniform distribution. This is expected as an\nADDoperation costs O(Nlogjej). The operation time on\nBloom Vector is linear too with Zipf distribution. However,\nthe A DDoperation takes more time on Bloom Vector than\nBloom Matrix with both distributions. This is because the\n\n9\n10\u0000710\u0000410\u0000110\u0000210\u00001100\nFP RateAvg. Lookup Time (ms)Uniform\nBM\nSBM\nBV\n10\u0000510010\u0000310\u0000210\u00001100\nFP RateZipf\nFigure 7: Lookup time of a single label on Bloom Multiﬁlters\nwith different FP rates. We used optimal mandkfor each\npoint.\ntime depends on the size of the vector that it uses to store\nthe result of E NCODE operation, i.e., V ENCODE (\u0005; e),\nmultiplied by k(O(jej+jVjk)).\nMemory Overhead Vs FP Rate. Figure 6 demonstrates the\nmemory overhead of the structures as the FP rate decreases.\nWe notice that all the Bloom Multiﬁlters are well below the\nhorizontal line when representing the dataset of Uniform\ndistribution. Bloom Vector occupies a similar space to its\nbasic counterpart, i.e., the Bloom Matrix. This is because all\nthe Bloom Filters of Bloom Vectors are of the same size;\ntherefore, both structures have similar parameters. On the\nother hand, the Sparse Bloom Matrix occupies the least\nspace; the Sparse Bloom Matrix sets the ordering on the\nsetEto maximize the number of zeros at the end of each\nrow. This makes it the most suitable Bloom Multiﬁlter to\nrepresent uniformly distributed data if our goal is to spare\nas much space as possible.\nOn the other hand, with Zipf distributed dataset, both\nBloom Matrix, and Sparse Bloom Matrix perform poorly\nconcerning space. Although the Sparse Bloom Matrix per-\nforms better than Bloom Matrix, they both are above the\ndashed line for a speciﬁc FP rate. Bloom Vector is the most\nspace efﬁcient with a Zipf distribution and remains below\nthe dashed line even with an FP rate of 10\u00006. The ex-\nplanation is simple: Bloom Vector uses different optimized\nsizes for each row, i.e., mis different for different Bloom\nFilters, where each row represents the labels associated with\na particular element e2E. Therefore, with sparse rows of\nthe distribution, it does not waste space as the other Bloom\nMultiﬁlters do.\nLOOKUP Operation Times Vs FP Rates. Once the Bloom\nMultiﬁlters are constructed, we perform L OOKUP operation\non the structures. In Figure 7, we can see that Both Bloom\nMatrix and Sparse Bloom Matrix have a similar lookup time,\nand they are faster than Bloom Vector in all situations. This\nis because the lookup time of the Bloom Matrix depends\nonly on k, (\u0002(k)). The Bloom Vector takes much more time\nand has a linear time too ( O(Nk+jVj)). This might be\nbecause the L OOKUP operation requires to iterate of a E\nnumber of Bloom Filters in the Vector, and ends up having\nan increasing workload.10\u0000610\u0000510\u0000410\u0000310\u0000210\u00001100100\nFP RateMemory Overhead (MB)BM\nSBM\nBV\nFigure 8: Memory usage by FP rate. The horizontal dashed\nline represents memory usage by conventional data struc-\ntures representing 20ng-test-stemmed data.\n10010110210310\u0000510\u0000410\u0000310\u00002\n# Lookup LabelsAvg. Lookup Time (ms)BM\nSBM\nBV\nFigure 9: Lookup time for one and multiple labels from the\nBloom Multiﬁlters at an expected FP rate of 0.1.\nSometimes it is crucial to ﬁnd labels sharing multiple\nitems. Instead of iterating over a list of labels, search them\ntogether. Multiple label lookup with our Bloom Multiﬁlters\nallows us to ﬁnd out the sets to which multiple labels belong\nin common without doing a lookup of each label. We present\nthe multiple label lookup with real datasets in the next\nsection.\n6.3 Discussion\nOur Bloom Multiﬁlters obey the principle of standard Bloom\nFilter. Their space efﬁciency depends on the distribution of\nthe labels in the input dataset and Bloom Vector is the most\nspace efﬁcient with Zipf distributed data. The performance\nof basic A DDand L OOKUP depends on their construction,\nand they are faster on Bloom Matrix, similar to the theoret-\nical analyses presented in Section 4.\n7 B LOOM TEST\nIn this section, we test our Bloom Multiﬁlters with a real\ndataset with unknown distribution, and illustrate the pro-\ncedure to ﬁnd the best Bloom Multiﬁlter through small and\nfast empirical experiments. Speciﬁcally, we used the 20ng-\ntest-stemmed corpus, which is the 20 Newsgroups corpus\ntest dataset with stemmed words, from the datasets for\nsingle-label text categorization [18]. The dataset has jEj\n= 7527 documents and jLj= 625635 words. In this case,\nwe do not have prior knowledge about the distribution of\nthe dataset; i.e., Uniform or Zipf. We construct the Bloom\nMultiﬁlters around certain Expected False Positive rates,\ni.e.,p2 f0:9;0:5;10\u00001;10\u00002;\u0001\u0001\u0001;10\u00006g. The value of m\n\n10\n10\u0000610\u0000510\u0000410\u0000310\u0000210\u0000110010\u0000610\u0000410\u00002100\nExpected FP RateComputed Avg. FP RateBM\nSBM\nBV\nFigure 10: Computed avg. FP rate vs Expected FP rates for\n20ng-test-stemmed dataset (unknown distribution). The avg.\nFP rates were computed for a L OOKUP operation with 1000\nlabels.\nandkwere computed according to equations 2a and 2b.\nIn the case of Bloom Matrix and Sparse Bloom Matrix,\nwe compute mbased on the average amount of words in\neach document, i.e., m=\u0000jLj\njNj\u0001lnp\nln22. Next, we compute\nkaccording to equation 2b. On the other hand, for Bloom\nVector, we compute mandkfor every document in Eusing\nequation 2a and 2b.\nPerformance. Figure 8 compares the memory overhead of\nthe structures with the real dataset. We notice that all the\nstructures perform similarly and even with 10\u00006FP rate\nthe occupy 45% of the actual size. Figure 9 shows the\nperformance of L OOKUP operation with multiple labels for\ngiven FP rates. We notice that the number of labels has a\nnegligible effect on lookup time. As we discussed before,\nthis is because the only additional workload for multiple\nlabels is that the lookup function needs to compute the hash\nneighborhood of all the labels. The lookup time increases\nas the FP rate decreases, however, Bloom Matrix seems to\nbe both space efﬁcient and the lookup operation is also the\ntime efﬁcient.\nNevertheless, the L OOKUP operation works with a com-\nplete set of queries. In other words, all the labels are\nsearched together by ^operation. Performing an _opera-\ntion, i.e., V _^G[i;], on Bloom Matrix for subset queries\nwill provide invalid indexes of the items while performing\nthe D ECODE (\u0005; V)operation, for example. In order to ﬁnd\nthe match for such subset of the queries, it requires to look\nfor the items of each label and then perform OR operation,\ni.e., after the D ECODE (\u0005; V)operation.\nBloom Test. We next evaluate the performance of the struc-\ntures in terms of FP rate. While we construct them with ex-\npected FP rates, we also compute the FP rates for the lookup\noperations performed in this section. Figure 10 compares\nthe expected FP rate with the computed FP rate. We notice\nthat only the computed FP rate of Bloom vector follow the\nexpected FP rate, whereas the FP rates of Bloom matrix and\nSparse Bloom Matrix always higher than 10\u00002. Although\nsuch FP rates should be sufﬁcient for many applications, this\nbehavior of Bloom Matrix also exposes some characteristic\nabout the input data. In other words, the labels in 20ng-test-\nstemmed are not uniformly distributed among the items. In\norder to verify this, we further performed similar exper-10\u0000510\u0000410\u0000310\u0000210\u0000110010\u0000510\u0000310\u00001\nExpected FP RateComputed Avg. FP RateBM\nSBM\nBV\nFigure 11: Computed avg. FP rate vs Expected FP rates for\nthe synthetic data (Uniform distribution). The avg. FP rates\nwere computed for a L OOKUP operation with 1000 labels.\niments with the uniformly distributed synthetic data. We\nconstructed all three Bloom Multiﬁlters with the mentioned\nexpected FP rates. Figure 11 shows that the FP rates for the\nLOOKUP operations on them follow the expected FP rates of\nthe structures. The FP rates were computed according to the\nfollowing:\nFPR=(jf(l)j\u0000jej)\n(jEj\u0000jf(l)j) + (jf(l)j\u0000jej)(14)\nwhere (jEj\u0000jf(l)j)is the true negative and (jf(l)j\u0000jej)is\nthe false positive. f(l)is the number of items returned by\nthe L OOKUP function andjejis the actual number of items\ncontaining label lin the input sets.\nThe above ﬁnding suggests that Bloom Matrix can be\nused to indicate whether an input dataset, with multiple\nsets, follows Uniform distribution or not. Since the FP rates\nof all the structures follow the expected rates until 10\u00002,\na simple test can ﬁrst construct a Bloom Matrix with an\nexpected FP rate 10\u00003and then compare with the FP rate\nof a lookup operation. If the difference is signiﬁcant, the\ntest can conclude that the input data distribution is not\nUniform. Therefore, the test can be used to decide, based\non desired and real FP rates, which structure we should\nchoose for having an FP rate close to what we want. With all\nthe required computation for A DDoperation and L OOKUP\noperation for 1000 labels, the test requires only 1.3 MB space\nand 12 seconds for the 20ng-test-stemmed dataset with the\nmentioned hardware.\nNevertheless, the unexpected performance of Bloom\nMatrix stems from the fact that it has to rely on average\nsize of the input sets for unknown distribution, i.e.,jLj\njEj. In\nother words, the sizes of the corresponding Bloom Filters\nestimated as m=\u0000jLj\njNj\u0001lnp\nln22are too small compared to the\nactual size of the input sets.\n8 R ELATED WORK\nThere are many extensions of the Bloom Filter. In this\nsection, we discuss some of them.\nCounting Bloom Filter. The Counting Bloom Filter [21]\nis a variant of standard Bloom Filter that allows a delete\noperation without creating the chance of FNs. However,\ninstead of using a bitset, it uses an array of integers. The\n\n11\nADDoperation increments the integers to which a label\nis mapped to with the hash functions, while the delete\noperation decrements them. The lookup function simply\nchecks whether all of the integers to which a label is mapped\nto are higher than 0. Stateful Bloom Filter [22] also extends\nstandard Bloom ﬁlter. The stored elements are neither bits\nnor counters, rather each element of mrepresents a value\ncorresponding to a state, i.e., state, and state counter, for\nidentifying P2P trafﬁc and congestion control for video\ntrafﬁc.\nCompressed Bloom Filter. The Compressed Bloom Filter\n[23] was proposed to reduce the number of bits broadcast\nin network applications, FP rate, and lookup time. This\nadvantage comes at the cost of introducing a processing\ntime for compression and decompression. The compression\nalgorithm proposed in the original work [23] is Arithmetic\nCoding [24], which is a lossless data compression technique.\nUnlike standard Bloom Filter, the optimal kis chosen to\noptimize the result of the compression algorithm in a Com-\npressed Bloom Filter. This results in a choice of klower than\nin a standard Bloom Filter.\nSplit Bloom Filter. The Split Bloom Filter [25] uses a bitset\nsplit in multiple bins. Each bin has an associated hash\nfunction, and the hash functions are all different from each\nother. Whenever an element is added, it is added to all bins.\nMore formally, a Split Bloom Filter is composed by k\nbinsG=fB1; :::; B kgeach having size m, where kis also\nthe number of hash functions. Each hash function hi(x)is\nassociated to the bitset Bihaving the same index. Whenever\nan element xis added, we set to 1 the bits Bi[hi(x)], for\n1\u0014i\u0014k.\nThe lookup operation works similarly, but it checks\nwhether all bits Bi[hi(x)], for1\u0014i\u0014k, are set to 1.\nScalable/Dynamic Bloom Filter A Scalable Bloom Filter [26]\nstarts with a Split Bloom Filter [25] with k0bins and P0\nexpected FP rate, which can support at most a number of\nelements that keep the FP rate below P0. When the ﬁlter\ngets full, another one is added with k1bins and P1=P0r\nexpected FP rate, where ris a tightening ratio decided\nduring the implementation. This is useful when the number\nof labels in a set is unknown. Alternatively, dynamic Bloom\nFilters [27] can be used as the size of the data grows with\ntime.\nBloomier Filter. The Bloom Filter can encode only Boolean\nfunctions. The Bloomier Filter [16] was proposed to repres-\nent arbitrary functions on ﬁnite sets.\nLetE=fe1; :::; e NgandR=f1; :::;jRj\u00001g. LetA=\nf(e1; v1); :::;(eN; vN)gbe an assignment, where vi2Rfor\n1\u0014i\u0014N. The encoding of such assignment also be seen\nas a function f:E!Rdeﬁned as:\nf(x) =(\nviifx2E\n?otherwise\nThe Bloomier ﬁlter uses a bit matrix to encode the\nfunction previously deﬁned. In order to build such a matrix,\nit uses a non-trivial algorithm, which can be found in the\noriginal work [16]. In this algorithm, the Bloomier Filter uses\ntwo functions called E NCODE and D ECODE . In Section 3\nwe deﬁne two similar functions that we call with the same\nnames, which our Bloom Multiﬁlters use.Bloom Multiﬁlter. The closest work related to ours is the\nBloom Multiﬁlter, devised by Xu et al. [17], which extends\nstandard Bloom Filter to check multiple elements on mul-\ntiple sets at once.\nLetS=fS1; :::; S ng, where each Siis a set of multiple\nelements. To check whether there is an S2S which contains\nall the elements in a query qwe need to implement a\nBoolean function fdeﬁned as:\nf(q) =(\n19S2S:q\u0012S\n0otherwise\nSimilarly to the Bloomier Filter, the Bloom Multiﬁlter\nuses a bit matrix to represent multiple sets, and each set\nhas an assigned ID. Whenever an element is added to such\nset, it is mapped to the rows having the indices equal to its\nhash neighborhood; the ID of the set, represented in binary,\nis then added to such rows using the bit-wise OR operation.\nTo check whether multiple elements belong to one of\nthe sets, they are mapped to multiple rows according to\ntheir hash neighborhood, then the bit-wise AND operation\nis performed on such rows. If the result is a value greater\nthan 0, then all those elements are probably in one of the\nsets.\nBoth standard Bloom Filter and Bloomier Filter [16] were\nmeant to encode only one set, and the Bloom Multiﬁlter [17]\ncan only answer whether it is true or false that one or\nmultiple elements are in one of the represented sets. In\nthis article, we have extended standard Bloom Filter not\nonly to encode multiple sets and to efﬁciently check the\nmembership of an element in all the sets but also to answer\nto which of the sets such element belongs to. Besides, a\nnumber of new approaches aim to model the input data,\nthat a standard Bloom ﬁlter is going to present, with a pre-\nﬁlter. These approaches use machine learning [28] or rely on\nstandard Bloom Filter for pre-ﬁltering [29]. Our Bloom Test\nalso can be used for pre-ﬁltering data with multiple sets.\n9 C ONCLUSIONS\nIn this article, we presented two statistical data structures\nwhich are able to answer not only the membership of the\nlabels but also can answer to which sets they are associated\nwith. At the same time, they are space-efﬁcient and thus can\nbe cached in RAM where the replication is less expensive in\nterms of storage. With randomly distributed labels amongst\nthe sets, the variant of Bloom Matrix i.e., Sparse Bloom\nMatrix, is more space efﬁcient at an expense of an reordering\ncost. With Zipf distributed labels amongst the sets, Bloom\nVector is the most space efﬁcient structure. Therefore, these\nstructures are also statistically meaningful. We evaluated\ntheir performance for basic Bloom Filter operations. Finally,\nwe introduced Bloom Test to ﬁnd whether the input sets\ntogether follow Uniform distribution or not. The test result\ncan be used to determine which structure is suitable to\nachieve an FP rate of less than 10\u00002.\nACKNOWLEDGEMENTS\nWe would like to thank Dr. Antonio Cano, who worked on\nearly Bloom Multiﬁlter designs while visiting Helsinki.\n\n12\nREFERENCES\n[1] S. Brin and L. Page, “Reprint of: The anatomy of a large-scale\nhypertextual web search engine,” Computer Networks , vol. 56,\nno. 18, pp. 3825–3833, 2012, the fWEBgwe live in.\n[2] Y. W. Park, K. H. Baek, and K. D. Chung, “Reducing network\ntrafﬁc using two-layered cache servers for continuous media data\non the internet,” in Computer Software and Applications Conference,\n2000. COMPSAC 2000. The 24th Annual International , 2000, pp. 389–\n394.\n[3] M. O. Rabin, “Efﬁcient dispersal of information for security, load\nbalancing, and fault tolerance,” J. ACM , vol. 36, no. 2, pp. 335–348,\nApr. 1989.\n[4] B. A. Shirazi, K. M. Kavi, and A. R. Hurson, Eds., Scheduling and\nLoad Balancing in Parallel and Distributed Systems . Los Alamitos,\nCA, USA: IEEE Computer Society Press, 1995.\n[5] K. G. Kakoulis and I. G. Tollis, “Algorithms for the multiple label\nplacement problem,” Computational Geometry , vol. 35, no. 3, pp.\n143–161, 2006.\n[6] ——, “Labeling algorithms,” pp. 489–515, 2013. [Online]. Avail-\nable: http://cs.brown.edu/ \u0018rt/gdhandbook/chapters/labeling.\npdf\n[7] A. Kumar, J. Xu, and J. Wang, “Space-code bloom ﬁlter for efﬁcient\nper-ﬂow trafﬁc measurement,” IEEE Journal on Selected Areas in\nCommunications , vol. 24, no. 12, pp. 2327–2339, Dec 2006.\n[8] S. Cohen and Y. Matias, “Spectral bloom ﬁlters,” in Proceedings of\nthe 2003 ACM SIGMOD International Conference on Management of\nData , ser. SIGMOD ’03. New York, NY, USA: ACM, 2003, pp.\n241–252.\n[9] T. Yang, A. X. Liu, M. Shahzad, Y. Zhong, Q. Fu, Z. Li, G. Xie, and\nX. Li, “A shifting bloom ﬁlter framework for set queries,” Proc.\nVLDB Endow. , vol. 9, no. 5, pp. 408–419, Jan. 2016.\n[10] F. M. Cuenca-Acuna, C. Peery, R. P . Martin, and T. D. Nguyen,\n“Planetp: using gossiping to build content addressable peer-\nto-peer information sharing communities,” in High Performance\nDistributed Computing, 2003. Proceedings. 12th IEEE International\nSymposium on , June 2003, pp. 236–246.\n[11] P . Reynolds and A. Vahdat, “Efﬁcient peer-to-peer keyword\nsearching,” in Proceedings of the ACM/IFIP/USENIX 2003 Interna-\ntional Conference on Middleware , ser. Middleware ’03. New York,\nNY, USA: Springer-Verlag New York, Inc., 2003, pp. 21–40.\n[12] S. C. Rhea and J. Kubiatowicz, “Probabilistic location and rout-\ning,” in Proceedings.Twenty-First Annual Joint Conference of the IEEE\nComputer and Communications Societies , vol. 3, 2002, pp. 1248–1257\nvol.3.\n[13] J. Tapolcai, J. B ´ır´o, P . Babarczi, A. Guly ´as, Z. Heszberger, and\nD. Trossen, “Optimal false-positive-free bloom ﬁlter design for\nscalable multicast forwarding,” IEEE/ACM Trans. Netw. , vol. 23,\nno. 6, pp. 1832–1845, Dec. 2015.\n[14] G. Cybenko, “Dynamic load balancing for distributed memory\nmultiprocessors,” Journal of Parallel and Distributed Computing ,\nvol. 7, no. 2, pp. 279–301, 1989.\n[15] M. E. Baran and F. F. Wu, “Network reconﬁguration in distribution\nsystems for loss reduction and load balancing,” IEEE Transactions\non Power Delivery , vol. 4, no. 2, pp. 1401–1407, Apr 1989.\n[16] B. Chazelle, J. Kilian, R. Rubinfeld, and A. Tal, “The bloomier\nﬁlter: An efﬁcient data structure for static support lookup tables,”\ninProceedings of the Fifteenth Annual ACM-SIAM Symposium on\nDiscrete Algorithms , ser. SODA ’04. Philadelphia, PA, USA: Society\nfor Industrial and Applied Mathematics, 2004, pp. 30–39.\n[17] C. Xu, Q. Liu, and W. Rao, BMF: An Indexing Structure to Support\nMulti-element Check . Cham: Springer International Publishing,\n2016, pp. 441–453.\n[18] A. Cardoso-Cachopo, “Improving Methods for Single-label\nTextCategorization,” PdD Thesis, Instituto Superior Tecnico, Uni-\nversidade Tecnica de Lisboa, 2007.\n[19] B. H. Bloom, “Space/Time Trade-offs in Hash Coding with Al-\nlowable Errors,” Commun. ACM , vol. 13, no. 7, pp. 422–426, Jul.\n1970.\n[20] P . Bose, H. Guo, E. Kranakis, A. Maheshwari, P . Morin, J. Morrison,\nM. Smid, and Y. Tang, “On the false-positive rate of bloom ﬁlters,”\nInformation Processing Letters , vol. 108, no. 4, pp. 210–213, 2008.\n[21] L. Fan, P . Cao, J. Almeida, and A. Z. Broder, “Summary cache:\na scalable wide-area web cache sharing protocol,” IEEE/ACM\nTransactions on Networking , vol. 8, no. 3, pp. 281–293, Jun 2000.\n[22] F. Bonomi, M. Mitzenmacher, R. Panigrah, S. Singh, and G. Var-\nghese, “Beyond bloom ﬁlters: From approximate membership\nchecks to approximate state machines,” SIGCOMM Comput. Com-\nmun. Rev. , vol. 36, no. 4, pp. 315–326, Aug. 2006.[23] M. Mitzenmacher, “Compressed bloom ﬁlters,” IEEE/ACM Trans-\nactions on Networking , vol. 10, no. 5, pp. 604–612, Oct 2002.\n[24] A. Moffat, R. M. Neal, and I. H. Witten, “Arithmetic coding\nrevisited,” ACM Trans. Inf. Syst. , vol. 16, no. 3, pp. 256–294, Jul.\n1998.\n[25] F. Chang, W. chang Feng, and K. Li, “Approximate caches for\npacket classiﬁcation,” in INFOCOM 2004. Twenty-third AnnualJoint\nConference of the IEEE Computer and Communications Societies , vol. 4,\nMarch 2004, pp. 2196–2207.\n[26] P . S. Almeida, C. Baquero, N. Preguic ¸a, and D. Hutchison, “Scal-\nable bloom ﬁlters,” Information Processing Letters , vol. 101, no. 6,\npp. 255–261, 2007.\n[27] D. Guo, J. Wu, H. Chen, Y. Yuan, and X. Luo, “The dynamic\nbloom ﬁlters,” IEEE Transactions on Knowledge and Data Engineer-\ning, vol. 22, no. 1, pp. 120–133, Jan 2010.\n[28] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The case\nfor learned index structures,” in Proceedings of the 2018 International\nConference on Management of Data , ser. SIGMOD ’18, 2018, pp. 489–\n504.\n[29] M. Mitzenmacher, “Optimizing Learned Bloom Filters by Sand-\nwiching,” arXiv e-prints , p. arXiv:1803.01474, Mar. 2018.\nFrancesco Concas is a doctoral student in\nComputer Science at the University of Helsinki.\nHis main research interests are algorithms and\nmachine learning, and in particular probabilistic\nmodels.\nPengfei Xu is a doctoral student in the Depart-\nment of Computer Science, University of Hel-\nsinki. His current research topic are string pro-\ncessing, DBMS query optimization, and Big Data\nalgorithms.\nMohammad A. Hoque is a postdoctoral re-\nsearcher at the University of Helsinki. He ob-\ntained his M.Sc degree in Computer Science\nand Engineering in 2010, and Ph.D in 2013 from\nAalto University. His research interests include\nenergy efﬁcient mobile computing, data ana-\nlysis, distributed computing, and resource-aware\nscheduling.\nJiaheng Lu is an Associate Professor of the\nDepartment of Computer Science at the Uni-\nversity of Helsinki, Finland. His recent research\ninterests include multi-model database manage-\nment systems and job optimization for big data\nplatform.\nSasu Tarkoma (SMIEEE’12) is a Professor of\nComputer Science at the University of Helsinki,\nand Head of the Department of Computer Sci-\nence. He has authored 4 textbooks and has pub-\nlished over 160 scientiﬁc articles. His research\ninterests are Internet technology, distributed sys-\ntems, data analytics, and mobile and ubiquitous\ncomputing. He has seven granted US Patents.\nHis research has received several Best Paper\nawards and mentions, for example at IEEE Per-\nCom, ACM CCR, and ACM OSR.",
  "textLength": 60322
}