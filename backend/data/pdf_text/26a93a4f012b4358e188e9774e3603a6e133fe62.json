{
  "paperId": "26a93a4f012b4358e188e9774e3603a6e133fe62",
  "title": "deepBF: Malicious URL detection using Learned Bloom Filter and Evolutionary Deep Learning",
  "pdfPath": "26a93a4f012b4358e188e9774e3603a6e133fe62.pdf",
  "text": "Springer Nature 2021 L ATEX template\ndeepBF: Malicious URL detection using Self-adjusted Bloom\nFilter and Evolutionary Deep Learning\nRipon Patgiri1*, Anupam Biswas1yand Sabuzima Nayak1y\n1*Department of Computer Science & Engineering, National Institute of Technology\nSilchar, Silchar, 788010, Assam, India.\n*Corresponding author(s). E-mail(s): ripon@cse.nits.ac.in;\nContributing authors: anupam@cse.nits.ac.in; sabuzima rs@cse.nits.ac.in;\nyThese authors contributed equally to this work.\nAbstract\nMalicious URL detection is an emerging research area due to continuous modernization of var-\nious systems, for instance, Edge Computing. In this article, we present a novel malicious URL\ndetection technique, called deepBF (deep learning and Bloom Filter). deepBF is presented in\ntwo-fold. Firstly, we propose a self-adjusted Bloom Filter using 2-dimensional Bloom Filter. We\nexperimentally decide the best non-cryptography string hash function. Then, we derive a modi\fed\nnon-cryptography string hash function from the selected hash function for deepBF by introduc-\ning biases in the hashing method and compared among the string hash functions. The modi\fed\nstring hash function is compared to other variants of diverse non-cryptography string hash func-\ntions. It is also compared with various \flters, particularly, counting Bloom Filter, Kirsch et al. ,\nand Cuckoo Filter using various test cases. The test cases unearth weakness and strength of the\n\flters. Secondly, we propose a malicious URL detection mechanism using deepBF. We apply the\nevolutionary convolutional neural network to identify the malicious URLs. The evolutionary convo-\nlutional neural network is trained and tested with malicious URL datasets. The output is tested\nin deepBF for accuracy. We have achieved many conclusions from our experimental evaluation\nand results and are able to reach various conclusive decisions which are presented in the article.\nKeywords: Bloom Filter, Learned Bloom Filter, Multidimensional Bloom Filter, Membership Filter,\nMalicious URL Detection, Deep Learning, Evolutionary Deep Neural Networks, Deep Convolutional Neural\nNetworks, Neural Networks, Computer Networking.\n1 Introduction\nBloom Filter [1] is a famous hash data struc-\nture for membership \fltering which uses a tiny\namount of memory. It is known as an approximate\nmembership \flter. This tiny \flter is applied in\nnumerous research \felds. For instance, BigTable\n[2] uses Bloom Filter to enhance the lookup perfor-\nmance. BigTable reduces unnecessary HDD accessby deploying Bloom Filter. Similarly, it is deployed\nin various domains, namely, Big Data, Network\nSecurity [3, 4], Network Tra\u000ec, IoT [5], and Bioin-\nformatics [6]. Besides, there are an abundant of\nnetwork devices that depends on Bloom Filter.\nThus, there is an immense necessity for a high\naccuracy Bloom Filter in Computer Networking\nas well as other domains because Bloom Filter\n1arXiv:2103.12544v2  [cs.CR]  26 Feb 2022\n\nSpringer Nature 2021 L ATEX template\n2 deepBF\ncan foster a system's performance and reduces the\nmain memory consumption.\nThere are diverse variants of Bloom Filters\nwhich are introduced to address several issues,\nfor instance, counting Bloom Filter for caching\nURL purposes [7, 8]. There are also similar vari-\nants of Bloom Filter, particularly, Cuckoo Filter\n[9]. Moreover, Patgiri et al. introduces multidi-\nmensional Bloom Filter, called rDBF [10]. HFil\nis a high accuracy Bloom Filter extended from\nrDBF [11]. Recently, a learned Bloom Filter (LBF)\nis introduced by M. Mitzenmacher [12]. LBF is\ncurrently trending in Bloom Filter. It is a com-\nbination of machine learning and Bloom Filter.\nInspired from this LBF, we propose a novel tech-\nnique to identify the malicious URL using evo-\nlutionary convolutional neural network (evoCNN)\nand Bloom Filter.\nIn this article, we propose a novel self-adjusted\nBloom Filter, called deepBF (Deep Learning and\nBloom Filter). The complete proposed system is\nas follows- let,  be a URL, \u0016BFbe the Bloom\nFilter to cache malignant URL, \fBFbe the Bloom\nFilter to cache benign URLs and \u000fCNN be the\nevolutionary convolutional neural networks. First,\na query item  is queried to \u0016BFfor membership\nand if\u0016BFreturns true, then deepBF will block\nthe URL . Otherwise, query to \fBFfor mem-\nbership. If \fBFreturns true, then the URL  is\nallowed. Otherwise,  is a new URL. Therefore,\nthe new URL  is input to \u000fCNN for classi\fca-\ntion. If\u000fCNN identify the URL  as malignant,\nthen deepBF will insert the URL  into\u0016BFand\nblocks the URL  . Otherwise, deepBF will insert\nthe URL into\fBFand allow the URL. This pro-\ncedure reduces the load on \u000fCNN signi\fcantly. It\nalso reduces loads on computational devices.\nTo achieve our proposed system, we present\nit in two-fold. Firstly, deepBF is designed by\nperforming contest among the non-cryptography\nstring hash functions in 2-Dimensional Bloom Fil-\nter (2D Bloom Filter) [10] using various use cases\nand select the best non-cryptography string hash\nfunctions. Experimental results provide the jus-\nti\fcation for not selecting cryptography string\nhash functions. As per our observation, the mur-\nmur2 hash function is a consistent performer and\nselected it to use in deepBF. The Murmur2 hash\nfunction is modi\fed for higher performance and\nthe resultant hash function is used in deepBF. The\nresultant hash function contains high biases andredundancies. However, our experimental results\nshow that higher biases and redundancies do\nnot a\u000bect the false positive probability (FPP) of\nBloom Filter. After building a modi\fed string\nhash function, deepBF is compared with Kirsch et\nal.[8], counting Bloom Filter [7, 13] and Cuckoo\nFilter (CF) [9]. Kirsch et al. is a modi\fed con-\nventional Bloom Filter, CBF is a counting Bloom\nFilter and CF is a similar variant of Bloom Fil-\nter. Thus, our proposed Bloom Filter is compared\nto prominent variant of \flters. Our result shows,\ndeepBF outperforms in di\u000berent use cases. Sec-\nondly, deepBF is tested using malicious URL\ndetection using evoCNN and proposed Bloom Fil-\nter. evoCNN is trained and tested with malicious\nURL dataset and we have used the dataset of [14]\nhosted in [15]. The malignant and benign URLs\nare also tested in Bloom Filter. From this arti-\ncle, we have revealed strengths and weaknesses\nof the \flters. Also, we present numerous concrete\ndecision on Bloom Filters from our experimental\nresults.\nThis article establishes preliminaries, termi-\nnologies and techniques in Section 2 which are\nto be used in further sections. It presents con-\ncise descriptions of Bloom Filter and its oper-\nations, and non-cryptography string hash func-\ntions. Then, provides a few related works in\nSection 3. Our proposed work is described clearly\nthrough \fgures, equations and algorithms in\nSection 4. Section 5 demonstrates the experimen-\ntal environment, experimenting process and its\nresults. Similarly, Section 6 provides detailed anal-\nysis on our proposed systems. Likewise, a brief\ndiscussion is carried out in Section 7. Finally,\nthis article is concluded with several decisions in\nSection 7.\n2 Preliminary\n2.1 Bloom Filter\nBloom Filter is a probabilistic data structure\nfor membership \fltering capable of \fltering the\nmassive amount of data using a small memory\nfootprint. Bloom Filter has two key issues, namely,\nfalse positives and false negatives. When a Bloom\nFilter avoids deletion operation, the false negative\nprobability becomes zero, therefore, the accuracy\nof Bloom Filter depends on the false positive prob-\nability (FPP) of the \flter. There are many variants\n\nSpringer Nature 2021 L ATEX template\ndeepBF 3\nof Bloom Filter which are introduced to reduce the\nissues of Bloom Filter [16]. Also, diverse variants\nof Bloom Filters are introduced to address various\nchallenges in diverse applications [17{19]. The per-\nformance and false positive probability of Bloom\nFilter depend on number of hash functions. There-\nfore, an optimal number of hash functions are used\nin Bloom \flter [8]. If the number of hash function\ncalls is large then it can degrade the insertion and\nlookup performance. If the number of hash func-\ntion calls is small, then it can increase the false\npositive probability, but enhance the performance\nof insertion and lookup operations. To increase\nperformance, we reduce the number of hash func-\ntions calls while maintaining a low false positive\nprobability.\nLet,Bbe the Bloom Filter of size mbits.\nThe Bloom Filter has 1 ;2;3; :::; m cells\nwhere each cell can hold one bit, either 0 or 1.\nLet, U =fK1;K2;K3;:::gbe the universe.\nAn itemKj2U is mapped into Bloom Filter\nusing\u0015hash functions, let the hash functions\nbeH1(Kj);H2(Kj);H3(Kj); :::;H\u0015(Kj). A\u0015\nnumber of hash functions are invoked in inser-\ntion, deletion and query (lookup) operations. Let,\nS=fKi\n1;Ki\n2;Ki\n3;:::;Ki\nngbe the inserted set of\nthe Bloom Filter BwhereS\u001aU andnis the total\nnumber of items inserted into the Bloom Filter.\nLet,Kibe the random query. The true positive,\nfalse positive, false negative and true negative are\nde\fned in De\fnition 1, 2, 3 and 4 respectively.\nDe\fnition 1. IfKi2S andKi2B, then the\nresult of Bloom Filter Bis called true positive.\nDe\fnition 2. IfKi62S andKi2B, then the\nresult of Bloom Filter Bis called false positive.\nDe\fnition 3. IfKi2S andKi62B, then the\nresult of Bloom Filter Bis called false negative.\nDe\fnition 4. IfKi62S andKi62B, then the\nresult of Bloom Filter Bis called true negative.\nBloom Filter Busesmbits fornitems. There-\nfore, the probability of a bit to be 0 is (1 \u00001\nm).\nThe probability of a bit not set to 1 using \u0015hashfunction is\n\u0012\n1\u00001\nm\u0013\u0015\n=\u0012\u0012\n1\u00001\nm\u0013m\u0013\u0015\nm\n=e\u0000\u0015=m(1)\nwhere\nlim\nm!1\u0012\n1\u00001\nm\u0013m\n=1\ne\nAfter insertion of nitems, he probability of a bit\nnot set to 1 is e\u0000\u0015n=m. Therefore, the probability\nof the bit to be 1 is 1 \u0000e\u0000\u0015n=m. Let,\"be the\ndesired false positive probability, then the all bits\nto be set to 1 is\n\"= (1\u0000e\u0000\u0015n=m)\u0015(2)\nThe value of \u0015that minimizes false positive prob-\nability is given in Equation (3).\n\u0015=m\nnln2 (3)\nReplacing value of \u0015and taking lnin both sides\nin Equation (2), we get\nm=\u0000n ln \"\n(ln2)2(4)\nEquation (4) gives us the total memory require-\nments forninput items.\n01    2    3    4    5    011 1 1 11 1 0 0 0\nFig. 1 : Mapping ofK1,K2andK3into Bloom\nFilter using k= 3 hash functions and these hash\nfunctions areH1(),H2(),H3().\n2.2 Operations\nBloom Filter supports three operations, namely,\ninsertion, deletion and query (lookup) opera-\ntions. For these operations, Bloom Filter does not\nrequire complex hash functions. Instead, Bloom\nFilter requires the fastest non-cryptography string\nhash functions. Cryptography string hash function\nmakes Bloom Filter slower, and thus, it is not wise\n\nSpringer Nature 2021 L ATEX template\n4 deepBF\nto use MD5 and SHA2. Murmur, SuperFastHash\nand xxHash hash functions can be used in Bloom\nFilter for its operations. Bloom Filter does not\nrequire cryptography string hash function due to\ntwo reasons, namely, a) it slows down the Bloom\n\flter performance, and b) it is unable to reduce\nto false positive probability. Therefore, most of\nthe Bloom Filter uses Murmur hash functions, for\ninstance, rDBF [10].\n2.3 Hashing Techniques\nHashing is another factor that in\ruences the per-\nformance of a Bloom Filter. The time complexity\nof the Bloom Filter operations depends on the\nnumber of hashing operations performed.\n2.3.1 Murmur\nMurmurhash is designed by Austin Appleby in\n2008 [20]. The name is constructed using two basic\noperations murmurhash perform in its inner loop,\nnamely, multiply (MU) and rotate (R). It is a\nnon-cryptographic hash function which helps in\ncommon hash based query. It is open to public.\nVarious versions are also developed to improve\nthe performance. Currently the latest version is\nMurmurHash3.\n2.3.2 FNV\nFowler/Noll/Vo (FNV) [21] is a non-cryptography\nhashing technique. The technique maintains a low\ncollision rate. FNV has high dispersion. It makes\nFNV suitable for hashing of similar items. In FNV,\nitems are quickly processed while maintaining low\ncollision rate. The cryptography hashing tech-\nnique is computationally expensive to strongly\nprevent brute force inversion, but FNV is inex-\npensive. A cryptography hash function does not\nremain in a single state for a long time. However,\nin FNV hash value may be 0 and also remains in\nthat state until a non-zero item is encountered.\nMoreover, when a small unpredictable item gets\nincluded in the input set FNV produces a 0 hash\nvalue, and a cryptography hash function gener-\nates a complex hash value to increase complexity,\nbut in FNV the least signi\fcant bits of the hash\nvalue are easily visible. The available versions are\nFNV-1 and FNV-1a. FNV-1a performs multiply\nand XOR operations in a di\u000berent order comparedto FNV-1. This change in the order of opera-\ntion resulted in better avalanche characteristics.\nAvalanche characteristic is a property of cryptog-\nraphy technique which refers to slight variation in\ninput item heavily a\u000bects the hash value.\n2.3.3 FastHash\nFastHash [22] is simple non-cryptography string\nhash function. By default, FastHash produces 64\nbits hash code. For 32 bits hash code, it deducts\n32 bits code from 64 bits hash code. It is similar\nto Murmur hash function.\n2.3.4 CRC32\nPeterson and Brown [23] proposed cyclic redun-\ndancy check (CRC) for error detection. It is com-\nmonly used in networking and storage devices. It\nhelps to detect accidental alteration to data. CRC\nname is derived from the operations performed.\nThe check value produced by CRC is redundancy.\nAnd, the algorithm uses cyclic codes. CRC gen-\nerates a binary string of \fxed length called check\nvalue. The check value is included to transmitting\ndata. A check value is included in each data block\nto form a codeword. On the receiver side, again a\ncheck value is calculated for the data block or CRC\nis applied on whole codeword. Then, both the\ncodewords are compared with a residue constant.\nIn case the values di\u000ber, then data error is present\nin the block. CRC is used for hashing because it\nproduces a \fxed length check value. CRC32 is a\n32-bit cyclic redundancy code. It returns a 32 bit\nlong string as output. It hashes the string with less\ncollisions. Advantages of CRC are easy implemen-\ntation using a binary hardware, simple and easy\nmathematical analysis, and e\u000eciently determines\ncommon errors caused by transmission channel\nnoise.\n2.3.5 SuperfastHash\nPaul Hsieh [24] developed a non-cryptography\nhash function called Superfasthash. This algo-\nrithm uses fewer instructions per input fragment.\nThe input fragment is of 16 bits. The inner loop of\nthe algorithm interleaves two 16 bit words. More-\nover, the parameters used in the algorithm tries\nto give high avalanche e\u000bect.\n\nSpringer Nature 2021 L ATEX template\ndeepBF 5\n2.3.6 xxHash\nxxHash [25] is a non-cryptography hashing algo-\nrithm developed by Yann Collet. It optimizes all\noperations to execute faster. It partition the input\nitems into four independent streams. The respon-\nsibility of each stream is to execute block of 4 bytes\nper step. Each stream stores a temporary state.\nIn the \fnal step, all four states are combined to\nobtain a single state. The most important advan-\ntage of xxHash is that it's code generator gets\nmany opportunities to re-order opcodes to prevent\ndelay.\n3 Related work\nKirsch et al. proposes to reduce the number of\nhash functions while maintaining same FPP [8].\nThe proposed method improves the lookup and\ninsertion performance of Bloom Filter by reduc-\ning the number of hash functions in the con-\nventional Bloom Filter. Counting Bloom Filter\n(CBF) introduces counters for insertion and dele-\ntion operations [7]. Counters are decremented in\ndeletion operations and incremented in insertion\noperations. It is the \frst variant of Bloom Fil-\nter to e\u000eciently handle deletion operation with\nalmost false negative free. Conventional Bloom\nFilter avoids deletion operation due to the false\nnegative issue. Interestingly, CBF removes this\nissue using counters. However, CBF has also false\nnegatives if counters under\row. However, this case\nis rare. Another kind of membership \fltering is\nCuckoo Filter (CF) [9]. CF uses cuckoo hashing\n[26] and it is faster than Bloom Filter.\n3.1 Learned Bloom Filter\nLearned Bloom Filter (LBF) is proposed by M.\nMitzenmacher [27] which was derived from Kraska\net al. [28]. LBF becomes popular from the work of\nM. Mitzenmacher [27] which is generalized form.\nAlso, M. Mitzenmacher [27] propose sandwich\nstructured LBF using a combination of machine\nlearning with Bloom Filter. This structure saves\ntime and space of a system.\n3.2 Malicious URL\nFeng et al. [29] use Bloom Filter to \flter malicious\nURL. In their work, they have used multi-layer\ncounting Bloom Filter (MCBF) for caching themalignant and benign URLs. However, deletion\noperation is merely used malicious URL detection.\nDeletion operation causes false negatives. There-\nfore, conventional Bloom Filter avoids deletion\noperation to get rid of the false negative issue.\nCounting Bloom Filter (CBF) is a nearly false neg-\native free. But, it may also occur when the counter\nunder\rows. Moreover, CBF uses higher memory\nfootprint than conventional Bloom Filter. Dai and\nShrivastava [30] propose malicious a URL detec-\ntion mechanism with M. Mitzenmacher's LBF,\ncalled Ada-BF and disjoint Ada-BF. Ada-BF is\nbased on M. Mitzenmacher and grouping the keys\nto be hashed into the Bloom Filter. Based on\nthe score, Ada-BF hashes the keys into di\u000berent\ngroup in the Bloom Filter. Disjoint Ada-BF, also\ngroup keys based on score, however, the Bloom\nFilters are also independent, i.e., disjoint Ada-\nBF creates several Bloom Filters and insert the\nkeys into a particular Bloom Filter based on the\nscore. Both Ada-BF and disjoint Ada-BF may\nhave skewed load. For instance, a few groups are\noverloaded and rest groups are under-loaded. This\nmay happen in real life scenarios. Gerbet et al.\n[31] argues that non-cryptography hash functions\nare more vulnerable to cryptography string hash\nfunctions in Bloom Filter. We argue that this is\nnot true for Bloom Filter. If non-cryptography\nhash functions are vulnerable, then cryptography\nhash functions are. Bloom Filter reduces hashes\nthe keys using hash function and places the keys\nby modulus operations. Good string hash func-\ntion may not improve the performance and FPP\nof Bloom Filter. Inversely, introducing more biases\nin the string hash function can increase the perfor-\nmance and reduce the FPP. On the contrary, if we\nuse SHA or MD5, then false positive may increase\nand performance may also be a\u000bected adversely.\n3.3 Evolutionary convolutional\nNeural Network\nDeep learning models are immensely used for\nnumerous classi\fcation problems in di\u000berent\ndomains and proven to be superior over feature-\nbased machine learning techniques [32]. However,\nthe success of any deep learning model is depen-\ndent on several factors like tuning of appropri-\nate di\u000berent hyper-parameters, neural network\narchitecture, optimizer, etc. To learn neural net-\nwork weights, gradient-based optimizer such as\n\nSpringer Nature 2021 L ATEX template\n6 deepBF\nstochastic gradient descent, min-batch gradient\ndescent, and the Adam optimizer are widely\nused. However, the architecture of neural net-\nwork and hyper-parameters are have to be tuned\nmanually for better performance of the model.\nevoCNN models are gaining attention in recent\nyears to overcome the manual tuning of hyper-\nparameters and the network architecture, (refer to\ndetailed survey [33]). Currently, Several evoCNN\nmodels have been developed, mainly based on\nnature-inspired evolutionary optimization tech-\nniques such as Genetic Algorithm (GA), Parti-\ncle Swarm Optimization (PSO), and Ant Colony\nOptimization (ACO). The work of Miller et al. [34]\nin 1989 was probably the \frst such model, which\nconsidered GA to design simple neural network.\nThey had considered simple binary representa-\ntion of neural network elements like neural units,\nconnections, and biases etc. Angeline et al. [35]\ndeveloped GA based model to construct recur-\nrent networks. The foundation for the modern\nevoCNN model using GA has been laid down by\nStanley and Miikkulainen [36], which learns both\nstructure and weighting parameters of the neural\nnetwork. The neural evolution follows simple feed-\nforward learning and mainly does three things:\ncrossover between topologies, tracking the evolu-\ntionary units and update the topologies. Leung et\nal.[37] proposed another model with an improved\nGA to further optimize the network structure con-\nsidering learning of the input{output relationship.\nGasc\u0013 on-Moreno et al. [38] proposed hyperheuristic\napproach to adjust the number of nodes de\fned\nin each layer of the network, the number of layers,\nand the polynomial type. Recently, Sun et al. [39]\nhave developed evolving deep convolutional neu-\nral network (CNN) model using GA for evolving\nthe architectures and connection weight initial-\nization values to e\u000bectively address the image\nclassi\fcation tasks.\n4 deepBF- The proposed\nsystem\nWe present a novel malicious URL detection mech-\nanism, called deepBF. deepBF uses 2-dimensional\nBloom Filter (2D Bloom Filter) to implement\nself-adjusted Bloom Filter using machine learn-\ning techniques [27]. It deploys evolutionary deep\nlearning technique to identify the malicious URLs.Our proposed system maintains two self-adjusted\nBloom Filter, called \u0016BFand\fBFfor storing\nmalignant and benign URLs respectively. Initially,\nURL is queried to \u0016BFand\fBFto know\nwhether is malignant or benign. If both \flters\nresponse negative, then the URL  is a new URL.\nTherefore,  is input to evolutionary convolu-\ntional neural networks ( \u000fCNN ) for classi\fcation.\nIf\u000fCNN mark it as benign, then the URL  is\ninserted into \fBFand allow it for further process-\ning. Otherwise, the URL  is inserted into \u0016BF\nand blocks the URL  from further processing.\nThe proposed system is described in three\nphases; particularly, a) architecture of 2D Bloom\nFilter and it enhancement process, b) making 2D\nBloom Filter as self-adjusted Bloom Filter, and c)\nthe \fnal outcome as deepBF with malicious URL\ndetection.\nInsert     into 2DBF\n0123456789. . .0123456789. . .\n0123456789. . .\n00\n00000000\n111\n000 00 0 0 00 1 11 1000000000\n0\n111\nFig. 2 : Architecture self-adjusting Bloom Filter\nof deepBF depicting with \fve hash functions. The\n\fve hash functions are invoked for 10M items.\n4.1 Insertion\nAn item is inserted into self-adjusting Bloom Fil-\nter of deepBF as depicted in Figure 2. Algorithm\n1 implements the insertion process of self-adjusted\nBloom Filter in deepBF where a set of input items\nis inserted into self-adjusting Bloom Filter.\n\nSpringer Nature 2021 L ATEX template\ndeepBF 7\nAlgorithm 1 Self-adjusted Bloom Filter (2D\nBloom Filter) insertion algorithm in deepBF\n1:procedure Insertion (2DBloomFilter; File )\n2: whileK Read input from File do\n3:h1=H(K;Seed 1)\n4:h2=H(K;Seed 2)\n5:h3=H(K;Seed 3)\n6:h4=H(K;Seed 4)\n7:h5=H(K;Seed 5)\n8: Insert2D Bloom Filter (K;h1)\n9: Insert2D Bloom Filter (K;h2)\n10: Insert2D Bloom Filter (K;h3)\n11: Insert2D Bloom Filter (K;h4)\n12: Insert2D Bloom Filter (K;h5)\n13: end while\n14:end procedure\ndeepBF uses self-adjusted Bloom Filter which\nis implemented using 2D Bloom Filter. Moreover,\n2D Bloom Filter uses three modulus operations to\nplace an item in a particular bit position. Let us\nassume, BM;Nbe a 2-dimensional unsigned long\nintarray to implement Bloom Filter which is ini-\ntialized by zero and assuming unsigned long int\noccupies 64 bits. The M6=Nare the dimensions\nof the Bloom Filter and both are prime num-\nber. Equation (4) gives m, the number of memory\nrequired for nitems. We maintain a prime num-\nber array and the index is calculated for \fnding\nthe value of MandN. Let,P=fp1;p2;p3;:::g\nbe the array of prime numbers and i \u0000pm.\nThe two dimensions are assigned by M \u0000Pi\u00001\nandN \u0000Pi+1whereiis a index. It is observed\nthat the distance between two prime numbers is an\nimportant factor. It reduces the false positive rate,\nbecause the distance between Pi\u00003andPi+3are\nmore than the distance between Pi\u00001andPi+1.\n2D Bloom Filter also requires three parameters to\nset a bit in BM;N, namely,i; j, and\u001awhere\u001ais\nthe bit position of a particular cell, say, Bi;j. Thei\nandjrepresent particular row and column respec-\ntively. The cell size of Bi;jdepends on the memory\noccupied by the \flter for each cell, termed as \f,\nfor example, 64 bitsforunsigned long int . Now,\n2D Bloom Filter sets a bit in Bi;jto insert itemK\nby invoking Equation (5).\nBi;j Bi;jOR(1\u001c\u001a) (5)\nwhereORis a bitwise operator and \u001cis the bit-\nwise left shift operator. Now, the Murmur hash\nfunctionsH(K) returns a value and assigned the\nreturned value to hbyh H (K). To placeK, 2D\nBloom Filter calculates the parameters as follows:rowi h%M, columnj h%N, and bit posi-\ntion\u001a h%\f, where % is a modulus operator\nand\fis the bit size per cell of the Bloom Fil-\nter array. Thus,Kis inserted using the Equation\n(5). It is observed that \f= 61 have less the false\npositive probability than \f= 63 or\f= 64. More-\nover, the number of hash functions plays critical\nrole in reducing the false positive probability. The\noptimized value of number of hash functions, \u0015,\nis calculated as \u0015=m\nnln2. In our proposed sys-\ntems, 2D Bloom Filter calculates the number of\nhash functions for achieving desired false positive\nprobability. Therefore, 2D Bloom Filter requires\n\u0015=d\u0015\n2ehash function calls.\n4.2 Membership Query\nSimilar to insertion operation, all parameters\n(i; jand\u001a) are calculated for lookup operation.\nEquation (6) is invoked to query whether the item\nKis a member of 2D Bloom Filter or not.\nFlag 1 (Bi;jAND (1\u001c\u001a))\u001d\u001a (6)\nwhereAND is a bitwise operator. If Flag 1= 0,\nthenKis not a member of 2D Bloom Filter.\nAlgorithm 2 2D Bloom Filter membership query\nof deepBF\n1:procedure Insertion (2DBloomFilter; File )\n2: whileK Read input from File do\n3:h1=H(K;Seed 1)\n4:h2=H(K;Seed 2)\n5:h3=H(K;Seed 3)\n6:h4=H(K;Seed 4)\n7:h5=H(K;Seed 5)\n8: ifqueryMember2D Bloom Filter (K;h1) =true\nthen\n9: ifqueryMember2D Bloom Filter (K;h2) =\ntrue then\n10: ifqueryMember2D Bloom Filter (K;h3) =\ntrue then\n11: ifqueryMember3DBF (K;h4) =true\nthen\n12: if\nqueryMember2D Bloom Filterr (K;h5) =true then\n13: Found Found + 1\n14: end if\n15: end if\n16: end if\n17: end if\n18: end if\n19: end while\n20:end procedure\n\nSpringer Nature 2021 L ATEX template\n8 deepBF\n4.3 2D Bloom Filter as self-adjusted\nBloom Filter\nBloom Filter does not understand patterns. How-\never, it can be trained to learn about the pat-\nterns using Machine Learning techniques. Sim-\nilar to the concept of M. Mitzenmacher [27],\nwe deploy evolutionary convolutional neural net-\nworks to identify the patterns and train deepBF.\ndeepBF is deployed in malicious URL detec-\ntion which is much faster than lookup in any\nmachine learning techniques. Because, it combines\nboth Bloom Filter and evolutionary convolutional\nneural networks to improve overall performance\nof identifying pattern. Self-adjusted Bloom Fil-\nter continuously learns about the patterns after\ndeploying it in real project using the evolutionary\nconvolutional neural networks.\nDe\fnition 5. LetPbe a pattern, and Bis the\nBloom Filter. If Bcan identify the pattern P, then\nBis called learned Bloom Filter.\nDe\fnition 5 de\fnes the learned Bloom Filter,\ncoined by M. Mitzenmacher [27]. Notably, Bloom\nFilter does not understand the patterns. There-\nfore, a machine learning algorithm is required to\nassist the identi\fcation of patterns by the Bloom\nFilter. Therefore, deepBF can provide fast identi-\n\fcation of patterns using Bloom Filter and Deep\nLearning method. In our proposed system, we con-\nsider Malicious URL detection as case study to\nvalidate the veracity. But deepBF can be deployed\ndiverse applications, for instance, DDoS. As we\nknow that Bloom Filter plays important role in\nthe malicious URL detection. The machine learn-\ning algorithms are time consuming as compared to\nBloom Filter. Moreover, the loads on a tiny device\ncan be reduced by Bloom Filter. Also, machine\nlearning algorithms require more memory than\nBloom Filter. Therefore, Bloom Filter acts as the\n\frst layer of \fltering process to reduce the load on\nthe machine learning process. We propose a self-\nadjusted Bloom Filter which uses 2D Bloom Filter\nin deepBF. There are two situation in of 2D Bloom\nFilter in our proposed system; particularly, a)\ntrained the 2D Bloom Filter before deploying it, or\nb) deploy 2D Bloom Filter without training it. In\nthe both cases, deepBF works. We know that the\nlearned Bloom Filter is trained before deployingit in a real environment. However, 2D Bloom Fil-\nter does not require any training in deepBF but it\ncan also be trained before deploying it on real-life.\nMoreover, 2D Bloom Filter can be self-adjusted\nthroughout the life-cycle which is demonstrated in\nFigure 3. Therefore, our proposed 2D Bloom Filter\nis termed as self-adjusted Bloom Filter. Note-\nworthy that the \u000fCNN requires training before\ndeploying it in real environment.\n4.4 Malicious URL Detection\nURL\nIs     a\nmember of \nIs     a\nmember of \nInput            \nto the          Block\nAllowYes\nNo\n \nBenign MalignantNoYes\nInput            \ninto               Input            \ninto              \nFig. 3 : Malicious URL detection using two self-\nadjusted Bloom Filters, namely, \u0016BF and\fBF for\nmalignant and benign URLs respectively.\nLet, be the unknown URL, \u0016BFand\fBF\nbe the self-adjusted Bloom Filter for malignant\nand benign URLs, respectively. Let \u000fCNN be the\nevolutionary convolutional deep learning. Figure 3\ndemonstrates the \row of an URL  . Firstly, is\nqueried to\u0016BFto know whether the  is malig-\nnant or not. If  is a member of \u0016BF, then the\nURL is blocked. Otherwise,  is queried to \fBF.\nIf is a member of \fBF, then the URL  is benign\nand it is allowed; otherwise,  is a new URL. This\nnew URL is input into \u000fCNN for pattern recog-\nnition. The outcome of \u000fCNN is either malignant\nor benign. If the  is malignant, then insert  into\n\u0016BFand block . Otherwise, it is inserted into\n\fBFand the is allowed. If blocked URL  is\nqueried for the next time, then it does not require\n\nSpringer Nature 2021 L ATEX template\ndeepBF 9\nto input into the \u000fCNN because the self-adjusted\nBloom Filter blocks the URL for further process-\ning. It saves times of the checking whether the\ninput item is benign or malignant. Therefore, the\ntwo self-adjusted Bloom Filters grow their inputs\nover a time period which are much faster than the\nmachine learning algorithms. Over a time period,\nonly the new URLs are passed to \u000fCNN which\nare very less as compared to the beginning of the\nlife-cycle of the project.\n5 Experimental Results\nTo evaluate our proposed system, we conduct\na series of rigorous test in the low cost desk-\ntop environment. The con\fguration of the system\nis Intel(R) Core(TM) i7-7700 CPU @ 3.60GHz,\nUbuntu 18.04.4 LTS with 8GiB RAM. The exper-\nimental environment is depicted in Table 1.\nTable 1 :Experimental Environment Setup\nName Description\nCPU Intel(R) Core(TM) i7-7700\nCPU @ 3.60GHz\nL1 Cache 32K\nL2 Cache 256K\nRAM 8GB\nHDD 500GB\nGPU Intel ® HD Graphics 630\n(KBL GT2)\nOperating Sys-\ntemUbuntu 18.04.1 LTS 64-bits\nWe present the experimental results as follows-\na) selection of suitable hash function for 2D Bloom\nFilter, b) comparing 2D Bloom Filter with other\nstate-of-the-art Bloom Filters, c) training and\ntesting evolutionary convolutional Neural Net-\nwork, and d) the \fnal results of deepBF with\ncombining 2D Bloom Filter and evolutionary con-\nvolutional Neural Network as shown in Figure\n3.\n5.1 Test cases\nIn this experimentation, we have created three\ndi\u000berent test cases to evaluate the Bloom Fil-\nter's performance. We have created three datasets,\nparticularly, same set, mixed set and disjoint set\nwhich are de\fned in De\fnitions 6, 7 and 8. The\nsize of three datasets is 10 million (10M). Initially,\n10M unique keys are inserted into 2D Bloom Filterwhich takes 8 :999744 seconds. The same inserted\nkeys are queried into 2D Bloom Filter which is\ntermed as same set. The mixed set is also a unique\nset of items, but 50% of query dataset items match\nwith inserted dataset which is termed as mixed\nset. In disjoint set, query dataset does not match\nwith inserted dataset. The disjoint set is a set of\nrandom keys. These test cases are used to vali-\ndate the veracity of the 2D Bloom Filter in every\naspect. The test cases are designed such that it can\nwork in any kind of dataset in real environment.\nMost of the cases, the data are repetitive in nature;\nfor instance, URL data. Therefore, these three test\ncases are enough to verify and validate the perfor-\nmance of a Bloom Filter in every aspect. If Bloom\nFilter passes these three test cases with low false\npositive probability, then it can withstand any\nkind of situation.\nInterestingly, Figure 4 demonstrates the time\nmeasurement of 2D Bloom Filter in the three use\ncases. The insertion and query times are almost\nsame for same set, however, query operation takes\nmore times than insertion operation as shown in\nFigure 5, but the insertion operation takes more\ntimes as compared to the mixed set and disjoint\nset. The total false positives count is reported in\nFigure 7.\nLet,S=fs1;s2;s3;:::;smginput set and\ninput into the 2D Bloom Filter.\nDe\fnition 6. Let,Qis a set queried where Q=\nS, then the setQis called same set.\nDe\fnition 7. Let,Q=fq1;q2gbe a query set\nwhereq1\u001aS andq2\\S=\u001e, then, the setQis\ncalled mixed set.\nDe\fnition 8. Let,Qbe a query set where Q\\S =\n\u001e, then, the setQis called disjoint set.\nDe\fnition 9. Let,Qbe a query set of randomly\ngenerated strings or keys, then, the set Qis called\nrandom set.\nThe test cases (De\fnition 6, 7, 8 and 9) are\ncreated to identify the strength and weakness of a\nBloom Filter. The Bloom Filter does not exhibit\nsame behavior in di\u000berent test cases. Moreover,\nthese test cases help us to evaluate the perfor-\nmance of the \flters. We expose the strength and\nweakness of the \flters through these test cases.\n\nSpringer Nature 2021 L ATEX template\n10 deepBF\n5.2 Settings of the \flters\nThe required settings of the \flter is m,n,\u0015and\n\". In our experiments, the desired false positive\nprobability is \"= 0:001 for all. From the \"andn,\nthe total required memory is calculated as shown\nin Equation (4). Also, \u0015can be calculated from m\nandnas shown in Equation (3).\n5.3 Selection of Hash Function\nTo select the best hash function for deepBF,\nwe have conducted an extensive experiment to\nobserve the behavior of the hash functions. We\nhave considered eight hash functions to test\nthe performances and accuracy, namely, FNV1,\nFNV1a, CRC32, Murmur2, SuperFastHash and\nxxHash. 2D Bloom Filter implements these hash\nfunctions to execute the insertion and query oper-\nations in 2D Bloom Filter. The best hash function\nis selected based on the performance of 2D Bloom\nFilter. The criteria for selecting the hash function\nto deploy in deepBF is outlined below-\n\u000fTakes the least amount of time to process the\nquery and insertion operation.\n\u000fGives high accuracy, i.e., low false positives.\nDe\fnition 10. Million operation per second\n(MOPS) is standard in comparison of Bloom Fil-\nter performance. It is calculated as MOPS =\nn\n\u001c\u00021000000wherenis the number of items and \u001cis\nthe total time taken to process the nitems.\nMMurmur Murmur2\nSuperFastHashxxHash CRC32FastHashFNV1FNV1a246Time in secondsTime MOPS\nFig. 4 : Time taken in insertion process of 10M\nkeys into 2D Bloom Filter using various non-\ncryptographic string hash functions. Lower is bet-\nter for Time and Higher is better for million\noperations per second (MOPS, De\fnition 9).The di\u000berent test cases are created to evaluate\nthe non-cryptography string hash function in 2D\nBloom Filter platform. The test cases are de\fned\nin De\fnitions 6, 7, 8 and 9. The non-cryptography\nhash functions are Murmur, Murmur2, Super-\nFastHash, xxHash, CRC32, FastHash, FNV1 and\nFNV1a. We have introduced more biased in Mur-\nmur2 to achieve higher speed and lower false\npositive probability. The modi\fed Murmur hash\nfunction is termed as MMurmur for short. Figure 4\ndepicts the insertion performance of all eight hash\nfunctions in 2D Bloom Filter platform. MMur-\nmur with high biases is faster than rest hash\nfunctions in insertion of 10Million (10M) unique\nkeys. MMurmur hash function is a modi\fcation\nand replacement of the costly operators with low-\ncost operators, for instance, the bitwise operators\nare faster than other operators. Also, number of\noperations are reduced. Thus, the MMurmur hash\nfunction is able to achieve higher performance\nthan other hash functions.\nSame set Mixed setDisjoint set Random set246Time in secondsMMurmur Murmur2 SuperFastHash\nxxHash FastHash CRC32\nFNV1 FNV1a\nFig. 5 : Time taken in lookup of 10M keys of dif-\nferent use cases in 2D Bloom Filter using various\nnon-cryptographic string hash functions. Lower is\nbetter.\nInsertion operation of Bloom Filter is not as\nimportant as lookup operation. Lookup opera-\ntion is crucial in Bloom Filter because inser-\ntion operations are rare, but lookup operations\nare more frequent. Therefore, it is important to\nimprove the performance of lookup operations.\nFigure 5 demonstrates the performance of non-\ncryptography string hash function in 2D Bloom\nFilter platform. MMurmur hash function is at\nleast 1:98\u0002, 2:32\u0002, 2:95\u0002and 2:89\u0002faster than\nthe other hash functions in the same set, mixed\n\nSpringer Nature 2021 L ATEX template\ndeepBF 11\nset, disjoint set and the random set respectively.\nAlternatively, MMurmur hash function improves\nat least 49:38% compared to other hash functions.\nSame set Mixed setDisjoint set Random set510MOPSMMurmur Murmur2 SuperFastHash\nxxHash FastHash CRC32\nFNV1 FNV1a\nFig. 6 : Million Operations Per Second (MOPS)\nin lookup of 10M keys of di\u000berent use cases in\n2D Bloom Filter using various non-cryptography\nstring hash functions. Higher is better.\nFigure 6 illustrates performance in MOPS.\nMMurmur hash function outperforms all hash\nfunctions in 2D Bloom Filter platform. MMur-\nmur hash function performs 5 :48 MOPS, 7 :43\nMOPS, 10:18 MOPS, 10 :06 MOPS in low-cost\nhardware for same set, mixed set, disjoint set\nand random set respectively. However, other hash\nfunctions perform lower MOPS than MMurmur\nhash function.\nMixed set Disjoint set Random set0:000010:00010:0010:01False positive probabilityMMurmur Murmur2 SuperFastHash\nxxHash FastHash CRC32\nFNV1 FNV1a\nFig. 7 : False positive probability of lookup of 10M\nkeys of di\u000berent use cases in 2D Bloom Filter using\nvarious non-cryptography string hash functions.\nLower is better.Finally, the utmost crucial factor of Bloom\nFilter is false positive probability and it directly\nproportionate to the accuracy. Hence, Bloom Fil-\nter requires higher accuracy within desired false\npositive probability. The false positive probability\ndepends on memory and the number of hash func-\ntions. Bloom Filter should not take more memory\nand hash functions. The number of hash function\ncalls, reduce lookup and insertion performances.\nMoreover, Bloom Filter is used due to its lower\nmemory footprint. Therefore, 2D Bloom Filter is\nmeasured in 0 :001 desired false positive probabil-\nity which directly translates to 10 hash functions\ncalls and 17 :14MB primary memory consumption\nfor 10Mkeys. However, 2D Bloom Filter allocates\n17:36MB. Therefore, the MMurmur hash func-\ntion is measured in the above mentioned settings.\nNotably, the false positive probability is lower\nthan the desired false positive probability with the\nsame settings. For all hash functions, there are no\nfalse positives for the same set. However, there are\nfalse positive probability in mixed set, disjoint set\nand random set. All hash functions exhibit simi-\nlar false positive probability except the MMurmur\nhash function. MMurmur hash function exhibits\nextremely low false positive probability as com-\npared to other hash functions which is depicted in\nFigure 7.\n5.4 Comparison with other \flters\nWith the same settings, 2D Bloom Filter is com-\npared with other Filters, i.e., the desired false pos-\nitive probability is 0 :001, the number of hash func-\ntions is 10, the memory requirement is 17 :14MB\nor equivalent and the total 10 Munique keys\nare inserted. This article compares and demon-\nstrates that 2D Bloom Filter with other \flters\nthat uses MMurmur hash function. 2D Bloom Fil-\nter uses \fve hash functions which is half of the\nconventional Bloom Filter.\nBloom Filter Memory in MB\n2D Bloom Filter 17.37\nCF 24\nKirsch et al. 17.14\nCBF 68.56\nTable 2 : Memory used for 10 Mkeys to achieve\ndesired false positive probability of 0.001 by 2D\nBloom Filter, CF, Kirsch et al. , and CBF.\n\nSpringer Nature 2021 L ATEX template\n12 deepBF\nTable 2 provides the total memory require-\nments of the \flters. 2D Bloom Filter is compared\nwith Cuckoo Filter (CF) [9, 40], Kirsch et al.\n[8], and counting Bloom Filter (CBF) [7, 13].\n2D Bloom Filter, CF, Kirsch, and CBF take\n17:37MB, 24MB, 17:14MB and 68:56MB of\nmemory respectively. The CBF takes higher mem-\nory than other Bloom Filters, i.e., CBF has higher\nfalse positive probability than any other Filters to\nachieve a desired false positive probability. If CBF\nor CF uses 17.14 MB memory, then both have\na higher false positive probability. Alternatively,\nKirsch et al. and 2D Bloom Filter has higher\naccuracy.\n2D Bloom Filter CF Kirsch CBF246810False positive probabilityTime MOPS\nFig. 8 : Insertion time of 10M keys of di\u000berent\nuse cases of 2D Bloom Filter, Cuckoo Filter (CF),\nKirsch et al. and CBF. Lower is better for Time\nand Higher is better for MOPS.\nCuckoo \flter is quite fast \flter and it is faster\nthan our proposed Bloom Filter, 2D Bloom Fil-\nter with MMurmur, and other Bloom \flters in\ninsertion. Figure 8 demonstrates the time taken in\ninsertions and its MOPS. CF takes less time than\nother Bloom Filters. Also, it's MOPS is better\nthan other Bloom Filters.\nIn the lookup of 10M keys, the performance of\n2D Bloom Filter and CF are similar. Noteworthy\nthat CF outperforms other Bloom Filters in same\nset and mixed sets. However, 2D Bloom Filter out-\nperforms CF and other Bloom Filters in disjoint\nset and random set. Therefore, CF is useful in a\ncon\fned environment where most of the queries\nare true positives and its performance is quite sat-\nisfactory, but 2D Bloom Filter is useful in random\nenvironment where most of the queries are true\nnegatives.\nMOPS of CF is higher than other Bloom Fil-\nters in same set and mixed sets. However, 2DSame set Mixed setDisjoint set Random set246Time in seconds2D Bloom Filter CF Kirsch\nCBF\nFig. 9 : Time taken in lookup of 10M keys with dif-\nferent use cases of 2D Bloom Filter, Cuckoo Filter\n(CF), Kirsch et al. and CBF. Lower is better.\nSame set Mixed setDisjoint set Random set510MOPS2D Bloom Filter CF Kirch\nCBF\nFig. 10 : MOPS in lookup of 10M keys with dif-\nferent use cases in 2D Bloom Filter, CF, Kirsch et\nal., and CBF. Higher is better.\nBloom Filter outperforms CF and other Bloom\nFilters in disjoint set and random set. Undoubt-\nedly, CF is the fastest \flter, but it su\u000bers due to\nkicking operation in negative queries.\nMixed setDisjoint set Random set0:00010:011FPP2D Bloom Filter CF Kirch\nCBF\nFig. 11 : FPP in lookup of 10M keys with di\u000berent\nuse cases in 2D Bloom Filter, CF, Kirsch et al. ,\nand CBF. Lower is better.\n\nSpringer Nature 2021 L ATEX template\ndeepBF 13\nTable 3 : Accuracy of 2D Bloom Filter, CF, Kirsch\net al. , and CBF in lookup of 10M keys with\ndi\u000berent use cases. (in percentage %)\nUse\ncases2D\nBloom\nFilterCF Kirsch CBF\nMixed\nset99.966 99.94408 99.8972 99.8973\nDisjoint\nset99.9963 42.51 99.8988 99.9004\nRandom\nset99.9964 0.4649 99.9011 99.9002\nFalse positive rate is the most important cri-\nteria to opting a \flter. All \flter shows zero false\npositives in the same set. However, there are dif-\nferent false positive rate in mixed set. 2D Bloom\nFilter out performs all other \flters in false posi-\ntive rate. The false positive rate of CF in disjoint\nset and random set is nearly '1'. This happens due\nto kicking process in negative queries. Neverthe-\nless, CF outperforms Kirsch and CBF in mixed\nset, but both Bloom Filter outperforms CF in dis-\njoint set and random set as depicted in Figure 11.\nFrom the above benchmark, we found that CF is\nnot suitable for some situation even though it is\na fast \flter. Kirch et al. uses two Murmur2 hash\nfunction calls and the rest are manipulated bet-\nter technique to reduce execution time, but still, it\nuses 10 hash functions for 10M items with desired\nfalse positive probability of 0.001. CBF performs\nmoderate in all cases. However, CBF outperforms\nKirsch et al. in false positive rate. Therefore, the\naccuracy of 2D Bloom Filter, CF, Kirsch et al. ,\nand CBF are demonstrated in Table 3. CF exhibits\nlowest accuracy in disjoint set and random set.\n5.5 Evolutionary Deep Learning\nAs discussed above, the proposed malicious URL\ndetection method consists two major components:\nself-adjusted Bloom Filter and evolutionary deep\nneural network. The self-adjusted Bloom Filter\nis used to block the queried URL, say  based\non its membership \u0016BFor\fBF. Whereas, the\nevolutionary deep neural network is used to clas-\nsify the newly URL  whose membership is\nnot de\fned in learn Bloom Filter. Though, deep\nlearning models perform well in most of the clas-\nsi\fcation problems, the performance depends on\ndesigning of architecture of neural network and\ntuning of hyper-parameters. On the other hand,evolutionary deep learning tackles both architec-\nture and hyper-parameters of neural network. We\nhave considered recently developed, evolutionary\nconvolutional neural network (evoCNN) [39] for\nclassifying queried new URL  . Before deployment\nof evoCNN, the model has to be trained on URL\ndata.\n5.5.1 Prepossessing\nThe evoCNN implemented on tensor\row plat-\nform [41] accepts speci\fc shape of input dataset.\nTherefore, the dataset has to be processed and\nreshaped to \ft the required input format of\nevoCNN.\n•NaN value removal: Presence of NaN value in\nthe dataset a\u000bects training of model and the\nmodel may not learn properly. Therefore, all\nNaN values present in the dataset is replaced\nwith zeros.\n•Zero padding: Generally, the shape of input con-\nsidered for the model as a square matrix. The\ndataset may not contain required numbers of\nfeatures to rearrange those as square matrix.\nTherefore, additional zeros are added to com-\nplete the required shape of square matrix as\nshown below:\n[3;5;0;1;6;2;4] =) [3;5;0;1;6;2;4;0;0]\n \u0000appended two zeros\n•Input reshaping: The evoCNN model takes 2 D\nimage like data to work on convolution lay-\ners. The zero padded individual instances in\nURL dataset is still 1 Ddata, which requires to\nreshape into 2 Dimage like data. Each instance\nin the URL data contains 79 features, so two\nzeros are appended to reshape it to 9 \u00029 matrix.\nIn addition to this, though there has no RGB\nfeatures as we have in case of colored images,\nstill additional one dimension have to added.\nWe considered only one channel, another dimen-\nsion has to be added to this. Thus, \fnally each\ninstance in URL data has been reshaped as 4 D\ndata. An example of 3 \u00023 to 4Dis shown below:\n2\n43 5 0\n1 6 2\n4 0 03\n5=)2\n4:::2\n4:::2\n43 5 0\n1 6 2\n4 0 03\n5:::3\n5:::3\n5\n\nSpringer Nature 2021 L ATEX template\n14 deepBF\n5.5.2 Experimental setup\nWe have considered URL dataset [14, 15],\nwhich contains \fve di\u000berent categories of URLs:\nspam, defacement, malware, phishing and benign.\nAmong these \frst \fve are broadly classi\fed as\nmalignant. The dataset contains, separate sets\nof URL features for each of the four malignant\ncategories labeled as benign or speci\fc malig-\nnant categories. In addition, one set contains all\nlabeled categories. All these \fve sets are labeled\ninto classes malignant and benign, irrespective of\ntheir malignant category. Experimentation is done\nthese \fve datasets. For training and testing of\nevoCNN on these \fve datasets di\u000berent parame-\nter values are considered as follows. Parameters\nrelated to GA are set as: number of generations\n50, population size 50, and others kept default val-\nues. Parameters related to evoCNN model are set\nas: batch size 100, number of epochs 10, cross-\nentropy loss function and Adam optimizer. The\nmaximum lengths of the convolution layers, the\npooling layers, and the fully connected layers are\nset as same for all, i.e., 5. For each of \fve datasets,\n60% training, 25% validation and 15% testing are\nconsidered. The size of training, validation and\ntesting for each of the datasets along with total no\nof samples are shown in the Table 4.\nDatasets #Instances #Training #Validation #Testing\nSpam 14479 8687 4923 869\nDefacement 15711 9426 5342 943\nMalware 14493 8695 4928 870\nPhishing 15367 9220 5224 923\nAll 36707 22024 12480 2203\nTable 4 : Details about datasets and sizes of\ntraining, validation and testing instances.\n5.5.3 URL Classi\fcation Results\nThe results obtained with evoCNN for URL classi-\n\fcation are presented in Figure 12 and Figure 13.\nThe URL classi\fcation with evoCNN shows train-\ning accuracy ranging 98% to 100% and training\nloss ranging 15% to 19%. Results on datasets\nwith individual malignant categories as well as\nall combined shows high training accuracy and\nmarginal loss. Interestingly, testing results also\nshow high accuracy ranging 95% to 98% and\na similar amount of loss as training. Thus, thedeployment of evoCNN in the proposed architec-\nture enables highly accurate classi\fcation of new\nURLs to the LBF.\nSpam\nDefacementMalware PhishingAll50100100 99 98 98 98\n17:8817:3619:8318:0915:41Values in PercentageTraining Accuracy Training Loss\nFig. 12 : Training accuracy and loss of evoCNN\non URL classi\fcation\nSpam\nDefacementMalware PhishingAll5010098:3996:1597:8596:2996:35\n19:3717:3819:0927:9119:56Values in PercentageTesting Accuracy Testing Loss\nFig. 13 : Testing accuracy and loss of evoCNN on\nURL classi\fcation\n5.6 deepBF in action\nLBF is tested using the output of the evoCNN\nwith the dataset [14]. We have classi\fed malignant\nand benign of all data. Therefore, there are total\n129988 malignant and 35378 benign URLs as com-\nbined. We present this experimentation in two fold\nFirstly,\u0016BFand\fBFare empty. Secondly, \u0016BF\nis \flled with malignant URLs and tested using\nbenign URLs.\nTable 5 demonstrates performance of 2D\nBloom Filter, CF, Kirsch et al. , and CBF using\n\nSpringer Nature 2021 L ATEX template\ndeepBF 15\nTable 5 : Accuracy and performance testing\nthrough deduplication of malicious URLs.\nFilters FPP Dedup\ntimeAccuracy Memory\nin KB\n2D\nBloom\nFilter0.002523 0.073035 99.7477 252.098\nCF 0.0000385 0.202823 99.996 488.328\nKirsch 0.071814 0.096732 92.8186 228.1396\nCBF 0.077876 0.087116 92.2124 912\ndeduplication of malignant URLs. In terms of\naccuracy, CF exhibits highest accuracy, however,\nit takes high memory. 2D Bloom Filter is the\nfastest \flter in the deduplication process and CF\nis the slowest. Kirsch et al. takes lowest memory\nwhile CBF consumes the highest memory.\nTable 6 : Comparison of various Bloom Filter\nwith 2D Bloom Filter for malicious URL detection\nby inserting malignant URLs and testing using\nbenign URLs.\nFilter FPP Insertion\ntimeLookup\ntimeMemory\nin KBAccuracy\n2D\nBloom\nFilter0.000283 0.051451 0.013258 252.098 99.97\nCF 1 0.091545 0.02458 488.328 0\nKirsch 0.000763 0.069181 0.019478 228.139 99.92\nCBF 0.000537 0.044664 0.015823 912 99.95\nTable 6 demonstrates the comparison of 2D\nBloom Filter with CF, Kirsch et al. , and CBF for\nfalse positive probability of 0 :001. In this exper-\niment, malignant URLs are input to \u0016BFand\ntested with benign URLs for accuracy. 2D Bloom\nFilter exhibits the lowest false positive rate and\nlookup time. Also, 2D Bloom Filter has highest\naccuracy with optimal memory sized. CBF con-\nsumed the highest memory which is 912 KB but\nexhibits the fastest insertion time. Similarly, CF\nalso takes higher memory than 2D Bloom Filter\nand Kirsch et al. CF exhibits 100% false positive\nrate and thus its accuracy is zero. Also, it exhibits\nthe highest insertion and lookup time. Kirsch et\nal.occupies the lowest memory.\n6 Analysis\ndeepBF uses 2D Bloom Filter and a cell can\naccommodate many input items, since, an inputitem occupies a single bit. For example, unsigned\nlong int occupies 8bytes . Therefore, the cell can\nretain information of at most 64 di\u000berent input\nitems. However, it depends on the prime number\n\f. The\f= 64 is not a prime number, thus, the col-\nlision probability in a cell is high. However, \f= 61\ncan lower the collision probability in a cell.\nTheorem 1. Let,S=fs1;s2;s3;:::;smgbe the\ninput set. Let, BFis the 2D Bloom Filter and S\nis inserted into BF. 2D Bloom Filter exhibits low\nperformance in lookup for same set.\nProof Same set is de\fned in De\fnition 6. The query\nsetS=Q. In this case, lookup process has to invoke\nEquation (6) for hash value h1,h2,h3,h4andh5\nas shown in Algorithm 2. Invoking Equation (6) for\nall hash value are true, and hence, there are no early\ntermination of any IFcondition in Algorithm 2. Thus,\nit takes similar time as insertion. \u0003\nTheorem 2. 2D Bloom Filter exhibits high per-\nformance in disjoint set.\nProof The disjoint set is de\fned in De\fnition 8. The\nnecessary condition for disjoint set is S \\ Q =\u001e. 2D\nBloom Filter shows excellent performance in this case.\nAny negative query can be detected by as early as pos-\nsible by IFcondition in Algorithm 2. Therefore, 2D\nBloom Filter terminates as early as possible if detected\nas negative query. Therefore, it shows excellent per-\nformance which is also shown in experimental results.\n\u0003\nCorollary 1. 2D Bloom Filter exhibits medium\nperformance for mixed set.\nDe\fnition 7 de\fnes a mixed set as Q=fq1;q2g\nwhereq1\u001aSandq2\\S=\u001eorq1\\S=\u001eandq2\u001a\nS. In this case, 2D Bloom Filter exhibits medium\nperformance which is shown in the experimental\nresults.\nTheorem 3. Let,\u0010Kbe a cryptography string\nhash function of input item K,&Kbe the hash\nvalue of\u0010K,\u0007Kbe the non-cryptography string\nhash function of input item Kand\u001dKbe the hash\nvalue of \u0007K. The performance of Bloom Filter B\nusing\u001dKis higher than &K.\n\nSpringer Nature 2021 L ATEX template\n16 deepBF\nProof If\u0010Kis MD5, SHA1 or SHA256, then &Kis 128\nbits, 160 bits or 256 bits long. The \u001dKcan be either\n32 bits or 64 bits long. In our experiment, we have\nused 32 bits hash functions. Therefore, &K>\u001dK. The\nhash functions are used to distribute the keys fairly\namong available slots of Bloom Filter. Undoubtedly,\nthe SHA256 or SHA512 produces strong hash values\nwhich can be used to hash the keys among the avail-\nable slots. However, there is a modulus operator in\nhashing techniques to map a key in the slot of Bloom\nFilter. For instance, Bloom Filter size is m. There-\nfore,h\u0010=&K%mshould be better than h\u0007=\u001dK%m.\nHowever, the ground truth di\u000bers. Firstly, \u0010Kis much\nslower than \u0007K. Secondly, h\u0010andh\u0007are also depen-\ndent on the value of m. Them << &Korm < \u001dK.\nTherefore, the hash value is scaled under musing mod-\nulus operator. The modulus operation destroys the\ndistribution property of the hash functions. Moreover,\nh\u0010andh\u0007do not fairly distribute the keys among\navailable Bloom Filter slots if mis even number. Like-\nwise, a MMurmur hash function has higher accuracy\nthan Murmur hash function while the Murmur hash\nfunction is the \fnest non-cryptography hash function.\nTherefore, the performance of Bloom Filter using \u0010K\nlower than \u0007K. \u0003\n7 Discussion and Conclusion\nFrom the above experimental results, we can eas-\nily conclude that there is no requirement of the\ncryptography string hash function. To illustrate,\nthe MMurmur hash function is outrun all \flters\nwhere MMurmur has higher biased and redun-\ndant. Whereas, cryptography hash string hash\nfunctions have well distribution of keys. Gerbet\net al. claims that the cryptography string hash\nfunction can resist preimage and other issues.\nApparently, cryptography string hash functions\nare not required in Bloom Filter which has been\nproved experimentally in the experimental results\nand Theorem 3.\nObservation from the experiment, CBF has\nhigher memory footprint issue. With the same\nmemory footprint, conventional Bloom Filter is\nable to gain higher accuracy than CBF. However,\nCBF has a false negative free Bloom Filter pro-\nvided that there is no the counter under\row. CBF\nis easy to handle the deletion operations of Bloom\nFilter. However, it occupies more memory than\nany other \flters, that is, it has a higher false posi-\ntive probability. There is a few observations in CF.\nFirst, CF is not applicable is disjoint set which isde\fned in De\fnition 8, i.e., if the input set and\nquery set are disjoint, then the performance of CF\ndegrades. Also, false positive increases. Moreover,\nCF consumes higher memory footprint than other\nvariant of Bloom Filters. If CF is run again and\nagain with the same settings, then it can crash at\na point of time due to poor design of hashing. CF\nuses murmur2 hash function which is the \fnest.\nBut the utilization of murmur2 hash function with\nthe seed value becomes vulnerable to crash. Most\nimportantly, the FPP is not predictable in CF.\nThe FPP changes if CF is run again and again\nwith the same settings. Furthermore, CF mem-\nory footprint is higher if individual key sizes are\nlarge. The memory requirements depend on the\nindividual key size.\ndeepBF depends on prime numbers, for\ninstance, the dimensions m6=nof the Bloom Fil-\nter array are prime numbers. However, deepBF is\nable to perform with fewer hash functions due to\ntwo modulus operations in 2D Bloom Filter, which\nare performed by mandn. The key drawback\nof deepBF is the false positive in Bloom Filters.\nParticularly, if \u0016BFreturnstrue which is a false\npositive. Then, the valid URL is blocked. However,\nthe false positive probability is very less as shown\nin our experimental results. The deepBF com-\nprises of two-dimensional Bloom Filter (2D Bloom\nFilter) and evolutionary convolutional neural net-\nwork (evoCNN). deepBF uses two 2D Bloom\nFilter for malignant and benign URLs to \flter\nand these two \flters are \frst layer of the scanner.\nNaturally, Bloom Filters are very fast and if it is\nplaced in the \frst layer of the scanner, then load\non the machine is reduced. Firs, URLs are queried\nto the \flters. If the URLs are in the 2D Bloom Fil-\nters, it saves huge times. However, if a new URL\nis input, then both 2D Bloom Filters returns false.\nTherefore, evoCNN classi\fes the URL as malig-\nnant or benign. Again, these URLs are inserted\ninto the 2D Bloom Filters. Thus, 2D Bloom Fil-\nter implements learning patterns. Also, deepBF\ndepends on evoCNN. Finally, we conclude that\nthis work can be deployed in real world project\nto \flter out all malignant URLs e\u000bectively and\ne\u000eciently in diverse devices.\nStatements and Declarations\nCompeting Interests. The research work of\nDr. Anupam Biswas is supported by the Science\n\nSpringer Nature 2021 L ATEX template\ndeepBF 17\nand Engineering Board (SERB), Department of\nScience and Technology (DST) of the Government\nof India under (Grant No. EEQ/2019/000657) and\n(Grant No. ECR/2018/000204).\nReferences\n[1] Bloom, B.H.: Space/time trade-o s in hash\ncoding with allowable errors. Comm. of the\nACM 13(7), 422{426 (1970)\n[2] Chang, F., Dean, J., Ghemawat, S., Hsieh,\nW.C., Wallach, D.A., Burrows, M., Chan-\ndra, T., Fikes, A., Gruber, R.E.: Bigtable:\nA distributed storage system for struc-\ntured data. ACM Trans. Comput. Syst.\n26(2), 4{1426 (2008). https://doi.org/10.\n1145/1365815.1365816\n[3] Liu, W., Qu, W., He, X., Liu, Z.: Detect-\ning superpoints through a reversible counting\nbloom \flter. The Journal of Supercomput-\ning63(1), 218{234 (2013). https://doi.org/\n10.1007/s11227-010-0511-2\n[4] Patgiri, R., Nayak, S., Borgohain, S.K.:\nPassdb: A password database with strict pri-\nvacy protocol using 3d bloom \flter. Infor-\nmation Sciences 539, 157{176 (2020). https:\n//doi.org/10.1016/j.ins.2020.05.135\n[5] Singh, A., Garg, S., Batra, S., Kumar,\nN., Rodrigues, J.J.P.C.: Bloom \flter based\noptimization scheme for massive data han-\ndling in iot environment. Future Gener-\nation Computer Systems 82(2018), 440{\n449 (2017). https://doi.org/10.1016/j.future.\n2017.12.016\n[6] Nayak, S., Patgiri, R.: A review on role of\nbloom \flter on dna assembly. IEEE Access 7,\n66939{66954 (2019)\n[7] Fan, L., Cao, P., Almeida, J., Broder, A.Z.:\nSummary cache: A scalable wide-area web\ncache sharing protocol. IEEE/ACM Trans.\nNetw. 8(3), 281{293 (2000). https://doi.org/\n10.1109/90.851975\n[8] Kirsch, A., Mitzenmacher, M.: Less hashing,\nsame performance: Building a better bloom\n\flter. Random Struct. Algorithms 33(2),187{218 (2008)\n[9] Fan, B., Andersen, D.G., Kaminsky, M.,\nMitzenmacher, M.D.: Cuckoo \flter: Prac-\ntically better than bloom. In: Proceedings\nof the 10th ACM Intl. Conf. on Emerg-\ning Networking Experiments and Technolo-\ngies. CoNEXT '14, pp. 75{88. IEEE, Syd-\nney, Australia (2014). https://doi.org/10.\n1145/2674005.2674994\n[10] Patgiri, R., Nayak, S., Borgohain, S.K.:\nrDBF: A r-dimensional bloom \flter for mas-\nsive scale membership query. Journal of Net-\nwork and Computer Applications 136, 100{\n113 (2019). https://doi.org/10.1016/j.jnca.\n2019.03.004\n[11] Patgiri, R.: H\fl: A high accuracy bloom \flter.\nIn: 2019 IEEE 21st International Confer-\nence on High Performance Computing and\nCommunications; IEEE 17th International\nConference on Smart City; IEEE 5th Inter-\nnational Conference on Data Science and\nSystems (HPCC/SmartCity/DSS), pp. 2169{\n2174 (2019)\n[12] Mitzenmacher, M.: Compressed bloom \flters.\nIEEE/ACM Trans. Netw. 10(5), 604{612\n(2002). https://doi.org/10.1109/TNET.2002.\n803864\n[13] Lopez, P.: Dablooms: A Scalable, Counting,\nBloom Filter. Retrieved on April, 2020 from\nhttps://github.com/bitly/dablooms\n[14] Mamun, M.S.I., Rathore, M.A., Lashkari,\nA.H., Stakhanova, N., Ghorbani, A.A.:\nDetecting malicious urls using lexical anal-\nysis. In: Chen, J., Piuri, V., Su, C., Yung,\nM. (eds.) Network and System Security, pp.\n467{482. Springer, Cham (2016)\n[15] Mamun, M.S.I., Rathore, M.A., Lashkari,\nA.H., Stakhanova, N., Ghorbani, A.A.:\nURL dataset (ISCX-URL-2016). Retrieved\non April 2020 from https://www.unb.ca/cic/\ndatasets/url-2016.html\n[16] Luo, L., Guo, D., Ma, R.T.B., Rottenstreich,\nO., Luo, X.: Optimizing bloom \flter: Chal-\nlenges, solutions, and comparisons. IEEE\n\nSpringer Nature 2021 L ATEX template\n18 deepBF\nCommunications Surveys Tutorials 21(2),\n1912{1949 (2019)\n[17] Mun, J.H., Lim, H.: New approach for e\u000e-\ncient ip address lookup using a bloom \flter\nin trie-based algorithms. IEEE Transactions\non Computers 65(5), 1558{1565 (2016)\n[18] Singh, A., Garg, S., Kaur, K., Batra, S.,\nKumar, N., Choo, K.R.: Fuzzy-folded bloom\n\flter-as-a-service for big data storage in the\ncloud. IEEE Transactions on Industrial Infor-\nmatics 15(4), 2338{2348 (2019)\n[19] Lim, H., Lee, J., Byun, H., Yim, C.:\nTernary bloom \flter replacing counting\nbloom \flter. IEEE Communications Letters\n21(2), 278{281 (2017). https://doi.org/10.\n1109/LCOMM.2016.2624286\n[20] Appleby, A.: MurmurHash.\nRetrieved on Jan 2019 from\nhttps://sites.google.com/site/murmurhash/\n(2019)\n[21] Fowler, G., Noll, L.C., Vo, K.-P.: FNV\nHash. Retrieved on Aug 2019 from\nhttp://www.isthe.com/chongo/tech/comp/fnv/index.html\n(2012)\n[22] Eric: FastHash. Retrieved on April 2020 from\nhttps://github.com/ztanml/fast-hash\n[23] Peterson, W.W., Brown, D.T.: Cyclic codes\nfor error detection. Proceedings of the IRE\n49(1), 228{235 (1961). https://doi.org/10.\n1109/JRPROC.1961.287814\n[24] Hsieh, P.: Superfasthash.\nRetrieved on Aug 2019 from\nhttp://www.azillionmonkeys.com/qed/hash.html\n(2004)\n[25] Collet, Y.: XXHash. Retrieved on\nAug 2019 from https://create.stephan-\nbrumme.com/xxhash/ (2004)\n[26] Pagh, R., Rodler, F.F.: Cuckoo hashing.\nJournal of Algorithms 51(2), 122{144 (2004)\n[27] Mitzenmacher, M.: A model for learnedbloom \flters and optimizing by sandwich-\ning. In: Bengio, S., Wallach, H., Larochelle,\nH., Grauman, K., Cesa-Bianchi, N., Garnett,\nR. (eds.) Advances in Neural Information\nProcessing Systems 31, pp. 464{473. Curran\nAssociates, Inc., ??? (2018)\n[28] Kraska, T., Beutel, A., Chi, E.H., Dean,\nJ., Polyzotis, N.: The case for learned\nindex structures. In: Proceedings of the\n2018 International Conference on Man-\nagement of Data. SIGMOD '18, pp.\n489{504. Association for Computing\nMachinery, New York, NY, USA (2018).\nhttps://doi.org/10.1145/3183713.3196909.\nhttps://doi.org/10.1145/3183713.3196909\n[29] Feng, Y., Huang, N., Chen, C.: An e\u000ecient\ncaching mechanism for network-based url \fl-\ntering by multi-level counting bloom \flters.\nIn: 2011 IEEE International Conference on\nCommunications (ICC), pp. 1{6 (2011)\n[30] Dai, Z., Shrivastava, A.: Adaptive Learned\nBloom Filter (Ada-BF): E\u000ecient Utilization\nof the Classi\fer (2019)\n[31] Gerbet, T., Kumar, A., Lauradoux, C.: The\npower of evil choices in bloom \flters. In:\n2015 45th Annual IEEE/IFIP International\nConference on Dependable Systems and Net-\nworks, pp. 101{112 (2015)\n[32] Pourbabaee, B., Roshtkhari, M.J., Kho-\nrasani, K.: Deep convolutional neural net-\nworks and learning ecg features for screening\nparoxysmal atrial \fbrillation patients. IEEE\nTransactions on Systems, Man, and Cyber-\nnetics: Systems 48(12), 2095{2104 (2018)\n[33] Darwish, A., Hassanien, A.E., Das, S.: A\nsurvey of swarm and evolutionary comput-\ning approaches for deep learning. Arti\fcial\nIntelligence Review 53(3), 1767{1812 (2020)\n[34] Miller, G.F., Todd, P.M., Hegde, S.U.:\nDesigning neural networks using genetic algo-\nrithms. In: ICGA, vol. 89, pp. 379{384 (1989)\n[35] Angeline, P.J., Saunders, G.M., Pollack, J.B.:\nAn evolutionary algorithm that constructs\nrecurrent neural networks. IEEE transactions\n\nSpringer Nature 2021 L ATEX template\ndeepBF 19\non Neural Networks 5(1), 54{65 (1994)\n[36] Stanley, K.O., Miikkulainen, R.: Evolving\nneural networks through augmenting topolo-\ngies. Evolutionary computation 10(2), 99{\n127 (2002)\n[37] Leung, F.H.-F., Lam, H.-K., Ling, S.-H.,\nTam, P.K.-S.: Tuning of the structure and\nparameters of a neural network using an\nimproved genetic algorithm. IEEE Transac-\ntions on Neural networks 14(1), 79{88 (2003)\n[38] Gasc\u0013 on-Moreno, J., Salcedo-Sanz, S.,\nSaavedra-Moreno, B., Carro-Calvo, L.,\nPortilla-Figueras, A.: An evolutionary-based\nhyper-heuristic approach for optimal con-\nstruction of group method of data handling\nnetworks. Information Sciences 247, 94{108\n(2013)\n[39] Sun, Y., Xue, B., Zhang, M., Yen, G.G., Lv,\nJ.: Automatically designing cnn architectures\nusing the genetic algorithm for image classi-\n\fcation. IEEE Transactions on Cybernetics\n(2020)\n[40] Fan, B.: cuckoo\flter. Retrieved on April\n2020 from https://github.com/e\u000ecient/\ncuckoo\flter\n[41] Abadi, M., Agarwal, A., Barham, P., Brevdo,\nE., Chen, Z., Citro, C., Corrado, G.S., Davis,\nA., Dean, J., Devin, M., et al.: Tensor\row:\nLarge-scale machine learning on heteroge-\nneous systems (2015)",
  "textLength": 66386
}