{
  "paperId": "27d14f0602469e75a559932ca65dfe2fd978a98f",
  "title": "Superseding traditional indexes by orchestrating learning and geometry",
  "pdfPath": "27d14f0602469e75a559932ca65dfe2fd978a98f.pdf",
  "text": "Superseding traditional indexes by orchestrating\nlearning and geometry\nGiorgio Vinciguerra\nDepartment of Computer Science, University of Pisa, Pisa, Italy\ngiorgio.vinciguerra@phd.unipi.it\nhttps://orcid.org/0000-0003-0328-7791\nPaolo Ferragina\nDepartment of Computer Science, University of Pisa, Pisa, Italy\npaolo.ferragina@unipi.it\nhttps://orcid.org/0000-0003-1353-360X\nMichele Miccinesi\nDepartment of Computer Science, University of Pisa, Pisa, Italy\nm.miccinesi@studenti.unipi.it\nhttps://orcid.org/0000-0003-3138-8063\nAbstract\nThe explosion of big data and the new generation of applications and computing paradigms,\npossibly related to mobile and IoT scenarios, raise new challenges, such as strict latency, energy\nand storage constraints, which usually vary among devices, users and time because of intrinsic\nand extrinsic conditions not predictable in advance. The algorithmic community addressed these\nchallenges by focusing, among the others, on memory hierarchy utilisation [15, 38], query pro-\ncessing on streams [11], space eﬃciency [30, 35], parallel and distributed processing [19], and\nsoftware auto-tuning [29], just to mention a few.\nIn this paper, we contribute to these studies in a twofold manner. We design the ﬁrst learned\nindex (à la [27]) that solves the dictionary problem with provably better asymptotic time and\nspace complexity than classic indexing data structures for hierarchical memories, such as B-trees\nand Cache-Sensitive Search trees [34], and modern learned indexes [16, 27]. We call our solution\nthePiecewise Geometric Model index (shortly, PGM-index) because it turns the indexing of a\nsequence of keys into the coverage of a sequence of 2D-points via linear models (i.e. segments)\nsuitably learned to trade, in a principled way, query time vs space eﬃciency. This idea comes\nfrom the heuristic results of [16, 27] which we strengthen by showing that the minimal number\nof such segments can be computed via known and optimal streaming algorithms [32, 42]. The\nPGM-index is then obtained by recursively applying this geometric idea and by guaranteeing a\nsmoothed adaptation to the “geometric complexity” of the input data. Finally, we propose a\nvariant of the PGM-index that adapts itself not only to the distribution of the dictionary keys\nbut also to their access frequencies, thus obtaining the ﬁrst distribution-aware learned index .\nThe second main contribution of this paper is the proposal and study of the concept of\nMulticriteria Data Structure , namely one that adds to the classic requirements of being space\nand time eﬃcient, the novel feature of being ﬂexible enough to dynamically adapt itself to the\nconstraints imposed by the application of use. We show that the PGM-index is a multicriteria\ndata structure because its signiﬁcant ﬂexibility in storage and query time can be exploited by a\nproperly designed optimisation algorithm that eﬃciently ﬁnds its best design setting in order to\nmatch the input constraints (either in time or in space).\nSuch theoretical contributions are corroborated by a thorough experimental analysis over\nthree known and large datasets, showing that the PGM-index and its multicriteria variant im-\nprove uniformly, over both time and space, classic and learned indexes up to several orders of\nmagnitude. This makes our geometrically-learned approach potentially useful in modern DB-\nscenarios [26] and paves the way to novel investigations in the classic realm of data structure\ndesign for other problems, some of which are stated in the concluding section of this paper.\n\n:2 Superseding traditional indexes by orchestrating learning and geometry\n2012 ACM Subject Classiﬁcation Information systems →Data structures, Theory of compu-\ntation→Data structures and algorithms for data management, Theory of computation →Data\ncompression\nKeywords and phrases Multicriteria data structures, hybrid indexes, linear and nonlinear mod-\nels, external memory\nFunding Part of this work has been supported by the EU grant for the Research Infrastruc-\nture “SoBigData: Social Mining & Big Data Ecosystem” (INFRAIA- 1-2014-2015, agreement\n#654024) and by a Google Faculty Award 2016 on “Data Compression”\n1 Introduction\nThe ever-growing amount of information coming from Web, social networks and Internet of\nThings seriously slows down the management of available data. Advances in CPUs, GPUs and\nmemories hardly solve this problem without properly devised algorithmic solutions. Hence,\nmuch research has been devoted to dealing with this enormous amount of data, particularly\nfocusing on memory hierarchy utilisation [ 15,38], query processing on streams [ 11], space\neﬃciency [30, 35], parallel and distributed processing [19].\nDespite the formidable results achieved in these areas, we still miss proper algorithmic\nsolutions that are ﬂexible enough to work under computational constraints that vary across\nusers, devicesandtime[ 43,17]. Asanexample, fogandedgecomputingneedtheorchestration\nof heterogeneous devices like routers, smartphones, wearables and sensors so diverse in latency,\nenergy and storage constraints that it is too diﬃcult for a software engineer to choose the\ncorrect algorithmic solution on a per-device basis [28].\nIn this paper, we aim at formally digging into these issues, and so we restrict our attention\nto the case of indexing data structures that solve the classical static dictionary problem .\nClassic solutions can be grouped into four main families [ 22]: (i) hash-based, which range\nfrom traditional hash tables to recent techniques, like Cuckoo hashing [ 33]; (ii) tree-based,\nsuch asB-treesand its variants [ 18,34]; (iii) bitmap-based [ 8], which can take advantage\nof compression techniques; and (iv) trie-based, which are commonly used for string keys.\nUnfortunately, hash-based indexes do not support predecessor or range searches; bitmap-\nbased indexes can be expensive to store, maintain and decompress [ 39]; whereas tree- and\ntrie-based indexes are mostly pointer-based and, apart from recent compression results [ 14],\nkeys are stored uncompressed thus taking space proportional to the dictionary size.\nA novel and somewhat surprising approach to dictionary indexing was recently proposed\nby [27], expanding and improving some previous results of [ 2]. These authors suggested that\nindexes can be interpreted as models mapping keys to their rank in the sorted order (i.e.\ntheir positions). To better understand this statement, let us sort and store the nkeys of the\ninput dictionary Din an array A. Then, we can interpret any indexing data structure as\na function index :U→ [1,n]from the universe of keys to a position into A. IfDcontains\nrepeated keys, indexreturns the location of the ﬁrst occurrence in Aof the queried key (if it\nexists), so that the records sharing the same key can be retrieved by scanning the following\npositions in optimal output-sensitive manner. The function indexcould be easily extended\nto manage the case of keys not in Dby returning either the special value 0or the position in\nAof itspredecessor . Looking from the perspective of a mapping, this interpretation of the\nindexfunction seems not much a new one. Any one of the previous four families of indexes\nprovides eﬃcient, sometime asymptotically optimal, implementations of index. The novelty\nof the scenario proposed in [ 27] becomes apparent when we look at the input keys k∈Aas\n\nG. Vinciguerra, P. Ferragina, and M. Miccinesi :3\nk1 k5 k10 k15 k20 k2511020304050\nkeyposition\nFigure 1 A dictionary of ordered keys, represented as 2D-points (k,index (k)).\npoints (k,index (k))into the Cartesian plane, as shown in Figure 1. In this case, we can look\nat the implementation of indexas a Machine Learning (ML) problem in which we search for\nthe most succinct and eﬃcient model that best approximates that function.\nTo better understand the potentiality of this approach, let us start with a trivial example.\nConsider the case of a dictionary of integer keys a,a+ 1,a+ 2,...,a +n−1, whereamay be\nany integer. Here, index (k)can be computed easily as k−a+ 1, and thus it takes constant\ntime and space to be implemented, independently of the number nof keys to be indexed.\nThis example can be generalised to any set of keys kiwhose pairs (ki,i)are distributed over\na line, with a proper slopeandintercept . In this case too, the indexfunction for a key k\ncan still be implemented in constant time and space as index (k) =slope×k+intercept ,\nwhich is again independent of the number nof keys to be indexed. These simple examples\nshed light on the potential compression opportunities oﬀered by patterns and trends in data\ndistribution which are more frequent than expected, as we will show in the experimental\nsections. However, we cannot argue that all datasets follow a “linear trend”, nor that the\nalgorithm designers develop ad-hoc solutions for each real-world data distribution. In the\ngeneral setting, we have therefore to design proper ML techniques that learntheindex\nfunction by extracting the patterns present in the input data by means of proper models\nwhich may range from linear to more sophisticated functions. Clearly, learning has to be\norchestrated with eﬃciency in query time and occupied space of the learned model, since\nindexmust be used on-the-ﬂy in some running applications.\nThis is exactly the design goal pursued very recently by Kraska et al. [ 27] with their\nRecursive Model Index (RMI), which uses a hierarchy of ML models organised as a Directed\nAcyclic Graph (DAG) and trained to learn the input data distribution, such as the one of\nFigure 1. At query time each model, starting from the top one, takes the queried key kas\ninput and picks the following model in the DAG that is “responsible” for that key. The\noutput of RMI is the position returned by the last queried ML model, which is, however, an\napproximate position for the queried key in A. A ﬁnal binary search is thus executed within\na range of neighbouring positions whose size depends on the prediction error of RMI.\nOne could presume that ML models cannot provide the guarantees ensured by traditional\nindexes, both because they can fail to learn the distribution and because they can be expensive\nto evaluate [ 26]. Unexpectedly, on datasets of 200M entries, Kraska et al. [ 27] reported\nthat RMI always dominates the B-treeperformance, being up to 1.5–3 ×faster and, very\nsigniﬁcantly, two orders of magnitude smaller in space.\n\n:4 Superseding traditional indexes by orchestrating learning and geometry\n30 35 40 4512345691011121314\nkfs(k)[pos−ε,pos+ε]\nkey\nFigure 2 An example of linear approximation of a set of integer keys within the range [27,46].\nThe encoding of the linear model takes only two ﬂoats, and thus it is independent of the number of\n“encoded” keys, provided that they can be “linearly learned” with an ε-error.\nThis surprising result is stimulating a lot of interest in the algorithmic and the DB\ncommunities [ 4,22,26] on RMI’s design and limitations. The RMI introduces another set of\nspace-time trade-oﬀs between model size and query time which are diﬃcult to control because\nthey depend on the data distribution, on the DAG structure and on the complexity of the\nML models adopted. This motivated the recent introduction of the A-tree [ 16] which uses\nlinear models only, a B-treestructure to index them, and provides an integer parameter ε≥1\ncontrolling the size of the region in which the ﬁnal binary search step has to be performed.\nFigure 2 shows an example of a linear model fsapproximating fourteen keys within the range\n[27,46]and its use in determining the approximate position of a key k= 37, which is indeed\nfs(k)≈8instead of the correct position 6, thus making an error ε= 2. The experiments\nof [16] show that the A-tree improves the time performance of the B+-treewith a space\nsaving of up to four orders of magnitude. However, the major limitation of A-trees lies in the\nway they compute the linear approximation of the input keys: the algorithm is sub-optimal\nin theory and pretty ineﬃcient in practice. This prevents us to fully exploit the space-time\npotentiality of such learned indexes (as we will show in the experimental sections).\nIn this paper, we contribute to both the design of optimallearned indexes and the\nautomatic selection of the best (learned) index that ﬁts the requirements of an underlying\napplication in ﬁve main steps, which we summarise below.\n1.We orchestrate geometric and learned indexes via a novel recursive approach, so to obtain\nwhat we call the Recursive PGM-index . This index adapts smoothly to the “geometric\ncomplexity” of the input keys, for which we also provide some non-trivial lower bounds\n(see Section 2.1.2). As a result, the PGM-index is the ﬁrst learned index which is provably\nbetter than classic and learned indexing data structures (see Section 2.3, Theorem 6,\nand Table 1).\n2.We propose a variant of the PGM-index that adapts itself not only to the distribution of\nthe dictionary keys but also to their access frequencies. This novel contribution, which\nwe callDistribution-Aware PGM-index , has the query time of biased data structures [ 3,\n5,13,25,36], but a space occupancy that does not depend on the number of keys and\nadapts to the “geometric complexity” of the dataset, thus resulting very succinct in space\ntoo (see Section 3 and Theorem 7).\n3.We show how to generalise the PGM-index to use not only linear models but also more\ncomplex regression models, such as neural networks of few neurons (just to keep their\n\nG. Vinciguerra, P. Ferragina, and M. Miccinesi :5\nspace succinct). In this case, we devise an optimisation strategy that explores the space\nof regression models to be ﬁt in the structure of the PGM-index. We call this new data\nstructure HybridPGM-index , and we show that it empowers the hybrid indexes introduced\nby Kraska et al. [ 27] with the additional features to be constructed automatically and\nwith a user-controlled approximation quality. The net result is that our PGM-index turns\nout to be as general as the RMI [ 27] and with theoretical guarantees better than the\nA-tree [16], therefore resulting in a replacement for both of them (see Section 2.2).\n4.We design a framework that automatically and eﬃciently optimises the PGM-index\ngiven a space or a time constraint. This is where the combination between multicriteria\noptimisation and learned indexes shows its full potential, and leads us to devise the novel\nconcept of Multicriteria Data Structure that we introduce in this paper and which ﬁnds in\nthePGM-index one possible instance (see Section 4). A multicriteria data structure, for\na given problem P, is deﬁned by a pair /angbracketleftF,A/angbracketrightPwhereFis a family of data structures,\neach one solving Pwith a proper trade-oﬀ in the use of some computational resources\n(such as time, space, energy, etc.), and Ais a properly designed optimisation algorithm\nthat eﬃciently selects in Fthe data structure that “best ﬁts” an instance of P. We\ndemonstrate the fruitfulness of this new class of data structures by focusing on the\nparadigmatic static dictionary problem in which Fis the family of PGM-index es, and the\noptimisation algorithm exploits a simple space-time cost model to explore Feﬃciently.\nThis result makes the PGM-index the ﬁrst multicriteria data structure to date.1\n5.Our last contribution is a thorough set of experiments over some known and large datasets\n(see Section 5), in which we compare the performance of our PGM-index against classic\nindexes (namely, B-trees and CSS-trees) and learned indexes (namely, A-trees and RMIs).\nFor example, we show that our index improves the space occupancy of the A-tree by\n60%, of the CSS-trees by a factor 82.7 ×, and of the B-treeby more than four orders of\nmagnitude, while achieving their same or even better query eﬃciency. With respect to the\nRMI, the PGM-index oﬀers theoretical guarantees on query time and space occupancy,\nand the experiments on its Multicriteria variant show improved performance. In summary,\nour experimental results for the Multicriteria PGM-index support the vision of a new\ngeneration of big data processing systems, in which data structures and their algorithms\ncan be tailored to the application, device and user [21, 26].\nOverall we believe that our results, apart from their individual contributions, pave the way\nto many other challenging theoretical and algorithm-engineering problems that we summarise\nin the last Section 6.\n2 The Piecewise Geometric Model Index\nThe Piecewise Geometric Model index ( PGM-index ) is a parameterised data structure which\nsolves the static dictionary problem over a sorted array Aofnkeys from a universe Uof real\nnumbers. Precisely, given an integer parameter ε≥1, thePGM-index learns an approximate\nmapping between keys from the universe Uand their positions in the input array A.2Given\na query key k, thePGM-index ﬁnds ﬁrst a position posinAwhich is at most εaway from\n1For completeness, we mention that the concept of multicriteria optimisation has been already applied in\nAlgorithmics to data compression [ 12], compiler optimisation [ 20], and software auto-tuning [ 29], but as\nfar as we know it is new in data structures design.\n2As anticipated in the introduction, if k/negationslash∈Athe “position” of kis the one of its predecessor inA.\n\n:6 Superseding traditional indexes by orchestrating learning and geometry\nA\n[pos−ε, pos +ε]layers [3]k\n1 2 3 4 5 6 7s1s2s3s4s5s6s7\ns4= (key,slope,intercept )layers [2]s8s9s10s11 layers [1]\npos=fs4(k)\nFigure 3 An example of PGM-index . The mapping between keys and positions in Ais modelled\nby linear models (i.e. segments). Each segment in the last level levels [3]is guaranteed to approximate\nthe data distribution in a subarray of A, depicted by wavy lines, with accuracy ε. The next upper\nlevels contain segments that approximate the distribution of a subset of keys, namely the ﬁrst ones\n“covered” by the segments in the level below, delimited by dotted lines, still with accuracy ε. In the\nﬁgure, s9inlevels [2]approximates with accuracy εthe distribution of the ﬁrst keys of the segments\ns4ands5inlevels [3]. At query time, the search for a key kstarts from the root node in levels [1]\nand exploits its (unique) segment, i.e. s11, to ﬁnd in constant time the ε-approximate position of k\namong the covered segments, i.e. s8, s9, s10atlevels [2]. This approximate position is then reﬁned\nvia a binary search over a subsequence of 2εsegments centred in that position. This ﬁnds, in the\nexample, that kis covered by segment s9; hence, the search goes on recursively at levels [3]and then\nreaches the last level containing the array A.\nthe correct position of k, and then searches kin the subarray A[pos−ε,pos+ε]via binary\nsearch or other approaches.\nThe key tool on which the deﬁnition of the PGM-index hinges on is a Piecewise Linear\nApproximation model (PLA-model) that implements eﬃciently in time and space the ap-\nproximate mapping from a set of keys to their positions in the sorted array A. By leveraging\nthis tool, described in Section 2.1, the PGM-index turns an array of keys Ainto an array of\nlinear models (i.e. segments) each one taking a constant space (namely, two ﬂoats and one\nkey) and constant query time to return the ε-approximate position of a queried key in the\nindexed subarray. Therefore, every segment is independent in time and space of the number\nof its indexed keys. In the rest of this section, we will concentrate the description of the\nPGM-index to the case of linear models and defer to Section 2.2 its generalisation to the\ncase of nonlinear models which are used to ε-approximate the positions of the keys in A.\nThePGM-index is built on top of the array of linear models in several ways which\nare explored in Section 2.3, e.g. via a B-treeor a Cache-Sensitive Search tree (CSS-tree).\nActually, any indexing data structure could be adopted on top of the array of models to route\nthe search for a key among them. However, each of these solutions would not make the most\nof the constant space-time indexing feature oﬀered by a single linear model. As a consequence,\n\nG. Vinciguerra, P. Ferragina, and M. Miccinesi :7\nwe design the overall PGM-index by recursively constructing a series of PLA-model as follows.\nFirstly, we construct the PLA-model over the whole array A. Secondly, we turn the individual\nmodels of this PLA-model into a “proper set of keys”. Thirdly, we construct another (and\nsmaller) PLA-model over those keys. This process continues recursively until one single\nlinear model is obtained, which will form the root of our data structure.\nOverall, each PLA-model forms a level of the PGM-index , and each model of that PLA-\nmodel forms a node of the data structure at that level, thus originating a sort of B-tree. The\nspeciality of the PGM-index with respect to a B-treeis threefold: (i) the routing table at\neach node is given by a single linear model which guarantees constant space occupancy and\nlogarithmic query time driven by the binary search over a subarray of size 2ε; (ii) the fan-out\nof each node is variable and typically very large, depending on the “geometric complexity” of\nthe set of indexed keys, so that the traversal of the PGM-index is fast; (iii) the PGM-index\nadapts its structure and its space occupancy to the distribution of the input keys, resulting\nas much independent as possible of their number. Figure 3 provides a pictorial example of a\nPGM-index built over Aand with a PLA-model of seven segment at the last level.\n2.1 The piecewise linear model\nLet us be given a sorted array A= [k1,k2,...,k n]ofn(real and possibly repeated) keys\ndrawn from a universe U, and let us denote with index :U→ [1,n]the mapping from keys\ninAto the position of their ﬁrst occurrence in that array. We “extend” index (k)to return\nthe position of the predecessor ofkinAifk/negationslash∈A.\nThe goal of this section is to describe an eﬃcient implementation of index (k)via a\nPiecewise Linear Approximation model, shortly referred hereafter with PLA-model. Given\nan integerε≥1, we say that a PLA-model built on Aand with error εis a sequence of\nmlinear models (i.e. segments) s1,s2,...,s m, each one approximating up to error εthe\nposition of the keys in a subarray of A. Precisely, the jth segment sjis deﬁned by a triple\n(kij,slopej,interceptj)that indexes the range of the universe keys [kij,kij+1)via a segment\nof slope slopejand intercept interceptj. Each segment takes constant space to be stored, just\ntwo ﬂoats and one key, and it is used to approximate the position of any key k∈[kij,kij+1)\nby using the function fsj(k) =k×slopej+interceptj, as depicted in Figure 2. We say that\nthe position fsj(k)isε-approximate in the sense that the correct position index (k)is no\nmore thanεpositions away from the position fsj(k)returned by the segment sj. Therefore,\neach segment can be seen as an approximate predecessor search data structure for its covered\nrange of keys (namely, [kij,kij+1)) with constant query time and constant occupied space.\nWe remark again that this is the interesting property of these segments because their space\noccupancy and access time do not depend on the size of the indexed range and thus, in turn,\non the number of indexed keys.\nGiven the integer parameter ε≥1, thepiecewise linear ε-approximation problem consists\nof computing the PLA-model which minimises the number of segments m, provided that each\nof them oﬀers an ε-approximation of the positions of the keys in Aoccurring in its covered\nrange. The following Sections 2.1.1 and 2.1.2 describe a linear time and space algorithm\nthat solves optimally this problem and provide some upper and lower bounds for mand the\nnumber of points covered by each segment, respectively.\n2.1.1 Optimal construction of the optimal PL-model\nThe piecewise linear approximation problem can be solved by dynamic programming in O(n3)\ntime which is prohibitive on big data. Thus some authors [ 16] attacked this problem via a\n\n:8 Superseding traditional indexes by orchestrating learning and geometry\nheuristic approach, called shrinking cone, which runs in linear time, but it does not guarantee\nany bound on the number of computed segments. Actually, these same authors showed\nexamples of distributions of keys that are very worse for the eﬃcacy of that algorithm.\nIt is interesting to notice that this problem has been extensively studied for lossy\ncompression and similarity search of time series (see e.g. [32, 7, 10, 9, 42] and refs therein),\nand it admits streaming algorithms which take O(n)optimal time. The key idea of this\nfamily of approaches is to reduce the ε-approximation problem to the one of constructing\nconvex hulls of a set of points S, which in our case is S={(ki,index (ki))}i=1,...,n. Then,\nthe convex hull is enclosed in the thinnest rectangle (or strip) of height no more than 2ε\nfrom which the PLA-model is computed by taking the line which splits that rectangle in two\nhalves.\nFor our application to the dictionary problem, we implemented the optimal algorithm\nof [42]. Theoretically, we will make then use of the following result:\nITheorem 1. Let us be given a sequence S={(xi,yi)}i=1,...,nof two-dimensional points\nthat are nondecreasing in their x-coordinates. There exists a streaming algorithm that in\nlinear time and space computes the piecewise linear ε-approximation of Sformed by the\nminimum number of segments.\n2.1.2 Some simple bounds\nIn typical applications occurring mainly in the storage of posting lists of search engines or of\nthe adjacency lists of graphs (e.g. Web graphs or Social Network graphs), the keys are n\nincreasing integers (i.e. the docIDs or the nodeIDs) which occur in a range [d,d+U). We\ncan give a tight upper bound on the minimum number of strips that are needed to cover the\npoints as a function of 2ε, the upper bound which we impose on their height.\nWhen counting strips, the last one is a special case because it is not yet completed since\nit can be continued by adding other points to the right; to distinguish it from the other\ncases, we will call the other full strips . We use/lscriptto denote the length of a full strip, from\nits beginning to the beginning of the next strip, and mto denote the number of points it\ncontains.\nIn Figure 4 we consider a worst-case situation in which the ﬁrst mpoints are aligned and\nconsecutive in their x-coordinates, so that they impose the maximum slope in the ﬁnal strip\nthat covers them. Then we take the (m+ 1)th point far away to the right at distance /lscriptfrom\nthe ﬁrst point. We notice that the minimal strip including all these m+ 1points has the\nlower edge passing through the points (0,0)and(/lscript,m), and the upper edge parallel to the\nlower edge and passing through the point (m−1,m−1). The height hof that strip can be\ncomputed by simple geometric considerations, and it is (m−1)(/lscript−m)//lscript. By imposing that\nthe current strip cannot include the (m+ 1)point and still guarantee a 2ε-approximation,\nnamelyh>2ε, we obtain /lscript>m (m−1)/(m−1−2ε).\nLet us now study the function f(m) =m(m−1)/(m−1−2ε)as our continuous lower\nbound to the length of a full strip with mpoints. Our ﬁrst observation is that the “average\nspace” per point (namely, its average covered x-range) is at leastf(m)\nm= 1 + 2ε/(m−1−2ε),\nwhich is decreasing with mas expected. The second observation is that fhas a minimum\nforˆm=√2ε+ 1(√\n2ε+√2ε+ 1), and for that value it is f(ˆm) = (√\n2ε+√2ε+ 1)2, so\nfull strips will be longer than 8ε+ 1. But more importantly, we see that fis a convex\nfunction, so in order to determine an upper bound to the number of full strips needed to\ncover all the points, we can consider all of them to have the same length and thus impose:\n⌊n/m⌋f(m) + (nmodm)≤U. Focusing on the ﬁrst addendum, which is the one of the full\n\nG. Vinciguerra, P. Ferragina, and M. Miccinesi :9\n0 1 2 3 4 5 6 7 8 910 11 1201234\nh\n(0,0)≡begin of the strip(m−1, m(m−1)//lscript)(/lscript, m) begin of the next strip ≡\n(m−1, m−1)\nkeyspositions\nFigure 4 Points inducing a full strip of height h >2ε.\nstrips, we ﬁnd f(m)≤(Um)/n, and then substituting the deﬁnition above for f(m), we\nderivem≥1 + 2εU/(U−n)which clearly holds on average over the maximum number of\nfull strips created to cover Swith height at most 2ε.\nILemma 2. Let us be given a sequence of ntwo-dimensional points with integer coordinates,\nin a range of size Usuch that their x-coordinates are distinct and sorted increasingly, and\ntheiry-coordinates grow of one unit at each point. The full strips will have length at least\n8ε+ 1.\nWhen the points are displaced in such a way to produce the maximum number of full\nstrips, beholding the constraints on their range and their number, the optimal algorithm of\nTheorem 1 creates full strips which on average contain at least 1 +2εU\nU−npoints each.\nWe notice that as U→n, all points have contiguous x-coordinate, and they are thus\npacked in a range of size n. Then, the lower bound diverges because we can ﬁt all points\nofSin one strip. If instead U→+∞, the lower bound converges to 1 + 2ε. Actually, this\nlower bound holds for any U, even for points with repeated x-coordinate, so we have:\nICorollary3. Let us be given a sequence of ntwo-dimensional points with integer coordinates,\nin a range of size Usuch that their x-coordinates are sorted nondecreasingly, and their y-\ncoordinates grow of one unit at each point. The optimal algorithm of Theorem 1 creates\nsegments which contain at least 2εpoints each.\nProof.Any chunk of 2εconsecutive keys ki,ki+1,...,k i+2ε−1inAcan be covered by the\nε-approximate segment having null slope and intercept equal to i+εbecause those keys\ngenerate the points (ki,i),(ki+1,i+ 1),..., (ki+2ε−1,i+ 2ε−1)and thus they have y-distance\nat mostεfrom the line y=i+ε. J\nFrom the previous calculations, it is also immediate to derive the desired upper bound on\nthe number of strips.\nICorollary 4. When the keys are ndistinct increasing integers occurring in a range [d,d+U)\nwithn=αU, the optimal algorithm of Theorem 1 computes at most/ceilingleftBig\nn\n1+2ε\n1−α/ceilingrightBig\nsegments for\nan optimal piecewise linear ε-approximation of S.\nAs a last comment we notice that, by allowing each key to be repeated at most ktimes\n(which means vertical runs in S’s points when they are mapped to the Cartesian plane), the\n\n:10 Superseding traditional indexes by orchestrating learning and geometry\n120406080100\nkeyposition\nkey\nFigure 5 The case of a PLA-model (left) vs a PNA-model (right): the former uses more “pieces”\n(i.e. 4 vs 2) to guarantee the same ε-approximation of the input sequence of 2D-points.\neﬀect on the geometry is the possibility to stretch the coordinates of the points up to ktimes,\nso that the upper bound in Corollary 4 above can be rewritten by substituting αwithα/k.\nIn Section 4.1, the lower bound of Corollary 3 will be used to model the space occupancy\nof a PGM-index, which is an essential ingredient of its multicriteria version.\n2.2 Beyond linear models\nIn Section 2.1.1, we described an optimal solution to the piecewise linear ε-approximation\nproblem, thus focusing on the ε-approximation of a sequence of points via segments. Non-\netheless, it is reasonable to expect that more sophisticated models are more likely to cover\nlarger regions of the key space due to their increased power in capturing complex data rela-\ntionship. The net result could be a reduction in the number of models used in the piecewise\napproximation of the key space and hence a reduction of space overhead, as depicted in\nFigure 5. Another beneﬁt could be that some kind of models, like neural networks, could\ntake advantage of the speed-up given by GPUs, TPUs and other ML accelerators, which not\nonly are ubiquitous (even in consumer hardware such as iPhone’s Neural Engine or Google’s\nPixel Visual Core) but are also expected to be greatly improved in the future [23, 26].\nFor these reasons, we now turn our attention to the general (i.e. nonlinear) piecewise\nε-approximation problem of a sequence of keys stored in an array A. In this problem, we\nare given an integer parameter ε≥1, a family of model types Msorted by complexity\n(i.e. space occupancy), and we are asked to compute the minimum number mof models\nwhichε-approximate the positions of the keys in A. The result is called Piecewise Nonlinear\nApproximation model (PNA-model), and it clearly generalises the PLA-model in that it uses\nboth linear and nonlinear models in ε-approximating the positions of the input keys.\nThe approach we propose consists of reﬁning in a top-down fashion the current PNA-model\nby adding more and more models until it guarantees the desired ε-approximation over all the\nkeys inA. We call this algorithm Top-Down-Regression . Precisely, it starts building the\nPNA-model from the simplest model f∈M(i.e. the ﬁrst one, and thus less complex model\nin the sorted order, i.e. the segment), trained on the whole array A. Then, it checks whether\nfis anε-approximation of A, i.e. it computes the absolute distance between the estimated\npositionf(k)and the true position index (k)for each key k∈A. If every such distance is no\nmore thanε, the algorithm stops and returns f. Otherwise, it picks the next (more complex)\nmodel inMand repeats the training. If no model in Mis able toε-approximate A[1,n],\n\nG. Vinciguerra, P. Ferragina, and M. Miccinesi :11\nthen a breakpoint pis suitably chosen, and the procedure is repeated recursively on the\nsubarraysA[1,p]andA[p+ 1,n].\nThe choice of the breakpoint pfor a subarray A[a,b]is crucial for the eﬃciency and\neﬃcacy of the overall approach. Following [ 24], we could choose pas the breakpoint that\nminimises the error made by the two models that cover A[a,p]andA[p+ 1,b]. However,\nthe computation of pwould incur in a quadratic time cost because we should recompute\nthe errors made by the two models for every breakpoint. This is prohibitive in time for the\ndataset sizes we will manage in our experiments.\nInstead, we suggest to scan the array of errors made for the keys in A[a,b]and placepin\na “problematic area” of A, intuitively, an area where the approximation error is too high or\ngrows too much. In the experimental Section 5.1.2, we will explore diﬀerent deﬁnitions of\nproblematic area and ﬁnd that the best choice is the one that puts two breakpoints at the\nstart/end positions of the longest chain of errors larger than ε, i.e., consecutive keys of A\nwhose error in approximating their position with the simplest model is larger than ε.\nAt the end of the top-down phase, the algorithm scans the array of computed models\nand merges some neighbours if the approximation induced by the resulting merged model\nis no more than ε. The rationale of this last step is to limit the number of models created,\nas the placement of breakpoints could be too much greedy and thus overﬁt the distribution\nof the keys in A. For example, merging a deep neural network with its following segment s\nconsists of training the deep neural network also on the pairs {(k,index (k))}k∈S, whereS\nis the set of keys covered by s. A model can be merged only with simpler models, not the\nother way around because if a region has been covered with a complex model, it means that\nit is too irregular to be covered by a simpler one.\nThe pseudocode of the algorithm is shown in Figure 6. Its actual time complexity depends\non the input data. In the best case, the simplest (i.e. linear) model is enough to ε-approximate\nthe whole array A, thus the time cost is O(n). In the worst case the complexity is O(n3), but\nin practice it will result that the breakpoint taken at Line 14 of Top-Down-Regression\nwill partition the subarrays into balanced parts thus inducing a time-complexity recurrence\nofT(n) = 2T(n/2) + Θ(n|M|). Reasonably assuming that |M|is a constant, the recurrence\nhas solution T(n) = Θ(nlogn). At the end, the procedure Mergetries to merge the m\nmodelsε-approximating array A, by calling procedure Model-Merge and taking O(nm)\ntime. So we conclude that Top-Down-Regression runs inO(nlogn+nm)time, under the\nassumptions that the subproblems have comparable size at every step and that Mconsists\nof a constant number of models.\n2.3 Indexing piecewise (non)linear models\nThe two algorithms introduced in Sections 2.1.1 and 2.2 return a piecewise ε-approximation\nof the positions of the keys in the input array Avia a sequence Mofmmodels that are\neither linear, as in the ﬁrst algorithm, or possibly nonlinear, as in the second algorithm.\nNow, in order to solve the (static) dictionary problem, we need a way to ﬁnd the model f\nresponsible for estimating the ε-approximate position posof a queried key k, namely, the\nrightmost model fsuch thatf.key≤k. Whenmis small, this search can be implemented\nby a linear scan over the attribute f.key. Otherwise, when mis large, we need to binary\nsearchMor index it via a proper data structure, such as a multi-way search tree such as\nB-treeor CSS-tree [ 34]. The membership query is thus answered in three steps. First, the\nmulti-way search tree is queried to ﬁnd the rightmost model fsuch thatf.key≤k. Second,\nthe modelfis used to estimate the position pos=f(k)for the queried key k. Third, the\nexact position of kis determined via a binary search within A[pos−ε,pos+ε]. It is possible\n\n:12 Superseding traditional indexes by orchestrating learning and geometry\nTop-Down-Regression (A, ε,M)\n1 let modelsbe an empty list\n2 let stackbe an empty stack\n3Push (stack,(1, n))\n4whilenotEmpty (stack )\n5 a, b=Pop(stack )\n6 foreachModel∈M\n7 f=new Model (A, a, b )\n8 errors =Compute-Errors (A, a, b, f )\n9 ifMax (errors )≤ε\n10 break\n11 ifMax (errors )≤ε\n12 List-Append (models , f)\n13 else\n14 choose a breakpoint pforA[a, b]\n15 Push (stack,(p+ 1, b))\n16 Push (stack,(a, p))\n17return Merge (A, ε, models )Merge (A, ε, models )\n1 let mergedbe a list containing only models [1]\n2fori= 2toSize(models )\n3 let tailbe the last item in merged\n4 ifnottail.Is-Mergeable (models [i])\n5 List-Append (merged ,models [i])\n6 else\n7 f=tail.Model-Merge (models [i])\n8 ifCompute-Errors (models [i])≤ε\n9 List-Delete-Tail (merged )\n10 List-Append (merged , f)\n11 else\n12 List-Append (merged ,models [i])\n13return merged\nFigure 6 The algorithm for the piecewise nonlinear ε-approximation problem. The procedure\nModel (A, a, b )trains a model f∈MonA[a, b], while the procedure Compute-Errors (A, a, b, f )\nreturns an array of size b−a+ 1whose ith item is the error induced by model fin estimating the\nposition of key k=A[a+i−1], namely,|⌊f(k)⌋−index (k)|.\nto support also successor, predecessor3and range queries with simple adjustments to the\nﬁnal step.\nThe combination of an indexing data structure with the piecewise (linear or nonlinear) ε-\napproximation of the positions of the keys in Aconstitutes our novel index, called PGM-index ,\nwhich stands for Piecewise Geometric Model index (see Figure 3 for a pictorial example).\nITheorem 5. ThePGM-index takes Θ(m)space and answers membership, successor and\npredecessor queries in O(tindex(m) +logε)time, where the integer ε≥1denotes the error\nguaranteed in the approximation of the positions of keys in A[1,n]by them(linear or\nnonlinear) models created over them, and tindex(m)is the time needed to ﬁnd the model\nresponsible for the queried key by using the data structure built over those mmodels.\nThe eﬃciency in time and space of the PGM-index strictly relies on the value of mwhich\nis smaller than or equal to n/(2ε)because of Corollary 3. Hence, m<ngiven thatε≥1. In\npractice, as we will show in the experimental Section 5.1, mis signiﬁcantly smaller than nso\nthat the space savings are up to several orders of magnitude. This will impact onto tindex(m)\ntoo, and thus onto the eﬃciency of the query operations.\nHowever, the indexing strategy above does not take full advantage of the Piecewise\nGeometric Modelling of a sequence of keys that we are introducing in this paper because\nit resorts a classic data structure to index M. Therefore, we introduce a third (successful)\nstrategy to index Mwhich consists of repeating the Piecewise Geometric Modelling process\nrecursively on a proper set of keys derived from the previous modelling steps until there is\nonly one model left. More precisely, we start with the sequence of models Mconstructed over\nthe whole input array A, we then take the ﬁrst key of Acovered by each model in Mand\nﬁnally construct another (and smaller) PNA-model over those m=|M|keys. We proceed\nthis way recursively until the PNA-model consists of one unique model. Each PNA-model\n3Deﬁned as successor (q) =kisuch that ki−1< q≤ki. Symmetrically, predecessor (q) =kisuch that\nki≤q < k i+1.\n\nG. Vinciguerra, P. Ferragina, and M. Miccinesi :13\nRecursive-Geometric-Modelling (A, ε)\n1 let levelsbe an empty list\n2keys =A\n3repeat\n4 l=Build-Piecewise-Model (keys, ε)\n5 List-Prepend (levels , l)\n6 m=Size(l)\n7 keys = [l[1].key, . . . , l [m].key]\n8until m= 1\n9return levelsQuery-R (A, ε, levels , k)\n1pos=levels [1][1]( k)\n2fori= 2toSize(levels )\n3 lo= max{pos−ε,1}\n4 hi= min{pos+ε,Size(levels [i])}\n5 f=rightmost model f/primeinlevels [i][lo, hi ]\ns.t.f/prime.key≤k, found by binary search\n6 g=the model at the right of f\n7 pos= min{⌊f(k)⌋,⌊g(g.key)⌋}\n8lo= max{pos−ε,1}\n9hi= min{pos+ε, n}\n10returnbinary search for kinA[lo,hi]\nFigure 7 The pseudocode to build and query a PGM-index where the models are in turn indexed\nbyseveralrecursivelevels. Build-Piecewise-Model iseitherthealgorithmdescribedinSection2.1.1\nto build linear models, or the one in Section 2.2 to build (non)linear models that ε-approximate the\npositions of the keys in A.\nwill form a level of the PGM-index , and each model of that PNA-model will form a node\nof the data structure at that level, thus forming a sort of B-tree. The speciality of this\nRecursive PGM-index is threefold: (i) the routing table at each B-treenode is given by a\nsingle model which guarantees, unlike classic arrays in B-trees, constant space occupancy\nand logarithmic query time driven by the parameter ε; (ii) the fan-out of each node, unlike\nclassicB-trees, is variable and typically very large, depending on the ε-approximability of\nthe positions of the indexed keys; (iii) given the previous point (ii), the depth of the B-tree\nwill be usually very small, and in general the Recursive PGM-index will adapt its structure\nand its space occupancy to the “geometric distribution” of the input keys resulting highly\ncompressed, as we will show in the experimental section.\nThe membership query over the Recursive PGM-index works as follows. At every level,\nwe take the model referring to the visited node and use it to estimate the position of the\nsearched key kamong the keys of the next level. The real position is then found by a binary\nsearch in a range of size 2εcentred around the estimated position. Given that every key on\nthe next level is the ﬁrst key covered by a node on that level, we have identiﬁed the next node\nto visit, and the process continues until the last level of A’s keys is reached.4The pseudocode\nof this construction process, which we call Recursive-Geometric-Modelling , is shown\nin Figure 7 together with the procedure Query-R that implements the search for a key into\nthe obtained Recursive PGM-index . For a pictorial example of a Recursive PGM-index , we\nrefer the reader to the previous Figure 3.\nWe observe that, when working in hierarchical memories, it could be advantageous to\nmodify Recursive-Geometric-Modelling to use a diﬀerent ε/lscriptin each level /lscript∈[1,L]of\nthe Recursive PGM-index . In fact, we could ﬁt the ﬁrst (L−1)levels in the cache and set\nε/lscriptto the cache-line size, and we could store the input array Ain the secondary memory and\nsetεLto a multiple of the disk-page size.\nITheorem 6. The Recursive PGM-index takes Θ(m)space and answers membership,\nsuccessor and predecessor queries in O(logm)time. In the External Memory (EM) model,\n4Some care has to be taken to correctly approximate the positions of keys falling between the last key\ncovered by a model fand the ﬁrst key (i.e. g.key) covered by the model gat the right of f. This is\nthe case of keys that do not occur in A, and for which we are asking to search for their predecessor or\nsuccessor. The subtle issue is that, for these keys, model fis not guaranteed to return an ε-approximate\nposition in A. This subtlety is managed by Line 7 of procedure Query-R (shown in Figure 7).\n\n:14 Superseding traditional indexes by orchestrating learning and geometry\nqueries take O((logcm)log(ε/B))I/Os, where the integer ε≥1denotes the error guaranteed\nin the approximation of the positions of keys in A[1,n]by them(linear or nonlinear) models\ncreated over them, and c≥2εdenotes the fan-out of the data structure.\nProof.Each step of Recursive-Geometric-Modelling reduces the number of models by\na variable factor cwhich is nonetheless larger than 2εbecause of Corollary 3. The number\nof levels is, therefore, L=O(logcm), and the total space required by the recursive geometric\nmodel is/summationtextL\n/lscript=0m/ε/lscript= Θ(m). For the membership query, the bounds on the running time\nand the I/O complexity follow easily by observing that Query-R performsLbinary searches\nover intervals having size at most 2ε. J\nTheorem 6 highlights the main novelty of the PGM-index : its space overhead does not grow\nlinearly with n, as in the traditional indexes mentioned in Section 1, but it grows with the\n“geometric complexity” of the input array Aand decreases with the value of ε. From a\ntheoretical perspective, the PNA-model at the last level of a PGM-index cannot have more\nmodels than n/(2ε), hencem<ngiven that ε≥1(see Corollary 3). And since this fact\nholds for the recursive levels too, it follows that asymptotically the PGM-index cannot be\nworse in space and time than a 2ε-way tree (just take c= 2ε= Θ(B)in Theorem 6). In\npractice, as we will show in the experimental Section 5.2, the PGM-index is much faster and\nsuccinct than a 2ε-way tree due to the use of segments as routing tables, and due to the large\nvariable fan-out cthey allow to achieve. For instance, a query takes O(log(ε/B)) =O(1)\nI/Os because typically in practice c/greatermuch2ε.\nOther than traditional tree-based indexes, the PGM-index generalises (i) the A-tree [ 16],\nin that it recursively and level-wise applies linear models to form a tree structure of (possibly)\nlarge fan-out, and (ii) the RMI [ 27], in that the tree structure of the PGM-index is not\nﬁxed in advance, but adapts itself to the distribution of the keys available at every level.\nConsequently, the number of levels, the number of nodes at every level, and the fan-out\nof each node are not ﬁxed but they are learned from the input data as a function of the\napproximate error ε. The Multicriteria framework, explained in Section 4, will show how to\ncontrol in a principled way the parameter ε, and thus the space and the time eﬃciency of\nthe PGM-index.\nTable 1 summarises the query costs, both in the RAM model and in the EM model (with\npage sizeB), of the three strategies described in this section. In the case of range queries,\ntime costs increase by the output-sensitive term of O(K)(in EM model, O(K/B )), whereK\nis the number of keys satisfying the range query. To appreciate the computational eﬃciency\nof the Recursive PGM-index , we notice that in theory L=O(logεm)andm=O(n/ε)\nwithε≥1, but in practice those upper bounds are very pessimistic in that Lturns out\nto be a very small constant and mresults many orders of magnitude smaller than n. The\nthorough experimental results of Section 5 performed on large datasets will support these\ntheoretical considerations by showing improvements of several orders of magnitude in space\nand signiﬁcant improvements in time. This is the reason why we prefer to leave the bounds\nas a function of mrather than n.5\n5As a practical consideration, the PGM-index indexed the biggest and most complex dataset available\nto us of about 715M items with m=423K linear models and taking ε= 64(see Section 5.1.1). This\nis a reduction in the index size of three orders of magnitude. We also found in our experiments that\nmdecreases as a power of ε, thus making the upper levels of the Recursive PGM-index pretty much\nnegligible in terms of occupied space.\n\nG. Vinciguerra, P. Ferragina, and M. Miccinesi :15\nData structure SpaceRAM model\nworst case timeEM model\nworst case I/OsEM model\nbest case I/Os\nPlain sorted array O(1) O(logn) O(logn\nB) O(logn\nB)\nMultiway tree (e.g., B-tree) Θ(n) O(logn) O(logBn) O(logBn)\nPGM-index w. binary search Θ(m)O(logm+ log ε) O(logm\nB) O(logm\nB)\nPGM-index w. multiway tree Θ(m)O(logm+ log ε) O(logBm) O(logBm)\nRecursive PGM-index Θ(m) O(logm)O(logcm)\nc≥2ε= Ω(B)O(1)\nTable 1 Time and I/O complexity of the predecessor query operation with traditional data\nstructures, and the three variants of PGM-index described in Section 2.3. Recall that the integer\nε≥1denotes the error guaranteed in the approximation of the positions of keys in Aby the m\n(linear or nonlinear) models. In the EM model we assumed ε= Θ(B). In theory, m≤n/(2ε), but in\npractice this upper bound is very pessimistic in that mis many orders of magnitude smaller than n\n(see Section 5).\n3 The Distribution-Aware PGM-index\nThePGM-index of Theorem 6 implicitly assumes that queries are uniformly distributed, but\nthis seldom happens in practice. For example, in search engines queries are very well known\nto follow very skewed distributions such as the Zipf’s law [ 41]. In such cases, it is desirable\nto have an index that answers the most frequent queries faster than the rare ones, so to\nachieve a higher query throughput. Previous work exploited query distribution in the design\nof binary trees [5, 25], treaps [36], skip lists [3], and text indexes [13], just to cite a few.\nIn this section, we introduce an orthogonal and very simple approach that builds upon the\nPGM-index by proposing a variant that adapts itself not only to the distribution of the keys\nbut also to the distribution of the queries. This turns out to be the ﬁrstdistribution-aware\nlearned index to date, with the additional positive feature of being very succinct in space.\nFormally speaking, given a sequence of weighted keys S={(k1,p1),..., (kn,pn)}, where\npiis the probability to query the key ki, we want to solve the distribution-aware dictionary\nproblem, which asks for a data structure that searches for a key kiin timeO(log(1/pi)),\nso that the average query time coincides with the entropy of the query distribution H=/summationtext\ni=1,...,npilog(1/pi).\nWe recall that O’Rourke [ 32] proposed an algorithm that, given a y-range for each one\nofnpoints in the plane, ﬁnds the set of all directions that intersect those ranges in O(n)\ntime. This algorithm can be turned into an optimal one which ﬁnds the minimum number\nof segments intersecting the y-range of those keys, via an incremental approach that starts\na new direction (i.e. segment) as soon as the set of possible directions intersecting a given\nsubset of points is empty [42].\nOur key idea is then to deﬁne, for every key ki, ay-range of size yi=min{1/pi,ε}, for\na given integer parameter ε≥1, and then apply O’Rourke’s algorithm on that set of keys\nand ranges. Clearly, for the keys whose y-range isεwe can use Theorem 6 and derive the\nsame space bound of O(m); whereas for the keys whose y-range is 1/pi<εwe observe that\nthese keys are no more than εand thepis sum up to 1, so they induce in the worst case\n2εextra segments. Therefore, the total space occupancy of the leaf level of the recursive\ndistribution-aware index is Θ(m+ε), wheremis the one deﬁned in Theorem 6. Now, let us\nassume that the search for a key kiarrived at the last level of this PGM-index and thus we\n\n:16 Superseding traditional indexes by orchestrating learning and geometry\nknow in which segment to search for ki: the ﬁnal binary search step within the approximate\nrange returned by that segment takes O(log min{1/pi,ε}) =O(log(1/pi))as we aimed for.6\nBut how do we ﬁnd that segment in a distribution-aware manner? We proceed as in\nthe recursive PGM-index but, here, we need to be very careful in designing the recursive\nstep because of the probabilities (and thus the variable y-ranges) assigned to the recursively\ndeﬁned set of keys.\nLet us consider the segment covering the range of keys S[a,b]={(ka,pa),..., (kb,pb)},\nand denote by qa,b=max i∈[a,b]pithe maximum probability of a key in S[a,b], and by\nPa,b=/summationtextb\ni=apithe cumulative probability of all keys in S[a,b](which is indeed the probability\nto end up in that segment when searching for one of its keys). We create the new set\nS/prime={...,(ka,qa,b/Pa,b),...}formed by the ﬁrst key kacovered by each segment (as in the\nrecursive PGM-index ) and setting its associated probability to qa,b/Pa,b. Then, we construct\nthe next upper level of the PGM-index by applying O’Rourke’s algorithm on S/prime. If we iterate\nthe above analysis for this new level of weighted segments, we conclude that: if we know\nfrom the search executed on the levels above that ki∈S[a,b], the time cost to search for kiin\nthis level is O(log min{Pa,b/qa,b,ε}) =O(log(Pa,b/pi)).\nLet us repeat this argument for another upper level in order to understand the “structure”\nof the search time complexity. Let us denote the segment that covers the range of keys which\nincludekiwithS[a/prime,b/prime]⊃S[a,b], the cumulative probability with Pa/prime,b/prime, and thus assign to\nits ﬁrst key ka/primethe probability r/Pa/prime,b/prime, whereris the maximum probability of the form\nPa,bof the ranges included in [a/prime,b/prime]; in other words, if [a/prime,b/prime]is partitioned into [z1,...,z c],\nthenr=max i∈[1,c)Pzi,zi+1. Reasoning as done previously, if we know from the search\nexecuted on the levels above that ki∈S[a/prime,b/prime], the time cost to search for kiin the this level\nisO(log min{Pa/prime,b/prime/r,ε}) =O(log(Pa/prime,b/prime/Pa,b))because [a,b]is, by deﬁnition, one of these\nranges in which [a/prime,b/prime]is partitioned.\nRepeating this design until one single segment is obtained (whose cumulative probability\nis one), we get a total time cost for the search in all levels of the PGM-index but the last one\nwhich is equal to a sum of logarithms whose arguments “cancel out” and get O(log(1/pi)).\nAs far as the space bound is concerned, we recall that the number of levels in the\nPGM-index isL=O(logεm)and that we have to account for the ε-extra segments per\nlevel returned by the O’Rourke’s algorithm. Consequently, this distribution-aware variant\nof thePGM-index takesO(m+Lε) =O(m+εlogεm)space. This space bound is indeed\nO(m)becauseεis a constant parameter with typically very small values in practice (see\nSection 5.1). We have thus proved the following:7\nITheorem 7. The Distribution-Aware PGM-index takesO(m)space and solves the dis-\ntribution-aware dictionary problem in O(H)average time, where His entropy of the query\ndistribution and mis the optimally minimum number of segments created over the ninput\nkeys (e.g. by O’Rourke’s algorithm [32]).\nAswerepeatedlyobservedintheprevioussections(seealsoTable1), intheory m≤n/(2ε),\nbut in practice this upper bound is very pessimistic because mis many orders of magnitude\nsmaller than n. This means that our distribution-aware PGM-index oﬀers the very precious\nfeature of being very succinct in its space occupancy.\n6In an actual implementation, one would avoid storing the yis that delimit the binary search ranges and\nperform instead an exponential search.\n7We recall that, with respect to hashing, our solution can be extended to answer predecessor queries on\nkeys of the universe not present in the dictionary; moreover, it takes an extra space, other than the\nkeys, which depends on m/lessmuchnand thus turns out to be very succinct in practice.\n\nG. Vinciguerra, P. Ferragina, and M. Miccinesi :17\n4 The multicriteria version of the PGM-index\nTuning a data structure to match the application’s needs is often a diﬃcult and error-prone\ntask for a software engineer, not to mention that these needs may change over time due\nto mutations in data distribution, devices, resource requirements, and so on. The typical\napproach is to grid search the various instances of the data structure to be tuned until the\none that matches the application’s needs is found. As an example, the RMI of [ 27] has two\ndimensions to tune (the number of stages, i.e. levels of its DAG structure, and the number\nof models in each stage), with a possibly huge search space to explore. Hence the software\nengineer has to restrict the large search space of the parameters in order to make the grid\nsearch eﬃcient, and this may potentially miss the optimal solution.\nIn the following two subsections we show that this tuning process can be eﬃciently\nautomated over the PGM-index via an optimisation strategy that: (i) given a space constraint\noutputsthe PGM-index thatminimisesitsquerytime(Section4.1); symmetrically, (ii)givena\nmaximum query time outputs the PGM-index that minimises its occupied space (Section 4.2).\n4.1 The time-minimisation problem\nWe start by modelling the query time and the space occupancy of the Recursive PGM-index\nas functions of the εparameter. The former can be modelled, according to Theorem 6, as\nt(ε) =c(log2εm)log(2ε/B), whereBis the page size of the EM model, mis the number\nof models in the last level of the PGM-index , andcdepends on the access latency of the\nmemory, which we assume constant for simplicity. For the latter, we introduce s/lscript(ε), which\ndenotes the number of models needed to have accuracy εover the positions of the keys\navailable at level /lscriptof the Recursive PGM-index , and we compute the overall number of\nmodels ass(ε) =/summationtextL\n/lscript=1s/lscript(ε). By Corollary 3, we know that sL(ε) =m≤n/(2ε)for any\nε≥1and thats/lscript−1(ε)≤s/lscript(ε)/(2ε). So thats(ε)≤/summationtextL\n/lscript=0m/(2ε)/lscript= (2εm−1)/(2ε−1).\nGiven a space bound smax, the problem is to minimise t(ε)subject tos(ε)≤smax.8\nThe greatest challenge of this problem is that we do not have a closed formula for s(ε), but\nonly an upper bound which does not depend on the underlying dataset as instead s(ε)does.\nSection 5.1.1 will show that in practice we can choose to model m=sL(ε)with a simple\npower-law relation having the general form aε−b, whose parameters aandbwill be properly\nestimated on the dataset at hand. This modelling covers both the rather pessimistic case\ndescribed by Corollary 3 (just take a=n/2andb= 1) and the lucky one in which the\ndataset is strictly linear (just take asmall andbequal to zero).\nSolving the time-minimisation problem. The time-minimisation problem is illustrated in\nFigure 8. As one would expect, the query time t(ε)of the Recursive PGM-index increases\nwith increasing ε, since the area [pos−ε,pos+ε]gets larger. Higher values of smax, instead,\npush to the left the vertical dashed line, which delimits the feasible area. As a matter of\nfact, with more space we are able to introduce more models and thus have a more accurate\nindex. It should be clear from the ﬁgure that solving the time-minimisation problem reduces\nto ﬁnding the position of the vertical dashed line, i.e., the value of ε, for which s(ε) =smax\nbecause it is the lowest εthat we can aﬀord. Since s(ε)is monotonically decreasing, such\n8Throughout this section, we assume that a disk page contains exactly Bkeys. This assumption, which\nsimpliﬁes our formulas, can be relaxed by putting the proper machine- and application-dependent\nconstants in front of t(ε)ands(ε). For an actual implementation it would be more appropriate to\nexpress s(ε)andsmaxin bytes, as we will do in the experiments described in Section 5.3.\n\n:18 Superseding traditional indexes by orchestrating learning and geometry\n200 300 400 500 600 700 800 900 1,000200300400500\nεTime (ns)\nt(ε)\nFigure 8 A graphical description of the time-minimisation problem. The solid line represents the\nfunction t(ε), while the area highlighted by diagonal lines represents feasible values for ε, i.e., the\nones for which s(ε)≤smax.\nvalue ofεcan be found by a binary search in the bounded interval E= [B/2,n/2]which is\nderived by requiring that each model errs at least a page size (i.e. 2ε≥B), since lower ε\nvalues do not save I/Os, and by observing that one model is the minimum possible space (i.e.\n2ε≤n, according to Corollary 3).\nSpeeding up the search. Provided that our power-law approximation holds, we show that\nit is possible to speed up the solution based on binary search by suitably guessing the next\nvalue ofεrather than taking the midpoint of the current search interval. In fact, we can ﬁnd\nthe root of s(ε)−smax, i.e. the value εgfor whichs(εg) =smax. We emphasise that such εg\nmay not be the solution of our problem, as it may be the case that the approximation or the\nﬁtting ofs(ε)by means of a power-law is not precise. Thus, more iterations of the search\nmay be needed to ﬁnd the optimum εvalue. Our experiments will show that actually the\nnumber of guesses is suﬃciently small that the approach will be very fast in practice, surely\nfaster than binary search.\nSince the method of guessing the next εclosely resembles interpolation search, we adopt\nthe method described by [ 18] in order to further limit useless steps: we gradually switch\nto a standard binary search by biasing the guess εgtowards the midpoint of the current\nsearch range. To be speciﬁc, let |E|be the size of our search space. Let guessesbe the\nnumber of times the guess εgwas used to choose the next iterate during the execution of\nthe algorithm, and threshold =⌈log log|E|⌉be the maximum number of guesses allowed.\nSuppose that εmandεgare, respectively, the current midpoint of the binary search and\nthe guess. If guesses<threshold , we choose the next iterate of εas/rho1×εm+ (1−/rho1)×εg,\nwhere/rho1=guesses/threshold . Otherwise, we choose the next iterate of εasεm. This gradual\ntransition from εgtoεmavoids the deterioration to linear search, which occurs when the\ntrues(ε)is diﬀerent from the approximation that we use as the guess and which, we must\nremark, was observed empirically in our diverse albeit small collection of datasets. As a\nresult, with such a choice of the next iterate, we have the same worst-case performance of a\nbinary search even when the guess εgis wrong.\nTo summarise, we search the value εgiving the smallest query time t(ε)within space\nboundsmaxas follows:\n1.We run several iterations of binary search (e.g., 4 or 5), and save in a set Dthe pairs\n(εi,sL(εi))obtained at each step by running Build-Piecewise-Model .\n2.We ﬁt a power-law of the form aε−bon the set of points D, thus determining the values\nof the parameters aandb. Then we compute a goodness-of-ﬁt measure, such as R2.\n\nG. Vinciguerra, P. Ferragina, and M. Miccinesi :19\n3.If the ﬁtted model is good enough (i.e., R2is close to one), we choose the next iterate of\nεusing the biased guess described in the text above.\n4.Otherwise, we jump to 1 and continue with the standard binary search, with the hope\nthat collecting new pairs in Dimproves the ﬁtting of the power-law.\nNotethatthecostoftheﬁttingatStep2isnegligiblewithrespecttotheindexconstruction\nat Step 1, since it is CPU bounded and not I/O bounded. In practice, the number of iterations\nand index constructions that Step 3 saves with respect to a plain binary search is signiﬁcant,\nas we will see in the experimental Section 5.3.\n4.2 The space-minimisation problem\nGiven a time bound tmax, the space-minimisation problem consists of minimising s(ε)subject\ntot(ε)≤tmax. As for the problem in the previous section, we can binary search inside\nthe intervalEand look for the maximum εwhich satisﬁes the constraint. Likewise, we\ncould guess the next iterate for εby solving the equation t(ε) =tmax, that is solving\nc(log2εsL(ε))log(2ε/B) =tmax, in whichsL(ε)is replaced by the power-law approximation\naε−bfor properaandb, andcis replaced by the measured memory latency of the given\nmachine. However, this approach raises a subtle issue: the time model could not be a correct\nestimate of the true (empirical) query time because of hardware-dependent factors such as\nthe presence of several caches and the CPU pre-fetching. To further complicate this issue,\nwe note that both s(ε)andt(ε)depend on the power-law approximation aε−b.\nFor these reasons, instead of using the time model t(ε)to steer the search, we suggest to\nmeasure and use the actual average query time t(ε)of thePGM-index over a ﬁxed batch\nof random queries. Moreover, instead of binary searching inside the whole E, we run an\nexponential search starting from the solution of the dominating term clog(2ε/B) =tmax,\ni.e. the cost of searching the data. Eventually, we stop the search of the best εas soon as\nthe searched range is smaller than a given threshold because t(ε)is subject to measurement\nerrors (e.g. due to an unpredictable CPU scheduler).\n5 Experiments\nWe experimented with an implementation in C ++of the algorithms described in this paper\non a machine with a 2.3 GHz Intel Xeon Gold and 192 GiB memory.9We used the following\nthree datasets, each having diﬀerent data distributions, regularities and patterns:\n1.Web logs [27]. Contains timestamps of about 715M requests to a web server of a computer\nscience department. This dataset exhibits several patterns, for example, fewer requests\nduring summer and night time.\n2.Longitude [31]. Contains longitudes of about 166M points-of-interest from OpenStreetMap.\n3.IoT[27]. Contains timestamps of about 26M events recorded by IoT sensors (e.g.,\ndoor, motion, power) installed throughout an academic building. Trends in this dataset\ngenerally reﬂect human activity, such as a high concentration of events during daytime\nand semesters.\n9The implementation will be released soon.\n\n:20 Superseding traditional indexes by orchestrating learning and geometry\n23242526272829210211212213214102103104105106107\nεmWeb logs\nLongitude\nIoT\n(a)mover the whole datasets.232425262728292102112122132140K50K100K150K\n.1%.3%.6%\nεWeb logs\nLongitude\nIoT\n(b)mover the ﬁrst 25M entries of each dataset.\nFigure 9 Number of segments min the optimal PLA-model for several values of ε. The ratio\nm/nis shown in percentage on the right of the right chart.\n5.1 Piecewise geometric model\nIn this batch of experiments, we evaluated the goodness of the PNA-model in terms of the\nnumber of models it uses to ε-approximate the keys in the three datasets above. Savings\nin the number of models impact onto the space occupancy but also onto the query time of\nthePGM-index , as there are fewer comparisons to ﬁnd the segment responsible for a key.\nSection 5.1.1 focuses on the PLA-model, while Section 5.1.2 focuses on the PNA-model with\nboth linear and nonlinear models.\n5.1.1 Piecewise linear model\nFigure9ashowsalog-logplotofthesize mofthePLA-modelbuiltbytheoptimalconstruction\nalgorithm of [ 42] for several given values of ε(see Section 2.1.1). Even when εis as little as\n8, the number mof segments is more than two orders of magnitude smaller than the original\ndatasets size n. To dig into the “geometric complexity” of the three datasets, we repeated\nthe previous experiment on the ﬁrst 25M entries and discovered that Web logs and IoT are\nthe most complex to index because they require more segments than Longitude (Figure 9b).\nIn order to understand the impact of the optimal construction algorithm of [ 42] onto the\nspace occupancy of the PGM-index , we compared its number of computed segments against\nthe currently best and heuristic algorithm proposed in [ 16,37], called shrinking cone (shortly,\nSC). The optimal algorithm signiﬁcantly improves SC by reducing the number of segments\nof about 38%–63.3% (Figure 10) while maintaining the same time eﬃciency in practice and\noptimality in asymptotic time complexity.\nThen, for designing a multicriteria PGM-index (see Section 4 and Section 5.3), we studied\nthe behaviour of m=sL(ε)by varying ε. We ﬁtted ninety diﬀerent functions over about\ntwo hundred points (ε,sL(ε))generated beforehand by a long-running grid search over our\ndatasets. At the end, looking at the ﬁttings, we chose to model sL(ε)with a simple power\nlawhaving the general form aε−b.\nFor completeness, we report that the algorithm with ε= 8built a PLA-model for Web\nlogs in 2.59 seconds, whereas it took less than 1 second for Longitude and IoT datasets.\n\nG. Vinciguerra, P. Ferragina, and M. Miccinesi :21\n0 10 20 30 40 50 60 70IoT Longit. Web logs\nGain (%)8\n16\n32\n64\n128\n256\n512\n1024\n2048\nFigure 10 Gain in number of segments with respect to SC [16] for various values of ε.\nISummary. The optimal algorithm [ 42] to construct the PLA-model is very fast (i.e. less\nthan 3 seconds for the about 715M keys of the Web logs dataset) and induces a signiﬁcant\ngain in the space occupancy of the PGM-index , which indeed results up to 60% more succinct\nthan what was achieved by previously known heuristic algorithms (i.e. SC [ 16,37]), and\noverall from two to ﬁve orders of magnitude smaller than the original datasets size.\n5.1.2 Piecewise nonlinear model\nWe experimented with an implementation of the Top-Down-Regression algorithm de-\nscribed in Section 2.2. The pool of models consisted either of segments or of three neural\nnetworks selected with a random search of neural networks with 0 hidden layers or 1 hidden\nlayer with 2–5 hidden units, and activations in {elu,softplus,tanh,sigmoid}. Among this\nplethora of possible neural networks we selected a speciﬁc subset as follows: we randomly\nselected three datasets slices of sizes 103,104,105, and we ran a random search with 5-fold\ncross-validation on each of these three datasets; then we selected three neural networks which\nare capable of approximating short/medium/long sequences of input points.\nFor the choice of the breakpoint position pat Line 14 of Top-Down-Regression , four\nstrategies were tested: (i) midpoint p= (a+b)/2, which we used as baseline; (ii) random\nchoice ofp∈(a,b); (iii) index of the highest error p=a+argmaxierrors [i], where errorsis\nthe array computed at Line 8; (iv) two breakpoints p1,p2at the start/end positions of the\nlongest chain of consecutive elements in errorsthat are greater than ε. On the Web logs\ndataset and ε∈{23,..., 28}, we found that with respect to the baseline (ii) produced on\naverage 1.72% fewer models, (iii) produced 0.31% more models, and (iv) produced 11.08%\nfewer models. Therefore, we chose the strategy (iv) for the selection of the breakpoints at\nLine 14 of Top-Down-Regression .10\nThe Longitude dataset with ε= 1024was covered with 2181 segments and 54 neural\nnetworks with four hidden units, thus a total of 2235 models. Restricting the pool to only\nsegments, the algorithm produced instead 2254 segments, thus 19 more models. Perhaps\nsurprisingly, the reduction in the number of models does not correspond to a reduction in\n10In terms of linear models, Top-Down-Regression is a greedy algorithm which produces on average\n25.33% less linear models than the known greedy SC. In any case, we should also observe that it creates\non average 48.61% more linear models than the optimal construction algorithm.\n\n:22 Superseding traditional indexes by orchestrating learning and geometry\nspace, since each neural network needed 13 parameters to be stored (in contrast to the two\nof a segment). The same phenomenon was observed on Web logs and IoT, and other εvalues.\nProbably the reason is that Top-Down-Regression assigns the responsibility of an area to\nthe ﬁrst model that does not make any error >ε, and it does not check whether the same\narea could have been covered by a more space eﬃcient combination of simpler models. Doing\nthis check naively would require a lengthy exhaustive search of all the combinations, so this\nissue deserves further research in the future.\nISummary. The Hybrid PGM-index based on linear models and three very simple neural\nnetworks achieves up to 11% reduction in the number of models needed to ε-approximate the\npositions of the input keys. Nonetheless, the overall space occupancy is no better than the\nRecursive PGM-index based on linear models only. Despite these unsatisfying results, we\nthink that the Hybrid PGM-index is very promising, but one needs to choose carefully the\nset of neural networks to select from (according to their oﬀered trade-oﬀ between geometric\npower against space occupancy), and study techniques to build eﬃciently the PGM-index that\nsuitably selects the best models from the set of available ones while avoiding the combinatorial\nexplosion of an exhaustive search.\n5.2 Query performance of the PGM-index\nWe evaluated the average query performance of the PGM-index and other indexing data\nstructures on Web logs, the biggest and most complex dataset available to us. The dataset\nwas loaded in memory as a contiguous array of integers represented with 8 bytes and with\n128 bytes payload. Slopes and intercepts were stored as double-precision ﬂoats. Each index\nwas presented with 10M queries randomly generated on the ﬂy.\nPGM-index variants. We experimented with the performance of the PGM-index using\nthe three diﬀerent indexing strategies described in Section 2.3: binary search, multiway\ntree (speciﬁcally we implemented a version of the CSS-tree [ 34]), and recursive geometric\nmodelling. We refer to them, respectively, with PGM ◦BIN, PGM◦CSS and PGM◦REC. We\nsetε/lscript= 4for all but the last level of PGM ◦REC, that is the one that includes the segments\nbuilt over the input dataset by the optimal algorithm of the previous section.11Likewise, the\nnode size of the CSS-tree was set to B= 2ε/lscriptfor a fair comparison with the corresponding\nPGM◦REC.\nFigure 11(left) shows that PGM ◦REC dominates PGM ◦CSS forε≤256, and has better\nquery performance than PGM ◦BIN. The advantage of PGM ◦REC over PGM◦CSS is also\nevident in terms of index height since the former has ﬁve levels whereas the latter has seven\nlevels, thus a shorter traversal induced by a higher branching factor. For ε >256all the\nthree variants behave similarly. The reason is that the PLA-model ﬁts entirely into the L2\ncache, hence there is no advantage in using an index on top of the models.\nFinally, we considered larger values of ε≥256and tested the performance of PGM ◦BIN\n(recall that for those values all indexes behave similarly) with an exponential search that\nstarts from the predicted position of the key in the leaf level. We found that exponential\nsearch is on average 16% slower than binary search for 256≤ε≤4096, but it is 7% faster for\n11The reason of this choice is that the number of segments is small, and recurring with a low ε/lscriptin\nsuch cases permits both a fast per-level search and a small space overhead for the index on top of the\nsegments.\n\nG. Vinciguerra, P. Ferragina, and M. Miccinesi :23\n700 800 900 1,000 1,1000246810\nTime (ns)Index space (MiB)PGM◦BIN\nPGM◦CSS\nPGM◦REC\nε= 64\nε= 128\nε= 256\nε= 512\nε= 1024\nε= 2048\nε= 4096\nε= 8192\n600 800 1,0001,2001,4001,6001,80002468101214\nTime (ns)B+-tree 16KiB\nCSS-tree 4KiB\nCSS-tree 8KiB\nCSS-tree 16KiB\nRMI 10K\nRMI 50K\nRMI 100K\nRMI 200K\nRMI 400K\nFigure 11 On the left, the trade-oﬀs oﬀered by several conﬁgurations of PGM-index . On the\nright, the best conﬁgurations of PGM-index compared to the RMI with diﬀerent second-stage sizes,\nand to traditional indexes with diﬀerent page size, expressed in bytes.\n8192≤ε≤262144. The reason is that the large jumps of the binary search are expensive in\nterms of I/Os when εis large, whereas the exponential search is more likely to have better\nI/O performances due to increased I/O locality.\nPGM-index vs traditional indexes. We compared the PGM-index against two traditional\nindexes: the B+-treeand the CSS-tree [ 34]. For the former, we chose a well-known library [ 6],\nused as baseline also in [16, 27]. For the latter, we used our implementation.\nThePGM-index dominated these traditional indexes, as shown in Figure 11(right) for\npage sizes in the range of 4–16 KiB. Performances for smaller page sizes are not plotted\nbecause they were too far from the main plot range. For example, the fastest CSS-tree in\nour machine had page size set to 128 bytes (twice the cache line), occupied 341 MiB and was\nmatched in query performance by a PGM ◦REC withε= 128which actually occupied only 4\nMiB, thus inducing a reduction in space of 82.7 ×. As another example, the fastest B+-tree\nhad page size set to 256 bytes, occupied 874 MiB and was matched in query performance by\na PGM◦REC withε= 4096which actually occupied only 87 KiB, thus inducing a reduction\nin space of four orders of magnitude. It goes without saying that the B+-treemay also\nsupport insertions and deletions, which require a more ﬂexible and space-hungry structure,\nso it is not a surprise that it performs worse than the CSS-tree and the PGM-index . Instead,\nwhat it is surprising in this comparison, it is the amount of improvement in space occupancy\nachieved by the PGM-index which is four orders of magnitude with respect to B-treesand\ntwo orders of magnitude with respect to CSS-trees.\nOverall, as stated in Section 1, traditional indexes are blind to the data distribution, and\nthey miss the compression opportunities oﬀered by its pattern and trends. On the contrary,\nadapting to the data distribution through linear approximations allows the PGM-index\nto uncover previously unknown space-time trade-oﬀs, as we have demonstrated in this\nexperiment.\n\n:24 Superseding traditional indexes by orchestrating learning and geometry\nPGM-index vs RMI. We implemented the 2-stage RMI of [ 27] using linear models and\nseveral numbers of models in the second stage.12Figure 11(right) shows that the PGM-index\ndominated RMI. The former has indeed better latency guarantees because, instead of ﬁxing\nthe structure beforehand and inspecting the errors afterwards, it is dynamically and optimally\nadapted to the input data distribution while guaranteeing the desired ε-approximation and\ntrying to use the least possible space. Conversely, models in RMI are agnostic to the power of\nthe models in the subsequent stages. Because of this, distributions of keys can be unbalanced\nin the last stage and can result in underused models. The most compelling evidence is the\nMean Absolute Error (MAE) between the approximated and the predicted position, e.g.,\nthePGM-index withε= 512needed ~32K segments and had MAE 226 ±139, while an RMI\nwith the same number of second stage models (i.e. number of models at the last level) had\nMAE 892±3729 (3.9 ×more), thus higher and less predictable latency in the query execution.\nWe expect this ineﬃciency of RMI to be even more severe when the data is stored on slower\nlevels of the memory hierarchy, i.e., on disks or remote servers.\nISummary. There is suﬃcient experimental evidence that the Recursive PGM-index is\nvery ﬂexible in trading query eﬃciency by compressed space occupancy, as demanded by big\ndata applications. Moreover, the plots in Figure 11 show that the Recursive PGM-index\ndominates in time and space both the classic and the learned index structures. In particular, it\nimproves the space occupancy of the CSS-tree by a factor 82.7 ×and the one of the B-treeby\nmore than four orders of magnitude while achieving the same or even better query eﬃciency.\n5.3 Experimental performance of the Multicriteria PGM-index\nOur implementation of the Multicriteria PGM-index operates in two modes: the time-\nminimisation mode (shortly, min- t) and the space-minimisation mode (min- s), which imple-\nment the algorithms described in Sections 4.1 and 4.2 respectively. In min- tmode, inputs to\nthe program are smaxand a tolerance tolon the space occupancy of the solution, and the\noutput is the value of εwhich guarantees a space bound smax±tol. In min-smode, inputs\nto the program are tmaxand a tolerance tolon the time of the solution, and the output is\nthe value of εwhich guarantees a time bound tmax±tolin the query operations. We note\nthat the introduction of a tolerance parameter allows us to stop the search earlier when any\nfurther step would not appreciably improve the solution (i.e., we seek only improvements of\nseveral bytes or nanoseconds). So tolis not a parameter that has to be tuned but rather\na stopping criterion like the ones used in iterative methods. As further design choices we\npoint out that: (i) the ﬁtting of the power law, approximating s(ε)ort(ε), was performed\nwith the Levenberg–Marquardt algorithm, while root ﬁnding was performed with Newton’s\nmethod; (ii) the search space for εwas set toE= [8,n/2](since a cache line holds eight 64\nbits integers); and ﬁnally (iii) the number of guesses was set to 2⌈log logE⌉.\nExperiments with the min-time mode. Suppose that a database administrator wants the\nmost eﬃcient PGM-index for the Web logs dataset that ﬁts into an L2 cache of 1 MiB. Our\nsolver derived an optimal PGM-index matching that space bound by setting ε= 393and\ntaking 10 iterations and a total of 19 seconds. This result was obtained by approximating\nsL(ε)with the power law 46032135ε−1.16which guarantees an error of no more than 4.8%\n12Personal correspondence with the authors clariﬁed that the results in [ 27] were obtained with RMIs\nwith linear models in both the stages, as we do in our experiments.\n\nG. Vinciguerra, P. Ferragina, and M. Miccinesi :25\nover the range ε∈[8,1024]. For comparison, the solver based on binary search took 27\niterations and 50 seconds.\nNow, suppose that a database administrator wants the most eﬃcient PGM-index for\nthe Longitude dataset that ﬁts into an L1 cache of 32 KiB. Our solver derived an optimal\nPGM-index matching that space bound by setting ε= 1050and taking 14 iterations and a\ntotal of 9 seconds. For comparison, the solver based on binary search took 20 iterations and\n14 seconds.\nExperiments with the min-space mode. Suppose that a database administrator wants\nthe most compressed PGM-index for the IoT dataset that answers any query in less than\n500 ns. Our solver derived an optimal PGM-index matching that time bound by setting\nε= 432, occupying 74.55 KiB of space, and taking 9 iterations and a total of 6 seconds. For\ncomparison, the solver based on binary search took 19 iterations and 12 seconds.\nNow, suppose that a database administrator wants the most compressed PGM-index for\nthe Web logs dataset that answers any query in less than 800 ns. Our solver derived an\noptimalPGM-index matching that time bound by setting ε= 1217, occupying 280.05 KiB\nof space, and taking 8 iterations and a total of 17 seconds. For comparison, the solver based\non binary search took 24 iterations and 48 seconds.\nISummary. There is suﬃcient experimental evidence that the multicriteria PGM-index\neﬀectively trades query eﬃciency by compressed space occupancy, as demanded by big data\napplications. With respect to the best known learned index, namely the RMI, which uses\nan expensive grid search, ours adopts a fast optimisation process that makes it suitable for\napplications with rapidly-changing data distributions and constraints.\n6 Conclusions and future work\nIn this paper, we introduced the PGM-index , a novel learned data structure for the dictionary\nproblem. Compared to traditional data structures, our index is designed to trade smoothly,\nand more eﬀectively than before, query time versus space occupancy by using a parameter ε\nand the proper orchestration of geometric and learning concepts. We experimentally showed\nthat it improves both query performance and space occupancy up to orders of magnitude,\ngoing well beyond classic indexes and modern learned indexes. Then, we extended its design\nto automatically deploy complex or simple regression models, and we tested its potential with\nneural networks. We introduced also a third variant of the PGM-index that adapts itself not\nonly to the key distribution but also to the query distribution, still achieving very succinct\nspace occupancy. Finally, we introduced the concept of Multicriteria Data Structures, showed\nthat the PGM-index is an eﬃcient and eﬀective instance of this novel concept, and then\ndesigned an eﬃcient solver that allows to specify a maximum query time and obtain the\nPGM-index that minimises the space, or vice versa. We demonstrated empirically that our\nsolver halves the time to ﬁnd the solution with respect to a baseline approach.\nNumerous open problems and research opportunities need to be addressed:\nHybrid indexes, other regression models and compressed tools. We saw in Section 5.1.2\nthat when one employs nonlinear models, the procedure Top-Down-Regression can no\nlonger guarantee that an area is covered by the PNA-model with the minimum possible\nspace (see Section 2.2). Eﬃciently solving this problem is a challenging extension of our\nwork and could open the way to designing very powerful hybrid indexes. Even more\n\n:26 Superseding traditional indexes by orchestrating learning and geometry\nintriguing is the possibility of orchestrating segments, nonlinear models and techniques\ncoming from the compression domain [ 1,30,40]. We believe that their use could be very\nsuitable within our Multicriteria framework to design more powerful and eﬀective hybrid\nlearned indexes.\nA compressed variant of the PGM-index. In this paper, we have focused only on the com-\npactness of the index and assumed that the keys are stored uncompressed in a separate\narrayA. In the future, we plan to study how to compress Awhen it consists of integer\nkeys while still being able to support fast membership, successor and predecessor queries.\nWe remind the reader that indexing integer keys is of growing interest because of their\noccurrence in the posting lists of search engines (i.e. the docIDs) or in the adjacency lists\nof graphs (i.e. the nodeIDs, Web graphs or Social Network graphs) [39].\nThe Data Calculator Engine. We believe that the idea of building a PLA-model on a\nsequence of keys (Theorem 1) could become an eﬀective design element for other data\nstructures too. In this respect, we foresee its integration within the novel Data Calculator\nengine [22]. This could boost the design of new, or the re-design of classic, data structures.\nHandling deletions and insertions. The incremental construction algorithm of the minimal-\nheight strip shown in Section 2.1 allows managing insertions to the right of the current\ninput keys. However, if the insertions can be arbitrary, then we could resort to split-merge\noperations over segments, like in classic B-trees, but this could possibly lose the minimality\nof the overall piecewise linear model (unless some overall reconstruction process is applied\nat ﬁxed time instant given its high speed). The eﬃcacy and closeness to minimality of\nthis approach should be theoretically and experimentally investigated.\nAcknowledgements. We thank Carsten Binnig for providing us with the Web logs and IoT\ndatasets. Tim Kraska for his advice on our implementation of his RMI. Gianna Del Corso\nfor her helpful advice on the numerical algorithms implemented in our solver.\nReferences\n1Daniel Abadi, Samuel Madden, and Miguel Ferreira. Integrating compression and execu-\ntion in column-oriented database systems. In Proceedings of the 2006 ACM International\nConference on Management of Data , SIGMOD, pages 671–682, New York, NY, USA, 2006.\nACM.\n2Naiyong Ao, Fan Zhang, Di Wu, Douglas S. Stones, Gang Wang, Xiaoguang Liu, Jing Liu,\nand Sheng Lin. Eﬃcient parallel lists intersection and index compression algorithms using\ngraphics processing units. Proceedings of the VLDB Endowment , 4(8):470–481, May 2011.\n3Amitabha Bagchi, Adam L. Buchsbaum, and Michael T. Goodrich. Biased skip lists. Al-\ngorithmica , 42(1):31–48, 2005.\n4Peter Bailis, Kai Sheng Tai, Pratiksha Thaker, and Matei Zaharia. Don’t throw out your\nalgorithms book just yet: Classical data structures that can outperform learned indexes,\nDec 2018. URL: https://dawn.cs.stanford.edu/2018/01/11/index-baselines/ .\n5Samuel W. Bent, Daniel Dominic Sleator, and Robert Endre Tarjan. Biased search trees.\nSIAM J. Comput. , 14(3):545–568, 1985.\n6Timo Bingmann. STX B+-tree C ++template classes. http://panthema.net/2007/stx-\nbtree. Version 0.9.\n7Chiranjeeb Buragohain, Nisheeth Shrivastava, and Subhash Suri. Space eﬃcient streaming\nalgorithms for the maximum error histogram. In ICDE, pages 1026–1035. IEEE Computer\nSociety, 2007.\n\nG. Vinciguerra, P. Ferragina, and M. Miccinesi :27\n8Chee-Yong Chan and Yannis E. Ioannidis. Bitmap index design and evaluation. In Proceed-\nings of the 1998 ACM International Conference on Management of Data , SIGMOD, pages\n355–366, New York, NY, USA, 1998. ACM.\n9Danny Z. Chen and Haitao Wang. Approximating points by a piecewise linear function.\nAlgorithmica , 66(3):682–713, 2013.\n10Qiuxia Chen, Lei Chen, Xiang Lian, Yunhao Liu, and Jeﬀrey Xu Yu. Indexable PLA for\neﬃcient similarity search. In VLDB, pages 435–446. ACM, 2007.\n11Graham Cormode. Data sketching. Communications of the ACM , 60(9):48–55, 2017.\n12Andrea Farruggia, Paolo Ferragina, Antonio Frangioni, and Rossano Venturini. Bicriteria\ndata compression. In Proceedings of the Twenty-ﬁfth Annual ACM-SIAM Symposium on\nDiscrete Algorithms , SODA, pages 1582–1595, 2014.\n13Paolo Ferragina, Jouni Sirén, and Rossano Venturini. Distribution-aware compressed full-\ntext indexes. Algorithmica , 67(4):529–546, 2013.\n14Paolo Ferragina and Rossano Venturini. Compressed cache-oblivious String B-tree. ACM\nTrans. Algorithms , 12(4):52:1–52:17, August 2016.\n15Matteo Frigo, Charles E. Leiserson, Harald Prokop, and Sridhar Ramachandran. Cache-\noblivious algorithms. In 40th Annual Symposium on Foundations of Computer Science ,\nFOCS, pages 285–298, 1999.\n16A. Galakatos, M. Markovitch, C. Binnig, R. Fonseca, and T. Kraska. A-tree: A bounded\napproximate index structure, 2018. arXiv:1801.10207 .\n17A. Gani, A. Siddiqa, S. Shamshirband, and F. Hanum. A survey on indexing techniques\nfor big data: taxonomy and performance evaluation. Knowledge and Information Systems ,\n46(2):241–284, 2016.\n18Goetz Graefe. B-tree indexes, interpolation search, and skew. In Proceedings of the 2nd\nInternational Workshop on Data Management on New Hardware , DaMoN, New York, NY,\nUSA, 2006. ACM.\n19Ananth Grama, George Karypis, Vipin Kumar, and Anshul Gupta. Introduction to Parallel\nComputing . Addison-Wesley, 2 edition, 2003.\n20Kenneth Hoste and Lieven Eeckhout. COLE: Compiler Optimization Level Exploration. In\nProceedings of the 6th annual IEEE/ACM International Symposium on Code Generation\nand Optimization , CGO, 2008.\n21Stratos Idreos, Kostas Zoumpatianos, Manos Athanassoulis, Niv Dayan, Brian Hentschel,\nMichael S. Kester, Demi Guo, Lukas M. Maas, Wilson Qin, Abdul Wasay, and Yiyou Sun.\nThe periodic table of data structures. IEEE Data Eng. Bull. , 41(3):64–75, 2018.\n22Stratos Idreos, Kostas Zoumpatianos, Brian Hentschel, Michael S. Kester, and Demi Guo.\nThe data calculator: Data structure design and cost synthesis from ﬁrst principles and\nlearned cost models. In Proceedings of the 2018 International Conference on Management\nof Data, SIGMOD, pages 535–550, New York, NY, USA, 2018. ACM.\n23Norman P. Jouppi, Cliﬀ Young, Nishant Patil, and David Patterson. A domain-speciﬁc\narchitecture for deep neural networks. Communications of the ACM , 61(9):50–59, August\n2018.\n24Eamonn J. Keogh, Selina Chu, David Hart, and Michael J. Pazzani. An online algorithm\nfor segmenting time series. In Proceedings of the 2001 IEEE International Conference on\nData Mining , ICDM, pages 289–296, Washington, DC, USA, 2001. IEEE Computer Society.\n25Donald E. Knuth. Optimum binary search trees. Acta Inf. , 1:14–25, 1971.\n26Tim Kraska, Mohammad Alizadeh, Alex Beutel, Ed H. Chi, Jialin Ding, Ani Kristo, Guil-\nlaume Leclerc, Samuel Madden, Hongzi Mao, and Vikram Nathan. SageDB: A learned\ndatabase system. In Conference on Innovative Data Systems Research (CIDR) , 2019.\n\n:28 Superseding traditional indexes by orchestrating learning and geometry\n27Tim Kraska, Alex Beutel, Ed H. Chi, Jeﬀrey Dean, and Neoklis Polyzotis. The case for\nlearned index structures. In Proceedings of the 2018 ACM International Conference on\nManagement of Data , SIGMOD, pages 489–504, 2018.\n28Carla Mouradian, Diala Naboulsi, Sami Yangui, Roch H Glitho, Monique J Morrow, and\nPaul A Polakos. A comprehensive survey on fog computing: State-of-the-art and research\nchallenges. IEEE Communications Surveys & Tutorials , 20(1):416–464, 2018.\n29Ken Naono, Keita Teranishi, John Cavazos, and Reiji Suda, editors. Software Automatic\nTuning, From Concepts to State-of-the-Art Results . Springer, 2010.\n30Gonzalo Navarro. Compact data structures: A practical approach . Cambridge University\nPress, 2016.\n31OpenStreetMap contributors. OpenStreetMap Data Extract for Italy. https://www.\nopenstreetmap.org . Retrieved from http://download.geofabrik.de on May 9, 2018.\n32Joseph O’Rourke. An on-line algorithm for ﬁtting straight lines between data ranges. Com-\nmun. ACM , 24(9):574–578, September 1981.\n33Rasmus Pagh and Flemming Friche Rodler. Cuckoo hashing. Journal of Algorithms ,\n51(2):122 – 144, 2004.\n34Jun Rao and Kenneth A. Ross. Cache conscious indexing for decision-support in main\nmemory. In Proceedings of the 25th International Conference on Very Large Data Bases ,\nVLDB, pages 78–89, San Francisco, CA, USA, 1999. Morgan Kaufmann Publishers Inc.\n35David Salomon and Giovanni Motta. Handbook of Data Compression . Springer Publishing\nCompany, Incorporated, 5 edition, 2009.\n36Raimund Seidel and Cecilia R. Aragon. Randomized search trees. Algorithmica ,\n16(4/5):464–497, 1996.\n37Jack Sklansky and Victor Gonzalez. Fast polygonal approximation of digitized curves.\nPattern Recognition , 12(5):327 – 331, 1980.\n38Jeﬀrey Scott Vitter. External memory algorithms and data structures: Dealing with\nmassive data. ACM Comput. Surv. , 33(2):209–271, 6 2001.\n39Jianguo Wang, Chunbin Lin, Yannis Papakonstantinou, and Steven Swanson. An exper-\nimental study of bitmap compression vs. inverted list compression. In Proceedings of the\n2017 ACM International Conference on Management of Data , SIGMOD, pages 993–1008,\nNew York, NY, USA, 2017. ACM.\n40Till Westmann, Donald Kossmann, Sven Helmer, and Guido Moerkotte. The implement-\nation and performance of compressed databases. SIGMOD Rec. , 29(3):55–67, September\n2000.\n41Ian H. Witten, Alistair Moﬀat, and Timothy C. Bell. Managing Gigabytes (2Nd Ed.):\nCompressing and Indexing Documents and Images . Morgan Kaufmann Publishers Inc.,\nSan Francisco, CA, USA, 1999.\n42Qing Xie, Chaoyi Pang, Xiaofang Zhou, Xiangliang Zhang, and Ke Deng. Maximum\nerror-bounded piecewise linear representation for online stream approximation. VLDB\nJ., 23(6):915–937, 2014.\n43H.Zhang, G.Chen, B.C.Ooi, K.Tan, andM.Zhang. In-memorybigdatamanagementand\nprocessing: A survey. IEEE Transactions on Knowledge & Data Engineering , 27(7):1920–\n1948, 2015.",
  "textLength": 94560
}