{
  "paperId": "59d95ec4a2aa758109582c67c692a2e41f8b5f07",
  "title": "Learning from Data to Speed-up Sorted Table Search Procedures: Methodology and Practical Guidelines",
  "pdfPath": "59d95ec4a2aa758109582c67c692a2e41f8b5f07.pdf",
  "text": "1\nLearning from Data to Speed-up Sorted Table\nSearch Procedures: Methodology and Practical\nGuidelines\nDomenico Amato, Raffaele Giancarlo, Giosu ´e Lo Bosco\nAbstract —Sorted Table Search Procedures are the quintessential query-answering tool, with widespread usage that now includes also\nWeb Applications, e.g, Search Engines (Google Chrome) and ad Bidding Systems (AppNexus). Speeding them up, at very little cost\nin space, is still a quite signiﬁcant achievement. Here we study to what extend Machine Learning Techniques can contribute to obtain\nsuch a speed-up via a systematic experimental comparison of known efﬁcient implementations of Sorted Table Search procedures,\nwith different Data Layouts, and their Learned counterparts developed here. We characterize the scenarios in which those latter can be\nproﬁtably used with respect to the former, accounting for both CPU and GPU computing. Our approach contributes also to the study of\nLearned Data Structures, a recent proposal to improve the time/space performance of fundamental Data Structures, e.g., B-trees, Hash\nTables, Bloom Filters. Indeed, we also formalize an Algorithmic Paradigm of Learned Dichotomic Sorted Table Search procedures that\nnaturally complements the Learned one proposed here and that characterizes most of the known Sorted Table Search Procedures as\nhaving a learning phase that approximates Simple Linear Regression.\nIndex Terms —Design of Algorithms, Machine Learning, Neural Networks, Learning from Data, Linear Regression, Sorted Table Search,\nData Layouts\nF\n1 I NTRODUCTION\nSOrted Table Search Procedures are a fundamental part\nof Computer Science [1]. As well pointed out by Khoug\nand Morin [2], in view of the novel hardware architec-\ntures now available, a speed-up of those procedures it is\nstill worth of investigation and useful in many application\ndomains, e.g. Decision Support in Main Memory [3], Web\nSearch Engines and ad Bidding [2].\nAn emerging trend, in order to obtain time/space im-\nprovements in classic Data Structures, is to combine Ma-\nchine Learning techniques with algorithmic techniques. This\nnew area goes under the name of Learned Data Structures\n[4]. The State of the Art is well presented in a recent re-\nview by Ferragina and Vinciguerra [5]. The theme common\nto those new approaches to Data Structures Design and\nEngineering is that a query to a data structure is either\nintermixed with or preceded by a query to a Classiﬁer [6]\nor a Regression Model [7], those two being the learned part\nof the data structure.\nSomewhat surprisingly, the rather simple Sorted Table\nSearch Procedures, i.e., Binary [1] and Interpolation Search\n[8], have not been considered in this new scenario. Indeed,\na rather delicate balance between the power of a learning\n\u000fThis research is funded in part by MIUR Project of National Relevance\n2017WR7SHH Multicriteria Data Structures and Algorithms: from\ncompressed to learned indexes, and beyond. We also acknowledge an\nNVIDIA Higher Education and Research Grant (donation of a Titan V\nGPU). Additional support to RG has been granted by Project INdAM -\nGNCS Analysis and Processing of Big Data based on Graph Models.\n\u000fD. Amato, R. Giancarlo and G.Lo Bosco are with the Department of\nMathematics and Computer Science, University of Palermo, Palermo,\nItaly.\n\u000fG. Lo Bosco is the corresponding author, E-mail: giosue.lobosco@unipa.itmethod and the speed of those routines must be thoroughly\ninvestigated. We anticipate that the hearth of this research\nregards this balance. Informally, a Learned Sorted Table\nSearch procedure consists of a learning phase, in which the\nCDF of the data distribution that has generated the table is\nlearned via the elements present in the table with the use\nof a Regression Model, e..g., a regression line. Then, given a\nquery, the model is used to predict an interval in the table\nwhere the query element must be searched for. We provide\na systematic experimental analysis of the following aspects:\n(1) which model complexities, e.g., Neural Networks (NN\nfor short) or “closed form” Linear Regression, can be af-\nforded in order for Learned Sorted Search procedures to be\ncompetitive with respect to their standard counterparts; (2)\nhow learnable must be a table for Learned Sorted Search\nprocedures to be proﬁtable; and ﬁnally (3) when GPU com-\nputing can be afforded for query batch processing, assess to\nwhat extend are Learned Sorted Search procedures worth of\nconsideration both when implemented in CPU and GPU.\nThis latter aspect is investigated since one of the moti-\nvations for the use of Machine Learning to improve data\nstructures performance is given by the perceived advan-\ntage of Tensor computing as opposed to the classic “if-\nthen-else” approach. Apart from the practical indications\nthat our experimental study provides, necessarily bound to\nthe hardware architectures we use, it also sheds light on\nmethodological issues regarding Learned Data Structures\nthat have been either sketched in previous work, i.e., point\n(1), or largely ignored, i.e., (2) and (3). Details on our\nexperimental methodology and ﬁndings follow.\nOur study considers several versions of Binary [1] and\nInterpolation Search [8], together with efﬁcient data layouts.\nThey are presented in Section 2. Following Kraska et al., wearXiv:2007.10237v3  [cs.LG]  30 Jul 2020\n\n2\ncast the problem of Sorted Table Search in terms of Regres-\nsion Analysis [7], the novelty with respect to the State of the\nArt [5] being the formalization of an Algorithmic Paradigm,\nhere referred to as Learned Dichotomic Sorted Table Search\nprocedures. In that paradigm, standard procedures have\na “learning phase” that is an approximation of Simple\nLinear Regression. The paradigm, mostly of methodological\ninterest, makes explicit an overlooked connection between\nLearned and standard Sorted Table Search procedures, ac-\ncounting also, for some recent heuristics that have been\nproposed. This part is in Section 3. For our experiments,\nwe consider benchmark datasets that have appeared in the\nLiterature and in order to conduct an in-depth investigation\nof Learned Sorted Table Search procedures, we also consider\nsynthetic datasets generated from three distribution, whose\nCDF is representative of the CDFs associated to the major\ndistributions considered for Data Analysis and Modelling.\nThat is, our synthetic datasets well represent the spectrum\nof CDFs that a model for Learned Table Search procedures\nmust be asked to learn. This part is in Section 4.2. Our\nexperiments are reported in Sections 5 and 6. Based on them,\na synopsis of our additional ﬁndings is given next.\nWhen no batch query processing can be afforded, or a\nGPU is not available, Learned Binary Search can be prof-\nitably used, provided that (a) the data is easy to learn via\na very fast procedure such as Simple Linear Regression; (b)\nthe table ﬁts in the cache memory hierarchy. When batch\nprocessing can be afforded and a GPU is available, Learned\nSorted Table Search procedures (both in GPU and CPU)\nare not competitive with respect to a simple-minded GPU\nBinary Search. Interestingly, Learned Interpolation Search is\nconsistently superior to Interpolation Search, but with no\npractical effect, since Binary Search with an Eytzinger array\nlayout is consistently superior to both.\nIt is important to point out another methodological ﬁnd-\ning coming out from this research and that contributes to\nconsolidate an emerging trend. The most suggestive part of\nLearned Data Structures, i.e., the delegation to NNs to learn\nfrom data, results to be computationally expensive with\nrespect to the tight loop of Sorted Table Search procedures.\nHere we use 0-2 layer NNs with ReLU activators. It is\nof interest to highlight that an analogous conclusion was\nreached in regard to Learned Bloom Filters [9], with the use\nof Recurrent NN. Apparently, the learning and prediction\npower of the current NNs is a time performance mismatch\nwith respect to Data Structures that require only a few\ninstructions per iteration in order to answer a query. This\nasks for NNs and Classiﬁers that are fast to train and query\nand effective in learning.\n2 B INARY AND INTERPOLATION SEARCH\n2.1 Binary Search: Array Layouts and their Branch-\nFree Algorithms\nFollowing research in [2], we review here four basic layouts\nof a tableAfor Binary Search, as follows.\n1 Sorted. It is the classic textbook layout for standard\nBinary Search. In particular, We consider the branch-\nfree implementation provided in [11], referring to\nit as BFS, with prefetching (see Algorithm 1). Theterm branch-free refers to the fact that there is no\nbranching in the while loop. Indeed, as explained\nin [11], the case statement in line 8 of Algorithm\n1 is translated into a conditional move instruction\nthat does not alter the ﬂow of the assembly program\ncorresponding to that C++ code. In turn, that has the\nadvantage of a better use of instruction pipelining\nwith respect to the branchy version of the same code.\nPrefetching, i.e., instructions 6 and 7 in Algorithm\n1, refers to the fact that the procedure loads in the\ncache data ahead of their possible use. As shown\nin [11], such an implementation of Binary Search\nis substantially faster than its branchy counterpart,\nwhich we refer to as BBS.\n2 Eytzinger [2]. The sorted table is now seen as stored\nin a virtual complete balanced binary search tree.\nSuch a tree is laid-out in Breadth-First Search order\nin an array. An example is provided in Fig. 1. Also,\nin this case, we adopt a branch-free version with\nprefetching of the binary search procedure corre-\nsponding to this layout. It is reported in Algorithm\n2. In what follows, we refer to this procedure as BFE.\n3 B-tree. The sorted table is now seen as a B+ 1search\ntree [10], which is then laid out in analogy with an\nEytzinger layout. An example is provided in Fig. 2.\nAlso, in this case, we adopt a branch-free version\nof the binary search procedure corresponding to this\nlayout, with prefetching. It is taken from the software\nassociated to [2]. Due to its length, it is not reported\nhere. In what follows, we refer to this procedure as\nBFB .\n4 Van Edme Boas [11]. The sorted table is again seen\nas stored in a virtual complete binary tree. Such a\ntree, assuming that it stores nelements and letting h\ndenote its height, is laid out recursively as follows.\nIfn= 1, the element is stored in A[0]. Else, the top\npart of the tree of height bh=2cis laid out recursively\ninA[0\u0001\u0001\u0001;21+bh=2c\u00002]. At the leaves of this top\ntree, there are at most 21+bh=2csubtrees that are\nlaid out recursively and from left to right, starting\nat position 21+bh=2c\u00001of the array. An example\nis provided in Fig. 3. It is worth pointing out that\nthe extensive experiments conducted in [2], as well\nas further results presented in [12], indicate that\nsuch a layout, on occasions and on large datasets,\nmay be superior to the ones mentioned earlier. Due\nto such inconsistency in performance and since the\nintent of this research is methodological, it will not\nbe included in the experimental part of this study.\n2.2 Interpolation Search\nSuch a Sorted Table Search technique was introduced by\nPeterson [8] and it has received some attention in terms of\nanalysis [13] since it works very well on data following a\nUniform or nearly Uniform distribution and very poorly on\ndata following other distributions. The procedure adopted\nhere is reported in Algorithm 3. In what follows, we refer\nto such a procedure as IBS. It is of interest to point out\nthat whether a branch-free implementation of this method\nis advantageous with respect to its branchy version has\n\n3\nAlgorithm 1 Branch-free implementation of classic Binary\nSearch with prefetching. The code is as in [2].\n1:int prefetchBranchfreeBS(int *A, int x, int left, int right) f\n2: const int *base = A;\n3: int n = right;\n4: while (n>1)f\n5: const int half = n / 2;\n6: builtin prefetch(base + half/2, 0, 0);\n7: builtin prefetch(base + half + half/2, 0, 0);\n8: base = (base[half] <x) ? &base[half] : base;\n9: n -= half;\n10:g\n11: return (*base ¡ x) + base - A;\n12:g\n5\n79\n1113\n15 316\n1819\n2021\n25\n22 27\n20 16 9 215131925371115 18 22 27\nFigure 1. An example of Eyzinger layout of a table with 15 elements (see\nalso [2]).\nnot been extensively investigated. Experiments conducted\nfor this study (available upon request) show that there no\nadvantage. As for prefetching, due to the very irregular\naccess pattern that this procedure has to the table, it is\nobvious that it brings no advantage.\nFor completeness, we mention that modiﬁed versions of\nInterpolation Search have been designed, both in the static\nand dynamic case (see [14], [15] and references therein).\nThose versions can be shown to have a O(loglogn )or even\nanO(1)expected time performance if the data is drawn\nfrom suitable distributions that exhibit some mathematically\ndeﬁned notion of regularity (see references for details).\nSomewhat unfortunately, those variants, although of theo-\nretic interest, do not seem to be well suited to compete with\nstandard Interpolation Search in real settings. In particular,\nthe (static) variants proposed by Willard depend on three\nparameters \u000b,\u0002and \bthat are used to regularize the\nruntime of the procedure and that must be set through an\nempirical analysis. Another problem is the introduction of\n16\n16 920\n513\n1925\n37\n11 1518\n2122\n27\n16\n18 22 137 20212527 16 95 19 3 11 15\nFigure 2. An example of (B+ 1) layout of a table with 15 elements and\nB= 2(see also [2]).Algorithm 2 Branch-free implementation of Binary Search\nwith Eyzinger layout and prefetching. The code is as in [2].\nint eytzBS(int *A, int x, int left, int right) f\nint i = 0;\n3: int n = right;\nwhile (i<n)f\nbuiltin prefetch(A+(multiplier*i + offset));\n6: i = (x<=A[i]) ? (2*i + 1) : (2*i + 2);\ng\nint j = (i+1) >> builtin ffs(\u0018(i+1));\n9: return (j == 0) ? n : j-1;\ng\n5\n79\n1113\n15 316\n1819\n2021\n25\n22 27\n20 21 169 53 7 1311151918 2227 25\nFigure 3. An example of Van Edme Boas layout of a table with 15\nelements (see also [2]).\nsome arithmetic and selection operations on ﬂoating-point\ndata types that are computationally expensive to calculate\non today’s CPUs and introduce a substantial overhead in ad-\ndition to element search operations. More recently, heuristic\nand somewhat engineered variants of Interpolation Search\nhave been introduced in [16]. To the best of our experience\nwith the available code and with our own implementation\nof the heuristics, they may err when an element searched for\nis not in the table. For the reasons just outlined for each of\nthem, the variants mentioned above are not included in this\nstudy.\nAlgorithm 3 Implementation of Classic Interpolation\nSearch.\nint interpolation Search(int *arr, int x, int start, int end) f\n2: int lo = start, hi = (end - 1);\nwhile (lo<=hi && x>=arr[lo] && x <=arr[hi])f\n4: if (lo == hi)f\nif (arr[lo] == x) return lo;\n6: return -1;\ng\n8: int pos = lo + (((double)(hi - lo) / (arr[hi] - arr[lo])) *\n(x - arr[lo]));\nif (arr[pos] == x) return pos;\n10: if (arr[pos]<x) lo = pos + 1;\nelse hi = pos - 1;\n12:g\nreturn pos;\n14:g\n\n4\n3 R EGRESSION ANALYSIS , L EARNING FUNC-\nTIONS ,AND SORTED TABLE SEARCH\nIt is well known that Sorted Table Search can be phrased\nas Predecessor Search Problem ( PSP, for short): for a given\nquery element x, return the index jsuch thatA[j]\u0014x <\nA[j+ 1] . Kraska et al. [4] have proposed an approach that\ntransforms PSP into a Regression Analysis problem ( RA,\nfor short) [7]. In what follows, we ﬁrst review the regression\ntechniques used for this research (Section 3.1) and then cast\nPSP into an RA(Section 3.2).\n3.1 Regression Analysis\nIt is a methodology for estimating a given function F:\nRm!Rvia a speciﬁc function model ~F. The independent\nvariables in x2Rmand the dependent variable y2R\nare usually referred to as predictors and outcome, respec-\ntively. The parameters of ~Fare estimated by minimizing an\nerror function, computed using a sample set of predictors-\noutcome measurements. The most commonly used Regres-\nsion Loss Function is the Mean Square Error. Such a task can\nbe accomplished in several ways. Here we follow the meth-\nods outlined in [17]. In particular, we ﬁrst present closed-\nform formulae solving the posed minimization problem,\nwith a linear (as a matter of fact, polynomial) model (Section\n3.1.1). Then, we outline a gradient descent method based on\nNN (Section 3.1.2).\n3.1.1 Multivariate Linear Regression\nLinear Regression ( LR, for short) is a speciﬁc approach to the\nregression that assumes a linear function (i.e. polynomial of\ndegree 1) as a model. The case of one predictor is referred to\nas Simple Linear Regression ( SLR , for short), and otherwise\nas Multivariate Linear Regression ( MLR , for short).\nFor MLR , given a sample set of npredictor-outcome\ncouples (xi;yi), where xi2Rmandyi2R, the goal is\nto characterize the linear function model ~F(x) =^ wxT+^b\nby estimating the parameters ^ w2Rmand^b2R, using the\nsample set. We can deﬁne a matrix Zof sizen\u0002(m+ 1)\n(usually referred to as the design matrix), where Ziis thei-\nth row of Zsuch that Zi= [xi;1]. Moreover, yindicates\nthe vector of size nsuch that the outcome yjis itsj-th\ncomponent. The Mean Square Error minimization on the\nbasis of the estimation is:\nMSE (w;b) =1\nn\r\r\r[w;b]ZT\u0000y\r\r\r2\n2(1)\nMSE is a convex quadratic function on [w;b], so that the\nunique values that minimize it can be obtained by setting\nits gradientrw;bequal to zero. The closed form solution for\nthe parameters w;bis\n[^ w;^b] =yZ(ZTZ)\u00001(2)\nIt is to be noted that, for SLR , the case of models more\ncomplicated than a polynomial of degree one, speciﬁcally\npolynomial models with degree g > 1, can be reduced to\nMLR . Indeed, we can consider the model\n~F(z) =gX\ni=1wixi+b=wzT+b;\nFigure 4. The atomic element uses (x1;::;xd)and(w1;::;w d)as inputs.\nEachxiis a binary digit while each wiis a real value. The ﬁnal output\ny=max(0;\u0006d\ni=1wixi).\nwherewis of sizeg,z= [x;::;xg\u00001;xg]2Rgis the predictor\nvector for MLR .\n3.1.2 Multi Layer Feed Forward Neural Networks with\nReLU Activators for Regression\nAnother approach to regression is to use a universal ap-\nproximator such as a neural network. Here, we concentrate\nonfeed forward neural networks to learn a function Ffrom a\nsample set (xi;yi). The general strategy is simple. Indeed,\nNN uses a learning phase which is iterative, starting from an\ninitial approximation ~F0. At each step i, the current approx-\nimation is reﬁned, using the sample set, leading to a new\nsolution ~Fisuch thatE(~Fi(x);y)\u0014E(~Fi\u00001(x);y), where\nErepresents a suitably chosen error function. The process\nhalts when we have only marginal gains on the error. That\nis, given a tolerance \u000e, the process stops when ~F=~Fisuch\nthatjE(~Fi(x);y)\u0000E(~Fi\u00001(x);y)j \u0014\u000e. The approach to\nregression by an NN involves the representation of the input\nxinto a binary string of size dof each of its components. The\ncase of interest here is univariate and we denote by\u0000 !xsuch\na binary string, outlining the relevant features of the NN\nnext [18]:\n(1) ARCHITECTURE TOPOLOGY . It is characterized\nby the following:\n(1.a) The neuron, i.e. the atomic element of\nthe NN . A scheme is provided in Figure 4. The\nbasic operations on its 2dbinary input lines and\nthe activation via function fare relevant. For the\nﬁrst, we use the scalar product between the input\nvectors\u0000 !wand\u0000 !x. The vector\u0000 !wis referred to as\nweight . For the second, we use is the so called reLU\ni.e.f(x) =max(0;x),.\n(1.b) The number of hidden layers K.\n(1.c) For each layer i, its number niof atomic\nelements .\n(1.d) The connection between layers . The layers\nare fully connected, i.e. each atomic element at layer\nKis connected to all the atomic elements at layer\nK+ 1.\n(2) LEARNING ALGORITHM : This is characterized as\nfollows:\n(2.a) The error function E, used to measure how\nclose the approximation ~Fis toF.\nThe speciﬁc NN models used for our experiments are:\n\u000fNN0 : A zero hidden layer network, i.e., an NN com-\nposed by only one input layer of n0= 256 neurons\nand an output layer with 1neuron.\n\u000fNN1 : A one hidden layer network, i.e., one hidden\nlayer composed of n1= 256 neurons inserted be-\ntween the input and output layers of NN0 .\n\n5\nFigure 5. A table Aof 15000 elements is shown on the abscissa (a small\nvertical line for each element). The curve corresponding to its EPis a\nsigmoid shown in bold. The two vertical lines delimit the range of indices\nof the table. The dotten line in bold corresponds to the ~EPobtained via\nSLR. Points (b) and (c) denote \u000f2and\u000f3for that table and prediction.\nAs for\u000f1, is computed with the use of the regression line between points\n(b) and (c)\n\u000fNN2 : A two hidden layer network, i.e., a hidden\nlayer ofn2= 256 neurons inserted between the input\nand output layers of NN1.\n3.2 Predecessor Search as a Regression Analysis\nProblem\nConsider a sorted table Aofnkeys, taken from a universe\nU. LetEP be the empirical predecessor function of elements\nofUwith respect to (sample) A. That is, for each z2U,\nEP(z) =jfy2Ajy\u0014zgj. Notice that knowledge of EP\nprovides a solution to PSP since, given an element z2U,\nonly one evaluation of EP provides the predecessor index\nwe are looking for. Usually, EP is not available, implying\nthat an implicit or explicit mathematical expression that we\ncan use over and over again to solve PSP must be estimated\nnumerically . As already mentioned, this is now a Regression\nAnalysis problem, where the sample set is given by the n\npairs (A[j];j). Assuming that an estimate ~EP ofEP has\nbeen found, its precision with respect to the elements in the\ntableAis computed as follows.\n(a)\n1) For each jin[1;n]such that 1\u0014~EP(A[j])\u0014n, let\n\u000f1=maxjj~EP(A[j])\u0000j)j. Once that ~EP and\u000f1are\navailable for A, we can solve PSP as follows. For a\nqueryx, the value of ~EP(x)is computed. When 1\u0014\n~EP(x)\u0014n, the search is completed via a standard\nSorted Search Procedure in the interval [~EP(x)\u0000\n\u000f1;~EP(x) +\u000f1]. An example is provided in Fig. 5\nwith anEP that follows a sigmoid function. That\nis, a function \u001b(y) =1\n1+e\u0000y.\n2) For each jin[1;n]such that ~EP(A[j])<1, let\u000f2\nbe the maximum such a j. Once that ~EP and\u000f2\nare available for A, we can solve PSP as in case\n(a), when ~EP(x)<1with the interval [1;\u000f2]. An\nexample is provided in Fig. 5.\n3) For each jin[1;n]such that ~EP(A[j])> n, let\u000f3\nbe the minimum such a j. Once that ~EP and\u000f3\nare available for A, we can solve PSP as in case\n(a), when ~EP(x)> n with the interval [\u000f3;n]. An\nexample is provided in Fig. 5.If we use the technique outlined in Section 3.1.1, i.e.,\nequation (2), it is immediate to see that the estimation of\n~EP can be performed in polynomial time in n, assuming\nthatm\u0014n. Indeed,m= 1 forSLR and it is a constant\nfor polynomial models of degree at least two. The approach\nbased on NNs is more heuristic, in the sense that there is\nno guarantee to ﬁnd the optimal solution. Moreover, being\nan iterative process with no guarantee of convergence, it is\nquite difﬁcult to provide an estimate of the computational\ncomplexity of the entire procedure unless one ﬁxes a priori\nthe number of iterations required for halting rather than the\nachievement of a given level of precision.\nLet nowmaxleft = max (2\u000f1+ 1;\u000f2;\u000f3)=n. It represents\nthe fraction of the table that is left to search after a prediction\nis made. In what follows we refer to 1\u0000maxleft as the\nreduction factor . It represents the percentage of the table that\nthe prediction has excluded from further consideration, e.g.,\nthe ﬁrst iteration of binary search has a reduction factor of\n50% . It is to be noted that, although Sorted Table Search\nprocedures may beneﬁt in theory from a reduction to be\nfaster than their standard counterparts, in practice there is\na trade-off between the cost of prediction and the reduction\nin size that is attained. That is, experimentally, it may as\nwell be that a Learned Sorted Table Search procedure may\nbe slower than its standard counterpart, if the estimate ~EP\nis not particularly sharp. In order to study such a trade-\noff, that provides valuable information for the real use\nof Learned Sorted Table Search procedures, we construct\nsynthetic datasets (see Section 4.3) and we experiment with\nthem (see Section 6 ).\n3.3 Dichotomic Sorted Table Search: Prediction via Re-\ngression Analysis Approximations\nDichotomic Sorted Table Search procedures, i.e., the stan-\ndard and well-known strategies to search in a sorted table,\ncan also be viewed as procedures that, in order to choose\nan index, use an approximation to a prediction that can\nbe made via Regression Analysis. It is of interest to make\nexplicit such a generic paradigm, which naturally comple-\nments the one of the previous section.\nConsider a black box BBM (A;k;l )that gets as inputs a\nsorted table Aand two indices k;lofA, and returns a model\nMfor~EP computed on A[k;l]. Then, given a query element\nx,M(x)provides an index ^jin[l;k]that can be used for\nthe “dichotomic decision” in that interval (for clarity of\nexposition, we are assuming that the prediction always falls\nwithin the required interval). Algorithm GDSA , reported\nin Figure 4, is the corresponding paradigm to Sorted Table\nSearch via the black box.\nLet us consider the case when the BBM returns a model\nMwhich uses a line of equation y=\u000bz+\fto make the\nprediction. In this case, ^j=M(x) =d\u000bx+\fe. WhenM\nis the SLR modelLRM forEP,GDSA specializes to a\nnew dicothomic search procedure that, although unlikely\nto be practical, it is optimal in terms of approximating the\nEP function of the data at each step of the procedure. The\nknown search procedures are particularly convenient ap-\nproximations to LRM in terms of Mean Square Error. When\nM(x) =\u0006l+k\n2\u0007\n, we have a Binary Search Model (BSM ) that\nspecializesGDSA to Binary Search. When M(x) =Fg\u00001,\n\n6\nAlgorithm 4 The generic search algorithm, GDSA\nint GDSA(int *arr, int x, int start, int end) f\nint lo = start, hi = (end - 1);\nwhile (lo<=hi && x>=arr[lo] && x <=arr[hi])f\n4: if (lo == hi)f\nif (arr[lo] == x) return lo;\nreturn (arr[lo]¡x ? lo : lo-1);\ng\n8: M=BBM(arr,lo,hi);\npos = M(x);\nif (arr[pos] == x) return pos;\nif (arr[pos]<x) lo = pos + 1;\n12: else hi = pos - 1;\ng\nreturn pos;\ng\nwheregis the least integer such that Fg+1\u0015(k\u0000l+ 1) and\nFjdenotes the j-th Fibonacci number, we have a Fibonacci\nSearch Model (FSM ) that specializes GDSA to Fibonacci\nSearch (see [1]). When\nM(x) =\u0018\nk+l\u0000k\nA[l]\u0000A[k](x\u0000A[k])\u0019\nwe have an Interpolation Search Model (ISM ) that specializes\nGDSA to Interpolation Search. One can extend the class of\nmodels also to Slope Reuse Interpolation Search (SIM ) [16],\ncharacterized by the line:\nM(x) =dk+s(x\u0000A[k])e\nwheresdenotes a precomputed slope.\nFinally, it is worth pointing out that models more com-\nplicated than linear can be used. For instance, Three Point\nInterpolation Search TIP has a model for BBM , which\nis characterized by a curve rather than a line [16]. The\nalgorithm selects three increasing positions k\u0014l\u0014m, and\nfound the prediction ^jin the following way:\nM(x) =l+\n(A[l]\u0000x)(l\u0000m)(l\u0000k)(A[m]\u0000A[k])\n(A[m]\u0000x)(l\u0000m)(A[k]\u0000A[l]) +A[k](l\u0000k)(A[l]\u0000A[m])\n4 E XPERIMENTAL SETUP\n4.1 Hardware\nAll the experiments have been performed on a workstation\nequipped with an Intel Core i7-8700 3.2GHz CPU and an\nNvidia Titan V GPU. The total amount of system memory is\n32 Gbyte of DDR4. The GPU is also supplied with its own\n12 Gbyte of DDR5 memory and adopts a CUDA parallel\ncomputing platform and an API. The single computational\nelement of a GPU, referred to as CUDA core, is speciﬁcally\ndesigned for one single-precision multiply-accumulate op-\neration. It is the basic step of each computational cell of a\nfeed-forward neural network. Therefore, such an architec-\nture very well ﬁts the NN models chosen for this study [19].\nFor reference in the following, it is useful to discuss the\noverhead in data I/O between CPU and GPU. The slowestlink involved in GPU computing is memory transfers from\nsystem to GPU. This is actually a technological limit and it\nis due to the difference in bandwidths between the system\nmemory, the GPU device memory and the PCI express\nbus connecting them. In particular, the system memory is\nlimited to the DDR4 generation, which allows at most a\nbandwidth of 25Gbyte/sec. The PCIe 3, which is actu-\nally the standard, supports a maximum bandwidth of 32\nGB/sec. Such values are undersized with respect to the\nDDR5 memory bandwidth, which in the case of our GPU\ndevice is 600 GByte/sec. We expect that such differences\nwill be smoothed out by the introduction of DDR5 system\nmemory and PCI-Express 5bus technology.\n4.2 Datasets\nWe use two different types of data. The ﬁrst, described in\nSection 4.2.2, can be thought of as a standard in this area of\nresearch, e.g., [4]. As anticipated in Section 3.2, the second\ndataset type is designed to get a quantitative estimate of\nthe reduction factor that the procedures in that section must\nattain in order for Learned Sorted Search procedures to be\nadvantageous with respect to their standard counterparts.\nThose datasets are described in Section 4.3.\n4.2.1 Preliminary Deﬁnitions\nIt is useful to recall the deﬁnitions of Uniform, Log-normal\nand Logit distributions and the type of EP functions, in\nterms of CDF, that correspond to them and which must be\neventually learned when tables are built by sampling from\nthose distributions. The Uniform distribution Uis\nU(x;a;b ) =\u001a1\nb\u0000aifx2[a;b]\n0otherwise\nwhereaandbare the parameters of the distribution. The\nCDF corresponding to it is a straight line in the interval\n[a;b].\nThe Log-normal distribution Lhas two parameters: its\nmean\u0016and standard deviation \u001b. It is deﬁned for x>0as\nfollows:\nL(x;\u0016;\u001b ) =e\u0000(lnx\u0000\u0016)2\n2\u001b2\nxp\n2\u0019\u001b\nFor this study, the parameters are chosen so that the\nCDF is a sigmoid function, where the concave part is pre-\ndominant. This case is representative of other well-known\ndistributions such as Poisson and \u001f2.\nThe Logit distribution is deﬁned as\nG(x;\u0016;s ) =e\u0000(x\u0000\u0016)\ns\ns(1 +e\u0000(x\u0000\u0016)\ns)\nwith mean \u0016and standard deviation s >0. Its CDF corre-\nsponds to a sigmoid function. This case is representative of\nother well-known distributions such as Normal, Binomial\nand Hypergeometric.\nWhen sampling from those distributions, we take inte-\ngers in the interval is [1;2r\u00001\u00001], whereris determined by\nthe precision used to represent integers, i.e., 32 or 64 bits.\n\n7\n4.2.2 Dataset1: Literature Benchmarks\nWe use synthetic and real datasets. The ﬁrst ones are gener-\nated according to the Uniform (with parameters a= 1 and\nb= 2r\u00001\u00001and Log-normal (with standard parameters\n\u0016= 0 and\u001b= 1 ) distributions. The second ones are\ndomain-dependent because they are obtained from different\napplication areas (such as IoT or the Web applications). In\nparticular, the real-weblogs [4], [20] contains timestamps of\nabout 715M requests performed by a web server during\n2016. The real-iot dataset [4], [20] consists of timestamps\nof about 26M events recorded during 2017 by IoT sensors\ndeployed in academic buildings. All the datasets are sorted\nand do not contain duplicate values. Their characteristics in\nterms of size are summarized in Table 1. The method used\nto extract query datasets takes as input a dataset U among\nthose described above, and it returns three query datasets\nof the following size with respect to U: 10%;50%;80% ,\nrespectively. Each of those consists of roughly 50% of items\nin U. The notation regarding those query datasets is as\nfollows:<dataset name >-query<percentage>, e.g., uni-01-\nquery10 refers to the ﬁrst uniform dataset in which the size\nof the query dataset is 10% of it. For this research, query\ndatasets are not sorted.\nTable 1\nA summary of the Dataset1. For each dataset in the collection, we\nshow: the name we have used (column Name ), its size in Kilobyte\n(column Size (KB) ), the number of elements in it (column Items ), and\nthe type of its elements (ﬁnal column). ( Type ).\nDatasets: Uniform Distribution\nName Size (KB) Items Type\nuni-01 5:37 5:12\u0002102integer\nuni-02 8:58\u00021018:19\u0002103integer\nuni-03 1:10\u00021041:05\u0002106integer\nuni-04 2:81\u00021062:68\u0002108integer\nDatasets: Log-normal Distribution\nName Size (KB) Items Type\nlogn-01 5:13 5:12\u0002102integer\nlogn-02 8:23\u00021018:19\u0002103integer\nlogn-03 1:05\u00021041:05\u0002106integer\nlogn-04 2:71\u00021062:68\u0002108integer\nDatasets: Real Distribution\nName Size (KB) Items Type\nreal-wl 3:48\u00021053:16\u0002107integer\nreal-iot 1:67\u00021051:52\u0002107integer\n4.3 Datasets2: Reduction Factor Simulations\nIn order to estimate the effectiveness of the Learned Sorted\nTable Search procedures as a function of the reduction factor,\nwe generate synthetic datasets that may be thought of as\nbeing challenging for them to handle. Binary Search has a\nstrategy that is insensitive to the distribution underlying\nthe dataset, e.g., Uniform or Logit, and the worst-case\nconsists of queries for elements that are not in the table.\nWe use the Log-Normal distribution. As for Interpolation\nSearch, its performance depends heavily on the distribution\ncharacterizing a given table. Therefore, we generate datasets\naccording to the three distributions deﬁned in Section 4.2.1\nsince their CDFs are representative of many common dis-\ntributions. For completeness, it is worth mentioning that\nboth the Log-normal and the Logit distributions are notregular according to the deﬁnitions provided in [14], [15]\nand that no lower bound is available on the performance of\nInterpolation Search on tables extracted from those distribu-\ntions. However, their non-regularity can be taken as a strong\nindication that they are challenging for Interpolation Search.\nDetails regarding the generation of the tables are provided\nSection 4.3.2.\nAs for Binary Search and in regard to the generation\nof query datasets, we proceed as follows. Given a table T,\nwe compute a query dataset Qconsisting of one million\nelements not in T. For each xinQ, we also generate\nan artiﬁcial table interval in which to search for x. The\ninterval contains the predecessor index of x. The length of\nthe interval, equal within each query dataset, allows us to\ncontrol time performance as a function of predetermined\nreduction factor values. As for interpolation Search, we use\nthe same query datasets, ignoring the artiﬁcial intervals\nsince the reduction factor is generated via the techniques\nin Section 3.1. The details are provided in Section 4.3.2.\nAlgorithm 5 Query Generation for a given Table\n1://Input(T,p,m)!Q\n2:Fori= 1 tomdo:\n3: Letqibe an element chosen uniformly and at random\nin[1;2r\u00001\u00001]so thatqi=2T\n4: Search for the predecessor index jofqiinT.\n5: Choose uniformly and at random an integer kin\n[0;d(1\u0000p)\u0003ne]\n6: LetIthe table interval obtained by juxtaping [0;d(1\u0000\np)\u0003ne]to theTso thatkandjcoincide\n7: Report inQthe extremities of interval Iand the\nelementqi.\n8:ReturnQ\n4.3.1 Generation of Tables\nWe describe the process for the Log-normal distribution,\nbeing the same for the Uniform and Logit. The parameters of\nthe Uniform and Log-normal distributions are as in Section\n4.2.2. As for the Logit, we use \u0016= 0:5and\u001b= 0:04, since\nthose parameters ensure that the EP of the sampled data is\na sigmoid function. Let nbe the number of elements a table\nmust have. Then, that many integers are chosen according\nto the Log-normal distribution. The number nof elements\nis chosen according to the capacity of each component of\nthe internal memory hierarchy. Indeed, as pointed out in\n[2], such a hierarchy has a subtle and deep impact on the\nperformance of Binary Search. For the computer architecture\nwe are using, the details of the tables we generate are as\nfollows:\n\u000fﬁtting in L1 cache and GPU Memory: cache size\n32Kb and each element represented with 32 bits.\nTherefore, we choose n= 7.5K (this table is denoted\nL1).\n\u000fﬁtting in L2 cache and GPU Memory: cache size\n256Kb and each element represented with 32 bits.\nTherefore, we choose n= 63K (this table is denoted\nL2).\n\u000fﬁtting in L3 cache and GPU Memory: cache size\n8Mb and each element represented with 32 bits.\n\n8\nTherefore, we choose n= 1.5M (this table is denoted\nL3).\n\u000fﬁtting in GPU Memory and PC Main Memory: GPU\nmemory 12Gb and each element represented with\n64 bits. Therefore, we choose n= 1.25G (this table is\ndenotedL4).\n\u000fﬁtting Workstation Main Memory: Main memory\n32Gb and each element represented with 64 bits.\nTherefore, we choose n= 3.75G (this table is denoted\nL5).\n4.3.2 Query dataset generation for a given table T and\nreduction factor p\nGiven a previously created table Tand a reduction factor p,\nwe create a query dataset of melements, in which each is\nnot inT. We also provide an artiﬁcial prediction interval\nfor each of those elements. The details are in Algorithm\n5. It takes as input a table T, a reduction factor pand the\nnumber of query elements mto generate. It returns a query\ndatasetQ, containing the queries elements and the ﬁctitious\nprediction interval. Such a procedure is systematically used\nin Section 6 in order to ﬁnd the breakeven reduction factor\npoint in which Learned Binary Search becomes superior to\nits standard counterpart.\n5 F UNCTION MODEL COMPLEXITIES THAT BI-\nNARY AND INTERPOLATION SEARCH CANPROF-\nITABLY USE AS “ORACLES ”\nWith reference to Section 3.1, the models that are considered\nfor this study can be divided into two complexity categories:\nlinear, i.e., NN0 and SLR , and non-linear, i.e., NN1 ,NN2\nand MLR with degree higher than one. Due to the simplicity\nand compactness in terms of code of the search procedures\nwe are considering, it is not clear which model is best suited\nto be “an oracle” for the mentioned procedures. The aim of\nthe experiments presented in this Section is to shed light on\nthis aspect with the use of benchmark datasets in this area,\ni.e., the ones described in Section 4.2.2.\n5.1 The Cost of Learning: Multivariate Linear Regres-\nsion is Better Than NNs\nModels need to learn the function one is trying to approxi-\nmate. For the uses intended in this research, two indicators\nare important. The time required for learning and the re-\nduction factor that one obtains. Indeed, it is worth recalling\nfrom Section 3.2 that, once an index has been predicted, a\npercentage of the table corresponding to its reduction factor\nis no longer considered for searching. As a comparison\nbaseline for training time, we adopt the time that it takes to\nsort the table being learned since such a step can intuitively\nbe assumed to be the “learning step” of a Sorted Table\nSearch procedure.\nFor the training of NNs, we have used Tensorﬂow\n[21] with GPU support, while for SLR and MLR , we use\nour own C++ implementation of the procedure outlined in\nSection 3.1.1. The sorting routine is the standard C++ Qsort\nutility [22]. A representative synopsis of the entire set of\nexperiments is reported in Tables 2 and 3. Based on the per-\nformed experiments, it is clear that NNs are not competitive.Table 2\nNN training with the use of Tensorﬂow on GPU. The ﬁrst column\nindicates the dataset, the second the time per elements in seconds,\nwhile the third gives the table reduction expressed in percentage. For\ntiming comparisons, the standard C++ Qsort utility, used to sort the\nmentioned datasets, takes \u00191:19\u000210\u00007s per item.\nNN0\nDataset Training Time (s) % Reduction Factor\nuni03 2:55\u000210\u0000494:08\nlogn03 1:39\u000210\u0000454:40\nreal-wl 2:50\u000210\u0000499:99\nreal-iot 1:28\u000210\u0000489:90\nNN1\nDataset Training Time (s) % Reduction Factor\nuni03 4:18\u000210\u0000499:89\nlogn03 3:79\u000210\u0000494:21\nreal-wl 2:31\u000210\u0000499:88\nreal-iot 4:20\u000210\u0000498:54\nNN2\nDataset Training Time (s) % Reduction Factor\nuni03 4:49\u000210\u0000499:87\nlogn03 8:60\u000210\u0000497:14\nreal-wl 2:33\u000210\u0000499:84\nreal-iot 3:57\u000210\u0000497:31\nTable 3\nSLR and MLR Models. The Table Legend is as in Table 2.\nSLR\nDataset Training Time (s) % Reduction Factor\nuni03 8:20\u000210\u0000899:94\nlogn03 5:61\u000210\u0000877:10\nreal-wl 5:82\u000210\u0000899:99\nreal-iot 7:70\u000210\u0000896:48\nQuadratic MLR\nDataset Training Time (s) % Reduction Factor\nuni03 1:27\u000210\u0000799:98\nlogn03 1:02\u000210\u0000790:69\nreal-wl 1:14\u000210\u0000799:99\nreal-iot 1:25\u000210\u0000799:10\nCubic MLR\nDataset Training Time (s) % Reduction Factor\nuni03 1:84\u000210\u0000799:97\nlogn03 1:74\u000210\u0000795:76\nreal-wl 1:24\u000210\u0000799:45\nreal-iot 1:63\u000210\u0000798:87\nThey are slower to train with respect to SLR and MLR mod-\nels and do not seem to be better in terms of approximation,\neven if they use a highly engineered platform with GPU\nsupport. Without the use of a GPU, the time performance\nof their training would considerably worsen (experiments\nnot shown and available upon request). However, GPU has\na limited amount of memory, which makes it unusable on\nlarge datasets (experimental data not shown and available\nupon request). On the other hand, SLR and MLR models\nhave a training time comparable with the pre-processing\nstep of sorting the table, yielding no substantial overhead\nto such a step. This aspect is very important for Decision\nSupport in Main Memory applications. Indeed, as indicated\nin [3], in that important Data Bases domain one can use\nsorted tables and binary search as long as the table can be\nquickly rebuilt to accommodate for changes. In our setting,\nthat would involve a new training phase.\n\n9\nTable 4\nPrediction Effectiveness Baseline. The Table Legend follows the\nterminology introduced in the main text. The timing is reported as time\nper query in seconds.\nDataset-Query BFE L-BFS\nuni03 50% 1:10\u000210\u000079:42\u000210\u00008\nlogn03 50% 1:08\u000210\u000071:60\u000210\u00007\nreal-wl 50% 1:79\u000210\u000075:05\u000210\u00008\nreal-iot 50% 1:64\u000210\u000078:32\u000210\u00008\nTable 5\nPrediction Effectiveness for Baseline. The Table Legend follows the\nterminology introduced in the main text. The timing is reported as time\nper query in seconds.\nDataset-Query IBS L-IBS\nuni03 50% 7:71\u000210\u000081:67\u000210\u00008\nlogn03 50% 3:87\u000210\u000079:38\u000210\u00008\nreal-wl 50% 9:42\u000210\u000081:70\u000210\u00008\nreal-iot 50% 8:68\u000210\u000072:51\u000210\u00008\n5.2 Simple Linear Regression is the “Oracle” of Choice\n5.2.1 Synopsis\nOnce that we have “an oracle” whose prediction, given a\nquery, conﬁnes the search to a restricted part of the table,\nit is important to assess how useful such an approach is in\nspeeding up Binary and Interpolation Search. To this end,\nwe take as baseline BFE, which, as indicated in [2], seems to\nbe the best method with the used dataset sizes and IBS.\nAs for the “search with an oracle”, we take as baseline\nSLR prediction (denoted simply L) with BFS and IBS. We\nalso consider other models as “oracles”. In particular, the\nones that have been discussed in the previous section.\nWe have performed experiments on all of the datasets\nand query sets described in Section 4.2.2. We report only a\nrepresentative synopsis of them, in Tables 4,5, 6 and 7. They\nrefer to queries containing a number of elements equal to\n50% of the reference dataset dimension. Coherently with the\nfull set of experiments, the synopsis shows that only SLR is\ncompetitive as an “oracle” to improve Binary and Interpo-\nlation Search. Therefore, from now on, we concentrate only\nonSLR .\n5.2.2 A Detailed Analysis of Simple Linear Regression as\nan “Oracle”\nIn order to synthetically show the full set of experimental\nresults on Dataset1 , we use the following criterion. Each\nexperiment is labelled with TRUE when the learned routine\nTable 6\nPrediction Effectiveness-Neural Networks Models. NN0-BFS refers to\nBinary Search with NN0 as an “oracle”, while the other two columns\nrefer to the time taken by NN1 and NN2 to predict the search interval\nonly. The remaining part of the Table Legend is as in Table 4. When the\nmodel and the queries are too big to ﬁt in main memory, we report a\nspace error (CPU or GPU)\nDataset-Query NN0-BFS NN1 NN2\nuni03 50% 1:31\u000210\u000071:56\u000210\u000065:16\u000210\u00006\nlogn03 50% 1:92\u000210\u000071:69\u000210\u000065:24\u000210\u00006\nreal-wl 50% 4:59\u000210\u00007Space Error Space Error\nreal-iot 50% 4:76\u000210\u000071:90\u000210\u000061:94\u000210\u00005Table 7\nPrediction Effectiveness-Polynomial Regression Models. The Table\nreports result with Quadratic (denoted Q) and Cubic (denoted C)\nregression. The timing results for IBSare analogous and omitted for\nbrevity. The Table Legend is as in Table 4.\nDataset-Query Q-BFS C-BFS\nuni03 50% 8:11\u000210\u000089:39\u000210\u00008\nlogn03 50% 1:59\u000210\u000071:54\u000210\u00007\nreal-wl 50% 2:12\u000210\u000071:80\u000210\u00007\nreal-iot 50% 1:99\u000210\u000072:57\u000210\u00007\n0,00\n5,00\n10,00\n15,00\n20,00\n25,00\n30,00\nreal-wl\nlogn04\nBBS\nBFE\nL-BFS\nIBS\nL-IBS\nFigure 6. Average per query of the number of iterations for each method\non two datasets (y axis). In both cases, the learned procedures perform\nless iterations than their standard counterparts. Y et, in the second case,\nL-BFS is worse than BFE.BBS indicates the standard Binary Search\nprocedure.\nis better than the standard one, else with FALSE. Finally, in\nTable 8, we report only the FALSE cases. Interestingly, we\nhave that L-IBS is always better than its standard counter-\npart, while there are instances in which L-BFS loses to its\nstandard counterpart. In order to get an insight into such\na behaviour, we have conducted experiments to monitor\nthe savings in terms of the number of iterations achieved\nby the “learned” procedures with respect to their standard\ncounterparts. Although a reduction in the interval size to\nsearch in yields a gain in terms of iterations, as elementary\ntheory indicates, such a gain is not enough to offset the\nadditional time spent by the earned procedures to make the\nprediction. An example is illustrated in Figure 6, where we\nhave taken a dataset in which the learned procedures “win”\nand another in which L-BFS loses. For completeness, we\nalso include Interpolation Search. In both cases, the query\ndataset is 50% of the entire table.\nTable 8\nResults of timing performance on the Benchmark Datasets of Section\n4.2.2. Only BFS FALSE results are reported. All results for IBSreturn\nTRUE.\nDataset C1\nuni04 50% FALSE\nuni04 80% FALSE\nlogn01 10% FALSE\nlogn03 10% FALSE\nlogn03 50% FALSE\nlogn03 80% FALSE\nlogn04 10% FALSE\nlogn04 50% FALSE\nlogn04 80% FALSE\n\n10\n6 A D EEPER LOOK AT THE CONVENIENCE OF\nLEARNED SORTED TABLE SEARCH PROCEDURES\nThe results of the previous section are certainly encouraging,\nyet they leave unresolved the issue of how small a reduction\nfactor one can afford in order to have learned procedures\nthat are superior to the standard ones. In regard to Binary\nSearch, due to its data-independent nature, it is possible\nto devise a simulation in which we search the breakeven\nreduction factor point, i.e., the minimum reduction factor\nfor which Learned Binary Search is better than its standard\ncounterpart, up to 99:95% . To this end, by varying the re-\nduction factor, we generate datasets as discussed in Section\n4.3, sampling from the Log-normal distribution since for\nother distributions the results are the same. In addition to\nBFE and BBS , we have also used the procedure with a B-\ntree layout (see Section 2.1). We have also included the time\nit takes for a simple parallel GPU implementation of Binary\nSearch to process a batch of queries. The motivation is as\nfollows. In their seminal paper [4], Kraska et. al. discuss the\npotential advantages of GPU use in the realm of Learned\nData Structures, in particular for training NN models. Yet, if\na GPU is available, it can also be used for Binary Search.\nTherefore, it is of interest to establish if Learned Sorted\nTable Search procedures in CPU can outperform a simple\nGPU implementation of Binary Search. On the other hand,\nit is of interest to establish whether GPU implementations\nof Learned Binary Search can be proﬁtable. The results are\nreported in Table 9, with the exception of the B-tree layout\nsince it is inferior to both BFE and BBS . The results of\nthe GPU implementation of Learned Binary Search is not\nreported since its time performance is quite close to the\nsimple GPU implementation. This is due to the fact that the\ncomputation is I/O-bound (CPU-GPU data transmission-\nsee discussion in Section 4.1).\nInterestingly, for datasets that exceed the cache hierarchy\ncapacity, i.e, datasets L4 and L5, the Learned Binary Search\nprocedure is unlikely to be of any use unless the table is\nreally easy to learn via SLR . The situation slightly improves\nfor datasets ﬁtting into the cache. Unfortunately, when a\nGPU is available, the simple-minded GPU implementation\nof Binary Search is the method of choice. That is, if the\nhardware is available for training, its availability must be\nconsidered also for parallel implementations of the algo-\nrithms at hand and accounted for in assessing to what\nextend Learned Data Structures are of value. In the case of\nBinary Search, the answer is negative.\nAs for Interpolation Search, since its performance is very\nmuch dependent on the data distribution, we must limit\nourselves to provide an indication as to whether on datasets\ngenerated via the representative distributions included in\nthis study, the reduction factor obtained via SLR is such\nto yield a better time performance for L-IBS with respect\ntoIBS. To this end, we have used the same setting as for\nBinary Search, except that for each table, we have computed\nthe exact reduction factor via SLR , rather searching for the\nbreakeven point. In analogy with Binary Search, we have\nalso investigated potential advantages of learned Interpo-\nlation Search on a GPU, with results that are analogous to\nthe case of Binary Search. It is also of interest to include, for\ncomparison, the time performance of BFE and BBS on thoseTable 9\nAn estimate of how learnable a table must be for L-BFS to be of\nadvantage. For each dataset, the column legend is as follows. The ﬁrst\ncolumn reports the time for BFE orBBS, where an asterisk denotes\nwhen the latter is better than the former. The second column report the\nbreakeven reduction factor(BRF) above which L-BFS is superior to BFE\norBBS. The third column reports the time for L-BFS at the breakeven\npoint. The ﬁnal column reports the time of a simple implementation of\nBinary Search on GPU. All times are per query in seconds. A dash\nindicates that time is not available.\nLog-normal\nBFE/BBS* L-BFS GPU\nTime (s) % BRF Time (s) Time (s)\nL1 8:46\u000210\u0000898:35 8:31\u000210\u000083:17\u000210\u00009\nL2 1:08\u000210\u0000799:25 1:07\u000210\u000073:41\u000210\u00009\nL3 1:54\u000210\u0000799:85 1:51\u000210\u000075:03\u000210\u00009\nL4 4:60\u000210\u00007>99:95 - 9:28\u000210\u00007\nL5 *1:03\u000210\u00006>99:95 - -\nTable 10\nFor each distribution, an estimate of how IBSperforms with respect to\nL-IBF . For each column, the legend is analogous to the one in Table 9\nUniform\nIBS L-IBS GPU BFE/BBS*\nTime (s) %BRF Time (s) Time (s) Time (s)\nL1 6:56\u000210\u0000898.78 5:74\u000210\u000081:81\u000210\u000098:93\u000210\u00008\nL2 7:26\u000210\u0000899.48 6:31\u000210\u000081:80\u000210\u000091:13\u000210\u00007\nL3 1:42\u000210\u0000799.92 1:49\u000210\u000071:82\u000210\u000091:57\u000210\u00007\nL4 8:81\u000210\u0000799.99 9:57\u000210\u000079:16\u000210\u000074:69\u000210\u00007\nL5 3:17\u000210\u0000699.99 2:61\u000210\u00007- *1:03\u000210\u00006\nLog-normal\nIBS L-IBS GPU BFE/BBS*\nTime (s) %BRF Time (s) Time (s) Time (s)\nL1 8:74\u000210\u0000864:42 1:13\u000210\u000071:81\u000210\u000098:46\u000210\u00008\nL2 1:20\u000210\u0000765:11 3:60\u000210\u000078:37\u000210\u000091:08\u000210\u00007\nL3 3:26\u000210\u0000765:01 6:61\u000210\u000071:82\u000210\u000091:54\u000210\u00007\nL4 2:65\u000210\u0000429:67 1:26\u000210\u000069:33\u000210\u000074:60\u000210\u00007\nL5 4:18\u000210\u0000458:25 1:49\u000210\u00004- *1:03\u000210\u00006\nSigmoid\nIBS L-IBS GPU BFE/BBS*\nTime (s) %BRF Time (s) Time (s) Time (s)\nL1 3:47\u000210\u0000782:08 1:05\u000210\u000074:35\u000210\u000098:52\u000210\u00008\nL2 5:31\u000210\u0000781:98 1:46\u000210\u000074:38\u000210\u000091:12\u000210\u00007\nL3 1:35\u000210\u0000681:96 3:32\u000210\u000074:41\u000210\u000091:43\u000210\u00007\nL4 1:82\u000210\u0000582:08 1:36\u000210\u000068:95\u000210\u000074:82\u000210\u00007\nL5 2:19\u000210\u0000582:08 4:36\u000210\u00006- *1:03\u000210\u00006\ndatasets. The results are reported in Table 10. Although\nLearned Interpolation Search is better than its standard\ncounterpart, due to the GPU or Binary Search results, such\na fact is of methodological interest only.\n7 C ONCLUSIONS\nWe have provided a systematic review of simple Sorted\nSearch procedures in the setting of Learned Data Structures.\nAlthough the idea of using Machine Learning in order to\nspeed-up Index Data Structures, for the important case of\nSorted Table Search, the results are somewhat limited, in\nparticular when a GPU is available.\nACKNOWLEDGMENTS\nMany thanks to Giorgio Vinciguerra for helpful discusions\nand comments and for being so kind to run some of the\nexperiments reported here on hardware available at his\nInstitution. We are also grateful to Umberto Ferraro Petrillo\nfor making available to us the Tarastat cluster at the Di-\nparrtimento di Scienze Statistiche, Univdersit ´a di Roma “La\nSapienza”.\n\n11\nREFERENCES\n[1] D. E. Knuth, “The Art of Computer Programming, vol. 3 (sorting\nand searching),” Addison-Wesley Publishing Company , vol. 3, pp.\n481–489, 1973.\n[2] P .-V . Khuong and P . Morin, “Array Layouts for Comparison-Based\nSearching,” J. Exp. Algorithmics , vol. 22, pp. 1.3:1–1.3:39, 2017.\n[3] J. Rao and K. A. Ross, “Cache Conscious Indexing for Decision-\nSupport in Main Memory,” in Proceedings of the 25th International\nConference on Very Large Data Bases . Morgan Kaufmann Publishers\nInc., 1999, pp. 78–89.\n[4] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The case\nfor learned index structures,” in Proceedings of the 2018 International\nConference on Management of Data . ACM, 2018, pp. 489–504.\n[5] P . Ferragina and G. Vinciguerra, “Learned Data Structures,”\ninRecent Trends in Learning From Data . Springer International\nPublishing, 2020, pp. 5–41. [Online]. Available: https://doi :org/\n10:1007/978-3-030-43883-8 2\n[6] R. O. Duda, P . E. Hart, and D. G. Stork, Pattern Classiﬁcation, 2nd\nEdition . Wiley, 2000.\n[7] D. Freedman, Statistical Models : Theory and Practice . Cambridge\nUniversity Press, August 2005.\n[8] W. W. Peterson, “Addressing for random-access storage,” IBM\nJournal of Research and Development , vol. 1, no. 2, pp. 130–146, 1957.\n[9] Liang, Ciang and Ma, Jason , “An Empirical Analysis of the\nLearned Bloom Filter and its Extensions,” https://github :com/\nJasonMa2016/CS222.\n[10] D. Comer, “Ubiquitous B-tree,” ACM Computing Surveys (CSUR) ,\nvol. 11, no. 2, pp. 121–137, 1979.\n[11] H. Prokop, “Cache-oblivious algorithms,” Master’s thesis, Mas-\nsachusetts Institute of Technology, 7 1999.\n[12] Khuong, Paul-Virak and Morin, Pat, “Memory Layouts for Binary\nSearch,” http://cglab :ca/\u0018morin/misc/arraylayout/.\n[13] K. Mehlhorn and A. Tsakalidis, “Data Structures,” in Handbook\nof Theoretical Computer Science (Vol. A): Algorithms and Complexity .\nCambridge, MA, USA: MIT Press, 1991, p. 302341.\n[14] A. Kaporis, C. Makris, S. Sioutas, A. Tsakalidis, K. Tsichlas, and\nC. Zaroliagis, “Dynamic interpolation search revisited,” 07 2006,\npp. 382–394.\n[15] D. E. Willard, “Searching unindexed and nonuniformly generated\nﬁles in loglog N time,” SIAM J Comput. Vol.14 , pp. 1013–1029, 1985.\n[16] P . Van Sandt, Y. Chronis, and J. M. Patel, “Efﬁciently Searching\nIn-Memory Sorted Arrays: Revenge of the Interpolation Search?”\ninProceedings of the 2019 International Conference on Management of\nData , ser. SIGMOD ’19. New York, NY, USA: ACM, 2019, pp.\n36–53.\n[17] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning . The\nMIT Press, 2016.\n[18] C. M. Bishop, Neural Networks for Pattern Recognition . New York,\nNY, USA: Oxford University Press, Inc., 1995.\n[19] R. Raina, A. Madhavan, and A. Ng, “Large-scale deep unsuper-\nvised learning using graphics processors,” vol. 382, 01 2009, p.\n110.\n[20] A. Galakatos, M. Markovitch, C. Binnig, R. Fonseca, and\nT. Kraska, “FITing-Tree: A Data-Aware Index Structure,” in\nProceedings of the 2019 International Conference on Management of\nData , ser. SIGMOD 19. New York, NY, USA: Association for\nComputing Machinery, 2019, p. 11891206. [Online]. Available:\nhttps://doi:org/10:1145/3299869 :3319860\n[21] M. Abadi et al. , “Tensorﬂow: Large-scale machine\nlearning on heterogeneous distributed systems,” http:\n//download :tensorﬂow:org/paper/whitepaper2015 :pdf, 2015.\n[22] “C++ Qsort routines,” http://www :cplusplus:com/reference/\ncstdlib/qsort/.",
  "textLength": 55104
}