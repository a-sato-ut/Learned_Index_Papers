{
  "paperId": "d42904a39aed848eb6f7303675a0dbc5b3f02ee4",
  "title": "A Power and Area Efficient Lepton Hardware Encoder with Hash-based Memory Optimization",
  "pdfPath": "d42904a39aed848eb6f7303675a0dbc5b3f02ee4.pdf",
  "text": "> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n 1\nAbstract—Although it has been surpassed by many \nsubsequent coding standards, JPEG occupies a large share \nof the storage load of the current data hosting ser vice. To \nreduce the storage costs, DropBox proposed a lossle ss \nsecondary compression algorithm, Lepton, to further  \nimprove the compression rate of JPEG images. Howeve r, \nthe bloated probability models defined by Lepton se verely \nrestrict its throughput and energy efficiency. To s olve this \nproblem, we construct an efficient access probabili ty-based \nhash function for the probability models, and then propose \na hardware-friendly memory optimization method by \ncombining the proposed hash function and the N-way Set-\nAssociative unit. After that, we design a highly \nparameterized hardware structure for the probabilit y \nmodels and finally implement a power and area effic ient \nLepton hardware encoder. To the best of our knowled ge, \nthis is the first hardware implementation of Lepton . The \nsynthesis result shows that the proposed hardware \nstructure reduces the total area of the probability  models by \n70.97%. Compared with DropBox’s software solution, the \nthroughput and the energy efficiency of the propose d \nLepton hardware encoder are increased by 55.25 and 4899 \ntimes respectively. In terms of manufacturing cost,  the \nproposed Lepton hardware encoder is also significan tly \nlower than the general-purpose CPU used by DropBox.  \n \nIndex Terms —Hardware encoder, Hash function, Image \ncompression, JPEG, Lepton, N-way Set-Associative un it \n \nI.  INTRODUCTION  \nith the explosive growth of Internet applications, storage \noverhead has increasingly become the main cost of c loud \nstorage, social networks, and e-commerce platforms.  Although \nthere are already a large number of new image compr ession \nstandards, such as JPEG2000 [1], BPG [2], and WebP [3], \nwhich have proven to be far superior to JPEG's comp ression \n \nThis work was supported in part by Alibaba Innovati ve Research (AIR) \nProgram, in part by the Shanghai Science and Techno logy Committee (STCSM) \nunder Grant 19511104300, in part by National Natura l Science Foundation of \nChina under Grant 61674041, in part by the Innovati on Program of Shanghai \nMunicipal Education Commission, in part by the Fuda n University-CIOMP \nJoint Fund (FC2019-001). (Corresponding author: Yib o Fan) \nXiao Yan, Yibo Fan, Bowen Huang, Minjiang Li, and X iaoyang Zeng are \nwith the State Key Laboratory of ASIC and System, F udan University, performance, JPEG [4] is still widely adopted due t o simplicity \nand royalty-free. In terms of cloud storage service s, according \nto Dropbox’s statistics, JPEG images account for \napproximately 35% of the amount of data stored by i ndividual \nusers [5]. For e-commerce platforms like Alibaba, t he \nproportion of JPEG images in the entire data storag e is even \nhigher. To store images more efficiently, DropBox p roposed a \nlossless encoding algorithm, Lepton, to perform a s econdary \ncompression on JPEG images. Diversified test result s show that \nLepton can compress JPEG images by nearly 23% [5].  \nThe transformation, storage, and recovery processes  of the \nimage files are shown in Fig. 1. The operation on t he data center \nside is mainly composed of JPEG decoding and Lepton  \nencoding. The data center uses a standard JPEG deco der to \ndecode the JPEG file into 8x8 quantized coefficient  blocks and \nthen uses the Lepton encoding algorithm to re-encod e these \nquantized coefficients into a more compact Lepton f ile. The \nbackend servers only need to save these Lepton file s, which \nsignificantly reduces the storage cost. When the us er requests \nShanghai 200433, China (e-mail: {19112020084, fanyi bo, 19212020120, \n17212020017, xyzeng}@fudan.edu.cn;)  \nZhixiong Di is with School of Information Science a nd Technology, \nSouthwest Jiaotong University, Chengdu, China (e-ma il: \ndizhixiong2@126.com) \nWenqiang Wang is with Alibaba Group, Hangzhou, Zhej iang 311121, China \n(e-mail: channing.wwq@alibaba-inc.com) Xiao Yan, Zhixiong Di, Bowen Huang, Minjiang Li, We nqiang Wang, Xiaoyang Zeng, Yibo Fan A Power and Area Efficient Lepton Hardware \nEncoder with Hash-based Memory Optimization \nW \nFig. 1. Lepton-based efficient image storage soluti on . If the process of \nrestoring Lepton files to JPEG files is done on the  terminal, it can further save \n23% of the transmission cost.  \n  \n\n\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n 2\nimage data, DropBox decodes the Lepton files into q uantized \nDCT coefficients and then encodes these coefficient s into the \noriginal JPEG file. The task of restoring the Lepto n file to a \nJPEG image can be executed either on the server-sid e or by the \nterminal. Performing restoration on the terminal si de can further \nsave transmission bandwidth.  \nTo obtain significant compression gains, Lepton emp loys the \nbinary arithmetic coding algorithm and greatly expa nds the \nprobability models, resulting in a huge cost in sto rage and \ncalculation. Binary arithmetic coding is a high-eff iciency \nentropy coding algorithm, which needs to estimate t he \nprobability distribution of the input data [6]. Lep ton defines a \nbunch of huge probability models, containing a tota l of 721564 \nbins to track the spatial correlation of the image.  During the \narithmetic coding, the probability models are used to record the \nhistory data and estimate the probability of 0 and 1 in the next \nbit.   \nIn DropBox’s storage system, Lepton is executed by the file \nservers or a dedicated server cluster, sharing prod uction hosts \nwith other memory-hungry processes. Therefore, the memory \nconsumption caused by the probability models severe ly restricts \nthe execution efficiency of Lepton. According to Ho rn et al. [5], \nan Intel Xeon E5 2650 v2 at 2.6GHz can only process  5.75 \npictures per second and one kWh can be traded for a n average \nof 72,300 Lepton conversions of images sized at an average of \n1.5 MB each. It can be seen that the limited throug hput and \nenergy efficiency are the main bottleneck of Lepton  application. \nIn this work, we propose an optimized Lepton hardwa re \nencoder to significantly increase the throughput of  Lepton \nencoding while greatly reducing its energy consumpt ion. \nSimilar to the software solution, the huge probabil ity models \nalso pose severe challenges to the hardware encoder . Therefore, \nthe memory optimization method of the probability m odels \nbecomes a key issue. Although the hierarchical memo ry \narchitecture is usually used in hardware to impleme nt high-\nefficient memory access [7], designing a dedicated hierarchical \nmemory architecture for the Lepton hardware encoder  will \ncause an unacceptable area overhead. For software \nimplementations, hash mapping is usually used to ma p a large \nrange of indexes to a compact physical memory space  [8] [9] \n[10], but it is usually difficult to construct a ne arly collision-\nfree and hardware-friendly hash function. By observ ing the \nLepton coding process, we find that there are signi ficant \ndifferences between the access probabilities of dif ferent bins in \nthe probability models. Based on the statistical ch aracteristics \nof the probability models accessing, we propose a m emory \noptimization method for the probability models by c ombining \nhash mapping and N-way Set-Associative unit, and th en design \na high throughput, high energy efficiency, and low- cost Lepton \nhardware encoder.   \nThe main contributions of this work are: 1. we prop ose an \nefficient hash function for Lepton’s probability mo dels.  Kraska \net al. [11] pointed out that for read-only database s, the \ncumulative distribution function (CDF) can be used as a hash \nfunction for data index. Inspired by this, this wor k further \nconstructs a high-efficient hash function for write -heavy sparse \ndata access. 2. By combining the hash function with  the N-way Set-Associative Unit, we propose a hardware-friendl y memory \noptimization method and reduce the area of the prob ability \nmodels by 70.97%. We argue that the proposed memory  \noptimization method is also meaningful for software  solutions \nand can benefit a large number of sparse memory acc ess in \nspecific domains. 3. Based on the memory optimizati on method, \nwe design a low-cost Lepton hardware encoder, which  greatly \nimproves the throughput and energy efficiency of Le pton \ncoding. Compared with DropBox’s software solution, the \nthroughput of the Lepton hardware encoder is increa sed by \nmore than 55.25 times, and the power consumption is  reduced \nby 4899 times. Besides, the cost of Lepton hardware  encoder is \nmuch lower than the general-purpose CPU used by Dro pBox. \nThe rest of this paper is organized as follows. Sec tion II \nintroduces the basics of Lepton coding and Section III reviews \nthe related works of memory management. The propose d hash \nfunction and memory optimization method is describe d in detail \nin Section IV. Section V introduces the VLSI archit ecture of the \nLepton hardware encoder. Experiments and results ar e \npresented in Section VI. Finally, conclusions are d rawn in \nSection VII.   \nII.  BASICS OF LEPTON CODING  \nA.  The main steps of Lepton coding \nAs shown in Fig. 2, the 8x8 quantized coefficient b lock \nobtained by JPEG decoding is divided into DC, x edg e, y edge, \nand 7x7 AC regions, and then Lepton losslessly re-e ncodes \nthese coefficients through binarization and binary arithmetic \ncoding to obtain a higher compression rate. \nThe syntax elements and coding order of Lepton are shown in \nTable I. The 8x8 blocks are coded sequentially in t he raster \norder. Lepton predicts the DC coefficient based on the neighbor \nblocks and then encodes its residual value. For x e dge, y edge, \nand 7x7 AC regions, Lepton starts from the first co efficient and \nencodes to the last non-zero coefficient. Once ther e is no non-\nzero coefficient in a region, Lepton will skip the coefficient \nencoding process. \nBefore binary arithmetic coding, the data needs to be \nbinarized. For the number of non-zero coefficients,  Lepton \ndirectly uses its fixed-length binary code as the r esult of \nbinarization. For the coefficient data, Lepton conv erts it into \nexponent, sign, and residual by using the Exp-Golom b code[12].  \nB.  Binary arithmetic coding and probability models \nThe crucial improvement of Lepton is to replace the  Huffman  \nFig. 2.  Regions of 8x8 quantized coefficient block. Lepton encodes the \nquantized coefficients sequentially according to th eir region, and the number in \nthis figure is the coding order within each region.  \n \n\n\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n 3\ncoding in JPEG with the binary arithmetic coding. T he binary \narithmetic coding is the most widely used entropy c oding \nmethod in image compression, its coding performance  depends \nlargely on the accuracy of the probability model. T he \nprobability model consists of a large number of bin s, and each \nbin records the corresponding historical informatio n in the \nencoding process. We take the exponent of the coeff icients in \n7x7 AC region, namely exp_7x7_data, as an example t o \nillustrate Lepton’s probability models. All bits of  exp_7x7_data \nuse the same probability model index \n/g1835/g1800/g1850/g185ℎ/g18ℎ0 /g3)12/g3)15/g3)30/g3)31 = {/g1858/g1801/g1853/g1859 /g3)3), /g1808/g18ℎ)/g1801/g180ℎ/g18ℎ), /g1800/g18ℎ3/g1805_/g1800/g18ℎ8_/g1801/g185ℎ/g1858/g18ℎ2, /g1801/g1850/g18ℎ0/g3)53/g3)30/g3)31/g3)53/g3)28/g3)31 }.    (1) \nwhere /g1858/g1801/g1853/g1859 /g3)3) is the flag of the color component, prior  is \ncalculated from the reference blocks, /g1800/g18ℎ3/g1805_/g1800/g18ℎ8_/g1801/g185ℎ/g1858/g18ℎ2  is \ndetermined by the number of the left non-zero coeff icients in \nthe 7x7 AC region, /g1801/g1850/g18ℎ0/g3)53/g3)30/g3)31/g3)53/g3)28/g3)31  is the zigzag index of the current \ncoefficient. Table II shows the relationship betwee n the \nprobability models and each bit when exp_7x7_data i s 0x3ff, \n0x3e, and 0x6. The most significant bit of exp_7x7_ data \ncorresponds to probability model exp_7x7_0, and the  \nsubsequent bits are arranged in sequence. It can be  seen that \nexp_7x7_10 will only be used when exp_7x7_data is l arge \nenough while exp_7x7_0 is always used. Denote the b it width \nof /g1835/g1800/g1850/g185ℎ/g18ℎ0 /g3)12/g3)15/g3)30/g3)31  as /g1851/g18ℎ5 , then directly using  /g1835/g1800/g1850/g185ℎ/g18ℎ0 /g3)12/g3)15/g3)30/g3)31  will require \neach probability model to have 2/g3)29/g3)5)  bins. Since each field of \n/g1835/g1800/g1850/g185ℎ/g18ℎ0 /g3)12/g3)15/g3)30/g3)31  has a limited range, we use  \n/g1835/g1800/g1850/g185ℎ/g18ℎ0 /g3)11/g3)32/g3)5) =  ∑ (/g1832/g1801/g1801/g185ℎ/g1850 /g3)30∗∏ /g1811/g1853/g1800/g1859/g185ℎ /g3)3ℎ/g3)11\n/g3)3ℎ/g288)/g3)30/g28ℎ8/g2809 )/g3)11\n/g3)30/g288)/g2808            (2) \nwhere /g1832/g1801/g1801/g185ℎ/g1850 /g3)30 is the value of the i-th field of the /g1835/g1800/g1850/g185ℎ/g18ℎ0 /g3)12/g3)15/g3)30/g3)31 , \n/g1811/g1853/g1800/g1859/g185ℎ /g3)3ℎ is the number of valid values for the j-th field, to make the probability models more compact.  \nTo obtain significant compression gains, Lepton use s a bunch \nof probability models to independently record the p robability \ndistribution for each bit of each syntax element. T he  probability \nmodels defined by Lepton [13] are shown in Table II I. After the \nconversion of (2), Lepton's probability models cont ain 685034 \nbins and require a total of more than 11M bits memo ries. It will \nbe an unacceptable cost for hardware design. \nTo solve this problem, we establish an efficient ha sh function \nand combine the N-way Set-Associative unit to propo se a \nhardware-friendly memory optimization method. After  that, we \ndesign a highly reusable structure for the probabil ity models \nand finally realize a high-performance Lepton hardw are \nencoder with low cost and low power consumption. \nIII.  Related Works \nA.  Hierarchical memory architecture \nIn hardware design, memory architecture is often a key factor \nin circuit performance, cost, and power consumption  [14] [15] \n[16]. Hierarchy memory architecture is commonly use d to \noptimize storage costs and access latency [17]. In hierarchy \nmemory architecture, the most frequently used data is placed in \na smaller, high-cost, faster cache, and only when t he cache \naddressing fails, that is, cache misses, will the l arge-capacity, \nlow-cost, slower memory be accessed. In this way, a  cheap \nlarge-capacity storage structure that exhibits low latency in \nmost cases is realized. \nThe cache is generally composed of N-way Set-Associ ative \nunits. If a piece of data could be placed in a set consisting of a \nlimited number of locations in the cache, the cache  is called Set-\nAssociative. If the set contains N memory spaces, i t is called an \nN-way Set-Associative unit [18]. The N-way Set-Asso ciative \nunit improves memory utilization by sharing physica l memories \nbetween input indexes. An example of N-way Set-Asso ciative \nunits is shown in Fig. 3.  \nSince Lepton’s probability model access does not sh ow a \ntemporal or spatial correlation that crucial to the  hierarchy \nmemory architecture, the resulting cache misses wil l severely \ndegrade the throughput of the Lepton hardware encod er. \nBesides, designing a hierarchical memory architectu re for the \nLepton hardware encoder also significantly increase s its costs. TABLE  I \nTHE SYNTAX ELEMENTS AND CODING ORDER OF LEPTON  \nCoding \nOrder  Syntax Elements Description \n1 num_nonzero_7x7 number of non-zero coefficients i n \n7x7 AC region  \n2 7x7 AC coefficients from the first to the last no n-zero \ncoefficient in 7x7 AC region  \n3 num_nonzero_x_edge  number of non-zero coefficients in x \nedge region  \n4 x edge coefficients from the first to the last no n-zero \ncoefficient in x edge region  \n5 num_nonzero_y_edge  number of non-zero coefficients in y \nedge region  \n6 y edge coefficients from the first to the last no n-zero \ncoefficient in y edge region  \n7 DC residual  the residual value of DC  \n  \nTABLE  II \nCORRESPONDENCE BETWEEN EXP _7 X7_ DATA AND ITS PROBABILITY MODELS  \nprobability \nmodel s The value of exp_7x7_data  \n0x3ff  0x3 e 0x6  \nexp_7x7_0  1 1 1 \nexp_7x7_1  1 1 1 \nexp_7x7_2  1 1 0 \nexp_7x7_3  1 1 NA  \nexp_7x7_4  1 1 NA  \nexp_7x7_5  1 0 NA  \nexp_7x7_6  1 NA  NA  \nexp_7x7_7  1 NA  NA  \nexp_7x7_8  1 NA  NA  \nexp_7x7_9  1 NA  NA  \nexp_7x7_ 10  1 NA  NA  \n Fig. 3.  Structure of typical N-way Set-Associative  units . Suppose we need to \nmap the input index to 8 physical addresses. (a) sh ows the case of two 4- way \nSet-Associative units. For the input index, we firs t determine it s corresponding \n4-ways Set-Associative unit, and then the index can  use any memory space in \nthis unit. Similarly, (b) shows the case of four 2- ways Set-Associative units. \n  \n\n\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n 4\nB.  Hash mapping \nIn software, sparse memory access is generally impl emented \nthrough hash mapping. Hash mapping maps the sparse access \nindex to a compact physical memory space to reduce the \nrequired storage capacity. In hash mapping, the ind ex is called \nKey, the memory address is called Value, and the Ke y-Value \ncorrespondence is determined by the hash function. Hash \nfunction has been widely used in data encryption an d data \nmanagement for a long time [19] [20] [21]. When dif ferent keys \nare mapped to the same hash index, a hash collision  occurs. In \npractice, solutions such as linked lists and hash r emapping are \nused to deal with hash collision [22] [23] [24]. Th e state-of-the-\nart hash mapping is the learned index structure [11 ], which is \ncommitted to using neural networks to learn a cumul ative \ndistribution function-based hash function for large -scale read-\nonly databases. Fig. 4 shows an example of an ideal  hash \nmapping for the read-only database. For read-only d atabases, it \nis clear whether an index exists, so the probabilit y distribution \nfunction of the index, P(index) , and the hash function, H(index) , \ncan be represented as \n/g1812(/g1801/g1800/g1850/g185ℎ/g18ℎ0 /g3)30)= /g312)1,    /g1801/g1858  /g1801/g1800/g1850/g185ℎ/g18ℎ0 /g3)30 /g1801/g18ℎ1 /g185ℎ/g18ℎ0/g1801/g185ℎ/g18ℎ1/g18ℎ2 \n0,    /g180ℎ/g18ℎ2ℎ/g185ℎ/g18ℎ)/g18ℎ5/g1801/g18ℎ1/g185ℎ                   .            (3) /g1831(/g1801/g1800/g1850/g185ℎ/g18ℎ0 /g3)30)=  ∑ /g1812/g3135/g1801/g1800/g1850/g185ℎ/g18ℎ0 /g3)3ℎ/g3139                  /g3)30\n/g3)3ℎ/g288)/g2808 .            (4) \nFrom a statistical point of view, H(index)  is the cumulative \ndistribution function (CDF) of the index.  \nUnlike the situation in Kraska et al. [11], the pro bability \nmodel of the Lepton encoder is a write-heavy databa se, and we \nmust determine the hash function before the binary arithmetic \ncoding starts. Although Ding et al. [25], Hadian et  al. [26] \nproposed several updatable learning-based index str uctures to \nimprove the access efficiency of large-scale writea ble databases, \nthese methods are difficult to be used in hardware design due to \nthe need for real-time expansion of data nodes and retraining of \nneural network models. \nIV.  MEMORY OPTIMIZATION FOR PROBABILITY MODELS  \nIn this section, we first analyze the statistical c haracteristics \nof probability model access, then introduce an effi cient access \nprobability-based hash function for Lepton’s probab ility \nmodels, and finally propose a hardware-friendly mem ory \noptimization method for the Lepton hardware encoder .  \nA.  Statistical characteristics of probability model access \nOnce a bin of the probability model is used when en coding \nan image, we must allocate memory space for it and maintain \nthe memory space until the encoding is completed. T he \nutilization rate of a probability model is defined as the ratio of \nthe number of the used bins to the total number of bins. The test \ndata shows that the utilization rate of the huge pr obability \nmodels declared by Lepton coding algorithm is very low. In \naddition, there are huge differences between the ac cess \nprobabilities of different bins in the same probabi lity model. \nFor example, for the index of exp_7x7_data, the pro bability of \n{/g1800/g18ℎ3/g1805_/g1800/g18ℎ8_/g1801/g185ℎ/g1858/g18ℎ2, /g1801/g1850/g18ℎ0/g3)53/g3)30/g3)31/g3)53/g3)28/g3)31 } = {9,17} is very small, since it \nrequires all high-frequency coefficients in current  8x8 block to \nbe non-zero. Similarly, when /g1801/g1850/g18ℎ0/g3)53/g3)30/g3)31/g3)53/g3)28/g3)31  = 63, /g1800/g18ℎ3/g1805_/g1800/g18ℎ8_/g1801/g185ℎ/g1858/g18ℎ2  \nwill never be greater than 1. Therefore, we try to provide more \nmemory space for bins with higher access probabilit y, share \nmemory space between bins with lower access probabi lity, and Fig. 4.  Example of an ideal hash mapping. (a) show s an ideal read- only \ndatabase. The data is accessed through the index, a nd Pos re presents the \nposition where the data is stored. Ideally, the data is stored intensively, causing \nthe indexes to be discontinuous. Hash mapping is us ed to generate the physical \nstorage position based on access index. (b) shows t he ideal hash function. It ca n \nbe seen that the position only increases at the exi sting index since the non-\nexistent index does not occupy any memory space. \n \nTABLE  III   \nTHE  PROBABILITY MODELS DEFINED BY LEPTON  \nprobability \nmodel  Number of \nbins  probability \nmodel  Number of \nbins  probability \nmodel  Number of \nbins  probability \nmodel  Number of \nbins  \nexp_7x7_0  10780  exp_edge_0  2156  exp_dc_0  204  res_thres _0  4096  \nexp_7x7_1  10780  exp_edge_1  2156  exp_dc_1  204  res_thres _1  8192  \nexp_7x7_2  10780  exp_edge_2  2156  exp_dc_2  204  res_thres _2  16384  \nexp_7x7_3  10780  exp_edge_3  2156  exp_dc_3  204  res_thres _3  32768  \nexp_7x7_4  10780  exp_edge_4  2156  exp_dc_4  204  res_thres _4  65536  \nexp_7x7_5  10780  exp_edge_5  2156  exp_dc_5  204  res_thres _5  131072  \nexp_7x7_6  10780  exp_edge_6  2156  exp_dc_6  204  res_thres _6  262144  \nexp_7x7_7  10780  exp_edge_7  2156  exp_dc_7  204  res_thres _7  4096  \nexp_7x7_8  10780  exp_edge_8  2156  exp_dc_8  204  res_edge_2  196  \nexp_7x7_9  10780  exp_edge_9  2156  exp_dc_9  204  res_edge_1  196  \nexp_7x7_10  10780  exp_edge_10  2156  exp_dc_10  204  res_edge_0  196  \nres_7x7_0  1260  res_7x7_1  1260  res_7x7_2  1260  res_7x7_3  1260  \nres_7x7_4  1260  res_7x7_5  1260  res_7x7_6  1260  res_7x7_7  1260  \nres_7x7_8  1260  res_7x7_9  1260  res_dc_0  12  res_dc_1  12  \nres_dc_2  12  res_dc_3  12  res_dc_4  12  res_dc_5  12  \nres_dc_6  12  res_dc_7  12  res_dc_8  12  res_dc_9  12  \nnz_7x7_0  500  nz_7x7_1  260  nz_7x7_2  140  nz_7x7_3  80  \nnz_7x7_4  40  nz_7x7_5  20  nz_edgex_0  512  nz_edgex_1  256  \nnz_edgex_2  128  nz_edgey_0  512  nz_edgey_1  256  nz_edgey_2  128  \nsign  66        \n \n\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n 5\ndon’t allocate memory space for bins that cannot be  accessed, \nthereby greatly reducing the cost of the lepton har dware \nencoder. \nB.  The access probability-based hash function  \nInspired by Kraska et al. [11], this work attempts to develop \nan efficient access probability-based hash function  for Lepton. \nSince Lepton's probability model is a write-heavy d atabase, \nwhich requires the hash function to be determined b efore the \ndata access starts, we cannot establish the corresp onding \ncumulative distribution function based on the actua l data \ndistribution. To solve this problem, this work firs t establishes \nthe probability distribution function of the access  index, \n/g1812(/g1801/g1800/g1850/g185ℎ/g18ℎ0 /g3)30), for each probability model by analyzing the Lepto n \nencoding process of a large number of images, then obtains the \ncumulative distribution function,  /g1829/g183)/g1832 (/g1801/g1800/g1850/g185ℎ/g18ℎ0 /g3)30), through partial \nsummation, and finally constructs a similar CDF-bas ed hash \nfunction \n/g1831(/g1801/g1800/g1850/g185ℎ/g18ℎ0 /g3)30)=  /g1829/g183)/g1832 (/g1801/g1800/g1850/g185ℎ/g18ℎ0 /g3)30)∗ /g1839/g185ℎ/g1805_/g1850/g185ℎ/g1808/g18ℎ2ℎ .            (5) \nwhere Mem_depth  is the depth of the physical memory budget. \nAccording to the essence of hash mapping and cumula tive \ndistribution function, it can be concluded that the  memory space \noccupied by /g1801/g1800/g1850/g185ℎ/g18ℎ0 /g3)30 is \n/g1815/g1808/g1853/g1855/g185ℎ (/g1801/g1800/g1850/g185ℎ/g18ℎ0 /g3)30)= /g1831 (/g1801/g1800/g1850/g185ℎ/g18ℎ0 /g3)30)− /g1831 (/g1801/g1800/g1850/g185ℎ/g18ℎ0 /g3)30/g28ℎ9/g2809) \n                                   = /g1812 (/g1801/g1800/g1850/g185ℎ/g18ℎ0 /g3)30)∗ /g1839/g185ℎ/g1805_/g1850/g185ℎ/g1808/g18ℎ2ℎ          (6) \nEquation (6) shows that the core idea of /g1831(/g1801/g1800/g1850/g185ℎ/g18ℎ0 /g3)30) is to use \nthe access probability as a weight to allocate memo ry for each \nbin. For bins with a small access probability, the memory space \nallocated to them will be a decimal, which means th at they share \nphysical memory space with adjacent bins. Since the  \nprobability model must be initialized before encodi ng every \nsingle picture, the validity period of the probabil ity model is \nlimited. In this case, we believe that the probabil ity of hash \ncollision caused by memory sharing between bins wit h a small \naccess probability is also limited. This hash funct ion concisely \nachieves the purpose of compressing the required ph ysical \nstorage space with as few hash collisions as possib le. \nIt should be noted that the weight of memory alloca tion \nshown in (6) needs simple correction. In practice, one single bin \nof the probability model only needs one memory spac e at most, \nso the maximum weight should not be greater than \n1/Mem_depth . The final memory allocation weight function \n/g1819(/g1801/g1800/g1850/g185ℎ/g18ℎ0 ) and hash function /g1831(/g1801/g1800/g1850/g185ℎ/g18ℎ0 ) are obtained by the \npseudo-code shown in Fig. 5. To ensure the normaliz ation of \nthe allocation weights, the clip operation is perfo rmed \niteratively. Since the hash function only needs to be established \nonce, the iterative operation does not introduce co mputational \ncosts in the Lepton coding. Another advantage of ou r method is \nthat since the ∑/g1819(/g1801/g1800/g1850/g185ℎ/g18ℎ0)  will be less than 1 after the clip \noperation, the renormalization will adaptively ampl ify the \nallocation weight for the bins with lower access pr obability. \nTherefore, the resulting hash function elegantly re moves the \ndummy bins, and always efficiently allocate memory resource \nfor active bins according to the access probability  and the \nphysical memory budget. When the physical memory bu dget is \ngreater than or equal to the number of active index es, the \ngenerated hash function can theoretically guarantee  that no hash collision occurs. Since the memory access of most a pplications \nhas distinct statistical characteristics, we argue that other sparse \nmemory accesses can also benefit from this hash fun ction. \nC.  The hardware-friendly memory optimization method \nAfter establishing the hash function, we need to ex press it in \nthe application. These functions are often difficul t to express as \nsimple analytical forms. In fact, the main purpose of the learned \nindex [11] [25] [26] is to fit the CDF by using the  neural \nnetwork. To utilize the proposed hash function in h ardware \ndesign, we further propose a hardware-friendly memo ry \noptimization method based on the N-way Set-Associat ive units. \nThe proposed method divides the index range into M intervals \naccording to the access probability-based hash func tion, and \nthen stores the M-1 boundary indexes. When the prob ability \nmodel  is accessed, we first identify the interval in whi ch the \nindex is located through parallel comparison and th en \ndetermine the corresponding N-way Set-Associative u nit. The \nindexes in one interval share the N physical memory  spaces in \nan N-way Set-Associative unit. Obviously,  \n/g1839 =  /g1839/g185ℎ/g1805_/g1850/g185ℎ/g1808/g18ℎ2ℎ//g181) .                         (7) \nThe N-way Set-Associative unit sequentially allocat es \nmemory for the input index and records the allocati on status at \nthe same time. For the input index, the N-way Set-A ssociative Fig. 6.  Example of the proposed hardware-friendly memory optimization \nmethod. (a) shows the access probability-based hash  function of the probability \nmodel exp_7x7_0. The 10780 indexes of exp_7x7_0 are  mapped to 160 \nphysical addresses by the hash function, and then t he entire index range is \ndivided into five intervals according to the physic al address, each interval \nshares 32 memories (M=5, N=32, Mem_depth=160). (b ) shows the access of \nthe probability model in the Lepton hardware encode r. When the probability \nmodel exp_7x7_0 is accessed, the hardware encoder d etermines the interval in \nwhich the index is located through parallel comparison, and then enabl es the \ncorresponding 32-ways Set-Associative unit to complete the read and write \noperations.  \n \n1 /g1819(/g1801/g1800/g1850/g185ℎ/g18ℎ0 )= /g1812(/g1801/g1800/g1850/g185ℎ/g18ℎ0 ) \n2 /g1819ℎ/g1801/g1801/g185ℎ (/g1839/g1853/g18ℎ0 (/g1819(/g1801/g1800/g1850/g185ℎ/g18ℎ0 ))>1//g1839/g185ℎ/g1805 _/g1850/g185ℎ/g1808/g18ℎ2 ℎ): \n3     #clip  operation  \n4     Foreach /g1801/g1800/g1850/g185ℎ/g18ℎ0 /g3)30: \n5         if(  /g1819(/g1801/g1800/g1850/g185ℎ/g18ℎ0 /g3)30)>1//g1839/g185ℎ/g1805 _/g1850/g185ℎ/g1808/g18ℎ2 ℎ):  \n6            /g1819(/g1801/g1800/g1850/g185ℎ/g18ℎ0 /g3)30) = 1//g1839/g185ℎ/g1805 _/g1850/g185ℎ/g1808/g18ℎ2 ℎ \n7     #renormalization  \n8     Foreach /g1801/g1800/g1850/g185ℎ/g18ℎ0 /g3)30: \n9         /g1819/g3)18/g3)13/g3)31/g3)28/g3)1ℎ/g3)32 (/g1801/g1800/g1850/g185ℎ/g18ℎ0 /g3)30) =  /g1819(/g1801/g1800/g1850/g185ℎ/g18ℎ0 /g3)30)/∑/g1819(/g1801/g1800/g1850/g185ℎ/g18ℎ0 ) \n10     /g1819(/g1801/g1800/g1850/g185ℎ/g18ℎ0 )= /g1819/g3)18/g3)13/g3)31/g3)28/g3)1ℎ/g3)32 (/g1801/g1800/g1850/g185ℎ/g18ℎ0 ) \n11 Foreach /g1801/g1800/g1850/g185ℎ/g18ℎ0 /g3)30: \n12     /g1829/g183)/g1832 (/g1801/g1800/g1850/g185ℎ/g18ℎ0 /g3)30)= /g3533 /g1819/g3135/g1801/g1800/g1850/g185ℎ/g18ℎ0 /g3)3ℎ/g3139/g3)30\n/g3)3ℎ/g288)/g2808 \n13     /g1831(/g1801/g1800/g1850/g185ℎ/g18ℎ0 /g3)30) = /g1829/g183)/g1832 (/g1801/g1800/g1850/g185ℎ/g18ℎ0 /g3)30)∗/g1839/g185ℎ/g1805 _/g1850/g185ℎ/g1808/g18ℎ2 ℎ \nFig. 5. the pseudo code for establishing the memory  allocation weight function \n/g1819(/g1801/g1800/g1850/g185ℎ/g18ℎ0 ) and hash function /g1831(/g1801/g1800/g1850/g185ℎ/g18ℎ0 ) according to the access probability \ndistribution function /g1812(/g1801/g1800/g1850/g185ℎ/g18ℎ0 ). \n \n \n\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n 6\nunit first queries whether it already has a memory space, if so, \nthe Set-Associative unit directly gets the memory a ddress, \notherwise, it allocates memory for this index. When  \nM=Mem_depth, this scheme is completely equivalent t o the \nproposed hash function. When M=1, this scheme is co mpletely \nequivalent to the fully-associative scheme [7]. The  memory \nutilization and the complexity of Set-Associative u nit will both \nincrease as M decreases. If more than N different i ndexes are \naccessed in any index interval during encoding an i mage, the \nprobability model overflow occurs. Fig. 6 is an exa mple of the \nproposed memory optimization method. \nCompared with using the hash function alone, the pr oposed \nmemory optimization method  has the following advantages: 1. \nIt only needs to store M-1 boundary indexes to real ize the \npurpose of allocating memory resources according to  the access \nprobability, avoiding fitting the complex hash func tions. 2. \nSince different indexes share N physical memories, this method \nhas a higher memory utilization. 3. This method is meaningful \nfor both software and hardware design. For the soft ware design, \nwe can use B-trees to store and index these boundar y indexes \nand use linked lists to implement N-way Set-Associa tive units, \nwhich makes it possible to implement the access pro bability \nbased-hash function through a simple traditional in dex structure. \nFor the hardware designs, this method transforms th e complex \nfunction fitting problem into a series of hardware- friendly \nparallel comparison operations, which greatly reduc es the \ndesign complexity and implementation cost.  \nD.  probability model overflow solution \nAlthough the proposed memory optimization method ca n \nensure that no probability model overflow occurs wh en the \nmemory capacity is greater than the number of activ e indexes, \nroughly expanding the memory capacity may not be th e optimal \nchoice. When the overflow rate is low, the marginal  benefit of \nexpanding the memory capacity will gradually decrea se. To \nfurther reduce the cost of the Lepton hardware enco der without \nsignificantly degrading the processing performance,  this work \nadopts a combination of software and hardware desig ns. When \nthe probability model overflow occurs, the Lepton h ardware \nencoder generates an interrupt signal and records t he overflow \nindex, and then the interrupt service program will call the \nsoftware lepton encoder to re-encode the current im age. As long as the overflow rate is low enough, the impact of s uch \noperations on the overall throughput is negligible.  In addition, \nthe interrupt service program can also analyze the overflow \nindexes, refresh the boundary indexes according to the updated \naccess probability-based hash function, and adaptiv ely reduce \nthe overflow rate.  \nV.  VLSI  ARCHITECTURE \nA.  Lepton hardware encoder \nThe structure of the Lepton hardware encoder is sho wn in Fig. \n7. The JPEG images are first decoded into quantized  DCT \ncoefficients and quantization table data. The Line Buffer \nmodule saves the quantized coefficients and outputs  the current \nblock, the left reference block, and the upper left  reference \nblock to the subsequent module. The preprocess part  calculates \nthe number of non-zero coefficients in the 7x7 AC, x edge, and \ny edge regions of the current block, the residual v alue of the DC \ncoefficient, and serially outputs the coefficient d ata that needs \nto be coded in the 7x7 AC, x edge and y edge region s. The \nbinarization and index calculation module converts the \ncoefficient data into exponent, sign, and residual by using the \nExp-Golomb code, and generates the probability mode l index \nfor each bit. The probability model reads and updat es the \ncorresponding bin according to the input index, cal culates the \nprobability value. The probability data p2s module receives the \ncoding bits and the probability values from the pro bability \nmodels in parallel and outputs them to the 4-bits p arallel \narithmetic encoder in the order specified by Lepton  to generate \nthe final Lepton code stream. \nThe probability model part integrates all the proba bility \nmodels defined in the Lepton coding algorithm, whic h results \nin the main cost of the Lepton hardware encoder. Fi g. 7 only \nshows the schematic diagram of the probability mode ls \naccording to the data category. In fact, the hardwa re encoder \ncontains 77 independent instances of probability mo dels, \ncorresponding to different memory capacities. There fore, the \ncost, performance, and reusability of the probabili ty model \nhardware structure are carefully considered. \nB.  probability models \nAs shown in Fig. 8, the probability model is mainly  Fig. 7.  The structure of Lepton hardware encoder. \n \n\n\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n 7\ncomposed of enable generator, N-way Set-Associative  unit \narray, and probability model controller. The enable  generator \ncompares the input index with the M-1 boundary inde xes and \nthen generates the enable signal for the correspond ing N-way \nSet-Associative unit through simple logical operati ons. The i-th \nN-way Set-Associative unit includes N 1-bit initial ization flags \nand N /g1811/g3)30-bit index records, /g1811/g3)30 is determined by  \n/g1811/g3)30=  /g1811/g180ℎ/g18ℎ3/g1800/g1850/g18ℎ3/g1808(log /g28ℎ)(/g1828/g1835 /g3)30/g28ℎ8/g2809− /g1828/g1835 /g3)30)).             (8) \nwhere /g1828/g1835 /g3)30 is the i-th boundary index. When the input index i s \nvalid, the N-way Set-Associative unit will compare the input \nindex with the R-bit index records. If there is a m atching index \nrecord, we then directly get the corresponding memo ry address, \notherwise, it is further judged whether there is fr ee memory \nspace. If the memory space is not full, the N-way S et-\nAssociative unit will allocate memory to the input index, set the \ncorresponding initialization flag, and then add the  input index \ninto the index records, otherwise the circuit will raise the \noverflow signal and report the current index.  \nThe probability model controller reads and updates the \nprobability model data according to the memory addr ess and \nenable signal, and calculates the corresponding pro bability \nvalue based on the read data.  \nIt is worth noting that, for the largescale memory \nmanagement circuits, the quantity, shape, and coupl ing \nrelationship of the memories often pose severe chal lenges to the \nlayout and routing in the back-end process [27]. In  our design, \nthe address synthesizer allows the physical memory depth \nmanaged by each probability model controller to be flexibly \nadjusted by a step of N. If the address synthesizer  synthesizes \nthe outputs of K N-way Set-Associative units, the s ubsequent probability model controller will manage a physical  memory \ndepth of K × N . The final output is generated by the output \nsynthesizer according to the enable signal. \nIn summary, the advantages of the proposed VLSI str ucture \nare: 1. As shown in Fig. 8, the comparison operatio n is executed \nin parallel and the subsequent output synthesizer c an be easily \ndesigned as a pipeline, the circuit delay does not rise as M \nincreases. It means that the proposed structure can  efficiently \nadapt to the large-scale probability model; 2. The circuit \nstructure is very simple, and the M, N, R, and K me ntioned \nabove are all configurable parameters, which makes the \nmodules highly reusable. 3. The memory shape can be  selected \nflexibly, which is conducive to the place and routi ng in the \nback-end process. 4. The boundary indexes can be up dated \nthrough the configurable register, allowing the int errupt service \nprogram to dynamically refresh the hash function. \nVI.  EXPERIMENTS AND RESULTS  \nA.  Image set and statistical method \nSince the interrupt service program handles the pro bability \nmodel overflow in the unit of the entire image, we use a similar \nstandard to calculate the overflow rate of the Lept on hardware \nencoder, that is, as long as any probability model overflow \noccurs, the whole encoding of the current image is considered \nto be a failure, and the overflow rate is the propo rtion of images \nthat have failed to encode. Under this regulation, the higher the \nimage resolution, the lower the tolerance for proba bility model \noverflow. In order to get more convincing results, we establish \nexperiment based on the high-resolution image sets DIV2K and TABLE  IV \nTHE UTILIZATION RATES OF EXP _7 X7_0- EXP _7 X7_10 \nprobability model  exp_7x7_0  exp_7x7_ 1 exp_7x7_ 2 exp_7x7_ 3 exp_7x7_ 4 exp_7x7_ 5 \nutilization rate  50.78%  49.59%  46.42%  43.22%  40.03%  33.75%  \nprobability model  exp_7x7_ 6 exp_7x7_ 7 exp_7x7_ 8 exp_7x7_ 9 exp_7x7_ 10   \nutilization rate  24.25%  14.73%  8.07%  3.17%  0.24%   \n Fig. 8.  The structure of the probability models. \n \n\n\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n 8\nKaggle4K. DIV2K is a high-resolution image set of t he NTIRE \n2017 challenge on single image super resolution, in cluding 800 \ntraining images, 100 validation images, and 100 tes t images \n[28]. Kaggle is an open-source platform for develop ers and data scientists to host machine learning competitions an d share \ndatabases. Kaggle4K is a high-resolution image set on Kaggle, \ncontaining 2056 4K images [29].  \nWe use the Lepton reference model [13] to analyze t he  \n(a)                                                                                (b)                                                                                 (c) \nFig. 9.  The access probability distribution of the  probability model. The horizontal axis is the inde x of each probability model, and the vertical axis is the pro bability \nthat the index is accessed during the encoding proc ess. (a), (b), (c) correspond to the probability mo dels exp_7x7_0, exp_7x7_1, exp_7x7_2, respectively.  \n  \n \n(a)                                                                                 (b)                                                                                 (c) \nFig. 10.  The memory allocation weight function of exp_7x7_0 under different mem depths, the horizonta l axis is the index of the probab ility model, and the \nvertical axis is the memory allocation weight of ea ch index, (a), (b), (c) correspond to the memory de pth of 1000, 2000, 3000, respectively. \n  \n \n(a)                                                                          (b)                                                                             (c) \n \n                                              (d)                                                                            (e)                                                                             (f) \nFig. 11.  The overflow rate of the probability mode ls exp_7x7_0-exp_7x7_2 with different memory depths . (a), (b), (c) correspond to the overflow rate of \nexp_7x7_0-exp_7x7_2 on the statistical images, (d),  (e), (f) correspond to the overflow rate of exp_7x 7_0-exp_7x7_2 on the test images, respectively. \n  \n\n\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n 9\nstatistical characteristic of the probability model  access. In \norder to demonstrate the generalization ability of the proposed \nmemory optimization method, we extract statistical features on \nKaggle4K 0001-2000 and DIV2K 0001-0799, and calcula te the \noverflow rate on Kaggle4K 2001-2056 and DIV2K 0800- 1000. \nIt can be seen from Table III that exp_7x7_0-exp_7x 7_10, \nres_7x7_0-res_7x7_9, exp_edge_0-exp_edge_10, and \nres_thres_0-res_thres_7 are the main part of Lepton ’s \nprobability models, so the memory optimization meth od is \nevaluated on these probability models. For brevity,  we take \nprobability models exp_7x7_0-exp_7x7_10 as examples  in the \ndiscussion and just show the optimization results o f the other \nprobability models.  \nB.  Results of memory optimization \nThe utilization rates of exp_7x7_0-exp_7x7_10 are s hown in \nTable IV. It can be seen that the utilization rates  of each \nprobability model in Lepton are very different, and  most of \nthem are quite low, which implies that Lepton's pro bability \nmodels have an obvious necessity of optimization.  \nThe access probabilities of different indexes of th e \nprobability model exp_7x7_0-exp_7x7_2 are shown in Fig. 9. \nWe can see that there are indexes with higher acces s probability, \nindexes with lower access probability, and indexes that cannot \nbe accessed in each probability model.  \nFig. 10 shows the memory allocation weight function  of \nexp_7x7_0 when memory depth is 1000, 2000, 3000 \nrespectively. It can be seen that as the memory dep th increases, \nthe proposed memory optimization method not only ac curately \nclips the upper limit of the allocation weights but  also \nadaptively amplifies the allocation weights of the indexes with \nlower access probability. \nFig. 11 shows the relationship between the overflow  rate and \nthe memory depth of probability model exp_7x7_0-exp _7x7_2 \nwhen N is 8, 16, 32, and 64 respectively. It can be  seen that the \nproposed memory optimization method can not only pe rfectly \nfit the statistical images, but also perform well o n the test \nimages.  \nBased on the criteria that no overflow occurs on bo th \nstatistical images and test images, we calculate th e required \nphysical memory depth of probability model exp_7x7_ 0-exp_7x7_10, res_7x7_0-res_7x7_9, exp_edge_0-exp_edg e_10, \nand res_thres_0-res_thres_7, and show the results i n Table V. \nIt means that if a software solution based on B-tre e and linked \nlist is adopted to implement the proposed memory op timization \nmethod, the memory space required by the Lepton pro bability \nmodels can be reduced by 89.34%~90.49% at the cost of \nslightly increasing the access delay.  \nC.  The area of the probability models \nIn order to illustrate the area reduction of the pr oposed \nprobability models, this work synthesizes the circu its with the \nSIMC 60nm process and uses the original scheme whic h \nallocates dedicated memory for each index as a comp arison. \nThe target clock frequency is set to 800MHz and no timing \nviolation exists. \nThe area of the probability models with different m emory \ndepths and N values is shown in Fig. 12. Since addi tional logic \nresources are required to manage the memories and m aintain \nthe allocation status, the area of the proposed pro bability model \nis naturally larger than that of the original metho d when their \nmemory capacities are equal. In addition, it can be  seen from \nFig. 12 that for each value of N, the area of the p robability \nmodels and the memory depth are approximately in a linear \nrelationship. Therefore, we use the Mean Area (MA) of the \nmemories in each probability model to evaluate the area benefit \nachieved by the proposed memory optimization method .  \nFor the original scheme, the required memory depth equals \nto the index range, so its total area is given by \n/g182ℎ/g18ℎ)/g185ℎ/g1853 /g180ℎ/g18ℎ)/g1801/g1859 =  /g1839/g182ℎ /g180ℎ/g18ℎ)/g1801/g1859 ∗ /g1801/g1800/g1850/g185ℎ/g18ℎ0_/g18ℎ)/g1853/g1800/g1859/g185ℎ .           (9) \nSimilarly, the area of the proposed probability mod el is \n/g182ℎ/g18ℎ)/g185ℎ/g1853 /g180ℎ/g1808/g18ℎ2 =  /g1839/g182ℎ /g180ℎ/g1808/g18ℎ2 ∗ /g1839/g185ℎ/g1805_/g1850/g185ℎ/g1808/g18ℎ2h .            (10) \nTherefore, when \n/g1839/g185ℎ/g1805_/g1850/g185ℎ/g1808/g18ℎ2ℎ//g1801/g1800/g1850/g185ℎ/g18ℎ0_/g18ℎ)/g1853/g1800/g1859/g185ℎ /g31)ℎ /g1839/g182ℎ /g180ℎ/g18ℎ)/g1801/g1859 //g1839/g182ℎ /g180ℎ/g1808/g18ℎ2 ,        (11) \nthe proposed probability model will reduce the area  of the \nLepton hardware encoder. For the memory optimizatio n \nmethod, we define memory reduction as the ratio of the reduced \nphysical memory to the index range. Taking N=32 as an \nexample, according to Fig. 12, /g1839/g182ℎ /g180ℎ/g18ℎ)/g1801/g1859 //g1839/g182ℎ /g180ℎ/g1808/g18ℎ2  is about 0.3, that \nis, for the probability models with a memory reduct ion greater  \nFig. 12.   The area of different probability models . We use SIMC 65nm process \nto synthesize the proposed probability models for t he cases of N = [4,8,16,32] \nand Memory Depth = [3072,4096,5120,6144]. T he clock frequency is 800MHz \nand no timing violation exists. \n \nTABLE  VI \nTHE AREA OF THE PROBABILITY MODELS  \nProbability Models The original \nmethod  The optimized \nmethod (N=32)  \nexp_7x7_0 -exp_7x7_10  5.30  3.59  \nexp_edge_0 -exp_edge_10  1.06  0.90  \nres_ 7x7_0 -res_ 7x7 _9  0.56  0.35  \nres_thres_0 -res_thres_ 7 21.70  2.07  \nTotal of Lepton  30.59  8.88  \n TABLE  V \nTHE MEMORY DEPTH REQUIRED BY EACH PROBABILITY MODELS  \nProbability Model s N = 8  N = 16  N = 32  N = 64  \nexp_7x7_0 -exp_7x7_10  36544  35520  33152  32128  \nexp_edge_0 -exp_edge_10  10624  10112  9536  9472  \nres_ 7x7_0 -res_ 7x7 _9  4160  4096  3968  3968  \nres_thres_0 -res_thres_ 7 15872  15552  14336  13760  \nTotal of Lepton  73050  71130  66842  65178  \nMemory Saving (%)  89.3 4 89.6 2 90.24  90.4 9 \n \n\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n 10  \nthan 70%, the proposed memory optimization scheme s hould \nbe used. Therefore, we use the proposed memory opti mization \nmethod to implement the probability models that mee t this \ncriterion, while the other probability models are i mplemented \nby the original scheme. The area results are shown in Table VI. \nIt can be seen that the proposed memory optimizatio n method \nreduces the total area of the probability models by  70.97%. \nD.  The Lepton hardware encoder \nWe synthesize the proposed Lepton hardware encoder with \nthe SIMC 60nm process and compare the results with \nDropBox’s software-based solution in Table VII. The  energy \nefficiency is defined as the energy consumed for ge nerating \n1MB Lepton bitstream and the throughput is defined as the \nnumber of FHD images that can be encoded per second . It is \nworth noting that although the test results show th at the Lepton \nsoftware model [13] requires an average of 8.9 seco nds to \nencode one FHD image on a single CPU core, which me ans that \nthe 16-core parallel encoding could process 1.8 ima ges per \nsecond, we still quote the data in Horn et al. [5],  which is 5.75 \nimages per second, as a comparison. Testing results  show that \nthe Lepton hardware encoder can encode more than 31 7.69 \nFHD images per second, and no overflow occurs on th e test \nimages. Compared to the CPU-based software encoder,  the \nthroughput of the Lepton hardware encoder is increa sed by \n55.25 times, and the energy efficiency is increased  by more than \n4899 times. In addition, the manufacturing cost of Lepton \nhardware encoders is much lower than the CPU core.  \nVII.  CONCLUSION  \nIn this work, we greatly reduce the memory requirem ents of \nLepton’s probability models and implement a high th roughput, \nhigh energy efficiency, low-cost Lepton hardware en coder. The \nhash function and hardware-friendly memory optimiza tion \nmethod proposed in this work achieve the purpose of  effectively \nallocating memory space according to access probabi lity and \nmemory budget with low design complexity. In additi on, the \ntremendous energy efficiency improvement of the pro posed \nLepton hardware encoder can greatly reduce the oper ating costs \nof data centers. Since actual sparse data access us ually has \nsignificant statistical characteristics, we believe  that our method \ncan benefit a large number of similar data access a pplications. \nREFERENCES  \n[1]  A. Skodras, C. Christopoulos and T. Ebrahimi, \"The JPEG 2000 still \nimage compression standard,\" in IEEE Signal Processing Magazine , vol. \n18, no. 5, pp. 36-58, Sept. 2001, doi: 10.1109/79.9 52804. \n[2]  F. Bellard. BPG image format. Available: http://bel lard.org/bpg/.  \n[3]  Google. WebP: Compression techniques. Available: \nhttp://developers.google.com/speed/webp/docs/compre ssion \n[4]  G. K. Wallace, “The JPEG still picture compression standard,”  IEEE \nTrans. Consumer Electronics , vol. 38, no. 1, pp. xviii-xxxiv, Feb. 1992. [5]  Horn, Daniel Reiter, et al. \"The design, implementa tion, and deployment \nof a system to transparently compress hundreds of p etabytes of image files \nfor a file-storage service.\" 14th {USENIX} Symposium on Networked \nSystems Design and Implementation ({NSDI} 17) . 2017. \n[6]  D. Marpe, H. Schwarz and T. Wiegand, \"Context-based  adaptive binary \narithmetic coding in the H.264/AVC video compressio n standard,\" in \nIEEE Transactions on Circuits and Systems for Video  Technology , vol. \n13, no. 7, pp. 620-636, July 2003, doi: 10.1109/TCS VT.2003.815173. \n[7]  Patterson, David A., and John L. Hennessy. Computer Organization and \nDesign ARM Edition: The Hardware Software Interface . Morgan \nkaufmann, 2016. \n[8]  F. Song et al., \"Design of New Hash Mapping Functio ns,\" 2009 Ninth \nIEEE International Conference on Computer and Infor mation \nTechnology , Xiamen, China, 2009, pp. 45-50, doi: 10.1109/CIT. 2009.51. \n[9]  M. R. Sumalatha, V. Vaidehi, A. Kannan, M. Rajaseka r and M. \nKarthigaiselvan, \"Hash Mapping Strategy for Improvi ng Retrieval \nEffectiveness in Semantic Cache System,\"  2007 International Conference \non Signal Processing, Communications and Networking , Chennai, India, \n2007, pp. 233-237, doi: 10.1109/ICSCN.2007.350737. \n[10]  S. Richter, V. Alvarez, and J. Dittrich. A seven-di mensional analysis of \nhashing methods and its implications on query proce ssing. Proc. VLDB \nEndow ., 9(3):96–107, Nov. 2015. \n[11]  Kraska, Tim, et al. \"The case for learned index str uctures.\"  Proceedings \nof the 2018 International Conference on Management of Data . 2018. \n[12]  TEUHOLA, J. A compression method for clustered bit- vectors. \nInformation processing letters  7, 6 (1978), 308–311. \n[13]  Lepton source repository. Available: http://github. com/dropbox/lepton. \n[14]  S. M. R. Shahshahani and H. R. Mahdiani, \"A High-Pe rformance Scalable \nShared-Memory SVD Processor Architecture Based on J acobi Algorithm \nand Batcher’s Sorting Network,\" in IEEE Transactions on Circuits and \nSystems I: Regular Papers , vol. 67, no. 6, pp. 1912-1924, June 2020, doi: \n10.1109/TCSI.2020.2973249. \n[15]  D. T. Nguyen, N. H. Hung, H. Kim and H. Lee, \"An Ap proximate \nMemory Architecture for Energy Saving in Deep Learn ing Applications,\" \nin IEEE Transactions on Circuits and Systems I: Regula r Papers , vol. 67, \nno. 5, pp. 1588-1601, May 2020, doi: 10.1109/TCSI.2 019.2962516. \n[16]  T. Venkata Mahendra, S. Wasmir Hussain, S. Mishra a nd A. Dandapat, \n\"Energy-Efficient Precharge-Free Ternary Content Ad dressable Memory \n(TCAM) for High Search Rate Applications,\" in IEEE Transactions on \nCircuits and Systems I: Regular Papers , vol. 67, no. 7, pp. 2345-2357, \nJuly 2020, doi: 10.1109/TCSI.2020.2978295. \n[17]  P. Conway, N. Kalyanasundharam, G. Donley, K. Lepak  and B. Hughes, \n\"Cache Hierarchy and Memory Subsystem of the AMD Op teron \nProcessor,\" in IEEE Micro , vol. 30, no. 2, pp. 16-29, March-April 2010, \ndoi: 10.1109/MM.2010.31. \n[18]  Malathy, E., and Chandra Segar Thirumalai. \"Review on non-linear set \nassociative cache design.\" IJPT  8.4 (2016): 5320-5330. \n[19]  E. Khan, M. W. El-Kharashi, F. Gebali and M. Abd-El -Barr, \"Design and \nPerformance Analysis of a Unified, Reconfigurable H MAC-Hash Unit,\" \nin IEEE Transactions on Circuits and Systems I: Regula r Papers , vol. 54, \nno. 12, pp. 2683-2695, Dec. 2007, doi: 10.1109/TCSI .2007.910539. \n[20]  Y. Zhang et al., \"A New Message Expansion Structure for Full Pipe line \nSHA-2,\" in IEEE Transactions on Circuits and Systems I: Regula r Papers , \nvol. 68, no. 4, pp. 1553-1566, April 2021, doi: \n10.1109/TCSI.2021.3054758. \n[21]  N. At, J. Beuchat, E. Okamoto, İ. San and T. Yamaza ki, \"Compact \nHardware Implementations of ChaCha, BLAKE, Threefis h, and Skein on \nFPGA,\" in IEEE Transactions on Circuits and Systems I: Regula r Papers , \nvol. 61, no. 2, pp. 485-498, Feb. 2014, doi: 10.110 9/TCSI.2013.2278385. \n[22]  Pagh, Rasmus, and Flemming Friche Rodler. \"Cuckoo h ashing.\" Journal \nof Algorithms  51.2 (2004): 122-144. \n[23]  Herlihy, Maurice, Nir Shavit, and Moran Tzafrir. \"H opscotch \nhashing.\" International Symposium on Distributed Computing . Springer, \nBerlin, Heidelberg, 2008. \n[24]  Hopgood, F. R. A., and J. Davenport. \"The quadratic  hash method when \nthe table size is a power of 2.\" The Computer Journal  15.4 (1972): 314-\n315. \n[25]  Ding, Jialin, et al. \"ALEX: an updatable adaptive l earned \nindex.\" Proceedings of the 2020 ACM SIGMOD International \nConference on Management of Data . 2020. \n[26]  Hadian, Ali, and Thomas Heinis. \"Considerations for  handling updates in \nlearned index structures.\" Proceedings of the Second International \nWorkshop on Exploiting Artificial Intelligence Tech niques for Data \nManagement . 2019. TABLE  VII \nTHE SYNTHESIS RESULT OF LEPTON HARDWARE ENCODER  \n Area \n(/g1805/g1805 /g28ℎ)) Energy \nEfficiency \n(MB/J)  Frequency  \n(MHz) Throughput \n(images/Sec)  \nDropBox’s  - 0.03  - 5.75  \nOurs  10.13  146.97  617.59  317.69  \n \n\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n 11  \n[27]  M. Dehyadegari, S. Mohammadi and N. Yazdani, \"Evalu ating location of \nmemory controller in on-chip communication networks ,\" The 16th CSI \nInternational Symposium on Computer Architecture an d Digital Systems \n(CADS 2012) , Shiraz, Iran, 2012, pp. 133-138, doi: \n10.1109/CADS.2012.6316433. \n[28]  E. Agustsson and R. Timofte, \"NTIRE 2017 Challenge on Single Image \nSuper-Resolution: Dataset and Study,\" 2017 IEEE Conference on \nComputer Vision and Pattern Recognition Workshops ( CVPRW) , \nHonolulu, HI, 2017, pp. 1122-1131. \n[29]  Kaggle: Your machine learning and data science comm unity. Available: \nhttps://www.kaggle.com/evgeniumakov/images4k \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n ",
  "textLength": 56694
}