{
  "paperId": "cb05ef87cf3c8616626e3648f6a4146199acf57f",
  "title": "Machine Unlearning in Learned Databases: An Experimental Analysis",
  "pdfPath": "cb05ef87cf3c8616626e3648f6a4146199acf57f.pdf",
  "text": "To appear in SIGMOD 2024: Proceedings of the 2024 International Conference on Management of Data\nMachine Unlearning in Learned Databases:\nAn Experimental Analysis\nMeghdad Kurmanji\nMeghdad.Kurmanji@warwick.ac.uk\nUniversity of WarwickEleni Triantafillou\netriantafillou@google.com\nGoogle DeepMindPeter Triantafillou\np.triantafillou@warwick.ac.uk\nUniversity of Warwick\nABSTRACT\nMachine learning models based on neural networks (NNs) are en-\njoying ever-increasing attention in the Database (DB) community,\nboth in research and practice. However, an important issue has\nbeen largely overlooked, namely the challenge of dealing with the\ninherent, highly dynamic nature of DBs, where data updates are fun-\ndamental, highly-frequent operations (unlike, for instance, in ML\nclassification tasks). Although some recent research has addressed\nthe issues of maintaining updated NN models in the presence of\nnew data insertions, the effects of data deletions (a.k.a., \"machine\nunlearning\") remain a blind spot. With this work, for the first time\nto our knowledge, we pose and answer the following key questions:\nWhat is the effect of unlearning algorithms on NN-based DB mod-\nels? How do these effects translate to effects on key downstream DB\ntasks, such as cardinality/selectivity estimation (SE), approximate\nquery processing (AQP), data generation (DG), and upstream tasks\nlike data classification (DC)? What metrics should we use to assess\nthe impact and efficacy of unlearning algorithms in learned DBs?\nIs the problem of (and solutions for) machine unlearning in DBs\ndifferent from that of machine learning in DBs in the face of data\ninsertions? Is the problem of (and solutions for) machine unlearning\nfor DBs different from unlearning in the ML literature? what are\nthe overhead and efficiency of unlearning algorithms (versus the\nnaive solution of retraining from scratch)? What is the sensitivity of\nunlearning on batching delete operations (in order to reduce model\nupdating overheads)? If we have a suitable unlearning algorithm\n(forgetting old knowledge), can we combine it with an algorithm\nhandling data insertions (new knowledge) en route to solving the\ngeneral adaptability/updatability requirement in learned DBs in\nthe face of both data inserts and deletes? We answer these ques-\ntions using a comprehensive set of experiments, various unlearning\nalgorithms, a variety of downstream DB tasks (such as SE, AQP,\nand DG), and an upstream task (DC), each with different NNs, and\nusing a variety of metrics (model-internal, and downstream-task\nspecific) on a variety of real datasets, making this also a first key\nstep towards a benchmark for learned DB unlearning.\nKEYWORDS\nLearned database systems, data deletions, machine unlearning\n1 INTRODUCTION\nML has been recently enjoying increasing attention from the DB\ncommunity. ML models are being developed to aid DB systems in\nperforming better on a large variety of tasks, including cardinal-\nity/selectivity estimation (SE) [ 15,33,44,46,48,49,53], Approxi-\nmate Query Processing (AQP) [ 16,28,29,40], query cost estima-\ntion [ 38,52], learned indices [ 4,5,23,32], Data Generation (DG)\n[3, 34, 47], workload forecasting [54], DB tuning [27, 43], etc.One critical challenge learned DB systems are facing is adapting\nto the dynamic nature of database systems. Analytical DBs face fre-\nquent insertions, while transactional DBs face frequent insertions,\ndeletions, and in-place updates (the latter can be modelled as a\ndeletion followed by an insertion). The need for updatability/adapt-\nability of the learned DB models is thus a fundamental require-\nment for learned DBs. Recently, this has started being addressed\nin-depth. Specifically, Li et al . [26] have introduced a method to\nupdate learned Cardinality Estimators (CE) when there is a drift\nin data or workload. Although a step forward, their framework\ncan only update workload-driven models and for CE applications\nonly. More recently, [ 24] have provided a solution based on trans-\nfer learning and knowledge-distillation to update learned DBs in\nthe presence of data insertions carrying out-of-distribution (OOD)\ndata. These papers clearly show that in the presence of distribution\ndrifts, appropriate methods must be developed for model updating.\nHowever, these solutions are limited to data insertions and have\nnot been studied under data deletions.\nIn this paper, we initiate the study of data deletion in NN-based\nlearned DBs. Specifically, we study its effect on the learned DB\ncomponents and inform the community of our findings vis-a-vis\nlessons learned from related research in ML and from insertion\nupdates in DBs. In ML, data deletion materializes as the problem of\n\"Machine Unlearning\". This is the problem of removing information\nrelated to a part of a dataset from a trained model, without hurting\nthe information about the rest of the data [ 9]. This has also been\nreferred to as â€œforgettingâ€, â€œscrubbingâ€, â€œremovingâ€, and â€œdeletionâ€.\nWe will use these terms interchangeably.\nMachine unlearning in ML research is motivated by the need to\ndeal with out-of-date, noisy, poisoned, or outlier data. Another im-\nportant reason for machine unlearning is to protect usersâ€™ privacy\nand guarantee the \"right to be forgotten\". Machine unlearning due\nto DB deletes is qualitatively and quantitatively different from the\nsetup studied in the ML literature, as (i) deletes are very common-\nplace, sometimes even more frequent than queries themselves, and\n(ii) typically, â€œdownstream tasksâ€ are different (e.g., AQP, SE, DG,\nDC, etc.). Nonetheless, the aims are the same: to update a trained\nmodel in such a way that the â€œeffect of deleted data is removed from\nthe trained modelâ€, while, at the same time, the model does not\nlose any knowledge it has learned about non-deleted data. Consider\nan AQP engine like DBEst++ [ 28] or a cardinality estimator like\nNaru/NeuroCard [ 48,49]. These models essentially learn the dataâ€™s\nunderlying probability distributions and perform query inference\nbased on these. When a cohort of data is removed, the models\nshould be updated to reflect the correct densities (and/or correla-\ntions) for accurate inference. That is, we need to ensure that the\nupdated model makes correct predictions when querying either the\ndeleted rows and/or the remaining rows.\n1arXiv:2311.17276v1  [cs.DB]  28 Nov 2023\n\nAlbeit an important problem, removing information from a\ntrained neural network (without damaging the accuracy of the\nremaining data) is unfortunately a very challenging task. Ideally,\none could remove the to-be-forgotten data from the DB and retrain\na new model from scratch. However, as also has been shown by\n[24], training neural networks is prohibitively expensive.\nFurthermore, it is not easy to measure how well a model has truly\nforgotten the deleted cohort of data after an unlearning (forgetting)\nalgorithm is applied, as defining an appropriate set of evaluation\nmetrics is an open problem in and of itself. A contribution of our\nwork is to employ metrics for unlearning that are appropriate in\nDBs, drawing where possible inspiration from the ML literature\nthat queries various aspects of a modelâ€™s outputs like the error\n(accuracy) for classification tasks, the loss values, or the entropy of\nits predictions. We will elaborate on this in Section 6.\n2 MACHINE UNLEARNING IN LEARNED DB\nOur study aims to be general enough so that its conclusions are\npertinent to different NNs and to different DB downstream tasks for\nwhich these NNs were designed. (The selected NNs are not meant\nto imply any judgment on their being the best for selected tasks.\nRather they are meant to boost the generality of drawn conclusions).\nConsider a database relation ğ‘…with a set of columns {ğ¶1,ğ¶2,...,ğ¶ğ‘š},\nwith tuples{<ğ‘ğ‘–\n1,ğ‘ğ‘–\n2,...,ğ‘ğ‘–ğ‘š>}|ğ‘…|\nğ‘–=1. This can be a raw table or the\nresult of a join query. Also, let ğ‘“(Â·;ğœƒ)be a neural network with\na set of trainable parameters ğœƒthat parameterize a function ğ‘“.ğ‘“\nthen could be used for any downstream task like AQP, SE, or DG,\netc. Since here we wish to be general enough to handle different\ntasks/applications and different types of NNs, ğ‘“might be trained\nwith different objectives.\nAssume we have a dataset ğ·={<ğ‘ğ‘–\n1,ğ‘ğ‘–\n2,...,ğ‘ğ‘–ğ‘š>}|ğ‘…|\nğ‘–=1of tabular\ndata with|ğ‘…|rows withğ‘šcolumns. We will denote by ğœƒğ‘œthe pa-\nrameters of the (â€œoriginalâ€, i.e. before unlearning) model trained on\nğ·, using a stochastic algorithm Alike Stochastic Gradient Descent.\nThat is,ğœƒğ‘œ=A(ğ·,ğœƒğ‘Ÿ), withğœƒğ‘Ÿdenoting the random weights that\nthe network is initialized with. Now, at unlearning time, assume\nwe are given a partition of ğ·into two disjoint sets of data rows:\nthe â€œdelete-setâ€ (a.k.a â€œforget-setâ€ or â€œdeleted dataâ€), ğ·ğ‘‘, and the\nâ€œretain-setâ€, ğ·ğ‘Ÿ, whereğ·ğ‘Ÿ=ğ·\\ğ·ğ‘‘; that is, the retain-set is the\ncomplement of the delete-set in ğ·. Informally, the goal of machine\nunlearning is to transform the modelâ€™s parameters into a new set\nof parameters ğœƒğ‘¢=U(ğ·ğ‘Ÿ,ğ·ğ‘“,ğœƒğ‘œ)that do not contain any â€œknowl-\nedgeâ€ about ğ·ğ‘‘but retains all â€œknowledgeâ€ about ğ·ğ‘Ÿ, whereU\ndenotes an unlearning algorithm that has access to the original\nparameters and the retain and forget sets.\nFormally defining unlearning and quantifying success is a chal-\nlenging problem in and of itself [ 42]. A common viewpoint for the\ngoal of unlearning in the literature is that we desire the weights ğœƒğ‘¢\nto be indistinguishable from the oracle weights ğœƒâˆ—[11], or the out-\nputsğ‘“(Â·;ğœƒğ‘¢)to be indistinguishable from the outputs ğ‘“(Â·;ğœƒâˆ—)[12],\nwhereğœƒâˆ—=A(ğ·ğ‘Ÿ,ğœƒğ‘Ÿ)denotes the weights of the â€œoracleâ€ of re-\ntraining from scratch using only ğ·ğ‘Ÿ. More precisely, previous work\ndefines the goal of (exact) unlearning as achieving P(ğœƒğ‘¢)=P(ğœƒâˆ—),\nwith the probability distribution here being over model weights re-\nsulting from training with different random seeds (i.e. starting from\ndifferent initial random weights ğœƒğ‘Ÿand seeing a different orderingof mini-batches) or as achieving P(ğ‘“(.;ğœƒğ‘¢))=P(ğ‘“(.;ğœƒâˆ—)). For deep\nneural networks, the only known family of exact unlearning is\nbased on retraining from scratch. To mitigate its inefficiency, recent\nwork turned to approximate unlearning: achieving P(ğœƒğ‘¢)â‰ˆP(ğœƒâˆ—)\norP(ğ‘“(.;ğœƒğ‘¢))â‰ˆP(ğ‘“(.;ğœƒâˆ—)), sometimes accompanied by theoretical\nguarantees about the quality of that approximation.\nIn this work, we translate these definitions into a set of metrics\nthat is suitable for DB tasks. Specifically, in learned DBs, generative\nmodels are frequently derived, that model the distribution in ğ·,\nand subsequently we are interested in performance in downstream\ntasks. To be comprehensive in our study, we thus create two sets\nof metrics for quantifying unlearning quality: 1) with respect to\nthe â€œinternal stateâ€ of the model (i.e. its estimate of the probability\ndistribution) and 2) with respect to the performance on downstream\ntasks. For completeness, in addition, we will also study discrimina-\ntive models using a NN for an upstream DC task. To quantify the\nsuccess of unlearning in terms of downstream task performance,\nwe utilize task-specific errors, where lower is better. Intuitively,\nwe want the model to always make correct predictions (i.e. predic-\ntions matching the ground truth) for any query, whether the query\nrelates to retained or deleted data.\n3 LITERATURE REVIEW\n3.1 Machine Learning for Databases\nMany ML-based DB components have emerged recently, using\ndifferent models for different applications like Database indices\n[4,5,23,32], learned cardinality/selectivity estimators [ 15,33,44,\n46,48,49,53] and approximate query processors [ 28,29,40], query\noptimization and join ordering [ 22,31], cost estimation [ 38,52],\nand auto-tuning databases [ 27,43,50]. However, these do not pro-\nvide any insights or solutions to adapt to changes due to DB data\nupdates. The recent works in [ 24,26] tackled distribution changes\nfor CE tasks and CE/AQP/DG tasks respectively. ML methods have\nbeen used also for deriving samples of join query results without\nexecuting the join [ 36]. However, these approaches cannot account\nfor data deletions, which as we shall see pose different updatability\nproblems to NN-based learned DBs.\nAs in [ 24,26], we also consider updates in batches - the isolated\neffect of single data rows in learned models is negligible. In envi-\nronments like Online Transactional Processing (OLTP), however,\nupdate frequencies are higher and data may be deleted/inserted\nin single records. In such a setting the approach would be: The\nset of updates in a batch would be preprocessed and the final in-\nsert/delete records/operations would be identified. Delete records\nwould comprise the forget set for unlearning. Insert records would\ncomprise the new batch in [ 24,26]). As the results in Section 7\nshow, unlearning can work well with smaller or larger batches and\nin conjunction with data insertions.\n3.2 Machine Unlearning\nThe problem of machine unlearning was first introduced in [ 2],\nwhere they provide an exact forgetting algorithm for statistical\nquery learning. [ 1] proposed a training framework by sharding\ndata and creating multiple models, enabling exact unlearning of\ncertain data partitions. [ 9] was the first paper to introduce a prob-\nabilistic definition for machine unlearning which was inspired\n2\n\nby Differential Privacy [ 6], and formed the origin of the idea of\nthe model indistinguishability definition discussed above. More\nrecently, [ 14,17,35,45] built upon this framework and introduced\nunlearning methods that can provide theoretical guarantees under\ncertain assumptions. [ 30] surveyed methods for linear classification,\nshowing different certifiability versus efficiency trade-offs.\nRecently, approximate unlearning methods were developed that\ncan be applied to deep neural networks. [ 11] proposed an information-\ntheoretic procedure for removing information about ğ·ğ‘‘from the\nweights of a neural network and [ 10,12] proposed methods to\napproximate the weights that would have been obtained by un-\nlearning via a linearization inspired by NTK theory [ 18] in the first\ncase, and based on Taylor expansions in the latter.\nHowever, [ 11,12] scale poorly with the size of the training\ndataset, as computing the forgetting step scales quadratically with\nthe number of training samples. [ 10] addresses this issue, albeit\nunder a different restrictive assumption. Specifically, they assume\nthat a large dataset is available for pre-training that will remain\nâ€œstaticâ€, in the sense that no forgetting operations will be applied\non it; an assumption that unfortunately canâ€™t always be made in\npractice.\nSome recent works suggest modifying the original modelâ€™s train-\ning for better unlearning in the future. [ 41] propose a regularizer\nto reduce â€˜verification errorâ€™ making unlearning easier. However,\nit may impact model performance. [ 51] introduce a process with\nquantized gradients and randomized smoothing to avoid future\nunlearning, but large changes in data distribution, as a result of\ndeletion, may exceed the â€˜deletion budgetâ€™ invalidating assumptions.\nMore recent works try to directly identify the parameters in the\noriginal model that are significantly influenced by the forget-set,\naiming to modify these parameters to eliminate the impact of the\nforget-set. [ 8,37] leverage fisher information scores to identify the\nimportant parameters for the forget-set. [ 37] take a straightfor-\nward approach by fine-tuning the model on the retain-set while\nkeeping the remaining parameters frozen. In contrast, [ 8] tries to\nâ€˜dampenâ€™ those parameters while minimizing adverse effects on\nthose essential to the retain-set. [ 19] introduce a â€™sparsity-awareâ€™\nunlearning technique, integrating unlearning through fine-tuning\non the retain-set with a sparsification policy employing model prun-\ning techniques. [ 7], on the other hand, proposes a â€™saliency-awareâ€™\nunlearning approach, utilizing the loss functionâ€™s gradient to learn\na mask identifying â€™salientâ€™ parameters related to the forget-set.\nThese parameters are subsequently unlearned using existing un-\nlearning baselines such as â€˜random labellingâ€™. Despite the diversity\nin these methods, a common challenge persistsâ€”efficiently identify-\ning and modifying the important parameters tied to the forget-set\nwithout adversely affecting those essential to the retain-set.\nOn the other hand, SCRUB [ 25] is a recent machine unlearning\nmethod for computer vision image classification tasks that scales\nbetter than previous works without making restrictive assumptions.\nSCRUB reveals different requirements and metrics for different un-\nlearning applications (e.g., removing biases, correcting erroneous\nmislabelling or attack-poisoned labels, and preserving user pri-\nvacy). SCRUB is shown to be the most consistently well-performing\napproach on a wide range of architectures, datasets and metrics.4 LEARNING TASKS (APPLICATIONS)\nWe will study unlearning in the context of four well-studied, key\ntasks for learned DBs and data analytics (AQP, SE, DG, DC), using\neach a different NN type (as used in previous work). We now give\nan overview of each of these tasks.\n4.1 Downstream DB Applications\nApproximate Query Processing . AQP approximates the results\nof aggregation queries. This is a key task in analytical DBs, partic-\nularly for very large tables, where obtaining exact results can be\nprohibitively expensive [16, 24, 28, 40].\nSelectivity Estimation . SE refers to the process of estimat-\ning the number of rows that a query will return. This is key for\nquery optimization in RDBMSs, as it helps the query planner make\ninformed decisions about best execution plans [24, 48, 49].\nData Generation . (Synthetic) data generation involves creating\nartificial data that mimics the characteristics of real-world data. DG\nis important for overcoming issues of insufficient or sensitive real\ndata, thus dealing with privacy concerns or data scarcity issues.\nData Classification . DC has numerous real-world applications.\nThe integration of classifiers within DBMSs is key in enhancing\nbusiness intelligence and analytical services. For example, Microsoft\nSQL Server provides a â€˜data discovery and classificationâ€™ feature.\nSimilarly, Googleâ€™s BigQuery empowers data mining processes\nthrough a repertoire of classifiers. Such classifiers typically use\na categorical attribute whose values define the different classes\naccording to which tuples are classified.\n4.2 Machine Learning Models\nMixture Density Networks (MDNs) for AQP . MDNs consist of\nan NN to learn feature vectors and a mixture model to learn the\nprobability density function (pdf) of data. Ma et al . [28] propose\nDBest++ which uses MDNs with Gaussian nodes to perform AQP.\nFor the Gaussian Mixture, the last layer of MDN consists of three\nsets of nodes{ğœ”ğ‘–,ğœ‡ğ‘–,ğœğ‘–}ğ‘€\nğ‘–=1that form the pdf according to Eq. 1,\nwhereğ‘€is the number of Gaussian components in the mixture.\nLetğ‘¦be the dependent variable (a target numerical attribute), x\nthe vector(ğ‘¥1,...,ğ‘¥ğ‘›)of independent variables (a set of categorical\nattributes). We also define ğ‘“to be a NN that takes the encoded\ninput vectors xand transforms them into learned â€œfeature vectorsâ€\nâ„. The likelihood under the mixture of Gaussians is then given by:\nË†ğ‘ƒ(ğ‘¦|â„)=ğ‘€âˆ‘ï¸\nğ‘–=1ğœ”ğ‘–ğ’©(â„;ğœ‡ğ‘–,ğœğ‘–) (1)\nwhere\nğœ”=ğ‘”1(â„;ğœƒğœ”);ğœ‡=ğ‘”2(â„;ğœƒğœ‡);ğœ=ğ‘”3(â„;ğœƒğœ);â„=ğ‘“(x;ğœƒğ‘ğ‘ğ‘ ğ‘’)(2)\nwhere each of ğ‘”1,ğ‘”2andğ‘”3is a single-layer network that pro-\nduces the weight (â€œmixing proportionâ€) ğœ”ğ‘–, the meanğœ‡ğ‘–and stan-\ndard deviation ğœğ‘–, respectively, for each of the ğ‘–Gaussians in the\nmixture. Since the mixing proportions should sum up to 1, ğ‘”1has\na Softmax activation function, whereas ğ‘”2andğ‘”3use a ReLU acti-\nvation. We train all parameters of the system, ( ğœƒğ‘ğ‘ğ‘ ğ‘’,ğœƒğœ”,ğœƒğœ‡,ğœƒğœ)\njointly, by minimizing the negative log of the likelihood in Eq. 1.\n3\n\nDeep Autoregressive Networks for SE . The Naru and Neu-\nroCard cardinality/selectivity estimators [ 48,49] use deep autore-\ngressive networks (DARNs) to approximate a fully factorized data\ndensity. DARNs are generative models capable of learning full con-\nditional probabilities of a sequence using a masked autoencoder via\nmaximum likelihood. Once the conditionals are available, the joint\ndata distribution can be represented by the product rule as follows:\nË†ğ‘ƒ(ğ‘¥1,ğ‘¥2,...,ğ‘¥ğ‘š)=Ë†ğ‘ƒ(ğ‘¥1)Ë†ğ‘ƒ(ğ‘¥2|ğ‘¥1)...Ë†ğ‘ƒ(ğ‘¥ğ‘š|ğ‘¥1,...,ğ‘¥ğ‘šâˆ’1)\nwhereğ‘¥ğ‘–is an attribute in a relation ğ‘…withğ‘šcolumns and\nthe probability of the ğ‘–ğ‘¡â„conditional, Ë†ğ‘ƒ(ğ‘¥ğ‘–|ğ‘¥1,...,ğ‘¥ğ‘–âˆ’1), is param-\neterized via a neural network ğ‘“(Â·;ğœƒğ‘–). Naru and NeuroCard use\ncross-entropy between input and conditionals as the loss function,\nto train the parameters of the NNs ğœƒ1,...,ğœƒğ‘›.\nVariational Autoencoders for DG . VAEs [ 21] are a commonly-\nused model for data generation. A VAE is a probabilistic auto-\nencoder, that uses a pair of neural networks to parameterize an\nencoder and a decoder module. In DB systems, [ 40] used VAEs\nto build AQP engines, [ 15] exploited them for CE, and [ 47] in-\ntroduced a synthetic tabular data generator called Tabular-VAE\n(TVAE). VAEs are trained using a different loss function, known\nas Evidence-Lower-Bound (ELBO) loss (which amounts to a lower-\nbound estimation of the likelihood). Here we will use TVAE for\nlearned synthetic tabular data generation, which is of particular im-\nportance in privacy-sensitive environments, or when data is scarce\nfor data augmentation purposes, or when wishing to train models\nover tables and accessing raw data is costly.\nDeep NNs for Classification for DC . Our previous three appli-\ncations are based on generative models , trained in an unsupervised/semi-\nsupervised fashion. To complete the picture, we add a data classifi-\ncation (DC) task, using a discriminative NN model. Note that there\nis no downstream task. Traditionally, DC over tabular data has\nbeen tackled using decision trees, random forests, support vector\nmachines, etc. However, deep NNs have emerged as a powerful\nalternative that can effectively learn complex relationships and\npatterns in tabular data. Deep NNs for tabular DC leverage various\narchitectures (e.g., feedforward NNs, convolutional NNs (CNNs),\nor recurrent NNs (RNNs)). Gorishniy et al . [13] have shown that,\nfor tabular data, ResNet-like architectures are strong performers.\nResidual Neural Network (ResNet) [ 20], is a family of deep learning\narchitectures that have significantly advanced the field.\n5 UNLEARNING METHODS\nIn this section, we describe key baselines and a state-of-the-art\nmethod that has been proposed by the unlearning research in the\nML community, in the context of classification tasks in computer\nvision. Each of these offers a different procedure to utilize the retain-\nsetğ·ğ‘Ÿand / or the delete-set ğ·ğ‘‘to achieve unlearning.\nRetrain . The â€œoracleâ€ solution is to remove the to-be-forgotten\ndata from the DB and retrain a new model on only ğ·ğ‘Ÿ. Retraining\nneural networks â€œfrom scratchâ€, however, is prohibitively expensive\nand not practical in many settings [24].\nStale . This leaves the model stale as it was trained on the original\ndatağ·. For DB tasks this is more complex as most of the learned\nDB components comprise learned NNs, and non-learned modules\nsuch as the auxiliary meta-data. The stale baseline here updates themeta-data (like table cardinalities, or frequency tables), and leaves\nthe learned model stale.\nFine-tune . This simple baseline fine-tunes the original model\nfor a small number of epochs on the remaining rows. Concretely,\nit continues training the NN with data from ğ·ğ‘Ÿonly, steering the\nmodel towards forgetting the delete-set. In ML classification tasks,\nit has been shown that this method will not erase the information\ncompletely [ 11]. Interestingly, fine-tuning has also been suggested\nby some of the learned DB components to support insertion updates\n[28,49]. There, fine-tuning is performed on the new data to force\nthe model to learn it. This method has been shown not to perform\nwell with OOD data insertions [24].\nNegGrad . An interesting idea is to continue training the model,\nbut instead of using gradient descent on only ğ·ğ‘Ÿas in Fine-tune ,\nuse gradient ascent on onlyğ·ğ‘‘(or equivalently, gradient descent\nonğ·ğ‘‘with a negated gradient; earning the nickname NegGrad).\nThe intuition for this is to attempt to â€œdeleteâ€ ğ·ğ‘‘by maximizing\nthe loss on that data, aiming to â€œundoâ€ the process that the network\nhad previously undergone to learn that data.\nNegGrad+ . NegGrad may degrade the performance on the retain\nset, since it has no incentive to protect useful information from\nbeing deleted when performing the gradient ascent. Intuitively, if\nretained data is â€œsimilarâ€ to data in the delete-set, then NegGrad â€™s\nobjective of maximizing the loss on ğ·ğ‘‘may indirectly also lead to\nmaximizing the loss on (parts of) ğ·ğ‘Ÿ, which is of course undesirable.\nTo address this, we use a stronger baseline that simultaneously per-\nforms gradient ascent (as in NegGrad ) on the delete-set and gradient\ndescent (as in Fine-tune ) on the retain-set, aiming to strike a good\nbalance between maximizing the loss on ğ·ğ‘‘but keeping it small\nonğ·ğ‘Ÿ. We refer to this stronger baseline as NegGrad+ . Formally, it\nobtains the unlearned weights ğœƒğ‘¢by initializing them from ğœƒğ‘œand\nthen minimizing the following w.r.t ğœƒğ‘¢:\nğ›½1\n|ğ·ğ‘Ÿ|âˆ‘ï¸\nğ‘¥ğ‘Ÿâˆˆğ·ğ‘Ÿğ‘™(ğ‘“(ğ‘¥ğ‘Ÿ;ğœƒğ‘¢))âˆ’( 1âˆ’ğ›½)1\n|ğ·ğ‘“|âˆ‘ï¸\nğ‘¥ğ‘‘âˆˆğ·ğ‘‘ğ‘™(ğ‘“(ğ‘¥ğ‘‘;ğœƒğ‘¢))(3)\nWhereğ‘™is a task-specific loss function like negative likelihood and\nğ›½âˆˆ[0,1]is a hyperparameter. Note that we can recover Fine-tune\nby settingğ›½to 1 and NegGrad by setting ğ›½to 0.\nSCRUB . This is a more sophisticated state-of-the-art algorithm\nfor unlearning in image classification [ 25]. SCRUB makes use of a\nteacher-student framework where the teacher is the original model,\ntrained on the entire dataset, and the student is initialized from the\nteacher and is subsequently trained to keep only the information\nfrom the teacher that pertains to the retain set. Specifically, the stu-\ndent is trained with two objectives: one that minimizes the distance\nbetween the teacher and the student for the retain-set, and one\nthat maximizes this distance for the delete-set, leading to surgically\nremoving information about the delete-set. Similarly to NegGrad+,\nSCRUB aims to find a sweet spot between deleting information\naboutğ·ğ‘‘while retaining information about ğ·ğ‘Ÿbut does in the\nform of (positive and negative, for ğ·ğ‘Ÿandğ·ğ‘‘, respectively) knowl-\nedge distillation from the â€œall-knowingâ€ original model. Formally,\nSCRUB obtains the unlearned (student) weights ğœƒğ‘¢by initializing\nthem from the original (teacher) weights ğœƒğ‘œand then minimizing\nthe following objective w.r.t ğœƒğ‘¢:\n4\n\nğ›½1\n|ğ·ğ‘Ÿ|âˆ‘ï¸\nğ‘¥ğ‘Ÿâˆˆğ·ğ‘Ÿğ‘‘(ğ‘¥ğ‘Ÿ;ğœƒğ‘œ,ğœƒğ‘¢)âˆ’(1âˆ’ğ›½)1\n|ğ·ğ‘“|âˆ‘ï¸\nğ‘¥ğ‘‘âˆˆğ·ğ‘‘ğ‘‘(ğ‘¥ğ‘‘;ğœƒğ‘œ,ğœƒğ‘¢)(4)\nwhereğ‘‘is a measure of the distance between the student and\nteacher models, to be defined in an application-dependent man-\nner. While SCRUB is originally defined for classification, using\nKL-divergence for ğ‘‘, here we propose an adaptation of it for two of\nour DB tasks: SE and AQP.\nFor SE, where we use deep autoregressive models to estimate\nthe joint (over columns) probability density Ë†ğ‘ƒof the data, we define\nğ‘‘as the KL divergence between the estimated probabilities of the\nteacher and student. Formally,\nğ‘‘(ğ‘¥;ğœƒğ‘œ,ğœƒğ‘¢)=ğ¾ğ¿(Ë†ğ‘ƒ(ğ‘¥;ğœƒğ‘œ)||Ë†ğ‘ƒ(ğ‘¥;ğœƒğ‘¢)) (5)\nFor AQP, where Mixture Density Networks are used, it is not\nstraightforward how to apply SCRUB. We propose the following\nform forğ‘‘to capture the discrepancy between the mixture distri-\nbutions of the teacher and student:\nğ‘‘(ğ‘¥;ğœƒğ‘œ,ğœƒğ‘¢)=1\nğ‘€ğ‘€âˆ‘ï¸\nğ‘–(ğ‘€ğ‘†ğ¸(ğœ‡ğ‘œ\nğ‘–,ğœ‡ğ‘¢\nğ‘–)+ğ‘€ğ‘†ğ¸(ğœğ‘œ\nğ‘–,ğœğ‘¢\nğ‘–)+ğ¾ğ¿(ğœ”ğ‘œ\nğ‘–||ğœ”ğ‘¢\nğ‘–))\n(6)\nwhere for a given ğ‘¥,ğœ”is obtained as ğ‘”1(ğ‘“(ğ‘¥;ğœƒğ‘ğ‘ğ‘ ğ‘’);ğœƒğœ”), and anal-\nogously for ğœ‡andğœ, as explained for MDNs in Section 4. ğ‘€ğ‘†ğ¸ is\nMean Squared Error, M is the number of components in the mixture,\nand the parameters ğœƒ(each ofğœƒğ‘¢,ğœƒğ‘œ, correspondingly) are the set:\nğœƒ={ğœƒğ‘ğ‘ğ‘ ğ‘’,ğœƒğœ”,ğœƒğœ‡,ğœƒğœ} (7)\nSISA . Sharded, Isolated, Sliced, and Aggregated (SISA) is an en-\nsemble method that first splits the data into ğ‘disjoint partitions, and\nfurther slices each partition into ğ‘ slices. SISA trains a constituent\nmodel for each partition, by incrementally incorporating slices of\nthat partition. During unlearning, when a delete-set is requested to\nbe unlearned, SISA finds all the models that have been trained with\nthe examples from the delete-set, removes those examples from the\ndataset and retrains the affected models from scratch. As such, SISA\nis an â€˜exact unlearningâ€™ method, like Retrain, unlike the rest of the\nbaselines we consider which perform â€˜approximate unlearningâ€™.\nSince SISA was originally built for classification tasks, it uses a\nmajority vote aggregation. However, for most of the applications\nthat we study, including AQP, SE and DG this aggregation is not\napplicable. Therefore, we design new aggregations: For AQP and\nSE where the models evaluate a workload of sumorcount queries,\nwe calculate the result of the query using each model and sum up\nthe results. For DG, we generate samples using each model, and\nconcatenate all the samples to form the final synthetic data.\n6 MEASURING THE IMPACT OF DELETION\nAs mentioned earlier, this is an open problem of its own right in the\nML literature. And for learned DBs, as we discuss below, several\nadjustments need to be made.\nIn the ML literature unlearning is mostly studied for classification\nproblems, and the accuracy of the retain-set and the test-set are used\nto measure the modelâ€™s performance on the remaining data and its\ngeneralization, respectively. We desire an unlearning algorithm that\nsuccessfully â€˜forgetsâ€™ the delete-set without deteriorating either ofthese. We measure the â€˜forget qualityâ€™ using the accuracy on the\ndelete-set which is ideally close to the delete-set accuracy of the\nRetrain oracle (that truly never trained on the delete-set).\nThe commonly-studied applications for unlearning in the ML\nliterature (i.e., classification) concern the direct output of learned\nmodels (i.e., upstream tasks). Our setting is different; our learned\nDB models are developed for downstream tasks , that are not the\nimmediate output of the trained model. For instance, the AQP en-\ngine in [ 28] infers the query answer using an integral estimation\nover the learned Gaussians produced by MDNs. Similarly, the car-\ndinality estimators in [ 48,49] perform a progressive sampling over\nthe learned DARN to infer the cardinality. Given this, we will care\nboth about what the model has learned and unlearned, as well as\nthe accuracy of the downstream tasks. We establish a distinction\nbetween, on the one hand, evaluating the quality of unlearning\nwith respect to the internal state of the model itself (how well has\nit forgotten the requested data) and, on the other hand, how its\nperformance on the downstream tasks of interest is affected. This\ndistinction is interesting from a scientific perspective, as it enables\nthe study of several research questions concerning unlearning in\ngenerative pretrained models: Can we achieve the desired outcome\non a set of downstream tasks without having optimally amended\nthe modelâ€™s internal state in light of deletions? What is the rela-\ntionship between unlearning quality upstream and downstream? In\nthis work, we initiate this investigation and propose a set of metrics\nfor each of these evaluation facets.\nTo study the downstream taskâ€™s performance, we use the original\nmetrics that have been used to evaluate the task, with a small change.\nInspired by the accuracy evaluation in unlearning in ML, we divide\nthe evaluation workload into two separate groups that target the\ndelete-set and the retain-set. More specifically, for AQP and CE, one\nworkload only queries the deleted rows, and the other workload\nqueries the remaining rows. In both cases, the lower the error, the\nbetter (unlike other ML applications where higher error is desirable\n/ indicates better forgetting). Intuitively, considering a COUNT\nquery for example, when the user requests a query that targets\nthe deleted part, the engine should correctly answer 0. For DG,\nthe generated data could be used for different tasks. We follow the\nevaluation in [ 47] where we use a classification task and evaluate on\nthe test-set and measure forgetting via accuracy on the delete-set.\nAdditionally, we evaluate unlearning in the modelâ€™s internal\nstate in two ways. First, by inspecting the likelihood, we can assess\nwhat data is â€œlikelyâ€, or â€œcompatibleâ€ with the modelâ€™s internal\nunderstanding of the distribution. Indeed, likelihood is often the\ntraining objective of learned DB models and, in fact, in some cases,\nlike in MDNs, the exact likelihood is available through the mixture\nof Gaussians. Intuitively, we would like the model to assign a higher\nlikelihood to the retain-set on average, and a lower likelihood to\nthe delete-set. Second, since learned DB systems are usually gen-\nerative models that learn the data density, we can directly inspect\nthe learned probability distribution via sampling, before and after\ndeletion. Intuitively, we want the unlearning process to modify the\nlearned distribution in such a way that it accurately reflects the\nupdated true â€œstate of the worldâ€ after the deletions.\nFinally, we use Membership Inference Attacks (MIAs) to asses\nthe forgetability of the models. In an MIA, the adversary tries to\ninfer whether a data point has been involved in training a model. We\n5\n\nfocus on the common black-box attack setting where the attacker\nobserves only outputs of the model. Designing MIAs generally,\nand especially as a metric for forgetability, is an open research\nproblem. Nevertheless, we take two attacks that have been used in\nthe unlearning literature and apply them to our DC models.\n7 EXPERIMENTAL EVALUATION\nWe empirically evaluate and analyze the aforementioned unlearn-\ning algorithms, in the context of several applications and eval-\nuation metrics. We consider two scenarios: Deletion in â€œone-goâ€,\nwhere a single round of deletion is performed, as well as â€œsequential\ndeletionâ€, where several deletion requests are carried out sequen-\ntially. The code and information for reproducibility and availabil-\nity purposes can be found at https://github.com/meghdadk/DB_\nunlearning.git.\n7.1 Experimental Setup\n7.1.1 Datasets. We have used three real-world tabular datasets,\ntypically found in the literature for our downstream tasks, namely:\nCensus: 48k rows, Forest: 580K rows, and DMV: 11M rows.\n7.1.2 Delete Operations. Deletes have the following skeletons:\n1 . DELETE FROM ğ‘‡ğ‘ğ‘ğ‘™ğ‘’ WHERE L <= ğ‘ğ‘¡ğ‘¡ <= U\n2 . DELETE FROM ğ‘‡ğ‘ğ‘ğ‘™ğ‘’ WHERE ( L <= ğ‘ğ‘¡ğ‘¡ <= U) and\n(ğ‘Ÿğ‘œğ‘¤-ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ % 2 == 0 )\nwhereğ‘ğ‘¡ğ‘¡is a numerical attribute, ğ¿andğ‘ˆdefine the range of\nvalues that will be deleted, and ğ‘Ÿğ‘œğ‘¤-ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ is a primary incremental\nindex started from 1. In the former, a whole data subspace is deleted\n(â€œfull deletionâ€) whereas in the latter, only some parts within a\nsubspace are deleted (â€œselective deletionâ€).\nFor Census, ğ‘ğ‘¡ğ‘¡refers to the ğ‘ğ‘”ğ‘’attribute, where ğ¿=30and\nğ‘ˆ=35. For Forest, ğ‘ğ‘¡ğ‘¡refers toğ¸ğ‘™ğ‘’ğ‘£ğ‘ğ‘¡ğ‘–ğ‘œğ‘› andğ¿=2500,ğ‘ˆ=2700.\nFor DMV,ğ‘ğ‘¡ğ‘¡isğ‘šğ‘ğ‘¥-ğ‘”ğ‘Ÿğ‘œğ‘ ğ‘  -ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡ ,ğ¿=7000 , andğ‘ˆ=8200 . In all\ncases, full (selective) deletion queries delete 10% ( 5%) of the dataset.\n7.1.3 Queries and Metrics. For the AQP and SE tasks, after a delete\nis performed, two types of queries are generated: Query-Retain (QR)\ntargets only the remaining (non-deleted) data. On the other hand,\nQuery-Delete (QD) queries only target deleted data. For QR, we\nreport the â€œrelative errorâ€ (relative to the ground truth) to evaluate\nAQP and SE tasks. For QD, however, the ground truth is always 0\nfor both tasks (since the sum, mean and count over deleted rows\nare 0); thus, we report the â€œabsolute errorâ€ instead of the relative\nerror, to avoid a division by 0. Furthermore, for DBEst++ MDNs, as\ntheir mixture of Gaussians provides the exact likelihood, we also\nreport the average likelihood for QD and QR.\nFor the DG task we evaluate the modelâ€™s data generation quality\nvia the accuracy of an XGboost classifier trained on the synthetic\nsamples generated by the model (TVAE) after training, as in [ 47].\nWe hold out 30% of the synthetic table as the test set and train a\nclassifier with XGBoost. Then we predict the classes of the held-out\ndata test set. We report the macro f1-score for the classifier. For\nCensus, Forest and DMV, we use: income ,cover-type , and fuel-type ,\nas the target class, respectively. Here we created a smaller version\nof DMV with only 1M records, as training TVAE on the whole DMV\nis very time/resource-consuming. For this smaller DMV, instead offorming the deletion query via a range, as shown above, we delete\nall rows that satisfy ğ‘†ğ‘¡ğ‘ğ‘¡ğ‘’ =â€˜ğ‘‚ğ¾â€™.\nFor the DC task, we use: marital-status ,cover-type , and fuel-type\nfor Census, Forest and DMV, as the target class, respectively. We\nsplit the tables into a 80%-10%-10% splits of train-validation-test.\n7.1.4 Workloads. Each model is evaluated using 2000 randomly\ngenerated QR and 2000 QD queries. For Naru/NeuroCard, we use\ntheir generator to synthesize QR and QD queries: It randomly se-\nlects the number of filters per query. For Forest, this number is\nrandomly selected from the range [3,8], for Census from [5,12],\nand for DMV from [5,12] Then, it uniformly selects a row of the\ntable and randomly assigns operators [=,>=,<=]to the columns\ncorresponding to the selected filters. Columns with a domain less\nthan 10 are considered categorical and only equality filters are used.\nFor DBest++, we randomly select a ğ‘™ğ‘œğ‘¤ğ‘’ğ‘Ÿ -ğ‘ğ‘œğ‘¢ğ‘›ğ‘‘ and aâ„ğ‘–ğ‘”â„ğ‘’ğ‘Ÿ -ğ‘ğ‘œğ‘¢ğ‘›ğ‘‘\nfor the range filter and uniformly select a category from the cate-\ngorical column for the equality filter.\nSELECT AGG(Â·) FROMğ‘‡ğ‘ğ‘ğ‘™ğ‘’ WHEREğ¹ğ‘˜ANDğ¹ğ‘›\nwhere AGG is an aggregation function over a numerical attribute,\nandğ¹ğ‘˜(ğ¹ğ‘›) is a filter over a categorical (numerical) attribute. Specif-\nically,ğ¹ğ‘›is a range operation and the aggregation function is COUNT ,\nSUM, orAVG. We select the following columns from each dataset:\nCensus:[ age, country ]; Forest:[ slope, elevation ]; DMV:[ body\ntype, max gross weight ]; where the first/second attribute is cat-\negorical/numeric. Naru/NeuroCard is a cardinality estimator and\nwe only evaluate COUNT queries for it.\n7.1.5 Modelsâ€™ configurations. For DBEst++, for Census, we build\nan MDN with 2 fully connected layers of size 128 each, and use\na MoG of size 30 for the last layer. For Forest and DMV, we use\nthe same number of fully connected layers with the same size, but\nwith 80 MoG for the last layer. We train the Original andRetrain\nmodels for 50 epochs, with a learning rate (lr) of 0.001 (decaying\nwith a rate of 0.1 at epochs 10, 20, 30), and a batch size (bs) of 128,\nusing the Adam optimizer. For Naru/NeuroCard, and TVAE, we\nuse the same configuration as in the original work and tune hyper-\nparameters for smaller errors, for the datasets that have not been\nused in the original work. For the unlearning baselines, we tune\nall the hyper-parameters of the models including lr, bs, decay rate,\noptimizer, and the methodâ€™s specific hyper-parameters, to achieve\nthe smallest retain error and forget error.\nFor SISA, we set ğ‘=10andğ‘ =10for the DC task, and ğ‘=5and\nğ‘ =5for other applications. For DC, we use a smaller neural net-\nwork with 3 fully connected layers. For AQP/DBEst++ application\nwe use an MDN with fully connected layers of size 64 and 15 MoG\nfor Census and 30 for Forest and DMV. For SE/Naru-NeuroCard and\nDG/TVAE we decrease the modelsâ€™ depth by 1 layer for each of the\nencoder and the decoder. Finally, for better efficiency of SISA, we\nsort each dataset based on the column that is queried for deletion.\n7.2 Deletion in â€œOne-Goâ€ (Large Batch)\nDeletion (in â€œone-goâ€â€™ â€“ i.e. using a large batch of deletions) is\nstudied using both downstream task and model-internal metrics.\n7.2.1 Results on Downstream Tasks. AQP/DBEst++ . These results\nare reported in Tables 1 and 2. Overall, the main conclusions from\n6\n\nTable 1 are as follows: We first notice that NegGrad performs well\nfor QD but poorly for QR, which is expected as it has no incentive\nto retain knowledge of the retain-set, only to erase knowledge for\nthe delete-set. For QR-count, Retrain ,Fine-tune , and SCRUB , all\nperform similarly (with mostly overlapping CIs) NegGrad+ follows,\nwith a slightly worse performance for Census and Forest, but strong\nperformance on DMV. For QR-sum, the findings are very similar.\nFor QD, as expected, NegGrad performs very well, as it was de-\nsigned to update the model specifically for deletes. With respect\ntoStale , we would expect it to have very poor performance for\ndeleted data, as this has not been updated to learn it. This is indeed\nthe case. For QD-count: We note that NegGrad+ is a top performer,\nhand-in-hand with Retrain and that Fine-tune does very well\n(very close to Retrain ).SCRUB follows in performance, but in ab-\nsolute terms, it achieves small errors (e.g., being much closer to\nRetrain than to Stale . And for DMV SCRUB â€™s performance has\noverlapping CIs with Retrain . For QD-sum, the conclusions are\nvery similar to those of QD-count. NegGrad may be a top performer\nfor QD, but note that its performance for QR is dismal.\nFor selective deletion (Table 2), perhaps surprisingly, Stale ap-\npears to be doing better than would be expected. However, recall\nthat: (i) queries here cover both remaining and deleted data simul-\ntaneously, due to the interleaved manner with which the retain and\nforget sets are defined, and Stale is expected to do well for retained\ndata, and (ii) Stale may be leaving the model unchanged but it does\nupdate other statistics (such as frequency tables) used in the end\nto predict the aggregate. Given these facts, the good performance\non this interleaved set is not so surprising. This finding highlights\nwhy we used downstream task errors ğ‘ğ‘›ğ‘‘internal model metrics.\nSpecifically, looking at Table 2 we clearly see that Stale has a much\nhigher likelihood for deleted data than the other unlearned models.\nThis is so because Stale has not unlearned and deleted data is\nstill part of its training set. We will discuss this mismatch between\ninternal state and downstream task metrics in later sections.\nSE/Naru-NeuroCard . We have reported the results in Table 3.\nThe results are for both full and selective deletion. As mentioned\nearlier, Naru-NeuroCard involve a sampling process during infer-\nence that automatically zeroes-out the queries on the delete-set as\nthe sample does not find any record from that region of the table to\nsample from. We observe that Fine-tune is the top performer. In\nfact, fine-tuning the model results in better accuracy than retrain-\ning from scratch. This is an interesting finding of its own right, as\nsometimes fine-tuning may reinforce old knowledge while unlearn-\ning as well. Similarly, NegGrad+ also performs great. SCRUB is also\na top performer, except for full deletion on DMV. Stale also does\nwell (as also observed above for AQP). But, note that Stale per-\nforms poorly for full and selective deletion for DMV. And as before,\nits performance for full deletion is worse than that for selective\ndeletion. Again, it is crucial to evaluate across models, downstream\ntasks, internal model metrics, and datasets for a complete picture.\nFinally, as before, NegGrad degrades the model drastically.\nDG/TVAE . Finally, in Table 4 we show the results stemming\nfrom the classification accuracy over the synthetic data generated\nby TVAE across different unlearning methods and datasets. We do\nnot report SCRUB as it is unknown how to apply its loss and inte-\ngrate it with a TVAE. We also do not show numbers for NegGrad as\nits performance was very poor, suffering from exploding gradients,even with very small learning rates and a very small number of\niterations. The first main observation is that all methods perform\nvery close to each other. Even Stale . This experiment helps bring\nto the surface additional interesting issues. Why do ğ‘ğ‘™ğ‘™methods,\neven Stale , perform equally? This occurs because the delete oper-\nation in this particular case happens to not affect the classification\ntask. We investigated this by computing the Pearson correlation\ncoefficient values among the dependent and independent variables\n(omitted for space reasons) and we found that essentially all cor-\nrelations remain unaffected before and after deletion. Please note\nthat this reflects a perfectly reasonable real-world scenario as not\nall delete operations affect downstream task accuracy. Nonetheless,\nan additional concern for such cases is whether the ML models\nthemselves were affected at all by the deletion. And this is why\nmodel-internal metrics should be used to reveal such differences.\nIn fact, as we shall show in the next subsection, there are clear\ndifferences between models of different methods with respect to\nthe effect of even such deletion operations; and, it is shown that\ntheStale model fails to register the deletion.\n7.2.2 Results using Model-Internal Metrics. As we have seen above,\nwhile investigating the effect of various unlearning methods, look-\ning at only downstream tasks results can be misleading or reveal\nonly part of the full picture. Such results depend on underlying data\ndistributions, on the particular delete operations, and on the actual\nanalytics task at hand. For example, the data subspace affected by a\ndelete and its correlation to the dependent variable may be such\nthat downstream task accuracy results are unaffected, showing all\nmethods (even poor unlearning methods, or even Stale ) to perform\nvery well. In our example tasks studied here, all ML models are gen-\nerative in nature. Hence, they could generate data items, according\nto the distributions they have learned. Then one can compare these\ndistributions against the original (ground truth) data distributions\nand do so before and after deletions. In this section, we highlight\nresults using such distributions. Note that additional model-internal\nmetrics can be utilized. One such example is using a modelâ€™s likeli-\nhood numbers, as we have done above for the AQP/DBEst++ case\nwhere the underlying MDN model provides such likelihoods. For\nspace reasons, we show these distributions for the AQP/DBEst++\ncase for the Census dataset in Figure 1 (results for the other datasets\nare very similar). In addition to the visual understanding provided\nby these histograms, we also measure the divergence between the\ndistributions produced by unlearning algorithms and the real distri-\nbutions after deletions. We use the Jensen-Shannon divergence (a\nsymmetrical version of KL divergence) between these distributions.\nJS-divergence values range from 0 to ğ‘™ğ‘›2(ca. 0.69). The results\nare reported in Table 5. Note that all values are very close to zero,\nshowing strong unlearning performance for the model internally.\nIn the results shown in Figure 1, the first two subplots show\nthe real data distribution before and after the deletion. The other\nsubplots show the distribution of the samples generated by each\nmodel after unlearning. The goal is to match the ground truth his-\ntogram that is obtained after deletion, shown in subplot (b). One can\neasily see that Stale does not unlearn, as is expected. Furthermore,\nNegGrad is shown to introduce unwanted artifacts in the distribu-\ntion of the rest of the data as well, showing its limitations. All other\nmethods are shown to unlearn the part of the data that is deleted.\n7\n\nTable 1: Full deletion results for AQP/DBEst++. For QR we show the average relative-error and 95 % CI. For QD we show the\naverage absolute-error and 95 % CI. â€œcountâ€ (â€œsumâ€) refer to queries with COUNT (SUM) aggregation function. Likelihood\nnumbers show the average likelihood the MDN model predicts for the retained and the deleted rows using Eq. 1.\ndataset modelQuery-Retain - QR Query-Delete - QD likelihoods\ncount sum count sum remain delete\nCensusRetrain 16.26 Â±1.06 16.53 Â±1.11 0.09Â±0.04 2.29 Â±0.83 0.88 0.04\nStale 22.44 Â±1.38 23.03 Â±1.54 26.62 Â±6.60 824.70 Â±211.52 0.74 0.98\nFine-tune 16.88 Â±1.10 17.58 Â±1.39 0.83Â±0.19 24.69 Â±5.15 0.86 0.13\nNegGrad+ 20.20 Â±1.39 19.46 Â±1.46 0.27Â±0.02 11.34 Â±6.81 0.87 0.07\nNegGrad 153.27 Â±15.78 141.54 Â±13.31 0.00Â±0.00 0.00 Â±0.00 0.22 0.00\nSCRUB 17.39 Â±1.14 17.83 Â±1.35 1.94Â±0.51 57.48 Â±14.23 0.85 0.18\nForestRetrain 9.50Â±1.02 9.25 Â±0.84 0.00Â±0.00 0.69 Â±0.13 1.36 0.01\nStale 48.12 Â±34.32 24.59 Â±3.37 4.23Â±0.22 9148.42 Â±512.56 1.24 0.49\nFine-tune 10.15 Â±1.08 9.96 Â±0.98 0.01Â±0.00 14.68 Â±5.97 1.36 0.01\nNegGrad+ 13.55 Â±1.10 13.81 Â±1.07 0.00Â±0.00 1.75 Â±3.20 1.36 0.00\nNegGrad 137.93 Â±37.00 157.15 Â±62.51 0.00Â±0.00 0.63 Â±0.88 1.01 0.00\nSCRUB 11.16 Â±1.47 10.29 Â±0.93 0.05Â±0.00 72.93 Â±7.49 1.36 0.03\nDMVRetrain 85.06 Â±35.36 82.96 Â±21.45 0.46Â±0.17 2448.82 Â±711.35 85.09 8.27\nStale 56.05 Â±11.46 58.42 Â±7.77 2.13Â±0.70 13367.03 Â±2780.23 40.78 42.90\nFine-tune 106.07 Â±49.79 89.86 Â±26.12 1.10Â±0.39 5545.70 Â±1251.51 39.68 16.42\nNegGrad+ 81.52 Â±25.01 119.98 Â±91.18 0.52Â±0.15 3457.04 Â±1071.17 41.65 4.58\nNegGrad 8083.59 Â±527.80 1131923741.20 Â±64946.12 0.00Â±0.00 0.00 Â±0.00 2.78 0.00\nSCRUB 69.49 Â±14.05 102.54 Â±70.92 0.50Â±0.25 2642.01 Â±487.11 46.06 12.77\n(a)\n (b)\n (c)\n (d)\n (e)\n (f)\n (g)\n (h)\nFigure 1: Histogram of \"age\" for Census. Native-Country=\"United States\". AQP/DBEst++. \"Full\" deletion. (a) Original data (b)\nOriginal data after deletion (c) Retrain (d)Stale (e)Fine-tune (f)SCRUB (g)NegGrad+ (h)NegGrad\nTable 2: Selective deletion results for AQP/DBEst++. In this\nsetting queries cover ranges of remained and deleted values\nand the ground truth is therefore never zero. So, we show the\naverage relative-error and 95 % CI. The likelihood numbers\nshow the average likelihood the model predicts for the re-\ntained rows and the deleted rows using Eq. 1.\ndataset modelQueries likelihoods\ncount sum remain delete\nCensusRetrain 15.36 Â±1.09 14.12 Â±0.97 0.78 0.61\nStale 17.68 Â±1.21 16.15 Â±0.99 0.77 0.98\nFine-tune 16.02 Â±1.08 14.69 Â±0.97 0.78 0.64\nNegGrad+ 15.31 Â±1.01 14.16 Â±0.89 0.78 0.59\nNegGrad 166.76 Â±19.62 157.35 Â±18.32 0.19 0.00\nSCRUB 15.66 Â±1.07 14.48 Â±0.87 0.77 0.65\nForestRetrain 8.59Â±1.00 10.65 Â±1.18 1.25 0.27\nStale 14.64 Â±1.65 15.47 Â±1.62 1.20 0.49\nFine-tune 9.68Â±1.14 11.45 Â±1.26 1.25 0.28\nNegGrad+ 10.87 Â±1.10 12.71 Â±1.45 1.27 0.17\nNegGrad 162.41 Â±92.24 147.14 Â±38.94 0.96 0.00\nSCRUB 9.97Â±1.24 11.31 Â±1.35 1.25 0.29\nDMVRetrain 92.28 Â±41.29 60.70 Â±12.69 46.93 31.32\nStale 82.33 Â±48.65 54.10 Â±7.16 40.97 42.91\nFine-tune 104.31 Â±71.19 60.22 Â±16.38 40.79 30.64\nNegGrad+ 66.30 Â±17.51 57.98 Â±9.63 37.71 23.93\nNegGrad 288478.30 Â±152969.35 221261.35 Â±119654.25 0.01 0.00\nSCRUB 157.40 Â±96.48 64.61 Â±14.19 41.75 31.96This scenario and discussion raise the following key issue: As not all\ndeletions will affect downstream task accuracy, one may not wish\nto apply any unlearning algorithms as downstream accuracy will be\nunaffected. So a â€œdetectorâ€ for such cases is warranted. Naturally, if\nthe same model is being used for different downstream tasks, such\ntask-specific detectors may not be appropriate, as downstream task\naccuracy for other tasks may be affected.\n7.3 Sequential Deletion (Smaller Batches)\nIn the above experiments, we performed data deletion in â€œone-\ngoâ€. Here, we study a different setting where deletes are executed\nsequentially (in smaller batches). We aim to address two questions\nin this section. First, how does accuracy evolve when models are\nunlearned sequentially? Second, is it better to execute deletion\nrequests on-demand, or to group them and perform unlearning at\nonce? For these experiments, we use the â€œFullâ€ delete operation\ndescribed earlier for each dataset. We split it into 5 smaller deletes.\nAt every step, we perform unlearning on the models as updated\nin the previous step. Figures 2 to 4 show how the median error of\ndownstream tasks evolves in the different settings. Overall, three\nmain conclusions can be drawn from these figures. First, Fine-tune ,\nNegGrad+ andSCRUB have errors for QR queries that are close to\nthose of Retrain . Second, these errors do not accumulate across\n8\n\nTable 3: Full and selective deletion for SE/Naru-NeuroCard.\nNumbers are average relative-error and 95 % confidence in-\nterval for remain queries-QR (full deletion) and queries cov-\nering remain and deleted data (selective deletion).\ndataset model Full Deletion Selective Deletion\nCensusRetrain 16.87 Â±1.08 14.32 Â±1.07\nStale 16.20 Â±0.86 13.81 Â±0.99\nFine-tune 10.23 Â±0.71 9.48 Â±0.79\nNegGrad+ 10.42 Â±0.70 9.19 Â±0.79\nNegGrad 97.62 Â±26.17 107.35 Â±43.48\nSCRUB 11.71 Â±0.74 10.93 Â±0.87\nForestRetrain 24.96 Â±4.44 21.66 Â±1.99\nStale 23.78 Â±4.61 20.78 Â±1.71\nFine-tune 21.48 Â±3.67 18.34 Â±1.64\nNegGrad+ 22.43 Â±4.61 19.38 Â±2.05\nNegGrad 74.23 Â±18.83 76.81 Â±17.15\nSCRUB 23.12 Â±4.78 19.94 Â±1.86\nDMVRetrain 7.37Â±0.58 8.68 Â±0.61\nStale 13.63 Â±0.61 11.95 Â±1.00\nFine-tune 5.13Â±0.56 5.77 Â±0.66\nNegGrad+ 5.34Â±0.56 9.06 Â±0.70\nNegGrad 2892.63 Â±3283.17 40.90 Â±2.63\nSCRUB 18.67 Â±1.87 10.25 Â±0.91\nTable 4: Full and selective deletion for DG/TVAE. â€˜retain\nsynthâ€™ refers to the f1-score of the xgboost classifier trained\nwith synthetic data and evaluated on the held-out test-set.\nâ€˜delete synthâ€™ refers to the f1-score of the xgboost classifier\ntrained with the synth data and evaluate on the deleted rows.\ndataset modelFull Deletion Selective Deletion\nretain synth delete synth retain synth deleted synth\nCensusRetrain 61.13 Â±0.84 62.84 Â±3.99 61.33 Â±0.93 69.79 Â±0.99\nStale 61.27 Â±0.19 69.33 Â±0.71 60.77 Â±0.95 70.10 Â±0.33\nFine-tune 60.11 Â±1.18 68.84 Â±0.49 61.32 Â±1.18 70.18 Â±0.21\nNegGrad+ 60.41 Â±0.83 69.09 Â±1.01 61.35 Â±0.65 69.80 Â±1.07\nForestRetrain 41.46 Â±1.00 16.39 Â±0.87 44.57 Â±0.77 21.55 Â±2.66\nStale 46.94 Â±0.80 32.31 Â±1.83 46.15 Â±0.46 32.49 Â±1.68\nFine-tune 48.39 Â±1.49 28.85 Â±2.04 48.89 Â±0.73 34.65 Â±1.07\nNegGrad+ 49.95 Â±0.78 31.16 Â±1.16 46.32 Â±1.01 33.74 Â±1.13\nDMVRetrain 41.69 Â±0.72 36.19 Â±3.76 41.02 Â±0.40 43.64 Â±1.62\nStale 41.12 Â±0.51 36.99 Â±4.47 41.12 Â±0.48 35.70 Â±5.44\nFine-tune 41.34 Â±0.51 36.08 Â±1.19 40.39 Â±0.29 37.59 Â±5.05\nNegGrad+ 40.72 Â±0.56 37.40 Â±4.12 41.22 Â±0.72 52.21 Â±6.96\nTable 5: JS divergence between distributions of the original\ndata and the synthetic data generated after deletion.\nmethodFull Selective\nCensus Forest DMV Census Forest DMV\nTrain 0.0156 0.0085 0.0275 0.0096 0.0697 0.0174\nRetrain 0.0118 0.0086 0.0015 0.0053 0.0147 0.0315\nFinetune 0.0198 0.0470 0.0782 0.0019 0.0726 0.0216\nNegGrad+ 0.0062 0.0255 0.0158 0.0019 0.6407 0.0264\nNegGrad 0.3084 0.3346 0.0874 0.1001 0.6926 0.6891\nSCRUB 0.0278 0.0092 0.0017 0.0015 0.0639 0.0661\n(a)\n (b)\n(c)\n (d)\nFigure 2: Error evolution. Deleting sequentially. Census\ndataset, AQP/DBEst++ model.\nsequential deletes. NegGrad andStale conversely show an increase\nof error in consecutive steps. Third, the errors on QD queries show\nthatNegGrad performs consistently well in terms of forgetting (as\nexpected) along with Retrain ,Fine-tune , and NegGrad+ , while,\nStale again shows a growing error. These conclusions addressed\nour first question w.r.t the evolution of errors.\nFigures 5 and 6 address the second question for AQP/DBEst++\nand SE/Naru-NeuroCard for the errors of the queries on the remain-\ning rows. Results for accuracy on deleted rows are very similar and\ndeleted for space reasons. We also have plotted the horizontal line\nof 1, to signal the point of no difference between errors in sequen-\ntial vs the one-go settings. The results reveal interesting behaviors.\nFirst, looking at Figures 5 and 6, we see that for the Census datasets\nand for AQP/DBEst++ and SE/Naru-NeuroCard there is hardly any\ndifference between sequential vs one-go deletion for all methods.\nLooking at the Figures for Forest dataset, however, we see emerging\ndifferences. Namely, for AQP/DBEst++ and Forest, one-go deletion\nforRetrain andNegGrad+ emerges as preferable. And this does not\nhold for Forest and SE/Naru-NeuroCard (Figure 6). This highlights\nthe fact that conclusions depend on downstream tasks and datasets.\nInterestingly, Fine-tune appears to be less affected than Retrain\nacross these tasks and datasets. This is important as Fine-tune has\nproved to be a very strong performer from all previous experiments,\ncompetitive to Retrain . It is obviously also faster than Retrain .\nAnd now we see that it appears to be more robust than Retrain\nwith respect to how often it should be run. So, it appears to be even\nless expensive.\n7.4 Insertions and Deletions\nLearned DB updatability in general requires support for both data\nupdate operations, insertions, and deletions. We have shown thus\nfar that Fine-tune (and NegGrad+ ) perform well for unlearning\nin a learned DB model. However, recent research in learned DB\nupdatability for data insertions in [ 24] showed that fine-tuning is\nnot accurate and more complex algorithms are needed.. Thus, it\nis unknown how these different mechanisms for insertions and\n9\n\n(a)\n (b)\n(c)\n (d)\nFigure 3: Error evolution. Deleting sequentially. Forest\ndataset, AQP/DBEst++ model.\n(a)\n (b)\nFigure 4: Error evolution. Sequential deletion. Census dataset,\nSE/Naru-NeuroCard.\n(a)\n (b)\nFigure 5: Ratio of relative errors: deleting in one-go over\nsequential deletion. AQP/DBEst++ on QR. a) Census, b) Forest.\n(a)\n (b)\nFigure 6: Ratio of relative errors: deleting in one-go over\nsequential deletion. SE/Naru-NeuroCard in Forest on QR. a)\nCensus, b) Forest.Table 6: Combining Deletions and Insertions. Numbers show\naverage relative error Â±95 % CIs. NB: Naru/NeuroCard does\nnot support sum queries (shown as NA).\nModel Step OperationQueriesJS-divergenceCount Sum\nDBEst++1 Train 15.05 Â±1.09 15.62 Â±1.08 0.0136\n2 Delete 17.13 Â±1.39 16.00 Â±0.98 0.0129\n3 Insert 17.10 Â±1.08 17.79 Â±1.19 0.0044\nNeuroCard1 Train 20.85 Â±1.12 NA 0.0934\n2 Delete 10.15 Â±0.71 NA 0.1003\n3 Insert 19.50 Â±0.89 NA 0.1056\ndeletions would interact with one another. In other words, can this\nsimple fine-tuning method for unlearning perform well even in\nconjunction with a method for data insertions? We answer this\nquestion now using the code for [ 24] and combining it with fine-\ntuning for deletions, as follows: we assume insertions and deletions\ncome in batches. First, we apply the fine-tuning method for deleting\na subspace (e.g., all tuples with age values in [20,30]). Then we\napply the method in [ 24] on the fine-tuned model, for inserting\nback all tuples in the same age group. Ideally, we would get to a\ndistribution very close to the original, before any deletions and\ninsertions. Table 6 shows the results. After training the original\nmodel in Step 1, we perform the deletions in Step 2. Finally, we insert\nthe deleted data again in Step 3. We show average relative errors\nfor SUM and COUNT queries after each step. We also report the JS\ndivergence between the original data distribution and generated\nsample distribution after each step to illustrate that internally, the\nmodels at each step stay close to the original data. Note that both\nerrors and JS divergences are very small after Steps 2 and 3.\n7.5 A Data Classification (Upstream) Task\nWe report results for the DC task. Following the ML unlearning\nliterature like [ 25], we perform both class unlearning (delete all\nexamples/tuples of a class) and selective unlearning (deleted only a\nsubset of the class examples/tuples). For Census, for class unlearn-\ning, we remove all tuples of class â€™ Divorcedâ€™. And for selective\nunlearning, we delete only tuples whose age value is between 30\nand 40 from all classes . In both cases, the delete-set amounts to 10%\nof the whole data. Table 7 shows the classification accuracy for\nCensus - the results for DMV and Forest are very similar. We can\nsee that Fine-tune continues to enjoy high performance.\n7.6 Efficiency\nUnlearning algorithms are motivated by wanting to avoid the high\ncosts of retrain-from-scratch. We thus evaluated the efficiency of\nthe approximate unlearning algorithms studied. Table 8 shows the\nspeed-up of each algorithm over Retrain for our three downstream\ntasks and their NN models. (The results are very similar for the\nupstream DC task). We can see that speedups, especially for the\nFine-tune andNegrad+ are high. Note that these speedups can be-\ncome much higher when much larger datasets are used (as training\ntimes depend on dataset sizes).\n10\n\nTable 7: DC task results with a ResNet architecture. The numbers are average accuracy Â±95% CIs.\nDatasetMethodClass Unlearning Selective Unlearning\nTest Retain Forget Test Retain Forget\nCensusOriginal 83.69 Â±1.58 88.65 Â±0.41 64.72 Â±2.17 83.48 Â±0.60 85.87 Â±0.39 81.87 Â±0.84\nRetrain 79.78 Â±1.00 93.12 Â±0.17 0.00 Â±0.00 84.94 Â±0.51 86.19 Â±0.44 80.66 Â±1.12\nFine-tune 79.03 Â±1.37 92.27 Â±0.96 0.00 Â±0.00 84.34 Â±1.84 85.16 Â±1.52 80.56 Â±4.18\nNegGrad 52.39 Â±8.37 60.79 Â±1.24 0.00 Â±0.00 1.56Â±1.51 1.49 Â±1.81 0.99 Â±1.61\nNegGrad+ 76.65 Â±5.83 89.51 Â±4.13 0.00 Â±0.00 79.51 Â±8.22 80.80 Â±7.08 76.66 Â±8.93\nSCRUB 78.70 Â±0.49 92.19 Â±0.48 0.00 Â±0.00 83.53 Â±0.47 85.15 Â±0.41 80.62 Â±0.87\nTable 8: Speed-up of unlearning algorithms over the â€˜Retrainâ€™ oracle. Stale is obviously not included.\nmethodAQP/DBest++ SE/Naru-NeuroCard DG/TVAE\nFull Selective Full Selective Full Selective\nCensus Forest DMV Census Forest DMV Census Forest DMV Census Forest DMV Census Forest DMV Census Forest DMV\nRetrain 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nFine-tune 11.47 17.05 11.36 28.87 18.37 9.46 1.81 2.00 2.30 2.66 1.94 4.5 16.27 11.86 17.04 19.42 12.57 18.31\nNegGrad+ 10.90 10.28 6.67 11.55 10.61 5.70 1.86 2.09 2.28 2.5 2.21 2.45 16.27 15.98 43.08 19.42 15.11 19.88\nNegGrad 10.38 10.23 15.22 12.83 9.69 5.64 2.4 5.86 5.67 5.71 5.22 2.88 NA NA NA NA NA NA\nSCRUB 3.63 5.79 1.90 3.72 6.19 1.55 1.56 1.58 1.40 2.15 1.47 2.03 NA NA NA NA NA NA\n7.7 SISA Results\nIn this section, we summarize SISA â€™s results, reported in Table 9 to\nTable 12. We only report results for Census dataset due to space,\nbut the conclusions hold for Forest and DMV.\nFor AQP/DBEst++, SISA consistently demonstrates compara-\nble or improved error rates in comparison to regular training. Re-\ntraining SISA generally results in lower errors for deleted queries\ncompared to regular retraining. When examining likelihoods, itâ€™s\nimportant to note that direct comparisons with regular settings may\nnot be valid due to different model architectures. Nevertheless, the\ntables reveal that retraining SISA leads to lower average likelihoods\nfor deleted rows, showcasing the impact of unlearning.\nSISA shows significantly higher relative error during training\ncompared to regular training in SE/Naru-NeuroCard. Our detailed\nexperiments revealed that constituent models perform better on\ntheir own data chunks but struggle with larger data ranges. The\nintroduced sparsity from creating embeddings for the entire table\nwhile training on a small shard decreases performance. Additionally,\naggregating results across models further increases error.\nIn DG/TVAE, SISA â€™s classification performance is comparable to\nregular training, but retraining during unlearning leads to a slight\ndecrease in accuracy for both retained and deleted sets.\nFor DC, SISA for training and unlearning has a slight decrease in\ntest and retain accuracy, compared to regular training. This differ-\nence persists when comparing regular retraining and SISA retrain-\ning. However, surprisingly, the forget accuracy after unlearning\nSISA is quite similar to the regular retraining from scratch.\nFinally, we report SISA â€™s speed-up both in the training phase\ncompared to the regular training, as well as in the unlearning phase\ncompared to regular retraining. Overall, while training SISA is\nslower than regular training, during unlearning, retraining SISA is\nconsiderably faster than regular retraining. The speed-up is espe-\ncially remarkably higher in the DC task where we have a higher\nnumber of partitions and slices.7.8 Membership Inference Attacks\nWe perform two types of MIAs to evaluate unlearning quality across\nbaselines. The first is a loss-based attack [ 25]. The second is a\nconfidence-based attack [ 11]. The core idea behind these attacks is\nto train a binary classifier to learn to distinguish between the mem-\nbers (retain-set or forget-set) and the non-members (validation-set).\nHowever, achieving a high accuracy in this classifier doesnâ€™t guar-\nantee successful membership detection if the input distributions\nare very different. To address this, the validation set should follow\nthe distribution of the forget-set, while we must additionally ensure\nthat there is no overlap of rows between those two sets.\nApplying MIAs to generative models is an underexplored prob-\nlem since these models do not have a clear output signal like loss or\nconfidence that indicates memorization. Therefore, we only applied\nthe above MIAs to the DC task.\nGiven the above explanations, we split the table into train, test,\nand validation sets. Then we randomly unlearn 5% of the train-\nset using different unlearning baselines. The results are reported\nin Table 13. In both attacks, the attackâ€™s accuracy on the original\nmodel is higher than the unlearned models, except for NegGrad\nwhich results in a very high confidence-based MIA accuracy.\n7.9 Trade-offs\nMachine Unlearning algorithms trade off three aspects: model util-\nity, forget quality, and speed. In our experiments, model utility is\nquantified as the modelsâ€™ performance (accuracy) on downstream\ntasks. Forget quality is captured using likelihood, sample distri-\nbution, MIA, as well as accuracy on the forget set. Our results\nthroughout illustrate what axes of these trade-offs are improved or\nsacrificed by each unlearning algorithm.\nIn the exact unlearning algorithms, while Retrain does a perfect\njob of maintaining utility and forgetting at the same time, it could\nbe very slow, as evidenced by the speedups associated with some\napproximate unlearning algorithms. The SISA version for exact\nunlearning can also unlearn quickly and with high quality, but\nthe utility is not always guaranteed (as is the case for SE/Naru-\nNeuroCard (e.g., Table 10)).\n11\n\nTable 9: Full/Selective deletion results for AQP/DBEst++ and SISA. Corresponding to Table 1 and Table 2.\nmodelQuery-Retain - QR Query-Delete - QD likelihoodsSpeedupQueries likelihoodsSpeedupcount sum count sum remain delete sum count remain delete\nSISA-Train 14.74 Â±3.15 12.92 Â±3.39 NA NA NA NA 0.77 11.68 Â±2.36 11.31 Â±2.66 NA NA 0.77\nSISA-Retrain 13.82 Â±3.12 15.97 Â±3.91 0.00Â±0.00 2.17 Â±1.57 8.03 0.03 10.02 16.54 Â±2.91 13.87 Â±2.73 3.88 2.96 9.81\nTable 10: The full and selective deletion results for SE/Naru-NeuroCard and SISA. Corresponding to Table 3.\ndataset model Full Deletion Selective Deletion Speedup (full, selective)\nCensusSISA-Train 62.81 Â±2.95 62.79 Â±2.95 0.91\nSISA-Retrain 48.43 Â±2.31 45.47 Â±2.21 (8.4, 7.9)\nTable 11: Full and selective deletion results for DG/TVAE and SISA. Corresponding to Table 4.\ndataset modelFull Deletion Selective DeletionSpeedup (full, selective)retain synth delete synth retain synth deleted synth\nCensusSISA-Train 61.81 Â±2.17 NA NA NA 0.84\nSISANARetrain 54.09 Â±0.73 18.80 Â±0.11 55.52 Â±1.74 19.81 Â±1.67 (7.84, 7.01)\nTable 12: SISA for the DC task. Corresponding to Table 7\nMethodClass Unlearning Selective UnlearningSpeedupTest Retain Forget Test Retain Forget\nSISA-Train 81.27 Â±0.01 87.64 Â±0.01 42.31 Â±0.09 81.27 Â±0.01 81.55 Â±0.01 81.07 Â±0.00 0.81\nSISA-Retrain 77.81 Â±0.01 90.33 Â±0.00 0.00 Â±0.00 80.11 Â±0.01 80.05 Â±0.02 80.72 Â±0.00 22.01\nFor approximate unlearning algorithms, while all of them show\nspeed-ups over Retrain , their utility vs. forgetting trade-offs are\ndifferent. NegGrad usually does a top job of unlearning w.r.t. differ-\nent metrics (likelihoods and accuracy on the deleted data). How-\never, it catastrophically damages utility (e.g., Table 1). NegGrad+\nandSCRUB show better trade-offs between forgetting, utility and\nspeed-up over Retrain .Fine-tune , on the other hand, consistently\nperforms well by efficiently forgetting, without damaging utility,\nand while offering significant speedups over Retrain .\nComparing SISA exact unlearning vs the top performers of ap-\nproximate unlearning, we note the following: First, in general, SISA\nspeedups highly depend on how many of its models need to be\nretrained. This, in turn, depends on how data partitions are defined\nand how the forget set at any given time is distributed over these par-\ntitions. Our results show several cases where Fine-tune speedups\nare significantly higher than SISA â€™s. With respect to model utility\nalso, for some tasks, SISA may underperform, likely owing to the\naggregation of involved models whereby errors may accumulate.\nFor forgetting quality, SISA is generally a top performer.\n8 LESSONS LEARNED\nIs machine unlearning in learned DBs different than updating learned\nDBs with new data insertions? Our investigation has revealed an\ninteresting, perhaps surprising finding: when it comes to deleting\ndata in learned database systems, with commonly-used models\non commonly-studied tasks, simple methods (like Fine-tune and\nNegGrad+ ) do very well. This finding is interesting because it is in\nstark contrast with two other observations: First, in the context oflearned database systems, inserting data is a hard problem and sim-\nple fine-tuning approaches evidently fail badly [ 24]. When it comes\nto the first discrepancy, one hypothesis is that, in the current scenar-\nios studied in learned databases, inserting data is a harder problem\nthan deleting data. One contributing factor to this is that the new\nâ€œknowledgeâ€ that is inserted may interfere with old â€œknowledgeâ€.\nSimple methods, like Fine-tune , interfere with old knowledge by\nfine-tuning for new data, while â€catastrophically forgettingâ€ old\ndata. The issue of catastrophic forgetting in the machine learning\ncommunity is a challenging one and requires dedicated solutions\nthat are more sophisticated than simple fine-tuning. Instead, remov-\ning knowledge does not face this difficulty. However, it is still an\nopen problem to identify whether there are other unique difficulties\nassociated with deleting knowledge from learned components of\ndatabase systems that were not surfaced in our initial investigation\ninto the topic. For instance, a systematic understanding of how\ndifferent aspects of the deletion problem (e.g. size and homogeneity\nof delete-set, relationship between delete-set and retain-set) affect\nresults is an open problem for future research.\nIs machine unlearning in learned DBs different than machine un-\nlearning in image classification? In the context of image classification\n(IC), which is the common testbed for unlearning algorithms in\nthe ML community, the simple approach of fine-tuning yields poor\nresults [ 11,12,25]. And, interestingly, (our adaptation of) SCRUB ,\na top-performing unlearning method for IC substantially outper-\nforming Fine-tune (and NegGrad+ ) on IC tasks, does not generally\ndo better than Fine-tune for our DB tasks.\nAn important difference between the setup we study in this pa-\nper compared to the standard unlearning benchmarks in IC tasks\n12\n\nTable 13: Membership Inference Attack\nMethod Test Accuracy Retain Accuracy Forget Accuracy loss-MIA confidence-MIA\nOriginal 82.11 Â±0.57 91.15 Â±1.96 89.43 Â±2.39 57.48 Â±03.18 60.64 Â±2.82\nRetrain 82.48 Â±1.45 90.32 Â±2.73 72.18 Â±3.08 49.25 Â±02.91 54.45 Â±3.72\nFine-tune 83.84 Â±0.91 85.68 Â±0.32 76.40 Â±1.00 51.31 Â±04.10 52.22 Â±3.32\nNegGrad 8.32Â±4.76 8.68Â±5.38 7.39Â±3.75 52.52 Â±02.82 66.60 Â±78.10\nNegGrad+ 72.72 Â±26.68 72.82 Â±25.75 63.52 Â±24.86 50.00 Â±03.36 48.59 Â±11.74\nSCRUB 84.29 Â±0.65 84.44 Â±0.27 76.27 Â±3.09 47.10 Â±01.99 49.12 Â±5.03\nSISA-Train 83.32 Â±0.27 82.92 Â±0.12 76.05 Â±0.64 49.60 Â±0.21 57.40 Â±0.34\nSISA-Retrain 83.04 Â±0.22 82.72 Â±0.37 75.40 Â±0.87 49.60 Â±0.66 57.72 Â±0.46\nis that the latter uses significantly deeper neural networks, with\nsignificantly more trainable parameters. Given this, we hypothe-\nsize that perhaps the success of simple approaches like fine-tuning\nin our case is (at least partially) due to the shallower nature of\nthe neural networks we use. More concretely, we have 2, 5, and\n4 hidden layers in our networks for the MDN in AQP/DBEst++,\nfor the DARN in SE/Naru-NeuroCard, and for the TVAE in DG,\nrespectively. While, for instance, [ 25] uses models with more than\n20 hidden layers for image classification. To investigate this, we\nran an experiment on the computer vision tasks with shallower\nmodels than those typically used (in Table 14), and indeed we ob-\nserved that this intervention led to a significantly reduced gap in\nthe performance of the state-of-the-art algorithms over fine-tuning.\nThis is an important finding for this community, as it indicates that\ntransitioning to using deeper networks in the future may come with\ntough growing pains for data deletion in learned data systems. It is\nalso an important finding for ML unlearning research, as it ties un-\nlearning performance to characteristics of the network architecture\n(shallower vs deeper) - a connection not previously made.\nOn internal-model accuracy and downstream task accuracy for\nunlearning. We have also learned that looking at only downstream\ntask accuracy may be misleading. As our experiments have shown,\ndownstream accuracy is very much dependent on dataset char-\nacteristics, delete operation characteristics, relationships between\ndeleted data and retained data, and the downstream task itself. In\nsome cases, even Stale performs well, for example. It is instruc-\ntive, therefore, to use model-internal metrics (like likelihood, in\ncases the ML models provide them, and generated learned distri-\nbutions of ML models and related distribution-distances like JS\ndivergence) in addition which can help explain downstream task\naccuracy. Looking at such internal-model metrics is also very valu-\nable in any experimental analysis, where inevitably only a select\nset of experiments are (can be) performed. Thus, looking only at\ndownstream task accuracy may be dependent on data and/or model\nand/or delete operation peculiarities and may be misleading.\nOn accuracy, overheads, and frequency of unlearning. The ideal\nunlearning method would ensure high accuracy on retained and\ndeleted data at very low overheads. This is the raison dâ€™ etre of\nthis research field; that is, to avoid running Retrain which (espe-\ncially for very large datasets) can be prohibitively expensive. The\nexperiments of sequential vs one-go deletions showed that different\nmethods have different sensitivities on datasets and downstream\ntasks. Fine-tune appears to be less sensitive to these, affording\nthe luxury of running it less frequently (i.e. on larger batches of\ndeletion requests). So Fine-tune (and NegGrad+ ), in addition totheir accuracy being very competitive under all tasks/models and\ndatasets studied, and to the fact that they are very efficient methods\n(high speedups versus Retrain), they can also be run less frequently,\nas errors are shown not to be accumulating over time.\nOn combining algorithms for continuous learning and unlearning.\nThe simplicity of Fine-tune as an unlearning algorithm does not\nadversely interact with more complex algorithms needed to ensure\ncontinuous learning, as new data insertions occur. We have seen\nthat these can be combined (even deleting and inserting data in over-\nlapping data spaces) providing a comprehensive highly accurate\nsolution for both continuous learning and unlearning.\n9 CONCLUSION\nWe have presented the first study of unlearning in learned data-\nbase systems. This is a crucial ingredient for successfully updating\nmodels in NN-based learned DBs in the face of frequent data up-\ndates that characterize DBs. And is the only ingredient currently\ncompletely lacking thorough research and findings. Our investiga-\ntion covered three different downstream tasks (AQP, SE, and DG),\neach employing a different generative neural-network-based model,\nand one upstream task based on a discriminative NN model (DC),\nand across three different datasets. It studied different unlearning\nmethods, ranging from simple baselines, such as Fine-tune and\nNegGrad , more sophisticated methods, such as NegGrad+ , and a\nstate-of-the-art unlearning algorithm, SCRUB , adapted from the ma-\nchine unlearning literature. Our investigation proposed and studied\nappropriate metrics including downstream-task specific, as well\nas model-internal specific metrics in order to substantiate and in-\nterpret results. Our results answer a large number of related key\nquestions with respect to key learned DBs. They also point to dif-\nferent conclusions compared to those from research in insertion\nupdates in learned DBs and from the ML communityâ€™s findings for\nunlearning in image classification tasks. The work puts forth the\nbasic skeleton (for instance, unlearning algorithm baselines, per-\nformance metrics, and downstream and upstream tasks) as well as\nrelated findings en route to a much-needed benchmark for machine\nunlearning in learned DBs.\nREFERENCES\n[1]Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hen-\ngrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. 2021.\nMachine unlearning. In 2021 IEEE Symposium on Security and Privacy (SP) . IEEE,\n141â€“159.\n[2]Yinzhi Cao and Junfeng Yang. 2015. Towards making systems forget with machine\nunlearning. In 2015 IEEE Symposium on Security and Privacy . IEEE, 463â€“480.\n[3]Edward Choi, Siddharth Biswal, Bradley Malin, Jon Duke, Walter F Stewart,\nand Jimeng Sun. 2017. Generating multi-label discrete patient records using\n13\n\nTable 14: Results for image classification (IC) on Cifar5 and Lacuna5 datasets. There are 5 classes and 100 samples in each\nclass [ 11]. We unlearn class 0. An All-CNN network [ 39] with 3 CNN layers (much shallower than the usual models for IC\ntasks, which have 20+ layers). Fine-tune and SCRUB perform similarly in terms of delete-error â€“ a very different conclusion\ncompared to the IC results with deeper networks [ 25]. In terms of test- and retain-error, the results are mixed. This is an\nimportant finding: For shallower networks, the effect of more sophisticated unlearning algorithms is much less pronounced.\nFor shallower networks, for IC tasks Fine-tune does as well as the others across 3 errors. But, for these shallow networks test\nand retain-errors are very bad, so deeper networks are needed.\nmodelCifar5 Lacuna5\ntest-error retain-error delete-error test-error retain-error delete-error\noriginal 42.75 Â±1.1 37.75 Â±0.7 71.0 Â±1.9 49.2Â±1.2 43.5 Â±0.9 54.0 Â±1.0\nRetrain 43.0Â±1.3 30.2 Â±0.0 100.0 Â±0.0 46.5Â±1.4 44.7 Â±0.0 100.0 Â±0.0\nFine-tune 40.5Â±0.7 30.5 Â±0.8 98.0 Â±1.0 41.7Â±1.8 39.5 Â±2.1 100.0 Â±0.0\nSCRUB 44.5Â±2.1 39.7 Â±1.8 97.0 Â±0.8 37.0Â±1.4 35.7 Â±1.5 100.0 Â±0.0\ngenerative adversarial networks. In Machine learning for healthcare conference .\nPMLR, 286â€“305.\n[4]Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\net al.2020. ALEX: an updatable adaptive learned index. In Proceedings of the 2020\nACM SIGMOD International Conference on Management of Data . 969â€“984.\n[5]Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. 2020.\nTsunami: A learned multi-dimensional index for correlated data and skewed\nworkloads. arXiv preprint arXiv:2006.13282 (2020).\n[6]Cynthia Dwork, Aaron Roth, et al .2014. The algorithmic foundations of differ-\nential privacy. Foundations and Trends Â®in Theoretical Computer Science 9, 3â€“4\n(2014), 211â€“407.\n[7]Chongyu Fan, Jiancheng Liu, Yihua Zhang, Dennis Wei, Eric Wong, and Si-\njia Liu. 2023. SalUn: Empowering Machine Unlearning via Gradient-based\nWeight Saliency in Both Image Classification and Generation. arXiv preprint\narXiv:2310.12508 (2023).\n[8]Jack Foster, Stefan Schoepf, and Alexandra Brintrup. 2023. Fast Machine Unlearn-\ning Without Retraining Through Selective Synaptic Dampening. arXiv preprint\narXiv:2308.07707 (2023).\n[9]Antonio Ginart, Melody Guan, Gregory Valiant, and James Y Zou. 2019. Making\nai forget you: Data deletion in machine learning. Advances in neural information\nprocessing systems 32 (2019).\n[10] Aditya Golatkar, Alessandro Achille, Avinash Ravichandran, Marzia Polito, and\nStefano Soatto. 2021. Mixed-privacy forgetting in deep networks. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 792â€“801.\n[11] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. 2020. Eternal sunshine\nof the spotless net: Selective forgetting in deep networks. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition . 9304â€“9312.\n[12] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. 2020. Forgetting outside\nthe box: Scrubbing deep networks of information accessible from input-output\nobservations. In European Conference on Computer Vision . Springer, 383â€“398.\n[13] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. 2021.\nRevisiting deep learning models for tabular data. Advances in Neural Information\nProcessing Systems 34 (2021), 18932â€“18943.\n[14] Chuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van Der Maaten.\n2019. Certified data removal from machine learning models. arXiv preprint\narXiv:1911.03030 (2019).\n[15] Shohedul Hasan, Saravanan Thirumuruganathan, Jees Augustine, Nick Koudas,\nand Gautam Das. 2020. Deep learning models for selectivity estimation of multi-\nattribute queries. In Proceedings of the 2020 ACM SIGMOD International Conference\non Management of Data . 1035â€“1050.\n[16] Benjamin Hilprecht, Andreas Schmidt, Moritz Kulessa, Alejandro Molina, Kristian\nKersting, and Carsten Binnig. 2019. DeepDB: Learn from Data, not from Queries!\nProceedings of the VLDB Endowment 13, 7 (2019).\n[17] Zachary Izzo, Mary Anne Smart, Kamalika Chaudhuri, and James Zou. 2021.\nApproximate data deletion from machine learning models. In International Con-\nference on Artificial Intelligence and Statistics . PMLR, 2008â€“2016.\n[18] Arthur Jacot, Franck Gabriel, and ClÃ©ment Hongler. 2018. Neural tangent ker-\nnel: Convergence and generalization in neural networks. Advances in neural\ninformation processing systems 31 (2018).\n[19] Jinghan Jia, Jiancheng Liu, Parikshit Ram, Yuguang Yao, Gaowen Liu, Yang Liu,\nPranay Sharma, and Sijia Liu. 2023. Model Sparsity Can Simplify Machine\nUnlearning. In Annual Conference on Neural Information Processing Systems .\n[20] S Jian, H Kaiming, R Shaoqing, and Z Xiangyu. 2016. Deep residual learning for\nimage recognition. In IEEE Conference on Computer Vision & Pattern Recognition .\n770â€“778.[21] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes.\narXiv preprint arXiv:1312.6114 (2013).\n[22] Andreas Kipf, Thomas Kipf, Bernhard Radke, Viktor Leis, Peter Boncz, and\nAlfons Kemper. 2018. Learned cardinalities: Estimating correlated joins with\ndeep learning. arXiv preprint arXiv:1809.00677 (2018).\n[23] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe case for learned index structures. In Proceedings of the 2018 International\nConference on Management of Data . 489â€“504.\n[24] Meghdad Kurmanji and Peter Triantafillou. 2023. Detect, Distill and Update:\nLearned DB Systems Facing Out of Distribution Data. In Proceedings of the 2023\nInternational Conference on Management of Data (to appear) .\n[25] Meghdad Kurmanji, Peter Triantafillou, Jamie Hayes, and Eleni Triantafillou.\n2023. Towards Unbounded Machine Unlearning. arXiv preprint arXiv:2302.09880,\nto appear in NeurIPS 2024, Conference in Neural Information Processing Systems\n(2023).\n[26] Beibin Li, Yao Lu, and Srikanth Kandula. 2022. Warper: Efficiently Adapting\nLearned Cardinality Estimators to Data and Workload Drifts. In Proceedings of\nthe 2022 International Conference on Management of Data .\n[27] Guoliang Li, Xuanhe Zhou, Shifu Li, and Bo Gao. 2019. Qtune: A query-aware\ndatabase tuning system with deep reinforcement learning. Proceedings of the\nVLDB Endowment 12, 12 (2019), 2118â€“2130.\n[28] Qingzhi Ma, Ali Mohammadi Shanghooshabad, Mehrdad Almasi, Meghdad Kur-\nmanji, and Peter Triantafillou. 2021. Learned Approximate Query Processing:\nMake it Light, Accurate and Fast.. In CIDR .\n[29] Qingzhi Ma and Peter Triantafillou. 2019. Dbest: Revisiting approximate query\nprocessing engines with machine learning models. In Proceedings of the 2019\nInternational Conference on Management of Data . 1553â€“1570.\n[30] Ananth Mahadevan and Michael Mathioudakis. 2021. Certifiable machine un-\nlearning for linear models. arXiv preprint arXiv:2106.15093 (2021).\n[31] Ryan Marcus, Parimarjan Negi, Hongzi Mao, Chi Zhang, Mohammad Alizadeh,\nTim Kraska, Olga Papaemmanouil, and Nesime Tatbul. 2019. Neo: A learned\nquery optimizer. arXiv preprint arXiv:1904.03711 (2019).\n[32] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020. Learn-\ning multi-dimensional indexes. In Proceedings of the 2020 ACM SIGMOD Interna-\ntional Conference on Management of Data . 985â€“1000.\n[33] Parimarjan Negi, Ryan Marcus, Andreas Kipf, Hongzi Mao, Nesime Tatbul, Tim\nKraska, and Mohammad Alizadeh. 2021. Flow-loss: Learning cardinality estimates\nthat matter. Proceedings of the VLDB Endowment (2021).\n[34] Noseong Park, Mahmoud Mohammadi, Kshitij Gorde, Sushil Jajodia, Hongkyu\nPark, and Youngmin Kim. 2018. Data synthesis based on generative adversarial\nnetworks. arXiv preprint arXiv:1806.03384 (2018).\n[35] Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh.\n2021. Remember what you want to forget: Algorithms for machine unlearning.\nAdvances in Neural Information Processing Systems 34 (2021), 18075â€“18086.\n[36] Ali Mohammadi Shanghooshabad, Meghdad Kurmanji, Qingzhi Ma, Michael\nShekelyan, Mehrdad Almasi, and Peter Triantafillou. 2021. Pgmjoins: Random\njoin sampling with graphical models. In Proceedings of the 2021 International\nConference on Management of Data . 1610â€“1622.\n[37] Jiaeli Shi, Najah Ghalyan, Kostis Gourgoulias, John Buford, and Sean Moran. 2023.\nDeepClean: Machine Unlearning on the Cheap by Resetting Privacy Sensitive\nWeights using the Fisher Diagonal. arXiv preprint arXiv:2311.10448 (2023).\n[38] Tarique Siddiqui, Alekh Jindal, Shi Qiao, Hiren Patel, and Wangchao Le. 2020.\nCost models for big data query processing: Learning, retrofitting, and our findings.\nInProceedings of the 2020 ACM SIGMOD International Conference on Management\nof Data . 99â€“113.\n14\n\n[39] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Ried-\nmiller. 2014. Striving for simplicity: The all convolutional net. arXiv preprint\narXiv:1412.6806 (2014).\n[40] Saravanan Thirumuruganathan, Shohedul Hasan, Nick Koudas, and Gautam Das.\n2020. Approximate query processing for data exploration using deep generative\nmodels. In 2020 IEEE 36th International Conference on Data Engineering (ICDE) .\nIEEE, 1309â€“1320.\n[41] Anvith Thudi, Gabriel Deza, Varun Chandrasekaran, and Nicolas Papernot. 2022.\nUnrolling sgd: Understanding factors influencing machine unlearning. In 2022\nIEEE 7th European Symposium on Security and Privacy (EuroS&P) . IEEE, 303â€“319.\n[42] Anvith Thudi, Hengrui Jia, Ilia Shumailov, and Nicolas Papernot. 2022. On the\nnecessity of auditable algorithmic definitions for machine unlearning. In 31st\nUSENIX Security Symposium (USENIX Security 22) . 4007â€“4022.\n[43] Dana Van Aken, Andrew Pavlo, Geoffrey J Gordon, and Bohan Zhang. 2017.\nAutomatic database management system tuning through large-scale machine\nlearning. In Proceedings of the 2017 ACM International Conference on Management\nof Data . 1009â€“1024.\n[44] Xiaoying Wang, Changbo Qu, Weiyuan Wu, Jiannan Wang, and Qingqing Zhou.\n2020. Are We Ready For Learned Cardinality Estimation? arXiv preprint\narXiv:2012.06743 (2020).\n[45] Yinjun Wu, Edgar Dobriban, and Susan Davidson. 2020. Deltagrad: Rapid retrain-\ning of machine learning models. In International Conference on Machine Learning .\nPMLR, 10355â€“10366.\n[46] Ziniu Wu, Parimarjan Negi, Alizadeh Mohammad, Tim Kraska, and Madden\nSamuel. 2023. FactorJoin: A New Cardinality Estimation Framework for Join\nQueries. 1, 1 (2023).[47] Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, and Kalyan Veeramacha-\nneni. 2019. Modeling tabular data using conditional gan. Advances in Neural\nInformation Processing Systems 32 (2019).\n[48] Zongheng Yang, Amog Kamsetty, Sifei Luan, Eric Liang, Yan Duan, Xi Chen,\nand Ion Stoica. 2020. NeuroCard: one cardinality estimator for all tables. arXiv\npreprint arXiv:2006.08109 (2020).\n[49] Zongheng Yang, Eric Liang, Amog Kamsetty, Chenggang Wu, Yan Duan, Xi Chen,\nPieter Abbeel, Joseph M Hellerstein, Sanjay Krishnan, and Ion Stoica. 2019. Deep\nunsupervised cardinality estimation. arXiv preprint arXiv:1905.04278 (2019).\n[50] Ji Zhang, Yu Liu, Ke Zhou, Guoliang Li, Zhili Xiao, Bin Cheng, Jiashu Xing,\nYangtao Wang, Tianheng Cheng, Li Liu, et al .2019. An end-to-end automatic\ncloud database tuning system using deep reinforcement learning. In Proceedings\nof the 2019 International Conference on Management of Data . 415â€“432.\n[51] Zijie Zhang, Yang Zhou, Xin Zhao, Tianshi Che, and Lingjuan Lyu. 2022. Prompt\ncertified machine unlearning with randomized gradient smoothing and quantiza-\ntion. Advances in Neural Information Processing Systems 35 (2022), 13433â€“13455.\n[52] Johan Kok Zhi Kang, Sien Yi Tan, Feng Cheng, Shixuan Sun, and Bingsheng\nHe. 2021. Efficient Deep Learning Pipelines for Accurate Cost Estimations Over\nLarge Scale Query Workload. In Proceedings of the 2021 International Conference\non Management of Data . 1014â€“1022.\n[53] Rong Zhu, Ziniu Wu, Yuxing Han, Kai Zeng, Andreas Pfadler, Zhengping Qian,\nJingren Zhou, and Bin Cui. 2020. FLAT: Fast, Lightweight and Accurate Method\nfor Cardinality Estimation. arXiv preprint arXiv:2011.09022 (2020).\n[54] Yonghua Zhu, Weilin Zhang, Yihai Chen, and Honghao Gao. 2019. A novel\napproach to workload prediction using attention-based LSTM encoder-decoder\nnetwork in cloud environment. EURASIP Journal on Wireless Communications\nand Networking 2019, 1 (2019), 1â€“18.\n15",
  "textLength": 89154
}