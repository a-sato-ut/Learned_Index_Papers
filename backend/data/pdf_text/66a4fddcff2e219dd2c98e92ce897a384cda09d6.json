{
  "paperId": "66a4fddcff2e219dd2c98e92ce897a384cda09d6",
  "title": "Partitioner Selection with EASE to Optimize Distributed Graph Processing",
  "pdfPath": "66a4fddcff2e219dd2c98e92ce897a384cda09d6.pdf",
  "text": "Partitioner Selection with EASE to Optimize\nDistributed Graph Processing\nNikolai Merkel\u0003, Ruben Mayer\u0003, Tawkir Ahmed Fakir\u0003and Hans-Arno Jacobseny\n\u0003Technical University of Munich,yUniversity of Toronto\nfnikolai.merkel, ruben.mayer, tawkir.fakir g@tum.de\njacobsen@eecg.toronto.edu\nAbstract —For distributed graph processing on massive graphs,\na graph is partitioned into multiple equally-sized parts which\nare distributed among machines in a compute cluster. In the last\ndecade, many partitioning algorithms have been developed which\ndiffer from each other with respect to the partitioning quality, the\nrun-time of the partitioning and the type of graph for which they\nwork best. The plethora of graph partitioning algorithms makes\nit a challenging task to select a partitioner for a given scenario.\nDifferent studies exist that provide qualitative insights into the\ncharacteristics of graph partitioning algorithms that support a\nselection. However, in order to enable automatic selection, a\nquantitative prediction of the partitioning quality, the partitioning\nrun-time and the run-time of subsequent graph processing jobs\nis needed. In this paper, we propose a machine learning-based\napproach to provide such a quantitative prediction for different\ntypes of edge partitioning algorithms and graph processing\nworkloads. We show that training based on generated graphs\nachieves high accuracy, which can be further improved when\nusing real-world data. Based on the predictions, the automatic\nselection reduces the end-to-end run-time on average by 11.1%\ncompared to a random selection, by 17.4% compared to selecting\nthe partitioner that yields the lowest cut size, and by 29.1%\ncompared to the worst strategy, respectively. Furthermore, in\n35.7% of the cases, the best strategy was selected.\nIndex Terms —graph partitioning, automatic partitioner selec-\ntion, distributed graph processing, machine learning\nI. I NTRODUCTION\nIn the last decade, many distributed graph processing sys-\ntems and databases have emerged to process large graphs with\nbillions of edges, such as Pregel [1], Giraph [2], PowerGraph\n[3], PowerLyra [4], GraphX [5] and Neo4j [6]. In order to en-\nable distributed graph processing, a graph must be partitioned\ninto multiple equally sized parts which are distributed among\nmultiple machines in a compute cluster [7]–[9]. Each machine\nperforms computations on its partition and communication\nbetween the machines takes place via the network. The amount\nof communication between the machines is inﬂuenced by the\nquality of the partitioning and affects the performance of the\ndistributed graph processing [9]–[12].\nMany graph partitioning algorithms have been developed\nand can be categorized as follows: Streaming algorithms [4],\n[5], [13]–[23] stream the graph, e.g., as an edge list and, de-\npendent on the cut model, assign edges or vertices to partitions\none after the other on the ﬂy. In-memory algorithms [8], [9],\n[24]–[27] load the entire graph into memory for partitioning.\nHybrid algorithms [11] partition one part of the graph in-\nmemory and the remaining part in a streaming fashion. Thepartitioning algorithms differ from each other with respect to\nthe partitioning quality, the run-time of the partitioning and the\ntype of graph for which they work best [11], [12], [28]–[30].\nThe plethora of graph partitioning algorithms makes it a\nchallenging task to select a partitioner for a given scenario. It is\nhard to tell which partitioner will lead to the best partitioning\nquality on a given graph, or how much will be the quality\ndifference between two partitioners. It is even harder to select\nthe partitioning algorithm that leads to the lowest end-to-\nend run-time of both partitioning run-time and subsequent\ngraph processing run-time. However, the choice of partitioning\nalgorithm matters a lot. As we show in our paper, the best\nchoice can lead to a reduction of end-to-end run-time of up to\n71% compared to the worst choice.\nExisting studies [12], [28]–[30] on graph partitioning algo-\nrithms provide interesting qualitative insights into the char-\nacteristics of these algorithms. For example, it was shown\nthat there is not a single partitioner that works best for all\ngraph processing workloads, that partitioning run-time is not\nalways negligible and that the degree distribution of a graph\ncan inﬂuence that partitioning result. However, the studies are\nnot sufﬁcient to automatically select a graph partitioner which\nminimizes the end-to-end time for a given graph and graph\nprocessing workload. In order to automatically choose among\nmultiple partitioners, a quantitative prediction of the partition-\ning run-time and the graph processing run-time is needed. For\nexample, if the end-to-end time should be minimized, the sum\nof the predicted partitioning run-time and the graph processing\nrun-time matters.\nWe propose a machine learning-based approach to provide\nsuch a prediction and thereby enable automatic partitioner\nselection to either minimize the graph processing or end-\nto-end processing run-time. To this end, we proﬁle different\ntypes of partitioners on a wide range of graphs and measure\nthe partitioning run-time, partitioning quality metrics and\nproperties of the graph. Then, we execute different graph\nprocessing algorithms on the partitioned graphs to measure\nthe graph processing run-time. Finally, we use the proﬁled data\nto build machine learning models to predict the partitioning\nrun-time and the partitioning quality metrics on an arbitrary\ngraph for different partitioners. In addition, we train a model to\npredict the run-time of different graph processing algorithms\nbased on the predicted partitioning quality metrics and graph\nproperties. While there are different formalizations of graph\n(c) 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media,\nincluding reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers\nor lists, or reuse of any copyrighted component of this work in other works. The deﬁnitive version is published in Proceedings of the 2023 IEEE 39th\nInternational Conference on Data Engineering (ICDE ’23).arXiv:2304.04976v1  [cs.DC]  11 Apr 2023\n\npartitioning, we focus on the edge partitioning problem in\nthis work, as it is commonly used in many distributed graph\nprocessing frameworks [3], [5], [31] and subject to vibrant\nresearch [7], [8], [11], [25].\nA major challenge in using machine learning for the said\nprediction tasks is the need for training data which covers a\nwide spectrum of possible graphs (with different properties).\nIt can be difﬁcult to acquire enough representative real-world\ngraphs for training for the following reasons. First, the number\nof available real-world graphs can be limited. Second, only\nknown graphs can be used for the training, but new graphs with\ndifferent properties may appear in future workloads. Third, the\ngraphs may need to be downloaded from different sources in\ndifferent formats which is a time-intensive and brittle process.\nAn additional challenge is the selection of features for the\nmachine learning models and how to ﬁne tune the models.\nIn our work, we make the following contributions:\n1) We propose EASE , a machine learning-based system\nto predict the partitioning and processing run-time for\na given graph, graph partitioner and graph processing\nalgorithm. We show that these predictions enable au-\ntomatic edge partitioner selection which reduces the\nend-to-end run-time on average by 11.1% compared to\na random selection, by 17.4% compared to selecting\nthe partitioner that yields the lowest cut size, and by\n29.1% compared to the worst strategy, respectively.\nFurthermore, in 35.7% of the cases, the best strategy\nwas selected. Our approach is extensible so that new\npartitioners and graph processing algorithms can be in-\ncorporated without needing to re-train the entire model.\n2) We show how to tackle the challenge of acquiring a large\nvariety of graphs for training by using a graph generator.\nThis makes our approach easy to apply and reproduce,\nas we provide the graph generator along with its settings,\nso that the model can be trained without being dependent\non external graph datasets.\n3) We describe how the prediction accuracy of the machine\nlearning model can be further improved by enriching\nthe synthetic graphs with real-world graphs, if available.\nThis enables the user to include real-world graphs in\norder to further increase the performance of the machine\nlearning model for speciﬁc graph types. With enrich-\nment, we decrease the prediction error for the replication\nfactor by a factor of up to 3:2\u0002which leads to a\nreduction of end-to-end time of 5% for the enriched type\nof graph.\nThe rest of the paper is organized as follows. In Section II,\nwe introduce the edge partitioning problem, partitioning qual-\nity metrics and common graph properties. In Section III, we\nshow the importance of partitioning algorithm selection for\ngraph processing performance. In Section IV, we introduce\nour machine learning based approach for partitioning quality,\npartitioning run-time and processing run-time prediction. In\nSection V, we evaluate our approach. In Section VI, we discuss\nrelated work. Finally, we conclude in Section VII.II. B ACKGROUND\nLetG= (V;E)be a graph consisting of a set of vertices\nVand a set of edges E\u0012V\u0002V. The goal of edge\npartitioning is to cut G into kpartitions. In edge partitioning\n[10], the edges are divided into kpairwise disjoint partitions\nP=fp1;:::;pkgwith[k\ni=1pi=E. Each partition picovers\na set of vertices V(pi) =fv2Vj9u2V: (u;v)2\npi_(v;u)2pig. The vertices covered by a partition can be\nfurther categorized into source and destination vertices deﬁned\nasVsrc(pi) =fu2Vj9v2V: (u;v)2pigandVdst(pi) =\nfv2Vj9u2V: (u;v)2pig, respectively. Vertices which\nare covered by multiple partitions are cut and need to be\nreplicated. The goal of edge partitioning [25] is to minimize\nthe number of replicated vertices with the constraint that the\nedge partitions are \u000b-balanced:8pi2P:jpij\u0014\u000b\u0001jEj\nk.\nA. Partitioning Quality Metrics\nDifferent partitioning quality metrics exist. First, the replica-\ntion factor which is deﬁned as RF(P) =1\njVjP\ni2[k]jV(pi)j.\nIt represents the average number of partitions a vertex spans.\nThe replication factor is closely related to the communication\ncost [10], [28]. Second, different balancing metrics that mea-\nsure how balanced a partitioning is. In distributed graph pro-\ncessing, a balanced partitioning is important in order to avoid\noverloading machines that become stragglers [28]. Often, the\nedge balance is considered in vertex-cut partitioning (e.g., [7],\n[8], [10], [11], [16], [20], [21]). In addition, the vertex balance\ncan inﬂuence the processing performance [8], [11] which we\nalso show in Section III. For directed graphs, one can further\ndifferentiate between source vertex balance and destination\nvertex balance if the computation or communication load is\ndependent on the number of source or destination vertices per\npartition.\nLetM=fm1;:::;mkgbe a set containing natural num-\nbers. We deﬁne max(M)as the largest element x2Ms.t.\n8y2M:y\u0014x, andavg(M) =1\njMjP\ni2[k]mias the\naverage of the set. Now, we deﬁne four balancing metrics.\n1)Edge balance: Bedge(P) =max(jp1j;:::;jpkj)\navg(jp1j;:::;jpkj)\n2)Vertex balance: Bv(P) =max(jV(p1)j;:::;jV(pk)j)\navg(jV(p1)j;:::;jV(pk)j))\n3)Source balance: Bsrc(P) =max(jVsrc(p1)j;:::;jVsrc(pk)j)\navg(jVsrc(p1)j;:::;jVsrc(pk)j))\n4)Destination balance:\nBdst(P) =max(jVdst(p1)j;:::;jVdst(pk)j)\navg(jVdst(p1)j;:::;jVdst(pk)j))\nIn this work, we build machine learning models to predict\nthese ﬁve partitioning quality metrics (i.e., replication factor\nand the four balancing metrics) as a basis for graph processing\nrun-time prediction.\nB. Graph Properties\nThere are different properties that characterize a graph. The\nfollowing graph properties are used as features for our machine\nlearning models. In Section IV-B, a detailed discussion is\nprovided why these graph properties are promising feature\ncandidates for our prediction tasks.\n1)Density: The density is deﬁned as dens(G) =\njEj\njVj\u0001(jVj\u00001)and describes how many edges are contained\n\nin a graph compared to all possible edges which could\nbe created with the given vertices [32].\n2)Average degree: The average degree is deﬁned as\ndeg(G) =2\u0001jEj\njVj.\n3)Average number of triangles: A triangle is a complete\nsubgraphT=fVT;ETgof an undirected graph G=\n(V;E)with exactly three vertices [33]. The number of\ntriangles of a vertex is deﬁned as t(v) =jfTjv2VTgj.\nThe average number of triangles is deﬁned as t(G) =\n1\njVjP\nv2Vt(v).\n4)Average local clustering coefﬁcient (LCC): The local\nclustering coefﬁcient [33]–[35] of a vertex vis deﬁned\nas\nc(v) =t(v)\n0:5\u0001deg(v)\u0001(deg(v)\u00001), with deg(v)being the\ndegree ofvandt(v)the number of triangles (see above).\nThe average local clustering coefﬁcient is deﬁned as\nC(G) =1\njVjP\nv2Vc(v).\n5)Skewness: For all metrics which are calculated for each\nvertexvin the graph (e.g., degree), also, the skewness of\nthe values can be described with Pearson’s ﬁrst skewness\ncoefﬁcient skew(values ) =mean (values )\u0000mode (values )\n\u001b,\nwith\u001bbeing the standard deviation of the values.\nIII. G RAPH PARTITIONER SELECTION\nAs mentioned in Section I, partitioning algorithms differ a\nlot from each other. The properties of graph partitioning algo-\nrithms pose a complex trade-off when selecting a partitioner.\nWhile a better partitioning quality in many cases leads to faster\ndistributed graph processing, it may also be more expensive\nin terms of partitioning run-time to achieve it.\nWe showcase this in the following experiments for two\ngraph processing algorithms: PageRank [36] and Label Propa-\ngation [37]. We choose PageRank as a communication-bound\nalgorithm which is sensitive to the replication factor and\nLabel Propagation as a computation-bound algorithm which\nis sensitive to load balancing.\nA. PageRank\nPageRank is executed on the graphs Friendster [38] and sk-\n2005 [39]–[41] for 50 iterations on a Spark/GraphX cluster\nwith 64 machines. Friendster andsk-2005 consists of 1:8B\nand1:9Bedges and 66Mand51Mvertices, respectively.\nThe graphs are partitioned into 64 partitions with 2D [5],\n2PS [21] and NE [25] as representatives for stateless stream-\ning, stateful streaming and in-memory partitioning algorithms,\nrespectively. Additionally, we use CRVC [5] as a baseline. In\nFigure 1, we report the replication factor, the partitioning run-\ntime and the graph processing run-time.\nWe observe that a better replication factor leads to a lower\nprocessing run-time. However, algorithms that yield a low\nreplication factor impose a longer partitioning run-time. In-\nmemory partitioning with NE is on both graphs much better\nthan stateless streaming with 2D in terms of replication factor\nand graph processing run-time, but takes a much higher par-\ntitioning run-time. The graph partitioning run-time of stateful\nstreaming with 2PS is between the run-time of 2D and NE,\nCRVC 2D 2PS NE\npartitioner051015replication factor(a) Replication factor\n(FR).\nCRVC 2D2PS NE\npartitioner050100partitioning time \n (min)(b) Partitioning\nrun-time (FR).\nCRVC 2D 2PS NE\npartitioner0204060processing time \n (min)(c) PageRank run-\ntime (FR).\nCRVC 2D 2PS NE\npartitioner01020replication factor\n(d) Replication factor\n(SK).\nCRVC 2D 2PS NE\npartitioner01020partitioning time \n (min)(e) Partitioning\nrun-time (SK).\nCRVC 2D 2PS NE\npartitioner0255075processing time \n (min)(f) PageRank run-time\n(SK).\nFig. 1: Performance comparison of graph partitioning algo-\nrithms for PageRank computation on Friendster (FR) andsk-\n2005 (SK) graph.\nDBH 2D NE\npartitioner02040processing time \n (min)\n(a) Label Propagation\nrun-time (FB).\nDBH 2D NE\npartitioner012vertex balance(b) Vertex balance\n(FB).\nDBH 2D NE\npartitioner0.00.51.01.5replication factor(c) Replication factor\n(FB).\nFig. 2: Performance comparison of graph partitioning algo-\nrithms for Label Propagation computation on Socfb-A-anon\n(FB) graph.\nwhile the replication factor and the graph processing run-\ntime for 2PS depends on the graph. On sk-2005 , 2PS is very\nclose to NE and therefore also much better than 2D. On\nFriendster , 2PS is much worse than NE and close to 2D. These\nexamples show a correlation between replication factor and\ngraph processing run-time. Therefore, the replication factor is\nan important partitioning feature for graph processing run-time\nprediction.\nB. Label Propagation\nLabel Propagation is executed on the graph Socfb-A-anon\n[42] for 10 iterations on a Spark/GraphX cluster with 4\nmachines. Socfb-A-anon consist of 3:1Mvertices and 24M\nedges. The graphs are partitioned with 2D [5], DBH [16] and\nNE [25] into 4 partitions. In Figure 2, we report the graph\nprocessing run-time, the vertex balance and the replication\nfactor. The graph partitioning run-time is not reported since it\nis negligible compared to the graph processing run-time in this\nscenario. We observe that a better vertex balance (i.e., close\nto1:0) leads to a lower processing run-time. The replication\nfactor is less important since label propagation is computation-\nbound. We conclude that balancing metrics can be important\nfeatures for graph processing run-time prediction.\nThese examples show that the prediction of graph process-\ning run-time can depend on different partitioning metrics, and\n\nPartitioner Selection\nDistributed\nGraph\nProcessing1\n3\n- Stateless Streaming: 2D, DBH, ...  \n- Stateful Streaming: 2PS, HDRF , ... \n- In-Memory: NE, ...  \n- Hybrid: HEP  \n with selected\npartitionerGraph PartitioningProcessing  \nAlgorithm  \nInput\nGraph2Fig. 3: EASE is part of a graph processing pipeline and is used\nfor automatic partitioner selection.\nPartitioning  \nTime PredictorProcessing  \nTime Predictor \nGraph Partitioner:  \n- HDRF ,  \n- DBH  \n- ...  Processing Algorithm:  \n- PageRank  \n- Shortest Paths  \n- ... \nGraph Properties:  \n- #Edges  \n- #Vertices  \n-  ... Partitioning Quality Predictor:  \n- Replication Factor  \n- Edge-Balance  \n- Vertex-Balance  \n- Source-Balance  \n- Destination-Balance  Partitioner  \nSelector  Optimization Goal: End-to-end T ime or Graph Processing T ime \nPartitioner Selection with EASE\nFig. 4: EASE - System Overview.\nthat partitioning run-time can also play a signiﬁcant role in\nselecting a graph partitioner. We use these insights to design\nEASE , as described in the next section.\nIV. A PPROACH\nMotivation: Figure 3 illustrates a graph processing pipeline\nconsisting of three phases: First, a graph partitioner is selected.\nSecond, the graph is partitioned and ﬁnally, the distributed\ngraph processing is performed. The partitioner selection is an\nimportant phase in this pipeline since it inﬂuences the run-\ntime of the graph partitioning and graph processing phase.\nHowever, until now, the graph partitioner can only be selected\nwith manual heuristics and best practices which make it a\nchallenging task for unexperienced users. It is hard to tell\nwhich selection will lead to the lowest end-to-end run-time for\na given graph processing workload and graph. Additionally,\nthe selection phase needs to be really fast so that it does\nnot slow down the pipeline. In order to tackle these chal-\nlenges, we propose a machine learning-based system called\nEdge p Artitioner SElection (EASE ) which enables efﬁcient\nautomatic partitioner selection.\nApproach Overview: EASE’s design is visualized in Fig-\nure 4. The system consists of four components: (1) Partition-\ningQualityPredictor , to predict different partitioning quality\nmetrics (replication factor, edge balance, vertex balance, des-\ntination balance and source balance) for different partitioners\non a given graph. (2) PartitioningTimePredictor , to predict\nthe partitioning run-time for different partitioners on a given\ngraph. (3) ProcessingTimePredictor , to predict the graph pro-\ncessing run-time of a given graph processing algorithm for a\npartitioned graph with the corresponding partitioning quality\nmetrics. (4) PartitionerSelector which, based on the prediction\nof partitioning and processing run-time, automatically selects\na partitioner to either minimize the processing run-time or the\nend-to-end run-time.\nTraining Phase Overview: The training phase of EASE con-\nsists of four steps (see Figure 5). First, graphs with different\nproperties are collected or generated. Second, the graphs are\nGenerate or collect\ntraining graphs Partition graphs. Measure\npartitioning quality metrics\nand partitioning run-time Execute graph processing\nalgorithms and measure\nrun-timeTrain\nEASE1 2 3 4Fig. 5: The training phase of EASE consists of four steps.\npartitioned with different partitioning algorithms into different\nnumbers of partitions and the partitioning quality metrics and\nthe partitioning run-time are measured. Third, different graph\nprocessing algorithms are executed for each combination of\ngraph and partitioner and the processing run-time is measured.\nForth, the acquired data is used to train machine learning\nmodels for PartitioningQualityPredictor ,PartitioningTimePre-\ndictor andProcessingTimePredictor . Finally, the models can\nbe applied to enable automatic partitioner selection.\nIn the following, we show how to acquire enough training\ndata which covers a wide spectrum of graphs with different\nproperties (Section IV-A) and which graph properties to select\nas features for the machine learning models (Section IV-B).\nAfterwards, we describe how to select and ﬁne tune the\nmachine learning models (Section IV-C) and how to apply\nthe models to a given graph problem (Section IV-D). Finally,\nwe discuss design decisions for EASE (Section IV-E).\nA. Training Data Acquisition\nThe goal of data acquisition is to cover a wide spectrum\nof graph properties of unseen real-world graphs that are\nexpected as a workload and for which the three prediction\ntasks should be performed. At the same time, it should be\neasy to obtain these graphs. In Section I, we described that\nit can be challenging for several reasons to acquire enough\nreal-world graphs. We tackle this challenge by using synthetic\ngraphs. By doing so, we can generate many graphs and cover\na wide spectrum of graph properties. This is our basis to\nbuild machine learning models. Then, the models are tested on\nreal-world graphs and possible weaknesses can be identiﬁed,\ne.g., that certain combinations of graph types and partitioning\nalgorithms do not yield accurate results. For these weaknesses\nof the model, the training can be reﬁned by enriching the\nsynthetic data with real-world graphs, if available.\nIn our work, we use R-MAT [43] as graph generator and\nuse the implementation of Khorasani et al. [44]. R-MAT is\nlightweight, scales well, produces realistic graphs [45]–[47]\nand is used in the well-known Graph500 benchmark [48].\nAccording to the original R-MAT paper [43], the partitions a\nanddcan be seen as communities that are connected by edges\nin the partitions bandc. The recursive generation process leads\nto sub-communities. In order to generate different graphs, we\nchoose different combinations of parameters. First, we use\ndifferent number of vertices ( 215to227) and edges (1 M to\n200 M). All combinations are shown in Table Ia. Thereby,\nwe can systematically generate graphs with different average\ndegrees and densities (which are seen in graph repositories like\nSNAP [38] and KONECT [49]). Second, for each combination\nof numbers of vertices and edges, we use nine combinations\nof values for a,b,candd. By doing so, we inﬂuence\nthe skewness of the degree distribution, the average local\n\nclustering coefﬁcient and the mean number of triangles. For\nall graphs, we ﬁxed dto 5%. We use 19% or 34% for c, which\nleads to different numbers of edges between community aand\nd. Now, we use different combinations for aandb. Larger\nvalues foralead to more skew in the degree distribution\nand a larger community a. Larger values for blead to more\nedges between aandd(inter-cluster edges). All nine R-MAT\nparameter combinations are shown in Table II.\nWe compare the graph properties of the 297 generated\ngraphs with real-world graphs. The violin plots in Figures 6a\nto 6e show the distribution of the graph property values along\nwith the minimal, maximal and median value of the real-\nworld and generated graphs. The graph properties of real-\nworld graphs are covered by the generated graphs to a large\nextent, which indicates good coverage of real-world graph\nproperties.\nIn Figure 6f, it is shown that the different combinations\nof values for a,b,canddinﬂuence the clustering coef-\nﬁcient of the generated graphs and how “easy” the graphs\ncan be partitioned. In the ﬁgure, multiple graphs are rep-\nresented as follows. All graphs have jEj= 160 M edges.\nEach line in the diagram represents how many vertices n2\nf222;223;224;225;226;227gthe graph contains. The markers\non the lines are the nine different parameter combinations of\nvalues fora,b,c, anddand therefore represent one single\ngraph each. On the y-axis, the clustering coefﬁcient of the\ngraphs is shown. On the x-axis, the replication factor achieved\nby partitioning the graph with the streaming partitioner High-\nDegree Replicated First (HDRF) [7] into 64partitions is\nshown. We observe a correlation between high clustering\ncoefﬁcients and low replication factors. Similar results were\nobserved for other graphs and partitioning algorithms.\nThe 297 R-MAT graphs are used to train PartitioningQual-\nityPredictor . However, the graphs are too small to mea-\nsure representative ﬁgures of the performance of distributed\ngraph processing. Hence, we additionally generated 180 larger\ngraphs with the same parameter combinations for the R-\nMAT generator from above (see Table II). These graphs\ncontain 1:8M to 50M vertices and 100M to 500M edges\n(see Table Ib). The larger graphs are used to train both\nProcessingTimePredictor andPartitioningTimePredictor .\nWe also attempted to use the Barabasi-Albert model [50]\nas graph generator (implementation of Hagberg et al. [51]).\nThe generator expects the total number of vertices jVjand\nthe number of edges mwhich are added for each new vertex.\nWe created 70 graphs with jVj= 1M andm2f1;2;:::; 70g\nto obtain graphs with average degrees between 2and140.\nThe average degree inﬂuences the replication factor). However,\nif we ﬁxmand changejVj, the replication factor does not\nchange for all partitioning algorithms. Therefore, it is not\npossible to create graphs with the same average degree that\nlead to different replication factors. In contrast to R-MAT, it is\nalso not possible to cover all the graph properties of real-world\ngraphs.\nIn conclusion, using R-MAT for graph generation yields\npromising results, while the Barabasi-Albert model is notTABLE I: Different combinations of the number of edges and\nvertices in the generated R-MAT graphs used for training of (a)\nPartitioningQualityPredictor and (b) ProcessingTimePredictor\nandPartitioningTimePredictor .\n(a) R-MAT-SMALL\njEj(M)jVj\n1 215,216,217,218,219\n40 221,222,223,224,225\n80 221,222,223,224,225,226\n120 222,223,224,225,226\n160 222,223,224,225,226,227\n200 222,223,224,225,226,227(b) R-MAT-LARGE\njEj(M)jVj(M)\n100 1.8, 2.5, 4, 10\n200 3.6, 5, 8, 20\n300 5.4, 7.5, 12, 30\n400 7.3, 10, 16, 40\n500 9.1, 12.5, 20, 50\nTABLE II: Nine combinations Cifor the R-MAT parameters\na, b, c and d used for generating training graphs.\nC1C2C3C4C5C6C7C8C9\na 0.35 0.45 0.55 0.60 0.40 0.50 0.60 0.65 0.70\nb 0.26 0.16 0.06 0.01 0.36 0.26 0.16 0.11 0.06\nc 0.34 0.34 0.34 0.34 0.19 0.19 0.19 0.19 0.19\nd 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\nﬂexible enough to generate a broad variety of graphs.\nB. Machine Learning Features\nFor all three prediction components of EASE , we use two\nkinds of features. (1) Common graph properties in order\nto map the graph to the closest graph(s) we have already\nprocessed and (2) other features which are speciﬁc to the\nprediction component.\n1) Graph Properties: In total we use three different sets of\ngraph properties as features (see Table III). In the following,\nwe show which features are used for which prediction task.\nPartitioningQualityPredictor uses the basic and advanced\ngraph properties as features. The rationale for using two\ndifferent feature sets is that the basic graph properties can\nbe easily computed if not known, compared to the advanced\ngraph properties which are more compute-intensive to obtain.\nHowever, the advanced features provide more insights into the\ncharacteristics of the graph and therefore may lead to a better\nprediction. In many graph dataset repositories [38], [42], [49],\nadvanced properties are precomputed. The basic features are\nthemean degree of the graph, the skewness of the in-degree\ndistribution , the skewness of the out-degree distribution and\nthedensity of the graph. These are common properties of\ngraphs, and it is expected that they provide insights about how\nwell a graph can be partitioned. In experimental studies [12],\n[28], the degree distribution was considered for the selection\nof the partitioner. Furthermore, it was observed that different\npartitioners can be sensitive to the skewness of the degree\ndistribution [7]. The advanced features are, in addition to the\nbasic features, the average number of triangles and the average\nlocal clustering coefﬁcient . It was observed that many real-\nworld networks have a community structure which leads to\na high clustering coefﬁcient [34]. This can be an interesting\nindicator for how well a graph can be partitioned. In [52],\nthe authors observed that their partitioner performed better (in\nterms of cut-size) on graphs with higher clustering coefﬁcients.\nTherefore, we use the advanced features to investigate how\n\nR-MAT RW BA0100mean degree(a)\nR-MAT RW BA0.00.5clustering coeff. (b)\nR-MAT RW BA0.02.55.0mean triangles1e4 (c)\nR-MAT RW BA0.00.5in-deg. skew (d)\nR-MAT RW BA0.00.5out-deg. skew (e)\n2.5 5.0 7.5\nreplication factor0.00.5clustering coeff.2^22\n2^23\n2^24\n2^25\n2^26\n2^27 (f)\nFig. 6: (a)-(e): Graph properties of generated R-MAT and Barabasi-Albert (BA) graphs and real-world (RW) graphs,\n(f) correlation between high clustering coefﬁcients and low replication factors.\nmuch they can improve the replication factor prediction.\nHowever, we have not found any literature that shows the\nadvanced features would inﬂuence the balancing. Therefore,\nwe use the advanced features only for the replication factor\nprediction and not for balancing prediction.\nProcessingTimePredictor only considers the simple features.\nMany graph processing algorithms perform computations for\neach edge or vertex. Therefore, the number of edges and\nvertices are the most important graph properties.\nFor the PartitioningTimePredictor , all feature sets are con-\nsidered. For the stateless partitioners, the simple features\nwill be most important as they represent the size of the\ngraph. However, for the more advanced partitioners, also the\nbasic and advanced properties can be important features. For\nexample, in HEP [11], the decision of how much of the graph\nis partitioned in-memory and how much in a streaming fashion\nis decided based on the mean degree. Therefore, the mean\ndegree inﬂuences the partitioning-runtime [11]. As another\nexample, 2PS performs clustering as a pre-processing step and\nsorts the clusters [21]. Therefore, the clustering coefﬁcient can\nbe an important feature.\n2) Other features: In addition to the graph properties,\nwe consider features which are speciﬁc to the prediction\ncomponent.\nPartitioningQualityPredictor always considers as speciﬁc\nfeatures the graph partitioner and the number of partitions.\nSome graph partitioners use a conﬁguration parameter that\ninﬂuences the partitioning quality. We treat different settings\nof partitioner-speciﬁc parameters as if they were separate\npartitioners. We showcase this in Section V for the HEP parti-\ntioner [11] by using different parameters for \u001c2f1;10;100g\nthat inﬂuence how many edges of the graph are partitioned\nin-memory and how many in a streaming fashion. The three\nsettings result in three different partitioners HEP-1, HEP-10\nand HEP-100.\nProcessingTimePredictor uses the partitioning quality met-\nrics (replication factor, edge balance, vertex balance, source\nbalance and destination balance) as speciﬁc features. For\neach graph processing algorithm, a separate model is trained.\nThereby, new graph processing algorithms can easily be added\nand trained individually.\nPartitioningTimePredictor considers the partitioner as spe-\nciﬁc feature. As for the quality prediction, we treat different\nsettings of partitioner-speciﬁc parameters as if they were\nseparate partitioners.C. Training\nFor each of the three prediction components, we com-\npare six supervised machine learning algorithms. Polynomial\nRegression is effective in predicting computational costs in\npractice [53]. Support Vector Regression (SVR) leads to a\nvery good performance and is considered as an important\ntechnique for regression tasks [54], [55]. Random Forest\nRegressor (RFR) is relatively robust to outliers and noise, can\neasily be parallelized and the importance of the used features\ncan be interpreted [56] - we leverage the interpretability of\nRFR in Section V-E. Extreme Gradient Boosting (XGB) leads\nto state-of-the-art results and is commonly used in well-known\nmachine learning competitions [57]. Decision tree ensembles\nsuch as RFR or XGB can compete and even outperform deep\nneural networks on tabular data [58]–[61]. K-nearest Neigh-\nbors Regressor (KNN) serves as a simple baseline. In addition\nto these traditional machine learning methods, we use a fully-\nconnected multi-layer perceptron (MLP) as a representative for\na feed-forward deep neural network.\nData Splitting: All models are trained on the syntheti-\ncally generated R-MAT graphs. PartitioningQualityPredictor\nis trained on the smaller 297 R-MAT graphs and Partition-\ningTimePredictor andProcessingTimePredictor are trained on\nthe larger 180 R-MAT graphs. The hyper-parameter tuning\nis performed for each machine learning algorithm on the\ntraining set by using 5-fold cross-validation [62]. The models\nare compared against each other based on the cross-validation\nscore and are retrained with the best hyper-parameter on the\ncomplete training set. Finally, the models are evaluated on the\ntest set, which consists only of real-world graphs.\nPreprocessing: We preprocess the generated data as fol-\nlows: The data was standardized with z-score normaliza-\ntion [63]. For PartitioningQualityPredictor andPartitioning-\nTimePredictor , one-hot encoding is used for the partitioning\nalgorithms to use them as a numerical feature.\nHyper-Parameter Search: For all six machine learning\nalgorithms, hyper-parameters can be tuned. We performed a\ngrid search for each machine learning algorithm to identify\nthe best hyper-parameters. Please ﬁnd the used combinations\nin our GitHub repository [64].\nTest & Enrichment: As the last step, our models are tested\non real-world graphs. In order to identify weaknesses of\nPartitioningQualityPredictor , the prediction performance is\nevaluated for different combinations of partitioning algorithm\nand graph type. If one combination does not perform well, but\nis relevant for the user, the training set can be enriched with\nreal-world graphs. For example, if a user mainly processes\n\nTABLE III: Overview which graph properties are used in which feature set and which features are used for which prediction\ncomponent. I is the number of iterations of a given graph processing algorithm (e.g., PageRank), k is the number of partitions.\nGraph Properties (Sec. II-B) Other Features (Sec. IV-B2)\nDescription jEjjVjMean\nDegree DensityIn-Deg.\nDist.Out-Deg.\nDist.Average\n#triangles LCCQuality Metrics\n(Sec. II-A) k Partitioner IOpt.\nGoal\nFeature Sets\nSimple 3 3\nBasic 3 3 3 3 3 3\nAdvanced 3 3 3 3 3 3 3 3\nPrediction Tasks and Partitioner Selector\nPartitioning Quality 3 3 3 3 3 3 3 3\nPartitioning Time 3 3 3 3 3 3 3\nProcessing Time 3 3 3 3\nSelector 3 3 3 3 3 3 3 3 3 3 3 3 3\nwiki graphs, but the predictions for this graph type are not\naccurate, the user can enrich the training data with real-world\nwiki graphs in order to improve the prediction performance.\nD. Inference\nIn order to make predictions for an unseen graph, for Par-\ntitioningQualityPredictor , the graph properties, the partitioner\nand the number of partitions need to be provided. For Process-\ningTimePredictor , the graph processing algorithm to execute,\nthe graph properties and the partitioning quality metrics need\nto be provided. In contrast to graph processing algorithms that\nrun until convergence, for graph processing algorithms that run\nfor a given number of iterations, also the number of iterations\nneeds to be provided. For PartitioningTimePredictor , the graph\nproperties and the partitioner is needed for inference. In order\nto automatically select a partitioner with PartitionerSelector ,\nthe optimization goal to either minimize the processing or the\nend-to-end time, needs to be provided. Table III shows which\nfeatures are needed for which prediction task.\nE. Discussion of Design Choices\nSeperate Machine Learning Models for Prediction Tasks:\nWe discuss two alternatives to our approach. First, instead of\npredicting the partitioning and processing run-time separately,\nit is possible to train a single model which directly predicts\nthe end-to-end time (Alternative 1). Second, for the processing\nrun-time prediction, the partitioner could be used as a feature\n(Alternative 2) instead of the partitioning quality metrics.\nTherefore, in Alternative 2 it is not necessary to predict\npartitioning quality metrics.\nWe decided against Alternative 1 for two reasons. First,\nit is less ﬂexible than our approach because only the end-\nto-end run-time can be predicted. Some users may only\nbe interested in minimizing the graph processing run-time,\ne.g., when partitioning can be performed ofﬂine on a cheap\ncompute node, while processing is running on an expensive\ncompute cluster, and therefore, only processing costs should\nbe minimized. Second, if a new partitioner is added or the\nprocessing framework is changed, the entire model needs\nto be retrained. With our approach, only the model for the\npartitioning run-time orthe processing run-time prediction\nneeds to be retrained in such a scenario.\nWe decided against Alternative 2 for the following reasons.\nFirst, with our approach, new partitioning algorithms can beincorporated with less overhead: If a new partitioner should be\nincluded, it simply needs to be executed on the training graphs\nto measure the quality metrics and the partitioning run-time.\nThen, PartitioningQualityPredictor and PartitioningTimePre-\ndictor can be retrained. ProcessingTimePredictor does not\nneed to be retrained, since a model already exists that predicts\nthe graph processing run-time based on the quality metrics. In\ncontrast, with Alternative 2, all graph processing algorithms\nwould need to be re-executed on the training graphs, since\nthe partitioner and not the partitioning quality is used as a\nfeature. Second, with our approach, additional insights can\nbe gained on how the partitioning quality metrics inﬂuence\nthe processing run-time. This can be useful for researchers to\nbetter understand for which graph processing algorithm which\nquality metrics are most important. This insight may be useful\nfor the design of new partitioning algorithms or optimizations\nof graph processing frameworks. Third, the replication factor\nprediction is also useful for memory-bound processing as each\nreplica produces a copy of the vertex state. For example, in\ngraph neural networks (GNNs), the vertex state can consist of\nthousands of features and memory is heavily restricted because\nthe processing is performed on GPUs. In such a scenario,\nthe predicted replication factor can be an important decision\ncriterion for selecting a partitioner.\nUse of Graph Embeddings as Features: It would also be\npossible to use complex graph features for the prediction\ntasks by computing graph embeddings with graph neural\nnetworks (GNNs) to represent a graph with low dimensional\nvectors [65]–[68]. In our experiments1, we measure the in-\nference time of a two-layer GraphSage GNN [66] with mean\npooling on a CPU and on two different GPUs for all graphs\nof Table IV. We observe that the inference time on the CPU is\n9 to 19 times the partitioning time of the slowest partitioner.\nTherefore, it is possible to run all partitioners and to measure\nthe actual metrics instead of predicting them, so we could\nnot amortize the high inference time end-to-end. Inference\non GPUs at this scale is heavily bounded by GPU memory.\nEven on a large GPU with 48 GB GPU memory, we can only\ncompute embeddings for the ﬁrst four graphs of Table IV and\nrun out of GPU memory for the remaining graphs. It will not\n1In our GitHub repository, we report all inference times on a CPU and two\ndifferent GPUs and compare them with EASE. Here, we summarize these\nresults.\n\nTABLE IV: Test set (real-world graphs) for Partitioning-\nTimePredictor andProcessingTimePredictor .\nGraph Edges (M) Vertices (M)\ncom-orkut.ungraph 117.2 3.1\nenwiki-2021 150.1 6.3\neu-2015-tpd 165.7 6.7\nhollywood-2011 229.0 2.0\nout.orkut-groupmemberships 327.0 8.7\neu-2015-host 379.7 11.3\ngsh-2015-tpd 581.2 30.8\npay off to use such heavy-weight processing just to optimize\nthe partitioning step of distributed graph processing. Therefore,\nwe do not use GNNs in our system.\nV. E VALUATION\nIn the following, we evaluate the ability of EASE to predict\nthe partitioning quality metrics, the partitioning run-time and\nthe processing run-time. We also compare EASE to manual\nheuristics in selecting a graph partitioner, such as random\nselection or selection of the partitioner that yields to the lowest\nreplication factor.\nA. Evaluation Metrics\nWe evaluate the machine learning models using evaluation\nmetrics that are commonly used for regression tasks. In the\nfollowing formulas, yiis the true value (e.g., the replication\nfactor) for the i-th sample, ^yiis the predicted value for the i-th\nsample,yis the mean value of all samples and \u000fis a small\nvalue in order to prevent divisions by zero.\n1) Root Mean Squared Error is deﬁned as\nRMSE (y;^y) =2q\n1\nnPn\u00001\ni=0(yi\u0000^yi)2. The closer the\nvalue is to 0, the better.\n2) Mean Absolute Percentage Error is deﬁned as\nMAPE (y;^y) =1\nnPn\u00001\ni=0jyi\u0000^yij\nmax(\u000f;jyij). The closer the\nvalue is to 0, the better.\nB. Test Set\nFor the evaluation of PartitioningQualityPredictor , we col-\nlected 175 real-world graphs from SNAP [38], KONECT [49]\nand the Network Data Repository [42]. The graphs can be\ncategorized into the following graph types: (1) 12 afﬁliation\ngraphs, (2) 3 citation graphs, (3) 6 collaboration graphs, (4) 5\ninteraction graphs, (5) 5 internet graphs, (6) one product\nnetwork, (7) 31 social networks, (8) 12 web graphs, (9) 101\nwiki graphs. All graphs except 96 of the 101 wiki graphs\nare used as the ﬁnal test set for all evaluations. The 96 wiki\ngraphs are later used to evaluate training data enrichment.\nFrom the same graph data repositories, we collected 7 real-\nworld graphs for the evaluation of PartitioningTimePredictor\nandProcessingTimePredictor (see Table IV). The links to all\ngraph datasets are provided in our GitHub repository [64].\nC. Training, Validation & Test\nData Generation: For all three prediction components\nwe measure the graph properties introduced in Section II-B\nof the training and the test graphs. We use 11 different\npartitioners of four categories. (1) Stateless streaming with1-dimensional hashing of destination vertices (1DD) and\nsource vertices (1DS) [5], 2-dimensional hashing (2D) [5],\ncanonical random vertex cut (CRVC) [5] and degree-based\nhashing (DBH) [16]. (2) Stateful streaming with high-\ndegree replicated ﬁrst (HDRF) [7] and two phase stream-\ning (2PS) [21]. (3) In-memory partitioning with Neighborhood\nExpansion (NE) [25]. (4) Hybrid partitioning with Hybrid\nEdge Partitioner (HEP) [11]. HEP uses a parameter \u001cto\ncontrol how many edges are partitioned in-memory and how\nmany in a streaming fashion [11]. As the authors suggest, we\nconﬁgured HEP with \u001c2f1;10;100gabbreviated as HEP-\n1, HEP-10, HEP-100 and treat them as different partitioners.\nTherefore, we have 11 partitioners in total. The selected\npartitioning algorithms are strong baselines in their respective\ncategory and claim to have a good balance in terms of\npartitioning quality and partitioning run-time. Therefore, all of\nthem are potential candidates to minimize end-to-end run-time\nand it is difﬁcult to manually decide on the optimal partitioning\nstrategy.\nForPartitioningQualityPredictor , we partition the graphs\nintok2K=f4;8;16;32;64;128gpartitions and measure\nthe ﬁve partitioning quality metrics discussed in Section II-A.\nForProcessingTimePredictor , we partition all graphs with\nall partitioners into 4 partitions and measure the partitioning\nquality metrics. Then, we execute 6 graph processing algo-\nrithms on a Spark/GraphX cluster with 4 machines: PageR-\nank (PR) for 10 iterations, Connected Components (CC),\nSingle Source Shortest Paths (SSSP) with a randomly selected\nseed vertex and K-Cores with k=deg(G). These algorithms\nare characteristic for different sorts of workloads. In PageR-\nank, all vertices are active in each iteration. In Connected\nComponents, in the ﬁrst iteration all vertices are active and\nthe number of active vertices decreases over time. In Single\nSource Shortest Paths, in the ﬁrst iteration only one vertex\nis active. In the following iterations, the number of active\nvertices ﬁrst increases and then decreases until no vertex is\nactive anymore. In K-cores, in the ﬁrst iteration many vertices\nare active and become inactive over time. In addition to these\n4 real-word algorithms, we implement a synthetic algorithm:\nEach vertex in the graph contains a feature vector with s64-bit\ndoubles and sends its feature vector along the outgoing edges\nin each iteration. With s, the amount of communication can\nbe inﬂuenced. We set sto 1 and to 10 and abbreviate these\ntwo conﬁgurations with Synthetic-Low and Synthetic-High for\nlow and high communication load, respectively. We set the\nnumber of iterations to 5. For both the synthetic workloads and\nPageRank, the computation and communication load remains\nconstant throughout the iterations and the number of iterations\nis a parameter of the algorithms. Therefore, we measure the\naverage iteration time which is also the prediction target.\nThe total processing run-time is the average iteration time\nmultiplied with the number of iterations. For the remaining\nalgorithms in which the computation and communication load\nchanges over time and which run until convergence, we\nmeasure the total processing time until convergence.\nFor the PartitioningTimePredictor , we partition all graphs\n\nTABLE V: Quality metrics for ProcessingTimePredictor on\ntest set (average across real-world graphs).\nAlgorithm Model MAPE\nConnected Components PolyRegression 0.272\nK-Cores XGB 0.401\nPageRank XGB 0.295\nSingle Source Shortest Paths XGB 0.300\nSynthetic-High PolyRegression 0.259\nSynthetic-Low PolyRegression 0.271\nwith all partitioners into 4 partitions and measure the parti-\ntioning run-time.\nTraining and Validation: For all three prediction com-\nponents, we train regression models with all six machine\nlearning algorithms mentioned in Section IV-C and select the\nbest model based on the 5-fold cross-validation score on the\nsynthetic graphs. In the following, we evaluate the selected\nmodels on the test set consisting of real-world graphs.\nTest: We evaluated PartitioningTimePredictor andProcess-\ningTimePredictor on the seven real-world graphs listed in Ta-\nble IV. For PartitioningTimePredictor we achieved a MAPE of\n0.335 on the test set with XGB. For ProcessingTimePredictor ,\nXGB led to a MAPE of 0.295, 0.401, 0.300 for PageRank,\nK-Cores and Single Source Shortest Paths, respectively (see\nTable V). For Connected Components, Synthetic-Low and\nSynthetic-High, Polynomial Regression led to a MAPE of\n0.272, 0.271, 0.259, respectively. We evaluated Partition-\ningQualityPredictor with the 80 real-world graphs mentioned\nin Section V-B. Table VI shows the results. RFR led to a\nMAPE of 0.152, 0.144, 0.079, 0.154 for vertex balance, source\nbalance, edge balance and destination balance, respectively. In\nterms of replication factor, XGB led to a MAPE of 0.296 for\nthe basic features and could be slightly improved to 0.288 by\nusing the advanced features. This means that the balancing\nmetrics can be predicted more accurately than the replication\nfactor. We further investigate for which combination of graph\ntype and partitioning algorithm the model works well and\nfor which combinations further training would be useful. The\nresults for the replication factor are shown in Figure 7a. We\nobserve that all partitioning algorithms achieve good scores for\nthe graph types citation ,interaction ,internet ,product-network\nandsocial . However, the predictions for the partitioners 2PS,\nHDRF, HEP and NE on the graph types collaboration ,web\nand wikis are not as good. Similar results are achieved for\nthe advanced features. The results for the vertex balance are\nshown in Figure 7c. The results for the other balancing metrics\nare omitted here due to space limitations, but are available in\nour GitHub repository [64].\nCompared to the replication factor prediction, the prediction\nresults for the vertex balance are less dependent on the graph\ntype, but more on the partitioning algorithm. It is notable\nthat for NE and HEP-100, accuracy is rather low. When\ninvestigating the problem in more detail, we found that this\nis likely related to the unstable behavior of the partitioners\nthemselves: We observed that when NE is executed multiple\ntimes on the same graph with the same number of partitions,\nthe vertex balance can heavily ﬂuctuate by a factor of up toTABLE VI: Quality metrics for PartitioningQualityPredictor\non test set (average across real-world graphs).\nTarget Model Features MAPE RMSE\nRF(P) XGB basic 0.296 1.197\nRF(P) XGB adv. 0.288 1.238\nBv(P) RFR basic 0.152 0.921\nBsrc(P) RFR basic 0.144 0.851\nBedge(P) RFR basic 0.079 1.055\nBdst(P) RFR basic 0.154 1.020\n2.1 between different runs. This is due to the random seed\nvertex selection in NE, which leads to different partitionings\nin different runs. HEP is, to a lesser extent, affected by the\nsame problem. Different from vertex balance, the replication\nfactor remains stable between different runs of NE and HEP,\nrespectively.\nD. Enrichment\nWe suggest to use real-world graphs to enrich the training\nset for PartitioningQualityPredictor when the model trained\nwith synthetic data shows weaknesses at speciﬁc combinations\nof graph types and partitioners.\nThe replication factor predictions for the partitioners 2PS,\nHDRF, HEP and NE on the graph types collaboration ,weband\nwikis show weaknesses when compared to the other combina-\ntions (see Figure 7a). Hence, we enrich the synthetic training\nset with up to 96 additional real-world wiki graphs ( Gwiki).\nIn order to investigate how many graphs are needed for the\nenrichment, we select subsets of 19, 38, 57, 76 and 96 of the\nwiki graphs ( Gselected ) for the enrichment. We use the same\ntest set as in Section V-B. Since we randomly select Gselected\nfromGwiki, we repeat each experiment three times and report\nthe mean quality metrics along with the standard deviation.\nFor the balancing metrics and replication factor, we use the\nRFR models of Table VI. XGB only leads to the slightly better\nMAPE of 0.296 vs. 0.303 when using the basic features, but\nthe training time per enrichment level takes much longer for\nXGB than for RFR (140 minutes vs. 1 minute).\nThe results for an enrichment of 96 real-world graphs are\nshown in Figure 7b for each combination of partitioning\nalgorithm and graph type. In Figure 8, the inﬂuence of the\nsize of the enrichment data set on the quality metrics is shown.\nThe main observations for the replication factor prediction are\nas follows:\n(1) When enriching with all 96 real-world graphs, MAPE\nfor the wiki graphs decreases from 0.555 to 0.244 and from\n0.566 to 0.178 for the basic features and the advanced features,\nrespectively. Therefore, through enrichment, the weakness of\nthe synthetically trained model for the wiki graphs could be\nreduced.\n(2) The advanced features lead to a better MAPE (0.178 vs.\n0.242) with enrichment. Therefore, when using enrichment, it\nis worth to consider the advanced features. A possible expla-\nnation for this observation is that through enrichment, wiki\ngraphs with more realistic values for the advanced features\nare used in the training and therefore, the unseen graphs can\nbe mapped to them better.\n\n1dd\n1ds\n2d\n2ps\ncrvc\ndbh\nhdrf\nhep1\nhep10\nhep100\nne\npartitioneraffiliation\ncitation\ncollaboration\ninteraction\ninternet\nproduct_network\nsoc\nweb\nwikitype0.34 0.37 0.34 0.20.36 0.26 0.22 0.24 0.20.21 0.19\n0.10.05 0.16 0.18 0.20.35 0.26 0.13 0.33 0.28 0.29\n0.12 0.12 0.15 0.65 0.26 0.11 0.44 0.55 0.76 0.72 0.77\n0.27 0.18 0.17 0.22 0.23 0.22 0.24 0.24 0.23 0.21 0.2\n0.16 0.11 0.16 0.26 0.25 0.21 0.23 0.20.31 0.26 0.21\n0.18 0.15 0.22 0.21 0.23 0.18 0.11 0.10.37 0.32 0.33\n0.13 0.18 0.15 0.22 0.16 0.18 0.20.25 0.30.32 0.33\n0.16 0.17 0.15 0.86 0.15 0.14 0.64 0.74 0.91 0.84 0.73\n0.15 0.27 0.23 0.77 0.2 0.20.35 0.83 1.1 1 1\n0.00.20.40.60.81.0(a) Replication factor.\n1dd\n1ds\n2d\n2ps\ncrvc\ndbh\nhdrf\nhep1\nhep10\nhep100\nne\npartitioneraffiliation\ncitation\ncollaboration\ninteraction\ninternet\nproduct_network\nsoc\nweb\nwikitype0.36 0.33 0.33 0.20.34 0.31 0.22 0.18 0.19 0.18 0.18\n0.08 0.07 0.14 0.17 0.21 0.31 0.24 0.12 0.34 0.30.32\n0.12 0.14 0.18 0.58 0.24 0.08 0.42 0.5 0.70.68 0.7\n0.19 0.16 0.12 0.20.21 0.18 0.18 0.17 0.16 0.14 0.14\n0.17 0.11 0.15 0.29 0.24 0.21 0.25 0.21 0.32 0.27 0.22\n0.15 0.14 0.22 0.23 0.23 0.16 0.13 0.11 0.39 0.34 0.35\n0.13 0.19 0.16 0.19 0.15 0.19 0.20.18 0.23 0.26 0.27\n0.17 0.18 0.17 0.65 0.17 0.15 0.48 0.56 0.67 0.59 0.55\n0.15 0.21 0.17 0.29 0.17 0.13 0.24 0.29 0.41 0.31 0.33\n0.00.20.40.60.81.0 (b) Replication factor (with enrichment).\n1dd\n1ds\n2d\n2ps\ncrvc\ndbh\nhdrf\nhep1\nhep10\nhep100\nne\npartitioneraffiliation\ncitation\ncollaboration\ninteraction\ninternet\nproduct_network\nsoc\nweb\nwikitype0.23 0.01 0.08 0.10.02 00.06 0.14 0.26 0.27 0.28\n0.07 0.02 0.02 0.15 00.01 0.02 0.14 0.17 0.23 0.35\n0.04 0.03 0.05 0.20.03 0.04 0.09 0.10.24 0.39 0.39\n0.04 0.06 0.03 0.15 0.01 0.01 0.04 0.22 0.31 0.34 0.49\n0.12 0.21 0.12 0.13 0.04 0.02 0.10.08 0.30.39 0.68\n0.01 0.01 0.04 0.06 0 00.01 0.08 0.28 0.43 0.44\n0.07 0.13 0.05 0.18 0.01 0.01 0.08 0.12 0.24 0.37 0.44\n0.13 0.06 0.04 0.22 0.01 00.13 0.13 0.22 0.39 0.46\n0.17 0.03 0.02 0.28 0.02 0.01 0.08 0.21 0.27 0.50.51\n0.00.20.40.60.81.0 (c) Vertex balance.\nFig. 7: MAPE scores for replication factor and vertex balance predictions.\n0 19 38 57 76 96\n# graphs0.20.30.40.5MAPE\nall\nrealworld-affiliation\nrealworld-citation\nrealworld-collaboration\nrealworld-interactionrealworld-internet\nrealworld-product_network\nrealworld-soc\nrealworld-web\nrealworld-wiki\nFig. 8: Mean MAPE along with the standard deviation for\ndifferent graph types with different enrichment levels.\n(3) As can be seen in Figure 8, the larger the enrichment,\nthe better the prediction. However, even with an enrichment of\nonly 19 graphs, the prediction quality can be increased a lot\n(improvement of MAPE from 0.555 to 0.370). In other words,\nalready a small number of real-world graphs in the training\ndata set can improve the prediction for “weak spots” of the\nmodel.\n(4) In addition to the wiki graphs, also the prediction error\nfor other graph types (especially web graphs) was decreased\nby the enrichment or at least kept stable (see Figure 8). It is\nexpected that the wiki graphs are similar to web graphs and\ntherefore insights gained at the basis of the wiki graphs can\nbe transferred to web graphs.\nThe prediction performance for the balancing metrics could\nalso be improved for the wiki graphs by enrichment. On\naverage, MAPE slightly decreases from 0.156 to 0.114 for\nthe wiki graphs. However, the overall MAPE for all graph\ntypes slightly increases from 0.132 to 0.154. Overall, balanc-\ning predictions are already remarkably accurate even without\nenrichment. Hence, enrichment is less effective in this case.\nE. Feature Importance\nIn order to understand how important the different features\nare for the prediction of the partitioning quality metrics, we\ncalculate the feature importance for each feature for the RFR\nmodel. A random forest is an ensemble of multiple decision\ntrees. The decision trees split all samples recursively into\nsmaller subsets based on the features. A purity metric is used\nin order to decide which feature to use for splitting a node. InTABLE VII: Feature importance for quality metrics.\nFeature RF Bv Bsrc Bdst Bedge\nPartitioner 0.299 0.268 0.245 0.542 0.244\nMean Degree 0.274 0.065 0.059 0.058 0.036\n#Partitions 0.256 0.271 0.293 0.177 0.472\nDegree Distr. 0.165 0.368 0.372 0.214 0.214\nDensity 0.007 0.028 0.031 0.009 0.034\nthe machine learning framework we used, the mean squared\nerror is used as the purity metric [69]. The importance of a\nfeature is expressed by how much the purity is increased by\nsplitting a node based on that feature, weighted by how many\nsamples are contained in that node divided by the total number\nof samples.\nTable VII lists the feature importance of the basic features\nfor all partitioning quality metrics. We make the following\nobservations: First, both the partitioning algorithm and the\nnumber of partitions are very important for all partitioning\nquality metrics, ranging from 0.244 to 0.542 and from 0.177\nto 0.477, respectively. Second, the degree distribution is an\nimportant feature with 0.165 for the replication factor and\na very important feature (0.214 to 0.372) for the balancing\nmetrics. Third, the mean degree is an important feature (0.274)\nfor the replication factor and a less import feature (0.036 to\n0.065) for the balancing metrics. The density is for all quality\nmetrics the least important feature (0.007 to 0.034).\nThese results seem reasonable. First of all, it is expected that\nboth, the partitioning algorithm and the number of partitions,\nare important to the machine learning model. Second, it seems\nplausible that the mean degree is important for the replication\nfactor. We observed for all partitioning algorithms, that the\nhigher the average degree, the higher the replication factor.\nThird, the importance of the degree distribution is also ex-\npected. It was already observed that the degree distribution can\ninﬂuence partitioning algorithms [7], [21]. Interestingly, the\ndensity is not important. Probably, the mean degree captures\nsimilar characteristics and no additional density feature is\nnecessary. Therefore, the density could be discarded from the\nfeature set.\nF . Automatic Partitioner Selection\nIn the following, we evaluate how well PartitionerSelector\ncan automatically select a partitioner based on the predic-\n\nTABLE VIII: Comparison of different partitioner selection\nstrategies:SPS,SO,SSRF,SRandSWrepresent the time\nto which P artitionerS elector (our approach), the o ptimal par-\ntitioner, the partitioner with the s mallest r eplication f actor, a\nrandomly selected partitioner and the w orst partitioner lead.\nWe differentiate between two selection goals: minimize the\nend-to-end time (E2E) or the processing time (Pro.).\n(a) No Enrichment\nGoal AlgorithmSPSin % of baselines SSRF in %\nofSO SOSSRFSRSW\nE2E SSSP 102 69 84 67 152\nE2E CC 103 58 76 57 184\nE2E PR 110 96 96 79 119\nE2E K-Cores 111 76 91 73 152\nE2E Synthetic-High 113 98 92 73 119\nE2E Synthetic-Low 117 99 95 76 121\nPro. SSSP 106 93 94 80 117\nPro. CC 107 92 91 74 121\nPro. PR 111 99 96 79 116\nPro. K-Cores 113 94 97 80 123\nPro. Synthetic-High 114 99 92 72 117\nPro. Synthetic-Low 119 101 96 76 120\n(b) Enrichment\nGoal Enrich.SPSin % of baselines\n(Enwiki-2021)SPSin % of baselines\n(All Graphs)\nSOSRSWSOSRSW\nE2E No 113 94 78 110 89 71\nE2E Yes 107 89 74 112 91 72\nPro. No 111 91 73 114 96 78\nPro. Yes 106 88 71 117 98 80\ntions of PartitioningQualityPredictor ,PartitioningTimePredic-\ntorandProcessingTimePredictor to minimize the end-to-end\ntime and the graph processing time. We compare PartitionerS-\nelector (SPS) against four baseline strategies: (1) Select the\nworst partitioner ( SW), (2) select a r andom partitioner ( SR),\nselect the o ptimal partitioner ( SO) and (4) select the partitioner\nwith the s mallest r eplication f actor (SSRF).\nOur main observations are as follows. In 26.2% and 35.7%\nof the cases, SPSselects the optimal partitioner out of the 11\npartitioners to minimize the processing and end-to-end time,\nrespectively. In comparison, manual selection strategies like\nSSRF andSRfall behind.SSRF selects in 7.1% and 14.3% of\nthe cases the best partitioner in terms of processing and end-to-\nend time, respectively. SRselects only in 9.1% ( =1\n11\u0001100% )\nof the cases the best partitioner for both optimization goals.\nFurther, we analyze the processing time and the end-to-\nend time to which the selection of SPSlead compared to\nthe four baselines. Table VIIIa (see columns 3-6) shows both\nthe processing and end-to-end time of SPS’s selection in\npercentage of the respective baseline for each graph processing\nalgorithm (lower is better).\nWorst selection (SW):SPSleads to a processing time\nwhich is on average between 72% (Synthetic-High) and 80%\n(SSSP) ofSWand to an end-to-end time which is on average\nbetween 57% (CC) and 79% (PR) of SW(see column SW\nin Table VIIIa). Therefore, SPSleads in all cases to a much\nbetter performance.\nRandom selection (SR):The end-to-time of SPSis on\naverage between 76% (CC) to 96% (PR) of SRand the\nSPS\nSSRF\nhep100\nhep10\nne\ndbh\nhep1\n2ps\nhdrf\n2d\n1dd\n1ds\ncrvc\nPartitioner02000end-to-end \n run-time (s)(a)SPSandSSRF select HEP-\n100.\nSPS\ndbh\nhdrf\n1ds\nhep1\n2d\nhep10\n1dd\nSSRF\nhep100\ncrvc\n2ps\nne\nPartitioner050100end-to-end \n run-time (s)(b)SPSselects DBH. SSRF se-\nlects HEP-100.\nFig. 9: End-to-End time comparison for different partitioners\nand the selection strategies SPSandSSRF on the graph\nEnwiki-2020 for (a) Synthetic-High and (b) CC.\nprocessing run-time is on average between 91% (CC) to 97%\n(K-cores) of SR(see column SRin Table VIIIa). Therefore,\nSPSalso outperforms random selection.\nOptimal selection (SO):Compared to the optimal selection\nSO,SPSleads to an end-to-end time which is on average\nbetween 102% (SSSP) and 117% (Synthetic-Low) of SOand\nto a processing time which is on average between 106%\n(SSSP) and 119% (Synthetic-Low) of SO(see column SO\nin Table VIIIa). Therefore, for the algorithms SSSP, CC\nand PR, the selection is relatively close to the optimum for\nboth optimization goals, while for K-Cores and the synthetic\nalgorithms there is more room for improvement.\nSmallest replication factor (SSRF):SPSleads to an end-to-\nend time which is on average between 58% (CC) and 99%\n(Synthetic-Low) of SSRF. The processing time is between\n92% (CC) and 101% (Synthetic-Low) of SSRF (see column\nSSRF in Table VIIIa). Only in one case (for the algorithm\nSynthetic-Low) SPSleads to a graph processing time which\nis 101% ofSSRF.\nWe further compare SSRF with the optimal strategy SO. In\nthe last column of Table VIIIa, we report the processing and\nend-to-end time of SSRF’s selection in percentage of SOfor\neach graph processing algorithm.We observe that even if the\nreplication factor of all partitioners would be known, it is not\nsufﬁcient for partitioner selection. First of all, in addition to the\nreplication factor, the balancing metrics are important. Second,\nthe partitioning time is important and there are cases where\nit is not worth to invest much time for partitioning to achieve\nthe smallest replication factor, as it can not be amortized\nby a lower graph processing time. This can be seen in\nFigure 9: HEP-100 achieves the smallest replication factor and\nminimizes the end-to-end time for the communication-bound\nalgorithm Synthetic-High (see. Table 9a). The partitioning time\ncan be amortized. However, for CC, fast partitioning with\nDBH leads to the lowest end-to-end time (see Table 9b). SPS\ntakes into account both partitioning and processing time, and\nhence, makes a good choice in a wide range of workloads. We\nfurther want to point out that SSRF is a rather hypothetical\nstrategy, as the replication factor is usually not known before\na graph has been partitioned.\nWe also investigated whether through enrichment with the\nwiki graphs, the performance of SPSforenwiki-2021 can be\nimproved. We used the same random forest models to predict\n\nthe partitioning quality with and without enrichment as in\nthe experiments in Section V-D. Compared to SPSwithout\nenrichment, SPSwith enrichment reduces the processing and\nend-to-end time for enwiki-2021 on average by 4% and 5%,\nrespectively. Evaluated on allgraphs, the selection of SPSwith\nenrichment increases the processing and end-to-end time on\naverage by 2% and 3%, respectively. In Table VIIIb, we report\nthe average processing and end-to-end time of SPS’s selection\n(with and without enrichment) in percentage of the respective\nbaseline evaluated on enwiki-2021 (see columns 3-5) and\nevaluated on allgraphs (see columns 6-8). Improvements\nin the target category are larger than the degradation of\nperformance in other categories, so that enrichment pays off\nif the majority of workloads concern the enriched graph type.\nSummary: Our experiments showed that EASE can provide\naquantitative prediction of the partitioning run-time and\npartitioning quality for different types of graph partitioning\nalgorithms. On that basis, EASE is able to predict the run-\ntime of graph processing algorithms with different workload\ncharacteristics. We achieved a high prediction accuracy by\nsolely training with synthetic graphs which can be improved\nby enriching the training data with real-world graphs. Based on\nrun-time predictions, we provide automatic graph partitioner\nselection to either minimize the graph processing or the\nend-to-end run-time. In both cases, the automatic selection\noutperforms manual selection heuristics.\nVI. R ELATED WORK\nIn the past years, experimental studies [12], [28]–[30], [70]\nhave been conducted that provide qualitative insights into the\ncharacteristics of different graph partitioning algorithms to\nhelp selecting one. However, the studies are not sufﬁcient for\nautomatic partitioner selection for a given scenario and do not\ninclude an automatic method to incorporate new partitioning\nand graph processing algorithms. In our work, we provide a\nquantitative prediction of the expected partitioning quality, the\npartitioning and graph processing time that is easily extensible\nto include new partitioning and graph processing algorithms.\nLots of research has been conducted in the ﬁeld of graph\ngenerators (e.g., [34], [43], [45], [46], [50], [71]–[73]) to\ngenerate synthetic graphs with real-world properties. Graph\ngenerators are an important component of our approach and we\npropose to select a suitable graph generator that can generate\nrepresentative graphs which are expected as a workload and\nfor which the partitioning quality, the partitioning and graph\nprocessing run-time should be predicted. In our experiments,\nwe used R-MAT as a domain-agnostic general graph generator\n[46] which can create graphs with real-world properties [45].\nHowever, further graph generators such as [73]–[75] can be\nexplored for our approach in the future.\nMany graph partitioning algorithms have been developed\nand can be categorized into (1) streaming algorithms (2) in-\nmemory algorithms and (3) hybrid partitioning algorithms.\nStreaming partitioning algorithms [4], [5], [13], [14], [16]–\n[23], [76] stream the graph, e.g., as an edge list and assignedges (vertex-cut) or vertices (edge-cut) on the ﬂy to partitions.\nStreaming algorithms can be stateless or stateful. In-memory\npartitioners [8], [9], [24]–[27], [77] load the complete graph\ninto memory to perform partitioning. Therefore, in contrast\nto streaming partitioning, a complete view of the graph is\navailable for partitioning. This leads in many cases to lower\nreplication factors, but the partitioning run-time and memory\noverhead may be higher. Hybrid edge partitioning [11] is a\ncombination of in-memory and streaming partitioning. One\npart of the graph is partitioned in-memory and the remaining\npart in a streaming fashion. In the evaluation we showed\nthat the partitioning quality and run-time of representative\npartitioning algorithms of each category could be predicted\nby our system. We also showed on the example of HEP that\nour approach can handle partitioners that use a parameter that\ninﬂuences the partitioning quality and the run-time. This could\nalso be applied to other partitioners [15], [18] that have a\nsimilar parameter.\nFan et al. [78] proposed an approach to incrementalize\nvertex-cut and edge-cut partitioners. Our approach can be used\nto select a partitioner, which can then be incrementalized. Fan\net al. [79] further studied dynamic scaling for parallel graph\nprocessing. In order to scale in or out, the graph often needs to\nbe re-partitioned. Our approach can be applied to predict how\nthe partitioning quality metrics, the partitioning and processing\nrun-time will change.\nFan et al. [53] proposed an application-driven partitioning\nstrategy. They showed how for a graph processing task, a given\nvertex or edge partitioning can be reﬁned and transformed to\na hybrid partitioning. Our approach can be used to support the\nselection of the initial partitioning which can then be reﬁned\nand transformed to a hybrid partitioning.\nThere are several works for using machine learning to opti-\nmize data management systems, e.g., to learn index structures\n[80], cardinality estimation [81] or for automatic conﬁguration\ntuning [82]. Our work is among the ﬁrst approaches to use\nmachine learning to optimize graph processing systems.\nVII. C ONCLUSIONS\nIn order to enable distributed graph processing, a graph\nneeds to be partitioned. However, the plethora of graph par-\ntitioning algorithms makes it a challenging task to select a\npartitioner for a given scenario. In this paper, we propose a\nmachine learning-based approach to predict the partitioning\nquality, the partitioning run-time and the graph processing run-\ntime for different types of partitioning algorithms and graph\nprocessing algorithms on unseen graphs. We show that such\na quantitative prediction enables automatically selecting the\npartitioner that minimizes the graph processing or end-to-end\ntime.\nIn future work, we plan to extend our approach to hyper-\ngraph partitioning algorithms [83]–[87] and systems [88], [89].\nAcknowledgements: This work is funded in part by the\nDeutsche Forschungsgemeinschaft (DFG, German Research\nFoundation) - 438107855.\n\nREFERENCES\n[1] G. Malewicz, M. H. Austern, A. J. Bik, J. C. Dehnert, I. Horn,\nN. Leiser, and G. Czajkowski, “Pregel: A system for large-scale graph\nprocessing,” in Proceedings of the 2010 ACM SIGMOD International\nConference on Management of Data , ser. SIGMOD ’10. New York,\nNY , USA: Association for Computing Machinery, 2010. [Online].\nAvailable: https://doi.org/10.1145/1807167.1807184\n[2] A. Ching, S. Edunov, M. Kabiljo, D. Logothetis, and S. Muthukrishnan,\n“One trillion edges: Graph processing at facebook-scale,” Proc.\nVLDB Endow. , vol. 8, no. 12, Aug. 2015. [Online]. Available:\nhttps://doi.org/10.14778/2824032.2824077\n[3] J. E. Gonzalez, Y . Low, H. Gu, D. Bickson, and C. Guestrin, “Pow-\nergraph: Distributed graph-parallel computation on natural graphs,” in\nProceedings of the 10th USENIX Conference on Operating Systems\nDesign and Implementation , ser. OSDI’12. USA: USENIX Association,\n2012.\n[4] R. Chen, J. Shi, Y . Chen, and H. Chen, “Powerlyra: Differentiated\ngraph computation and partitioning on skewed graphs,” in Proceedings\nof the Tenth European Conference on Computer Systems , ser. EuroSys\n’15. New York, NY , USA: Association for Computing Machinery,\n2015. [Online]. Available: https://doi.org/10.1145/2741948.2741970\n[5] J. E. Gonzalez, R. S. Xin, A. Dave, D. Crankshaw, M. J. Franklin, and\nI. Stoica, “Graphx: Graph processing in a distributed dataﬂow frame-\nwork,” in Proceedings of the 11th USENIX Conference on Operating\nSystems Design and Implementation , ser. OSDI’14. USA: USENIX\nAssociation, 2014.\n[6] J. Webber, “A programmatic introduction to neo4j,” in Proceedings of\nthe 3rd Annual Conference on Systems, Programming, and Applications:\nSoftware for Humanity , ser. SPLASH ’12. New York, NY , USA:\nAssociation for Computing Machinery, 2012. [Online]. Available:\nhttps://doi.org/10.1145/2384716.2384777\n[7] F. Petroni, L. Querzoni, K. Daudjee, S. Kamali, and G. Iacoboni,\n“Hdrf: Stream-based partitioning for power-law graphs,” in Proceedings\nof the 24th ACM International on Conference on Information and\nKnowledge Management , ser. CIKM ’15. New York, NY , USA:\nAssociation for Computing Machinery, 2015. [Online]. Available:\nhttps://doi.org/10.1145/2806416.2806424\n[8] M. Hanai, T. Suzumura, W. J. Tan, E. Liu, G. Theodoropoulos,\nand W. Cai, “Distributed edge partitioning for trillion-edge graphs,”\nProc. VLDB Endow. , vol. 12, no. 13, Sep. 2019. [Online]. Available:\nhttps://doi.org/10.14778/3358701.3358706\n[9] C. Martella, D. Logothetis, A. Loukas, and G. Siganos, “Spinner: Scal-\nable graph partitioning in the cloud,” in 2017 IEEE 33rd International\nConference on Data Engineering (ICDE) , April 2017, pp. 1083–1094.\n[10] F. Bourse, M. Lelarge, and M. V ojnovic, “Balanced graph edge\npartition,” in Proceedings of the 20th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining , ser. KDD ’14.\nNew York, NY , USA: Association for Computing Machinery, 2014,\np. 1456–1465. [Online]. Available: https://doi.org/10.1145/2623330.\n2623660\n[11] R. Mayer and H.-A. Jacobsen, “Hybrid edge partitioner: Partitioning\nlarge power-law graphs under memory constraints,” in Proceedings\nof the 2021 International Conference on Management of Data , ser.\nSIGMOD/PODS ’21. New York, NY , USA: Association for Computing\nMachinery, 2021. [Online]. Available: https://doi.org/10.1145/3448016.\n3457300\n[12] A. Pacaci and M. T. ¨Ozsu, “Experimental analysis of streaming\nalgorithms for graph partitioning,” in Proceedings of the 2019\nInternational Conference on Management of Data , ser. SIGMOD ’19.\nNew York, NY , USA: Association for Computing Machinery, 2019.\n[Online]. Available: https://doi.org/10.1145/3299869.3300076\n[13] C. Tsourakakis, C. Gkantsidis, B. Radunovic, and M. V ojnovic,\n“Fennel: Streaming graph partitioning for massive scale graphs,”\ninProceedings of the 7th ACM International Conference on Web\nSearch and Data Mining , ser. WSDM ’14. New York, NY , USA:\nAssociation for Computing Machinery, 2014. [Online]. Available:\nhttps://doi.org/10.1145/2556195.2556213\n[14] I. Stanton and G. Kliot, “Streaming graph partitioning for large\ndistributed graphs,” in Proceedings of the 18th ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining ,\nser. KDD ’12. New York, NY , USA: Association for Computing\nMachinery, 2012, p. 1222–1230. [Online]. Available: https://doi.org/10.\n1145/2339530.2339722[15] L. Hoang, R. Dathathri, G. Gill, and K. Pingali, “Cusp: A customizable\nstreaming edge partitioner for distributed graph analytics,” in 2019 IEEE\nInternational Parallel and Distributed Processing Symposium (IPDPS) ,\nMay 2019, pp. 439–450.\n[16] C. Xie, L. Yan, W.-J. Li, and Z. Zhang, “Distributed power-law graph\ncomputing: Theoretical and empirical analysis.” in Nips, vol. 27, 2014,\npp. 1673–1681.\n[17] N. Jain, G. Liao, and T. L. Willke, “Graphbuilder: Scalable graph\netl framework,” in First International Workshop on Graph Data\nManagement Experiences and Systems , ser. GRADES ’13. New York,\nNY , USA: Association for Computing Machinery, 2013. [Online].\nAvailable: https://doi.org/10.1145/2484425.2484429\n[18] H. P. Sajjad, A. H. Payberah, F. Rahimian, V . Vlassov, and S. Haridi,\n“Boosting vertex-cut partitioning for streaming graphs,” in 2016 IEEE\nInternational Congress on Big Data (BigData Congress) , 2016, pp. 1–8.\n[19] J. Nishimura and J. Ugander, “Restreaming graph partitioning: Simple\nversatile algorithms for advanced balancing,” in Proceedings of\nthe 19th ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining , ser. KDD ’13. New York, NY , USA:\nAssociation for Computing Machinery, 2013. [Online]. Available:\nhttps://doi.org/10.1145/2487575.2487696\n[20] C. Mayer, R. Mayer, M. A. Tariq, H. Geppert, L. Laich, L. Rieger,\nand K. Rothermel, “Adwise: Adaptive window-based streaming edge\npartitioning for high-speed graph processing,” in 2018 IEEE 38th\nInternational Conference on Distributed Computing Systems (ICDCS) ,\nJuly 2018, pp. 685–695.\n[21] R. Mayer, K. Orujzade, and H. Jacobsen, “2ps: High-quality edge\npartitioning with two-phase streaming,” CoRR , vol. abs/2001.07086,\n2020. [Online]. Available: https://arxiv.org/abs/2001.07086\n[22] R. Mayer, K. Orujzade, and H.-A. Jacobsen, “Out-of-core edge parti-\ntioning at linear run-time,” in 2022 IEEE 38th International Conference\non Data Engineering (ICDE) , 2022, pp. 2629–2642.\n[23] M. A. K. Patwary, S. Garg, and B. Kang, “Window-based streaming\ngraph partitioning algorithm,” in Proceedings of the Australasian\nComputer Science Week Multiconference , ser. ACSW 2019. New\nYork, NY , USA: Association for Computing Machinery, 2019. [Online].\nAvailable: https://doi.org/10.1145/3290688.3290711\n[24] G. Karypis and V . Kumar, “Parallel multilevel k-way partitioning\nscheme for irregular graphs,” in Proceedings of the 1996 ACM/IEEE\nConference on Supercomputing , ser. Supercomputing ’96. USA: IEEE\nComputer Society, 1996. [Online]. Available: https://doi.org/10.1145/\n369028.369103\n[25] C. Zhang, F. Wei, Q. Liu, Z. G. Tang, and Z. Li, “Graph\nedge partitioning via neighborhood heuristic,” in Proceedings of\nthe 23rd ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining , ser. KDD ’17. New York, NY , USA:\nAssociation for Computing Machinery, 2017. [Online]. Available:\nhttps://doi.org/10.1145/3097983.3098033\n[26] D. Margo and M. Seltzer, “A scalable distributed graph partitioner,”\nProc. VLDB Endow. , vol. 8, no. 12, Aug. 2015. [Online]. Available:\nhttps://doi.org/10.14778/2824032.2824046\n[27] S. Schlag, C. Schulz, D. Seemaier, and D. Strash, Scalable Edge\nPartitioning , 2019, pp. 211–225.\n[28] S. Verma, L. M. Leslie, Y . Shin, and I. Gupta, “An experimental\ncomparison of partitioning strategies in distributed graph processing,”\nProc. VLDB Endow. , vol. 10, no. 5, Jan. 2017. [Online]. Available:\nhttps://doi.org/10.14778/3055540.3055543\n[29] Z. Abbas, V . Kalavri, P. Carbone, and V . Vlassov, “Streaming graph\npartitioning: An experimental study,” Proc. VLDB Endow. , vol. 11,\nno. 11, Jul. 2018. [Online]. Available: https://doi.org/10.14778/3236187.\n3236208\n[30] G. Gill, R. Dathathri, L. Hoang, and K. Pingali, “A study of\npartitioning policies for graph analytics on large-scale distributed\nplatforms,” Proc. VLDB Endow. , vol. 12, no. 4, Dec. 2018. [Online].\nAvailable: https://doi.org/10.14778/3297753.3297754\n[31] V . Md, S. Misra, G. Ma, R. Mohanty, E. Georganas, A. Heinecke,\nD. Kalamkar, N. K. Ahmed, and S. Avancha, “Distgnn: Scalable\ndistributed training for large-scale graph neural networks,” in\nProceedings of the International Conference for High Performance\nComputing, Networking, Storage and Analysis , ser. SC ’21. New\nYork, NY , USA: Association for Computing Machinery, 2021. [Online].\nAvailable: https://doi.org/10.1145/3458817.3480856\n[32] S. Fortunato, “Community detection in graphs,” Physics Reports ,\n\nvol. 486, no. 3, pp. 75–174, 2010. [Online]. Available: https:\n//www.sciencedirect.com/science/article/pii/S0370157309002841\n[33] U. Brandes and T. Erlebach, Eds., Network Analysis . Berlin, Germany:\nSpringer-Verlag, 2005.\n[34] D. J. Watts and S. H. Strogatz, “Collective dynamics of small-world\nnetworks,” Nature , vol. 393, no. 6684, pp. 440–442, 1998.\n[35] M. E. J. Newman, “The structure and function of complex networks,”\nSIAM Review , vol. 45, no. 2, pp. 167–256, 2003. [Online]. Available:\nhttps://doi.org/10.1137/S003614450342480\n[36] L. Page, S. Brin, R. Motwani, and T. Winograd, “The pagerank citation\nranking: Bringing order to the web.” Stanford InfoLab, Tech. Rep., 1999.\n[37] U. N. Raghavan, R. Albert, and S. Kumara, “Near linear time algorithm\nto detect community structures in large-scale networks,” Physical review\nE, vol. 76, no. 3, p. 036106, 2007.\n[38] J. Leskovec and A. Krevl, “SNAP Datasets: Stanford large network\ndataset collection,” http://snap.stanford.edu/data, Jun. 2014.\n[39] P. Boldi, B. Codenotti, M. Santini, and S. Vigna, “Ubicrawler: A scalable\nfully distributed web crawler,” Software: Practice & Experience , vol. 34,\nno. 8, pp. 711–726, 2004.\n[40] P. Boldi and S. Vigna, “The WebGraph framework I: Compression\ntechniques,” in Proc. of the Thirteenth International World Wide Web\nConference (WWW 2004) . Manhattan, USA: ACM Press, 2004, pp.\n595–601.\n[41] P. Boldi, M. Rosa, M. Santini, and S. Vigna, “Layered label propaga-\ntion: A multiresolution coordinate-free ordering for compressing social\nnetworks,” in Proceedings of the 20th international conference on World\nWide Web , S. Srinivasan, K. Ramamritham, A. Kumar, M. P. Ravindra,\nE. Bertino, and R. Kumar, Eds. ACM Press, 2011, pp. 587–596.\n[42] R. A. Rossi and N. K. Ahmed, “The network data repository with\ninteractive graph analytics and visualization,” in AAAI , 2015. [Online].\nAvailable: http://networkrepository.com\n[43] D. Chakrabarti, Y . Zhan, and C. Faloutsos, R-MAT: A Recursive Model\nfor Graph Mining , 2004, pp. 442–446.\n[44] F. Khorasani, R. Gupta, and L. N. Bhuyan, “Scalable simd-efﬁcient\ngraph processing on gpus,” in Proceedings of the 24th International\nConference on Parallel Architectures and Compilation Techniques , ser.\nPACT ’15, 2015, pp. 39–50.\n[45] D. Chakrabarti and C. Faloutsos, “Graph mining: Laws, generators, and\nalgorithms,” ACM Comput. Surv. , vol. 38, no. 1, Jun. 2006. [Online].\nAvailable: https://doi.org/10.1145/1132952.1132954\n[46] A. Bonifati, I. Holubov ´a, A. Prat-P ´erez, and S. Sakr, “Graph generators:\nState of the art and open challenges,” ACM Comput. Surv. , vol. 53,\nno. 2, Apr. 2020. [Online]. Available: https://doi.org/10.1145/3379445\n[47] H. Park and M.-S. Kim, “Trilliong: A trillion-scale synthetic\ngraph generator using a recursive vector model,” in Proceedings\nof the 2017 ACM International Conference on Management of\nData , ser. SIGMOD ’17. New York, NY , USA: Association\nfor Computing Machinery, 2017, p. 913–928. [Online]. Available:\nhttps://doi.org/10.1145/3035918.3064014\n[48] R. C. Murphy, K. B. Wheeler, B. W. Barrett, and J. A. Ang, “Introducing\nthe graph 500,” Cray Users Group (CUG) , vol. 19, pp. 45–74, 2010.\n[49] J. Kunegis, “Konect: The koblenz network collection,” in Proceedings\nof the 22nd International Conference on World Wide Web , ser. WWW\n’13 Companion. New York, NY , USA: Association for Computing\nMachinery, 2013. [Online]. Available: https://doi.org/10.1145/2487788.\n2488173\n[50] A.-L. Barab ´asi and R. Albert, “Emergence of scaling in random\nnetworks,” Science , vol. 286, no. 5439, pp. 509–512, 1999. [Online].\nAvailable: https://science.sciencemag.org/content/286/5439/509\n[51] A. A. Hagberg, D. A. Schult, and P. J. Swart, “Exploring network\nstructure, dynamics, and function using networkx,” in Proceedings of\nthe 7th Python in Science Conference , G. Varoquaux, T. Vaught, and\nJ. Millman, Eds., Pasadena, CA USA, 2008, pp. 11 – 15.\n[52] Z. Zeng, B. Wu, and H. Wang, “A parallel graph partitioning\nalgorithm to speed up the large-scale distributed graph mining,” in\nProceedings of the 1st International Workshop on Big Data, Streams\nand Heterogeneous Source Mining: Algorithms, Systems, Programming\nModels and Applications , ser. BigMine ’12. New York, NY , USA:\nAssociation for Computing Machinery, 2012. [Online]. Available:\nhttps://doi.org/10.1145/2351316.2351325\n[53] W. Fan, R. Jin, M. Liu, P. Lu, X. Luo, R. Xu, Q. Yin,\nW. Yu, and J. Zhou, “Application driven graph partitioning,” in\nProceedings of the 2020 ACM SIGMOD International Conference on\nManagement of Data , ser. SIGMOD ’20. New York, NY , USA:Association for Computing Machinery, 2020. [Online]. Available:\nhttps://doi.org/10.1145/3318464.3389745\n[54] C.-H. Wu, J.-M. Ho, and D. Lee, “Travel-time prediction with support\nvector regression,” IEEE Transactions on Intelligent Transportation\nSystems , vol. 5, no. 4, pp. 276–281, 2004.\n[55] S. Alrefaee, S. Al Bakal, and Z. Algamal, “Hyperparameters\noptimization of support vector regression using black hole algorithm,”\nInternational Journal of Nonlinear Analysis and Applications , vol. 13,\nno. 1, pp. 3441–3450, 2022. [Online]. Available: https://ijnaa.semnan.\nac.ir/article 6107.html\n[56] L. Breiman, “Random forests,” Machine learning , vol. 45, no. 1, pp.\n5–32, 2001.\n[57] T. Chen and C. Guestrin, “Xgboost: A scalable tree boosting system,”\ninProceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining , ser. KDD ’16. New York, NY ,\nUSA: Association for Computing Machinery, 2016, p. 785–794.\n[58] V . Borisov, T. Leemann, K. Seßler, J. Haug, M. Pawelczyk, and\nG. Kasneci, “Deep neural networks and tabular data: A survey,” IEEE\nTransactions on Neural Networks and Learning Systems , pp. 1–21, 2022.\n[59] L. Grinsztajn, E. Oyallon, and G. Varoquaux, “Why do tree-based\nmodels still outperform deep learning on tabular data?” [Online].\nAvailable: https://arxiv.org/abs/2207.08815\n[60] Y . Gorishniy, I. Rubachev, V . Khrulkov, and A. Babenko, “Revisiting\ndeep learning models for tabular data,” Advances in Neural Information\nProcessing Systems , vol. 34, pp. 18 932–18 943, 2021.\n[61] R. Shwartz-Ziv and A. Armon, “Tabular data: Deep learning is not all\nyou need,” Information Fusion , vol. 81, pp. 84–90, 2022.\n[62] M. Mohri, A. Rostamizadeh, and A. Talwalkar, Foundations of Machine\nLearning , ser. Adaptive Computation and Machine Learning series.\nMIT Press, 2012.\n[63] E. Kreyszig, Advanced Engineering Mathematics . John Wiley & Sons,\n2010.\n[64] N. Merkel, R. Mayer, T. A. Fakir, and H.-A. Jacobsen, “Github\nrepository,” jan 2023. [Online]. Available: https://github.com/EASE23/\nEASE23\n[65] H. Cai, V . W. Zheng, and K. C.-C. Chang, “A comprehensive survey\nof graph embedding: Problems, techniques, and applications,” IEEE\nTransactions on Knowledge and Data Engineering , vol. 30, no. 9, pp.\n1616–1637, 2018.\n[66] W. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation\nlearning on large graphs,” in Advances in Neural Information Processing\nSystems , I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett, Eds., vol. 30. Curran Associates,\nInc., 2017.\n[67] T. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with graph\nconvolutional networks,” in International Conference on Learning Rep-\nresentations (ICLR) , 2017.\n[68] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini,\n“The graph neural network model,” IEEE Transactions on Neural\nNetworks , vol. 20, no. 1, pp. 61–80, 2009.\n[69] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion,\nO. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg et al. ,\n“Scikit-learn: Machine learning in python,” the Journal of machine\nLearning research , vol. 12, pp. 2825–2830, 2011.\n[70] I. G. Kolokasis and P. Pratikakis, “Cut to ﬁt: Tailoring the partitioning\nto the computation,” in Proceedings of the 2nd Joint International\nWorkshop on Graph Data Management Experiences Systems (GRADES)\nand Network Data Analytics (NDA) , ser. GRADES-NDA’19. New\nYork, NY , USA: Association for Computing Machinery, 2019. [Online].\nAvailable: https://doi.org/10.1145/3327964.3328498\n[71] P. Erd ˝os and A. R ´enyi, “On the evolution of random graphs,” in\nPUBLICATION OF THE MATHEMATICAL INSTITUTE OF THE HUN-\nGARIAN ACADEMY OF SCIENCES , 1960, pp. 17–61.\n[72] J. Leskovec, D. Chakrabarti, J. Kleinberg, and C. Faloutsos, “Real-\nistic, mathematically tractable graph generation and evolution, using\nkronecker multiplication,” in Knowledge Discovery in Databases: PKDD\n2005 , A. M. Jorge, L. Torgo, P. Brazdil, R. Camacho, and J. Gama, Eds.\nBerlin, Heidelberg: Springer Berlin Heidelberg, 2005, pp. 133–145.\n[73] L. Akoglu and C. Faloutsos, “Rtg: A recursive realistic graph generator\nusing random typing,” in Joint European Conference on Machine\nLearning and Knowledge Discovery in Databases . Springer, 2009,\npp. 13–28.\n[74] T. G. Kolda, A. Pinar, T. Plantenga, and C. Seshadhri, “A scalable\ngenerative graph model with community structure,” SIAM Journal on\n\nScientiﬁc Computing , vol. 36, no. 5, pp. C424–C452, 2014. [Online].\nAvailable: https://doi.org/10.1137/130914218\n[75] S. Edunov, D. Logothetis, C. Wang, A. Ching, and M. Kabiljo,\n“Generating synthetic social graphs with darwini,” in 2018 IEEE 38th\nInternational Conference on Distributed Computing Systems (ICDCS) ,\nJuly 2018, pp. 567–577.\n[76] M. Zwolak, Z. Abbas, S. Horchidan, P. Carbone, and V . Kalavri,\n“Gcnsplit: Bounding the state of streaming graph partitioning,” in\nExploiting Artiﬁcial Intelligence Techniques for Data (aiDM’22 ) .\nNew York, NY , USA: Association for Computing Machinery, 2022.\n[Online]. Available: https://doi.org/10.1145/3533702.3534920\n[77] G. M. Slota, S. Rajamanickam, K. Devine, and K. Madduri, “Partitioning\ntrillion-edge graphs in minutes,” in 2017 IEEE International Parallel and\nDistributed Processing Symposium (IPDPS) , May 2017, pp. 646–655.\n[78] W. Fan, M. Liu, C. Tian, R. Xu, and J. Zhou, “Incrementalization of\ngraph partitioning algorithms,” Proceedings of the VLDB Endowment ,\nvol. 13, no. 10, pp. 1261–1274, 2020.\n[79] W. Fan, C. Hu, M. Liu, P. Lu, Q. Yin, and J. Zhou, “Dynamic scaling\nfor parallel graph computations,” Proceedings of the VLDB Endowment ,\nvol. 12, no. 8, pp. 877–890, 2019.\n[80] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The case\nfor learned index structures,” in Proceedings of the 2018 International\nConference on Management of Data , ser. SIGMOD ’18. New York,\nNY , USA: Association for Computing Machinery, 2018. [Online].\nAvailable: https://doi.org/10.1145/3183713.3196909\n[81] X. Wang, C. Qu, W. Wu, J. Wang, and Q. Zhou, “Are we ready for\nlearned cardinality estimation?” Proc. VLDB Endow. , vol. 14, no. 9, may\n2021. [Online]. Available: https://doi.org/10.14778/3461535.3461552\n[82] J. Zhang, Y . Liu, K. Zhou, G. Li, Z. Xiao, B. Cheng, J. Xing,\nY . Wang, T. Cheng, L. Liu, M. Ran, and Z. Li, “An end-to-end\nautomatic cloud database tuning system using deep reinforcement\nlearning,” in Proceedings of the 2019 International Conference on\nManagement of Data , ser. SIGMOD ’19. New York, NY , USA:\nAssociation for Computing Machinery, 2019. [Online]. Available:\nhttps://doi.org/10.1145/3299869.3300085\n[83] L. Gottesb ¨uren, T. Heuer, P. Sanders, and S. Schlag, “Scalable shared-\nmemory hypergraph partitioning,” in 2021 Proceedings of the Workshop\non Algorithm Engineering and Experiments (ALENEX) . SIAM, 2021,\npp. 16–30.\n[84] I. Kabiljo, B. Karrer, M. Pundir, S. Pupyrev, and A. Shalita, “Social\nhash partitioner: A scalable distributed hypergraph partitioner,” Proc.\nVLDB Endow. , vol. 10, no. 11, aug 2017. [Online]. Available:\nhttps://doi.org/10.14778/3137628.3137650\n[85] G. Karypis, R. Aggarwal, V . Kumar, and S. Shekhar, “Multilevel\nhypergraph partitioning: applications in vlsi domain,” IEEE Transactions\non Very Large Scale Integration (VLSI) Systems , vol. 7, no. 1, 1999.\n[86] U. Catalyurek and C. Aykanat, “Hypergraph-partitioning-based decom-\nposition for parallel sparse-matrix vector multiplication,” IEEE Transac-\ntions on Parallel and Distributed Systems , vol. 10, no. 7, pp. 673–693,\nJuly 1999.\n[87] C. Mayer, R. Mayer, S. Bhowmik, L. Epple, and K. Rothermel, “Hype:\nMassive hypergraph partitioning with neighborhood expansion,” in 2018\nIEEE International Conference on Big Data (Big Data) , Dec 2018, pp.\n458–467.\n[88] W. Jiang, J. Qi, J. X. Yu, J. Huang, and R. Zhang, “Hyperx: A scalable\nhypergraph framework,” IEEE Transactions on Knowledge and Data\nEngineering , vol. 31, no. 5, pp. 909–922, 2019.\n[89] B. Heintz, R. Hong, S. Singh, G. Khandelwal, C. Tesdahl, and A. Chan-\ndra, “Mesh: A ﬂexible distributed hypergraph processing system,” in\n2019 IEEE International Conference on Cloud Engineering (IC2E) ,\n2019, pp. 12–22.",
  "textLength": 92347
}