{
  "paperId": "7d655f6ac307f20ceeddb39b18508e5fd97b0b53",
  "title": "Learned Indexing in Proteins: Extended Work on Substituting Complex Distance Calculations with Embedding and Clustering Techniques",
  "pdfPath": "7d655f6ac307f20ceeddb39b18508e5fd97b0b53.pdf",
  "text": "Learned Indexing in Proteins: Extended Work\non Substituting Complex Distance Calculations\nwith Embedding and Clustering Techniques\nJaroslav Olha[0000\u00000003\u00001824\u0000468X], Ter\u0013 ezia Slanin\u0013 akov\u0013 a[0000\u00000003\u00000502\u00001145],\nMartin Gendiar, Matej Antol[0000\u00000002\u00001380\u00005647], and Vlastislav\nDohnal[0000\u00000001\u00007768\u00007435]\nFaculty of Informatics, Masaryk University, Botanick\u0013 a 68a, 602 00 Brno, Czech\nRepublicfolha, 492606, xslanin, dohnal g@mail.muni.cz, antol@muni.cz\nAbstract. Despite the constant evolution of similarity searching re-\nsearch, it continues to face the same challenges stemming from the com-\nplexity of the data, such as the curse of dimensionality and computation-\nally expensive distance functions. Various machine learning techniques\nhave proven capable of replacing elaborate mathematical models with\ncombinations of simple linear functions, often gaining speed and sim-\nplicity at the cost of formal guarantees of accuracy and correctness of\nquerying.\nThe authors explore the potential of this research trend by presenting\na lightweight solution for the complex problem of 3D protein structure\nsearch. The solution consists of three steps { (i) transformation of 3D\nprotein structural information into very compact vectors, (ii) use of a\nprobabilistic model to group these vectors and respond to queries by\nreturning a given number of similar objects, and (iii) a \fnal \fltering\nstep which applies basic vector distance functions to re\fne the result.\nKeywords: protein database, embedding non-vector data, learned met-\nric index, similarity search, machine learning for indexing\n1 Introduction\nThe methods and approaches developed by the similarity searching community\nare used by a wide range of scienti\fc \felds, both within computer science and\nbeyond. While some applications require provable accuracy guarantees, veri\fable\nalgorithms, or support for complex similarity functions, many of the similarity\nsearching problems emerging in various areas of research do not have such strict\nformal constraints.\nThe publication of this paper and the follow-up research was supported by the ERDF\n\"CyberSecurity, CyberCrime and Critical Information Infrastructures Center of Excellence\"\n(No.CZ.02.1.01/0.0/0.0/16 019/0000822). Part of this work was carried out with the support\nof ELIXIR CZ Research Infrastructure (ID LM2018131, MEYS CR). Computational resources\nwere supplied by the project \"e-Infrastruktura CZ\" (e-INFRA CZ LM2018140 ) supported by the\nMinistry of Education, Youth and Sports of the Czech Republic.arXiv:2208.08910v2  [cs.IR]  5 Oct 2022\n\n2 J. Olha et al.\nOften, the representations of data or their similarity functions are inherently\nunreliable, so the idea of a perfect similarity search is unattainable regardless\nof the quality of the search method. Other times, the problems are too compu-\ntationally complex, and instead of a perfect answer that would take too long\nto compute, a best-e\u000bort answer in a reasonable amount of time is preferable\n(perhaps to be re\fned later using more costly methods). Since the relevance\nof research on similarity searching is determined by its ability to solve current\nproblems, it should strive to employ the widest possible array of problem-solving\ntools.\nThere are multiple similarity search indexes, falling under the umbrella of\napproximate searching, capable of adjusting to such use cases by lowering their\naccuracy thresholds or returning partial results [22]. However, in recent years, an\nentirely new approach has begun to gain traction { along with most other areas\nof computer science, the area of data retrieval has started to incorporate various\nmachine learning approaches. These approaches have the potential to not only\nexpand the available toolbox of problem-solving instruments, but also to o\u000ber a\nnew way of thinking about the problems themselves. Notably, in 2018, Kraska\net al. [11] suggested that all conventional index structures could be viewed as\nmodels of data distributions, implying that machine and deep learning models\ncould be used in their place. Even though the idea was originally proposed and\ntested on structured data, this reframing of the problem has already inspired\nsimilar work in the realm of unstructured datasets [1][7][21].\nTo investigate the potential of these approaches further, we have chosen to\nexamine the problem of 3D protein structure similarity search [16]. This is an\nimportant open problem in biochemical and medical research, which can be\nviewed as an instance of similarity searching in non-vector datasets, because\nsimilarity between a pair of protein structures is usually calculated using a series\nof non-trivial, computationally expensive operations. Additionally, the amount\nof 3D protein structure data is currently exploding due to a recent breakthrough\nin the \feld [9], and the demand for versatile similarity searching approaches is\nlikely to grow in the near future.\nIn this paper, we demonstrate that even a relatively complex interdisciplinary\nproblem such as 3D protein structure retrieval can be tackled with fast and\nlightweight solutions. We present a simple pipeline where protein structures are\n\frst transformed into short vectors and used to train multiple partitioning and\nclassi\fcation models { these are linked together to form a learned index structure.\nThe index then answers queries by returning several candidate leaf nodes, and\n\fltering the objects stored therein using basic vector (similarity) functions1.\nThis approach, while based on probabilities rather than mathematical guar-\nantees, provides a reasonable quality of results at a fraction of the computational\ncosts required by previous methods. In addition, its modularity allows us to\nchange algorithms or their parameters for various trade-o\u000bs between complexity\nand accuracy, depending on the particular use case.\n1The entire functionality is publicly available as a search engine prototype at https:\n//disa.fi.muni.cz/demos/lmi-app-protein/\n\nLearned Indexing in Proteins 3\n2 Related work\nLearned indexing was \frst introduced in [11] with the core idea of learning a\ncumulative distribution function (CDF) to map a key to a record position. This\nproposition challenged the long-standing paradigm of building indexes solely\nwith classic data structures such as B-trees and allowed for reduction in searching\ncosts compared to the traditional methods. To allow for indexing of large data\ncollections, the authors introduced Recursive model index (RMI) { a hierarchical\ntree index structure of simple interconnected machine learning models, each\nlearning the mapping on a subset of the dataset. RMI is, however, limited to\nsorted datasets of structured data, and cannot accommodate multi-dimensional\ndata.\nThe generalization of the learned indexing concept to spatial and low-dimen-\nsional data was explored primarily by the Z-order model [25], which makes use\nof the space-\flling curve encoding to produce 1D representation of the original\ndata, and ML-index [6] which achieves the same with the use of iDistance [8].\nRSMI [17] introduced a recursive partitioning strategy, building on the Z-order\nmodel. Furthermore, LISA [13] and Flood [15] both partition the data into grid\ncells based on data distribution, improving the range and kNN performance of\nprior approaches. These approaches were recently directly compared to (and\nsurpassed by) LIMS [21], which generalizes to metric spaces and establishes a\nnew state-of-the-art performance on datasets of up to 64 dimensions.\nIndexing solutions for approximate, rather than precise queries were explored\nby Chiu et al. [5] introducing a probability-based ranking model able to learn\nthe neighborhood relationships within data, which can be integrated with ex-\nisting product quantization (PQ) methods. This method was tested on 1-billion\ndatasets with approximately 100 dimensions and has been shown to boost the\nperformance of traditional PQ methods.\nFollowing the architectural design of RMI, we proposed the Learned metric\nindex (LMI) [1], which can use a series of arbitrary machine learning models to\nsolve the classi\fcation problem by learning a pre-de\fned partitioning scheme.\nThis was later extended to a fully unsupervised (data-driven) version introduced\nin [19], which is utilized in this work.\nFinally, the ideas of creating an indexing solution with machine learning\nhave found their use in many di\u000berent domains, e.g., in trajectory similarity\nsearch [18] or information retrieval, using a single transformer model [20].\nProtein representation\nTo enable computational approaches to the problem of uncovering functional\nproperties of proteins, a great amount of research attention has been directed\nto creating representative (numerical) embeddings of protein structures. There\nare two distinct categories of embeddings based on the input data { those that\noperate with sequences and those working with 3D structures . Although they\nserve a similar purpose, these two categories are completely distinct in terms\nof the technical approaches they employ. Speci\fcally, sequence embeddings use\n\n4 J. Olha et al.\ntechniques such as hidden Markov models [10] or various natural language pro-\ncessing techniques [2] to derive meaning from protein sequences, treating them as\nencoded sequences of characters { this approach is not applicable to our research.\nEmbeddings representing protein 3D structures are generally less elaborate,\nsince the information content is more robust to begin with. The most common\nencoding is a protein distance map, which produces a symmetric 2D matrix of\npairwise distances between either atoms, groups of atoms or amino acid residues.\nThis distance map can be transformed into a protein contact map, which is a\nbinary image where each pixel indicates whether two residues are within a certain\ndistance from one another or not. Contact maps have been used in conjunction\nwith machine learning techniques for prediction of protein structure [4]; another\nstudied problem is the reconstruction of 3D structure based on information in\ncontact maps [24]. While these techniques are related to our own approach, we\nproduce embeddings that are considerably more compact and re\rective of our\nsimilarity searching use case, as will be shown in the following sections.\n3 Data domain\nWe have chosen to test our approach on 3D protein structures for several reasons.\nFirst, while protein structure data is very widely used, and the study of this data\nis vital for almost every area of biochemical research, the issue of e\u000ecient search\nand comparison of protein structures is still unresolved to some extent, with\nmany databases still relying on time-consuming brute-force linear search [14].\nThis data is also publicly available in a single database, called the Protein Data\nBank (PDB), which is used by the majority of protein researchers and widely\nagreed upon as the standard. Even though this database is large enough to\nrequire e\u000ecient search methods, its size ( \u0018500;000 structures as of 2022) still\nmakes it possible to download the entire database and scan it exhaustively for\nground truth answers if necessary.\nJust as importantly, it is clear that the issue of e\u000ecient search within this\ndata will only become more crucial and challenging in the next few years. Firstly,\nthe common dataset of empirically solved protein structures continues to grow\nexponentially [3] { its current contents constitute a mere fraction of all the\nprotein structures in nature, and the complex laboratory procedures needed to\nobtain these structures are being re\fned every year. Secondly, the computational\nprediction of protein structures from their sequences has recently seen rapid\nimprovement with the release of AlphaFold 2 in 2021 [9] { this has already\nresulted in hundreds of thousands of new reliable 3D structures that are of great\ninterest to researchers, with tens of millions more to be added in the coming\nmonths [23].\nInterestingly, the question of e\u000ecient protein structure search is not only\nimportant for storage and retrieval purposes { for instance, researchers often\ndiscern the function of unknown proteins by comparing them to other, better-\nknown proteins. Since the function of a protein is entirely dictated by its 3D\nstructure, this is equivalent to searching a database for the most similar protein\n\nLearned Indexing in Proteins 5\nstructures. However, the speci\fc needs of this type of research can vary { while\nsome researchers are looking for extremely speci\fc deviations among a group of\nvery similar proteins (e.g., when studying mutations or conformational changes),\nothers might be looking for much broader patterns of similarity between distant\nprotein families.\nStructurally, a protein chain consists of a linear sequence of interconnected\nbuilding blocks called amino acids. Within the right biochemical environment,\nthis linear sequence folds in on itself to form a complex 3D structure. Protein\nstructures are sometimes cited as a typical example of complex unstructured\ndata, since they cannot be meaningfully ordered according to any objective cri-\nteria (any search method needs to rely on pairwise similarity), and the similarity\nof two protein structures often cannot be determined by a single vector oper-\nation. Typically, protein molecule data are stored using the three-dimensional\ncoordinates of each of their atoms, with the protein randomly oriented in space.\nIn order to compare a pair of protein structures, they \frst need to be properly\nspatially aligned in terms of translation and rotation, and a subset of atoms\nmust be selected for alignment. This typically involves gradual optimization of a\nspatial distance metric (such as the root-mean-square deviation of all the atom\ncoordinates), which is a computationally expensive process that cannot be di-\nrectly mapped to a simple vector operation.\nOne commonly-used measure of protein similarity is the Qscore [12], which is\ncalculated by dividing the number of aligned amino acids in both protein chains\nby the spatial deviation of this alignment and the total number of amino acids\nin both chains. Even though this measure is imperfect and not appropriate for\nall use cases, it is used in several prominent applications, including the PDB's\nown search engine.\nNote that two identical structures have a Qscore of 1, and completely di\u000berent\nstructures have a Q-score of 0: as a result, the score needs to be inverted in order\nto be used as a distance metric ( d(x; y) = 1 \u0000Qscore(x; y)). In the following\nsections, we will refer to this inverted value as Qdistance .\nWhile there are a few types of protein structure embeddings that are invariant\nto the spatial alignment of the molecules (see the Related Work section), these\nwere not developed for the purpose of fast data retrieval. As a result, they tend to\nbe too detailed and cumbersome to serve as simple data descriptors. By contrast,\nthe embedding presented in this work has been designed speci\fcally to contain\nthe optimal amount of information for e\u000ecient and accurate similarity searching,\nas will be shown in the following sections.\n4 Fast searching in proteins\nWe present a pipeline (visualized in Figure 1) consisting of three separate com-\nponents: (i) a simple embedding technique for protein data in the PDB format,\n(ii) the use of a machine-learning-based index { Learned Metric Index (LMI) {\nto locate a candidate set of similar protein structures, and (iii) fast \fltering to\nproduce the \fnal query answer.\n\n6 J. Olha et al.\nFig. 1. A diagram of the proposed solution.\nEmbedding Size File Size Index Build Time (256-64) Index Build Time (128-128)\n5\u00025 16 MB 246s 184s\n10\u000210 51 MB 350s 270s\n30\u000230 456 MB 927s 655s\n50\u000250 1275 MB 2391s 1814s\nTable 1. File size of the protein dataset (518,576 protein chains) stored using protein\nembeddings, and build times of two di\u000berent LMI architectures. Note that the size of\nthe original database is 8.2 GB.\nThe embedding we propose divides the protein sequence into Nconsecutive\nsections { the positions of the atoms within each section are averaged, and the\nsection is subsequently treated as a single point in space. We then calculate\ndistances between each pair of these sections, creating an incidence matrix. In\nthis matrix, we prune all the values exceeding a cuto\u000b, and normalize the rest.\nThe matrix is symmetrical and all the diagonal values are 0. The half of this\nmatrix (omitting the main diagonal) is then reduced into a single row in a\nM\u0002(N2\u0000N\n2) matrix, where Mis the number of proteins in the entire dataset\n(see Figure 1).\nThis produces a very compact embedding for all the proteins, and the entire\ndataset can be represented by a \fle that is up to two orders of magnitude smaller\nthan the original database { see Table 1.\nTo reduce the search space to a small number of candidate protein structures,\nwe used the Learned Metric Index (LMI), a tree index structure where each\ninternal node is a learned model trained on a sub-section of data assigned to it\nby its parent node [1]. Speci\fcally, we used the data-driven version of LMI, where\nthe partitioning is determined in an unsupervised manner. We explored di\u000berent\narchitectural setups { both in terms of the number of nodes at each level (index\nbreadth), as well as the number of levels (index depth). As the learned models,\nwe explored K-Means, Gaussian Mixture Models, and K-Means in combination\nwith Logistic regression (see [19] for details regarding the model setups). For the\nsake of compactness, in the experimental evaluation we only present the results\nachieved with the best-performing setup { a two-level LMI structure with arity\nof 256 on level 1 and 64 on level 2 (i.e., 256 root descendants, each of them with\n64 child nodes), with K-Means chosen as the partitioning algorithm. After LMI\nis built, we search within it using a query protein structure and return target\ncandidate sets; the size of the candidate sets is determined by a pre-selected\n\nLearned Indexing in Proteins 7\nFig. 2. Evaluation of range queries after LMI search and before \fltering, using the K-\nMeans method and a 256-64 LMI architecture: (left) Range=0.1, (middle) Range=0.3,\n(right) Range=0.5.\nstop condition (for instance, a stop condition of 1% of the dataset corresponds\nto\u00185;000 candidate answers per query).\nIn the \fnal step, we \flter the candidate set according to a particular distance\nfunction. In our experiments, we have examined \fltering based on the Euclidean\ndistance as well as the cosine distance of the vector embeddings, but the \fltering\nstep could theoretically be performed using any distance metric, or even the\noriginal Qscore similarity of the full protein structures. The \fltering step returns\na subset of the candidate set based on the speci\fed criteria (i.e., kNN or range).\n5 Experimental evaluation\nWe evaluated our approach using range queries, with 512 randomly chosen pro-\ntein chains from the dataset used as query objects. In order to compare our\nresults against the ground truth, we needed to know the Qdistances (based on\nQscore) between the 512 protein chains and all the other chains in the database\n{ these distances were kindly provided by the researchers behind [14], where the\nsame 512 objects were used as the pivots for their search engine. The objects\nwere chosen uniformly randomly with respect to protein chain length, which en-\nsures that even very long proteins are represented among our queries (despite\nconstituting a relatively small portion of the dataset).\nWe expected the performance of our method to deteriorate as the range of\nthe queries expands, since a wider search range would require the method to\n\n8 J. Olha et al.\nFig. 3. Distribution of ob-\njects in the LMI leaf nodes.\n(A completely balanced struc-\nture would hold \u001830 objects\nin each bucket).\nFig. 4. Correlation between Qdistance and the Eu-\nclidean distance used for \fltering: Blue points rep-\nresent proteins returned by the LMI; orange points\nrepresent proteins which are present in the ground\ntruth answer but were not returned by the LMI.\ncorrectly identify more objects which are less similar. To examine this e\u000bect, we\nhave chosen three representative query ranges of 0.1, 0.3 and 0.5 { in a real use\ncase, the range would be chosen by a domain expert based on the particular use\ncase. As a rule of thumb, a range of 0.1 represents a high degree of similarity,\nwhile a range of 0.5 represents low (but still biologically signi\fcant) similarity;\nthe biological relevance of answers drops sharply beyond this range [14].\nFirst, we evaluated the performance of the LMI, before the \fltering step. The\nrecall shown in Figure 2 pertains to the entire candidate set of objects (i.e. how\nmuch of the ground truth answer is contained in the 1%/5%/10% of the dataset\nreturned by the LMI for further \fltering)2.\nThis \fgure presents us with two important pieces of information { \frstly, it is\nclear that LMI can reach very high recall even when trained on the smaller 10x10\nembedding { this makes the embedding a natural choice for further evaluation,\nsince it is e\u000ecient while signi\fcantly reducing the memory and CPU costs of\ntraining compared with the larger embeddings. It can also be seen that, especially\nin the lower query ranges which are of most interest to us, the 1% stop condition\nrepresents a sensible trade-o\u000b between recall and search time, returning relatively\nfew candidate objects ( \u00185;000) while minimizing the amount of false negatives.\nIn addition to recall, we also need to investigate the distribution of objects\nin the index, to ensure that the occupancy of leaf nodes (i.e. data buckets) is not\noverly imbalanced { an extremely imbalanced index would achieve high recall,\nbut it would also be more likely to return overly large candidate sets, which\nwould be detrimental to the \fltering step. The size distribution of the buckets\nis shown in Figure 3, and it con\frms that the distribution is not overly skewed\n2Precision is not evaluated in this step { at this point in the pipeline, it is very low\nand not particularly relevant.\n\nLearned Indexing in Proteins 9\nFig. 5. E\u000bects of \fltering on the recall and precision of the candidate set of objects\n(relative to the ground truth answer).\ntowards large buckets, even when the embeddings are quite small, as is the case\nwith N=10. However, the embeddings can only be condensed up to a point { the\n5x5 embedding (which transforms each protein structure into a vector of only\n10 values) causes a large portion of the objects to concentrate in a single bucket,\nas the LMI can no longer distinguish among groups of objects.\nDuring the \fltering step, recall naturally decreases over time (since the\nmethod occasionally \flters out relevant answers), while precision should improve\nas the portion of relevant objects in the candidate set increases. This e\u000bect will\nbe di\u000berent depending on the distance metric used for \fltering, as well as the\ndataset { we show the e\u000bects for two distance metrics (Euclidean distance and\ncosine distance) in Figure 5. By analyzing the correlation between Qdistance and\neach of the two distance metrics, we have determined that the Euclidean distance\nis the better \fltering function for this dataset.3\nTable 2 shows the \fnal results of the range queries with the best-performing\ncon\fguration of parameters: embedding size N=10, the K-means clustering model,\n256-64 LMI architecture, and \fltering after the 1% stop condition using the Eu-\nclidean distance metric. The results, especially in the lower query ranges, are\nvery encouraging, although the \fltering stage seems to introduce a surprisingly\n3Note that the actual Qdistance function scales di\u000berently from the Euclidean distance\nused to \flter answers (see Figure 4) { this requires simple re-scaling (e.g., to \fnd\nrange=0.5 queries, we set the Euclidean distance cuto\u000b at 0.75).\n\n10 J. Olha et al.\nRange 0.1 Range 0.3 Range 0.5\nMean # of objects: 83 Mean # of objects: 236 Mean # of objects: 519\nLMI Recall 0.973 (1.000) 0.895 (0.999) 0.755 (0.867)\nRecall after \fltering 0.742 (0.878) 0.649 (0.711) 0.530 (0.637)\nF1 after \fltering 0.712 (0.855) 0.669 (0.766) 0.592 (0.673)\nTable 2. Overall evaluation of protein range queries: the average values, as well as the\nmedian values (in parentheses) are shown.\nlarge amount of false negatives by \fltering out parts of the correct answer. It\nis likely that the \fltering metric we have chosen was slightly too na \u0010ve, and the\n\fltering step could have bene\fted from a di\u000berent distance function, or at least\na di\u000berent weighting of the vectors before calculating their Euclidean distance.\nIn the future, this presents a natural point of focus to improve our results even\nfurther.\nFigure 4 further illustrates the strengths and weaknesses of our approach by\nshowing the relationship between the Qdistance metric and the Euclidean dis-\ntances of vector embeddings. There is clear correlation between these two met-\nrics, making a strong case for the simpler one, and LMI is much more successful\nat \fnding objects that are more similar to the query (left side of the graph) than\nit is at \fnding less similar objects (right side of the graph). It should be restated\nthat even though we use the Qdistance metric as the \"ground truth\", it is merely\na subjective similarity metric, and the relevance of results that are close to the\nedge of the query range is debatable (i.e., an object with a Qdistance of 0.499 is\nnot necessarily more relevant than an object with a Qdistance of 0.501).\nDistance computations on long proteins pose a considerable challenge for\nsimilarity searching methods, typically requiring a disproportionate amount of\ncomputing time [14]. This does not apply to our embedding approach, which\ntransforms all the proteins into \fxed-length vectors using the same algorithm,\nregardless of their length. It would therefore intuitively follow that such an ap-\nproach should achieve lower recall on the longer protein queries, since more\ninformation is lost in the embedding. However, in practice, that is not the case,\nas can be seen in Figure 6.\nThis is probably due to the simple fact that the distribution of chain lengths\nin protein databases is not uniform { in fact, the long chains are much less\nnumerous. As a result, losing a lot of information about the long chains is not\na problem, since they are still relatively easy to \fnd. This gives the \fxed-length\nembedding approach a signi\fcant performance advantage { in a dataset where\nthere are relatively few long protein chains, it would be a waste of resources to\ncalculate and store an excessive amount of data about them, the way many other\nmethods do.\nAs has been mentioned in the previous section, in addition to variable chain\nlengths, the proteins also have a variable number of neighbors in any given range.\nThis can, again, call into question the reliability of the results based on recall,\nsince recall is a relative measure, and the size of the error will be di\u000berent based\non the size of the actual answer. For instance, if all range(0.1) query answers only\n\nLearned Indexing in Proteins 11\nFig. 6. Distribution of recall for di\u000berent protein chain lengths (left to right: the short-\nest 10% of chains, each quartile from shortest to longest, and the longest 10% of chains).\nFig. 7. The number of results returned by the LMI compared to the ground truth. The\nX-axis shows the absolute number of objects in a given query answer (according to the\nground truth), while the Y-axis shows the portion of the objects that were found by\nLMI (i.e., the recall). Each blue point in the scatter plot represents one query, with\nthe larger and more warmly-colored points representing multiple queries with identical\ngraph coordinates.\nconsisted of a single easy-to-\fnd object, evaluating recall on its own would give\nus a biased idea of the searching e\u000eciency. Figure 7 shows how many neighbors\neach protein structure has according to the ground truth, and how many of these\nneighbors have been found using our approach { while there are signi\fcantly\nmore proteins with fewer than 10 neighbors in the lower query ranges, these are\nnot the majority, and the errors are distributed evenly relative to query answer\nsize.\nFinally, to provide broader context for the pipeline's performance, we have\nevaluated it against a more conventional approach. While there is no analogous\nmethod for searching 3D protein structures using range queries, there is a similar,\nrecently-published method for nearest neighbor retrieval in the same database of\n3D protein structures [14]. This method uses a three-stage search engine which\ncompares bit-strings in the Hamming space (\"sketches\") to approximate the\ndistance of protein chains.\n\n12 J. Olha et al.\nLMI + Filtering4Sketch-based method PDB Engine\nAccuracy (median) 0:660 1:0 1:0\nAccuracy (mean) 0:626 0:937 1:0\nTime (median) 0:094 s 2:5 s 183 s\nTime (max) 0:145 s 6109 s 14321 s\nIndex size587 MB 178 MB6N/A\nTable 3. The accuracy, search times, and memory requirements of 30NN protein search\nqueries with a maximum distance radius of 0.5.\nTo allow for a relatively fair comparison of these two methods, we modi\fed\nour search parameters to more closely match the search results presented in [14].\nSpeci\fcally, since their similarity queries were mainly 30NN queries limited by\nthe range 0.5, we have performed range queries in the same range (even though\nour method's performance is substandard in this range), and in the \fltering\nstep, we \fltered out all objects beyond the 30 best-ranking answers. Since the\nsketch-based method was originally compared with the linear search of the PDB\ndatabase, we present this benchmark as well { naturally, this is always the slowest\nmethod by far, but it requires no index and always \fnds the exact answer.\nAll of these results can be found in Table 3 { while our method clearly does\nnot match the high accuracy of the sketch-based method in this experimental\nsetup, it is faster by at least an order of magnitude, occasionally exceeding 4\norders of magnitude since it does not su\u000ber from an extreme \"tail\" of worst-case\nsearch times caused by evaluation of long proteins.\n6 Summary and Conclusions\nIn an e\u000bort to investigate the potential of new data retrieval techniques in the\n\feld of similarity searching, we have developed and evaluated a novel approach\nto the problem of protein structure search, resulting in a short pipeline consist-\ning of a concise vector embedding, learned indexing, and distance-based answer\n\fltering. By successfully applying this approach on a well-established database\nof 3D protein structures, we have shown that even in a domain that may, at \frst,\nseem poorly suited to simple vector-based transformations, a surprising amount\nof information can be discerned by learned models.\nOne advantage of our modular approach is that every part of the pipeline\ncan be evaluated separately, allowing experts to identify the weakest spots and\nalter them based on the current use case and dataset. The experiments presented\n4Before the \fltering step, the candidate set returned by LMI has a median recall of\n1.0 and an average recall of 0.87; however, since the candidate set is much larger\nthan the \fnal answer ( \u00185;000 objects), the accuracy is insigni\fcant and has thus\nbeen omitted from the table.\n5The size of the internal structure of the index, excluding raw protein data.\n6495,085 * (320b (sketches) + 1024b (sketches) + 4*6*64b (PPP-codes)) + 512 *\n16kB (pivots)\n\nLearned Indexing in Proteins 13\nin this paper serve as a good example { after evaluating each part of our own\npipeline, it is clear that we have chosen an overly simplistic \fltering method for\nour data. In the future, we plan to investigate more sophisticated options for\nvector-based \fltering, as well as a completely di\u000berent approach to reducing the\nsize of query answers.\nWhile it is di\u000ecult to compare our work with the state of the art (since there\nare no direct analogues to our method in the chosen data domain), we have\nmade an e\u000bort to modify our method for the fairest possible comparison with a\nrecent, more conventional similarity searching approach in the same domain. In\nthis comparison, our solution, although coming up short in terms of accuracy, is\nconsistently faster by multiple orders of magnitude, and maintains much lower\nmemory requirements.\nOur work aims to make a case for less conventional solutions to similarity\nsearching problems { ones that rely on learned approaches, rather than tradi-\ntional indexing models, to discern the natural distribution of data. This is by\nno means an argument for universal adoption of machine-learning-based tech-\nniques in all similarity searching applications. The approach in this paper is\nproblem-speci\fc, and has several potential drawbacks that need to be weighed\nagainst its strengths. Nevertheless, we provide some insight as to how the trend\nof machine-learned pattern recognition, which has already caused minor and ma-\njor revolutions in most \felds of computer science, could be applied in practical\nsolutions to current similarity searching problems.\nBased on these results, we remain convinced that learned indexing approaches\n(such as the Learned Metric Index used in this work) will play an integral part\nin shaping the future of similarity searching.\nReferences\n1. Antol, M., Ol'ha, J., Slanin\u0013 akov\u0013 a, T., Dohnal, V.: Learned metric index | propo-\nsition of learned indexing for unstructured data. Information Systems 100(2021)\n2. Asgari, E., Mofrad, M.R.: Continuous distributed representation of biological se-\nquences for deep proteomics and genomics. PloS one 10(11), e0141287 (2015)\n3. Burley, S.K., Bhikadiya, C., Bi, C., Bittrich, S., Chen, L., Crichlow, G.V., Christie,\nC.H., Dalenberg, K., Di Costanzo, L., Duarte, J.M., et al.: Rcsb protein data\nbank: powerful new tools for exploring 3d structures of biological macromolecules\nfor basic and applied research and education in fundamental biology, biomedicine,\nbiotechnology, bioengineering and energy sciences. Nucleic acids research 49(D1),\nD437{D451 (2021)\n4. Cheng, J., Baldi, P.: Improved residue contact prediction using support vector\nmachines and a large feature set. BMC bioinformatics 8(1), 1{9 (2007)\n5. Chiu, C.Y., Prayoonwong, A., Liao, Y.C.: Learning to index for nearest neigh-\nbor search. IEEE transactions on pattern analysis and machine intelligence 42(8),\n1942{1956 (2019)\n6. Davitkova, A., Milchevski, E., Michel, S.: The ML-index: A multidimensional,\nlearned index for point, range, and nearest-neighbor queries. In: EDBT. pp. 407{\n410 (2020)\n\n14 J. Olha et al.\n7. H unem order, M., Kr oger, P., Renz, M.: Towards a learned index structure for ap-\nproximate nearest neighbor search query processing. In: International Conference\non Similarity Search and Applications. Springer (2021), to appear\n8. Jagadish, H.V., Ooi, B.C., Tan, K.L., Yu, C., Zhang, R.: idistance: An adaptive\nB+-tree based indexing method for nearest neighbor search. ACM Transactions\non Database Systems (TODS) 30(2), 364{397 (2005)\n9. Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O.,\nTunyasuvunakool, K., Bates, R., \u0014Z\u0013 \u0010dek, A., Potapenko, A., et al.: Highly accurate\nprotein structure prediction with alphafold. Nature 596(7873), 583{589 (2021)\n10. Koski, T.: Hidden Markov models for bioinformatics, vol. 2. Springer Science &\nBusiness Media (2001)\n11. Kraska, T., Beutel, A., Chi, E.H., Dean, J., Polyzotis, N.: The case for learned index\nstructures. In: Proceedings of the 2018 International Conference on Management\nof Data. p. 489{504. SIGMOD '18, Association for Computing Machinery (2018)\n12. Krissinel, E., Henrick, K.: Secondary-structure matching (ssm), a new tool\nfor fast protein structure alignment in three dimensions. Acta Crystallo-\ngraphica Section D: Biological Crystallography 60(12), 2256{2268 (2004).\nhttps://doi.org/10.1107/S0907444904026460\n13. Li, P., Lu, H., Zheng, Q., Yang, L., Pan, G.: Lisa: A learned index structure for\nspatial data. In: Proceedings of the 2020 ACM SIGMOD international conference\non management of data. pp. 2119{2133 (2020)\n14. Mic, V., Ra\u0014 cek, T., K\u0014 renek, A., Zezula, P.: Similarity search for an extreme appli-\ncation: Experience and implementation. In: International Conference on Similarity\nSearch and Applications. pp. 265{279. Springer (2021)\n15. Nathan, V., Ding, J., Alizadeh, M., Kraska, T.: Learning multi-dimensional in-\ndexes. In: Proceedings of the 2020 ACM SIGMOD International Conference on\nManagement of Data. pp. 985{1000 (2020)\n16. Olha, J., Slanin\u0013 akov\u0013 a, T., Gendiar, M., Antol, M., Dohnal, V.: Learned indexing\nin proteins: Substituting complex distance calculations with embedding and clus-\ntering techniques. In: Skopal, T., Falchi, F., Loko\u0014 c, J., Sapino, M.L., Bartolini,\nI., Patella, M. (eds.) Similarity Search and Applications. pp. 274{282. Springer\nInternational Publishing, Cham (2022)\n17. Qi, J., Liu, G., Jensen, C.S., Kulik, L.: E\u000bectively learning spatial indices. Pro-\nceedings of the VLDB Endowment 13(12), 2341{2354 (2020)\n18. Ramadhan, H., Kwon, J.: X-\fst: Extended \rood index for e\u000ecient similarity search\nin massive trajectory dataset. Information Sciences (2022)\n19. Slanin\u0013 akov\u0013 a, T., Antol, M., O \u0014Iha, J., Ka\u0014 na, V., Dohnal, V.: Data-driven learned\nmetric index: an unsupervised approach. In: International Conference on Similarity\nSearch and Applications. pp. 81{94. Springer (2021)\n20. Tay, Y., Tran, V.Q., Dehghani, M., Ni, J., Bahri, D., Mehta, H., Qin, Z., Hui, K.,\nZhao, Z., Gupta, J., et al.: Transformer memory as a di\u000berentiable search index.\narXiv preprint arXiv:2202.06991 (2022)\n21. Tian, Y., Yan, T., Zhao, X., Huang, K., Zhou, X.: A learned index for exact\nsimilarity search in metric spaces. arXiv preprint arXiv:2204.10028 (2022)\n22. Vadicamo, L., Gennaro, C., Amato, G.: On generalizing permutation-based rep-\nresentations for approximate search. In: International Conference on Similarity\nSearch and Applications. pp. 66{80. Springer (2021)\n23. Varadi, M., Anyango, S., Deshpande, M., Nair, S., Natassia, C., Yordanova, G.,\nYuan, D., Stroe, O., Wood, G., Laydon, A., et al.: Alphafold protein structure\ndatabase: massively expanding the structural coverage of protein-sequence space\nwith high-accuracy models. Nucleic acids research 50(D1), D439{D444 (2022)\n\nLearned Indexing in Proteins 15\n24. Vendruscolo, M., Kussell, E., Domany, E.: Recovery of protein structure from con-\ntact maps. Folding and Design 2(5), 295{306 (1997)\n25. Wang, H., Fu, X., Xu, J., Lu, H.: Learned index for spatial queries. In: 2019 20th\nIEEE International Conference on Mobile Data Management (MDM). pp. 569{574.\nIEEE (2019)",
  "textLength": 38571
}