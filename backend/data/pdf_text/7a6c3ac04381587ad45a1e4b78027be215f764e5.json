{
  "paperId": "7a6c3ac04381587ad45a1e4b78027be215f764e5",
  "title": "Optimizing Learned Bloom Filters: How Much Should Be Learned?",
  "pdfPath": "7a6c3ac04381587ad45a1e4b78027be215f764e5.pdf",
  "text": "This is a postprint version of the following published document :\nDai, Z., Shrivastava, A., Reviriego, P. & Hernandez, J. \nA. (2022, septiembre). Optimizing Learned Bloom \nFilters: How Much Should Be Learned? IEEE \nEmbedded Systems Letters, 14(3), 123-126. \nDOI: 10.1109/les.2022.3156019\n © 2022 IEEE. Personal use of this material  is permitted. Permission \nfrom IEEE must be obtained for all  other uses, in any  current or \nfuture media, including reprinting/republishing this material for \nadvertising or promotional\n purposes, creating new collective works, \nfor resale\n or redistribution to servers or lists, or reuse of any \ncopyrighted component of this work in other works. \n\n1\nO p t im i z i n gL e a r n e dB l o om F i l t e r s :H owM u c h \nS h o u l dB eL e a r n e d ? \nZ h e nw e iD a i 1,A n s h um a l iS h r i v a s t a v a 1,P e d r oR e v i r i e g o 2,J o s ´eA l b e r t oH e r n ´a n d e z 2\nA b s t r a c t —Th eL e a rn edB l o omF i l t e r(LBF )c omb in e sam a - \nch in el e a rn in gm od e l( l e a rn e r )w i that r ad i t i on a lB l o omﬁ l t e rt o \nimp r o v eth ef a l s ep o s i t i v er a t e(FPR )th a tc anb ea ch i e v edf o ra \ng i v enm em o r ybud g e t .Th eLBFh a sr e c en t l yb e eng en e r a l i z edb y \nm ak in gu s eo fth efu l lsp e c t rumo fth el e a rn e r ’ sp r ed i c t i ons c o r e . \nH ow e v e r ,ina l lth o s ed e s i gn s ,th em a ch in el e a rn in gm od e li sﬁ x ed . \nInth i sp ap e r ,f o rth eﬁ r s tt im e ,th ed e s i gno fl e a rn edB l o om \nﬁ l t e r si sp r op o s edande v a lu a t edb yc on s id e r in gth em a ch in e \nl e a rn in gm od e la son eo fth ev a r i ab l e sinth ep r o c e s s .Ind e t a i l , \nf o rag i v enm em o r ybud g e t ,s e v e r a lLBF sa r ec on s t ru c t edu s in g \nd i f f e r en tm a ch in el e a rn in gm od e l sandth eon ew i thth el ow e s t \nf a l s ep o s i t i v er a t ei ss e l e c t ed .W ed em on s t r a t eth a tou rapp r o a ch \nc ana ch i e v emu chb e t t e rp e r f o rm an c eth ane x i s t in gLBFd e s i gn s \np r o v id in gr edu c t i on so fth eFPRo fupt o9 0%ins om es e t t in g s . \nIn d e xT e rm s —L e a rn edB l o omF i l t e r s ;N e tw o rk in g ;M a ch in e \nL e a rn in g ;URLc l a s s iﬁ c a t i on . \nI .I NTRODUCT ION \nB l o omﬁ l t e r sa r ew i d e l yu s e di ns o f tw a r ed eﬁ n e dn e tw o r k - \ni n gf o re x am p l et om a n a g eﬂ owt a b l e s[ 1 ] ,t oc l a s s i f yp a c k e t s \n[ 2 ]o rt or e d u c et h ef o rw a r d i n gs t a t e[ 3 ] .B l o omﬁ l t e r sc a n \ne fﬁ c i e n t l yim p l em e n ta p p r o x im a t em em b e r s h i pc h e c k i n ga n d \nm a n yv a r i a n t sa n do p t im i z a t i o n sh a v eb e e np r o p o s e do v e rt h e \ny e a r s[ 4 ] . \nAr e c e n tu s eo fm a c h i n el e a r n i n gi st oo p t im i z et h eim p l e - \nm e n t a t i o no fB l o omﬁ l t e r s .I np a r t i c u l a r ,t h eL e a r n e dB l o om \nF i l t e r(LBF )w a si n t r o d u c e di n[ 5 ] .T h eLBFc om b i n e sa \nm a c h i n el e a r n i n gm o d e lw i t hat r a d i t i o n a lB l o omﬁ l t e ra n d \ni sa b l et os i g n iﬁ c a n t l yr e d u c et h et o t a lm em o r yr e q u i r e dt o \na c h i e v eag i v e nf a l s ep o s i t i v er a t ei nm a n yp r a c t i c a ls e t t i n g s . \nT h ei n i t i a lLBFh a sb e e ns u b s e q u e n t l ye x t e n d e db yu s i n g \na d d i t i o n a lB l o omF i l t e r st h a ta r ec h e c k e db e f o r e[ 6 ]o ra f t e r \nt h em a c h i n el e a r n i n gm o d e l[ 7 ] ,[ 8 ]a n df u r t h e rr e d u c et h e \nf a l s ep o s i t i v er a t e .A l s o ,s c h em e st os u p p o r tt h eu s eo ft h e \nLBFi ns t r e am i n ga p p l i c a t i o n so nw h i c he l em e n t sa r ei n s e r t e d \na n dr em o v e dd y n am i c a l l yh a v eb e e nr e c e n t l yp r o p o s e d[ 9 ] . \nA ni n t e r e s t i n go b s e r v a t i o nt h a tt ot h eb e s to fo u rk n ow l e d g e \nh a sn o tb e e nm a d eb e f o r e ,i st h a ti nb o t ht h eo r i g i n a lLBF \nd e s i g na n di ni t se x t e n s i o n sa n do p t im i z a t i o n s ,t h em a c h i n e \nl e a r n i n gm o d e li sg i v e na n dﬁ x e d .T h i sm e a n st h a tt h el e a r n e d \nB l o omﬁ l t e r sc a np o t e n t i a l l yb ef u r t h e ro p t im i z e db yc o n s i d - \ne r i n gt h em a c h i n el e a r n i n gm o d e la sa n o t h e rv a r i a b l ei nt h e \nd e s i g np r o c e s s .I nm o r ed e t a i l ,f o re x am p l e ,d i f f e r e n tv a l u e s \no ft h ep a r am e t e r so ft h el e a r n e rc a nb eu s e dt ob u i l dd i f f e r e n t \n1Z .  D a i  a n d  A .  S h r i v a s t a v a  a r e  w i t h  t h e  D e p t .  o f  S t a t i s t i c s \na n d  D e p t .  o f  C om p u t e r  S c i e n c e ,  R i c e {zhenwei.dai, \nanshumali }@rice.edu \n2P .R e v i r i e g oa n dJ .A .H e r n ´a n d e za r ew i t hU n i v e r s i d a dC a r l o sI I Id e \nM a d r i d ,L e g a n ´e s2 8 9 1 1 , {revirieg,jahgutie }@it.uc3m.es LBFi n s t a n c e sw i t ht h es am em em o r yb u d g e ta n dt h eo n ew i t h \nt h el ow e s tFPRc a nb es e l e c t e d . \nI nt h i sp a p e r ,w em a k et h ef o l l ow i n gc o n t r i b u t i o n s : \n1 )E x p l o r et h eu s eo ft h el e a r n e ra sav a r i a b l ei nt h ed e s i g n \no fLBF sa n de v a l u a t ei t sp e r f o rm a n c e . \n2 )S h owt h a tt h ep r o p o s e da p p r o a c hc a nr e d u c es i g n i f - \ni c a n t l yt h eFPRf o rd i f f e r e n td a t a s e t sa n dm a c h i n e \nl e a r n i n gm o d e l sw i t hr e d u c t i o n so fu pt o9 0% . \nT h er e s to ft h ep a p e ri so r g a n i z e da sf o l l ow s .I ns e c t i o n \nI I ,B l o omﬁ l t e r sa n dl e a r n e dB l o omﬁ l t e r sa r ed i s c u s s e dt o \np r o v i d et h eb a c k g r o u n dn e e d e df o rt h er e s to ft h ep a p e r .S e c - \nt i o nI I I ,i n t r o d u c e sa n dd i s c u s s e st h eo p t im i z a t i o no fl e a r n e d \nB l o omﬁ l t e r sb yc o n s i d e r i n gt h em a c h i n el e a r n i n gm o d e la s \nav a r i a b l ei nt h ep r o c e s s .T h ep r o p o s e da p p r o a c hi se v a l u a t e d \ni ns e c t i o nIVf o rap r a c t i c a lc a s es t u d yt os h owi t sp o t e n t i a l \nb e n eﬁ t s .F i n a l l y ,t h ep a p e re n d sw i t ht h ec o n c l u s i o na n ds om e \ni d e a sf o rf u t u r ew o r ki ns e c t i o nV . \nI I .B LOOM FILTERSAND LEARNED BLOOM FILTERS \nT h i ss e c t i o nc o v e r st h eb a c k g r o u n do fB l o omﬁ l t e r sa n d \nl e a r n e dB l o omﬁ l t e r s . \nA .B l o omF i l t e r(BF ) \nB l o omﬁ l t e r s(BF s )w e r ei n t r o d u c e dm o r et h a nﬁ f t yy e a r s \na g ot op r o v i d ea p p r o x im a t es e tr e p r e s e n t a t i o nw i t hf a s tm em - \nb e r s h i pt e s t i n ga n dsm a l lm em o r yf o o t p r i n t[ 1 0 ] .BF sh a v e \nb e e nw i d e l ya p p l i e di nd i f f e r e n tﬁ e l d ss u c ha sc om p u t i n ga n d \nn e tw o r k i n g[ 1 1 ] .T h o u g hp r e v i o u ss t u d i e sh a v ep r o v i d e dt h e \nt h e o r e t i c a ll im i to fBF[ 1 2 ] ,t h eo p t im i z a t i o na n da p p l i c a t i o n \no fBFr em a i n sa na c t i v er e s e a r c ha r e a[ 1 3 ] . \nAB l o omﬁ l t e ri sab i ta r r a yo fs i z e Mi n i t i a l i z e dw i t h \nz e r o .T oc om p r e s sas e to fk e y s ,S ,t ot h eb i ta r r a y ,e v e r yk e y \nxi sm a p p e dt o kd i f f e r e n tb u c k e t su s i n g ki n d e p e n d e n th a s h \nf u n c t i o n s , h1,h 2, . . . ,h k.T oi n s e r tt h ek e y x,t h eb i tv a l u e so f \nhi(x)a r es e tt oo n e .C o n v e r s e l y ,t oc h e c kw h e t h e raq u e r y q\ni si nt h es e t ,w er e t u r np o s i t i v ei fa l lt h eb i tv a l u e so f hi(x)\na r es e tt oo n e .O t h e rw i s ean e g a t i v ei sr e t u r n e d .T h ed e s i g no f \nBFe n s u r e sz e r of a l s en e g a t i v er a t e(FNR ) .H ow e v e r ,BFm a y \ng e n e r a t es om ef a l s ep o s i t i v e s .I th a p p e n sw h e na l lt h eh a s h i n g \nl o c a t i o n so f x,hi(x),c o l l i d ew i t ht h ek e y s . \nI ne x p e c t a t i o n ,t h eFPRc a nb ea p p r o x im a t e dw h e n Mi s \nl a r g eb y : \nE(FPR )≈1−ek·n\nMk\n.( 1 ) \nE q .1s u g g e s t s t h a tt h e E(FPR )o fBFd e p e n d so nt h e \nr a t i o n\nM.I n s e r t i n gm o r e k e y si n t oBFi n t r o d u c e saw o r s eFPR . \nG i v e nat a r g e tFPRl e v e l ,e q .1c a nb eu s e dt od e t e rm i n et h e \nm em o r yb u d g e t . \n\nB. Learned Bloom\nFilter (LBF)\nLearned Bloom ﬁlter (LBF) incorporates a classiﬁcation\nmodel to the BF. It can achieve a lower FPR by reducing\nthe number of keys inserted into the BF. First, LBF trains a\nclassiﬁcation model on the available data to determine whether\nthe given query xis positive or not based on the observed\nfeatures. Then, LBF selects a threshold, T, where the query x\nis identiﬁed to be positive if f(x)> T. Otherwise, query xis\npassed to a backup ﬁlter to check its membership as shown.\nSimilar to the standard BF, LBF also has no false negatives.\nThe false positives of LBF can be either caused by the false\npositives of the classiﬁcation model (f (xjx =2S)\u0015T) or that\nof the backup Bloom ﬁlter. If the learner classiﬁes most of the\nkeys accurately, LBF can insert fewer keys into the backup\nﬁlter, which achieves a better trade-off and helps reducing the\noverall FPR. This initial design can be optimized by having\na ﬁlter before and after the learned model as proposed in the\nsandwiched learned Bloom ﬁlter [6].\nC. Adaptive Learned Bloom Filter (Ada-BF)\nAdaptive learned Bloom ﬁlter (Ada-BF) improves the LBF\nby making use of the full spectrum of the prediction scores.\nLBF partitions the prediction score space into two regions,\nf(x)\u0015Tandf(x)< T. When f(x)\u0015T, the membership\nis fully determined by the learner (zero hash function is used).\nWhile f(x)< T, the membership of xis further determined\nusing khash functions by the backup ﬁlter. Ada-BF partitions\nthe score space into multiple regions, and varies number of\nhash functions in different regions. Hence, Ada-BF achieves\ndifferent FPRs across regions. Generally, most of the non-keys\nfall in the low score regions but only a few of them fall in\nthe high score regions. And the keys have an opposite trend.\nHence, in the low score regions, Ada-BF tends to use more\nhash functions to achieve a smaller FPR (for most the non-\nkeys). While in the high regions, Ada-BF uses fewer hash\nfunctions and tolerates a higher FPR. By tuning the number\nof hash functions adaptively to the score distribution, Ada-BF\ncould achieve a lower overall FPR compared to the LBF [7].\nD. Partitioned Learned Bloom Filter (PLBF)\nThe partitioned learned Bloom ﬁlter [8] partitions the score\nspace into multiple regions like the Ada-LBF and uses differ-\nent Bloom ﬁlters for each of those regions. The main contribu-\ntions of PLBF is to generalize Ada-LBF by using independent\nBloom ﬁlters for each region rather than just varying the\nnumber of hash functions and to formulate the derivation of\nthe regions and ﬁlter parameters as an optimization problem\nand providing and analytical solution. As a result, PLBF is\nexpected to outperform the Ada-LBF in terms of FPR for a\ngiven memory budget.\nIII. O PTIMIZING LBF S\nThe current methods to design LBFs to minimize the FPR\nfor a given memory budget consider the following problem:\n(P1) given a memory budget B, a learned model, and a target\nLBF architecture with parameters A, ﬁnd the values of A\nthat minimize the FPR of the LBF . Therefore, the learned\nmodel is ﬁxed and not part of the design process. Instead,the proposed approach to optimize LBFs can be formally\nformulated as this alternative problem: (P2) given a memory\nbudget B, a target learner algorithm with parameters P, and\na target LBF architecture with parameters A, ﬁnd the values\nofPandAthat minimize the FPR of the LBF . For example,\nwe have a random forest algorithm whose parameters are the\nnumber of trees and leaves and an Ada-LBF whose parameters\nare the score regions, the number of hash functions for each\nregion and the size of the backup Bloom ﬁlter and we want to\ndetermine the settings for all those parameters that minimize\nthe FPR while using a memory smaller than a target size.\nAs discussed in the previous section, a learned Bloom ﬁlter\nis formed by a machine learning model and one or more\nbackup ﬁlters. Therefore in the design of all the learned Bloom\nﬁlters, the memory budget is divided in two parts, one for the\nBloom ﬁlters and the other for the machine learning model. To\nachieve the lowest FPR for a given memory budget, we have\nto explore the optimal memory allocation to both parts. In the\ncase of BF, the relationship between memory and performance\n(expected FPR) is well understood and can be evaluated by a\nsimple closed form formula. Instead, for the machine learn-\ning models, the relation between memory and classiﬁcation\naccuracy depends on the data set and algorithm used and can\nnot be easily modeled. Intuitively, allocating more memory to\nthe machine learning model improves the accuracy, but also\nreduces the memory available for the backup ﬁlter. Therefore,\nﬁnding the learner that achieves the best trade-off between\naccuracy and model size is not straightforward.\nThe size of the learner depends both on the model structure\nand some hyper-parameters. For example, let us consider a\nRandom Forest (RF) classiﬁer. For a RF, the learner’s size\ndepends on the number of decision trees and the size of each of\ntree. Therefore, by selecting different number of trees and tree\nsizes, models that require different memory can be constructed.\nSimilarly for a Support Vector Machine, the number of support\nvectors determines the amount of memory needed and for\na neural network, the number of neurons and connections\nbetween them. Therefore, for each model structure, it is\npossible to provide implementations with different memory\nusage and accuracy by varying the model hyper-parameters.\nFor the learned Bloom ﬁlters, given the machine learning\nmodel, there are still some hyper-parameters to tune in order\nto achieve the optimal performance, i.e. the threshold Tin\nLBF.\nDetermining analytically the parameters of the machine\nlearning model that will result in the LBF implementation with\nthe lowest FPR is a challenging theoretical problem. Therefore,\nto show the potential beneﬁts of the proposed approach, we use\na simpler experimental method to ﬁnd the best conﬁguration\nby trying different choices of the hyper-parameters of the\nmachine learning model, and testing the FPR achieved by\nthe learned Bloom ﬁlters under each hyper-parameter setup.\nThis allows us to evaluate the potential of considering the\nmachine learning model as part of the LBF design process.\nThe development of a theoretical framework to determine the\nbest conﬁguration is left for future work.\n\nIV.\nEVALUATION\nTo illustrate the potential beneﬁts of the proposed approach,\nwe test the FPR of three different learned Bloom ﬁlters, LBF,\nAda-BF and Partitioned LBF, under several machine learn-\ning models: Random Forest (RF), Support Vector Machines\n(SVM) and Neural Networks (NNs) with a single hidden layer.\nOur experiments are performed on the malicious URLs and\nmalware datasets used in [7]. The malicious URLs dataset\nincludes 485,730 unique URLs, where 16.47% of them are\ntagged as malicious. We extracted 17 lexical features to the\ntrain of learner. The malware dataset includes 41,323 benign\nﬁles and 96,724 viral ﬁles. As in [7], we also train the models\nusing the well-known Python implementation, Scikit-Learn.\nBy varying the number of leaves and number of classiﬁcation\ntrees, number of support vectors and number of neurons in the\nhidden layer we evaluate the effect of the classiﬁcation models\non the overall FPR achieved by the learned Bloom ﬁlters.\nThe three models are trained using 30% of the samples.\n1) To train random forest models with different accuracy, we\nvary the number of trees from 6 to 15 and the number of\nleaves from 2 to 20; 2) For the SVM models, we raise the\npenalty weight Cfrom 0:01\u000225to0:01\u0002220. The scale\nof penalty weight is negatively correlated with the number of\nsupport vectors and model size; 3) The NNs are tuned through\nincreasing the hidden layer dimension from 30to310.\nTo have a fair comparison of Ada-BF, PLBF and LBF\nunder different learner setups, we ﬁx the total memory budget\n= 400K, 600K 800K for the URL data set and of 400K,\n500K and 600K for the malware dataset. The backup ﬁlter\nsize equals to the memory budget minus the learner size. We\nrandomly choose 30% samples to tune the hyper-parameters\nof LBF, PLBF and Ada-BF. Since the learned Bloom ﬁlters\nhave no false negatives, the performance of learned Bloom\nﬁlters is measured by the FPR.\nThe size of the models increases linearly with the number\nof trees and leaves, support vectors, and neurons. Instead, the\nmarginal increment of the model accuracy decreases. Hence,\nwhen the model size becomes larger, it may not be beneﬁcial\nto keep investing more memory on the learner to improve\nthe model accuracy. The results for the RF are summarized\nin Figures 1 and 2. We can observe that the FPR decreases\nsharply as we start to increase the number of leaves. But after\nthe number of leaves is larger than 4, the FPR starts to increase\nslowly though the FPR has some small ﬂuctuations. Therefore,\nto optimize the performance of the learned Bloom ﬁlters, we\nmay not choose the classiﬁcation model with best prediction\naccuracy, suggesting the importance of jointly optimizing the\nlearner and Bloom ﬁlters. Compared to the design in [7] where\nthe number of trees is 10 and number of leaves is 20, jointly\noptimizing the Ada-BF, LBF, and PLBF reduces the FPR by\naround 70% under different memory budgets for the URL\ndataset, and up to 90% for the malware dataset (Table I). This\nis a remarkable improvement for real applications.\nThe results for SVMs and NNs are summarized in Figures 3\nand 4 for the URL dataset. It can be seen that depending on\nthe memory budget and learning ﬁlter type, the lowest FPR\nis obtained for different hyperparameter values in both theSVM and the NN. This conﬁrms that considering the machine\nlearning model as part of the learned Bloom ﬁlter design would\nreduce the FPR for a given memory budget. In the case of the\nNNs, there is a trend to increase the FPR as the number of\nneurons in the hidden layer increases suggesting that there is\nno beneﬁt in investing more memory on the NN. For the SVM,\nthe lowest FPRs are typically achieved by values in the middle\nof the penalty weight range explored.\nV. C ONCLUSIONS AND FUTURE WORK\nLearned Bloom ﬁlters that combine a machine learning\nmodel with Bloom ﬁlters have shown signiﬁcant reductions in\nfalse positive rate over traditional Bloom ﬁlters. In this paper,\nwe proposed to use the machine learning model as a variable\nin the design showing that it can further reduce the false\npositive rate of learned Bloom ﬁlters. Our experiments suggest\nthat tuning the learner can signiﬁcantly reduce the FPR and\nimproves the learned Bloom ﬁlters signiﬁcantly. Our ﬁndings\nprovide a strong motivation to further study the optimization\nof learned Bloom ﬁlters considering the learner model as\none of the design elements. Future work can for example\nexplore how to ﬁnd the best parameters for the learned model\nwithout exhaustively testing all the possible combinations and\nto formalize the optimization problem.\nACKNOWLEDGMENT\nThe authors would like to acknowledge the support of the\nEU H2020 project PIMCITY (grant no. H2020-871370) to the\ndevelopment of this work.\nREFERENCES\n[1] R. Challa, Y . Lee, and H. Choo, “Intelligent eviction strategy for efﬁcient\nﬂow table management in openﬂow switches,” in 2016 IEEE NetSoft\nConference and Workshops (NetSoft), 2016, pp. 312–318.\n[2] P. Reviriego, J. Mart ´ınez, D. Larrabeiti, and S. Pontarelli, “Cuckoo ﬁlters\nand bloom ﬁlters: Comparison and application to packet classiﬁcation,”\nIEEE Transactions on Network and Service Management, pp. 1–1, 2020.\n[3] A. Craig, B. Nandy, and I. Lambadaris, “Forwarding state reduction for\nmulti-tree multicast in software deﬁned networks using bloom ﬁlters,”\ninIEEE International Conference on Communications (ICC), 2019.\n[4] L. Luo, D. Guo, R. T. B. Ma, O. Rottenstreich, and X. Luo, “Optimizing\nbloom ﬁlter: Challenges, solutions, and comparisons,” IEEE Communi-\ncations Surveys Tutorials, vol. 21, no. 2, pp. 1912–1949, 2019.\n[5] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The case\nfor learned index structures,” in Proceedings of the 2018 International\nConference on Management of Data, ser. SIGMOD ’18. New York,\nNY , USA: Association for Computing Machinery, 2018, p. 489–504.\n[6] M. Mitzenmacher, “A model for learned bloom ﬁlters, and optimizing\nby sandwiching,” arXiv, 2019.\n[7] Z. Dai and A. Shrivastava, “Adaptive learned bloom ﬁlter (ada-bf):\nEfﬁcient utilization of the classiﬁer,” arXiv, 2019.\n[8] K. Vaidya, E. Knorr, T. Kraska, and M. Mitzenmacher, “Partitioned\nlearned bloom ﬁlter,” arXiv, 2020.\n[9] Q. Liu, L. Zheng, Y . Shen, and L. Chen, “Stable learned bloom ﬁlters for\ndata streams,” Proc. VLDB Endow., vol. 13, no. 12, p. 2355–2367, Jul.\n2020. [Online]. Available: https://doi.org/10.14778/3407790.3407830\n[10] B. H. Bloom, “Space/time trade-offs in hash coding with allowable\nerrors,” Commun. ACM, vol. 13, no. 7, p. 422–426, Jul. 1970. [Online].\nAvailable: https://doi.org/10.1145/362686.362692\n[11] S. Tarkoma, C. E. Rothenberg, and E. Lagerspetz, “Theory and practice\nof bloom ﬁlters for distributed systems,” IEEE Communications Surveys\nTutorials, vol. 14, no. 1, pp. 131–155, 2012.\n[12] L. Carter, R. Floyd, J. Gill, G. Markowsky, and M. Wegman, “Exact and\napproximate membership testers,” in Proceedings of the tenth annual\nACM symposium on Theory of computing. ACM, 1978, pp. 59–65.\n[13] P. Reviriego and O. Rottenstreich, “The tandem counting bloom ﬁlter -\nit takes two counters to tango,” IEEE/ACM Transactions on Networking,\nvol. 27, no. 6, pp. 2252–2265, 2019.\n\nURL Ada-BF LBF PLBF\nmemory MS (T,\nL) opt FPR (\n[7]) MS (T,\nL) opt FPR (\n[7]) MS (T,\nL) opt FPR\nM=400K 52.7K (6, 4) 0.450% (1.655%) 113.6K (6, 13) 1.400% (3.384%) 93.3K (6, 10) 0.281% (1.460%)\nM=600K 52.7K (6, 4) 0.087% (0.244%) 52.7K (6, 4) 0.280% (0.963%) 120.3K (6, 14) 0.050% (0.200%)\nM=800K 133.8K (6, 16) 0.022% (0.041%) 52.7K (6, 4) 0.083% (0.250%) 79.7K (6, 8) 0.013% (0.040%)\nMalware Ada-BF LBF PLBF\nmemory MS (T,\nL) opt FPR (\n[7]) MS (T,\nL) opt FPR (\n[7]) MS (T,\nL) opt FPR\nM=400K 115.3K (7, 11) 0.092% (0.308%) 138.9K (7, 14) 0.181% (0.728%) 138.9K (7, 14) 0.058% (0.217%)\nM=500K 154.7K (7, 16) 0.017% (0.162%) 115.3K (7, 11) 0.097% (0.453%) 138.9K (7, 14) 0.039% (0.094%)\nM=600K 236.9K (9, 20) 0.010% (0.056%) 154.7K (7, 16) 0.060% (0.281%) 115.3K (7, 11) 0.015% (0.070%)\nTABLE I: The\nsize of optimal classiﬁcation model and comparison of the FPR. The column MS (T, L) gives the size of the learner and\nthe corresponding number of trees and number of leaves. The column opt FPR ( [7]) provides the optimal FPR and the FPR achieved under\nthe learner setup in [7].\n(a) FPR of\nAda-BF with different learners\n (b) FPR of\nLBF with different learners\n (c) FPR of\nPLBF with different learners\nFig. 1: Performance of Ada-BF, LBF, and PLBF with a Random Forest in the malicious URL dataset\n(a) FPR of\nAda-BF with different learners\n (b) FPR of\nLBF with different learners\n (c) FPR of\nPLBF with different learners\nFig. 2: Performance of Ada-BF, LBF, and PLBF with a Random Forest in the malware dataset\n(a) FPR of\nAda-BF with different learners\n (b) FPR of\nLBF with different learners\n (c) FPR of\nPLBF with different learners\nFig. 3: Performance of Ada-BF, LBF, and PLBF with a SVM in the malicious URL dataset\n(a) FPR of\nAda-BF with different learners\n (b) FPR of\nLBF with different learners\n (c) FPR of\nPLBF with different learners\nFig. 4: Performance of Ada-BF, LBF, and PLBF with a single hidden layer NN in the malicious URL dataset",
  "textLength": 23853
}