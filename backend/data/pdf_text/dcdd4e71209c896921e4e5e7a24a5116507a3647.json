{
  "paperId": "dcdd4e71209c896921e4e5e7a24a5116507a3647",
  "title": "Accelerating Deep Learning Inference via Learned Caches",
  "pdfPath": "dcdd4e71209c896921e4e5e7a24a5116507a3647.pdf",
  "text": "Accelerating Deep Learning Inference via Learned Caches\nArjun Balasubramanian\u0003\nAmazon Web ServicesAdarsh Kumar\u0003\nAmazonYuhan Liu\nUniversity of Wisconsin - Madison\nHan Cao\nUniversity of Wisconsin - MadisonShivaram Venkataraman\nUniversity of Wisconsin - Madison\nAditya Akella\nUniversity of Wisconsin - Madison\nAbstract: Deep Neural Networks (DNNs) are witnessing in-\ncreased adoption in multiple domains owing to their high\naccuracy in solving real-world problems. However, this high\naccuracy has been achieved by building deeper networks,\nposing a fundamental challenge to the low latency inference\ndesired by user-facing applications. Current low latency so-\nlutions trade-off on accuracy or fail to exploit the inherent\ntemporal locality in prediction serving workloads.\nWe observe that caching hidden layer outputs of the DNN\ncan introduce a form of late-binding where inference requests\nonly consume the amount of computation needed. This en-\nables a mechanism for achieving low latencies, coupled with\nan ability to exploit temporal locality. However, traditional\ncaching approaches incur high memory overheads and lookup\nlatencies, leading us to design learned caches - caches that\nconsist of simple ML models that are continuously updated.\nWe present the design of GATI, an end-to-end prediction serv-\ning system that incorporates learned caches for low-latency\nDNN inference. Results show that GATIcan reduce inference\nlatency by up to 7.69 \u0002on realistic workloads.\n1 Introduction\nMachine learning models based on deep neural networks\n(DNNs) have surpassed human-level accuracy on tasks rang-\ning from speech recognition [74], image classiﬁcation [33,68]\nto machine translation [31]. As a result, several enterprises are\nnow deploying DNNs as a part of their applications, many of\nwhich are user-facing and hence latency-sensitive [23, 43, 79].\nThis gain in accuracy has largely come from models becom-\ning more complex, typically deeper or having more layers. As\neach layer of DNN inference depends on the output from the\nprevious layer, such deep models exacerbate inference latency,\nhurting the performance of applications that rely on them. For\ninstance, the top-5 classiﬁcation accuracy for ImageNet [9]\nincreased from 71% in 2012 to 97% in 2015 but the models\nbecame 20\u0002more computationally expensive, and inference\nlatency has correspondingly worsened ~15 \u0002.\n\u0003Equal Contribution. Work done while at UW-Madison.\nPREDICTION Y\n... ...Input A\nInput B(a)\n(b)   Cache Miss   Cache HitFigure 1: A caching mechanism where (a) Hidden layer outputs along\nwith associated predictions are cached. (b) During inference, we do a\ncache lookup after every layer. Cache hits yield faster predictions.\nA number of recent efforts have been aimed at reducing\ninference latency, such as: (a) quantization-based approaches\nthat perform computation at lower precision [22, 81], (b)\nmodel pruning-based approaches that prune dependencies\nacross layers [17], and (c) distillation based techniques that\nteach a smaller model to behave like a larger model [34]\n(§2.1.1). These approaches face two key issues. (1) All the\nabove approaches trade-off some ﬁxed amount of accuracy\nfor latency improvements as shown in Figure 2b. In other\nwords, they bind to a speciﬁc point in the latency-accuracy\ntrade-off space forallprediction inputs. However, as prior\nwork has shown [42,64], cheaper DNNs (e.g., Resnet-18) suf-\nﬁce for easier inputs while deeper models (e.g., Resnet-152)\nare only necessary to handle more difﬁcult examples. This\nsuggests that postponing the binding to during inference can,\nin theory, lead to lower latencies for easier inputs without\nsigniﬁcantly impacting accuracy. (2) Prediction serving sys-\ntems typically serve user facing applications which are often\ntemporally dominated by a small number of classes [35, 64].\nThus requests have a notion of temporal locality . Existing\napproaches like distillation can leverage this temporal locality\nby retraining or ﬁne-tuning the model. However, this only\nimproves the accuracy, not the latency of future requests, and\nretraining could also be expensive for deep models [76].\nTraditionally, caches are used in web-services to improve\nlatency for workloads with temporal locality. This paper ex-\nplores using caches to both provide lower latencies for easier\ninputs and exploit temporal locality. To apply such a caching-\nbased approach to inference, we propose that during inference,\nif the result of any intermediate computation matches what we\nhave seen earlier, we can skip the computations of the remain-\n1arXiv:2101.07344v1  [cs.LG]  18 Jan 2021\n\ning layers of the DNN (Figure 1). Caches at deeper layers in\na model capture information about progressively harder-to-\nclassify inputs. Our approach to caching thus paves the way\nforlate-binding the work performed for inference based on the\ninput’s hardness , allowing for lowering of prediction latency,\ndepending upon the layer at which we observe a cache hit,\nwithout impacting accuracy. Intermediate (or hidden) layer\ncaches can be initially populated using user-provided vali-\ndation data and updated to hold the intermediate outputs for\nrecent inputs, thereby exploiting temporal locality.\nHowever, using a direct approach of storing hidden layer\noutputs as a cache leads to many challenges: hidden layer\noutputs in DNNs are high dimensional (e.g., 262144 ﬂoats\nfor block 31of ResNet-50) needing signiﬁcant memory for\nstorage and high latency at lookup (§2.2). To address these\nchallenges, we propose using simple machine learning models\nto represent the cache at each layer. Given the hidden layer\noutput, such a learned cache model at a layer predicts if we\nhave a cache-hit, and if so the ﬁnal result to use. This provides\nimproved latency during cache-hits; cache misses continue\ninference on the base model leading to no loss in accuracy.\nUsing ML models for the learned cache means that we\nneed a methodology to both determine the predicted output\nof the base model and if this can be considered as a cache\nhit. We address this problem using a novel approach where\nthe learned cache for a given layer of the base DNN uses\ntwo trained networks, a predictor network that generates the\npredicted output and a selector network that decides if the\nprediction can be used as a cache hit.\nWe develop GATI, a system for DNN inference that auto-\nmatically integrates learned caches into the inference work-\nﬂow given a pre-trained base DNN model and a validation\ndataset. During an initial deployment phase, GATIautomati-\ncally explores model architectures for predictor and selector\nnetworks, estimating their hit-rate and accuracy using the vali-\ndation dataset. Using this, GATIdetermines the optimal set of\nlayers that should include a learned cache and the architecture\nto use to minimize the overall average latency, taking into\naccount memory and compute resources available.\nAt runtime, GATIperforms two main functions: query plan-\nning when using a directed acyclic graph (DAG) of models\nto process a user query, and incremental retraining of learned\ncaches to provide temporal locality. Using learned caches\nenables run-time reﬁnement of query plans, or incremental\nreplanning , in systems like Nexus [63] that execute a DAG of\nmodels. We describe how GATIcan incrementally improve\nprediction accuracy given a latency SLO for a DAG of models,\nbased on which models in the DAG obtained a cache hit.\nFinally, temporal shifts in input data might lead to worse\nhit rate for learned caches. For example, if we train an object\ndetection model where the validation data consists of video\nsnippets of car trafﬁc, our learned caches will give a higher hit\n1ResNet-50 consists of 16 blocks, where each block has 3 convolutional\nlayers and a residual connection.\nInput\nObject\nDetect\nFace\nRecog.V ehicle\nRecog.\nOutput(a)\n020406080100120140160180\nLatency (ms)556065707580859095100Accuracy3040\n45 10\n8\n6ResNet-101\nResNet-50\nResNet-34Late Binding\nPruning(x%)\nQuantization(n-bit)\nDistillation (b)\nFigure 2: (a) DAG of models to be executed for a trafﬁc analysis ap-\nplication (b) Latency-accuracy trade-off for ResNet-152 on CIFAR-100.\nAccuracies are wrt. base ResNet-152 model. G ATIenables late-binding.\nrate for input frames with cars. However the class distribution\nmight change over time. We handle that in GATIby designing\nan online retraining scheme that periodically and quickly\nretrains predictor and selector networks when their hit-rate is\nworse than the expected hit rate.\nWe evaluate GATI against both systems and algorith-\nmic techniques for low latency inference using popular\ndatasets [6, 49] and video streams [4, 7, 11]. Results show\nthatGATIcan opportunistically get cache hits for easier in-\nputs and improve average latency by up to 1.95\u0002over the\nbase model and 1.39\u0002over existing low latency techniques\non image datasets. On real-world videos that exhibit temporal\nlocality, GATIgives up to 7.69\u0002improvement in average la-\ntency. Additionally, when using a DAG of models (Figure 2a),\nwe see that incremental replanning simultaneously improves\nthe overall accuracy, with respect to ground truth, by ~1%\nand prediction latency by 1.26\u0002.\n2 Background and Motivation\nA Deep Neural Network (DNN) consists of a sequence of\nlayers, each of which is a mathematical function on the output\nof the previous layer. We refer to the ﬁrst layer of the DNN\nthat accepts the input as the input layer and the ﬁnal layer\nthat represents the prediction as the output layer . We term the\nremaining intermediate layers of the DNN as hidden layers .\n2.1 Prediction Serving and DNN Inference\nA prediction serving system serves machine learning models\nused for inference [23, 61, 63]. It accepts queries from clients\nwhich may require performing inference on a single model or\na sequence of models. For instance, consider a trafﬁc analysis\napplication (Figure 2a). The analysis would ﬁrst require ob-\nject detection followed by either face or vehicle recognition\ndepending upon the object, in order to deliver the ﬁnal output.\nWe can therefore view the query as a DAG of ML models\nthat need to be executed. Client requests are often associated\nwith a latency SLO within which the entire query needs to be\nexecuted. Prediction serving systems are deployed on a ﬂeet\nof servers consisting of CPUs and accelerators (GPUs, TPUs,\ncustom ASICs [3, 28, 41]).\nDNN inference in prediction serving systems must meet\n2\n\ntwo requirements: (1) Low Latency. As trained models often\nsupport user-facing applications, low inference latency ( ~10-\n100ms) [79] is key. However, executing DNN inference is\ncomputationally intensive and imposes high latency today [42,\n63, 79]. Though some hardwares (e.g., GPUs) offer higher\nthroughput when executing requests in batches, many systems\nperform minimal or no batching to keep the promise of low\nlatency [20, 32, 45, 79]. (2) High Accuracy. With DNNs\nbeing increasingly deployed in critical applications such as\nfraud detection and personal assistants, predictions need to\nhave high accuracy.\n2.1.1 Current state of DNN Inference\nIncreasingly, deeper DNN models with many layers are being\nused to achieve high accuracy, but they severely impact la-\ntency. Viewing the dependencies among DNN computations\nperformed during inference as a computation graph [13], in-\nference latency depends on: (i) the per-node latencies in the\ngraph, and (ii) the critical path length. Existing techniques to\nreduce inference latency can be categorized on how they ma-\nnipulate the computation graph. All of these techniques come\nwith an inherent trade-off in accuracy as shown in Figure 2b:\nReducing cost of individual nodes. This is used in model\nquantization [22, 38, 81] based techniques which perform\ncomputations at lower precision rather than 32-bit ﬂoating\npoint, thereby reducing the cost of individual computations.\nWe see (Figure 2b) that quantization can result in ~5-10% loss\nin accuracy, while giving 1.2 \u0002-1.5\u0002latency improvement.\nReducing length of critical path. Model distillation [34]\nis an example here, where a smaller DNN (say ResNet-50)\ntermed the student is taught by a deeper trained DNN (say\nResNet-152). We again see that it can result in ~8-30% loss\nin accuracy, while giving 2.7 \u0002-4.1\u0002improvement in latency.\nReducing the number of nodes and edges. Network prun-\ning [17] encompasses a broad set of techniques that fall into\nthis category. For example, Network Slimming [54] exploits\nchannel-level sparsity and removes insigniﬁcant channels\nfrom convolutional neural networks. Our experiments show\nthat pruning can result in ~3-7% loss in accuracy, while giving\n2.6\u0002-2.7\u0002improvement in latency.\n2.1.2 Opportunities in DNN Inference\nNext, we present opportunities to improve DNN inference:\n(O1) Opportunity from DNN model architecture. Exist-\ning solutions for lowering latencies pick a ﬁxed point on the\nlatency-accuracy trade-off space (Figure 2b). Thus, they early-\nbind, as they require committing to a particular latency and\naccuracy value prior to DNN deployment.\nWe observe that deeper DNNs are built to offer an incre-\nmental accuracy beneﬁt over their shallower counterparts.\nFor instance, ResNet-50 [33] has an accuracy of 94.2% on\nCIFAR-10 [49] while a shallower ResNet-18 has an accuracy\nof 93.3%. Intuitively, ResNet-18 seems to sufﬁce for obtain-\ning accurate predictions for 93.3% seemingly \"easy\" inputs,\nwhile the \"extra\" layers in ResNet-50 offer an accuracy beneﬁtfor an additional ~1% \"hard\" inputs. This raises the question -\ncan we obtain lower latencies without trading-off accuracy by\nopportunistically deciding the number of layers to compute?\nAchieving this necessitates late-binding the decision of the\nnumber of layers that need to be computed at inference time,\nthereby obtaining progressively lower latencies for easier in-\nputs without trading off on accuracy (Figure 2b). In practice\nwe ﬁnd that late-binding can improve average latency by\n~1.95\u0002for real-world workloads (§7.1).\nToday, using an ensemble of models [23] or cascades of\nspecialized models [42, 64] are some techniques that enable a\nlimited spectrum of progressive latencies. However, as shown\nin our experiments ( §7.1), these techniques either inﬂate tail\nlatencies or require extra resources.\n(O2) Opportunity from workload. Since prediction serving\nsystems typically serve user-facing web applications, requests\nhave a skewed distribution dominated by few classes [27, 51].\nVideo analytics represent another important workload class\nthat uses prediction serving systems. Both workloads exhibit\ntemporal locality , where certain classes may be popular for a\ngiven window of time [35, 64]. Current solutions can adapt to\ntemporal locality by retraining or ﬁne-tuning the model [76].\nHowever, this only improves accuracy and not latency and\nretraining can be expensive for deep models [76].\nWe note that temporal locality has been exploited in other\ndomains [18,27,30,60,80] like web-caching to extract latency\nbeneﬁts. Given an ability to late-bind the amount of compu-\ntation depending on whether an input is \"easy\" or \"hard\",\nwe see an opportunity to similarly exploit temporal locality\nand opportunistically obtain lower latencies for the \"easier\"\nfrequently occurring requests.\n(O3) Opportunity from system architecture. Prediction\nserving systems typically consist of a query scheduler respon-\nsible for orchestrating the execution of multiple DNN models\nconstituting a query [63]. The scheduler’s query planner de-\ntermines how to apportion the available query latency SLO\namongst different DNN models that constitute the query, and\ncomputes a query evaluation plan (QEP), i.e., which DNN\nmodel is to be executed at each stage of the query.\nCurrent approaches offer ﬁxed latencies and hence the\nquery planner determines a ﬁxed QEP prior to query exe-\ncution [63]. Late-binding however provides an opportunity\nforincremental replanning - this can distribute the “saved”\nlatency from early execution of an \"easy\" input to down-\nstream model executions. This could help use more expen-\nsive/complex downstream DNNs, resulting in higher accuracy\nfor the query while not violating the latency SLO (§5.1).\n2.2 A Proposal for Caching\nTo exploit the above opportunities, we propose adopting the\nuse of caches to complement DNNs during inference. Infer-\nence involves performing a forward-pass on the DNN. During\nthe forward pass, if the result of any intermediate computation\nmatches what we have seen earlier, we can skip the computa-\n3\n\nScheme Lookup Latency (GPU) Memory Accuracy\nk-NN (k=50) 2075.5 ms 5000 MB 79.84 %\nLSH 71.4 ms 5000 MB 39.77%\nk-means (k=100) 1.511 ms 333 MB 76.06%\nTable 1: Overheads incurred by caching mechanisms at a single layer\n[ResNet-18, Block 3]. k-NN returns the majority label amongst knear-\nest neighbors as prediction. k-means clusters hidden layer outputs and\nreturns a representative label from the nearest cluster as prediction.\ntions of the remaining layers of the DNN and directly arrive\nat the ﬁnal prediction.\nInspired by the design of multi-level caches, we propose as-\nsociating a cache with each hidden layer of the DNN. Figure 1\nillustrates a simple mechanism to realize the caching of DNN\ncomputations. As a pre-processing step, we can cache the\nhidden layer outputs along with the ﬁnal predictions from the\nvalidation dataset. During inference, if a hidden layer output\nmatches what was cached earlier, we deem this as a cache hit\nand skip performing computations for the remaining layers\nand directly deliver the ﬁnal prediction. Caching thus paves\nthe way for late-binding inference computation .\nChallenges in caching DNN computations. While caching\nholds promise as also observed in prior work [50], high di-\nmensionality of hidden layer outputs introduces a number\nof challenges in designing caches. Cache lookups for DNN\ncomputations require distance-based similarity search such\nask-nearest neighbors ( k-NN), k-means clustering or locality\nsensitive hashing (LSH) [71] to infer cache hits and obtain\npredictions (Table 1). High dimensionality means that the\nmemory overhead associated with such caches would be high\n(as in k-NN). This makes caching challenging on accelerators\nsuch as GPUs that have limited memory. Further, the ﬁdelity\nof distance-based search degrades in high-dimension [15],\nwhich could result in errors in cache lookup as observed in\nTable 1. Techniques such as LSH which can deal with high\ndimensions cannot be used due to the high memory over-\nheads associated with storing the hidden layer outputs. High\ndimensionality also exacerbates lookup latencies.\n2.3 Towards Learned Caches\nTo address the challenges mentioned above, we propose adopt-\ning a learning-based approach for caching. Instead of com-\nplementing each layer of the DNN with a traditional cache\nconsisting of data entries, our key idea is to use a simple\nmachine learning model (Figure 3a). We train a model that\nmimics a cache lookup function and term this as a learned\ncache . The interface exposed by a learned cache remains the\nsame; i.e., the learned cache takes in a hidden layer output\nand delivers a prediction in the event of a cache hit.\nUsing a learned cache alleviates the issues associated with\nhigh dimensionality. First, ML models are effective at han-\ndling high dimension inputs, thereby allaying concerns about\nthe ﬁdelity of cache lookups. Second, a learned cache does\nnot actually store any data but is instead a succinct representa-\ntion encapsulated by the weights of the model. The size of a\nlearned cache depends on the model architecture and is inde-\npendent of the number of items that constitute the cache. Thus,Selector T=1 Selector T=0\nPredictor Correct True Positive (TP) False Negative (FN)\nPredictor Wrong False Positive (FP) True Negative (TN)\nTable 2: Confusion matrix for learned caches\nLayer\nHit \n/ MissHidden \nLayer Output\nTraditional Cache Hidden Layer Output Prediction\n<0.5, 0.1, ...>         2\n<0.1, 0.2, ...>       3\nLearned CacheLayer\nModel \n(E.g. NN)Hit / MissHidden\n Layer Output\n(a)\nSelector NetworkPredictor  Network\nSelect ?\nCache Hit\nReturn  PREDICTIONYesNoPREDICTION\nCache \nMissHidden Layer Output (b)\nFigure 3: (a) Difference between traditional and learned cache. (b)\nStructure and operation of a learned cache.\na learned cache can be used to encapsulate a large number of\ndata entries without additional memory overheads. Third, the\nlatency of a learned cache lookup is dictated by the cost of\nexecuting an ML model. By choosing an appropriate model\narchitecture, it is possible to keep lookup latencies below\nan acceptable limit. Additionally, simple ML models with a\nsmall number of parameters can be retrained quickly using\nrecent data, thereby allowing learned caches to effectively\nexploit temporal locality.\n3 Learned Cache Design\nGiven our goal of using ML models to represent caches, we\nobserve that ML models are directly amenable to generating\na prediction; e.g., for a classiﬁcation problem we can train\na model that predicts the ﬁnal class given the hidden layer\noutput so far. However, to avoid returning incorrect results\nwe also need to determine if the prediction is correct or not.\nThus, we observe that we can decompose the cache lookup\noperation into two sub-operations as shown in Figure 3b:\n(i) A prediction operation that computes an output prediction\ngiven a hidden layer output. (ii) A selection operation that\ndetermines whether the prediction is a cache hit or not.\nWe model the two sub-operations using two different neu-\nral networks : a predictor network and a selector network. We\nuse neural networks as they are universal function approxi-\nmators and provide a direct mechanism to approximate the\nbase DNN model. Next, we describe the neural network ar-\nchitectures that can be used to build predictor and selector\nnetworks, and systems requirements that guide their design.\n3.1 Predictor and Selector Networks\nUsing a neural network for predictors and selectors opens up\na large design space. We start by discussing systems require-\nments that guide our design.\nLatency, Memory Usage. As the selector and predictor net-\nworks are run in sequence, the cache lookup latency is the sum\nof their execution times. The memory usage is similarly the\ntotal amount of memory required to store model parameters\nas well as the runtime memory used while doing inference on\nthese networks. Thus, we target using a simple network with a\n4\n\nName Architecture FLOPS\nFully-Connected InputFC\u0000!Hidden LayerFC\u0000!Out put 33.5M\nFC(h) (h)\nPooling InputPool\u0000\u0000! Hidden LayerFC\u0000!Output 4.3M\nPool(h) (h)\nConvolution InputConv\u0000\u0000! Hidden LayerFC\u0000!Output 0.3M\nConv(k,s) (k,s)\nTable 3: Model architectures currently supported in G ATIfor the pre-\ndictor network. All of them use ReLU as the activation function.\nlimited number of hidden layers for low-latency and memory.\nHit-Rate, Accuracy. We deﬁne hit-rate to be fraction of\nlookups that yield a cache hit. Since learned caches are ap-\nproximate, we also need to account for the accuracy of the\nprediction, which we deﬁne as the fraction of lookups that do\nnot yield incorrect cached predictions. Learned caches need\na high hit rate with accuracy above a certain threshold (say\n97%) relative to base model. Note that false negatives (FN;\nTable 2), where the learned cached returned a miss but the\nprediction was correct, do not hurt accuracy as we fall back\nto the base DNN on a cache miss.\nGiven these requirements we next present various design\nchoices and how they meet the requirements above.\nPredictor Network Design. We restrict predictor networks\nto simple neural networks to satisfy latency, memory require-\nments, and consider three possibilities (Table 3): (a) a fully\nconnected architecture (FC) that can learn non-linear combi-\nnations of inputs and project it onto the output dimension, (b)\na convolution based architecture that limits non-linear combi-\nnations to local regions of the input and (c) a pooling-based\narchitecture that reduces the spatial size of the inputs before\nprojecting them onto the output dimension. We choose these\nthree options as they represent different points in terms of\nFLOPs required (Table 3). Our system is extensible and can\ninclude other model architectures.\nSelector Network Design. While the predictor network at-\ntempts to mimic the base DNN, the selector network has a\nsimpler role as it only needs to perform a binary classiﬁcation\nof whether we have a cache hit or miss. We ﬁnd that using\na simple neural network with one hidden layer that projects\nthe output of the predictor network onto an output layer that\nenables a binary decision to be made is sufﬁcient and other\narchitectures do not provide any signiﬁcant beneﬁts.\nTraining Learned Caches. Consider a base DNN model with\nNhidden layers L1, ..,LNand that we are given Minput\nsamples from a validation dataset, X1, ..,XM, to construct\nlearned caches. To train a predictor network, we ﬁrst run a\nforward pass of the base DNN over the Minput samples.\nDuring the forward pass for each input Xj, we collect the\nhidden layer output at layer iasHO i;jand also record the\nﬁnal prediction of the base DNN as Yj, where Yjis a vector\nrepresenting the distribution of class probabilities for classiﬁ-\ncation ( argmaxjYjis predicted class). The collection of data\n<HO i;1;Y1>;::;< HO i;M;YM>is then used to train a pre-\ndictor network at layer iof the DNN. Given that the predictor\nnetwork is trying to mimic the behavior of the rest of the base\nDNN, we borrow insights from distillation [34] and use a lossBlock Arch. Accuracy Hit CPU GPU Memory\nRate Latency Latency Cost\n3 FC(1024) 97.3% 38.8% 6.08 ms 0.43 ms 268 MB\n3 Pool(8192) 96.7% 34.1% 1.32 ms 0.53 ms 33 MB\n3 Conv(3,1) 96.2% 20.4% 1.66 ms 0.48 ms 2 MB\n6 FC(1024) 99.5% 62.9% 2.94 ms 0.47 ms 134 MB\n6 Pool(8192) 96.2% 54.4% 0.64 ms 0.5 ms 33 MB\n6 Conv(3,1) 99.3% 49.4% 0.68 ms 0.49 ms 0.8 MB\nTable 4: Trade-off space exposed by different model architectures for\npredictor networks at the end of the 3rdand6thResNet-18 block.\nfunction that takes into account the true labels and also the\ndistribution of class probabilities.\nA selector network at layer iis a function T=si(PR),\nwhere PRis the class distribution prediction from the predic-\ntor network and Tis a binary decision. To train a selector\nnetwork at layer i, we ﬁrst construct ground truth labels ( G)\nin the following manner:\nGi;j=8\n<\n:1;if argmax\njPRi;j=argmax\njYj\n0;otherwise\nThe collection of data <PRi;1;Gi;1>;::;< PRi;M;Gi;M>can\nbe used to train a selector network at layer i. A key function of\nthe selector network is to reduce the number of false positives\n(FP) so as to achieve high accuracy. To this end, we employ a\ncustom cross-entropy loss function that levies a higher loss\npenalty for FPs in comparison for FNs, since FNs do not\nimpact accuracy as discussed earlier.\n3.2 Predictor Design Trade-offs\nWe now discuss the trade-offs involved in choosing the appro-\npriate architecture for predictor networks by considering three\nlearned cache variants for the 3rdand6thblocks in ResNet-18\nas illustrated in Table 4. We train all variants per the procedure\ndescribed earlier (§3.1).\nHit-rate vs. System resources. From Table 4, we notice\nthat architectures that are more computationally expensive\ntake up more systems resources (have larger lookup latencies\nand memory cost), but offer greater hit rate and hence greater\nreduction in the average end-to-end inference latency (see\nFC(1024) vs. Pool(8192) for both block 3 and 6).\nHardware dependent behavior. From Table 4, we notice\nthat the nature of the trade-off depends on the target hardware\ndue to differences in the underlying lookup latencies. For\ninstance, we notice that FC(1024) has both a better hit rate\nand lookup latency on GPU compared to Pool(8192), while\nFC(1024) has higher lookup latency on CPU. This is because\nthe fully-connected layer requires a large dense matrix mul-\ntiplication and this operation can be effectively parallelized\nacross many thread blocks available on a GPU.\nRole of base DNN layer. From Table 4, we also see that\nthe dynamics of the trade-off space varies across layers of\nthe base DNN. We see that all architectures have a higher hit\nrate at block 6 compared to block 3 indicating that caching\nis easier as we get closer to the output layer. Additionally,\nthe lookup latencies and memory costs also reduce at block 6\n5\n\nsince dimensionality of the hidden layer output reduces as we\ngo deeper in the base DNN.\nCollectively the three observations indicate that the appro-\npriate choice of network architecture depends on the systems\nresources available, the hardware being used, and the base-\nDNN layer being considered. This motivates the need for a\nscheme that can reason about various learned cache variants\nat each layer of the DNN and collectively optimize the system\nfor end-to-end latency beneﬁts. We next present a general ap-\nproach to address this challenge of composing learned caches\nfor end-to-end beneﬁts.\n4 Composing Learned Caches\nThe trade-offs presented in the previous section indicate that\na number of learned cache variants could be applicable for\neach layer of the DNN. To allow for systematic exploration,\nour system ﬁrst runs an exploration phase , where for each\nvariant corresponding to a base DNN layer, we compute the\nexpected hit rate, accuracy, lookup latency, and memory cost.\nThen, we use these as inputs to select a subset of these\nlearned cache variants for inference. GATIaccomplishes this\nin acomposition phase by formulating it as an optimization\nproblem (Figure 4). The above two phases are one-time opera-\ntions performed when inference service owners upload trained\nDNNs. Once the learned caches are deployed, GATIexploits\ntemporal locality by ﬁne-tuning or retraining the chosen cache\nvariants in an online manner (§5.4).\nConsider the following toy example that illustrates the prob-\nlem underlying the ﬁnal choice of the optimal learned caches\nto use during inference. Suppose we have an aggregate mem-\nory budget of 564 MB available for learned caches. Table 4\nshows that a potential choice can be to greedily select archi-\ntectures at earlier layers that offer high hit-rates. This would\nmean selecting FC(1024) at block 3 which yields cache hits\nfor 38.75% of requests. However, it uses up all of the available\nmemory budget and hence the remaining 62.25% of requests\nwould incur the latency of running the entire base DNN. The\nlatency for the 38.75% cached requests would be the time\nto compute the ﬁrst three blocks of ResNet plus the lookup\nlatency (6.08 ms).\nAnother potential choice can be to select Pool(8192) at\nblock 3 and FC(1024) at block 6. Combined, they have a\nmemory cost of 264 MB, which is within the memory budget.\nWe would get cache hits for 34.1% of requests at block 3.\nInterestingly, due to a lower lookup latency, the latency for\nthese requests would be lesser than the latency of requests\nwith cache hits at block 3 in the ﬁrst option. Further, an ad-\nditional 28.8% of requests would get cache hits at block 6,\nand the remaining 37.1% of requests would incur the latency\nof running the entire base DNN. The second option is prefer-\nable since it can signiﬁcantly reduce the average latency of\nrequests, even though a small additional fraction of requests\nachieve higher latency relative to the ﬁrst option. Composing\nan optimal set of caches thus requires a global view of the\nComposition Phase Exploration Phase   Pooling        Fully-Connected     ConvolutionFigure 4: (a) Exploration phase - G ATIconsiders multiple cache variants\nat each layer of the DNN (b) Composition phase - G ATIchooses a subset\nof variants to complement the base DNN during inference.\ntrade-off space across layers of the DNN.\nGoal: The goal of the composition phase is to jointly select\na global set of learned caches that minimize the expected aver-\nage latency , while ensuring minimal degradation in accuracy\nand meeting compute and memory constraints.\nWe can formulate this problem as a mix-integer quadratic\nprogram but we ﬁnd that the formulation is intractable while\nhandling deep networks. We present the optimization problem\nin full detail in Appendix A.1. GATIuses a relaxation of the\nformulation whose key details we explain below.\nLet us assume that we have a DNN with Nlayers and K\ncache variants at each layer. We index each variant by its\nlayer iand variant number j. From the exploration phase, we\nobtain the following metrics for each variant - Hit Rate ( Hi;j),\nAccuracy ( Ai;j), Lookup Latency ( Ti;j), and Memory Cost\n(Mi;j). Additionally, we proﬁle the latency for the computation\nof each layer ( Li). Binary variable bi;jindicates if learned\ncache variant jat layer iis chosen. bi;j=1means that the\nvariant is selected.\nWe use a three-step approach to simplify the composition\nproblem formulation:\nStep 1. Accuracy Filter: We do not consider variants whose\naccuracy Ai;jis below a minimum accuracy threshold A.\nStep 2. Score Computation: Motivated by the example from\nbefore, we consider two factors in determining the importance\nof a particular variant: (i) the hit rate of the variant and (ii) the\nlatency gain obtained by using the model variant in the event\nof a cache hit. We compute latency gain (LG) for a learned\ncache variant as the ratio of running time for the entire DNN\nto the running time assuming that a cache hit is obtained at\nthe given learned cache variant.\nLGi;j=N\nå\nk=1Lk=(i\nå\nk=1Li+Ti;j) (1)\nWe prefer higher hit rates and higher latency gains. How-\never, these are fundamentally at odds with each other since\nhigher latency gains are obtained using variants at earlier\nlayers of the base DNN where the hit rates would be lower,\nand vice versa. To balance these two factors, we compute a\nscore ( S- higher is better) that captures the beneﬁt of using a\nvariant:\nSi;j=a:(Hi;j) + (1\u0000a):(LGi;j) (2)\n6\n\nModule DNN Latency Accuracy\nResNet-18 27.36 ms 91.1%\nObj. Detect ResNet-34 41.05 ms 92.9%\nResNet-50 54.5 ms 94.1%\nSE-LResNet9E-IR 17.38 ms 95.5%\nSE-LResNet18E-IR [24] 36.75 ms 97.6%\nFace SE-LResNet50E-IR 58.34 ms 98.1%\nSE-LResNet101E-IR 110.32 ms 99.1%\nResNet-9 16.14 ms 90.2%\nResNet-18 23.68 ms 91.8%\nVehicle ResNet-50 54.12 ms 92.6%\nResNet-101 111.42 ms 93.4%\nTable 5: DNN model options at each node of trafﬁc analysis application.\nwhere ais a knob that lies in [0,1] and controls the relative\nimportance of hit rate and latency gain.\nStep 3. Resource Constraints: We constrain the total mem-\nory occupied by chosen variants to be within a memory budget\nM. For computation, to avoid latency inﬂation, we wish to run\nthe learned caches asynchronously while the computation of\nthe base DNN proceeds. To minimize the amount of resources\nrequired, we specify a computational constraint that we can\natmost perform one cache lookup at a given point of time2.\nFinally, the objective of our formulation is to maximize the\nsum of scores for chosen variants:\nmax.N\nå\ni=1K\nå\nj=1bi;j:Si;j (3)\nThe computed values of bi;jthen determine which learned\ncache variants should be used along with the base DNN during\ninference. We next describe how the above composition phase\nis integrated into the end-to-end query lifecycle and present\nthe design of our system G ATI.\n5 G ATISystem Design\nWe design GATI, an end-to-end prediction serving system that\nleverages learned caches to speed up DNN inference serving\n(Figure 5). Users interact with GATIby issuing a query along\nwith a latency SLO (query completion deadline). Similar\nto [63], GATIdetermines the dataﬂow DAG of DNNs that\nneed to be executed for a query. Like prior work [63], GATI\nconsiders simple DAGs with chains or fork-join dependencies,\nthat have a single input and a single output prediction. In\naddition, GATIallows the inference service owner to specify\nan array of possible DNN models with different accuracies\nthat can be used at each node of the dataﬂow DAG.\nExample: Consider the trafﬁc analysis application as shown\nin Figure 2a which represents a DAG of models that need\nto be executed. As shown in Table 5, the inference service\nprovider can specify an array of possible DNNs that can be\nused for each node of the DAG. These options can vary from\ncheap DNNs that have lower accuracy to more expensive\nDNNs that have higher accuracy.\n2Our proﬁling on GPUs suggests that running more than two concurrent\nmodels using MPS or CUDA streams imposes a 10-15% overhead.Pseudocode 1 Query Replanning Algorithm\n1:\u0000\u0000!PLB .Map holding partial latency budgets\n2:.Prepare a QEP for DAG D with LatencySLO L and begin execution\n3:procedure ONRECEIVE QUERY (DAG D, LatencySLO L)\n4:\u0000\u0000!PLB = C OMPUTE PARTIAL BUDGET WITHPOLICY (D,L)\n5: DNN = P ICKBESTMODEL (D.ROOT ,\u0000\u0000!PLB)\n6: EXECUTE DNN(DNN)\n7:end procedure\n8:.Called when a DNN D ﬁnishes execution\n9:procedure ONEXECUTION COMPLETE (DNN D)\n10: .Redistribute the saved latency amongst downstream nodes\n11: SAVED LATENCY = D.B ASEMODEL LATENCY -\u0000\u0000!PLB(D)\n12:\u0000\u0000!PLB = R EDISTRIBUTE LATENCY WITHPOLICY (\u0000\u0000!PLB,SAVED LATENCY )\n13: for all NODE2D:downstreamNodes do\n14: DNN = P ICKBESTMODEL (D.ROOT ,\u0000\u0000!PLB)\n15: EXECUTE DNN(DNN)\n16: end for\n17: end procedure\n5.1 Query Planner\nGiven a query, GATI’s query planner formulates a query eval-\nuation plan (QEP) that captures what DNN model to pick for\nexecution at each node of the dataﬂow DAG, while ensuring\nthat the latency SLO for the query is met.\nExample: Consider a query for the trafﬁc analysis applica-\ntion (Figure 2a) with a latency SLO of 80 ms. First, the planner\nneeds to split the available latency SLO budget amongst the\ndifferent nodes in the dataﬂow DAG. There can be multiple\npolicies [43, 63,67] to compute these partial budgets (§5.2.1).\nLet us assume a simple policy that divides the latency budget\nequally among all nodes: this will allocate 40 ms for object\ndetection and 40 ms for face/vehicle recognition.\nNext, the planner needs to choose a DNN with maxi-\nmum accuracy that can be executed within the partial la-\ntency budget. For object detection, state-of-the-art static plan-\nners [63, 75] would choose ResNet-18, and SE-LResNet18E-\nIR and ResNet-18 for face and vehicle recognition respec-\ntively. This constitutes the QEP for a latency SLO of 80 ms.\nLearned caches enable incremental replanning: Consider\na scenario where learned caches are used and, because of\ncache hit, say object detection can execute in 20 ms instead\nof 27 ms. This leaves 60 ms instead of 53 ms for face/vehicle\nrecognition. With a new latency budget of 60 ms, we can now\nuseSE-LResNet50E-IR andResNet-50 for face and vehicle\nrecognition respectively, without violating the latency SLO.\nThese options have higher accuracy than the originally chosen\nones. Thus, learned caches enable incremental replanning of\nthe yet-to-be-traversed DAG nodes, leading to higher end-to-\nend accuracy for the query.\n5.2 Incremental Replanning Algorithm\nGATI’s query planner realizes incremental replanning by de-\nferring the selection of a particular DNN at a node to when\nthe node needs to be scheduled for execution (Algorithm 1).\n(i) On receiving a query, the planner allocates partial latency\nbudgets to each node of the DAG according to a policy to\ncompute partial latency budgets (described below) and begins\nquery execution at the root of the DAG. (ii) When a node\n7\n\nneeds to be executed, the planner picks the highest accuracy\nDNN at that node that can be executed within the partial\nlatency budget (Line 5 in Algorithm 1). (iii) When a DNN\nﬁnishes execution, the planner computes the saved latency by\nsubtracting the observed execution time from the allocated\npartial latency budget. This saved latency is redistributed\namongst downstream nodes according to the partial latency\nbudget allocation policy.\n5.2.1 Computing Partial Latency Budgets\nThe query planner considers all possible critical paths from\nthe input to the output in the DAG to compute partial latency\nbudgets. First, GATIcomputes a partial latency budget for\neach node by considering one DAG-input-to-DAG-output\npath at a time. Borrowing from planners that support the\nexecution of a DAG for prediction serving tasks [43,63], GATI\ncurrently supports two policies to compute partial latency\nbudgets.\nEqual Split. This splits the latency SLO equally amongst\neach node in a path. If Lis the latency SLO and there are N\nnodes in the path, then each node receives a budget ofL\nN.\nProportional Split. This policy splits the latency SLO\namong nodes in a critical path in proportion to the maximum\nexecution time of any DNN option in that node. If there are\nNnodes in an input-output path and Ciis the maximum exe-\ncution time among options at node i, then the partial latency\nbudget for node iis computed as -\nLi=Ci\nåN\nj=1Cj\u0003L (4)\nNext, GATIcomputes the ﬁnal partial latency budget at a\nnode as the minimum partial latency budget available for the\nnode from all possible paths, so as to meet the latency SLO\nfor any path that the request might go through.\nGATIallows any policy to be plugged into the incremental\nreplanning framework as shown in Algorithm 1, which we\nalso evaluate in §7.3.\n5.3 Retraining Learned Caches\nGATIretrains learned caches to exploit the temporal locality\ninherently present in online workloads (§2.1.2): retraining\nachieves the effect of ﬁne-tuning the caches, leading to better\nhit rates. We sample input queries that users issue to GATIand\nmaintain a window of samples obtained over time. Then, we\nperiodically retrain the predictor and selector network for each\nlearned cache variant chosen in the composition phase using\na mix of data from the window of samples and the original\nvalidation dataset provided by the user [44]. While picking\nsampled inputs, we weigh recent samples more heavily [76].\nWe obtain the training data needed for retraining by running\na forward pass over the base DNN on the chosen sampled\ninputs as described earlier in §3.1.\nWe have two parameters to control retraining: the number\nof samples stored and the frequency at which retraining is\ntriggered. Our evaluation shows that using ~20% samples we\nTRAINED\nDNNVALIDA TION\nDATA\nINITIAL UPLOAD\nLEARNED CACHE\nCONSTRUCTION\nEXPLORA TION COMPOSITION\nMODEL REPOSIT ORY\nTRAINED\nDNNsVERSIONED\nLEARNED\nCACHESSTOREDEPLOY\nFETCH UPDA TED CACHES PERIODICALL Y\nCACHE ADAPT ATION SER VICE\nFINE-TUNE AND\nADAPT CACHESSTORE\nINFERENCE SER VICEUSER\nQUER Y\nSCHEDULER QUER Y REPLANNER\nSEND DNN REQUEST\n31\n221\n3RECORDFigure 5: End-to-end system design of G ATI. Dashed lines show deploy-\nment phase; solid lines show online phase.\ncan perform retraining within 500ms, owing to the simplicity\nof the predictor/selector networks’ designs, and that a retrain-\ning period of 15 minutes works well for realistic workloads.\nWe present more detailed results in Section 7.2.2.\n5.4 Overall System Design\nAs outlined in Figure 5, users interact with GATIin two ways:\n(i)Deployment Phase : Inference service owners begin DNN\ndeployment by uploading a trained model and its validation\ndataset. Given this, GATIconstructs an initial set of learned\ncaches by exploring multiple variants and composing an opti-\nmal set of learned caches (§4). The base DNN and along with\nlearned caches are then deployed to the inference service.\n(ii)Online Phase : Queries issued to GATIgo through a front-\nend query planner that formulates a QEP for the query and\nhandles its execution. The cache adaptation service records\nsamples of the query inputs from the inference service and\nperiodically retrains the learned caches. The inference service\nperiodically pulls new versions of learned caches.\n6 Implementation\nWe developed a prototype of GATIthat implements the above\nsystem design. Each of the services (query scheduler, infer-\nence service, cache adaptation service) are written as Apache\nThrift [1] services that communicate using RPCs. Our proto-\ntype currently supports DNNs written in PyTorch [12] and\nsupports inference on CPUs and GPUs.\nFor the exploration of learned cache variants we load the\nPyTorch model from the checkpoint and add hooks to save\nhidden layer outputs by traversing the list of Modules in the\nmodel. The composition phase uses Gurobi [8] to compute\nthe optimal set of learned caches after exploration.\nAt runtime, the query scheduler accepts queries using a\nREST API. Each DNN and its learned caches reside on the\nsame dedicated instance. We use similar hooks as in the explo-\nration phase so that cache lookups can be performed during\ninference. The learned caches lookups are done asynchronous\nto the execution of the base DNN. On CPUs, we achieve this\nby running the base DNN execution and learned cache lookup\nin different processes. Each process is pinned to a dedicated\n8\n\nset of CPU cores that do not overlap and the intermediate layer\noutput is shared between processes using pipes. This ensures\nisolation between the computations and guarantees that the\nexecution time does not exceed that of the vanilla base DNN.\nOn GPUs, we overlap computations using CUDA streams [5].\nThe base DNN computation proceeds on the default stream\nwhile the learned cache computations are issued on a different\nstream. Our cluster deployment design of GATIis similar to\nexisting prediction serving systems [23,45,63] and thus GATI\ninherits their horizontal scaling and fault-tolerance properties.\n7 Evaluation\nWe use our prototype implementation of GATIto evaluate\nifGATIcan deliver on the opportunities identiﬁed in §2.1.2.\nOur evaluation shows the following:\n(i) Across a range of datasets and state-of-the-art DNNs, GATI\nmaintains high accuracy and offers up to 1.95 \u0002improvement\nin average latency compared to the base DNN (§7.1).\n(ii)GATIexploits temporal locality in real-world videos, giv-\ning up to 7.69\u0002improvement in average latency (§7.2.1).\n(iii) Incremental replanning, when using a DAG of models,\nhelps overcome the fundamental latency-accuracy trade-off\nby simultaneously improving accuracy, with respect to the\nground truth, by ~1% and prediction latency by 1.26 \u0002(§7.3).\nTestbed Setup: We deployed GATIon a heterogeneous\ncluster comprising of 2 p3.2xlarge GPU instances and 12\nc5.4xlarge instances on AWS [2] thus measuring how GATI\nhelps on both CPU and GPUs. Each GPU instance has 1\nNVIDIA Tesla V100 GPU. For GPU experiments, we use\n1 GPU instance for the inference service and the other for\nthe cache adaptation service. For CPU inference, we use the\nc5.4xlarge instance and allocate 8 vCPUs each for the base\nmodel and learned cache computation. Each service (query\nscheduler, inference service, and cache adaptation service)\nruns as a daemon. Since the construction of initial caches is a\none-time operation, we use multiple AWS GPU spot instances\nand CloudLab [26] for exploring multiple cache variants.\nDefault Parameters: We consider six learned cache vari-\nants - FC(1024), FC(512), Pool(8192), Pool(4096), Conv(3,1),\nConv(5,2). For the composition phase, we choose the target\naccuracy Ato be 97%. We evaluate higher accuracy targets\nin §7.1.2. We use a batch size of 1 in all of our experiments\nand evaluate larger batch sizes in §7.2.1.\nLearned Cache Training Methodology: We use the vali-\ndation data from each dataset to train the predictor and se-\nlector networks. The initial cache construction uses 80% of\nthe training data, sampled at random for training each predic-\ntor/selector network. The remaining 20% is used for obtaining\nthe metrics needed by the composition phase (§4).\n7.1 Late Binding Beneﬁts\nWe ﬁrst evaluate the latency beneﬁts of using learned caches\nduring inference. To study this aspect in isolation, we disable\nthe cache adaptation service for these experiments.Model (Dataset) CPU Latency Gain GPU Latency Gain\nResNet-50 (CIFAR-10) 1.95\u0002 1.63\u0002\nResNet-18 (CIFAR-10) 1.72\u0002 1.28\u0002\nResNet-152 (CIFAR-100) 1.24\u0002 1.21\u0002\nVGG-16 (Google Commands) 1.96\u0002 1.54\u0002\nTable 6: Overall latency beneﬁts of G ATIover base models. Accuracies\nmeet the 97% target.\nWorkload: Our evaluation considers popular image classiﬁ-\ncation (CIFAR-10 and CIFAR-100 [49]) and speech recogni-\ntion (Google Commands [6]) tasks. We consider 4 different\nbase DNN architectures: VGG-16 [66], ResNet-18, ResNet-\n50, ResNet-152 [33]. These represent state-of-the-art DNNs\nthat have different number of layers. We measure accuracy,\nlatency using the respective test datasets.\nBaselines: We compare against existing techniques (§2.1.1):\n(i)Quantization (n-bit). (ii) Model distillation . (iii) Model\npruning (x%). We compare against an approach that employs\nacascade of specialized models having varying number of\nlayers, similar to [42,64]. We assume that models in a cascade\nare run serially to enable a fair comparison with GATIwhich\nuses only a single hardware resource for inference.\n7.1.1 Learned caches with ResNet-50\nWe ﬁrst consider Resnet-50 with CIFAR-10 data as the base\nDNN and use CPUs for inference. From Figure 6a, we ob-\nserve that GATIexhibits an average latency of ~34 ms, which\nis1.95\u0002lower than the latency of running the entire DNN.\nFigure 6(b) shows that GATIexhibits a spectrum of latencies\nwith an accuracy of 96.97% with respect to the base ResNet-\n50 model, which is very close to the accuracy target of 97%.\nThe learned caches occupy 1277.5 MB memory.\nAgainst quantization: GATIoutperforms both 8 and 10-\nbit quantization, improving average latencies by a factor of\n1.25\u0002and 1.74\u0002respectively while also improving accuracy\nby 0.57% and 0.97% respectively. Against 12-bit quantization,\nGATI’s accuracy is ~1% lower but it provides lower latency\nfor ~87% of requests.\nAgainst distillation: GATIoutperforms ResNet-50 distilled\nto ResNet-34 with an average latency improvement of 1.41 \u0002\nand an accuracy improvement of 0.57%. ResNet-50 distilled\nto ResNet-18 and a simple 5-layer CNN have better average\nlatency than GATIbut incur signiﬁcant trade-off on accuracy.\nAgainst pruning: GATIhas better accuracies than pruning at\n50%, 60%, and 70%, while also improving average latency by\na factor of 1.39\u0002-1.42\u0002. Pruning at 40% has higher accuracy,\nbut G ATIoffers better latency for 70% of the requests.\nAgainst cascades: We evaluate GATIagainst a cascade of\nprogressively deeper models (ResNet-18, ResNet-50). We\nconﬁgure the conﬁdence threshold to achieve an accuracy of\n97%. Figure 8a shows that cascades can give lower latencies\nthan GATIfor a greater % of requests and even offer a slightly\nlower average latency. However, GATI’s tail latency (99%-ile)\nis1.68\u0002lower. Importantly, we observe that learned caches\ncancomplement the models in a cascade . In this case we\ndeploy learned caches for both models in the cascade and this\ngives a 1.66\u0002improvement over cascades’ average latency.\n9\n\n010203040506070\nLatency (ms)020406080100% Requests40,5012 8\nResNet-34ResNet-185-layerCNN\nResNet-50\nPruning(x%)\nQuantization(n-bit)\nDistillation\nGati(a) Latency distribution\n010203040506070\nLatency (ms)8588919497100Accuracy40%\n50%\n60%12\n108\nResNet-34\nResNet-18\n5-layerCNNPruning(x%)\nQuantization(n-bit)\nDistillation\nGati (b) Latency vs. accuracy\nFigure 6: [CPU inference] Beneﬁts of late-binding for ResNet-50 on\nCIFAR-10 dataset. In (b), Size of each G ATImarker is proportional to\n% of requests at the corresponding latency point.\n10 8 6 4 2 1\nMemory Budget (GB)03691215Latency (ms)\n(a)\n024681012\nFactor of Improvement020406080100% RequestsResNet-50\nResNet-18 (b)\nFigure 7: (a) [GPU Inference, CIFAR-10] Memory budget impact\n(ResNet-50). (b) [CPU Inference, CIFAR-10] ResNet-18 vs. ResNet-50.\nUsing GPUs: Figure 8b shows that GATIhas an average la-\ntency of ~7.98 ms, which is 1.63\u0002lower than running ResNet-\n50 on a GPU. GATIexhibits a spectrum of latencies with an\noverall accuracy of 96.8% with respect to the base ResNet-\n50 model. Interestingly, the learned caches in this scenario\noccupy 8502 MB memory, which is 6.66 \u0002times the memory\noccupied for CPU inference. This can be attributed to the ob-\nservation that on GPUs, GATIused sixlearned caches where\npredictor networks had a fully-connected architecture, while\nGATIdid not pick caches with fully-connected architecture\non CPU due to high lookup latencies. Among this, we also\nnoticed that GATIpreferred to pick fully-connected architec-\ntures for initial layers in order to maximize the number of\nearly cache hits. This observation follows from the differ-\nences between CPUs and GPUs highlighted in Table 4. We\nalso notice that some baselines do not work as well on GPUs.\nFor example, comparing Figure 6b and Figure 8b, pruning\noffers signiﬁcant latency beneﬁts on CPUs but not on GPUs.\nGATIoffers similar beneﬁts on both CPUs and GPUs.\nOne-time cost to train learned caches: From our testbed,\nwe found that the time to train learned caches depended on\nthe layer of the base DNN. Training took ~1 GPU hour for\nlearned caches associated with earlier layers since the hidden\nlayer output dimensions are typically much higher. We found\nthat training later layers took ~20 GPU minutes. Overall, we\nfound that training learned caches For ResNet-50 required ~50\nGPU hours. Note that individual learned caches can be trained\nin parallel and are hence amenable to speedup via scale-out.\nFinally composing an optimal set of learned caches involves\nsolving an optimization problem that takes ~5 seconds.\nRuntime Overhead: For inference on GPUs, we noticedthat learned cache computation on an average increased GPU\nutilization by ~14%. Even with this, we found that GPUs were\nstill not fully utilized. We note that the tail latencies are within\n1%of the latency of the base DNN. Our measurements show\nthat adopting asynchronous cache lookups reduces tail latency\nby 50.11% compared to synchronous lookups.\n7.1.2 Other Workloads\nWe evaluate GATIon three other workloads and observe that\nGATIcan give similar latency beneﬁts with an accuracy close\nto 97%. For sake of brevity, we present the graphs in Appendix\n(§A.2) and summarize our ﬁndings in Table 6. We highlight a\nnumber of interesting insights from these workloads:\n(i)GATIhas better beneﬁts on deeper models: From Fig-\nure 7b, we observe that a greater number of requests get higher\nlatency improvements for a deeper ResNet-50 model for CPU\ninference. This directly follows from the discussion in §2.1.2\nthat deeper networks are built to achieve higher accuracy for a\nfew \"hard\" requests, thereby allowing GATIto extract greater\nlatency beneﬁts for \"easier\" requests through early cache hits.\nWe observe similar trends for GPU inference.\n(ii)GATIoffers high accuracy on difﬁcult tasks: From Fig-\nure 8c, we observe that GATIworks well and minimally trades-\noff on accuracy even when the base DNN has lower accuracy\n(94.2% for CIFAR-10 vs 76.4% CIFAR-100). However, we\nobserve that obtaining earlier cache hits is difﬁcult when the\nbase model itself does not have high ﬁdelity. Despite this, we\nnote that nearly 40% of requests get ~2 \u0002latency beneﬁt.\n(iii)GATIoffers beneﬁts even under constrained memory bud-\ngets: We emulated this in our testbed by giving reduced mem-\nory budgets in the composition phase. This models scenarios\nwhere GATIis deployed on low end GPUs.3From Figure 7a,\nwe see that when memory is not a bottleneck (10 GB), GATI\nimproves average latency by 1.62 \u0002with respect to the base\nmodel. With a memory budget of 1 GB, the average latency\nis1.44\u0002better than the base model. Further, GATIresponds\nto reduction in memory budget by choosing learned cache\nvariants that have smaller memory footprints. With a budget\nof 10 GB, 6 of the 13 chosen variants have a fully-connected\narchitecture, which has a greater memory requirement. With\na budget of 1 GB, GATIchooses pooling and convolutional\narchitectures, which have a smaller memory footprint.\n(iv)GATIoffers beneﬁts across accuracy targets: We mod-\niﬁed the accuracy target to be 99% instead of 97% and ob-\nserved that GATIwas able still able to give a 1.41 \u0002improve-\nment in average latency over the base ResNet-50 model with\nan accuracy of 99.12%. Though the latency beneﬁt at an ac-\ncuracy target of 99% is lower than 97% by 1.38 \u0002,GATIis\nable to deliver on its promise of meeting the accuracy target\nand still offer latency beneﬁts.\n7.2 Adapting to Temporal Locality\nIn this section, we evaluate the ability of GATIto exploit\ntemporal locality in online workloads.\n3This assumes that latencies remain same on lower end GPUs.\n10\n\n0 20 40 60 80 100\nLatency (ms)020406080100% RequestsResNet-50\nCascades\nGati + ResNet-50\nGati + Cascade(a)\n02468101214\nLatency (ms)8588919497100Accuracy40%\n50%\n60%12\n108ResNet-34\nResNet-18\n5-layerCNNPruning(x%)\nQuantization(n-bit)\nDistillation\nGati (b)\n0 40 80 120 160 200\nLatency (ms)707580859095100Accuracy3040\n4510\n8ResNet-101\nResNet-50\nPruning(x%)\nQuantization(n-bit)\nDistillation\nGati (c)\nFigure 8: (a)[CPU Inference] G ATIvs. a cascade of DNNs (b)[GPU Inference] Accuracy-latency trade-off(ResNet-50 on CIFAR-10). (c)[CPU Inference]\nAccuracy-latency trade-off(ResNet-152 on CIFAR-100). Size of each G ATImarker in b,c is proportional to % of requests at that latency point.\nbend oxford bellevue sittard\nVideo0612182430Latency (ms)Static-Gati Gati\n(a) CPU Inference\nbend oxford bellevue sittard\nVideo12345Latency (ms)Static-Gati Gati (b) GPU Inference\nFigure 9: Latency beneﬁts on various videos. Thickness shows the den-\nsity of requests at that latency. Dashed line for ResNet-18.\nWorkload: We use 4 publically available videos - Two of\nthese are trafﬁc camera videos ( bellevue [35] and bend [7])\nwhile the other two are surveillance camera videos ( ox-\nford [11] and sittard [4]). Each of these has 5-6 hours of video\nfootage. Similar to [42], we perform object classiﬁcation on\nthe video at 1 fps. All of our evaluations use ResNet-18 model\ntrained on CIFAR-10. The initial set of learned caches are\nconstructed using validation data from CIFAR-10.\nParameters: The inference service accumulates a batch of\n100 requests and samples 20% for use in the cache adaptation\nservice. We maintain 60 minutes worth of windowed samples.\nThe cache adaptation service retrains caches every 15 minutes\nand immediately deploys them. We use a learning rate of\n0.002 and retrain the learned caches for 5 epochs.\nBaseline: We compare against STATIC -GATI, where the\ncache adaptation service is disabled. Latencies get inﬂated\nwhen the inference service updates learned caches before\nexecuting a request. We ﬁlter such latencies in our results.\n7.2.1 Results\nFigure 9 shows the latency beneﬁts of adapting to inherent\ntemporal locality on both CPUs and GPUs. Across all videos,\nwe observe that GATIis able to improve the average inference\nlatency by a factor of 1.45\u0002-7.69\u0002on CPUs with respect to\nthe base ResNet-18 model. Additionally, none of the inference\nrequests required computing the entire DNN. We also observe\nthat the accuracy in all of the cases is above the target of 97%.\nGATIalso outperforms STATIC -GATIby up to a factor of\n4.61\u0002. The relative performance between the two varies ac-\ncording to the video. We see that in one case (the bend video),\nSTATIC -GATIis unable to yield any cache hits and the perfor-\nmance matches that of the base DNN. This happens when theBlock STATIC -GATIAcc./Hit-Rate GATIAcc./Hit-Rate\n1 27.95%/0% 90.56%/11%\n3 30.21%/0% 93.55%/64%\n6 85.85%/31% 94.82%/83%\nTable 7: [oxford video] Blockwise improvement in the predictor network\naccuracy yields hit rate improvements for G ATIover S TATIC -GATI.\nlearned caches are trained on data that looks different from\nthe inference requests. Retraining learned caches accounts for\nthis variance in addition to exploiting temporal locality.\nSources of Improvement: Figure 9 shows that GATIis able\nto get more cache hits at earlier layers leading to a number of\nrequests with low latencies. Table 7 shows the reason behind\nthis trend. We observe that retraining drastically improves the\nﬁdelity of predictions from the predictor network compared to\nSTATIC -GATI, with the change being highest in earlier layers.\nHigh ﬁdelity predictions allow the selector network to yield\nmore cache hits and improve the end-to-end latency.\nImpact of batching: To evaluate the impact of batching\nrequests, we modify GATIto wait for a batch and then run\ninference on a GPU. We execute each batch until both requests\nhave either had a cache hit or reached the end of DNN. We\nrecord the latency from when inference starts on GPU to\ndiscount the effects of queuing while forming batches. With\na batch size of 2, GATIgives 2.88\u0002-2.93\u0002improvement in\naverage latency in comparison to 3.67 \u0002-3.75\u0002with batch\nsize of 1. This is because, with batching, some requests might\nneed to wait for other requests in the batch to ﬁnish execution.\nRetraining is quick: Our measurements show that using sim-\nple predictor and selector networks enables retraining learned\ncaches to ﬁnish in ~480 ms. This enables fast adaptation to\nworkloads where temporal locality could change rapidly.\n7.2.2 Insights\nWe now present some insights showing the impact of the\nworkload characteristics and system parameters on the ability\nof G ATIto exploit temporal locality windows.\n(i)Beneﬁts on more skewed output class distributions: Since\nit is hard to control the distribution of objects over time in\nreal videos, we use a synthetic workload to study the impact\nof varying class distributions. We use a Zipﬁan distribution\nwhere the parameter ais used to control the distribution skew\nof object classes with lower values indicating greater skew.\nWe consider two skew levels with avalues of 1.5 and 3.0. The\n11\n\n0369121518212427\nLatency (ms)020406080100% RequestsGati(3)\nStatic-Gati(3)\nGati(1.5)\nStatic-Gati(1.5)(a)\n010 20 30 40 50 60\nTime (mins)0.000.250.500.751.00Hit Rate\n T=10\nT=40 (b)\nFigure 10: (a) Impact of skew in distribution. avalues in legend. (b)\nImpact of retraining intervals [Block 3, ResNet-18]. Markers indicate\nretraining and dashed line shows distribution change.\ndominant class in the distribution changes every 15 minutes.\nFigure 10a shows that GATIperforms better when the distri-\nbution is more skewed, offering 3.6 \u0002improvement in latency\ncompared to a 3.2\u0002gain with lesser skew. This correlates well\nto the intuition that caches perform better when the workload\nhas high occurrences of some popular objects.\n(ii)Small sampling rates sufﬁce : We observe that the end-to-\nend latency of inference is not impacted unless the sampling\nrate becomes less than 10%, motivating us to choose 20%\nfor our experiments. Interestingly, this points out that a small\nnumber of samples are sufﬁcient to capture temporal locality\neffects in realistic workloads such as videos.\n(iii) GATIis sensitive to the periodicity of retraining: To\nevaluate this aspect, we consider the same Zipﬁan workload\nused in (i) with a=1.5 and a temporal window of 20 minutes.\nWe vary the interval at which we retrain caches. We observe\nfrom Figure 10b that a lower interval of 10 mins helps GATI\nmaintain high cache hit rates and adapt quickly to changes\nin distribution. With a larger update interval (40 mins), we\nobserve that the hit rate drops continuously and increases\nagain only after the learned caches have been retrained.\n7.3 Incremental Replanning Beneﬁts\nWe deploy GATIwith the cache adaptation service disabled\nto study the beneﬁts of incremental replanning in isolation.\nWorkload: We use the same trafﬁc analysis application ex-\nample as discussed in §5. We use learned caches in the ob-\nject detection module. We sample images from the LFW\ndataset [37], Stanford Cars dataset [48], and CIFAR-100 and\nissue them as input requests. The input distribution consists\nof 46% faces and 45% vehicles. Each DNN model outlined\nin Table 5 is deployed on a separate CPU instance.\nBaselines: We consider both the equal split and smart split\npolicy (§5.2.1). We evaluate the performance of GATIagainst\na baseline that does not use learned caches or replanning.\nResults: Figure 11 shows that incremental replanning gives\naccuracy beneﬁts for a variety of latency SLOs, with up to\n~1%improvement in end-to-end accuracy and this applies\nto both the equal and smart split policy. We observe greater\nbeneﬁts at tight latency SLOs ( \u0014100 ms) since this is when\ncheaper models can be replaced with more expensive ones\nthrough replanning. Figure 12 provides insight into why re-\nplanning is able to give accuracy beneﬁts. As an example, at a\n50 80 110 140 170\nLatency SLO (ms)848586878889909192AccuracyVanilla\nGati(a) Equal Split\n50 80 110 140 170\nLatency SLO (ms)8485868788899091AccuracyVanilla\nGati (b) Proportional Split\nFigure 11: Accuracy beneﬁt from incremental replanning in G ATI\nFigure 12: DNNs used for (a) face (b) vehicle recognition for different\nSLOs. Shaded bars are for G ATIand indicate that incremental replan-\nning allows more accurate DNNs to be executed.\nlatency SLO of 80 ms, the baseline uses SE-LResNet18E-IR\nfor face recognition and ResNet-18 for vehicle recognition.\nDue to saved latencies obtained by learned cache hits, GATI\nis able to use a more accurate SE-LResNet50E-IR model for\n~79% of face recognition tasks and a more accurate ResNet-\n50 for ~80% of vehicle recognition tasks. Both of these lead\nto greater end-to-end accuracy for the query.\nWe see that GATIcan reduce average latencies by up\nto1.26\u0002along with an accuracy improvement of ~ 1%.\nThus, incremental replanning helps overcome the fundamen-\ntal latency-accuracy trade-off by improving on both metrics.\n8 Related Work\nPrediction Serving Systems. GATIis a prediction serving\nsystem just like [14, 23, 45, 46, 52, 61, 63]. GATIenables\nlate-binding in DNNs through learned caches unlike [23]\nwhich treats the model as a black-box and applies caching\nonly at the input layer. GATIborrows the idea of expressing\nqueries as DAGs from [63], but uses it to improve accuracy\nand not for resource efﬁciency. [45] is orthogonal to GATI\nsince it focuses on failure resilience and improving tail la-\ntencies. NoScope [42] uses a cascade of models to achieve a\nlimited form of late-binding. We show in §7.1.1 that learned\ncaches can complement cascades to give further latency ben-\neﬁts. [35, 46, 61] focus on optimizing the serving pipeline\nbut do not focus improving DNN inference latency. [14] uses\ncaching to improve predictions for recent data but targets\nlinear models like logistic regression and not DNNs.\nAccuracy-latency trade-off. [36, 73] complement DNNs\nwith auxiliary networks just like in GATI, but provide an\noption for conﬁgurable latency-accuracy trade-off. A number\nof systems [72,78] also enable such trade-offs. GATIhowever\nlowers latencies without signiﬁcant trade-off in accuracy.\nHigh performance inference. A number of systems [10,19,\n12\n\n39] optimize the computation graph for faster DNN inference.\nOthers speciﬁcally focus on RNNs [29, 79] and CNNs [62].\nAll of these complement GATI’s goal of late-binding the\namount of computation to the hardness of the input.\nQuery Planning. [75] outlines an idea of automatically pick-\ning models based on the latency SLO. GATIbuilds on this\nidea and provides inference service owners an option to spec-\nify an array of models at each node of the query DAG. In\nother domains, several other systems have used query re-\nplanning and dynamic adaptation to resource or workload\nchanges [25, 40, 56, 57, 70] to get performance beneﬁts.\nUsing ML in Systems. A number of projects use ML to\nimprove system design. This includes use of ML for database\nindices [47], cache replacement [65, 69], memory manage-\nment [55]. cluster scheduling [16, 59], resource manage-\nment [21], packet classiﬁcation [53], and video content deliv-\nery [58, 76, 77]. Per our knowledge, GATIis the ﬁrst work to\nuse ML models as caches to accelerate DNN inference.\n9 Conclusion\nIn this paper we presented GATI, a low-latency prediction\nserving system that uses caching to late-bind DNN inference\ncomputation. We proposed using learned caches , where the\ncache is represented by neural networks and outlined a design\nfor their construction. Our evaluations show that GATIcan\nimprove average latency for easier inputs and exploit temporal\nlocality to help online workloads.\nReferences\n[1] Apache Thrift. https://thrift.apache.org/ .\n[2]AWS EC2 instance types. https://aws.amazon.com/\nec2/instance-types/ .\n[3]Aws Inferentia. https://aws.amazon.com/\nmachine-learning/inferentia/ .\n[4]City cam, webcamsittard. https://www.youtube.\ncom/watch?v=iKxhsl3rurA .\n[5]CUDA streams. https://devblogs.nvidia.com/\ngpu-pro-tip-cuda-7-streams-simplify-concurrency/ .\n[6]Google speech commands dataset.\nhttps://ai.googleblog.com/2017/08/\nlaunching-speech-commands-dataset.html .\n[7]Greenwood avenue bend, oregon. https://www.\nyoutube.com/watch?v=YqyERQwXA3U .\n[8] Gurobi optimization. https://www.gurobi.com/ .\n[9] Imagenet. http://www.image-net.org/ .\n[10] NVIDIA TensorRT. https://developer.nvidia.\ncom/tensorrt .[11] Oxford martin school webcam - broad street, ox-\nford. https://www.youtube.com/watch?v=\nQhq4vQdfrFw .\n[12] PyTorch. http://pytorch.org/ .\n[13] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis,\nJ. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard,\nM. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G.\nMurray, B. Steiner, P. Tucker, V . Vasudevan, P. Warden,\nM. Wicke, Y . Yu, and X. Zheng. Tensorﬂow: A sys-\ntem for large-scale machine learning. In 12th USENIX\nSymposium on Operating Systems Design and Imple-\nmentation (OSDI 16) , pages 265–283, Savannah, GA,\nNov. 2016. USENIX Association.\n[14] D. Agarwal, B. Long, J. Traupman, D. Xin, and L. Zhang.\nLaser: A scalable response prediction platform for on-\nline advertising. In Proceedings of the 7th ACM Inter-\nnational Conference on Web Search and Data Mining ,\nWSDM ’14, page 173–182, New York, NY , USA, 2014.\nAssociation for Computing Machinery.\n[15] C. C. Aggarwal, A. Hinneburg, and D. A. Keim. On\nthe surprising behavior of distance metrics in high di-\nmensional spaces. In Proceedings of the 8th Interna-\ntional Conference on Database Theory , ICDT ’01, page\n420–434, Berlin, Heidelberg, 2001. Springer-Verlag.\n[16] Y . Bao, Y . Peng, and C. Wu. Deep learning-based job\nplacement in distributed machine learning clusters. In\n2019 IEEE Conference on Computer Communications,\nINFOCOM 2019, Paris, France, April 29 - May 2, 2019 ,\npages 505–513. IEEE, 2019.\n[17] D. Blalock, J. J. Gonzalez Ortiz, J. Frankle, and J. Gut-\ntag. What is the state of neural network pruning? In\nProceedings of Machine Learning and Systems 2020 ,\npages 129–146. 2020.\n[18] H. Chen, M. Song, J. Zhao, Y . Dai, and T. Li. 3d-based\nvideo recognition acceleration by leveraging temporal\nlocality. In Proceedings of the 46th International Sympo-\nsium on Computer Architecture , ISCA ’19, page 79–90,\nNew York, NY , USA, 2019. Association for Computing\nMachinery.\n[19] T. Chen, T. Moreau, Z. Jiang, L. Zheng, E. Yan, H. Shen,\nM. Cowan, L. Wang, Y . Hu, L. Ceze, C. Guestrin, and\nA. Krishnamurthy. TVM: An automated end-to-end\noptimizing compiler for deep learning. In 13th USENIX\nSymposium on Operating Systems Design and Imple-\nmentation (OSDI 18) , pages 578–594, Carlsbad, CA,\nOct. 2018. USENIX Association.\n[20] E. Chung, J. Fowers, K. Ovtcharov, M. Papamichael,\nA. Caulﬁeld, T. Massengill, M. Liu, M. Ghandi, D. Lo,\n13\n\nS. Reinhardt, S. Alkalay, H. Angepat, D. Chiou, A. Forin,\nD. Burger, L. Woods, G. Weisz, M. Haselman, and\nD. Zhang. Serving dnns in real time at datacenter scale\nwith project brainwave. IEEE Micro , 38:8–20, March\n2018.\n[21] E. Cortez, A. Bonde, A. Muzio, M. Russinovich, M. Fon-\ntoura, and R. Bianchini. Resource central: Understand-\ning and predicting workloads for improved resource\nmanagement in large cloud platforms. In Proceedings\nof the 26th Symposium on Operating Systems Principles ,\nSOSP ’17, page 153–167, New York, NY , USA, 2017.\nAssociation for Computing Machinery.\n[22] M. Courbariaux and Y . Bengio. Binarynet: Training\ndeep neural networks with weights and activations con-\nstrained to +1 or -1. CoRR ,abs/1602.02830 , 2016.\n[23] D. Crankshaw, X. Wang, G. Zhou, M. J. Franklin, J. E.\nGonzalez, and I. Stoica. Clipper: A low-latency online\nprediction serving system. In 14th USENIX Sympo-\nsium on Networked Systems Design and Implementa-\ntion (NSDI 17) , pages 613–627, Boston, MA, Mar. 2017.\nUSENIX Association.\n[24] J. Deng, J. Guo, N. Xue, and S. Zafeiriou. Arcface:\nAdditive angular margin loss for deep face recognition.\nIn2019 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR) , pages 4685–4694, 2019.\n[25] A. Deshpande, Z. Ives, and V . Raman. Adaptive query\nprocessing. Foundations and Trends ®in Databases ,\n1(1):1–140, 2007.\n[26] D. Duplyakin, R. Ricci, A. Maricq, G. Wong, J. Duerig,\nE. Eide, L. Stoller, M. Hibler, D. Johnson, K. Webb,\nA. Akella, K. Wang, G. Ricart, L. Landweber, C. El-\nliott, M. Zink, E. Cecchet, S. Kar, and P. Mishra. The\ndesign and operation of CloudLab. In Proceedings of\nthe USENIX Annual Technical Conference (ATC) , pages\n1–14, July 2019.\n[27] R. Fonseca, V . Almeida, M. Crovella, and B. Abrahao.\nOn the intrinsic locality properties of web reference\nstreams. In IEEE INFOCOM 2003. Twenty-second\nAnnual Joint Conference of the IEEE Computer and\nCommunications Societies (IEEE Cat. No.03CH37428) ,\nvolume 1, pages 448–458 vol.1, 2003.\n[28] J. Fowers, K. Ovtcharov, M. Papamichael, T. Massen-\ngill, M. Liu, D. Lo, S. Alkalay, M. Haselman, L. Adams,\nM. Ghandi, S. Heil, P. Patel, A. Sapek, G. Weisz,\nL. Woods, S. Lanka, S. K. Reinhardt, A. M. Caulﬁeld,\nE. S. Chung, and D. Burger. A conﬁgurable cloud-scale\ndnn processor for real-time ai. In Proceedings of the\n45th Annual International Symposium on Computer Ar-\nchitecture , ISCA ’18, page 1–14. IEEE Press, 2018.[29] P. Gao, L. Yu, Y . Wu, and J. Li. Low latency rnn in-\nference with cellular batching. In Proceedings of the\nThirteenth EuroSys Conference , EuroSys ’18, New York,\nNY , USA, 2018. Association for Computing Machinery.\n[30] B. S. Gill and D. S. Modha. Wow: Wise ordering for\nwrites - combining spatial and temporal locality in non-\nvolatile caches. In Proceedings of the 4th Conference on\nUSENIX Conference on File and Storage Technologies\n- Volume 4 , FAST’05, page 10, USA, 2005. USENIX\nAssociation.\n[31] H. Hassan, A. Aue, C. Chen, V . Chowdhary, J. Clark,\nC. Federmann, X. Huang, M. Junczys-Dowmunt,\nW. Lewis, M. Li, S. Liu, T.-Y . Liu, R. Luo, A. Menezes,\nT. Qin, F. Seide, X. Tan, F. Tian, L. Wu, S. Wu, Y . Xia,\nD. Zhang, Z. Zhang, and M. Zhou. Achieving human\nparity on automatic chinese to english news translation,\n2018.\n[32] K. Hazelwood, S. Bird, D. Brooks, S. Chintala, U. Diril,\nD. Dzhulgakov, M. Fawzy, B. Jia, Y . Jia, A. Kalro,\nJ. Law, K. Lee, J. Lu, P. Noordhuis, M. Smelyanskiy,\nL. Xiong, and X. Wang. Applied machine learning at\nfacebook: A datacenter infrastructure perspective. In\n2018 IEEE International Symposium on High Perfor-\nmance Computer Architecture (HPCA) , pages 620–629,\n2018.\n[33] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-\ning for image recognition. In 2016 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) ,\npages 770–778, June 2016.\n[34] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowl-\nedge in a neural network. In NIPS Deep Learning and\nRepresentation Learning Workshop , 2015.\n[35] K. Hsieh, G. Ananthanarayanan, P. Bodik, S. Venkatara-\nman, P. Bahl, M. Philipose, P. B. Gibbons, and O. Mutlu.\nFocus: Querying large video datasets with low latency\nand low cost. In Proceedings of the 12th USENIX Con-\nference on Operating Systems Design and Implemen-\ntation , OSDI’18, page 269–286, USA, 2018. USENIX\nAssociation.\n[36] H. Hu, D. Dey, M. Hebert, and J. A. Bagnell. Learning\nanytime predictions in neural networks via adaptive loss\nbalancing. CoRR ,abs/1708.06832 , 2017.\n[37] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-\nMiller. Labeled faces in the wild: A database for study-\ning face recognition in unconstrained environments.\nTechnical Report 07-49, University of Massachusetts,\nAmherst, October 2007.\n14\n\n[38] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang,\nA. Howard, H. Adam, and D. Kalenichenko. Quan-\ntization and training of neural networks for efﬁcient\ninteger-arithmetic-only inference. In The IEEE Con-\nference on Computer Vision and Pattern Recognition\n(CVPR) , June 2018.\n[39] Z. Jia, O. Padon, J. Thomas, T. Warszawski, M. Zaharia,\nand A. Aiken. Taso: Optimizing deep learning computa-\ntion with automatic generation of graph substitutions. In\nProceedings of the 27th ACM Symposium on Operating\nSystems Principles , SOSP ’19, page 47–62, New York,\nNY , USA, 2019. Association for Computing Machinery.\n[40] A. Jonathan, A. Chandra, and J. Weissman. Rethinking\nadaptability in wide-area stream processing systems. In\n10th USENIX Workshop on Hot Topics in Cloud Com-\nputing (HotCloud 18) , Boston, MA, July 2018. USENIX\nAssociation.\n[41] N. P. Jouppi, C. Young, N. Patil, D. Patterson,\nG. Agrawal, R. Bajwa, S. Bates, S. Bhatia, N. Boden,\nA. Borchers, R. Boyle, P.-l. Cantin, C. Chao, C. Clark,\nJ. Coriell, M. Daley, M. Dau, J. Dean, B. Gelb, T. V .\nGhaemmaghami, R. Gottipati, W. Gulland, R. Hag-\nmann, C. R. Ho, D. Hogberg, J. Hu, R. Hundt, D. Hurt,\nJ. Ibarz, A. Jaffey, A. Jaworski, A. Kaplan, H. Khaitan,\nD. Killebrew, A. Koch, N. Kumar, S. Lacy, J. Laudon,\nJ. Law, D. Le, C. Leary, Z. Liu, K. Lucke, A. Lundin,\nG. MacKean, A. Maggiore, M. Mahony, K. Miller,\nR. Nagarajan, R. Narayanaswami, R. Ni, K. Nix, T. Nor-\nrie, M. Omernick, N. Penukonda, A. Phelps, J. Ross,\nM. Ross, A. Salek, E. Samadiani, C. Severn, G. Sizikov,\nM. Snelham, J. Souter, D. Steinberg, A. Swing, M. Tan,\nG. Thorson, B. Tian, H. Toma, E. Tuttle, V . Vasudevan,\nR. Walter, W. Wang, E. Wilcox, and D. H. Yoon. In-\ndatacenter performance analysis of a tensor processing\nunit. In Proceedings of the 44th Annual International\nSymposium on Computer Architecture , ISCA ’17, page\n1–12, New York, NY , USA, 2017. Association for Com-\nputing Machinery.\n[42] D. Kang, J. Emmons, F. Abuzaid, P. Bailis, and M. Za-\nharia. Noscope: Optimizing neural network queries over\nvideo at scale. Proc. VLDB Endow. , 10(11):1586–1597,\nAug. 2017.\n[43] R. S. Kannan, L. Subramanian, A. Raju, J. Ahn, J. Mars,\nand L. Tang. Grandslam: Guaranteeing slas for jobs in\nmicroservices execution frameworks. In Proceedings of\nthe Fourteenth EuroSys Conference 2019 , EuroSys ’19,\nNew York, NY , USA, 2019. Association for Computing\nMachinery.\n[44] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness,\nG. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ra-\nmalho, A. Grabska-Barwinska, and et al. Overcomingcatastrophic forgetting in neural networks. Proceed-\nings of the National Academy of Sciences , 114(13), Mar\n2017.\n[45] J. Kosaian, K. V . Rashmi, and S. Venkataraman. Parity\nmodels: Erasure-coded resilience for prediction serving\nsystems. In Proceedings of the 27th ACM Symposium on\nOperating Systems Principles , SOSP ’19, page 30–46,\nNew York, NY , USA, 2019. Association for Computing\nMachinery.\n[46] P. Kraft, D. Kang, D. Narayanan, S. Palkar, P. Bailis,\nand M. Zaharia. Willump: A statistically-aware end-\nto-end optimizer for machine learning inference. In\nProceedings of Machine Learning and Systems 2020 ,\npages 147–159. 2020.\n[47] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzo-\ntis. The case for learned index structures. In Proceedings\nof the 2018 International Conference on Management\nof Data , SIGMOD ’18, page 489–504, New York, NY ,\nUSA, 2018. Association for Computing Machinery.\n[48] J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object\nrepresentations for ﬁne-grained categorization. In 4th\nInternational IEEE Workshop on 3D Representation and\nRecognition (3dRR-13) , Sydney, Australia, 2013.\n[49] A. Krizhevsky, V . Nair, and G. Hinton. Learn-\ning multiple layers of features from tiny im-\nages. https://www.cs.toronto.edu/~kriz/\nlearning-features-2009-TR.pdf , 2009.\n[50] A. Kumar, A. Balasubramanian, S. Venkataraman, and\nA. Akella. Accelerating deep learning inference via\nfreezing. In 11th USENIX Workshop on Hot Topics\nin Cloud Computing (HotCloud 19) , Renton, WA, July\n2019. USENIX Association.\n[51] A. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings,\nI. Krasin, J. Pont-Tuset, S. Kamali, S. Popov, M. Mal-\nloci, T. Duerig, and V . Ferrari. The open images dataset\nv4: Uniﬁed image classiﬁcation, object detection, and\nvisual relationship detection at scale. arXiv:1811.00982 ,\n2018.\n[52] Y . Lee, A. Scolari, B.-G. Chun, M. D. Santambrogio,\nM. Weimer, and M. Interlandi. Pretzel: Opening the\nblack box of machine learning prediction serving sys-\ntems. In Proceedings of the 12th USENIX Confer-\nence on Operating Systems Design and Implementation ,\nOSDI’18, page 611–626, USA, 2018. USENIX Associ-\nation.\n[53] E. Liang, H. Zhu, X. Jin, and I. Stoica. Neural packet\nclassiﬁcation. In Proceedings of the ACM Special In-\nterest Group on Data Communication , SIGCOMM ’19,\n15\n\npage 256–269, New York, NY , USA, 2019. Association\nfor Computing Machinery.\n[54] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang.\nLearning efﬁcient convolutional networks through net-\nwork slimming. In 2017 IEEE International Conference\non Computer Vision (ICCV) , pages 2755–2763, 2017.\n[55] M. Maas, D. G. Andersen, M. Isard, M. M. Javanmard,\nK. S. McKinley, and C. Raffel. Learning-based memory\nallocation for c++ server workloads. In Proceedings of\nthe Twenty-Fifth International Conference on Architec-\ntural Support for Programming Languages and Oper-\nating Systems , ASPLOS ’20, page 541–556, New York,\nNY , USA, 2020. Association for Computing Machinery.\n[56] S. Madden, M. Shah, J. M. Hellerstein, and V . Raman.\nContinuously adaptive continuous queries over streams.\nInProceedings of the 2002 ACM SIGMOD Interna-\ntional Conference on Management of Data , SIGMOD\n’02, page 49–60, New York, NY , USA, 2002. Associa-\ntion for Computing Machinery.\n[57] K. Mahajan, M. Chowdhury, A. Akella, and S. Chawla.\nDynamic query re-planning using QOOP. In 13th\nUSENIX Symposium on Operating Systems Design and\nImplementation (OSDI 18) , pages 253–267, Carlsbad,\nCA, Oct. 2018. USENIX Association.\n[58] H. Mao, R. Netravali, and M. Alizadeh. Neural adap-\ntive video streaming with pensieve. In Proceedings of\nthe Conference of the ACM Special Interest Group on\nData Communication , SIGCOMM ’17, page 197–210,\nNew York, NY , USA, 2017. Association for Computing\nMachinery.\n[59] H. Mao, M. Schwarzkopf, S. B. Venkatakrishnan,\nZ. Meng, and M. Alizadeh. Learning scheduling al-\ngorithms for data processing clusters. In Proceedings\nof the ACM Special Interest Group on Data Communi-\ncation , SIGCOMM ’19, page 270–288, New York, NY ,\nUSA, 2019. Association for Computing Machinery.\n[60] A. Mukkara, N. Beckmann, M. Abeydeera, X. Ma, and\nD. Sanchez. Exploiting locality in graph analytics\nthrough hardware-accelerated traversal scheduling. In\n2018 51st Annual IEEE/ACM International Symposium\non Microarchitecture (MICRO) , pages 1–14, 2018.\n[61] C. Olston, N. Fiedel, K. Gorovoy, J. Harmsen, L. Lao,\nF. Li, V . Rajashekhar, S. Ramesh, and J. Soyke.\nTensorﬂow-serving: Flexible, high-performance ml serv-\ning. CoRR ,abs/1712.06139 , 2016.\n[62] S. Rajbhandari, Y . He, O. Ruwase, M. Carbin, and\nT. Chilimbi. Optimizing cnns on multicores for scala-\nbility, performance and goodput. In Proceedings of theTwenty-Second International Conference on Architec-\ntural Support for Programming Languages and Oper-\nating Systems , ASPLOS ’17, page 267–280, New York,\nNY , USA, 2017. Association for Computing Machinery.\n[63] H. Shen, L. Chen, Y . Jin, L. Zhao, B. Kong, M. Phili-\npose, A. Krishnamurthy, and R. Sundaram. Nexus: A\ngpu cluster engine for accelerating dnn-based video anal-\nysis. In Proceedings of the 27th ACM Symposium on\nOperating Systems Principles , SOSP ’19, page 322–337,\nNew York, NY , USA, 2019. Association for Computing\nMachinery.\n[64] H. Shen, S. Han, M. Philipose, and A. Krishnamurthy.\nFast video classiﬁcation via adaptive cascading of deep\nmodels. In 2017 IEEE Conference on Computer Vision\nand Pattern Recognition, CVPR 2017, Honolulu, HI,\nUSA, July 21-26, 2017 , pages 2197–2205. IEEE Com-\nputer Society, 2017.\n[65] Z. Shi, X. Huang, A. Jain, and C. Lin. Applying deep\nlearning to the cache replacement problem. In Pro-\nceedings of the 52nd Annual IEEE/ACM International\nSymposium on Microarchitecture , MICRO ’52, page\n413–425, New York, NY , USA, 2019. Association for\nComputing Machinery.\n[66] K. Simonyan and A. Zisserman. Very deep convolu-\ntional networks for large-scale image recognition. In\nY . Bengio and Y . LeCun, editors, 3rd International Con-\nference on Learning Representations, ICLR 2015, San\nDiego, CA, USA, May 7-9, 2015, Conference Track Pro-\nceedings , 2015.\n[67] A. Singhvi, K. Houck, A. Balasubramanian, M. D.\nShaikh, S. Venkataraman, and A. Akella. Archipelago:\nA scalable low-latency serverless platform. arXiv\npreprint arXiv:1911.09849 , 2019.\n[68] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. E. Reed,\nD. Anguelov, D. Erhan, V . Vanhoucke, and A. Rabi-\nnovich. Going deeper with convolutions. In IEEE Con-\nference on Computer Vision and Pattern Recognition,\nCVPR 2015, Boston, MA, USA, June 7-12, 2015 , pages\n1–9. IEEE Computer Society, 2015.\n[69] G. Vietri, L. V . Rodriguez, W. A. Martinez, S. Lyons,\nJ. Liu, R. Rangaswami, M. Zhao, and G. Narasimhan.\nDriving cache replacement with ml-based lecar. In Pro-\nceedings of the 10th USENIX Conference on Hot Topics\nin Storage and File Systems , HotStorage’18, page 3,\nUSA, 2018. USENIX Association.\n[70] R. Viswanathan, G. Ananthanarayanan, and A. Akella.\nCLARINET: Wan-aware optimization for analytics\nqueries. In 12th USENIX Symposium on Operating\nSystems Design and Implementation (OSDI 16) , pages\n16\n\n435–450, Savannah, GA, Nov. 2016. USENIX Associa-\ntion.\n[71] J. Wang, H. T. Shen, J. Song, and J. Ji. Hashing for\nsimilarity search: A survey. CoRR ,abs/1408.2927 ,\n2014.\n[72] W. Wang, J. Gao, M. Zhang, S. Wang, G. Chen, T. K.\nNg, B. C. Ooi, J. Shao, and M. Reyad. Raﬁki: Machine\nlearning as an analytics service system. Proc. VLDB\nEndow. , 12(2):128–140, Oct. 2018.\n[73] X. Wang, F. Yu, Z. Dou, T. Darrell, and J. E. Gonza-\nlez. Skipnet: Learning dynamic routing in convolutional\nnetworks. In V . Ferrari, M. Hebert, C. Sminchisescu,\nand Y . Weiss, editors, Computer Vision - ECCV 2018 -\n15th European Conference, Munich, Germany, Septem-\nber 8-14, 2018, Proceedings, Part XIII , volume 11217\nofLecture Notes in Computer Science , pages 420–436.\nSpringer, 2018.\n[74] W. Xiong, J. Droppo, X. Huang, F. Seide, M. L. Seltzer,\nA. Stolcke, D. Yu, and G. Zweig. Toward human parity\nin conversational speech recognition. IEEE/ACM Trans-\nactions on Audio, Speech, and Language Processing ,\n25(12):2410–2423, 2017.\n[75] N. J. Yadwadkar, F. Romero, Q. Li, and C. Kozyrakis.\nA case for managed and model-less inference serving.\nInProceedings of the Workshop on Hot Topics in Op-\nerating Systems , HotOS ’19, page 184–191, New York,\nNY , USA, 2019. Association for Computing Machinery.\n[76] F. Y . Yan, H. Ayers, C. Zhu, S. Fouladi, J. Hong,\nK. Zhang, P. Levis, and K. Winstein. Learning in situ:\na randomized experiment in video streaming. In 17th\nUSENIX Symposium on Networked Systems Design and\nImplementation (NSDI 20) , pages 495–511, Santa Clara,\nCA, Feb. 2020. USENIX Association.\n[77] H. Yeo, Y . Jung, J. Kim, J. Shin, and D. Han. Neural\nadaptive content-aware internet video delivery. In 13th\nUSENIX Symposium on Operating Systems Design and\nImplementation (OSDI 18) , pages 645–661, Carlsbad,\nCA, Oct. 2018. USENIX Association.\n[78] H. Zhang, G. Ananthanarayanan, P. Bodik, M. Philipose,\nP. Bahl, and M. J. Freedman. Live video analytics at\nscale with approximation and delay-tolerance. In 14th\nUSENIX Symposium on Networked Systems Design and\nImplementation (NSDI 17) , pages 377–392, Boston, MA,\nMar. 2017. USENIX Association.\n[79] M. Zhang, S. Rajbhandari, W. Wang, and Y . He. Deep-\ncpu: Serving rnn-based deep learning models 10x\nfaster. In 2018 USENIX Annual Technical Conference\n(USENIX ATC 18) , pages 951–965, Boston, MA, July\n2018. USENIX Association.[80] Y . Zhou, J. Philbin, and K. Li. The multi-queue re-\nplacement algorithm for second level buffer caches. In\nProceedings of the General Track: 2001 USENIX An-\nnual Technical Conference , page 91–104, USA, 2001.\nUSENIX Association.\n[81] C. Zhu, S. Han, H. Mao, and W. J. Dally. Trained ternary\nquantization. In 5th International Conference on Learn-\ning Representations, ICLR 2017, Toulon, France, April\n24-26, 2017, Conference Track Proceedings . OpenRe-\nview.net, 2017.\n17\n\nA Appendix\nA.1 Composing learned caches\nThe goal of the composition phase is to jointly select a global\nset of learned caches that minimize the expected average la-\ntency, while ensuring minimal degradation in accuracy, and\nmeeting computation and memory constraints. We can formu-\nlate this as a mix-integer quadratic program.\nLet us assume that we have a DNN with Nlayers for which\nwe wish to construct learned caches. Let us consider that\nfor each layer of the DNN, we have an array of Kpossible\nvariants. Let us consider that we have a memory budget of M.\nFrom the exploration phase, we obtain the following met-\nrics for each variant - Hit Rate ( Hi;j), Accuracy ( Ai;j), Lookup\nLatency ( Ti;j), and Memory Cost ( Mi;j). Additionally, we pro-\nﬁle the latency for the computation of each layer ( Li).\nA.1.1 Optimization Problem\nGiven the above, we wish to decide which layer(s) of the\nDNN to use learned caches for and which variant to use at\nthose layers.\nIndicator Variables.\nWe use the indicator variable bi;jto indicate if a variant j\nat layer iis chosen. bi;j=1means we chose that particular\nvariant at that layer; 0 means we do not choose.\nWe use the indicator variable ci;jto indicate if layer jis\nthe ﬁrst layer after layer ito have a learned cache (This corre-\nsponds to a 1). For instance, consider a neural network with 5\nlayers - L1,L2, ..L5. Let us assume that we ultimately need\ncaches only for layers L1,L3, and L5. In such a case, c1;3and\nc3;5will be 1 and the rest will be 0.\nDerived Metrics.\nThe hit rate Hi;jis an independent metric that is measured\nfor each variant at each layer. However, when the cache at\nan earlier layer gives hits, it reduces the number of input\nsamples available at a later layer. To model this, we measure\naneffective cache-hit rate EH i;jas deﬁned below -\nEHi;j:=Hi;j\u0000i\u00001\nå\nk=1(ck;i:K\nå\nm=1(bk;m:EHk;m)) (5)\nThe above equation subtracts the effective cache hit rate of the\nprevious layer that has a learned cache in a recursive fashion.\nThe base case is EH1;k=H1;kfor all k2K.\nConstraints\nFrom the deﬁnition of the indicator variable ci;j, we formu-\nlate a constraint that at most one layer jafter a layer ican\nhave ci;j=1.\n8iN\nå\nj=i+1ci;j\u00141 (6)\nAccuracy constraint. Given that variant at each layer has an\naccuracy Ai;j, we need to ensure that the cumulative accuracyis greater than a minimum accuracy threshold A -\nN\nå\ni=1K\nå\nj=1(bi;j:EHi;j:Ai;j) + (1\u0000N\nå\ni=1K\nå\nj=1(bi;j:EHi;j))\u0015A(7)\nThe ﬁrst part of the equation measures the effective accuracy\nfor all inputs predicted by learned caches while the second part\ncaptures the accuracy component for inputs that go through\nthe entire DNN (which get 100% accuracy).\nResource constraints. There are two resource constraints to\nconsider - (i) memory and (ii) computation.\nMemory constraint: The total memory occupied by the\nchosen variants should be within the memory budget M.\nN\nå\ni=1K\nå\nj=1(bi;j:Mi;j)\u0014M (8)\nComputational constraint: The underlying hardware im-\nposes computational constraints on the cost of that computa-\ntion in terms of latency (captured by Ti;j), and the degree of\nparallelism with which the computation can be done. We as-\nsume a simple computational model that we can have atmost\none cache lookup at any given point of time. This lookup\nhappens asynchronous to the base DNN computation.\nWith this information, we need to ensure that cache look-\nups do not coincide in time. Hence, we have -\n8iK\nå\nk=1(bi;k:Ti;k)\u0014N\nå\nk=i+1Lk\u0000N\nå\nk=i+1(ci;k:N\nå\nm=k+1Lm)(9)\nIn the above constraint, the LHS captures the running time\nof the cache lookup at layer i. The RHS subtracts the remain-\ning running time for layer ito the end of the DNN from the\nremaining running time for layer jto the end of the DNN,\nwhere jis the earliest layer after ito have a learned cache.\nEssentially, for a layer ithat has a chosen variant, we measure\nthe running time between layer iandjand constraint it to be\nat least as large as the cache lookup time at layer i.\nObjective We wish to minimize the expected latency for a\ngiven input request. We capture this as below -\nmin.N\nå\ni=1K\nå\nj=1(bi;j:EHi;j:(i\nå\nk=1Li+Ti;j))\n+(1\u0000N\nå\ni=1K\nå\nj=1(bi;j:EHi;j))(N\nå\ni=1Li)(10)\nA.1.2 Relaxed Formulation\nThe problem with above solution is that it not linear with\nrespect to the indicator variables. GATIadopts a three-step\napproach to simplify the above formulation. The key idea is\nto separate out the concerns of accuracy, overall latency, and\n18\n\nresource contraints (computation and memory). We use the\nsame indicator variables bi;jandci;jas described above.\nStep 1. Accuracy Filter: As a ﬁrst step, we ﬁlter out model\nvariants Mi;jwhose accuracy Ai;jis below a minimum thresh-\noldA. To formulate this as a constraint -\nbi;j=0;8i,jAi;j<A (11)\nStep 2. Score Computation: We consider two factors in\ndetermining the importance of a particular variant: (i) The hit\nrate of the variant and (ii) The latency gain obtained by using\nthe model variant in the event of a cache hit.\nWe compute latency gain (LG) for a learned cache variant as\nthe ratio of running time for the entire DNN to the running\ntime assuming that a cache hit is obtained at the given learned\ncache variant.\nLGi;j=N\nå\nk=1Lk=(i\nå\nk=1Li+Ti;j) (12)\nWe prefer higher hit rates and higher latency gains. How-\never, these are fundamentally at odds with each other since\nhigher latency gains are obtained using variants at earlier\nlayers of the base DNN where the hit rates would be lower,\nand vice versa. To balance these two factors, we compute a\nscore ( S- higher is better) that captures the beneﬁt of using a\nvariant:\nSi;j=a:(Hi;j) + (1\u0000a):(LGi;j) (13)\nwhere ais a knob that lies in [0,1] and controls the relative\nimportance of hit rate and latency gain.\nStep 3. Resource Constraints: The memory and computa-\ntional constraint remain the same as in the initial formulation\n- Eqn. 8 and Eqn. 9 respectively.\nObjective: Finally, the objective of our formulation is to max-\nimize the sum of scores for chosen variants:\nmax.N\nå\ni=1K\nå\nj=1bi;j:Si;j (14)\nThe computed values of bi;jthen determine which learned\ncache variants should be used along with the base DNN during\ninference.\nA.2 Late Binding Beneﬁts\n(i) CIFAR-100 on ResNet-152: From Figure 13,14, we ob-\nserve that GATIexhibits an average latency of 148.34 ms\n(CPU), which is 1.24Xlower than the latency of running the\nentire DNN. GATIexhibits a spectrum of latencies with an\noverall accuracy of 97.79% (CPU) with respect to the base\nResNet-152 model. We also observe that for ResNet-152,\n15.32% of inputs run through the entire DNN, which is a\nlarge increase comparing to 3.51% (ResNet-18) and 1.53%\n(ResNet-50) on CIFAR-10 dataset. The reason is that CIFAR-\n100 has a lesser proportion of easier examples, which reduces\nthe chances for early hits.\n0255075100125150175200\nLatency (ms)020406080100% Requests3040 10\n8\nResNet-101ResNet-50ResNet-34\nResNet-152\nPruning(x%)\nQuantization(n-bit)\nDistillation\nGatiFigure 13: [CPU Inference]CDF of request latencies comparing G ATIvs\nbaselines for ResNet-152 on CIFAR-100 dataset.\n0 40 80 120 160 200\nLatency (ms)707580859095100Accuracy3040\n4510\n8ResNet-101\nResNet-50\nPruning(x%)\nQuantization(n-bit)\nDistillation\nGati\nFigure 14: [CPU Inference]Accuracy vs latency trade-off for ResNet-152\non CIFAR-100 dataset. Each dot for G ATIis sized in proportion to the\nnumber of inference requests served at that particular latency point.\n02468101214161820\nLatency (ms)020406080100% Requests30,40,50 10,12VGG-11\nVGG-16\nPruning(x%)\nQuantization(n-bit)\nDistillation\nFreeze\nFigure 15: [CPU Inference]CDF of request latencies comparing G ATIvs\nbaselines for VGG-16 on Google Voice dataset.\n02468101214161820\nLatency (ms)949698100Accuracy40\n5012-bit\n10-bit\n8-bitVGG-11Pruning(x%)\nQuantization(n-bit)\nDistillation\nGati\nFigure 16: [CPU Inference]Accuracy vs latency trade-off for VGG-16 on\nGoogle Voice dataset dataset. Size of each G ATImarker is proportional\nto % of requests at the corresponding latency point.\n(ii) Voice Commands on VGG-16: From Fig-\nure 15,16,17,18 we observe that GATIexhibits an average\nlatency of 9.42 ms (CPU) and 2.52 ms (GPU), which is\n1.96Xand1.54 Xlower than the latency of running the entire\nDNN. GATIexhibits a spectrum of latencies with an overall\naccuracy of 97.88% (CPU) and 98.28% (GPU) with respect\nto the base VGG-16 model.\n(iii) CIFAR-10 on ResNet-18: From Figure 19,20,21,22 we\nobserve that GATIexhibits an average latency of 16.04 ms\n19\n\n0 1 2 3 4\nLatency (ms)020406080100% Requests40,30\n5010,12VGG-11\nVGG-16\nPruning(x%)\nQuantization(n-bit)\nDistillation\nGatiFigure 17: [GPU Inference]CDF of request latencies comparing G ATIvs\nbaselines for VGG-16 on Google Voice dataset.\n0 1 2 3 4\nLatency (ms)949698100Accuracy40\n5012\n10VGG-11\nPruning(x%)\nQuantization(n-bit)\nDistillation\nGati\nFigure 18: [GPU Inference]Accuracy vs latency trade-off for VGG-16 on\nGoogle Voice dataset dataset. Size of each G ATImarker is proportional\nto % of requests at the corresponding latency point.\n036912151821242730\nLatency (ms)020406080100% Requests20 40,30 10\n8\nVGG-11AlexNet5-layerCNN\nResNet-18\nPruning(x%)\nQuantization(n-bit)\nDistillation\nGati\nFigure 19: [CPU Inference]CDF of request latencies comparing G ATIvs\nbaselines for ResNet-18 on CIFAR-10 dataset.\n036912151821242730\nLatency (ms)8486889092949698100Accuracy30\n40\n50VGG-11\nAlexNet\n5-layerCNN12\n10\n8\nPruning(x%)\nQuantization(n-bit)\nDistillation\nGati\nFigure 20: [CPU Inference]Accuracy vs latency trade-off for ResNet-18\non CIFAR-10 dataset. Size of each G ATImarker is proportional to %\nof requests at the corresponding latency point.\n(CPU) and 4.15 ms (GPU), which is 1.72Xand1.28 Xlower\nthan the latency of running the entire DNN. GATIexhibits\na spectrum of latencies with an overall accuracy of 96.86%\n(CPU) and 96.54% (GPU) with respect to the base ResNet-18\nmodel.\nA.2.1 Learned Caches Design Analysis\nWe analyze the impact of speciﬁc design choices for learned\ncaches adopted by GATIthat enable better latencies while\nensuring that the accuracy is close to the target accuracy.\nUsing distillation loss function in predictor network: We\n0 1 2 3 4 5 6\nLatency (ms)020406080100% Requests20,30 8 10\nVGG-11 AlexNet\nResNet-18\nPruning(x%)\nQuantization(n-bit)\nDistillation\nGatiFigure 21: [GPU Inference]CDF of request latencies comparing G ATIvs\nbaselines for ResNet-18 on CIFAR-10 dataset.\n0 1 2 3 4 5 6\nLatency (ms)8486889092949698100Accuracy30\n40\n5012-bit\n10-bit\n8-bit VGG-11\nAlexNet\n5-layerCNNPruning(x%)\nQuantization(n-bit)\nDistillation\nGati\nFigure 22: [GPU Inference]Accuracy vs latency trade-off for ResNet-18\non CIFAR-10 dataset. Size of each G ATImarker is proportional to %\nof requests at the corresponding latency point.\n010 20 30 40 50 60 70\nLatency (ms)020406080100% RequestsThreshold=0.9 \n(74.58%)\nThreshold=0.97 \n(89.82%)\nThreshold=0.99 \n(98.5%)\nSelector \n(96.67%)\nFigure 23: [CPU Inference]Beneﬁts of decoupling prediction and se-\nlection for ResNet-50 on CIFAR-10 dataset. Accuracies for different\nschemes are labeled in brackets.\nFC \nonlyPooling \nOnlyConv \nOnlyOne \nHyper-\nparamTwo\n Hyper-\nparam\n(Gati)\nVariants for Exploration010203040506070Latency (ms)\nFigure 24: [CPU Inference]Beneﬁts of exploring multiple predictor net-\nwork architectures for ResNet-50 on CIFAR-10 dataset.\ncompare the beneﬁt of using a loss function inspired by dis-\ntillation against a standard cross-entropy loss function for\nResNet-50 on CIFAR-10. We observe that the distillation\nloss function gives a greater number of cache hits at earlier\nlayers. The distillation loss function results in more accurate\npredictor networks, thereby allowing the selector network to\ninfer more points as conﬁdent cache hits. Overall, we observe\nthat this loss function yields 5.9% improvement in average\nlatency.\nDecoupling prediction and selection in learned caches:\n20\n\n0.00.20.40.60.81.0\nAlpha2528313437Latency (ms)\n97.77597.82597.87597.925\nAccuracy\nFigure 25: [CPU Inference]Latency-accuracy trade-off for different a\nvalues in the composition phase for ResNet-50 on CIFAR-10 dataset.\nDecoupling the decisions helps give cache hits with the de-\nsired accuracy and hit rate properties. We compare this choice\nagainst a baseline that uses only the predictor network and\nestablishes a threshold over the softmax scores from the pre-\ndictor network to infer cache hits. We notice from Figure 23\nthat applying different thresholds on predictor networks in-\nduces trade-offs between accuracy and hit rate that is hard to\ncontrol. The selector network provides a binary classiﬁcation\nmechanism that allows for optimal control of the trade-off\nbetween accuracy and hit rate.\nExploring multiple predictor network architectures: We\nevaluate the beneﬁt of exploration by comparing against base-\nlines that consider either only one type of network architec-\nture or only one hyper-parameter for each architecture. While\nthere is not much impact overall accuracy, we observe from\nFigure 24, that exploring more variants leads to better latency\nbeneﬁts, since it provides more data points for the composi-\ntion phase to pick from. The minimum latency (5.57 ms) by\nconsidering all variants matches the minimum latency by con-\nsidering only variants with pooling architecture. Similarly, the\nmedian latency (27.5 ms) by considering all variants matches\nthe minimum latency by considering only variants with fully-\nconnected architecture. Exploration thus gives combined ben-\neﬁts of exploring individual variants. A trade-off incurred in\nexploring multiple variants is that it requires more resources\nduring the initial cache construction phase. Developers can\nlimit the number of variants to be explored depending upon\nthe resource availability.\nOptimal cache composition: During the composition phase,\nGATIevaluates multiple avalues and picks a value that mini-\nmizes the expected latency while maximizing accuracy. For\nResNet-50 on CIFAR-10, Figure 25 shows that avalues of\n0.2 provide optimal results. We observe similar values for\nGPU based inference. GATIselects the ﬁnal set of learned\ncache variants based on this value.\nWe evaluate the beneﬁt of formulating the composition\nphase as an optimization problem by comparing it to two\ngreedy approaches - (i) A latency greedy approach that greed-\nily picks variants with maximum latency gain while respect-\ning computational and memory constraints. (ii) A hit-rate\ngreedy approach that greedily picks variants with maximum\nhit-rate by respecting computational and memory constraints.\nWe observe that the optimization problem has the most opti-\nmal latency proﬁle, offering up to 5%improvement in aver-age latency. This can be tied down to a better visibility of the\ntrade-offs between the various metrics for learned caches that\nan optimization formulation can capture.\nHardware-aware proﬁling: In the composition phase, GATI\nproﬁles the lookup latency on the same target hardware where\nthe model will be deployed for prediction serving. We ob-\nserve that proﬁling in such a hardware-aware manner yields a\n12.93% improvement in average latency.\n21",
  "textLength": 103045
}