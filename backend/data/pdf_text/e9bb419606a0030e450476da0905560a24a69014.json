{
  "paperId": "e9bb419606a0030e450476da0905560a24a69014",
  "title": "MLOS: An Infrastructure for Automated Software Performance Engineering",
  "pdfPath": "e9bb419606a0030e450476da0905560a24a69014.pdf",
  "text": "MLOS: An Infrastructure for Automated\nSoftware Performance Engineering\nCarlo Curino, Neha Godwal, Brian Kroth, Sergiy Kuryata, Greg Lapinski, Siqi Liu, Slava Oks,\nOlga Poppe, Adam Smiechowski, Ed Thayer, Markus Weimer, Yiwen Zhu\nfirst.last@microsoft.com\nABSTRACT\nDeveloping modern systems software is a complex task that\ncombines business logic programming and Software Perfor-\nmance Engineering (SPE). The later is an experimental and\nlabor-intensive activity focused on optimizing the system for a\ngiven hardware, software, and workload (hw/sw/wl) context.\nToday’s SPE is performed during build/release phases by\nspecialized teams, and cursed by: 1) lack of standardized and\nautomated tools, 2) significant repeated work as hw/sw/wl\ncontext changes, 3) fragility induced by a “one-size-fit-all”\ntuning (where improvements on one workload or component\nmay impact others). The net result: despite costly investments,\nsystem software is often outside its optimal operating point—\nanecdotally leaving 30 to 40% of performance on the table.\nThe recent developments in Data Science (DS) hints at\nan opportunity: combining DS tooling and methodologies\nwith a new developer experience to transform the practice of\nSPE. In this paper we present: MLOS, an ML-powered infras-\ntructure and methodology to democratize and automate Soft-\nware Performance Engineering. MLOS enables continuous,\ninstance-level, robust, and trackable systems optimization.\nMLOS is being developed and employed within Microsoft\nto optimize SQL Server performance. Early results indicated\nthat component-level optimizations can lead to 20%-90%\nimprovements when custom-tuning for a specific hw/sw/wl,\nhinting at a significant opportunity. However, several research\nchallenges remain that will require community involvement.\nTo this end, we are in the process of open-sourcing the MLOS\ncore infrastructure, and we are engaging with academic insti-\ntutions to create an educational program around Software 2.0\nand MLOS ideas.\nCCS CONCEPTS\n•Computing methodologies →Modeling and simulation ;\nMachine learning ;•Information systems →Data manage-\nment systems ;Information systems applications ;•Applied\ncomputing →Enterprise computing ;•Software and its en-\ngineering ;\nKEYWORDS\nsystems, software engineering, performance, optimization,\nmachine learning, data science1 INTRODUCTION\nSoftware Performance Engineering (SPE) is hard because per-\nformance depends on the software and hardware environment\nwhere the system is run andwhat workload it is processing.\nThis situation is even more difficult, since these influences are\nnot static in time. In response to these challenges, real-world\nSPE has developed as an amorphous collection of experi-\nments and tribal best practices aimed at tuning parameters\nand policies to satisfy non-functional performance require-\nments (e.g., latency, throughput, utilization, etc.). Consider\nas an example the design and tuning of a hash table. Imple-\nmenting a working in-memory hash table is relatively simple.\nBut tuning it requires a deep understanding of the workload,\ncomplex hardware capabilities and interactions, and local as\nwell as global performance/resource trade-offs of the entire\nsystem—as shown in our experiments in §3.\nOur key observation is that: while business logic develop-\nment is a necessary and high-value activity, manual tuning of\nparameters/heuristics is an often low-yield (yet important) ac-\ntivity for engineers. The key reasons for the current situation\nare: 1) lack of integrated tools to explore a large parameter\nspace by running, and tracking experiments, 2) a need to\ncontinuously re-tune systems as hardware/software/workload\n(hw/sw/wl) context evolve, and 3) the manual nature of the\nprocess, that forces one-size-fits-all configurations.\nAs a consequence, instead of being a pervasive and contin-\nuous activity (like unit testing), SPE is relegated to elite teams\nwith local expertise and ad-hoc knowledge and tools. The cost\nof SPE can only be justified where performance is critical—\nMicrosoft and Intel employ 100s of performance engineers\nthat jointly tune core systems for every major hw/sw release.\nThe alternative is accepting sub-optimal performance. Intel’s\nperformance engineers shared that, most system software is\ntypically 30-40% away from what the hardware can achieve1.\nThis situation is exaggerated by increasingly diverse hardware\navailable in the cloud and on the edge.\nThanks to advancements in data management and machine\nlearning, a fundamental shift in how systems are built and\ntuned is possible and required. In short, we argue in favor\n1That is the typical boost that their specialized SPE team achieves when\nengaged to tune previously unoptimized software for a new hardware release.arXiv:2006.02155v2  [cs.DC]  4 Jun 2020\n\nC. Curino, et al.\nTraditional SPEpublic class BufferPool {    int pageSize = 2048;    int numPages = 50;...}BUILDDEPLOYBENCHMARK4096BUILDDEPLOYBENCHMARKCODE EDIT(STATIC)ONLINE(PRODUCTION)mlos.registerParams(…)public class BufferPool {    int pageSize = mlos.get(“pagesize”);    int numPages = mlos.get(“numPages”);...}public void testBufferPool(){  mlos.time(doWork());}\nMLOS\nONLINE(PRODUCTION)\nWORKLOADML OPTIMIZATION(ON/OFFLINE)workload(w1)optimize(throughput)subject_to(mem<2GB,           cpu<10%,          latency<1ms)\nFigure 1: The MLOS Developer Experience\nofmelding Data Science into the practice of Software En-\ngineering to make SPE mainstream . Beyond the maturation\nof Data Science (DS), MLOS2is enabled by the move to\nthe cloud. Cloud operations provide us with: workload ob-\nservability, increased incentives to optimize software, and an\nunprecedented experimentation platform.\nWe present MLOS, an infrastructure and engineering prac-\ntice aimed at systematizing and automating the performance\ntuning of a system via Data Science. With MLOS, we em-\npower engineers with tooling for data-driven optimizations.\nThis improves the end-to-end process of development and\nlong term maintenance costs, increases documentation and\nreproducibility of performance tuning, and leads to more per-\nformant, efficient, well tuned systems—discussed in §2.\nWe observe that the SPE process closely mimics the DS\nprocess, and MLOS brings the rigor and tools of DS to it.\nLike SPE, DS relies on experimentation, and changing pa-\nrameters as often as necessary. However, in existing systems\nsoftware, many parameters are hard-coded and long build\ntimes prohibit rapid experimentation. We believe this is a\nmajor impediment to data-driven, automated SPE. So, one\nkey architectural decision we make in MLOS is to separate\nthe core system and the data-driven tuning of it. In §2.1 we\ndescribe the infrastructure in more detail, highlighting this\nfundamental architectural choice.\nIn building MLOS, we are faced with hard challenges:\n(1) Creating a “natural” development experience, enabling\nengineers to take advantage of advanced and rigorous DS\nprocesses. Annotations+Automation allow for a rich DS ex-\nperience with minimal effort. (2) Introducing observabil-\nity/tunability without disrupting the performance of the tar-\nget system. We achieve this using a side-agent and channel\nto create low-cost communications with a DS experience.\n(3) Global optimization across thousands of parameters is\nan open problem. We cope with this by decomposing the\nproblem (as business logic does using APIs) into microbench-\nmarks for sub-components and by introducing a Resource\nPerformance Interface (RPI)—the SPE equivalent of an API.\n2Short for Machine Learning [Optimized|Operating] Systems.We highlight the results of applying our initial MLOS\nimplementation to tackle (1) and (2) by optimizing core data\nstructures in SQL Server in §3.\nAt Microsoft, we are developing and using MLOS within\nSQL Server engineering and are in the process of open-\nsourcing all of the general purpose system components [ 2].\nFurthermore, we are collaborating with several academic in-\nstitutions with the intention to create an initial class and lab.\n2 THE MLOS DEVELOPER EXPERIENCE\nMLOS aims to democratize and automate SPE. Specifically,\nwe leverage existing Data Science technologies, including:\nnotebooks [ 1], model training [ 13,20], tracking [ 24] and\ndeployment [ 5]. We concentrate our net-new development\non what we believe to be a key gap that prevents system\ndevelopers from using DS for SPE efficiently: the lack of\nobservability and tunability of low level system internals.\nFigure 1 highlights the difference between traditional SPE\nand MLOS. For traditional SPE, the developer’s “tuning loop”\ninvolves: hard-coding \"magic\" constants (or at best exposing\nthem as start-up parameters), recompiling/redeploying (or\nrestarting) the system, running an experiment, and analyzing\nthe results. This process is lengthy, manual, and error-prone.\nBy contrast, MLOS enables externally observable and dy-\nnamically tunable constants, and allows the developer to\n“drive” and track experiments in a fashion akin to a DS ex-\nperience. The loop is faster and more carefully tracked (e.g.,\nwe record all experiment conditions, including OS/HW coun-\nters). Moreover, MLOS enables automated parameter tun-\ning, allowing for continuous and dynamic re-tuning, as well\nas instance-level optimization (i.e., optimize for a specific\nhw/sw/wl context).\nOur primary concern in designing MLOS is to create a nat-\nural development experience . The use of auto-parameters (the\nterm we use to refer to DS-tuned system parameters) must be\nnatural and easier than hard-coding a constant. The first step\nin this journey is providing APIs and annotation mechanisms\nthat are native to the system language. In SQL Server (C ++),\nwe leverage C#language Attributes to declare certain param-\neters as tunable, and associate metrics to critical section of\nthe component code for observability. As we extend MLOS\n\nMLOS: An Infrastructure for Automated Software Performance Engineering\nto more languages, we will select mechanisms idiomatic to\neach language (e.g., Java annotations).\nThrough code-gen mechanisms (§ 2.1), each of these pa-\nrameters and their associated metrics is made observable\nand tunable external to the core system without impacting\nthe delicate performance inner-loop of the system—our sec-\nond core challenge. This externalization allows us to pro-\nvide an agile notebook-driven experience (similar to a stan-\ndard DS workbench). This enables a software engineer with\nrudimentary DS skills to visualize, analyze, model, and op-\ntimize component parameters. We require the developer to\nprovide (micro)benchmarks, but minimal input regarding the\nchoice of optimization function (e.g., maximize the through-\nput of a component), subject to certain constraints (e.g., mem-\nory/cpu/latency caps) and the choice of workload(s).\nIn many cases simple AutoML or well chosen default op-\ntimization pipelines will likely provide a significant initial\nlift over the manual configuration. The reason is simple: we\ncompare a static/global configuration of parameters to a one\ndynamically tuned for a given hw/sw/wl. Another value-add\nfrom MLOS is that the developer need only provide a few\napplication level metrics specific to their component (e.g.,\ntiming of a critical section) and MLOS automatically gathers\na large amount of contextual information (e.g., OS/HW coun-\nters). These are leveraged to gain additional insight and can\nhelp in modeling a system. Finally, in MLOS we follow DS\nbest practices and integrate with tools such as MLFlow [ 24]\nfor versioning and tracking of all models/experiments. On\nits own this provides rigor and reproducibility and makes the\nSPE process continuous rather than a one-off optimization\nthat grows stale over time. To this end we expect that in most\ncases the “Data Science” aspects of MLOS will deliver gains,\nmore than its ML sophistication.\nResource Performance Interface (RPI) A developer can\noptimize either the end-to-end system (e.g., TPC-E on SQL\nServer) or a specific component (e.g., microbenchmark on\nspinlocks, as we do in §3). Locally optimizing individual\ncomponents out of context may not align with end-to-end\noptimization since those components typically share and com-\npete for resources in the larger system. For this reason we\nintroduce the notion of a Resource Performance Interface\n(RPI) . Conceptually an RPI allows the developer to define\nthecontext of the component in the larger system in the form\nof resource and performance expectations. Practically, this\ndescribes an acceptable “envelope” of resources for a given\ncomponent stressed by a chosen workload. Note that the val-\nues here may be initially specified by the developer, or learned\nfrom an existing system (e.g., during build-test runs). RPIs\nprovide the grounding for systematic performance regression\ntesting, at the component level. This is key to decompose the\nperformance behavior of a large system, and it is the moral\nHistoricalTelemetryMLOS AgentSystem to Optimize(e.g., SQL Server)OS / VMShared MemoryMLOS hooksLive Model InferenceData Ingestion/CorrelationData Science ExperienceNotebook\nWorkloadExp.Archive\nOS, HW CountersTraces / StatsTuning CommandsModelsExperimentsTrainingTrackingVersioningValidation21453RPIsFigure 2: The MLOS Architecture\nequivalent of an API for the non-functional performance as-\npects of a system. Importantly, the RPI is notspecified as\npart of the system code, but as part of the DS experience.\nThe rationale for this is that the same piece of business logic,\nmight be used in different parts of a system or in different\nenvironments and could be optimized differently. More pre-\ncisely this means that the same API could be associated with\nmultiple RPIs depending on the context of usage.\nNext, we describe how we deliver this developer experience.\n2.1 The MLOS Architecture\nA fundamental choice in MLOS is to enable DS-driven op-\ntimization without fundamentally modifying the underlying\nsystem. The practical motivation comes from our initial target\ncase: SQL Server. This massive codebase is hard to modify,\nand the size of it makes the build process very costly, slowing\nthe experimental SPE loop. Next we describe the mechanics\nof how we decouple the system from the modeling and tun-\ning of it. The key challenge here is to provide observability\nand tunability with minimal impact to the underlying system\nperformance—a performance variant of the Socratic oath.\nMLOS is depicted in Figure 2 and operates as follows:\n(1) Given an annotated code base (per §2), MLOS performs\ncode gen of: a) hooks in the system, b) a low latency shared\nmemory communication channel, and c) the MLOS Agent (a\ndaemon operating side-by-side the main system). This is a\nhigh-performance mechanism to externalize all tuning.\n(2) A Data Science experience is enabled on top of the appli-\ncation metrics and OS/HW counter data collected. Developers\ncan leverage popular data science tools for visualization, mod-\neling, and tracking of experiments.\n(3) The Data Science experience also provides generic APIs\nto drive system-specific workloads, allowing us to focus on\ncollecting data points of interest interactively or automatically\nduring CI/CD and complement observations of live systems.\n(4) Models, Rules, and Optimizations are deployed via the\nDS experience into the MLOS Agent, where they are hosted\nfor online inferencing based on live and contextual conditions\n\nC. Curino, et al.\nOpenRowSet BuﬀerManager\n Search SpaceOptimization Strategies\nFigure 3: Tuning of two hash-tables in SQL Server\n(e.g., observed load on the server, internal system metrics).\n(5) Finally commands are sent via the shared memory channel\nand enacted by the MLOS hooks (e.g., updated the value for\nthe maximum spin count of a spinlock).\nThis approach creates the independence required for fast\nexperimentation, and reduces to a minimum the impact on the\nsystem inner-loop. Clearly not all parameters are easily tuned\ndynamically, as some would incur costly re-initialization op-\nerations. As we gain experience with MLOS in practice we\nwill develop simple development patterns and libraries of\ncomponents that are amenable to dynamic tuning. This is why\nwe focus our first application of MLOS to core datastructures\nand functions in SQL Server: SQLOS [18].\n3 EVALUATION\nOur first experiments focus on tuning the parameters of\nhashtables in SQL Server. Figure 3 shows the effects of tun-\ning two instances (OpenRowSet and BufferManager). We\nobserve: 1) DS-driven optimization can improve against SQL\nServer highly tuned parameters by 20% to 90% (the initial\npoint in the strategy graphs), 2) the optimization surface for\nthe same component may vary with the workload (smooth\nfor OpenRowSet and very jagged for BufferManager) and\n3) Random Search (RS) performs very competitively with\nBayesian Optimization (BO), shown using Gausian Processes\n(GP), GP with Mattern 3/2 kernels, and 4) optimizing multi-\nple parameters at a time can be faster than one at a time (lines\nmarked as (1) in graph). In other experiments (ommited due\nto space) BO was more efficient, indicating that the choice\nof optimization mechanism is non-trivial and our agile DS\nexperience is indeed important.\nNext we showcase how the ability of MLOS to gather\nHW/OS counters during experiments allows us to adapt to\ndifferent operating conditions. Leveraging a similar Hashtable\nFigure 4: HW/OS Counters to tune in a resource-aware fashion\n0 5000 10000 15000 20000 25000\nmax spin count before backoff0100000200000300000400000spinlocks acquired per secondSOS default spin wait\n128 long ops\n256512\n10242048\n40968192\nFigure 5: Optimal spinlock polling length varies by workload.\nexperiment Figure 4 shows that a larger hashtable (more buck-\nets, hence more memory), reduces collisions (app metric)\nleading to better latency. Interestingly, up to 5MB of allo-\ncation this also offers a good reduction of cpu-cycles and\ncache-misses (HW metrics), but this effect disappears beyond\nthat. This indicates that at least 5MB allocation is advisable to\nsave CPU resources in most conditions, but that past that the\ntradeoff of collisions/memory dominates. Importantly, auto-\nmatic collection of HW/OS counters make it easy to observe\nthese tradeoffs, and the MLOS optimizer allows the developer\nto focus on the desired objective (e.g., a tradeoff of resources\nand performance) abstracting away the parameter fiddling. Fi-\nnally, we note that MLOS can perform this tuning continously\nadapting to changing hw/sw/wl conditions.\nOur last experiment showcases how the workload may\ninfluence optimal parameters choice. Figure 5 shows the per-\nformance of a spinlock when changing the maximum spin-\nning before backoff. We show 7 workloads each comprised\nof several threads performing little work, and one perform-\ning a larger and larger number of operations under the lock.\nSubtle changes in the workload (or hardware platform) can\nsubstantially affect the optimal choice of parameters.\nDiscussion These experiments show great promise for\ncomponent-level optimizations. A key open challenge: how\nto tackle the impractical parameter space for global optimiza-\ntion. Our current practical solution is to leverage the power\nof RPIs combined with developer intuition. More research in\nthis space is needed.\n4 RELATED WORK\nMLOS and Software 2.0 [ 3] are related, but not the same.\nMLOS does not make a distinction between developers who\nwrite code (\"Developer 1.0\") and those that curate data and\n\nMLOS: An Infrastructure for Automated Software Performance Engineering\ntrain models (\"Developer 2.0\"). Instead, MLOS augments sys-\ntems developers with enough data science tools and method-\nology to meet the challenges of the day. Similarly, MLOS\nisn’t geared at problems that can only be solved by replac-\ning functions with models (e.g. image recognition). Instead,\nit fuses traditional solutions (e.g. OS/HW measurement of\nbenchmarks) to problems solvable with them (e.g. hashtable\noptimization) with data science to organize and optimize\nthem for the context in which they are applied. Recent work\n[8,12,15,17] on replacing sub-components of a DBMS with\nmodels is incredibly promising, but as of the time of this\nwriting still brittle for our production applications.\nAnother key area of related work, is tools that tune ex-\nternal parameters of a system [ 9,19,22,25]. Closer points\nof comparison are approaches such as [ 14,16,21,23] that\nleverage ML to guide/improve existing query optimizers. All\nof these efforts are complementary to MLOS, which focuses\non making system internals observable and tunable. We are\nin fact, considering to port some of them onto the MLOS\ninfrastructure.\nPast efforts have also tried to address this problem at a\nlibrary [ 6,10] or compiler level [ 4,7,11]. In some ways these\nare early applications of MLOS style optimization for those\nareas. While complementary, though don’t focus on exposing\nthe DS for the developer to improve with as well.\n5 CONCLUSION\nSoftware Performance Engineering (SPE) is a very valuable\nbut costly activity. With MLOS we fill an infrastructure gap\nthat will enable developers to leverage the tools and practices\nof Data Science (DS).\nREFERENCES\n[1] ADS. https://github.com/Microsoft/azuredatastudio.\n[2] MLOS. https://github.com/microsoft/MLOS.\n[3]Andrej Karpathy. Software 2.0. https://medium.com/@karpathy/\nsoftware-2-0-a64152b37c35, 2017.\n[4]A. H. Ashouri, W. Killian, J. Cavazos, G. Palermo, and C. Silvano. A\nsurvey on compiler autotuning using machine learning. ACM Comput-\ning Surveys (CSUR) , 51(5):1–42, 2018.\n[5]J. Bai, F. Lu, K. Zhang, et al. Onnx: Open neural network exchange.\nhttps://github.com/onnx/onnx, 2019.\n[6]V . Carbune, T. Coppey, A. Daryin, T. Deselaers, N. Sarda, and J. Yagnik.\nSmartchoices: Hybridizing programming and machine learning. arXiv\npreprint arXiv:1810.00619 , 2018.\n[7]D. Chen, T. Moseley, and D. X. Li. Autofdo: Automatic feedback-\ndirected optimization for warehouse-scale applications. In 2016\nIEEE/ACM International Symposium on Code Generation and Op-\ntimization (CGO) , pages 12–23. IEEE, 2016.\n[8]J. Ding, U. F. Minhas, H. Zhang, Y . Li, C. Wang, B. Chandramouli,\nJ. Gehrke, D. Kossmann, and D. Lomet. Alex: an updatable adaptivelearned index. arXiv preprint arXiv:1905.08898 , 2019.\n[9]S. Duan, V . Thummala, and S. Babu. Tuning database configura-\ntion parameters with ituned. Proceedings of the VLDB Endowment ,\n2(1):1246–1257, 2009.\n[10] J. Eastep, D. Wingate, and A. Agarwal. Smart data structures: an online\nmachine learning approach to multicore data structures. In Proceedings\nof the 8th ACM international conference on Autonomic computing ,\npages 11–20, 2011.\n[11] G. Fursin, Y . Kashnikov, A. W. Memon, Z. Chamski, O. Temam,\nM. Namolaru, E. Yom-Tov, B. Mendelson, A. Zaks, E. Courtois, et al.\nMilepost gcc: Machine learning enabled self-tuning compiler. Interna-\ntional journal of parallel programming , 39(3):296–327, 2011.\n[12] S. Idreos, K. Zoumpatianos, B. Hentschel, M. S. Kester, and D. Guo.\nThe data calculator: Data structure design and cost synthesis from\nfirst principles and learned cost models. In Proceedings of the 2018\nInternational Conference on Management of Data , pages 535–550.\n[13] M. Interlandi, S. Matusevych, S. Amizadeh, S. Zahirazami, and\nM. Weimer. Machine learning at microsoft with ml.net. 2018.\n[14] A. Jindal, H. Patel, A. Roy, S. Qiao, Z. Yin, R. Sen, and S. Krishnan.\nPeregrine: Workload optimization for cloud query engines. In Pro-\nceedings of the ACM Symposium on Cloud Computing , pages 416–427,\n2019.\n[15] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case\nfor learned index structures. In Proceedings of the 2018 International\nConference on Management of Data , pages 489–504, 2018.\n[16] R. Marcus, P. Negi, H. Mao, N. Tatbul, M. Alizadeh, and T. Kraska.\nBao: Learning to steer query optimizers. arXiv:2004.03814 , 2020.\n[17] R. Marcus, P. Negi, H. Mao, C. Zhang, M. Alizadeh, T. Kraska, O. Pa-\npaemmanouil, and N. Tatbul. Neo: A learned query optimizer. Proceed-\nings of the VLDB Endowment , 12(11):1705–1718, 2019.\n[18] S. Oks. Platform layer for sql server. https://docs.microsoft.com/en-\nus/archive/blogs/slavao/platform-layer-for-sql-server.\n[19] A. Pavlo, M. Butrovich, A. Joshi, L. Ma, P. Menon, D. Van Aken, L. Lee,\nand R. Salakhutdinov. External vs. internal: An essay on machine\nlearning agents for autonomous database management systems. IEEE\nData Engineering , 11:1910–1913, 2019.\n[20] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion,\nO. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg, et al.\nScikit-learn: Machine learning in python. Journal of machine learning\nresearch , 12(Oct):2825–2830, 2011.\n[21] T. Siddiqui, A. Jindal, S. Qiao, H. Patel, et al. Cost models for big\ndata query processing: Learning, retrofitting, and our findings. arXiv\npreprint arXiv:2002.12393 , 2020.\n[22] D. Van Aken, A. Pavlo, G. J. Gordon, and B. Zhang. Automatic data-\nbase management system tuning through large-scale machine learning.\nInProceedings of the 2017 ACM International Conference on Manage-\nment of Data , pages 1009–1024, 2017.\n[23] C. Wu, A. Jindal, S. Amizadeh, H. Patel, W. Le, S. Qiao, and S. Rao.\nTowards a learning optimizer for shared clouds. Proceedings of the\nVLDB Endowment , 12(3):210–222, 2018.\n[24] M. Zaharia, A. Chen, A. Davidson, A. Ghodsi, S. A. Hong, A. Konwin-\nski, S. Murching, T. Nykodym, P. Ogilvie, M. Parkhe, et al. Accelerat-\ning the machine learning lifecycle with mlflow. IEEE Data Eng. Bull. ,\n41(4):39–45, 2018.\n[25] Y . Zhu, J. Liu, M. Guo, Y . Bao, W. Ma, Z. Liu, K. Song, and Y . Yang.\nBestconfig: tapping the performance potential of systems via automatic\nconfiguration tuning. In Proceedings of the 2017 Symposium on Cloud\nComputing , pages 338–350, 2017.",
  "textLength": 25713
}