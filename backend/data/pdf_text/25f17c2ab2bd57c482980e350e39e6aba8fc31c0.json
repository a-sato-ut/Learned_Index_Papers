{
  "paperId": "25f17c2ab2bd57c482980e350e39e6aba8fc31c0",
  "title": "SparDL: Distributed Deep Learning Training with Efficient Sparse Communication",
  "pdfPath": "25f17c2ab2bd57c482980e350e39e6aba8fc31c0.pdf",
  "text": "arXiv:2304.00737v2  [cs.LG]  23 Feb 2024SparDL : Distributed Deep Learning Training with\nEfﬁcient Sparse Communication\nMinjun Zhao†, Yichen Yin†, Yuren Mao†, Qing Liu†, Lu Chen†, Yunjun Gao†\n†College of Computer Science, Zhejiang University, Hangzho u, China\n†{minjunzhao, yichenyin, yuren.mao, qingliucs, luchen, gao yj}@zju.edu.cn\nAbstract —Top-ksparsiﬁcation has recently been widely used\nto reduce the communication volume in distributed deep lear ning.\nHowever, due to the Sparse Gradient Accumulation (SGA)\ndilemma, the performance of top- ksparsiﬁcation still has limita-\ntions. Recently, a few methods have been put forward to handl e\nthe SGA dilemma. Regrettably, even the state-of-the-art me thod\nsuffers from several drawbacks, e.g., it relies on an inefﬁc ient\ncommunication algorithm and requires extra transmission s teps.\nMotivated by the limitations of existing methods, we propos e a\nnovel efﬁcient sparse communication framework, called SparDL .\nSpeciﬁcally, SparDL uses the Spar-Reduce-Scatter algorithm,\nwhich is based on an efﬁcient Reduce-Scatter model, to handl e\nthe SGA dilemma without additional communication operatio ns.\nBesides, to further reduce the latency cost and improve the\nefﬁciency of SparDL , we propose the Spar-All-Gather algorithm.\nMoreover, we propose the global residual collection algori thm\nto ensure fast convergence of model training. Finally, exte nsive\nexperiments are conducted to validate the superiority of SparDL .\nIndex Terms —Distributed machine learning, Sparse gradients,\nCommunication efﬁciency\nI. I NTRODUCTION\nA. Background\nNowadays, due to the large data volume and complex\nmodels, training deep learning models is becoming more and\nmore time-consuming. To this end, many distributed model\ntraining techniques have been proposed for deep learning [1 0],\n[11], [18], [27], [28], [48], [50]. Among them, data-parall el\nsynchronous mini-batch stochastic gradient descent (S-SG D)\nis widely used [27], [29], [40]. Speciﬁcally, when the S-\nSGD is employed for distributed model training, at the end\nof each iteration, workers synchronize their local gradien ts,\ncommonly by All-Reduce operation [22], and update their\ntraining models with the same global gradients [19]. Howeve r,\nS-SGD involves signiﬁcant data communications [34], [41].\nTop-ksparsiﬁcation [3], [22], [24] is one of the ways to reduce\nthe communication overhead, which can sparsify the local\ngradients to about 1%density while not impairing model con-\nvergence. Regrettably, top- ksparsiﬁcation sometimes suffers\nfrom the sparse gradient accumulation (SGA) dilemma.\nSpeciﬁcally, when efﬁcient All-Reduce methods (e.g.,\nRabenseifner’s All-Reduce [45]) are used for synchronizin g\nsparsiﬁed gradients, the selected gradients undergo multi ple\ntransmission and summation steps to obtain global gradient s.\nEach summation increases the volume of sparse gradients for\nthe following transmission process since the sparse gradie nts\nof different workers may come from differing indexes, re-\nsulting in the SGA dilemma. The SGA dilemma will lead(a) The sparse gradi-\nents from worker 1(b) The sparse gradi-\nents from worker 2(c) The sparse gradi-\nents of worker 2 after\nsummation\nFig. 1. Illustration of the sparse gradient accumulation (S GA) dilemma\nto a quick increase in gradients, which may degrade to\ndense gradients. For example, Figures 1(a) and 1(b) show\nthe initial 3 gradients of workers 1 and 2, respectively. Aft er\nworker 2 receives worker 1’s gradients and adds it to its own\ngradients, worker 2 gets 5 gradients, as shown in Figure 1(c) .\nAs transmission steps continue, the number of gradients for\nworkers increases. The SGA dilemma signiﬁcantly degrades\nthe communication efﬁciency of distributed model training\nwith sparsiﬁcation. It is necessary to design an efﬁcient me thod\nto alleviate the SGA dilemma.\nB. State-of-the-art Methods\nIn the literature, some sparse All-Reduce methods have been\nproposed to handle the SGA dilemma, which is summarized\nin Table I [22], [34], [41], [45]. Speciﬁcally, Top kA [34]\nalleviates the SGA dilemma by using the recursive doubling\nalgorithm, which leads to the bandwidth cost proportional t o\nthe number of workers [22], [41]. Top kDSA [34] consists\nof Reduce-Scatter and All-Gather operations. In the Reduce -\nScatter operation, Top kDSA addresses the SGA dilemma by\ndirectly sending gradients, which also causes high latency\ncost. In the All-Gather operation, Top kDSA allows the SGA\ndilemma to happen and switches to dense transmission when\nthe sparse gradient is large enough, leading to high bandwid th\ncost. gTop k[41], [42] utilizes reduction tree and broadcast\ntree models, both of which have high bandwidth cost, to solve\nthe SGA dilemma.\nOk-Topk[22] is the state-of-the-art method among four\nmethods, which improves Top kDSA with two compression\nsteps. However, O k-Topkalso suffers from some limitations.\nFirst, Ok-Topkdirectly sends gradients to the target workers\nin the Reduce-Scatter operation, which has high latency cos t.\nSecond, in the All-Gather operation, O k-Topkemploys several\nextra communication operations to balance the uneven distr i-\nbution of sparse gradient among workers, causing high laten cy\ncost and large upper bound of bandwidth cost for the All-\nGather operation. Third, as shown in Table I, the upper bound\nof Ok-Topk’s bandwidth cost is 6P−1\nPkβ. However, in reality,\nthe bandwidth cost of O k-Topkmay be higher than 6P−1\nPkβ.\n\nTABLE I\nCOMMUNICATION COMPLEXITY OF SPARSE ALL-REDUCE METHODS (PIS\nTHE NUMBER OF WORKERS ,nIS THE NUMBER OF DENSE GRADIENTS ,kIS\nTHE NUMBER OF SPARSE GRADIENTS (k≪n)ANDdIS THE NUMBER OF\nTEAMS IN SPARDL)\nAlgorithms Latency Cost Bandwidth Cost\nTopkA [34] logPα 2(P−1)kβ\nTopkDSA [34] (P+2logP)α[4P−1\nPkβ,P−1\nP(2k+n)β]\ngTopk[41] 2logPα 4logPkβ\nOk-Topk[22]2(P+logP)α[2P−1\nPkβ,6P−1\nPkβ]\nSparDL 2logPα 4P−1\nPkβ\nSparDL\n(R-SAG)(2logP\nd+logd)α2(2P−2d\nP+d\nPlogd)kβ\nSparDL\n(B-SAG)(2logP\nd+log2d)α[2d2+P−2d\nPdkβ,\n2d2+2P−3d\nPkβ]\nThe reason is two-folded. (i) The local sparse gradients fro m\neach worker are balanced every 64 iterations, which causes\nuneven distribution before the next balancing and leads to\ndifferent communication volumes for different workers in\nthe Reduce-Scatter operation. (ii) O k-Topkutilizes threshold\npruning, which may select more than ksparse gradients.\nTherefore, the performance of sparse All-Reduce still has\nroom for improvement.\nC. Our Solutions\nMotivated by the limitations of existing methods, in this\npaper, we propose a novel sparse communication framework,\ncalled SparDL , for distributed deep learning training. SparDL\nincorperates three novel algorithms, i.e., Spar-Reduce-Scatter ,\nglobal residual collection , and Spar-All-Gather . To be speciﬁc,\nﬁrst, SparDL evenly divides all workers into dteams. Sec-\nond, SparDL utilizes Spar-Reduce-Scatter to Reduce-Scatter\nsparse gradients for each team. Then, SparDL uses Spar-\nAll-Gather to synchronize gradients among dteams. Finally,\nSparDL adopts Bruck All-Gather [4] to synchronize gradients\nfor each team. In both Spar-Reduce-Scatter and Spar-All-\nGather ,SparDL employs global residual collection to collect\nthe discarded gradients. The details of Spar-Reduce-Scatter ,\nglobal residual collection , and Spar-All-Gather are as follows.\n•Spar-Reduce-Scatter is proposed to efﬁciently Reduce-\nScatter sparse gradients for each team. To this end, we\ndevise a non-recursive Reduce-Scatter structure for Spar-\nReduce-Scatter to ensure the efﬁciency of SparDL even for\nthe arbitrary number of workers. Thus, Spar-Reduce-Scatter\ncan partition the gradients into blocks and apply block top- k\nselection on every gradient block before each transmission\nstep to maintain the volume of sparse gradients. With the\nhelp of Spar-Reduce-Scatter ,SparDL does not require extra\ntransmission steps to balance the gradients among workers\nand avoids resorting to unstable threshold pruning to achie ve\nglobal compression.\n•Global residual collection is devised to ensure fast conver-\ngence of the training process. For maintaining the volume of\nsparse gradients, SparDL utilizes multiple top- kselections,\nwhich may discard important gradients and result in slower\nconvergence rate. To keep the fast convergence rate of the\ntraining process, we propose the global residual collection\nalgorithm to collect and reuse all the discarded gradients.•Spar-All-Gather is based on the divide-and-conquer strategy\nand is proposed to reduce latency cost and improve the\nefﬁciency of SparDL further. Speciﬁcally, Spar-All-Gather\ndivides workers into dteams. By adjusting the number of\nteams, we can adjust the ratio of latency and bandwidth in\nthe total communication complexity. For different situati ons,\nwe use two versions of Spar-All-Gather , i.e., R-SAG and\nB-SAG. R-SAG is used when the number of teams is 2i\n(i∈N∗). Otherwise, we employ B-SAG.\nContributions. This paper makes the following contributions.\n•We present a novel sparse communication framework, called\nSparDL , to handle the SGA dilemma for efﬁcient dis-\ntributed deep learning.\n•We propose three novel algorithms, i.e, Spar-Reduce-\nScatter ,global residual collection , and Spar-All-Gather , and\nintegrate these three algorithms seamlessly into SparDL .\n•We conduct extensive experiments on seven different dis-\ntributed deep learning cases to verify the effectiveness\nand efﬁciency of SparDL . Experimental results show that\nSparDL is up to 4.9 ×faster than the state-of-the-art meth-\nods, while maintaining comparable effectiveness.\nOutline. The rest of this paper is organized as follows.\nSection II introduces preliminaries. Section III presents the\nframework of SparDL , and the details of Spar-Reduce-Scatter ,\nglobal residual collection , and Spar-All-Gather . Section IV re-\nports experimental results. Section V reviews the related w ork.\nSection VI discuss the limitations and practical implicati ons\nofSparDL . Finally, we conclude the paper in Section VII.\nII. P RELIMINARIES\nCost model for communication complexity. The aforemen-\ntioned latency ( α)-bandwidth ( β) cost model ( α-βmodel) [15],\n[38] is widely used to analyze the communication complex-\nity [22], [32]. In this cost model, αis the latency cost unit of\na transmission between two workers. βis the bandwidth cost\nunit. All communication cost can be modeled by xα+yβ\nwherexcan be counted as the number of transmission rounds\nandycan be counted as the total volume of data received by\na worker during the communication process [5].\nAll-Reduce, Reduce-Scatter and All-Gather. All-Reduce,\nReduce-Scatter and All-Gather are common communication\noperations. Figure 2 shows the results after these operatio ns.\nAll-Reduce operation is commonly used to synchronize and\nsum gradients from all workers [5], [6], [8], [12], [23], [39 ].\nBesides, using Reduce-Scatter and All-Gather successivel y\ncan achieve the same results as All-Reduce. Reduce-Scatter\nreduces each part of gradients into its corresponding worke r.\nAnd All-Gather gathers all parts of gradients from all worke rs\nin each worker. There are two popular, efﬁcient All-Gather\nalgorithms called recursive doubling [5], [45] and Bruck Al l-\nGather [4], as shown in Figure 3. The recursive doubling\nalgorithm is efﬁcient when the number of workers is a power\nof 2, but cannot be used directly when the number is not\na power of 2 [5], [45]. In contrast, Bruck All-Gather is\nefﬁcient and reach the lower bound of bandwidth cost at any\nnumber of workers [45]. For recursive doubling, each worker\n\nBefore Operation After Operation \nxěj xěj xěj xěj\nx\nx\nx\nxx\nx\nx\nxx\nx\nx\nxx\nx\nx\nxx\nReduce  \n-Scatter ěj\nxěj\nxěj\nxěj\nx\nx\nx\nxx x x x\nx\nx\nx\nxx\nx\nx\nxx\nx\nx\nxx\nx\nx\nx( j All- \nReduc \nAll- \nGathere) ( j ) ( j ) ( j )\n(0)\n(0)\n(0)\n(0)(1)\n(1)\n(1)\n(1)(2)\n(2)\n(2)\n(2)(3)\n(3)\n(3)\n(3)( j )\n( j )\n( j )\n( j )(0) (1) (2) (3)\n0\n1\n2\n31\n2\n30\n1\n2\n30\n1\n2\n30\n1\n2\n301\n2\n30\n1\n2\n30\n1\n2\n30\n1\n2\n30 0\n1\n2\n3\nWorker 2 Worker 1 Worker 3 Worker 4 Worker 2 Worker 1 Worker 3 Worker 4\nFig. 2. Illustration of All-Reduce, Reduce-Scatter and All -Gather operation\ncommunicates and exchanges all data with a worker at a\ndistance of 2tat thet-th step. And for Bruck All-Gather, each\nworker sends data to a worker at a distance of 2ton one side\nand receives data from a worker at a distance of 2ton the\nother side at the t-th step. The communication complexity of\nrecursive doubling and Bruck All-Gather, when the number of\nworkers is a power of 2, is\nTAll−Gather= log2Pα+nP−1\nPβ. (1)\nTop-ksparsiﬁcation. Top-ksparsiﬁcation is usually used in\nefﬁcient distributed deep learning for sparsifying gradie nts [3],\n[22], [24]. It selects kgradients with the largest absolute\nvalues from dense gradients where kis usually set to a certain\nproportion multiplied by the number of dense gradients. Top -k\nsparsiﬁcation does not seriously affect learning performa nce,\nwhich has been proved by many previous works theoreti-\ncally [3], [42], [44] and empirically [2], [22], [24], [34].\nIII. T HEPROPOSED SPARDL\nIn this section, we describe the proposed SparDL , an\nefﬁcient distributed deep learning framework that solves t he\nSGA dilemma for sparse gradients. SparDL is composed of\nthree proposed algorithms: Spar-Reduce-Scatter ,global resid-\nual collection andSpar-All-Gather . In the following sections,\nwe ﬁrst overview the framework of SparDL , and then provide\nan in-depth introduction to these algorithms.\nA. Overview\nThe overview of the SparDL framework is shown in Fig. 4.\nSpeciﬁcally, for a cluster with Pworkers, we ﬁrst generate\ngradients at each worker from forward propagation and back-\nward propagation in parallel. Next, we plus the locally stor ed\nresiduals from the global residual collection algorithm to the\nlocal gradients. Then we divide all Pworkers into dteams\nequally. We use the Spar-Reduce-Scatter algorithm, Spar-All-\nGather algorithm and an All-Gather operation successively to\nget the ﬁnal gradients after synchronization. If the user se ts\nd= 1, only Spar-Reduce-Scatter and an All-Gather operation\nare used. Meanwhile, the global residual collection algorithm\ncollects discard gradients as residuals during the Spar-Reduce-\nScatter and Spar-All-Gather algorithms at each worker. AtSent block Rceived block Send \nWorker 1\nWorker 2\nWorker 3\nWorker 4\nStep 1 Step 2Final block \nFinal state \n(a) Recursive doubling for All-Gather\nWorker 1\nWorker 2\nWorker 5\nWorker 61\n52\n6\nStep 1Worker 3\nWorker 43\n4\nStep 21\n1\n5\n56\n2\n4\n63\n32\n4\nStep 31\n1\n5\n56\n2\n4\n63\n32\n44\n5\n6\n1\n2\n35\n6\n1\n2\n3\n4\nState before shift 32\n43\n54\n65\n16\n211\n1\n5\n56\n2\n4\n63\n32\n44\n5\n6\n1\n2\n35\n6\n1\n2\n3\n4\n(b) Bruck All-Gather for All-Gather\nFig. 3. Illustration of recursive doubling and Bruck All-Ga ther\nlast, we update the model replica with the ﬁnal gradients at\neach worker. Besides, unless noted otherwise, SparDL denotes\nSparDL withd= 1 and thereby only utilizes Spar-Reduce-\nScatter and an All-Gather operation.\nB. Spar-Reduce-Scatter Algorithm\nTo address the SGA dilemma with low communication\ncomplexity, we propose Spar-Reduce-Scatter (SRS) algorithm,\nwhich combines the Reduce-Scatter phase with multiple bloc k-\nwise sparsiﬁcation processes for the ﬁrst time. Moreover, w e\npropose a non-recursive structure for Reduce-Scatter to en sure\nlow communication complexity and efﬁciently work on any\nnumber of workers without additional operations. A worker’ s\nsource and target workers differ at one transmission step in this\nstructure. A six-worker example is provided to illustrate S RS,\nas shown in Figure 5. Besides, to clarify the SRS algorithm,\nwe setd= 1and separate SRS into two processes: partitioning\nand transmission with sparsiﬁcation.\n1) Partitioning. For worker rw, which is the w-th worker of\nallPworkers, given gradients G(w,t)from the latest iteration\ntafter training one batch (forward propagation and backward\npropagation) and residuals ξt−1\nwfrom last iteration t−1. First,\nworkerrwsums up G(w,t)andξt−1\nw as newG(w,t). Then\nit partitions G(w,t)intoPblocks{G(w,t)\n0,...,G(w,t)\nP}and\nsparsiﬁes its local dense gradients by selecting top-k\nPvalues of\neach block. Then we partition Pblocks into one preservation\nbagB0andl=⌈log2P⌉sending bags {Bw\n1,...,Bw\nl}in\nsequence. These sending bags will be used as sending units\nin the following transmission steps, and we will preserve th e\npreservation bag. Speciﬁcally, we start from the block G(w,t)\nw\nand put it into the preservation bag Bw\n0. Then we put the\nnext20blocks from w+20tow+21−1into sending bag\n\nSpar-Reduce-Scatter Generate Gradients Global Residual Collection Algorithm \nDivide into d teams Final \ngradients \nSpar-All-Gather All-Gather Update Model Parameter \nreplica Data \nshard Reuse global residuals Transmission after sparsification Collecting or reusing residuals P workers workers P\nd\nFig. 4. An overview of SparDL framework\nBw\n1, and the following 21blocks from w+21tow+22−1\nintoBw\n2. Repeat this process until all the blocks are put into\ndifferent bags. Note that, in the partitioning process, Pblocks\nare considered to line up in a circle, i.e., if reaching block P\nin the partitioning process, continue putting blocks from b lock\n1. It is possible that the remaining blocks are not enough to\nﬁll the last sending bag Bl, and the remaining number will be\nE=P−2⌈log2P⌉−1.\nExample 1. Figure 5 presents the partitioning process. Sup-\npose we have 6 workers. Take worker 1 for example, we put its\nﬁrst block {1}into preservation bag B1\n0, andl=⌈log26⌉= 3,\nits next20block{2}into sending bag B1\n1and the next\n21blocks{3,4}into sending bag B1\n2. The last sending\nbagB1\n3should be put 22blocks, but worker 1 has only\nE= 6−2⌈log26⌉−1= 2 blocks{5,6}left. Therefore, we\nput block {5,6}into sending bag B1\n3, which is not full.\n2) Transmission with sparsiﬁcation. After partitioning blocks\ninto bags, we transmit all sending bags of blocks from the\nlast bagBw\nltoBw\n1in sequence. Speciﬁcally, for worker rw\nat each step i, it regards worker rw+2l−ias target and regards\nworkerrw−2l−ias source. It sends bag Bw\nl−i+1to worker\nrw+2l−iand receives bag Bw−2l−i\nl−i+1from worker rw−2l−i.\nAfter receiving a sending bag from the source, each worker\nadds the sparse gradients from the blocks in the received\nsending bag to the gradients in the locally remaining blocks\naccording to the gradient indexes. Then we utilize block-\nwise sparsiﬁcation in each worker, i.e., selecting top-k\nPvalues\nin each locally remaining block after the summation and\nremoving the unselected gradients. Through this sparsiﬁca tion\nin each step, the number of gradients of each block in sending\nbag is maintained ask\nP, which solves the SGA dilemma. At\neach step, the received sending bag is always a subset of the\nheld blocks, i.e., blocks that have not been sent, according\nto Theorem 1. Thus, every worker’s number of held blocks\ndecreases at every step. After lsteps, all sending bags have\nbeen sent. And each worker only holds one block with a\ndifferent rank related to the worker’s rank at the preservat ion\nbag, e.g., rwholds block G(w,t)\nw, which is in line with the\npurpose of Reduce-Scatter. Besides, this process works in a\nnon-recursive way, where each worker’s source and target\ncan be different at each transmission step. Thus, the SRS\nalgorithm is available on any number of workers directly,\nwithout additional operations.\nExample 2. Figure 5 also presents the transmission and\nsparsiﬁcation process. Take worker 1 for example, at step\n1, the communication distance is 23−1= 4. Thus, it sendsbagB3with 2 blocks to worker 5 and receives a bag with 2\nblocks from worker 2. At step 2, the communication distance\nis23−2= 2. Thus, worker 1 sends bag B2with 2 blocks to\nworker3and receives a bag with 2 blocks from worker 5.\nAt the last step, worker 1 sends bag B1with 1 block to the\nworker2and receives a bag with 1 block from the worker 6.\nTheorem 1. At each step iin transmission, the ranks of blocks\nin sending bag from the w-th worker are a subset of those of\nthe blocks held by the w+2l−i-th worker.\nProof. The left side of sending blocks from w-th worker at\nstepiisw+2l−i. Besides, the blocks held by the w+2l−i-\nth worker is from w+ 2l−itow+ 2l−i+ 2l−i−1. Thus,\nat stepi, the left side of sending bag is also the left side\nof reserving bag. Since the size of the last sending bag is\nE=P−2⌈log2P⌉−1andE < P −E, the right side of\nthe last sending bag will be in the remaining blocks of the\nw+2l−i-th worker. Thus the Theorem holds at step 1. Since\nthe range of other sending blocks from w-th worker at step i\nis fromw+2l−itow+2l−i+1−1and thew+2l−i-th worker\nholds block from w+2l−itow+2l−i+2l−i−1at stepi, the\ntheorem holds at step i,i/n⌉}ationslash= 1. Therefore, the Theorem holds\nat each step.\nAfter SRS, each worker holds a block ofk\nPsparse gradients\nat a different position. At last, we use the All-Gather opera tion\non the block of each worker, which sends the local data\nof the worker to all other workers. We choose the Bruck\nalgorithm [4] for the All-Gather operation (mentioned in\nSection 2 and shown in Figure 3) since it also efﬁciently work s\non any number of workers without additional operations. Aft er\nBruck All-Gather, all workers have the same global gradient s,\nwhich means synchronization is complete. Therefore, SparDL\nis efﬁcient for every number of workers because both SRS and\nBruck algorithms are efﬁcient for every number of workers.\nCommunication Complexity Analysis. In the transmission\nand sparsiﬁcation process, each worker takes l=⌈log2P⌉\nrounds of communication in parallel. Thus, the latency of\nSRS is⌈log2P⌉α. During the transmission process, each\nworker sends P−1blocks in sending bags. Since sparse\ngradients should be stored with indices and values with the\ncommonly used form, i.e., coordinate (COO) format, the\ntransmission volume of each block in sending bag is2k\nP.\nHence, the bandwidth of transmission in SRS is 2kP−1\nPβ. The\ncommunication complexity of SRS is\nT1=⌈log2P⌉α+2kP−1\nPβ. (2)\nThe volume of sparse gradients is 2k\nP(indexes and values) in\neach worker. Thus, the communication complexity of Bruck\nalgorithm is\n\n1 2 3 4 5 6\n1 2 3 4 5 6\n1 2 3 4 5 6\n1 2 3 4 5 6\n1 2 3 4 5 6\n1 2 3 4 5 6Worker 1\nWorker 2\nWorker 3\nWorker 4\nWorker 5\nWorker 6\nPartitioning Step 1 Step 2 Step 31 2 3 4 5 6\n1 2 3 4 5 6\n1 2 3 4 5 6\n1 2 3 4 5 6\n1 2 3 4 5 6\n1 2 3 4 5 6\nInitial Gradients 1 2 3 4 5 6\n1 2 3 4 5 6\n1 2 3 4 5 6\n1 2 3 4 5 6\n1 2 3 4 5 6\n1 2 3 4 5 61 2 3 4 5 6\n1 2 3 4 5 6\n1 2 3 4 5 6\n1 2 3 4 5 6\n1 2 3 4 5 6\n1 2 3 4 5 61 2 3 4 5 6\n1 2 3 4 5 6\n1 2 3 4 5 6\n1 2 3 4 5 6\n1 2 3 4 5 6\n1 2 3 4 5 6\nFig. 5. Spar-Reduce-Scatter Algorithm. The number in the bl ock represents the position of the block. Each block contain s part of gradients (dense or sparse)\nof its corresponding position.\n1 2\n0.23\n0.2 0.1Block b in \nWorker A \n2 3\n0.35\n0.2 0.1Block b in \nWorker B \nStep 1Send 2 3\n0.55\n0.2 0.3Block b in \nWorker B \nSum 1\n0.1\nDiscarded \n2 3\n0.75\n0.5 0.3\nBlock b in \nWorker B 1 3\n0.15\n0.3 0.4\nBlock b in \nWorker C Final \nStep lSend 1 3\n0.85\n0.8 0.4\nBlock b in \nWorker C Sum 2\n0.3in-procedure \nresidual \nend-procedure \nresidual \nFig. 6. Illustration of the difference between in-procedur e residual and end-\nprocedure residual\nT2=⌈log2P⌉α+2kP−1\nPβ. (3)\nTherefore, the total communication complexity of SparDL is\nTall=T1+T2= 2⌈log2P⌉α+4kP−1\nPβ. (4)\nOptimization for SRS. After observation, We ﬁnd it un-\nnecessary to sparsify the summed blocks immediately after\nreceiving a sending bag. Reducing unnecessary top- kselection\ntimes can reduce the time cost per iteration and accelerate\nconvergence. To solve the SGA dilemma, we only need to\nensure that gradients are sparse enough in the next sending b ag\nbefore the next transmission step. Consequently, we modiﬁe d\nthe sparsiﬁcation timing: instead of performing sparsiﬁca tion\non blocks after summation, each worker now only sparsiﬁes\nthe blocks destined for the next transmission step after add ing\nthe received gradients.\nC. Global Residual Collection Algorithm\nSparDL uses multiple top- kselections between transmis-\nsion steps to solve the SGA dilemma. However, multiple\nselections discard many gradients, which may contain im-\nportant gradients. Losing these crucial gradients may lead to\nslower convergence, e.g., lower accuracy at the same iterat ions.\nTherefore, we propose the global residual collection algor ithm\nto store all discarded gradients and ensure fast convergenc e.\nExisting compensation methods [22], [24], [41] collect the\ndiscarded gradients from sparsiﬁcation as residuals and pl us\nthem to new gradients at the next iteration. There are three\ntypes of discarded gradients in SparDL . We call them local\nresidual, end-procedure residual, and in-procedure resid ual.\nThe local residuals are gradients that are sparsiﬁed and dis -\ncarded at each worker locally before transmission. As for\nend-procedure residuals and in-procedure residuals, they are\nboth discarded gradients between transmission steps. The\ndifference between them is that the gradients correspondin g\nto the end-procedure residuals are completely discarded in\nthe communication process, while the gradients correspond ing\nto the in-procedure residuals are not. In other words, theindexes of end-procedure residuals do not appear in the ﬁnal\nglobal gradients and the indexes of in-procedure residuals still\nexist in the ﬁnal global gradients. To illustrate the differ ence\nbetween in-procedure residual and end-procedure residual ,\nwe consider the example in Figure 6. From this ﬁgure, the\ndiscarded gradient with index 2 is end-procedure residual a nd\nthe discarded gradient with index 1 is in-procedure residua l.\nThe reason is that the gradient with index 2 is completely\ndiscarded, and none of the gradient value under this index is\nshown in the ﬁnal global gradients; On the contrary, index 1\nstill exists in the ﬁnal global gradients, which indicate th at\nthere are some gradients with index 1 are selected in the\ncommunication process. The existing compensation methods\nonly collect local residuals [24] or local and end-procedur e\nresiduals [22], [41] but cannot collect in-procedure resid uals.\nNonetheless, SparDL generates plenty of in-procedure residu-\nals on each worker due to its multiple sparsiﬁcation process es.\nAs a result, existing methods are unsuitable for SparDL . To\naddress this issue, we propose the global residual collecti on\nalgorithm, which encompasses the collection of all three ty pes\nof residuals.\nSince the indexes of in-procedure residuals still exist in t he\nﬁnal gradients like normal global gradients, it is impossib le to\ncollect them only according to the ﬁnal indexes. Therefore, we\ncollect these residuals throughout the communication proc ess.\nAs presented in Algorithm 1, we collect the residuals in two\nsteps. Since existing methods tend to store the residuals on the\nworker who generated them, we also store the local and end-\nprocedure residuals on the local worker (lines 13-14). But t he\nin-procedure residuals may come from multiple workers, and\ncollecting these residuals on the generated workers requir es\nhigh communication cost, severely weakening the meaning\nof sparse communication. For this case, we save this part of\nresiduals directly on the workers who perform the sparsiﬁ-\ncation (line 8). Although these residuals are collected on t he\nprocessing worker, from the perspective of the entire train ing\ncluster, the discarded gradients are still collected and wi ll be\nused sooner or later.\nD. Spar-All-Gather Algorithm\nThe need for low latency and low bandwidth varies in dif-\nferent network environments. Latency cost may dominate the\ntime consumption in some situations. For example, bandwidt h\ncost converges to a constant value when there are plenty of\nworkers in the cluster since the upper bound of bandwidth\nexists, while latency cost does not converge. Thus, in order\n\nAlgorithm 1: SparDL with Global Residual Collec-\ntion Algorithm\nInput: Local gradients G(w,t)at worker rwand iteration t,\nresiduals ξt−1\nwfrom iteration t−1at worker rw, the\nnumber of workers P\n//Worker rw,w←1,...,Nin parallel\n1G(w,t)←G(w,t)+ξt−1\nw.\n2Partition G(w,t)intoPlocal blocks\nG(w,t)←{G(w,t)\n0,...,G(w,t)\nP}.\n3CopyG(w,t)toG(w,t)\ncopy.\n4Partitions blocks of gradients into different bags. for\nstep←1toldo\n5 foreach block G(w,t)\ni that will be transmitted next do\n6 SparsifyG(w,t)\ni by selecting top- kgradients gk\ni.\n7 ξ(w,t)\ni←G(w,t)\ni−gk\ni.\n8 Transmission and summation.\n9Sparsify the only reserved block G(w,t)\nw in worker rw.\n10All-Gather all blocks from all workers.\n11forgk\ni∈{gk\n0,...,gk\nP}do\n12 DenoteIithe indexes which are in gk\ni.\n13 Replace part of G(w,t)\ncopy at indexes Iiwithξ(w,t)\ni at\nindexesIi.\nto further improve the efﬁciency of SparDL , it is crucial to\nreduce the latency cost and make our SparDL adjustable to\nthe focus of the two cost. Therefore, we propose the Spar-\nAll-Gather (SAG) algorithm for SparDL based on the idea of\ndivide-and-conquer.\nSpeciﬁcally, we ﬁrst divide all Pworkers into dteams\nequally, which should satisfy d|P. Then theP\ndworkers in\neach team perform the SRS algorithm in parallel. Since there\nareP\ndworkers in each team, each worker divides their gradi-\nents intoP\ndblocks during SRS. After SRS, each worker holds\none block of sparse gradients. Subsequently, we synchroniz e\ndifferent teams and make the workers with the same ranks of\neach team hold the same L(k,d,p) =dk\nPsparse gradients after\nsynchronization. We propose two different Spar-All-Gathe r\n(SAG) algorithms to synchronize teams in two situations. At\nlast, we use Bruck All-Gather in each team.\n1)dis a power of 2. For the case where the number of\nteams (i.e., d) is a power of 2, we propose the recursive-\nbased Spar-All-Gather (R-SAG). R-SAG is based on recursive\ndoubling [45], as shown in Figure 3(a), and combines top-\nkselection after each transmission step. Speciﬁcally, the R -\nSAG’s communication occurs between teams and teams, with\nthet-th transmission step when the team Xand a team Y\nat a distance of 2texchange all data. For a worker in X, it\ncommunicates with the worker at the same position in Y. They\nexchange the only block of sparse gradients they hold. Unlik e\nSRS, the sent block is still held after transmission. Then, e ach\nworker adds the received gradients to the held gradients. Du e\nto index inconsistency, the summed sparse gradients are als o\nsubject to SGA problems. Thus, we sparsify the gradients by\nselecting the top- L(k,d,p)values.\nIn recursive doubling communication, the source and targetin one transmission are the same for any worker. Thus, both\nsides of the transmission hold the same sparse gradients aft er\nsummation and discard the same gradients after selection.\nTherefore, we collect half value of the discarded gradients\nas residuals on each side of the transmission.\nAfter communicating log2(d)times, all teams will be\nsynchronized. Then we All-Gather sparse gradients of each\nworker using Bruck All-Gather in each team.\nCommunication Complexity Analysis. Since the size of\nthe message in each transmission is 2L(k,d,p)(indexes and\nvalues), the cost of R-SAG step is\nT′\n2= log2dα+2dk\nPlog2dβ. (5)\nWe still use Bruck All-Gather. Hence, the communication cos t\nof SRS and the ﬁnal All-Gather is both\nT′\n1=T′\n3=⌈log2P\nd⌉α+2kP−d\nPβ. (6)\nTherefore, the total communication cost of SparDL with R-\nSAG is:\nT′\nall=T′\n1+T′\n2+T′\n3= (2⌈log2P\nd⌉+log2d)α\n+2k(2P−2d\nP+d\nPlog2d)β.(7)\nThe impact of din R-SAG. Sincedin R-SAG is a power\nof 2, we analyze the change when increasing dto2d. Then,\nwe can ﬁnd that latency cost decreases α, but bandwidth\ncost increases by 2dk\nplog2dβ. Thus, when dis set to 2,\nSparDL with R-SAG has lower latency cost than SparDL\nwithout SAG (i.e., d= 1) and has the same bandwidth cost.\nThen, with the increase of d, the increase in bandwidth cost\nbecomes progressively more pronounced, possibly exceedin g\nthe reduction in latency cost when dis too large.\n2)dis not a power of 2. For the case where the number\nof teams is not a power of 2, recursive doubling cannot be\nused directly. Instead, the Bruck All-Gather algorithm can\nbe utilized to synchronize gradients among different teams .\nHowever, the communication consistency will be damaged\nwhen Bruck All-Gather combines with sparsiﬁcation like Spa r-\nReduce-Scatter and R-SAG to solve the SGA issue. As shown\nin Figure 3(b), if we use top- L(k,d,p)selection at each\nstep after addition, then at the end of the communication,\nthe gradient blocks held by worker 1 and worker 2 expe-\nrience entirely different compression orders, e.g., block 1\nis compressed with block 6 in worker 1 but with block 2\nin worker 2 after step 1. Thus, the ﬁnal sparse gradients\nare different for worker 1 and worker 2. Such inconsistency\ndefeats the purpose of synchronous SGD in training distribu ted\ndeep learning models and will make different workers hold\ndifferent deep learning models after several iterations. T hus,\nthe SGA problem still exists when using the Bruck All-Gather\nto synchronize gradients among different teams.\nObservation. Previous study [22] has observed that the in-\ndexes distribution of the selected sparse gradients from ea ch\nworker changes slowly with regard to iterations. Besides, t he\nnumber of sparse gradients, after using Bruck All-Gather to\nsynchronize gradients among teams, also changes slowly, as\n\n0 2000 4000 6000 8000 10000\nBatch1.52.02.5Number of gradients1e5\nnumber of gradients\nFig. 7. Number of sparse gradients after using Bruck All-Gat her to synchro-\nnize gradients among teams during 20 epochs\nshown in Figure 7. Thus, if we make an additional top-\nhselection with a suitable number hbefore Bruck All-\nGather communication and change hslowly in the subsequent\ncommunication rounds, we can make the number of gradients\nafter Bruck All-Gather near L(k,d,p). In this way, using\nadditional top- hselection can reduce the bandwidth cost with\nno excessive gradients lost compared to using selection dur ing\ncommunication. Therefore, we propose the Bruck-based spar se\nAll-Gather algorithm (B-SAG).\nSolution. First, we specify the range of [k\nP,dk\nP]forhwhen the\nnumber of gradients after B-SAG is equal to L(k,d,p). The\nlower and upper bounds are taken as entirely non-overlappin g\nand entirely overlapping gradient indexes between differe nt\nworkers, respectively. Second, through the Figrue 7, we can\nﬁnd that the number of gradients after Bruck All-Gather\nis stable within successive iterations. And there are some\nﬂuctuations throughout the training process. Thus, we need\nto obtain a suitable value of has soon as possible to make the\nnumber of gradients after B-SAG close to L(k,d,p), and then\nadjusthaccording to the ﬂuctuation. Therefore, we propose a\ncompression ratio hadjustment algorithm for B-SAG, which\nis motivated by the CWnd algorithm [9].\nAs shown in Algorithm 2, we adjust hby a step size with\ndirection (i.e., sign). The initial hisk\nP, and the initial step size\nis0.01×kd−1\nPwith positive direction. After each iteration, we\nseth=h+step and adjust the step according to the number of\ngradients amount after Bruck All-Gather in this iteration ( line\n12). If the gradient amount is greater than L(k,d,p), we set\nthe direction as positive, and if the gradient amount is not\ngreater than L(k,d,p), we set the direction as negative. In\naddition, if two consecutive step adjustments are in the sam e\ndirection, we double the step (lines 3-8), and if the directi on\nis changed, we divide the step by 2 (lines 9-11). Through the\nstep adjustments, we can change hto make the number of\ngradients recover to near L(k,d,p)as soon as possible and\nadjusthslowly when the number changes slowly.\nAfter completing the sparse Bruck All-Gather, we sparsiﬁed\nthe number of sparse gradients to L(k,d,p). After B-SAG, the\namount of gradients held by each worker may be greater than\nL(k,d,p), so we perform sparsiﬁcation on each worker. Since\neach team has the same gradients, each worker collects1\ndvalue\nof the discarded gradients as residuals. Besides, since dif ferent\nworkers in the same team still have blocks of gradients with\ndifferent positions, we All-Gather these blocks by Bruck Al l-\nGather as we do in SparDL with R-SAG.\nCommunication Complexity Analysis. The size of gradientsAlgorithm 2: Compression Ratio Adjustment Algo-\nrithm for B-SAG\nInput: The number of workers P, the number of teams d,\nthe number of gradients after B-SAG Ntat iteration\nt, the selecting number k\n1Initialh←k\nP, the step size step←0.01×kd−1\nP,\nflag←False , andL(k,d,p)←dk\nP.\n2fort←1toTdo\n3 ifNt> L(k,d,p)⊕step >0then\n4 ifflag then\n5 step←step×2.\n6 flag←False .\n7 else\n8 flag←True .\n9 else\n10 step←−step×1\n2.\n11 flag←False .\n12h←h+step.\n13 Proceed B-SAG with top- hsparsiﬁcation.\n14 CountNtas the number of gradients after B-SAG.\nsending before B-SAG is 2h∈[2k\nP,2dk\nP]. Thus, the bandwidth\noverhead of B-SAG is [2kd−1\nPβ,2kd2−d\nPβ]. The communica-\ntion cost of B-SAG is:\n⌈log2(d)⌉α+2kd−1\nPβ≤T′\n2≤ ⌈log2(d)⌉α+2kd2−d\nPβ.(8)\nThe size of gradients after B-SAG is [2k\nP,2dk\nP], the commu-\nnication cost of ﬁnal All-Gather is\n⌈log2P\nd⌉α+2kP−d\nPdβ≤T′\n3≤ ⌈log2P\nd⌉α+2kP−d\nPβ.(9)\nTherefore, the total communication overhead of SparDL with\nB-SAG is:\n(2/ceilingleftbig\nlog2P\nd/ceilingrightbig\n+⌈log2d⌉)α+2kd2+P−2d\nPdβ\n≤T′\nall=T′\n1+T′\n2+T′\n3\n≤(2/ceilingleftbig\nlog2P\nd/ceilingrightbig\n+⌈log2d⌉)α+2kd2+2P−3d\nPβ.(10)\nThe impact of din B-SAG. dof B-SAG may not be a\npower of 2, but, like R-SAG, every time dis increased to 2d,\nlatency cost will be reduced by α. Different from R-SAG, the\nbandwidth cost of B-SAG has a value range, so we consider it\nfrom the lower bound and the upper bound, respectively. The\nlower bound of bandwidth cost decreases with the increase\nofdwhend <√p, but increases when d >√p; the upper\nbound of bandwidth cost increases when d >1.5, and the\nupper bound of bandwidth cost when d= 2 is the same as\nthe bandwidth cost of SparDL without SAG (i.e., d= 1).\nThus, an appropriate dreduces the communication time of\nSparDL with B-SAG, but a too large dmay cause an increase\nin the communication time since the increase in bandwidth\ncost exceeds the decrease in latency cost.\nThe selection of the optimal dfor SAG algorithm. For each\nnetwork condition and model size, there is an optimal number\nof groups dto make R-SAG or B-SAG get the fastest training\nefﬁciency. Through the experiment (in Section IV-G), we ﬁnd\nthat the time consumption of each epoch is relatively stable .\nThus, the dwith the least time consumption in the ﬁrst epoch\nis likely to be the optimal d. Besides, since dshould satisfy\n\nTABLE II\nDEEP LEARNING CASES USED FOR EVALUATION\nCase Task Type Models Dataset\nCase 1 Type 1 VGG-16 [43] CIFAR-10 [20]\nCase 2 Type 1 VGG-19 CIFAR-100\nCase 3 Type 1 ResNet-50 [13] ImageNet [37]\nCase 4 Type 2 VGG-11 House [1]\nCase 5 Type 3 LSTM-IMDB IMDB [25]\nCase 6 Type 4 LSTM-PTB [14] PTB [26]\nCase 7 Type 5 BERT [7] Wikipedia [7]\nd|P, there are only a few available values for d. Therefore, to\nselect the optimal number of groups d, we suggest that users\nrun one epoch for each d, and choose dwith the least time\nconsumption as the optimal d.\nComputation and Space Complexity Analysis for SparDL .\nLetPbe the number of workers, nbe the number of dense\ngradients, kbe the number of sparse gradients ( k≪n),dbe\nthe number of teams in SparDL with SAG. SparDL ﬁrst parti-\ntions dense gradients intoP\ndblocks and l=⌈log2P\nd⌉sending\nbags, which takes O(P\nd)time and O(1)space. Then, SparDL\ndirectly restores all gradients for the global residual col lection\nalgorithm and takes O(n)time andO(n)space. Next, SparDL\ntransmits gradients ltimes, which takes O(P\ndkd\nP) =O(k)\ntime and O(n)space for gradients summation and receiving\nbuffer, and sparsiﬁesP\ndblocks of gradients to selectk\nP\nd=kd\nP\ngradients each block, which takes O(P\ndnd\nP) =O(n)time and\nO(lognd\nP)space to select top-kd\nPgradients using Quicksort-\nbased algorithms [24], [46], in the Spar-Reduce-Scatter st ep.\nThen, ifd >1, we use R-SAG or B-SAG. Otherwise, we\nskip to the next step. For R-SAG, SparDL transmits and spar-\nsiﬁes gradients log2dtimes, which takes O(kd\nPlogd)time and\nO(logkd\nP)space for sparsiﬁcation and O(kd\nPlogd)time and\nO(nd\nP)space for transmission. For B-SAG, SparDL sparsiﬁes\ngradients twice and transmits gradients ⌈log2d⌉times. This\nstep takes O(kd\nP)time and O(logkd\nP)space for sparsiﬁcation\nandO(kd2\nP)time and O(n)space for transmission.\nAfter SAG step, SparDL All-Gathers all blocks and takes\nO(1) time and O(n)space. At last, the global residual\ncollection algorithm takes O(n)time and O(1)space at most.\nOverall, SparDL (R-SAG) takes O(n+kd\nPlogd)computation\ncomplexity; SparDL (B-SAG) takes O(n+kd2\nP)computation\ncomplexity; SparDL without SAG takes O(n)computation\ncomplexity. They all take O(n)space complexity.\nIV. E XPERIMENTS\nA. Experimental Setup\nIn this section, we evaluate the effectiveness and efﬁcienc y\nof the proposed SparDL using multiple practical deep learning\nmodels and datasets, compared with four baselines.\nDatasets and models. To comprehensively evaluate the ef-\nﬁcacy of the proposed SparDL in diverse scenarios, we\nleverage ﬁve distinct types of tasks: image classiﬁcation ( Type\n1), image regression (Type 2), text classiﬁcation (Type 3),\nlanguage modeling (Type 4), and language processing (Type\n5). We use seven different real-world datasets and sevenCommunication cost Computation cost \n0.00.4\n0.2Per-update time (s)\nTop kDSA Top kA Ok-Top kSparDL \n(a) VGG-19 on CIFAR1000.00.2\n0.1Per-update time (s)\nTop kDSA Top kA Ok-Top kSparDL \n(b) VGG-11 on House\n0.00.5\n0.2Per-update time (s)\nTop kDSA Top kA Ok-Top kSparDL \n(c) LSTM-IMDB on IMDB0.01.0\n0.5Per-update time (s)\nTop kDSA Top kA Ok-Top kSparDL \n(d) LSTM-PTB on PTB\nFig. 8. Per-update time with 14 workers\nwidely-utilized deep learning models. Specﬁcally, these m od-\nels contain 14.7M, 20.1M, 23.5M, 9.2M, 35.2M, 66M, 133.5M\nparameters, respectively. LSTM-IMDB and LSTM-PTB are\n2-layer RNN models with LSTM units, and LSTM-PTB is\nsimilar as in [24] and [41]. The deep learning models and\ndatasets are summarized in Table II.\nCompetitors. We compare the proposed SparDL with four\nexisting sparse All-Reduce methods: gTop k[41], [42], O k-\nTopk[22], Top kA [34], and Top kDSA [34].\nExperimental setting. All experiments in this paper are\nimplemented in PyTorch 1.11.0 [30] with mpi4py 3.0.3. All\nexperiments are evaluated in a GPU cluster with 14 GPU\nmachines. Each GPU machine is installed with CentOS-7.9,\nNvidia driver 510.39.01, and CUDA 11.6 and has 256 GB\nRAM, two Intel(R) Xeon(R) 4314 CPU @ 2.40GHz proces-\nsors, one GeForce RTX A40 GPU. All machines are connected\nto an Ethernet with default setting.\nB. Performance in Four Deep Learning Cases\nTo verify the superiority of our proposed SparDL frame-\nwork on communication cost, we compare it with O k-\nTopk[22], Top kA [34] and Top kDSA [34] on four distinct\ndistributed deep learning cases where 14 workers involved.\nThese four cases are corresponding to image classiﬁcation,\nimage regression, text classiﬁcation, and language modeli ng,\nrespectively. The experimental results are illustrated in Fig.8.\nFrom this ﬁgure, we can observe that SparDL has the lowest\ncommunication cost in all four cases. Speciﬁcally, Fig.8(a )\nshows that, when training VGG-19 on CIFAR-100, SparDL\nexhibits6.4×faster than Top kDSA,5.1×faster than Top kA,\nand1.6×faster than O k-Topkfor communication cost individ-\nually. Similarly, in Fig. 8(b), while training VGG-11 on Hou se,\nSparDL is the fastest and achieves 5.6×,4.7×,2.2×speedup\nover the state-of-the-art methods in communication cost. F rom\nFig. 8(c) and Fig. 8(d), SparDL is2.7×,3.8×,1.8×and\n5.0×,4.5×,2.3×faster than the baselines on LSTM-IMDB\nand LSTM-PTB, respectively.\nAmong the compared methods, Top kDSA is the slowest\ndue to its incomplete resolution of the Sparse Gradient Ac-\ncumulation (SGA) dilemma, leading to increasing transmis-\n\nSparDL Ok-Top k Top kDSA Top kA\n45 .075 .0\n13000 2600060 .0Accuracy (%) \n0\nTraining time (s)\n(a) VGG-19 on CIFAR-1000.00 0.08 \n90 180 0.04 Loss \n0\nTraining time (s)\n(b) VGG-11 on House\n60 .090 .0\n2050 410075 .0Accuracy (%) \n0\nTraining time (s)\n(c) LSTM-IMDB on IMDB4.50 7.50 \n20000 40000 6.00 Loss \n0\nTraining time (s)\n(d) LSTM-PTB on PTB\nFig. 9. Training different distributed deep learning cases with 14 workers\nsion volume after each step in one iteration. Besides, the\nperformance of Top kA is also inferior, for it solves the SGA\ndilemma only using the All-Gather operation, incurring hig h\nbandwidth cost even with the fast recursive doubling algo-\nrithm. Additionally, SparDL also signiﬁcantly outperforms\nOk-Topkin communication cost, since O k-Topkintroduces\nmany additional transmission operations to balance gradie nts\namong workers for solving the SGA dilemma, resulting in\nhigh latency cost and large upper bound of bandwidth cost.\nMoreover, O k-Topkemploys threshold pruning instead of\ntop-kselection, which leads to the actual time consumption\nof Ok-Topkexceeding its theoretical time consumption. In\ncontrast, SparDL , utilizing top- kpruning, avoids this issue\nentirely. We can also observe that the communication cost\nof training VGG-11 is lower than that of VGG-19 for each\nmethod. The reason is that VGG-19 has more parameters\nthan VGG-11, and more parameters cause more bandwidth\ncost, subsequently increasing the communication time. For the\nsame reason, the communication cost of training LSTM-PTB\nis higher than LSTM-IMDB. As a result, SparDL consistently\nachieves faster iteration speeds than Top kDSA, Top kA, and\nOk-Topkacross various task types, models, and datasets.\nC. Convergence in Four Deep Learning Cases\nTo demonstrate SparDL ’s capability in reducing the overall\nconvergence time, we record the test accuracy (or test loss)\nofSparDL w.r.t. the training time and compare it with that\nfor the baselines, illustrated in Fig. 9. From the experimen tal\nresults, it is evident that SparDL outperforms all the baseline\nmethods, exhibiting the shortest training completion time\nwhile converging to similar accuracy (or loss) as the baseli ne\nmethods. Speciﬁcally, as shown in Fig. 9, SparDL achieves\n4.9×,4.0×,1.4×faster than Top kA, TopkDSA and O k-Topk\nfor VGG-19, respectively. In the case of VGG-11, the speedup\nis3.9×,3.3×,1.7×. As for LSTM-IMDB, the speedup is\n2.6×,3.6×,1.7×over the baselines. And for LSTM-PTB,Communication cost Computation cost \n0.00.4\n0.2Per-update time (s)\nOk-Top kSparDL \n(a) ResNet-50 on ImageNet0.00.8\n0.4Per-update time (s)\nOk-Top kSparDL \n(b) BERT on Wikipedia\nFig. 10. Per-update time on ResNet-50 and BERT with 14 worker s\nSparDL Ok-Top k\n0.00 0.80 \n200000 4000000.40 \n0\nTraining time (s)Accuracy (%) \n(a) ResNet-50 on ImageNet2.00 12 .0\n150000 3000007.00 Loss \n0\nTraining time (s)\n(b) BERT on Wikipedia\nFig. 11. Convergence on ResNet-50 and BERT with 14 workers\nSparDL is4.6×,4.3×and2.2×faster, respectively. More-\nover, from Fig. 9(a) to Fig. 9(d), the models trained with\nthe four sparse All-Reduce methods all converge to similar\naccuracy (or loss) after an equivalent number of epochs.\nSparDL expedites the training process by accelerating the\ncommunication process within each iteration. Since the com -\nputation cost is stable using different communication meth ods,\nthe acceleration impact of SparDL on per-update time is\nmarginally inferior to its impact on communication cost.\nNonetheless, SparDL still achieves a considerable acceleration\nin the training process. This is attributed to its efﬁcient\nresolution of the SGA dilemma without requiring additional\ntransmission. It accomplishes this by partitioning gradie nts\ninto blocks and maintaining the block size to ensure optimal\nefﬁciency, which substantially improves communication sp eed.\nBesides, SparDL can achieve comparable accuracy or loss\nas the baselines after the same number of epochs. This is\ncredited to the proposed global residual collection algori thm\nemployed by SparDL , enabling it to collect all gradients\npruned in each top- kselection for re-utilization and thereby\nmaintaining SparDL ’s convergence rate. In summary, SparDL\nhas comparable convergence rate as Top kA, TopkDSA and\nOk-Topkwhile accelerating training across different tasks with\ndifferent model types and data types.\nD. Comparison on large datasets: ImageNet and Wikipedia\nwith ResNet-50 and BERT\nTo further validate the efﬁciency of SparDL , this paper fur-\nther evaluates SparDL with the large datasets (i.e., ImageNet\nand Wikipedia) and large models (i.e., ResNet-50 and BERT).\nIn these experiments, we compare SparDL with Ok-Topk,\nwhich is the most efﬁcient method among the baselines. The\nresults are illustrated in Fig. 10, which veriﬁes the superi ority\nof our proposed SparDL . Fig. 10(a) and Fig. 10(b) depict the\nper-update time. For ResNet-50, we can observe that SparDL\nachieves 2.3×acceleration of communication cost compared\n\nSparDL Ok-Top k gTop k Top kDSA Top kA\n08\n8 14 4Speedup \n5\nNumber of workers 11 \n(a) Speedup w.r.t. number of workers45 .075 .0\n17500 3500060 .0Accuracy (%) \n0\nTraining time (s)\n(b) Case 2 with 8 workers\nFig. 12. Performance and convergence with different number of workers\nto Ok-Topk. For BERT, SparDL is2.0×faster than O k-Topk\nin terms of communication cost. Fig. 11(a) and Fig. 11(b) plo t\nthe test accuracy and training loss w.r.t. the training time of\nResNet-50 and BERT, respectively. These ﬁgures show that\nSparDL maintains convergence rate comparable to O k-Topk\nwhile completing the training process obviously faster tha n\nOk-Topkon large datasets. Besides, SparDL achieves 1.7×\nspeedup over O k-Topkfor ResNet-50, and a similar speedup\nof1.7×for BERT. Therefore, SparDL demonstrates efﬁciency\nand effectiveness on large datasets with different task typ es.\nE. Scalability\nIn this set of experiments, we evaluate the scalability of\nSparDL with evaluation metrics delineated in literature [50]\nand compare it with that for the baselines. The experimental re-\nsults are recorded in Fig. 12, which demonstrates that SparDL\nhas the highest scalability. Speciﬁcally, we set the averag e\ntraining time required to complete a single epoch of VGG-19\non CIFAR-100, employing Top kDSA as the communication\nmethod within an 8-worker cluster, as the reference time.\nSubsequently, we compute the speedup achieved by gTop k,\nTopkA, TopkDSA, and O k-Topkwith varying numbers of\nworkers, relative to this reference time. We only evaluate\ngTopkwith 8 workers since it only works in clusters with\npower-of-two workers. As shown in Fig. 12(a), SparDL ex-\nhibits superior scalability compared to other sparse All-R educe\nmethods. Besides, Fig. 12(b) depicts the test accuracy w.r. t.\ntraining time of VGG-19 using gTop k, TopkA, TopkDSA, Ok-\nTopkandSparDL with 8 workers. It is evident that SparDL\nsurpasses others in terms of speed.\nThe superior scalability of SparDL can be attributed to\nSparDL ’s lower communication complexity relative to the\nother approaches. As the number of workers Pincreases, the\nspeed gap between SparDL and the other methods widens.\nBesides, from Fig. 12(b), we can observe that the accelerati on\nmargin of SparDL over other methods with 8 workers is\nless pronounced than when trained with 14 workers, given\nSparDL ’s enhanced speedup with more workers. Additionally,\ngTopkis slower than SparDL , mainly attributable to its\ninefﬁcient synchronization of gradients via reduction tre e and\nbroadcast tree models, resulting in high bandwidth cost.\nF . Impact of Spar-All-Gather algorithm.\nIn this set of experiments, we evaluate the impact of\nthe Spar-All-Gather (SAG) algorithm on SparDL . The SAG85 .095 .0\n1600 320090 .0Accuracy (%) \n0\nTraining time (s)d = 1\nd = 2\n(a)SparDL with R-SAG85 .095 .0\n1600 320090 .0Accuracy (%) \n0\nTraining time (s)d = 1\nd = 2\nd = 7\nd = 14 \n(b)SparDL with B-SAG\nFig. 13. SparDL with different SAG algorithms in the cluster of 14 workers\n20 .026.0\n14 23 .0Per-epoch time (s)1\nThe number of teams d2 7R-SAG \nB-SAG \n(a) Imapact of dwith 14 workers24 .030.0\n12 27 .0Per-epoch time (s)1\nThe number of teams d2 63 4R-SAG \nB-SAG \n(b) Imapact of dwith 12 workers\nFig. 14. Impact of dwith different number of workers\nalgorithm is proposed to reduce the latency cost and further\nimprove efﬁciency. And there are two SAG algorithms, i.e.,\nR-SAG and B-SAG. The experimental results are depicted in\nFig. 13 , which prove that both SAG algorithms can accelerate\nSparDL . Speciﬁcally, we set the team number, d, to 1 and 2 for\nR-SAG, and set dto 1, 2, 7, and 14 for B-SAG. Afterward, the\neffectiveness of these algorithms is evaluated via the trai ning\nof VGG-16 on CIFAR-10, utilizing SparDL with either R-\nSAG or B-SAG as the communication method. As shown in\nFig. 13(a), SparDL with R-SAG ( d= 2) complete the training\nprocess slightly faster compared with SparDL without SAG,\ni.e.,SparDL with R-SAG or B-SAG ( d= 1). Besides, from\nFig. 13(b), all SparDL frameworks with B-SAG ( d >1)\nare faster than SparDL without SAG, signifying that B-SAG\naccelerates SparDL . In particular, B-SAG ( d= 7) and B-\nSAG (d= 14 ) can signiﬁcantly improve the speed by 1.25×\nand1.2×, respectively. Additionally, the experimental results\nalso show that SparDL frameworks with SAG have similar\nconvergence rate as SparDL without SAG.\nNext, we observe the inﬂuence of different don the ef-\nﬁciency of SAG with different numbers of workers. For 14\nworkers in Fig. 14(a), the reason why R-SAG has this effect\nonSparDL is that R-SAG with 2 teams marginally decreases\nthe latency cost of SparDL while keeping the bandwidth cost\nunchanged, thereby causing a slight reduction in training t ime.\nBesides, B-SAG greatly improves the speed since it reduces\nnot only the latency cost but also the bandwidth cost of\nSparDL . We can also observe that B-SAG ( d= 14 ) is slower\nthan B-SAG ( d= 7). This is because the upper bound of\nbandwidth increases with the increase of d, and the lower\nbound of bandwidth also increases with the increase of dwhen\nd >√p. Thus, a large deventually weakens the effect of\nimproving the speed compared with the best d(d= 7). For\n12 workers in Fig. 14(b), we can observe that the efﬁciency\nimprovement of R-SAG from 2 to 4 is less than that from\n1 to 2. The reason is that R-SAG ( d= 2) decreases the\nlatency cost of SparDL while keeping the bandwidth cost\n\n19 .029 .0\n10 24 .0Per-epoch time (s)\n1\nTraining epoch 1 R2\nB2 B7 B14 \n4 7\n(a) Training time with 14 workers24 .032 .0\n10 28 .0Per-epoch time (s)\n1\nTraining epoch 1 R2 B2\nB4 B6 B3 B12 R4\n4 7\n(b) Training time with 12 workers\nFig. 15. Training time among different epochs\n0.00 100\n8000 1600050 .0Accuracy (%) \n0\nTraining time (s)k/n=1e-1\nk/n=1e-2\nk/n=1e-3\nk/n=1e-4\nk/n=1e-5\n(a) VGG-16 on CIFAR-100.00 80 .0\n15000 3000040 .0Accuracy (%) \n0\nTraining time (s)k/n=1e-1\nk/n=1e-2\nk/n=1e-3\nk/n=1e-4\nk/n=1e-5\n(b) VGG-19 on CIFAR-100\nFig. 16. SparDL with different kfor sparsiﬁcation\nunchanged, but the R-SAG ( d= 4) reduces the same latency\ncost while increasing the bandwidth cost. As for B-SAG, B-\nSAG (d= 4) is slower than B-SAG ( d= 3). It is because\nB-SAG (d= 4) has the same latency cost and the same lower\nbound of bandwidth cost as B-SAG ( d= 3), and the upper\nbound of bandwidth cost of B-SAG ( d= 4) is higher. In\naddition, SparDL with B-SAG ( d= 14 ) attains the lowest\ntest accuracy, as shown in Fig. 13(b). This arises when d\nequals the number of workers (i.e., P) in the cluster, creating\na scenario where all workers aggregate after just one local t op-\nh, without consideration of global gradients at all. This lea ds\nto the discarding of many globally signiﬁcant gradients but\nlocally less important, thus impacting the convergence rat e.\nG. Time-consuming stability of SparDL in different training\nepochs with different team number d.\nIn this set of experiments, we evaluate the time-consuming\nstability of SparDL in different epochs with different team\nnumbers d. The experimental results are shown in Fig. 15.\nRx,Bxand 1 in the ﬁgure mean R-SAG with d=x, B-SAG\nwithd=xandSparDL without SAG, respectively. From\nthe experimental results, it is evident that the SparDL with\nthe optimal dis steadily faster than SparDL with other d.\nSpeciﬁcally, B-SAG ( d= 7) takes the least time in each of the\nﬁrst ten rounds in Fig. 15(a), and B-SAG ( d= 6) takes the least\ntime in each of the ﬁrst ten rounds in Fig. 15(b). They all have\nthe optimal din their own experimental settings. Therefore,\nusers can choose the optimal dby comparing the training time\nofSparDL with different din the ﬁrst epoch.\nH. Impact of the hyper-parameter kfor sparsiﬁcation.\nIn this set of experiments, we demonstrate the impact\nofkfor sparsiﬁcation on the training efﬁciency and the\nconvergence rate. The experimental results are illustrate d in\nFig. 16. From this ﬁgure, we can observe that the smaller\nthe ratio of kton, i.e., the number of dense gradients,\nthe shorter the time it takes for SparDL to train the sameSparDL- SparDL- SparDL- GRES PRES LRES \n45 .075 .0\n60 .0Accuracy (%) \nTraining epoch 40 160 0 120 80 \n(a) VGG-19, SparDL85 .095 .0\n90 .0Accuracy (%) \nTraining epoch 40 120 0 80 \n(b) VGG-16, SparDL\n85 .095 .0\n90 .0Accuracy (%) \nTraining epoch 40 120 0 80 \n(c) VGG-16, SparDL (R-SAG)85 .095 .0\n90 .0Accuracy (%) \nTraining epoch 120 40 0 80 \n(d) VGG-16, SparDL (B-SAG)\nFig. 17. Convergence on SparDL using different residual collection algo-\nrithms with 14 workers\nnumber of epochs. However, the convergence rate gradually\ndecreases. Speciﬁcally, for VGG-16 and VGG-19, after the\nk/n is reduced from 1e-1 to 1e-2, the training time is greatly\nreduced (0.21 ×and 0.22×), and the accuracy has no obvious\nchange. After k/n is reduced from 1e-2 to 1e-3, the training\ntime is slightly reduced (0.75 ×and 0.70×), and the accuracy\nis slightly reduced. However, after k/n is reduced from 1e-3 to\n1e-4 and 1e-5, the training time has hardly decreased (highe r\nthan 0.95 ×), and the accuracy has decreased signiﬁcantly,\nespecially 1e-5. The reason why the training time is stable a fter\nk/n=1e-3 is that the communication time comes from latency\ncost and bandwidth cost. With the decrease of k, bandwidth\ncost will continue to decrease, but latency cost will remain\nunchanged, so the communication time remains stable after i t\nis reduced to a certain extent. Therefore, the optimal kshould\nbe chosen as k/n=1e-2 or 1e-3 when the communication time\nis low and maintains fast convergence rate.\nI. Impact of different residual collection algorithms.\nIn this set of experiments, we evaluate the effectiveness of\nthe proposed global residual collection algorithm compare d\nto existing residual collection algorithms. The experimen tal\nresults are shown in Fig. 17. It is observed from the re-\nsults that the global residual collection algorithm mainta ins\nthe fast convergence rate of SparDL . Speciﬁcally, we com-\npare the convergence rate of SparDL using three different\nresidual collection algorithms: our global residual colle ction\nalgorithm ( SparDL -GRES), partial residual collection algo-\nrithm [22], [41] ( SparDL -PRES) and local residual collection\nalgorithm [24] ( SparDL -LRES). We utilize these communi-\ncation methods to train VGG-19 on CIFAR-100 and VGG-16\non CIFAR-10. Fig. 17(a) to Fig. 17(d) plot the test accuracy\nw.r.t. the training epoch under four different conditions. It is\nobserved that SparDL -GRES consistently exhibits the most\nsuperior convergence rate, with the accuracy of SparDL -\n\nCommunication cost Computation cost \n0.00 0.10 \n0.05 Per-update time (s)\nTop kDSA Top kA Ok-Top kSparDL \n(a) VGG-19 on CIFAR-1000.00 0.32 \nOk-Top kSparDL 0.16 Per-update time (s)\n(b) BERT on Wikipedia\nFig. 18. Per-update time using RDMA network with 5 workers\nGRES consistently surpassing others after the 80th epoch\nwhen the learning rate is reduced.\nThe reason for the distinction of SparDL -GRES is that\nthe global residual collection algorithm accumulates all d is-\ncarded gradients throughout the training process within th e\ncluster, thereby maintaining the convergence rate. In con-\ntrast, SparDL -PRES and SparDL -LRES with partial residual\ncollection algorithm and local residual collection algori thm\noverlook in-procedure residuals. However, such residuals , gen-\nerated in substantial quantities within SparDL , play a crucial\nrole in the training process. Neglecting to accumulate thes e\nresiduals slows down the convergence rate of the training\nmodel. In conclusion, the global residual collection algor ithm\nenables SparDL to preserve fast convergence rate when train-\ning different models with or without SAG methods.\nJ. Performance with RDMA network.\nTo further evaluate the efﬁciency of SparDL with higher\nnetwork bandwidth, we compared SparDL with baselines in\na different GPU cluster on a small dataset CIFAR-100 using\nVGG-19 and a large dataset Wikipedia using BERT. There are\nﬁve GPU machines in the cluster. Each machine is equipped\nwith one NVIDIA A800 GPU, and all machines are connected\nto an InﬁniBand network with RDMA service. The results are\nillustrated in Fig. 18, which veriﬁes the superiority of our\nproposed SparDL even in the RDMA network. Fig. 18(a) and\nFig. 18(b) depict the per-update time. For VGG-19, we can ob-\nserve that SparDL achieves4.0×,3.4×and3.0×acceleration\nof communication cost compared to the baselines, respectiv ely.\nFor BERT, SparDL is4.2×faster than O k-Topk, i.e., the most\nefﬁcient baseline, in terms of communication cost. Therefo re,\nSparDL demonstrates efﬁciency when training deep learning\nmodels with high bandwidth networks.\nV. R ELATED WORK\nAll-Reduce operation [6], [8], [11], [12], [27], [39] is\ncommonly used in data parallelism [17], [21], [31], [35], [3 6],\n[47] distributed deep learning. Nonetheless, existing efﬁ cient\nAll-Reduce methods [5], [16], [29] are primarily designed f or\ndense gradients, and are inefﬁcient for synchronization wi th\nsparse gradients because of the sparse gradient accumulati on\n(SGA) dilemma [22], [34], [41]. Several sparse All-Reduce\nmethods [22], [34], [41], [42] have been proposed to address\nthe SGA dilemma. But they still suffer from low efﬁciency.\nTherefore, we aim to solve the SGA dilemma more efﬁciently\nto accelerate communication with sparse gradients.VI. D ISCUSSION\nLimitations and Future Work. (i)Heterogeneous environ-\nment. SparDL tries to accelerate All-Reduce, which is mainly\nused in homogeneous environments. However, the heteroge-\nneous environment also appears in real-world clusters, and\nthere are some variants of All-Reduce proposed recently for\nthis environment. In the future, we can extend SparDL to this\nenvironments. (ii) Combining with quantization methods. Spar-\nsiﬁcation and quantization are both common communication\ncompression techniques. SparDL studies the sparsiﬁcation and\ntries to solve the SGA dilemma efﬁciently. In the future,\nwe may want to extend SparDL by combining quantization\nmethods to further accelerate the communication.\nPractical Implications and Potential Applications. Training\ndeep learning models is usually time-consuming. Thus, it is\nimportant to use distributed deep learning (DDL) for efﬁcie nt\ntraining. SparDL aims at accelerating the communication\nin DDL to further reduce the training time. And SparDL\ncan be used in CV , NLP and other deep learning tasks. In\naddition, a variety of large models have been proposed recen tly\nand these models are often trained by DDL. Because of the\nmassive parameters of these models, it will lead to plenty of\ncommunication consumption. SparDL can be used in DDL\nand speed up the training of these large models by reducing th e\ncommunication volume and efﬁcient sparse communication.\nRelationship between SparDL and FSDP. SparDL is or-\nthogonal to FSDP [49] or ZeRO-3 [33]. The main purpose\nof FSDP and ZeRO-3 is to reduce the memory cost, so that\nlarger models can be trained more easily. In these framework s,\nthe gradients are also synchronized by All-Reduce or Reduce -\nScatter operations. Since these works communicate with den se\ngradients, SparDL is able to accelerate the communication\nof All-Reduce or Reduce-Scatter by using efﬁcient sparse\ncommunication. Therefore, our SparDL can be combined into\nFSDP or ZeRO-3 frameworks.\nVII. C ONCLUSIONS\nIn this paper, we analyze the low efﬁciency of existing\nsparse All-Reduce frameworks and propose SparDL to tackle\nthese problems. For the ﬁrst time, SparDL combines multiple\nselection processes and the Reduce-Scatter operation to de al\nwith the Sparse Gradient Accumulation (SGA) dilemma. Be-\nsides, the SparDL uses the global residual collection algorithm\nto collect all discarded gradients in the cluster, ensuring\nfast training convergence. In addition, the Spar-All-Gath er\nalgorithm further improves the communication efﬁciency of\nSparDL and makes the ratio of latency and bandwidth cost\nadjustable. After conducting experiments on a wide range of\ncommon deep learning tasks, we observe that our SparDL\nachieves up to 4.9 ×speedup compared to the state-of-the-art\nmethods while maintaining comparable effectiveness.\nACKNOWLEDGMENT\nThis was supported in part by the NSFC under Grants\nNo. (62302436, U23A20296, 62025206), Ningbo Science\nand Technology Special Projects under Grant No. 2023Z212.\nYuren Mao is the corresponding author of the work.\n\nREFERENCES\n[1] E. H. Ahmed and M. Moustafa. House price estimation from v isual and\ntextual features. In IJCCI , pages 62–68, 2016.\n[2] A. F. Aji and K. Heaﬁeld. Sparse communication for distri buted gradient\ndescent. In EMNLP , pages 440–445, 2017.\n[3] D. Alistarh, T. Hoeﬂer, M. Johansson, N. Konstantinov, S . Khirirat, and\nC. Renggli. The convergence of sparsiﬁed gradient methods. InNeurIPS ,\npages 5977–5987, 2018.\n[4] J. Bruck, C. Ho, S. Kipnis, E. Upfal, and D. Weathersby. Ef ﬁcient\nalgorithms for all-to-all communications in multiport mes sage-passing\nsystems. TPDS , 8(11):1143–1156, 1997.\n[5] E. Chan, M. Heimlich, A. Purkayastha, and R. A. van de Geij n.\nCollective communication: Theory, practice, and experien ce.Concurr.\nComput. Pract. Exp. , 19(13):1749–1783, 2007.\n[6] C. Chu, X. Lu, A. A. Awan, H. Subramoni, J. M. Hashmi, B. Elt on, and\nD. K. Panda. Efﬁcient and scalable multi-source streaming b roadcast\non GPU clusters for deep learning. In ICPP , pages 161–170, 2017.\n[7] J. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: pre-t raining of\ndeep bidirectional transformers for language understandi ng. In NAACL-\nHLT, pages 4171–4186, 2019.\n[8] J. Dong, Z. Cao, T. Zhang, J. Ye, S. Wang, F. Feng, L. Zhao, X . Liu,\nL. Song, L. Peng, Y . Guo, X. Jiang, L. Tang, Y . Du, Y . Zhang, P. P an, and\nY . Xie. EFLOPS: algorithm and system co-design for a high per formance\ndistributed training platform. In HPCA , pages 610–622, 2020.\n[9] N. Dukkipati, T. Reﬁce, Y . Cheng, J. Chu, T. Herbert, A. Ag arwal,\nA. Jain, and N. Sutin. An argument for increasing tcp’s initi al congestion\nwindow. Comput. Commun. Rev. , 40(3):26–33, 2010.\n[10] A. Fard, A. Le, G. Larionov, W. Dhillon, and C. Bear. Vert ica-ml:\nDistributed machine learning in vertica database. In SIGMOD , pages\n755–768, 2020.\n[11] Y . Guo, Z. Zhang, J. Jiang, W. Wu, C. Zhang, B. Cui, and J. L i. Model\naveraging in distributed machine learning: A case study wit h apache\nspark. VLDB J. , 30(4):693–712, 2021.\n[12] I. Hakimi, R. Z. Aviv, K. Y . Levy, and A. Schuster. LAGA: l agged\nallreduce with gradient accumulation for minimal idle time . In ICDM ,\npages 171–180, 2021.\n[13] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learnin g for image\nrecognition. In CVPR , pages 770–778, 2016.\n[14] S. Hochreiter and J. Schmidhuber. Long short-term memo ry.Neural\ncomputation , 9(8):1735–1780, 1997.\n[15] R. W. Hockney. The communication challenge for MPP: int el paragon\nand meiko CS-2. Parallel Comput. , 20(3):389–398, 1994.\n[16] T. Hoeﬂer, W. Gropp, R. Thakur, and J. L. Tr¨ aff. Toward p erformance\nmodels of MPI implementations for understanding applicati on scaling\nissues. In EuroMPI , volume 6305, pages 21–30, 2010.\n[17] Y . Huang, T. Jin, Y . Wu, Z. Cai, X. Yan, F. Yang, J. Li, Y . Gu o,\nand J. Cheng. Flexps: Flexible parallelism control in param eter server\narchitecture. PVLDB , 11(5):566–579, 2018.\n[18] J. Jiang, F. Fu, T. Yang, and B. Cui. Sketchml: Accelerat ing distributed\nmachine learning with data sketches. In SIGMOD , pages 1269–1284,\n2018.\n[19] J. Jiang, F. Fu, T. Yang, Y . Shao, and B. Cui. Skcompress: Compressing\nsparse and nonuniform gradient in distributed machine lear ning. VLDB\nJ., 29(5):945–972, 2020.\n[20] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features\nfrom tiny images . Citeseer, 2009.\n[21] M. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed, V . Josifovski,\nJ. Long, E. J. Shekita, and B. Su. Scaling distributed machin e learning\nwith the parameter server. In OSDI , pages 583–598, 2014.\n[22] S. Li and T. Hoeﬂer. Near-optimal sparse allreduce for d istributed deep\nlearning. In PPoPP , pages 135–149, 2022.\n[23] S. Li, Y . Zhao, R. Varma, O. Salpekar, P. Noordhuis, T. Li , A. Paszke,\nJ. Smith, B. Vaughan, P. Damania, and S. Chintala. Pytorch di stributed:\nExperiences on accelerating data parallel training. PVLDB , 13(12):3005–\n3018, 2020.\n[24] Y . Lin, S. Han, H. Mao, Y . Wang, and B. Dally. Deep gradien t\ncompression: Reducing the communication bandwidth for dis tributed\ntraining. In ICLR , 2018.\n[25] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y . Ng, and C. Potts.\nLearning word vectors for sentiment analysis. In ACL, pages 142–150,\n2011.[26] M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. Bui lding a large\nannotated corpus of english: The penn treebank. Comput. Linguistics ,\n19(2):313–330, 1993.\n[27] X. Miao, X. Nie, Y . Shao, Z. Yang, J. Jiang, L. Ma, and B. Cu i.\nHeterogeneity-aware distributed machine learning traini ng via partial\nreduce. In SIGMOD , pages 2262–2270, 2021.\n[28] X. Miao, H. Zhang, Y . Shi, X. Nie, Z. Yang, Y . Tao, and B. Cu i. HET:\nscaling out huge embedding model training via cache-enable d distributed\nframework. PVLDB , 15(2):312–320, 2021.\n[29] H. Mikami, H. Suganuma, Y . Tanaka, Y . Kageyama, et al. Ma s-\nsively distributed sgd: Imagenet/resnet-50 training in a ﬂ ash. CoRR ,\nabs/1811.05233, 2018.\n[30] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. C hanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A . K¨ opf,\nE. Z. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,\nL. Fang, J. Bai, and S. Chintala. Pytorch: An imperative styl e, high-\nperformance deep learning library. In NeurIPS , pages 8024–8035, 2019.\n[31] Y . Peng, Y . Zhu, Y . Chen, Y . Bao, B. Yi, C. Lan, C. Wu, and\nC. Guo. A generic communication scheduler for distributed D NN\ntraining acceleration. In SOSP , pages 16–29, 2019.\n[32] J. Pjesivac-Grbovic, T. Angskun, G. Bosilca, G. E. Fagg , E. Gabriel,\nand J. J. Dongarra. Performance analysis of MPI collective o perations.\nInIPDPS , 2005.\n[33] S. Rajbhandari, J. Rasley, O. Ruwase, and Y . He. Zero: me mory\noptimizations toward training trillion parameter models. InSC, page 20.\nIEEE/ACM, 2020.\n[34] C. Renggli, S. Ashkboos, M. Aghagolzadeh, D. Alistarh, and T. Hoeﬂer.\nSparcml: high-performance sparse communication for machi ne learning.\nInSC, pages 11:1–11:15, 2019.\n[35] A. Renz-Wieland, R. Gemulla, Z. Kaoudi, and V . Markl. Nu ps: A\nparameter server for machine learning with non-uniform par ameter\naccess. In SIGMOD , pages 481–495, 2022.\n[36] A. Renz-Wieland, R. Gemulla, S. Zeuch, and V . Markl. Dyn amic\nparameter allocation in parameter servers. PVLDB , 13(11):1877–1890,\n2020.\n[37] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S . Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. S. Bernstein, A. C. Berg, and\nF. Li. Imagenet large scale visual recognition challenge. Int. J. Comput.\nVis., 115(3):211–252, 2015.\n[38] S. Sarvotham, R. H. Riedi, and R. G. Baraniuk. Connectio n-level\nanalysis and modeling of network trafﬁc. In SIGCOMM IMW , pages\n99–103, 2001.\n[39] D. D. Sensi, S. D. Girolamo, S. Ashkboos, S. Li, and T. Hoe ﬂer. Flare:\nFlexible in-network allreduce. In SC, page 35, 2021.\n[40] S. Shi, X. Chu, and B. Li. MG-WFBP: efﬁcient data communi cation\nfor distributed synchronous SGD algorithms. In INFOCOM , pages 172–\n180, 2019.\n[41] S. Shi, Q. Wang, K. Zhao, Z. Tang, Y . Wang, X. Huang, and X. Chu. A\ndistributed dynchronous SGD algorithm with global top-k sp arsiﬁcation\nfor low bandwidth networks. In ICDCS , pages 2238–2247, 2019.\n[42] S. Shi, K. Zhao, Q. Wang, Z. Tang, and X. Chu. A convergenc e analysis\nof distributed SGD with communication-efﬁcient gradient s parsiﬁcation.\nInIJCAI , pages 3411–3417, 2019.\n[43] K. Simonyan and A. Zisserman. Very deep convolutional n etworks for\nlarge-scale image recognition. In ICLR , 2015.\n[44] S. U. Stich, J. Cordonnier, and M. Jaggi. Sparsiﬁed SGD w ith memory.\nInNeurIPS , pages 4452–4463, 2018.\n[45] R. Thakur, R. Rabenseifner, and W. Gropp. Optimization of collective\ncommunication operations in MPICH. IJHPCA , 19(1):49–66, 2005.\n[46] Z. Xue, R. Li, H. Zhang, X. Gu, and Z. Xu. Dc-top-k: A novel top-\nk selecting algorithm and its parallelization. In ICPP , pages 370–379,\n2016.\n[47] S. Zhang, A. Choromanska, and Y . LeCun. Deep learning wi th elastic\naveraging SGD. In NeurIPS , pages 685–693, 2015.\n[48] Z. Zhang, B. Cui, Y . Shao, L. Yu, J. Jiang, and X. Miao. PS2 : parameter\nserver on spark. In SIGMOD , pages 376–388, 2019.\n[49] Y . Zhao, A. Gu, R. Varma, L. Luo, C. Huang, M. Xu, L. Wright , H. Sho-\njanazeri, M. Ott, S. Shleifer, A. Desmaison, C. Balioglu, P. Damania,\nB. Nguyen, G. Chauhan, Y . Hao, A. Mathews, and S. Li. Pytorch F SDP:\nexperiences on scaling fully sharded data parallel. PVLDB , 16(12):3848–\n3860, 2023.\n[50] P. Zhou, Q. Lin, D. Loghin, B. C. Ooi, Y . Wu, and H. Yu.\nCommunication-efﬁcient decentralized machine learning o ver heteroge-\nneous networks. In ICDE , pages 384–395, 2021.",
  "textLength": 73451
}