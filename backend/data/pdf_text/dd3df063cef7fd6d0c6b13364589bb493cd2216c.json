{
  "paperId": "dd3df063cef7fd6d0c6b13364589bb493cd2216c",
  "title": "A New Paradigm in Tuning Learned Indexes: A Reinforcement Learning Enhanced Approach",
  "pdfPath": "dd3df063cef7fd6d0c6b13364589bb493cd2216c.pdf",
  "text": "A New Paradigm in Tuning Learned Indexes: A Reinforcement\nLearning Enhanced Approach\nTaiyi Wang\nUniversity of Cambridge\nCambridge, United Kingdom\nTaiyi.Wang@cl.cam.ac.ukLiang Liang‚àó\nEPFL\nLausanne, Switzerland\nliang.liang@epfl.chGuang Yang\nImperial College London\nLondon, United Kingdom\nguang.yang15@imperial.ac.uk\nThomas Heinis\nImperial College London\nLondon, United Kingdom\nt.heinis@imperial.ac.ukEiko Yoneki‚Ä†\nUniversity of Cambridge\nCambridge, United Kingdom\neiko.yoneki@cl.cam.ac.uk\nABSTRACT\nLearned Index Structures (LIS) have significantly advanced data\nmanagement by leveraging machine learning models to optimize\ndata indexing. However, designing these structures often involves\ncritical trade-offs, making it challenging for both designers and\nend-users to find an optimal balance tailored to specific workloads\nand scenarios. While some indexes offer adjustable parameters that\ndemand intensive manual tuning, others rely on fixed configura-\ntions based on heuristic auto-tuners or expert knowledge, which\nmay not consistently deliver optimal performance.\nThis paper introduces LITune , a novel framework for end-to-\nend automatic tuning of Learned Index Structures. LITune employs\nan adaptive training pipeline equipped with a tailor-made Deep\nReinforcement Learning (DRL) approach to ensure stable and ef-\nficient tuning. To accommodate long-term dynamics arising from\nonline tuning, we further enhance LITune with an on-the-fly up-\ndating mechanism termed the O2 system. These innovations allow\nLITune to effectively capture state transitions in online tuning\nscenarios and dynamically adjust to changing data distributions\nand workloads, marking a significant improvement over other tun-\ning methods. Our experimental results demonstrate that LITune\nachieves up to a 98% reduction in runtime and a 17-fold increase\nin throughput compared to default parameter settings given a se-\nlected Learned Index instance. These findings highlight LITune ‚Äôs\neffectiveness and its potential to facilitate broader adoption of LIS\nin real-world applications.\nKEYWORDS\nLearned Index, Reinforcement Learning, Parameter Tuning\nACM Reference Format:\nTaiyi Wang, Liang Liang, Guang Yang, Thomas Heinis, and Eiko Yoneki.\n2025. A New Paradigm in Tuning Learned Indexes: A Reinforcement Learn-\ning Enhanced Approach. In SIGMOD ‚Äô25: the International Conference on\nManagement of Data, June 22-27, 2025, Berlin, Germany. ACM, New York,\nNY, USA, 15 pages.\n‚àóWork has been partly performed at Imperial College London, London, UK.\n‚Ä†Corresponding author.\nSIGMOD ‚Äô25, June 22-27, 2025, Berlin, Germany\n¬©2025 Association for Computing Machinery.\nThis is the author‚Äôs version of the work. It is posted here for your personal use. Not\nfor redistribution. The definitive Version of Record will be published in SIGMOD ‚Äô25:\nthe International Conference on Management of Data, June 22-27, 2025, Berlin, Germany .1 INTRODUCTION\nThe intersection of data management and machine learning has\ngiven rise to learned index structures. These indexes integrate ma-\nchine learning, replacing traditional algorithmic components, to\ncapture data distributions and optimize search times. Notable exam-\nples include RMI [ 21], ALEX [ 10] and PGM [ 11], etc., which have\nbecome subjects of extensive research.\nThe effective design of a learned index involves deliberate trade-\noffs to achieve optimal performance for varying workloads. For\ninstance, ALEX favors combined search and update performance\nby introducing gaps at the expense of space efficiency [ 10]. On the\nother hand, the dynamic PGM Index prioritizes update efficiency\nover search performance [ 11]. These design trade-offs also lead to\nmore complex structures which generate configurable parameters.\nTuning these parameters is the key to balancing the trade-offs that\nensure higher performance over traditional indexes.\nBeyond the primary parameters, learned indexes like ALEX have\nmore subtle tunable factors that are often overlooked for simplicity.\nThese parameters affect various aspects of the index performance,\nfrom operation cost (e.g., search and insertion cost) to the structure\nof the index (e.g., heights of the tree). For example, for ALEX, the\nMax Node Size parameter changes the size of the nodes, thereby\naffecting the height of the tree. On the other hand, Split Policy and\nGap Ratio affect how insertion is carried out. These parameters are\nintertwined, and adjusting them in real-world scenarios can lead to\nsubstantial performance improvements, though it requires a more\ncomplex tuning process.\nSelecting the right tuning approach for learned indexes involves\nnavigating a myriad of parameter configurations [ 16,41]. For exam-\nple, in practice, parameterized indexes can exhibit vastly different\nperformance due to parameter choices. This is illustrated in Fig-\nure 1(a), where adjusting just two parameters leads to significant\nvariability in runtime1. This variability underscores the complexi-\nties and potential performance swings when considering the full\nspectrum of high-dimensional and continuous parameter configura-\ntions, which can scale into thousands or millions. This complexity is\ncompounded by the fact that a misconfiguration can lead to drastic\nperformance deviations, emphasizing the importance of precise\n1In our experiments shown in Figure 1(a) and (d), we executed 16 million write and\n16 million read queries on a 1 million SOSD dataset [ 18] using ALEX [ 10], selectively\nvarying two parameters to validate the substantial performance gap among parameters.arXiv:2502.05001v2  [cs.DB]  18 Feb 2025\n\nSIGMOD ‚Äô25, June 22-27, 2025, Berlin, Germany Taiyi Wang, Liang Liang, Guang Yang, Thomas Heinis, and Eiko Yoneki\n(a)(b)(c)(b)(a)(d)LITune‚Äòs performance gains over default experts‚Äôsettings\n(s)\nFigure 1: (a) shows the performance surface of a learned index (ALEX) under a wild exploration of the parameter space. (b)\nhighlights the optimal performance speedup achieved by LITune compared to default expert-selected parameters. (c) illustrates\nthe continuous tuning performance of our system alongside other out-of-the-box methods under default configurations. (d)\ncompares the tuning stability and costs across methods to reach their respective optimal performance levels.\ntuning. Besides, our empirical experiments demonstrate that pa-\nrameter interactions in learned indexes exhibit complex, workload-\ndependent relationships with no dominant parameters. As shown\nin Figure 2, where colors represent normalized parameter values\nand percentages show impact scores (ratio of individual-parameter\nto full-parameter tuning improvements), no parameter consistently\nexerts greater influence, with all impact scores falling between 10-\n25%. This heterogeneous distribution of parameter values across\nworkloads, coupled with the balanced impact scores, indicates that\nperformance optimization requires holistic parameter tuning rather\nthan focusing on individual parameters. Moreover, unlike algo-\nrithmic indexes such as B+trees that perform well out of the box,\nlearned indexes are distribution-dependent. Furthermore, systems\nare not designed to automatically tune indexes and typically do not\nallocate substantial resources for this purpose. However, there is a\ncritical requirement for indexes to perform optimally; this necessi-\ntates that the tuner identifies high-quality solutions within\na limited budget (Challenge C.1 ).\nexpectedInsertFrackMinDensity\nkDensityinterval_med kDensityinterval_high\nkAppendMostlyThresholdmaxNodeSize_factor\nkMinOutOfDomainKeys\nkOutOfDomainToleranceFactorkinterval\napproximateCostComputationapproximateModelComputationMIX-balanced\nMIX-read_heavy\nMIX-write_heavy\nOSM-balanced\nOSM-read_heavy\nOSM-write_heavy\nFB-balanced\nFB-read_heavy\nFB-write_heavy15.6% 24.3% 21.0% 19.0% 12.3% 12.3% 10.9% 23.0% 19.0% 20.6% 10.3%\n24.5% 22.5% 13.2% 12.7% 12.8% 14.6% 17.9% 16.5% 14.4% 19.2% 12.1%\n14.4% 15.5% 16.8% 21.8% 13.0% 17.7% 18.9% 10.7% 19.1% 12.6% 11.0%\n24.2% 24.5% 22.1% 14.6% 11.5% 20.3% 16.6% 11.8% 17.4% 10.5% 23.6%\n13.9% 19.9% 14.7% 17.8% 18.2% 12.8% 24.5% 21.6% 24.1% 23.4% 19.0%\n23.8% 11.3% 12.9% 10.7% 14.9% 15.8% 14.1% 22.4% 15.4% 14.2% 18.1%\n12.1% 22.0% 11.1% 24.8% 21.6% 13.0% 10.1% 22.2% 20.6% 20.9% 21.6%\n11.1% 15.4% 11.7% 22.9% 19.3% 15.0% 11.0% 14.7% 14.9% 20.9% 19.6%\n23.3% 17.1% 11.8% 20.7% 21.4% 18.4% 21.6% 17.4% 17.8% 16.4% 10.4%\n0.00.20.40.60.81.0Normalized Parameter Value\nFigure 2: Selected parameter value distributions and their\nimpact scores across different workloads when tuning on\nALEX. The heatmap colors represent normalized optimal\nparameter values, while the percentages indicate each pa-\nrameter‚Äôs individual tuning impact.\nWhile learned indexes strive to be user-friendly by abstracting\naway underlying parameters, this abstraction can inadvertently\nlead to significant performance degradation, as hiding these critical\nparameters from the user may result in suboptimal default settings\nfor various scenarios. By fine-tuning these parameters, we canachieve significant performance improvements. Figure 1(b) empha-\nsizes the substantial performance gains (measured by query runtime\nspeedup) achieved by our tuning system over SOSD [ 18] through\ncomparisons between the optimal solution found by LITune and\nthe default settings provided by experts. Importantly, unlike many\nDBMSs whose default configurations are often overly conserva-\ntive [ 40,49], the default settings in learned indexes are decided\nbased on instance-optimized designs [ 9] which allow them to adapt\nto specific system environments given a \"good\" tuning algorithm.\nCurrently, these algorithms are chosen by experts to optimize for\ncommon system environments. However, there hasn‚Äôt been any\nend-to-end tuning system that is adaptive across data distributions.\nThus, tuning is necessary and can lead to considerable enhance-\nments.\nMoreover, in real-world usage, data distributions and query types\nare not constant, which makes tuning more challenging. Figure 1(c)\nshows the degradation of tuning performance among existing out-\nof-box tuning methods during continuous online tuning when fac-\ning dynamic workloads, highlighting the need for adaptive tun-\ning to workloads and data distributions (Challenge C.2 ). We in-\ntroduce four different workloads derived from mixture-distributed\ndata from the SOSD dataset [ 18], involving various query types by\nadjusting the read-write ratio over time. To ensure fair comparisons,\neach tuning method is provided with a preparation period, depicted\nas grey areas within the workload intervals. During this period,\na preliminary smaller dataset reflecting future trends is used for\nwarming up, initial tuning will happen during this period to make\nsure the system begins with reasonably optimized parameters.\nAnother challenge arises from the need for safe tuning, especially\nwhen dealing with a large parameter space and concurrent tuning\ndemands. Recently, Reinforcement Learning enhanced by deep neu-\nral networks (DRL) has already been proved by many works [ 15,49]\nas a good tuner when working within a large parameter space due\nto its intrinsic exploration abilities. Equipped with a learned module,\nan RL-based approach can be easily generalized and deployed to\nvarious data conditions. However, DRL, as a trial-and-error-based\napproach, normally presents aggressive tuning towards the optimal\nsolutions, whose potential risks to the existing system were largely\nignored. In such cases, it is crucial to ensure tuning remains\nboth safe and stable during exploration within the extensive\nparameter space (Challenge C.3 ). Figure 1(d) shows the failure\nrates caused by improper parameter settings from aggressive tuning\n\nA New Paradigm in Tuning Learned Indexes: A Reinforcement Learning Enhanced Approach SIGMOD ‚Äô25, June 22-27, 2025, Berlin, Germany\napproaches, particularly those resulting from exploratory vanilla\nDRL methods (PPO [ 31]). We also present the differences in tuning\ncosts until each method reaches optimality, further emphasizing the\nmotivation for introducing LITune . These observations underscore\nthe necessity of a tailor-made tuning system rather than relying on\nout-of-box methods.\nTheLITune system, utilizing a tailored Deep Reinforcement\nLearning (DRL) framework, significantly improves the tuning of\nlearned index parameters, overcoming the constraints of traditional\nmethods and optimizing performance without requiring the exten-\nsive training data needed by supervised models. Key contributions\ninclude:\n(1)Introduction of an Automatic Tuning System Using\nDRL (Addressing C.1): This system efficiently navigates large\nparameter spaces in real-time, dynamically adjusting to changing\nperformance requirements, capturing the stateful transitions, thus\nrapidly finding optimal solutions in complex online environments.\n(2)Adaptive Design (Addressing C.2): During the training\nstage, we leverage Meta-RL to efficiently transfer knowledge from\na smaller pre-training set to unseen tuning tasks through solid\ninitialization and fast gradient descent. In the online tuning stage,\nLITune employs an on-the-fly updating system, termed the O2\nsystem, to further boost adaptability over the longer term. This\ndesign enhances the tuner‚Äôs ability to quickly adapt to new or\nevolving workloads and data distributions.\n(3)Operational Stability and Reliability (Addressing C.3):\nFeaturing a Context-RL-based risk mitigation strategy (ET-MDP\nsolver), LITune avoids dangerous configurations, ensuring the tun-\ning process maintains the system‚Äôs operational stability and relia-\nbility.\nTo our best knowledge, LITune is the first system to en-\nable stateful, online tuning of learned indexes using tailored\nDeep Reinforcement Learning methods, integrated with safety-\naware mechanisms and O2 system to ensure reliable and adap-\ntive performance.\nThe paper is organized as follows: Section 2 reviews related work.\nSection 3 discusses our motivation and the design of LITune . Sec-\ntion 4 details our novel RL-based tuning methodologies. Section 5\npresents our experimental analysis and results. Finally, Section 6\nconcludes with a summary and discussion.\n2 RELATED WORKS\n2.1 Parameter Tuning for System\nParameter tuning is a common practice to optimize systems. For\nexample, knob tuning in database systems, where specific values or\nknobs can be adjusted to optimize for specific query operators and\nimprove data access efficiency [ 50]. Traditional search strategies\ninclude using random and grid search to find the optimal parame-\nters for a given workload. Advanced frameworks such as GPTune\n[25], Spearmint [ 33], and Sequential Model-Based Optimization\n(SMBO) [ 29] attempt to refine this process through Bayesian Op-\ntimization, integrating multi-task and transfer learning. However,\nthese methods struggle with accuracy and computational efficiency,\nas they require starting anew with each shift in workload or datadistribution. The inefficiencies amplify when faced with real-time\ntuning requirements for learned indexes [12].\n2.2 Deep Reinforcement Learning for\nParameter Tuning\nRecently, Deep reinforcement learning (DRL) has shown promis-\ning results in optimizing complex systems under dynamic condi-\ntions [ 6,15]. Notably, CDBTune [ 49] uses deep deterministic policy\ngradients (DDPG [ 24]) to automatically navigate and tune the high-\ndimensional continuous space of database configurations. It has\nshown the adaptability and efficiency of DRL methods over tra-\nditional tuning tools and expert DBA interventions for handling\ndynamic conditions. However, CDBTune cannot easily adapt to the\nunique challenges in index tuning, not present database configura-\ntion tuning. Index tuning requires rapid and precise adjustments\nwithout system resets or prolonged downtime while answering\nqueries and updating records within dynamic data distributions\nand workload patterns. Misconfiguration has severe consequences\nfor performance and must be avoided. LITune , on the other hand, ef-\nficiently tackles these challenges while keeping the full advantages\nof DRL tuning.\n2.3 Learned Index Tuning\nLearned indexes [ 21] replace traditional indexing algorithms (like\nB+Trees) with models that predict the approximate location of a\nkey using the Empirical Cumulative Distribution Function (CDF),\npotentially reducing search operations. Research on both static\n[11,19,21,34] and updatable indexes [ 10,11,14,22,23,38,44,\n46,48] demonstrates that performance heavily depends on data\ndistribution, and to achieve the best performance, the indexes must\nbe \"tuned\" according to the data distribution.\nCurrently, there are two approaches for considering data distri-\nbutions when designing learned indexes: (1) exposing parameters\nfor adjustment by users or future research, as seen in [ 11,21,48],\nand (2) implementing self-tuning mechanisms through cost models,\nutilized by indexes such as [ 10,23,46]. In both approaches, the de-\nfault parameter settings are crucial, with index designers asserting\nthat tuning is unnecessary for satisfactory performance. However,\nthis assumption only holds when the empirical cost of the default\nparameters is effective [ 22,26,38], and for updatable learned in-\ndexes, the insert cost model must also be valid [ 10,23]. Early studies\nlike [ 36] use grid search to find optimal default parameters but fail\nto capture the complexities of tuning learned indexes. Automating\nthis tuning process remains under-explored and is fundamental\nto our work with LITune . This challenge is exacerbated for in-\ndexes supporting dynamic workloads, which must be reorganized\nto maintain model accuracy as data distributions shift. Unlike tradi-\ntional indexes, learned index parameters depend not only on dataset\nsize but also on rapidly changing data distributions. Additionally,\nupdatable indexes require structural modifications‚Äîsuch as gaps\n[10,23], hierarchical structures [ 11,45], and buffers [ 14,23]‚Äîto\nmitigate the impact of distribution shifts. These complexities create\nintricate dependencies between parameters (as shown in Figure\n1(a)), making automated tuning particularly challenging.\nCDFShop [ 27] automates learned index optimization by fine-\ntuning cumulative distribution function (CDF) models specifically\n\nSIGMOD ‚Äô25, June 22-27, 2025, Berlin, Germany Taiyi Wang, Liang Liang, Guang Yang, Thomas Heinis, and Eiko Yoneki\nMethod Parameter Selection/Tuning Method\nB-trees [7] Expert selection, Heuristics\nRMI [21] Expert selection, Heuristics\nALEX [10] Heuristic cost model, Grid search\nSWIX [23] Heuristic cost model, Grid search\nCARMI [48] Expert selection, Hardware-aware\nCDFShop [27] Heuristics; Pareto front\nAirIndex [4] Heuristics, Graph-based search\nRusKey [28] Vanilla DRL (DDPG)\nOurs Safe RL approach\nTable 1: Summary of Existing Indexing and Tuning Works\nfor the RMI index, adjusting high-level hyperparameters (e.g., model\ntype, branching factors). However, it is confined to RMI, lacks safety-\naware mechanisms, and relies on iterative, game-theoretic, and\nheuristic exploration methods that can be time-consuming and less\nscalable for complex indexes or larger datasets. Consequently, we\nexclude CDFShop from our baselines, comparing LITune instead\nwith more general Bayesian Optimization and heuristic-based meth-\nods. AirIndex [ 4] employs a graph-based approach to automatically\ntune learned index structures but often has limited exploration\nby focusing on top- ùêæheuristics, incurring high overhead despite\nparallelization and complicating optimization in high-dimensional\nparameter spaces. Recently, Reinforcement Learning (RL) has been\napplied to tuning learned indexes: RusKey [ 28] optimizes LSM-tree-\nbased key-value stores [ 37], marking the first RL-driven LSM-tree\ntransformations under dynamic workloads. Unlike white-box cost\nmodels centered on I/O complexities, RusKey‚Äôs black-box RL ap-\nproach offers a holistic view similar to LITune [8], yet it tunes only\na single parameter and thus cannot be directly used for general\nlearned index tuning.\nTo clarify our selection criteria and highlight the key tuning\nmethods utilized in prior research, we have summarized relevant\nstudies in Table 1. We observe that irrespective of the specific index\ntype being targeted, the underlying tuning methods are generally\ntransferable across different contexts. Consequently, we have ex-\ntracted these transferable methods and adopted them as baselines\nfor our experiments. This approach allows us to systematically\nevaluate the effectiveness of our proposed tuning system against\nestablished methodologies.\n2.4 Index Advisor\nIndex advisors are tools used in physical database design to help\nadministrators select optimal indexes at the schema level based\non query workloads [ 3,39]. They focus on recommending which\ncolumns or combinations of columns should be indexed to im-\nprove query execution times and overall system efficiency. Recent\nRL-based index selection methods [ 20,32,43,51] continue this ap-\nproach by automating the index selection process but still operate at\nthe schema level. In contrast, our work operates on a fundamentally\ndifferent level by tuning the internal parameters of learned index\nstructures themselves rather than selecting which indexes to create.\nLearned indexes leverage machine learning models to predict data\npositions within a dataset [ 21], and their performance heavily de-\npends on hyperparameters and model configurations. Traditionalindex advisors do not address the challenges associated with opti-\nmizing these internal parameters. Therefore, our DRL-based tuning\nframework enhances the performance of learned indexes through\ndynamic internal optimization, offering adaptability and efficiency\nimprovements that traditional index advising methods do not pro-\nvide. This distinction underscores that we are addressing a different\nproblem space, focusing on the internal mechanics of learned in-\ndexes rather than on schema-level index selection.\n3LITUNE SYSTEM\n3.1 Motivation\nUnlike existing index tuning works that concentrate on a limited set\nof observable parameters directly reflected in data structures, our\nwork tackles the more complex challenge of tuning within a vast\nparameter space where parameters interact in non-independent\nand intertwined ways. This complexity demands stable and effi-\ncient tuning strategies, particularly in the context of online and\ncontinuous learning tasks. Since parameter metrics are difficult\nto capture and do not readily lend themselves to the integration\nof strong heuristics or expert knowledge, we focus on capturing\nstateful transitions and propose tailored end-to-end tuners.\nFurthermore, navigating complex parameter spaces poses sig-\nnificant challenges for traditional search strategies and advanced\nmodel-based approaches [ 49]. Traditional search strategies (random\nsearch, grid search, heuristics) fall short in navigating the extensive\nparameter space of learned indexes, while advanced out-of-box\ntuning frameworks (e.g., SMBO [ 29]) require starting a new naviga-\ntion cycle with each shift in workload or data distribution, making\nthem resource-intensive. Furthermore, establishing universal states\nfor different index structures with different parameter sets creates\ncomplications. In this regard, LITune is designed to offer online\nand stateful index tuning using deep reinforcement learning for\ndynamic workloads and provides fast and safe configurations for\nmultiple learned indexes. Our approach mitigates the instabilities\nand aimless explorations that often accompany the use of generic,\nout-of-the-box tuning methods.\n3.2 System Overview\nAt the core of LITune is reinforcement learning (RL), which en-\nhances adaptability to dynamic workloads beyond the capabilities\nof traditional cost models. Specifically, we design a Markov De-\ncision Process framework that involves the RL agent interacting\nwith an environment to maximize rewards. The decisions are made\nbased on observed states and chosen parameters [37], which, in this\ncase, are the structural responses to shifting data distributions and\ntuned parameters, respectively.\nLITune operates in two primary phases: the Training Stage and\nthe Online Tuning Stage. In the Training Stage, we implement an\nefficient adaptive training pipeline to generate a generalizable pre-\ntrained model. Once this model is deployed, it undergoes continuous\nfine-tuning in the Online Tuning Stage, ensuring it remains cur-\nrent and effective under evolving operational conditions. To avoid\nmisconfigurations during tuning, LITune adopts a context-aware\nRL system that prevents early terminations, ensuring stability and\nspeed throughout the process. Additionally, to keep the RL agent\n\nA New Paradigm in Tuning Learned Indexes: A Reinforcement Learning Enhanced Approach SIGMOD ‚Äô25, June 22-27, 2025, Berlin, Germany\nupdated during online use, we employ an O2 system for ongoing\ntraining and adjustments.\n3.3 Training Stage\nPart A of Figure 3 depicts the initial generation of a pre-trained\nRL agent. This agent is crucial as it forms the foundation of our\nadaptive RL-based tuner, designed to efficiently handle diverse\ntuning scenarios right from deployment.\n3.3.1 Offline Training Preparation. The foundational step in our\napproach involves preparing a Deep Reinforcement Learning (DRL)\nagent for optimizing learned index configurations. This process\ncommences with the generation of varied datasets and query sets,\nestablishing a diverse training environment. Each training episode,\ndefined by a unique dataset-query set combination, allows the agent\nto explore a spectrum of configurations through a trial-and-error\nstrategy, thereby accumulating a rich set of initial training data.\nTraining Data: Unlike supervised learning, our methodology\nrelies on the automatic collection of training quintuples ‚ü®ùëë,ùëû,ùëé,ùë†,ùëü‚ü©\nby the RL agent. ùëëdenotes the dataset, ùëûa set of queries, ùëéthe\nparameters for index construction, ùë†the index states, and ùëüthe per-\nformance metric. This approach ensures comprehensive feedback\nfor the agent, which is pivotal for its learning process. Given the\nNP-hard nature of optimizing learned index configurations in a\ncontinuous parameter space, our model employs general DRL tech-\nniques. This choice enables the exploration of novel configurations\nand mitigates the risk of entrapment in local optima.\n3.3.2 Adaptive Training (Meta Training) Design. As mentioned in\nIntroduction, the C.2 requires robust generalization of Deep Re-\ninforcement Learning (DRL) agents in LITune to handle real-world\nscenarios with unseen queries and variable data distributions [ 47].\nTo achieve this, we introduce an adaptive training design based\non Meta Reinforcement Learning (Meta-RL), which enhances the\ntuner‚Äôs adaptability to new situations and addresses.\nOur method employs the Model-Agnostic Meta-Learning (MAML)\napproach [ 13], which trains agents to rapidly adapt with minimal\nupdates. In the context of learned index tuning, the \"tuning in-\nstances\" represent specific scenarios with unique data distributions\nand query types. MAML integrates into the RL training process\nthrough a two-level training loop:\nInner Loop‚ÄîAdaptation to Tuning Instances: Agents perform tuning-\ninstance-specific updates to optimize performance on sampled tun-\ning instances. This involves adjusting policy parameters based on\ninteractions characterized by specific workload types and data dis-\ntributions.\nOuter Loop‚ÄîMeta-Update for Generalization: The initial policy\nparameters are updated across all tuning instances to improve\ngeneral performance. This enhances the agent‚Äôs ability to generalize\nto new, unseen tuning scenarios.\nThis dual-loop process enables the agent to handle individual\ntuning scenarios effectively while maintaining broad adaptability\nacross diverse operational conditions, which is essential for effi-\nciency in the dynamic landscape of learned index tuning. Here is a\nquick example on how it works in practice:\nExample 3.1. Practice of Meta-RL\nConsider two tuning instances for a learned index system:(1)Instance A : Primarily range queries on a uniformly distributed\ndataset.\n(2)Instance B : A mix of insert and range queries on a skewed,\nnon-uniform dataset.\nA traditional RL agent trained only on Instance A performs well\nfor similar uniform range queries. However, when applied to In-\nstance B, it struggles to maintain performance, requiring extensive\nretraining to adjust its policy. In contrast, a Meta-RL agent trained\nwith the MAML approach has encountered diverse tuning instances\nduring meta-training. When faced with Instance B, it can swiftly\nadapt its policy with just a few gradient updates, leveraging prior\nknowledge. This enables the Meta-RL agent to maintain robust per-\nformance across varying scenarios without significant retraining.\n‚ñ°\n3.4 Online Tuning Stage\nAfter establishing a strong foundation in the training stage, we now\ntransition to the online tuning stage, where the pre-trained model\nis put into practical use. This section describes how the LITune\nsystem operates and adapts to continuous tuning needs on the fly.\nParts B and C of Figure 3 illustrate the architecture of the online\ntuning system. The dashed box at the top represents the client\nand data storage system where end users send their queries to\ntheLITune system below. This system is designed to handle the\ncontinuous adaptation required by varying data environments.\nThe operational flow is presented in Part B of Figure 3: Once the\ndata storage setup and tuning requests are confirmed, the learned\nindex and tuning system process the current data as the underlying\ndistribution. The system executes queries on this data, generat-\ning performance data and states. Based on these observations, the\ntuning system automatically suggests adjustments to the learned\nindex parameters to optimize future query handling and improve\nperformance.\nFurthermore, LITune leverages the pre-trained model to provide\nreal-time recommendations for parameter settings during online\ntuning scenarios. It also continuously refines this model by incorpo-\nrating feedback from each tuning request into its training process.\nThis integration of Online Tuning and Offline Training (O2 Sys-\ntem) ensures that LITune dynamically adapts to any changes in\nworkload and data distribution, maintaining high efficiency and\nadaptability over time.\n3.4.1 Online Tuning. End users can easily tune the target learned\nindex by submitting a request to LITune . Upon receiving a request,\nthe system gathers the necessary data and utilizes the pre-trained\nRL model for online tuning, ultimately recommending the best-\nperforming parameters.\n3.4.2 O2 System. As depicted in Part C of Figure 3, the O2 system\nintegrates Online and Offline RL models to address C.2, enhancing\nreal-time adaptability and performance optimization. The system\nemploys the online tuner with a pre-trained model for immediate\nindex adjustments when no data changes occur. Conversely, signif-\nicant data changes activate both the online and offline models: the\noffline model refines itself with new data, while the online model\nhandles real-time optimizations.\n\nSIGMOD ‚Äô25, June 22-27, 2025, Berlin, Germany Taiyi Wang, Liang Liang, Guang Yang, Thomas Heinis, and Eiko Yoneki\nLearned \nIndex\nPerformance \nin \nthe \nLoop\n<Reward>\n<Agent>\nTuner\nMeta \nTraining\nO2 \nSys\nPerformance\nResponses\nAvoid \nTerminations \nand \nFailures\nBoost \nAdaptabilities \nto \nUnseen \nWorkloads\nKeep \nthe \nRL \nModel \nEfficient\nand \nUp-To-Date\n<Action>\nSafety\n<Environment>\nParams\nMetrics\n<States>\nC.1\nC.2\nC.3\nM\nM\nM\nM\nM\nM\nM\nM\nM\nM\nM\n...\n...\nM\nM\nChanging \nWorkload \nand \nData \nDistribution\nStage \n1: \nTraining\nO2 \nSystem\nNew \nTrained \nData\nNew \nEval \nData\nReal-Time \nData\nOffline \nModel\nFine-Tuning \nTuner\nPerformance \nImproves?\nYes\nUpdate \nModel\nKeep \nOld \nModel\nNo\nPart \nC\nOnline \nModel\nEval\nPart \nB\nPart \nA\nOnline \nTuning\nMeta \nTraining\n \non \nvarious \nqueries \nand \ndata \ndistributions\nContext-RL \nmodels\nET-MDP \nConstraints\nQuery\n... \n...\n... \n...\nQuery\nUnderlying \nDistribution\nQuery\nQuery \nCharacteristic \nStage \n2: \nOnline \nTuning\nPre-Trained \nModel \nDeployment\nFigure 3: The architecture of LITune . Part A illustrates the training phase, where RL-based models are trained. Once the training\nis complete, these models are deployed as online tuners in Part B. The operational details of the O2 system are explained in\nPart C.\nThe O2 system routinely assesses the necessity for model up-\ndates by comparing the online model‚Äôs performance against new\ndata and predefined criteria, including statistical divergence and\nuser-defined thresholds. This assessment occurs at regular inter-\nvals or user-defined checkpoints, ensuring the O2 system remains\nresponsive to changing data and workloads. During updates, the\nsystem carefully balances the current online model‚Äôs performance\nwith the insights gained from the offline model‚Äôs continuous learn-\ning, thus maintaining optimal tuning efficacy. This efficiency also\ncontributes to addressing C.1.\nIt should be noted that the real-world queries encountered dur-\ning online tuning might differ from those used in the offline training\nprocess. For this reason, this structure allows LITune to stay adap-\ntive, leveraging the offline system‚Äôs incremental fine-tuning for\nbetter alignment with real-world workloads, while the online sys-\ntem provides swift responses to immediate tuning needs. Here is a\npractical working example of O2 system:\nExample 3.2. Running O2 System\nConsider a learned index tuning scenario with two distinct\nphases of data workload:\n(1)Stable Phase : The dataset experiences minimal changes, and\nthe workload consists mainly of read-heavy range queries.\n(2)Dynamic Phase : The dataset undergoes significant updates,\nincluding frequent insertions and deletions, and the work-\nload shifts to a mix of read and write queries.\nDuring the Stable Phase , the O2 system utilizes the online tuner\nwith the pre-trained model to make immediate index adjustments,\nensuring efficient query processing without the overhead of model\nretraining.\nWhen transitioning to the Dynamic Phase , the significant data\nchanges trigger the activation of both Online and Offline models.\nThe offline model begins refining the tuning strategy by learning\nfrom the new data distribution and varied query patterns. Simultane-\nously, the online model continues to handle real-time optimizations\nto maintain query performance. ‚ñ°3.5 LITune Working Process\nSummarizing the learned index parameter tuning in LITune , the\nlearned index (tuning target) serves as the RL environment, while\nthe deep RL model acts as the agent, recommending configurations\nbased on the learned index state. As the index is constructed and\nqueries are executed, the learned index state alters, reflected in the\nmetrics. These metrics evaluate learned index performance, calcu-\nlating the corresponding RL reward value, and the agent updates\nits policy accordingly, continuing until the tuning time budget is\nexhausted, ultimately revealing the most fitting parameter settings.\nThus, the RL-based tuner can easily capture and memorize the state\nresponses to the data distribution and workload shifts.\nTo illustrate how our RL-based tuner is specifically designed for\nlearned index scenarios, Figure 4(a) demonstrates a tuning exam-\nple using ALEX. As the workload transitions from a balanced to a\nwrite-heavy workload, LITune detects changes in ALEX‚Äôs states\nand performance metrics. Initially, it increases the maximum node\nsize from the default 16MB and reduces the threshold for out-of-\ndomain inserts before triggering node expansion. This adjustment\nleads to a notable decrease in the \"no_expand_and_retrain\" metric.\nWith positive feedback from the training environment, LITune\nstores new data and asynchronously fine-tunes the model using\nthe O2 system, evaluating potential updates based on pre-defined\ncriteria. This enables continuous learning with minimal disrup-\ntion, allowing swift adaptation to workload changes. Eventually,\nLITune recommends further increasing the maximum node size to\n64MB and raising the minimum number of out-of-domain inserts\nrequired before expansion for future trials. The Safe RL module\nensures system safety by automatically preventing the selection\nof dangerous states when configuring more aggressive values for\nthe maximum node size and minimum number of out-of-domain\ninserts. We avoid these aggressive settings, which may yield im-\nmediate rewards but could lead to system failure in the long run.\nWhile some learned indexes (like ALEX) require full reconstruction\nfor structural parameter changes due to their codebase constraints,\nLITune provides flexible on-the-fly reconfiguration mechanisms\nacross different index implementations. For cases where reconstruc-\ntion is unavoidable, LITune minimizes overhead through efficient\n\nA New Paradigm in Tuning Learned Indexes: A Reinforcement Learning Enhanced Approach SIGMOD ‚Äô25, June 22-27, 2025, Berlin, Germany\nsampling: maintaining a small reservoir ( ‚âà1%of dataset) and pro-\nportionally selecting queries across workload types (read/write),\nonly applying final configurations on the full dataset. This sampling\nstrategy enables rapid performance estimation while preserving\nworkload characteristics, with detailed cost analysis provided in\nSection 5.4.4.\n4 METHODOLOGY\nIn this section, we explore the integration of novel Reinforcement\nLearning (RL) models to address the unique challenges of tuning\nlearned indexes. While vanilla RL frameworks like Deep Deter-\nministic Policy Gradient (DDPG) methods [ 24,49] are adept at\nmanaging high-dimensional spaces and continuous actions, they\noften fall short in ensuring safety for tuning systems. To address\nthese limitations, we have augmented the standard DDPG frame-\nwork by incorporating context-aware learning. Notably, our online\ntuner components are designed with flexibility in mind and can\nbe replaced by any existing vanilla DRL methods, showcasing our\nframework‚Äôs adaptability to accept similar DRL approaches. De-\ntailed discussions on these DDPG framework enhancements are\npresented in the subsequent sections.\n4.1 RL formalization for LITune\nUsing RL in LITune requires a nuanced formalization of learned\nindex-tuning scenarios. Part B in Figure 3 illustrates the interaction\ndiagram of LITune components and the functional workflow of\nLITune in practice.\nAgent : Viewed as the tuning system, the agent receives rewards\nand states from the learned index, updating the policy to steer\nparameter adjustments towards higher rewards.\nEnvironment : Representing the tuning target, the environment\nis an execution instance of the learned index.\nState : InLITune , the state of the agent, denoted as ùë†ùë°, represents\nthe current condition of the learned index after applying recom-\nmended parameter settings. We categorize states into two main\ncategories: structural and operational. Structural metrics might in-\nclude features such as the number of internal nodes or tree height\nthat represent the structure and measurable mechanisms of the\nindex. However, not all features can be captured with just struc-\ntural features. Therefore, we define a new category of operational\nmetrics that captures which parameters affect the actual operation\nof the index. For each of the key operations (search, insert, update,\nand delete), we define a set of features that capture the cost of these\noperations using non-index-specific metrics. For example, we use\nsearch distance to capture the search cost, which is universal to\nall indexes regardless of the exact search algorithm used (linear,\nbinary, or exponential). These metrics act as empirical proxies due\nto the complex inter-dependencies among features, offering a de-\ntailed snapshot of the index‚Äôs internal states necessary for effective\ntuning across various scenarios.\nTuning-oriented Reward Design. We define the reward ùëüùë°as\na scalar that accounts for performance changes from both the initial\nbaseline (ùê∑0) and the immediately preceding step ( ùë°‚àí1). The idea\nis to capture the sort of incremental, forward-looking decisions\nthat human experts make when tuning complex systems, balanc-\ning short-term gains with a broader trajectory of improvement.Concretely, the RL agent starts from an initial performance ùê∑0and\nseeks an improved state ùê∑ùëõ. Once tuning begins, the system transi-\ntions toùê∑1and the RL-agent computes Œî(ùê∑1,ùê∑0). From there, each\niteration aims to surpass its predecessor, reflecting the principle\nthatùê∑ùëñshould exceed ùê∑ùëñ‚àí1for allùëñ<ùëõ. This design effectively\nencodes both immediate and foundational improvements in a single\nreward function.\nOur focus metric for performance, denoted as ùëÖ, signifies the\nend-to-end runtime, which is paramount for understanding and\noptimizing query performance. To track optimization progress, we\ndefine two key differential metrics: Œî=(\nŒîùë°‚Üí0=‚àíùëÖùë°+ùëÖ0\nùëÖ0\nŒîùë°‚Üíùë°‚àí1=‚àíùëÖùë°+ùëÖùë°‚àí1\nùëÖùë°‚àí1\nInspired by [49], the reward, ùëü, is articulated as:\nùëü=(\n((1+Œîùë°‚Üí0)2‚àí1)ùúî(1+Œîùë°‚Üíùë°‚àí1)ùúÖ, ifŒîùë°‚Üí0>0\n‚àí((1‚àíŒîùë°‚Üí0)2‚àí1)ùúî(1‚àíŒîùë°‚Üíùë°‚àí1)ùúÖ,ifŒîùë°‚Üí0‚â§0\nHere,Œîùë°‚Üí0andŒîùë°‚Üíùë°‚àí1measure performance changes relative to\nthe initial setting and the previous step. The scalar parameters ùúî\n(odd) andùúÖ(even) control how strongly the reward emphasizes\nnear-term versus longer-term improvements: ùúîgoverns the sig-\nnificance of changes from the initial baseline, and ùúÖdictates the\nimpact of recent performance gains or losses. By adjusting these\nscalars, practitioners can configure how aggressively the system\npursues performance gains, how quickly it penalizes regressions,\nand how it balances near-term improvements against long-term\nobjectives. In our practice, ùúî=1andùúÖ=2often strike a useful\nbalance between achieving notable gains and maintaining stability.\nMulti-objective trade-offs and achieving the tuning goal\nvia RL. The reward function underpins our RL-based tuning pro-\ncess by connecting the search mechanism to diverse performance\nobjectives. By maximizing the expected cumulative discounted re-\nward, maxùúãEùúè‚àºùúãh√çùëá\nùë°=0ùõæùë°ùëüùë°i\n,whereùúãis the policy, ùúèthe trajectory,\nùõæthe discount factor, and ùëüùë°the immediate reward, the RL agent\nexplores parameter configurations aligned with user priorities. It\nrefines its policy via trial-and-error (temporal-difference methods)\nto approximate expected returns across diverse states. Practitioners\ncan adjust the performance metric ùëÖto emphasize or de-emphasize\nlatency or throughput (e.g., ùëÖ=0.8¬∑latency+0.2¬∑throughput‚àí1),\nsteering the tuner‚Äôs optimization without altering the underlying\nRL framework, ensuring that the tuner can effectively meet the final\ngoal‚Äîbe it latency-sensitive, throughput-oriented, or a balanced\nmix of both.\nAction : Denoted as ùëéùë°, actions derive from the parameter con-\nfigurations space and correspond to parameter tuning operations,\ninfluencing all tunable parameters concurrently.\nPolicy : Policy, mapping from state to action, maintains state\ntransitions and is represented by a deep neural network. RL aims\nto learn the optimal policy.\n4.2 Backbone: Safe RL approach for LITune\nAs mentioned in the introduction, we face the challenge of opti-\nmizing performance while ensuring safety in the context of tuning\nlearned index ( C.3), i.e., avoiding configurations that lead to sys-\ntem instability or failures such as out-of-memory errors or endless\n\nSIGMOD ‚Äô25, June 22-27, 2025, Berlin, Germany Taiyi Wang, Liang Liang, Guang Yang, Thomas Heinis, and Eiko Yoneki\nprev StatesLITuneSafe RL BackboneO2 Coreno_expand_and_retrain,etc.Observed Metrics Update modelcurr StatesBalanced WorkloadUpdatesLookupsALEXAuto-tuned toout-of-domain inserts = 16Max_node_size = 64MB          ...By defualtout-of-domain inserts = 1Max_node_size = 16MB...Avoid Potential System Terminations In the Future (Unsafe Tuning)Workload Shift -- Entering Tuning Stage\nAuto-tuned toout-of-domain inserts = 40Max_node_size = 128MB50%               50% ALEXWrite-heavy WorkloadUpdatesLookups70%               30% \nCritic NetworkActor NetworkIndexing FailureOut of MemoryInfinite Runtime...LSTM ModuleSafe RL Backbone:Context-Aware RL Model Safety-Aware Memories\n(a)(b)\nFigure 4: (a) Running example of LITune . This example demonstrates how LITune ‚Äôs tuner components respond to changes in\nperformance metrics and adjust to workload shifts. (b) The safe-RL approach prevents aggressive tuning by learning from\ninstabilities encountered during training.\nruntime. To address this, we propose a minimalist approach by\nemploying an Early Terminated Markov Decision Process (ET-MDP)\nsolver that triggers an early termination whenever the learning\npolicy violates predefined constraints. Early termination has been\npreviously used to improve sample efficiency in solving regular\nMDPs [ 42], as it accelerates learning by reducing the policy search\nspace and shortening the time horizon. Moreover, an ideal policy\nshould never violate the constraints, eliminating the need to learn\nto proceed or recover after violations.\nTo effectively handle the constraints in our tuning problem, we\nmodel it as a Constrained Markov Decision Process (CMDP) and\ntransform it into its early terminated counterpart, the ET-MDP [ 35].\nThis transformation allows us to apply standard RL algorithms\nwhile ensuring safety through the early termination mechanism.\nDefinition 4.1 (Constrained Markov Decision Process). ACon-\nstrained Markov Decision Process (CMDP) is a deterministic MDP\nwith a fixed horizon ùêª‚ààN+, defined by the tuple (S,A,ùêª,ùëü,ùëê,ùê∂, T),\nwhereSandArepresent the state and action spaces; ùëü:S√óA‚Üí R\nis the reward function; ùëê:S√óA‚Üí Ris the cost function rep-\nresenting constraints; ùê∂‚ààR+is the upper bound on the permit-\nted expected cumulative cost; and T:S√óA‚ÜíS is the tran-\nsition function. The policy class Œ†consists of stationary policies\nùúã:S√óA‚Üí[ 0,1]such that√ç\nùëéùúã(ùëé|ùë†)=1for allùë†‚ààS.\nIn our learned index tuning problem, constraints such as out-\nof-memory errors and endless runtime define dangerous or con-\nstrained states, as illustrated in Figure 4(b). We incorporate these\nconstraints into the CMDP framework by defining appropriate\ncost functions. Specifically, we assign costs (e.g., ùëêùëöfor memory\nviolations and ùëêùëüfor runtime violations) which are set to 1 upon\nviolation, ensuring that each type of violation contributes equally to\nthe cumulative cost. This approach penalizes the policy for entering\nunsafe areas and guides it toward safer trajectories.\nRemark. By incorporating system constraints into the cost func-\ntion, we can effectively model the safe learned index tuning problem\nas a CMDP.\nTo handle the constraints and ensure safety during the learning\nprocess, we transform the CMDP into an Early Terminated MDP (ET-\nMDP), which introduces an absorbing termination state whenever\nthe cumulative cost exceeds a predefined threshold.Definition 4.2 (Early Terminated MDP). For any CMDP, we de-\nfine its Early Terminated MDP (ET-MDP) as a new unconstrained\nMDP(S‚à™{ùë†ùëí},A,ùêª,ùëü‚Ä≤,T‚Ä≤), whereùë†ùëíis the absorbing state after\ntermination. The transition function and reward function in the\nET-MDP are adjusted to handle terminations:\nT‚Ä≤(ùë†,ùëé)=T(ùë†,ùëé) 1(ùëèùë°‚â§ùê∂)+ùë†ùëí 1(ùëèùë°>ùê∂),\nùëü‚Ä≤(ùë†,ùëé)=ùëü(ùë†,ùëé) 1(ùëèùë°‚â§ùê∂)+ùëüùëí 1(ùëèùë°>ùê∂).\nwhereùëèùë°=√çùë°\nùúè=1ùëêùëöùúè+ùëêùëüùúèrecords the cumulative costs up to\ntimeùë°, andùëüùëí‚ààRis a small termination reward. The parameter ùê∂\nrepresents the total tolerated failures we can accept during training.\nThese adjustments integrate the costs into the ET-MDP frame-\nwork to ensure that once a constraint is violated, the associated\ncost is counted and added to the cumulative costs. This guides the\nagent to avoid actions leading to high-penalty states, effectively\nsteering the policy towards safer and more optimal trajectories.\nRemark. By converting the CMDP into an ET-MDP and solving it\nusing an appropriate RL algorithm, we can effectively ensure that the\nlearned policy respects the constraints by avoiding actions that lead\nto early termination.\nSolving the ET-MDP: The transformation of the CMDP into\nan ET-MDP simplifies the problem by converting it into an uncon-\nstrained MDP where constraints are implicitly handled via early\ntermination. The goal is to find an optimal policy ùúãthat maximizes\nthe expected cumulative reward while respecting the constraints:\nmax\nùúã‚ààŒ†Eùúè‚àºùúã,T\"ùêª‚àëÔ∏Å\nùë°=1ùëüùë°#\n,s.t.Eùúè‚àºùúã,T\"ùêª‚àëÔ∏Å\nùë°=1ùëêùë°#\n‚â§ùê∂,\nwhereùúè=(ùë†1,ùëé1,ùëü1,...,ùë†ùêª,ùëéùêª,ùëüùêª)represents the trajectory gen-\nerated by policy ùúã.\nTo solve this constrained optimization problem, the Lagrangian\nmethod relaxes it to an unconstrained one with a penalty term:\nùúã‚àó=arg max\nùúã‚ààŒ†min\nùúÜ‚â•0Eùúè‚àºùúã,T\"ùêª‚àëÔ∏Å\nùë°=1ùëüùë°‚àíùúÜùêª‚àëÔ∏Å\nùë°=1ùëêùë°#\n+ùúÜùê∂, (1)\nWhereùúÜ‚â•0is the Lagrangian multiplier. In practice, if the policy\nùúãis parameterized by ùúÉ, i.e.,ùúã=ùúãùúÉ, the optimization over ùúÉandùúÜ\ncan be conducted iteratively through policy gradient ascent for ùúÉ\nand stochastic gradient descent for ùúÜaccording to Eqn. (1).\n\nA New Paradigm in Tuning Learned Indexes: A Reinforcement Learning Enhanced Approach SIGMOD ‚Äô25, June 22-27, 2025, Berlin, Germany\nHowever, as pointed out by [ 5], one possible defect of the La-\ngrangian methods is the violation of constraints during training,\nas the method may not strictly enforce the constraints at every\niteration. This issue can be mitigated by incorporating context mod-\nels[35].\nContext models in an ET-MDP solver learn generalizable rep-\nresentations across similar tasks. In our setting, each state corre-\nsponds to a different task within the same distribution, allowing\ncontext models to transfer policies to unseen states and avoid con-\nstraint violations. By modeling tuning as a CMDP and transforming\nit into an ET-MDP, we naturally incorporate safe RL constraints\ninto learned index tuning.\nImplementation in LITune :InLITune , we handle constraints\nsuch as memory limits and runtime bounds by triggering early ter-\nmination when these constraints are violated. The normal CMDP\nsolver, integrated with the Deep Deterministic Policy Gradient\n(DDPG) algorithm enhanced with Long Short-Term Memory (LSTM),\nallows the system to manage large state and action spaces effec-\ntively. The LSTM module maintains context from past explorations,\nenabling the RL agent to adapt to dynamic data environments and\nexplore safely while maximizing reward collection.\nFigure 4 summarizes how the ET-MDP solver in LITune ad-\ndresses safe tuning issues. The core strategy involves linking dan-\ngerous system metrics to the learned index states (understood as\nthe structural information of the constructed index). The RL agent\nlearns to avoid these dangerous states and maximize rewards within\nthe given constraints during training. Once deployed, with memory\nunits embedded in LSTM, the RL agent can autonomously identify\nsafe tuning areas and achieve more reliable tuning performance.\nThis strategy is significantly effective to handle the C.3.\n5 EXPERIMENTAL STUDY\nIn this section, we evaluate the performance of LITune in compari-\nson to existing tuning approaches, focusing on two specific learned\nindex instances across various workloads and datasets.\n5.1 Key Insights\nBefore presenting our experimental results, we highlight several\nessential insights from our study:\n(a) Tuning is essential: Effective default parameters are hard\nto set due to the complexity and variability of systems and prob-\nlems. Unlike claims in [ 10,48], default settings often fail to achieve\noptimal performance because they can‚Äôt handle parameter inter-\ndependencies and dynamic data conditions. This highlights the\ncrucial need for tuning to enhance learned index performance by\nsynchronously adjusting parameters. (details in section 5.4.1)\n(b)LITune is efficient: Contrary to the belief that deep models\nsacrifice tuning efficiency for quality, LITune uses online tuning\nmethods to significantly improve efficiency. Experiments demon-\nstrate that LITune outperforms other methods in performance\ngains, regardless of the tuning budget. (details in section 5.4.3, cor-\nresponding to C.1)\n(c)LITune is adaptive: LITune effectively handles online and\ncontinuous tuning scenarios, adapting to different data distribu-\ntions and workload dynamics. Its ability to adjust to varying tuningTable 2: Parameter space characteristics of learned indexes\nIndex # Dims Parameter Types\nALEX 14‚Ä¢5 Continuous [0,1] (density, workload ratios,etc.)\n‚Ä¢3 Boolean (computation, splitting controls, etc.)\n‚Ä¢4 Integer (node sizes, buffer limits, etc.)\n‚Ä¢2 Discrete choice (tree policies, etc.)\nCARMI 13‚Ä¢10 Continuous (operation timings, etc.)\n‚Ä¢2 Integer (node sizes, etc.)\n‚Ä¢1 Hybrid continuous/discrete (lambda, etc.)\nbudgets showcases its robustness and applicability across diverse in-\ndex types, proving its effectiveness under changing data conditions.\n(details in section 5.4.5, corresponding to C.2)\n(d)LITune is safe: While setting safe ranges for individual\nparameters is simple, adjusting multiple parameters simultaneously\nposes safety challenges. LITune employs a context-aware strat-\negy that links failure situations with states, using constraints and\npenalties to maintain parameters within safe zones. This approach\nensures system stability and safety during online tuning, as demon-\nstrated by our experimental results. (details in section 5.5.2, corre-\nsponding to C.3)\n5.2 Experimental Settings\n5.2.1 Platform. Experiments are conducted on a machine equipped\nwith an NVIDIA Quadro RTX 8000 GPU, an Intel(R) Xeon(R) Gold\n5218 CPU @ 2.30GHz, 8 vCPUs, and 64GB RAM. Since most of the\nparameter tuning methods have a certain degree of randomness,\nwe repeated each experiment 5 times with different seeds.\n5.2.2 Tuned Indexes and parameters. ALEX [10] is a dynamic, up-\ndatable learned index that allows tuning to optimize workload\nperformance while not requiring parameters for basic operation.\nIt offers statistics such as the number of model and data nodes,\nas well as expansions and splits, which serve as states for tuning\nparameters like read-write ratios, node sizes, and gap settings.\nCARMI [48], based on RMI, enhances cache-awareness in learned\nindexes. It does not expose statistics directly but allows for state\nidentification, including structural elements like leaf node counts\nand operational factors such as query visits and keys scanned. These\nstates are tuned for parameters like leaf size and search operation\nweights, showcasing our method‚Äôs capability to adapt tuning strate-\ngies to various learned index configurations.\nWe selected these two as our tuned instances because they ex-\nemplify two distinct parameterization mechanisms, each utiliz-\ning different optimization methods. Specifically, ALEX employs\na heuristic-based cost model to automatically tune its parame-\nters, whereas CARMI optimizes its parameters involving hardware-\nspecific considerations.\nTable 2 presents an overview of the parameter space in learned\nindexes, highlighting the marked differences in tuning complexity\nacross existing methods. While other recent index tuning works\ntypically handle only a few parameters‚ÄîCDFShop [ 27] focuses\non 2‚Äì4 RMI-specific parameters, RusKey [ 28] primarily tunes the\ncompaction policy ùêæin FLSM-Tree (fewer than three parameters),\nand AirIndex [ 10] employs layer-wise parameters which, despite\ntheir seemingly large number, are governed by strong hierarchical\n\nSIGMOD ‚Äô25, June 22-27, 2025, Berlin, Germany Taiyi Wang, Liang Liang, Guang Yang, Thomas Heinis, and Eiko Yoneki\nconstraints, pruned to a top- ùêæset, predominantly focused on I/O-\naware optimizations, and limited to static workload conditions‚Äîour\nsystem manages a substantially larger optimization space of around\n10-15 parameters.\nThe complexity of our parameter space, with its high dimen-\nsionality and mixed discrete-continuous variables, poses significant\nchallenges for traditional optimization methods under limited tun-\ning budgets. RL-based methods excel by learning parameter inter-\nactions through experience, enabling efficient global exploration\nwhile avoiding local optima.\n5.2.3 Evaluation and Training Datasets. We evaluate LITune using\nthe Search On Sorted Data (SOSD) benchmark suite [ 18], which\nincludes datasets with up to 200 million 64-bit keys from various\ndomains: Amazon books, OpenStreetMap (OSM), Facebook user\nIDs, and MIX (a combination of uniform, FB, books, and OSM\ndistributions).\nTo prevent overfitting and ensure the tuner encounters unseen\ndistributions during evaluation, we adopt strategies from prior RL-\nbased training [ 28,49]. Instead of training directly on SOSD, we\ngenerate synthetic 1-D key-value pairs with diverse distributions\n(e.g., uniform, beta, normal) and vary the Write-Read Ratio (W/R\nRatio) between 1:10 and 10:1. These experiments test the gener-\nalization ability of the methods across diverse query patterns in\nvarious tuning scenarios.\n5.2.4 Workloads. LITune evaluation highlights its versatility across\nstatic and dynamic datasets, establishing it as a robust tuning solu-\ntion for learned indexes. All runtime performances depicted in the\nsection‚Äôs figures represent the average runtime for basic operations\n(INSERT, SEARCH, and DELETE) based on the Write-Read Ratio\nwithin workloads.\n(a) Static Workload: We evaluate LITune on static datasets\n(OSM, books, Facebook, MIX) to demonstrate its adaptability to\nvaried data distributions. Our setup uses 90M records from a 100M\ndataset2, reserving 10M for a fixed distribution and 80M for IN-\nSERT/DELETE operations. Workloads differ by Write-Read Ratio\n(W/R): Balanced (1), Read-Heavy (1/3), and Write-Heavy (3). We\nevaluate tuning efficiency over varying steps (Figure 5) and also\nconduct \"extensive tuning\" (up to 50 steps or 1000 seconds) to\ngauge near-optimal performance (Figure 6).\n(b) Data-shifting Workload: We divide 100M records into 30\ntumbling windows [ 30], each with 1M base data and 8M updates.\nLITune frequently adjusts parameters to handle rapid data evolu-\ntion, with at most 5 tuning steps or 100 seconds (whichever first),\nas shown in Figures 9 and 103.\n5.2.5 Evaluation Metrics. The key metric used for evaluating the\ndifferent tuning methods is End-to-end runtime performance. We\ncrafted a dual-faceted experimental setup to encompass both static\nand data-shifting cases. In addition to query runtime performance\nshown in figure 6 , we also evaluate throughput in static settings, as\nshown in figure 7. It‚Äôs worth noting that these throughput metrics\n2A 90M subset aligns with the NIPS mlforsys‚Äô19 fb distribution in the SOSD\nbenchmark.\n3These workload combinations reflect their worst performance under streaming\nconstraints.were obtained under continuous system tuning requests, which in-\ntroduces unique considerations that may account for the observed\ndiscrepancies with the runtime speed-up results. We also docu-\nmented the effects and insights related to tuning the structural\nparameters of the learned index, which are detailed in section 5.4.1.\n5.3 Baseline Methodologies\nOur evaluation sought to underscore the efficacy of LITune by\ncontrasting it with various established tuning methodologies. Ini-\ntially, the Default method, utilizing unaltered system parameter\nsettings from index designers or experts, set a fundamental base-\nline for comparative analysis. This is the adaptive configuration\nthat responds to dynamic workloads and is designed by the au-\nthors of the corresponding indexes. Random Search scanned the\nparameter space indiscriminately, while Grid Search exhaustively\ntested predefined parameter combinations. Notably, Grid Search\ndoes notrely on expert defaults but instead uses a fixed parame-\nter grid determined at the outset. Heuristic Search , implemented\nvia a simulated annealing kernel from OpenTuner [ 1], pursued\nmore focused exploration by leveraging existing expert heuristics.\nSequential Model-Based Optimization (SMBO) employed the TPE\nmethod [ 2,29], effectively managing complex parameter spaces\n(an approach similar to OtterTune [40]). Lastly, we incorporated a\nvanilla DDPG-based tuner [24], referred to as \"DDPG\" , pretrained\nand fine-tuned with the same data as LITune , to demonstrate that\ndirect RL pipelines from other domains (e.g., DBMS [ 49]) may offer\nmoderate gains when embedded in our framework, but are less\neffective overall than LITune .\n0 5 10 15 20 25 300.700.750.800.850.900.95\nRuntime Ratio over Steps (Lower is Better)LITune DDPG SMBO Random Search Heuristic Search\n0 5 10 15 20 25 30\nTuning Steps1.021.041.061.081.101.121.141.16\nThroughput Ratio over Steps (Higher is Better)\nFigure 5: Tuning efficiency‚ÄìPerformance as tuning steps in-\ncrease. Above: runtime ratio (best found vs. default settings).\nBelow: throughput ratio (best found vs. default settings).\n5.4 Results Overview\nOur comprehensive experimentation yielded results strongly in\nfavor of LITune , with the method outperforming all baseline meth-\nods under varying conditions. Notably, LITune ‚Äôs advantage be-\ncomes especially pronounced whether the available tuning steps\nare restricted or extended, emphasizing its efficacy and efficiency\nin optimizing learned index types.\n5.4.1 Tuning results and takeaways. The values marked in Figure 6\n& 7 show the performance gains compared against the default pa-\nrameters as evidence of the effectiveness and need for a tuning\n\nA New Paradigm in Tuning Learned Indexes: A Reinforcement Learning Enhanced Approach SIGMOD ‚Äô25, June 22-27, 2025, Berlin, Germany\nFigure 6: Runtime performance (average on operation tuples) with extensive tuning. Performance improvements relative to\ndefault settings are marked.\nFigure 7: Throughput performance (ops/sec) with extensive\ntuning. Performance improvements relative to default set-\ntings are marked.\nsystem. We dive into some insight from the parameters of ALEX.\nAcross all workloads, the thresholds for out-of-domain insertions\nexhibit significant increases compared to default parameters. Specif-\nically, the minimum threshold shows an 80‚Äì100 √óincrease and the\nmaximum threshold ranges from 3‚Äì5 √ó, which suggests that ALEX\ncan achieve better by \"buffering\" out-of-domain keys in each node\nprior to expansion. Moreover, LITune adjusts workload-specific\nparameters such as the expected insertion fraction (otherwise fixed\nto a write-only workload) and toggles between approximate or\nexact model/cost computations. Specifically, ALEX benefits more\nfrom exact computations under balanced workloads, whereas read-\nheavy or write-heavy scenarios may favor approximate approaches.\nAdditionally, LITune minimizes expansions and retrains, reducing\nretrains for the OSM dataset under a balanced workload from 7196\nto nearly zero. This outcome aligns with recent findings [ 17,36]\nemphasizing the overhead of retraining in learned indexes.\n5.4.2 Radar Chart: Overall Evaluations among Tuning Methods.\nThe showcase in Figure 8, based on 200 tuning trials using the MIX\ndata distribution (the most complex) and a balanced workload on\nCARMI, quantitatively demonstrates the advantages of LITune . It\nprovides a quick comparison of various tuning approaches across\nfive attributes: Adaptability, Solution Quality, Stability, Tuning Ef-\nficiency, and Preparation Time. Scores are normalized on a scale\nFigure 8: Navigating the Parameter Tuning Landscape:\nLITune vs Other Tuning Methods\nfrom 0 to 9 to enable direct comparison. The results are illustrated\nin a radar chart, emphasizing the strengths and weaknesses of\neach method. Metrics are defined as: Adaptability: Lower runtime\nvariances across scenarios indicates better adaptability. Solution\nQuality: Higher average runtime performance signifies superior\nresults. Stability: More successful trials indicate higher stability.\nTuning Efficiency: Higher performance-to-tuning budget ratios\nshow better efficiency. Preparation Time: Shorter setup and pre-\nto-tuning time are preferred.\n5.4.3 Comparative E2E Performance of LITune .LITune demon-\nstrates strong End-to-End (E2E) performance across diverse datasets\nand workloads, leveraging Meta-RL, the ET-MDP solver, and O2\nSystem components. As shown in Figure 5, on the MIX dataset with\na Balanced workload on ALEX, it rapidly achieves over 60% of the\noptimal result within 10 steps, while others require at least twice\nas many. Its tuning efficiency‚Äîquery runtime improvements per\nstep‚Äîis about 2√óthat of SMBO and DDPG, and 3√óthat of random\nsearch, surpassing baselines from the very first recorded step. Only\ninference is required online, consuming just seconds per step on\nmodern GPUs. Figure 6 highlights CARMI‚Äôs notable optimization\n\nSIGMOD ‚Äô25, June 22-27, 2025, Berlin, Germany Taiyi Wang, Liang Liang, Guang Yang, Thomas Heinis, and Eiko Yoneki\nTable 3: Training and tuning cost comparison across different meth-\nods. LITune-X denotes X% sampling ratio, and Col 3-Col 6 indicate\ntuning time required to achieve target performance improvements.\nMethod Training Tuning Time Best Perf.\n-5% -10% -20% -45% (default 403s)\nGrid Search - 32m 1.0h - - ‚Üí360s\nHeuristic Search - 15s 35s 5m - ‚Üí312s\nSMBO - 12s 25s 3m - ‚Üí314s\nDDPG ([24, 28]) 12h 28s 35s 45s - ‚Üí326s\nLITune-0.1% 5h 6s 12s 18s 25s ‚Üí288s\nLITune-1%(ours) 6h 8s 15s 22s 28s ‚Üí212s\nLITune-10% 7h 12s 20s 26s 32s ‚Üí211s\nLITune-Full 12h 18s 25s 32s 38s ‚Üí208s\nheadroom, with over 90% runtime reduction (compared to 30‚Äì40%\nfor ALEX). Complex datasets like MIX and OSM pose additional\nchallenges, yet LITune consistently excels.\nWhen looking into tuning methods, Grid Search and Random\nSearch display contrasting outcomes. Grid Search consistently under-\nperforms across all tuning budget scenarios, illustrating the chal-\nlenges of exhaustively navigating extensive parameter spaces, and\nthus is not included in Figure 5. In contrast, Random Search oc-\ncasionally reaches optimal or near-optimal configurations under\nmore generous budgets despite its inherent performance variability.\nSMBO , while generally effective, can under-perform due to its re-\nliance on historical evaluations, which may lead it into sub-optimal\nsearch areas influenced by noise or model discrepancies. Moreover,\nSMBO risks venturing into system risk areas, potentially wasting\nthe tuning budget without efficient exploration. Heuristic Search\nmaintains stable and reasonable performance but requires specific\nheuristic designs, which may not be universally applicable across\nall systems or scenarios. The performance of Heuristic Search is\nnotable in the ALEX scenario due to effective heuristic discovery.\nYet, it faces limitations in the CARMI scenario where appropriate\ninsights into structures and parameter impacts are lacking.\nFailure of DDPG-Tuner: As shown in Figure 6 and Figure 7, vanilla\nDDPG only matches the performance levels of SMBO and heuristic\nsearches, lagging 10-15% behind LITune . The under-performance\nof a vanilla RL tuner using DDPG, compared to LITune , can be\nattributed to several key factors. Firstly, DDPG-Tuner often fails to\ncapture the complex dependencies and dynamics within the tun-\ning environment due to its simpler reinforcement learning model.\nThis leads to underfitting, where the tuner is unable to generalize\nwell from its training data to new or unseen scenarios. In contrast,\nLITune integrates advanced Meta-RL and the O2 system, which\nnot only accelerates learning adaptations but also enhances its\npredictive accuracy by utilizing historical tuning experiences. Fur-\nthermore, DDPG-Tuner lacks the advanced memory and learning\nmechanisms of LITune , making it less effective in handling di-\nverse tuning tasks, leading to sub-optimal decisions and reduced\nperformance in complex environments.\n5.4.4 Tuning and Training Costs of LITune .While LITune offers\nsignificant performance improvements, managing training and tun-\ning costs is essential for practical deployment. Under a scaling\nworkload with 400M balanced queries on ALEX using OSM data,\nTable 3 presents a comprehensive comparison of tuning overheadacross different methods. We evaluate LITune under different sam-\npling rates to demonstrate the effectiveness of our sampling strategy.\nFor instance, to achieve a 20% runtime reduction, LITune with 1%\nsampling requires only 22 seconds of tuning time, compared to 25\nseconds for vanilla DDPG and several minutes to hours for tradi-\ntional approaches. We choose the 1% sampling rate as it achieves\nnearly identical performance (212s vs 208s) to LITune-Full while\nhaving training and tuning overhead close to LITune-0.1%, which\nsacrifices too much performance (288s vs 212s). Our results also\nshowcase the highest attainable performance for various tuning\nmethods when given substantial tuning budgets (1000s), while not-\ningGrid Search ‚Äôs limitation of becoming computationally infeasible\ndue to its expansive search domain.\nThe training phase represents a one-time investment in computa-\ntional resources, where LITune develops generic tuning strategies\nacross various index scenarios. As shown in Table 3, LITune ‚Äôs train-\ning time with 1% sampling (6 hours) is half that of DDPG used\nin RusKey [ 28] (12 hours), while providing superior tuning capa-\nbilities - achieving 47% runtime reduction compared to DDPG‚Äôs\n19%. Furthermore, our O2 System (detailed in Section 3.4.2) allows\ninstant model updates for real-time applications without additional\ntraining overhead.\n5.4.5 Adaptability of LITune .LITune consistently exhibits supe-\nrior performance and adaptability across varied queries, data dis-\ntributions, and data shifts, as demonstrated in Figures 6, 7, and\n9.\nThis robust performance amid diverse and shifting scenarios\ncan be ascribed to the foundational Deep Reinforcement Learning\n(DRL) framework, which enables LITune to continually refine its\npolicies and swiftly adapt its parameter settings, ensuring optimal\nperformance amidst varied query contexts and data scenarios. The\nLITune not only promotes intelligent and dynamic exploration\nand exploitation of the parameter space but also facilitates quick\nconvergence to optimal or near-optimal configurations, making it\nespecially adept at navigating complex, heterogeneous, and dynam-\nically evolving data and query environments, thereby addressing\nC.2. Specifically, LITune demonstrates:\n1. Consistency across Query Types: Different rows in Figure 6\ncorrespond to diverse query types (B, RH, WH) when read vertically.\nNotably, LITune ‚Äôs framework efficiently navigate through different\nquery types, adapting its parameter settings in real time to ensure\noptimal performance amidst shifting query contexts.\n2. Versatility across Data Distributions: Different columns in Fig-\nure 6 correspond to varied data distributions (OSM, books, fb, MIX)\nwhen read horizontally. LITune also showcases adaptability to var-\nied data distributions, adeptly managing complex scenarios like\nthe MIX distribution by autonomously identifying and applying\noptimal parameter configurations.\n3. Robustness amidst Data Shifts: As evident in Figure 9, LITune\nsustains high performance across continuous data chunks in online\ntuning, quickly adapting to evolving data distributions without\nnecessitating re-initialization. It is important to note that we did\nnot include DDPG or vanilla-DRL methods for comparison here,\nas their extreme instability in handling dynamic scenarios without\ntailored designs made them unsuitable for this evaluation.\n\nA New Paradigm in Tuning Learned Indexes: A Reinforcement Learning Enhanced Approach SIGMOD ‚Äô25, June 22-27, 2025, Berlin, Germany\n246810\nData Chunk Streams2468Avg Runtime( s)\nALEX OSM  (balanced)\n246810\nData Chunk Streams246\nALEX OSM  (read-heavy)\n246810\nData Chunk Streams481216\nALEX OSM  (write-heavy)\n246810\nData Chunk Streams0246Avg Runtime( s)\nCARMI MIX  (balanced)\n246810\nData Chunk Streams246\nCARMI MIX  (read-heavy)\n246810\nData Chunk Streams36912\nCARMI MIX  (write-heavy)\ndefault LITune Heuristic Search SMBO Random Search\nFigure 9: Online and continuous tuning performance (aver-\naged over operation tuples) in data streams (ALEX+OSM and\nCARMI+MIX)\n5.5 Ablation Study\n0 5 10 15 20\nData Chunk Streams1.02.03.04.0Avg Runtime( s)\nCARMI_fb (balanced)\n0 5 10 15 20\nData Chunk Streams1.02.03.04.05.0CARMI_fb (read-heavy)\n0 5 10 15 20\nData Chunk Streams3.06.09.012.0CARMI_fb (write-heavy)\n0 5 10 15 20\nData Chunk Streams1.41.51.71.8Avg Runtime( s)\nALEX_MIX (balanced)\n0 5 10 15 20\nData Chunk Streams1.82.02.22.4ALEX_MIX (read-heavy)\n0 5 10 15 20\nData Chunk Streams3.23.64.04.44.8ALEX_MIX (write-heavy)\ndefault LITune (With O2) LITune (W/O O2 )\nFigure 10: Benefits of the O2 system in online and continuous\ntuning, comparing with pre-trained models W/O O2 system\n(CARMI+fb and ALEX+MIX), averaged over operation tuples.\nFigure 11: Exploring Parameter Spaces Across Tuning Meth-\nods(ALEX + OSM + Balanced).\n5.5.1 Effects of O2 System. Resilience to Unseen Data: Figure\n10 shows that the O2 system‚Äôs online component quickly adapts\nto new trends using a sliding window of recent queries, enhancing\nresilience to unforeseen data and ensuring adaptability.\nHandling Data Distribution Shifts: Demonstrated in the MIX\ndataset with ALEX (See Figure 10), the O2 system adeptly handles\ndata distribution shifts. The offline tuner, enriched with diversetraining data, robustly supports varied distributions. When the\nonline model detects significant shifts, a threshold in the divergence\nmeasure initiates a model swap, ensuring continuous adaptation.\nConsistency in Performance: As demonstrated in Figure 10,\nLITune equipped with the O2 system consistently outperforms\nversions without it. This superior performance is attributed to its\nhybrid approach: the online model rapidly adapts to new condi-\ntions, while the offline model, trained on a wider array of data\nscenarios, seamlessly maintains performance. This synergy ensures\ndependable and stable operation across diverse data environments.\n5.5.2 Safe Tuning. As mentioned in the Introduction, exploring\nparameter spaces can enhance system performance but also intro-\nduces risks, especially when parameters significantly impact system\nreliability and stability during tuning and operation ( C.3).\n(a)(b)\n04080120160200630-2\nFigure 12: (a) Training stability comparison between LITune\nvariants with and without Safe-RL (ALEX). (b) End-to-end\nruntime performance of LITune variants after tuning on the\nMIX dataset for ALEX.\nFigure 12(a) demonstrates the importance of our Safe-RL module\nthrough training performance comparison. Without Safe-RL, the re-\nward signals exhibit high volatility during the latter stages of train-\ning due to severe penalty signals triggered by system terminations\n(e.g., infinite loops, memory crashes) from aggressive parameter\nexploration. In contrast, LITune with Safe-RL maintains stable re-\nward improvement throughout training, achieving both better final\nperformance and more consistent learning progress compared to\nvanilla DDPG. Besides, within 200 epochs, we found that the vanilla\nDDPG cannot learn a converged policy and requires further train-\ning. This stability difference is particularly pronounced after the\n100th training episode, where LITune without Safe-RL shows large\nreward fluctuations from frequent system failures, while the Safe-\nRL version maintains steady improvement by proactively avoiding\nparameter combinations that could lead to system termination. As\nshown in Figure 12(b), this training stability translates directly to\nbetter end-to-end performance from trained policies: In average,\nLITune with Safe-RL achieves 30% lower runtime with 60% less\nvariance compared to the version without Safe-RL, demonstrating\nthat safer exploration leads to both better and more reliable tuning\noutcomes\nFigure 11 further illustrates the benefits of our safety-aware de-\nsign. Figures 11(a)‚Äì(e) show how four methods explore two critical\nparameters: kMaxOutOfDomainKeys andkOutOfDomainTolerance-\nFactor , which are crucial under specific configurations ( fanoutSelec-\ntionMethod = 1 ,splittingPolicyMethod = 1 ,allowSplittingUpwards =\nTrue). The red-highlighted Dangerous Zone in subfigure (a) marks\n\nSIGMOD ‚Äô25, June 22-27, 2025, Berlin, Germany Taiyi Wang, Liang Liang, Guang Yang, Thomas Heinis, and Eiko Yoneki\nparameter regions prone to causing system instabilities and de-\ngraded performance, emphasizing the need for a tuner like LITune .\nsubfigure (f) presents the cumulative number of index system fail-\nures (e.g., infinite loops, memory issues) during tuning over five\ntrials, highlighting the effectiveness of our safety design.\nRandom Search , lacking predictive or memory capabilities, ex-\nplores the parameter space indiscriminately, entering risky zones\nand introducing instability into the learned index structures. SMBO\nincorporates a model to partially understand the parameter space,\noffering semi-protected exploration but still occasionally ventures\ninto risky regions due to limited learning capacity. It tends to fo-\ncus sampling in specific areas, sometimes within dangerous zones,\nincreasing the risk of system instability. LITune (without Safe-RL) ,\nlacking a context-aware design, also struggles with risky areas but\nperforms better than other baselines because its reward function\npenalizes long runtimes, raising awareness of safe tuning regions.\nIn contrast, LITune embeds stability into exploration by leverag-\ning historical knowledge to avoid the Dangerous Zone , ensuring\nreliable and stable index structures.\n6 DISCUSSION AND CONCLUSION\nLITune is effective across diverse learned indexes, though its im-\npact varies with parameter space size and interdependencies. By\ntreating tuning as a black-box problem, it first identifies optimal\nconfigurations, then maps them back for analysis. Our studies indi-\ncate RL tolerates certain redundant parameters without harming\nperformance, provided dimensionality stays manageable. While\nwe currently rely on domain expertise to pinpoint key parame-\nters, a systematic approach could automate this. Techniques like\ncheckpointing and caching successful configurations further reduce\nretraining overhead as workloads evolve. In future work, prelimi-\nnary sensitivity analysis can prune superfluous parameters for new\nor evolving indexes.\nFurthermore, while LITune shows considerable promise in learned\nindex optimization, several areas warrant further exploration. Like\nother RL-based methods, it occasionally faces convergence and\nstability issues during training, highlighting the need for future\nresearch to address these challenges to improve system reliability.\nAdditionally, accelerating offline training (e.g., using paralleliza-\ntion) could further enhance the efficiency of LITune ‚Äôs deployment.\nDespite these challenges, LITune ‚Äôs innovative approach, combin-\ning Meta-RL and context-aware strategies, sets a new standard by\nimproving system efficiency and query performance while ensuring\nstability through proactive risk management.\nREFERENCES\n[1]Jason Ansel, Shoaib Kamil, Kalyan Veeramachaneni, Jonathan Ragan-Kelley,\nJeffrey Bosboom, Una-May O‚ÄôReilly, and Saman Amarasinghe. 2014. Opentuner:\nAn extensible framework for program autotuning. In Proceedings of the 23rd\ninternational conference on Parallel architectures and compilation . 303‚Äì316.\n[2]James Bergstra, Dan Yamins, David D Cox, et al .2013. Hyperopt: A python\nlibrary for optimizing the hyperparameters of machine learning algorithms. In\nProceedings of the 12th Python in science conference , Vol. 13. Citeseer, 20.\n[3]Surajit Chaudhuri and Vivek Narasayya. 1997. An Efficient Cost-Driven Index\nSelection Tool for Microsoft SQL Server. In Proceedings of the 23rd International\nConference on Very Large Data Bases (VLDB) . 146‚Äì155.\n[4]Supawit Chockchowwat, Wenjie Liu, and Yongjoo Park. 2023. Airindex: versatile\nindex tuning through data and storage. Proceedings of the ACM on Management\nof Data 1, 3 (2023), 1‚Äì26.\n[5]Yinlam Chow, Ofir Nachum, Edgar Duenez-Guzman, and Mohammad\nGhavamzadeh. 2018. A lyapunov-based approach to safe reinforcement learning.Advances in neural information processing systems 31 (2018).\n[6]Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. 2018.\nDeep reinforcement learning in a handful of trials using probabilistic dynamics\nmodels. Advances in neural information processing systems 31 (2018).\n[7]Douglas Comer. 1979. Ubiquitous B-tree. ACM Computing Surveys (CSUR) 11, 2\n(1979), 121‚Äì137.\n[8]Niv Dayan and Stratos Idreos. 2018. Dostoevsky: Better space-time trade-offs for\nLSM-tree based key-value stores via adaptive removal of superfluous merging. In\nProceedings of the 2018 International Conference on Management of Data . 505‚Äì520.\n[9]Jialin Ding, Ryan Marcus, Andreas Kipf, Vikram Nathan, Aniruddha Nrusimha,\nKapil Vaidya, Alexander van Renen, and Tim Kraska. 2022. Sagedb: An instance-\noptimized data analytics system. Proceedings of the VLDB Endowment 15, 13\n(2022).\n[10] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\net al.2020. ALEX: an updatable adaptive learned index. In Proceedings of the 2020\nACM SIGMOD International Conference on Management of Data . 969‚Äì984.\n[11] Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-index: a fully-dynamic\ncompressed learned index with provable worst-case bounds. Proceedings of the\nVLDB Endowment 13, 8 (2020), 1162‚Äì1175.\n[12] Matthias Feurer, Jost Springenberg, and Frank Hutter. 2015. Initializing bayesian\nhyperparameter optimization via meta-learning. In Proceedings of the AAAI\nConference on Artificial Intelligence , Vol. 29.\n[13] Qiming Fu, Zhechao Wang, Nengwei Fang, Bin Xing, Xiao Zhang, and Jianping\nChen. 2023. MAML2: meta reinforcement learning via meta-learning for task\ncategories. Frontiers of Computer Science 17, 4 (2023), 174325.\n[14] Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and Tim\nKraska. 2019. Fiting-tree: A data-aware index structure. In Proceedings of the 2019\ninternational conference on management of data . 1189‚Äì1206.\n[15] Sanket Kamthe and Marc Deisenroth. 2018. Data-efficient reinforcement learning\nwith probabilistic model predictive control. In International conference on artificial\nintelligence and statistics . PMLR, 1701‚Äì1710.\n[16] Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. 2017. Learn-\ning combinatorial optimization algorithms over graphs. Advances in neural\ninformation processing systems 30 (2017).\n[17] Minsu Kim, Jinwoo Hwang, Guseul Heo, Seiyeon Cho, Divya Mahajan, and Jongse\nPark. 2024. Accelerating String-key Learned Index Structures via Memoization-\nbased Incremental Training. arXiv preprint arXiv:2403.11472 (2024).\n[18] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2019. SOSD: A benchmark for learned\nindexes. arXiv preprint arXiv:1911.13014 (2019).\n[19] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2020. RadixSpline: a single-pass learned\nindex. In Proceedings of the third international workshop on exploiting artificial\nintelligence techniques for data management . 1‚Äì5.\n[20] Jan Kossmann, Onur Mutlu, Scott Posner, and Felix N√∂th. 2022. SWIRL: Selection\nof Workload-aware Indexes via Reinforcement Learning. In Proceedings of the\n2022 International Conference on Management of Data (SIGMOD) . 1570‚Äì1583.\n[21] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe case for learned index structures. In Proceedings of the 2018 international\nconference on management of data . 489‚Äì504.\n[22] Pengfei Li, Yu Hua, Jingnan Jia, and Pengfei Zuo. 2021. FINEdex: a fine-grained\nlearned index scheme for scalable and concurrent memory systems. Proceedings\nof the VLDB Endowment 15, 2 (2021), 321‚Äì334.\n[23] Liang Liang, Guang Yang, Ali Hadian, Luis Alberto Croquevielle, and Thomas Hei-\nnis. 2024. SWIX: A Memory-efficient Sliding Window Learned Index. Proceedings\nof the ACM on Management of Data 2, 1 (2024), 1‚Äì26.\n[24] TP Lillicrap. 2015. Continuous control with deep reinforcement learning. arXiv\npreprint arXiv:1509.02971 (2015).\n[25] Yang Liu, Wissam M Sid-Lakhdar, Osni Marques, Xinran Zhu, Chang Meng,\nJames W Demmel, and Xiaoye S Li. 2021. Gptune: Multitask learning for autotun-\ning exascale applications. In Proceedings of the 26th ACM SIGPLAN Symposium\non Principles and Practice of Parallel Programming . 234‚Äì246.\n[26] Baotong Lu, Jialin Ding, Eric Lo, Umar Farooq Minhas, and Tianzheng Wang.\n2021. APEX: a high-performance learned index on persistent memory. arXiv\npreprint arXiv:2105.00683 (2021).\n[27] Ryan Marcus, Emily Zhang, and Tim Kraska. 2020. Cdfshop: Exploring and\noptimizing learned index structures. In Proceedings of the 2020 ACM SIGMOD\nInternational Conference on Management of Data . 2789‚Äì2792.\n[28] Dingheng Mo, Fanchao Chen, Siqiang Luo, and Caihua Shan. 2023. Learning to\nOptimize LSM-trees: Towards A Reinforcement Learning based Key-Value Store\nfor Dynamic Workloads. arXiv preprint arXiv:2308.07013 (2023).\n[29] Yoshihiko Ozaki, Yuki Tanigaki, Shuhei Watanabe, and Masaki Onishi. 2020.\nMultiobjective tree-structured parzen estimator for computationally expensive\noptimization problems. In Proceedings of the 2020 genetic and evolutionary com-\nputation conference . 533‚Äì541.\n[30] Kostas Patroumpas and Timos Sellis. 2006. Window specification over data\nstreams. In International Conference on Extending Database Technology . Springer,\n\nA New Paradigm in Tuning Learned Indexes: A Reinforcement Learning Enhanced Approach SIGMOD ‚Äô25, June 22-27, 2025, Berlin, Germany\n445‚Äì464.\n[31] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\n2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347\n(2017).\n[32] Tarique Siddiqui and Wentao Wu. 2024. ML-Powered Index Tuning: An Overview\nof Recent Progress and Open Challenges. ACM SIGMOD Record 52, 4 (2024),\n19‚Äì30.\n[33] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. 2012. Practical bayesian\noptimization of machine learning algorithms. Advances in neural information\nprocessing systems 25 (2012).\n[34] Mihail Stoian, Andreas Kipf, Ryan Marcus, and Tim Kraska. 2021. Towards\nPractical Learned Indexing. arXiv preprint arXiv:2108.05117 (2021).\n[35] Hao Sun, Ziping Xu, Zhenghao Peng, Meng Fang, Taiyi Wang, Bo Dai, and\nBolei Zhou. 2022. Constrained MDPs can be Solved by Eearly-Termination\nwith Recurrent Models. In NeurIPS 2022 Foundation Models for Decision Making\nWorkshop .\n[36] Zhaoyan Sun, Xuanhe Zhou, and Guoliang Li. 2023. Learned Index: A Com-\nprehensive Experimental Evaluation. Proceedings of the VLDB Endowment 16, 8\n(2023), 1992‚Äì2004.\n[37] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An intro-\nduction . MIT press.\n[38] Chuzhe Tang, Youyun Wang, Zhiyuan Dong, Gansen Hu, Zhaoguo Wang, Minjie\nWang, and Haibo Chen. 2020. XIndex: a scalable learned index for multicore data\nstorage. In Proceedings of the 25th ACM SIGPLAN symposium on principles and\npractice of parallel programming . 308‚Äì320.\n[39] Grubb Valentin, Michael Zuliani, Diego Zilio, Guy M. Lohman, and Alan Skelley.\n2000. DB2 Advisor: An Optimizer Smart Enough to Recommend Its Own Indexes.\nInProceedings of the 16th International Conference on Data Engineering (ICDE) .\n101‚Äì110.\n[40] Dana Van Aken, Andrew Pavlo, Geoffrey J Gordon, and Bohan Zhang. 2017.\nAutomatic database management system tuning through large-scale machine\nlearning. In Proceedings of the 2017 ACM international conference on management\nof data . 1009‚Äì1024.\n[41] Linnan Wang, Yiyang Zhao, Yuu Jinnai, Yuandong Tian, and Rodrigo Fonseca.\n2020. Neural architecture search using deep neural networks and monte carlotree search. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 34.\n9983‚Äì9991.\n[42] Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Lan-\nglois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. 2019. Bench-\nmarking Model-Based Reinforcement Learning. arXiv preprint arXiv:1907.02057\n(2019).\n[43] Zijia Wang, Haoran Liu, Chen Lin, Zhifeng Bao, Guoliang Li, and Tianqing Wang.\n2024. Leveraging Dynamic and Heterogeneous Workload Knowledge to Boost\nthe Performance of Index Advisors. Proceedings of the VLDB Endowment 17, 7\n(2024), 1642‚Äì1654.\n[44] Jiacheng Wu, Yong Zhang, Shimin Chen, Jin Wang, Yu Chen, and Chunxiao\nXing. 2021. Updatable learned index with precise positions. arXiv preprint\narXiv:2104.05520 (2021).\n[45] Jiacheng Wu, Yong Zhang, Shimin Chen, Jin Wang, Yu Chen, and Chunxiao\nXing. 2021. Updatable learned index with precise positions. arXiv preprint\narXiv:2104.05520 (2021).\n[46] Guang Yang, Liang Liang, Ali Hadian, and Thomas Heinis. 2023. FLIRT: A Fast\nLearned Index for Rolling Time frames.. In EDBT . 234‚Äì246.\n[47] Wei Ying, Yu Zhang, Junzhou Huang, and Qiang Yang. 2018. Transfer learning\nvia learning to transfer. In International Conference on Machine Learning . PMLR,\n5085‚Äì5094.\n[48] Jiaoyi Zhang and Yihan Gao. 2021. Carmi: A cache-aware learned index with a\ncost-based construction algorithm. arXiv preprint arXiv:2103.00858 (2021).\n[49] Ji Zhang, Yu Liu, Ke Zhou, Guoliang Li, Zhili Xiao, Bin Cheng, Jiashu Xing,\nYangtao Wang, Tianheng Cheng, Li Liu, et al .2019. An end-to-end automatic\ncloud database tuning system using deep reinforcement learning. In Proceedings\nof the 2019 International Conference on Management of Data . 415‚Äì432.\n[50] Xinyang Zhao, Xuanhe Zhou, and Guoliang Li. 2023. Automatic database knob\ntuning: a survey. IEEE Transactions on Knowledge and Data Engineering 35, 12\n(2023), 12470‚Äì12490.\n[51] Wei Zhou, Chen Lin, Xuanhe Zhou, and Guoliang Li. 2024. Breaking It Down:\nAn In-Depth Study of Index Advisors. Proceedings of the VLDB Endowment 17,\n10 (2024), 2405‚Äì2418.",
  "textLength": 89297
}