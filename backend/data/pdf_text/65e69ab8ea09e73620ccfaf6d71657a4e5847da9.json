{
  "paperId": "65e69ab8ea09e73620ccfaf6d71657a4e5847da9",
  "title": "LiLIS: Enhancing Big Spatial Data Processing with Lightweight Distributed Learned Index",
  "pdfPath": "65e69ab8ea09e73620ccfaf6d71657a4e5847da9.pdf",
  "text": "JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1\nLiLIS: Enhancing Big Spatial Data Processing with\nLightweight Distributed Learned Index\nZhongpu Chen, Wanjun Hao, Ziang Zeng, Long Shi, Yi Wen, Zhi-Jie Wang, and Yu Zhao\nAbstract —The efficient management of big spatial data is\ncrucial for location-based services, particularly in smart cities.\nHowever, existing systems such as Simba and Sedona, which\nincorporate distributed spatial indexing, still incur substantial\nindex construction overheads, rendering them far from optimal\nfor real-time analytics. Recent studies demonstrate that learned\nindices can achieve high efficiency through a well-designed\nmachine learning model, but how to design learned index for\ndistributed spatial analytics remains unaddressed. In this article,\nwe present LiLIS, a Lightweight distributed Learned Index for\nbig Spatial data. LiLIS amalgamates machine-learned search\nstrategies with spatial-aware partitioning within a distributed\nframework, and efficiently implements common spatial queries,\nincluding point query, range query, k-nearest neighbors ( kNN),\nand spatial joins. Extensive experimental results over real-world\nand synthetic datasets show that LiLIS outperforms state-of-the-\nart big spatial data analytics by 2-3 orders of magnitude for most\nspatial queries, and the index building achieves 1.5-2 ×speed-up.\nThe code is available at https://github.com/SWUFE-DB-Group/\nlearned-index-spark.\nIndex Terms —Learned index, spatial data, spatial databases,\ndistributed analytics.\nI. I NTRODUCTION\nTTHE widespread adoption of GPS-enabled devices has\nled to the generation of extensive spatial data, enabling\nthe collection and dissemination of trajectory information\nthrough location-based services (LBS) such as Google Maps,\nUber, and geo-tagged social media platforms [1]–[4]. This\ndevelopment provides researchers with unprecedented oppor-\ntunities to mine large scale spatial data. For example, DiDi\nChuxing1, a leading ride-sharing platform in China, receives\nmore than 25 million trip orders and produces over 15 bil-\nlion location points per day [5]. To efficiently manage the\nvoluminous spatial datasets, technological innovations in terms\nof distributed storage and processing have been proposed to\nhandle the limitation of a single-node machine [6], [7]. State-\nof-the-art big spatial data analytics, such as Simba [8] and\nSedona [9], are able to provide high throughput by provid-\ning inherent spatial indexing and optimization. Nevertheless,\ncurrent solutions which rely on traditional spatial indices\nsuch R-tree [10] and Quadtree [11] are still hindered by\nconsiderable index construction overhead and runtime query\nZ. Chen, W. Hao, Z. Zeng, L. Shi (corresponding author),\nand Y. Zhao are with the School of Computing and Artificial\nIntelligence, Southwestern University of Finance and Economics,\nChengdu 611130, China. Email: zpchen@swufe.edu.cn, {222081202012,\n224081200045 }@smail.swufe.edu.cn ,{shilong, zhaoyu }@swufe.edu.cn.\nY. Weng and Z.-J. Wang are with the College of Computer\nScience, Chongqing University, Chongqing 400044, China. Email:\n20230490@stu.cqu.edu.cn, cszjwang@cqu.edu.cn .\n1https://web.didiglobal.com/cost. Therefore, developing efficient approaches to manage and\nprocess large-scale spatial data remains a critical challenge for\nmodern data systems [12].\nIn recent years, learned indices, which utilize machine\nlearning (ML) models to predict the data position directly,\nhave paved the way to efficient search with a succinct data\nstructure [13], [14]. Subsequently, various learned indices over\nspatial data are proposed to support two-dimensional spatial\ndata access [15]–[19], reducing the search time from O(logN)\ntoO(1)given Nspatial objects. At a high level, a learned\nindex can be seen as a function which is able to map the\nkey (e.g., spatial coordinates in our work) to the position of\nunderlying data directly with a pre-defined error bound, as\nillustrated in Figure 1. However, to the best of our knowledge,\nthere is no any big spatial system that is integrated with learned\nindex.\n(x, y, v)Modeltrain\nerror\nboundpredict\nFig. 1. An illustration of a learned index in which the key is a spatial\ncoordinate (x, y)with a value v, and a model maps the key to the position\nin memory/disk directly with an error bound.\nThere are several key challenges to design learned spatial\nindices for distributed analytics. Firstly, training a machine\nlearning model to achieve high-quality predictions is no\ntrivial task, as it often involves a time-consuming training\nprocess, especially when dealing with large-scale datasets\n(C1). Secondly, most model-based learned index methods are\nnot compatible with existing query paradigms, necessitating\nsignificant refactoring efforts ( C2). Last but not the least,\nexisting learned index methods support only a limited range\nof spatial queries, restricting their applicability in diverse real-\nworld scenarios ( C3). To bridge the gap mentioned above,\nwe present a L ightweight di stributed L earned I ndex for big\nSpatial data systems, dubbed as LiLIS. Specifically, In order\nto address those challenges, several novel techniques in terms\nof distributed learned spatial index are developed in this paper:arXiv:2504.18883v3  [cs.DB]  6 May 2025\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2\n•We leverage an error-bounded spline interpolation to\nimplement LiLIS by learning the two-dimensional distri-\nbution of the underlying spatial data. This learned index\ncan be realized with very few parameters using simple\nyet effective interpolating techniques, making the spatial\nindex considerable lightweight compared to traditional\nspatial indices (e.g., R-tree and KD-tree), and it also\nserves as the foundation for efficient spatial queries. ( C1)\n•Unlike other learned indices, LiLIS is compatible with\nexisting execution engines of big data systems, and thus\nprovides native features such as scalability and fault\ntolerance. For example, LiLIS can utilize spatial-aware\npartitioning and two-phase pruning to boost the perfor-\nmance. The LiLIS prototype implemented on Apache\nSpark is straightforward without hacking the underlying\nexecution engine. ( C2)\n•LiLIS employs a novel decoupling of data partitioning\nand indexing components, enabling seamless extension\nto support diverse spatial query types including point\nqueries, range queries, k-nearest neighbors ( kNN), and\nspatial joins. Notably, while maintaining the same learned\nindex model architecture, LiLIS is able to adapt different\npartitioning strategies based on data and query distribu-\ntions. ( C3)\nIn summary, our contributions are threefold:\n1) We designed LiLIS, a lightweight distributed learned\nindex for big spatial data. To the best of our knowledge,\nthis is the first work to introduce spatial learned indices\nto big data analytics.\n2) We implemented efficient distributed spatial algorithms\nto support common spatial queries, including point\nquery, range query, kNN, and spatial join based on LiLIS\non Apache Spark.\n3) Our comprehensive experimental evaluation on both\nreal-world and synthetic datasets demonstrates LiLIS’s\nsignificant performance advantages. The results indicate\nthat LiLIS achieves a 2-3 order of magnitude speed\nimprovement over state-of-the-art big spatial analytics\nsystems for most queries, while also delivering 1.5-2×\nfaster index construction.\nThe remainder of this article is organized as follows: Sec-\ntion II reviewed the related work. The overview design and\nkey methodology of LiLIS are described in Section III. We\ndiscussed the details of efficient distributed spatial algorithms\nin Section IV. Section V presents the results of our experi-\nments, providing insights about the efficiency and scalability\nof LiLIS, and Section VI concludes this article and suggests\ndirections for future research.\nII. R ELATED WORK\nA. Spatial Data Indexing\nA spatial index is a carefully designed structure engineered\nto manage spatial objects by leveraging their spatial attributes,\nsuch as location, geometry, properties, and relationships [20].\nSpatial indices are widely studied and commonly implemented\nin mainstream spatial databases, with tree-based index struc-\ntures (e.g., KD-tree [21], Quadtree [22], and R-tree [23])being the most commonly used. Tree-based spatial indices are\ntypically designed as hierarchical structures, partitioning space\nrecursively. Consequently, for most queries, they can generally\nachieve an average-case time complexity of O(logN), where\nNrepresents the number of spatial objects. In addition to\ntree-based index structures, a simple yet effective technique\nis grid partitioning, which divides the two-dimensional data\nspace into fixed-size or adaptively adjusted grid cells [24],\n[25].\nConventional spatial indexing schemes mentioned above\nare typically designed as general-purpose structures without\noptimizations aligned to particular data distribution patterns or\ndomain-specific characteristics. Consequently, when applied to\nlarge-scale, high-dimensional datasets, they frequently suffer\nfrom reduced query throughput and incur significant storage\nand I/O overheads [26]–[28]. Therefore, there is a compelling\nneed for a lightweight yet efficient solution to mitigate these\nchallenges.\nB. Learned Indices\nA learned index is an advanced methodology that leverages\nmachine learning models to replace conventional database\nindexing structures, such as B-trees and hash tables. The core\nprinciple of a learned index is to develop a function that\nmaps a search key to the corresponding storage address of\na data object, thereby optimizing query performance [14],\n[29]–[31]. T. Kraska et al. [13] first introduced the concept\nof learned indexes, and they proposed the Recursive Model\nIndex (RMI), which hierarchically models data distributions\nthrough a multi-stage neural network [32]. However, while\ntheir foundational work demonstrates that a learned index\ncan deliver up to a threefold reduction in search time and\nan order-of-magnitude decrease in memory footprint [13], it\nremains confined to static, read-only workloads. Subsequently,\nJ. Ding et al. introduced ALEX [33], an updatable learned\nindex that adaptively partitions data to sustain high query and\ninsert performance under dynamic workloads. Beyond the one-\ndimensional learned indices, later work further extends them\ninto multi-dimensional settings [34]. The Z-order model [16]\nextends RMI to spatial data. Their method uses a space-filling\ncurve to order points and learns the Cumulative Distribution\nFunction (CDF) to map each point’s key to its rank. The\nrecursive spatial model index [35] further develops these\nprinciples by employing the rank space–based transformation\ntechnique [36].\nAlthough research on learned indices has significantly en-\nhanced query performance, current studies are confined to\nsingle-machine environments and fall short in addressing\nlarge-scale data processing. At present, there is a significant\nresearch gap in the field of distributed learned indices.\nC. Distributed Spatial Analytics\nTo process large scale spatial data, a large body of big\nspatial analytics have been proposed over the years [37], [38].\nSimba [8] enhances SparkSQL with DataFrame based spatial\nqueries using two-stage RDD indexing. SpatialSpark [39]\nimplements node-local R-trees on Spark RDDs to minimize\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3\ndata transfer. GeoMesa [40] provides spatio-temporal indexing\nacross multiple database backends with Kafka integration.\nAmong these, Apache Sedona [41] stands as state-of-the-\nart solution, and it delivers superior performance for large-\nscale geospatial processing through its innovative distributed\nindexing and query optimization architecture.\nExisting distributed spatial systems typically rely on tradi-\ntional indexing methods [19], such as R-trees or grids, which\nimpose considerable I/O and memory overhead during both\nconstruction and querying. However, there is no any distributed\nspatial analytics equipped with learned spatial indies yet.\nIII. L ILIS D ESIGN\nHDFS\nCSV JSON SHPLegend\nWell\nRiver\nLake\n2 km...RDDSpatial RDD Learned IndexSpatial-aware PartitionerPoint Query Range Query kNN JOIN ...Find all cof fee shops within this area\nFig. 2. The architecture of LiLIS on Apache Spark. It introduces spatial RDD\nwith learned index, and supports a wide range of spatial queries.\nIn this paper, we mainly adopt Apache Spark, one of\nthe most popular distributed computing engine nowadays, as\nthe underlying framework to implement LiLIS to support\nspatial points, and the architecture is illustrated in Figure 2.\nAs we can see, it introduces an extra wrapper beyond the\nresilient distributed dataset (RDD) so that LiLIS is able to\nsupport 1) spatial functions (e.g., geometry transformation\nand distance computation); and 2) spatial predicates (e.g.,\ncontains ,intersects andwithin ). Notably, a leaned\nindex component is implemented within partitions of spatial\nRDDs, which are generated by several spatial-aware partition\nstrategies. By decoupling indexing and partitioning, LiLIS\napplies learned models to replace traditional spatial search\nwith a learned alternative while maintaining compatibility with\nexisting components in distributed frameworks, enabling the\nfeasible implementation of a diverse range of spatial query\nalgorithms. It is worthy to note that the overall design of LiLIS\nis independent with the specific distributed framework, and the\nporting to alternatives such as Apache Flink is left as a future\nwork. In what follows, the design of spatial-aware partitioner\nand learned index are detailed.A. Spatial-aware Partitioner\nTypically, data in a distributed system shall be partitioned\namong machines in a cluster. For the sake of load balance and\ndata locality, data-aware partitioners beyond plain range and\nhash strategies are designed to boost the performance [42].\nTherefore, LiLIS adopts the similar method used in Simba [8]\nto leverage the spatial locality. For the large scale spatial data,\nwe observed that partitioning based on sampling can achieve\na great trade-off between efficiency and effectiveness. In this\npaper, we set sampling rate to 1% in a uniform way. It is\nworth to note that partitioner itself can be seen as a global\nindex, and LiLIS provides several built-in grid-based (e.g.,\nfixed and adaptive grid) and tree-based (e.g., R-tree, Quadtree,\nand KD-tree) spatial-aware partitioning methods to adapt for\na wide range of applications. As for tree-based partitioners,\nLiLIS only maintains the leaves as the partitions. When there\nis no ambiguity, the leaves generated from tree-based methods\nare also called grids in this paper. Notably, since R-tree\nis generally built in a bottom-up way, the sampling-based\npartitioning may not cover all spatial objects. Therefore, we\nintroduce a novel concept in LiLIS, dubbed as an overflow\ngrid, referring to a special grid for all remaining overflowed\nspatial objects that do not belong to any leaf in R-tree.\nSubsequently, each grid is assigned to a unique identifier (an\nint in this paper) for re-partition. Without loss of generality,\nwe assume the partitioner is R-tree based, and the partitioning\nalgorithm is illustrated in Algorithm 1.\nAlgorithm 1: Partitioning algorithm in LiLIS\nData: Spatial RDD R\n1SR←sampling over R;\n2Construct grids list Gthrough the specified method\noverSR;\n;/*Outer foreach is a parallel map\nin Apache Spark */\n3foreach object o∈Rdo\n4 isOverflow ←True ;\n5 foreach grid(id, g)∈enumerate (G)do\n6 ifgcontains othen\n7 R∗.add(id,o);\n8 isOverflow ←False ;\n9 break;\n10 end\n11 end\n12 ifisOverflow then\n13 R∗.add(len( G),o);\n14 end\n15end\n16Re-partition over R∗by keys;\n17Map (id, o)toooverR∗;\nThe outer for-loop (Lines 3-15) is indeed a parallel map\noperation in Apache Spark, and each spatial object owill be\nmapped into a tuple (id, o), where idis the identifier of the\ngrid in which olocates. The enumerate function borrows from\nPython (Line 5), emitting a tuple containing an id(starting\nfrom 0) and the grid obtained from iterating over G. As for\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4\noverflowed object, its partition idwill be assigned to the size\nof grids (Lines 12-14). Therefore, a spatial RDD Rwill be\ntransformed into a key-value RDD R∗, and a re-partitioning\nprocedure (Line 16) through shuffling can be conducted over\nR∗by the key (i.e., the idof the partition). Finally, R∗will be\nmapped back into a spatial RDD with objects only (Line 17).\nSince the partitioner acts as a global index, the master node\nmust maintain all partitions’ properties within G, including\ntheir spatial boundaries and identifiers.\nB. Distributed Learned Spatial Index\nTo keep the compatibility with existing systems [8], [9],\n[42], LiLIS follows the two-phase filtering solution, and the\nlocal index is implemented via a learned model within a given\npartition. Assume that a spatial object is (x, y, v ), where x,\ny, and vis its latitude, longitude, and extra attribute (e.g., the\ntextural description), respectively. The two-dimensional spatial\npoints shall be projected into one dimension for sorting, and\npossible sorting criteria can be either one arbitrary axis or\nsome aggregated value (e.g., Z-order curve and GeoHash). In\nthis paper, we denote the chosen sorting criteria as the keyof\na spatial object, so each spatial point can be represented as a\ntuple of (key, x, y, v ).\nThe main idea of the learned spatial index in LiLIS is to\nconstruct a spline model so that it can predicate the position of\nevery spatial point with error guarantees [43]. To be specific,\ngiven an object (key, x, y, v )whose position in sorted dataset\nDisp, the spline model Scan be described as S(key) = ˆp±ϵ,\nwhere ϵis a pre-specified position error bound so that |ˆp−p| ≤\nϵ. To build model Sover sorted dataset D, we need to find a\nset of representative objects so that the maximum interpolation\nerror is not larger than ϵ, as illustrated in Figure 3. As for\nthe one-pass error-bounded spline algorithms that construct\nthe model, we refer the readers to [44]. Given a partition P\nwith Nspatial objects, the distributed learned spatial index\nis conducted by mapPartitions in Apache Spark, without\ntime-consuming data shuffling between machines.\nCompared to the commonly-used R-tree index using Sort-\nTile-Recursive (STR) algorithm [45] whose time complexity is\nO(NlogN+Nlogf×logfN), where fis the fanout, the time\ncomplexity of the learned index in LiLIS is O(NlogN+N).\nAs for searching, the time complexity of the learned index is\nO(logM), where M≪Nis the size of spline points set. As\nan optimization, the searching time can be further deduced to\nbe constant on average if a radix table is introduced [43].\nThe radix table was proposed to handle the unsigned integer\nkeys (e.g., uint32 anduint64 ) only. To this end, LiLIS\nextends this design to support floating numbers and strings to\nfit spatial data. Algorithm 2 outlines how to build a radix table\nfor floating keys. The size of the radix table is bounded by\na user-defined number of bits b. Generally, this radix table is\nto compress the spline points set (Lines 6-14) with a scaling\nfactor f(Line 3). Therefore, given a search key k, letk′be\n(int)(k−min)×f, the lower bound and upper bound position\nareS[T[k′]]andS[T[k′+ 1]] , respectively.\nRemark 1 : Converting a string to an unsigned integer is a\nwell-established process, as it can be efficiently accomplished\n(kl, pl)(kr, pr)\n(k,ˆp)\n≤ϵ\nStep 1:\nˆp=pl+ (k−kl)pr−pl\nkr−klStep 2:\nbinary search\nwithin ˆp±ϵ\nkeypositionFig. 3. The main idea of spline index is to obtain an estimated position ˆp\nof a given key kbased on two adjacent points (Step 1), and then perform a\nbinary search within positions ˆp±ϵin sorted dataset (Step 2). The overall\nsearching time complexity is constant after retrieving the lower bound kland\nupper bound kr.\nAlgorithm 2: Building a radix table for floating keys\nInput: Spline points S, number of radix bits b\nOutput: Radix table T\n1T←new int[1 ≪b + 2];\n2min, max ←S.first.key, S.last.key ;\n3f←(1≪b)/(max−min);\n4T[0]←0;\n5prev←0;\n6foreach point (i, p)∈enumerate (S)do\n7 curr←(int)((p.key−min)×f);\n8 foreach j∈[prev + 1, curr ]do\n9 T[j] =i;\n10 end\n11end\n12for ( ;prev < T.size −1;prev ++){\n13 T[prev + 1]←n−1;\n14}\n15return T;\nby leveraging robust, thoroughly validated hash code compu-\ntation techniques, such as the product-sum method, which is\nnatively implemented in programming languages like Java.\nC. Discussions on LiLIS Design\n1) Enhancing Rather Than Replacing: Prior work on\nlearned indices often focuses on replacing traditional indexing\nstructures, which necessitates significant refactoring of sys-\ntem architectures and algorithm implementations. In contrast,\nLiLIS adopts a complementary approach by enhancing the\nexisting computation engine without requiring disruptive over-\nhauls, thus reducing adoption barriers for real-world systems.\n2) Lightweight: Motivated by the objective of enhancing ef-\nficiency, LiLIS prioritizes a lightweight design. Consequently,\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5\nconstructing a learned index with LiLIS entails minimal over-\nhead due to its reliance on a limited number of parameters.\nFurthermore, an additional benefit lies in the ability to develop\na dual-indexing framework, enabling the selection of an appro-\npriate index based on specific requirements. For instance, when\neither xoryserves as the key, the construction of an R-tree\nand a learned index can leverage a shared sorting procedure,\nthereby optimizing the indexing process.\nIV. Q UERY ALGORITHMS IN LILIS\nIn this section, we discuss how to design efficient query\nalgorithms based on the learned index proposed in Section III.\nA. Point Query\nGiven a query point q= (x, y)and dataset D, a point query\nreturns true if qis within D, and returns false otherwise.\nObviously, there is at most one partition that is overlapped\nwithq, and this can be done with RDD’s filter method. If\nthe candidate partition exists, then we use the learned index\nto predict the estimated position, and refine the final result\naccording to Algorithm 3. As illustrated in Figure 3, we first\ncompute the estimated position (Line 1), and then search the\nkey within the error bound (Line 2). If there is no such key,\nthe algorithm returns false directly (Lines 3-5). Otherwise, we\nscan in both directions (Lines 6-17).\nAlgorithm 3: Search in point query\nInput: Candidate partition P, and query point q\nOutput: True if q∈ P, Flase otherwise\n1ˆp← P.learnedSearch (q.key );\n2p←search q.key overP[ˆp−ϵ..ˆp+ϵ];\n3ifp==null then\n4 return False ;\n5end\n6pos←p;\n7while p <P.size andP[p].key ==q.key do\n8 ifq.x==P[p].xandq.y==P[p].ythen\n9 return True;\n10 end\n11 p++;\n12end\n13p←pos−1;\n14while p≥0andP[p].key ==q.key do\n15 ifq.x==P[p].xandq.y==P[p].ythen\n16 return True;\n17 end\n18 p–;\n19end\n20return False ;\nB. Range Query\nGiven a rectangle range q= (xl, yl, xh, yh), where (xl, yl)\nand(xh, yh)are the lower left and upper right point, respec-\ntively, a spatial range query returns all points in Dcontained\ninq. Similar to the point query, the globally filtering can beconducted through a linear scan directly. After obtaining the\ncandidate partitions, we apply mapPartitions to compute\nthe resulting points. The overall procedure is similar to Algo-\nrithm 3. To be specific, we search from lower bound (xl, yl)\nto upper bound (xh, yh)in a given partition. In practice, it\ncan be optimized from several aspects. For example, if the\npartition is enveloped entirely within q, then all points should\nbe returned without further checking.\nRemark 2 : The range query above is indeed a rectangle\nrange query. As for a circle range query with a center point\np= (x, y)and a radius r, a practical solution is to construct\na minimal bounding rectangle (MBR) for this circle, and per-\nform a regular rectangle range query first. Finally, a subsequent\nrefinement step is necessary to filter out false positives.\nC.kNN Query\nFormally, given a query point q= (x, y), dataset D, and the\ndistance function d(·,·), akNN query returns R ⊂ D so that\n|R|=k, and for any point o∈ D\\R , and point p∈ R, it holds\nthatd(q, p)≤d(q, o). In LiLIS, the distributed kNN query is\nimplemented with the range query. Firstly, an estimated range\nis computed according to the following equations [46]:\nr=p\nk/(πd) (1)\nwhere dis density of the dataset, given by\nd=N/area (2)\nwhere Nandarea are the size and area of D, respectively.\nThe first phase of kNN search is to conduct a range query\nwith ˆq= (xq−r, yq−r, xq+r, yq+r). If the size of the\nresults is less than k, a new searching window is constructed.\nFork >1, the times of calling the range query is theoretically\nbounded:\n⌈lnp\n(xu−xl)2+ (yu−yl)2−lnq\nk(xu−xl)(yu−yl)\nπN\nln4k\nπ(k−1)⌉(3)\nwhere (xl, yl)and(xu, yu)are the lower bound and upper\nbound of D, respectively. For the detailed proof, we refer the\nreader to [46]. In practice, we found that most kNN (k <10)\ncan be answered using no more than twice of range queries.\nD. Join Query\nA spatial join takes two datasets ( AandB) and a spatial\npredicate ( θ) as the input, and it returns a set of pairs in which\nthe predicate holds, i.e., σθ(A × B ). Due to the diversity of\nθ, the implementations of join queries exhibit significant vari-\nation. In LiLIS, we currently provide the support for the join\nbetween polygons PGand data points D, and the predicate\ncontains is to check whether a data point is contained in\na polygon. For example, the data points can be shops in a\ncity, and the polygons delineate the commercial zones that are\nselected on-the-fly. A meaningful join query in this context\ncould aim to identify which shops fall within each commercial\nzone, enabling an analysis of retail density and distribution\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6\npatterns. In a practical setting, |PG| ≪ |D|, so polygons can\nbe broadcasted to each partition before performing join. As for\nthe filtering stage, we can compute MBR for a polygon, and\nthen perform a range query whose range is the MBR above\nto get candiate tuples.\nRemark 3 : LiLIS also support a few other types of joins\n(e.g., kNN-join) out of the box, but how to leverage the learned\nindex to design a unified framework for more types of joins\nis left as a future work.\nV. E XPERIMENT\nIn this section, we conduct extensive experiments to demon-\nstrate the superiority of LiLIS over both real-world and syn-\nthetic datasets. All algorithms in this paper were implemented\nin Java, and all experiments were performed in a cluster with 7\nCentOS machines on Tencent Cloud, in which each machine\nis shipped with 8 GB memory, 80 GB SSD and Intel Duo\nProcessor of 3.0 GHz. As mentioned before, the design of\nLiLIS is independent with the specific distributed engine, and\nwe present it on Apache Spark 3.0 using Standalone mode,\nin which one machine is chosen as the master node, and the\nremaining 6 machines are slave nodes. All configurations in\nApache Spark, including garbage collection (GC) and shuffle\noperations, were maintained at their default settings for the\nexperiments.\nA. Setup\n1) Datasets: The datasets used in the experiments are sum-\nmarized in Table I. Both CHI2and NYC3contain extra infor-\nmation in addition to spatial locations, while SYN consists of\nspatial points only, which are generated by Spider4. As we can\nsee, although it is possible to load CHI and SYN in a single-\nnode machine, the performance in a distributed setting among\ndifferent frameworks is a meaningful research question, and\nthe evaluation is enough to demonstrate the effectiveness and\nefficiency of LiLIS. As for spatial join queries, we constructed\n7000 polygons in Chicago, and perform a spatial join with\nCHI. For other queries, we use NYC as the default dataset.\nTABLE I\nSPATIAL DATASETS\nName Size #items Description\nCHI 1.9 GB 7M Crime events in Chicago\nNYC 20 GB 300M New York taxi rides\nSYN 3 GB 100M Randomly generated points\nRemark 4 : While it is true that the default dataset (i.e.,\nNYC) can be accommodated within the memory of a single\ncontemporary machine, the primary objective of this study is\nto demonstrate the feasibility and efficacy of LiLIS within\ndistributed cloud environments. In such settings, cost-effective\ncommodity hardware with limited memory capacities (e.g., 8\nGB), as utilized in our experiments, is typically employed,\nhighlighting the system’s scalability and performance under\nresource-constrained conditions.\n2https://data.cityofchicago.org/\n3https://www.nyc.gov/site/tlc/about/data.page\n4https://spider.cs.ucr.edu/2) Baselines and Implementations: For evaluation, we com-\npare LiLIS with the state-of-the-art big spatial analytics Se-\ndona [9]. Sedona is shipped with two indices (including R-tree\nand Quadtree) and two partitioners (including KD-tree and\nQuadtree), and we use Sedona- {I}{P}to denote its various\nimplementation variants, in which {I}and{P}mean the\nindex type and partitioner, respectively. For example, Sedona-\nRK uses R-tree as the index while adopts KD-tree as the\npartitioner. The four variants of Sedona are summarized in\nTable II. Notably, Sedona also allows the no-indexing solution,\nand we donate it as Sedona-N. As for the implementation using\nvanilla Apache Spark, we donate it as Spark. As discussed\nin Section III, LiLIS can adopt different partitioners, we use\nLiLIS-F, LiLIS-A, LiLIS-Q, LiLIS-K, and LiLIS-R to denote\nthe implementations with fixed-grid, adaptive-grid, quadtree,\nKD-tree, and R-tree, respectively. In our experiments, we\nadopt the KD-tree as the default partitioning method for LiLIS.\nConsequently, LiLIS-K serves as the default implementation\nin our proposed framework.\nTABLE II\nSEDONA VARIANTS\nIndexPartitionerKD-tree Quadtree\nR-tree Sedona-RK Sedona-RQ\nQuadtree Sedona-QK Sedona-QQ\nUnlike other learned indices requiring complicated param-\neters and tuning skills, there are only two hyperparameters in\nLiLIS, i.e., error bound (default is 32) and number of spline\nbits (default is 10). The similar setting is also adopted in [43].\n3) Workloads: Unlike the workload in Sedona [9] which\nonly focuses on different types of spatial queries, in this paper\nwe further consider more subtle factors for range queries [27].\nTo be specific, the selectivity, which is the ratio between the\narea of a query window and the area of the whole dataset,\nranges from 0.00001% (default) to 0.1%. In addition, we\nfurther adjust skewness for range queries, in which a skewed\nquery (default) follows the distribution of the underlying\nspatial data, while a uniform query is to construct the input\nrandomly. For kNN queries, the default kis 10, ranging from\n1 to 1000. To mitigate randomness, all experimental results\nare averaged over 50 independent runs.\n4) Research Questions: In this paper, we mainly focus on\nthe following research questions in terms of LiLIS through\ncomprehensive experimental evaluations:\n•RQ1 : How good is LiLIS for common distributed spa-\ntial queries compared to the state-of-the-art big spatial\nanalytics?\n•RQ2 : What is the effect of different partitioners in LiLIS?\n•RQ3 : How good is LiLIS across different datasets?\n•RQ4 : What is the effect of subtle factors for range (e.g.,\nvarying selectivities) and kNN (e.g., varying k) queries\nin LiLIS?\n•RQ5 : What is the cost of building indices in LiLIS?\nB. RQ1: Overall Performance\nTo answer RQ1, we compare LiLIS-K with all other alter-\nnative big spatial analytics for range queries, point queries,\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7\nandkNN under the default settings. Because Quad-tree index\nin Sedona does not support kNN, we only report Sedona-RQ\nand Sedona-RK for kNN queries. As for join queries, since the\nspatial join predicate used in LiLIS is not found in Sedona, we\nonly compare LiLIS-K with Apache Spark. The evaluation re-\nsults are illustrated in Figure 4. As we can see, LiLIS performs\nconsistently the best compared with other implementations.\nSince the y-axis in Figure 4 uses a logarithmic scale, the\nactual performance improvement is significantly greater than\nthe observed values suggest. For example, LiLIS-K takes about\n472 millisecond for range queries, while Sedona-RQ takes\n521282 millisecond. Interestingly, we can also observe that\nSedona variants with indices are not better than Sedona-N and\nSpark under our settings, and this phenomenon is caused by\nthe underlying data distribution as well as its Filter and Refine\nmodel. The authors of Sedona [9] also verify this finding.\nLilis-K Sedona-RQ Sedona-RK Sedona-QQ Sedona-QK Sedona-N Spark102103104105106\n4.41\n12.99\n12.94\n12.9\n12.85\n13.11\n96.16\n13.16\n12.95\n12.88\n12.78\n12.55\n9.736.48\n13.73\n13.58\n12.66\n10.246.43\n13.23Time (ms)\nPoint Range kNN Join\nFig. 4. The overall performance under default settings.\nTakeaway 1\nLiLIS is the fastest for spatial points, and it generally\noutperforms competitors by 2–3 orders of magnitude.\nAs for RQ1, We also conducted the experiments in terms\nof throughput ( jobs per minute ) of LiLIS and its competitors\nof point and range queries. To be specific, we followed the\nsimilar settings in Simba [8], and issued 100 queries on\nthe driver program using a thread-pool with 8 threads. The\nresults are shown in Figure 5. It can be observed that LiLIS-\nK consistently achieves higher throughput, but the advantage\nis not as obvious as that in query time, and this is mainly\nbecause the throughput is influenced by additional factors such\nas I/O overhead, CPU cores, and memory bandwidth that are\nnot solely dependent on query execution efficiency.\nC. RQ2: Varying Partitioners\nAn appropriate partitioner is essential for the best perfor-\nmance. Since the name of a LiLIS variant can indicate the\nchosen paritioner, we denote the paritioner as its implemen-\ntation directly in Table III for the sake of simplicity. We\nLilis-K Sedona-RQ Sedona-RK Sedona-QQ Sedona-QK Sedona-N Spark100101102Throughput (jobs/min)Point RangeFig. 5. The throughput (jobs per minute) of point and range queries under\ndefault settings.\nhighlight the best results in light green and the second-best\nresults in yellow. Our analysis reveals that both Quad-tree and\nKD-tree serve as highly effective partitioning strategies across\na broad range of query types, while the R-tree partitioner\nemerges as the optimal choice for most queries, with the\nnotable exception of join queries. This distinction underscores\na critical tradeoff in spatial indexing: the R-tree’s optimization\nfor efficient individual object retrieval is less effective for join\noperations, where its structural characteristics are less suited\nto the combinatorial demands of such queries. In contrast, the\nmore uniform spatial division strategies employed by Quad-\ntree and KD-tree enable them to handle join queries more\nadeptly, highlighting their robustness in scenarios requiring\ncomplex relational operations.\nTABLE III\nVARYING PARTITIONERS (MS)\nMethodQueryPoint Query Range Query kNN Query Join Query\nLiLIS-F 218.08 704.15 1107.20 636710\nLiLIS-A 199.09 521.04 1767.00 1322970\nLiLIS-Q 141.3 340.57 773.50 276557\nLiLIS-K 82.59 468.64 650.20 228581\nLiLIS-R 76.68 471.55 618.80 21492013\nTakeaway 2\nLiLIS is sensitive to various partitioners, and tree-based\npartitioners performs better than grid-based partitioners\ngenerally.\nD. RQ3: Varying Datasets\nAs illustrated in Figure 6, the experimental results demon-\nstrate significant performance variations across three query\ntypes when executed on three distinct datasets. Point queries\nexhibited substantial performance disparities, Interestingly,\ndespite the NYC dataset being approximately 10 times larger\nthan CHI, it demonstrated significantly better performance\nfor all queries. We can also notice that queries over SYN\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8\nconsistently performs the best, and this is because uniformly\ngenerate spatial objects can be best modeled by both spatial\npartitioner and learned index. These findings highlight the\nimportance of considering both dataset size and intrinsic\ncharacteristics when optimizing spatial database operations.\nThe results demonstrate that larger datasets do not necessarily\nlead to proportionally longer query times.\nPoint Query Range Query kNN Query0200400600800\n607 .05710 .22743 .74\n82.59340 .57650 .2\n141 .76130 .67185 .5Time (ms)CHI NYC SYN\nFig. 6. LiLIS performance while varying datasets.\nThe results in Figure 6 also indicate that kNN is more\ntime-consuming than both point and range query. Therefore,\nwe further take it as the running example to compare LiLIS\nwith its competitors among various datasets. In Table IV, we\nhighlight the best results in bold, demonstrating that LiLIS\nconsistently outperforms other methods across all datasets.\nTABLE IV\nVARYING DATASETS FOR kNN QUERIES (MS)\nMethodDatasetCHI NYC SYN\nLiLIS-K 743 650 185\nSedona-RK 7862 790993 83170\nSedona-N 6881 314243 49590\nTakeaway 3\nLiLIS is affected by data characteristics as well as the\nsize of dataset; LiLIS consistently outperforms other\ncompetitors across all datasets.\nE. RQ4: Subtle Factors for Range and kNN Queries\nAs described in the experimental setup, both selectivity\nand skewness are essential for range queries. To answer\nthose subtle factors, we vary selectivity from 0.000001%\nto 0.1% under both skewed and uniform queries, and the\nresults are reported in Figure 7. Uniform queries consistently\noutperform skewed queries due to their alignment with the\nunderlying data distribution, allowing the learned index to\nmake more accurate predictions. Skewed queries exhibit an\ninverse relationship between selectivity and execution time. Tobe specific, higher selectivity often reduces query latency. This\noccurs because skewed queries with small selectivity values\nfrequently target sparse or irregular data regions, leading to\ninefficient index traversal. In contrast, as selectivity increases,\nthe query workload becomes more balanced, mitigating the\nimpact of skewness.\n10−510−410−310−210−10100200300400\nSelectivity (%)Time (ms)Skewed\nUniform\nFig. 7. Skewed and uniform range queries under different selectivities.\nNext, we further conduct experiments over all variants of\nLiLIS while varying kfrom 1 to 100 for kNN queries.\nAccording to the results in Figure 8, we can conclude that\nkNN queries is relatively stable when varying k, and this is\nconsistent with the analysis presented in Section IV-C. This\nstability can be attributed to the query processing mechanism:\nregardless of how kchanges, the number of partitions that\nneed to be accessed remains relatively small, as the query\nradius determination primarily depends on data distribution\nrather than the kvalue itself. Additionally, we can also\nnotice that R-tree partitioner consistently outperforms other\npartitioning strategies across different kvalues. This superior\nperformance can be explained by the R-tree’s effective spatial\nindexing capabilities, which enable more efficient pruning of\nirrelevant partitions during query processing, thereby reducing\nthe overall computational cost.\n1 10 10005001,0001,500\nVarying kTime (ms)LiLIS-F\nLiLIS-A\nLiLIS-Q\nLiLIS-K\nLiLIS-R\nFig. 8. kNN queries in LiLIS when varying k.\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9\nTakeaway 4\nRange queries in LiLIS are sensitive to the skewness,\nand uniform range queries run stably under different\nselectivities; kNN queries in LiLIS are insensitive to\ncommon k(k <100).\nF . RQ5: Index Cost\n20406080100\nLiLIS-FLiLIS-ALiLIS-Q\nLiLIS-K\nLiLIS-RSedeno-RQ\nSedeno-RK\nSedeno-QQ\nSedeno-QKTime (s)\nFig. 9. Index building cost.\nTo demonstrate the superior efficiency of LiLIS in index\nconstruction, we conduct comparative experiments measuring\nthe build time for all index variants in both LiLIS and\nSedona. As illustrated in Figure 9, LiLIS consistently achieves\nfaster index construction times compared to traditional spatial\nindices. For instance, building a LiLIS-K index requires ap-\nproximately 50 seconds, whereas constructing a Sedona-RQ\nindex takes about 100 seconds, representing a 1.5-2.0 ×speed-\nup. Notably, while the performance advantage is significant,\nthe gap in construction time is less pronounced than the\nquery time improvements we observed above. This observation\naligns with our theoretical analysis presented in Section III-B.\nTakeaway 5\nBuilding indices in LiLIS is faster than in competitors,\nthough the speed-up is less significant than its query\nperformance advantage.\nVI. C ONCLUSION\nPrior work shows that learned indices enable efficient search\nby using ML models to predict data positions. However,\ndistributed spatial analytics like Sedona still face high index\nconstruction overheads, hindering real-time processing. In this\nstudy, we propose LiLIS, a lightweight distributed learned\nindex, and develop efficient spatial algorithms to support com-\nmon spatial queries on Apache Spark, including point queries,\nrange queries, k-nearest neighbor ( kNN) queries, and spatial\njoins. LiLIS achieves a 2–3 orders of magnitude speed-up over\nstate-of-the-art systems and 1.5-2 ×faster index construction.\nMost notably, LiLIS is the first study to our knowledge toinvestigate the effectiveness and efficiency of learned index for\nbig spatial data. Future work should investigate alternative in-\ndexing approaches (e.g., piece-wise linear model, simple non-\nlinear model and neural network) beyond linear interpolation\nto make trade-offs between model complexity, accuracy, and\ninference speed for distributed spatial analytics.\nREFERENCES\n[1] C. Luo, T. Dan, Y . Li, X. Meng, and G. Li, “Why-not questions about\nspatial temporal top-k trajectory similarity search,” Knowledge-Based\nSystems , vol. 231, p. 107407, 2021.\n[2] M. Fort, J. A. Sellar `es, and N. Valladares, “Nearest and farthest spa-\ntial skyline queries under multiplicative weighted euclidean distances,”\nKnowledge-Based Systems , vol. 192, p. 105299, 2020.\n[3] M. Nie, Z. Wang, J. Yin, and B. Yao, “Reachable region query and its\napplications,” Information Sciences , vol. 476, pp. 95–105, 2019.\n[4] Y . Li, W. Zhang, C. Luo, X. Du, and J. Li, “Answering why-not\nquestions on top-k augmented spatial keyword queries,” Knowledge-\nBased Systems , vol. 223, p. 107047, 2021.\n[5] W. Jiang, “Data-driven analysis of taxi and ride-hailing services: Case\nstudy in chengdu, china,” Computer and Decision Making: An Interna-\ntional Journal , vol. 2, pp. 357–373, 2025.\n[6] Apache Software Foundation, “Apache Spark,” 2025, accessed:\n2025-03-18. [Online]. Available: https://spark.apache.org/\n[7] ——, “Apache Flink,” 2025, accessed: 2025-03-18. [Online]. Available:\nhttps://flink.apache.org/\n[8] D. Xie, F. Li, B. Yao, G. Li, L. Zhou, and M. Guo, “Simba: Efficient in-\nmemory spatial analytics,” in ACM SIGMOD International Conference\non Management of Data , 2016, pp. 1071–1085.\n[9] J. Yu, Z. Zhang, and M. Sarwat, “Spatial data management in apache\nspark: the geospark perspective and beyond,” GeoInformatica , vol. 23,\npp. 37–78, 2019.\n[10] N. Beckmann, H.-P. Kriegel, R. Schneider, and B. Seeger, “The r*-tree:\nAn efficient and robust access method for points and rectangles,” in ACM\nSIGMOD International Conference on Management of Data , 1990, pp.\n322–331.\n[11] H. Samet, “The quadtree and related hierarchical data structures,” ACM\nComputing Surveys , vol. 16, no. 2, pp. 187–260, 1984.\n[12] J. Wu, W. Gan, H.-C. Chao, and P. S. Yu, “Geospatial big data: Survey\nand challenges,” IEEE Journal of Selected Topics in Applied Earth\nObservations and Remote Sensing , 2024.\n[13] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The case for\nlearned index structures,” in ACM SIGMOD International Conference on\nManagement of Data , 2018, pp. 489–504.\n[14] Z. Sun, X. Zhou, and G. Li, “Learned index: A comprehensive experi-\nmental evaluation,” Proceedings of the VLDB Endowment , vol. 16, no. 8,\npp. 1992–2004, 2023.\n[15] P. Li, H. Lu, Q. Zheng, L. Yang, and G. Pan, “Lisa: A learned index\nstructure for spatial data,” in ACM SIGMOD International Conference\non Management of Data , 2020, pp. 2119–2133.\n[16] G. Liu, J. Qi, C. S. Jensen, J. Bailey, and L. Kulik, “Efficiently learning\nspatial indices,” in IEEE International Conference on Data Engineering .\nIEEE, 2023, pp. 1572–1584.\n[17] A. Al-Mamun, H. Wu, Q. He, J. Wang, and W. G. Aref, “A survey\nof learned indexes for the multi-dimensional space,” arXiv preprint\narXiv:2403.06456 , 2024.\n[18] Y . Sheng, X. Cao, Y . Fang, K. Zhao, J. Qi, G. Cong, and W. Zhang,\n“Wisk: A workload-aware learned index for spatial keyword queries,”\nACM SIGMOD International Conference on Management of Data ,\nvol. 1, no. 2, pp. 1–27, 2023.\n[19] M. Li, H. Wang, H. Dai, M. Li, C. Chai, R. Gu, F. Chen, Z. Chen,\nS. Li, Q. Liu et al. , “A survey of multi-dimensional indexes: past and\nfuture trends,” IEEE Transactions on Knowledge and Data Engineering ,\nvol. 36, no. 8, pp. 3635–3655, 2024.\n[20] V . Pandey, A. van Renen, A. Kipf, and A. Kemper, “How good are\nmodern spatial libraries?” Data Science and Engineering , vol. 6, no. 2,\npp. 192–208, 2021.\n[21] G. Guti ´errez, R. Torres-Avil ´es, and M. Caniup ´an, “ckd-tree: A compact\nkd-tree,” IEEE Access , vol. 12, pp. 28 666–28 676, 2024.\n[22] K. Park, “A hierarchical binary quadtree index for spatial queries,”\nWireless Networks , vol. 25, pp. 1913–1929, 2019.\n[23] J. Yang and G. Cong, “Platon: Top-down r-tree packing with learned\npartition policy,” in Proceedings of the ACM on Management of Data .\nACM New York, NY , USA, 2023, pp. 1–26.\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10\n[24] J. Jin, P. Cheng, L. Chen, X. Lin, and W. Zhang, “Gridtuner: Rein-\nvestigate grid size selection for spatiotemporal prediction models,” in\nIEEE International Conference on Data Engineering . IEEE, 2022, pp.\n1193–1205.\n[25] X. Li, X. Zhao, H. Zhang, and J. Han, “Grid adaptive bucketing\nalgorithm based on differential privacy,” Mobile Information Systems ,\nvol. 2022, no. 1, p. 6988976, 2022.\n[26] J. Ding, V . Nathan, M. Alizadeh, and T. Kraska, “Tsunami: A learned\nmulti-dimensional index for correlated data and skewed workloads,”\narXiv preprint arXiv:2006.13282 , 2020.\n[27] A. Pandey, R. Kumar, M. Li, and W. Zhang, “Enhancing in-memory\nspatial indexing with learned search,” IEEE Transactions on Knowledge\nand Data Engineering , vol. 35, no. 4, pp. 1234–1245, 2023.\n[28] S. Zhang, S. Ray, R. Lu, and Y . Zheng, “Efficient learned spatial index\nwith interpolation function based learned model,” IEEE Transactions on\nBig Data , vol. 9, no. 2, pp. 733–745, 2023.\n[29] P. Ferragina, F. Lillo, and G. Vinciguerra, “Why are learned indexes so\neffective?” in International Conference on Machine Learning , 2020, pp.\n3123–3132.\n[30] S. F. Ahmed, M. S. B. Alam, M. Hassan, M. R. Rozbu, T. Ishtiak,\nN. Rafa, M. Mofijur, A. Shawkat Ali, and A. H. Gandomi, “Deep learn-\ning modelling techniques: current progress, applications, advantages, and\nchallenges,” Artificial Intelligence Review , vol. 56, no. 11, pp. 13 521–\n13 617, 2023.\n[31] Q. Liu, S. Han, Y . Qi, J. Peng, J. Li, L. Lin, and L. Chen, “Why are\nlearned indexes so effective but sometimes ineffective?” arXiv preprint\narXiv:2410.00846 , 2024.\n[32] T. Gu, K. Feng, G. Cong, C. Long, Z. Wang, and S. Wang, “The rlr-\ntree: A reinforcement learning based r-tree for spatial data,” in ACM\nSIGMOD International Conference on Management of Data , 2023, pp.\n1–23.\n[33] J. Ding, U. F. Minhas, J. Yu, C. Wang, J. Do, Y . Li, H. Zhang, B. Chan-\ndramouli, J. Gehrke, D. Kossmann, D. Lomet, and T. Kraska, “Alex:\nAn updatable adaptive learned index,” in ACM SIGMOD International\nConference on Management of Data , 2020, pp. 969–984.\n[34] Q. Liu, M. Li, Y . Zeng, Y . Shen, and L. Chen, “How good are\nmulti-dimensional learned indexes? an experimental survey,” The VLDB\nJournal , vol. 34, no. 2, pp. 1–29, 2025.\n[35] J. Qi, G. Liu, C. S. Jensen, and L. Kulik, “Effectively learning spatial\nindices,” Proceedings of the VLDB Endowment , vol. 13, no. 12, pp.\n2341–2354, 2020.\n[36] J. Qi, Y . Tao, Y . Chang, and R. Zhang, “Theoretically optimal and\nempirically efficient r-trees with strong parallelizability,” Proceedings\nof the VLDB Endowment , vol. 11, no. 5, pp. 621–634, 2018.\n[37] H. Shin, K. Lee, and H.-Y . Kwon, “A comparative experimental study\nof distributed storage engines for big spatial data processing using\ngeospark,” The Journal of supercomputing , vol. 78, no. 2, pp. 2556–\n2579, 2022.\n[38] Y . Xu, B. Yao, Z. Wang, X. Gao, J. Xie, and M. Guo, “Skia: Scalable\nand efficient in-memory analytics for big spatial-textual data,” IEEE\nTransactions on Knowledge and Data Engineering , vol. 32, no. 12, pp.\n2467–2480, 2020.\n[39] M. M. Alam, L. Torgo, and A. Bifet, “A survey on spatio-temporal data\nanalytics systems,” ACM Computing Surveys , vol. 54, no. 10s, pp. 1–38,\n2022.\n[40] LocationTech GeoMesa Project, “Geomesa: Store, index, query, and\ntransform spatio-temporal data at scale,” 2025, accessed: 2025-03-18.\n[Online]. Available: https://www.geomesa.org/\n[41] Apache Software Foundation, “Apache Sedona,” 2025, accessed:\n2025-03-18. [Online]. Available: https://sedona.apache.org\n[42] Z. Chen, B. Yao, Z.-J. Wang, W. Zhang, K. Zheng, P. Kalnis, and\nF. Tang, “Itiss: an efficient framework for querying big temporal data,”\nGeoInformatica , vol. 24, pp. 27–59, 2020.\n[43] A. Kipf, R. Marcus, A. van Renen, M. Stoian, A. Kemper, T. Kraska, and\nT. Neumann, “Radixspline: a single-pass learned index,” in International\nWorkshop on Exploiting Artificial Intelligence Techniques for Data\nManagement , 2020, pp. 1–5.\n[44] T. Neumann and S. Michel, “Smooth interpolating histograms with error\nguarantees,” in British National Conference on Databases . Springer,\n2008, pp. 126–138.\n[45] S. T. Leutenegger, M. A. Lopez, and J. Edgington, “Str: A simple and\nefficient algorithm for r-tree packing,” in International Conference on\nData Engineering . IEEE, 1997, pp. 497–506.\n[46] D. Liu, E.-P. Lim, and W.-K. Ng, “Efficient k nearest neighbor queries\non remote spatial databases using range estimation,” in International\nConference on Scientific and Statistical Database Management . IEEE,\n2002, pp. 121–130.\nZhongpu Chen is currently an Assistant Professor at\nSouthwestern University of Finance and Economics.\nHe received the PhD degree in computer science\nfrom Shanghai Jiao Tong University. His research\ninterests include database systems, processing and\noptimization on spatial-temporal data, large lan-\nguage models, and AI4DB.\nWanjun Hao is a third-year Master student in computer science at South-\nwestern University of Finance and Economics. His research interests include\nbig data processing and algorithm optimization.\nZiang Zeng is a first-year Master student of computer science in Southwestern\nUniversity of Finance and Economics. His research interests include algorithm\noptimizations and database systems.\nYi Wen is pursuing a bachelor’s degree in computer science at Chongqing\nUniversity. Her research interests include algorithm optimization and machine\nlearning.\nLong Shi is currently an Associate Professor at\nSouthwestern University of Finance and Economics.\nHis research interests include large language models,\ntrustworthy AI, and federated learning.\nZhi-Jie Wang received the PhD degree in computer\nscience from Shanghai Jiao Tong University, and\ncompleted a postdoctoral fellowship at The Hong\nKong Polytechnic University. He is currently an\nAssociate Professor with the College of Computer\nScience, Chongqing University.\nYu Zhao is currently a Professor at Southwestern\nUniversity of Finance and Economics. His research\ninterests include large language models, AI agents\nand natural language processing.",
  "textLength": 51560
}