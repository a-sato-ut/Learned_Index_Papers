{
  "paperId": "653e829635bb7f19cf0ee0c66dd5d7fe955d4d04",
  "title": "AnnotatedTables: A Large Tabular Dataset with Language Model Annotations",
  "pdfPath": "653e829635bb7f19cf0ee0c66dd5d7fe955d4d04.pdf",
  "text": "AnnotatedTables: A Large Tabular Dataset with\nLanguage Model Annotations\nYaojie Hu\nDepartment of Computer Science\nIowa State University\njhu@iastate.eduIlias Fountalis\nRelationalAI\nilias.fountalis@relational.ai\nJin Tian\nDepartment of Computer Science\nIowa State University\njtian@iastate.eduNikolaos Vasiloglou\nRelationalAI\nnik.vasiloglou@relational.ai\nAbstract\nTabular data is ubiquitous in real-world applications and abundant on the web,\nyet its annotation has traditionally required human labor, posing a significant\nscalability bottleneck for tabular machine learning. In this paper, we introduce\nmethods that leverage Large Language Models (LLMs) to understand and annotate\ntabular data. Our methodology can successfully annotate a large amount of tabular\ndata and can be flexibly steered to generate various types of annotations based on\nspecific research objectives, as we demonstrate with SQL annotation and input-\ntarget column annotation as examples. As a result, we release AnnotatedTables,\na collection of 32,119 databases with LLM-generated annotations. The dataset\nincludes 405,616 valid SQL programs, making it the largest SQL dataset with\nassociated tabular data that supports query execution. To further demonstrate the\nvalue of our methodology and dataset, we perform two follow-up research studies.\n1) We investigate whether LLMs can translate SQL programs to Rel programs,\na database language previously unknown to LLMs, while obtaining the same\nexecution results. Using our Incremental Prompt Engineering methods based on\nexecution feedback, we show that LLMs can produce adequate translations with\nfew-shot learning. 2) We evaluate the performance of TabPFN, a recent neural\ntabular classifier trained on Bayesian priors, on 2,720 tables with input-target\ncolumns identified and annotated by LLMs. On average, TabPFN performs on\npar with the baseline AutoML method, though the relative performance can vary\nsignificantly from one data table to another, making both models viable for practical\napplications depending on the situation. Our findings underscore the potential of\nLLMs in automating the annotation of large volumes of diverse tabular data.\n1 Introduction\nTabular data is one of the most common data types with pervasive real-world applications such as\nfinance, healthcare management, and marketing analytics (Shwartz-Ziv & Armon, 2022; Borisov\net al., 2022; Chui et al., 2018). The tabular format can organize structured data as relational databases.\nMachine learning models are able to understand and generate SQL queries against the relational\ndatabases (Zhong et al., 2017; Yan et al., 2020), and large language models (LLMs) have emerged as\na prominent method in this field (Liu et al., 2023a; Li et al., 2023a). Recent studies show that LLMs\ncan achieve state-of-the-art SQL generation with carefully engineered prompts (Gao et al., 2023),\nPreprint. Under review.arXiv:2406.16349v1  [cs.LG]  24 Jun 2024\n\nowing thanks to the massive amount of open-source SQL code in their training set (Li et al., 2023b).\nAlthough valuable for training LLMs, SQL code alone is not enough for other tabular machine\nlearning problems. For example, to evaluate the text-to-SQL ability where the model generates SQL\ncode based on a description, both tabular data and SQL code would be required to calculate the\nexecution accuracy (Zhong et al., 2017; Xu et al., 2017; Chang & Fosler-Lussier, 2023). While either\nSQL code or tabular data can be abundant, obtaining both at the same time can be far more difficult.\nNot limited to SQL code, annotations for tabular datasets can be generally difficult to collect, which\nwe call tabular dataset annotation problem in this paper. We use executable SQL code as a lens to\nexplore this tabular dataset annotation problem in detail and propose our machine learning solutions.\nWikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018b) are two popular datasets with both SQL\ncode and tabular data suitable for execution. However, their sizes are limited, mainly because their\nannotation methodologies do not scale up to make full use of either the tabular data or the SQL code\non the web. WikiSQL gathers its tabular data from Wikipedia pages, which is a limited source and\ndoes not reflect the complications of real-world tabular data. Also, its SQL programs are annotated\nmanually by a crowd-sourcing service called Amazon Mechanical Turk. Every crowd-sourced SQL\nprogram in WikiSQL only queries one table at a time, without table-join operations. Spider, on the\nother hand, does include relatively more complex queries on multiple tables in a database, but it\nalso relies on human annotations from 11 college students to collectively write the SQL programs.\nHuman annotation is common in all tabular datasets with executable SQL code prior to our work\n(Table 1), an expensive methodology to reproduce and expand upon (Hsueh et al., 2009). However,\nrecent work shows that LLMs are able to synthesize SQL code robustly given instructions (Liu et al.,\n2023a), which motivates us to explore the potential to use LLMs to synthesize the SQL code for the\nabundant, unannotated tabular data. Our work focuses on SQL code, but SQL code is not the only\ntype of annotation that tabular machine learning systems may need. Can LLMs create other types of\nannotations and generally replace expensive human annotations for tabular datasets? Inspired by the\nmulti-tasking (Srivastava et al., 2022) and emergent abilities (Wei et al., 2022; Schaeffer et al., 2024)\nof LLMs, we see both value and potential in tackling the tabular dataset annotation problem.\nTable 1: The sizes of the executable SQL\ndatasets in the literature compared to ours.\nDataset SQL DB\nWikiSQL (Zhong et al., 2017) 80654 26251\nSpider (Yu et al., 2018b) 9693 200\nCspider (Min et al., 2019) 9691 166\nIMDB (Yaghmazadeh et al., 2017) 111 1\nYelp (Yaghmazadeh et al., 2017) 68 1\nSparC (Yu et al., 2019) 15598 200\nTableQA (Sun et al., 2020) 49974 5291\nDuSQL (Wang et al., 2020) 23797 200\nKaggleDBQA (Lee et al., 2021) 272 8\nChase (Guo et al., 2021) 17940 280\nBIRD (Li et al., 2023a) 12751 95\nAnnotatedTables (Ours) 405616 32119In this paper, we introduce AnnotatedTables, a large\nscale tabular dataset with large language model anno-\ntations. To overcome the bottleneck of labor-intensive\nhuman annotations, we use LLMs to understand tab-\nular data and generate synthetic annotations. Our\ndataset construction starts with diverse cross-domain\ntabular data encountered in real-world data science\npractices, while maintaining general applicability to\nother tabular data sources. With careful prompt de-\nsign, we instruct LLMs to synthesize SQL code with\nzero-shot learning, which, despite its large scale,\nproves to be a quick and cost-effective method to\ncreate a rich dataset without expensive human labor.\nTo ensure the quality of LLM-generated SQL code,\nwe verify them with execution on the tabular data, and we report the validity of generated SQL queries\nfor analysis. To date, AnnotatedTables is the largest collection of SQL code that can be executed on\nthe associated tabular data (Table 1). SQL programs are not the only annotations LLMs can produce.\nSome columns in a table may serve as model input to predict others, and such input-target columns\nare usually identified by people manually. The LLM annotation methodology can be extended to\nannotate input-target columns for our tables, making them suitable for tabular classification. Our\nwork shows that LLMs can simplify and automate the process of tabular dataset construction.\nTo validate the soundness and value of our dataset, we conduct two follow-up studies. 1) We study\nLLM’s ability to learn a new programming language in-context. 2) We scale up the evaluation of a\nrecent tabular classification model and test its performance on diverse real-world data. Both studies\ndemonstrate how LLM annotation can flexibly adapt our dataset to target specific research problems.\nOur code and data will be released publicly.\nThe contributions of our paper are summarized as follows.\n• We introduce in-context learning methods of LLMs to understand and annotate a large amount of\nreal-world tabular data, addressing the scaling bottleneck of traditional human annotation.\n2\n\n•We use a LLM to annotate public data tables and construct a cross-domain dataset named Annotat-\nedTables, currently the largest SQL dataset with tabular data that supports query execution.\n•We extensively assess the quality of the LLM-generated annotations throughout the study. We\ndemonstrate the research value of AnnotatedTables dataset through two follow-up studies.\n•We introduce Incremental Prompt Engineering to teach LLMs to translate SQL programs to Rel\nprograms with adequate accuracy, where Rel is a new language not previously learned by LLMs.\n•We use the input-target columns annotated by LLMs to study TabPFN’s performance on a wide\nvariety of tabular classification problems. We find that TabPFN performs overall on par with the\nbaseline method, with each model potentially performing better depending on the tables.\n2 Related Work\nDatabase management systems and deep learning For text-to-SQL, language models can be\ntrained or fine-tuned to synthesize SQL queries given natural language descriptions (Yavuz et al.,\n2018; Hwang et al., 2019; Yu et al., 2018a). For cardinality estimation, neural networks can predict\nthe number of rows returned by an intermediate operation for query optimization (Liu et al., 2015;\nNegi et al., 2020; Woltmann et al., 2019; Yang et al., 2019, 2020), and end-to-end neural query\noptimizers can be built (Fischer et al., 2019; Gupta et al., 2020; Marcus et al., 2019, 2021). Other deep\nlearning for database problems include query answering (Hilprecht et al., 2019), anomaly detection\n(Pang et al., 2021), ontology matching (Bento et al., 2020), relational data modeling (Fan et al., 2020;\nLiu, 2022), and database indexing (Kraska et al., 2018). All these problems could potentially benefit\nfrom a large-scale multi-domain dataset like AnnotatedTables. LLM annotation can also be applied\nto these problems to acquire a large amount of tabular training data. Moreover, our dataset supports\nquery execution, which is required for text-to-SQL and cardinality estimation to collect ground truth.\nSQL datasets with tabular data. SQL annotation is a focus of our paper. AnnotatedTables is\nthe largest SQL dataset with associated tabular data that supports query execution. Currently, the\nmost commonly used SQL datasets with tabular data are listed in Table 1, surveyed in (Deng et al.,\n2022; Qin et al., 2022; Li et al., 2023a). All of them are annotated by humans, and our work is the\nfirst tabular dataset annotated by LLMs. Our LLM methods can cheaply generate a large amount of\nannotations that match the volume of tabular data available on the web. Existing SQL datasets with\ntabular data often include English descriptions for text-to-SQL evaluation. While we collect these\ndescriptions, we do not assess or ensure their quality. Unlike SQL programs, which can be validated\nthrough execution (Yu et al., 2019), English descriptions cannot be executed.\nLarge Language Models for data annotation. LLMs have been used for data annotation on text\ndomains (Mao et al., 2024) and can outperform human annoators (Gilardi et al., 2023). LLMs can\ndetect implicit hate speech and provide natural language explanations as annotations (Huang et al.,\n2023). LLMs are also used for natural language generation evaluation, showing a high alignment\nwith human evaluation (Liu et al., 2023b). Texts can be directly understood and annotated by LLMs,\nwhile tabular data requires additional annotation methods, which we introduce.\n3 AnnotatedTables\n3.1 Dataset Construction\nCollection of public data tables The construction of AnnotatedTables starts by collecting public\ntabular data. Our goal is to tap into the large quantity of un-annotated tabular data that exist on\nthe web, with a preference for curated tables encountered in real-world data science applications.\nKaggle, a data science platform, meets our requirements (Risdal & Bozsolik, 2022; Quaranta et al.,\n2021; Wang et al., 2021). Kaggle hosts data tables that are crowd sourced by a community of data\nenthusiasts and professionals. Users can share and analyze datasets on a wide variety of real-world\ntopics, including football players, U.S airline flights, renewable energy output, weather forecasting,\nand e-commerce product reviews. Users may rate, improve, and analyze the datasets on Kaggle,\ncontributing to the curation of useable and reliable data. Although individual Kaggle datasets have\nbeen studied in prior works (Bojer & Meldgaard, 2021; Taieb & Hyndman, 2014; Yang et al., 2018;\n3\n\nFigure 1: An illustration of the SQL code annotation process with a large language model.\nTolkachev et al., 2020; Yang & Ding, 2020), our research aggregates and examines the data tables en\nmasse, uncovering an previously overlooked goldmine of diverse and high-quality data tables.\nWe use Kaggle’s public API to search for datasets with sizes ranging from 10 megabytes to 1 gigabyte\nfor faster data collection and processing. We also require every dataset’s “usability rating” to be\nhigher than 0.1. The “usability rating” is a quality score given by Kaggle users and helps us filter out\nlow-quality datasets and datasets with no usages. We collect 70,000 Kaggle datasets before stopping\nfor further processing. Our LLM annotation methodology is flexible and automated. The rest of our\nmethods can be applied to other data sources and is not limited to Kaggle datasets alone.\nDescribing databases to LLMs with schema and example rows In order for LLMs to understand\nand annotate tabular data, we need to design a textual description for each database as a part of the\nprompt. Using entire tables in the prompt could exceed the maximum input length of LLMs, and we\nuse a much shorter description instead that contains a basic schema and example rows.\nTable schema is a summary description of its structure, commonly used to define the table during\nits creation (Uschold, 2015). For our annotation purposes, we use a basic schema that includes the\ntable name, the table’s column names, and the data type of each column (Figure 1). We find tabular\ndata stored as CSV files in the Kaggle datasets and automate schema extraction. As CSV files do not\ninclude table names in their content, we opt to use the file names as table names. Column names are\nincluded in the CSV files as the header row. Upon inspection, we find that the column names and\nfile names are typically well-labeled and descriptive enough for LLMs to infer the information in\nthe table. Lastly, we use an existing data analysis software (pandas) to load the tables and infer the\ncolumn data types. Our schema extraction method is practical and effective.\nA Kaggle dataset may contain multiple tables and therefore multiple table schemas. The set of table\nschemas describe the database structure: how many tables there are, what the columns and types are,\nand whether a column appears in multiple tables that could potentially be a JOIN key.\nExample rows contain content from the table, and they are necessary for LLMs to write many\ncommon SQL queries, such as WHERE statements that filter the table based on a condition. For a\nconcrete example, in SELECT * FROM CUSTOMERS WHERE COUNTRY=‘MEXICO’ , the table name\nCUSTOMERS and column name COUNTRY can be found in the database schemas, while ‘MEXICO’ is\na value that can only be found in a table row. Providing row examples in addition to the schema\nis necessary for LLMs to synthesize WHERE statements in SQL. For the example row, we use the\nfirst row of each table, converted to text through JSON formatting. Our schema and example row\ndescriptions are generic and allow almost all tabular data to be annotated by LLMs.\nSQL annotations with zero-shot synthesis by LLMs Through zero-shot learning, we instruct\nLLMs to synthesize SQL code that queries the data tables we collected as one type of annotation.\nCurrently, OpenAI’s ChatGPT (Ouyang et al., 2022) is one of the best performing LLMs on various\nbenchmarks (Hendrycks et al., 2021; Zellers et al., 2019; Sakaguchi et al., 2021). For our paper, we\nuse ChatGPT with GPT-3.5 backbone as our annotation LLM, referred to as “the LLM” from now on.\nTo perform zero-shot SQL annotation for each database, we create a prompt that contains the\ninstruction, the database schema, and the example row of every table, illustrated in Figure 1. The\ninstruction asks the LLM to write SQL queries from “typical users who access this database”. In\naddition, the instruction asks for queries “with relatively high complexity”, which steers the synthesis\nresults to more complicated SQL code and makes the dataset more challenging. Lastly, the instruction\n4\n\nTable 2: Examples of LLM-generated English and SQL annotations for Kaggle tabular data. LLMs\ncan generate SQL programs that cannot be executed, and we mark the incorrect parts in red.\nEnglish Annotation SQL Annotation Execution\n1 Get the average humidity for each location in the morning and afternoon: SELECT Location, AVG(Humidity9am) AS\nAverageHumidity9am, AVG(Humidity3pm) AS\nAverageHumidity3pm FROM WeatherAus GROUP BY Location;Valid\n2 Retrieve rows where the tempo is greater than the average tempo: SELECT * FROM LikedsongsGV WHERE Tempo > (SELECT\nAVG(Tempo) FROM LikedsongsGV);Valid\n3 Retrieve records from a table that meet a certain condition: SELECT * FROM table_name WHERE condition; Error\n4Retrieve rows where column 2 is less than 0.1 and column 5 is greater than 0.2: SELECT * FROM Preds001 WHERE 2 < 0.1 AND 5 > 0.2; Error\n5 Get the total number of matches played in a specific season SELECT COUNT(*) FROM IPLMatches WHERE Season = 2019; Empty Result\nasks the language model to create more JOIN queries when multiple tables are present, as JOIN\nqueries are especially valuable data points when studying problems including cardinality estimation\nand query optimization (Yang et al., 2020; Marcus et al., 2021). For every Kaggle dataset with tabular\ndata, we prompt the LLM to generate 15 SQL program annotations with natural language annotations.\nExecution-based validation Some of the LLM-generated SQL programs could be incorrect, and\nit is important to examine and ensure the quality of the LLM annotations. As AnnotatedTables has\ntabular data available for execution, we verify the validity of the SQL programs by executing them\n(Yu et al., 2019). If a SQL program can be executed and returns non-empty results, then we classify it\nas a valid SQL annotation. Executable queries with empty results are marked. In our follow-up study\nof SQL-to-Rel translation, a pair of SQL and Rel queries with empty results cannot be compared\nby execution accuracy, and SQL queries with empty results will not be used. We do not validate the\nEnglish descriptions. All intermediate data artifacts are released as a part of AnnotatedTables.\n3.2 Features, Statistics, and Examples of AnnotatedTables\nTable 3: SQL code synthesized by LLM catego-\nrized by the types of SQL components.\nComponent Total Valid % Valid % Error % Empty\nTotal 493134 405616 82.25 9.96 7.79\nFROM 493093 405612 82.26 7.78 9.96\nWHERE 276710 209393 75.67 8.07 16.26\nAND 88182 61814 70.10 10.81 19.09\nGROUP BY 77004 66061 85.79 9.74 4.47\nAVG 65797 58238 88.51 8.83 2.66\nCOUNT 58586 53486 91.29 6.41 2.29\nORDER BY 52677 45941 87.21 8.98 3.81\nLIKE 38865 28189 72.53 5.90 21.57\nJOIN 30079 14879 49.47 23.00 27.53\nMAX 29693 26188 88.20 9.29 2.52\nSUM 24801 21324 85.98 10.14 3.87\nLIMIT 24241 21573 88.99 8.23 2.77\nNOT 19180 15111 78.79 7.22 13.99\nIN 16011 9656 60.31 23.76 15.93\nMIN 12368 10938 88.44 9.38 2.18\nOR 11199 8346 74.52 11.09 14.39\nUNION 10408 7375 70.86 24.95 4.19\nHAVING 2507 1427 56.92 15.96 27.12\nINTERSECT 184 101 54.89 10.33 34.78\nEXCEPT 37 26 70.27 13.51 16.22Human-like annotation We find that the\nLLM can write SQL programs with human-\nlike intent and usage. Examples of the LLM-\nannotated SQL programs are provided in Table 2.\nInstructed by our prompt to play the role of a\ntypical user of a database, the LLM writes mean-\ningful queries to look for useful data analytics.\nCorrectness and complexity Given a high\npercentage (82.25%) of valid SQL programs,\nthe annotation quality is good. The LLM can\nwrite complex programs with a variety of SQL\ncomponents (Table 3). The percentages of valid\nqueries in all SQL component categories except\nJOIN exceed 50%. 49.47% of JOIN queries are\nvalid, possibly reflecting a higher level of data\nunderstanding and programming skills required\nto combine multiple tables.\nFor limitations, we note that some simple SQL statements appear often, in the form of SELECT *\nFROM table , reflected by a high percentage (48.4% among valid SQL programs) of FROM statements\nwithout other SQL components. Although valid, they may not be as valuable as the more complicated\nSQL programs for downstream learning and analysis. In the prompt, we steer the LLM to generate\nSQL programs “with relative high complexity” and see success to some extent (Appendix B). However,\nthe dual objectives to generate complex queries and correct queries may inherently conflict with each\nother, and LLMs could be inclined to generate simple ones to prioritize correctness over complexity.\nInvalid SQL annotations As shown in Table 2, some LLM-annotated SQL programs may be\ninvalid for different reasons. At times, the LLM hallucinates non-existing table names such as\n“table_name ” (Example 3) or leaves a place holder text “ condition ” instead of the exact selection\ncondition (Example 3). In some CSV files, the column names may be numerical, which are invalid\ncolumn names in a SQL statement, and the LLM is unable to annotate (Example 4). Lastly, as\nexecution result needs to be non-empty so that execution accuracy can be computed for downstream\ntasks, some syntactically correct SQL programs that return empty results are labeled (Example 5).\n5\n\nTable 4: Examples of the SQL-to-Rel translations produced, categorized by “correct translations”,\n“execution errors”, and “different results”.\nSQL Rel Translated Ex. Acc.\n1SELECT Departments, MAX(AverageMontlyHours)\nAS MaxMonthlyHours FROM Attrition GROUP BY\nDepartments;def output[departments] = max[row_id, avg_monthly_hours:\nAttrition:AverageMontlyHours(row_id, avg_monthly_hours) and\nAttrition:Departments[row_id, departments]]Correct\n2SELECT (SELECT COUNT(*) FROM Oof) + (SELECT\nCOUNT(*) FROM Submission) as TotalCount;def count_oof_rows = count[row_id: Oof[_, row_id, _]]\ndef count_submission_rows = count[row_id: Submission[_, row_id, _]]\ndef output = count_oof_rows + count_submission_rowsCorrect\n3SELECT COUNT(*) FROM DgemFormatValidation; def count_rows = count[row_id: DgemFormatValidation[_, row_id, _]]\ndef output = count_rowsCorrect\n4SELECT Education, AVG(Age) AS avg_age FROM\nMLPdata GROUP BY Education;def output[education] = average[row_id, age: MLPdata:Education[row_id,\neducation] and MLPdata:Age[row_id, age]] groupby educationError\n5SELECT COUNT(*) FROM Dict; def output = count[row, _ : Dict[_, row, _]] Error\n6SELECT * FROM V9d3 ORDER BY Labels DESC; def output[col, row_id, val] = V9d3[col, row_id, val] orderby\nV9d3:Labels[row_id] descError\n7SELECT MAX(LENGTH(Description)) FROM\nCryptosentiment;def output = max[row_id, length: Cryptosentiment:Description[row_id,\nlength]]Diff. Results\n8SELECT * FROM Comments20200605 WHERE\nReplyCount >= 5;def output[col, row_id, val] = Comments20200605[col, row_id, val] and\nComments20200605:ReplyCount[row_id] >= 5Diff. Results\n9SELECT AVG(MonthlyRevenue) FROM\nCell2celltrain;def output = average[idx, monthly_revenue: Cell2celltrain:MonthlyRevenue\n[idx, monthly_revenue]]Diff. Results\n4 AnnotatedTables for In-Context Translation from SQL to Rel\nWe use AnnotatedTables to study large language models’ ability to learn to code in Rel, a new\ndatabase language (RelationalAI, 2021). For new programming languages, there is not much open-\nsource code available to train machine learning models with gradient-descent methods, and, with few\ntraining examples, few-shot in-context learning (ICL) becomes a natural candidate instead (Min et al.,\n2022). ICL has been studied, but ICL of a new programming language is a novel research problem.\nNew programming languages are rare, and the lens of this study is unique and timely.\nAll SQL programs in AnnotatedTables are executable, which is necessary in order to evaluate\nthe correctness of the translations with execution accuracy. AnnotatedTables contains many SQL\nprograms and data tables, which can measure the general performance of our SQL-to-Rel translation\nmethod. SQL programs in AnnotatedTables emulate the usage and intent of typical database users,\nand, by translating these queries, we indirectly synthesize a large dataset of non-trivial Rel programs\nthat is valuable for downstream analysis and machine learning of the new programming language.\nMachine learning of a new programming language may seem challenging with limited training data,\nbut as our results show, our translation accuracy increases steadily as more in-context examples are\nincluded, and with 45 translation pairs in the prompt, the translation accuracy can reach around 40%.\n4.1 Task Formulation of SQL-to-Rel Translation\nGiven a SQL program Sthat can be executed on table T, the task is to build a model Mthat\nlearns to translate Sinto a Rel program R, such that SandRreturn the same query results when\nexecuted on table T. In our case, the model Mis a LLM parameterized by an instruction prompt\nthat contains few-shot examples. Execution accuracy is the percentage of queries S, R that return the\nsame non-empty results. Execution accuracy is a common metric for SQL generation (Zhong et al.,\n2017; Xu et al., 2017; Chang & Fosler-Lussier, 2023). Program equivalence is generally undecidable\n(Rice, 1953; Strichman, 2018), and it is not possible to directly compare SQL and Rel programs.\n4.2 Extending AnnotatedTables with Few-shot SQL-to-Rel Translation Pairs\nWhile LLMs have already learned to program in SQL from comprehensive pre-training (Brown et al.,\n2020; OpenAI, 2023), they do not know how to write programs in Rel. To enable ICL, one idea is to\nuse the Rel language specification and documentation as the prompt. However, language models such\nas ChatGPT have limited context span up to 16,000 tokens (around 12,000 English words) (Tay et al.,\n2020; Beltagy et al., 2020), and it is currently infeasible for the prompt to contain the complete Rel\nspecification and documentation (estimated 500,000 words). Instead, we use SQL-to-Rel translation\nexamples to teach the LLM to code in Rel, which is simple and effective. Rel is a declarative language\nsimilar to Datalog and Prolog. LLMs may transfer their background knowledge in related languages\nto learn Rel. ICL through translation examples cannot comprehensively cover all the edge cases of the\nRel language, but, as our results show, around 40% of our SQL code can be successfully translated.\n6\n\nIncremental Prompt Engineering of translation examples To develop SQL-to-Rel translation\nexamples to teach LLMs to code in Rel, we introduce a technique called Incremental Prompt\nEngineering (IPE). For step i= 1,2, ..., k , we start with some manually annotated SQL-to-Rel\ntranslation examples Eiand instruct the language model to translate some unseen SQL programs Si\nto Rel programs Ri. We then calculate the execution accuracy by comparing the query results of Si\nandRiat every step i. The execution accuracy is used as a feedback, and for the SQL code in Sithat\nthe LLM fails to translate, we manually annotate the ones that represent common failure patterns\nto add to Eias the few-shot examples Ei+1at the next step. In practice, we start with 5 translation\nexamples and add multiple manual annotations at a time until the execution accuracy converges with\naround 45 translation examples (Figure 2). All examples used in the prompt are in Appendix A.\n4.3 ResultsTable 5: The LLM’s few-shot SQL-to-Rel translation\nexecution results, by the types of SQL components.\nSQL Comp. # Ex. Translated Correct % Correct % Error % Diff.\nTotal 45 285140 116327 40.80 53.60 5.60\nFROM 45 285136 116327 40.80 53.60 5.60\nWHERE 28 141893 49576 34.94 59.02 6.04\nGROUP BY 9 40254 7737 19.22 78.36 2.42\nAND 7 37963 13086 34.47 59.72 5.81\nAVG 5 37318 12601 33.77 60.08 6.16\nCOUNT 6 36769 17542 47.71 48.90 3.39\nORDER BY 2 28618 434 1.52 97.74 0.75\nLIKE 1 18399 2037 11.07 82.67 6.26\nMAX 2 17984 7698 42.80 52.46 4.74\nSUM 5 14107 4077 28.90 69.69 1.41\nLIMIT 0 13696 73 0.53 98.53 0.94\nNOT 0 8754 326 3.72 93.24 3.04\nJOIN 8 8358 526 6.29 88.32 5.38\nMIN 0 7344 3043 41.44 55.51 3.05\nIN 2 5365 1045 19.48 76.07 4.45\nUNION 0 4530 263 5.81 88.92 5.28\nOR 1 4071 1292 31.74 60.21 8.06\nHAVING 0 677 1 0.15 99.26 0.59\nINTERSECT 0 56 2 3.57 92.86 3.57\nEXCEPT 0 11 1 9.09 81.82 9.09The LLM can learn a new programming\nlanguage adequately from dozens of ex-\namples Table 4 shows examples of SQL\nprograms and their Rel translations pro-\nduced by the LLM. As seen in the exam-\nples, the two languages differ greatly, and\nit is not an easy task to translate them,\neven for experienced programmers. For\nexample, SQL aggregators such as MAX\nhave their counterpart maxfunction in Rel,\nbut the syntax differs greatly (Example 1).\nNevertheless, the LLM is able to perform\nadequate translations by learning a new\nprogramming language from just 45 exam-\nples in-context. Aggregators including MAX,\nMIN, and COUNT have higher-than-average\ntranslation accuracy. This is an impressive result for a challenging problem. Notably, no manual\nexamples illustrate how to translate MINqueries, and LLM has inferred its usage from MAXexamples.\nExecution accuracy measures the translation correctness, where every SQL and Rel pair are executed\non the same database and compare their results. Table 5 shows that the overall execution accuracy is\n40.8%, showing that the LLM can generate adequate translations from SQL to Rel. Machine learning\nof new programming languages is challenging and valuable. Since the best text-to-SQL models have\n60%-80% accuracy (Liu et al., 2023a; Li et al., 2023a), 40% SQL-to-Rel is a promising first step,\nwhich could be sufficient for applications such as coding suggestions and automatic test generation.\nTranslation errors and limitations 53.6% of the Rel translations have errors before returning\nresults, while 5.6% of the Rel translations finish execution but have different results (Table 5).\nThis suggests that most of the failed translations have syntactical problems and cannot be executed.\nExamples of inaccurate translations are in Table 4. LLMs may use SQL language constructs such as\n“groupby ”, “orderby ”, and “ desc ” directly in Rel programs (Example 4, 6), causing syntactically\nincorrect programs. In Example 5, the first underscore “_” caused a syntax error. In Example 7, the\nSQL method “ LENGTH ” is not demonstrated in-context, and the LLM used “length” as a variable\nname instead, causing a change in semantics and different execution results. Examples 8 and 9 show\ndifferent execution results, but the translations are correct by our inspection. Rel is a language based\non the Sixth-Normal-Form (6NF) (Date et al., 2004; Knowles, 2012), which represents all data as sets\nof tuples with only one value column, without null values or empty rows. Rel’s data representation is\nnot the same as SQL’s data representation, and we make conservative conversions when comparing\nthe results to prioritize precision over recall by allowing some false negatives in execution accuracy.\nTherefore, some correct Rel translations may have different results due to the conversion, though\nsuch cases are rare, as the percentage of Rel errors is 10x higher than different results. Discounting\nthis limitation, the SQL-to-Rel translation accuracy may be even higher than that reported in Table 5.\nPerformance convergence of Incremental Prompt Engineering To study the effectiveness of\nIPE, we vary the number of few-shot examples incrementally added and measure the translation\n7\n\naccuracy on a randomly selected subset of 2,000 SQL programs. We plot the execution accuracy\nversus the number of translation examples in Figure 2.\nFigure 2: With Incremental Prompt Engineering,\ntranslation accuracy gradually improves and con-\nverges as more examples are added.Conceptually, Incremental Prompt Engineering\nadds demonstrations for the failed translations in\nthe previous step and integrates execution feed-\nback into prompt engineering. It is an effective\nway to improve translation accuracy, as we see\na clear increase in the percentage of correctly\ntranslated Rel programs as more examples are\nadded. When the number of few-shot examples\nincreases above 30, we see a diminishing return\non the accuracy improvements. At 45 examples,\nthe accuracy converges around 40%. Due to the\nconvergence, more examples may not improve the accuracy beyond 40%, and additional prompt\nengineering or LLM techniques may be required.\n5 AnnotatedTables for an Extended Evaluation of TabPFN\nIn this section, we use AnnotatedTables to comprehensively evaluate TabPFN’s tabular classification\nperformance on diverse, real-world data tables. TabPFN (Hollmann et al., 2023) is a Prior-Data Fitted\nNetwork (PFN), a new type of neural networks trained on synthetic data drawn from a Bayesian prior.\nOnce trained, it takes the entire test set as the input and produces all predictions in a single forward\npass. TabPFN represents a novel approach to tabular classification, a domain dominated by traditional\nmethods and largely uncontested by deep learning methods (Shwartz-Ziv & Armon, 2022). Inspired\nby its state-of-the-art results on a small curated benchmark (Hollmann et al., 2023), we wonder if\nTabPFN will perform robustly against the complications of real-world tabular data.\nTabPFN evaluation and ours TabPFN was evaluated on the OpenML-CC18 benchmark suite\n(Bischl et al., 2017). The benchmark has 30 datasets in total, and 18 of them have only numerical\nfeatures with no missing values, the same as the Bayesian prior that TabPFN is trained on. The\nauthors choose five standard ML methods and two state-of-the-art AutoML methods as baselines to\ncompare with TabPFN. The ML methods include K-nearest-neighbors (KNN), logistic regression,\nand three gradient boosting methods – XGBoost (Chen & Guestrin, 2016), CatBoost (Prokhorenkova\net al., 2018), LightGBM (Ke et al., 2017). The AutoML baseline methods are AutoGluon (Erickson\net al., 2020; Shi et al., 2021) and AutoSkLearn (Feurer et al., 2022). On the 18 numerical datsets,\nTabPFN outperformed all baselines, and AutoGluon was the best performing baseline (see Table 1 of\n(Hollmann et al., 2023)), which we will use as the baseline in our study. In this section, we follow\nTabPFN authors’ experimental settings and use AnnotatedTables to vastly scale up the evaluation, with\na focus on assessing its real-world performance. We evaluate TabPFN on 2,720 tabular classification\nproblems, a 90x increase over OpenML-CC18. Moreover, in comparison to the OpenML-CC18\nbenchmark, AnnotatedTables contains diverse tabular data gathered from a public platform, which\nwould be a more comprehensive evaluation of TabPFN’s robustness in real world problems.\n5.1 Task Formulation of Tabular Classification\nFor tabular classification, a table Thas rows {R1, R2, ...}. A row Rihas one or more column values\nxiand a categorical column value yi. A tabular classification model faims to use xito predict yi.\nOur experimental settings follow the TabPFN authors’ settings. We train AutoGluon with a time\nbudget of one minute and five minutes per table, and we split the training and test set once per table.\nTo reduce the total training time as we have many more tables than OpenML-CC18, we do not repeat\ntraining on every table with a five-fold training-test set split. We use AutoGluon as our baseline,\nbecause it is the best performing baseline model in the original evaluation (Hollmann et al., 2023).\nOur work does not alter the TabPFN model architecture, and we use the authors’ publicly released\npre-trained checkpoint. For more model details, we refer readers to the paper (Hollmann et al., 2023).\n8\n\nTable 6: The AUROC (one-versus-one), cross entropy, and time statistics for TabPFN and the\nAutoGluon baseline on the tabular classification problems in AnnotatedTables.\n1 Min (2725 problems) 5 Mins (2720 problems)\nTabPFN AutoGluon Paired Diff. TabPFN AutoGluon Paired Diff.\nAUROCMean 0.760 ±0.173 0.723 ±0.214 0.036 ±0.152 0.760 ±0.174 0.755 ±0.187 0.005 ±0.095\nMedian 0.794 0.767 0.004 0.793 0.794 -0.000\nCEMean 0.753 ±0.550 0.886 ±2.129 -0.133 ±1.944 0.753 ±0.550 0.807 ±1.929 -0.054 ±1.726\nMedian 0.655 0.693 -0.011 0.656 0.659 0.001\nTime (sec.) Mean 2.002 73.654 -71.652 2.342 315.038 -312.696\n5.2 Extending AnnotatedTables with LLM-Annotated Input-Target Columns\nThe Kaggle data tables collected in AnnotatedTables could be used for tabular classification task,\nif LLMs can identify the potential input and target columns. We give the schema description from\nSec 3.1 to the LLM to identify potential input and target columns in the tables. TabPFN requires\nthat the input columns are numeric and the target column is categorical for classification. LLM\nannotation is flexible, and we instruct the LLM to find columns that satisfy this data type requirement\n(for complete prompt, see Appendix A). We use the input-target columns annotated to train TabPFN\nand AutoGluon, and if the models cannot be trained, the annotations are discarded (Appendix E).\n5.3 Results\nFigure 3: The AUROC (OVO) of\nTabPFN versus baseline AutoGluon with\n1-minute time budget on the tabular clas-\nsification problems in AnnotatedTables.TabPFN performs robustly on diverse tabular data\nWe fit TabPFN and AutoGluon on 2,720 tabular classifi-\ncation problems in AnnotatedTables, and we compute the\nAUROC (one-versus-one) and cross entropy for every one\nof them, shown in Table 6. WilCoxon signed-rank test\nshows that all the paired differences between TabPFN and\nAutoGluon for both AUROC and cross entropy and for\nboth 1 minute and 5 minutes are non-zero with statistical\nsignificance (p-values and discussions in Appendix D).\nAnnotatedTables allows us to conduct a large number of\nexperiments to draw statistically significant conclusions,\nbut the paired differences are small and may not be infor-\nmative for model selection, an issue known generally as\n“statistical significance versus practical significance” (Kirk,\n1996; Peeters, 2016; McShane & Gelman, 2022). Overall,\nTabPFN performs on par with the AutoGluon baseline.\nThe classification problems in AnnotatedTables are diverse and have lots of inherent variations, which\nis reflected by the standard deviation of both models’ performance. Between the two, TabPFN has a\nlower standard deviation for both AUROC and cross entropy, showing a more consistent performance.\nThe inherent diversity of AnnotatedTables can be seen from the dot plots in Figure 3, where we\nplot the AUROC of both models versus each other. Given a relatively high spread of AUROC and\noverall on-par performance, it is not possible to pre-determine which model will perform better, and\neither model could be better on a specific table. Practitioners may fit both models and select the\nbetter performing one in practice. Given that AutoGluon is a widely used AutoML framework, we\nbelieve that the newly introduced TabPFN is a welcome addition that would raise the state-of-the-art\nperformance on many real-world tabular classification problems, especially as it takes only a fraction\nof the time budget per table (2 seconds) compared to AutoGluon (1 or 5 minutes in our experiments).\nLLM annotation quality Running TabPFN and AutoGluon on LLM-generated annotation proves\nthat LLMs can successfully identify potential input and target columns in a table and annotate them,\ngiven only a prompt that describes each table’s name, column names and data types. As seen in\nFigure 3, most of the tabular classification problems annotated by LLMs have an AUROC over 0.5\nfor both TabPFN and AutoGluon. This means, for most tables, the target columns identified by LLMs\ndo have non-trivial relationships with the input columns and can be predicted to some extent, and the\nLLM successfully found the suitable tables for the tabular classification task.\n9\n\n6 Conclusions and Discussions\nPrior to this work, human labor had been necessary to acquire tabular dataset annotations. Large\nlanguage models can bring a radical change to how machine learning datasets are constructed, and\nAnnotatedTables is the first to pioneer tabular dataset annotation with LLMs. LLMs can be steered\nto generate annotations based on detailed instructions, and tabular machine learning researchers\ncould easily acquire tailored annotations that fit their research directions. With the human annotation\nbottleneck opened, our methodology could enable more studies with reduced costs and automation.\nReferences\nIz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150 , 2020.\nAlexandre Bento, Amal Zouaq, and Michel Gagnon. Ontology matching using convolutional neural\nnetworks. In Proceedings of the Twelfth Language Resources and Evaluation Conference , pp.\n5648–5653, 2020.\nBernd Bischl, Giuseppe Casalicchio, Matthias Feurer, Pieter Gijsbers, Frank Hutter, Michel Lang,\nRafael G Mantovani, Jan N van Rijn, and Joaquin Vanschoren. Openml benchmarking suites.\narXiv preprint arXiv:1708.03731 , 2017.\nCasper Solheim Bojer and Jens Peder Meldgaard. Kaggle forecasting competitions: An overlooked\nlearning opportunity. International Journal of Forecasting , 37(2):587–603, 2021.\nVadim Borisov, Tobias Leemann, Kathrin Seßler, Johannes Haug, Martin Pawelczyk, and Gjergji\nKasneci. Deep neural networks and tabular data: A survey. IEEE Transactions on Neural Networks\nand Learning Systems , 2022.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.\nShuaichen Chang and Eric Fosler-Lussier. How to prompt LLMs for text-to-SQL: A study in zero-\nshot, single-domain, and cross-domain settings. In NeurIPS 2023 Second Table Representation\nLearning Workshop , 2023. URL https://openreview.net/forum?id=5sOZNkkKh3 .\nTianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the\n22nd acm sigkdd international conference on knowledge discovery and data mining , pp. 785–794,\n2016.\nMichael Chui, James Manyika, Mehdi Miremadi, Nicolaus Henke, Rita Chung, Pieter Nel, and\nSankalp Malhotra. Notes from the ai frontier: Insights from hundreds of use cases. McKinsey\nGlobal Institute , 2:267, 2018.\nCJ Date, Hugh Darwen, and Nikos A Lorentzos. A detailed investigation into the application of\ninterval and relation theory to the problem of temporal database management, 2004.\nNaihao Deng, Yulong Chen, and Yue Zhang. Recent advances in text-to-SQL: A survey of what\nwe have and what we expect. In Nicoletta Calzolari, Chu-Ren Huang, Hansaem Kim, James\nPustejovsky, Leo Wanner, Key-Sun Choi, Pum-Mo Ryu, Hsin-Hsi Chen, Lucia Donatelli, Heng\nJi, Sadao Kurohashi, Patrizia Paggio, Nianwen Xue, Seokhwan Kim, Younggyun Hahm, Zhong\nHe, Tony Kyungil Lee, Enrico Santus, Francis Bond, and Seung-Hoon Na (eds.), Proceedings\nof the 29th International Conference on Computational Linguistics , pp. 2166–2187, Gyeongju,\nRepublic of Korea, October 2022. International Committee on Computational Linguistics. URL\nhttps://aclanthology.org/2022.coling-1.190 .\nNick Erickson, Jonas Mueller, Alexander Shirkov, Hang Zhang, Pedro Larroy, Mu Li, and Alexander\nSmola. Autogluon-tabular: Robust and accurate automl for structured data. arXiv preprint\narXiv:2003.06505 , 2020.\n10\n\nJu Fan, Tongyu Liu, Guoliang Li, Junyou Chen, Yuwei Shen, and Xiaoyong Du. Relational data\nsynthesis using generative adversarial networks: A design space exploration. arXiv preprint\narXiv:2008.12763 , 2020.\nMatthias Feurer, Katharina Eggensperger, Stefan Falkner, Marius Lindauer, and Frank Hutter. Auto-\nsklearn 2.0: Hands-free automl via meta-learning. Journal of Machine Learning Research , 23\n(261):1–61, 2022.\nMarc Fischer, Mislav Balunovic, Dana Drachsler-Cohen, Timon Gehr, Ce Zhang, and Martin Vechev.\nDl2: training and querying neural networks with logic. In International Conference on Machine\nLearning , pp. 1931–1941. PMLR, 2019.\nDawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and Jingren Zhou.\nText-to-sql empowered by large language models: A benchmark evaluation. arXiv preprint\narXiv:2308.15363 , 2023.\nFabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. Chatgpt outperforms crowd workers for\ntext-annotation tasks. Proceedings of the National Academy of Sciences , 120(30):e2305016120,\n2023.\nJiaqi Guo, Ziliang Si, Yu Wang, Qian Liu, Ming Fan, Jian-Guang Lou, Zijiang Yang, and Ting\nLiu. Chase: A large-scale and pragmatic chinese dataset for cross-database context-dependent\ntext-to-sql. In Proceedings of the 59th Annual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Conference on Natural Language Processing (Volume\n1: Long Papers) , pp. 2316–2331, 2021.\nUdit Gupta, Samuel Hsia, Vikram Saraph, Xiaodong Wang, Brandon Reagen, Gu-Yeon Wei, Hsien-\nHsin S Lee, David Brooks, and Carole-Jean Wu. Deeprecsys: A system for optimizing end-to-\nend at-scale neural recommendation inference. In 2020 ACM/IEEE 47th Annual International\nSymposium on Computer Architecture (ISCA) , pp. 982–995. IEEE, 2020.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. In International Conference on\nLearning Representations , 2021. URL https://openreview.net/forum?id=d7KBjmI3GmQ .\nBenjamin Hilprecht, Andreas Schmidt, Moritz Kulessa, Alejandro Molina, Kristian Kersting, and\nCarsten Binnig. Deepdb: Learn from data, not from queries! arXiv preprint arXiv:1909.00607 ,\n2019.\nNoah Hollmann, Samuel Müller, Katharina Eggensperger, and Frank Hutter. TabPFN: A transformer\nthat solves small tabular classification problems in a second. In The Eleventh International\nConference on Learning Representations , 2023. URL https://openreview.net/forum?id=\ncp5PvcI6w8_ .\nPei-Yun Hsueh, Prem Melville, and Vikas Sindhwani. Data quality from crowdsourcing: a study of\nannotation selection criteria. In Proceedings of the NAACL HLT 2009 workshop on active learning\nfor natural language processing , pp. 27–35, 2009.\nFan Huang, Haewoon Kwak, and Jisun An. Is chatgpt better than human annotators? potential and\nlimitations of chatgpt in explaining implicit hate speech. In Companion proceedings of the ACM\nweb conference 2023 , pp. 294–297, 2023.\nWonseok Hwang, Jinyeong Yim, Seunghyun Park, and Minjoon Seo. A comprehensive exploration\non wikisql with table-aware word contextualization. arXiv preprint arXiv:1902.01069 , 2019.\nGuolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan\nLiu. Lightgbm: A highly efficient gradient boosting decision tree. Advances in neural information\nprocessing systems , 30, 2017.\nRoger E. Kirk. Practical significance: A concept whose time has come. Educational and Psy-\nchological Measurement , 56(5):746–759, 1996. doi: 10.1177/0013164496056005002. URL\nhttps://doi.org/10.1177/0013164496056005002 .\nCurtis Knowles. 6nf conceptual models and data warehousing 2.0. 2012.\n11\n\nTim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned\nindex structures. In Proceedings of the 2018 international conference on management of data , pp.\n489–504, 2018.\nChia-Hsuan Lee, Oleksandr Polozov, and Matthew Richardson. KaggleDBQA: Realistic evaluation\nof text-to-SQL parsers. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.),\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural Language Processing (Volume 1: Long\nPapers) , pp. 2261–2273, Online, August 2021. Association for Computational Linguistics. doi:\n10.18653/v1/2021.acl-long.176. URL https://aclanthology.org/2021.acl-long.176 .\nJinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying\nGeng, Nan Huo, et al. Can llm already serve as a database interface? a big bench for large-scale\ndatabase grounded text-to-sqls. Advances in Neural Information Processing Systems , 36, 2023a.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou,\nMarc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with\nyou! arXiv preprint arXiv:2305.06161 , 2023b.\nAiwei Liu, Xuming Hu, Lijie Wen, and Philip S Yu. A comprehensive evaluation of chatgpt’s\nzero-shot text-to-sql capability. arXiv preprint arXiv:2303.13547 , 2023a.\nHenry Liu, Mingbin Xu, Ziting Yu, Vincent Corvinelli, and Calisto Zuzarte. Cardinality estimation\nusing neural networks. In Proceedings of the 25th Annual International Conference on Computer\nScience and Software Engineering , pp. 53–59, 2015.\nLinfeng Liu. Learning from Relational Data via Graph Neural Networks . PhD thesis, Tufts University,\n2022.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: NLG\nevaluation using gpt-4 with better human alignment. In Houda Bouamor, Juan Pino, and Kalika Bali\n(eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing ,\npp. 2511–2522, Singapore, December 2023b. Association for Computational Linguistics. doi: 10.\n18653/v1/2023.emnlp-main.153. URL https://aclanthology.org/2023.emnlp-main.153 .\nRui Mao, Guanyi Chen, Xulang Zhang, Frank Guerin, and Erik Cambria. GPTEval: A survey\non assessments of ChatGPT and GPT-4. In Nicoletta Calzolari, Min-Yen Kan, Veronique\nHoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue (eds.), Proceedings of the 2024\nJoint International Conference on Computational Linguistics, Language Resources and Evalua-\ntion (LREC-COLING 2024) , pp. 7844–7866, Torino, Italia, May 2024. ELRA and ICCL. URL\nhttps://aclanthology.org/2024.lrec-main.693 .\nRyan Marcus, Parimarjan Negi, Hongzi Mao, Chi Zhang, Mohammad Alizadeh, Tim Kraska,\nOlga Papaemmanouil, and Nesime Tatbul. Neo: A learned query optimizer. arXiv preprint\narXiv:1904.03711 , 2019.\nRyan Marcus, Parimarjan Negi, Hongzi Mao, Nesime Tatbul, Mohammad Alizadeh, and Tim Kraska.\nBao: Making learned query optimization practical. In Proceedings of the 2021 International\nConference on Management of Data , pp. 1275–1288, 2021.\nBlakeley B McShane and Andrew Gelman. Selecting on statistical significance and practical impor-\ntance is wrong. Journal of Information Technology , 37(3):312–315, 2022.\nQingkai Min, Yuefeng Shi, and Yue Zhang. A pilot study for Chinese SQL semantic parsing.\nIn Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-IJCNLP) , pp. 3652–3658, Hong Kong,\nChina, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1377.\nURL https://aclanthology.org/D19-1377 .\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke\nZettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In\nProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pp.\n11048–11064, 2022.\n12\n\nParimarjan Negi, Ryan Marcus, Hongzi Mao, Nesime Tatbul, Tim Kraska, and Mohammad Alizadeh.\nCost-guided cardinality estimation: Focus where it matters. In 2020 IEEE 36th International\nConference on Data Engineering Workshops (ICDEW) , pp. 154–157. IEEE, 2020.\nOpenAI. Gpt-4 technical report, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. Advances in neural information processing systems , 35:27730–\n27744, 2022.\nGuansong Pang, Chunhua Shen, Longbing Cao, and Anton Van Den Hengel. Deep learning for\nanomaly detection: A review. ACM computing surveys (CSUR) , 54(2):1–38, 2021.\nPapersWithCode. Table annotation, 2024. URL https://paperswithcode.com/task/\ntable-annotation .\nMichael J Peeters. Practical significance: Moving beyond statistical significance. Currents in\nPharmacy Teaching and Learning , 8(1):83–89, 2016.\nLiudmila Prokhorenkova, Gleb Gusev, Aleksandr V orobev, Anna Veronika Dorogush, and Andrey\nGulin. Catboost: unbiased boosting with categorical features. Advances in neural information\nprocessing systems , 31, 2018.\nBowen Qin, Binyuan Hui, Lihan Wang, Min Yang, Jinyang Li, Binhua Li, Ruiying Geng, Rongyu\nCao, Jian Sun, Luo Si, et al. A survey on text-to-sql parsing: Concepts, methods, and future\ndirections. arXiv preprint arXiv:2208.13629 , 2022.\nLuigi Quaranta, Fabio Calefato, and Filippo Lanubile. Kgtorrent: A dataset of python jupyter\nnotebooks from kaggle. In 2021 IEEE/ACM 18th International Conference on Mining Software\nRepositories (MSR) , pp. 550–554. IEEE, 2021.\nRelationalAI. The rel language, Jan 2021. URL https://docs.relational.ai/rel .\nHenry Gordon Rice. Classes of recursively enumerable sets and their decision problems. Transactions\nof the American Mathematical society , 74(2):358–366, 1953.\nMegan Risdal and Timo Bozsolik. Meta kaggle, 2022. URL https://www.kaggle.com/ds/9 .\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An\nadversarial winograd schema challenge at scale. Communications of the ACM , 64(9):99–106,\n2021.\nRylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language\nmodels a mirage? Advances in Neural Information Processing Systems , 36, 2024.\nXingjian Shi, Jonas Mueller, Nick Erickson, Mu Li, and Alex Smola. Multimodal autoML on\nstructured tables with text fields. In 8th ICML Workshop on Automated Machine Learning\n(AutoML) , 2021. URL https://openreview.net/forum?id=OHAIVOOl7Vl .\nRavid Shwartz-Ziv and Amitai Armon. Tabular data: Deep learning is not all you need. Information\nFusion , 81:84–90, 2022.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam\nFisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the\nimitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint\narXiv:2206.04615 , 2022.\nOfer Strichman. program equivalence. Formal Methods in System Design , 52:227–228, 2018.\nNingyuan Sun, Xuefeng Yang, and Yunfeng Liu. Tableqa: a large-scale chinese text-to-sql dataset for\ntable-aware sql generation. arXiv preprint arXiv:2006.06434 , 2020.\nSouhaib Ben Taieb and Rob J Hyndman. A gradient boosting approach to the kaggle load forecasting\ncompetition. International journal of forecasting , 30(2):382–394, 2014.\n13\n\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,\nLiu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient\ntransformers. arXiv preprint arXiv:2011.04006 , 2020.\nAlexey Tolkachev, Ilyas Sirazitdinov, Maksym Kholiavchenko, Tamerlan Mustafaev, and Bulat\nIbragimov. Deep learning for diagnosis and segmentation of pneumothorax: The results on the\nkaggle competition and validation against radiologists. IEEE Journal of Biomedical and Health\nInformatics , 25(5):1660–1672, 2020.\nMichael Uschold. Ontology and database schema: What’s the difference? Applied Ontology , 10(3-4):\n243–258, 2015.\nApril Yi Wang, Dakuo Wang, Jaimie Drozdal, Xuye Liu, Soya Park, Steve Oney, and Christopher\nBrooks. What makes a well-documented notebook? a case study of data scientists’ documentation\npractices in kaggle. In Extended Abstracts of the 2021 CHI Conference on Human Factors in\nComputing Systems , pp. 1–7, 2021.\nLijie Wang, Ao Zhang, Kun Wu, Ke Sun, Zhenghua Li, Hua Wu, Min Zhang, and Haifeng\nWang. DuSQL: A large-scale and pragmatic Chinese text-to-SQL dataset. In Bonnie Webber,\nTrevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empir-\nical Methods in Natural Language Processing (EMNLP) , pp. 6923–6935, Online, November\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.562. URL\nhttps://aclanthology.org/2020.emnlp-main.562 .\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models.\nTransactions on Machine Learning Research , 2022.\nFrank Wilcoxon. Individual comparisons by ranking methods. Biometrics Bulletin , 1(6):80–83, 1945.\nISSN 00994987. URL http://www.jstor.org/stable/3001968 .\nLucas Woltmann, Claudio Hartmann, Maik Thiele, Dirk Habich, and Wolfgang Lehner. Cardinality\nestimation with local deep learning models. In Proceedings of the second international workshop\non exploiting artificial intelligence techniques for data management , pp. 1–8, 2019.\nXiaojun Xu, Chang Liu, and Dawn Song. Sqlnet: Generating structured queries from natural language\nwithout reinforcement learning. arXiv preprint arXiv:1711.04436 , 2017.\nNavid Yaghmazadeh, Yuepeng Wang, Isil Dillig, and Thomas Dillig. Sqlizer: query synthesis from\nnatural language. Proceedings of the ACM on Programming Languages , 1(OOPSLA):1–26, 2017.\nZeyu Yan, Jianqiang Ma, Yang Zhang, and Jianping Shen. Sql generation via machine reading\ncomprehension. In Proceedings of the 28th International Conference on Computational Linguistics ,\npp. 350–356, 2020.\nXulei Yang and Jie Ding. A computational framework for iceberg and ship discrimination: Case\nstudy on kaggle competition. IEEE Access , 8:82320–82327, 2020.\nXulei Yang, Zeng Zeng, Sin G Teo, Li Wang, Vijay Chandrasekhar, and Steven Hoi. Deep learning\nfor practical image recognition: Case study on kaggle competitions. In Proceedings of the 24th\nACM SIGKDD international conference on knowledge discovery & data mining , pp. 923–931,\n2018.\nZongheng Yang, Eric Liang, Amog Kamsetty, Chenggang Wu, Yan Duan, Xi Chen, Pieter Abbeel,\nJoseph M Hellerstein, Sanjay Krishnan, and Ion Stoica. Deep unsupervised cardinality estimation.\narXiv preprint arXiv:1905.04278 , 2019.\nZongheng Yang, Amog Kamsetty, Sifei Luan, Eric Liang, Yan Duan, Xi Chen, and Ion Stoica.\nNeurocard: one cardinality estimator for all tables. arXiv preprint arXiv:2006.08109 , 2020.\nSemih Yavuz, Izzeddin Gür, Yu Su, and Xifeng Yan. What it takes to achieve 100% condition\naccuracy on wikisql. In Proceedings of the 2018 Conference on Empirical Methods in Natural\nLanguage Processing , pp. 1702–1711, 2018.\n14\n\nTao Yu, Zifan Li, Zilin Zhang, Rui Zhang, and Dragomir Radev. Typesql: Knowledge-based\ntype-aware neural text-to-sql generation. arXiv preprint arXiv:1804.09769 , 2018a.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li,\nQingning Yao, Shanelle Roman, et al. Spider: A large-scale human-labeled dataset for complex\nand cross-domain semantic parsing and text-to-sql task. arXiv preprint arXiv:1809.08887 , 2018b.\nTao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria Lin, Suyi Li, Heyang Er, Irene Li,\nBo Pang, Tao Chen, Emily Ji, Shreya Dixit, David Proctor, Sungrok Shim, Jonathan Kraft, Vincent\nZhang, Caiming Xiong, Richard Socher, and Dragomir Radev. SParC: Cross-domain semantic\nparsing in context. In Anna Korhonen, David Traum, and Lluís Màrquez (eds.), Proceedings of the\n57th Annual Meeting of the Association for Computational Linguistics , pp. 4511–4523, Florence,\nItaly, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1443. URL\nhttps://aclanthology.org/P19-1443 .\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a\nmachine really finish your sentence? In Anna Korhonen, David Traum, and Lluís Màrquez\n(eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics ,\npp. 4791–4800, Florence, Italy, July 2019. Association for Computational Linguistics. doi:\n10.18653/v1/P19-1472. URL https://aclanthology.org/P19-1472 .\nVictor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from\nnatural language using reinforcement learning. arXiv preprint arXiv:1709.00103 , 2017.\nA Example Prompts for Large Language Models Annotation\nAn example of the prompt for the zero-shot synthesis of SQL code is shown below.\n1We have a SQL database with 2 tables. For each table, here is the schema and\n2a sample row data.\n3Schema for table 0:\n4CREATE TABLE MaLga12345(\n5 Saledate TEXT,\n6 MA BIGINT,\n7 Type TEXT,\n8 Bedrooms BIGINT\n9);\n10A sample row data from table 0:\n11{\"Saledate\": \"30/09/2007\", \"MA\": 441854, \"Type\": \"house\", \"Bedrooms\": 2}\n12Schema for table 1:\n13CREATE TABLE RawSales(\n14 Datesold TEXT,\n15 Postcode BIGINT,\n16 Price BIGINT,\n17 PropertyType TEXT,\n18 Bedrooms BIGINT\n19);\n20A sample row data from table 1:\n21{\"Datesold\": \"2007-02-07 00:00:00\", \"Postcode\": 2607, \"Price\": 525000,\n22\"PropertyType\": \"house\", \"Bedrooms\": 4}\n23Consider the typical users who access this database. What kind of SQLite\n24SELECT queries would they write? List 15 examples SQL code that are\n25directly executable with relatively high complexity, each following a\n26description. If possible, use joins. Reply in the format of a description\n27and the SQL code for each example.\nAn example of the prompt for SQL to Rel translation is shown below.\n1REL is a database management system language that is similar to datalog. REL is\nbased on the sixth normal form (6NF),\n2where every variable represents a relation, i.e. a set of tuples.\n3In REL, from-variables will not appear in the output, and for-variables will appear\nin the output.\n15\n\n4Strings in REL use double quotes.\n5Below are few examples of SQL code and REL code pairs that perform the same query.\n6Examples:\n7\n81.SQL:\n9‘‘‘\n10SELECT L.JobTitle, L.ExperienceLevel, V3.JobTitle, V3.ExperienceLevel FROM\nLatestDataScienceSalaries AS L INNER JOIN V3LatestDataScienceSalaries AS V3 ON\nL.CompanyLocation = V3.CompanyLocation WHERE L.CompanyLocation = \"United States\n\";\n11‘‘‘\n121.REL:\n13‘‘‘\n14def temp[x,y] = LatestDataScienceSalaries:CompanyLocation[x, company_location] and\nV3LatestDataScienceSalaries:CompanyLocation[y, company_location] and\ncompany_location = \"United States\" from company_location\n15def output[x_job_title, x_experience_level, y_job_title, y_experience_level] =\nLatestDataScienceSalaries:JobTitle[x, x_job_title], LatestDataScienceSalaries:\nExperienceLevel[x, x_experience_level], V3LatestDataScienceSalaries:JobTitle[y,\ny_job_title], V3LatestDataScienceSalaries:ExperienceLevel[y,\ny_experience_level] and temp[x,y] for x, y\n16‘‘‘\n172.SQL:\n18‘‘‘\n19SELECT p.ImageLink FROM Photos p JOIN Likes l ON p.Id = l.Photo JOIN Users u ON l.\nUser = u.Id LEFT JOIN Follows f ON u.Id = f.Follower AND f.Followee = p.UserID\nWHERE f.Follower IS NULL;\n20‘‘‘\n212.REL:\n22‘‘‘\n23def UserPhoto(user_id, photo_id) = Photos:UserID[l,user_id] and Photos:Id[l,\nphoto_id] from l\n24def UserLikesPhoto(user_id, photo_id) = Likes:User[l,user_id] and Likes:Photo[l,\nphoto_id] from l\n25def UserID(user_id) = Users:Id(l, user_id) from l\n26def UserFollowsUser(follower_id, followee_id) = Follows:Follower[l,follower_id] and\nFollows:Followee[l, followee_id] from l\n27def PhotoIDHasImageLink(photo_id, image_link) =\n28 Photos:Id(p, photo_id) and Photos:ImageLink(p, image_link) from p\n29def photos_liked_by_not_a_follower(photo) =\n30 UserPhoto(poster, photo)\n31 and UserLikesPhoto(liker, photo)\n32 and not UserFollowsUser(liker, poster)\n33 from poster, liker\n34def output = PhotoIDHasImageLink[photos_liked_by_not_a_follower]\n35‘‘‘\n363.SQL:\n37‘‘‘\n38SELECT MainTable.StateB, MainTable.Name FROM MainTable JOIN CaseUniverse ON\nMainTable.StateB = CaseUniverse.TargetState AND MainTable.StateBCode =\nCaseUniverse.TargetCOWID WHERE TargetState = \"FRN\" AND TargetCOWID = \"220\";\n39‘‘‘\n403.REL:\n41‘‘‘\n42def join[mt, cu] = MainTable:StateB[mt, sb] and CaseUniverse:TargetState[cu, sb] and\nMainTable:StateBCode[mt, sbc] and CaseUniverse:TargetCOWID[cu, sbc] and sb=\"\nFRN\" and sbc=\"220\" from sb, sbc\n43def output = MainTable:StateB[mt], MainTable:Name[mt] for mt, cu where join[mt, cu]\n44‘‘‘\n454.SQL:\n46‘‘‘\n47SELECT LD.Regions, SUM(DT.Punjab + DT.Haryana + DT.Rajasthan + DT.Delhi + DT.UP + DT\n.Uttarakhand) AS Total_Usage FROM DatasetTk DT INNER JOIN LongData LD ON DT.\nPunjab = LD.Usage AND LD.States = ’Punjab’ GROUP BY LD.Regions;\n48‘‘‘\n16\n\n494.REL:\n50‘‘‘\n51def join1[dt,ld]= DatasetTk:Punjab[dt, punjab] and LongData:Usage[ld, punjab] from\npunjab\n52def sum_usage = DatasetTk:Punjab[dt] + DatasetTk:Haryana[dt] +\n53DatasetTk:Rajasthan[dt] + DatasetTk:Delhi[dt] +\n54DatasetTk:UP[dt] + DatasetTk:Uttarakhand[dt] for dt\n55def output[region] = sum[dt, usage:\n56LongData:Regions[ld, region]\n57and sum_usage[dt, usage]\n58and join1[dt,ld] and LongData:States[ld] = \"Punjab\" for ld]\n59\n60‘‘‘\n615.SQL:\n62‘‘‘\n63SELECT Test.ID, Test.Age, Train.SpendingScore FROM Test INNER JOIN Train ON Test.ID\n= Train.ID WHERE Test.Age >= 40 OR Train.Age >= 40;\n64‘‘‘\n655.REL:\n66‘‘‘\n67def output[id, age, spending_score] = Test:ID[test_idx, id], Test:Age[test_idx, age\n], Train:SpendingScore[train_idx, spending_score] and Test:ID[test_idx, id]\nand Train:ID[train_idx, id] and (Test:Age[test_idx]>=40 or Train:Age[train_idx]\n>= 40) from test_idx, train_idx\n68‘‘‘\n696.SQL:\n70‘‘‘\n71SELECT bb.PlayerOut, m.WinningTeam FROM IPLBallByBall2022 bb JOIN IPLMatches2022 m\nON bb.ID = m.ID WHERE bb.ID = 1312200 AND bb.IsWicketDelivery = 1;\n72‘‘‘\n736.REL:\n74‘‘‘\n75def join[bb, m] = IPLBallByBall2022:ID[bb, id] and IPLMatches2022:ID[m, id] and\nIPLBallByBall2022:ID[bb]=1312200 and IPLBallByBall2022:IsWicketDelivery[bb]=1\nfrom id\n76def output = IPLBallByBall2022:PlayerOut[bb], IPLMatches2022:WinningTeam[m] from bb,\nm where join[bb,m]\n77‘‘‘\n787.SQL:\n79‘‘‘\n80SELECT il.AccountName, il.Ismarketplace FROM InstAccLabeled AS il JOIN InstAcc AS ia\nON il.AccountName = ia.Username WHERE ia.IsBusinessAccount = 1;\n81‘‘‘\n827.REL:\n83‘‘‘\n84def join[il, ia] = InstAccLabeled:AccountName[il, account_name] and InstAcc:Username\n[ia, account_name] and InstAcc:IsBusinessAccount[ia]=boolean_true from\naccount_name\n85def output = InstAccLabeled:AccountName[il], InstAccLabeled:Ismarketplace[il] for\nil, ia where join[il, ia]\n86‘‘‘\n878.SQL:\n88‘‘‘\n89SELECT table0.Area, table0.AveragePrice, table1.MedianSalary FROM\nHousingInLondonMonthlyVariables AS table0 JOIN HousingInLondonYearlyVariables\nAS table1 ON table0.Code = table1.Code WHERE table0.BoroughFlag = 1;\n90‘‘‘\n918.REL:\n92‘‘‘\n93def join[row_id_0, row_id_1] = HousingInLondonMonthlyVariables:Code[row_id_0, code]\nand HousingInLondonYearlyVariables:Code[row_id_1, code] and\nHousingInLondonMonthlyVariables:BoroughFlag[row_id_0]=1 from code\n94def output[area, average_price, median_salary] = HousingInLondonMonthlyVariables:\nArea[row_id_0, area] and HousingInLondonMonthlyVariables:AveragePrice[row_id_0,\n17\n\naverage_price] and HousingInLondonYearlyVariables:MedianSalary[row_id_1,\nmedian_salary] and join[row_id_0, row_id_1] for row_id_0, row_id_1\n95‘‘‘\n969.SQL:\n97‘‘‘\n98SELECT Gender, Segmentation, COUNT(*) FROM Test WHERE Segmentation IN (’B’, ’C’)\nGROUP BY Gender, Segmentation;\n99‘‘‘\n1009.REL:\n101‘‘‘\n102def output[gender, seg] = count[row_id: Test:Gender(row_id, gender) and Test:\nSegmentation(row_id,seg) and {\"B\"; \"C\"}(seg)]\n103‘‘‘\n10410.SQL:\n105‘‘‘\n106SELECT DISTINCT State FROM NCHSLeadingCausesOfDeathUnitedStates;\n107‘‘‘\n10810.REL:\n109‘‘‘\n110def output[state] = NCHSLeadingCausesOfDeathUnitedStates:State[x,state] from x\n111‘‘‘\n11211.SQL:\n113‘‘‘\n114SELECT * FROM HRCommaSep;\n115‘‘‘\n11611.REL:\n117‘‘‘\n118def output[col, row, val] = HRCommaSep[col, row, val]\n119‘‘‘\n12012.SQL:\n121‘‘‘\n122SELECT Title, Year FROM TvShows;\n123‘‘‘\n12412.REL:\n125‘‘‘\n126def output[title, year] = TvShows:Title[idx, title], TvShows:Year[idx, year] for idx\n127‘‘‘\n12813.SQL:\n129‘‘‘\n130SELECT AverageMontlyHours FROM HRCommaSep WHERE Department = ’sales’;\n131‘‘‘\n13213.REL:\n133‘‘‘\n134def output[average_monthly_hours] = HRCommaSep:Department(row_idx, \"sales\") and\nHRCommaSep:AverageMontlyHours(row_idx, average_monthly_hours) for row_idx\n135‘‘‘\n13614.SQL:\n137‘‘‘\n138SELECT Location, Age FROM User WHERE UserId = 1;\n139‘‘‘\n14014.REL:\n141‘‘‘\n142def output[location, age] = User:Location[idx, location], User:Age[idx, age], User:\nUserId[idx, 1] for idx\n143‘‘‘\n14415.SQL:\n145‘‘‘\n146SELECT * FROM GMPlayersStatistics WHERE isstreamer = 1 AND bulletwin > 70;\n147‘‘‘\n14815.REL:\n149‘‘‘\n150def output[col, row, val] = GMPlayersStatistics[col, row, val] and\nGMPlayersStatistics:IsStreamer[row] = boolean_true and GMPlayersStatistics:\nBulletWin[row] > 70\n151‘‘‘\n18\n\n15216.SQL:\n153‘‘‘\n154SELECT AVG(Visibilitykm), MAX(WindSpeedkmh) FROM WeatherData WHERE Weather = \"Rain\";\n155‘‘‘\n15616.REL:\n157‘‘‘\n158def all_row_ids_with_rain[row_id] = WeatherData:Weather[row_id]=\"Rain\"\n159def output = average[row_id, value_vis: all_row_ids_with_rain(row_id) and\nWeatherData:VisibilityKm(row_id, value_vis)], max[row_id, value_wind_speed :\nall_row_ids_with_rain(row_id) and WeatherData:WindSpeedKmh(row_id,\nvalue_wind_speed)]\n160‘‘‘\n16117.SQL:\n162‘‘‘\n163SELECT * FROM Data WHERE Danceability > 0.8;\n164‘‘‘\n16517.REL:\n166‘‘‘\n167def output[col, idx, val] = Data[col, idx, val] and Data:Danceability[idx] > 0.8\n168‘‘‘\n16918.SQL:\n170‘‘‘\n171SELECT * FROM SpaceCorrected WHERE StatusMission IN (’Success’, ’Failure’);\n172‘‘‘\n17318.REL:\n174‘‘‘\n175def output[col, row_id, val] = SpaceCorrected[col, row_id, val] and {\"Success\"; \"\nFailure\"}(SpaceCorrected:StatusMission[row_id])\n176‘‘‘\n17719.SQL:\n178‘‘‘\n179SELECT * FROM SpaceCorrected WHERE Location LIKE ’%\n180‘‘‘\n18119.REL:\n182‘‘‘\n183def output[col, row_id, val] = SpaceCorrected[col, row_id, val] and like_match(\"\\%\nFlorida\\%\", SpaceCorrected:Location[row_id])\n184‘‘‘\n18520.SQL:\n186‘‘‘\n187SELECT * FROM Cars24Combined WHERE Fuel = ’PETROL’;\n188‘‘‘\n18920.REL:\n190‘‘‘\n191def output[colname, row_idx, val] = Cars24Combined[colname, row_idx, val] and\nCars24Combined[:Fuel, row_idx, \"PETROL\"]\n192‘‘‘\n19321.SQL:\n194‘‘‘\n195SELECT DISTINCT Name FROM UCIDatasets;\n196‘‘‘\n19721.REL:\n198‘‘‘\n199def output[name] = UCIDatasets:Name[idx, name] from idx\n200‘‘‘\n20122.SQL:\n202‘‘‘\n203SELECT COUNT(*) FROM SupermarketSalesSheet1;\n204‘‘‘\n20522.REL:\n206‘‘‘\n207def count_rows = count[row_id: SupermarketSalesSheet1[_, row_id, _]]\n208 def output = count_rows\n209‘‘‘\n21023.SQL:\n19\n\n211‘‘‘\n212SELECT * FROM Jobs WHERE ActivelyHiring = 1.0;\n213‘‘‘\n21423.REL:\n215‘‘‘\n216def output[col, idx, val] = Jobs[col, idx, val] and Jobs:ActivelyHiring[idx] = 1.0\n217‘‘‘\n21824.SQL:\n219‘‘‘\n220SELECT marvelcomiccharactername, comicappearance1 FROM MarvelComicsLegacy WHERE\nmarvelcomiccharactername = ’Wolverine’;\n221‘‘‘\n22224.REL:\n223‘‘‘\n224def output[marvel_character_name, comic_appearance_1] = MarvelComicsLegacy:\nMarvelComicCharacterName[idx, marvel_character_name], MarvelComicsLegacy:\nComicAppearance1[idx, comic_appearance_1] and marvel_character_name = \"\nWolverine\" for idx\n225‘‘‘\n22625.SQL:\n227‘‘‘\n228SELECT * FROM WAFnUseCTelcoCustomerChurn WHERE Churn = \"Yes\";\n229‘‘‘\n23025.REL:\n231‘‘‘\n232def output[col, row_idx, val] = WAFnUseCTelcoCustomerChurn[col, row_idx, val] and\nWAFnUseCTelcoCustomerChurn:Churn[row_idx] = \"Yes\"\n233‘‘‘\n23426.SQL:\n235‘‘‘\n236SELECT AVG(Charges) FROM Insurance WHERE Age > 40;\n237‘‘‘\n23826.REL:\n239‘‘‘\n240def output = average[idx, charges: Insurance:Charges[idx, charges] and Insurance:Age\n[idx] > 40]\n241‘‘‘\n24227.SQL:\n243‘‘‘\n244SELECT * FROM CommentsCleaned WHERE EmojiUsed = \"yes\";\n245‘‘‘\n24627.REL:\n247‘‘‘\n248def output[col, row_id, val] = CommentsCleaned[col, row_id, val] and CommentsCleaned\n:EmojiUsed[row_id] = \"yes\"\n249‘‘‘\n25028.SQL:\n251‘‘‘\n252SELECT * FROM Cars24Combined WHERE Fuel = \"PETROL\" AND Location = \"HR-98\";\n253‘‘‘\n25428.REL:\n255‘‘‘\n256def output[col, idx, val] = Cars24Combined[col, idx, val] and Cars24Combined:Fuel[\nidx, \"PETROL\"] and Cars24Combined:Location[idx, \"HR-98\"]\n257‘‘‘\n25829.SQL:\n259‘‘‘\n260SELECT StateUT, AVG(NumberOfHouseholdsSurveyed) as AverageHouseholds FROM Datafile\nGROUP BY StateUT;\n261‘‘‘\n26229.REL:\n263‘‘‘\n264def output[state] = average[idx, num: Datafile:NumberOfHouseholdsSurveyed[idx, num]\nand Datafile:StateUT[idx, state]]\n265‘‘‘\n20\n\n26630.SQL:\n267‘‘‘\n268SELECT * FROM Cars24Combined WHERE Fuel = ’PETROL’ AND Location = ’HR-98’;\n269‘‘‘\n27030.REL:\n271‘‘‘\n272def output[col, idx, val] = Cars24Combined[col, idx, val] and Cars24Combined:Fuel[\nidx, \"PETROL\"] and Cars24Combined:Location[idx, \"HR-98\"]\n273‘‘‘\n27431.SQL:\n275‘‘‘\n276SELECT Gender, COUNT(*) FROM Diabetes GROUP BY Gender;\n277‘‘‘\n27831.REL:\n279‘‘‘\n280def output[gender] = count[row_id: Diabetes:Gender[row_id, gender]]\n281‘‘‘\n28232.SQL:\n283‘‘‘\n284SELECT JobTitle, AVG(Salary) AS AverageSalary\n285FROM PartiallyCleanedSalaryDataset\n286GROUP BY JobTitle;\n287‘‘‘\n28832.REL:\n289‘‘‘\n290def output[job_title] = average[idx, salary: PartiallyCleanedSalaryDataset:Salary[\nidx, salary] and PartiallyCleanedSalaryDataset:JobTitle[idx, job_title]]\n291‘‘‘\n29233.SQL:\n293‘‘‘\n294SELECT Race, COUNT(*) AS TotalPitstops\n295FROM Pitstops\n296GROUP BY Race;\n297‘‘‘\n29833.REL:\n299‘‘‘\n300def output[race] = count[row_id: Pitstops:Race[row_id, race]]\n301‘‘‘\n30234.SQL:\n303‘‘‘\n304SELECT Title, Rating FROM TopAnime WHERE Rating = (SELECT MAX(Rating) FROM TopAnime)\n;\n305‘‘‘\n30634.REL:\n307‘‘‘\n308def max_rating = max[x, rating : TopAnime:Rating[x, rating]]\n309def output[title, rating] = TopAnime:Title[idx, title], TopAnime:Rating[idx, rating]\nand TopAnime:Rating[idx, max_rating] for idx\n310‘‘‘\n31135.SQL:\n312‘‘‘\n313SELECT Category, SUM(Sales) AS TotalSales, SUM(Profit) AS TotalProfit\n314FROM SampleSuperstore\n315GROUP BY Category;\n316‘‘‘\n31735.REL:\n318‘‘‘\n319def output[category] = sum[row_id, sales: SampleSuperstore:Category(row_id, category\n) and SampleSuperstore:Sales(row_id, sales)], sum[row_id, profit:\nSampleSuperstore:Category(row_id, category) and SampleSuperstore:Profit(row_id,\nprofit)]\n320‘‘‘\n32136.SQL:\n322‘‘‘\n323SELECT SUM(CustomerCareCalls)\n21\n\n324FROM Train\n325WHERE DiscountOffered >= 50;\n326‘‘‘\n32736.REL:\n328‘‘‘\n329def output = sum[row_id,calls: Train:CustomerCareCalls[row_id, calls] and Train:\nDiscountOffered[row_id] >= 50]\n330‘‘‘\n33137.SQL:\n332‘‘‘\n333SELECT AVG(MonthlyCharges) FROM CustomerChurn WHERE Churn = ’Yes’;\n334‘‘‘\n33537.REL:\n336‘‘‘\n337def output = average[row_id, monthly_charges: CustomerChurn:MonthlyCharges(row_id,\nmonthly_charges) and CustomerChurn:Churn[row_id] = \"Yes\"]\n338‘‘‘\n33938.SQL:\n340‘‘‘\n341SELECT COUNT(DISTINCT StatusType) AS UniqueStatusTypes\n342FROM Live;\n343‘‘‘\n34438.REL:\n345‘‘‘\n346def output = count[status_type: Live:StatusType(row_id, status_type) from row_id]\n347‘‘‘\n34839.SQL:\n349‘‘‘\n350SELECT * FROM PCOSInfertility ORDER BY SlNo DESC;\n351‘‘‘\n35239.REL:\n353‘‘‘\n354def output[col, row_idx, val] = PCOSInfertility[col, row_idx, val]\n355‘‘‘\n35640.SQL:\n357‘‘‘\n358SELECT SUM(WeeklySales) AS TotalWeeklySales FROM WalmartStoreSales;\n359‘‘‘\n36040.REL:\n361‘‘‘\n362def output = sum[row_idx, weekly_sales : WalmartStoreSales:WeeklySales[row_idx,\nweekly_sales]]\n363‘‘‘\n36441.SQL:\n365‘‘‘\n366SELECT * FROM WeatherData ORDER BY RelativeHumidity ASC;\n367‘‘‘\n36841.REL:\n369‘‘‘\n370def output[col, row_id, val] = WeatherData[col, row_id, val]\n371‘‘‘\n37242.SQL:\n373‘‘‘\n374SELECT ShipMode, SUM(Sales) AS TotalSales FROM SampleSuperstore GROUP BY ShipMode;\n375‘‘‘\n37642.REL:\n377‘‘‘\n378def output[shipmode] = sum[row_id, value_sales: SampleSuperstore:Sales[row_id,\nvalue_sales] and SampleSuperstore:ShipMode[row_id, shipmode]]\n379‘‘‘\n38043.SQL:\n381‘‘‘\n382SELECT PlayerOfMatch, WinningTeam\n383FROM IPLMatches2022\n384WHERE WinningTeam = \"Rajasthan Royals\";\n22\n\n385‘‘‘\n38643.REL:\n387‘‘‘\n388def output[player_of_match, winning_team] = IPLMatches2022:PlayerOfMatch[idx,\nplayer_of_match], IPLMatches2022:WinningTeam[idx, winning_team] and\nIPLMatches2022:WinningTeam[idx, winning_team] and winning_team = \"Rajasthan\nRoyals\" for idx\n389‘‘‘\n39044.SQL:\n391‘‘‘\n392SELECT Title, Desc FROM MegaGymDataset WHERE Type = \"Strength\";\n393‘‘‘\n39444.REL:\n395‘‘‘\n396def output[title, desc] = MegaGymDataset:Title[idx, title], MegaGymDataset:Desc[idx,\ndesc] and MegaGymDataset:Type[idx, \"Strength\"] for idx\n397‘‘‘\n39845.SQL:\n399‘‘‘\n400SELECT TypeOfInternship, COUNT(*) as Count FROM Internship GROUP BY TypeOfInternship\n;\n401‘‘‘\n40245.REL:\n403‘‘‘\n404def output[type_of_internship] = count[row_id: Internship:TypeOfInternship[row_id,\ntype_of_internship]]\n405‘‘‘\n406Given examples above, translate the following SQL queries to REL programs. No\nexplanation is needed.\n407Output only the REL code one by one numbered with ’1.REL:’, ’2.REL:’, for example.\n408\n4091.SQL:\n410‘‘‘\n411SELECT * FROM BookingSaudiArabia ORDER BY Score DESC LIMIT 5;\n412‘‘‘\n4132.SQL:\n414‘‘‘\n415SELECT *\n416FROM VsrrProvisionalDrugOverdoseDeathCounts\n417WHERE DataValue > 5000;\n418‘‘‘\n4193.SQL:\n420‘‘‘\n421SELECT COUNT(*) FROM Tweets;\n422‘‘‘\n4234.SQL:\n424‘‘‘\n425SELECT AVG(PredSvr) FROM PredSvr INNER JOIN PredXgb ON PredSvr.PredSvr = PredXgb.\nXgCost;\n426‘‘‘\n4275.SQL:\n428‘‘‘\n429SELECT * FROM ChurnTrainTransformed WHERE CreditPerProduct < 400 OR EstSalaryScaled\nIS NULL\n430UNION\n431SELECT * FROM TestDfTransformed WHERE CreditPerProduct < 400 OR EstSalaryScaled IS\nNULL;\n432‘‘‘\n4336.SQL:\n434‘‘‘\n435SELECT * FROM Leads WHERE LastNotableActivity = ’Modified’;\n436‘‘‘\n4377.SQL:\n438‘‘‘\n23\n\n439SELECT CentralPressure, MaximumSustainedWindSpeed FROM TyphoonData WHERE\nIndicatorOfLandfallOrPassage = ’Landfall’;\n440‘‘‘\n4418.SQL:\n442‘‘‘\n443SELECT SUM(A28) FROM Texture WHERE A17 < (SELECT AVG(A17) FROM Texture);\n444‘‘‘\n4459.SQL:\n446‘‘‘\n447SELECT FlatModel, MAX(ResalePrice) AS MaxResalePrice\n448 FROM ResaleFlatPricesBasedOnApprovalDate2000Feb2012\n449 GROUP BY FlatModel;\n450‘‘‘\n45110.SQL:\n452‘‘‘\n453SELECT COUNT(*) FROM TestFile;\n454‘‘‘\nAn example of the prompt for input-target column annotation is shown below.\n1Consider a machine learning model that takes a few numeric input columns\n2and predict a single classification target column. Given the following\n3schema of a data table, suggest the input columns and target column, such\n4that the target may be predicted from the inputs non-trivially.\n5Schema: CREATE TABLE PartiallyCleanedSalaryDataset(\n6 CompanyName TEXT,\n7 JobTitle TEXT,\n8 SalariesReported BIGINT,\n9 Location TEXT,\n10 Salary FLOAT\n11);\n12Respond in JSON format with ‘input_columns’ and ‘output_column’.\nB Steering LLMs to Generate More Complex SQL Programs\nDuring prototyping, we refined the instructions to create the best annotation results. One of our goals\nwas to increase the complexity of SQL programs generated by the LLM. We experimented with\ndifferent prompts, and we found that instructing the LLM to generate SQL programs “with relatively\nhigh complexity” can increase the complexity and reduce the number of simple queries generated\nby the LLM. An example is given here to illustrate the effect of including the steering phrase and\nwithout the steering phrase.\nThe following is a prompt for the zero-shot synthesis of SQL code without the steering phrase “with\nrelatively high complexity”.\n1We have a SQL database with 2 tables. For each table, here is the schema and\n2a sample row data.\n3Schema for table 0:\n4CREATE TABLE MaLga12345(\n5 Saledate TEXT,\n6 MA BIGINT,\n7 Type TEXT,\n8 Bedrooms BIGINT\n9);\n10A sample row data from table 0:\n11{\"Saledate\": \"30/09/2007\", \"MA\": 441854, \"Type\": \"house\", \"Bedrooms\": 2}\n12Schema for table 1:\n13CREATE TABLE RawSales(\n14 Datesold TEXT,\n15 Postcode BIGINT,\n16 Price BIGINT,\n17 PropertyType TEXT,\n18 Bedrooms BIGINT\n24\n\n19);\n20A sample row data from table 1:\n21{\"Datesold\": \"2007-02-07 00:00:00\", \"Postcode\": 2607, \"Price\": 525000,\n22\"PropertyType\": \"house\", \"Bedrooms\": 4}\n23Consider the typical users who access this database. What kind of SQLite\n24SELECT queries would they write? List 15 examples SQL code that are\n25directly executable, each following a description. If possible, use joins.\n26Reply in the format of a description and the SQL code for each example.\nHere is the annotation generated by the LLM when the steering phrase is not used.\n11. Get all rows from MaLga12345 table:\n2‘‘‘sql\n3SELECT * FROM MaLga12345;\n4‘‘‘\n5\n62. Get all rows from RawSales table:\n7‘‘‘sql\n8SELECT * FROM RawSales;\n9‘‘‘\n10\n113. Find the average price of properties sold by postcode:\n12‘‘‘sql\n13SELECT Postcode, AVG(Price) as AveragePrice\n14FROM RawSales\n15GROUP BY Postcode;\n16‘‘‘\n17\n184. List all house sales from MaLga12345:\n19‘‘‘sql\n20SELECT * FROM MaLga12345\n21WHERE Type = ’house’;\n22‘‘‘\n23\n245. List all properties sold on a specific date from RawSales:\n25‘‘‘sql\n26SELECT * FROM RawSales\n27WHERE Datesold = ’2007-02-07 00:00:00’;\n28‘‘‘\n29\n306. Find the total number of bedrooms sold in RawSales:\n31‘‘‘sql\n32SELECT SUM(Bedrooms) as TotalBedrooms\n33FROM RawSales;\n34‘‘‘\n35\n367. Get the maximum MA value from MaLga12345:\n37‘‘‘sql\n38SELECT MAX(MA) as MaxMA\n39FROM MaLga12345;\n40‘‘‘\n41\n428. List properties sold in a specific postcode with more than 3 bedrooms from\nRawSales:\n43‘‘‘sql\n44SELECT * FROM RawSales\n45WHERE Postcode = 2607 AND Bedrooms > 3;\n46‘‘‘\n47\n489. Count the number of sales by property type from RawSales:\n49‘‘‘sql\n50SELECT PropertyType, COUNT(*) as SalesCount\n51FROM RawSales\n52GROUP BY PropertyType;\n53‘‘‘\n25\n\n54\n5510. Get all properties sold in a specific year from RawSales:\n56‘‘‘sql\n57SELECT * FROM RawSales\n58WHERE Datesold LIKE ’2007%\n59‘‘‘\n60\n6111. Find the sum of prices for all house sales from RawSales:\n62‘‘‘sql\n63SELECT SUM(Price) as TotalHouseSales\n64FROM RawSales\n65WHERE PropertyType = ’house’;\n66‘‘‘\n67\n6812. Join MaLga12345 and RawSales tables on bedrooms and list matching records:\n69‘‘‘sql\n70SELECT a.*, b.*\n71FROM MaLga12345 a\n72JOIN RawSales b ON a.Bedrooms = b.Bedrooms;\n73‘‘‘\n74\n7513. Find properties with the same sale date and date sold from both tables:\n76‘‘‘sql\n77SELECT a.*, b.*\n78FROM MaLga12345 a\n79JOIN RawSales b ON a.Saledate = b.Datesold;\n80‘‘‘\n81\n8214. Get the count of properties sold for each MA value from MaLga12345:\n83‘‘‘sql\n84SELECT MA, COUNT(*) as CountOfSales\n85FROM MaLga12345\n86GROUP BY MA;\n87‘‘‘\n88\n8915. List all properties from MaLga12345 and their corresponding prices from RawSales\nif they have the same number of bedrooms:\n90‘‘‘sql\n91SELECT a.*, b.Price\n92FROM MaLga12345 a\n93JOIN RawSales b ON a.Bedrooms = b.Bedrooms;\n94‘‘‘\nThe following is a prompt for the zero-shot synthesis of SQL code that has the steering phrase “with\nrelatively high complexity”.\n1We have a SQL database with 2 tables. For each table, here is the schema and\n2a sample row data.\n3Schema for table 0:\n4CREATE TABLE MaLga12345(\n5 Saledate TEXT,\n6 MA BIGINT,\n7 Type TEXT,\n8 Bedrooms BIGINT\n9);\n10A sample row data from table 0:\n11{\"Saledate\": \"30/09/2007\", \"MA\": 441854, \"Type\": \"house\", \"Bedrooms\": 2}\n12Schema for table 1:\n13CREATE TABLE RawSales(\n14 Datesold TEXT,\n15 Postcode BIGINT,\n16 Price BIGINT,\n17 PropertyType TEXT,\n18 Bedrooms BIGINT\n19);\n26\n\n20A sample row data from table 1:\n21{\"Datesold\": \"2007-02-07 00:00:00\", \"Postcode\": 2607, \"Price\": 525000,\n22\"PropertyType\": \"house\", \"Bedrooms\": 4}\n23Consider the typical users who access this database. What kind of SQLite\n24SELECT queries would they write? List 15 examples SQL code that are\n25directly executable with relatively high complexity, each following a\n26description. If possible, use joins. Reply in the format of a description\n27and the SQL code for each example.\nThe following is the annotation generated by the LLM when the steering phrase is used. Clearly, the\nSQL programs generated have a higher complexity, and they are more valuable as SQL annotations\nfor our data tables.\n11. Select all sales records from both tables where the property type is a \"house\":\n2‘‘‘sql\n3SELECT * FROM MaLga12345 WHERE Type = ’house’\n4UNION\n5SELECT * FROM RawSales WHERE PropertyType = ’house’;\n6‘‘‘\n7\n82. Retrieve the total price for houses sold in table 1:\n9‘‘‘sql\n10SELECT SUM(Price) as total_price FROM RawSales WHERE PropertyType = ’house’;\n11‘‘‘\n12\n133. List the average number of bedrooms for properties sold in each table:\n14‘‘‘sql\n15SELECT AVG(Bedrooms) as avg_bedrooms_table0 FROM MaLga12345;\n16SELECT AVG(Bedrooms) as avg_bedrooms_table1 FROM RawSales;\n17‘‘‘\n18\n194. Find the number of sales made in each table in the year 2007:\n20‘‘‘sql\n21SELECT COUNT(*) as sales_count_table0 FROM MaLga12345 WHERE Saledate LIKE ’%\n22SELECT COUNT(*) as sales_count_table1 FROM RawSales WHERE Datesold LIKE ’%\n23‘‘‘\n24\n255. Retrieve the sales records where the postcode is 2607 from table 1:\n26‘‘‘sql\n27SELECT * FROM RawSales WHERE Postcode = 2607;\n28‘‘‘\n29\n306. Identify the latest sale date recorded from both tables:\n31‘‘‘sql\n32SELECT MAX(Saledate) as latest_sale_date_table0 FROM MaLga12345;\n33SELECT MAX(Datesold) as latest_sale_date_table1 FROM RawSales;\n34‘‘‘\n35\n367. Display the details for sales records with a price greater than $500,000 in table\n1:\n37‘‘‘sql\n38SELECT * FROM RawSales WHERE Price > 500000;\n39‘‘‘\n40\n418. Retrieve the number of bedrooms for properties sold in table 0, grouped by\nproperty type:\n42‘‘‘sql\n43SELECT Type, AVG(Bedrooms) as avg_bedrooms FROM MaLga12345 GROUP BY Type;\n44‘‘‘\n45\n469. List the sales records from both tables where the number of bedrooms is greater\nthan 3:\n47‘‘‘sql\n48SELECT * FROM MaLga12345 WHERE Bedrooms > 3\n27\n\n49UNION\n50SELECT * FROM RawSales WHERE Bedrooms > 3;\n51‘‘‘\n52\n5310. Calculate the average price of properties sold in table 1 with 4 bedrooms:\n54‘‘‘sql\n55SELECT AVG(Price) as avg_price FROM RawSales WHERE Bedrooms = 4;\n56‘‘‘\n57\n5811. Count the number of distinct postcodes present in table 1:\n59‘‘‘sql\n60SELECT COUNT(DISTINCT Postcode) as unique_postcodes FROM RawSales;\n61‘‘‘\n62\n6312. Retrieve the sales records from table 1 where the price is greater than the\naverage price:\n64‘‘‘sql\n65SELECT * FROM RawSales\n66WHERE Price > (SELECT AVG(Price) FROM RawSales);\n67‘‘‘\n68\n6913. Join tables based on matching postal codes and display the combined dataset:\n70‘‘‘sql\n71SELECT * FROM MaLga12345\n72JOIN RawSales ON MaLga12345.MA = RawSales.Postcode;\n73‘‘‘\n74\n7514. Identify the property types present in the two tables without duplication:\n76‘‘‘sql\n77SELECT DISTINCT Type FROM MaLga12345\n78UNION\n79SELECT DISTINCT PropertyType FROM RawSales;\n80‘‘‘\n81\n8215. Show the top 5 highest-priced sales records from both tables:\n83‘‘‘sql\n84SELECT * FROM (SELECT * FROM MaLga12345\n85UNION\n86SELECT * FROM RawSales)\n87ORDER BY Price DESC\n88LIMIT 5;\n89‘‘‘\nSQL queries with higher complexity reflect better annotation quality. Thus we include the steering\nphrase “with relatively high complexity” in our prompt.\nC Costs of LLM Annotation\nWe use the public service from OpenAI to access ChatGPT with the GPT-3.5 large language model.\nTo support our claim that LLM annotations are cheap and accessible, we provide an estimate of our\nannotation cost.\n1. Generating SQL programs costs less than 400 U.S. dollars.\n2. Generating Rel translations for SQL programs costs less than 2000 U.S. dollars. The prompt for\nSQL-to-Rel translation is very long, as seen in Appendix A.\n3. Generating input-output column annotations for the data tables costs less than 100 U.S. dollars.\nThe time spent on annotation depends on the account limitations. OpenAI API calls can be made\nasynchronously to parallize the annotation process. All annotation processes combined can be\ncompleted within 2 days.\n28\n\nTable 7: The alternative hypotheses and p-values of paired Wilcoxon signed-rank test for TabPFN and\nAutoGluon performance on the tabular classification problems in AnnotatedTables. All alternative\nhypotheses are accepted with high statistical significance.\n1 Min (2725 problems) 5 Mins (2720 problems)\nAUROCAlternative hypothesis TabPFN > Autogluon TabPFN < Autogluon\np-value 3.798882e-20 6.643113e-11\nCross entropyAlternative hypothesis TabPFN < Autogluon TabPFN > Autogluon\np-value 4.25193e-63 2.44812e-06\n(a) AUROC with 1 minute\ntime budget.\n(b) Cross entropy with 1\nminute time budget.\n(c) AUROC with 5 min-\nutes time budget.\n(d) Cross entropy with 5\nminutes time budget.\nFigure 4: Bar plots for the performance metrics of TabPFN and AutoGluon. The outliers are not\nplotted as they are far from the quantile bars.\nD Wilcoxon Signed-Rank Test for TabPFN and AutoGluon Tabular\nClassification Performance Difference\nWilcoxon signed-rank test (Wilcoxon, 1945) is a non-parametric statistical test for paired data. We\nuse the test to determine if there is a statistically significant performance difference between TabPFN\nand AutoGluon on the tabular classification problems from AnnotatedTables. For every tabular\nclassification problem among the 2,720 problems, for every specific time budget (either 1 min or 5\nmins), the performance metrics (either AUROC or cross entropy) of TabPFN and AutoGluon form a\npaired data. The tests show that the median difference between the pairs is nonzero with statistical\nsignificance for all settings. The p-value of Wilcoxon signed-rank test is shown in Table 7. The paired\nbar plots are in Figure 4.\nJudging by the results of the Wilcoxon signed-rank test, TabPFN performs better than AutoGluon\nwith 1 minute time budget, and AutoGluon with 5 minutes time budget performs better than TabPFN.\nHowever, the difference of medians is small, and the standard deviation of performances are high.\nBased solely on the general results of Wilcoxon signed-rank test, we cannot predict if one model\nwill outperform the other for a particular tabular classification problem. The issue of “statistical\nsignificance verus practical significance” (Kirk, 1996) is reflected here: even if a result is statistically\nsignificant, it may not be practically significant if the effect size is very small.\n29\n\nE Data Retention Rate During Dataset Construction\nDuring the dataset construction, we discard the data point if data processing fails at any step in order\nto maintain high quality results. In this section, we list the main data processing steps and the number\nof data points remaining at every step.\nAll intermediate data artifacts will be released as a part of the AnnotatedTables dataset. We hope\nfuture researchers can study the quality of LLM annotations and introduce methods for improvements.\nSQL Annotation SQL annotation starts with 70,000 Kaggle datasets and create 405,616 SQL\nprograms with 32,119 databases, in the following data processing steps.\n1. 70,000 Kaggle datasets are collected.\n2. 42,776 Kaggle datasets have CSV files to load as databases, representing 61.11% of the\ndatasets collected from the last step.\n3. 34,460 SQL databases can be loaded, for which the schema and example row descriptions can\nbe extracted, representing 80.56% of the datasets with CSV files from the last step.\n4. 32,926 SQL databases have SQL annotations generated by the Large Language Model, rep-\nresenting 95.54% of the datasets from the last step. The total number of SQL programs\nsynthesized is 493,495.\n5. We execute 493,134 SQL programs from 32,902 SQL databases within a time out of 120\nseconds per query to collect execution results, representing 99.93% of the SQL programs\nand 99.93% of the databases from the last step. Every table in the database is truncated to a\nmaximum of 1,000 rows before execution of SQL programs.\n6. 405,616 SQL programs from 32,119 databases have execution results that are not empty,\nlabeled as “Valid” in Table 3, representing 82.25% of the SQL programs executed.\nSQL-to-Rel translation SQL-to-Rel translation uses the SQL annotations in AnnotatedTables to\ntranslate to Rel programs.\n1. 405,616 SQL programs from 32,119 databases are valid.\n2. 28,514 databases have at least 10 SQL programs each, forming 285,140 SQL programs to be\ntranslated, reported in Table 5, representing 70.30% of the valid SQL programs from the last\nstep.\n3. 116,327 Rel programs are translated correctly, with the same execution results as the SQL\nqueries, representing 40.80% of the source SQL programs from the last step.\nInput-output column annotation The input-output columns are annotated for the evaluation of\nTabPFN. For a given table, numeric input columns and a categorical output column need to be\nidentified for the tabular classification task. For this annotation, we start with the 70,000 Kaggle\ndatasets and evaluate TabPFN and AutoGluon on 2,720 tabular classification problems.\n1. 70,000 Kaggle datasets are collected.\n2. 42,776 Kaggle datasets have CSV files to load as databases, representing 61.11% of the\ndatasets collected from the last step.\n3. 34,009 tables are annotated with input-output columns by the LLM. One table is taken from\neach database to avoid re-using similar or related tables from the same database. 79.50% of\nthe tables from the previous step have input-output columns annotated from the LLM.\n4. 10,529 tables have numeric input columns and a categorical output column, to satisfy TabPFN’s\nrequirements. The LLM may still generate an annotation when the table does not have columns\nthat satisfy the requirement. We do not remove missing values from the tables or tables with\nmissing values.\n5. We run both TabPFN and AutoGluon on every table from the last step. Data is processed using\nTabPFN authors’ code, with data truncated to at most 1,000 training samples, 100 numerical\nfeatures, and 10 classes. 2,725 problems can be run and finish running within 1 minute time\nbudget for both models, and 2,720 problems within 5 minute time budget.\n30\n\nF Tabular Dataset Annotation and Semantic Annotation of Tabular Data\nOur paper studies the “tabular dataset annotation problem”, that is the problem of dataset annotation\nlimited to the tabular data domain. Based on the common concept of dataset annotation, tabular\ndataset annotation is the process of labeling tabular data for machine learning. Currently, tabular\ndataset annotation is done through expert annotators or crowd-sourced human workers.\nThe “tabular dataset annotation problem” should not be confused with “semantic annotation of\ntabular data”, which is also called “tabular data annotation” in some context (PapersWithCode, 2024).\n“Semantic annotation of tabular data” aims to match tabular data with knowledge graph entities.\n31\n\nNeurIPS Paper Checklist\n1.Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper’s contributions and scope?\nAnswer: [Yes]\nJustification: The abstract and introductions made claims that are accurate and supported\nby the results in the paper. Our dataset is the largest SQL dataset with tabular data that\nsupport execution in the literature. Our paper is also the first to use large language models\nto annotate tabular datasets, to the best of our knowledge.\nGuidelines:\n•The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n•The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n•The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n•It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2.Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer:[Yes]\nJustification: We measure and report the performance of LLM annotations, including statis-\ntics and analysis on the erroneous annotations throughout this paper. We have paragraphs in\nthe paper devoted to discussing the limitations of our results.\nGuidelines:\n•The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n• The authors are encouraged to create a separate \"Limitations\" section in their paper.\n•The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n•The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n•The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n•The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n•If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n•While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren’t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3.Theory Assumptions and Proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\nAnswer: [NA]\nJustification: We have no theoretical results.\nGuidelines:\n32\n\n• The answer NA means that the paper does not include theoretical results.\n•All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n•All assumptions should be clearly stated or referenced in the statement of any theorems.\n•The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n•Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n• Theorems and Lemmas that the proof relies upon should be properly referenced.\n4.Experimental Result Reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: We publicly release our code and data.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n•If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n•If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n•Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n•While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a)If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b)If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c)If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d)We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5.Open access to data and code\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\nAnswer: [Yes]\nJustification: We publicly release our code and data. The access instructions are in the\nsupplementary material. The source Kaggle datasets can be accessed publicly.\nGuidelines:\n• The answer NA means that paper does not include experiments requiring code.\n•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/\npublic/guides/CodeSubmissionPolicy ) for more details.\n33\n\n•While we encourage the release of code and data, we understand that this might not be\npossible, so “No” is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n•The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines ( https:\n//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.\n•The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n•The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n•At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n•Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6.Experimental Setting/Details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: Our experimental settings are discussed in the paper with great details. We\nalso release our code as the supplemental material.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n•The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n•The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7.Experiment Statistical Significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [Yes]\nJustification: For experiments, p-values and bar plots are included in the paper and the\nappendix.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n•The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n•The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n•The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n• The assumptions made should be given (e.g., Normally distributed errors).\n•It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n•It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n•For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n•If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8.Experiments Compute Resources\n34\n\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes]\nJustification: Our LLM annotation methods aim to reduce the costs of dataset annotation,\nand the estimated costs are in the Appendix.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n•The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n•The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n•The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn’t make it into the paper).\n9.Code Of Ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?\nAnswer: [Yes]\nJustification: We have reviewed the NeurIPS Code of Ethics and confirm that our research\nconforms to it.\nGuidelines:\n•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n•If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n•The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10.Broader Impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [NA]\nJustification: Dataset annotation is a standard component in all machine learning projects,\nand improvements on the dataset annotation process should bring no risks of negative\nsocietal impacts.\nGuidelines:\n• The answer NA means that there is no societal impact of the work performed.\n•If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n•Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n•The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n•The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n•If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11.Safeguards\n35\n\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: Dataset annotation is a standard component in all machine learning projects\nand our work poses no risks of misuse.\nGuidelines:\n• The answer NA means that the paper poses no such risks.\n•Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n•Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n•We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12.Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [Yes]\nJustification: The assets we created are released with public licenses. The original licenses\nof the Kaggle datasets can be found on the dataset webpages, and we do not re-release their\ndata or modify their licenses to conform to Kaggle’s terms of use. Users may download the\nKaggle datasets on their own through public Kaggle API access, following our process or\nusing our tools. Kaggle datasets have licenses that permit data analysis usages.\nGuidelines:\n• The answer NA means that the paper does not use existing assets.\n• The authors should cite the original paper that produced the code package or dataset.\n•The authors should state which version of the asset is used and, if possible, include a\nURL.\n• The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n•For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n•If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n•For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n•If this information is not available online, the authors are encouraged to reach out to\nthe asset’s creators.\n13.New Assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [Yes]\nJustification: Our model and dataset are well documented and released publicly.\nGuidelines:\n• The answer NA means that the paper does not release new assets.\n•Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n•The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n•At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14.Crowdsourcing and Research with Human Subjects\n36\n\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: The paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n•The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n•Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human\nSubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: The paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n•The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n•Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n•We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n•For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n37",
  "textLength": 114999
}