{
  "paperId": "21c04d5cb3e486a19c95ba68f9e8787aa06ce695",
  "title": "A survey of sketches in traffic measurement: Design, Optimization, Application and Implementation",
  "pdfPath": "21c04d5cb3e486a19c95ba68f9e8787aa06ce695.pdf",
  "text": "A survey of sketches in trafﬁc measurement:\nDesign, Optimization, Application and\nImplementation\nShangsen Li, Lailong Luo, Deke Guo, Qianzhen Zhang, Pengtao Fu\nScience and Technology on Information System Engineering Laboratory,\nNational University of Defense Technology, Changsha Hunan 410073, P.R. China.\nAbstract —Network measurement probes the underlying net-\nwork to support upper-level decisions such as network manage-\nment, network update, network maintenance, network defense\nand beyond. Due to the massive, speedy, unpredictable features of\nnetwork ﬂows, sketches are widely implemented in measurement\nnodes to approximately record the frequency or estimate the\ncardinality of ﬂows. At their cores, sketches usually maintain one\nor multiple counter array(s), and rely on hash functions to select\nthe counter(s) for each ﬂow. Then the space-efﬁcient sketches\nfrom the distributed measurement nodes are aggregated to\nprovide statistics of the undergoing ﬂows. Currently, tremendous\nredesigns and optimizations have been proposed to improve the\nsketches for better network measurement performance. However,\nexisting reviews or surveys mainly focus on one particular\naspect of measurement tasks. Researchers and engineers in the\nnetwork measurement community desire an all-in-one survey that\ncovers the entire processing pipeline of sketch-based network\nmeasurement. To this end, we present the ﬁrst comprehensive\nsurvey of this area. We ﬁrst introduce the preparation of ﬂows\nfor measurement, then detail the most recent investigations of\ndesign, aggregation, decoding, application and implementation\nof sketches for network measurement. To summarize the existing\nefforts, we carry out an in-depth study of the existing literature,\ncovering more than 90 sketch designs and optimization strategies.\nFurthermore, we conduct a comprehensive analysis and qual-\nitative/quantitative comparison of the sketch designs. Finally,\nwe highlight the open issues for future sketch-based network\nmeasurement research.\nIndex Terms —Sketch; Counter; Network Measurement; Traf-\nﬁc Measurement\nI. I NTRODUCTION\nMeasurement-based performance evaluation of network traf-\nﬁc is a fundamental prerequisite for network management,\nsuch as load balance [1], routing, fairness [2], instruction\ndetection [3], caching [4], trafﬁc engineering [5], performance\ndiagnostics [6] and policy enforcement [7]. With the explo-\nration of network users and network scale, the underlying\nnetwork trafﬁc becomes massive and unpredictable, which\nchallenges and complicates all the network functions. Network\nmeasurement probes the underlying ﬂows and provides the\nmetadata to support network-related decisions and threat di-\nagnosis. Efﬁcient network measurement requires various levels\nof trafﬁc statistics, both as an aggregate and on a per-ﬂow basis\n[8]. These statistics include the number of distinct ﬂows (ﬂowcardinality), the number of packets for a ﬂow (ﬂow size), the\ntrafﬁc volume contributed by a speciﬁc ﬂow (ﬂow volume)\nand the time information. These statistics together proﬁle\nthe network and provide essential information for network\nmanagement and protection/defense.\nTABLE I: Comparison with existing surveys\nFirst author # Variants #Measurement tasks Year\nCormode [9]\u001415 3 2010\nCormode [10]\u001420 3 2011\nZhou [11]\u001410 4 2014\nYassine [12] — — 2015\nGibbons [13]\u001410 1 2016\nYan [14] — 1 2016\nThis survey\u001590 13 2021\nThe term sketch refers to a kind of synopsis data structure\nto record the frequency or estimate the cardinality of items\nin a multiset or stream approximately. Basically, sketches\nhash each packet according to the interested ﬂow key into a\nunion of cells or counters. Sketches offer an efﬁcient method\nto record the existence of the active ﬂow and its statistical\ninformation for later passive trafﬁc measurement tasks. As a\ncompact summary, sketches are typically much smaller than\nthe size of input and adaptive to solve network measurement\ntasks. Moreover, sketches guarantee constant time overhead\nof element insertion and information query operations with\nguaranteed accuracy.\nThe primary motivation of conducting this survey is two-\nfold. Firstly, a lot of sketch designs and optimizations have\nbeen proposed to perform measurement tasks in recent years.\nExisting surveys, however, are somehow out-of-date and do\nnot cover these new proposals. The latest survey [13] was\npublished four years ago and only covers a small partition of\nwork. Secondly, the existing surveys [12] [14] solely focus on\na particular or several aspects of measurement tasks and lack of\na comprehensive view on sketch design and optimization. The\nearlier work [9] [10] only covered less than 20 sketch variants\nand were published nearly ten years ago. These existing\nsurveys did not provide comprehensive reviews on sketch-\nbased network measurement. As shown in Table. I, compared\nwith the existing surveys, our survey covers more sketch\nvariants and considers more measurement tasks. In particular,arXiv:2012.07214v2  [cs.DS]  20 Jul 2021\n\nPreparation for traffic measurementPreparation for traffic measurement\nPreparation for traffic measurement\nCollaborative traffic measurementCollaborative traffic measurement\nOptimization for speed and spaceOptimization for speed and spaceOptimization in sketch structure Optimization in sketch structure\nHashing strategyHashing strategy\nCounter level optimizationCounter level optimization\nSketch level optimizationSketch level optimizationOptimization in  post-processing stage Optimization in  post-processing stage\nOptimization for Optimization for Measurement tasks (application)Measurement tasks (application)\nTime dimensionTime dimension\nVolume dimensionVolume dimension\nTime -Volume  dimension Time -Volume  dimensionPreparation for traffic measurementPreparation for traffic measurement\nPreparation for traffic measurement\nCollaborative traffic measurementCollaborative traffic measurement\nOptimization for speed and spaceOptimization for speed and spaceOptimization in sketch structureOptimization in sketch structure\nOptimization in sketch structure\nHashing strategyHashing strategy\nCounter level optimizationCounter level optimization\nSketch level optimizationSketch level optimizationOptimization in  post-processing stage Optimization in  post-processing stage\n Optimization in  post-processing stage\nOptimization for sketch collectionOptimization for sketch collection\nInformation extractionInformation extractionMeasurement tasks (application)Measurement tasks (application)\nMeasurement tasks (application)\nTime dimensionTime dimension\nVolume dimensionVolume dimension\nTime -Volume dimension Time -Volume dimension\nMonitor  placement\nTraffic rerouting\nFlow distribution\nSampling\nRandomization \nBatchingSmall counter for larger range\nVirtual register design\nHierarchical counter -sharing\nVariable -width counters\nNovel counter update strategy\nLess memory access\nSlot enforcement\nMulti -layer\nMulti -sketch\nSliding windowSketch c ompression\nSketch m erging\nFlow key reversible\nProbability theory based \nMachine learning Sketch\nPersisten ce detection\nFlow latency\nPer-flow measurement\nCardinality estimation \nSuper spreader/D Dos\nHeavy Hitters\nHierarchical Heavy Hitters (HHH) \nTop-k detection\nEntropy estimationQuery...Packets\nUpdate\nPreparation\nConventional hash mapping\nLearning -based mapping\nPersistent Sketch\nChange detection\nBurst Detection\nItem batch detectionMonitor  placement\nTraffic rerouting\nFlow distribution\nSampling\nRandomization \nBatchingSmall counter for larger range\nVirtual register design\nHierarchical counter -sharing\nVariable -width counters\nNovel counter update strategy\nLess memory access\nSlot enforcement\nMulti -layer\nMulti -sketch\nSliding windowSketch c ompression\nSketch m erging\nFlow key reversible\nProbability theory based \nMachine learning Sketch\nPersisten ce detection\nFlow latency\nPer-flow measurement\nCardinality estimation \nSuper spreader/D Dos\nHeavy Hitters\nHierarchical Heavy Hitters (HHH) \nTop-k detection\nEntropy estimationQuery...Packets\nUpdate\nPreparation\nConventional hash mapping\nLearning -based mapping\nPersistent Sketch\nChange detection\nBurst Detection\nItem batch detectionFig. 1: Framework of this survey\nour survey concentrates on the design and optimization for the\nsketch from a perspective of dataﬂow in sketch algorithm.\nIn this survey, we ﬁrst classify the sketch design and\noptimization from the perspective of data process pipeline\nof the sketch algorithm. Speciﬁcally, from the data recording\nstage to the information extraction stage, the core components\nfor a sketch data structure are update strategy, design of data\nstructure, query strategy, and the enabled functionality. Thus,\nwe survey the existing works from three-dimension, i.e., prepa-\nration for sketch-based network measurement, optimization in\nsketch data structure, and optimization in the post-processing\nstage. Besides, we further provide a comprehensive survey\nof sketch-based measurement tasks and implementation. The\ncomprehensive framework is illustrated in Fig. 1. For each\nmeasurement task, we compare the corresponding sketch de-\nsign quantitatively and qualitatively.\nFollowing the above directions, the remainder of this paper\nis organized as follows. Section II gives a basic description\nof sketch-based network measurement. Section III details the\nrelated work about preparation for trafﬁc measurement, includ-\ning collaborative trafﬁc measurement and packet preprocess-\ning. Section IV focuses on the design and optimization in the\nsketch data structure, including hashing strategy, counter level\noptimization, and sketch level optimization. Section V focuses\non the process when extracting information from the sketch\nand gives a detailed summary about technology for sketch\ncompression, sketch merging, data supplement, and informa-\ntion extraction. In Section VI, we compare the sketch-based\nmethods from the perspective of measurement tasks in terms\nof both the time dimension and volume dimension. Further, we\nconclude the related hardware and software implementation of\nsketch-based measurement in that section. Finally, we give the\nrelated open issues in Section VII.II. S KETCH BASED NETWORK MEASUREMENT\nNetwork measurement is indispensable for understanding\nthe performance of a network infrastructure and managing\nnetworks efﬁciently. There are two ways to measure network\nperformance: active or passive techniques.\nIn active measurement, network ﬂows are continuously mea-\nsured by sending a series of probe packets over the network\npaths. With the help of such probes, network operators are able\nto measure one-way delay, round-trip-time (RTT), the average\npacket loss, connection bandwidth and adjust the forwarding\npolicies in case of load changes. It estimates the network\nperformance by tracking how the probe packets are treated\nin the network [15]. The accuracy is closely associated with\nthe probe frequency in general. Active measurement offers\ndifferent granularity of end-to-end QoS measurement and is\nﬂexible since you can measure what you want. However, send-\ning probes frequently may impose signiﬁcant measurement\noverhead that may disturb critical trafﬁc.\nBy contrast, passive measurement provides an efﬁcient tool\nfor charging, engineering, managing and securing the commu-\nnication tools [16]. Without probing packets, passive measure-\nment monitors the trafﬁc traversing the measurement points\nto collect statistical information. In particular, a passive mea-\nsurement system is able to summarize ﬂow-level or packet-\nlevel trafﬁc, estimate ﬂow/packet-size distribution, evaluate the\nﬁne-grained volume of network trafﬁc with different attributes\nfor usage-based charging, and more. Besides, passing mea-\nsurement can help identify abnormal trafﬁc patterns to ease\nnetwork management tasks, such as trafﬁc engineering, load\nbalancing, and intrusion detection. Especially, a ﬂowkey is\noften deﬁned as the ﬁve ﬁelds in the packet headers, i.e., for\nIPv4 protocol, a ﬂow identiﬁer includes Source IP address\n(32 bits), Destination IP address (32 bits), Source Port (16\nbits), Destination Port (16 bits), and Protocol (8 bits), i.e.,\n\n100101102103104105106107\nFlow size00.20.40.60.81CDFFig. 2: The CDF of ﬂow size distribution. There are 424.6\nmillion IPv4 packets in total from WIDE trafﬁc trace [18].\nf=hSrcIP;SrcPort;DstIP;DstPort;Protocol i. A ﬂow\nfis deﬁned as a set of packets that share the same ﬂow key,\nwhich can be ﬂexibly assigned according to high-level need.\nTherefore, the number of ﬂow identiﬁers can be as large as\n2104[17].\nActive measurement obtains the network state, such as\nbandwidth, by injecting probe packets into the network. How-\never, the measurement packets will disrupt the network, espe-\ncially when sending measurement trafﬁc with high frequency.\nIn contrast, passive measurement measures network without\ncreating or modifying any trafﬁc. And we concentrate on\nsketch-based passive network measurement in this survey.\nA. Challenges of passive measurement\nThe challenge of passive measurement mainly lies in two\naspects, i.e., resources limitations and unpredictable trafﬁc\ncharacteristics. Therefore, most researchers envision an al-\ngorithm that can obtain interesting statistical information of\ntrafﬁc with limited resources.\nResource Limitation. When implementing a measurement\ntask, a key challenge is how to perform it on a small high-\nspeed memory efﬁciently [19]. Nowadays, the core routers\nforward most packets with the fast-forwarding path between\nnetwork interface cards directly, without going through the\nCPU nor main memory. To keep up with such high speed,\nmeasurement tasks are expected to perform in SRAM. How-\never, the SRAM capacity is limited and is shared by all online\nnetwork functions of routing, management, performance and\nsecurity. Hence, the available space for measurement tasks\nis limited. To address this challenge, as stated in Section\nVI-B1,VI-B2,VI-B3, lots of proposals have been investigated\nto ﬁt the measurement algorithms into SRAM, hybrid SRAM-\nDRAM, and DRAM. Moreover, researchers have tried to\nperform measurement tasks with FPGA and TCAM as summa-\nrized in Section VI-B4VI-B5 as the development of hardware.\nFurthermore, with the emergence of software-deﬁned network-\ning (SDN), efforts have been made to perform measurement\ntasks in P4Switch and Open vSwitch, as shown in Section\nVI-B6VI-B7VI-B8.\nFlow Unpredication. Real network trafﬁc is always high-\nspeed and non-uniform distributed. Such characteristics furtherbring challenges to measurement algorithm design. On the one\nhand, the high-speed feature make it difﬁcult to record the size\nof trafﬁc ﬂows accurately. On the other hand, the skewness of\nﬂow in the network further aggregates the measurement task.\nUsually, the ﬂow size/volume follows Zipf [20] or Power-\nlaw [21] distribution, as illustrated in Fig. 2 wherein trafﬁc\ntrace was collected by MAWI Working Group [18] from\nits samplepoint-G at 2:00 pm to 2:15 pm on 29/02/2020.\nSpeciﬁcally, most ﬂows are small in size, often known as\nmouse ﬂows; while a few of them are extremely large,\ncommonly known as elephant ﬂows. For most measurement\ntasks, elephant ﬂows are more important than the small ones.\nTherefore, the size of each counter should be capable of\ncounting the largest ﬂows accurately. In effect, the network\noperators may not know the size of elephant ﬂows in advance,\nwhich makes it tricky to set the length of each counter.\nThe optimal measurement strategy for a speciﬁc measure-\nment objective typically assumes a priori knowledge about\nthe trafﬁc characteristics. However, both trafﬁc characteristics\nand measurement objectives can dynamically change over\ntime, potentially rendering a previously optimal placement of\nmonitors suboptimal. For instance, a ﬂow of interest can avoid\ndetection by not traversing the deployed monitors. The optimal\nmonitor deployment for one measurement task might become\nsuboptimal once the objective changes. We survey the work\nwhich targeted at handling the skewness of ﬂows in Section.\nIV. Further, packet batching and sampling strategies, which\nare proposed to match up with the high line rate, are detailed\nin Subsection. III-B. Note that, as illustrated in Fig. 5, the\npackets of a ﬂow may go through multiple paths from source\nto destination. To maximize the ﬂow coverage and minimize\nthe redundancy, the switch in the middle should take part\nin measurement tasks. The related work are summarized in\nSubsection. III-A.\nB. Performance requirements for network measurement\nDue to the limitations and challenges of passive network\nmeasurement, we further elaborate the performance require-\nment of measurement tasks.\nHigh Accuracy. Most of the existing measurement methods\nprovide approximate results. They provide theoretical guar-\nantees that the estimated result has a relative error \u000fwith a\nconﬁdence probability 1\u0000\u000e, where\u000fand\u000eare conﬁgurable\nparameters between 0 and 1 [22]. Novel updating strategies,\nwhich are designed to improve measurement accuracy, can be\nfound in Subsection. IV-B6. And several multi-layer sketch\ndesigns to improve the measurement accuracy are detailed in\nSubsection. IV-C1.\nMemory Efﬁciency. Take a compact data structure to\nperform trafﬁc measurement is a core fundamental requirement\nfor both hardware and software implementations. Because\nof the vast number of ﬂows passing through a network\ndevice, memory space becomes a signiﬁcant concern. For\na hardware implementation, high memory usage aggravates\nchip footprints and heat consumptions, thereby increasing\nmanufacturing costs [22]. For software switches, although\n\nsevers have plenty of memory [23], high memory usage will\ndeplete the memory of co-located applications (e.g., VMs or\ncontainers). The related work for improving space efﬁciency\nare detailed in Section IV-B. Furthermore, sampling strategies\nare also deployed to reduce the space overhead, as speciﬁed\nin Section III-B1.\nReal Time Response. Most measurement tasks should\nrespond to trafﬁc anomalies in time to avoid potentially\ncatastrophic events. However, some sketch designs can support\nqueries only after the entire measurement epoch. For example,\nbefore querying a certain ﬂow, Counter Braids [24] must\nobtain the information of all ﬂows in advance, and decoding\nshould be executed ofﬂine in a batched way. As a result, the\nquery speed is signiﬁcantly slowed down. Additionally, several\nwork that focus on the most recent information are detailed in\nSection IV-C3.\nFast Per-packet Processing. Processing packets at high\nspeed by limiting the memory access overhead of per-packet\nupdates is essential for network measurement. For example,\na reduction in latency for partition/aggregate workloads can\nbe achieved if we can identify trafﬁc bursts in near-real\ntime [6]. The main challenges of high processing speed stem\nfrom high line rates, the large scale of modern networks,\nand frequent memory accesses. A hardware implementation\nusually prefers running algorithms in TCAM or SRAM, rather\nthan DRAM, for better high-speed processing. For a software\nimplementation, as can be envisioned in upcoming SDN and\nNFV realizations, the ability to perform computations on time\ngreatly depends on whether the data structures ﬁt in the\nhardware cache and whether the relevant memory pages can be\npinned to avoid swapping [8]. The related works can be found\nin Section III-BVI-B3VI-B1. The per-ﬂow measurement meth-\nods are discussed in Section IV. The optimization strategies\nfor less memory access are detailed in Section IV-B5.\nDistributed Scalability. Due to the space limitation of a\nsingle switch, it is advisable to deploy a collaborative trafﬁc\nmeasurement system wherein each switch is only responsible\nfor a part of the underlying ﬂows in the network. For more\ndetails of collaborative trafﬁc measurement, the related works\nare introduced in Section III-A.\nGenerality. There are various network measurement tasks.\nDesigning a speciﬁc algorithm for each different type of task\nis costly and inefﬁcient. Moreover, it requires sophisticated\nresource allocation across different measurement tasks to\nprovide accuracy guarantees [25] [26]. A general sketch-based\nmeasurement method means to support a variety of network-\nmeasurement tasks simultaneously. Typical methods, which\nare general or partially general to trafﬁc measurement tasks,\nare listed in Table. V.\nReversibility. Although traditional sketch designs can an-\nswer the point query, they fail to return the corresponding\nﬂow identiﬁer from the counters. Thus, many efforts have been\nmade to make sketches invertible and capable of identifying\nthe interested ﬂows from the sketches directly. The details are\ndescribed in Section V-B1.\nMeasurement \ntasksCollectorQuery\nRespond\nSketchFig. 3: Sketch-based network trafﬁc measurement\nC. Sketch as a perfect choice\nSketch-based network measurement belongs to the passive\nnetwork measurement, which does not cause any overhead in\nthe network since it does not send any probe packets. It allows\nthe processing of local trafﬁc states and the global behavior of\ntrafﬁc ﬂows passing through speciﬁc network partitions. For\npassive network measurement, monitors require full access to\nthe network devices such as routers and switches, as shown\nin Fig. 3. Generally, the sketch-based trafﬁc measurement\ncan be divided into the following steps: 1) the distributed\nmeasurement points acknowledge their tasks and collect the\ncorresponding trafﬁc statistical information passively, and 2)\nthe collectors merge the measurement results from the subor-\ndinate monitors and respond to the measurement query.\nAs illustrated in Fig. 1, sketch-based network measurement\nexperiences the same pipeline as stream processing. It is\nconcerned with processing a long stream of trafﬁc packets in\none pass using a small working memory to estimate speciﬁc\nstatistics of ﬂows/packets approximately. To enable numerous\nper-ﬂow counters in a scalable way, replacing exact numbers\nwith estimated counters is an intrinsic feature of sketch-based\nmeasurement. These estimates reach an acceptable trade-off\nmemory occupation and estimation accuracy. Sketch offers\nan efﬁcient method to record the existence of active ﬂows\nand their information in passive network measurement with\naccuracy guarantees. As a compact summary, sketches are\ntypically much smaller than the size of the input.\nA sketch organizes its working memory as a synopsis data\nstructure, specializing in capturing as much information about\nthe target statistics as possible. For the vast ﬂow key space,\nwe are interested in the identity of active ﬂows (during the\nmeasurement epoch). To record the massive active ﬂows,\na brute-force method is to count and record them directly.\nHowever, such a plain design will incur high memory access\nand computing overhead. By contrast, a sketch-based method\nusually maps the ﬂow keys into a table with multiple buck-\nets/cells within constant time. The corresponding buckets/cells\nwill store the interested information about the ﬂows for later\nqueries within constant time. Various sketches are proposed for\ndifferent statistics about the ﬂows. More generally, there are\ntwo primary operations in a sketch-based algorithm: update\nandquery . The ﬂows in a measurement epoch are represented\n\n+1\n+1\n+1\n+1\n+1\n+1 w buckets \n d rows Flow \nkeyFig. 4: The CM sketch structure\nby the sketch with the update operations and thereafter queried\nfor upper-level network analysis. Note that, different sketches\nmay provide distinct update and query strategies. Moreover,\nmost kinds of sketches enable function distributed measure-\nment scalability by merging operations. Some sketches embed\nadditional information into cells to enable reversibility.\nD. Count-Min sketch as an example\nAs the most widely applied sketch, Count-Min (CM) sketch\n[27] has been used to perform per-ﬂow measurement, heavy\nhitter detection, entropy estimate [28], inner-product esti-\nmation [29], privacy preserve [30], clustering over a high-\ndimensional space [29] and personalized page rank [31].\nThe CM sketch can represent high dimensional data (like\nthe 5-tuple ﬂow key space) and answer point query with strong\naccuracy guarantees within constant time. With such design,\nCM sketch supports various queries, such as heavy hitters,\nﬂow size distribution, top- kdetection, and beyond. Since the\ndata structure can efﬁciently process updates in the form of\ninsertion or deletion, it is capable of working under the trafﬁc\nstream at high rates. The data structure maintains the linear\nprojection of the data with several other random vectors, which\nare deﬁned implicitly by simple hash functions. Increasing\nthe range of hash functions or the number of hash functions\nimproves the accuracy of the summary [32]. Because of this\nlinearity, CM sketches can be scaled, added, and subtracted,\nto produce summaries of the corresponding scaled network\ntrafﬁc. However, due to the irreversible process of hash, CM\nsketch can not support ﬂow key reversibility.\nSpeciﬁcally, it hashes an arbitrary ﬂow into each row of\nthe hash table and leverages the corresponding counters to\nrecord the multiplicity or size information. It consists of\ndarrays,A1:::Ad, each of which contains wcounters, as\nshown in Fig. 4. When measuring the network trafﬁc, each\nﬂow key is extracted from the packet header and hashed by\nthedindependent hash functions, h1:::hd. Thereafter, the d\ncorresponding counters, i.e., 81\u0014k\u0014d;Ak[hk(f) %w]are\nadded up by the ﬂow sizes. Because computing each hash\nfunction takes O(1)(constant) time, the total time to perform\nan update is O(d), independent from w. To query the size of\na ﬂowf, CM returns the minimum among the dcounters as\nthe estimate. Moreover, CM only incurs over-estimated errors\nfollowing accuracy guarantee.\n132\n46\n785\ningressoutgressFig. 5: Example topology for multiple routing path\nTheorem 1 [27] : Ifw=de\n\"eandd=dln1\n\u000ee, the estimate ^sihas\nthe following guarantees: si<^si; and with probability at least\n1\u0000\u000e.\n^si\u0014si+\"kfk1 (1)\nwheresiis the real size of the ithﬂow in the ﬂow set f;^si\nrepresents the point query result of the ithﬂow from the CM\nsketch; and eis the base of the natural logarithm and it is a\nconstant chosen to optimize the space overhead for a given\naccuracy requirement.\nIII. P REPARATION FOR NETWORK MEASUREMENT\nIn practice, the measurement module is integrated as soft-\nware inside a switch or deployed as standalone measurement\nhardware that taps into a communication link. As shown in\nFig. 3, once a measurement module is placed or deployed on\na link, it can capture packets carried by the link according\nto its conﬁguration. As illustrated in Fig. 5, each ﬂow may\ntraverse multiple switches on its routing path, and any of\nthem can perform measurement tasks. Moreover, introducing\na measurement module on a link needs additional deployment\ncost, including hardware/software investment, space cost and\nmaintenance cost [33]. There is no need for one switch\nto measure all passing trafﬁc. Therefore, the network-wide\nmeasurement functions should be installed as efﬁciently as\npossible. Thus, performing measurement environment prepa-\nration is necessary before measuring the ﬂows. Besides, the\nactual measurement operation performed by monitors also\nfactors into its operating cost. Due to the per-packet operation\ncost of each monitor depends mainly on the line speed, it\nis nontrivial to perform network measurement for high-speed\ntrafﬁc within limited resources. It is necessary to pre-process\ntrafﬁc packets whenever possible to reduce the computation\nand storage overhead.\nA. Preparation for measurement environment\n1) Monitor Placement:\nIn collaborative trafﬁc measurement scenarios, it is urgent\nto minimize the overhead of hardware, software, and main-\ntenance, with the guarantee of measurement performance.\nTo ensure network-wide coverage, previous work focus on\nthe optimal deployment of monitors across the network with\nthe target to maximize the measurement utility, such as the\ncoverage of measured trafﬁc and the efﬁciency of measurement\n\nTABLE II: Collaborative trafﬁc measurement\nCollaborative measurementMethods Core idea\nMonitoring PlacementSuh et al. [33] Minimize cost and maximize coverage\nSharma and Byers [34] Minimize duplication of effort\nCantieni et al. [35] Decide which switch should be actived\nOpenTM [36] Choose which switches to measurement\nTrafﬁc ReroutingMMPR [37]\nMake trafﬁc adaptive to netowrk and requirementMeasuRouting [38]\nFlow DistributionOpenWatch [39] Compute the optimal balanced task assignment\nLEISURE [40] Consider load-balancing under different objectives\ncSamp [41] Specify hash ranges per OD-pair per router\ncSamp-T [42] Without relying on OD-identiﬁers\nDECOR [43] From local to global optimization\nDCM [44] Leverage the BF [45] to allocate responsibility\nXu et al. [46] Leverage hash range to allocate responsibility\nfunctions, and minimize the cost of the measurement. The\noptimal strategies are in the form of trafﬁc routing path\nor the optimal measurement point allocation in the whole\nmeasurement-enable network.\nUnder the measurement functionality un-powered situation\nin switch, Suh et al. [33] consider the minimum cost and\nmaximum trafﬁc coverage problems under various budget\nconstraints. They derive out a monitor placement strategy\nand specify the sampling rates of each monitor in order to\nmaximize the fraction of sampled IP ﬂows. They ﬁrst ﬁnd the\nlinks should be measured and then determines the sampling\nrates with an optimization algorithm. Besides, Minimum Edge\nCost Flow (MECF) [47] studies the problem of assigning tap\ndevices for passive measurement. They consider three main\nproblems, the ﬁrst one maximizes the volume of captured\ntrafﬁc under cost constraints. The second problem minimizes\nthe deployment cost to achieve a measurement objective and\nthe last one minimizes both installation and operational cost\nunder the same objective. The authors present a combinatorial\nview of the problem and derive complexity and approximately\nresults and efﬁcient and versatile Mixed Integer Programming\n(MIP) formulation. For passive measurement, they study the\nproblem of sampling packets and then present an efﬁcient way\nof placing monitor devices and how to control their sampling\nrates. They show that all these problems are NP-complete and\nthey present heuristics to approximate the optimal solution\nfor each one. They evaluate the performance of the proposed\nalgorithms with simulation on topologies discovered by the\nRocketfuel utility and with generated trafﬁc matrices. In addi-\ntion, this work only considers the static placement of monitors.\nIn practice, current routers deployed in operational networks\nare already equipped with measurement modules (e.g., Net-\nﬂow [48], Openﬂow [49]). There is no need to turn on all\nthese measurement functionalities because of their expensive\noperational cost and measurement redundancy. Because there\nare potentially hundreds of candidate measurement points to\nchoose for network-wide measurements. Cantieni et al. [35]\nfocus on which monitors should be activated and what sam-\npling rate should be set on these monitors in order to achieve a\ngiven measurement task with high accuracy and low resourceoccupation. Sharma and Byers [34] propose space-efﬁcient\ndata structures for gossip-based protocols to approximately\nsummarize sets of measured ﬂows. With some ﬁne-tuning of\nthe methods, they ensure that all ﬂows are observed by at least\none monitor, and only a tiny fraction of ﬂows are measured\nredundantly. OpenTM [36] explores several algorithms to\nchoosing switches for query. They further, reveal that there is a\ntrade-off between the accuracy of measurements and the worst-\ncase maximum load on individual switches. The work shows\na non-uniform distributed query strategy that tends to query\nswitches closer to the destination with a higher probability has\na better performance compared to the uniform schemes.\nMeasurement point placement problem focuses on the opti-\nmal placement strategies under the measurement functionality\nun-powered situation [33] [47] or the measurement activation\nproblem in measurement powered situation [35] [36]. As the\ncore of collaborative measurement, these works are essential\nfor setting up the environment for sketch-based measurement.\n2) Trafﬁc Rerouting:\nThe above works focus on the optimal measurement point\nplacement across the network. However, both trafﬁc charac-\nteristics and measurement tasks can dynamically change over\ntime so that a previous optimal placement of measurement\npoints may be suboptimal for the current network state. When\nit is not feasible to adaptively redeploy/reconﬁgure measure-\nment infrastructures to cater to such changing requirements\n[38], a possible strategy is to reroute the trafﬁc across these\nmonitoring points to get better measurement performance.\nMeasuRouting [38] addresses the problem where differ-\nent measurement points have different measurement abilities\nfor certain kinds of trafﬁc by rerouting interested trafﬁc\nacross measurement points. Building upon MeasuRouting,\nMeasurement-aware Monitor Placement and Routing (MMPR)\n[37] considers that not only the number of deployed monitors\nis limited, but the trafﬁc characteristics and measurement\nobjectives change both continually and simultaneously. The\nMMPR framework jointly optimizes monitors placement, dy-\nnamical turning on and off the monitors, and dynamic routing\nstrategy to achieve maximum measurement utility, quantifying\nhow well each ﬂow is monitored. MMPR formulates the\n\nproblem as a MILP (Mixed Integer Linear Programming) prob-\nlem and proposes several heuristic algorithms to approximate\nthe optimal solution and reduce the computation complexity.\nConsidering dynamics issues to estimate the ﬂow importance\nand conﬁgure the routing table entries of both MeasuRouting\nand MMPR, Dynamic Measurement aware Routing (DMR)\n[50] summarizes these challenges and proposes solutions.\n3) Flow Distribution/Allocation:\nA natural problem of collaborative measurement is how\nto assign the ﬂows to the measurement nodes/switches to\nensure both load balance and measurement completeness.\nSuch a problem is formulated as a ﬂow distribution/allocation\nproblem. By allocating each ﬂow to a sequence of switches,\nevery switch only measures a subset of ﬂows to save memory\nand computation resources. Hitherto, there are a dozen works\nthat try to tackle this question. DCM [44], OpenWatch [39]\nand LEISURE [40] focus on the measurement load balance\nstrategies making. While the hash-based works [41] [42] [46]\nmake efforts on lightweight implementation of strategies.\nDistributed and Collaborative Monitoring system (DCM)\n[44] leverages Bloom ﬁlters to record which ﬂows should be\nmeasured by the switch. It requires each switch to store two\nBloom ﬁlters, with the ﬁrst one encoding the set of ﬂows\nto be measured locally and the second one helping remove\nfalse positives from the ﬁrst ﬁlter. However, the memory\noverhead is signiﬁcant because it takes a Bloom ﬁlter 10\nbits per ﬂow on average to ensure a false-positive ratio of\nless than 1%. Moreover, the lookup of each Bloom ﬁlter\ntakeskhash operations and kmemory accesses, where k\nis 7 for an optimal Bloom ﬁlter with 1% false positive\nratio. OpenWatch [39] designs an efﬁcient method to ofﬂoad\nthe measurement loads from the ingress switches to other\nswitches along with the routing paths. It leverages the global\nview of network topology and routing path for each ﬂow\nto compute the optimal task assignment for all the switches\nin the network. By contrast, Load-EqualIzed meaSUREment\n(LEISURE) [40] balances the network measurement across\ndistributed monitors. Speciﬁcally, it considers various load-\nbalancing problems under different objectives and studies their\nextensions to support different deployment scenarios.\nSeveral works leverage hash functions to tradeoff com-\nputation and space overhead when allocating ﬂows across\nroute paths. cSamp [41] is a centralized hash-based packet\nselection system, which allows distributed monitors to mea-\nsure disjoint sets of trafﬁc without explicit communications.\ncSAMP speciﬁes the set of ﬂows identiﬁed by OD-pairs\n(origin-destination) that each measurement point is required\nto record by considering a hybrid measurement objective\nthat maximizes the total ﬂow-coverage subject. An improved\nmethod cSamp-T [42] provides comparable measurement ca-\npabilities to cSamp without relying on OD-pair identiﬁers.\nEach router only uses only local information, such as packet\nheaders and local routing tables, rather than the global OD-\npair identiﬁers. DECOR [43] provides a solution to coordinate\nnetwork resources and avoids controller bottleneck, message\ndelay, and a single point of failure. DECOR divides a networkinto smaller pieces called optimization units. Therefore, it tries\nto achieve local optimization in each optimization unit and\nthen extends to global optimization. The DECOR framework\ncan be applied to cSAMP, as cSAMP is also a resource\nassignment strategy. The comprehensive experiments conclude\nthat the DECOR-based cSAMP is superior to others. Xu et al.\n[46] propose a new lightweight solution to ﬂow distribution\nproblems for collaborative trafﬁc measurement in SDN. The\nproposed framework minimizes the memory/space overhead\non each switch with a single sampling probability value p. The\nprocessing overhead is at most one hash operation per packet\nto implement sampling if the packet is not recorded by one of\nthe other switches on the routing path; if the packet is recorded\nearlier, the hash operation will not be performed. Thus, the\ncontroller can decide the optimal Probability Assignment for\nIngress Switches (PAIS) and Network-wide Switch Probability\nAssignment (NSPA) collaborative measurement scenarios.\nCompared with [39]–[44], up to now, the method proposed\nin [46] is the most lightweight strategy to complete the ﬂow\ndistribution problem. By allocating different hash range to a\nseries of switch, NSPA elegantly allocates the responsibility\nto the switch across the link. In this way, the additional\nprocessing overhead is at most one hash operation per packet\nto implement sampling, and the additional space overhead on\neach switch is only a single sampling probability value p.\nB. Preparation for measurement data\nIntuitively, after deploying a measurement environment,\nmeasurement points can directly perform measurement tasks.\nHowever, in the high-speed network, performing measurement\non each packet is a heavy overhead task. Sampling, batch\nprocessing, and randomization are powerful tools to reduce\nthe processing overhead in various systems [51].\n1) Trafﬁc Sampling:\nProduction network measurement tools, such as NetFlow\n[48] and sFlow [52], always sample the network ﬂows for\nresource saving and fast processing. However, they incur low\nestimation accuracy since many ﬂows are missed. Given a\nsampling probability of p, ﬂow sampling means that every\nmonitor processes one packet per 1=ppackets in each ﬂow.\nwhile packet sampling indicates that every monitor processes\none packet per 1=ppackets of all trafﬁc. The linear sampling\nstrategies can both miss ﬂows and sample the same packets for\nmultiple times along the routing path. Some non-linear sam-\npling sketches have been proposed to overcome the drawback\nof uniform random sampling strategy.\nTo realize the constant relative error, small values of ﬂow\nsize are incremented with high probability while large ones\nwith low probability. The authors in [53] propose to set the\npacket sampling rate as a decreasing function of the ﬂow sizes.\nThis strategy can signiﬁcantly increase the packet sampling\nrate of small and medium ﬂows by sampling less large ﬂows.\nBy doing so, more accurate estimations of various network\nstatistics can be guaranteed. However, the exact sizes of all\nﬂows are available only if we keep per-ﬂow information for\nevery ﬂow, which is prohibitively expensive for highspeed\n\nlinks. Sketch guided sampling (SGS) [53] solves the estimation\nproblem by using a synopsis data structure called counting\nsketch to estimate the approximate sizes of all ﬂows. Adaptive\nRandom Sampling [54] bounds the sampling error (relative\nestimation error) within a pre-speciﬁed tolerance level and\nproposes an adaptive random sampling technique to adjust the\nsampling probability and minimize the number of samples.\nAdaptive Non-Linear Sampling (ANLS) [55] updates the\ncounters of sketch with sampling probability function p(c),\nwherecis the counter value and p(c)is a pre-deﬁned function.\nIn this work, the authors provide the general principles to\nguide the selection of sampling function p(c)for sampling\nprobability adjustment. The intuition of ANLS is to implement\na large sampling rate for small ﬂows while a small sampling\nrate for large ﬂows. The authors also derive the unbiased\nﬂow size estimation, the bound of the relative error, and the\nbound of the required counter size for ANLS. It updates a\ncounter from ctoc+1with probability p(c). Self-tuning ANLS\n[56] selects one speciﬁc sampling function according to the\nprinciples ANLS [55] proposed. Set the p(c)as follow:\np(c) =1\n(1+a)c\u00001(2)\nwhere ais a pre-deﬁned parameter 0<a< 1. This paper\nfocus on the method which adjusts the parameter aduring\nthe statistic process in order to provide the best and ideal\ntradeoff between accuracy and counting range. DISCO [16]\nfurther extends ANLS by regulating the counter value to be a\nreal increasing concave function of the actual ﬂow length to\nsupport the counting of ﬂow size, ﬂow volume counting, and\nﬂow byte count simultaneously.\nHowever, in order to accurately estimate the largest counter,\nthe above methods compromise the accuracy of the smaller\nﬂows [57], where the sampling probability function can be\nscaled to achieve a higher counting range at the cost of a\nlarger estimation error. Independent Counter Estimation Buck-\nets (ICE-Buckets) [57] ﬁrst presents a closed-form explicit\nrepresentation of an optimal estimation function, which is used\nto determine these probabilities and estimate the real value of\na counter. ICE Buckets separate the ﬂows into buckets and\nconﬁgure the optimal estimation function according to each\nbucket’s counting range, thereby signiﬁcantly reducing the\noverall error by efﬁciently utilizing multiple counter scales.\nBoth linear and nonlinear sampling may incur the problem\nthat some ﬂows are over-estimated (sampled more than the\nsampling probability), while some are under-estimated (sam-\npled less than the sampling probability). Starting with a simple\nidea that “independent per-ﬂow packet sampling provides the\nmost accurate estimation of each ﬂows”, SketchFlow [51]\nperforms an approximated systematic sampling for ﬂows to\nprovide a better tradeoff between accuracy and overhead for\na given sampling rate 1=p. This strategy is achieved by\nrecognizing a sketch saturation event for a ﬂow and only\nsample the saturated packets. However, SketchFlow is only a\ngeneral sampler strategy and can not be deployed to measure\nthe trafﬁc individually.\nbcdaacbaa a 10 updates\nSketch\nAgg a 5 b 2 c 2 d 1 4 updatesFig. 6: Agg-Evict strategy\nGenerally speaking, sampling is the optimal strategy to\nreduce the per-ﬂow measurement. From the uniform sam-\npling, adaptive sampling [53] [54] [56] [55] [16] to separated\nestimated-bucket strategy [57], the estimation accuracy keep\nincreasing. Although sampling still suffers from minor esti-\nmation errors, these strategies have played an essential role in\nsketch-based network measurement.\n2) Randomization Processing:\nDifferent from packet sampling, randomization processing\nis an alternative approach to reduce the packet processing\noverhead in which only the sampled counters are updated.\nRandomized HHH (Hierarchical Heavy Hitters) [58] pro-\nposes a randomized constant-time algorithm for HHH. Un-\nlike the deterministic algorithms whose update complexity\nis proportional to the hierarchy’s size H, RHHH randomly\nsamples only a single preﬁx to update using its respective\ninstance of heavy-hitters rather than updating all preﬁxes for\neach incoming packet. This randomization strategy decreases\nthe update time from O(H)toO(1), but requires a speciﬁc\nnumber of packets to provide desired accuracy guarantees.\nNitroSketch [59] is a careful synthesis of rigorous yet\npractical sketch design to reduce the number of per-packet\nCPU and memory operations. Like the mechanism used in\nRandomized HHH [58], NitroSketch only samples a small\nportion of packets by geometric sampling, and the sampled\npackets need to go through one hash computation, update to\none row of counters, and occasionally to a top- kstructure.\nThe experiments demonstrate that the accuracy is comparable\nto unmodiﬁed sketches while attaining up to two orders of\nmagnitude speedup and a 45% reduction in CPU usage.\nTo catch up with the line rate and reduce the processing\noverhead, both RHHH [58] and NitroSketch [59] takes the\nrandomization strategy to update the sampled counters in their\nsketches. Although they all reduce the updating overhead,\nthe corresponding query is complicated to cope with the\nincomplete recorded information, and a minimal volume of\npackets is required to achieve the desired accuracy guarantee.\n3) Batch Processing:\nDue to the high speed of trafﬁc packets and the per-ﬂow\nprocessing requirement, the operational overhead is rather\nheavy. In contrast to the sampling strategy, the inspiration of\nbatching is to aggregate the packets for the same ﬂow and\nthen update them together as a whole to reduce the overall\ncomputation overhead, as illustrated in Fig. 6.\nRandomized DRAM-based counter scheme [60] takes a new\ncounter update by looking up the cache to see if there is\nalready a waiting update request to the same counter. The\nscheme modiﬁes that request in the cache rather than creating\na new request (e.g., change the request from +1to+n).\n\nAgg-Evict [61], a typical network measurement tool, also\nemploys the same idea to leverage the beneﬁts of caching. It\nconstructs a data structure, which improves the efﬁciency of\nnetwork trafﬁc measurement in software and proposes a low-\nlevel mechanism to improve the efﬁciency of various schemes.\nSkimmed Sketch [62] and Augmented Sketch (ASketch) [63]\nfocus on aggregating the most frequent items in the ﬁlter\nand evict the cold items into the second stage, while Cold\nﬁlter [64] captures the cold items in the ﬁrst ﬁlter and evicts\nthe hot items to the second stage. ASketch and Skimmed\nSketch need two-direction communication between two stages\nto exchange elements because of the difﬁculty to capture hot\nitems accurately. However, Cold ﬁlter only needs one-direction\ncommunication and less memory access.\nThe core idea in batch processing [60] [61] [63] [64] is to\ntake the incoming packets for a speciﬁc ﬂow into a batch and\nthen update them as a whole into the synopses data structure.\nIn this way, the memory access to the sketch is signiﬁcantly\nreduced. However, this strategy needs to augment an additional\ncache to maintain the information of the previous packets’\nﬂow.\nC. Summary and lessons learned\nIn this section, we cover the aspect of preparation before\nmeasurement, including measurement environment and mea-\nsurement data preparation. By preparing the environment, each\nmonitor is optimally placed or activated in the corresponding\nmeasurement point to get the best measurement performance\nconsidering measurement cost. And all of them are allocated\nthe responsibility of measurement tasks. By data preparation,\neach measurement point performs the strategies on the raw\ntrafﬁc stream to reduce the measurement overhead. Note\nthat both sampling and randomization have to balance the\ntradeoff between accuracy and processing overhead. After the\npreparation phase, the sketch algorithms can be installed and\nperformed in each monitor point.\nIV. O PTIMIZATION IN SKETCH STRUCTURE\nThe sketches are responsible for recording the active ﬂows\nand the corresponding size or volume information. The ﬂow\ninformation is recorded in the sketch for later information\nretrieval and extraction in the updating stage. As shown in\nFig. 7, a lot of strategies have optimized sketches in terms\nofhashing strategy ,counter level optimization andsketch\nlevel optimization . In this section, we will review these work\nin detail with the consideration of performance optimization.\nA. Hashing strategy\nA sketch algorithm needs a feasible amount of space to store\nthe counters and a ﬂow-to-counter association rule so that ar-\nriving packets can update corresponding counters at line speed.\nThis subsection gives a detailed summary of hashing strategies,\nincluding traditional and learn-based hash strategies. Almost\nall existing sketch-based network measurements follow these\ndifferent ﬂow-to-counter rule designs as the basis.1) Conventional hash-based data structures:\nThe existing sketches are mainly based on traditional\nhash functions to maintain the mapping from ﬂow keys to\ncells/buckets in the data structures.\nHash-based frequency estimation. We regard CM Sketch\n[27], BF [45] and Cuckoo ﬁlter [65] as the hash base for\nfrequency estimation. CM sketch [27] consists of darrays,\nA1;:::;Ad, each of which contains wcounters. It hashes\nincoming packets into each row of the hash table and locates\none counter in each row to update the multiplicity or size\ninformation. Bloom ﬁlter [45] leverages the kbits in a vector\nto represent the membership of an element within set. Despite\nthe constant-time complexity and the space-efﬁciency features,\nBloom ﬁlter cannot support the deletion and record the multi-\nplicity of ﬂows. To this end, Counting Bloom Filter (CBF) [66]\nreplaces each bit in the vector with a counter of multiple bits.\nWhenever a ﬂow is mapped into cells, the kcounters will be\nincremented by the ﬂow size. Then the deletion of elements is\nstraightforwardly realized by decreasing the kcorresponding\ncounters. Cuckoo ﬁlter (CF) is a hash table with bbuckets, and\neach bucket has wslots to accommodate at most welements.\nFan et al . [65] further represents the elements by recording\ntheir ﬁngerprints in the cuckoo ﬁlter. They further apply the\npartial-key strategy to determine the candidate buckets during\nthe insertion phase and locate the candidate bucket [67] during\nthe reallocation phase. However, CF can not support constant-\ntime insertion because of the unbalanced probe during its\nreallocation phase.\nInstead of maintaining mapping from ﬂow key to ﬂow\ncounters directly, a novel strategy chooses to maintain mapping\nfrom ﬂow key to several bit/bits counters according to bit-\nrepresentation of ﬂow key. Group Testing [68] arranges a\nnumber of items into a group together to ﬁnd out active\nitems. It ﬁrstly decides which group the item belongs to\nand then updates these group counters as follows. The ﬁrst\ncounter records the total number of items and the following\ncounters are increased according to bits representation of the\nitem. And the majority item can be identiﬁed by traversing\nthe total counters. Fast sketch [69] takes the same strategy\nas Group Testing [68]. It hashes each incoming packet into\nseveral predeﬁned numbers of rows and adds its size to the\nﬁrst counter in each row. After that, the corresponding counters\nalso adds the ﬂow size according to the bit representation of\nthe quotient of the ﬂow. By collecting and adding sketches\nfrom multiple switches, the ﬁrst counter in each row is ﬁrstly\nchecked to recover the heavy-change ﬂows. SketchLearn [22]\nalso takes the bit-mapping strategy to extract large ﬂows from\nthe multi-level sketch and leaves the residual counters for\nsmall ﬂows to form Gaussian distributions. XY-Sketch [70]\ninvestigates the decomposition-and-recomposition framework\nto transform the problem of item frequency estimation into the\nproblem of probability estimation. Speciﬁcally, it decomposes\neach item into a sequence of sub-items and hashes them into\ncorresponding rows of the sketch. When responding to a point\nquery, XY-Sketch recomposes the element and returns the\nproduct of all the sub-items probabilities appearing in the data\n\nHashing strategy Counter level Sketch levelSpace \noptimization\nImprove \naccuracy\nLess memory \naccess    \n    \n      Functionality \nenrichment\nCASE [174]     \n \n  \n                      \n  \n  \n  \nVirtual \nRegister \nSharing[89]\nVirtual HLL[91]\nSliding \nHLL[77]Heavy\nGuardian [119]\nNitroSketch [ 59]   R HHH [5 8] FSS[142]\nFCM[122]  Cuckoo filter[65]Fast sketch[69]  OPA [182]\nSequential hashing[134]\nModular\nhashing[139] Bitcount[162] CBF[66]\nSketchLearn[22]Group \nTesting[68]FlowRadar[127]\nLD-\nSketch [125]LTC [178] LDA[124]\nMV-\nsketch [116]IBF[128]\nMAPLE [180]\nANLS[55]TinyTable[ 101]\nDiamond[96]CML Sketch [87]\nDISCO[16]ABC [104]\nOSM[188] RCC[106]SAC[ 85]\nCEDAR [212]\nHLL -TailCut+ [76]\nFrequent[112]\nCountMax[115]RAP[111] ACE[145] LCF[214]\nRefined LL[187]Conservative update [108]CSketch [83]\nSpace Saving[113] LR(T)[215]CSM [109]\nCHHFR[191]\nFSS[142] RCC[106]Univmon[140]\nIMP[132]SketchVisor[168]\nWCSS[149] RCD[135]\nSWAMP[147]IBF[128]\nIBF[128]\nIBF[128]IBF[128]CounterMap[230]\nASketch[63]\nBrick[ 99] Bloom \nSketch[ 97]\nUnivmon[140]\nHeavyKeeper[118]\nHeavyGuardian[119]ICE-Buckets[57]\nHyperSight[150]MultiResBitmap[71] Cold Filter[64]DCF[141]  Univon[140]   SBF[98]\nSF-sketch[136]   MRSCBF[131]\nDiamond \nSketch[96]\nMRSCBF[131] XY-\nSketch [70]RL-Sketch[126]Defeat [129]\nReversible Sketch [139]HeavyGuardian [119]\nPMC  sketch  \n[143]\nLSS[80] \nsandwiched \nLBF[170]\nMLsketch [165]Kraska  et \nal.[154]BF. Michael[155]FineComb [182]Spread Sketch [189]\nBraids[24]Counter \ntree[93]\nSketch\nVisor[168]OM sketch[95] Pyramid[94]\n  Cuckoo counter[107]SA[84]SALSA [105]SF sketch[136]PMC[143]Elastic sketch [158]\nActiveCM+ [88]Fig. 7: The taxonomy of the existing sketch methods. On the one head, the sketches are classiﬁed in three main aspects\nfrom the structure optimization perspective, i.e., hashing strategy, counter level optimization and sketch level optimization. On\nthe other head, to improve the performance, dozens of variants devote themselves to improve the space efﬁciency, increase\nmeasurement accuracy, incur less memory access and enrich the functionalities.\nstream and the total number of items.\nHash-based cardinality estimation. To count the number of\ndistinct elements in a ﬂow stream, different hash strategy was\nproposed. Bitmap [71] maps all the stream uniformly among\na bit array. By encoding each element as an index of the bit\narray, replicated elements can be ﬁltered automatically. How-\never, the space of this strategy is linear to the cardinality [72].\nAnother strategy is to prepare an array of sample buckets with\na reducing sampling probability exponentially. Each bucket\nis allocated a bit to record whether the bucket receives the\nstream elements. By extracting information for this bit array,\nPCSA [73], LogLog [74], HyperLogLog [75], HLL-TailCut+\n[76], Sliding HyperLogLog [77] all take this underlying data\nstructure as the base to estimate the cardinality.\nRecently, Zhou et al. [78] proposed a generalized sketch\nframework, including bSketch (based on the structure underly-\ning the counting Bloom ﬁlter), cSketch (based on the structure\nof CountMin) and vSketch (based on the memory sharing\nmechanism), which aims to incorporate the sketch design\nunder a general implementation structure, with the ﬂexibility\nof plug-n-play and many options for the tradeoff. Besides, to\nreverse ﬂow key from sketches, a lot of work separately hash\nseparated bit or a partition of ﬂow key, instead of the whole\nﬂow key, to record the statistical information of bit or bits.\nThe details can be referred to in Section V-B1. As the basis\nof the sketch algorithm, ﬂow-to-counter rule is the ﬁrst step to\nobtain the index of corresponding counters inside the sketch.\n2) Learning-based mapping:\nApart from the traditional hash strategies, a new trend of\ncombining learning-based methods, such as k-means cluster-\ning and neural network, with the hash table is emerging to\nreduce the space overhead and improve estimation accuracy.\nTo compute the optimal mapping scheme, Bertsimas et al.\n[79] propose a mixed-integer linear optimization formulation,and an efﬁcient block coordinate descent algorithm to compute\nthe near-optimal hashing scheme for the seen ﬂows. A multi-\nclass classiﬁer is responsible for mapping elements to counters\nbased on their features for the unseen ﬂows. Fu et al. [80]\nput forward a class of locality-sensitive sketch (LSS) by\nformulating a theoretical equivalence relationship between the\nsketching error and the approximation error of the k-means\nclustering. Composed with a cuckoo table as a per-ﬁlter to\ntest if it is a seen ﬂow and a cluster model to collect the ﬂow\nwith the closest cluster center for this ﬂow, LSS can efﬁciently\nmitigate the error variance optimize the estimation. Hsu et\nal. [81] combines a neural network to predict the log of the\npacket counts for each ﬂow. The heavy ﬂows are separately\nrecorded from non-heavy ones by this learned oracle to reduce\nthe interruption between them. Theoretical analysis proves that\nthe error of the learned CM sketch is up to a logarithmic factor\nsmaller than that of its non-learning counterpart. Based on\n[81], Aamand [82] further provide a simple tight analysis of\nthe expected error incurred by CM Sketch and the ﬁrst error\nbounds for both standard and learned version of CSketch [83].\nBy such designs, the learning-based sketches show a new\ntrend for measurement. We believe there will be a body of\nsuch work in the future. Although learning-based mapping\nincurs less space overhead, the implementation of such designs\nrequires substantial computation resources, which may not\nbe advisable for computation-scarce situations. Besides, the\nupdating of the out-of-data learning model is also no-trivial.\nB. Counter level optimization\nGiven the limited on-chip memory, if the counters in the\nsketch use many bits, the number of counters will be small,\nleading to poor estimation accuracy. In this case, most counters\nonly record mouse ﬂows; hence, those untapped bits are\nwasted. By contrast, if the counters occupy only a few bits,\n\n1 1 1/0 1/0 … … 0split digit sign part counting part\n0 1 0 1 1 1 1 1\n0 1 1 0 0 0 0 0sign part:2 counting part:31\nsign part:+1 counting part:0\nincrease the counting part \nby 1 with probability 1/4\nset the counting part to 00 0 0 0 … …A mode(a) SA\n1 1 1/0 1/0 … … 0split digit sign part counting part\n0 1 0 1 1 1 1 1\n0 1 1 0 0 0 0 0sign part:2 counting part:31\nsign part:+1 counting part:0\nincrease the counting part \nby 1 with probability 1/4\nset the counting part to 00 0 0 0 … …A mode\n(b) When insert an item (e;1)into SA\n1 1 1/0 1/0 … … 0split digit sign part counting part\n0 1 0 1 1 1 1 1\n0 1 1 0 0 0 0 0sign part:2 counting part:31\nsign part:+1 counting part:0\nincrease the counting part \nby 1 with probability 1/4\nset the counting part to 00 0 0 0 … …A mode\n(c) Dynamic SA\nFig. 8: Self-adaptive counter [84] and small active counter [85]\nthe sketch fails to represent elephant ﬂows. Thus, many efforts\nhave been made to address this dilemma.\n1) Small counter for larger range:\nIt is space-efﬁcient to use a small counter to keep approxi-\nmate counts of large numbers, as the resulting expected error\ncan be rather precisely controlled. Firstly proposed in Approx-\nimate Counting [86], Count-Min-Log Sketch [87], SAC [85]\nand SA [84] have led this idea to network measurement.\nTraditional sketches use binary -based counters to record\nthe frequency of ﬂows. The counter is increased by 1 once in-\ncoming a corresponding packet. CML Sketch [87] replaces the\nclassicalbinary -based counters by log-based counting cells.\nThelog-based counters increase with a predeﬁned probability\nx\u0000c, where xis thelogbase and cis the current estimation.\nConsequently, the log-based counter can use fewer bits and\nincrease the number of counters for the same storage space\nto improve the accuracy. However, CML Sketch sacriﬁces\nthe ability to support deletions for a more extensive counting\nrange.\nSmall active counters (SAC) [85] divides a cell into two\npart: ak-bit estimation part Aand al-bit exponent part\nmode , as shown in Fig. 8(a). Like ﬂoating-point number\nrepresentation, the estimation of SAC is ^n=A\u00012r\u0001mode, and\nris a global variable for all counters. An incoming packet\nwith size cwill cause the counter Aupdate as follows.\nIfc\u00152r\u0001mode, SAC ﬁrst increases Abybc\n2r\u0001modecand then\nincreasesAby 1 with probability given by normalized value\nof the residue. If c<2r\u0001mode, SAC increases Aby 1 with\nprobabilityc\n2r\u0001mode. WhenAoverﬂows, the mode part will be\nincreased. However, if mode overﬂows, renormalization step\nwill move the counting range to a higher scale, which is only\nsupported by increasing the global variable rand renormalize\nall counters at once to avoid estimation collision. This up-\nscaling strategy is inﬂexible. ActiveCM [88] uses a variant of\ncompressed counters in [85] and enlarges a 32-bit counter the\nmaximum counting range, which is far greater than 232.\nCompared with SAC [85], Self-Adaptive counters (SA) [84]\nRegister arrayEstimator 1 Estimator 2\n... ...\nBits poolEstimator 1\n...Estimator 2\n...(a) Register sharing [89]\nRegister arrayEstimator 1 Estimator 2\n... ...\nBits poolEstimator 1\n...Estimator 2\n...\n(b) Bits sharing [90]\nFig. 9: Virtual register design\ncounter is a more generic technique to adapt to different\ncounting ranges. Let ndenote the number of bits in a\ncounter. For each counter, it has s-bits for sign and ( n\u0000s)-\nbits for counting. Moreover, SA augments an expansion array,\n\r[0];\r[1];\u0001\u0001\u0001;\r[k\u00001], wherek=2s. When updating a SA\ncounter with value 1, SA ﬁrst gets the sign part s0, and then\nadds 1 to the counting part c0with probability1\n\r[s0], as Fig.\n8(b) illustrated. If the counting part reaches 2n\u0000s, the sign part\nwill be increased by one, and the counting part will be set to\nzero. Finally, the estimation can be calculated as follows in\nEqu. 3 and Equ. 4:\n\u001astage [0] =0\nstage [i] =2n\u0000s\u0002Pi\u00001\nj=0\r[j]; i> 0(3)\nestimate =c0\u0002\r[s0] +stage [s0] (4)\nTo address the inﬂexibility of the ﬁxed-length sign part,\nDynamic SA (DSA) augments a split bit and dynamically\nadjusts the sign bits, as shown in Fig. 8(c). The length of\nthe sign part is initialized to 0. Except for the split digit, all\nother bits are used to count. With the value represented by\nthe counter becomes larger, SA moves the place of split bit\nand increases the bits of sign part. The length of the sign part\nis increased to i, the counting part is shortened by ibit. The\nstages change to:\n\u001astage [0] =0\nstage [i] =Pi\u00001\nj=0\u0000\n\r[j]\u00022n\u0000j\u00001\u0001\n; i> 0(5)\nIn this way, DSA can accurately record mouse ﬂows while\nbeing able to deal with elephant ﬂows.\nThe above algorithms all focus on a single counter opti-\nmization for a more extensive counting range. CML Sketch\nleverages a ﬁxed logbase and updates counters according to\nthe current count. In comparison, SAC and DSA take more\nelastic strategies to enlarge the count range of little bits. All\nthese three estimators can represent large values with small\nsymbols at the price of a minor error because they all increase\nthe hashed counters with probabilities.\n\n2) Virtual register for spread estimation:\nIn this subsection, we focus on spread estimation, which\naims to record the distinct connections with hosts and takes\ncardinality estimation register to solve DDos, port scan, and\nsuper spreader detection. More works about cardinality es-\ntimation tasks are detailed in Section VI-A4. And the spread\nestimation can be divided into register sharing and bits sharing,\nas illustrated in Fig. 9.\nFor register-sharing. Virtual HyperLogLog Counter (VHC)\n[91] shares HLL registers in a shared pool to measure ﬂow\nsize. VHC contains an online encoding module to record\nthe packets in real-time, while the ofﬂine estimation module\nmeasures the size of all ﬂows based on the counters recorded\nby the online module. Furthermore, Virtual Register Sharing\n[89] dynamically creates an estimator for each ﬂow by ran-\ndomly selecting several registers from a multi-bits registers\npool to estimate the cardinality of elephant ﬂows. The register\nis the basic cardinality estimation unit, such as PCSA [73],\nLogLog [74] and HyperLogLog [75]. By sharing registers\namong many ﬂows, as shown in Fig. 9(a), the space of register\nis fully utilized. The array of registers are physical, while the\nestimators are logical and created on the ﬂy without additional\nmemory allocation.\nFor bits-sharing. Compact spread estimator (CSE) [90]\ncreates a virtual bit vector for each source by taking bits from a\nshared pool of available bits. It counts the number of distinct\nDstIP for each SrcIP and can be abstracted as multiple-set\nversion of linear counting [92]. Every individual SrcIP has its\nown virtual bits vector for linear counting, as illustrated in\nFig. 9(b), a portion of which is randomly shared with another\nSrcIP ’s virtual vector.\nBoth register-sharing and bits-sharing construct virtual esti-\nmators with common memory space. Using a compact memory\nspace can estimate the cardinality with a wide range and\nreasonable accuracy guarantees. These space-efﬁcient strate-\ngies enable on-chip implementation of spread estimation for\nnetwork measurement with line speed.\n3) Hierarchical counter-sharing for skewed trafﬁc:\nIn conventional sketches, all the counters are allocated the\nsame size and tailored in order to accommodate the maximum\nﬂow size. However, the elephant ﬂows only occupy a small\npartition of the trafﬁc. Therefore, the high-order bits in most\ncounters of conventional sketches are wasted. This waste leads\nto space inefﬁciency. To solve this problem, a lot of works,\nsuch as Counter Braids [24], Counter Tree [93], Pyramid\nSketch [94], One memory access sketch (OM sketch) [95]\nand Diamond sketch [96] have designed hierarchical counter-\nsharing scheme to record the ﬂow size, as shown in Fig.\n10. They organize the counters as a hierarchical structure in\nwhich the higher layers possess fewer memory. The lower\nlayer counters mainly record the information of mouse ﬂows,\nwhile the high layers record the number of overﬂows at the\nlow layers (the signiﬁcant bits of the elephant ﬂows size).\nMoreover, higher layer counters can be shared by multiple\nﬂows in order to reduce space overhead.\nLower level counterHigh level counterFig. 10: Hierarchical counter-sharing\nCounter Braids [24] compress counters into multi-layers\nby “braiding” a hierarchy of counters with random graphs.\nBraiding results in drastic space reduction by sharing counters\namong ﬂows. A random mapping strategy is implemented to\nconstruct the relation between ﬂows and the ﬁrst-layer counters\nor two consecutive layers of counters. Multiple ﬂows share\nthe common bits by sharing the high-layer counters to act as\nthe overﬂow counters. The low-layer and high-layer counters\nfunction as an biger counters to record the size of ﬂows.\nHowever, before querying a speciﬁc ﬂow, the information of\nall ﬂows must be obtained in advance, and the decoding should\nbe executed ofﬂine in batching way. As a result, the query\nspeed is signiﬁcantly slowed down.\nCounter tree [93] is a two-dimensional counter sharing\nscheme, including horizontal counter sharing and vertical\ncounter sharing. Physical counters are logically organized in\na tree structure with multiple layers to compose a new big\nvirtual counter. Different child counters may share the same\nparent counters in vertical counter sharing. Virtual counters are\nshared among different ﬂows in horizontal counter sharing.\nThe virtual counter array for a particular ﬂow consists of\nmultiple counters pseudo-randomly chosen from the virtual\ncounters. However, as pointed in [91], the experiments based\non real network trace data show that Counter tree cannot work\nwell under a tight memory space. Pyramid Sketch [94] further\ndevises a framework that not only prevents counters from\noverﬂowing without the need to know the frequency of the\nhottest item in advance but also can achieve high accuracy,\nupdate speed, and query speed simultaneously. It consists of\nmultiple counter layers where each layer has half counters\nof the last layer. The ﬁrst layer only has pure counters to\nrecord multiplicity, while other layers are composed of hybrid\ncounters. Different from the Bloom sketch [97], Pyramid\nsketch uses two ﬂag bit in hybrid counter for one child counter\nto record whether the child counter has overﬂowed or not.\nUnfortunately, as pointed out in SA [84], for an elephant\nﬂow, the hierarchical counter-sharing strategy needs to ac-\ncess all layers, requiring many memory accesses for each\ninsertion and query. OM sketch [95] has multiple layers with\na decreasing number of counters. When an overﬂow occurs\nthe ﬁrst time, the ﬂag in the low-layer counter will be set.\nTo alleviate the speed decline in the worst case, OM sketch\nonly takes a two-layer counter array to record multiplicity\ninformation. Moreover, OM Sketch leverages word constraint\nto constrain the corresponding hashed counters within one\nor several machine words. To improve the accuracy of OM\nsketch, OM Sketch records the ﬁngerprints of the overﬂowed\nﬂows in their corresponding machine words at the lower\nlayer to distinguish them from non-overﬂowed ﬂows during\n\nqueries. Cold Filter [64] also consists of two counter layers.\nBloom sketch [97] and Pyramid Sketch [94] take ﬂag bit to\nrecord if the low layer has overﬂowed. By contrast, Cold\nFilter will not change the low layer counters when the lower\ncounter overﬂows and take no ﬂag bit. For hot items, the\nsize is the addition of two-layer counters. Furthermore, cold\nﬁlter takes one-memory-access strategy to handle the memory-\naccess bottleneck, and the details can be referred to in Section\nIV-B5.\n4) Variable-width counters for space efﬁciency:\nUnlike the ﬁxed-width counter design, many efforts have\ndesigned variable-width counters to improve space efﬁciency\nand estimation accuracy under skewed trafﬁc.\nCounter resizability. Spectral bloom ﬁlters (SBF) [98] ex-\ntends bits vector of BF [45] as counters vector. The counters,\npacked one after another, are located by a hierarchical indexing\nstructure for fast memory access. However, an update causing\nthe width of counter ito grow will cause a shift of counters\ni+1;i+2;\u0001\u0001\u0001, which can have a global cascading effect,\nmaking it prohibitively expensive when there can be millions\nof counters [99]. Although the expected amortized cost per\nupdate remains constant and the global cascading effect of\ncounter shift is small in the average case, the worst case\ncannot be tightly bounded. Therefore, SBF with variable-\nwidth encoding is not an effective counter solution as it\ncannot ensure quick per-packet updates when a packet arrives.\nBucketized Rank Indexed Counters (BRICK) [99] adopts a\nsophisticated variable-width encoding of counters, which uses\nﬁxed-size buckets and Rank Indexing Technique [100] to\nsupport variable-length counters with a restricted counter sum.\nSpeciﬁcally, it bundles a ﬁxed number of counters randomly\nselected from the array into buckets. And Brick only allocates\nenough bits to each counters, i.e., if its current value is\nc, Brick allocates dlg (c) + 1ebits to it. In each bucket,\nBrick reorganizes counters into sub-counter arrays with a\ndecreasing number of counters. Moreover, multiple counters\nare composed of a new variable-width counter. Brick can\nhold more counters as the average counter is shorter than the\nlargest one [57]. Unfortunately, with the average counter value\nincreasing, the encoding becomes less efﬁcient.\nBucket Expandability. Inspired by Rank Indexing Technique\n[100], TinyTable [101] reorganizes space into chained ﬁnger-\nprints and counters. Using the same block for both ﬁngerprints\nand counters, Tinytable chains associate counter and ﬁnger-\nprints where each chain always starts with a ﬁngerprint and\nthat counterparts are always associated with the ﬁngerprint\nto the left if necessary. In order to handle bucket overﬂows,\nTinyTable dynamically modiﬁes the bucket size according to\nthe actual load in that speciﬁc bucket. If there is not enough\nspace in the bucket, Tinytable will borrow space from the\nfollowing buckets by the Bucket Expand operation, which\ntakes its inspiration from a linear probing hash table. This\nstrategy may cause the bucket to overﬂow again, in that case,\nTinyTable repeats the process. However, the complexity of\nadditions and removals increases with the table load. Com-\npared with SBF [98], TinyTable only requires a single hashfunction and accesses memory in add/remove operations in\na serial manner. We also note that, Counting Quotient Filter\n(CQF) [102] augments variable-sized counters in quotient ﬁlter\n[103] just like TinyTable [101] does. To make CQF work, an\n“escape sequence ” is proposed to determine whether a slot\nholds a remainder or part of a counter and how many slots\nare used by that counter.\nBits borrow or combination. Adjacent Borrow and Carry\n(ABC) [104] takes a novel bit reorganization strategy, which is\ndesigned for non-uniform multiset. The experiments on real-\nworld datasets show that when an elephant ﬂow is recorded,\nthe neighboring counters are often empty or occupied by\nmouse ﬂows. As a result, the unused bits can be borrowed\nby the adjacent elephant ﬂows to enlarge their count ranges.\nBy borrowing bits from the adjacent counter or combining the\nmapped counter with its adjacent counter into a big counter,\nABC can improve the memory efﬁciency at the cost of three\nbits to mark combined counters. However, the encoding of\nABC is cumbersome and slows down sketch signiﬁcantly.\nSelf-Adjusting Lean Streaming Analytics (SALSA) [105] dy-\nnamically re-sizes counters by merging neighbors to represent\nlarger numbers. Compared with ABC, SALSA only needs one\nbit to mark combined counters and encodes more efﬁciently.\nThe variable-width counter provides more ﬂexible and\nelastic space efﬁciency for sketch design. With an overhead\nof complicated operations, bit-shifting expansion [98], rank-\nindex-technique [99] [101], bit-borrowing/combination [104]\nand ﬁngerprint-counter chain [101] [102] all provide variable-\nsized counters, which enables the counters to use substantially\nless space for Zipﬁan and other skewed ﬂow distributions.\nSuch variable-width counters make a proper tradeoff between\nspace efﬁciency and the encoding operation overhead.\n5) Optimization for less memory access:\nWhile sketches can provide an estimation from multiple\nshared counters, they can not achieve both high accuracy\nand line speed at the same time. Accessing multiple counters\nrequires multiple memory accesses and hash computations.\nFrequent memory access can become the bottleneck of sketch-\nbased measurement. Several techniques have been developed\nto reduce memory access and hash computation when updating\nor querying sketches to handle this dilemma.\nOM sketch [95] leverages the word acceleration strategy\nto constrain the corresponding counters within one or several\nmachine words and achieves nearly one memory access for\neach insertion. Speciﬁcally, for the lower layer Ll,dl+1hash\nfunctions are associated with it. The ﬁrst hash functions are\nused to locate one machine word at the low layer, and the other\ndlhash functions are used to locate the dlcounters in this\nmachine word. For the higher layer Lh,dh+2hash functions\nare associated with it. The ﬁrst two hash function is used to\nlocate two machine words at the high layer, and the other\ndhhash functions are used to locate the dhcounters in this\nmachine word. Cold ﬁlter [64] also proposes its one-memory -\naccess strategy to access the low layer L1. Speciﬁcally, it\nconﬁnes the d1counters within a machine word of Wbits to\nreduce the memory access. Cold ﬁlter further splits the value\n\nproduced by a hash function into multiple segments, and each\nsegment is used to locate a machine word or a counter. In this\nway, OM Sketch and Cold ﬁlter can locate multiple counters\nwithin one machine word. To reduce the time consumption of\nmemory access, Recyclable Counter with Conﬁnement (RCC)\n[106] also limits a virtual vector to one memory block so\nthat it requires only one memory block access to read and\nwrite a virtual vector. To maintain only one memory access for\neach bucket operation, Cuckoo counter [107] conﬁgures each\nbucket as 64 bits. Furthermore, to efﬁciently handle skewed\ndata streams, Cuckoo counter implements entries with unequal\nlengths to insulate mice ﬂows from elephant ﬂows.\nBy conﬁning the counters for a speciﬁc ﬂow within one or\nseveral words, sketches can limit the memory access and speed\nup insertion and query overhead. The above sketches achieve\nless memory access overhead at the cost of lower accuracy.\n6) Counter update strategies for higher accuracy:\nNovel counter updating strategies are proposed to improve\nthe estimation accuracy of sketches. In this subsection, we\nreview such strategies by classifying them as CM-Sketch\nvariants, top- kdetection and various-hash design.\nCM-Sketch variants. Conservative update [108] increases\ncounters as less as possible. The intuition is that, since the\npoint query returns the minimum of all the dvalues, CM\nshould update a counter only when it is necessary. This\ndesign avoids unnecessary update of counter values to lessen\nthe over-estimation problem. CM-CU sketch also applies a\nconservative update method. Speciﬁcally, CM-CU updates a\nﬂowfwith sizecasmaxfsk[k;hk(f)];bc(f) +cg, where\nsk[k;hk(f)]represents the value of counters before updating,\nandbc(f)is the query result of ﬂow fbefore updating,\ni.e.,81\u0014k\u0014d;bc(f) =minfsk[k;hk(f)]g. SBF [98]\nalso optimizes with the “ minimum increase ” scheme. The\nminimum increase scheme prefers conservative insertion, i.e.,\nonly increase the smallest counter(s). Count sketch (CSketch)\n[83] introduces one more hash function gk()for thekth\narray to map ﬂows onto f+1;\u00001g. When updating a ﬂow\nfwith sizec, the corresponding counter will be increased\nbyc\u0001gk(). CSketch returns the median over dcounters, i.e.,\n81\u0014k\u0014d;medianfsk[k;hk(f)]\u0001gk(f)g, as an unbiased\nestimate of the point query, where sk[k;hk(f)]represents the\ncount of ﬂow fin thektharray. Counter Sum estimation\nMethod (CSM sketch) [109] splits hot items into small pieces\nand stores them into small counters. Flows are hashed into\nlcounters, and the ﬂow size is divided into lroughly-equal\nshares, each of which is stored in one counter. The CSM\nsketch randomly increments one of the hashed counters during\ninsertions and reports the sum of all the hashed counters\nsubtracted by the noise during queries. Reviriego et al. [110]\nﬁrst propose protection techniques to minimize the soft errors\nthat ﬂips the contents of memory cells. They evaluate the effect\nof soft errors on CM Sketch and propose to leverage upper\ncounter bit encoding a single parity bit per counter to detect\nerrors.\nTop-kdetection. Randomized admission policy (RAP) [111]\nand Frequent [112] update the ﬁxed key-value table by arandomized method for top- kidentiﬁcation and frequency\nestimation. When the table is full, RAP kicks out the item\ncwith the minimum counter value cmwith probability1\ncm+1;\notherwise, the incoming item will be discarded. Frequent de-\ncreases all counters and evicts the zero-number ﬂows to make\nspace for new elements. While Space Saving [113] always\nevicts the item with the minimal count directly. Unbiased\nspace saving [114] has more robust frequent item estimation\nproperties than Sample and Hold and gives unbiased estimates\nfor any subset-sum. CountMax [115] and MV-sketch [116]\napply the majority vote algorithm ( MJRTY ) [117] to track\nthe candidate heavy ﬂow in each bucket. When the incoming\npacket is the same as the recorded, the corresponding counter\nwill be increased; otherwise, decreased. Furthermore, the\nheavy candidate item will be replaced by the new item if the\ncounter is negative. HeavyKeeper [118] and HeavyGuardian\n[119] leverage a probabilistic method called “ exponential-\nweakening decay ”. When the incoming item does not match\nthe stored items, the ﬂow size will decay with a probability. As\na result, the elephants will be stored in the buckets, and the\nmouses will be decayed and replaced with high probability.\nThe core idea of these methods is to treat each element as the\nhot item and insert them into the top- kdata structure. After\nthe data structure space runs out, it gradually kicks out cold\nelements with a certain probability. However, as most elements\nare cold ones, processing so many cold items incurs not only\nextra overhead but also causes signiﬁcant inaccuracy for the\nranking and frequency estimation of top- kitems [120].\nVarious-hash design. Weighted Bloom ﬁlter [121] allocates\nkehash functions for each element edepending on the query\nfrequency and its likelihood of being a member. This strategy\ncan also be gracefully integrated into the frequency estimators\nto maintain a set of elephant elements. Frequency-aware\nCounting (FCM) [122] addresses the problem of inaccuracy of\nlow frequency ﬂows. FCM uses many hash functions per item\nby dynamically identifying low- and high-frequency stream\nitems. The low- and high-frequency items are classiﬁed by\nMG counter [123] which keeps track of counts of unique\nitems. This strategy is realized by setting two additional hash\nfunctions to determine the initial offset in rows and the gaps\nbetween two adjacent rows. To make high-frequency items\nupdate fewer counters than low-frequency items, the gap of\nlow-frequency items is smaller than the high-frequency items.\nThus the accuracy of low-frequency items will be increased.\nBesides, there are various speciﬁc counter updating strate-\ngies, such as the counter updating related to the aforemen-\ntioned four Section IV-B1,IV-B2,IV-B3,IV-B4, sampling in\nSection III-B1, and randomization in Section III-B2. The\ndetails can be referred in the corresponding subsection.\n7) Slot enforcement for various functionalities:\nAs shown in Fig. 11, because of the various measurement\nrequirement, sketches have enforced their buckets with other\ncells to support different additional functions. By augmenting\ntimestamp, packet-header ﬁeld counter, or key- XOR cell, the\nsketch can support latency detection, heavy hitter traction, and\nkey-enumeration functionality.\n\nLDA\ntimestamp Packet counter\nLD-Sketch\nkey-value array array length estimation error\nFlowRadar\nFlowXOR FlowCount PacketCount\nHeavyGuardian\nelephant guardian mouse\nMV-Sketch\nelephant guardian mouseTrack fine -grained latencies\nHeavy key detection\nSeparate and guard elephantsNetwork -wide flow decoding\nInvertible sketch  for \nheavy Flow DetectionFig. 11: Slot enforcement strategy\nLatency detection. Lossy Difference Aggregator (LDA)\n[124] is designed to measure packet loss and delay over short\ntime scales. LDA maintains timestamp accumulator-counter\npairs in an array. The packets are divided into groups according\nto the hash function. For each group, the sum of timestamps\nand the total count of packets are sent to the receiver. Upon\nreceiving such information, the receiver ﬁrst applies the same\nhash function to divide packets into groups. Suppose the total\npacket counter of the group at the receiver matches that at\nthe sender, the receiver can calculate the average delay for\nsuch a group. Otherwise, the receiver will discard such a\ngroup. Therefore, LDA signiﬁcantly reduces the measurement\noverhead by only transmitting the sum of timestamps and\npacket counter. However, when packets are lost or reordered in\na group, the entire group should be discarded; thus the delay\nfor such a group cannot be estimated.\nHeavy hitter traction. LD-Sketch [125] augments each\nbucketSi;jwith one additional component: Ai;j, an associa-\ntive array to keep track of the hashed heavy key candidates,\nand three parameters: Vi;jis incremented by the size of each\nincoming ﬂow; li;j, the maximum length of the array; and\nei;j, the maximum estimation error for the true sums of the\nkeys hashed to the buckets. To keep tracking of only heavy\nﬂows candidates, LD-Sketch need to create or delete the key-\nvalue in the associative array and li;j=b(k+ 1) (k+ 2)\u00001c, in\nwhichk=bVi;j=Tc. However, the detection accuracy declines\ndramatically when the frequency of heavy ﬂows is high [126].\nAt the beginning of detection, LD-Sketch is prone to cause\nfrequent jitter of arrays. Moreover, the detection accuracy\nand memory are sensitive to the ordering of incoming keys.\nThe later work [125] proposes three enhancement heuristics\nto address this problem. RL-Sketch [126] further uses Rein-\nforcement Learning to evict trivial ﬂows while leaving heavy\nkey candidates. MV-sketch [116] augments each bucket with\nthree components: Vi;j, the total sum of values of all ﬂows\nhashed to the buckets; Ki;jthe current candidate heavy ﬂow\nkey in the bucket; Ci;j, which indicates if the candidate heavy\nﬂow should be kept or replaced. Using a sufﬁcient number\nof buckets, MV-sketch can signiﬁcantly reduce the probability\nthat two heavy ﬂows are hashed into the same bucket and\nhence accurately track multiple heavy ﬂows. HeavyGuardian\n[119] enlarges each bucket to store multiple key-value pairs asheavy part and several small counters as light part. A heavy\npart is used to precisely store frequencies of hot items, and a\nlight part is used to store the frequencies of cold items approx-\nimately. The heavy part takes Exponential Decay strategy to\ndecrement the count of the weakest items with a probability. In\nthis way, HeavyGuardian can intelligently separate and guard\nthe information of hot items while approximately record the\nfrequencies of cold items.\nKey-reversible functionality. FlowRadar [127] extends the\nInvertible Bloom Filter (IBF) [128] design with three ﬁelds,\nFlowXOR, PacketCount, and FlowCount. It records the num-\nber of ﬂows and number of packets that have been hashed\ninto andXORs the corresponding ﬂow identiﬁers together\nin a cell. When a packet arrives, FlowRadar ﬁrst checks the\nﬂow ﬁlter to see if the ﬂow has been stored in the ﬂow set or\nnot. If the packet belongs to a new ﬂow, all three ﬁelds will\nbe updated. Otherwise, only PacketCount will be increased.\nWhen the collector receives the encoded ﬂows, it takes the\ndecoding strategy to derive the ﬂow ID reversely. Speciﬁcally,\nit can decode the per-ﬂow counters by looking for cells that\ninclude a single ﬂow (called pure sell). By locating the cells,\nincluding this ﬂow, FlowRadar removes them from ﬂow set.\nThis process ends when there are no pure cells.\nBy augmenting the counter with other ﬁelds, sketches\ncan support various functionalities. In the future, with the\ndevelopment of the network, there will be new-generated\nfunctionalities that require other ﬁelds in a cell/bucket.\nC. Sketch level optimization\nAfter summarizing the counter-level optimization, the base\nof sketch, we will get into details of the integral-sketch design.\nThe sketch-level optimization can be divided into multi-layer\ncomposition, multi-sketch composition and sliding window\ndesign. The researchers have developed novel algorithms to\nenlarge the sketch functions skillfully by composing multiple\nbasic units or different sketches together.\n1) Multi-layer composition:\nMany work have proposed multi-layer design based on one\nbasic unit to enable different measurement tasks. We list the\nrelated work in Table. III, which contains the basic unit and\nthe enabled function.\nFor entropy estimation. Defeat [129] leverages the entropy\nof the empirical distribution of trafﬁc features to detect unusual\ntrafﬁc patterns. It treats the abnormalities as unusual distribu-\ntions of these features. Defeat generates multiple sketches of\nSrcIP ,SrcPort ,DstIP ,DstPort separately. Moreover, multiple\nhistogram-sketches from different switches are summed to\nform a global sketch, which can be utilized to detect, identify,\nand classify attacks. However, the complex construction of em-\npirical histograms makes it less appealing for online realization\n[138]. Intersection Measurable Property (IMP Sketch) [132]\nobserves that the entropy of an OD ﬂow can be estimated by\na function of just two different Lpnorms (Lp1;Lp2;p16=p2)\nof OD ﬂows. IMP can deduce the entropy of the intersection\nof two streams AandBfrom the sketches of AandB.\nSpeciﬁcally, every participating ingress and egress node in the\n\nTABLE III: Multi-layer design\nMulti-layerMethods Basic unit Enabled function\nDefeat [129] Histogram-sketch Apply the subspace method [130] to detect anomalies\nMRSCBF [131] SCBF [131] Improve the estimate accuracy by different resolution\nIMP [132] LpSketch [133] Deduce the entropy of the intersection of two trafﬁc\nSequential Hashing [134] Hash table Enable key reversible function\nRCD sketch [135] Bit array Reverse the host with large connection degree\nSF-sketch [136] CM Sketch, Count Sketch Maintain a small sketch for transmission\nHashPipe [137] match-action hash table Use P4Switch to HH detection\nSketchLearn [22] Bit-level sketch Leverage multiple bit-level Gaussian distributions\nDiamond sketch [96] Atom sketches Perform per-ﬂow measurement on skewed trafﬁc\nnetwork maintains a sketch of the trafﬁc passing by it during\neach measurement interval. When measuring the entropy of\nan OD ﬂow between an ingress iand an egress point j, IMP\ncollects the sketches of ingress stream Oiand egress stream\nDj. It leverages Lpsketch [133] to derive the Lpnorm of\nthe trafﬁc stream based on the property of the median and\nestimates the entropy of OD ﬂows accordingly.\nFor more accurate. Diamond Sketch [96] composes atom\nsketches (counter array) into three parts: increment part, carry\npart, and deletion part. The increment part records the size\nof the ﬂow. The carry part records the overﬂow depth of\neach ﬂow. The deletion part is used to support deletions.\nIt assigns an appropriate number of atom sketches to the\nelephants and mouses dynamically. This strategy improves\nthe accuracy considerably while keeping a comparable speed.\nSlim-Fat (SF sketch) [136] takes a small-large sketch compo-\nsition to relax bandwidth requirement when the local monitor\ntransfers information to the remote controller. The large-sketch\nis used to assist small-sketch during insertions and deletions\nto improve accuracy within a limited space. In contrast,\nthe small sketch is transferred and used to answer queries.\nMulti-Resolution Space-Code Bloom Filter (MRSCBF) [131]\nemploys multiple SCBFs but operating at different sampling\nresolutions. SCBF can approximately represent trafﬁc ﬂows as\na multiset. With different sampling probability, the elements\nwith low multiplicities will be estimated by SCBF of higher\nresolutions, while high multiplicities will be estimated by the\nlower resolution SCBF. However, MRSCBF is very slow in\nterms of memory access and hash computations. And the\nbitmap nature of MRSCBF determines that it is not memory\nefﬁcient for counting [93].\nFor reversible design. SketchLearn [22] maintains l+1\nlevels of sketch, each level corresponds to a sketch formed\nby a counter matrix with rrows and ccolumns. The level- 0\nsketch records the statistics of all packets, while the level- k\nsketch for 1\u0014k\u0014lrecords the statistics for the kthbit of\nthe ﬂowkey. Vi;j[k]represents the counter value of the level-\nksketch atithrow andjthcolumn. SketchLearn focuses\nonRi;j[k] =Vi;j[k]\nVi;j[0], which denotes the ratio of the counter\nvalueVi;j[k]to the overall frequency Vi;j[0]of all ﬂows\nhashed to stack ( i;j). By leveraging the Gaussian distribution\nassumption, SketchLearn separates the large and small ﬂows.\nSpeciﬁcally, it uses Bayes’ theorem to estimate the bit-level\nprobabilities ^p[k], and further obtains a template ﬂowkeycomposed of zero, one and \u0003. By estimating frequencies of\neach k-bit using maximum likelihood estimation, SketchLearn\ngets frequency estimate of candidate ﬂows.\nReversible connection degree (RCD Sketch) [135] is a\nnew data streaming method for locating hosts with a large\nconnection degree. Based on the remainder characteristics of\nthe number theory, the in-degree/out-degree with a given host\ncan be accurately estimated. A connection degree sketch is\ndenoted asB= (B1;\u0001\u0001\u0001;BH).Bi(1\u0014i\u0014H)is av\u0002mi\nbit arrays.Bi[j] [k] (0\u0014j <v; 0\u0014k<mi)associates with\na hash function hi:f0;1;\u0001\u0001\u0001;n\u00001g!f 0;1;\u0001\u0001\u0001;mi\u00001g,\nwherenis the size of the source space of packets. All\nBi[j] [k] (0\u0014j <v; 0\u0014k<mi)share a hash function fi:\nf0;1;\u0001\u0001\u0001;E\u00001g ! f 0;1;\u0001\u0001\u0001;v\u00001g, whereEis the size\nof combination of source and destination space, and His\nthe number of data array. The bits arrays are all set zero\nat the beginning of measurement epoch. For an incoming\npacketpi=(si;di), the sketch will be updated as, 81\u0014\nk\u0014H; Bk[f(sijjdi)] [hk(si)] =1 . Thus, for any source\ns, the packets with source sare all hashed to column\nBi(s) =Bi[\u0001] [hi(s)] (1\u0014i\u0014H)in the sketch. Thus we can\nobtain the sum of the out-degrees of source s. Moreover, the\nhost addresses which are associated with large in-degree/out-\ndegree can be reconstructed by simple equation based on\nChinese Reminder Theory without using address information.\nSequential Hashing [134] constructs reversible multi-level\nhash arrays for fast and accurate detection of heavy items.\nThe idea of Sequential Hashing is that combining the heavy\nsub-bits of key can efﬁciently discover the full heavy items’\nkey. The original problem is divided into nested sub-problems.\nThe preﬁxes with different lengths of ﬂow key are extracted as\nthe sub-keys. It enumerates each sub-key space and combines\nthe recovered sub-keys to form the heavy ﬂows. However, the\nupdate costs of both Reversible Sketch [139] and Sequential\nHashing [134] increase with the key length. Both of them\nutilize the position information to recover the keys in their\nsketch. The running time to recover keys in them cannot\nachieve the sub-linear bound [69].\nFor universal sketching. Univmon [140] maintains log(n)\nparallel copies of a “ L2-heavy hitter” ( L2-HH) sketch instance,\nwhere nis unique elements in trafﬁc ﬂow stream. Univmon\ncreates substreams of decreasing lengths as the jthinstance is\nexpected to have all of the hash functions agree to sample half\nas often as the (j\u00001)thsketch. Each L2-HH instance outputs\n\nTABLE IV: Multi-sketch design\nMulti-sketchMethods Combined methods Enabled function\nCount-Min Heap [27] Count-min Sketch+heap Find top- kelements\nDCF [141] SBF [98]+CBF [66] Represent the multisets compactly\nFSS [142] Bitmap ﬁlter+Space Saving [113] Minimize updates on the top- klist\nPMC [143] FM sketch [73]+HitCounting [92] High-speed per-ﬂow measurement\nASketch [63] Filter+Sketch(CM [83], Count sketch [27]) Improve accuracy and overall throughput\nBloom sketch [97] BF [45]+multi-levels (Section IV-B3) Memory-efﬁcient counting\nL2heavy hitters Q(Q0,\u0001\u0001\u0001,Qlog(n)) and their estimated\ncounts. By leveraging the results from the L2-HH instance,\nUnivmon can use the Recursive Sum Algorithm from the\ntheory of universal sketching to get an unbiased estimator of\nG-sum functions of interest. Speciﬁcally, it recursively applies\nthe result from Qlog(n)toQ0sketch to function gto form\nthe interested G-sum using these sampled streams. However,\nUnivMon incurs large and variable per-packet processing over-\nhead, resulting in a signiﬁcant throughput bottleneck in high-\nrate packet streaming. ActiveCM+ [88] proposes a progressive\nsampling techniques to solve this problem. Speciﬁcally, it\nonly updates the last sketch where ﬂow fis sampled instead\nof variably multiple sub-sketches. Moreover, the progressive\nsampling also reduces the moment estimation error by more\nthan half that UnivMon. In this way, ActiveCM+ reduces\nmemory footprint and improves measurement accuracy.\n2) Multi-sketch composition:\nBy composing different data structures, sketch-based meth-\nods can further support various functions. We also list the\nrelated work and the corresponding basic sketch in Table. IV.\nFor memory efﬁciency. Dynamic count ﬁlters (DCF) [141]\nextends the concept of SBF [98] and improves the memory\nefﬁciency of SBF by using two ﬁlters. The DCF is designed for\nspeed and adaptiveness in a straightforward way. It captures\nthe best of SBF and CBF, the dynamic counters from SBF,\nand the fast memory access from CBF. The ﬁrst ﬁlter is\ncomposed of ﬁxed size counters. The novelty is that the size of\ncounters in the second ﬁlter is dynamically adjusted to record\nthe corresponding counters’ overﬂow times in the ﬁrst ﬁlter.\nUnfortunately, the two ﬁlters increase the complexity of DCF,\nwhich degrades its query and update speed [136].\nFor hot-cold separation. Probabilistic Multiplicity Count-\ning (PMC) [143] leverages FM sketch [73] to measure hot\nitems and modiﬁed HitCounting [92] to estimate cold items.\nSpeciﬁcally, the FM sketches of all estimators share their bits\nfrom a common bit pool uniformly at random so that mostly\nunused higher-order bits in the registers can be utilized. PMC\ncan record information on a passing-by packet by setting only\none single bit in the packet header ﬁeld. Based on the packet\nheader, a speciﬁc hashing mechanism determines the enabled\nbit position. Moreover, PMC is designed initially to estimate\nﬂow size, but it can be easily modiﬁed for ﬂow cardinality\nestimation, which other ﬂow-size estimators do not support.\nCount-Min Heap [27] augments CM Sketch with a heap used\nto track all heavy ﬂows and their estimated frequency. If the\nincoming ﬂow whose estimation exceeds the threshold, it is\nadded to the heap or replaces the minimum ﬂow when the heapis full. The heap is kept small by checking that the estimated\ncount for the item with the lowest count is above the threshold.\nIf not, the item is deleted from the heap. At the end of each\nepoch, all the heavy hitters can be output by scanning the\nwhole heap. ActiveCM+ [88] also augments min-heap to track\nthe top-kheavy hitters within each sub-sketch.\nFor ﬁltering packets. Filtered Space-Saving (FSS) [142]\naugments Space Saving with a pre-ﬁltering approach. A\nbitmap is used to ﬁlter and minimize updates on the measured\nlist. When a new item is incoming, the bitmap counter is ﬁrst\nchecked if there are already measured items in the bitmap; and\nthe measured list is searched to see if this element is already\nthere. The update is conducted according to the check result\nabout the bitmap and measured list. However, FSS needs to\nsearch a certain ﬂow key or to identify the minimum counter in\nthe list. It needs to traverse the whole heap in the worst case\nwithout additional memory support. ASketch [63] augments\na pre-ﬁltering stage to identify the elephants and leaves the\nmouses to sketch. Based on the skewness of the underlying\nstream, Asketch improves the frequency estimation accuracy\nfor the most frequent items by ﬁltering them out earlier. For\nthe items that are stored in the sketch, Asketch reduces their\ncollisions with the high-frequency items, as the items stored\nin the ﬁlter are no longer hashed into the sketch. This reduces\nthe possibility that a low-frequency item would appear as a\nhigh-frequency item, therefore reducing the misclassiﬁcation\nerror. Bloom Sketch [97] is composed of multiple layers of\nsketch +BF composition. The low layers are responsible for\nlow-frequency items, and the high layers process the items\nwhose multiplicity can not be counted by the lower sketches.\nMoreover, the BFs record whether the high layers record the\nitems. It leverages BF to recognize items whose multiplicity\nis not successfully recorded in the low layer. By this way,\nthe low-frequency items are recorded in the low layer, while\nhigh-frequency items are recorded in both low and high layers.\nSketchtree [144] comprises multiple ﬁlters, each associated\nwith a speciﬁc task to measure both elephants’ ﬂows efﬁciently\nand aggregate mice ﬂow. Speciﬁcally, Sketchtree is task-\noriented, and each ﬁlter is responsible for sending the related\nﬂows to the corresponding sketch and others to the next ﬁlter\nfor other measurement tasks.\n3) Sliding window design:\nTraditional sketch-based methods focus on estimating ﬂow\nsizes/volumes from the beginning of trafﬁc stream (landmark\nwindow model) [145]. In this model, beginning at a “ land-\nmark ” time point, sketch-based methods mainly focus on the\ndata which falls between the landmark and the current time\n\npoint. With time passing, more and more packets pass through\nthe monitor, the sketch runs out of space and has to be\nreset periodically [146], which is the intrinsic disadvantage of\nthis model. For many real-time applications, the most recent\nelements of a stream are more signiﬁcant than that arrived\na long time ago. This requirement gives rise to the sliding\nwindow sketch-model. Based on the sketch-based methods, the\nsliding strategy removes the expired elements as the incoming\nof new elements, thereby it always maintains the most recent\nWelements in the data stream. A lot of effort has been\ninvested in this area to get the most recent information.\nFirst-in-ﬁrst-out design. This strategy inserts the new el-\nements and evicts the stale ones. Sliding HyperLogLog [77]\naims to estimate the number of distinct ﬂows over the last w\nunit of time, which is smaller than the time window W. A\nlist of arrival timestamps and the position of the leftmost 1-\nbit in the binary representation of the hashed value associated\nwith the packet are maintained to record the recent packets’\ninformation in Wtime units. When packets are out of the\nwindow, they are evicted from the list, and the incoming item\nis updated. Although maintaining the timestamp exactly is the\nmost accurate strategy for the recent information, it incurs a\nheavy space overhead. The randomized aging strategy may be\na better tradeoff between the estimation accuracy and mainte-\nnance overhead. Sliding Window Approximate Measurement\nProtocol (SWAMP) [147] stores ﬂow ﬁngerprints in a cyclic\nbuffer and the frequencies are maintained in TinyTable [101].\nThe cyclic corresponds to a measurement window W. Within\nthe insertion of a new element, its ﬁngerprint replaces the\noldest items in the buffer; furthermore, the corresponding\ncount in TinyTable will be decreased, while the counter of\narriving ﬂow will be increased.\nPartitioned blocks design. By applying partitioned blocks\nto record the information, the sketch can randomly perform the\naging operation in the oldest block and update the operation in\nthe newest block. Hokusai [148] uses a series of CM Sketch for\ndifferent time intervals. It uses large sketches for the recent\nintervals and small sketches for older intervals to adapt the\nerror within a limited space. As time passing, a large sketch\ncan be halved into a small sketch by adding one half of\nthe sketch to the other half. Window Compact Space-Saving\n(WCSS) [149] divides trafﬁc stream into frames of size W.\nIt maintains a queue for each block that overlaps with the\ncurrent window. When a block no longer overlaps with the\nwindow, the related queue will be removed. This strategy keeps\nthe number of queues constant at all times. WCSS supports\npoint queries with constant-time complexity under the sliding\nwindow model. Segment Aging Counter Estimation (S-ACE)\n[145] uses multiple data synopses to store the elements arriv-\ning in different window segments. This design can maintain\nthe relative order between the window segments with their data\nsynopses and improve the probability of deleting the correct\noutdated elements. HyperSight [150] also applies a partitioned\nBloom ﬁlter to maintain a coarse-grained time order for testing\npacket behavior changes. However, this kind of strategy keeps\ntoo many sketches which incur expensive memory overhead.Randomized aging design. Aging Counter Estimation\n(ACE) [145] maintains the most recent Welements and adopts\nthe counter sharing idea [109] to the sliding window model.\nFor ACE, an aging algorithm is proposed to eliminate one\nelement when a new element comes. It randomly picks a\ncounter in the array and decreases it by one if applicable. It is\nsimple and efﬁcient. However, as tbecomes large, the sliding\nwindow accumulates many expired elements, introducing more\nnoise for size estimation. Memento [151] is a family of sliding\nwindow algorithms for the HH and HHH problems in the\nsingle-device and network-wide settings. Speciﬁcally, for each\npacket, Memeto performs a Full update operation (delete the\nstale data and add the new) with a probability or make a\nwindow update (delete outdated data).\nTime adaptive updating. Ada-Sketch [152] proposes a time\nadaptive scheme which is inspired by the well-known digital\nDolby noise reduction procedure. Speciﬁcally, when updating,\nAda-Sketch applies pre-emphasis and artiﬁcially inﬂates the\ncounts of more recent items compared to the older items, i.e.,\nmultiplying the updates ct\niwithf(t), which is a monotonically\nincreasing function of time t. Then when querying, Ada-\nSketch applies de-emphasis to estimate the frequency of item\niat timet, i.e., dividing the query results by f(t).\nScanning aging design. Sliding sketch [153] maintains\nmultiple counters in each cell and updates the most recent\ncounter. Unlike the above strategies, Sliding sketch adapts\nscanning operation to delete the out-dated by using a scanning\npointer upon the sketch. The speed of the scanning pointer\nis determined by the length of the sliding window. However,\nthe process of scanning pointer will inﬂuence the accuracy of\nestimation a lot.\nD. Summary and lessons learned\nIn this section, we detail the structure optimization in terms\nof hashing strategy, counter level optimization and sketch level\noptimization. Until now, there are a lot of work developed to\nenforce the sketches. From optimization techniques from ﬂow-\nto-counter rules and basic counter unit to the whole sketch,\nthe sketches can be updated with the techniques described in\nthis section. In the future, we believe learned hash-map [154]\n[155] [156] will play an important role in ﬂow-to-counter\nrules design because of less space overhead and hash collision\nerrors. And the novel counter and sketch architecture design\nwill be guided by trafﬁc characteristics and the development\nof architecture in the network.\nV. O PTIMIZATION IN POST -PROCESSING STAGE\nAfter recording trafﬁc statistics into sketches, we must per-\nform some operations before obtaining valuable information\nfrom the sketches, such as sketch compression and merging.\nFurthermore, some novel information extraction techniques are\nproposed to enrich the sketch functionality. In this section, we\nsurvey the optimization in the post-processing stage.\nA. Techniques for sketch compression and merging\nAlthough sketches have been designed as synopses that\nrequire only a few memory accesses for each insertion, there\n\n1\n5\n62\n4\n33\n9\n9SUM\n1\n5\n62\n4\n31\n4\n3MIN\nCompress(a) Compression operation\n1\n5\n62\n4\n33\n9\n9SUM\n1\n5\n62\n4\n31\n4\n3MIN\nCompress\n(b) SUM merging operation\nFig. 12: Sketch merging and compression\nremains scope for improvement in terms of reducing the\nbandwidth bottleneck. Remote controllers require high-speed\nmemory to receive ﬁlled sketches from multiple measurement\nnodes. Consequently, (1) the size of the ﬁlled sketch that\neach measurement node sends to the collector should be\nextremely small, and (2) the compressed or merged sketch\nshould contain sufﬁcient information that allows the remote\ncollector to answer queries accurately. The accuracy with\nwhich a sketch can provide answers to queries quantiﬁes how\nclose the value of the estimation from the sketch is to the actual\nvalue of the frequency [136]. Several studies have proposed\nmerging or compressing sketches before sending them to the\ncollector to reduce bandwidth overhead.\n1) Sketch compression:\nSeveral studies have proposed compressing sketches, as\nshown in Fig. 12(a). Some studies have proposed compressing\nsketches when adapting to the available bandwidth before\nsending them in order to save space for time-adaptive updating.\nCompression for cardinality estimation. Arithmetic-\ncoding-based compression [157] uses arithmetic coding to\ncompress FM sketches [73] and HyperLogLog [75] in a bit-\nwise and register-wise manner, respectively. The compression\nscheme can be understood as a separation of the represented\nvalue and the signiﬁcantly larger duplicate insensitivity in-\nformation. However, this approach was only designed for\ncardinality estimation sketches, and it does not meet the\nrequirements for compressing CM sketches.\nCompression for transformation. SF-sketch [136] main-\ntains a fat sketch and a slim sketch at a single measurement\npoint. The fat sketch is responsible for recording trafﬁc ﬂow\naccurately and allows the slim sketch to improve accuracy with\nless space overhead. When inserting or deleting an item, SF-\nsketch ﬁrst updates the fat sketch and then updates the slim\nsketch based on the estimation from the fat sketch. With this\ndesign, the trafﬁc ﬂow information can be compressed into a\nsmaller sketch for transmission. An elastic sketch [158] ﬁrst\ngroups the counters in the CM sketch and then compresses\nthe counters in the same group into a single counter. Given\na sketchSof sizezw0\u0002d(wherew=w0\u0002zis the width, d\nis the depth, and zis an integer representing the compressionrate), an elastic sketch splits Sintozequal divisions or sub-\nsketches, each of which has a size of w0\u0002d. By combining\nthe same index in its division (\b\nSk\ni[j]\t\nk=1;\u0001\u0001\u0001;z) in the same\ngroup, an elastic sketch builds a compressed sketch Band a set\nBi[j] =OPz\nk=1\b\nSk\ni[j]\t\n(1\u0014i\u0014d;1\u0014j\u0014z), whereOP\nis the compression operation (e.g., Max orSum ). The authors\nproved that after Sum compression, the error bound of the\nCM sketch does not change, whereas after Max compression,\nthe error bound becomes tighter. Same as Elastic Sketch\n[158], Cluster-reduce [159] also divides counters into groups\nbased on adjacency. After that, the counters in the nearby\ngroups which have similar values are rearranged into clusters.\nAnd each cluster are then reduced to a unique representative\nvalue. Cluster-reduce achieves high efﬁciency of compression\nprocedure, support of direct query without decompression, and\nhigh accuracy of compressed sketches simultaneously.\nCompression for time-adaptive updating. Hokusai [148]\nmaintains a series of CM sketches for different intervals to\nestimate the frequency of any item for a given time or interval.\nThe sketch space for recent intervals is larger than that for\nolder intervals so as to make the sketch time-adaptive. As\ntime passes, recent sketches are updated with incoming trafﬁc\nand older sketches are derived by compressing the most recent\nsketches one by one. Speciﬁcally, a sketch is compressed by\nadding one half of the sketch to the other half. Although this\nis a natural solution for achieving time adaptability, it still\nhas several shortcomings, such as discontinuity, inﬂexibility,\nexcessive sketches, and large overhead for shrinking [152].\n2) Sketch merging:\nFor collaborative measurement, distributed sketches must be\nmerged into a large summary, as shown in Fig. 12(b).\nMerging for CM-based sketches. For merging, homoge-\nneous sketches can be added or subtracted directly using\nthe same hash functions and structure for all the sketches,\nor isomeric sketches can be used to ﬁnd the corresponding\ncounters to obtain the ﬁnal result with different hash functions\nand sketch sizes. SketCh REsource Allocation for Measure-\nment (SCREAM) [26] allocates different sketch sizes at each\nswitch so that each sketch may have an array of different\nwidths and cannot be summed directly. SCREAM sums the\nminimum within each sketch to formulate the merged sketch\nin a distributed measurement by ﬁnding the corresponding\ncounters for each ﬂow.\nMerging for top- ksummary. Mergeable summaries [160]\nsuch as MG summary [123] and Space Saving [113] are\nmergeable and isomorphic. For tracking the top- kﬂows, two\ntop-krecords can be merged by subtracting the MG counters,\nwhich are lower than the (k+ 1)thcounter in the merged MG\nsummary. The procedure can be completed with a constant\nnumber of sorts and scans of summaries of size O(k).\nInspired by In-band Network Telemetry (INT), Light-\nGuardian [161] divides sketch into sketchlets, which can\nbe encoded into packets headers for further aggregation,\nreconstruction and analysis in the end-hosts. By this design,\nLightGuardian can collect per-ﬂow per-hop information for all\nthe ﬂows in the network with negligible overhead.\n\nB. Techniques for information extraction\n1) Flow key reversible techniques:\nTraditional methods only record the volume information\nof ﬂow into sketches according to the hash result of ﬂow\nkeys and provide a fast scheme for insertion and querying\nof trafﬁc ﬂow. However, no scheme can efﬁciently recover\nthe ﬂow key from the sketch itself except by enumerating the\nﬂow key space. Both direct recordings of the ﬂow key and\nenumeration are inefﬁcient for trafﬁc measurement because of\nspace restrictions and real-time response requirements. Many\nstudies have proposed solutions to this problem using bit\ncount, sub-key count, and IBF-based or other methods.\nDirect key recording. LD-sketch [125] maintains a two-\ndimensional array of buckets and associates each bucket with\nan associative array to track the candidate heavy ﬂows that\nare hashed to the bucket. However, updating the associative\narray involves high memory access overhead, which increases\nwith the number of heavy ﬂows. In particular, LD-sketch\noccasionally expands the associative array to hold more can-\ndidate heavy ﬂows; nevertheless, dynamic memory alloca-\ntion involves high costs and is challenging to implement in\nhardware. MV-sketch [116] also enforces each bucket with\na ﬂow key ﬁeld to record the candidate heavy ﬂow and an\nindicator counter to implement the majority vote algorithm\n(MJRTY ). Both LD-sketch and MV-sketch detect heavy key by\ndirectly recording the candidate ﬂow keys. Nevertheless, direct\nrecording is not a desirable strategy for per-ﬂow measurement.\nBit reversibility. BitCount [162] maintains a counter for\neach bit in the IP address to count the total number of 1-bits.\nIt can recover the heavy hitter keys by combining the bits\nwhose counter is larger than a given threshold. Meanwhile,\nDeltoid [68] comprises multiple counter groups with 1+ L\ncounters each (where Lis the number of bits in a key), in\nwhich one counter tracks the total sum of the group and\nthe remaining Lcounters correspond to the bit positions of\na key. It maps each ﬂow key to a subset of groups and\nincreases the counters whose corresponding key bits are 1.\nTo recover heavy ﬂows, Deltoid ﬁrst identiﬁes all groups\nwhose total sums exceed the threshold. If each such group\nhas only one heavy ﬂow, the heavy ﬂow can be recovered: the\nbit is 1 if a counter exceeds the threshold and 0 otherwise.\nFast Sketch [69] is similar to Deltoid [68], except that it\nmaps the quotient of a ﬂow key to the sketch. An incoming\nﬂow is ﬁrst added to the ﬁrst counter in the hashed rows.\nMoreover, the quotient function determines the counters in\neach row to add the ﬂow size. After checking the rows one\nby one and recovering the quotient bits that contribute to the\nheavy change, Fast Sketch can identify the heavy changes\nusing the inverse function according to the quotient technique.\nSketchLearn [22] combines the sketch design of Deltoid [68]\nwith automated statistical inference. Speciﬁcally, it maintains\na data structure similar to Deltoid and provides a fundamental\nproperty of bit-level Gaussian distributions. It proves that if\nthere is no large ﬂow, the bit counter values will follow a\nGaussian distribution. SketchLearn extracts large ﬂows frommulti-level sketches and leaves the residual counters for small\nﬂows to form Gaussian distributions.\nSub-key reversibility. Reversible Sketch [139] ﬁnds heavy\nﬂows by pruning the enumeration space of ﬂow keys. It\ndivides a ﬂow key into smaller sub-keys that are hashed\nindependently and concatenates the hash results to identify the\nhashed buckets. To recover heavy ﬂows, it enumerates each\nsub-key space and combines the recovered sub-keys to form\nthe heavy ﬂows. Sequential Hashing [134] follows a design\nsimilar to that of Reversible Sketch [139]; however, it hashes\nkey preﬁxes of different lengths into multiple smaller sketches.\nThe update costs of both Reversible Sketch and Sequential\nHashing increase with the key length.\nIBF variants. FlowRadar [127] takes an extension of IBF\n[128] as the encoded ﬂowset. Each counting table consists of\nthree ﬁelds, namely FlowXOR ,FlowCount , and PacketCount ,\nwhich record the XOR of all ﬂows, the number of ﬂows,\nand the number of packets of all ﬂows mapped in the bucket,\nrespectively. By ﬁnding the bucket that includes just one\nﬂow (called a pure cell), FlowRadar can decode the ﬂow\nby leveraging hash functions to locate the other cells of this\nﬂow and remove it from all the cells (by XORing with the\nFlowXOR ﬁelds, subtracting the PacketCount , and decrement-\ning the FlowCount ). The decoding process will continue to\nﬁnd new pure cells until there is no pure cell.\nDeltoid [68], Reversible Sketch [139], Sequential Hashing\n[134], Fast Sketch [69], SketchLearn [22], and BitCount\n[162] are all bit-level or sub-key (multi-bit)-level sketches\ndesigned for reversibility to deduce ﬂow keys directly. They\nﬁrst iteratively check the collected sketch for bits or sub-keys\nto identify the candidate ﬂow sub-key for measurement tasks.\nThe candidate ﬂow key is then composed of these sub-keys\nand further examined with the information recorded by the\nentire sketch to form the ﬁnal measurement result.\n2) Probability-theory-based techniques:\nIn contrast to the traditional counter estimation techniques,\nsome methods abstract counters as random variables and\nleverage probability-theory-based techniques, such as maxi-\nmum likelihood estimation andBayesian estimation , to obtain\nthe estimated counter of the ﬂow. By formulating the probabil-\nity models of the related variables, sketch methods can deduce\nthe desired statistical properties of the ﬂow trafﬁc.\nBayes’ Theorem. Multi-Resolution Array of Counters\n(MRAC) [163] ﬁrst uses counting algorithms and Bayesian\nestimation for accurate estimation of the ﬂow size distribution.\nIt increases the underlying counter for each incoming packet\nin a lossy data structure and infers the most likely ﬂow size\ndistribution from the observed counters after the collision.\nBayesian statistics are used to recover as much information\nfrom the streaming results as possible. SketchLearn is the ﬁrst\napproximate measurement approach to build on the charac-\nterization of the inherent statistical properties of sketches. It\nabstracts the kthbit of a ﬂow key as a random variable k.\nFurther,p[k]represents the probability that the kthbit is equal\nto 1. The multi-level sketch provides a fundamental property,\ni.e., if there is no large ﬂow, its counter values follow a Gaus-\n\nsian distribution. Based on this property, SketchLearn extracts\nlarge ﬂows from multi-level sketches and leaves the residual\ncounters for small ﬂows to form Gaussian distributions.\nMaximum likelihood estimation. MLM sketch [109] ab-\nstracts the counter in the storage vector of ﬂow fas a random\nvariableX, which is the sum of Y, i.e., the portion contributed\nby the packets of ﬂow f, andZ, i.e., the portion contributed by\nthe packets of other ﬂows. Based on the probability distribu-\ntion ofYandZ, MLM sketch formulates the probability of the\nobserved value of a counter. Moreover, the estimated size of\nﬂowfcan be obtained by maximizing the likelihood function.\nPersistent items Identiﬁcation and Estimation (PIE) [164] uses\nthe information recorded at the observation points during all\nthe measurement periods, formulates the occurrence estimation\nproblem as a maximum likelihood estimation (MLE) problem,\nand uses the traditional MLE approach for estimation. It\ndivides mcells in the Space-Time Bloom Filter (STBF) of\nthe measurement period iinto empty, singleton, and collided\ncells, the numbers of which are represented by Zi0,Zi1, and\nZiC, respectively. By maximizing the likelihood function, the\nnumber of distinct items can be estimated.\n3) Machine learning techniques:\nThe statistical trafﬁc properties change from time to time.\nOwing to this uncertainty, the traditional sketch design and\noptimization are not suitable for various measurement tasks,\nwhich leads to a considerable burden in terms of redesigning\nand tuning up the sketch design. Recently, several studies\nhave combined machine learning (ML) techniques with a\nmeasurement framework to relieve or eliminate the binding of\ntrafﬁc ﬂow characteristics and the sketch design. By selecting\na set of features from the sketch itself, where every feature\nis a property of the data contributing to the estimation, ML\ntechniques can reformulate the measurement tasks with these\nfeatures instead of the measured sketch and thus reduce the\ndesign complexity drastically.\nML sketch framework. MLsketch [165] presents a gener-\nalized ML framework to reduce the dependence of the sketch\naccuracy on the network trafﬁc characteristics. It continuously\nretrains ML models using a small number of samples from\nthe same trafﬁc whose information is being stored in the\nsketch. Thus, ML models can continue to adapt to any\nnetwork trafﬁc characteristic variations without requiring to\nmanually foresee scenarios or design statistical techniques\nfor different scenarios. The ML model can be trained using\nthe learning sketch and extracted features, and it reﬂects a\nmapping from the current trafﬁc distribution to the metrics\nof interest. This design is applicable to different network\nenvironments. Furthermore, designing the features and ML\nmodel usually involves just a selection from the available\nfeatures and models, which is much simpler than designing\na new sketch algorithm.\nLearning assistant parameter setting. To accurately evalu-\nate the frequency and rank of the top- khot items, the thresh-\nolds that are used to determine the type of each item (cold\nitems, potential hot items, or hot items) should be dynamically\nadjusted to adapt to the ﬂow distribution. To address this issue,SSS [120] assumes that the data stream obeys a Zipﬁan distri-\nbution, employs historical data to learn the parameters of the\ndistribution using an ML program, and sets thresholds based\non the distribution function. Speciﬁcally, it leverages the linear\nregression model in the ML program to learn the threshold\nused to identify hot items. The intelligent SDN-based Traf-\nﬁc (de)Aggregation and Measurement Paradigm (iSTAMP)\n[166] employs an intelligent sampling algorithm to select\nthe most informative trafﬁc ﬂows using information collected\nthroughout the measurement process. The main goal is to\nadaptively track and measure the most rewarding/informative\ntrafﬁc ﬂows, which, if measured accurately, can yield the\nbest improvement in the overall measurement utility. For this\npurpose, multi-armed bandit sequential resource allocation\nalgorithms are used to adaptively sample the most “rewarding”\nﬂows in order to improve the ﬂow measurement performance.\nLearning cardinality estimation. To adapt to changes in\nﬂow size distribution, Cohen and Nezti [169] proposed a\nsampling-based adaptive cardinality estimation based on the\nonline ML method. The trafﬁc ﬂow stream was divided into\nbatches of packets. Each batch was sampled, the selected\nfeatures and statistical properties were extracted from these\nsamples, and the exact cardinality of each batch was calcu-\nlated. Then, these values were used to train online ML models.\nMoreover, they analyzed various possible features, parameters,\nand online ML algorithms for their framework and proposed\nthe most suitable combination.\nLearning frequency estimation. Existing learning fre-\nquency estimation involves two strategies. On the one hand,\n[81] and [82] ﬁrst learned a classiﬁer to separately record the\nheavy and mouse ﬂows in order to reduce the mutual interfer-\nence collision. On the other hand, [79] and [80] leveraged the\nconcept of locality sensitivity to cluster similar items together\nto reduce the approximate error. By averaging the recorded\nfrequency, we can derive the approximate result.\nLearning membership testing. Although BF has been well\nexplored and evaluated in both academia and industry, Kraska\net al. [154] suggested that ML models such as neural networks\ncan be combined with BF to improve space utilization and\nquery efﬁciency further. They proposed a method in which\nML is used to model a pre-ﬁlter stage for the backup BF.\nMichael [155] further clariﬁed a formal model of this structure\nand stated what guarantees can and cannot be associated\nwith such a structure. The sandwiched LBF [170] optimizes\nBF by surrounding the learning model with two BF layers.\nSpeciﬁcally, the backup BF removes the false negatives from\nthe learning model, while the initial BF removes the false pos-\nitives upfront. The Partitioned Learned Bloom Filter (PLBF)\n[171] was developed to overcome the inability of previous\nmethods to exploit the learning model fully. It partitions the\nscore space into multiple regions using multiple thresholds and\nemploys an individual backup BF for each region. Experiments\nhave demonstrated that PLBF can achieve near-optimal perfor-\nmance in many cases. Based on the Stable Bloom Filter (SBF)\n[172], the Stable Learned Bloom Filter (SLBF) [173] replaces\nBFs in the sandwiched LBF [170] and PLBF [171] with SBF to\n\nTABLE V: General methods\nGeneral purposeFrequency HH Heavy Change Entropy Super Spreader/DDos Flow size distribution Cardinality\nOpenSketch [167] \u0002 X X \u0002 X X \u0002\nUnivmon [140] \u0002 X X X X \u0002 \u0002\nSketchVisor [168] \u0002 X X X X X X\nSketchLearn [22] X X X X X X X\nHeavyGuardian [119] X X X X \u0002 X \u0002\nElastic Sketch [158] X X X X \u0002 X X\nMLsketch [165] X X \u0002 \u0002 \u0002 \u0002 X\nHyperSight [150] \u0002 X X \u0002 \u0002 \u0002 \u0002\nTABLE VI: Persistence detection\nPersistence detectionMethods Enable sliding windows Store the whole ID Multi-structure False positive False negative\nPIE [164] X \u0002 X\u0002 X\nLDF [174] X \u0002 X X X\nLahiri et al. [175] X X \u0002 X X\nHuang et al. [176] X \u0002 X X \u0002\nLTC [177] X X \u0002 X X\nfacilitate data streaming applications. All the above-mentioned\nlearning sketches face a common problem in terms of updating\nthe out-of-date classiﬁer.\nC. Summary and insights\nIn this section, we reviewed studies related to optimiza-\ntion in the post-processing stage. The linearity of CM-based\nsketches provides an intrinsic characteristic for sketch com-\npression and merging. [157] and [160] proposed feasible\nsolutions for problems related to other sketches, such as\ncardinality registers and top- ksummaries. Furthermore, we\nsurveyed techniques for information extraction, such as ﬂow\nkey reversible, probability-theory-based, and ML techniques.\nThe information extraction techniques are directly related to\nthe sketch structure and updating operation. Other information\nextraction techniques will be proposed as novel sketch designs.\nVI. A PPLICATION AND IMPLEMENTATION\nWe have surveyed related studies on the sketch design from\nthe perspective of data ﬂow. In the following, we survey\nrelated studies from another perspective, i.e., application and\nimplementation of sketches. In total, we have summarized 13\nmeasurement tasks in terms of time and volume dimension.\nFurthermore, we survey the software and hardware implemen-\ntation when using sketches in practice.\nA. Measurement tasks\nAlthough the methods listed in Table V are designed for\nsupporting a comprehensive range of measurement tasks, there\nare some measurement tasks that they cannot support. To\nthis end, many studies have designed sketches for speciﬁc\nor several measurement tasks. In this subsection, we review\nthe related measurement tasks in terms of time, volume and\ntime-volume dimension.\n1) Persistence detection: In contrast to a frequent item, a\npersistent item does not necessarily occur more frequently than\nother items over a short period. Instead, it persists and occurs\nmore frequently than other items only over a long period.Persistence is typical of many stealthy types of trafﬁc on the\nInternet. For example, Xiao et al. [19] observed that legitimate\nusers connect to a server intermittently, whereas attacking\nhosts keep sending packets to the server. By identifying\npersistent items, operators can detect potentially malicious\ntrafﬁc behavior. Table VI compares existing persistence detec-\ntion methods in terms of their data structure characteristics,\nfunctionality, and measurement accuracy.\nMulti-layer design. Long-Duration Flows (LDFs) [174]\nmaintain two counting Bloom ﬁlters ( B1andB2) and one\nsmall hash table simultaneously. At the starting time interval,\nLDF records all the ﬂows that appear during this interval in\nB1. At the next time interval, LDF adds new ﬂows, which\nappear in both B1and the current interval into B2. Then,\nB1andB2are iterated by switching roles in the subsequent\nintervals. When a ﬂow is identiﬁed as LDF, it will be added to\nthe hash table. Lahiri et al. [175] presented the ﬁrst small-space\napproximation algorithm for identifying persistent items. A\nhash-based ﬁlter ﬁrst processes the items in each measurement\nperiod to record the ﬂow ID and track the persistence in future\nperiods. Furthermore, they considered the sliding model under\nwhich only items belonging to the Wmost recent windows\nare considered. PIE [164] uses Raptor codes to encode the ID\nof each item in a ﬂow trafﬁc stream. The persistent item can\nbe retrieved and decoded from the measurement point if there\nare sufﬁcient encoded bits for the ID in sufﬁcient measurement\nperiods. Speciﬁcally, PIE uses STBF to record information.\nDuring each epoch, PIE maintains a new STBF and transfers\nit to the permanent storage region for subsequent analysis.\nAfterTepochs, PIE uses MLE to estimate the number of\noccurrences of any given item. However, PIE must maintain\nTSTBFs to track persistent items with high space overhead.\nOn-off sketch [178] introduces a state ﬂag (on-off) into each\ncounter of sketch instead of maintaining a bloom ﬁlter to\nrecord the existence of element. At the beginning of each\nwindow, the ﬂag is set as on, once an element is hashed into\nthe counter, the ﬂag turn as off and the counters are only\n\nTABLE VII: Latency detection\nFlow latencyMethods Time stamp Probe packet Per-packet delay Deal with loss and reordering Abnormal delay detection\nLDA [124] \u0002 \u0002 \u0002 \u0002 \u0002\nColate [180] \u0002 \u0002 X \u0002 X\nOPA [181] \u0002 \u0002 X X X\nMarple [179] X\u0002 X X X\nFineComb [182] \u0002 \u0002 \u0002 partially \u0002\nRLI [183] \u0002 X\u0002 X \u0002\nincreased when the ﬂag bit is on. In this way, the counter is\nmost increased once in a time window.\nk out of t persistence. Although we can separate attackers\nfrom legitimates by ﬁnding ﬂows appearing during all mea-\nsurement periods, Huang et al. [176] considered the situation\nin which attackers might forgo sending packets during several\nperiods. To solve this problem, they formulated a new problem\ncalledk-persistent spread estimation, which measures persis-\ntent trafﬁc elements in each ﬂow that appear during at least\nkout oftmeasurement periods. Yang et al. [177] realized\nthat for some applications, people care about essential items\nthat are both persistent and frequent. They formulated this\nproblem and proposed LTC to solve it, including the Long-\ntail Replacement technique and modiﬁed CLOCK algorithm.\n2) Flow latency: Delay is an essential metric for under-\nstanding and improving system performance. When managing\nnetworks with stringent latency demands, operators must mea-\nsure the latency between two observation points, such as a port\nin a middlebox and a network card in the end host. Moreover,\nas reported in [179], both ﬁne-grained and coarse-grained\ndelay measurement are of great importance in performance\nmeasurement, system diagnosis, trafﬁc engineering, etc.\nFor per-ﬂow latency measurement. MAPLE [179] attaches\na timestamp to every packet. Speciﬁcally, when measuring\nthe packet passing through observation point StoR,S\nattaches a timestamp to the packets, and Rcalculates the\npacket’s latency from StoRby subtracting the timestamp\nfrom its current time. To reduce the space required to store\nall the packets’ latency values, MAPLE predeﬁnes a set of\nlatency values and maps each packet to its closest value.\nFor each latency value, MAPLE augments a Bloom ﬁlter to\ntest whether a given packet has been mapped to the latency\nvalue. Reference Latency Interpolation (RLI) [183] uses packet\nprobing. Speciﬁcally, Sinserts probe packets with a timestamp\ninto the ﬂow, and Rcalculates the latency of each probe\npacket similarly to MAPLE. Suppose that the latency of two\nprobe packets has been calculated as l1andl2; to calculate\nthe latency of the regular packets between two probe packets,\nRuses the straight-line equation to calculate the two probe\npackets’ latency. Further, Colate [180] is the ﬁrst per-ﬂow\nlatency measurement scheme that requires no probe packets or\ntimestamp. It records the time information of packets at each\nobservation point and purposely allows noise to be introduced\nin the recorded information to minimize storage.\nFor aggregate latency. The aggregate latency refers to\nthe average and deviation of the latencies experienced by\nall the packets that pass through two observation points. InLDA [124], both the sender and the receiver maintain several\ncounter vectors composed of counter pairs, the timestamp\ncounter for accumulating packet timestamps, and the packet\ncounter for counting the number of arriving/departing packets.\nFor each packet, LDA ﬁrst maps the packet into the counter\nvector with a sampling probability. In the long run, the packet\nis mapped into a counter-pair, the timestamp of the packet\nis added to the timestamp counter, and the packet counter is\nincremented by one. LDA selects all the counter pairs with the\nsame packet counter for both the sender and the receiver by\nchecking the packet counters. Then, LDA can easily calculate\nthe total number of successfully delivered packets and the sum\nof their timestamps. However, in LDA, packets belonging to\none segment may be misidentiﬁed with other segments due\nto packet reordering. Thus, these groups of packets cannot be\nused for delay measurement.\nFineComb [182] proposes a data structure called stash,\nwhich maintains the information for packets near the boundary\nof the segments at the receiver to solve this problem. However,\nfor groups with lost packets, FineComb cannot calculate their\nper-packet delay. Order Preserving Aggregator (OPA) [181]\nleverages the fact that lost and reordering packets are usu-\nally much fewer than legitimate packets to facilitate efﬁcient\nordering as well as loss of information representation and\nrecovery. It proposes a two-layer design to convey ordering and\ntimestamps and efﬁciently derive the per-packet delay. How-\never, compared with LDA [124] and FineComb [182], OPA\nimproves the accuracy of delay measurement with additional\noverhead for a high data link.\nAs illustrated in Table VII, we borrow the idea from OPA\n[181] and compare ﬂow latency measurement methods in\nterms of their functionality, performance, and the basic idea.\n3) Per-ﬂow measurement: Providing per-ﬂow statistics\nplays a fundamental role in network measurement. As a\nprobabilistic data structure, sketches have been extensively\ninvestigated for per-ﬂow measurement, including ﬂow size (the\nnumber of packets in a ﬂow) and ﬂow volume (the number of\nbytes in a ﬂow).\nTable VIII summarizes related studies on per-ﬂow mea-\nsurement. Apart from the CM sketch [27] and its variants,\nmost research focuses on the skewness characteristic of the\ntrafﬁc stream. Many methods have been proposed to solve\nthis unbalance situation in terms of the trafﬁc size/volume.\nWe have discussed the related work in detail in Section IV.\n4) Cardinality estimation: Estimating the number of dis-\ntinct ﬂows (or cardinality) is an essential issue in many\napplications, such as measurement and anomaly detection.\n\nTABLE VIII: Per-ﬂow measurement\nPer-ﬂow measurementFunctionality Methods\nCountMin [27] and its variantsConservative update [108], CSM Sketch [109]\nCount MeanMin sketch [184], Count sketch [83]\nAdaptive to skewed trafﬁcVariable counters size SBF [98], DCF [141], TinyTable [101], CQF [102]\nEnlarge count range SAC [85], Cuckoo counter [107], SA [84]\nMulti-level countersCounter Braids [24] Brick [99], RCC [106], Pyramid Sketch [94],\nCounterTree [93], OM sketch [95], ABC [104], Cold ﬁlter [64],\nDiamond sketch [96]\nMulti-layer sketch Bloom Sketch [97], ASketch [63]\nFrequency-aware updating FCM [122]\nTABLE IX: Cardinality estimation [76]\nCardinality estimationMethods Std.Err.( \u000e) Memory units\nMinCount [185] 1:00=pm 32-bit keys\nPCSA [73] 0:78=pm 32-bit register\nLogLog [74] 1:30=pm 5-bit register\nHLL [75] 1:04=pm 5-bit register\nSliding HLL [77] 1:04=pm 5-bit register\nHLL-TailCut+ [76] 1:00=pm 3-bit register\nReﬁned LL [186] 1:00=pm 5-bit register\nCohen and Nezti [169] \u0000\u0000 \u0000\u0000\nAs shown in Table IX, we borrow insights from the well-\nknown cardinality estimation in [76], where mis the total\nnumber of memory units. Linear Counting [92] maps all\nﬂows uniformly among a bit array, and each element can be\nmapped to a bit. However, it cannot work efﬁciently under\na strict condition where the memory is roughly linear to\nthe cardinality [72]. PCSA [73] uses stochastic averaging ,\nwhich allocates multiple registers to produce independent\nestimations, and returns the average. However, to achieve high\naccuracy,mshould be large, and mlognbits is the cost of the\nlimited memory in the monitors [76]. This limitation of PCSA\nmotivates another more efﬁcient algorithm called LogLog [74].\nBoth LogLog [74] and HLL [75] are based on the pattern\n“0R1” in the binary representation of hashed values. More\nspeciﬁcally, after hashing all the packets, the position of the\nleftmost 1-bit is denoted by R. However, they adopt different\nmethods for extracting the estimation from a set of registers.\nLogLog uses geometric averaging whereas HLL uses harmonic\nmeans to mitigate the impact of outliers with abnormally\nlarge estimations. Sliding HLL [77] improves HLL for stream\nprocessing by adding a sliding window mechanism. HLL-\nTailCut+ [76] can further reduce the memory requirements\nof HLL by 45% on the basis of an extended tail cutting\ntechnique that compresses the information across all registers\nwhile reducing the variance among the registers. Its basic idea\nis that the memory per register may be compressed into four\nbits or even into three bits with no signiﬁcant loss of useful\ninformation. HLL-TailCut+ can get many registers, and a more\naccurate estimation result given the same space overhead.\nReﬁned LogLog [186] uses a ﬁne-grained common ratio\ninstead of 2 used in FM Sketch [73] and LogLog [74] to\nnarrow the gaps between two recordable values. Using geo-metric hash functions with a smaller common ratio, Reﬁned\nLogLog overcomes the restriction that the former method can\nonly record 2k, resulting in a large gap with the actual number.\nMoreover, in real-world networks, we do not have cardinality,\nwhich would enable us to choose the appropriate size of\nbitmaps. A self-adaptive version of LogLog was proposed to\nadapt to cardinality by changing its common ratio dynamically.\nIt achieves high accuracy when the cardinality is small and can\nalso store large cardinality.\nCohen and Nezti [169] proposed a sampling-based adaptive\ncardinality estimation. The trafﬁc ﬂow stream is divided into\nbatches of packets. Each batch is sampled, and the selected\nfeatures are extracted from the samples. The features are then\npassed to the predict ()operation of the online ML algo-\nrithm, which returns an estimation of the batch’s cardinality.\nSubsequently, the entire batch is sent for training. In the\ntraining phase, partialfit()is provided with the batch’s set\nof features and its cardinality. The exact cardinality of a batch\ncan be calculated using a simple hash table.\n5) Super spreader: Spreader estimation aims to identify\na source IP that communicates with more than a threshold\nnumber of distinct destination IP/port pairs.\nIn this case, the ﬂow is identiﬁed by the source address.\nThe elements under measurement are the destination addresses\nin the headers of the packets. An increase in the cardinality\nof a speciﬁc ﬂow may signal a DDoS attack against the\nﬂow’s destination address. DDoS is a malicious attempt to\ndisrupt regular trafﬁc of a targeted server, service, or network\nby overwhelming the target or its surrounding infrastructure\nwith a trafﬁc ﬂood. In this case, a ﬂow is identiﬁed by the\ndestination address, and we must count the number of distinct\nsource addresses in each ﬂow.\n\nTABLE X: Super Spreader/DDos\nSuper Spreader/DDosMethods Basic idea\nOne/two level Filtering [187] Set sampling probability to ﬁlter\nOSM [188] Share bitmaps\nMultiResBitmap [71] One bitmap one source\nCSE [90] Bit sharing\nVirtual Register Sharing [89] Multi-bits sharing\nRCD sketch [135] Remainder characteristics of number theory\nTable X compares the studies in terms of the basic idea.\nOne-level ﬁltering [187] samples distinct source-destination\npairs in the packets such that each distinct pair is included\nin the sample with probability p=1\nkto identify the k-super\nspreaders. Two-level ﬁltering [187] maintains two levels of\nﬁlters. The ﬁrst level ﬁlters out the SrcIP that contacts only\na small number of distinct DstIP with a small sampling\nprobability, which is smaller than that of the second ﬁlter.\nCompared with one-level ﬁltering, two-level ﬁltering is more\nspace-efﬁcient. Both levels can capture the real super spreaders\nand the SrcIP that contacts with a few DstIP can be ﬁltered\nout efﬁciently in the ﬁrst level. By embedding estimated fan-\nouts into binary hash strings, multiple Spread Sketch [189]\ncan be merged to provide a network-wide measurement view\nfor recovering super spreaders and their estimated fan-outs.\nMultiResBitmap [71] uses bitmaps to save space instead of\nstoring the actual source/destination addresses in each sample.\nIt allocates a bitmap, where each bit is set for each destina-\ntion/source that the source/destination contacts. However, such\na spread estimator cannot ﬁt in a tight space where only a few\nbits are available for each source [89]. OSM [188] allocates\neach source randomly to lcolumns in a bit matrix (where\ncolumns are bitmaps) through lhash functions. Moreover, a\ncolumn may be shared by multiple sources. To reduce the\nintroduced noise, OSM proposes a method to remove the noise\nand estimate the spread of the source. Nevertheless, on the\none hand, OSM makes lmemory accesses and uses lbits\nfor storing one contact; on the other hand, the noise may be\ntoo much to be removed in a compact memory space when a\nsigniﬁcant number of all the bits are set. CSE [90] also creates\na virtual bit vector for each source by taking bits uniformly at\nrandom from a common pool of available bits. This sharing\nstrategy provides an excellent property: the probability that one\nsource’s contacts cause noise in any other source is the same.\nCompared with the noise in [188], such uniform noise is easy\nto measure and remove. Virtual Register Sharing [89] develops\na framework of virtual estimators that allow sharing of the\ncardinality estimation solutions, including PCSA [73], LogLog\n[74], and HyperLL [75]. Moreover, it shows that sharing at the\nmulti-bit level is superior to sharing at the bit level. RCD [135]\nlocates hosts with a large connection degree by estimating the\nin-degree/out-degree associated with a given host through the\nremainder characteristics of number theory.\n6) Heavy Hitters: The ﬂows which are identiﬁed as heavy\nhitters are the ones that exceed a speciﬁed volume of the total\ntrafﬁc. In many applications, such as congestion and anomalydetection, identifying heavy-hitter ﬂows is crucial [190].\nCM sketch and Count-min heap [27] can detect a heavy\nhitter by checking whether a ﬂow is a heavy hitter by ex-\namining if its estimated sum exceeds the threshold. However,\nCM Sketch is non-invertible. Lossy Counting [191] treats all\nincoming ﬂows as elephant ﬂows. If the ﬂow’s counter exists,\nLC increases the corresponding counter on every arrival; if\nthe counter is not in the table, it is allocated a counter\nvalue of 1 if available. LC keeps the table size bounded\nby periodically decreasing table counters and evicting items\nwhose counter reaches 0. However, LC requires a maximum\nnumber of1\n\u000flog(n)table entries. Probabilistic Lossy Counting\n(PLC) [192] requires fewer table entries on average but only\nprovides a probabilistic guarantee. PLC makes the error bound\nsigniﬁcantly smaller than the deterministic error bound of\nLossy Counting [191]. In lossy counting, the error bound\nreﬂects the potential error in the estimated frequency of an\nelement owing to possible prior element removal(s) from the\ntable. PLC shows that the error of more than 90% of the ﬂows\nis signiﬁcantly smaller than the deterministic error bound.\nUsing a probabilistic error bound instead of a deterministic\nerror bound to remove ﬂows in a ﬁnished epoch, PLC improves\nthe memory consumption of the algorithm. Furthermore, PLC\nreduces the false positives of lossy counting and achieves a\nlow estimation error, albeit slightly higher than that of LC.\nCaching Heavy-hitter Flows with Replacement (CHHFR)\n[190] leverages LRU to track heavy hitters. It not only focuses\non the update time of the trafﬁc ﬂows but also adds another\nvariableCtrto ﬁnd a replaced item with an underlying double-\nlinked list of ﬂow units to record the frequency of items. MV-\nsketch [116] is designed for supporting heavy ﬂow detection\nwith small and static memory allocation. It tracks candidate\nheavy ﬂows inside the reversible sketch data structure via\nthe idea of majority voting. LD-sketch [125] combines the\ntraditional counter-based and sketch-based methods to reduce\nfalse positives by aggregating multiple detection results. By\naugmenting an associative array in each bucket, LD-sketch\ncan track heavy key candidates that are hashed to the bucket.\nBitCount [162] maintains a set of counters to count the total\nnumber of 1-bits in each bit position of the binary repre-\nsentation of IP addresses. However, owing to the additional\nproperty of bit counters, this method may involve inevitable\nfalse positive errors. IM-SUM [8] ﬁrst introduces the elephant\ndetection scheme, which focuses on the elephants in terms of\nvolume. It achieves constant update time consumption, i.e., an\namortized time of O(1), by periodically removing many small\n\nTABLE XI: Heavy Hitter detection\nHeavy hitterMethods Space Updating time Query time\nCM sketch [27] O\u0010\nH\n\u000flog1\n\u000e\u0011\nO\u0000\nlog1\n\u000e\u0001\nO(n)\nCount-min heap [27] O\u00001\n\u000flog1\n\u000e+Hlogn\u0001\nO\u0010\nlogH\n\u000e\u0011\nO(H)\nCHHFR [190] \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000\nMV-Sketch [116] O\u00001\n\u000flog1\n\u000elogn\u0001\nO\u0000\nlog1\n\u000e\u0001\nO\u00001\n\u000flog21\n\u000e\u0001\nLD-sketch [125] O\u0010\nH\n\u000flog1\n\u000e\u0011\nO\u0000\nlog1\n\u000e\u0001\nO\u0010\nH\n\u000flog1\n\u000e\u0011\nLC [191] O\u00001\n\u000flog (n)\u0001\nO(1) O\u00001\n\u000flog (n)\u0001\nPLC [192] O\u00001\n\u000flog (n)\u0001\nO(1) O\u00001\n\u000flog (n)\u0001\nBitCount [162] O\u00001\n\u000f\u0001k\u0001\nO(k) O(k)\nSequential Hashing [134] O\u0010\nH\n\u000flognlog1\n\u000e\u0011\nO\u0000\nlognlog1\n\u000e\u0001\nO\u0010\nH\n\u000flognlog1\n\u000e\u0011\nIM-SUM [8] O\u00001\n\u000f\u0001\nO(1) O\u00001\n\u000f\u0001\nDIM-SUM [8] O\u00001\n\u000f\u0001\nO(1) O\u00001\n\u000f\u0001\nFast sketch [69] O\u0012\nH\n\u000flog1\n\u000elogn\u000f\nHlog1\n\u000e\u0013\nO\u0012\nlog1\n\u000elogn\u000f\nHlog1\n\u000e\u0013\nO\u0012\nH\n\u000flog31\n\u000elogn\u000f\nHlog1\n\u000e\u0013\nTABLE XII: Hierarchical Heavy Hitters\nHHHMethods Space Updating time Enable mHHH\nSeparator [195] O\u0010\nH2\n\u000f\u0011\nO(H)\u0002\nTCAM-based HHH [196] O\u00002\nT\u0001\nO(H)\u0002\nRecursive Lattice Search [193] \u0000\u0000 O(NlogH) X\nRandomized HHH [58] O\u0010\nH\n\u000f\u0011\nO(1) X\nTruong et al. [194] O\u0010\nH3=2\n\u000f\u0011\nO\u0010\nH3=2\n\u000f\u0011\nX\nCormode et al. [197] O\u0010\nHlog(N\u000f)\n\u000f\u0011\nO(Hlog (N\u000f))X\nMitzenmacher et al. [198] O\u0010\nH\n\u000f\u0011\nO(Hlog\u000fN)X\nﬂows from the table at once instead of removing the minimum\nﬂow upon the arrival of a non-resident ﬂow. However, IM-\nSUM [8] performs only a small portion of the maintenance\nprocedure to achieve a worst-case complexity of O(1).\nTable XI compares various heavy hitter detection methods\nin terms of their space overhead, updating time, and query\ntime. In the table, Hrepresents the maximum number of\nheavy hitters in a measurement epoch, nis the trafﬁc ﬂow\nkey domain, \u000f(0<\u000f<1)denotes the approximation parameter,\nand\u000e(0<\u000e<1)is the error probability.\n7) Hierarchical Heavy Hitters (HHH): The structure of\nIP addresses implies a preﬁx-based hierarchy. Network trafﬁc\ncharacteristics can be better presented by using ﬂow aggregates\nby their 5-tuple ﬁelds [193]. This strategy makes aggregation\nof ﬂows a powerful means for trafﬁc measurement and a valu-\nable component for anomaly detection to identify attacks and\nscans. Moreover, identifying pairs of source and destination\npreﬁxes or other attributes that give rise to a signiﬁcant amount\nof global trafﬁc refers to a novel measurement task, namely\nmultidimensional Hierarchical Heavy Hitters (mHHH) [194].\nSeparator [195] consists of Hstream-summary structures,\neach corresponding to a level of the domain tree in the\nHHH detection problem. It combines bottom-up and top-\ndown processing strategies to detect HHH. An incoming item\nupdates the ﬁrst-layer counters or higher-layer counters if there\nis no space available or no matched counters in the lower\nlayer. Randomized HHH [58] proposes a randomized constant-\ntime algorithm for HHH, which achieves a worst-case updatetime ofO(1)for each packet. Recursive Lattice Search [193]\nrevisits the commonly accepted deﬁnition of HHH and applies\nZ-ordering [199] to use a recursive partitioning algorithm.\nTheZ-order makes the ordering consistent with the ancestor-\ndescendant relationship in the hierarchy, and it transforms the\nHHH problem into simple space partitioning of a quadtree.\nTable XII compares the HHH detection methods according\nto their space consumption and updating time. Speciﬁcally,\nNrepresents the total number of packets, His the size of\nthe hierarchy, and \u000fdenotes the allowed relative estimation\nerror for every single ﬂow’s frequency. For TCAM-based HHH\n[196],Tis a threshold. The ﬂows that consume at least Tof\nthe link capacity in each interval can be deﬁned as heavy\nhitters, and space refers to the TCAM matching rules.\n8) Top-kdetection: Top-kﬂows detection is a critical task\nin network trafﬁc measurement, with many applications in\ncongestion control, anomaly detection, and trafﬁc engineer-\ning [118], including data mining, databases, network trafﬁc\nmeasurement, and security [120]. Table XIII summarizes the\nrelated methods and their basic ideas.\nSketch-based methods such as CM Sketch [27] and CSketch\n[83] record the packet number of all ﬂows. By augmenting\nan additional heap, these methods can report the top- kheavy\nﬂows. However, they also record the frequencies of mice. Such\ninformation is useless, detrimental to ﬁnding the top- kheavy\nﬂows, and requires additional memory usage [120].\nTo eliminate small ﬂows and add suspected large ﬂows,\nMG counter [123] performs O(k)operations to update k\n\nTABLE XIII: Top- kheavy detection\nTop-kMethods Basic idea\nSS [113] Evict the item with the minimal value\nFSS [142] Augment a sketch to ﬁlter items\nSSS [120] Insert the item according to scoring\nCSS [149] Require no pointer and use statistical memory\nWCSS [149] Point queries in O(1)under sliding model\nFrequent [112] Evict the item by decreasing all the items\nRAP [111] Randomized admission policy\nCountMax [115] Lightweight and cooperative measurement\nHeavyKeeper [118] Count with exponential decay\nHashPipe [137] Use emerging programmable (P4)\nDiscrete Tensor Completion [200] Data plane implementations\ncounters in a hash table; the overhead becomes signiﬁcant\nwhen there are many mice to be kicked out. Pandey et al.\n[201] show that by using modern storage devices and building\nupon on recent advances in external-memory dictionaries, MG\nleveled external-memory reporting tables (LERTs) can process\nmillions of stream events per second. Space Saving [113] and\nFrequent [112] both treat each incoming packet fas a sus-\npected hot item. They assign a high frequency to fand insert\nit into the top- kdata structure. Further, they gradually evict the\nmice from the top- kdata structure with a certain probability\nas time passes. However, in real trafﬁc traces, most items are\nmice, and processing these mice involves additional overhead\nand makes the ranking and frequency estimation of the top-\nkﬂows inaccurate. FSS citeDBLP:journals/isci/HomemC10\nuses a ﬁltering approach to improve Space Saving. It uses\na bitmap to ﬁlter and minimize updates on the top- kdata\nstructure and estimate the error associated with each element.\nSSS [120] adds a small queue and a Scoreboard to insert\nonly the elephants into Stream Summary instead of all the\nitems. The scoreboard is responsible for recording potential\nhot items to indicate whether an item is an elephant item.\nCSS [149] redesigns Space Saving and achieves a reduction\nin space overhead of up to 85%; it works at O(1)with\nhigh probability. Moreover, WCSS is the ﬁrst algorithm that\nsupports point queries with constant time complexity under the\nsliding window model. This property makes WCSS a practical\nchoice for matching the line rate.\nCountMax [115] and MV-sketch [116] adopt the majority\nvote algorithm ( MJRTY ) [117] to track the candidate heavy\nﬂow in each bucket. Moreover, it has loose bounds on the\nestimated values of the top- kﬂows. Compared with SSS\n[120], CountMax consumes only 1/3-1/2 computing over-\nhead and reduces the average estimation error by 20%–30%\nunder the same memory size. RAP [111] is a randomized\nadmission policy for allocating counters to non-measurement\nﬂows. More speciﬁcally, this strategy ignores most of the tail\nﬂows and can still admit the elephants eventually. Heavy-\nKeeper uses the strategy from HeavyGuardian [119], i.e.,\ncount with exponential decay . Speciﬁcally, HeavyKeeper\nonly keeps track of a small number of ﬂows. Mice entering\nthe data structure will decay away to make room for the\ntrue elephants. Discrete Tensor Completion [200] solves the\nchallenging problem of inferring the top- kelephants in anefﬁcient system with incomplete measurement data as a result\nof sub-sampling for scalability or missing data.\nHashPipe [137] is an algorithm for tracking the kheaviest\nﬂows with high accuracy within the features and constraints\nof programmable switches. HashPipe is heavily inspired by\nSpace Saving [113]. It maintains both the ﬂow key and the\nfrequency in a pipeline of hash tables to track heavy ﬂows by\nevicting lighter ﬂows from the switch memory over time.\n9) Entropy estimation: As a critical metrics, entropy has at-\ntracted considerable attention for network measurement [132].\nUsing the entropy of trafﬁc distributions, we can perform\na wide variety of network applications such as anomaly\ndetection, clustering to reveal interesting patterns, and trafﬁc\nclassiﬁcation [202]. IMP [132] is the ﬁrst method to measure\nthe entropy of the trafﬁc between every origin-destination pair\n(OD-pair ﬂows). Moreover, IMP can measure the entropy\nof the intersection of two data streams, AandB, from\nthe sketches of AandB, which are recorded from two\nmeasurement points. Lall et al. [202] contributed to the inves-\ntigation and application of streaming algorithms to compute\nthe entropy over network trafﬁc streams. They also identiﬁed\nthe appropriate estimator functions for calculating the entropy\naccurately and provided proof of approximation guarantees\nand resource usage. AMS-estimator [203] designs an algorithm\nfor approximating the empirical entropy of a stream of m\nvalues in a single pass, using O\u0000\n\u000f\u00002log\u0000\n\u000e\u00001\u0001\nlogm\u0001\nwords\nof space, which is near-optimal in terms of its dependence on\n\u000f, where\u000fis the approximation parameter and \u000erepresents\nthe error probability. Defeat [129] uses the entropy of the\nempirical distribution of each feature to detect unusual trafﬁc\npatterns, where the trafﬁc feature represents an entry in a\npacket header ﬁeld.\n10) Change detection: A sudden increase in network traf-\nﬁc owing to the emergence of elephant ﬂows is essential\nfor network provisioning, management, and security because\nsigniﬁcant patterns often imply events of interest [134]. For\nexample, the beginning of a DDoS attack or trafﬁc rerouting\nowing to link failures will cause trafﬁc to change heavily. A\nheavy changer is deﬁned as a ﬂow that contributes more than a\nthreshold of the total capacity over two consecutive intervals.\nTo detect the signiﬁcant changes in the two consecutive\ntime intervals, Fast Sketch [69], MV-sketch [116], LD-sketch\n[125], Modular Hashing [139], Deltoid [68], Reversible k-\n\nTABLE XIV: Heavy change detection\nHeavy changeMethods Memory Updating time Query time False positive False negative\nFast sketch [69] O\u0000\nklogn\nk\u0001\nO\u0000\nlogn\nk\u0001\nO\u0000\nklogn\nk\u0001\nX X\nMV-Sketch [116] \u0002(klogn) O(k) O\u0000\nk2\u0001\nX X\nGroup testing [205] O\u0000\nklogn\nk\u0001\nO(logn)O\u0000\nklogn\nk\u0001\nX\u0002\nDefeat [129] O\u0000\nn1=2\u0001\nO(1) O\u0000\nk2\u0001\nX\u0002\nSequential Hashing [134] \u0002(klogn) \u0002(logn)\u0002(klogn) X X\nReversiblek-ary sketch [204] \u0002\u0012\n(logn)\u0002(1)\nlog logn\u0013\n\u0002(logn)O\u0010\nkn1\nlog lognlog logn\u0011\nX X\nDeltoid [68] \u0002(klogn) \u0002(logn)O(klogn) X X\nLD-Sketch [125] \u0002\u0000\nk2logn\u0001\nO(k) O(k) X\u0002\nModular hashing [139] O\u0010\nn1\nlog lognlog logn\u0011\nO\u0010\nlogn\nlog logn\u0011\nO\u0010\nkn1\nlog lognlog logn\u0011\nX X\nary sketch [204], and Group testing [205] all derive the\ndifference sketch Sd=jS2\u0000S1j, whereS1andS2are the\nsketches recorded for the two consecutive time intervals.\nBecause of the linearity of sketches, the difference in ﬂows can\nbe estimated by subtraction. For any key whose value recorded\ninSdexceeds the threshold, we can conclude that this ﬂow\nis a suspected heavy changer and proposed as a set of heavy\nchange ﬂows. Furthermore, Deltoid [68] refers to a ﬂow as\ndeltoid if it has a large difference, including absolute ,relative ,\norvariational , instead of only absolute difference. MV-sketch\n[116] and LD-sketch [125] both record the ﬂow ID to achieve\nreversibility. Fast Sketch [69], Modular Hashing [139], and\nReversiblek-ary sketch [204] are all reversible sketch designs\nthat adopt different strategies to identify the heavy changer.\nThey separately record the volume information of bit or bits.\nBy identifying the heavy bit/bits changer, these methods can\nreverse the heavy change ﬂows.\nInterestingly, HyperSight [150] exploits packet behavior\nchanges in response to network incidents achieving both high\nmeasurement coverage and high accuracy. By applying a\nseries of Bloom ﬁlters, it checks whether the corresponding\npacket behaviors have been changed. Moreover, HyperSight\ndesigns a packet behavior query language to express multiple\nmeasurement queries based on the packet behavior change\nprimitive to support a wide range of network event queries.\nTable XIV summarizes existing heavy change detection\nmethods in terms of their memory overhead, updating time\ncomplexity, and query time complexity, where nrepresents the\nrange of ﬂow keys and kis an upper bound on the number of\nanomalous keys of interest. Moreover, we focus on the false\npositive and false negative errors involved in these methods.\n11) Burst detection: Paul et al. [206] deﬁne burst as the\nacceleration over the incoming rate of an event mentioning.\nBased on Persistent CM sketch (PCM) [207], they explore the\nefﬁciency and approximation quality tradeoff in the design of\ndata summaries. They further present PBE to ﬁnd burst events\nat any time for the entire history. Zhong et al. [208] refer\nto a sudden increase in terms of arrival rate followed by a\nsudden decrease as a burst. They propose BurstSketch ﬁrst to\nselect potential burst items using Running Track technique ef-\nﬁciently and then monitor the potential burst items and capture\nthe key features of burst pattern by Snapshotting technique.\nSpeciﬁcally, Runnning track can be most frequent elementsﬁnding strategies, such as frequent [146], probabilistic decay\n[119] and probabilistic replacement [111]. The idea behind\nthe Snapshotting is that a burst can be described as a sudden\nincrease and a sudden decrease. Compared with PBE [206],\nBurstSketch care more about real-time burst detection in high-\nspeed data streams.\n12) Persistent sketch: A persistent sketch, also known as a\nmulti-version data structure in the database literature, is a data\nstructure that preserves all its previous versions as it is updated\nover time [207]. By maintain such a data structure, network\noperators can perform historical window queries and historical\nqueries on the whole history data instead of just a measurement\nperiod. Wei et al. [207] introduce Piecewise Linear Approxi-\nmation into basic sketch structure to support historical window\nqueries and historical queries. They propose persistent CM\nSketch and persistent AMS Sketch, which leverage piecewise-\nlinear functions to approximate each counter of the sketch over\ntime. Persistent Bloom Filter [209] decomposes the time into\na binary representation of temporal range and maintains a BF\nfor each to support historical window queries and historical\nqueries.\n13) Item batch detection: Chen et al. [210] deﬁne item\nbatch as a consecutive sequence of identical items that are\nclose in time in a data stream. They argue that item batch is\na useful data stream pattern in the cache, burst detection, and\nAPT detection. They ﬁrst propose ClockSketch to delete the\nstale information as much as possible while maintaining all\nitems visited within the latest time window.\nB. Hardware or Software implementation\nAs the Internet enters the era of big network data, trafﬁc\nﬂow may be massive in some essential scenarios. Modern\nhigh-speed routers forward packets at the speed of hundreds\nof Gigabits or even hundreds of Terabits per second [89]. The\nnumber of trafﬁc ﬂows that traverse a core router may be of\nthe order of tens of millions. Moreover, the number of bytes\nper ﬂow can also be large. Simultaneous tracking of such a\nlarge number of ﬂows is a unique challenge for trafﬁc mea-\nsurement hardware and software. This subsection introduces\nsome methods that focus on the measurement implementations\nof hardware and software in the passive measurement area.\n1) Implementation in SRAM: To maintain high throughput,\nrouters forward packets from incoming ports to outgoing ports\n\nvia a switching fabric, bypassing the main memory and CPU.\nIf the measurement is implemented as an online module to\nprocess packets in real-time, one way is to implement it on\nnetwork processors at the incoming/outgoing ports and use\non-chip cache memory. However, the commonly used cache\non processor chips is SRAM, typically with limited hardware\nresources, which may have to be shared among multiple\nfunctions for routing, performance, and security purposes. In\nsuch a context, the amount of memory allocated for measure-\nment may be tiny. Several methods aim to leverage SRAM\nto accomplish measurement tasks. Most existing studies are\ndevoted toward excellent counter representation to ensure that\nthe counters occupy as little memory as possible to ﬁt into\nthe off-chip SRAM. Furthermore, several studies have been\nconducted to break the off-chip SRAM’s throughput bound to\nguarantee per-packet processing.\nSingle counter compression. Many studies have adopted\na single-counter-based compressed representation approach.\nSAC [85] and SA counter [84] both propose a method such as\nﬂoating-point number representation to enlarge the counting\nrange or compress the counters. SAC statically divides the\navailable memory of qbits in a unit into two parts Aand\nmode of sizelandk, respectively, and represents a counter\nasA\u00012r\u0001mode, while SA counter augments a ﬂag bit to\ndynamically adjust the mode bits to enlarge the counting range.\nSampling strategy. Several sampling methods have been\ndeveloped to reduce the processing overhead and improve\nthe estimation accuracy of the counter estimator. ANLS [55]\nproposes an estimator that dynamically adjusts the sampling\nrate for a ﬂow depending on the number of packets that\nhave been counted to improve the measurement accuracy for\nsmall ﬂows. DISCO [16] extends ANLS by regulating the\ncounter value to be a real increasing concave function of\nthe actual ﬂow length and supports both packet counting and\nbyte counting with high accuracy. Counter Estimation Decou-\npling for Approximate Rates (CEDAR) [211] decouples and\nshares pre-given estimation values by pointers. By sampling\nthe packets according to the current and following counters,\nCEDAR can dynamically update the pointers. ICE-Buckets\n[57] presents a closed-form representation of the optimal\nestimation function and introduces an independent bucket-\nbased counter architecture to improve the accuracy further.\nBit-sharing strategy. There are several shared-bit compres-\nsion approaches for reducing the wastage of high-order bits\nof cold items. ABC [104] uses the space from the adjacent\ncounters by operations such as bit borrowing and combination\nwhen a counter overﬂows. Counter Tree [93] shares the high-\norder bits of large virtual counters, which are constructed from\nmultiple physical counters organized in a tree structure and\nspan a path crossing multiple levels of the tree. OM sketch\n[95] shares the high-order bits by a hierarchical CSM sketch\n[109]. It can achieve close to one memory access and one\nhash computation for each insertion or query while achieving\nhigh accuracy. Pyramid Sketch [94] dynamically assigns an\nappropriate number of bits for different items with different\nfrequencies. It also employs a tree structure.Counter-sharing strategy. CSM sketch [109] and Counter\nBraids [24] are representative counter-sharing compression\napproaches. CSM sketch splits each ﬂow’s size among several\ncounters that are randomly selected from a counter pool.\nThis strategy is implemented by randomly selecting a counter\nfrom the ﬂow’s counter vector and increasing the counter\nby one. Counter Braids establishes a hierarchical counter\narchitecture by braiding the counter bits with random graphs to\nsolve the central problems (counter space and ﬂow-to-counter\nrule). These approaches achieve a reasonable compression rate;\nhowever, the decompression process is relatively slow.\nVirtual estimators. Some virtual estimators that facilitate\nmemory sharing for recent cardinality estimation solutions\nhave also been proposed. Virtual HyperLogLog [91] shares\na register array CofmHLL registers to store the packet\ninformation of all ﬂows. For an arbitrary ﬂow f, wherefis\nthe ﬂow label, it pseudo-randomly selects sregisters from Cto\nlogically form a virtual counter Cf, and useCfto encode the\npackets in ﬂow f. Virtual Register Sharing [89] allocates each\nﬂow with a virtual estimator, including PCSA [73], LogLog\n[74], and HyperLogLog [75], and these virtual estimators share\na common memory space. It demonstrates that sharing at the\nmulti-bit register level is superior to sharing at the bit level.\nApart from the above-mentioned methods, some other\nstrategies have been developed to solve the throughput bound\nproblem by exploiting the speed of SRAM. To overcome\nthe bottleneck, Cache-assisted Stretchable Estimator (CASE)\n[212] uses the on-chip memory as the fast cache of the off-\nchip SRAM. It counts hot items in the on-chip memory and\nwrites back to the off-chip memory according to the cache’s\nestimation. Because of the heavy-tailed distribution of trafﬁc,\nmost accesses to the counters will occur in the cache. The\nthroughput can be increased remarkably while the accuracy\nwill be improved due to the non-compression counting.\n2) Implementation in DRAM: The prevailing view is that\nSRAM is too expensive to implement large counter arrays,\nwhereas DRAM is too slow to provide line speed updates. This\nview is the main premise of a number of hybrid SRAM/DRAM\narchitectural proposals [213] [214] [215] [216] that still require\nsubstantial amounts of SRAM for large arrays. However, some\nstudies [217] [60] [218] [219] have presented a contrarian\nview that modern commodity DRAM architectures, driven by\naggressive performance roadmaps for consumer applications,\nhave advanced architectural features that can be exploited to\nmake DRAM solutions practical.\nLin and Xu [217] ﬁrst highlighted the potential of leveraging\nDRAM for per-ﬂow network measurement. They proposed\ntwo schemes, one based on the replication of counters across\nmemory banks and the other based on the randomized dis-\ntribution of counters across memory banks to maintain wire-\nspeed updates to large counter arrays. Compared with existing\nhybrid SRAM/DRAM counter architectures, this strategy can\nachieve the same update rates to counters without needing\na nontrivial amount of SRAM for storing partial counts.\nFurther, they proposed a randomized DRAM architecture in\n[60] [218], which harnesses the performance of modern com-\n\nmodity DRAM architectures by interleaving counter updates\nin multiple memory banks. The architecture consists of a\nsimple randomization scheme, a small cache, and small request\nqueues to statistically guarantee a near-perfect load-balancing\nof counters to the DRAM banks.\n3) Implementation in hybrid SRAM-DRAM: Statistics\ncounters are the fundamental unit of sketch-based trafﬁc\nmeasurement. Maintaining these counters at high line speed\nis a challenging research problem. Brieﬂy, two requirements\nmust be met to solve the per-ﬂow measurement problem [85]:\n(1) to store a large number of counters and (2) to update\na large number of counters per second. Although chip and\nlarge DRAM can easily satisfy these requirements, DRAM\naccess times of 50-100 ns cannot accommodate many counter-\nupdates. Meanwhile, expensive SRAM with access times of\n2-6 ns allows many more counter increments per second;\nhowever, is not economical for many counters.\nA hybrid SRAM/DRAM structure can help store the mas-\nsive number of counters to solve this problem. Several methods\nhave been developed to use DRAM to maintain statistics\ncounters with a small, ﬁxed amount of SRAM. DRAM main-\ntains Ncounters of Mbits, while SRAM stores Ncounters\nofmbits, where ( m<M ). The SRAM counters will handle\nincrements at very high speed, and once an SRAM counter\nreaches a value of 2m(overﬂow), its value must be ﬂushed to\nits corresponding DRAM counter. Further, the access speed of\nDRAM (departure rate from the queue) should be higher than\nthe average rate of counter overﬂow (arrival rate to the queue).\nSeveral counter management algorithms (CMAs) have been\nproposed to manage updates from SRAM to DRAM while\nensuring correct line-rate operations for many counters.\nLCF (Largest Counter First) [213] employs a heap-based\npriority queue to select the counter that is the closest to\noverﬂow. This greedy algorithm makes the best possible se-\nlection decision (without future knowledge). LCF is provably\noptimal in terms of the amount of SRAM required. However,\nmaintaining a heap in hardware involves high implementation\ncomplexity and a large amount of SRAM, i.e., around twice\nthe amount required to store the SRAM counters. LR( T)\n(Largest Recent with threshold T) [214] avoids the expensive\noperation of maintaining a priority queue by keeping track of\nonly those counters that are larger than a threshold Tusing a\nbitmap. A tree structure is imposed on the bitmap (resulting\nin a hierarchical bitmap) to allow for fast retrieval of the next\ncounter to be ﬂushed . This retrieval operation has a complexity\nofO(logN ), whereNis the number of counters in the array;\nhowever, using a large base such as 8 makes the complexity\nessentially a small constant. The hardware control logic in\nLR(T) is more straightforward than that in LCF and uses a\nmuch smaller amount of SRAM.\nZhao et al. [216] removed the fundamental design objective\nof previous approaches that do not allow SRAM-counter\noverﬂows and achieved a theoretically minimum SRAM cost\nof 4-5 bits per counter when the SRAM-to-DRAM access\nlatency ratio is between 1/10 (4 ns/40 ns) and 1/20 (3 ns/60 ns).\nThey augmented a small SRAM FIFO buffer between SRAMcounters and DRAM counters to temporarily hold the SRAM\ncounters’ indices that have overﬂowed and must be made to\nthe DRAM in the future. A simple randomized algorithm also\nguarantees that SRAM counters do not overﬂow in bursts that\nare sufﬁciently large to ﬁll the write-back buffer even in the\nworst case. This approach consumes signiﬁcantly fewer bits in\nSRAM and proposes a simple CMA scheme compared with a\nheap in LCF [213] and tree-like structure in LR( T) [214].\nRoeder et al. [215] extended LCF [213] and LR( T) [214]\nwith multilevel counter memory architecture to reduce the\namount of fast memory required for nonuniform trafﬁc pat-\nterns. They observed that many counters in previous methods\nnever reach a counter close to overﬂow; hence, using several\nSRAM levels to store the lower bits of every counter may be a\nbetter strategy. Whenever a counter overﬂows, i.e., exceeds the\nmaximum value, it will update the corresponding counters in\nthe next level. Comprehensive experiments have demonstrated\nthat multi-level counter memory architecture can reduce the\nrequired amount of fast memory storage by as much as 28 %.\nLall et al. [220] propose a hybrid SRAM/DRAM algorithm\nfor measuring all elephant and medium-sized ﬂows with strong\naccuracy guarantees. They employed a spectral Bloom ﬁlter\nin SRAM to ﬁlter out small ﬂows and preferentially sample\nmedium and large ﬂows to a DRAM ﬂow table. The sketch\nautomatically eliminates the burden of having to perform\nexpensive updates for tens of millions of small ﬂows typically\nfound in network trafﬁc. Once the ﬂow is identiﬁed as greater\nthan the threshold, it creates a record for it in the ﬂow table\nand samples it by SGS [53] thereafter. In all these counter\narray schemes [213] [215] [216], the read operations can\nbe performed only at DRAM speed. To solve reading being\ntoo slow , a few SRAM bits are augmented per counter to\napproximate the value range of the actual counter.\nHAC [85] uses augmented DRAM for storing informa-\ntion on recent history. The idea is that the primary counter\ninformation is stored in SRAM while DRAM is used to\nstore information only from the recent history to reduce the\nstandard error. For every \u0011SRAM access, there can be one\nDRAM access. The counters in DRAM will be updated with\nprobability1\n\u0011and the counters in SRAM will be increased\nwhen the counters in DRAM overﬂow. This strategy allows an\nestimate of full counters on a per-packet basis without DRAM\naccess. HAC does not have an exact counter value compared\nto the other counter architectures but stores only its estimate.\nAll these methods are designed for hybrid SRAM/DRAM\narchitectures. LCF [213], LR( T) [214], and BCMA [216] are\nCMA designs for managing updates from SRAM to DRAM\nfor per-ﬂow measurement. Whereas [220] leverages SRAM to\nﬁlter the cold items and only measures elephant and medium-\nsized ﬂows, HAC [85] solves the problem of reading being too\nslow by switching the role of SRAM and DRAM and designs\na compressed representation of counts in SRAM.\n4) Implementation in FPGA: FPGAs can be programmed\nfor fast pattern matching because of their ability to exploit\nreconﬁgurable hardware and parallelism. Counter Braids [24],\nHeavyKeeper [118], Reversible Sketch [139], CEDAR [211],\n\nand elastic sketch [158] have developed their parallel versions\nfor FPGA hardware. Because methods related to FPGA mainly\ndeal with program coding, we omit those details.\n5) Implementation in TCAM: Ternary Content Addressable\nMemory (TCAM) is a type of memory that can perform\nparallel search at high speeds. A TCAM consists of a set of\nentries. The top entry of the TCAM has the smallest index,\nwhereas the bottom entry has the largest index. Each entry\nis a bit vector of cells, where every cell can store one bit. A\nTCAM entry can be used to match the incoming packet header.\nTCAM-based ﬂow tables in commodity switches can contain\nstatistic counters for trafﬁc measurement. However, owing to\nthe cost and power consumption of TCAM, the TCAM-based\nﬂow table size is limited [73].\nBandi et al. [221] ﬁrst showed how the integrated architec-\nture of the Network Processing Unit (NPU) and TCAM could\nbe used to develop high-speed stream summarization solutions.\nThey discussed the problems that occur when Lossy Counting\n[191] and Space Saving [113] are adapted to the TCAM\nmodel for ﬁnding the most common elements in a data stream.\nCompared to their software-implemented counterparts, these\ntechniques involve a constant and smaller TCAM operations\nper ﬂow. When performing HHH detection tasks, because the\ntrafﬁc keeps changing, we do not know the HHHs in advance,\nand there are not good rules to measure all the preﬁxes.\nTCAM-based HHH [196] proposes three approaches to reduce\nthe amount of TCAM required.\nConsidering that the TCAM-based ﬂow table is always\nembedded into OpenFlow switches [222], such as HP5406zl\nswitches [6] and Broadcom Trident switches [223], the meth-\nods related to TCAM are implemented conducted in the\ncontext of SDN; we will introduce the related method in the\nfollowing SDN subsection.\n6) Implementation in Programmable Switch: P4 [224] is\na language that speciﬁes how switches process packets. It\nprovides a register abstraction that offers a form of stateful\nmemory that can store user-deﬁned data structures and be\narranged into one-dimensional arrays of user-deﬁned length.\nRegister cells can be read or updated by P4 actions and are\naccessible through the control plane API. Actions are installed\nin match-action tables to describe speciﬁc processing logic,\nwhere each action is associated with a user-deﬁned matching\nrule. The P4 framework allows users to deﬁne the API for\ncontrol-data plane communication.\nUnivmon [140] develops a network-wide coordination\nschemes and demonstrates an implementation in P4. It is pro-\ncessed by sketching, storage, and a sampling table sequentially\nuntil the ﬁrst level, where it does not get sampled for every\npacket. The control plane then uses a simple RPC protocol to\nimport sketching manifests and query the contents of the data\nplane register arrays. SketchLearn [22] implements each level\nof counters as an array of registers. Furthermore, the counters\nare updated by a stateful ALU of hash computation action in\na dedicated table. A multi-thread approach is used for model\nlearning and query runtime in the data plane to form the model\nand compute trafﬁc statistics. FCM-Sketch [225] designs a\nMeasurement tasks deployment Control plane\nData planeFig. 13: SDN measurement architecture\nlightweight implementation of CM Sketch in emerging PISA\nprogrammable switch. By collecting data in the data-plane,\nFCM-Sketch enables complex measurement tasks in the con-\ntrol plane. Furthermore, Elastic Sketch [158], HashPipe [137],\nFlowRadar [127], HyperSight [150], HeavyKeeper [118], and\nMarple [179] have been deployed on P4Switch and validated\ntheir performance.\n7) Implementation in Open vSwitch: Open vSwitch [226] is\na production quality, multi-layer virtual switch licensed under\nthe open-source Apache 2.0 license. It is designed to allow\nmassive network automation through programmatic extension\nwhile supporting standard management interfaces and proto-\ncols (e.g., NetFlow, sFlow). The architectural design of OVS\n[227] consists of two main components: ovs-vswitchd and\novsdb-server. The actions given by ovs-vswitchd will instruct\ndatapath to handle packets. The DPDK version allows OVS\nto perform the entire packet processing pipeline in the user\nspace, thereby reducing the overhead of context switches and\ncopied data between the user and kernel spaces. The vswitchd\nthread has a three-tier look-up cache hierarchy. The ﬁrst-level\nworks as an Exact Match Cache (EMC) and has the highest\nlookup speed. The second-level works as a Tuple Space Search\nand may trigger the third level managed by an OpenFlow-\ncompliant controller. BUS [228] empowers the OVS project\nwith sampling capabilities and implements such capabilities\nfor both physical and virtual ports. BUS implementation rede-\nﬁnes a forwarding action as a branch of the datapath. Sampled\npackets are forwarded and unsampled packets are dropped.\nDREAM [25], FlowRadar [127], CounterMap [229], CO-FCS\n[230], Rﬂow [231], R-HHH [58], CountMax [115], NSPA\n[46], and NitroSketch [59] implemented their framework in\nOVS to evaluate their performance.\n8) Implementation in SDN: Software network devices are\ngaining popularity owing to the rise of two new network\narchitecture concepts: Software-Deﬁned Networking (SDN)\nand Network Function Virtualization (NFV). SDN facilitates\nnetwork management by separating the control and data\nplanes. Network measurement in SDN is a lightweight task,\nas operators only need to install a measurement module\ninto the controller. Moreover, the SDN controller conﬁgures\n\nswitches to measurement trafﬁc for measurement tasks, col-\nlects statistics, produces a measurement report, and exhibits\nthe architecture shown in Fig. 13.\nMeasurement framework. OpenSketch [167] is designed to\nmake sketch-based measurements on network devices more\ngeneral and ﬂexible by using a customized data plane and\ncontrol plane. The OpenSketch data plane relies on hashing\nand wildcard rules, which are realized by the classiﬁcation\nfunction of TCAM, to choose a speciﬁc ﬂow to be collected\nand measured. Its control plane consists of a measurement\nlibrary (CM sketch, reversible sketch, etc.) and applications.\nCurrently, OpenFlow-based SDN is widely used in both\nacademia and industry. OpenFlow is the de facto standard\ncommunication interface between the control and data planes.\nFurthermore, based on the packet behavior change primitives,\nHyperSight [150] can reconcile both the measurement cov-\nerage scalabilities. Based on the coupon collector problem ,\nBeauCoup [232] supports multiple distinct counting queries\nwith at most one memory access per packet. Compared with\nsketch-based or sampling-based solutions, BeauCoup achieves\nthe same accuracy while using 4 \u0002fewer memory accesses.\nCollaborative measurement. CO-FCS [230] ﬁrst highlights\ndifferent schemes for ﬂow statistics collection (FCS) in SDN.\nIt argues that both per-ﬂow collection and per-switch collection\nmight lead to massive costs in terms of the control plane\nbandwidth and long processing delay on switches in dynamic\nnetworks. This method implements the fast and selective FCS\non a switch using the wildcard-based FCS requests to avoid\nlong-delay collection on some switches and massive trafﬁc\nload on control links. FlowCover [15] leverages the global\nview of the network topology and active ﬂows to minimize the\ncommunication cost by formulating the problem as a weighted\nset cover, which is proved to be NP-hard. Heuristics have\nbeen presented to obtain the polling scheme efﬁciently and\nhandle ﬂow changes practically. OmniMon [233] carefully\ncoordinates the collaboration among different types of entities\nin the network to execute telemetry operations. The resource\nconstraints of each entity are satisﬁed without compromising\naccuracy. Furthermore, from a global perspective, the con-\ntroller can allocate measurement tasks to a series of switches;\nthe related method is detailed in Section III-A.\nResource allocation strategy. DREAM [25] is a system for\na TCAM-based software-deﬁned measurement framework. It\nfocuses on ﬂow-based counters in TCAM and dynamically\nallocates TCAM resources to multiple measurement tasks\nin order to achieve their given accuracy bounds. DREAM\ndevelops accuracy estimators for TCAM-based zoom in/out\nalgorithms, and comprehensive evaluations have shown that\nDREAM is better than simple task-type agnostic schemes such\nas equal TCAM allocation. By contrast, SCREAM [26] ex-\nplores sketch resource allocation for measurement and allows\ndynamic resource allocation for many concurrent measurement\ntasks while achieving the desired accuracy for each task.\nCentralization/costs tradeoff. DevoFlow [6] explores the\ncentralization/costs tradeoffs in terms of the hardware lim-\nitations in OpenFlow. The authors argued that full controland visibility over all ﬂows are not the best choices. The\ncontroller should focus on signiﬁcant ﬂows and have visibility\nover these ﬂows and packet samples. In comparison, the\nother ﬂows should be maintained in switches as much as\npossible. DevoFlow is designed to allow aggressive use of\nthe wild-carded OpenFlow rule in order to reduce the number\nof switch-controller interactions and the number of TCAM\nentries through new mechanisms to detect signiﬁcant ﬂows.\nAccuracy/resource tradeoff. Moshref et al. [234] ﬁrst quali-\ntatively exploited the resource/accuracy tradeoffs for measure-\nment primitives in SDN. They argued that different primitives\nneed different amounts of resources. Counters occupy precious\nTCAM memory; hash-based data structures need SRAM,\nwhile code fragments require CPU for processing. These\napproaches differ in terms of the amount of network bandwidth\nthat they require to communicate intermediate results with the\nSDN controller. They present a qualitative understanding of\nthese primitives, resource usage, and accuracy as a function\nof the spatial and temporal granularity of the measurement.\nOblivSketch [235] implements the sketch-based measurement\nas a full-ﬂedge service integrated with the off-the-shelf SDN\nframework to prevent the ﬂow statistics leakage and abuse. By\nusing Intel SGX, OblivSketch employs hardware enclave for\nsecurity network statistics generation and queries.\nAccuracy/computation tradeoff. Alipourfard et al. [23]\n[236] showed that saving memory through extra computation\nis not worthwhile in terms of achieving high performance\nand high accuracy in software. They argued that, in software,\nthe key metric is not memory usage but packet processing\nperformance (i.e., throughput and latency). Complex design\nfor computing the location of counters may delay the packet\nprocessing and affect the throughput. A linear hash table\nand count array outperform complex data structures such as\nCuckoo hashing, CM sketches, and heaps in various scenarios.\nC. Summary and insights\nIn this section, we reviewed the corresponding research\nrelated to measurement tasks and sketch implementation.\nWe surveyed 10measurement tasks in terms of time and\nvolume and presented a comprehensive comparison of all\nthe measurement tasks’ performance and resource overhead.\nFurthermore, we reviewed the related studies on sketch hard-\nware and software implementation. From traditional hardware\nto programmable switches, the functionalities for network\nmeasurement have been enhanced considerably. Furthermore,\nnew problems have emerged, such as coordinating multi-\nple monitors, resource competition with other core network\nfunctions, and compatibility across different platforms. In the\nfuture, with the emergence of more ﬂexible and novel platform\ndesigns, sketch-based measurement technology must embrace\nthese changes and evolve as the network itself.\nVII. O PEN ISSUES\nBased on review of existing works on the design, optimiza-\ntion, application, and implementation of sketches, we now\noutline some open issues that may be investigated in the future.\n\nMore adaptive to trafﬁc characteristics. Some methods\nhave considered the characteristic of trafﬁc, such as the ﬂow\nsize distribution. However, as shown in Fig. 2, these assump-\ntions are not always correct. There may exist distributions of\ntwo different parameters Zip in one trafﬁc trace. Moreover,\nmost of these methods are designed based on the assumption\nof static parameters; hence, the designed sketch may not be\naccurate when applied to the dynamic network. Elastic sketch\n[158] is designed to be adaptive to the ﬂow size distribution.\nHowever, it only focuses on the heavy ﬂow part and proposes\na dynamical adjustment of the heavy part. In the future, it will\nbe necessary to design a more ﬂexible sketch design that is\nmore adaptive to various situations of skewed trafﬁc traces.\nMore adaptive to the evolution of the network. With\nthe increasing deployment of the IPv6 protocol, the network\nhas emerged as double-stack wherein IPv4 and IPv6 are\nimplemented. IPv6 was invented and put into practice to\nresolve the problem of IP address shortage by offering 2128\nIP addresses. Despite the consistently increasing use of IPv6,\nIPv4 still dominates the network. Due to the investment and\ncompatibility, adoption of emerging technologies such as NAT\nand DHCP, the double-stack circumstance is expected to last\nfor a long time. D-Sketch [237] proposes that IPv4 and\nIPv6 ﬂows are inequivalent in terms of both ﬂow cardinality\nand ﬂow size. Besides, the bit lengths of IPv4 and IPv6\nare different. Therefore, invertible sketch design will not be\napplicable. It is necessary to design a new strategy to make\nthe sketch more adaptive to double-stack networks. Moreover,\nwith the development of SDN [238], NFV [239] and SDN-\nNV [240] [241], the new technologies have emerged in new\nareas, such as 5G [242] and SD-WAN. The novel sketch-based\nmeasurement design is expected in the corresponding areas.\nMore adaptive to SDN and programmable switch. SDN\nensures the separation of the control unit from the underlying\nrouters and switches and introduces programmability into the\nnetwork. How to coordinate the network management and\nmeasurement task together under SDN is crucial for the\nnetwork’s performance. Many network functions are moving\nfrom hardware to software to achieve better programmability\nat a lower cost. Network policy is deﬁned in the control plane;\nthe control plane enforces the policy while the data plane exe-\ncutes it by forwarding data accordingly [243]. Meanwhile, the\ncontrol plane is responsible for managing the management and\nmonitoring logic and then allocating this logic into data plane\nimplementation, whereas the data plane corresponds to the net-\nworking devices responsible for (efﬁciently) forwarding data.\nThe control plane represents the protocols and measurement\nlogic used to populate the data plane elements’ forwarding\ntables. Moreover, TCAM-based ﬂow tables are also used to\nprovide measurement statistics information; however, owing\nto the limited TCAM entries and high power consumption,\nthe counters can only provide aggregate statistics of matching\nﬂows. To achieve ﬁne-grained measurement, researchers have\nalso proposed performing the measurement task in SRAM.\nHowever, the SRAM size in the programmable switch is\nlimited, and the space resources must also be shared withall the online network functions for routing, management,\nperformance, and security purposes. How to keep up with\nthe online measurement and maintain the network function\nstability are also important.\nEnforce with ML. There are two aspects in which we\ncan make the sketch more efﬁcient and accurate. First, as\nreported in [154], hash maps typically use a hash function\nto deterministically map keys to positions inside an array,\nwhich is essential to point query in the sketch data structure.\nBy contrast, the idea of learning a model to replace the hash\nfunction in order to reduce the conﬂict may be a better choice.\nThe core idea is to avoid too many distinct keys from being\nmapped into the same position inside the hash map. Second,\nduring the information extraction phase, the traditional sketch\nalgorithm uses the basic counter unit to obtain statistical\ninformation by tracking the entire data structure. According\nto [165], combining sketches with ML extraction technology\ncan make measurement tasks more accurate. Moreover, in the\nfuture, researchers may focus on adopting ML to solve some\ntasks, such as frequency estimation and anomaly detection.\nVIII. C ONCLUSION\nSketches have been widely employed in trafﬁc measurement\nbecause of their high space efﬁciency. As stated in this survey,\nsketches can act as basic data synopses to record and retrieve\nstatistical trafﬁc information. This survey reviews existing\nsketch variants from two main aspects, i.e., data processing\npipeline and deployment. From the aspect of data processing,\nwe survey the preparation for sketch-based network measure-\nment, design and optimization for the sketch data structure\nitself, and optimization in the post-processing stage. This\nperspective provides us with a complete view from processing\nthe raw packet trafﬁc to statistical information useful for\nnetwork management. Furthermore, we survey existing sketch\napplications and implements them for network measurement.\nIn addition, we comprehensively compare sketch-based net-\nwork measurement tasks, including time and volume. We\nalso discuss the corresponding implementation in hardware,\nsoftware, and SDN. Finally, we summarize the open issues for\nfuture work in the ﬁeld of sketch-based network measurement.\nWe anticipate developing further applications and redesigning\nsketches in the next generation of networks, communication\nsystems, and beyond.\nREFERENCES\n[1] M. Al-Fares, S. Radhakrishnan, B. Raghavan, N. Huang, and A. Vahdat,\n“Hedera: Dynamic ﬂow scheduling for data center networks,” in Proc.\nof USENIX NSDI, April 28-30, 2010, San Jose, CA, USA , pp. 281–296.\n[2] A. Kabbani, M. Alizadeh, M. Yasuda, R. Pan, and B. Prabhakar, “AF-\nQCN: approximate fairness with quantized congestion notiﬁcation for\nmulti-tenanted data centers,” in Proc. of IEEE HOTI, August 18-20,\n2010, Google Campus, Mountain View, California, USA , pp. 58–65.\n[3] P. Garcia-Teodoro, J. E. D ´ıaz-Verdejo, G. Maci ´a-Fern ´andez, and\nE. V ´azquez, “Anomaly-based network intrusion detection: Techniques,\nsystems and challenges,” Comput. Secur. , vol. 28, no. 1-2, pp. 18–28,\n2009.\n[4] G. Einziger, R. Friedman, and B. Manes, “Tinylfu: A highly efﬁcient\ncache admission policy,” TOS, vol. 13, no. 4, pp. 35:1–35:31, 2017.\n\n[5] V . Vasudevan, A. Phanishayee, H. Shah, E. Krevat, D. G. Andersen,\nG. R. Ganger, G. A. Gibson, and B. Mueller, “Safe and effective ﬁne-\ngrained TCP retransmissions for datacenter communication,” in Proc.\nof the ACM SIGCOMM, August 16-21, 2009, Barcelona, Spain , pp.\n303–314.\n[6] A. R. Curtis, J. C. Mogul, J. Tourrilhes, P. Yalagandula, P. Sharma,\nand S. Banerjee, “Devoﬂow: scaling ﬂow management for high-\nperformance networks,” in Proc. of ACM SIGCOMM, August 15-19,\n2011, Toronto, ON, Canada , pp. 254–265.\n[7] J. Sommers, P. Barford, N. G. Dufﬁeld, and A. Ron, “Accurate and ef-\nﬁcient SLA compliance monitoring,” in Proc. of the ACM SIGCOMM,\nAugust 27-31, 2007, Kyoto, Japan , pp. 109–120.\n[8] R. Ben-Basat, G. Einziger, R. Friedman, and Y . Kassner, “Optimal\nelephant ﬂow detection,” in Proc. of IEEE INFOCOM, May 1-4, 2017,\nAtlanta, GA, USA , pp. 1–9.\n[9] G. Cormode and M. Hadjieleftheriou, “Methods for ﬁnding frequent\nitems in data streams,” VLDB J. , vol. 19, no. 1, pp. 3–20, 2010.\n[10] G. Cormode, “Sketch techniques for approximate query processing,”\ninSynposes for Approximate Query Processing: Samples, Histograms,\nWavelets and Sketches, Foundations and Trends in Databases. NOW\npublishers , 2011.\n[11] Z. AP, C. G, and G. XJ, “High-speed network trafﬁc measurement\nmethod,” Ruan Jian Xue Bao/Journal of Software , vol. 25, no. 1, pp.\n135–153, 2014.\n[12] A. Yassine, H. Rahimi, and S. Shirmohammadi, “Software deﬁned\nnetwork trafﬁc measurement: Current trends and challenges,” IEEE\nInstrum. Meas. Mag. , vol. 18, no. 2, pp. 42–50, 2015.\n[13] P. B. Gibbons, “Distinct-values estimation over data streams,” in Data\nStream Management - Processing High-Speed Data Streams , 2016, pp.\n121–147.\n[14] Q. Yan, F. R. Yu, Q. Gong, and J. Li, “Software-deﬁned networking\n(SDN) and distributed denial of service (ddos) attacks in cloud com-\nputing environments: A survey, some research issues, and challenges,”\nIEEE Commun. Surv. Tutorials , vol. 18, no. 1, pp. 602–622, 2016.\n[15] Z. Su, T. Wang, Y . Xia, and M. Hamdi, “Flowcover: Low-cost ﬂow\nmonitoring scheme in software deﬁned networks,” in Proc. of IEEE\nGLOBECOM, December 8-12, 2014, Austin, TX, USA , pp. 1956–1961.\n[16] C. Hu, B. Liu, H. Zhao, K. Chen, Y . Chen, Y . Cheng, and H. Wu,\n“Discount counting for fast ﬂow statistics on ﬂow size and ﬂow\nvolume,” IEEE/ACM Trans. Netw. , vol. 22, no. 3, pp. 970–981, 2014.\n[17] B. Forouzanwithsophiachungfegan, TCP/IP PROTOCOL SUITE , 2003.\n[18] MAWILab, “Mawi working group trafﬁc archive,” http://mawi.wide.\nad.jp/mawi/, 2020 April, (Accessed on 04/24/2020).\n[19] Q. Xiao, Y . Qiao, Z. Mo, and S. Chen, “Estimating the persistent\nspreads in high-speed networks,” in Proc. of IEEE ICNP, October 21-\n24, 2014, Raleigh, NC, USA , pp. 131–142.\n[20] D. M. W. Powers, “Applications and explanations of zipf’s law,”\ninProc. of ACL NeMLaP/CoNLL, January 11-17, 1998, Macquarie\nUniversity, Sydney, NSW, Australia , pp. 151–160.\n[21] Z. De-fan and H. Chuan-feng, “The power law of social and eco-\nnomic indexes based on languages on the web,” in Proc. of Second\nInternational Conference on Intelligent Computation Technology and\nAutomation , 2009.\n[22] Q. Huang, P. P. C. Lee, and Y . Bao, “Sketchlearn: relieving user burdens\nin approximate measurement with automated statistical inference,” in\nProc. of ACM SIGCOMM, August 20-25, 2018, Budapest, Hungary ,\nS. Gorinsky and J. Tapolcai, Eds., pp. 576–590.\n[23] O. Alipourfard, M. Moshref, Y . Zhou, T. Yang, and M. Yu, “A\ncomparison of performance and accuracy of measurement algorithms\nin software,” in Proc. of ACM SOSR, March 28-29, 2018, Los Angeles,\nCA, USA , pp. 18:1–18:14.\n[24] Y . Lu, A. Montanari, B. Prabhakar, S. Dharmapurikar, and A. Kabbani,\n“Counter braids: a novel counter architecture for per-ﬂow measure-\nment,” in Proc. of ACM SIGMETRICS, June 2-6, 2008, Annapolis,\nMD, USA , pp. 121–132.\n[25] M. Moshref, M. Yu, R. Govindan, and A. Vahdat, “DREAM: dynamic\nresource allocation for software-deﬁned measurement,” in Proc. of\nACM SIGCOMM, August 17-22, 2014, Chicago, IL, USA , pp. 419–\n430.\n[26] M. Moshref, M. Yu, R. Govindan, and A. Vahdat, “SCREAM: sketch\nresource allocation for software-deﬁned measurement,” in Proc. of\nACM CoNEXT, December 1-4, 2015, Heidelberg, Germany , pp. 14:1–\n14:13.[27] G. Cormode and S. Muthukrishnan, “An improved data stream sum-\nmary: The count-min sketch and its applications,” in Proc. of Springer\nLATIN, April 5-8, 2004, Buenos Aires, Argentina, , ser. Lecture Notes\nin Computer Science, M. Farach-Colton, Ed., vol. 2976, pp. 29–38.\n[28] L. Bhuvanagiri and S. Ganguly, “Estimating entropy over data streams,”\ninProc. of Springer Algorithms - ESA, September 11-13, 2006 , ser.\nLecture Notes in Computer Science, Y . Azar and T. Erlebach, Eds.,\nvol. 4168, pp. 148–159.\n[29] F. Rusu and A. Dobra, “Statistical analysis of sketch estimators,” in\nProc. of the ACM SIGMOD, June 12-14, 2007, Beijing, China , pp.\n187–198.\n[30] M. Roughan and Y . Zhang, “Secure distributed data-mining and its\napplication to large-scale network measurements,” Computer Commu-\nnication Review , vol. 36, no. 1, pp. 7–14, 2006.\n[31] T. Sarl ´os, A. A. Bencz ´ur, K. Csalog ´any, D. Fogaras, and B. R ´acz, “To\nrandomize or not to randomize: space optimal summaries for hyperlink\nanalysis,” in Proc. ACM WWW, May 23-26, 2006, Edinburgh, Scotland,\nUK, pp. 297–306.\n[32] G. Cormode, M. N. Garofalakis, P. J. Haas, and C. Jermaine, “Synopses\nfor massive data: Samples, histograms, wavelets, sketches,” Found.\nTrends Databases , vol. 4, no. 1-3, pp. 1–294, 2012.\n[33] K. Suh, Y . Guo, J. F. Kurose, and D. F. Towsley, “Locating network\nmonitors: complexity, heuristics, and coverage,” in Proc. of IEEEIN-\nFOCOM, 13-17 March 2005, Miami, FL, USA , pp. 351–361.\n[34] M. R. Sharma and J. W. Byers, “Scalable coordination techniques for\ndistributed network monitoring,” in Proc. of Springer PAM, March 31\n- April 1, 2005, Boston, MA, USA , ser. Lecture Notes in Computer\nScience, vol. 3431, pp. 349–352.\n[35] G. R. Cantieni, G. Iannaccone, C. Barakat, C. Diot, and P. Thiran,\n“Reformulating the monitor placement problem: optimal network-wide\nsampling,” in Proc. of ACM CoNEXT, December 4-7, 2006, Lisboa,\nPortugal , p. 5.\n[36] A. Tootoonchian, M. Ghobadi, and Y . Ganjali, “Opentm: Trafﬁc matrix\nestimator for openﬂow networks,” in Proc. of Springer PAM, April 7-\n9, 2010, Zurich, Switzerland , ser. Lecture Notes in Computer Science,\nA. Krishnamurthy and B. Plattner, Eds., vol. 6032, pp. 201–210.\n[37] G. Huang, C. Chang, C. Chuah, and B. Lin, “Measurement-aware mon-\nitor placement and routing: A joint optimization approach for network-\nwide measurements,” IEEE Trans. Network and Service Management ,\nvol. 9, no. 1, pp. 48–59, 2012.\n[38] S. Raza, G. Huang, C. Chuah, S. Seetharaman, and J. P. Singh,\n“Measurouting: A framework for routing assisted trafﬁc monitoring,”\nIEEE/ACM Trans. Netw. , vol. 20, no. 1, pp. 45–56, 2012.\n[39] Y . Zhang, “An adaptive ﬂow counting method for anomaly detection\nin SDN,” in Proc. of ACM CoNEXT, December 9-12, 2013, Santa\nBarbara, CA, USA , K. C. Almeroth, L. Mathy, K. Papagiannaki, and\nV . Misra, Eds., pp. 25–30.\n[40] C. Chang, G. Huang, B. Lin, and C. Chuah, “LEISURE: A framework\nfor load-balanced network-wide trafﬁc measurement,” in Proc. of\nACM/IEEE ANCS, October 3-4, 2011, Brooklyn, NY, USA , pp. 250–\n260.\n[41] V . Sekar, M. K. Reiter, W. Willinger, H. Zhang, R. R. Kompella, and\nD. G. Andersen, “csamp: A system for network-wide ﬂow monitoring,”\ninProc. of USENIX NSDI , April 16-18, 2008, San Francisco, CA, USA ,\npp. 233–246.\n[42] V . Sekar, A. Gupta, M. K. Reiter, and H. Zhang, “Coordinated sampling\nsans origin-destination identiﬁers: Algorithms and analysis,” in Proc.\nof IEEE COMSNETS, January 5-9, 2010, Bangalore, India , pp. 1–10.\n[43] S. Shen and A. Akella, “DECOR: A distributed coordinated resource\nmonitoring system,” in Proc. of IEEE IWQoS, June 4-5, 2012 Coimbra,\nPortugal , pp. 1–9.\n[44] Y . Yu, C. Qian, and X. Li, “Distributed and collaborative trafﬁc moni-\ntoring in software deﬁned networks,” in Proc. of ACM HotSDN, August\n22, 2014, Chicago, Illinois, USA , A. Akella and A. G. Greenberg, Eds.,\npp. 85–90.\n[45] B. H. Bloom, “Space/time trade-offs in hash coding with allowable\nerrors,” Commun. ACM , vol. 13, no. 7, pp. 422–426, 1970.\n[46] H. Xu, S. Chen, Q. Ma, and L. Huang, “Lightweight ﬂow distribution\nfor collaborative trafﬁc measurement in software deﬁned networks,” in\nProc. of IEEE INFOCOM, April 29 - May 2, 2019 Paris, France , pp.\n1108–1116.\n[47] C. Chaudet, E. Fleury, I. G. Lassous, H. Rivano, and M. V oge, “Optimal\npositioning of active and passive monitoring devices,” in Proc. of ACM\n\nCoNEXT, October 24-27, 2005, Toulouse, France , M. Diaz, A. Azcorra,\nP. Owezarski, and S. Fdida, Eds., pp. 71–82.\n[48] B. Claise, “Cisco systems netﬂow services export version 9,” RFC , vol.\n3954, pp. 1–33, 2004.\n[49] N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar, L. Peterson,\nJ. Rexford, S. Shenker, and J. Turner, “Openﬂow: enabling innovation\nin campus networks,” ACM SIGCOMM Computer Communication\nReview , vol. 38, no. 2, pp. 69–74, 2008.\n[50] G. Huang, C. Chuah, S. Raza, and S. Seetharaman, “Dynamic\nmeasurement-aware routing in practice,” IEEE Netw. , vol. 25, no. 3,\npp. 29–34, 2011.\n[51] R. Jang, D. Min, S. Moon, D. A. Mohaisen, and D. Nyang, “Sketch-\nﬂow: Per-ﬂow systematic sampling using sketch saturation event,” in\nProc. of IEEE INFOCOM, July 6-9, 2020, Toronto, ON, Canada , pp.\n1339–1348.\n[52] “sﬂow.org - making the network visible,” https://sﬂow.org/, (Accessed\non 02/02/2021).\n[53] A. Kumar and J. J. Xu, “Sketch guided sampling - using on-line\nestimates of ﬂow size for adaptive data collection,” in Proc. of IEEE\nINFOCOM, 23-29 April 2006, Barcelona, Catalunya, Spain, .\n[54] B. Choi, J. Park, and Z. Zhang, “Adaptive random sampling for\ntrafﬁc load measurement,” in Proc. of IEEE ICC, 11-15 May, 2003,\nAnchorage, Alaska, USA , pp. 1552–1556.\n[55] C. Hu, S. Wang, J. Tian, B. Liu, Y . Cheng, and Y . Chen, “Accurate and\nefﬁcient trafﬁc monitoring using adaptive non-linear sampling method,”\ninProc. of IEEE INFOCOM, 13-18 April 2008, Phoenix, AZ, USA , pp.\n26–30.\n[56] C. Hu and B. Liu, “Self-tuning the parameter of adaptive non-linear\nsampling method for ﬂow statistics,” in Proc. of IEEE CSE, August\n29-31, 2009, Vancouver, BC, Canada , pp. 16–21.\n[57] G. Einziger, B. Fellman, R. Friedman, and Y . Kassner, “Ice buckets:\nImproved counter estimation for network measurement,” IEEE/ACM\nTransactions on Networking , vol. 26, no. 3, pp. 1165–1178, 2018.\n[58] R. Ben-Basat, G. Einziger, R. Friedman, M. C. Luizelli, and E. Wais-\nbard, “Constant time updates in hierarchical heavy hitters,” in Proc.\nof ACM SIGCOMM, August 21-25, 2017, Los Angeles, CA, USA , pp.\n127–140.\n[59] Z. Liu, R. Ben-Basat, G. Einziger, Y . Kassner, V . Braverman, R. Fried-\nman, and V . Sekar, “Nitrosketch: robust and general sketch-based\nmonitoring in software switches,” in Proc. of ACM SIGCOMM, August\n19-23, 2019, Beijing, China , J. Wu and W. Hall, Eds., pp. 334–350.\n[60] H. C. Zhao, H. Wang, B. Lin, and J. J. Xu, “Design and performance\nanalysis of a dram-based statistics counter array architecture,” in Proc.\nof the ACM/IEEE ANCS, October 19-20, 2009, Princeton, New Jersey,\nUSA, pp. 84–93.\n[61] Y . Zhou, O. Alipourfard, M. Yu, and T. Yang, “Accelerating network\nmeasurement in software,” Computer Communication Review , vol. 48,\nno. 3, pp. 2–12, 2018.\n[62] S. Ganguly, M. N. Garofalakis, and R. Rastogi, “Processing data-stream\njoin aggregates using skimmed sketches,” in Proc. of Springer EDBT,\nMarch 14-18, 2004, Heraklion, Crete, Greece , ser. Lecture Notes in\nComputer Science, vol. 2992, pp. 569–586.\n[63] P. Roy, A. Khan, and G. Alonso, “Augmented sketch: Faster and more\naccurate stream processing,” in Proc. of ACM SIGMOD, June 26 -\nJuly 01, 2016, San Francisco, CA, USA , F. ¨Ozcan, G. Koutrika, and\nS. Madden, Eds., pp. 1449–1463.\n[64] Y . Zhou, T. Yang, J. Jiang, B. Cui, M. Yu, X. Li, and S. Uhlig,\n“Cold ﬁlter: A meta-framework for faster and more accurate stream\nprocessing,” in Proc. of ACM SIGMOD, June 10-15, 2018, Houston,\nTX, USA , G. Das, C. M. Jermaine, and P. A. Bernstein, Eds., pp. 741–\n756.\n[65] B. Fan, D. G. Andersen, M. Kaminsky, and M. Mitzenmacher, “Cuckoo\nﬁlter: Practically better than bloom,” in Proc. of ACM CoNEXT, De-\ncember 2-5, 2014Sydney, Australia, , A. Seneviratne, C. Diot, J. Kurose,\nA. Chaintreau, and L. Rizzo, Eds., pp. 75–88.\n[66] L. Fan, P. Cao, J. M. Almeida, and A. Z. Broder, “Summary cache:\na scalable wide-area web cache sharing protocol,” IEEE/ACM Trans.\nNetw. , vol. 8, no. 3, pp. 281–293, 2000.\n[67] A. W. Richa, M. Mitzenmacher, and R. Sitaraman, “The power of two\nrandom choices: A survey of techniques and results,” Combinatorial\nOptimization , vol. 9, pp. 255–304, 2001.\n[68] G. Cormode and S. Muthukrishnan, “What’s new: ﬁnding signiﬁcant\ndifferences in network data streams,” IEEE/ACM Trans. Netw. , vol. 13,\nno. 6, pp. 1219–1232, 2005.[69] Y . Liu, W. Chen, and Y . Guan, “A fast sketch for aggregate queries over\nhigh-speed network trafﬁc,” in Proc. of the IEEE INFOCOM, March\n25-30, 2012, Orlando, FL, USA , A. G. Greenberg and K. Sohraby,\nEds., pp. 2741–2745.\n[70] Y . Liu and X. Xie, “Xy-sketch: on sketching data streams at web scale,”\ninProc. of ACMWWW, April 19-23, 2021, Virtual Event / Ljubljana,\nSlovenia , J. Leskovec, M. Grobelnik, M. Najork, J. Tang, and L. Zia,\nEds., pp. 1169–1180.\n[71] C. Estan, G. Varghese, and M. E. Fisk, “Bitmap algorithms for counting\nactive ﬂows on high-speed links,” IEEE/ACM Trans. Netw. , vol. 14,\nno. 5, pp. 925–937, 2006.\n[72] A. Metwally, D. Agrawal, and A. E. Abbadi, “Why go logarithmic if\nwe can go linear?: Towards effective distinct counting of search trafﬁc,”\ninProc. of ACM EDBTMarch 25-29, 2008, Nantes, France , ser. ACM\nInternational Conference Proceeding Series, vol. 261, pp. 618–629.\n[73] P. Flajolet and G. N. Martin, “Probabilistic counting algorithms for data\nbase applications,” J. Comput. Syst. Sci. , vol. 31, no. 2, pp. 182–209,\n1985.\n[74] M. Durand and P. Flajolet, “Loglog counting of large cardinalities\n(extended abstract),” in Proc. of Springer Algorithms - ESA, September\n16-19, 2003, Hungary , ser. Lecture Notes in Computer Science, G. D.\nBattista and U. Zwick, Eds., vol. 2832, pp. 605–617.\n[75] S. Heule, M. Nunkesser, and A. Hall, “Hyperloglog in practice:\nalgorithmic engineering of a state of the art cardinality estimation\nalgorithm,” in Proc. of ACM EDBT/ICDT, March 18-22, 2013, Genoa,\nItaly, pp. 683–692.\n[76] Q. Xiao, Y . Zhou, and S. Chen, “Better with fewer bits: Improving the\nperformance of cardinality estimation of large data streams,” in Proc.\nof IEEE INFOCOM, May 1-4, 2017, Atlanta, GA, USA , pp. 1–9.\n[77] Y . Chabchoub and G. H ´ebrail, “Sliding hyperloglog: Estimating car-\ndinality in a data stream over a sliding window,” in Proc. of IEEE\nICDMW, 13 December 2010, Sydney, Australia , W. Fan, W. Hsu, G. I.\nWebb, B. Liu, C. Zhang, D. Gunopulos, and X. Wu, Eds., pp. 1297–\n1303.\n[78] Y . Zhou, Y . Zhang, C. Ma, S. Chen, and O. Odegbile, “Generalized\nsketch families for network trafﬁc measurement,” Proc. ACM Meas.\nAnal. Comput. Syst. , vol. 3, no. 3, pp. 51:1–51:34, 2019.\n[79] D. Bertsimas and V . D. Jr., “Frequency estimation in data streams:\nLearning the optimal hashing scheme,” CoRR , vol. abs/2007.09261,\n2020.\n[80] Y . Fu, D. Li, S. Shen, Y . Zhang, and K. Chen, “Clustering-preserving\nnetwork ﬂow sketching,” in Proc. of IEEE INFOCOM, July 6-9, 2020,\nToronto, ON, Canada , pp. 1309–1318.\n[81] C. Hsu, P. Indyk, D. Katabi, and A. Vakilian, “Learning-based fre-\nquency estimation algorithms,” in Proc. of OpenReview.net ICLR, May\n6-9, 2019, New Orleans, LA, USA .\n[82] A. Aamand, P. Indyk, and A. Vakilian, “(learned) frequency estimation\nalgorithms under zipﬁan distribution,” CoRR , vol. abs/1908.05198,\n2019.\n[83] G. Cormode and M. Hadjieleftheriou, “Finding frequent items in data\nstreams,” PVLDB , vol. 1, no. 2, pp. 1530–1541, 2008.\n[84] T. Yang, J. Xu, X. Liu, P. Liu, L. Wang, J. Bi, and X. Li, “A generic\ntechnique for sketches to adapt to different counting ranges,” in Proc.\nof IEEE INFOCOM,April 29 - May 2, 2019, Paris, France , pp. 2017–\n2025.\n[85] R. Stanojevic, “Small active counters,” in Proc. of IEEE INFOCOM,\n6-12 May 2007, Anchorage, Alaska, USA , pp. 2153–2161.\n[86] R. H. M. Sr., “Counting large numbers of events in small registers,”\nCommun. ACM , vol. 21, no. 10, pp. 840–842, 1978.\n[87] G. Pitel and G. Fouquier, “Count-min-log sketch: Approximately count-\ning with approximate counters,” CoRR , vol. abs/1502.04885, 2015.\n[88] Q. Xiao, Z. Tang, and S. Chen, “Universal online sketch for tracking\nheavy hitters and estimating moments of data streams,” in Proc. of\nIEEE INFOCOM, July 6-9, 2020, Toronto, ON, Canada , pp. 974–983.\n[89] Q. Xiao, S. Chen, Y . Zhou, M. Chen, J. Luo, T. Li, and Y . Ling,\n“Cardinality estimation for elephant ﬂows: A compact solution based\non virtual register sharing,” IEEE/ACM Trans. Netw. , vol. 25, no. 6,\npp. 3738–3752.\n[90] M. Yoon, T. Li, S. Chen, and J. Peir, “Fit a compact spread estimator\nin small high-speed memory,” IEEE/ACM Trans. Netw. , vol. 19, no. 5,\npp. 1253–1264, 2011.\n[91] Y . Zhou, Y . Zhou, M. Chen, Q. Xiao, and S. Chen, “Highly compact\nvirtual counters for per-ﬂow trafﬁc measurement through register shar-\n\ning,” in Proc. of IEEE GLOBECOM, December 4-8, 2016, Washington,\nDC, USA , pp. 1–6.\n[92] K. Whang, B. T. V . Zanden, and H. M. Taylor, “A linear-time\nprobabilistic counting algorithm for database applications,” ACM Trans.\nDatabase Syst. , vol. 15, no. 2, pp. 208–229, 1990.\n[93] M. Chen, S. Chen, and Z. Cai, “Counter tree: A scalable counter\narchitecture for per-ﬂow trafﬁc measurement,” IEEE/ACM Trans. Netw. ,\nvol. 25, no. 2, pp. 1249–1262, 2017.\n[94] T. Yang, Y . Zhou, H. Jin, S. Chen, and X. Li, “Pyramid sketch: a sketch\nframework for frequency estimation of data streams,” PVLDB , vol. 10,\nno. 11, pp. 1442–1453, 2017.\n[95] Y . Zhou, P. Liu, H. Jin, T. Yang, S. Dang, and X. Li, “One memory\naccess sketch: A more accurate and faster sketch for per-ﬂow measure-\nment,” in Proc. of IEEE GLOBECOM, December 4-8, 2017, Singapore ,\npp. 1–6.\n[96] T. Yang, S. Gao, Z. Sun, Y . Wang, Y . Shen, and X. Li, “Diamond\nsketch: Accurate per-ﬂow measurement for real IP streams,” in Proc.\nIEEE INFOCOM Workshops, April 15-19, 2018, Honolulu, HI, USA ,\npp. 1–2.\n[97] Y . Zhou, H. Jin, P. Liu, H. Zhang, T. Yang, and X. Li, “Accurate per-\nﬂow measurement with bloom sketch,” in Proc. of IEEE INFOCOM\nWorkshops, April 15-19, 2018, Honolulu, HI, USA , pp. 1–2.\n[98] S. Cohen and Y . Matias, “Spectral bloom ﬁlters,” in Proc. of ACM\nSIGMOD, June 9-12, 2003, San Diego, California, USA , pp. 241–252.\n[99] N. Hua, J. J. Xu, B. Lin, and H. C. Zhao, “BRICK: a novel exact active\nstatistics counter architecture,” IEEE/ACM Trans. Netw. , vol. 19, no. 3,\npp. 670–682, 2011.\n[100] N. Hua, H. C. Zhao, B. Lin, and J. J. Xu, “Rank-indexed hashing: A\ncompact construction of bloom ﬁlters and variants,” in Proc. of IEEE\nICNP, 19-22 October 2008, Orlando, Florida, USA , pp. 73–82.\n[101] G. Einziger and R. Friedman, “Counting with tinytable: every bit\ncounts!” in Proc. of ACM ICDCN, January 4-7, 2016, Singapore , pp.\n27:1–27:10.\n[102] P. Pandey, M. A. Bender, R. Johnson, and R. Patro, “A general-purpose\ncounting ﬁlter: Making every bit count,” in Proc. of ACM SIGMOD,\nMay 14-19, 2017, Chicago, IL, USA , pp. 775–787.\n[103] M. A. Bender, M. Farach-Colton, R. Johnson, R. Kraner, B. C.\nKuszmaul, D. Medjedovic, P. Montes, P. Shetty, R. P. Spillane, and\nE. Zadok, “Don’t thrash: How to cache your hash on ﬂash,” PVLDB ,\nvol. 5, no. 11, pp. 1627–1637, 2012.\n[104] J. Gong, T. Yang, Y . Zhou, D. Yang, S. Chen, B. Cui, and X. Li,\n“ABC: A practicable sketch framework for non-uniform multisets,” in\nProc. of IEEE BigData, December 11-14, 2017, Boston, MA, USA , pp.\n2380–2389.\n[105] R. B. Basat, G. Einziger, M. Mitzenmacher, and S. Vargaftik, “SALSA:\nself-adjusting lean streaming analytics,” CoRR , vol. abs/2102.12531,\n2021. [Online]. Available: https://arxiv.org/abs/2102.12531\n[106] D. Nyang and D. Shin, “Recyclable counter with conﬁnement for real-\ntime per-ﬂow measurement,” IEEE/ACM Trans. Netw. , vol. 24, no. 5,\npp. 3191–3203, 2016.\n[107] J. Qi, W. Li, T. Yang, D. Li, and H. Li, “Cuckoo counter: A novel\nframework for accurate per-ﬂow frequency estimation in network\nmeasurement,” in Proc. of ACM/IEEE ANCS, September 24-25, 2019,\nCambridge, United Kingdom , pp. 1–7.\n[108] C. Estan and G. Varghese, “New directions in trafﬁc measurement and\naccounting: Focusing on the elephants, ignoring the mice,” ACM Trans.\nComput. Syst. , vol. 21, no. 3, pp. 270–313, 2003.\n[109] T. Li, S. Chen, and Y . Ling, “Fast and compact per-ﬂow trafﬁc\nmeasurement through randomized counter sharing,” in Proc. of IEEE\nINFOCOM, 10-15 April 2011, Shanghai, China , pp. 1799–1807.\n[110] P. Reviriego, J. A. Mart ´ınez, and M. Ottavi, “Soft error tolerant count\nmin sketches,” IEEE Trans. Computers , vol. 70, no. 2, pp. 284–290,\n2021.\n[111] R. Ben-Basat, G. Einziger, R. Friedman, and Y . Kassner, “Randomized\nadmission policy for efﬁcient top-k and frequency estimation,” in Proc.\nof IEEE INFOCOM, May 1-4, 2017, Atlanta, GA, USA , pp. 1–9.\n[112] E. D. Demaine, A. L ´opez-Ortiz, and J. I. Munro, “Frequency estimation\nof internet packet streams with limited space,” in Proc. of Springer\nAlgorithms - ESA, September 17-21, 2002, Rome, Italy , ser. Lecture\nNotes in Computer Science, R. H. M ¨ohring and R. Raman, Eds., vol.\n2461, pp. 348–360.\n[113] A. Metwally, D. Agrawal, and A. E. Abbadi, “Proc. of Springer ICDT,\njanuary 5-7, 2005, edinburgh, uk,” ser. Lecture Notes in Computer\nScience, T. Eiter and L. Libkin, Eds., vol. 3363, pp. 398–412.[114] D. Ting, “Data sketches for disaggregated subset sum and frequent item\nestimation,” in Proc. of ACM SIGMOD, June 10-15, 2018, Houston,\nTX, USA , G. Das, C. M. Jermaine, and P. A. Bernstein, Eds., pp. 1129–\n1140.\n[115] X. Yu, H. Xu, D. Yao, H. Wang, and L. Huang, “Countmax: A\nlightweight and cooperative sketch measurement for software-deﬁned\nnetworks,” IEEE/ACM Trans. Netw. , vol. 26, no. 6, pp. 2774–2786,\n2018.\n[116] L. Tang, Q. Huang, and P. P. C. Lee, “Mv-sketch: A fast and compact\ninvertible sketch for heavy ﬂow detection in network data streams,” in\nProc. of IEEE INFOCOM, April 29 - May 2, 2019, Paris, France , pp.\n2026–2034.\n[117] R. S. Boyer and J. S. Moore, “MJRTY: A fast majority vote algorithm,”\ninAutomated Reasoning: Essays in Honor of Woody Bledsoe , ser.\nAutomated Reasoning Series, R. S. Boyer, Ed., pp. 105–118.\n[118] T. Yang, H. Zhang, J. Li, J. Gong, S. Uhlig, S. Chen, and X. Li,\n“Heavykeeper: An accurate algorithm for ﬁnding top-k elephant ﬂows,”\nIEEE/ACM Trans. Netw. , vol. 27, no. 5, pp. 1845–1858, 2019.\n[119] T. Yang, J. Gong, H. Zhang, L. Zou, L. Shi, and X. Li, “Heavyguardian:\nSeparate and guard hot items in data streams,” in Proc. of ACM\nSIGKDD, August 19-23, 2018, London, UK , Y . Guo and F. Farooq,\nEds., pp. 2584–2593.\n[120] J. Gong, D. Tian, D. Yang, T. Yang, T. Dai, B. Cui, and X. Li, “SSS: an\naccurate and fast algorithm for ﬁnding top-k hot items in data streams,”\ninProc. of IEEE BigComp, January 15-17, 2018, Shanghai, China , pp.\n106–113.\n[121] J. Bruck, J. Gao, and A. Jiang, “Weighted bloom ﬁlter,” in Proc. of\nIEEE ISIT, July 9-14, 2006, The Westin Seattle, Seattle, Washington,\nUSA, pp. 2304–2308.\n[122] D. Thomas, R. Bordawekar, C. C. Aggarwal, and P. S. Yu, “On efﬁcient\nquery processing of stream counts on the cell processor,” in Proc. of\nIEEE ICDE, March 29 2009 - April 2 2009, Shanghai, China , Y . E.\nIoannidis, D. L. Lee, and R. T. Ng, Eds., pp. 748–759.\n[123] J. Misra and D. Gries, “Finding repeated elements,” Sci. Comput.\nProgram. , vol. 2, no. 2, pp. 143–152, 1982.\n[124] R. R. Kompella, K. Levchenko, A. C. Snoeren, and G. Varghese,\n“Every microsecond counts: tracking ﬁne-grain latencies with a lossy\ndifference aggregator,” in Proc. of ACM SIGCOMM, August 16-21,\n2009, Barcelona, Spain , pp. 255–266.\n[125] Q. Huang and P. P. C. Lee, “A hybrid local and distributed sketching\ndesign for accurate and scalable heavy key detection in network data\nstreams,” Computer Networks , vol. 91, pp. 298–315, 2015.\n[126] Z. Zhou, D. Zhang, and X. Hong, “Rl-sketch: Scaling reinforcement\nlearning for adaptive and automate anomaly detection in network data\nstreams,” in Proc. of IEEE LCN, October 14-17, 2019, Osnabrueck,\nGermany , K. Andersson, H. Tan, and S. Oteafy, Eds., pp. 340–347.\n[127] Y . Li, R. Miao, C. Kim, and M. Yu, “Flowradar: A better netﬂow for\ndata centers,” in Proc. of USENIX NSDI, March 16-18, 2016, Santa\nClara, CA, USA , K. J. Argyraki and R. Isaacs, Eds., pp. 311–324.\n[128] D. Eppstein, M. T. Goodrich, F. Uyeda, and G. Varghese, “What’s the\ndifference?: efﬁcient set reconciliation without prior context,” in Proc.\nof the ACM SIGCOMM, August 15-19, 2011, Toronto, ON, Canada ,\nS. Keshav, J. Liebeherr, J. W. Byers, and J. C. Mogul, Eds., pp. 218–\n229.\n[129] X. Li, F. Bian, M. Crovella, C. Diot, R. Govindan, G. Iannaccone, and\nA. Lakhina, “Detection and identiﬁcation of network anomalies using\nsketch subspaces,” in Proc. of the 6th ACM SIGCOMM, October 25-\n27, 2006, Rio de Janeriro, Brazil , J. M. Almeida, V . A. F. Almeida,\nand P. Barford, Eds., pp. 147–152.\n[130] A. Lakhina, M. Crovella, and C. Diot, “Mining anomalies using trafﬁc\nfeature distributions,” in Proc. of the ACM SIGCOMM, August 22-26,\n2005, Philadelphia, Pennsylvania, USA , R. Gu ´erin, R. Govindan, and\nG. Minshall, Eds., pp. 217–228.\n[131] A. Kumar, J. Xu, and J. Wang, “Space-code bloom ﬁlter for efﬁcient\nper-ﬂow trafﬁc measurement,” IEEE J. Sel. Areas Commun. , vol. 24,\nno. 12, pp. 2327–2339, 2006.\n[132] H. C. Zhao, A. Lall, M. Ogihara, O. Spatscheck, J. Wang, and J. J. Xu,\n“A data streaming algorithm for estimating entropies of od ﬂows,” in\nProc. of ACM SIGCOMM, October 24-26, 2007, San Diego, California,\nUSA, C. Dovrolis and M. Roughan, Eds., pp. 279–290.\n[133] P. Indyk, “Stable distributions, pseudorandom generators, embeddings\nand data stream computation,” in Proc. of IEEE FOCS, 12-14 Novem-\nber 2000, Redondo Beach, California, USA , pp. 189–197.\n\n[134] T. Bu, J. Cao, A. Chen, and P. P. C. Lee, “Sequential hashing: A ﬂexible\napproach for unveiling signiﬁcant patterns in high speed networks,”\nComputer Networks , vol. 54, no. 18, pp. 3309–3326, 2010.\n[135] X. Guan, P. Wang, and T. Qin, “A new data streaming method\nfor locating hosts with large connection degree,” in Proc. of IEEE\nGLOBECOM, 30 November - 4 December 2009, Honolulu, Hawaii,\nUSA, pp. 1–6.\n[136] T. Yang, L. Liu, Y . Yan, M. Shahzad, Y . Shen, X. Li, B. Cui, and G. Xie,\n“Sf-sketch: A fast, accurate, and memory efﬁcient data structure to store\nfrequencies of data items,” in Proc. of IEEE ICDE, April 19-22, 2017,\nSan Diego, CA, USA , pp. 103–106.\n[137] V . Sivaraman, S. Narayana, O. Rottenstreich, S. Muthukrishnan, and\nJ. Rexford, “Heavy-hitter detection entirely in the data plane,” in Proc.\nof SDN SOSR, April 3-4, 201, Santa Clara, CA, USA7 , pp. 164–176.\n[138] M. G. Kallitsis, S. A. Stoev, S. Bhattacharya, and G. Michailidis,\n“AMON: an open source architecture for online monitoring, statistical\nanalysis, and forensics of multi-gigabit streams,” IEEE J. Sel. Areas\nCommun. , vol. 34, no. 6, pp. 1834–1848, 2016.\n[139] R. T. Schweller, Z. Li, Y . Chen, Y . Gao, A. Gupta, Y . Zhang,\nP. A. Dinda, M. Kao, and G. Memik, “Reversible sketches: enabling\nmonitoring and analysis over high-speed data streams,” IEEE/ACM\nTrans. Netw. , vol. 15, no. 5, pp. 1059–1072, 2007.\n[140] Z. Liu, A. Manousis, G. V orsanger, V . Sekar, and V . Braverman,\n“One sketch to rule them all: Rethinking network ﬂow monitoring\nwith univmon,” in Proc. of the ACM SIGCOMM, August 22-26, 2016,\nFlorianopolis, Brazil , pp. 101–114.\n[141] J. Aguilar-Saborit, P. Trancoso, V . Munt ´es-Mulero, and J. Larriba-Pey,\n“Dynamic count ﬁlters,” SIGMOD Record , vol. 35, no. 1, pp. 26–32,\n2006.\n[142] N. Homem and J. P. Carvalho, “Finding top-k elements in data\nstreams,” Inf. Sci. , vol. 180, no. 24, pp. 4958–4974, 2010.\n[143] P. Lieven and B. Scheuermann, “High-speed per-ﬂow trafﬁc mea-\nsurement with probabilistic multiplicity counting,” in Proc. IEEE\nINFOCOM, 15-19 March 2010, San Diego, CA, USA , pp. 1253–1261.\n[144] H. Wang, H. Xu, L. Huang, and Y . Zhai, “Fast and accurate trafﬁc mea-\nsurement with hierarchical ﬁltering,” IEEE Trans. Parallel Distributed\nSyst., vol. 31, no. 10, pp. 2360–2374, 2020.\n[145] Y . Zhou, Y . Zhou, S. Chen, and Y . Zhang, “Per-ﬂow counting for big\nnetwork data stream over sliding windows,” in Proc. of IEEE/ACM\nIWQoS, June 14-16, 2017, Vilanova i la Geltr ´u, Spain , pp. 1–10.\n[146] L. Golab, D. DeHaan, E. D. Demaine, A. L ´opez-Ortiz, and J. I. Munro,\n“Identifying frequent items in sliding windows over on-line packet\nstreams,” in Proc. ACM SIGCOMM IMC, October 27-29, 2003, Miami\nBeach, FL, USA , pp. 173–178.\n[147] E. Assaf, R. Ben-Basat, G. Einziger, and R. Friedman, “Pay for a\nsliding bloom ﬁlter and get counting, distinct elements, and entropy\nfor free,” in Proc. of IEEE INFOCOM, April 16-19, 2018, Honolulu,\nHI, USA , pp. 2204–2212.\n[148] S. Matusevych, A. J. Smola, and A. Ahmed, “Hokusai - sketching\nstreams in real time,” in Proc. of AUAI Press UAI, August 14-18, 2012,\nCatalina Island, CA, USA , pp. 594–603.\n[149] R. Ben-Basat, G. Einziger, R. Friedman, and Y . Kassner, “Heavy hitters\nin streams and sliding windows,” in Proc. of IEEE INFOCOM, April\n10-14, 2016, San Francisco, CA, USA , pp. 1–9.\n[150] Y . Zhou, J. Bi, T. Yang, K. Gao, J. Cao, D. Zhang, Y . Wang, and\nC. Zhang, “Hypersight: Towards scalable, high-coverage, and dynamic\nnetwork monitoring queries,” IEEE J. Sel. Areas Commun. , vol. 38,\nno. 6, pp. 1147–1160, 2020.\n[151] R. Ben-Basat, G. Einziger, I. Keslassy, A. Orda, S. Vargaftik, and\nE. Waisbard, “Memento: making sliding windows efﬁcient for heavy\nhitters,” in Proc. of ACM CoNEXT, December 04-07, 2018, Heraklion,\nGreece , X. A. Dimitropoulos, A. Dainotti, L. Vanbever, and T. Benson,\nEds., pp. 254–266.\n[152] A. Shrivastava, A. C. K ¨onig, and M. Bilenko, “Time adaptive sketches\n(ada-sketches) for summarizing data streams,” in Proc. of ACMSIG-\nMOD, June 26 - July 01, 2016, San Francisco, CA, USA , pp. 1417–\n1432.\n[153] X. Gou, L. He, Y . Zhang, K. Wang, X. Liu, T. Yang, Y . Wang, and\nB. Cui, “Sliding sketches: A framework using time zones for data\nstream processing in sliding windows,” in Proc. of ACM SIGKDD,\nVirtual Event, August 23-27, 2020, CA, USA, pages = 1015–1025, .\n[154] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The case\nfor learned index structures,” in Proc. of ACM SIGMOD June 10-15,\n2018, Houston, TX, USA, , pp. 489–504.[155] M. Mitzenmacher, “A model for learned bloom ﬁlters and related\nstructures,” CoRR , vol. abs/1802.00884, 2018.\n[156] H. Oosterhuis, J. S. Culpepper, and M. de Rijke, “The potential of\nlearned index structures for index compression,” in Proc. of ACM\nADCS, December 11-12, 2018, Dunedin, New Zealand , pp. 7:1–7:4.\n[157] B. Scheuermann and M. Mauve, “Near-optimal compression of prob-\nabilistic counting sketches for networking applications,” in Proc. of\nDIALM-POMC, August 16, 2007, Portland, Oregon, USA .\n[158] T. Yang, J. Jiang, P. Liu, Q. Huang, J. Gong, Y . Zhou, R. Miao,\nX. Li, and S. Uhlig, “Elastic sketch: adaptive and fast network-wide\nmeasurements,” in Proc. of ACM SIGCOMM, August 20-25, 2018,\nBudapest, Hungary , S. Gorinsky and J. Tapolcai, Eds., pp. 561–575.\n[159] Y . Zhao, Z. Zhong, Y . Li, Y . Zhou, Y . Zhu, L. Chen, Y . Wang, and\nT. Yang, “Cluster-reduce: Compressing sketches for distributed data\nstreams,” in Proc. of ACM KDD, August 14–18, 2021, Virtual Event,\nSingapore. ACM, New York, NY, USA , p. 11.\n[160] M. Cafaro, P. Tempesta, and M. Pulimeno, “Mergeable summaries with\nlow total error,” CoRR , vol. abs/1401.0702, 2014.\n[161] Y . Zhao, K. Yang, Z. Liu, T. Yang, L. Chen, S. Liu, N. Zheng, R. Wang,\nH. Wu, Y . Wang, and N. Zhang, “Lightguardian: A full-visibility,\nlightweight, in-band telemetry system using sketchlets,” in Proc. of\nUSENIX, NSDI, April 12-14, 2021 , pp. 991–1010.\n[162] F. Wang and L. Gao, “Simple and efﬁcient identiﬁcation of heavy\nhitters based on bitcount,” in Proc. of IEEE HPSR, May 26-29, 2019,\nXi’an, China , pp. 1–6.\n[163] A. Kumar, M. Sung, J. J. Xu, and J. Wang, “Data streaming algorithms\nfor efﬁcient and accurate estimation of ﬂow size distribution,” in Proc.\nof ACM SIGMETRICS, June 10-14, 2004, New York, NY, USA , pp.\n177–188.\n[164] H. Dai, M. Shahzad, A. X. Liu, M. Li, Y . Zhong, and G. Chen, “Iden-\ntifying and estimating persistent items in data streams,” IEEE/ACM\nTrans. Netw. , vol. 26, no. 6, pp. 2429–2442, 2018.\n[165] T. Yang, L. Wang, Y . Shen, M. Shahzad, Q. Huang, X. Jiang, K. Tan,\nand X. Li, “Empowering sketches with machine learning for network\nmeasurements,” in Proc. of ACM AI & ML, NetAI@SIGCOMM, August\n24, 2018, Budapest, Hungary , pp. 15–20.\n[166] M. Malboubi, L. Wang, C. Chuah, and P. Sharma, “Intelligent SDN\nbased trafﬁc (de)aggregation and measurement paradigm (istamp),” in\nProc. of IEEE INFOCOM, April 27 - May 2, 2014, Toronto, Canada ,\npp. 934–942.\n[167] M. Yu, L. Jose, and R. Miao, “Software deﬁned trafﬁc measurement\nwith opensketch,” in Proc. of USENIX NSDI, April 2-5, 2013, Lombard,\nIL, USA , N. Feamster and J. C. Mogul, Eds., pp. 29–42.\n[168] Q. Huang, X. Jin, P. P. C. Lee, R. Li, L. Tang, Y . Chen, and\nG. Zhang, “Sketchvisor: Robust network measurement for software\npacket processing,” in Proc. of ACM SIGCOMM, August 21-25, 2017,\nLos Angeles, CA, USA , pp. 113–126.\n[169] R. Cohen and Y . Nezri, “Cardinality estimation in a virtualized net-\nwork device using online machine learning,” IEEE/ACM Trans. Netw. ,\nvol. 27, no. 5, pp. 2098–2110, 2019.\n[170] M. Mitzenmacher, “A model for learned bloom ﬁlters and optimizing\nby sandwiching,” in Proc. of NeurIPS, December 3-8, 2018, Montr ´eal,\nCanada , S. Bengio, H. M. Wallach, H. Larochelle, K. Grauman,\nN. Cesa-Bianchi, and R. Garnett, Eds., pp. 462–471.\n[171] K. Vaidya, E. Knorr, T. Kraska, and M. Mitzenmacher, “Partitioned\nlearned bloom ﬁlter,” 2020.\n[172] F. Deng and D. Raﬁei, “Approximately detecting duplicates for stream-\ning data using stable bloom ﬁlters,” in Proc. of ACM SIGMOD, June\n27-29, 2006, Illinois, USA , S. Chaudhuri, V . Hristidis, and N. Polyzotis,\nEds., pp. 25–36.\n[173] Q. Liu, L. Zheng, Y . Shen, and L. Chen, “Stable learned bloom ﬁlters\nfor data streams,” Proc. of VLDB Endow. , vol. 13, no. 11, pp. 2355–\n2367, 2020.\n[174] A. Chen, Y . Jin, and J. Cao, “Tracking long duration ﬂows in network\ntrafﬁc,” in Proc. of IEEE INFOCOM, 15-19 March 2010, San Diego,\nCA, USA , pp. 206–210.\n[175] B. Lahiri, S. Tirthapura, and J. Chandrashekar, “Space-efﬁcient tracking\nof persistent items in a massive data stream,” Statistical Analysis and\nData Mining , vol. 7, no. 1, pp. 70–92, 2014.\n[176] H. Huang, Y . Sun, S. Chen, S. Tang, K. Han, J. Yuan, and W. Yang,\n“You can drop but you can’t hide: K-persistent spread estimation in\nhigh-speed networks,” in Proc. of IEEE INFOCOM, April 16-19, 2018,\nHonolulu, HI, USA , pp. 1889–1897.\n\n[177] T. Yang, H. Zhang, D. Yang, Y . Huang, and X. Li, “Finding signiﬁcant\nitems in data streams,” in Proc. of IEEE ICDE, April 8-11, 2019,\nMacao, China , pp. 1394–1405.\n[178] Y . Zhang, J. Li, Y . Lei, T. Yang, Z. Li, G. Zhang, and B. Cui, “On-\noff sketch: A fast and accurate sketch on persistence,” Proc. of VLDB\nEndow. , vol. 14, no. 2, pp. 128–140, 2020.\n[179] S. Narayana, A. Sivaraman, V . Nathan, P. Goyal, V . Arun, M. Alizadeh,\nV . Jeyakumar, and C. Kim, “Language-directed hardware design for\nnetwork performance monitoring,” in Proc. of ACM SIGCOMM, August\n21-25, 2017, Los Angeles, CA, USA , pp. 85–98.\n[180] M. Shahzad and A. X. Liu, “Accurate and efﬁcient per-ﬂow latency\nmeasurement without probing and time stamping,” IEEE/ACM Trans.\nNetw. , vol. 24, no. 6, pp. 3477–3492, 2016.\n[181] J. Wang, S. Lian, W. Dong, X. Li, and Y . Liu, “Every packet\ncounts: Loss and reordering identiﬁcation and its application in delay\nmeasurement,” IEEE/ACM Trans. Netw. , vol. 24.\n[182] M. Lee, S. Goldberg, R. R. Kompella, and G. Varghese, “Finecomb:\nMeasuring microscopic latency and loss in the presence of reordering,”\nIEEE/ACM Trans. Netw. , vol. 22, no. 4, pp. 1136–1149, 2014.\n[183] M. Lee, N. G. Dufﬁeld, and R. R. Kompella, “Not all microseconds\nare equal: ﬁne-grained per-ﬂow measurements with reference latency\ninterpolation,” in Proc. of the ACM SIGCOMM, August 30 -September\n3, 2010, New Delhi, India , pp. 27–38.\n[184] F. Deng, “New estimation algorithms for streaming data: Count-min\ncan do more (extended version). http://www.cs.ualberta.ca/˜fandeng/\npaper/cmm full.pdf.”\n[185] Z. Bar-Yossef, T. S. Jayram, R. Kumar, D. Sivakumar, and L. Trevisan,\n“Counting distinct elements in a data stream,” in Proc. of Springer\nRANDOM, September 13-15, 2002, Cambridge, MA, USA , ser. Lecture\nNotes in Computer Science, J. D. P. Rolim and S. P. Vadhan, Eds., vol.\n2483.\n[186] L. Wang, T. Yang, H. Wang, J. Jiang, Z. Cai, B. Cui, and X. Li, “Fine-\ngrained probability counting for cardinality estimation of data streams,”\nWorld Wide Web , vol. 22, no. 5, pp. 2065–2081, 2019.\n[187] S. Venkataraman, D. X. Song, P. B. Gibbons, and A. Blum, “New\nstreaming algorithms for fast detection of superspreaders,” in Proc. of\nThe Internet Society NDSS 2005, San Diego, California, USA .\n[188] Q. Zhao, J. J. Xu, and A. Kumar, “Detection of super sources and des-\ntinations in high-speed networks: Algorithms, analysis and evaluation,”\nIEEE J. Sel. Areas Commun. , vol. 24, no. 10, pp. 1840–1852, 2006.\n[189] L. Tang, Q. Huang, and P. P. C. Lee, “Spreadsketch: Toward invertible\nand network-wide detection of superspreaders,” in Proc. of IEEE\nINFOCOM, July 6-9, 2020, Toronto, ON, Canada , pp. 1608–1617.\n[190] T. Mori, T. Takine, J. Pan, R. Kawahara, M. Uchida, and S. Goto,\n“Identifying heavy-hitter ﬂows from sampled ﬂow statistics,” IEICE\nTransactions , vol. 90-B, no. 11, pp. 3061–3072, 2007.\n[191] G. S. Manku and R. Motwani, “Approximate frequency counts over\ndata streams,” in Proc. of Morgan Kaufmann VLDB, August 20-23,\n2002, Hong Kong , pp. 346–357.\n[192] X. A. Dimitropoulos, P. Hurley, and A. Kind, “Probabilistic lossy\ncounting: an efﬁcient algorithm for ﬁnding heavy hitters,” Computer\nCommunication Review , vol. 38, no. 1, p. 5, 2008.\n[193] K. Cho, “Recursive lattice search: hierarchical heavy hitters revisited,”\ninProc. of ACM IMC, November 1-3, 2017, London, United Kingdom ,\nS. Uhlig and O. Maennel, Eds., pp. 283–289.\n[194] P. Truong and F. Guillemin, “Identiﬁcation of heavyweight address\npreﬁx pairs in IP trafﬁc,” in Proc. of IEEEITC, September 15-17, 2009,\nParis, France , pp. 1–8.\n[195] Y . Lin and H. Liu, “Separator: Sifting hierarchical heavy hitters\naccurately from data streams,” in Proc. of Springer ADMAAugust 6-\n8, 2007, Harbin, China , ser. Lecture Notes in Computer Science, vol.\n4632, pp. 170–182.\n[196] L. Jose and M. Yu, “Online measurement of large trafﬁc aggregates\non commodity switches,” in Proc. of USENIX Hot-ICE’11, March 29,\n2011, Boston, MA, USA , A. Shaikh and K. van der Merwe, Eds.\n[197] G. Cormode, F. Korn, S. Muthukrishnan, and D. Srivastava, “Finding\nhierarchical heavy hitters in streaming data,” ACM Trans. Knowl.\nDiscov. Data , vol. 1, no. 4, pp. 2:1–2:48, 2008.\n[198] J. Thaler, M. Mitzenmacher, and T. Steinke, “Hierarchical heavy hitters\nwith the space saving algorithm,” in Proc. of SIAM / Omnipress\nALENEX, January 16, 2012, The Westin Miyako, Kyoto, Japan , D. A.\nBader and P. Mutzel, Eds., pp. 160–174.\n[199] G. M. Morton, “A computer oriented geodetic data base and a new\ntechnique in ﬁle sequencing,” 1966.[200] K. Xie, J. Tian, X. Wang, G. Xie, J. Wen, and D. Zhang, “Efﬁciently\ninferring top-k elephant ﬂows based on discrete tensor completion,” in\nProc. of IEEE INFOCOM, April 29 - May 2, 2019, Paris, France , pp.\n2170–2178.\n[201] P. Pandey, S. Singh, M. A. Bender, J. W. Berry, M. Farach-Colton,\nR. Johnson, T. M. Kroeger, and C. A. Phillips, “Timely reporting of\nheavy hitters using external memory,” in Proc. of ACM SIGMOD, June\n14-19, 2020, online conference [Portland, OR, USA] , pp. 1431–1446.\n[202] A. Lall, V . Sekar, M. Ogihara, J. J. Xu, and H. Zhang, “Data streaming\nalgorithms for estimating entropy of network trafﬁc,” in Proc. of ACM\nSIGMETRICS/Performance, June 26-30, 2006, Saint Malo, France , pp.\n145–156.\n[203] A. Chakrabarti, G. Cormode, and A. McGregor, “A near-optimal algo-\nrithm for computing the entropy of a stream,” in Proc. of ACM-SIAM\nSODA, January 7-9, 2007, New Orleans, Louisiana, USA , N. Bansal,\nK. Pruhs, and C. Stein, Eds., pp. 328–335.\n[204] R. T. Schweller, A. Gupta, E. Parsons, and Y . Chen, “Reversible\nsketches for efﬁcient and accurate change detection over network data\nstreams,” in Proc. of ACM SIGCOMM IMC, October 25-27, 2004,\nTaormina, Sicily, Italy , pp. 207–212.\n[205] G. Cormode and S. Muthukrishnan, “What’s hot and what’s not:\ntracking most frequent items dynamically,” ACM Trans. Database Syst. ,\nvol. 30, no. 1, pp. 249–278, 2005.\n[206] D. Paul, Y . Peng, and F. Li, “Bursty event detection throughout\nhistories,” in Proc. of IEEE ICDE, April 8-11, 2019, Macao, China ,\npp. 1370–1381.\n[207] Z. Wei, G. Luo, K. Yi, X. Du, and J. Wen, “Persistent data sketching,”\ninProc. of ACM SIGMOD, May 31 - June 4, 2015, Melbourne, Victoria,\nAustralia , T. K. Sellis, S. B. Davidson, and Z. G. Ives, Eds., pp. 795–\n810.\n[208] Z. Zhong, S. Yan, Z. Li, D. Tan, T. Yang, and B. Cui, “Burstsketch:\nFinding bursts in data streams,” in Proc. of ACM SIGMOD, June 20-25,\n2021, Virtual Event, China , pp. 2375–2383.\n[209] Y . Peng, J. Guo, F. Li, W. Qian, and A. Zhou, “Persistent bloom ﬁlter:\nMembership testing for the entire history,” in Proc. of ACM SIGMOD,\nJune 10-15, 2018, Houston, TX, USA , G. Das, C. M. Jermaine, and\nP. A. Bernstein, Eds., pp. 1037–1052.\n[210] P. Chen, D. Chen, L. Zheng, J. Li, and T. Yang, “Out of many we are\none: Measuring item batch with clock-sketch,” in Proc. ACM SIGMOD,\nJune 20–25, 2021, Virtual Event, China. ACM, New York, NY, USA ,\np. 13.\n[211] E. Tsidon, I. Hanniel, and I. Keslassy, “Estimators also need shared\nvalues to grow together,” in Proc. of IEEE INFOCOM, , March 25-30,\n2012, Orlando, FL, USA , pp. 1889–1897.\n[212] Y . Li, H. Wu, T. Pan, H. Dai, J. Lu, and B. Liu, “CASE: cache-assisted\nstretchable estimator for high speed per-ﬂow measurement,” in Proc.\nof IEEE INFOCOM, April 10-14, 2016, San Francisco, CA, USA , pp.\n1–9.\n[213] D. Shah, S. Iyer, B. Prabhakar, and N. McKeown, “Maintaining\nstatistics counters in router line cards,” IEEE Micro , vol. 22, no. 1,\npp. 76–81, 2002.\n[214] S. Ramabhadran and G. Varghese, “Efﬁcient implementation of a\nstatistics counter architecture,” in Proc. of ACM SIGMETRICS, June\n9-14, 2003, San Diego, CA, USA , pp. 261–271.\n[215] M. Roeder and B. Lin, “Maintaining exact statistics counters with\na multi-level counter memory,” in Proc. of IEEE GLOBECOM, 29\nNovember - 3 December 2004, Dallas, Texas, USA , pp. 576–581.\n[216] Q. Zhao, J. J. Xu, and Z. Liu, “Design of a novel statistics counter\narchitecture with optimal space and time efﬁciency,” in Proc. of ACM\nSIGMETRICS/Performance, June 26-30, 2006, Saint Malo, France ,\nR. A. Marie, P. B. Key, and E. Smirni, Eds., pp. 323–334.\n[217] B. Lin and J. J. Xu, “DRAM is plenty fast for wirespeed statistics\ncounting,” SIGMETRICS Performance Evaluation Review , vol. 36,\nno. 2, pp. 45–51, 2008.\n[218] H. Wang, H. C. Zhao, B. Lin, and J. J. Xu, “Dram-based statistics\ncounter array architecture with performance guarantee,” IEEE/ACM\nTrans. Netw. , vol. 20, no. 4, pp. 1040–1053, 2012.\n[219] H. Wang, B. Lin, and J. J. Xu, “Robust statistics counter arrays with\ninterleaved memories,” IEEE Trans. Parallel Distrib. Syst. , vol. 24,\nno. 9, pp. 1894–1907, 2013.\n[220] A. Lall, M. Ogihara, and J. J. Xu, “An efﬁcient algorithm for measuring\nmedium- to large-sized ﬂows in network trafﬁc,” in Proc. of IEEE\nINFOCOM, 19-25 April 2009, Rio de Janeiro, Brazil , pp. 2711–2715.\n\n[221] N. Bandi, A. Metwally, D. Agrawal, and A. E. Abbadi, “Fast data\nstream algorithms using associative memories,” in Proc. of ACM\nSIGMOD, June 12-14, 2007, Beijing, China , pp. 247–256.\n[222] “Openﬂow switch speciﬁcation,” https://www.opennetworking.org/\nwp-content/uploads/2014/10/openﬂow-switch-v1.5.1.pdf/, accessed\nJuly 18, 2020.\n[223] B. E. Stephens, A. L. Cox, W. Felter, C. Dixon, and J. B. Carter,\n“PAST: scalable ethernet for data centers,” in Proc. of ACM CoNEXT,\nDecember 10 - 13, 2012, Nice, France , C. Barakat, R. Teixeira, K. K.\nRamakrishnan, and P. Thiran, Eds., pp. 49–60.\n[224] “P4switch,” https://p4.org/, accessed July 18, 2020.\n[225] C. H. Song, P. G. Kannan, B. K. H. Low, and M. C. Chan, “Fcm-\nsketch: generic network measurements with data plane support,” in\nProc. of ACM CoNEXT, December, 2020, Barcelona, Spain , D. Han\nand A. Feldmann, Eds., pp. 78–92.\n[226] “Open vswitch,” http://www.openvswitch.org/, accessed July 18, 2020.\n[227] B. Pfaff, J. Pettit, T. Koponen, E. J. Jackson, A. Zhou, J. Rajahalme,\nJ. Gross, A. Wang, J. Stringer, P. Shelar, K. Amidon, and M. Casado,\n“The design and implementation of open vswitch,” vol. 40, no. 2, 2015.\n[228] G. Einziger, M. C. Luizelli, and E. Waisbard, “Constant time weighted\nfrequency estimation for virtual network functionalities,” in Proc. of\nIEEE ICCCN, July 31 - Aug. 3, 2017, Vancouver, BC, Canada , pp.\n1–9.\n[229] J. Liu, P. Zhang, H. Wang, and C. Hu, “Countermap: Towards generic\ntrafﬁc statistics collection and query in software deﬁned network,” in\nProc. of IEEE/ACM IWQoS, June 14-16, 2017, Spain , pp. 1–5.\n[230] H. Xu, Z. Yu, C. Qian, X. Li, Z. Liu, and L. Huang, “Minimizing\nﬂow statistics collection cost using wildcard-based requests in sdns,”\nIEEE/ACM Trans. Netw. , vol. 25, no. 6, pp. 3587–3601, 2017.\n[231] R. Jang, D. Cho, Y . Noh, and D. Nyang, “Rﬂow+: An sdn-based\nWLAN monitoring and management framework,” in Proc. of IEEE\nINFOCOM, May 1-4, 2017, Atlanta, GA, USA , pp. 1–9.\n[232] X. Chen, S. L. Feibish, M. Braverman, and J. Rexford, “Beaucoup:\nAnswering many network trafﬁc queries, one memory update at a time,”\ninProc. of ACM/IEEE SIGCOMM, August 10-14, 2020, Virtual Event,\nUSA, pp. 226–239.\n[233] Q. Huang, H. Sun, P. P. C. Lee, W. Bai, F. Zhu, and Y . Bao, “Omnimon:\nRe-architecting network telemetry with resource efﬁciency and full\naccuracy,” in Proc. of ACM/IEEE SIGCOMM, August 10-14, 2020,\nVirtual Event, USA , pp. 404–421.\n[234] M. Moshref, M. Yu, and R. Govindan, “Resource/accuracy tradeoffs in\nsoftware-deﬁned measurement,” in Proc. of ACM SIGCOMM HotSDN,\nAugust 16, 2013, The Chinese University of Hong Kong, Hong Kong,\nChina , N. Foster and R. Sherwood, Eds., pp. 73–78.\n[235] S. Lai, X. Yuan, J. K. Liu, X. Yi, Q. Li, D. Liu, and S. Nepal,\n“Oblivsketch: Oblivious network measurement as a cloud service,” in\nProc. of The Internet Society NDSS, February 21-25, 2021 .\n[236] O. Alipourfard, M. Moshref, and M. Yu, “Re-evaluating measurement\nalgorithms in software,” in Proc. ACM HotNet, November 16 - 17,\n2015, Philadelphia, PA, USA , J. de Oliveira, J. Smith, K. J. Argyraki,\nand P. Levis, Eds., pp. 20:1–20:7.\n[237] S. Li, L. Luo, D. Guo, and P. Fu, “D-sketch: A differentiated sketch\nstrategy for double-stack networks,” IEEE Commun. Lett. , vol. 25,\nno. 3, pp. 817–820, 2021.\n[238] “Software-deﬁned networks: A systems approach–software-deﬁned\nnetworks: A systems approach version 2.0-dev documentation,” https:\n//sdn.systemsapproach.org/index.html, (Accessed on 07/11/2021).\n[239] J. Yen, J. Wang, S. Supittayapornpong, M. A. M. Vieira, R. Govindan,\nand B. Raghavan, “Meeting slos in cross-platform NFV,” in Proc.\nof ACM CoNEXT, December, 2020, Barcelona, Spain , D. Han and\nA. Feldmann, Eds., pp. 509–523.\n[240] G. Yang, H. Jin, M. Kang, G. J. Moon, and C. Yoo, “Network\nmonitoring for SDN virtual networks,” in Proc. of IEEE INFOCOM,\nJuly 6-9, 2020, Toronto, ON, Canada , pp. 1261–1270.\n[241] J. Nam, J. Seo, and S. Shin, “Probius: Automated approach for VNF\nand service chain analysis in software-deﬁned NFV,” in Proc. of ACM\nSOSR, March 28-29, 2018, Los Angeles, CA, USA , pp. 14:1–14:13.\n[242] “Preface–5g mobile networks: A systems approach version 1.1-\ndev documentation,” https://5g.systemsapproach.org/preface.html, (Ac-\ncessed on 07/11/2021).\n[243] D. Kreutz, F. M. V . Ramos, P. J. E. Ver ´ıssimo, C. E. Rothenberg,\nS. Azodolmolky, and S. Uhlig, “Software-deﬁned networking: A com-\nprehensive survey,” Proc. IEEE , vol. 103, no. 1, pp. 14–76, 2015.\nShangsen Li received the B.S. degree in automa-\ntion, in 2019, from the Northeastern University,\nShenyang, China. He is currently working toward\nthe M.S. degree with the College of Systems Engi-\nneering in National university of defense technology.\nHis research interests include network measurement,\nSDN and sketch data structure.\nLailong Luo received the BS, MS, and PhD de-\ngrees from the College of Systems Engineering, Na-\ntional University of Defence Technology, Changsha,\nChina, in 2013, 2015, and 2019, respectively. He is\ncurrently a lecturer with the School of Systems, Na-\ntional University of Defense Technology, Changsha,\nChina. His research interests include data structure\nand distributed networking systems.\nDeke Guo (Senior Member, IEEE) received the\nBS degree in industry engineering from the Beijing\nUniversity of Aeronautics and Astronautics, Beijing,\nChina, in 2001 and the PhD degree in management\nscience and engineering from the National Univer-\nsity of Defense Technology, Changsha, China, in\n2008. He is currently a professor with the College\nof Systems Engineering, National University of De-\nfense Technology. His research interests include dis-\ntributed systems, software-deﬁned networking, data\ncenter networking, wireless and mobile systems, and\ninterconnection networks. He is a member of ACM.\nQianzhen Zhang received his BSc and MSc degrees\nin Computer Science from Zhengzhou University\nand Guangxi University in 2014 and 2018, respec-\ntively. Currently, he is a PhD student at College\nof Systems Engineering at National University of\nDefense Technology, China. His research interests\ninclude Continuous Subgraph Matching, Graph Data\nAnalytics and Knowledge Graph.\nPengtao Fu received the bachelor degree from the\nNational University of Defense Technology, China,\nin 2020, where he is currently pursuing the mas-\nter degree with the Science and Technology on\nInformation Systems Engineering Laboratory. His\nresearch interests include data structure and network\nmeasurement.",
  "textLength": 243642
}