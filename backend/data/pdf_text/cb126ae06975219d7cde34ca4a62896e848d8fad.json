{
  "paperId": "cb126ae06975219d7cde34ca4a62896e848d8fad",
  "title": "Machine Learning for Database Management Systems",
  "pdfPath": "cb126ae06975219d7cde34ca4a62896e848d8fad.pdf",
  "text": "www.ijecs.in  \nInternational Journal Of E ngineering And Computer Science  \nVolume 9 Issue 0 8 August  2020, Page No. 25132-25147 \nISSN: 231 9-7242 DOI : 10.18535/ijecs/v9i0 8.4520 \n  \nSai Tanishq N , IJECS Volume 09  Issue 0 8 August , 2020 Page No.  25132-25147 Page 25132  Machine Learning for Database Management Systems  \nSai Tanishq N  \nGokaraju Rangaraju Institute of Engineering And Technology  \nAbstract: \nMachine Learning (ML) is transforming the world with research breakthroughs that are leading to the progress \nof every field. We are living in an era of data explosion. This further improves the output as data that can be fed \nto the models is more than it h as ever been. Therefore, prediction algorithms are now capable of solving many \nof the complex problems that we face by leveraging the power of data. The models are capable of correlating a \ndataset and its features with an accuracy that humans fail to achie ve. Bearing this in mind, this research takes an \nin-depth look into the of problem - solving potential of ML in the area of Database Management Systems \n(DBMS). Although ML hallmarks significant scientific milestones, the field is still in its infancy. The \nlimitations of ML models are also studied in this paper.  \n1. Introduction  \nA computer program is said to learn from \nexperience E with respect to some class of tasks T \nand performance measure P if its performance at \ntasks in T, as measured by P, improves with \nexperience E [33]. In order to fix the ideas, it is \nuseful to introduce the machine learning \nmethodology as an alternative to the conventional \nengineering approach for the design of an \nalgorithmic solution [34]. The goal of machine \nlearning  is to program computers to use example \ndata or past experience to solve a given problem. \nMany successful applications of machine learning \nexist already, including systems that analyze past \nsales data to predict customer behavior, optimize \nrobot behavior s o that a task can be completed using \nminimum resources, and extract knowledge from \nbioinformatics data. [35] Integrating machine  \nlearning into DBMS is an ongoing effort in both \nacademia and industry. The combination of ML and \nDBMS is attractive because businesses have \nmassive amounts of data residing in their existing \nDBMS [36]. Furthermore, relational operators can \nbe used to pre -process and denormalize a complex \nschema conveniently before executing ML tasks \n[37].  \nDBMS developers have to use heuristics, restrict the problem space, or even rely on human intervention \nto solve problems such as query optimization, \nphysical database des ign optimization, and buffer \nmanagement. The goal of these heuristics and \nrestrictions is not to find an optimal solution for a \nparticular instance, but to find a solution which has \na safe worst -case performance on all cases. \nAlternatively, ML provides a f lexible framework to \nautomatically learn an efficient program to solve \nthese problems for a specific instance without being \nexplicitly programmed by a human developer.  \nThe paper is primarily focused at relational database \nmanagement syst ems (RDBMSs) because they \nremain the most widely used DBMS type. We \nsurvey the existing landscape of ML to solve hard \nprogramming problems in DBMSs. DBMS is \ndivided into three main sub -components in this \npaper: 1) Query Parser, 2) Relational Engine, and 3)  \nExecution Engine. After providing some \nbackground on each of these subcomponents and \ndifferent ML methods, several systems that have \nbeen proposed using ML are identified. These \ninclude systems in Query Parser, Relational Engine, \nand Execution Engine in S ection 3, 4, and 5, \nrespectively. In Section 6, three overarching design \ndecisions are identified that a DBMS developer has \n\n \nSai Tanishq N , IJECS Volume 09  Issue 0 8 August , 2020 Page No.  25116-25147 Page 25132   \n \nFigure 1: Systems surveyed in this paper, DBMS comp onent they optimize, and the design choices they make \n(DL: Deep Learning, RL: Reinforcement Learning).  \nto make when incorporating ML into a DBMS. An \nabstract summary of where the surveyed systems \nfall in this taxonomy is presented in Figure 1. In \nSection 7, we identify three open challenges to ML: \n1) improving robustness, 2) re -thinking the DBMS \narchitectur e, and 3) exploiting transfer learning.  \n2. Background  \nWe provide some background on the database \nmanagement system internals and machine learning \nmethods to help understand the rest of this survey.  \n2.1 Database Management Systems  \n \nFigure 2 : Steps in query processing  We identify three main components of a DBMS: 1) \nQuery Parser, 2) Relational Engine, and 3) \nExecution Engine. To give a brief introduction to \neach of these components, we next explain the \njourney of a typical SQL query.  \nExample Query. Alice is a data analyst at an \nonline e -commerce website and she wants to find \nthe top 10 products in terms of sales revenue. She \nwrites a SQL query using the SQL client in her \nlaptop and submits it to the DMBS for execution.  \nQuery Parser. At the DBMS, the SQL query will \nbe first intercepted by the query parser. The query \nparser will verify the query is 1) free from syntax \nand semantic errors, 2) verify the user is authorized \nto execute the query, and 3) convert the query into \nthe internal fo rmat used by the system.  \nRelational Engine. The parsed query is then sent to \nthe relational engine which outputs an optimal \nquery evaluation plan (QEP) for the given query. It \ndoes so  \nby searching the space of possible query evaluation \nplans and estimating the cost of each option. For \nthis task, it relies on the metadata information and \nstatistics about the data in the DBMS, which has to \n\n \nSai Tanishq N , IJECS Volume 09  Issue 0 8 August , 2020 Page No.  25116-25147 Page 25133   be generated beforehand. QEP dictates the order o f \nexecution of each relational operator and which \nphysical operator implementation to be used.  \nExecution Engine. Finally, the QEP is sent to the \nexecution engine for execution, which is \nresponsible for managing all the low -level system \nresources such as m emory buffers and thread pools, \naccessing the data and indices, and coordinating the \nexecution of the query either on a single machine or \non multiple machines. It also handles concurrency \ncontrol and failure recovery of the DBMS. The \noutput generated by ex ecuting the query is sent back \nto Alice.  \nDatabase Administrator. Being complex software \nsystems, DBMSs require a significant amount of \ntuning to achieve good performance for a particular \nuse case. To worsen the situation, the set of default \nconfigurations  in the DBMS are often obsolete and \ndo not match the resource availability of modern \nhardware (e.g., MySQL default buffer pool size is \n128MB!). Thus, in Alice’s use case, the DBMS has \nto be tuned to the schema of the database and the set \nof widely used que ries by the analysts in her \ncompany. Furthermore, for query optimization the \nrelevant data statistics have to be generated \nbeforehand and kept up to date. Typically, these \ntasks are performed by a database administrator \nwho has specialized knowledge about the internals \nof the DBMS.  \n2.2 Machine Learning Methods  \nFor the purpose of this paper, we divide machine \nlearning into two major model families: 1) deep \nlearning and 2) classical machine learning.  \nDeep Learning. Deep learning (DL) [1] is the name \ngiven to the family of ML models that are \ncomposed of artificial neural networks (ANNs). \nANNs are inspired by the structure and function of \nthe human brain. They learn a hierarchy of \nparametric features using layers of va rious types \n(e.g., fully connected, ReLU). All parameters are \ntrained using a technique called back -propagation. \nTraining a deep learning  \nmodel incurs massive costs: they typically need \nmany GPUs for reasonable runtimes, huge labeled \ndatasets, and complex hyper -parameter tuning. \nRecently, DL methods have been able to produce \nsuperior accuracy results outperforming other ML \nmethods on hard ta sks like computer vision and natural language processing. In some cases, they \nhave even surpassed the human -level accuracy.  \nClassical Machine Learning. Despite many \nsuccesses using DL -based ML methods, in many \napplications that involve working with struct ured \ndata, ML model families like generalized linear \nmodels, decision tree models, and Bayesian models \nare widely used. Typically, these model families are \ncollectively referred to as classical machine \nlearning , a term coined to contrast them with DL \nmodel s. Unlike DL models, which can all be trained \nusing back -propagation method, each sub -family in \nclassical ML uses different statistical learning \nfoundations and learning methods. Furthermore, \ntheir characteristics are also significantly different \namong dif ferent sub -families.  \nFor the above two ML model families, we further \nidentify three different learning paradigms: 1) \nsupervised learning, 2) unsupervised learning, and \n3) reinforcement learning.  \nSupervised Learning. In supervised learning, \ntraining data consists of a set of input (also called \nfeatures) and output pairs. The goal of the ML \nmodel is to learn a prediction function that takes in \nunseen inputs and predicts an output value such that \nthe discrepancy between the predicted value and the \nactual val ue corresponding to the unseen input is \nminimized. Supervised learning is applicable in \nsettings where there is a direct observable mapping \nbetween input and output, a large amount of \ntraining data available, and we are confident that the \ntraining data cov ers the entire data distribution.  \nUnsupervised Learning. In unsupervised learning, \nthe training data contains only the input and no \nexplicit output. The purpose of the ML model is to \nlearn a function that can discern the latent structure \nof the inputs. Gi ven an unseen test input, the trained \nunsupervised ML model can be used to predict the \nstructural properties of the input. Popular  \napplications of unsupervised learning include \nprobability density function estimation and data \nclustering.  \nReinforcement Learning. The goal of \nreinforcement learning methods is to learn a \nfunction that takes in an input state and generates an \naction that will maximize the overall cumulative \nreward (not the immediate reward as in supervised \nlearning). Unlike sup ervised learning methods, they \n\n \nSai Tanishq N , IJECS Volume 09  Issue 0 8 August , 2020 Page No.  25116-25147 Page 25134   do not make any assumptions about the data \ngenerating process and also do not require a training \nset of state -action (input -output) pairs to be \npresented. State -action pairs are generated as the \nmodel interacts with the envir onment and models \nuse an explore -exploit paradigm to learn while \nbeing used to make predictions at the same time. \nReinforcement learning techniques are useful in \nsettings where the reward of action is not directly \nobservable and we want the model to try di fferent \nthings and pick the best option. However, as there is \nno initial assumption on the data generating process \n(e.g., through training data), in some problems \nreinforcement learning method will fail or take a \nvery long time to learn.  \n3. Query Parser  \nThe task of the query parsing sub -component in a \nDBMS is to check whether a given SQL query in \ntext format is free from syntax and grammar errors \nand if so, translate it into a relational algebra \nexpression, which is an already solved problem. \nHence, most of the new work on query parsing \nfocuses on supporting query modalities beyond text \nand relaxing the grammar of SQL to support natural \nlanguage -based querying. Recent advancements in \nnatural language processing using deep learning \ntechniques provide a prom ising opportunity to \nachieve these goals.  \n3.1 Expanding the Query Modalities  \n3.1.1 Speaking SQL Queries  \nSpeakQL [2] provides a speech -driven querying \ninterface, which can be used to query data by \nspeaking out a SQL query instead of typing it. It \nuses an off the shelf automatic speech recognition \nengine to compile a spoken SQL query into text  \nform and use knowledge about the schema of the \nunderlying data to refine the structure and the \nliterals of the query. SpeakQL is able to capture \ncomplex SQL queries but puts significant cognitive \nload on the user when dictating such queries.  \n3.1.2 Speakin g Natural Language Queries  \nSeq2SQL [3] and SQLNet [4] are two systems that \nfocus on compiling natural language queries into \nSQL. Seq2SQL uses a large dataset (n=84,000) of \nmanually annotated natural language -SQL pairs and \ntrains a deep reinforcement learn ing model to \ncompile natural language queries into SQL. \nSQLNet uses the same training dataset and uses a neural machine translation approach. While these \nsystems have shown some early promising results \nfor supporting natural language queries over single \ntable data, their accuracy significantly suffers for \ncomplex queries that involve joins over multiple \ntables.  \n4. ML for Relational Engine  \nRelational Engine is one of the most important \ncomponents in a database management system, that \nhas been extensively studied for the last 40 years. \nWe identify three different sub -areas of ML \napplications in the relational engine: ML for (1) \nquery optimiza tion, (2) physical database design \nautomation, and (3) approximate query processing. \nNext, we discuss some of the most prominent works \nin each of these sub -areas.  \n4.1 Query Optimization  \nThe goal of query optimization is to transform an \ninput relational algebra expression into an optimal \nquery evaluation plan (QEP). To achieve this, \ntraditional query optimizers perform a search over \nthe space of potential QEPs and pick the one with \nthe lea st total cost. However, estimating the total \ncost is a complex task and it is often approximated \nby estimating the total size of the intermediate \ntuples generated during query evaluation. This is \ncalled the cardinality estimation problem, which \nremains sti ll an unsolved problem despite \nadvancements over many decades. Database \noptimizers often make assumptions such as \nuniformity, independence, and the principle of \ninclusion [5] to perform cardinality  \nestimation. These assumptions often do not hold in \nreal d ata and optimizers tend to under - or over - \nestimate the query cost and pick sub -optimal query \nevaluation plans, which can be worse by orders of \nmagnitude. We found that the use of ML for query \noptimization can be broadly divided into two major \napproaches. The first approach is to train ML \nmodels for cardinality estimation and integrate them \nwith the existing search strategies in the optimizer. \nThe second approach is to completely replace the \ntraditional query optimizer by using ML to generate \nthe QEP, end -to-end.  \n4.1.1 Cardinality Estimation  \nWe identify three different methods for learning \nML models for cardinality estimation.  \nPredicate Level Models. LEO [6] is one of the \n\n \nSai Tanishq N , IJECS Volume 09  Issue 0 8 August , 2020 Page No.  25116-25147 Page 25135   very first systems developed to use ML techniques \nto improve the cardinality estimation. It uses the \noptimizer estimated cardinalities and the \ncardinalities observed during execution to learn \npredicate level linear models to predict t he correct \noutput cardinalities. However, not accounting for \nQEP structure and using the optimizer estimated \ncardinality as the only feature prevents LEO from \nachieving high accuracies.  \nSub-graph Level Models. CardLearner [7] extends \nthe idea of LEO and b uilds ML models to predict \ncardinalities of commonly occurring query \ntemplates instead of each predicate. A template is a \nfamily of queries with the same structure but \nvarying parameters and inputs. Instead of training \none single model for all commonly occ urring query \ntemplates, it trains separate ML models for each \ntemplate. CardLerarner uses several hand -\nengineered features including metadata features, \ninput cardinalities of all input datasets, and features \nassociated with operators. Operating on query \ntemplates enables CardLearner to capture the \nsemantics of the QEP and correlation in data and \nyield better results compared to predicate level \napproaches such as LEO. However, its accuracy \ndegrades when faced with new queries and queries \nwhich have unseen su b- graphs.  \nGraph Level Models. One could also train ML \nmodels to predict the output cardinality of entire \nQEPs. QPPNet uses a novel neural network \narchitecture that matches the QEP structure. It is \ncomposed of stacking sub -neural -modules \ncorresponding to each operator in the QEP which \ntakes in hand -engineered features as well as output \nfrom other sub - neural -modules. As a result, it can \nlearn the correlation in data, relationships between \noperator features, and plan structure to predict the \nQEP cost more accurately. However, one of the \nlimitations of replacing just the cardinality \nestimation component in the optimizer is that the \nmodel will have zero knowledge about the plans \nthat were never generated by the optimizer, which \nlimits the optimizer’s ability to generate new and \nbetter plans.   \nThus, instead of learning from query workloads, \nNaru [8], and DeepDB [9] try to solve the \ncardinality estimation problem by modeling the \njoint probability distribution of the data. Naru uses \ndeep autoregressive models and combines it with a novel Monte C arlo integration scheme to efficiently \nsupport range and wildcard queries. DeepDB uses \nRelational Sum -Product Networks (RSPNs), a \nvariant of a probabilistic graphical model, to model \nthe joint probability distribution.  \nIt is important to mention that some  of the above \nsystems (e.g., LEO in IBM DB2 and CardLearner in \nMicrosoft SCOPE) have been (or are being) used in \nenterprise systems.  \nQuery Evaluation Plan Generation The QEP \nsearch strategy in the optimizer closely resembles \nthe reinforcement learning (RL) methods in \nmachine learning. Hence, several systems have tried \nto replace the entire QEP generation process using \nRL instead of using ML models to augment an \nexistin g optimizer. This ability to learn from the \nfeedback from the chosen QEPs enables RL models \nto avoid choosing the same bad QEP over and over \nagain. We identify two different methods of using \nRL for generating QEPs end -to-end.  \nIntra -Query Classical RL. SkinnerDB [10] \nproposes an intra -query regret -bounded RL learning \nstrategy to find the optimal join ordering for a QEP.  \nInstead of learning from past query executions, it \nuses UTC algorithm [11] to learn from the current \nquery execution to optimize the remai ning of the \ncurrent query. While this approach incurs some \noverheads due to cold starting for every query, the \noverall overheads remain negligible as it can avoid \ncatastrophic join order choices.  \nDeep RL. ReJoin [12] and Neo [13] use recent \nadvancements in deep reinforcement learning to \ngenerate optimal QEPs. ReJoin uses the existing \ncost model of the optimizer to learn a policy \nnetwork that can outperform the optimizer search \nstrategy after more online tra ining. Neo uses the \nobserved latency of QEP executions to learn a value \nnetwork to predict the latency of any new QEP. To \nreduce learning time and avoid choosing and \nevaluating prohibitively expensive join orders, Neo \nbootstraps the value network using lat encies \nobserved for the QEPs generated by a traditional \nquery optimizer.  \nWhile the above systems have shown some early \npromising results on the ability of ML techniques to \nreplace the traditional query optimizers, a vast \nnumber of important challenges rem ain open to \nenable practical adoption. For example, these \n\n \nSai Tanishq N , IJECS Volume 09  Issue 0 8 August , 2020 Page No.  25116-25147 Page 25136   systems make simplifying assumptions on the \ngrammar of supported queries, do not support \nphysical operator selection, and in some cases \nassumes the availability of customized execution \nengines (e.g. , SkinnerDB [10]).  \n4.2 Physical Database Design Automation  \nOne of the most important properties of DBMSs is \nphysical data independence. This allows changing \nthe physical structure of the database without \nrequiring to change the user queries. However, the \nphysical database design significantly affects the \nperforman ce and has to be tuned for a specific use \ncase. Physical database design choices include \ncreating indices, selecting materialized views, and \nselecting data partitioning. Traditionally, this has \nbeen the responsibility of database administration \npersonnel a nd they have used heuristics and human \njudgment to perform these tasks. Several systems \nhave used ML techniques that learn from workload  \npatterns either to develop decision support systems \nfor database administrators or to automate the \nprocess. We identif y two different methods for \nintegrating ML for physical database design \nautomation.  \n4.2.1 Reactive Systems  \nSQL Server AutoAdmin [14] system is one of the \nvery first systems to adopt ML techniques for the \nphysical database design process. It adopts a \nreactive strategy where it takes in a historical query \nwork -load and searches for a configuration that \nminimizes the cost of execution of the workload and \nrecommends that to the database administrator. The \nchosen configuration dictates which indices and \nmaterialized -views to be created and which data \npartitioning scheme to be used. The search strategy \nuses a variant of frequent itemsets mining \ntechniques to efficiently explore the enormous \nsearch space generated by a large number of \npossibilities. The search strategy also requires \nestimating the cost of a new configuration without \nactually executing the workload. To a chieve this, \nAutoAdmin extends the query optimizer’s cost \nmodel to support what -if queries which can assume \nthe presence of a selected set of configurations \n(either hypothetical or materialized) and ignore the \npresence of other configurations. However, due  to \nthe well -known limitations of the optimizer’s cost \nmodel, a configuration that the optimizer thinks is \nbetter than others can be worse when implemented. In a followup work [15], AutoAdmin uses ML \nmodels trained on past QEP execution experiences \nto obta in confidence in the selected configuration \nbefore making the actual change. It does so by \nformulating a classification problem to predict \nwhether the new configuration will be better than \nthe current configuration.  \n \n4.2.2 Proactive Systems  \nConfigurations  chosen by reactive systems like \nAutoAdmin may be sub -optimal for the workload in \nthe near future. On the contrary systems like \nQB5000 [16] and DQM [17] takes a proactive \nstrategy for physical database design that \ncompletely automates the process without a ny \nintervention of a human. QB5000 trains ML  \nmodels to predict the query workload to the future \nand uses that to select the best set of indices. DQM \ntrains a deep RL model to learn a policy to \nopportunistically select and evict materialized views \nsubject to storage constraints, that will have the \nmost benefit into the future. While these systems \nhave shown promising initial results, much work is \nstill needed to improve the robustness before \nincorporating them into enterprise systems.  \n4.3 Approximate Query Processing  \n \nFigure 3 : Input Data Size vs Query Response \nTime graph  \nWhile the ever -growing volumes of data enable us \nto glean unprecedented insights, the associated high \ncomputational and resource costs often become a \nbottleneck. Approximate query processing (AQP) \ntechniques try to mitigate this issue by generating \napproxi mate answers to the original query at a \nfraction of the time and cost of the original query \nexecution. The conventional approach to AQP is to \nuse query time data sampling and data statistics to \n\n \nSai Tanishq N , IJECS Volume 09  Issue 0 8 August , 2020 Page No.  25116-25147 Page 25137   answer queries. After the query is executed, the \nwork done for  that query is never reused. However, \nevery new query execution reveals some new \ninformation about the data and executing more and \nmore queries over time enables us to refine that \ninformation even further. Thus, machine learning \ntechniques provide an inter esting opportunity to \nlearn from past query executions and use that \nlearning to approximately answer future queries. \nWe identify two different methods of using ML for \nAQP.  \n4.3.1 Augmenting Existing AQP Components \nVerdict [18] is one of the first systems t o apply this \ntechnique in the context of AQP. It uses a data \nstructure called Query Synopsis to store the past \nquery results and uses it to refine the approximate \nanswer generated by the system for new queries. \nThus, Verdict can reduce the runtime required  for \nan approximate query for specified error bound or \nreduce the error bound for an approximate query \nwith a specified time budget. This is achieved by \nmodeling a multivariate normal distribution model \nusing the principle of maximum entropy.  \n4.3.2 Replac ing Existing AQP Components \nInstead of learning from past queries and \naugmenting sampling -based methods with ML \nmethods at execution time, DBEst [19] takes a pure \nML-based approach for answering AQP queries. It \nsamples data corresponding to each predicate \nattribute and group by attribute values and trains \nregression and density models. At execution time it \nuses the corresponding models and performs \nintegration over those models to generate AQP \nresult. Similarly, DeepDB [9] also uses ML \nmethods to learn the joint probability distribution of \nthe data, which can be used to answer AQP queries. \nThe type of the ML models used by DeepDB is \ncalled Relational Sum -Product Networks (RSPN), \nwhich belongs probabilistic graphical model family. \nGiven a relational database and the correlations \nbetween columns, DeepDB trains RSPNs for the \ntables and use the RSPN ensemble to answer SQL \nqueries by compiling them into inference \nprocedures over the RSPNs.  \n \n5. ML for Execution Engine  \nIn this section, we identify several prominent works \nthat use ML methods to either augment or replace critical components in a database execution engine. \nWe categorize them into three major sub -areas: ML \nfor 1) knob tuning, 2) scheduling and resource \nprovi sioning, and 3) data structures and algorithms.  \n5.1 Knob Tuning  \nThe performance of the database execution engine \nis highly dependant on the chosen values of the \ntunable knobs which control nearly all aspects of \nthe runtime  \nOperations . For example, these knobs control \naspects such as how much memory to be used for \ncaching data versus transaction log buffer, how \noften the data to be written to the disk, and things \nlike query execution parallelism. Similar to physical \ndata- base design, finding a good knob \nconfiguration for a target query workload is \ngenerally the responsibility of database \nadministration personnel, for which they either use \ncommon sense heuristics and/or a trial and error \nprocedure. Alternatively, one could use  ML \ntechniques to automatically find an optimal knob \nconfiguration for a target query workload.  \niTuned [20] approaches this problem by executing a \nseries of carefully -planned experiments and picking \nthe configuration which minimizes the overall \nworkload t ime. Given a target query workload \niTuned uses Latin hypercube sampling to pick an \ninitial set of configurations and execute them to \nobtain the execution time. The results are then \nmodeled using a Gaussian process representation to \npick the next configurat ion to be executed. This \nprocess continues until a good enough configuration \nis found. It also uses several techniques to reduce \nthe overall tuning time including early elimination \nof configurations with insignificant improvements, \nexecuting parallel exper iments, and compressing the \nquery workload.  \nSimilarly, OtterTune [21] also executes a series of \nexperiments chosen by modeling a Gaussian \nrepresentation process. However, instead of starting \nwith a sampled set of initial configurations, it maps \nthe curren t workload to a similar previous workload \nwhich has been already tuned and uses that to pick \nthe next experiment configuration. Leveraging the \nlearning from previous workloads helps OtterTune \nto finish the tuning process much faster than \niTuned. Workload m apping is achieved through a \nworkload characterization step which combines \n\n \nSai Tanishq N , IJECS Volume 09  Issue 0 8 August , 2020 Page No.  25116-25147 Page 25138   factor analysis (over metrics obtained through \ndatabase monitoring tools) and k -means clustering. \nIt also prunes irrelevant knob configurations ranked \nusing Lasso feature selection and incrementally \nincrease the number of tuned configuration based \non their importance as tuning progresses.  \niBTune [22] is a system for reducing buffer pool \nsizes of cloud OLTP database instances to reclaim \nmemory while conforming to the service level \nagreements (SLA) on query response time. It \niteratively uses data from other instances with \nsimilar workloads to  choose a target buffer pool size \nusing large deviation analysis. But before making \nthe change, it uses a pairwise deep neural network \nto predict the new response time and proceeds only \nif the predicted value is within the SLA. One of the \nmajor bottlenecks  for ML -based knob tuning \nmethods is not having access to low -level/sub - \ncomponent level system performance metrics. \nWhile database systems do provide metrics, they \noften happen to be aggregated at the entire system \nlevel and are less informative to tune s ub-\ncomponent level knobs. Another bottleneck is the \ninability to change system configuration values \nwithout complete system restarts. Database systems \nare not designed to be tuned by iteratively executing \nmultiple experiments. But, ML -based methods \nrequire  on the fly configuration testing which incurs \nsignificant overheads.  \n5.2 Resource Provisioning and Scheduling  \nExecution engines have to make several planning \ndecisions to meet the service level objectives \nimposed by users as they execute QEPs. This \nincludes resource provisioning : deciding how many \nmachines to be used for execution and/or \nscheduling : deciding which QEP to be executed on \nwhich worker and in which order. The emergence \nof cloud computing environments makes these \nplanning decisions even more important because of \ntheir pay - as-you-go model. The existing approach \ntaken by many database systems is to use human -\nengineered rules or heuristics -based approaches, \nwhich are often too rigid and slow to respond to the \nrapid and dynamic query workloads. A lternatively, \nML provides an opportunity to automatically learn \nthese planning decisions using past query \nworkloads. We identify two different settings of \napplying ML for resource provisioning and \nscheduling.  5.2.1 Provisioning and Scheduling for Batch \nProcessing  \nWiSeDB [23] is a workload management service \nfor cloud databases, which holistically addresses \nboth the resource provisioning and scheduling \nproblems. It takes in a query workload, the runtime \nof each  query on each machine type, a user -defined \ntarget performance goal (e.g., average/max latency, \npercentile -based metrics), and recommends a set of \nstrategies and their associated costs to the user. \nUnder certain assumptions, solving the original \nplanning p roblem can be reduced to the bin packing \nproblem which is an NP -hard problem. WiSeDB \nsamples a large number of small workloads from \nthe original workload and solves them using a brute \nforce graph search algorithm. It then extracts \ndecisions and features fr om each of the decisions \nmade by the graph algorithm and trains a decision \ntree model. Finally, this decision tree model is used \nto generate the planning strategies and their \nassociated costs for the original work -load. By \nusing a learning -based adaptive s trategy, WiSeDB \ncan outperform many heuristic -based techniques \nwith little training overhead. One of the major \nlimitations of WiSeDB is that it requires the user to \nprovide cost estimates for each query, and \naccurately estimating the cost of previously uns een \nqueries is an open challenge.  \n5.2.2 Provisioning and Scheduling for Online \nProcessing  \nSimilar to WiSeDB, Bandit [24] is another system \nfor solving the provisioning and scheduling problem \nof cloud database systems which focuses on online \nscheduling rather than batch scheduling. It models \nthe planning problem as a multi -tiered contextual \nmulti-arm bandit problem (CMAB), a well -known \nreinforcement learning technique. The tiers in the \nCMAB corresponds to the different VM types and \nthey are organized in the descending order of their \ncapacity/cost. The goal of the CMAB is to reduce \nthe overall cos t, which can be both monetary cost or \ncost due to not meeting a deadline. When a new \nquery arrives, starting from the first  \nVM of the first tier the CMAB model iteratively \nmakes one of three choices,  \n1) Assign  the query to the current machine,  \n2) Send  the query to next machine of the current \ntier,  \n\n \nSai Tanishq N , IJECS Volume 09  Issue 0 8 August , 2020 Page No.  25116-25147 Page 25139   and  \n3) Send  the query to the next tier. As the context, it \nuses features from both the query and the current \nstate of the VM. Hence, it can implicitly estimate \nthe cost of a query as part of the learnin g problem \nand support previously unseen queries. PerfEnforce \n[25] also uses ML techniques based on \nreinforcement learning and multi -layer perceptrons \nto automatically scale the size of a data processing \ncluster to meet the user -defined target performance \ngoal.  \nWhile the above systems have shown promising \nresults on the ability to replace provisioning and \nscheduling components using ML, there has not \nbeen much adoption in real -world systems, and \nthere is a reluctance among systems developers to \nput scalability de cisions in the hands of machine \nlearning algorithms. One of the major reasons for \nthis is the inability of ML techniques to provide \nbounds on the worst -case scenario. But as ML \ntechniques get more robust, it can be expected that \nexisting heuristics -based p lanning modules will get \nreplaced by learned components.  \n5.3 Data Structures and Algorithms  \nEvery operation that is executed by the execution \nengine heavily relies on core data structures such as \nindex structures and algorithms such as sorting. \nThese dat a structures and algorithms do not make \nany assumptions on the distribution of the data and \ngive guarantees on the worst -case performance. \nHowever, in some cases, if we know certain \nproperties about the distribution of the underlying \ndata, it is possible t o come up with data structures \nand algorithms that can yield superior performance. \nML provides a flexible framework for learning the \nempirical data distribution and a recent line of \nresearch has shown the possibility of using ML \nmodels to replace core data  structures and \nalgorithms in DBMSs.  \n \n5.3.1 Learned B -Tree Indices  \nLearned Index [26] is the first system to propose the \nidea of using ML models to replace core data \nstructures and algorithms. It focussed mainly on \nreplacing B -Tree indices. Given a key, what a B - \nTree index essentially does is finding its position on \na sorted list using a series of tree traversals. B - \nTrees also give a guarantee on the error on the selected position: the maximum error is bounded by \nthe page size. In this sense, a B -Tree index is a \nmodel that captures the cumulative distribution \nfunction (C DF) of the key values. Thus, it is \npossible to replace the B -Tree index with an ML \nmodel that is trained to capture the CDF of the keys. \nThe error guarantee of the ML model can be found \nduring training, which is the maximum training \nerror for any key. Inte restingly, unlike other ML \napplications, the objective here is to minimize the \ntraining error and not the generalization error. The \nadvantage of replacing a B -Tree index using an ML \nmodel is that it reduces both the lookup time and \nmemory footprint of the index.  \nFITing -Tree [27] is another system that replaces B - \nTree indices using learned ML models. It uses a set \nof piece -wise disjoint linear functions to \napproximate the CDF distribution of the keys and \nuses a conventional B -Tree index to map a key to \nthe correct  linear function. Using linear functions \nhelps FITing -Tree achieve fast look -ups and also \nsupport inserts, which was one of the major \nlimitations of the Learned Index system. It also \nprovides a tunable knob to make the index optimize \neither for faster look -ups or smaller memory \nfootprint.  \nXIndex [28] is a concurrent learned index that \nsupports read, write, and update operations. The \narchitecture of XIndex is similar to that of FITing - \nTree. However, the root node in XIndex also uses a \nlinear model, unlike the B -Tree index in the FITing - \nTree. For enabling concurrent updates it uses a delta \nindex and periodically runs a two -phase compaction \nscheme to merge and copy the delta index with the \nmain index structure. Concurrency during this \ncompaction stage is enabled through classical \nconcurrency constructs such as the read -copy -\nupdate  \nbarrier. In addition to this, XIndex also adapts its \nstructure at runtime to optimize for the query \ndistribution. During learning, both Learned Index \nand FITing -Tree try to minimize the worst -case \nerror. In practice , most query workloads are highly \nskewed and their runtime performance will be \ndetermined by the performance on a small set of \nhotkeys. XIndex monitors the observed error during \nruntime and tries to split the regions that observe \nhigh error into multiple m odels to reduce the error.  \n\n \nSai Tanishq N , IJECS Volume 09  Issue 0 8 August , 2020 Page No.  25116-25147 Page 25140   5.3.2 Other Learned Data Structures and \nAlgorithms  \nLearned Index system also proposed methods to \nreplace several other DBMS data structures and \nalgorithms using learned ML models such as \nlearned hash maps, sorting, and bloom filters. One \nof the main issues with hash map structures is \ngetting hash collision s which increases the latency \nof retrievals. An ML model that captures the CDF \ncan be used to replace the hash function and thus \nreduce the number of hash collisions. The same \nCDF model can be used to sort the data more \nefficiently by first ordering the da ta in nearly sorted \norder and then using insertion/bubble sort. Bloom \nfilters can be replaced by training a classification \nmodel for which the decision threshold is chosen \nsuch that the false -negative rate is zero. While these \nsystems have shown initial pr omising results for the \nfeasibility of replacing core data structures and \nalgorithms using ML models, much work is still \nneeded to make them available for enterprise \nDBMSs.  \n6. Design Choices  \nWe discuss three main overarching design choices \nthat one has t o make when integrating ML \ncomponents into DBMSs:  \n1) integration mode, 2) learning source and 3) \nchoice of ML paradigm.  \n6.1 Integration Mode  \nWe found that there are two main engineering \napproaches for integrating ML components into \nDBMSs: external vs. internal integration [29].  \n6.1.1 External Integration  \nModern DBMSs are complex systems and they \nallow human database administrators to control the \nquery execution performance by (1) optimizing the \nphysical database design, (2) providing query \noptimization hints, (3) knob tuning, and (4) resource \nprovisioning.  They also provide information about \nthe system such as resource usage, query traces, and \nperformance metrics. Under this context, the focus \nof externally integrated ML components is to \nprovide recommendations to human database \nadministrators or replace th em and automatically \nperform the tasks using the standard configuration \nendpoints provided by the DBMS.  \nAn enterprise -grade DBMS typically requires \ndecades of highly advanced software development efforts and thus there is huge resistance among \nDBMS develo pers to integrate new components that \nrequire significant architectural changes. External \nintegration keeps the ML components outside the \ncritical path of a DBMS and still provides a value. \nFor this reason, most of the systems surveyed in this \npaper fall i nto this category (see Figure 1) and some \nhave even been successfully adopted by several \nenterprise DBMSs.  \nHowever, external integration also faces several \nlimitations. First developing multiple external \ncomponents that operate on different sub -problems \nmay lead to interference among the decisions taken \nby those systems. For example, assume an external \nquery optimization component that hints a specific \nquery plan to DBMS assuming the absence of a \nparticular index. At the same time assume there is \nanother p hysical database design component that \ndecides to create this index which renders the \nchosen evaluation plan become sub -optimal. \nAvoiding this kind of interference requires co -\nordination among different components, which is \ndifficult to implement in extern al components. \nSecond, external components for knob tuning and \nresource provisioning take an iterative approach \nwhere they try out several different settings before \npicking the best option. Existing DBMSs are not \noptimized for  \nsuch rapid experimentation and hence require \nsystem downtimes or restarts for the configurations \nto take effect. This significantly increases the time \nrequired for knob tuning by an ML component. \nFinally, the system information metrics provided by \nthe DBMS s are primarily intended to be consumed \nby human database administrators for diagnosing \nperformance bottlenecks. Thus, they can be too \nhigh- level for ML components to learn from.  \n6.1.2 Internal Integration  \nInternal integration of ML components tries to \nmitigate much of the above -mentioned limitations \nby changing the DBMS architecture to treat ML \ncomponents as first -class components. As a result, \nML components get more access to the low -level \ninformation and more fine -grained control to the \nDBMS. Coherenc e among the decisions taken by \nmultiple ML components inside a DBMS can be \nachieved by having a centralized coordinator that \ntakes suggested actions from different ML \ncomponents and execute them only if they don’t \n\n \nSai Tanishq N , IJECS Volume 09  Issue 0 8 August , 2020 Page No.  25116-25147 Page 25141   interfere with other decisions. However, i nternal \nintegration requires tight coupling between the \ncomponents inside a DBMS and can pose query \nexecution performance degradation when training \nthe ML models. Hence, they are mostly applicable \nto new data system developments that are being \ndeveloped fr om scratch (also called greenfield \nsystems).  \nNoisePage [30] is one such green field system for \nin- memory hybrid transactional and analytic \nprocessing, which can automatically optimize query \nexecution without a human database administrator. \nIt focusses on knob tuning, resource provisioning, \nand physi cal database design optimization. It also \nhas a modular architecture optimized for efficient \noffline training data collection by ML components. \nTraining data for each module (e.g., transaction \nmanager) can be obtained in isolation without the \nneed of going  through the entire DBMS execution \npath. These offline collected data is then combined \nwith the data collected through online query \nexecution to learn ML models. The ML pipeline in \nNoisePage has three main phases: 1) modeling, 2) \nplanning, 3) deployment. I n the modeling stage, it  \nbuilds models to predict the future query workload \nand models to predict the behavior of system \ncomponents under different configuration values. In \nthe planning stage, it uses reinforcement learning to \npick actions based on the mo dels trained in the \nmodeling phase, instead of interacting with the \nactual system. Finally, in the deployment phase, the \nchosen actions are applied and the observed \nperformance metrics are later fed back to modeling \nand planning models to improve their per formance.  \nSageDB [31] is another system that proposes a \nnovel DBMS architecture that uses ML models \ncombined with program synthesis techniques to \ngenerate internal system components like data \nstructures and algorithms. To balance the training \ntime vs. acc uracy it proposes using multiple ML \nmodels each specialized for a particular task. ML \nmodels in SageDB are optimized to capture the \nempirical data distribution of the data and not \noptimized for the ability to generalize to unseen \ndata. These synthesized sy stems components are \nused for optimizing data access (e.g., indices), \nquery optimization (e.g., cardinality estimation), \nand query execution (e.g., sorting).  6.2 Learning Source  \nThe main goal of using ML for DBMS components \nis to improve the performance of the system for \nfuture query work -loads. Thus, one way for \nadopting ML methods is to learn from past or \ncurrent (in the case of reinforcement learning) \nqueries. But the performan ce of the queries is \ndependant on the state of the underlying data in the \nDBMS. Hence, in some cases, it is also possible to \nachieve the same goal by learning directly from the \ndata.  \n6.2.1 Learning from Queries  \nAs shown in Figure 1, learning from query \nworkloads is the most widely used approach for \nintegrating ML into DBMS across all components. \nLearning from queries enables the ML models to \nlearn a narrow -scoped problem which is much \neasier to model/learn and he nce improve the overall \nperformance of the system.  \nHowever, this approach faces three main \nchallenges. First, collecting training data for this \napproach can  \nbe expensive as each query needs to be executed on \nlarge databases. Second, it does not generaliz e well \nfor unseen workload queries and causes significant \nperformance degradation at execution time. Third, \nchanges in the workload patterns or underlying data \nrequire capturing new training data and expensive \nretraining which can cause system downtime.  \n6.2.2 Learning from Data  \nMore recently several systems have been proposed \nthat learn from DB -resident data, instead of query \nworkloads. These systems train models to learn the \nempirical data distribution of the data and use them \nto improve DBMS performanc e. For example, \nDBEst [19] and DeepDB [9] uses probability \ndistribution models to answer AQP queries. Naru \n[8] and DeepDB [9] use joint probability \ndistribution models in the relational engine to \noptimize cardinality estimates. Empirical data \ndistribution models are also the main building block \nin learned data structures and algorithms such as \nLearned Index [26], FITing -Tree [27], and XIndex \n[28].  \nML models trained using data can be reused despite \nchanges in the workload pattern and are also more \nrobust to  small changes in the data. More \nimportantly, DeepDB [9] has shown that the same \n\n \nSai Tanishq N , IJECS Volume 09  Issue 0 8 August , 2020 Page No.  25116-25147 Page 25142   probability distribution model can be used in \nmultiple tasks such as AQP and cardinality \nestimation, reducing the total training time required. \nHowever, accurately capturing t he joint probability \ndistribution of a relational dataset with multiple \ntables is a complex learning task that requires \nmodels with high learning capacity and training \ntime. Furthermore, not all DBMS components can \nbe purely learned from data (e.g., execut ion engine \nknob tuning).  \n6.2.3 Hybrid Methods  \nWhile most of the existing systems fall into one of \nthe above two approaches, it would be interesting to \nexplore the possibility of combining both \napproaches. Some early work in this regard has \nbeen proposed in the XIndex [28] system. XIndex is \na learned i ndex structure that replaces B -tree \nindices. It does so by learning the empirical  \ncumulative distribution function of the keys of the \ndata. During training, the goal is to minimize the \nmaximum error made by the model for predicting \nthe position of a key. But the performance of a \nparticular workload will be dominated by the errors \nmade by the model on the keys that are frequently \nused in the workload. Hence, XIndex performs the \nsecond phase of learning where it dynamically \nfurther minimizes the error on the  most frequently \nused keys.  \n6.3 Choice of the ML Paradigm  \nWe cover the use of different ML model and \nlearning paradigms for integrating ML into DBMSs.  \n \n6.3.1 ML Model Family  \nWe observed two prominent families of ML \nmodels: 1) deep learning and 2) classical ML.  \nDeep Learning. It should be noted that much of the \nrecent renaissance in applying ML methods for \nDBMS internals, and also systems in general, has to \nbe credited to the recent advancements in deep \nlearning. Deep learning models have high model \ncapacities and hence can learn highly complex data \ndistributions. They have shown superior \nperformance in hard tasks such as in natural \nlanguage processing (NLP). As a result, the s ame \ndeep NLP models have been used in systems like \nSpeakQL [2], SeqSQL [3], and SQLNet [4] to \nprovide spoken and/or natural language interfaces \nfor DBMSs. Naru [8] uses a transformer -based deep learning model to learn the joint probability \ndistribution of the data for cardinality estimation. \nNeo [13], DQM [17], and iBTune [22] also use \ndeep learning models for query optimization (QEP \ngeneration), physical database design \n(materialization optimization), and knob tuning \n(buffer size tuning), respectively.  \nHowever, the high model capacity of deep learning \nmodels and their ability to learn highly accurate \nmodels come at a cost. First, these models require \nsignificantly large amounts of training data without \nwhich the models will start to overfit. In many \nDBMS c omponents, collecting large amounts of \ntraining data is expensive as the queries have to be \nexecuted potentially on large databases. Second,  \ndeep learning models are highly compute -intensive \nwhich can require few hours to a few days of \ntraining even when using expensive hardware \naccelerators such as GPUs. This can cause \ndegradation of DBMS query execution \nperformance. Furthermore, typically dee p learning \ninference times are in few hundreds of milliseconds \nand do not match with the performance \nrequirements of the DBMS components such as \ncardinality estimators and indices which have to be \nin the order of few milliseconds. Finally, the \nexplainabili ty/debuggability of deep learning \nmodels is still an active area of research and there is \nminimal understanding of the internal workings of \nthem. As a result, while deep learning -based \nmethods have shown promising accuracy results, \nthey are not yet widely integrated into enterprise \nDBMSs due to the above open challenges.  \nClassical ML. We found that classical ML -based \nmethods are widely used to integrate ML into \nDBMS components and some of them (e.g., LEO \n[6], CardLearner [7], AutoAdmin [14], and \nVerdictDB [18]) have been even integrated in the \nenterprise systems. Classical ML methods \novercome much of the limitations of deep learning \nmethods: they are less compute -intensive to train, \nrequire much less training data, have faster \ninference times, and generate much easy to explain \npredictions. However, most classical ML models \nare known to have fewer model capacities and have \nless predictive power compared to deep learning \nmodels. Thus, their accuracy can be lower.  \n6.3.2 Learning Paradigm  \n\n \nSai Tanishq N , IJECS Volume 09  Issue 0 8 August , 2020 Page No.  25116-25147 Page 25143   We explain the use of three learning paradigms that \nsystems have used to integrate ML into DBMSs: 1) \nsupervised learning, 2) reinforcement learning, and \n3) unsupervised learning.  \nSupervised Learning. Out of the systems we \nsurveyed supervised learning appr oach is the most \nwidely used learning paradigm (see Figure 1). The \ntraining data required for supervised learning is \ncollected either beforehand or continuously during \nquery execution. Seq2SQL [3] and SQLNet [4] use \na large manually generated dataset of SQ L and \nnatural language query pairs. Most other systems  \n(e.g., CardLearner [7], QPPNet [32], Naru [8], \nAutoAdmin [14, 15], and Learned Index [26]) \npreform initial training from previously collected \ntraining data and then perform periodic retraining as \nnew data becomes available or the query workload \nor the data in the DBMS significantly change. In \nsome cases, collecting training data can be \nexpensive as it requires executing a large number of \nqueries potentially on large databases. However, the \nprevalence o f cloud databases has mitigated these \nissues as cloud operators have access to large \namounts of query execution traces from many \ntenants.  \nReinforcement Learning. Reinforcement learning \n(RL) methods are particularly applicable when a \nsystem component has to make a series of decisions \nand the reward of each decision is not directly \nobservable. Thus, RL methods have been used in \ntasks including QEP generation (e.g., S kinnerDB \n[10], ReJoin [12], Neo [13]), materialization \noptimization (e.g., DQM [17]) and scheduling (e.g., \nBandit [24]). RL methods perform on the job \ntraining and collect data as the DBMS execute \nqueries. Initially, they may generate worse results as \nthe models have not converged yet. To overcome \nthis, one could use a bootstrapping strategy called \nlearn by demonstration where the existing DBMS \ncomponent is used to generate initial training data \nfor the RL model to bootstrap. After this initial \ntraining, th e RL model will continue to learn on the \njob and become better than the existing DBMS \ncomponent. For example, Neo [13] showed the \nfeasibility of building an RL -based learned query \noptimizer which surpasses the PostgreSQL DBMS \nquery optimizer, even though t he RL model was \ninitially bootstrapped using it.  Unsupervised Learning. We found that \nunsupervised learning techniques are widely used \nfor DBMS knob tuning in systems like iTuned [20], \nOtterTune [21], and QB5000 [16]. One such \npopular technique is to redu ce the number of \ndifferent queries by performing clustering based on \nquery templates. While DBMS may encounter a \nlarge number of different queries, most of them are \ndifferent parameterizations of the same query \ntemplate. Thus, by reducing the queries into query  \ntemplates the complexity for the ML model can be \nsignificantly reduced. iTuned [20] and OtterTune \n[21] also use Gaussian mixture models, another \nunsupervised learning method, to model the DBMS \nperformance corresponding to different systems \nconfigura tions settings.  \n7. Open Challenges  \nIntegrating ML methods into DBMS components \nhas proven to optimize average system \nperformance. Some systems have already integrated \nML into enterprise DBMSs [6, 7, 18]. However, the \nfield is still in its infancy and requires solving many \nopen challenges to  realize the full potential. Next, \nwe identify three such major open challenges:  \n7.1 Improving Robustness  \nWhile ML methods improve the average query \nexecution performance, they can make \nmispredictions that are significantly off and lend the \nsystem become unrobust. On the contrary, \ntraditional software components are designed to \nminimize the worst -case performa nce cost. Worst - \ncase performance guarantees are a crucial aspect of \nsoftware systems as one single fault can have ripple \neffects and make the entire system unusable \neventually (e.g., the evaluation time difference \nbetween a good QEP and bad QEP can be ord ers of \nmagnitude big). Understanding the worst -case \nbehavior of ML -driven software components is still \nan untouched area and it is possible that coming up \nwith tight theoretical guarantees is a very hard \nproblem.  \nAnother approach to solving the same probl em \nwould be to integrate adaptive query execution \nstrategies with ML -driven components. This \nrequires observing the outcome of the decisions \ntaken by ML -driven components and dynamically \nadjusting them when a performance degradation is \ndetected. Some initi al work on this regard is \n\n \nSai Tanishq N , IJECS Volume 09  Issue 0 8 August , 2020 Page No.  25116-25147 Page 25144   proposed in SkinnerDB [10]. SkinnerDB finds the \nbest join ordering for a query by using an intra -\nquery RL method that switches between different \norders before it finds the optimal one. It also \nprovides worst -case performance guara ntees for this \nmethod. However, this space is still very open and \nmuch  \nwork is needed to make ML -driven DBMS \ncomponents more robust.  \n7.2 Rethinking the DBMS Architecture  \nWhen people designed DBMS architectures several \ndecades ago, enabling autonomous control was not \na design goal. As a result, when one integrates ML \ninto DBMS components, they have to face several \nfundamental architectural limitations. For example, \nthe sepa ration of concerns such that the relational \nengine making all the intelligent decisions and the \nexecution engine passively executing them no \nlonger holds. Decisions taken by the relational \nengine when compiling the QEP may turn out to be \nwrong when executi ng it. Thus, the relational \nengine should be able to observe the performance of \na QEP as it executes and refine the decisions as new \nin- formation becomes available. Such an approach \nwill require tight -coupling between the relational \nand execution engines with feedback -loops.  \nAlso, integrating ML into DBMS components \nrequires the ability to easily experiment and having \naccess to fine -grained system information. In \ncurrent DBMSs, especially with external \nintegration, it is very hard to profile and generate \ntraining data for a specific component without \ninvoking the full QEP execution path, which is \ncostly. Furthermore, the level of system information \nexposed by the DBMS is very coarse -grained. They \nare intended to be consumed by humans for \ndebugging purposes  and are too high -level for ML \nmodel training. Some of these limitations have been \nalready identified and are being actively worked on \n[30].  \nExploiting Transfer Learning  \nThere is limited success in learning transferable \nknowledge that can be reused in multiple different \nsettings. For example, most of the ML models used \nin existing DBMS components perform poorly \nwhen there is a deviation in the query work -load or \na change i n the system context or data. The situation \nis even worse when they are applied to a new DBMS instance or it is not possible to apply to a \nnew instance at all. This significantly increases the \ncost of training and maintaining ML models and \nfaces  \nproblems like cold -start and the need to \ncontinuously retrain to keep up with the changes.  \nTransfer Learning is a technique that can be applied \nto overcome this limitation, which is popular in \nother fields such as computer vision and natural \nlanguage processing. Instead of training separate \nmodels for different tasks from scratch, transfer \nlearning enables us to reuse a master model and \nfine-tune it to the task at hand using limited \nresources (e.g., compute power and training data). \nThis master model is trained on a very large dataset \nso that it can learn most of the relevant information \nfor any related task. For example, ImageNet is a \npopular computer vision transfer learning dataset \nthat has over 1 million hand -labeled images. \nIdentifying and curating such a dataset for DBMSs \nrequire solving several open challenges. For \nexample, one has to select a common represe ntation \nformat that can capture the data schema, data \nstatistics, query structure, and hardware properties. \nCollecting such a large dataset is also a challenge. \nHowever, the migration of DBMSs into the cloud \nprovides a unique opportunity to centrally colle ct all \nthe relevant information.  \n8. Conclusion  \nCombining machine learning (ML) tasks with \ndatabase management systems (DBMS) is an active \nresearch field and there have been many efforts \nexploring this both in research and industry. This \ncombination is at tractive because businesses have \nmassive amounts of data in their existing DBMS \nand there is a high potential for using ML to extract \nvaluable information from it [38]. In addition, the \nrich relational operators provided by the DBMS can \nbe used convenientl y to denormalize a complex \nschema for the purposes of ML tasks [39]. Although \nthe field of machine learning is scaling heights, it is \nridden with several limitations as well. The \nchallenge for machine learning is to recover the \ndiscipline’s original breadt h of vision and its \naudacity to develop learning mechanisms that cover \nthe full range of abilities observed in humans —who \nremain our only example of truly intelligent systems \n[40].  \n\n \nSai Tanishq N , IJECS Volume 09  Issue 0 8 August , 2020 Page No.  25116-25147 Page 25145   References  \n[1.] Ian Goodfellow, Yoshua Bengio, and Aaron \nCourville. Deep Learning . MIT press, 2016.  \n[2.] Vraj Shah, Side Li, Arun Kumar, and \nLawrence Saul. SpeakQL: Towards Speech -\ndriven Multimodal Querying of Structured \nData.  \n[3.] Victor Zhong, Caiming Xiong, and Richard \nSocher. Seq2sql: Generating Structured \nQueries from Natural Language Using \nReinforcement Learning. arXiv preprint \narXiv:1709.00103 , 2017.  \n[4.] Xiaojun Xu, Chang Li u, and Dawn Song. \nSQLNet: Generating Structured Queries \nfrom Natural Language without \nReinforcement Learning. arXiv preprint \narXiv:1711.04436 , 2017.  \n[5.] Viktor Leis, Bernhard Radke, Andrey \nGubichev, Atanas Mirchev, Peter Boncz, \nAlfons Kemper, and Thomas N eumann. \nQuery Optimization Through the Looking \nGlass, and What We Found Running the \nJoin Order Benchmark. The VLDB Journal , \n27(5):643 –668, 2018.  \n[6.] Michael Stillger, Guy M Lohman, Volker \nMarkl, and Mokhtar Kandil. LEO -DB2’s \nlearning optimizer. In VLDB , volume 1, \npages 19 –28, 2001.  \n[7.] Chenggang Wu, Alekh Jindal, Saeed \nAmizadeh, Hiren Patel, Wangchao Le, Shi \nQiao, and Sriram Rao. Towards a Learning \nOptimizer for Shared Clouds. Proceedings \nof the VLDB Endowment , 12(3):210 –222, \n2018.  \n[8.] Zongheng Yang, Eri c Liang, Amog \nKamsetty, Chenggang Wu, Yan Duan, Xi \nChen, Pieter Abbeel, Joseph M Hellerstein, \nSanjay Krishnan, and Ion Stoica. Deep \nUnsupervised Cardinality Estimation. \nProceedings of the VLDB Endowment , \n13(3):279 –292, 2019.  \n[9.] Benjamin Hilprecht, Andrea s Schmidt, \nMoritz Kulessa, Alejandro Molina, Kristian \nKersting, and Carsten Binnig. DeepDB: \nLearn from Data, not from Queries! \nProceedings of the VLDB Endowment , \n13(7):992 – 1005, 2020.  \n[10.] Immanuel Trummer, Junxiong Wang, \nDeepak Maram, Samuel Moseley, Sa ehan \nJo, and Joseph Antonakakis. SkinnerDB: Regret -Bounded Query Evaluation via \nReinforcement Learning. In Proceedings of \nthe 2019 International Conference on \nManagement of Data , pages 1153 –1170, \n2019.  \n[11.] Levente Kocsis and Csaba Szepesvári. \nBandit base d Monte -Carlo Planning. In \nEuropean conference on machine learning , \npages 282 –293. Springer, 2006.  \n[12.] Ryan Marcus and Olga Papaemmanouil. \nDeep Reinforcement Learning for Join \nOrder Enumeration. In Proceedings of the \nFirst International Workshop on Explo iting \nArtificial Intelligence Techniques for Data \nManagement , pages 1 –4, 2018.  \n[13.] Ryan Marcus, Parimarjan Negi, Hongzi \nMao, Chi Zhang, Mohammad Alizadeh, \nTim Kraska, Olga Papaemmanouil, and \nNesime Tatbul. Neo: A Learned Query \nOptimizer. Proceedings of t he VLDB \nEndowment , 12(11):1705 –1718, 2019.  \n[14.] Surajit Chaudhuri and Vivek Narasayya. \nSelf-Tuning Database Systems: A Decade of \nProgress. In Proceedings of the 33rd \ninternational conference on Very large data \nbases , pages 3 –14, 2007.  \n[15.] Bailu Ding, Su dipto Das, Ryan Marcus, \nWentao Wu, Surajit Chaudhuri, and Vivek \nR Narasayya. AI meets AI: Leveraging \nQuery Executions to Improve Index \nRecommendations. In Proceedings of the \n2019 International Conference on \nManagement of Data , pages 1241 –1258, \n2019.  \n[16.] Lin Ma, Dana Van Aken, Ahmed Hefny, \nGustavo Mezerhane,  Andrew Pavlo, and \nGeoffrey J Gordon. Query -based Workload \nForecasting for Self -Driving Database \nManagement Systems. In Proceedings of the \n2018 Int ernational Conference on \nManagement of Data, pages 631 –645, 2018.  \n[17.] Xi Liang, Aaron J Elmore, and Sanjay \nKrishnan. Opportunistic View \nMaterialization with Deep Reinforcement \nLearning. arXiv preprint arXiv:1903.01363, \n2019.  \n[18.] Yongjoo Park, Ahmad Shaha b Tajik, \nMichael Cafarella, and Barzan Mozafari. \nDatabase Learning: Toward a Database that \nBecomes.  \n\n \nSai Tanishq N , IJECS Volume 09  Issue 0 8 August , 2020 Page No.  25116-25147 Page 25146   Smarter Every Time. In Proceedings of the \n2017 ACM International Conference on \nManagement of Data, pages 587 –602, 2017.  \n[19.] Qingzhi Ma and Peter Triantafil lou. DBEst: \nRevisiting Approximate Query Processing \nEngines with Machine Learning Models. In \nProceedings of the 2019 International \nConference on Management  of Data, pages \n1553 –1570, 2019.  \n[20.] Songyun Duan, Vamsidhar Thummala, and \nShivnath Babu. Tuning Dat abase \nConfiguration Parameters with iTuned. \nProceedings of the VLDB Endowment, \n2(1):1246 –1257, 2009.  \n[21.] Dana Van Aken, Andrew Pavlo, Geoffrey J \nGordon, and Bohan Zhang.  \nAutomatic Database Management System \nTuning through Large -Scale Machine \nLearning. In Proceedings of the 2017 ACM \nInternational Conference on Management of \nData, pages 1009 –1024, 2017.  \n[22.] Jian Tan, Tieying Zhang, Feifei Li, Jie \nChen, Qixing Zheng, Ping Zhang, Honglin \nQiao, Yue Shi, Wei Cao, and Rui Zhang. \niBTune: Individualized Buffer Tun ing for \nLarge -Scale Cloud Databases. Proceedings \nof the VLDB Endowment, 12(10):1221 –\n1234, 2019.  \n[23.] Ryan Marcus and Olga Papaemmanouil. \nWiSeDB: A Learning -Based Workload \nManagement Advisor for Cloud Databases. \nProc. VLDB Endow., 9(10):780 –791, June \n2016.  \n[24.] Ryan Marcus and Olga Papaemmanouil. \nReleasing Cloud Databases for the Chains \nof Performance Prediction Models. In \nCIDR, 2017.  \n[25.] perfenforce demonstration: Data analytics \nwith performance guarantees.  \n[26.] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey \nDean, and Neoklis Polyzotis. The Case for \nLearned Index Structures. In Proceedings of \nthe 2018 International Conference on \nManagement of Data, pages 489 –504, 2018.  \n[27.] Alex Galakatos, Michael Markovitch, \nCarsten Binnig, Rodrigo Fonseca, and Tim \nKraska. FITing -Tree: A Data -aware Index \nStructure. In Proceedings of the 2019 \nInternational Conference on Management of \nData, pages 1189 –1206, 2019.  [28.] Chuzhe Tang, Youyun Wang, Zhiyuan \nDong, Gansen Hu, Zhaoguo Wang,  Minjie \nWang, and Haibo Chen. XIndex: A Scalable \nLearned Index for Multicore Data Storage. \nIn Proceedings of the 25th ACM SIGPLAN \nSymposium on Principles and Practice of \nParallel Programming, pages 308 –320, \n2020.  \n[29.] Andrew Pavlo, Matthew Butrovich, Anany a \nJoshi, Lin Ma, Prashanth Menon, Dana Van \nAken, Lisa Lee, and Ruslan Salakhutdinov. \nExternal vs. Internal: An Essay on Machine \nLearning Agents for Autonomous Database \nManagement Systems. IEEE Data \nEngineering, 11:1910 –1913, 2019.  \n[30.] Andrew Pavlo, Gustav o Angulo, Joy \nArulraj, Haibin Lin, Jiexi Lin, Lin Ma, \nPrashanth Menon, Todd C Mowry, Matthew \nPerron, Ian Quah, et al. Self -Driving \nDatabase Management Systems. In CIDR, \nvolume 4, page 1, 2017.  \n[31.] Tim Kraska, Mohammad Alizadeh, Alex \nBeutel, Ed H Chi, Jial in Ding, Ani Kristo, \nGuillaume Leclerc, Samuel Madden, \nHongzi Mao, and Vikram Nathan. Sagedb: \nA Learned Database System. 2019.  \n[32.] Ryan Marcus and Olga Papaemmanouil. \nPlan-Structured Deep Neural Network \nModels for Query Performance Prediction. \nProceedings  of the VLDB Endowment, \n12(11):1733 –1746, 2019.  \n[33.] Mitchell, T. (1997). Machine Learning. \nMcGraw Hill. p. 2. ISBN 978 -0-07-042807 -\n2. \n[34.] O. Simeone, \"A Very Brief Introduction to \nMachine Learning With Applications to \nCommunication Systems,\" in IEEE \nTran sactions on Cognitive Communications \nand Networking, vol. 4, no. 4, pp. 648 -664, \nDec. 2018, doi: \n10.1109/TCCN.2018.2881442.  \n[35.] Alpaydin, Ethem (2010). Introduction to \nMachine Learning. London: The MIT Press. \nISBN 978 -0-262-01243 -0. Retrieved 1 \nAugust 202 0. \n[36.] K. Kara, K. Eguro, C. Zhang, and G. \nAlonso. ColumnML: Column -Store \nMachine Learning with On -the-Fly Data \nTransformation. PVLDB, 12(4):348 –361, \n2018.  \n\n \nSai Tanishq N , IJECS Volume 09  Issue 0 8 August , 2020 Page No.  25116-25147 Page 25147   [37.] A. Kumar, J. Naughton, and J. M. Patel. \nLearning generalized linear models over \nnormalized dat a. In Proceedings of the 2015 \nACM SIGMOD International 360 \nConference on Management of Data, pages \n1969 –1984. ACM, 2015.  \n[38.] Alonso, G., Istvan, Z., Kara, K., Owaida, M. \nand Sidler, D., 2019. doppioDB 1.0: \nMachine Learning inside a Relational Engine. IEEE  Data Eng. Bull., 42(2), pp.19 -\n31 \n[39.] A. Kumar, J. Naughton, and J. M. Patel, \n―Learning Generalized Linear Models Over \nNormalized Data,‖ in SIGMOD’15.  \n[40.] Langley, Pat (2011). \"The changing science \nof machine learning\". Machine Learning. 82 \n(3): 275 –279. doi:10.1007/s10994 -011-\n5242 -y. \n. \n ",
  "textLength": 69491
}