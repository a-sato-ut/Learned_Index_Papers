{
  "paperId": "190a18a5863c4cbee9cb860f3cf0f3821a26c164",
  "title": "Selective Use of Yannakakis' Algorithm to Improve Query Performance: Machine Learning to the Rescue",
  "pdfPath": "190a18a5863c4cbee9cb860f3cf0f3821a26c164.pdf",
  "text": "arXiv:2502.20233v2  [cs.DB]  20 Jun 2025Selective Use of Yannakakis’ Algorithm to Improve\nQuery Performance: Machine Learning to the\nRescue\nDaniela B ¨ohm\nMatthias Lanzinger\nReinhard Pichler\nAlexander Selzer\nTU Wien\nVienna, AustriaGeorg Gottlob\nDavide Longo\nUniversity of Calabria\nRende, ItalyCem Okulmus\nPaderborn University\nPaderborn, Germany\nAbstract —Query optimization has played a central role in\ndatabase research for decades. However, more often than not,\nthe proposed optimization techniques lead to a performance\nimprovement in some, but not in all, situations. Therefore, we\nurgently need a methodology for designing a decision procedure\nthat decides for a given query whether the optimization technique\nshould be applied or not. In this work, we propose such a\nmethodology with a focus on Yannakakis-style query evaluation\nas our optimization technique of interest. More specifically,\nwe formulate this decision problem as an algorithm selection\nproblem and we present a Machine Learning based approach for\nits solution. Empirical results with a variety of database systems\nshow that our approach indeed leads to a statistically significant\nand practically notable performance improvement.\nI. I NTRODUCTION\nQuery optimization has played a central role in database\nresearch for decades. Some generally accepted techniques such\nas replacing Cartesian product plus selection by a join or\nprojecting out attributes not needed further up in the query\nplan as soon as possible are guaranteed to (almost) always\nlead to a performance improvement. However, more often\nthan not, optimization techniques proposed in the literature\nlead to a performance improvement in some , but not in all,\nsituations. Moreover, it is usually a non-trivial task to delineate\nthe situations where the application of a specific optimization\nis advantageous and where it is not. This also applies to\noptimization techniques which, in theory, should (almost)\nalways outperform the conventional query evaluation methods.\nA prominent example of an optimization technique is Yan-\nnakakis’ algorithm [1] in case the query is acyclic. Several\napplications and extensions of this algorithm in recent time\n(see, e.g., [2]–[11]) witness the renewed interest in this ap-\nproach. The key idea of this algorithm is to first eliminate all\ndangling tuples (= tuples that will not contribute to the final\nresult of the query) via semi-joins and then compute the join\nwith the guarantee that all intermediate results thus obtained\nwill be extended to tuples in the final result. Hence, in a sense,\nYannakakis’ algorithm solves the (in general NP-complete)\njoin ordering problem that aims at avoiding the explosion ofintermediate results. In theory, such a strategy of completely\navoiding the generation of useless intermediate results should\nalways be superior to conventional techniques that just try\nto compute the joins in an optimal order and thus aim at\nminimizing useless intermediate results. However, in practice,\nit turns out that Yannakakis’ algorithm leads to a performance\nimprovement in some cases but, by no means, in allcases.\nIn this work, we revisit a sub-class of acyclic queries called\n0MA (= zero materialization aggregate) queries [12], where\nthe theoretical advantage of Yannakakis-style query evaluation\nis even more extreme: 0MA-queries are a restricted class\nof join queries with an aggregate on top and which can be\nevaluated by carrying out only semi-joins, i.e., completely\navoiding the need for computing any joins. More precisely,\nafter traversing the join tree Tof such a query by semi-joins\nin bottom-up direction, the result can be computed from the\nrelation resulting at the root node of T. (Formal definitions\nof acyclic queries, 0MA queries, etc. are given in Section II.)\nNote that the query given in Figure 1, which is a slightly\nmodified version of a query from the STATS benchmark [13]\n(the main modification is that we have replaced COUNT (*) in\ntheSELECT clause by a MIN-expression), falls into this class.\nIndeed, if we consider a join tree of this query with the\ncomments relation at the root node, then we can evaluate this\nMIN-expression after the bottom-up traversal with semi-joins\nby only considering the resulting relation at the root node.\nIn theory, it is “clear” that such a join-less evaluation must\nalways outperform conventional query evaluation techniques\nthat first fully evaluate the underlying join query and only then\napply the aggregate as a kind of post-processing. Alas, em-\npirical evaluation on queries from several benchmarks shows\nthat, in practice, this is not necessarily the case. Actually, for\nthe query in Figure 1, Yannakakis-style evaluation was signifi-\ncantly faster (by a factor of ca. 30) than the original evaluation\nmethod of PostgreSQL. However, when we modified one of\nthe conditions in the WHERE clause from ≥0to≥8(which,\ntogether with the ≤8condition, yields = 8), then suddenly\nthe original evaluation method of PostgreSQL is faster.\n\nSELECT MIN (c.Id)\nFROM comments ASc, posts ASp, votes ASv, users ASu\nWHERE u.Id = p.OwnerUserId AND u.Id = c.UserId AND\nu.Id = v.UserId AND u.Views >=0AND p.Score >=0AND\np.Score <=28AND p.ViewCount >=0AND p.ViewCount <=6517\nAND p.AnswerCount >=0AND p.AnswerCount <=5AND\np.FavoriteCount >=0AND p.FavoriteCount <=8AND\nc.CreationDate >='2010-07-27 12:03:40 'AND\np.CreationDate >='2010-07-27 11:29:20 'AND\np.CreationDate <='2014-09-13 02:50:15 'AND\nu.CreationDate >='2010-07-27 09:38:05 '\nFig. 1. Slightly modified query 121-097 from the STATS benchmark: original\nruntime on PostgreSQL (3.38s) vs. Yannakakis-style evaluation (0.11s). After\nchanging the filter condition to p.FavoriteCount >=8: original runtime on\nPostgreSQL (0.05s) vs. Yannakakis-style evaluation (0.09s).\nIn order to reap the fruits of optimizations such as Yanna-\nkakis-style query evaluation, we urgently need a method to\ndesign a decision procedure that decides when to apply the\noptimization technique and when to stick to the original eval-\nuation method of the database management system (DBMS).\nWe therefore aim at developing a methodology for the design\nof such a decision procedure, with a focus on Yannakakis-\nstyle query evaluation as our optimization technique of in-\nterest. More specifically, on one hand, we consider general\nacyclic queries (which join all relations after the semi-join-\nbased elimination of dangling tuples) and, on the other hand,\nwe consider 0MA queries (which only require a bottom-up\ntraversal of the join tree with semi-joins).\nTo cover a diverse set of database technologies, we study\nthree quite different DBMSs, namely PostgreSQL [14] (as a\n“classical” row-oriented relational DBMS), DuckDB [15] (as\na column-oriented, embedded database), and Spark SQL [16]\n(as a database engine specifically designed for distributed data\nprocessing in a cluster). This raises the question how to test\nan optimization strategy for different DBMSs with reasonable\neffort. We therefore adapt previous query rewriting approaches\non SQL-level (see, e.g., [7], [12], [17]–[19]). That is, we take\na SQL-query as input and rewrite it to an equivalent sequence\nofSQL-statements that forces (or, at least, guides) the DBMS\ntowards Yannakakis-style query evaluation.\nWe treat the design of a decision procedure between dif-\nferent query evaluation techniques as an algorithm selection\nproblem , that we want to solve by applying Machine Learning\n(ML) techniques. In recent years, many works have used ML\ntechniques to solve database problems – above all cardinality\nestimation and join order optimization (see, e.g., the survey\npaper [20]. We will elaborate more on related work in gen-\neral, and on ML techniques applied to database problems in\nparticular, in Section III). Our focus, however, is different: we\nare interested in the overall runtime of query evaluation under\ndifferent evaluation methods. We ultimately aim to improve\nend-to-end runtimes over entire workloads, rather than local\noptimizations of some specific subtask of query evaluation.\nIn the first place, we have to fix the specifics of our\nalgorithm selection problem. This includes the selection of\nqueries and data from benchmarks as well as the identification\nof relevant features that characterize these queries. The inputto this ML task is formed by a collection of feature vectors\n(characterizing the selected queries and databases) together\nwith runtimes obtained on our target DBMSs for the origi-\nnal query evaluation method and for the query optimization\nmethod we wish to test.\nIn order to solve this ML task, several decisions have to\nbe made and obstacles have to be overcome. For instance,\nbenchmarks aim at testing specific features of DBMSs and\nare relatively small for training and testing ML models. We\ntherefore create a new dataset by adapting and combining\nseveral benchmarks followed by data augmentation methods to\nachieve the variety and size of the dataset that makes it suitable\nfor our concrete algorithm selection problem. Apart from\nchoosing suitable ML model types together with appropriate\nchoices for their hyperparameters, we have to make the basic\ndecision how to actually define the ML task, e.g.: as classifi-\ncation (which evaluation method to choose) or as regression\n(to predict the runtime difference between the two evaluation\nmethods, which is then used as basis for the decision between\nthe two query evaluation methods). We also have to define\nthe precise criteria for determining the “best” ML model,\ne.g.: purely quantitative criteria (such as highest accuracy and\nprecision in case of classification or mean squared error in\ncase of regression) or qualitative criteria (which require an\ninspection of the misclassifications produced by each ML\nmodel and analyzing, for instance, by how much the chosen\nevaluation method was slower than the optimal one).\nContributions. Our main contribution is the development of a\nmethodology for designing a decision procedure that chooses\nbetween different query evaluation techniques. We thus present\nan approach for tackling the design of a decision procedure\nbetween different query evaluation techniques as an algorithm\nselection problem. This approach includes several steps:\n•We present an approach for solving this algorithm se-\nlection problem via ML methods. This includes several\nsubsteps such as determining suitable ML model types,\nidentifying relevant features, and defining selection cri-\nteria for determining the “best” model. In this work, we\nfocus on optimization techniques based on Yannakakis’\nalgorithm. However, we believe that the methodology of\ndefining and solving the algorithm selection problem has\nthe potential for a much wider applicability.\n•We introduce a new dataset and benchmark – focusing\non hard join queries – which is extensive and diverse\nenough for effective model training and testing. This\nnew benchmark, which we will call MEAMBench\n(Materialization Explosion Augmented MetaBench mark )\nis constructed by adapting, combining and augmenting\nqueries from several common benchmarks. Beside the\nalgorithm selection problem we are focusing on here,\nwe expect this dataset to be valuable also for other\nproblems, such as query performance prediction.\n•To prove the practical potential of our approach, we\nimplement it in a system called SMASH, short for\nSupervised Machine-learning for Algorithm Selection\n\nHeuristics . SMASH already supports multiple DBMSs,\nincluding PostgreSQL, DuckDB and SparkSQL. It aims\nat deciding which query optimization method to use for\na given query and DBMS.\n•We report on extensive empirical evaluation that clearly\nconfirms the significance of the improvements achieved\nby the decision procedure compared with the original\nquery evaluation method of the DBMS or always apply-\ning the optimization technique.\nStructure. The paper is organized as follows: After recalling\nbasic definitions and results in Section II, we give an overview\nof related work in Section III. In Section IV, we formulate our\nalgorithm selection problem, and we present a methodology\nfor solving this problem in Section V. Empirical results are\npresented in Section VI. A conclusion and outlook to future\nwork is given in Section VII.\nThe source code of the implementation, information on\nrunning the benchmarks and model training, as well as the\ndata presented here can be found in the following repos-\nitory: https://github.com/dbai-tuw/yannakakis-rewriting. The\nMEAMBench dataset can be found in the following reposi-\ntory: https://github.com/dbai-tuw/MEAMBench Some details,\nwhich are omitted in the main body of the text, are provided\nin the appendix. We also note that the repositories includes\nvarious tools, such as Jupyter Notebooks, allowing one to rerun\nexperiments themselves, including the training of all tested\nmachine learning models.\nII. P RELIMINARIES\nConjunctive Queries and beyond. The basic form of queries\nstudied here are Conjunctive Queries (CQs), which correspond\nto select-project-join queries in the Relational Algebra. It is\nconvenient to consider CQs as Relational Algebra expressions\nof the form Q=πU(R1▷ ◁ . . . ▷ ◁ R n).Here we assume\nw.l.o.g., that equi-joins have been replaced by natural joins\nvia appropriate renaming of attributes. Moreover, we assume\nthat selections applying to a single relation have been pushed\nimmediately in front of this relation and the Ri’s are the result\nof these selections. The projection list Uconsists of attributes\noccurring in the Ri’s.\nTo go beyond CQs, we will also consider the extension of\nRelational Algebra by the grouping operator γand aggregate\nexpressions. In other words, we are interested in queries of\nthe form\nQ=γg1,...,g ℓ, A1(a1),...,A m(am)\u0000\nR1▷ ◁···▷ ◁ R n\u0001\n(1)\nwhere γg1,...,g ℓ, A1(a1),...,A m(am)denotes the grouping op-\neration for attributes g1, . . . , g ℓand aggregate expressions\nA1(a1), . . . , A m(am). The grouping attributes g1, . . . , g ℓare\nattributes occurring in the relations R1, . . . , R n, the functions\nA1, . . . , A mare (standard SQL) aggregate functions such as\nMIN, MAX, COUNT, SUM, A VG, etc., and a1, . . . , a mare\nexpressions formed over the attributes from R1, . . . , R n. We\nhave omitted the projection πUin Equation (1), since it cancomments\nusers\nposts votes\nFig. 2. Join tree for the query in Fig. 1\nbe taken care of by the grouping. A simple query of the form\nof Equation (1) is given in SQL-syntax in Figure 1.\nAcyclicity . An acyclic conjunctive query (an ACQ, for short)\nis a CQ Q=πU(R1▷ ◁ . . . ▷ ◁ R n)that has a join tree , i.e., a\nrooted, labeled tree ⟨T, r, λ⟩with root r, such that (1) λis a\nbijection that assigns to each node of Tone of the relations in\n{R1, . . . , R n}and (2) λsatisfies the so-called connectedness\ncondition , i.e., if some attribute Aoccurs in both relations\nλ(ui)andλ(uj)for two nodes uianduj, then Aoccurs in\nthe relation λ(u)for every node ualong the path between ui\nanduj. Deciding if a CQ is acyclic and, in the positive case,\nconstructing a join tree can be done very efficiently by the\nGYO-algorithm (named after the authors of [21], [22]). The\njoin query underlying the SQL query in Figure 1 can be easily\nseen to be acyclic. A possible join tree is shown in Figure 2.\nYannakakis’ algorithm. In [1], Yannakakis showed that ACQs\ncan be evaluated in time O((||D||+||Q(D)||)·||Q||), i.e., linear\nw.r.t. the size of the input and output data and w.r.t. the size of\nthe query. This bound applies to both, set and bag semantics.\nLet us ignore grouping, aggregation, and projection for a while\nand consider an ACQ Qof the form R1▷ ◁ . . . ▷ ◁ R nwith\njoin tree ⟨T, r, λ⟩. Yannakakis’ algorithm (no matter whether\nwe consider set or bag semantics) consists of a preparatory\nstep followed by 3 traversals of T:\nIn the preparatory step we associate with each node uin the\njoin tree Tthe relation λ(u). If the CQ originally contained\nselection conditions on attributes of relation λ(u), then we can\nnow apply this selection. The 3 traversals of Tconsist of (1) a\nbottom-up traversal of semi-joins, (2) a top-down traversal of\nsemi-joins, and (3) a bottom-up traversal of joins. Formally,\nletube a node in Twith child nodes u1, . . . , u kofuand\nlet relations R,Ri1, . . . , R ikbe associated with the nodes u,\nu1, . . . , u kat some stage of the computation. In the 3 traversals\n(1), (2), and (3), respectively, they are modified as follows:\n(1)R= ((( R⋉Ri1)⋉Ri2). . .)⋉Rik,\n(2)Rij=Rij⋉Rfor every j∈ {1, . . . , k }, and\n(3)R= ((( R ▷ ◁ R i1)▷ ◁ R i2). . .)▷ ◁ R ik\nThe result of the query is the final relation associated with the\nroot node rofT. Grouping and the evaluation of aggregates\ncan be carried out as post-processing after the evaluation of\nthe join query. In contrast, projection πUcan be integrated\ninto this algorithm by projecting out in the second bottom-up\ntraversal all attributes that neither occur in Unor further up in\nT. Attributes neither occurring in Unor in any join condition\nare projected out as part of the preparatory step.\n\n0MA queries. In [12], the class of 0MA (short for “zero-\nmaterialization answerable” )queries was introduced. A query\nof the form given in Equation (1) is 0MA if it satisfies the\nfollowing conditions:\n•Guardedness , meaning that there exists a relation Ri\nthat contains all grouping attributes g1, . . . , g ℓand\nall attributes occurring in the aggregate expressions\nA1(a1), . . . , A m(am). Then Riis called the guard of\nthe query. If several relations satisfy this property, we\narbitrarily choose one guard.\n•Set-safety : we call an aggregate function set-safe , if it\nis invariant under duplicate elimination, i.e., its value\nover any set Sof tuples remains unchanged if duplicates\nare eliminated from S. A query satisfies the set-safety\ncondition, if all its aggregate functions A1. . . , A mare\nset-safe.\nThe root of the join tree can be arbitrarily chosen. For a\n0MA query, we choose the node labeled by the guard as\nthe root node. Hence, if all aggregate functions are set-safe\n(i.e., multiplicities do not matter), then we can apply the\ngrouping and aggregation γ[g1, . . . , g ℓ, A1(a1), . . . , A m(am)]\nto the relation at the root node right after the first bottom-up\ntraversal. In SQL, in particular, the MIN and MAX aggregates\nare inherently set-safe. Moreover, an aggregate becomes set-\nsafe when combined with the DISTINCT keyword. For instance,\nCOUNT DISTINCT is a set-safe aggregate function.\nAn example of a 0MA query is given in Figure 1: it\nis trivially guarded (i.e., there is no grouping and the only\naggregate expression is over a single attribute) and set-safe\n(since the only aggregate function in this query is MIN).\nIII. R ELATED WORK\nAcyclic queries and Yannakakis-style query evaluation. Yan-\nnakakis’ algorithm [1] has recently received renewed attention\nfor the optimization of hard join queries. Several works have\naimed at bringing its advantages into DBMSs from the outside\nvia SQL query rewriting [7], [12], [17], [18], and similar\nmethods such as generating Scala code expressing Yannakakis’\nalgorithm as Spark RDD-operations [10]. An important line of\nresearch in this area has been concerned with the integration\nof ideas of Yannakakis-style query evaluation into DBMSs\nwhile avoiding the overhead of several traversals of the join\ntree via semi-joins and joins [6], [7], [11], [23]. It should\nhowever be noted that, despite all the progress made in\nintegrating and fine-tuning Yannakakis’ algorithm, we are still\nleft with the fact that this optimization leads to a performance\nimprovement in some butnot all cases. Indeed, not even for\n0MA queries (as one of the simplest forms of acyclic queries),\nan improvement in allcases is guaranteed, as will be confirmed\nby our empirical study presented in Section VI.\nDecompositions. In order to go beyond acyclic queries, a\nmajor area of research seeks to extend Yannakakis-style query\nanswering to ”almost-acyclic” queries via various notions of\ndecompositions and their associated width measures, such as\nhypertree-width, soft hypertree-width, generalized hypertree-\nwidth, and fractional hypertree-width [18], [24]–[26]. Severalimplementations [10], [27]–[29] combine Yannakakis-style\nquery execution with worst-case optimal joins [30]. To address\nthe problem of minimal-width decompositions not necessarily\nbeing cost-optimal, approaches of integrating statistics about\nthe data into the search for the best decomposition have been\nproposed and implemented [18], [31].\nQuery rewriting. Optimizing queries before they enter the\nDBMS is a different strategy towards query optimization\nthat has been successfully applied in standard DBMSs [19],\n[32]. Although DBMSs already perform optimizations on the\nexecution of the query, it has been shown that rewriting the\nquery itself can still be highly effective. The WeTune [33]\nsystem goes even further, and can be used to automatically\ndiscover rewrite rules but comes with the disadvantage of\nextremely long runtimes.\nMachine learning for databases. There has been growing\ninterest in the application of machine learning techniques to\nincrease the performance of database systems, as can be seen\nby a recent survey on this broad area [20]. We proceed to\ngive a very brief overview of the general topics as to how\nmachine learning has been adapted for database research. For a\nmore detailed account on the rich interaction between machine\nlearning and databases, we refer to [20]. In this survey,\nthe authors categorize the different efforts of using machine\nlearning for core database tasks into several groups. The first\ngroup is “learning-based data configuration”. These are works\nthat aim to utilize machine learning for knob tuning, and view\nadvisor and index advisor tasks [34]–[43]. Related work that\nalso falls into this category is presented in [44], [45]. The next\ngroup is “learning-based data optimization”. These works aim\nto tackle important, computationally intractable problems such\nas join-order selection and cardinality estimation of joins [46]–\n[57]. Another group is “learning-based design for databases”.\nThese works aim more specifically at exploring the use of\nmachine-learning in the construction of various data structures\nused by modern databases, such as indexes, hashmaps, bloom\nfilters and so on [58]–[61]. A further group listed in the survey\nis “learning-based data monitoring”. As the name suggests,\nthese works aim to use machine learning to create systems\nthat automate the task of running a database and detecting\nand reacting to anomalies [53], [62]–[65]. Lastly the survey\nmentions “learning-based database security”. This category is\non how to use ML methods to help with critical problems, such\nas confidentiality, data integrity and availability [66]–[70].\nQuery performance prediction. Predicting the performance of\na query – usually the runtime, or sometimes the resource\nrequirements – is related to the problem of deciding whether\nto rewrite a query. Runtime prediction has been performed by\nconstructing cost models based on statistical information of the\ndata [71], on SQL queries [72], and XML queries [65]. Further\napproaches use machine learning and deep learning to predict\nthe runtimes of single queries [53], [73], [74] or concurrent\nqueries (workload performance prediction) [75], [76].\n\nBenchmark\nQueriesRewritten\nQueries\nAugmented\nQueriesRuntimes\nQuery\nFeaturesML\nmodelsDecision\nProcedure\nData\naugmentationRewritingRuntime\nmeasurements\nModel\ntraining\nFeature\nselectionEvaluation\nof the models\nFeature\nextraction\nFig. 3. Methodology workflow.\nIV. F ORMULATING THE ALGORITHM SELECTION\nPROBLEM\nWhenever a new, optimized method for query evaluation\nis presented, we need a decision program that decides when\nthe new method should be applied. We are thus faced with\nan algorithm selection problem, where we have to decide,\nfor every database instance and query, which query evaluation\nmethod should be applied. In this section, we describe the steps\nneeded to formulate the precise algorithm selection problem.\nAn overview of this workflow is given in Figure 3.\nIt consists of the following steps: We first have to create a\nsuitable dataset that features the required variety and size for\nthe ML task at hand. On one hand, we thus have to (1) select\nand adapt common benchmarks and select those queries to\nwhich the optimization technique is applicable. On the other\nhand, we (2) apply data augmentation to ensure a suitable\nsize of the dataset. We then need to (3) select DBMSs on\nwhich we want to test the effectiveness of the new optimization\ntechniques. Implementing these new techniques into a query\nengine is a non-trivial and expensive task. Hence, to save\neffort and to enable testing with various DBMSs, we make\nuse of the specific nature of Yannakakis-style query evaluation\nand (4) rewrite the given SQL queries into sequences of SQL\ncommands that “force” the selected DBMSs into a particular\nevaluation strategy. To prepare for the ML task, we then have\nto (5) do a feature selection . That is, we want to characterize\nevery query in terms of a feature vector. We are then ready\nto (6) run the experiments , i.e., we execute all queries with\nand without the optimization and measure the times of each\nrun. The result will be runtimes for each query (characterized\nby a particular feature vector) both, when the optimization is\napplied and when it is not applied. This is then the input to\nthemodel training step and, ultimately, to the development of\na decision procedure, which will be described in Section V.\nBelow, we describe steps (1) – (6) in some detail for\nYannakakis-style query evaluation of two classes of queries,\nnamely, enumeration queries (i.e., queries that ask for the\nenumeration of the result tuples for an acyclic conjunctive\nquery) and 0MA-queries (i.e., the subclass of aggregate queries\nwith underlying ACQs [12]). Recall from Section II, that 0MA\nqueries allow for a particularly simple evaluation strategy,\nsince they can be evaluated based on semi-joins only, i.e.,\nwithout computing any (full) joins.A. Benchmark Data\nWhen constructing our new dataset MEAMBench, we pur-\nsue two major goals to make the dataset suitable for model\ntraining and testing: it should be sufficiently big and suffi-\nciently diverse. To address the diversity aspect, we collect di-\nverse sets of data and queries from different domains, designed\nfor different purposes, and representing challenging cases of\nan explosion of intermediate results and materialization. Thus,\nas a basis, we have chosen several widely used benchmarks,\nwhich contain join queries over several relations: (1) The JOB\n(Join Order Benchmark) [77], which was introduced to study\nthe join ordering problem, is based on the real-world IMDB\ndataset and contains realistic join-aggregation queries with\nmany joins and various filter conditions, (2) STATS/STATS-\nCEB [13] is based on the Stackexchange-dataset, and con-\ntains many join queries not following FK/PK-relationships,\n(3) Four different datasets (namely cit-Patents, wiki-topcats,\nweb-Google and com-DBLP) from SNAP (Stanford Network\nAnalysis Project) [78], a collection of graphs, which we\ncombine with synthetic queries introduced in [5], (4) LSQB\n(Large-Scale Subgraph Query Benchmark) [79], which was\ndesigned to test graph databases as well as relational databases,\nconsists of synthetic data and hard queries based on a social\nnetwork scenario, and (5) HETIONET [80]. The latter is less\nknown in the database world. It contains real-world queries on\nreal-world data from a heterogeneous information network of\nbiochemical data, and is part of the new CE benchmark [81],\nwhich has, for instance, been recently used in [6] and [82].\nWe focus on ACQs, which make up the vast majority of\nthe queries in the base benchmarks. Most of the queries in the\nchosen benchmarks are CQs with additional filter conditions\napplied to single tables. These filter conditions can be taken\ncare of by the preparatory step; so they pose no problem.\nHowever, not all of the CQs are acyclic; so we have to\neliminate the cyclic ones from further consideration. The\nnumber of (acyclic) CQs of each dataset is given in Table I.\nNote that some of the queries in the benchmarks are\nenumeration queries and some already contain some aggregate\n(in particular, MIN) and satisfy the 0MA conditions. Of course,\nalso from the enumeration queries, we can derive 0MA queries\nby putting an appropriate aggregate expression (again, in\nparticular, with the MINaggregate function) into the SELECT\nclause of the query. We do this by randomly choosing a table\n\nTABLE I\nOVERVIEW OF THE 0MA AND ENUMERATION QUERIES AFTER\nAUGMENTATION . IN TOTAL ,WE GET 4677 QUERIES ,CONSISTING OF 2936\n0MA QUERIES AND 1741 ENUM QUERIES .\nDataset # ACQs +filter +filter&agg +filter&enum\nSTATS 146 432 1876 1264\nSNAP 40 40 244 120\nJOB 15 45 264 135\nLSQB 2 2 14 6\nHETIONET 26 72 538 216\nTotal 219 591 2936 1741\noccurring in the query and one column of this table. We will\nsee in Section IV-B, that it makes no significant difference\nwhich table and attribute we choose for turning a query into\n0MA form, as we will vary the table and attribute anyway.\nB. Data Augmentation\nOur collection of data and queries from different bench-\nmarks results in 219 acyclic queries, as can be seen in Table I.\nSince our goal is to use our new dataset MEAMBench for\ntraining and testing ML models, this is clearly not a sufficient\namount. Therefore, we perform data augmentation, as will be\ndetailed next.\nFor our dataset, we decided to use the following two steps\nfor data augmentation: ”filter augmentation” (for all queries)\nfollowed by ”aggregate-attribute augmentation” (for 0MA-\nqueries) and “enumeration augmentation” (for enumeration\nqueries), respectively.\nWith the filter augmentation we want to get duplicates of\nall queries having filters (i.e., selection conditions on a single\ntable) and then change some filters in a way that the sizes of\nthe resulting relations vary between these queries. If the query\nhad only one filter we change the specific value it is equal to,\ngreater or smaller of the filter condition. For these cases, we\nget twice as many queries as before. For all queries having\ntwo or more filters we choose two filters, which we change,\neach at a time. Here we try to replace the filters in a way\nthat once the number of answer tuples gets bigger and once\nsmaller. This gives us triples for each of these queries.\nExample 4.1: Consider the STATS query ‘005-024’, named\nqhere. To illustrate the filter augmentation, we present two\npossible augmentations on q. One option to augment qis\nto swap the filter condition, v.BountyAmount >=0, with a\ntransformed one, such as v.BountyAmount >=40, producing the\nquery qaug1. Another option is to swap u.DownVotes=0 with\nu.DownVotes=10 , producing qaug2.\nq: SELECT MIN (u.Id)\nFROM votes ASv, badges ASb, users ASu\nWHERE u.Id = v.UserId AND v.UserId = b.UserId\nAND v.BountyAmount >=0AND v.BountyAmount <=50\nAND u.DownVotes=0\nqaug1:SELECT MIN (u.Id)\nFROM votes ASv, badges ASb, users ASu\nWHERE u.Id = v.UserId AND v.UserId = b.UserId\nAND v.BountyAmount >=40 AND v.BountyAmount <=50AND u.DownVotes=0\nqaug2:SELECT MIN (u.Id)\nFROM votes ASv, badges ASb, users ASu\nWHERE u.Id = v.UserId AND v.UserId = b.UserId\nAND v.BountyAmount >=0AND v.BountyAmount <=50\nAND u.DownVotes=10\nFor 0MA queries, we next apply the ”aggregate-attribute\naugmentation” to vary the table from which we take the MIN-\nattribute. This is done in a way that every table occurring in\nthe query appears once in the MIN-expression. The column of\nthe chosen table does not really matter, which means we just\ntake the first column of the table. Depending on the number of\ntables involved in the query, this leads to a different number\nof new queries per query.\nExample 4.2: We give an example for the aggregate-\nattribute augmentation on 0MA queries. As in Example 4.1, we\nagain focus on the STATS query 005-024. For this query, we\nthus create 3 versions by taking either MIN(u.Id) ,MIN(v.Id) ,\norMIN(b.Id) in the SELECT clause. This aggregate-attribute\naugmenation is applied to the original queries and to the filter\naugmented ones alike. Hence, the original STATS query 005-\n024 gives rise to 9 distinct queries after the whole augmenta-\ntion process.\nIn Table I, we summarize the numbers of 0MA queries that\nwe get after each step of the augmentation. The SNAP and\nLSQB queries do not have filter conditions, which means there\nis no filter augmentation for them.\nWe also take the enumeration queries, for which filter aug-\nmentation has already been done, and apply an enumeration\naugmentation step. To this end, we randomly choose two of\nthe attributes used in join conditions and write them into the\nSELECT clause of the query. This is done three times for each\nfilter augmented query if at least three different join attributes\nexist in the query. On the other hand, a query with only one\njoin gives rise to only a single enumeration query (with the\njoin attributes in the SELECT clause) in our dataset.\nIn summary, after applying the data augmentation step to\nthe 0MA and enumeration queries, we have 4677 queries in\ntotal. In Appendix A, we give further examples on the data\naugmentation, namely on the enumeration augmentation.\nC. Selection of DBMSs\nWe want to check the effectivity of the optimization via\nYannakakis-style query evaluation on a wide range of database\ntechnologies. We have therefore chosen three significantly\ndifferent DBMSs, namely (1) PostgreSQL 13.4 [14] as a “clas-\nsical” row-oriented relational DBMS, (2) DuckDB 0.4 [15]\nas a column-oriented, embedded database, and (3) Spark\nSQL 3.3 [16] as a database engine specifically designed\nfor distributed data processing in a cluster. These DMBSs\nrepresent a broad spectrum of architectures and characteristics\nand they, therefore, give a good overview of the range of\nexisting DBMSs.\n\nD. Query Rewriting\nTo recall our motivation, we aim to find methods that\ndetermine the effectiveness of various optimization methods –\nwith a focus in this work on Yannakakis-style query evaluation\n– on various DBMSs before potentially having to commit\nto the significant effort of modifying existing query engines.\nHence, we make use of ideas from recent works [10], [12],\n[17], [18] that present query rewritings, where a single SQL\nquery is rewritten into an equivalent series of queries, to\nguide DBMSs to utilize a given optimization method. In\nAppendix A, we illustrate the query rewriting approach that we\nuse in this paper, using an example query from our benchmark.\nE. Feature Selection\nWe choose different kinds of features that we derive from\nthe structure of the query itself, from the join tree constructed\nin the process of rewriting the query, and from statistics\ndetermined by PostgreSQL or DuckDB over the database. The\nlatter kind of features is extracted from the query optimizer’s\nestimates, and obtained via the EXPLAIN command. Note\nthat Spark SQL does not provide an EXPLAIN command.\nHowever, we will explain below how to circumvent this\nshortcoming. Another challenge are features that are based on\na set, of variable length, containing numeric values. In order to\nreduce such a set of values into a fixed-length list of values, we\ncalculate, for each set, several statistics: min, the 0.25-quantile\n(referred to as q25), median, the 0.75-quantile (referred to as\nq75), max, and mean. In the list of features below, we use∗\n(e.g. B7*) to mark which features consist of variable-length\nsets, and hence will get reduced to the mentioned collection\nof 6 values.\nFeatures derived from the query. The following features are\neasily obtained by inspecting the query itself:\nFeature B1 :is 0MA? indicates (1 or 0) if the query is 0MA,\nFeature B2 :number of relations ,\nFeature B3 :number of conditions , which refers to the number\nof (in)equality conditions in the WHERE clause of the query,\nFeature B4 :number of filters , which more specifically only\ncounts the (in)equality conditions occurring in the query, and\nFeature B5 :number of joins .\nFeatures based on the join tree. The following features are\ninspired by the work in [83] on tree decompositions:\nFeature B6 :depth , which is the maximal distance between\nthe root of the used join tree and a leaf node,\nFeature B7* :container counts , which is a set of numbers,\nindicating for each variable in the query the number of nodes\nin the join tree it occurs in. This measure indicates how many\nrelations are joined on the same variable, and\nFeature B8* :branching degrees , which is a set of numbers,\nindicating for each node the number of children it has.\nThese eight features (B1)-(B8*) are the shared ”basic\nfeatures” . In addition, we can use statistical information\nfrom the database and the estimates for the query evaluation,\nthough the exact features that are exposed differs between\nDBMSs. In case of PostgreSQL and DuckDB, we have theEXPLAIN command at our disposal to obtain relevant further\ninformation. For PostgreSQL, we thus select the following\nadditional features, which we refer to as ”PSQL features” :\nFeature P1 :estimated total cost (of the query),\nFeature P2* :estimated single table rows , which stands for\nthe estimated number of rows for each table involved in the\nquery after the application of the filter conditions, and\nFeature P3* :estimated join rows , the estimated number of\nrows of each join before the application of the filter conditions.\nThe EXPLAIN command for DuckDB behaves differently. It\nallows us to derive a single ”DDB feature” :\nFeature D1* :estimated cardinalities , the estimated number\nof rows after each node in the logical plan, such as filters and\njoins.\nAs SparkSQL does not perform cost-based optimization,\nit also does not have any statistical information of the data,\nand cannot estimate cardinalities or costs of a plan. However,\nsince SparkSQL also does not provide a persistent storage\nlayer, tables are commonly imported from another database via\nJDBC. This implies that, in practice, the statistical features can\neasily be extracted from this database and used for the decision\nwhether to rewrite the query in SparkSQL. For the experiments\npresented in Section VI, we extracted these features from\nPostgreSQL, hence SparkSQL will have the same feature set\nas PostgreSQL.\nIn Appendix A, we illustrate the basic features as well as the\nadditional features for PostgreSQL and DuckDB by looking\nat two queries from the HETIONET benchmark.\nF . Running the Queries\nThe whole evaluation is performed on a server with 128GB\nRAM, a 16-core AMD EPYC-Milan CPU, and a 100GB SSD\ndisk, running Ubuntu 22.04.2 LTS. After a warm-up run, the\noriginal query, as well as the rewritten version, is evaluated\nfive times, and then we take the mean of those five runtimes. In\ntotal we get 6 data points for each of the 4677 queries: each\nquery is run against 3 DBMSs, where the query evaluation\nhappens once with and once without optimization. Aggregated\ninformation on these runtimes is provided in Section VI.\nIn addition to the six runtimes, we also add a “feature\nvector”, consisting of the features described in Section IV-E.\nThese provide the input to the training of ML models to be\ndescribed next.\nV. S OLVING THE ALGORITHM SELECTION PROBLEM\nSeveral decisions have to be made to solve the algorithm\nselection problem that results from the steps described in\nthe previous section. In particular, we have to (1) formulate\nthe concrete ML task and then (2) select ML model types\ntogether with hyperparameters appropriate to our context.\nBefore we can start training and testing the models, we have\nto (3) split the data (in our case the SQL queries) into\ntraining/validation/test data. Finally, we have to (4) define\nselection criteria for determining the “best” model, which\nwill then be used as basis of our decision program between\n\nthe original query evaluation method of each system and a\nYannakakis-style evaluation.\nA. Formulating the ML Task\nOur ultimate goal is the development of a decision program\nbetween two evaluation methods for acyclic queries. Hence,\nwe are clearly dealing with a classification problem with 2\npossible outcomes . In the sequel, we will refer to these two\npossible outcomes as 0 vs. 1 to denote the original evalua-\ntion method of the DBMS vs. a Yannakakis-style evaluation\n(enforced by our query rewriting).\nOn the other hand, it also makes sense to consider a\nregression problem first and, depending on the predicted value,\nclassify a query as 0 (if the predicted value suggests faster\nevaluation by the original method of the DBMS within a\ncertain threshold) or 1 (otherwise). As target of the regression\nproblem we would like to choose the difference trewritten −\ntoriginal , where we write trewritten andtoriginal to denote the\ntime needed by Yannakakis-style evaluation and by the original\nevaluation method of the DBMS, respectively. However, as\nwill become clear in our presentation of the experimental re-\nsults in Section VI, the actual runtime values are very skewed,\nin the sense that their distribution shows high variance. Hence,\nthe difference we focus on is also highly skewed. To get\nmore reliable results, we therefore perform a (variant of) log-\ntransformation as described next: Since we may have negative\nvalues, we cannot apply the logarithm directly. Instead, we\nmultiply the log of the absolute values with the sign they had\nbefore. Additionally, since we have a lot of values close to zero\n(which leads to very small log values) we add 1 to the absolute\nvalues before applying the log, which is a common method\nin such situations. The transformation therefore results in the\nfollowing formula: xnew=sgn(x)∗log(|x|+1). In Figure 4,\nwe can see this difference function under log transformation\nover the data from our experiments shown in Section VI.\nB. Selecting the ML Model Types\nWe have chosen 7 Machine Learning model types for our\nalgorithm selection problem, namely k-nearest neighbors (k-\nNN), decision tree, random forest, support vector machine\n(SVM), and 3 forms of neural networks (NNs): multi-layer\nperceptron (MLP), hypergraph neural network (HGNN) and a\ncombination of the two. MLP is the “classical” deep neural\nnetwork type. Hypergraph neural networks, introduced in [84],\nare less known. With their idea of representing the hypergraph\nstructure in a vector space, HGNNs seem well suited to capture\nstructural aspects of conjunctive queries. Just like MLPs, also\nthe HGNNs produce an output vector. In our combination of\nthe two model types, we provide yet another neural network,\nwhich takes as input the two output vectors produced by the\nMLP and the HGNN and combines them to a joint result using\nadditional layers.\nA major task after choosing these ML model types is to\nfix the hyperparameters. An overview of some basic hyper-\nparameters is shown in Table III. Of course, in particular for\nthe 3 types of neural networks, many more hyperparametershave to be fixed. A detailed list of all hyperparameter values,\nin particular for the 3 types of neural networks used here, is\nprovided in Appendix A.\nC. Labeling and Splitting the Data\nAfter running the 4677 queries mentioned in Section IV-B\non the 3 selected DBMSs according to Section IV-C, we have\nto prepare the input data for training the ML models of the\n7 types mentioned in Section V-B. Recall from Section IV-E\nthat each query is characterized by a feature vector specific\nto each of the 3 DBMSs. For our supervised learning tasks\n(classification and regression), we have to label each feature\nvector for each of the 3 DBMSs. As explained in Section V-B,\nwe want to train our models both, for classification and for\nregression. Hence, on the one hand, each feature vector gets\nlabeled 0 or 1 (meaning that the original evaluation of the\nDBMS or the Yannakakis-style evaluation is faster; in case of\na tie, we assign the label 0) for the classification task. On the\nother hand, each feature vector is labeled with the difference\nof the runtime of the original evaluation minus the runtime of\nthe Yannakakis-style evaluation for the regression task.\nThe labeled data can then be split into training data,\nvalidation data, and test data. In principle, we choose a quite\ncommon ratio between these three sets by letting the training\nset contain 80% of the data and the other two contain 10%\neach. However, to get more accurate results, we have decided\nto do 10-fold cross validation. That is, we split the 90% of\nthe data that were chosen for training and validation in 10\ndifferent ways in a ration 80:10 into training:validation data\nand, thus, repeat the training-validation step 10 times.\nD. Selection Criteria for the “Best” Model\nIn order to ultimately choose the “best” model for our\ndecision program between the original evaluation method of\neach DBMS and the Yannakakis-style evaluation, we compare,\nfor every feature vector, the predicted classification with the\nactual labeling. We refer to classification 1 as “positive”\nand classification 0 as “negative”. This leads to 4 possible\noutcomes of the comparison between predicted and actual\nvalue, namely TP (true positive) and TN (true negative) for\ncorrect classification and FP (false positive) and FN (false\nnegative) for misclassification. They give rise to the 3 most\ncommon metrics: accuracy (shortened to “Acc”), which is\nthe proportion of correct classifications, precision (shortened\nto “Prec”), which is defined by TP / (TP + FP), and recall\n(shortened to “Rec”), defined as TP / (TP + FN).\nOf course, the natural goal when selecting a particular\nmodel is to maximize the accuracy. However, in our context,\nwe consider the precision equally important. That is, we find it\nparticularly important to minimize false positives, i.e., in case\nof doubt, it is better to stick to the original evaluation method\nof the DBMS rather than wrongly choosing an alternative\nmethod.\nFor regression, we aim at minimizing the mean squared\nerror (MSE). But ultimately, we also map the (predicted and\nactual) difference between the runtime of the original minus\n\nTABLE II\nPERFORMANCE OF MACHINE LEARNING CLASSIFIERS ON THE\nPOSTGRE SQL RUNTIMES . W E SHOW ACCURACY ,PRECISION AND RECALL\nFOR BINARY CLASSIFIERS THAT PREDICT WHETHER REWRITING TO\nYANNAKAKIS STYLE EVALUATION LEADS TO PERFORMANCE GAIN .\nAlgorithm 0MA Queries Acyc. Queries\nAcc. (%) ↑Prec.↑Rec.↑Acc. (%) ↑Prec.↑Rec.↑\nDecision Tree 0.94 0.92 0.97 0.95 0.95 0.92\nRandom forest 0.94 0.92 0.97 0.95 0.94 0.93\nk-NN 0.91 0.91 0.90 0.91 0.88 0.91\nSVM 0.85 0.85 0.84 0.84 0.82 0.77\nMLP 0.87 0.89 0.86 0.85 0.84 0.77\nHGNN 0.83 0.84 0.85 0.79 0.70 0.75\nHGNN+MLP 0.82 0.78 0.93 0.81 0.77 0.72\nYannakakis-style evaluation to a 0 or 1 classification. Hence,\nwe can again measure the quality of a model in terms of\naccuracy, precision, and recall.\nApart from the purely quantitative assessment of a model\nin terms of accuracy, precision, and recall, we also carry out a\nqualitative analysis. That is, for each of the misclassified cases,\nwe want to investigate by how much the chosen evaluation\nmethod is slower than the optimal method. And here, we are\nagain particularly interested in the false positive cases. Apart\nfrom aiming at high accuracy and precision, we also want\nto make sure for the false positive classifications, that the\ndifference in the runtimes between the two evaluation methods\nis rather small.\nVI. E XPERIMENTAL RESULTS\nIn this section, we present experimental results obtained\nby putting the algorithm selection method described in Sec-\ntions IV and V to work. We thus first evaluate in Section VI-A\nthe performance of various machine learning models on the\nraw dataset of query runtimes obtained for the selected and\naugmented benchmarks on the chosen DBMSs. Afterwards,\nin Section VI-D, we evaluate the performance gains by com-\nbining the best algorithm selection model with our rewriting\nmethod for evaluating acyclic queries. In particular, we per-\nform experiments to answer the following key questions:\nQ1 How well can machine learning methods predict\nwhether Yannakakis-style query evaluation is prefer-\nable over standard query execution?\nQ2 Can we use these machine learning models to\ngain insights about the circumstances in which\nYannakakis-style query evaluation is preferable over\nstandard query execution?\nQ3 How well does good algorithm selection perfor-\nmance translate to query evaluation times on different\nDBMSs?\nQ4 To what extent can we optimize for precision while\nmaintaining end-to-end runtime and accuracy?\nIn the remainder of this section, we present our experimental\nresults and a discussion as to how they answer our key\nquestions. Further details are provided in Appendix A.TABLE III\nCHOSEN HYPERPARAMETERS .\nModel Hyperparameters\nRandom Forest #estimators = 100\nkNN k = 5\nSVM kernel = linear\nMLP layers = 30-60-40-2\nHGNN layers = 1-16-32-2\nTABLE IV\nPERFORMANCE OF REGRESSION MODELS . W E SHOW MSE AND MAE\nFOR REGRESSION MODELS PREDICTING THE DIFFERENCE BETWEEN THE\nRUNTIME OF THE ORIGINAL AND THE REWRITTEN QUERY . ADDITIONALLY ,\nWE PRESENT THE ACCURACY AND PRECISION AFTER CONVERTING THE\nREGRESSION MODEL TO A CLASSIFICATION MODEL BY SETTING A\nTHRESHOLD AT A PREDICTED TIME DIFFERENCE OF 0SECONDS .\nAlgorithm Acyc. Queries\nMSE MAE Acc. Prec. Rec.\nDecision Tree 0.02 0.06 0.96 0.96 0.94\nRandom forest 0.03 0.08 0.95 0.94 0.93\nk-NN 0.17 0.18 0.91 0.88 0.91\nSVM 1.02 0.61 0.79 0.76 0.72\nMLP 0.39 0.32 0.81 0.72 0.77\nHGNN 0.74 0.48 0.71 0.57 0.85\nA. Model Training\nIn a first step, we will compare the performance of various\nlearned models in terms of accuracy, precision, and recall.\nTable II compares the performance of the best classification\nmodels (with the hyperparameters given in Table III on the\n0MA queries only, as well as on all queries (i.e., 0MA and\nenumeration), on the runtime data from PostgreSQL. Decision\ntrees and random forests, with roughly the same performance,\nachieve the best accuracy, precision and recall out of all\nclassifiers. kNN achieves the next best performance, although\nsignificantly less at 91% accuracy compared to the 95%\naccuracy of the decision tree-based models. The random forest\nclassifier has a similar performance to the simple decision\ntrees. Hence, its disadvantage of additional complexity is\nnot outweighed by a significant performance improvement.\nTherefore, the simple decision tree classifier is the clear choice\nout of all compared.\nNext, we compare the performance of the regression variants\nof these models, presented in Table IV and (applying the\ntransformation described in Section V-A) in Figure 4 on\nall queries, again on the PostgreSQL runtimes. We can see\nthat the regression performance in terms of MSE and MAE\ncorresponds closely to the classification performance of the\nclassification models, with the decision tree and random forest\nmodels again at the top. However, the gap to the next best\nmodel is even larger, corresponding to a 5-8x increase in\nMSE and a 2-3x increase in MAE, making the decision tree-\nbased models again preferable in this situation. From these\nresults, it can be observed that we are able to predict the\nruntime difference (i.e., runtime of the rewritten query minus\nruntime of the original query) quite accurately. This predicted\n\n−4 −2 0 2 4100101102103\ndiff:=rewritten −original runtime\nsgn(diff) log(|diff|+ 1)\nFig. 4. Distribution of the regression response, understood as the time\ndifference between rewritten runtime and original runtime for PostgreSQL,\nunder the given log transformation. Above we show a histogram of this\ndistribution, using log scaling to allow for more visual clarity.\n0.4\n 0.2\n 0.0 0.2 0.4\nthreshold50%60%70%80%90%100%accuracy precision recall\n1265s1270s1275s1280s1285s1290s1295s\nruntime\nFig. 5. Accuracy, precision, recall, as well as the e2e runtime over the test set,\nof the regression model converted to a classifier, depending on the threshold\nset (decision tree regression, PostgreSQL, all queries).\nruntime difference naturally lends itself to classification by\nchoosing the query rewriting if and only if this difference\nis below 0. This classification derived from the decision tree\nregression model leads to a similar (actually slightly better\nby 0.5%) performance than the decision tree classification.\nMoreover, as will be discussed next, classification based on\nthe predicted runtime difference provides additional flexibility\nfor fine-tuning the trade-off between precision and recall.\nHence, we have chosen classification based on the decision\ntree regression model as basis for our decision procedure.\nB. Fine-tuning precision and recall\nSo far we have only considered choosing a threshold of\nthe predicted runtime difference of the regression model at 0\n(i.e., if the predicted runtime difference is below this threshold,\nwe choose Yannakakis-style query evaluation, and, otherwise,\nwe opt for the original query execution). However, we can\nmake use of the regression model as a useful tool to configure\nthe trade-off between precision and recall, depending on the\nrequirements of the application. To simplify the presentation,TABLE V\nMOST IMPORTANT FEATURES ACCORDING TO GINI COEFFICIENT OF THE\nDECISION TREE MODELS . THE FEATURES ARE DESCRIBED IN SECTION\nIV-E\nPostgreSQL DuckDB\nFeature Gini Feature Gini\nP3∗max(est. join rows) 0.369 B1 is 0MA? 0.280\nB1 is 0MA? 0.157 B7∗mean(cont. c.) 0.206\nP2∗q75(est. sing. table rows) 0.069 D1∗max(est. card.) 0.147\nP1 est. total cost 0.053 D1∗q25(est. card.) 0.061\nP3∗min(est. join rows) 0.051 D1∗q75(est. card.) 0.057\nwe focus on one DBMS and on one model, namely Post-\ngreSQL and decision trees. The extension to other models\nand DBMSs is straightforward and yields similar results. In\nFigure 5, we show how changing the threshold affects the\naccuracy, precision and recall of the resulting classification\nmodel. Clearly, the maximum accuracy is at the 0-threshold.\nThe accuracy, however, continues to be high in the direction of\nnegative thresholds, while falling off quickly in the direction\nof a positive threshold. In particular, this shows that an\noptimization of precision can be performed without sacrificing\nmuch recall.\nTo summarize our findings on the quality of predicting when\nYannakakis-style query evaluation is better: the results from\nTable II and Table IV that show very high levels of accuracy,\nprecision and recall for our chosen model of decision trees, as\nwell as the ability to further optimize for precision as seen in\nFigure 5 allow us to positively answer the key question Q1. In\nFigure 5 we also get an answer for our key question Q4: by\nchoosing the right threshold one can achieve almost perfect\nprecision, with only modest reductions in accuracy and e2e\nruntime (which we discuss in detail in Section VI-D).\nC. Insights from Decision Trees\nIt is of particular interest that decision trees are among\nthe top-performing models as they are highly interpretable\nand can provide us with deeper insights into the features\nthat strongly affect the prediction. In Table V, we present\nthe Gini coefficients of the top 5 most relevant attributes,\nfor the PostgreSQL and the DuckDB decision tree classifiers\n(for details on SparkSQL and further attributes, see Table IX\nin the Appendix). Note that the feature set between the two\nsystems is different , as explained in Section IV-E, so we cannot\ncompare the Gini coefficients of all the same features. The\nGini coefficient [85] measures the contribution of the feature\nto the outputs of the model. Looking at the Gini coefficients,\nwe see that, although the performance of the models trained on\nthe PostgreSQL and DuckDB features and runtimes are very\nsimilar, the decision trees rely on different features. In the\ncase of DuckDB, the feature indicating whether the query is a\n0MA query has the highest importance. While the PostgreSQL\nmodel strongly relies on the maximum join rows in the plan,\nDuckDB’s second-most-important feature is the mean number\nof container counts, a feature extracted from the join tree.\nThe model for PostgreSQL, on the other hand, only use a\n\nBase Rewriting SMASH02,0004,0006,000\n125618493965e2e-time (s)PostgreSQL\nBase Rewriting SMASH02,0004,0006,000\n162721625841SparkSQL\nBase Rewriting SMASH02,0004,0006,000\n60218242661DuckDB\nBase 0MA Base Enum Rewriting 0MA Rewriting Enum SMASH 0MA SMASH Enum\nFig. 6. Comparison of e2e performance over the test set queries for three database systems: PostgreSQL, Spark (via SparkSQL), and DuckDB. The full bar\nindicates the runtime over all test set queries, the lower mark indicates the time for 0MA queries.\nsingle basic feature, namely B1(is 0MA?), among the most\nimportant features. This ability of decision trees to highlight\nwhich specific features affect the prediction the most, helps\nus answer the key question Q2positively. We hope our full\nexperimental artifacts, which include the full decision trees,\nwill help foster further research into using these features to\nimprove query engine optimization in general.\nD. Effects on Database Performance\nSo far, we have evaluated purely the performance of the\nmachine learning models on the dataset that we created for our\nalgorithm selection problem. However, our initial hypothesis\nwas that machine learning based algorithm selection can solve\nthe complex challenge of deciding when evaluation in the\nstyle of Yannakakis’ algorithm is preferable. We therefore now\nmove on to evaluating the performance of the resulting full\nsystem that uses our trained algorithm selection models to\ndecide when to rewrite, and thus execute queries using the\npredicted best query execution method.\nWe first need to decide which of the two models, the\ndecision tree model or the regression model, we pick for\nthe experiments, since we aim to identify the best algorithm\nselection mechanism. However, looking at Table IV, which\nshows the performance of the regression model, and Table II,\nwhich shows the performance of the classification model, we\nsee that the two are fairly comparable, with a slight advantage\nfor the regression model. As we also saw, the regression-based\napproach is especially promising for real-world applications as\nit allows us to fine-tune the decision threshold. We thus choose\nthe regression approach with a threshold at 0for the analysis\npresented in this section.\nAs mentioned in the introduction, we will refer to this inte-\ngrated method, that decides whether to rewrite to Yannakakis-\nstyle evaluation based on the prediction of the decision tree\nmodel as SMASH, short for Supervised Machine-learning\nforAlgorithm Selection Heuristics . In this section, we will\nperform all experiments only on the test set queries. No queriesfrom these experiments were seen by the model at training\ntime, inluding to select the best model.\nAt the most fundamental level, we are interested in improv-\ning overall query answering performance. We investigate this\nby analyzing the end-to-end (e2e) time necessary to answer all\nqueries in the test set, where “end-to-end” refers to taking the\ntime of running all the benchmark queries and looking at it\ncumulatively. We note that this also includes the time for the\nalgorithm selection included in SMASH, which was around 2\nmilliseconds end-to-end.\nWe summarize our analysis in Figure 6, where Base refers\nto the baseline of executing the queries directly in the DBMS,\nRewriting refers to the time always using the rewriting to\nYannakakis-style evaluation, and SMASH refers to the use of\nour algorithm selection model as described above. To study the\nrobustness of our approach we perform these experiments on\nthree different DBMSs: PostgreSQL, Spark (via SparkSQL),\nand DuckDB. The significant technical differences between the\nthree systems provide us with a way to study the performance\nof our method independently of specific DBMS technologies.\nWe report timeouts as follows: if only one of the evaluation\nmethods (rewriting or base case) times out, then we report in\nFigure 6 for such queries as runtime the value of the timeout\n(= 100s). On the other hand, if both evaluation methods time\nout, we exclude them from the comparison, since algorithm\nselection cannot affect anything in such a case. Out of the\n441 queries involved in the test set, there were 27 queries\nthat timed out for both evaluation methods on PostgreSQL,\n13 queries that timed out for both evaluation methods on\nSparkSQL, and 30 queries that timed out for both evaluation\nmethods on DuckDB.\nConsistently over all systems, we can observe a large\nimprovement over both alternatives by using algorithm selec-\ntion. Furthermore, we see that even always rewriting overall\ncauses significant improvements over baseline execution of all\nthree systems tested. However, this improvement comes from\nspeedups specifically on queries that are hard for traditional\nRDBMS execution. These large improvements offset more\n\nPostgreSQL SparkSQL DuckDB020406059%63% 63%\n2% 2% 3%Slowdown cases (%)Rewriting SMASH\nFig. 7. Analysis of how often a query is slower than the base case when\nalways rewriting to Yannakakis-style execution vs. when rewriting depending\non our trained algorithm selection models (out of 441 queries in the test set).\ncommon minor slowdowns using the Rewriting approach.\nUsing SMASH we are able to get the best of both worlds,\nthe major speedups without the minor slower cases.\nWe conclude that, for our key question Q3regarding the\neffect of the quality of algorithm selection on query evaluation\ntimes, we can give a positive answer. Indeed, our algorithm se-\nlection clearly improves the e2e query evaluation times across\na number of different queries and three different DBMSs.\nTo provide further insight into how e2e query evaluation\nperformance is affected, Figure 7 shows the percentage of\nqueries in which Rewriting andSMASH are slower than Base .\nWhile the rewriting approach yields large improvements in\ne2e performance, it also causes minor slowdown of queries in\nover half of all cases. The data illustrates clearly that SMASH\nprovides a convincing solution to the problem, with minor\nslowdowns on only around 2% of the queries, combined with\neven larger improvements in e2e performance.\nE. The effect of Data Augmentation.\nWe explore the impact of data augmentation on model\nperformance through a focused ablation study, carefully exam-\nining the effectiveness of our augmentation strategies. Specif-\nically, we compare models trained on two distinct training\nsets: one encompassing the fully augmented dataset derived\nfrom the complete MEAMBench, and another restricted solely\ntobase queries with all augmented data removed. Table VI\nsuccinctly summarizes this comparison, presenting accuracy,\nprecision, and recall metrics for both the ”Base” set comprised\nof 0MA and enumeration queries without filter augmentation,\nand the full augmented training set. Our analysis reveals that\nincorporating data augmentation substantially enhances accu-\nracy and recall, with an even more pronounced improvement\nobserved in precision.\nF . Significance of Performance Improvements\nThe results presented in this section, particularly the run-\ntimes shown in Figure 6, demonstrate the clear effectiveness of\nour decision procedure in selecting the best evaluation method.\nFor completeness, we also conducted statistical significance\ntests to rigorously confirm that these improvements stem fromTABLE VI\nABLATION STUDY COMPARING THE PERFORMANCE OF A MODEL ON A\nTRAINING SET WITH AUGMENTED DATA TO ONE WITHOUT . VALUES ARE\nBASED ON THE EVALUATION OF THE REGRESSION MODEL WITH A\n0.5- THRESHOLD ON THE TEST SET .\nBase Augmented\nDBMS Acc. Prec. Rec. Acc. Prec. Rec.\nPostgres 0.88 0.83 0.89 0.95 0.95 0.93\nDuckDB 0.86 0.79 0.84 0.91 0.88 0.87\nSparkSQL 0.85 0.79 0.81 0.93 0.92 0.88\nthe proposed optimization techniques and are not the result of\nrandom variation.\nWe compared mean and median runtimes of our decision\nprocedure against always using the original DBMS method\nand always applying Yannakakis-style evaluation. Since the\ncomparisons are performed on the same test set, we employed\nstatistical tests that account for such dependencies. For median\ncomparisons, we used the Wilcoxon signed-rank test [86], and\nfor mean comparisons, the paired sample t-test [87]. Further\ndetails of these tests are provided in ??0b.\nHere, we simply report that all computed p-values are well\nbelow the common threshold of 0.005, clearly establishing that\nour results are statistically significant. In answering our key\nquestion Q3, we can thus clearly observe that the performance\nimprovements achieved by our algorithm selection method are\nindeed significant .\nVII. C ONCLUSION\nIn this paper, we tackled a persistent and fundamental\nchallenge in query optimization: many techniques that promise\nsignificant improvements deliver real-world performance gains\nonly in some , but not all, scenarios. Our study focused\non Yannakakis-style query evaluation, a method with strong\ntheoretical foundations that aims to reduce unnecessary in-\ntermediate results through semi-joins. Nevertheless, despite\nthis promising foundation, Yannakakis-style evaluation is of-\nten slower than conventional two-way join trees in practice,\ndepending on the specific query and data characteristics.\nTo unlock this potential, we framed the choice between\nYannakakis-style optimization and conventional DBMS eval-\nuation as an algorithm selection problem. Our decision proce-\ndure, grounded in empirical evidence across multiple popular\nRDBMSs, delivers substantial performance improvements and\ndemonstrates that learning when to apply an optimization is\njust as critical as the optimization itself.\nAlthough we focused on acyclic and 0MA queries, the\nmethodology we propose is broadly applicable. It opens the\ndoor to more intelligent optimization strategies across a wide\nrange of techniques, including decomposition-based methods\nand cyclic queries. In particular, we plan to combine our ap-\nproach with recent advances in hypertree decomposition-based\noptimizations [18], which have shown promising but variable\nperformance across different workloads. In such settings, a\nprincipled algorithm selection framework is key to maximizing\ntheir practical impact.\n\nAcknowledgements. This work has been funded\nby the Vienna Science and Technology Fund (WWTF)\n[10.47379/ICT2201,10.47379/VRG18013].\nREFERENCES\n[1] M. Yannakakis, “Algorithms for acyclic database schemes,” in Proceed-\nings of the 7th International Conference on Very Large Databases, VLDB\n1981, Cannes , pp. 82–94, VLDB, 1981.\n[2] M. Idris, M. Ugarte, and S. Vansummeren, “The dynamic yannakakis\nalgorithm: Compact and efficient query processing under updates,” in\nProceedings of the 2017 ACM International Conference on Management\nof Data, SIGMOD Conference 2017, Chicago, IL, USA, May 14-19,\n2017 (S. Salihoglu, W. Zhou, R. Chirkova, J. Yang, and D. Suciu, eds.),\npp. 1259–1274, ACM, 2017.\n[3] M. Idris, M. Ugarte, S. Vansummeren, H. V oigt, and W. Lehner, “General\ndynamic yannakakis: conjunctive queries with theta joins under updates,”\nVLDB J. , vol. 29, no. 2-3, pp. 619–653, 2020.\n[4] Q. Wang, X. Hu, B. Dai, and K. Yi, “Change propagation without joins,”\nCoRR , vol. abs/2301.04003, 2023.\n[5] M. Lanzinger, R. Pichler, and A. Selzer, “Avoiding materialisation for\nguarded aggregate queries,” CoRR , vol. abs/2406.17076, 2024. to be\npresented at VLDB 2025.\n[6] A. Birler, A. Kemper, and T. Neumann, “Robust join processing with\ndiamond hardened joins,” Proc. VLDB Endow. , vol. 17, no. 11, pp. 3215–\n3228, 2024.\n[7] Q. Wang, B. Chen, B. Dai, K. Yi, F. Li, and L. Lin, “Yannakakis+:\nPractical acyclic query evaluation with theoretical guarantees,” CoRR ,\nvol. abs/2504.03279, 2025.\n[8] X. Hu and Q. Wang, “Computing the difference of conjunctive queries\nefficiently,” Proc. ACM Manag. Data , vol. 1, no. 2, pp. 153:1–153:26,\n2023.\n[9] Q. Wang and K. Yi, “Conjunctive queries with comparisons,” in SIG-\nMOD ’22: International Conference on Management of Data, Philadel-\nphia, PA, USA, June 12 - 17, 2022 (Z. G. Ives, A. Bonifati, and A. E.\nAbbadi, eds.), pp. 108–121, ACM, 2022.\n[10] B. Dai, Q. Wang, and K. Yi, “Sparksql+: Next-generation query planning\nover spark,” in Companion of the 2023 International Conference on\nManagement of Data, SIGMOD/PODS 2023, Seattle, WA, USA, June\n18-23, 2023 (S. Das, I. Pandis, K. S. Candan, and S. Amer-Yahia, eds.),\npp. 115–118, ACM, 2023.\n[11] Y . Yang, H. Zhao, X. Yu, and P. Koutris, “Predicate transfer: Efficient\npre-filtering on multi-join queries,” in 14th Conference on Innovative\nData Systems Research, CIDR 2024, Chaminade, HI, USA, January 14-\n17, 2024 , www.cidrdb.org, 2024.\n[12] G. Gottlob, M. Lanzinger, D. M. Longo, C. Okulmus, R. Pichler, and\nA. Selzer, “Structure-guided query evaluation: Towards bridging the gap\nfrom theory to practice,” CoRR , vol. abs/2303.02723, 2023.\n[13] Y . Han, Z. Wu, P. Wu, R. Zhu, J. Yang, L. W. Tan, K. Zeng, G. Cong,\nY . Qin, A. Pfadler, Z. Qian, J. Zhou, J. Li, and B. Cui, “Cardinality\nestimation in DBMS: A comprehensive benchmark evaluation,” Proc.\nVLDB Endow. , vol. 15, no. 4, pp. 752–765, 2021.\n[14] M. Stonebraker and G. Kemnitz, “The postgres next generation database\nmanagement system,” Commun. ACM , vol. 34, no. 10, pp. 78–92, 1991.\n[15] M. Raasveldt and H. M ¨uhleisen, “DuckDB: an Embeddable Analytical\nDatabase,” in Proceedings of the 2019 International Conference on\nManagement of Data, SIGMOD Conference 2019 , pp. 1981–1984,\nACM, 2019.\n[16] M. Zaharia, R. S. Xin, P. Wendell, T. Das, M. Armbrust, A. Dave,\nX. Meng, J. Rosen, S. Venkataraman, M. J. Franklin, A. Ghodsi,\nJ. Gonzalez, S. Shenker, and I. Stoica, “Apache spark: a unified engine\nfor big data processing,” Commun. ACM , vol. 59, no. 11, pp. 56–65,\n2016.\n[17] G. Gottlob, M. Lanzinger, D. M. Longo, C. Okulmus, R. Pichler, and\nA. Selzer, “Reaching back to move forward: Using old ideas to achieve\na new level of query optimization (short paper),” in Proceedings of the\n15th Alberto Mendelzon International Workshop on Foundations of Data\nManagement (AMW 2023), Santiago de Chile, Chile, May 22-26, 2023\n(B. Kimelfeld, M. V . Martinez, and R. Angles, eds.), vol. 3409 of CEUR\nWorkshop Proceedings , CEUR-WS.org, 2023.\n[18] M. Lanzinger, C. Okulmus, R. Pichler, A. Selzer, and G. Gottlob, “Soft\nand constrained hypertree width,” arXiv preprint arXiv:2412.11669 ,\n2024. to appear in PODS 2025.[19] X. Zhou, G. Li, J. Wu, J. Liu, Z. Sun, and X. Zhang, “A learned query\nrewrite system,” Proc. VLDB Endow. , vol. 16, no. 12, pp. 4110–4113,\n2023.\n[20] X. Zhou, C. Chai, G. Li, and J. Sun, “Database meets artificial\nintelligence: A survey,” IEEE Trans. Knowl. Data Eng. , vol. 34, no. 3,\npp. 1096–1116, 2022.\n[21] M. H. Graham, “On The Universal Relation,” tech. rep., University of\nToronto, 1979.\n[22] C. T. Yu and M. Z. ¨Ozsoyo ˘glu, “An algorithm for tree-query membership\nof a distributed query,” in The IEEE Computer Society’s Third Inter-\nnational Computer Software and Applications Conference, COMPSAC\n1979 , pp. 306–312, 1979.\n[23] J. Zhao, K. Su, Y . Yang, X. Yu, P. Koutris, and H. Zhang, “Debunking\nthe myth of join ordering: Toward robust SQL analytics,” CoRR ,\nvol. abs/2502.15181, 2025.\n[24] G. Gottlob, N. Leone, and F. Scarcello, “Hypertree decompositions and\ntractable queries,” J. Comput. Syst. Sci. , vol. 64, no. 3, pp. 579–627,\n2002.\n[25] I. Adler, G. Gottlob, and M. Grohe, “Hypertree width and related\nhypergraph invariants,” Eur. J. Comb. , vol. 28, no. 8, pp. 2167–2181,\n2007.\n[26] M. Grohe and D. Marx, “Constraint solving via fractional edge covers,”\nACM Trans. Algorithms , vol. 11, no. 1, pp. 4:1–4:20, 2014.\n[27] C. R. Aberger, A. Lamb, S. Tu, A. N ¨otzli, K. Olukotun, and C. R ´e,\n“Emptyheaded: A relational engine for graph processing,” ACM Trans.\nDatabase Syst. , vol. 42, no. 4, pp. 20:1–20:44, 2017.\n[28] A. Perelman and C. R ´e, “Duncecap: Compiling worst-case optimal\nquery plans,” in Proceedings of the 2015 ACM SIGMOD International\nConference on Management of Data, Melbourne, Victoria, Australia,\nMay 31 - June 4, 2015 (T. K. Sellis, S. B. Davidson, and Z. G. Ives,\neds.), pp. 2075–2076, ACM, 2015.\n[29] S. Tu and C. R ´e, “Duncecap: Query plans using generalized hypertree\ndecompositions,” in Proceedings of the 2015 ACM SIGMOD Inter-\nnational Conference on Management of Data, Melbourne, Victoria,\nAustralia, May 31 - June 4, 2015 (T. K. Sellis, S. B. Davidson, and\nZ. G. Ives, eds.), pp. 2077–2078, ACM, 2015.\n[30] H. Q. Ngo, E. Porat, C. R ´e, and A. Rudra, “Worst-case optimal join\nalgorithms,” J. ACM , vol. 65, no. 3, pp. 16:1–16:40, 2018.\n[31] F. Scarcello, G. Greco, and N. Leone, “Weighted hypertree decompo-\nsitions and optimal query plans,” in Proceedings of the Twenty-third\nACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database\nSystems, June 14-16, 2004, Paris, France (C. Beeri and A. Deutsch,\neds.), pp. 210–221, ACM, 2004.\n[32] X. Zhou, G. Li, C. Chai, and J. Feng, “A learned query rewrite system\nusing monte carlo tree search,” Proc. VLDB Endow. , vol. 15, no. 1,\npp. 46–58, 2021.\n[33] Z. Wang, Z. Zhou, Y . Yang, H. Ding, G. Hu, D. Ding, C. Tang,\nH. Chen, and J. Li, “Wetune: Automatic discovery and verification\nof query rewrite rules,” in SIGMOD ’22: International Conference on\nManagement of Data, Philadelphia, PA, USA, June 12 - 17, 2022 (Z. G.\nIves, A. Bonifati, and A. E. Abbadi, eds.), pp. 94–107, ACM, 2022.\n[34] T. D ¨okeroglu, M. A. Bayir, and A. Cosar, “Robust heuristic algorithms\nfor exploiting the common tasks of relational cloud database queries,”\nAppl. Soft Comput. , vol. 30, pp. 72–82, 2015.\n[35] J. Zhang, Y . Liu, K. Zhou, G. Li, Z. Xiao, B. Cheng, J. Xing, Y . Wang,\nT. Cheng, L. Liu, M. Ran, and Z. Li, “An end-to-end automatic\ncloud database tuning system using deep reinforcement learning,” in\nProceedings of the 2019 International Conference on Management of\nData, SIGMOD Conference 2019, Amsterdam, The Netherlands, June\n30 - July 5, 2019 , pp. 415–432, ACM, 2019.\n[36] Y . Zhu, J. Liu, M. Guo, Y . Bao, W. Ma, Z. Liu, K. Song, and Y . Yang,\n“Bestconfig: tapping the performance potential of systems via automatic\nconfiguration tuning,” in Proceedings of the 2017 Symposium on Cloud\nComputing, SoCC 2017, Santa Clara, CA, USA, September 24-27, 2017 ,\npp. 338–350, ACM, 2017.\n[37] D. V . Aken, A. Pavlo, G. J. Gordon, and B. Zhang, “Automatic database\nmanagement system tuning through large-scale machine learning,” in\nProceedings of the 2017 ACM International Conference on Management\nof Data, SIGMOD Conference 2017, Chicago, IL, USA, May 14-19,\n2017 , pp. 1009–1024, ACM, 2017.\n[38] G. Li, X. Zhou, S. Li, and B. Gao, “Qtune: A query-aware database\ntuning system with deep reinforcement learning,” Proc. VLDB Endow. ,\nvol. 12, no. 12, pp. 2118–2130, 2019.\n\n[39] J. Tan, T. Zhang, F. Li, J. Chen, Q. Zheng, P. Zhang, H. Qiao, Y . Shi,\nW. Cao, and R. Zhang, “ibtune: Individualized buffer tuning for large-\nscale cloud databases,” Proc. VLDB Endow. , vol. 12, no. 10, pp. 1221–\n1234, 2019.\n[40] H. Herodotou, H. Lim, G. Luo, N. Borisov, L. Dong, F. B. Cetin, and\nS. Babu, “Starfish: A self-tuning system for big data analytics,” in Fifth\nBiennial Conference on Innovative Data Systems Research, CIDR 2011,\nAsilomar, CA, USA, January 9-12, 2011, Online Proceedings , pp. 261–\n272, www.cidrdb.org, 2011.\n[41] N. Nguyen, M. M. H. Khan, and K. Wang, “Towards automatic tuning\nof apache spark configuration,” in 11th IEEE International Conference\non Cloud Computing, CLOUD 2018, San Francisco, CA, USA, July 2-7,\n2018 , pp. 417–425, IEEE Computer Society, 2018.\n[42] S. Chaudhuri and V . R. Narasayya, “An efficient cost-driven index\nselection tool for microsoft SQL server,” in VLDB’97, Proceedings of\n23rd International Conference on Very Large Data Bases, August 25-29,\n1997, Athens, Greece , pp. 146–155, Morgan Kaufmann, 1997.\n[43] K. Schnaitter, S. Abiteboul, T. Milo, and N. Polyzotis, “On-line index se-\nlection for shifting workloads,” in Proceedings of the 23rd International\nConference on Data Engineering Workshops, ICDE 2007, 15-20 April\n2007, Istanbul, Turkey , pp. 459–468, IEEE Computer Society, 2007.\n[44] R. Marcus, P. Negi, H. Mao, N. Tatbul, M. Alizadeh, and T. Kraska,\n“Bao: Making learned query optimization practical,” in SIGMOD ’21:\nInternational Conference on Management of Data, Virtual Event, China,\nJune 20-25, 2021 (G. Li, Z. Li, S. Idreos, and D. Srivastava, eds.),\npp. 1275–1288, ACM, 2021.\n[45] R. Marcus, P. Negi, H. Mao, N. Tatbul, M. Alizadeh, and T. Kraska,\n“Bao: Making learned query optimization practical,” SIGMOD Rec. ,\nvol. 51, no. 1, pp. 6–13, 2022.\n[46] S. Mizzaro, J. Mothe, K. Roitero, and M. Z. Ullah, “Query performance\nprediction and effectiveness evaluation without relevance judgments:\nTwo sides of the same coin,” in The 41st International ACM SIGIR\nConference on Research & Development in Information Retrieval, SIGIR\n2018, Ann Arbor, MI, USA, July 08-12, 2018 (K. Collins-Thompson,\nQ. Mei, B. D. Davison, Y . Liu, and E. Yilmaz, eds.), pp. 1233–1236,\nACM, 2018.\n[47] Y . Park, S. Zhong, and B. Mozafari, “Quicksel: Quick selectivity\nlearning with mixture models,” in Proceedings of the 2020 International\nConference on Management of Data, SIGMOD Conference 2020, online\nconference [Portland, OR, USA], June 14-19, 2020 , pp. 1017–1033,\nACM, 2020.\n[48] J. Ortiz, M. Balazinska, J. Gehrke, and S. S. Keerthi, “An em-\npirical analysis of deep learning for cardinality estimation,” CoRR ,\nvol. abs/1905.06425, 2019.\n[49] Y . Dong, P. Indyk, I. P. Razenshteyn, and T. Wagner, “Learn-\ning sublinear-time indexing for nearest neighbor search,” CoRR ,\nvol. abs/1901.08544, 2019.\n[50] A. Dutt, C. Wang, A. Nazi, S. Kandula, V . R. Narasayya, and S. Chaud-\nhuri, “Selectivity estimation for range predicates using lightweight\nmodels,” Proc. VLDB Endow. , vol. 12, no. 9, pp. 1044–1057, 2019.\n[51] A. Kipf, T. Kipf, B. Radke, V . Leis, P. A. Boncz, and A. Kemper,\n“Learned cardinalities: Estimating correlated joins with deep learning,”\nin9th Biennial Conference on Innovative Data Systems Research, CIDR\n2019, Asilomar, CA, USA, January 13-16, 2019, Online Proceedings ,\nwww.cidrdb.org, 2019.\n[52] R. Marcus, P. Negi, H. Mao, C. Zhang, M. Alizadeh, T. Kraska,\nO. Papaemmanouil, and N. Tatbul, “Neo: A learned query optimizer,”\nProc. VLDB Endow. , vol. 12, no. 11, pp. 1705–1718, 2019.\n[53] R. Marcus and O. Papaemmanouil, “Plan-structured deep neural network\nmodels for query performance prediction,” Proc. VLDB Endow. , vol. 12,\nno. 11, pp. 1733–1746, 2019.\n[54] M. Heimel, M. Kiefer, and V . Markl, “Self-tuning, gpu-accelerated\nkernel density models for multidimensional selectivity estimation,” in\nProceedings of the 2015 ACM SIGMOD International Conference on\nManagement of Data, Melbourne, Victoria, Australia, May 31 - June 4,\n2015 , pp. 1477–1492, ACM, 2015.\n[55] M. Stillger, G. M. Lohman, V . Markl, and M. Kandil, “LEO - db2’s\nlearning optimizer,” in VLDB 2001, Proceedings of 27th International\nConference on Very Large Data Bases, September 11-14, 2001, Roma,\nItaly, pp. 19–28, Morgan Kaufmann, 2001.\n[56] I. Trummer, J. Wang, Z. Wei, D. Maram, S. Moseley, S. Jo, J. Anton-\nakakis, and A. Rayabhari, “Skinnerdb: Regret-bounded query evaluation\nvia reinforcement learning,” ACM Trans. Database Syst. , vol. 46, no. 3,\npp. 9:1–9:45, 2021.[57] K. Tzoumas, T. Sellis, and C. S. Jensen, “A reinforcement learning\napproach for adaptive query processing,” History , pp. 1–25, 2008.\n[58] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The case for\nlearned index structures,” in Proceedings of the 2018 International Con-\nference on Management of Data, SIGMOD Conference 2018, Houston,\nTX, USA, June 10-15, 2018 , pp. 489–504, ACM, 2018.\n[59] J. Ding, U. F. Minhas, J. Yu, C. Wang, J. Do, Y . Li, H. Zhang,\nB. Chandramouli, J. Gehrke, D. Kossmann, D. B. Lomet, and T. Kraska,\n“ALEX: an updatable adaptive learned index,” in Proceedings of the\n2020 International Conference on Management of Data, SIGMOD\nConference 2020, online conference [Portland, OR, USA], June 14-19,\n2020 , pp. 969–984, ACM, 2020.\n[60] Y . Wu, J. Yu, Y . Tian, R. Sidle, and R. Barber, “Designing succinct\nsecondary indexing mechanism by exploiting column correlations,” in\nProceedings of the 2019 International Conference on Management of\nData, SIGMOD Conference 2019, Amsterdam, The Netherlands, June\n30 - July 5, 2019 , pp. 1223–1240, ACM, 2019.\n[61] L. Ma, D. V . Aken, A. Hefny, G. Mezerhane, A. Pavlo, and G. J. Gordon,\n“Query-based workload forecasting for self-driving database manage-\nment systems,” in Proceedings of the 2018 International Conference on\nManagement of Data, SIGMOD Conference 2018, Houston, TX, USA,\nJune 10-15, 2018 , pp. 631–645, ACM, 2018.\n[62] M. Ma, Z. Yin, S. Zhang, S. Wang, C. Zheng, X. Jiang, H. Hu, C. Luo,\nY . Li, N. Qiu, F. Li, C. Chen, and D. Pei, “Diagnosing root causes of\nintermittent slow queries in large-scale cloud databases,” Proc. VLDB\nEndow. , vol. 13, no. 8, pp. 1176–1189, 2020.\n[63] R. Taft, N. El-Sayed, M. Serafini, Y . Lu, A. Aboulnaga, M. Stonebraker,\nR. Mayerhofer, and F. J. Andrade, “P-store: An elastic database system\nwith predictive provisioning,” in Proceedings of the 2018 International\nConference on Management of Data, SIGMOD Conference 2018, Hous-\nton, TX, USA, June 10-15, 2018 (G. Das, C. M. Jermaine, and P. A.\nBernstein, eds.), pp. 205–219, ACM, 2018.\n[64] H. Grushka-Cohen, O. Biller, O. Sofer, L. Rokach, and B. Shapira,\n“Diversifying database activity monitoring with bandits,” CoRR ,\nvol. abs/1910.10777, 2019.\n[65] X. Zhou, J. Sun, G. Li, and J. Feng, “Query performance prediction\nfor concurrent queries using graph embedding,” Proc. VLDB Endow. ,\nvol. 13, no. 9, pp. 1416–1428, 2020.\n[66] R. Bhaskar, S. Laxman, A. D. Smith, and A. Thakurta, “Discovering\nfrequent patterns in sensitive data,” in Proceedings of the 16th ACM\nSIGKDD International Conference on Knowledge Discovery and Data\nMining, Washington, DC, USA, July 25-28, 2010 (B. Rao, B. Krishna-\npuram, A. Tomkins, and Q. Yang, eds.), pp. 503–512, ACM, 2010.\n[67] P. Colombo and E. Ferrari, “Efficient enforcement of action-aware\npurpose-based access control within relational database management\nsystems,” in 32nd IEEE International Conference on Data Engineering,\nICDE 2016, Helsinki, Finland, May 16-20, 2016 , pp. 1516–1517, IEEE\nComputer Society, 2016.\n[68] M. Lodeiro-Santiago, C. Caballero-Gil, and P. Caballero-Gil, “Col-\nlaborative sql-injections detection system with machine learning,” in\nProceedings of the 1st International Conference on Internet of Things\nand Machine Learning, IML 2017, Liverpool, United Kingdom, October\n17-18, 2017 (H. Hamdan, D. E. Boubiche, and F. Klett, eds.), pp. 45:1–\n45:5, ACM, 2017.\n[69] N. M. Sheykhkanloo, “A learning-based neural network model for the\ndetection and classification of SQL injection attacks,” Int. J. Cyber Warf.\nTerror. , vol. 7, no. 2, pp. 16–41, 2017.\n[70] L. O. Batista, G. A. de Silva, V . S. Ara ´ujo, V . J. S. Ara ´ujo, T. S. Rezende,\nA. J. Guimar ˜aes, and P. V . de Campos Souza, “Fuzzy neural networks to\ncreate an expert system for detecting attacks by SQL injection,” CoRR ,\nvol. abs/1901.02868, 2019.\n[71] B. He and I. Ounis, “Query performance prediction,” Inf. Syst. , vol. 31,\nno. 7, pp. 585–594, 2006.\n[72] W. Wu, Y . Chi, S. Zhu, J. Tatemura, H. Hacig ¨um¨us, and J. F. Naughton,\n“Predicting query execution time: Are optimizer cost models really un-\nusable?,” in 29th IEEE International Conference on Data Engineering,\nICDE 2013, Brisbane, Australia, April 8-12, 2013 (C. S. Jensen, C. M.\nJermaine, and X. Zhou, eds.), pp. 1081–1092, IEEE Computer Society,\n2013.\n[73] N. Zhang, P. J. Haas, V . Josifovski, G. M. Lohman, and C. Zhang,\n“Statistical learning techniques for costing XML queries,” in Proceed-\nings of the 31st International Conference on Very Large Data Bases,\nTrondheim, Norway, August 30 - September 2, 2005 (K. B ¨ohm, C. S.\n\nJensen, L. M. Haas, M. L. Kersten, P. Larson, and B. C. Ooi, eds.),\npp. 289–300, ACM, 2005.\n[74] Y . Zhou and W. B. Croft, “Query performance prediction in web\nsearch environments,” in SIGIR 2007: Proceedings of the 30th Annual\nInternational ACM SIGIR Conference on Research and Development in\nInformation Retrieval, Amsterdam, The Netherlands, July 23-27, 2007\n(W. Kraaij, A. P. de Vries, C. L. A. Clarke, N. Fuhr, and N. Kando,\neds.), pp. 543–550, ACM, 2007.\n[75] J. Duggan, U. C ¸ etintemel, O. Papaemmanouil, and E. Upfal, “Perfor-\nmance prediction for concurrent database workloads,” in Proceedings of\nthe ACM SIGMOD International Conference on Management of Data,\nSIGMOD 2011, Athens, Greece, June 12-16, 2011 (T. K. Sellis, R. J.\nMiller, A. Kementsietsidis, and Y . Velegrakis, eds.), pp. 337–348, ACM,\n2011.\n[76] M. Akdere, U. C ¸ etintemel, M. Riondato, E. Upfal, and S. B. Zdonik,\n“Learning-based query performance modeling and prediction,” in IEEE\n28th International Conference on Data Engineering (ICDE 2012),\nWashington, DC, USA (Arlington, Virginia), 1-5 April, 2012 (A. Ke-\nmentsietsidis and M. A. V . Salles, eds.), pp. 390–401, IEEE Computer\nSociety, 2012.\n[77] V . Leis, A. Gubichev, A. Mirchev, P. A. Boncz, A. Kemper, and\nT. Neumann, “How good are query optimizers, really?,” Proc. VLDB\nEndow. , vol. 9, no. 3, pp. 204–215, 2015.\n[78] J. Leskovec and R. Sosic, “SNAP: A general-purpose network analysis\nand graph-mining library,” ACM Trans. Intell. Syst. Technol. , vol. 8,\nno. 1, pp. 1:1–1:20, 2016.\n[79] A. Mhedhbi, M. Lissandrini, L. Kuiper, J. Waudby, and G. Sz ´arnyas,\n“LSQB: a large-scale subgraph query benchmark,” in GRADES-NDA\n’21: Proceedings of the 4th ACM SIGMOD Joint International Workshop\non Graph Data Management Experiences & Systems (GRADES) and\nNetwork Data Analytics (NDA), Virtual Event, China, 20 June 2021\n(V . Kalavri and N. Yakovets, eds.), pp. 8:1–8:11, ACM, 2021.\n[80] D. S. Himmelstein, A. Lizee, C. Hessler, L. Brueggeman, S. L.\nChen, D. Hadley, A. GreenP, ouya Khankhanian, and S. E. Baranzini,\n“Systematic integration of biomedical knowledge prioritizes drugs for\nrepurposing,” eLife , vol. 6, no. e26726, pp. 1–35, 2017.\n[81] J. Chen, Y . Huang, M. Wang, S. Salihoglu, and K. Salem, “Accurate\nsummary-based cardinality estimation through the lens of cardinality\nestimation graphs,” SIGMOD Rec. , vol. 52, no. 1, pp. 94–102, 2023.\n[82] L. Bekkers, F. Neven, S. Vansummeren, and Y . R. Wang, “Instance-\noptimal acyclic join processing without regret: Engineering the yan-\nnakakis algorithm in column stores,” CoRR , vol. abs/2411.04042, 2024.\n[83] M. Abseher, N. Musliu, and S. Woltran, “Improving the efficiency of\ndynamic programming on tree decompositions via machine learning,” J.\nArtif. Intell. Res. , vol. 58, pp. 829–858, 2017.\n[84] Y . Feng, H. You, Z. Zhang, R. Ji, and Y . Gao, “Hypergraph neural\nnetworks,” in The Thirty-Third AAAI Conference on Artificial Intelli-\ngence, AAAI 2019, The Thirty-First Innovative Applications of Artificial\nIntelligence Conference, IAAI 2019, The Ninth AAAI Symposium on\nEducational Advances in Artificial Intelligence, EAAI 2019, Honolulu,\nHawaii, USA, January 27 - February 1, 2019 , pp. 3558–3565, AAAI\nPress, 2019.\n[85] L. Breiman, Classification and regression trees . Routledge, 2017.\n[86] F. Wilcoxon, “Individual comparisons by ranking methods,” Biometrics\nBulletin , vol. 1, no. 6, pp. 80–83, 1945.\n[87] A. Ross and V . L. Willson, Paired Samples T-Test , pp. 17–19. Rotterdam:\nSensePublishers, 2017.\n[88] D. B ¨ohm, “To rewrite or not to rewrite: Decision making in query\noptimization of sql queries,” Master’s thesis, Technische Universit ¨at\nWien, 2024.\n\nFig. 8. Workflow for rewriting the queries.\nAPPENDIX\nIn this section we aim to give further details into the data\naugmentation. In Theorem 4.1, we have illustrated the filter\naugmentation. We continue here by adding another example\nto illustrate enumeration augmentation.\nExample A.1:\nConsider the query qaug1, given in Theorem 4.1. We present\nthree different enumeration augmentations, whereby the query\noutput is transformed from producing a single output to enu-\nmerating a number of rows. One option to “enum-augment”\nqaug1is to swap the aggregation function MIN(v.Id) , with a\nprojection on the Idcolumn of the users table, and the UserID\ncolumn of the votes table producing the query qe1\naug1. Other\noptions, shown below, lead to the queries qe2\naug1andqe3\naug1.\nqaug1:SELECT MIN (u.Id)\nFROM votes ASv, badges ASb, users ASu\nWHERE u.Id = v.UserId AND v.UserId = b.UserId\nAND v.BountyAmount¿=40 AND v.BountyAmount¡=50\nAND u.DownVotes=0\nqe1\naug1:SELECT u.Id, v.UserId\nFROM votes ASv, badges ASb, users ASu\nWHERE u.Id = v.UserId AND v.UserId = b.UserId\nAND v.BountyAmount¿=40 AND v.BountyAmount¡=50\nAND u.DownVotes=0\nqe2\naug1:SELECT u.Id, b.UserId\nFROM votes ASv, badges ASb, users ASu\nWHERE u.Id = v.UserId AND v.UserId = b.UserId\nAND v.BountyAmount¿=40 AND v.BountyAmount¡=50\nAND u.DownVotes=0\nqe3\naug1:SELECT v.UserId, b.UserId\nFROM votes ASv, badges ASb, users ASu\nWHERE u.Id = v.UserId AND v.UserId = b.UserId\nAND v.BountyAmount¿=40 AND v.BountyAmount¡=50\nAND u.DownVotes=0\nIn this section we aim to give the reader technical insights\ninto the query rewriting process, where we follow the approach\nof [12]. This approach involves rewriting each query into\nan equivalent sequence of SQL queries (i.e. one that gives\nthe same result) but which “forces” (or, at least, guides) the\nDBMSs to apply a Yannakakis-style evaluation strategy. The\nrewrite process is visualized in Figure 8.\nWe thus take a SQL query as input and first transform it\ninto a hypergraph. The approach by [12] begins by applying\nthe GYO-reduction [22]. One thus verifies that the CQ is\nacyclic and, if so, constructs a join tree. This approach does not\nsupport cyclic queries, since they do not permit join trees. In\ncase of 0MA queries, containing an aggregate expression withusers\nbadges votes\nFig. 9. Join tree for q.\na single attribute, one takes the relation of this single attribute\nas the root of the join tree, as this allows to significantly simply\nthe Yannakis’ algorithm by skipping multiple traversals of the\njoin tree, thus allowing for a more efficient evaluation. The\nrewriting proceeds by creating a sequence of SQL queries.\nOne traverses the tree in a bottom-up fashion, starting from the\nleafs, and produces auxillary tables via CREATE VIEW , for the\nleaf nodes, and CREATE UNLOGGED TABLE , for the inner nodes\nof the join tree. For 0MA queries the aggregation is performed\nlast, after the creation of auxiliary tables.\nWe illustrate the query rewrite process by revisiting the\n0MA query qfrom Theorem 4.1. To make it easier to follow,\nwe provide the join tree of qin Figure 9 and also repeat the\ndefinition of qitself.\nq:SELECT MIN (u.Id)\nFROM votes asv, badges asb, users asu\nWHERE u.Id = v.UserId AND v.UserId = b.UserId\nAND v.BountyAmount¿=0 AND v.BountyAmount¡=50\nAND u.DownVotes=0\nqr1:CREATE VIEW E3AS SELECT *\nFROM users ASusers\nWHERE users.DownVotes = 0\nqr2:CREATE VIEW E2AS SELECT *\nFROM badges ASbadges\nqr3:CREATE UNLOGGED TABLE E3E2 AS SELECT *\nFROM E3WHERE EXISTS (SELECT 1\nFROM E2\nWHERE E3.Id=E2.UserId)\nqr4:CREATE VIEW E1AS SELECT *\nFROM votes ASvotes\nWHERE CAST (votes.BountyAmount ASINTEGER) ¿= 0\nAND CAST (votes.BountyAmount ASINTEGER) ¡= 50\nqr5:CREATE UNLOGGED TABLE E3E2E1 AS\nSELECT MIN (Id) ASEXPR$0\nFROM E3E2 WHERE EXISTS (SELECT 1\nFROM E1\nWHERE E3E2.Id=E1.UserId)\nqr6:SELECT *FROM E3E2E1\nIn the rewriting of the example query, we can see that\nusers and badges are joined first, even if they did not have\nan equality condition together in the original query. This is\ndue to the fact that the three relations are joined on the same\nattribute and, after the replacement of equi-joins by natural\njoins, the distinction between explicit and implicit equality\ncondition is irrelevant (and also not visible anymore). The use\nofUNLOGGED TABLE is an optimization to guide the DBMS to\navoid writing the intermediate views to disk.\nWe note that the rewriting also produces a sequence of\nDROP statements to delete the created tables after the evalu-\n\nFig. 10. Join tree of q1.\nFig. 11. Join tree of q2.\nation.\nIn this section, we aim to illustrate the complete feature\nvector for two example queries from our data set, namely\n“HETIO 2-01-CbGaD ”, termed q1and “ HETIO 3-06-CdGuCtD ”\ntermed q2, shown below.\nq1:SELECT MIN (c.nid)\nFROM compound c, binds b, gene g, associates a, disease d\nWHERE c .nid = b.sid AND b.tid = g.nid AND\ng.nid = a.tid AND a.sid = d.nid\nq2:SELECT MIN (c1.nid)\nFROM compound c1, downregulates d1, gene g,\nupregulates u2, compound c2, treats t, disease d\nWHERE c1.nid = d1.sid AND d1.tid = g.nid AND\ng.nid = u2.tid AND u2.sid = c2.nid AND\nc2.nid = t.sid AND t.tid = d.nid\nTheir calculated join trees are given in Figures 10 and 11, re-\nspectively. Recall from Section IV-E that some of the features\nof the queries are determined by the join tree. In Table VII, the\nvalues of all features of these two queries are given. For the\nset-based features, we also explicitly show the six statistical\ndata points that are extracted from the set to produce the\nfeature vector used for the models to be trained.\nWe provide here a list of all hyperparameter values for\nthe various model types considered here, i.e., three types of\nneural network models, k-NN, random forests, and SVM, and\ndecision trees. The details are given in Table VIII. The experi-\nments presented here are a continuation of the work in [88], to\nwhich we can refer for even more detailed explanations of the\nmodels and hyperparameters, as well as further literature on\nthe machine learning concepts. In cases where mutiple hyper-\nparameters are mentioned (such as ’kernel=linear/poly/rbf’),\nwe have experimented with all of these. We provide a short\ndescrition of the most important hyperparameters:k-NN k is the number of neighbors used by the model\nRandom forest The number of estimators refers to the number of\nindividual decision trees making up the forest\nSVM The kernel of the SVM is the function which defines\nthe decision boundary.\nMLP, HGNN, MLP+HGNN For classification, we used cross-entropy loss, and\nfor regression MSE (mean-squared-error). The loss\nfunction is used in the training of the model (gradient\ndescent) to compare the actual output to the desired\noutput. The batch size is the number of samples used\nin a single training step. The number of epochs refers\nto the number of steps used in training. Learning\nrate is a parameter controlling how ”fast” gradient\ndescent adapts the weights of the model.\nThe layer-configurations of the neural network mod-\nels, such as ’in-40-10-out’, describe how the nodes\nare arranged in the layers of the models. In this\nexample, ’in’ is the input layer, of in most cases\n30 features, and ’40’ and ’10’ refer to the sizes\nof the intermediate layers. Layers are always fully\nconnected, with ReLU activation functions.\n\nTABLE VII\nACOMPLETE OVERVIEW OF THE FEATURES FOR THE QUERIES q1ANDq2. W E REFER TO SECTION IV-E FOR AN EXPLANATION OF EACH FEATURE .\nRECALL THAT EACH FEATURE THAT CONSISTS OF A SET OF NUMBERS IS INDICATED BY THE USE OF AN ASTERIX IN ITS NAME E .G. B7∗. FOR THESE\nSET-BASED FEATURES WE ALSO STATE THE STATISTICAL DATA THAT IS USED IN THE TRAINING TO PRODUCE A FIXED -WIDTH VECTOR .\nFeature q1 q2\nB1: is 0MA? 1 1\nB2: number of relations 5 7\nB3: number of conditions 4 6\nB4: number of filters 0 0\nB5: number of joins 4 6\nB6: depth 2 3\nB7∗: container counts {1, 1, 1, 1, 1, 2, 3 } { 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3 }\nmin 1\nmax 3\nmean 1.43\nq25 1\nmed. 1\nq75 1.5min 1\nmax 3\nmean 1.27\nq25 1\nmed. 1\nq75 1\nB8∗: branching degrees {3, 1} { 2, 3, 1 }\nmin 1\nmax 3\nmean 2\nq25 1.5\nmed. 2\nq75 2.5min 1\nmax 3\nmean 2\nq25 1.5\nmed. 2\nq75 2.5\nP1: estimated total cost 1175.3 10283.6\nP2∗: estimated single table rows {23142, 25246, 137,1, 1552 } { 154076, 1552, 146276, 1510, 137, 1552, 20945 }\nmin 1\nmax 25246\nmean 10015.6\nq25 137\nmed. 1552\nq75 23142min 137\nmax 154076\nmean 46578.29\nq25 1531\nmed. 1552\nq75 83610.5\nP3∗: estimated join rows {1190, 2361, 626, 626 } { 493338, 22993, 2578, 2578, 446, 446 }\nmin 626\nmax 2361\nmean 1200.75\nq25 626\nmed. 908\nq75 1482.75min 446\nmax 493338\nmean 87063.17\nq25 979\nmed. 2578\nq75 17889.25\nD1∗: estimated cardinalities {31829, 29051, 26334, 26334, {4993749, 354275, 338789, 322069,2299,\n418, 2299, 29051, 24035,26334 } 2299, 338789, 2090, 24035,2299, 322069, 418, 2090 }\nmin 418\nmax 31829\nmean 21742.78\nq25 24035\nmed. 26334\nq75 29051min 418\nmax 4993749\nmean 515790\nq25 2299\nmed. 24035\nq75 338789\n\nTABLE VIII\nAN OVERVIEW OF ALL HYPERPARAMETERS OF THE MODELS USED DURING TRAINING . FOR THE MLP MODEL ,THE MEANING OF “IN”IN THE LAYER\nCOLUMN REFERS TO THE NUMBER OF FEATURES ,AND THE WORLD “OUT”REFERS TO THE NUMBER OF CLASSES (2,3 AND 1FOR REGRESSION ).\nmodel hyperparameter layer\nk-NN k=5 -\nDecision tree - -\nRandom forest n estimators=100 -\nSVM kernel=linear/poly/rbf -\nMLPLoss=Cross-Entropy/MSE\nBatch size=100\nEpochs=300 (saving best model)\nLearning rate=0.1in-5-out\nin-10-out\nin-20-out\nin-25-out\nin-40-out\nin-60-out\nin-10-5-out\nin-20-10-out\nin-40-20-out\nin-40-10-out\nin-60-40-out\nin-60-20-out\nin-80-50-out\nsmall median, best MLP\nsmall mean, best MLP\nsmall min, best MLP\nsmall max, best MLP\nsmall q25, best MLP\nsmall q75, best MLP\ncustom, best MLP\nHGNNLoss=Cross-Entropy/MSE\nEpochs=100 (saving best model)\nLearning rate=0.001\nMax-Poolingkernel 3x3, 1-16-32-out\nkernel 3x3, 1-32-16-out\nkernel 3x3, 1-16-32-16-out\nkernel 3x3, 1-32-64-out\nkernel 3x3, 1-4-16-out\ncombinedLoss=Cross-Entropy/MSE\nEpochs=100 (saving best model)\nLearning rate=0.001\nMax-Poolingbest MLP-2/best HGNN-2/4-out\nbest MLP-5/best HGNN-5/10-out\nbest MLP-5/best HGNN-5/10-20-out\nbest MLP-10/best HGNN-10/20-40-2\nbest MLP-10/best HGNN-10/20-60-20-2\n\nTABLE IX\nMOST IMPORTANT FEATURES ACCORDING TO GINI COEFFICIENTS OF THE\nDECISION TREE MODELS .PostgreSQLFeature Gini\nmax(est join rows) 0.369\nis 0MA? 0.157\nq75(est. sing. table rows) 0.069\ntotal cost 0.053\nmin(est. sing. join rows) 0.051\nq25(est. sing. join rows) 0.044\nmedian(est. join rows) 0.033\nq25(est. single. table rows) 0.033\nDuckDBFeature Gini\nis 0MA? 0.280\nmean(cont. c.) 0.206\nmax(est. card.) 0.147\nq25(est. card.) 0.061\nq75(est. card.) 0.057\nmedian(est. card.) 0.047\nmean(est. card.) 0.040\nmin(est. card.) 0.037SparkSQLFeature Gini\nis 0MA? 0.246\nmax(est. sing. join rows) 0.212\n#joins 0.135\nest. total cost 0.092\nq25(est. sing. table rows) 0.053\nmedian(est. sing. table rows) 0.044\nmax(est. sing. table rows) 0.037\nmean(cont. c.) 0.032\nWe present here further information on the experiments\nconducted in Section VI.\na) Full details on Gini coefficients.: Recall that in Sec-\ntion VI we mention the role of Gini coefficients and list only\nthe five features with the highest coefficients for PostgreSQL\nand DuckDB. As was mentioned, the Gini coefficient, as used\nin this paper, measures the contribution of the feature to the\noutputs of the model. In Table IX we then give a complete\noverview of the 8 highest Gini coefficients among the features\nof the trained decision tree models for all three DBMSs,\nincluding SparkSQL.\nb) Further Details on the Significance Tests.: We briefly\ncommented in Section VI on the significance tests we per-\nformed. Here we give a more detailed report, including expla-\nnations on how these tests are defined.\nFor our significance tests, we compare the mean or me-\ndian of the runtimes achieved when applying our decision\nprocedure versus always using the original evaluation method\nof the DBMSs and always using Yannakakis-style evaluation,\nrespectively.\nSince we compare runtimes obtained with the same test set,\nwe need to use statistical tests, which take these dependencies\ninto consideration. For the median we take the Wilcoxon sign-\nrank test [86] and for the mean we use a paired sample t-\ntest [87].\nWilcoxon sign-rank test. The null hypothesis of this test fortwo (dependent) groups A and B is that the medians are equal:\nH0:median (A) =median (B). To get the test statistic the\ndifferences between all pairs of group A and B are calculated\nand ranked. Additionally, the sign of the difference is used,\nso that all ranks of the positive differences are summed and\nthe same for the negative ones. The minimum of these two is\nthe test statistic, which then can be compared to the Wilcoxon\nsigned rank table to get the p-value. If the p-value is smaller\nthan a chosen alpha-level, the null can be rejected and the two\ncases lead to significantly different medians.\nPaired sample t-test. The null hypothesis of this test for two\n(dependent) groups AandBis that the means are equal: H0:\nmean (A) =mean (B). Again, the differences of the pairs of\nvalues are used to calculate the test statistic. In this case it is\na t-test statistic with n−1degrees of freedom and looks like\nthe following.\nt=¯d√n\ns, with ¯d=1\nnnX\ni=1di, s=vuut1\nn−1nX\ni=1(di−¯d)2\nHere the t-test tables can be used to get the p-value and again\nif it is smaller than alpha, the null can be rejected and we can\nconclude that the means are significantly different.\nFor both tests, we choose 0.1 as alpha value as a common\nchoice.\nIn Table X, we have the p-values for the Wilcoxon sign-\nrank test (Wilcoxon s.-r.) in the left group of 2 columns\nwith numbers and for the paired sample t-test (p.s. t-test)\nin the right-most group of 2 columns. Inside each group,\nthe first column compares the median resp. mean of the\nruntimes obtained by the original evaluation method (Orig)\nof each DBMS with the evaluation method chosen by the\ndecision procedure (Dec). The second column of each group\ncompares the median resp. mean of the runtimes obtained by\nthe rewritten queries (Rewr, i.e., by Yannakakis-style query\nevaluation) with the evaluation method chosen by the decision\nprocedure (Dec). We can see that all p-values are extremely\nsmall. For any reasonable choice of alpha value (note that\nalpha values such as 0.1 or 0.05 are common choices; the p-\nvalues in the table are much smaller) we can reject the null\nhypothesis in all cases. This means that the decision procedure\nindeed yields a significant performance improvement (in terms\nof means and median of runtimes).\n\nTABLE X\nP-VALUES RESULTING FROM THE WILCOXON SIGN -RANK TEST AND\nPAIRED SAMPLE T -TEST ,COMPARING THE ORIGINAL EVALUATION\nMETHOD OF EACH DBMS WITH THE DECISION PROGRAM .\nMedian (Wilcoxon s.-r.)\nSystem Orig/SMASH Rewr/SMASH\nPostgreSQL 1.1·10−281.5·10−41\nSpark 1.8·10−234.8·10−48\nDuckDB 7.2·10−222.1·10−42\nMean (p.s. t-test)\nSystem Orig/SMASH Rewr/SMASH\nPostgreSQL 1.0·10−100.0002\nSpark 6.9·10−121.8·10−7\nDuckDB 2.0·10−82.2·10−5\nTABLE XI\nRUNTIME BREAKDOWN BY BENCHMARK FOR POSTGRE SQL.\nAccur. e2e e2e e2e e2e\nBenchmark orig rewr SMASH opt #\n(sec) (sec) (sec)\nHETIO 94.5% 701 198 192 192 73\nSTATS 94.8% 938 335 308 307 306\nSNAP 100.0% 1978 733 731 731 21\nJOB 97.5% 23 384 23 23 40\nLSQB 100.0% 9 9 9 9 1",
  "textLength": 102499
}