{
  "paperId": "8b0531da015c830f9ae31cb5356516285d73f2fe",
  "title": "(Optimal) Online Bipartite Matching with Degree Information",
  "pdfPath": "8b0531da015c830f9ae31cb5356516285d73f2fe.pdf",
  "text": "(Optimal) Online Bipartite Matching with Degree\nInformation\nAnders Aamand\nMIT\naamand@mit.eduJustin Y. Chen\nMIT\njustc@mit.eduPiotr Indyk\nMIT\nindyk@mit.edu\nAbstract\nWe propose a model for online graph problems where algorithms are given access\nto an oracle that predicts (e.g., based on modeling assumptions or on past data) the\ndegrees of nodes in the graph. Within this model, we study the classic problem\nof online bipartite matching, and a natural greedy matching algorithm called\nMinPredictedDegree, which uses predictions of the degrees of ofﬂine nodes. For\nthe bipartite version of a stochastic graph model due to Chung, Lu, and Vu where the\nexpected values of the ofﬂine degrees are known and used as predictions, we show\nthat MinPredictedDegree stochastically dominates anyother online algorithm, i.e.,\nit is optimal for graphs drawn from this model. Since the “symmetric” version of\nthe model, where all online nodes are identical, is a special case of the well-studied\n“known i.i.d. model”, it follows that the competitive ratio of MinPredictedDegree\non such inputs is at least 0.7299. For the special case of graphs with power\nlaw degree distributions, we show that MinPredictedDegree frequently produces\nmatchings almost as large as the true maximum matching on such graphs. We\ncomplement these results with an extensive empirical evaluation showing that\nMinPredictedDegree compares favorably to state-of-the-art online algorithms for\nonline matching.\n1 Introduction\nOnline algorithms are algorithms that process their inputs “on the ﬂy”, making irrevocable decisions\nbased only on the data seen so far. Since they do not make any assumptions about the future, they are\nversatile and work even for adversarial inputs. Unfortunately, by focusing on the worst case, their\nperformance in “typical” cases can be sub-optimal. As a result there has been a large body of research\nstudying various relaxations of the worst-case model, where some extra information about the inputs,\nor the distribution they are selected from, is available [1].\nMotivated by the developments in machine learning, over the last few years, many papers have\nstudied online algorithms with predictions [47]. Such algorithms are equipped with a predictor that,\nwhen invoked, provides an (imperfect) prediction of some features of the future part of the input,\nwhich is then used by the algorithm to improve its performance. The speciﬁc information provided\nby such predictors is problem-dependent. For graph problems studied in this paper, predictions could\ninclude: the list of edges incident to a given vertex [ 33], the weight of an edge adjacent to a given\nnode in an optimal solution [4], or vertex weights that guide a proportional allocation scheme [36].\nIn this paper we focus on online graph problems, and propose a model where an algorithm is equipped\nwith a “degree predictor”, i.e., an oracle that, given any vertex, predicts the degree of that vertex\nin the full graph (containing yet-unseen edges). This predictor has multiple appealing features.\nFirst it is simple, natural, and easy to interpret. Second, it is useful: vertex degree information is\nemployed in many heuristic and approximation algorithms for graph optimization, for problems such\nas maximum independent set [ 24] or maximum matching [ 53]. Third (as demonstrated in Section 7)\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2110.11439v3  [cs.DS]  14 Nov 2022\n\nsuch predictors can be easily obtained. Finally, degree prediction is closely related to the problem\nof estimating the frequencies of elements in a data set1, and frequency predictors have been already\nshown to improve the performance of algorithms for multiple data analysis problems [ 25,28,19,17].\nThe speciﬁc graph problem studied in this paper is online bipartite matching , where we are given\na bipartite graph G= (U[V;E), and the goal is to ﬁnd a maximum matching in G. In the online\nsetting, the set Uis known beforehand, while the vertices in Varrive online one by one. When a\nnew vertexvarrives, the edges in Gadjacent tovare provided as well. Online maximum bipartite\nmatching is a classic question studied in the online algorithms literature, with many applications [ 41].\nIt is known that a randomized online greedy algorithm, called Ranking, computes a matching of size\nat least 1\u00001=e\u00190:632times the optimum [ 31], and that this bound is tight in the worst-case. A\nlarge body of work studied various relaxations of the problem, obtained by assuming that vertex\narrivals are random [ 21] or that the graph itself is randomly generated from a given “known i.i.d.\nmodel” [ 20]. In this paper we extend the basic online model by assuming access to a predictor that,\ngiven any “ofﬂine” vertex u2U, returns an estimate of its degree. (Note that the degree of any\nvertex inVis known immediately upon its arrival.)\nOur results We study the following simple greedy algorithm for bipartite matching: upon the\narrival of a vertex v, if the set of neighbors N(v)ofvinGcontains any yet-unmatched vertex,\nthe algorithm selects u2N(v)of minimum predicted degree in Gand adds the edge (u;v)to the\nmatching. This algorithm, which we call MinPredictedDegree (MPD) , is essentially identical2to\nthe algorithm proposed in [ 10] which in turn was inspired by the ofﬂine matching algorithm called\nMinGreedy [ 53]. The intuition is that vertices with higher degree will have more chances to be\nmatched in the future.\nOur main contributions are as follows. First, following in a long line of work on the average-\ncase analysis of matching algorithms initiated by [ 30], we analyze MPD under a natural random\nbipartite graph model we refer to as CLV-B , a bipartite version of the Chung-Lu-Vu random graph\nmodel [ 12]. A CLV-B random graph is parameterized by n=jUj,m=jVj, and two weight vectors\np=fpign\ni=12[0;1]nandq=fqigm\ni=12[0;1]m. For anyui2Uandvj2V, the edgefui;vjg\nappears in the graph with probability piqjand these events are mutually independent. This model\ncorresponds to the setting where consumers pick their edges with probabilities proportional to the\nvector pwhich describes the relative distribution over the producers.\nMany natural families of random graphs can be described in the CLV-B model. Of particular interest\nis the case when q= (1;:::; 1), corresponding to the consumers picking their edges i.i.d.; we will\nrefer to this case as the symmetric CLV-B model . The symmetric version can be viewed as a special\ncase of the well-studied known i.i.d. model of [ 20]. If we further let p= (p;:::;p ), then the CLV-B\ngraph is an Erd ˝os-Rényi random bipartite graph with edge probability p.\nTheoretical Results For the CLV-B model and the MPD algorithm which uses the expected degrees\nas predictions, we make the following theoretical contributions:\n•We show that MPD stochastically dominates anyother online algorithm, i.e., it is optimal for\ngraphs drawn from the CLV-B model (Section 5). Speciﬁcally, we show that for any degree\ndistribution, any algorithm Aand any integer t, the probability that Aproduces matching\nof size at least tis upper bounded by the analogous probability for MPD. Since symmetric\nCLV-B is a special case of the known i.i.d. model, it follows that the competitive ratio of\nMPD for this model is at least equal to the best competitive ratio of any algorithm that works\nfor the known i.i.d. model. By the result of [11], this ratio is at least 0.7299.\n•We analyze MPD on symmetric CLV-B model with power law degree distribution (Section 6).\nOur theoretical predictions demonstrate that the competitive ratio achieved by our algorithm\non such graphs is very high. In particular, for several different power law distributions, it\nexceeds 0:99.\n•We also analyze MPD on Erd ˝os-Rényi bipartite random graphs where all edges appear with\nthe same probability (Appendix K). In particular, the competitive ratio of the algorithm on\n1The degree of a node is simply the number of times the node appears in the union of all edges.\n2The main differences are syntactic: the algorithm of [ 10] computes the degrees based on the given “type\ngraph” (see Section 2), while in this paper we allow arbitrary predictors.\n2\n\nsuch graphs is at least 0:831. Since in this case all expected degrees are equal, the prediction\noracle is of no help. Thus, we conjecture that this is the worst distribution for MPD among\nall distributions in the CLV-B model class.\n•Finally, we observe that the competitive ratio of MPD is 1=2forworst case graphs, and\nthat this bound is tight. In addition, we show that the worst-case competitive ratio of\nanyalgorithm with access to the ofﬂine degrees is at most 1\u00001=e, implying that degree\npredictions do not help in the worst-case though they prove to be useful in the random model\nas well as in practice. See Appendix E for details.\nExperiments We complement our theoretical studies with an extensive empirical evaluation of\nMPD for multiple random graph models and real graph benchmarks in Section 7. Our experiments\nshow that, on most benchmarks, MPD has the best performance among about a dozen state-of-the-art\nonline algorithms, even when compared to algorithms that use much more information about the\ninput. These experimental results demonstrate that MPD performs well beyond the average-case\ninstances we study theoretically.\nPrediction Error For our theoretical results on the CLV-B graphs, MPD is given only the expected\n(as opposed to the actual) degrees. Although this models the uncertainty in the input, it is natural to\nask how MPD performs when even the expected degrees are mispredicted. To this end, in Appendix D,\nwe suppose that the ofﬂine nodes are prioritized in an arbitrary order \u00190which may be different from\nthe order\u0019obtained by sorting the nodes according to their expected degrees. Letting \u0001be the\nminimum number of ofﬂine nodes that needs to be deleted such that \u0019and\u00190induce the same order\non the remaining nodes, we prove that using a noisy degree predictor which induces \u00190instead of\n\u0019can shrink the size of the matching produced by MPD by at most \u0001. We note that the number of\nmispredicted nodes is an upper bound on \u0001, but in general \u0001could be much smaller.\nImportantly, we also note that the empirical performance of MPD shows its resilience to prediction\nerror. Our experiments on real graphs use predictors which are noisy and which degrade over time\nbut still ﬁnd large matchings. Furthermore, on synthetic Zipﬁan data, we experiment with artiﬁcally\nadding noise and show a gradual degradation of MPD’s performance as error increases.\n2 Preliminaries\nCLV-B model CLV-B is the bipartite version of the Chung-Lu-Vu model used in prior work [ 12,42].\nGiven vectors p=fpign\ni=1andq=fqjgm\nj=1, the edgefui;vjgappears in the graph independently\nwith probability piqj. From the vectors pandq, we obtain the vector of ofﬂine expected degrees\nd=fdign\ni=1=fpi\u0001kqk1gn\ni=1. For our theoretical results within this model, our algorithm\nMinPredictedDegree uses the degree predictor which returns the expected degree for each ofﬂine\nnode:\u001b(ui) =di(see Appendix D for extension to noisy predictors). The particular case of\nsymmetric CLV-B where q= (1;:::; 1)corresponds to the case where consumers (online) pick\ntheir edges i.i.d. over producers (ofﬂine) and MPD has knowledge of the average preferences over\nproducers.\nKnown i.i.d. model In the known i.i.d. model of [ 20], algorithms are given access to a type graph\nG= (U[V;E)and a distributionP:V![0;1]. The nodes in Vand their incident edges represent\n“types” of online nodes. An input instance ^G= (U[^V;^E)is formed by picking monline nodes\ni.i.d. fromVaccording to the probabilities described by P. Note that the symmetric CLV-B model\ndeﬁned earlier is a special case of this model. In our experiments, the degree predictions are given by\nthe expected degrees of the ofﬂine nodes.\n3 Related Work\nOnline bipartite matching and its generalizations have been investigated extensively. The survey [ 41]\nand the recent paper [ 9] provide excellent overviews of this area. The state of the art competitive\nratios are 1\u00001=e\u00190:632in the worst case [ 31] and\u00190:7299 for the known i.i.d. model [ 11]. See\n[9] for an extensive empirical study of the existing algorithms. Other algorithms examined in the\nexperimental section include [20, 5, 40, 26, 18, 10].\n3\n\nAlgorithm 1 MinPredictedDegree\nInput: Ofﬂine nodes Uand degree predictor \u001b:U!R\u00150\nOutput: MatchingM\nInitializeM ;.\nwhile online node v2Varrives do\nN(v) unmatched neighbors of v\nifjN(v)j>0then\nu\u0003 arg minu2N(v)\u001b(u)(ties broken arbitrarily)\nM M[f(u\u0003;v)g\nend if\nend while\nMore generally, there has been lots of interest in online algorithms with predictions over the last\nfew years, for problems like caching [ 39,50,55,29], ski-rental and its generalizations [ 49,22,2,3],\nscheduling [ 45,35] matching [ 33,4,36] and learning [ 14,7]. Other areas impacted by learning-based\nalgorithms include combinatorial optimization [ 13,6,15], similarity search [ 52,56,27,54,16], data\nstructures [ 32,44] and streaming/sampling algorithms [ 25,28,19]. See [ 47] for an excellent survey\nof this area.\n4 Algorithm\nOnline Bipartite Matching The online bipartite matching problem is deﬁned as follows. Given a\nbipartite graph G= (U[V;E), we callUthe “ofﬂine” side and Vthe “online” side of the bipartition.\nLetn=jUjandm=jVj. The nodes in Uare known beforehand and the nodes in Varrive one at a\ntime, along with their incident edges. An online bipartite matching algorithm maintains a matching\nthroughout the process, with the goal of maximizing the size of the matching. As each node v2V\narrives, the algorithm can pick one of its neighboring edges to add to the matching.\nMinPredictedDegree In addition to knowing the ofﬂine nodes Ubeforehand, MinPredictedDegree\n(MPD) is given a degree predictor \u001b:U!R\u00150. In practice, this predictor could be inferred from\nadditional knowledge about the graph or from past data. When a node v2Varrives, MPD (see\nAlgorithm 1) uses this predictor to greedily select the minimum predicted degree neighbor u\u0003ofv\nthat is not already covered in the matching and then adds the edge fu\u0003;vgto the matching. If no\nsuch valid neighbor exists, MPD does nothing with v. Intuitively, low degree ofﬂine nodes should be\nmatched as early as possible as they only appear a few times while we will have many chances to\nmatch high degree ofﬂine nodes.\nThe MPD algorithm has similar structure to the worst-case optimal Ranking algorithm [ 31] which\nassigns a random cost to each ofﬂine node and at each step greedily matches with the lowest cost\nofﬂine neighbor. Speciﬁcally, if the degree predictor is random, MPD and Ranking are equivalent.\nAs we show in the later sections, if the predictor is “good enough”, MPD often performs much better\nthan Ranking, both in theory and in practice.\n5 Optimality of MPD on CLV-B graphs\nIn this section we show that the size of the matching found by the the MPD algorithm stochastically\ndominates the size of the matching found by any other algorithm. We start by providing some\npreliminaries for the analysis.\nPreliminaries Forp2[0;1]nandq2[0;1]m, letIp;qdenote an instance of a CLV-B graph with\nn=jUjofﬂine nodes, m=jVjonline nodes, and weight vectors pandq, such that the probability\nthat an edge (ui;vj)exists is equal to piqjfor anyi2[n];j2[m]. Assume with no loss of generality\nthatpis ordered,p1\u0014p2\u0014:::\u0014pn. Note that the expected degree of the ofﬂine node uiispikqk1,\ni.e., it is proportional to the weight pi.\nIn the online setting, the nodes of Varrive sequentially in the order v1;:::;vmwith the random\nneighborhood of vj2Vbeing revealed at the arrival of vj. Whenvjarrives, an online bipartite\n4\n\nalgorithmAcan matchvjto any of its unmatched neighbors in Ubut cannot change its decision later.\nFor any online bipartite matching algorithm A, letA(Ip;q)denote the size of the matching attained\nbyAon the instance Ip;q. LetA0be the MinPredictedDegree algorithm which matches a node vj\nwith neighborhood Sto an unmatched node ui2Ssuch thatpiminimal, i.e. to an available node in\nSwith minimal expected degree (ties broken arbitrarily but consistently, e.g. by sorted order of the\nofﬂine node id’s). Let p(A;S)be the resulting set of weights after algorithm A(potentially) chooses\na neighbor in Sto match with.\nConsider two ordered weight vectors p;p0both of length n. We say that p0dominatesp, equivalently\np\u0016p0, ifpi\u0014p0\nifor alli2[n]. We are now ready to state our main result on the optimality of MPD.\nTheorem 5.1. Letp2[0;1]nandq2[0;1]m. LetAbe any online algorithm and let t\u00150. Then,\nP(A(Ip;q)\u0015t)\u0014P(A0(Ip;q)\u0015t):\nTo prove the theorem, we will need two technical Lemmas. Informally, Lemma 5.2 states that for any\nS6=;, it is an advantage for A0if the neighborhood of the ﬁrst arriving node is Srather than the\nempty set. Lemma 5.3 (the proof of which is the main technical challenge) states that if p\u0016p0, then\nA0(Ip0;q)stochastically dominates A0(Ip;q). Intuitively, Theorem 5.1 then follows from Lemma 5.3\nby inducting on the number of online nodes m. For any algorithm Aand non-empty subset S\u0012[n],\nifAmatchesv1to a node in the neighborhood S, thenp(A;S)\u0016p(A0;S), and we can apply\nLemma 5.3 together with the induction hypothesis with m\u00001online nodes. We need Lemma 5.2 to\nhandle the issue that Amay not to match v1even in the case that Sis non-empty. The proofs of the\ntwo lemmas and of Theorem 5.1 are postponed to Appendices A, B and C.\nLemma 5.2. Letp2[0;1]nandq2[0;1]mbe weight vectors. Let p\u00032[0;1]n\u00001be obtained from\npby removing its i’th entry for some i2[n]. For anyt\u00150,\nP(A0(Ip;q)\u0015t)\u0014P(A0(Ip\u0003;q)\u0015t\u00001):\nLemma 5.3. Letp;p02[0;1]n, be ordered weight vectors and q2[0;1]m. Suppose that p\u0016p0.\nFor anyt\u00150,\nP(A0(Ip;q)\u0015t)\u0014P(A0(Ip0;q)\u0015t):\nWhile optimally only holds when the predicted degrees are the expected degrees (or at least induce\nthe same ordering over the ofﬂine nodes), the performance of MPD cannot be much worse if the\npredictions are slightly off. Formally, for an arbitrary degree predictor \u001b, letp[\u001b]be the array of CLV-\nB ofﬂine weights ordered by \u001band let LIS(p[\u001b])be the size of the longest increasing subsequence\nin this array. We show (via a more general result) in Appendix D that MPD will match at most\nn\u0000LIS(p[\u001b])fewer nodes than when given the expected degrees as predictions.\n6 Competitive ratio of MPD on symmetric CLV-B random graphs\nThough we know that MPD is optimal within the CLV-B model, this result does not give explicit\ncompetitive ratios for MPD. In this section we analyze MPD under the symmetric CLV-B model,\nand derive a set of equations that give a lower bound on MPD’s competitive ratio. To recap, the\nsymmetric model is parameterized by n=jUj,m=jVj, and a vector d=fdign\ni=1corresponding\nto the expected degrees of the ofﬂine nodes. Formally, for any ui2Uandvj2V, the edgefui;vjg\nappears in the graph with probability di=m.\nAs in the previous section, we analyze MPD when the degree predictions are given by the expected\ndegrees d. Our main results within this model are a set of equations that describe the size of the\nmatching produced by MPD as well as the size of the maximum matching.\n•Given a set of expected degrees d, Equation 4 models the behavior of MPD on a symmetric\nCLV-B( d) graph. We extend these results to the asymptotic case in Appendix J, giving the expected\nmatching size as n;m!1 for a given distribution of expected degrees.\n•Given a set of expected degrees d,in Appendix I, we give an upper bound on the expected size of the\nmaximum matching on a symmetric CLV-B( d) graph, and in Appendix J, we give the asymptotic\nequivalent. Empirically, we ﬁnd this upper bound to be close to the maximum matching size when\ndfollows a power law distribution.\n5\n\n•Using these equations, we show that in expectation MPD returns matchings almost as large as the\nmaximum when the expected degrees of the ofﬂine nodes follow a power law distribution (see\nTable 1 and Figure 6). For both MPD and the maximum matching, we show that the matching\nsizes are concentrated about their expectations (Appendix L and M), implying that on these graphs,\nMPD achieves a large competitive ratio.\n6.1 Competitive ratios on power law graphs\nCUTOFF\u0015 \u000b = 0:5\u000b= 1\u000b= 1:5\u000b= 2\n10 0.967 0.948 0.934 0.928\n100 0.998 0.986 0.958 0.937\n1000 1.000 0.995 0.966 0.940\n10000 1.000 0.997 0.969 0.940\n100000 1.000 0.998 0.970 0.940\nTable 1: Lower bound on the competitive ratio of MPD on symmetric CLV-B graphs with ofﬂine\nexpected degrees following a power law with exponential cutoff distribution as n;m!1 . The\nfraction of ofﬂine nodes with expected degree dis proportional to d\u0000\u000be\u0000d=\u0015ford=f1;2;:::g.\nIn Table 1, we show the competitive ratio of MPD on symmetric CLV-B graphs with expected ofﬂine\ndegrees following a power law with exponential cutoff distribution [ 9,46] and withn;m!1 . For\nd=f1;2;:::g, the fraction of ofﬂine nodes with expected degree dis proportional to d\u0000\u000be\u0000d=\u0015for\nexponent\u000band cutoff\u0015. Note that in the asymptotic case, as the sizes of MPD’s matching and\nthe maximum matching are concentrated about their expectations (Theorems L.1, M.1), the ratio\nof expectations is equivalent to the competitive ratio (expectation of ratio). When the exponent is\nsmall or the cutoff is large, MPD achieves a better competitive ratio, with the ratio exceeding 0:99\nwhen both occur. When \u000b= 2, while MPD still achieves a competitive ratio of up to 0:94, the\ncompetitive ratio is not as affected by a larger cutoff as with smaller exponents (the power law factor\nis already signiﬁcantly limiting the fraction of ofﬂine nodes with large expected degree). The analysis\nwe develop is general and can be used to evaluate MPD on symmetric CLV-B graphs with different\nparameters than those we have considered.\n6.2 Differential equation analysis of MPD\nLetYt\ndbe the number of ofﬂine nodes with expected degree dwho are unmatched by MPD after\nseeing thetth online node. Within this random graph model, fYt\ndgm\nt=0form a Markov chain with the\nfollowing expected evolution:\nE[Yt+1\nd\u0000Yt\nd] =\u0000\u0010\n1\u0000(1\u0000d=m)Yt\nd\u0011Y\nd0<d(1\u0000d0=m)Yt\nd0:(1)\nThe ﬁrst term corresponds to the probability that at least one unmatched ofﬂine node with expected\ndegreedis incident on the (t+ 1) st online node while the second term corresponds to the probability\nthat this online node has no neighboring unmatched ofﬂine nodes with smaller expected degree\n(which would be prioritized).\nLetkd=\u0000log(1\u0000d=m). To simplify the analysis of MPD, it will be helpful to consider the random\nvariablesZt\nd=\u0000kd\u0003Yt\ndwhere\nE[Zt+1\nd\u0000Zt\nd] =kd\u0010\n1\u0000eZt\nd\u0011Y\nd0<deZt\nd0:(2)\nFollowing the work of Kurtz and many subsequent researchers [ 34,57,43,38,48], we show that the\nbehavior of MPD as described by these Markov chains is well approximated by the trajectory of the\nfollowing system of differential equations for all unique expected degrees dind:\ndzd(t)\ndt=kd\u0010\n1\u0000ezd(t)\u0011Y\nd0<dezd0(t): (3)\n6\n\nThese functions zd(t)represent continuous-time approximations of the Markov chains with their\nderivatives corresponding to expected change from Equation 2. In Appendix G, we give the solution\nto these differential equations. Relying on past work [ 38], we give the following theorem (see\nAppendix H for proof).\nTheorem 6.1. LetGbe a symmetric CLV-B random graph with unique expected ofﬂine degrees\nf\u000eig`\ni=1. Letfd=\u0015d\u0001nbe the number of ofﬂine nodes with expected degree d. Then, the expected\n(over the randomness in G) size of the matching formed by MPD approaches\n`X\ni=1f\u000ei+z\u000ei(m)=k (4)\nasn=mapproach inﬁnity, where z\u000ei(t)fori2f1;:::;`gform the solution to the system of\ndifferential equations in Equation 3.\nThe solution to the system of differential equations gives us a closed form continuous-time approxi-\nmation for expected performance of MPD in terms of d. In particular, in the asymptotic case, the\nequations give the exact expected performance and in the non-asymptotic case give an approximation\non the number of unmatched ofﬂine nodes (and thus the matching size).\n7 Experiments\nIn this section, we evaluate the empirical performance of MPD on real and synthetic data. For each\ndataset, we report the empirical competitive ratio of MPD and a variety of baselines. In each case, the\nempirical competitive ratio is the average, over 100 trials, of the ratios of the sizes of the matchings\noutputted by a given algorithm and the sizes of the maximum matching. In addition to the average\nratio, we report one standard deviation of the ratio across the trials.\nDatasets We evaluate MPD on the following datasets.\n•Oregon: 9graphs3sampled over 3months representing a communication network of internet\nrouters from the Stanford SNAP Repository [ 37]. Each graph has\u001810knodes on each side of the\nbipartition and\u001840kedges. For MPD, the ofﬂine degree predictor \u001b:U!Ris based on the\nﬁrst graph: if an ofﬂine node u(i.e. a speciﬁc router) appeared in the ﬁrst graph, \u001b(u)is the degree\nofuin that graph. If an ofﬂine node udid not appear in the ﬁrst graph, \u001b(u) = 1 . For each trial,\nthe order of arrival of the online nodes is randomized.\n•CAIDA: 122graphs3sampled approximately weekly over 4years representing a communication\nnetwork of internet routers from the Stanford SNAP Repository [ 37]. Each graph has \u001820k\nnodes on each side of the bipartition and \u0018100kedges. The degree predictor is the same as for\nthe Oregon dataset (for each year, the ﬁrst graph of the year is used to form the predictor). As\nseen in Figure 8 (see Appendix N), the degree distribution of the graphs for both the Oregon and\nCaida datasets are long-tailed and the error of the ﬁrst graph predictor increases over time as the\nunderlying graph evolves. For each trial, the order of arrival of the online nodes is randomized.\n•Symmetric CLV-B random graph: We consider symmetric CLV-B model where the expected\nofﬂine degrees are distributed according to Zipf’s Law, a popular power law distribution where\ndi=C\u0001i\u0000\u000b[46]. In our experiments, we set size n=m= 1000 , setC=m=2, and vary the\nexponent\u000b.\n•Known i.i.d.: Finally, we compare MPD to algorithms for the known i.i.d. model, copying the\nmethodology of Borodin et al. [ 9] for synthetic power law graphs (Molloy Reed and Preferential\nAttachment) and real world graphs. In the Molloy Reed experiments, the type graph is sample from\na family of random graphs with degrees distributed according to a power law with exponential\ncutoff. In the Preferential Attachment experiments, the type graph is formed by the preferential\nattachment model in which edges are added sequentially with edges between high degree nodes\nbeing more likely. The Real World graphs are comprised of a variety of graphs from the Network\nRepository [51]. See Appendix N for more results on Real World graphs.\n3The graphs in the Oregon and CAIDA datasets are made bipartite following the bipartite double cover or\nduplicating method used in prior work [ 9]. Given a graph G= (V;E ), the bipartite double cover of Gis the\ngraphG0= (U0[V0;E0)whereU0andV0are copies of Vand there is an edge fu0\ni;v0\njg 2E0if and only if\nfvi;vjg 2E.\n7\n\nBaselines We compare our algorithm to a variety of baseline algorithms.\n•Ranking In all experiments, we compare to the classic, worst-case optimal Ranking algorithm [ 31].\n•MinDegree The MinDegree algorithm is a version of MPD with a perfect oracle, i.e. \u001b(u)returns\nthe true degree of u. In comparison with MPD, MinDegree shows the effect of prediction error on\nthe performance of MPD.\n•Known i.i.d. baselines For the experiments in the known i.i.d. case, we also compare to the\nbaselines in the extensive empirical study of [ 9]–see their paper for detailed descriptions of all\nalgorithms. The code is distributed under the GPL license. Notably, the algorithms Category-\nAdvice and 3-Pass are notstrictly online algorithms: they take multiple passes over the data, using\nsome limited information from previous passes to make better decisions in the next pass. It should\nalso be noted that BKPMinDegree is distinct from either the MPD or MinDegree algorithms we\nhave described–it does not use the type graph but rather maintains and updates an estimate of the\ndegree of the ofﬂine nodes throughout the runtime of the algorithm.\nMost known i.i.d. baselines are notgreedy–they do not always match an online node even if it\nhas unmatched neighbors. [ 9] evaluate greedy augmentations of these algorithms (denoted by\nAlgorithm(g)) which match to an arbitrary unmatched neighbor in these cases and generally show\nthem to outperform their non-greedy counterparts. We additionally evaluate MPD augmented\nversions of these algorithms (denoted by Algorithm(MPD)) which applies the MPD rule in these\ncases using the expected degrees as predictions.\n1 2 3 4 5 6 7 8 9\nGraph #0.950.960.970.980.991.00Competitive RatioMinDegree MinPredictedDegree Ranking\nFigure 1: Comparison of empirical competitive ratios on the Oregon dataset. The ﬁrst graph is used\nto form predictions.\n1 3 5 7 9 11\nGraph #0.940.960.981.00Competitive Ratio2004\n1 3 5 7 9 11\nGraph #2005\n0 10 20 30 40 50\nGraph #2006\n0 10 20 30 40\nGraph #2007MinDegree MinPredictedDegree Ranking\nFigure 2: Comparison of empirical competitive ratios on the CAIDA dataset. For each subﬁgure, the\nﬁrst graph of the year is used to form predictions for the rest of the year.\nResults Across the various datasets, MPD performs well compared to the baseline algorithms. For\nthe Oregon, CAIDA, and symmetric CLV-B random graph datasets, MPD signiﬁcantly outperforms\nRanking, and for Oregon and CAIDA, the performance of the algorithm mildly declines as the degree\npredictions degrade. For the known i.i.d. datasets, MPD often outperforms all online baselines,\ndespite making only limited use of the known i.i.d. model. Additionally, augmenting the known i.i.d.\nalgorithms with the (MPD) rule often improves their performance over both the base and the greedy\n(g) versions of the algorithms.\n•Oregon and CAIDA (Figures 1, 2): On the Oregon dataset, MPD achieves a competitive ratio\nof\u00180:99across the graphs compared with competitive ratios ranging from 0:95to0:97for\n8\n\n0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0\nExponent0.850.900.951.00Competitive RatioMinDegree MinPredictedDegree Ranking(a) Comparison across Zipf’s Law exponents.\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nFraction subsampled0.900.920.940.960.98Competitive Ratio\nMinPredictedDegree (noisy) Ranking (b) Analysis of predictor noise (exponent\n\u000b= 1).\nFigure 3: Comparison of empirical competitive ratios on symmetric CLV-B random graphs with\nofﬂine expected degrees following Zipf’s Law with exponent \u000b. In (a), we vary \u000band MPD uses\nthe expected degree as its predictor. In (b), the degree predictor is the ofﬂine degree in a random\nsubgraph using a (varying) fraction of the online nodes.\n0.7 0.8 0.9 1.0\nCompetitive RatioManshadiEtAlFeldmanEtAlBahmaniKapralovBrubachEtAlJailletLuBKPKarpSipserSimpleGreedyRankingBKPMinDegreeManshadiEtAl(g)ManshadiEtAl(MPD)BrubachEtAl(g)BrubachEtAl(MPD)JailletLu(g)JailletLu(MPD)FeldmanEtAl(g)BahmaniKapralov(g)BahmaniKapralov(MPD)FeldmanEtAl(MPD)MinPredictedDegreeCategory-Advice3-PassMinDegreeOPTt=0.5, k=11\n0.7 0.8 0.9 1.0\nCompetitive RatioFeldmanEtAlBahmaniKapralovManshadiEtAlBrubachEtAlJailletLuBKPKarpSipserRankingSimpleGreedyBKPMinDegreeBrubachEtAl(g)JailletLu(g)ManshadiEtAl(g)BrubachEtAl(MPD)BahmaniKapralov(g)FeldmanEtAl(g)ManshadiEtAl(MPD)JailletLu(MPD)BahmaniKapralov(MPD)FeldmanEtAl(MPD)MinPredictedDegreeMinDegreeCategory-Advice3-PassOPTt=0.5, k=41\n0.7 0.8 0.9 1.0\nCompetitive RatioFeldmanEtAlBahmaniKapralovManshadiEtAlBrubachEtAlJailletLuSimpleGreedyRankingBKPKarpSipserBKPMinDegreeBrubachEtAl(g)ManshadiEtAl(g)JailletLu(g)ManshadiEtAl(MPD)BrubachEtAl(MPD)JailletLu(MPD)BahmaniKapralov(MPD)FeldmanEtAl(MPD)FeldmanEtAl(g)MinPredictedDegreeBahmaniKapralov(g)Category-Advice3-PassMinDegreeOPTt=1, k=96\n0.7 0.8 0.9 1.0\nCompetitive RatioManshadiEtAlBrubachEtAlFeldmanEtAlJailletLuBahmaniKapralovRankingSimpleGreedyBKPKarpSipserBKPMinDegreeBrubachEtAl(g)BrubachEtAl(MPD)ManshadiEtAl(MPD)ManshadiEtAl(g)JailletLu(g)JailletLu(MPD)FeldmanEtAl(g)BahmaniKapralov(g)MinPredictedDegreeBahmaniKapralov(MPD)FeldmanEtAl(MPD)Category-Advice3-PassMinDegreeOPTt=2, k=96\nFigure 4: Comparison of empirical competitive ratios for Known i.i.d. Molloy-Reed graphs. Algo-\nrithms depicted in gray are notonline algorithms (they use extra information or multiple passes).\nAlgorithms in green are augmented with MPD.\nRanking. Compared with MinDegree, which uses knowledge of the true ofﬂine degrees, MPD’s\nperformance slowly degrades over time as the graphs become less similar to Graph #1(see Figure 8\nin Appendix N for quantitative details).\nSimilarly, on the CAIDA dataset, MPD does signiﬁcantly better than Ranking, achieving com-\npetitive ratios almost always greater than 0:98compared to ratios around 0:95, respectively. As\nthe performance of the degree predictor degrades over time, the performance of MPD gradually\ndeclines (though it still signiﬁcantly outperforms Ranking for both datasets).\n•Symmetric CLV-B random graph (Figure 3): For symmetric CLV-B random graphs with ofﬂine\nexpected degrees following Zipf’s Law, MPD outperforms Ranking across a spectrum of exponents\n\u000branging from 0:2to2. For exponents less than 0:5and greater than 1:5, MPD achieves a\ncompetitive ratio close to 1(greater than 0:995). All of the online algorithms have worse competitive\nratios when the exponent is closer to one with MPD achieving a ratio of \u00180:93and Ranking\nachieving a ratio of \u00180:86when\u000b= 0:8. Though MPD does worst at \u000b= 0:8, it also achieves\nits greatest improvement over Ranking at this setting.\nIn Figure 3b, we analyze the performance of MPD with a noisy degree predictor on Zipf’s Law\nsymmetric CLV-B random graphs with exponent 1. To introduce noise, the degree predictor \u001b(u)\nis given by the number of neighbors uhas with a random subset of the online nodes V. As\nwe decrease the fraction of Vwe subsample, thus increasing the variance of the predictor, the\nperformance of MPD steadily declines. Even when the degree predictor only uses 10% or even 1%\n(the leftmost point on the graph) of the online nodes, it still outperforms Ranking.\n•Known i.i.d. (Figures 4, 5): Across all of the experiments in the known i.i.d. model, MPD is\namong the top online algorithms, and is often the best performing online algorithm (note the\nalgorithms in gray are notstrictly online algorithms). Most of the algorithms (e.g. BahamiKapralov\nand ManshadiEtAl) rely heavily on the type graph, including precomputing an optimal matching\non the type graph. By contrast, MPD only uses ﬁrst-order information: it only looks at degrees\n9\n\n0.7 0.8 0.9 1.0\nCompetitive RatioFeldmanEtAlBrubachEtAlManshadiEtAlJailletLuBahmaniKapralovRankingSimpleGreedyBKPKarpSipserBKPMinDegreeBrubachEtAl(g)JailletLu(g)BrubachEtAl(MPD)JailletLu(MPD)ManshadiEtAl(g)ManshadiEtAl(MPD)FeldmanEtAl(g)BahmaniKapralov(g)BahmaniKapralov(MPD)FeldmanEtAl(MPD)MinPredictedDegreeCategory-Advice3-PassMinDegreeOPTPreferential (c=2.1)\n0.7 0.8 0.9 1.0\nCompetitive RatioFeldmanEtAlBahmaniKapralovBrubachEtAlJailletLuManshadiEtAlBKPKarpSipserRankingSimpleGreedyBrubachEtAl(g)ManshadiEtAl(g)JailletLu(g)FeldmanEtAl(g)BahmaniKapralov(g)BrubachEtAl(MPD)JailletLu(MPD)BKPMinDegreeManshadiEtAl(MPD)Category-Advice3-PassBahmaniKapralov(MPD)FeldmanEtAl(MPD)MinPredictedDegreeMinDegreeOPTPreferential (c=8.1)\n0.7 0.8 0.9 1.0\nCompetitive RatioFeldmanEtAlBahmaniKapralovManshadiEtAlJailletLuBrubachEtAlRankingSimpleGreedyBKPKarpSipserBKPMinDegreeManshadiEtAl(g)FeldmanEtAl(g)JailletLu(g)BrubachEtAl(g)BahmaniKapralov(g)Category-Advice3-PassManshadiEtAl(MPD)JailletLu(MPD)BahmaniKapralov(MPD)FeldmanEtAl(MPD)BrubachEtAl(MPD)MinPredictedDegreeMinDegreeOPTCaltech36\n0.7 0.8 0.9 1.0\nCompetitive RatioFeldmanEtAlBrubachEtAlBahmaniKapralovJailletLuManshadiEtAlSimpleGreedyRankingBKPKarpSipserBKPMinDegreeManshadiEtAl(g)FeldmanEtAl(g)BrubachEtAl(g)JailletLu(g)BahmaniKapralov(g)Category-Advice3-PassManshadiEtAl(MPD)BrubachEtAl(MPD)JailletLu(MPD)BahmaniKapralov(MPD)FeldmanEtAl(MPD)MinPredictedDegreeMinDegreeOPTReed98Figure 5: Comparison of empirical competitive ratios on Known i.i.d. Preferential Attachment graphs\nand Real World graphs. Algorithms depicted in gray are notonline algorithms (they use extra\ninformation or multiple passes). Algorithms in green are augmented with MPD. See Appendix N for\nmore Real World results.\nand does not rely on any information about speciﬁc edges. Even so, in most cases, it outperforms\nall of the other online algorithms. Additionally, the (MPD) augmented versions of the known\ni.i.d. algorithms always beat the base algorithms and often beat the greedy (g) versions, indicating\nthe potential of predicted degrees to be integrated with other algorithms. Note that while the\nstandard deviations are quite wide (the known i.i.d. model is inherently stochastic), as the results\nare summarized over 100 trials, relatively small differences in the average performance of these\nalgorithms are statistically signiﬁcant as the standard error is small.\nAcknowledgements This research was supported in part by the NSF TRIPODS program (awards\nCCF-1740751 and DMS-2022448), NSF award CCF-2006798, Simons Investigator Award, NSF\nGraduate Research Fellowship under Grant No. 1745302, MathWorks Engineering Fellowship, and\nDFF-International Postdoc Grant 0164-00022B from the Independent Research Fund Denmark.\nReferences\n[1]Special Semester on Algorithms and Uncertainty , 2016. https://simons.berkeley.edu/\nprograms/uncertainty2016 .\n[2]Keerti Anand, Rong Ge, and Debmalya Panigrahi. Customizing ml predictions for online\nalgorithms. In International Conference on Machine Learning , pages 303–313. PMLR, 2020.\n[3]Spyros Angelopoulos, Christoph Dürr, Shendan Jin, Shahin Kamali, and Marc Renault. Online\ncomputation with untrusted advice. In 11th Innovations in Theoretical Computer Science\nConference (ITCS 2020) , 2020.\n[4]Antonios Antoniadis, Themis Gouleakis, Pieter Kleer, and Pavel Kolev. Secretary and online\nmatching problems with machine learned advice. In 34th Conference on Neural Information\nProcessing Systems , 2020.\n[5]Bahman Bahmani and Michael Kapralov. Improved bounds for online stochastic matching. In\nEuropean Symposium on Algorithms , pages 170–181. Springer, 2010.\n[6]Maria-Florina Balcan, Travis Dick, Tuomas Sandholm, and Ellen Vitercik. Learning to branch.\nInInternational Conference on Machine Learning , 2018.\n[7]Aditya Bhaskara, Ashok Cutkosky, Ravi Kumar, and Manish Purohit. Online learning with\nimperfect hints. In International Conference on Machine Learning , pages 822–831. PMLR,\n2020.\n[8]Benjamin Birnbaum and Claire Mathieu. On-line bipartite matching made simple. SIGACT\nNews , 39(1):80–87, March 2008.\n[9]Allan Borodin, Christodoulos Karavasilis, and Denis Pankratov. An experimental study of\nalgorithms for online bipartite matching. Journal of Experimental Algorithmics (JEA) , 25:1–37,\n2020.\n10\n\n[10] Allan Borodin, Denis Pankratov, and Amirali Salehi-Abari. On conceptually simple algorithms\nfor variants of online bipartite matching. Theory of Computing Systems , 63(8):1781–1818,\n2019.\n[11] Brian Brubach, Karthik Abinav Sankararaman, Aravind Srinivasan, and Pan Xu. New al-\ngorithms, better bounds, and a novel model for online stochastic matching. In 24th Annual\nEuropean Symposium on Algorithms (ESA 2016) . Schloss Dagstuhl-Leibniz-Zentrum fuer\nInformatik, 2016.\n[12] Fan Chung, Linyuan Lu, and Van Vu. The spectra of random graphs with given expected\ndegrees. Internet Mathematics , 1(3):257–275, 2004.\n[13] Hanjun Dai, Elias Khalil, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial\noptimization algorithms over graphs. In Advances in Neural Information Processing Systems ,\npages 6351–6361, 2017.\n[14] Ofer Dekel, Arthur Flajolet, Nika Haghtalab, and Patrick Jaillet. Online learning with a hint. In\nNIPS , pages 5299–5308, 2017.\n[15] Michael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii.\nFaster matchings via learned duals. Advances in Neural Information Processing Systems , 34,\n2021.\n[16] Yihe Dong, Piotr Indyk, Ilya P Razenshteyn, and Tal Wagner. Learning space partitions for\nnearest neighbor search. ICLR , 2020.\n[17] Elbert Du, Franklyn Wang, and Michael Mitzenmacher. Putting the “learning” into learning-\naugmented algorithms for frequency estimation. In International Conference on Machine\nLearning , 2021.\n[18] Christoph Dürr, Christian Konrad, and Marc Renault. On the power of advice and randomization\nfor online bipartite matching. In The 24th European Symposium on Algorithms (ESA) , 2016.\n[19] Talya Eden, Piotr Indyk, Shyam Narayanan, Ronitt Rubinfeld, Sandeep Silwal, and Tal Wagner.\nLearning-based support estimation in sublinear time. In International Conference on Learning\nRepresentations , 2021.\n[20] Jon Feldman, Aranyak Mehta, Vahab Mirrokni, and Shan Muthukrishnan. Online stochastic\nmatching: Beating 1-1/e. In 2009 50th Annual IEEE Symposium on Foundations of Computer\nScience , pages 117–126. IEEE, 2009.\n[21] Gagan Goel and Aranyak Mehta. Online budgeted matching in random input models with\napplications to adwords. In SODA , volume 8, pages 982–991, 2008.\n[22] Sreenivas Gollapudi and Debmalya Panigrahi. Online algorithms for rent-or-buy with expert\nadvice. In International Conference on Machine Learning , pages 2319–2327. PMLR, 2019.\n[23] Philip Hall. On representatives of subsets. Journal of The London Mathematical Society-second\nSeries , pages 26–30, 1935.\n[24] Magnús M Halldórsson and Jaikumar Radhakrishnan. Greed is good: Approximating indepen-\ndent sets in sparse and bounded-degree graphs. Algorithmica , 18(1):145–163, 1997.\n[25] Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency estimation\nalgorithms. In International Conference on Learning Representations , 2019.\n[26] Patrick Jaillet and Xin Lu. Online stochastic matching: New algorithms with better bounds.\nMathematics of Operations Research , 39(3):624–646, 2014.\n[27] Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor\nsearch. IEEE transactions on pattern analysis and machine intelligence , 33(1):117–128, 2011.\n[28] Tanqiu Jiang, Yi Li, Honghao Lin, Yisong Ruan, and David P Woodruff. Learning-augmented\ndata stream algorithms. ICLR , 2020.\n[29] Zhihao Jiang, Debmalya Panigrahi, and Kevin Sun. Online algorithms for weighted paging\nwith predictions. In 47th International Colloquium on Automata, Languages, and Programming\n(ICALP 2020) . Schloss Dagstuhl-Leibniz-Zentrum für Informatik, 2020.\n[30] R. Karp and M. Sipser. Maximum matchings in sparse random graphs. In FOCS 1981 , 1981.\n11\n\n[31] Richard M Karp, Umesh V Vazirani, and Vijay V Vazirani. An optimal algorithm for on-line\nbipartite matching. In Proceedings of the twenty-second annual ACM symposium on Theory of\ncomputing , pages 352–358, 1990.\n[32] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned\nindex structures. In Proceedings of the 2018 International Conference on Management of Data\n(SIGMOD) , pages 489–504. ACM, 2018.\n[33] Ravi Kumar, Manish Purohit, Aaron Schild, Zoya Svitkina, and Erik Vee. Semi-online bipartite\nmatching. 10th Innovations in Theoretical Computer Science , 2019.\n[34] Thomas Kurtz. 8. Approximations of Density Dependent Jump Markov Processes , pages 51–61.\n1981.\n[35] Silvio Lattanzi, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Online schedul-\ning via learned weights. In Proceedings of the Fourteenth Annual ACM-SIAM Symposium on\nDiscrete Algorithms , pages 1859–1877. SIAM, 2020.\n[36] Thomas Lavastida, Benjamin Moseley, R Ravi, and Chenyang Xu. Learnable and instance-robust\npredictions for online matching, ﬂows and load balancing. arXiv preprint arXiv:2011.11743 ,\n2020.\n[37] Jure Leskovec and Andrej Krevl. SNAP Datasets: Stanford large network dataset collection.\nhttp://snap.stanford.edu/data , June 2014.\n[38] Michael Luby, Michael Mitzenmacher, M Amin Shokrollahi, and Daniel Spielman. Efﬁcient\nerasure correcting codes. IEEE Transactions on Information Theory , 47(2):569–584, 2001.\n[39] Thodoris Lykouris and Sergei Vassilvtiskii. Competitive caching with machine learned advice.\nInInternational Conference on Machine Learning , pages 3296–3305. PMLR, 2018.\n[40] Vahideh H Manshadi, Shayan Oveis Gharan, and Amin Saberi. Online stochastic matching:\nOnline actions based on ofﬂine statistics. Mathematics of Operations Research , 37(4):559–573,\n2012.\n[41] Aranyak Mehta. Online matching and ad allocation. Foundations and Trends ®in Theoretical\nComputer Science , 8(4):265–368, 2013.\n[42] Raghu Meka, Prateek Jain, and Inderjit Dhillon. Matrix completion from power-law distributed\nsamples. In Advances in Neural Information Processing Systems , volume 22, pages 1258–1266.\nCurran Associates, Inc., 2009.\n[43] Michael Mitzenmacher. Studying balanced allocations with differential equations. Combina-\ntorics, Probability, and Computing , 8:473–482, 1997.\n[44] Michael Mitzenmacher. A model for learned bloom ﬁlters and optimizing by sandwiching. In\nAdvances in Neural Information Processing Systems , 2018.\n[45] Michael Mitzenmacher. Scheduling with predictions and the price of misprediction. In ITCS ,\n2020.\n[46] Michael Mitzenmacher and Eli Upfal. Probability and Computing . 01 2005.\n[47] Michael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. arXiv preprint\narXiv:2006.09123 , 2020.\n[48] Nathan Noiry, Vianney Perchet, and Flore Sentenac. Online matching in sparse random graphs:\nNon-asymptotic performances of greedy algorithm. In 35th Conference on Neural Information\nProcessing Systems , 2021.\n[49] Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ml predictions.\nInAdvances in Neural Information Processing Systems , pages 9661–9670, 2018.\n[50] Dhruv Rohatgi. Near-optimal bounds for online caching with machine learned advice. In\nProceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms , pages\n1834–1845. SIAM, 2020.\n[51] Ryan A. Rossi and Nesreen K. Ahmed. The network data repository with interactive graph\nanalytics and visualization. In AAAI , 2015.\n[52] Ruslan Salakhutdinov and Geoffrey Hinton. Semantic hashing. International Journal of\nApproximate Reasoning , 50(7):969–978, 2009.\n12\n\n[53] Gottfried Tinhofer. A probabilistic analysis of some greedy cardinality matching algorithms.\nAnnals of Operations Research , 1(3):239–254, 1984.\n[54] Jun Wang, Wei Liu, Sanjiv Kumar, and Shih-Fu Chang. Learning to hash for indexing big data -\na survey. Proceedings of the IEEE , 104(1):34–57, 2016.\n[55] Alexander Wei. Better and simpler learning-augmented online caching. In Approximation, Ran-\ndomization, and Combinatorial Optimization. Algorithms and Techniques (APPROX/RANDOM\n2020) . Schloss Dagstuhl-Leibniz-Zentrum für Informatik, 2020.\n[56] Yair Weiss, Antonio Torralba, and Rob Fergus. Spectral hashing. In Advances in neural\ninformation processing systems , pages 1753–1760, 2009.\n[57] Nicholas C. Wormald. Differential equations for random processes and random graphs. Annals\nof Applied Probability , 5(4):1217–1235, 1995.\nChecklist\n1. For all authors...\n(a)Do the main claims made in the abstract and introduction accurately reﬂect the paper’s\ncontributions and scope? [Yes]\n(b) Did you describe the limitations of your work? [Yes] See, for example, Appendix E.\n(c)Did you discuss any potential negative societal impacts of your work? [No] We\ndid not include a discussion of this in the text of the paper as our paper is focused\non fundamental research. That being said, as online bipartite matching has a wide\nvariety of applications, developing and better understanding algorithms for this problem\nmay have impact on these diverse applications. In particular, metrics other than the\ncompetitive ratio of algorithms (such as certain notions of fairness) are important to\nconsider in applications involving individuals and are being studied in other works.\n(d)Have you read the ethics review guidelines and ensured that your paper conforms to\nthem? [Yes]\n2. If you are including theoretical results...\n(a) Did you state the full set of assumptions of all theoretical results? [Yes]\n(b) Did you include complete proofs of all theoretical results? [Yes]\n3. If you ran experiments...\n(a)Did you include the code, data, and instructions needed to reproduce the main experi-\nmental results (either in the supplemental material or as a URL)? [Yes] See supplemen-\ntary material.\n(b)Did you specify all the training details (e.g., data splits, hyperparameters, how they\nwere chosen)? [Yes]\n(c)Did you report error bars (e.g., with respect to the random seed after running experi-\nments multiple times)? [Yes]\n(d)Did you include the total amount of compute and the type of resources used (e.g., type\nof GPUs, internal cluster, or cloud provider)? [No] The experiments are relatively\nlightweight and the focus of this paper is not on computational resources (rather, we\nfocus on the quality of the algorithms’ output). That being said, MPD is a very simple\nand computationally efﬁcient algorithm (assuming accessing the degree predictor is\nnot too expensive). The experiments were all run on a 2018 MacBook Pro.\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n(a) If your work uses existing assets, did you cite the creators? [Yes]\n(b) Did you mention the license of the assets? [Yes]\n(c)Did you include any new assets either in the supplemental material or as a URL? [Yes]\nSee code in supplement.\n(d)Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? [N/A] All data was gathered from open sources intended for research\nuse.\n13\n\n(e)Did you discuss whether the data you are using/curating contains personally identiﬁable\ninformation or offensive content? [N/A] None of our data falls within this category.\n5. If you used crowdsourcing or conducted research with human subjects...\n(a)Did you include the full text of instructions given to participants and screenshots, if\napplicable? [N/A]\n(b)Did you describe any potential participant risks, with links to Institutional Review\nBoard (IRB) approvals, if applicable? [N/A]\n(c)Did you include the estimated hourly wage paid to participants and the total amount\nspent on participant compensation? [N/A]\n14\n\nA The Proof of Lemma 5.2\nIn this appendix, we provide the proof of Lemma 5.2.\nProof of Lemma 5.2. The result follows by coupling the instances Ip;qandIp\u0003;q. LetU=\n(u1;:::;un),V= (v1;:::;vm), andU\u0003= (u1;:::ui\u00001;ui+1;:::;un). For any instance of a\nbipartite graph Ion(U;V)we can generate an instance I\u0003on(U\u0003;V)as follows: For each j2[m],\nifN(vj)is the neigborhood of vjinI, then the neighborhood of vjinI\u0003isN(vj)\\U\u0003. It is readily\nchecked that if Iis distributed as Ip;q, thenI\u0003is distributed as Ip\u0003;q. To prove the result, it therefore\nsufﬁces to show that for any instance I, ifA0(I)\u0015t, thenA0(I\u0003)\u0015t\u00001, or equivalently, that\nA0(I\u0003)\u0015A0(I)\u00001. We prove this by induction on m. The casem= 0is trivial, so assume that\nm> 0and inductively that the result holds for smaller values of m. LetMj\u0012Udenote the set of\nmatched nodes by MPD on Iafter the arrival of v1;:::;vjand let similarly M\u0003\nj\u0012U\u0003denote the set\nof matched nodes by MPD on I\u0003after the arrival of v1;:::;vj.\nWe will proceed by cases. First, consider the case in which MPD does not match any vertex vjto\nuiduring its run on I. Then MPD’s behavior on IandI\u0003will be identical as other than uiand its\nincident edges, the two graphs are the same. In particular, A0(I) =jMmj=jM\u0003\nmj=A0(I\u0003).\nConsider next the case in which MPD does match uito some vertex vjduring its run on I. Up\nuntil the arrival of vj, MPD has behaved similarly on IandI\u0003, and in particular, Mj\u00001=M\u0003\nj\u00001.\nNow if the neighborhood of vjinU\u0003does not contain an unmatched node, then Mj=M\u0003\nj[fuig,\nand from this point on, MPD will behave similarly on the two instances IandI\u0003. In particular,\nA0(I) =jMm(I)j=jM\u0003\nm(I)[fuigj=A0(I\u0003) + 1 , as desired. If on the other hand, MPD on I\u0003\nmatchesvjto some node ui06=uiinU\u0003, thenjMjj=jM\u0003\njj. Moreover, by the induction hypothesis,\njMmnMjj\u0014jM\u0003\nmnM\u0003\njj+1. It follows that A0(I) =jMjj+jMmnMjj\u0014jM\u0003\njj+jM\u0003\nmnM\u0003\njj+1 =\nA0(I\u0003) + 1 , as desired.\nB The Proof of Lemma 5.3\nIn this appendix, we provide the proof of Lemma 5.3. We start with some preliminaries. For\nsome subset of ofﬂine nodes S\u0012[n], letPp(NS)be the probability that the ﬁrst online node’s\nneighborhood is exactly the set SinIp;q. Note that Pp(NS)depends only on pandq1and equalsQ\ni2S(piq1)Q\ni2[n]nS(1\u0000piq1). Letp(A;S)be the ordered vector of weights of the unmatched\nofﬂine nodes remaining after running Aon the ﬁrst online node if this node has neighborhood S. If\nA=A0, thenp(A;S)is obtained by removing the entry piofpcorresponding to the ui2Swith\nminimalpi(ifS=;, thenp(A;S) =p), but generally, Acould behave differently even choosing\nnot to match v1to any node in S6=;.\nProof of Lemma 5.3. We will prove the lemma by induction on the number of online nodes, m. The\nbase casem= 0is trivial. Indeed, in this case, A0(Ip;q) =A0(Ip0;q) = 0 with probability 1, so the\nprobabilities of attaining matching size at least tare equal for the two weight vectors pandp0.\nNow, we will consider the inductive case. Consider any m> 0and assume the statement holds for\nm\u00001online nodes. Deﬁne the (m\u00001)-dimensional vector q\u0003= (q2;:::;qm). Then,\nP(A0(Ip;q)\u0015t) =P\np(N;)P(A0(Ip;q\u0003)\u0015t) +X\nS\u0012[n];S6=;P\np(NS)P(A0(Ip(A0;S);q\u0003)\u0015t\u00001);(5)\nand a similar identity holds with preplaced by p0.\nDenote byr=q1p= (q1p1;:::;q 1pn), andr0=q1p0= (q1p0\n1;:::;q 1p0\nn), so thatriandr0\niare\nthe probabilities that v1has an edge to uiandu0\niin respectively Ip;qandIp0;q. Asp\u0016p0, also\nr\u0016r0. In particular, we can write 1\u0000r0\ni= (1\u0000si)(1\u0000ri)for somesi2[0;1]. Fori2[n], we\nletXiandYibe independent Bernoulli variables with P[Xi= 1] =riandP[Yi= 1] =si. Let\nfurthermore Zibe the Bernoulli variable which is 1if eitherXi= 1orYi= 1, and zero otherwise.\nThenP[Zi= 1] =r0\ni. We now let N=fi2[n]jXi= 1gandN0=fi2[n]jZi= 1g, noting\nthatN\u0012N0. For anyT\u0012[n]we can then write\nP\np0(NT) =P[N0=T] =X\nS\u0012TP[N=S]P[N0=TjN=S] =X\nS\u0012TP\np(NS)\u0001(S;T);\n15\n\nwhere we have put P[N0=TjN=S] = \u0001(S;T). We note for later use that for any S\u0012[n], it\nholds thatP\nT\u0013S\u0001(S;T) = 1 . Indeed, conditioned on N=S, it holds that S\u0012N0with probability\n1. Combining this with (5), we can write the probability of MPD exceeding size ton the graphs\nparameterized by p0as\nP(A0(Ip0;q)\u0015t) =P\np0(N;)P(A0(Ip0;q\u0003)\u0015t) +X\nT\u0012[n];T6=;P\np0(NT)P(A0(Ip0(A0;T);q\u0003)\u0015t\u00001)\n=P\np0(N;)P(A0(Ip0;q\u0003)\u0015t)+\nX\nT\u0012[n];T6=;X\nS\u0012TP\np(NS)\u0001(S;T)P(A0(Ip0(A0;T);q\u0003)\u0015t\u00001)\n=P\np(N;)\u0001(;;;)P(A0(Ip0;q\u0003)\u0015t)+\nP\np(N;)X\nT\u0012[n];T6=;\u0001(;;T)P(A0(Ip0(A0;T);q\u0003)\u0015t\u00001)+\nX\nS\u0012[n];S6=;P\np(NS)X\nT\u0013S\u0001(S;T)P(A0(Ip0(A0;T);q\u0003)\u0015t\u00001); (6)\nwhere the ﬁnal steps follows by interchanging summations in the second term, and splitting into the\ncasesS=;andS6=;.\nNote that if Sis non-empty and S\u0012T, then by the MPD rule, p0(A0;S)\u0016p0(A0;T)as the\nminimum degree within the set of neighbors cannot be larger in Tthan inS. Moreover, it is\nreadily checked that the assumption p\u0016p0implies that p(A0;S)\u0016p0(A0;S)(after an appropriate\npermutation). Using the induction hypothesis, we get that for S6=;andT\u0013S,\nP(A0(Ip0(A0;T);q\u0003)\u0015t\u00001)\u0015P(A0(Ip(A0;S);q\u0003)\u0015t\u00001); (7)\nand that\nP(A0(Ip0;q\u0003)\u0015t)\u0015P(A0(Ip;q\u0003)\u0015t): (8)\nMoreover, an application of Lemma 5.2 and the induction hypothesis gives that\nP(A0(Ip0(A0;T);q\u0003)\u0015t\u00001)\u0015P(A0(Ip0;q\u0003)\u0015t)\u0015P(A0(Ip;q\u0003)\u0015t): (9)\nPlugging the bounds of (7),(8), and (9) into (6), it follows that\nP(A0(Ip0;q)\u0015t)\u0015P\np(N;)\u0001(;;;)P(A0(Ip;q\u0003)\u0015t)+\nP\np(N;)X\nT\u0012[n];T6=;\u0001(;;T)P(A0(Ip;q\u0003)\u0015t)+\nX\nS\u0012[n];S6=;P\np(NS)X\nT\u0013S\u0001(S;T)P(A0(Ip(A0;S);q\u0003)\u0015t\u00001):\nNow combining the ﬁrst two terms above and using thatP\nT\u0013S\u0001(S;T) = 1 for anyS\u0012[n], we\nobtain that\nP(A0(Ip0;q)\u0015t)\u0015P\np(N;)P(A0(Ip;q\u0003)\u0015t) +X\nS\u0012[n];S6=;P\np(NS)P(A0(Ip(A0;S);q\u0003)\u0015t\u00001)\n=P(A0(Ip;q)\u0015t);\nwhere the ﬁnal equality follows from (5). This is the desired result.\nC The Proof of Theorem 5.1\nIn this appendix, we provide the proof of Theorem 5.1. As in Appendix B, for subsets of ofﬂine\nnodesS\u0012[n], we let Pp(NS)denote the probability that the ﬁrst online node’s neighborhood is\nexactly the set SinIp;q.\n16\n\nProof of Theorem 5.1. We will prove the theorem by induction over m. For the base case, m= 0,\nthe inequality holds with equality as both algorithms yield empty matchings.\nFor the inductive case, let m > 0and assume the inequality holds with m\u00001online nodes. Let\nq\u0003= (q2;:::;qm). LetRA\u0012P([n])be set of subsets of [n]s.t. for anyS2RA,p(A;S) =p, i.e.\nalgorithmAmatches no edges if the set of neighbors of the current online node is S. Consider the\nprobability of algorithm Aattaining a matching with size at least t. By the induction hypothesis,\nP(A(Ip;q)\u0015t) =X\nS2RAP\np(NS)P(A(Ip;q\u0003)\u0015t) +X\nS\u0012[n];S=2RAP\np(NS)P(A(Ip(A;S);q\u0003)\u0015t\u00001)\n\u0014X\nS2RAP\np(NS)P(A0(Ip;q\u0003)\u0015t) +X\nS\u0012[n];S=2RAP\np(NS)P(A0(Ip(A;S);q\u0003)\u0015t\u00001):\nNow ifS2RA, thenp(A;S)\u0016p(A0;S). Thus, by applying Lemma 5.2 to the ﬁrst term and\nLemma 5.3 to the second term above,\nP(A(Ip;q)\u0015t)\u0014P\np(N;)P(A0(Ip;q\u0003)\u0015t) +X\nS2RAnf;gP\np(NS)P(A0(Ip(A0;S);q\u0003)\u0015t\u00001)+\nX\nS\u0012[n];S=2RAP\np(NS)P(A0(Ip(A0;S);q\u0003)\u0015t\u00001)\n=P\np(N;)P(A0(Ip;q\u0003)\u0015t) +X\nS\u0012[n];S6=f;gP\np(NS)P(A0(Ip(A0;S);q\u0003)\u0015t\u00001)\n=P(A0(Ip;q\u0003\u0015t);\ncompleting the proof.\nD MPD with Noisy Predictions\nIn Section 5, we show that given expected degrees as predictions, MPD is the optimal algorithm on\nCLV-B graphs. Here, we extend that analysis to show that even if the predictor is noisy, MPD can\nstill return a large matching.\nConsider two degree predictors \u001band\u001b0(or more generally, two orderings of the ofﬂine nodes). Let\n\u0001(\u001b;\u001b0)denote the minimum number of ofﬂine nodes which must be removed such that \u001band\u001b0\ninduce the same ordering over the remaining nodes.\nTheorem D.1. For any graph G= (U[V;E), the matching returned by MPD with degree predictor\n\u001bhas at most \u0001(\u001b;\u001b0)more edges than the matching returned by MPD with degree predictor \u001b0.\nOn CLV-B graphs, let p[\u001b]be the array of ofﬂine weights ordered by \u001b. So, if\u001b\u0003returns the expected\ndegrees,p[\u001b\u0003]is in sorted order. Let LIS(p)denote the size of the longest increasing subsequence\nofp. Note that \u0001(\u001b\u0003;\u001b) =n\u0000LIS(p). Then, as a corollary to Theorem D.1, we can bound the\nperformance of a noisy degree predictor compared to the performance of MPD with expected degrees.\nCorollary D.2. On CLV-B graphs, the expected size of the matching returned by MPD with degree\npredictor\u001bhas at mostn\u0000LIS(p[\u001b])fewer nodes than the expected size of the matching returned\nby MPD given the expected degrees.\nAs MPD with expected degrees is the optimal online algorithm for CLV-B graphs, this implies that as\nlong asn\u0000LIS(p[\u001b])is small, MPD is still near-optimal. Note that n\u0000LIS(p[\u001b])is equal to the\nminimum number of ofﬂine nodes that would need to be removed s.t. the ofﬂine weights are in sorted\norder. This quantity is clearly upper bounded by the number of mispredicted nodes as removing each\nmispredicted node will leave a remaining sequence which is sorted.\nIn order to prove the theorem, we will use the following lemmas.\nLemma D.3 (Corollary of Lemma 2 of [ 8]).Consider any graph G= (U[V;E), ofﬂine node\nu2U, and degree predictor \u001b. MPD with degree predictor \u001bwhen run on G0= (Unfug[V;E)\nwill produce a matching with at most one fewer edge than when run on G.\nThis follows by [ 8] as these two matchings will differ by at most one alternating path, which can\nreduce the matching size by at most one.\n17\n\nLemma D.4. Consider any graph G= (U[V;E), ofﬂine node u2U, and degree predictor \u001b.\nMPD with degree predictor \u001bwhen run on G0= (Unfug[V;E)will produce a matching no bigger\nthan when run on G.\nProof. LetNG(v)be the neighborhood of a node v2Vin the graph G, only including unmatched\nofﬂine nodes at the point that varrives. We will prove via induction that for any online node v,\nNG0(v)\u0012NG(v). For the base case, consider the ﬁrst online node v1. In this case, if u =2NG(v1),\nNG0(v1) =NG(v1). Otherwise NG0(v1) =NG(v1)nfug.\nNow, consider the inductive case for some online node vifori >1. Assume that for all j < i ,\nNG0(vj)\u0012NG(vj). Assume, for the sake of contradiction, that there exists some u0s.t.u02NG0(vi)\nbutu0=2NG(vi). This can only happen if at some point previously, u0was matched by MPD running\nonGbut was never matched by MPD running on G0. Letv0be the online node that matched with u0\nwhen run on G. Note that as u02NG0(vi), it must be the case that u02NG0(v0)as MPD running on\nG0has not yet matched u0andu0was inNG(v0). Letu\u0003be the node that v0matched with in G0. As\nMPD had the choice to match v0withu0oru\u0003onG0, it must be the case that \u001b(u\u0003)<\u001b(u0)(breaking\nties arbitrarily but consistently). On the other hand, by the inductive hypothesis, u0;u\u00032NG(v0),\nso MPD run on Gchose to match v0withu0overu\u0003, so\u001b(u0)< \u001b(u\u0003), and we have reached a\ncontradiction.\nTo complete the proof, note that for MPD (or any greedy algorithm), the size of the matching is\nexactlymminus the number of times an online node has no available neighbors. The inductive\nstatement implies that for all online nodes v,jNG0(v)j\u0014jNG(v)j. Therefore, the number of times\nan online node will have no available neighbors is at least as large for MPD when run on G0as on\nG.\nProof of Theorem D.1. LetS\u001aUbe the \u0001(\u001b;\u001b0)online nodes which, if removed, would leave the\nremaining ofﬂine weights in sorted order. Let A\u001b(G)refer to size of matching returned by MPD with\ndegree predictor \u001bon graphG. LetG\u0000S= ((UnS)[V;E).\nBy repeated invocation of Lemma D.4,\nA\u001b0(G)\u0015A\u001b0(G\u0000S)\nas removing an ofﬂine node never increases the size of the matching returned by MPD. By repeated\ninvocation of Lemma D.3,\nA\u001b(G)\u0014A\u001b(G\u0000S) + \u0001(\u001b;\u001b0)\nas removing an ofﬂine node can decrease the size of the matching returned by MPD by at most 1.\nTo combine these bounds, note that A\u001b(G\u0000S) =A\u001b0(G\u0000S)as\u001band\u001b0induce the same ordering\nafter removing the ofﬂine nodes in S. Therefore,\nA\u001b0(G)\u0015A\u001b(G)\u0000\u0001(\u001b;\u001b0);\ncompleting the proof.\nE Worst-Case Bound and Failure Modes\nIn the worst-case, MinPredictedDegree achieves a competitive ratio of 1=2. When\u001bgives arbitrary\npredictions, MinPredictedDegree is equivalent to the simple greedy algorithm for online bipartite\nmatching which achieves a competitive ratio of 1=2[31,41]. Even if\u001bis the perfect degree predictor\nwhere\u001b(u) =deg(u)for allu2U, MinPredictedDegree is still (1=2)-competitive in the worst-case.\nAs MinPredictedDegree forms a maximal matching, the matching it returns is always at least half the\nsize of the maximum matching. For the matching upper bound on the competitive ratio in the perfect\npredictor case, consider the graph G= (U[V;E)wheren=m= 6with the following adjacency\n18\n\nlist:\nfu1:v1;v2;v3g;\nfu2:v1;v2;v3g;\nfu3:v1;v2;v3g;\nfu4:v1;v4g;\nfu5:v2;v5g;\nfu6:v3;v6g:\nThe ﬁrst half of the ofﬂine and online nodes form a complete bipartite subgraph while the second\nhalf of the ofﬂine nodes each connect to one online node in the ﬁrst half and one online node\nin the second half. MinPredictedDegree with a perfect degree predictor will return the matching\nM=ffu4;v1g;fu5;v2g;fu6;v3gg, matching the ﬁrst half of the online nodes with their lower\ndegree neighbors in the second half of the ofﬂine nodes, therefore leaving the ﬁrst half of the ofﬂine\nnodes unmatched.\nMore generally, MinPredictedDegree can perform poorly if the degree predictor is arbitrary or if high\ndegree nodes have poor edge expansion compared to low degree nodes (making it disadvantageous to\nalways prioritize low degree nodes as seen in the example above). However, often these adversarial\nstructures do not appear in practice and matching low degree nodes ﬁrst leads to better results. In\nfact, the hard instance given in the seminal paper by Karp, Vazirani, and Vazirani [ 31] that introduced\nthe online bipartite matching problem and the Ranking algorithm relies on the fact that algorithms\nwith no extra information on the graph (e.g. no degree predictions) must often mistakenly match high\ndegree left nodes rather than low degree left nodes when given a choice.\nBetter worst-case algorithms with degree predictions? As demonstrated in [ 31], no algorithm\nfor the online matching problem has a competitive ratio better than 1\u00001=e. It is however a natural\nquestion whether an algorithm with access to the ofﬂine degrees can obtain a better worst-case\ncompetitive ratio. We answer this question in the negative by modifying the example in [ 31] to show\nthat no algorithm with knowledge of the true ofﬂine degrees has a competitive ratio of more than\n1\u00001=e. The hard example in [ 31] is annbynbipartite graph with ofﬂine nodes U=fu1;:::;ung\nand online nodes V=fv1;:::;vng. Nodevihas an edge to each node in fuj:i\u0014j\u0014ng. The\nmaximum matching clearly has size nbut it is shown in [ 31] that if the online nodes arrive in a\nrandom order, then no algorithm matches more than (1\u00001=e+o(1))nnodes in expectation. Note\nthat given the true ofﬂine degrees as predictions, MPD will actually achieve the maximum matching\nfor this example.\nWe augment this result in a black box manner to construct a bad example even when the ofﬂine\ndegrees are known to the algorithm. Assume with no loss of generality that nis a square. Let\nU1;:::;UpnandV1;:::;Vpnbe disjoint vertex sets each of sizepn. Let the ofﬂine nodes be\nU=S\ni\u0014pnUiand the online nodes be V=S\ni\u0014pnVi. For each 1\u0014i\u0014pn\u00001, we let (Ui;Vi)\nform an instance of the hard graph from [ 31] (as described above) withpnonline/ofﬂine nodes.\nMoreover, (Upn;Vpn)form a complete bipartite graph. Finally, for each ofﬂine node u2U, we addpn\u0000deg(u)edges fromutoVpn. Then all of the ofﬂine degrees are the same (they are allpn), so\nthe degree oracle is of no use. If we sequentially for 1\u0014i\u0014pn\u00001let the nodes of Viarrive in a\nrandom order (and ﬁnally the nodes from Vpnin any order), it follows from the result in [ 31] that the\nexpected size of the produced matching is at most\n(pn\u00001)(1\u00001=e+o(1))pn+pn=n(1\u00001=e+o(1))\nAs the maximum matching has size n, the upper bound on the competitive ratio follows.\nF Additional Competitive Ratio Results\nUsing the equations derived in Section 6, in Figure 6 we plot the competitive ratio for symmetric\nCLV-B graphs with expected ofﬂine degrees following Zipf’s Law (at n=m= 1000 , this setup\ncorresponds to the experiment shown in Figure 3). Across all choices of the exponent other than\n\u000b= 1, MPD’s performance relative to the size of the maximum matching increases as the size of the\ngraph grows. Further, for larger graphs, in many settings MPD achieves a ratio close to 1and even\nfor smaller graph achieves ratios above 0:9for almost all settings.\n19\n\n101102103104105106\nn=m0.900.920.940.960.981.00Ratio of expected matching size\n=0.6\n=0.8\n=1.0\n=1.2\n=1.4\nFigure 6: Ratio of MPD’s expected matching size and the expected maximum matching size on\nsymmetric CLV-B random graphs with ofﬂine expected degrees following Zipf’s Law with exponent\n\u000b. Theith ofﬂine node has expected degree di=Ci\u0000\u000bwithC=m=2.\nNotably, at\u000b= 1, the ratio plateaus around 0:955, and at\u000b= 0:8, the ratio decreases from n= 10\nton= 100 before rising again as nincreases. At \u000b= 1across all values of nas well as at \u000b= 0:8\nwithn= 100 , a large fraction of the ofﬂine nodes have expected degree close to one. Many of\nthese nodes will have actual degree 1and many will have actual degree \u00152. MPD has no way of\ndistinguishing between these two types of nodes as it only uses expected degrees and will mistakenly\nnot match some ofﬂine nodes that only appear once. While there is always a discrepancy between\nactual and expected degrees, the issue of prioritizing a node with actual degree \u00152over a node with\nactual degree 1is most detrimental, leading to worse performance when there are many ofﬂine nodes\nwith expected degree close to one.\nG Solution to System of Differential Equations in Equation 3\nRecall the system of differential equations from Equation 3:\ndzd(t)\ndt=kd\u0010\n1\u0000ezd(t)\u0011Y\nd0<dezd0(t)(10)\nfor all unique expected degrees dind.\nThe solution to the system of differential equations is given by the following equations. Let f\u000eig`\ni=1\nbe the ordered set of unique expected degrees and let fdbe the number of ofﬂine nodes with expected\ndegreed. We will deﬁne the auxiliary functions \u000b\u000ei(t)fori2f2;:::;`gand variables C\u000eifor\ni=f1;:::;`gas follows:\n\u000b\u000e2(t) =C\u000e1+ek\u000e1t\n\u000b\u000ei(t) = (\u000b\u000ei\u00001(t))k\u000ei\u00001=k\u000ei\u00002+C\u000ei\u00001(fori\u00153)\nwhere\nC\u000e1=ek\u000e1f\u000e1\u00001\nC\u000ei= (\u000b\u000ei(0))k\u000ei=k\u000ei\u00001(ek\u000eif\u000ei\u00001)(fori\u00152):\nThen,\nz\u000e1(t) =\u0000log(C\u000e1e\u0000k\u000e1t+ 1)\nz\u000ei(t) =\u0000log(C\u000ei(\u000b\u000ei(t))\u0000k\u000ei=k\u000ei\u00001+ 1) (fori\u00152):(11)\nIn the rest of this section, we show that Equation 11 give the correct solutions to the differential\nequations in Equation 3.\nLemma G.1. Fori2f2;3;:::;`g,\nd\ndt(\u000b\u000ei(t))\nk\u000ei\u00001\u000b\u000ei(t)=i\u00001Y\nj=1ez\u000ej(t): (12)\n20\n\nProof. We will prove the lemma by induction. Consider the base case of i= 2:\nd\ndt(\u000b\u000e2(t))\nk\u000e1\u000b\u000e2(t)=k\u000e1ek\u000e1t\nk\u000e1(C\u000e1+ek\u000e1t)\n=1\nC\u000e1e\u0000k\u000e1t+ 1\n=ez\u000e1(t):\nNow consider the inductive case of i>2under the assumption thatd\ndt(\u000b\u000ei\u00001)\nk\u000ei\u00002\u000b\u000ei\u00001=Qi\u00002\nj=1ez\u000ej:\nd\ndt(\u000b\u000ei(t))\nk\u000ei\u00001\u000b\u000ei(t)=k\u000ei\u00001\nk\u000ei\u00002\u000b(k\u000ei\u00001=k\u000ei\u00002)\u00001\n\u000ei\u00001d\ndt(\u000b\u000ei\u00001)\nk\u000ei\u00001(\u000bk\u000ei\u00001=k\u000ei\u00002\n\u000ei\u00001+C\u000ei\u00001)\n=\u000bk\u000ei\u00001=k\u000ei\u00002\n\u000ei\u00001\n\u000bk\u000ei\u00001=k\u000ei\u00002\n\u000ei\u00001+C\u000ei\u00001\u0001d\ndt(\u000b\u000ei\u00001)\nk\u000ei\u00002\u000b\u000ei\u00001\n=1\nC\u000ei\u00001\u000b\u0000k\u000ei\u00001=k\u000ei\u00002\n\u000ei\u00001+ 1\u0001i\u00002Y\nj=1ez\u000ej(t)\n=i\u00001Y\nj=1ez\u000ej(t):\nThis completes the proof.\nLemma G.2. The expressions for z\u000ei(t)in Equation 11 give a solution to system of differential\nequations in Equation 3 with initial conditions z\u000ei(0) =\u0000k\u000e1f\u000ei.\nProof. We will split the proof into two cases for \u000e1and for\u000eiwithi\u00152. Starting with i= 1, recall\nthat\nz\u000e1(t) =\u0000log(C\u000e1e\u0000k\u000e1t+ 1):\nFirst, we will show that this function has the correct derivative.\ndz\u000e1(t)\ndt=\u00001\nC\u000e1e\u0000k\u000e1t+ 1(C\u000e1)(\u0000k\u000e1)e\u0000k\u000e1t\n=k\u000e1C\u000e1e\u0000k\u000e1t\nC\u000e1e\u0000k\u000e1t+ 1\n=k\u000e1\u0010\n1\u0000ez\u000e1(t)\u0011\nIt remains to be shown that z\u000e1(0) =\u0000k\u000e1f\u000e1:\nz\u000e1(0) =\u0000log(C\u000e1+ 1)\n=\u0000log(ek\u000e1f\u000e1)\n=\u0000k\u000e1f\u000e1:\nNow, consider the case where i\u00152. Then,\ndz\u000ei(t)\ndt=d\ndt\u0010\n\u0000log(C\u000ei(\u000b\u000ei(t))\u0000k\u000ei=k\u000ei\u00001+ 1)\u0011\n=\u0000C\u000ei\u0000k\u000ei\nk\u000ei\u00001(\u000b\u000ei(t))\u0000k\u000ei=k\u000ei\u00001\u00001\nC\u000ei(\u000b\u000ei(t))\u0000k\u000ei=k\u000ei\u00001+ 1d\ndt(\u000b\u000ei(t))\n=k\u000eiC\u000ei(\u000b\u000ei(t))\u0000k\u000ei=k\u000ei\u00001\nC\u000ei(\u000b\u000ei(t))\u0000k\u000ei=k\u000ei\u00001+ 1d\ndt(\u000b\u000ei(t))\nk\u000ei\u00001\u000b\u000ei(t)\n=k\u000ei\u0010\n1\u0000ez\u000ei(t)\u0011i\u00001Y\nj=1ez\u000ej(t):\n21\n\nThe last step makes use of Lemma G.1. Finally, we must show that z\u000ei(0) =\u0000k\u000eif\u000ei:\nz\u000ei(0) =\u0000log(C\u000ei\u000b\u000ei(0)\u0000k\u000ei=k\u000ei\u00001+ 1)\n=\u0000log(ek\u000eif\u000ei\u00001 + 1)\n=\u0000k\u000eif\u000ei:\nThus, the given solution to the system of differential equations is correct.\nH Proof of Theorem 6.1\nWe give the proof of Theorem 6.1 which states that the solution to the differential equations models\nthe size of the matching returned by MinPredictedDegree.\nProof of Theorem 6.1. The proof of Theorem 6.1 follows a direct application of Theorem 1 in Luby\net al. [ 38]. We must show three conditions are satisﬁed: (i) that jZt+1\nd\u0000Zt\ndjis bounded, (ii) that\ndzd(t)\ndt=E[Zt+1\nd\u0000Zt\ndjHt](whereHtis the history up to time t, and (iii) thatdzd(t)\ndtsatisﬁes a\nLipschitz condition when zd(t)\u00140(recall thatZt\ndis always nonpositive as Yt\ndis always nonnegative).\nIf these conditions hold, then the solution to the system of differential equations gives the asymptotic\nexpected behavior of the variables Zt\ndand thus the asymptotic expected behavior of the variables Yt\nd,\nwhich govern the size of the matching returned by MinPredictedDegree.\nIt remains to show that these three conditions are met. Condition (i) is satisﬁed as the number of\nnodes of a given expected degree can change by at most one per timestep, so jZt+1\nd\u0000Zt\ndj\u0014kd.\nCondition (ii) is satisﬁed by construction in Equation 3. Finally, Condition (iii) is satisﬁed asdzd(t)\ndtis\ncomprised of a product of several terms resembling C1\u0001e\u0000C2xfor nonnegative constants C1;C2and\nwithxnonnegative. Therefore,dzd(t)\ndthas constant bounded ﬁrst derivatives when zd(t)\u00140.\nI Upper Bound on Expected Maximum Matching Size\nI.1 Overview\nTo analyze the maximum matching size within this model, we rely on a upper bound based on the\nmatching version of Hall’s marriage theorem [23]. We ﬁrst state the classic theorem.\nTheorem I.1 (Hall’s Theorem) .LetG= (U[V;E)be a bipartite graph. For any subset of nodes\nS, letN(S)be the set of neighbors of the nodes in X. Then,Ghas a perfect matching if and only if\nfor allS\u001aUandT\u001aV,jSj\u0014jN(S)jandjTj\u0014jN(T)j.\nIntuitively, if there is any subset Swith few neighbors, then only jN(S)jof the members of Scan\npossibly be matched. Let \u0016(G)correspond to the size of the maximum matching in G. For any\nbipartite graph G= (U[V;E)andS\u001aU,\n\u0016(G)\u0014n\u0000(jSj\u0000jN(S)j) (13)\nas out of all of the nodes in S, onlyjN(S)jcan be matched. Therefore, if we can calculate the\nexpected size ofjSjandjN(S)jfor some subset of a random symmetric CLV-B graph G, we\nimmediately get an upper bound on the expected size of the maximum matching in G.\nOur upper bound involves constructing a speciﬁc subset S\u0003of the ofﬂine nodes that makes use of\nour focus on power law graphs to provide a useful bound that is easy to evaluate. We empirically\ntest how good of a bound Equation 13 gives using S\u0003and ﬁnd that for symmetric CLV-B random\ngraphs with ofﬂine degrees following a power law distribution, the upper bound on \u0016(G)given by\nn\u0000(jS\u0003j\u0000jN(S\u0003)j)is close to maximum matching size (often achieving the same value and in all\ntrials was less than 2%greater than the true value). In addition to providing a good bound, we show\nthat we can evaluate E[jSj]andE[jN(S)j]in Equations 14, 15, 16. By linearity of expectation, this\ndirectly gives us an upper bound on the expected size of the maximum matching.\nIn Appendix M, we show that on symmetric CLV-B random graphs with power law distributed\ndegrees, our bound on the maximum matching size is concentrated about its expectation. Combined\nwith Theorem L.1, this implies that our analytic results on the ratio of the expected sizes in Section 6\nare closely related to the competitive ratio.\n22\n\nI.2 Construction\nLetS\u0003be the subset of Uconstructed as follows. Let U1be the set of degree 1 nodes in U(here\ndegree 1 referring to the actual degree of the node rather than the expected degree in the CLV-B\nmodel). Then, S\u0003is the maximal set of nodes in Us.t.N(S\u0003)\u0012N(U1). In other words, S\u0003is the\nmaximal subset of nodes in Uwhose neighbors completely overlap with the neighbors of the degree\n1 nodes ofU.\nAsn\u0000(jS\u0003j\u0000jN(S\u0003)j)gives an upper bound on the maximum matching size, E[n\u0000(jS\u0003j\u0000jN(S\u0003)j)]\ngives an upper bound on the expected maximum matching size. The expected sizes of S\u0003andN(S\u0003)\nin a CLV-B random graph with expected degrees dare given by the following equations. The expected\nsize ofN(S\u0003)is simply the sum over all online nodes v2Vof the probability that vhas at least one\ndegree 1neighbor. The expected size of S\u0003is broken down as the sum over all ofﬂine nodes u2U\nof the probability that uhas actual degree \u0001and then the probability that all \u0001ofu’s neighbors\nare members of N(S\u0003). LetS\u0003\n\u0001be the subset of nodes in S\u0003whose actual (as opposed to expected)\ndegree are \u0001and let\f\u0001\nifor\u00012f0;:::;mgandi2f1;:::;ngbe deﬁned as\n\f0\ni=\f1\ni= 1\n\f\u0001\ni= 1 +\u0001X\nr=1(\u00001)r\u0012\u0001\nr\u0013Y\ni06=i\"\n1\u0000r\u0012di0\nm\u0013\u0012\n1\u0000di0\nm\u0013m\u00001#\n(for\u0001\u00152):\n\f\u0001\nirepresents the probability of ui2Sconditioned on uihaving actual degree \u0001. Then,\nE[jN(S\u0003)j] =m \n1\u0000nY\ni=1\"\n1\u0000di\nm\u0012\n1\u0000di\nm\u0013m\u00001#!\n(14)\nand\nE[jS\u0003j] =mX\n\u0001=0E[jS\u0003\n\u0001j] (15)\nwhere\nE[jS\u0003\n\u0001j] =nX\ni=1\u0012m\n\u0001\u0013\u0012di\nm\u0013\u0001\u0012\n1\u0000di\nm\u0013m\u0000\u0001\n\f\u0001\ni: (16)\nIn the rest of this section, we show that the equations for the expected size of jS\u0003jandjN(S\u0003)jare\ncorrect by showing their derivations.\nFirst, consider E[jN(S\u0003)j]. Recall that N(S\u0003)is the set of online nodes that have a neighbor with\nactual degree 1. Therefore, the expected size of N(S\u0003)ismminus the expected number of online\nnodes that have no degree one neighbors. For any online node v2V, the probability that vhas no\ndegree 1 neighbors is\nY\nu2UP(uis not a deg 1 nbr of v)\n=Y\nu2U[1\u0000P(unbr ofv)P(uhas no other nbrs )]\n=Y\nu2U\"\n1\u0000du\nm\u0012\n1\u0000du\nm\u0013m\u00001#\n:\nBy linearity of expectation,\nE[jN(S)j] =m \n1\u0000Y\nu2U\"\n1\u0000du\nm\u0012\n1\u0000du\nm\u0013m\u00001#!\n:\nNow, we will deal with E[jS\u0003\n\u0001j]. Recall that S\u0003\n\u0001is the set of ofﬂine nodes with actual degree \u0001with\nall of their online neighbors having at least one ofﬂine neighbor with actual degree 1. For a given\n23\n\nofﬂine node u2Uwith expected degree du, the probability of ubeing inS\u0003\n\u0001is the product of the\nprobability of uhaving actual degree \u0001and the conditional probability of all of u’s neighbors having\na degree 1 neighbor given uhaving actual degree \u0001. We will call the ﬁrst event Au;\u0001and the second,\nconditional event Bu;\u0001jAu;\u0001. The probability of Au;\u0001occurring corresponds to a Binomial random\nvariable with size parameter mand probability parameter du=mtaking on value \u0001:\nP(Au;\u0001) =\u0012m\n\u0001\u0013\u0012du\nm\u0013\u0001\u0012\n1\u0000du\nm\u0013m\u0000\u0001\n:\nThe probability of Bu;\u0001jAu;\u0001equals 1 if \u0001 = 0 or\u0001 = 1 as eitheruhas no neighbors or uis\nitself a degree 1 neighbor of its neighbors, respectively. If \u0001\u00152, then P(Bu;\u0001jAu;\u0001)is equal\nto the complement of the event that at least one of u’s neighbors has no degree 1 neighbor. Let\nCu;\u0001;rjAu;\u0001be the event that any subset of rofu’s\u0001neighbors have no degree 1 neighbor given\nAu;\u0001.P(Cu;\u0001;rjAu;\u0001)can be expressed as\n\u0012\u0001\nr\u0013Y\nu06=u\"\n1\u0000r\u0012du0\nm\u0013\u0012\n1\u0000du0\nm\u0013m\u00001#\nwhere the term within the product represents the probability of no ofﬂine nodes (excluding u) being\ndegree one neighbors of a speciﬁc set of ronline nodes (similarly to when expressing E[jN(S\u0003)j]\nabove). By the inclusion-exclusion rule,\nP(Bu;\u0001jAu;\u0001) = 1 +\u0001X\nr=1(\u00001)rP(Cu;\u0001;rjAu;\u0001);\nthus completing the derivation of Equations 14, 15, and 16.\nJ Analysis on CLV-B random graphs in asymptotic case\nIn this section, we give slight modiﬁcations of the Equation 11 and Equations 14, 15, 16 in the\ncase wheren;m!1 to allow us to evaluate the equations to produce the results in Table 1. The\nmodel will change slightly when considering the asymptotic case: we will describe the set of ofﬂine\nexpected degrees dby a set of unique degrees f\u000eig`\ni=1and corresponding fractions f\u0015ig`\ni=1where a\n\u0015ifraction of the ofﬂine nodes have expected degree \u000ei.\nImportantly for the asymptotic results in Table 1, while there are ofﬂine nodes with expected degree\napproaching inﬁnity, a ﬁnite number of unique expected degrees account for all but an exponentially\nsmall fraction of the ofﬂine nodes, allowing us to evaluate the equations up to negligible error. In the\nfollowing calculations, we will consider both `as well as all \u000eifori=f1;:::;`gto be ﬁnite.\nJ.1 Asymptotic analysis of MinPredictedDegree\nTo start, we will replace fdwithm\u0001\u0015dand we will replace twith\u001c=t=m. Recallkd=\n\u0000log(1\u0000d=m). The Taylor expansion of log(1\u0000x)atx= 0is\u0000P1\nn=1xn\nn. Within Equation 11,\nkdappears in terms kd=kd0andkd\u0001fd=kd\u0001m\u0001\u0015d. In the asymptotic case, we will use the following\nsubstitutions for those terms:\nlim\nm!1kd=kd0=d=d0:\nand\nlim\nm!1kd\u0001m\u0001\u0015d=\u0000d\u0001\u0015d:\nIn both cases, we use the fact that as m!1 , the ﬁrst term in the Taylor series (d=m)dominates.\nUsing these substitutions, we can rewrite the equations for MinPredictedDegree as follows.\n\u000b\u000e2(\u001c) =C\u000e1+e\u000e1\u001c\n\u000b\u000ei(\u001c) = (\u000b\u000ei\u00001(\u001c))\u000ei\u00001=\u000ei\u00002+C\u000ei\u00001(fori\u00153)\nwhere\nC\u000e1=e\u000e1\u0015\u000e1\u00001\nC\u000ei= (\u000b\u000ei(0))\u000ei=\u000ei\u00001(e\u000ei\u0015\u000ei\u00001)(fori\u00152):\n24\n\nThen,\nz\u000e1(\u001c) =\u0000log(C\u000e1e\u0000\u000e1\u001c+ 1)\nz\u000ei(\u001c) =\u0000log(C\u000ei(\u000b\u000ei(\u001c))\u0000\u000ei=\u000ei\u00001+ 1) (fori\u00152):(17)\nRecall that the expected number of ofﬂine nodes with expected degree dis given by\u0000zd(\u001c)=kd\nevaluated when t=m. Then, in the asymptotic case, the expected fraction of ofﬂine nodes matched\nis\n`X\ni=1\u0015i+z\u000ei(1)=\u000ei (18)\nwherez\u000ei(\u001c)are given by Equation 17.\nJ.2 Asymptotic analysis of maximum matching\nFor the equations for the upper bound on the expected maximum matching size, the key fact we\nwill use is limx!0(1 +x) =ex. Therefore, we can replace all terms of (1\u0000d\nm)m\u0000Cwithe\u0000d. In\naddition, we can replace all terms of\u0000m\nC\u0001\u0000d\nm\u0001CwithdC\nC!. These substitutions give the following\nequations.\n\f0=\f1= 1\n\f\u0001= 1\u0000\u0001X\nr=1(\u00001)r\u0012\u0001\nr\u0013`Y\ni=1\u0014\n1\u0000r\u0012\u000ei\nm\u0013\ne\u0000\u000ei\u0015m\u0003\u0015i\n= 1\u0000\u0001X\nr=1(\u00001)r\u0012\u0001\nr\u0013`Y\ni=1e\u0000r\u000ei\u0015ie\u0000\u000ei(for\u0001\u00152):\nNote that the \f\u0001terms are no longer indexed by ias conditioning on the actual degree of a single\nnode makes no difference on the probability in the asymptotic case. Then,\nE\u0014jN(S\u0003)j\nm\u0015\n= \n1\u0000`Y\ni=1e\u0000\u000ei\u0015ie\u0000\u000ei!\n(19)\nand\nE\u0014jS\u0003j\nm\u0015\n\u0015CX\n\u0001=0E\u0014jS\u0003\n\u0001j\nm\u0015\n(20)\nwhere\nE\u0014jS\u0003\n\u0001j\nm\u0015\n=`X\ni=1\u0015i(\u000ei)\u0001\n\u0001!e\u0000\u000ei\f\u0001\ni: (21)\nK Analysis of MPD for Erd ˝os-Rényi Random Bipartite Graphs\nWe here analyze the performance of MPD on CLV-B instances, Ip;1, where p= (p;:::;p )2[0;1]n\nand1= (1;:::; 1)2[0;1]m. Such an instance is an Erd ˝os-Rényi bipartite random graph where all\nedges appear with the same probability p. In particular, the expected degrees of the ofﬂine vertices\nare the same and equal to d:=mp. Note that in this case, MPD is equivalent to any other greedy\nalgorithm. Letting c=m=n , we show that for a wide range of the parameters m;n , andp, the\nexpected fraction of matched ofﬂine vertices is 1 +c\u0000cln(ed+ed=c\u00001)\ndup to a small additive error.\nCombining this bound, with the asymptotic upper bound on the maximum matching of Appendix J,\nwe obtain that for these graphs, the asymptotic competitive ratio of MPD is at least 0.831 which\nis signiﬁcantly better than the 0:7299 bound from [ 11]. We conjecture that Erd ˝os-Rényi random\nbipartite graphs are in fact worst-case instances for MPD, in the sense that a lower bound on the\ncompetitive ratio of MPD for Erd ˝os-Rényi random graphs, also holds for general CLV-B random\ngraphs.\n25\n\nTheorem K.1. Letp2[0;1]andn;m2N. Assume that4n1\u0000o(1)\u0014m\u0014n1+o(1),p=o(logn)=m,\nandp\u00151=n1+o(1). Let p= (p;:::;p )2[0;1]n, and q= (1;:::; 1)2[0;1]m. LetMbe the size of\nthe matching output by any greedy algorithm Aon inputIp;q. Letc=m=n . Then\nE[M]\nn= 1 +c\u0000cln(ed+ed=c\u00001)\nd\u0006n\u00001=2+o(1):\nMoreover,jM\u0000E[M]j=O(pmlogn)with high probability in n.\nProof. LetFidenote the\u001b-algebra generated by the neighborhoods of the ﬁrst iarriving online\nnodes. Deﬁning Xi=E[MjFi], we have that (Xi)m\ni=0is a martingale with X0=E[M]and\nXn=M. It is easy to check that jXi\u0000Xi\u00001j\u00141for1\u0014i\u0014m, so it follows from Azuma’s\ninequality that for any t>0,\nP[jM\u0000E[M]j\u0015t]\u0014exp\u0012\u0000t2\n2m\u0013\nIn particular,jM\u0000E[M]j=O(pmlogn)with high probability, say at least 1\u0000n\u000010as claimed\nin the theorem. Moreover, the assumption pm=o(logn)implies that E[M]\u0014n(1\u0000n\u0000o(1)). This\nis clear when m\u0014n=2. On the other hand when m>n= 2, the probability that any given ofﬂine\nnode is never picked by an online node is (1\u0000p)m=e\u0000o(logn)=n\u0000o(1), using the assumption\npm=o(logn).\nFor0\u0014j\u0014n\u00001, we letTjdenote the number of online vertices vsuch that when varrives,\nthe matching found by Aso far has size j. ThenTjis geometrically distributed with parameter\n1\u0000(1\u0000p)n\u0000j, and so, E[Tj] =1\n1\u0000(1\u0000p)n\u0000j. Moreover,Tj=O(logn\n1\u0000(1\u0000p)n\u0000j)with high probability.\nDeﬁnen\u0003=bE[M]c. LetL1=n(1\u0000n\u0000o(1))be such that max(M;E[M])\u0014L1with probability\nat least 1\u0000n\u000010and letA1be the event that max(M;E[M])>L 1. Note that when j\u0014L1, then\nE[Tj]\u00141\n1\u0000(1\u0000p)n1\u0000o(1)\u00141\n1\u0000exp(\u0000pn1\u0000o(1))=1\n1\u0000exp(\u0000n\u0000o(1))=no(1):\nWe can therefore pick L2=no(1)such that max(T1;:::;TL1)\u0014L2with probability at least\n1\u0000n\u000010and we letA2be the event that max(T1;:::;TL1)>L 2. Finally, let L3=O(pmlogn)\nbe such thatjM\u0000E[M]j\u0014L3with probability at least 1\u0000n\u000010and letA3be the event that\njM\u0000E[M]j>L 3\nIf neitherA1,A2, orA3occur, which happens with probability at least 1\u00003n\u000010, then\n\f\f\f\f\f\fm\u0000X\ni\u0014n\u0003Ti\f\f\f\f\f\f\u0014\f\f\f\f\f\fm\u0000X\ni\u0014MTi\f\f\f\f\f\f+L2L3\u0014L2+L2L3=n1=2+o(1):\nFrom this, it particularly follows that\n\f\f\f\f\f\fm\u0000EX\ni\u0014n\u0003Ti\f\f\f\f\f\f\u0014n1=2+o(1): (22)\nLetn0be minimal such thatP\nj<n 0E[Tj]\u0015m. SinceTi\u00151for everyi, it follows from (22)\nthatjn0\u0000n\u0003j\u0014n1=2+o(1), and in particular that jn0\u0000E[M]j\u0014n1=2+o(1). To ﬁnish the proof, it\ntherefore sufﬁces to show that n0=nsatisﬁes the bound in the theorem.\nFor this, we ﬁrst note that\nm\u0014X\nj<n 0E[Tj] =X\nj<n 01\n1\u0000(1\u0000p)n\u0000j\u0014X\nj<n 01\n1\u0000e\u0000p(n\u0000j)\u0014Zn0\n01\n1\u0000e\u0000p(n\u0000x)dx:\n=n0\u00001\np\u0010\nln(1\u0000ep(n0\u0000n))\u0000ln(1\u0000e\u0000pn)\u0011\n4Here,o(1)!0asn! 1\n26\n\nThe right hand side is an increasing function of n0vanishing at 0and turning to inﬁnity as n0!n.\nSolving forn0then gives that\nn0\u0015n+m\u0000ln(epm+epn\u00001)\np;\nso that\nn0=n\u00151 +m=n\u0000ln(epm+epn\u00001)\npn= 1 +c\u0000cln(ed+ed=c\u00001)\nd\nwhich yields the lower bound in the proof of the theorem upon dividing by n.\nFor the upper bound, we use the inequality (1 +x\nn)n\u0015ex(1\u0000x2\nn)holding forn\u00151andjxj\u0014n,\nfrom which it follows that\nE[Tj]\u00151\n1\u0000e\u0000p(n\u0000j)(1\u0000(n\u0000j)p2)\u00151\n1\u0000e\u0000p(n\u0000j)a;\nwhere we have put a= 1\u0000np2. Note that by the deﬁnition of n0,P\nj<n 0\u00001E[Tj]<m , and so it\nfollows similarly to above that\nm0:=m+E[Tn0\u00001+Tn0]\u0015X\nj\u0014n0E[Tj]\u0015Zn0\n01\n1\u0000e\u0000p(n\u0000x)adx\n\u0015n0\u00001\np\u0010\nln(1\u0000aep(n0\u0000n))\u0000ln(1\u0000ae\u0000pn)\u0011\n:\nThe right hand side is again an increasing function of n0, so solving for n0gives that\nn0\u0014m0+n\u0000ln(epn+aepm\u0000a)\np=m0+n\u0000ln(epn+epm\u00001\u0000p2n(epm\u00001))\np\n=m0+n\u0000ln(epn+epm\u00001)\np+O(pnepm) =m+n\u0000ln(epn+epm\u00001)\np+ 2no(1):\nThe desired result follows after dividing by n.\nCompetitive ratio of MPD with uniform expected degree sequence. We now demonstrate how\nto combine the bound of Theorem K.1 with the upper bounds on the maximum matching of Ap-\npendix J, to obtain the better bound of 0.831 on the asymptotic competitive ratio of MPD for\nErd˝os-Rényi bipartite random graph. We start with the following lemma that allows us to focus on\nthe case where both c=m=n andd=pmare constants, d;c= \u0002(1) .\nLemma K.2. There exists an \" >0, so that if (c;d)2[\";1=\"]2, andnis sufﬁciently large, then\nthe competitive ratio of MPD on instance Ip;qis at least 0:99. Here, p= (p;:::;p )2[0;1]n, and\nq= (1;:::; 1)2[0;1]m.\nProof (sketch). LetS1denote the set of non-isolated ofﬂine nodes, S2the set of non-isolated online\nnodes,M\u0003the maximum matching, and Mthe matching found by MPD. First of all, it is easy to check\nthat there exists \"1>0, so that if if c<\" 1andnis large enough, then both j1\u0000E[jM\u0003j]\nE[jS2j]j<1\n1000\nandj1\u0000E[jMj]\nE[jS2j]j<1\n1000. The ﬁrst bound follows from observing that for csmall, nearly every\nnon-isolated online node can be matched: Even if, we just consider a single random edge leaving\neach of the non-isolated online nodes, the number of pair of such edges that are both incident to the\nsame ofﬂine node is O(jS2j2=n) =O(cjS2j), and thus the matching has size at least jS2j(1\u0000O(c)).\nThe second bound follows by observing that for each arriving non-isolated online node, MPD will\nmatch it with probability at least 1\u0000m=n = 1\u0000c.\nSimilarly to above, there exists \"2>0, such that if c >1=\"2, andnis large enough, then j1\u0000\nE[jM\u0003j]\nE[jS1j]j<1\n1000. Moreover, we can choose \"2such that ifc>1=\"2, andnis large enough, it also\nholds thatj1\u0000E[jMj]\nE[jS1j]j<1\n1000. Indeed, ifd\u001410, and\"2is small enough, then an 1\u000010=\"2fraction\nof the nodes in S1, will have a degree one neighbor, and therefore be matched, and the case d\u001510\nreduces to the case d= 10 .\n27\n\nCombining the above bounds it follows from some calculations that if we choose \"3=min(\"1;\"2),\nthen ifc =2[\"3;1=\"3], andnis large enough, the competitive ratio of MPD is at least 0:99.\nWe next assume that c2[\"3;1=\"3]. Using this assumption on c, we can then choose \"4>0such\nthat ifd<\" 4, then 99% of the edges (u;v)of the instance, will satisfy that the vertices uandvhave\ndegree 1. As MPD includes all those edges, and the total number of edges of the graph is an upper\nbound onjM\u0003j, it follows that in this case, the competitive ratio is at least 0:99. Finally, it is easy to\ncheck by splitting into the cases m\u0014nandm>n that we can choose \"5>0, such that if d\u00151=\"5,\nthenj1\u0000E[jMj]\nmin(n;m)j<1\n1000. AsjM\u0003j\u0014min(m;n), this gives that the competitive ratio in this case\nis at least 0:99. Setting\"= min(\"3;\"4;\"5), we obtain the desired result.\nNext, we show how to use the techniques of Appendix J to obtain an upper bound on the maximum\nmatching size in the case of Erd ˝os-Rényi random bipartite graphs in the case that d;c= \u0002(1) .\nLemma K.3. Letc;dbe given with c;d= \u0002(1) . Deﬁne\nA(c;d) = 1\u0000e\u0000cde\u0000d;andB(c;d) =e\u0000d(e\u0000dA(c;d)+d(1\u0000A(c;d)))\nFor instances Ip;q, where p= (p;:::;p )2[0;1]n, and q= (1;:::; 1)2[0;1]m, the expected\nfraction of matched ofﬂine vertices is at least (1\u0006o(1))C(c;d)asn!1 , whereC(c;d) =\n1 +cA(c;d)\u0000B(c;d).\nProof. LetN(S)denote the set of nodes of Vwith at least one degree one neighbor in U, and letS\ndenote the nodes of Uwhose neighborhood is fully contained in N(S). The suggestive notation is\njustiﬁed asN(S)is indeed the neighborhood of S. As we saw in the Appendix J, for v2V,\nP[v2N(S)] = 1\u0000 \n1\u0000d\nn\u0012\n1\u0000d\nn\u0013m\u00001!n\n= (1\u0006o(1))\u0010\n1\u0000e\u0000cde\u0000d\u0011\n= (1\u0006o(1))A(c;d);\nwhere we have used the approximation exfor1 +xwhich is sharp enough to get the bound above,\nsinced= \u0002(1) andm= \u0002(n)with the assumptions on candd. Next for a ﬁxed node u2U, we let\nAkdenote the event that uhas degreek. Then for any \u0001,\nP[u2S]\u0015\u0001X\nk=0P[u2S\\Ak]:\nAssuming that \u0001 =O(1)and2\u0014k\u0014\u0001, we can approximate\nP[u2S\\Ak] = (1\u0006o(1))\u0012m\nk\u0013\u0012d\nm\u0013k\u0012\n1\u0000d\nm\u0013m\u0000k\nA(c;d)k= (1\u0006o(1))e\u0000d(A(c;d)d)k\nk!:\nMoreover, P[u2S\\A0] =P[A0]andP[u2S\\A1] =P[A1], so we can bound\nP[u2S\\A0] = (1\u0006o(1))e\u0000dand P[u2S\\A1] = (1\u0006o(1))de\u0000d:\nNow for any constant \u0001, we can upper bound the expected fraction of matched ofﬂine vertices by\n1\u0000P[u2S] +cP[v2N(s)]\u00141\u0000 \u0001X\nk=0P[u2S\\Ak]!\n+cP[v2N(s)]\n=1\u0000(1\u0006o(1))e\u0000d \n1 +d+\u0001X\nk=2(A(c;d)d)k\nk!!\n+ (1\u0006o(1))cA(c;d)\nSince this bound holds for any constant \u0001, and the series converges, it also holds in the limit. Thus,\nwe can conclude that as n!1 , the expected fraction of matched ofﬂine nodes is at most\n(1\u0006o(1))\u0010\n1\u0000e\u0000d(eA(c;d)d+d(1\u0000A(c;d))) +cA(c;d)\u0011\n= (1\u0006o(1))(1\u0000B(c;d)+cA(c;d)) = (1\u0006o(1))C(c;d);\nas desired.\n28\n\nRemark When bounding the size of the maximum matching, with Lemma K.3, we can switch\nthe roles of mandn. Lettingd0=pnandc0=n=m , it holds that c0= 1=candd0=d=c, and\nwe can use Lemma K.3, to conclude that the expected fraction of matched online nodes is at most\n(1\u0006o(1))C(1=c;d=c )asn!1 . In particular, we can upper bound the expected fraction of ofﬂine\nnodes in a maximum matching by (1\u0006o(1))D(c;d), where\nD(c;d) = min(C(c;d);cC(1=c;d=c ))\nWe next show that for Erd ˝os-Rényi bipartite random graph, the asymptotic competitive ratio of MPD\nis at least 0:831.\nLemma K.4. LetIp;q, where p= (p;:::;p )2[0;1]n, and q= (1;:::; 1)2[0;1]mbe an Erd˝ os-\nRényi bipartite random graph. The asymptotic competitive ratio of MPD on Ip;qasn!1 is at least\n0:831.\nProof. By Lemma K.2, we can assume that (c;d)2[\";1=\"]2for some small enough constant, as\notherwise, the competitive ratio of MPD is at least 0.99. Then we are in position to apply Theorem K.1\nand Lemma K.3 (in fact the remark following the lemma). Letting E(c;d) = 1 +c\u0000cln(ed+ed=c\u00001)\nd,\nwe conclude that the asymptotic competitive ratio of MPD is at mostE(c;d)\nD(c;d). Using some calculus, it\ncan be checked that in any region [\";1=\"]2, where\"is sufﬁciently large,E(c;d)\nD(c;d)has a unique minimum\nattained at (c0;d0), wherec0= 1andd0\u00192:7997 . Moreover,E(c0;d0)\nD(c0;d0)\u00190:83105\u00150:831.\nL Concentration of MinPredictedDegree\nIn this section, we prove that MinPredictedDegree’s performance on CLV-B random graphs is\nconcentrated about its expectation.\nTheorem L.1. LetGbe a symmetric CLV-B random graph with expected ofﬂine degrees dand let\nXbe the random variable corresponding to the size of the matching returned by MPD. Then,\nP(jX\u0000E[X]j\u00152pmlogm)\u00142\nm: (23)\nProof. LetHjrepresent the state of MinPredictedDegree after it has processed the jth online node,\nand letYj=E[XjHj]be the expectation of the size of the returned matching conditioned on the\nhistory of the algorithm up to time j. ThenfYjgm\nj=0form a Doob martingale. We will proceed by\nboundingjYj\u0000Yj\u00001j.\nIf an ofﬂine node iwas matched with online node j, then the conditional expectation of the ﬁnal\nmatching size increases by 1\u0000P(imatchedjHj\u00001). For each unmatched ofﬂine node that is not\nmatched with online node j, the conditional expectation decreases by the sum over all such nodes i0\nofP(i0matched with jjHj\u00001). As both the increment and decrement are bounded in magnitude by 1,\nthe martingale has bounded differences jYj\u0000Yj\u00001j\u00141.\nApplying the standard Azuma’s inequality bounds, we get the concentration result:\nP(jYm\u0000Y0j\u00152pmlogm)\u00142\nm:\nAsY0=E[X]andYm=X, this completes the proof.\nM Concentration of the Upper Bound on Maximum Matching Size\nIn this section, we show that our upper bound on the size of a maximum matching in CLV-B random\ngraphs with power law distributed degrees is concentrated about its expectation.\nTheorem M.1. Let G be a CLV-B random graph with n=mand with expected ofﬂine degrees\nfollowing a power law distribution with exponent \u000b>3, and letXbe the random variable corre-\nsponding to the difference jS\u0003j\u0000jN(S\u0003)jwhereS\u0003andN(S\u0003)are the subsets of the nodes in G\ndescribed in Section I. Then, there exists some constant Cs.t.\nP(jX\u0000E[X]j\u0015Cpnlogn)\u00141\nn: (24)\n29\n\nProof of Theorem M.1. LetHtrepresent the tofﬂine nodes with the smallest degrees (ties broken\narbitrarily) as well as their incident edges and let Yt=E[(jS\u0003j\u0000jN(S\u0003)j)jHt]be the expected\ndifference in the sizes of S\u0003andN(S\u0003)given knowledge of Ht. Note that here we are using true\ndegrees and notexpected degrees. Then, fYtgn\nt=0form a Doob martingale.\nLetutbe the ofﬂine node with the tth smallest degree and let deg(ut)be the its degree. We will\nproceed by cases to show that the martingale has bounded differences.\n1.Assumedeg(ut)>1. Then, from Ht\u00001we knowN(S\u0003)as all degree one nodes have already\nbeen seen. Therefore, the contribution of utto the difference (jS\u0003j\u0000jN(S\u0003)j)is independent of\nany subsequent ofﬂine nodes. Speciﬁcally, if ut2S\u0003,Yt\u0000Yt\u00001= 1\u0000P(ut2S\u0003jHt\u00001), and if\nut=2S\u0003,Yt\u0000Yt\u00001=\u0000P(ut2S\u0003jHt\u00001)where\nP(ut2S\u0003jHt\u00001) = E\ndeg(ut)jHt\u000012\n4deg(ut)\u00001Y\nk=0\u0012N(S\u0003)\u0000k\nm\u00133\n5: (25)\nIn any case,jYt\u0000Yt\u00001j\u00141.\n2.Assumedeg(ut)\u00141. In this case, we have to deal with the fact that utcan affectN(S\u0003)as well\nasS\u0003. Part of the difference Yt\u0000Yt\u00001is due to the inclusion of utinS\u0003and the subsequent\npossibility that utcontributes a node to N(S\u0003)ifdeg(ut) = 1 and its neighbor is not already in\nN(S\u0003). This part of the difference is bounded in magnitude by one as these events change the\ndifferenceS\u0003\u0000N(S\u0003)by at most one.\nThe other part of the difference Yt\u0000Yt\u00001is due to whether utincrements the size of N(S\u0003)via\nits neighbor, affecting the probabilities P(ut02S\u0003)fort0wheredeg(ut0)>1as in Equation 25.\nAs the expected size of N(S\u0003)can change by at most one, the change in P(ut02S\u0003)for eacht0\nwheredeg(ut0)>1is at most\ndeg(ut0)\u00001Y\nk=0\u0012t0\u0000k\nm\u0013\n\u0000deg(ut0)\u00001Y\nk=0\u0012t0\u0000k\u00001\nm\u0013\n: (26)\nThe factors of t0in the numerators come from the fact that N(S\u0003)\u0014t0ifdeg(ut0)>1. As both\nparts of the difference contain many of the same terms, we can simplify Expression 26 as\ndeg(ut0)\nmdeg(ut0)\u00001Y\nk=1\u0012t0\u0000k\nm\u0013\n\u0014deg(ut0)\nm: (27)\nAs we assume that the expected degrees are distributed according to a power law distribution\nwith exponent \u000b > 3, the expectation and variance of the degree of a given node uwill be\nconstant. Thus, with high probability, the sum over all ofﬂine degreesP\nudeg(u) =O(m). The\ncontribution to Yt\u0000Yt\u00001by Expression 27 is thus bounded byP\nudeg(u)\nm=O(1). Overall, with\nhigh probability,jYt\u0000Yt\u00001j=O(1).\nAs in both cases, the martingale has constant bounded differences (with high probability), Azuma’s\ninequality directly gives us the theorem.\nN Additional Experiments\nFigure 7 shows additional experiments on Real World graphs from the known i.i.d. model (based on the\nmethodology of [ 9]). Overall, the results are very similar to those in Section 7, MinPredictedDegree\ndoes very well compared to the other online baselines (depicted in light blue) despite making relatively\nlittle use of the type graph information. Additionally, augmenting the known i.i.d. baselines with\nMinPredictedDegree (e.g. using the MinPredictedDegree rule when the base algorithm does not match\nthe current online node even though it has unmatched neighbors) often improves the performance\nover the baseline algorithm and the greedy augmentation.\nFigure 8 shows the degree distribution of the Oregon and Caida 2004 datasets as well as the `2\nprediction error (square root of the sum of the squared error of the degree prediction for each ofﬂine\nnode in the current graph) over time of using the ﬁrst days degrees as a prediction for future degrees.\nAs the prediction quality degrades, the performance of MinPredictedDegree slowly declines.\n30\n\nFeldmanEtAlJailletLuBrubachEtAlBahmaniKapralovManshadiEtAlSimpleGreedyRankingBKPKarpSipserBKPMinDegreeBrubachEtAl(g)FeldmanEtAl(g)JailletLu(g)FeldmanEtAl(MPD)BahmaniKapralov(MPD)MinPredictedDegreeJailletLu(MPD)BrubachEtAl(MPD)MinDegreeManshadiEtAl(g)BahmaniKapralov(g)ManshadiEtAl(MPD)Category-Advice3-PassOPTCE-GN\nFeldmanEtAlJailletLuBrubachEtAlManshadiEtAlBahmaniKapralovBKPKarpSipserRankingSimpleGreedyBKPMinDegreeBrubachEtAl(g)FeldmanEtAl(g)JailletLu(g)BrubachEtAl(MPD)JailletLu(MPD)MinPredictedDegreeBahmaniKapralov(MPD)FeldmanEtAl(MPD)MinDegreeManshadiEtAl(g)ManshadiEtAl(MPD)BahmaniKapralov(g)Category-Advice3-PassOPTCE-PG\n0.7 0.8 0.9 1.0\nCompetitive RatioFeldmanEtAlJailletLuBrubachEtAlBahmaniKapralovManshadiEtAlSimpleGreedyBKPKarpSipserRankingBrubachEtAl(g)FeldmanEtAl(g)JailletLu(g)BKPMinDegreeManshadiEtAl(g)BahmaniKapralov(g)BrubachEtAl(MPD)MinPredictedDegreeBahmaniKapralov(MPD)FeldmanEtAl(MPD)MinDegreeJailletLu(MPD)ManshadiEtAl(MPD)Category-Advice3-PassOPTbeause\n0.7 0.8 0.9 1.0\nCompetitive RatioFeldmanEtAlBahmaniKapralovJailletLuBrubachEtAlManshadiEtAlSimpleGreedyRankingBrubachEtAl(g)FeldmanEtAl(g)BKPKarpSipserJailletLu(g)ManshadiEtAl(g)BahmaniKapralov(g)BrubachEtAl(MPD)BKPMinDegreeJailletLu(MPD)Category-Advice3-PassManshadiEtAl(MPD)BahmaniKapralov(MPD)FeldmanEtAl(MPD)MinPredictedDegreeMinDegreeOPTmbeaflwFigure 7: Additional comparison of empirical competitive ratios on Real World graphs. Algorithms\ndepicted in gray are notonline algorithms (they use extra information or multiple passes). Algorithms\nin green are augmented with MPD.\n0 1000 2000\nDegree101103FrequencyOregon\n0 1000 2000\nDegree101103Caida 2004\n1 3 5 7 9\nDataset0200400L2 Prediction ErrorOregon\n1357911\nDataset02505007501000Caida 2004\nFigure 8: Degree distribution (left) and `2prediction error over time (right) for the Oregon and Caida\n2004 datasets.\n31",
  "textLength": 97146
}