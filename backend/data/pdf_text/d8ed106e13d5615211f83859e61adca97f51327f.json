{
  "paperId": "d8ed106e13d5615211f83859e61adca97f51327f",
  "title": "The “AI + R” - tree: An Instance-optimized R - tree",
  "pdfPath": "d8ed106e13d5615211f83859e61adca97f51327f.pdf",
  "text": "The “AI+R”-tree: An Instance-optimized R-tree\nAbdullah-Al-Mamun\nPurdue University\nmamuna@purdue.eduCh. Md. Rakin Haider\nPurdue University\nchaider@purdue.eduJianguo Wang\nPurdue University\ncsjgwang@purdue.eduWalid G. Aref\nPurdue University\naref@purdue.edu\nAbstract —The emerging class of instance-optimized systems\nhas shown potential to achieve high performance by specializing\nto a speciﬁc data and query workloads. Particularly, Machine\nLearning (ML) techniques have been applied successfully to build\nvarious instance-optimized components (e.g., learned indexes).\nThis paper investigates to leverage ML techniques to enhance\nthe performance of spatial indexes, particularly the R-tree, for\na given data and query workloads. As the areas covered by the\nR-tree index nodes overlap in space, upon searching for a speciﬁc\npoint in space, multiple paths from root to leaf may potentially be\nexplored. In the worst case, the entire R-tree could be searched. In\nthis paper, we deﬁne and use the overlap ratio to quantify the de-\ngree of extraneous leaf node accesses required by a range query.\nThe goal is to enhance the query performance of a traditional\nR-tree for high-overlap range queries as they tend to incur long\nrunning-times. We introduce a new AI-tree that transforms the\nsearch operation of an R-tree into a multi-label classiﬁcation task\nto exclude the extraneous leaf node accesses. Then, we augment\na traditional R-tree to the AI-tree to form a hybrid “AI+R”-\ntree. The “AI+R”-tree can automatically differentiate between\nthe high- and low-overlap queries using a learned model. Thus,\nthe “AI+R”-tree processes high-overlap queries using the AI-\ntree, and the low-overlap queries using the R-tree. Experiments\non real datasets demonstrate that the “AI+R”-tree can enhance\nthe query performance over a traditional R-tree by up to 500%.\nIndex Terms —ML for Database Systems, Spatial Indexing,\nInstance-optimized components, Learned Indexes\nI. I NTRODUCTION\nTraditional spatial indexes have been used successfully over\nthe years as an efﬁcient access method for location data. In\nthe area of spatial databases, the R-tree [1] is a widely used\nindex structure. In the multi-dimensional space, the R-tree is\nanalogous to the one-dimensional index structure B+-tree [2].\nThese traditional index structures, e.g., the B+-tree or the R-\ntree, do not make any assumptions about the underlying data\ndistribution. They are designed to work on a variety of data\nand query workloads. As a result, an index is not necessarily\noptimized for a particular data and query workloads.\nRecently, there is an emerging class of instance-optimized\nsystems proposed to optimize system performance for a spe-\nciﬁc data and query workloads, e.g., [3], [4]. Following the\nsame direction, we target to design an index for a particular\ndata and query workloads, i.e., an instance-optimized index; a\nlearned index that has better search and lower space require-\nments than their traditional counterparts [3], [5], [6]. Particu-\nlarly, ML techniques have been successfully applied to build\nTo appear in the proceedings of The 23rd IEEE International Conference\non Mobile Data Management (2022)\n(a) An example of leaf nodes in an\nR-tree\n(b) List of the accessed leaf\nnodes during query process-\ning\nFig. 1: An example of R-tree range query processing\ninstance-optimized system components [4], [5]. Although ML\nmodels are normally trained to generalize over a variety\nof datasets, in the context of designing instance-optimized\ncomponents, overﬁtting of ML models can be desired if the\nmodels learn only from a known dataset [3].\nIn this paper, we focus on answering range and point\nqueries over an R-tree due to their wide applicability in spatial\ndatabases [7]. In the R-tree, objects are stored using Minimum\nBounding Rectangles (MBRs). Notice that in the B+-tree,\nnodes do not overlap in space. However, the MBRs of non-\nleaf and leaf nodes of an R-tree can overlap in space. Figure 1\nillustrates the impact of node overlap in an R-tree to answer a\nrange query. Only the MBRs of the leaf nodes are displayed\nin Figure 1. Notice that the number of accessed leaf nodes\ndirectly impacts the query response time of an R-tree [7].\nFor a disk-based R-tree, descending multiple paths in the R-\ntree incurs high I/O cost [8]. The leaf nodes of the R-tree\nare labelled R7-R14. Consider Range Queries Q1, Q2, and\nQ3 in Figure 1. To process Q1, the R-tree searches both R12\nand R13, but the output data object is only present in R12.\nHence, accessing R13 is wasted. Similarly, to process Q2, the\nR-tree searches R10, R11, R13 and R14, but the output data\nentries are only in R10 and R14. In both Q1 and Q2, the R-\ntree accesses 50% more leaf nodes than the true number of\nleaf nodes containing the data objects. In contrast, for Query\nQ3, the R-tree searches both R7 and R8, and data objects are\nexactly found in both nodes.\nIn this case, the number of visited leaf nodes by the R-tree\nmatches the true number of leaf nodes required to answer Q3.\nThus, in terms of the number of leaf node accesses, we can\nidentify Q1 and Q2 as high-overlap queries and Q3 as a low-arXiv:2207.00550v1  [cs.DB]  1 Jul 2022\n\noverlap query. Observe that the R-tree searches extraneous leaf\nnodes to answer Q1 or Q2 but performs optimally for Q3. We\nFig. 2: Spectrum of the overlap ratio \u000bwith Threshold \u001cto\nidentify high- and low-overlap queries.\ndeﬁne an overlap ratio \u000bto quantify the degree of extraneous\nleaf node accesses required by a query. Speciﬁcally, for a range\nquery, we divide the number of true leaf nodes by the total\nnumber of visited leaf nodes to estimate \u000b, e.g., in Figure 1,\nto answer Q2, the number of visited leaf nodes is 4while the\nnumber of true leaf nodes is 2making\u000b= 0:50. Similarly,\nfor Q1 and Q3, \u000bis0:50and1, respectively. Notice that the\nnumber of true leaf nodes cannot exceed the number of visited\nleaf nodes. Hence, \u000branges from [0\u00001].\nFor the purposes of this paper, the high- and low-overlap\nqueries are determined as follows: Based on a pre-deﬁned\nthreshold\u001c, queries with overlap ratio \u000b\u0014\u001c(i.e., closer to\n0) are high-overlap while queries with \u000b > \u001c (i.e., closer\nto1) are low-overlap. The spectrum of the of the overlap\nratio\u000bwith Threshold \u001cis shown in Figure 2. To process\nhigh-overlap queries, we propose to ﬁnd the true R-tree leaf\nnodes using Multi-label Classiﬁcation ; a supervised ML task,\nwhere an input object can be classiﬁed into one or multiple\ncategories at once [9]. For example, classifying a research\npaper into a Systems, Theory, or ML paper is a multi-label\nclassiﬁcation task as a paper can be both a Systems and ML\npaper. Analogously, we can cast answering a range query\nover the R-tree, as a multi-label classiﬁcation task, where the\nclasses are the R-tree leaf nodes, and we need to ﬁnd these\nnodes that overlap the range query and that contain the output\nobjects to the query.\nMotivated by the beneﬁts of instance-optimized components\n(e.g., learned indexes) and considering the issue of node\noverlap in the R-tree, the following important questions arise:\nWhich workloads degrade the performance of range query\nprocessing in a traditional R-tree? Can we leverage ML\ntechniques to make R-tree range query processing faster?\nWe propose to use the overlap ratio \u000bto identify the high-\noverlap queries for which an R-tree accesses many extraneous\nleaf nodes. Moreover, we propose to build an ML-enhanced\nR-tree, termed the AI-tree, that leverages multi-label classiﬁ-\ncation techniques [9]. Finally, we adopt a hybrid structure,\ntermed the “AI+R”-tree, to avail the beneﬁts of both the\nAI-tree and the traditional R-tree (refer to Figure 3). The\nideas behind the AI-tree is as follows: First, we perform a\npreprocessing step to assign IDs to the leaf nodes of the R-tree.\nThen, we treat the queries as input and the corresponding true\nleaf node IDs as class labels. In the example R-tree in Figure 1,\nforQ1;Q2;andQ3, the corresponding class labels are the IDs\nfR12g,fR10;R14g, andfR7;R8g, respectively. Moreover,\nwe prepare training data by processing each of the queries\nin a traditional R-tree and storing the corresponding true leaf\nnode IDs as their class labels. Then, a multi-label classiﬁer\nFig. 3: Workﬂow of the “AI+R”-tree leveraging both the AI-\ntree and R-tree\nis constructed based on this training data. Motivated by the\nbeneﬁts of using multiple ML models (instead of a single\nmodel) [5], we adopt a multi-model approach that indexes the\nlearned ML models in a grid-based structure.\nTo realize the “AI+R”-tree, we leverage a binary classiﬁ-\ncation technique [10] to learn the value of the overlap ratio\n\u000bgiven an input range query. This enables the “AI+R”-tree\nto differentiate between high- and low-overlap queries. Notice\nthat the AI-tree is likely to perform better for high-overlap\nqueries while a traditional R-tree is expected to perform\nbetter for low-overlap queries (due to the fact that there is\nlimited scope for improvement). Notice further that the AI-tree\nperforms exact (i.e., not approximate) range query processing.\nThus, the “AI+R”-tree leverages the beneﬁts of both the AI-\ntree and the R-tree.\nThe contributions of this paper are as follows:\n1) We introduce an instance-optimized AI-tree that trans-\nforms the R-tree search operation into a multi-label\nclassiﬁcation task. While learned indexes are centered\naround the idea of learning the index , the AI-tree adds\nto that the idea of indexing the learned models .\n2) We leverage ML to differentiate between high- and low-\noverlap range queries. This gives rise to the “AI+R”-tree\nthat processes both query types efﬁciently.\n3) For ﬁxed query workloads, experiments on real spatial\ndata demonstrate that the “AI+R”-tree enhances the\nperformance of a traditional R-tree by up to 500%.\nThe remainder of this paper proceeds as follows: Section II\npresents the problem formulation. Section III introduces the\nAI-tree. Section IV introduces the hybrid “AI+R”-tree. Sec-\ntion V presents the experimental results. Section VI gives an\noverview of the related work. Finally, Section VII presents\nconcluding remarks and suggestions for future research.\nII. B ACKGROUND AND PROBLEM FORMULATION\nA. The R-tree: An Overview\nThe R-tree [1] is a balanced hierarchical index for multi-\ndimensional objects. Each leaf or non-leaf node of the R-tree\ncontains at least mand at most Mentries. A rectangular range\nquery is expressed as follows: Q ( Xmin,Ymin,Xmax,Ymax),\nwhere (Xmin,Ymin) and (Xmax,Ymax) represent the bottom-\nleft and top-right points of the query rectangle, respectively.\nTo process a range query Q, we start from the root of the tree,\nand check the MBR for each child of the root against Q to\n\nFig. 4: An example of R-tree with overlapping nodes\ntest which child nodes overlap Q. In case of an overlap, we\nsearch the sub-tree rooted at the corresponding child. When\nwe reach a leaf node, we report the objects that overlap Q.\nB. Classiﬁcation: A Supervised Machine Learning Technique\nClassiﬁcation [10] is a commonly used ML technique. It\ncan be divided into the following three categories: (1) Binary\nClassiﬁcation, where the number of classes is restricted to two,\n(2) Multi-class Classiﬁcation, where the number of classes n\nis more than 2, and the goal is to classify an object into exactly\none of thenclasses, and (3) Multi-label Classiﬁcation [9],\n[11], where we also have n(>2) classes, but the goal is to\nclassify an object into cclasses, where 0\u0014c\u0014n.\nC. Problem Formulation\nRefer to Figure 4 for illustration. Consider Queries Q1-Q3.\nFor Q1, we search the R-tree down the 2 paths (from root to\nleaf):R1!R5!R12andR1!R5!R13while only the\nlatter path contains actual query results. For Q2, we search the\nR-tree along 4 paths: R1!R5!R13,R2!R4!R10,\nR2!R6!R11, andR2!R6!R14while only the\n2nd and 4th paths contain output data objects. Notice that, for\nQuery Q3, the R-tree searches two paths: R1!R3!R7and\nR1!R3!R8, where both of them will contain output data\nobjects. Thus, for processing Q1 and Q2, the R-tree searches\nextraneous leaf nodes. Hence, we formulate our problem as\nfollows: Given a range query Q(Xmin;Ymin;Xmax;Ymax),\nwe need to predict the true leaf nodes of the R-tree that\ncontain output data objects, and only access these nodes\nwithout accessing extraneous ones.\nWe propose to formulate this problem as a multi-label\nclassiﬁcation task. For example, assume that an R-tree has\nfour leaf nodes with unique IDs 1–4. For a range query, the\nR-tree may have to access any number of leaf nodes out of\nthese four leaf nodes. We transform this problem into a multi-\nlabel classiﬁcation task by treating the leaf node IDs as the\nclass labels. At query time, the trained multi-label classiﬁer\npredicts the true leaf node IDs that contain data entries that\nfall inside the query region. Hence, we only need to access\nthe predicted leaf nodes to process the query.\nIII. T HEAI- TREE\nA. The Preprocessing Phase\n1) Assigning Unique Identiﬁers to the R-tree Leaf Nodes: In\nthe preprocessing step, each R-tree node is assigned a uniqueinteger identiﬁer (ID) based on a Depth First Search (DFS)\norder. Thus, all sibling leaf nodes of the R-tree will have\nconsecutive integers as their IDs.\n2) Deﬁnition of the Overlap Ratio \u000b:We deﬁne an overlap\nratio\u000bto quantify the degree of extraneous leaf node accesses\nrequired by a range query. Given a range query Q, to calculate\nthe value of \u000b, we use two metrics: the t rue n umber of leaf-\nnode accesses required to process Q (TN(Q), for short), and\nthe n umber of leaf nodes v isited by the R-tree index to answer\nQ (VN(Q), for short). For the range query Q, the deﬁnition of\n\u000bis as follows (the value of \u000bis in the interval [0;1]):\n\u000b=TN(Q)\nVN(Q)\n3) Query Workload Categorization: Given a query work-\nload, we categorize each query based on its selectivity. After\nidentifying the selectivity of a query, the overlap ratio \u000bof the\nquery is calculated to further categorize the queries based on\ntheir value of \u000b. This is achieved by executing the query during\nthe preprocessing phase, computing the query’s selectivity, the\nleaf nodes being touched, and the true leaf nodes.\n4) Preparing Training Data: This is a two-step process. In\nthe ﬁrst step, all queries in the query workload are executed\none at a time on the constructed R-tree over the given dataset.\nFor each executed query, we collect the following information:\nThe IDs of the leaf nodes that the R-tree visits to answer the\nquery, and the true leaf node IDs that contain the output data\nobjects that are actually inside the query region.\nTABLE I: Step-1 of Training Data Preparation\nQuery Visited Nodes True Nodes\nQ1 R12,R13 R13\nQ2 R13,R10,R11,R14 R10,R14\nQ3 R7,R8 R7,R8\nAssume that the ID assignment for the R-tree presented in\nFigure 4 is as follows: R7 and R8 have IDs 1 and 2, R12 and\nR13 have IDs 3 and 4, R9 and R10 have IDs 5 and 6, and\nR11 and R14 have IDs 7 and 8. For Query Q1, the visited leaf\nnodes are R12 and R13 but the true leaf node is R13. Thus, for\ntraining purposes, for Q1, we set the ID of R13, i.e., 3, as the\noutput label for the multi-label classiﬁer problem. Similarly,\nfor Query Q2, we have the ID of R10 and R14 (6 and 8) as\nthe labels for the multi-label classiﬁer. Moreover, for Q3, we\nhave the ID of R7 and R8 (1 and 2) as the labels. These steps\nare summarized in Tables I and II. In Table I, for each query,\nwe list the visited and the true leaf nodes. In Table II, we list\nthe leaf node IDs as the class labels for each of the queries.\nTABLE II: Step-2 of Training Data Preparation\nQuery Input Feature Labels\nQ1 (Xmin;Ymin;Xmax;Ymax ) 3\nQ2 (Xmin;Ymin;Xmax;Ymax ) 6, 8\nQ3 (Xmin;Ymin;Xmax;Ymax ) 1, 2\n\n5) Feature Representation: For an input range query Q,\nwe use the values (Xmin;Ymin;Xmax;Ymax)of the query\nrectangle as input features to the ML model without any addi-\ntional transformation. Thus, the same input can be processed\nseamlessly by both the AI-tree and the R-tree. Moreover, for\nmulti-label classiﬁcation, the output labels are encoded using\none-hot encoding, where we represent the class labels using\nbinary values, which is suitable for training the multi-label\nclassiﬁer, e.g., in Table II, for query Q1;Q2andQ3, the labels\nwill be encoded as 00100000;00000101 and11000000 .\nB. Learning the R-tree Index: ML Model Training and Testing\nRefer to Figure 5. The workﬂow for training and testing\nthe multi-label classiﬁer is as follows. (1) While the given\nFig. 5: Workﬂow of model training and testing\nqueries are executed by the R-tree, for each query, the IDs\nof the visited and true leaf nodes are captured. Thus, fol-\nlowing the approach in Section III-A4 (i.e., using the feature\nrepresentation of the queries and the true leaf node IDs as\nlabels), the training data is prepared for a particular data\nand query workloads. (2) Then, the training data is used to\ntrain the multi-label classiﬁer. Because the goal is to enhance\nthe performance of the R-tree search operation for a ﬁxed\ndata and query workloads, the ML models are intentionally\noverﬁtted on the training data. In this paper, we use a multi-\nlabel decision tree classiﬁer [11], [12] due to its ability to\noverﬁt the training data. Notice that the choice of the multi-\nlabel classiﬁer is not limited to the family of decision tree\nclassiﬁers. Because R-tree search has been cast as a multi-\nlabel classiﬁcation problem, we have the opportunity to use\nany suitable multi-label classiﬁer [13]. (3) A trained multi-\nlabel classiﬁer will be created after the training phase. (4) As\nthe AI-tree is optimized for a ﬁxed query workload, the queries\nwill be re-used as input for both the training phase of the\nmulti-label classiﬁer and the testing phase. This approach is\nsimilar to the previous works that leverage overﬁtting to build\ninstance-optimized systems components [3], [5]. (5) At query\ntime, the pre-trained multi-label classiﬁer is invoked to directly\npredict the true leaf node IDs that contain the query result.\nIndexing the Learned Models: A Multi-model Approach\nThe idea of indexing multiple learned models using a tra-\nditional index structure has been used in the context of music\nretrieval [14] and in handwritten and time series data [15].\nMoreover, the beneﬁt of indexing the learned models using a\nrecursive model index is also shown in [5]. Notice that forexact range query processing, our goal is to perfectly (i.e.,\n100% prediction accuracy) ﬁt the ML models to a particular\ndata and query workloads. However, even with overﬁtting, it\nmight not be possible to train a single ML model to capture\nthe entire underlying distribution of the training data [5]. As\na result, to achieve high prediction accuracy on the training\ndataset, multiple ML models are trained, e.g., several multi-\nlabel decision tree classiﬁers instead of a single ML model.\nIn the AI-tree, we use a simple index structure, e.g., a coarse\ngrid to partition the training queries. Then, we train a separate\nML model over queries inside each grid partition. The grid\nserves as an index to the localized learned ML models. As a\nresult, at query time, we only invoke the ML models whose\ngrid cell overlap the query rectangle. This concept is illustrated\nin Figure 6. The steps are as follows: Initially, the underlying\nFig. 6: Indexing the learned models\nspace is divided into equally sized grid cells. Then, during\nthe training phase, if a query overlaps a single grid cell, the\nML model corresponding to that cell is trained for that query.\nSimilarly, if a query overlaps multiple grid cells, all the cells’\ncorresponding ML models are trained for that query. Notice\nthat if no query overlaps a particular grid cell, we do not train\nany ML model for that grid cell. For example, for a 10X10\ngrid, it is not always the case that 100ML models need to be\ntrained. Finally, during query processing, only the ML models\nwhose grid cells overlap the range query are executed.\nFor example, in Figure 6, the space is partitioned using a\n4X4grid. Thus, at most 16 ML models (M1 to M16) can be\ntrained using the grid. In the training phase, we incrementally\nsearch for the grid size that produces the best ﬁt over the\ntraining data [16]. In Figure 6, as Query Q1 falls completely\ninside the top left grid cell, Model M1 is trained for Q1.\nOn the other hand, as Query Q2 overlaps four grid cells,\nthe four models M6, M7, M10, and M11 are trained for Q2.\nIf multiple models are trained for a particular query, during\nquery processing, we aggregate their prediction results from\nall the overlapping cells. This aggregated result is produced\nby performing a union of the predictions of the individual ML\nmodels.\nC. Query Processing\nGiven a range query, ﬁrst, the ML models are identiﬁed\nwhose grid cells overlap the range query. Then, only the\ndesignated ML models are executed to process the query. After\nthe prediction of the ML models, the results are aggregated in\n\nterms of leaf node IDs. Then, only the leaf nodes whose IDs\nhave been predicted by the ML models are accessed. Finally,\nall the data entries inside these leaf nodes are scanned to check\nwhich entries are actually contained within the input query\nrectangle. This ensures that the AI-tree never produces a false-\npositive result. Notice that we only access the predicted leaf\nnodes without traversing the R-tree or accessing the non-\nleaf nodes or the extraneous leaf nodes. Thus, if we predict\nthe leaf nodes accurately, we access the minimum number\nof leaf nodes needed to answer a query. This reduces the\nnumber of disk I/Os for processing a range query .\nNotice that in rare cases, the multi-label classiﬁer (Sec-\ntion II-B) might not predict any label for a particular query.\nIn other words, the classiﬁer might predict no leaf node ID\nfor a particular query. For the AI-tree, if the set of predicted\nleaf node IDs is empty for a particular query, we invoke a\nregular R-tree search operation. Moreover, if an ML model\npredicts a leaf node that does not contain any data object that\nis qualiﬁed in the result (i.e., mispredicts) of the given range\nquery, we may resolve to search the regular R-tree. Thus, the\nAI-tree performs exact query processing by combining both\nthe multi-model approach and the regular R-tree.\nIV. T HE”AI+R”- TREE\nTo achieve the best of both the AI-tree and the R-tree, we\nadopt a hybrid approach that we term the “AI+R”-tree (see\nFigure 3). We process the high-overlap queries using the AI-\ntree and the low-overlap queries using the traditional R-tree.\nHowever, this is non-trivial because the overlap ratio \u000bof\na query is unknown until we process the query. Hence, we\nleverage ML techniques to learn how to distinguish between\nhigh- and low-overlap queries. Speciﬁcally, the problem of\nclassifying the range queries based on the value of \u000band\nthe threshold \u001ccan be formulated as a binary classiﬁcation\ntask II-B. In order to prepare the training data for a particular\ndataset, we combine the queries for each of the \u000bvalues. Then,\nwe assign Label 0for the queries whose \u000bvalue is less than\nor equal to the threshold \u001c, and assign Label 1for the queries\nwhose\u000bvalue is greater than the threshold \u001c. Next, a binary\nclassiﬁer is trained on the training data. Finally, we can use the\ntrained binary classiﬁer to classify an incoming range query\ninto either a high- or a low-overlap query.\nA. Range Query Processing in the “AI+R”-tree\nGiven a range query Q, the binary classiﬁer is invoked ﬁrst\n(see Figure 3) to predict whether the incoming query Q is high-\nor low-overlap. If Q is classiﬁed as a high-overlap query, the\nAI-tree processes the query. Otherwise, the R-tree processes\nthe query. Notice that query processing using the “‘AI+R”-tree\nincurs a prediction cost before accessing the leaf nodes. Hence,\nthe cost of query processing of the “AI+R”-tree is: ML model\nprediction cost + I/O cost. Thus, we expect to get the beneﬁt\nof the AI-tree for processing the high-overlap queries whose\n\u000bvalue is closer to zero. On the other side of the spectrum of\n\u000b(Figure 2), for the queries with \u000bcloser to one, the R-tree\nis expected to perform better than the AI-tree.To demonstrate query processing in the “AI+R”-tree, con-\nsider the three queries in Figure 1. For Queries Q1 and Q2,\nthe overlap ratio \u000b= 0:50. If the “AI+R”-tree can accurately\npredict the leaf nodes, 50% less number of leaf nodes will\nbe accessed to answer the query. Notice that we have room\nfor improvement to process Q1 and Q2 using the AI-tree\ncomponent of the “AI+R”-tree. In contrast, for Q3, \u000b= 1.\nThus, both the visited leaf nodes contain data entries that fall\ninside the query rectangle. Thus, it is not possible for the AI-\ntree to process the query using less leaf node accesses than\nthe R-tree. Thus, we use the R-tree in this case.\nV. E VALUATION\nWe run all experiments on an Ubuntu 18:04with Intel Xeon\nPlatinum 8168 (2:70GHz) and 3TB of total available memory.\nA. Datasets\nWe use two datasets from the UCR Spatio-Temporal Active\nRepository, namely UCR-STAR [17]. Speciﬁcally, we use\ntwo real-world datasets with two-dimensional location data\n(in the form of longitude and latitude). The Tweets location\ndataset contains the locations of real tweets, and the other\ndataset contains the locations of Chicago crimes. Moreover,\nwe have preprocessed the datasets before using them for the\nexperiments. First, we eliminate the duplicate and missing\nvalues from both datasets. For the Tweets location dataset, we\ncreate a processed dataset containing the ﬁrst 2million tweet\nlocations. On the other hand, after removing the duplicate\nvalues from the dataset of Chicago crimes locations, we get a\nprocessed dataset containing 872;127records.\nB. Parameter Settings\n1) R-tree Parameters: All R-tree variants attempt to reduce\nthe amount of node overlap. However, with dynamic updates,\nthe shape of the R-tree deteriorates over time. Thus, we\nconstruct the R-tree using a one-at-a-time tuple insertion\nmethod to replicate the scenario of a dynamic environment.\nWhen constructing the R-tree, we set the minimum leaf node\nsizemto50% of the maximum leaf node size M. Another\nparameter of the R-tree is the node-splitting algorithm. In the\nexperiments, we use the linear node-splitting algorithm.\n2) Query Selectivity and Values of \u000b:For a particular\ndataset, to demonstrate the query performance for a particular\nvalue of\u000b, (at most) 1000 synthetic range queries are used\nin the experiments with a ﬁxed selectivity. For example, in\nthe case of the Tweets location dataset, a range query with\nSelectivity 0:00001 returns approximately 20objects, and a\nquery with Selectivity 0:00005 returns approximately 100\nobjects. In the experiments, the selectivity varies between\n0:00001 and0:00005 . We categorize the queries into ﬁve\ndifferent values of \u000b[0:1;0:25;0:5;0:75;1:0]. Thus, for each\ndataset, we use up to 5000 queries with various values of \u000b.\n3) The ”AI+R”-tree Parameters: The “AI+R”-tree has two\nparameters: The size of the grid (see Section III-B) and\nthe choice of the threshold \u001c(see Figure 2). Similar to the\nidea of hyperparameter tuning [16] for ML models, we start\n\nfrom a grid size 2X2and increase the size (e.g., 4X4) to\nget the best ﬁt for the training data. In all the experiments,\nwe have achieved the best ﬁt over the training data with a\nmaximum grid size of 20X20. Notice that using the multi-\nmodel approach and invoking the regular R-tree in case of\na misprediction, the AI-tree can achieve 100% prediction\naccuracy over the training data. As a result, both the AI-tree\nand the “AI+R”-tree can perform exact (i.e., not approximate)\nrange query processing.\nOn the other hand, for a query Q with \u000b= 0:75, theTN(Q)\nV N(Q)\ncan be e.g.,15\n20. Thus, there is room for improvement unless\n\u000b= 1. Thus, we set Threshold \u001c= 0:75. In other words, for\nan incoming range query, if \u000b\u00140:75, it is identiﬁed as a\nhigh-overlap query. If \u000b>0:75, it is considered low-overlap.\nC. Choosing the ML Models\n1) The Multi-label Classiﬁer: A decision-tree classiﬁer [12]\nhas the ability to overﬁt the training data, and hence can\nachieve high prediction accuracy for the training dataset.\nMoreover, a decision-tree classiﬁer is simple and explainable.\nAs a result, we use a multi-label decision tree model as the\nmulti-label classiﬁer [11]. However, with proper training, any\nmulti-label classiﬁer [13] can be used in the “AI+R”-tree.\nFor the ML models, we use the standard scikit-learn python\nlibrary [18]. We use the default parameters for the decision\ntree classiﬁer except the maximum-depth that is set to 30. This\nmaximum-depth is set to a high value to allow the decision\ntree classiﬁer to overﬁt the training data.\n2) The Binary Classiﬁer: For binary classiﬁcation, the goal\nis to train an ML model to classify an incoming query as\nhigh- or low-overlap. Notice that the goal is not to overﬁt but\nrather to generalize, and the same learned model will be able\nto classify high- vs. low-overlap queries across different query\nworkloads. We use a random forest classiﬁer [19] as the binary\nclassiﬁer. However, with proper training, any binary classiﬁer\ncan be used in the “AI+R”-tree. The training process of the\nbinary random forest classiﬁer is as follows: For a particular\ndata and query workloads, we combine the queries for each\n\u000b[0:1;0:25;0:5;0:75;1:0]. For a particular dataset with ﬁxed\nselectivity queries, we will have up to 5000 queries in total. For\nthe binary classiﬁer, we create the training data as follows: We\nassign Label 0for queries where \u000b\u0014\u001c(e.g.,\u000b\u00140:75), and\nLabel 1for queries where \u000b>\u001c (e.g.,\u000b >0:75). Moreover,\nwe split the training data where we use 80% for training and\n20% for testing. We use the scikit-learn python library [18],\nand use the default scikit-learn settings for the random forest\nbinary classiﬁer. The prediction accuracy of the binary random\nforest classiﬁer is around 80% over all values of \u000b.\nD. Implementation and Measurements\nWe realize the “AI+R”-tree using an open-source python\nlibrary for the R-tree available on Github1. We integrate\nthe “AI+R”-tree inside the library and run the experiments\n1https://github.com/sergkr/rtreelibusing Python Version 3.6.9. On the other hand, for a disk-\nbased R-tree index realized inside a practical system, in most\nof the cases, only the leaf nodes are stored in disk pages,\nand the internal nodes are kept in-memory. As a result, the\nperformance of a query depends on both the CPU cost and\nthe number of leaf node accesses. In the experiments, we\nassume that the required number of disk I/Os is equivalent\nto the number of leaf node accesses [20]. For a query, we\nmeasure the CPU time, and count the number of leaf node\naccesses. Then, we multiply the number of leaf node accesses\nby a standard disk I/O access time. Finally, we sum the CPU\nand disk I/O times to report the average query processing time\n(in milliseconds). In the experiments, we use a disk I/O access\ntime of thirteen milliseconds [21]. This approach is similar to\nthe experimental setup of a previous work [20].\nAlso, we report the size of the R-tree and the size of ML\nmodels2. The size of the ML models contains the summation\nof the sizes of both the multi-label and the binary classiﬁers.\nNotice that for a particular query workload with ﬁxed\nselectivity, to demonstrate the performance for each value of\n\u000b, we run each experiment individually for each value of \u000b.\nThis enables us to report the average query processing time\nand the size of the ML models for each value of \u000b.\nE. Experimental Results\n1) Tweet Locations Dataset: We construct an R-tree using\nthe minimum leaf capacity m= 100 and maximum leaf\ncapacityM= 200 . The selectivities of the synthesized queries\niare:0:00001 , and 0:00005 . As a result, for each range query,\nthe result contains approximately 20and100 data points,\nrespectively. Moreover, the value of the threshold \u001cis set\nto0:75. In each of the ﬁgures for this dataset, we show the\nvalue of overlap ratio \u000bin the X-axis and the average query\nprocessing time (in milliseconds) in the Y-axis. Also, we report\nthe average query processing time taken by the standard R-\ntree, the AI-tree, and the “AI+R”-tree.\n2) Effect of Selectivity for the Tweets Location Dataset:\nFigure 7a gives the results for Selectivity 0:00001 . From the\nﬁgure, both the AI-tree and the “AI+R”-tree enhance the per-\nformance of the R-tree by up to 3:69X and 3:58X, respectively.\nNotice that the performance loss is minimal between the AI-\ntree and the “AI+R”-tree, where the latter exhibits a hybrid\napproach to indexing. This same pattern of performance gains\nfor both trees applies for the cases of \u000b= 0:25. For\u000b= 0:25,\nthe AI-tree and the “AI+R”-tree enhance the performance of\nthe R-tree up to 2:06X and 1:97X, respectively. Moreover, the\n“AI+R”-tree performs better than the R-tree up to \u000b= 0:50.\nAfter that the R-tree starts to perform better. Notice that the\nhybrid approach reduces the query processing time of the AI-\ntree when the \u000b= 1.In summary, the “AI+R”-tree gets\nthe best of both worlds. In the case of high-overlap (low\n\u000bvalue), the “AI+R”-tree performs similar to the AI-tree,\nwhile in the case of low-overlap (high \u000bvalue), the “AI+R”-\ntree performs similar to the standard R-tree.\n2https://docs.python.org/3.6/library/sys.html\n\n0.1 0.25 0.5 0.75 1.0\nAlpha50100150200250Average Query Time(ms)\nR-tree\nAI-tree\n“AI+R”-tree(a) R-tree (M=200, m=100), and\nquery selectivity = 0:00001\n0.1 0.25 0.5 0.75 1.0\nAlpha100150200250300350Average Query Time(ms)\nR-tree\nAI-tree\n“AI+R”-tree(b) R-tree (M=200, m=100), and\nquery selectivity = 0:00005\n0.1 0.25 0.5 0.75 1.0\nAlpha50100150200250Average Query Time(ms)\nR-tree\nAI-tree\n“AI+R”-tree(c) R-tree (M=400, m=200), and\nquery selectivity = 0:00001\n0.1 0.25 0.5 0.75 1.0\nAlpha5075100125150175200225Average Query Time(ms)\nR-tree\nAI-tree\n“AI+R”-tree(d) R-tree (M=800, m=400), and\nquery selectivity = 0:00001\nFig. 7: Results on Tweet locations dataset\nFigure 7b gives the results for the same setup but for a\nselectivity of 0:00005 . As a result, each of the queries returns\napproximately 100points. From the ﬁgure, the AI-tree and the\n“AI+R”-tree exhibit the similar trend in performance gains.\n3) The Effect of Node Capacity for the Tweets Location\nDataset: In the next experiment, we vary the leaf node\ncapacity. We cover the cases for M= 200;400, and 800.\nWe ﬁx the selectivity of the synthesized queries to: 0:00001\n(The query result will contain approximately 20data points).\nThe AI-tree can perfectly ﬁt the training data with a 10X10\ngrid size for R-tree with node capacity M= 400 , and 800.\nFigures 7a, 7c, and 7d give the performance results of the\nAI-tree and the “AI+R”-tree for maximum leaf node capacities\nof 200, 400, and 800, respectively. The performance trends\nare the same. Overall, the performance gains of the AI-tree\nand the “AI+R”-tree over the R-tree increase as the node\ncapacities increase. The reason is that as the node capacity\nincreases, any additional extraneous leaf nodes retrieved by the\ntraditional R-tree will be very expensive due to the reﬁnement\nstep. Basically, as the node capacity increases, more leaf data\nobjects will need to be checked against the query range to\nreﬁne the results and form the actual output data objects from\namong the ones in the leaf node. In other words, due to the\nhigher leaf node capacities, the penalty of an unnecessary scan\ninside an extraneous leaf node reduces the R-tree performance\nin contrast to the AI-tree and the “AI+R”-tree. In Figure 7d, for\nnode capacity 800, the AI-tree enhances the performance of the\nR-Tree up to 6:06X for\u000b= 0:10. Also, the “AI+R”-tree does\nnot decrease the performance of the AI-tree by a large margin.\nTo be precise, the “AI+R”-tree enhances the performance of\nthe R-tree up to 5:39X for\u000b= 0:10.\nTABLE III: The R-tree and ML model sizes for the “AI+R”-\ntree for each \u000b(in MBs) for the Tweets Location dataset\n“AI+R”-tree with various values of \u000b\nSelec-\ntivityMax\nEntriesR-tree 0:10 0:25 0:50 0:75 1:0\n0.00001 200 978.05 9.50 9.50 9.51 11.38 9.50\n0.00005 200 978.05 9.44 9.50 9.52 9.52 9.51\n0.00001 400 972.05 1.97 2.02 2.97 2.96 2.01\n0.00001 800 969.52 1.02 2.07 1.55 1.07 1.064) Space Consumption of the ML Models for Tweets Lo-\ncation Dataset: Table III lists the sizes of the R-tree and\nthe ML models of the “AI+R”-Tree in Megabytes (MB, for\nshort). Notice that the reported ML model size includes the\nsizes of both the multi-label and the binary classiﬁers. The\nspace requirements of the ML models for the “AI+R”-tree with\nlarger leaf capacity (e.g., M=400, and M=800) are less than\nthose for the R-tree with leaf capacity 200. The reason is that\nthe number of leaf nodes is less for the larger node capacities.\nAlso, notice that a grid of size 10X10is sufﬁcient for “AI+R”-\ntree with larger node capacity. Hence, less models are likely\nto be trained to ﬁt the data. Thus, the size of the ML models is\neven less than the cases of using a larger grid of size 20X20.\n5) The Chicago Crimes Dataset: Overall, the Chicago\nCrimes dataset reﬂects the same performance trends as those\nfor the Tweets Location dataset in favor of the “AI+R”-tree\nover the R-tree. We give the performance results for the\nthe Chicago Crimes dataset below. For the Chicago crimes\ndataset, initially, we construct an R-tree using the maximum\nleaf capacity M= 200 , and minimum leaf capacity m= 100 .\nMoreover, the selectivity of the synthesized queries is: 0:00001\nand0:00005 . For each range query, the result contains (approx-\nimately) 9and44data points, respectively. Moreover, the AI-\ntree can perfectly ﬁt the training data for this query workload\nwith a grid size 20X20.\u001cis set to 0:75.\n6) Effect of Selectivity for the Chicago Crimes Dataset:\nFigures 8a and 8b give the performance results for the AI-tree,\nthe “AI+R”-tree, and the R-tree for Selectivities 0.00001 and\n0.00005. The performance gains of the “AI+R” over the R-\ntree is up to 3:6X for high overlap queries ( \u000b= 0:10) while\nis very close to the R-tree for \u000b= 1:0. This is consistent for\nboth selectivity values.\n7) Effect of Node Capacity for Chicago Crimes Dataset:\nWe vary leaf node capacity to cover for M= 200;400, and\n800. We ﬁx query selectivity to: 0:00001 . Moreover, the AI-\ntree can perfectly ﬁt the training data for a 10X10grid size and\nR-tree with node capacity M= 400 and800. Figures 8a, 8c,\nand 8d give the performance results of the AI-tree and the\n“AI+R”-tree for the Chicago Crimes dataset for maximum leaf\nnode capacities M=200, 400, and 800. The performance\ntrends are consistent with those of the Tweets Location dataset.\nOverall, the performance gains of the AI-tree and the “AI+R”-\n\n0.1 0.25 0.5 0.75 1.0\nAlpha406080100Average Query Time(ms)\nR-tree\nAI-tree\n“AI+R”-tree(a) R-tree (M=200, m=100), and\nquery selectivity = 0:00001\n0.1 0.25 0.5 0.75 1.0\nAlpha406080100120140Average Query Time(ms)\nR-tree\nAI-tree\n“AI+R”-tree(b) R-tree (M=200, m=100), and\nquery selectivity = 0:00005\n0.1 0.25 0.5 0.75 1.0\nAlpha20406080100Average Query Time(ms)\nR-tree\nAI-tree\n“AI+R”-tree(c) R-tree (M=400, m=200), and\nquery selectivity = 0:00001\n0.1 0.25 0.5 0.75 1.0\nAlpha20406080100120Average Query Time(ms)\nR-tree\nAI-tree\n“AI+R”-tree(d) R-tree (M=800, m=400), and\nquery selectivity = 0:00001\nFig. 8: Results on Chicago crimes dataset\ntree over the R-tree increase as the node capacities increase,\ne.g., forM= 800 , the “AI+R”-tree enhances the performance\nover the R-tree by 5:14X for\u000b= 0:10. Also, the “AI+R”-tree\nreduces the query performance of the AI-tree for \u000b= 1 to be\nclose to that of the R-tree.\n8) Space Consumption of the ML Models for the Chicago\nCrimes Dataset: Table IV lists the sizes of the R-tree and the\nML models of the “AI+R”-Tree (in MBs). The reported ML\nmodel size includes the sizes of both the multi-label and the\nbinary classiﬁers. The space requirements of the ML models\nfor the “AI+R”-tree with larger leaf capacity (e.g., M=400, and\nM=800) are less than for the R-tree with M= 200 . A grid\nof size 10X10is found to be sufﬁcient for the “AI+R”-tree in\nthese cases. Hence, less models are likely to be trained to ﬁt\nthe data. Also, the size of the ML models are even less than\nthe cases for using a larger grid of size 20X20.\nTABLE IV: The R-tree and ML model sizes for the “AI+R”-\ntree for each \u000b(in MBs) for the Chicago Crimes dataset.\n“AI+R”-tree with various values of \u000b\nSelec-\ntivityMax\nEntriesR-tree 0:10 0:25 0:50 0:75 1:0\n0.00001 200 426.46 3.39 2.57 3.40 3.40 3.40\n0.00005 200 426.46 2.52 2.57 2.58 3.41 3.41\n0.00001 400 423.86 0.48 0.53 0.53 0.53 0.53\n0.00001 800 422.84 0.27 0.32 0.33 0.33 0.32\n9)Discussion :From Figures 7a and 8a, the R-tree per-\nforms better than the AI-tree for \u000b= 0:75. As we set the\nthreshold\u001c= 0:75, the “AI+R”-tree also degrades in perfor-\nmance because it uses the AI-tree to process these queries\nwith\u000b= 0:75. In both cases, the R-tree has relatively small\nleaf capacity (i.e., M= 200 ). As the leaf capacity increases,\nfor the same value of threshold \u001c= 0:75, the “AI+R”-tree\nperformance enhances (see Figure 7d for the Tweets Location\ndataset and Figure 8d for Chicago Crimes dataset).\nFor each\u000b, the ML models increase the space requirement\nof the R-tree by no more than 1:1%(see Table III). Also, the\nspace overhead of the ML models for all values of \u000bdoes\nnot increase the size of the R-tree by more than 5:04%\n(Table III). Thus, the space requirement of the ML models\ncan be as low as 0:37% of the R-tree size (Table IV).VI. R ELATED WORK\nMany variants of the R-tree have been introduced, e.g.,\nsee [1], [7], [22]–[25]. The R+-tree [22] recognizes the\nproblem of node overlap in the R-tree, and creates an R-\ntree so that no two nodes overlap in space. The R\u0003-tree [23]\nreduces node overlap by introducing the forced re-insertion of\nentries. The RR\u0003-tree [24] is a further improvement over the\nR\u0003-tree for dynamic data. The RR\u0003-tree improves over the R\u0003-\ntree by restricting the insertion to a single path and dropping\nthe idea of re-insertion. In [26], the Clipped Bounding Box\n(CBB) based R-tree further improves the I/O performance of\nthe R\u0003-tree. The priority R-tree [27] can answer a query with\nan asymptotically optimal number of I/Os. The Hilbert R-\ntree [28] leverages the Hilbert space-ﬁlling curve to impose an\nordering on the R-tree nodes to achieve good space utilization.\nA worst-case optimal R-tree packing strategy that uses space-\nﬁlling curves can be found in [29]. Notice that regardless of\nthe type of the R-tree, all R-tree variants attempt to reduce\nthe amount of node overlap. However, with dynamic updates,\nthe shape of an constructed R-tree deteriorates. As a result,\nour design principles for “AI+R”-tree can be applied to other\nR-tree variants.\nThe concept of separating objects into partitions based on\ntheir size and indexing each partition with a space ﬁlling curve\ncan be found in [30], [31]. However, in the case of “AI+R”-\ntree, we do not partition the objects based on their size, but\nrather we group the queries using the grid to train multiple\nML models. Moreover, we do not use any space ﬁlling curve\n(i.e., as a projection function).\nThe initial research on learned indexes [5], [32] has\nintroduced the idea that “Indexes are models” by proposing\na Recursive Model Index (RMI, for short) for read-only\nworkloads. Many followup research has been conducted that\nis inspired by RMI both in the single and Multi-dimensional\nspace [6], [33]–[35].\nIn the case of multi-dimensional indexes, some initial effort\nto extend the idea of RMI into the multi-dimensional space can\nbe found in [36]. In [37], Z/Morton order is used to project\nthe data into the one dimensional space. Then, an RMI-like\nstructure can be used to build the learned index. However,\nlearning the projection function, e.g., Z-order [38], [39], from\nthe multi-dimensional space to the one-dimensional space is\n\nhard. Thus, it has been proposed to choose a layout that is\neasy to learn by an ML model. An efﬁcient scaling method\nhas been proposed in [40]. In our proposed “AI+R”-tree, we\navoid using a projection function, and operate directly on the\noriginal multi-dimensional representation of the spatial data\nobjects. In [41], an in-memory learned multi-dimensional in-\ndex, termed Flood, is introduced to efﬁciently support queries\nfor a particular dataset and (read-only ) query workloads. An\nextension to Flood has been proposed that can adapt to changes\nin the query workload [42]. Reinforcement Learning has been\nused to build an efﬁcient data layout [4], [43] for a particular\ndataset and query workload. A learned spatial index for disk-\nbased systems can be found in [8]. In [44], another disk-\nbased spatial index, termed RSMI, leverages a rank-space-\nbased transformation. The transformation has been used to\nget an easily learnable CDF. Notice that the goal of the above\nmentioned learned multi-dimensional indexes is to replace a\ntraditional index. However, in the case of the “AI+R”-tree, our\ntarget is not to replace the existing index structure rather to\nenhance its performance using ML models.\nThe idea of using helper ML models inside traditional\nindexes to enhance their performance have been presented\nin the multi-dimensional space, e.g., see [45]–[47]. In [45],\ninterpolation-based learned spatial indexes are proposed.\nIn [46], techniques from [41] have been applied to ﬁve\ntraditional multi-dimensional indexes. Recently, a disk-based\nML-enhanced index to process k-nearest-neighbor queries over\nhigh-dimensional time-series data has been proposed [47].\nThe goal of the proposed method [47] is to re-organize the\naccess order of the leaf nodes. In the context of ML-enhanced\nmulti-dimensional indexes, the focus of the above mentioned\ntechniques is not on analyzing (i.e., high- vs. low-overlap\nqueries) and optimizing the index for a given query workload.\nNotice that, in the case of the “AI+R”-tree, the focus is on\nanalyzing the query workload to identify the queries for which\na traditional disk-based spatial index (in this case, the R-tree)\ndoes not perform well. Moreover, we propose to adopt a hybrid\napproach to leverage the beneﬁt of both the proposed AI-tree,\nand the traditional R-tree.\nSurveys on the topic of learned data structures can be found\nin [33], [48]. Recently, several tutorials related to learned\nindexes have been presented in different venues [6], [34], [35],\n[49], [50].\nVII. C ONCLUSION\nIn this paper, we leverage machine learning techniques to\nbuild an instance-optimized R-tree for a given data and query\nworkloads. Although the paper focuses on the R-tree, the\nproposed design principles in the paper apply to other spatial\nindexes as long as node overlaps exist, and hence multiple\ntree paths are explored during search. Notice that we avoid\nusing a projection function, and operate directly on the original\nrepresentations of the spatial data objects. Also, because the\n“AI+R”-tree operates at the leaf node level of an R-tree, the\nproposed method can support different types of objects (e.g.,\nobjects with extension). Additionally, we adopt a multi-modelapproach and index the learned ML models using a grid-\nbased structure. We further leverage ML techniques to train\na binary classiﬁer to differentiate between high- and low-\noverlap queries. Finally, we advocate for a hybrid approach,\nnamely the “AI+R”-tree by combining both the traditional R-\ntree structure and the learned R-tree (i.e., the AI-tree) structure\nto maximize query processing performance. In the future, we\nplan to investigate alternative choices for the ML models, and\nhow to support k-NN query and spatial join using the proposed\n“AI+R”-tree. As we maintain a hybrid structure inside the\n“AI+R”-tree, we will be able to sustain updates using its R-tree\ncomponent. However, propagating the updates to the AI-tree\ncomponent is an interesting future research direction. Finally,\nwe plan to investigate challenges related to the integration of\nthe proposed “AI+R”-tree into practical database systems.\nACKNOWLEDGMENTS\nWalid Aref acknowledges the support of the U.S. National\nScience Foundation under Grant Numbers: III-1815796 and\nIIS-1910216.\nREFERENCES\n[1] A. Guttman, “R-trees: A dynamic index structure for spatial searching,”\ninProceedings of the ACM SIGMOD international conference on\nManagement of data , 1984, pp. 47–57.\n[2] D. Comer, “Ubiquitous b-tree,” ACM Computing Surveys (CSUR) ,\nvol. 11, no. 2, pp. 121–137, 1979.\n[3] T. Kraska, “Towards instance-optimized data systems,” Proceedings of\nthe VLDB Endowment , vol. 14, no. 12, p. 3222–3232, 2021.\n[4] J. Ding, U. F. Minhas, B. Chandramouli, C. Wang, Y . Li, Y . Li,\nD. Kossmann, J. Gehrke, and T. Kraska, “Instance-optimized data\nlayouts for cloud analytics workloads,” in Proceedings of the ACM\nSIGMOD International Conference on Management of Data , 2021, pp.\n418–431.\n[5] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The case\nfor learned index structures,” in Proceedings of the ACM SIGMOD\nInternational Conference on Management of Data , 2018, pp. 489–504.\n[6] A. Al-Mamun, H. Wu, and W. G. Aref, “A tutorial on learned multi-\ndimensional indexes,” in Proceedings of the 28th ACM SIGSPATIAL\nInternational Conference on Advances in Geographic Information Sys-\ntems, 2020, pp. 1–4.\n[7] Y . Manolopoulos, A. Nanopoulos, A. N. Papadopoulos, and Y . Theodor-\nidis, R-trees: Theory and Applications . Springer Science & Business\nMedia, 2010.\n[8] P. Li, H. Lu, Q. Zheng, L. Yang, and G. Pan, “Lisa: A learned\nindex structure for spatial data,” Proceedings of the ACM SIGMOD\nInternational Conference on Management of Data , 2020.\n[9] F. Herrera, F. Charte, A. J. Rivera, and M. J. Del Jesus, “Multilabel\nclassiﬁcation,” in Multilabel Classiﬁcation . Springer, 2016, pp. 17–31.\n[10] C. C. Aggarwal, “Data classiﬁcation,” in Data Mining . Springer, 2015,\npp. 285–344.\n[11] E. Gibaja and S. Ventura, “A tutorial on multilabel learning,” ACM\nComputing Surveys (CSUR) , vol. 47, no. 3, pp. 1–38, 2015.\n[12] J. R. Quinlan, “Induction of decision trees,” Machine learning , vol. 1,\nno. 1, pp. 81–106, 1986.\n[13] P. Szymanski and T. Kajdanowicz, “Scikit-multilearn: a scikit-based\npython environment for performing multi-label classiﬁcation,” The Jour-\nnal of Machine Learning Research , vol. 20, no. 1, pp. 209–230, 2019.\n[14] H. Jin and H. Jagadish, “Indexing hidden markov models for music\nretrieval.” in ISMIR , 2002.\n[15] W. Aref, D. Barbar ´a, and P. Vallabhaneni, “The handwritten trie:\nIndexing electronic ink,” in SIGMOD Rec. , 1995, p. 151–162.\n[16] J. Bergstra, R. Bardenet, Y . Bengio, and B. K ´egl, “Algorithms for hyper-\nparameter optimization,” Advances in neural information processing\nsystems , vol. 24, 2011.\n[17] S. Ghosh, T. Vu, M. A. Eskandari, and A. Eldawy, “Ucr-star: The ucr\nspatio-temporal active repository,” SIGSPATIAL Special , vol. 11, no. 2,\npp. 34–40, 2019.\n\n[18] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion,\nO. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg, J. Vander-\nplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay, “Scikit-learn: Machine learning in Python,” Journal of Machine\nLearning Research , vol. 12, pp. 2825–2830, 2011.\n[19] L. Breiman, “Random forests,” Machine learning , vol. 45, no. 1, pp.\n5–32, 2001.\n[20] A. R. Mahmood, W. G. Aref, A. M. Aly, and S. Basalamah, “Indexing\nrecent trajectories of moving objects,” in Proceedings of the 22nd\nACM SIGSPATIAL International Conference on Advances in Geographic\nInformation Systems , 2014, pp. 393–396.\n[21] Y . Deng, “What is the future of disk drives, death or rebirth?” ACM\nComputing Surveys (CSUR) , vol. 43, no. 3, pp. 1–27, 2011.\n[22] T. Sellis, N. Roussopoulos, and C. Faloutsos, “The r+-tree: A dynamic\nindex for multi-dimensional objects.” Tech. Rep., 1987.\n[23] N. Beckmann, H.-P. Kriegel, R. Schneider, and B. Seeger, “The r*-\ntree: an efﬁcient and robust access method for points and rectangles,”\ninProceedings of the ACM SIGMOD international conference on\nManagement of data , 1990, pp. 322–331.\n[24] N. Beckmann and B. Seeger, “A revised r*-tree in comparison with\nrelated index structures,” in Proceedings of the ACM SIGMOD Interna-\ntional Conference on Management of data , 2009, pp. 799–812.\n[25] H. Samet, Foundations of multidimensional and metric data structures .\nMorgan Kaufmann, 2006.\n[26] D. Sidlauskas, S. Chester, E. T. Zacharatou, and A. Ailamaki, “Improv-\ning spatial data processing by clipping minimum bounding boxes,” in\n34th International Conference on Data Engineering (ICDE) . IEEE,\n2018, pp. 425–436.\n[27] L. Arge, M. D. Berg, H. Haverkort, and K. Yi, “The priority r-tree: A\npractically efﬁcient and worst-case optimal r-tree,” ACM Transactions\non Algorithms (TALG) , vol. 4, no. 1, pp. 1–30, 2008.\n[28] I. Kamel and C. Faloutsos, “Hilbert r-tree: An improved r-tree using\nfractals,” in Proceedings of the 20th International Conference on Very\nLarge Data Bases , 1994, p. 500–509.\n[29] J. Qi, Y . Tao, Y . Chang, and R. Zhang, “Packing r-trees with space-ﬁlling\ncurves: Theoretical optimality, empirical efﬁciency, and bulk-loading\nparallelizability,” ACM Transactions on Database Systems (TODS) ,\nvol. 45, no. 3, pp. 1–47, 2020.\n[30] R. Zhang, J. Qi, M. Stradling, and J. Huang, “Towards a painless index\nfor spatial objects,” ACM Transactions on Database Systems (TODS) ,\nvol. 39, no. 3, pp. 1–42, 2014.\n[31] N. Koudas and K. C. Sevcik, “Size separation spatial join,” in Proceed-\nings of the ACM SIGMOD international conference on Management of\ndata, 1997, pp. 324–335.\n[32] R. Marcus, E. Zhang, and T. Kraska, “Cdfshop: Exploring and opti-\nmizing learned index structures,” Proceedings of the ACM SIGMOD\nInternational Conference on Management of Data , 2020.\n[33] P. Ferragina and G. Vinciguerra, “Learned data structures,” Recent\nTrends in Learning From Data , pp. 5–41, 2020.\n[34] S. Idreos and T. Kraska, “From auto-tuning one size ﬁts all to self-\ndesigned and learned data-intensive systems,” in Proceedings of the ACM\nSIGMOD International Conference on Management of Data , 2019, pp.\n2054–2059.[35] I. Sabek and M. F. Mokbel, “Machine learning meets big spatial data,”\nin36th International Conference on Data Engineering (ICDE) . IEEE,\n2020, pp. 1782–1785.\n[36] T. Kraska, M. Alizadeh, A. Beutel, E. H. Chi, J. Ding, A. Kristo,\nG. Leclerc, S. Madden, H. Mao, and V . Nathan, “Sagedb: A learned\ndatabase system,” in 9th Biennial Conference on Innovative Data\nSystems Research , 2019.\n[37] H. Wang, X. Fu, J. Xu, and H. Lu, “Learned index for spatial queries,”\nin20th International Conference on Mobile Data Management (MDM) .\nIEEE, 2019, pp. 569–574.\n[38] H. Sagan, Space-ﬁlling Curves . Springer Science & Business Media,\n2012.\n[39] M. F. Mokbel, W. G. Aref, and I. Kamel, “Analysis of multi-dimensional\nspace-ﬁlling curves,” GeoInformatica , vol. 7, no. 3, pp. 179–209, 2003.\n[40] A. Davitkova, E. Milchevski, and S. Michel, “The ml-index: A multidi-\nmensional, learned index for point, range, and nearest-neighbor queries.”\ninInternational Conference on Extending Database Technology (EDBT) ,\n2020, pp. 407–410.\n[41] V . Nathan, J. Ding, M. Alizadeh, and T. Kraska, “Learning multi-\ndimensional indexes,” in Proceedings of the ACM SIGMOD Interna-\ntional Conference on Management of Data , 2020, pp. 985–1000.\n[42] J. Ding, V . Nathan, M. Alizadeh, and T. Kraska, “Tsunami: a learned\nmulti-dimensional index for correlated data and skewed workloads,”\nProceedings of the VLDB Endowment , vol. 14, no. 2, pp. 74–86, 2020.\n[43] Z. Yang, B. Chandramouli, C. Wang, J. Gehrke, Y . Li, U. F. Minhas,\nP.-˚A. Larson, D. Kossmann, and R. Acharya, “Qd-tree: Learning data\nlayouts for big data analytics,” in Proceedings of the ACM SIGMOD\nInternational Conference on Management of Data , 2020, pp. 193–208.\n[44] J. Qi, G. Liu, C. S. Jensen, and L. Kulik, “Effectively learning spatial\nindices,” Proceedings of the VLDB Endowment , vol. 13, no. 12, pp.\n2341–2354, 2020.\n[45] A. Hadian, A. Kumar, and T. Heinis, “Hands-off model integration\nin spatial index structures,” in Proceedings of the 2nd International\nWorkshop on Applied AI for Database Systems and Applications , 2020.\n[46] V . Pandey, A. van Renen, A. Kipf, I. Sabek, J. Ding, and A. Kemper,\n“The case for learned spatial indexes,” arXiv preprint arXiv:2008.10349 ,\n2020.\n[47] R. Kang, W. Wu, C. Wang, C. Zhang, and J. Wang, “The case for ml-\nenhanced high-dimensional indexes,” in Proceedings of the 3rd Interna-\ntional Workshop on Applied AI for Database Systems and Applications ,\n2021.\n[48] X. Zhou, C. Chai, G. Li, and J. Sun, “Database meets artiﬁcial\nintelligence: A survey,” IEEE Transactions on Knowledge and Data\nEngineering , 2020.\n[49] G. Li, X. Zhou, and L. Cao, “Ai meets database: Ai4db and db4ai,”\ninProceedings of the ACM SIGMOD international conference on\nManagement of data , 2021, pp. 2859–2866.\n[50] K. Echihabi, K. Zoumpatianos, and T. Palpanas, “New trends in high-d\nvector similarity search: al-driven, progressive, and distributed,” Pro-\nceedings of the VLDB Endowment , vol. 14, no. 12, pp. 3198–3201,\n2021.",
  "textLength": 56846
}