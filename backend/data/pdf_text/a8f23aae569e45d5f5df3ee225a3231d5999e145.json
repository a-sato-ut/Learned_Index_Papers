{
  "paperId": "a8f23aae569e45d5f5df3ee225a3231d5999e145",
  "title": "PCF Learned Sort: a Learning Augmented Sort Algorithm with O(n log log n) Expected Complexity",
  "pdfPath": "a8f23aae569e45d5f5df3ee225a3231d5999e145.pdf",
  "text": "Published in Transactions on Machine Learning Research (10/2025)\nPCF Learned Sort: a Learning Augmented Sort Algorithm\nwithO(nlog logn)Expected Complexity\nAtsuki Sato a_sato@hal.t.u-tokyo.ac.jp\nGraduate School of Information Science and Technology\nThe University of Tokyo\nYusuke Matsuimatsui@hal.t.u-tokyo.ac.jp\nGraduate School of Information Science and Technology\nThe University of Tokyo\nReviewed on OpenReview:https: // openreview. net/ forum? id= wVkb8WHbvR\nAbstract\nSorting is one of the most fundamental algorithms in computer science. Recently, Learned\nSorts, which use machine learning to improve sorting speed, have attracted attention. While\nexistingstudiesshowthatLearnedSortisempiricallyfasterthanclassicalsortingalgorithms,\nthey do not provide theoretical guarantees about its computational complexity. We propose\nPiecewise Constant Function (PCF) Learned Sort, a theoretically guaranteed Learned Sort\nalgorithm. We prove that the expected complexity of PCF Learned Sort isO(nlog logn)\nunder mild assumptions on the data distribution. We also confirm empirically that PCF\nLearned Sort has a computational complexity ofO(nlog logn)on both synthetic and real\ndatasets. This is the first study to theoretically support the empirical success of Learned\nSort, and provides evidence for why Learned Sort is fast.\n1 Introduction\nSorting is one of the most fundamental algorithms in computer science and has been extensively studied for\nmanyyears. Recently, anovelsortingmethodcalledLearnedSorthasbeenproposed(Kraskaetal.,2019). In\nLearned Sort, a machine learning model is trained to estimate the distribution of elements in the input array,\nspecifically, the cumulative distribution function (CDF). The model‚Äôs predictions are then used to rearrange\nthe elements, followed by a minor refinement step to complete the sorting process. Empirical results show\nthat Learned Sort is faster than classical sorting algorithms, including highly optimized counting-based\nsorting algorithms, comparison sorting algorithms, and hybrid sorting algorithms.\nOn the other hand, there are few theoretical guarantees regarding the computational complexity of Learned\nSort. The first proposed Learned Sort algorithm (Kraska et al., 2019) has a best-case complexity ofO(n), but\nits expected or worst-case complexity is not discussed. The more efficient Learned Sort algorithms proposed\nlater (Kristo et al., 2020; 2021a) also haveO(n)best-case complexity, butO(n2)worst-case complexity (or\nO(nlogn)with some modifications). The goal of this paper is to develop a Learned Sort that is theoretically\nguaranteed to be computationally efficient.\nWe propose Piecewise Constant Function (PCF) Learned Sort, which can sort with an expected complexity\nO(nlog logn)under mild assumptions on the data distribution. In addition, we show that PCF Learned Sort\nadmits a worst-case complexity guarantee that depends on the choice of the internal sorting algorithm. For\ninstance, if MergeSort‚Äîwhose worst-case complexity isO(nlogn)‚Äîis used, then the worst-case complexity\nof PCF Learned Sort isO(nlogn). If we instead use the algorithm by Han (2020), which has a worst-case\ncomplexity ofO(n‚àölogn), then the worst-case complexity of PCF Learned Sort becomesO(n‚àölogn). We\nthen empirically confirm that our Learned Sort can sort with a complexity ofO(nlog logn)on both synthetic\nand real datasets.\n1arXiv:2405.07122v3  [cs.DS]  10 Nov 2025\n\nPublished in Transactions on Machine Learning Research (10/2025)\nIndependently and concurrently, Zeighami & Shahabi (2024) explored complexity-guaranteed Learned Sort.\nWhile our PCF Learned Sort is motivated by similar principles and incorporates comparable design choices,\nseveral key distinctions exist between our approach and theirs. A comprehensive comparison between our\nmethod and that of Zeighami & Shahabi (2024) is provided in Section 5.\nThis paper is organized as follows. Section 2 reviews related literature. Section 3 introduces PCF Learned\nSort and provides complexity theorems with proof sketches. Section 4 provides empirical validation of the\ntheoretical results. We discuss implications and limitations in Section 5, and conclude in Section 6.\n2 Related Work\nOur research is in the context of algorithms with machine learning (Section 2.1). There are two types of\nsorting algorithms, comparison sorts (Section 2.2) and non-comparison sorts (Section 2.3), and our proposed\nmethodisanon-comparisonsort. However, theidea, implementation, andproofofcomputationalcomplexity\nof our method are similar to those of sample sort, which is a type of comparison sort. Furthermore, our\nproposedalgorithmandproofofcomputationalcomplexityarebasedonthoseofLearnedIndex(Section2.4).\n2.1 Algorithms with Machine Learning\nOur work lies in the emerging area of Learned Sort, where machine learning models accelerate sorting. The\nfirst approach by Kraska et al. (2019) trained a model ÀúF(q)to approximate the CDFF(q)and placed\neach element at positionn ÀúF(x), and then applies insertion sort to complete the ordering. While this yields\nO(n)in favorable cases, the insertion-based refinement causes aO(n2)worst case. Later implementations\nimproved cache efficiency (Kristo et al., 2020), robustness to duplicates (Kristo et al., 2021a), and even\nintroduced cache-friendly CDF models tailored for sorting (Ferragina & Odorisio, 2025), but still relied on\ninsertion sort and thus retained a quadratic worst-case complexity. Using stronger fallback algorithms such\nas Introsort (Musser, 1997) or TimSort (McIlroy, 1993) reduces this toO(nlogn), though this only matches\nclassical comparison-based sorts and does not fully explain the empirical advantage of Learned Sort.\nParallelization has also been explored: Carvalho (2022) proposed IPLS, integrating learned partitioning into\nIPS4o (Axtmann et al., 2017), and Carvalho & Lawrence (2023) framed LearnedSort as a sample sort with a\nparallel IPS4o implementation. These studies primarily highlight the engineering potential of learned sorting\nin parallel settings, which is complementary but orthogonal to our focus on providing theoretical guarantees\nfor the sequential case.\nIn a broader context, Li et al. (2020) showed that neural programs can achieve strong generalization and\nreplicate efficient algorithmic behaviors such as sorting, demonstrating that machine learning can recover\nalgorithmic complexity classes likeO(nlogn)without explicit manual design. This provides a useful per-\nspective on the generalizability of learned algorithms.\nRelated to this, the field ofalgorithms with predictionsstudies how machine learning predictions accelerate\nclassical algorithms (Mitzenmacher & Vassilvitskii, 2022), with applications in caching (Narayanan et al.,\n2018;Rohatgi,2020;Lykouris&Vassilvitskii,2021;Imetal.,2022), skirental(Purohitetal.,2018;Gollapudi\n& Panigrahi, 2019; Shin et al., 2023), scheduling (Gollapudi & Panigrahi, 2019; Lattanzi et al., 2020; Lassota\net al., 2023), and matching (Antoniadis et al., 2023; Dinitz et al., 2021; Sakaue & Oki, 2022). Sorting with\npredictions has also been analyzed (Lu et al., 2021; Chan et al., 2023; Erlebach et al., 2023), including\ntight guarantees by Bai & Coester (2023). In the context of algorithms with predictions, machine learning\npredictions are typically assumed to be available at no cost, and the models are treated as an opaque box.\nThis exclusion contrasts with our problem setting, where we ensure that the computational complexity covers\nthe entire process from receiving an unsorted array to returning a sorted array.\n2.2 Comparison Sorts\nSorting algorithms that use comparisons between keys and require no other information about the keys\nare called comparison sorts. It is well-known that the worst-case complexity of a comparison sort is at\nleast‚Ñ¶(nlogn). Commonly used comparison sorting algorithms include QuickSort, heap sort, merge sort,\n2\n\nPublished in Transactions on Machine Learning Research (10/2025)\nand insertion sort. The GNU Standard Template Library in C++ uses Introsort (Musser, 1997), an algo-\nrithm that combines QuickSort, heap sort, and insertion sort. Java (Java, 2023) and Python up to version\n3.10 (Peters, 2002) use TimSort (McIlroy, 1993), an improved version of merge sort. Python 3.11 and later\nuse Powersort (Munro & Wild, 2018), a merge sort that determines a near-optimal merge order.\nSample sort (Frazer & McKellar, 1970) extends QuickSort by using multiple pivots instead of one. Sample\nsort samples a small number of keys from the array, determines multiple pivots, and uses them to parti-\ntion the array into multiple buckets. The partitioning is repeated recursively until the array is sufficiently\nsmall. Among its implementations, the in-place parallel superscalar sample sort (Axtmann et al., 2017) was\nintroduced as a highly efficient parallel algorithm, and later engineering refinements further improved its\nperformance (Axtmann et al., 2022). Its computational and cache efficiency is theoretically guaranteed by a\ntheorem about the probability that the pivots partition the array (nearly) equally.\n2.3 Non-Comparison Sorts\nNon-comparison sorts use information other than key comparison. Radix sort and counting sort are the\nmost common types of non-comparison sorts. Radix sort uses counting sort as a subroutine for each digit.\nWhen the number of digits in the array element isw, the computational complexity of radix sort isO(wn).\nThus, radix sort is particularly effective when the number of digits is small. There are several variants of\nradix sort, such as Spreadsort (Ross, 2002), which integrates the advantages of comparison sort into radix\nsort and is implemented in the Boost C++ Libraries, and RegionSort (Obeya et al., 2019), which enables\nefficient parallelization by modeling and resolving dependencies among the element swaps.\nIn addition, non-comparison sorting algorithms tailored for specific data types have been developed. For\ninteger arrays, a deterministic algorithm with worst-case complexity ofO(nlog logn)(Han, 2002) and a ran-\ndomized algorithm with expected complexity ofO(n‚àölog logn)(Han & Thorup, 2002) have been proposed.\nFor real-valued arrays, recent advances have led to the development of a sorting algorithm with a worst-case\ncomplexity ofO(n‚àölogn)(Han, 2020). Our PCF Learned Sort also targets real-valued arrays and, under\nmild assumptions on the distribution, achieves an expected complexity ofO(nlog logn). Moreover, when\nthe algorithm of Han (2020) is used as the internal sorting component, PCF Learned Sort has a worst-case\ncomplexity ofO(n‚àölogn), which matches that of Han (2020).\n2.4 Learned Index\nKraska et al. (2018) showed that index data structures such as B-trees and Bloom filters can be made\nfaster or more memory efficient by combining them with machine learning models and named such novel\ndata structures Learned Index. Since then, learning augmented B-trees (Wang et al., 2020; Kipf et al., 2020;\nFerragina & Vinciguerra, 2020; Zeighami & Shahabi, 2023), Bloom filters (Mitzenmacher, 2018; Vaidya et al.,\n2021; Sato & Matsui, 2023; 2024), and even range-minimum query structures (Ferragina & Lari, 2025) have\nbeen proposed. There are several works on learning augmented B-trees whose performance is theoretically\nguaranteed. PGM-index (Ferragina & Vinciguerra, 2020) is a learning augmented B-tree that is guaranteed\nto have the same worst-case query complexity as the classical B-tree, i.e.,O(logn). Zeighami & Shahabi\n(2023) proposed a learning augmented B-tree with an expected query complexity ofO(log logn)under mild\nassumptions on the distribution. More broadly, this line of research connects to earlier studies on compressed\ndata structures with provable guarantees, such as the foundational work of Sadakane (2007).\n3 Methods\nThis section introduces ourLearned Sort frameworkand its PCF-based instantiation. In Section 3.1, we set\nup the notation, in Section 3.2 we present the general framework, and in Section 3.3 we instantiate it with\na PCF-based CDF model to obtain concrete complexity guarantees.\n3\n\nPublished in Transactions on Machine Learning Research (10/2025)\n3648593822477443342421566223805750457140\n362234242123\n3847434540\n485056575962717480\n485974566280575071\n384340\n4745\n222123\n363424\n384043\n4547\n212223\n243436\n2122232434363840434547485056575962717480\nstandard sortif |bucket| <ùùâstandard sortif |bucket| ‚â•ùúπ\nRecursive model-based bucketing until |bucket| <ùùâ\nConcatenate\nCDF Model\nCDF Model\nCDF Model\n010.55040607030ùëûùêπùíôùëû#ùêπ(ùëû)CDF Model(Piecewise Constant Function)\n: Output of CDF model: Empirical CDF\nùíô ùíÑ! ùíÑ\" ùíÑ#$! \nùíô%&'()* \nFigure 1: PCF Learned Sort: First, the input array is partitioned intoŒ≥+ 1buckets using a CDF model-\nbased method. Buckets larger thanŒ¥or smaller thanœÑare sorted with a standard sort (e.g., IntroSort\nor QuickSort). Otherwise, the recursive model-based bucketing is repeated. Finally, the sorted arrays are\nconcatenated. The CDF model used for bucketing is a Piecewise Constant Function (PCF). The function is\nconstant within each interval, and the interval widths are constant.\n3.1 Notation and Setup\nFollowing the learned-sort paradigm Kraska et al. (2019); Kristo et al. (2020; 2021a), our learned sort\nalgorithmrecursivelyappliesmodel-based bucketing. Model-basedbucketingpartitionsthearrayintomultiple\nbuckets using a CDF model trained on the input so that the bucket with the larger ID gets the larger value.\nLetD‚äÜRbe the domain of possible key values andx‚ààDnbe the input array. For vector/arrayv, we\nwrite|v|for its length (number of elements), e.g.,|x|=n. If|x|is smaller than a fixed thresholdœÑ‚ààN,\nthe algorithm falls back to a classic sorting algorithm. We refer to this fallback classical sorting algorithm\nas thestandard sort. As the standard sort, we can use any sorting algorithm, such as IntroSort, QuickSort,\nor the algorithm of Han (2020).\nFor model-based bucketing, we define functionsŒ±(n),Œ≤(n),Œ≥(n),Œ¥(n) :N‚ÜíNas follows:Œ±(n)is the number\nof samples used to train the CDF model,Œ≤(n)is the number of PCF bins (a hyperparameter; see Section 3.3),\nŒ≥(n) + 1is the number of buckets, andŒ¥(n)is the threshold that determines the behavior after bucketing\n(see Section 3.2). For brevity, we writeŒ±,Œ≤,Œ≥,Œ¥.\n3.2 Learned-Sort Framework\nAlgorithm description.At a high level, our algorithm is a recursive model-based bucketing scheme aug-\nmented with an exception-handling mechanism for bucketing ‚Äúfailures.‚Äù This exception-handling mechanism\ngreatly simplifies the analysis of both expected and worst-case complexity. The overall workflow is visualized\nin Figure 1, and the pseudocode is given in Algorithm 1.\nIf the length of the input array is less thanœÑ, our algorithm sorts the input array using a standard sort (e.g.,\nIntroSort or QuickSort). Otherwise, model-based bucketing is performed.\nThe model-based bucketing methodMtakes an input arrayxand partitions it into several buckets. First,\nall or some elements ofxare used to train the CDF model ÀúF:D‚Üí[0,1]. The ÀúF(q)is trained to approximate\nthe empirical CDFF x(q) =|i‚àà{1,...,n}|x i‚â§q|/|x|. Any non-decreasing model can serve as the CDF\nmodel ÀúF(q)(e.g., linear models, monotonic MLPs, or the PCF introduced in Section 3.3).\n4\n\nPublished in Transactions on Machine Learning Research (10/2025)\nAlgorithm 1The Learned Sort algorithm\n1:Input:x‚ààRnOutput:x sorted‚ààRn(the sorted version ofx)\n2:Subroutines:\n3:Standard-Sort(x): The standard sort with complexity guarantees (e.g, IntroSort or QuickSort)\n4:CDF-Model(x): Instantiate a CDF Model ÀúF(q)that estimatesF x(q)\n5:functionLearned-Sort(x)\n6:n‚Üê|x|\n7:ifn<œÑthen‚ñ∑Small bucket\n8:returnStandard-Sort(x)\n9: ÀúF(q)‚ÜêCDF-Model(x)‚ñ∑Model-based bucketing\n10:c 1‚Üê[ ],c 2‚Üê[ ], ... ,c Œ≥+1‚Üê[ ]\n11:fori= 1,2,...,n\n12:j‚Üê/floorleftbigÀúF(xi)Œ≥/floorrightbig\n+ 1\n13:c j.Append(x i)\n14:forj= 1,2,...,Œ≥+ 1‚ñ∑Recursively sort and concatenate\n15:if|c j|‚â•Œ¥then\n16:c j‚ÜêStandard-Sort(c j)\n17:else\n18:c j‚ÜêLearned-Sort(c j)\n19:x sorted‚ÜêConcatenate(c 1,c2,...,cŒ≥+1)\n20:returnx sorted\nAfter training, the CDF model is used to partition the input arrayxintoŒ≥+ 1buckets. AllŒ≥+ 1buckets,\n{cj}Œ≥+1\nj=1, are initialized to be empty, and then for eachi‚àà {1,...,n},x iis appended toc‚åäÀúF(xi)Œ≥‚åã+1.\nThis is based on the intuition that the number of elements less than or equal tox iin the arrayx(i.e.,\n|{j‚àà{1,...,n}|x j‚â§xi}|) is approximately equal ton ÀúF(xi).\nWe restrict the CDF model ÀúFto non-decreasing functions to ensure that the bucket with the larger ID gets\nthe larger value, i.e.,p‚ààc j‚àßq‚ààck‚àßj <k‚áíp<q. This means that each bucket is responsible for a\ndisjoint and continuous interval. Lett j= minx‚ààcjx(j= 1,...,Œ≥+ 1), t Œ≥+2=‚àû, then thej-th bucketc j\n(j= 1,...,Œ≥+ 1) is responsible for a continuous intervalI j:= [tj,tj+1).\nAfter model-based bucketing, our algorithm determines for each bucket whether the bucketing ‚Äúsucceeds‚Äù or\n‚Äúfails.‚Äù For eachj‚àà{1,...,Œ≥+ 1}, we check whether the size of bucketc jis less thanŒ¥. If|c j|‚â•Œ¥(which\nmeans the bucketing ‚Äúfails‚Äù), the bucket is sorted using the standard sort (e.g., IntroSort or QuickSort). If\n|cj|< Œ¥(which means the bucketing ‚Äúsucceeds‚Äù), the bucket is sorted by recursively calling our Learned\nSort algorithm. Note that the parameters such asŒ≥andŒ¥are redetermined for each recursion according to\nthe size of the bucket (i.e., the input array in the next recursion step), and the CDF model is retrained for\neach bucket. Finally, the sorted buckets are concatenated to form the sorted array.\nWorst-case complexity.The following lemma guarantees the worst-case complexity of our Learned Sort.\nLemma 3.1.Assume that there exists a model-based bucketing algorithmMsuch thatMcan perform\nbucketing (including model training and inferences) an array of lengthnintoŒ≥+ 1 =O(n)buckets with a\nworst-case complexity ofO(n). Also, assume that the standard sort has a worst-case complexity ofO(nU(n)),\nwhereU(n)is a non-decreasing function. Then, the worst-case complexity of our Learned Sort with suchM\nandŒ¥=‚åänd‚åã(wheredis a constant satisfying0<d<1) isO(nU(n) +nlog logn).\nThis lemma can be intuitively shown from the following two points: (1) the maximum recursion depth is\nO(log logn), and (2) each element of the input arrayxundergoes several bucketing and only one standard\nsort. The first point can be shown from the fact that the size of the bucket in thei-th recursion depth is\nless thanndi. The second point is evident from the algorithm‚Äôs design since the buckets sorted by standard\nsort are now left only to be concatenated. The exact proof is given in Section A.1.\n5\n\nPublished in Transactions on Machine Learning Research (10/2025)\nNote that this guarantee critically relies on the exception-handling mechanism: without it, the recursion\ndepth could reachŒò(n)and the worst-case time would degrade toŒò(n2), whereas with it the recursion\ndepth is always bounded byO(log logn).\nExpected complexity.Next, we introduce a lemma about the expected computational complexity of our\nLearned Sort. The following assumption is necessary to guarantee the expected computational complexity.\nAssumption 3.2.The input arrayx‚ààDnis formed by independent sampling according to a probability\ndensity distributionf(x):D‚ÜíR ‚â•0.\nWe definefI(x):I‚ÜíR‚â•0to be the conditional probability density distribution off(x)under the condition\nthatx‚ààIfor a intervalI‚äÜD, i.e.,f I(x):=f(x)/integraltext\nIf(y)dy.\nThe expected computational complexity of our proposed Learned Sort is guaranteed by the following lemma.\nLemma 3.3.Letx I(‚ààIn)be the array formed by samplingntimes independently according tof I(x).\nAssume that there exist a model-based bucketing algorithmMand a constantd(‚àà(0,1))that satisfy the\nfollowing three conditions for any intervalI(‚äÜD): (i)Mcan perform bucketing (including model training\nand inferences) on an array of lengthn, with an expected complexity ofO(n), (ii)Œ≥+ 1 =O(n), and (iii)\nPr[‚àÉj,|cj|‚â•‚åänd‚åã] =O/parenleftÔ£¨ig\n1\nlogn/parenrightÔ£¨ig\n. Also, assume that the standard sort has an expected complexity ofO(nlogn).\nThen, the expected complexity of our Learned Sort with suchMandŒ¥=‚åänd‚åãisO(nlog logn).\nThis lemma can be proved intuitively by the following two points: (1) the maximum recursion depth is\nO(log logn), and (2) the expected total computational complexity from thei-th to the(i+ 1)-th recursion\ndepth isO(n). The first point is the same as in the explanation of the proof of Theorem 3.1. The second\npoint holds because the expected computational complexity from thei-th to the(i+ 1)-th recursion depth\nisO(nlogn)with probabilityO(1\nlogn), andO(n)in other cases. See Section A.2 for the exact proof.\nNote that the assumption of Theorem 3.3 includes ‚ÄúMworks well with high probability for anyI(‚äÜD).‚Äù\nThis is because our Learned Sort algorithm recursively repeats the model-based bucketing. The range of\nelements in the bucket, i.e., the input array in the next recursion step, can be any intervalI(‚äÜD).\n3.3 PCF Learned Sort\nWe now instantiate the framework withPCF Learned Sort, which satisfies the assumptions of Theorem 3.1\nand Theorem 3.3, thereby providing both worst-case and expected-time guarantees. PCF Learned Sort\napproximates the CDF using aPiecewise Constant Function (PCF)withŒ≤equal-width bins; the model\noutput is constant within each bin (see the right panel of Figure 1). The study that develops a Learned\nIndex with a theoretical guarantee on its complexity (Zeighami & Shahabi, 2023) also used PCF as a CDF\nmodel. While our framework admits more expressive CDF models (such as spline-based or neural models),\nwe focus on PCF due to its minimal training and inference cost and the tractability of its theoretical analysis.\nThe model-based bucketing method in PCF Learned SortM PCFtrains the CDF model ÀúFas follows. The\nparametersŒ±‚àà{1,...,n}andŒ≤‚ààNare determined byn, whereŒ±is the number of samples used to train\nthe model andŒ≤is the number of intervals in the PCF. The PCF is trained by counting the number of\nsamples in each interval. We definei(x) =‚åä(x‚àíx min)Œ≤/(x max‚àíxmin)‚åã+ 1, wherex min= minixiand\nxmax= maxixi. Fromx,Œ±samples are taken to forma‚ààDŒ±, andi(x)is used to constructb‚ààZŒ≤+1\n‚â•0with\nbi=|{j‚àà{1,...,Œ±}|i(a j)‚â§i}|. This counting procedure trains the PCF. Note thatbis an non-decreasing\nnon-negative array andb Œ≤+1=Œ±, i.e.,0‚â§b 1‚â§b2‚â§¬∑¬∑¬∑‚â§b Œ≤+1=Œ±.\nInference for ÀúF(x)is then given by ÀúF(x) =bi(x)\nŒ±. Sincei(x)andbare non-decreasing, ÀúF(x)is also non-\ndecreasing. Also,0‚â§ ÀúF(x)‚â§1because0‚â§b i‚â§Œ±for everyi.\nThe following is a lemma to bound the probability thatM PCFwill ‚Äúfail‚Äù bucketing. This lemma is important\nto guarantee the expected computational complexity of PCF Learned Sort.\nLemma 3.4.LetœÉ 1andœÉ 2be respectively the lower and upper bounds of the probability density distribution\nf(x)inD, and assume that0<œÉ 1‚â§œÉ2<‚àû. That is,x‚ààD ‚áíœÉ 1‚â§f(x)‚â§œÉ 2. Then, in model-based\n6\n\nPublished in Transactions on Machine Learning Research (10/2025)\nbucketing ofx I(‚ààIn)to{cj}Œ≥+1\nj=1usingM PCF, the following holds for any intervalI(‚äÜD):\nK:=Œ≥Œ¥\n2n‚àí2œÉ2Œ≥\nœÉ1Œ≤‚â•1‚áíPr[‚àÉj,|c j|>Œ¥]‚â§2n\nŒ¥exp/braceleftÔ£¨igg\n‚àíŒ±K\n2Œ≥/parenleftbigg\n1‚àí1\nK/parenrightbigg2/bracerightÔ£¨igg\n.(1)\nThe proof of this lemma is based on and combines proofs from two existing studies. The first is Lemma\n5.2. from a study of IPS4o (Axtmann et al., 2022), an efficient sample sort. This lemma guarantees the\nprobability of a ‚Äúsuccessful recursion step‚Äù when selecting pivots from samples and using them to perform a\npartition. This lemma is for the method that does not use the CDF model, so the proof cannot be applied\ndirectly to our case. Another proof we refer to is the proof of Lemma 4.5. from a study that addressed\nthe computational complexity guarantee of the Learned Index (Zeighami & Shahabi, 2023). This lemma\nprovides a probabilistic guarantee for the error between the output of the PCF and the empirical CDF.\nSome modifications are required to adapt it to the context of sorting and to attribute it to the probability\nof bucketing failure, i.e.,Pr[‚àÉj,|c j|> Œ¥]. By appropriately combining the proofs of these two lemmas,\nTheorem 3.4 is proved. The exact proof is given in Section A.3.\nHere, we emphasize that the assumption of this lemma,0< œÉ 1‚â§œÉ 2<‚àû, is sufficiently reasonable and\n‚Äúmild‚Äù as described in (Zeighami & Shahabi, 2023). It asserts that the probability density functionf(x)is\nboth bounded and nonzero over its domainD. This class of distributions covers the majority of real-world\nscenarios because real-world data is commonly derived from bounded and continuous phenomena, e.g., age,\ngrades, and data over a period of time. Empirically, Zeighami & Shahabi (2023) further suggest that the\nratioœÉ 2/œÉ1tends to remain close to 1 in a wide variety of practical datasets, and even for more challenging\ncases like OSM, the ratio still appears to remain at most around 20.\nUsing Theorem 3.1, Theorem 3.3, and Theorem 3.4, we can prove the following theorems.\nTheorem 3.5.IfM PCFis the bucketing method, the worst-case complexity of standard sort isO(nU(n))\n(whereU(n)is a non-decreasing function), andŒ±=Œ≤=Œ≥=Œ¥=‚åän3/4‚åã, then the worst-case complexity of\nPCF Learned Sort isO(nU(n) +nlog logn).\nProof.WhenŒ±=Œ≤=Œ≥=‚åän3/4‚åã, the computational complexity for model-based bucketing isO(n)because\n(i) the PCF is trained inO(Œ±+Œ≤) =O(n3/4), and (ii) the total complexity of inference fornelements is\nO(n), since the inference is performed inO(1)per element. Therefore, sinceŒ≥+ 1 =O(n), the worst-case\ncomplexity of standard sort isO(nU(n)), andŒ¥=‚åän3/4‚åã, we can prove the worst-case complexity of PCF\nLearned Sort isO(nU(n) +nlog logn)by Theorem 3.1.\nTheorem 3.6.LetœÉ 1andœÉ 2be the lower and upper bounds, respectively, of the probability density distri-\nbutionf(x)inD, and assume that0<œÉ 1‚â§œÉ2<‚àû. Then, ifM PCFis the bucketing method, the expected\ncomplexity of standard sort isO(nlogn), andŒ±=Œ≤=Œ≥=Œ¥=‚åän3/4‚åã, then the expected complexity of PCF\nLearned Sort isO(nlog logn).\nProof.WhenŒ±=Œ≤=Œ≥=‚åän3/4‚åã, the computational complexity for model-based bucketing isO(n). Since\nK= ‚Ñ¶(‚àön)whenŒ±=Œ≤=Œ≥=Œ¥=‚åän3/4‚åã,K‚â•1for sufficiently largen, and\n2n\nŒ¥exp/braceleftÔ£¨igg\n‚àíŒ±K\n2Œ≥/parenleftbigg\n1‚àí1\nK/parenrightbigg2/bracerightÔ£¨igg\n=O(n1\n4exp(‚àí‚àön))‚â§O/parenleftbigg1\nlogn/parenrightbigg\n.(2)\nGiven thatŒ≥+ 1 =O(n), the expected complexity of standard sort isO(nlogn), andŒ¥=‚åän3/4‚åã, it follows\nfrom Theorem 3.3 and Theorem 3.4 that the expected complexity of PCF Learned Sort isO(nlog logn).\nNote that the exact value ofœÉ 1andœÉ 2is not required to run PCF Learned Sort since the parameters for\nthis algorithm, i.e.,Œ±,Œ≤,Œ≥, andŒ¥, are determined without any prior knowledge. In other words, PCF\nLearned Sort can sort in expectedO(nlog logn)complexity as long as0<œÉ 1‚â§f(x)‚â§œÉ 2<‚àû, even if it\ndoes not know the exact value ofœÉ 1andœÉ 2. IfœÉ 1andœÉ 2do not satisfy the assumption of Theorem 3.6,\nthe expected complexity of PCF Learned Sort increases toO(nU(n) +nlog logn). Such scenarios arise, for\n7\n\nPublished in Transactions on Machine Learning Research (10/2025)\ninstance, in heavy-tailed distributions, datasets with extremely sparse regions (œÉ 1= 0), or cases where the\ndensity is highly concentrated at particular values (œÉ 2=‚àû). However, thanks to the worst-case bound in\nTheorem 3.5, the complexity never exceedsO(nU(n)+nlog logn). Thus, the algorithm remains robust even\nunder distributions that do not fully satisfy the assumption.\nAlternative CDF Models.Our framework is not limited to PCF; it can also accommodate more expres-\nsive CDF models. For instance, we can replace PCF with a spline-based model that approximates the CDF\nby interpolating the empirical distribution at bin boundaries. Concretely, givenŒ≤intervals, we evaluate the\nempirical CDF at the endpoints of each interval and then construct a piecewise linear spline that connects\nthese values, yielding a continuous and non-decreasing CDF approximation. We refer to the algorithm that\napplies this spline-based CDF model within our Learned Sort framework asSpline Learned Sort. We show\nthat spline-based models constructed in this way still satisfy the assumptions of Theorem 3.3, and thus the\nexpected time complexity guarantee ofO(nlog logn)remains valid. A detailed theoretical proof for the\nspline-based case is provided in Section A.5.\n4 Experiments\nIn this section, we empirically validate our theoretical results. First, in Section 4.1, we confirm that the\ncomplexity of PCF Learned Sort isO(nlog logn)for both synthetic and real datasets. Then, in Section 4.2,\nwe conduct experiments with various parameter settings and empirically confirm Theorem 3.4, a lemma that\nbounds the probability of bucketing failure and plays a crucial role in guaranteeing the expected complexity\nof PCF Learned Sort. Finally, in Section 4.3, we present from sorting time measurements.\nSetup.We experimented with synthetic datasets created from the following four distributions: uniform\n(min= 0,max= 1), normal (¬µ= 0,œÉ= 1), exponential (Œª= 1), lognormal (¬µ= 0,œÉ= 1). The input array\nwas generated by independently takingnsamples from each distribution. Only the uniform distribution\nsatisfies the theoretical assumptions required for our complexity guarantees. The other distributions violate\nthese assumptions, but we include them to evaluate the empirical robustness of PCF Learned Sort.\nWe also used the following 16 real datasets, includingChicago [Start, Tot](Chicago, 2021),NYC\n[Pickup, Dist, Tot](nyc, 2020),SOF [Humidity, Pressure, Temperature](Mavrodiev, 2019),Wiki,\nOSM,Books,Face(Marcus et al., 2020), andStocks [Volume, Open, Date, Low](Onyshchak, 2020).\nFurther dataset details are provided in Appendix B. For each dataset, we randomly samplenelements,\nshuffle them, and use them as an input array to examine the complexity of the sort algorithms. Since these\nare real-world datasets, we cannot definitively determine whether they satisfy our theoretical assumptions.\nHowever, as shown in the histograms in Figure 2, Chicago [Start], NYC [Pickup, Tot], SOF [Humidity], and\nFace appear to distribute values across a relatively dense and continuous domain, aligning well with our as-\nsumptions. In contrast, other datasets exhibit long tails or sparse regions, suggesting that our assumptions\nmay not hold. Furthermore, note that for several datasets (Chicago [Start, Tot], NYC [Dist, Tot], SOF\n[Humidity, Pressure, Temperature], and Stocks [Volume, Open, Date, Low]), the number of unique values is\nextremely small (less than 3.2% of the total elements, as detailed in Appendix B).\nAll experiments were run on a Linux machine equipped with an Intel¬ÆCore‚Ñ¢i9-11900H CPU @ 2.50GHz\nand 62GB of memory. GCC version 9.4.0 was used for compilation, employing the-O3optimization flag.\n4.1 Computational Complexity of PCF Learned Sort\nWe meticulously counted the total number of basic operations for sorting the input array to observe the\ncomputational complexity of each sorting algorithm. Here, the basic operations consist of four arithmetic\noperations, powers, comparisons, logical operations, assignments, and memory access. We chose this metric,\nwhich counts basic operations, to mitigate the environmental dependencies observed in other metrics, such as\nCPU instructions and CPU time, which are heavily influenced by compiler optimizations and the underlying\nhardware. This is the same idea as the metric selection in the experiment of (Zeighami & Shahabi, 2023).\n8\n\nPublished in Transactions on Machine Learning Research (10/2025)\nFigure2: Numberofoperationstosortthearray. Beloweachgraphisahistogramvisualizingthedistribution\nof each dataset. The standard deviation of the10measurements is represented by the shaded area. Our\nPCF Learned Sort consistently achieves a complexity lower thanO(nlogn), while Learned Sort 2.0 (Kristo\net al., 2021a), which hasO(n2)worst-case complexity, occasionally requires huge operations.\nThe parameters of PCF Learned Sort are set as in Theorems 3.5 and 3.6,Œ±=Œ≤=Œ≥=Œ¥=‚åän3/4‚åãand\nœÑ= 100. As the standard sort, we used QuickSort, which has an expected complexity ofO(nlogn). For\nease of implementation and straightforward measurement of comparison counts, we chose QuickSort. We\ncompared our PCF Learned Sort against (plain) QuickSort, radix sort, and Learned Sort 2.0 (Kristo et al.,\n2021b). Learned Sort 2.0 was selected as the learned-method baseline because it allows relatively simple\ncounting of operations (a broader comparison appears in the sorting time experiments of Section 4.3).\nFigure 2 shows the number of operations divided by the length of the input array,n. Each point represents\nthe average over 10 runs, with the shaded region indicating the standard deviation. Note that the horizontal\naxis is logarithmic. As a result, the curve for QuickSort (withO(nlogn)complexity) appears approximately\nlinearinthisplotforsyntheticdatasetsandreal-worlddatasetswithfewduplicates(i.e., NYC[Pickup], Wiki,\nOSM, Books, and Face). In contrast, the curve for PCF Learned Sort is nearly flat, suggesting a complexity\nsignificantly lower thanO(nlogn). PCF Learned Sort consistently requires the fewest operations across all\nconditions, achieving up to 2.8 times fewer operations than QuickSort. These results confirm not only our\ntheoretical analysis but also the robustness of PCF Learned Sort under assumption-violating scenarios.\n9\n\nPublished in Transactions on Machine Learning Research (10/2025)\nùëéùëéùëéùëèùëèùëêùëèùëêùëëùëêùëëùëëùëê=0.75,ùëë=0.75ùëè=0.75,ùëë=0.75ùëè=0.75,ùëê=0.75ùëé=0.75,ùëë=0.75ùëé=0.75,ùëê=0.75ùëé=0.75,ùëè=0.75Pr‚àÉùëó,ùíÑ!‚â§ùõø\nFigure 3: Heatmap showing the empirical frequency of bucketing failure, i.e.,‚àÉj,|c j|> Œ¥. The variables\na,b,c,d, except those on the x- and y-axes, were set to0.75. The white dotted line represents the parameters\nthat make the right side of Equation (1) equal to 0.5. The close alignment between this white dotted line\nand the actual success/fail boundery suggests the theoretical bound by is Equation (1) reasonably tight.\nFor datasets with many duplicates, QuickSort, Learned Sort 2.0 (Kristo et al., 2021a), and PCF Learned\nSort all exhibit relatively few operations. This behavior arises because highly duplicated datasets often result\nin buckets containing only one distinct value, allowing the algorithms to terminate early. Even under such\nconditions, PCF Learned Sort performs fewer operations than the other algorithms.\nThe curve for radix sort is also flat but lies consistently above that of PCF Learned Sort. This is due to the\ndifference in partitioning strategies. While the radix sort partitions at a fixed granularity, our PCF Learned\nSort performs partitioning using intervals that are adaptively set by the learning model.\nIn particular, on the SOF [Temperature] dataset with medium input sizes (2√ó105<n‚â§2√ó107), Learned\nSort 2.0 incurs extremely high costs: while all other methods keep the number of operations pernbelow\n100, it can exceed50,000. This shows that Learned Sort methods without worst-case guarantees may suffer\nfrom pathological overhead. By contrast, our method maintains both expected and worst-case guarantees,\nensuring robust performance across diverse datasets.\n4.2 Confirmation of Theorem 3.4\nTheorem 3.4 bounds the probability that a bucket of size greater thanŒ¥exists. This is an important\nlemma that allows us to guarantee the expected computational complexity of PCF Learned Sort. Here, we\nempirically confirm that this upper bound is appropriate.\nWe have experimented withŒ±=‚åäna‚åã,Œ≤=‚åänb‚åã,Œ≥=‚åänc‚åã,Œ¥=‚åänd‚åã, varyinga,b,c,dfrom 0.05 to 0.95 at\n0.05 intervals. For eacha,b,c,dsetting, the following was repeated100times: we tookn= 106elements\nfrom the uniform distribution to form the input array and divided the array intoŒ≥+ 1buckets byM PCF,\nand checked whether or not‚àÉj,|c j|> Œ¥. Thus, for eacha,b,c,d‚àà{0.05,0.10,...,0.95}, we obtained the\nempirical frequency at which bucketing ‚Äúfails.‚Äù\nHeat maps in Figure 3 show the empirical frequency of bucketing failures when two of thea,b,c,dparameters\nare fixed, and the other two parameters are varied. The values of the two fixed variables are set to 0.75,\ne.g., in the upper left heap map of Figure 3 (horizontal axis isaand vertical axis isb),c=d= 0.75. The\nwhite dotted line represents the parameter so that the right side of Equation (1) is 0.5. That is, Theorem 3.4\nasserts that ‚Äúin the region upper right of the white dotted line, the probability of bucketing failure is less\nthan 0.5.‚Äù\nWeobservethatthewhitedottedlineiscloseto(orslightlytotheupperrightof)theactualboundofwhether\nbucketing ‚Äúsucceeds‚Äù or ‚Äúfails‚Äù more often. This suggests that the theoretical upper bound from Theorem 3.4\nagrees well (to some extent) with the actual probability. We can also confirm that, as Theorem 3.4 claims,\nthe probability of bucketing failure is indeed small in the region upper right of the white line.\n4.3 Experiments on Sorting Time\nWe empirically compare the sorting time of our PCF Learned Sort against several baselines. Note that\nthe metric used in Section 4.1, the number of operations, does not change depending on the machine or\ncompilation method, but the sorting time does. As the standard sort used in PCF Learned Sort, we used\n10\n\nPublished in Transactions on Machine Learning Research (10/2025)\nFigure4: Timetosortthearray. Thestandarddeviationofthe10measurementsisrepresentedbytheshaded\narea. Our PCF Learned Sort is significantly faster thanstd::sort, and more importantly, it maintains\nrobust performance across all datasets, whereas other learned sorts without worst-case guarantees can suffer\ncatastrophic slowdowns, as seen with Learned Sort 2.0 on the SOF [Temperature] dataset.\nstd::sort, which has a worst-case complexity ofO(nlogn). We compared our PCF Learned Sort with\nstd::sort, radix sort,boost::sort::spreadsort::float_sort(Boost C++ implementation of Spread-\nsort (Ross, 2002)), and Learned Sort 2.0 (Kristo et al., 2021b). In addition, we evaluated more recent\nstate-of-the-art learned sorting algorithms (Ferragina & Odorisio, 2025)‚ÄîBalanced Learned Sort (BLS),\nUnbalanced Learned Sort (ULS), and Learned Sort 2.1‚Äîas well as IS4o (Axtmann et al., 2017), one of the\nstate-of-the-art non-learned sequential sample sort algorithms.\nFigure 4 shows the sorting time divided byn. It shows the mean and standard deviation of the10mea-\nsurements for each condition. Note that the horizontal axis is logarithmic, and therefore the curve of\nstd::sort, which has a complexity ofO(nlogn), is almost linear in this plot for synthetic datasets and\nreal-world datasets with few duplicates. In contrast, our PCF Learned Sort graph shows a relatively slow\nincrease, suggesting that PCF Learned Sort has a complexity much smaller thanO(nlogn). We see that our\nPCF Learned Sort is up to 2.5 times faster thanstd::sort. Furthermore, we find that for relatively large\nn(>106), our PCF Learned Sort usually outperforms not only radix sort but also Spreadsort, an algorithm\nthat cleverly incorporates the advantage of comparison sort into radix sort.\nThe figure also shows that highly optimized methods like IS4o and other learned sorts often outperform PCF\nLearned Sort in average speed. This is because these methods are highly optimized implementations that\nconsider factors like cache efficiency, whereas our implementation prioritizes providing rigorous theoretical\nguarantees. However, this speed comes at the cost of robustness: learned sorts without worst-case guarantees\ncansuffercatastrophicslowdowns. Forexample, ontheSOF[Temperature]datasetwithn= 2√ó107, Learned\nSort 2.0 took up to 326.4 seconds, while our method consistently finished in 0.45 seconds. Moreover, as shown\nin our adversarial analysis (Section C), other recent learned sorts (BLS, ULS, Learned Sort 2.1) also exhibit\nvulnerabilities. These results highlight the practical value of our theoretical guarantees.\n11\n\nPublished in Transactions on Machine Learning Research (10/2025)\nTo better understand runtime behavior, we profiled PCF Learned Sort across its stages (Section D). The\nresults show that while the training stage is consistently lightweight across datasets and input sizes, the\nbucketing stage often becomes the bottleneck. The dominant cost varies depending on bothnand the\ndataset characteristics, suggesting opportunities for further optimization.\n5 Discussion\nComparison with (Zeighami & Shahabi, 2024).Most recently, a concurrent work (Zeighami & Sha-\nhabi, 2024) introduced a theoretical framework for learned database operations, including sorting, indexing,\nandcardinalityestimation. Akeystrengthoftheirapproachistheformaldefinitionof‚Äúdistributionlearnabil-\nity‚Äù and its applicability to a broad class of distributions, including those subject to distribution shifts. Using\nthis framework, they developed a Learned Sort algorithm with an expected running time ofO(nlog logn)\nunder certain distributional assumptions. While their approach employs a bucketing-based sorting algorithm\nsimilar to ours, there are several key differences between their method and ours.\nFirst, their algorithm relies on detailed prior knowledge about the distribution; it explicitly requires a\nparameterŒ∫ 2, which represents the ‚Äúlearning possibility‚Äù of the distribution (see Definition 3.2 in (Zeighami\n& Shahabi, 2024) for details). SinceŒ∫ 2depends onœÅ 1andœÅ 2, their algorithm cannot be executed without\nknowing the values ofœÅ 1andœÅ 2(or at least their lower and upper bounds). In contrast, our algorithm does\nnot require these values, making it more widely applicable.\nSecond, they do not provide experimental results. Since estimatingœÅ 1andœÅ 2from real data is challenging,\nempirical evaluation of their method is difficult. On the other hand, since our algorithm does not depend\non these specific values, it is easy to implement and evaluate experimentally.\nFinally, their algorithm lacks a worst-case complexity guarantee. In particular, there are cases where the\nalgorithm may not terminate, making it impossible to give an upper bound on its worst-case complexity. In\ncontrast, we provide a formal worst-case complexity guarantee.\nLimitations of the Current Theoretical Framework.Our proof of theO(nlog logn)expected com-\nplexity (Theorem 3.6), does not extend to distributions withf(x) = 0or‚àû. A similar limitation is observed\nin Learned Indexes (Zeighami & Shahabi, 2023), and extending the theory to cover a broader class of dis-\ntributions remains an open direction in both Learned Indexes and Learned Sorts contexts. One promising\ndirection is to integrate more advanced CDF approximation methods with theoretical guarantees into our\nLearned Sort algorithm. By adopting a refined CDF model and a bucketing algorithm that satisfies the\nconditions of Theorem 3.3, it may be possible to achieve stronger theoretical guarantees.\nImplementation Considerations and Optimizations.A straightforward implementation of PCF\nLearned Sort is not in-place, as it requires an auxiliary buffer for buckets nearly as large as the input.\nEngineering techniques from highly optimized sample-sort implementations (Axtmann et al., 2017; 2022)\nsuggest in-place variants without changing the asymptotic structure of our algorithm. Another important\naspect is real-world efficiency: optimizing memory access patterns to enhance cache efficiency, reducing cache\nmisses, and leveraging cache-aware data layouts could improve empirical performance without compromising\ntheoretical guarantees. Dynamically tuning parameters such asŒ±,Œ≤,Œ≥, andŒ¥based on the input distribution\nmay yield further performance gains while maintaining guarantees.\nParallelization Potential.Although our analysis targets the sequential setting, the structure of PCF\nLearned Sort admits parallelization at several stages: training the CDF model, computing bucket IDs, scat-\ntering into buckets, and sorting buckets are all amenable to data parallelism. This observation is consistent\nwith prior engineering work that integrates learned partitioning with high-performance (in-place) sample-\nsort pipelines and parallel learned-sorting frameworks (Axtmann et al., 2017; Carvalho, 2022; Carvalho &\nLawrence, 2023). These results complement our theoretical analysis and suggest opportunities for developing\nparallel learned-sorting algorithms with provable guarantees.\n12\n\nPublished in Transactions on Machine Learning Research (10/2025)\n6 Conclusion\nWeproposedPCFLearnedSort, whichprovidesguaranteesonbothitsexpectedandworst-casecomplexities.\nWe then confirmed these computational complexities empirically on both synthetic and real data. This is\nthe first study to support the empirical success of Learned Sort theoretically and provides insight into why\nLearned Sort is fast.\n13\n\nPublished in Transactions on Machine Learning Research (10/2025)\nReferences\nTlc trip record data, 2020. URLhttps://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page.\nAntonios Antoniadis, Christian Coester, Marek Eli√°≈°, Adam Polak, and Bertrand Simon. Online metric\nalgorithms with untrusted predictions.ACM Transactions on Algorithms, 2023.\nMichael Axtmann, Sascha Witt, Daniel Ferizovic, and Peter Sanders. In-Place Parallel Super Scalar Sam-\nplesort (IPSSSSo). In25th Annual European Symposium on Algorithms (ESA 2017), volume 87 ofLeibniz\nInternational Proceedings in Informatics (LIPIcs), pp. 9:1‚Äì9:14. Schloss Dagstuhl‚ÄìLeibniz-Zentrum fuer\nInformatik, 2017. doi: 10.4230/LIPIcs.ESA.2017.9.\nMichael Axtmann, Sascha Witt, Daniel Ferizovic, and Peter Sanders. Engineering in-place (shared-memory)\nsorting algorithms.ACM Transactions on Parallel Computing, 2022.\nXingjian Bai and Christian Coester. Sorting with predictions.Advances in Neural Information Processing\nSystems, 2023.\nIvan Carvalho. Towards parallel learned sorting.arXiv preprint arXiv:2208.06902, 2022.\nIvan Carvalho and Ramon Lawrence. Learnedsort as a learning-augmented samplesort: Analysis and par-\nallelization. InProceedings of the 35th International Conference on Scientific and Statistical Database\nManagement, pp. 1‚Äì9, 2023.\nT.-H. Hubert Chan, Enze Sun, and Bo Wang. Generalized sorting with predictions revisited. InFrontiers\nof Algorithmics, 2023.\nCity of Chicago. Taxi trips: City of chicago: Data portal, May 2021. URLhttps://data.cityofchicago.\norg/Transportation/Taxi-Trips/wrvz-psew#column-menu.\nMichael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Faster matchings\nvia learned duals.Advances in Neural Information Processing Systems, 2021.\nThomas Erlebach, Murilo de Lima, Nicole Megow, and Jens Schl√∂ter. Sorting and hypergraph orientation\nunder uncertainty with predictions. InProceedings of the International Joint Conference on Artificial\nIntelligence (IJCAI), 2023.\nPaolo Ferragina and Filippo Lari. Fl-rmq: A learned approach to range minimum queries. In36th Annual\nSymposium on Combinatorial Pattern Matching (CPM 2025), 2025.\nPaolo Ferragina and Mattia Odorisio. Fast, robust, and learned distribution-based sorting.IEEE Access,\n13:45198‚Äì45214, 2025.\nPaolo Ferragina and Giorgio Vinciguerra. The pgm-index: a fully-dynamic compressed learned index with\nprovable worst-case bounds.Proceedings of the Very Large Data Bases Endowment, 2020.\nW Donald Frazer and Archie C McKellar. Samplesort: A sampling approach to minimal storage tree sorting.\nJournal of the Association for Computing Machinery, 1970.\nSreenivas Gollapudi and Debmalya Panigrahi. Online algorithms for rent-or-buy with expert advice. In\nProceedings of the International Conference on Machine Learning, 2019.\nYijie Han. Deterministic sorting in o(n log log n) time and linear space. InProceedings of the Annual ACM\nSymposium on Theory of Computing, 2002.\nYijie Han. Sorting real numbers in o(n sqrt(log n)) time and linear space.Algorithmica, 2020.\nYijie Han and Mikkel Thorup. Integer sorting in o(n sqrt(log log n)) expected time and linear space. In\nProceedings of the Symposium on Foundations of Computer Science, 2002.\nSungjin Im, Ravi Kumar, Aditya Petety, and Manish Purohit. Parsimonious learning-augmented caching.\nInProceedings of the International Conference on Machine Learning, 2022.\n14\n\nPublished in Transactions on Machine Learning Research (10/2025)\nJava. List (java se 21 & jdk 21). URL:https://docs.oracle.com/en/java/javase/21/docs/api/java.\nbase/java/util/List.html#sort(java.util.Comparator), 2023. Accessed on 2024-01-18.\nAndreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper, Tim Kraska, and Thomas\nNeumann. Radixspline: a single-pass learned index. InProceedings of the international workshop on\nexploiting artificial intelligence techniques for data management, 2020.\nTim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned index\nstructures. InProceedings of the International Conference on Management of Data, 2018.\nTim Kraska, Mohammad Alizadeh, Alex Beutel, Ed H Chi, Jialin Ding, Ani Kristo, Guillaume Leclerc,\nSamuel Madden, Hongzi Mao, and Vikram Nathan. Sagedb: A learned database system. InProceedings\nof the Conference on Innovative Data Systems Research, 2019.\nAni Kristo, Kapil Vaidya, Ugur √áetintemel, Sanchit Misra, and Tim Kraska. The case for a learned sorting\nalgorithm. InProceedings of the ACM SIGMOD International Conference on Management of Data, 2020.\nAni Kristo, Kapil Vaidya, and Tim Kraska. Defeating duplicates: A re-design of the learnedsort algorithm.\narXiv preprint arXiv:2107.03290, 2021a.\nAni Kristo, Kapil Vaidya, and Tim Kraska. LearnedSort, License: GPL 3.0. URL:https://github.com/\nanikristo/LearnedSort, 2021b. Accessed on 2024-01-18.\nAlexandra Anna Lassota, Alexander Lindermayr, Nicole Megow, and Jens Schl√∂ter. Minimalistic predictions\nto schedule jobs with online precedence constraints. InProceedings of the International Conference on\nMachine Learning, 2023.\nSilvio Lattanzi, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Online scheduling via learned\nweights. InProceedings of the ACM-SIAM Symposium on Discrete Algorithms, 2020.\nYujia Li, Felix Gimeno, Pushmeet Kohli, and Oriol Vinyals. Strong generalization and efficiency in neural\nprograms.arXiv preprint arXiv:2007.03629, 2020.\nPinyan Lu, Xuandi Ren, Enze Sun, and Yubo Zhang. Generalized sorting with predictions. InSymposium\non Simplicity in Algorithms (SOSA), 2021.\nThodoris Lykouris and Sergei Vassilvitskii. Competitive caching with machine learned advice.Journal of\nthe Association for Computing Machinery, 2021.\nRyan Marcus, Andreas Kipf, Alexander van Renen, Mihail Stoian, Sanchit Misra, Alfons Kemper, Thomas\nNeumann, and Tim Kraska. Benchmarking learned indexes.Proceedings of the Very Large Data Bases\nEndowment, 2020. Accessed on 2024-01-18.\nHristo Mavrodiev. Sofia air quality dataset, Sep 2019. URLhttps://www.kaggle.com/hmavrodiev/\nsofia-air-quality-dataset.\nPeter McIlroy. Optimistic sorting and information theoretic complexity. InProceedings of the ACM-SIAM\nSymposium on Discrete algorithms, 1993.\nMichael Mitzenmacher. A model for learned bloom filters and optimizing by sandwiching.Advances in\nNeural Information Processing Systems, 2018.\nMichael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions.Communications of the ACM,\n2022.\nJ Ian Munro and Sebastian Wild. Nearly-optimal mergesorts: Fast, practical sorting methods that optimally\nadapt to existing runs. InEuropean Symposium on Algorithms, 2018.\nDavid R Musser. Introspective sorting and selection algorithms.Software: Practice and Experience, 1997.\n15\n\nPublished in Transactions on Machine Learning Research (10/2025)\nArvind Narayanan, Saurabh Verma, Eman Ramadan, Pariya Babaie, and Zhi-Li Zhang. Deepcache: A deep\nlearning based framework for content caching. InProceedings of the Workshop on Network Meets AI &\nML, 2018.\nOmar Obeya, Endrias Kahssay, Edward Fan, and Julian Shun. Theoretically-efficient and practical par-\nallel in-place radix sorting. InProceedings of the ACM Symposium on Parallelism in Algorithms and\nArchitectures, 2019.\nOleh Onyshchak. Stock market dataset, Apr 2020. URLhttps://www.kaggle.com/jacksoncrow/\nstock-market-dataset.\nTim Peters. Python: list.sort. URL:https://github.com/python/cpython/blob/main/Objects/\nlistsort.txt, 2002. Accessed on 2024-01-18.\nManish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ml predictions.Advances\nin Neural Information Processing Systems, 2018.\nDhruv Rohatgi. Near-optimal bounds for online caching with machine learned advice. InProceedings of the\nACM-SIAM Symposium on Discrete Algorithms, 2020.\nSteven J Ross. The spreadsort high-performance general-case sorting algorithm. InPDPTA, 2002.\nKunihiko Sadakane. Compressed suffix trees with full functionality.Theory of Computing Systems, 41(4):\n589‚Äì607, 2007.\nShinsaku Sakaue and Taihei Oki. Discrete-convex-analysis-based framework for warm-starting algorithms\nwith predictions.Advances in Neural Information Processing Systems, 2022.\nAtsuki Sato and Yusuke Matsui. Fast partitioned learned bloom filter.Advances in Neural Information\nProcessing Systems, 2023.\nAtsuki Sato and Yusuke Matsui. Fast construction of partitioned learned bloom filter with theoretical\nguarantees.arXiv preprint arXiv:2410.13278, 2024.\nYongho Shin, Changyeol Lee, Gukryeol Lee, and Hyung-Chan An. Improved learning-augmented algorithms\nfor the multi-option ski rental problem via best-possible competitive analysis. InProceedings of the Inter-\nnational Conference on Machine Learning, 2023.\nKapil Vaidya, Eric Knorr, Tim Kraska, and Michael Mitzenmacher. Partitioned learned bloom filter. In\nInternational Conference on Learning Representations, 2021.\nYouyun Wang, Chuzhe Tang, Zhaoguo Wang, and Haibo Chen. Sindex: a scalable learned index for string\nkeys. InProceedings of the ACM SIGOPS Asia-Pacific Workshop on Systems, 2020.\nSepanta Zeighami and Cyrus Shahabi. On distribution dependent sub-logarithmic query time of learned\nindexing. InProceedings of the International Conference on Machine Learning, 2023.\nSepanta Zeighami and Cyrus Shahabi. Theoretical analysis of learned database operations under distribution\nshift through distribution learnability. InProceedings of the 41st International Conference on Machine\nLearning, 2024.\n16\n\nPublished in Transactions on Machine Learning Research (10/2025)\nA Proofs\nHere, we give the proofs omitted in the main paper. In Section A.1, we give the proof of Theorem 3.1,\nwhich is important for proving the worst-case complexity of PCF Learned Sort. Section A.2 and Section A.3\ngive proofs of Theorem 3.3 and Theorem 3.4, respectively, which are important for proving the expected\ncomputational complexity of PCF Learned Sort. The quantization-aware version of Theorem 3.4 and Theo-\nrem 3.6 is defined and the proof is given in the Section A.4. Finally, we provide the proofs of the theorems\nguaranteeing the complexity of Spline Learned Sort in Section A.5.\nA.1 Proof of Theorem 3.1\nProof.LetP(n)be the worst-case complexity of our Learned Sort when using the model-based bucketing\nalgorithmMas assumed in Theorem 3.1 andŒ¥=‚åänd‚åã. LetS(n)be the worst-case complexity of the\nstandard sort andR(n)be the worst-case complexity of model-based bucketing (including model training\nandinferences). SinceS(n) =O(nU(n))andthestandardsortterminatesafterafinitenumberofoperations,\n‚àÉC1,l1(>0), n‚â•0‚áíS(n)‚â§C 1+l1nU(n).(3)\nSinceR(n) =O(n)andŒ≥+ 1 =O(n),\n‚àÉn2,l2(>0), n‚â•n 2‚áíR(n)‚â§l 2n.(4)\n‚àÉn3,l3(>0), n‚â•n 3‚áíŒ≥+ 1‚â§l 3n.(5)\nIn the following, we proveP(n) =O(nU(n) +nlog logn)by mathematical induction.\nFirst, forn<max(n 2,n3,œÑ) = :n0, there exists a constantC(>0)such thatP(n)‚â§C. That is, forn<n 0,\nour Learned Sort terminates in a finite number of operations. This is because, sinceŒ¥ <n, the bucket will\neither be smaller than the original array lengthn, or the bucket will be immediately sorted by the standard\nsort.\nNext, assume that there exists a constantC(>0)andl(>0)such thatP(n)‚â§C+l(nU(n) +nlog logn)\nfor alln < k, wherekis an integer such thatk‚â•n 0. Without loss of generality, we assume thatC‚â•C 1\nandl‚â•l 1. LetSŒ≥,kbe the set consisting of all(Œ≥+ 1)-dimensional vectors of positive integers whose sum\nisk, i.e.,S Œ≥,k:=/braceleftÔ£¨ig\ns‚ààZŒ≥\n‚â•0|/summationtextŒ≥+1\ni=1si=k/bracerightÔ£¨ig\n. Then,\nP(k)‚â§R(k) + max\ns‚ààSŒ≥,kŒ≥+1/summationdisplay\ni=1/braceleftbig\n1[si‚â•‚åäkd‚åã]¬∑S(si) +1[si<‚åäkd‚åã]¬∑P(si)/bracerightbig\n‚â§l2k+ max\ns‚ààSŒ≥,kŒ≥+1/summationdisplay\ni=1/braceleftbig\n1[si‚â•‚åäkd‚åã]¬∑(C 1+l1siU(si)) +1[si<‚åäkd‚åã]¬∑(C+l(s iU(si) +silog logsi))/bracerightbig\n‚â§l2k+ max\ns‚ààSŒ≥,kŒ≥+1/summationdisplay\ni=1/parenleftbig\nC+lsiU(si) +lsilog logkd/parenrightbig\n‚â§l2k+C(Œ≥+ 1) +lkU(k) +lklog logkd\n‚â§l2k+Cl 3k+lkU(k) +lklog logk+lklogd\n‚â§/parenleftbigg\nl2+Cl 3‚àíllog1\nd/parenrightbigg\nk+ (C+l(kU(k) +klog logk)).(6)\nTherefore, if we takelsuch that\nl2+Cl 3\nlog1\nd‚â§l,(7)\nthenP(k)‚â§C+l(kU(k) +klog logk)(note that the left side of Equation (7) is a constant independent of\nk).\nHence, by mathematical induction, it is proved that there exists a constantC(>0)andl(>0)such that\nP(n)‚â§C+l(kU(k) +lklog logk)for alln‚ààN.\n17\n\nPublished in Transactions on Machine Learning Research (10/2025)\nA.2 Proof of Theorem 3.3\nProof.The proof approach is the same as in Theorem 3.1, but in Theorem 3.3, the ‚Äúexpected‚Äù computa-\ntional complexity is bounded. The following two randomnesses are considered for computing the ‚Äúexpected‚Äù\ncomputational complexity: (i) the randomness with whichnelements are independently sampled according\nto the probability density functionf(x)in the process of forming the input arrayx, and (ii) the randomness\nof the PCF Learned Sort algorithm samplingŒ±elements from the input arrayxfor training the PCF.\nLetT(n)be the expected complexity of our Learned Sort when using the model-based bucketing algorithm\nMas assumed in Theorem 3.3 andŒ¥=‚åänd‚åã. LetS(n)be the expected complexity of the standard sort and\nR(n)be the expected complexity of model-based bucketing (including model training and inferences). Since\nS(n) =O(nlogn)and the standard sort terminates after a finite number of operations,\n‚àÉC1,l1(>0), n‚â•0‚áíS(n)‚â§C 1+l1nlogn.(8)\nSinceR(n) =O(n),Pr[‚àÉj,|c j|‚â•‚åänd‚åã] =O(1/logn), andŒ≥+ 1 =O(n),\n‚àÉn2,l2(>0), n‚â•n 2‚áíR(n)‚â§l 2n,(9)\n‚àÉn3,l3(>0), n‚â•n 3‚áíPr[‚àÉj,|c j|‚â•‚åänd‚åã]‚â§l3\nlogn,(10)\n‚àÉn4,l4(>0), n‚â•n 4‚áíŒ≥+ 1‚â§l 4n.(11)\nIn the following, we proveT(n) =O(nlog logn)by mathematical induction.\nFirst, forn <max(n 2,n3,n4,œÑ) = :n0, there exists a constantC(>0)such thatT(n)‚â§C. That is, for\nn<n 0, our Learned Sort terminates in a finite number of operations.\nNext, assume that there exists a constantC(>0)andl(>0)such thatT(n)‚â§C+lnlog lognfor alln<k,\nwherekis an integer such thatk‚â•n 0. Then, fromk‚â•n 2,\nT(k)‚â§R(k) +EÔ£Æ\nÔ£∞Œ≥+1/summationdisplay\nj=11[|cj|‚â•‚åäkd‚åã]¬∑S(|cj|) +1[|cj|<‚åäkd‚åã]¬∑T(|cj|)Ô£π\nÔ£ª\n‚â§l2k+ Pr/bracketleftbig\n‚àÉj,|cj|‚â•‚åäkd‚åã/bracketrightbig\n¬∑EÔ£Æ\nÔ£∞Œ≥+1/summationdisplay\nj=1S(|cj|)Ô£π\nÔ£ª+EÔ£Æ\nÔ£∞Œ≥+1/summationdisplay\nj=11[|cj|<‚åäkd‚åã]¬∑T(|cj|)Ô£π\nÔ£ª\n‚â§l2k+ Pr[‚àÉj,|c j|‚â•‚åäkd‚åã]¬∑{C 1(Œ≥+ 1) +l 1klogk}+E/bracketleftÔ£¨iggŒ≥+1/summationdisplay\ni=1T(min(‚åäkd‚åã,|cj|))/bracketrightÔ£¨igg\n.(12)\nHere, fromk‚â•n 3andk‚â•n 4,\nPr[‚àÉj,|cj|‚â•‚åäkd‚åã]¬∑{C 1(Œ≥+ 1) +l 1klogk}‚â§l3\nlogk¬∑(C1l4k+l 1klogk)\n‚â§(C 1l3l4+l1l3)k.(13)\nAlso, from the assumption of induction andk‚â•n 4,\nE/bracketleftÔ£¨iggŒ≥+1/summationdisplay\ni=1T(min(‚åäkd‚åã,|cj|))/bracketrightÔ£¨igg\n‚â§E/bracketleftÔ£¨iggŒ≥+1/summationdisplay\ni=1/braceleftbig\nC+l¬∑min(‚åäkd‚åã,|cj|) log log min(‚åäkd‚åã,|cj|)/bracerightbig/bracketrightÔ£¨igg\n‚â§E/bracketleftÔ£¨igg\nC(Œ≥+ 1) +Œ≥+1/summationdisplay\ni=1l¬∑|cj|log log‚åäkd‚åã/bracketrightÔ£¨igg\n‚â§Cl 4k+lklog log‚åäkd‚åã\n‚â§Cl 4k+lklogd+lklog logk.(14)\n18\n\nPublished in Transactions on Machine Learning Research (10/2025)\nTherefore,\nT(k)‚â§l 2k+ (C 1l3l4+l1l3)k+Cl 4k+lklogd+lklog logk\n‚â§/braceleftbigg\nl2+C 1l3l4+l1l3+Cl 4‚àíllog1\nd/bracerightbigg\nk+ (C+lklog logk).(15)\nTherefore, if we takelsuch that\nl2+C 1l3l4+l1l3+Cl 4\nlog1\nd‚â§l,(16)\nthenT(k)‚â§C+lklog logk(note that the left side of Equation (16) is a constant independent ofk).\nHence, by mathematical induction, it is proved that there exists a constantC(>0)andl(>0)such that\nT(n)‚â§C+lnlog lognfor alln‚ààN.\nA.3 Proof of Theorem 3.4\nWe first present the following lemma to prove Theorem 3.4.\nLemma A.1.Lete(‚ààIn)be a sorted version ofx I(‚ààIn)and‚àÜ := (x max‚àíxmin)/Œ≤(wherex minand\nxmaxare the minimum and maximum values ofx I, respectively).\nAlso, define the setS randTras follows (r= 1,...,n):\nSr={k|e max(1,r‚àíŒ¥/2) + ‚àÜ<ek‚â§er‚àí‚àÜ},T r={k|er+ ‚àÜ‚â§ek<emin(r+Œ¥/2,n)‚àí‚àÜ}.(17)\nUsing this definition, defineY jr,Zjr,Yr,Zras follows (j= 1,...,Œ±, r= 1,...,n):\nYjr=/braceleftÔ£¨igg\n1 (j‚ààS r)\n0 (else), Zjr=/braceleftÔ£¨igg\n1 (j‚ààT r)\n0 (else), Yr=Œ±/summationdisplay\nj=1Yjr, Zr=Œ±/summationdisplay\nj=1Zjr (18)\nWhen usingM PCF, if the size of the bucket to whiche ris allocated is greater than or equal toŒ¥, the following\nholds:/parenleftbigg\nr‚â•Œ¥\n2+ 1‚àßYr‚â§/floorleftbiggŒ±\nŒ≥/floorrightbigg/parenrightbigg\n‚à®/parenleftbigg\nr‚â§n‚àíŒ¥\n2‚àßZr‚â§/floorleftbiggŒ±\nŒ≥/floorrightbigg/parenrightbigg\n.(19)\nProof.We prove the contraposition of the lemma. That is, we prove thate ris allocated to a bucket smaller\nthanŒ¥under the assumption that/parenleftÔ£¨ig\nr<Œ¥\n2+ 1‚à®Yr>/floorleftÔ£¨ig\nŒ±\nŒ≥/floorrightÔ£¨ig/parenrightÔ£¨ig\n‚àß/parenleftÔ£¨ig\nr>n‚àíŒ¥\n2‚à®Zr>/floorleftÔ£¨ig\nŒ±\nŒ≥/floorrightÔ£¨ig/parenrightÔ£¨ig\n.\nFor convenience, we hypothetically definee 0=‚àí‚àû,en+1=‚àû, and assigne 0to the 0th bucket ande n+1to\nthe(Œ≥+ 2)-th bucket. The size of the 0th bucket and the(Œ≥+ 2)-th bucket are both 1.\nFirst, we prove thate rande min(r+Œ¥/2,n+1) are assigned to different buckets. Whenn‚àíŒ¥/2<r‚â§n,e rand\nemin(r+Œ¥/2,n+1) =en+1are obviously assigned to different buckets. Whenr‚â§n‚àíŒ¥/2, the ID of the bucket\nto whiche ris assigned is\n‚åäÀúF(er)Œ≥‚åã+ 1 =/floorleftÔ£¨igŒ≥\nŒ±bi(er)/floorrightÔ£¨ig\n+ 1\n‚â§Œ≥\nŒ±bi(er)+ 1\n=Œ≥\nŒ±|{j|i(aj)‚â§i(er)}|+ 1\n‚â§Œ≥\nŒ±|{j|aj‚â§er+ ‚àÜ}|+ 1.(20)\n19\n\nPublished in Transactions on Machine Learning Research (10/2025)\nThe ID of the bucket to whiche min(r+Œ¥/2,n+1) =er+Œ¥/2is assigned is\n‚åäÀúF(er+Œ¥/2 )Œ≥‚åã+ 1 =/floorleftÔ£¨igŒ≥\nŒ±bi(er+Œ¥/2 )/floorrightÔ£¨ig\n+ 1\n>Œ≥\nŒ±bi(er+Œ¥/2 )\n=Œ≥\nŒ±/vextendsingle/vextendsingle{j|i(aj)‚â§i(er+Œ¥/2 )}/vextendsingle/vextendsingle\n‚â•Œ≥\nŒ±/vextendsingle/vextendsingle{j|aj‚â§er+Œ¥/2‚àí‚àÜ}/vextendsingle/vextendsingle.(21)\nThus, taking the difference between these two bucket IDs,\n/parenleftbig\n‚åäÀúF(er+Œ¥/2 )Œ≥‚åã+ 1/parenrightbig\n‚àí/parenleftbig\n‚åäÀúF(er)Œ≥‚åã+ 1/parenrightbig\n>Œ≥\nŒ±/vextendsingle/vextendsingle{j|aj‚â§er+Œ¥/2‚àí‚àÜ}/vextendsingle/vextendsingle‚àí/parenleftÔ£¨igŒ≥\nŒ±|{j|aj‚â§er+ ‚àÜ}|+ 1/parenrightÔ£¨ig\n=Œ≥\nŒ±/vextendsingle/vextendsingle{j|er+ ‚àÜ<aj‚â§er+Œ¥/2‚àí‚àÜ}/vextendsingle/vextendsingle‚àí1\n=Œ≥\nŒ±|Tr|‚àí1\n=Œ≥\nŒ±Œ±/summationdisplay\nj=1Zjr‚àí1\n=Œ≥\nŒ±Zr‚àí1\n‚â•0.(22)\nTherefore,e rande min(r+Œ¥/2,n+1) are assigned to different buckets.\nIn the same way, we can prove thate max(0,r‚àíŒ¥/2) anderare also assigned to different buckets. Thus, the size\nof the bucket to whiche ris assigned is at mostŒ¥‚àí1(at most frome max(0,r‚àíŒ¥/2)+1 toe min(r+Œ¥/2,n+1)‚àí1 ),\nand the contraposition of the lemma is proved.\nUsing Theorem A.1, we can prove Theorem 3.4.\nProof.Letq= max\ny/integraltexty+‚àÜ\nyfI(x)dx(whereyis a value such that(y,y+ ‚àÜ)‚äÜI). Then, fromœÉ 1‚â§f(x)‚â§œÉ 2\nfor allx‚ààI,\nq‚â§maxy/integraltexty+‚àÜ\nyf(y)dy/integraltext\nIf(x)dx\n‚â§maxy/integraltexty+‚àÜ\nyœÉ2dy/integraltext\nIœÉ1dx\n‚â§œÉ2‚àÜ\nœÉ1(xmax‚àíxmin)\n=œÉ2\nœÉ1Œ≤.(23)\nThus, whenr‚â•Œ¥\n2+ 1,\nE/bracketleftbiggŒ¥\n2‚àí|Sr|/bracketrightbigg\n=E/bracketleftbiggŒ¥\n2‚àí/vextendsingle/vextendsingle{k|er‚àíŒ¥/2 + ‚àÜ<ek‚â§er‚àí‚àÜ}/vextendsingle/vextendsingle/bracketrightbigg\n=E/bracketleftbig/vextendsingle/vextendsingle{k|er‚àíŒ¥/2<ek‚â§er‚àíŒ¥/2 + ‚àÜ}/vextendsingle/vextendsingle/bracketrightbig\n+E[|{k|e r‚àí‚àÜ<ek‚â§er}|]\n‚â§nq+nq\n‚â§2œÉ2n\nœÉ1Œ≤.(24)\n20\n\nPublished in Transactions on Machine Learning Research (10/2025)\nThus, whenr‚â•Œ¥\n2+ 1,\nE[Yr] =Œ±\nnE[|Sr|]\n=Œ±\nn/parenleftbiggŒ¥\n2‚àíE/bracketleftbiggŒ¥\n2‚àí|Sr|/bracketrightbigg/parenrightbigg\n‚â•Œ±Œ¥\n2n‚àí2œÉ2Œ±\nœÉ1Œ≤\n=Œ±K\nŒ≥.(25)\nHere, whenK‚â•1, we have\n0‚â§1‚àíŒ±\nŒ≥E[Yr]<1.(26)\nTherefore, from the Chernoff bound,\nPr/bracketleftbigg\nYr‚â§Œ±\nŒ≥/bracketrightbigg\n= Pr/bracketleftbigg\nYr‚â§/braceleftbigg\n1‚àí/parenleftbigg\n1‚àíŒ±\nŒ≥E[Yr]/parenrightbigg/bracerightbigg\nE[Yr]/bracketrightbigg\n‚â§exp/braceleftÔ£¨igg\n‚àí1\n2/parenleftbigg\n1‚àíŒ±\nŒ≥E[Yr]/parenrightbigg2\nE[Yr]/bracerightÔ£¨igg\n‚â§exp/braceleftÔ£¨igg\n‚àíŒ±K\n2Œ≥/parenleftbigg\n1‚àí1\nK/parenrightbigg2/bracerightÔ£¨igg\n.(27)\nIn the same way, we can prove that whenr‚â§n‚àíŒ¥\n2,\nPr/bracketleftbigg\nZr‚â§Œ±\nŒ≥/bracketrightbigg\n‚â§exp/braceleftÔ£¨igg\n‚àíŒ±K\n2Œ≥/parenleftbigg\n1‚àí1\nK/parenrightbigg2/bracerightÔ£¨igg\n.(28)\nThus, by definingE rto be the event ‚Äúe ris allocated to a bucket with size greater than or equal toŒ¥,‚Äù from\nTheorem A.1,\nPr[Er]‚â§Pr/bracketleftbigg/parenleftbigg\nr‚â•Œ¥\n2+ 1‚àßYr‚â§/floorleftbiggŒ±\nŒ≥/floorrightbigg/parenrightbigg\n‚à®/parenleftbigg\nr‚â§n‚àíŒ¥\n2‚àßZr‚â§/floorleftbiggŒ±\nŒ≥/floorrightbigg/parenrightbigg/bracketrightbigg\n‚â§Pr/bracketleftbigg/parenleftbigg\nr‚â•Œ¥\n2+ 1‚àßYr‚â§/floorleftbiggŒ±\nŒ≥/floorrightbigg/parenrightbigg/bracketrightbigg\n+ Pr/bracketleftbigg/parenleftbigg\nr‚â§n‚àíŒ¥\n2‚àßZr‚â§/floorleftbiggŒ±\nŒ≥/floorrightbigg/parenrightbigg/bracketrightbigg\n‚â§Ô£±\nÔ£≤\nÔ£≥exp/braceleftÔ£¨ig\n‚àíŒ±K\n2Œ≥/parenleftbig\n1‚àí1\nK/parenrightbig2/bracerightÔ£¨ig\n,/parenleftbig\nr<Œ¥\n2+ 1‚à®r>n‚àíŒ¥\n2/parenrightbig\n2 exp/braceleftÔ£¨ig\n‚àíŒ±K\n2Œ≥/parenleftbig\n1‚àí1\nK/parenrightbig2/bracerightÔ£¨ig\n,(else)\n‚â§2 exp/braceleftÔ£¨igg\n‚àíŒ±K\n2Œ≥/parenleftbigg\n1‚àí1\nK/parenrightbigg2/bracerightÔ£¨igg\n.(29)\nTherefore,\nE/bracketleftÔ£¨iggn/summationdisplay\nr=11[Er]/bracketrightÔ£¨igg\n‚â§2nexp/braceleftÔ£¨igg\n‚àíŒ±K\n2Œ≥/parenleftbigg\n1‚àí1\nK/parenrightbigg2/bracerightÔ£¨igg\n(30)\nThen, noting that the number of buckets with size greater than or equal toŒ¥is less than or equal to/summationtextn\nr=11[Er]/Œ¥,\nE[|{j||c j|>Œ¥}|]‚â§2n\nŒ¥exp/braceleftÔ£¨igg\n‚àíŒ±K\n2Œ≥/parenleftbigg\n1‚àí1\nK/parenrightbigg2/bracerightÔ£¨igg\n.(31)\n21\n\nPublished in Transactions on Machine Learning Research (10/2025)\nThen, from Markov‚Äôs inequality, we have\nPr[‚àÉj,|cj|>Œ¥] = Pr[|{j||c j|>Œ¥}|‚â•1]\n‚â§2n\nŒ¥exp/braceleftÔ£¨igg\n‚àíŒ±K\n2Œ≥/parenleftbigg\n1‚àí1\nK/parenrightbigg2/bracerightÔ£¨igg\n.(32)\nA.4 The Quantization-Aware Version of Theorem 3.4 and Theorem 3.6\nIn general, computers represent numbers in a finite number of bits, so the numbers they handle are inherently\ndiscrete. However, Theorem 3.6, which states that the expected computational complexity of PCF Learned\nSort isO(nlog logn), does not cover discrete distributions. Here, we define the quantization process by which\na computer represents numbers in finite bits and then show that the expected computational complexity of\nPCF Learned Sort is stillO(nlog logn), under the assumption that ‚Äúthe quantization is fine enough.‚Äù\nFirst, we assume the sampling and quantization process is as follows:\nAssumption A.2.For a range of valuesD(‚äÜR), definem(‚ààN)contiguous regionsD 1,D2,...,Dm\nsuch that (i) they are disjoint from each other and (ii) together they formD. For each region, determine\nrepresentative valuesr 1,r2,...,rm. Here,r 1,r2,...,rmare values contained inD 1,D2,...,Dm, respectively.\nFor a valuex(‚ààD), the quantized value ofx,x‚Ä≤, is obtained asx‚Ä≤=ri, whereiis the (only)isuch that\nx‚ààDi. The valuexis sampled according to the probability density functionf(x), but a computer keepsx‚Ä≤\n(instead ofx) inlog2mbits with some quantization error.\nAlso, for the intervalI‚äÜD, we defineŒ∑(I)as follows.\nDefinition A.3.Œ∑(I)is the maximum width ofD ithat intersects with intervalI, i.e.,\nŒ∑(I) := max{|D i||Di‚à™IÃ∏=‚àÖ,i= 1,...,m},(33)\nwhere|Di|is the width of the rangeD i.\nWe can prove thatŒ∑(I)is the upper bound of the quantization error ofxinI, i.e.,|x‚àíx‚Ä≤|‚â§Œ∑(I)when\nx‚ààI.\nNow, the assumption that the quantization is ‚Äúfine enough‚Äù is specifically defined as follows.\nAssumption A.4.(Recall that our PCF Learned Sort recursively calls its own algorithm. Each time of\nrecursion, the range of values of interestIchanges, and the length of the array of interestnalso changes.)\nFor allIandnthat appear in the algorithm, the following holds:\nŒ≤Œ∑(I)\n|I|‚â§1\n2.(34)\nWe show intuitively and empirically that this is a satisfactory assumption.\nFirst, to show intuitively that this assumption is satisfactory, we give an example. LetIbe a range of values\nthat can be represented by a 64-bit double (1 bit for the sign, 11 bits for the exponent part, and 52 bits for\nthe mantissa part), that is,I= [‚àí1.79√ó10308,1.79√ó10308]. The quantization is performed by mapping each\nvalue to the nearest number that can be represented by a 64-bit double. In this setting,Œ∑(I) = 1.99√ó10292,\n|I|= 3.59√ó10308.Œ∑(I)/|I|= 5.55√ó10‚àí17. Thus, for usualŒ≤,Œ≤‚â§9√ó1015, Equation (34) holds.\nSecond, we show that Equation (34) is a satisfactory assumption empirically. In the 1,280 measurements,\nwhere 10 measurements each for 16 differentn(‚àà{103,2√ó103,5√ó103,...,108})on 8 different datasets,\n5.23√ó107pairs of(I,Œ≤)appeared (we setŒ≤=/floorleftbig\nn3/4/floorrightbig\n), and the left side of Equation (34) is at most\n8.11√ó10‚àí9, indicating that Equation (34) is always true with a margin.\nUnder this definition and assumption about quantization, we can prove the quantization-aware version of\nTheorem 3.4.\n22\n\nPublished in Transactions on Machine Learning Research (10/2025)\nLemma A.5(Quantization-aware version of Theorem 3.4).LetœÉ 1andœÉ 2be respectively the lower and\nupper bounds of the probability density distributionf(x)inD, and assume that0<œÉ 1‚â§œÉ2<‚àû. That is,\nx‚ààD ‚áíœÉ 1‚â§f(x)‚â§œÉ 2. Also, letx‚Ä≤\nIbe the array created by the quantization ofx(‚ààIn)in the manner\ndefined in Theorem A.2.\nThen, in model-based bucketing ofx‚Ä≤\nI(‚ààIn)to{cj}Œ≥+1\nj=1usingM PCF, the following holds for any interval\nI(‚äÜD)under Theorem A.4:\nK:=Œ≥Œ¥\n2n‚àí4œÉ2Œ≥\nœÉ1Œ≤‚â•1‚áíPr[‚àÉj,|c j|>Œ¥]‚â§2n\nŒ¥exp/braceleftÔ£¨igg\n‚àíŒ±K‚Ä≤\n2Œ≥/parenleftbigg\n1‚àí1\nK‚Ä≤/parenrightbigg2/bracerightÔ£¨igg\n.(35)\nThe proof of Theorem A.5 is done in the same way as Theorem 3.4. That is, we first prove the following\nlemma.\nLemma A.6(Quantization-aware version of Theorem A.6).Lete‚Ä≤(‚ààIn)be a sorted version ofx‚Ä≤\nI(‚ààIn)\nand‚àÜ := (x‚Ä≤\nmax‚àíx‚Ä≤\nmin)/Œ≤(wherex‚Ä≤\nminandx‚Ä≤\nmaxare the minimum and maximum values ofx‚Ä≤\nI, respectively).\nAlso, define the setS‚Ä≤\nr,T‚Ä≤\nras follows (r= 1,...,n):\nS‚Ä≤\nr={k|e max(1,r‚àíŒ¥/2) + ‚àÜ + 2Œ∑(I)<e k‚â§er‚àí‚àÜ‚àí2Œ∑(I)},(36)\nT‚Ä≤\nr={k|er+ ‚àÜ + 2Œ∑(I)‚â§e k<emin(r+Œ¥/2,n)‚àí‚àÜ‚àí2Œ∑(I)}.(37)\nUsing this definition, defineY‚Ä≤\njr,Z‚Ä≤\njr,Y‚Ä≤\nr,Z‚Ä≤\nras follows (j= 1,...,Œ±, r= 1,...,n):\nY‚Ä≤\njr=/braceleftÔ£¨igg\n1 (j‚ààS‚Ä≤\nr)\n0 (else), Z‚Ä≤\njr=/braceleftÔ£¨igg\n1 (j‚ààT‚Ä≤\nr)\n0 (else), Y‚Ä≤\nr=Œ±/summationdisplay\nj=1Y‚Ä≤\njr, Z‚Ä≤\nr=Œ±/summationdisplay\nj=1Z‚Ä≤\njr.(38)\nIf the size of the bucket to whiche‚Ä≤\nris allocated is greater than or equal toŒ¥, then the following holds:\n/parenleftbigg\nr‚â•Œ¥\n2+ 1‚àßY‚Ä≤\nr‚â§/floorleftbiggŒ±\nŒ≥/floorrightbigg/parenrightbigg\n‚à®/parenleftbigg\nr‚â§n‚àíŒ¥\n2‚àßZ‚Ä≤\nr‚â§/floorleftbiggŒ±\nŒ≥/floorrightbigg/parenrightbigg\n.(39)\nProof.The proof method is exactly the same as for Theorem A.1. That is, by taking the difference between\nthe IDs of the buckets to whiche‚Ä≤\nr+Œ¥/2ande‚Ä≤\nrare assigned,\n/parenleftÔ£¨ig\n‚åäÀúF(e‚Ä≤\nr+Œ¥/2 )Œ≥‚åã+ 1/parenrightÔ£¨ig\n‚àí/parenleftbig\n‚åäÀúF(e‚Ä≤\nr)Œ≥‚åã+ 1/parenrightbig\n>Œ≥\nŒ±/vextendsingle/vextendsingle{j|aj‚â§er+Œ¥/2‚àí‚àÜ‚àí2Œ∑(I)}/vextendsingle/vextendsingle‚àí/parenleftÔ£¨igŒ≥\nŒ±|{j|aj‚â§er+ ‚àÜ + 2Œ∑(I)}|+ 1/parenrightÔ£¨ig\n=Œ≥\nŒ±/vextendsingle/vextendsingle{j|er+ ‚àÜ + 2Œ∑(I)<a j‚â§er+Œ¥/2‚àí‚àÜ‚àí2Œ∑(I)}/vextendsingle/vextendsingle‚àí1\n=Œ≥\nŒ±Z‚Ä≤\nr‚àí1\n‚â•0,(40)\nwhenZ‚Ä≤\nr>/floorleftÔ£¨ig\nŒ±\nŒ≥/floorrightÔ£¨ig\n. Thus, we can prove that when\n/parenleftbigg\nr<Œ¥\n2+ 1‚à®Y‚Ä≤\nr>/floorleftbiggŒ±\nŒ≥/floorrightbigg/parenrightbigg\n‚àß/parenleftbigg\nr>n‚àíŒ¥\n2‚à®Z‚Ä≤\nr>/floorleftbiggŒ±\nŒ≥/floorrightbigg/parenrightbigg\n(41)\nholds,e‚Ä≤\nr+Œ¥/2ande‚Ä≤\nrare assigned to the different bucket ande‚Ä≤\nr‚àíŒ¥/2ande‚Ä≤\nrare assigned to the different\nbucket. Then, the contraposition of the lemma is proved.\nUsing Theorem A.6, we can prove Theorem A.5.\n23\n\nPublished in Transactions on Machine Learning Research (10/2025)\nProof.Letq‚Ä≤= max\ny/integraltexty+‚àÜ2Œ∑(I)\nyfI(x)dx(whereyis a value such that(y,y+ ‚àÜ + 2Œ∑(I)‚äÜI). Then, from\nœÉ1‚â§f(x)‚â§œÉ 2for allx‚ààI,\nq‚Ä≤‚â§maxy/integraltexty+‚àÜ+2Œ∑(I)\nyf(y)dy/integraltext\nIf(x)dx\n‚â§maxy/integraltexty+‚àÜ+2Œ∑(I)\nyœÉ2dy/integraltext\nIœÉ1dx\n=œÉ2(‚àÜ + 2Œ∑(I))\nœÉ1(x‚Ä≤max‚àíx‚Ä≤\nmin)\n‚â§œÉ2\nœÉ1Œ≤¬∑/parenleftbigg\n1 +2Œ≤Œ∑(I)\n|I|/parenrightbigg\n‚â§2œÉ2\nœÉ1Œ≤.(42)\nThe last inequality is obtained by Theorem A.4.\nThus, whenr‚â•Œ¥\n2+ 1,\nE/bracketleftbiggŒ¥\n2‚àí|S‚Ä≤\nr|/bracketrightbigg\n=E/bracketleftbiggŒ¥\n2‚àí/vextendsingle/vextendsingle{k|er‚àíŒ¥/2 + ‚àÜ + 2Œ∑(I)<e k‚â§er‚àí‚àÜ‚àí2Œ∑(I)}/vextendsingle/vextendsingle/bracketrightbigg\n=E/bracketleftbig/vextendsingle/vextendsingle{k|er‚àíŒ¥/2<ek‚â§er‚àíŒ¥/2 + ‚àÜ + 2Œ∑(I)}/vextendsingle/vextendsingle/bracketrightbig\n+E[|{k|e r‚àí‚àÜ‚àí2Œ∑(I)<e k‚â§er}|]\n‚â§nq‚Ä≤+nq‚Ä≤\n‚â§4œÉ2n\nœÉ1Œ≤.(43)\nThus, whenr‚â•Œ¥\n2+ 1,\nE[Y‚Ä≤\nr] =Œ±\nnE[|S‚Ä≤\nr|]\n=Œ±\nn/parenleftbiggŒ¥\n2‚àíE/bracketleftbiggŒ¥\n2‚àí|S‚Ä≤\nr|/bracketrightbigg/parenrightbigg\n‚â•Œ±Œ¥\n2n‚àí4œÉ2Œ±\nœÉ1Œ≤\n=Œ±K‚Ä≤\nŒ≥.(44)\nFrom this point forward, by proceeding in exactly the same way as the proof of Theorem 3.4, we can prove\nthe following using Theorem A.1:\nK‚Ä≤‚â•1‚áíPr[‚àÉj,|c j|>Œ¥]‚â§2n\nŒ¥exp/braceleftÔ£¨igg\n‚àíŒ±K‚Ä≤\n2Œ≥/parenleftbigg\n1‚àí1\nK‚Ä≤/parenrightbigg2/bracerightÔ£¨igg\n.(45)\nUsing Theorem A.5, we can prove the following theorem.\nTheorem A.7(Quantization-aware version of Theorem 3.6).LetœÉ 1andœÉ 2be the lower and upper bounds,\nrespectively, of the probability density distributionf(x)inD, and assume that0< œÉ 1‚â§œÉ 2<‚àû. Also,\nassume that the input array is quantized in a way that satisfies Theorem A.2 and Theorem A.4. Then, the\nexpected complexity of PCF Learned Sort withM PCFas the bucketing method andŒ±=Œ≤=Œ≥=Œ¥=‚åän3/4‚åã\nisO(nlog logn).\n24\n\nPublished in Transactions on Machine Learning Research (10/2025)\nProof.WhenŒ±=Œ≤=Œ≥=‚åän3/4‚åã, the computational complexity for model-based bucketing isO(n). Since\nK‚Ä≤= ‚Ñ¶(‚àön)whenŒ±=Œ≤=Œ≥=Œ¥=‚åän3/4‚åã,K‚Ä≤‚â•1for sufficiently largen, and\n2n\nŒ¥exp/braceleftÔ£¨igg\n‚àíŒ±K‚Ä≤\n2Œ≥/parenleftbigg\n1‚àí1\nK‚Ä≤/parenrightbigg2/bracerightÔ£¨igg\n=O(n1\n4exp(‚àí‚àön))‚â§O/parenleftbigg1\nlogn/parenrightbigg\n.(46)\nTherefore, from Theorem 3.3 and Theorem A.5, the expected computational complexity of PCF Learned\nSort isO(nlog logn).\nA.5 Proofs for Spline Learned Sort\nDefinition of Spline Learned Sort.We now formally defineSpline Learned Sort, which we propose as\na variant of PCF Learned Sort. In particular, Spline Learned Sort replacesM PCF‚Äîthe bucketing method\nthat uses PCF as the CDF model‚ÄîwithM Spline, a bucketing method based on a spline-based CDF model.\nBelow, we give a rigorous description ofM Spline.\nThe training algorithm of the CDF model inM Splineis the same as inM PCF. That is, the parameters\nŒ±,Œ≤‚ààNare determined byn. Then, following this setting, we sample an arrayaof lengthŒ±from the input\narrayx, which is subsequently bin-counted (as described in Section 3.3).\nFor later explanation and proofs, we introduce additional terminology. We define the vector of thresholds\nused for counting ast‚ààRŒ≤+2:ti=x min+ (x max‚àíxmin)(i‚àí1)/Œ≤fori= 1,2,...,Œ≤+ 2. Thus, thei-th\nbin during CDF model training corresponds to the interval[t i,ti+1). Accordingly, the non-decreasing, non-\nnegative arraybformed by bin-counting is given byb i=|{j‚àà{1,...,Œ±}|a j<ti+1}|fori= 1,2,...,Œ≤+ 1.\nWealsodenotebyF a(x)theempiricalCDFofthesampledarraya:F a(x) =|{j‚àà{1,2,...,Œ±}|a j‚â§x}|/Œ±.\nThe inference algorithm of the CDF model inM Splineis similar to but slightly different from that in\nMPCF. In PCF, the prediction is constant within each interval[t i,ti+1). In contrast, in the spline-based\ncase, the prediction is obtained by linearly interpolating the empirical CDF values at the endpoints of\nthe interval. Specifically, the output ÀúF(x)for inputxis obtained as follows. As in PCF, we compute\ni(x) =/floorleftÔ£¨ig\n(x‚àíx min)Œ≤\nxmax‚àíxmin/floorrightÔ£¨ig\n+ 1, so thatx‚àà[t i(x),ti(x)+1 ). While PCF returnsF a(ti(x)+1 ) =bi(x)/Œ±as the CDF\nprediction, the spline-based method linearly interpolates betweenF a(ti(x))andF a(ti(x)+1 ):\nÀúF(x) =F a(ti(x)) +x‚àíti(x)\nti(x)+1‚àíti(x)¬∑(Fa(ti(x)+1 )‚àíF a(ti(x)))(47)\n=1\nŒ±/parenleftbigg\nbi(x)‚àí1 +x‚àíti(x)\nti(x)+1‚àíti(x)¬∑(bi(x)‚àíbi(x)‚àí1 )/parenrightbigg\n,(48)\nwithb 0= 0.\nUnlike PCF, which only considers which bin a value falls into, the spline-based method also accounts for\nthe relative position of the value within the bin to compute the predicted CDF ÀúF(x). The asymptotic\ncomputational complexity for training and inference is the same as PCF:O(n)for training andO(1)per\nelement for inference. However, unlike PCF (which only requires array lookups), the spline-based approach\nrequires additional subtractions and multiplications during inference, leading to a larger constant factor in\nruntime.\nTheorems Guaranteeing the Complexity of Spline Learned Sort.WhenM Splineis used as the\nbucketing method in our learned sort framework, the same complexity guarantees as in PCF Learned Sort\nhold. Inparticular, weobtainthefollowingworst-caseguarantee(TheoremA.8)andexpected-caseguarantee\n(Theorem A.9).\nTheoremA.8(SplineversionofTheorem3.5).IfM Splineis the bucketing method, the worst-case complexity\nof the standard sort isO(nU(n))(whereU(n)is a non-decreasing function), andŒ±=Œ≤=Œ≥=Œ¥=‚åän3/4‚åã,\nthen the worst-case complexity of Spline Learned Sort isO(nU(n) +nlog logn).\nProof.The proof is identical to that of Theorem 3.5, using Theorem 3.1.\n25\n\nPublished in Transactions on Machine Learning Research (10/2025)\nTheorem A.9(Spline version of Theorem 3.6).LetœÉ 1andœÉ 2be the lower and upper bounds, respectively,\nof the probability densityf(x)onD, and assume0< œÉ 1‚â§œÉ 2<‚àû. IfM Splineis the bucketing method,\nthe expected complexity of the standard sort isO(nlogn), andŒ±=Œ≤=Œ≥=Œ¥=‚åän3/4‚åã, then the expected\ncomplexity of Spline Learned Sort isO(nlog logn).\nTo prove this theorem, we first establish the following lemma.\nLemma A.10(Spline version of Theorem A.1).Definee,S r,Tr,Yjr,Zjr,Yr,Zrexactly as in Theorem A.1\n(j= 1,...,Œ±, r= 1,...,n). That is, lete‚ààInbe the sorted version ofx I‚ààInand set‚àÜ := (x max‚àíxmin)/Œ≤\n(wherex minandx maxare the minimum and maximum values ofx I, respectively). Also define\nSr={k|e max(1,r‚àíŒ¥/2) + ‚àÜ<ek‚â§er‚àí‚àÜ},T r={k|er+ ‚àÜ‚â§ek<emin(r+Œ¥/2,n)‚àí‚àÜ},(49)\nand\nYjr=/braceleftÔ£¨igg\n1 (j‚ààS r)\n0 (otherwise), Zjr=/braceleftÔ£¨igg\n1 (j‚ààT r)\n0 (otherwise), Yr=Œ±/summationdisplay\nj=1Yjr, Zr=Œ±/summationdisplay\nj=1Zjr.(50)\nWhen usingM Spline, if the size of the bucket to whiche ris assigned is at leastŒ¥, then the same condition\nas in Equation(19)holds; namely,\n/parenleftÔ£¨ig\nr‚â•Œ¥\n2+ 1‚àßYr‚â§/floorleftÔ£¨ig\nŒ±\nŒ≥/floorrightÔ£¨ig/parenrightÔ£¨ig\n‚à®/parenleftÔ£¨ig\nr‚â§n‚àíŒ¥\n2‚àßZr‚â§/floorleftÔ£¨ig\nŒ±\nŒ≥/floorrightÔ£¨ig/parenrightÔ£¨ig\n.(51)\nProof.The proof proceeds exactly as in Theorem A.1, by contraposition. That is, assume/parenleftÔ£¨ig\nr<Œ¥\n2+ 1‚à®Yr>‚åäŒ±\nŒ≥‚åã/parenrightÔ£¨ig\n‚àß/parenleftÔ£¨ig\nr>n‚àíŒ¥\n2‚à®Zr>‚åäŒ±\nŒ≥‚åã/parenrightÔ£¨ig\n, and prove thate ris assigned to a bucket smaller\nthanŒ¥.\nFirst, we show thate rande min(r+Œ¥/2,n+1) are assigned to different buckets. As in the proof of Theorem A.1,\nthe casen‚àíŒ¥/2<r‚â§nis immediate; hence we focus onr‚â§n‚àíŒ¥/2. The bucket ID ofe ris‚åäÀúF(er)Œ≥‚åã+ 1,\nwhile that ofe r+Œ¥/2is‚åäÀúF(er+Œ¥/2 )Œ≥‚åã+1. By the definition of the spline-based CDF model and the inequalities\nti(er)+1‚â§er+ ‚àÜ, ti(er+Œ¥/2 )‚â•er+Œ¥/2‚àí‚àÜ, we obtain\nÀúF(er)‚â§F a(ti(er)+1) =1\nŒ±/vextendsingle/vextendsingle{j|aj‚â§ti(er)+1}/vextendsingle/vextendsingle‚â§1\nŒ±|{j|aj‚â§er+ ‚àÜ}|,(52)\nÀúF(er+Œ¥/2 )‚â•F a(ti(er+Œ¥/2 )) =1\nŒ±/vextendsingle/vextendsingle/vextendsingle{j|aj‚â§ti(er+Œ¥/2 )}/vextendsingle/vextendsingle/vextendsingle‚â•1\nŒ±/vextendsingle/vextendsingle{j|aj‚â§er+Œ¥/2‚àí‚àÜ}/vextendsingle/vextendsingle.(53)\nHence, their bucket IDs satisfy\n‚åäÀúF(er)Œ≥‚åã+ 1‚â§Œ≥\nŒ±|{j|aj‚â§er+ ‚àÜ}|+ 1,(54)\n‚åäÀúF(er+Œ¥/2 )Œ≥‚åã+ 1>Œ≥\nŒ±/vextendsingle/vextendsingle{j|aj‚â§er+Œ¥/2‚àí‚àÜ}/vextendsingle/vextendsingle,(55)\nwhich are identical to Equations (20) and (21). Thus,\n/parenleftbig\n‚åäÀúF(er+Œ¥/2 )Œ≥‚åã+ 1/parenrightbig\n‚àí/parenleftbig\n‚åäÀúF(er)Œ≥‚åã+ 1/parenrightbig\n>0,(56)\nand hencee rander+Œ¥/2are assigned to different buckets.\nBy a symmetric argument,e max(0,r‚àíŒ¥/2) anderare also assigned to different buckets. Therefore, the bucket\ncontaininge rhas size at mostŒ¥‚àí1, completing the contraposition.\nFrom Lemma A.10, we obtain the following theorem.\nTheorem A.11(Spline version of Theorem 3.4).LetœÉ 1andœÉ 2be the lower and upper bounds of the\nprobability densityf(x)onD, and assume0<œÉ 1‚â§œÉ2<‚àû(i.e.,œÉ 1‚â§f(x)‚â§œÉ 2for allx‚ààD). Then, in\nmodel-based bucketing ofx I‚ààIninto{cj}Œ≥+1\nj=1usingM Spline, the following holds for any intervalI‚äÜD:\nK:=Œ≥Œ¥\n2n‚àí2œÉ2Œ≥\nœÉ1Œ≤‚â•1‚áíPr[‚àÉj,|c j|>Œ¥]‚â§2n\nŒ¥exp/braceleftbigg\n‚àíŒ±K\n2Œ≥/parenleftbig\n1‚àí1\nK/parenrightbig2/bracerightbigg\n.(57)\nProof.The proof is identical to that of Theorem 3.4, using Lemma A.10.\n26\n\nPublished in Transactions on Machine Learning Research (10/2025)\nTable 1: Statistics of real-world datasets used in our experiments.\nDataset Size # Unique Values % of Duplicates\nChicago [Start] (Chicago, 2021) 39,588,772 216,610 99.45\nChicago [Tot] (Chicago, 2021) 39,199,154 6,887 99.98\nNYC [Pickup] (nyc, 2020) 100,000,000 26,666,741 73.33\nNYC [Dist] (nyc, 2020) 200,107,656 2,060 100.00\nNYC [Tot] (nyc, 2020) 199,964,459 7,428 100.00\nSOF [Humidity] (Mavrodiev, 2019) 96,318,228 8,128 99.99\nSOF [Pressure] (Mavrodiev, 2019) 96,317,180 862,502 99.10\nSOF [Temperature] (Mavrodiev, 2019) 96,432,856 5,638 99.99\nWiki (Marcus et al., 2020) 200,000,000 90,437,011 54.78\nOSM (Marcus et al., 2020) 800,000,000 799,469,195 0.07\nBooks (Marcus et al., 2020) 800,000,000 799,994,961 0.00\nFace (Marcus et al., 2020) 199,998,000 199,998,000 0.00\nStocks [Volume] (Onyshchak, 2020) 27,596,686 325,654 98.82\nStocks [Open] (Onyshchak, 2020) 27,596,686 830,758 96.99\nStocks [Date] (Onyshchak, 2020) 27,596,686 14,646 99.95\nStocks [Low] (Onyshchak, 2020) 27,596,686 863,701 96.87\nFinally, using Theorem A.11, we can prove Theorem A.9.\nProof of Theorem A.9.The proof follows exactly the same steps as Theorem 3.6, applying Theorem A.11 to\nTheorem 3.3.\nB Real Dataset Details\nIn the main text, we provided only a concise list of the real datasets used in our experiments. Here we\ndescribe each dataset in more detail.\n‚Ä¢Chicago [Start, Tot]: The Chicago Taxi Trips dataset includes taxi trips reported to the City of\nChicago in its role as a regulatory agency over the last six years. The data to be sorted includes\ntrip starting timestamps and total fare amounts.\n‚Ä¢NYC [Pickup, Dist, Tot]: The New York City yellow taxi trip dataset includes trip pickup\ndatetimes, trip distances, and total fare amounts.\n‚Ä¢SOF [Humidity, Pressure, Temperature]: The Sofia dataset contains time-series air quality\nmetrics (humidity, pressure, and temperature) measured at 1-minute intervals from outdoor sensors\nin Sofia, Bulgaria.\n‚Ä¢Wiki: The Wikipedia dataset contains article edit timestamps (Marcus et al., 2020).\n‚Ä¢OSM:UniformlysampledOpenStreetMaplocationsrepresentedasGoogleS2CellIds(Marcusetal.,\n2020).\n‚Ä¢Books: Book sale popularity data from Amazon (Marcus et al., 2020).\n‚Ä¢Face: The FB dataset contains an upsampled set of Facebook user IDs obtained via random walks\non the FB social graph (Marcus et al., 2020). As in (Kristo et al., 2021a; Ferragina & Odorisio,\n2025), the outliers greater than the 0.99999 quantile are discarded.\n‚Ä¢Stocks [Volume, Open, Date, Low]: The Stocks dataset contains historical daily opening, low\nprices, trading volumes, and dates for all NASDAQ tickers (stocks and ETFs), retrieved via the\nyfinance Python package up to April 1, 2020.\n27\n\nPublished in Transactions on Machine Learning Research (10/2025)\nFigure 5: Time to sort the array in adversarial environments. Below each graph is a histogram that visualizes\nthe distribution of each dataset. The standard deviation of the10measurements is represented by the shaded\narea.\nFurthermore, the detailed statistics of these datasets, including their sizes, the number of unique values,\nand the percentage of duplicates, are summarized in Table 1. The percentage of duplicates is computed\nas100√ó/parenleftÔ£¨ig\n1‚àí|unique_values|\nn/parenrightÔ£¨ig\n, wherenis the total number of elements andunique_valuesis the set of\ndistinct elements. In particular, for several datasets (Chicago [Start, Tot], NYC [Dist, Tot], SOF [Humidity,\nPressure, Temperature], and Stocks [Volume, Open, Date, Low]), the number of unique values is extremely\nsmall, accounting for less than 3.2% of the total elements.\nC Experiments in Adversarial Environments\nTo evaluate the robustness of our Learned Sort, we conducted experiments under adversarially constructed\ninputs that explicitly violate the assumptions of Theorem 3.3. Specifically, given an original dataset of size\nn, we injectednduplicate elements of a randomly chosen value inside the range[x min,xmax], wherex minand\nxmaxis the minimun and maximum value of the input arrayx. As a result, the array length doubled to2n,\nand the constructed distribution contained a point mass of probability one-half, leading to a huge probability\ndensity at that value. This setting intentionally breaks the assumptionœÉ 2<‚àûrequired in Theorem 3.3.\n28\n\nPublished in Transactions on Machine Learning Research (10/2025)\nFigure 5 reports the sorting time on such adversarial datasets. The results show that the expected bound\nofO(nlog logn)for our PCF Learend Sort no longer holds once the assumptions are violated, and the\nperformance of PCF Learned Sort occasionally degrades to the level ofstd::sort. However, we consistently\nobservedthatPCFLearnedSortneverexceededtheworst-casecomplexityofO(nlogn), andthusitsruntime\nremained comparable tostd::sort.\nOn the other hand, we found that Learned Sort 2.0 (Kristo et al., 2021b), which hasO(n2)worst-case\ncomplexity, sometimes exhibited severe slowdowns. For example, on data of sizen= 105sampled from\na Normal distribution with injected duplicates, the sorting time of Learned Sort 2.0 reached up to 10.89\nseconds, whereas PCF Learned Sort required at most 0.021 seconds.\nWe also found that BLS, ULS, and Learned Sort 2.1 (Ferragina & Odorisio, 2025), which also haveO(n2)\nworst-case complexity, sometimes exhibited substantial performance degradation. For example, on data of\nsizen= 106sampled from a Normal distribution with injected point masses, Learned Sort 2.1 took up to\n0.154 seconds, compared with 0.042 seconds for PCF Learned Sort and 0.078 seconds forstd::sort.\nThese results highlight the fragility of sorting algorithms without strong worst-case complexity guarantees, a\nlimitationsharedbymanyexistinglearnedsortingalgorithms. Theyunderscoretheimportanceofdeveloping\nlearned sorting algorithms‚Äîsuch as our PCF Learned Sort‚Äîthat combine practical efficiency with rigorous\nworst-case complexity guarantees.\nD Runtime Profiling\nTo better understand the practical behavior of PCF Learned Sort, we profiled the runtime of its major\ncomponents: (i) model training, (ii) bucketing, (iii) standard sort applied to buckets that are too small\n(<œÑ), and (iv) standard sort applied to buckets that are too large (‚â•Œ¥). The results are shown in Figure 6.\nForsmallinputsizesn, themajorityoftheruntimeistypicallyspentonapplyingthestandardsorttobuckets\nsmaller thanœÑ. Asnincreases, the relative cost shifts, and most of the runtime is instead consumed by the\nbucketing stage. Model training remains consistently lightweight, comparable to or faster than bucketing,\nand never exceeds roughly one quarter of the total runtime. Thus, training does not become the dominant\nfactor in practice.\nFigure7showstheresultsforadversariallyclusteredinputs(asdescribedinSectionC).Undertheadversarial\nsetting, the bucketing algorithm more frequently produces large buckets, which are then handled by standard\nsort. In these cases, a larger fraction of the runtime is attributed to this fallback mechanism.\nOverall, these analyses clarify which components of PCF Learned Sort dominate the runtime under different\nconditions. They also highlight that the algorithm adapts gracefully: training and bucketing scale well, while\nthe worst-case fallback ensures robustness without catastrophic slowdowns.\n29\n\nPublished in Transactions on Machine Learning Research (10/2025)\nFigure 6: Breakdown of runtime components in PCF Learned Sort. The total runtime is decomposed into\n(i) model training, (ii) bucketing, (iii) standard sort applied to buckets smaller thanœÑ, and (iv) standard\nsort applied to buckets larger thanŒ¥.\n30\n\nPublished in Transactions on Machine Learning Research (10/2025)\nFigure 7: Breakdown of runtime components in PCF Learned Sort in adversarial environments. The total\nruntime is decomposed into (i) model training, (ii) bucketing, (iii) standard sort applied to buckets smaller\nthanœÑ, and (iv) standard sort applied to buckets larger thanŒ¥.\n31",
  "textLength": 87240
}