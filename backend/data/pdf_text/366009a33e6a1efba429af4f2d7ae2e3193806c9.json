{
  "paperId": "366009a33e6a1efba429af4f2d7ae2e3193806c9",
  "title": "Consistent and Flexible Selectivity Estimation for High-Dimensional Data",
  "pdfPath": "366009a33e6a1efba429af4f2d7ae2e3193806c9.pdf",
  "text": "Consistent and Flexible Selectivity Estimation for\nHigh-Dimensional Data\nYaoshu Wang1, Chuan Xiao2,3, Jianbin Qin1, Rui Mao1, Makoto Onizuka2, Wei Wang4,5, Rui Zhang6,\nand Yoshiharu Ishikawa3\n1Shenzhen Institute of Computing Sciences, Shenzhen University,2Osaka University,3Nagoya University,4Dongguan\nUniversity of Technology,5University of New South Wales,6www.ruizhang.info\nyaoshuw@sics.ac.cn,{chuanx,onizuka}@ist.osaka-\nu.ac.jp,{qinjianbin,mao}@szu.edu.cn,weiw@cse.unsw.edu.au,rui.zhang@ieee.org,ishikawa@i.nagoya-u.ac.jp\nABSTRACT\nSelectivity estimation aims at estimating the number of database\nobjects that satisfy a selection criterion. Answering this problem\naccurately and efficiently is essential to many applications, such as\ndensity estimation, outlier detection, query optimization, and data\nintegration. The estimation problem is especially challenging for\nlarge-scale high-dimensional data due to the curse of dimension-\nality, the large variance of selectivity across different queries, and\nthe need to make the estimator consistent (i.e., the selectivity is\nnon-decreasing in the threshold). We propose a new deep learning-\nbased model that learns a query-dependent piecewise linear function\nas selectivity estimator, which is flexible to fit the selectivity curve\nof any distance function and query object, while guaranteeing that\nthe output is non-decreasing in the threshold. To improve the ac-\ncuracy for large datasets, we propose to partition the dataset into\nmultiple disjoint subsets and build a local model on each of them.\nWe perform experiments on real datasets and show that the pro-\nposed model consistently outperforms state-of-the-art models in\naccuracy in an efficient way and is useful for real applications.\nCCS CONCEPTS\nâ€¢Information systems â†’Query optimization ;Entity resolu-\ntion;â€¢Computing methodologies â†’Neural networks .\nKEYWORDS\nselectivity estimation; high-dimensional data; piecewise linear func-\ntion; deep neural network\n1 INTRODUCTION\nIn this paper, we consider the following selectivity estimation prob-\nlem for high-dimensional data: given a query object x, a distance\nfunctionğ‘‘ğ‘–ğ‘ ğ‘¡(Â·,Â·), and a distance threshold ğ‘¡, estimate the number\nof objects os in a database that satisfy ğ‘‘ğ‘–ğ‘ ğ‘¡(x,o)â‰¤ğ‘¡. This prob-\nlem is also known as local density estimation [ 54] or spherical\nrange counting [ 9] in theoretical computer science. It is an essen-\ntial procedure in density estimation in statistics [ 52] and density-\nbased outlier detection [ 11] in data mining. For example, for text\nanalysis, one may want to determine the popularity of a topic;\nfor e-commerce, an analyst may want to find out if a user/item\nis an outlier; for clustering, the algorithm may converge faster if\nwe start with seeds in denser regions. In the database area, accu-\nrate estimation helps to find an optimal query execution plan in\nW. Wang, R. Mao and J. Qin are the joint corresponding authors.databases dealing with high-dimensional data [ 24]. Hands-off en-\ntity matching systems [16] extract paths from random forests and\ntake each path â€“ a conjunction of similarity predicates over multi-\nple attributes (e.g., â€œ EU(name)â‰¤0.25AND EU(affiliations)â‰¤0.4\nAND EU(research interests)â‰¤0.45â€, where EU()measures the Eu-\nclidean distance between word embeddings) â€“ as a blocking rule,\nand efficient blocking can be achieved if we find a good query exe-\ncution plan [ 50]. In addition, many text or image retrieval systems\nresort to distributed representations. Given a query, a similarity\nselection is often invoked to obtain a set of candidates to be fur-\nther verified by sophisticated models. Estimating the number of\ncandidates helps to estimate the overall query processing time an\nend-to-end system to create a service level agreement.\nSelectivity estimation for large-scale high-dimensional data is\nstill an open problem due to the following factors: (1) Large vari-\nance of selectivity . The selectivity varies across queries and may\ndiffer by several orders of magnitude. A good estimator is supposed\nto predict accurately for both small and large selectivity values.\n(2)Curse of dimensionality . Many methods that work well on low-\ndimensional data, such as histograms [ 26], are intractable when\nwe seek an optimal solution, and they significantly lose accuracy\nwith the increase of dimensionality. (3) Consistency requirement .\nWhen the query object is fixed, selectivity is non-decreasing in the\nthreshold. Hence users may want the estimated selectivity to be\nnon-decreasing and interpretable in applications such as density\nestimation. This requirement rules out many existing methods.\nTo address the above challenges, we propose a novel deep re-\ngression method that guarantees consistency. We holistically ap-\nproximate the selectivity curve using a query-dependent piecewise\nlinear function consisting of control points that are learned from\ntraining data. This function family is flexible in the sense that it can\nclosely approximate the selectivity curve of any distance function\nand any input query object; e.g., using more control points for the\npart of the curve where selectivity changes rapidly. Together with\na robust loss function, we are able to alleviate the impact of large\nvariance across different queries. To handle high dimensionality,\nwe incorporate an autoencoder that learns the latent representation\nof the query object with respect to the data distribution. The query\nobject and its latent representation are fed to a query-dependent\ncontrol point model, enhancing the fit to the selectivity curve of the\nquery object. To ensure consistency, we achieve the monotonicity\nof estimated selectivity by converting the problem to a standard\nneural network prediction task, rather than imposing additional\nlimitations such as restricting weights to be non-negative [ 15] or\nlimiting to multi-linear functions [ 17]. To improve the accuracyarXiv:2005.09908v4  [cs.DB]  27 May 2021\n\nYaoshu Wang1, Chuan Xiao2,3, Jianbin Qin1, Rui Mao1, Makoto Onizuka2, Wei Wang4,5, Rui Zhang6, and Yoshiharu Ishikawa3\non large-scale datasets, we propose a partition-based method to\ndivide the database into disjoint subsets and learn a local model on\neach of them. Since update may exists in the database, we employ\nincremental learning to cope with this issue.\nWe perform experiments on six real datasets. The results show\nthat our method outperforms various state-of-the-art models. Com-\npared to the best existing model [ 50], the improvement of accuracy\nis up to 5 times in mean squared error and consistent across datasets,\ndistance functions, and error metrics. The experiments also demon-\nstrate that our method is competitive in estimation speed, robust\nagainst update in the database, and useful in estimating overall\nquery processing time in a semantic search application.\n2 RELATED WORK\nTraditional Estimation Models. Selectivity estimation has been ex-\ntensively studied in database systems, where prevalent approaches\nare based on sampling [ 53,55], histograms [ 26], or sketches [ 14].\nHowever, few of them are applicable to high-dimensional data due\nto data sparsity or the curse of dimensionality. For cosine similarity,\nWuet al. [54] proposed to use locality-sensitive hashing (LSH) as\na means of importance sampling to tackle data sparsity. Kernel\ndensity estimation ( KDE ) [24,36] has been proposed to handle se-\nlectivity estimation in metric space. Mattig et al. [36] proposed to\nalleviat the curse of dimensionality by focusing on the distribution\nin metric space. However, strong assumptions are usually imposed\non the kernel function (e.g., only diagonal covariance matrix for\nGaussian kernels), and one kernel function may be inadequate to\nmodel complex distributions in high-dimensional data.\nRegression Models without Consistency Guarantee. Selectivity es-\ntimation can be formalized as a regression problem with query\nobject and threshold as input features, if the consistent require-\nment is not enforced. A representative approach is quantized re-\ngression [ 7,8]. Recent trend uses deep regression models. Vanilla\ndeep regression [ 32,46,47] learns good representations of input\npatterns. The mixture of expert model ( MoE ) [43] has a sparsely-\ngated mixture-of-experts layer that assigns data to proper experts\n(models) which lead to better generalization. The recursive model\nindex ( RMI) [31] is a regression model that can be used to replace\nthe B-tree index in relational databases. Deep regression has also\nbeen used to predict selectivities (cardinalities) [ 29,45] in relational\ndatabases, amid a set of recent advances in learning methods for this\ntask [ 23,38,39,48,56]. They target SQL queries where each predi-\ncate involves one attribute. [ 23,56] employ autoregressive models.\n[39] only deals with low dimensionality. [ 29,38,45] become a deep\nneural network if we regard a vector as an attribute.\nModels with Consistency Guarantee. Gradient boosting trees (e.g.,\nXGBoost [13] and LightGBM [49]) support monotonic regression.\nLattice regression [ 17,19,21,57] uses a multi-linearly interpolated\nlookup table for regression. By enforcing constraints on its param-\neter values, it can guarantee monotonicity. To accommodate high\ndimensional inputs, Fard et al. [17] proposed to build an ensem-\nble of lattice using subsets of input features. Deep lattice network\n(DLN ) [57] was proposed to interlace non-linear calibration layers\nand ensemble of lattice layers. Recently, lattice regression has alsobeen used to learn a spatial index [ 33].UMNN [51] is an autore-\ngressive flow model which adopts Clenshaw-Curtis quadrature to\nachieve monotonicity. Other monotonic models include isotonic\nregression [ 22,44] and MinMaxNet [15].CardNet [50] is a recently\nproposed method for monotonic selectivity estimation of similarity\nselection query for various data types. It maps original data to bi-\nnary vectors and the threshold to an integer ğœ, and then predicts\nthe selectivity for distance [0,1,...,ğœ]respectively with(ğœ+1)\nencoder-decoder models. When applying to high-dimensional data,\nits has the following drawbacks: the mapping from the input thresh-\nold toğœis not injective, i.e., multiple thresholds may be mapped\nto the same ğœand the same selectivity is always output for them;\nthe overall accuracy is significantly affected if one of the (ğœ+1)\ndecoders is not accurate for some query.\n3 PRELIMINARIES\nProblem 1 (Selectivity Estimation for High-Dimensional\nData). Given a database of ğ‘‘-dimensional vectors D={oğ‘–}ğ‘›\nğ‘–=1,oğ‘–âˆˆ\nRğ‘‘, a distance function ğ‘‘ğ‘–ğ‘ ğ‘¡(Â·,Â·), a scalar threshold ğ‘¡, and a query object\nxâˆˆRğ‘‘, estimate the selectivity in the database, i.e., |{o|ğ‘‘ğ‘–ğ‘ ğ‘¡(x,o)â‰¤\nğ‘¡,oâˆˆD}| .\nWhile we assume ğ‘‘is a distance function, it is easy to extend\nit to consider ğ‘‘as a similarity function by changing â‰¤toâ‰¥in the\nabove definition. In the rest of the paper, to describe our method,\nwe focus on the case when ğ‘‘is a distance function. In addition, the\nquery object does not have to be in the database, and we do not\nmake any assumption on the distance function, meaning that the\nfunction does not have to be metric.\nWe can view the selectivity (i.e., the ground truth label) ğ‘¦of a\nquery object xand a threshold ğ‘¡as generated by a function ğ‘¦=\nğ‘“(x,ğ‘¡,D). We callğ‘“thevalue function . Our goal is to estimate\nğ‘“(x,ğ‘¡,D)using another function Ë†ğ‘“(x,ğ‘¡,D).\nOne unique requirement of our problem is that the estimator\nË†ğ‘“needs to be consistent :Ë†ğ‘“is consistent if and only if it is non-\ndecreasing in the threshold ğ‘¡for every query object x; i.e.,âˆ€x,\nË†ğ‘“(x,ğ‘¡â€²,D)â‰¥ Ë†ğ‘“(x,ğ‘¡,D)iff.ğ‘¡â€²â‰¥ğ‘¡.\n4 OBSERVATIONS AND IDEAS\nWhen|D|is large, it is hard to estimate ğ‘“directly. One of the main\nchallenges is that ğ‘“may be non-smooth with respect to the input\nvariables. In the worst case, we have:\nâ€¢For any vector Î”x, there exists a database Dofğ‘›objects and a\nquery(x,ğ‘¡)such thatğ‘“(x,ğ‘¡,D)=0andğ‘“(x+Î”x,ğ‘¡,D)=ğ‘›.\nâ€¢For anyğœ–>0, there exists a database Dofğ‘›objects and a query\n(x,ğ‘¡)such thatğ‘“(x,ğ‘¡,D)=0andğ‘“(x,ğ‘¡+ğœ–,D)=ğ‘›.\nThis means any model that directly approximates ğ‘“is hard.\nOur idea to mitigate this issue is: instead of estimating one func-\ntionğ‘“, we estimate multiple functions such that each functionâ€™s\noutput range is a small fraction of the selectivity ğ‘¦. For example,\nsupposeğ‘¦=ğ‘¦1+ğ‘¦2andğ‘¡=ğ‘¡1+ğ‘¡2. Ifğ‘¦1andğ‘¦2are approximately\nlinear in[0,ğ‘¡1]and(ğ‘¡1,ğ‘¡2], respectively, but with different slopes,\nthen we can use two linear models for the two threshold ranges.\nWe may also exploit this idea and divide ğ‘¦with disjoint subsets of\nD. Hence we adopt the following two partitioning schemes.\n\nConsistent and Flexible Selectivity Estimation for High-Dimensional Data\nThreshold Partitioning. Assume the maximum threshold we sup-\nport isğ‘¡max. We consider dividing it with an increasing sequence of\n(ğ¿+2)values:[ğœ0,ğœ1,...,ğœğ¿+1]such thatğœğ‘–<ğœğ‘—ifğ‘–<ğ‘—,ğœ0=0, and\nğœğ¿+1=ğ‘¡max+ğœ–, whereğœ–is a small positive quantity1. Letğ‘”ğ‘–(x,ğ‘¡)\nbe an interpolant function for interval [ğœğ‘–âˆ’1,ğœğ‘–). Then we have\nË†ğ‘“(x,ğ‘¡,D)=ğ¿+1âˆ‘ï¸\nğ‘–=11Jğ‘¡âˆˆ[ğœğ‘–âˆ’1,ğœğ‘–)KÂ·ğ‘”ğ‘–(x,ğ‘¡), (1)\nwhere 1JKdenotes the indicator function.\nData Partitioning. We partition the database Dintoğ¾disjoint\npartsD1,...,Dğ¾, and letğ‘“ğ‘–denote the value function defined on\ntheğ‘–-th part. Then we have Ë†ğ‘“(x,ğ‘¡,D)=Ãğ¾\nğ‘–=1Ë†ğ‘“ğ‘–(x,ğ‘¡,Dğ‘–).\n5 SELECTIVITY ESTIMATOR\n5.1 Threshold Partitioning\nOur idea is to approximate ğ‘“using a regression model Ë†ğ‘“(x,ğ‘¡,D;Î˜).\nRecall the sequence [ğœ0,ğœ1,...,ğœğ¿+1]in Section 4. We consider the\nfamily of continuous piecewise linear function to implement the\ninterpolation ğ‘”ğ‘–(x,ğ‘¡),ğ‘–âˆˆ[0,ğ¿+1]. A piecewise linear function is a\ncontinuous function of (ğ¿+1)pieces, each being a linear function\ndefined on[ğœğ‘–âˆ’1,ğœğ‘–). Theğœğ‘–values are called control points . Given a\nquery object x, letğ‘ğ‘–denote the estimated selectivity for a threshold\nğœğ‘–. For theğ‘”ğ‘–function in Eq. (1), we have\nğ‘”ğ‘–(x,ğ‘¡)=ğ‘ğ‘–âˆ’1+ğ‘¡âˆ’ğœğ‘–âˆ’1\nğœğ‘–âˆ’ğœğ‘–âˆ’1Â·(ğ‘ğ‘–âˆ’ğ‘ğ‘–âˆ’1). (2)\nHence the regression model is parameterized by Î˜def={(ğœğ‘–,ğ‘ğ‘–)}ğ¿+1\nğ‘–=0.\nNote thatğœğ‘–andğ‘ğ‘–values are dependent on x; i.e., the piecewise\nlinear function is query-dependent.\nUsing the above design for Î˜has the following property to guar-\nantee the consistency2.\nLemma 1. Given a databaseDand a query object x, ifğ‘ğ‘–â‰¥ğ‘ğ‘–âˆ’1\nforâˆ€ğ‘–âˆˆ[1,ğ¿+1], then Ë†ğ‘“(x,ğ‘¡,D;Î˜)is non-decreasing in ğ‘¡.\nAnother salient property of our model is that it is flexible in\nthe sense that it can arbitrarily well approximate the selectivity\ncurve. Piecewise linear functions have been well explored to fit\none-dimensional curves [ 40]. With a sufficient number of control\npoints, one can find an optimal piecewise linear function to fit\nany one-dimensional curve. The idea is that a small range of input\nis highly likely to be linear with the output. When xandDare\nfixed, the selectivity only depends on ğ‘¡, and thus the value function\ncan be treated as a one-dimensional curve. To distinguish different\nx, we will design a deep learning approach to learn good control\npoints and corresponding selectivities. As such, our model not only\ninherits the good performance of piecewise linear function but also\nhandles different query objects.\nEstimation Loss. In the regression model, the ğ¿ ğœğ‘–values and\nthe(ğ¿+2)ğ‘ğ‘–values are the parameters to be learned. We use the\nexpected loss between ğ‘“and Ë†ğ‘“:\nğ½est(Ë†ğ‘“)=âˆ‘ï¸\n((x,ğ‘¡),ğ‘¦)âˆˆT trainâ„“(ğ‘“(x,ğ‘¡,D),Ë†ğ‘“(x,ğ‘¡,D)), (3)\n1ğœ–is used to cover the corner case of ğ‘¡=ğ‘¡maxin Eq. (1).\n2Proof is provided in Appendix A.whereTtrain denotes the set of training data, and â„“(ğ‘¦,Ë†ğ‘¦)is a loss\nfunction between the true selectivity ğ‘¦and the estimated value\nË†ğ‘¦of a query(x,ğ‘¡). We choose the Huber loss [ 25] applied to the\nlogarithmic values of ğ‘¦and Ë†ğ‘¦. To prevent numeric errors, we also\npad the input by a small positive quantity ğœ–. Letğ‘Ÿdef=ln(ğ‘¦+ğœ–)âˆ’\nln(Ë†ğ‘¦+ğœ–). Then\nâ„“(ğ‘¦,Ë†ğ‘¦)=(ğ‘Ÿ2\n2, if|ğ‘Ÿ|â‰¤ğ›¿;\nğ›¿(|ğ‘Ÿ|âˆ’ğ›¿\n2), otherwise.\nğ›¿is set to 1.345, the standard recommended value [ 18]. The reason\nfor designing such a loss function is that the selectivity may differ\nby several orders of magnitude for different queries. If we use the\nâ„“2loss, it encourages the model to fit large selectivities well, and\nif we useâ„“1loss, it pays more attention to small selectivities. To\nachieve robust prediction, we reduce the value range by logarithm\nand the Huber loss.\n5.2 Learning Piecewise Linear Function\nWe choose a deep neural network to learn the piecewise linear\nfunction. It has the following advantages: (1) Deep learning is able\nto capture the complex patterns in control points and corresponding\nselectivities for accurate estimation of different queries. (2) Deep\nlearning generalizes well on queries that are not covered by training\ndata. (3) The training data for our problem can be unlimitedly\nacquired by running a selection algorithm on the database, and this\nfavors deep learning which often requires large training sets.\nIn our model, ğœğ‘–andğ‘ğ‘–values are generated separately for the\ninput query object. We also require non-negative increments be-\ntween consecutive parameters to ensure they are non-decreasing.\nIn the following, we explain the learning of ğœğ‘–s andğ‘ğ‘–s, followed\nby the overall neural network architecture.\nControl Points ( ğœğ‘–s).We learn the increments between ğœğ‘–s.\nğœğ‘–(x)=ğ‘–âˆ’1âˆ‘ï¸\nğ‘—=0Î”ğœ(x)[ğ‘—], (4)\nwhereÎ”ğœ(x)=Normğ‘™2(ğ‘”(ğœ)(x))Â·ğ‘¡ğ‘šğ‘ğ‘¥. (5)\nNormğ‘™2is a normalized squared function defined as\nNormğ‘™2(t)=[ğ‘¡2\n1+ğœ–\nğ¿\ntTt+ğœ–,...,ğ‘¡2\nğ¿+ğœ–\nğ¿\ntTt+ğœ–],\nwhereğœ–is a small positive quantity to avoid dividing by zero, and\nğ‘¡ğ‘–denotes the value of the ğ‘–-th dimension of t. The model takes\nxas input and outputs ğ¿distinct thresholds in (0,ğ‘¡max).ğ‘”(ğœ)is\nimplemented by a neural network. Then we have a vector ğœ=\n[0;ğœ1;ğœ2;...;ğœğ¿;ğ‘¡ğ‘šğ‘ğ‘¥].\nOne may consider using Softmax(t), which is widely used for\nmulti-classification and (self-)attention. We choose Normğ‘™2(t)rather\nthan Softmax(t)for the following reasons: (1) Due to the exponen-\ntial function in Softmax(t), a small change of tmight lead to large\nvariations of the output. (2) Softmax aims to highlight the impor-\ntant part rather than partitioning t, while our goal is to rationally\npartition the range [0,ğœğ‘šğ‘ğ‘¥]into several intervals such that the\npiecewise linear function can fit well.\n\nYaoshu Wang1, Chuan Xiao2,3, Jianbin Qin1, Rui Mao1, Makoto Onizuka2, Wei Wang4,5, Rui Zhang6, and Yoshiharu Ishikawa3\nxxxzAEFFNM/u1D70Fptâˆ‘*\nReLUNorml2y tmaxSMpsumS\nFigure 1: Network architecture.\nSelectivities at Control Points ( ğ‘ğ‘–s).We learn(ğ¿+2)ğ‘ğ‘–values in a\nsimilar fashion to control points, using another neural network to\nimplementğ‘”(ğ‘).\nğ‘ğ‘–(x)=ğ‘–âˆ‘ï¸\nğ‘—=0Î”ğ‘(x)[ğ‘—], (6)\nwhereÎ”ğ‘(x)=ReLU(ğ‘”(ğ‘)(x)). (7)\nThen we have a vector p=[ğ‘0;ğ‘1;...;ğ‘ğ¿+1]. Here, we learn(ğ¿+\n1)increments ( ğ‘ğ‘–âˆ’ğ‘ğ‘–âˆ’1) instead of directly learning (ğ¿+2)ğ‘ğ‘–s.\nThereby, we do not have to enforce a constraint ğ‘ğ‘–âˆ’1â‰¤ğ‘ğ‘–for\nğ‘–âˆˆ[1,ğ¿+1]in the learning process, and thus the learned model\ncan better fit the selectivity curve.\nNetwork Architecture. Figure 1 shows our network architecture.\nThe input xis first transformed to z, a latent representation ob-\ntained by an autoencoder (AE). The use of the AE encourages the\nmodel to exploit latent data and query distributions in learning the\npiecewise linear function, and this helps the model generalize to\nquery objects outside the training data. To learn the latent distri-\nbutions ofD, we pretrain the AE on all the objects of D, and then\ncontinue to train the AE with the queries in the training data. Due\nto the use of AE, the final loss function is a linear combination of\nthe estimation loss (Eq. (3)) and the loss of the AE for the training\ndata (denoted by ğ½AE):\nğ½(Ë†ğ‘“)=ğ½est(Ë†ğ‘“)+ğœ†Â·ğ½AE. (8)\nxis concatenated with z, i.e.,[x;z]. Then[x;z]is fed into two\nindependent neural networks: a feed-forward network ( FFN) and\na modelğ‘€(introduced later). Two multiplications, denoted by ğ‘†\noperators in Figure 1, are needed to separately convert the output of\nFFN and the output of model ğ‘€to the ğœandpvectors, respectively.\nThey use a scalar ğ‘¡maxand a matrix Mpsum which, once multiplied\non the right to a vector, perform prefix sum operation on the vector.\nMpsum=ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°1 0... 0\n1 1... 0\n............\n1 1... 1ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£».\nThe output of these networks, together with the threshold ğ‘¡,\nare fed into the operatorÃâˆ—in Figure 1, which is implemented by\nEqs. (2),(5), and (7), to compute the output of the piecewise linear\nfunction, i.e., the estimated selectivity.\nFigure 2: Data partitioning by cover tree.\nModelğ‘€.To achieve better performance, we learn pusing an\nencoder-decoder model. In the encoder, an FFN is used to generate\n(ğ¿+2)embeddings:\n[h0;h1;...;hğ¿+1]=FFN([x;z]), (9)\nwhere hğ‘–s are high-dimensional representations. Here, we adopt\n(ğ¿+2)embeddings, i.e., h0,..., hğ¿+1, to represent the latent infor-\nmation of p. In the decoder, we adopt (ğ¿+2)linear transformations\nwith the ReLU activation function:\nğ‘˜ğ‘–=ReLU(wT\nğ‘–hğ‘–+ğ‘ğ‘–).\nThen we have p=[ğ‘˜0,ğ‘˜0+ğ‘˜1,...,Ãğ¿+1\nğ‘–=0ğ‘˜ğ‘–].\n5.3 Data Partitioning\nTo improve the accuracy of estimation on large-scale datasets, we\ndivide the database into multiple disjoint subsets D1,...,Dğ¾with\napproximately the same size, and build a local model on each of\nthem. Let Ë†ğ‘“ğ‘–denote each local model. Then the global model for\nselectivity estimation is Ë†ğ‘“=Ã\nğ‘–Ë†ğ‘“ğ‘–.\nWe have considered several design choices and propose the fol-\nlowing configuration that achieves the best empirical performance:\n(1) Partitioning is obtained by a cover tree-based strategy. (2) We\nadopt the structure in Figure 1 so that all local models share the\nsame input representation [x;z], but each has its own neural net-\nworks to learn the control points.\nPartitioning Method. We utilize a cover tree [ 27] to partition\nDinto several parts. A partition ratio ğ‘Ÿis predefined such that\nthe cover tree will not expand its nodes if the number of inside\nobjects is smaller than ğ‘Ÿ|D|. Given a query(x,ğ‘¡), the valid region\nis the circles that intersect the circle with xas center and ğ‘¡as\nradius. For example, in Figure 2, x(the red point) and ğ‘¡form the\nred circle, and data are partitioned into 6 regions. The valid region\nof(x,ğ‘¡)is the green circles that intersect the red circle. Albeit\nimposing constraints, cover tree might still generate too many ball\nregions, i.e., leaf nodes, which lead to large number of parameters\nof the model and the difficulty of training. Reducing the number\nof ball regions is necessary. To remedy this, we adopt a merging\nstrategy as follows. First, we still partition Dintoğ¾â€²regions using\ncover tree. Then we cluster these regions into ğ¾(ğ¾â€²â‰¤ğ¾) clusters\nD1,...,Dğ¾by the following greedy strategy: The ğ¾â€²regions are\nsorted in decreasing order of the number of inside objects. We begin\nwithğ¾empty clusters. Then we scan each region and assign it to\nthe cluster with the smallest size. The regions that belong to the\nsame cluster are merged to one region. We consider an indicator\n\nConsistent and Flexible Selectivity Estimation for High-Dimensional Data\nğ‘“ğ‘:(x,ğ‘¡)â†’{ 0,1}ğ¾such thatğ‘“ğ‘(x,ğ‘¡)[ğ‘–]=1if and only if the\nquery(x,ğ‘¡)intersects clusterDğ‘–, and employ it in our model:\nË†ğ‘“(x,ğ‘¡,D)=ğ¾âˆ‘ï¸\nğ‘–=0ğ‘“ğ‘(x,ğ‘¡)[ğ‘–]Â·Ë†ğ‘“ğ‘–(x,ğ‘¡,Dğ‘–).\nSince cover trees deal with metric spaces, for non-metric functions\n(e.g, cosine similarity), if possible, we equivalently convert it to a\nmetric (e.g, Euclidean distance, as cos(u,v)=1âˆ’âˆ¥u,vâˆ¥2\n2for unit\nvectors uandv). Then the cover tree partitioning still works. For\nthose that cannot be equivalently converted to a metric, we adopt\nrandom partitioning and modify ğ‘“ğ‘asğ‘“ğ‘:(x,ğ‘¡)â†’{ 1}ğ¾.\nTraining Procedure. We have several choices on how to train the\nmodels from multiple partitions. The default is directly training the\nglobal model Ë†ğ‘“, with the advantage that no extra work is needed.\nThe other choice is to train each local model independently, using\nthe selectivity computed on the local partition as training label.\nWe propose yet another choice: we pretrain the local models for ğ‘‡\nepochs, and then train them jointly. In the joint training stage, we\nuse the following loss function:\nğ½joint=ğ½est(Ë†ğ‘“)+ğ›½Â·âˆ‘ï¸\nğ‘–ğ½est(Ë†ğ‘“ğ‘–)+ğœ†Â·ğ½AE.\nThe indicators ğ‘“ğ‘(Â·,Â·)s of all(x,ğ‘¡)are precomputed before training.\n5.4 Dealing with Data Updates\nWhen the database Dis updated with insertion or deletion, we first\ncheck whether our model Ë†ğ‘“(x,ğ‘¡,D)is necessary to update. In other\nwords, when minor updates occur and Ë†ğ‘“(x,ğ‘¡,D)is still accurate\nenough, we ignore them. To check the accuracy of Ë†ğ‘“(x,ğ‘¡,D), we\nupdate the labels of all validation data, and re-test the mean abso-\nlute error ( MAE ) ofË†ğ‘“(x,ğ‘¡,D). If the difference between the original\nMAE and the new one is no larger than a predefined threshold ğ›¿ğ‘ˆ,\nwe do not update our model. Otherwise, we adopt an incremental\nlearning approach as follows. First, we update the labels in the\ntraining and the validation data to reflect the update in the data-\nbase. Second, we continue training our model with the updated\ntraining data until the validation error ( MAE ) does not increase in\n3 consecutive epochs. Here the training does not start from scratch\nbut from the current model. We incrementally train our model with\nall the training data to prevent catastrophic forgetting.\n6 DISCUSSIONS\n6.1 Model Complexity Analysis\nWe assume an FFN has hidden layers a1,..., ağ‘›. The complexity\nof an FFN with input xand output yis|FFN(x,y)|=|x|Â·|a1|+Ãğ‘›âˆ’1\nğ‘–=1|ağ‘–|Â·|ağ‘–+1|+|ağ‘›|Â·|y|.\nOur model contains three components: AE, FFN, and ğ‘€. The com-\nplexity of AE is|FFN(x,z)|. The complexity of FFN is |FFN([x;z],t)|,\nwhere tis theğ¿-dimensional vector after Normğ‘™2. Component ğ‘€\nconsists of an FFN and (ğ¿+2)linear transformations. Its complexity\nis|FFN([x;z],H)|+(ğ¿+2)Â·|hğ‘–|+(ğ¿+2), where H=[h0;...;hğ¿+1].\nThus, the final model complexity is |FFN(x,z)|+| FFN([x;z],t)|+\n|FFN([x;z],H)|+(ğ¿+2)Â·|hğ‘–|+(ğ¿+2).\n(a) Simplified DLN\n (b) Our Model\nFigure 3: Comparison of simplified DLN and our model.\n6.2 Comparison with Other Models\nLattice Regression. Lattice regression models [ 17,19,21,57] are\nthe latest deep learning architectures for monotonic regression. We\nprovide a comparison between ours and them applied to selectivity\nestimation. For the sake of an analytical comparison, we assume x\nandDare fixed so the selectivity only depends on ğ‘¡, and consider\na shallow version of DLN [57] with one layer of calibrator and one\nlayer of a single lattice.\nWith the above simplification, the DLN can be analytically rep-\nresented as: Ë†ğ‘“DLN(ğ‘¡)=â„(ğ‘”(ğ‘¡;w);ğœƒ0,ğœƒ1), whereğ‘”:ğ‘¡âˆˆ[0,ğ‘¡max]â†¦â†’\nğ‘§âˆˆ[0,1]andâ„(ğ‘§;ğœƒ0,ğœƒ1)=(1âˆ’ğ‘§)ğœƒ0+ğ‘§ğœƒ1. Hence it degenerates to\nfitting a linear interpolation in a latent space. There is little learning\nfor the function â„, as its two parameters ğœƒ0andğœƒ1are determined\nby the minimum and maximum selectivity values in the training\ndata. Thus, the workhorse of the model is to learn the non-linear\nmapping of ğ‘”. The calibrator also uses piecewise linear functions\nwithğ¿control points equivalent to our (ğœğ‘–,ğ‘ğ‘–)ğ¿\nğ‘–=1. However, ğœğ‘–s\nare equally spaced between 0andğ‘¡max, and onlyğ‘ğ‘–s are learnable.\nThis design is not flexible for many value functions; e.g., if the\nfunction values change rapidly within a small interval, the calibra-\ntor will not adaptively allocate more control points to this area.\nWe show this with 8 control points for both models to learn the\nfunctionğ‘¦=ğ‘“(ğ‘¡)=1\n10exp(ğ‘¡),ğ‘¡âˆˆ[0,10]. The training data are 80\n(ğ‘¡ğ‘–,ğ‘“(ğ‘¡ğ‘–))pairs where ğ‘¡ğ‘–s are uniformly sampled in [0,10]. We plot\nboth modelsâ€™ estimation curves and their learned control points in\nFigure 3. The ğ‘§values at the control points of DLN are shown on\nthe right side of Figure 3(a). We observe: (1) The calibrator virtually\ndetermines the estimation as â„()degenerates to a simple scaling.\n(2) The calibratorâ€™s control points are evenly spaced in ğ‘¡, while\nour model learns to place more controls points in the â€œinteresting\nareaâ€, i.e., where ğ‘¦values change rapidly. (3) As a result, our model\napproximates the value function much better than DLN .\nFurther, for DLN , the non-linear mapping on ğ‘¡is independent of\nx(even though we do not model xhere). Even in the full-fledged\nDLN model, the calibration is performed on each input dimension\nindependently. The full-fledged DLN model is too complex to ana-\nlyze, so we only study it in our empirical evaluation. Nonetheless,\nwe believe that the above inherent limitations still remain. Our\nempirical evaluation will also show that query-dependent fitting\nof the value function is critical in our problem. Apart from DLN ,\nrecent studies also employ lattice regression and/or piecewise linear\nfunctions for learned index [ 30,33]. Like DLN , their control points\nare also query independent, albeit not equally spaced.\n\nYaoshu Wang1, Chuan Xiao2,3, Jianbin Qin1, Rui Mao1, Makoto Onizuka2, Wei Wang4,5, Rui Zhang6, and Yoshiharu Ishikawa3\nTable 1: Statistics of datasets.\nDataset Source Domain # Objects Dimensionality Distance\nfastText [1] text 1M 300 Euclidean\nGloVe [2] text 1.9M 300 Euclidean\nMS-Celeb [20] image 2M 128 cosine\nYouTube [3] video 0.35M 1770 cosine\nDEEP [4] image 100M 96 cosine\nSIFT [5] image 200M 128 cosine\nClenshaw-Curtis Quadrature. Clenshaw-Curtis quadrature [ 37]\nis able to approximate the integralâˆ«ğœğ‘šğ‘ğ‘¥\n0Ë†ğ‘”(x,ğ‘¡,D)dğ‘¡, where Ë†ğ‘”=\nğœ•Ë†ğ‘“(x,ğ‘¡,D)\nğœ•ğ‘¡in our problem. UMNN [51] is a recent work that adopts\nthe idea to solve the autoregressive flow problem, and uses a neu-\nral network to model Ë†ğ‘”. In [ 37], the cosine transform of Ë†ğ‘”(ğ‘ğ‘œğ‘ ğœƒ)\nis adopted and the discrete finite cosine transform is sampled at\nequidistant points ğœƒ=ğœ‹ğ‘ \nğ‘, whereğ‘ =1,...,ğ‘ , andğ‘is the number\nof sample points. Similar to DLN , it adopts the same integral approx-\nimation for different queries and ignores that integral points should\ndepend on x. In contrast, our method addresses this issue by using\na query-dependent model, thereby delivering more flexibility.\nQuery-Driven Quantized Regression. The main idea of query-\ndriven quantized regression [ 7,8] is to quantize the query space\nand find prototypes (the closest one or multiple related ones) for\nthe given query object. Then the output space is quantized by proto-\ntypes, and localized regressions are used to estimate the selectivity\nfor corresponding prototypes. Like our model, they also employ a\nquery-dependent design. The differences from ours are: (1) [ 7,8]\ndivide the query space of (x,ğ‘¡)while we divide the range of thresh-\noldğ‘¡using x. (2) The number of prototypes is finite and often up to\nthousands in [ 7,8], while our model chooses the selectivity curve\nfor the query object via an FFN and model ğ‘€(Figure 1), which yield\nan unlimited number of curves in Rğ¿+2. (3) We employ deep regres-\nsion for higher accuracy. (4) We directly partition the database D\nand train multiple deep models to deal with the subsets of Dthat\nmay differ in data distribution, while the data subspace in [ 8] is\ndefined by its query prototype.\n7 EVALUATIONS\n7.1 Experimental Settings\nDatasets. We use six datasets. The statistics is given in Table 1.\nWe preprocess MS-Celeb byfaceNet [42] to obtain vectors. The\nother datasets have already been transformed to high-dimensional\ndata. GloVe ,YouTube ,DEEP , and SIFT were also used in previous\nwork [50] or nearest neighbor search benchmarks [10, 34].\nWe randomly sample 0.25M vectors from each dataset Das query\nobjects.\nThe resulting query workload, denoted by Q, was uniformly split\nin 8:1:1 (by query objects) into training, validation, and test sets. So\nnone of the test query objects has been seen by the model during\ntraining or validation. Note that labels (i.e., true selectivities) are\ncomputed onD, notQ. For each training query object, we iterate\nthrough all the generated thresholds and add them to the training\nset. We randomly choose 3 generated thresholds for each validation\nor test query object. Due to the large number of training data,we randomly select training instances for each batch instead of\ncontinuously loading them, and the training procedure terminates\nwhen the mean squared error of the validation set does not increase\nin 5 consecutive epochs. For each setting, we tested on 5 sampled\nworkloads to mitigate the effect of sampling error.\nMethods. We compare the following approaches3.\nâ€¢RSis a random sampling approach. For each query, we uniformly\nsample 0.1%|D|objects for the first four datasets and 0.01%|D|\nobjects for DEEP andSIFT. Then we use scipy.spatial.distance.cdist\nto compute the distances to the query objects in a batch manner.\nâ€¢IS[54] is an importance sampling approach using locality-sensitive\nhashing. It only works for cosine similarity due to the use of\nSimHash [ 12]. We enforce monotonicity by using deterministic\nsampling w.r.t. the query object.\nâ€¢KDE [36] is based on adaptive kernel density estimation for\nmetric distance functions. To cope with cosine similarity, we nor-\nmalize data to unit vectors and run KDE for Euclidean distance.\nâ€¢QR-1 [7] and QR-2 [8] are two query-driven quantized regres-\nsion models. We use the linear model in [7] for QR-1 .\nâ€¢LightGBM [49] is based on gradient boosting decision trees\n(CARTs). Each rule in a CART is in the form of ğ‘¥ğ‘–<ğ‘(ğ‘¥ğ‘–is the\nğ‘–-th dimension of x) orğ‘¡<ğ‘.\nâ€¢Deep regression models: DNN , a vanilla feed-forward network;\nMoE [43], a mixture of expert model with sparse activation;\nRMI [31], a hierarchical mixture of expert model; and Card-\nNet [50], a regression model based on incremental prediction\n(we enable the accelerated estimation [50]).\nâ€¢Lattice regression models: We adopt DLN [57] in this category.\nâ€¢Clenshaw-Curtis quadrature model: We adopt UMNN [51].\nâ€¢Our model is dubbed SelNet4. The default setting of ğ¿(number\nof control points) is 50 and ğ¾(partition size) is 3. The predefined\nthresholdğ›¿ğ‘ˆfor incremental learning is 20. We also evaluate\ntwo ablated models: (1) SelNet -ctisSelNet without the cover\ntree partitioning, and (2) SelNet -ad-ct isSelNet -ctwithout the\nquery-dependent feature for control points (disabled by feeding\na constant vector into the FFN that generates the ğœvector).\nError Metrics. We evaluate Mean Squared Error ( MSE ), Mean Ab-\nsolute Percentage Error ( MAPE ), and Mean Absolute Error ( MAE ).\nEnvironment. Experiments were run on a server with an Intel\nXeon E5-2640 @2.40GHz CPU and 256GB RAM, running Ubuntu\n16.04.4 LTS. Models were implemented in Python and Tensorflow.\n7.2 Accuracy\nWe report accuracies in Table 2, where monotonic models are\nmarked with *, and best values are marked in boldface. Our model,\nSelNet , consistently outperforms existing models. It achieves sub-\nstantial error reduction against the best of state-of-the-art methods,\nin all the three error metrics and all the settings. Compare to the\nrunner-up model on each dataset, the improvement is 2.0 â€“ 5.0\ntimes in MSE , 1.3 â€“ 3.3 times in MAE , and 1.2 â€“ 1.7 times in MAPE ,\nand is more significant on larger datasets.\nWe examine each category of models. We start with the sampling-\nbased methods. KDE works better than RSandISin most settings.\n3Please see Appendix B for model settings.\n4The source code is available at [6].\n\nConsistent and Flexible Selectivity Estimation for High-Dimensional Data\nTable 2: Accuracy ( MSE andMAE measured in 105and 102, respectively).\nModelfastText GloVe MS-Celeb YouTube DEEP SIFT\nMSE MAE MAPE MSE MAE MAPE MSE MAE MAPE MSE MAE MAPE MSE MAE MAPE MSE MAE MAPE\nRS* 22.38 7.45 1.40 34.85 8.71 1.09 28.64 8.09 1.30 2.95 1.89 0.88 84732.32 725.21 1.26 30317437.60 9496.55 0.98\nIS* - - - - - -104.58 14.25 1.25 2.85 1.83 0.76 50242.10 314.12 0.97 32049511.88 10612.42 0.92\nKDE * 21.46 6.57 1.28 37.52 8.93 0.87 36.43 8.42 1.02 2.93 1.90 0.70 39497.24 298.10 0.85 27651109.31 9581.89 0.91\nQR-1 41.79 8.95 1.02 50.13 9.21 0.99 74.35 11.60 1.06 3.42 2.01 0.71 37054.11 292.16 0.78 28077423.18 9953.34 0.94\nQR-2 34.35 8.03 0.97 42.47 9.01 0.86 42.94 9.05 0.89 2.74 1.85 0.55 31485.85 273.22 0.67 22048511.27 8542.73 0.71\nLightGBM 98.77 9.56 1.04 72.11 10.87 0.89 101.29 9.51 0.45 4.01 2.00 0.52 44036.45 301.88 0.85 23849121.36 9005.17 0.75\nDNN 63.54 11.25 1.33 52.31 9.39 0.91 110.77 17.14 0.89 2.78 1.77 0.51 20454.11 192.13 0.69 24465910.26 7144.68 0.55\nMoE 45.90 8.50 0.91 30.14 7.05 0.91 21.25 4.32 0.30 1.58 1.59 0.53 18068.93 170.51 0.65 14750194.30 6327.47 0.40\nRMI 26.16 6.10 0.87 29.32 6.89 0.74 22.16 6.07 0.35 1.77 1.62 0.55 9498.21 116.54 0.67 8906108.00 4650.29 0.42\nCardNet * 25.67 6.16 0.90 27.05 6.19 0.78 13.67 4.08 0.27 1.41 1.44 0.48 9230.48 117.37 0.67 7248693.54 4851.43 0.40\nDLN * 77.50 11.56 1.53 52.26 10.27 0.89 82.35 11.85 0.97 2.94 1.92 0.69 58291.42 353.08 0.94 23059384.16 8058.49 0.51\nUMNN * 33.26 7.20 0.92 33.50 7.98 0.86 16.75 4.70 0.36 2.06 1.69 0.49 10603.68 131.04 0.73 10201332.32 5443.31 0.43\nSelNet * 7.87 3.56 0.76 9.17 3.83 0.68 4.96 2.43 0.23 0.72 1.13 0.36 2243.42 51.92 0.51 1464247.70 1406.63 0.23\nTable 3: Empirical monotonicity (%) on MS-Celeb .\nRS* IS* KDE * QR-1 QR-2\n100 100 100 85.39 84.86\nLightGBM DNN MoE RMI\n86.34 78.22 94.82 90.48\nCardNet *DLN *UMNN *SelNet *\n100 100 100 100\nIn fact, KDE â€™s performance even outperforms some deep learning\nregression based methods in a few cases (e.g., MSE onfastText ).\nAmong non-deep learning models, these is no best model across\nall the datasets, though QR-2 prevails on more datasets than oth-\ners. Among the deep learning models other than ours, CardNet\nis generally the best thanks to its incremental prediction for each\nthreshold interval. The performance of DLN is mediocre. The main\nreason is analyzed in Section 6.2. The accuracy of UMNN , which\nuses the same integral points for different queries, though better\nthan DLN , still trails behind ours by a large margin.\n7.3 Consistency Test\nWe compute the empirical monotonicity measure [ 15] and show the\nresults in Table 3. The measure is the percentage of estimated pairs\nthat violate the monotonicity, averaged over 200 queries. For each\nquery, we sampled 100 thresholds, which form\u0000100\n2\u0001pairs. A low\nscore indicates more inconsistent estimates. As expected, models\nwithout consistency guarantee cannot produce 100% monotonicity.\n7.4 Ablation Study\nTable 4 shows that the partitioning ( SelNet v.s.SelNet -ct) improves\nMSE ,MAE , and MAPE by up to 3.4, 2.1, and 1.2 times, respectively,\nand the effect is more remarkable on large datasets. This is because\neach model deals with a subset of the dataset for better fit and\nthe ground truth label values for each model are reduced, which\nmakes it easier to fit our piecewise linear function with the same\nnumber of control points, as the value function is less steep. Using\nquery-dependent control points ( SelNet -ctv.s.SelNet -ad-ct ) also has\na significant impact on accuracy across all the settings and all the\nerror metrics. The improvements in MSE ,MAE , and MAPE are up\nto 3.1, 2.0, and 3.6 times, respectively.Table 4: Ablation study.\nDataset Model MSE (Ã—105)MAE (Ã—102)MAPE\nfastTextSelNet 7.87 3.56 0.76\nSelNet -ct 12.63 4.37 0.81\nSelNet -ad-ct 39.59 8.72 2.90\nGloVeSelNet 9.17 3.83 0.68\nSelNet -ct 22.43 5.82 0.70\nSelNet -ad-ct 32.59 6.92 0.90\nMS-CelebSelNet 4.96 2.43 0.23\nSelNet -ct 5.31 2.92 0.24\nSelNet -ad-ct 16.02 4.65 0.37\nYouTubeSelNet 0.72 1.13 0.36\nSelNet -ct 0.90 1.20 0.39\nSelNet -ad-ct 1.65 1.59 0.53\nDEEPSelNet 2243.42 51.92 0.51\nSelNet -ct 5861.43 72.18 0.58\nSelNet -ad-ct 9012.57 101.42 0.71\nSIFTSelNet 1464247.70 1406.63 0.23\nSelNet -ct 4958113.22 2911.86 0.27\nSelNet -ad-ct 6904808.25 3855.53 0.54\n7.5 Estimation Time\nTable 5 reports the estimation times of the competitors. We also\nreport the time of running a state-of-the-art selection algorithm\n(CoverTree [27]) to obtain the exact selectivity. All the models except\nISare at least one order of magnitude faster than CoverTree , and\nthe gaps increase to three orders of magnitude on DEEP andSIFT.\nOur model is on a par with other deep learning models (except\nDNN ) and faster than sampling and quantized regression methods.\n7.6 Training\nTable 6 shows the training times. Non-deep models are faster to\ntrain. Our models spend 5 â€“ 6 hours, similar to other deep models. In\nFigure 4, we show the performances, measured by MSE , of the deep\nlearning models by varying the scale of training examples from\n20% to 100% of the original training data. All the models perform\nworse with fewer training data, but our models are more robust,\nshowing moderate accuracy loss.\n7.7 Data Update\nWe generate a stream of 100 update operations, each with an inser-\ntion or deletion of 5 records on fastText andMS-Celeb , to evaluate\n\nYaoshu Wang1, Chuan Xiao2,3, Jianbin Qin1, Rui Mao1, Makoto Onizuka2, Wei Wang4,5, Rui Zhang6, and Yoshiharu Ishikawa3\nTable 5: Average estimation time (milliseconds).\nModel fastText GloVe MS-Celeb YouTube DEEP SIFT\nCoverTree 8.14 8.85 9.65 6.11 214 395\nRS* 0.46 0.51 0.49 0.52 2.54 4.72\nIS* - - 1.08 2.35 4.97 6.81\nKDE * 0.79 0.68 0.59 0.94 1.48 2.05\nQR-1 0.86 0.98 0.97 1.03 2.21 2.82\nQR-2 0.79 0.99 0.95 1.10 2.32 2.91\nLightGBM 0.28 0.30 0.18 0.52 0.26 0.26\nDNN 0.07 0.10 0.03 0.16 0.11 0.10\nMoE 0.36 0.33 0.27 0.49 0.29 0.33\nRMI 0.34 0.38 0.25 0.47 0.27 0.30\nCardNet * 0.19 0.26 0.14 0.31 0.22 0.28\nDLN * 0.83 0.69 0.65 1.22 0.64 0.80\nUMNN * 0.39 0.32 0.24 0.52 0.26 0.32\nSelNet * 0.35 0.31 0.24 0.51 0.29 0.36\nTable 6: Training time (hours).\nModel fastText GloVe MS-Celeb YouTube DEEP SIFT\nKDE * 1.1 1.5 0.7 0.8 2.5 3.6\nQR-1 1.8 2.1 1.5 1.2 3.9 4.6\nQR-2 1.5 2.0 1.6 1.2 3.6 4.7\nLightGBM 2.1 1.9 2.2 2.1 1.9 2.1\nDNN 2.9 2.1 2.8 2.9 2.5 2.9\nMoE 4.9 5.4 4.9 4.7 4.3 4.4\nRMI 5.4 5.6 4.8 5.3 4.6 4.8\nCardNet * 3.8 3.9 3.3 3.2 3.5 3.8\nDLN * 6.9 7.1 6.0 6.5 6.3 6.4\nUMNN * 5.5 5.7 4.9 5.2 5.4 4.6\nSelNet * 6.0 5.5 5.2 5.6 5.2 5.0\nTable 7: Varying number of control points on fastText .\nError MetricNumber of Control Points\n10 50 90 130\nMSE (Ã—105) 13.06 7.87 7.93 10.47\nMAE (Ã—102) 4.85 3.56 3.56 3.92\nMAPE 0.87 0.76 0.76 0.79\nour incremental learning technique. Figure 5 plots how MSE and\nMAPE change with the stream. The general trend is that the MSE is\ndecreasing when there are more updates, while MAPE fluctuates or\nkeeps almost the same. Such difference is caused by the change of\nlabels (i.e., true selectivities) in the stream. Nonetheless, the result\nindicates that incremental learning is able to keep up with the up-\ndated data. Besides, SelNet only spends 1.5 â€“ 2.0 minutes for each\nincremental learning, showcasing its speed to cope with updates.\n7.8 Evaluation of Hyper-Parameters\nTable 7 shows the accuracy when we vary the number of control\npointsğ¿onfastText . A small value leads to underfitting towards\nthe curve of thresholds, while a large value increases the learning\ndifficulty.ğ¿=50achieves the best performance.\nTable 8 reports the accuracy when we vary the partition size\nğ¾onfastText . There is no partitioning when ğ¾=1. We observe\nthat the partitioning is useful, but the improvement is small when\npartition size exceeds 3, and estimation time also substantially in-\ncreases. This means a small partition size ( ğ¾=3) suffices to achieve\n20 40 60 80 100\nTraining Size101102MSE (105)\nSelNet\nCardNetRMI\nMoEDLN\nUMNN(a)MSE ,fastText\n20 40 60 80 100\nTraining Size20406080MSE (105)\nSelNet\nCardNetRMI\nMoEDLN\nUMNN (b)MSE ,GloVe\n20 40 60 80 100\nTraining Size101102MSE (105)\nSelNet\nCardNetRMI\nMoEDLN\nUMNN\n(c)MSE ,MS-Celeb\n20 40 60 80 100\nTraining Size102030405060MSE (104)\nSelNet\nCardNetRMI\nMoEDLN\nUMNN (d)MSE ,YouTube\n20 40 60 80 100\nTraining Size020406080MSE (108)\nSelNet\nCardNetRMI\nMoEDLN\nUMNN\n(e)MSE ,DEEP\n20 40 60 80 100\nTraining Size01020304050MSE (1011)\nSelNet\nCardNetRMI\nMoEDLN\nUMNN (f)MSE ,SIFT\nFigure 4: Varying training data size.\n0 20 40 60 80 100\nSequence of Operations45678MSE (105)\nfastText MS-Celeb\n(a)MSE\n0 20 40 60 80 100\nSequence of Operations20406080100MAPE (%)\nfastText MS-Celeb (b)MAPE\nFigure 5: Data update.\ngood performance. For partitioning strategy, we compare cover\ntree partitioning (CT) with random partitioning (RP) and ğ‘˜-means\npartitioning (KM) in Table 9. CT delivers the best performance. KM\nis the worst because it tends to cause imbalance in the partition.\n7.9 Generalizability\nTo show the generalizability of our model, we evaluate the per-\nformance on the queries that significantly differ from the records\nin the training data. To prepare such queries, we first perform a\n\nConsistent and Flexible Selectivity Estimation for High-Dimensional Data\nTable 8: Varying partition size on fastText .\nError MetricPartition Size\n1 3 6 9\nMSE (Ã—105) 12.63 7.87 6.82 6.75\nMAE (Ã—102) 4.37 3.56 3.36 3.11\nMAPE 0.81 0.76 0.77 0.74\nEstimation Time (ms) 0.16 0.35 0.79 1.24\nTable 9: Varying partitioning method on fastText .\nError Metric CT (3) RP (3) KM (3)\nMSE (Ã—105) 7.87 8.02 9.14\nMAE (Ã—102) 3.56 3.57 3.64\nMAPE 0.76 0.78 0.79\n[0, 1) [1, 10) [10, 100)â‰¥100\nSelectivity Range (103)1051061071081091010MSESelNet CardNet RMI KDE\n(a)DEEP\n[0, 1) [1, 10) [10, 100)â‰¥100\nSelectivity Range (103)1061081010MSESelNet CardNet RMI KDE (b)SIFT\nFigure 6: Generalizability.\nTrueTime ExactSelSelNetCardNetRMI KDERS010000200003000040000Estimated Time (s)\n(a)AMiner-Paper\nTrueTime ExactSelSelNetCardNetRMI KDERS05001000150020002500Estimated Time (s) (b)Quora\nFigure 7: Estimated search time (10,000 queries).\nğ‘˜-means clustering on D. We randomly sample 10,000 query ob-\njects fromD(excluding the queries used for training) and add\nGaussian noise [ 58]. Then we pick the top-2,000 ones having the\nlargest sum of squared distance to the ğ‘˜centroids. Figure 6 show\nthe performances of KDE,RMI,CardNet , and SelNet onDEEP and\nSIFT, measured by MSE . The queries are grouped by selectivity\nrange. In each selectivity group, SelNet consistently outperforms\nthe other models, and the advantage is around one order of magni-\ntude. This result demonstrates that our model generalizes well for\nout-of-dataset queries.\n7.10 Performance in Semantic Search\nTo evaluate the usefulness of SelNet , we consider estimating the\noverall processing time for a query workload of semantic search:\ngiven a query text entry, we want to find matching records in\nthe database. Estimating the query processing time may help to\ncreate a service level agreement. We use two datasets, AMiner-Paper\npublications (2.1M records) and Quora questions (0.8M records).\nAMiner-Paper has four attributes: title, authors, venue, and year.We follow [ 35] and concatenate attribute names and values as one\nstring. Quora has one attribute. Then we embed each record to a\n768-dimensional vector by Sentence-BERT [41].\n10,000 records are sampled from each dataset as queries. To pro-\ncess a query, we first embed it by Sentence-BERT [41], and then\nuseFaiss [28] to find candidate records whose cosine similarity to\nthe query embedding is no less than 0.9. The candidates are ver-\nified using DITTO [35]. Hence the overall query processing time\ncan be estimated as: avg_ Faiss _timeÃ—10000+avg_ DITTO _timeÃ—\nFaiss _recallÃ—Ã10000\n1estimated_selectivity_of_query_ ğ‘–. The aver-\nage times and Faiss recall are obtained by running a small query\nworkload. For selectivity, we consider RS,KDE ,RMI,CardNet ,\nSelNet , and an oracle that outputs the exact selectivity ( ExactSel ).\nWe plot the estimated time of processing 10,000 queries in Fig-\nure 7, where TrueTime indicates the ground truth. The models tend\nto underestimate on AMiner-Paper and overestimate on Quora .\nSelNet â€™s high accuracy in selectivity estimation pays off. Compared\nto the ground truth, SelNet â€™s error is 13% on AMiner-Paper and\n16% on Quora , close to ExactSel â€™s and much lower than the other\nmodelsâ€™ (at least 39%). SelNet is also efficient; e.g., running the\nworkload to obtain the ground truth on AMiner-Paper spends 13\nhours, which is twice the time of preparing training data +training\nSelNet+estimating for 10,000 queries. Seeing SelNet â€™s scalability\nin estimation time (Table 5) and training time (Table 6), we believe\nthat the advantage will be more substantial on larger datasets.\n8 CONCLUSION\nWe tackled the selectivity estimation problem for high-dimensional\ndata. Our method is based on learning monotonic query-dependent\npiece-wise linear function. This provides the flexibility of our model\nto approximate the selectivity curve while guaranteeing the con-\nsistency of estimation. We proposed a partitioning technique to\ncope with large-scale datasets and an incremental learning tech-\nnique for updates. Our experiments showed the superiority of the\nproposed model in accuracy across a variety of datasets, distance\nfunctions, and error metrics. The experiments also demonstrated\nthe usefulness of our model in a semantic search application.\nAcknowledgements This work was supported by NSFC 62072311\nand U2001212, Guangdong Basic and Applied Basic Research Foun-\ndation 2019A1515111047 and 2020B1515120028, Guangdong Peral\nRiver Recruitment Program of Talents 2019ZT08X603, JSPS Kak-\nenhi 16H01722, 17H06099, 18H04093, and 19K11979, and ARC DPs\n170103710 and 180103411.\nREFERENCES\n[1] https://fasttext.cc/docs/en/english-vectors.html.\n[2] https://nlp.stanford.edu/projects/glove/.\n[3] http://www.cs.tau.ac.il/~wolf/ytfaces/index.html.\n[4] http://http://sites.skoltech.ru/compvision/noimi/.\n[5] http://http://corpus-texmex.irisa.fr/.\n[6] https://github.com/yyssl88/SelNet-Estimation.\n[7]C. Anagnostopoulos and P. Triantafillou. Learning set cardinality in distance\nnearest neighbours. In ICDM , pages 691â€“696, 2015.\n[8]C. Anagnostopoulos and P. Triantafillou. Query-driven learning for predictive\nanalytics of data subspace cardinality. ACM Trans. Knowl. Discov. Data , 11(4):47:1â€“\n47:46, 2017.\n[9]S. Arya, T. Malamatos, and D. M. Mount. Space-time tradeoffs for approximate\nspherical range counting. In SODA , pages 535â€“544, 2005.\n[10] M. AumÃ¼ller, E. Bernhardsson, and A. J. Faithfull. Ann-benchmarks: A bench-\nmarking tool for approximate nearest neighbor algorithms. Inf. Syst. , 87, 2020.\n\nYaoshu Wang1, Chuan Xiao2,3, Jianbin Qin1, Rui Mao1, Makoto Onizuka2, Wei Wang4,5, Rui Zhang6, and Yoshiharu Ishikawa3\n[11] M. M. Breunig, H. Kriegel, R. T. Ng, and J. Sander. LOF: identifying density-based\nlocal outliers. In SIGMOD , pages 93â€“104, 2000.\n[12] M. S. Charikar. Similarity estimation techniques from rounding algorithms. In\nSTOC , pages 380â€“388, 2002.\n[13] T. Chen and C. Guestrin. Xgboost: A scalable tree boosting system. In KDD , pages\n785â€“794, 2016.\n[14] G. Cormode, M. N. Garofalakis, P. J. Haas, and C. Jermaine. Synopses for massive\ndata: Samples, histograms, wavelets, sketches. Foundations and Trends in Databases ,\n4(1-3):1â€“294, 2012.\n[15] H. Daniels and M. Velikova. Monotone and partially monotone neural networks.\nIEEE Transactions on Neural Networks , 21(6):906â€“917, 2010.\n[16] S. Das, P. S. G. C., A. Doan, J. F. Naughton, G. Krishnan, R. Deep, E. Arcaute,\nV. Raghavendra, and Y. Park. Falcon: Scaling up hands-off crowdsourced entity\nmatching to build cloud services. In SIGMOD , pages 1431â€“1446, 2017.\n[17] M. M. Fard, K. Canini, A. Cotter, J. Pfeifer, and M. Gupta. Fast and flexible monotonic\nfunctions with ensembles of lattices. In NIPS , pages 2919â€“2927, 2016.\n[18] J. Fox. Robust regression: Appendix to an r and s-plus companion to applied\nregression, 2002.\n[19] E. Garcia and M. Gupta. Lattice regression. In NIPS , pages 594â€“602, 2009.\n[20] Y. Guo, L. Zhang, Y. Hu, X. He, and J. Gao. MS-Celeb-1M: A dataset and benchmark\nfor large scale face recognition. In ECCV , 2016.\n[21] M. Gupta, A. Cotter, J. Pfeifer, K. Voevodski, K. Canini, A. Mangylov, W. Moczyd-\nlowski, and A. Van Esbroeck. Monotonic calibrated interpolated look-up tables.\nThe Journal of Machine Learning Research , 17(1):3790â€“3836, 2016.\n[22] Q. Han, T. Wang, S. Chatterjee, and R. J. Samworth. Isotonic regression in general\ndimensions. arXiv preprint arXiv:1708.09468 , 2017.\n[23] S. Hasan, S. Thirumuruganathan, J. Augustine, N. Koudas, and G. Das. Deep\nlearning models for selectivity estimation of multi-attribute queries. In SIGMOD ,\npages 1035â€“1050, 2020.\n[24] M. Heimel, M. Kiefer, and V. Markl. Self-tuning, GPU-accelerated kernel density\nmodels for multidimensional selectivity estimation. In SIGMOD , pages 1477â€“1492,\n2015.\n[25] P. J. Huber et al. Robust estimation of a location parameter. The annals of mathe-\nmatical statistics , 35(1):73â€“101, 1964.\n[26] Y. Ioannidis. The history of histograms (abridged). In VLDB , pages 19â€“30, 2003.\n[27] M. Izbicki and C. R. Shelton. Faster cover trees. In ICML , pages 1162â€“1170, 2015.\n[28] H. JÃ©gou, tthijs Douze, and J. Johnson. Facebook ai similarity search (faiss). https:\n//github.com/facebookresearch/faiss.\n[29] A. Kipf, T. Kipf, B. Radke, V. Leis, P. A. Boncz, and A. Kemper. Learned cardinalities:\nEstimating correlated joins with deep learning. In CIDR , 2019.\n[30] A. Kipf, R. Marcus, A. van Renen, M. Stoian, A. Kemper, T. Kraska, and T. Neumann.\nRadixspline: a single-pass learned index. In aiDM@SIGMOD , pages 5:1â€“5:5, 2020.\n[31] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for learned index\nstructures. In SIGMOD , pages 489â€“504, 2018.\n[32] S. LathuiliÃ¨re, P. Mesejo, X. Alameda-Pineda, and R. Horaud. A comprehensive\nanalysis of deep regression. arXiv preprint arXiv:1803.08450 , 2018.\n[33] P. Li, H. Lu, Q. Zheng, L. Yang, and G. Pan. LISA: A learned index structure for\nspatial data. In SIGMOD , pages 2119â€“2133, 2020.\n[34] W. Li, Y. Zhang, Y. Sun, W. Wang, M. Li, W. Zhang, and X. Lin. Approximate\nnearest neighbor search on high dimensional data - experiments, analyses, and\nimprovement. IEEE Trans. Knowl. Data Eng. , 32(8):1475â€“1488, 2020.[35] Y. Li, J. Li, Y. Suhara, A. Doan, and W.-C. Tan. Deep entity matching with pre-trained\nlanguage models. PVLDB , 14(1):50â€“60, 2020.\n[36] M. Mattig, T. Fober, C. Beilschmidt, and B. Seeger. Kernel-based cardinality esti-\nmation on metric data. In EDBT , pages 349â€“360, 2018.\n[37] M. Novelinkova. Comparison of clenshaw-curtis and gauss quadrature. In WDS ,\nvolume 11, pages 67â€“71, 2011.\n[38] J. Ortiz, M. Balazinska, J. Gehrke, and S. S. Keerthi. An empirical analysis of deep\nlearning for cardinality estimation. CoRR , abs/1905.06425, 2019.\n[39] Y. Park, S. Zhong, and B. Mozafari. Quicksel: Quick selectivity learning with\nmixture models. In SIGMOD , pages 1017â€“1033, 2020.\n[40] L. Prunty. Curve fitting with smooth functions that are piecewise-linear in the\nlimit. Biometrics , pages 857â€“866, 1983.\n[41] N. Reimers and I. Gurevych. Sentence-BERT: Sentence embeddings using siamese\nBERT-networks. In EMNLP-IJCNLP , pages 3980â€“3990, 2019.\n[42] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A unified embedding for face\nrecognition and clustering. In CVPR , pages 815â€“823, 2015.\n[43] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean.\nOutrageously large neural networks: The sparsely-gated mixture-of-experts layer.\narXiv preprint arXiv:1701.06538 , 2017.\n[44] J. Spouge, H. Wan, and W. Wilbur. Least squares isotonic regression in two\ndimensions. Journal of Optimization Theory and Applications , 117(3):585â€“605,\n2003.\n[45] J. Sun and G. Li. An end-to-end learning-based cost estimator. PVLDB , 13(3):307â€“\n319, 2019.\n[46] Y. Sun, X. Wang, and X. Tang. Deep convolutional network cascade for facial point\ndetection. In CVPR , pages 3476â€“3483, 2013.\n[47] A. Toshev and C. Szegedy. Deeppose: Human pose estimation via deep neural\nnetworks. In CVPR , pages 1653â€“1660, 2014.\n[48] B. Walenz, S. Sintos, S. Roy, and J. Yang. Learning to sample: Counting with\ncomplex queries. PVLDB , 13(3):390â€“402, 2019.\n[49] D. Wang, Y. Zhang, and Y. Zhao. Lightgbm: An effective mirna classification\nmethod in breast cancer patients. In ICCBB , pages 7â€“11, 2017.\n[50] Y. Wang, C. Xiao, J. Qin, X. Cao, Y. Sun, W. Wang, and M. Onizuka. Monotonic car-\ndinality estimation of similarity selection: A deep learning approach. In SIGMOD ,\npages 1197â€“1212, 2020.\n[51] A. Wehenkel and G. Louppe. Unconstrained monotonic neural networks. In\nNeurIPS , pages 1543â€“1553, 2019.\n[52] K.-Y. Whang, S.-W. Kim, and G. Wiederhold. Dynamic maintenance of data\ndistribution for selectivity estimation. VLDB J. , 3(1):29â€“51, 1994.\n[53] W. Wu, J. F. Naughton, and H. Singh. Sampling-based query re-optimization. In\nSIGMOD , pages 1721â€“1736, 2016.\n[54] X. Wu, M. Charikar, and V. Natchu. Local density estimation in high dimensions.\nInICML , pages 5293â€“5301, 2018.\n[55] Y. Wu, D. Agrawal, and A. El Abbadi. Query estimation by adaptive sampling. In\nICDE , pages 639â€“648, 2002.\n[56] Z. Yang, E. Liang, A. Kamsetty, C. Wu, Y. Duan, P. Chen, P. Abbeel, J. M. Hellerstein,\nS. Krishnan, and I. Stoica. Deep unsupervised cardinality estimation. PVLDB ,\n13(3):279â€“292, 2019.\n[57] S. You, D. Ding, K. Canini, J. Pfeifer, and M. Gupta. Deep lattice networks and\npartial monotonic functions. In NIPS , pages 2981â€“2989, 2017.\n[58] D. Zhang and Z. Yang. Word embedding perturbation for sentence classification.\narXiv preprint arXiv:1804.08166 , 2018.\n\nConsistent and Flexible Selectivity Estimation for High-Dimensional Data\nAPPENDIX\nA PROOF\nLemma 1.\nProof. Assumeğ‘¡âˆˆ[ğœğ‘–âˆ’1,ğœğ‘–), thenğ‘¡+ğœ–is in[ğœğ‘–âˆ’1,ğœğ‘–)or[ğœğ‘–,ğœğ‘–+1).\nIn the first case, Ë†ğ‘“(x,ğ‘¡+ğœ–,D;Î˜)âˆ’Ë†ğ‘“(x,ğ‘¡,D;Î˜)=ğœ–\nğœğ‘–âˆ’ğœğ‘–âˆ’1Â·(ğ‘ğ‘–âˆ’\nğ‘ğ‘–âˆ’1) â‰¥ 0. In the second case, Ë†ğ‘“(x,ğ‘¡,D;Î˜) â‰¤ğ‘ğ‘–and Ë†ğ‘“(x,ğ‘¡+\nğœ–,D;Î˜)â‰¥ğ‘ğ‘–. Therefore, Ë†ğ‘“(x,ğ‘¡,D;Î˜)is non-decreasing in ğ‘¡.â–¡\nB EXPERIMENT SETUP\nB.1 Model Settings\nHyperparameter and training settings are given below.\nâ€¢ISandKDE : The sample size is 2000.\nâ€¢QR-1 andQR-2 : The number of query prototypes is 2000.\nâ€¢LightGBM : The number of CARTs is 1000.\nâ€¢DNN is a vanilla FFN with four hidden layers of sizes 512, 512,\n512, and 256.\nâ€¢MoE consists of 30 expert models, each an FFN with three hidden\nlayers of sizes 512, 512, and 512. We used top-3 experts for the\nprediction.\nâ€¢RMI has three levels, with 1, 4, and 8 models, respectively. Each\nmodel is an FFN with four hidden layers with sizes 512, 512, 512,\nand 256.\nâ€¢DLN is an architecture of six layers: calibrators, linear embedding,\ncalibrators, ensemble of lattices, calibrators, and linear embed-\nding.\nâ€¢UMNN is an FFN with four hidden layers of sizes 512, 512, 512\nand 256 to implement the derivative.ğœ•ğ‘“(x,ğ‘¡,D)\nğœ•ğ‘¡.ğ‘“(x,ğ‘¡,D)is com-\nputed by Clenshaw-Curtis quadrature with learned derivatives.\nâ€¢SelNet : We use an FFN with two hidden layers to estimate ğœ, and\nan FFN in Equation 9 with four hidden layers to estimate p. The\nencoder and decoder of AE are implemented with an FFN with\nthree hidden layers. For MS-Celeb andYouTube , the sizes of the\nfirst three (or two, if it only has two) hidden layers of the threeFFNs are 512, and the sizes of all the other hidden layers are 256.\nForfastText ,GloVe ,DEEP , and SIFT, the sizes of the first hidden\nlayer of the these FFNs are 1024, and the others remain the same\nas above. The number of control parameters ğ¿is 50. The default\npartition size ğ¾is 3.ğ‘¡maxis 54 for Euclidean distance. For cosine\nsimilarity, we equivalently convert it to Euclidean distance on\nunit vectors, and set ğ‘¡max=1. The learning rates of MS-Celeb ,\nfastText ,YouTube ,GloVe ,DEEP , and SIFT are 0.00003, 0.00002,\n0.00003, 0.0001, 0.0001, and 0.0001, respectively. |hğ‘–|(0â‰¤ğ‘–â‰¤ğ¿+1)\nin modelğ‘€is 100. The batch size is 512 for all the datasets. We\ntrain all the models in 1500 epochs and select the ones with the\nsmallest validation error. For training with data partitioning, we\nuseğ‘‡=300andğ›½=0.1.ğ›¿ğ‘ˆfor incremental learning is 20.\nFor the learning models, we train them with the same Huber loss\nover the logarithms of the ground truth and the predicted value.\nAll the hyper-parameters are fine-tuned to minimize the validation\nerror. DNN ,MoE andRMI cannot directly handle the threshold ğ‘¡.\nWe learn a non-linear transformation of ğ‘¡into anğ‘š-dimensional\nembedding vector, i.e., t=ReLU(wğ‘¡). Then we concatenate it with\nxas the input to these models.\nB.2 Evaluation Metrics\nWe evaluate Mean Squared Error ( MSE ), Mean Absolute Error\n(MAE ), and Mean Absolute Percentage Error ( MAPE ). They are\ndefined as:\nMSE=1\nğ‘šğ‘šâˆ‘ï¸\nğ‘–=1(Ë†ğ‘¦ğ‘–âˆ’ğ‘¦ğ‘–)2,\nMAE =1\nğ‘šğ‘šâˆ‘ï¸\nğ‘–=1|Ë†ğ‘¦ğ‘–âˆ’ğ‘¦ğ‘–|,\nMAPE =1\nğ‘šğ‘šâˆ‘ï¸\nğ‘–=1\f\f\f\fË†ğ‘¦ğ‘–âˆ’ğ‘¦ğ‘–\nğ‘¦ğ‘–\f\f\f\f,\nwhereğ‘¦ğ‘–is the ground truth value and Ë†ğ‘¦ğ‘–is the estimated value.",
  "textLength": 59278
}