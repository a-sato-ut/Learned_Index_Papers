{
  "paperId": "b08fe5e696290daef253daef58dad4f61bbb4202",
  "title": "SmartIX: A database indexing agent based on reinforcement learning",
  "pdfPath": "b08fe5e696290daef253daef58dad4f61bbb4202.pdf",
  "text": "Automated Database Indexing Using Model-Free Reinforcement Learning\nGabriel Paludo Licksyand Felipe Meneguzzi z\nPontiﬁcal Catholic University of Rio Grande do Sul (PUCRS), Brazil\nGraduate Program in Computer Science, School of Technology\nygabriel.licks@edu.pucrs.br\nzfelipe.meneguzzi@pucrs.br\nAbstract\nConﬁguring databases for efﬁcient querying is a complex\ntask, often carried out by a database administrator. Solving\nthe problem of building indexes that truly optimize database\naccess requires a substantial amount of database and do-\nmain knowledge, the lack of which often results in wasted\nspace and memory for irrelevant indexes, possibly jeopardiz-\ning database performance for querying and certainly degrad-\ning performance for updating. We develop an architecture to\nsolve the problem of automatically indexing a database by\nusing reinforcement learning to optimize queries by indexing\ndata throughout the lifetime of a database. In our experimen-\ntal evaluation, our architecture shows superior performance\ncompared to related work on reinforcement learning and ge-\nnetic algorithms, maintaining near-optimal index conﬁgura-\ntions and efﬁciently scaling to large databases.\nIntroduction\nDespite the multitude of tools available to manage and gain\ninsights from very large datasets, indexing databases that\nstore such data remains a challenge with multiple opportu-\nnities for improvement [27]. Slow information retrieval in\ndatabases entails not only wasted time for a business but\nalso indicates a high computational cost being paid. Unnec-\nessary indexes or columns that should be indexed but are\nnot, directly impact the query performance of a database.\nNevertheless, achieving the best indexing conﬁguration for\na database is not a trivial task [4, 5]. To do so, we have to\nlearn from queries that are running, take into account their\nperformance, the system resources, and the storage budget\nso that we can ﬁnd the best index candidates [18].\nIn an ideal scenario, all frequently queried columns\nshould be indexed to optimize query performance. Since cre-\nating and maintaining indexes incur a cost in terms of stor-\nage as well as in computation whenever database insertions\nor updates take place in indexed columns [21], choosing an\noptimal set of indexes for querying purposes is not enough\nto ensure optimal performance, so we must reach a trade-\noff between query and insert/update performance. Thus, this\nCopyright c\r2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.is a fundamental task that needs to be performed continu-\nously, as the indexing conﬁguration directly impacts on a\ndatabase’s overall performance.\nWe developed an architecture for automated and dynamic\ndatabase indexing that evaluates query and insert/update per-\nformance to make decisions on whether to create or drop\nindexes using Reinforcement Learning (RL). We performed\nexperiments using a scalable benchmark database, where we\nempirically evaluate our architecture results in comparison\nto standard baseline index conﬁgurations, database advisor\ntools, genetic algorithms, and other reinforcement learning\nmethods applied to database indexing. The architecture we\nimplemented to automatically manage indexes through re-\ninforcement learning successfully converged in its training\nto a conﬁguration that outperforms all baselines and related\nwork, both in performance and in storage usage by indexes.\nBackground\nReinforcement Learning\nReinforcement learning is an approach to learn opti-\nmal agent policies in stochastic environments modeled as\nMarkov Decision Processes (MDPs) [2]. It is characterized\nby a trial-and-error learning method, where an agent inter-\nacts and transitions through states of an MDP environment\nmodel by taking actions and observing rewards [23, Ch. 1].\nMDP are formally deﬁned as a tuple M=hS;A;P;R;\ri,\nwhereSis the state space,Ais the action space, Pis a tran-\nsition probability function which deﬁnes the dynamics of the\nMDP,Ris a reward function and \r2[0;1]is a discount fac-\ntor [23, Ch. 3].\nIn order to solve an MDP, an agent needs to know the\nstate-transition and the reward functions. However, in most\nrealistic applications, modeling knowledge about the state-\ntransition or the reward function is either impossible or im-\npractical, so an agent interacts with the environment tak-\ning sequential actions to collect information and explore the\nsearch space by trial and error [23, Ch. 1]. The Q-learning\nalgorithm is the natural choice for solving such MDPs [23,\nCh. 16]. This method learns the values of state-action pairs,\ndenoted byQ(s;a), representing the value of taking action\nain a states[23, Ch. 6].arXiv:2007.14244v1  [cs.DB]  25 Jul 2020\n\nAssuming that states can be described in terms of features\nthat are well informative, such problem can be handled by\nusing linear function approximation, which is to use a pa-\nrameterized representation for the state-action value func-\ntion other than a look-up table [25]. The simplest differen-\ntiable function approximator is through a linear combination\nof features, though there are other ways of approximating\nfunctions such as using neural networks [23, Ch. 9, p. 195].\nIndexing in Relational Databases\nAn important technique to ﬁle organization in a DBMS is in-\ndexing [21, Ch. 8, p. 274], and is usually managed by a DBA.\nHowever, index selection without the need of a domain ex-\npert is a long-time research subject and remains a challenge\ndue to the problem complexity [27, 5, 4]. The idea is that,\ngiven the database schema and the workload it receives, we\ncan deﬁne the problem of ﬁnding an efﬁcient index conﬁg-\nuration that optimizes database operations [21, Ch. 20, p.\n664]. The complexity stems from the potential number of\nattributes that can be indexed and all of its subsets.\nWhile DBMSs strive to provide automatic index tuning,\nthe usual scenario is that performance statistics for optimiz-\ning queries and index recommendations are offered, but the\nDBA makes the decision on whether to apply the changes\nor not. Most recent versions of DBMSs such as Oracle [13]\nand Azure SQL Database [19] can automatically adjust in-\ndexes. However, it is not the case that the underlying system\nis openly described.\nA general way of evaluating DBMS performance is\nthrough benchmarking. Since DBMSs are complex pieces\nof software, and each has its own techniques for optimiza-\ntion, external organizations have deﬁned protocols to eval-\nuate their performance [21, Ch. 20, p. 682]. The goals of\nbenchmarks are to provide measures that are portable to dif-\nferent DBMSs and evaluate a wider range of aspects of the\nsystem, e.g., transactions per second and price-performance\nratio [21, Ch. 20, p. 683].\nTPC-H Benchmark\nThe tools provided by TPC-H include a database genera-\ntor (DBGen) able to create up to 100 TB of data to load\nin a DBMS, and a query generator (QGen) that creates\n22 queries with different levels of complexity. Using the\ndatabase and workload generated using these tools, TPC-\nH speciﬁes a benchmark that consists of inserting records,\nexecuting queries, and deleting records in the database to\nmeasure the performance of these operations.\nThe TPC-H Performance metric is expressed in Queries-\nper-Hour (QphH @Size ), which is achieved by computing\nthePower @Size and theThroughput @Size metrics [24].\nThe resulting values are related to its scale factor ( @Size ),\ni.e., the database size in gigabytes. The Power @Size eval-\nuates how fast the DBMS computes the answers to single\nqueries. This metric is computed using Equation 1:\nPower @Size =3600\n24q\n\u001922\ni=1QI(i;0)\u0002\u00192\nj=1RI(j;0)\u0002SF (1)where 3600 is the number of seconds per hour and QI(i;s)\nis the execution time for each one of the queries i.RI(j;s)\nis the execution time of refresh functions j(insert/update) in\nthe query stream s, andSFis the scale factor or database\nsize, ranging from 1to100;000according to its @Size .\nTheThroughput @Size measures the ability of the sys-\ntem to process the most queries in the least amount of time,\ntaking advantage of I/O and CPU parallelism [24]. It com-\nputes the performance of the system against a multi-user\nworkload performed in an elapsed time, using Equation 2:\nThroughput @Size =S\u000222\nTS\u00023600\u0002SF (2)\nwhereSis the number of query streams executed, and TSis\nthe total time required to run the test for sstreams.\nQphH @Size =p\nPower @Size\u0002Throughput @Size (3)\nEquation 3 shows the Query-per-Hour Performance\n(QphH @Size ) metric, which is obtained from the geomet-\nric mean of the previous two metrics and reﬂects multiple\naspects of the capability of a database to process queries.\nTheQphH @Size metric is the ﬁnal output metric of the\nbenchmark and summarizes both single-user and multiple-\nuser overall database performance.\nArchitecture\nIn this section, we introduce our database indexing architec-\nture to automatically choose indexes in relational databases,\nwhich we refer to as SmartIX. The main motivation of Smar-\ntIX is to abstract the database administrator’s task that in-\nvolves a frequent analysis of all candidate columns and ver-\nifying which ones are likely to improve the database index\nconﬁguration. For this purpose, we use reinforcement learn-\ning to explore the space of possible index conﬁgurations in\nthe database, aiming to ﬁnd an optimal strategy over a long\ntime horizon while improving the performance of an agent\nin the environment.\nThe SmartIX architecture is composed of a reinforcement\nlearning agent, an environment model of a database, and a\nDBMS interface to apply agent actions to the database. The\nreinforcement learning agent is responsible for the decision\nmaking process. The agent interacts with an environment\nSmartIX\nRL Agent\nEnvironment\nDBMS Interface\nDBMSAction atState st+1\nReward rt+1\nIndexing\noption\nChanges\nto applyDatabase\nstatsStructured\nDB stats\nFigure 1: SmartIX architecture.\n\nmodel of the database, which computes system transitions\nand rewards that the agent receives for its decisions. To make\nchanges persistent, there is a DBMS interface that is respon-\nsible for communicating with the DBMS to create or drop\nindexes and get statistics of the current index conﬁguration.\nAgent\nOur agent is based on the Deep Q-Network agent proposed\nby [11]. Algorithm 1 consists of a reinforcement learning\nmethod built around the Q-learning, using a neural network\nfor function approximation, and a replay memory for ex-\nperience replay. The neural network is used to approximate\nthe action-value function and is trained using mini-batches\nof experience randomly sampled from the replay memory.\nAt each time step, the agent performs one transition in the\nenvironment. That is, the agent chooses an action using an\nepsilon-greedy exploration function at the current state, the\naction is then applied in the environment, and the environ-\nment returns a reward signal and the next state. Finally, each\ntransition in the environment is stored in the replay buffer,\nand the agent performs a mini-batch update in the action-\nvalue function.\nAlgorithm 1 Indexing agent with function approximation\nand experience replay. From [23, Ch. 6, p. 131] and [11].\n1: Random initialization of the value function\n2: Empty initialization of a replay memory D\n3:s DBinitialindexconfiguration\n4:foreach step do\n5:a epsilongreedy (s)\n6:s0;r execute (a)\n7: Store experience e=hs;a;r;s0iinD\n8: Sample random mini-batch of experiences e\u0018D\n9: Perform experience replay using mini-batch\n10:s s0\nEnvironment\nThe environment component is responsible for computing\ntransitions in the system and computing the reward function.\nTo successfully apply a transition, we implement a model of\nthe database environment, modeling states that contain fea-\ntures that are relevant to the agent learning, and a transition\nfunction that is able to modify the state with regard to the\naction an agent chooses. Each transition in the environment\noutputs a reward signal that is fed back to the agent along\nwith the next state, and the reward function has to be infor-\nmative enough so that the agent learns which actions yield\nbetter decisions at each state.\nState representation The state is the formal representa-\ntion of the environment information used by the agent in the\nlearning process. Thus, deciding which information should\nbe used to deﬁne a state of the environment is critical for\ntask performance. The amount of information encoded in a\nstate imposes a trade-off for reinforcement learning agents.\nSpeciﬁcally, that if the state encodes too little information,\nthen the agent might not learn a useful policy, whereas if the\nstate encodes too much information, there is a risk that thelearning algorithm needs too many samples of the environ-\nment that it does not converge to a policy.\nFor the database indexing problem, the state representa-\ntion is deﬁned as a feature vector ~S=~I\u0001~Q, which is a\nresult of a concatenation of the feature vectors ~Iand~Q. The\nfeature vector ~Iencodes information regarding the current\nindex conﬁguration of the database, with length j~Ij=C,\nwhereCis a constant of the total number of columns in the\ndatabase schema. Each element in the feature vector ~Iholds\na binary value, containing 1or0, depending on whether the\ncolumn that corresponds to that position in the vector is in-\ndexed or not. The second part of our state representation is\na feature vector ~Q, also with length j~Qj=C, which en-\ncodes information regarding which indexes were used in last\nqueries received by the database. To organize such informa-\ntion, we set a constant value of Hthat deﬁnes the horizon of\nqueries that we keep track of. To each of the last queries in\na horizonH, we verify whether any of the indexes currently\ncreated in the database are used to run such queries. Each\nposition in the vector ~Qcorresponds to a column and holds\na binary value that is assigned 1if such column is indexed\nand used in the last Hqueries, else 0. Finally, the concate-\nnate both~Iand~Qto generate our binary state vector ~Swith\nlengthj~Sj= 2C.\nActions In our environment, we deﬁne the possible actions\nas a setAof sizeC+ 1. Each one of the Cactions refers\nto one column in the database schema. These actions are\nimplemented as a “ﬂip” to create or drop an index in the cur-\nrent column. Therefore, for each action, there are two pos-\nsible behaviors: CREATE INDEX orDROP INDEX on the cor-\nresponding column. The last action is a “do nothing” action,\nthat enables the agent not to modify the index conﬁguration\nin case it is not necessary at the current state.\nReward Deciding the reward function is critical for the\nquality of the ensuing learned policy. On the one hand, we\nwant the agent to learn that indexes that are used by the\nqueries in the workload must be maintained in order to op-\ntimize such queries. On the other hand, indexes that are\nnot being used by queries must not be maintained as they\nconsume system resources and are not useful to the current\nworkload. Therefore, we compute the reward signal based\non the next state’s feature vector ~Safter an action is applied,\nsince our state representation encodes information both on\nthe current index conﬁguration and on the indexes used in\nthe last queries, i.e. information contained in vectors ~Iand\n~Q. Our reward function is computed using Equation 4:\nR(op;use ) = (1\u0000op)((1\u0000use)(1) + (use)(\u00005))\n+(op)((1\u0000use)(\u00005) + (use)(1))(4)\nwhereop=Icanduse=Qc. That is, the ﬁrst parameter\nopholds 0if the last action represents a dropped index in\ncolumnc, or1if created an index. The latter parameter, use,\nholds 0if an index in column cis not being used by the last\nHhorizon queries, and 1otherwise.\n\nTable 1: TPC-H database - Table stats and indexes\nTable Total Indexed Indexable\nREGION 3 1 2\nNATION 4 2 2\nPART 9 1 8\nSUPPLIER 7 2 5\nPARTSUPP 5 2 3\nCUSTOMER 8 2 6\nORDERS 9 2 7\nLINEITEM 16 4 12\nTotals 61 16 45\nTherefore, our reward function returns a value of +1if\nan index is created and it actually beneﬁts the current work-\nload, or if an index is dropped and it is not beneﬁcial to the\ncurrent workload. Otherwise, the function returns \u00005to pe-\nnalize the agent if an index is dropped and it is beneﬁcial to\nthe current workload, or an index is created and it does not\nbeneﬁt the current workload. The choice of values +1and\n\u00005is empirical. However, we want the penalization value to\nbe at least twice smaller than the +1value, so that the val-\nues do not get canceled when accumulating with each other.\nFinally, if the action corresponds to a “do nothing” opera-\ntion, the environment simply returns a reward of 0, without\ncomputing Equation 4.\nExperiments\nExperimental setup\nDatabase setup For experimental purposes and due to its\nusage in literature for measuring database performance, we\nchoose to run experiments using the database schema and\ndata provided by the TPC-H benchmark. The tools provided\nby TPC-H include a data generator (DBGen), which is able\nto create up to 100TB of data to load in a DBMS, and a\nquery generator (QGen) that creates 22 queries with differ-\nent levels of complexity. The database we use in these exper-\niments is populated with 1GB of data. To run benchmarks\nusing each baseline index conﬁguration, we implemented\nthe TPC-H benchmark protocol using a Python script that\nruns queries, fetches execution time, and computes the per-\nformance metrics.\nTo provide statistics on the database, we show the in Ta-\nble 1 the number of columns that each table contains and an\nanalysis on the indexing possibilities. For that, we mapped\nfor each table in the TPC-H database the total number of\ncolumns, the columns that are already indexed (primary\nand foreign keys, indexed by default), and the remaining\ncolumns that are available for indexing.\nBy summing the number of indexable columns in each\ntable, we have a total of 45 columns that are available for\nindexing. Since a column is either indexed or not, there\nare two possibilities for each of the remaining 45 index-\nable columns. This scenario indicates that we have exactly\n35;184;372;088;832(245), i.e. more than 35 trillion, pos-\nsible conﬁgurations of simple indexes. Thus, this is also the\nnumber of states that can be assumed by the database index-\ning conﬁguration and therefore explored by the algorithms.For comparison purposes, we manually check which\ncolumns compose the ground truth optimal index conﬁg-\nuration. We manually create each index possibility and\ncheck whether an index beneﬁts at least one query within\nthe 22 TPC-H queries. To check whether an index is\nused or not, we run the EXPLAIN command to visual-\nize the execution plan of each query. Finally, we have 6\ncolumns from the TPC-H that compose our ground truth op-\ntimal indexes: CACCTBAL ,LSHIPDATE ,OORDERDATE ,\nPBRAND ,PCONTAINER ,PSIZE.\nBaselines The baselines comprise different indexing con-\nﬁgurations using different indexing approaches, including\ncommercial and open-source database advisors, and re-\nlated work on genetic algorithms and reinforcement learn-\ning methods. Each baseline index conﬁguration is a result\nof training or analyzing the same workload of queries, from\nthe TPC-H benchmark, in order to make an even compar-\nison between the approaches. The following list brieﬂy in-\ntroduces each of them. Default : indexes only on primary\nand foreign keys; All indexed : all columns indexed. Ran-\ndom: indexes randomly explored by an agent; EDB 2019\nandPOWA 2019: indexes obtained using a comercial and\nan open-source advisor tool, respectively. ITLCS 2018 and\nGADIS 2019: indexes obtained using genetic algorithms re-\nlated work; NoDBA 2018 and rCOREIL 2016: indexes ob-\ntained using reinforcement learning related work.\nThe EDB 2019, POWA 2019, and ITLCS 2018 index\nconﬁgurations are a result of a study conducted by Pe-\ndrozo, Nievola and Ribeiro 2018. The authors 2018 em-\nploy these methods to verify which indexes are suggested\nby each method to each of the 22 queries in the TPC-H\nworkload, whose indexes constitute the respective conﬁgu-\nrations we use in this analysis. The index conﬁgurations of\nGADIS 2019, NoDBA 2018, and rCOREIL 2016 are a result\nof experiments we ran using source-code provided by the\nauthors. We execute the author’s algorithms without modi-\nfying any hyper-parameter except conﬁguring the database\nconnection. The index conﬁguration we use in this analysis\nis the one in which each algorithm converged to, when the al-\ngorithm stops modifying the index conﬁguration or reaches\nthe end of training.\nAgent training\nTraining the reinforcement learning agent consists of time\nsteps of agent-environment interaction and value function\nupdates until it converges to a policy as desired. In our\ncase, to approximate the value function, we use a simple\nmulti-layer perceptron neural network with two hidden lay-\ners and ReLU activation, and an Adam optimizer with mean-\nsquared error loss, both PyTorch 1.5.1 implementations us-\ning default hyperparameters [15]. The input and output di-\nmensions depend on the number of columns available to in-\ndex in the database schema, as shown in Section .\nThe hyperparameters used while training are set as fol-\nlows. The ﬁrst, learning rate \u000b= 0:0001 anddiscount fac-\ntor\r= 0:9, are used in the update equation of the value\nfunction. The next are related to experience replay, where\nreplay memory size = 10000 deﬁnes the number of experi-\n\n0 100 200 300 400 500\nStep300\n250\n200\n150\n100\n50\n050Accumulated reward(a) Accumulated reward per 128\nsteps.\n0 100 200 300 400 500\nStep0200400600800100012001400Accumulated loss(b) Accumulated loss per 128 steps.\n0 10000 20000 30000 40000 50000 60000\nStep0510152025Number of indexesTotal indexes\nTotal optimal indexes\nGround truth optimal indexes (c) Index conﬁgurations while\ntraining.\nFigure 2: Training statistics.\nences the agent is capable of storing, and replay batch size =\n1024 deﬁnes the number of samples the agent uses at each\ntime step to update the value function. The last are related\nto the epsilon-greedy exploration function, where we de-\nﬁne an epsilon initial = 1 as maximum epsilon value, an\nepsilon ﬁnal = 0:01as epsilon minimum value, a percent-\nage in which epsilon decays = 1% , and the interval of\ntime steps at each decay = 128 .\nWe train our agent for the course of 64 thousand time\nsteps in the environment. Training statistics are gathered ev-\nery 128 steps and are shown in Figure 2. Sub-ﬁgure 2a shows\nthe total reward accumulated by the agent at each 128 steps\nin the environment, which consistently improves over time\nand stabilizes after the 400th x-axis value. Sub-ﬁgure 2b\nshows the accumulated loss at each 128 steps in the envi-\nronment, i.e. the errors in predictions of the value function\nduring experience replay, and illustrates how it decreases to-\nwards zero as parameters are adjusted and the agent approx-\nimates the true value function.\nTo evaluate the agent behavior and the index conﬁgura-\ntion in which the agent is converging to, we plot in Figure 2c\neach of the index conﬁgurations explored by the agent in the\n64 thousand training steps. Each index conﬁguration is rep-\nresented in terms of total indexes andtotal optimal indexes\na conﬁguration contains. Total indexes is simply a count on\nthe number of indexes in the conﬁguration, while total opti-\nmal indexes is a count on the number of ground truth optimal\nindexes in the conﬁguration. The lines are smoothed using a\nrunning mean of the last 5 values, and a ﬁxed red dashed\nline across the x-axis represents the conﬁguration in which\nthe agent should converge to. As we can see, both the total\namount of indexes and the total optimal indexes converge\ntowards the ground truth optimal indexes. That is, the agent\nlearns both to keep the optimal indexes in the conﬁguration,\nas well as to drop irrelevant indexes for the workload.\nPerformance Comparison\nWe now evaluate each baseline index conﬁguration in com-\nparison to the one in which our agent converged to in the last\nepisode of training. We show the TPC-H performance metric\n(QphH, i.e. the query-per-hour metric) and the index size of\neach conﬁguration. Figure 3a shows the query-per-hour met-\nric of each conﬁguration (higher values denote better perfor-\nmance). The plotted values are a result of a trimmed mean,\nwhere we run the TPC-H benchmark 12 times for each in-dex conﬁguration, removing the highest and the lowest result\nand averaging the 10 remaining results. Figure 3b shows the\ndisk space required for the indexes in each conﬁguration (in-\ndex size in MB), which allows us to analyze the trade-off in\nthe number of indexes and the resources needed to maintain\nit. In an ideal scenario, the index size is just the bare min-\nimum to maintain the indexes that are necessary to support\nquery performance.\nYet SmartIX achieves the best query-per-hour-metric, the\ntwo genetic algorithms [12] and [17] have both very similar\nquery-per-hour and index size metrics in comparison to our\nagent. GADIS [12] itself uses a similar state-space model to\nSmartIX, with individuals being represented as binary vec-\ntors of the indexable columns. The ﬁtness function GADIS\noptimizes is the actual query-per-hour metric, and it runs the\nwhole TPC-H benchmark every time it needs to compute\nthe ﬁtness function. Therefore, it is expected that it ﬁnds an\nindividual with a high performance metric, although it is un-\nrealistic for real-world applications in production due to the\ncomputational cost of running the benchmark.\nIndexing all columns is among the highest query-per-hour\nresults and can seem to be a natural alternative to solve the\nindexing problem. However, it results in the highest amount\nof disk used to maintain indexes stored. Such alternative\nis less efﬁcient in a query-per-hour metric as the bench-\nmark not only takes into account the performance of SE-\nLECT queries, but also INSERT and DELETE operations,\nwhose performance is affected by the presence of indexes\ndue to the overhead of updating and maintaining the struc-\nture when records change [21, Ch. 8, p. 290-291]. It has the\nlowest ratio value due to the storage it needs to maintain in-\ndexes.\nDefaultRandomNoDBA POWArCOREILEDB\nAll indexesITLCSGADISSmartIX\nIndex configuration95097510001025105010751100QphH@1GB\n(a) Query-per-hour metric\nDefaultSmartIXITLCSGADISEDBPOWANoDBARandomrCOREIL\nAll indexes\nIndex configuration05001000150020002500Size in MB (b) Index size (in MB)\nFigure 3: Static index conﬁgurations results.\n\nWhile rCOREIL [1] is the most competitive reinforce-\nment learning method in comparison to SmartIX, the amount\nof storage used to maintain its indexes is the highest among\nall baselines (except for having all columns indexed). rCOR-\nEIL does not handle whether primary and foreign key in-\ndexes are already created, causing it to create duplicate in-\ndexes. The policy iteration algorithm used in rCOREIL is a\ndynamic programming method used in reinforcement learn-\ning, which is characterized by complete sweeps in the state\nspace at each iteration in order to update the value function.\nSince dynamic programming methods are not suitable to\nlarge state spaces [23, Ch. 4, p. 87], this can become a prob-\nlem in databases that contain a larger number of columns to\nindex.\nAmong the database advisors, the commercial tool\nEDB [6] achieves the highest query-per-hour metric in com-\nparison to the open-source tool POWA [20], while its in-\ndexes occupy virtually the same disk space. Other baselines\nand related work are able to optimize the index conﬁguration\nand have lightweight index sizes, but are not competitive in\ncomparison to the previously discussed methods in terms of\nthe query-per-hour performance metric. Finally, among all\nthe baselines, the index conﬁguration obtained using Smar-\ntIX not only yields the best query-per-hour metric but also\nthe smallest index size (except for the default conﬁguration),\ni.e. it ﬁnds the balance between performance and storage, as\nshown in the ratio plot.\nDynamic conﬁgurations\nThis section aims to evaluate the behavior of algorithms\nthat generate policies, i.e. generate a function that guides\nan agent’s behavior. The three algorithms that generate poli-\ncies are SmartIX, rCOREIL, and NoDBA. The three are\nreinforcement learning algorithms, although using different\nstrategies (see Sec. ). While rCOREIL and SmartIX show a\nmore interesting and dynamic behavior, the NoDBA algo-\nrithm shows a ﬁxed behavior and keeps only three columns\nindexed over the whole time horizon, without changing the\nindex conﬁguration over time (see its limitations in Sec. ).\nTherefore, we do not include NoDBA in the following anal-\nysis and focus the discussion on rCOREIL and SmartIX.\nFixed workload\nWe now evaluate the index conﬁguration of rCOREIL and\nSmartIX over time while the database receives a ﬁxed work-\nload of queries. Figure 4 shows the behavior of rCOREIL\nand SmartIX, respectively. Notice that rCOREIL takes some\ntime to create the ﬁrst indexes in the database, after receiv-\ning about 150 queries, while SmartIX creates indexes at the\nvery beginning of the workload. On the one hand, rCOR-\nEIL shows a ﬁxed behavior maintains all ground truth opti-\nmal indexes, but it creates a total of 22 indexes, 16 of those\nbeing unnecessary indexes and the remaining 6 are optimal\nindexes. On the other hand, SmartIX shows a dynamic be-\nhavior and consistently maintains 5 out of the 6 ground truth\noptimal indexes, and it does not maintain unnecessary in-\ndexes throughout most of the received workload.\n0 200 400 600 800 1000\nStep0510152025Number of indexesTotal indexes\nTotal optimal indexes\nGround truth optimal indexes(a) rCOREIL\n0 200 400 600 800\nStep0510152025Number of indexesTotal indexes\nTotal optimal indexes\nGround truth optimal indexes (b) SmartIX\nFigure 4: Agent behavior with a ﬁxed workload.\n0 200 400 600 800\nStep0510152025Number of indexesWorkload shifts\nGround truth optimal indexes\nTotal indexes\nTotal optimal indexes\n(a) rCOREIL\n0 200 400 600 800\nStep0510152025Number of indexesWorkload shifts\nGround truth optimal indexes\nTotal indexes\nTotal optimal indexes (b) SmartIX\nFigure 5: Agent behavior with a shifting workload.\nShifting workload\nWe now evaluate the algorithm’s behavior while receiving a\nworkload that shifts over time. To do so, we divide the 22\nTPC-H queries into two sets of 11 queries, where for each\nset there is a different ground truth set of indexes. That is,\nout of the 6 ground truth indexes from the previous ﬁxed\nworkload, we now separate the workload to have 3 indexes\nthat are optimal ﬁrst set of queries, and 3 other indexes that\nare optimal for the second set of queries. Therefore, we aim\nto evaluate whether the algorithms can adapt the index con-\nﬁguration over time when the workload shifts and a different\nset of indexes is needed according to each of the workloads.\nThe behavior of each algorithm is shown in Figure 5. The\nvertical dashed lines placed along the x-axis represent the\ntime step where the workload shifts from one set of queries\nto another, and therefore the set of ground truth optimal in-\ndexes also changes. On the one hand, notice that rCOREIL\nshows a similar behavior from the one in the previous ﬁxed\nworkload experiment, in which it takes some time to create\nthe ﬁrst indexes, and then maintains a ﬁxed index conﬁg-\nuration, not adapting as the workload shifts. On the other\nhand, SmartIX shows a more dynamic behavior with regard\nto the shifts in the workload. Notice that, at the beginning\nof each set of queries in the workload, there is a peak in the\ntotal indexes, which decreases as soon as the index conﬁgu-\nration adapts to the new workload and SmartIX drops the un-\nnecessary indexes with regard to the current workload. Even\nthough rCOREIL maintains all 3 ground truth indexes over\ntime, it still maintains 16 unnecessary indexes, while Smar-\ntIX consistently maintains 2 out of 3 ground truth optimal\nindexes and adapts as the workload shifts.\nScaling up database size\nIn the previous sections, we showed that the SmartIX archi-\ntecture can consistently achieve near-optimal index conﬁgu-\n\n0 200 400 600 800\nStep0510152025Number of indexesTotal indexes\nTotal optimal indexes\nGround truth optimal indexes(a) 10GB TPC-H database.\n0 200 400 600 800\nStep0510152025Number of indexesTotal indexes\nTotal optimal indexes\nGround truth optimal indexes (b) 100GB TPC-H database.\nFigure 6: Agent behavior in larger databases.\nrations in a database of size 1GB. In this section, we report\nexperiments on indexing larger databases, where we transfer\nthe policy trained in the 1GB database to perform indexing\nin databases with size 10GB and 100GB. We plot the behav-\nior of our agent in Figure 6.\nAs we can see, the agent shows a similar behavior to the\none using a 1GB database size reported in previous experi-\nments. The reason is that both the state features and the re-\nward function are not inﬂuenced by the database size. The\nonly information relevant to the state and the reward func-\ntion is the current index conﬁguration and the workload be-\ning received. Therefore, we can successfully transfer the\nvalue function learned in smaller databases to index larger\ndatabases, consuming fewer resources to train the agent.\nRelated Work\nMachine learning techniques are used in a variety of tasks\nrelated to database management systems and automated\ndatabase administration [26]. One example is the work from\nKraska et al. [8], which outperforms traditional index struc-\ntures used in current DBMS by replacing them with learned\nindex models, having signiﬁcant advantages under particu-\nlar assumptions. Pavlo et. al [16] developed Peloton, which\nautonomously optimizes the system for incoming workloads\nand uses predictions to prepare the system for future work-\nloads. In this section, though, we further discuss related\nwork that focused on developing methods for optimizing\nqueries through automatic index tuning. Speciﬁcally, we fo-\ncus our analysis on work that based their approach on rein-\nforcement learning techniques.\nBasu et al. [1] developed a technique for index tuning\nbased on a cost model that is learned with reinforcement\nlearning. However, once the cost model is known, it be-\ncomes trivial to ﬁnd the conﬁguration that minimizes the\ncost through dynamic programming, such as the policy itera-\ntion method used by the authors. They use DBTune [3] to re-\nduce the state space by considering only indexes that are rec-\nommended by the DBMS. Our approach, on the other hand,\nfocuses on ﬁnding the optimal index conﬁguration without\nhaving complete knowledge of the environment and without\nheuristics of the DBMS to reduce the state space.\nSharma et al. [22] use a cross-entropy deep reinforce-\nment learning method to administer databases automatically.\nTheir set of actions, however, only include the creation of\nindexes, and a budget of 3 indexes is set to deal with space\nconstraints and index maintenance costs. Indexes are onlydropped once an episode is ﬁnished. A strong limitation in\ntheir evaluation process is to only use the L INEITEM table\nto query, which does not exploit how indexes on other tables\ncan optimize the database performance, and consequently\nreduces the state space of the problem. Furthermore, they\ndo not use the TPC-H benchmark performance measure to\nevaluate performance but use query execution time in mil-\nliseconds.\nOther papers show that reinforcement learning can also be\nexplored in the context of query optimization by predicting\nquery plans: Marcus et al. [10] proposed a proof-of-concept\nto determine the join ordering for a ﬁxed database; Ortiz et\nal. [14] developed a learning state representation to predict\nthe cardinality of a query. These approaches could possibly\nbe used alongside ours, generating better plans to query ex-\necution while we focus on maintaining indexes that these\nqueries can beneﬁt from.\nConclusion\nIn this research, we developed the SmartIX architecture for\nautomated database indexing using reinforcement learning.\nThe experimental results show that our agent consistently\noutperforms the baseline index conﬁgurations and related\nwork on genetic algorithms and reinforcement learning. Our\nagent is able to ﬁnd the trade-off concerning the disk space\nits index conﬁguration occupies and the performance metric\nit achieves. The state representation and the reward function\nallows us to successfully index larger databases while train-\ning in smaller databases and consuming fewer resources.\nRegarding the limitations of our architecture, we do not\nyet deal with composite indexes due to the resulting state\nspace of all possible indexes that use two or more columns.\nOur experiments show results using workloads that are read-\nintensive (i.e. intensively fetching data from the database),\nwhich is exactly the type of workload that beneﬁts from in-\ndexes. However, experiments using write-heavy workloads\n(i.e. intensively writing data to the database) can be inter-\nesting to verify whether the agent learns to avoid indexes\nin write-intensive tables. Considering these limitations, in\nfuture work, we plan to: (1) investigate techniques that al-\nlow us to deal with composite indexes; (2) improve the re-\nward function to provide feedback in case of write-intensive\nworkloads; (3) investigate pattern recognition techniques to\npredict incoming queries to index ahead of time; and (4)\nevaluate SmartIX on big data ecosystems (e.g. Hadoop).\nFinally, our contributions include: (1) a formalization of\na reward function shaped for the database indexing prob-\nlem, independent of DBMS’s statistics, that allows the agent\nto adapt the index conﬁguration according to the workload;\n(2) an environment representation for database indexing that\nis independent of schema or DBMS; and (3) a reinforce-\nment learning agent that efﬁciently scales to large databases,\nwhile trained in small databases consuming fewer resources.\nAt last, as a result of this research, we published a paper at\nthe Applied Intelligence journal [9].\nIn closing, we envision this kind of architecture being de-\nployed in cloud platforms such as Heroku and similar plat-\nforms that often provide database infrastructure for various\nclients’ applications. The reality is that these clients do not\n\nprioritize, or it is not in their scope of interest to focus on\ndatabase management. Especially in the case of early-stage\nstart-ups, the aim to shorten time-to-market and quickly ship\ncode motivates externalizing complexity on third party so-\nlutions [7]. From an overall platform performance point of\nview, having efﬁcient database management results in an op-\ntimized use of hardware and software resources. Thus, in the\nlack of a database administrator, the SmartIX architecture is\na potential stand-in solution, as experiments show that it pro-\nvides at least equivalent and often superior indexing choices\ncompared to baseline indexing recommendations.\nReferences\n[1] Basu, D.; Lin, Q.; Chen, W.; V o, H. T.; Yuan, Z.; Senel-\nlart, P.; and Bressan, S. 2016. Regularized cost-model\noblivious database tuning with reinforcement learning.\nInTransactions on Large-Scale Data-and Knowledge-\nCentered Systems XXVIII . Springer. 96–132.\n[2] Bellman, R. 1957. A markovian decision process. Jour-\nnal of Mathematics and Mechanics 679–684.\n[3] DB Group at UCSC. 2019. DBTune.\n[4] Duan, S.; Thummala, V .; and Babu, S. 2009. Tuning\ndatabase conﬁguration parameters with ituned. Proceed-\nings of the VLDB Endowment 2(1):1246–1257.\n[5] Elfayoumy, S., and Patel, J. 2012. Database perfor-\nmance monitoring and tuning using intelligent agent as-\nsistants. In IKE proceedings , 1.\n[6] EnterpriseDB. 2019. Enterprise Database.\n[7] Giardino, C.; Paternoster, N.; Unterkalmsteiner, M.;\nGorschek, T.; and Abrahamsson, P. 2016. Software de-\nvelopment in startup companies: the greenﬁeld startup\nmodel. IEEE Transactions on Software Engineering\n42(6):585–604.\n[8] Kraska, T.; Beutel, A.; Chi, E. H.; Dean, J.; and Polyzo-\ntis, N. 2018. The case for learned index structures. In\nProceedings of the 2018 SIGMOD , 489–504. ACM.\n[9] Licks, G. P.; Couto, J. C.; de F ´atima Miehe, P.; De Paris,\nR.; Ruiz, D. D.; and Meneguzzi, F. 2020. Smartix: A\ndatabase indexing agent based on reinforcement learning.\nApplied Intelligence 1–14.\n[10] Marcus, R., and Papaemmanouil, O. 2018. Deep re-\ninforcement learning for join order enumeration. CoRR\nabs/1803.00055:1–7.\n[11] Mnih, V .; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.;\nVeness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.;\nFidjeland, A. K.; Ostrovski, G.; et al. 2015. Human-\nlevel control through deep reinforcement learning. Na-\nture518(7540):529–533.\n[12] Neuhaus, P.; Couto, J.; Wehrmann, J.; Ruiz, D.; and\nMeneguzzi, F. 2019. GADIS: A genetic algorithm for\ndatabase index selection. In 31st SEKE proceedings , 39–\n42.\n[13] Olofson, C. W. 2018. Ensuring a fast, reliable, and\nsecure database through automation: Oracle autonomousdatabase. White paper, IDC Corporate USA, Sponsored\nby: Oracle Corp.\n[14] Ortiz, J.; Balazinska, M.; Gehrke, J.; and Keerthi,\nS. S. 2018. Learning state representations for query\noptimization with deep reinforcement learning. CoRR\nabs/1803.08604:1–5.\n[15] Paszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury,\nJ.; Chanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.;\nAntiga, L.; Desmaison, A.; Kopf, A.; Yang, E.; DeVito,\nZ.; Raison, M.; Tejani, A.; Chilamkurthy, S.; Steiner, B.;\nFang, L.; Bai, J.; and Chintala, S. 2019. Pytorch: An\nimperative style, high-performance deep learning library.\nIn32nd NeurIPS proceedings . Curran Associates, Inc.\n8024–8035.\n[16] Pavlo, A.; Angulo, G.; Arulraj, J.; Lin, H.; Lin, J.; Ma,\nL.; Menon, P.; Mowry, T. C.; Perron, M.; Quah, I.; et al.\n2017. Self-driving database management systems. In\nCIDR proceedings , 1–6.\n[17] Pedrozo, W. G.; Nievola, J. C.; and Ribeiro, D. C.\n2018. An adaptive approach for index tuning with learn-\ning classiﬁer systems on hybrid storage environments. In\nHAIS , 716–729. Springer.\n[18] Petraki, E.; Idreos, S.; and Manegold, S. 2015. Holistic\nindexing in main-memory column-stores. In 2015 SIG-\nMOD proceedings , 1153–1166. New York, NY , USA:\nACM.\n[19] Popovic, J. 2017. Automatic tuning - SQL Server.\nAccessed: 2019-06-17.\n[20] POWA. 2019. PostreSQL Workload Analyzer.\n[21] Ramakrishnan, R., and Gehrke, J. 2003. Database\nManagement Systems . New York, NY , USA: McGraw-\nHill, Inc., 3 edition.\n[22] Sharma, A.; Schuhknecht, F. M.; and Dittrich, J. 2018.\nThe case for automatic database administration using\ndeep reinforcement learning. CoRR abs/1801.05643:1–\n9.\n[23] Sutton, R. S., and Barto, A. G. 2018. Reinforcement\nlearning: An introduction . Cambridge, Massachusetts:\nMIT press.\n[24] Thanopoulou, A.; Carreira, P.; and Galhardas, H. 2012.\nBenchmarking with tpc-h on off-the-shelf hardware. In\n14th ICEIS , 205–208. Wroclaw, Poland: Springer.\n[25] Tsitsiklis, J. N., and Roy, B. V . 1996. An analysis\nof temporal-difference learning with function approxima-\ntion. Technical report, Laboratory for Information and\nDecision Systems, MIT.\n[26] Van Aken, D.; Pavlo, A.; Gordon, G. J.; and Zhang,\nB. 2017. Automatic database management system tuning\nthrough large-scale machine learning. In 2017 SIGMOD ,\n1009–1024. ACM.\n[27] Wang, J.; Liu, W.; Kumar, S.; and Chang, S.-F. 2016.\nLearning to hash for indexing big data—a survey. Pro-\nceedings of the IEEE 104(1):34–57.",
  "textLength": 43226
}