{
  "paperId": "872e5893a327d77e5b1e48e7f8e770f7972d5087",
  "title": "How Good Are Multi-dimensional Learned Indices? An Experimental Survey",
  "pdfPath": "872e5893a327d77e5b1e48e7f8e770f7972d5087.pdf",
  "text": "Noname manuscript No.\n(will be inserted by the editor)\nHow Good Are Multi-dimensional Learned Indices? An\nExperimental Survey\nQiyu Liu*,†·Maocheng Li*·Yuxiang Zeng*·Yanyan Shen ·Lei Chen\nReceived: date / Accepted: date\nAbstract Efficient indexing is fundamental for multi-\ndimensional data management and analytics. An emerg-\ning tendency is to directly learn the storage layout of\nmulti-dimensional data by simple machine learning mod-\nels, yielding the concept of Learned Index . Compared\nwith the conventional indices used for decades (e.g.,\nkd-tree and R-tree variants), learned indices are em-\npirically shown to be both space- and time-efficient on\nmodern architectures. However, there lacks a compre-\nhensive evaluation of existing multi-dimensional learned\nindices under a unified benchmark, which makes it dif-\nficult to decide the suitable index for specific data and\nqueries and further prevents the deployment of learned\nindices in real application scenarios. In this paper, we\npresent the first in-depth empirical study to answer the\nquestion of how good multi-dimensional learned indices\nare. Six recently published indices are evaluated under a\nunified experimental configuration including index im-\n*These three authors contributed equally to this work.\n†Qiyu Liu is the corresponding author.\nQiyu Liu\nSouthwest University\nE-mail: qyliu.cs@gmail.com\nMaocheng Li\nHong Kong University of Science and Technology\nE-mail: csmichael@cse.ust.hk\nYuxiang Zeng\nBeihang University\nE-mail: yxzeng@buaa.edu.cn\nYanyan Shen\nShanghai Jiao Tong University\nE-mail: shenyy@sjtu.edu.cn\nLei Chen\nHong Kong University of Science and Technology & Hong\nKong University of Science and Technology (GZ)\nE-mail: leichen@ust.hkplementation, datasets, query workloads, and evalua-\ntion metrics. We thoroughly investigate the evaluation\nresults and discuss the findings that may provide in-\nsights for future learned index design.\nKeywords Learned Index ·Spatial Index ·Bench-\nmark·Experimental Study\n1 Introduction\nMulti-dimensional data management and analytics play\nan important role in various domains such as business\nintelligence [11], smart transportation [69], neural sci-\nence [55], climate studies [16], etc. As the data vol-\nume grows at an exponential speed, conventional multi-\ndimensional indices like R-tree [23] and its variants [5,\n28,3,59] are designed to speedup data access and query\nprocessing over big multi-dimensional databases.\nAlthough traditional index structures like B+-tree\nand R-tree have been studied and embedded into prac-\ntical DBMS for decades (e.g., Oracle [29] and Post-\ngreSQL [48]), a recent proposal [32] introduced a new\nindex design paradigm called Learned Index based on\nthe observation that data indexing can be modeled as\na machine learning problem where the input is a search\nkey and the output is its corresponding location onto\nthe storage. Supposing that a set of Nkeys are sorted\nand stored as consecutive data pages, a B+-tree can\nbe viewed as a mapping from key xto its page ID.\nThus, an error-bounded Cumulative Distribution Func-\ntion (CDF) model is functionally equivalent to a B+-\ntree index. Compared with traditional indexes, learned\nindex is supposed to be both space- and time-efficient\nas a trained model (e.g., piece-wise linear function) is\nusually compact and simple for inference.arXiv:2405.05536v1  [cs.DB]  9 May 2024\n\n2 Qiyu Liu et al.\nZM-Index\nML-IndexFlood\nTsunami\nIF-IndexLisa\nRSMIR-tree\nQuad-tree\nM-tree\niDistancekd-tree\nGrid\nAn edge from index A to B\ndenotes that A compares with\nB in previous literature.A B\nFig. 1: Illustration of limited comparison in previous studies.\nInspired by the impressive results obtained from 1-\ndimensional learned index [32,38,18,63,57], learned in-\ndices on multi-dimensional data are intensively stud-\nied during the past years such as ZM-Index [62], ML-\nIndex [13], IF-Index [24], RSMI [50], LISA [35], Flood\n[41], and Tsunami [15]. These works independently claim\nthat they are empirically more performant than tradi-\ntional spatial indices like R-tree or kd-tree. However, to\nthe best of our knowledge, there lacks a comprehensive\nevaluation for published multi-dimensional learned in-\ndices under a unified experimental configuration, which\nobscures the impact and future direction of this promi-\nnent research field. The limitations of existing experi-\nments and evaluations are summarized as follows.\nFirst, the newly proposed indices lack enough com-\nparisons with previous studies. Figure 1 visualizes the\ncomparison relationship of existing works where a node\nrefers to an index and an edge A→Brefers to that\nindex Acompares with Bin previous literature. From\nFigure 1, most of the previous works only compare with\nZM-Index [62] that combines the space-filling curves\nand 1-dimensional learned index. However, the orig-\ninal ZM-Index implementation is less optimized (see\nSection 5.3 for details), meaning that a weak baseline\nwas most frequently compared. Besides, existing com-\nprehensive benchmarks [38,57] evaluate 1-D learned in-\ndices only, which cannot be reused for multi-dimensional\ncases.\nSecond, the existing learned indices are not com-\npared under a unified configuration including index im-\nplementation, datasets, query workloads, and evalua-\ntion metrics. For example, a class of multi-dimensional\nlearned indices [62,31,13,41,15] utilize 1-D learned in-\ndex (e.g., RMI [32]) as building blocks, but different\nimplementations are used in different indices, leading\nto an unfair comparison and unconvincing results on\ntheir true performance.Unlike the experiments in previous studies where\nonly the ZM-Index [62] was compared, in this work,\nwere-implement andoptimize six multi-dimensional\nlearned indices to perform a comprehensive evaluation,\nwhich is a nearly complete coverage to the best of our\nknowledge. To make the comparison fair and the eval-\nuation results more convincing, we standardize the ex-\nperiment configurations, including the index implemen-\ntation, datasets, and evaluation query workloads. This\nalso benefits future research as newly proposed multi-\ndimensional learned indices can be easily evaluated on\nour benchmark. In addition, though IO-efficiency and\nperformance in distributed DBMS are also important,\nwe focus on the in-memory andsingle-machine1\nperformance of indices over both synthetic and real\nquery workloads, which is similar to a recent 1-D learned\nindex benchmark SOSD [38]. Though losing generality\nto some extent, our empirical study can provide insight-\nful results for in-memory analytical applications, which\nare becoming increasingly prominent [68,65,58].\nIn summary, our experimental study makes the fol-\nlowing contributions.\n–To deliver a fair comparison among existing multi-\ndimensional learned indices, we unify the index im-\nplementation, model training process, query work-\nloads, and evaluation pipeline. Our benchmark im-\nplementation is fully open-sourced2to benefit future\nstudies in this prominent direction.\n–To the best of our knowledge, our work is the first\ncomprehensive andin-depth evaluation for multi-\n1Although there exist multi-dimensional learned indices\nsuch as LISA [35] and RSMI [50] claiming that they are\ndisk-based indices, unfortunately, they do not adopt any disk-\nbased optimization objectives (e.g., page access or IO cost).\nIn these works, the index structures are loaded in the memory,\nbut data pages are stored on the disk, which is intrinsically\nthe same as other memory-based learned indices.\n2https://github.com/qyliu-hkust/learnedbench\n\nHow Good Are Multi-dimensional Learned Indices? An Experimental Survey 3\ndimensional learned indices under a uniform experi-\nmental environment.\n–By thoroughly analyzing the evaluation results, we\ngive the answer to the vital question concerned by\npractical DBMS users, i.e., “ How good are multi-\ndimensional learned indices? ” In addition, based on\nthe key experimental findings, we also identify the\npotential research opportunities for future studies.\nThe rest of this paper is structured as follows. We\nreview the background of learned index and formu-\nlate the multi-dimensional data model and correspond-\ning queries in Section 2. We establish a taxonomy and\nprovide an overview of the existing multi-dimensional\nlearned indices in Section 3. Section 4 introduces the\nindex implementation details and experimental setups.\nSection 5 presents the evaluation results and in-depth\ndiscussions. Finally, we conclude the paper and discuss\nfuture directions in Section 6.\n2 Preliminaries and Background\nIn this section, we overview the backgrounds of multi-\ndimensional data indexing and learned index structures.\n2.1 Multi-dimensional Data Indexing\nAs adopted by most of the existing works, we consider\na collection of Npoints from d-dimensional Euclidean\nspace, and we focus on the range query and knearest\nneighbor query ( kNN) defined as follows.\nDefinition 1 (Point) A point o∈Rdis a vector in\nd-dimensional Euclidean space. Let oidenote the i-th\ncoordinate of oandOdenote the collection of Npoints.\nDefinition 2 (Range Query) Given a collection of\npoints O, a range query takes a d-dimensional hyper-\nrectangle Ras input and returns all the points lying in\nR, i.e., range (R) ={o|o∈R, o∈ O} .\nDefinition 3 ( k-Nearest Neighbor Query) Ak-\nnearest neighbor query kNN (q, k) retrieves kobjects\nfromOwhose distances to qare ranked in ascending or-\nder. Formally, for ∀o∈kNN (q, k),∄o′∈ O\\ kNN (q, k)\nsuch that ||o′, q||2≤ ||o, q||2.\nIndexing and corresponding query processing over\nmulti-dimensional data have been studied and applied\nin commercial DBMS for decades. For low-dimensional\ndata, conventional indices include R-tree [23] and its\nvariants like R*-tree [5], STR-tree [34] and Hilbert R-\ntree [28], kd-tree [6], Grid File [42], etc. On the other\nhand, the data space is becoming inevitably sparse forhigh-dimensional space due to the curse of dimension-\nality, and query processing like kNN query based on the\naforementioned indices will be no better than a na¨ ıve\nlinear scan. To this end, pivot-based methods are gener-\nally adopted to index high-dimensional data, e.g., iDis-\ntance [27], vantage-point (VP) tree [67], MVP-tree [10],\netc. A recent proposal [12] surveyed and evaluated the\nfamily of pivot-based indices on high-dimensional query\nprocessing.\nIn this work, we focus on evaluating indices over\nmulti-dimensional points instead of complicated geome-\ntries like lines, rectangles, or polygons. This is because\nall of our evaluated learned indices do not support these\nnon-point geometries. Moreover, it is non-trivial to make\nsuch extensions by following the extended routines [54]\nof conventional indexes (e.g., R-trees) due to two rea-\nsons. First, existing learned indexes have assumed that\nthe object is a spatial point and utilized techniques\n(e.g., space-filling curve) that heavily rely on this as-\nsumption. Second, these complicated geometries involve\nmore complicated spatial predicates, such as intersect\nandtouch [8], which are currently not supported by ex-\nisting learned indices.\n2.2 Learned Data Indexing\n1-Dimensional Learned Index. A 1-D learned in-\ndex is intrinsically an error-bounded CDF model (scaled\nby the data size N). In the seminal work [32], Kraska\net al. proposed the first learned index RMI that is em-\npirically shown to be Pareto optimal compared with a\nB+-tree index. Following RMI, Kip et al. proposed a\nsimple learned index called RadixSpline [30] that re-\nquires only a single pass of data to construct. Further-\nmore, PGM-Index [18] adopted the optimal piece-wise\nlinear approximation [44] as the underlying CDF model,\nleading to strong theoretical results on the space and\ntime complexity. To handle dynamic operations like key\ninsertion and deletion, Ding et al. proposed ALEX [14]\nby using a gapped array in their structure to handle\nrecord updates; LIPP [63] further improved the up-\ndate efficiency by reducing the last-mile search error\nin leaf nodes. More discussions and comparisons about\n1-D learned indices can be found in a benchmark pa-\nper SOSD [38]. Besides data indexing, there are emerg-\ning attempts of embedding learned models into conven-\ntional data structure and algorithm design, e.g., learned\nBloom filters [32,39,37], learned sorting [33], learned\ndata compression [7,36], etc.\nMulti-Dimensional Learned Index. Similar to how\nan R-tree is a multi-dimensional analog to a B+-tree in-\ndex, it is natural to extend the 1-dimensional learned\n\n4 Qiyu Liu et al.\nTable 1: Overview of the existing multi-dimensional learned index structures.\nIndex ZM-Index ML-Index LISA IF-Index RSMI Flood Tsunami\nReference [62] [13] [35] [24] [50] [41] [15]\nType projection projection projection augmentation augmentation grid grid\nData Ordering Z-curveprojection\nfunctionprojection\nfunctionselected\ndimensionZ-curveselected\ndimensionselected\ndimension\nData Layout order-based order-based order-basedspace\npartitionspace\npartitiongrid grid\nModel RMI RMIpiece-wise\nlinearlinear\ninterpolationMLP RMI RMI\nModel Training algorithmic algorithmic numpy algorithmic PyTorch algorithmic algorithmic\nUpdatable ✕ ✕ ✓ ✓ ✓ ✕ ✕\nSupport Dim. ≥2 ≥2 ≥2 ≥2 2 ≥2 ≥2\nRange Query ✓ ✓ ✓ ✓ ✓ ✓ ✓\nkNN Query ✕ ✓ ✓ ✕ ✓ ✕ ✕\nindex to multi-dimensional datasets by directly learn-\ning the mapping from multi-dimensional keys to their\nstorage location. Designing multi-dimensional learned\nindices has rapidly become a promising research direc-\ntion in the past years. Typical works in this area include\nZM-Index [62], ML-Index [13], IF-Index [24], RSMI [50],\nLisa [35], Flood [41], and Tsunami [15]. In addition, a\nrecent system SageDB [31] also incorporates a learned\ngrid index, which can be viewed as a simplified version\nof Flood [41]. The details of these learned indices will\nbe discussed in Section 3. As we will reveal in this work\nlater, existing multi-dimensional learned indices mainly\nfocus on read-only and single-machine workloads. In ad-\ndition, the query support of these learned indices is still\npreliminary (majorly range query and kNN query), re-\nmaining a tremendous number of research opportunities\n(see discussions in Section 5 and Section 6).\n3 Multi-dimensional Learned Index\nIn this section, we first investigate the data layouts\nadopted by different indices in Section 3.1. Then, we\nprovide an index taxonomy in Section 3.2 and overview\nindices in each category in Section 3.3–3.5. Table 1 sum-\nmarizes the major technical features of existing works.\n3.1 Data Layout\nWe first discuss the multi-dimensional data layout, based\non which we establish our taxonomy for existing multi-\ndimensional learned index structures.\nData layout specifies how an index organizes the\ndata points onto the storage (i.e., disk or memory),\nwhich plays an important role in multi-dimensional in-\ndex design. As shown in Table 1, we identify that the\nexisting learned indices usually employ one of the threetypes of data layouts: ❶order-based layout, ❷space\npartition-based layout, and ❸grid-based layout.\nOrder-based Layout. Multi-dimensional data points\nare organized on storage consistent with a pre-defined\nsorting order. Different from the 1-D case, there is no\nintrinsic sorting order for multi-dimensional data, and\nexisting works usually select a sorting dimension or em-\nploy the space-filling curves (e.g., Z-order curve [51] or\nHilbert curve [28]) to order data.\nPartition-based Layout. The space partition-based\nlayout recursively divides the data space under some\nstrategy (e.g., the middle-point strategy in kd-tree [6])\nuntil a partition threshold is reached. In this case, data\npoints within the same partition are grouped together\nand sequentially materialized to the storage.\nGrid-based Layout. The grid-based layout can be re-\ngarded as a special case of the partition-based layout.\nThe data space is partitioned into grid cells, and points\nfalling into the same grid cell are placed in storage con-\nsecutively.\n3.2 Taxonomy\nBased on different data layouts and how learned CDF\nmodels are integrated, we classify the existing multi-\ndimensional learned index studies into three categories:\n❶projection-based index, ❷augmentation-based in-\ndex, and ❸grid-based index.\nProjection-based Index adopts a projection function\nto map k-dimensional keys to 1-D values, and then train\na 1-D learned index (e.g., RMI [32] or PGM-Index [18])\nover the set of mapped values. To preserve the spa-\ntial locality, existing studies usually employ space-filling\ncurves like Z-order or Hilbert curves as the choice for\nprojection functions. Such an idea is not new and has\nbeen commonly adopted in conventional spatial index\ndesign (e.g., UB-tree [52] and Hilbert R-tree [28]). The\n\nHow Good Are Multi-dimensional Learned Indices? An Experimental Survey 5\nMulti-dimensional\nDataprojection function1-D CDF\nFig. 2: Workflow of projection-based indices.\nprojection-based indices include SageDB [31], LISA [35],\nZM-Index [62], and ML-Index [13].\nAugmentation-based Index extends the traditional\nindex structures that are based on recursive space par-\ntitioning but augment the ordinary node search with\nmodel-based search. These indices employ similar space\npartition and node split strategies from the existing\nmulti-dimensional indices like R-tree and kd-tree, thus\ninheriting their high generality and wide applications.\nThe augmentation-based indices include IF-Index [24]\nand RSMI [50].\nGrid-based Index employs grids as the data layout.\nDifferent from the ordinary grid indices, the learned\ngrid index does not physically store grid cells; instead,\nlearned CDF functions over a set of selected dimensions\nare trained to locate the correct grid cell. Suppose that\nthej-th dimension in the grid is partitioned into m\nbuckets, a point owill be placed in the ⌊CDF j(oj)·m⌋-\nth bucket. The grid-based indices include Flood [41]\nand Tsunami [15].\nRecent studies like Qd-tree [66] and RLR-tree [22]\nalso leverage machine learning techniques, especially\ndeep reinforcement learning (DRL), to construct multi-\ndimensional data index. These DRL-based index design\nstudies focus on finding a more reasonable data parti-\ntion, whereas their indexing layer is still the traditional\nindex like R-tree. The DRL techniques are used only\nfor finding better data layout, not for locating data\nrecords on the storage. Thus, they are out of the scope\nof learned index and not evaluated in our work.\n3.3 Projection-based Index\nFigure 2 shows the basic workflow of projection-based\nindices. For a d-dimensional dataset O, a projection\nfunction f:Rd→Ris applied to convert Oto a set\nof 1-D values O′. Then, O′is sorted and stored in con-\nsecutive data pages, and a 1-D learned index (e.g., PG-\nMIndex) is built to serve as the approximate mapping\nto storage location (i.e., block ID or index in a dense\nsorted array). Similar to the 1-D learned indices, to en-\nsure the correctness of point search, the projection func-\ntionfshould be also a monotonic mapping, i.e., for two\npoints oando′where o′dominates oon each dimen-\nsion, f(o)≤f(o′). Then, any range search query can\nLearned 1-D\nCDF Modelcluster center\nprojection\ntrainingFig. 3: Illustration of the projection function of ML-\nIndex [13] where the k-means centers are used as refer-\nence points.\nbe transformed to 1-D interval search over the mapped\nvalues. The major difference of existing works in this\nclass is the choice of projection function f.\nZM-Index [62] is the first multi-dimensional learned\nindex where the Z-order curve is chosen as the projec-\ntion function. To efficiently compute the Z-addresses,\nthe data space should be partitioned into grids such\nthat the bit interleaving technique can be used [52]. To\nprocess a range query, the query box is first decomposed\ninto intervals of Z-addresses using the same technique\nin the UB-tree [52], and then the trained CDF model\nis queried to efficiently find the corresponding storage\nlocation.\nML-Index [13] employs an improved iDistance func-\ntion, which is usually used to index high dimensional\ndata for efficient nearest neighbour searches [27], to\nproject the multi-dimensional data. As shown in Fig-\nure 3, given a set of selected reference points (RP)\nα1,···, αm(e.g., obtained using the k-means algorithm),\nthe input data Oare partitioned into mpartitions based\non the distance to each RP. For any point o∈ O, sup-\nposing that αiis the closest RP to o, the ML-Index\nadopts the following projection function,\nf(o) =offset i+dist(o, αi),\noffset i=X\nj<imax\no′∈Ojdist(o′, αj),(1)\nwhere Oj={o|αjis the closest RP to o, o∈ O} .\nCompared with the original iDistance method, i.e.,\niDist (o) =i·C+dist(o, αi) where Cis a large con-\nstant, Eq. (1) eliminates the overlap between different\ndata partitions and reduces the gaps between consec-\nutive partitions, making the learning of a CDF model\non the mapped keys much easier. The query processing\n(range queries or kNN queries) on ML-Index is simi-\nlar to the iDistance [27] method, where a B+-tree is\nconstructed to maintain the iDistance values. Since the\ncomputation of the projection function in Eq. (1) only\nrequires a valid distance metric dist(·,·), the ML-Index\nis also available on data from general metric spaces\n(e.g., strings and graphs). However, as all the other\n\n6 Qiyu Liu et al.\n3 4 5 ... 0 1 2\n... ...\nFig. 4: Illustration of the projection function of\nLISA [35] based on a 3 ×3 grid partition. Note, the\nLebesgue measure in 2-D space is the area of a rectan-\ngular region.\nlearned indices do not support metric space indexing,\nwe focus on the performance of ML-Index in Euclidean\nspace and leave the extension to general metric space\nas an interesting future work.\nLISA [35] majorly attacks that the space-filling curve-\nbased projection usually accesses data blocks that are\nirrelevant to the query rectangle. To solve this issue,\nLISA employs a grid-based projection function. For a\nd-dimensional dataset, data points are first partitioned\nintoT1×T2··· × Tdequal-depth grid cells, and each\ncellCis associated with a unique ID t, i.e., Ct=\n[θ(1)\nl, θ(1)\nh)× ··· × [θ(d)\nl, θ(d)\nh) and t= ((( i1×T2+i2)×\nT2+i3)×··· )×Td+idwhere θ(j)\nl, θ(j)\nhare the grid cell\nboundaries on the j-th dimension and ijis the parti-\ntion ID along the j-th dimension ( j∈1,···, d). Then,\nfor an arbitrary point ofalling into Ct, the projection\nfunction is defined as follows,\nf(o) =t+λ(Ht)\nλ(Ct),\nHt=[θ(1)\nl, o(1))× ··· × [θ(d)\nl, o(d)),(2)\nwhere λ(·) is the Lebesgue measure (i.e., area in 2-D\nspace). The projection function of LISA is illustrated\nin Figure 4 where the areas of red and black dashed\nregions are the Lebesgue measures of C4andH4for\nthe point in purple. Intuitively, Eq. (2) is also similar\nto the iDistance method [27] where grid cells can be\nviewed as reference points. Clearly, points falling into\ncellCtwill be mapped to the same interval [ t, t+ 1),\nwhich preserves the spatial locality.\nAfter the grid construction, all the points are pro-\njected to 1-D space by using Eq. (2) and partitioned\nto shards, and a model called shard prediction function\n(SP), which should be a monotonic function, is trained\nto map each point to their shard ID. Finally, points\nbelonging to the same shard are stored in data pages,\nand a local model (i.e., 1-D learned index) is trained to\nlocate the correct data page.\nNote that, although a grid partition is required, in-\nstead of physically storing a grid index, LISA encodes\n...\nData Page\nRecords ordered by a sort dimensionModel Meta Data\ndimFig. 5: Illustration of IF-Index [24] where dim is the se-\nlected sorting dimension, ∆is the maximum prediction\nerror, and a,bare the slope and interception of the lin-\near model.\nGrid cell Model Data block\nFig. 6: Illustration of RSMI [50] where the partition\nthreshold N′= 5. Note that each data block maintains\na pointer to the next block for efficient scanning.\nall the grid information into a projection function (i.e.,\nEq. (2)) and orders data based on the projected values.\nThus, in Table 1, we classify LISA into the projection-\nbased index, instead of the grid-based index.\n3.4 Augmentation-based Index\nWe then introduce the augmentation-based indices, where\nlearned models are plugged into conventional index struc-\ntures (e.g., R-tree or kd-tree) to accelerate the search\nefficiency.\nIF-Index [13] replaces the leaf node search procedure\nin existing tree structures (e.g., R-tree) with 1-D learned\nindex-based search. The structure of IF-Index based\non the R-tree is illustrated in Figure 5. In this struc-\nture, the non-leaf nodes are constructed the same way\nas an ordinary R-tree. However, in the leaf nodes, not\nonly the corresponding data page is stored, but also the\nmodel metadata. The model metadata contains the di-\nmension dim, which is used to order the points on the\ndata page, as well as a linear interpolation model for\npredicting the location of the search key on the data\npage. In order to determine the best sorting dimen-\nsion, IF-Index evaluates the interpolation cost for each\ndimension on every leaf node, resulting in a time com-\nplexity of O(dN′logN′), where drepresents the num-\n\nHow Good Are Multi-dimensional Learned Indices? An Experimental Survey 7\nDatasetOptimized Grid Layout Query\nLocate intersecting cells\nModel-based Filter\nResult Refinement\nResultsQuery\nWorkload\nDim 1Dim 2\nDim 3\nFig. 7: Illustration of the Flood [41] index framework\non a 3-dimensional dataset, where (1) Dim1 and Dim2\nare used to generate a grid layout, and (2) Dim3 is used\nto sort the data within each grid cell.\nber of dimensions and N′denotes the number of points\ncontained within the leaf node.\nRSMI [50] takes a further step by using the model-\nbased search in both leaf and non-leaf nodes. As shown\nin Figure 6, to construct the RSMI index, spatial points\nare first partitioned into 2⌊log4N′/B⌋×2⌊log4N′/B⌋equal-\ndepth grid cells, where Bis the block size and N′is the\npartition threshold parameter. In practice, N′is typ-\nically assumed to be the number of points for which\na learned function can achieve high accuracy. Then,\ngrid cells are ordered using space-filling curves, and a\nlearned model is trained to map each grid cell to its\nsorting order (e.g., M0,0in Figure 6). As each grid cell\ncan still contain many points (i.e., ≫N′), the above\ndata partitioning and model training procedures are\nrecursively invoked until each partition has at most\nN′points. After completing the partitioning, the data\npoints are packed into data blocks based on the order\nof rank space Z-order curve, and finally, leaf models are\ntrained to predict the corresponding block ID for each\npoint (e.g., M1,0∼M1,3in Figure 6).\nSimilar to a quad-tree, RSMI also employs grids to\nrecursively partition the space, leading to a grid tree\nstructure. The learned models in RSMI are used to\nperform faster searches in each level of the grid tree,\ninstead of directly locating the grid cells. Thus, RSMI\nis classified as the augmentation-based index, instead\nof the grid-based index.\n3.5 Grid-based Index\nThe grid-based indices Flood [41] and Tsunami [15] tar-\nget on learning compact multi-dimensional grids to ef-\nficiently process orthogonal range predicates.\nFlood [41] adopts the common grid index as the data\nlayout. To construct Flood on a d-dimensional dataset,\na sorting dimension is first chosen for ordering the data\nwithin each grid cell, and the remaining d−1 dimen-\nsions are adopted to overlay a grid. Assume, w.l.o.g.,\nRoot\nx < 15 15≤ x <65 x ≥ 65\nCDF(x) CDF(x)\nf: y→x CDF(x|y)historical query range\nR1R2\nR3\nR1 R2 R3grid cell boundaries\nGrid T ree\nAugmented GridFig. 8: Illustration of a Tsunami [15] index built on a\n2-dimensional correlated dataset and tuned by skewed\nquery workloads. The red solid lines refer to the parti-\ntion boundaries of the grid tree, and the blue dashed\nlines refer to the bucket boundaries of augmented grids.\nthat the d-th dimension is the sorting dimension. Dif-\nferent from an ordinary grid index, Flood uses learned\nCDF models (i.e., 1-D learned index) to construct grid\npartitions. Specifically, a Flood grid is typically a multi-\ndimensional array Gof cells, and the corresponding cell\nof an arbitrary point oinGis\nG(o) = (⌊cd f1(o1)·K1⌋,⌊cd f2(o1)·K2⌋,···,\n⌊cd fd−1(od−1)·Kd−1⌋),(3)\nwhere cd fi(·) is the CDF trained on the i-th dimensional\nvalues of the dataset, and Kiis the partition number\nfor the i-th dimension. To enable faster refinement of a\nrange filter, for each bucket, an auxiliary CDF model is\nalso trained using the sorting dimension.\nAs shown in Figure 7, Flood establishes a cost model\nand uses a historical query workload to select the sort-\ning dimension and tune the hyper-parameters (e.g., par-\ntition number Ki). To process a range query, cells in-\ntersecting the query hyper-rectangle are first retrieved\nby Eq. (3), and then their intra-cell CDF models are\nqueried to apply efficient filtering based on the range\npredicates.\nTsunami [15] further improves Flood’s performance on\ncorrelated data and skewed query workloads. As shown\nin Figure 8, Tsunami’s structure consists of two parts:\na grid tree that adapts to skewed query workloads and\nan augmented grid index that is optimized to capture\ndata correlations. Similar to a kd-tree, the grid tree\nis also a space partitioning tree based on a selected\nsubset of dimensions, which divides the whole space into\nseveral disjoint regions such that the query skewness of\na historical workload is reduced within each region (e.g.,\nR1,R2,R3in Figure 8). Then, an augmented grid is\nconstructed for each region.\nDifferent from Flood, where a grid cell is deter-\nmined by independent CDF models on each dimension,\nTsunami captures correlation patterns using both func-\ntional dependency ( f:y→xinR1) and conditional\n\n8 Qiyu Liu et al.\nCDF model ( CDF (x|y) inR2). To determine the gran-\nularity of each augmented grid, historical workloads are\nconsidered such that frequently queried regions are in-\ntensively partitioned (e.g., region R2) while less queried\nregions are mildly partitioned (e.g., region R1andR3).\nThe range query processing on Tsunami is similar to\nthat of Flood, i.e., locating the intersected regions and\ngrids and then refining the results.\nBoth Flood and Tsunami set up a periodical mecha-\nnism to monitor the workload shifts. Once the workload\ndistribution significantly differs from the one used to\nconstruct the index, the whole grid layout and learned\nmodels will be re-tuned using the latest collected work-\nload characteristics.\n3.6 Discussion\nIn this section, we discuss several omitted details re-\nlated to the design choices of multi-dimensional learned\nindices, including ❶index update, ❷model selection,\nand❸model training.\nIndex Update. Most of the multi-dimensional learned\nindices except RSMI [50] and LISA [35] do not support\ndynamic operations like insertion and deletion. The ma-\njor bottleneck comes from the hardness of updating out-\ndated models. An interesting finding is that both RSMI\nand LISA adopt a model-based data layout to handle\ndynamic updates. To insert (or delete) a record, RSMI\nand LISA simply queries the underlying learned models\nto obtain the ID of block to be inserted into and final-\nize the insertion if the block is not full. In this case, the\nblock ID predicted by the model is always regarded to\nbecorrect due to the model-based layout.\nSuch a strategy to handle dynamic operations is\nsimilar to that of existing updatable 1-D learned in-\ndices (e.g., ALEX [14] and LIPP [63]), where new keys\nare inserted to a sorted array with gaps based on the\nmodel prediction. However, existing indices merely con-\nsider the query performance decay problem when the\ndata distribution significantly shifts, and in the worst\ncase, re-constructing the index is inevitable.\nModel Selection. A learned index can be conceptu-\nally regarded as a combination of “data layout + learned\nmodel”. Though playing an important role, in most\ncases, the underlying learned models in existing works\ncan be safely replaced by another one as long as it is\nerror-bounded and monotonic to ensure the correctness\nof query processing results.\nFigure 9 roughly depicts the popular learned model\nchoices used for index design. Clearly, simple models\nlike linear interpolation or piece-wise linear approxi-\nmation (PLA) are efficient to learn and infer with a\nLinear\nInterpolation\nPiece-wise\nLinear Model\nSimple Nonlinear\nModel\nNeural NetworkExpected Error\nModel Complexity Low HighHighFig. 9: Characteristics of different model choices.\nsacrifice of limited model capacity, leading to poten-\ntially higher error. On the other hand, complex mod-\nels like neural networks can achieve potentially better\naccuracy while suffering from extra training and infer-\nence overheads caused by deep learning runtimes. This\nincreases the freedom of index design and enables a se-\nries of trade-offs. For example, a sophisticated model\nis usually costly to learn and store; however, it can ef-\nfectively filter out unnecessary points to be examined\nwhen processing a range query.\nModel Training and Inference. Besides model se-\nlection, effective and efficient model training and in-\nference also play an important role in the learned in-\ndex framework. As shown in Table 1, existing works\nemploy either self-designed algorithms (i.e., algorithmic\napproach) or utilize mature libraries (usually based on\nPython) like Pytorch [49], Tensorflow [60], or numpy [43].\nAlthough these external libraries show much higher flex-\nibility in terms of model design choices, they usually\nrequire extra runtime overheads (e.g., libtorch for Py-\ntorch) and suffer from longer training time. On the\nother hand, well-designed algorithmic approaches are\nusually more efficient for training and inference with\na sacrifice of model design flexibility, e.g., the error-\nbounded piece-wise linear approximation [44] used in\nPGM-Index [18] and the top-down training strategy\nused in RMI [32]. From our evaluation results (Sec-\ntion 5), compared with indices internally using PGM-\nIndex, the Pytorch-based solution is not significantly\nbetter in terms of query processing efficiency, but takes\n∼9000×longer time to train the model.\n4 Experiment Setup\nThis section introduces the index implementation de-\ntails and experiment setups. We implement the whole\nbenchmark in C++ and compile it using GCC 9.5 where\nthe optimization level is set to O3. All experiments are\nperformed on a Ubuntu Linux machine with Intel(R)\n\nHow Good Are Multi-dimensional Learned Indices? An Experimental Survey 9\nTable 2: Summary of the compared indices.\nIndex Type Range kNN Updatable\nZMI learned ✓ ✓ ✕\nMLI learned ✓ ✓ ✕\nRSMI learned ✓ ✓ ✓\nIFI learned ✓ ✕ ✓\nLISA learned ✓ ✓ ✓\nFlood learned ✓ ✕ ✕\nSTRtree tree ✓ ✓ ✓\nR*tree tree ✓ ✓ ✓\nkdtree tree ✓ ✓ ✓\nqdtree tree ✓ ✕ ✓\nANN tree ✕ ✓ ✓\nUG grid ✓ ✕ ✓\nEDG grid ✓ ✕ ✓\nCore(TM) i7-10700K CPU and 32 GB memory. We dis-\nable the CPU’s Turbo Boost feature and lock the CPU\nfrequency to 4.67 GHz in BIOS.\n4.1 Index Implementation Details\nWe first present the implementation details of all the\ncompared multi-dimensional indices (learned and non-\nlearned). Table 2 summarizes all 13 compared methods\nwhere 7 indices are conventional indices and 6 indices\nare learned indices.\n❶ZMI: the ZM-Index [62] that combines the Z-order\ncurve and 1-D learned index. To compute the Z-order\ncurve values, for a dataset of N d-dimensional points,\nwe uniformly partition each dimension into N1/dbuck-\nets, implying an equal-width grid layout of Nbuckets.\nA fine-grained grid can improve the pruning power of\nunnecessary data access but increase the space and time\nefficiency of index construction and query processing.\nWe try different grid partition resolutions and find that\naN1/d× ··· × N1/duniform grid can robustly achieve\nthe best performance on different datasets.\n❷MLI: the ML-Index [13] that combines the improved\niDistance function (Eq. (1)) and 1-D learned index. To\nappropriately set the reference points, Lloyd’s k-means\nalgorithm is invoked with the k-means++ initialization\nstrategy [4]. According to [27], for pivot-based indices,\nthe query efficiency improvement is minor when the\nnumber of reference points (i.e., P) is larger than 25.\nIn our implementation, the partition number Pis set\nto 20 for datasets of size <20M and 40 for datasets of\nsize>20M.\n❸LISA : the LISA index [35] that employs the grid-\nbased projection function (Eq. (2)). The original imple-\nmentation of LISA is in Python and highly depends on\nNumpy. As all the learned indices are based on C++,\nwe re-implement and optimize the LISA index struc-\nture at our best. To compute Eq. (2), we construct anequal-depth grid where each grid cell contains roughly\nB= 2000 points. By such setting, the range of Eq. (2)\nis in [0 , N/B + 1].\n❹IFI: the IF-Index [24] (a.k.a., Hands-off) based on R-\ntree. The original IF-Index employs linear interpolation\nto estimate the key location, whose prediction error is\nlarge, especially for non-uniform data. To reduce the\nerror and thus improve the query efficiency, we train\nlinear models using the least square method, yielding\na 2%–15% performance improvement with a sacrifice\nof 10%–20% more training time. Besides, through our\npractice, we find that it is unnecessary to select different\nsorting dimensions for each leaf node, as we do not know\nthe distribution of query rectangles when building the\nindex. The capacities of leaf and non-leaf nodes are set\nto 1000 and 64, which can achieve the best trade-off be-\ntween index size and performance on different datasets.\n❺RSMI : the recursive spatial model index [50]. We se-\nlect their original implementation and follow the same\nindex tuning strategies as discussed in their paper (e.g.,\nselection of the partition threshold parameter N′). Note\nthat, RSMI only supports 2-D datasets as it mainly tar-\ngets spatial applications. The model training and infer-\nence of RSMI is based on Pytorch [49]. Thus, to make\nit fair when comparing with other indices, we choose the\nCPU-only version of Pytorch but enable multi-threading\nto accelerate the model training (otherwise it fails to\nterminate in 5 hours for a 20M dataset).\n❻Flood : the learned grid index [41]. As Flood is not\nopen-sourced at the current stage, we implement Flood\nat our best. The original Flood index requires a query\nworkload to tune its hyper-parameters (e.g., selection of\nthe sorting dimension). However, since other indices are\nnot optimized using query histories, we do not imple-\nment this part to make the comparison fair. Similarly,\nTsunami [15] is a fully workload-driven index and thus\nis not compared in this benchmark due to the same\nreason. To avoid the exponentially growing number of\ngrid cells w.r.t. data dimension, the partition number\non each dimension except the sorting dimension is set\nto (N/B )1/(d−1)where B= 2000 is the (rough) number\nof points in each cell.\nExcept RSMI and IFIthat employ deep learning\nand linear regression model, we choose the PGMIn-\ndex [18] as the default underlying 1-D learned index\nof the multi-dimensional learned indices (i.e., ZMI,MLI,\nLISA , and Flood ) to unify their implementations. Note\nthat, the default error parameter ϵfor PGMIndex is\nset to 64, which is an empirically robust and optimized\nvalue across different datasets and configurations (see\nSection 5.6 for more details).\nHowever, according to the results of a recent bench-\nmark on 1-D learned indices [38], a well-optimized re-\n\n10 Qiyu Liu et al.\ncursive model index (RMI) can slightly outperform the\nPGMIndex. The reasons that we do not choose RMI [32]\nas the underlying 1-D learned index in our implemen-\ntation are threefold.\n–The state-of-the-art RMI implementation [38] is in-\ntrinsically an “index compiler” that takes a dataset\nas input and generates static index data and header\nfiles separately, which is less flexible to be embedded\ninto other indices.\n–The parameter tuning of PGMIndex is much easier\nthan RMI as only one error parameter ϵis required\nfor PGMIndex. On the other hand, RMI requires tun-\ning of the error parameters, number of models, and\nmodel types for each level of the index.\n–In this experimental study, we focus more on eval-\nuating the performance w.r.t. different index design\nchoices (e.g., data layout and projection function)\nunder a unified environment, instead of a replaceable\ncomponent that has a minor influence on the relative\nperformance.\nBesides learned indices, we also implement and com-\npare 8 non-learned baselines that are commonly used in\npractice.\n❶FullScan : the na¨ ıve sequential scan algorithm. To\nspeed up range query processing, we sort the points\nbased on Z-order values to keep spatial locality and\nallow the early pruning of unnecessary points.\n❷R*tree : the R*-tree index [5] that optimizes the R-\ntree structure by minimizing the leaf node overlap.\n❸STRTree : the R-tree index with the sort-tile-recursive\n(STR) bulk-loading strategy [34], which is a simple yet\neffective R-tree variant. As the indexed geometries are\nmulti-dimensional points, there is no overlap for the\nMinimum Bounding Regions (MBRs) of leaf nodes.\nNote that, for both R*tree andSTRTree , we choose\nthe implementations from the Boost geometry library [8],\nand the node capacity (i.e., fanout) is set to 128 based\non a benchmark on popular spatial libraries [47].\n❹kdtree : the kd-tree index [6] that recursively parti-\ntion the space by the median of each dimension in a\nround-robin fashion. We adopt the implementation in\nthe nanoflann project [40].\n❺ANN: another kd-tree variant provided in ANN[2], a\nlibrary for efficient exact and approximate kNN search.\nFor both ANNand kdtree , the leaf node size (i.e., the\nthreshold of a kd-tree turns to use brute-force search)\nis set to 16.\n❻qdtree : the quad-tree index that can be viewed as an\nadaptive grid index. The implementation from GEOS\nlibrary [20] is adopted. Note that qdtree only supports\nrange queries.Table 3: Summary of datasets.\nDataset Size #Points #Dim. Skewness\nUniform N.A. 5–100M 2–8 low\nNormal N.A. 5–100M 2–8 mid\nLognormal N.A. 5–100M 2–8 high\nFourSquare 181 MB 3.7M 2 mid\nToronto3d 1.52 GB 21M 3 high\nOSM 3.1 GB 63M 2 mid\n❼UG: a uniform grid index (a.k.a., equal-width grid in-\ndex). The partition number of each dimension is set to\n(N/2000)1/d.\n❽EDG: an equal-depth grid index where each dimen-\nsion is sorted and partitioned into ( N/2000)1/dparts\nof equal size. EDGcan be regarded as the non-learned\nversion of the Flood index. Compared with the uniform\ngrid, EDGcan better deal with skewed data distribution\nwith a sacrifice of higher construction cost.\n4.2 Datasets and Query Generation\nWe evaluate all the compared methods in Section 4.1\non both real and synthetic datasets, spanning a wide\nspectrum of data size, dimensionality, and distribution\nskewness. Table 3 summarizes the dataset statistics.\nReal Datasets. We adopt three commonly used real\ndatasets in our evaluation.\n–FourSquare is a location dataset extracted from a\ngeo-social network. Each data point represents the\ncoordinates of a co-location event of two users in the\nsocial network [19].\n–Toronto3d is a public dataset of LiDAR images of\nurban roadways [61]. We extract the spatial coordi-\nnates (i.e., X, Y, Z) from the raw data to form a 3-D\npoint cloud.\n–OSMis a recent dump of OpenStreatMap [45]. We only\nuse the point objects (i.e., pair of latitude and lon-\ngitude), and other geometries like polygons and line-\nstrings are excluded as querying these complicated\ngeometries is not commonly supported by existing\nlearned index studies.\nSynthetic Datasets. To evaluate the scalability w.r.t.\ndifferent data sizes and dimensions, we also sample ran-\ndom points from uniform, normal, and log-normal dis-\ntributions where the dimension correlation is ignored.\nWe vary the scale factors of each distribution (e.g., the\nstandard deviation of normal distribution) to make sure\nthat the density of generated data is similar. For all syn-\nthetic datasets, the default #Points and #Dim. are set\nto 20 million and 2, respectively3.\n3#Dim. is set to 2 by default as some compared methods\nlikeRSMI only support 2-D datasets.\n\nHow Good Are Multi-dimensional Learned Indices? An Experimental Survey 11\nTable 4: Index construction time (seconds) on real datasets and synthetic datasets of default configurations. STR\nis selected as the baseline index to compute the relative ratio. Note that, RSMI is not available on Toronto3d and\nfails to terminate construction in 5 hours for dataset OSM.\nIndexDataUniform Normal Lognormal Foursquare Toronto3d OSM\nZMI 1.89 (▼1.76×) 1.63 (▼1.99×) 1.13 (▼2.85×) 0.17 (▼2.76×) 1.09 (▼3.0×) 3.08 (▼2.25×)\nMLI 93.54 ( ▲28.1×) 90.49 ( ▲27.8×) 83.99 ( ▲26.1×) 14.48 ( ▲30.8×) 83.13 ( ▲25.4×)209.22 ( ▲30.2×)\nIFI 4.88 (▲1.47×) 5.04 (▲1.55×) 5.08 (▲1.58×) 0.71 (▲1.51×) 4.61 (▲1.41×) 10.15 ( ▲1.46×)\nRSMI 8978 (▲2696×)13017 ( ▲4005×)10311 ( ▲3202×)996.8 ( ▲2121×) N.A. N.A.\nLISA 7.46 (▲2.24×) 7.45 (▲2.29×) 7.48 (▲2.32×) 1.25 (▲2.66×) 7.55 (▲2.31×) 15.62 ( ▲2.25×)\nFlood 5.34 (▲1.60×) 5.44 (▲1.67×) 5.57 (▲1.73×) 0.99 (▲2.11×) 9.80 (▲3.00×) 18.05 ( ▲2.60×)\nSTR 3.33 (1 .0×) 3.25 (1 .0×) 3.22 (1 .0×) 0.47 (1 .0×) 3.27 (1 .0×) 6.93 (1 .0×)\nR*tree 128.92 ( ▲38.7×)128.80 ( ▲39.6×)120.46 ( ▲37.4×)18.22 ( ▲38.8×)155.34 ( ▲47.5×)389.35 ( ▲56.2×)\nkdtree 28.61 ( ▲8.59×) 27.96 ( ▲8.60×) 29.03 ( ▲9.02×) 3.87 (▲8.23×) 14.66 ( ▲4.48×) 30.53 ( ▲4.41×)\nqdtree 2.15 (▼1.55×) 2.46 (▼1.32×) 2.77 (▼1.16×) 0.619 ( ▲1.32×)12.587 ( ▲3.85×)21.20 ( ▲3.06×)\nANN 21.61 ( ▲6.49×) 20.43 ( ▲6.29×) 20.75 ( ▲6.44×) 3.21 (▲6.83×) 2.99 (▼1.09×) 22.33 ( ▲3.22×)\nUG 0.73 (▼4.56×) 0.51 (▼6.37×) 0.34 (▼9.47×) 0.07 (▼6.71×) 0.48 (▼6.81×) 1.24 (▼5.59×)\nEDG 4.17 (▲1.25×) 4.16 (▲1.28×) 4.15 (▲1.29×) 0.63 (▲1.34×) 4.53 (▲1.39×) 8.44 (▲1.22×)\n0.0 0.1 0.2 0.3 0.40.02.55.07.510.012.515.017.520.0 FourSquare\n0.0 0.2 0.4 0.6 0.802468101214OSM\n0.1 0.2 0.3 0.401234567Toronto3d\n0.00 0.05 0.10 0.15 0.200.02.55.07.510.012.515.017.520.0Uniform\n0.0 0.2 0.4 0.6 0.805101520Gaussian\n0.0 0.2 0.4 0.6 0.802468101214Lognormal\nFig. 10: Selectivity distribution of generated range\nqueries. The x-axis and y-axis refer to the query se-\nlectivity and number of queries, respectively.\nRange Query Generation. To generate range queries\nfor evaluation, we first randomly sample S= 100 points\nfrom a given dataset as the lower corners of query rect-\nangles; then, for each dimension i= 1,···, d, a random\nwidth δiis added to the lower corner to generate the\nupper corner. For a generated query box R, the selectiv-\nity of Ris the percentage of points that Ris expected\nto cover, i.e.,\nSel(R) =#(Points covered by R)\n#(Points). (4)\nFigure 10 presents the selectivity distributions of gen-\nerated range queries on different datasets. The default\nselectivity in the subsequent evaluations is 1%.\nkNN Query Generation. AkNN query is a pair of\nquery point qand a result size k. To generate kNNqueries, for each k∈{1, 10, 100, 1000, 10000 }, we ran-\ndomly pick S= 20 points from each dataset to generate\na set of kNN queries. The default value of kin the eval-\nuation is set to 1000.\nIn the subsequent evaluations, we report the average\nquery processing time of different selectivities for range\nqueries and different kforkNN queries. Note that, we\ndo not iterate each query many times, which will warm\nthe CPU cache and thus possibly reduce the query pro-\ncessing time. Such a setting can well simulate the index\nusage scenarios in real-world applications.\n5 Experiment Results\nIn this section, we report the evaluation results based\non the above configurations to answer the core ques-\ntion: How good are multi-dimensional learned indices?\nIn particular, we are interested in the following 8 sub-\nquestions.\nQ1: Construction Time : whether the construction\ntime of learned indices is as fast as the non-learned\nbaselines ( ▷Section 5.1);\nQ2: Space Cost : whether the space cost of learned\nindices is significantly lower than the non-learned\nbaselines ( ▷Section 5.2);\nQ3: Range Query Efficiency : whether the learned\nindices can outperform baselines in terms of range\nquery processing ( ▷Section 5.3);\nQ4: kNN Query Efficiency : whether the learned in-\ndices can outperform baselines in terms of kNN\nquery processing ( ▷Section 5.4);\nQ5: Scalability : whether the learned indices can well\nscale to datasets of larger sizes and higher dimen-\nsions ( ▷Section 5.5);\n\n12 Qiyu Liu et al.\nTable 5: Index memory overhead (MB) on real datasets and synthetic datasets of default configurations. STR\nis chosen as the baseline to compute the relative comparison ratio. Note that, we exclude the memory cost for\nstoring pointers to each data point from the overall index size where each pointer occupies 8 bytes on most modern\narchitectures.\nIndexDataUniform Normal Lognormal Foursquare Toronto3d OSM\nZMI 0.02 (▼7374×) 0.05 (▼3120×) 0.03 (▼5070×) 0.03 (▼907×) 0.28 (▼1244×) 1.94 (▼262×)\nMLI 0.04 (▼3863×) 0.04 (▼3863×) 0.04 (▼3863×) 0.05 (▼564×) 0.08 (▼4518×) 0.29 (▼1749×)\nIFI 1.12 (▼145×) 1.12 (▼145×) 1.12 (▼145×) 0.21 (▼140×) 1.39 (▼248×) 3.50 (▼145×)\nRSMI 21.5 (▼7.55×) 22.4 (▼7.24×) 29.8 (▼5.44×) 21.7 (▼1.38×) N.A. N.A.\nLISA 0.53 (▼305×) 0.53 (▼305×) 0.53 (▼305×) 0.09 (▼321×) 0.30 (▼1160×) 1.97 (▼258×)\nFlood 0.33 (▼489×) 0.37 (▼436×) 0.37 (▼436×) 0.28 (▼106×) 2.81 (▼122×) 4.62 (▼110×)\nSTR 162.2 (1 .0×) 162.2 (1 .0×) 162.2 (1 .0×) 29.9 (1 .0×) 343.4 (1 .0×) 508.9 (1 .0×)\nR*tree 295.5 ( ▲1.82×) 292.5 ( ▲1.80×) 287.5 ( ▲1.77×) 56.0 (▲1.88×)581.6 ( ▲1.69×)1021.9 ( ▲2.01×)\nkdtree 120.5 ( ▼1.35×) 118.6 ( ▼1.37×) 116.1 ( ▼1.40×) 16.7 (▼1.79×)117.0 ( ▼2.94×)200.0 ( ▼2.54×)\nqdtree 1516.1 ( ▲9.35×)1542.8 ( ▲9.51×)1570.4 ( ▲9.68×)266.5 ( ▲8.93×)121.0 ( ▼2.84×)4583.0 ( ▲9.01×)\nANN 123.8 ( ▼1.31×) 122.2 ( ▼1.33×) 118.9 ( ▼1.36×) 16.1 (▼1.85×)126.7 ( ▼2.71×)204.7 ( ▼2.49×)\nUG 0.22 (▼731×) 0.22 (▼731×) 0.22 (▼731×) 0.08 (▼360×) 0.18 (▼1951×) 1.43 (▼356×)\nEDG 0.22 (▼731×) 0.22 (▼731×) 0.22 (▼731×) 0.08 (▼360×) 0.18 (▼1951×) 1.43 (▼356×)\nQ6: Parameter Setting: how to properly configure\nthe learned index hyper-parameters (e.g., the error\nparameter ϵ) (▷Section 5.6);\nQ7: Dynamic Update Efficiency: whether existing\nlearned indices can support efficient dynamic up-\ndates ( ▷Section 5.7);\nQ8: Explanation of Performance: why learned in-\ndices are effective (or ineffective) ( ▷Section 5.8).\n5.1 Index Construction Time (Q1)\nWe first report the index construction time of the de-\nfault configuration (#Points=20M and #Dim=2) in\nTable 4. We set a threshold of 5 hours, and if the\nprogram cannot finish in 5 hours, it will be killed and\nN.A. will be reported.\nAs shown in Table 4, the projection-based indices\nexcept MLI(i.e., ZMIandLISA ) are comparable with or\neven faster than the non-learned baselines in terms of\nindex construction time. This is because we unify the\nunderlying 1-D learned index of these indices as the\nPGMIndex, which requires only one pass of data (i.e.,\nO(N) time) to obtain the minimized number of error-\nbounded line segments [18,38]. MLIis slower because\nfinding reference points via k-means is costly, which oc-\ncupies >95% of the total index building time. Surpris-\ningly, ZMIis∼2×faster than the bulk-loading R-tree\nindex STRand is only slower than the simple uniform\ngrid index UG. This is because, in addition to adopting\nthe PGMIndex, ZMIalso benefits from the bit manipu-\nlation instruction set provided by modern CPUs, which\ncan significantly accelerate the computation of Z-order\ncurve values [70].Indices that adopt the grid layouts, i.e., LISA and\nFlood , take slightly longer construction time due to the\ngeneration of internal equal-depth grid cells. The con-\nstruction time of IFIis 1.5×ofSTRbut much faster\nthan R*tree (about 30 ×faster). This is because, in\nour implementation of IFI, we adopt a similar bulk-\ning loading strategy to find the leaf nodes. Besides, the\nleaf node capacity of IFIis generally larger (e.g., 1000)\nthan ordinary R-tree indices (e.g., 128), which also alle-\nviates the extra overhead caused by fitting linear mod-\nels. Compared with all the other learned indices, RSMI\ntakes a much longer time to build (104×slower than\nZMI) and even fails to terminate on the largest dataset\nOSM. This is because the deep learning models adopted\nbyRSMI are much more complex than the other in-\ndices and the CPU-only training is generally inefficient\ncompared with using GPUs. Though accelerating query\nprocessing using novel hardware like GPU and TPU is\ngetting popular recently [53], in this paper, we focus on\nthe general application scenarios of multi-dimensional\nindices where a powerful GPU is usually not available,\nthus the CPU-based RSMI is evaluated.\nTakeaways. The construction time highly depends on\nthe internal model choices of multi-dimensional learned\nindices. Indices based on PGMIndex are generally as\nfast as efficient non-learned baselines; on the other hand,\nindices employing deep learning techniques are much\nmore slower.\n5.2 Index Memory Cost (Q2)\nIn this section, we report the memory cost of each com-\npared method. Note that, for indices RSMI ,kdtree ,\n\nHow Good Are Multi-dimensional Learned Indices? An Experimental Survey 13\n0.1% 1% 4% 10% 20%\nSelectivity102103104105Avg. Time (us)kdtree\nSTRR*tree\nUGEDG\nFSFlood\nIFILisa\nMLIRSMI\nZMIoctree\n(a) Uniform: Time v.s. Selectivity\n0.7% 4% 10% 35% 73%\nSelectivity103104105106Avg. Time (us)kdtree\nSTRR*tree\nUGEDG\nFSFlood\nIFILisa\nMLIRSMI\nZMIoctree (b) Gaussian: Time v.s. Selectivity\n0.9% 12% 29% 48% 74%\nSelectivity103104105106Avg. Time (us)kdtree\nSTRR*tree\nUGEDG\nFSFlood\nIFILisa\nMLIRSMI\nZMIoctree\n(c) Lognormal: Time v.s. Selectivity\n0.4% 1.7% 4% 10% 26%\nSelectivity102103104Avg. Time (us)kdtree\nSTRR*tree\nUGEDG\nFSFlood\nIFILisa\nMLIRSMI\nZMIoctree (d) FourSquare: Time v.s. Selectivity\n4.8% 10% 20% 30% 38%\nSelectivity101102103104105106Avg. Time (us)kdtree\nSTRR*tree\nUGEDG\nFSFlood\nIFILisa\nMLIRSMI\nZMIoctree\n(e) Toronto3d: Time v.s. Selectivity\n0.5% 5% 15% 31% 62%\nSelectivity101102103104105106Avg. Time (us)kdtree\nSTRR*tree\nUGEDG\nFSFlood\nIFILisa\nMLIRSMI\nZMIoctree (f) OSM: Time v.s. Selectivity\nFig. 11: Range query evaluation results on real datasets and synthetic datasets of the default configuration\n(#Points=20M and #Dim=2) where the y-axis and x-axis refer to the average query processing time and av-\nerage selectivity of test queries, respectively. Note that, RSMI is not available on Toronto3d and fails to terminate\ntraining on OSM.\nqdtree , and ANN, we cannot programmatically retrieve\ntheir memory cost at runtime. Thus, we perform heap\nprofiling for these indices via gperftools [21] starting\nbefore building the index and ending after the index is\nconstructed4.\nTable 5 shows the memory cost evaluation results.\nAs we fix the error threshold ϵof the underlying PG-\nMIndex to 64, the resulting learned multi-dimensional\nindices except RSMI share a similar level of memory\ncost. And all the learned indices achieve significant im-\nprovement of memory overhead compared with popular\n4Such a method for measuring memory cost at runtime\nis also adopted in a recent benchmark on modern spatial li-\nbraries [47].non-learned indices (e.g., on Toronto3d dataset, LISA\nis∼1000×smaller than STR,∼2000×smaller than\nR*tree , and∼400×smaller than kdtree ).\nNote that, grid indices like UGand EDGalso have\nlow space cost (sometimes even better than learned in-\ndices). This is because these grid indices only require to\nstore the partition boundaries on each dimension. How-\never, the grid indices generally perform badly on query\nprocessing, especially for skewed and high-dimensional\ndatasets, which will be discussed in Section 5.3.\nAs a learned index augmented on top of R-tree, IFI\ncan also achieve remarkable memory cost as its leaf\nnode capacity is much larger (i.e., 1000) than an or-\ndinary R-tree (i.e., 64), thus requiring much fewer tree\n\n14 Qiyu Liu et al.\nnodes to store. As for RSMI , though its index construc-\ntion time is significantly higher than that of other in-\ndices (9124 ×larger than ZMIon dataset Lognormal ,\nsee Table 4), the trained model is generally more com-\npact for storage and not sensitive to the size of various\ndatasets (about 20MB for all datasets). Thus, an ideal\nuse-case of RSMI is to perform offline training using ma-\nchines equipped with powerful GPUs and then deploy\nthe trained models for subsequent query processing.\nTakeaways. All evaluated learned indices can obtain\na much more compact structure (up to 7374 ×reduc-\ntion) than the commonly used non-learned indices with\na sacrifice of affordable index construction cost (except\nRSMI , which has a much higher construction overhead\ncaused by training deep models).\n5.3 Range Query Processing (Q3)\nWe then report the range query evaluation results using\nthe queries generated in Section 4.2. Figure 11 shows\nthe average query processing time w.r.t. different levels\nof query selectivities.\nAmong non-learned baselines, STRand EDGare the\nfastest indices on range query processing. Compared\nwith these two methods, learned indices IFI,Flood ,\nand LISA can always achieve better performance re-\ngardless of data skewness and query selectivity. Not\nsurprisingly, learned indices perform much better for\nqueries of lower selectivity. For example, on dataset\nLognormal ,Flood is 2.19×faster than STRwhen query\nselectivity is less than 1%; on the other hand, the speed-\nup ratio decreases to 1 .31 when the selectivity ratio\nincreases to 48%. This is because, the pruning power\nof learned models becomes more significant for small\nquery ranges, and in the worst case, all the indices\n(learned and non-learned) are no better than a linear\nscan.\nRSMI does not show satisfactory performance, espe-\ncially on non-uniform data. This is because RSMI aims\nto design a disk-based index structure where page ac-\ncess is the major optimized objective. Besides, RSMI also\nsupports approximate range query processing, which is\nabout an order of magnitude faster than the exact query\nprocessing when query selectivity is lower than 5%. The\nperformance of MLIis also not satisfactory as its metric-\nbased projection function (Eq. (1)) cannot well encode\nlocality information and thus is not suitable for orthog-\nonal range query processing.\nAn interesting point is that, as the first learned\nmulti-dimensional index, ZMIis usually regarded as a\nweak baseline in previous studies [50,35,41]. However,\nfrom our evaluation, the performance of ZMIis not con-\nsiderably worse and is comparable with STR(recallingthat ZMIhas very low construction time and memory\ncost). The major reason is that the original implementa-\ntion of ZMI[62] adopts a coarse pruning strategy where\nthe whole range of [Z(min corner), Z(max corner)] should\nbe searched. In this work, we use the BigMin algo-\nrithm [52] to divide a query box into fine-grained candi-\ndate Z-value ranges and perform a learned index-based\nsearch over the points sorted by Z-values, leading to a\nsignificant performance improvement.\nTakeaways. Learned indices IFI,Flood , and LISA can\nrobustly outperform the non-learned baselines across\nall levels of query selectivity. The speedup ratio is es-\npecially significant for uniformly distributed datasets\nandlow-selectivity queries. On the other hand, other\nindices like ZMI,MLI, and RSMI cannot systematically\noutperform non-learned indices.\n5.4 kNN Query Processing (Q4)\nWe further evaluate the kNN query processing efficiency.\nNote that, learned indices IFIandFlood focus on range\nquery processing and do not support kNN queries, thus\nare excluded from this set of experiments. Figure 12 re-\nports the average kNN query processing time w.r.t. dif-\nferent k∈ {1,10,100,1000,10000}.\nFor non-learned indices, two kd-tree variants kdtree\nandANNperform the best for k≤100; and R-tree vari-\nants STRand R*tree are more efficient for k > 100.\nDifferent from range queries where learned indices can\ngenerally outperform non-learned baselines, only MLI,\nLISA , and ZMcan outperform the highly optimized R-\ntree and kd-tree libraries on some datasets and some k.\nFor example, on synthetic datasets, MLIis 1.1∼1.3×\nfaster than STRwhen k≤10; however, MLIcan be up\nto 3.6×slower compared with STRwhen kincreases to\n10000. And unfortunately, no multi-dimensional learned\nindex can beat the best non-learned baselines on all\nthree real-world datasets.\nThe intrinsic reason is that, most of the existing\nlearned indices process kNN queries by repeatedly in-\nvoking range queries of progressively increased search\nradius until the nearest- kresults are found. Thus, the\nkNN processing time highly depends on the proper set-\nting of the initial search range. In the worst case, such\na progressive search method requires invoking range\nqueries from a very small range to the whole data space,\nwhere a large portion of range queries are unnecessary\nto retrieve kNN results. Besides, different from R-tree\norkd-tree variants that are based on recursive space\npartitioning, it is generally intractable to inject local\naggregation information like range count values into the\nlearned index structures, also making it hard to perform\nefficient candidate pruning.\n\nHow Good Are Multi-dimensional Learned Indices? An Experimental Survey 15\n1 10 100 1000 10000\nk100101102103104105Avg. Time (us)kdtree\nSTRR*tree\nFSLisa\nMLIRSMI\nZMoctree\nANN\n(a) Uniform: Time v.s. k\n1 10 100 1000 10000\nk100101102103104105Avg. Time (us)kdtree\nSTRR*tree\nFSLisa\nMLIRSMI\nZMoctree\nANN (b) Gaussian: Time v.s. k\n1 10 100 1000 10000\nk101102103104105Avg. Time (us)kdtree\nSTRR*tree\nFSLisa\nMLIRSMI\nZMoctree\nANN\n(c) Lognormal: Time v.s. k\n1 10 100 1000 10000\nk100101102103104105Avg. Time (us)kdtree\nSTRR*tree\nFSLisa\nMLIRSMI\nZMoctree\nANN (d) FourSquare: Time v.s. k\n1 10 100 1000 10000\nk100101102103104105Avg. Time (us)kdtree\nSTRR*tree\nFSLisa\nMLIRSMI\nZMoctree\nANN\n(e) Toronto3d: Time v.s. k\n1 10 100 1000 10000\nk100101102103104105Avg. Time (us)kdtree\nSTRR*tree\nFSLisa\nMLIRSMI\nZMoctree\nANN (f) OSM: Time v.s. k\nFig. 12: kNN query evaluation results on default settings. The y-axis refers to the average query processing time\nfor each k. Note that IFIand Flood do not support kNN queries, and RSMI is not available on Toronto3d and\nfails to terminate training on OSM.\nTakeaways. Different from range queries, the nature of\nprogressive range search prevents efficient kNN query\nprocessing for existing multi-dimensional learned in-\ndices. The kNN processing efficiency for learned indices\ncannot systematically outperform non-learned base-\nlines, especially on datasets of high dimensions.5.5 Scalability Evaluation (Q5)\nThis section studies the index performance (including\nindex construction, range query processing, and kNN\nquery processing) when scaling the size and dimension\nof datasets.\n\n16 Qiyu Liu et al.\n2 4 6 8\nData Dimension103104105Time (ms)kdtree\nSTRR*tree\nUGEDG\nFloodIFI\nLisaMLI\nZMIoctree\nANN\n(a) Uniform: Construction Time v.s. d\n2 4 6 8\nData Dimension102103104105106Avg. Time (us)kdtree\nSTRR*tree\nUGEDG\nFSFlood\nIFILisa\nMLIZMI\noctree (b) Uniform: Range Query Time v.s. d\n2 4 6 8\nData Dimension102103104105106Avg. Time (us)kdtree\nSTRR*tree\nFSLisa\nMLIZMI\noctreeANN\n(c) Uniform: kNN Query Time v.s. d\n2 4 6 8\nData Dimension103104105Time (ms)kdtree\nSTRR*tree\nUGEDG\nFloodIFI\nLisaMLI\nZMIoctree\nANN (d) Gaussian: Construction Time v.s. d\n2 4 6 8\nData Dimension103104105106Avg. Time (us)kdtree\nSTRR*tree\nUGEDG\nFSFlood\nIFILisa\nMLIZMI\noctree\n(e) Gaussian: Range Query Time v.s. d\n2 4 6 8\nData Dimension102103104105106Avg. Time (us)kdtree\nSTRR*tree\nFSLisa\nMLIZMI\noctreeANN (f) Gaussian: kNN Query Time v.s. d\nFig. 13: Scalability evaluation w.r.t. dimension d. The range query selectivity is in [10−5,10−2], and k= 1000 for\nkNN queries.\nFigure 13 shows the scalability evaluation results by\nvarying data dimension d∈ {2,4,6,8}for two synthetic\ndatasets. The construction time for all compared indices\ngrows as dincreases. For all grid-based learned indices\n(i.e., Flood and LISA ), we set the partition number of\neach dimension to ( N/B )1/d, which avoids the expo-\nnentially growing (w.r.t. d) of the grid size and makeseach grid cell contains roughly Bpoints. Note that,\nZMIis insensitive to the increase of das increasing d\nslightly slows down the computation of Z-curve values\nwhich occupies a relatively small portion of the whole\nindex construction. For query processing efficiency, the\nresults are similar to those discussed in Section 5.3\nand Section 5.4. Specifically, Flood and LISA can ro-\n\nHow Good Are Multi-dimensional Learned Indices? An Experimental Survey 17\n1M 10M 50M 100M\nData Size102103104105106Time (ms)kdtree\nSTR\nR*treeUG\nEDGFlood\nIFILisa\nMLIRSMI\nZMIoctree\nANN\n(a) Uniform: Construction time v.s. N\n1M 10M 50M 100M\nData Size102103104105106Avg. Time (us)kdtree\nSTR\nR*treeUG\nEDGFS\nFloodIFI\nLisaMLI\nRSMIZMI\noctree (b) Uniform: Range Query time v.s. N\n1M 10M 50M 100M\nData Size102103104105106Avg. Time (us)kdtree\nSTRR*tree\nFSLisa\nMLIRSMI\nZMIoctree\nANN\n(c) Uniform: kNN Query time v.s. N\n1M 10M 50M 100M\nData Size101102103104105106Time (ms)kdtree\nSTR\nR*treeUG\nEDGFlood\nIFILisa\nMLIRSMI\nZMIoctree\nANN (d) Normal: Construction time v.s. N\n1M 10M 50M 100M\nData Size102103104105Avg. Time (us)kdtree\nSTR\nR*treeUG\nEDGFS\nFloodIFI\nLisaMLI\nRSMIZMI\noctree\n(e) Normal: Range Query time v.s. N\n1M 10M 50M 100M\nData Size102103104105106Avg. Time (us)kdtree\nSTRR*tree\nFSLisa\nMLIRSMI\nZMIoctree\nANN (f) Normal: kNN Query time v.s. N\nFig. 14: Scalability evaluation w.r.t. data size N. The range query selectivity is in [10−4,10−2], and k= 1000 for\nkNN queries.\nbustly achieve better range query performance when d\nscales. As for kNN query processing, MLIis the best\nlearned index on low dimensional data ( d≤3) as MLI\ncan be viewed as a learned version of “iDistance + B-\ntree”, which encodes the distance information and thus\nis more suitable for kNN query processing. However,\nthe performance of MLIis worse than that of FloodandLISA when dincreases. This is because the projec-\ntion function based on Euclidean distance (i.e., Eq. (1))\ncannot well distinguish the differences of two points to a\nquery point when dis large [1]. Besides, the cost of com-\nputing Eq. (1) also increases as dincreases, becoming a\nnon-neglectable part of the total kNN query processing\noverhead.\n\n18 Qiyu Liu et al.\n4 16 64 256 1024\nEps103104Time (sec)\nFlood\nLisaMLI\nZMI\n(a) Foursquare: construction time v.s. ϵ\n4 16 64 256 1024\nEps103Avg. Time (us)\nFlood\nLisaMLI\nZMI (b) Foursquare: range query v.s. ϵ\n4 16 64 256 1024\nEps104105106Memory (byte)\nFlood\nLisaMLI\nZMI (c) Foursquare: memory cost v.s. ϵ\n4 16 64 256 1024\nEps103104105Time (sec)\nFlood\nLisaMLI\nZMI\n(d) Toronto3d: construction time v.s. ϵ\n4 16 64 256 1024\nEps1042×1043×1044×1046×104Avg. Time (us)\nFlood\nLisaMLI\nZMI (e) Toronto3d: range query v.s. ϵ\n4 16 64 256 1024\nEps104105106107Memory (byte)\nFlood\nLisaMLI\nZMI (f) Toronto3d: memory cost v.s. ϵ\nFig. 15: Evaluation results w.r.t. error parameter ϵ(for learned indices that are based on PGM-Index) on two real\ndatasets Foursquare andToronto3d .\nFigure 14 also shows the scalability evaluation re-\nsults by varying the size of synthetic datasets from 1M\nto 100M. The index construction time for learned in-\ndices is basically proportional to the increase of data\nsizeNconsidering that all the learned indices require\nsorting the data based on some attributes and then\ntraining models on the sorted data. RSMI fails to ter-\nminate training for data sizes larger than 50M (i.e., ex-\nceeding 5 hours). For range queries of a fixed selectivity,\nthe processing time of all indices increases as there are\nmore points that need to be reported; however, the rel-\native ratio between learned and non-learned indices is\nsimilar to that of the default setting (Figure 11).\nTakeaways. Existing multi-dimensional learned indices\ncan generally scale to large-scale datasets (up to 100M)\nand a medium level of dimensions (up to 8). Specifically,\nindices Flood and LISA can robustly outperform con-\nventional indices in range query processing time. Index\nRSMI fails to scale to datasets of size up to 50M as the\nmodel training cannot be terminated within 5 hours.\n5.6 Effects of Error Parameter (Q6)\nIn this section, we study the effect of the error threshold\nparameter ϵfor the indices internally using PGM-Indexas the 1-D learned index implementation (i.e., Flood ,\nLISA ,MLI, and ZMI). Figure 15 shows the index con-\nstruction time, range query processing time, and index\nmemory cost when varying ϵin the range [4, 1024] on\ntwo real datasets.\nThe index construction time is generally insensitive\ntoϵas the optimal piece-wise linear fitting algorithm\nadopted by PGM-Index requires only one pass of data\n(i.e., O(N) time), which contributes a minor part com-\npared with the overhead caused by data partitioning\nand projection function computation.\nAs for range query processing, different from 1-D\nlearned indices where a small ϵdefinitely leads to less\nsearch time, the relationship between query time and\nϵis not monotonic. The reason is that a smaller ϵcan\nonly reduce the overhead of a single point search; how-\never, the most costly part of range query processing is\nwhether the learned models and the underlying data\nlayout can well filter unnecessary result points.\nAs for the space cost, our results are similar to the\n1-D case reported in [18] where decreasing ϵwill en-\nlarge the space overhead as more line segments are re-\nquired to satisfy the maximum error constraint. Accord-\ning to a recent theoretical analysis of PGM-Index [17],\nthe space cost for a PGM-Index of error parameter ϵis\n\nHow Good Are Multi-dimensional Learned Indices? An Experimental Survey 19\nInsertion 102\n101\n100101102Avg. Time (us)kdtree\nSTR\nR*treeUG\nEDG\nIFILisa\nRSMI\noctree\n(a) Foursquare: insertion Time\nDeletion103\n102\n101\n100101102103104Avg. Time (us)kdtree\nSTR\nR*treeUG\nEDG\nIFILisa\nRSMI\noctree (b) Foursquare: deletion Time\nRange Query 103104Avg. Time (us)kdtree\nSTR\nR*treeUG\nEDG\nIFILisa\nRSMI\noctree (c) Foursquare: range Time\nkNN Query 102103104Avg. Time (us)kdtree\nSTRR*tree\nLisaRSMI\noctree (d) Foursquare: knn Time\nInsertion 102\n101\n100101Avg. Time (us)kdtree\nSTR\nR*treeUG\nEDG\nIFILisa\nRSMI\noctree\n(e) Toronto3d: insertion Time\nDeletion103\n102\n101\n100101102103Avg. Time (us)kdtree\nSTR\nR*treeUG\nEDG\nIFILisa\nRSMI\noctree (f) Toronto3d: deletion Time\nRange Query 103104105106Avg. Time (us)kdtree\nSTR\nR*treeUG\nEDG\nIFILisa\nRSMI\noctree (g) Toronto3d: range Time\nkNN Query 102103104Avg. Time (us)kdtree\nSTRR*tree\nLisaRSMI\noctree (h) Toronto3d: knn Time\nFig. 16: Data update evaluation results. Note that, RSMI fails to terminate on both datasets and IFIfails to\nterminate on the Toronto3d dataset.\nbounded by O(N/ϵ2), which coincides with the memory\ncost evaluation results as shown in Figure 15.\nTakeaways. For multi-dimensional learned indices in-\nternally employing PGM-Index, much different from\nthe 1-D case, both index construction time and query\nprocessing time are insensitive to the setting of ϵ. In\nsummary, we suggest ϵ= 64 for all the learned indices\nthat employ the PGM-Index in their structure, as such\nanϵcan well trade-off the index building time, memory\noverhead, and query processing efficiency.\n5.7 Index Update Efficiency Evaluation (Q7)\nIn this section, we report the update efficiency evalu-\nation results for the learned indices that support dy-\nnamic operations (i.e., LISA ,RSMI , and IFI). Specifi-\ncally, for each dataset, we first use the first 80% points\nto construct an index. Then, we sequentially insert the\nremaining 20% points into the index and randomly re-\nmove 20% points from the existing ones.\nFigure 16 reports the average insertion and dele-\ntion time on two real-world datasets ( FourSquare and\nToronto3d ). We can observe that, for the learned in-\ndices marked as updatable (i.e., IFI,LISA , and RSMI ),\nthey are generally less performant than conventional\nindex structures in terms of update efficiency. More-\nover, during the data updates, we randomly generate 20range queries and kNN queries5at different timestamps\nto evaluate the query efficiency under a dynamic envi-\nronment. In contrast to the previous findings observed\nunder a static environmental condition, our evaluation\nresults indicate that, with regard to the efficiency of\nrange query and kNN query processing, learned indices\ngenerally incur higher costs compared to non-learned\nbaselines.\nThe reason is that current updatable learned in-\ndices on multi-dimensional data usually employ simple\nstrategies to accommodate dynamic updates like set-\nting a threshold of index retraining, introducing non-\nnegligible cost. Moreover, existing techniques for de-\nsigning updatable 1-D learned index (e.g., ALEX [14]\nand LIPP [63]) cannot be seamlessly applied to han-\ndle the case of multi-dimensional databases. This is\nbecause, as previously mentioned, a multi-dimensional\nlearned index comprises two primary components: data\nlayouts and learned models. Consequently, dynamic op-\nerations on multi-dimensional learned indices necessi-\ntate efficient updates to both the underlying data lay-\nouts (e.g., grids or partitions) and the learned models\n(e.g., PGM-Index).\nTakeaways. Among the fewlearned indices that are\ncapable of handling dynamic operations, their efficiency\n5The query generation process follows the same way as we\ndescribed in Section 4.2\n\n20 Qiyu Liu et al.\nTable 6: CPU performance counter statistics (obtained by perf ) when processing range queries on real and\nsynthetic datasets of default configurations. #CR,CMR% ,#BR,BMR% refer to the count of total cache references, cache\nmiss rate, count of CPU branches, and CPU branch predication miss rate, respectively. The cell colors from left\nto right ■■■■■ represent range query processing costs ranging from high to low.\nIndexUniform Normal Lognormal Foursquare Toronto3d OSM\n#CR (CMR%) #CR (CMR%) #CR (CMR%) #CR (CMR%) #CR (CMR%) #CR (CMR%)\n#BR (BMR%) #BR (BMR%) #BR (BMR%) #BR (BMR%) #BR (BMR%) #BR (BMR%)\nZMI229M (33.86%) 136M (31.51%) 81M (38.48%) 51M (11.37%) 288M (48.45%) 581M (53.11%)\n979M (0.10%) 544M (0.10%) 331M (0.10%) 207M (0.09%) 1.15B (0.07%) 2.32B (0.04%)\nMLI557M (45.33%) 362M (46.62%) 351M (50.12%) 127M (30.12%) 2.34B (54.55%) 1.50B (58.83%)\n1.36B (6.60%) 899M (7.14%) 842M (3.47%) 299M (1.45%) 4.14B (4.12%) 3.39B (6.30%)\nIFI149M (41.54%) 114M (49.37%) 54M (49.08%) 42M (24.45%) 225M (56.71%) 356M (54.24%)\n197M (1.23%) 314M (0.90%) 76M (1.54%) 130M (0.33%) 528M (1.03%) 453M (0.53%)\nLISA153M (41.82%) 98M (35.39%) 55M (43.71%) 37M (18.40%) 215M (52.58%) 421M (54.67%)\n451M (0.88%) 275M (1.21%) 147M (0.92%) 107M (1.00%) 592M (2.37%) 1.14B (0.38%)\nFlood152M (40.77%) 97M (33.81%) 55M (43.99%) 34M (15.16%) 198M (52.02%) 419M (56.44%)\n426M (0.65%) 253M (0.83%) 139M (0.69%) 92M (0.70%) 486M (2.16%) 1.10B (0.29%)\nSTR175M (47.95%) 106M (45.06%) 64M (54.49%) 35M (30.53%) 205M (48.99%) 470M (57.19%)\n499M (0.42%) 281M (0.57%) 171M (0.50%) 88M (0.48%) 487M (0.71%) 1.25B (0.27%)\nR*tree195M (50.74%) 117M (49.81%) 70M (56.06%) 38M (32.88%) 214M (52.47%) 505M (59.04%)\n505M (0.54%) 278M (0.67%) 174M (0.73%) 89M (0.60%) 477M (0.58%) 1.25B (0.30%)\nqdtree50.4B (90.50%) 19.2B (95.25%) 7.20B (93.71%) 4.79B (94.60%) 13.6B (88.43%) 10.4B (80.44%)\n137B (1.50%) 43.8B (1.00%) 18.7B (0.67%) 12.0B (0.88%) 45.8B (0.38%) 81.8B (0.03%)\nUG169M (40.67%) 104M (42.01%) 94M (57.05%) 43M (24.68%) 350M (57.75%) 434M (56.57%)\n518M (1.10%) 304M (3.32%) 338M (7.20%) 135M (3.95%) 1.09B (0.85%) 1.18B (0.16%)\nEDG171M (37.04%) 104M (38.17%) 62M (39.31%) 37M (17.57%) 212M (51.65%) 423M (57.92%)\n522M (1.10%) 297M (1.61%) 176M (1.16%) 106M (2.20%) 589M (1.00%) 1.13B (0.13%)\nFullscan3.2B (66.21%) 1.7B (68.53%) 620M (65.63%) 501M (63.78%) 2.50B (60.56%) 5.67B (61.88%)\n14.4B (16.56%) 8.3B (15.23%) 2.7B (9.82%) 2.3B (11.93%) 9.6B (0.51%) 28.3B (0.19%)\nin terms of insertion, deletion, and dynamic query pro-\ncessing typically falls short when compared to tradi-\ntional spatial indices, such as the variants of R-tree and\nkd-tree.\n5.8 Deep Dive into Learned Index Performance (Q8)\nTo gain deeper insights into the performance of learned\nindices, we conduct a comprehensive evaluation of CPU\nperformance counter statistics during the processing of\nrange queries. Specifically, Table 6 presents the total\ncache reference count ( #CR), cache miss ratio ( CMR% ),\ntotal CPU branch count ( #BR), and branch prediction\nmiss ratio ( BMR% )6.\nTo better measure the CPU overhead caused by in-\ndex access, we pick a collection of range queries that\nare less selective ( Sel≤0.1) to reduce the influence\ncaused by extra overhead of reporting query results. In\naddition, to shed light on the relationship between the\nquery processing efficiency and these CPU performance\ncounter statistics, we mark the cell colors of Table 6\nfrom the color map ■■■■■ . The color towards the\nleft of the color map (e.g., the dark red color ■) in-\ndicates the longest query processing time (ranked the\n6The hardware performance counters (HPC) are accessed\nby invoking the standard Linux tool perf.three slowest); on the other hand, the color towards the\nright of the color map (e.g., the dark green color ■) in-\ndicates the shortest query processing time (ranked the\ntwo fastest).\nLearned indices including IFI,LISA andFlood ob-\ntain smaller query processing costs in general, as in-\ndicated by their green cell colors in Table 6. It could\nbe observed that these learned indices generally ob-\ntain smaller values in all of the four CPU performance\ncounter statistics. For example, on synthetic dataset\nNormal ,Flood produces 97 million total cache refer-\nences with a cache miss rate of 33.81%, where both val-\nues are the smallest among all other compared methods\n(in the same column). Meanwhile, Flood produces 253\nmillion CPU branches with a 0.83% branch prediction\nmiss rate, where both values are the smallest compared\nwith all other methods.\nWe then visualize the relationship between range\nquery processing time w.r.t. 4 CPU performance coun-\nters in Figure 17. A positive correlation can be indi-\nvidually observed for each CPU performance counter\nstatistics. To further quantitatively depict the correla-\ntion between the query processing time (i.e., the depen-\ndant variable y) and the four CPU performance counter\nstatistics (i.e., the independent variables x1: #Cache\nReferences, x2: Cache Miss Rate, x3: #CPU Branches,\n\nHow Good Are Multi-dimensional Learned Indices? An Experimental Survey 21\n0.0 0.1 0.2 0.3 0.4 0.5 0.6\n#Cache References050000100000150000200000250000time (us)\n0.2 0.4 0.6 0.8\nCache Misses (%)20000\n020000400006000080000100000time (us)\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7\n#CPU Braches050000100000150000200000time (us)\n0.000 0.025 0.050 0.075 0.100 0.125 0.150\nBranch Misses (%)020000400006000080000100000time (us)\nFig. 17: Range query processing time w.r.t. difference CPU performance counter statistics, i.e., #Cache References\n(CR), Cache Miss Rate (CMR), #CPU Branches (BR), and Branch Miss Rate (BRM).\nTable 7: Regression analysis results. The dependent\nvariable is the range query processing time and inde-\npendent variables are #CR,CMR% , and BMR% . To avoid the\ninfluence of outliers, we perform a robust linear regres-\nsion with Huber loss [26]. We report the coefficient esti-\nmations, standard errors, and significance values based\non Z-statistics.\nVariable Coef Std Err Z P>|Z|\nconst -6813.1178 2555.357 -2.666 0.008\n#CR 1.804e+05 7723.199 23.357 0.000\nCMR% 1.943e+04 5866.650 3.312 0.001\nBMR% 2.389e+05 2.02e+04 11.832 0.000\nandx4: Branch Miss Rate), we perform a linear regres-\nsion analysis to fit a linear model y=w0+w1x1+w2x2+\nw4x4. Note that, we omit feature x3(#CPU Branches)\nas it is highly linearly correlated to x1(#Cache Ref-\nerences). The results are shown in Table 7 where each\nindependent variable excluding the constant variable is\npositively correlated to the query processing time with\nhigh significance ( P < 0.01).\nAs validated by the linear regression analysis, there\nis a general positive linear correlation between the query\nprocessing costs and the four CPU performance counter\nvalues. However, it does not mean that smaller values inone of the statistics solely (e.g., CPU cache miss rates)\nnecessarily lead to shorter query processing time. For\nexample, for grid-based method EDGon the Lognormal\ndataset, the CPU cache miss rate is 39.31%, which is\nsmaller than ones of Flood ,LISA andIFI. But as indi-\ncated by the green cell colors, the range query process-\ning time of Flood ,LISA andIFIare all less than that of\nEDG. The reason is that the above 3 learned indices have\na better capability of pruning unnecessary candidates\ngiven a specific query range, leading to less memory\nfootprint (i.e., small total cache references count). As\nan example, the count of total cache references (e.g.,\n54 million, 55 million, and 55 million for IFI,LISA and\nFlood , respectively) are all smaller than the 62 mil-\nlion references produced by EDG. As a consequence, the\noverall effect of smaller total cache references results in\nshorter query processing time.\nTakeaways. There is a general positive correlation be-\ntween the query processing time and the CPU perfor-\nmance counters, including the counts of cache refer-\nences, cache miss rates, the number of CPU branches,\nand the branch miss rate. Performant learned indices\nsuch as Flood ,LISA and IFIhave a better capabil-\nity of pruning unnecessary candidates given a specific\nrange query, resulting in fewer cache references, and\neventually leading to faster query processing.\n\n22 Qiyu Liu et al.\nTable 8: Summary of evaluation results. The cell colors from left to right ■■■■ represent the performance levels\nof each evaluation item from worst to best.\nIndexIndex Construction Evaluation Query Processing Evaluation\nConstruction\nTimeMemory\nCostParameter\nTunnigRange Query\nEfficiencykNN Query\nEfficiencyScalabilityDynamic\nUpdate\nZMIbetter than\nR-treeexcellent easysometimes better\nthan R-treesometimes better\nthan kd-treeexcellent not supported\nMLIcomparable\nto R-treeexcellent easyno better\nthan R-treesometimes better\nthan kd-treegood not supported\nIFIcomparable\nto R-treegood N.A.robustly better\nthan R-treenot supported excellent limited support\nRSMImuch larger\nthan R-treegood N.A.sometimes better\nthan R-treesometimes better\nthan kd-treebad limited support\nLISAcomparable\nto R-treegood easyrobustly better\nthan R-treesometimes better\nthan kd-treeexcellent limited support\nFloodcomparable\nto R-treegood easyrobustly better\nthan R-treenot supported excellent not supported\n5.9 Summary of Results\nBased on the experimental results, we compile a table\nto summarize our evaluation for each learned index in\nTable 8 from two major aspects: index construction and\nquery processing.\nIndex Construction (Q1, Q2, Q6). In terms of the\ncost of index construction (shown in the Construction\nTime column in Table 8), learned indices using PGMIn-\ndex as their internal models are generally as fast as non-\nlearned baselines, while deep learning-based learned in-\ndices are much slower than non-learned baselines due\nto the heavy deep learning runtimes.\nAs for the memory footprint of the constructed in-\ndex (see the Memory Cost column in Table 8), multi-\ndimensional learned indices generally achieve robust and\nsignificant space reduction compared to non-learned in-\ndices like R-tree.\nIn addition, our evaluation results also indicate that\nmuch different from the 1-dimensional scenario, the er-\nror parameter ϵfor multi-dimensional learned indices\nhas minimal impact on index construction time and\nand query efficiency. Therefore, by a careful trade-off\nbetween efficiency and space cost, we suggest setting ϵ\nto64to achieve satisfactory performance.\nQuery Processing (Q3, Q4, Q5, Q7). As for range\nquery processing efficiency (see the corresponding col-\numn in Table 8), learned indices like IFI,Flood , and\nLISA can consistently outperform non-learned baselines\nlike R-tree variants for queries with varying selectiv-\nity levels. The speedup ratio is particularly notable for\ndatasets with uniform distribution and queries with low\nselectivity.\nThe evaluation results of the kNN query processing\n(see the kNN Query Efficiency column in Table 8) con-\ntrast to the conclusion of range queries where there is nolearned index that can systematically outperform non-\nlearned baselines like kd-tree variants. The progressive\nrange search nature hinders efficient kNN query pro-\ncessing for learned indices.\nAs for the scalability (see the Scalability column in\nTable 8), most methods scale well to large datasets (up\ntoN= 100M) and moderate dimensions (up to d= 8),\nwith an exception of RMSI , on which we fail to train\nthe underlying deep learning models when the data size\nreaches 50M.\nFor the capability of handling dynamic operations\n(see the Dynamic Update column in Table 8), the ex-\nisting learned index structures either cannot support\ndynamic operations or provide very limited support,\nwhich cannot systematically outperform the conven-\ntional indices like R-tree and kd-tree variants.\nPerformance Analysis (Q8). Our deep-dive analy-\nsis into the performance of learned indices in Section 5.8\nreveals that, there is a general positive correlation be-\ntween the query processing time and hardware perfor-\nmance counter statistics (e.g., cache reference counts\nand cache miss rates). The most performant learned in-\ndices such as Flood ,LISA andIFIgenerally have a bet-\nter capability of pruning unnecessary candidates given\na specific range query, resulting in fewer cache refer-\nences, and a better capability of utilizing cache locality\nin their data layout design. In addition to cache refer-\nences, methods with a higher number of CPU branches\nor higher branch miss rates tend to have longer query\nprocessing time.\n6 Conclusion and Future Studies\n6.1 Conclusion\nEfficient indexing method holds paramount importance\nfor multi-dimensional data management and analytics.\n\nHow Good Are Multi-dimensional Learned Indices? An Experimental Survey 23\nInspired by the seminal work of 1-D learned index [32],\nthere is a growing trend of utilizing machine learning\nmodels to directly learn the storage layouts for multi-\ndimensional data. However, by thoroughly investigating\nthe experiments reported in existing multi-dimensional\nlearned index studies, we find out that there lacks a\ncomprehensive benchmark to evaluate these indices un-\nder a uniform configuration. In this study, we first sur-\nvey the recent multi-dimensional learned indices and\ndeliver a taxonomy based on how learned models are\nintegrated into the data layouts. Then, we develop an\nopen-soured benchmark and evaluate 6 representative\nmulti-dimensional learned indices by unifying the index\nimplementation, datasets, query workloads, and evalu-\nation metrics.\nThe key experimental findings are threefold.\nF1: Compared with traditional indices like R-tree and\nkd-tree variants, multi-dimensional learned indices\n(especially LISA ,Flood , and IFI) can significantly\nreduce the index size (up to 3 orders of magnitude)\nwhile achieving better range query processing ef-\nficiency (up to 2 .19×).\nF2: In terms of kNN query processing and dynamic op-\nerations, learned indices cannot consistently out-\nperform conventional methods due to the lack of\nproper query processing algorithms and the intrin-\nsic hardness of updating models and data layouts\nat the same time.\nF3: We identify the correlations between the perfor-\nmance of learned indices and hardware performance\ncounters, revealing that efficient learned indices\nare usually cache-efficient. They have stronger can-\ndidate pruning power (reflecting on fewer cache\nreference counts) and better data layouts to main-\ntain spatial locality (reflecting on lower cache miss\nrates).\n6.2 Future Studies\nLearned index design on multi-dimensional data is a\nprominent and rapidly growing field in the data man-\nagement community. Our thorough experimental study\nfinds out that, though achieving significant improve-\nments in some cases, there are still multiple technical\nissues to be addressed for the multi-dimensional learned\nindex before being widely applied to practical systems.\nWe identify the following potential research oppor-\ntunities based on our key experimental findings.\nAdvanced Query Support. According to F1andF2,\nexisting multi-dimensional learned indices are capable\nof accelerating range query processing while signifi-\ncantly reducing the space cost. On the other hand, thesupport to kNN queries and more advanced analytical\noperators like skyline query [9] and spatial joins [25,64]\nare either limited or missing. It would be interesting\nand challenging to extend existing learned index struc-\ntures or design new learned data layouts to support\nthese analytical queries efficiently.\nEfficient Updatable Index. As we discussed in Sec-\ntion 3.6 and F2, most of the existing learned indices\nfocus on the read-only workloads. Though indices like\nRSMI [50] and LISA [35] support dynamic insertion and\ndeletion via a model-based data layout, it is inevitable\nto re-train the whole models when the data distribu-\ntion significantly shifts from the initial one that is used\nto build the index. A fully updatable learned multi-\ndimensional index requires efforts on novel data layout\ndesign and efficient model update strategy. In addition,\na well-crafted cost model should be established to em-\nbed learned indices into DBMS with cost-based opti-\nmizers seamlessly.\nIO-Efficient and Cache-Efficient Learned Index.\nExisting works on learned indices mainly target on in-\nmemory query processing. Although LISA [35] and\nRSMI [50] claim that they can be extended to disk-\nbased indices, unfortunately, they do not adopt any op-\ntimization considering the disk access characteristics. In\naddition, although F3claims that some learned index\nstructures are cache-efficient, there are still optimiza-\ntion opportunities to further reduce the cache miss rate\nby designing new ML-based data layouts that carefully\nconsider the memory hierarchy.\nDistributed Spatial Analytics. To process and an-\nalyze web-scale multi-dimensional data, practical so-\nlutions usually adopt distributed computation engines\nlike Spark [56] to serve the backend, e.g., Location-\nSpark [58], GeoSpark [68], Simba [65]. As reported in\nan evaluation study [46], the index memory overhead\nof existing systems is generally high. Due to the signif-\nicant space-time trade-off ( F1), it would be promising\nto design new distributed multi-dimensional analytical\nsystems powered by learned indices.\nReferences\n1. Aggarwal, C.C., Hinneburg, A., Keim, D.A.: On the sur-\nprising behavior of distance metrics in high dimensional\nspace. In: Database Theory—ICDT 2001: 8th Interna-\ntional Conference London, UK, January 4–6, 2001 Pro-\nceedings 8, pp. 420–434. Springer (2001)\n2. ANN Project. http://www.cs.umd.edu/ ~mount/ANN/ .\nAccessed: 2024-04-15\n3. Arge, L., de Berg, M., Haverkort, H.J., Yi, K.: The prior-\nity r-tree: A practically efficient and worst-case optimal\nr-tree. ACM Trans. Algorithms 4(1), 9:1–9:30 (2008)\n4. Arthur, D., Vassilvitskii, S.: k-means++: the advantages\nof careful seeding. In: SODA, pp. 1027–1035. SIAM\n(2007)\n\n24 Qiyu Liu et al.\n5. Beckmann, N., Kriegel, H., Schneider, R., Seeger, B.: The\nR*-tree: An efficient and robust access method for points\nand rectangles. In: SIGMOD Conference, pp. 322–331.\nACM Press (1990)\n6. Bentley, J.L.: Multidimensional binary search trees used\nfor associative searching. Commun. ACM 18(9), 509–517\n(1975)\n7. Boffa, A., Ferragina, P., Vinciguerra, G.: A ”learned” ap-\nproach to quicken and compress rank/select dictionaries.\nIn: ALENEX, pp. 46–59. SIAM (2021)\n8. Boost Geometry. http://boost.org/libs/geometry . Ac-\ncessed: 2024-04-15\n9. B¨ orzs¨ onyi, S., Kossmann, D., Stocker, K.: The skyline op-\nerator. In: ICDE, pp. 421–430. IEEE Computer Society\n(2001)\n10. Bozkaya, T., Ozsoyoglu, M.: Indexing large metric spaces\nfor similarity search queries. ACM Transactions on\nDatabase Systems (TODS) 24(3), 361–404 (1999)\n11. Chen, H., Chiang, R.H., Storey, V.C.: Business intelli-\ngence and analytics: From big data to big impact. MIS\nquarterly pp. 1165–1188 (2012)\n12. Chen, L., Gao, Y., Zheng, B., Jensen, C.S., Yang, H.,\nYang, K.: Pivot-based metric indexing. Proceedings of\nthe VLDB Endowment 10(10) (2017)\n13. Davitkova, A., Milchevski, E., Michel, S.: The ML-index:\nA multidimensional, learned index for point, range, and\nnearest-neighbor queries. In: EDBT, pp. 407–410. Open-\nProceedings.org (2020)\n14. Ding, J., Minhas, U.F., Yu, J., Wang, C., Do, J., Li, Y.,\nZhang, H., Chandramouli, B., Gehrke, J., Kossmann, D.,\nLomet, D.B., Kraska, T.: ALEX: an updatable adaptive\nlearned index. In: SIGMOD Conference, pp. 969–984.\nACM (2020)\n15. Ding, J., Nathan, V., Alizadeh, M., Kraska, T.: Tsunami:\nA learned multi-dimensional index for correlated data\nand skewed workloads. Proc. VLDB Endow. 14(2), 74–86\n(2020)\n16. Faghmous, J.H., Kumar, V.: Spatio-temporal data min-\ning for climate data: Advances, challenges, and opportu-\nnities. In: Data mining and knowledge discovery for big\ndata, pp. 83–116. Springer (2014)\n17. Ferragina, P., Lillo, F., Vinciguerra, G.: Why are learned\nindexes so effective? In: ICML, Proceedings of Ma-\nchine Learning Research , vol. 119, pp. 3123–3132. PMLR\n(2020)\n18. Ferragina, P., Vinciguerra, G.: The PGM-index: a fully-\ndynamic compressed learned index with provable worst-\ncase bounds. Proc. VLDB Endow. 13(8), 1162–1175\n(2020)\n19. FourSquare Data. https://sites.google.com/site/\nyangdingqi/home/foursquare-dataset . Accessed: 2024-\n04-15\n20. GEOS. https://github.com/libgeos/geos . Accessed:\n2024-04-15\n21. gperftools. https://github.com/gperftools/\ngperftools . Accessed: 2024-04-15\n22. Gu, T., Feng, K., Cong, G., Long, C., Wang, Z., Wang,\nS.: The RLR-tree: A reinforcement learning based r-tree\nfor spatial data. CoRR abs/2103.04541 (2021)\n23. Guttman, A.: R-trees: A dynamic index structure for spa-\ntial searching. In: SIGMOD Conference, pp. 47–57. ACM\nPress (1984)\n24. Hadian, A., Kumar, A., Heinis, T.: Hands-off model in-\ntegration in spatial index structures. In: AIDB@VLDB\n(2020)\n25. Hjaltason, G.R., Samet, H.: Incremental distance join al-\ngorithms for spatial databases. In: SIGMOD Conference,\npp. 237–248. ACM Press (1998)26. Huber, P.J.: Robust estimation of a location parameter.\nIn: Breakthroughs in statistics: Methodology and distri-\nbution, pp. 492–518. Springer (1992)\n27. Jagadish, H.V., Ooi, B.C., Tan, K., Yu, C., Zhang, R.:\niDistance: An adaptive B+-tree based indexing method\nfor nearest neighbor search. ACM Trans. Database Syst.\n30(2), 364–397 (2005)\n28. Kamel, I., Faloutsos, C.: Hilbert R-tree: An improved\nR-tree using fractals. In: VLDB, pp. 500–509. Morgan\nKaufmann (1994)\n29. Kanth, K.V.R., Ravada, S., Abugov, D.: Quadtree and\nr-tree indexes in oracle spatial: a comparison using GIS\ndata. In: SIGMOD Conference, pp. 546–557. ACM (2002)\n30. Kipf, A., Marcus, R., van Renen, A., Stoian, M., Kemper,\nA., Kraska, T., Neumann, T.: Radixspline: a single-pass\nlearned index. In: aiDM@SIGMOD, pp. 5:1–5:5. ACM\n(2020)\n31. Kraska, T., Alizadeh, M., Beutel, A., Chi, E.H., Kristo,\nA., Leclerc, G., Madden, S., Mao, H., Nathan, V.:\nSageDB: A learned database system. In: CIDR.\nwww.cidrdb.org (2019)\n32. Kraska, T., Beutel, A., Chi, E.H., Dean, J., Polyzotis,\nN.: The case for learned index structures. In: SIGMOD\nConference, pp. 489–504. ACM (2018)\n33. Kristo, A., Vaidya, K., C ¸etintemel, U., Misra, S., Kraska,\nT.: The case for a learned sorting algorithm. In: SIGMOD\nConference, pp. 1001–1016. ACM (2020)\n34. Leutenegger, S.T., Edgington, J.M., L´ opez, M.A.: STR:\nA simple and efficient algorithm for R-tree packing. In:\nICDE, pp. 497–506. IEEE Computer Society (1997)\n35. Li, P., Lu, H., Zheng, Q., Yang, L., Pan, G.: LISA: A\nlearned index structure for spatial data. In: SIGMOD\nConference, pp. 2119–2133. ACM (2020)\n36. Liu, Q., Shen, Y., Chen, L.: Hap: an efficient hamming\nspace index based on augmented pigeonhole principle.\nIn: Proceedings of the 2022 International Conference on\nManagement of Data, pp. 917–930 (2022)\n37. Liu, Q., Zheng, L., Shen, Y., Chen, L.: Stable learned\nbloom filters for data streams. Proc. VLDB Endow.\n13(11), 2355–2367 (2020)\n38. Marcus, R., Kipf, A., van Renen, A., Stoian, M., Misra,\nS., Kemper, A., Neumann, T., Kraska, T.: Benchmarking\nlearned indexes. Proc. VLDB Endow. 14(1), 1–13 (2020)\n39. Mitzenmacher, M.: A model for learned bloom filters and\noptimizing by sandwiching. In: NeurIPS, pp. 462–471\n(2018)\n40. nanoflann. https://github.com/jlblancoc/nanoflann .\nAccessed: 2024-04-15\n41. Nathan, V., Ding, J., Alizadeh, M., Kraska, T.: Learning\nmulti-dimensional indexes. In: SIGMOD Conference, pp.\n985–1000. ACM (2020)\n42. Nievergelt, J., Hinterberger, H., Sevcik, K.C.: The grid\nfile: An adaptable, symmetric multikey file structure.\nACM Trans. Database Syst. 9(1), 38–71 (1984)\n43. Numpy. https://numpy.org/ . Accessed: 2024-04-15\n44. O’Rourke, J.: An on-line algorithm for fitting straight\nlines between data ranges. Communications of the ACM\n24(9), 574–578 (1981)\n45. OpenStreet Map. https://planet.openstreetmap.org .\nAccessed: 2024-04-15\n46. Pandey, V., Kipf, A., Neumann, T., Kemper, A.: How\ngood are modern spatial analytics systems? Proc. VLDB\nEndow. 11(11), 1661–1673 (2018)\n47. Pandey, V., van Renen, A., Kipf, A., Kemper, A.: How\ngood are modern spatial libraries? Data Sci. Eng. 6(2),\n192–208 (2021)\n\nHow Good Are Multi-dimensional Learned Indices? An Experimental Survey 25\n48. PostgreSQL: Postgresql: The world’s most advanced\nopen source relational database. Web resource:\nhttps://www.postgresql.org/ (2021)\n49. PyTorch. https://pytorch.org/ . Accessed: 2024-04-15\n50. Qi, J., Liu, G., Jensen, C.S., Kulik, L.: Effectively learn-\ning spatial indices. Proc. VLDB Endow. 13(11), 2341–\n2354 (2020)\n51. Qi, J., Tao, Y., Chang, Y., Zhang, R.: Packing R-trees\nwith space-filling curves: Theoretical optimality, empir-\nical efficiency, and bulk-loading parallelizability. ACM\nTrans. Database Syst. 45(3), 14:1–14:47 (2020)\n52. Ramsak, F., Markl, V., Fenk, R., Zirkel, M., Elhardt, K.,\nBayer, R.: Integrating the UB-tree into a database sys-\ntem kernel. In: VLDB, pp. 263–272. Morgan Kaufmann\n(2000)\n53. Rosenfeld, V., Breß, S., Markl, V.: Query processing on\nheterogeneous cpu/gpu systems. ACM Computing Sur-\nveys (CSUR) 55(1), 1–38 (2022)\n54. Samet, H.: Foundations of multidimensional and metric\ndata structures. Morgan Kaufmann series in data man-\nagement systems. Academic Press (2006)\n55. Sejnowski, T.J., Churchland, P.S., Movshon, J.A.:\nPutting big data to good use in neuroscience. Nature\nneuroscience 17(11), 1440–1441 (2014)\n56. Apache Spark. https://spark.apache.org/ . Accessed:\n2024-04-15\n57. Sun, Z., Zhou, X., Li, G.: Learned index: A comprehen-\nsive experimental evaluation. Proc. VLDB Endow. 16(8),\n1992–2004 (2023)\n58. Tang, M., Yu, Y., Malluhi, Q.M., Ouzzani, M., Aref,\nW.G.: LocationSpark: A distributed in-memory data\nmanagement system for big spatial data. Proc. VLDB\nEndow. 9(13), 1565–1568 (2016)\n59. Tao, Y., Papadias, D., Sun, J.: The TPR*-tree: An\noptimized spatio-temporal access method for predictive\nqueries. In: VLDB, pp. 790–801. Morgan Kaufmann\n(2003)\n60. Tensorflow. https://www.tensorflow.org/ . Accessed:\n2024-04-15\n61. Toronto3D Data. https://github.com/WeikaiTan/\nToronto-3D . Accessed: 2024-04-15\n62. Wang, H., Fu, X., Xu, J., Lu, H.: Learned index for spatial\nqueries. In: MDM, pp. 569–574. IEEE (2019)\n63. Wu, J., Zhang, Y., Chen, S., Chen, Y., Wang, J., Xing,\nC.: Updatable learned index with precise positions. Proc.\nVLDB Endow. 14(8), 1276–1288 (2021)\n64. Xia, C., Lu, H., Ooi, B.C., Hu, J.: Gorder: An efficient\nmethod for KNN join processing. In: VLDB, pp. 756–767.\nMorgan Kaufmann (2004)\n65. Xie, D., Li, F., Yao, B., Li, G., Zhou, L., Guo, M.: Simba:\nEfficient in-memory spatial analytics. In: SIGMOD Con-\nference, pp. 1071–1085. ACM (2016)\n66. Yang, Z., Chandramouli, B., Wang, C., Gehrke, J., Li,\nY., Minhas, U.F., Larson, P., Kossmann, D., Acharya,\nR.: Qd-tree: Learning data layouts for big data analytics.\nIn: SIGMOD Conference, pp. 193–208. ACM (2020)\n67. Yianilos, P.N.: Data structures and algorithms for nearest\nneighbor. In: Proceedings of the fourth annual ACM-\nSIAM Symposium on Discrete algorithms, vol. 66, p. 311.\nSIAM (1993)\n68. Yu, J., Wu, J., Sarwat, M.: GeoSpark: a cluster comput-\ning framework for processing large-scale spatial data. In:\nSIGSPATIAL/GIS, pp. 70:1–70:4. ACM (2015)\n69. Zhu, L., Yu, F.R., Wang, Y., Ning, B., Tang, T.: Big data\nanalytics in intelligent transportation systems: A survey.\nIEEE Trans. Intell. Transp. Syst. 20(1), 383–398 (2019)\n70. morton-nd. https://github.com/morton-nd/morton-nd .\nAccessed: 2024-04-15",
  "textLength": 100311
}