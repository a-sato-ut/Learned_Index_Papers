{
  "paperId": "6a3f980e3cd4e96a05a0e24b5d3d0b2164d9656d",
  "title": "Learning Everywhere: Pervasive Machine Learning for Effective High-Performance Computation",
  "pdfPath": "6a3f980e3cd4e96a05a0e24b5d3d0b2164d9656d.pdf",
  "text": "arXiv:1902.10810v1  [cs.DC]  27 Feb 2019Learning Everywhere: Pervasive Machine Learning\nfor Effective High-Performance Computation\nGeoffrey Fox1, James A. Glazier1, JCS Kadupitiya1, Vikram Jadhao1,\nMinje Kim1, Judy Qiu1, James P. Sluka1, Endre Somogyi1, Madhav Marathe2,\nAbhijin Adiga2, Jiangzhuo Chen2, Oliver Beckstein3, Shantenu Jha4,5\n1Indiana University, Bloomington, IN\n2University of Virginia, Charlottesville, VA\n3Arizona State University, Tempe, AZ\n4Rutgers, the State University of New Jersey, Piscataway, NJ 08854, USA\n5Brookhaven National Laboratory, Upton, New York, 11973\nAbstract —The convergence of HPC and data intensive method-\nologies provide a promising approach to major performance\nimprovements. This paper provides a general description of\nthe interaction between traditional HPC and ML approaches\nand motivates the ”Learning Everywhere” paradigm for HPC.\nWe introduce the concept of ”effective performance” that on e\ncan achieve by combining learning methodologies with simu-\nlation based approaches, and distinguish between traditio nal\nperformance as measured by benchmark scores. To support the\npromise of integrating HPC and learning methods, this paper\nexamines speciﬁc examples and opportunities across a serie s of\ndomains. It concludes with a series of open computer science and\ncyberinfrastructure questions and challenges that the Lea rning\nEverywhere paradigm presents.\nI. I NTRODUCTION\nThis paper describes opportunities at the interface betwee n\nlarge-scale simulations, experiment design and control, m a-\nchine learning (ML including deep learning DL) and High-\nPerformance Computing. We describe both the current status\nand possible research issues in allowing machine learning\nto pervasively enhance computational science. How should\none do this and where is it valuable? We focus on research\nchallenges on computing for science and engineering (as\nopposed to commercial) use cases for both big data and big\nsimulation problems. More details including further citat ions\ncan be found at [1].\nThe convergence of HPC and data-intensive methodolo-\ngies [2] provide a promising approach to major performance\nimprovements. Traditional HPC simulations are reaching th e\nlimits of original progress. The end of Dennard scaling of\ntransistor power usage and the end of Moores Law as origi-\nnally formulated has yielded fundamentally different proc essor\narchitectures. The architectures continue to evolve, resu lting\nin highly costly if not damaging churn in scientiﬁc codes tha t\nneed to be ﬁnely tuned to extract the last iota of parallelism\nand performance.\nIn domain sciences such as biomolecular sciences, advances\nin statistical algorithms and runtime systems have enabled\nextreme scale ensemble based applications [3] to overcome\nlimitations of traditional monolithic simulations. Howev er, inspite of several orders of magnitude improvement in efﬁcien cy\nfrom these adaptive ensemble algorithms, the complexity\nof phase space and dynamics for modest physical systems,\nrequire additional orders of magnitude improvements and\nperformance gains.\nIn many application domains, integrating traditional HPC\napproaches with machine learning methods arguably holds th e\ngreatest promise towards overcoming these barriers. The ne ed\nfor performance increase underlies the international effo rts\nbehind the exascale supercomputing initiatives and we beli eve\nthat integration of ML into large scale computations (for bo th\nsimulations and analytics) is a very promising way to get eve n\nlarge performance gains. Further, it can enable paradigms s uch\nas control or steering and provide a fundamental approach\nto coarse-graining which is a difﬁcult but essential aspect of\nthe many multi-scale application areas. Papers at two recen t\nworkshops BDEC2 [4] and NeurIPS [5] conﬁrm our point\nof view and our approach is synergistic with the BDEC2\nprocess with its emphasis on new application requirements\nand their implications for future scientiﬁc computing soft ware\nplatforms. We would like to distinguish between traditiona l\nperformance measured by operations per second or benchmark\nscores and the effective performance that one gets by combin -\ning learning with simulation and gives increased performan ce\nas seen by the user without changing the traditional system\ncharacteristics. This is of particular interest in cases wh ere\nthere is a tight coupling between the learning and simulatio n\ncomponents (as outlined below for MLforHPC). The need\nfor signiﬁcant enhancement in the effective performance of\nHPC motivates the introduction of a new paradigm in HPC:\nLearning Everywhere!\nDifferent Interfaces of ML and HPC: We have identiﬁed\n[6], [4] several important distinctly different links betw een ma-\nchine learning (ML) and HPC. We deﬁne two broad categories:\nHPCforML and MLforHPC,\n•HPCforML: Using HPC to execute and enhance ML\nperformance, or using HPC simulations to train ML\nalgorithms (theory guided machine learning), which are\nthen used to understand experimental data or simulations.\n\n•MLforHPC: Using ML to enhance HPC applications and\nsystems\nThis categorization is related to Jeff Dean’s ”Machine\nLearning for Systems and Systems for Machine Learning” [7]\nand Satoshi Matsuoka’s convergence of AI and HPC [8].We\nfurther subdivide HPCforML as\n•HPCrunsML: Using HPC to execute ML with high\nperformance\n•SimulationTrainedML: Using HPC simulations to train\nML algorithms, which are then used to understand exper-\nimental data or simulations.\nWe also subdivide MLforHPC as\n•MLautotuning: Using ML to conﬁgure (autotune) ML or\nHPC simulations. Already, autotuning with systems like\nATLAS is hugely successful and gives an initial view\nof MLautotuning. As well as choosing block sizes to\nimprove cache use and vectorization, MLautotuning can\nalso be used for simulation mesh sizes [9] and in big data\nproblems for conﬁguring databases and complex systems\nlike Hadoop and Spark [10], [11].\n•MLafterHPC: ML analyzing results of HPC as in trajec-\ntory analysis and structure identiﬁcation in biomolecular\nsimulations\n•MLaroundHPC: Using ML to learn from simulations\nand produce learned surrogates for the simulations. The\nsame ML wrapper can also learn conﬁgurations as well as\nresults. This differs from SimulationTrainedML as there\ntypically a learnt network is used to redirect observation\nwhereas in MLaroundHPC we are using the ML to\nimprove the HPC performance.\n•MLControl: Using simulations (with HPC) in control\nof experiments and in objective driven computational\ncampaigns [12]. Here the simulation surrogates are very\nvaluable to allow real-time predictions.\nAll 6 topics above are important and pose many research\nissues in computer science and cyberinfrastructure, direc tly in\napplication domains and in the integration of technology wi th\napplications. However, in this paper, we focus on topics in\nMLforHPC, with close coupling between ML, simulations, and\nHPC. We involve applications as a driver for the requirement s\nand evaluation of the computer science and infrastructure. In\nresearching MLaroundHPC we will consider ML wrappers\nfor either HPC simulations or complex ML algorithms imple-\nmented with HPC. Our focus is on how to increase effective\nperformance with the learning everywhere principle and how\nto build efﬁcient learning everywhere parallel systems.\nOne can view the use of ML learned surrogates as a per-\nformance boost that can lead to huge speedups as calculation\nof a prediction from a trained network, can be many orders\nof magnitude faster than full execution of the simulation as\nshown in section III-D. One can reach Exa or even Zetta scale\nequivalent performance for simulations with existing hard ware\nsystems. These high-performance surrogates are valuable i n\neducation and control scenarios by just speeding existing\nsimulations. Simple examples are the use of a surrogate torepresent a chemistry potential or a larger grain size to sol ve\nthe diffusion equation underlying cellular and tissue leve l\nsimulations. Development of systematic ML-based coarse-\ngraining techniques in both socio-technical simulations a nd\nnano-bio(cell)- tissue layering arises as an important are a of\nresearch. In general, Domain-speciﬁc expertise will be nee ded\nto understand the needed accuracy and the number of training\nsimulation runs needed.\nThere are many groups working in MLaroundHPC but\nmost of the work is just starting and not built around a\nsystematic study of research issues as we propose. There is\nsome deep work in building reduced dimension models to use\nin control scenarios [13]. We look at three distinct importa nt\nareas: Networked systems with socio-technical simulation s,\nmultiscale cell and tissue simulations and at a ﬁner scale\nbiomolecular and nanoscale molecular systems.\nWe note that biomolecular and biocomplexity areas which\nrepresent 40% of the HPC cycles used on NSF computational\nresources and so this is an area that is particularly ready\nand valuable. Molecular sciences has had several successfu l\nexamples of using ML for autotuning and ML for analyz-\ning the output of HPC simulation data. Several ﬁelds have\nmade progress in using MLaroundHPC, e.g., Cosmoﬂow and\nCosmoGAN [14] are amongst the better known projects; and\nthe Materials community is actively exploring the uptake of\nMLControl for the design of materials [4].\nThis paper does not cover development of new ML al-\ngorithms but rather the advancing the understanding of ML,\nincluding Deep Learning (DL) in support of MLaroundHPC.\nOf course, the usage experience is likely to suggest new ML\napproaches of value outside the MLaroundHPC arena. If one\nis to use an ML to replace a simulation, then an accuracy\nestimate is essential and as discussed in III-B there is a nee d to\nbuild on initial work on UQ (Uncertainty Quantiﬁcation) wit h\nML [15] such as that using dropout regularization to build\nensembles for UQ. There are more sophisticated Bayesian\nmethods to investigate. The research must also address er-\ngodicity, viz., have we learned across the full phase space\nof initial values. Here methods taken from Monte-Carlo aren a\ncould be useful as reliable integration over a domain is rela ted\nto reliable estimates of values deﬁned across a domain. Furt her\nmuch of our learning is for analytic functions whereas much\nof the existing DL experience is for discrete-valued classi ﬁers\nof commercial importance.\nSection III discusses cyberinfrastructure and computer sc i-\nence questions, section III-B covers uncertainty quantiﬁc ation\nfor learnt results while section III-C the infrastructure r equire-\nments needed to implement MLforHPC. Section III-D gives a\ngeneral performance analysis method and applies to current\ncases, Section III-E covers new opportunities and research\nissues.\nII. S CIENCE EXEMPLARS\nA. Machine learning for Networked Systems\nIn this section we describe a hybrid method that fuses\nmachine learning and mechanistic models to overcome the\n\nchallenges posed by scenarios where data is sparse and\nknowledge of underlying mechanism is inadequate. Across\ndomains, the two approaches have been compared [16]. Ma-\nchine learning approach usually needs a large amount of\nobservation data for training, and does not explicitly acco unt\nfor mechanisms that govern the the complex phenomenon.\nOn the other hand, mechanistic models (like agent-based\nmodels) result from a bottom-up approach; but they tend to\nhave too many parameters, are compute intensive and hard to\ncalibrate. In recent years, there have been several efforts to\nstudy physical processes under the umbrella of theory-guid ed\ndata science (TGDS), with focus on artiﬁcial neural network s\n(ANN) as the primary learning tool. [17] provides a survey\nof these methods and their application to hydrology, climat e\nscience, turbulence modeling, etc. where the underlying th eory\ncan be used to reduce the variance in model parameters by\nintroducing constraints or priors in the model space.\nHere we consider a particular class of mechanistic models\n- network dynamical systems, which have been applied in\ndiverse domains such as epidemiology and computational\nsocial science. A network dynamical system is composed of\na network where nodes of the network are agents (repre-\nsenting population, computers, etc.) and the edges capture\nthe interactions between them. A popular example of such\nsystems is the SEIR model of disease spread in a social\nnetwork [18]. The complexity of the dynamics in such a\nnetwork, due to individual level heterogeneity and interac tions,\nmakes it difﬁcult to train a machine learning model that\ncan be generalized to patterns not yet presented in historic al\ndata. Completely data driven models cannot discover higher\nresolution details (e.g. county level incidence) from lowe r\nresolution ground truth data (e.g. state level incidence).\nLearning from observational and simulation data: Data\nsparsity is often a challenge for applying machine learning ,\nespecially deep learning methods to forecasting problems i n\nsocio-technical systems. One example of such problems is\nto predict weekly incidence in future weeks in an inﬂuenza\nepidemic. In such socio-technical systems, we usually have\nonly limited observational data, e.g. weekly incidence num ber\nreported to the Centers for Disease Control and Prevention\n(CDC). Such data is of low spatial temporal resolution (week ly\nat state level), not real time (at least one week delay), inco m-\nplete (reported cases are only a small fraction of actual one s),\nand noisy (adjusted several times after being published), t hus\nnecessitating a hybrid framework for forecasting by learni ng\nfrom observational and simulation data.\nObservations need to be augmented with existing domain\nknowledge and behavior encapsulated in the agent-based\nmodel to inform the learning algorithm. In such hybrid\nframework, the network dynamical system is used to guide\nthe learning algorithm so that it conforms to the principles\n(consistency ). At the same time, the learning algorithm will\nfacilitate model selection in a principled manner. Moreove r,\nthe synthetic data goes beyond the observation data, thus\nhelps voiding overﬁtting and makes the learned model capabl e\nof processing patterns unseen in the observation data ( gen-eralizability ). When the dynamical system is more detailed\n(e.g. individual level) than the observation data, the hybr id\nframework allows detailed forecasting ( high resolution ).\nEpidemic Forecasting: Simulation trained machine learn-\ning methods can be used for epidemic forecasting. An ex-\nample of such a framework is DEFSI (Deep Learning Based\nEpidemic Forecasting with Synthetic Information) propose d\nin [19]. It consists of ( i) a model conﬁguration module\nthat estimates a distribution for each parameter in an agent\nbased epidemic model based on coarse surveillance data; ( ii)\nsimulation-geenrated synthetic training data module whic h\ngenerates high-resolution training data by running HPC sim -\nulations parameterized from distributions estimated in th e\nprevious module; ( iii) a two-branch deep neural network\ntrained on the synthetic training dataset and used to make\ndetails forecasts with coarse surveillance data as inputs.\nExperimental results show that DEFSI performs comparably\nor better than the other methods for state level forecasting ; and\nit outperforms the EpiFast method for county level forecast ing.\nSee Ref. [1] and citations therein for details.\nB. ML for Virtual Tissue and Cellular Simulations\n1) Virtual Tissue Models: Virtual Tissue (VT) simulations\n[20] are mechanism-based multiscale spatial simulations o f\nliving tissues that address questions about development, m ain-\ntenance, damage and repair. They also ﬁnd application in the\ndesign of tissues (tissue engineering) and the development\nof medical therapies, especially personalized therapies. VT\nsimulations are computationally challenging for a number o f\nreasons: 1) VT simulations are agent-based, with the core\nagent often representing biological cells. The number of ce lls\nin a real tissue is often of the order of 108or more. 2) Agents\nare often hierarchical, with agents composed of multiple ag ents\nat smaller scales. 3) Agents interact strongly with each oth er,\noften over signiﬁcant ranges [21]. 3) Individual agents typ i-\ncally contain complex sub models that control their propert ies\nand behaviors. 4) Materials properties may be complex, like\nthe shear thickening or thinning or swelling or contraction of\nﬁber networks. 5) Modeling transport and diffusion is compu te\nintensive. 6) Models are typically stochastic, so predicti vity\nrequires many replicas. 7) Simulations involve uncertaint y\nboth in model parameters and in model structure. 8) Bi-\nological and medical time-series data are often qualitativ e,\nsemi-quantitative or differential, making their use in cla ssical\noptimization difﬁcult. 9) VT models often produce movies of\nconﬁgurations over time. 10) Finally, simulating populati ons\ncan add several orders of magnitude to the computational\nchallenge. It is possible that ML techniques can be used to\nshort circuit implementations at and between scales.\n2) Virtual Tissue Modelling and AI + MLandHPC: AI can\ndirectly beneﬁt VT applications in a number of ways:\n1) Short-circuiting: The replacement of computationally\ncostly modules with learned analogues.\n2) Parameter ﬁtting in high dimensional parameter spaces.\n3) Treating stochasticity in results as information rather\nthan noise.\n\n4) Prediction of bifurcations in models.\n5) Design of maximally discriminatory experiments – pre-\ndict the parameter sets by which two models can be\ndifferentiated.\n6) Run time backwards, to determine initial conditions that\nlead to observed endpoints.\n7) The elimination of short time scales, e.g., short-circui t\nthe calculations of advection-diffusion.\n8) Generating additional spatial data sets from experiment al\nimages.\nRepresentative prior work by Karniadakis [13], Kevrekidis\n[22] and Nemenman [23] shows that neural networks can\nreproduce the temporal behaviors of biochemical regulator y\nand signaling networks. Ref. [24] has shown that networks ca n\nlearn nonlinear biomechanics simulations of the aorta–bei ng\nable to predict the stress and strain distribution in the hum an\naorta from the morphology observable with MRI or CT.\nC. Machine Learning and Molecular Simulations\n1) Nanoscale simulation: Despite the employment of the\noptimal parallelization techniques suited for the size and\ncomplexity of the system, nanoscale simulations remain tim e\nconsuming. In research settings, simulations can take up to\nseveral days and it is often desirable to foresee expected ov er-\nall trends in key quantities; for example, how does the conta ct\ndensity vary as a function of ion concentration in nanoscale\nconﬁnement or how the peak positions of the pair correlation\nfunctions characterizing nanoparticle assembly evolve as the\nenvironmental parameters are tuned. Given the dramatic ris e\nin ML and HPC technologies, it is not the question of if, but\nwhen, ML can be integrated with HPC to enhance nanoscale\nsimulation methods. Recent years have seen a surge in the\nuse of ML to accelerate material simulation techniques: ML\nhas been used to predict parameters, generate conﬁguration s\nin material simulations, and classify material properties (see\nRef [1] and citations therein). At this time, it is critical t o\nunderstand and develop the software frameworks to build ML\nlayers around HPC to 1) enhance simulation performance 2)\nenable real-time and anytime engagement, and 3) broaden the\napplicability of simulations for both research and educati on\n(in-classroom) usage.\nIn the context of nanoscale simulation, an initial set of\napplications for the MLaroundHPC framework can be the\nprediction of the structure or correlation functions (outp uts)\ncharacterizing the nanoscale system over a broad range of\nexperimental control parameters (inputs). MLaroundHPC ca n\nenable the following outcomes:\n1) Learn pre-identiﬁed critical features associated with t he\nsimulation output.\n2) Generate accurate predictions for un-simulated state-\npoints (by entirely bypassing simulations).\n3) Exhibit auto-tunability (with new simulation runs, the\nML layer gets better at making predictions).\n4) Enable real-time, anytime, and anywhere access to sim-\nulation results (particularly important for education use ).5) No run is wasted. Training needs both successful and\nunsuccessful runs.\nTo illustrate these outcomes, we discuss nanoscale simula-\ntions aimed at the computation of the structure of ions conﬁn ed\nby surfaces that are nanometers apart which has been the focu s\nof recent experiments and computational studies (see Ref [1 ]\nand citations therein). Typically, the entire ionic distri bution\naveraged over sufﬁcient number of independent samples gen-\nerated during the simulation is a quantity of interest. Howe ver,\nin many important cases, average values of contact density\nor center density directly relate to important experimenta lly-\nmeasured quantities such as the osmotic pressure [25]. Furt her,\noften it is useful to visualize expected trends in the behavi or\nof contact or mid-point density as a function of solution\nconditions or ionic attributes, before running simulation s to\nexplore speciﬁc system conditions. It is thus desirable tha t a\n“smart” simulation framework provide rapid estimates of th ese\ncritical output features with high accuracy. MLaroundHPC c an\nenable precisely this as we recently showed that an artiﬁcia l\nneural network successfully learns from completed simulat ion\nresults the desired features associated with the output ion ic\ndensity proﬁles to rapidly generate predictions for contac t,\npeak, and center densities in excellent agreement with the\nresults from explicit simulations [26].\n2) Biomolecular simulations: The use of ML and in par-\nticular DL approaches for biomolecular simulations [27] la gs\nbehind other areas such as nano-science and materials scien ce\n[28]. This might be partly due to the difﬁculty to account\nfor large heterogeneous systems with important interactio ns\nat short and long length scales. But it might also indicate\nthat the commonly used classical empirical force ﬁelds are\nsurprisingly successful [29] and it is not easy to outperfor m\nthem at this level of approximation. Therefore, one primary\ndirection of research in this area is to improve the accuracy of\nthe simulation while maintaining the performance of empiri cal\nenergy functions.\nOne promising approach is based on work by Behler and\nParrinello [30] who devised a NN-based potential that was\ntrained on quantum mechanical DFT energies; their key in-\nsight was to represent the total energy as a sum of atomic\ncontributions and represent the chemical environment arou nd\neach atom by an identically structured NN, which takes\nas input appropriate symmetry functions that are rotation\nand translation invariant as well as invariant to exchange o f\natoms while correctly reﬂecting the local environment that\ndetermines the energy [31]. Based on this work, Gastegger\net al. [32] used ML to accelerate ab-initio MD (AIMD) to\ncompute accurate IR spectra for organic molecules includin g\nthe biological Ala+\n3tripeptide in the gas phase. Interestingly,\nthe ML model was able to reproduce anharmonicities and\nincorporate proton transfer reactions between different A la+\n3\ntautomers without having been explicit trained on such a\nchemical event, highlighting the promise of such an approac h\nto incorporate a wide range of physically relevant effects w ith\nthe right training data. The ML model was >1000 faster\nthan the traditional evaluation of the underlying quantum\n\nmechanical physical equations.\nRoitberg et al. [33] trained a NN on QM DFT calculations,\nbased on modiﬁed Behler-Parrinello symmetry functions. Th e\nresulting ANI-1 model was shown to be chemically accurate,\ntransferrable, with a performance similar to a classical fo rce\nﬁeld, thus enabling ab-initio molecular dynamics (AIMD) at a\nfraction of the cost of ”true” DFT AIMD. Extensions of their\nwork with an active learning (AL) approach demonstrated tha t\nproteins in an explicit water environment can be simulated\nwith a NN potential at DFT accuracy [34]. The AL approach\nreduced the amount of required training data to 10% of\nthe original model [34] by iteratively adding training data\ncalculations for regions of chemical space where the curren t\nML model could not make good predictions. Using transfer\nlearning, the ANI-1 potential was also extended to predict\nenergies at the highest level of quantum chemistry calculat ions\n(coupled cluster CCSD(T)), with speedups in the billion.\nIn general the focus has been on achieving DFT-level\naccuracy because NN potentials are not cheaper to evaluate\nthan most classical empirical potentials. However, replac ing\nsolvent-solvent and solvent-solute interactions, which t ypically\nmake up 80%-90% of the computational effort in a classical\nall-atom, explicit solvent simulation, with a NN potential\npromises large performance gains at a fraction of the cost\nof traditional implicit solvent models and with an accuracy\ncomparable to the explicit simulations [35], as also discus sed\nabove in the case of electrolyte solutions. Furthermore, in -\nclusion of polarization, which is expensive (factor 3-10 in\ncurrent classical polarizable force ﬁelds [36]) but of grea t\ninterest when studying the interaction of multivalent ions with\nbiomolecules might be easily achievable with appropriatel y\ntrained ML potentials.\nIII. I NTEGRATING ML AND HPC: B ACKGROUND AND\nOPPORTUNITIES\nA primary contribution of this paper is in the categorizatio n,\ndescription and examples of the different ways in which ML\ncan enhance HPC (MLforHPC). Before we expound upon\nMLforHPC and open research issues, we provide a a summary\nstatus of HPC for ML (beyond the obvious and well-studied\nuse of GPUs for ML).\nA. HPC for Machine Learning\nThere has been substantial community progress here with\nthe Industry supported MLPerf [37] machine learning bench-\nmark activity and Ubers Horovod Open Source Distributed\nDeep Learning Framework for TensorFlow [38]. We have stud-\nied different parallel patterns (kernels) of machine learn ing ap-\nplications, looking in particular at Gibbs Sampling, Stoch astic\nGradient Descent (SGD), Cyclic Coordinate Descent (CCD)\nand K-means clustering [39]. These algorithms are fundamen -\ntal for large-scale data analysis and cover several importa nt\ncategories: Markov Chain Monte Carlo (MCMC), Gradient\nDescent and Expectation and Maximization (EM). We show\nthat parallel iterative algorithms can be categorized into four\ntypes of computation models (a) Locking, (b) Rotation, (c)Allreduce, (d) Asynchronous, based on the synchronization\npatterns and the effectiveness of the model parameter updat e.\nA major challenge of scaling is owing to the fact that compu-\ntation is irregular and the model size can be huge. At the\nmeantime, parallel workers need to synchronize the model\ncontinually. By investigating collective vs. asynchronou s meth-\nods of the model synchronization mechanisms, we discover\nthat optimized collective communication can improve the\nmodel update speed, thus allowing the model to converge\nfaster. The performance improvement derives not only from\naccelerated communication but also from reduced iteration\ncomputation time as the model size may change during the\nmodel convergence. To foster faster model convergence, we\nneed to design new collective communication abstractions.\nWe identify all 5 classes of data-intensive computation[2] ,\nfrom pleasingly parallel to machine learning and simulatio ns.\nTo re-design a modular software stack with native kernels\nto effectively utilize scale-up servers for machine learni ng\nand data analytics applications. We are investigating how\nsimulations and Big Data can use common programming\nenvironments with a runtime based on a rich set of collective s\nand libraries for a model-centric approach [40], [41].\nParallel Computing: We know that heterogeneity can\nlead to difﬁculty in parallel computing. This is extreme for\nMLaroundHPC as the ML learnt result can be huge factors\n(105in our initial example[26]) faster than simulated answers.\nFurther learning can be dynamic within a job and within\ndifferent runs of a given job. One can address by load\nbalancing the unlearnt and learnt separately but this can le ad\nto geometric issues as quite likely that ML learning works\nmore efﬁciently (for more potential simulations) in partic ular\nregions of phase space.\nB. Uncertainty Quantiﬁcation for Deep Learning\nAn important aspect of the use of a learned ML model is\nthat one must learn not just the result of a simulation but\nalso the uncertainty of the prediction e.g. if the learned re sult\nis valid enough to be used. This can be explained in the\nsense of the bias-variance trade-off, which is based on the\ndecomposition of the expected error into two parts: varianc e\nand bias. The variance part explains the uncertainty of the\nmodel training process due to the randomness in the training\nalgorithms or the lack of representativeness of the trainin g set.\nA regularization scheme can reduce the variance so that the\nmodel complexity is in control and can result in a smoother\nmodel. However, the regularization approach comes at the co st\nof an increased amount of bias, which is another term in\nthe expected error decomposition that explains the ﬁtness o f\nthe model—by regularizing the model the training algorithm\ncan do only a limited effort to minimize the training error.\nOn the contrary, an unregularized model with a higher model\ncomplexity than necessary can also result in a minimal train ing\nerror, while it suffers from high variance.\nIdeally, the bias-variance trade-off can be resolved to som e\ndegree by averaging trained instances of an originally comp lex\nmodel. Once these model instances are complex enough to ﬁt\n\nthe training data set, we can use the averaged predictions as\nthe output of the model. However, averaging many different\nmodel instances implies a practical difﬁculty that one has t o\nconduct multiple optimization tasks to secure a statistica lly\nmeaningful sample distribution of the predictions. Given t he\nassumption that the model might as well be a complex one\nto minimize the bias component (e.g. a deep neural network),\nthe model averaging strategy is computationally challengi ng.\nDropout has been extensively used in deep learning as a\nregularization technique [42], but recent researches revi sit it\nas an uncertainty quantiﬁcation (UQ) tool [43]. The dropout\nprocedure can be seen as an efﬁcient way to maintain a\npool of multiple network instances for the same optimizatio n\ntask. It is an efﬁcient ensemble technique as it applies a\nrandomly sampled Bernoulli mask to a layer-wise input unit,\nthus exposing the optimization process to many differently\nstructured instances of the network.\nA a set of differently thinned versions of the network can\nform a sample distribution of predictions to be used as a\nUQ metric. The dropout-based UQ scheme can provide an\nopportunity for the MLaroundHPC simulation experiments.\nAs a data-driven model it is reasonable to assume that a\nbetter ML surrogate can be found once the training routine\nsees more examples generated from the simulation experimen t.\nHowever, creating more examples to train a better ML model\nis a conﬂicting requirement as the purpose of training the ML\nsurrogate is to avoid such computation. The UQ scheme can\nplay a role here to provide the training routine with a way\nto quantify the uncertainty in the prediction—once it is low\nenough, the training routine might less likely need more dat a.\nC. Machine Learning for HPC\nHere we review the nature of the Machine Learning needed\nfor MLforHPC in different application domains. The Machine\nLearning (ML) load depends on 1) Time interval between its\ninvocations, which will translate into the number of traini ng\nsamples S and 2) size D of data set specifying each sample.\nThis size could be as large as the number of degrees of\nfreedom in simulation or could be (much) smaller if just a\nfew parameters are needed to deﬁne simulation. We note two\ngeneral issues\n•There can very important data transfer and storage issues\nin linking the Simulations and Machine Learning parts of\nsystem. This could need carefully designed architectures\nfor both hardware and software.\n•The Simulations and Machine Learning subsystems are\nlikely to require different node optimizations as in differ -\nent types and uses of accelerators.\nD. Science Exemplar: Nanosimulations\nIn this subsection, using the example of Nanosimulations,\nwe show progress in all areas at the intersection of HPC and\nML are having an impact.\nIn each of two cases below, one is using scikit-learn,\nTensorﬂow and the Keras wrapper for Tensorﬂow, as the MLsubsystem. The papers [26], [9] are using ML to learn results\n(ionic density at a given location) of a complete simulation\n•D=5 with the ﬁve specifying features as conﬁnement\nlength h, positive valency zp, negative valency zn, salt\nconcentration c, and the diameter of the ions d.\n•S= 4805 which 70% of total 6864 runs with 30% of the\ntotal runs used for testing.\nIn [9], one is not asking ML to predict a result as in [26],but\nrather training an Artiﬁcial Neural Net (ANN) to ensure that\nthe simulation runs at its optimal speed (using for example,\nthe lowest allowable timestep dt and ”good” simulation cont rol\nparameters for high efﬁciency) while retaining the accurac y of\nthe ﬁnal result (e.g. density proﬁle of ions). For this parti cular\napplication, we could get away by dividing a 10 million time-\nstep run ( 10 nanoseconds that is a typical timescale to reach\nequilibrium and get data in such systems) into 10 separate\nruns.\n•Input data size D= 6 (1 input uses 64 bits ﬂoats and 5\ninputs use 32 bits integers - total 224 bits)\n•Input number of samples (S) = 15640 (70% training 30%\ntest)\n•Hidden layer 1 = 30\n•Hidden layer 1 = 48\n•Output variables = 3\nCreation of the training dataset took = 64 cores * 80 hrs *\n5400 simulation runs = 28160000 or 28 million CPU hours\non Indiana University’s BigRed2 GPU compute nodes. Each\nrun is 10 million steps long, and you use/learn/train ML ever y\n1 million steps (so block size is a million), yielding 10 time s\nmore samples than runs.\nGeneralizing this, the hardware needs will depend on how\noften you block, to stop and train the network, and then eithe r\non-the-ﬂy or post-simulation, use that training to acceler ate\nsimulation or evaluate structure respectively. Blocking e very\ntimestep will not improve the training as typically, it won’ t\nproduce a statistically independent data point to evaluate any\nstructure you desire. So you want to block at a timescale that\nis at least greater than the autocorrelation time dc; this is , of\ncourse, dependent on example you are looking at – and so\nyour blocking and learning will depend on the application. I n\n[26], it is small and dc is 3-5 dt; in glasses, it can be huge\nas the viscosity is high; and in biomolecular simulations, i t\nwill also depend on the level of coarse-graining and will be\ndifferent in fully atomistic or very coarse-grained system s.\nThe training effort will also depend on the input data size\nD, and the complexity of the relationship you are trying to\nlearn which change the number of hidden layers and nodes\nper layer. For example, suppose you are tracking a particle ( a\nside atom on a molecule in a typical nanoscale simulation),\nin order to come up with a metric (e.g. distance between two\nside atoms on different molecules) to track the diversity of\nclusters of particles during the self-assembly process. Th is\ncomes from expectation that correlations between side atom s\nmay be critical to a macroscopic property (such as formation\nof these particles into a FCC crystal). In this case your D is\n\nhuge, and your ML objectives may be looking for a deep\nrelationship, and you may have to invoke an ensemble of\nANN’s and this will change hardware needs.\nScaling of Effective Performance: An initial approach\nto estimate speedup in a hybrid MLaroundHPC situation is\ngiven in [26] for a nano simulation. One can estimate the\nspeedup in terms in terms of four times Tseqthe sequential\nexecution time of simulation; Ttrain the time for the parallel\nexecution of simulation to give training data; Tlearn is the\ntime per sample to train the learning networkl; and Tlookup\nis the inference time to predict the results of the simulatio n\nby using the trained network. In the formula below, Nlookup\nis the number of trained neural net inferences and Ntrain the\nnumber of parallel simulations used in training.\nEffectiveSpeedupS =Tseq(Nlookup+Ntrain)\nTlookupNlookup+(Ttrain+Tlearn)Ntrain\nThis formula reduces to the classic simpleTseq\nTtrainwhen there is\nno machine learning and in the limit of largeNlookup\nNtrainbecomes\nTseq\nTlookupwhich can be huge!\nThere are many caveats and assumptions here. We are\nconsidering a simple case where one runs the Ntrain sim-\nulations, followed by the learning and then all the Nlookup\ninferences. Further we assume the training simulations are\nuseful results and not just overhead. We also have not proper ly\nconsidered how to build in the likelihood that training, lea rning\nand lookup phases are probably using different hardware\nconﬁgurations with different node counts.\nE. Opportunities and Research Issues\nResearch Issues: In addition to the six categories at the\ninterface of ML and HPC, the research issues we identify\nreﬂect the multiple interdisciplinary activities linked i n our\nstudy of MLforHPC, including application domains describe d\nin sections II-A, II-B, II-C1 and II-C2, as well as coarse\ngraining studied in our case for network science and nano-\nbio areas.\nWe have identiﬁed the following research areas, which can\nbe categorized into Algorithms and Methods (1-5), Applied\nMath (10), Software Systems (6,7), Performance Measuremen t\nand Engineering (8,11).\n1) Where can application domains use MLaroundHPC and\nMLautotuning effectively and what science is enabled\nby this\n2) Which ML and DL approaches are most relevant and\nhow can they be set up to enable broad user-friendly\nMLaroundHPC and MLautotuning in domain science\n3) How can Uncertainty Quantiﬁcation be enabled and\nseparately study ergodicity (bias) and accuracy issues?\n4) Is there new area of algorithmic research focusing on\nﬁnding algorithms that can be most effectively learnt?\n5) Is there a general multiscale approach using\nMLaroundHPC.\n6) What are appropriate systems frameworks for\nMLaroundHPC and MLautotuning. For example,should we wrap microservices invoked by a Function\nas a Service environment? Where and how should we\nenable learning systems? Is Dataﬂow useful?\n7) The different characters of surrogate and real execution s\nproduce system challenges as surrogate execution is\nmuch faster and invokes distinct software and hardware.\nThis heterogeneity gives challenges for parallel com-\nputing, workload management and resource scheduling\n(heterogeneous and dynamic workﬂows). The implica-\ntion for performance is brieﬂy discussed in sections\nIII-A and III-D.\n8) Scaling applications that are composed of multiple het-\nerogeneous computational (execution) units, and have\ndistinct forms of parallelism that need balanced perfor-\nmance. Consider a workload comprised of NLlearn-\ning units, NSsimulations units. The relative number\nof learning units to simulation units will vary with\napplication and problem type. The relative values will\neven vary over execution time of the application, as the\namount of data generated as a ratio of training data will\nvary. This requires runtime systems that are capable of\nreal-time performance tuning and adaptive execution for\nworkloads comprised of multiple heterogeneous tasks.\n9) The application of these ideas to statistical physics\nproblems may need different techniques than those used\nin deterministic time evolutions.\n10) The existing UQ frameworks based on the dropout tech-\nnique can provide the level of certainty as a probabilistic\ndistribution in the prediction space. However, it does\nnot always mean that the quality of the distribution is\ndependent on the quality/quantity of data. For example,\ntwo models with different dropout rates can produce\ndifferent UQ results. If the goal of UQ in MLaroundHPC\ncontext is to supply only an adequate amount of data,\nwe need a more reliable UQ method tailored for this\npurpose rather than the dropout technique that tends to\nmanipulate the architecture of the model.\n11) Application agnostic description and deﬁntion of effec -\ntive performance enhancement.\nCONCLUSIONS\nBroken Abstractions, New Abstractions: In traditional\nHPC the prevailing orthodoxy is Faster is Better has driven\nthe quest for abstractions of hierarchical parallelism to s peed-\ning up single units of works. Relinquishing the orthodoxy\nbased upon hierarchical (vertical) parallelism as the only\nroute to performance is necessary. The new paradigm in HPC\n— Learning Everywhere, implies new performance, scaling\nand execution approaches. In this new paradigm, multiple,\nconcurrent heterogeneous units of work replace single larg e\nunits of works, which thus require both hierarchical (verti cal)\nparallelism as well horizontal (many task) parallelism.\nACKNOWLEDGMENTS\nThis work was partially supported by NSF CIF21 DIBBS\n1443054 and nanoBIO 1720625; the Indiana University Preci-\n\nsion Health initiative and Intel through the Parallel Compu ting\nCenter at Indiana University. JPS and JAG were partially\nsupported by NSF 1720625, NIH U01 GM111243 and NIH\nGM122424. SJ was partially supported by ExaLearn – a DOE\nExascale Computing project.\nREFERENCES\n[1] Geoffrey Fox, James A. Glazier, JCS Kadupitiya, Vikram J adhao,\nMinje Kim, Judy Qiu, James P. Sluka, Endre Somogyi, Madhav\nMarathe, Abhijin Adiga, Jiangzhuo Chen, Oliver Beckstein, and\nShantenu Jha. Learning Everywhere: Pervasive machine lear n-\ning for effective High-Performance computation: Applicat ion back-\nground. Technical report, Indiana University, February 20 19.\nhttp://dsc.soic.indiana.edu/publications/Learning Everywhere.pdf.\n[2] Geoffrey Fox, Judy Qiu, Shantenu Jha, Saliya Ekanayake, and Supun\nKamburugamuve. Big data, simulations and HPC convergence. In\nSpringer Lecture Notes in Computer Science LNCS 10044 , 2016.\n[3] Peter M Kasson and Shantenu Jha. Adaptive ensemble simul ations\nof biomolecules. Current Opinion in Structural Biology , 52:87 – 94,\n2018. Cryo electron microscopy: the impact of the cryo-EM re volution\nin biology Biophysical and computational methods - Part A.\n[4] NSF1849625 workshop series BDEC2: Toward a common digit al con-\ntinuum platform for big data and extreme-scale computing (B DEC2).\nhttps://www.exascale.org/bdec/.\n[5] Jos´ e Miguel Hern´ andez-Lobato, Klaus-Robert M¨ uller , Brooks Paige,\nMatt J. Kusner, Stefan Chmiela, Kristof T. Sch¨ utt. Machine learning for\nmolecules and materials. In Proceedings of the NeurIPS 2018 Workshop ,\nDecember 2018.\n[6] Oliver Beckstein,Geoffrey Fox, Judy Qiu, David Crandal l, Gregor von\nLaszewski, John Paden, Shantenu Jha, Fusheng Wang, Madhav M arathe,\nAnil Vullikanti, Thomas Cheatham. Contributions to High-P erformance\nbig data computing. Technical report, Digital Science Cent er, September\n2018.\n[7] Jeff Dean. Machine learning for systems and systems for m achine\nlearning. In Presentation at 2017 Conference on Neural Information\nProcessing Systems , 2017.\n[8] Satoshi Matsuoka. Post-K: A game changing supercompute r for con-\nvergence of HPC and big data / AI. Multicore 2019, February 20 19.\n[9] JCS Kadupitiya, Geoffrey C. Fox, Vikram Jadhao. Machine learning\nfor parameter auto-tuning in molecular dynamics simulatio ns: Efﬁcient\ndynamics of ions near polarizable nanoparticles. Technica l report,\nIndiana University, November 2018.\n[10] Microsoft Research. AI for database and data analytic s ystems at\nmicrosoft faculty summit. https://youtu.be/Tkl6ERLWAbA , 2018. Ac-\ncessed: 2019-1-29.\n[11] Microsoft Research. AI for AI systems at microsoft facu lty summit.\nhttps://youtu.be/MqBOuoLﬂpU, 2018. Accessed: 2019-1-29 .\n[12] Francis J. Alexander, Shantenu Jha. Objective driven c omputational\nexperiment design: An ExaLearn perspective. In Terry Moore , Geoffrey\nFox, editor, Online Resource for Big Data and Extreme-Scale Computing\nWorkshop , November 2018.\n[13] Maziar Raissi, Paris Perdikaris, and George Em Karniad akis. Physics\ninformed deep learning (part i): Data-driven solutions of n onlinear partial\ndifferential equations. arXiv , November 2017.\n[14] Mustafa Mustafa, Deborah Bard, Wahid Bhimji, Rami Al-R fou, and\nZarija Luki. Creating virtual universes using generative a dversarial\nnetworks. Technical report, Lawrence Berkeley National La boratory,\nBerkeley, CA 94720, 06 2017.\n[15] Shing Chan and Ahmed H Elsheikh. A machine learning appr oach for\nefﬁcient uncertainty quantiﬁcation using multiscale meth ods. J. Comput.\nPhys. , 354:493–511, February 2018.\n[16] A Townsend Peterson, Monica Pape \\cs, and Jorge Sober´ on. Mechanistic\nand correlative models of ecological niches. European Journal of\nEcology , 1(2):28–38, 2015.\n[17] Anuj Karpatne, Gowtham Atluri, James H. Faghmous, Mich ael Stein-\nbach, Arindam Banerjee, Auroop Ganguly, Shashi Shekhar, Na giza\nSamatova, and Vipin Kumar. Theory-guided data science: A ne w\nparadigm for scientiﬁc discovery from data. IEEE Transactions on\nKnowledge and Data Engineering , 29(10):2318–2331, 2017.\n[18] Mark EJ Newman. Spread of epidemic disease on networks. Physical\nreview E , 66(1):016128, 2002.[19] Lijing Wang, Jiangzhuo Chen, and Madhav Marathe. DEFSI : Deep\nlearning based epidemic forecasting with synthetic inform ation. In\nProceedings of the 30th innovative Applications of Artiﬁci al Intelligence\n(IAAI) , 2019.\n[20] James M Osborne, Alexander G Fletcher, Joe M Pitt-Franc is, Philip K\nMaini, and David J Gavaghan. Comparing individual-based ap proaches\nto modelling the self-organization of multicellular tissu es.PLoS Comput.\nBiol. , 13(2):e1005387, February 2017.\n[21] James P. Sluka, Xiao Fu, Maciej Swat, Julio M. Belmonte, Alin\nCosmanescu, Sherry G. Clendenon, John F. Wambaugh, and Jame s A.\nGlazier. A liver-centric multiscale modeling framework fo r xenobiotics.\nPLoS ONE , 11(9), 2016.\n[22] Qianxiao Li, Felix Dietrich, Erik M. Bollt, and Ioannis G. Kevrekidis.\nExtended dynamic mode decomposition with dictionary learn ing: A\ndata-driven adaptive spectral decomposition of the koopma n oper-\nator. Chaos: An Interdisciplinary Journal of Nonlinear Science ,\n27(10):103111, 2017.\n[23] Adam A Margolin, Ilya Nemenman, Katia Basso, Chris Wigg ins,\nGustavo Stolovitzky, Riccardo Dalla Favera, and Andrea Cal ifano.\nARACNE: an algorithm for the reconstruction of gene regulat ory\nnetworks in a mammalian cellular context. BMC Bioinformatics , 7 Suppl\n1:S7, March 2006.\n[24] Liang Liang, Minliang Liu, Caitlin Martin, John A Eleft eriades, and\nWei Sun. A machine learning approach to investigate the rela tionship\nbetween shape features and numerically predicted risk of as cending\naortic aneurysm. Biomech. Model. Mechanobiol. , 16(5):1519–1533,\nOctober 2017.\n[25] Jos W. Zwanikken and Monica Olvera de la Cruz. Tunable so ft structure\nin charged ﬂuids conﬁned by dielectric interfaces. Proceedings of the\nNational Academy of Sciences , 110(14):5301–5308, 2013.\n[26] JCS Kadupitiya , Geoffrey C. Fox , and Vikram Jadhao. Mac hine learn-\ning for performance enhancement of molecular dynamics simu lations.\nTechnical report, Indiana University, December 2018.\n[27] Adri` a P´ erez, Gerard Mart´ ınez-Rosell, and Gianni De Fabritiis. Simula-\ntions meet machine learning in structural biology. Current Opinion in\nStructural Biology , 49:139 – 144, 2018.\n[28] Keith T Butler, Daniel W Davies, Hugh Cartwright, Olexa ndr Isayev,\nand Aron Walsh. Machine learning for molecular and material s science.\nNature , 559(7715):547–555, July 2018.\n[29] Stefano Piana, John L Klepeis, and David E Shaw. Assessi ng the accu-\nracy of physical models used in protein-folding simulation s: quantitative\nevidence from long molecular dynamics simulations. Curr Opin Struct\nBiol, 24:98–105, Feb 2014.\n[30] Jrg Behler and Michele Parrinello. Generalized Neural -Network Rep-\nresentation of High-Dimensional Potential-Energy Surfac es. Physical\nReview Letters , 98(14):146401, April 2007.\n[31] Jrg Behler. First Principles Neural Network Potential s for Reactive\nSimulations of Large Molecular and Condensed Systems. Angewandte\nChemie International Edition , 56(42):12828–12840, 2017.\n[32] Michael Gastegger, Jrg Behler, and Philipp Marquetand . Machine\nlearning molecular dynamics for the simulation of infrared spectra.\nChemical Science , 8(10):6924–6935, 2017.\n[33] J. S.Smith, O. Isayev, and A. E.Roitberg. ANI-1: an exte nsible neural\nnetwork potential with DFT accuracy at force ﬁeld computati onal cost.\nChemical Science , 8(4):3192–3203, 2017.\n[34] Justin S. Smith, Ben Nebgen, Nicholas Lubbers, Olexand r Isayev, and\nAdrian E. Roitberg. Less is more: Sampling chemical space wi th active\nlearning. The Journal of Chemical Physics , 148(24):241733, May 2018.\n[35] Jiang Wang, Christoph Wehmeyer, Frank No´ e, , and Cecil ia Clementi.\nMachine learning of coarse-grained molecular dynamics for ce ﬁelds.\narXiv , 1812.01736v2, 2018.\n[36] Pedro E. M. Lopes, Olgun Guvench, and Alexander D. MacKe rell. Cur-\nrent Status of Protein Force Fields for Molecular Dynamics S imulations ,\npages 47–71. Springer New York, New York, NY , 2015.\n[37] MLPERF benchmark suite for measuring performance of ML soft-\nware frameworks, ML hardware accelerators, and ML cloud pla tforms.\nhttps://mlperf.org/. Accessed: 2019-2-8.\n[38] Uber Engineering. Horovod: Uber’s open source distrib uted deep\nlearning framework for TensorFlow. https://eng.uber.com /horovod/.\nAccessed: 2019-2-8.\n[39] Intel R/circlecopyrtparallel computing center at indiana university led by Judy Qiu.\nhttp://ipcc.soic.iu.edu/. Accessed: 2018-9-30.\n[40] Bingjing Zhang, Bo Peng, and Judy Qiu. Parallelizing bi g data machine\nlearning applications with model rotation. In Fox, G., Geto v, V .,\n\nGrandinetti, L., Joubert, G.R., Sterling, T., editor, Advances in Parallel\nComputing: New Frontiers in High Performance Computing and Big\nData . IOS Press, 2017.\n[41] Intel parallel universe issue 32 page 31: Judy Qiu,\nHarp-DAAL for High-Performance big data computing.\nhttps://software.intel.com/sites/default/ﬁles/paral lel-universe-issue-32.pdf,http://dsc.soic.indiana.e du/publications/Intel-Magazine-HarpDAAL10.pdf.\nAccessed: 2018-9-30.\n[42] George E Dahl, Tara N Sainath, and Geoffrey E Hinton. Imp roving\ndeep neural networks for lvcsr using rectiﬁed linear units a nd dropout.\nInAcoustics, Speech and Signal Processing (ICASSP), 2013 IEE E\nInternational Conference on , pages 8609–8613. IEEE, 2013.\n[43] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian a pproxima-\ntion: Representing model uncertainty in deep learning. In international\nconference on machine learning , pages 1050–1059, 2016.",
  "textLength": 51039
}