{
  "paperId": "447e5b78ce9064d057f30210e592036e566ada02",
  "title": "VIP Hashing - Adapting to Skew in Popularity of Data on the Fly",
  "pdfPath": "447e5b78ce9064d057f30210e592036e566ada02.pdf",
  "text": "VIP Hashing - Adapting to Skew in Popularity of Data on\nthe Fly (extended version)\nAarati Kakaraparthy Jignesh M. Patel\nUniversity of Wisconsin, Madison\nfaaratik, jignesh g@cs.wisc.eduBrian P . Kroth Kwanghyun Park\nMicrosoft Gray Systems Lab\nfbpkroth, kwpark g@microsoft.com\nABSTRACT\nAll data is not equally popular. Often, some portion of data\nis more frequently accessed than the rest, which causes a\nskew in popularity of the data items. Adapting to this skew\ncan improve performance, and this topic has been studied\nextensively in the past for disk-based settings. In this work,\nwe consider an in-memory data structure, namely hash table ,\nand show how one can leverage the skew in popularity for\nhigher performance.\nHashing is a low-latency operation, sensitive to the e\u000bects\nof caching, branch prediction, and code complexity among\nother factors. These factors make learning in-the-loop espe-\ncially challenging as the overhead of performing any addi-\ntional operations can be signi\fcant. In this paper, we pro-\npose VIP hashing, a fully online hash table method, that\nuses lightweight mechanisms for learning the skew in pop-\nularity and adapting the hash table layout. These mecha-\nnisms are non-blocking, and their overhead is controlled by\nsensing changes in the popularity distribution to dynami-\ncally switch-on/o\u000b the learning mechanism as needed.\nWe tested VIP hashing against a variety of workloads\ngenerated by Wiscer , a homegrown hashing measurement\ntool, and \fnd that it improves performance in the presence\nof skew (22% increase in fetch operation throughput for a\nhash table with one million keys under low skew, 77% in-\ncrease under medium skew) while being robust to insert and\ndelete operations, and changing popularity distribution of\nkeys. We \fnd that VIP hashing reduces the end-to-end exe-\ncution time of TPC-H query 9, which is the most expensive\nTPC-H query, by 20% under medium skew.\n1. INTRODUCTION\nHash tables are widely used data structures with a sim-\nple point lookup interface { mapping a key to a value. In\ndatabase systems, they are used for in-memory indexing and\nalso in query processing operations such as hash joins and\naggregation. The lightweight computation involved and the\nconstant time lookup guarantees are two reasons that en-\nable hash tables to achieve high throughput when processing\npoint queries.\nHowever, not all keys contribute equally to the perfor-\nmance, and requests are often skewed towards a smaller\nset of \\hot\" keys. In multiple studies involving production\nworkloads, fetch requests have been observed to follow the\npower law [17,26,30] where the popularity of keys exponen-\ntially decays with the rank. The Very Important key-value\nPairs (VIPs) are the keys with lower rank, as they constitute\na larger portion of requests and have a greater impact on the\n(a)Default con\fguration: VIPs at ran-\ndom spots\n(b) VIP con\fguration: VIPs at the\nfront\nFigure 1: Hash Table con\fgurations with VIP keys\n(in yellow) at (a) random spots, vs. (b) at the front.\nThe throughput of the hash table can be improved\nby giving VIPs more favorable spots in the front.\nthroughput. It is possible to further improve the through-\nput obtained from the hash table by leveraging this skew in\npopularity, as we show in our work.\nFig. 1 shows the core motivation behind VIP Hashing {\ngiving more favorable spots to more popular keys. In the\nVIP con\fguration (Fig. 1b), the keys are ordered in de-\nscending order of popularity and the VIPs are in the front,\nanalogous to seating VIPs in the front row for an event. By\nplacing the popular keys at the start, they can be accessed\nfaster due to multiple reasons such as fewer memory accesses\nand lesser computation (discussed in §4), which improves the\noverall throughput obtained from the hash table.\nWhile attaining the VIP con\fguration is straightforward\nif the popularity of keys is known in advance (keys can be\ninserted in the right position in the chain according to their\npopularity), one might not have this information up front.\nAlso, the popularity of the keys can change over time re-\nsulting in a di\u000berent set of VIPs. Thus, more generally, one\nneeds to learn the popularity of keys and adapt on the \ry.\nIt is important to note that learning requires some amount\nof computation and storage. In case of disk-based data\nstructures, this overhead can be relatively small compared\nto the high latency of accessing storage devices. However,\nthis is not true for hash tables which are typically resident in\nmemory and involve lightweight computation. Even adding\na small counter per entry in the hash table can degrade per-arXiv:2206.12380v1  [cs.DB]  24 Jun 2022\n\nformance considerably, as we show in §5.1. Thus, the learn-\ning mechanisms need to be designed keeping the overhead\nin check compared to the gains.\nOur contributions in this paper are as follows {\n1.Wiscer (§3) { We developed a con\fgurable tool for mea-\nsuring the performance of hash tables. Wiscer can be\nused to generate workloads with varying levels of skew in\npopularity, with di\u000berent ratios of fetch, insert and delete\noperations, and shifting hot set of keys over time. To our\nknowledge, no existing benchmarking tool captures all of\nthis behavior in one place.\n2.Roo\rine Analysis of the VIP con\fguration (§4) {\nWe study the bene\ft of the VIP con\fguration (Fig. 1b)\ngiven prior knowledge of popularity. Since there is no\noverhead of learning, this analysis shows the maximum\ngain one can obtain from adapting to the skew (for a hash\ntable with 10M keys at load factor 0.6, we observe a 57%\nincrease in throughput from the VIP con\fguration in the\nbest case).\n3.Learning on a budget (§5) { We developed lightweight\nmechanisms for learning the popularity distribution on\nthe \ry, adapting to the skew, sensing changes in the\npopularity distribution, and dynamically switching on/o\u000b\nlearning to control the overhead. Put together, they give\nus the VIP Hashing method for learning the skew in pop-\nularity on the \ry.\n4.Application to hash joins (§6.1) { We study the ap-\nplication of VIP hashing to PK-FK hash joins, and we\nobtain a 13-23% reduction in canonical join query execu-\ntion time (for a cardinality ratio of 1:16 in the relations\nand a hash table with load factor of 1.4). We imple-\nmented VIP hashing in DuckDB [41] to speed up PK-FK\nhash joins in single-threaded mode, and we obtain a net\nreduction of 20% in end-to-end execution time of TPC-H\nquery 9 [20] under low and medium skew.\n5.Application to point queries (§6.2) { Another common\nuse of hash tables is processing point queries. We test\nVIP hashing at a load factor of 0.95 under a variety of\nworkloads involving insert and delete operations, shifting\npopularity distribution, di\u000berent rates of shift, etc. A\ngain in throughput of 22% (77%) is obtained under low\n(medium) skew, while our choice of parameters ensures\nthat the loss due to the overhead of learning is capped in\nthe worst case.\nOverall, our experiments in §6 show that the VIP hash-\ning is a fully online non-blocking learning method that cap-\ntures the skew in popularity on the \ry, while being robust\nto inserts, deletes, and shifting popularity distribution. We\ndiscuss related work in §7 and conclude in §8.\n2. BACKGROUND\n2.1 Hash Tables\nA hash table [5] is an associative data structure that maps\nkeys to values. In our work, we focus on chained hashing\n(hereafter referred to as hash table). A hash table (Fig. 1)\nuses a hash function to map each key to a unique index\norbucket . Since more than one key can be mapped to the\nsame bucket, the data structure resolves these collisions by\nmaintaining a chain (linked list) of entries belonging to the\nFigure 2: Popularity distribution of keys (number\nof keysN= 100 ) for di\u000berent Zip\fan skew factors s.\nbucket. The \rexibility provided by this data structure for\nperforming insert and delete operations, along with variable\nlength keys and values make it a popular choice in many\ndata systems [3,4,18,21].\n2.1.1 On Properly Conﬁguring the Hash Table\nIn this paper, we focus on hashing of 8-byte integer keys\nand values, which is a well studied problem in past re-\nsearch [28, 42]. It is important to con\fgure the hash ta-\nble correctly to draw reliable conclusions, and there are two\nimportant factors to consider. The \frst is the choice of\nthe hash function. In our work, we use MurmurHash [15],\nwhich is a strong hash function that provides good collision\nresistance in practice. The second critical aspect is the load\nfactor , which is the ratio of keys to the number of buckets\nin the hash table. Higher load factors correspond to fewer\nbuckets, which lead to longer chains on an average, whereas\nlower load factors require more buckets and consume more\nmemory. Informed by parameter choices in popular open-\nsource systems [3,7,21], we maintain a load factor between\n0:5 and 1:5 to ensure that collisions are at an acceptable level\nwhile utilizing memory e\u000eciently. Wherever applicable, we\nrehash the hash table to maintain this range of load factor.\nThe number of buckets in the hash table are set to be a\npower of two, which is a common choice [1,7,21] that speeds\nup the computation of the hash function. If the load factor\nexceeds 1.5 (falls under 0.5), we double (half) the number\nof buckets in the hash table.\n2.2 Some Probability Bounds and Theorems\nBelow we discuss some tools related to probabilistic ran-\ndom variables that we use in our work.\n\u000fZip\fan distribution : We use Zip\fan distribution [25]\nto model varying levels of skew in fetch operations is-\nsued to keys in a hash table. Zip\fan distribution has\nbeen adopted by multiple studies in the past [26,28,44] to\nstatistically model skew in popularity, as it captures the\npower law [17] characteristics of workloads that are often\nobserved in practice [26,30].\n\u000fEstimating mean and variance : LetXbe a random\nvariable with mean \u0016and variance \u001b2. LetX1; X2;:::Xn\nbenindependent and identically distributed (i.i.d.) mea-\nsurements of X. The estimated mean ^ \u0016and estimated\nvariance ^\u001b2can be evaluated as\n^\u0016=nP\ni=1Xi\nn;^\u001b2= nP\ni=1X2\ni\nn\u00001\u0000\u0010nP\ni=1Xi\u00112\nn(n\u00001)!\n\nTable 1: Con\fguration options supported by Wiscer\nOption Description\nzipf The zip\fan factor of the popularity distribution. zipf= 0 corresponds to uniform popularity.\ninitialSize Initial number of keys in the hash table before running any operations.\noperationCount Total number of operations (fetch, inserts, etc.) to run on the hash table.\n(fetch/insert/delete)\nProportionProportion of operations that are fetch/insert/delete.\ndistShiftFreq A shift in popularity distribution occurs after every distShiftFreq operations.\ndistShiftPrct The popularity distribution shifts by distShiftPrct % every distShiftFreq operations.\nstorageEngineWhich storage engine to benchmark. Options are\nChainedHashing (default), VIPHashing , and none (store workload to disk).\nkeyPattern The pattern of keys to generate { random (default) or sequential (1 ton).\nkeyOrderThe popularity rank of keys relative to the insertion order. Options are\nrandom (default) and sorted (where keys are inserted in increasing order of popularity; a.k.a. latest).\nrandomSeedThe seed value (unsigned integer) to initialize the random number generator (default = 0).\nThe random number generator is used to populate the hash table and generate the workload.\nDi\u000berent seed values result in di\u000berent instances of keys and the workload.\n\u000fGaussian tail bound con\fdence interval : For a ran-\ndom variable X(refer above), the central limit theorem\n(CLT) [13] states that the error in estimated mean (^ \u0016\u0000\u0016)\nis approximately Gaussian distributed N(0;\u001b2=n). By ap-\nplying the Gaussian pdf, a con\fdence interval can be ob-\ntained for the error (^ \u0016\u0000\u0016) as follows\nP(j^\u0016\u0000\u0016j\u0014t)\u0015\u0012\n1\u0000exp\u0012\u0000nt2\n2\u001b2\u0013\u0013\n=L\n100\nThus, we can at least be L% con\fdent that the error j^\u0016\u0000\u0016j\nis less than t. Note that the con\fdence increases exponen-\ntially withn(number of samples Xidrawn). It is impor-\ntant to note that (^ \u0016\u0000\u0016) is only approximately Gaussian,\nso the con\fdence interval obtained from applying Gaus-\nsian tail bound is a heuristic.\n3. SKEWED WORKLOAD GENERATION\nWITH WISCER\n3.1 Overview\nWiscer [22] is a workload generation tool that we propose\nin this paper. Wiscer has multiple con\fguration options (Ta-\nble 1) that can be used to generate workloads with di\u000berent\nlevels of skew, varying proportions of fetch, insert, delete op-\nerations, di\u000berent rates of popularity shift, etc. Below are\nsome key features of Wiscer:\n\u000fLevel of skew : Increasing levels of skew in the popu-\nlarity distribution can be simulated by increasing the zipf\nfactor. For instance, zipf = 0 andzipf = 4 correspond\nto uniform distribution and very high skew respectively\n(see Fig. 2).\n\u000fSimulating popularity distribution shift : The two\nrelated con\fguration options are distShiftFreq and dist-\nShiftPrct . After every distShiftFreq fetch operations, the\ntopmost popular keys that constitute distShiftPrct of the\nrequests are randomly replaced by less popular keys. This\nsimulates a behavior where keys in the hot set become less\npopular after some time, which has also been observed in\nsome real-world workloads [26].\u000fBenchmarking hash table implementations : Wiscer\ncan optionally be used to compare di\u000berent hash table im-\nplementations (option StorageEngine ) to directly process\nthe generated workloads without intermediate storage.\n\u000fFine-grained performance metrics using hardware\ncounters : When using Wiscer for benchmarking, oper-\nations are issued to the con\fgured hash table in batches\nof one million requests at a time, and \fne-grained met-\nrics are collected per batch. Wiscer uses hardware coun-\nters provided by the Intel's Performance Monitoring Unit\n(PMU) [9] to get low-level performance metrics such as\ncache misses, number of cycles, retired instructions, etc.\n3.2 Experimental Conﬁguration\nAll experiments in this paper are run on a Cloudlab [35]\nmachine with two 10-core Intel Xeon Silver 4114 CPUs with\na peak frequency of 3.0GHz. The benchmarking process is\npinned to a single core to avoid any overhead of context\nswitching. The CPU scaling governor of the core has been\nset to performance , thus \fxing the frequency to 3.0GHz at\nall times. The CPU has an L3 cache of 13.75MB, and the\nserver machine has 192GB of RAM. This CPU belongs to\nthe Skylake Intel architecture family [11], and the PMU's\nhardware counters are programmed accordingly. The server\nmachine is used exclusively for running Wiscer to mitigate\ninterference from any concurrent processes.\n4. ROOFLINE STUDY\nIn this section, we compare the performance of the Default\nand VIP con\fgurations when the popularity of keys is static\nand known in advance. Since there is no overhead of learning\ninvolved in this case, this roo\rine study shows the maximum\ngain one can get from the VIP con\fguration for di\u000berent\nlevels of skew ( §4.2) in popularity at di\u000berent load factors\n(§4.3) of the hash table.\n4.1 Default vs VIP Conﬁguration\n4.1.1 Motivation\nFig. 3 shows an example of processing fetch requests in\nthe Default and the VIP con\fgurations. A key parameter\nto note is the displacement encountered, which is the to-\ntal number of keys that were accessed to process the fetch\n\ndisplacement = 2 \nB A \nHash \nFunction \nC [0]\n[1]\n[2]displacement = 1 \ndisplacement = 3 Fetch Ops \nA \nA \nB \nB \nC \nC ＊\nTotal Displacement = 12 Effective Hot Set (a)Default con\fguration. A total displacement of\n12 (=2\u0002(2+1+3)) is required to process the fetch re-\nquests. The less popular keys in the path of popular\nkeys need to accessed as well.\nA \nB \nC Hash \nFunction [0]\n[1]\n[2]displacement = 1 Fetch Ops \nA \nA \nB \nB \nC \nC ＊\nTotal Displacement = 6 Effective Hot Set displacement = 1 \ndisplacement = 1 \n(b) VIP con\fguration. A total displacement of 6\n(=2\u0002(1+1+1)) is required to process the fetch re-\nquests. Only the popular keys are accessed.\nFigure 3: Processing fetch requests in the Default vs\nthe VIP con\fguration. Unpopular keys have been\ngrayed out. The total displacement (number of keys\naccessed) is higher in the Default con\fguration re-\nquiring more pointer dereferences. Also, the e\u000bec-\ntive hot set is larger, increasing the likelihood of\ncache misses relative to the VIP con\fguration.\nrequests. Accessing a key requires dereferencing a pointer\nand some additional computation. The displacement en-\ncountered in the Default con\fguration is higher as the less\npopular keys in the path to VIPs need to be accessed when\nprocessing the fetch requests and e\u000bectively become part\nof the hot set. A larger hot set increases the likelihood of\ncache misses, and we observe this trend in our experiments\ndescribed next.\n4.1.2 Generating the conﬁgurations using Wiscer\nIn the VIP con\fguration, keys in the hash table are ar-\nranged in descending order of popularity in the bucket chains\n(see Fig. 3b). We attain this con\fguration by running\nWiscer with the default storage engine ( ChainedHashing )\nand inserting keys in increasing order of popularity ( key-\norder=sorted , default is random ). Insert operations on the\nhash table are performed at the front of the bucket chain\n(§2.1). Thus, when inserting keys in the sorted order, entries\nare automatically placed in decreasing order of popularity\nas more popular keys are inserted later and are ahead in the\nbucket chain. The Default con\fguration is generated using\nthe default parameters of Wiscer.\n4.2 Impact of Increasing Skew\n4.2.1 Workload\nWe compare the throughput of fetch operations in the De-\nfault and VIP con\fgurations. We use Wiscer (Table 1) to\ngenerate fetch requests with increasing levels of skew ( zipf\n= 0 to 5 in steps of 0.5) which are issued to a hash table\nwith 10 million keys at a load factor of 0 :6 (= 107=224).\nFor each level of skew and hash table con\fguration, Wisceris run with 10 distinct random seed values to populate the\nhash table and generate the workload. Each random seed\nresults in a di\u000berent arrangement of keys in the hash ta-\nble. The popularity distribution is static, i.e., the rank of\nthe keys remains the same throughout a run. One billion\nfetch requests are issued to the hash table for each random\nseed, and the data points reported in Fig. 4 are the median\nstatistics over the 10 runs. We have run experiments on\nsmaller (1M entries) and larger (100M entries) hash tables\nand found the trends to be similar.\n4.2.2 Results\nThe results of this experiment are shown in Fig. 4. The\ngain in throughput ranges from 9%-57% depending upon the\nlevel of skew in popularity. Below we discuss our takeaways\nfrom the performance metrics measured using Wiscer:\n\u000fThroughput : The gap in performance between the VIP\nand the Default con\fguration increases up to zipf = 2\n(medium skew), and gradually diminishes as the skew be-\ncomes very high ( zipf = 4:5 or 5). This behavior is cor-\nrelated with the hot sets becoming smaller as the skew\nincreases and progressively becoming (L1/2/3) cache res-\nident at di\u000berent rates for the two con\fgurations.\n\u000fDisplacement : As expected, the displacement encountered\nin the VIP con\fguration is lower than the Default (see\nFig. 3). For zipf = 1:5 and up, the total displacement be-\ncomes close to 1B (for 1B fetch requests), indicating that\npopular keys are at the front of their chains (displacement\n= 1) in the VIP con\fguration. For the Default con\fgu-\nration, the median displacement approaches 1B at higher\nlevels of skew ( zipf\u00154), but the variance is high as some\nrandom seeds can result in the popular keys placed further\nin the chains (however the likelihood of this happening is\nlow as the load factor is not very high).\n\u000fInstructions Executed : The instructions executed are lower\nin the VIP con\fguration (up to 6% lower in the best case).\nThe relative trend observed is similar to that of displace-\nment, as the number of instructions executed is correlated\nwith the number of keys accessed.\n\u000fCache misses : The VIP con\fguration becomes L3 and L1\ncache resident (at zipf = 2 and 2:5 respectively) more\nquickly compared to the Default con\fguration (at zipf =\n3:0 and 4:5 respectively), which is expected as the hot set\nof the former is smaller than the latter (Fig. 3). At very\nhigh skew ( zipf = 4:5 and 5), both the con\fgurations are\nL1 resident and correspondingly, we do not observe much\ndi\u000berence in the throughput. This indicates that caching\nhas a big impact on the performance of hash tables.\nOverall, we note that since the hot set of the VIP con\fgu-\nration is smaller than the Default, we encounter lower cache\nmisses at all levels of cache. This contributes to the gain in\nperformance we obtain from the VIP con\fguration.\nAnother important observation we make is that displace-\nment indicates the goodness of the hash table con\fguration.\nThe VIP con\fguration has lower displacement than the De-\nfault in all cases (in fact, the VIP con\fguration has the\nlowest possible displacement for a given data set, hash ta-\nble size, hash function, and request skew; we discuss this in\n§5.2.3). We use this metric in building the mechanisms for\nsensing and dynamically switching-on/o\u000b learning ( §5.2.3).\n\n(a)Fetch operation throughput\n (b)Displacement\n (c)Retired instructions\n(d)L1 cache misses\n (e)L2 cache misses\n (f)L3 cache misses\nFigure 4: Relative performance of the VIP vs the Default con\fgurations as the skew in popularity increases.\nOne billion fetch requests are issued to a hash table with 10M keys (load factor 0:6) for varying levels of\nskew from zipf = 0tozipf = 5. Each reported data point is the median over 10 runs with di\u000berent random\nseeds. Percentage di\u000berence indicated at the top of each plot is the di\u000berence between median metrics of the\nVIP vs the Default con\fguration. The gain in fetch operation throughput varies with skew, and we obtain\n53% increase in throughput for medium skew ( zipf = 2:0). Lesser number of cache misses and instructions\nexecuted contribute to the gain obtained from the VIP con\fguration. Results are discussed in §4.2.2.\n4.3 Impact of Increasing Load Factor\n4.3.1 Workload\nIn this experiment, we increase the load factor while hold-\ning the size of the hash table constant. Similar to §4.2.1,\nwe run one billion fetch operations on a hash table with 224\nbuckets while varying the load factor from 0 :5 to 1:5 in steps\nof 0:25 (this is achieved by increasing initialSize from 223to\n3\u0001223). Each con\fguration is run with 10 distinct random\nseeds and we compare the median statistics over the 10 runs.\n4.3.2 Results\nFig. 5 shows the median gain obtained as we increase\nthe load factor from 0.5 to 1.5. We obtain 1.6x, 2.6x, and\n1.8x higher throughput from the VIP con\fguration at low\n(zipf = 1), medium ( zipf = 2), and high skew ( zipf = 3)\nrespectively at load factor 1.5. In all cases, the gain from\nthe VIP con\fguration increases as the load factor increases,\nwhich is expected as the likelihood of collisions is higher\nwhen more keys are present in the hash table. We \fnd\nthat the performance metrics of the VIP con\fguration are\nmostly stable (refer to Table 2) indicating a stable hot set\nsize, while the performance of the Default con\fguration be-\ncomes steadily worse as the e\u000bective hot set grows larger\nwith the load factor.\n5. LEARNING POPULARITY ON-THE-FLY\nIn this section, we highlight the challenges of learning\nin-the-loop ( §5.1), which motivated the lightweight mech-\nanisms we built for VIP hashing. We describe how we learn,\nadapt, sense, and dynamically control the overhead on the\n\ry ( §5.2-3).\n／\n◼\nFigure 5: Roo\rine gain in operation throughput\nfrom the VIP vs the Default con\fguration as the\nload factor increases. While keeping the number\nof buckets \fxed at 224, we increase the load factor\nfrom 0:5to1:5. The performance gain obtained from\nthe VIP con\fguration increases with the load factor,\nand can be as high as 160% (2.6x) for medium skew\n(zipf = 2) at load factor 1:5.\nTable 2: Relative Metrics of VIP vs Default con\fgu-\nration as we increase the load factor ( lf) atzipf = 2.\nThe trends for low and high skew are similar.\nlfThroughput\n(fetch ops/s)Avg. Disp-\n-lacementL3\nMissesL1\nMisses\n0.5235M vs\n188M\n(+25%)1.0vs1.03\n(-3%)378M vs\n385M\n(-1.8%)380M vs\n412M\n(-8%)\n1236M vs\n134M\n(+77%)1.0vs1.17\n(-15%)376M vs\n387M\n(-2.6%)380M vs\n436M\n(-13%)\n1.5236M vs\n90M\n(+160%)1.0vs1.62\n(-38%)382M vs\n392M\n(-2.6%)382M vs\n458M\n(-17%)\n\n(a)Loss in performance when adding a 1-byte counter\nper key in the hash table. Both hash tables are identi-\ncal (in Default con\fguration) except for the size of the\nentries (16 vs 17 bytes).\n-66% \n+22% \n+11% \n+14% \n+28% \n0% \n(b)Relative metrics for zipf = 0. Instructions exe-\ncuted and cache misses increase after adding the 1-\nbyte counter.\nFigure 6: The e\u000bect of adding a 1-byte requests\ncounter per key in the hash table. 500M fetch opera-\ntions are issued to a hash table with 1M keys at load\nfactor 0:95. Performance can take a signi\fcant hit {\nwe observe a 66% loss in fetch operation through-\nput atzipf = 0. This experiment demonstrates the\nchallenges of learning in-the-loop with hash tables.\n5.1 Learning In-the-Loop is Costly\nHash tables execute a tight loop of instructions { com-\npute the hash function, access keys in the bucket, and per-\nform required operations to process the request. Adding\nany amount of additional computation or storage to this\nloop can have a signi\fcant impact on the performance. To\ndemonstrate this behavior, we conduct a simple experiment\nof adding a 1-byte requests counter per key in the hash ta-\nble, such that the entries become 17 bytes long (8 byte key\nand value, and 1 byte counter).\nWe use Wiscer to compare the performance of the vanilla\nimplementation of hash table (16 byte entries) to the im-\nplementation with request counters (17 byte entries). We\nissue 500M fetch requests to a hash table with 1M entries\n(load factor 0 :95 = 106=220) for di\u000berent levels of skew in\nthe popularity distribution ( zipf = 0 to 5 in steps of 1).\nThe remaining con\fguration options of Wiscer are set to\nthe defaults (refer to Table 1). Fig. 6 shows the relative\nperformance of the two hash table implementations at dif-\nferent levels of skew in the workload. There is a signi\fcant\nloss in throughput ranging from 11-66% due to increase in\ncache misses and instructions executed.\nCounting requests is a fundamental requirement for learn-\ning the popularity distribution. However, this experiment\nshows that even adding a small amount of additional mem-\nory can hurt performance signi\fcantly in the extreme case.\nThus, the challenge here is to work with a restricted \\bud-\nget\" when learning in-the-loop, to balance the gains against\nthe overhead of learning.5.2 VIP Hashing\nFrom §5.1, we know that using additional memory and\ncomputation can really hurt the performance of hash tables.\nIn this section, we describe how VIP hashing overcomes\nthese challenges by using lightweight mechanisms for learn-\ning and adapting to the popularity distribution ( §5.2.2),\nwhile controlling the overhead by sensing and dynamically\nswitching-on/o\u000b learning as necessary ( §5.2.3). We \frst give\nan overview of VIP hashing ( §5.2.1) followed by describing\nthe mechanisms used in detail ( §5.2.2-3).\n5.2.1 Overview\nFig. 7 shows the VIP hashing method. At any given\ntime, there are three possible modes that the hash table im-\nplementation can be in { learn+adapt ,sense , and default (or\nvanilla). In the learn+adapt mode, the hash table learns the\npopularity distribution and rearranges keys to move closer\nto the VIP con\fguration. This mode is costly in terms of\nboth computation and storage, and we control how much\nwe run this mode by con\fguring the parameter N L. The\nlearn+adapt mode is run at the start, and subsequent trig-\ngers of this mode happen only if the popularity distribution\nchanges, which is determined during the sense mode.\nThe sense mode is triggered after the learn+adapt mode\nto measure some statistics ( \rB) that characterize the pop-\nularity distribution. These statistics require a total of 24\nbytes of memory for the whole hash table (irrespective of\nthe size) and a few additional arithmetic operations in the\nloop. Since the memory and computation footprint of this\nmode is low, it does not add much overhead to the execu-\ntion. The sense mode is run for N Srequests at a time, and\nis triggered periodically (every N Drequests) to characterize\nthe popularity distribution at the time ( \rC). Comparing the\nstatistics (\rBand\rC) helps determine if the popularity dis-\ntribution has changed, and informs the decision of whether\nto switch on learning.\nThe default mode is the vanilla implementation of chained\nhashing ( §2.1) with 16 byte entries. There is no additional\noverhead of storage or computation. This mode is run most\nof the time (N D>NL, NS), so the performance is close to\nthe vanilla implementation of hash table in the worst case.\nIn the following sections, we discuss the mechanisms we\nuse for the learn+adapt ( §5.2.2) and sense ( §5.2.3) modes.\nWe discuss our choice of parameters (N L, NS, ND, etc.) in\n§5.3, that allow us to balance the performance gains against\nthe overhead of learning.\n5.2.2 Learning & Adapting\nAlgorithm 1 describes how we learn the popularity distri-\nbution and adapt to the skew on the \ry. The popularity of\na key is estimated as the proportion of requests made to the\nkey ( §2.2). Thus, learning the popularity distribution re-\nquires counting requests, which we know is challenging from\nthe discussion in §5.1.\nTo overcome the challenge of counting requests in-the-\nloop, we perform two optimizations. First, we count re-\nquests in a separate data structure that mimics the hash ta-\nble in arrangement (for every entry in the hash table, there\nis a corresponding entry in the request counting hash table).\nAlthough this temporarily requires more memory (about 50-\n60% increase in memory usage depending on the load factor)\nthan maintaining a counter per key in the hash table, the\n\nLearn + \nAdapt Sense Default Sense \nRequests ...\nT = 0 YesLearn + \nAdapt Sense \n... Default \nNoSense <> <>Yes\nNo......1. Learn the \npopularity \ndistribution \nand adapt for \nNL requests. 2. Sense the \npopularity \ndistribution for \nNS requests. \nLearn baseline \nparameters          .   3. Switch off \nlearning for the \nnext ND \nrequests. Run in \ndefault mode. 4. Sense the current \ndistribution, learn \nparameters            . \nCompare to          .  5. Learn and adapt if popularity \ndistribution has changed. Update \nparameters           . \n6. Run in default \nmode otherwise. 7. Periodically sense \nthe popularity \ndistribution. \n8. Trigger learning \nonly if popularity \ndistribution has \nchanged. \nNLNSND\nFigure 7: Overview of VIP Hashing. At any time, the hash table is in one of the three modes { learn+adapt ,\nsense , or default . The amount of time spent on learning and adapting is limited since it is costly. The\npopularity distribution is sensed periodically to detect changes and trigger learning only when necessary.\nAlgorithm 1 Learning and Adapting on-the-\ry\n1:procedure FetchAdaptive (requests)\n2: ht getHashTable()\n3: /* Requests are counted in a separate data structure*/\n4: reqcntht getRequestsCountingHashTable()\n5: forrinrequests do\n6: hash murmurHash( r.key )\n7: htentry ht[hash ]\n8: reqentry reqcntht[hash ]\n9: /* Keep track of entry with minimum requests */\n10: min reqhtentry =htentry\n11: min reqentry =reqentry\n12: whilehtentry andhtentry:key6=r:key do\n13: ifreqentry:count<min reqentry:count then\n14: minreqhtentry =htentry\n15: minreqentry =reqentry\n16: htentry =htentry: next()\n17: reqentry =reqentry: next()\n18: ifhtentry ==nullthen\n19: r:found = false\n20: continue\n21:r:found = true\n22:r:value =htentry:value\n23:reqentry:count =reqentry:count + 1\n24: ifreqentry:count>min reqentry:count then\n25: /* Swap this entry with the min requests entry */\n26: swap( htentry, min reqhtentry )\n27: swap( reqentry ,min reqentry )\n28: /* Reclaim cache space by clearing reqcntht*/\n29: clearCache( reqcntht)\ncost is incurred only during the learn+adapt mode. Sec-\nond, at the end of the learn+adapt mode, we clear the re-\nquest counting hash table ( reqcntht) from the cache by\nissuing cache \rush instructions ( mmclflushopt on Intel\nCPUs [8]), thus restricting the cache pollution caused by the\nadditional data structure to learn+adapt mode.\nTo attain the VIP con\fguration, we need to sort the keys\nin descending order of popularity in the bucket chains. Given\nthat the proportion of requests made to a key is an esti-\nmate of popularity, we use Algorithm 1 to stochastically sort\nthe keys in descending order of requests received on the \ry.\nWhen performing a fetch operation, we keep track of the en-\ntry with minimum requests ( minreqhtentry ) encountered\nin the path to the entry being fetched. If the entry being\nfetched has received more requests, then it is swapped withtheminreqhtentry and it moves forward in the chain. We\npropose the following theorem:\nTheorem 1.Let there be a bucket chain with nkeysK1;\nK2:::Knwhich have popularity p1>p2:::>pn>0. Let the\nkeys be in a random order in the chain. Then, by applying\nAlgorithm 1, the keys will converge to the sorted order of\npopularity as number of fetch requests N!1 .\nWe formally prove this theorem in Appendix A. There\nare two properties of Algorithm 1 that are worth noting.\nFirst, the VIPs move to the front quickly, as they can skip\nover multiple entries in the chain in a single fetch request.\nThis algorithm is, in essence, similar to selection sort as we\nare moving the entry with minimum requests to the end of\nthe (sub-)chain being accessed. An alternative would be to\ncompare only adjacent keys (bubble sort), which empirically\nrequires more requests for a VIP to move to the front.\nSecond, the cost of swapping is amortized, as there is at\nmost one swap performed per fetch operation. This ap-\nproach is faster compared to performing a full sort on every\nrequest, or sorting at the end after counting requests for\nsome time (we will have to access all the buckets in order\nto perform a full sort; this will incur cache misses and also\npollute the cache).\n5.2.3 Sensing & Dynamically Switch-on/off Learning\nAlgorithm 2 describes how we sense some key statistics of\nthe popularity distribution, which enable us to dynamically\nswitch-on learning only when the distribution has changed\n(Algorithm 3). While there are multiple ways to quantify the\ndi\u000berence between two probability mass functions ( pmfs)\n[6, 12, 24], we choose a lightweight statistic to compare dis-\ntributions { average displacement . In §4.2.2, we saw that\ndisplacement encountered indicates the \\goodness\" of the\nhash table con\fguration. Every popularity distribution im-\nposes a pmf over the displacement encountered on a request,\nwhich is a derived random variable. Formally stated:\nAxiom 1.LetK1; K2; :::; KNbeNkeys in the hash ta-\nble with popularity p1; p2; :::;pN(Ppi= 1) at displacement\nd1; d2; :::; dN(di\u0014N). Let D be the random variable of\nthe displacement encountered on a successful fetch request.\nThen,\n\nAlgorithm 2 Sensing\n1:procedure FetchSensing (requests)\n2: ht getHashTable()\n3: /* Metrics to track */\n4: disp 0 .cumulative displacement\n5: disp sq 0 .cumulative disp. square\n6: count 0 .number of requests\n7: c= 0:95 .con\fdence level of the interval\n8: forrinrequests do\n9: hash murmurHash( r.key )\n10: htentry ht[hash ]\n11: d 1\n12: while htentry and htentry!key6=r:key do\n13: htentry =htentry: next()\n14: d=d+ 1\n15: ifhtentry ==nullthen\n16: r:found = false\n17: continue\n18:r:found = true\n19:r:value =htentry:value\n20:count =count + 1\n21:disp =disp+d\n22:dispsq=dispsq+d\u0002d\n23: /* Estimating mean u, variance v, and C.I. width w*/\n24:u=disp=count\n25:v=dispsq=(count\u00001)\u0000disp2=(count\u0003(count\u00001))\n26:w=p\n\u00002:v:log (1\u0000c)=count . Gaussian tail bound\n27:\r= (u;w)\n28: return\r\nP(D=d) =NX\ni=1pi\u0001 1di=d\ni.e, the probability that displacement dis encountered on\na successful fetch request is the probability that any of the\nkeys with displacement dwere fetched. The average dis-\nplacement is calculated as\n\u0016D=E[D] =NX\ni=1i\u0001P(D=i)\nWe make the following observation:\nAxiom 2.The VIP con\fguration minimizes E[D] over all\npossible arrangements of keys in the hash table for a \fxed\nload factor, popularity distribution, and hash function.\nThe VIP con\fguration orders keys by popularity, thus giv-\ning more \\weight\" to lower values of Dwhich minimizes the\naverage displacement. It is straightforward to see that for a\ngiven hash table con\fguration, two popularity distributions\nwith di\u000berent average displacement will not be identical (al-\nthough the opposite is not true). Thus, a change in average\ndisplacement re\rects a shift in the popularity distribution.\nThe parameters we learn from sensing are \r= (^\u0016D;^wD) =\n(u;w) (Algorithm 2), where ^ \u0016Dis the estimated average\ndisplacement, and ^ wDis the width of the con\fdence interval\naround ^\u0016Dobtained using Gaussian tail bounds ( §2.2). We\nestimate the average displacement as\n^\u0016D=NSP\ni=1Di\nNSAlgorithm 3 Dynamically Switch-on/o\u000b Learning\n1:procedure HasDistributionChanged (\rB;\rC)\n2: (uB;wB) =\rB\n3: (uC;wC) =\rC\n4: ifjuB\u0000uCj>(wB+wC)then\n5: return true\n6: else\n7: return false\nwhich is the sample mean1of displacement encountered Di\n(1\u0014i\u0014NS) overNSfetch requests in the sense mode.\nSimilarly, we also estimate sample variance ^ \u001b2\nD(§2.2).\nWe further characterize the pmf by building a con\fdence\ninterval using Gaussian tail bounds ( §2.2). The width ( ^ wD)\nof the interval at con\fdence level c(c= 0:95 in our experi-\nments) is calculated as\n^wD=s\n\u00002\u0001^\u001b2\nD\u0001(1\u0000c)\nNS\nNote that ^\u001bDis estimated variance from a sample of NS\nobservations, and (^ \u0016D\u0000\u0016D) only approximately Gaussian\naccording to CLT ( §2.2). Thus, the width ^ wDobtained by\napplying Gaussian tail bounds is a heuristic.\nWe switch-on learning (Algorithm 3) only if we detect a\nsigni\fcant change in the average displacement. Given two\nsets of parameters \rB= (uB;wB) and\rC= (uC;wC) where\nuBanduCare estimated means, we check if the con\fdence\nintervals are disjoint. If so, then heuristically with a proba-\nbilityc2= (0:95)2= 0:9, we can be sure that the real means\nare not equal and the distributions have diverged. Thus, we\ndetect changes in popularity distribution in a non-intrusive\nmanner by computing lightweight statistics.\n5.3 Parameters\nThe parameters N L, NS, and N Ddetermine how long the\nhash table runs in learn+adapt, sense, and default modes\nrespectively. Our goal is to choose these parameters such\nthat the gains of learning are balanced against the overhead.\nOur choice of parameters is general, made using theoret-\nical and empirical evidence that is independent of the pop-\nularity distribution. Thus, our techniques ( §5.2) apply to\nany distribution with skew irrespective of its speci\fc prop-\nerties. Note that it is possible to further tune the parame-\nters and the techniques with additional knowledge such as\ntotal number of requests, patterns in the workloads, family\nof distribution, etc.\n5.3.1 Allocating the budget for learning {NLvsND\nLearning in-the-loop is costly { in our experiments, we\n\fnd that the learn+adapt mode can be as much as 4 xslower\nthan the vanilla implementation in the worst case under no\nskew (we tested di\u000berent hash table sizes from 1M to 100M\nkeys). If a total of ( NL+ND) requests are issued to VIP\nhashing, the loss in throughput due to learning would be:\n1\u0000Tvanilla\nTvip\u0014\u0010\n1\u0000ND:t+NL:t\nND:t+NL:4:t\u0011\nassuming that the vanilla implementation takes time ton\nan average to process each request. We cap the overhead of\n1Note that instead of sampling, we could also use the request\ncounting data structure ( reqcnthtin§5.2.2). However, this\nwould incur cache misses and also pollute the cache a\u000becting per-\nformance ( §5.1).\n\nlearning to at most 5% by choosing ND= 60\u0001NLin our\nexperiments (i.e, learn+adapt mode is run for at most1=61\nof the total requests). More generally, the cap on overhead\nis (1\u000061=(60 +k)), wherekdepends on the experimental con-\n\fguration ( k= 4 on our hardware). Thus, \fxing a budget\nforNL=NDlimits the overhead of learning in the worst case.\n5.3.2 Choosing NL{how much to learn?\nThe learn+adapt mode is run for NLrequests at a time.\nOur goal is to capture the popularity distribution as much as\npossible while learning for a \fnite number of requests. From\nprevious work [31], we know that it takes \u0002( N) i.i.d. sam-\nples to learn a probability mass function over Nitems (with\nerror\u000f= 1 in KL divergence compared to the true pmf).\nWhen the cardinality of the hash table is not known/can\nvary, we choose NL= 1:5\u0001(htsize ), i.e, 1.5 times the num-\nber of buckets in the hash table. Since we maintain a load\nfactor of at most 1 :5 at all times, the number of keys in the\nhash tableN\u00141:5\u0001htsize , which satis\fes our requirements.\n5.3.3 Parameters for sensing {Nsand c\nWe sense the distribution for N Srequests at a time to\nestimate the average displacement ^ \u0016Dand build an inter-\nval with con\fdence c. Since the load factor is low and the\nlongest chain length is likely to be low as well (except in\npathological cases where many keys are hashed to the same\nbucket), we have found that choosing NSto be a large num-\nber (1000) has been su\u000ecient in our experiments. We build\nac= 95% con\fdence interval that heuristically gives us a\nprobability of c2= (0:95)2= 0:9 when we detect a shift in\npopularity. By increasing (decreasing) the con\fdence level,\nwe can be less (more) sensitive to changes in popularity.\n6. APPLICATIONS\n6.1 PK-FK Hash Joins\nHash tables are frequently used in database systems for\nprocessing join queries. In this section, we describe how VIP\nhashing can improve the performance of primary key-foreign\nkey (PK-FK) hash joins in the presence of skew.\n6.1.1 Experimental Setup\nMotivated by past research [27, 28, 38], we consider the\ncanonical PK-FK join query on tables RandS(jRj\u0014jSj)\nwith 8-byte integer attributes (16-byte tuples). Skew can\narise in PK-FK relations [27,28] when some keys occur more\nfrequently than others in the outer relation S. We use Wis-\ncer to instantiate RandSusing the sequential key pattern\nfor primary keys in R, and varying the level of skew in the\nouter relation Sfrom uniform ( zipf = 0) to high ( zipf = 3)\nfor 10 distinct random seeds. We compare the performance\nof the canonical hash join algorithm [27, 38] implemented\nusing the default and VIP hash tables, while materializing\npointers to output tuple pairs. We assume that the tuples\ninSare i.i.d, i.e, the popularity distribution is static. We\nexplore e\u000bects of dynamic popularity distribution in §6.2.\n6.1.2 Default vs VIP Hash Join\nFig. 8 shows the relative execution time of the default vs\nVIP hash join implementations. The cardinalities of Rand\nSare 12M and 192M respectively ( jRj:jSj= 1 : 16) [27,28],\nand the load factor is 1 :4 (= 12\u0001106=223). For medium skew\nFigure 8: Performance of PK-FK canonical hash join\non tables RandS(jRj:jSj= 1 : 16 ), when using the\ndefault and VIP hash table implementations. For\nmedium skew, we observe a 22.5% reduction in me-\ndian (over 10 random seeds) total execution time.\nTable 3: Relative metrics for default and VIP hash\njoin atzipf = 2,jRj:jSj= 1 : 16 .\nMetric Default VIP Di\u000b\nTime 3.4s 2.6s -22.5%\nAvg. Displacement 1.23 1.0003 -18.7%\nL3 Misses 75.5M 75.3M -0.3%\nL2 Misses 127.9M 124.6M -2.6%\nL1 Misses 161.2M 155.7M -3.4%\nInstructions 8.5B 8.2B -3.5%\nin the outer relation, the average displacement encountered\nby the default hash join implementation is 1 :23 (Table 3)2.\nFor the case of canonical hash join query, the learning\nbudget of the VIP hash table implementation can be calcu-\nlated in advance while maintaining NL:ND= 1 : 60 ( §5.3)\nsince we almost always know the cardinalities of the relations\nfrom system catalogs. Learning is triggered at the beginning\nof the probe phase with a budget of NL=min(jRj;jSj\n61)\n=16\u0001jRj\n61= 0:26\u0001jRjlookups from the outer relation. Learn-\ning takes about 3% of the total execution time, ranging from\n70-600ms depending on the level of skew. Note that the av-\nerage displacement of the VIP hash join implementation is\nvery close to 1 (Table 3) indicating that the learning mech-\nanism e\u000eciently captures the popularity distribution, and\nreduces cache misses and instructions executed.\nTo show the impact of varying the learning budget, we\nrepeated the experiment for lower and higher cardinality ra-\ntios. For a ratio of 1 : 4, we have a learning budget of\n4\u0001jRj\n61= 0:07\u0001jRjrequests and the overall reduction in exe-\ncution time is 18.6%. On the other hand, a cardinality ratio\nof 1 : 64 allows a learning budget of jRj=min(jRj;64\u0001jRj\n61)\nand results in 25.8% reduction in execution time. Thus, the\navailable learning budget impacts the gain in performance.\n6.1.3 Application to Skewed TPC-H\nWe focus our attention on TPC-H query 9 [20], which is\nthe most expensive TPC-H query involving multiple PK-FK\njoin operations. We implemented VIP hashing in DuckDB [41],\nan in-memory vectorized DBMS, to speed up PK-FK hash\n2Note that the average displacement is low for the default con-\n\fguration in this case, since the keys are sequential. Holding the\nload factor constant, randomly generated keys result in a median\n(over 10 random seeds) average displacement of 1.48.\n\nFigure 9: Execution time of TPC-H query 9 (scale\nfactor = 1) on DuckDB. VIP hashing speeds up PK-\nFK hash join probes, and results in 20% reduction\nin median (over 10 random seeds) end-to-end query\nexecution time at zipf = 1andzipf = 1:5.\njoins in single-threaded mode. Fig. 9 shows the median ex-\necution time of VIP hash join relative to the default, tested\non skewed TPC-H data (Appendix B) at varying levels of\nskew (from zipf = 0 tozipf = 1:5) for 10 di\u000berent random\nseeds. VIP hash join reduces the end-to-end query execution\ntime by 20% at zipf = 1 andzipf = 1:5, while the increase\nin execution time at lower skew is negligible. The remaining\nTPC-H queries spend <1% of the total execution time in\nskewed PK-FK hash joins, and consequently the impact of\nVIP hashing is negligible.\n6.2 Point Queries\nAnother common use of hash tables is for in-memory in-\ndexing in database systems [4,14] and in key-value stores [3,\n21] for processing point queries. In this section, we evaluate\nVIP hashing against a range of workloads generated using\nWiscer that highlight the robustness of our techniques for\nlearning in-the-loop under di\u000berent conditions. In all the\nexperiments, we assume no prior knowledge of the charac-\nteristics of the request distribution. The \frst two work-\nloads ( §6.2.1- §6.2.2) involve fetch operations, and the last\ntwo ( §6.2.3- §6.2.4) perform insert and delete operations.\nWe run these workloads on a hash table with 1M entries\n(load factor 0.95 = 106=220) in the Default con\fguration at\nthe start. Each of these workloads issue 500M operations to\nthe hash table at varying levels of skew ranging from uniform\n(zipf = 0) to medium skew ( zipf = 2). The remaining\ncon\fguration options of Wiscer are set to the defaults (refer\nto Table 1). We compare the performance of VIP hashing\nto the default hash table in Fig. 10-14.\n6.2.1 Static Popularity\nIn this workload, the popularity of keys in the hash ta-\nble remains the same throughout the experiment. We run\n500M fetch operations at four levels of skew from zipf = 0\ntozipf = 2 in steps of 0 :5. For the case of uniform pop-\nularity distribution ( zipf = 0), the overall loss in through-\nput is 2% (Fig. 10a) which is within our allocated budget\nof 5% ( §5.3.1), whereas for low skew ( zipf = 1), we ob-\ntain a net gain of 22% (Fig. 10b). The performance gain is\nhigher at medium levels of skew { the gain in throughput\natzipf = 1:5 andzipf = 2 is 77% (Fig. 10c) and 116%\n(Fig. 10d) respectively. Since the popularity distribution is\nstatic, the learn+adapt mode is triggered only at the start\nof the experiment for 1 :5\u0001htsize requests in all cases. The\nperiodic runs of the sense mode do not detect a change in\npopularity and the learn+adapt mode is not triggered again.\nThus, learning is run only when necessary, and the overhead\nof VIP hashing is minimized6.2.2 Popularity Churn\nIn this workload, we study how VIP hashing adapts to\nchanging popularity distribution over time. We simulate two\nrates of shift { medium and high. For the case of medium\nchurn, the popularity distribution shifts by 25% every 100M\nfetch operations (about 3s at zipf = 1). Fig. 11 shows\nthe behavior of VIP hashing under medium churn. Note\nthat the sense mode triggers learning only when necessary.\nFor instance, learning was triggered 3 out of the 4 times at\nzipf = 1 only when there was a substantial change in av-\nerage displacement due to shift in popularity (accompanied\nby a decrease in performance). The gain in throughput for\nzipf = 1 andzipf = 1:5 is 19% and 49% respectively.\nFor the case of high churn (Fig. 12), the popularity dis-\ntribution shifts by 50% every 10M fetch operations ( <1s),\ni.e., popularity shift occurs 50 times during the experiment.\nEvery run of the sense mode detects a change in distribution\nand learning is triggered every time for both levels of skew.\nWe obtain a net increase of 12% and 22% in throughput for\nzipf = 1 andzipf = 1:5 respectively. Thus, VIP hashing\nis able to sense changes in the distribution, and re-learn on\nthe \ry.\n6.2.3 Steady State\nNext, we test a workload with 98% fetch requests, 1% in-\nsert requests, and 1% delete requests (Fig. 13). The cardi-\nnality of the hash table doesn't change substantially during\nthe experiment, as the number of insert and delete oper-\nations are approximately balanced. The keys are inserted\n(deleted) in random positions of the popularity order. We\nobserve that as new keys (which are less popular with high\nprobability) are inserted at the front of the chains, the hash\ntable arrangement steadily becomes worse and the perfor-\nmance of VIP hashing approaches the default for zipf = 1.\nAtzipf = 1:5, the trend is similar, but is less stable as\na small number of topmost keys carry most of the popu-\nlarity weight. A change in average displacement is sensed\nevery time and learning is triggered, which bounces back\nthe performance of VIP hashing. We obtain a net gain in\nthroughput of 5.4% and 3% for zipf = 1 andzipf = 1:5\nrespectively.\n6.2.4 Read Mostly\nIn this workload, we issue 98% fetch requests and 2%\ninsert requests. New keys are inserted in arbitrary posi-\ntions in the popularity order. Similar to §6.2.3, we observe\nthat the performance steadily becomes worse as new keys\nare inserted at the front of the bucket chains for zipf = 1\n(Fig. 14a). Inserting new keys increase the load factor, which\ndegrades the throughput of the default implementation as\nwell (Fig. 14a). The rate of degradation at zipf = 1:5 as the\npopularity weight lies with a smaller portion of topmostly\nkeys. Rehashing is triggered when the load factor exceeds\n1:5 (happens every 75 \u0001htsize requests), which bounces back\nthe performance for both the default and VIP hashing im-\nplementations for both levels of skew. The periodicity at\nwhich sensing is triggered (every 90 \u0001htsize requests) in-\ncreases every time rehashing is performed, as we update the\nparameters N Sand N Laccording to the size of the hash ta-\nble (htsize ). Given that the change in the distribution is\nsubstantial, every run of the sense mode detects a change in\npopularity and triggers learning. The net gain in through-\nput forzipf = 1 andzipf = 1:5 is 1% and 11% respectively.\n\nPeriodic Sensing \nLearning + \nAdapting (a)Static popularity ( §6.2.1) with zipf = 0 (uniform distri-\nbution). Since there is no skew in popularity, no performance\ngain can be obtained from VIP hashing. Learning adds over-\nhead to VIP hashing (4 xslower), and is only triggered at\nthe start for (1 :5\u0001220) requests (0.3s). Subsequent sensing of\nthe popularity distribution does not detect any change, and\nlearning is not triggered. Total loss in throughput is 1.9%,\nwhich is within our allocated budget.\nPeriodic Sensing \nLearning + \nAdapting (b) Static popularity ( §6.2.1) with zipf = 1 (low skew).\nLearning is only triggered at the start and is 3x slower than\nthe default (0.13s vs 0.05s respectively). Sensing does not\ndetect any changes to the popularity distribution, so learn-\ning is not triggered again. The overhead of learning is o\u000bset\nby the gain in performance from the VIP con\fguration. We\nobserve an overall increase in throughput of 21.8%.\nPeriodic Sensing \nLearning + \nAdapting \n(c)Static popularity ( §6.2.1) with zipf = 1:5 (medium\nskew). Learning is only triggered at the start for 0.03s, and\nsubsequent triggers of sense mode do not detect any changes\nto popularity. A net gain of 76.5% in throughput is observed.\nPeriodic Sensing \nLearning + \nAdapting (d)Static popularity ( §6.2.1) with zipf = 2 (medium skew).\nLearning is only triggered at the start for 0.02s, and subse-\nquent triggers of sense mode do not detect any changes to\npopularity. A net gain of 116.1% in throughput is observed.\nFigure 10: Performance of VIP hashing under static popularity distribution at increasing levels of skew\nranging from zipf = 0(uniform distribution) to zipf = 2(medium skew). Fetch requests are issued to a hash\ntable with 1M entries at load factor 0.95 with keys in a random order initially. Learning is only triggered at\nthe start2, and the performance gain ranges from 22% to 116% depending on the level of skew. The overhead\nof VIP hashing in the case of uniform distribution ( zipf = 0) is 2%, which is within our allocated budget.\n(a)Medium churn rate ( §6.2.2) with zipf = 1. Sensing\ntriggers learning 3 times, when it detects a signi\fcant dif-\nference in average displacement. Throughput increases by\n18.9% overall.\n(b)Medium churn rate ( §6.2.2) with zipf = 1:5. Sensing\ntriggers learning only 1 time, when it detects a signi\fcant\ndi\u000berence in average displacement. Throughput increases by\n49.0% overall.\nFigure 11: Performance of VIP hashing under medium popularity churn at zipf = 1andzipf = 1:5. Fetch\nrequests are issued to a hash table with 1M entries at load factor 0.95 with keys in a random order initially.\nPopularity distribution shifts every 100M requests by 25% (top 21 out of 1M keys at zipf = 1, and the\ntopmost key at zipf = 1:5, are replaced by less popular key(s) at random). Distribution shift increases average\ndisplacement and can reduce performance (notice drop in performance of VIP hashing at 400M requests for\nzipf = 1:5). Sensing triggers learning2whenever it detects a signi\fcant increase in average displacement. Net\nincrease in throughput for zipf = 1andzipf = 1:5is 19% and 49% respectively.\n2The triggers of sense mode and learn+adapt mode have been marked using green circles and orange squares respectively. The unmarked\nperiodic dips in throughput for both the VIP and default implementations are due to monitoring activity performed by the Cloudlab [35]\nenvironment, and are unrelated to VIP hashing.\n\nLearning+ \nAdapting Periodic Sensing \n....Popularity \nDistribution \nshifts (a)High churn rate ( §6.2.2) with zipf = 1. Overall, 11.8%\nincrease in throughput is observed.\nPeriodic Sensing \n....Learning + \nAdapting (b)High churn rate ( §6.2.2) with zipf = 1:5. Overall, 21.9%\nincrease in throughput is observed.\nFigure 12: Performance of VIP hahsing under high popularity churn at zipf = 1andzipf = 1:5. Fetch requests\nare issued to a hash table with 1M entries at load factor 0.95 with keys in a random order initially. Popularity\ndistribution shifts every 10M requests by 50% (top 750 out of 1M keys at zipf = 1, and the top 2 keys at\nzipf = 1:5, are replaced by less popular keys at random). The bene\ft of learning dimishes as the popularity\norder becomes shu\u000fed. Periodic sensing triggers learning every time, as frequent distribution shifts cause\nsigni\fcant change in average displacement. Net increase in throughput for zipf = 1andzipf = 1:5is 12% and\n22% respectively.\nLearning + \nAdapting Periodic Sensing \n(a)Steady state ( §6.2.3) with zipf = 1. An overall gain of\n5.4% is observed.\nLearning + \nAdapting Periodic Sensing (b)Steady state ( §6.2.3) with zipf = 1:5. An overall gain of\n3.1% is observed.\nFigure 13: Performance of VIP hashing in a steady state consisting of 98% fetch requests, 1% insert requests,\nand 1% delete requests at zipf = 1andzipf = 1:5. 500M fetch requests are issued to a hash table with\n1M entries at load factor 0.95 with keys in a random order initially. With new keys being inserted at the\nfront of the buckets and existing keys being deleted, the hash table arrangement steadily becomes worse.\nPeriodic sensing triggers learning every time which bounces back the performance. The net throughput gain\nforzipf = 1andzipf = 1:5is 5% and 3% respectively.\n(a)Ready mostly workload ( §6.2.4) with zipf = 1. Overall,\nwe observe a gain of 1% in throughput.\nPeriodic Sensing \nRehashing Learning + \nAdapting (b)Ready mostly workload ( §6.2.4) with zipf = 1:5. Over-\nall, we observe a gain of 10.6% in throughput.\nFigure 14: Performance of VIP hashing under a ready mostly workload with 98% fetch requests and 2%\ninsert requests at zipf = 1andzipf = 1:5. 500M fetch requests are issued to a hash table with 1M entries at\nload factor 0.95 with keys in a random order initially. For this workload, rehashing is triggered when the\nload factor reaches 1.5, which happens every 75\u0001htsize requests. Whenever rehashing occurs, we double the\nperiodicity of sensing (N S) and the duration of learning (N L), i.e., learning is triggered less frequently for\nlonger duration each time. The net gain in throughput for zipf = 1andzipf = 1:5is 1% and 11% respectively.\n\n7. RELATED WORK\nHash tables are well studied data structures in literature.\nTwo major categories of hash tables are chained hashing [5]\nwhere collisions are resolved by chaining ( §2.1), and open\naddressing [16] where collisions are resolved by searching for\nalternate positions in an array. Richter et al. [42] study\ndi\u000berent hash table implementations spanning both the cat-\negories, hash functions, workload patterns, etc. while high-\nlighting the variability in the performance of hash tables\nbased on a host of factors. Similar to our work, they con-\nsider the problem of hashing 8-byte integer keys and values.\nMultiple open source hash tables [2,10,19] use both cate-\ngories of implementations. For instance, Google's \rat hash\ntable [19] takes an open addressing approach, while the\nbytell (byte linked list) hash table [2] uses chaining to resolve\ncollisions. When it comes to data systems, DBMS such as\nSQLite3 [18] and PostgreSQL [7], as well as key-value stores\nsuch as Redis [1] and Memcached [21] use data structures\nthat involve chaining of entries. Thus, we \fnd that chained\nhash tables are a popular choice commonly used in practice.\nSkew in popularity is a well studied phenomenon. Multi-\nple studies involving production workloads have found fetch\nrequests to follow a power-law behavior [26,30], which is of-\nten captured using the zip\fan distribution [28, 34, 44]. For\ninstance, the request distribution in the core workloads of\nYCSB [23] is zip\fan by default. Alongside skew in popular-\nity, previous work [26] also discusses e\u000bects such as churn in\npopular keys in real world workloads. This is a key feature\ncaptured by Wiscer ( §3), which is not present in any of the\nexisting workload generators to the best of our knowledge.\nBroadly speaking, caching algorithms such as LRU-k [40],\nMRU [33], etc. that track the recency of access are attempt-\ning to capture the current popularity distribution. Key-\nvalue stores designed for disk-based settings, such as Anna [44]\nand Faster [32] incorporate techniques to leverage the skew\nin popularity by moving hot data to memory. Recent work\nby Herodotou et al. [37] uses machine learning to automati-\ncally move data between di\u000berent storage tiers in clusters. A\nrecurring trend to note here is that the complexity of these\nexisting schemes vary depending on the \\budget\" allowed by\nthe setting, ranging from relatively simple LRU approach is\nused even in processor caches, to a more complex approach\ninvolving machine learning in large-scale clusters.\nTo this end, the budget available for learning in-the-loop\nwith hash tables is extremely limited, as we see in our work\n(Fig. 6). In the seminal paper on learned indexes by Kraska\net al. [39], the authors propose learning a hash function\nfrom the keys in the hash table such that collisions can be\navoided altogether. However, recent work on learned hash\nfunctions [43] shows that this approach hits the wall due to\ntwo main reasons { cache sensitivity, and model complex-\nity. While larger models are necessary to accurately capture\narbitrary key distributions, the computation times become\nprohibitively high (50 xhigher [43]) due to increased cache\nmisses from accessing the model parameters. The high cache\nsensitivity and low latency requirements of hash tables pre-\nclude the use of costly ML techniques for learning.\nA noteworthy aspect of the VIP hashing method is that\nlearning is performed online, i.e., the hash table does not\npause operation at any time. In contrast, recent work [36,43]\ninvolves learning from the data o\u000fine before populating the\nhash table. Adapting to changing key distributions remains\na challenge with these approaches, with the fallback beingreverting to the default hash table implementation [36] or\nrelearning [39, 43], both of which require costly rehashing\nthat pauses execution.\n8. CONCLUSIONS & FUTURE WORK\nHashing is a low-latency operation that runs a tight loop\nof operations, and is sensitive to the e\u000bects of caching. In-\ncreasing the memory and computation footprint even by a\nsmall proportion can have a signi\fcant impact on perfor-\nmance as we see in §5.1. Given these constraints, learning\nin-the-loop precludes the use of costly techniques and makes\nit necessary to use lightweight schemes while controlling the\noverhead as much as possible.\nOverall, VIP hashing is comprised of four mechanisms\n{learning ,adapting ,sensing , and dynamically switching-\non/o\u000b learning. These mechanisms ( §5.2), along with our\nchoice of parameters ( §5.3) keep the overhead of learning\nin check compared to the gains. We evaluate VIP hashing\nusing an extensive set of workloads (Fig. 8-14) that demon-\nstrate the ability to learn on the \ry in the presence of insert\nand delete operations, and shifting distributions. Our exper-\niments involving PK-FK hash joins show that VIP hashing\nreduces the end-to-end execution time by 22%, while the\ngain in performance for point queries ranges from 3%-77%\nunder medium skew. While the performance gain depends\non the a host of factors (level of skew, proportion of insert\nand delete, etc.), the distinguishing property of VIP hashing\nis the ability to learn in a non-blocking, online fashion.\nBroadly speaking, our work highlights the challenges of\nlearning with cache sensitive, low latency data structures.\nWhile the major source of performance gain for VIP hashing\nhas been from improvement in cache locality, the sensitiv-\nity of hash tables to e\u000bects of caching make learning very\nchallenging ( §5.1, [43]). Possible future work could involve\nstudying other low latency data structures such as bloom\n\flters [29], to see how cache locality can be improved by\nadapting to the data. Learning tasks involving such cache\nsensitive data structures will necessitate controlling the over-\nhead, perhaps by using our approach of budgeted learning\nand non-intrusive sensing.\n9. ACKNOWLEDGMENTS\nThis research was supported in part by a grant from the\nMicrosoft Jim Gray Systems Lab, by the National Science\nFoundation under grant OAC-1835446, and by CRISP, one\nof six centers in JUMP, a Semiconductor Research Corpora-\ntion (SRC) program, sponsored by MARCO and DARPA.\n10. REFERENCES\n[1] A little internal on Redis hash table implementation.\nhttps://bit.ly/3pfVvTm .\n[2] Bytell hash map. https://bit.ly/3fB8NX6 .\n[3] Data types in Redis.\nhttps://redis.io/topics/data-types .\n[4] Hash join in MySQL 8.\nhttps://mysqlserverteam.com/\nhash-join-in-mysql-8/ .\n[5] Hash table.\nhttps://en.wikipedia.org/wiki/Hash_table .\n[6] Hellinger's distance. https://en.wikipedia.org/\nwiki/Hellinger_distance .\n\n[7] Indexes in PostgreSQL. https://bit.ly/3c7L52A .\n[8] Intel Intrinsics. https://intel.ly/3nxA416 .\n[9] Intel performance monitoring events.\nhttps://perfmon-events.intel.com/ .\n[10] Intel TBB hash map. https://intel.ly/3uDtNAQ .\n[11] Intel Xeon Silver 4114 processor.\nhttps://intel.ly/3fDidSb .\n[12] Kullback-Leibler divergence.\nhttps://en.wikipedia.org/wiki/Kullback%\nE2%80%93Leibler_divergence .\n[13] Lindeberg-Levy CLT. https://bit.ly/34A19WJ .\n[14] MariaDB Storage Index Types. https://mariadb.\ncom/kb/en/storage-engine-index-types/ .\n[15] MurmurHash3. https://github.com/aappleby/\nsmhasher/wiki/MurmurHash3 .\n[16] Open addressing. https:\n//en.wikipedia.org/wiki/Open_addressing .\n[17] Power Law.\nhttps://en.wikipedia.org/wiki/Power_law .\n[18] SQLite hash table implementation.\nhttps://sqlite.org/src/file/src/hash.c .\n[19] Swiss Tables and absl::Hash. https:\n//abseil.io/blog/20180927-swisstables .\n[20] TPC-H Benchmark (Version 3).\nhttp://www.tpc.org/tpch/ .\n[21] Understanding the Memcached source code.\nhttps://holmeshe.me/\nunderstanding-memcached-source-code-V/ .\n[22] Wiscer. https://github.com/aarati-K/wiscer .\n[23] YCSB Core Workloads.\nhttps://github.com/brianfrankcooper/\nYCSB/wiki/Core-Workloads .\n[24] Z-test.\nhttps://en.wikipedia.org/wiki/Z-test .\n[25] Zipf's law. https://bit.ly/3yTN0BO .\n[26] B. Atikoglu, Y. Xu, E. Frachtenberg, S. Jiang, and\nM. Paleczny. Workload Analysis of a Large-Scale\nKey-Value Store. Sigmetrics Performance Evaluation\nReview - SIGMETRICS , 2012.\n[27] C. Balkesen, J. Teubner, G. Alonso, and M. T. Ozsu.\nMain-memory hash joins on multi-core CPUs: Tuning\nto the underlying hardware. In 2013 IEEE 29th\nInternational Conference on Data Engineering .\n[28] S. Blanas, Y. Li, and J. M. Patel. Design and\nEvaluation of Main Memory Hash Join Algorithms for\nMulti-Core CPUs. In Proceedings of the 2011 ACM\nSIGMOD International Conference on Management of\nData . Association for Computing Machinery.\n[29] B. Bloom. Space/time trade-o\u000bs in hash coding with\nallowable errors. Commun. ACM , 1970.\n[30] L. Breslau, P. Cao, L. Fan, G. Phillips, and\nS. Shenker. Web caching and Zipf-like distributions:\nevidence and implications. In IEEE INFOCOM '99 .\n[31] C. L. Canonne. A short note on learning discrete\ndistributions. arXiv: Statistics Theory , 2020.\n[32] B. Chandramouli, G. Prasaad, D. Kossmann,\nJ. Levandoski, J. Hunter, and M. Barnett. FASTER:\nA Concurrent Key-Value Store with In-Place Updates.\nIn2018 ACM SIGMOD International Conference on\nManagement of Data (SIGMOD '18) .\n[33] H. Chou and D. DeWitt. An Evaluation of Bu\u000berManagement Strategies for Relational Database\nSystems. Algorithmica , 2005.\n[34] B. F. Cooper, A. Silberstein, E. Tam,\nR. Ramakrishnan, and R. Sears. Benchmarking Cloud\nServing Systems with YCSB. In Proceedings of the 1st\nACM Symposium on Cloud Computing , SoCC 2010.\n[35] D. Duplyakin, R. Ricci, A. Maricq, G. Wong,\nJ. Duerig, E. Eide, L. Stoller, M. Hibler, D. Johnson,\nK. Webb, A. Akella, K. Wang, G. Ricart,\nL. Landweber, C. Elliott, M. Zink, E. Cecchet, S. Kar,\nand P. Mishra. The Design and Operation of\nCloudLab. In Proceedings of the USENIX Annual\nTechnical Conference (ATC) , 2019.\n[36] B. Hentschel, U. Sirin, and S. Idreos. Entropy-learned\nhashing: Constant time hashing with controllable\nuniformity. In Proceedings of the 2022 International\nConference on Management of Data , SIGMOD '22,\n2022.\n[37] H. Herodotou and E. Kakoulli. Automating\ndistributed tiered storage management in cluster\ncomputing. Proc. of the VLDB Endowment , 2019.\n[38] M. Kitsuregawa, H. Tanaka, and T. Moto-Oka.\nApplication of hash to data base machine and its\narchitecture. New Generation Computing , 2009.\n[39] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and\nN. Polyzotis. The Case for Learned Index Structures.\nCoRR , abs/1712.01208, 2017.\n[40] E. O'neil, P. O'Neil, G. Weikum, and E. Zurich. The\nLRU{K Page Replacement Algorithm For Database\nDisk Bu\u000bering. SIGMOD Record (ACM Special\nInterest Group on Management of Data) , 1996.\n[41] M. Raasveldt and H. M uhleisen. DuckDB: An\nEmbeddable Analytical Database. In Proceedings of\nthe 2019 International Conference on Management of\nData , SIGMOD '19.\n[42] S. Richter, V. Alvarez, and J. Dittrich. A\nSeven-Dimensional Analysis of Hashing Methods and\nIts Implications on Query Processing. Proceedings of\nthe VLDB Endowment , 2015.\n[43] I. Sabek, K. Vaidya, D. Horn, A. Kipf, and T. Kraska.\nWhen Are Learned Models Better Than Hash\nFunctions? CoRR , abs/2107.01464, 2021.\n[44] C. Wu, V. Sreekanti, and J. Hellerstein. Autoscaling\ntiered cloud storage in Anna. Proceedings of the\nVLDB Endowment , 2019.\nAPPENDIX\nA. PROOF OF THEOREM 1\nTheorem 1 ( §5.2.2) states that given keys K1; K2; :::;Kn\nin a bucket with probability p1> p 2> :: > pn, such that\nthe keys are in a random order initially. Then by applying\nAlgorithm 1, the keys will converge to the sorted order of\npopularity as the number of fetch requests N!1 . We \frst\nmake the following observation:\nLemma 1.Given two keys K1andK2with popularity p\nand(1\u0000p)respectively. Let p > 0:5. GivenNsuccessful\nfetch requests are made, and keys K1andK2receiveN1and\nN2requests respectively. Then,\nlim\nN!1N1\u0000N2\nN= (2\u0001p\u00001)>0\n\nDimension\n(Primary Key)Fact\n(Foreign Key)Skew added\nto FK?Comments\nPart\n(ppartkey)PartSupp\n(pspartkey)NoEach part has a \fxed number of suppliers (4 suppliers\nper part). Thus, pspartkey cannot be skewed.\nSupplier\n(ssuppkey)PartSupp\n(pssuppkey)Yespssuppkey is zip\fan distributed, i.e., a supplier is chosen\nfrom a zip\fan distribution over ssuppkey . We ensure\nthat each part has 4 distinct suppliers.\nPartSupp\n(pspartkey, ps suppkey)Lineitem\n(lpartkey, l suppkey)Yeslpartkey is zip\fan distributed. lsuppkey is picked\nrandomly from the available suppliers of the chosen part.\nCustomer\n(ccustkey)Orders\n(ocustkey)Yesocustkey is zip\fan distributed, i.e., ocustkey is\ndrawn from a zip\fan distribution over all ccustkey .\nOrders\n(oorderkey)Lineitem\n(lorderkey)NoEach order (oorderkey) can have limited number of\nlineitems (1 to 7). Thus, lorderkey cannot be skewed.\nNation\n(nnationkey)Supplier\n(snationkey)Yessnationkey is zip\fan distributed, i.e., snationkey is\ndrawn from a zip\fan distribution over all nnationkey .\nNation\n(nnationkey)Customer\n(cnationkey)Yescnationkey is zip\fan distributed, i.e., cnationkey is\ndrawn from a zip\fan distribution over all nnationkey .\nRegion\n(rregionkey)Nation\n(nnationkey)No Each nation belongs to a \fxed region (continent).\nTable 4: Introducing skew in TPC-H relations. We introduce skew in the FK attribute wherever possible\nunder existing constraints. For 5 out of 8 cases where skew was introduced, the level of skew can be con\fgured\nthrough the zip\fan coe\u000ecient.\nThe above lemma follows from the frequentist de\fnition\nof probability. Thus, as N!1 , we can be sure that more\npopular keys will receive more requests. This will hold pair-\nwise for all the keys K1; K 2; :::;Knin the bucket chain,\nwhich motivates the following claim.\nLemma 2.LetfKigbe keys in a bucket with probability\nfpig; i2[N]. LetK1be the most popular key in the bucket,\ni.e.,p1>pj8j2f2;::;Ng. Let the initial order of keys be\nrandom. Then, by running Algorithm 1, K1will be at the\nfront of the chain as number of fetch requests N!1 .\nProof. SupposeK1is at displacement d>1. Let there\nbe keysK0\n1; ::; K0\nd\u00001in front of K1. Let the keys have\nreceived requests n1; ::; nd\u00001. LetK1have received nre-\nquests. From Lemma 2, we know that\nlim\nN!1n>ni;8i2[(d\u00001)]\nThus,K1would have received more requests than all the\nkeys in front of it as N!1 . From Algorithm 1, on the\nlast request that K1received, it should have been swapped\nwith a key with lower number of requests ahead of it. This\ncontradicts our assumption that K1is at position d>1.\nThus, the most popular key in the chain will be in the front\nas number of requests approaches in\fnity. By recursively ap-\nplying Lemma 3 to the remaining keys in the bucket, we can\nprove that the keys will be in the sorted order of popularity\nasN!1 .\nB. INTRODUCING SKEW IN TPC-H\nFig. 15 shows the PK-FK (primary key-foreign key) con-\nstraints in TPC-H schema. Skew can arise in PK-FK re-\nlations when a some primary keys occur more frequently\nPART (P_) \nSF*200,000 \nP AR TKEY \nSUPPLIER (S_) \nSF*10,000 \nSUPPKEY \nNATION (N_) \n25REGION (R_) \n5NA TIONKEY \nREGIONKEY PARTSUPP \n(PS_) \nSF*800,000 \nP AR TKEY \nSUPPKEY \nCUSTOMER (C_) \nSF*10,000 \nCUSTKEY \nREGIONKEY LINEITEM \n(L_) \nSF*6,000,000 \nORDERS (O_) \nSF*1,500,000 NA TIONKEY NA TIONKEY \nORDERKEY ORDERKEY \nP AR TKEY \nSUPPKEY \nCUSTKEY Figure 15: PK-FK constraints in TPC-H schema.\nThe cardinalities of the tables have been indicated at\nthe top (SF denotes scale factor), and only primary\nkey and foreign key attributes have been shown.\nthan others in the fact (FK) relation, i.e., the distribution\nof the FK attribute is skewed. Note that primary keys are\nunique, and thus by de\fnition, skew cannot arise in the PK\nattribute. We considered the existing constraints in TPC-H\nschema (Fig. 15), and introduced skew in the FK attribute\nwherever possible. Table 4 details our \fndings { we have\nintroduced skew in 5 out of 8 FK attributes, and we also\ndescribe the reasons for cases where skew could not be in-\ntroduced. Wherever applicable, the level of skew can be\nadjusted by con\fguring the zip\fan coe\u000ecient.",
  "textLength": 73095
}