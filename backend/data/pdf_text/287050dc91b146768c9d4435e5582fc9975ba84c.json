{
  "paperId": "287050dc91b146768c9d4435e5582fc9975ba84c",
  "title": "Learned Cardinalities: Estimating Correlated Joins with Deep Learning",
  "pdfPath": "287050dc91b146768c9d4435e5582fc9975ba84c.pdf",
  "text": "Learned Cardinalities:\nEstimating Correlated Joins with Deep Learning\nAndreas Kipf\nTechnical University of Munich\nkipf@in.tum.deThomas Kipf\nUniversity of Amsterdam\nt.n.kipf@uva.nlBernhard Radke\nTechnical University of Munich\nradke@in.tum.de\nViktor Leis\nTechnical University of Munich\nleis@in.tum.dePeter Boncz\nCentrum Wiskunde & Informatica\nboncz@cwi.nlAlfons Kemper\nTechnical University of Munich\nkemper@in.tum.de\nABSTRACT\nWe describe a new deep learning approach to cardinality estimation.\nMSCN is a multi-set convolutional network, tailored to representing\nrelational query plans, that employs set semantics to capture query\nfeatures and true cardinalities. MSCN builds on sampling-based\nestimation, addressing its weaknesses when no sampled tuples\nqualify a predicate, and in capturing join-crossing correlations. Our\nevaluation of MSCN using a real-world dataset shows that deep\nlearning significantly enhances the quality of cardinality estimation,\nwhich is the core problem in query optimization.\n1 INTRODUCTION\nQuery optimization is fundamentally based on cardinality estima-\ntion. To be able to choose between different plan alternatives, the\nquery optimizer must have reasonably good estimates for inter-\nmediate result sizes. It is well known, however, that the estimates\nproduced by all widely-used database systems are routinely wrong\nby orders of magnitude—causing slow queries and unpredictable\nperformance. The biggest challenge in cardinality estimation are\njoin-crossing correlations [ 16,18]. For example, in the Internet\nMovie Database (IMDb), French actors are more likely to partici-\npate in romantic movies than actors of other nationalities.\nThe question of how to better deal with this is an open area of\nresearch. One state-of-the-art proposal in this area is Index-Based\nJoin Sampling (IBJS) [ 17] that addresses this problem by probing\nqualifying base table samples against existing index structures.\nHowever, like other sampling-based techniques, IBJS fails when\nthere are no qualifying samples to start with (i.e., under selective\nbase table predicates) or when no suitable indexes are available.\nIn such cases, these techniques usually fall back to an “educated”\nguess—causing large estimation errors.\nThe past decade has seen the widespread adoption of machine\nlearning (ML), and specifically neural networks (deep learning), in\nmany different applications and systems. The database community\nalso has started to explore how machine learning can be leveraged\nwithin data management systems. Recent research therefore inves-\ntigates ML for classical database problems like parameter tuning [ 2],\nquery optimization [13, 23, 27], and even indexing [12].\nThis article is published under a Creative Commons Attribution License\n(http://creativecommons.org/licenses/by/3.0/), which permits distribution and repro-\nduction in any medium as well as allowing derivative works, provided that you attribute\nthe original work to the author(s) and CIDR 2019.\n9th Biennial Conference on Innovative Data Systems Research (CIDR ‘19), January 13-16,\n2019, Asilomar, California, USAWe argue that machine learning is a highly promising technique\nfor solving the cardinality estimation problem. Estimation can be\nformulated as a supervised learning problem, with the input being\nquery features and the output being the estimated cardinality. In\ncontrast to other problems where machine learning has been pro-\nposed like index structures [ 12] and join ordering [ 23], the current\ntechniques based on basic per-table statistics are not very good.\nIn other words, an estimator based on machine learning does not\nhave to be perfect, it just needs to be better than the current, inac-\ncurate baseline. Furthermore, the estimates produced by a machine\nlearning model can directly be leveraged by existing, sophisticated\nenumeration algorithms and cost models without requiring any\nother changes to the database system.\nIn this paper, we propose a deep learning-based approach that\nlearns to predict (join-crossing) correlations in the data and ad-\ndresses the aforementioned weak spot of sampling-based tech-\nniques. Our approach is based on a specialized deep learning model\ncalled multi-set convolutional network (MSCN) allowing us to\nexpress query features using sets (e.g., both (AZB)ZCand\nAZ(BZC)are represented as {A,B,C}). Thus, our model does\nnot waste any capacity for memorizing different permutations (all\nhaving the same cardinality but different costs) of a query’s fea-\ntures, which results in smaller models and better predictions. The\njoin enumeration and cost model are purposely left to the query\noptimizer.\nWe evaluate our approach using the real-world IMDb dataset [ 16]\nand show that our technique is more robust than sampling-based\ntechniques and even is competitive in the sweet spot of these\ntechniques (i.e., when there are many qualifying samples). This\nis achieved using a (configurable) low footprint size of about 3 MiB\n(whereas the sampling-based techniques have access to indexes\ncovering the entire database). These results are highly promising\nand indicate that ML might indeed be the right hammer for the\ndecades-old cardinality estimation job.\n2 RELATED WORK\nDeep learning has been applied to query optimization by three\nrecent papers [ 13,23,27] that formulate join ordering as a reinforce-\nment learning problem and use ML to find query plans . This work, in\ncontrast, applies supervised learning to solve cardinality estimation\nin isolation. This focus is motivated by the fact that modern join\nenumeration algorithms can find the optimal join order for queries\nwith dozens of relations [ 26]. Cardinality estimation, on the otherarXiv:1809.00677v2  [cs.DB]  18 Dec 2018\n\nCIDR’19, January 13-16, 2019, Asilomar, California, USA Kipf et al.\nhand, has been called the “Achilles heel” of query optimization [ 21]\nand causes most of its performance issues [16].\nTwenty years ago the first approaches to use neural networks\nfor cardinality estimation where published for UDF predicates [ 14].\nAlso, regression-based models have been used before for cardinality\nestimation [ 1]. A semi-automatic alternative for explicit machine\nlearning was presented in [ 22], where the feature space is parti-\ntioned using decision trees and for each split a different regres-\nsion model was learned. These early approaches did not use deep\nlearning nor included features derived from statistics, such as our\nsample-based bitmaps, which encode exactly which sample tuples\nwere selected (and we therefore believe to be good starting points\nfor learning correlations). The same holds for approaches that used\nmachine learning to predict overall resource consumption: running\ntime, memory footprint, I/O, network traffic [ 6,19], although these\nmodels did include course-grained features (the estimated cardinal-\nity) based on statistics into the features. Liu et al. [ 20] used modern\nML for cardinality estimation, but did not focus on joins, which are\nthe key estimation challenge [16].\nOur approach builds on sampling-based estimation by includ-\ning cardinalities or bitmaps derived from samples into the training\nsignal. Most sampling proposals create per-table samples/sketches\nand try to combine them intelligently in joins [ 3,5,30,31]. While\nthese approaches work well for single-table queries, they do not\ncapture join-crossing correlations and are vulnerable to the 0-tuple\nproblem (cf. Section 4.2). Recent work by Müller et al. [ 25] aims\nto reduce the 0-tuple problem for conjunctive predicates (albeit at\nhigh computational cost), but still cannot capture the basic case of a\nsingle predicate giving zero results. Our reasonably good estimates\nin 0-tuple situations make MSCN improve over sampling, includ-\ning even the idea of estimation on materialized join samples (join\nsynopses [28]), which still would not handle 0-tuple situations.\n3 LEARNED CARDINALITIES\nFrom a high-level perspective, applying machine learning to the\ncardinality estimation problem is straightforward: after training a\nsupervised learning algorithm with query/output cardinality pairs,\nthe model can be used as an estimator for other, unseen queries.\nThere are, however, a number of challenges that determine whether\nthe application of machine learning will be successful: the most\nimportant question is how to represent queries (“featurization”) and\nwhich supervised learning algorithm should be used. Another issue\nis how to obtain the initial training dataset (“cold start problem”).\nIn the remainder of this section, we first address these questions\nbefore discussing a key idea of our approach, which is to featurize\ninformation about materialized samples.\n3.1 Set-Based Query Representation\nWe represent a query q∈Qas a collection(Tq,Jq,Pq)of a set of\ntables Tq⊂T, a set of joins Jq⊂Jand a set of predicates Pq⊂P\nparticipating in the specific query q.T,J, and Pdescribe the sets of\nall available tables, joins, and predicates, respectively.\nEach table t∈Tis represented by a unique one-hot vectorvt(a\nbinary vector of length |T|with a single non-zero entry, uniquely\nidentifying a specific table) and optionally the number of quali-\nfying base table samples or a bitmap indicating their positions.\nLinearLinearReLUReLULinearLinearReLUReLULinearLinearReLUReLUAvg. poolConcatLinearLinearReLUSigmoid\nAvg. poolAvg. pool\nTable set TqJoin set JqPredicate set PqCardinality prediction woutAverage over setConcatenate output of each set moduleFigure 1: Architecture of our multi-set convolutional net-\nwork. Tables, joins, and predicates are represented as sep-\narate modules, comprised of one two-layer neural network\nper set element with shared parameters. Module outputs are\naveraged, concatenated, and fed into a final output network.\nSimilarly, we featurize joins j∈Jwith a unique one-hot encoding.\nFor predicates of the form (col,op,val), we featurize columns col\nand operators opusing a categorical representation with respective\nunique one-hot vectors, and represent valas a normalized value\n∈[0,1], normalized using the minimum and maximum values of\nthe respective column.\nApplied to the query representation (Tq,Jq,Pq), our MSCN model\n(cf. Figure 1) takes the following form:\nTable module: wT=1\n|Tq|Í\nt∈TqMLP T(vt)\nJoin module: wJ=1\n|Jq|Í\nj∈JqMLP J(vj)\nPredicate module: wP=1\n|Pq|Í\np∈PqMLP P(vp)\nMerge & predict: wout=MLP out([wT,wJ,wP])\nFigure 2 shows an example of a featurized query.\n3.2 Model\nStandard deep neural network architectures such as convolutional\nneural networks (CNNs), recurrent neural networks (RNNs), or\nsimple multi-layer perceptrons (MLPs) are not directly applicable\nto this type of data structure, and would require serialization , i.e.,\nconversion of the data structure to an ordered sequence of elements.\nThis poses a fundamental limitation, as the model would have to\nspend capacity to learn to discover the symmetries and structure\nof the original representation. For example, it would have to learn\nto discover boundaries between different sets in a data structure\nconsisting of multiple sets of different size, and that the order of\nelements in the serialization of a set is arbitrary.\nGiven that we know the underlying structure of the data a priori ,\nwe can bake this information into the architecture of our deep\n\nLearned Cardinalities CIDR’19, January 13-16, 2019, Asilomar, California, USA\nSELECT COUNT(*) FROM title t, movie_companies mc WHERE t.id = mc.movie_id AND t.production_year > 2010 AND mc.company_id = 5{ [ 0 1 0 1 … 0 ], [ 0 0 1 0 … 1 ] }{ [ 0 0 1 0 ] }{ [ 1 0 0 0 0 1 0 0 0.72 ], [ 0 0 0 1 0 0 1 0 0.14 ] }Table setJoin setPredicate settable idsamplesjoin idoperator idcolumn idvalue\nFigure 2: Query featurization as sets of feature vectors.\nlearning model and effectively provide it with an inductive bias that\nfacilitates generalization to unseen instances of the same structure,\ne.g., combinations of sets with a different number of elements not\nseen during training.\nHere, we introduce the multi-set convolutional network (MSCN)\nmodel. Our model architecture is inspired by recent work on Deep\nSets[32], a neural network module for operating on sets. A Deep\nSets module (sometimes referred to as set convolution ) rests on the\nobservation that any function f(S)on a set Sthat is permutation\ninvariant to the elements in Scan be decomposed into the form\nρ[Í\nx∈Sϕ(x)]with appropriately chosen functions ρandϕ. For\na more formal discussion and proof of this property, we refer to\nZaheer et al. [ 32]. We choose simple fully-connected multi-layer\nneural networks (MLPs) to parameterize the functions ρandϕand\nrely on their function approximation properties [ 4] to learn flexible\nmappings f(S)for arbitrary sets S. Applying a learnable mapping\nfor each set element individually (with shared parameters) is similar\nto the concept of a 1×1convolution, often used in CNNs for image\nclassification [29].\nOur query representation consists of a collection of multiple sets,\nwhich motivates the following choice for our MSCN model archi-\ntecture: for every set S, we learn a set-specific, per-element neural\nnetwork MLP S(vs), i.e., applied on every feature vector vsfor every\nelement s∈Sindividually1. The final representation wSfor this\nset is then given by the average2over the individual transformed\nrepresentations of its elements, i.e., wS=1/|S|Í\ns∈SMLP S(vs).\nWe choose an average (instead of, e.g., a simple sum) to ease gener-\nalization to different numbers of elements in the set S, as otherwise\nthe overall magnitude of the signal would vary depending on the\nnumber of elements in S. In practice, we implement a vectorized\nversion of our model that operates on mini-batches of data. As the\nnumber of set elements in each data sample in a mini-batch can\nvary, we pad all samples with zero-valued feature vectors that act\nas dummy set elements so that all samples within a mini-batch\nhave the same number of set elements. We mask out dummy set\nelements in the averaging operation, so that only the original set\nelements contribute to the average.\nFinally, we merge the individual set representations by concate-\nnation and subsequently pass them through a final output MLP:\nwout=MLP out([wS1,wS2, . . . , wSN]), where Nis the total number\nof sets and[·,·]denotes vector concatenation. Note that this repre-\nsentation includes the special case where each set representation\nwSis transformed by a subsequent individual output function (as\nrequired by the original theorem in [ 32]). One could alternatively\n1An alternative approach here would be to combine the feature vectors before feeding\nthem into the MLP. For example, if there are multiple tables, each of them represented\nby a unique one-hot vector, we could compute the logical disjunction of these one-hot\nvectors and feed that into the model. Note that this approach does not work if we\nwant to associate individual one-hot vectors with additional information such as the\nnumber of qualifying base table samples.\n2Note that an average of one-hot vectors uniquely identifies the combination of one-hot\nvectors, e.g. which individual tables are present in the query.process each wSindividually first and only later merge and pass\nthrough another MLP. We decided to merge both steps into a single\ncomputation for computational efficiency.\nUnless otherwise noted, all MLP modules are two-layer fully-\nconnected neural networks with ReLU(x)=max(0,x)activation\nfunctions. For the output MLP, we use a sigmoid(x)=1/(1+\nexp(−x))activation function for the last layer instead and only\noutput a scalar, so that wout∈[0,1]. We use ReLU activation func-\ntions for hidden layers as they show strong empirical performance\nand are fast to evaluate. All other representation vectors wT,wJ,\nwP, and hidden layer activations of the MLPs are chosen to be\nvectors of dimension d, where dis a hyperparameter, optimized on\na separate validation set via grid search.\nWe normalize the target cardinalities ctarget as follows: we first\ntake the logarithm to more evenly distribute target values, and then\nnormalize to the interval [0,1]using the minimum and maximum\nvalue after logarithmization obtained from the training set3. The\nnormalization is invertible, so we can recover the unnormalized\ncardinality from the prediction wout∈[0,1]of our model.\nWe train our model to minimize the mean q-error [24]q(q≥1).\nThe q-error is the factor between an estimate and the true cardinal-\nity (or vice versa). We further explored using mean-squared error\nand geometric mean q-error as objectives (cf. Section 4.8). We make\nuse of the Adam [10] optimizer for training.\n3.3 Generating Training Data\nOne key challenge of all learning-based algorithms is the “cold\nstart problem”, i.e., how to train the model before having concrete\ninformation about the query workload. Our approach is to obtain\nan initial training corpus by generating random queries based on\nschema information and drawing literals from actual values in the\ndatabase.\nA training sample consists of table identifiers, join predicates,\nbase table predicates, and the true cardinality of the query result.\nTo avoid a combinatorial explosion, we only generate queries with\nup to two joins and let the model generalize to more joins. Our\nquery generator first uniformly draws the number of joins |Jq|\n(0≤|Jq|≤2) and then uniformly selects a table that is referenced\nby at least one table. For |Jq|>0, it then uniformly selects a new\ntable that can join with the current set of tables (initially only\none), adds the corresponding join edge to the query and (overall)\nrepeats this process |Jq|times. For each base table tin the query,\nit then uniformly draws the number of predicates |Ptq|(0≤|Ptq|≤\nnum non-key columns ). For each predicate, it uniformly draws the\npredicate type ( =,<, or>) and selects a literal (an actual value) from\nthe corresponding column. We configured our query generator to\nonly generate unique queries. We then execute these queries to\n3Note that this approach requires complete re-training when data changes (iff the\nminimum and maximum values have changed). Alternatively, one could set a high\nlimit for the maximum value.\n\nCIDR’19, January 13-16, 2019, Asilomar, California, USA Kipf et al.\nobtain their true result cardinalities, while skipping queries with\nempty results. Using this process, we obtain the initial training set\nfor our model.\n3.4 Enriching the Training Data\nA key idea of our approach is to enrich the training data with infor-\nmation about materialized base table samples. For each table in a\nquery, we evaluate the corresponding predicates on a materialized\nsample and annotate the query with the number of qualifying sam-\npless(0≤s≤1000 for 1000 materialized samples) for this table.\nWe perform the same steps for an (unseen) test query at estimation\ntime allowing the ML model to utilize this knowledge.\nWe even take this idea one step further and annotate each table\nin a query with the positions of the qualifying samples represented\nas bitmaps. As we show in Section 4, adding this feature has a\npositive impact on our join estimates since the ML model can now\nlearn what it means if a certain sample qualifies (e.g., there might\nbe some samples that usually have many join partners). In other\nwords, the model can learn to use the patterns in the bitmaps to\npredict output cardinalities.\n3.5 Training and Inference\nBuilding our model involves three steps: i) generate random (uni-\nformly distributed) queries using schema and data information, ii)\nexecute these queries to annotate them with their true cardinalities\nand information about qualifying materialized base table samples,\nand iii) feed this training data into an ML model. All of these steps\nare performed on an immutable snapshot of the database.\nTo predict the cardinality of a query, the query first needs to\nbe transformed into its feature representation (cf. Section 3.1). In-\nference itself involves a certain number of matrix multiplications,\nand (optionally) querying materialized base table samples (cf. Sec-\ntion 3.4). Training the model with more query samples does not\nincrease the prediction time. In that respect, the inference speed\nis largely independent from the quality of the predictions. This\nis in contrast to purely sampling-based approaches that can only\nincrease the quality of their predictions by querying more samples.\n4 EVALUATION\nWe evaluate our approach using the IMDb dataset which contains\nmany correlations and therefore proves to be very challenging for\ncardinality estimators [ 16]. The dataset captures more than 2.5 M\nmovie titles produced over 133 years by 234,997 different companies\nwith over 4 M actors.\nWe use three different query workloads4: i) a synthetic workload\ngenerated by the same query generator as our training data (using\na different random seed) with 5,000 unique queries containing both\n(conjunctive) equality and range predicates on non-key columns\nwith zero to two joins, ii) another synthetic workload scale with\n500 queries designed to show how the model generalizes to more\njoins, and iii) JOB-light , a workload derived from the Join Order\nBenchmark (JOB) [ 16] containing 70 of the original 113 queries. In\ncontrast to JOB, JOB-light does not contain any predicates on strings\nnor disjunctions and only contains queries with one to four joins.\nMost queries in JOB-light have equality predicates on dimension\n4https://github.com/andreaskipf/learnedcardinalitiesnumber of joins 0 1 2 3 4 overall\nsynthetic 1636 1407 1957 0 0 5000\nscale 100 100 100 100 100 500\nJOB-light 0 3 32 23 12 70\nTable 1: Distribution of joins.\nPostgreSQL Random Samp. IB Join Samp. MSCN (ours)\n0 1 2 0 1 2 0 1 2 0 1 21e41e211e21e4\nnumber of joins← undere.  [log scale]  overe. →\nFigure 3: Estimation errors on the synthetic workload. The\nbox boundaries are at the 25th/75th percentiles and the hor-\nizontal “whisker” lines mark the 95th percentiles.\ntable attributes. The only range predicate is on production_year .\nTable 1 shows the distribution of queries with respect to the number\nof joins in the three query workloads. The non-uniform distribution\nin the synthetic workload is caused by our elimination of duplicate\nqueries.\nAs competitors we use PostgreSQL version 10.3, Random Sam-\npling (RS), and Index-Based Join Sampling (IBJS) [ 17]. RS executes\nbase table predicates on materialized samples to estimate base ta-\nble cardinalities and assumes independence for estimating joins. If\nthere are no qualifying samples for a conjunctive predicate, it tries\nto evaluate the conjuncts individually and eventually falls back to\nusing the number of distinct values (of the column with the most\nselective conjunct) to estimate the selectivity. IBJS represents the\nstate-of-the-art for estimating joins and probes qualifying base table\nsamples against existing index structures. Our IBJS implementation\nuses the same fallback mechanism as RS.\nWe train and test our model on an Amazon Web Services (AWS)\nml.p2.xlarge instance using the PyTorch framework5and use CUDA.\nWe use 100,000 random queries with zero to two joins and 1,000\nmaterialized samples as training data (cf. Section 3.3). We split the\ntraining data into 90% training and 10% validation samples. To\nobtain true cardinalities for our training data, we use HyPer [8].\n4.1 Estimation Quality\nFigure 3 shows the q-error of MSCN compared to our competitors.\nWhile PostgreSQL’s errors are more skewed towards the positive\nspectrum, RS tends to underestimate joins, which stems from the\nfact that it assumes independence. IBJS performs extremely well in\nthe median and 75th percentile but (like RS) suffers from empty base\ntable samples. MSCN is competitive with IBJS in the median while\nbeing significantly more robust. Considering that IBJS is using much\nmore data—in the form of large primary and foreign key indexes—in\ncontrast to the very small state MSCN is using (less than 3 MiB),\n5https://pytorch.org/\n\nLearned Cardinalities CIDR’19, January 13-16, 2019, Asilomar, California, USA\nmedian 90th 95th 99th max mean\nPostgreSQL 1.69 9.57 23.9 465 373901 154\nRandom Samp. 1.89 19.2 53.4 587 272501 125\nIB Join Samp. 1.09 9.93 33.2 295 272514 118\nMSCN (ours) 1.18 3.32 6.84 30.51 1322 2.89\nTable 2: Estimation errors on the synthetic workload.\nmedian 90th 95th 99th max mean\nPostgreSQL 4.78 62.8 107 1141 21522 133\nRandom Samp. 9.13 80.1 173 993 19009 147\nMSCN 2.94 13.6 28.4 56.9 119 6.89\nTable 3: Estimation errors of 376 base table queries with\nempty samples in the synthetic workload.\nMSCN captures (join-crossing) correlations reasonably well and\ndoes not suffer as much from 0-tuple situations (cf. Section 4.2).\nTo provide more details, we also show the median, percentiles,\nmaximum, and mean q-errors in Table 2. While IBJS provides the\nbest median estimates, MSCN outperforms the competitors by up\nto two orders of magnitude at the end of the distribution.\n4.2 0-Tuple Situations\nPurely sampling-based approaches suffer from empty base table\nsamples (0-tuple situations) which can occur under selective predi-\ncates. While this situation can be mitigated using, e.g., more sam-\nples or employing more sophisticated—yet still sampling-based—\ntechniques (e.g., [ 25]), it remains inherently difficult to address by\nthese techniques. In this experiment, we show that deep learning,\nand MSCN in particular, can handle such situations fairly well.\nIn fact, 376 (22%) of the 1636 base table queries in the synthetic\nworkload have empty samples (using MSCN’s random seed). We\nwill use this subset of queries to illustrate how MSCN deals with\nsituations where it cannot build upon (runtime) sampling informa-\ntion (i.e., all bitmaps only contain zeros). We also include Random\nSampling (which uses the same random seed—i.e., the same set of\nmaterialized samples as MSCN) and PostgreSQL in this experiment.\nThe results, shown in Table 3, demonstrate that MSCN addresses\nthe weak spot of purely sampling-based techniques and therefore\nwould complement them well.\nRecall that Random Sampling extrapolates the output cardinality\nbased on the number of qualifying samples (zero in this case). Thus,\nit cannot simply extrapolate from this number and has to fall back\nto an educated guess—in our RS implementation either using the\nproduct of selectivities of individual conjuncts or using the number\nof distinct values of the column with the most selective predicate.\nIndependent of the concrete implementation of this fallback, it\nremains an educated guess. MSCN, in contrast, can use the signal\nof individual query features (in this case the specific table and\npredicate features) to provide a more precise estimate.\n4.3 Removing Model Features\nNext, we highlight the contributions of individual model features\nto the prediction quality (cf. Figure 4). MSCN (no samples) is the\nmodel without any (runtime) sampling features, MSCN (#samples)\nMSCN (no samples) MSCN (#samples) MSCN (bitmaps)\n0 1 2 0 1 2 0 1 21e41e211e21e4\nnumber of joins← undere.  [log scale]  overe. →Figure 4: Estimation errors on the synthetic workload with\ndifferent model variants.\nPostgreSQL MSCN\n0 1 2 3 4 0 1 2 3 41e41e211e21e4\nnumber of joins← undere.  [log scale]  overe. →\nFigure 5: Estimation errors on the scale workload showing\nhow MSCN generalizes to queries with more joins.\nrepresents the model with one cardinality (i.e., the number of quali-\nfying samples) per base table, and MSCN (bitmaps) denotes the full\nmodel with one bitmap per base table.\nMSCN (no samples) produces reasonable estimates with an over-\nall 95th percentile q-error of 25.3, purely relying on (inexpensive to\nobtain) query features. Adding sample cardinality information to\nthe model improves both base table and join estimates. The 95th\npercentile q-errors of base table, one join, and two join estimates\nreduce by 1.72×, 3.60×, and 3.61×, respectively. Replacing cardinal-\nities with bitmaps further improves these numbers by 1.47 ×, 1.35×,\nand 1.04×. This shows that the model can use the information\nembedded in the bitmaps to provide better estimates.\n4.4 Generalizing to More Joins\nTo estimate a larger query, one can of course break the query down\ninto smaller sub queries, estimate them individually using the model,\nand combine their selectivities. However, this means that we would\nneed to assume independence between two sub queries which is\nknown to deliver poor estimates with real-world datasets such as\nIMDb (cf. join estimates of Random Sampling in Section 4).\nThe question that we want to answer in this experiment is how\nMSCN can generalize to queries with more joins than it was trained\non. For this purpose, we use the scale workload with 500 queries\nwith zero to four joins (100 queries each). Recall that we trained the\nmodel only with queries that have between zero and two joins. Thus,\nthis experiment shows how the model can estimate queries with\nthree and four joins without having seen such queries during train-\ning (cf. Figure 5). From two to three joins, the 95th percentile q-error\n\nCIDR’19, January 13-16, 2019, Asilomar, California, USA Kipf et al.\nmedian 90th 95th 99th max mean\nPostgreSQL 7.93 164 1104 2912 3477 174\nRandom Samp. 11.5 198 4073 22748 23992 1046\nIB Join Samp. 1.59 150 3198 14309 15775 590\nMSCN 3.82 78.4 362 927 1110 57.9\nTable 4: Estimation errors on the JOB-light workload.\nincreases from 7.66 to 38.6. To give a point of reference, PostgreSQL\nhas a 95th percentile q-error of 78.0 for the same queries. And fi-\nnally, with four joins, MSCN’s 95th percentile q-error increases\nfurther to 2,397 (PostgreSQL: 4,077).\nNote that 58 out of the 500 queries in this workload exceed\nthe maximum cardinality seen during training. 12 of these queries\nhave three joins and another 46 have four joins. When excluding\nthese outliers, the 95th percentile q-errors for three and four joins\ndecrease to 23.8 and 175, respectively.\n4.5 JOB-light\nTo show how MSCN generalizes to a workload that was not gener-\nated by our query generator, we use JOB-light.\nTable 4 shows the estimation errors. Recall that most queries in\nJOB-light have equality predicates on dimension table attributes.\nConsidering that MSCN was trained with a uniform distribution\nbetween =,<, and >predicates, it performs reasonably well. Also,\nJOB-light contains many queries with a closed range predicate\nonproduction_year , while the training data only contains open\nrange predicates. Note that JOB-light also includes five queries\nthat exceed the maximum cardinality that MSCN was trained on.\nWithout these queries, the 95th percentile q-error is 115.\nIn summary, this experiment shows that MSCN can generalize\nto workloads with distributions different from the training data.\n4.6 Hyperparameter Tuning\nWe tuned the hyperparameters of our model, including the number\nof epochs (the number of passes over the training set), the batch\nsize (the size of a mini-batch), the number of hidden units, and\nthe learning rate. More hidden units means larger model sizes and\nincreased training and prediction costs with the upside of allowing\nthe model to capture more data, while learning rate and batch size\nboth influence convergence behavior during training.\nWe varied the number of epochs (100, 200), the batch size (64,\n128, 256, 512, 1024, 2048), the number of hidden units (64, 128, 256,\n512, 1024, 2048), and fixed the learning rate to 0.001, resulting in 72\ndifferent configurations. For each configuration, we trained three\nmodels6using 90,000 samples and evaluated their performance\non the validation set consisting of 10,000 samples. On average\nover the three runs, the configuration with 100 epochs, a batch\nsize of 1024 samples, and 256 hidden units performed best on the\nvalidation data. Across many settings, we observed that 100 epochs\nperform better than 200 epochs. This is an effect of overfitting: the\nmodel captures the noise in the training data such that it negatively\nimpacts its prediction quality. Overall, we found that our model\n6Note that the weights of the neural network are initialized using a different random\nseed in each training run. To provide reasonably stable numbers, we tested each\nconfiguration three times.\n●\n●\n●●●●●●●\n●●\n●●●\n●●\n●●●●\n●●\n●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●● 369\n0 25 50 75 100\nnumber of epochsmean q−errorFigure 6: Convergence of the mean q-error on the validation\nset with the number of epochs.\nperforms well across a wide variety of settings. In fact, the mean\nq-error only varied by 1% within the best 10 configurations and\nby 21% between the best and the worst configuration. We also\nexperimented with different learning rates (0.001, 0.005, 0.0001) and\nfound 0.001 to perform best. We thus use 100 epochs, a batch size\nof 1024, 256 hidden units, and a learning rate of 0.001 as our default\nconfiguration.\n4.7 Model Costs\nNext, we analyze the training, inference, and space costs of MSCN\nwith our default hyperparameters. Figure 6 shows how the vali-\ndation set error (the mean q-error of all queries in the validation\nset) decreases with more epochs. The model requires fewer than\n75 passes (over the 90,000 training queries) to converge to a mean\nq-error of around 3 on the 10,000 validation queries. An average\ntraining run with 100 epochs (measured across three runs) takes\nalmost 39 minutes.\nThe prediction time of our model is in the order of a few millisec-\nonds, including the overhead introduced by the PyTorch framework.\nIn theory (neglecting the PyTorch overhead), a prediction using\na deep learning model (as stated earlier) is dominated by matrix\nmultiplications which can be accelerated using modern GPUs. We\nthus expect performance-tuned implementations of our model to\nachieve very low prediction latencies. Since we incorporate sam-\npling information, the end-to-end prediction time will be in the\nsame order of magnitude as that of (per-table) sampling techniques.\nThe size of our model (when serialized to disk) is 1.6 MiB, 1.6 MiB,\nand 2.6 MiB for MSCN (no samples), MSCN (#samples), and MSCN\n(bitmaps), respectively.\n4.8 Optimization Metrics\nBesides optimizing the mean q-error, we also explored using mean-\nsquared error and geometric mean q-error as optimization goals.\nMean-squared error would optimize the squared differences be-\ntween the predicted and true cardinalities. Since we are more in-\nterested in minimizing the factor between the predicted and the\ntrue cardinalities (q-error) and use this metric for our evaluation,\noptimizing the q-error directly yielded better results. Optimizing\nthe geometric mean of the q-error makes the model put less em-\nphasis on heavy outliers (that would lead to large errors). While\nthis approach looked promising at first, it turned out to be not as\nreliable as optimizing the mean q-error.\n\nLearned Cardinalities CIDR’19, January 13-16, 2019, Asilomar, California, USA\n5 DISCUSSION\nWe have shown that our model can beat state-of-the-art approaches\nfor cardinality estimation. It does a good job in addressing 0-tuple\nsituations and in capturing join-crossing correlations, especially\nwhen combined with runtime sampling. To make it suitable for\ngeneral-purpose cardinality estimation, it can be extended into\nmultiple dimensions, including complex predicates, uncertainty\nestimation, and updatability. In the following, we will discuss these\nand sketch possible solutions.\nGeneralization. MSCN can to some extent generalize to queries\nwith more joins than seen during training (cf. Section 4.4). Never-\ntheless, generalizing to queries that are not in the vicinity of the\ntraining data remains challenging.\nOf course, our model can be trained with queries from an actual\nworkload or their structures. In practice, we could replace any\nliterals in user queries with placeholders to be filled with actual\nvalues from the database. This would allow us to focus on the\nrelevant joins and predicates.\nAdaptive training. To improve training quality, we could adap-\ntively generate training samples: based on the error distribution\nof queries in the validation set, we would generate new training\nsamples that shine more light on difficult parts of the schema.\nStrings. A simple addition to our current implementation are\nequality predicates on strings. To support these, we could hash\nthe string literals to a (small) integer domain. Then an equality\npredicate on a string is essentially the same as an equality predicate\non an ID column where the model also needs to process a non-linear\ninput signal.\nComplex predicates. Currently, our model can only estimate\nqueries with predicate types that it has seen during training. Com-\nplex predicates, such as LIKE or disjunctions, are not yet supported\nsince we currently do not represent them in the model. An idea\nto allow for any complex predicate would be to purely rely on the\nsampling bitmaps in such cases. Note that this would make our\nmodel vulnerable to 0-tuple situations. To mitigate that problem, we\ncould featurize information from histograms. Also, the distribution\nof bitmap patterns might vary significantly from simple predicates\nobserved at training time, to more complicated predicates at test\ntime, which can make generalization challenging.\nMore bitmaps. At the moment, we use a single bitmap indicating\nthe qualifying samples per base table. To increase the likelihood\nfor qualifying samples, we could additionally use one bitmap per\npredicate. For example, for a query with two conjunctive base\ntable predicates, we would have one bitmap for each predicate, and\nanother bitmap representing the conjunction. In a column store\nthat evaluates one column at a time, we can obtain this information\nalmost for free. We have already shown that MSCN can use the\ninformation embedded in the bitmaps to make better predictions.\nWe expect that it would benefit from the patterns in these additional\nbitmaps.\nThis approach should also help MSCN with estimating queries\nwith arbitrary (complex) predicates where it needs to rely on infor-\nmation from the (many) bitmaps. Of course, this approach does notwork in 0-tuple situations, or more specifically in situations where\nnone of the (predicate) bitmaps indicates any qualifying samples.\nUncertainty estimation. An open question is when to actually\ntrust the model and rely on its predictions. One approach is to use\nstrict constraints for generating the training data and enforce them\nat runtime, i.e., only use the model when all constraints hold (i.e.,\nonly PK/FK joins, only equality predicates on certain columns).\nA more appealing approach would be to implement uncertainty\nestimation into the model. However, for a model like ours, this is a\nnon-trivial task and still an area of active research. There are some\nrecent methods [ 7,9,15] that we plan to investigate in future work.\nUpdates. Throughout this work, we have assumed an immutable\n(read-only) database. To handle data and schema changes, we can\neither completely re-train the model or we can apply some modifi-\ncations to our model that allow for incremental training.\nComplete re-training comes with considerable compute costs\n(for re-executing queries to obtain up-to-date cardinalities and for\nre-training the model) but would allow us to use a different data\nencoding. For example, we could use larger one-hot vectors to\naccommodate for new tables and we could re-normalize values\nin case of new minimum or maximum values. Queries (training\nsamples) of which we know to still have the same cardinality (e.g.,\nsince there has not been any update to the respective data range)\nwould of course not need to re-executed.\nIn contrast, incremental training (as implied by its name) would\nnot require us to re-train with the original set of samples. Instead,\nwe could re-use the model state and only apply new samples. One\nchallenge with incremental training is to accommodate changes in\nthe data encoding, including one-hot encodings and the normal-\nization of values. To recall, there are two types of values that we\nnormalize: literals in predicates (actual column values) and output\ncardinalities (labels). For both types, setting a high limit on the max-\nimum value seems most appropriate. The main challenge, however,\nis to address catastrophic forgetting , which is an effect that can be\nobserved with neural networks when data distribution shifts over\ntime. The network would overfit to the most recent data and forget\nwhat it has learned in the past. Addressing this problem is an area\nof active research with some recent proposals [11].\n6 CONCLUSIONS\nWe have introduced a new approach to cardinality estimation based\non MSCN, a new deep learning model. We have trained MSCN\nwith generated queries, uniformly distributed within a constrained\nsearch space. We have shown that it can learn (join-crossing) cor-\nrelations and that it addresses the weak spot of sampling-based\ntechniques, which is when no samples qualify. Our model is a first\nstep towards reliable ML-based cardinality estimation and can be\nextended into multiple dimensions, including complex predicates,\nuncertainty estimation, and updatability.\nAnother application of our set-based model is the prediction of\nthe number of unique values in a column or in a combination of\ncolumns (i.e., estimating the result size of a group-by operator).\nThis is another hard problem where current approaches achieve\nundesirable results and where machine learning seems promising.\n\nCIDR’19, January 13-16, 2019, Asilomar, California, USA Kipf et al.\n7 ACKNOWLEDGEMENTS\nThis work has been partially supported by the German Federal Min-\nistry of Education and Research (BMBF) grant 01IS12057 (FAST-\nDATA). It is further part of the TUM Living Lab Connected Mobility\n(TUM LLCM) project and has been funded by the Bavarian Min-\nistry of Economic Affairs, Energy and Technology (StMWi) through\nthe Center Digitisation.Bavaria, an initiative of the Bavarian State\nGovernment. T.K. acknowledges funding by SAP SE.\nREFERENCES\n[1]M. Akdere, U. Çetintemel, M. Riondato, E. Upfal, and S. B. Zdonik. Learning-based\nquery performance modeling and prediction. In ICDE , pages 390–401, 2012.\n[2]D. V. Aken, A. Pavlo, G. J. Gordon, and B. Zhang. Automatic database management\nsystem tuning through large-scale machine learning. In SIGMOD , 2017.\n[3]Y. Chen and K. Yi. Two-level sampling for join size estimation. In SIGMOD , 2017.\n[4]G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathe-\nmatics of control, signals and systems , 2(4), 1989.\n[5]C. Estan and J. F. Naughton. End-biased samples for join cardinality estimation.\nInICDE , 2006.\n[6]A. Ganapathi, H. A. Kuno, U. Dayal, J. L. Wiener, A. Fox, M. I. Jordan, and D. A.\nPatterson. Predicting multiple metrics for queries: Better decisions enabled by\nmachine learning. In ICDE , pages 592–603, 2009.\n[7]C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. On calibration of modern neural\nnetworks. In ICML , 2017.\n[8]A. Kemper and T. Neumann. HyPer: A Hybrid OLTP & OLAP Main Memory\nDatabase System Based on Virtual Memory Snapshots. In ICDE , pages 195–206.\nIEEE Computer Society, Apr. 2011.\n[9]A. Kendall and Y. Gal. What uncertainties do we need in bayesian deep learning\nfor computer vision? In Advances in neural information processing systems , pages\n5574–5584, 2017.\n[10] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization.\narXiv:1412.6980 , 2014.\n[11] J. Kirkpatrick, R. Pascanu, N. C. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu,\nK. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath,\nD. Kumaran, and R. Hadsell. Overcoming catastrophic forgetting in neural\nnetworks. CoRR , abs/1612.00796, 2016.\n[12] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for learned\nindex structures. In SIGMOD , 2018.\n[13] S. Krishnan, Z. Yang, K. Goldberg, J. Hellerstein, and I. Stoica. Learning to\noptimize join queries with deep reinforcement learning. arXiv:1808.03196 , 2018.[14] M. S. Lakshmi and S. Zhou. Selectivity estimation in extensible databases - A\nneural network approach. In VLDB , pages 623–627, 1998.\n[15] B. Lakshminarayanan, A. Pritzel, and C. Blundell. Simple and scalable predictive\nuncertainty estimation using deep ensembles. In Advances in Neural Information\nProcessing Systems , pages 6402–6413, 2017.\n[16] V. Leis, A. Gubichev, A. Mirchev, P. Boncz, A. Kemper, and T. Neumann. How\ngood are query optimizers, really? PVLDB , 9(3), 2015.\n[17] V. Leis, B. Radke, A. Gubichev, A. Kemper, and T. Neumann. Cardinality estimation\ndone right: Index-based join sampling. In CIDR , 2017.\n[18] V. Leis, B. Radke, A. Gubichev, A. Mirchev, P. Boncz, A. Kemper, and T. Neumann.\nQuery optimization through the looking glass, and what we found running the\nJoin Order Benchmark. The VLDB Journal , 2018.\n[19] J. Li, A. C. König, V. R. Narasayya, and S. Chaudhuri. Robust estimation of\nresource consumption for SQL queries using statistical techniques. PVLDB ,\n5(11):1555–1566, 2012.\n[20] H. Liu, M. Xu, Z. Yu, V. Corvinelli, and C. Zuzarte. Cardinality estimation using\nneural networks. In CASCON , 2015.\n[21] G. Lohman. Is query optimization a solved problem? http://wp.sigmod.org/?p=\n1075, 2014.\n[22] T. Malik, R. C. Burns, and N. V. Chawla. A black-box approach to query cardinality\nestimation. In CIDR , pages 56–67, 2007.\n[23] R. Marcus and O. Papaemmanouil. Deep reinforcement learning for join or-\nder enumeration. In International Workshop on Exploiting Artificial Intelligence\nTechniques for Data Management , 2018.\n[24] G. Moerkotte, T. Neumann, and G. Steidl. Preventing bad plans by bounding the\nimpact of cardinality estimation errors. PVLDB , 2(1):982–993, 2009.\n[25] M. Müller, G. Moerkotte, and O. Kolb. Improved selectivity estimation by com-\nbining knowledge from sampling and synopses. PVLDB , 11(9):1016–1028, 2018.\n[26] T. Neumann and B. Radke. Adaptive optimization of very large join queries. In\nSIGMOD , 2018.\n[27] J. Ortiz, M. Balazinska, J. Gehrke, and S. S. Keerthi. Learning state representations\nfor query optimization with deep reinforcement learning. In Workshop on Data\nManagement for End-To-End Machine Learning , 2018.\n[28] V. Poosala and Y. E. Ioannidis. Selectivity estimation without the attribute value\nindependence assumption. In VLDB , 1997.\n[29] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the\ninception architecture for computer vision. In Proceedings of the IEEE conference\non computer vision and pattern recognition , pages 2818–2826, 2016.\n[30] D. Vengerov, A. C. Menck, M. Zaït, and S. Chakkappen. Join size estimation\nsubject to filter conditions. PVLDB , 8(12):1530–1541, 2015.\n[31] W. Wu, J. F. Naughton, and H. Singh. Sampling-based query re-optimization. In\nSIGMOD , pages 1721–1736, 2016.\n[32] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. R. Salakhutdinov, and A. J.\nSmola. Deep sets. In Advances in Neural Information Processing Systems , 2017.",
  "textLength": 46769
}