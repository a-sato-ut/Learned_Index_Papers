{
  "paperId": "f7e3086a8b4417e2fbf7f7d26d9fceb225328861",
  "title": "Towards a Hands-Free Query Optimizer through Deep Learning",
  "pdfPath": "f7e3086a8b4417e2fbf7f7d26d9fceb225328861.pdf",
  "text": "Towards a Hands-Free Query Optimizer through Deep\nLearning\nRyan Marcus\nBrandeis University\nryan@cs.brandeis.eduOlga Papaemmanouil\nBrandeis University\nolga@cs.brandeis.edu\nABSTRACT\nQuery optimization remains one of the most important and\nwell-studied problems in database systems. However, tradi-\ntional query optimizers are complex heuristically-driven sys-\ntems, requiring large amounts of time to tune for a particular\ndatabase and requiring even more time to develop and main-\ntain in the \frst place. In this vision paper, we argue that a\nnew type of query optimizer, based on deep reinforcement\nlearning, can drastically improve on the state-of-the-art. We\nidentify potential complications for future research that in-\ntegrates deep learning with query optimization, and describe\nthree novel deep learning based approaches that can lead the\nway to end-to-end learning-based query optimizers.\n1. INTRODUCTION\nQuery optimization, e.g. transforming SQL queries into\nphysical execution plans with good performance, is a critical\nand well-studied problem in database systems (e.g. [3, 10,\n31,35]). Despite their long research history, the majority of\nexisting query optimization systems share two problematic\nproperties:\n1. They are, or are composed of, carefully tuned and com-\nplex heuristics designed using many years of developer-\nbased experience. Furthermore, these heuristics often re-\nquire even more tuning by expert DBAs to improve query\nperformance on each individual database (e.g. tweaking\noptimization time cuto\u000bs, adding query hints, updating\nstatistics, tuning optimizer \\knobs\").\n2. They take a \\ \fre and forget \" approach in which the ob-\nserved performance of a execution plan is never leveraged\nby the optimization process in the future, hence prevent-\ning query optimizers from systematically \\learning from\ntheir mistakes.\"\nOf course, there are several notable exceptions. Many op-\ntimizers use feedback from query execution to update cardi-\nnality estimates [1,7,32], and many adaptive query process-\ning systems [13, 34] incorporate feedback as well. However,\nin this vision paper, we argue that recent advances in deep\nThis article is published under a Creative Commons Attribution License\n(http://creativecommons.org/licenses/by/3.0/), which permits distribution\nand reproduction in any medium as well allowing derivative works, pro-\nvided that you attribute the original work to the author(s) and CIDR 2019.\n9th Biennial Conference on Innovative Data Systems Research (CIDR â€˜19)\nJanuary 13-16, 2019 , Asilomar, California, USA.reinforcement learning (DRL) [2] can be applied to query\noptimization, resulting in a \\hands-free\" optimizer that (1)\ncan tune itself for a particular database automatically with-\nout requiring intervention from expert DBAs, and (2) tightly\nincorporates feedback from past query optimizations and ex-\necutions in order to improve the performance of query exe-\ncution plans generated in the future.\nDeep reinforcement learning is a process in which a ma-\nchine learns a task through continuous feedback with the\nhelp of a neural network [28]. It is a iterative learning pro-\ncess where the machine (an agent ) repeatedly selects actions\nand receives feedback about the quality of the actions se-\nlected. DRL algorithms train a neural network model over\nmultiple rounds ( episodes ), aiming to maximize the perfor-\nmance of their selected actions ( policies ). This performance\nfeedback, the indicator of whether or not an agent is per-\nforming well or poorly, is referred to as the reward signal .\nWhile deep learning has been previously applied to database\nsystems (e.g. indexes [15], physical design [23], and en-\ntity matching [21]), deep reinforcement learning has not\nreceived much attention. Despite applications in multiple\ndomains [2], applying DRL algorithms to query optimiza-\ntion generates a number of research challenges. First, DRL\nalgorithms initially perform very poorly, and require exten-\nsive training data before achieving competitive performance.\nSecond, it is generally assumed that that the reward signal is\ncheap to calculate. In query optimization, the most natural\nperformance indicator to use is the query latency. However,\ntraining on (and hence executing) large numbers of query\nplans (especially poorly optimized query plans) and collect-\ning their latency for feedback as a reward signal to a DRL\nagent can be extremely expensive. Using the optimizer's\ncost model as a performance indicator is also problematic,\nas cost models are themselves complex, brittle, and often\nrely on inaccurate statistics and oversimpli\fed assumptions.\nSecond, the enormous size of the query plan search space\nfor any given query causes naive applications of DRL to fail.\nFor instance, while DRL can be used to learn policies that\ntackle join order enumeration [18], training these models to\nadditionally capture physical operator and access path selec-\ntion dramatically lengthens the training process and hinders\nconvergence to an e\u000bective policy.\nIn this vision paper, we describe and analyze potential so-\nlutions to the above challenges, each representing directions\nfor further research that tightly integrates deep learning-\nbased theory with query optimization. We propose two\nnovel DRL approaches: learning from demonstration and\ncost model bootstrapping . The \frst approach involves ini-arXiv:1809.10212v2  [cs.DB]  18 Dec 2018\n\ntially training a model to imitate the behavior of a state-\nof-the-art query optimizer, and then \fne-tuning that model\nfor increased performance. The second approach involves\nusing existing cost models as guides to help DRL models\nlearn more quickly. Finally, we propose and analyze the\ndesign space of an incremental training approach that in-\nvolves learning the complexities of query optimization in a\nstep-by-step fashion.\nWe start in Section 2 with an brief introduction to DRL\nand an overview of a case study DRL-based join enumer-\nator in Section 3. In Section 4, we detail the three main\nchallenges that DRL-based query optimizers need to over-\ncome. In Section 5, we analyze our proposed future research\ndirections, and we conclude in Section 6.\n2. DEEP REINFORCEMENT LEARNING\nReinforcement Learning (RL) [36] is a machine learning\ntechnique that enables an agent to learn in an interactive\nenvironment by trial and error using feedback from its own\nactions and experiences. More formally, an agent inter-\nacts with an environment . The environment tells the agent\nits current state, st, and a set of potential actions At=\nfa0;a1;:::;angthat the agent may perform. The agent se-\nlects an action a2At, and the environment gives the agent\narewardrtbased on that action. The environment addition-\nally gives the agent a new state st+1and a new action set\nAt+1. This process repeats until the agent reaches a termi-\nnal state , where no more actions are available. This marks\nthe end of an episode , after which a new episode begins.\nThe agent's goal is to maximize its reward over episodes by\nlearning from its experience (previous actions, states, and re-\nwards). This is achieved by balancing the exploration of new\nnever-before-tried actions with the exploitation of knowledge\ncollected from past actions.\nPolicy Gradient One subset of reinforcement learning tech-\nniques is policy gradient methods [37]. Here the agents se-\nlect actions based on a parameterized policy\u0019\u0012, where\u0012is a\nvector that represents the policy parameters. Given a state\nstand an action set At, the policy \u0019\u0012outputs one of the\npotential actions from At.\nReinforcement learning aims to optimize the policy \u0019\u0012\nover episodes, i.e., to identify the policy parameters \u0012that\noptimizes the expected reward. The expected reward that a\npolicy will receive per episode is denoted J\u0019(\u0012). A reinforce-\nment learning agent thus seeks the vector \u0012that maximizes\nthe reward J\u0019(\u0012), but the reward J\u0019(\u0012) is typically not fea-\nsible to precisely compute. Hence, policy gradient methods\nsearch for such a vector \u0012by constructing an estimator Eof\nthegradient of the expected reward: E(\u0012)\u0019r\u0012J\u0019(\u0012).\nReal-world applications require that any change to the\npolicy parameterization has to be smooth, as drastic changes\ncan (1) be hazardous for the system and (2) cause the pol-\nicy to \ructuate too severely, without ever converging. For\nthese reasons, given an estimate E,gradient ascent/descent\nmethods [25] tune the initial parameters \u0012by increasing each\nparameter in \u0012iby a small value when the gradient r\u0012iJ\u0019(\u0012)\nis positive (the positive gradient indicates that a larger value\nof\u0012iwill increase the reward), and decreasing the parame-\nters in\u0012iby a small value when the gradient is negative.\nDeep Reinforcement Learning In DRL, policy gradient\ndeep learning methods (e.g., [29,30]) represent the policy \u0019\u0012\nas a neural network, where \u0012is the network weights. The\npolicy is improved by adjusting the weights of the networkbased on the reward signal from the environment. Here,\nthe neural network receives as input a representation of the\ncurrent state, and transforms it through a number of hid-\nden layers. Each layer transforms (through an activation\nfunction) its input data and and passes its output to the\nsubsequent layer. Eventually, data is passed to the \fnal\naction layer. Each neuron in the action layer represents an\naction, and these outputs are normalized to form a probabil-\nity distribution. The policy selects actions by sampling from\nthis probability distribution, aiming to balance exploration\nand exploitation. Selecting the mode of the distribution in-\nstead of sampling from the distribution would represent a\npure exploitation strategy. Choosing an action uniformally\natrandom would represent a pure exploration strategy.\n3. CASE STUDY: REJOIN\nOne of the key challenges in applying RL to a particular\ndomain is \\massaging\" the problem into the terms of rein-\nforcement learning (i.e., designing its actions, states, and\nrewards). In this section, we present a case study of Re-\nJOIN, a deep reinforcement learning join order enumerator.\nWe \frst give a brief overview1of ReJOIN, and highlight key\nexperimental results. While ReJOIN focused exclusively on\njoin order enumeration (it did not perform operator or index\nselection), it represents an example of how query optimiza-\ntion may be framed in the terms of reinforcement learning.\nOverview ReJOIN performs join ordering in a bottom-up\nfashion, modeling the problem in the terms of reinforcement\nlearning. Each query sent to the optimizer represents an\nepisode, and ReJOIN learns over multiple episodes (i.e., con-\ntinuously learning as queries are sent). Each state represents\nsubtrees of a binary join tree, in addition to information\nabout query join and selection predicates. Each action rep-\nresents combining two subtrees together into a single tree.\nA subtree can represent either an input relation or a join be-\ntween subtrees. The episode ends when all input relations\nare joined (a terminal state). At this point, ReJOIN assigns\na reward to the \fnal join ordering based on the optimizer's\ncost model. The \fnal join ordering is sent to the optimizer\nto perform operator selection, index selection, etc., and the\n\fnal physical plan is executed.\nIntuitively, ReJOIN uses a neural network to iteratively\nbuild up a join order. When the optimizer's cost model de-\ntermines that the resulting query plan (using the join order-\ning selected by ReJOIN) is good (i.e., a low cost), ReJOIN\nadjusts its neural network to produce similar join orderings.\nWhen the optimizer's cost model determines the resulting\nplan is bad (i.e., a high cost), ReJOIN adjusts its neural\nnetwork to produce di\u000berent join orderings.\nState and Actions The framework is shown in Figure 1.\nFormally, given a query qaccessing relations r1;r2;:::;rn,\nwe de\fne the initial state of the episode for qass1=\nfr1;r2;:::;rng. This state is expressed as a state vector .\nThis state vector is fed through a neural network, which\nproduces a probability distribution over potential actions.\nThe action set Aifor any state is every unique ordered pair\nof integers from 1 to jsij, inclusive: Ai= [1;jsij]\u0002[1;jsij].\nThe action ( x;y)2Airepresents joining the xth andyth\nelements of sitogether. The output of the neural net-\nwork is used to select an action (i.e., a new join), which is\nsent back to the environment, which transitions to a new\n1Details about ReJOIN can be found in [18].\n\nSQL Vectorization\nInitial state\nActionSelectionReinforcem\nent Learning Join Order Enum\nerator ExperienceReward\nExecution EngineTerminal state\nOperator SelectionEnvironmentFigure 1: The ReJOIN Framework\nAction: [2, 3]12BACD\n3\nAction: [1, 2]12AC\nBD\nTerminal State Action: [1, 3]123ABCD\n4AC\nBDSELECT * FROM A, B, C, D WHERE ...; States\nFigure 2: ReJOIN example\nstate. The state si+1after selecting the action ( x;y) is\nsi+1= (si\u0000fsi[x];si[y]g)[fsi[x]./ si[y]g. The new state\nis fed into the neural network. The reward for every non-\nterminal state (a partial ordering) is zero, and the reward\nfor an action arriving at a terminal state sf(a complete or-\ndering) is the reciprocal of the cost of the join tree t,M(t),\nrepresented by sf,1\nM(t). Periodically, the agent uses its ex-\nperience to tweak the weights of the neural network, aiming\nto earn larger rewards.\nExample Figure 2 shows an example of this process. Each\nof the relations in the SQL query are initially treated as sub-\ntrees. At each step, the set of possible actions contains every\npossible pair of subtrees. For example, in Figure 2, ReJOIN\nselects the action [1,3] , so relations AandCare joined.\nThe reward for this action is determined by a DBMS' op-\ntimizer cost model. At the next step, ReJOIN selects the\naction [2, 3] , so relations BandDare joined. Finally,\nthe action [1, 2] is selected, and the A ./ C andB ./ D\nsubtrees are joined. The resulting state of the system is\na terminal state, as no more actions can be selected. The\nresulting join ordering is sent to a traditional query opti-\nmizer, and the optimizer's cost model is used to determine\nthe quality of the join ordering (the reward).\nExperimental Results Figure 3 shows several key exper-\nimental results from ReJOIN. Figure 3a shows the average\nperformance of ReJOIN compared to PostgreSQL during\ntraining. The graph demonstrates that ReJOIN has the abil-\nity to learn join orderings that lead to query executions plan\nwith latency close and even better than the ones of Post-\ngreSQL. However, converging to a good model takes time.\nEven for the \\limited\" search space of join order enumera-\ntion, ReJOIN had to process nearly 9000 queries to become\ncompetitive with PostgreSQL.\nFigure 3b shows that the \fnal join orderings selected by\nReJOIN (after training) are superior to PostgreSQL accord-\ning to the optimizer's cost model. While the produced query\nplans were faster in terms of latency as well [18], potential\nerrors in the cost model, and the high human cost of devel-\noping and maintaining the cost model, makes directly op-\ntimizing for latency much more desirable. Figure 3c shows\nthe time required for PostgreSQL and ReJOIN to select a\njoin ordering. Counter-intuitively, ReJOIN's deep reinforce-\nment learning algorithm (after training) is faster than Post-\ngreSQL's built-in join order enumerator in many cases.Summary Our experiental analysis of ReJOIN [18] yielded\ninteresting conclusions:\n1. While ReJOIN is eventually able to learn a join ordering\npolicy that outperforms PostgreSQL (both in terms of\noptimizer cost and query latency), doing so requires a\nsubstantial, but not prohibitive, training overhead.\n2. ReJOIN's use of a traditional query optimizer's cost model\nas a reward signal allowed for join orderings to be evalu-\nated quickly. However, this implies that ReJOIN's per-\nformance depends on the existence of a well-tuned cost\nmodel.\n3. Counter-intuitively, ReJOIN's DRL algorithm is faster\nthan PostgreSQL's built-in join order enumerator in many\ncases. Notably, the bottom-up nature of ReJOIN's algo-\nrithm isO(n), where PostgreSQL's greedy bottom-up\nalgorithm is O(n2).\nReJOIN is, to the best of our knowledge, the \frst direct\napplication of deep reinforcement learning to query opti-\nmization. Another promising work [22] has examined how\ndeep reinforcement learning can produce embedded repre-\nsentations of substeps of the query optimization process which\ncorrelate strongly with cardinality, with an eye towards a\nmore principled deep reinforcement learning powered query\noptimizer. Even more recent work [16] demonstrates how\na deep Q-learning [20] approach, with a small amount of\npre-training, can perform well when true cardinalities are\nused as inputs and the optimization target is one of several\nanalytic cost models.\n4. LEARNING-BASED QUERY OPTIMIZA-\nTION: RESEARCH CHALLENGES\nInspired by our experience with ReJOIN [18] as well as\nother existing work in the area [22], we argue that applica-\ntions of DRL theory to query optimization is both promising\nand possible. However, we next identify three key research\nchallenges that must be overcome in order to achieve an\nend-to-end DRL-powered query optimizer.\nSearch Space Size While previous work [18] has demon-\nstrated that reinforcement learning techniques can \fnd good\npolicies in limited search spaces (e.g., join order enumera-\ntion in isolation), the entire search space for execution plans\nis signi\fcantly larger. The ReJOIN prototype required 9000\niterations to become competitive with the PostgreSQL op-\ntimizer, and in that case only join ordering was considered\n(no index or operator selection, etc.). Accounting for opera-\ntor selection, access path selection, etc. creates such a large\nsearch space that approaches from earlier work cannot be\neasily scaled up. In fact, a naive extension of ReJOIN to\ncover the entire execution plan search space yielded a model\nthat did not out-perform random choice even with 72 hours\nof training time. Theoretical results [14] support this obser-\nvation, suggesting that adding additional non-trivial dimen-\nsions to the problem increases convergence time drastically.\nPerformance Indicator Deep reinforcment learning algo-\nrithms generally make several assumptions about the metric\nto optimize, i.e., the reward signal , that are di\u000ecult to guar-\nantee in the context of query optimization. Abstractly, the\nmetric to optimize in query optimization is the latency of\nthe resulting execution plan. However, we next discuss why\nusing latency as a reward signal leads to two unfortunate\ncomplications, namely that the query latency o\u000bers neither\nadense nor a linear reward signal.\n\n0%100%200%300%400%500%600%700%800%900%\n 0  2000  4000  6000  8000  10000  12000  14000Plan Cost (rel. to Postgres)\nNumber of episodes (queries)ReJOIN\nPostgres(a) ReJOIN convergence\n 0 10000 20000 30000 40000 50000\n1a 1b 1c 1d 8c12b 13c 15a 16b 22cOptimizer Cost\nQuery 700000 750000 800000 850000Postgres\nReJOIN (b) Cost of generated plans\n 40 50 60 70 80 90 100 110 120 130 140 150\n4 5 6 7 8 910 11 12 14 17Planning Time (ms)\n# RelationsPostgreSQL\nReJOIN (c) Optimization time\nFigure 3: E\u000bectiveness and e\u000eciency results\nMany deep reinforcement learning algorithms [20, 29] as-\nsume that, or perform substantially better when, the reward\nsignal is dense : provided progressively as the environment is\nnavigated, e.g. each action taken by a reinforcement learning\nagent achieves some reward. Furthermore, DRL algorithms\noften assume that rewards are linear , i.e. the algorithms at-\ntempt to maximize the sum of many small rewards within an\nepisode. Neither of these assumptions hold in the context of\nquery optimization: query latency is not dense (it can only\nbe measured after a plan has been executed), and it is not\nlinear (e.g., subtrees may be executed in parallel).\nOne may reasonably consider using a traditional query\noptimizer's cost model as a reward signal instead of query\nlatency, as the optimizer's cost model may appear to provide\na dense linear reward. This approach has two major draw-\nbacks. First, these cost models tend to be complex, hand-\ntuned (by database engineers and DBAs) heuristics. Using a\ncost model as the reward signal for a DRL query optimizer\nsimply \\kicks the can down the road,\" moving complexity\nand human e\u000bort from designing optimization heuristics to\ntweaking optimizer cost models. Second, the cost model's\nestimation of the quality of an execution plan may not al-\nways accurately represent the latency of the execution plan\n(e.g., a query with a high optimizer cost might outperform a\nquery with lower optimizer cost). Therefore, using DRL to\n\fnd execution plans with a low cost as determined by a cost\nmodel might not always achieve the best possible results.\nPerformance Evaluation Overhead An often-unstated\nassumption made by many DRL algorithms is that the re-\nward of an action can be determined in constant time { e.g.,\nthat determining the performance of an agent for a partic-\nular episode in which the agent performs poorly is no more\ntime-consuming than calculating the reward for an episode\nin which the agent performs well. For example, the time\nto determine the current score of a player in a video game\ndoes not change based on whether or not the score is high or\nlow. If the latency of an execution plan is used as a reward\nsignal, this assumption does not hold: poor execution plans\ncan take signi\fcantly longer to evaluate than good execution\nplans (hours vs. seconds). Since traditional DRL algorithms\nstart with no information, their initial policies cannot be\nbetter than random choice, which will often result in very\npoor plans [17]. Hence, a naive DRL approach that sim-\nply uses query latency as the reward signal would take a\nprohibitive amount of time to converge to good results.2\n2We con\frmed this experimentally by using query latency as the\nreward signal in ReJOIN. The initial query plans produced could\nnot be executed in any reasonable amount of time.\nSQL Optimizer\nCost Model\nEnumerators\nHeuristics\nExec. Engine\nPhysical \nPlan\nDRL AgentParserOutput\nLatency\nInput\nPhase 1SQL\nExec. Engine\nPhysical \nPlanDRL AgentParser\nLatency\nPhase 2OutputInputFigure 4: Learning from demonstration\n5. RESEARCH DIRECTIONS\nHere, we outline potential approaches to handle the chal-\nlenges we highlighted. First, we discuss two drastically dif-\nferent approaches, demonstration learning and cost-model\nbootstrapping , which both avoid the pitfalls identi\fed in Sec-\ntion 4 in interesting ways. We then touch upon incremental\nlearning , and propose three techniques that decompose the\nproblem of query optimization in a principled way across\nvarious axes, and analyze the resulting design space.\n5.1 Learning From Demonstration\nOne way to avoid the pitfalls of using query latency di-\nrectly as the performance indicator (reward) for DRL algo-\nrithms is learning from demonstration (LfD) [11, 26]. Intu-\nitively, this approach works by \frst training a model to im-\nitate the behavior of an expert. Once this mimicry reaches\nacceptable levels, the model is \fne-tuned by applying it to\nthe actual environment. This learn-by-imitation technique\nmirrors how children learn basic behaviors like language and\nwalking by watching adults, and then \fne-tune those behav-\niors by practicing themselves.\nHere, we propose using a traditional DBMS' query op-\ntimizer { such as the PostgreSQL query optimizer { as an\nexpert. In this approach, illustrated in Figure 4, a model is\ninitially allowed to observe how the traditional query opti-\nmizer (the expert) optimizes a query. During this phase, the\nmodel is trained to mimic the optimizer's selected actions\n(e.g., indexes, join orderings, pruning of bad plans, etc).\nAssuming that a traditional optimizer will be able to prune-\n\nout unfeasible plans, this process allows a DRL model to\nlearn by observing the execution time of only feasible plans.\nOnce the model achieves good mimicry, it is then used to\noptimize queries directly, bypassing the optimizer. In this\nsecond phase, the model initially closely matches the ac-\ntions of the traditional query optimizer, but now begins to\nslowly \fne-tune itself based on the observed query latency.\nHere, the agent updates its neural network based on the\nlatency of the execution plans it constructs. If the perfor-\nmance of the model begins to slip, it is re-trained to match\nthe traditional query optimizer until performance improves.\nIn practice, choosing the point at which the model is again\ntrained to mimic the traditional query optimizer is critical\nto improve the performance of the algorithm [11]. By lever-\naging learning from demonstration, one can train a query\noptimization model that learns with small overhead, with-\nout having to execute a large number of bad plans, therefore\nmassively accelerating learning.\nWhile speci\fc techniques and formalizations vary [8, 11,\n26,36], we outline the general process here.\n1. A large query workload, W, is executed one query at\na time. Each q2Wis transformed by the traditional\nquery optimizer into a physical plan through a number\nof actionsaiat various intermediary states si, which are\nrecorded as an episode history :\nHq= [(a0;s0);(a1;s1);:::; (an;sn)]\nFor example, at the initial state s0, a query optimizer\nperforming a greedy bottom-up join order selection pro-\ncess may choose an action a0signifying that two partic-\nular relations should be joined, or a query optimizer that\n\frst performs storage selection may choose an action sig-\nnifying that data for a certain relation should come from\na particular index. All episode histories are saved.\n2. The resulting physical plans are executed, and the la-\ntency of each query q2W,Lq, is measured and saved.\n3. Next, the agent is trained, for each q2W, on theHq\nandLqdata (Phase 1 in Figure 4). Speci\fcally, for each\naction/state pair ( ai;si)2Hq, the agent is taught to\npredict that taking action aiin statesieventually re-\nsults in a query latency of Lq. Similar to the o\u000b-policy\nlearning approach of [22], the agent thus learns a reward\nprediction function : a function that guesses the quality\nof a given action at a given state.\n4. Once the agent has pro\fciency guessing the outcome of\nthe traditional optimizer's actions, the agent can \fne-\ntune itself. Now, the agent will be creating a query plan\nfor an incoming query q. For a given state si, an action\naiis selected by running every possible action though\nthe reward prediction function and selecting the action\nwhich is predicated to result in the lowest latency.3This\nprocess repeats until a physical execution plan is created\nand executed. The model is then trained (\fne-tuned) on\nthe resulting history Hqand observed latency Lq.\n5. Hopefully, the performance of the model will eventually\nexceed the performance of the traditional query opti-\nmizer. However, if the model's performance slips, it\nis partially re-trained with samples from the traditional\nquery optimizer's choices when processing the queries in\nthe initial workload W.\n3In many implementations, an action besides the one predicted\nto result in the lowest latency may be selected with small proba-\nbility [20] to enable additional exploration.\nDRL Agent\nPhysical \nPlanSQL\nRewardCost Model\nCardinality Est.\nOperator Models\nCost\nRewardExecution \nEngine\nLatency\nPhase 1 Phase 2DRL Agent\nPhysical \nPlanSQLFigure 5: Cost Model Bootstrapping\nSince the behavior of the model in the second phase should\nnot initially stray too far from the behavior of the expert sys-\ntem [11], we do not have to worry about executing any ex-\nceptionally poor query plans. Additionally, since the second\ntraining phase only needs to \fne-tune an already-performant\nmodel, the delayed reward signal is of far less consequence.\nIn fact, the initial behavior of the model may outpeform\nthe traditional query optimizer in certain circumstances, for\nexample if the trained model were to observe a systemic er-\nror in the performance of traditional optimizer, such as the\ntraditional optimizer handling two similar situations in two\nsigni\fcantly di\u000berent ways, one of which causes substantially\nincreased query latency. In this case, the trained model may\nautomatically avoid the errors of the traditional optimizer\n(which has no capability to learn from its mistakes) through\nobservation alone.\nAn important issue here is that, since the experience col-\nlected based on the traditional optimizer is necessarily cov-\nering a narrow part of the action space (it excludes \\bad\"\nplans, and thus also excludes the corresponding sequence of\nactions that would produce them), many state-actions have\nnever been observed and have no training data to ground\nthem to realistic cost. For instance, a nested-loop-join or\na table scan may never/rarely be picked by the traditional\noptimizer for a particular workload/database, and hence the\nmodel does not learn how to evaluate these actions correctly.\nHowever, since the model is trained on experiences contain-\ning signi\fcantly faster execution plans, there is no reason for\nthe model to attempt to explore these extremely poor plans.\nExperimental results from other problem domains (e.g. ar-\ncade games [11] and a few systems applications [27]), show\nthat deep reinforcement learning agents which initially learn\nfrom demonstration can master tasks with signi\fcantly less\ntraining time than their tabula rasa counterparts. This re-\nsult holds even when the expert is \rawed (e.g. when the ex-\npert is a human player who does not know a particular short-\ncut or strategy), implying that learning-from-demonstration\ntechniques can improve upon, and not just imitate, existing\nexpert systems.\n5.2 Cost Model Bootstrapping\nA traditional, but still widely used and researched, ap-\nproach to improving the performance of reinforcement learn-\ning algorithms on problems when the performance indicator\n(reward) is only available at the end of an episode (sparse)\nis to craft a heuristic reward function . This heuristic re-\nward function estimates the utility of a given state using a\nheuristic constructed by a human being: for example, when\na robot is learning to navigate a maze, it may use an\\as-the-\ncrow-\ries\" heuristic to estimate its proximity to the maze's\n\nexit. In the game of chess, a popular heuristic to evalu-\nate the value of a particular board position is to count the\nnumber of pieces captured by both sides. Sometimes, this\nheuristic may be incorrect (e.g., it may rate a dead-end very\nnear the exit as a desirable position, or it may highly-rate\na board position in which many pieces have been captured\nbut the opponent has an obvious winning move), but in gen-\neral there is a strong relationship between the value of the\nheuristic function and the actual reward.\nLuckily, the database community has invested signi\fcantly\ninto designing optimizer cost models, which can be used for\nexactly this purpose. While imperfect, modern cost models,\nlike \\as-the-crow-\ries\" distance, can normally di\u000berentiate\nbetween good and catastrophic plans. We thus propose us-\ning these cost models as heuristic reward functions. This\napproach, depicted in Figure 5, \frst uses the optimizer's\ncost model as a reward signal (Phase 1) and then, once\ntraining has converged, switches the reward signal to the\nobserved query latency (Phase 2). In this way, the opti-\nmizer's cost model acts as \\training wheels,\" allowing the\nDRL model to explore strategies that produce catastrophic\nquery plans without requiring execution. Once the DRL\nmodel has stabilized and starts to pick predominately good\nplans, the \\training wheels\" can be removed and the DRL\nmodel can \fne-tune itself using the \\true\" reward signal,\nquery latency.\nCost model bootstrapping brings about a number of com-\nplications which require further exploration by the database\ncommunity. Generally, an optimizer's cost model output is\na unitless value, meant to compare alternative query plans\nbut not meant to directly correlate with execution latency.\nFor example, an optimizer's cost estimate for a set of query\nplans may range from 10 to 50, but the latency of these\nquery plans may range from 100s to 200s. Switching the\nrange of the reward signal from 10-50 to 100-200 will cause\nthe DRL model to assume that its performance has sud-\ndenly decreased (the DRL model was getting query plans\nwith costs in the range 10-50 in Phase 1, and at the start\nof Phase 2 the costs suddenly jump to be in range 100-200).\nThis sudden change could cause the DRL model to begin\nexploring previously-discarded strategies, requiring the ex-\necution of poor execution plans. The change in variance\ncould also have a detrimental e\u000bect [12].\nOne way to potentially \fx this issue would be to tune\nthe units of the cost model to more precisely match exe-\ncution latency, but the presence of cardinality estimation\nerrors makes this di\u000ecult [17]. Instead of adjusting the op-\ntimizer's estimates to match the query latency, another ap-\nproach could be to adjust the query latency to match the\noptimizer cost. This could be implemented by simply scal-\ning the query latency observed in Phase 2 to fall within the\nrange of cost model estimates observed in Phase 1.\nOne could implement this scaling by noting the optimizer\ncost estimates andquery execution latencies during the end\nof Phase 1 (when the DRL model has converged). Let Cmax\nandCminbe the maximum and minimum observed opti-\nmizer cost, and let LmaxandLminbe the maximum and\nminimum observed query execution times. Then, in Phase\n2, when the DRL model proposes an execution plan with an\nobserved latency of l, the reward rlcould be:\nrl=Cmin+l\u0000Lmin\nLmax\u0000Lmin(Cmax\u0000Cmin)\nIndex selectionAgg. operators# Relations5\n4\n3\n2\n1\nJoin orderJoin operatorseasyhardFigure 6: Complexity diagram\nThis scaling could be done linearly, as above, or using\na more complex (but probably monotonic) function. This\nsimple solution would likely need to be adjusted to handle\nworkload shifts, changes in hardware, changes in physical\ndesign, etc.\nAnother potential approach, partially suggested in [16], is\nto \frst train a neural network model to optimize for the op-\nerator cost, and then transfer the weights of the later layers\nof the network into a new network that trains directly on\nquery latency. This technique, known as \\transfer learning\",\nhas seen wide success in other \felds [5,38].\n5.3 Incremental Learning\nIn this section, we discuss potential techniques to incre-\nmentally learn query optimization by \frst training a model\nto handle simple cases and slowly introducing more com-\nplexity. This approach makes the extremely large search\nspace more manageable by dividing it into smaller pieces.\nSimilar incremental approaches has shown success in other\napplications of reinforcement learning [6,9,33].\nWe begin by examining how the task of query optimization\ncan be decomposed into simpler pieces in a number of ways.\nWe note that the di\u000eculty of a query optimization task is\nprimarily controlled by two dimensions: the number of re-\nlations in the query, and the number of optimization tasks\nthat need to be performed. This is illustrated in Figure 6.\nThe \frst axis is the number of relations in the query. If a\nDRL model must optimize queries containing only a single\nrelation, then the search space of query plans is very small\n(there are no join orderings or join operators to consider).\nHowever, if a DRL model must optimize queries containing\nmany relations, then the search space is much larger.\nThe second axis is the number of optimization tasks to\nperform. Consider a simpli\fed query optimization pipeline\n(illustrated in Figure 8) containing four phases: join order-\ning, index selection, join operator selection, and aggregate\noperator selection. Performing any pre\fx of the pipeline is\na simpler task than performing the entire pipeline: e.g., de-\ntermining a join ordering and selecting indexes is a simpler\ntask than determining a join ordering, selecting indexes, and\ndetermining join operators.\nThus, the lower-left hand side of Figure 6 corresponds to\n\\easy\"cases, e.g. few stages of the pipeline and few relations.\nThe upper-right hand side of Figure 6 corresponds to \\hard\"\ncases, e.g. most stages of the pipeline and many relations.\nThis insight illuminates a large design space for incremental\nlearning approaches. In general, an incremental learning\napproach will be divided into phases. The \frst phase will\nuse \\easier\" cases (the bottom left-hand part of the chart),\ntraining until relatively good performance is achieved. Then,\nsubsequent phases will introduce more complex examples to\n\n# Relations5\n4\n3\n2\n1\nJoin orderIndex selectionJoin operatorsAgg. operators\n# Relations5\n4\n3\n2\n1\nJoin orderIndex selectionJoin operatorsAgg. operatorsIndex selectionAgg. operators# Relations5\n4\n3\n2\n1\nJoin orderJoin operatorsPipeline Relations HybridFigure 7: Potential decompositions\nJoin Order \nSelectionIndex\nSelectionJoin\nOperator\nSelectionAgg.\nOperator\nSelectionPlan \nExecutor\nPhase 1\nPhase 2\nPhase 3\nPhase 4\nFigure 8: Learning incrementally\nthe model, allowing the model to slowly and smoothly learn\nmore complex cases (the top right-hand part of the chart).\nFigure 7 illustrates three simple incremental learning ap-\nproaches, with light colors representating the initial training\nphases and dark colors representing the subsequent training\nphases. We next discuss each of these approaches in detail.\n5.3.1 Increasing optimization actions (pipeline)\nOur \frst proposed approach is pipeline-based incremental\nlearning , illustrated in Figure 8. A model is \frst trained on\na small piece of the query optimization pipeline, e.g. join or-\nder selection. During this \frst phase, traditional query opti-\nmization techniques are used to take the output of the model\nand construct a complete execution plan (ReJOIN [18] is es-\nsentially this \frst phase). Once the model achieves good\nperformance in this \frst phase, the model is then slightly\nmodi\fed and trained on the \frst two phases of the query\noptimization pipeline, e.g. join order selection and index se-\nlection. This process is repeated until the model has learned\nthe entire pipeline.\nExtending ReJOIN to support this approach would be rel-\natively straightforward. As shown in [18], the \frst phase of\nquery optimization (join order enumeration) can be e\u000bec-\ntively learned. Once this initial training is complete, the\naction space can be extended to support index selection: in-\nstead of having one action per relation, the extended action\nspace would have one action per relational data structure,\ne.g. one action for a relation's B-tree index, one action for a\nrelation's row-order storage, one action for a relation's hash\nindex, etc. The knowledge gained from the previous train-\ning phase should help the model train signi\fcantly faster in\nsubsequent phases.\nThe pipeline approach has the advantages of incremental\nlearning (e.g., a managable growth of the state space), but\ncomes with several drawbacks that need to be further in-\nvestigated. First, the early training phases requires access\nto a traditional implementation of the later stages of the\nquery optimization pipeline. While such implementations\nare available in a range of DBMSes today, the dependency\non a traditional query optimizer is not ideal. Second, each\nphase of the training process will not bring about a uni-\nSELECT * \nFROM A \nWHERE ...;\nDRL Agent\nAScan\nExec Eng\nLatency / RewardSELECT * \nFROM A, B \nWHERE ...;\nDRL Agent\nAScan\nExec Eng\nLatency / Reward\nBScanMJSELECT * \nFROM A, B, ... \nWHERE ...;\nDRL Agent\nAScan\nExec Eng\nLatency / Reward\nBScan...\n...\nPhase 1 Phase 2 Phase NFigure 9: Learning from small examples\nform increase in complexity. It is conceivable that some\nstages of the pipeline are fundamentally more complex than\nothers (for example, join order selection is likely more dif-\n\fcult than aggregate operator selection). The non-linearity\nof complexity going through the query optimization pipeline\nmeans that some training phases will require overcoming\nmuch larger jumps in complexity than others. This could\nresult in unpredictable training times, or, in the worst case,\na jump in complexity to large to learn all at once.\n5.3.2 Increasing relations\nWhile the previous approach reduces the size of the search\nspace by focusing on larger and larger parts of the query op-\ntimization pipeline, this section proposes limiting the search\nspace by focusing on larger and larger queries . The proposed\napproach is depicted in Figure 9. In the \frst training phase,\nthe model learns to master queries over a single relation. In\nsubsequent training phases, the model is trained on queries\nover two relations, then three relations, etc. In each phase,\nthe entire query optimization pipeline is performed.\nThis approach dodges some pitfalls of the pipeline stage\napproach. Generally, the increase in complexity between\noptimizing a query with nrelations and optimizing a query\nwithn+ 1 relations is small. Even though there is an ex-\nponential increase in the number of potential join orderings,\nthis is a \\quantitative\" change as opposed to a \\qualitative\"\nchange { intuitively, it is easier to learn how to create a join\nplan with a single additional relation than it is to learn how\nto perform a new pipeline step.\nA major challenge of this approach is \fnding candidate\nqueries. Generally, real-world workloads will contain very\nfew queries over a single relation. Even synthetic workloads\nhave very few low-relation-count queries (TPC-H [24] has\nonly two such templates, JOB [17] has none). Queries with\nlow relation counts could be synthetically generated, but do-\ning so while matching the characteristics of real-world work-\nloads is a complex task.\n5.3.3 Hybrid\nThe last approach we explicitly discuss is the hybrid ap-\nproach, depicted on the right-hand side of Figure 7. In this\nhybrid approach, the initial training phase learns only the\n\frst step of the query optimization pipeline (e.g. join or-\nder selection) using only queries over two or fewer relations.\nThe next training phase introduces both another step of the\npipeline (e.g. index selection) andqueries over three or fewer\nrelations. After all stages of the query optimization pipeline\nhave been incorporated, subsequent training phases increase\n\nthe number of relations considered. This approach provides\nthe smallest increase in complexity from training phase to\nsubsequent training phase. However, the hybrid approach\nsu\u000bers from some of the disadvantages of both the relations\nand pipeline based approach: it depends on a traditional op-\ntimizer and it requires queries with relatively few relations\nfor training purposes.\n6. CONCLUSIONS\nWe have argued that recent advances in deep reinforce-\nment learning open up new research avenues towards a\\hands-\nfree\" query optimizer, potentially improving the speed of re-\nlational queries and signi\fcantly reducing time spent tuning\nheuristics by both DBMS designers and DBAs. We have\nidenti\fed how the large search space, delayed reward sig-\nnal, and costly performance indicators provide substantial\nhurdles to naive applications of DRL to query optimization.\nFinally, we have analyzed how recent advances in reinforce-\nment learning, from learning from demonstration to boot-\nstrapping to incremental learning, open up new research di-\nrections for directly addressing these challenges.\nOther complexities We argue that deep reinforcement\nlearning can greatly decrease the amount of human e\u000bort\nrequired to develop and tune database management sys-\ntems. However, these deep learning techniques come with\ntheir own complexities as well: training con\fgurations (e.g.\nlearning rate), network architectures, activation function se-\nlection, etc. While deep learning researchers are quickly\nmaking inroads towards automating many of these deci-\nsions [4, 19], future research should carefully analyze the\ntradeo\u000bs between tuning deep learning systems and tuning\ntraditional query optimizers.\nOther applications While query optimization is a good\ncandidate for applying DRL to database internals, a wide\nvariety of other core DBMS concepts (e.g.cache manage-\nment, concurrency control) could bene\ft from applications\nof machine learning as well. Careful applications of machine\nlearning across the entire DBMS, not just the query opti-\nmizer, could bring about a massive increase in performance\nand capability.\n7. REFERENCES\n[1]Aboulnaga, A., et al. Self-tuning Histograms: Building\nHistograms Without Looking at Data. In SIGMOD '99 .\n[2]Arulkumaran, K., et al. A Brief Survey of Deep\nReinforcement Learning. IEEE Signal Processing '17 .\n[3]Babcock, B., et al. Towards a Robust Query Optimizer:\nA Principled and Practical Approach. In SIGMOD '05 .\n[4]Baker, B., et al. Designing Neural Network Architectures\nusing Reinforcement Learning. In ICLR '17 .\n[5]Bengio, Y. Deep Learning of Representations for\nUnsupervised and Transfer Learning. In ICML WUTL '12 .\n[6]Buffet, O., et al. Incremental Reinforcement Learning\nfor Designing Multi-agent Systems. In AGENTS '01 .\n[7]Chen, C. M., et al. Adaptive Selectivity Estimation Using\nQuery Feedback. In SIGMOD '94 .\n[8]de la Cruz Jr, G. V., et al. Pre-training Neural\nNetworks with Human Demonstrations for Deep\nReinforcement Learning. arXiv '17 .\n[9]Erickson, N., et al. Dex: Incremental Learning for\nComplex Environments in Deep Reinforcement Learning.\narXiv '18 .\n[10]Graefe, G., et al. The Volcano Optimizer Generator:\nExtensibility and E\u000ecient Search. In ICDE '93 .[11]Hester, T., et al. Deep Q-learning from Demonstrations.\nInAAAI '18 .\n[12]Ioffe, S., et al. Batch Normalization: Accelerating Deep\nNetwork Training by Reducing Internal Covariate Shift. In\nICML'15 .\n[13]Kaftan, T., et al. Cuttle\fsh: A Lightweight Primitive for\nAdaptive Query Processing. arXiv '18 .\n[14]Kearns, M., et al. Near-Optimal Reinforcement Learning\nin Polynomial Time. Machine Learning '01 .\n[15]Kraska, T., et al. The Case for Learned Index\nStructures. In SIGMOD '18 .\n[16]Krishnan, S., et al. Learning to Optimize Join Queries\nWith Deep Reinforcement Learning. arXiv '18 .\n[17]Leis, V., et al. How Good Are Query Optimizers, Really?\nVLDB '15 .\n[18]Marcus, R., et al. Deep Reinforcement Learning for Join\nOrder Enumeration. In aiDM '18 .\n[19]Miikkulainen, R., et al. Evolving Deep Neural Networks.\narXiv '17 .\n[20]Mnih, V., et al. Human-level control through deep\nreinforcement learning. Nature '15 .\n[21]Mudgal, S., et al. Deep Learning for Entity Matching: A\nDesign Space Exploration. In SIGMOD '18 .\n[22]Ortiz, J., et al. Learning State Representations for Query\nOptimization with Deep Reinforcement Learning. In\nDEEM '18 .\n[23]Pavlo, A., et al. Self-Driving Database Management\nSystems. In CIDR '17 .\n[24]Poess, M., et al. New TPC Benchmarks for Decision\nSupport and Web Commerce. SIGMOD '00 .\n[25]Ruder, S. An overview of gradient descent optimization\nalgorithms. arXiv '16 .\n[26]Schaal, S. Learning from Demonstration. In NIPS'96 .\n[27]Schaarschmidt, M., et al. LIFT: Reinforcement Learning\nin Computer Systems by Learning From Demonstrations.\narXiv '18 .\n[28]Schmidhuber, J. Deep learning in neural networks: An\noverview. NN '15 .\n[29]Schulman, J., et al. Proximal Policy Optimization\nAlgorithms. arXiv '17 .\n[30]Schulman, J., et al. Trust Region Policy Optimization. In\nICML '15 .\n[31]Selinger, P. G., et al. Access Path Selection in a\nRelational Database Management System. In SIGMOD '89 .\n[32]Stillger, M., et al. LEO - DB2's LEarning Optimizer. In\nVLDB '01 .\n[33]Taylor, M. E., et al. Transfer Learning for\nReinforcement Learning Domains: A Survey. JMLR '09 .\n[34]Tzoumas, K., et al. A Reinforcement Learning Approach\nfor Adaptive Query Processing. In Technical Report, 08 .\n[35]Waas, F., et al. Join Order Selection (Good Enough Is\nEasy). In BNCD '00 .\n[36]Watkins, C. J., et al. Q-learning. Machine learning '92 .\n[37]Williams, R. J. Simple statistical gradient-following\nalgorithms for connectionist reinforcement learning. In\nMachine Learning '92 .\n[38]Yosinski, J., et al. How Transferable Are Features in\nDeep Neural Networks? In NIPS '14 .",
  "textLength": 47866
}