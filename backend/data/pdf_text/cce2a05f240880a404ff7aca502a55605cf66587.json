{
  "paperId": "cce2a05f240880a404ff7aca502a55605cf66587",
  "title": "Outback: Fast and Communication-efficient Index for Key-Value Store on Disaggregated Memory",
  "pdfPath": "cce2a05f240880a404ff7aca502a55605cf66587.pdf",
  "text": "Outback: Fast and Communication-efficient Index for Key-Value\nStore on Disaggregated Memory\nYi Liu\nUniversity of California Santa Cruz\nyliu634@ucsc.eduMinghao Xie\nUniversity of California Santa Cruz\nmhxie@ucsc.eduShouqian Shi\nUniversity of California Santa Cruz\nsshi27@ucsc.edu\nYuanchao Xu\nUniversity of California Santa Cruz\nyxu314@ucsc.eduHeiner Litz\nUniversity of California Santa Cruz\nhlitz@ucsc.eduChen Qian\nUniversity of California Santa Cruz\nqian@ucsc.edu\nABSTRACT\nDisaggregated memory systems achieve resource utilization ef-\nficiency and system scalability by distributing computation and\nmemory resources into distinct pools of nodes. RDMA is an attrac-\ntive solution to support high-throughput communication between\ndifferent disaggregated resource pools. However, existing RDMA\nsolutions face a dilemma: one-sided RDMA completely bypasses\ncomputation at memory nodes, but its communication takes mul-\ntiple round trips; two-sided RDMA achieves one-round-trip com-\nmunication but requires non-trivial computation for index lookups\nat memory nodes, which violates the principle of disaggregated\nmemory. This work presents Outback, a novel indexing solution for\nkey-value stores with a one-round-trip RDMA-based network that\ndoes not incur computation-heavy tasks at memory nodes. Outback\nis the first to utilize dynamic minimal perfect hashing and separates\nits index into two components: one memory-efficient and compute-\nheavy component at compute nodes and the other memory-heavy\nand compute-efficient component at memory nodes. We implement\na prototype of Outback and evaluate its performance in a public\ncloud. The experimental results show that Outback achieves higher\nthroughput than both the state-of-the-art one-sided RDMA and\ntwo-sided RDMA-based in-memory KVS by 1.06-5.03 Ã—, due to the\nunique strength of applying a separated perfect hashing index.\nPVLDB Reference Format:\nYi Liu, Minghao Xie, Shouqian Shi, Yuanchao Xu, Heiner Litz, and Chen\nQian. Outback: Fast and Communication-efficient Index for Key-Value\nStore on Disaggregated Memory. PVLDB, 18(2): 335-348, 2024.\ndoi:10.14778/3705829.3705849\nPVLDB Artifact Availability:\nThe source code, data, and/or other artifacts have been made available at\nhttps://github.com/yliu634/outback.\n1 INTRODUCTION\nDisaggregated memory systems [ 33,39,42,43,47,49,53,61] repre-\nsent a transformative departure from traditional computing archi-\ntectures, distributing memory storage and computational resources\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license. For any use beyond those covered by this license, obtain permission by\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 18, No. 2 ISSN 2150-8097.\ndoi:10.14778/3705829.3705849\nMultiple round trips are requiredData RNICIndexCPU RNIC\nSearch(Key)\nRDMA_READ\nBypass CPUCompute the index \n& dataâ€™s logical addrAddrCompute node\nMemory node\n(a) An example of one-sided RDMA.\nBtree : Traverse nodes layer by layer\nHash Table: Traverse buckets in the chain\nDataCPU RNICBTreeCompute node\nMemory nodeCPU RNIC\nSearch(Key)\nRDMA RPCKey \nHash \nTableâ· â¶\n (b) An example of two-sided RDMA.\nFigure 1: Examples of two types of RDMA systems.\ninto distinct pools of nodes â€“ compute pools include nodes that\ncarry rich CPU resources, and memory pools include nodes that\ncarry rich DRAM and storage resources. This framework is preva-\nlent in contemporary data centers and cloud infrastructures [ 60,62],\nproviding benefits such as enhanced resource utilization efficiency\nand flexibility to scale the system out by deploying more hardware.\nDisaggregated memory systems can harness Remote Direct Mem-\nory Access (RDMA)-capable networks [ 14,26,40,43,46], featuring\nsubstantial throughput capacities (ranging from 40 to 400 Gbps)\nand small latency within the microsecond range. Memory-intensive\napplications, such as transaction systems [ 11,52,57,58] and key-\nvalue stores (KVSs) [ 14,20,28,66], store the data and index data\nstructures at the memory nodes and perform computation tasks at\nthe compute nodes.\nExisting RDMA networks for disaggregated memory can be\ncategorized into two types. 1) One-sided RDMA [ 28,34,37,48,66]\nas shown in Fig. 1(a). This type of network completely separates\ncomputation and memory access tasks. Each data request requires\nmultiple round trips of communication between the compute node\nand the memory node. At least two round trips are necessary:\none to access the index and the other to access the stored data.\nNote that many indices require multiple layers of accesses [ 37,\n48], hence they need much more than two round trips [ 36,37]. 2)\nTwo-sided RDMA or RDMA RPC [ 21,22], as depicted in Fig. 1(b),\ninvolves computation tasks on both compute and memory nodes,\nrequiring only a single round-trip communication for each request.\nHowever, two-sided RDMA cannot bypass the CPU on the memory\nnode, necessitating the CPU on the memory node to execute the\ncomputation of the index structure, such as hash computations and\nkey comparisons. Since the CPU resource on a memory node is\nvery limited in disaggregated systems, this design may lead to CPUarXiv:2502.08982v1  [cs.DB]  13 Feb 2025\n\nbottlenecks and potentially higher latency compared to one-sided\nRDMA [17, 48].\nA natural question arises: \"Can we design a one-round-trip\nRDMA-based network that does not incur computation-heavy tasks\non memory nodes?\" Achieving this goal is extremely challenging\nbecause putting the index on memory nodes leads to CPU bottle-\nnecks while putting the index on compute nodes causes memory\nbottlenecks and consistency issues.\nThis paper presents the first solution to this research problem.\nOur key innovation is to design and implement an RDMA RPC-\nbased system, called Outback, which decouples its index into two\ncomponents. The first component is memory-efficient and includes\nmost computation operations of the index, which is placed onto\nthe compute nodes. The second component contributes to the most\nmemory cost of the index, but its computation is trivial, and it\nis on the memory nodes. Such a design principle of decoupling\nthe index is ideal for disaggregated memory systems: all compu-\ntation tasks for Getrequests and the majority of computation for\ndata Insert requests are offloaded on compute nodes, while mem-\nory nodes focus on providing service for memory read and write.\nHence, this approach is particularly effective for real-world work-\nloads dominated by Getrequests. It is also well-suited for emerging\ndisaggregated memory systems equipped with SmartNICs with\nlimited computation resources [1, 6, 54].\nSimilar to prior one-round-trip RDMA networks [ 21,22], Out-\nback also relies on two-sided RDMA. We implement Outback as a\ndistributed KVS application. The index design of Outback is mo-\ntivated by a recent advance of dynamic minimal perfect hashing\n(DMPH), called Ludo hashing [ 44]. The original design of Ludo\nhashing did not decouple the index into computation-heavy and\nmemory-heavy components, but its perfect hashing property offers\nthe opportunity for a novel decoupling approach that allows data\nGetrequests in one round trip with trivial computation on memory\nnodes. For data Insert requests, we design additional operations\nto update the index on both the compute and memory nodes to\nensure data consistency.\nOverall, this paper makes the following contributions:\nâ€¢We present a novel solution that provides one-round-trip\nRDMA with RPC that incurs minimal computation tasks on\nmemory nodes. The design principle of decoupling the in-\ndex works effectively for emerging disaggregated memory\nsystems.\nâ€¢We design the Outback system as a distributed KVS. We\ndesign a decoupled index based on a recent data structure\nof DMPH. We also designed the algorithms and protocols\nfor supporting data operations and system updates.\nâ€¢We implement a prototype of Outback and evaluate the\nperformance on YCSB workloads [ 12] and four real-world\ndatasets from SOSD [ 35]. The experimental results show\nthat Outback achieves higher throughput than both the\nstate-of-the-art one-sided RDMA and two-sided RDMA-\nbased in-memory KVS by 1.06-5.03 Ã—.\n2 BACKGROUND\n2.1 Disaggregated Memory with RDMA\nDisaggregated memory systems with RDMA can be categorized\ninto two types: one-sided RDMA systems [ 28,34,37,48,66], and\nvalue\n57\n6212\nâ€¦seed1\nseed2\nseednQuery(key)\nbucket \nlocatorb1b045\n24 12\nCompute -heavy part Memory -heavy partFigure 2: Ludo hashing.\ntwo-sided RDMA (RDMA-RPC) systems [ 21,22]. An example of\none-sided RDMA systems [ 28,34,37,48,66] is illustrated in Fig. 1(a).\nThese systems support applications such as KVS and transaction\nsystems with various index data structures, including B/B+ trees,\nhash tables, radix trees, and learned indexes. However, it is widely\nrecognized that multiple round-trip communications are needed\nfor each Getrequest: at least one for querying the index and one\nfor reading data. The high communication cost results in both long\nlatency and network congestion.\nTwo-sided RDMA-based systems [ 21,22] have been investigated\nto dispatch compute nodesâ€™ requests to the memory node via RPC\nover the RDMA network with only one round trip. As depicted in\nFig. 1(b), a data index, such as a B-Tree or hash table, is maintained\nat the memory node. When a data query occurs, in addition to\npolling the RNIC and posting messages, the CPU of the memory\nnode is responsible for traversing the index. The memory node\nhas to perform computational tasks, including hash computation,\nfingerprint checking, and key comparisons. This process introduces\nadditional computational overhead and memory accesses. Existing\nsolutions [ 11,29,66] that store keysâ€™ fingerprints in their hash\ntables to save memory usage also introduce extra computation. For\nexample, if the memory node employs the state-of-the-art (2,4)-\nCuckoo hash table [ 38], each Getrequest requires one fingerprint\ncomputation and, at most eight rounds of fingerprint checking.\n2.2 Dynamic minimal perfect hashing\nIn this subsection, we first introduce the background of DMPH and\nthen present an existing MPH implementation, Ludo hashing [ 44].\nPerfect hashing [ 16] represents a family of schemes that designs\nand manipulates hash algorithms to distribute keys to different\nbuckets in a hash table without collisions. Since it is impractical to\nfind a single hash function that generates no collisions for a large\nset of keys, a common approach is to use two levels of mapping.\nThe first level maps keys to a number of groups, each of which\ncontains several keys. The second level addresses key collisions\ninside each group. Minimal perfect hashing maps ğ‘›keys to exactly ğ‘›\nbuckets, but it is inflexible for key insertions and only applicable to a\nstatic set. To allow key dynamics, dynamic minimal perfect hashing\n(DMPH) may use(1+ğœ–)ğ‘›positions for ğ‘›keys [ 44,64]. One primary\nadvantage of perfect hashing is that it does not need to store the keys\nin the hash table. Since perfect hashing eliminates collisions, a key\nquery does not need to compare keys to address collisions. Avoiding\nstoring keys can significantly reduce memory costs, because as a\nsecondary index, the size of keys (usually hundreds of bits) is much\nlonger than the queried value in a hash table (usually a storage\naddress in tens of bits).\n\nOne of the most recent solutions of DMPH is called Ludo hash-\ning [ 44]. As shown in Fig. 2, Ludo hashing [ 44] first uses a data\nstructure called Othello [ 56], a dynamic implementation of Bloomier\nfilters [ 8] with two arrays, as the bucket locator to distribute keys\ninto different buckets, each of which includes exactly 4 slots. Then,\nin each bucket ğµğ‘–, Ludo hashing uses brute force to find a hash\nseedğ‘ ğ‘–such that the hash function with ğ‘ ğ‘–can map the 4 keys in\nthe bucket to 4 different slots without collision. Hence, there is no\nneed to store keys in the table for collision resolution. The space\ncost of Ludo is 3.76+1.05ğ‘™bits per key, where ğ‘™is the length of the\nrecord value, which is claimed to be the smallest memory cost in\nthe literature [ 44]. The bucket locator leverages Othello arrays [ 56],\nwhich costs 2.33 bits per key. Each bucket contains a 5-bit long\nseed shared by four keys in Ludo, i.e., 1.25/0.95 bits per key when\nwe set the load factor as 95%. Also, the majority of memory cost is\nfor storing the values in the buckets, costing 1.05ğ‘™bits per key. We\nobserved that the computation for looking up the slot only needs\nthe bucket locator and the seeds, which are memory efficient. On\nthe other side, the hash table buckets/slots part storing all data\nvalues contributed to most memory of this index, but it requires\nlittle computation.\n3 MEASUREMENT AND MOTIVATION\nWe wonder if, we remove the computation cost at the memory node,\nwill RDMA-RPC demonstrate much higher throughput than the\nstate-of-the-art one-sided RDMA? If the answer is \"Yes\", then\nthere is a great opportunity to design a high-throughput\nRDMA-based KVS by reducing the computation cost at the\nmemory node.\nToward this objective, we conduct experiments to analyze the\nthroughput performance of both one-sided RDMA and RDMA-RPC\nsystems with 9 r320 servers in CloudLab [ 15], each is configured\nwith a Mellanox CX3 adapter (50Gbits). We compare the perfor-\nmance of the following systems with Get-only workload. (1) RACE\nhashing [ 66], a state-of-the-art one-sided RDMA-based scheme. Its\nhashing index is crafted for disaggregated memory, facilitating data\nretrieval within two round trips. (2) RPC-hash table, a two-sided\nRDMA method whose compute nodes and memory nodes commu-\nnicate in RDMA unreliable datagram (UD) mode. Each memory\nnode maintains a chained hash table in its local memory to handle\nremote data requests. (3) RPC-Dummy. A hypothetical RDMA-RPC\nmethod that incurs minimal computation cost at each memory\nnode. RPC-Dummy only implements one memory access and then\nreturns any data in the accessed memory at the memory node, with\nno extra computation tasks. RPC-Dummyâ€™s throughput can be con-\nsidered the upper bound among all possible RDMA-RPC systems.\nWe use this method to explore the performance potential of our\ndesign objectives. We vary the number of memory node threads as\n1, 2, and 4 in RPC-based approaches, and each memory node thread\nmaintains one Queue Pair (QP) and runs in a distinct CPU core.\nThe results are shown in Fig. 3(a). For one memory node thread\n(one core), RPC-hash table achieves a throughput similar to that\nof RACE hashing. For RACE hashing, multiple reasons limit its\nthroughput, including the two round trips to complete one data Get\noperation and multiple RC connections of the compute node threads\nthat incur resource contention in the RNIC cache [ 10]. RPC-hash\ntable requires only one round trip, but the complexity of querying\nRACE hashing (One -sided RDMA scheme)\nRPC -hash table (w./ 1, 2, and 4 server threads)\nRPC -Dummy (w./ 1, 2, and 4 server threads)(a) Throughput of different systems with limited number of memory node threads.\nNormalized \nCPU time=1\n(b) The CPU time breakdown on a memory node with one thread.\nFigure 3: Observations from the microbenchmarks.\nthe index on the memory node introduces extra latency and limits\nits throughput. The throughput of RPC-hash table increases cor-\nrespondingly when we increase the number of threads to 2 and 4.\nIn contrast, RACE hashing maintains a static performance. RPC-\nDummy can outperform RPC-hash table by around 2 Ã—under the\ncases of both single and multiple memory node threads. Hence an\nRDMA-RPC network that introduces little computation overhead to\nthe memory node can achieve higher throughput than both existing\none-sided RDMA and RDMA-RPC solutions. The results suggest\nthat RPC-based KVS has a potential for throughput improvement\nby reducing computation tasks at memory nodes, which motivates\nthe design of this project.\nCPU utilization breakdown for RPC-based approaches.\nWe run RDMA-RPC with different indices: hash table, Btree, and\nlearned index, at the memory node. The CPU time consumed by\nthese four RPC-based KVS systems while handling an equal num-\nber of data Getrequests is normalized and presented in Fig. 3(b),\nwith the number of compute node threads fixed at 64. RPC-Dummy\ntakes the least time. Other approaches consume more time in differ-\nent amounts. For RPC-Btree, in addition to the communication\noverheads for polling mlx4 _poll _qp(4.03%), posting messages\nmlx4 _post _send (7.52%) and UD transport (6.85%) from connection\nmanagement, the most CPU-consuming event is the RPC callback\nfunction (70.59%), which executes local index lookup and data ac-\ncess. In all four schemes, the RPC callback function consumes the\nmost CPU time, and the variations in CPU consumption among\nthem are mainly attributed to differences in the RPC callback func-\ntion. RPC-Btree consumes the most CPU time for RPC callback,\nfollowed by RPC-hash table. RPC-Dummy spends the least CPU\ntime on the RPC callback function (46.11%) and serves the most data\nrequests because there is no computation burden for the memory\nnode in RPC-Dummy. In disaggregated systems, tasks such as com-\nputing hash functions on a hash table, traversing tree nodes in a\n\nCompute PoolCompute Nodes Shard\nCompute Node1\nBucket locator\n& Seeds\nMemory PoolMemory Node Shard\nMemory Node\nBucket \nSeeds\nDMPH\nBuckets\nOverflowed \nCache\nKV Data AreaRDMA\nRPC\nData \noperations\nCompute Node2\nBucket locator\n& SeedsFigure 4: Outback overview\nB-Tree, and executing learned models on a learned index are not ide-\nally suited for memory nodes. The throughput of RDMA-RPC\nmethods is mainly limited by CPU usage during the RPC\ncallback function for index lookups and data reads. High\nCPU consumption from complex index computations on\nmemory nodes reduces throughput, particularly when CPU\nresources are constrained, indicating that optimizing these\ncomputations can enhance performance.\n4 DESIGN OF OUTBACK\n4.1 Overview\nBased on the motivation presented in the previous section, we de-\nsign and implement an RDMA-RPC network that aims to minimize\ncomputation tasks on memory nodes, consequently enhancing the\nsystem throughput. This section presents the design of Outback, a\nscalable RDMA RPC-based disaggregated KVS that tackles the per-\nformance limitations of existing RDMA RPC and one-sided RDMA-\nbased schemes. To accomplish this design objective, we decouple\nthe index of Outback into two components: 1) a computation-heavy\ncomponent running on compute nodes, and 2) a memory-heavy\ncomponent running on memory nodes. In particular, DMPH pro-\nvides an opportunity for this decoupling. By carefully examining\nthe DMPHâ€™s read and insertion operations, we observe that the\nfinal step consistently is directly retrieving the value from a spe-\ncific memory location, while all the previous steps are employed to\ndetermine that location. Contrary to DMPH, other hash tables ne-\ncessitate retrieving the key from the hashed location by key probing\nand comparison, and only when the key matches the search key, the\nvalue can be returned. The distinctive process of DMPH motivates\nus to store all values in the memory-heavy components because\nthey can be read without extra computation. And the steps to deter-\nmine the location of the value can be placed in the compute-heavy\ncomponent running on the compute nodes.\nOutback requires only a single round trip for data requests while\nsupporting a large number of concurrent compute nodesâ€™s requests.\nIn contrast to other RDMA RPC-based approaches [ 20,22], Outback\nsubstantially reduces CPU resources required on the memory node.\nIn the following, we elaborate on the components maintained in\nthe compute pool and memory pool of Outback.\nFig. 4 depicts the overall structure of Outback, which leverages a\nshared-nothing architecture [ 47] for separating data into different\nshards with consistent hashing [23]. The compute pool comprises\nmultiple compute shards, each accommodating several compute\ncache bit fingerprint length Addr\nKey_len Value_len Key Valueslot slot slot slot DMPH Bucketï¼š\n1bit 6bit 9bit 48bit\nSlotï¼š\nKV Block ï¼š32BFigure 5: The data layout in a DMPH bucket.\nnodes. Note that the configuration for the number of shards and\nthe number of compute nodes depends on the memory budget\nin compute nodes and the whole size of the datasets. For each\nshard, an index is built based on the keys of the shard, and the\nreturned values of the index represent the memory locations that\nstore the corresponding data associated with the keys. The index is\ndecoupled into the compute-heavy and memory-heavy components.\nEach compute node is allocated a memory budget for caching the\ncompute-heavy component, including the bucket locator and the\nseeds. The default setting is there are 64 million keys in a shard,\nand the memory overhead on each compute node is less than 50MB\n(Â§5.8). This is considered a small overhead because recent one-sided\nRDMA solutions cost over 300 MB on each compute node for index\ncaching and other purposes [ 28,50]. All compute nodes in the same\nshard will connect to the memory node with RDMA RPC for data\noperations and one-sided RDMA for new bucket locator fetching\nafter index resizing â€“ the details will be explained in Â§4.4. Each shard\nconsists of one memory node, which contains the most updated\nbucket seeds, overflowed cache, DMPH buckets, and KV data in the\nshard. The DMPH buckets store the data addresses in the KV data\nmemory space of the keys in the shard. The latest bucket seeds are\nmaintained to ensure the consistency of data insertion. Additionally,\nthe overflowed cache for KV pairs is used to temporarily hold the\npair of the new key and the address, which cannot be inserted\ninto DMPH buckets without the need for hash table resizing. We\nleverage a hash table to work as the overflowed cache in Outback.\nThe KV data in each shard is replicated to two other shards, serving\nas replicas with checkpoints. These two replica shards can be chosen\nas the two successive shards in the consistent hashing ring. Each\nkeyâ€™s primary replica shard is referred to as the primary shard of the\nkey. Each shard is identified by a uuid . We assume there is a service\nlayer in front of the compute nodes responsible for only forwarding\ndata requests to one of the compute nodes in the primary shard\nbased on the keyâ€™s hash value in the consistent hashing ring. After\nthe memory node in the primary shard completes a data update\noperation, it forwards the update to its replica shards. To ensure\nload balance among compute nodes within a shard, the service layer\nmaintains a counter for each shard and distributes requests to the\ncompute nodes in a round-robin fashion.\n4.2 Decoupled DMPH index\nIn this section, we explain the detailed data structure and its com-\nponents maintained in the compute node and the memory node.\nWe reuse the design of Ludo hashing as introduced in Â§2. There are\ntwo candidate buckets for each key, and the bucket locator runs a\ndata structure called Othello [ 56] to determine which bucket the\nvalue of the search key is stored in. Each Ludo bucket contains one\n\nSeeds arrayMemory node\n10 A\n1 0 BhashA(k)\nhashB(k)Get \nInd_bkt sGet ind_slot\nBucket locatorAddrPair < ind_bkt , ind_slot >\nRPC\nDirectly go to \ntargeted slot to \nget data addr\nGet data back\nAccess \ndataData back\nDMPH bucket Data AreaNetwork\nâ·â¶â¹\nâ¸Compute node\nâ¼\nâ»âºData back(a)Getoperation.\n10 A\n1 0 BsGet the slot index \nfor search key\nBucket locator & seeds arrayAddrWrite \ndata \nDMPH buckets, overflowed cache and dataNetworkâ¶\nâ¸Write slot \nvalueâ·\ncacheWrite in cache\nif the slot is occupiedMemory node\nRPC Status backCompute node\nStatus back\nâ¹Tuple <key, val, ind_bkt > (b)Insert operation.\n10 A\n1 0 Bs AddrCheck key and \nupdate/delete data\nDMPH buckets and data area Networkâ¶â·â¸Memory node\nRPC Status backCompute node\nStatus back\nGet the slot index \nfor search key\nBucket locator & seeds arrayTuple <key, val, ind_bkt >\nAccess addr\nstored in slot (c)Update andDelete operation.\nFigure 6: Data operation protocols in Outback.\nseed and four slots. By computing a hash value with the search key\nand the seed, the key is mapped to an exact slot of the bucket with-\nout colliding with other keys within the same bucket. The value\nstored in the slot represents the keyâ€™s data address and is utilized\nto retrieve the corresponding data.\nWe decouple the entire data structure of Ludo hashing into two\ncomponents. The compute-heavy component running on each com-\npute node stores both the bucket locator and the seeds for all DMPH\nbuckets. This component completes all computations related to find-\ning the location that stores the value of the search key and costs\nonly 3.76ğ‘›bits â€“ 2.33ğ‘›bits for the bucket locator and 1.43ğ‘›bits\nfor the seeds, where ğ‘›refers to the number of KV pairs in a shard.\nWithin the memory node, the memory-heavy component consists\nof all DMPH buckets that store the data addresses for all keys in\nthe shard. Assuming the load factor of the DMPH table is set to ğœ–\nwith a default value of 0.95, the number of DMPH buckets will be\nğ‘›/(4Â·ğœ–)as each bucket accommodates four slots. The detailed lay-\nout for each DMPH bucket is illustrated in Fig. 5, and each bucket\nis 32-Byte long with four packed slots. There are four fields in each\nslot: cache bit (1 bit), fingerprint (6 bits), length (9 bits), and data\naddress (48 bits). The cache bit serves as an indicator to identify\nwhether another key(s) share the same slot, with its index stored\nin the overflowed cache. Meanwhile, the 6-bit fingerprint is only\nutilized during the index update process to verify if the KV data\nreferenced by the address in this slot corresponds to the search key\nor not. This fingerprint check is exclusively applied during data\nwrite requests, and any false positives do not impact the final result.\nThis is because a comprehensive recheck of the full key occurs after\naccessing the actual KV data block on the compute node side. Note\nthat read requests do not need to check the fingerprint. The address\nsignifies the starting offsets of the KV block, while the length indi-\ncates the byte length of the entire KV block in the underlying KV\ndata area. In the underlying data area, the KV block is compactly\nstored with four fields. The initial two numbers, each occupying 8\nbytes, denote the length of the key and the subsequent value field.\nThe overflowed cache accommodates the key-address pair that\ncannot be inserted into the mapped DMPH bucket without modify-\ning the bucket locator or resizing the entire hash table.\nFor an estimation, if ğœ–=0.95, the component at the compute\nnode contributes to only 5.5% of the total memory size of the indexwhile the component at the memory node accounts for the larger\nportion of 94.5%.\n4.3 Outback operations and protocols\nThis subsection presents the data operations and the corresponding\nprotocol of Outback, including the data Get,Insert ,Update , and\nDelete operations, as shown in Fig. 6.\n4.3.1 Data Get operation. As shown in Fig. 6(a), the compute node\nmaintains the bucket locator (two Othello arrays ğ´andğµ) and the\nseed arrayğ‘ . Meanwhile, the memory node maintains the DMPH\nbuckets that store KV addresses and the KV data in a disjoint mem-\nory area. When there is a data Getrequest for key ğ‘˜, the compute\nnode will â¶compute the bucket index from the bucket locator by\nlooking up two bits on the two arrays, respectively. Assuming the\nbucket index that stores the queried key is ğ‘–ğ‘›ğ‘‘_ğ‘ğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡ , the compute\nnode will then proceed to â·compute the slot number within the\nbucket with the hash function and the seed ğ‘ [ğ‘–ğ‘›ğ‘‘_ğ‘ğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡]. At this\npoint, the compute node â¸gets both the bucket index and slot index\nin the MPH buckets, and it â¹posts them to memory nodes with\nRDMA_SEND in the opaque fields. After the memory node gets\nthe message and parses the index numbers of the bucket and slot,\nğ‘–ğ‘›ğ‘‘_ğ‘ğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡ andğ‘–ğ‘›ğ‘‘_ğ‘ ğ‘™ğ‘œğ‘¡, it will âºgo directly to the MPH buckets\nto access the exact slot without any extra computation. Then, the\nmemory node â»gets the data offset in the underlying KV data area\nfrom the last 48-bit field of the slot. At last, â¼the KV data will be\nread back and returned to the initiating compute node for full key\ncheck. For example, when a compute node requests data for key 5, it\ncomputes the bucket index 10 and slot index 0 based on the bucket\nlocator and the locally stored seeds. Then, the pair of indices (10,0)\nis sent to the memory node. The data index stored in the indicated\nslot of the memory node is read, and the corresponding data block\nis returned. Lastly, the compute node checks the cache bit and a\nfull key to see if the Makeup Getis needed.\nThere could be some KV pairs that are temporarily inserted into\nthe overflowed cache during the updates and reconstruction of\nthe index. In this circumstance, the compute node is tasked with\nchecking the cache bit, ensuring that the returned full key aligns\nwith the queried one. If the key does not match the requested one,\nand the cache bit in the slot is set to 1, the compute node will\ninitiate another Getmakeup request with the ğ‘–ğ‘›ğ‘‘_ğ‘ ğ‘™ğ‘œğ‘¡specified\nas -1, signaling the memory node that the returned key does not\nmatch the requested key. While it is possible to offload the full key\n\ncomparison task to the memory node, saving one round trip, this\napproach introduces computation overheads on the limited remote\ncore resources. To make the common case easy, we opt to assign\nthe full key check task to compute nodes.\nMakeup Getrequest. When the KV data returned to the com-\npute node does not match the requested key, there are two reasons:\n(1) The requested key is kept in the overflowed cache. The KV pair\nis inserted after the DPMH table is constructed, and the hashed slot\nis occupied by another key. (2) The requested key is in another slot\nof the hashed bucket. This case results from changing the order of\nkeys based on the new seed within the bucket when the inserted\nkey can fit into the current DMPH table (detailed in Section 4.3.2).\nDue to the above two situations, the compute node will send the\nmakeup Getrequest with the ğ‘–ğ‘›ğ‘‘_ğ‘ ğ‘™ğ‘œğ‘¡as -1 to the memory node.\nThe memory node will search the overflowed cache first; if there\nis a cached item matching the full key of the requested key, it will\nread the data and return it to the compute node. If not, it will read\nout all the KV blocks referred by the hashed bucket (at most four)\nand compare the keys until it finds the requested key. Additionally,\nthe new seed will be returned back to the compute node if the key is\nfound in another slot, and the compute node will update the copied\nseeds array for this bucket locally.\n4.3.2 Data Insert operation. The main idea of implementing the\ndata Insert operation of Outback is to determine if we can insert\nthe key into the index without significant changes to the current\nbucket locator. If an Insert operation only requires changing the\nvalue in one DMPH bucket, Outback can make this change directly.\nHowever, if a Insert operation will cause the index to resize, which\nusually happens after a number of insertions, Outback needs to en-\nsure the correctness of the Insert operation and following lookups\nduring index resizing. As shown in Fig. 6(b), like Getoperation, the\ncompute node will get ğ‘–ğ‘›ğ‘‘_ğ‘ğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡ andğ‘–ğ‘›ğ‘‘_ğ‘ ğ‘™ğ‘œğ‘¡from the bucket\nlocator and the seeds through multiple hashing computations. Dif-\nferent from Get, the RPC message posted to QP should include the\nfull key. Thus, the memory node can parse the ğ‘–ğ‘›ğ‘‘_ğ‘ğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡ ,ğ‘–ğ‘›ğ‘‘_ğ‘ ğ‘™ğ‘œğ‘¡,\nand the key from the message and execute the following steps. â¶\nthe memory node will write the data into the underlying data area,\nthen it can get the data length and the address (offset in the data\narea) for indexing. After the memory node composes the value from\nthe corresponding slot with the cache bit (set to zero by default),\nfingerprint, length as well as address, it â·will try to insert it in the\nDMPH table.\nWe discuss the rest of Insert in three cases:\nâ€¢Insert without bucket locator and seed change. The mem-\nory node checks the slot indicated by ğ‘–ğ‘›ğ‘‘_ğ‘ğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡ andğ‘–ğ‘›ğ‘‘_ğ‘ ğ‘™ğ‘œğ‘¡. If\nthe length field is empty (length is 0), signifying there is no key\nassociated with this slot, the memory node inserts the composed\nslot value (Fig. 5) into this location and returns SUCCESS to the\ncompute node. Conversely, if the length is non-zero, indicating that\nan existing key is using this slot, the memory node proceeds to\ncheck the fingerprint and compares the full key to determine if the\noriginal key in this slot matches the inserted key. If they match,\nthe insertion is resolved and treated as an Update operation. The\nfingerprint can prevent the memory node from reading the full key\nin the KV data area if they are not the same.\nâ€¢Insert with seed changes but the bucket locator remains\nthe same. If the key associated with the targeted slot does notmatch the newly inserted one, an examination is made to deter-\nmine if there is another available slot within this bucket. Assuming\nthere are only three keys in this bucket, and the slot indicated by\nğ‘–ğ‘›ğ‘‘_ğ‘ ğ‘™ğ‘œğ‘¡is already occupied by a different key, the memory node\nendeavors to find a new seed that accommodates all four keys in\nthe bucket without causing collisions, thereby preserving perfect\nhashing policy in this bucket. The other three keys are read from\nthe underlying KV data area, and the memory node employs a\nbrute-force approach to identify a new seed for perfect hashing\nwithin this bucket. Importantly, the bucket locator does not need\nto change because all four keys remain in the same bucket. Subse-\nquently, the updated seed for this bucket is returned to the compute\nnode, which then propagates this modification to other compute\nnodes in the same shard.\nâ€¢Insert data to overflowed cache. When all four slots within\nthe bucket are occupied, and the memory node is unable to find an\nempty slot for the inserted key, the pair of the key and the KV block\naddress will be â¸placed in the overflowed cache. Also, the cache\nbit in the conflicted DMPH slot will be set to 1 to indicate at least\none key in the overflowed cache sharing the same hash slot. Instead,\nwhen the number of KV pairs in the overflowed cache reaches a\npredefined threshold, the memory node initiates the index resizing\nprocess to accommodate more KV pairs in a new DMPH table.\nThe data insertion process on each memory node works as fol-\nlows. At first, the memory node will lock the data operations on\nthe targeted bucket to prevent the potential data operations on this\nbucket. The inserted key might have been stored in the DMPH table\nbefore. Thus, the memory node will check if the insert request can\nbe resolved to a data update operation by comparing the fingerprint\nand the underlying full key. Then, the memory node first writes the\nKV block to the underlying data area and processes the data insert\nrequest based on the stored bucket keys into the mentioned three\ncases. Finally, the memory node unlocks the bucket after it finishes\nthe data insert operation. Note that the data insert request tuple\nsent by the compute node consists of the KV pair and ğ‘–ğ‘›ğ‘‘_ğ‘ğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡ ,\nnot including ğ‘–ğ‘›ğ‘‘_ğ‘ ğ‘™ğ‘œğ‘¡. The reason is that the memory node keeps\nthe most update seeds array in the shard and can use the seeds to do\nthe hash computation as the slot locator. Also, the bucket locator is\nnot maintained in the memory node, and the data insert operation\nwill not modify it after the DMPH table is constructed every time.\nThis choice is made because modifying the bucket locator requires\nchanging seeds for keys in at least two buckets, leading to more\ncomputational overhead.\n4.3.3 Data Update and Delete operations. For data update and dele-\ntion, the compute node also acquires the ğ‘–ğ‘›ğ‘‘_ğ‘ğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡ andğ‘–ğ‘›ğ‘‘_ğ‘ ğ‘™ğ‘œğ‘¡\nfrom the bucket locator and the seeds array. Like the Insert oper-\nation, the compute node transmits the full key to the memory node.\nAs illustrated in Fig. 6(c), the memory node directly accesses the\naddress of the KV data from the DMPH bucket and verifies whether\nthe requested key matches the underlying data. Once the memory\nnode confirms the key, for Delete , it marks the length of the slot\nvalue as zero and returns the corresponding status. In the case of\nUpdate , it writes the new data to the underlying data area. If the\ncache bit is set to 1 and the keys differ, the memory node will go to\nthe overflowed cache to get the data address.\n\nBucket \nSeeds\nDMPH \nTable 212000110 11\nDirectory \nindex\nHash tables \nindices\nBucket \nSeedsBucket \nLocator000110 11\nHash \ntables Local \ndepth\nCompute node Memory nodeDirectory \nindexFigure 7: Extendible hashing in Outback.\n4.3.4 Concurrency control. Each bucket in the DMPH table within\nthe memory node has a mutex lock. Prior to executing any Insert ,\nUpdate , orDelete operation, the relevant bucket is locked, blocking\nany access to its indices. Subsequently, the operation is executed\nand the lock is released. During the lock period, all other operations\ntargeting this bucket are buffered and only processed once the lock\nis released.\n4.4 Index resizing\nWhen the number of KV pairs in the overflowed cache surpasses\na predefined threshold, index resizing and reconstruction become\nnecessary to accommodate the KV pairs into a new hash table.\nThis resizing process introduces two challenges: (1) managing data\noperation requests during resizing and (2) efficiently coordinating\nthe compute node and memory node to transfer the bucket locator\nand seeds.\nTo support data requests on runtime while index resizing, we\napply extendible hashing [ 32,66] to allocate a new DMPH table\nto accommodate more keysâ€™ indices, and a directory index is used\nto identify the multiple DMPH tables, which is an additional hash\nlayer as shown in Fig. 7. This approach reduces the number of\nkeys that need to be moved during index resizing and shortens\nthe resizing duration. Compute nodes maintain the bucket locator\nand seeds array for each single hash table, while memory nodes\nstore the most update seeds array and DMPH tables, as well as local\ndepth array [32, 66].\nIn each shard, we have two size thresholds for overflowed cache;\nOne is for slowing down insertions, ğ‘ ğ‘ ğ‘™ğ‘œğ‘¤. The memory node reach-\ning this threshold will enter the index resizing process. The other\nthreshold is the size when the memory node stops any follow-\ning insertions ğ‘ ğ‘ ğ‘¡ğ‘œğ‘ even if the index resizing is not finished and\nğ‘ ğ‘ ğ‘¡ğ‘œğ‘>ğ‘ ğ‘ ğ‘™ğ‘œğ‘¤. We setğ‘ ğ‘ ğ‘™ğ‘œğ‘¤ as the load factor of the DMPH table\nbecomes 97%, or the overflowed cache is filled with half of the size.\nğ‘ ğ‘ ğ‘™ğ‘œğ‘¤ is set when the overflowed cache is filled with over 90% space.\nAs shown in Fig. 8, when â¶the overflowed cache size reaches\nğ‘ ğ‘ ğ‘™ğ‘œğ‘¤ after an Insert request from a compute node, â·the memory\nnode will return the status PRE_RESIZE to the compute node, and\nthe compute node will create a new connection manager for prepar-\ning and listening to build a one-sided RDMA connection with the\nmemory node. The memory node will return PRE_RESIZE to the\ndata requests for all compute nodes in this shard and count up the\nnumber of compute nodes that got the information. After all the\ncompute nodes get it or the overflowed cache size reaches ğ‘ ğ‘ ğ‘¡ğ‘œğ‘,\nThe memory node will build the one-sided RDMA connection (RC)\nwith all compute nodes. The registered memory area in the memory\nnode consists of five fields: (1) The value of the first eight bytes\nğ‘ğ‘ğ‘ğ‘œğ‘‘ğ‘’ indicates the number of compute nodes in this shard, but\ns\ns\ns\nMemory NodeCompute Nodes1\n2\n3Seeds and BktlocatorClients request to insert ğ‘˜ğ‘¡\nCache is almost full after inserting ğ‘˜ğ‘¡ â·\nâ¸â¶\ndata insert data query RDMA_READ the Bucket \nlocator arrays and seedss\nNcNode Len\nâ¹Polling until NcNode is \ngreater than zero\n10 A\n1 0 BGlobald\n1\n0\nRequests\ncacheStale DMPH TableFigure 8: Index resizing in Outback.\nit is set to zero at the beginning to indicate that the new index has\nnot been completely reconstructed. After it finishes, the value will\nbe set to the number of compute nodes in this shard; (2) the second\nvalue of the following eight bytes ğ‘™ğ‘’ğ‘›refers to the total length of the\nnewly written bucket locator arrays and seeds array; (3) ğºğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘‘\nrefers Global depth [ 32] value in current extendible hashing; (4)\nnewly computed seeds array; and (5) bucket locator arrays ğ´andğµ.\nOn the compute node, once a connection is established with\nthe memory node, it continuously sends RDMA_READ requests\nto retrieve the first two values ğ‘ğ‘ğ‘ğ‘œğ‘‘ğ‘’ andğ‘™ğ‘’ğ‘›in the registered\nmemory of the memory node. If ğ‘ğ‘ğ‘ğ‘œğ‘‘ğ‘’ is greater than zero, that\nmeans the bucket locator arrays and the seeds array have been\nsuccessfully constructed and written into the memory area. â¸The\ncompute node then issues another RDMA_READ requests to fetch\nall the subsequent ğ‘™ğ‘’ğ‘›data. Additionally, an atomic primitive of\nfetch-and-add FAAis executed to decrement ğ‘ğ‘ğ‘ğ‘œğ‘‘ğ‘’ by one, signi-\nfying the completion of a compute node fetching the new index\ndata.\nBefore the new bucket locator and seeds array is constructed,\nupon receiving an Insert orDelete request, the memory node\nreturns a FALSE status to compute nodes. Then, the memory node\ncaches the Insert /Delete requests and implements them later\nafter the index data moves to the new DMPH table. For Getand\nUpdate requests, the memory node will continue serving it on\nthe stale DMPH table. The reason is that no new data insertion\nwould be implemented during resizing, and the keysâ€™ ğ‘–ğ‘›ğ‘‘_ğ‘ğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡\nandğ‘–ğ‘›ğ‘‘_ğ‘ ğ‘™ğ‘œğ‘¡will not change.\nOnce all compute nodes have obtained the new bucket locator\narrays and seeds, ğ‘ğ‘ğ‘ğ‘œğ‘‘ğ‘’ in the memory node becomes zero. The\nmemory node detects this change through periodic checks at a fre-\nquency of 2 times a second. It proceeds to discard the bucket locator\narrays to free up memory space, as they will remain unchanged\nuntil the next MPH resizing. The memory node will also delete all\nmoved keys in the stale DMPH table by marking the length field\nas 0. Then, the reliable connections with all the compute nodes\nwill be terminated by the memory node, and all the compute nodes\nshift to use both the DMPH tables with the extendible hashing for\nprocessing data requests.\nNote that all hash table-based disaggregated KVS require en-\nlargement and shrinking capacity at runtime. The computation\ntime for the extendible hashing layer is the same for Outback and\nprior works [ 14,32,66]. In Section 5.9, we will show the influence\non Outback throughput during index resizing.\n\n4.5 Analysis\nIn this section, we provide the theoretical analysis of the time\ncomplexity of the various data operations, as well as the estimation\nof the memory cost in both compute nodes and memory nodes.\nTime complexity. ForGetoperations, each compute node is tasked\nwith determining locations of the DMPH bucket and slot that stores\nthe address of the requested KV. This involves two hash compu-\ntations, namely â„ğ‘ğ‘ â„ğ´(ğ‘˜)andâ„ğ‘ğ‘ â„ğµ(ğ‘˜), to access two bits in the\nbucket locator arrays. Subsequently, an additional hash computa-\ntion with the bucket seed is performed to locate the specific slot.\nThen, the memory node can access the slot without further compu-\ntation and proceed to read data from the referenced KV block. By\ndefault, we use a (2,4)-Cuckoo hash table [ 38] as a fallback table if\nno seeds can perfectly hash the four elements. In the worst case,\naccessing the Cuckoo hash table requires two additional hash com-\nputations and at most 8 key checks, resulting in a time complexity\nof O(1) for operations involving the Cuckoo hash table. Therefore,\nthe worst case complexity remains O(1). For both the compute node\nand the memory node, the data Getoperation incurs a small con-\nstant time. This time complexity extends to data update and data\nremoval operations.\nThe only difference in Insert lies in the potential time overhead\nincurred in finding a new seed for the keys in the bucket. To address\nthis, we have set a maximum number of trying times to 256 (8-bit\nseed). The reason is that we have not encountered a scenario in\nwhich no seed can be found within [0, 255] to separate those four\nkeys without collision. We also have a fallback table (storing the\nkey and the KV block address) to deal with rare cases when a group\nof keys appears that cannot be distributed into distinct slots by\nMPH. Statistically, we have observed no buckets that cannot be\nperfectly hashed with a seed length of 8. Therefore, the time cost\nassociated with data insertion is also constant.\nMemory usage. In compute nodes, the memory usage is allocated\nto the bucket locator and bucket seeds. According to Ludo [ 44], the\nbucket locator arrays consume 2.33 bits per key. The 8-bit seed is\nshared among four keys in a bucket. Assuming there are ğ‘›KV pairs\nin a shard, with a load factor of ğœ–for the MPH table, the memory\ncost in a compute node is calculated as (2.33+2/ğœ–)ğ‘›bits.\nIn addition to the underlying KV data, memory nodes allocate\nmemory to encompass the latest bucket seeds, DMPH buckets, and\nthe overflowed cache. Each bucket incurs a cost of 32 bytes, and the\ncache item contains the full key size and the data address. Given\na cache size of ğ‘šand a cache item size of ğ‘bits, the overall space\nbudget (in bits) for indexing in a memory node is 66ğ‘›/ğœ–+ğ‘šÂ·ğ‘.\n4.6 Discussion\nGeneral applicability on traditional data structures. The de-\nsign principle of Outback can boost data search in traditional data\nstructures with the capability of serving range queries. Specifically,\nperfect hashing can boost the search process with one-time hash\ncomputation with low memory costs that can be cached in compute\nnodes. For example, the binary search in B/B+ tree leaf nodes can\nbe replaced by perfect hashing computation by searching a seed\nfor hashing keys in leaf nodes.\nShip computation to data. Outback decouples the process of\nDMPH into a memory-heavy component at memory nodes and a\ncompute-heavy component at compute nodes and allows them tocommunicate via RDMA-RPC primitives. However, the memory\naccessing based on the given ğ‘–ğ‘›ğ‘‘_ğ‘ğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡ andğ‘–ğ‘›ğ‘‘_ğ‘ ğ‘™ğ‘œğ‘¡still needs\na weak power computation unit close to data [ 55]. We can apply\nOutback to another two promising approaches without using two-\nsided RDMA verbs.\nâ€¢Extended RDMA READ verb. PRISM [ 7] proposes and simulates\nan extended one-sided RDMA indirect reading verb RDMA_READ\n(ptraddr,size len,bool indirect ), where indirect indicates if\nRNIC is supposed to read back the data pointed by the addr. This\nembedded one-sided RDMA verb can free the memory nodeâ€™s\nCPU and offload the memory reading task in Outback to RNICs.\nThe reason is that Outback can get the exact requested data\naddress without potential data probing.\nâ€¢Performance capacity of Outback with hardware accelerators. In-\nnetwork computation [ 63] has gained attention for accelerating\ndata services in distributed systems by offloading tasks to in-\nnetwork computation devices [ 18,59] such as SmartNICs/DPUs\nand CXL [ 2]. The idea of Outback can reduce the computation\nburden on SmartNICs by employing one round-trip, one-sided\nRDMA_READ. For example, a SmartNIC [ 6,30,41,45] can be\nplaced on the memory node side, and function as an additional\ncomputation unit, and indirect data access tasks can be offloaded\nto it [ 51]. After the compute nodes in Outback issue a one-sided\nRDMA to read the queried keyâ€™s slot and retrieve the address from\nthe DMPH buckets, the SmartNIC can read the memory again via\nthe PCIe switch and obtain the queried data through an additional\nPCIe round trip. The computation and data search tasks offloaded\nto the SmartNIC can be alleviated with the assistance of DMPH\nfor the least computation burden.\nShared-nothing architecture. Outback utilizes a shared-nothing\narchitecture [ 47] to prevent the update of cached seeds across com-\npute nodes in different shards. The number of KV pairs in each\nshard depends on the overall size of the database and the number\nof shards. A greater number of shards results in fewer KV pairs\non each memory node. Consequently, the memory allocation for\nDMPH seeds and bucket locator on each compute node can be\nreduced, although additional memory nodes are required. Deter-\nmining the granularity for sharding KV pairs has always been a\ntradeoff [ 65], and it is recommended to choose the configuration\nbased on the specific application.\n5 PERFORMANCE EVALUATION\n5.1 Methodology\nTestbed. We run experiments in two environments. 1) 6 r650 ma-\nchines from a public cluster CloudLab [ 15]; each of them is equipped\nwith one Two 36-core Intel Xeon Platinum 8360Y CPU at 2.4GHz,\n256 GiB DRAM and one Dual-port Mellanox ConnectX-6 (CX-6) 100\nGbE NIC with Driver version as MLNX_OFED_LINUX-4.9-5.1.0.0.\nWe conduct experiments with two shards, and each shard contains\n3 machines. We use one machine as the memory node and the other\ntwo as compute nodes. The memory node registers the memory\nwith huge pages to reduce RNICâ€™s page cache misses, which is\nbeneficial for memory-intensive applications [ 50,66]. On compute\nnodes, we use two coroutines on each client thread to increase the\nquery efficiency (See analysis in Section. 5.5). This is the default ex-\nperiment environment unless otherwise stated. 2) 9 r320 machines\nin CloudLab [ 15], each of them is equipped with one Xeon E5-2450\n\n(a) Workload A.\n (b) Workload B.\n (c) Workload C.\n (d) Workload D.\n (e) Workload F.\nFigure 9: Throughput under YCSB benchmark with single memory node thread with Mellanox CX-6.\n(a) Workload A.\n (b) Workload B.\n (c) Workload C.\n (d) Workload D.\n (e) Workload F.\nFigure 10: Throughput under YCSB benchmark with Mellanox CX-3 RNICs.\nCPU (8 cores, 2.1Ghz), 16 GiB DRAM, and one Mellanox MX354A\nDual port FDR CX3 adapter. We use 1 machine as the memory\nnode and the other 8 as compute nodes. We utilize 64-byte RDMA\nmessages for all workloads to encapsulate various operation types\n(RC READ, UD SEND, and UD RECV), ensuring each request is\npadded to span two cache lines [ 21]. We do not use batching at any\nlayer to minimize the latency in all evaluations.\nWorkloads. To evaluate the overall performance of Outback and\nother baselines, we employ YCSB [ 3,12] workloads along with\ntwo diverse real-world datasets [ 35]. These datasets are (1) FB, en-\ncompassing a random assortment of Facebook user IDs to analyze\npatterns within social media interactions; (2) OSM, providing digi-\ntized infrastructure footprints from Open Street Map to represent\ngeographical and spatial data usage; To ensure the datasets reflect\ngeneral, unsorted data conditions, we shuffle them if initially sorted\nupon loading. Unless specified, we use 8B keys and 8B address\nvalues to configure all workloads like existing schemes [ 25,28]\nfor comprehensive evaluations. For each run, we precondition the\nmemory node and warm up the database with 64 million KV pairs\nat first and then issue 10M requests to the benchmark on top of it.\nBaselines. We develop a prototype of Outback based on RDMA\nlibraries rlib and r2 [ 52] with over 4000 LoC in C++. We compare\nOutback with the other three baselines, one is a recently proposed\none-sided RDMA scheme, RACE hashing [ 66], which utilizes RDMA\nRC READs for its operations; The other two are two-sided RDMA\nschemes that operate on RDMA SENDS/RECVs, differing in their un-\nderlying data structures â€“ MICA [ 20,29] and Cluster hashing [ 11].\nâ€¢RACE hashing. RACE hashing [ 66] is a representative one-sided\nRDMA scheme developed recently. It offloads all data operations\nto compute nodes to free the memory node CPU with one-sided\nRDMA primitives. RACE Hashing adopts an RDMA-friendly hash\ntable to combine the overflow bucket for collided keys and the\nhashed bucket. Thus, all the candidate buckets containing therequested key can be read back together. We develop RACE hash-\ning with over 1,400 lines of C++ code, excluding the benchmark\npart that is shared with other baselines.\nâ€¢RDMA RPC-MICA. RPC-MICA is a two-sided RDMA-based\nscheme with a data structure MICA [ 20,29], which is an efficient\nhopscotch hash table and it has been used in existing two-sided\nRDMA [ 20,22]. The overflowed KV pairs can be stored in the\nbucket adjacent to its hashed bucket. We implement hash com-\nputation for the bucket number on the compute node and send\nthe queried keyâ€™s fingerprint and bucket number to save com-\nputation on the memory node. We apply the open-source code\nfrom MICA [ 29] in our benchmark, utilizing it as the underlying\ndata structure for the RPC-based approach without batching.\nâ€¢RDMA RPC-Cluster hashing. RPC-Cluster hashing is a two-\nsided RDMA baseline with Cluster hashing, a chained-based\nhash table with associativity, running on memory nodes [ 11,52].\nThe overflow keys that are hashed to a full bucket will be put\nin the linked indirect bucket. Each slot in a bucket includes 14\nbits of fingerprint for key comparison. We apply the open-source\ncode [ 4] of the cluster hashing as the data backend of our RPC-\nbased scheme suit.\n5.2 Performance on YCSB\nPerformance with CX-6 RNICs. We show the throughput of all\nevaluated methods by increasing the request load of running 8,\n12, 20, 72, 108, and 144 compute node threads in a shard. On the\nmemory node, we consistently allocate only one thread to run on\na single core. As shown in Fig 9, these five figures illustrate the\nthroughput and latency results under YCSB workloads A, B, C, D,\nand F, respectively.\nGetand Update workloads (YCSB A and B). YCSB A and\nB workloads include 50% and 5% data Update respectively and\nthe remaining is Get. Outback can achieve 5.50 and 5.82 Mops\n\nthroughput for YCSB A and B, as shown in Fig. 9(a) and Fig. 9(b).\nAll other methods show lower throughput with the same number\nof threads. Outback can provide up to 1.07 Ã—and 1.06Ã—throughput\nimprovements on workloads A and B respectively, compared to RPC-\ncluster hashing. Compared to other RPC baselines with associative\nhash tables, the memory node in Outback is offloaded with less\ncomputation because it only needs to read the targeted key, and no\ndata probing or traversing is needed to find the targeted value of\nthe key. RACE hashing requires three round trips for updating data\nconsistently, significantly increasing the latency and limiting the\nthroughput. By comparing the results between workloads A and\nB, when more Update requests are issued, Outback spends more\ncomputation resources for value rewriting and key checking by\nreading the underlying KV blocks indicated by the computed MPH\nslot. Hence, Outback under YCSB B provides higher throughput\nthan Outback under YCSB A.\nGet-only workload (YCSB C). ForGet-only workload, Out-\nback can achieve 6.01 Mops throughput. When the number of com-\npute node threads reaches 72, Outback outperforms RACE hashing,\nMICA, and Cluster hashing by 1.31 Ã—, 2.43Ã—, and 1.11Ã—on total\nthroughput, respectively. The performance of RACE hashing is\nbottle-necked by its two round trips and the limited RNIC memory\nto cache queue pair (QP) state of a larger number of reliable con-\nnections. Outback reduces the average memory nodeâ€™s CPU time\nfor data Getrequest with less computation overhead than the other\ntwo RPC-based baselines while looking up a key.\nGetand Insert workloads (YCSB D and F). YCSB D contains\n5%Insert and 95% Getoperations. YCSB F contains 25% Insert ,\n25%Update , and 50% Getoperations. Under YCSB D, Outback still\nshows the highest throughput among all methods. For Insert oper-\nations, Outback will check if a slot in the target bucket is available.\nKey-checking is also required, and a new seed will be calculated if\nthe target slot stores an existing value. The high rate of Insert op-\nerations in YCSB F pulls the throughput down to 3.62 Mops, which\nis similar to RPC-Clustering hashing (3.64 Mops) when the number\nof client threads reaches 144.\nPerformance with CX-3 RNICs. As shown in Fig. 10, we show\nthe throughput with the 4 memory node threads and a set of com-\npute node threads numbers 8, 16, 24, 32, 48, and 64, respectively.\nOutback can consistently achieve the highest throughput for read-\nintensive workloads (A, B, C, and D). Significantly, Outback outper-\nforms RACE hashing, MICA, and Cluster hashing by 5.03 Ã—, 1.79Ã—,\nand 1.23Ã—on total throughput for workload C, respectively. When\nwe use a weaker CPU, the advantage of Outback is more signifi-\ncant. Unfortunately, CloudLab does not offer a weaker CPU with a\nhigh-performance network.\nIn summary, Outback demonstrates the highest throughput for\nmost types of workload (YCSB A, B, C, and D). For a workload that\nisInsert -intensive such as YCSB F, Outback provides comparable\nthroughput to other RDMA-RPC methods but still higher than that\nof one-sided RDMA.\n5.3 Evaluations on Real-World Datasets\nWe leverage the SOSD datasets [ 35] for evaluations. Fig. 11 illus-\ntrates throughput results with the number of compute node threads\nas 8, 12, 20, 72, 108, and 144 in a shard. We set the number of mem-\nory node threads to 1. Each compute node thread issues 10 million\n(a) Dataset FB, uniform workload.\n (b) Dataset OSM, uniform workload.\n(c) Dataset FB, zipfian workload.\n (d) Dataset OSM, uniform workload.\nFigure 11: Data Getthroughput performance with SOSD\ndatasets with uniform and zipfian-0.99 workloads.\nkey lookup requests selected from the datasets in a uniform or\nzipfian distribution.\nCompared to RACE, Outback achieves throughput of 1.38 Ã—,\n1.35Ã—, 1.39Ã—, and 1.38Ã—respectively on these four different settings\nwhen the number of threads reaches 144. RACEâ€™s performance is\nconstrained by the multiple round trips. Compared to RPC-MICA\nand Cluster hashing, Outback achieves a throughput of 2.03 Ã—and\n1.1Ã—respectively on dataset FB when the threads number reaches\n144 in Fig. 11(a). The reason that Outback can outperform them\nis that Outback can go directly to access data without extra check\ncomputation and indirect data accessing to probe the hash chain\nor buckets. Also, Outback outperforms RACE hashing, RPC-MICA\nand RPC-CLuster hashing by 1.35 Ã—, 2.05Ã—, and 1.13Ã—respectively\non dataset FB when the workload follows the Zipfian distribution,\nas shown in Fig. 11(c). We observe the same trend in performance\ncomparison with the dataset OSM.\n5.4 Scalability with memory node threads\nIn this set of experiments, we vary the number of memory node\nthreads from 1 to 3 and observe the throughput of different meth-\nods using real-world datasets FB and OSM. To exhaust the CPU\nresources on the memory node side, we use four r650 servers as\ncompute nodes with 288 compute node threads.\nFig. 12 shows the throughput of three RDMA-RPC schemes, by\nvarying the memory node threads from 1 to 3. The throughput of\nOutback is around 1.10-1.21 Ã—of Cluster hashing and around 3 Ã—of\nMICA for dataset FB. The results of the two datasets exhibit the\nfact that as the number of compute node threads increases, the per-\nformance ratio between Outback and RPC-Cluster hashing/MICA\nremains similar. The reason is that Outback can ease the CPU bur-\nden on the memory node and allow it to handle more data requests\nfrom the compute node threads by offloading the computation of\nindexing to compute nodes.\n\n(a) Scalability with memory node threads\non dataset FB.\n(b) Scalability with memory node threads\non dataset OSM.\nFigure 12: Throughput vs. the number of memory node\nthreads.\n(a) Latency-throughput curve on YCSB-C\nwith 1 memory node thread.\n(b) Latency-throughput curve on YCSB-C\nwith 2 memory node threads.\nFigure 13: Latency vs. the number of coroutines.\nThe fact that Outback achieves higher relative throughput to\nother RPC methods under a small number of memory node threads\nactually demonstrates the main advantage of Outback: achieving\nhigh performance when the memory node carries weak CPU power\nin a disaggregated memory system.\nNote that the aim of Outback is not to saturate RNIC but\nto increase the throughput when there are limited CPU re-\nsources in a memory node with two-sided RDMA primitives.\nThe results in this section show that Outback can achieve\nhigher CPU efficiency with the same throughput goal, and\nOutback can realize higher throughput with the same CPU\nresources. In disaggregated systems, this can motivate the in-\ndustry to satisfy the userâ€™s throughput goal with less TCO by\nreducing the CPU resources equipped on memory-optimized\ncloud instances [5].\n5.5 Influence of the number of coroutines\nThe coroutines within compute node threads are designed to yield\nupon dispatching a request and resume operation upon receiving\nresponses from two-sided RPCs. The default setup of Outback uses\ntwo coroutines per thread, but we extend our evaluation to explore\nthe influence of one or more per thread to ascertain the optimal con-\nfiguration for maximizing server CPU utilization. Fig. 13 studies the\nlatency-throughput performance of Outback in YCSB-C workload\nwith different numbers of coroutines in a compute node thread. In\nFig. 12(a), we have only one worker thread in the memory node and\nvary the total of compute node threads as 8,20,72,144 and 216 dis-\ntributed among three compute nodes, respectively. We can observe\nthat a larger number of coroutines results in higher throughput\nwhen the number of compute node threads is less than 72, and the\nlatency doubles or triples after the throughput reaches around 6\nFigure 14: Influence of differ-\nent load factor set in DMPH.\nFigure 15: Influence of the\nvaried number of KV pairs.\nFigure 16: Memory usage on compute node with the varied\nnumber of KV pairs.\nMops, the maximum throughput one memory thread can support.\nThis phenomenon is similar when the number of memory node\nthreads is 2, as shown in Fig. 12(b), because the CPU resource on the\nmemory node can handle 144 compute node threads, and the total\nthroughput of a memory node can reach to 9.89 Mops. However, the\nextra coroutines will incur high latency of the data query after the\nnumber of memory node threads becomes a bottleneck for serving\n216 threads.\n5.6 Influence of load factor in DMPH\nThe load factor in a hash table is the ratio of stored elements to the\ntotal number of available slots or buckets. Maintaining an optimal\nload factor balances memory usage and data operation throughput.\nWe evaluate the data Getthroughput in Outback with varied load\nfactors from 0.75 to 0.95.\nAs shown in Fig. 14, Outback can achieve around 6 Mops with 72\ndata query threads from compute nodes in a shard for the dataset FB.\nSimilarly, the influence of the varied load factors on the throughput\nis trivial based on the results of the dataset OSM.\n5.7 Influence of the number of KV pairs\nFig. 15 studies the impact of the number of KV pairs in each shard.\nWe load 20M, 50M, and 80M KV pairs in Outback and evaluate\nthe data Getthroughput on two real-world datasets, respectively.\nOutbackâ€™s read throughput decreases from 6.02 to 5.83 Mops as\ndatabase size enlarges on the dataset FB. Similarly, we can observe\nthe data read throughput decreases by 3.1% on the dataset OSM.\n5.8 Memory usage in compute nodes\nIn a disaggregated memory system, compute nodes are regarded\nas the ones with rich computing resources but limited memory\nspace. To make the memory node serve data requests with the\nleast computation based on RDMA RPC primitives, we offload as\nmuch computation to the compute side with the help of DMPH.\nIn this section, we evaluate the memory cost of Outback on each\ncompute node with the varied number of KV pairs in each shard.\nThe memory usage on a compute node consists of the bucket locator\nand the seeds array.\n\nresizing\nresizing resizingFigure 17: Influence of extendible hashing resizing.\nAs shown in Fig. 16, we vary the load factor used in the DMPH\ntable from 0.80 to 0.95, and we use an 8-bit seed for keys in each\nbucket. The memory usage at each compute node for 20 million\nKV pairs per shard is around 12.5MB, and the cost is below 60MB\nfor 100M KV pairs per shard. This is considered a small overhead\nbecause recent one-sided RDMA solutions cost hundreds of MBs\nor more on each compute node for index caching and other pur-\nposes [ 28,50]. For example, in XStore [ 50], 100 million key-value\npairs require over 600MB of memory at a compute node without\nincluding the cache.\n5.9 Throughput during index resizing\nWe evaluate the throughput changes during index reconstruction\nand resizing. In this set of experiments, we bulk-load 20M keys\nto the database with the initial DMPH table to warm up, and we\nset one compute node with 8, 12, and 16 threads connecting to the\nmemory node running only one thread, respectively. This emulates\na challenging scenario because the memory node has limited com-\nputing resources to handle both resizing and lookups. The workload\nrunning on compute nodes is YCSB D, which contains 5% data in-\nsert and 95% read. As shown in Fig. 17, it takes around 3 seconds to\nrecalculate the bucket locator and the seed for each bucket. Outback\nstill supports partial Getrequests during resizing with a decreased\nthroughput by approximately 52% with only one thread in the mem-\nory node. The CPU contention causes a performance drop, and the\nperformance goes back to normal after resizing.\n5.10 Summary of evaluation\nData lookup throughput. Outback achieves 1.11-2.43 Ã—and 1.23-\n5.03Ã—higher throughput than baselines with Mellanox CX-6 100Gb\nand CX-3 50Gb RNICs in data search workload, respectively.\nMemory usage. The memory usage at each compute node for 20\nmillion KV pairs per shard is around 12.5MB per shard, around 5\nbits per key, with a load factor of 0.85 in DMPH.\nScalability of memory node threads. When the compute nodes\nwith enough threads exhaust the compute capability on the memory\nnode, Outback can achieve at least 18% performance advantage over\nother RPC-based baselines on read workload.\nLoad factors in Outback. The load factor value in DMPH causes\na trivial impact on data lookup throughput with the same compute\ncomplexity. We recommend 0.8-0.9 to achieve the balance between\nmemory usage and low frequent resizing, as the low load factor\nsupports more incremental data insertion into the hash table.\n6 RELATED WORK\nRDMA-based storage systems. Existing RDMA-based storage\ncan be classified into one-sided RDMA, RPC, or hybrid methods.\nOne-sided RDMA-based approaches [ 9,11,14,28,34,66] can by-\npass the memory nodeâ€™s CPU, managing data by RDMA_READ,RDMA_WRITE and other atomic verbs. Two-sided RDMA-based\nschemes [ 21,22,24,31,54] need only one round trip but suffer from\nthe remote CPU bottleneck, posing challenges in saturating RNIC\nbandwidth due to the computation burden for the callback data ser-\nvice. The index data structures of existing two-sided RDMA, such\nas hash table [ 29,36], learned index [ 27,28] and Blink Tree [ 65],\nput the memory nodeâ€™s CPU in charge of nontrivial computation\ntasks. The hybrid methods [ 17,20,37,52] combine two of the above\napproaches to boost the throughput.\nIn addition to examining design primitives and communication\nprotocols within RDMA-based systems. Cowbird [ 9] frees the CPU\nburden in compute nodes by offloading RDMA posting tasks on\nin-network computation devices (e.g. programmable switch [ 19]),\nso that the compute node can focus on computation duties. Smart-\nNIC [ 7,41,45,51] can also be put in the network interface and\nworks as an extra compute core on the critical data path, and it\nenables compute nodes to access data without network or RPC\noverhead. Note that the computation resource required in memory\nnodes of Outback can also be offloaded to SmartNIC or SmartSSD,\nwhose SOCs are closer to data.\nMinimal perfect hashing for networked systems. Perfect\nhashing offers a rapid method for data indexing, effectively prevent-\ning hash collisions. Moreover, DMPH enhances memory efficiency\nby eliminating the need to store keys and mapping ğ‘elements into\n(1+ğœ–)ğ‘space within the table. Besides the Ludo hashing shown\nin Â§ 2, Setsep [ 64] leverages a novel two-level hashing scheme that\ndistributes billions of keys across cluster servers with a memory\ncost of 0.5+1.5ğ‘™bits/key. BuRR [ 13] is another MPH scheme that\ninvolves manipulating a matrix for each key, and the multiplication\nvalues of keys determine various ranks within the bucket.\n7 CONCLUSION\nThis paper introduces Outback, an RDMA RPC-based index for key-\nvalue stores on disaggregated memory, designed to achieve high\nthroughput with lower CPU utilization. The key innovation of Out-\nback is the division of the data index into two distinct components:\na compute-intensive component cached on compute nodes and a\nmemory-intensive component residing on memory nodes. The per-\nformance improvements stem from the memory nodeâ€™s ability to\naccess underlying data with minimal computational overhead with\nperfect hashing. We also design protocols for Outback that support\ndata operations and index resizing using extendible hashing, en-\nsuring both the correctness of operations and system consistency\nduring updates. We conduct extensive experiments to evaluate the\nperformance of Outback. The results show that Outback achieves\nhigher throughput and requires smaller memory space on compute\nnodes, compared to the state-of-the-art baselines under most types\nof workload, especially for Get-heavy workload.\nACKNOWLEDGMENTS\nWe thank our three anonymous reviewers for their insightful sug-\ngestions and comments. This research was supported by the IAB\nmembers of the Center for Research in Systems and Storage (CRSS),\nand the National Science Foundation (NSF) under grants CNS-\n1841545, CCF-1942754, CNS-2322919, CNS-2420632, CNS-2426031,\nand CNS-2426940. The views expressed are those of the authors\nand do not necessarily reflect those of the funding agencies.\n\nREFERENCES\n[1] [n.d.]. AMD Alveo â„¢Adaptable Accelerator Cards. https://www.amd.com/en/\nproducts/accelerators/alveo.html\n[2] [n.d.]. Compute Express Link: The Breakthrough CPU-to-Device Interconnect.\nhttps://www.computeexpresslink.org/about-cxl\n[3] [n.d.]. https://github.com/basicthinker/YCSB-C.\n[4] [n.d.]. https://github.com/SJTU-IPADS/drtm.\n[5] [n.d.]. Memory Optimized Amazon EC2 Instance. https://aws.amazon.com/ec2/\ninstance-types/?nc1=h_ls\n[6][n.d.]. NVIDIA BlueField Networking Platform. https://nvidia.com/en-us/\nnetworking/products/data-processing-unit\n[7] Matthew Burke, Sowmya Dharanipragada, Shannon Joyner, Adriana Szekeres,\nJacob Nelson, Irene Zhang, and Dan RK Ports. 2021. PRISM: Rethinking the\nRDMA interface for distributed systems. In Proceedings of the ACM SIGOPS 28th\nSymposium on Operating Systems Principles . 228â€“242.\n[8] Bernard Chazelle, Joe Kilian, Ronitt Rubinfeld, and Ayellet Tal. 2004. The bloomier\nfilter: an efficient data structure for static support lookup tables. In Proceedings\nof the fifteenth annual ACM-SIAM symposium on Discrete algorithms . Citeseer,\n30â€“39.\n[9]Xinyi Chen, Liangcheng Yu, Vincent Liu, and Qizhen Zhang. 2023. Cowbird:\nFreeing CPUs to Compute by Offloading the Disaggregation of Memory. In\nProceedings of the ACM SIGCOMM 2023 Conference . 1060â€“1073.\n[10] Youmin Chen, Youyou Lu, and Jiwu Shu. 2019. Scalable RDMA RPC on reli-\nable connection with efficient resource sharing. In Proceedings of the Fourteenth\nEuroSys Conference 2019 . 1â€“14.\n[11] Yanzhe Chen, Xingda Wei, Jiaxin Shi, Rong Chen, and Haibo Chen. 2016. Fast\nand general distributed transactions using RDMA and HTM. In Proceedings of\nthe Eleventh European Conference on Computer Systems . 1â€“17.\n[12] Brian F Cooper, Adam Silberstein, Erwin Tam, Raghu Ramakrishnan, and Russell\nSears. 2010. Benchmarking cloud serving systems with YCSB. In Proceedings of\nthe 1st ACM symposium on Cloud computing . 143â€“154.\n[13] Peter C Dillinger, Lorenz HÃ¼bschle-Schneider, Peter Sanders, and Stefan Walzer.\n2021. Fast succinct retrieval and approximate membership using ribbon. arXiv\npreprint arXiv:2109.01892 (2021).\n[14] Aleksandar DragojeviÄ‡, Dushyanth Narayanan, Miguel Castro, and Orion Hod-\nson. 2014. FaRM: Fast remote memory. In 11th USENIX Symposium on Networked\nSystems Design and Implementation (NSDI 14) . 401â€“414.\n[15] Dmitry Duplyakin, Robert Ricci, Aleksander Maricq, Gary Wong, Jonathon\nDuerig, Eric Eide, Leigh Stoller, Mike Hibler, David Johnson, Kirk Webb, Aditya\nAkella, Kuangching Wang, Glenn Ricart, Larry Landweber, Chip Elliott, Michael\nZink, Emmanuel Cecchet, Snigdhaswin Kar, and Prabodh Mishra. 2019. The\nDesign and Operation of CloudLab. In Proceedings of the USENIX Annual Technical\nConference (ATC) . 1â€“14. https://www.flux.utah.edu/paper/duplyakin-atc19\n[16] Edward A Fox, Lenwood S Heath, Qi Fan Chen, and Amjad M Daoud. 1992.\nPractical minimal perfect hash functions for large databases. Commun. ACM 35,\n1 (1992), 105â€“121.\n[17] Shukai Han, Mi Zhang, Dejun Jiang, and Jin Xiong. 2023. Exploiting Hybrid\nIndex Scheme for RDMA-based Key-Value Stores. In Proceedings of the 16th ACM\nInternational Conference on Systems and Storage . 49â€“59.\n[18] Junhyeok Jang, Hanjin Choi, Hanyeoreum Bae, Seungjun Lee, Miryeong Kwon,\nand Myoungsoo Jung. 2023. {CXL-ANNS}:{Software-Hardware }collaborative\nmemory disaggregation and computation for {Billion-Scale}approximate near-\nest neighbor search. In 2023 USENIX Annual Technical Conference (USENIX ATC\n23). 585â€“600.\n[19] Xin Jin, Xiaozhou Li, Haoyu Zhang, Robert SoulÃ©, Jeongkeun Lee, Nate Foster,\nChanghoon Kim, and Ion Stoica. 2017. Netcache: Balancing key-value stores\nwith fast in-network caching. In Proceedings of the 26th Symposium on Operating\nSystems Principles . 121â€“136.\n[20] Anuj Kalia, Michael Kaminsky, and David G Andersen. 2014. Using RDMA\nefficiently for key-value services. In Proceedings of the 2014 ACM Conference on\nSIGCOMM . 295â€“306.\n[21] Anuj Kalia, Michael Kaminsky, and David G Andersen. 2016. Design guide-\nlines for high performance RDMA systems. In 2016 USENIX Annual Technical\nConference (USENIX ATC 16) . 437â€“450.\n[22] Anuj Kalia, Michael Kaminsky, and David G Andersen. 2016. FaSST: Fast, Scalable\nand Simple Distributed Transactions with Two-Sided (RDMA) Datagram RPCs.\nIn12th USENIX Symposium on Operating Systems Design and Implementation\n(OSDI 16) . 185â€“201.\n[23] David Karger, Eric Lehman, Tom Leighton, Rina Panigrahy, Matthew Levine, and\nDaniel Lewin. 1997. Consistent hashing and random trees: distributed caching\nprotocols for relieving hot spots on the World Wide Web. In Proceedings of the\ntwenty-ninth annual ACM symposium on Theory of computing .\n[24] Ana Klimovic, Heiner Litz, and Christos Kozyrakis. 2017. Reflex: Remote flash =\nlocal flash. ACM SIGARCH Computer Architecture News 45, 1 (2017), 345â€“359.\n[25] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe case for learned index structures. In Proceedings of the 2018 international\nconference on management of data . 489â€“504.[26] Sekwon Lee, Soujanya Ponnapalli, Sharad Singhal, Marcos K Aguilera, Kimberly\nKeeton, and Vijay Chidambaram. 2022. DINOMO: an elastic, scalable, high-\nperformance key-value store for disaggregated persistent memory. Proceedings\nof the VLDB Endowment 15, 13 (2022), 4023â€“4037.\n[27] Pengfei Li, Yu Hua, Jingnan Jia, and Pengfei Zuo. 2021. FINEdex: a fine-grained\nlearned index scheme for scalable and concurrent memory systems. Proceedings\nof the VLDB Endowment 15, 2 (2021), 321â€“334.\n[28] Pengfei Li, Yu Hua, Pengfei Zuo, Zhangyu Chen, and Jiajie Sheng. 2023. ROLEX:\nA Scalable RDMA-oriented Learned Key-Value Store for Disaggregated Memory\nSystems. In 21st USENIX Conference on File and Storage Technologies (FAST 23) .\n99â€“114.\n[29] Hyeontaek Lim, Dongsu Han, David G Andersen, and Michael Kaminsky. 2014.\nMICA: A Holistic Approach to Fast In-Memory Key-Value Storage. In 11th\nUSENIX Symposium on Networked Systems Design and Implementation (NSDI\n14). 429â€“444.\n[30] Jiaxin Lin, Adney Cardoza, Tarannum Khan, Yeonju Ro, Brent E Stephens, Hassan\nWassel, and Aditya Akella. 2023. RingLeader: Efficiently Offloading Intra-Server\nOrchestration to NICs. In 20th USENIX Symposium on Networked Systems Design\nand Implementation (NSDI 23) . 1293â€“1308.\n[31] Yi Liu, Shouqian Shi, Minghao Xie, Heiner Litz, and Chen Qian. 2023. Smash:\nFlexible, fast, and resource-efficient placement and lookup of distributed storage.\nProceedings of the ACM on Measurement and Analysis of Computing Systems 7, 2\n(2023), 1â€“22.\n[32] Baotong Lu, Xiangpeng Hao, Tianzheng Wang, and Eric Lo. 2020. Dash: Scalable\nhashing on persistent memory. arXiv preprint arXiv:2003.07302 (2020).\n[33] Baotong Lu, Kaisong Huang, Chieh-Jan Mike Liang, Tianzheng Wang, and Eric\nLo. 2024. DEX: Scalable Range Indexing on Disaggregated Memory [Extended\nVersion]. arXiv:2405.14502 [cs.DB]\n[34] Xuchuan Luo, Pengfei Zuo, Jiacheng Shen, Jiazhen Gu, Xin Wang, Michael R Lyu,\nand Yangfan Zhou. 2023. SMART: A High-Performance Adaptive Radix Tree\nfor Disaggregated Memory. In 17th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI 23) . USENIX Association.\n[35] Ryan Marcus, Andreas Kipf, and Alex van Renen. 2019. Searching on Sorted\nData. https://doi.org/10.7910/DVN/JGVF9A\n[36] Christopher Mitchell, Yifeng Geng, and Jinyang Li. 2013. Using One-Sided RDMA\nReads to Build a Fast, CPU-Efficient Key-Value Store. In 2013 USENIX Annual\nTechnical Conference (USENIX ATC 13) . 103â€“114.\n[37] Christopher Mitchell, Kate Montgomery, Lamont Nelson, Siddhartha Sen, and\nJinyang Li. 2016. Balancing CPU and Network in the Cell Distributed B-Tree\nStore. In 2016 USENIX Annual Technical Conference (USENIX ATC 16) . 451â€“464.\n[38] Rasmus Pagh and Flemming Friche Rodler. 2004. Cuckoo hashing. Journal of\nAlgorithms 51, 2 (2004), 122â€“144.\n[39] Xi Pang and Jianguo Wang. 2024. Understanding the performance implications\nof the design principles in storage-disaggregated databases. Proceedings of the\nACM on Management of Data 2, 3 (2024), 1â€“26.\n[40] Waleed Reda, Marco Canini, Dejan KostiÄ‡, and Simon Peter. 2022. {RDMA}is\nTuring complete, we just did not know it yet!. In 19th USENIX Symposium on\nNetworked Systems Design and Implementation (NSDI 22) . 71â€“85.\n[41] Henry N Schuh, Weihao Liang, Ming Liu, Jacob Nelson, and Arvind Krishna-\nmurthy. 2021. Xenic: SmartNIC-accelerated distributed transactions. In Pro-\nceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles .\n740â€“755.\n[42] Yizhou Shan, Will Lin, Zhiyuan Guo, and Yiying Zhang. 2022. Towards a fully\ndisaggregated and programmable data center. In Proceedings of the 13th ACM\nSIGOPS Asia-Pacific Workshop on Systems . 18â€“28.\n[43] Jiacheng Shen, Pengfei Zuo, Xuchuan Luo, Tianyi Yang, Yuxin Su, Yangfan Zhou,\nand Michael R Lyu. 2023. FUSEE: A Fully Memory-Disaggregated Key-Value\nStore. In 21st USENIX Conference on File and Storage Technologies (FAST 23) .\n81â€“98.\n[44] Shouqian Shi and Chen Qian. 2020. Ludo hashing: Compact, fast, and dynamic\nkey-value lookups for practical network systems. Proceedings of the ACM on\nMeasurement and Analysis of Computing Systems 4, 2 (2020), 1â€“32.\n[45] David Sidler, Zeke Wang, Monica Chiosa, Amit Kulkarni, and Gustavo Alonso.\n2020. StRoM: smart remote memory. In Proceedings of the Fifteenth European\nConference on Computer Systems . 1â€“16.\n[46] Shin-Yeh Tsai, Yizhou Shan, and Yiying Zhang. 2020. Disaggregating persistent\nmemory and controlling them remotely: An exploration of passive disaggregated\nKey-Value stores. In 2020 USENIX Annual Technical Conference (USENIX ATC 20) .\n33â€“48.\n[47] Jianguo Wang and Qizhen Zhang. 2023. Disaggregated Database Systems. In\nCompanion of the 2023 International Conference on Management of Data . 37â€“44.\n[48] Qing Wang, Youyou Lu, and Jiwu Shu. 2022. Sherman: A write-optimized dis-\ntributed b+ tree index on disaggregated memory. In Proceedings of the 2022\nInternational Conference on Management of Data . 1033â€“1048.\n[49] Ruihong Wang, Jianguo Wang, Stratos Idreos, M Tamer Ã–zsu, and Walid G Aref.\n2022. The case for distributed shared-memory databases with RDMA-enabled\nmemory disaggregation. arXiv preprint arXiv:2207.03027 (2022).\n[50] Xingda Wei, Rong Chen, and Haibo Chen. 2020. Fast RDMA-based Ordered\nKey-Value Store using Remote Learned Cache. In 14th USENIX Symposium on\n\nOperating Systems Design and Implementation (OSDI 20) . 117â€“135.\n[51] Xingda Wei, Rongxin Cheng, Yuhan Yang, Rong Chen, and Haibo Chen. 2023.\nCharacterizing Off-path SmartNIC for Accelerating Distributed Systems. In 17th\nUSENIX Symposium on Operating Systems Design and Implementation (OSDI 23) .\n987â€“1004.\n[52] Xingda Wei, Zhiyuan Dong, Rong Chen, and Haibo Chen. 2018. Deconstructing\nRDMA-enabled Distributed Transactions: Hybrid is Better!. In 13th USENIX\nSymposium on Operating Systems Design and Implementation (OSDI 18) . 233â€“251.\n[53] Chenyuan Wu, Mohammad Javad Amiri, Jared Asch, Heena Nagda, Qizhen\nZhang, and Boon Thau Loo. 2022. FlexChain: an elastic disaggregated blockchain.\nProceedings of the VLDB Endowment 16, 1 (2022), 23â€“36.\n[54] Minghao Xie, Chen Qian, and Heiner Litz. 2020. Reflex4arm: Supporting 100gbe\nflash storage disaggregation on arm soc. In OCP Future Technology Symposium .\n[55] Jie You, Jingfeng Wu, Xin Jin, and Mosharaf Chowdhury. 2021. Ship compute\nor ship data? why not both?. In 18th USENIX Symposium on Networked Systems\nDesign and Implementation (NSDI 21) . 633â€“651.\n[56] Ye Yu, Djamal Belazzougui, Chen Qian, and Qin Zhang. 2017. A concise for-\nwarding information base for scalable and fast name lookups. In 2017 IEEE 25th\nInternational Conference on Network Protocols (ICNP) . IEEE, 1â€“10.\n[57] Ming Zhang, Yu Hua, and Zhijun Yang. 2024. Motor: Enabling Multi-Versioning\nfor Distributed Transactions on Disaggregated Memory. In 18th USENIX Sym-\nposium on Operating Systems Design and Implementation (OSDI 24) . USENIX\nAssociation.\n[58] Ming Zhang, Yu Hua, Pengfei Zuo, and Lurong Liu. 2022. FORD: Fast One-sided\nRDMA-based Distributed Transactions for Disaggregated Persistent Memory. In\n20th USENIX Conference on File and Storage Technologies (FAST 22) . 51â€“68.\n[59] Penghao Zhang, Heng Pan, Zhenyu Li, Penglai Cui, Ru Jia, Peng He, Zhibin\nZhang, Gareth Tyson, and Gaogang Xie. 2021. NetSHa: In-network accelerationof LSH-based distributed search. IEEE Transactions on Parallel and Distributed\nSystems 33, 9 (2021), 2213â€“2229.\n[60] Qizhen Zhang, Philip A Bernstein, Daniel S Berger, and Badrish Chandramouli.\n2021. Redy: remote dynamic memory cache. arXiv preprint arXiv:2112.12946\n(2021).\n[61] Qizhen Zhang, Yifan Cai, Sebastian Angel, Ang Chen, Vincent Liu, and\nBoon Thau Loo. 2020. Rethinking data management systems for disaggregated\ndata centers. In Conference on Innovative Data Systems Research .\n[62] Qizhen Zhang, Yifan Cai, Xinyi Chen, Sebastian Angel, Ang Chen, Vincent Liu,\nand Boon Thau Loo. 2020. Understanding the effect of data center resource\ndisaggregation on production dbmss. Proceedings of the VLDB Endowment 13, 9\n(2020).\n[63] Changgang Zheng, Haoyue Tang, Mingyuan Zang, Xinpeng Hong, Aosong Feng,\nLeandros Tassiulas, and Noa Zilberman. 2023. DINC: Toward distributed in-\nnetwork computing. Proceedings of the ACM on Networking 1, CoNEXT3 (2023),\n1â€“25.\n[64] Dong Zhou, Bin Fan, Hyeontaek Lim, David G Andersen, Michael Kaminsky,\nMichael Mitzenmacher, Ren Wang, and Ajaypal Singh. 2015. Scaling up clustered\nnetwork appliances with ScaleBricks. In Proceedings of the 2015 ACM Conference\non Special Interest Group on Data Communication . 241â€“254.\n[65] Tobias Ziegler, Sumukha Tumkur Vani, Carsten Binnig, Rodrigo Fonseca, and\nTim Kraska. 2019. Designing distributed tree-based index structures for fast\nrdma-capable networks. In Proceedings of the 2019 International Conference on\nManagement of Data . 741â€“758.\n[66] Pengfei Zuo, Jiazhao Sun, Liu Yang, Shuangwu Zhang, and Yu Hua. 2021. One-\nsided RDMA-Conscious Extendible Hashing for Disaggregated Memory. In 2021\nUSENIX Annual Technical Conference (USENIX ATC 21) . 15â€“29.",
  "textLength": 83424
}