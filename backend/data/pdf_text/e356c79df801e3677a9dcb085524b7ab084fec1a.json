{
  "paperId": "e356c79df801e3677a9dcb085524b7ab084fec1a",
  "title": "RadixSpline: a single-pass learned index",
  "pdfPath": "e356c79df801e3677a9dcb085524b7ab084fec1a.pdf",
  "text": "RadixSpline: A Single-Pass Learned Index\nAndreas Kipf⋆Ryan Marcus⋆†Alexander van Renen Mihail Stoian\nAlfons Kemper Tim Kraska⋆Thomas Neumann\nTUM MIT CSAIL⋆Intel Labs†\n{renen, stoian, kemper, neumann}@in.tum.de {kipf, ryanmarcus, kraska}@mit.edu\nABSTRACT\nRecent research has shown that learned models can outper-\nform state-of-the-art index structures in size and lookup\nperformance. While this is a very promising result, existing\nlearned structures are often cumbersome to implement and\nare slow to build. In fact, most approaches that we are aware\nof require multiple training passes over the data.\nWe introduce RadixSpline (RS), a learned index that can\nbe built in a single pass over the data and is competitive\nwith state-of-the-art learned index models, like RMI, in size\nand lookup performance. We evaluate RS using the SOSD\nbenchmark and show that it achieves competitive results on\nall datasets, despite the fact that it only has two parameters.\n1 INTRODUCTION\nIn [13], Kraska et al. proposed learned index structures, a\nnew type of index for sorted data which use learned models\nto predict the position of a lookup key. These learned index\nstructures can be realized via supervised learning techniques,\nusing the cumulative distribution function (CDF) of the un-\nderlying data for training. More recently, the SOSD bench-\nmark [ 11] demonstrated that learned index structures (which\ncan be viewed as CDF approximators) can compete favorably\nwith state-of-the-art index structures [ 2,4,8,9,14,32] in\nterms of size and lookup performance.\nHowever, some learned index structures, such as RMIs [ 13],\ndo not support inserts and cannot be constructed in a single\npass over the data, which severely limits their applications.\nThe recent learned index proposals ALEX [ 3] and PGM [ 5]\nAndreas Kipf, Ryan Marcus, and Alexander van Renen contributed equally.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for profit or commercial advantage and that\ncopies bear this notice and the full citation on the first page. Copyrights\nfor components of this work owned by others than the author(s) must\nbe honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific\npermission and/or a fee. Request permissions from permissions@acm.org.\naiDM’20, June 14–19, 2020, Portland, OR, USA\n©2020 Copyright held by the owner/author(s). Publication rights licensed\nto ACM.\nACM ISBN 978-1-4503-8029-4/20/06. . . $15.00\nhttps://doi.org/10.1145/3401071.3401659\n01234567\nKeyIndex4718310     \n1011 1000 0100 11112 Lookup key:\nRadix table\nSpline point\nCDFPointerFigure 1: A radix spline index and example lookup\nprocess. The r(here, 3) most significant bits bof the\nlookup key are used as an index into the radix ta-\nble. Then, a binary search is performed on the spline\npoints between the bth pointer and the b+1th pointer.\nadd support for inserts. In this work, we argue that there are\napplications where indexes do not need to support individual\nupdates and where it is sufficient to be able to build them\nefficiently. The most prominent example are LSM-trees [ 23].\nIn an LSM-tree, data is stored in several files, each of which\nis sorted by a key column. Each file generally stores addi-\ntional metadata, such as a Bloom filter or an index. This\nmetadata can be used at query time to either exclude a file\nfrom a lookup (via the Bloom filters or lower/upper bound)\nor to quickly locate the relevant tuples in a file (via the index).\nThese files are organized into multiple levels, with higher\nlevels containing exponentially more files than lower lev-\nels. New data is inserted into files in lower levels, which\nare periodically merged with files in higher levels. Unfamil-\niar readers may wish to see [ 15] for an in-depth survey of\nLSM-trees, but for this work one only needs to note that\nthis merge process between two files is the perfect time to\nre-build a learned index. The merge produces data in sorted\norder, which can be passed through a single-pass training\nalgorithm before it is written back to disk. Since the merge\noperation is expensive on its own and is usually done asyn-\nchronously, training such a one-pass learned index could\nonly incur a negligible constant overhead. However, existing\nlearned indexes do not allow for an efficient build.arXiv:2004.14541v2  [cs.DB]  22 May 2020\n\naiDM’20, June 14–19, 2020, Portland, OR, USA Kipf et al.\nIn this work, we introduce RadixSpline (RS), a learned in-\ndex that can be built in a single pass over sorted data. Notably,\nthe proposals of FITing-Tree [ 6] and PGM [ 5] also support\nsingle-pass builds. However, for both of these indexes the\namount of work per element is logarithmic in the number\nof levels (similar to inserts in a BTree). With RS, we propose\nthe first single-pass learned index with a constant amount of\nwork per new element.\nBeing an ordered index, RS supports both equality and\nrange predicates (e.g., lower bound lookups). RS is built in\ntwo steps. First, a linear spline is fit to the CDF of the data\nthat guarantees a certain error bound. This results in a set\nof spline points which can be significantly smaller than the\nunderlying data. Second, we build a radix table (a flat radix\nstructure) that serves as an approximate index into the spline\npoints. Similar to the Node256 in the Adaptive Radix Tree\n(ART) [ 14], we extract a certain radix prefix (e.g., the first\n20bits, neglecting common prefix bits shared by all keys)\nand use those as an offset into the radix table. Both steps can\nbe performed in a single pass over the sorted data.\nRS is not only efficient to build, but also competitive\nwith state-of-the-art RMI models in size and lookup per-\nformance. Index size is an especially important factor for\nLSM-tree applications because indexes are kept in main mem-\nory (whereas each large sorted file is stored on disk). Fur-\nthermore, RS’s implementation only consists of roughly one\nhundred lines of C++ code and does not have any external\ndependencies. Finally, RS only takes two hyper parameters\n(spline error and radix table size). Both have an intuitive and\nreliable impact on size and lookup latency. As a result, tun-\ning RS is easier than tuning more complex learned indexes\nstructures with many hyperparameters [ 20]. One caveat is\nthat RS can be impacted by heavy skew, rendering the radix\ntable largely ineffective. In such cases, one could fall back\nto a tree-structured radix table or handle outliers separately.\nHowever, we are yet to encounter such extreme skew in\nreal-world data. In summary, we believe that RS is a practi-\ncal learned index structure with potentially high impact in\nwrite-once/read-many settings such as LSM-trees.\nMore broadly, this work follows recent trends in integrat-\ning machine learning components into systems [ 7], espe-\ncially database systems [10, 12, 16–19, 21, 24, 26–31].\n2 RADIXSPLINE\nA RadixSpline (RS) index is designed to map a lookup key\nto an index (the position of the key in the underlying data).\nLike an RMI [ 13], radix spline indexes require the underlying\ndata to be sorted on the lookup key in a flat array.\nAn RS index consists of two components: a set of spline\npoints and a radix table. The set of spline points is a subset of\nthe keys, selected so that spline interpolation for any lookup\nKeyIndexSpline point\nCDF\nInterpolation\nMax interpolation errorFigure 2: A single spline segment. We select enough\nspline points so that the maximum interpolation error\n(dashed line) stays within a specified bound.\nkey will result in a predicted lookup location within a preset\nerror bound. For example, if the preset error bound is 32,\nthen the location of any lookup key can be no more than 32\npositions away from the location predicted by the RS index.\nThe radix table helps to quickly locate the correct spline\npoints for a given lookup key. Intuitively, the radix table\nlimits the range of possible spline points to search over for\nevery possible b-length prefix of a lookup key.\nAt lookup time, the radix table is used to determine a\nrange of spline points to examine. These spline points are\nsearched until the two spline points surrounding the key\nare found. Then, linear interpolation is used to predict the\nlocation (index) of the lookup key in the underlying data.\nBecause the spline interpolation is error-bounded, only a\n(small) range of the underlying data needs to be searched.\nIn contrast to other learned indexes [ 3,13], RS can be\nbuilt in a single pass over the sorted data. While the use of\nsplines and a bottom-up approach has been explored before\nin FITing-Tree [ 6] and others [ 5,25], in this work, we com-\nbine the ideas from [ 6] with radix trees, making it highly\ncompetitive with top-down built indexes [13].\n2.1 Construction\nRS indexes, like PGM indexes [ 5] or FITing-Trees [ 6], are built\n“bottom-up”. First, we construct an error-bounded spline on\ntop of the the underlying data. Then, the selected spline\npoints themselves are indexed in a radix table.\nBuild Spline. As observed in [ 13], all index structures can be\nthought of as models that map lookup keys to positions. Let\nthe dataset to index Dbe an indexed set of tuples, with Di=\n(ki,pi)where Direpresents the ith datapoint, kirepresents\nthe key of the ith datapoint, and pirepresents the position\n(offset) of the ith datapoint. A radix spline index first builds a\nspline model S, such that S(ki)=pi±e, where eis a specified\nconstant. In other words, the spline model Salways predicts\nthe correct location of the data within a constant error of e.\nThis error-bounded model is realized via spline interpo-\nlation (we use GreedySplineCorridor [22]). The parame-\nters of the model S,Knots(S), are a set of spline points, or\n\nRadixSpline: A Single-Pass Learned Index aiDM’20, June 14–19, 2020, Portland, OR, USA\nknots, which are a representative set of datapoints (see Fig-\nure 1). These data points are chosen such that, for any lookup\nkeyx, linearly interpolating between the two closest spline\npoints in Knots(S)will produce an estimate with error no\nlarger than e(cf. Figure 2). Formally, to evaluate S(x), letting\n(kleft,pleft)∈Knots(S)be the knot with the greatest key such\nthatkleft≤xand letting(kright,pright)∈Knots(S)be the knot\nwith the smallest key such that kright >x, we compute\nS(x)=pleft+(x−kleft)×pright−pleft\nkright−kleft.\nFor more details on the error-bounded spline algorithm,\nwe refer the reader to [22].\nBuild Radix Table. Next, we build a radix table on top\nof the selected spline points to quickly find the two spline\npoints surrounding the lookup key. The radix table is a flat\nuint32_t array that maps fixed-length key prefixes (“radix\nbits”) to the first spline point with that prefix. The key pre-\nfixes are the offsets into the radix table while the spline points\nare represented as uint32_t values stored in the radix table\n(cf. pointers in Figure 1).\nThe radix table takes the number of radix bits ras a param-\neter. For example, for r=18we allocate an array with 218\nmany entries (1 MiB in size). A larger rgrows the size of the\ntable exponentially ( 2r) but may also increase its precision.\nThat is, we may need to search a more narrow range of spline\npoints to find the two spline points surrounding the lookup\nkey. In Section 3, we show the impact of this parameter on\nsize and lookup performance.\nThe build process itself is very straightforward and ex-\ntremely fast: we first allocate an array of the appropriate size\n(2rmany entries), then we go through all spline points and\nwhenever we encounter a new r-bit prefix b, we insert the\noffset of the spline point (a uint32_t value) into the slot at\noffset bin the radix table. Since the spline points are ordered,\nthe radix table is filled in consecutive order from left to right.\nAs an optimization, we eliminate common prefix bits shared\nby all keys when building the radix table.\nSingle Pass. Building the CDF, the spline, and the radix\ntable can all be performed on-the-fly, in a single pass over\nthe sorted datapoints. When encountering a new CDF point\n(i.e., when the key changes), we pass that point to the spline\nconstruction algorithm [ 22]. Filling the pre-allocated radix\ntable within the same pass is also straightforward: whenever\nwe encounter a new r-bit prefix in a selected spline point,\nwe make a new entry to the table.\nLookups. Using the example in Figure 1, the lookup logic is\nas follows: We first extract an r-bit prefix bof the lookup key\n(101 in this case). Then, we use the extracted bits bto make an\noffset access into the radix table retrieving the two pointersstored at positions bandb+1(here, positions 5 and 6). These\npointers (marked in orange) define a narrowed search range\non the spline points. Next, we search this range for the two\nspline points surrounding the lookup key using binary search.\nSubsequently, we perform a linear interpolation between\nthese two spline points to obtain an estimated position pof\nthe key. Finally, we perform a binary search within the error\nbounds ( p±e) to find the first occurrence of the key.\n3 EVALUATION\nWe evaluate RadixSpline (RS) using the SOSD benchmark [ 11]\non a c5.4xlarge AWS machine. We use six 64-bit datasets,\neach of them containing 200 M key/value pairs (3.2 GiB) in\nsize: amzn (book popularity data), face (Facebook user IDs),\nlogn (synthetic lognormal distributed data), osmc (compos-\nite cell IDs from Open Street Map) and wiki (timestamps of\nWikipedia edits). For details on these datasets, see [11].\nLike [ 11], we build indexes on top of sorted arrays. An\nindex takes in a key and produces a search range in the\nunderlying data. This range must contain the lookup key if\nthe lookup key exists, and must otherwise contain the first\nkey not larger than the lookup key (lower bound search).\nThen, binary search is used to locate the exact key within the\nsearch range. Indexes are evaluated based on their end-to-\nend performance: the time to produce a search range plus the\ntime to execute the binary search. We perform 10M lookups\n(1 thread) on a given dataset and report the average lookup\nlatency. Lookup keys are uniformly chosen from the keys.\nWe compare RS against three traditional, non-learned ap-\nproaches: ART [ 14], STX B+-tree (BTree) [ 1], and binary\nsearch (BS). For ART and BTree, we use a stride of 32 (mean-\ning that every 32nd key is inserted into the index – this pro-\nvides better space and performance compared to indexing\neach key). We also compare against the public implementa-\ntion [ 20] of the recursive model index (RMI) [ 13], a learned\napproach that is built “top-down” (i.e., starting with a loose\nfit and then progressively learning finer-grained models) and\ninternally uses a range of models (e.g., linear, cubic, or even\nBTrees). Hash-based methods are excluded because hash-\nbased methods do not support lower bound searches. Since\nART does not support duplicate keys, it does not have results\nforwiki (the only dataset with duplicates).\nBuild Times. (Figure 3, left). Due to its single-pass build\nprocess, RS is almost as efficient to build as ART or BTree\nand is significantly faster than RMI, which performs multiple\ntraining passes over the sorted datapoints.\nLookup Latency. (Figure 3, middle). Binary search (BS)\ntakes around 850ns per lookup across all datasets. BTree\nimproves upon BS by using the cache more efficiently, and\nrequires only around 600ns. Like BS, it is largely independent\n\naiDM’20, June 14–19, 2020, Portland, OR, USA Kipf et al.\n0510\namzn face logn osmc wikiBuild [s]ARTBSBTreeRMIRS\n0250500750\namzn face logn osmc wikiLatency [ns]ARTBSBTreeRMIRS\n0200400600\namzn face logn osmc wikiSize [MiB]ARTBSBTreeRMIRS\nFigure 3: Build time (left), lookup latency (middle), and size (right) for lookup-optimized index configurations.\n10152025\n2 16 128 1024\nSpline errorNumber of radix bits\n2.02.53.03.5Build [s]\n10152025\n2 16 128 1024\nSpline errorNumber of radix bits\n400500600Latency [ns]\n10152025\n2 16 128 1024\nSpline errorNumber of radix bits\n0.30.60.9Size [GiB]\nFigure 4: Build time (left), lookup latency (middle), and index size (right) for different RadixSpline configurations.\n0500100015002000\nAvg read/write Avg read Avg writeLatency [ns]BTreeRS\nFigure 5: Average operator cost, LSM-tree.\nof the data distribution. Both learned approaches, RMI and\nRS, are significantly faster than the traditional indexes but\nalso more affected by the data distribution. Note that we\nhave tuned both approaches for minimum lookup latency.\nIndex Size. (Figure 3, right). Except for BS and a few outliers,\nall indexes consume around 100 MiB which corresponds to\n6.6% of the uncompressed key size (200 M 64-bit keys). For\ntheface dataset, RS uncharacteristically requires more than\n600 MiB (39%) to achieve its best performance. Next, we\ninvestigate different RS configurations for this dataset.\nConfiguration Space. The best RS configuration (Figure 3)\nforface uses a lot of memory ( ≈650MiB). However, we can\neasily trade lookup performance for memory as shown in\nFigure 4: Both build time (left) and size (right) mostly depend\non the spline error. Starting with an error of 16, RS can be\nbuilt within 2s for this dataset and requires less than 200 MiB.\nFor example, with a spline error of 16 (instead of 2) and 20\ninstead of 25 radix bits, RS trades performance (-11.5%) for a\nsignificant space reduction (-99.9%).LSM Performance. To validate the applicability of RS to\nLSMs, we performed a preliminary experiment where we\nsubstitute the BTree index with a RadixSpline in RocksDB.\nWe use the osmc dataset and executed 400 M operations,\n50% reads and 50% writes (cf. Figure 5). When using RS, the\naverage write time increased by ≈4%, but the average read\ntime decreased by over 20%. The total execution time fell\nto 521 seconds from 712 seconds with a BTree. In addition,\nthe RS variant used ≈45%less memory, potentially creating\nspace for larger Bloom filters or increased caching. While\nobviously preliminary, this experiment indicates potential\nbenefits of RS in LSMs.\n4 CONCLUSIONS\nWe have described a new learned index, called RadixSpline,\nthat can be built in a single pass over sorted data. Notably, RS\nonly takes two hyper parameters and thus is rather easy to\ntune to a given dataset and memory budget. Our experiments\nwith real-world data have shown that RS is competitive with\na state-of-the-art learned index in size and lookup perfor-\nmance while being as efficient to build as traditional indexes.\nWhile our radix table has a constant size, it may become less\nuseful as dataset size grows, or under large outliers.\nIn future work, we also plan to investigate how RS can be\nautomatically tuned with minimal user-interaction, balanc-\ning memory footprint and performance. Such an auto-tuning\ncould be informed by metrics extracted from the data. RS\ncurrently does not take advantage of any multi-threading,\nanother potential direction for performance improvements.\n\nRadixSpline: A Single-Pass Learned Index aiDM’20, June 14–19, 2020, Portland, OR, USA\nAcknowledgments. This research is supported by Google,\nIntel, and Microsoft as part of the MIT Data Systems and\nAI Lab (DSAIL) at MIT, NSF IIS 1900933, DARPA Award 16-\n43-D3M-FP040, and the MIT Air Force Artificial Intelligence\nInnovation Accelerator (AIIA).\nREFERENCES\n[1] STX B+ Tree, https://panthema.net/2007/stx-btree/.\n[2]R. Binna, E. Zangerle, M. Pichl, G. Specht, and V. Leis. HOT: A height\noptimized trie index for main-memory database systems. In Proceed-\nings of the 2018 International Conference on Management of Data , SIG-\nMOD ’18, pages 521–534, New York, NY, USA, 2018. Association for\nComputing Machinery.\n[3]J. Ding, U. F. Minhas, H. Zhang, Y. Li, C. Wang, B. Chandramouli,\nJ. Gehrke, D. Kossmann, and D. Lomet. ALEX: An Updatable Adaptive\nLearned Index. arXiv:1905.08898 [cs] , May 2019.\n[4]P. Fent, M. Jungmair, A. Kipf, and T. Neumann. START âĂŤ Self-Tuning\nAdaptive Radix Tree. In 2020 IEEE 36th International Conference on\nData Engineering Workshops (ICDEW) , pages 147–153, 2020.\n[5]P. Ferragina and G. Vinciguerra. The PGM-index: A fully-dynamic\ncompressed learned index with provable worst-case bounds. Proceed-\nings of the VLDB Endowment , 13(8):1162–1175, Apr. 2020.\n[6]A. Galakatos, M. Markovitch, C. Binnig, R. Fonseca, and T. Kraska.\nFITing-Tree: A Data-aware Index Structure. In Proceedings of the 2019\nInternational Conference on Management of Data , SIGMOD ’19, pages\n1189–1206, New York, NY, USA, 2019. ACM.\n[7]J. Gottschlich, A. Solar-Lezama, N. Tatbul, M. Carbin, M. Rinard,\nR. Barzilay, S. Amarasinghe, J. B. Tenenbaum, and T. Mattson. The\nthree pillars of machine programming. In Proceedings of the 2nd ACM\nSIGPLAN International Workshop on Machine Learning and Program-\nming Languages , MAPL 2018, pages 69–80, Philadelphia, PA, USA, June\n2018. Association for Computing Machinery.\n[8]G. Graefe. B-tree indexes, interpolation search, and skew. In Proceed-\nings of the 2nd International Workshop on Data Management on New\nHardware , DaMoN ’06, Chicago, Illinois, June 2006. Association for\nComputing Machinery.\n[9]C. Kim, J. Chhugani, N. Satish, E. Sedlar, A. D. Nguyen, T. Kaldewey,\nV. W. Lee, S. A. Brandt, and P. Dubey. FAST: Fast architecture sensitive\ntree search on modern CPUs and GPUs. In Proceedings of the 2010\nInternational Conference on Management of Data , SIGMOD ’10, 2010.\n[10] A. Kipf, T. Kipf, B. Radke, V. Leis, P. Boncz, and A. Kemper. Learned\nCardinalities: Estimating Correlated Joins with Deep Learning. In 9th\nBiennial Conference on Innovative Data Systems Research , CIDR ’19,\n2019.\n[11] A. Kipf, R. Marcus, A. van Renen, M. Stoian, A. Kemper, T. Kraska,\nand T. Neumann. SOSD: A Benchmark for Learned Indexes. In ML for\nSystems at NeurIPS , MLForSystems @ NeurIPS ’19, Dec. 2019.\n[12] A. Kipf, D. Vorona, J. Müller, T. Kipf, B. Radke, V. Leis, P. Boncz, T. Neu-\nmann, and A. Kemper. Estimating Cardinalities with Deep Sketches.\nInProceedings of the 2019 International Conference on Management of\nData , SIGMOD ’19, pages 1937–1940, Amsterdam, Netherlands, June\n2019. Association for Computing Machinery.\n[13] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The Case\nfor Learned Index Structures. In Proceedings of the 2018 International\nConference on Management of Data , SIGMOD ’18, pages 489–504, New\nYork, NY, USA, 2018. ACM.\n[14] V. Leis, A. Kemper, and T. Neumann. The adaptive radix tree: ARTful\nindexing for main-memory databases. In Proceedings of the 2013 IEEE\nInternational Conference on Data Engineering , ICDE ’13, pages 38–49,USA, 2013. IEEE Computer Society.\n[15] C. Luo and M. J. Carey. LSM-based storage techniques: A survey.\nPVLDB , 29(1):393–418, Jan. 2020.\n[16] R. Marcus, P. Negi, H. Mao, N. Tatbul, M. Alizadeh, and T. Kraska. Bao:\nLearning to Steer Query Optimizers. arXiv:2004.03814 [cs] , Apr. 2020.\n[17] R. Marcus, P. Negi, H. Mao, C. Zhang, M. Alizadeh, T. Kraska, O. Pa-\npaemmanouil, and N. Tatbul. Neo: A Learned Query Optimizer. PVLDB ,\n12(11):1705–1718, 2019.\n[18] R. Marcus and O. Papaemmanouil. Deep Reinforcement Learn-\ning for Join Order Enumeration. In First International Workshop\non Exploiting Artificial Intelligence Techniques for Data Management ,\naiDM@SIGMOD ’18, Houston, TX, 2018.\n[19] R. Marcus and O. Papaemmanouil. Towards a Hands-Free Query\nOptimizer through Deep Learning. In 9th Biennial Conference on\nInnovative Data Systems Research , CIDR ’19, 2019.\n[20] R. Marcus, E. Zhang, and T. Kraska. CDFShop: Exploring and Op-\ntimizing Learned Index Structures. In Proceedings of the 2020 ACM\nSIGMOD International Conference on Management of Data , SIGMOD\n’20, Portland, OR, June 2020.\n[21] P. Negi, R. Marcus, H. Mao, N. Tatbul, T. Kraska, and M. Alizadeh. Cost-\nGuided Cardinality Estimation: Focus Where it Matters. In Workshop\non Self-Managing Databases , SMDB @ ICDE ’20, 2020.\n[22] T. Neumann and S. Michel. Smooth interpolating histograms with\nerror guarantees. In Sharing Data, Information and Knowledge, 25th\nBritish National Conference on Databases , BNCOD ’08, pages 126–138,\n2008.\n[23] P. O’Neil, E. Cheng, D. Gawlick, and E. O’Neil. The log-structured\nmerge-tree (LSM-tree). Acta Informatica , 33(4):351–385, June 1996.\n[24] J. Ortiz, M. Balazinska, J. Gehrke, and S. S. Keerthi. Learning State\nRepresentations for Query Optimization with Deep Reinforcement\nLearning. In 2nd Workshop on Data Managmeent for End-to-End Ma-\nchine Learning , DEEM ’18, 2018.\n[25] N. Setiawan, B. Rubinstein, and R. Borovica-Gajic. Function Interpola-\ntion for Learned Index Structures. In Database Theory and Applications ,\nDTA ’20, 2020.\n[26] Shrainik Jain, Jiaqi Yan, Thiery Cruanes, and Bill Howe. Database-\nAgnostic Workload Management. In 9th Biennial Conference on Inno-\nvative Data Systems Research , CIDR ’19, 2019.\n[27] J. Sun and G. Li. An end-to-end learning-based cost estimator. Pro-\nceedings of the VLDB Endowment , 13(3):307–319, Nov. 2019.\n[28] I. Trummer, S. Moseley, D. Maram, S. Jo, and J. Antonakakis. Skin-\nnerDB: Regret-bounded Query Evaluation via Reinforcement Learning.\nPVLDB , 11(12):2074–2077, 2018.\n[29] D. Van Aken, A. Pavlo, G. J. Gordon, and B. Zhang. Automatic Database\nManagement System Tuning Through Large-scale Machine Learning.\nInProceedings of the 2017 ACM International Conference on Management\nof Data , SIGMOD ’17, pages 1009–1024, New York, NY, USA, 2017.\nACM.\n[30] L. Woltmann, C. Hartmann, M. Thiele, D. Habich, and W. Lehner.\nCardinality estimation with local deep learning models. In Proceedings\nof the Second International Workshop on Exploiting Artificial Intelligence\nTechniques for Data Management , aiDM ’19, pages 1–8, Amsterdam,\nNetherlands, July 2019. Association for Computing Machinery.\n[31] Z. Yang, E. Liang, A. Kamsetty, C. Wu, Y. Duan, X. Chen, P. Abbeel, J. M.\nHellerstein, S. Krishnan, and I. Stoica. Deep unsupervised cardinality\nestimation. Proceedings of the VLDB Endowment , 13(3):279–292, Nov.\n2019.\n[32] H. Zhang, H. Lim, V. Leis, D. G. Andersen, M. Kaminsky, K. Keeton,\nand A. Pavlo. SuRF: Practical Range Query Filtering with Fast Succinct\nTries. In Proceedings of the 2018 International Conference on Manage-\nment of Data , SIGMOD ’18, pages 323–336, Houston, TX, USA, May\n2018. Association for Computing Machinery.",
  "textLength": 26309
}