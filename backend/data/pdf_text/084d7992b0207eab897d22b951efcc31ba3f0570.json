{
  "paperId": "084d7992b0207eab897d22b951efcc31ba3f0570",
  "title": "Unsupervised Space Partitioning for Nearest Neighbor Search",
  "pdfPath": "084d7992b0207eab897d22b951efcc31ba3f0570.pdf",
  "text": "Unsupervised Space Partitioning for Nearest Neighbor Search\nAbrar Fahim\nBangladesh University of\nEngineering and Technology\nDhaka, Bangladesh\n1605075@ugrad.cse.buet.ac.bdMohammed Eunus Ali\nBangladesh University of\nEngineering and Technology\nDhaka, Bangladesh\neunus@cse.buet.ac.bdMuhammad Aamir Cheema\nFaculty of Information Technology,\nMonash University\nAustralia\naamir.cheema@monash.edu\nABSTRACT\nApproximate Nearest Neighbor Search (ANNS) in high dimen-\nsional spaces is crucial for many real-life applications (e.g., e-\ncommerce, web, multimedia, etc.) dealing with an abundance of\ndata. This paper proposes an end-to-end learning framework that\ncouples the partitioning (one critical step of ANNS) and learning-\nto-search steps using a custom loss function. A key advantage of\nour proposed solution is that it does not require any expensive\npre-processing of the dataset, which is one of the critical limi-\ntations of the state-of-the-art approach. We achieve the above\nedge by formulating a multi-objective custom loss function that\ndoes not need ground truth labels to quantify the quality of a\ngiven data-space partition, making it entirely unsupervised. We\nalso propose an ensembling technique by adding varying input\nweights to the loss function to train an ensemble of models to\nenhance the search quality. On several standard benchmarks\nfor ANNS, we show that our method beats the state-of-the-art\nspace partitioning method and the ubiquitous K-means clustering\nmethod while using fewer parameters and shorter offline training\ntimes. We also show that incorporating our space-partitioning\nstrategy into state-of-the-art ANNS techniques such as ScaNN\ncan improve their performance significantly. Finally, we present\nour unsupervised partitioning approach as a promising alterna-\ntive to many widely used clustering methods, such as K-means\nclustering and DBSCAN.\n1 INTRODUCTION\nğ¾-Nearest Neighbor Search ( ğ‘˜-NNS) that finds the ğ‘˜closest (or\nmost similar) data points for a given query point in a high-\ndimensional space is a well-studied problem [ 4,42,45,46]. The\nvast amount of high-dimensional data that applications have to\ndeal with today and an ever-greater need to quickly search for\nrelevant content necessitate a scalable and efficient search solu-\ntion for many domains, including multimedia, e-commerce, and\nrecommendation systems. Exact solutions to the ğ‘˜-NNS problem,\nwhere we seek the exact ğ‘˜nearest neighbors, are challenging\nand computationally intractable due to the phenomenon of the\ncurse of dimensionality [19]. Thus, they are not practical for many\napplications. Recent research has shifted to Approximate Nearest\nNeighbors Search (ANNS) [ 4,5,33] to scale the NNS solution\nto larger datasets with more dimensions. ANNS aims to quickly\nfind as many of the true nearest neighbors of the query point\nas possible by slightly trading off the returned answerâ€™s accu-\nracy. This paper proposes an end-to-end unsupervised learning\nsolution using neural networks to solve the ANNS problem.\nÂ©2023 Copyright held by the owner/author(s). Published in Proceedings of the\n26th International Conference on Extending Database Technology (EDBT), 28th\nMarch-31st March, 2023, ISBN 978-3-89318-088-2 on OpenProceedings.org.\nDistribution of this paper is permitted under the terms of the Creative Commons\nlicense CC-by-nc-nd 4.0.The established way to search for the k-Nearest-Neighbors\n(k-NNs) is to first reduce the search space for finding the most rel-\nevant points using indexing methods (such as KD-trees [ 7], quan-\ntization using K-means [ 22], PCA trees [ 1,43], LSH [ 3,30] etc.),\nand then to speed up the search within those relevant points using\nsketching methods (e.g., ScaNN [ 16], ITQ [ 15], etc.). This paper\nfocuses on improving the indexing part to speed up ANNS. Most\nexisting indexing approaches rely on algorithmic constructions\nthat are either entirely independent or only weakly dependent\non the data distribution (e.g., KD-trees [ 7], LSH [ 3,30], random\ntrees [ 9,24]). These approaches cannot correctly curate the cre-\nated partitions to specific data distributions. Notably, K-means\nclustering, a simple and prominent approach for clustering used\nin the implementation of the state-of-the-art ANNS technique\nScaNN [ 16], can only form convex (mostly spherical) clusters of\nthe dataset. These simple cluster shapes may not be sophisticated\nenough to represent more complex data distributions.\nRecently, there has been an increased interest in machine-\nlearning-based solutions (particularly supervised learning) for in-\ndex creation on the data to facilitate efficient search. Notably, [ 23,\n26] argue the case for learning index structures and show the\nbenefits and potential of replacing core components of database\nsystems with learned models. A recent approach, Neural LSH [11],\nuses neural nets and graph partitioning to create a space parti-\ntioning index, which divides the ambient space of the dataset into\nsmaller parts. Neural LSH outperforms previous data partitioning\nbaselines. Neural LSH first creates a ğ‘˜-NN graph from the dataset\nand then partitions the graph to divide the dataset into several\nbins using a combinatorial graph-partitioning algorithm [ 40].\nUsing the resulting graph partition, it trains a neural network\nto learn to classify new query points into specific bins of the\npartition. By assigning query points to specific bins, Neural LSH\nrestricts the further search to the data points within the queryâ€™s\nassigned bins to find the nearest neighbors. This approach has\nseveral shortcomings: (i) Ground truth labels needed to train the\nmodel are generated in a separate pre-processing step, (ii) the\ngraph-partitioning algorithm used to create the ground truth\nlabels takes hours on million-sized datasets, and most impor-\ntantly, (iii) the neural network is only used to learn to classify\nquery points into bins, with the partitioning step not forming a\npart of the learning pipeline. As a result, Neural LSH does not\ncapitalize on the power of function approximation in creating\nspace-partitioning indexes.\nTo address the limitations of traditional (e.g., LSH, K-means\nclustering, etc.) and learning-based (e.g., Neural LSH) partitioning\nsolutions, we propose an end-to-end learning solution for scalable\nand efficient ANNS. The key intuition of our approach is that we\ncan create superior partitions of the dataset by having the neural\nnetwork itself learns the partition in an unsupervised manner.\nWe do this by devising a customized cost function, enabling\nthe neural network to learn the partition without generating\nprior training labels. We also propose an ensemble approach that\nallows us to merge multiple complementary partitions to improvearXiv:2206.08091v2  [cs.LG]  19 Oct 2022\n\nindexing performance. Even though we primarily design our\napproach to solve the ANNS problem, without loss of generality,\nour unsupervised partitioning approach is a promising alternative\nto many widely used clustering methods like K-means clustering,\nDBSCAN [12], and spectral clustering [35].\nWe conduct extensive experiments with two standard Nearest\nNeighbor Search (NNS) benchmark datasets [ 5], which show that\nour proposed approach yields 5âˆ’10%performance improvement\nover the current state-of-the-art models. Moreover, we show that\nby incorporating our unsupervised space partitioning strategy,\nwe can improve the performance of the current best-performing\nANNS method, namely ScaNN, by approximately 40%.\nIn summary, our contributions in this work are as follows.\nâ€¢We introduce an end-to-end learning framework for learn-\ning partitions of the dataset without any expensive pre-\nprocessing steps.\nâ€¢We couple the partitioning and learning stages into a sin-\ngle step to make both the components aware of each other,\nincreasing the overall frameworkâ€™s training efficiency.\nâ€¢We introduce a custom loss function that can score output\npartitions and is differentiable. This loss function is model-\nagnostic and thus can be applied to any machine learning\narchitecture (including neural networks) to learn a richer\nclass of division boundaries.\nIn our experiments (Section 5), we show that our loss\nfunction makes any model learn better partitions than\nthose created by the baseline methods in most real-world\nsettings.\nâ€¢We propose an ensembling technique by adding varying\ninput weights to the loss function to train an ensemble\nof models to create multiple high-quality complementary\npartitions of the same dataset, which enhances indexing\nperformance.\nWe organized the rest of this paper as follows: We first discuss\nsome related work in the field of similarity search in Section 2. We\nthen formally define the approximate ğ‘˜-nearest neighbor search\nproblem in Section 3. Then, in Section 4, we discuss our learning-\nbased approach for space partitioning and Nearest Neighbor\nSearch (NNS) in detail. We present our experiments by comparing\nthe performance of our method with other space-partitioning\nbaselines in Section 5. Finally, we close with a summary of our\ncontributions in Section 6.\n2 RELATED WORK\nThe two major paradigms to solve the ANNS (or NNS) problem\nareindexing andsketching .\nIndexing methods generally construct a data structure that,\ngiven a query point ğ‘, returns a subset of the dataset called a\ncandidate set that includes the nearest neighbors of the ğ‘. On\nthe other hand, sketching methods compress the data points\nto compute approximate distances quickly [ 29,39,45,46]. The\ntwo paradigms are often combined in real-world applications to\nmaximize the overall performance [16, 21, 47].\n2.1 Sketching: Making Distance\nComputations Faster\nIn the sketching approach, we compute a compressed represen-\ntation of the data points to transform the dataset from Rğ‘‘toRğ‘‘â€²,\nsuch that distances in Rğ‘‘are preserved in Rğ‘‘â€². This transforma-\ntion makes each distance computation between the query pointand a data point easier since distances are now computed in Rğ‘‘â€²\ninstead of in Rğ‘‘(ğ‘‘â€²<ğ‘‘). In order to find the nearest neighbors\nunder this paradigm, the whole dataset (compressed version) still\nneeds to be scanned and distances computed between all points\nin the dataset and the query point.\nMachine learning methods have been instrumental in the\nsketching approach. Most machine learning methods use a fairly\nsimple optimization objective to minimize reconstruction error in\nthe lower dimensional space to preserve distances in the higher\ndimensional space. There have been many such works under\n\"Learning to Hash.\" [ 45,46]. We highlight the recent work ScaNN [16],\nwhich develops a novel quantization loss function that outper-\nforms previous sketching methods and forms the current state-\nof-the-art in the sketching domain.\n2.2 Indexing: Reducing the Search Space\nUnder the indexing paradigm, we discuss graph-based and space-\npartitioning approaches. We then explore the benefits of learning\nspace-partitions for indexing.\n2.2.1 Graph-Based Approaches. Graph-based algorithms are\none class of algorithms that reduce the number of points to search\nthrough. Graph-based algorithms [ 13,17,18,32] construct a\ngraph from the dataset (can be a ğ‘˜-NN graph) and then perform a\ngreedy walk for each query, eventually converging on the nearest\nneighbor(s). While graph-based methods are very fast, they have\nsuboptimal locality of reference and access the datasets adap-\ntively in rounds. This makes graph search not ideal in modern\ndistributed systems that often store the data points in an external\nstorage medium since access to that medium could be very slow\nrelative to searching and processing indices of data points [11].\n2.2.2 Space Partitioning Methods. Another class of algorithms\nis space-partitioning algorithms. These methods partition the\nsearch space into several bins by dividing the ambient space of\nthe dataset Rğ‘‘. In this paper, we focus on the space-partitioning\napproach. Given a query point ğ‘, we identify the bin containing\nğ‘and produce a list of nearby candidates from the data points\npresent in the same bin (or, to boost the k-NN recall, in nearby\nbins as well).\nSpace partitioning methods have numerous benefits [ 11]. First,\nthey are naturally applicable in distributed settings, where dif-\nferent machines can store points in different bins. Furthermore,\neach machine can do a nearest neighbor search locally using\nother NNS methods to speed up the search further. Finally, unlike\ngraph-based methods, space partitioning/data clustering meth-\nods only access the data points in one shot, only requiring access\nto the dataset points once it finds a candidate set and identifies\nthe relevant points within it.\nPopular space partitioning methods include LSH [ 3,10,30],\nQuantization-based approaches, where partitions are obtained\nusing K-Means clustering of the dataset [ 22], and tree-based\napproaches such as random-projection or PCA trees [ 6,9,24,43].\nClassical space-partitioning algorithms like LSH [ 3,10,30],\nKD-trees, and random projection trees [ 8,9] cannot effectively\noptimize a partition to a specific data distribution. In our experi-\nments in Section 5, we show that these approaches (especially\nLSH and random trees projection trees) perform poorly com-\npared to the other baselines. To create partitions better tailored\nto individual data distributions, we now look into learning based\nmethods for space partitioning.\n\n2.3 Learning Indexes for Space Partitioning\nThere has been some prior work on incorporating machine learn-\ning techniques to improve space partitioning in [ 7,28,38]. We\nhighlight in particular the work in [ 28], termed Boosted Search\nForest , which introduces a custom loss function similar to our\nmethod. However, Boosted Search Forest, like [ 7] and [ 38], can\nonly learn hyperplane partitions to split the dataset. This limits\ntheir partitioning performance as hyperplanes may not be suffi-\ncient to split more sophisticated data distributions. In contrast,\nour loss allows any machine learning model to learn a wider\nclass of partitions for a dataset. Moreover, using our loss, even\na simple logistic regression model can learn better hyperplane\npartitions than these prior learning approaches, indicating that\nour loss function can better score partitions than the loss used in\nBoosted Search Forest.\nA recent relevant work, Neural LSH [11] uses supervised learn-\ning with neural networks to create a space partitioning index by\nfirst creating a k-NN graph of the input dataset and running a\ncombinatorial graph partitioning algorithm to obtain a balanced\ngraph partition. The graph partition divides the dataset into sev-\neral bins. It then trains the neural network to correctly classify\nout-of-sample query points to specific bins of the partition.\nApart from the above, other notable recent works on learned\nindexes such as Flood [34] and Tsunami [34] are summarized\nin [2]. While these learned indexes are very efficient, they do not\nscale well to high-dimensional datasets, which is our focus in\nthis paper.\n3 PROBLEM DEFINITION\nLetRğ‘‘be ağ‘‘-dimensional space. Given a dataset ğ‘‹={ğ‘1,...,ğ‘ğ‘›}\nof sizeğ‘›inRğ‘‘and a query point ğ‘âˆˆRğ‘‘,ğ‘˜-nearest neighbor\nsearch returns the top- ğ‘˜ranked points from ğ‘‹that are the most\nsimilar to the query point ğ‘. We can use the Euclidean dis-\ntance or any custom distance function to define the distance\nbetween any two points, ğ‘¥andğ‘¦, in the data space. For exam-\nple, if the distance function ğ·is Euclidean distance, then we\ndefine the distance between ğ‘and data point ğ‘ğ‘–asğ·(ğ‘,ğ‘ğ‘–)=âˆšï¸ƒ\n(ğ‘1âˆ’ğ‘1\nğ‘–)2+(ğ‘2âˆ’ğ‘2\nğ‘–)2+Â·Â·Â·+(ğ‘ğ‘‘âˆ’ğ‘ğ‘‘\nğ‘–)2. In modern large-scale\napplications, either ğ‘›,ğ‘‘, or both are large, with ğ‘›often being\nbillions or more. When answering nearest neighbor queries\nin real-time, explicitly computing ğ·(ğ‘,ğ‘ğ‘–)for all points in the\ndataset can be prohibitively expensive. If ğ‘›is large, traversing\nthe whole dataset to find ğ‘˜-NN is intractable, and if ğ‘‘is large,\ncomputing the ğ·function itself is time-consuming for each data\npoint.\nThus, in Approximate ğ‘˜-Nearest Neighbor Search (ANNS) , we\nrelax the requirement of retrieving the exact top-k ranked points\nfromğ‘‹w.r.tğ‘. In ANNS, we return ğ‘˜points close to ğ‘, ensuring\nthat as many of them are the true ğ‘˜-nearest neighbors of ğ‘as\npossible. Let ğ‘â€²\nğ‘˜(ğ‘)be the answer set of ğ‘˜data points returned\nby the ANNS, and ğ‘ğ‘˜(ğ‘)be the answer set of true ğ‘˜-NN for the\nquery point ğ‘. Thus, in the ANNS, we aim to maximize k-NN\naccuracy of the answer set, where,\nk-NN accuracy =|ğ‘â€²\nğ‘˜(ğ‘)Ã‘ğ‘ğ‘˜(ğ‘)|\nğ‘˜(1)\n4 OUR METHOD\nThis section presents the details of our proposed method to solve\nthe ANNS problem using an unsupervised learning-based ap-\nproach. First, we give a high-level overview of the proposedapproach. We then discuss the details of the different core compo-\nnents of the system. Finally, we present a couple of enhancements\nthat include ensembling and hierarchical partitioning schemes.\nIn this work, we improve upon the state-of-the-art partitioning\nmethod Neural LSH [11]. Neural LSH takes hours to preprocess\na million-sized dataset to generate training labels to pass to the\nneural network. In contrast, our model takes less than two hours\nto learn high-quality partitions, even on constrained hardware\nresources. More importantly, Neural LSH does not use the neural\nnetwork to create the partitions themselves. We introduce an end-\nto-end learning method that uses a novel loss function to create\ndataset partitions and learn to classify out-of-sample queries in a\nsingle learning step.\n4.1 Overview\nWe present a high-level overview of our proposed approach in\nFigure 1. In general, the ANNS consists of two distinct phases,\n(i) the offline phase, where we train the model to partition the\ndataset, and (ii) the online phase, to answer queries in real-time\nusing the trained model.\nIn the offline phase, we use the dataset points in ğ‘‹, the NN\nmatrix (described in Section 4.2.1), and the loss function (de-\nscribed in Section 4.2.2) to train the model in the training loop.\nThe trained model is then used to partition the dataset and create\na lookup table to speed up the retrieval of candidate sets in the\nonline phase. In the online phase, the trained model identifies\nthe most likely bins to which the query ğ‘belongs. The dataset\npoints inside these bins are retrieved using the lookup table to\nform a candidate set of points containing probable nearby points\nofğ‘. Finally, we search the reduced point sets in the candidate\nset to find the ANN of ğ‘.\n4.2 The Offline Phase\nIn the offline phase, we use the ğ‘›points in the dataset ğ‘‹to train\na machine learning model, ğ‘€, to create a partition of the data\nspace intoğ‘šbins.\n4.2.1 Preprocessing. In this step, we create a ğ‘˜â€²-NN matrix1\nfrom the dataset ğ‘‹. Theğ‘–ğ‘¡â„row of theğ‘˜â€²-NN matrix contains\ntheğ‘˜â€²nearest neighbors of ğ‘ğ‘–fromğ‘‹.\nThis matrix captures the geometry and distribution of ğ‘‹and\nprovides this information to the model and the loss function.\nTheğ‘˜â€²-NN matrix is essentially a ğ‘˜â€²-NN graph many indexing\nmethods use, represented as an adjacency list. Figure 2 shows\nthe representation of the ğ‘˜â€²-NN matrix, where ğ‘ğ‘–represents the\nğ‘–ğ‘¡â„point inğ‘‹. Theğ‘–ğ‘¡â„row in the matrix corresponds to all the\nğ‘˜â€²NNs of theğ‘ğ‘–. The matrix shown is a 5-NN matrix: Each row\ncontains the five nearest neighbors of the corresponding point.\nNote that this is the only preprocessing in our proposed approach.\nPreparing this matrix takes approximately 30 minutes on\nthe million-sized dataset we used in our experiments. We com-\npute all pairwise distances by traversing the whole dataset only\nonce in the offline phase. In practical applications, the ğ‘˜â€²-NN\nmatrix is computed in the offline phase beforehand and stored in\ndisk/cache for fast retrieval.\n4.2.2 The Loss Function. In this section, we discuss our\nproposed loss function, which is the key to our unsupervised\nlearning-based solution. The key intuition of the custom loss\n1Note that this ğ‘˜â€²can be different from the ğ‘˜used at query time for finding the\napproximate ğ‘˜nearest neighbors w.r.t. the query.\n\nFigure 1: Overview of our method.\nğ‘0ğ‘7ğ‘10ğ‘3ğ‘21ğ‘11\nğ‘1ğ‘0ğ‘20ğ‘19ğ‘7ğ‘5\nğ‘2ğ‘4ğ‘9ğ‘20ğ‘17ğ‘8\n... ... ... ... ... ...\nFigure 2: 5-NN matrix created from the dataset before\nmodel training in the offline phase.\nfunction to obtain a quality dataset partition comes from the\nfollowing two objectives:\n(1)Quality of candidate sets generated : Intuitively, for a given\nquery point ğ‘, a high-quality candidate set would have\nmost or all of the nearest neighbors of ğ‘contained within\nthe candidate set.\n(2)Even distribution of data points among all bins : Ensuring\neven distribution of the ğ‘›data points among all the ğ‘šbins\nof the partition (roughly ğ‘›/ğ‘špoints per bin) results in\nsmaller candidate set sizes generated per query on average.\nWe desire fewer points per candidate set (ğ¶)since the\ncandidate set size|ğ¶|is proportional to computation cost:\nWe need to iterate through the points in ğ¶to return the\nnearest neighbors of ğ‘.\nThe loss computes how far away a given partition is from our\ndesired objectives. The loss has two factors: (i) the quality cost ,\nwhich measures how bad on average a candidate set is for a query,\nand (ii) the computational cost , which measures how far away the\npartition is from being a balanced one.\nWe define the terms used in the loss formulation in Table 1:\n2Note that our model returns the probability distribution of a point being in different\nbins of the given partition. In order to formulate the loss during model training, we\nonly consider the most likely bin the model assigns to an input point.Notation Meaning\nğ‘‹âˆˆRğ‘‘Theğ‘‘dimensional dataset to be partitioned\nğ‘„ Set of queries{ğ‘1,ğ‘2,..}, not necessarily\npresent inğ‘‹\nğ‘… A partition that divides ğ‘‹intoğ‘šbins\nğ‘ğ‘˜â€²(ğ‘) set of trueğ‘˜â€²-nearest neighbors of point ğ‘\nfromğ‘‹\nğ‘…(ğ‘) the most likely bin2inğ‘…that might contain\nğ‘\nğ¶(ğ‘) Candidate set of ğ‘\nTable 1: Notations used\nInğ‘ğ‘˜â€²(ğ‘),ğ‘âˆˆRğ‘‘can either be a query point not present in\nğ‘‹, or a data point in ğ‘‹. Note that the ğ‘˜â€²-NN matrix we defined\nearlier helps us to quickly retrieve ğ‘ğ‘˜â€²(ğ‘ğ‘–)for any point ğ‘ğ‘–by\nsimply indexing into the ğ‘–th row of the ğ‘˜â€²-NN matrix.\nFor a given partition ğ‘…,ğ¶(ğ‘)is the set of all points in ğ‘‹that\nare present the bin ğ‘…(ğ‘). Therefore, for a point ğ‘,ğ¶(ğ‘)denotes\nitscandidate set .\nFinally,ğ‘„denotes the set of all query points, where points in\nğ‘„are not necessarily present in ğ‘‹.\nWe can now define the quality cost and the computation cost\nofğ‘…as follows:\nâ€¢The quality cost of ğ‘…,ğ‘ˆ(ğ‘…), can be defined as:\nğ‘ˆ(ğ‘…)=âˆ‘ï¸\nğ‘âˆˆğ‘„âˆ‘ï¸\nğ‘âˆˆğ‘ğ‘˜â€²(ğ‘)1ğ‘…(ğ‘)â‰ ğ‘…(ğ‘) (2)\nâ€“Where 1is the indicator function. The factor 1ğ‘…(ğ‘)â‰ ğ‘…(ğ‘)\ncan otherwise be expressed as:\n1ğ‘…(ğ‘)â‰ ğ‘…(ğ‘)=(\n1,ifğ‘…(ğ‘)â‰ ğ‘…(ğ‘)\n0,otherwise(3)\n\nwhereğ‘…(ğ‘)â‰ ğ‘…(ğ‘)if the bin in ğ‘…that contains ğ‘is not\nthe same as the bin that contains ğ‘.\nâ€¢The average computation cost of ğ‘…,ğ‘†(ğ‘…), can be deter-\nmined by taking the mean of the candidate set sizes of all\nthe query points:\nğ‘†(ğ‘…)=mean\nğ‘âˆˆğ‘„|ğ¶(ğ‘)| (4)\nTo create a partition that serves as an efficient index for search-\ning theğ‘˜nearest neighbors, we need to find ğ‘…that minimizes\nbothğ‘ˆ(ğ‘…)andğ¶(ğ‘…). Mathematically,\nğ‘…optimal =min\nğ‘…{ğ‘ˆ(ğ‘…)+ğœ‚.ğ‘†(ğ‘…)} (5)\nwhereğœ‚is a balance parameter that trades off between the\ntwo factors of the cost.\nWe can implement our loss function using any standard mod-\nern machine learning library that supports tensor operations\nwith automatic differentiation, which will allow the framework\nto compute the gradients of our loss function with respect to the\nparameters of any machine learning model without explicitly\nformulating them.\nComputing quality cost : For simplicity, let us assume that\nany data point ğ‘ğ‘–can be a query. Now, we show how to compute\nğ‘ˆ(ğ‘…)for a single data point, ğ‘ğ‘–âˆˆRğ‘‘, inğ‘‹.\nFirst, we input ğ‘ğ‘–into the model ğ‘€, to getğ‘ğ‘–as follows.\nğ‘€(ğ‘ğ‘–)=ğ‘ğ‘–=\u0000ğ‘1\nğ‘–ğ‘2\nğ‘–... ğ‘ğ‘š\nğ‘–\u0001(6)\nHereğ‘€(ğ‘ğ‘–)is the modelâ€™s output for the point ğ‘ğ‘–, andğ‘ğ‘—\nğ‘–is the\nprobability of point ğ‘–being assigned to bin ğ‘—.\nWe now determine to which bin ğ‘ğ‘–should be assigned if the\npartition is optimal. To do this, we use the ğ‘˜â€²-NN matrix to quickly\nretrieveğ‘ğ‘˜â€²(ğ‘ğ‘–), the set of true ğ‘˜â€²-nearest neighbors of ğ‘ğ‘–from\nğ‘‹, as:\nğ‘ğ‘˜â€²(ğ‘ğ‘–)=\u0000Ë†ğ‘1Ë†ğ‘2... Ë†ğ‘ğ‘˜â€²\u0001(7)\nHere Ë†ğ‘ğ‘—is theğ‘—th nearest neighbor of ğ‘ğ‘–inğ‘‹.\nWe pass all the points in ğ‘ğ‘˜â€²(ğ‘ğ‘–)through the model to get the\nmodelâ€™s outputs for the ğ‘˜â€²-nearest neighbors of ğ‘ğ‘–.\nğ‘€{ğ‘ğ‘˜â€²(ğ‘ğ‘–)}=Ë†ğ‘1\n...\nË†ğ‘ğ‘˜â€²Â©Â­Â­\nÂ«Ë†ğ‘1\n1Ë†ğ‘2\n1... Ë†ğ‘ğ‘š\n1\n...\nË†ğ‘1\nğ‘˜â€²Ë†ğ‘2\nğ‘˜â€²... Ë†ğ‘ğ‘š\nğ‘˜â€²ÂªÂ®Â®\nÂ¬(8)\nHere, Ë†ğ‘ğ‘—is the modelâ€™s output for the Ë†ğ‘ğ‘—.\nNext, we determine the distribution of the points in ğ‘ğ‘˜â€²(ğ‘ğ‘–)\namong the available bins. To do this, we take the proportion of\npoints assigned to each bin from ğ‘€{ğ‘ğ‘˜â€²(ğ‘ğ‘–)}to get the following.\nğµğ‘˜â€²(ğ‘ğ‘–)=\u0010\nË†ğµ1Ë†ğµ2... Ë†ğµğ‘š\u0011\n(9)\nwhere,ğµğ‘˜â€²(ğ‘ğ‘–)lists the proportion of points among the ğ‘˜â€²-\nNNs ofğ‘ğ‘–that belong to each bin.\nIdeally, we want the model output for ğ‘ğ‘–to indicate the dis-\ntribution of its nearest neighbors over all the bins. Therefore,\nwe takeğµğ‘˜â€²(ğ‘ğ‘–)as the ground truth labels for the point ğ‘ğ‘–and\ncomputeğ‘ğ‘–â€™s quality loss as the cross entropy loss between\nğµğ‘˜â€²(ğ‘ğ‘–)andğ‘€(ğ‘ğ‘–):\nğ‘ˆ(ğ‘…)forğ‘ğ‘–=cross_entropy_loss (ğ‘ğ‘–,ğµğ‘˜â€²(ğ‘ğ‘–)) (10)\nFinally, to compute ğ‘ˆ(ğ‘…)for the entire dataset ğ‘‹, we calculate\nğ‘ˆ(ğ‘…)using Equation 10 for every point in ğ‘‹and then take the\naverage.\nComputational cost : For determining the computation cost\nfactor of the loss function, ğ‘†(ğ‘…), we need the modelâ€™s output onall the points in the dataset ğ‘‹. We pass the entire ğ‘‹through the\nmodel,ğ‘€, to get the following output as ğ‘€(ğ‘‹).\nÂ©Â­\nÂ«ğ‘1\n1ğ‘2\n1... ğ‘ğ‘š\n1\n...\nğ‘1ğ‘›ğ‘2ğ‘›... ğ‘ğ‘šğ‘›ÂªÂ®\nÂ¬(11)\nHere,ğ‘ğ‘—\nğ‘–is the probability that the model assigned point ğ‘–to\nbinğ‘—.\nOur target is to make the model evenly distribute the ğ‘›points\ninğ‘‹among all the ğ‘šavailable bins. Therefore, we ideally want\neach bin to contain ğ‘›/ğ‘špoints. In the model outputs, ğ‘€(ğ‘‹), in\nEquation 11, each ğ‘–th row denotes the model outputs for the\nğ‘–th point inğ‘‹, and theğ‘—th column denotes the probabilities of\nassigning each of the ğ‘–points to the ğ‘—th bin.\nTo ensure an even distribution of points between the available\nbins, we want all the ğ‘›points in the dataset to be assigned to the\nğ‘šavailable bins evenly, such that each bin has approximately\nğ‘›/ğ‘špoints assigned to it. For each query point, ğ‘, our model\noutputs a probability distribution over the available bins for as-\nsigningğ‘. We assign ğ‘to the bin with the highest probability\nfrom this distribution. Therefore, for a balanced partition, we\nwant each column to only have ğ‘›/ğ‘šhigh values, since the ğ‘–th\nhigh probability value in the ğ‘—th column corresponds to point ğ‘–\nbeing assigned to the ğ‘—th bin. To that end, we filter the highest\nğ‘›/ğ‘šprobability values by selecting the highest ğ‘›/ğ‘švalues in\neach column of the output matrix to get a window ,ğ‘¤, of high\nprobability values:\nğ‘¤=maxğ‘›/ğ‘švalues across each column of ğ‘€(ğ‘‹)\n=Â©Â­Â­\nÂ«ğ‘1\n1ğ‘2\n1... ğ‘ğ‘š\n1\n...\nğ‘1\nğ‘›/ğ‘šğ‘2\nğ‘›/ğ‘š... ğ‘ğ‘š\nğ‘›/ğ‘šÂªÂ®Â®\nÂ¬(12)\nTo calculate ğ‘†(ğ‘…), we sum all the entries in the window, ğ‘¤,\nfrom Equation 12 and negate it:\nğ‘†(ğ‘…)=âˆ’âˆ‘ï¸\nw (13)\nMinimizing ğ‘†(ğ‘…)leads to higher values in the ğ‘›/ğ‘šwindow,\ncreating a more balanced partition.\nCaveats : In the operations detailed above, we calculate the\nloss using only the data points inğ‘‹, even though our loss for-\nmulation in Equations 2 and 4 requires a set of query points .\nIn our formulation, we assume that the query points follow the\nsame distribution as the data points in ğ‘‹. Therefore, we can use\nonly the points in ğ‘‹to compute the loss.\nAnother caveat of our loss is that we can only calculate it over\na batch of input points and not for individual data points like\nin other loss functions typically used in machine learning (We\ncalculateğ‘†(ğ‘…)over the entire batch of points). We need a batch of\npoints to compute ğ‘†(ğ‘…)because the model cannot learn anything\nabout the underlying distribution of ğ‘‹from a single data point.\nAs a result, we need special care when using mini-batches for\nmodel training.\nBatching : So far, we assume that the output matrix of the\nwhole dataset is available to us for calculating the loss value. In\npractice, the output matrix of the entire dataset may not fit in\nGPU or CPU memory during model training. In this case, we\ncan approximate the data distribution by randomly sampling a\nsmaller batch of points from the dataset for each iteration of the\ntraining loop. As long as our sampling technique is uniform (i.e.,\nwe choose every point in ğ‘‹for a particular mini-batch with equal\nprobability), the sampled mini-batch will have roughly the same\n\nAlgorithm 1 Offline Phase - Train model to create space parti-\ntioning index\nInput : Datasetğ‘‹âˆˆRğ‘‘, nearest neighbors to use ğ‘˜â€²>0,\nnumber of bins ğ‘š, Distance function ğ·\n(1)Create ağ‘˜â€²-NN matrix by computing pairwise distances\nusingğ·between all points in ğ‘‹, then storing indices of\ntrueğ‘˜â€²nearest neighbors of each point.\n(2)Train a machine learning model ğ‘€with the loss function\ndefined in 4.2.2. This model jointly learns a partition of ğ‘‹\nand learns to classify new points to assign queries into\nbins.\n(3)Run inference on all points in ğ‘‹to form a partition ğ‘…of\nğ‘‹. Store the point indices to keep track of the points in ğ‘‹\nassigned to each bin in a lookup table.\ndistribution of points as ğ‘‹. Our experiments show that sampling\neven justâ‰ˆ4%of the dataset per mini-batch leads to relatively\nhigh-quality learned partitions.\n4.2.3 Training the Model. Algorithm 1 outlines the whole\nlearning process. We detail the algorithm steps below.\nIn Step 1 we create the ğ‘˜â€²-NN matrix using a given distance\nmeasureğ·. Then, in Step 2, we use the points in ğ‘‹, theğ‘˜â€²-NN\nmatrix, and the loss function defined above to train a model to\ncreate a partition of the dataset ğ‘‹withğ‘›pointsâˆˆRğ‘‘, dividing it\ninto a predetermined number (say ğ‘š) of bins. We use the machine\nlearning model in this setting to output a probability distribution\nover the bins assigned to ğ‘.\nWe want our model to generalize well to query points (ğ‘âˆˆRğ‘‘)\noutside ofğ‘‹(i.e., queryWedel has never seen during training).\nTherefore, we have to cluster the dataset ğ‘‹intoğ‘šbins and also\npartition the entire Rğ‘‘for the range occupied by the dataset. Neu-\nral networks are suitable for this task. They can learn complex\ndecision boundaries optimized for a specific dataset and use reg-\nularization techniques to prevent overfitting on the training data.\nWe learn the partition by minimizing the loss function defined\nin Section 4.2.2.\nAfter the model training is complete, in Step 3, we pass the\nentire dataset of points ( ğ‘‹) through the model to obtain the\nlearned partition of the dataset ğ‘‹. In the online phase, we need\nto quickly retrieve all the points in ğ‘‹belonging to a particular\nbin. To speed up this retrieval, we store the indices of the points\ninğ‘‹assigned to each bin in a lookup table.\n4.3 The Online Phase\nOnce the system trains the model and creates the lookup table\noutlined in the previous section, it is ready to answer queries in\nthe online phase. Algorithm 2 outlines the online phase.\nIn Step 1, we pass the given query point ğ‘through the model\nto getğ‘€(ğ‘), a probability distribution over assigned bins of ğ‘.\nIn step 2,ğ‘€(ğ‘)is used to determine the set of bins ğ‘ğ‘the query\npoint might belong to. Then, using the lookup table created in\nthe offline phase, we retrieve all the points in ğ‘‹that also belong\nto the bins in ğ‘ğ‘to form the candidate set of points,ğ¶(ğ‘). Finally,\nin Step 3, we search through the points in ğ¶(ğ‘)to return the\nğ‘˜-Nearest Neighbors of ğ‘. Hence, we reduce the search space\nfrom the entire dataset to just ğ¶.\nInstead of searching in just one bin, we use the probability dis-\ntribution output by the model to search in the ğ‘šâ€²most probable\nbins. This way, we trade-off higher nearest neighbors accuracy(since we are more likely to find neighbors close to ğ‘simply\nby searching through more nearby points) at the cost of higher\nsearch time (since we need to search through a larger candidate\nset).\nAlgorithm 2 Online Phase: Return the k-nearest neighbors for\na query point\nInput : Query Point ğ‘âˆˆRğ‘‘, number of bins to search ğ‘šâ€²,\nnumber of nearest neighbors to return ğ‘˜, Distance function ğ·,\nTrained model ğ‘€.\n(1) Run inference on point ğ‘by computing ğ‘€(ğ‘)\n(2)Fromğ‘€(ğ‘), for the most probable ğ‘šâ€²assigned bins ğ‘ğ‘=\n{ğ‘1,ğ‘2,...,ğ‘ğ‘šâ€²}, retrieve all points from ğ‘‹that are assigned\nto any ofğ‘ğ‘, using the lookup table from Step 3 in Algo-\nrithm 1, to form the Candidate Set(ğ¶)\n(3)For all points in ğ¶, computeğ·(ğ‘,ğ‘ğ‘–), and return the ğ‘˜\nmost similar points to the query.\n4.4 Optimizations\nIn this section, we propose two additional components: (i) A\nboosting method that uses an ensemble of models to create mul-\ntiple partitions, and ii) a hierarchical partitioning strategy that\nrecursively divides the dataset to get finer dataspace partitions.\n4.4.1 Ensembling. In applications where high ğ‘˜-NN accuracy\nis crucial, we can boost the accuracy by training multiple models\nsequentially, with each model generating a different partition\nfor the same dataset. We call this approach ensembling , where\nwe create an ensemble of models . Ensembling allows us to create\na set of complementary partitions for a single dataset. The intu-\nition behind ensembling is that different models can specialize\nin different regions of the data space. Working together, these\nmodels can increase the quality of candidate sets generated for\nany query point. Figure 3 illustrates this intuition.\nAlgorithm 3 Ensembling\nInput : Datasetğ‘‹âˆˆRğ‘‘containingğ‘›points, Initial input weights\nğ‘Š1={ğ‘¤1\n1,ğ‘¤1\n2,...,ğ‘¤1ğ‘›}, Number of models in ensemble ğ‘’\n(1)forğ‘—âˆˆ1,2,...ğ‘’do:\n(a)Train model ğ‘šğ‘—to learn partition ğ‘Ÿğ‘—, using weights ğ‘Šğ‘—,\nby modifying the quality cost of the loss function:\nğ‘ˆ(ğ‘Ÿğ‘—)=ğ‘›âˆ‘ï¸\nğ‘–=1ğ‘ğ‘–.ğ‘¤ğ‘—\nğ‘–âˆ‘ï¸\nğ‘âˆˆğ‘ğ‘˜â€²(ğ‘ğ‘–)1ğ‘Ÿğ‘—(ğ‘)â‰ ğ‘Ÿğ‘—(ğ‘ğ‘–) (14)\n(b) Obtain new weights for use in the next model:\nğ‘¤ğ‘—+1\nğ‘–=âˆ‘ï¸\nğ‘âˆˆğ‘ğ‘˜â€²(ğ‘ğ‘–)1ğ‘…(ğ‘)â‰ ğ‘…(ğ‘ğ‘–)\nğ‘¤ğ‘—+1\nğ‘–=ğ‘¤ğ‘—+1\nğ‘–.ğ‘¤ğ‘—\nğ‘–\nOur ensembling algorithm is based on AdaBoost [ 41]. How-\never, unlike AdaBoost, instead of training many weak learners ,\nwe use this boosting formulation to create many complementary\npartitions, to improve the quality of the generated candidate set.\nBoosted Search Forest [28] used this concept in a similar fashion.\nTo create an ensemble of models, we first assign weights to\neach point in ğ‘‹. We update the quality cost factor of the loss\nfunction as in Equation 14 in Algorithm 3 to incorporate these\n\nFigure 3: Ensembling with two models. Here, Model 2 (M2) performs better with the yellow query point, resulting in the\nsecond model outputting a higher confidence value.\nAlgorithm 4 Querying with ensembling\nInput : Query point ğ‘, Ensemble of trained models\n(ğ‘€1,ğ‘€2,...,ğ‘€ğ‘’)\n(1)Run inference on the query point ğ‘on all the models\n(ğ‘€1,ğ‘€2,...,ğ‘€ğ‘’)in the ensemble to get corresponding bin\nassignments of each model.\n(2) Each model, ğ‘€ğ‘–, returns a candidate set, ğ‘ğ‘–,\nğ¶={ğ‘1,ğ‘2,...,ğ‘ğ‘’}\n(3)Take each modelâ€™s highest probability as its confidence\nvalue,ğœğ‘–:\nğ‘†={ğœ1,ğœ2,...,ğœğ‘’}\n(4)the best candidate set is the one with the highest confi-\ndence score:\nğ‘ğ‘ğ‘’ğ‘ ğ‘¡=ğ¶[ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥ğ‘†]\n(5)search through the items as before on only the best candi-\ndate set to return the nearest neighbors of ğ‘\nweights. We train the different models in the ensemble sequen-\ntially. We assign equal weights to all the data points for training\nthe first model. After training the first model, we use the trained\nmodel to obtain new input weights for the second model. We can\nthen train the second model using the new input weights and so\non. In Algorithm 3, ğ‘¤ğ‘—\nğ‘–represents the ğ‘–ğ‘¡â„data pointâ€™s weight for\ntheğ‘—ğ‘¡â„model in the ensemble.\nIntuitively, each model tries to optimize its partition to perform\nbetter for points with which allthe previous models performed\npoorly. Each model in the ensemble will tune its partition to\ngive more importance to \"difficult\" points (i.e., points with a high\nweight value) since they contribute more to the quality factor\nof the loss. We ensure that the weights of the following models\nin the ensemble only try to optimize for the points in which\nprevious models could not do well by multiplying the weights of\nall points with the weights of the previous models. Multiplying\nthe weights like this ensures that only points with high weights\nfor all previous models will have high weights for the next model.\nIn the online phase, we pass the query point ğ‘through all the\nmodels in the ensemble. Since each model ğ‘€ğ‘–returns a probability\ndistribution over assigned bins, we can return the highest proba-\nbility as the confidence value of ğ‘€ğ‘–. Then, we select the candidate\nset corresponding to the model with the highest confidence value\nFigure 4: Dividing a dataset hierarchically with three\nmodels (one root model and two leaf models), finally\nresulting in a partition with four bins\nas the output candidate set of the ensemble. Algorithm 4 outlines\nthe querying process.\n4.4.2 Hierarchical Partitioning. When the number of required\nbinsğ‘šis large, training can become difficult as we attempt to\npartition a large dataset into many bins in a single pass. In order\nto make training more efficient, we can recursively partition the\ndataset into ğ‘š1bins at the first level, then subdivide each of\nthose bins into ğ‘š2bins at the second level, and so on, resulting\nin a total of ğ‘š1Â·ğ‘š2Â·....Â·ğ‘šğ‘™bins forğ‘™level-partitioning. This is\nillustrated in Figure 4.\nFor a query point ğ‘, we passğ‘from the top of the tree down to\nthe leaves. We multiply the assigned probabilities of each model\ndown the tree to obtain the final probability of assigning ğ‘to each\nof the bins in the leaves. Hierarchical partitioning allows us to\nsimplify the learning process for each model. Further, each model\ncan have fewer parameters and be simpler since each modelâ€™s\nlearning task is more straightforward. As a result, we can often\ntrain a tree of models that takes up lesser total memory than a\nsingle large and complex model needed to partition the same\ndataset in a single pass.\n4.5 Time Complexity Analysis\nThe online phase of our algorithm is sublinear as we do not have\nto traverse the entire dataset to find a queryâ€™s k-NNs. For a given\nquery point ğ‘, our algorithm follows two steps to find ğ‘â€™s k-NNs.\nFirst, we feed ğ‘to our model to find the associated bins of ğ‘and\n\nthus its candidate set. Second, we traverse the candidate set to\nfindğ‘â€™s nearest neighbors (by brute-force search). The first task\nis of orderğ‘‘, the dimensionality of ğ‘, since the input layer of the\ntrained model takes ğ‘‘values for multiplication. The second task\nis of orderğ‘ğ‘‘, whereğ‘is the largest candidate set size, since we\nneed to traverse the entire candidate set to find ğ‘â€™s k-NNs. Thus,\nfinding k-NNs of a single query point ğ‘using our approach is an\noperation of order ğ‘‚(ğ‘ğ‘‘+ğ‘‘).\n5 EXPERIMENTS\nWe present detailed experimental evaluations of our proposed\napproach and compare the results with the state-of-the-art base-\nlines using several real datasets. We first discuss the experimental\nsettings that include datasets, baselines, performance metrics,\nand parameters of the experiments. We then discuss the imple-\nmentation details of the algorithm and present our experimental\nevaluation. Finally, we compare our space-partitioning perfor-\nmance with that of common clustering methods.\n5.1 Experimental Settings\nHere, we discuss the datasets, state-of-the-art baseline approaches,\nand different parameters of our experiments.\n5.1.1 Datasets. For our experimental benchmarks, we used\ntwo standard ANN benchmark datasets [5]:\nâ€¢SIFT : 1M data points, each having 128 dimensions\nâ€¢MNIST : 60k data points, each having 784 dimensions\nBoth datasets come with 10k query points that are not present\nin the training dataset. We choose these datasets as they encom-\npass both aspects of large-scale datasets: a high number of points\n(SIFT has 1M points), and high dimensionality (MNIST has 784\ndimensions), with data taken from real-world applications.\n5.1.2 Baselines. We compare our approach with several space\npartitioning baselines, outlined in Section 5.2. Notably, we com-\npare with the state-of-the-art Neural LSH [ 11] and K-means\nclustering. Neural LSH [ 11] is currently the best-performing\ndeep learning based space-partitioning approach. On the other\nhand, K-means clustering is a well-known technique used in\nmany production systems for partitioning the dataset before\nANN search or other processing. For both baselines, we use the\nsame codebase and settings found in the Neural LSH [ 11] pa-\nper: https://github.com/twistedcubic/learn-to-hash. To demon-\nstrate how our partitioning strategy can enhance the performance\nof the state-of-the-art non-learning ANNS techniques, we incor-\nporate our method with ScaNN and compare the performance\nwith vanilla ScaNN [16], HNSW [31], and FAISS [21].\n5.1.3 Performance metrics. To evaluate the effectiveness of\nthe baseline approaches, we compare and evaluate the trade-offs\nbetween two key metrics:\n(1)Theğ‘˜-NN accuracy: The fraction of the true ğ‘˜-Nearest\nNeighbors (ğ‘˜-NN) that are present among the ğ‘˜returned\npoints by the algorithm.\n(2)The size of the candidate set: The number of points in the\ncandidate set ğ¶represents the query processing time, as\nwe need to search through all the points in ğ¶to return the\nğ‘˜-NN.\nIn general, more candidates present in the candidate set for any\npartitioning (or clustering) algorithm lead to a larger ğ‘˜-NN accu-\nracy.5.1.4 Parameters. Our algorithm exposes a lot of tuneable pa-\nrameters for the user to optimize the framework to their specific\napplication needs. Changing each of these parameters affects a\ndifferent part of the model. These parameters include:\n(1)Integerğ‘˜â€²: This value specifies the number of nearest\nneighbors to consider when building the ğ‘˜â€²-NN matrix in\nthe offline phase. Setting a larger ğ‘˜â€²provides more infor-\nmation to the model and loss during training at the cost of\nrequiring more memory during training. However, setting\nğ‘˜â€²too high would result in far-away points becoming near-\nest neighbors for many data points. We found that setting\nğ‘˜â€²to 10 creates sufficiently good dataset partitions while\nusing less memory during training. Also, setting larger\nvalues ofğ‘˜â€²does not appreciably increase the quality of\nthe created partitions.\n(2)Integerğ‘š, number of bins to split the dataset into: ğ‘šaffects\nhow finely the model splits the dataset during training\nand, in turn, how \"difficult\" the problem is for the neural\nnetwork. Setting ğ‘što 16 for a 1M sized dataset, for in-\nstance, means that the dataset will be almost evenly split\namong 16bins, resulting in about 1ğ‘€/16=62500 points\nper bin. On the other hand, setting ğ‘što 256 for a 1M sized\ndataset partitions the dataset into 256 bins, with each bin\nhaving 1ğ‘€/256â‰ˆ3900 points.\n(3)Integerğ‘’, number of models in the ensemble: ğ‘’denotes\nthe number of models to train for a single dataset. Each of\ntheğ‘’models describes a different partition of the dataset.\nSince each model optimizes for the poorly placed points\nin all previous partitions, having more models increases ğ‘˜-\nNN accuracy for the same candidate set size. Also, having\na largerğ‘’means that each model can be simpler and can af-\nford to learn simpler (might not be high-quality) partitions\n(using a neural network with fewer parameters). Learn-\ning simpler models does not sacrifice partitioning quality\nsince the greater number of models in the ensemble can\nboost the quality of the returned candidate set of the indi-\nvidually weak models. However, a larger ğ‘’comes at the\ncost of longer training times (since each of the ğ‘’models\ntrains sequentially) and higher memory usage (since each\nof the models must be stored, along with their individual\nlookup tables).\n(4)Model Complexity: In our proposed framework, we can\nuse any machine learning model architecture as ğ‘€, the\nmodel used to learn the partitions. For instance, increasing\nthe number/size of the hidden layers or using a more com-\nplex architecture (such as replacing a linear model with a\nneural network) results in better-learned partitions. How-\never, more complex models require longer training times\nand more memory to store the larger models. We demon-\nstrate this by training two different model architectures,\naneural network and a logistic regression model, and\npresenting their results in Sections 5.4.1 and 5.4.2.\n(5)ğœ‚: The balance parameter in the loss (Equation 5). This\nvalue quantifies the trade-off between the two factors of\nthe loss function. Increasing ğœ‚makes the partition more\nbalanced, but a value of ğœ‚too high makes it more difficult\nfor the model to optimize the quality cost factor of the\nloss function. We tuned ğœ‚and set it to the lowest value, re-\nsulting in a balanced partition. We mentioned the specific\nvalues ofğœ‚used in Table 3.\n\n5.2 Implementation Details\nWe demonstrate our partitioning performance with two different\nmodel architectures:\nâ€¢Neural Networks : Here, we used a small neural network\nwith one input layer and one hidden layer containing\n128 parameters. Each network layer consists of a fully\nconnected layer, and batch normalization [ 20], followed\nby ReLU activations. The final layer is an output layer\ncontainingğ‘šoutput nodes followed by a softmax layer,\nwhereğ‘šis the number of bins in the partition. To reduce\noverfitting and to generalize well to unseen queries, we\nuse dropout [ 44] with a probability of 0.1 during training.\nWe train each neural network for about 100 epochs. We\ncompare this modelâ€™s performance with baselines K-means\nclustering and Neural LSH [ 11]. We also include results\nfor the data oblivious Cross-polytope LSH [ 3] to show\nimprovements in the performance of learning methods\nover non-learning methods.\nâ€¢Logistic Regression : Here, we used a simple logistic re-\ngression model to divide the dataset into two bins at each\nlevel recursively to form a partitioning tree. Each model in\nthe tree has two output nodes in the final layer, followed\nby a softmax layer to output a probability distribution\nover two bins. We trained each logistic regression model\nfor less than 50 epochs. We compare this modelâ€™s perfor-\nmance with other tree-based partitioning methods that\nrecursively split the dataset using hyperplanes: Regression\nLSH [11] (A variant of Neural LSH that uses logistic re-\ngression instead of neural networks), 2-means tree, PCA\ntrees [ 1,27,43], Random Projection trees [ 9] , Learned\nKD-tree [7], and Boosted search forest [28].\nThe model weights were initialized for both architectures with\nGlorot initialization [ 14]. We trained both types of models using\nthe Adam optimizer [ 25]. To show the performance improve-\nments of ensembling, we used an ensemble of methods to boost\nthe retrieval performance of the neural network architecture in\nour experiments.\nIn our experiments, we use the same number of bins for all the\nmethods to evaluate our approachâ€™s representative performance.\nWe use PyTorch [36] to implement our algorithms.\n5.3 Training Efficiency\nWe trained our models on a hosted runtime with a single-core\nhyperthreaded Xeon processor, 12GB RAM, and a Tesla K80 GPU\nwith 12GB GDDR5 VRAM. Training multiple models in an en-\nsemble with million-sized datasets takes less than an hour, signifi-\ncantly lower than the several hours of preprocessing time needed\nfor Neural LSH. We highlight the different training times for\ndifferent specifications in Table 3. The training times mentioned\nin Table 3 are the total times needed to train three base models\nin the ensemble while keeping GPU usage under 6GB .\nWe also need significantly fewer parameters on even the\nlargest model sizes to beat Neural LSHâ€™s partitioning performance\nwhen dividing the dataset into 256 bins. We highlight this in\nTable 2.Neural LSH Ours K-Means\nNo. of bins 256\nTotal parameters 729k 183k 33k\nHidden layer size 512 128 -\nTable 2: Approximate number of learnable parameters of\nselected space-partitioning methods when dividing SIFT\ninto 256 bins.\nDataset No. of bins Training time (minutes) Value of ğœ‚\nMNIST 16 2min 7\nMNIST 256 12min 30\nSIFT 16 6min 7\nSIFT 256 40min 10\nTable 3: Comparing our methodâ€™s approximate offline\ntraining times and ğœ‚values with different configurations.\n5.4 Performance Evaluation\nWe evaluate the performance of our method by comparing it with\nspace-partitioning methods using a neural network model and\ntree-based methods using a logistic regression model.\nWe generate each of the graphs shown by successively search-\ning in more of the most probable bins returned by the algorithms.\nWe systematically note the k-NN accuracies with increasing can-\ndidate set size,|ğ¶|.\n5.4.1 Comparing with space-partitioning methods. Here, we\npresent the performance evaluation of our proposed approach,\nusing a neural network as the learning model. Figure 5 shows\nthe comparison between our method and the selected baselines:\nNeural LSH, K-means, and Cross polytope LSH. We test with 16\nand 256 bins for all the baselines for the experiments to show the\ntrade-off between candidate set sizes and 10-NN accuracies. We\nuse hierarchical partitioning when dividing the dataset into 256\nbins, first splitting into 16 bins and then sub-splitting each bin\ninto 16 more bins. Splitting the dataset into a greater number of\nbins allows us to control the candidate set size, |ğ¶|, more finely\nbecause searching each additional bin of points increases |ğ¶|\nby a smaller amount. This leads to more points in the graph in\nFigures 5c and 5d.\nWe see that our model performs better than Neural LSH even\nusing just one base model in the ensemble when partitioning\nthe dataset into 256 bins (in figures 5c and 5d). Partitioning the\ndataset into a larger number of bins is an expected configuration.\nIt leads to greater k-NN accuracy in the online phase with smaller\ncandidate set sizes at the expense of longer training times and\nlarger models.\nAs for partitioning into 16 bins, we see almost similar parti-\ntioning performance compared to Neural LSH with both datasets\nin Figure 5 when we do not use any ensembling and train just one\nmodel. The similarity in k-NN retrieval performance suggests\nthat our model learns similar partitions to Neural LSH without\nusing any graph partitioning algorithm in an unsupervised set-\nting and uses significantly less time. When using more than one\nmodel in an ensemble, we see up to about 10%improvement in\nk-NN accuracy using three models (Figure 5).\nTable 4 shows the relative decrease in our methodâ€™s average\ncandidate set sizes compared to Neural LSH and K-means when\n\n(a) SIFT, 16 bins\n (b) MNIST, 16 bins\n(c) SIFT, 256 bins\n (d) MNIST, 256 bins\nFigure 5: Comparing our method with space-partitioning baselines. X-axis: number of candidates retrieved in the\ncandidate set. Y axis: 10-NN accuracy (Up and to the left is better). Our method uses an ensemble of 3 models to boost\nperformance.\ndividing the SIFT dataset into 16 bins and maintaining a 10-NN\naccuracy of 85%. The smaller candidate set sizes speed up ANNS\nproportionately as we have to search through a smaller number\nof points to attain the same 10-NN accuracy.\nThe experiments show that while Neural LSH can create high-\nquality partitions of the dataset, our approach returns better can-\ndidate sets (i.e., Our candidate sets contain more of the k-Nearest\nNeighbors for any given query point.) for query points since\nwe use multiple complementary partitions per dataset through\nensembling.\n5.4.2 Comparing with tree-based methods. We compare the\nperformance of our approach with baselines that use hyperplanes\nto partition the dataset (Figure 6). In this setting, we use binary de-\ncision trees up to depth 10, which correspond to the dataset being\ndivided recursively into 210=1024 bins for each of the methods\ncompared. We note that our method, using a logistic regression\nlearner, significantly outperforms Regression LSH without any\nensembling. This is especially true in the high accuracy regime,\nwhere in SIFT, for instance, to obtain a 10-NN accuracy of about\n98%, our approach returns candidate set sizes that are about 60%\nsmaller than the best performing baselines.\n5.4.3 Comparing with non-learning ANNS methods. In this\nset of experiments, we demonstrate the ubiquitous effectiveness\nof our partitioning approach in improving the performance ofMethod Decrease in candidate set size for 10-NN search\nNeural LSH 33%\nK-means 38%\nTable 4: Relative decrease in candidate size when\nsearching for 10-Nearest Neighbors in SIFT, maintaining\n10-NN accuracy of 85% in Figure 5a\nnon-learning ANNS approaches. We incorporated our partition-\ning approach in the best-performing ANNS method ScaNN. We\nfirst partition the data using our approach, where we split the\ndataset into a predetermined number of bins. Then, for a given\nquery point ğ‘, we use our trained model to return a candidate\nset of points that are likely to be near ğ‘. Finally, we use ScaNN\nto search for the k-NNs of ğ‘from its candidate set. In particular,\nwe use ScaNNâ€™s novel anisotropic quantization method to speed\nup this search. We term this pipeline as USP + ScaNN algorithm,\nwhere USP refers to our proposed Unsupervised Space Partition-\ning approach. We show the effectiveness of this approach by\ncomparing USP + ScaNN with vanilla ScaNN (i.e., ScaNN without\nany data partitioning algorithm run beforehand), ScaNN with\nK-means tree partitioning (termed as K-means + ScaNN , where K-\nmeans trees partition the dataset before running ScaNN), HNSW,\nand FAISS. Figure 7 outlines the results of our experiments. On\n\n(a) SIFT, 1024 bins\n (b) MNIST, 1024 bins\nFigure 6: Comparing our method with binary decision trees that use hyperplane partitions. X-axis: number of candidates\nretrieved in the candidate set. Y axis: 10-NN accuracy (Up and to the left is better).\n(a) SIFT\n (b) MNIST\nFigure 7: Using our partitioning method to enhance ScaNNâ€™s performance (Up and to the left is better). ScaNN + Ours\noutperforms commonly used previous best ANNS baselines.\naverage, the experiments show a 40%speedup in 10-NN retrieval\ntimes compared to the best-performing approach, K-means +\nScaNN.\n5.5 Comparison with clustering methods\nThe previous experiments show that our partitioning algorithm\ngenerates superior partitions compared to state-of-the-art parti-\ntioning baselines. Clustering algorithms (such as K-means clus-\ntering) split datasets into clusters and thus create partitions. We\ncan similarly use our algorithm to create clusters of the dataset in\nan unsupervised manner. We show that the clusters created from\nour algorithm are better than the most commonly used clustering\nalgorithms.\nWe show the visualization of several 2D standard datasets\n(moon andcircles ) from scikit learn [ 37], which are often used to\ndetermine the pitfalls of clustering algorithms. We also test with\nanother sample dataset generated using make_classification from\nscikit learn with four clusters, which is challenging for many\nclustering algorithms. We compare our approach with common\nclustering algorithms DBSCAN [ 12], Spectral clustering [ 35], and\nK-means clustering in Table 5, where we show that our clusteringperformance is optimal for the test datasets. The results show that\nour approach successfully outputs the most natural clustering\nregardless of the shape of the data distribution.\nWe note that even though spectral clustering achieves a simi-\nlar quality clustering as ours, we cannot scale spectral clustering\nefficiently to large and high-dimensional datasets. Thus, our pro-\nposed partitioning approach can be a strong alternative to com-\nmonly used clustering techniques for high-dimensional datasets.\n6 CONCLUSIONS\nThis paper proposes an end-to-end unsupervised learning frame-\nwork that couples partitioning and learning to solve the ANNS\nproblem in a single step. To facilitate the above, we propose\na multi-objective custom loss function that guides the neural\nnetwork (or any other learning model) to partition the space\nsuitable for providing high-quality answers for ANNS. To fur-\nther improve the performance, we propose an ensembling tech-\nnique by adding varying input weights to the loss function to\n\nOur Approach DBSCAN K-means Spectral clustering\nTable 5: Comparing common clustering algorithms to our space-partitioning approach.\ntrain multiple models and enhance search quality. Our exper-\nimental evaluation shows that our method beats the state-of-\nthe-art learning-based ANNS approach while using fewer pa-\nrameters and shorter offline training times on several bench-\nmark datasets. We also show that our unsupervised partition-\ning approach boosts the current best-performing ANNS method,\nScaNN, by 40%. The code base of this paper is available at https:\n//github.com/abrar-fahim/Neural-Partitioner.\nAcknowledgments: This work is done at DataLab (data-\nlab.buet.io), Dept of CSE, BUET. Muhammad Aamir Cheema is\nsupported by ARC FT180100140.\nREFERENCES\n[1]Amirali Abdullah, Alexandr Andoni, Ravindran Kannan, and Robert\nKrauthgamer. 2014. Spectral Approaches to Nearest Neighbor Search.\narXiv:1408.0751 [cs] (Aug. 2014). http://arxiv.org/abs/1408.0751 arXiv:\n1408.0751.\n[2]Abdullah Al-Mamun, Hao Wu, and Walid G. Aref. 2020. A Tutorial on Learned\nMulti-dimensional Indexes. In Proceedings of the 28th International Conference\non Advances in Geographic Information Systems . ACM, Seattle WA USA, 1â€“4.\nhttps://doi.org/10.1145/3397536.3426358\n[3]Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and\nLudwig Schmidt. 2015. Practical and Optimal LSH for Angular Distance.\narXiv:1509.02897 [cs] (Sept. 2015). http://arxiv.org/abs/1509.02897 arXiv:\n1509.02897.\n[4]Alexandr Andoni, Piotr Indyk, and Ilya Razenshteyn. 2018. Approximate\nNearest Neighbor Search in High Dimensions. http://arxiv.org/abs/1806.09823\nNumber: arXiv:1806.09823 arXiv:1806.09823 [cs, stat].\n[5]Martin AumÃ¼ller, Erik Bernhardsson, and Alexander Faithfull. 2018. ANN-\nBenchmarks: A Benchmarking Tool for Approximate Nearest Neighbor Al-\ngorithms. arXiv:1807.05614 [cs] (July 2018). http://arxiv.org/abs/1807.05614\narXiv: 1807.05614.\n[6]Mayank Bawa, Tyson Condie, and Prasanna Ganesan. 2005. LSH forest: self-\ntuning indexes for similarity search. In Proceedings of the 14th international\nconference on World Wide Web - WWW â€™05 . ACM Press, Chiba, Japan, 651.\nhttps://doi.org/10.1145/1060745.1060840\n[7]Lawrence Cayton and Sanjoy Dasgupta. 2007. A Learning Framework for\nNearest Neighbor Search. In Proceedings of the 20th International Conference\non Neural Information Processing Systems (NIPSâ€™07) . Curran Associates Inc.,\nRed Hook, NY, USA, 233â€“240.\n[8]Sanjoy Dasgupta and Yoav Freund. 2008. Random projection trees and low\ndimensional manifolds. In Proceedings of the fortieth annual ACM symposium\non Theory of computing . ACM, Victoria British Columbia Canada, 537â€“546.\nhttps://doi.org/10.1145/1374376.1374452\n[9]Sanjoy Dasgupta and Kaushik Sinha. 2013. Randomized partition trees for\nexact nearest neighbor search. arXiv:1302.1948 [cs] (Feb. 2013). http://arxiv.\norg/abs/1302.1948 arXiv: 1302.1948.\n[10] Sanjoy Dasgupta, Charles F. Stevens, and Saket Navlakha. 2017. A neural\nalgorithm for a fundamental computing problem. Science 358, 6364 (Nov. 2017),793â€“796. https://doi.org/10.1126/science.aam9868\n[11] Yihe Dong, Piotr Indyk, Ilya P. Razenshteyn, and Tal Wagner. 2020. Learning\nSpace Partitions for Nearest Neighbor Search. In 8th International Conference\non Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net. https://openreview.net/forum?id=rkenmREFDr\n[12] Martin Ester, Hans-Peter Kriegel, and Xiaowei Xu. 1996. A Density-Based\nAlgorithm for Discovering Clusters in Large Spatial Databases with Noise.\n(1996), 6.\n[13] Cong Fu, Chao Xiang, Changxu Wang, and Deng Cai. 2018. Fast Approxi-\nmate Nearest Neighbor Search With The Navigating Spreading-out Graph.\narXiv:1707.00143 [cs] (Dec. 2018). http://arxiv.org/abs/1707.00143 arXiv:\n1707.00143.\n[14] Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of\ntraining deep feedforward neural networks. (2010), 249â€“256.\n[15] Yunchao Gong, Svetlana Lazebnik, Albert Gordo, and Florent Perronnin. 2013.\nIterative Quantization: A Procrustean Approach to Learning Binary Codes\nfor Large-Scale Image Retrieval. IEEE Transactions on Pattern Analysis and\nMachine Intelligence 35, 12 (Dec. 2013), 2916â€“2929. https://doi.org/10.1109/\nTPAMI.2012.193\n[16] Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix\nChern, and Sanjiv Kumar. 2020. Accelerating Large-Scale Inference with\nAnisotropic Vector Quantization. https://doi.org/10.48550/arXiv.1908.10396\narXiv:1908.10396 [cs, stat].\n[17] Kiana Hajebi, Yasin Abbasi-Yadkori, Hossein Shahbazi, and Hong Zhang. 2011.\nFast Approximate Nearest-Neighbor Search with k-Nearest Neighbor Graph.\n(Jan. 2011), 7.\n[18] Ben Harwood and Tom Drummond. 2016. FANNG: Fast Approximate Nearest\nNeighbour Graphs. In 2016 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) . IEEE, Las Vegas, NV, USA, 5713â€“5722. https://doi.org/10.\n1109/CVPR.2016.616\n[19] Piotr Indyk and Rajeev Motwani. 1998. Approximate nearest neighbors:\ntowards removing the curse of dimensionality. In Proceedings of the thirtieth\nannual ACM symposium on Theory of computing - STOC â€™98 . ACM Press, Dallas,\nTexas, United States, 604â€“613. https://doi.org/10.1145/276698.276876\n[20] Sergey Ioffe and Christian Szegedy. 2015. Batch Normalization: Accelerating\nDeep Network Training by Reducing Internal Covariate Shift. arXiv:1502.03167\n[cs](March 2015). http://arxiv.org/abs/1502.03167 arXiv: 1502.03167.\n[21] Jeff Johnson, Matthijs Douze, and HervÃ© JÃ©gou. 2017. Billion-scale similarity\nsearch with GPUs. https://doi.org/10.48550/arXiv.1702.08734 arXiv:1702.08734\n[cs].\n[22] H JÃ©gou, M Douze, and C Schmid. 2011. Product Quantization for Nearest\nNeighbor Search. IEEE Transactions on Pattern Analysis and Machine Intelli-\ngence 33, 1 (Jan. 2011), 117â€“128. https://doi.org/10.1109/TPAMI.2010.57\n[23] Rong Kang, Wentao Wu, Chen Wang, Ce Zhang, and Jianmin Wang.\n2021. The Case for ML-Enhanced High-Dimensional Indexes. In\nAIDB@VLDB 2021 . https://www.microsoft.com/en-us/research/publication/\nthe-case-for-ml-enhanced-high-dimensional-indexes/\n[24] Omid Keivani and Kaushik Sinha. 2018. Improved nearest neighbor search\nusing auxiliary information and priority functions. (2018), 2573â€“2581.\n[25] Diederik P. Kingma and Jimmy Ba. 2017. Adam: A Method for Stochastic\nOptimization. arXiv:1412.6980 [cs] (Jan. 2017). http://arxiv.org/abs/1412.6980\narXiv: 1412.6980.\n\n[26] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. arXiv:1712.01208 [cs] (April 2018).\nhttp://arxiv.org/abs/1712.01208 arXiv: 1712.01208.\n[27] Neeraj Kumar, Li Zhang, and Shree Nayar. 2008. What Is a Good Nearest\nNeighbors Algorithm for Finding Similar Patches in Images? In Computer\nVision â€“ ECCV 2008 , David Forsyth, Philip Torr, and Andrew Zisserman (Eds.).\nVol. 5303. Springer Berlin Heidelberg, Berlin, Heidelberg, 364â€“378. https://doi.\norg/10.1007/978-3-540-88688-4_27 Series Title: Lecture Notes in Computer\nScience.\n[28] Zhen Li, Huazhong Ning, Liangliang Cao, Tong Zhang, Yihong Gong, and\nThomas S. Huang. 2011. Learning to Search Efficiently in High Dimensions. In\nProceedings of the 24th International Conference on Neural Information Process-\ning Systems (NIPSâ€™11) . Curran Associates Inc., Red Hook, NY, USA, 1710â€“1718.\n[29] Venice Erin Liong, Jiwen Lu, Gang Wang, Pierre Moulin, and Jie Zhou. 2015.\nDeep hashing for compact binary codes learning. In 2015 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) . IEEE, Boston, MA, USA,\n2475â€“2483. https://doi.org/10.1109/CVPR.2015.7298862\n[30] Qin Lv, William Josephson, Zhe Wang, Moses Charikar, and Kai Li. 2007.\nMulti-Probe LSH: Efficient Indexing for High-Dimensional Similarity Search.\nInProceedings of the 33rd International Conference on Very Large Data Bases\n(VLDB â€™07) . VLDB Endowment, 950â€“961.\n[31] Yu A. Malkov and D. A. Yashunin. 2018. Efficient and robust approximate\nnearest neighbor search using Hierarchical Navigable Small World graphs.\nhttps://doi.org/10.48550/arXiv.1603.09320 arXiv:1603.09320 [cs].\n[32] Yu A. Malkov and D. A. Yashunin. 2020. Efficient and Robust Approximate\nNearest Neighbor Search Using Hierarchical Navigable Small World Graphs.\nIEEE Transactions on Pattern Analysis and Machine Intelligence 42, 4 (April\n2020), 824â€“836. https://doi.org/10.1109/TPAMI.2018.2889473\n[33] Marius Muja and David Lowe. 2009. Fast Approximate Nearest Neighbors with\nAutomatic Algorithm Configuration. In Proceedings of the Fourth International\nConference on Computer Vision Theory and Applications . SciTePress - Science\nand and Technology Publications, Lisboa, Portugal, 331â€“340. https://doi.org/\n10.5220/0001787803310340\n[34] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020.\nLearning Multi-dimensional Indexes. Proceedings of the 2020 ACM SIGMOD\nInternational Conference on Management of Data (June 2020), 985â€“1000. https:\n//doi.org/10.1145/3318464.3380579 arXiv: 1912.01668.\n[35] Andrew Y. Ng, Michael I. Jordan, and Yair Weiss. 2001. On Spectral Clustering:\nAnalysis and an Algorithm. In Proceedings of the 14th International Conference\non Neural Information Processing Systems: Natural and Synthetic (NIPSâ€™01) . MIT\nPress, Cambridge, MA, USA, 849â€“856.\n[36] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gre-\ngory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga,Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Rai-\nson, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai,\nand Soumith Chintala. 2019. PyTorch: An Imperative Style, High-Performance\nDeep Learning Library. (2019), 12.\n[37] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M.\nBlondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D.\nCournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn:\nMachine Learning in Python. Journal of Machine Learning Research 12 (2011),\n2825â€“2830.\n[38] P. Ram and A. G. Gray. 2013. Which Space Partitioning Tree to Use for Search?.\nInProceedings of the 26th International Conference on Neural Information Pro-\ncessing Systems - Volume 1 (NIPSâ€™13) . Curran Associates Inc., Red Hook, NY,\nUSA, 656â€“664.\n[39] Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, and HervÃ© JÃ©gou.\n2019. Spreading vectors for similarity search. arXiv:1806.03198 [cs, stat] (Aug.\n2019). http://arxiv.org/abs/1806.03198 arXiv: 1806.03198.\n[40] Peter Sanders and Christian Schulz. 2012. Think Locally, Act Globally: Per-\nfectly Balanced Graph Partitioning. http://arxiv.org/abs/1210.0477 Number:\narXiv:1210.0477 arXiv:1210.0477 [cs].\n[41] Robert E Schapire. 2013. Explaining adaboost. In Empirical inference . Springer,\n37â€“52.\n[42] Gregory Shakhnarovich, Trevor Darrell, and Piotr Indyk. 2005. Nearest-\nNeighbor Methods in Learning and Vision: Theory and Practice: Description\nof the series - need to check with Bob Prior what it is. Theory and Practice\n(2005), 26.\n[43] Robert F. Sproull. 1991. Refinements to nearest-neighbor searching ink-\ndimensional trees. Algorithmica 6, 1-6 (June 1991), 579â€“589. https://doi.\norg/10.1007/BF01759061\n[44] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Rus-\nlan Salakhutdinov. 2014. Dropout: A Simple Way to Prevent Neural Networks\nfrom Overfitting. (2014), 30.\n[45] Jun Wang, Wei Liu, Sanjiv Kumar, and Shih-Fu Chang. 2015. Learning to\nHash for Indexing Big Data - A Survey. arXiv:1509.05472 [cs] (Sept. 2015).\nhttp://arxiv.org/abs/1509.05472 arXiv: 1509.05472.\n[46] Jingdong Wang, Heng Tao Shen, Jingkuan Song, and Jianqiu Ji. 2014. Hashing\nfor Similarity Search: A Survey. arXiv:1408.2927 [cs] (Aug. 2014). http:\n//arxiv.org/abs/1408.2927 arXiv: 1408.2927.\n[47] Xiang Wu, Ruiqi Guo, Ananda Theertha Suresh, Sanjiv Kumar, Dan Holtmann-\nRice, David Simcha, and Felix X. Yu. 2017. Multiscale Quantization for Fast\nSimilarity Search. In Proceedings of the 31st International Conference on Neural\nInformation Processing Systems (NIPSâ€™17) . Curran Associates Inc., Red Hook,\nNY, USA, 5749â€“5757.",
  "textLength": 67478
}