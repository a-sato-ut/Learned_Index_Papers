{
  "paperId": "829b4839eeded3d4c42a945060029acc9d3b3763",
  "title": "The PGM-index",
  "pdfPath": "829b4839eeded3d4c42a945060029acc9d3b3763.pdf",
  "text": "arXiv:1910.06169v1  [cs.DS]  14 Oct 2019The PGM­index: a multicriteria, compressed and learned\napproach to data indexing\nPaolo Ferragina\nUniversity of Pisa, Italy\npaolo.ferragina@unipi.itGiorgio Vinciguerra\nUniversity of Pisa, Italy\ngiorgio.vinciguerra@phd.unipi.it\nABSTRACT\nThe recent introduction of learned indexes has shaken the\nfoundations of the decades-old ﬁeld of indexing data struc-\ntures. Combining, or even replacing, classic design ele-\nments such as B-tree nodes with machine learning models\nhas proven to give outstanding improvements in the space\nfootprint and time eﬃciency of data systems. However,\nthese novel approaches are based on heuristics, thus they\nlack any guarantees both in their time and space require-\nments.\nWeproposethePiecewise GeometricModelindex(shortly,\nPGM-index), which achieves guaranteed I/O-optimality in\nquery operations, learns an optimal number of linear mod-\nels, and its peculiar recursive construction makes it a pure ly\nlearned data structure, rather than a hybrid of traditional\nand learned indexes (such as RMI and FITing-tree). We\nshowexperimentallythatthePGM-indeximprovesthespace\nof the best known learned index, i.e. FITing-tree, by 63.3%\nand of the B-tree by more than four orders of magnitude,\nwhile achieving their same or even better query time eﬃ-\nciency.\nWe complement this result by proposing three variants of\nthe PGM-index which address some key issues occurring in\nthe design of modern big data systems. First, we design\nacompressed PGM-index that further reduces its succinct\nspace footprint by exploiting the repetitiveness at the lev el\nof the learned linear models it is composed of. Second, we\ndesign a PGM-index that adapts itself to the distribution\nof the query operations, thus resulting in the ﬁrst known\ndistribution-aware learned index to date. Finally, given its\nﬂexibility in the oﬀered space-time trade-oﬀs, we propose\nthemulticriteria PGM-indexwhose specialityistoeﬃciently\nauto-tune itself in a few seconds over hundreds of millions o f\nkeys tothe possibly evolving space-time constraints impos ed\nby the application of use.\n1. INTRODUCTION\nThe ever-growing amount of information coming from the\nWeb, social networks and Internet of Things severely im-\npairs the management of available data. Advances in CPUs,\nGPUsandmemorieshardlysolvethisproblemwithoutprop-\nerlydevisedalgorithmic solutions. Hence, muchresearchh as\nbeen devoted to dealing with this enormous amount of data,\nparticularly focusing onmemoryhierarchyutilisation [2, 34],\nquery processing on streams [10], space eﬃciency [23, 24],\nparallel and distributed processing [15]. But despite thes eformidable results, we still miss proper algorithms and dat a\nstructures that are ﬂexible enough to work under computa-\ntional constraints that vary across users, devices and appl i-\ncations, and possibly evolve over time.\nIn this paper, we restrict our attention to the case of in-\ndexing data structures for internal or external memory which\nsolve the so-called fully indexable dictionary problem. This\nproblem asks to store a multiset Sof real keys in order to\neﬃciently support the query rank(x), which returns for any\npossible key xthe number of keys in Swhich are smaller\nthanx. In formula, rank(x) =|{y∈S|y < x}|. Now,\nsuppose that the keys in Sare stored in a sorted array A.\nIt is not diﬃcult to deploy the rankprimitive to implement\nthe following classic queries:\n•member(x) =trueifx∈S,falseotherwise. Just\ncheck whether A[rank(x)] =x, sinceAstores items\nfrom position 0.\n•predecessor (x) = max {y∈S|y < x}. Return A[i],\nwherei=rank(x)−1.\n•range(x,y) =S∩[x,y]. Scan from A[rank(x)] up to\nkeys smaller than or equal to y.\nMoreover, we notice thatit is easy toderivefrom member(x)\nthe implementation of the query lookup(x), which returns\nthe satellite data of x∈S(if any), nilotherwise.\nIn the following, we will use the generic expression query\noperations to refer to any of the previous kinds of pointwise\nqueries, namely: member(x),predecessor (x) andlookup(x).\nOnthecontrary,wewill beexplicitinreferringto range(x,y)\nbecause of its variable-size output.\nBackground and related work. Existing indexing data\nstructures can be grouped into: (i) hash-based, which range\nfromtraditionalhashtablestorecenttechniques,likeCuc koo\nhashing [28]; (ii) tree-based, such as B-trees and its vari-\nants [2, 34, 31]; (iii) bitmap-based [36, 7], which allow ef-\nﬁcient set operations; and (iv) trie-based, which are com-\nmonly used for string keys. Unfortunately, hash-based in-\ndexes do not support predecessor or range searches; bitmap-\nbased indexes can be expensive to store, maintain and de-\ncompress [35]; trie-based indexes are mostly pointer-base d\nand, apart from recent results [12], keys are stored uncom-\npressedthustakingspace proportional tothedictionarysi ze.\nAs a result, B-trees and their variations remain the predom-\ninant data structures in commercial database systems for\nthese kinds of queries [29].1\n1For other related work we refer the reader to [18, 13], here\nwe mention only the results which are closer to our proposal.\n1\n\nVery recently, this old-fashioned research ﬁeld has been\nshaken up by the introduction of learned indexes [18], whose\ncombination, orevenreplacement, ofclassic designelemen ts,\nsuch as B-tree nodes, with machine-learned models have\nbeen shown to achieve outstanding improvements in the\nspace footprint and time eﬃciency of all the above query\noperations. The key idea underlying these new data struc-\ntures is that indexes are modelsthat we can train to map\nkeys to their location in the array A, given by rank. This\nparallel between indexing data structures and rankfunc-\ntions does not seem a new one, in fact any of the previous\nfour families of indexes oﬀers a speciﬁc implementation of\nit. But its novelty becomes clear when we look at the keys\nk∈Sas points ( k,rank(k)) in the Cartesian plane. As\nan example, let us consider the case of a dictionary of keys\na,a+1,...,a+n−1, where ais an integer. Here, rank(k)\ncan be computed exactlyask−a(i.e. via a line of slope\n1 and intercept −a), and thus it takes constant time and\nspace to be implemented, independently of the number nof\nkeys inS. This trivial example sheds light on the potential\ncompression opportunities oﬀered by patterns and trends in\nthe data distribution. However, we cannot argue that all\ndatasets follow exactly a “linear trend”.\nIn general, we have to design Machine Learning (ML)\ntechniques that learn rankby extracting the patterns in the\ndata through succinct models, ranging from linear to more\nsophisticated ones, which admit some “errors” in the output\nof the model approximating rankand that, in turn, can be\neﬃciently corrected to return the exact value of rank. This\nway, we can reframe the implementation of rankas a ML\nproblem in which we search for the model that is fast to be\ncomputed, is succinct in space, and best approximates rank\naccording to some criteria that will be detailed below.\nThis is exactly the design goal pursued by [18] with their\nRecursive Model Index(RMI), which uses a hierarchy of ML\nmodels organised as a Directed Acyclic Graph (DAG) and\ntrained to learn the input distribution ( k,rank(k)) for all\nk∈S. At query time each model, starting from the top\none, takes the query key as input and picks the following\nmodel in the DAG that is “responsible” for that key. The\noutput of RMI is the position returned by the last queried\nML model, which is, however, an approximate position. A\nﬁnal binary search is thus executed within a range of neigh-\nbouring positions whose size depends on the prediction erro r\nof RMI.\nOne could presume that ML models cannot provide the\nguaranteesensuredbytraditionalindexes, bothbecauseth ey\ncan fail to learn the distribution and because they can be\nexpensive to evaluate [17]. Unexpectedly, it was reported\nthat RMI dominates the B+-tree, being up to 1.5–3 ×faster\nand two orders of magnitude smaller in space [18].\nThis notwithstanding, the RMI introduces another set of\nspace-time trade-oﬀs between model size and query time\nwhich are diﬃcult to control because they depend on the\ndistribution of the input data, on its DAG structure and\non the complexity of the ML models adopted. This mo-\ntivated [13] to introduce the FITing-tree which uses only\nlinear models, a B+-tree to index them, and it provides an\ninteger parameter ε≥1 controlling the size of the region\nin which the ﬁnal binary search step has to be performed.\nFigure 1 shows an example of a linear model fsapproxi-\nmating 14 keys and its use in determining the approximate\nposition of a key k= 37, which is indeed fs(k)≈7 instead30 35 40 450123458910111213\nkfs(k)[pos−ε,pos+ε]\nkeyposition\nFigure 1: Linear approximation of a multiset of integer keys\nwithin the range [27 ,46]. The encoding of the (dashed) seg-\nmentfstakes only two ﬂoats, and thus its storage is inde-\npendent of the number of “encoded” keys. The key k= 37\nis repeated three times in the multiset S, starting from po-\nsition 5 in A, butfserrs byε= 2 in predicting the position\nof its ﬁrst occurrence.\nof the correct position 5, thus making an error ε= 2. Ex-\nperiments showed that the FITing-tree improves the time\nperformance of the B+-tree with a space saving of orders of\nmagnitude [13], but this result was not compared against\nthe performance of RMI. Moreover, the computation of the\nlinear models residing in the leaves of the FITing-tree is su b-\noptimal in theory and ineﬃcient in practice. This impacts\nnegatively on its ﬁnal space occupancy (as we will quantify\nin Section 6.1) and slows down its query eﬃciency because\nof an increase in the height of the B+-tree indexing those\nlinear models.\nOur contribution. In this paper, we contribute to the de-\nsign ofoptimallinear-model learned indexes, to their com-\npression and to the automatic selection of the best learned\nindex that ﬁts the requirements (in space, latency or query\ndistribution) of an underlying application in ﬁve main step s.\n1. We design the ﬁrst learned index that solves the fully\nindexabledictionaryproblemwithtimeandspacecom-\nplexities which are provably better than classic data\nstructures for hierarchical memories, such as B-trees,\nandmodern learned indexes. Our indexis I/O-optimal\naccording to the lower bound for predecessor search in\nexternal memory proved by [30]. We call it the Piece-\nwise Geometric Model index (PGM-index) because it\nturns the indexing of a sequence of keys into the cover-\nage of a sequence of points via segments. Unlike previ-\nous work [13, 18], the PGM-index is built upon an op-\ntimal number of linear models, and its peculiar recur-\nsive construction makes it a purely learned data struc-\nture, ratherthanhybridoftraditionalandlearneddata\nstructures. This aspect allows thePGM-index to make\nthe most of the constant space-time indexing feature\noﬀered by the linear models on which it is built upon\n(see Section 2.2, Theorem 1, and Table 1).\n2. We test the experimental eﬃciency of the PGM-index\nthrough a large set of experiments over three known\ndatasets (Section 6). We show that the PGM-index\nimproves the space occupancy of the FITing-tree by\n63.3%, of the Cache-Sensitive Search tree (CSS-tree)\n2\n\nby a factor 82.7 ×, and of the B-tree by more than\nfour orders of magnitude, while achieving their same\nor even better query eﬃciency (see Figure 6). Unlike\nthe RMI, the PGM-index oﬀers theoretical bounds on\nthe query time and space occupancy, and it guaran-\ntees a 4×increase in the precision of approximating\nthe position of the searched key which, in turn, in-\nduces a uniform improvement over all possible space-\ntime trade-oﬀs achieved by RMI.\n3. We then show that the (succinct) space footprint of a\nPGM-index can be further reduced by designing novel\ncompression algorithms for the building blocks of the\nlinear models (i.e. slopes and intercepts) on which\nour index hinges upon. In particular, we provide an\neﬃcient algorithm that reduces the number of dis-\ntinct slopes to be encoded to their optimal minimum\nnumber, which is an interesting algorithmic contri-\nbution in itself. In practice, in just 80 ms this al-\ngorithm improves the space occupancy of a PGM-in-\ndex over a dataset of hundreds of million keys by up\nto 52.2%. This makes the PGM-index the ﬁrst com-\npressed learned index to date (see Section 3).\n4. We also propose the ﬁrst example of a distribution-\naware learned index, namely one that adapts itself not\nonly to the distribution of the dictionary keys but also\nto their access frequencies. The resulting distribution-\naware PGM-index achieves the query time of biased\ndata structures [1, 3, 20, 32], but with a space oc-\ncupancy that adapts to the “regularity trend” of the\ninput dataset thus beneﬁting of the succinctness of\nlearned indexes (see Section 4 and Theorem 2).\n5. Given the ﬂexibility in space-time trade-oﬀs oﬀered by\nthe PGM-index (as shown in Section 6), we ﬁnally\nstudy the concept of Multicriteria Data Structures ,\nwhich combines in a formal yet eﬀective way multi-\ncriteria optimisation with data structures. A multicri-\nteria data structure, for a given problem P, is deﬁned\nby a pair /an}⌊ra⌋k⌉tl⌉{tF,A/an}⌊ra⌋k⌉tri}htPwhereFis a family of data struc-\ntures, each one solving Pwith a proper trade-oﬀ in the\nuse of some resources (e.g. time, space, energy), and A\nis an optimisation algorithm that selects in Fthe data\nstructure that “best ﬁts” an instance of P. We demon-\nstrate the fruitfulness of this concept by introducing\nthe Multicriteria PGM-index, which hinges upon an\noptimisation algorithm designed to eﬃciently explore\nFvia a proper space-time cost model for our PGM-\nindex (Section 5). In our experiments, we show that\nthe Multicriteria PGM-index is fast, taking less than\n20 seconds, to reorganise itself to bestindex a dataset\nof 750M keys within newly given space or time bound.\nThis supports the vision of a new generation of big\ndataprocessing systems designed upondata structures\nthat can be adjusted on-the-ﬂy to the application, de-\nvice and user needs, which may possibly change over\ntime, as foreseen in [16]. In a way, the multicriteria\nPGM-index solves their ambitious research challenge\nwithin a design space in which Fconsists of variantsof PGM-index, space-time constraints change contin-\nuously, and the data structure has to be optimised as\nfast as possible.2\n2. THE PGM­index\nGiven a multiset Sofnkeys drawn from a universe U,3\nthe PGM-index is a data structure parametric in an integer\nε≥1 which solves the fully indexable dictionary problem\nintroduced in Section 1. Let Abe a sorted array storing the\n(possibly repeated) keys of S.\nThe ﬁrst ingredient of the PGM-index is a Piecewise Lin-\near Approximation model (PLA-model), namely a mapping\nbetween keys from Uand their approximate positions in the\narrayA. Speciﬁcally, we aim to learn a mapping that re-\nturns a position for a key k∈ Uwhich is at most εaway\nfrom the correct one in A. We say piecewise because one\nsingle linear model (i.e. a segment) could be insuﬃcient to\nε-approximate the positions of all the keys from U. As a\nconsequence, the PGM-index learns a sequence of segments,\neach one taking constant space (two ﬂoats and one key) and\nconstant query time to return the ε-approximate position of\nkinA. We show below in Lemma 1 that there exists a lin-\near time and space algorithm which computes the optimal\nPLA-model, namely one that consists of the minimum num-\nber ofε-approximate segments. We also observe that the ε-\napproximate positions returned by the optimal PLA-model\ncan be turned into exactpositions via a binary search within\na range of ±εkeys inA, thus taking time logarithmic in the\nparameter ε, not in the size of A.\nThe second ingredient of the PGM-index is a recursive\nalgorithm which adapts the index structure to the distribu-\ntion of the input keys, thus resulting as much independent\nas possible from their number (see Figure 2 for pictorial ex-\nample). More precisely, in order to make the most of the\nability of a single segment to index in constant space and\ntime an arbitrarily long range of keys, we turn the optimal\nPLA-model built over the array Ainto a set of keys, and we\nproceed recursively by building another optimal PLA-model\nover these keys. This process continues until one single seg -\nment is obtained, which will form the root of our data struc-\nture. Overall, each PLA-model forms a level of the PGM-\nindex, and each segment of that PLA-model forms a node\nof the data structure at that level. The speciality of this\nrecursive construction with respect to known learned index\nproposals (cf. FITing-tree or RMI) is that the PGM-index\nis apure learned index which does not hinge on classic data\nstructures either in its structure (as in the FITing-tree) o r\nas a fallback when the ML models err too much (as in RMI).\nThe net result are three main advantages in its space-time\ncomplexity. First, the PGM-index is built upon the mini-\nmum number of segments, while other learned indexes, such\nas FITing-tree and RMI, compute a sub-optimal number\nof segments with a subsequent penalisation in their time\nand space eﬃciency. Second, the PGM-index uses these\nsegments as constant-space routing tables at all levels of\n2For completeness, we remark that the concept of multicri-\nteria optimisation has been already applied in Algorithmic s\nto data compression [11] and software auto-tuning [22].\n3The universe Uis a range of reals because of the arith-\nmetic operations required by the linear models. Our solu-\ntion works for any kind of keys that can be mapped to reals\nby preserving their order. Examples include integer keys,\nstring keys, etc.\n3\n\nA\n[pos−ε, pos+ε]levels[2]k\n0 1 2 3 4 5 6s0s1s2s3s4s5s6\ns3= (keyi3,slope3,intercept3)levels[1]s7s8s9s10 levels[0]\npos=fs3(k)\nFigure 2: Each segment in a PGM-index is “responsible” for ro uting the queried key to one of the segments of the level\nbelow. In the picture, the dotted lines show that the root seg ments10routes the queried key to one of the segments among\n{s7,s8,s9}(which together cover the same range of keys as s10), whereas s8routes the queried key to either s3ors4(which\ntogether cover the same range of keys as s8). Segments at the last level (i.e. levels[2]) areε-approximate segments for the\nsub-range of keys in Adepicted by wavy lines of which they are responsible for.\nthe data structure, while other indexes (e.g. FITing-tree,\nB-tree and variants) use space-consuming nodes storing a\nlarge number of keys which depends on the disk-page size\nonly, thus resulting blind to the possible regularity prese nt\nin the data distribution. Third, these routing tables of the\nPGM-index take constant time to restrict the search of a\nkey in a node to a smaller subset of the indexed keys (of size\nε), whereas nodes in the B+-tree and the FITing-tree incur\na search cost that grows with the node size, thus slowing the\ntree traversal during the query operations.\nThe following two subsections will detail the two main\ningredients of the PGM-index described above.\n2.1 The optimal PLA­model\nLet us be given a sorted array A= [k0,k1,...,k n−1] ofn\nreal and possibly repeated keys drawn from a universe U. In\nthis section, we describe how an ε-approximate implemen-\ntation of the mapping rankfrom keys to positions in Acan\nbe eﬃciently computed and succinctly stored via an optimal\nnumberof segments, which is one of the core design elements\nof a PGM-index. In the next section, we will comment on\nthe recursive construction of the whole PGM-index and the\nimplementation of the query operations.\nA segment sis a triple ( k,slope,intercept ) that indexes\na range of Uthrough the function fs(k) =k×slope+\nintercept , as depicted in Figure 1. An important character-\nistic of the PGM-index is the “precision” εof its segments.\nDeﬁnition 1. LetAbe a sorted array of nkeys drawn\nfrom auniverse Uandletε≥1beaninteger. Asegment s=\n(k,slope,intercept ) is said to provide an ε-approximate in-\ndexingoftherangeofall keysin[ ki,ki+r], forsome ki,ki+r∈\nA, if|fs(x)−rank(x)| ≤εfor allx∈ Usuch that ki≤x≤\nki+r.\nAnε-approximate segment can be seen as an approxi-\nmate predecessor search data structure for its covered rang eof keys oﬀering constant query time and constant occu-\npied space. One single segment, however, could be insuf-\nﬁcient to ε-approximate the rankfunction over the whole\nU; hence, we look at the computation of a sequence of seg-\nments, also termed Piecewise Linear Approximation model\n(PLA-model).\nDeﬁnition 2. Givenε≥1, thepiecewise linear ε-approxi-\nmation problem consists ofcomputingthe PLA-model which\nminimises the number of its segments {s0,...,s m−1}, pro-\nvided that each segment sjisε-approximate for its covered\nrange of keys, these ranges are disjoint and together cover\nthe entire universe U.\nA way to ﬁnd the optimal PLA-model for an array Ais\nby dynamic programming, but the O(n3) time it requires\nis prohibitive. The authors of the FITing-tree [13] attacke d\nthis problem via a heuristic approach, called shrinking con e,\nwhich is linear in time but does not guarantee to ﬁnd the op-\ntimal PLA-model, and indeed it performs poorly in practice\n(as we will show in Section 6.1).\nInterestingly enough, we found that this problem has been\nextensivelystudiedforlossycompression andsimilarity s earch\nof time series (see e.g. [27, 5, 8, 9, 37] and refs therein),\nand it admits streaming algorithms which take O(n) opti-\nmal time. The key idea of this family of approaches is to\nreduce the piecewise linear ε-approximation problem to the\none of constructing convex hulls of a set of points, which in\nour case is the set {(ki,rank(ki))}grown incrementally for\ni= 0,...,n−1. As long as the convex hull can be enclosed\nin a (possibly rotated) rectangle of height no more than 2 ε,\nthe index iis incremented and the set is extended. As soon\nas the rectangle enclosing the convex hull is higher than 2 ε,\nwe compute one segment of the PLA-model by taking the\nline which splits that rectangle into two equal-sized halve s.\nThen, the current set of processed elements is emptied and\n4\n\nthealgorithm restarts from therest oftheinputpoints. Thi s\ngreedy approach can be proved to be optimal and to have\nlinear time and space complexity. We can rephrase this re-\nsult in our context as follows.\nLemma 1 (Optimal PLA-model [27]). Given a sequence\n{(xi,yi)}i=0,...,n−1of points that are nondecreasing in their\nx-coordinate. There exists a streaming algorithm that in lin -\near time and space computes the minimum number of seg-\nments that ε-approximate the y-coordinate of each point in\nthat set.\nFor our application to the dictionary problem, the xis\nof Lemma 1 correspond to the input keys ki, and the yis\ncorrespond to their positions 0 ,...,n−1 in the sorted input\narrayA. Therefore, Lemma 1 provides an algorithm which\ncomputes in linear time and space the optimal PLA-model\nfor the keys stored in A.\nThe next step is to prove a simple but very useful bound\non the number of keys covered by a segment of the opti-\nmal PLA-model, which we will deploy in the analysis of the\nPGM-index.\nLemma 2.Given an ordered sequence of keys ki∈ Uand\nthe corresponding sequence {(ki,i)}i=0,...,n−1of points in the\nCartesian plane that are nondecreasing in both their coordi -\nnates. The algorithm of Lemma 1 determines a (minimum)\nnumbermoptof segments which cover at least 2εpoints each,\nso thatmopt≤n/(2ε).\nProof. For any chunk of 2 εconsecutive keys ki,ki+1,\n...,ki+2ε−1, let us take the horizontal segment y=i+\nε. It is easy to see that those keys generate the points\n(ki,i),(ki+1,i+1),...,(ki+2ε−1,i+2ε−1) and each of these\nkeys have y-distance at most εfrom that line, which is then\nanε-approximate segment for that range of 2 ε-keys. Hence,\nany segment of the optimal PLA-model covers at least 2 ε\nkeys.\n2.2 Indexing the PLA­model\nThealgorithm ofLemma 1returnsanoptimal PLA-model\nfor the input array Aas a sequence M= [s0,...,s m−1] of\nmsegments.4Now, in order to solve the fully indexable dic-\ntionary problem, we need a way to ﬁnd the ε-approximate\nsegment sj= (kij,slopej,interceptj) responsible for esti-\nmating the approximate position posof a query key k, i.e.\nthis is the rightmost segment sjsuch that kij≤k. Whenm\nis large, we could perform a binary search on the sequence\nM, or we could index it via a proper data structure, such\nas a multiway search tree (as done in the FITing-tree). In\nthis case, the membership query could then be answered in\nthree steps. First, the multiway search tree is queried to ﬁn d\nthe rightmost segment sjsuch that kij≤k. Second, that\nsegment sjis used to estimate the position pos=fsj(k)\nfor the query key k. Third, the exact position of kis deter-\nmined via a binary search within A[pos−ε,pos+ε]. The net\nconsequence is that a query over this data structure would\ntakeO(logBm+ logε) time, where Bis the fan-out of the\nmultiway tree and εis the error incurred by sjwhen ap-\nproximating rank(k).\nHowever, the indexing strategy above does not take full\nadvantage of thekey distribution because it resorts to acla s-\nsic data structure with ﬁxed fan-out to index M. Therefore,\n4To simplify the notation, we write minstead of mopt.we introduce a novel strategy which consists of repeating\nthe piecewise linear approximation process recursively on a\nset of keys derived from the sequence of segments. More\nprecisely, we start with the sequence Mconstructed over\nthe whole input array A, then we extract the ﬁrst key of A\ncovered by each segment and ﬁnally construct another opti-\nmal PLA-model over this reduced set of keys. We proceed\nin this recursive way until the PLA-model consists of one\nsegment.\nIf we map segments to nodes, then this approach con-\nstructs a sort of multiway search tree but with three main\nadvantages with respect to B-trees (and thus FITing-trees) :\n(i) its nodes have variable fan-out driven by the (typically\nlarge) number of keys covered by the segments associated\nwith those nodes; (ii) the segment in a node plays the role\nof a constant-space and constant-time ε-approximate rout-\ning table for the various queries to be supported; (iii) the\nsearch in each node corrects the ε-approximate position re-\nturned by that routing table via a binary search (see next),\nand thus it has a time cost that depends logarithmically on\nε, and hence it is independent of the number of keys covered\nby the corresponding segment.\nNow, a query operation over this Recursive PGM-index\nworks as follows. Ateverylevel, it usesthe segmentreferri ng\nto the visited node to estimate the position of the searched\nkeykamong the keys of the lower level.5The real position\nis then found by a binary search in a range of size 2 εcentred\naround the estimated position. Given that every key on the\nnext level is the ﬁrst key covered by a node on that level,\nwe have identiﬁed the next node to visit, and thus the next\nsegment to query, and the process continues until the last\nlevel is reached. Anexample of aqueryoperation is depicted\nin Figure 2.\nTheorem 1.LetAbe an ordered array of nkeys from\na universe U, andε≥1be a ﬁxed integer parameter. The\nRecursive PGM-index with parameter εindexes the array A\ntakingΘ(m)space and answers rank, membership and pre-\ndecessor queries in O(logm)time and O((logcm)log(ε/B))\nI/Os, where mis the minimum number of ε-approximate\nsegments covering A,c≥2εdenotes the variable fan-out of\nthe data structure, and Bis the block size of the External\nMemory model. Range queries are answered in extra (opti-\nmal)O(K)time and O(K/B)I/Os, where Kis the number\nof keys satisfying the range query.\nProof. Each step of the recursion reduces the number of\nsegments by a variable factor cwhich is nonetheless at least\n2εbecause of Lemma 2. The number of levels is, there-\nfore,L=O(logcm), and the total space required by the\nindex is/summationtextL\nℓ=0m/(2ε)ℓ= Θ(m). For the rank, membership\nand predecessor queries, the bounds on the running time\nand the I/O complexity follow easily by observing that a\nquery performs Lbinary searches over intervals having size\nat most 2 ε. In the case of range queries, we report the K\nkeys by scanning Afrom the position returned by the rank\nquery.\nThe main novelty of the PGM-index is that its space over-\nhead does not grow linearly with n, as in the traditional\n5To correctly approximate the position of a key kfalling\nbetween the last key covered by a segment sjand the ﬁrst\nkey covered by sj+1, we compute min {fsj(k),fsj+1(kij+1)}.\n5\n\nData structure SpaceRAM model\nworst case timeEM model\nworst case I/OsEM model\nbest case I/Os\nPlain sorted array O(1) O(logn) O(logn\nB) O(logn\nB)\nMultiway tree (e.g. B-tree) Θ( n) O(logn) O(logBn) O(logBn)\nFITing-tree Θ( mgreedy)O(logmgreedy+logε)O(logBmgreedy) O(logBmgreedy)\nRecursive PGM-index Θ( mopt)O(logmopt+logε)O(logcmopt)\nc≥2ε= Ω(B)O(1)\nTable 1: The Recursive PGM-index improves the time, I/O and s pace complexity of the query operations of traditional\nexternal memory indexes (e.g. B-tree) and learned indexes ( i.e. FITing-tree). The integer ε≥1 denotes the error we\nguarantee in approximating the positions of the input keys. We denote with moptthe minimum number of ε-approximate\nsegments, computed by Lemma 1, and with mgreedythe number of ε-approximate segments computed by the greedy algorithm\nat the core of the FITing-tree. Of course, mopt≤mgreedy. The learned index RMI is not included in the table because it lacks\nof guaranteed bounds.\nindexes mentioned in Section 1, but it depends on the “reg-\nularity trend”of theinput array Awhich also decreases with\nthe value of ε. Because of Lemma 2, the numberof segments\natthelast levelofaPGM-indexcannotbemorethan n/(2ε),\nso thatm < nsinceε≥1. Since this fact holds for the re-\ncursive levels too, the PGM-index cannot be asymptotically\nworse in space and time than a 2 ε-way tree, such as a FIT-\ning-tree, B+-tree or CSS-tree (just take c= 2ε= Θ(B) in\nTheorem 1). According to the lower bound proved by [30],\nwe can state that the PGM-index solves I/O-optimally the\nfully indexable dictionary problem with predecessor searc h,\nmeaning that it can potentially replace any existing index\nwith virtually no performance degradation.\nTable 1 summarises these bounds for the PGM-index and\nits competitors both in the RAM model and in the External\nMemory (EM) model for the rank query and its derivatives:\ni.e. predecessor, membership and lookup.\nThe thorough experimental results of Section 6 will sup-\nport further these theoretical achievements by showing tha t\nthe PGM-index is much faster and succinct than FITing-\ntree, B+-tree and CSS-tree because, in practice, it will be\nmopt≪nandc≫2ε.\n3. THE COMPRESSED PGM­index\nCompressingthePGM-indexboils downtoprovidingproper\nlossless compressors for the keys and the segments (i.e. in-\ntercepts and slopes) which constitute the building blocks\nof our learned data structure. In this section, we propose\ntechniquesspeciﬁcally tailored tothecompression ofthes eg-\nments, since the compression of the inputkeysis an orthogo-\nnal problem for which there exist a plethora of solutions (se e\ne.g. [21, 23] for integer keys, and [6, 19] for ﬂoating-point\nkeys).\nFor what concerns the compression of the intercepts, they\ncan be made increasing by using the coordinate system of\nthe segments, i.e. the one that computes the position of\nan element kasfsj(k) = (k−kij)×slopej+interceptj.\nThen, since the result of fsj(k) has tobe truncated toreturn\nan integer position in A, we store the intercepts as integers\n⌊interceptj⌋.6Finally, we exploit the fact that intercepts are\nincreasing integers smaller than nand thus use the succinct\ndata structure of [25] to obtain the following result.\nProposition 1.Letmbe the number of segments of a\nPGM-index indexing nkeys drawn from a universe U. The\n6Note that this transformation increases εby 1.intercepts of those segments can be stored using mlog(n/m)+\n1.92m+o(m)bits and be randomly accessed in O(1)time.\nThe compression of slopes is more involved, and we need\nto design a speciﬁc novel compression technique. The start-\ning observation is that the algorithm of Lemma 1 computes\nnotjustasinglesegmentbutawholefamily of ε-approximate\nsegmentswhose slopes identifytoanintervalofreals. Spec if-\nically, let us suppose that these slope intervals are I0=\n(a0,b0),...,I m−1= (am−1,bm−1), where each original slope\nslopejbelongs to Ijforj= 0, ...,m−1. Our goal is to\nreduce the entropy of the set of slopes by reducing their dis-\ntinct number from mtot. This will allow us to change the\nencoding of mﬂoats into the encoding tﬂoats plus mshort\nintegers, with the hope that t≪mas we will indeed show\nexperimentally in Section 6.1.\nWe achieve this goal by designing an algorithm that takes\nO(mlogm)time todetermine the minimum number tof dis-\ntinct slopes such that each interval Ijincludes one of them.\nGiven this result, we create a table Twhich stores the dis-\ntinct slopes (as tﬂoating-point numbers of 64 bits each) and\nthen change every initial slopej∈Ijinto one of the new t\ndistinctslopes, say slope′\nj, whichis still guaranteed tobelong\ntoIjand can be encoded in ⌈logt⌉bits.\nLet us now describe the algorithm to determine the mini-\nmumnumber tofdistinct slopes for theinitial slope intervals\nwhich are assumed to be ( a0,b0),...,(am−1,bm−1). First,\nwe sort lexicographically the slope intervals to obtain an a r-\nrayIin which overlapping intervals are consecutive. We\nassume that every pair keeps as satellite information the\nindex of the corresponding interval, namely jfor (aj,bj).\nThen, we scan Ito determine the maximal preﬁx of in-\ntervals in Ithat intersect each other. As an example, say\nthe sorted slope intervals are {(2,7),(3,6),(4,8),(7,9),...},\nthen the ﬁrst maximal sequence of intersecting intervals is\n{(2,7),(3,6),(4,8)}because they intersect each other but\nthe fourth interval (7 ,9) does not intersect the second inter-\nval (3,6) and thus it is not included.\nLet (l,r) be the intersection of all the intervals in the\ncurrent maximal preﬁx of I: this is (4 ,6) in the running\nexample. Then, any slope in ( l,r) is anε-approximate slope\nfor everyone of the intervals in that preﬁx of I. Therefore,\nwe choose one real in ( l,r) and assign it as the slope of\neach of those segments in that maximal preﬁx. The process\nthen continues by determining the maximal preﬁx of the\nremaining intervals, until the overall sequence Iis processed\n(details and optimally proof in the full version of the paper ).\n6\n\nLemma 3.Letmbe the number of ε-approximate seg-\nments of a PGM-index indexing nkeys drawn from a uni-\nverseU. There exists a lossless compressor for the segments\nwhich computes the minimum number of distinct slopes t≤\nmwhile preserving the ε-guarantee. The algorithm takes\nO(mlogm)time and compresses the slopes into 64t+m⌈logt⌉\nbits of space.\nProof. Thecompressed spaceoccupancyofthe tdistinct\nslopes in Tis, assuming double-precision ﬂoats, 64 tbits.\nThe new slopes slope′\njare still min their overall number,\nbut each of them can be encoded as the position 0 ,...,t−1\nintoTof its corresponding double-precision ﬂoat.\nAn interesting future work is to experiment how much\nthe use of universal coders for reals [19], as an alternative\nto ﬂoating-point numbers, can further reduce the additive\nterm 64t.\n4. THE DISTRIBUTION­A WARE PGM­index\nThe PGM-index of Theorem 1implicitly assumes that the\nqueries are uniformly distributed, but this seldom happens\nin practice. For example, queries in search engines are very\nwell known to follow skewed distributions such as the Zipf’s\nlaw [36]. In such cases, it is desirable to have an index that\nanswers the most frequent queries faster than the rare ones,\nso to achieve a higher query throughput. Previous work\nexploited query distribution in the design of binary trees [ 3],\nTreaps [32], and skip lists [1], to mention a few.\nIn this section, we introduce an orthogonal approach that\nbuilds upon the PGM-index by proposing a variant that\nadapts itself not only to the distribution of the input keys\nbut also to the distribution of the queries. This turns out to\nbetheﬁrst distribution-aware learned index todate, withthe\nadditional positive feature of being very succinct in space .\nFormallyspeaking, givenasequence S={(ki,pi)}i=1,...,n,\nwherepiis the probability to query the key ki(that is as-\nsumedtobeknown), we wanttosolve thedistribution-aware\ndictionary problem, which asks for a data structure that\nsearches for a key kiin timeO(log(1/pi)) so that the av-\nerage query time coincides with the entropy of the query\ndistribution H=/summationtext\ni=1,...,npilog(1/pi).\nWe note that the algorithm of Lemma 1 can be modi-\nﬁed so that, given a y-range for each one of npoints in the\nplane, ﬁndsalso theset ofall (segment)directions thatint er-\nsect those ranges in O(n) time (see [27]). This corresponds\nto ﬁnd the optimal PLA-model whose individual segments\nguarantee an approximation which is within the y-range\ngiven for each of those points. Therefore, our key idea is to\ndeﬁne, for every key ki, ay-range of size yi= min{1/pi,ε},\nand thenapplythe algorithm of Lemma 1on that set ofkeys\nandy-ranges. Clearly, for the keys whose y-range is εwe can\nuse Theorem 1 and derive the same space bound of O(m);\nwhereas for the keys whose y-range is 1 /pi< εwe observe\nthat these keysare nomore than ε(in fact, the pis sumup to\n1), but they are possible spread among all position in Aand\nthus theyinducein the worst case 2 εextrasegments. There-\nfore, the total space occupancy of the bottom level of the\nindex is Θ( m+ε), where mis the one deﬁned in Theorem 1.\nNow, let us assume that the search for a key kiarrived at\nthe last level of this Distribution-Aware PGM-index, and\nthus we know in which segment to search for ki: the ﬁnal\nbinary search step within the ε-approximate range returnedby that segment takes O(logmin{1/pi,ε}) =O(log(1/pi))\nas we aimed for.\nWe are left with showing how to ﬁnd that segment in\na distribution-aware manner. We proceed similarly to the\nRecursive PGM-index but with a careful design of the recur-\nsive step because of the probabilities (and thus the variabl e\ny-ranges) assigned to the recursively deﬁned set of keys.\nLet us consider the segment covering the sequence S[a,b]=\n{(ka,pa),...,(kb,pb)}, denote by qa,b= max i∈[a,b]pithe\nmaximumprobabilityofakeyin S[a,b], andbyPa,b=/summationtextb\ni=api\nthe cumulative probability of all keys in S[a,b](which is in-\ndeed the probability to end up in that segment when search-\ning for one of its keys). We create the new set S′={...,\n(ka,qa,b/Pa,b),...}formed by the ﬁrst key kacovered by\neach segment (as in the recursive PGM-index) and setting\nits associated probability to qa,b/Pa,b. Then, we construct\nthe next upper level of the Distribution-Aware PGM-index\nby applying the algorithm of Lemma 1 on S′. If we iterate\nthe above analysis for this new level of weighted segments,\nwe conclude that: if we know from the search executed on\nthe levels above that ki∈S[a,b], the time cost to search for\nkiin this level is O(logmin{Pa,b/qa,b,ε}) =O(log(Pa,b/pi)).\nLet us repeat this argument for another upper level in\norder to understand the inﬂuence on the search time com-\nplexity. We denote the segment that covers the range of\nkeys which include kiwithS[a′,b′]⊃S[a,b], the cumulative\nprobability with Pa′,b′, and thus assign to its ﬁrst key ka′\nthe probability r/Pa′,b′, whereris the maximum probabil-\nity of the form Pa,bof the ranges included in [ a′,b′]. In\nother words, if [ a′,b′] is partitioned into {z1,...,z c}, then\nr= max i∈[1,c)Pzi,zi+1. Reasoning as done previously, if\nwe know from the search executed on the levels above that\nki∈S[a′,b′], the time cost to search for kiin this level is\nO(logmin {Pa′,b′/r,ε}) =O(log(Pa′,b′/Pa,b)) because [ a,b]\nis, by deﬁnition, one of these ranges in which [ a′,b′] is par-\ntitioned.\nRepeating this design until one single segment is obtained\n(whose cumulative probability is one), we get a total time\ncost for the search in all levels of the PGM-index equal to\na sum of logarithms whose arguments “cancel out” and get\nO(log(1/pi)).\nAs far as the space bound is concerned, we recall that\nthe number of levels in the PGM-index is L=O(logcm)\nwithc≥2ε, and that we have to account for the εextra\nsegments per level returned by the algorithm of Lemma 1.\nConsequently, this distribution-aware variant of the PGM-\nindex takes O(m+Lε) space. This space bound is indeed\nO(m) because εis a constant parameter (see Section 6).\nTheorem 2.LetAbe an ordered array of nkeyskidrawn\nfrom a universe U, which are queried with (known) proba-\nbilitypi, and let ε≥1be a ﬁxed integer parameter. The\nDistribution-Aware Recursive PGM-index with parameter ε\nindexes the array AinO(m)space and answers queries in\nO(H)average time, where His the entropy of the query dis-\ntribution, and mis the number of segments of the optimal\nPLA-model for the keys in Awith error ε.\n7\n\n5. THE MULTICRITERIA PGM­index\nTuning a data structure to match the application’s needs\nis often a diﬃcult and error-prone task for a software engi-\nneer, not to mention that these needs may change over time\ndue to mutations in data distribution, devices, resource re -\nquirements, and so on. The typical approach is a grid search\non the various instances of the data structure to be tuned\nuntil the one that matches the application’s needs is found.\nHowever, the data structure may be not ﬂexible enough to\nadapt to those changes, or the search space can be so huge\nthat the reorganisation of the data structure takes too much\ntime.\nIn the rest of this section, we exploit the space-time ﬂexi-\nbility of the PGM-index by showing that this tuning process\ncan be eﬃciently automated over this data structure via an\noptimisation strategy that: (i) given a space constraint ou t-\nputs the PGM-index that minimises its query time; or sym-\nmetrically, (ii) given a maximum query time outputs the\nPGM-index that minimises its space footprint.\nThe time-minimisationproblem. AccordingtoTheorem 1,\nthe query time of a Recursive PGM-index can be described\nast(ε) =c(log2εm)log(2ε/B), where Bis the page size of\nthe EM model, mis the number of segments in the last level,\nandcdepends on the access latency of the memory. For the\nspace, we introduce sℓ(ε), which denotes the number of seg-\nments needed to have precision εover the keys available at\nlevelℓof the Recursive PGM-index, and compute the over-\nall number of segments as s(ε) =/summationtextL\nℓ=1sℓ(ε). By Lemma 2,\nwe know that sL(ε) =m≤n/(2ε) for any ε≥1 and\nthatsℓ−1(ε)≤sℓ(ε)/(2ε). So that s(ε)≤/summationtextL\nℓ=0m/(2ε)ℓ=\n(2εm−1)/(2ε−1).\nGiven a space bound smax, the “time-minimisation prob-\nlem” consists of minimising t(ε) subject to s(ε)≤smax.7\nThe greatest challenge here is that we do not have a closed\nformula for s(ε), but only an upper bound which does not\ndepend on the underlying dataset as s(ε) does. Section 6\nwill show that in practice we can model m=sL(ε) with a\nsimple power-law having the form aε−b, whose parameters\naandbwill be properly estimated on the dataset at hand.\nThe power-law covers both the pessimistic case of Lemma 2\nand the best case in which the dataset is strictly linear.\nClearly, the space occupancy decreases with increasing ε,\nwhereas the query time t(ε) increases with ε, since the num-\nber of keys on which it is executed a binary search at each\nlevel equals 2 ε. Therefore, the time-minimisation problem\nreduces to the problem of ﬁnding the value of εfor which\ns(ε) =smaxbecause it is the lowest εthat we can aﬀord.\nSuch value of εcould be found by a binary search in the\nbounded interval E= [B/2,n/2] which is derived by requir-\ning that each model errs at least a page size (i.e. 2 ε≥B),\nsince lower εvalues do not save I/Os, and by observing that\none model is the minimum possible space (i.e. 2 ε≤n, by\nLemma 2). However, provided that our power-law approxi-\nmation holds, we can speed up the search of that ”optimal”\nεby guessing the next value of εrather than taking the mid-\npoint of the current search interval. In fact, we can ﬁnd the\nroot ofs(ε)−smax, i.e. the value εgfor which s(εg) =smax.\nWe emphasise that such εgmay not be the solution of our\n7For simplicity, we assume that a disk page contains ex-\nactlyBkeys. This assumption can be relaxed by putting\nthe proper machine- and application-dependent constants i n\nfront oft(ε) ands(ε).problem, as it may be the case that the approximation or\nthe ﬁtting of s(ε) by means of a power-law is not precise.\nThus, more iterations of the search may be needed to ﬁnd\nthe optimum εvalue; anyway, we guarantee to be always\nfaster than a binary search by gradually switching to it.\nPrecisely, we bias the guess εgtowards the midpoint εmof\nthe current search range via a simple convex combination of\nthe two [14].\nThe space-minimisation problem. Given a time bound\ntmax, the space-minimisation problem consists of minimis-\nings(ε) subject to t(ε)≤tmax. As for the problem above,\nwe could perform a binary search inside the interval E=\n[B/2,n/2] and look for the maximum εwhich satisﬁes that\ntime constraint. Instead, we speed up this process by guess-\ning the next iterate for εvia the equation t(ε) =tmax, that\nis solving c(log2εsL(ε))log(2ε/B) =tmax, in which sL(ε) is\nreplaced by the power-law approximation aε−b, for proper\naandb, andcis replaced by the measured memory latency\nof the given machine.\nAlthough eﬀective, this approach raises a subtle issue,\nnamely, the time model could not be a correct estimate of\nthe actual query time because of hardware-dependent fac-\ntors such as the presence of several caches and the CPU\npre-fetching. To further complicate this issue, we note tha t\nboths(ε) andt(ε) depend on the power-law approximation\naε−b.\nFor these reasons, instead of using the time model t(ε)\nto steer the search, we measure and use the actual aver-\nage query time t(ε) of the PGM-index over a ﬁxed batch\nof random queries. Moreover, instead of performing a bi-\nnary search inside the whole E, we run an exponential search\nstartingfromthesolutionofthedominatingterm clog(2ε/B) =\ntmax, i.e. the cost of searching the data. Eventually, we\nstop the search of the best εas soon as the searched range\nis smaller than a given threshold because t(ε) is subject\nto measurement errors (e.g. due to an unpredictable CPU\nscheduler).\n6. EXPERIMENTS\nWe experimented with an implementation in C ++of the\nPGM-index on a machine with a 2.3 GHz Intel Xeon Gold\nand192GiBmemory.8Weusedthefollowing threestandard\ndatasets, each having diﬀerent data distributions, regula ri-\nties and patterns:\n•Web logs [18, 13] contains timestamps of about 715M\nrequests to a web server;\n•Longitude [26]containslongitudesofabout166M points-\nof-interest from OpenStreetMap;\n•IoT[18, 13] contains timestamps of about 26M events\nrecorded by IoT sensors installed throughout an aca-\ndemic building.\nWe also generated some synthetic datasets according to\nthe uniform distribution in the interval [0 ,u), to the Zipf\ndistribution with exponent s, and to the lognormal distri-\nbution with standard deviation σ, they will allow to test\nthoroughly the various indexes.\n8The implementation will be released on GitHub with the\nacceptance of this paper.\n8\n\nDatasetε\n8 32 128 512 2048\nUniform u= 22233.8 59.4 66.5 68.6 68.8\nUniform u= 23265.8 68.3 68.9 69.4 68.7\nZipfs= 1 47.9 59.0 62.8 44.7 29.0\nZipfs= 2 45.3 40.2 24.2 20.8 21.6\nLognormal σ= 0.5 66.1 68.5 68.8 62.1 35.6\nLognormal σ= 1.0 66.1 68.4 69.0 61.9 34.5\nTable 2: Space savings of PGM-index with respect to a FIT-\ning-tree for a varying εon six synthetic datasets of 1G el-\nements generated according to the speciﬁed distributions.\nThe PGM-index saved from 20.8% to 69.4%.\nεSpace saving (%)\n23242526272829210211405060Web logs\nLongitude\nIoT\nFigure 3: The PGM-index saved from 37.7% to 63.3% space\nwith respect to a FITing-tree over the three real-world\ndatasets. The construction time complexity of the two ap-\nproaches is the same in theory (i.e. linear in the number of\nprocessed keys) and in practice (a couple of seconds, up to\nhundreds of millions of keys).\n6.1 Space occupancy of the PGM­index\nIn this set of experiments, we estimated the size of the\noptimal PLA-model (see Section 2.1) returned by our imple-\nmentation of [37], which provides the segments stored in the\nbottom level of the PGM-index, and compared it against the\nnon-optimal PLA-model computed with the greedy shrink-\ningcone algorithm [13, 33]usedin theFITing-tree [13]. Thi s\ncomparison is important because the size of a PLA-model is\nthe main factor impacting the space footprint of a learned\nindex based on linear models.\nTable 2 shows that on synthetic datasets of 109keys the\nimprovements (i.e. relative change in the number of seg-\nments) ranged from 20.8% to 69.4%. Figure 3 conﬁrms\nthese trends also for real-world datasets, on which the im-\nprovements ranged from 37.7% to 63.3%. For completeness,\nwe report that the optimal algorithm with ε= 8 built a\nPLA-model for Web logs in 2.59 seconds, whereas it took\nless than 1 second for Longitude andIoTdatasets. This\nmeans that the optimal algorithm of Section 2.1 can scale\nto even larger datasets.\nSince it appears diﬃcult to prove a mathematical rela-\ntionship between the number of input keys and the num-2324252627282921021110−510−410−310−210−1\nεRatiom/nWeb logs\nLongitude\nIoT\nFigure 4: A log-log plot with the ratio between the number\nof segments m, stored in the last level of a PGM-index, and\nthe sizenof the real-world datasets as a function of ε. For\ncomparison, the plot shows with a dashed line the function\n1/(2ε) which is the fraction of the number of keys stored in\nthe level above the input data of B+-tree with B= 2ε(see\ntext). Note that mis 2–5 orders of magnitude less than n.\nber ofε-approximate segments (other than the rather loose\nbound we proved in Lemma 2), we pursued an empirical in-\nvestigation on this relation because it quantiﬁes the space\nimprovement of learned indexes with respect to classic in-\ndexes. Figure 4 shows that, even when εis as little as 8, the\nnumber mof segments is at least two orders of magnitude\nsmaller than the original datasets size n. This reduction\ngets impressively evident for larger values of ε, reaching ﬁve\norders of magnitude.\n6.2 Query performance of the PGM­index\nWe evaluated the query performance of the PGM-index\nand other indexing data structures on Web logs dataset, the\nbiggest and most complex dataset available to us. We have\ndropped the comparison against the FITing-tree, because of\nthe evident structural superiority of the Recursive PGM-in -\ndex and its indexing of the optimal (minimum) number of\nsegments in the bottom level (see Figure 4). Nonetheless,\nwe will investigate the performance of some variants of the\nPGM-index that will provide a clear picture of the improve-\nments determined by its recursive indexing, compared to\nthe classic approaches based on multiway search trees (` a la\nFITing-tree), CSS-tree [31] or B+-tree.\nIn this experiment, the dataset was loaded in memory as\na contiguous array of integers represented with 8 bytes and\nwith 128 bytes payload. Slopes and intercepts were stored\nas double-precision ﬂoats. Each index was presented with\n10M queries randomly generated on the ﬂy. The next three\nparagraphs present, respectively, the query performance o f\nthe three indexing strategies for the PGM-index, a compar-\nison between the PGM-index and traditional indexes, and a\ncomparison betweenthe PGM-indexandtheRMI[18]under\nthis experimental scenario.\nPGM-index variants. The three indexing strategies ex-\nperimented for the PGM-index are binary search, multiway\ntree (speciﬁcally, we implemented theCSS-tree [31]) and ou r\nnovel recursive construction (see Section 2.2). We refer to\n9\n\n700 750 800 850 900 950 1 ,0000246810\nTime (ns)Index space (MiB)PGM◦BIN\nPGM◦CSS\nPGM◦REC\nε= 64\nε= 128\nε= 256\nε= 512\nε= 1024\nε= 2048\nFigure 5: The query performance of several conﬁgurations\nof the PGM-index. The Recursive PGM-index, depicted as\na pentagon, had better space and time performance than all\nthe other conﬁgurations.\nthem with PGM ◦BIN, PGM ◦CSS and PGM ◦REC, respec-\ntively. We set εℓ= 4 for all but the last level of PGM ◦REC,\nthat is the one that includes the segments built over the in-\nput dataset. Likewise, the node size of the CSS-tree was set\ntoB= 2εℓfor a fair comparison with PGM ◦REC. Figure 5\nshows that PGM ◦REC dominates PGM ◦CSS for ε≤256,\nand has better query performance than PGM ◦BIN. The ad-\nvantage of PGM ◦REC over PGM ◦CSS is also evident in\nterms of index height since the former has ﬁve levels whereas\nthe latter has seven levels, thus PGM ◦REC experiences a\nshorter traversal time which is induced by a higher branch-\ning factor (as conjectured in Section 2.2). For ε >256 all\nthe three strategies behaved similarly because the index wa s\nso small to ﬁt into the L2 cache.\nPGM-index vs traditional indexes. We compared the\nPGM-indexagainst thecache-eﬃcientCSS-treeandtheclas-\nsic B+-tree. For the former, we used our implementation.\nFor the latter, we chose a well-known library [4, 13, 18].\nThe PGM-index dominated these traditional indexes, as\nshown in Figure 6 for page sizes of 4–16 KiB. Performances\nfor smaller page sizes were too far (i.e. worse) from the main\nplot range, and thus are not shown. For example, the fastest\nCSS-tree in our machine had page size of 128 bytes, occu-\npied 341 MiB and was matched in query performance by a\nPGM◦REC with ε= 128 which occupied only 4 MiB (82.7 ×\nless space). As another example, the fastest B+-tree had\npage size of 256 bytes, occupied 874 MiB and was matched\nin query performance by a PGM ◦REC with ε= 4096 which\noccupied only 87 KiB (four orders of magnitude less space).\nWhat is surprising in those plots is the improvement in\nspace occupancy achieved by the PGM-index which is four\norders of magnitude with respect to the B+-tree and two or-\nders of magnitude with respect to the CSS-tree. As stated\nin Section 1, traditional indexes are blind to the data distr i-\nbution, and they miss the compression opportunities which\ndata trends oﬀer. On the contrary, adapting to the data\ndistribution throughlinear approximations allows thePGM -\nindex to uncover previously unknown space-time trade-oﬀs,\nas demonstrated in this experiment. For completeness, we600 800 1 ,000 1,200 1,400 1,600 1,80002468101214\nCSS-tree 4KiB\nCSS-tree 8KiB\nCSS-tree 16KiBB+-tree 16KiB\nTime (ns)Index space (MiB)PGMε= 26\nPGMε= 27\nPGMε= 28\nPGMε= 29\nPGMε= 210\nPGMε= 211\nRMI 10K\nRMI 50K\nRMI 100K\nRMI 200K\nRMI 400K\nFigure 6: The Recursive PGM-index improved uniformly\nRMI with diﬀerent second-stage sizes and traditional in-\ndexes with diﬀerent page sizes over all possible space-time\ntrade-oﬀs. Results of traditional indexes for smaller page\nsizes are not shown because too far from the plot range. For\nexample, the fastest CSS-tree occupied 341 MiB and was\nmatched in performance by a PGM-index of only 4 MiB\n(82.7×less space); the fastest B+-tree occupied 874 MiB\nand was matched in performance by a PGM-index which\noccupied only 87 KiB (four orders of magnitude less space).\nreport that on the 90.6 GiB of key-payload pairs the fastest\nCSS-tree took 1.2 seconds to construct, whereas the PGM-\nindex matching its performance in 82.7 ×less space took\nonly 1.9 seconds more (despite using a single-threaded and\nnon-optimised computation of the PLA-model).\nPGM-index vs known learned indexes. Figures 3and5\nhave shown that the PGM-index improves the FITing-tree\n(see also the discussion at the beginning of this section).\nHere, we complete the comparison against the other known\nlearnedindex,i.e. the2-stageRMIwhichusesacombination\nof linear and other models in its two stages. Figure 6 shows\nthat the PGM-index dominates RMI, it has indeed better\nlatency guarantees because, instead of ﬁxing the structure\nbeforehand and inspecting the errors afterwards, it is dy-\nnamically and optimally adapted to the input data distri-\nbution while guaranteeing the desired ε-approximation and\nusing the least possible space. The most compelling evi-\ndence is the Mean Absolute Error (MAE) between the ap-\nproximated and the predicted position, e.g., the PGM-index\nwithε= 512 needed about 32K segments and had MAE\n226±139, while an RMI with the same number of second\nstage models (i.e. number of models at the last level) had\nMAE 892 ±3729 (3.9 ×more). This means that RMI expe-\nrienced a higher and less predictable latency in the query\nexecution. We report that RMI took 30.4 seconds to con-\nstruct, whereas the PGM-index took only 3.1 seconds.\nDiscussion. Overall, the experiments have shown that the\nPGM-index is fast in construction (about 3 seconds to index\na real-world table of 91 GiB with 715M key-value pairs) and\nhas space footprint that is up to 63.3% lower than what was\nachieved by a state-of-the-art FITing-tree. Moreover, the\nPGM-index dominated in space and time both the tradi-\ntional and other learned index structures (e.g. the RMI). In\n10\n\nεReduction in # slopes (%)\n23242526272829210211050100Web logs\nLongitude\nIoT\nFigure 7: The slope compression algorithm of Lemma 3 re-\nduces the number of distinct slopes by up to 99.9%.\nεSpace saving (%)\n23242526272829210211050100Web logs\nLongitude\nIoT\nFigure 8: Slope compression reduces the spacetaken by the\nslopes up to 81.2%. Longitude is the only dataset on which\ncompression does not help for ε≥29because of its special\nfeatures. In this case compression would not be adopted.\nparticular, it improved the space footprint of the CSS-tree\nby a factor 82.7 ×and the one of the B-tree by more than\nfour orders of magnitude, while achieving the same or even\nbetter query eﬃciency.\n6.3 The Compressed PGM­index\nWe investigated the eﬀectiveness of the compression tech-\nniques proposed in Section 3. Figure 7 shows that the slope\ncompression algorithm reduced the numberof distinct slopes\nsigniﬁcantly, up to 99.94%, still preserving the same optim al\nnumber of segments. As far as the space occupancy is con-\ncerned, and considering just the last level of a PGM-index\nwhich is the largest one, the reduction induced by the com-\npression algorithm was up to 81.2%, as shown in Figure 8.\nNote that in the Longitude datasets for ε≥29the slope\ncompression is not eﬀective enough. As a result, the map-\nping from segments to the slopes table causes an overhead\nthat exceeds the original space occupancy of the segments.\nClearly, a real-world application would turn oﬀ slope com-\npression in such situations.\nIn information theory, the compressibility of data is mea-\nsured with its entropy (as deﬁned by Shannon). We conjec-ε= 64 128 256 512 1024 2048\nSpace saving (%) 52.2 50.8 48.5 46.0 41.5 35.5\nTime loss (%) 13.7 22.6 24.5 15.1 11.7 9.9\nTable 3: Query performance of the Compressed PGM-index\nwith respect to the corresponding Recursive PGM-index.\nturethatasimilar measure, characterising“diﬃcult”data sets\nlikeLongitude , also exists in our “geometric setting” and is\nworth studying. Another interesting future work is to ex-\nploretherelation betweenthealgorithm ofLemma 1andthe\nslope compression algorithm. To explain, recall that dur-\ning the construction of the optimal PLA-model the range of\nslopes reduces each time a new point is added to the current\nconvex hull (segment). Therefore “shortening” a segment,\non the one hand, improves the performance of the slope\ncompression algorithm (because it enlarges the sizes of the\npossible slope intervals), but on the other hand, it increas es\nthe overall number of segments. Given that Lemma 3 of-\nfers a compressed space bound which depends on m(the\noverall number of segments) and t(the number of distinct\nsegments), balancing the above two eﬀects to achieve better\ncompression is an intriguing extension of this paper.\nAfterwards, we measured the query performance of the\nCompressed PGM-indexin whichcompression was activated\nover the intercepts and the slopes of the segments of all the\nlevels. Table 3showsthat, withrespecttothecorrespondin g\nRecursive PGM-index, the space footprint is reduced by up\nto 52.2% at the cost of moderately slower queries (no more\nthan 24.5%).\n6.4 The Multicriteria PGM­index\nOur implementation of the Multicriteria PGM-index op-\nerates in two modes: the time-minimisation mode (shortly,\nmin-time) and the space-minimisation mode (shortly, min-\nspace), whichimplementthealgorithms describedinSectio n 5.\nInmin-time mode, inputstotheprogram are smaxandatol-\nerancetolon the space occupancy of the solution, and the\noutput is the value of the error εwhich guarantees a space\nboundsmax±tol. In min-space mode, inputs to the pro-\ngram are tmaxand a tolerance tolon the query time of the\nsolution, and the output is the value of the error εwhich\nguarantees a time bound tmax±tolin the query operations.\nWe note that the introduction of a tolerance parameter al-\nlows us to stop the search earlier as soon as any further step\nwould not appreciably improve the solution (i.e., we seek\nonly improvements of several bytes or nanoseconds). So tol\nis notaparameter thathas tobetunedbutratherastopping\ncriterion like the ones used in iterative methods.\nTo model the space occupancy of a PGM-index, we stud-\nied empirically the behaviour of the number of segments\nmopt=sL(ε) forming the optimal PLA-model, by varying\nεand by ﬁtting ninety diﬀerent functions over about two\nhundred points ( ε,sL(ε)) generated beforehand by a long-\nrunning grid search over our real-world datasets. Looking\nat the ﬁttings, we chose to model sL(ε) with a power-law\nhaving the form aε−b. As further design choices we point\nout that: (i) the ﬁttingof the power-law was performed with\nthe Levenberg-Marquardt algorithm, while root-ﬁnding was\nperformed with Newton’s method; (ii) the search space for\nεwas set to E= [8,n/2] (since a cache line holds eight 64\n11\n\nbits integers); and ﬁnally (iii) the number of guesses was se t\nto 2⌈loglogE⌉.\nThe following experiments were executed by addressing\nsome use cases in order to show the eﬃcacy and eﬃciency\nof the multicriteria PGM-index.\nExperiments with the min-time mode. Suppose that\na database administrator wants the most eﬃcient PGM-in-\ndex for the Web logs dataset that ﬁts into an L2 cache of 1\nMiB. Our solver derived an optimal PGM-index matching\nthat space bound by setting ε= 393 and taking 10 itera-\ntions and a total of 19 seconds. This result was obtained by\napproximating sL(ε) with the power-law 46032135 ·ε−1.16\nwhich guaranteed a mean squared error of no more than\n4.8% over the range ε∈[8,1024].\nAs another example, suppose that a database administra-\ntor wants the most eﬃcient PGM-index for the Longitude\ndataset that ﬁts into an L1 cache of 32 KiB. Our solver de-\nrived an optimal PGM-index matching that space bound by\nsettingε= 1050 and taking 14 iterations and a total of 9\nseconds.\nExperiments with the min-space mode. Suppose that\na database administrator wants the PGM-index for the IoT\ndataset with the lowest space footprint that answers any\nquery in less than 500 ns. Our solver derived an optimal\nPGM-index matching that time bound by setting ε= 432,\noccupying 74.55 KiB of space, and taking 9 iterations and a\ntotal of 6 seconds.\nAs another example, suppose that a database administra-\ntor wants the most compressed PGM-index for the Web logs\ndataset that answers any query in less than 800 ns. Our\nsolver derived an optimal PGM-index matching that time\nbound by setting ε= 1217, occupying 280.05 KiB of space,\nand taking 8 iterations and a total of 17 seconds.\nDiscussion. In contrast to the FITing-tree and the RMI,\ntheMulticriteriaPGM-indexcantradeeﬃcientlyquerytime\nwith space occupancy, making it a promising approach for\napplications with rapidly-changing data distributions an d\nspace/time constraints. Overall, in both modes our ap-\nproach ran in less than 20 seconds.\n7. CONCLUSIONS AND FUTURE WORK\nWe have introduced the PGM-index, a learned data struc-\nture for the fully indexable dictionary problem which im-\nproves the query performance and the space occupancy of\nboth traditional and modern learned indexes up to several\norders of magnitude. We have also designed three variants\nof the PGM-index: one that improves its already succinct\nspace footprint using ad-hoc compression techniques, one\nthat adapts itself to the query distribution, and one that\nprovides estimations of the number of occurrences of indi-\nvidual or range of keys. Finally, we have demonstrated that\nthe PGM-index is a multicriteria data structure as it can\nbe fast optimised within some user-given constraint on the\nspace occupancy or the query time.\nA possible research direction is to experiment with the\nperformance of insertion and deletion of keys in a PGM-in-\ndex. To this end, we mention classic techniques such as the\nsplit-merge strategy in B-tree nodes [2, 34], and the use of\nbuﬀers that once full are merged into the index (cf. [13]).\nThe possibility of orchestrating segments, nonlinear mode ls\nand rank/select indexing techniques from the compressiondomain [23, 36] is another intriguing research direction, e s-\npecially within our Multicriteria framework.\n8. ACKNOWLEDGMENTS\nPart of this work has been supported by the Italian MIUR\nPRINproject“Multicriteria DataStructuresandAlgorithm s:\nfrom compressed to learned indexes, and beyond” (Prot.\n2017WR7SHH) and by Regione Toscana (under POR FSE\n2014/2020).\n9. REFERENCES\n[1] A. Bagchi, A. L. Buchsbaum, and M. T. Goodrich.\nBiased skip lists. Algorithmica , 42(1):31–48, 2005.\n[2] M. Bender, E. Demaine, and M. Farach-Colton.\nCache-oblivious b-trees. SIAM J. Comput. ,\n35(2):341–358, 2005.\n[3] S. W. Bent, D. D. Sleator, and R. E. Tarjan. Biased\nsearch trees. SIAM J. Comput. , 14(3):545–568, 1985.\n[4] T. Bingmann. STX B+-tree C++template classes,\n2013.http://panthema.net/2007/stx-btree . Version\n0.9.\n[5] C. Buragohain, N. Shrivastava, and S. Suri. Space\neﬃcient streaming algorithms for the maximum error\nhistogram. In Proceedings of the IEEE 23rd\nInternational Conference on Data Engineering , ICDE,\npages 1026–1035, Washington, D.C., USA, 2007. IEEE\nComputer Society.\n[6] M. Burtscher and P. Ratanaworabhan. Fpc: A\nhigh-speed compressor for double-precision\nﬂoating-point data. IEEE Transactions on Computers ,\n58(1):18–31, 2009.\n[7] C.-Y. Chan and Y. E. Ioannidis. Bitmap index design\nand evaluation. In Proceedings of the ACM\nInternational Conference on Management of Data ,\nSIGMOD, pages 355–366, New York, NY, USA, 1998.\nACM.\n[8] D. Z. Chen and H. Wang. Approximating points by a\npiecewise linear function. Algorithmica , 66(3):682–713,\n2013.\n[9] Q. Chen, L. Chen, X. Lian, Y. Liu, and J. X. Yu.\nIndexable pla for eﬃcient similarity search. In\nProceedings of the 33rd International Conference on\nVery Large Data Bases , VLDB, pages 435–446. VLDB\nEndowment, 2007.\n[10] G. Cormode. Data sketching. Communications of the\nACM, 60(9):48–55, 2017.\n[11] A. Farruggia, P. Ferragina, A. Frangioni, and\nR. Venturini. Bicriteria data compression. In\nProceedings of the 25th ACM-SIAM Symposium on\nDiscrete Algorithms , SODA, pages 1582–1595,\nPhiladelphia, PA, USA, 2014. Society for Industrial\nand Applied Mathematics.\n[12] P. Ferragina and R. Venturini. Compressed\ncache-oblivious String B-tree. ACM Trans.\nAlgorithms , 12(4):52:1–52:17, 2016.\n[13] A. Galakatos, M. Markovitch, C. Binnig, R. Fonseca,\nand T. Kraska. Fiting-tree: A data-aware index\nstructure. In Proceedings of the 2019 International\nConference on Management of Data , SIGMOD, pages\n1189–1206, New York, NY, USA, 2019. ACM.\n[14] G. Graefe. B-tree indexes, interpolation search, and\nskew. In Proceedings of the 2Nd International\n12\n\nWorkshop on Data Management on New Hardware ,\nDaMoN, New York, NY, USA, 2006. ACM.\n[15] A. Grama, G. Karypis, V. Kumar, and A. Gupta.\nIntroduction to Parallel Computing . Addison-Wesley\nLongman Publishing Co., Inc., Boston, MA, USA, 2\nedition, 2003.\n[16] S. Idreos, K. Zoumpatianos, S. Chatterjee, W. Qin,\nA. Wasay, B. Hentschel, M. Kester, N. Dayan,\nD. Guo, M. Kang, and Y. Sun. Learning data\nstructure alchemy. Bulletin of the IEEE Computer\nSociety Technical Committee on Data Engineering ,\n42(2):46–57, 2019.\n[17] T. Kraska, M. Alizadeh, A. Beutel, E. H. Chi,\nA. Kristo, G. Leclerc, S. Madden, H. Mao, and\nV. Nathan. Sagedb: A learned database system. In\nProceedings of the 9th Biennial Conference on\nInnovative Data Systems Research , CIDR, 2019.\n[18] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and\nN. Polyzotis. The case for learned index structures. In\nProceedings of the 2018 International Conference on\nManagement of Data , SIGMOD, pages 489–504, New\nYork, NY, USA, 2018. ACM.\n[19] P. Lindstrom, S. Lloyd, and J. Hittinger. Universal\ncoding of the reals: Alternatives to ieee ﬂoating point.\nInProceedings of the Conference for Next Generation\nArithmetic , CoNGA, pages 5:1–5:14, New York, NY,\nUSA, 2018. ACM.\n[20] K. Mehlhorn. Data Structures and Algorithms 1:\nSorting and Searching , volume 1 of EATCS\nMonographs on Theoretical Computer Science .\nSpringer-Verlag, Berlin, Heidelberg, New York, Tokyo,\n1984.\n[21] A. Moﬀat and A. Turpin. Compression and Coding\nAlgorithms . Springer, Boston, MA, USA, 2002.\n[22] K. Naono, K. Teranishi, J. Cavazos, and R. Suda,\neditors.Software Automatic Tuning, From Concepts to\nState-of-the-Art Results . Springer, New York,\nDordrecht, Heidelberg, London, 2010.\n[23] G. Navarro. Compact data structures: A practical\napproach . Cambridge University Press, New York, NY,\nUSA, 2016.\n[24] G. Navarro and V. M¨ akinen. Compressed full-text\nindexes. ACM Comput. Surv. , 39(1):2, 2007.\n[25] D. Okanohara and K. Sadakane. Practical\nentropy-compressed rank/select dictionary. In\nProceedings of the Meeting on Algorithm Engineering\n& Expermiments , pages 60–70, Philadelphia, PA, USA,\n2007. Society for Industrial and Applied Mathematics.\n[26] OpenStreetMap contributors. OpenStreetMap Data\nExtract for Italy. https://www.openstreetmap.org ,\n2018. Retrieved from http://download.geofabrik.de\non May 9, 2018.\n[27] J. O’Rourke. An on-line algorithm for ﬁtting straight\nlines between data ranges. Commun. ACM ,\n24(9):574–578, 1981.\n[28] R. Pagh and F. F. Rodler. Cuckoo hashing. Journal of\nAlgorithms , 51(2):122 – 144, 2004.\n[29] A. Petrov. Algorithms behind modern storage\nsystems. Commun. ACM , 61(8):38–44, 2018.\n[30] M. P˘ atra¸ scu and M. Thorup. Time-space trade-oﬀs for\npredecessor search. In Proceedings of the 38th ACMSymposium on Theory of Computing , STOC, pages\n232–240, New York, NY, USA, 2006. ACM.\n[31] J. Rao and K. A. Ross. Cache conscious indexing for\ndecision-support in main memory. In Proceedings of\nthe 25th International Conference on Very Large Data\nBases, VLDB, pages 78–89, San Francisco, CA, USA,\n1999. Morgan Kaufmann Publishers Inc.\n[32] R. Seidel and C. R. Aragon. Randomized search trees.\nAlgorithmica , 16(4/5):464–497, 1996.\n[33] J. Sklansky and V. Gonzalez. Fast polygonal\napproximation of digitized curves. Pattern\nRecognition , 12(5):327 – 331, 1980.\n[34] J. S. Vitter. External memory algorithms and data\nstructures: Dealing with massive data. ACM Comput.\nSurv., 33(2):209–271, 2001.\n[35] J. Wang, C. Lin, Y. Papakonstantinou, and\nS. Swanson. An experimental study of bitmap\ncompression vs. inverted list compression. In\nProceedings of the 2017 ACM International\nConference on Management of Data , SIGMOD, pages\n993–1008, New York, NY, USA, 2017. ACM.\n[36] I. H. Witten, A. Moﬀat, and T. C. Bell. Managing\nGigabytes (2Nd Ed.): Compressing and Indexing\nDocuments and Images . Morgan Kaufmann Publishers\nInc., San Francisco, CA, USA, 1999.\n[37] Q. Xie, C. Pang, X. Zhou, X. Zhang, and K. Deng.\nMaximum error-bounded piecewise linear\nrepresentation for online stream approximation. The\nVLDB Journal , 23(6):915–937, 2014.\n13",
  "textLength": 71384
}