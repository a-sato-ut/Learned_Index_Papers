{
  "paperId": "e087a7f9b346623e042f6cdfb35722f44e16c5e9",
  "title": "Fair-Count-Min: Frequency Estimation under Equal Group-wise Approximation Factor",
  "pdfPath": "e087a7f9b346623e042f6cdfb35722f44e16c5e9.pdf",
  "text": "arXiv:2505.18919v1  [cs.DS]  25 May 2025Fair-Count-Min : Frequency Estimation under Equal Group-wise\nApproximation Factor\nNima Shahbazi\nUniversity of Illinois Chicago\nnshahb3@uic.eduStavros Sintos\nUniversity of Illinois Chicago\nstavros@uic.eduAbolfazl Asudeh\nUniversity of Illinois Chicago\nasudeh@uic.edu\nABSTRACT\nFrequency estimation in streaming data often relies on sketches\nlike Count-Min (CM) to provide approximate answers with sub-\nlinear space. However, CM sketches introduce additive errors that\ndisproportionately impact low-frequency elements, creating fair-\nness concerns across different groups of elements. We introduce\nFair-Count-Min, a frequency estimation sketch that guarantees\nequal expected approximation factors across element groups, thus\naddressing the unfairness issue. We propose a column partitioning\napproach with group-aware semi-uniform hashing to eliminate\ncollisions between elements from different groups. We provide the-\noretical guarantees for fairness, analyze the price of fairness, and\nvalidate our theoretical findings through extensive experiments on\nreal-world and synthetic datasets. Our experimental results show\nthat Fair-Count-Min achieves fairness with minimal additional er-\nror and maintains competitive efficiency compared to standard CM\nsketches.\nPVLDB Reference Format:\nNima Shahbazi, Stavros Sintos, and Abolfazl Asudeh. Fair-Count-Min :\nFrequency Estimation under Equal Group-wise Approximation Factor.\nPVLDB, 14(1): XXX-XXX, 2020.\ndoi:XX.XX/XXX.XX\nPVLDB Artifact Availability:\nThe source code, data, and/or other artifacts have been made available at\nhttps://github.com/neemashahbazi/Fair-Count-Min/.\n1 INTRODUCTION\nEstimating item frequencies in data streams is a fundamental prob-\nlem in computer science, where given a universe of elements, the\nobjective is to count the appearance of each within the stream. How-\never, due to memory limitations and processing time in streaming\nsettings, exact counting is often impractical. Consequently, approx-\nimating the counts using techniques such as the Count-Min (CM)\nsketch has become widely adopted. The CM estimates the frequency\nof elements by randomly hashing the elements into a smaller set\nof buckets, while adding up the frequencies of all elements in each\nbucket. The frequency count of each bucket is then returned as an\nupper bound of the frequency of elements it contains.\nThe CM sketches offer space-efficient frequency estimations\nwith small error guarantees. Nevertheless, the error guarantees are\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license. For any use beyond those covered by this license, obtain permission by\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 14, No. 1 ISSN 2150-8097.\ndoi:XX.XX/XXX.XXadditive . Such errors may be negligible for popular elements with a\nhigh frequency in the data stream. However, as further elaborated\nin Example 1, the unpopular elements with low frequencies in the\nstream are disproportionately impacted under such guarantees.\nThat is because even small additive errors can cause a significant\nrelative increase in the count estimation of low-frequency elements.\nThis imbalance gives rise to a fairness issue â€“ an aspect overlooked\nin the existing work.\nTo address this issue, in this paper, we introduce Fair-Count-\nMin (FCM), a frequency estimation sketch with the equal group-\nlevel approximation factor guarantee, ensuring that the relative\ncount-estimation increase is equal for various groups, irrespective\nof their popularity in the data stream. To the best of our knowl-\nedge, FCM is the first frequency estimation sketch with provable\nmultiplicative error guarantees in its estimation.\nOur key idea in the design of the FCM is the partitioning of\nthe sketch buckets strategy based on a group-aware, semi-uniform\nhashing scheme that prevents collisions between elements of dif-\nferent groups. Then, we prove that, carefully selecting the number\nof buckets allocated to each group, our FCM sketch guarantees an\nequal expected approximation factor across the groups, i.e., equal\nratios of the true frequency to the estimated one. The FCM sketch\napplies to binary and non-binary grouping of the elements, while\nthe grouping can be based on element popularity or any other\ngrouping strategy, e.g., based on the demographic groups.\nFCM does not increase the memory requirement of the CM.\nFurthermore, FCM and CM have the same time complexities for\nupdate and query operations. In other words, the time taken to look\nup the frequency estimation of an element and the time to increase\nthe frequency of an element in the data stream are the same in CM\nand FCM.\nIn a general setting, where multiple hash functions are used in\nthe FCM, identifying the proper number of buckets allocated to\neach group requires solving an equation, for which we propose\nefficient exact and approximation algorithms.\nFurthermore, we theoretically analyze the price of fairness (PoF)\nof FCM. Interestingly, we show that achieving fairness may even\nreduce the total additive error for a specific case when the CM\nsketch uses a single random hash function, while experimentally\ndemonstrating that the PoF is small for the general settings.\nIn addition to theoretical analysis, we perform extensive experi-\nments on multiple real-world and synthetic datasets to evaluate the\nperformance of our proposed approach in practice. In summary, our\nexperiments verify (a) while CM is usually not fair, FCM achieves\nfairness in all experiments, (b) the price of fairness is negligible,\nand (c) FCM has the same memory and time efficiency as CM.\n\nSummary of contributions:\nâ€¢We propose a novel notion of group fairness in the frequency\nestimation context, defined by the requirement that the approxi-\nmation factor (the multiplicative overestimation error) remains\nequal across different groups. This contrasts with traditional\napproaches based on additive error bounds, which tend to dis-\nproportionately impact unpopular elements with low frequency.\nUsing this notion of fairness, we introduce Fair-Count-Min (FCM),\nthe frequency estimation sketch that guarantees group fairness.\nâ€¢We introduce a column-partitioning technique based on semi-\nuniform hash functions to develop FCM. This method assigns\ngroups to disjoint sets of hash buckets, thereby eliminating inter-\ngroup collisions and promoting fairness in estimation. Our pro-\nposed method, FCM, is agnostic to both the grouping strategy\nand the underlying distribution of the data elements. Further-\nmore, FCM does not increase the memory and time requirements\nof the regular count-min sketch.\nâ€¢We theoretically prove that FCM is fair, i.e., it ensures an equal ex-\npected approximation factor across groups. Furthermore, we an-\nalyze the price of fairness and show that it is typically smallâ€”and\nin some cases, can even be negative.\nâ€¢We design both exact and approximate algorithms to efficiently\ncompute the optimal allocation of buckets per group.\nâ€¢We empirically evaluate FCM using real-world and synthetic\ndatasets. The results demonstrate that FCM successfully achieves\ngroup fairness while incurring only a minor additional additive\nerror, and it preserves the time and space efficiency characteristic\nof standard CM sketches.\nPaper organization: The rest of the paper is organized as follows.\nIn Section 2, we provide necessary preliminaries, introduce the\nfairness challenge in CM sketches, and formally define the group-\nfair frequency estimation problem. Section 3 details our FCM sketch,\nincluding its construction via group-aware semi-uniform hashing\nand fairness guarantees. Section 4 presents efficient algorithms\nfor computing bucket allocations across groups. In Section 5, we\ntheoretically analyze the price of fairness. Section 6 reports our\nexperimental results, comparing FCM to standard CM and Row-\nPartitioning baselines. Finally, we conclude with a discussion of\nimplications and future work.\n2 PRELIMINARY\n2.1 Data Model\nLetDbe a data stream consisting of elements (or events), each\nbelonging to a type from a finite universe U={ğ‘’1,ğ‘’2,...,ğ‘’ğ‘›}. Let\nğ‘be the number of elements in the data stream D. We use the\nfunctionğ‘“:Uâ†’[ğ‘]to refer to the frequency of each element\ntype inD. That is,ğ‘“(ğ‘’)is the number of times an element of type\nğ‘’appears inD. Formally,\nğ‘“(ğ‘’)=âˆ‘ï¸‚\nğ‘¥âˆˆD1(ğ‘¥=ğ‘’),\nwhere 1(ğ‘¥=ğ‘’)is the indicator function that equals 1 if ğ‘¥is of type\nğ‘’and 0 otherwise.2.2 Count-Min Sketch\nFrequency estimation is a fundamental problem in streaming data\nprocessing. Given a data stream D, the goal is to efficiently esti-\nmate the frequency ğ‘“(ğ‘’)of each element type ğ‘’. It is easy to see\nthat finding an exact solution to the above problem requires Î˜(ğ‘›)\nspace, as it involves maintaining ğ‘›counters for each element type.\nTo overcome this limitation, frequency estimation data structures\n(a.k.a sketches) have been designed to provide approximate answers\nto such queries while using sub-linear space. In addition to being\nspace-efficient, these sketches provide probabilistic error guaran-\ntees through tunable parameters and enable constant-time update\nand query operations.\nCount-Min (CM) is among the most widely used sketches for\nfrequency estimation. It represents the frequency estimates using a\ntwo-dimensional array ğ¶ğ‘€ofğ‘‘rows andğ‘¤columns. The sketch\nemploysğ‘‘independent hash functions h1,h2,...,hğ‘‘, where each\nhğ‘–:Uâˆ’ â†’[ğ‘¤]maps an element type to a column index (called\nbucket orbinin the rest of the paper) in row ğ‘–. These hash functions\ndistribute element types across different buckets to reduce the\nimpact of hash collisions. When a new element of type ğ‘’arrives in\nthe stream, the sketch updates its counts as follows:\nğ¶ğ‘€[ğ‘–,hğ‘–(ğ‘’)]â†ğ¶ğ‘€[ğ‘–,hğ‘–(ğ‘’)]+1,âˆ€ğ‘–âˆˆ[ğ‘‘].\nEach row records a count for ğ‘’, though potential overestimations\nmay occur due to hash collisions. To estimate the frequency of an\nelement type1ğ‘’, the sketch returns:\nğ‘“Ë†(ğ‘’)=ğ‘‘\nmin\nğ‘–=1ğ¶ğ‘€[ğ‘–,hğ‘–(ğ‘’)].\nTaking the minimum across multiple rows mitigates over-counting\nerrors. As previously mentioned, the estimation error in CM sketch\noccurs due to hash collisions. For each element type ğ‘’the estimated\nfrequencyğ‘“Ë†(ğ‘’)is always an upper bound on the true frequency\nğ‘“(ğ‘’):\nğ‘“Ë†(ğ‘’)=ğ‘“(ğ‘’)+ğœ€ğ´(ğ‘’), (1)\nwhere the additive error ğœ€ğ´(ğ‘’)corresponds to the combined\nfrequency of all other elements that hash into the same bucket as ğ‘’.\nIn expectation, the additive error ğœ€ğ´(ğ‘’)is bounded byğ‘\nğ‘¤. Formally:\nE[ğ‘“Ë†(ğ‘’)]=ğ‘“(ğ‘’)+âˆ‘ï¸\nâˆ€ğ‘’â€²â‰ ğ‘’,h(ğ‘’â€²)=h(ğ‘’)ğ‘“(ğ‘’â€²)\nğ‘¤â‰¤ğ‘“(ğ‘’)+ğ‘\nğ‘¤(2)\nSeveral variations of the CM sketch exist, including the CM\nsketch with conservative updates [ 53], the Count-Mean-Min sketch [ 30],\nand other related frequency estimation structures such as the Count\nsketch [ 21] and Spectral Bloom Filters [ 25]. In this study, we fo-\ncus exclusively on the CM sketch, leaving the exploration of these\nalternatives for future work.\n2.3 Fairness: Equal Expected Approximation\nFactor\nExisting CM sketches provide an additive upper-bound error ğœ€ğ´on\nthe frequency estimation of the elements. However, while ğœ€ğ´may\nbe a negligible error for the popular, high-frequency elements, it\ncan be intolerable for the unpopular, low-frequency ones. In other\nwords, the significance of the frequency estimation error is relative\n1In the rest of the paper, we use â€œelement ğ‘’â€ and â€œelement type ğ‘’â€ interchangeability\nto refer to elements of type ğ‘’.\n2\n\nto the frequency value. To further clarify this using a toy example,\nlet us consider Example 1.\nExample 1: Consider a CM sketch with a hash function â„that maps the\nevents of typesU={ğ‘’1,ğ‘’2,...,ğ‘’ğ‘›}toğ‘¤=200bins. Letğ‘=10000 be the\nnumber of elements in the data stream. Hence, the additive estimation error\nis bounded by ğœ€ğ´â‰¤50. Suppose the frequencies of the element types ğ‘’1and\nğ‘’2areğ‘“(ğ‘’1)=1000 andğ‘“(ğ‘’2)=10. As a result, the expected frequency\nestimation is bounded by 1050 forğ‘’1, resulting in a small error of only 5%of\nğ‘“(ğ‘’1). On the other hand, this error is as high as 500% forğ‘’2.\nFrom Example 1, it is clear that the additive error does not provide\na strong guarantee independent of the element frequencies. Specifi-\ncally, providing large estimation errors relative to small frequency\nvalues, it is unfair for low-frequency element types. This moti-\nvates us to instead promote approximation factor as a stronger\nfrequency estimation guarantee.\nDefinition 1 (Approximation Factor ). The approximation\nfactor of count-min sketch CM for an element type ğ‘’isğ›¼(ğ‘’), if its\nestimationğ‘“Ë†(ğ‘’)of the frequency of ğ‘’satisfies the following condition:\nğ‘“(ğ‘’)\nğ‘“Ë†(ğ‘’)=ğ›¼(ğ‘’).\nUsing Definition 1, we define a notion of group fairness pro-\nvides equal multiplicative frequency estimation errors for various\nelement groups.\nLet each element type ğ‘’be associated with a group ğ‘”âˆˆG, where\nG={g1,g2,..., gâ„“}. For example, each group gğ‘–might contain ele-\nment types with comparable popularity. Then, a count-min sketch\nis called fair if it satisfies Definition 2.\nDefinition 2 (Group-Fair Count-Min). A count-min sketch is\ngroup-fair iff:\nâˆ€gğ‘–,gğ‘—âˆˆG,Eğ‘’âˆˆgğ‘–[ï¸\nğ›¼(ğ‘’)]ï¸\n=Eğ‘’â€²âˆˆgğ‘—[ï¸\nğ›¼(ğ‘’â€²)]ï¸\n.\nWithout loss of generality and to ease the explanations, we use\nthe binary groups ğ‘™(low-frequency ) andâ„(high-frequency ) for\nexplaining our sketches and drawing the analyses.\nOur results readily hold for non-binary and arbitrary grouping\nof element types, as we shall further discuss in Section 3.4.\n2.4 Problem Formulation\nWith the necessary terms and notations defined, we now formally\ndefine our problem of interest:\nDefinition 3. Given a data stream of elements Ddrawn from\na universe of element types Uwhere each element type belongs to a\ngroupğ‘”âˆˆG, design a group-fair count-min sketch.\n2.5 Solution Overview\nA main issue that causes the unbounded multiplicative error in\ntraditional count-min sketches is that high-frequency elements can\ncollide with low-frequency ones, adding a major overestimation to\ntheir counts.\nOne way to reduce the chance of such collisions is by significantly\nincreasing the number of bins ğ‘¤and/or the value of ğ‘‘, the number\nof independent hash functions used for the estimation, i.e., the1 2Â·Â·Â·ğ‘¤ğ‘™ğ‘¤ğ‘™+1Â·Â·Â·ğ‘¤ğ‘¤ğ‘™ ğ‘¤â„=ğ‘¤âˆ’ğ‘¤ğ‘™\nFigure 1: Illustration of a column-partitioning based group-\nfair min-count with one row, i.e., one hash function h(.).\nnumber of rows in the sketch. This, however, has two major issues:\nfirst, given a low-frequency element ğ‘’to ensure it is unlikely that a\nhigh-frequency element collides with it in at least one of the ğ‘‘rows,\nthe size of the sketch increases in the order of ğ‘›, the number of\nelement types â€“ losing its purpose. Second, even after increasing the\nsize of the sketch, it cannot provide a guarantee of a multiplicative\nerror as low and high-frequency elements can still collide.\nInstead, to design group-fair min-count sketches, our goal is to\nmake it impossible for elements in different groups to collide .\nSpecifically, we ensure our goal by designing group-aware â€œsemi-\nuniformâ€ hash schemes that isolate low and high-frequency ele-\nments in Section 3. Proving that our sketch satisfies Definition 2,\nwe study its price of fairness in Section 5.\n3 FAIR-COUNT-MIN USING GROUP-AWARE\nSEMI-UNIFORM HASHING\nOur first idea for generating group-fair count-min is to utilize â€œsemi-\nuniformâ€ hash functions that separate element types of low and high\nfrequency. Specifically, reserving ğ‘¤ğ‘™bins for the low-frequency\ngroupğ‘™andğ‘¤â„=ğ‘¤âˆ’ğ‘¤ğ‘™for the high-frequency groupâ„, the semi-\nuniform hash function h(.)assigns each low-frequency element to\nthe firstğ‘¤ğ‘™bins and the others to the remaining ğ‘¤â„bins (Figure 1):\nh:{ï¸„\nğ‘™â†’[ğ‘¤ğ‘™]\nâ„â†’ğ‘¤ğ‘™+[ğ‘¤â„]\nThe key question we shall answer in the rest of this section is\nwhether there exists a value ğ‘¤ğ‘™(henceğ‘¤â„=ğ‘¤âˆ’ğ‘¤ğ‘™) for which a\ncount-min sketch based on the semi-uniform hash scheme becomes\ngroup fair.\nIn the following, first, we focus on the case where ğ‘‘=1, i.e., the\nsketch is based on only one hash function â„. Next, we extend our\nanalysis toğ‘‘=2and finally to the general values of ğ‘‘.\n3.1ğ‘‘=1\nFirst, let us derive the expected approximation factor for an element\nğ‘’in the low-frequency groupğ‘™. Letğ‘›ğ‘™be the number of element\ntypes inğ‘™.\nLetğ¶ğ‘–be the total frequency counts in each low-frequency bucket\nğ‘–âˆˆ[ğ‘¤ğ‘™]. That is,\nğ¶ğ‘–=âˆ‘ï¸‚\nğ‘’ğ‘—:h(ğ‘’ğ‘—)=ğ‘–ğ‘“(ğ‘’ğ‘—)\nFor every element ğ‘’ğ‘—âˆˆğ‘™, the value of the bucket it hashed to\nis returned as its frequency estimation. That is, ğ‘“Ë†(ğ‘’ğ‘—)=ğ¶h(ğ‘’ğ‘—).\nTherefore, following the approximation factor definition,\nğ‘“Ë†(ğ‘’ğ‘—)=ğ‘“(ğ‘’ğ‘—)\nğ›¼(ğ‘’ğ‘—)=ğ¶h(ğ‘’ğ‘—).\n3\n\nHence,\nğ›¼(ğ‘’ğ‘—)=ğ‘“(ğ‘’ğ‘—)\nğ¶h(ğ‘’ğ‘—)(3)\nTherefore,\nE[ğ›¼ğ‘™]=1\nğ‘›ğ‘™âˆ‘ï¸‚\nğ‘’ğ‘—âˆˆğ‘™ğ›¼(ğ‘’ğ‘—)=1\nğ‘›ğ‘™âˆ‘ï¸‚\nğ‘’ğ‘—âˆˆğ‘™ğ‘“(ğ‘’ğ‘—)\nğ¶h(ğ‘’ğ‘—)\n=1\nğ‘›ğ‘™ğ‘¤ğ‘™âˆ‘ï¸‚\nğ‘–=1âˆ‘ï¸‚\nğ‘’ğ‘—:h(ğ‘’ğ‘—)=ğ‘–ğ‘“(ğ‘’ğ‘—)\nğ¶ğ‘–//breaking the calculation per cell\n=1\nğ‘›ğ‘™ğ‘¤ğ‘™âˆ‘ï¸‚\nğ‘–=11\nğ¶ğ‘–âˆ‘ï¸‚\nğ‘’ğ‘—:h(ğ‘’ğ‘—)=ğ‘–ğ‘“(ğ‘’ğ‘—) //factoring out the common term\n=1\nğ‘›ğ‘™ğ‘¤ğ‘™âˆ‘ï¸‚\nğ‘–=11\nğ¶ğ‘–ğ¶ğ‘– //replacing sum of frequencies with ğ¶ğ‘–\n=1\nğ‘›ğ‘™ğ‘¤ğ‘™âˆ‘ï¸‚\nğ‘–=11=ğ‘¤ğ‘™\nğ‘›ğ‘™(4)\nSimilarly, the expected approximation factor for the group â„can\nbe computed as\nE[ï¸\nğ›¼â„]ï¸\n=ğ‘¤â„\nğ‘›â„=ğ‘¤âˆ’ğ‘¤ğ‘™\nğ‘›âˆ’ğ‘›ğ‘™\nTo satisfy group fairness (Definition 2), we need to ensure that\nthe expected group fairness for the two groups is the same. That is,\nE[ï¸\nğ›¼ğ‘™]ï¸\n=E[ï¸\nğ›¼â„]ï¸\n.\nHence, to ensure fairness, ğ‘¤ğ‘™is selected as follows:\nğ‘¤ğ‘™\nğ‘›ğ‘™=ğ‘¤âˆ’ğ‘¤ğ‘™\nğ‘›âˆ’ğ‘›ğ‘™â‡’ğ‘¤ğ‘™=ğ‘›ğ‘™\nğ‘›ğ‘¤ (5)\nIn other words, according to Equation 5, if the number of bins\nallocated to each group is proportional to their size, i.e.,ğ‘›ğ‘™\nğ‘›, is\nsufficient to ensure fairness. Formally,\nTheorem 1. A Count-Min sketch with a group-aware semi-uniform\nhash function h(.)is group-fair, if the number of bins ğ‘™allocated to\neach group is proportional to the ratio of element types from that\ngroup. That is, ğ‘¤ğ‘™=ğ‘›ğ‘™\nğ‘›ğ‘¤,âˆ€gğ‘™âˆˆG.\nBefore moving to the larger values of ğ‘‘, i.e., count-min sketches\nwith more number of hash functions, let us take a closer look at\nTheorem 1 and what it indicates. A regular min-count sketch hashes\ntheğ‘›element types uniformly across the [ğ‘¤]bins (irrespective of\nwhich group they belong to). As a result, the expected number\nof element types in each bucket isğ‘›\nğ‘¤. According to Theorem 1,\nğ‘¤ğ‘™=ğ‘›ğ‘™\nğ‘›ğ‘¤,âˆ€ğ‘™âˆˆG. As a result, the number of elements in each\nbucket for each group ğ‘™is:\nğ‘›ğ‘™\nğ‘¤ğ‘™=ğ‘›ğ‘™\nğ‘›ğ‘™\nğ‘›ğ‘¤=ğ‘›\nğ‘¤(6)\nInterestingly, this means to ensure fairness, we need to allocate\nenough bins to each group such that expected number of element\ntypes hashed to each bucket remains unchanged , i.e.,ğ‘›\nğ‘¤. This helps\nus in extending ğ‘‘to more than one row by equalizing the expected\nnumber of element types hashed in each bin across various groups\nto ensure fairness.1 2Â·Â·Â·ğ‘¤ğ‘™ğ‘¤ğ‘™+1Â·Â·Â·ğ‘¤ğ‘¤ğ‘™ ğ‘¤â„=ğ‘¤âˆ’ğ‘¤ğ‘™\n1\n2\n...\nğ‘‘\nFigure 2: Illustration of a column-partitioning based group-\nfair min-count with ğ‘‘rows.\n3.2 General Value of ğ‘‘\nHaving established that a count-min sketch with a single group-\naware semi-fairness hash function ( ğ‘‘=1) ensures group fairness,\nwe now generalize this result to the case of ğ‘‘hash functions.\nSimilar toğ‘‘=1, our goal is to partition the ğ‘¤columns across\nvarious groups such that the group fairness requirement is satisfied,\ni.e., all groups have the same expected approximation factor.\nWe recall from Theorem 1 that achieving group fairness requires\nequalizing the expected number of element types hashed into the\nminimum-count bucket across ğ‘‘rows for all groups.\nWhenğ‘‘=1, the estimated frequency of an element is the fre-\nquency count of the bucket it is hashed into. On the other hand,\nwhenğ‘‘>1, a frequency estimation ğ‘“Ë†(ğ‘’)is the minimum of the\nfrequency counts of the buckets ğ‘’is hashed to at each row.\nLet us define the size of a bucket as the number of element types\nhashed to it. For an element ğ‘’ğ‘—âˆˆğ‘™and a rowğ‘–, letâ„“=hğ‘–(ğ‘’ğ‘—).\nLet the random integer variable ğ‘‹ğ‘–be the size of the bucket of\nthe element ğ‘’at rowğ‘–, i.e.,|{ğ‘’âˆˆğ‘™|hğ‘–(ğ‘’)=â„“}|. The elements are\nhashed uniformly at random and independently into the bins. As\na result, the probability that a random element is hashed into the\nbucketâ„“is1\nğ‘¤ğ‘™. As a result, ğ‘‹ğ‘–follows the binomial distribution\nğ‘‹ğ‘–âˆ¼ğµğ‘–ğ‘›(ï¸‚\nğ‘›ğ‘™,1\nğ‘¤ğ‘™)ï¸‚\n:\nPr(ğ‘‹ğ‘–=ğ‘¥)=(ï¸ƒğ‘›ğ‘™\nğ‘¥)ï¸ƒ\nÂ·(ï¸ƒ1\nğ‘¤ğ‘™)ï¸ƒğ‘¥\nÂ·(ï¸ƒğ‘¤ğ‘™âˆ’1\nğ‘¤ğ‘™)ï¸ƒğ‘›ğ‘™âˆ’ğ‘¥\nLetğœ‡ğ‘™be the expected frequency for group ğ‘™. Then, the expected\nfrequency count of each bucket is ğœ‡ğ‘™times the size of the bucket.\nFollowing this argument, we simplify our analysis by equalizing the\nexpected number of element types hashed into the minimum-size\nbucket across ğ‘‘rows for all groups.\nFix an element ğ‘’ğ‘—in the group ğ‘™. Letğ‘‹1,Â·Â·Â·,ğ‘‹ğ‘‘be the random\nvariables reflecting the sizes of the ğ‘‘bucketsğ‘’ğ‘—is hashed to across\nvarious rows, while each ğ‘‹ğ‘–following the Binomial distribution\nğ‘‹1,Â·Â·Â·,ğ‘‹ğ‘‘âˆ¼ğµğ‘–ğ‘›(ï¸ƒ\nğ‘›ğ‘™,1\nğ‘¤ğ‘™)ï¸ƒ\nDefine the random variable ğ‘Œğ‘—as the minimum of ğ‘‹1toğ‘‹ğ‘‘. That\nis,\nğ‘Œâˆ¼min(ğ‘‹1,Â·Â·Â·,ğ‘‹ğ‘‘).\n4\n\nWe aim to find the expected value of ğ‘Œ:\nE[ğ‘Œ]=ğ‘›âˆ‘ï¸‚\nğ‘¥=1ğ‘¥ğ‘ƒ(ğ‘Œ=ğ‘¥)\nSince the domain of the random variable ğ‘Œis the nonnegative inte-\ngers{1,2,Â·Â·Â·}, the expected value can alternatively be computed\nusing the following equation [38].\nE[ğ‘Œ]=ğ‘›âˆ‘ï¸‚\nğ‘¥=1ğ‘ƒ(ğ‘Œâ‰¥ğ‘¥)\nPr(ğ‘Œâ‰¥ğ‘¥)=Pr(min(ğ‘‹1,Â·Â·Â·,ğ‘‹ğ‘‘)â‰¥ğ‘¥)=Pr(ğ‘‹1â‰¥ğ‘¥,Â·Â·Â·,ğ‘‹ğ‘‘â‰¥ğ‘¥).\nSinceğ‘‹1,Â·Â·Â·,ğ‘‹ğ‘‘are independent and identically distributed, the\nprobability is computed as:\nPr(ğ‘Œâ‰¥ğ‘¥)=Pr(ğ‘‹1â‰¥ğ‘¥,Â·Â·Â·,ğ‘‹ğ‘‘â‰¥ğ‘¥)\n=ğ‘‘âˆï¸‚\nğ‘–=1Pr(ğ‘‹ğ‘–â‰¥ğ‘¥)=Pr(ğ‘‹â‰¥ğ‘¥)ğ‘‘\nwhere Pr(ğ‘‹=ğ‘¥)=(ï¸ğ‘›ğ‘™\nğ‘¥)ï¸Â·(ï¸‚\n1\nğ‘¤ğ‘™)ï¸‚ğ‘¥\nÂ·(ï¸‚ğ‘¤ğ‘™âˆ’1\nğ‘¤ğ‘™)ï¸‚ğ‘›ğ‘™âˆ’ğ‘¥\n. Hence, the proba-\nbility Pr(ğ‘‹â‰¥ğ‘¥)is:\nPr(ğ‘‹â‰¥ğ‘¥)=ğ‘›ğ‘™âˆ‘ï¸‚\nğ‘–=ğ‘¥(ï¸ƒğ‘›ğ‘™\nğ‘–)ï¸ƒ (ï¸ƒ1\nğ‘¤ğ‘™)ï¸ƒğ‘–(ï¸ƒğ‘¤ğ‘™âˆ’1\nğ‘¤ğ‘™)ï¸ƒğ‘›ğ‘™âˆ’ğ‘–\n.\nPutting everything together, we obtain the expected value of ğ‘Œ\nas:\nE[ğ‘Œ]=ğ‘›ğ‘™âˆ‘ï¸‚\nğ‘¥=1Pr(ğ‘‹â‰¥ğ‘¥)ğ‘‘\n=ğ‘›ğ‘™âˆ‘ï¸‚\nğ‘¥=1[ï¸„ğ‘›ğ‘™âˆ‘ï¸‚\nğ‘–=ğ‘¥(ï¸ƒğ‘›ğ‘™\nğ‘–)ï¸ƒ (ï¸ƒ1\nğ‘¤ğ‘™)ï¸ƒğ‘–(ï¸ƒğ‘¤ğ‘™âˆ’1\nğ‘¤ğ‘™)ï¸ƒğ‘›ğ‘™âˆ’ğ‘–]ï¸„ğ‘‘\n. (7)\nFinally, equalizing Equation 7 for the groups ğ‘™andâ„, we specify\nthe number of columns allocated to each group as ğ‘¤ğ‘™andğ‘¤â„=\nğ‘¤âˆ’ğ‘¤ğ‘™, by solving the Equation 8 ( ğ‘›â„=ğ‘›âˆ’ğ‘›ğ‘™):\nEğ‘’âˆˆğ‘™[ğ‘Œ]=Eğ‘’âˆˆâ„[ğ‘Œ]â‡’\nğ‘›ğ‘™âˆ‘ï¸‚\nğ‘¥=1[ï¸„ğ‘›ğ‘™âˆ‘ï¸‚\nğ‘–=ğ‘¥(ï¸ƒğ‘›ğ‘™\nğ‘–)ï¸ƒ (ï¸ƒ1\nğ‘¤ğ‘™)ï¸ƒğ‘–(ï¸ƒğ‘¤ğ‘™âˆ’1\nğ‘¤ğ‘™)ï¸ƒğ‘›ğ‘™âˆ’ğ‘–]ï¸„ğ‘‘\n(8)\n=ğ‘›âˆ’ğ‘›ğ‘™âˆ‘ï¸‚\nğ‘¥=1[ï¸„ğ‘›âˆ’ğ‘›ğ‘™âˆ‘ï¸‚\nğ‘–=ğ‘¥(ï¸ƒğ‘›âˆ’ğ‘›ğ‘™\nğ‘–)ï¸ƒ (ï¸ƒ1\nğ‘¤âˆ’ğ‘¤ğ‘™)ï¸ƒğ‘–(ï¸ƒğ‘¤âˆ’ğ‘¤ğ‘™âˆ’1\nğ‘¤âˆ’ğ‘¤ğ‘™)ï¸ƒğ‘›âˆ’ğ‘›ğ‘™âˆ’ğ‘–]ï¸„ğ‘‘\nLater in Section 4, we shall provide an efficient algorithm for\nsolving Equation 8.\n3.3 Negative Result: Row Partitioning Baseline\nFCM follows a column partitioning approach, using the group-\naware semi-uniform hashing schemes. Alternatively, one can par-\ntition the rows by allocating a specific number of exclusive rows\nto each group, where the allocation of rows to each group is deter-\nmined by the number of element types within that group (Figure 3).1 2Â·Â·Â·ğ‘¤\n1\n...\nğ‘‘ğ‘™\nğ‘‘ğ‘™+1\n...\nğ‘‘ğ‘‘â„=ğ‘‘âˆ’ğ‘‘ğ‘™ ğ‘‘ğ‘™\nFigure 3: Illustration of the row-partitioning baseline.\nFollowing Equation 7, the expected size of the minimum-size\nbucket across the ğ‘‘â€²rows,ğ‘¤â€²columns, and a group with ğ‘›â€²elements\ncan be computed as,\nE(ğ‘›â€²,ğ‘‘â€²,ğ‘¤â€²)=ğ‘›â€²âˆ‘ï¸‚\nğ‘¥=1[ï¸„ğ‘›â€²âˆ‘ï¸‚\nğ‘–=ğ‘¥(ï¸ƒğ‘›â€²\nğ‘–)ï¸ƒ (ï¸ƒ1\nğ‘¤â€²)ï¸ƒğ‘–(ï¸ƒğ‘¤â€²âˆ’1\nğ‘¤â€²)ï¸ƒğ‘›â€²âˆ’ğ‘–]ï¸„ğ‘‘â€²\n(9)\nLetğ‘‘ğ‘™andğ‘‘â„=ğ‘‘âˆ’ğ‘‘ğ‘™be the number of rows allocated to the\ngroupsğ‘™andâ„. Then, we can find the proper values of ğ‘‘ğ‘™andğ‘‘â„\nthat achieve fairness by solving the following equation:\nE(ğ‘›ğ‘™,ğ‘‘ğ‘™,ğ‘¤)=E(ğ‘›â„,ğ‘‘â„,ğ‘¤) (10)\nThere, however, is a major issue making this approach unlikely\nto work: unlike ğ‘¤, which is a relatively large number, ğ‘‘is a small\nconstant (usually in the order of tens). As a result, there are only\na small, constant number of choices for ğ‘‘ğ‘™âˆˆ[ğ‘‘]. This, in practice,\nmakes it unlikely for ğ‘‘to be divisible such that Equation 10 is\nsatisfied. We shall verify this negative result in our experiments in\nSection 6.\n3.4 Generalization to Multiple Arbitrary Groups\nSo far, we have explained our techniques for the binary grouping\nof the elements based on their frequencies.\nOur approach works for any (arbitrary) grouping of the elements\nsince none of our definitions, results, and analyses are based on\nany assumption about how elements are grouped. This is also ob-\nserved in our experiments in Section 6, where we, for example, use\ndemographic groups to group the elements.\nExtending our results to the non-binary grouping of the element\ntypes is straightforward. Let G={g1,g2,Â·Â·Â·,gâ„“}be the non-binary\nset of groups. For every group gğ‘™âˆˆG, transform the problem into\nthe binary grouping by merging all other groups as the super-group\nâ€œothersâ€. Then, for ğ‘‘=1, following Theorem 1, ğ‘¤ğ‘™=ğ‘›ğ‘™\nğ‘›ğ‘¤. Similarly,\nforğ‘‘>1, the value of ğ‘¤ğ‘™,âˆ€gğ‘™âˆˆG, can be computed by solving\nEquation 8. We propose an efficient algorithm for finding ğ‘¤ğ‘™values\nwhenğ‘‘>1in Section 4.\n5\n\n4 COMPUTING THE COLUMN WIDTHS\nOur algorithm is straightforward; we should compute the integer\nvalue ofğ‘¤ğ‘™such that Equation (8)holds. We note that Equation (8)\nmay not hold for an integer value of ğ‘¤ğ‘™. In this case the goal is to\ncompute the integer value of ğ‘¤ğ‘™such that the left side of Equation (8)\nis as close as possible to the right side of Equation (8). A naive\nimplementation is to try every value of ğ‘¤ğ‘™and calculate the value\nof the left and right side of Equation (8)by simply evaluate every\nterm in the sum. Such an implementation would take Î©(ğ‘›3)time\nbecause we should try ğ‘‚(ğ‘›)values ofğ‘¤ğ‘™, and for each such value\nthe calculation of each side of Equation (8)takes Î©(ğ‘›2)time to\nevaluate the summations.\nWe first show an exact near-linear time algorithm to compute\nthe value of ğ‘¤ğ‘™that solves Equation 8 and then we briefly discuss\nan approximation algorithm that is used in our experiments to\nefficiently evaluate the sums of the binomial function.\nEfficient exact computation. We first assume that we have two\ngroups, the high-frequency elements and the low-frequency el-\nements. Then we extend our method to multiple groups G=\n{g1,..., gâ„“}. Similarly, to the previous section, let E(ğ‘›ğ‘™,ğ‘‘,ğ‘¤ğ‘™)be\nthe function that represents the left side of Equation 8. Equivalently,\nthe right side of Equation (8)is captured byE(ğ‘›â„,ğ‘‘,ğ‘¤âˆ’ğ‘¤ğ‘™). By\ndefinition, the function Ecomputes the expected error of light (resp.\nheavy) elements. The values ğ‘›ğ‘™,ğ‘›â„, andğ‘‘are fixed, so the more\nbins allocated to light (respectively, heavy) items, the smaller the\nexpected error. Hence, the function E(ğ‘›ğ‘™,ğ‘‘,ğ‘¤ğ‘™)is monotone non-\nincreasing with respect to ğ‘¤ğ‘™, whileE(ğ‘›â„,ğ‘‘,ğ‘¤âˆ’ğ‘¤ğ‘™)is monotone\nnon-decreasing with respect to ğ‘¤ğ‘™. This property leads to the obser-\nvation that instead of trying all values of ğ‘¤ğ‘™, we can run a binary\nsearch in the range [1,...,ğ‘›]and compute the best value of ğ‘¤ğ‘™. The\noverall idea of our algorithm is the following. For every value of ğ‘¤ğ‘™\nin the binary search, we evaluate E(ğ‘›ğ‘™,ğ‘‘,ğ‘¤ğ‘™)andE(ğ‘›â„,ğ‘‘,ğ‘¤âˆ’ğ‘¤ğ‘™).\nIfE(ğ‘›ğ‘™,ğ‘‘,ğ‘¤ğ‘™)<E(ğ‘›â„,ğ‘‘,ğ‘¤âˆ’ğ‘¤ğ‘™)then we try larger values of ğ‘¤ğ‘™\nin the binary search. Otherwise, we continue the binary search\nwith smaller values. In the end, we return the value of ğ‘¤ğ‘™found\nby the binary search such that |E(ğ‘›ğ‘™,ğ‘‘,ğ‘¤ğ‘™)âˆ’E(ğ‘›â„,ğ‘‘,ğ‘¤âˆ’ğ‘¤ğ‘™)|is\nminimized.\nEven if someone naively applies binary search, the running time\nwould be Î©(ğ‘›2)to evaluate the sums. Given a value of ğ‘¤ğ‘™we show\nhow to computeE(ğ‘›ğ‘™,ğ‘‘,ğ‘¤ğ‘™)in near-linear time with respect to ğ‘›.\nThe computation of E(ğ‘›â„,ğ‘‘,ğ‘¤âˆ’ğ‘¤ğ‘™)is equivalent. First, for ğ‘¥=1\nwe compute the sumâˆ‘ï¸ğ‘›ğ‘™\nğ‘–=1(ï¸ğ‘›ğ‘™\nğ‘–)ï¸(ï¸‚\n1\nğ‘¤ğ‘™)ï¸‚ğ‘–(ï¸‚ğ‘¤ğ‘™âˆ’1\nğ‘¤ğ‘™)ï¸‚ğ‘›ğ‘™âˆ’ğ‘–as follows. For\nğ‘–=1, we compute ğ‘1=(ï¸ğ‘›ğ‘™\n1)ï¸,ğ‘¡1=1\nğ‘¤ğ‘™, andğ‘ 1=(ï¸‚ğ‘¤ğ‘™âˆ’1\nğ‘¤ğ‘™)ï¸‚ğ‘›ğ‘™âˆ’1\n. We\nstoreğ›¼1=ğ‘1Â·ğ‘¡1Â·ğ‘ 1. Then forğ‘–âˆˆ{2,...,ğ‘›ğ‘™}, we observe that\nğ‘ğ‘–=ğ‘ğ‘–âˆ’1Â·ğ‘›ğ‘™+1âˆ’ğ‘–\nğ‘–,ğ‘¡ğ‘–=ğ‘¡ğ‘–âˆ’1Â·1\nğ‘¤ğ‘™,ğ‘ ğ‘–=ğ‘ ğ‘–âˆ’1Â·ğ‘¤ğ‘™\nğ‘¤ğ‘™âˆ’1, and we set\nğ›¼ğ‘–=ğ‘ğ‘–Â·ğ‘¡ğ‘–Â·ğ‘ ğ‘–. Then, we set ğ›½ğ‘›ğ‘™=ğ›¼ğ‘›ğ‘™and for every ğ‘–=ğ‘›ğ‘™âˆ’1,..., 1,\nwe compute ğ›½ğ‘–=ğ›¼ğ‘–+ğ›½ğ‘–+1. The main observation of our algorithm\nis thatE(ğ‘›ğ‘™,ğ‘‘,ğ‘¤ğ‘™)=âˆ‘ï¸ğ‘›ğ‘™\nğ‘¥=1ğ›½ğ‘‘ğ‘¥.\nIn order to analyze the runtime of our algorithm, we assume a\nmodification of the real-RAM model of computation where every\nbasic arithmetic operation is computed in ğ‘‚(1)time. We note that\nfor any pair of positive integer numbers ğ‘,ğ‘˜, it holds that(ï¸ğ‘\nğ‘˜)ï¸=\nâˆï¸ğ‘˜\nğ‘–=1ğ‘+1âˆ’ğ‘–\nğ‘–so(ï¸ğ‘\nğ‘˜)ï¸can be computed in ğ‘‚(ğ‘˜)time. Finally, the ğ‘˜-th\npower of any real number can be computed in ğ‘‚(logğ‘˜)time.We analyze the runtime in one iteration of the binary search. By\ndefinition of our model of computation, we compute ğ‘1,ğ‘¡1inğ‘‚(1)\ntime andğ‘ 1inğ‘‚(logğ‘›)time. Then ğ›¼1is computed in ğ‘‚(1)time.\nFor every value of ğ‘–âˆˆ{2,...,ğ‘›ğ‘™}, givenğ‘ğ‘–âˆ’1,ğ‘¡ğ‘–âˆ’1,ğ‘ ğ‘–âˆ’1the values\nofğ‘ğ‘–,ğ‘¡ğ‘–,ğ‘ ğ‘–âˆ’1and henceğ›¼ğ‘–are computed in ğ‘‚(1)time. So all values\nğ›¼ğ‘–forğ‘–âˆˆ{1,...,ğ‘›ğ‘™}are computed in ğ‘‚(ğ‘›+logğ‘›)=ğ‘‚(ğ‘›)time in\ntotal. Next, we observe that given ğ›½ğ‘–+1, the valueğ›½ğ‘–is computed in\nğ‘‚(1)time, so all values ğ›½ğ‘–are computed in ğ‘‚(ğ‘›)time. Finally, the\nsumâˆ‘ï¸ğ‘›ğ‘™\nğ‘¥=1ğ›½ğ‘‘ğ‘¥is computed in ğ‘‚(ï¸âˆ‘ï¸ğ‘›\nğ‘¥=1log(ğ‘‘))ï¸=ğ‘‚(ğ‘›logğ‘‘)time.\nThe binary search is executed for ğ‘‚(logğ‘›)iterations, so in total the\nrunning time of our algorithm is ğ‘‚(ğ‘›log(ğ‘›)log(ğ‘‘)). Notice that\nusually,ğ‘›â‰«ğ‘‘so the total running time is bounded by ğ‘‚(ğ‘›log2ğ‘›).\nWe can extend our method to multiple groups G={g1,..., gâ„“}.\nFor each group gğ‘—âˆˆG, the functionE(ğ‘›ğ‘—,ğ‘‘,ğ‘¤ğ‘—)represents the\nexpected error of the elements in group gğ‘—as computed by Equa-\ntion (7). The goal is to compute ğ‘¤ğ‘—for every gğ‘—âˆˆG, given thatâˆ‘ï¸\nğ‘—âˆˆ[â„“]ğ‘¤ğ‘—=ğ‘¤. We solve this problem, by recursively execute our\nalgorithm for two groups (light and heavy) â„“âˆ’1times. Intuitively,\nif we haveâ„“groups, we construct an instance with two colors: one\nthat consists of the elements in the first group, and one that con-\nsists of the elements from all groups excluding the first group. For\na valueğ‘—âˆˆ{1,...,â„“âˆ’1}, letğ‘Šğ‘—be the number of bins that should\nbe assigned to groups gğ‘—,gğ‘—+1...,gâ„“. Initially, notice that ğ‘Š1=ğ‘¤.\nWe setğ‘›â€²\nğ‘—=âˆ‘ï¸â„“\nğ‘=ğ‘—+1ğ‘›ğ‘. We assume that there exist two groups, one\nwithğ‘›ğ‘—elements and the other one with ğ‘›â€²\nğ‘—elements, while the\ntotal number of bins is ğ‘Šğ‘—. We solve this instance using our efficient\nimplementation for two groups. Let ğ‘¤ğ‘—be the number we compute.\nWe setğ‘Šğ‘—+1â†ğ‘Šğ‘—âˆ’ğ‘¤ğ‘—and we continue recursively with ğ‘—â†ğ‘—+1.\nThe overall time of our algorithm for groups G={g1,..., gâ„“}is\nğ‘‚(â„“Â·ğ‘›Â·log2ğ‘›).\nApproximate practical implementation. While in theory, our pre-\nvious algorithm works in the real-RAM model of computation, a\ndirect calculation of(ï¸ğ‘›\nğ‘–)ï¸might cause overflow in a practical imple-\nmentation. In practice, while we run the binary search on ğ‘¤ğ‘™, instead\nof computing the binomial coefficients exactly, we use Stirlingâ€™s\napproximation, i.e. for every positive integer ğ‘˜,ğ‘˜!â‰ˆ(ï¸‚\nğ‘˜\ne)ï¸‚ğ‘˜\nÂ·âˆš\n2ğœ‹ğ‘˜,\nwhere eis the Eulerâ€™s number. It is known that the Gamma func-\ntionÎ“(ğ‘§)=âˆ«âˆ\n0ğ‘¡ğ‘§âˆ’1eâˆ’ğ‘¡ğ‘‘ğ‘¡satisfies Î“(ğ‘˜)=ğ‘˜!. Taking logarithms\nconverts the products in the definition of the binomial coefficient\ninto sums of logs which are easier to handle and avoid overflow.\nThen we approximate each logÎ“(Â·)term with Stirlingâ€™s expansion,\nlogÎ“(ğ‘§)â‰ˆğ‘§logğ‘§âˆ’ğ‘§+1\n2log(2ğœ‹)+1\n12ğ‘§+ğ‘‚(ğ‘§âˆ’3). In our practical\nimplementation, we use the logarithm of Î“function to estimate the\nvalues ofE(ğ‘›ğ‘™,ğ‘‘,ğ‘¤ğ‘™)andE(ğ‘›â„,ğ‘‘,ğ‘¤âˆ’ğ‘¤ğ‘™)for each value ğ‘¤ğ‘™in the\nbinary search.\n5 PRICE OF FAIRNESS ANALYSIS\nImproving fairness usually comes at a cost, known as the â€œprice\nof fairnessâ€ (aka cost of fairness) â€“ the reduction in the overall\nperformance as a result of resolving unfairness [ 14]. After intro-\nducing FCM and its details, in this section, we study its price of\nfairness. Following Equation 1, the overall performance of a CM\nsketch can be measured in terms of the sum of its expected additive\n6\n\nerror across all element types U={ğ‘’1,Â·Â·Â·,ğ‘’ğ‘›}. That is,\nLğ¶ğ‘€=ğ‘›âˆ‘ï¸‚\nğ‘–=1ğœ€ğ´(ğ‘’ğ‘–) (11)\nSubsequently, we define the price of fairness (PoF) of a FCM sketch\nas the increase in its total expected additive error, compared to the\nstandard CM sketch. That is,\nğ‘ƒğ‘œğ¹=Lğ¹ğ¶ğ‘€âˆ’Lğ¶ğ‘€ (12)\nIn the following, first, we provide our price of fairness analysis\nforğ‘‘=1, proving that fairness even improves the expected per-\nformance, i.e., ğ‘ƒğ‘œğ¹<0, whenğ‘‘=1. Forğ‘‘>1, however, fairness\ncomes at a positive (but small) cost.\n5.1ğ‘‘=1\nLet us begin by computing Lğ¶ğ‘€, the total additive error for the\nstandard CM sketch. By Chapter 3 of [ 28], the expected additive\nerror of an element type ğ‘’ğ‘—âˆˆU is\nğœ€ğ´(ğ‘’ğ‘—)=ğ‘âˆ’ğ‘“(ğ‘’ğ‘—)\nğ‘¤.\nThen, the total additive error of the standard CM sketch is computed\nas\nLğ¶ğ‘€=ğ‘›âˆ‘ï¸‚\nğ‘—=1ğœ€ğ´(ğ‘’ğ‘—)=ğ‘›âˆ‘ï¸‚\nğ‘—=1ğ‘âˆ’ğ‘“(ğ‘’ğ‘—)\nğ‘¤=ğ‘›ğ‘\nğ‘¤âˆ’ğ‘\nğ‘¤=(ğ‘›âˆ’1)ğ‘\nğ‘¤\n(13)\nNow, let us compute Lğ¹ğ¶ğ‘€ , the total additive error for the FCM\nsketch.\nLğ¹ğ¶ğ‘€=ğ‘›âˆ‘ï¸‚\nğ‘—=1ğœ€ğ´(ğ‘’ğ‘—)=âˆ‘ï¸‚\nğ‘’ğ‘—âˆˆg1ğœ€ğ´(ğ‘’ğ‘—)+Â·Â·Â·+âˆ‘ï¸‚\nğ‘’ğ‘—âˆˆgâ„“ğœ€ğ´(ğ‘’ğ‘—)\nFollowing the same calculation as in Equation (13), for every group\ngğ‘™=G,\nâˆ‘ï¸‚\nğ‘’ğ‘—âˆˆgğ‘™ğœ€ğ´(ğ‘’ğ‘—)=(ğ‘›ğ‘™âˆ’1)ğ‘ğ‘™\nğ‘¤ğ‘™\nHence,\nLğ¹ğ¶ğ‘€=âˆ‘ï¸‚\nğ‘’ğ‘—âˆˆg1ğœ€ğ´(ğ‘’ğ‘—)+Â·Â·Â·+âˆ‘ï¸‚\nğ‘’ğ‘—âˆˆgâ„“ğœ€ğ´(ğ‘’ğ‘—)=â„“âˆ‘ï¸‚\nğ‘™=1(ğ‘›ğ‘™âˆ’1)ğ‘ğ‘™\nğ‘¤ğ‘™\nFrom Theorem 1, we know, ğ‘¤ğ‘™=ğ‘›ğ‘™\nğ‘›ğ‘¤,âˆ€gğ‘™âˆˆG, whileâˆ‘ï¸â„“\nğ‘™=1ğ‘ğ‘™=\nğ‘. Therefore,\nLğ¹ğ¶ğ‘€=â„“âˆ‘ï¸‚\nğ‘™=1(ğ‘›ğ‘™âˆ’1)ğ‘ğ‘™\nğ‘¤ğ‘™\n=â„“âˆ‘ï¸‚\nğ‘™=1(ğ‘›ğ‘™âˆ’1)ğ‘ğ‘™\nğ‘›ğ‘™\nğ‘›ğ‘¤=â„“âˆ‘ï¸‚\nğ‘™=1ğ‘›(ï¸ƒ\n1âˆ’1\nğ‘›ğ‘™)ï¸ƒğ‘ğ‘™\nğ‘¤\n=ğ‘›â„“âˆ‘ï¸‚\nğ‘™=1ğ‘ğ‘™\nğ‘¤âˆ’â„“âˆ‘ï¸‚\nğ‘™=1ğ‘›\nğ‘›ğ‘™Â·ğ‘ğ‘™\nğ‘¤=ğ‘›ğ‘\nğ‘¤âˆ’â„“âˆ‘ï¸‚\nğ‘™=1ğ‘›\nğ‘›ğ‘™Â·ğ‘ğ‘™\nğ‘¤(14)Interestingly, combining Equations 12, 13, and 14, we show that\nğ‘ƒğ‘œğ¹<0, for a CM with random hashing scheme:\nğ‘ƒğ‘œğ¹=Lğ¹ğ¶ğ‘€âˆ’Lğ¶ğ‘€\nâ€˜=ğ‘›ğ‘\nğ‘¤âˆ’â„“âˆ‘ï¸‚\nğ‘™=1(ï¸ƒğ‘›\nğ‘›ğ‘™Â·ğ‘ğ‘™\nğ‘¤)ï¸ƒ\nâˆ’(ğ‘›âˆ’1)ğ‘\nğ‘¤\n=ğ‘\nğ‘¤âˆ’â„“âˆ‘ï¸‚\nğ‘™=1ğ‘›\nğ‘›ğ‘™Â·ğ‘ğ‘™\nğ‘¤\n<ğ‘\nğ‘¤âˆ’â„“âˆ‘ï¸‚\nğ‘™=1ğ‘ğ‘™\nğ‘¤//sinceğ‘›\nğ‘›ğ‘™>1,âˆ€ğ‘”ğ‘™âˆˆG\n=ğ‘\nğ‘¤âˆ’ğ‘\nğ‘¤=0\nThe negative PoF sounds counterintuitive, but as we shall further\nelaborate in our appendix , it can be explained by the difference\nbetween the random distribution of the elements versus their uni-\nform distribution to the buckets. We further show that the PoF of\nFCM is zero for ğ‘‘=1when uniform hashing is used.\n5.2ğ‘‘>1\nNext, we consider the price of fairness for multiple rows ğ‘‘>1. The\nerror of an element type ğ‘’ğ‘—âˆˆU is\nmin\nğ‘âˆˆ[ğ‘‘]âˆ‘ï¸‚\nğ‘’â‰ ğ‘’ğ‘—:hğ‘(ğ‘’)=hğ‘(ğ‘’ğ‘—)ğ‘“(ğ‘’),\nso the expected additive error is\nğœ€ğ´(ğ‘’ğ‘—)=Eâ¡â¢â¢â¢â¢â£min\nğ‘âˆˆ[ğ‘‘]âˆ‘ï¸‚\nğ‘’â‰ ğ‘’ğ‘—:hğ‘(ğ‘’)=hğ‘(ğ‘’ğ‘—)ğ‘“(ğ‘’)â¤â¥â¥â¥â¥â¦. (15)\nFor the standard CM sketch the total additive error is computed\nasLğ¶ğ‘€=âˆ‘ï¸ğ‘›\nğ‘—=1ğœ€ğ´(ğ‘’ğ‘—)using Equation (15)to compute the sum\nconsidering ğ‘¤buckets. For the FCM sketch the total additive er-\nror is computed as Lğ¹ğ¶ğ‘€=âˆ‘ï¸â„“\nğ‘™=1âˆ‘ï¸\nğ‘’ğ‘—âˆˆgğ‘™ğœ€ğ´(ğ‘’ğ‘—), where each sumâˆ‘ï¸\nğ‘’ğ‘—âˆˆgğ‘™ğœ€ğ´(ğ‘’ğ‘—)is computed using Equation (15)forğ‘¤ğ‘™buckets. Since\nwe do not have a closed form for the ğ‘¤ğ‘™values (forğ‘‘>1) from\nEquation (8), we cannot directly compute ğ‘ƒğ‘œğ¹=Lğ¶ğ‘€âˆ’Lğ¹ğ¶ğ‘€.\nInstead, we compute the PoF experimentally in real and synthetic\ndatasets in Section 6.6. Particularly, Table 1 and other experiments,\nincluding Figures 11 to 16 confirm the small PoF for ğ‘‘>1.\n6 EXPERIMENTS\nIn addition to the theoretical analysis, we perform extensive exper-\niments across a range of settings to validate the fairness as well as\nthe efficiency of our proposed sketch. Consistent with the theoreti-\ncal guarantees established in previous sections, the experimental\nresults confirm the effectiveness and efficiency of our sketch in\npractical scenarios.\n6.1 Experiments Setup\nThe experiments were conducted on an Apple M2 Pro processor, 16\nGB memory, running macOS. The algorithms were implemented in\nPython 3.12.\n7\n\n6.2 Datasets\nFor evaluation, we used two real-world datasets along with several\nsynthetic ones. To assess the scalability of our proposed sketch in\nlarge-scale scenarios, we selected datasets that are sufficiently large\nto reflect real-world streaming environments.\nCensus: This dataset contains a one percent sample of the Public\nUse Microdata Samples person records derived from the complete\n1990 census dataset. It includes around 2.5 million records with 68\ncategorical attributes. We use this dataset for experiments involving\narbitrarily defined groups. Entity types are defined by concatenating\nthe values of iYearsch ,dIndustry , and the grouping attribute. For\nbinary group experiments, the iSex attribute is used to define the\ngroups resulting in 430 element types. To extend the grouping to â„“\ngroups, we utilize other attributes with higher cardinalities.\nGoogle N-Grams: This dataset is a large-scale collection of n-gram\nfrequency counts extracted from texts in books digitized through\nthe Google Books project. It has been used in related research to\nevaluate frequency estimation sketches. In our evaluations, we use\na sample of 2-grams that begin with a_to generate a stream of\napproximately 1.25 million element types, with a total frequency of\naround 5 million. We utilize this dataset for experiments in which\ngroups are defined based on the frequency of elements associated\nwith a given element type. If an elementâ€™s frequency is below a\nspecified threshold, it is assigned to group ğ‘™; otherwise, it is assigned\nto groupâ„. This approach is further extended to partition elements\nintoâ„“frequency-based groups.\nSynthetic: Finally, we generate several synthetic datasets in which\nthe frequency of element types follows different distributions, in-\ncluding Gaussian, Zipfian, Exponential, and Uniform. We apply\na frequency-based grouping to these datasets, similar to the ap-\nproach used for the google n-grams dataset. Each dataset contains\nğ‘›=20,000element types, with the total frequency determined by\nthe parameters of the underlying distribution. In our experiments\non unfairness and the price of fairness, we report results for the\ncase where frequencies are drawn from a Zipfian distribution.\n6.3 Baselines\nTo demonstrate the effectiveness of our approach, we compare it\nagainst two baseline methods. The first baseline is the standard\nCount-Min sketch, which we use across all of our experiments.\nThe second baseline is the Row-Partitioning approach discussed\nin Section 3.3, which, as expected, does not consistently achieve\nequal group-wise approximation factors in practice. We use this\nbaseline in a subset of our experiments to illustrate the negative\nresult of row-partitioning.\n6.4 Evaluation Plan\nWe evaluate our proposed sketch based on three metrics 1) un-\nfairness, 2) price of fairness and 3) efficiency. For each metric, we\nexamine the impact of varying four parameters: the number of ele-\nment types in the disadvantaged group ğ‘›ğ‘™, the width of the sketch\nğ‘¤, the depth of the sketch ğ‘‘, and the number of groups â„“. Due to\nspace constraints and the confirmed similarity of results across\ndifferent datasets, for certain experiments, we present results for asingle dataset per setting, with the full results available in appen-\ndix. Finally, we repeat each experiment five times and report the\naverage values for each setting.\n6.5 Unfairness\nIn this experiment, we measure the impact of varying certain pa-\nrameters on the unfairness of the sketch. Unfairness is measured\nas the difference between the expected approximation factor of\nthe disadvantaged and advantaged groups, i.e., E[ğ›¼ğ‘™]âˆ’E[ğ›¼â„]. In\nthe multi-group setting, unfairness is measured as the difference\nbetween the maximum and minimum mean approximation factors\nacross all groups, i.e., minâˆ€gğ‘–âˆˆG(E[ğ›¼gğ‘–])âˆ’ maxâˆ€gğ‘—âˆˆG(E[ğ›¼gğ‘—]). A\nsketch is considered group-fair if the unfairness value is zero.\nVarying group size: We begin with the experiment that examines\nthe impact of varying the number of element types ğ‘›ğ‘™in the unpop-\nular group on unfairness. To do so, we vary the cutoff threshold on\nthe frequency of each element: elements with frequencies below\nthe threshold are assigned to group ğ‘™, while those above it are as-\nsigned to group â„. As the threshold increases, ğ‘›ğ‘™grows while ğ‘›â„\ndecreases, keeping the total number of elements ğ‘›constant. Let us\nnow focus on Figure 4. As ğ‘›ğ‘™increases, the standard CM sketch\nexhibits greater unfairness because the expected approximation\nfactor for group ğ‘™decreases as the group grows larger, while group\nâ„becomes smaller. This is due to the standard CM sketch favoring\nmore accurate estimates for the smaller group â„, which contains\nhigh-frequency elements, over the larger group ğ‘™, which consists\nof numerous low-frequency elements. For the Row-Partitioning\nbaseline, the optimal integral values of ğ‘‘ğ‘™andğ‘‘â„selected by the\nalgorithm gradually deviate from the optimal fractional values de-\ntermined by the equality 10. Specifically, with ğ‘‘=5, the sketch\nwould ideally assign a value 4<ğ‘‘ğ‘™<5(closer to 5) to group ğ‘™,\nwhich is not feasible. As a result, we observe a significant disparity\nbetween the approximation factors of the groups for larger values of\nğ‘›ğ‘™. On the other hand, the FCM maintains the approximation factor\ndifference near zero, regardless of the value of ğ‘›ğ‘™. Results conveying\na similar message for the synthetic dataset are also presented in\nFigure 5.\nVarying the number of columns: Next, we examine the impact\nof varying the sketch width ğ‘¤on unfairness. An instance of this\nexperiment is illustrated in Figure 6. Since ğ‘›ğ‘™=1.2ğ‘€is relatively\nlarge, the approximation factors for all groups and among all ap-\nproaches are initially close to zero because the values of ğ‘¤are\ncomparatively small. For the standard CM baseline, as ğ‘¤increases,\nthe approximation factor for group â„gradually approaches 1, while\nit remains relatively low for group ğ‘™. After surpassing a certain\nthreshold,ğ‘¤becomes large enough for the mean approximation\nfactor of group ğ‘™to also approach 1. For the Row-Partitioning base-\nline, asğ‘¤initially increases, it struggles to find suitable integral\nğ‘‘ğ‘™values to allocate rows to group ğ‘™. However, after surpassing a\ncertain threshold, as the sketch becomes wide enough to accommo-\ndate fewer elements per bucket, the approximation factor difference\nbegins to decrease. As expected, FCM maintains a zero difference\n2For this experiment, we used a larger sample of 18 million elements as the stream to\nensure that all groups contained a meaningful number of element types. This accounts\nfor the larger additive error values observed in this dataset compared to the other\nexperiments.\n8\n\nFigure 4: effect of varying disadvantaged\ngroup size ğ‘›ğ‘™on unfairness, google n-\ngrams ,ğ‘¤=65536,ğ‘‘=5.\nFigure 5: effect of varying disadvantaged\ngroup size ğ‘›ğ‘™on unfairness, synthetic ,\nğ‘›=20ğ¾,ğ‘¤=512,ğ‘‘=10.\nFigure 6: effect of varying sketch width\nğ‘¤on unfairness, google n-grams ,ğ‘›=\n1.2ğ‘€,ğ‘‘=5.\nFigure 7: effect of varying sketch depth\nğ‘‘on unfairness, google n-grams ,ğ‘›=\n1.2ğ‘€,ğ‘¤ =65536 .\nFigure 8: effect of varying sketch depth\nğ‘‘on unfairness, censusğ‘›=430,ğ‘¤=64.\nFigure 9: effect of varying number of\ngroupsâ„“on unfairness, google n-grams ,\nğ‘›=1.2ğ‘€,ğ‘¤ =65536,ğ‘‘=5.\nFigure 10: effect of varying number of\ngroupsâ„“on unfairness, census ,ğ‘›=430,ğ‘¤=\n64,ğ‘‘=10.\nFigure 11: effect of varying disadvan-\ntaged group size ğ‘›ğ‘™on price of fairness,\ngoogle n-grams ,ğ‘¤=65536,ğ‘‘=5.\nFigure 12: effect of varying sketch width\nğ‘¤on price of fairness, google n-grams ,\nğ‘›=1.2ğ‘€,ğ‘‘=5.\nFigure 13: effect of varying sketch depth\nğ‘‘on price of fairness, google n-grams ,\nğ‘›=1.2ğ‘€,ğ‘¤ =65536 .\nFigure 14: effect of varying sketch depth\nğ‘‘on price of fairness, census ,ğ‘›=430,ğ‘¤=\n64.\nFigure 15: effect of varying number of\ngroupsâ„“on price of fairness, google n-\ngrams ,ğ‘›=1.2ğ‘€,ğ‘¤ =65536,ğ‘‘=52.\n9\n\nbetween the approximation factors of the two groups, regardless of\nthe value of ğ‘¤.\nVarying the number of rows: Next, we evaluate the impact of\nincreasing the sketch depth ğ‘‘on unfairness. The results are pre-\nsented in Figures 7 and 8. For the standard CM baseline, the sketch\ninitially provides more accurate estimates (approximation factors\ncloser to 1) for group â„. Asğ‘‘increases, the accuracy improves for\nboth groups, but at a slightly faster rate for group â„. As before,\nFCM maintains zero unfairness regardless of the sketch depth. For\nthe Row-Partitioning approach, we note some interesting obser-\nvations in As shown in Figure 8, depending on the value of ğ‘‘and\nits divisibility, the unfairness fluctuates between nearly zero and\na significant value as ğ‘‘increases. In this experiment, ğ‘›ğ‘™andğ‘›â„\nare very close, so as long as the sketch can assign equal ğ‘‘ğ‘™andğ‘‘â„\nvalues to each group, it performs well w.r.t keeping unfairness at\nzero. This occurs only when ğ‘‘takes even values.\nVarying the number of groups: In our final experiment on un-\nfairness, we investigate the impact of increasing the number of\ngroups. The results are shown in Figures 9 and 10. Having demon-\nstrated that the Row-Partitioning baseline performs inconsistently\nand is highly sensitive to the value of ğ‘‘, we henceforth compare\nour approach only with the standard CM sketch. As expected, FCM\nmaintains zero unfairness, independent of the number of groups\nor the grouping strategy used. However, for the standard CM, an\nincrease in the number of groups generally leads to higher un-\nfairness. This is particularly noticeable in the case of equi-width\nfrequency-based grouping, where the sketch is more accurate for\nsmaller groups with high-frequency elements and less accurate for\nthose larger groups with low-frequency elements (Figure 9). In the\ncase of arbitrary grouping, unfairness is entirely dependent on the\nsize of each group. As shown in Figure 10, the number of groups is\ndetermined based on a different attribute with specific cardinality at\neach iteration. While the general trend is an increase in unfairness\nas the number of groups grows, in some iterations (e.g., 5 and 10),\nthe unfairness values are closer, indicating that the groups are more\nevenly sized. In contrast, for iteration 9, one group is significantly\nlarger or smaller than the others, resulting in higher unfairness.\n6.6 Price of Fairness\nThus far, we have established that FCM guarantees group-fairness\nregardless of the parameters involved. In the next set of experiments,\nwe examine the cost of enforcing this constraint compared to the\nstandard CM. Following the discussion in Section 5 and Equation 12,\nwe measure the price of fairness in terms of the total additive error\nrequired to achieve fairness.\nFirst, let us empirically validate the interesting result that PoF is\nnegative for ğ‘‘=1, proven in Section 5. To do so, we use a synthetic\ndataset where the frequencies of groups ğ‘™andâ„are drawn from\nGaussian distributions with ğœ‡ğ‘™=100,ğœğ‘™=50andğœ‡â„=1000,ğœâ„=\n500, respectively, and a total of ğ‘›=10,000element types, we vary\nthe unpopular group size ğ‘›ğ‘™from 9000 to 1000. The price of fairness\nis computed as the difference in total additive error between FCM\nand standard CM sketches ( PoF=LFCMâˆ’L CM). The results, along\nwith the empirical and theoretical additive errors shown in Table 1,emphasize the superiority of FCM over standard CM in terms of\nadditive error when ğ‘‘=1.\nHowever, when ğ‘‘>1, FCM incurs a positive but small price\nof fairness, and is generally outperformed by standard CM. We re-\npeated the above experiment with ğ‘‘=1, and the results in Table 1\nfurther support our findings. Still, we notice that the PoF is much\nsmaller in the real datasets compared to the synthetic dataset. That\nis because synthetic frequency values are generated using a Gauss-\nian distribution, resulting in larger values with lower variance. This\nis further clear when comparing Table 1 with Figure 11 â€“ the results\non the google n-grams dataset, where one can confirm the small\ngap between the total additive error of the CM (the blue line) with\nFCM (the orange line). To further evaluate the price of fairness\nforğ‘‘>1across various settings, similar to the previous section,\nwe varied (a) group size (Figure 11), (b) number of columns (Fig-\nure 12), (c) number of rows (Figures 13 and 14), and (d) the number\nof groups (Figures 15 and 16), and measured the total additive error\nfor CM versus FCM. The results across all settings were consistent,\nconveying a unified message: FCM achieves group-fairness at a\nnegligible, if any, price of fairness. Due to the space limitations,\nextensive results are moved to the appendix. We confirm that we\nobserved similar results across all settings and all other datasets.\n6.7 Efficiency\nHaving established the efficacy of the FCM sketch, we now turn\nour attention to its efficiency. A frequency-estimation sketch in-\nvolves two main stages: (1) construction and (2) query time. In the\nfollowing, we evaluate the impact of varying key parameters on\nthe time required for each stage and provide a comparison with the\nstandard CM sketch:\n6.7.1 Construction Time. We define the construction time as the\nduration required to initialize the sketch and insert the entire stream\ninto it. In summary, the construction time of FCM is comparable to\nthat of the standard CM. For both sketches, the construction time\nis independent of the disadvantaged group size ğ‘›ğ‘™(Figure 17), the\nsketch width ğ‘¤(Figure 18), and the number of groups â„“(Figure 20);\nvarying these parameters has no impact on construction time. The\nconstruction time for both sketches grows linearly with the sketch\ndepthğ‘‘, as expected, since it corresponds to the number of times\neach element is inserted into the sketch. This result is presented in\nFigure 19.\n6.7.2 Query Time. Query time is defined as the time required for\nthe sketch to perform an estimate operation for a given element.\nThe query time results exhibit similar patterns to the construction\ntimes, with FCM and standard CM showing comparable values\nand trends. The query times are independent of the disadvantaged\ngroup sizeğ‘›ğ‘™(Figure 21), the sketch width ğ‘¤(Figure 22), and the\nnumber of groups â„“(Figure 24). Similarly, the query time for both\nsketches also increases linearly with the sketch depth ğ‘‘, as shown\nin Figure 23.\n6.8 Validating Group Columns Allocation\nIn our final experiment, we evaluate experimentally validate our\nmathematical analysis in Section 3.2, using a Monte Carlo method\n10\n\nTotal Additive Error ğ‘›ğ‘™(ğ‘›=10,000)\n9000 8000 7000 6000 5000 4000 3000 2000 1000\ntheoreticalCM 19,047,019 28,054,816 37,081,001 46,092,289 55,121,050 64,147,002 73,186,882 82,323,243 91,265,426ğ‘‘=1FCM 18,985,224 28,028,987 37,003,399 46,050,674 55,115,538 64,089,586 73,165,549 82,249,909 91,228,281\nempiricalCM 19,039,343 28,085,806 37,053,427 46,096,489 55,113,194 64,234,176 73,227,052 82,283,374 91,328,074\nFCM 18,991,509 27,982,573 37,002,246 46,071,394 55,086,890 64,133,815 73,168,189 82,230,649 91,276,271\nPoFâ†“-47,834â†“-103,233â†“-51,181â†“-25,095â†“-26,304â†“-100,361â†“-58,863â†“-52,725â†“-51,803ğ‘‘=5\nempiricalCM 7,964,348 11,572,548 16,458,026 22,023,181 28,305,699 34,442,308 40,959,703 47,230,290 54,257,770\nFCM 11,695,556 17,114,933 22,666,115 28,053,739 33,856,154 39,448,942 44,913,211 50,089,698 55,893,637\nPoFâ†‘3,731,208â†‘5,542,385â†‘6,208,089â†‘6,030,558â†‘5,550,455â†‘5,006,634â†‘3,953,508â†‘2,859,408â†‘1,635,867\nTable 1: comparison of CM and FCM w.r.t additive errors and the price of fairness across different ğ‘›ğ‘™values forğ‘‘=1andğ‘‘=5.\nFigure 16: effect of varying number of\ngroupsâ„“on price of fairness, census ,ğ‘›=\n430,ğ‘¤=64,ğ‘‘=10.\nFigure 17: effect of varying disadvan-\ntaged group size ğ‘›ğ‘™on construction time,\ngoogle n-grams ,ğ‘›=1.2ğ‘€,ğ‘¤ =65536 .\nFigure 18: effect of varying sketch width\nğ‘¤on construction time, google n-grams ,\nğ‘›=1.2ğ‘€,ğ‘‘=5.\nFigure 19: effect of varying sketch depth\nğ‘‘on construction time, google n-grams ,\nğ‘›=1.2ğ‘€,ğ‘¤ =65536 .\nFigure 20: effect of varying number of\ngroupsâ„“on construction time, google n-\ngrams ,ğ‘›=1.2ğ‘€,ğ‘¤ =65536,ğ‘‘=5.\nFigure 21: effect of varying disadvan-\ntaged group size ğ‘›ğ‘™on query time, google\nn-grams ,ğ‘›=1.2ğ‘€,ğ‘¤ =65536 .\nFigure 22: effect of varying sketch width\nğ‘¤on query time, google n-grams ,ğ‘›=\n1.2ğ‘€,ğ‘‘=5.\nFigure 23: effect of varying sketch depth\nğ‘‘on query time, google n-grams ,ğ‘›=\n1.2ğ‘€,ğ‘¤ =65536 .\nFigure 24: effect of varying number of\ngroupsâ„“on query time, google n-grams ,\nğ‘›=1.2ğ‘€,ğ‘¤ =65536,ğ‘‘=5.\n11\n\nfor estimating the value of ğ‘¤ğ‘™â€“ the number of columns allocated\nto the group ğ‘™.\nWe note that instead of the mathematical derivation, one could\ndevelop the following Monte Carlo method based on repeated sam-\npling to estimate the value of ğ‘¤ğ‘™:\nConsiderğ‘›ğ‘™,ğ‘›â„,ğ‘¤,ğ‘‘, and the parameters of arbitrary distribu-\ntions from which the frequencies of groups ğ‘™andâ„are drawn. For\neach group gâˆˆ{ğ‘™,â„}, we drawğ‘›gsamples from the corresponding\ndistribution to serve as the frequencies of the elements. As a result,\nwe obtain a list of ğ‘›tuples of the form (frequency, group) . Next,\nwe perform a binary search over the range of ğ‘¤to find the optimal\nvalue ofğ‘¤ğ‘™that minimize the empirical difference in approximation\nfactors between the two groups, where the empirical difference is\nevaluated by inserting the sampled frequencies into a FCM sketch\nwith 1 row and measuring the approximation factors for each group.\nThis procedure is repeated ğ‘‘times (once for each sketch row), and\nwe compute the average of the resulting ğ‘¤ğ‘™values. Finally, we\nrepeat the entire process over multiple iterations and report the\noverall average of ğ‘¤ğ‘™.\nWe use this Monte Carlo method to estimate ğ‘¤ğ‘™under a range\nof synthetic distributions. We note from our analysis that the value\nofğ‘¤ğ‘™does not depend on the distribution of data. Hence, we expect\nto observe similar results from different runs of the Monte Carlo\nestimation. We then compare the number of columns assigned to\neach group by this method to the theoretical values derived from\nthe analysis in Section 3.2. The results are shown in Figure 25. The\nvalues ofğ‘¤ğ‘™were identical across different frequency distributions\nand, as expected, depended solely on the values of ğ‘›ğ‘™andğ‘›â„.\nFigure 25: Monte Carlo-based approach to find-\ningğ‘¤ğ‘™vs. theoretical value. ğ‘¤ğ‘™only depends on\nğ‘›ğ‘™and is independent of the distribution of fre-\nquencies of each group.\n7 RELATED WORK\nCount-Min sketch. The CM sketch was first described by Cor-\nmode and Muthukrishnan [ 26,27]. It uses hash functions to map\nevents to frequencies, but unlike a hash table uses only sub-linear\nspace, at the expense of overcounting some events due to collisions.\nCountâ€“min sketch is an alternative to count sketch [21] and AMS\nsketch [ 4] and can be considered an implementation of a count-\ning Bloom filter [ 36,41] or multistage-filter [ 26]. Over the years\nmultiple variations of min-count sketches have been proposed in\nthe literature [ 39,54,62,64,76] improving the estimation or the\nperformance under different settings. While CM sketches havebeen thoroughly studied there is still room for improvement. For\nexample, the random hashing procedure may induce substantial\nuncertainty in the estimation, especially for low-frequency tokens.\nFurthermore, it is often the case that there exists an a priori knowl-\nedge on the data, and therefore it may be desirable to incorporate\nsuch a knowledge into the estimates\nRecently, machine learning is used to resolve these issues. More\nspecifically, machine learning techniques [ 1,18,31,32,46,59,80]\nare used to learn either the hash functions or a partition of the\nelements into two groups (based on their frequency) to improve\nthe estimations or the performance compared to the traditional CM\nsketch. Interestingly, partitioned-learned CM sketches [ 46,59] learn\na partition of the elements into low frequency and high frequency\nelements. While this partition is the same as the one we used in our\nanalysis, our objective is orthogonal to their problem. They do not\nnecessarily aim to achieve the same estimation error between low\nand high frequency elements. Instead, they process light and heavy\nelements separately trying to improve the overall estimation error.\nTo the best of our knowledge, none of these traditional and learned\nschemes can handle fair count-min estimations with theoretical\nguarantees. Finally, learning has been used to obtain other data\nstructures as well, such as ğµ-trees [ 49] or bloom filters [ 49,56,77].\nFairness. Fairness in data-driven systems has been studied by var-\nious research communities but mostly in machine learning (ML) [ 13,\n55,61]. Most of the existing work is on training a ML model that\nsatisfies some fairness constraints. Some pioneering fair-ML efforts\ninclude [ 19,20,35,37,42,48,78]. Biases in data has also been stud-\nied extensively [ 8,9,60,65,69,74] to ensure data has been prepared\nresponsibly [ 58,66,67,71]. Recent studies of fair algorithm design\ninclude fair clustering [ 2,15,17,23,24,45,52,68], fairness in re-\nsource allocation and facility location problem [ 16,33,40,43,47,82],\nmin cut [ 50], max cover [ 6], set cover [ 29], game theoretic ap-\nproaches [ 3,34,81], hiring [ 5,63], ranking [ 7,72,73,79], recom-\nmendation [22, 51, 75], representation learning [44], etc.\nFairness in data structures is significantly under-studied, with\nthe existing work being limited to [ 10â€“12,70]. Aumuller et al. [ 10â€“\n12] study individual fairness in near-neighbor search, while to the\nbest of our knowledge, Shahbazi et al. [ 70] is the only work that\nstudies group fairness in data structure design, and more specifically\nin hashing. Both in nearest-neighbor search and fair hashing, the\nobjective is fundamentally different, so these techniques cannot be\nused for our fair-CM sketch.\n8 CONCLUSION\nIn this paper we introduced Fair-Count-Min, a new frequency esti-\nmation framework that ensures equal expected approximation fac-\ntors across different groups, addressing a key fairness limitation of\ntraditional Count-Min sketches. By proposing a fairness-preserving\ndesignâ€”column partitioning with group-aware hashing, our work\noffers strong theoretical guarantees and validates its effectiveness\nthrough extensive experiments. Future research directions include\nextending Fair-Count-Min integrating learned hash functions for\nfurther efficiency gains, and exploring fairness notions beyond\ngroup-wise guarantees, such as individual fairness in streaming\ndata sketches.\n12\n\nREFERENCES\n[1]Anders Aamand, Piotr Indyk, and Ali Vakilian. 2019. frequency estimation\nalgorithms under Zipfian distribution. arXiv preprint arXiv:1908.05198 (2019).\n[2]Sara Ahmadian, Alessandro Epasto, Marina Knittel, Ravi Kumar, Mohammad\nMahdian, Benjamin Moseley, Philip Pham, Sergei Vassilvitskii, and Yuyan Wang.\n2020. Fair hierarchical clustering. Advances in Neural Information Processing\nSystems 33 (2020), 21050â€“21060.\n[3]EncarnaciÃ³n Algaba, Vito Fragnelli, and JoaquÃ­n SÃ¡nchez-Soriano. 2019. The\nShapley value, a paradigm of fairness. Handbook of the Shapley value (2019),\n17â€“29.\n[4]Noga Alon, Yossi Matias, and Mario Szegedy. 1996. The space complexity of\napproximating the frequency moments. In Proceedings of the twenty-eighth annual\nACM symposium on Theory of computing . 20â€“29.\n[5]Mohammad Reza Aminian, Vahideh Manshadi, and Rad Niazadeh. 2023. Fair\nmarkovian search. Available at SSRN 4347447 (2023).\n[6]Abolfazl Asudeh, Tanya Berger-Wolf, Bhaskar DasGupta, and Anastasios\nSidiropoulos. 2023. Maximizing coverage while ensuring fairness: A tale of\nconflicting objectives. Algorithmica 85, 5 (2023), 1287â€“1331.\n[7]Abolfazl Asudeh, HV Jagadish, Julia Stoyanovich, and Gautam Das. 2019. De-\nsigning fair ranking schemes. In Proceedings of the 2019 international conference\non management of data . 1259â€“1276.\n[8] Abolfazl Asudeh, Zhongjun Jin, and HV Jagadish. 2019. Assessing and remedying\ncoverage for a given dataset. In 2019 IEEE 35th International Conference on Data\nEngineering (ICDE) . IEEE, 554â€“565.\n[9]Abolfazl Asudeh, Nima Shahbazi, Zhongjun Jin, and HV Jagadish. 2021. Iden-\ntifying insufficient data coverage for ordinal continuous-valued attributes. In\nProceedings of the 2021 international conference on management of data . 129â€“141.\n[10] Martin Aumuller, Sariel Har-Peled, Sepideh Mahabadi, Rasmus Pagh, and\nFrancesco Silvestri. 2021. Fair near neighbor search via sampling. ACM SIGMOD\nRecord 50, 1 (2021), 42â€“49.\n[11] Martin AumÃ¼ller, Sariel Har-Peled, Sepideh Mahabadi, Rasmus Pagh, and\nFrancesco Silvestri. 2022. Sampling a Near Neighbor in High Dimensionsâ€”Who\nis the Fairest of Them All? ACM Transactions on Database Systems (TODS) 47, 1\n(2022), 1â€“40.\n[12] Martin AumÃ¼ller, Rasmus Pagh, and Francesco Silvestri. 2020. Fair near neighbor\nsearch: Independent range sampling in high dimensions. In Proceedings of the\n39th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems\n(PODS) . 191â€“204.\n[13] Solon Barocas, Moritz Hardt, and Arvind Narayanan. 2017. Fairness in machine\nlearning. Nips tutorial 1 (2017), 2017.\n[14] Solon Barocas, Moritz Hardt, and Arvind Narayanan. 2023. Fairness and machine\nlearning: Limitations and opportunities . MIT press.\n[15] Suman Bera, Deeparnab Chakrabarty, Nicolas Flores, and Maryam Negahbani.\n2019. Fair algorithms for clustering. Advances in Neural Information Processing\nSystems 32 (2019).\n[16] VÃ­ctor Blanco and Ricardo GÃ¡zquez. 2023. Fairness in maximal covering location\nproblems. Computers & Operations Research 157 (2023), 106287.\n[17] Matteo BÃ¶hm, Adriano Fazzone, Stefano Leonardi, and Chris Schwiegelshohn.\n2020. Fair clustering with multiple colors. arXiv preprint arXiv:2002.07892 (2020).\n[18] Diana Cai, Michael Mitzenmacher, and Ryan P Adams. 2018. A Bayesian non-\nparametric view on count-min sketch. Advances in neural information processing\nsystems 31 (2018).\n[19] Flavio Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Rama-\nmurthy, and Kush R Varshney. 2017. Optimized pre-processing for discrimination\nprevention. Advances in neural information processing systems 30 (2017).\n[20] L Elisa Celis, Lingxiao Huang, Vijay Keswani, and Nisheeth K Vishnoi. 2019.\nClassification with fairness constraints: A meta-algorithm with provable guaran-\ntees. In Proceedings of the conference on fairness, accountability, and transparency .\n319â€“328.\n[21] Moses Charikar, Kevin Chen, and Martin Farach-Colton. 2002. Finding frequent\nitems in data streams. In International colloquium on automata, languages, and\nprogramming . Springer, 693â€“703.\n[22] Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan\nHe. 2023. Bias and debias in recommender system: A survey and future directions.\nACM Transactions on Information Systems 41, 3 (2023), 1â€“39.\n[23] Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, and Sergei Vassilvitskii. 2017.\nFair clustering through fairlets. Advances in neural information processing systems\n30 (2017).\n[24] Eden ChlamtÃ¡Ä, Yury Makarychev, and Ali Vakilian. 2022. Approximating fair\nclustering with cascaded norm objectives. In Proceedings of the 2022 annual\nACM-SIAM symposium on discrete algorithms (SODA) . SIAM, 2664â€“2683.\n[25] Saar Cohen and Yossi Matias. 2003. Spectral bloom filters. In Proceedings of the\n2003 ACM SIGMOD international conference on Management of data . 241â€“252.\n[26] Graham Cormode. 2009. Count-Min Sketch . Springer US, 511â€“516. https:\n//doi.org/10.1007/978-0-387-39940-9_87\n[27] Graham Cormode and Shan Muthukrishnan. 2005. An improved data stream\nsummary: the count-min sketch and its applications. Journal of Algorithms 55, 1\n(2005), 58â€“75.[28] Graham Cormode and Ke Yi. 2020. Small summaries for big data . Cambridge\nUniversity Press.\n[29] Mohsen Dehghankar, Rahul Raychaudhury, Stavros Sintos, and Abolfazl Asudeh.\n2025. Fair Set Cover. KDD (2025).\n[30] Fan Deng and Davood Rafiei. 2007. New estimation algorithms for streaming\ndata: Count-min can do more. Webdocs. Cs. Ualberta. Ca (2007).\n[31] Emanuele Dolera, Stefano Favaro, and Stefano Peluchetti. 2021. A Bayesian\nnonparametric approach to count-min sketch under power-law data streams. In\nInternational Conference on Artificial Intelligence and Statistics . PMLR, 226â€“234.\n[32] Emanuele Dolera, Stefano Favaro, and Stefano Peluchetti. 2023. Learning-\naugmented count-min sketches via Bayesian nonparametrics. Journal of Machine\nLearning Research 24, 12 (2023), 1â€“60.\n[33] Kate Donahue and Jon Kleinberg. 2020. Fairness and utilization in allocating\nresources with uncertain demand. In Proceedings of the 2020 conference on fairness,\naccountability, and transparency . 658â€“668.\n[34] Kate Donahue and Jon Kleinberg. 2023. Fairness in model-sharing games. In\nProceedings of the ACM Web Conference 2023 . 3775â€“3783.\n[35] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard\nZemel. 2012. Fairness through awareness. In Proceedings of the 3rd innovations\nin theoretical computer science conference . ACM, 214â€“226.\n[36] Li Fan, Pei Cao, Jussara Almeida, and Andrei Z Broder. 2000. Summary cache:\na scalable wide-area web cache sharing protocol. IEEE/ACM transactions on\nnetworking 8, 3 (2000), 281â€“293.\n[37] Michael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and\nSuresh Venkatasubramanian. 2015. Certifying and removing disparate impact.\nInProceedings of the 21th ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining . ACM, 259â€“268.\n[38] William Feller. 1991. An introduction to probability theory and its applications,\nVolume 2 . Vol. 2. John Wiley & Sons.\n[39] Ã‰ric Fusy and Gregory Kucherov. 2023. Count-min sketch with variable number\nof hash functions: an experimental study. In International Symposium on String\nProcessing and Information Retrieval . Springer, 218â€“232.\n[40] Pranay Gorantla, Kunal Marwaha, and Santhoshini Velusamy. 2023. Fair al-\nlocation of a multiset of indivisible items. In Proceedings of the 2023 Annual\nACM-SIAM Symposium on Discrete Algorithms (SODA) . SIAM, 304â€“331.\n[41] Deke Guo, Yunhao Liu, Xiangyang Li, and Panlong Yang. 2010. False negative\nproblem of counting bloom filter. IEEE transactions on knowledge and data\nengineering 22, 5 (2010), 651â€“664.\n[42] Moritz Hardt, Eric Price, and Nati Srebro. 2016. Equality of opportunity in\nsupervised learning. In Advances in neural information processing systems . 3315â€“\n3323.\n[43] Yuzi He, Keith Burghardt, Siyi Guo, and Kristina Lerman. 2020. Inherent trade-\noffs in the fair allocation of treatments. arXiv preprint arXiv:2010.16409 (2020).\n[44] Yuzi He, Keith Burghardt, and Kristina Lerman. 2020. A geometric solution to\nfair representations. In Proceedings of the AAAI/ACM Conference on AI, Ethics,\nand Society . 279â€“285.\n[45] Sedjro Salomon Hotegni, Sepideh Mahabadi, and Ali Vakilian. 2023. Approx-\nimation Algorithms for Fair Range Clustering. In International Conference on\nMachine Learning . PMLR, 13270â€“13284.\n[46] Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. 2019. Learning-based\nfrequency estimation algorithms.. In International Conference on Learning Repre-\nsentations .\n[47] Yanmin Jiang, Xiaole Wu, Bo Chen, and Qiying Hu. 2021. Rawlsian fairness in\npush and pull supply chains. European Journal of Operational Research 291, 1\n(2021), 194â€“205.\n[48] Faisal Kamiran and Toon Calders. 2012. Data preprocessing techniques for\nclassification without discrimination. Knowledge and Information Systems 33, 1\n(2012), 1â€“33.\n[49] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe case for learned index structures. In Proceedings of the 2018 international\nconference on management of data . 489â€“504.\n[50] Jason Li, Danupon Nanongkai, Debmalya Panigrahi, and Thatchaphol Saranurak.\n2023. Near-Linear Time Approximations for Cut Problems via Fair Cuts. In\nProceedings of the 2023 Annual ACM-SIAM Symposium on Discrete Algorithms\n(SODA) . SIAM, 240â€“275.\n[51] Yunqi Li, Hanxiong Chen, Shuyuan Xu, Yingqiang Ge, Juntao Tan, Shuchang\nLiu, and Yongfeng Zhang. 2022. Fairness in recommendation: A survey. arXiv\npreprint arXiv:2205.13619 (2022).\n[52] Yury Makarychev and Ali Vakilian. 2021. Approximation algorithms for socially\nfair clustering. In Conference on Learning Theory . PMLR, 3246â€“3264.\n[53] Younes Ben Mazziane, Sara Alouf, and Giovanni Neglia. 2022. Analyzing count\nmin sketch with conservative updates. Computer Networks 217 (2022), 109315.\n[54] Younes Ben Mazziane and Othmane Marfoq. 2024. Count-Min Sketch with\nConservative Updates: Worst-Case Analysis. arXiv preprint arXiv:2405.12034\n(2024).\n[55] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram\nGalstyan. 2021. A survey on bias and fairness in machine learning. ACM\ncomputing surveys (CSUR) 54, 6 (2021), 1â€“35.\n13\n\n[56] Michael Mitzenmacher. 2018. A model for learned bloom filters and optimizing\nby sandwiching. Advances in Neural Information Processing Systems 31 (2018).\n[57] Rajeev Motwani and Prabhakar Raghavan. 1996. Randomized algorithms. ACM\nComputing Surveys (CSUR) 28, 1 (1996), 33â€“37.\n[58] Fatemeh Nargesian, Abolfazl Asudeh, and HV Jagadish. 2021. Tailoring data\nsource distributions for fairness-aware data integration. Proceedings of the VLDB\nEndowment 14, 11 (2021), 2519â€“2532.\n[59] Thuy Trang Nguyen and Cameron N Musco. [n. d.]. Partitioned-Learned Count-\nMin Sketch. ([n. d.]).\n[60] Alexandra Olteanu, Carlos Castillo, Fernando Diaz, and Emre KÄ±cÄ±man. 2019.\nSocial data: Biases, methodological pitfalls, and ethical boundaries. Frontiers in\nbig data 2 (2019), 13.\n[61] Dana Pessach and Erez Shmueli. 2022. A review on fairness in machine learning.\nACM Computing Surveys (CSUR) 55, 3 (2022), 1â€“44.\n[62] Guillaume Pitel and Geoffroy Fouquier. 2015. Count-Min-Log sketch: Approxi-\nmately counting with approximate counters. In International Symposium on Web\nAlGorithms .\n[63] Manish Raghavan, Solon Barocas, Jon Kleinberg, and Karen Levy. 2020. Mitigat-\ning bias in algorithmic hiring: Evaluating claims and practices. In Proceedings of\nthe 2020 conference on fairness, accountability, and transparency . 469â€“481.\n[64] Ori Rottenstreich, Pedro Reviriego, Ely Porat, and S Muthukrishnan. 2021. Avoid-\ning flow size overestimation in Count-Min sketch with Bloom filter constructions.\nIEEE Transactions on Network and Service Management 18, 3 (2021), 3662â€“3676.\n[65] Babak Salimi, Bill Howe, and Dan Suciu. 2019. Data management for causal\nalgorithmic fairness. arXiv preprint arXiv:1908.07924 (2019).\n[66] Babak Salimi, Bill Howe, and Dan Suciu. 2020. Database repair meets algorithmic\nfairness. ACM SIGMOD Record 49, 1 (2020), 34â€“41.\n[67] Babak Salimi, Luke Rodriguez, Bill Howe, and Dan Suciu. 2019. Interventional\nfairness: Causal database repair for algorithmic fairness. In Proceedings of the\n2019 International Conference on Management of Data . 793â€“810.\n[68] Melanie Schmidt, Chris Schwiegelshohn, and Christian Sohler. 2020. Fair coresets\nand streaming algorithms for fair k-means. In Approximation and Online Algo-\nrithms: 17th International Workshop, WAOA 2019, Munich, Germany, September\n12â€“13, 2019, Revised Selected Papers 17 . Springer, 232â€“251.\n[69] Nima Shahbazi, Yin Lin, Abolfazl Asudeh, and HV Jagadish. 2023. Representation\nBias in Data: A Survey on Identification and Resolution Techniques. Comput.\nSurveys (2023).\n[70] Nima Shahbazi, Stavros Sintos, and Abolfazl Asudeh. 2024. Fairhash: A fair and\nmemory/time-efficient hashmap. Proceedings of the ACM on Management of Data\n2, 3 (2024), 1â€“29.\n[71] Suraj Shetiya, Ian P Swift, Abolfazl Asudeh, and Gautam Das. 2022. Fairness-\naware range queries for selecting unbiased data. In ICDE . IEEE, 1423â€“1436.\n[72] Ashudeep Singh and Thorsten Joachims. 2018. Fairness of exposure in rankings.\nInProceedings of the 24th ACM SIGKDD international conference on knowledge\ndiscovery & data mining . 2219â€“2228.\n[73] Ashudeep Singh and Thorsten Joachims. 2019. Policy learning for fairness in\nranking. Advances in neural information processing systems 32 (2019).\n[74] Julia Stoyanovich, Serge Abiteboul, Bill Howe, HV Jagadish, and Sebastian Schel-\nter. 2022. Responsible data management. Commun. ACM 65, 6 (2022), 64â€“74.\n[75] Ian P Swift, Sana Ebrahimi, Azade Nova, and Abolfazl Asudeh. 2022. Maximizing\nfair content spread via edge suggestion in social networks. Proceedings of the\nVLDB Endowment 15, 11 (2022), 2692â€“2705.\n[76] Daniel Ting. 2018. Count-min: Optimal estimation and tight error bounds using\nempirical error distributions. In Proceedings of the 24th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining . 2319â€“2328.\n[77] Kapil Vaidya, Eric Knorr, Michael Mitzenmacher, and Tim Kraska. 2020. Parti-\ntioned Learned Bloom Filters. In International Conference on Learning Represen-\ntations .\n[78] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P\nGummadi. 2017. Fairness beyond disparate treatment & disparate impact: Learn-\ning classification without disparate mistreatment. In Proceedings of the 26th\nInternational Conference on World Wide Web . International World Wide Web\nConferences Steering Committee, 1171â€“1180.\n[79] Meike Zehlike, Ke Yang, and Julia Stoyanovich. 2022. Fairness in ranking, part i:\nScore-based ranking. Comput. Surveys 55, 6 (2022), 1â€“36.\n[80] Meifan Zhang, Hongzhi Wang, Jianzhong Li, and Hong Gao. 2020. Learned\nsketches for frequency estimation. Information Sciences 507 (2020), 365â€“385.\n[81] Yan Zhao, Kai Zheng, Jiannan Guo, Bin Yang, Torben Bach Pedersen, and Chris-\ntian S Jensen. 2021. Fairness-aware task assignment in spatial crowdsourcing:\nGame-theoretic approaches. In 2021 IEEE 37th International Conference on Data\nEngineering (ICDE) . IEEE, 265â€“276.[82] Liping Zhou, Na Geng, Zhibin Jiang, and Xiuxian Wang. 2019. Public hospi-\ntal inpatient room allocation and patient scheduling considering equity. IEEE\nTransactions on Automation Science and Engineering 17, 3 (2019), 1124â€“1139.\nAPPENDIX\nA PRICE OF FAIRNESS: RANDOMNESS VS.\nUNIFORMITY\nIn Section 5, we observed a counterintuitive result for ğ‘‘=1, proving\na negative price of fairness (PoF) for FCM, compared to a regular\nCM sketch that uses a random hash function. This result can be\nexplained by the fact that a random hash function is unlikely to\nuniformly distribute the elements to the buckets (an interesting\nrelated topic is the occupant problem [57]). As a result, some of the\nbuckets will have more thanğ‘›\nğ‘¤elements in them. For example,\nFigure 26 illustrates a random hashing of ğ‘›=100element types\nintoğ‘¤=10buckets. While a uniform distribution would allocate\n10elements to each bucket, the random hashing allocated up to 18\nelements in one bucket.\n1 2 3 4 5 6 7 8 91005101520\n918\n11\n813\n810\n7\n610\nBucket NumberSize\nFigure 26: Illustration of a random distribution of ğ‘›=100\nelements to ğ‘¤=10buckets.\nElements in large buckets will suffer from a large additive error,\nand there are a large number of them in such buckets. Hence, such\nbuckets significantly increase the total additive error of the CM.\nFCM increases the size-uniformity of the buckets by reserving\nğ‘¤ğ‘™=ğ‘›ğ‘™\nğ‘›ğ‘¤for each group ğ‘”ğ‘™. For example, Figure 27 illustrates\nthe allocation of the 100 elements to the 10 buckets, by dividing\n(arbitrarily) the elements into two groups of size ğ‘›ğ‘™=50,ğ‘›â„=50.\nAs a result, each group is allocated 5buckets, and the distribution\nof the elements is more uniform, reducing the maximum size of the\nbuckets (in this example) to 14. Increasing the uniformity (reducing\nthe change of large-size buckets) helps to reduce the total additive\nerror of FCM versus CM.\n14\n\n1 2 3 4 5 6 7 8 91005101520\n1114\n9 9\n713\n711\n910\nBucket NumberSize\nFigure 27: Illustration of a random distribution of ğ‘›=100\nelements with two groups of sizes ğ‘›ğ‘™=50andğ‘›â„=50to\nğ‘¤=10buckets, based on FCM.\nA.1 PoF for Uniform hashing ( ğ‘‘=1)\nUsing a hashing scheme (such as data-informed hashmaps [ 49])\nthat ensures uniformity by allocating exactlyğ‘›\nğ‘¤elements to each\nbucket can resolve the non-uniformity issue in the CM sketches.\nIn the following, we compute the PoF of SCM for ğ‘‘=1when a\nuniform hashing scheme is used.\nSince the hashing is uniform, the size of each bucket ğ‘–in the CM\nsketch is\nğ¶ğ‘–=ğ‘›\nğ‘¤\nTherefore, each element ğ‘’ğ‘—collides with exactlyğ‘›\nğ‘¤âˆ’1other ele-\nments. In other words, the frequency of each element contributes\nto the additive error of exactlyğ‘›\nğ‘¤âˆ’1other elements.\nAs a result, the total additive error can be computed as\nLğ¶ğ‘€=ğ‘›âˆ‘ï¸‚\nğ‘—=1ğœ€ğ´(ğ‘’ğ‘—)\n=ğ‘›âˆ‘ï¸‚\nğ‘—=1ğ‘“(ğ‘’ğ‘—)(ï¸‚ğ‘›\nğ‘¤âˆ’1)ï¸‚\n=(ï¸‚ğ‘›\nğ‘¤âˆ’1)ï¸‚ğ‘›âˆ‘ï¸‚\nğ‘—=1ğ‘“(ğ‘’ğ‘—)\n=ğ‘(ï¸‚ğ‘›\nğ‘¤âˆ’1)ï¸‚\n(16)Now, let us compute Lğ¹ğ¶ğ‘€ , the total additive error for the regu-\nlar fair-count-min sketch.\nLğ¹ğ¶ğ‘€=ğ‘›âˆ‘ï¸‚\nğ‘—=1ğœ€ğ´(ğ‘’ğ‘—)=âˆ‘ï¸‚\nğ‘’ğ‘—âˆˆg1ğœ€ğ´(ğ‘’ğ‘—)+Â·Â·Â·+âˆ‘ï¸‚\nğ‘’ğ‘—âˆˆgâ„“ğœ€ğ´(ğ‘’ğ‘—)\nFollowing the same calculation as in Equation 16, for every group\ngğ‘™=G,\nâˆ‘ï¸‚\nğ‘’ğ‘—âˆˆgğ‘™ğœ€ğ´(ğ‘’ğ‘—)=ğ‘ğ‘™(ï¸‚ğ‘›ğ‘™\nğ‘¤ğ‘™âˆ’1)ï¸‚\nHence,\nLğ¹ğ¶ğ‘€=âˆ‘ï¸‚\nğ‘’ğ‘—âˆˆg1ğœ€ğ´(ğ‘’ğ‘—)+Â·Â·Â·+âˆ‘ï¸‚\nğ‘’ğ‘—âˆˆgâ„“ğœ€ğ´(ğ‘’ğ‘—)=â„“âˆ‘ï¸‚\nğ‘™=1ğ‘ğ‘™(ï¸‚ğ‘›ğ‘™\nğ‘¤ğ‘™âˆ’1)ï¸‚\nFrom Theorem 1, we know, ğ‘¤ğ‘™=ğ‘›ğ‘™\nğ‘›ğ‘¤,âˆ€gğ‘™âˆˆG, whileâˆ‘ï¸â„“\nğ‘™=1ğ‘ğ‘™=\nğ‘. Therefore,\nLğ¹ğ¶ğ‘€=â„“âˆ‘ï¸‚\nğ‘™=1ğ‘ğ‘™(ï¸‚ğ‘›ğ‘™\nğ‘¤ğ‘™âˆ’1)ï¸‚\n=â„“âˆ‘ï¸‚\nğ‘™=1ğ‘ğ‘™(ï¸ƒğ‘›ğ‘™\nğ‘¤ğ‘™)ï¸ƒ\nâˆ’â„“âˆ‘ï¸‚\nğ‘™=1ğ‘ğ‘™=â„“âˆ‘ï¸‚\nğ‘™=1ğ‘ğ‘™(ï¸ƒğ‘›ğ‘™\nğ‘¤ğ‘™)ï¸ƒ\nâˆ’ğ‘\n=(ï¸‚ğ‘›\nğ‘¤)ï¸‚â„“âˆ‘ï¸‚\nğ‘™=1ğ‘ğ‘™âˆ’ğ‘ //sinceğ‘›ğ‘™\nğ‘¤ğ‘™=ğ‘›\nğ‘¤,âˆ€gğ‘™âˆˆG\n=ğ‘(ï¸‚ğ‘›\nğ‘¤)ï¸‚\nâˆ’ğ‘=ğ‘(ï¸‚ğ‘›\nğ‘¤âˆ’1)ï¸‚\n(17)\nFinally, combining Equations 12, 16, and 17, we get,\nğ‘ƒğ‘œğ¹=Lğ¹ğ¶ğ‘€âˆ’Lğ¶ğ‘€=ğ‘(ï¸‚ğ‘›\nğ‘¤âˆ’1)ï¸‚\nâˆ’ğ‘(ï¸‚ğ‘›\nğ‘¤âˆ’1)ï¸‚\n=0(18)\nThat is, the PoF is zero for ğ‘‘=1, when a uniform hashing scheme\nis used.\nB ADDITIONAL EXPERIMENT RESULTS\nIn this section, we present comprehensive experimental results\nacross three datasets: 1) google n-grams , 2)census , and 3) synthetic .\nWe also provide the absolute values of the approximation factors\nand additive errors for each group to facilitate a clearer understand-\ning of the trends in unfairness and the associated price of fairness\nunder each setting.\n15\n\nFigure 28: effect of varying group size\nğ‘›ğ‘™on unfairness, google n-grams ,ğ‘¤=\n65536,ğ‘‘=5.\nFigure 29: effect of varying group size\nğ‘›ğ‘™on approximation factors, google n-\ngrams ,ğ‘¤=65536,ğ‘‘=5.\nFigure 30: effect of varying group size ğ‘›ğ‘™\non unfairness, synthetic ,ğ‘¤=512,ğ‘‘=10.\nFigure 31: effect of varying group size\nğ‘›ğ‘™on approximation factors, synthetic ,\nğ‘¤=512,ğ‘‘=10.\nFigure 32: effect of varying sketch width\nğ‘¤on unfairness, google n-grams ,ğ‘›=\n1.2ğ‘€,ğ‘‘=5.\nFigure 33: effect of varying sketch width\nğ‘¤on approximation factors, google n-\ngrams ,ğ‘›=1.2ğ‘€,ğ‘‘=5.\nFigure 34: effect of varying sketch width\nğ‘¤on unfairness, census ,ğ‘›=430,ğ‘‘=5.\nFigure 35: effect of varying sketch width\nğ‘¤on approximation factors, census ,ğ‘›=\n430,ğ‘‘=5.\nFigure 36: effect of varying sketch width\nğ‘¤on unfairness, synthetic ,ğ‘›=20000,ğ‘‘=\n10.\nFigure 37: effect of varying sketch width\nğ‘¤on approximation factors, synthetic ,\nğ‘›=20000,ğ‘‘=10.\nFigure 38: effect of varying sketch depth\nğ‘‘on unfairness, google n-grams ,ğ‘›=\n1.2ğ‘€,ğ‘¤ =65536 .\nFigure 39: effect of varying sketch depth\nğ‘‘on approximation factors, google n-\ngrams ,ğ‘›=1.2ğ‘€,ğ‘¤ =65536 .\n16\n\nFigure 40: effect of varying sketch depth\nğ‘‘on unfairness, census ,ğ‘›=430,ğ‘¤=64.\nFigure 41: effect of varying sketch depth\nğ‘‘on approximation factors, census ,ğ‘›=\n430,ğ‘¤=64.\nFigure 42: effect of varying sketch depth\nğ‘‘on unfairness, synthetic ,ğ‘›=20000,ğ‘¤=\n512.\nFigure 43: effect of varying sketch depth\nğ‘‘on approximation factors, synthetic ,\nğ‘›=20000,ğ‘¤=512.\nFigure 44: effect of varying number of\ngroupsâ„“on unfairness, google n-grams ,\nğ‘›=1.2ğ‘€,ğ‘Š =65536,ğ‘‘=5.\nFigure 45: effect of varying number of\ngroupsâ„“on unfairness, census ,ğ‘›=400,ğ‘¤=\n64,ğ‘‘=5.\nFigure 46: effect of varying number of\ngroupsâ„“on unfairness, synthetic ,ğ‘›=\n20000,ğ‘¤=512,ğ‘‘=10.\nFigure 47: effect of varying group size\nğ‘›ğ‘™on price of fairness, google n-grams ,\nğ‘¤=65536,ğ‘‘=5.\nFigure 48: effect of varying group size\nğ‘›ğ‘™on absolute additive errors, google n-\ngrams ,ğ‘¤=65536,ğ‘‘=5.\nFigure 49: effect of varying group size\nğ‘›ğ‘™on price of fairness, synthetic ,ğ‘¤=\n512,ğ‘‘=10.\nFigure 50: effect of varying group size\nğ‘›ğ‘™on absolute additive errors, synthetic ,\nğ‘¤=512,ğ‘‘=10.\nFigure 51: effect of varying sketch width\nğ‘¤on price of fairness, google n-grams ,\nğ‘›=1.2ğ‘€,ğ‘‘=5.\n17\n\nFigure 52: effect of varying sketch width\nğ‘¤on absolute additive errors, google n-\ngrams ,ğ‘›=1.2ğ‘€,ğ‘‘=5.\nFigure 53: effect of varying sketch width\nğ‘¤on price of fairness, census ,ğ‘›=430,ğ‘‘=\n5.\nFigure 54: effect of varying sketch width\nğ‘¤on absolute additive errors, census ,ğ‘›=\n430,ğ‘‘=5.\nFigure 55: effect of varying sketch width\nğ‘¤on price of fairness, synthetic ,ğ‘›=\n20000,ğ‘‘=10.\nFigure 56: effect of varying sketch width\nğ‘¤on absolute additive errors, synthetic ,\nğ‘›=20000,ğ‘‘=10.\nFigure 57: effect of varying sketch depth\nğ‘‘on price of fairness, google n-grams ,\nğ‘›=1.2ğ‘€,ğ‘¤ =65536 .\nFigure 58: effect of varying sketch depth\nğ‘‘on absolute additive errors, google n-\ngrams ,ğ‘›=1.2ğ‘€,ğ‘¤ =65536 .\nFigure 59: effect of varying sketch depth\nğ‘‘on price of fairness, census ,ğ‘›=430,ğ‘¤=\n64.\nFigure 60: effect of varying sketch depth\nğ‘‘on absolute additive errors, census ,ğ‘›=\n430,ğ‘¤=64.\nFigure 61: effect of varying sketch depth\nğ‘‘on price of fairness, synthetic ,ğ‘›=\n20000,ğ‘¤=512.\nFigure 62: effect of varying sketch depth\nğ‘‘on absolute additive errors, synthetic ,\nğ‘›=20000,ğ‘¤=512.\nFigure 63: effect of varying number of\ngroupsâ„“on price of fairness, google n-\ngrams ,ğ‘›=1.2ğ‘€,ğ‘Š =65536,ğ‘‘=5.\n18\n\nFigure 64: effect of varying number of\ngroupsâ„“on price of fairness, census ,ğ‘›=\n400,ğ‘¤=64,ğ‘‘=5.\nFigure 65: effect of varying number of\ngroupsâ„“on price of fairness, synthetic ,\nğ‘›=20000,ğ‘¤=512,ğ‘‘=10.\nFigure 66: effect of varying group size\nğ‘›ğ‘™on construction time, google n-grams ,\nğ‘¤=65536,ğ‘‘=5.\nFigure 67: effect of varying sketch width\nğ‘¤on construction time, google n-grams ,\nğ‘›=1.2ğ‘€,ğ‘‘=5.\nFigure 68: effect of varying sketch width\nğ‘¤on construction time, census ,ğ‘›=430,ğ‘‘=\n5.\nFigure 69: effect of varying sketch depth\nğ‘‘on construction time, google n-grams ,\nğ‘›=1.2ğ‘€,ğ‘¤ =65536 .\nFigure 70: effect of varying sketch depth\nğ‘‘on construction time, census ,ğ‘›=430,ğ‘¤=\n64.\nFigure 71: effect of varying number of\ngroupsâ„“on construction time, google n-\ngrams ,ğ‘›=1.2ğ‘€,ğ‘¤ =65536,ğ‘‘=5.\nFigure 72: effect of varying number of\ngroupsâ„“on construction time, census ,ğ‘›=\n430,ğ‘¤=64,ğ‘‘=5.\nFigure 73: effect of varying group size\nğ‘›ğ‘™on query time, google n-grams ,ğ‘¤=\n65536,ğ‘‘=5.\nFigure 74: effect of varying sketch width\nğ‘¤on query time, google n-grams ,ğ‘›=\n1.2ğ‘€,ğ‘‘=5.\nFigure 75: effect of varying sketch width\nğ‘¤on query time, census ,ğ‘›=430,ğ‘‘=5.\n19\n\nFigure 76: effect of varying sketch depth\nğ‘‘on query time, google n-grams ,ğ‘›=\n1.2ğ‘€,ğ‘¤ =65536 .\nFigure 77: effect of varying sketch depth\nğ‘‘on query time, census ,ğ‘›=430,ğ‘¤=64.\nFigure 78: effect of varying number of\ngroupsâ„“on query time, google n-grams ,\nğ‘›=1.2ğ‘€,ğ‘¤ =65536,ğ‘‘=5.\nFigure 79: effect of varying number of\ngroupsâ„“on query time, census ,ğ‘›=\n430,ğ‘¤=64,ğ‘‘=5.\n20",
  "textLength": 79456
}