{
  "paperId": "50648fd8c507712894e1e2798a605f0d2b82fb78",
  "title": "Similarity Driven Approximation for Text Analytics",
  "pdfPath": "50648fd8c507712894e1e2798a605f0d2b82fb78.pdf",
  "text": "Similarity Driven Approximation for Text Analytics\nGuangyan Hu\nRutgers University\nNew Brunswick, NJ\ngh279@cs.rutgers.eduYongfeng Zhang\nRutgers University\nNew Brunswick, NJ\nyz804@cs.rutgers.eduSandro Rigo\nUniversity of Campinas\nCampinas - SP, Brazil\nsrigo@unicamp.brThu D. Nguyen\nRutgers University\nNew Brunswick, NJ\ntdnguyen@cs.rutgers.edu\nAbstract —Enterprises are increasingly seeking to extract in-\nsights for decision making from text data sets. Yet, processing\nlarge text data sets using sophisticated algorithms is compu-\ntationally expensive. In this paper, we propose and evaluate a\nframework called EmApprox that uses approximation to speed\nup the processing of a wide range of queries over large text\ndata sets. The key insight is that different types of queries can\nbe approximated by processing subsets of data that are most\nsimilar to the queries. EmApprox builds a general index for\na data set by learning a natural language processing model,\nproducing a set of highly compressed vectors representing words\nand subcollections of documents. Then, at query processing time,\nEmApprox uses the index to guide sampling of the data set, with\nthe probability of selecting each subcollection of documents being\nproportional to its similarity to the query as computed using\nthe vector representations. We have implemented a prototype\nof EmApprox as an extension of the Apache Spark system,\nand used it to approximate three types of queries: aggregation,\ninformation retrieval, and recommendation. Experimental results\nshow that EmApprox’s similarity-guided sampling achieves much\nbetter accuracy than random sampling. Further, EmApprox can\nachieve signiﬁcant speedups if users can tolerate small amounts\nof inaccuracies. For example, when sampling at 10%, EmApprox\nspeeds up a set of queries counting phrase occurrences by almost\n10x while achieving estimated relative errors of less than 22%\nfor 90% of the queries.\nIndex Terms —Approximate computing, approximate query\nprocessing, text analytics\nI. I NTRODUCTION\nMotivation. Enterprises are increasingly seeking to extract\ninsights for decision making from text data sets. At the same\ntime, data is being generated at an unprecedented rate, so that\ntext data sets can get very large. For example, the Google\nBooks Ngram data set contains 2.2 TB of data [1] and the\nCommon Crawl corpus contains petabytes of data [2]. Pro-\ncessing such large text data sets using sophisticated algorithms\nis computationally expensive.\nThe above challenge is exacerbated when it is desirable to\nrun different types of queries against a data set, making it\nexpensive to build multiple indices to speedup query process-\ning. For example, given a data set comprising user reviews on\nproducts, an enterprise may want to count positive vs. negative\nreviews, use the reviews to make recommendations, or retrieve\nreviews relating to a particular product [3]. Currently, a\ndifferent index is required for quickly answering each of these\nquery types.\nEmApprox. In this paper, we propose a framework called\nEmApprox to speed up a wide range of queries over large text\nFig. 1: Overview. Snare subcollections of documents.\ndata sets. The key idea behind EmApprox is to build a general\nindex that guides the processing of a query toward a subset\nof the data that is most similar to the query. For example,\nconsider a query that seeks to count the number of occurrences\nof a given phrase. EmApprox would select a sample of the data\nset, preferentially choosing items most similar to the query\nphrase, count the occurrences in the sample, and use the count\nto estimate the number of occurrences in the entire data set.\nClearly, the result is approximate so that users of EmApprox\nwould need to tolerate some imprecision in the estimated\nresults. EmApprox allows users to trade off precision and\nperformance by adjusting the sampling rate.\nOur approach is related to the many approximate query pro-\ncessing (AQP) systems that answer aggregation queries over\nrelational data sets by processing estimators with error bounds\nusing samples of the data [4]–[7]. In essence, one can think\nof EmApprox as extending AQP to text analytics. EmApprox\nsupports the estimation of errors bounds when possible; e.g.,\nfor aggregation queries. However, EmApprox can also be used\nin scenarios where it is not possible to estimate error bounds\nsuch as information retrieval, making it widely applicable to\nmany different text analytic queries/applications.\nSystem overview. Figure 1 gives an overview of EmApprox.\nAs mentioned above, EmApprox executes a query on a sample\nof the data set to reduce query processing time. Straightfor-\nward use of random sampling can lead to large errors, however,\nwhen sampling from a skewed distribution [5]. To mitigate\nthis issue, EmApprox builds an index ofﬂine , then consults\nthe index at query processing time to guide sampling toward\nsubsets of data that are most similar to the query.\nSpeciﬁcally, EmApprox uses a natural language processing\n(NLP) model [8] to learn vector representations for unique\nwords and documents. The resulting vectors can be composed\nand used to compute a similarity metric. Then, assuming thatarXiv:1910.07144v2  [cs.DB]  12 Jan 2020\n\nthe data set is partitioned into a number of subcollections as\nshown in Figure 1, EmApprox computes a vector for each\nsubcollection from the vectors of the documents contained in\nit.1The ﬁnal index contains vectors for unique words together\nwith vectors for the subcollections.\nAt query processing time, EmApprox computes a vector\nfor the query using vectors of the words in the query. It then\ncomputes a sampling probability for each subcollection that\nis proportional to the similarity between the subcollection and\nthe query using their vector representations. Finally, it selects\na sample of subcollections using their sampling probabilities.\nEmApprox uses locality-sensitive hashing (LSH) to hash\neach real-valued vector to a bit vector [10] to reduce the\nstorage overhead of the index. LSH works well because it\npreserves the distance between the original vectors. Computing\nsimilarity using LSH bit vectors is also extremely cheap; it is\nsimply the Hamming distance of two bit vectors that can be\ncomputed efﬁciently using XOR. This optimization has greatly\nincreased the scalability and efﬁciency of EmApprox’s index.\nQueries. We have implemented a prototype of EmApprox\nand used it to support approximate processing for three\ndifferent types of queries: (1) aggregation queries that count\noccurrences within a text data set, (2) retrieval queries, both\nBoolean and ranked, that retrieve relevant documents, and\n(3) recommendation queries that predict users’ ratings for\nproducts. For aggregation queries, we show how to compute\nestimated error bounds along with the approximate results.\nWe also show that the training objective of PV-DBOW [8],\nthe speciﬁc NLP model that we use, is directly related to\nminimizing the variance of the estimated results when using\nsimilarity driven sampling. For the retrieval queries, we use\nEmApprox in a similar fashion to distributed information\nretrieval (DIR), where a query is only processed against\nsubcollections that are expected to be most relevant to the\nquery [5]. Finally, for the recommendation queries, we use\nthe user-centric collaborative ﬁltering (CF) algorithm [11] to\npredict a target user’s ratings using the average of other users’\nratings weighted by similarities between their product reviews.\nEvaluation. We generate a large number of queries for each\nquery type, and execute them on three different data sets.\nWe adopt equal probability cluster sampling [12] over sub-\ncollections as the baseline for our evaluation. We show that\nEmApprox can achieve signiﬁcant improvements on different\ndomain-speciﬁc metrics compared to the baseline with very\nlittle extra overhead during query processing. For example,\nto match the error bounds in aggregation queries achieved\nby EmApprox, the baseline would have to process \u00184x the\namount of data. We also show that EmApprox can achieve\nsigniﬁcant speedups if users can tolerate modest amounts of\nimprecision. For example, when sampling at 10%, EmApprox\nspeeds up a set of queries counting phrase occurrences by\nalmost 10x while achieving estimated relative errors of less\nthan 22% for 90% of the queries.\n1Data sets may be partitioned into subcollections for a variety of reasons,\nincluding storage in a distributed ﬁle system such as HDFS [9].EmApprox is extremely efﬁcient for processing queries\nthat estimate results such as aggregation and recommenda-\ntion queries. In contrast, like all sampling-based approaches,\nEmApprox is less effective for speeding up queries similar to\ninformation retrieval queries. This is because these queries are\nseeking speciﬁc data items in the data set, and it is impossible\nto estimate missed data items based on the sample.\nContributions. In summary, our contributions include: (i) to\nour knowledge, our work is the ﬁrst to leverage an NLP model\nto build a general-purpose index to guide the approximate\nexecution of text analytic queries; (ii) we show that the training\nobjective of PV-DBOW is directly correlated with minimizing\nthe variance of counting queries; (iii) we propose similar-\nity driven sampling that can signiﬁcantly increase accuracy\ncompared to random sampling for three distinct types of\napproximate queries in three different application domains;\n(iv) we show that hashing real-valued vectors into light-\nweight LSH bit vectors signiﬁcantly improves storage and\ncomputation efﬁciency without compromising precision.\nII. B ACKGROUND AND RELATED WORK\nOur work is inspired by recent work [13] that proposes\nusing machine learned models to build database index that\nprobabilistically map queried keys to the positions of desired\nrecords. A queried key and the matching record is analogous to\nan approximate query and relevant data in our work. However,\nour index targets approximating text analytic queries which are\nvery different workloads than typical database queries.\nA. Approximate Query Processing\nTraditional AQP systems have mainly targeted aggregation\nqueries over relational data sets. AQP++ [7] is a recent\ndatabase system that uses sampling-based AQP and precom-\nputed aggregates to achieve interactive response time for\naggregation queries. BlinkDB [4] is an AQP system that selects\nofﬂine-generated samples to answer queries. It uses query\ncolumn sets (QCSs) for representing the sets of columns\nappearing in past workloads, and stratiﬁed samples are created\nfor each QCS. It assumes QCSs are stable over time, which\ndoes not perform well for queries outside the QCS coverage in\nthe ofﬂine samples. ApproxHadoop [6] and ApproxSpark [14]\nare online cluster sampling based frameworks that supports\napproximating aggregation with error bounds. However, their\nresult estimation is prone to large error bounds over skewed\ndata. Sapprox [5] has an ofﬂine and an online component.\nIt collects the occurrences of sub-data sets ofﬂine, and the\ninformation to facilitate online cluster sampling. Sapprox\ntargets certain counting queries in relational data sets, whereas\nEmApprox targets text analytics with a more general-purpose\nindexing scheme.\nB. Cluster Sampling\nSimple random sampling, stratiﬁed sampling and cluster\nsampling are three common methods of sampling, where the\nmost computationally efﬁcient sampling method is cluster\nsampling since it avoids full scan of the data set. As large data\n2\n\nsets are usually partitioned, a cluster often would correspond\nto a partition [5], [6].\nSuppose we want to estimate the frequency \u001cof a phrase\nZoccurring in a large data set partitioned into disjoint\nsubcollections of documents. If we take a cluster sample from\nthe data set using subcollections as the sampling units, we can\nderive the estimator ^\u001cusing cluster sampling theory [12]:\n^\u001c=1\nnX\ns2S\u001cs\n\u001es\u0006\u000f (1)\nwhereSis the chosen sample, sis a subcollection in the\nsample,\u001csis the frequency of Zins,\u001esis the sampling\nprobability for s, and\u000fis the estimated error bounds. \u000fin turn\nis computed as:\n\u000f=tn\u00001;1\u0000\u000b=2q\n^V(^\u001c) =tn\u00001;1\u0000\u000b=2sP\ns2S(\u001cs\n\u001es\u0000^\u001c)2\nn(n\u00001)\n(2)\nwherenis the sample size and tn\u00001;1\u0000\u000b=2is the critical value\nof at-distribution at conﬁdence level \u000bwithn\u00001degrees of\nfreedom. We observe that as the \u001es’s approach\u001cs\n^\u001c,^V(^\u001c)and\nhence\u000fwill approach 0. The goal of probability proportional\nto size (pps) sampling [12] is to set each \u001esclose to\u001cs\n^\u001cby\nleveraging auxiliary information of each sampling unit, so that\nwe can reduce the error bound for our estimator ^\u001c. If^\u001cis a\ngood estimator, then \u001esshould be close to\u001cs\n\u001cwhich would\nmake the estimator have a small variance.\nC. Paragraph Vectors\nRecent advances in NLP have shown that semantically\nmeaningful representations of words and documents can be\nefﬁciently learned by neural embedding models [8], [15].\nWord2vec uses an unsupervised neural network to learn vec-\ntor representations (embeddings) for words [15]. It seeks to\nproduce vectors that are close in a vector space for words\nhaving similar contexts, which refers to the words surrounding\na word in a pre-determined window. For example, synonyms\nlike “smart” and “intelligent,” or closely related words such\nas “copper” and “iron,” are likely to be surrounded by similar\nwords, so that Word2vec will produce spatially close vector\nrepresentations for them. Similarity between two words can\nthus be scored based on the dot product distance between\ntheir corresponding vectors. The learned vectors also ex-\nhibit additive compositionality, enabling semantic reasoning\nthrough simple arithmetic such as element-wise addition over\ndifferent vectors. For example, vec(\\king\")\u0000vec(\\man\")\u0019\nvec(\\queen \")\u0000vec(\\woman \").\nParagraph Vector (PV) [8] is similar to Word2vec, which\njointly learns vector representations for words and variable-\nlength text ranging from sentences to entire documents in a\nmethod. Distributed Bag of Words PV (PV-DBOW) is a ver-\nsion of PV that has been shown to be effective in information\nretrieval due to its direct relationship to word distributions in\ntext data sets [16]. By setting PV-DBOW’s window size to beeach of the document’s length, the generative probability of\nwordwin a document dis modeled by a softmax function:\nPPV(wjd) =exp(~ w\u0001~d)P\nw02Vexp(~w0\u0001~d)(3)\nwhere~ wand~dare vector representations for wandd, and\nVis the vocabulary (i.e., the set of unique words in the\ndata set). PV-DBOW learns the word and document vectors\nusing standard maximum likelihood estimation (MLE), by\nmaximizing the likelihood of observing the training text data\nset under the distribution deﬁned by Eq (3). As a result, the\ntraining process will output word and document vectors that\nsatisfy Eq (3) which formulates the theoretical foundation of\nour approximation index.\nTo reduce the expense of computing Eq (3) during training,\na technique called negative sampling has been proposed [15]\nthat randomly samples a subset of words in that document\naccording to a noise distribution to approximate Eq (3). The\ntraining process is equivalent to implicitly factorizing a shifted\nmatrix of point-wise information ( PMI) between words and\ndocuments: [17]:\n~ w\u0001~d=PMI(w;d)\u0000log(k) (4)\nwherePMI (w;d)is the point-wise information between word\nwand document d, andkis a constant representing the\nnumber of negative samples for each positive instance in\nthe training process. PMI can be estimated empirically by\nobserving the frequencies of words in documents in the data\nset aslog#(w;d)\njdj\u0001jDj\n#(w;D), where #(w;d)is the frequency of\nwin document d,jdjis the length of (number of words in) d,\nDis the data set, #(w;D)is the total number of occurrences\nofwinD,jDjis the total number of words in D.\nGivenPMI ’s deﬁnition, Eq (4) reveals that the exponential\nof the distance between a document and word vector is pro-\nportional to the probability of document predicting this word\np(wjd), which indicates that if a word is chosen randomly\nfromd, then what is the probability that it would be w:\nexp(~ w\u0001~d) =p(wjd)\np(w)k/p(wjd) (5)\nWe can see from Eq (5) that the inner-product distance\nbetween the word vector and each document vector is pro-\nportional to the percentage of the total occurrence of w\ncontributed by each document d.\nD. Locality-Sensitive Hashing\nHashing methods have been studied extensively for search-\ning for similar data samples in a high-dimensional data set\nto solve the approximate nearest neighbor problem. LSH is\namong the most popular choices for indexing a data set using\nhashing [10]. The basic idea behind LSH is to transform each\nitem in a high dimensional space into a bit vector with bbits,\nusingbbinary-valued hash functions h0, ...,hb. In order for\nthe bit vector to preserve the original vectors’ similarity, each\nhash function hmust satisfy the property:\nPr[h(~ x) =h(~ y)]/sim(~ x;~ y)\n3\n\nwhere~ xand~ yare two vectors in the data set; sim is\na similarity measure, such as Jaccard, Euclidean or cosine.\nPr[h(~ x) =h(~ y)]is computed as one minus the ratio of\nthe Hamming distance between two bit vectors over the total\nnumber of bits in them. Similarity between two items is\npreserved in the mapping, that is, if two items’ LSH bit vectors\nare close in Hamming distance then the probability that they\nare close to each other in the original metric space is also high.\nThis property allows items’ LSH bit vectors to efﬁciently index\na data set for similarity search [10].\nIII. S IMILARITY -DRIVEN SAMPLING\nIn this section, we discuss cluster sampling with probabili-\nties driven by similarities of a query to subcollections. Cluster\nsampling has been adopted for approximating aggregation\nqueries (section 2.2) that seek to compute a sum/mean over\nthe data set, such as counting the occurrence of a phrase,\nnumber of documents in a given topic. We show that the\nsimilarities can be computed online using the ofﬂine trained\nPV-DBOW vectors. Finally we describe our approximation\nindex and using LSH to compress the real-valued vectors.\nQuery vector representation. We assume a query qis\nrepresented as a piece of text of lwordsfwig. Under the\nbag-of-words assumption, the probability of qin a document\ndis the joint probability of its words wi:\np(qjd) =Y\ni2lp(wijd) (6)\nIf we deﬁne ~ qas element-wise arithmetic sum of its individual\nwords’ vectors: ~ q=Pl\ni=1~ wi, then by combining Eq (5) and\nEq (6) we can derive that p(qjd)is actually proportional to the\nexponential of ~ q\u0001~d, where Eq (5) is derived from PV-DBOW’s\ntraining objective:\np(qjd) =Y\ni2lp(wijd)/exp(~ q\u0001~d)(7)\nwhich is the exactly the same form as Eq (5) where a query\nonly comprises a single word. Therefore by computing ~ qthis\nway, we can conveniently derive the probability of document\npredicting this query, under the assumption that the words are\nindependent.\nA. Sampling Probability Estimation\nWe probabilistically deﬁne a document’s similarity to a\nquery asp(qjd)the probability of dpredictingq.exp(~ q\u0001~ q)in\nEq (7) can be used to compute p(qjd)at sampling time using ~ q\nand~d, both derived from the ofﬂine trained PV-DBOW model.\nSuppose we use documents as clusters for ppssampling to\nestimate the quantity of qthroughout the data set, then we\ncan useexp(~ q\u0001~d)as each document’s auxiliary information\nfor setting sampling probabilities proportional to its similarity\ntoq:\n\u001ed(q) =p(qjd)P\nd02Dp(qjd0)=exp(~ q\u0001~d)P\nd02Dexp(~ q\u0001~d0)(8)A large data set is often partitioned in many subcollections,\nand cluster sampling with those as clusters is a more efﬁcient\nsampling design [5], [6]. Similar to sampling documents with\nsimilarity-driven probabilities, we propose to use a subcol-\nlection’s vector representation to compute its similarity to the\nquery and set sampling probabilities proportionally. Intuitively,\nwe propose to deﬁne a subcollection’ vector representation\nusing the element-wise arithmetic mean of the vectors of the\ndocuments in it: ~ s=1\nnP\nd2s~d, wherenis the number of\ndocuments in s, anddis a document in s.\nWe now analyze why arithmetic mean of document vectors\nis reasonable as subcollection’s vector representation. Given ~ s\nand Eq ( 7), we can derive the exponential of the dot product\nbetween a subcollection and query as the geometric mean of\neachp(qjd)ins:\nexp(~ q\u0001~ s) = nsY\nd2sexp(~ q\u0001~d)/nsY\nd2sp(qjd) (9)\nLetp(qjs)denotenpQ\nd2sp(qjd), then we can rewrite Eq (9)\nin a similar form as Eq (7):\np(qjs)/exp(~ q\u0001~ s) (10)\nwhere we use p(qjs)to express the similarity of a subcollection\nto the query. Similar to computing the similarity of qto a\ndocument, we can use Eq (10)’s left hand side to compute the\nsimilarity of qto a subcollection. Following the same idea as\nusing documents as sampling units, we deﬁne a probability\ndistribution \u001es(q)for each subcollection swith respect to q\nin the same form as Eq (8):\n\u001es(q) =p(qjs)P\ns02Dp(qjs0)=exp(~ q\u0001~ s)P\ns02Dexp(~ q\u0001~s0)(11)\nwhereexp(~ q\u0001~ s)can be computed at sampling time.\nA cluster sample over the subcollections with probabilities\nset according to Eq (11) will greatly reduce the uncertainty\nin estimating the occurrence of fwig. Variants of this query\ninclude estimating number of documents that contain fwig\nor number of documents similar to fwigin semantics. Both\ndistributions deﬁned in Eq (8) and (11) essentially normalize\nthe probability of a phrase appearing in a speciﬁc document\nor subcollection, against every document or subcollection in\nthe data set. Interestingly, Eq (8) and (11) have the same form\nas a softmax classiﬁer over query words fwigwhich predicts\nits probabilities conditioned on a document or subcollection.\nB. Approximation Index\nThe approximation index includes vectors for every word,\ndocument and subcollection. The index can occupy signiﬁcant\nstorage space for a large data set, for which we propose to\nmap the real-valued vectors to LSH bit vectors to reduce the\nrequired storage. And, the cost of computing the similarities\nbetween bit vectors is also much more efﬁcient than dot\nproduct between real-valued vectors.\n4\n\nLSH. LSH bit vectors can preserve different distance met-\nrics (e.g., cosine) between their real-valued vector counter-\nparts. Computing the Hamming distance of the LSH bit vectors\nusing XOR is also much more efﬁcient than dot product. We\nslightly modify the gradient descent-based training process of\nPV-DBOW: at each update step, we normalize the vectors to\nbe unit length so that the dot product of the trained vectors\nis equivalent to the cosine similarities between two vectors.\nThe value of the hash function for preserving cosine distance\ndepends on the dot product between a random plane ~ rand an\nitem vector ~ x, wherehr(~ x)evaluates 1 if ~ r\u0001~ x\u00150, and 0\notherwise, where ~ rusually has a standard multi-dimensional\nGaussian distribution N(0;I), and a new ~ ris generated\neach time the hash function is applied [10]. In order to\ngenerate the LSH signature for a real value vector, we ﬁrst\nchoose a dimension lfor the bit vector, then apply hash\nfunctionhr(~ x)ltimes to generate each bit, each choosing a\nrandom~ r. Speciﬁcally, we can approximate exp(~ w\u0001~d)using\nexp(cos(m\nl\u0019)), wheremis the Hamming distance between ~ w\nand~d’s corresponding LSH bit vectors.\nIV. B EYOND AGGREGATION QUERY\nA. Query characterization\nIn the previous section, we introduced approximating ag-\ngregation queries such as estimating occurrence of a phrase.\nIn this section, we describe approximating DIR and recom-\nmendation queries using EmApprox. We note that the goal\nof IR is to identify “similar” documents to a query, and that\nsome recommendation technique in data mining depend on\nidentify “similar” users to a target user. We characterize our\ntargeted queries as: 1) the query can be represented by words;\n2) relevancy of the query to a subset of data can be reduced\nto the generative probability of the query’s relevant data given\nthis subset - the deﬁnition of similarity; 3) efﬁciency of the\napproximate computation can be improved by processing the\nmost similar data in the data set.\nB. Distributed Information Retrieval\nInformation retrieval from disjoint subcollections of doc-\numents is known as distributed information retrieval (DIR),\nwhere many irrelevant subcollections are ignored for improved\nretrieval efﬁciency [18]. EmApprox can facilitate subcollec-\ntion selection under the vector space retrieval paradigm for\nDIR [18], [19]. We target Boolean and ranked retrieval models\nfor DIR in the following discussion.\nBoolean retrieval. Under the Boolean model, a query is a\nBoolean expression of words connected by Boolean opera-\ntors (e.g.,w0_(w1^w2)), where a term wionly evaluates to\ntrue when contained in a document [19]. The retrieval result\nis a set of documents that satisfy the Boolean expression.\nWhen approximating a Boolean query qb, we ﬁrst compute\nits similarity to each subcollection - p(qbjs)as we have\npreviously deﬁned. We can compute p(qbjs)using the same\nsequence as how the Boolean query is evaluated, i.e. ^\ntakes precedence over _. We ﬁrst compute each individualterm (wi)’sp(wijs)to a subcollection susing Eq (10), in order\nto compute the query’s overall similarity. Since wi^wjimplies\nthatwiandwjboth have to exist for it to be true, whereas sat-\nisfyingwi_wjrequires either wiorwjto exist, the generative\nprobability of wi^wjis equivalent of p(wijs)\u0001p(wjjs); by the\nsame token, the generative probability of wi_wjis therefore\np(wijs) +p(wjjs). For example, suppose we have a Boolean\nqueryqb=w0_(w1^w2), thenp(qbjs)can be computed as\np(w0js)+(p(w1js)\u0001p(w2js)), where each p(wijs)is computed\nusing Eq (10). Finally, we use the overall query similarity to\nthe subcollections p(qbjs)to compute sampling probability of\neach subcollection to sample a subset of subcollections. Then,\nonly documents in the chosen subcollections are evaluated\nagainst the Boolean query to retrieve matching documents.\nRanked retrieval. Instead of returning a set of documents\nthat precisely match a Boolean expression, ranked retrieval\nreturns a list of documents ranked by their relevancy to\nthe query. The query is usually a phrase comprising of a\nsequence of words. In terms of ranking, each document is\nassigned a score using a function with potentially many factors\nconsidered, such as the tf-idf of the query terms or cosine\nsimilarity between the vector representations of the query and\na document obtained under the same language model. Similar\nto Boolean retrieval, our proposed framework ﬁrst samples\na subset of subcollections with probabilities proportional to\nthe query’s similarity to each subcollection, which is more\nstraightforward to compute than Boolean model - we just use\nEq (11) to compute its similarity to a subcollection. We\nthen apply a user-speciﬁed scoring function to documents from\nthe chosen subcollections, such as BM25 [20] or a language\nmodel-based function [16].\nC. Recommendation\nOne of the most successful recommendation techniques\nin data mining is called neighborhood-based collaborative\nﬁltering (CF), which have been deployed in many commercial\nwebsites [11], [21]. User-centric neighborhood CF is that given\na target user u, it ﬁrst tries to identify a subset of more similar\nusers as neighbors , then uses the similarity score between u\nand each neighbor to predict a rating for item i. It computes the\nprediction by taking an average of all the ratings for ibyu’s\nneighbors weighted by their similarity scores. Symmetrically,\nneighborhood CF can be item-centric which predicts a rating\nfor an item ibased on similarities between iand other items\npurchased by u.\nWithout loss of generality, we will focus on user-centric\nCF. Review text embeds rich information which has been\nused to model users’ behaviors and items’ properties in past\nwork as an alternative to numerical rating only data [3]. A\nuser’s vector representation can be learned under PV-DBOW\nmodel by deﬁning a document as all the reviews a user has\nwritten [3]. Consequently, a user’s vector ~ uwould encode u’s\npreference. When the number of total users in the data set\nis large, the process of identifying neighbors from them to\na target user uwould be expensive. Our proposed index can\nmake it more amenable for large data sets by selecting only\n5\n\nmore similar subcollections of users. Suppose the review data\nset is sorted by users, then similarity of a user uto a collection\nof reviews can be computed using Eq (10) to sample most\nsimilar collections of users. Then predicted ratings for the\nnew user can be computed using any model/metric with the\nneighbors in the chosen subcollections.\nAs a concrete example, rating of an item iby useru’s\npredicted value can directly leverage u’s review text vector’s\nsimilarity to a user vwho has also rated item i, computed asP\nv2U0sim(u;v)r(v;i), whereU0is the set of all users who\nhave rated item iin the chosen subcollections, r(v;i)is user\nv’s rating for item i,sim(u;v)can be characterized by their\nsimilarity in the review texts uandvhave written, deﬁned asP\nv2U0exp(~ u\u0001~ v)P\nv02U0exp(~ u\u0001~v0).\nD. Discussion on Document Allocation\nThe document allocation policy can affect DIR’s perfor-\nmance, where the storage of documents needs to be skewed\nfor DIR to be effective [18]. This is because the query expects\nto retrieve as many relevant documents to the query as possible\nfrom only a subset of the data set. The document alloca-\ntion policy can also affect recommendation’s performance,\nsince identifying similar users is similar to retrieving relevant\ndocuments. On the other hand, the accuracy of aggregation\nestimators is not dependent on document allocation, since the\nlocal sum of each chosen subcollection is multiplied by the\ninverse of its own sampling probability as a scaling factor to\ncompute an overall estimator, i.e., subcollections with a large\nlocal sum will just have a small scaling factor.\nWe propose to allocate documents based on their vectors’\npair-wise cosine distance through clustering the documents in\nthe original data set using spherical K-means [22], that uses\nthe cosine as distance metric. The clustering process takes\nas input the collection of document vectors, and produces an\nallocation where semantically similar documents are clustered.\nWhen documents d1,d2, ...,dnare semantically similar, it\nindicates that the probabilities p(wjd1),p(wjd2), ...,p(wjdn)\nare also similar for a query word w. The result of clustering\nis a more skewed distribution of the documents, therefore\ndocuments that have the same probability of predicting a query\nword tend to be allocated together. As Eq (9) shows that\np(wjs) =npQ\nd02sp(wjd0)– the geometric mean of each\ndocument’s probability of predicting win that subcollection s\n– so ifp(wjd0)’s are similar in each s, then their geometric\nmeanp(wjs)will approach a local maximum equivalent to the\narithmetic mean of all the p(wjd0), according to the AM-GM\ninequality [23]. It therefore suggests that this allocation policy\nwould produce a skewed sampling probability distribution\n\u001es(w), which is desired for a retrieval query.\nV. L IMITATIONS\nEmApprox can approximate a range of text analytical\nqueries, we nevertheless highlight a couple of limitations.\nModel drift. We assume the text data set is historical and\nstable. If new documents are added to the existing data set,\nFig. 2: EmApprox architecture. fbNgare HDFS blocks, frng\nare Spark RDD partitions, where nis a subset of N. The\nofﬂine indexer and query both use the EmApprox library.\nPV-DBOW is able to infer the vectors for an unseen document\nusing the words in the new document [8]. However, the\noriginally trained PV-DBOW model may drift due to document\nupdates. Therefore PV-DBOW model should be retrained to\ncapture the true word/frequent distributions in the data set,\nwhich requires the ofﬂine index to be rebuilt.\nUnseen words in the query. Currently we assume the query\ndoes not include words outside the vocabulary of the data set,\nso that any word vector in the query can be directly obtained.\nVI. I MPLEMENTATION\nWe have implemented a prototype of EmApprox as a Python\nlibrary comprising two parts: one for building ofﬂine indexers\nand one for building approximate query processing applica-\ntions (queries for short). Users write indexers and queries\nas Spark programs in Python using the PySpark [24] and\nEmApprox libraries. Figure 2 gives an overview of a system\nbuilt using EmApprox, where documents are stored in blocks\nof an HDFS ﬁlesystem, with blocks considered subcollections\nof documents and used as sampling units. Note that a single\nindexer can be used to index many different data sets of the\nsame type, and many different queries can be executed against\neach index/data set.\nWe leave the task of writing indexers to the user because\nit allows the ﬂexibility for indexing many different types of\ndata (e.g., different data layouts and deﬁnitions of documents).\nIt is quite simple to write indexers given the EmApprox\nlibrary. Speciﬁcally, users need to write code to parse a given\ndata set to extract the documents (much of this code can\ncome from standardized libraries) and identify documents in\neach subcollection (HDFS block). All other functionalities are\nimplemented in the EmApprox library, and simply requires\nthe user program to call several functions. Similarly, the\nmain difference between an approximate query built using\nEmApprox and a precise query is the invocation of several\nEmApprox functions.\nAn ofﬂine indexer uses EmApprox to learn vector repre-\nsentations for words and documents, cluster documents (when\ndesired) using K-means as discussed in Section IV-D, compute\n6\n\nvectors for blocks (subcollections), compute corresponding\nLSH bit vectors, and prepare the index. This process is shown\nas stepsp1andp2in Figure 2. (We do not show the clustering\nfor simplicity.) We use Gensim [25] as the default library for\nPV-DBOW model training, but we can also use alternative\nimplementations that can run on distributed frameworks such\nas Tensorﬂow [26] to reduce the training time. We use Gensim\nin our prototype because it is a widely adopted PV-DBOW\nimplementation.\nThe execution of an approximate query is shown as steps a1\nthrougha5in Figure 2. In step a1, the query uses EmApprox\nto read the index into an in-memory hash table, compute\nsampling probabilities for all HDFS blocks, and choose a\nsample using ppssampling. Step a2launches a Spark job.\nStepsa3\u00004are part of the Spark job and use EmApprox\nand PySpark to read the sample from the data set into an\nRDD. Step a5is the execution of the rest of the Spark job.\nWe provide two simple reduce functions that compute the\nestimated sum and average, along with the conﬁdence interval\n(see Eq (1) and (2)).\nVII. E VALUATION\nWe evaluate EmApprox using synthetic queries of the three\ntypes discussed above. Table I summarizes the query types\nand the evaluation metrics. Table II summarizes the data sets\nthat we use in our evaluation.\nA. Evaluation Methodology and Metrics\nData. We use three data sets: a snapshot of Wikipedia [27],\na news corpus from Common Crawl (CCNews) [28], and a\nset of Amazon user reviews [3]. Table II summarizes the data\nsets, their use in different queries, the training time of PV-\nDBOW using Gensim, and the size of the resulting indices\nafter compression using LSH.\nExperimental platform. Experiments are run on a cluster of\n6 servers interconnected with 1Gbps Ethernet. Each server\nis equipped with a 2.5GHz Intel Xeon CPU with 12 cores,\n256GB of RAM, and a 100GB SSD. Data sets are stored in an\nHDFS ﬁle system hosted on the servers’ SSDs, conﬁgured with\na replication factor of 2 and block size of 32MB. Applications\nare written in Python 3 and run on Spark 2.0.2.\nIndex construction. We train PV-DBOW to produce word and\ndocument vectors with 100 dimensions. We use LSH vectors\nof 100 bits, which compresses the PV-DBOW learned vectors\nby a factor of 64. We explore the sensitivity of EmApprox to\nthese parameters in Section VII-E.\nBaselines. We analyze and compare EmApprox’s performance\nagainst simple random clustered sampling (SRCS) [12] of\nHDFS blocks and precise execution. We compare execution\ntimes (speedups) and query-speciﬁc metrics. We run the pre-\ncise executions as “pure” Spark programs on an unmodiﬁed\nSpark system. We run SRCS using the EmApprox prototype,\nreplacing pps sampling with simple random sampling.\nAggregation queries. We run aggregation queries that es-\ntimate the numbers of occurrences and the correspondingrelative errors of target phrases in the Wikipedia data set. We\ncreate 200 queries by randomly selecting phrases from the data\nset. The lengths of the phrases follow a normal distribution\nwith a mean of 2 words and a standard deviation of 1.\nThe approximate answer to each query executed with a\ngiven sampling rate includes the estimated count ( ^\u001c) and an\nestimated conﬁdence interval ( ^\u001c\u0006\u000f). We report the estimated\nrelative error at 95% conﬁdence level as the ratio of \u000fover\n^\u001c. We also compare the estimated relative error with the\nactual relative error, computed asj^\u001c\u0000\u001cj\n\u001c, where\u001cis the precise\nanswer.\nDIR queries. We cluster documents within the Wikipedia and\nCCNews data sets as explained in Section IV-D, setting the\nnumber of centroids equal to the number of HDFS blocks in\nthe ﬁle holding the data set.\nWe generate 200 sets of randomly chosen words, 100 from\nthe Wikipedia data set and 100 from CCNews. Set sizes follow\na normal distribution with an average size of 3 and a standard\ndeviation of 1. We randomly insert Boolean operators ( and\nandor) to form Boolean queries, and use the sets of words\ndirectly as queries in ranked retrieval. We use the BM25 rank-\ning function [20] in ranked retrieval. We choose BM25 from\na plethora of ranking functions, including functions that use\nParagraph Vector [16], because it is widely adopted by search\nplatforms such as Solr [29] and IR libraries such as Apache\nLucene [30]. In Boolean retrieval, we report recall , deﬁned as\nratio of the number of documents retrieved by the approximate\nquery processing to the number of documents retrieved by the\nprecise execution of the query. In ranked retrieval, we report\nprecision-at-k (P@k), deﬁned as the percentage of the top k\ndocuments retrieved by the approximate query processing that\nis in the top kretrieved by the precise query execution. We\nreport precision instead of recall in ranked retrieval because\nusers typically cares most about the top kranked answers,\nwherekis typically small [19].\nRecommendation queries. We approximate user-centric CF,\nwhich takes as input a target user with past reviews/purchase\nhistory then outputs predicted ratings for unpurchased items.\nIt also generates a top-k recommended item list sorted by their\npredicted ratings.\nWe rearrange the reviews in the original data set to group\nall reviews written by a unique user together. Each group of\nreviews written by a unique user is then considered a single\ndocument. We randomly select 100 users and remove 20% of\neach selected user’s ratings from the data set to be used as test\ndata. We then cluster the remaining documents as we did for\nthe DIR queries and construct the index. Finally, we construct\n100 queries for the selected users, where each query outputs\nthe predicted ratings as computed by the CF algorithm for the\nusers/reviews in the test data.\nThe rating scale is 1-5 in the Amazon data set. We report\nthe mean squared error (MSE) and P@k to measure prediction\nperformance. MSE is computed for the predicted vs. actual\nratings as a measure of accuracy for the predictions. P@k is\nthe percentage of the items recommended in the top-k list that\n7\n\nQuery Domain Description Domain-specifc metrics\nphrase occurrence aggregation estimates frequency for target phrase. error bound\nBoolean retrieval DIR retrieves a (sub)set of documents that precisely match\na Boolean query.recall\nranked retrieval DIR retrieves top-k documents ranked by a given scoring\nfunction over a set of query terms.precision\nuser-centric CF recommendation predicts ratings on unbought products and outputs top-\nk recommendations for a user.MSE, precision\nTABLE I: Approximation queries and metrics summary\nData set Description Queries Size Document T-Time Idx-size\nWikipedia \u00185 million Wikipedia articles\nin XML format.aggregation 62GB each Wikipedia article 4.3h 125MB\nCCNews \u001822 million news articles\ncrawled from the Web in 2016,\nin JSON format.aggregation, DIR 65GB each news article 6.2h 280MB\nAmazon reviews \u0018142 million user-product in-\nteractions from 5/1996- 7/2014\nin JSON format.recommendation 55GB all reviews written by\nthe same user3.8h 87.5MB\nTABLE II: Dataset descriptions, notion of a document in the data set (Document) for PV-DBOW training, PV-DBOW training\ntime (T-Time) and index sizes after converted to LSH.\n0.00.20.40.60.81.01.21.4\nEst. Relative Rrror0.00.20.40.60.81.0Fraction\nEm\nSRCS\n(a) 1% Sampling Rate\n0.0 0.2 0.4 0.6 0.8 1.0\nEst. Relative Rrror0.00.20.40.60.81.0Fraction\nEm\nSRCS (b) 2.5% Sampling Rate\n0.0 0.2 0.4 0.6 0.8 1.0\nEst. Relative Rrror0.00.20.40.60.81.0Fraction\nEm\nSRCS (c) 5% Sampling Rate\n0.0 0.2 0.4 0.6 0.8 1.0\nEst. Relative Rrror0.00.20.40.60.81.0Fraction\nEm\nSRCS (d) 10% Sampling Rate\nFig. 3: CDFs of estimated relative error for phrase occurrences under different block sampling rates.\n1 2.5 5 10\nSampling Rates(%)01020304050SpeedupEm\nSRCS\n(a) Avg. Speedup\n1 2.5 5 10\nSampling Rates(%)0.00.20.40.60.8Relative ErrorEmE\nEmA\nSRCSE\nSRCSA (b) Actual and estimated rel. errors\nFig. 4: Average speedups and relative errors for phrase occur-\nrence query. (b) shows the comparison of estimated (E) and\nactual average relative error (A) using EM and SRCS (e.g.\nEmE means estimated relative error using EmApprox, SRCSA\nmeans actual relative error using SRCS).\nwere actually purchased by the target user. (We assume that a\nuser purchased an item if s/he reviewed it.)\nB. Results for aggregation queries\nFigure 3 plots the CDFs of estimated relative errors when\nrunning the 200 queries under EmApprox and SRCS at 1%,\n2.5%, 5% and 10% sampling rates. We observe that: (1)\nEmApprox consistently achieves smaller estimated relative\nerrors than SRCS at the same sampling rate; (2) the “tails” ofthe CDFs are “shorter,” meaning that there are fewer query an-\nswers with large estimated relative errors; and, (3) EmApprox\nachieves smaller maximum estimated relative errors. Under\nSRCS, the estimated relative errors can be large at very low\nsampling rates. For example, the estimated relative errors are\n80% and 95% for the 50th(median) and 90thpercentile,\nrespectively, at 1% sampling rate. Under EmApprox, they are\nreduced to 25% and 45%. Errors become much smaller with\nincreasing sampling rates.\nFigures 4(a) and (b) show average speedups compared to\nprecise execution and average relative errors (both estimated\nand actual), respectively. We observe that speedups are slightly\nsmaller for EmApprox compared to SRCS. This is because\nEmApprox does incur a small amount of extra overhead to\ncompute the sampling probabilities for blocks. On the other\nhand, as already mentioned, EmApprox achieves much smaller\nrelative errors than SRCS. Speciﬁcally, SRCS has to process\nroughly 4x the amount of data processed by EmApprox to\nachieve similar relative errors: for example, the relative errors\nunder EmApprox at 2.5% sampling rate are similar to SRCS’s\nrelative errors at 10% sampling rate.\nIn summary, EmApprox signiﬁcantly outperforms SRCS.\nIf the user can tolerate the estimated relative error proﬁle\n8\n\n0.0 0.2 0.4 0.6 0.8 1.0\nRecall0.00.20.40.60.81.0Fraction\nEm\nSRCS(a) Wikipedia, 25% sampling\n0.0 0.2 0.4 0.6 0.8 1.0\nRecall0.00.20.40.60.81.0Fraction\nEm\nSRCS (b) Wikipedia, 50% sampling\n0.0 0.2 0.4 0.6 0.8 1.0\nRecall0.00.20.40.60.81.0Fraction\nEm\nSRCS (c) CCNews, 25% sampling\n0.0 0.2 0.4 0.6 0.8 1.0\nRecall0.00.20.40.60.81.0Fraction\nEm\nSRCS (d) CCNews, 50% sampling\nFig. 5: CDFs of recall for Boolean retrieval queries over Wikipedia and CCNews data sets at different sampling rates.\n25(W) 50(W) 75(W) 25(C) 50(C) 75(C)\nSampling Rates(%)0.01.53.04.5SpeedupEm\nSRCS\n(a) Avg. Speedup - Boolean\n25(W) 50(W) 75(W) 25(C) 50(C) 75(C)\nSampling Rates(%)0.00.20.40.60.8RecallEm\nSRCS (b) Avg. Recall - Boolean\n10 25 50 75\nSampling Rates(%)0.00.20.40.60.8P@10Em\nSRCS (c) Avg. P@10 - ranked\nFig. 6: (a) and (b) show speedup and recalls averaged across the test queries under 25%, 50% and 75% sampling rates for\nBoolean retrieval ((W) and (C) represent Wikipedia and CCNews data sets respectively). (c) shows ranked retrieval precision\nP@10 achieved by EmApprox and SRCS at different sampling rates using Wikipedia data set.\nfor EmApprox at 10% sampling rate (i.e., 30% and 40%\nat 50thand 90thpercentiles, respectively), then EmApprox\nachieves an average speedup of \u001810x for the 200 aggregation\nqueries. Further, the user can trade off between accuracy and\nperformance by adjusting the sampling rate.\nC. Results for DIR queries\nFigure 5 plots the CDFs of recall for the Boolean queries\nunder EmApprox and SRCS when run on the Wikipedia\n(100 queries) and CCNews (100 queries) data sets. Similar\nto the observations for aggregation queries, we observe that\nEmApprox signiﬁcantly outperforms SRCS. Unfortunately,\neven at a high sampling rate (e.g., 50%), EmApprox misses\nsigniﬁcant fractions of relevant documents for many queries.\nThis is because relevant documents can be spread out across\nmany subcollections even with clustering, as clustering is\nperformed based on overall contents of documents, which may\nbe very different from the few words in a Boolean query.\nFigures 6(a) and (b) show Boolean retrieval’s speedups\nover precise execution and the average achieved recall rates,\nrespectively. We observe that the much higher sampling rates\nrequired to achieve higher recall rates constrain achievable\nspeedups.\nFigure 6(c) shows the average P@10 results for ranked\nretrieval for the Wikipedia data set. EmApprox signiﬁcantly\noutperforms SRCS at lower sampling rates, but even its\nachieved precision levels are unlikely to be acceptable. EmAp-\nprox achieves much better precision levels at higher samplingrates; e.g., 0.57 and 0.78 at 50% and 75% sampling rates,\nrespectively.\nIn summary, EmApprox signiﬁcantly outperforms SRCS.\nHowever, the nature of the problem, which is to ﬁnd speciﬁc\nitems in a large data set, reduces the effectiveness of sampling,\neven when sampling is directed by some knowledge of content.\nThus, EmApprox can only achieve modest speedups while\nachieving relatively high recall rates and precision levels. For\nexample, EmApprox achieves an average speedup of 1.3x at\na sampling rate of 75%. Achieved average recall and P@10\nare 0.89 and 0.78, respectively.\nD. Results for recommendation queries\nFigure 7 shows average MSE and P@10 under different\nsampling rates for EmApprox and SRCS. Similar to results for\nthe other two query types, EmApprox outperforms SRCS. The\ndifferences between the two approaches are less pronounced,\nhowever. For example, EmApprox outperforms SRCS by 8%\nand 7.4% for average MSE and average P@10, respectively, at\na sampling rate of 25%. This is likely due to the fact that user-\ncentric CF itself does not achieve high accuracy—the precise\nexecution achieves an average MSE of 1.015 and average\nP@10 of 0.32%—so that selecting customers most similar to\nthe target customer does not have a large impact compared\nto random selection. EmApprox incurs minimal overheads at\nquery processing time, however, and so its increased accuracy\nis still desirable, especially when the number of customers\nis large. EmApprox speeds up the query processing time by\nalmost 9x while degrading P@10 by 18.7% at 10% sampling\n9\n\n10 25 50 75\nSampling Rates(%)0.00.40.81.21.6MSEPrecise\nEm\nSRCS(a) Avg. MSE\n10 25 50 75\nSampling Rates(%)0.00.10.20.30.4P@10(%)Precise\nEm\nSRCS (b) Avg. P@10 (%)\nFig. 7: Recommendation results showing avgerage prediction\nacross the customers under different sampling rates including\nMSE and precision@10, where the horizontal line indicates\nthe results under precise execution.\nrate. Speedup is over 3x with 12.5% degradation of P@10 at\n25% sampling.\nE. Sensitivity Analysis\nIn this subsection, we study the effect of PV-DBOW and\nLSH bit vectors’ dimensions ( \u00151and\u00152respectively). For DIR\nand recommendation, we also study the number of clusters ( k)\nin K-means performed in ofﬂine preprocessing.\nVector dimensions. Intuitively,\u00151is more important than \u00152\nas it directly reﬂects how accurate vectors can capture the word\ndistributions in the data set. It is because mapping to LSH bit\nvectors is an approximation to its corresponding real-valued\nvectors thus is unlikely to enhance their performance. As a\nresult, we ﬁrst tune a suitable value of \u00151and then\u00152. We\nﬁnd that the performance of EmApprox gradually improves\nwith the increase of \u00151, and then stabilizes when its size is\nsufﬁciently large across our data sets. We would like a value\nof\u00151that is as small as possible since large values will\nsigniﬁcantly slow down the training process of PV-DBOW.\nWe choose 100 as the default vector size during training\nfor convenience across the evaluations, since we observe that\nthe performance of the queries are relatively stable when the\nvector size reaches 100. Note that the best value of \u00151is still\ndependent on different data sets and queries combination.\nFigures 8(a) and (b) show the estimated error of phrase\noccurrence and MSE of the recommendation’s relationship to\nthe dimensions of vector. The observation for the effect of \u00151\nis similar in tuning \u00152, where the performance ﬁrst gradually\nincreases then stabilizes as \u00152gets larger. Our objective of\ntuning\u00152is to match its performance with directly using\nreal-valued vectors. Figure 8(c) and (d) show the impact of\n\u00152to the phrase occurrence and ranked retrieval queries. We\nnotice that the smallest value of \u00152to match the performance\nof its real-valued counterpart is related to both the data set\nand speciﬁc queries. For the experiments, we picked 100 for\nconvenience because we have observed that this dimension has\nstably performed on par with the original real-valued vectors\nacross the queries.\nNumber of Clusters for K-means. We have found that\nthe performance of EmApprox on DIR and recommendationqueries is sensitive to k(number of clusters) in K-means.\nFigure 9(a) and (b) show the P@10 metric for ranked retrieval\nover the Wikipedia data set and recommendation under differ-\nent values of k. We observe that the performance gradually\nimproves as kincreases and plateaus as it approaches the\nnumber of HDFS blocks in a data set. It is because when\nthere are too many clusters, the data set will be close to have\nbeen randomly shufﬂed.\nF . Discussion\nOur results show that EmApprox is more effective for\nqueries that estimate results from samples, than for queries\nthat are seeking speciﬁc data items within large data sets.\nThis matches intuition since EmApprox’s index is meant to\nguide sampling toward subcollections that are most similar to\na query, rather than identifying data items that are guaranteed\nto be relevant to the query as the traditional inverted index is\nmeant to do. Thus, EmApprox can speed up aggregation and\nrecommendation queries by up to 10x if the user can tolerate\nmodest imprecision. In addition, the user can gracefully trade\noff imprecision with performance by adjusting the sampling\nrate. While we can observe the same trade offs for DIR, it\nis likely that users cannot tolerate the imprecision at lower\nsampling rates. For example, even at a sampling rate of\n50%, EmApprox achieves an average P@10 of less than 0.8,\nimplying that the approximate processing misses over 4 of\nthe top 10 documents. In comparison, under 10% sampling,\nEmApprox achieves an estimated relative error of 18.2% for\naggregation, and a degradation on average of 18.7% in P@10\nfor recommendation queries.\nVIII. C ONCLUSION AND FUTURE WORK\nWe present an approximation framework for a wide range of\nanalytical queries over large text data sets, using a light-weight\nindex based on an NLP model (PV-DBOW). We formally\nshow that the training objective of PV-DBOW maximizes\nthe generative probability of a query given a collection of\ndocuments. Our experiment shows that our light-weight index\ncan reduce the execution time by almost an order of magnitude\nwhile degrading gracefully in approximation quality with\ndecreasing sampling rates. EmApprox is particularly useful\nfor exploratory text analytics.\nFuture Work. We believe our proposed index can be ef-\nfective under more scenarios and data sets. For example,\nour system can be used to efﬁciently detect the sentiment\nfor a large text data set; it can also potentially be used\ntoward selecting relevant users/products for more complex\nrecommendation methods, such as model-based algorithms\ninvolving multiple data sources [3]. We also plan to extend our\nkey methodology—learning an index directly from the data set\nto facilitate approximate computation—to other types of data\nsets, such as visual data, time series, etc.\nREFERENCES\n[1] “Google Book Ngrams,” 2019, http://storage.googleapis.com/books/\nngrams/books/datasetsv2.html.\n10\n\n10305075100 150 200\nvector dimension0.160.200.240.280.32Error\n5%\n10%(a) Aggegation - Error\n10305075100 150 200\nvector dimension1.11.21.31.41.5MSE\n10%\n25% (b) Recommendation-MSE\n1025 50 75100 125 150\nLSH size0.20.40.60.8Error\nLSH\nReal-valued (c) Aggegation - Error\n1025 50 75100 125 150\nLSH size0.10.20.30.4P@10\nLSH\nReal-valued (d) Ranked retrieval - P@10\nFig. 8: (a) and (b) show the impact of vector dimension over the phrase occurrence (aggregation) estimated error under 5%\nand 10% sampling rates, and MSE in the recommendation results under 10% and 25% sampling rates. (c) and (d) show the\nimpact of LSH bits to the results using the Wikipedia data set under 1% and 10% sampling rates\n0.1 0.5 1.0 1.5 2.0 2.5 3.0\nk (# of clusters) 1e30.00.10.20.30.4P@10\n10%\n25%\n# of blocks\n(a) Ranked retrieval\n0.1 0.50.8 1.3 1.9 2.32.6\nk (# of clusters) 1e30.10.20.30.4P@10\n10%\n25%\n# of blocks (b) Recommendation\nFig. 9: Impact of k in the K-means clustering over P@10\nfor ranked retrieval (Wikipedia) and CF, under 5% and 25%\nsampling rates.\n[2] “Common crawl,” https://registry.opendata.aws/commoncrawl/, 2018.\n[3] J. McAuley and J. Leskovec, “Hidden factors and hidden topics:\nUnderstanding rating dimensions with review text,” in Proceedings of\nthe 7th ACM Conference on Recommender Systems , ser. RecSys ’13.\nNew York, NY , USA: ACM, 2013, pp. 165–172. [Online]. Available:\nhttp://doi.acm.org/10.1145/2507157.2507163\n[4] S. Agarwal, B. Mozafari, A. Panda, H. Milner, S. Madden, and\nI. Stoica, “Blinkdb: Queries with bounded errors and bounded\nresponse times on very large data,” in Proceedings of the 8th\nACM European Conference on Computer Systems , ser. EuroSys ’13.\nNew York, NY , USA: ACM, 2013, pp. 29–42. [Online]. Available:\nhttp://doi.acm.org/10.1145/2465351.2465355\n[5] X. Zhang, J. Wang, and J. Yin, “Sapprox: Enabling efﬁcient and\naccurate approximations on sub-datasets with distribution-aware online\nsampling,” Proc. VLDB Endow. , vol. 10, no. 3, pp. 109–120, Nov.\n2016. [Online]. Available: https://doi.org/10.14778/3021924.3021928\n[6] I. Goiri, R. Bianchini, S. Nagarakatte, and T. D. Nguyen,\n“Approxhadoop: Bringing approximations to mapreduce frameworks,” in\nProceedings of the Twentieth International Conference on Architectural\nSupport for Programming Languages and Operating Systems , ser.\nASPLOS ’15. New York, NY , USA: ACM, 2015. [Online]. Available:\nhttp://doi.acm.org/10.1145/2694344.2694351\n[7] J. Peng, D. Zhang, J. Wang, and J. Pei, “Aqp++: connecting approx-\nimate query processing with aggregate precomputation for interactive\nanalytics,” in Proceedings of the 2018 International Conference on\nManagement of Data . ACM, 2018, pp. 1477–1492.\n[8] Q. Le and T. Mikolov, “Distributed representations of sentences and\ndocuments,” in International Conference on Machine Learning , 2014,\npp. 1188–1196.\n[9] K. Shvachko, H. Kuang, S. Radia, R. Chansler et al. , “The hadoop\ndistributed ﬁle system.” in MSST , vol. 10, 2010, pp. 1–10.\n[10] P. Indyk and R. Motwani, “Approximate nearest neighbors: Towards\nremoving the curse of dimensionality,” in Proceedings of the Thirtieth\nAnnual ACM Symposium on Theory of Computing , ser. STOC ’98.\nNew York, NY , USA: ACM, 1998, pp. 604–613. [Online]. Available:\nhttp://doi.acm.org/10.1145/276698.276876\n[11] F. Ricci, L. Rokach, and B. Shapira, “Recommender systems: introduc-\ntion and challenges,” in Recommender systems handbook . Springer,2015, pp. 1–34.\n[12] S. Lohr, Sampling: Design and Analysis , ser. Advanced (Cengage\nLearning). Cengage Learning, 2009. [Online]. Available: https:\n//books.google.com/books?id=aSXKXbyNlMQC\n[13] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The case\nfor learned index structures,” in Proceedings of the 2018 International\nConference on Management of Data . ACM, 2018, pp. 489–504.\n[14] G. Hu, S. Rigo, D. Zhang, and T. Nguyen, “Approximation with error\nbounds in spark,” in 2019 IEEE 27th International Symposium on Mod-\neling, Analysis, and Simulation of Computer and Telecommunication\nSystems (MASCOTS) , Oct 2019, pp. 61–73.\n[15] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,\n“Distributed representations of words and phrases and their composi-\ntionality,” in Advances in neural information processing systems , 2013,\npp. 3111–3119.\n[16] Q. Ai, L. Yang, J. Guo, and W. B. Croft, “Analysis of the paragraph\nvector model for information retrieval,” in Proceedings of the 2016\nACM International Conference on the Theory of Information Retrieval ,\nser. ICTIR ’16. New York, NY , USA: ACM, 2016, pp. 133–142.\n[Online]. Available: http://doi.acm.org/10.1145/2970398.2970409\n[17] O. Levy and Y . Goldberg, “Neural word embedding as implicit matrix\nfactorization,” in Proceedings of the 27th International Conference\non Neural Information Processing Systems - Volume 2 , ser. NIPS’14.\nCambridge, MA, USA: MIT Press, 2014, pp. 2177–2185. [Online].\nAvailable: http://dl.acm.org/citation.cfm?id=2969033.2969070\n[18] A. Kulkarni and J. Callan, “Selective search: Efﬁcient and effective\nsearch of large textual collections,” ACM Transactions on Information\nSystems (TOIS) , vol. 33, no. 4, p. 17, 2015.\n[19] W. B. Croft, D. Metzler, and T. Strohman, Search engines: Information\nretrieval in practice . Addison-Wesley Reading, 2015, vol. 283.\n[20] S. Robertson, H. Zaragoza et al. , “The probabilistic relevance frame-\nwork: Bm25 and beyond,” Foundations and Trends R\rin Information\nRetrieval , vol. 3, no. 4, pp. 333–389, 2009.\n[21] X. Ning, C. Desrosiers, and G. Karypis, “A comprehensive survey\nof neighborhood-based recommendation methods,” in Recommender\nsystems handbook . Springer, 2015, pp. 37–76.\n[22] S. Zhong, “Efﬁcient online spherical k-means clustering,” in Proceed-\nings. 2005 IEEE International Joint Conference on Neural Networks,\n2005. , vol. 5. IEEE, 2005, pp. 3180–3185.\n[23] M. D. Hirschhorn, “The am-gm inequality,” The Mathematical Intelli-\ngencer , vol. 29, no. 4, pp. 7–7, 2007.\n[24] “PySpark,” 2017, https://spark.apache.org/docs/latest/api/python/index.\nhtml.\n[25] “Gensim,” 2018, https://radimrehurek.com/gensim/.\n[26] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\nS. Ghemawat, G. Irving, M. Isard et al. , “Tensorﬂow: A system for large-\nscale machine learning,” in 12thfUSENIXgSymposium on Operating\nSystems Design and Implementation ( fOSDIg16), 2016, pp. 265–283.\n[27] “Wikipedia database,” http://en.wikipedia.org/wiki/Wikipedia database.,\n2018.\n[28] “Common crawl news dataset,” http://commoncrawl.org/2016/10/\nnews-dataset-available., 2016.\n[29] “Apache Solr,” 2019, http://lucene.apache.org/solr/.\n[30] “http://lucene.apache.org/,” 2019, http://lucene.apache.org/.\n11",
  "textLength": 59559
}