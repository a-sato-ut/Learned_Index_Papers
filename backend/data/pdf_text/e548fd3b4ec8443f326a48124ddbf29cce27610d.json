{
  "paperId": "e548fd3b4ec8443f326a48124ddbf29cce27610d",
  "title": "Lookback Prophet Inequalities",
  "pdfPath": "e548fd3b4ec8443f326a48124ddbf29cce27610d.pdf",
  "text": "Lookback Prophet Inequalities\nZiyad Benomar\nENSAE, Ecole Polytechnique,\nFairPlay joint team\nziyad.benomar@ensae.frDorian Baudry\nDepartment of Statistics,\nUniversity of Oxford\ndorian.baudry@ox.ac.uk\nVianney Perchet\nCREST, ENSAE, Criteo AI LAB\nFairplay joint team\nvianney.perchet@normalesup.org\nAbstract\nProphet inequalities are fundamental optimal stopping problems, where a decision-\nmaker observes sequentially items with values sampled independently from known\ndistributions, and must decide at each new observation to either stop and gain the\ncurrent value or reject it irrevocably and move to the next step. This model is\noften too pessimistic and does not adequately represent real-world online selection\nprocesses. Potentially, rejected items can be revisited and a fraction of their value\ncan be recovered. To analyze this problem, we consider general decay functions\nD1, D2, . . ., quantifying the value to be recovered from a rejected item, depending\non how far it has been observed in the past. We analyze how lookback improves, or\nnot, the competitive ratio in prophet inequalities in different order models. We show\nthat, under mild monotonicity assumptions on the decay functions, the problem\ncan be reduced to the case where all the decay functions are equal to the same\nfunction x7→γx, where γ= inf x>0infj≥1Dj(x)/x. Consequently, we focus on\nthis setting and refine the analyses of the competitive ratios, with upper and lower\nbounds expressed as increasing functions of γ.\n1 Introduction\nOptimal stopping problems constitute a classical paradigm of decision-making under uncertainty\n[Dynkin, 1963] Typically, in online algorithms, these problems are formalized as variations of the\nsecretary problem [Lindley, 1961] or the prophet inequality [Krengel and Sucheston, 1977]. In the\ncontext of the prophet inequality, the decision-maker observes a finite sequence of items, each having\na value drawn independently from a known probability distribution. Upon encountering a new item,\nthe decision-maker faces the choice of either accepting it and concluding the selection process or\nirreversibly rejecting it, with the objective of maximizing the value of the selected item. However,\nwhile the prophet inequality problem is already used in scenarios such as posted-price mechanism\ndesign [Hajiaghayi et al., 2007] or online auctions [Syrgkanis, 2017], it might present a pessimistic\nmodel of real-world online selection problems. Indeed, it is in general possible in practice to revisit\npreviously rejected items and potentially recover them or at least recover a fraction of their value.\nConsider for instance an individual navigating a city in search of a restaurant. When encountering\none, they have the choice to stop and dine at this place, continue their search, or revisit a previously\npassed option, incurring a utility cost that is proportional to the distance of backtracking. In another\nexample drawn from the real estate market, homeowners receive offers from potential buyers. The\ndecision to accept or reject an offer can be revisited later, although buyer interest may have changed,\nresulting in a potentially lower offer or even a lack of interest. Lastly, in the financial domain, an\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).arXiv:2406.06805v2  [cs.DS]  17 Nov 2024\n\nagent may choose to sell an asset at its current price or opt for a lookback put option, allowing them\nto sell at the asset’s highest price over a specified future period. To make a meaningful comparison\nbetween the two, one must account for factors such as discounting (time value of money) and the\ncost of the option.\n1.1 Formal problem and notation\nTo encompass diverse scenarios, we propose a general way to quantify the cost incurred by the\ndecision-maker for retrieving a previously rejected value.\nDefinition 1.1 (Decay functions) .LetD= (D1, D2, . . .)be a sequence of non-negative functions\ndefined on [0,∞). It is a sequence of decay functions if\n(i)D1(x)≤xfor all x≥0,\n(ii) the sequence (Dj(x))j≥1is non-increasing for all x≥0,\n(iii) the function Djis non-decreasing for all j≥1.\nIn the context of decay functions D, if a value xis rejected, the algorithm can recover Dj(x)afterj\nsubsequent steps. The three conditions defining decay functions serve as fundamental prerequisites\nfor the problem. The first and second conditions ensure that the recoverable value of a rejected item\ncan only diminish over time, while the final condition implies that an increase in the observed value x\ncorresponds to an increase in the potential recovered value. Although the non-negativity of the decay\nfunctions is non-essential, we retain it for convenience, as we can easily revert to this assumption by\nconsidering that the algorithm has a reward of zero by not selecting any item.\nThe motivating examples that we introduced can be modeled respectively with decay functions\nof the form Dj(x) =x−cjwhere (cj)j≥1is a non-decreasing positive sequence, Dj(x) =ξjx\nwithξj∼ B(pj)and(pj)j≥1a non-increasing sequence of probabilities, and Dj(x) =λjxwith\nλ∈[0,1]. In one of these examples (housing market), the natural model is to use random decay\nfunctions : the buyer makes the same offer if they are still interested, and offers 0otherwise. Definition\n1.1 can be easily extended to consider this case. However, to enhance the clarity of the presentation,\nwe only discuss the deterministic case in the rest of the paper. In Appendix D, we explain how all the\nproofs and theorems can be generalized to that case.\nTheD-prophet inequality. For any decay functions D, we define the D-prophet inequality problem,\nwhere the decision maker, knowing D, observes sequentially the values X1, . . . , X n, with Xidrawn\nfrom a known distribution Fifor all i∈[n]. If they decide to stop at some step τ, then instead of\ngaining Xτas in the classical prophet inequality, they can choose to select the current item Xτand\nhave its full value, or select any item Xiwithi < τ among the rejected ones but only recover a\nfraction Dτ−i(Xi)≤Xiof its value. Therefore, if an algorithm ALG stops at step τits reward is\nALGD(X1, . . . , X n) = max {Xτ, D1(Xτ−1), D2(Xτ−2), . . . , D τ−1(X1)}\n= max\n0≤i≤τ−1{Di(Xτ−i)},\nwith the convention D0(x) =x. If the algorithm does not stop at any step before n, then its reward is\nALGD(X1, . . . , X n) = max 1≤i≤n{Di(Xπ(n−i+1))}, which corresponds to τ=n+ 1.\nRemark 1.1. As in the standard prophet inequality, an algorithm is defined by its stopping time, i.e.,\nthe rule set to decide whether to stop or not. Hence, if DandD′are two different sequences of decay\nfunctions, any algorithm for the D-prophet inequality, although its stopping time might depend on the\nparticular sequence of functions D, is also an algorithm for the D′-prophet inequality. Consider for\nexample an algorithm ALG with stopping time τ(D)that depends on D. Its reward in the D′-prophet\ninequality is ALGD′(X1, . . . , X n) = max 0≤i≤τ−1{D′\ni(Xτ(D)−i)}.\nObservation order. Several variants of the prophet inequality problem have been studied, depending\non the order of observations. The standard model is the adversarial (or fixed) order: The instance\nof the distributions F1, . . . , F nis chosen by an adversary, and the algorithm observes the samples\nX1∼F1, . . . , X n∼Fnin this order [Krengel and Sucheston, 1977, 1978]. In the random order\nmodel, the adversary can again choose the distributions, but the algorithm observes the samples in a\n2\n\nuniformly random order. Another setting in which the observation order is no longer important is the\nIID model [Hill and Kertz, 1982, Correa et al., 2021b], where all the values are sampled independently\nfrom the same distribution F. TheD-prophet inequality is well-defined in each of these different\norder models: if the items are observed in the order Xπ(1), . . . , X π(n)withπa permutation of [n],\nthen the reward of the algorithm is ALGD(X1, . . . , X n) = max 0≤i≤τ−1{Di(Xπ(τ−i))}. In this\npaper, we study the D-prophet inequality in the three models we presented, providing lower and\nupper bounds in each of them.\nCompetitive ratio. In theD-prophet inequality, an input instance Iis a finite sequence of probability\ndistributions (F1, . . . , F n). Thus, for any instance I, we denote by E[ALGD(I)]the expected reward\nofALG given Ias input, and we denote by E[OPT(I)]the expected maximum of independent\nrandom variables (Xi)i∈[n], where Xi∼Fi. With these notations, we define the competitive ratio,\nwhich will be used to measure the quality of the algorithms.\nDefinition 1.2 (Competitive ratio) .LetDbe a sequence of decay functions and ALG an algorithm.\nWe define the competitive ratio of ALG by\nCRD(ALG) = inf\nIE[ALGD(I)]\nE[OPT(I)],\nwith the infimum taken over the tuples of all sizes of non-negative distributions with finite expectation.\nAn algorithm is said to be α-competitive if its competitive ratio is at least α, which means that for\nany possible instance I, the algorithm guarantees a reward of at least αE[OPT(I)]. The notion\nof competitive ratio is used more broadly in competitive analysis as a metric to evaluate online\nalgorithms [Borodin and El-Yaniv, 2005].\n1.2 Contributions\nIt is trivial that non-zero decay functions Dguarantee a better reward compared to the classical\nprophet inequality. However, in general, this is not sufficient to conclude that the standard upper\nbounds or the competitive ratio of a given algorithm can be improved. Hence, a first key question is:\nwhat condition on Dis necessary to surpass the conventional upper bounds of the classical prophet\ninequality? Surprisingly, the answer hinges solely on the constant γD, defined as follows,\nγD= inf\nx>0inf\nj≥1\u001aDj(x)\nx\u001b\n. (1)\nIn the adversarial order model, we demonstrate that the optimal competitive ratio achievable in the\nD-prophet inequality is determined by the parameter γDalone. Additionally, in both the random\norder and IID models, we demonstrate the essential requirement of γD>0for breaking the upper\nbounds of the classical prophet inequality. In particular, this implies that no improvement can be\nmade with decay functions of the form Dj(x) =x−cjwithcj>0, orDj(x) =λjxwithλ∈[0,1).\nSubsequently, we develop algorithms and provide upper bounds in the D-prophet inequality, uniquely\ndependent on the parameter γD. We illustrate them in Figure 1, comparing them with the identity\nfunction γ7→γ, which is a trivial lower bound.\n1.3 Related work\nProphet inequalities. The first prophet inequality was proven by Krengel and Sucheston [Krengel\nand Sucheston, 1977, 1978] in the setting where the items are observed in a fixed order, demonstrating\nthat the dynamic programming algorithm has a competitive ratio of 1/2, which is the best possible.\nIt was shown later that the same guarantee can be obtained with simpler algorithms [Samuel-Cahn,\n1984, Kleinberg and Weinberg, 2012], accepting the first value above a carefully chosen threshold.\nFor a more comprehensive and historical overview, we refer the interested reader to surveys on the\nproblem such as [Lucier, 2017, Correa et al., 2019]. Prophet inequalities have immediate applications\nin mechanism design [Hajiaghayi et al., 2007, Deng et al., 2022, Psomas et al., 2022, Makur et al.,\n2024], auctions [Syrgkanis, 2017, Dütting et al., 2020], resource management [Sinclair et al., 2023],\nand online matching [Cohen et al., 2019, Ezra et al., 2020, Jiang et al., 2021, Papadimitriou et al.,\n2021, Brubach et al., 2021]. Many variants and related problems have been studied, including, for\nexample, the matroid prophet inequality [Kleinberg and Weinberg, 2012, Feldman et al., 2016],\n3\n\n0.0 0.2 0.4 0.6 0.8 1.0\n/u1D6FED∈[0,1]1/21−1\n/u1D452≈0.632√\n3−1≈0.7324−log 3\n4−2//u1D4522≈0.7781\n[Adversarial] lower and upper bound\n[Random and IID] lower bound\n[Random] upper bound\n[IID] upper bound\n/u1D6FED/uni21A6.endl→/u1D6FEDFigure 1: Lower and upper bounds on the competitive ratio in the D-prophet inequality depending on\nγD, in the adversarial order (Thm 4.3), random order (Thm 4.4) and IID (Thm 4.6) models\nprophet inequality with advice [Diakonikolas et al., 2021], and variants with fairness considerations\n[Correa et al., 2021a, Arsenis and Kleinberg, 2022].\nRandom order and IID models. Esfandiari et al. [2017] introduced the prophet secretary problem,\nwhere items are observed in a uniformly random order, and they proved a (1−1\ne)-competitive\nalgorithm. Correa et al. [2021c] showed later a competitive ratio of 0.669, and Harb [2024] enhanced\nit to 0.6724, which currently stands as the best-known solution for the problem. They also proved\nan upper bound of√\n3−1≈0.732, which was improved to 0.7254 in [Bubna and Chiplunkar,\n2023] then 0.723 in [Giambartolomei et al., 2023]. Addressing the gap between the lower and upper\nbound remains an engaging and actively pursued open question. On the other hand, the study of\nprophet inequalities with IID random variables dates back to papers such as [Hill and Kertz, 1982,\nKertz, 1986], demonstrating guarantees on the dynamic programming algorithm. The problem was\ncompletely solved in [Correa et al., 2021b], where the authors show that the competitive ratio of the\ndynamic programming algorithm is 0.745, thus it constitutes an upper bound on the competitive ratio\nof any algorithm, and they give a simpler adaptive threshold algorithm matching it. Another setting\nthat we do not study in this paper, is the order selection model, where the decision-maker can choose\nthe order in which the items are observed, knowing their distributions [Chawla et al., 2010, Beyhaghi\net al., 2021, Peng and Tang, 2022].\nBeyond the worst-case. In recent years, there has been increasing interest in exploring ways to\nexceed the worst-case upper bounds of online algorithms by providing the decision-maker with\nadditional capabilities. A notable research avenue is learning-augmented algorithms [Lykouris\nand Vassilvtiskii, 2018], which equip the decision-maker with predictions or hints about unknown\nvariables of the problem. Multiple problems have been studied in this framework, such as scheduling\n[Purohit et al., 2018, Lassota et al., 2023, Benomar and Perchet, 2024b], matching [Antoniadis et al.,\n2020, Dinitz et al., 2021, Chen et al., 2022], caching [Antoniadis et al., 2023, Chlkedowski et al.,\n2021, Christianson et al., 2023], the design of data structures [Kraska et al., 2018, Lin et al., 2022,\nBenomar and Coester, 2024], and in particular, online selection problems [Dütting et al., 2021, Sun\net al., 2021, Benomar et al., 2024, Benomar and Perchet, 2024a, Diakonikolas et al., 2021]. More\nrelated to our setting, the ability to revisit items in online selection has been studied in problems\nsuch as the multiple-choice prophet inequality, where the algorithm can select up to kitems and its\nreward is the maximum selected value [Assaf and Samuel-Cahn, 2000]. This allows for revisiting\nup to kitems, chosen during the execution, for final acceptance or rejection decisions. Similarly, in\nPandora’s box problem [Weitzman, 1978, Kleinberg et al., 2016] and its variants [Esfandiari et al.,\n2019, Gergatsouli and Tzamos, 2022, Atsidakou et al., 2024, Gergatsouli and Tzamos, 2024, Berger\net al., 2024], the decision maker decides the observation order of the items, but a cost ciis paid for\nobserving each value Xi, with the gain being the maximum observed value minus the total opening\ncosts. A very recent work investigates a scenario closely related to the lookback prophet inequality\n4\n\n[Ekbatani et al., 2024] where, upon selecting a candidate Xi, the decision-maker has the option to\ndiscard it and choose a new value Xjat any later step j, at a buyback cost of fXi, where f >0. The\nauthors present an optimal algorithm for the case when f≥1, although the problem remains open\nforf∈(0,1). Other problems were studied in similar settings, such as online matching [Ekbatani\net al., 2022] and online resource allocation [Ekbatani et al., 2023].\n2 From D-prophet to the D∞-prophet inequality\nLet us consider a sequence Dof decay functions. By Definition 1.1, for any x∈[0,∞]the sequence\n(Dj(x))j≥1converges, since it is non-increasing and non-negative. Hence, there exists a mapping\nD∞such that for any x≥0,limj→∞Dj(x) =D∞(x). Furthermore, we can easily verify that D∞\nis non-decreasing and satisfies D∞(x)∈[0, x]for all x≥0.\nThanks to these properties, we obtain that (D∞)j≥1also satisfies Definition 1.1, and is hence a\nvalid sequence of decay functions. We thus refer to the corresponding problem as the D∞-prophet\ninequality. Since Dj≥D∞for any j≥1, it is straightforward that the stopping problem with the\ndecay functions D∞would be less favorable to the decision-maker. More precisely, for any random\nvariables X1, . . . , X n, observation order π, and algorithm ALG with stopping time τ, it holds that\nALGD(X1, . . . , X n):= max {Xπ(τ),max\ni<τDτ−i(Xπ(i))} ≥max{Xπ(τ),max\ni<τD∞(Xπ(i))},\nwhich corresponds to the output of ALG (with the same decision rule) when all the decay functions\nare equal to D∞. Therefore, any guarantees established for algorithms in the D∞-prophet inequality\nnaturally extend to the D-prophet inequality. However, it remains uncertain whether the D-prophet\ninequality can yield improved competitive ratios compared to the D∞-prophet inequality. In the\nfollowing, we prove that this is not the case, for all the order models presented in Section 1.\nTheorem 2.1. LetD∞be the pointwise limit of the sequence of decay functions D= (Dj)j≥1. Then\nfor any instance I= (F1, . . . , F n)of non-negative distributions, it holds in the adversarial and the\nrandom order models that\n∀ALG :CRD(ALG)≤sup\nAE[AD∞(I)]\nE[OPT(I)], (2)\nwhere the supremum is taken over all the online algorithms A. In the IID model, the same inequality\nholds with an additional O(n−1/3)term, which depends only on the size nof the instance.\nThe main implication of Theorem 2.1 is the following corollary.\nCorollary 2.1.1. In the adversarial order and the random order models, if A∞is an optimal algorithm\nfor the D∞-prophet inequality, i.e. with maximal competitive ratio, then A∞is also optimal for the\nD-prophet inequality. Moreover, it holds that\nCRD(A∞) =CRD∞(A∞).\nA direct consequence of this result is that, in the adversarial and the random order models, the\nasymptotic decay D∞entirely determines the competitive ratio that is achievable and the upper\nbounds for the D-prophet inequality. Therefore, we can restrict our analysis to algorithms designed\nfor the problem with identical decay function. In the IID model, the same conclusion holds if the\nworst-case instances are arbitrarily large, making the additional O(n−1/3)term vanish. This is the\ncase in particular in the classical IID prophet inequality [Hill and Kertz, 1982].\n2.1 Sketch of the proof of Theorem 2.1\nWhile we use different techniques for each order model considered, all the proofs share the same\nunderlying idea. Given any instance Iof non-negative distributions, we build an alternative instance\nJsuch that the reward of any algorithm on Iwith decay functions D= (Dj)jis at most its reward\nonJwith decay functions all equal to D∞. To do this, we essentially introduce an arbitrarily large\nnumber of zero values between two successive observations drawn from distributions belonging to\nI. Hence, under J, the algorithm cannot recover much more than a fraction D∞(X)for any past\nobservation Xcollected from a distribution F∈I.\n5\n\nIn the adversarial case, implementing this idea is straightforward, since nature can build Jby directly\ninserting mzeros between each pair of consecutive values, and the result is obtained by making m\narbitrarily large. For the random order model, we use the same instance J, but extra steps are needed\nto prove that the number of steps between two non-zero values is very large with high probability.\nMoving to the IID model, an instance Iis defined by a pair (F, n), where Fis a non-negative\ndistribution, and nis the size of the instance. In this scenario, we consider an instance consisting of\nm > n IID random variables (Yi)i∈[m], each sampled from Fwith probability n/m , and equal to\nzero with the remaining probability. We again achieve the desired result by letting mbe arbitrarily\nlarge compared to n. However, the number of variables sampled from Fis not fixed; it follows a\nBinomial distribution with parameters (m, n/m ). We control this variability by using concentration\ninequalities, which causes the additional term O(n−1/3).\n3 From D∞-prophet to the γD-prophet inequality\nAs discussed in Section 2, Theorem 2.1 implies that, for either establishing upper bounds or guarantees\non the competitive ratios of algorithms, it is sufficient to study the D∞-prophet inequality, where all\nthe decay functions are equal to D∞. The remaining question is then to determine which functions\nD∞allow to improve upon the upper bounds of the classical prophet inequality. Before tackling this\nquestion, let us make some observations regarding algorithms in the D∞-prophet inequality.\nIn the D∞-prophet inequality, it is always possible to have a reward of D∞(max i∈[n]Xi)by rejecting\nall the items and then selecting the maximum by the end. Thus, it is suboptimal to stop at a step i\nwhere Xi≤D∞(max j<iXj). An algorithm respecting this decision rule is called rational .\nLemma 3.1. For any rational algorithm ALG in the D∞-prophet inequality, if we denote by τits\nstopping time, then for any instance I= (F1, . . . , F n)andXi∼Fifor all i∈[n]we have\nALGD∞(X1, . . . , X n) =ALG0(X1, . . . , X n) +D∞\u0000\nmax\ni∈[n]Xi\u0001\n1τ=n+1,\nwhere ALG0denotes the reward of the algorithm in the standard prophet inequality. Moreover, the\noptimal dynamic programming algorithm in the D∞-prophet inequality is rational.\nThe best competitive ratio in the D∞-prophet inequality is achieved, possibly among others, by the\noptimal dynamic programming algorithm, which is a rational algorithm by the previous Lemma.\nHence, it suffices to prove upper bounds on rational algorithms. We use this observation to prove the\nnext propositions.\nProposition 3.2. In the D∞-prophet inequality, if infx>0D∞(x)\nx= 0, then it holds, in any order\nmodel, that\n∀ALG :CRD∞(ALG)≤sup\nACR0(A), (3)\nwhere the supremum is taken over all the online algorithms A, andCR0denotes the competitive ratio\nin the standard prophet inequality.\nProposition 3.2 implies that if infx>0D∞(x)\nx= 0, then, in any order model, any upper bound on\nthe competitive ratios of all algorithms in the classical prophet inequality is also an upper bound on\nthe competitive ratios of all algorithm in the D∞-prophet inequality. Consequently, for surpassing\nthe upper bounds of the classical prophet inequality, it is necessary to have, for some γ >0, that\nD∞(x)≥γxfor all x≥0. Furthermore, the next Proposition allows giving upper bounds in the\nD∞-prophet inequality that depend only on infx>0D∞(x)\nx.\nProposition 3.3. Letγ= inf x>0D∞(x)/x, and 0< a < b . Consider an instance Iof distributions\nwith support in {0, a, b}, then in any order model and for any algorithm ALG we have that\nCRD∞(ALG)≤sup\nAE[Aγ(I)]\nE[OPT(I)],\nwhere E[Aγ(I)]is the reward of Aif all the decay functions were equal to x7→γx.\nThe core idea for proving this proposition is that rescaling an instance, i.e. considering (rXi)i∈[n]\ninstead of (Xi)i∈[n], has no impact in the classical prophet inequality. However, in the D∞-prophet\n6\n\ninequality, rescaling can be exploited to adjust the ratioD∞(rx)\nrx. By considering instances with\nrandom variables taking values in {0, a, b}almost surely, where a < b , a reasonable algorithm facing\nsuch an instance would never reject the value b. Consequently, the value it recovers from rejected\nitems is either D∞(0) = 0 orD∞(a). Rescaling this instance by a factor r=s/aand taking the\nratio to the expected maximum, the termD∞(s)\nsappears, with sa free parameter that can be chosen\nto satisfyD∞(s)\ns→infx>0D∞(x)\nx=γD.\nAs a consequence, if infx>0D∞(x)\nx=γ, then any upper bound obtained in the γ-prophet inequality\n(when the decay functions are all equal to x7→γx) using instances of random variables (X1, . . . , X n)\nsatisfying Xi∈ {0, a, b}a.s. for all i, is also an upper bound in the D∞-prophet inequality.\nImplication Consider any sequence Dof decay functions, and define\nγD:= inf\nx>0\u001aD∞(x)\nx\u001b\n= inf\nx>0inf\nj≥1\u001aDj(x)\nx\u001b\n.\nFor any x >0andj≥1it holds that Dj(x)≥γDx, therefore, any guarantees on the competitive\nratio of an algorithm in the γD-prophet inequality are valid in the D-prophet inequality, under any\norder model. Furthermore, combining Theorem 2.1 and Proposition 3.3, we obtain that for any\ninstance Iof random variables taking values in a set {0, a, b}it holds that\n∀ALG :CRD(ALG)≤sup\nAE[AγD(I)]\nE[OPT(I)],\nwith an additional term of order O(n−1/3)in the IID model. In the particular case where γD= 0,\nProposition 3.3 with Theorem 2.1 give a stronger result, showing that no algorithm can surpass the\nupper bounds of the classical prophet inequality. This is true also for the IID model since the instances\nused to prove the tight upper bound of ≈0.745are of arbitrarily large size [Hill and Kertz, 1982].\nTherefore, by studying the γ-prophet inequality for γ∈[0,1], we can prove upper bounds and lower\nbounds on the D-prophet inequality for any sequence Dof decay functions.\n4 The γ-prophet inequality\nWe study in this section the γ-prophet inequality, where all the decay functions are equal to x7→γx,\nfor some γ∈[0,1]. For any algorithm ALG with stopping time τand random variables X1, . . . , X n,\nif the observation order is π, we use the notation\nALGγ(X1, . . . , X n) = max {Xπ(τ), γX π(τ−1), . . . , γX π(1)}.\nand we denote by CRγ(ALG)the competitive ratio of ALG in this setting. In the following, we\nprovide theoretical guarantees for the γ-prophet inequality.\nFor each observation order, we first derive upper bounds on the competitive ratio of any algorithm,\ndepending on γ, using only hard instances satisfying the condition of Proposition 3.3. This would\nguarantee that the upper bounds extend to the D-prophet inequality if γD=γ. Then, we design\nsingle-threshold algorithms with well-chosen thresholds depending on γand the distributions, with\ncompetitive ratios improving with γ. A crucial property of single-threshold algorithms, which we use\nto estimate their competitive ratios, is that their reward satisfies\nALGγ(X1, . . . , X n) =ALG0(X1, . . . , X n) +γ(max\niXi) 1(max iXi<θ). (4)\nThe additional term appearing due to γdepends only on max i∈[n]Xi, which is the reward of the\nprophet against whom we compete. This property is not satisfied by more general class of algorithms\nsuch as multiple-threshold algorithms, where each observation Xπ(i)is compared with a threshold θi.\nRemark 4.1. We only consider instances with continuous distributions in the proofs of lower bounds.\nThe thresholds θconsidered are such that Pr(max i∈[n]Xi≥θ) =g(γ, n, π ), with gdepending on\nγ, the order model πand the size of the instance n. Such a threshold is always guaranteed to exist\nwhen the distributions are continuous. However, as in the prophet inequality, the algorithms can\nbe easily adapted to non-continuous distributions by allowing stochastic tie-breaking. A detailed\nstrategy for doing this can be found for example in [Correa et al., 2021c].\n7\n\nBefore delving into the study of the different models, we provide generic lower and upper bounds,\nwhich depend solely on the bounds of the classical prophet inequality and γ.\nProposition 4.2. In any order model, if αis a lower bound in the classical prophet inequality, and β\nan upper bound, then, in the γ-prophet inequality\n(i) there exists a trivial algorithm with a competitive ratio of at least max{γ, α},\n(ii) the competitive ratio of any algorithm is at most (1−γ)β+γ.\n4.1 Adversarial order\nWe first consider the adversarial order model, and prove the upper bound of1\n2−γ. Then, we provide a\nsingle-threshold algorithm with a competitive ratio matching this upper bound, hence fully solving\ntheγ-prophet inequality in this adversarial order model.\nTheorem 4.3. In the adversarial order model, the competitive ratio of any algorithm is at most\n1\n2−γ. Furthermore, there exists a single threshold algorithm with a competitive ratio1\n2−γ: given any\ninstance (F1, . . . , F n), this is achieved with the threshold θsatisfying\nPrX1∼F1,...,X n∼Fn(max i∈[n]Xi≤θ) =1\n2−γ.\nThe upper bound in the previous theorem is proved using instances satisfying the condition of\nProposition 3.3. Hence it extends to the D∞- then to the D-prophet inequality, with γ=γD, by\nProposition 3.3 and Theorem 2.1.\n4.2 Random order\nConsider now that the items are observed in a uniformly random order Xπ(1), . . . , X π(n), and\nX∗= max i∈[n]Xi. As for the adversarial model, we first prove an upper bound on the competitive\nratio as a function of γ, and then prove a lower bound for a single-threshold algorithm. However, for\nthis model, there is a gap between the two bounds, as illustrated in Figure 1.\nWe first prove an upper bound that depends on γ, matching the upper bound√\n3−1of Correa et al.\n[2021c] when γ= 0and equal to 1when γ= 1. Our single-threshold algorithm has a competitive\nratio of at least (1−1\ne)when γ= 0, which is the best competitive ratio of a single threshold algorithm\nin the prophet inequality [Esfandiari et al., 2017, Correa et al., 2021c], and equal to 1forγ= 1.\nTheorem 4.4. The competitive ratio of any algorithm ALG in the γ-prophet inequality with random\norder satisfies\nCRγ(ALG)≤(1−γ)3/2(p\n3−γ−p\n1−γ) +γ .\nFurthermore, denoting by pγis the unique solution to the equation 1−(1−γ)p=1−p\n−logp, the\nsingle-threshold algorithm ALG θwithPrX1∼F1,...,X n∼Fn(max i∈[n]Xi≤θ) =pγsatisfies\nCRγ(ALG)≥1−(1−γ)pγ.\nSimilarly to the adversarial order model, we used instances satisfying the condition of Proposition\n3.3 to prove the upper bound, thus it extends to the D-prophet inequality with γ=γD.\nWhile the equation defining pγcannot be solved analytically, the solution can easily be computed\nnumerically for any γ∈[0,1]. Before moving to the IID case, we propose in the following a more\nexplicit lower bound derived from Theorem 4.4.\nCorollary 4.4.1. In the random order model, the single threshold algorithm with a threshold θ\nsatisfying Pr(max i∈[n]Xi≥θ) =1/e\n1−(1−1/e)γhas a competitive ratio of at least 1−(1−γ)/e\n1−(1−1/e)γ.\n4.3 IID Random Variables\nIn the classical IID prophet inequality, [Hill and Kertz, 1982] showed that the competitive ratio of\nany algorithm is at most ≈0.745. The proof of this upper bound is hard to generalize for the IID\nγ-prophet inequality. As an alternative, we prove a weaker upper bound, which equals ≈0.778\n8\n\nforγ= 0and1forγ= 1, and the proof relies on instances of arbitrarily large size satisfying the\ncondition of Proposition 3.3, hence the upper bound can be extended to the D-prophet inequality.\nSubsequently, we present a single-threshold algorithm with the same competitive ratio as the random\norder algorithm. However, the proof is different, leveraging the fact that the variables are identically\ndistributed. More precisely, we introduce a single-threshold algorithm with guarantees that depend\non the size nof the instance, then we show that its competitive ratio is at least that of the algorithm\npresented in Theorem 4.4, with equality when napproaches infinity.\nAlthough it might look surprising that the obtained competitive ratio in the IID model is not better\nthan that of the random-order model, the same behavior occurs in the classical prophet inequality.\nIndeed, Li et al. [2022] established that no single-threshold algorithm can achieve a competitive ratio\nbetter than 1−1/ein the standard prophet inequality with IID random variables, which is also the\nbest possible with a single-threshold algorithm in the random order. However, considering larger\nclasses of algorithms, the competitive ratios achieved in the IID model are better than those of the\nrandom order model.\nWe describe the algorithm and give a first lower bound on its reward depending on the size of the\ninstance in the following lemma.\nLemma 4.5. Letan,γbe the unique solution of the equation\u0010\n1\n(1−a/n)n−1\u0011\u00001\na−1\u0001\n=γ, then for\nany IID instance X1, . . . , X n, the algorithm with threshold θsatisfying Pr(X1> θ) =an,γ\nnhas a\nreward of at least\n1\nan,γ\u0010\n1−\u0010\n1−an,γ\nn\u0011n\u0011\nE[max\ni∈[n]Xi].\nWe can prove that the reward presented in the Lemma above is strictly better than that of the random\norder model. However, both are asymptotically equal as we show in the following theorem.\nTheorem 4.6. The competitive ratio of any algorithm in the IID γ-prophet inequality is at most\nU(γ) = 1−(1−γ)e2log(3−γ)−(2−γ)\n2(2e2−1)−(3e2−1)γ.\nIn particular, Uis increasing, U(0) =4−log 3\n2(2−1\ne2)≈0.778andU(1) = 1 . Furthermore, there exists a\nsingle-threshold algorithm ALG θsatisfying\nCRγ(ALG θ)≥1−(1−γ)pγ,\nwhere pγis defined in Theorem 4.4.\nTo prove the upper bound, we used instances satisfying the condition of Proposition 3.3, guaranteeing\nthat it remains true in the D∞-prophet inequality with γ=γD. On the other hand, Theorem 2.1\nensures that the upper bound extends to the D-prophet inequality, but with an additional O(1/n1/3)\nterm. The latter does change the result, as we considered instances of arbitrarily large size n→ ∞ .\n5 Conclusion\nIn this paper, we addressed the D-prophet inequality problem, which models a very broad spectrum of\nonline selection scenarios, accommodating various observation order models and allowing to revisit\nrejected candidates at a cost. The problem extends the classic prophet inequality, corresponding to\nthe special case where all decay functions are zero. The main result of the paper is a reduction from\nthe general D-prophet inequality to the γ-prophet inequality, where all the decay functions equal to\nx7→γxfor some constant γ∈[0,1]. Subsequently, we provide algorithms and upper bounds for\ntheγ-prophet inequality, which remain valid, by the previous reduction, in the D-prophet inequality.\nNotably, the proved upper and lower bounds match each other for the adversarial order model, hence\ncompletely solving the problem. Our analysis paves the way for more practical applications of\nprophet inequalities, and advances efforts towards closing the gap between theory and practice in\nonline selection problems.\n9\n\nLimitations and future work\nBetter upper bounds in the D∞-prophet inequality. Proposition 3.3 establishes that upper bounds\nproved in the γ-prophet inequality using instances of random variables with support in some set\n{0, a, b}remain true in the D∞-prophet inequality, hence in the D-prophet inequality by Theorem\n2.1. This is enough to establish a tight upper bound in the adversarial order model, but not in the\nrandom order and IID models. An interesting question to explore is if more general upper bounds can\nbe extended, or not, from the γ- to the D-prophet inequality.\nAlgorithms for the γ-prophet inequality. As explained in Section 4, our analysis of the competitive\nratio of single-threshold algorithms relies on the identity (4), which is not satisfied for instance by\nmultiple-threshold algorithms. In the adversarial order model, we proved that the optimal competitive\nratio1/(2−γ)can be achieved with a single-threshold algorithm. However, this is not the case in the\nrandom order or IID models. An interesting research avenue is to study other classes of algorithms in\ntheγ-prophet inequality.\nAcknowledgements\nThis research was supported in part by the French National Research Agency (ANR) in the framework\nof the PEPR IA FOUNDRY project (ANR-23-PEIA-0003) and through the grant DOOM ANR-\n23-CE23-0002. It was also funded by the European Union (ERC, Ocean, 101071601). Views and\nopinions expressed are however those of the author(s) only and do not necessarily reflect those of the\nEuropean Union or the European Research Council Executive Agency. Neither the European Union\nnor the granting authority can be held responsible for them.\nDorian Baudry thanks the support of the French National Research Agency: ANR-19-CHIA-02\nSCAI, ANR-22-SRSE-0009 Ocean, and ANR-23-CE23-0002 Doom; and of the European Research\nCouncil (GTIR project)\nReferences\nAntonios Antoniadis, Themis Gouleakis, Pieter Kleer, and Pavel Kolev. Secretary and online matching\nproblems with machine learned advice. Advances in Neural Information Processing Systems , 33:\n7933–7944, 2020.\nAntonios Antoniadis, Joan Boyar, Marek Eliás, Lene Monrad Favrholdt, Ruben Hoeksma, Kim S\nLarsen, Adam Polak, and Bertrand Simon. Paging with succinct predictions. In International\nConference on Machine Learning , pages 952–968. PMLR, 2023.\nMakis Arsenis and Robert Kleinberg. Individual fairness in prophet inequalities. In Proceedings of\nthe 23rd ACM Conference on Economics and Computation , pages 245–245, 2022.\nDavid Assaf and Ester Samuel-Cahn. Simple ratio prophet inequalities for a mortal with multiple\nchoices. Journal of applied probability , 37(4):1084–1091, 2000.\nAlexia Atsidakou, Constantine Caramanis, Evangelia Gergatsouli, Orestis Papadigenopoulos, and\nChristos Tzamos. Contextual pandora’s box. In Proceedings of the AAAI Conference on Artificial\nIntelligence , volume 38, pages 10944–10952, 2024.\nZiyad Benomar and Christian Coester. Learning-augmented priority queues. In The Thirty-\neighth Annual Conference on Neural Information Processing Systems , 2024. URL https:\n//openreview.net/forum?id=1ATLLgvURu .\nZiyad Benomar and Vianney Perchet. Advice querying under budget constraint for online algorithms.\nAdvances in Neural Information Processing Systems , 36, 2024a.\nZiyad Benomar and Vianney Perchet. Non-clairvoyant scheduling with partial predictions. In\nForty-first International Conference on Machine Learning , 2024b.\nZiyad Benomar, Evgenii Chzhen, Nicolas Schreuder, and Vianney Perchet. Addressing bias in\nonline selection with limited budget of comparisons. In The Thirty-eighth Annual Conference on\n10\n\nNeural Information Processing Systems , 2024. URL https://openreview.net/forum?id=\nBdGFgKrlHl .\nBen Berger, Tomer Ezra, Michal Feldman, and Federico Fusco. Pandora’s problem with deadlines.\nInProceedings of the AAAI Conference on Artificial Intelligence , volume 38, pages 20337–20343,\n2024.\nHedyeh Beyhaghi, Negin Golrezaei, Renato Paes Leme, Martin Pál, and Balasubramanian Sivan.\nImproved revenue bounds for posted-price and second-price mechanisms. Operations Research ,\n69(6):1805–1822, 2021.\nAllan Borodin and Ran El-Yaniv. Online computation and competitive analysis . cambridge university\npress, 2005.\nBrian Brubach, Nathaniel Grammel, Will Ma, and Aravind Srinivasan. Improved guarantees for\noffline stochastic matching via new ordered contention resolution schemes. Advances in Neural\nInformation Processing Systems , 34:27184–27195, 2021.\nArchit Bubna and Ashish Chiplunkar. Prophet inequality: Order selection beats random order. In\nProceedings of the 24th ACM Conference on Economics and Computation , pages 302–336, 2023.\nShuchi Chawla, Jason D Hartline, David L Malec, and Balasubramanian Sivan. Multi-parameter\nmechanism design and sequential posted pricing. In Proceedings of the forty-second ACM sympo-\nsium on Theory of computing , pages 311–320, 2010.\nJustin Chen, Sandeep Silwal, Ali Vakilian, and Fred Zhang. Faster fundamental graph algorithms via\nlearned predictions. In International Conference on Machine Learning , pages 3583–3602. PMLR,\n2022.\nJakub Chlkedowski, Adam Polak, Bartosz Szabucki, and Konrad Tomasz .Zolna. Robust learning-\naugmented caching: An experimental study. In International Conference on Machine Learning ,\npages 1920–1930. PMLR, 2021.\nNicolas Christianson, Junxuan Shen, and Adam Wierman. Optimal robustness-consistency tradeoffs\nfor learning-augmented metrical task systems. In International Conference on Artificial Intelligence\nand Statistics , pages 9377–9399. PMLR, 2023.\nAlon Cohen, Avinatan Hassidim, Haim Kaplan, Yishay Mansour, and Shay Moran. Learning to\nscreen. Advances in Neural Information Processing Systems , 32, 2019.\nJose Correa, Patricio Foncea, Ruben Hoeksma, Tim Oosterwijk, and Tjark Vredeveld. Recent\ndevelopments in prophet inequalities. ACM SIGecom Exchanges , 17(1):61–70, 2019.\nJose Correa, Andres Cristi, Paul Duetting, and Ashkan Norouzi-Fard. Fairness and bias in online\nselection. In International conference on machine learning , pages 2112–2121. PMLR, 2021a.\nJosé Correa, Patricio Foncea, Ruben Hoeksma, Tim Oosterwijk, and Tjark Vredeveld. Posted price\nmechanisms and optimal threshold strategies for random arrivals. Mathematics of operations\nresearch , 46(4):1452–1478, 2021b.\nJose Correa, Raimundo Saona, and Bruno Ziliotto. Prophet secretary through blind strategies.\nMathematical Programming , 190(1-2):483–521, 2021c.\nYuan Deng, Vahab Mirrokni, and Hanrui Zhang. Posted pricing and dynamic prior-independent\nmechanisms with value maximizers. Advances in Neural Information Processing Systems , 35:\n24158–24169, 2022.\nIlias Diakonikolas, Vasilis Kontonis, Christos Tzamos, Ali Vakilian, and Nikos Zarifis. Learning\nonline algorithms with distributional advice. In International Conference on Machine Learning ,\npages 2687–2696. PMLR, 2021.\nMichael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Faster\nmatchings via learned duals. Advances in neural information processing systems , 34:10393–10406,\n2021.\n11\n\nPaul Dütting, Thomas Kesselheim, and Brendan Lucier. An o (log log m) prophet inequality for\nsubadditive combinatorial auctions. ACM SIGecom Exchanges , 18(2):32–37, 2020.\nPaul Dütting, Silvio Lattanzi, Renato Paes Leme, and Sergei Vassilvitskii. Secretaries with advice. In\nProceedings of the 22nd ACM Conference on Economics and Computation , pages 409–429, 2021.\nEvgenii Borisovich Dynkin. The optimum choice of the instant for stopping a markov process. Soviet\nMathematics , 4:627–629, 1963.\nFarbod Ekbatani, Yiding Feng, and Rad Niazadeh. Online matching with cancellation costs. arXiv\npreprint arXiv:2210.11570 , 2022.\nFarbod Ekbatani, Yiding Feng, and Rad Niazadeh. Online resource allocation with buyback: Optimal\nalgorithms via primal-dual. In Proceedings of the 24th ACM Conference on Economics and\nComputation , pages 583–583, 2023.\nFarbod Ekbatani, Rad Niazadeh, Pranav Nuti, and Jan V ondrák. Prophet inequalities with cancellation\ncosts. In Proceedings of the 56th Annual ACM Symposium on Theory of Computing , pages 1247–\n1258, 2024.\nHossein Esfandiari, MohammadTaghi Hajiaghayi, Vahid Liaghat, and Morteza Monemizadeh.\nProphet secretary. SIAM Journal on Discrete Mathematics , 31(3):1685–1701, 2017.\nHossein Esfandiari, MohammadTaghi HajiAghayi, Brendan Lucier, and Michael Mitzenmacher. On-\nline pandora’s boxes and bandits. In Proceedings of the AAAI Conference on Artificial Intelligence ,\nvolume 33, pages 1885–1892, 2019.\nTomer Ezra, Michal Feldman, Nick Gravin, and Zhihao Gavin Tang. Online stochastic max-weight\nmatching: prophet inequality for vertex and edge arrival models. In Proceedings of the 21st ACM\nConference on Economics and Computation , pages 769–787, 2020.\nMoran Feldman, Ola Svensson, and Rico Zenklusen. Online contention resolution schemes. In\nProceedings of the twenty-seventh annual ACM-SIAM symposium on Discrete algorithms , pages\n1014–1033. SIAM, 2016.\nEvangelia Gergatsouli and Christos Tzamos. Online learning for min sum set cover and pandora’s\nbox. In International Conference on Machine Learning , pages 7382–7403. PMLR, 2022.\nEvangelia Gergatsouli and Christos Tzamos. Weitzman’s rule for pandora’s box with correlations.\nAdvances in Neural Information Processing Systems , 36, 2024.\nGiordano Giambartolomei, Frederik Mallmann-Trenn, and Raimundo Saona. Prophet inequalities:\nSeparating random order from order selection. arXiv preprint arXiv:2304.04024 , 2023.\nMohammad Taghi Hajiaghayi, Robert Kleinberg, and Tuomas Sandholm. Automated online mecha-\nnism design and prophet inequalities. In AAAI , volume 7, pages 58–65, 2007.\nElfarouk Harb. New prophet inequalities via poissonization and sharding, 2024.\nTheodore P Hill and Robert P Kertz. Comparisons of stop rule and supremum expectations of iid\nrandom variables. The Annals of Probability , pages 336–345, 1982.\nZhihao Jiang, Pinyan Lu, Zhihao Gavin Tang, and Yuhao Zhang. Online selection problems against\nconstrained adversary. In International Conference on Machine Learning , pages 5002–5012.\nPMLR, 2021.\nRobert P Kertz. Stop rule and supremum expectations of iid random variables: a complete comparison\nby conjugate duality. Journal of multivariate analysis , 19(1):88–112, 1986.\nRobert Kleinberg and Seth Matthew Weinberg. Matroid prophet inequalities. In Proceedings of the\nforty-fourth annual ACM symposium on Theory of computing , pages 123–136, 2012.\nRobert Kleinberg, Bo Waggoner, and E Glen Weyl. Descending price optimally coordinates search.\narXiv preprint arXiv:1603.07682 , 2016.\n12\n\nTim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned index\nstructures. In Proceedings of the 2018 international conference on management of data , pages\n489–504, 2018.\nUlrich Krengel and Louis Sucheston. Semiamarts and finite values. 1977.\nUlrich Krengel and Louis Sucheston. On semiamarts, amarts, and processes with finite value.\nProbability on Banach spaces , 4:197–266, 1978.\nAlexandra Anna Lassota, Alexander Lindermayr, Nicole Megow, and Jens Schlöter. Minimalistic\npredictions to schedule jobs with online precedence constraints. In International Conference on\nMachine Learning , pages 18563–18583. PMLR, 2023.\nBo Li, Xiaowei Wu, and Yutong Wu. Query efficient prophet inequality with unknown iid distributions.\narXiv preprint arXiv:2205.05519 , 2022.\nHonghao Lin, Tian Luo, and David Woodruff. Learning augmented binary search trees. In Interna-\ntional Conference on Machine Learning , pages 13431–13440. PMLR, 2022.\nDenis V Lindley. Dynamic programming and decision theory. Journal of the Royal Statistical Society:\nSeries C (Applied Statistics) , 10(1):39–51, 1961.\nBrendan Lucier. An economic view of prophet inequalities. ACM SIGecom Exchanges , 16(1):24–47,\n2017.\nThodoris Lykouris and Sergei Vassilvtiskii. Competitive caching with machine learned advice. In\nInternational Conference on Machine Learning , pages 3296–3305. PMLR, 2018.\nAnuran Makur, Marios Mertzanidis, Alexandros Psomas, and Athina Terzoglou. On the robustness\nof mechanism design under total variation distance. Advances in Neural Information Processing\nSystems , 36, 2024.\nChristos Papadimitriou, Tristan Pollner, Amin Saberi, and David Wajc. Online stochastic max-weight\nbipartite matching: Beyond prophet inequalities. In Proceedings of the 22nd ACM Conference on\nEconomics and Computation , pages 763–764, 2021.\nBo Peng and Zhihao Gavin Tang. Order selection prophet inequality: From threshold optimization to\narrival time design. In 2022 IEEE 63rd Annual Symposium on Foundations of Computer Science\n(FOCS) , pages 171–178. IEEE, 2022.\nAlexandros Psomas, Ariel Schvartzman Cohenca, and S Weinberg. On infinite separations between\nsimple and optimal mechanisms. Advances in Neural Information Processing Systems , 35:4818–\n4829, 2022.\nManish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ml predictions.\nAdvances in Neural Information Processing Systems , 31, 2018.\nEster Samuel-Cahn. Comparison of threshold stop rules and maximum for independent nonnegative\nrandom variables. the Annals of Probability , pages 1213–1216, 1984.\nSean R Sinclair, Felipe Vieira Frujeri, Ching-An Cheng, Luke Marshall, Hugo De Oliveira Barbalho,\nJingling Li, Jennifer Neville, Ishai Menache, and Adith Swaminathan. Hindsight learning for mdps\nwith exogenous inputs. In International Conference on Machine Learning , pages 31877–31914.\nPMLR, 2023.\nBo Sun, Russell Lee, Mohammad Hajiesmaili, Adam Wierman, and Danny Tsang. Pareto-optimal\nlearning-augmented algorithms for online conversion problems. Advances in Neural Information\nProcessing Systems , 34:10339–10350, 2021.\nVasilis Syrgkanis. A sample complexity measure with applications to learning optimal auctions.\nAdvances in Neural Information Processing Systems , 30, 2017.\nMartin Weitzman. Optimal search for the best alternative , volume 78. Department of Energy, 1978.\n13\n\nA From D-prophet to the D∞-prophet inequality\nIn this section, we prove the reduction from the D-prophet to the D∞-prophet inequality problem in\nthe adversarial and random order models, and the reduction up to an additional O(n−1/3)term in the\nIID model. First, we prove Corollary 2.1.1, which is the principal implication of Theorem 2.1.\nA.1 Proof of Corollary 2.1.1\nProof. Let us denote A∗,∞the algorithm taking optimal decisions for any instance in the D∞-prophet\ninequality (obtained via dynamic programming). Then, by Theorem 2.1 we obtain for the adversarial\nand random order models that\nsup\nALGCRD(ALG)≤inf\nIsup\nAE[AD∞(I)]\nE[OPT(I)]= inf\nIE[AD∞\n∗,∞(I)]\nE[OPT(I)]=CRD∞(A∗,∞) = sup\nACRD∞(A).\n(5)\nSince CRD(ALG)≥CRD∞(ALG)for any algorithm, we deduce that (5)is an equality. If we\nconsider now any algorithm A∞that is optimal for the D∞-prophet inequality, not necessarily A∗,∞,\nthen Equation (5) provides\nCRD(A∞)≥CRD∞(A∞) = sup\nALGCRD(ALG).\nThe previous inequality is again an equality, and it implies that ¯A∞is also optimal, in the sense of\nthe competitive ratio, for the D-prophet inequality, and\nCRD(A∞) =CRD∞(A∞).\nA.2 Auxilary Lemma\nThe efficiency of the proof scheme introduced in Section 2.1 relies on the following key argument: if\n(Dj)j≥1converges pointwise to D∞, then for any algorithm Aand any instance I, the output of A\nwhen all the decay functions are equal to Dmconverges to its output when all the decay functions are\nequal to D∞. IfX1, . . . , X nare the realizations of Iobserved by Aand if σis the order in which\nthey are observed, then denoting τthe stopping time of Awe can write that\nE[ADm(I)]−E[AD∞(I)]\n=E[max{Xσ(τ), Dm(max\ni<τXσ(i))}]−E[max{Xσ(τ), D∞(max\ni<τXσ(i))}]\n≤max\nπ∈Sn\nq∈[n]\u001a\nE[max{Xπ(q), Dm(max\ni<qXπ(i))}]−E[max{Xπ(q), D∞(max\ni<qXπ(i))}]\u001b\n,\nwhere Snis the set of all permutations of [n]. The latter upper bound is independent of σandA. We\nshow in the following lemma that it converges to 0when m→ ∞ .\nLemma A.1. LetSnbe the set of all permutations of [n]. For any fixed instance I= (F1, . . . , F n),\nconsidering Xi∼Fifor all i∈[n], define for all m≥1\nϵm(I) = max\nπ∈Sn\nq∈[n]\u001a\nE[max{Xπ(q), Dm(max\ni<qXπ(i))}]−E[max{Xπ(q), D∞(max\ni<qXπ(i))}]\u001b\n.\nIfE[max i∈[n]Xi]<∞, then lim\nm→∞ϵm(I) = 0 .\nProof. Let us denote f1, . . . , f nthe respective probability density functions of X1, . . . , X q. For\nanyq∈[n]andπ∈ Sn, let us define for all m≥0the function φπ,q\nm: [0,∞)q→[0,∞)by\nφπ,q\nm(x1, . . . , x q) = max {xπ(q), Dm(max i<qxπ(i))} −max{xπ(q), D∞(max i<qxπ(i))}.φπ,q\nmis\npositive because Dm≥D∞. The sequence (φπ,q\nm)mis non-increasing, converges to 0pointwise,\nand is dominated by (x1, . . . , x q)7→max i∈[q]xi, which is integrable with respect to the probability\n14\n\nmeasure (x1, . . . , x q)7→Qq\ni=1f(xi). Therefore, using the dominated convergence theorem, we\ndeduce that limm→∞E[φπ,q\nm(X1, . . . , X q)] = 0 . It follows that\nlim\nm→∞ϵm(I) = lim\nm→∞\u0010\nmax\nπ∈Sn\nq∈[n]E[φπ,q\nm(X1, . . . , X q)]\u0011\n= 0.\nA.3 Proof of Theorem 2.1\nProof of Theorem 2.1. We provide a separate proof for each of the adversarial order, random order\nand IID models.\nAdversarial order LetI= (F1, . . . , F n)be any instance and Xi∼Fifor all i∈[n]. Consider\nthe instance Im= (Y1, . . . , Y mn), where Ykm∼Fkfor any k∈[n]andYi= 0 a.s. for all\ni /∈ {m,2m, . . . , mn }. It is clear that no reasonable algorithm would stop at a zero value: if the\ncurrent observation is 0it is preferable to wait for a non-null value, or it would have been preferable\nto stop at the previous non-null value. Hence, τis a multiple of m:τ=ρmfor some ρ∈N⋆. Given\nthatDj(0) = 0 for all jand the sequence (Dj)jis non-increasing, we have that\nE[ALGD(Im)] =E[max\ni≤τDτ−i(Yi)]\n=E[max\nk≤ρDτ−km(Ykm)]\n=E[max\nk≤ρDρm−km(Xk)]\n=E[max{Xρ,max\nk<ρD(ρ−k)m(Xk)}]\n≤E[max{Xρ,max\nk<ρDm(Xk)}]\n≤E[max{Xρ,max\nk<ρD∞(Xk)}] +ϵm(I),\nwhere ϵm(I)is defined in Lemma A.1. We can then use that the first right-hand term is the output\nof some other algorithm that would choose a stopping time ρwhen facing Iin the context of the\nD∞-prophet inequality. More precisely, consider the algorithm Amwhich, given any instance\nI= (F1, . . . , F n), simulates the behavior of ALG facing the sequence Im, where at each step\ni∈[mn]\n• ifi /∈ {m, . . . , nm }:ALG observes Yi= 0,\n• otherwise, if i=kmfor some k∈[n]:Amobserves XkandALG observes Ykm=Xk\n• ifALG stops on Yρm, thenAmalso stops, and its reward is Xρ.\nAm(X1, . . . , X n)stops at the same value as ALG(Y1, . . . , Y m), their reward in the D∞-prophet\ninequality is the same, and since max i∈[n]Xi= max i∈[mn]Yithis yields to\nCRD(ALG)≤E[ALGD(Im)]\nE[OPT(Im)]≤E[AD∞\nm(I)] +ϵm(I)\nE[OPT(I)]≤sup\nA:algoE[AD∞(I)]\nE[OPT(I)]+ϵm(I)\nE[OPT(I)],\nand taking the limit when m→ ∞ gives the result, by making the second term vanish.\nRandom order LetI= (F1, . . . , F n)be an instance of distributions and Xi∼Fifori∈[n].\nUsing the notation δ0for the Dirac distribution in 0, we consider Im= (F1, . . . , F n, δ0, . . . , δ 0)\ncontaining mcopies of δ0so that the observations from this instance always contain at least mnull\nvalues. Let Y1, . . . , Y mbe a realization of this instance. For simplicity, say that Yi=Xifori∈[n],\nandYi= 0fori > n .\nWe first show that when m→ ∞ , since the observation order is drawn uniformly at random,\nthe algorithm observes a large number of zeros between every two random variables drawn from\n(F1, . . . , F n). Let us denote by πthe uniformly random order in which the observations are received,\n15\n\ni.e. the algorithms observes Yπ(1), Yπ(2), . . ., and let ℓ≥1be some positive integer, and t1, . . . , t n\nbe the increasing indices in which the variables Y1, . . . , Y nare observed, i.e. t1< . . . < t nand\n{t1, . . . t n}={π−1(1), . . . , π−1(n)}. Therefore, any observation outside {Yπ(t1), . . . , Y π(t1)}is\nzero. Using the notation L= min i∈[n−1]|ti+1−ti|, we obtain that\nPr(L≤ℓ) = Pr( ∪n−1\ni=1{ti+1−ti≤ℓ})\n= Pr\u0000\n∪n\nk=1∪k−1\nj=1\b\n|π−1(k)−π−1(j)| ≤ℓ\t\u0001\n≤n(n−1)\n2Pr(|π−1(1)−π−1(2)| ≤ℓ)\n=n(n−1)\n2Pr(\u0000\n∪n+m\nk=1(π−1(1) = k, π−1(2)∈ {k−ℓ, . . . , k +ℓ} \\ {k})\n≤n(n−1)\n2×(n+m)×1\nn+m×2ℓ\nn+m−1\n≤n2ℓ\nm.\nTaking ℓ=√m, we find that Pr(L≤ℓ)≤n2/√m. Therefore, for any algorithm ALG , observing\nthat the reward of ALGDis at most max i∈[n]Xia.s., and by independence of max i∈[n]XiandL,\nwe deduce that\nE[ALGD(Im)] =E[ALGD(Im) 1L>ℓ] +E[ALGD(Im) 1L≤ℓ]\n≤E[ALGD(Im)|L > ℓ ] +E[(max\ni∈[n]Xi) 1L≤ℓ]\n≤E[ALGD(Im)|L > ℓ ] +E[max\ni∈[n]Xi]n2\n√m. (6)\nLet us denote τthe stopping time of ALG andtρ= max j∈[n]{tj:tj≤τ}the last time when a\nvariable (Xj)j∈[n]was observed by ALG . The sequence of functions (Dj)j≥1is non-increasing,\nhence\nE[ALGD(Im)|L > ℓ ] =E[max\ni∈[τ]Dτ−i(Yπ(i))|L > ℓ ]\n=E[max\nj≤iDτ−tj(Yπ(tj))|L > ℓ ] (7)\n≤E[max\nj≤iDtρ−tj(Yπ(tj))|L > ℓ ] (8)\n=E[max\b\nYπ(tρ),max\nj<ρDtρ−tj(Yπ(tj))\t\n|L > ℓ ]\n≤E[max\b\nYπ(tρ),max\nj<ρDℓ(Yπ(tj))\t\n], (9)\nEquation (7)holds because the only non-zero values up to step τare(Yπ(tj))j∈[ρ]. Inequality (8)uses\nthat the sequence (Dj)j≥1is non-increasing, and (9)uses, in addition to that, the independence of L\nand(Yπ(tj))j∈[n]. We now argue that the term E[max\b\nYπ(tρ),max j<ρDℓ(Yπ(tj))\t\n]is the expected\nreward of an algorithm in the Dℓ-inequality. Given that πis a uniform random permutation of [n+m]\nand by definition of t1, . . . , t n, the application σ:k∈[n]7→π(tk)is a random permutation of [n].\nTherefore we consider the algorithm Amthat receives as input the instance I= (F1, . . . , F n), then\nconsiders the array u= (1, . . . , 1,0, . . . , 0)composed of nvalues equal to 1andmzero values, and\na uniformly random permutation πof[n+m], then simulates ALGD(Im)as follows: at each step\nj∈[n+m]\n• ifuπ(j)= 0, thenALG observes the value Yπ(j)= 0,\n• ifuπ(j)= 1, thenAmobserves the next value Xσ(k), andALG observes Yπ(j)=Xσ(k),\n• when ALG decides to stop, Amalso stops, and its reward is the current value Xσ(k).\nWith this construction, (Yj)j∈[n+m]is indeed a realization of the instance Im, andAmstops on the\nlast value sampled from F1, . . . , F nobserved by ALG . Therefore, denoting ρthe stopping time of\n16\n\nAm, and ϵℓ(I)as defined in Lemma A.1, we have\nE[ALGD(Im)|L > ℓ ]≤E[max\b\nYπ(tρ),max\nj<ρDℓ(Yπ(tj))\t\n]\n=E[max\b\nXσ(ρ),max\nj<ρDℓ(Xσ(tj))\t\n]\n=E[ADℓ\nm(I)]\n≤E[AD∞\nm(I)] +ϵℓ(I)\n≤sup\nA:algoE[AD∞(I)] +ϵℓ(I).\nTaking ℓ=√mand substituting into Equation (6), then observing that E[OPT(I)] =E[OPT(Im)],\ngives that\nCRD(ALG)≤E[ALGD(Im)]\nE[OPT(Im)]≤sup\nA:algoE[AD∞(I)]\nE[OPT(I)]+ϵ√m(I)\nE[OPT(I)]+n2\n√m.\nFinally, taking m→ ∞ and using Lemma A.1, we deduce that\nCRD(ALG)≤sup\nA:algoE[AD∞(I)]\nE[OPT(I)],\nwhich completes the proof for the random order.\nIID random variables For any probability distribution Fon[0,∞)and for any n≥1we denote\nE[OPT(F, n)]the expected maximum of nindependent random variables drawn from F, and for any\nalgorithm ALG we denote E[ALGD(F, n)]its expected output when given nIID variable sampled\nfrom Fas input. The proof of Theorem 2.1 for this last model is much more technical than for\nprevious models, so we first prove several auxiliary results that we will later use to provide a concise\nproof of the last part of the theorem.\nLemma A.2. For any probability distribution Fandn≥1,∆≥0, we have\nE[OPT(F, n+ ∆)] ≤\u0012\n1 +∆\nn\u0013\nE[OPT(F, n)].\nProof. We first write\nPr(OPT(F, n+ ∆) > x) = 1−F(x)n+∆\n=\u0012\n1 +F(x)n1−F(x)∆\n1−F(x)n\u0013\n(1−F(x)n)\n=\u0012\n1 +F(x)n1−F(x)∆\n1−F(x)n\u0013\nPr(OPT(F, n)> x),\nand then use that\nF(x)∆=e∆ log( F(x))≥1 + ∆ log( F(x)) = 1 −∆\nnlog(1/F(x)n)≥1−∆\nn\u00121−F(x)n\nF(x)n\u0013\n,\nso we directly obtain\nF(x)n1−F(x)∆\n1−F(x)n≤∆\nn,\nwhich gives that\nPr(OPT(F, n+ ∆) > x)≤\u0012\n1 +∆\nn\u0013\nPr(OPT(F, n)> x).\nAs we consider non-negative random variables, it follows directly that\nE[OPT(F, n+ ∆)] ≤\u0012\n1 +∆\nn\u0013\nE[OPT(F, n)].\n17\n\nLemma A.3. LetN∼ B(m, ε)and let n:=E[N] =εm, then we have\nE[OPT(F, N) 1N≥n+n2/3]≤6\nn1/3E[OPT(F, n+n2/3)],\nE[OPT(F, N)]≤\u0012\n1 +3\nn1/3\u0013\nE[OPT(F, n+n2/3)].\nProof. Let∆, s > 0such that ∆≤s. For any k≥1letWk= [s+ (k−1)∆, s+k∆).(Wk)k≥1\nis a partition of [s,∞), thus we have\nE[OPT(F, N) 1N≥s] =∞X\nk=1E[OPT(F, N) 1N∈Wk]\n≤∞X\nk=1E[OPT(F, s+k∆) 1N∈Wk]\n=∞X\nk=1E[OPT(F, s+k∆)] Pr( N∈Wk)\n≤∞X\nk=1\u0012\n1 +k∆\ns\u0013\nE[OPT(F, s)] Pr( N∈Wk)\n= \nPr(N≥s) +∆\ns∞X\nk=1kPr(N∈Wk)!\nE[OPT(F, s)],\nwhere we used Lemma A.2 in the penultimate inequality. Furthermore, observing that\n∞X\nk=1kPr(N∈Wk) =∞X\nk=1k−1X\nℓ=0Pr(N∈Wk) =∞X\nℓ=0∞X\nk=ℓ+1Pr(N∈Wk) =∞X\nℓ=0Pr(N≥s+ℓ∆),\nwe obtain, given ∆≤s, that\nE[OPT(F, N) 1N≥s]≤ \nPr(N≥s) +∆\ns∞X\nk=0Pr(N≥s+k∆)!\nE[OPT(F, s)] (10)\n≤ \n2∞X\nk=0Pr(N≥s+k∆)!\n. (11)\nNis a Binomial random variable with expectation n. Therefore, Chernoff’s inequality gives for any\nδ≥0that\nPr(N≥(1 +δ)n)≤exp\u0012\n−δ2n\n2 +δ\u0013\n≤exp\u0012\n−min(δ, δ2)n\n3\u0013\n,\nwhere the second inequality can be derived by treating separately δ <1andδ≥1. In particular, for\nanyk≥1, taking δ=k∆\nnsuch that ∆≤nyields\nPr(N≥n+k∆)≤exp\u0012\n−min(k∆, k2∆2/n)\n3\u0013\n≤exp\u0012\n−kmin(∆ ,∆2/n)\n3\u0013\n= exp\u0012\n−k∆2\n3n\u0013\n.\nSubstituting this Inequality into (11) with s=n+ ∆, we obtain\nE[OPT(F, N) 1N≥n+∆]≤ \n2∞X\nk=1Pr(N≥n+k∆)!\nE[OPT(F, n+ ∆)]\n≤ \n2∞X\nk=1exp\u0012\n−k∆2\n3n\u0013!\nE[OPT(F, n+ ∆)]\n=2\nexp\u0000∆2\n3n\u0001\n−1E[OPT(F, n+ ∆)]\n≤6n\n∆2E[OPT(F, n+ ∆)] ,\n18\n\nand taking ∆ =n2/3proves the first inequality of the lemma.\nLet us move now to the second inequality. We have\nE[OPT(F, N) 1N<s]≤E[OPT(F, s) 1N<s] =E[OPT(F, s)] Pr( N < s ),\nand thus, using Inequality (10), again with s=n+ ∆ and∆ =n2/3, it follows that\nE[OPT(F, N)] =E[OPT(F, N) 1N<s] +E[OPT(F, N) 1N≥s]\n≤ \n1 +∆\ns∞X\nk=0Pr(N≥s+k∆)!\nE[OPT(F, s)]\n≤ \n1 +∞X\nk=1Pr(N≥n+k∆)!\nE[OPT(F, s)]\n≤\u0012\n1 +3\nn1/3\u0013\nE[OPT(F, n+ ∆)] .\nLemma A.4. Letδ1, . . . , δ miid∼ B(ε), and N=Pm\ni=1δi. Denoting by n=E[N] =εm, ifn≥4\nthen\nE[N2OPT(F, N)]≤\u0012\n1 +8\nn3\u0013\nn2E[OPT(F, n+n2/3)].\nProof. For all k∈[m], denote by Nk=Pm\ni=kδi. We have that\nN2OPT(F, N) = mX\ni=1δi!2\nOPT(F, N)\n=\nmX\ni=1δ2\ni+ 2X\ni<jδiδj\nOPT(F, N),\nand observing that δ2\ni=δifor all i, we obtain in expectation\nE[N2OPT(F, N)] =mE[δ1OPT(F, δ1+N2)] +m(m−1)E[δ1δ2OPT(δ1+δ2+N3)]\n≤mE[δ1OPT(F,1 +N2)] +m(m−1)E[δ1δ2OPT(2 +N3)]\n=mεE[OPT(F,1 +N2)] +m(m−1)ε2E[OPT(2 +N3)]\n=mεE[OPT(F,1 +N2)] +m2ε2E[OPT(2 +N3)]. (12)\nForj∈ {1,2}, the proof of Lemma A.3 can be easily adjusted to prove an upper bound\nonE[OPT(F, j+Nj+1), by first bounding E[OPT(F, j+Nj+1) 1Nj+1≥s]thenE[OPT(F, j+\nNj+1) 1Nj+1<s]. The concentration arguments remain the same, replacing mbym−j. The\nexpectation of Nj+1isε(m−j) =n−εj, hence we obtain\nE[OPT(F, j+Nj+1)]≤\u0012\n1 +3\n(n−εj)1/3\u0013\nE[OPT(F, j+ (n−εj) + (n−εj)2/3)]\n=\u0012\n1 +3\n(n−εj)1/3\u0013\nE[OPT(F, j+n+n2/3)]\n≤\u0012\n1 +3\n(n−2)1/3\u0013\nE[OPT(F,2 +n+n2/3)]\n≤\u0012\n1 +4\nn1/3\u0013\nE[OPT(F,2 +n+n2/3)],\nwhere we used respectively in the last inequalities that j≤2andn≥4. Furthermore, Lemma A.2\ngives that\nE[OPT(F, j+Nj+1)]≤\u0012\n1 +4\nn1/3\u0013\u0012\n1 +2\nn+n2/3\u0013\nE[OPT(F, n+n2/3)]\n≤\u0012\n1 +6\nn1/3\u0013\nE[OPT(F, n+n2/3)],\n19\n\nwhere the last inequality is true for n≥4. Finally, substituting into 12 yields\nE[N2OPT(F, N)]≤(m2ε2+mε)\u0012\n1 +6\nn1/3\u0013\nE[OPT(F, n+n2/3)]\n= (n2+n)\u0012\n1 +6\nn1/3\u0013\nE[OPT(F, n+n2/3)]\n=\u0012\n1 +1\nn\u0013\u0012\n1 +6\nn1/3\u0013\nn2E[OPT(F, n+n2/3)]\n≤\u0012\n1 +8\nn1/3\u0013\nn2E[OPT(F, n+n2/3)].\nLemma A.5. Letδ1, . . . , δ miid∼ B(ε),N=Pm\ni=1δi,n=E[N] =εmandL= min i̸=j{|i−j|:\nδi= 1, δj= 1}, then for any ℓ≥0we have\nE[OPT(F, N) 1L≤ℓ]≤7mℓε2E[OPT(F, n+n2/3)].\nProof. The random variables NandLare not independent, thus we need to adequately compute the\ndistribution of Lconditional to N. For any ℓ≥0andk≥2we have\nPr(L≤ℓ, N=s) = Pr\u0010\nL≤ℓ,mX\ni=1δk=s\u0011\n= Pr\u0010\n∪m\ni=1∪i−1\nj=max(1 ,i−ℓ)\u0000\nδi=δj= 1,mX\ni=1δk=s\u0001\u0011\n≤mℓPr\u0000\nδ1=δ2= 1,mX\ni=3δk=s−2\u0001\n=mℓ\u0012m−2\ns−2\u0013\nεs(1−ε)m−s+2,\ntherefore\nPr(L≤ℓ|N=s) =Pr(L≤ℓ, N=s)\nPr(N=s)\n≤mℓ\u0000m−2\ns−2\u0001\nεs(1−ε)m−s+2\n\u0000m\ns\u0001\nεs(1−ε)m−s\n≤mℓ\u0000m−2\ns−2\u0001\n\u0000m\ns\u0001\n=mℓs(s−1)\nm(m−1)\n≤ℓs2\nm.\nUsing this inequality and Lemma A.4, we deduce that\nE[OPT(F, N) 1L≤ℓ] =E[OPT(F, N) Pr(L≤ℓ|N,OPT(F, N))]\n=E[OPT(F, N) Pr(L≤ℓ|N)]\n≤ℓ\nmE[N2OPT(F, N)]\n≤\u0012\n1 +8\nn1/3\u0013ℓn2\nmE[OPT(F, n+n2/3)]\n=\u0012\n1 +8\nn1/3\u0013\nmℓε2E[OPT(F, n+n2/3)]\n= 7mℓε2E[OPT(F, n+n2/3)].\n20\n\nwhere we used that 1 +8\nn1/3≤7forn≥4.\nUsing the previous lemmas, we can now prove the theorem. Let m > n ≥1,∆ =n2/3,ε=n/m\nand let Qbe the probability distribution of a random variable that is equal to 0with probability 1−ε,\nand drawn from Fwith probability ε.\nLet us consider mi.i.d. variables Y1, . . . , Y m∼Q, and for each i∈[m]we denote by δithe\nindicator that Yiis drawn from F. Define N=Pm\ni=1δi∼ B(m, ε)the number of random variables\nYidrawn from the distribution F. In the following, we upper bound the competitive ratio of any\nalgorithm by analyzing its ratio on this particular instance. For this, we first provide a lower bound\nonE[OPT(Q, m)]using Lemma A.2, and obtain\nE[OPT(F, n−∆)]≥1\n1 +2∆\nnE[OPT(F, n+ ∆)] ≥\u0012\n1−2∆\nn\u0013\nE[OPT(F, n+ ∆)] ,\nthus we have\nE[OPT(Q, m)] =E[OPT(F, N)]\n≥E[OPT(F, N) 1N≥n−∆]\n≥E[OPT(F, n−∆) 1N≥n−∆]\n=E[OPT(F, n−∆)] Pr( N≥n−∆)\n≥\u0012\n1−2∆\nn\u0013\nPr(N≥n−∆)E[OPT(F, n+ ∆)]\n≥\u0012\n1−2∆\nn−Pr(N < n −∆)\u0013\nE[OPT(F, n+ ∆)]\n≥\u0010\n1−2n−1/3−exp(−n1/3/2)\u0011\nE[OPT(F, n+ ∆)]\n≥\u0010\n1−4n−1/3\u0011\nE[OPT(F, n+ ∆)] , (13)\nwhere, for the last three inequalities, we used respectively Bernoulli’s inequality, Chernoff bound,\nthene−y≤1/y.\nThen, we upper bound the reward of any algorithm given the instance (Q, m)as input. Let L=\nmini̸=j{|i−j|:δi= 1, δj= 1}the smallest gap between two successive variables Yidrawn from\nF, and let t1< . . . < t Nthe indices for which δi= 1. We have for any algorithm ALG and positive\ninteger ℓthat\nE[ALGD(Q, m)] =E[ALGD(Q, m) 1N≥n+∆ orL≤ℓ] +E[ALGD(Q, m) 1N<n +∆,L>ℓ].(14)\nUsing Lemma A.3 and Lemma A.5, the first term can be bounded as follows\nE[ALGD(Q, m) 1N≥n+∆ orL≤ℓ]≤E[OPT(F, N) 1N≥n+∆ orL≤ℓ]\n≤E[OPT(F, N) 1N≥n+∆] +E[OPT(F, N) 1L≤ℓ]\n≤\u00126\nn1/3+ 7mℓε2\u0013\nE[OPT(F, n+n2/3)].\nRecalling that ε=m/n and taking ℓ=√m, we obtain\nE[ALGD(Q, m) 1N≥n+∆ orL≤ℓ]≤\u00126\nn1/3+7n2\n√m\u0013\nE[OPT(F, n+n2/3)]. (15)\n21\n\nRegarding the second term in Equation (14), letτbe the stopping time of ALG andtρ= max {j≤\nτ:δj= 1}the last value sampled from Fand observed by ALG before it stops. We have\nE[ALGD(Q, m) 1N<n +∆,L>ℓ]≤E[ALGD(Q, m)|N < n + ∆, L > ℓ ]\n=E[max\ni∈[τ]Dτ−i(Yi)|N < n + ∆, L > ℓ ]\n=E[max\nj∈[ρ]Dτ−tj(Yi)|N < n + ∆, L > ℓ ]\n≤E[max\nj∈[ρ]Dtρ−tj(Yi)|N < n + ∆, L > ℓ ]\n=E[max\b\nYtρ,max\nj<ρDtρ−tj(Ytj)\t\n|N < n + ∆, L > ℓ ]\n≤E[max\b\nYtρ,max\nj<ρDℓ(Ytj)\t\n|N < n + ∆] .\nWe then prove that the last term is the reward of an algorithm Amin the Dℓ-prophet inequality. Let us\nAmbe the algorithm that takes as input an instance X1, . . . , X n+∆−1ofn+ ∆ IID random variables,\nthen simulates ALGD(Q, m)|N < n + ∆ as follows: let δ1, . . . , δ miid∼ B(n/m)setNA= 0and\nfor each i∈[m]\n• ifδi= 0:ALG observes the value Yi= 0,\n• ifδi= 1: increment N, thenAmobserves the next value Xk, andALG observes Yi=Xk,\n• ifNA=n+ ∆−1orALG stops, then Amalso stops.\nWhen ALG decides to stop, the current value observed by AmisXρ: the last value Ytρobserved by\nALG such that δtρ= 0. Observe that stopping when NA=n+ ∆ + 1 , is equivalent to letting ALG\nobserve zero values until the end, and stopping when ALG stops. Hence, the variables Y1, . . . , Y m\nhave the same distribution as mIID samples from Qconditional to N < n + ∆. Denoting ρthe\nstopping time of Amandϵℓ(F, n+ ∆) as defined in Lemma A.1, we deduce that\nE[ALGD(Q, m)|N < n + ∆, L > ℓ ]≤E[max\b\nYtρ,max\nj<ρDℓ(Ytj)\t\n|N < n + ∆]\n=E[max\b\nXρ,max\nj<ρDℓ(Xj)\t\n]\n=E[ADℓ\nm(F, n+ ∆)]\n≤E[AD∞\nm(F, n+ ∆)] + ϵℓ(F, n+ ∆) . (16)\nSubstituting (15) and (16) in (14), with ℓ=√m, yields\nE[ALGD(Q, m)]≤\u00126\nn1/3+7n2\n√m\u0013\nE[OPT(F, n+ ∆)] + E[AD∞\nm(F, n+ ∆)] + ϵ√m(F, n+ ∆) ,\nand using Inequality 13, it follows that\nCRD(ALG)≤E[ALGD(Q, m)]\nE[OPT(Q, m)]\n≤6\nn1/3+7n2\n√m\n1−4\nn1/3+E[AD∞\nm(F, n+ ∆)] + ϵ√m(F, n+ ∆)\n(1−4\nn1/3)E[OPT(F, n+ ∆)]\n≤6\nn1/3+7n2\n√m\n1−4\nn1/3+1\n1−4\nn1/3 \nϵ√m(F, n+ ∆)\nE[OPT(F, n+ ∆)]+ sup\nA:algoE[AD∞(F, n+ ∆)]\nE[OPT(F, n+ ∆)]!\n,\n22\n\ntaking the limit m→ ∞ and using Lemma A.1 gives\nCRD(ALG)≤6\nn1/3\n1−4\nn1/3+1\n1−4\nn1/3 \nsup\nA:algoE[AD∞(F, n+ ∆)]\nE[OPT(F, n+ ∆)]!\n=6\nn1/3−4+\u0012\n1 +4\nn1/3−4\u0013 \nsup\nA:algoE[AD∞(F, n+ ∆)]\nE[OPT(F, n+ ∆)]!\n≤10\nn1/3−4+ sup\nA:algoE[AD∞(F, n+n2/3)]\nE[OPT(F, n+n2/3)].\nwhere the last inequality holds because E[AD∞(F, n+ ∆)] ≤E[OPT(F, n+ ∆)] for any algorithm\nA. From here, the statement of the theorem can be deduced by observing that, for k=n+n2/3, we\nhaven≥(n+n2/3)/2 =k/2, thus n1/3≥k1/3/2, and we obtain\nCRD(ALG)≤20\nk1/3−8+ sup\nA:algoE[AD∞(F, k)]\nE[OPT(F, k)]\n= sup\nA:algoE[AD∞(F, k)]\nE[OPT(F, k)]+O\u00121\nk1/3\u0013\n.\nB From D∞-prophet to the γD-prophet inequality\nB.1 Proof of Lemma 3.1\nProof. LetALG be any rational algorithm in the D∞-prophet inequality. If ALG stops at some step\nτ∈[n], then by definition we have that Xτ> D∞(max j<τXj), and thus ALGD∞(X1, . . . , X n) =\nALG0(X1, . . . , X n). Otherwise, if it stops at τ=n+ 1, then its reward is max i∈[n]D∞(Xi) =\nD∞(max i∈[n]Xi), because D∞-is non increasing.\nOn the other hand, let A∗be the optimal dynamic programming algorithm for the D∞-prophet inequal-\nity. At any step i, ifXi< D∞(max j<iXj), then stopping at igives a reward of D∞(max j<iXj),\nwhile by rejecting Xi, the final reward is guaranteed to be at least D∞(max j<iXj). Thus rejecting\nXican only increase the reward, it is therefore the optimal decision.\nB.2 Proof of Proposition 3.2\nProof. Let us place ourselves in any order model, or in the IID model. Assume that infx>0D∞(x)\nx=\n0, then there exist a sequence (sk)k≥1such that limk→∞D∞(sk)\nsk= 0.\nLetI= (F1, . . . , F n)an instance of non-negative random variables with finite expectation, and\nXi∼Fifor all i∈[n]. LetALG be a rational algorithm for the D∞-prophet inequality and let us\ndenote τits stopping time. Denoting X∗:= max i∈[n]Xiand using Lemma 3.1, we have for any\nconstant C >0that that\nE[ALGD∞(I)] =E[ALG0(I) 1τ≤n] +E[D∞(X∗) 1τ=n+1]\n≤E[ALG0(I)] +E[D∞(C)] +E[D∞(X∗) 1X∗>C]\n≤sup\nAE[A0(I)] +E[D∞(C)] +E[X∗1X∗>C]. (17)\nLetk≥1a positive integer, Ma positive constant, and consider the instance Ikof random\nvariables (Xk\n1, . . . , Xk\nn)withXk\ni∼sk\nMXifor all i∈[n]. We have that OPT(Ik) =sk\nMOPT(I)\nandsupAE[A0(Ik)] =sk\nMsupAE[A0(I)]. Therefore, using Inequality (17) withIkinstead of Iand\n23\n\nC=sk\nM, then dividing by OPT(Ik)gives that\nCRD∞(ALG)≤E[ALGD∞(Ik)]\nE[OPT(Ik)]≤sup\nAE[A0(I)]\nE[OPT(I)]+D∞(sk)\nsk\nME[OPT(I)]+E[sk\nMX∗1X∗>M]\nsk\nME[OPT(I)]\n= sup\nAE[A0(I)]\nE[OPT(I)]+\u0012M\nE[OPT(I)]\u0013D∞(sk)\nsk+E[X∗1X∗>M]\nE[OPT(I)],\nand taking the limit k→ ∞ , we obtain\nCRD∞(ALG)≤sup\nAE[A0(I)]\nE[OPT(I)]+E[X∗1X∗>M]\nE[OPT(I)],\nand since X∗has finite expectation, taking the limit M→ ∞ gives\nCRD∞(ALG)≤sup\nAE[A0(I)]\nE[OPT(I)].\nFinally, taking the infimum over all instances, we obtain that\nCRD∞(ALG)≤sup\nACR0(A).\nAs in the proof of Corollary 2.1.1, permuting infIandsupAis possible because there is an algorithm\n(dynamic programming) achieving the supremum for any instance. By Lemma 3.1, the inequality\nabove holds for in particular for the optimal dynamic programming algorithm, which has a maximal\ncompetitive ratio. Therefore, the inequality remains true for any other algorithm A, not necessarily\nrational.\nB.3 Proof of Proposition 3.3\nProof. Let us place ourselves in any order model or in the IID model. Since γ= inf x>0D∞(x)\nx,\nthere exists a sequence (sk)k≥1of positive numbers such that limk→∞D∞(sk)\nsk=γ.\nFor the random variables X1, . . . , X ntaking values in {0, a, b}a.s., in any order model, it is clear\nthat the optimal decision when observing a zero value is to reject it, and when observing the value\nbis to accept it. Let ALG be an algorithm satisfying this property and let τbe its stopping time.\nIfτ=n+ 1then necessarily max i∈[n]Xi̸=b, and the reward of ALG in that case is D∞(a)if\nmax i∈[n]Xiand0otherwise. In particular, ALG is rational in the D∞-prophet inequality and we\nhave by Lemma 3.1 that\nE[ALGD∞(I)] =E[ALG0(I)] +E[D∞(max\ni∈[n]Xi) 1τ=n+1]\n=E[ALG0(I)] +D∞(a) Pr(τ=n+ 1,max\ni∈[n]Xi=a). (18)\nConsider now the instance Ikof random variables (Xk\n1, . . . , Xk\nn)where Xk\ni=sk\naXifor all i∈[n].\nIksatisfies the same assumptions as Iwithak=skandbk=skb\na, and we have E[OPT(Ik)] =\nsk\naE[OPT(I)],E[ALG0(Ik)]≤sk\nasupAE[A0(I)]and(max i∈[n]Xk\ni=ak)⇐⇒ (max i∈[n]Xi=\na). It follows that\nE[ALGD∞(Ik)]\nE[OPT(Ik)]≤supAE[A0(I)]\nE[OPT(I)]+D∞(sk)\nsk\naE[OPT(I)]Pr(τ=n+ 1,max\ni∈[n]Xi=a)\n=supAE[A0(I)]\nE[OPT(I)]+\u0012D∞(sk)\nsk\u0013a\nE[OPT(I)]Pr(τ=n+ 1,max\ni∈[n]Xi=a).\nTaking the limit k→ ∞ gives\nCRD∞(ALG)≤supAE[A0(I)] +γaPr(τ=n+ 1,max i∈[n]Xi=a)\nE[OPT(I)]\n=supA(E[A0(I)] +E[γ(max i∈[n]Xi) 1τ=n+1])\nE[OPT(I)].\n24\n\nALG is also rational in the γ-prophet inequality. Therefore, using Lemma 3.1, we deduce that\nCRD∞(ALG)≤E[ALGγ(I)]\nE[OPT(I)]≤sup\nAE[Aγ(I)]\nE[OPT(I)].\nThis upper bound is true for the optimal dynamic programming algorithm, since it rejects all zeros\nand accepts b, therefore the upper bound also holds for any other algorithm.\nC The γD-prophet inequality\nC.1 Proof of Proposition 4.2\nProof. For the lower bound, it suffices to consider the following trivial algorithm: if α > γ then run\nAα, and if γ > α then observe all the items then select the one with maximum value.\nFor the upper bound, let I= (F1, . . . , F n)be an instance of the problem and Xi∼Fifor all i,\nand let βI:= supAE[A0(I)]\nE[OPT(I)]. LetAbe any algorithm, τits stopping time, and Yτ= max i<τXπ(i),\nwhere πis the observation order. With the previous notations, we can write that E[Aγ(I)] =\nE[max( Xπ(τ), γYτ)]. For any x, y≥0, the application γ7→max( x, γy)is convex on [0,1], hence\nit can be upper bounded by (1−γ)x+γmax( x, y). Therefore\nE[Aγ(I)]≤(1−γ)E[Xπ(τ)] +γE[max( Xπ(τ), Yτ)]\n≤(1−γ)E[A0(I)] +γE[OPT(I)]\n≤\u0000\n(1−γ)βI+γ\u0001\nE[OPT(I)].\nTherefore, CRγ(ALG)≤((1−γ)βI+γ). Taking the infimum over all the instances Igives the\nresult. Indeed, if we denote A∗the optimal dynamic programming algorithm for the standard prophet\ninequality, then\ninf\nIβI= inf\nIE[A0\n∗(I)]\nE[OPT(I)]=CR0(A∗)≤β .\nC.2 Proofs for the adversarial order model\nProof. We first prove the upper bound, and then analyze the single threshold algorithm proposed in\nthe theorem.\nUpper bound Letε∈(0,1−γ), and let a=1\n1−(1−ε)γ(such that 1 + (1 −ε)γa=a). Let X1, X2\nthe two random variables defined by X1=aalmost surely and\nX2=\u001a1\nεw.p.ε\n0w.p.1−ε.\nStopping at the first step gives a reward of a, while stopping at the second step gives\nE[max( γa, X 2)] =ε×1\nε+ (1−ε)×γa= 1 + (1 −ε)γa=a ,\nhence the expected output of any algorithm is exactly equal to a. On the other hand\nE[max( X1, X2)] = 1 + (1 −ε)a ,\nand we deduce that, for any algorithm ALG for the γ-prophet inequality, we have\nCR(ALG)≤E[ALG(X1, X2)]\nE[max( X1, X2)]=a\n1 + (1 −ε)a,\nand this is true for any ε∈(0,1−γ), thus taking ε→0gives\nCR(ALG)≤1\n1−γ\n1 +1\n1−γ=1\n2−γ.\n25\n\nLower bound Consider an algorithm with an acceptance threshold θ, i.e. that accepts the first value\nlarger than θ. Let I= (F1, . . . , F n)be any instance, such that Xi∼Fifor all i, and let us define\nX∗= max i∈[n]Xiandp= Pr( X∗≤θ). In the classical prophet inequality, if no value is larger\nthanθthen the reward of the algorithm is zero, and we have the classical lower bound\nE[ALG0(I)]≥(1−p)θ+pE[(X∗−θ)+],\nFor the γ-prophet, if no value is larger than θ(i.eX∗≤θ), then the algorithm gains γX∗instead of\n0. Therefore, it holds that\nE[ALGγ(I)] =E[ALG0(I)] +E[X∗1X∗≤θ]\n≥(1−p)θ+pE[(X∗−θ)+] +γE[X∗1X∗≤θ]\n= (1−p)θ+pE[(X∗−θ) 1X∗>θ] +γE[X∗1X∗≤θ]\n= (1−p)θ+p(E[X∗1X∗>θ]−(1−p)θ) +γE[X∗1X∗≤θ]\n= (1−p)2θ+pE[X∗1X∗>θ] +γE[X∗1X∗≤θ],\nand observing that\nθ=E[θ 1X∗≤θ]\np≥E[X∗1X∗≤θ]\np,\nwe deduce the lower bound\nE[ALGγ(I)]≥pE[X∗1X∗>θ] +\u0012\nγ+(1−p)2\np\u0013\nE[X∗1X∗≤θ]\n≥min\u001a\np, γ+(1−p)2\np\u001b\nE[X∗].\nThe right term is maximized for psatisfying p=γ+(1−p)2\np, that leads to\np=γ+(1−p)2\np⇐⇒ p2=γp+ 1−2p+p2\n⇐⇒ p=1\n2−γ.\nHence, by choosing a threshold θsatisfying Pr(X∗≤θ) =1\n2−γwe obtain a competitive ratio of at\nleast1\n2−γ.\nC.3 Proofs for the random order model\nWe prove here the upper and lower bounds stated in Theorem 4.4.\nC.3.1 Proof of Theorem 4.4\nProof. We first prove the upper bound, and then derive the analysis for single threshold algorithms.\nUpper bound Leta > 0, and let X1, . . . , X n+1be independent random variables such that\nXn+1=aa.s. and for 1≤i≤n\nXi∼\u001a\nnw.p.1\nn2\n0w.p.1−1\nn2.\nAny reasonable algorithm skips zero values and stops when observing the value n. The only strategic\ndecision to make is thus to stop or not when observing Xn+1=a. By analyzing the dynamic\nprogramming solution ALG ⋆we obtain that the optimal decision rule is to skip aif it is observed\nbefore a certain step j, and select it otherwise. The step jcorresponds to the time when the expectation\nof the future reward is less than a. Let πbe the random order in which the variables are observed.\nThen, if π−1(n+ 1) < j, i.e. if the value ais observed before time j,Xn=1is not selected. The\noutput of this algorithm is hence nif at least one random variable equals n, and γaotherwise. This\nleads to\nE[ALGγ\n⋆(X)|π−1(n+ 1)< j] =n\u0012\n1−\u0012\n1−1\nn2\u0013n\u0013\n+γa\u0012\n1−1\nn2\u0013n\n≤1 +γa ,\n26\n\nwhere we used the inequality\u0000\n1−1\nn2\u0001n≥1−1\nn. On the other hand, if π−1(n)≥j, then ais\nselected if the value nhas not been observed before it, hence for any i≥jwe have\nE[ALGγ\n⋆(X)|π−1(n+ 1) = i] =n \n1−\u0012\n1−1\nn2\u0013i−1!\n+a\u0012\n1−1\nn2\u0013i−1\n≤i−1\nn+a ,\nwe deduce that\nE[ALGγ\n⋆(X)]≤(1 +γa) Pr(π−1(n)≤j−1) +n+1X\ni=j\u0012i−1\nn+a\u0013\nPr(π−1(n) =i)\n=j−1\nn+ 1(1 +γa) +1\nn+ 1n+1X\ni=j\u0012i−1\nn+a\u0013\n= (1−(1−γ)a)j\nn−1\n2\u0012j\nn\u00132\n+1\n2+a+o(1)\n≤1 + 2 γa+ (1−γ)2a2+o(1),\nwhere the last inequality is obtained by maximizing over j/n. Finally, we directly obtain that\nE[max\niXi] =n\u0012\n1−\u0012\n1−1\nn2\u0013n\u0013\n+a\u0012\n1−1\nn2\u0013n\n= 1 + a+o(1),\nso for any algorithm ALG we obtain that\nCRγ(ALG)≤CRγ(ALG ⋆)≤1 + 2 γa+ (1−γ)2a2\n1 +a.\nThe function above is minimized for a=q\n3−γ\n1−γ−1, which translates to\nCRγ(ALG)≤(1−γ)3/2(p\n3−γ−p\n1−γ) +γ .\nLower bound We still denote by I= (F1, . . . , F n)the input instance and Xi∼Fifor all i∈[n].\nLetALG be the algorithm with single threshold θ, then it is direct that\nALGγ(I) =ALG0(I) +γX∗1X∗<θ. (19)\nWe start by giving a lower bound on E[ALG0]. We use from Correa et al. [2021c] (Theorem 2.1) that\nfor any x < θ it holds that\nPr(ALG0(I)≥x) = Pr( ALG0(I)≥θ) = Pr( X∗≥θ) = 1−p ,\nand for x≥θit holds that\nPr(ALG0(I)≥x)≥1−p\n−logpPr(X∗≥x),\nfrom which we deduce that\nE[ALG0(I)] =Z∞\n0Pr(ALG0(I)≥x)dx\n≥(1−p)θ+1−p\n−logpZ∞\nθPr(X∗≥x)dx . (20)\nOn the other hand, we obtain that\nE[X∗1X∗<θ] =Z∞\n0Pr(X∗1X∗<θ≥x)dx=Z∞\n0Pr(x≤X∗< θ)dx\n=Zθ\n0(Pr(X∗> x)−Pr(X∗≥θ))dx\n=Zθ\n0Pr(X∗> x)dx−(1−p)θ . (21)\n27\n\nUsing (19), (20) and (21) we deduce that\nE[ALGγ(I)]≥(1−p)θ+1−p\n−logpZ∞\nθPr(X∗≥x)dx+γZθ\n0Pr(X∗≥x)dx−γ(1−p)θ\n= (1−γ)(1−p)θ+γZθ\n0Pr(X∗≥x)dx+1−p\n−logpZ∞\nθPr(X∗≥x)dx\n≥((1−γ)(1−p) +γ)Zθ\n0Pr(X∗≥x)dx+1−p\n−logpZ∞\nθPr(X∗≥x)dx\n≥min\u001a\n(1−γ)(1−p) +γ ,1−p\n−logp\u001b Zθ\n0Pr(X∗≥x)dx+Z∞\nθPr(X∗≥x)dx!\n= min\u001a\n1−(1−γ)p ,1−p\n−logp\u001b\nE[X∗].\nFinally, choosing p=pγgives the result.\nC.3.2 Proof of Corollary 4.4.1\nProof. Forp=1/e\n1−(1−1/e)γ, we have immediately that\n1−(1−γ)p= 1−(1−γ)/e\n1−(1−1/e)γ,\nandp∈[1/e,1]for any γ∈[0,1]. Since the function x7→(1−x)/log(1/x)is concave, we can\nlower bound it on [1/e,1]byx7→1−1/e+x−1/e\ne−1, which is the line intersecting it in 1/eand1.\nTherefore we have\n1−p\n−logp≥1−1/e+p−1/e\ne−1= 1−(1−γ)/e\n1−(1−1/e)γ.\nFinally, using the previous theorem, this choice of pguarantees a competitive ratio of at least\nmin\u001a\n1−(1−γ)p ,1−p\n−logp\u001b\n= 1−(1−γ)/e\n1−(1−1/e)γ.\nC.4 Proofs for the IID model\nProof of Lemma 4.5\nProof of Lemma 4.5. LetFbe the cumulative distribution function of X1,a > 0andALG the\nalgorithm with single threshold θsuch that 1−F(θ) =a\nn. We denote X∗= max i∈[n]Xi. As in the\nprevious proofs, we will begin by lower bounding ALG0(F, n). For any i∈[n],ALG stops at step i\nif and only if Xi> θand all the previous items were rejected, i.e. Xj≤θfor all j < i . Thus we can\nwrite\nE[ALG0(F, n)] =EhnX\ni=1(Xi 1Xi>θ) 1∀j<i:Xj≤θi\n=nX\ni=1F(θ)i−1E[Xi 1Xi>θ]\n=1−F(θ)n\n1−F(θ)E[X1 1X1>θ]\n=1−F(θ)n\na×nE[X1 1X1>θ], (22)\n28\n\nwhere the second equality is true by the independence of the random variables (Xi)i. On the other\nhand, we can upper bound E[X∗1X∗>θ]as follows\nE[X∗1X∗>θ]≤Pr(X∗> θ)θ+E[(X∗−θ)+]\n≤Pr(X∗> θ)θ+EhnX\ni=1(Xi−θ)+i\n= Pr( X∗> θ)θ+n\u0000\nE[X1 1X1>θ]−Pr(X1> θ)θ\u0001\n= (1−F(θ)n−a)θ+nE[X1 1X1>θ].\nUsing the definition of θ, the independence of (Xi)ithen Bernoulli’s inequality we have that\nPr(X∗> θ) = 1−F(θ)n= 1−\u0010\n1−a\nn\u0011n\n≤1−(1−n×a\nn) =a ,\nand observing that θ=E[θ 1X∗≤θ]\nF(θ)n≥E[X∗1X∗≤θ]\nF(θ)n, we deduce that\nE[X∗1X∗>θ]≤ −\u0012\n1−1−a\nF(θ)n\u0013\nE[X∗1X∗≤θ] +nE[X1 1X1>θ].\nby substituting into (22), we obtain\nE[ALG0(F, n)]≥1−F(θ)n\na\u0012\nE[X∗1X∗>θ] +\u0012\n1−1−a\nF(θ)n\u0013\nE[X∗1X∗≤θ]\u0013\n.\nFinally, the reward in the γ-prophet inequality is\nE[ALGγ(F, n)] =E[ALG0(F, n)] +γE[X∗1X∗<θ]\n≥1−F(θ)n\naE[X∗1X∗>θ] +\u00121−F(θ)n\na\u0012\n1−1−a\nF(θ)n\u0013\n+γ\u0013\nE[X∗1X∗<θ]\n≥min\u001a1−F(θ)n\na,1−F(θ)n\na\u0012\n1−1−a\nF(θ)n\u0013\n+γ\u001b\nE[X∗].\nThe equation1−F(θ)n\na=1−F(θ)n\na\u0010\n1−1−a\nF(θ)n\u0011\n+γ, is equivalent to\n\u00121\n(1−a/n)n−1\u0013\u00121\na−1\u0013\n=γ , (23)\nand for any n≥2the function a7→\u00001\n(1−a/n)n−1\u0001\u00001\na−1\u0001\nis decreasing on (0,1]and goes from 1\nto0, thus Equation (23) admits a unique solution an,γ, and taking a=an,γguarantees a reward of\n1−F(θ)n\nan,γE[X∗] =1−(1−an,γ\nn)n\nan,γE[X∗].\nProof of Theorem 4.6\nProof. We first prove the upper bound, and then we give the single-threshold algorithm satisfying the\nlower bound.\nUpper bound We consider an instance similar to the one used in the proof of Theorem 4.4. Let\na, x > 0, and let X1, . . . , X nbe IID random variables with the following the distribution Fdefined\nby\nX1∼\n\nnw.p.1\nn2\naw.p.x\nn\n0w.p.1−x\nn−1\nn2.\nA reasonable algorithm would always reject the value 0and accept the value n. However, if the\nalgorithm faces an item with value a, it must decide to either accept it, or reject it with a guarantee\nof recovering γaat the end. By analyzing the dynamic programming algorithm ALG ⋆, we find that\nthe optimal decision is to reject aif observed before a certain step j, and accept it otherwise. Let us\ndenote τthe stopping time of ALG ⋆. By convention, we write τ=n+ 1to say that no value was\nselected by the algorithm, in which case the reward is γmax i∈[n]Xi.\n29\n\nIfτ≤j−1then necessarily Xτ=n, because ALG ⋆rejects the value aif it is met before step j,\nand if τ=n+ 1thenmax i∈[n]Xi∈ {0, a}, because otherwise the algorithm would have selected\nthe value nand stopped earlier. It follows that the expected output of ALG ⋆on this instance is\nE[ALGγ\n⋆(F, n)] =nPr(τ < j ) +nX\ni=jE[Xi|τ=i] Pr(τ=i)\n+γaPr(τ=n+ 1,max\ni∈[n]Xi=a). (24)\nLet us now compute the terms above one by one.\nPr(τ < j ) = Pr( ∃i∈[j−1] :Xi=n) = 1−\u0012\n1−1\nn2\u0013j−1\n≤j\nn2,\nwhere we used Bernoulli’s inequality (1−1/n2)j−1≥1−j−1\nn2>1−j\nn2. For i∈ {j, . . . , n },\nALG ⋆stops at iifXi∈ {a, n}and if it has not stopped before, i.e. Xk∈ {0, a}for all k < j and\nXk= 0for all k∈ {j, . . . , i −1}, hence\nPr(τ=i) = Pr( ∀k < j :Xk̸=nand∀j≤k≤i−1 :Xk= 0andXi̸= 0)\n=\u0012\n1−1\nn2\u0013j−1\u0012\n1−x\nn−1\nn2\u0013i−j\nPr(Xi̸= 0)\n≤\u0010\n1−x\nn\u0011i−j\nPr(Xi̸= 0) ,\nthe second equality is true by independence, and the last inequality holds because 1−1\nn2≤1and\n1−x\nn−1\nn2≤1−x\nn. By independence of the variables (Xk)k, we also have that\nE[Xi|τ=i] =E[Xi|Xi̸= 0] =E[Xi]\nPr(Xi̸= 0)=1 +ax\nnPr(Xi̸= 0).\nFinally, the event (τ=n+ 1,max i∈[n]Xi=a)is equivalent (max i∈[j−1]Xi=a,∀k≥j:Xk=\n0). In fact, the algorithm does not stop before n+ 1if and only if Xk̸=nfor all k < j andXk= 0\nfor all j≤k≤n, and under these conditions, it holds that max i∈[n]Xi= max i∈[j−1]Xi. Therefore\nPr(τ=n+ 1,max\ni∈[n]Xi=a) = Pr( max\ni∈[j−1]Xi=a,∀k≥j:Xk= 0)\n≤Pr( max\ni∈[j−1]Xi̸= 0) Pr( ∀k≥j:Xk= 0)\n= \n1−\u0012\n1−x\nn−1\nn2\u0013j−1!\u0012\n1−x\nn−1\nn2\u0013n−j\n=\u0000\n1−e−xj\nn+o(1)\u0001\u0000\ne−x+xj\nn+o(1)\u0001\n=\u0000\nexj\nn−1\u0001\ne−x+o(1).\nAll in all, we obtain by substituting into 24 that\nE[ALGγ\n⋆(F, n)]≤j\nn+\u00121 +ax\nn\u0013nX\ni=j\u0010\n1−x\nn\u0011i−j\n+γae−x\u0000\nexj\nn−1\u0001\n+o(1)\n=j\nn+\u00121 +ax\nn\u00131−(1−x/n)n−j+1\nx/n+γae−x\u0000\nexj\nn−1\u0001\n+o(1)\n=j\nn+\u00121\nx+a\u0013\u0000\n1−e−x+xj\nn+o(1)\u0001\n+γae−x\u0000\nexj\nn−1\u0001\n+o(1)\n=j\nn−h\u00001\nx+ (1−γ)a\u0001i\nexj\nn+1\nx+a−γae−x+o(1)\n≤max\ns>0n\ns−h\u00001\nx+ (1−γ)a\u0001i\nexso\n+1\nx+ (1−γe−x)a+o(1)\n=−1\nx\u0000\nlog(1 + (1 −γ)ax) + 1−x\u0001\n+1\nx+ (1−γe−x)a+o(1)\n=−1\nxlog(1 + (1 −γ)ax) + 1 + (1 −γe−x)a+o(1).\n30\n\nOn the other hand, we have that\nPr(max\ni∈[n]Xi=n) = 1−\u0012\n1−1\nn2\u0013n\n=1\nn+o(1/n),\nPr(max\ni∈[n]Xi= 0) =\u0012\n1−x\nn−1\nn2\u0013\n=e−x+o(1),\nPr(max\ni∈[n]Xi=a) = 1−Pr(max\ni∈[n]Xi= 0)−Pr(max\ni∈[n]Xi=n) = 1−e−x+o(1),\ntherefore, the expected maximum value is\nE[max\ni∈[n]Xi] =nPr(max\ni∈[n]Xi=n) +aPr(max\ni∈[n]Xi=a)\n= 1 +\u0000\n1−e−x\u0001\na+o(1).\nWe deduce that\nE[ALGγ\n⋆(F, n)]\nE[max i∈[n]Xi]≤−1\nxlog(1 + (1 −γ)ax) + 1 + (1 −γe−x)a\n1 +\u0000\n1−e−x\u0001\na+o(1)\n= 1−1\nxlog(1 + (1 −γ)ax)−(1−γ)ae−x\n1 +\u0000\n1−e−x\u0001\na+o(1).\nConsequently, for any a, x > 0and for any algorithm ALG we have\nCR(ALG)≤CR(ALG ⋆)\n≤lim\nn→∞E[ALGγ\n⋆(F, n)]\nE[max i∈[n]Xi]\n≤1−log(1 + (1 −γ)ax)−(1−γ)axe−x\nx+\u0000\n1−e−x\u0001\nax.\nIn particular, for x= 2anda=1−γ/2\n1−γwe find that\nCR(ALG)≤1−log(3−γ)−(2−γ)e−2\n2 +2−γ\n1−γ(1−e−2)\n= 1−(1−γ)e2log(3−γ)−(2−γ)\n2(2e2−1)−γ(3e2−1)\n=U(γ).\nThis proves the upper bound stated in the theorem, and we can verify that it is increasing, and satisfies\nU(0) =4−log 3\n4−2/e2U(1) = 1\nLower bound on the competitive ratio We will prove that the algorithm presented in Lemma 4.5\nhas a competitive ratio of at least (1−(1−γ)pγ), where pγ, first introduced in Theorem 4.4, is the\nunique solution of the equation (1−(1−γ)p) =1−p\n−logp, which is equivalent to\u00001\np−1\u0001\u00001\nlog(1/p)−1\u0001\n=\nγ.\nLetaγ=−log(pγ). It follows from the definition of pγthataγis the unique solution of the equation\n(ea−1)(1\na−1) = γ. For any n≥2andx≥0we have that (1−x/n)n≤e−x, hence, by definition\nofan,γandaγ\n\u00121\ne−an,γ−1\u0013\u00121\nan,γ−1\u0013\n≤\u00121\n(1−an,γ\nn)n−1\u0013\u00121\nan,γ−1\u0013\n=γ\n=\u00121\ne−aγ−1\u0013\u00121\naγ−1\u0013\n. (25)\n31\n\nMoreover, the function x7→(ex−1)(1/x−1)is decreasing on (0,1). In fact its derivative at any\npoint x∈(0,1)is\nd\ndx\u0014\u00121\ne−x−1\u0013\u00121\nx−1\u0013\u0015\n=\u00121\nx−1\u0013\nex−ex−1\nx2\n=1\nx2\u0000\n1−x2−(1−x)ex\u0001\n=1−x\nx2(1 +x−ex)<0.\nIt follows from (25) thataγ≤an,γ. Finally, given that x7→1−e−x\nxis non-increasing on (0,1], we\ndeduce that\n1−(1−an,γ\nn)n\nan,γ≥1−e−an,γ\nan,γ≥1−e−aγ\naγ.\nWe deduce that the competitive ratio of the algorithm described in Theorem 4.6 is at least1−e−aγ\naγ=\n1−pγ\nlog(1/pγ)= 1−(1−γ)pγ.\nD Random decay functions\nWhile we only studied deterministic decay functions in the paper, it is also possible to have scenarios\nwith random decay functions. Consider for example that rejected items remain available after jsteps\nwith a probability pj, this is modeled by Dj(x) =ξjxwithξja Bernoulli random variable with\nparameter pj. We explain in this section how the definitions and our results extend to this case.\nDefinition D.1 (Random process) .LetXis a non-empty set. A random process o Xis a collection\nof random variables {Zx}x∈X. Two random processes Z={Zx}x∈XandZ′={Z′\nx}x∈X′are\nindependent if any finite sub-process of Zis independent of any sub-process of Z′. For simplicity, let\nus say that the random processes {Z1\nx}x∈X1, . . . ,{Zm\nx}x∈Xmare mutually independent if, for any\nx1∈ X1, . . . , x m∈ Xm, the random variables Z1\nx1, . . . , Zm\nxmare mutually independent.\nDefinition D.2 (Random decay functions) .LetD= (D1, D2, . . .)be a sequence of mutually\nindependent random processes. We say that Dis a sequence of random decay functions if\n1.Pr(Dj(x)/∈[0, x]) = 0 for any x≥0andj≥1,\n2.j∈N≥17→Pr(Dj(x)≥a)is non-increasing for any x, a≥0,\n3.x≥07→Pr(Dj(x)≥a)is non-decreasing for any j∈N≥1anda≥0.\nThe second condition asserts that the random variable Dj−1(x)has first-order stochastic dominance\noverDj(x). Along with the first condition, reflect that the distributions of the rejected values become\nprogressively smaller. The last condition indicates that for any integer j≥1and non-negative real\nnumbers x < y ,Dj(y)has a first-order stochastic dominance over Dj(x), which means that, as the\nvalue of xincreases, so does the potential recovered value after jsteps.\nThe decision-maker In theD-prophet inequality with deterministic decay functions, we assumed\nthat the decision-maker has full knowledge of the functions D1, D2. . .. In the randomized setting,\nwe assume instead that the decision-maker knows the distributions of the decay functions, i.e. knows\nthe distribution of the random variables Dj(x)for all x≥0andj≥1. However, they do not\nobserve their values until they decide to stop. The online selection process is therefore as follows: the\nalgorithm knows beforehand the distributions of the decay functions, then at each step, it observes\na new item with value Xi, and decides to stop or continue. Once they decide to stop at some time\nτ, they observe the values D1(Xτ−1), . . . , D τ(X1)and then they choose the maximal one. As a\nconsequence, the stopping time τis independent of the randomness induced by the decay functions.\nAs in the deterministic case, the expected reward of any algorithm ALG can be written as\nE[ALGD(X1, . . . , X n)] =E[ max\n0≤i≤τ−1{Di(Xτ−i)}].\n32\n\nThe limit decay A key result in our paper is the reduction of the problem to the case where all the\ndecay functions are identical, and we prove this reduction by considering the pointwise limit of the\ndecay functions. In the case of random decay functions, instead of the pointwise convergence, it holds\nfor all x≥0that the random variables (Dj(x))jconverge in distribution to some random variable\nD∞(x). In fact, for any x≥0anda≥0, the sequence (Pr(Dj(x)≥a))j≥1is non-increasing\nand non-negative, thus it converges to some constant G(x, a). Given that x7→Pr(Dj(x)≥a)is\nnon-decreasing for any j, we obtain by taking the limit j→ ∞ thatx7→G(x, a)is non-increasing,\nand with similar argument we obtain, for any x≥0, that G(x, a) = 1 for all a≤0andG(x, a) = 0\nfor all a > x . Therefore, a7→1−G(x, a)defined the cumulative distribution of a random variable\nD∞such that\n•x≥07→Pr(D∞(x)≥a)is non-decreasing for all a≥0,\n•Pr(D∞(x)/∈[0, x]) = 0 for al x≥0.\nTherefore, for all x≥0,D∞(x)is the limit in distribution of (Dj(x))j, hence a sequence\nD′= (D′\n1, D′\n2, . . .)of mutually independent random processes such that D′\nj(x)∼D∞(x)for\nanyj≥1andx≥0defines a sequence of decay functions. We say in this case that all\nthe decay functions are identically distributed as D∞. Moreover, it holds for all x≥0that\nE[D∞(x)] = lim j→∞E[Dj(x)] = inf j≥1E[Dj(x)]\nFrom there, all the proofs of Section 2 can be easily generalized to the case of random decay\nfunctions, and it follows that we can restrict ourselves to studying identically distributed decay\nfunctions. Moreover, Proposition 3.2 can be generalized to the case of random decay functions, and\nthe necessary condition for surpassing 1/2becomes infx>0E[D∞(x)]\nx>0. Similarly, using that the\nstopping time τof the algorithm is independent of randomness induced by D∞, Proposition 3.3\nremains true with γ= inf x>0E[D∞(x)]\nx.\nLower bounds For establishing lower bounds, observe that, for any random decay functions D, if\nwe denote Hj(x) =E[Dj(x)]for all x, thenH= (H1, H2, . . .)defines a sequence of deterministic\ndecay functions. Furthermore, for any instance X1, . . . , X nand any algorithm ALG , it holds that\nE[ALGD(X1, . . . , X n)] =E[ max\n0≤i≤τ−1{Di(Xτ−i)}]\n=Eh\nE[ max\n0≤i≤τ−1{Di(Xτ−i)} |τ, X 1, . . . , X n]i\n≥Eh\nmax\n0≤i≤τ−1\b\nE[Di(Xτ−i)|τ, X 1, . . . , X n]\ti\n=Eh\nmax\n0≤i≤τ−1{Hi(Xτ−i)}i\n=E[ALGH(X1, . . . , X n)].\nIt follows that lower bounds established for deterministic decay functions can be extended to random\ndecay functions by considering their expectations.\nImplications With the previous observations, both the lower and upper bounds, depending on γD\nthat we proved in the deterministic D-prophet inequality can be generalized to the random D-prophet\ninequality, by taking\nγD= inf\nx>0inf\nj≥1E[Dj(x)]\nx.\n33",
  "textLength": 88754
}