{
  "paperId": "394cfe62f468d2b1a41cc76cf828fc902ad6ef77",
  "title": "Indices Matter: Learning to Index for Deep Image Matting",
  "pdfPath": "394cfe62f468d2b1a41cc76cf828fc902ad6ef77.pdf",
  "text": "Indices Matter: Learning to Index for Deep Image Matting\nHao LuyYutong DaiyChunhua Sheny\u0003Songcen Xuz\nyThe University of Adelaide, AustraliazNoah’s Ark Lab, Huawei Technologies\ne-mail:fhao:lu;yutong :dai;chunhua :sheng@adelaide :edu:au\nAbstract\nWe show that existing upsampling operators can be uni-\nﬁed with the notion of the index function. This notion is\ninspired by an observation in the decoding process of deep\nimage matting where indices-guided unpooling can recover\nboundary details much better than other upsampling oper-\nators such as bilinear interpolation. By looking at the in-\ndices as a function of the feature map, we introduce the con-\ncept of learning to index, and present a novel index-guided\nencoder-decoder framework where indices are self-learned\nadaptively from data and are used to guide the pooling and\nupsampling operators, without the need of supervision. At\nthe core of this framework is a ﬂexible network module,\ntermed IndexNet, which dynamically predicts indices given\nan input. Due to its ﬂexibility, IndexNet can be used as a\nplug-in applying to any off-the-shelf convolutional networks\nthat have coupled downsampling and upsampling stages.\nWe demonstrate the effectiveness of IndexNet on the task\nof natural image matting where the quality of learned in-\ndices can be visually observed from predicted alpha mat-\ntes. Results on the Composition-1k matting dataset show\nthat our model built on MobileNetv2 exhibits at least 16:1%\nimprovement over the seminal VGG-16 based deep mat-\nting baseline, with less training data and lower model ca-\npacity. Code and models has been made available at:\nhttps://tinyurl.com/IndexNetV1 .\n1. Introduction\nUpsampling is an essential stage for most dense pre-\ndiction tasks using deep convolutional neural networks\n(CNNs). The frequently used upsampling operators include\ntransposed convolution [50, 32], unpooling [2], periodic\nshufﬂing [41] (also known as depth-to-space), and naive in-\nterpolation [30, 4] followed by convolution. These oper-\nators, however, are not general-purpose designs and often\nhave different behaviors in different tasks.\nThe widely-adopted operator in semantic segmentation\n\u0003Corresponding author.\nFigure 1: Alpha mattes of different models. From left to right,\nDeeplabv3+ [4], ReﬁneNet [30], Deep Matting [49] and Ours.\nBilinear upsampling fails to recover subtle details, but unpool-\ning and our learned upsampling operator can produce much clear\nmattes with good local contrast.\nor depth estimation is bilinear interpolation, rather than\nunpooling. A reason is that the feature map generated\nby unpooling is too sparse, while bilinear interpolation is\nlikely to generate the feature map that depicts semantically-\nconsistent regions. This is particularly true for semantic\nsegmentation and depth estimation where pixels in a region\noften share the same class label or have similar depth. How-\never, bilinear interpolation performs much worse than un-\npooling in boundary-sensitive tasks such as image matting.\nA fact is that the leading deep image matting model [49]\nlargely borrows the design from the SegNet [2], where un-\npooling is introduced. When adapting other state-of-the-\nart segmentation models, such as DeepLabv3+ [4] and Re-\nﬁneNet [30], to this task, unfortunately, we observe both\nDeepLabv3+ and ReﬁneNet fail to recover boundary de-\ntails (Fig. 1), compared to SegNet. This makes us to ponder\nover what is missing in these encoder-decoder models. Af-\nter making a thorough comparison between different archi-\ntectures and conducting ablative studies (Section 5.2), the\nanswer is ﬁnally made clear— indices matter .\nCompared to the bilinearly upsampled feature map, un-\npooling uses max-pooling indices to guide upsampling.\nSince boundaries in the shallow layers usually have the\nmaximum responses, indices extracted from these re-\nsponses record the boundary locations. The feature map\nprojected by the indices thus shows improved boundary de-arXiv:1908.00672v1  [cs.CV]  2 Aug 2019\n\nlineation. Above analyses reveal a fact that, different up-\nsampling operators have different characteristics, and we\nexpect a speciﬁc behavior of the upsampling operator when\ndealing with speciﬁc image content in a certain visual task.\nIt would be interesting to pose the question: Can we de-\nsign a generic operator to upsample feature maps that bet-\nter predict boundaries and regions simultaneously? A key\nobservation of this work is that max unpooling, bilinear in-\nterpolation or other upsampling operators are some forms\nof index functions . For example, the nearest neighbor in-\nterpolation of a point is equivalent to allocating indices of\none to its neighbor and then map the value of the point. In\nthis sense, indices are models [24], therefore indices can be\nmodeled and learned. In this work, we model indices as a\nfunction of the local feature map and learn an index function\nto perform upsampling within deep CNNs. In particular, we\npresent a novel index-guided encoder-decoder framework,\nwhich naturally generalizes SegNet. Instead of using max-\npooling and unpooling, we introduce indexed pooling and\nindexed upsampling operators where downsampling and\nupsampling are guided by learned indices. The indices are\ngenerated dynamically conditioned on the feature map and\nare learned using a fully convolutional network, termed In-\ndexNet, without supervision. IndexNet is a highly ﬂexible\nmodule, which can be used as a plug-in applying to any off-\nthe-shelf convolutional networks that have coupled down-\nsampling and upsampling stages. Compared to the ﬁxed\nmax function, learned index functions show potentials for\nsimultaneous boundary and region delineation.\nWe demonstrate the effectiveness of IndexNet on natural\nimage matting as well as other visual tasks. In image mat-\nting, the quality of learned indices can be visually observed\nfrom predicted alpha mattes. By visualizing learned indices,\nwe show that the indices automatically learn to capture the\nboundaries and textural patterns. We further investigate al-\nternative ways to design IndexNet, and show through ex-\ntensive experiments that IndexNet can effectively improve\ndeep image matting both qualitatively and quantitatively. In\nparticular, we observe that our best MobileNetv2-based [39]\nmodel exhibits at least 16:1%improvement against the pre-\nvious best deep model, i.e., the VGG-16-based model in\n[49], on the Composition-1k matting dataset. We achieve\nthis with using less training data, and a much more compact\nmodel, therefore signiﬁcantly faster inference speed.\n2. Related Work\nWe review existing widely-used upsampling operators\nand the main application of IndexNet—deep image matting.\nUpsampling in Deep Networks Upsampling is an es-\nsential stage for almost all dense prediction tasks. It has\nbeen intensively studied about what is the principal way\nto recover the resolution of the downsampled feature map(decoding). The deconvolution operator, also known as\ntransposed convolution, was initially used in [50] to vi-\nsualize convolutional activations and latter introduced to\nsemantic segmentation [32]. To avoid checkerboard arti-\nfacts, a follow-up suggestion is the “resize+convolution”\nparadigm, which has currently become the standard con-\nﬁguration in state-of-the-art semantic segmentation mod-\nels [4, 30]. Aside from these, perforate [35] and unpool-\ning[2] are also two operators that generate sparse indices to\nguide upsampling. The indices are able to capture and keep\nboundary information, but the problem is that two opera-\ntors induce sparsity after upsampling. Convolutional layers\nwith large ﬁlter sizes must follow for densiﬁcation. In ad-\ndition, periodic shufﬂing (PS) was introduced in [41] as a\nfast and memory-efﬁcient upsampling operator for image\nsuper-resolution. PSrecovers resolution by rearranging the\nfeature map of size H\u0002W\u0002Cr2torH\u0002rW\u0002C.\nOur work is primarily inspired by the unpooling oper-\nator [2]. We remark that, it is important to keep the spa-\ntial information before loss of such information occurred in\nfeature map downsampling, and more importantly, to use\nstored information during upsampling. Unpooling shows a\nsimple and effective case of doing this, but we argue there\nis much room to improve. In this paper, we illustrate that\nthe unpooling operator is a special form of index function,\nand we can learn an index function beyond unpooling.\nDeep Image Matting In the past decades, image matting\nmethods have been extensively studied from a low-level\nview [1, 6, 7, 9, 14, 15, 28, 29, 45]; and particularly, they\nhave been designed to solve the matting equation. Despite\nbeing theoretically elegant, these methods heavily rely on\nthe color cues, rendering failures of matting in general nat-\nural scenes where colors cannot be used as reliable cues.\nWith the tremendous success of deep CNNs in high-\nlevel vision tasks [13, 26, 32], deep matting methods are\nemerging. Some initial attempts appeared in [8] and [40],\nwhere classic matting approaches, such as closed-form mat-\nting [29] and KNN matting [6], are still used as the back-\nends in deep networks. Although the networks are trained\nend-to-end and can extract powerful features, the ﬁnal per-\nformance is limited by the conventional backends. These\nattempts may be thought as semi-deep matting. Recently\nfully-deep image matting was proposed [49]. In [49] the au-\nthors presented the ﬁrst deep image matting approach based\non SegNet [2] and signiﬁcantly outperformed other com-\npetitors. Interestingly, this SegNet-based architecture be-\ncomes the standard conﬁguration in many recent deep mat-\nting methods [3, 5, 47].\nSegNet is effective in matting but also computation-\nexpensive and memory-inefﬁcient. For instance, the in-\nference can only be executed on CPU when testing high-\nresolution images, which is practically unattractive. We\n\nIndexed\nPoolingIndexed\nUpsampling\nEncoding stage with downsampling Decoding stage with upsamplingIndex \nBlock\nSigmoid\nSoftmaxIndexNetdecoder\nencoder\nAverage \nPooling4Indexed Pooling\n2x2, stride 2upsample\nx2\nnearest neighbor \ninterpolationIndexed Upsampling\nIndexNet\nEncoder feature maps Decoder feature maps Index maps Element‐wise multiplicationFigure 2: Index-guided encoder-decoder framework. The proposed IndexNet dynamically predicts indices for individual local regions,\nconditional on the input local feature map itself. The predicted indices are further utilized to guide the downsampling in the encoding\nstage and the upsampling in corresponding decoding stage.\nshow that, with our proposed IndexNet, even a lightweight\nbackbone such as MobileNetv2-based model can surpass\nthe VGG-16 based method in [49].\n3. An Indexing Perspective of Upsampling\nWith the argument that upsampling operators are index\nfunctions, here we offer an uniﬁed index perspective of up-\nsampling operators. The unpooling operator is straightfor-\nward. We can deﬁne its index function in a k\u0002klocal\nregion as an indicator function\nImax(x) = 1(x= max( X));x2X; (1)\nwhereX2Rk\u0002k. Similarly, if one extracts indices from\nthe average pooling operator, the index function takes the\nform\nIavg(x) = 1(x2X): (2)\nIf further using Iavg(x)during upsampling, it is equivalent\nto the nearest neighbor interpolation. Regarding the bilin-\near interpolation and deconvolution operators, their index\nfunctions have an identical form\nIbilinear=dconv (x) =W\n 1(x2X); (3)\nwhereWis the weight/ﬁlter of the same size as X, and\n\ndenotes the element-wise multiplication. The difference\nis that,Win deconvolution is learned, while Win bilin-\near interpolation stays ﬁxed. Indeed, bilinear upsampling\nhas been shown to be a special case of deconvolution [32].\nNotice that, in this case, the index function generates soft\nindices. The sense of index for the PSoperator [41] is evenmuch clear, because the rearrangement of the feature map\nper se is an indexing process. Considering PSa tensor Zof\nsize1\u00021\u0002r2to a matrix Zof sizer\u0002r, the index function\ncan be expressed by the one-hot encoding\nIl\nps(x) = 1(x=Zl);l= 1;:::;r2; (4)\nsuch that Zm;n=Z[Il\nps(x)], wherem= 1;:::;r ,n=\n1;:::;r , andl= (r\u00001)\u0003m+n.Zldenotes thel-th element\nofZ. A similar notation applies to Zm;n.\nSince upsampling operators can be uniﬁed by the notion\nof index function, in theory it is possible to learn an index\nfunction that adaptively captures local spatial patterns.\n4. Index-Guided Encoder-Decoder Framework\nOur framework is a natural generalization of SegNet, as\nschematically illustrated in Fig. 2. For ease of exposition,\nwe assume the downsampling and upsampling rates are 2,\nand the pooling operator has a kernel size of 2\u00022. At the\ncore of our framework is the IndexNet module that dynami-\ncally generates indices given the feature map. The proposed\nindexed pooling and indexed upsampling operators further\nreceive generated indices to guide the downsampling and\nupsampling, respectively. In practice, multiple such mod-\nules can be combined and used analogues to the max pool-\ning layers. We provide details as follows.\n4.1. Learning to Index, to Pool, and to Upsample\nIndexNet models the index as a function of the feature map\nX2RH\u0002W\u0002C. It generates two index maps for down-\nsampling and upsampling given the input X. An important\n\nconcept for the index is that an index can either be repre-\nsented in a natural order, e.g., 1, 2, 3, ..., or be represented\nin a logical form, i.e., 0, 1, 0, ..., which means an index map\ncan be used as a mask. In fact, this is how we use the index\nmap in downsampling and upsampling. The predicted index\nshares the same physical notation of the index in computer\nscience, except that we generate softindices for smooth op-\ntimization, i.e., for any index i,i2[0;1].\nIndexNet consists of a predeﬁned index block and two\nindex normalization layers. An index block can simply be a\nheuristically deﬁned function, e.g., a max function, or more\ngenerally, a neural network. In this work, the index block\nis designed to use a fully convolutional network. Accord-\ning to the shape of the output index map, we investigate\ntwo families of index networks: holistic index networks\n(HINs) anddepthwise (separable) index networks (DINs) .\nTheir conceptual differences are shown in Fig. 3. HINs\nlearn an index function I(X) :RH\u0002W\u0002C!RH\u0002W\u00021.\nIn this case, all channels of the feature map share a holis-\ntic index map. In contrast, DINs learn an index function\nI(X) :RH\u0002W\u0002C!RH\u0002W\u0002C, where the index map is of\nthe same size as the feature map. We will discuss concrete\ndesign of index networks in Sections 4.2 and 4.3.\nNote that the index map sent to the encoder and decoder\nare normalized differently. The decoder index map only\ngoes through a sigmoid function such that for any predicted\nindexi2(0;1). As for the encoder index map, indices of a\nlocal region Lare further normalized by a softmax function\nsuch thatP\ni2Li= 1. The reason behind the second nor-\nmalization is to guarantee the magnitude consistency of the\nfeature map after downsampling.\nIndexed Pooling (IP) executes downsampling using gen-\nerated indices. Given a local region E2Rk\u0002k,IPcalcu-\nlates a weighted sum of activations and corresponding in-\ndices overEasIP(E) =P\nx2EI(x)x, whereI(x)is the\nindex ofx. It is easy to infer that max pooling and aver-\nage pooling are both special cases of IP. In practice, this\noperator can be easily implemented with an element-wise\nmultiplication between the feature map and the index map,\nan average pooling layer, and a multiplication of a constant,\nas instantiated in Fig. 2.\nIndexed Upsampling (IU) is the inverse operator of IP.\nIUupsamplesd2R1\u00021that spatially corresponds to E\ntaking the same indices into account. Let I2Rk\u0002kbe\nthe local index map formed by I(x)s,IUupsamplesdas\nIU(d) =I\nD, where\ndenotes the element-wise multi-\nplication, and Dis of the same size as Iand is upsampled\nfromdwith the nearest neighbor interpolation. An impor-\ntant difference between deconvolution and IUis that, de-\nconvolution applies a ﬁxed kernel to all local regions, even\nif the kernel is learned, while IUupsamples different re-\ngions with different kernels (indices).\nHolistic Index Depthwise Index2x2xC1 x1x4 2x2xC1x1x4C\nHxWxCH xWx1 HxWxC HxWxCFigure 3: Conceptual differences between holistic index and\ndepthwise index.\n4.2. Holistic Index Networks\nHere we instantiate two types of HINs. Recall that HINs\nlearn an index function I(X) :RH\u0002W\u0002C!RH\u0002W\u00021. A\nnaive design choice is to assume a linear relationship be-\ntween the feature map and the index map.\nLinear Holistic Index Networks. An example is shown in\nFig. 4(a). The network is implemented in a fully convolu-\ntional way. It ﬁrst applies 2-stride 2\u00022convolution to the\nfeature map of size H\u0002W\u0002C, generating a concatenated\nindex map of size H=2\u0002W=2\u00024. Each slice of the in-\ndex map (H=2\u0002W=2\u00021) is designed to correspond to\nthe indices of a certain position of all local regions, e.g.,\nthe top-left corner of all 2\u00022regions. The network ﬁnally\napplies a PS-like shufﬂing operator to rearrange the index\nmap to the size of H\u0002W\u00021.\nIn many situations, assuming a linear relationship is not\nsufﬁcient. An obvious fact is that a linear function even\ncannot ﬁt the max function. Naturally the second design\nchoice is to add nonlinearity into the network.\nNonlinear Holistic Index Networks. Fig. 4(b) illustrates a\nnonlinear HIN where the feature map is ﬁrst projected to a\nmap of sizeH=2\u0002W=2\u00022C, followed by a batch normal-\nization layer and a ReLU function for nonlinear mappings.\nWe then use point-wise convolution to reduce the channel\ndimension to an indices-compatible size. The rest transfor-\nmations follow its linear counterpart.\nRemark 1 . Note that, the holistic index map is shared by\nall channels of the feature map, which means the index map\nshould be expanded to the size of H\u0002W\u0002Cwhen feeding\nintoIPandIU. Fortunately, many existing packages sup-\nport implicit expansion over the singleton dimension. This\nindex map could be thought as a collection of local atten-\ntion maps [34] applied to individual local spatial regions. In\nthis case, the IPandIUoperators can also be referred to\n“attentional pooling” and “attentional upsampling”.\n4.3. Depthwise Index Networks\nIn DINs, we ﬁnd I(X) :RH\u0002W\u0002C!RH\u0002W\u0002C, i.e.,\neach spatial index corresponds to each spatial activation.\nThis family of networks further has two high-level design\nstrategies that correspond to two different assumptions.\n\nConv\n2x2x4\nstride 2\nHxWxC H/2xW/2x4H xWx1\nConv+BN+ReLU\n2x2x2C, stride 2Shuffling\nHxWxC H/2xW/2x2C HxWx1 H/2xW/2x4Conv\n1x1x4(a)\n(b)ShufflingFigure 4: Holistic index networks. (a) a linear index network; (b)\na nonlinear index network.\nHxWxCHxWxC\nH/2xW/2xCGroup Conv\n1x1xC\ngroup N\nH/2xW/2xCBN+ReLU\nBN+ReLUBN+ReLUBN+ReLUBN: Batch Normalization\nFigure 5: Depthwise index networks. N=Cfor the O2O as-\nsumption, and N= 1 for the M2O. The masked modules are\ninvisible to linear networks.\nOne-to-One (O2O) Assumption assumes that each slice of\nthe index map only relates to its corresponding slice of the\nfeature map. It can be denoted by a local index function\nl(X) :Rk\u0002k\u00021!Rk\u0002k\u00021, wherekdenotes the size of\nlocal region. Similar to HINs, DINs can also be designed\nto have linear/nonlinear modeling ability. Fig. 5 shows an\nexample when k= 2. Note that, different from HINs, DINs\nfollow a multi-column architecture. Each column predicts\nindices speciﬁc to a certain spatial location of all local re-\ngions. The O2O assumption can be easily satisﬁed in DINs\nwith grouped convolution.\nLinear Depthwise Index Networks. As per Fig. 5, a feature\nmap goes through four parallel convolutional layers with\nthe same kernel size of 2\u00022\u0002C, a stride of 2, andC\ngroups, leading to four downsampled feature maps of size\nH=2\u0002W=2\u0002C. The ﬁnal index map is composed from the\nfour feature maps by shufﬂing and rearrangement. Note that\nthe parameters of four convolutional layers are not shared.\nNonlinear Depthwise Index Networks. Nonlinear DINs can\nbe easily modiﬁed from linear DINs by inserting four extra\nconvolutional layers. Each of them is followed by a BN\nlayer and a ReLU unit, as shown in Fig. 5. The rest remains\nthe same as the linear DINs.\nMany-to-One (M2O) Assumption assumes that each sliceof the index map relates with all channels of the fea-\nture map. The local index function is deﬁned as l(X) :\nRk\u0002k\u0002C!Rk\u0002k\u00021. Compared to O2O DINs, the only\ndifference in implementation is the use of standard convo-\nlution instead of group convolution, i.e., N= 1in Fig. 5.\nLearning with Weak Context . A desirable property of In-\ndexNet is that it can predict indices even from a large local\nfeature map, e.g., l(X) :R2k\u00022k\u0002C!Rk\u0002k\u00021. An intu-\nition behind this idea is that, if one identiﬁes a local max-\nimum point from a k\u0002kregion, its surrounding 2k\u00022k\nregion can further support whether this point is a part of a\nboundary or just an isolated noise point. This idea can be\neasily implemented by enlarging the convolutional kernel\nand is also applicable to HINs.\nRemark 2 . Both HINs and DINs have merits and draw-\nbacks. It is clear that DINs have higher capacity than HINs,\nso DINs may capture more complex local patterns but also\nbe at a risk of overﬁtting. By contrast, the index map gener-\nated by HINs is shared by all channels of the feature map, so\nthe decoder feature map can reserve its expressibility with-\nout forcibly reducing its dimensionality to ﬁt the shape of\nthe index map during upsampling. This gives much ﬂexi-\nbility for decoder design, while it is not the case for DINs.\n4.4. Relation to Other Networks\nIf considering the dynamic property of IndexNet,\nIndexNet shares a similar spirit with some recent networks.\nSpatial Transformer Networks (STNs) [21]. The STN\nlearns dynamic spatial transformation by regressing desired\ntransformation parameters \u0012with a localized network. A\nspatially-transformed output is then produced by a sampler\nparameterized by \u0012. Such a transformation is holistic for\nthe feature map, which is similar to HINs. The differences\nbetween STN and IndexNet are that their learning targets\nhave different physical deﬁnitions (spatial transformations\nvs. spatial indices), and that, STN is designed for global\ntransformation, while IndexNet predicts local indices.\nDynamic Filter Networks (DFNs) [22]. The DFN dynam-\nically generates ﬁlter parameters on-the-ﬂy with a so-called\nﬁlter generating network. Compared to conventional ﬁl-\nter parameters that are initialized, learned, and stayed ﬁxed\nduring inference, ﬁlter parameters in DFN are dynamic and\nsample-speciﬁc. The main difference between DFN and In-\ndexNet lies in the motivation of the design. Dynamic ﬁlters\nare learned for adaptive feature extraction, but learned in-\ndices are used for dynamic downsampling and upsampling.\nDeformable Convolutional Networks (DCNs) [10]. The\nDCN introduces deformable convolution and deformable\nRoI pooling. The key idea is to predict offsets for convo-\nlutional and pooling kernels, so DCN is also a dynamic net-\nwork. While these convolution and pooling operators con-\ncern spatial transformations, they are still built upon stan-\n\ndard max pooling and are not designed for upsampling pur-\nposes. By contrast, index-guided IPandIUare fundamen-\ntal operators and may be integrated into RoI pooling.\nAttention Networks [34]. Attention networks are a broad\nfamily of networks that adopt attention mechanisms. The\nmechanisms introduce multiplicative interactions between\ninferred attention maps and feature maps. In Computer Vi-\nsion, these mechanisms often refer to spatial attention [46],\nchannel attention [20] or both [48]. As aforementioned, IP\nandIUin HINs can be viewed as attentional operators to\nsome extent, which means indices are attention. In a re-\nverse sense, attention is also indices. For example, max-\npooling indices are a form of hard attention. Indices offer a\nnew perspective to understand attention. It is worth noting\nthat, despite IndexNet in its current implementation closely\nrelates to attention, it has a distinct physical deﬁnition and\nspecializes in upsampling rather than reﬁning feature maps.\n5. Results and Discussions\nWe evaluate our framework and IndexNet on the task of\nimage matting. This task is particularly suitable for visu-\nalizing the quality of learned indices. We mainly conduct\nexperiments on the Adobe Image Matting dataset [49]. This\nis so far the largest publicly available matting dataset. The\ntraining set has 431 foreground objects and ground-truth al-\npha mattes.1Each foreground is composited with 100 back-\nground images randomly chosen from MS COCO [31]. The\ntest set termed Composition-1k includes 100 unique ob-\njects. Each of them is composited with 10 background im-\nages chosen from Pascal VOC [12]. Overall, we have 43100\ntraining images and 1000 testing images. We evaluate\nthe results using widely-used Sum of Absolute Differences\n(SAD), Mean Squared Error (MSE), and perceptually-\nmotivated Gradient (Grad) and Connectivity (Conn) er-\nrors [37]. The evaluation code implemented by [49] is\nused. In what follows, we ﬁrst describe our modiﬁed\nMobileNetv2-based architecture and training details. We\nthen perform extensive ablation studies to justify choices\nof model design, make comparisons of different index net-\nworks, and visualize learned indices. We also report perfor-\nmance on the alphamatting :com online benchmark [37]\nand extend IndexNet to other visual tasks.\n5.1. Implementation Details\nOur implementation is based on PyTorch [36]. Here we\ndescribe the network architecture used and some essential\ntraining details.\nNetwork Architecture . We build our model based on\nMobileNetv2 [39] with only slight modiﬁcations to the\n1The original paper reported that there were 491 images, but the re-\nleased dataset only includes 431 images. As a result, we use fewer training\ndata than the original paper.\n320x320x4\n320x320x323x3x32, stride=1\n2x2 max pool\n160x160x32\n160x160x16\n2x2 max pool160x160x24\n80x80x24\n80x80x323x3x32, stride=1\n2x2 max pool\n40x40x323x3x16\n3x3x24, stride=1\n3x3x64, stride=1\n40x40x64\n2x2 max pool\n20x20x64\n3x3x96\n20x20x96\n3x3x160,  stride=1\n20x20x160\n2x2 max pool\n10x10x160\n3x3x320\n10x10x320ASPP10x10x1602x2 max unpool20x20x1605x5x9620x20x965x5x6420x20x642x2 max unpool40x40x645x5x3240x40x322x2 max unpool80x80x325x5x2480x80x242x2 max unpool160x160x245x5x16160x160x165x5x32160x160x322x2 max unpool320x320x325x5x32320x320x325x5x1320x320x1 Input layer\nConv+BN+ReLU\nDownsampling layer\nEncoder feature maps\nDecoder feature maps\nAtrous spatial pyramid pooling\n10x10x320Ouput layerUpsampling layer\nEncoding stage\nPropagate indicesFuse encoder featuresDepthwise Conv+BN+ReLU\nE1\nE2\nE3\nE4\nE5\nE6\nE7D0\nD1E0\nD2\nD3\nD4\nD5\nD6\nE8/D7Figure 6: Customized MobileNetv2-based encoder-decoder net-\nwork architecture. Our modiﬁcations are boldfaced.\nbackbone. An important reason why we choose Mo-\nbileNetv2 is that this lightweight model allows us to in-\nfer high-resolution images on a GPU, while other high-\ncapacity backbones cannot. The basic network conﬁgura-\ntion is shown in Fig. 6. It also follows the encoder-decoder\nparadigm same as SegNet. We simply change all 2-stride\nconvolution to be 1-stride and attach 2-stride 2\u00022max\npooling after each encoding stage for downsampling, which\nallows us to extract indices. If applying the IndexNet idea,\nmax pooling and unpooling layers can be replaced with IP\nandIU, respectively. We also investigate alternative ways\nfor low-level feature fusion and whether encoding context\n(Section 5.2). Notice that, the matting reﬁnement stage [49]\nis not considered in this paper.\nTraining Details . To enable a direct comparison with deep\nmatting [49], we follow the same training conﬁgurations\nused in [49]. The 4-channel input concatenates the RGB\nimage and its trimap. We follow exactly the same data aug-\nmentation strategies, including 320\u0002320random cropping,\nrandom ﬂipping, random scaling, and random trimap dila-\ntion. All training samples are created on-the-ﬂy. We use a\ncombination of the alpha prediction loss and the composi-\ntion loss during training as in [49]. Only losses from the\nunknown region of the trimap are calculated. Encoder pa-\nrameters are pretrained on ImageNet [11]. Note that, the\nparameters of the 4-th input channel are initialized with ze-\nros. All other parameters are initialized with the improved\nXavier [16]. The Adam optimizer [23] is used. We update\nparameters with 30epochs (around 90;000iterations). The\nlearning rate is initially set to 0:01and reduced by 10\u0002at\n\nNo. Architecture Backbone Fusion Indices Context OS SAD MSE Grad Conn\nB1 DeepLabv3+ [4] MobileNetv2 Concat No ASPP 16 60.0 0.020 39.9 61.3\nB2 ReﬁneNet [30] MobileNetv2 Skip No CRP 32 60.2 0.020 41.6 61.4\nB3 SegNet [49] VGG16 No Yes No 32 54.6 0.017 36.7 55.3\nB4 SegNet VGG16 No No No 32 122.4 0.100 161.2 130.1\nB5 SegNet MobileNetv2 No Yes No 32 60.7 0.021 40.0 61.9\nB6 SegNet MobileNetv2 No No No 32 78.6 0.031 101.6 82.5\nB7 SegNet MobileNetv2 No Yes ASPP 32 58.0 0.021 39.0 59.5\nB8 SegNet MobileNetv2 Skip Yes No 32 57.1 0.019 36.7 57.0\nB9 SegNet MobileNetv2 Skip Yes ASPP 32 56.0 0.017 38.9 55.9\nB10 UNet MobileNetv2 Concat Yes No 32 54.7 0.017 34.3 54.7\nB11 UNet MobileNetv2 Concat Yes ASPP 32 54.9 0.017 33.8 55.2\nTable 1: Ablation study of design choices. Fusion: fuse encoder features; Indices: max-pooling indices (when Indices is ‘No’, bilinear\ninterpolation is used for upsampling); CRP: chained residual pooling [30]; ASPP: atrous spatial pyramid pooling [4]; OS: output stride.\nThe lowest errors are boldfaced.\nthe20-th and 26-th epoch respectively. We use a batch size\nof16and ﬁx the BN layers of the backbone.\n5.2. Adobe Image Matting Dataset\nAblation Study on Model Design . Here we investigate\nstrategies for fusing low-level features (no fusion, skip fu-\nsion as in ResNet [17] or concatenation as in UNet [38]) and\nwhether encoding context for image matting. 11baselines\nare consequently built to justify model design. Results on\nthe Composition-1k testing set are reported in Table 1. B3\nis cited from [49]. We can make the following observations:\ni) Indices are of great importance. Matting can signiﬁcantly\nbeneﬁt from only indices (B3 vs. B4, B5 vs. B6); ii) State-\nof-the-art semantic segmentation models cannot be directly\napplied to image matting (B1/B2 vs. B3); iii) Fusing low-\nlevel features help, and concatenation works better than the\nskip connection but at a cost of increased computation (B5\nvs. B8 vs. B10 or B7 vs. B9 vs. B11); iv) Our intuition\ntells that the context may not help a low-level task like mat-\nting, while results show that encoding context is generally\nencouraged (B5 vs. B7 or B8 vs. B9 or B10 vs. B11). In-\ndeed, we observe that the context sometimes can help to\nimprove the quality of the background; v) A MobileNetv2-\nbased model can work as well as a VGG-16-based one with\nappropriate design choices (B3 vs. B11).\nFor the following experiments, we now mainly use B11.\nAblation Study on Index Networks . Here we compare\ndifferent index networks and justify their effectiveness. The\nconﬁgurations of index networks used in the experiments\nfollow Figs. 4 and 5. We primarily investigate the 2\u00022\nkernel with a stride of 2. Whenever the weak context is\nconsidered, we use a 4\u00024kernel in the ﬁrst convolutional\nlayer of index networks. To highlight the effectiveness of\nHINs, we further build a baseline called holistic max in-\ndex (HMI) where max-pooling indices are extracted from\na squeezed feature map X02RH\u0002W\u00021.X0is generated\nby applying the max function along the channel dimensionofX2RH\u0002W\u0002C. We also report the performance when\nsetting the width multiplier of MobileNetV2 used in B11\nto be 1:4(B11-1.4). This allows us to justify whether the\nimproved performance is due to increased model capacity.\nResults on the Composition-1k testing dataset are listed in\nTable 2. We observe that, except the most naive linear HIN,\nall index networks consistently reduce the errors. In partic-\nular, nonlinearity and the context generally have a positive\neffect on deep image matting. Compared to HMI, the direct\nbaseline of HINs, the best HIN (“Nonlinear+Context”) has\nat least 12:3%relative improvement. Compared to B11, the\nbaseline of DINs, M2O DIN with “Nonlinear+Context” ex-\nhibits at least 16:5%relative improvement. Notice that, our\nbest model even outperforms the state-of-the-art DeepMat-\nting [49] that has the reﬁnement stage, and is also computa-\ntionally efﬁcient with less memory consumption—the infer-\nence can be performed on the GTX 1070 over 1920\u00021080\nhigh-resolution images. Some qualitative results are shown\nin Fig. 7. Our predicted mattes show improved delineation\nfor edges and textures like hair and water drops.\nIndex Map Visualization . It is interesting to see what in-\ndices are learned by IndexNet. For the holistic index, the\nindex map itself is a 2D matrix and is easily to be visual-\nized. Regarding the depthwise index, we squeeze the index\nmap along the channel dimension and calculate the average\nresponses. Two examples of learned index maps are visual-\nized in Fig. 8. We observe that, initial random indices have\npoor delineation for edges, while learned indices automat-\nically capture the complex structural and textual patterns,\ne.g., the fur of the dog, and even air bubbles in the water.\n5.3. alphamatting.com Online Benchmark\nWe also report results on the alphamatting :comonline\nbenchmark [37]. We directly test our best model trained\non the Adobe Image Dataset, without ﬁne-tuning. Our ap-\nproach (IndexNet Matting) ranks the ﬁrst in terms of the\ngradient error among published methods, as shown in Ta-\n\nFigure 7: Qualitative results on the Composition-1k testing set. From left to right, the original image, trimap, ground-truth alpha matte,\nclosed-form matting [29], deep image image [29], and ours (M2O DIN with “nonlinear + context”).\nMethod #Param. GFLOPs SAD MSE Grad Conn\nB3 [49] 130.55M 32.34 54.6 0.017 36.7 55.3\nB11 3.75M 4.08 54.9 0.017 33.8 55.2\nB11-1.4 8.86M 7.61 55.6 0.016 36.4 55.7\nHMI 3.75M 4.08 56.5 0.021 33.0 56.4\nNL C \u0001\nHINs\n+4.99K 4.09 55.1 0.018 32.1 55.2\nX +19.97K 4.11 53.5 0.018 31.0 53.5\nX +0.26M 4.22 50.6 0.015 27.9 49.4\nXX +1.04M 4.61 49.5 0.015 25.6 49.2\nO2O DINs\n+4.99K 4.09 50.3 0.015 33.7 50.0\nX +19.97K 4.11 47.8 0.015 26.9 45.6\nX +17.47K 4.10 50.6 0.016 26.5 50.3\nXX +47.42K 4.15 50.2 0.016 26.8 49.3\nM2O DINs\n+0.52M 4.34 51.0 0.015 33.7 50.5\nX +2.07M 5.12 50.6 0.016 31.9 50.2\nX +1.30M 4.73 48.9 0.015 32.1 47.9\nXX +4.40M 6.30 45.8 0.013 25.9 43.7\nClosed-Form [29] 168.1 0.091 126.9 167.9\nDeepMatting w. Reﬁnement [49] 50.4 0.014 31.0 50.8\nTable 2: Results on the Composition-1k testing set. GFLOPs\nare measured on a 224\u0002224\u00024input. NL: Non-Linearity; C:\nContext. The lowest errors are boldfaced.\nFigure 8: Visualization of the randomly initialized index map\n(left) and the learned index map (right) of HINs (top) and DINs\n(bottom). Best viewed by zooming in.ble 3. According to the qualitative results in Fig. 9, our\napproach produces signiﬁcantly better mattes on hair.\n5.4. Extensions to Other Visual Tasks\nWe further evaluate IndexNet on other three visual\ntasks. For image classiﬁcation, we compare three classi-\nﬁcation networks (LeNet [27], MobileNet [18] and VGG-\n16 [43]) on the CIFAR-10 and CIFAR-100 datasets [25]\nwith/without IndexNet. For monocular depth estimation,\nwe attach IndexNet upon a recent ResNet-50 based base-\nline [19] and report the performance on the NYUDv2\ndataset [42]. On the task of scene understanding, we eval-\nuate SegNet [2] with/without IndexNet on the SUN-RGBD\ndataset [44]. Results show that IndexNet consistently im-\nproves the performance in all three tasks. We refer readers\nto the Supplement for quantitative and qualitative results.\n6. Conclusion\nInspired by an observation in image matting, we delve\ndeep into the role of indices and present an uniﬁed perspec-\ntive of upsampling operators using the notion of index func-\ntion. We show that an index function can be learned within\na proposed index-guided encoder-decoder framework. In\nthis framework, indices are learned with a ﬂexible network\nmodule termed IndexNet, and are used to guide downsam-\npling and upsampling using two operators called IPandIU.\nIndexNet itself is also a sub-framework that can be designed\ndepending on the task at hand. We instantiated, investi-\ngated three index networks, compared their conceptual dif-\nferences, discussed their properties, and demonstrated their\neffectiveness on the task of image matting, image classiﬁ-\ncation, depth prediction and scene understanding. We re-\nport state-of-the-art performance on image matting with a\nmodiﬁed MobileNetv2-based model on the Composition-\n1k dataset. We believe that IndexNet is an important step\ntowards the design of generic upsampling operators.\nOur model is simple with much room for improvement.\nIt may be used as a strong baseline for future research. We\nplan to explore the applicability of IndexNet to other dense\nprediction tasks.\n\nGradient ErrorAverage Rank Troll Doll Donkey Elephant Plant Pineapple Plastic Bag Net\nOverall S L U S L U S L U S L U S L U S L U S L U S L U S L U\nIndexNet Matting 9 7.3 7.6 12.3 0.2 0.2 0.2 0.1 0.1 0.3 0.2 0.2 0.2 0.2 0.2 0.4 1.7 1.9 2.5 1 1.1 1.3 1.1 1.2 1.2 0.4 0.5 0.5\nAlphaGAN [33] 13.2 12 10.8 16.8 0.2 0.2 0.2 0.2 0.2 0.3 0.2 0.3 0.3 0.2 0.2 0.4 1.8 2.4 2.7 1.1 1.4 1.5 0.9 1.1 10.5 0.5 0.6\nDeep Matting [49] 14.3 10.8 11 21 0.4 0.4 0.5 0.2 0.2 0.2 0.1 0.1 0.2 0.2 0.2 0.6 1.3 1.5 2.4 0.8 0.9 1.3 0.7 0.8 1.1 0.4 0.5 0.5\nTable 3: Gradient errors (top 3) on the alphamatting :comonline benchmark. The lowest errors are boldfaced.\nFigure 9: Qualitative results on the alphamatting :comdataset. From left to right, the original image, deep image matting, ours.\nAcknowledgments We would like to thank Huawei\nTechnologies for the donation of GPU cloud computing re-\nsources.\nReferences\n[1] Y . Aksoy, T. Ozan Aydin, and M. Pollefeys. Designing effec-\ntive inter-pixel information ﬂow for natural image matting.\nInProc. IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , pages 29–37, 2017. 2\n[2] V . Badrinarayanan, A. Kendall, and R. Cipolla. SegNet: A\ndeep convolutional encoder-decoder architecture for image\nsegmentation. IEEE Transactions on Pattern Analysis and\nMachine Intelligence , 39(12):2481–2495, 2017. 1, 2, 8\n[3] G. Chen, K. Han, and K.-Y . K. Wong. TOM-Net: Learn-\ning transparent object matting from a single image. In Proc.\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR) , pages 9233–9241, 2018. 2\n[4] L.-C. Chen, Y . Zhu, G. Papandreou, F. Schroff, and H. Adam.\nEncoder-decoder with atrous separable convolution for se-\nmantic image segmentation. In Proc. European Conference\non Computer Vision (ECCV) , 2018. 1, 2, 7\n[5] Q. Chen, T. Ge, Y . Xu, Z. Zhang, X. Yang, and K. Gai. Se-\nmantic human matting. In Proc. ACM Multimedia , pages\n618–626, 2018. 2\n[6] Q. Chen, D. Li, and C.-K. Tang. KNN matting. IEEE\nTransactions on Pattern Analysis and Machine Intelligence ,\n35(9):2175–2188, 2013. 2\n[7] X. Chen, D. Zou, S. Zhiying Zhou, Q. Zhao, and P. Tan. Im-\nage matting with local and nonlocal smooth priors. In Proc.\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR) , pages 1902–1907, 2013. 2[8] D. Cho, Y .-W. Tai, and I. Kweon. Natural image matting\nusing deep convolutional neural networks. In Proc. Euro-\npean Conference on Computer Vision (ECCV) , pages 626–\n643. Springer, 2016. 2\n[9] Y .-Y . Chuang, B. Curless, D. H. Salesin, and R. Szeliski. A\nbayesian approach to digital matting. In Proc. IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR) ,\npage 264. IEEE, 2001. 2\n[10] J. Dai, H. Qi, Y . Xiong, Y . Li, G. Zhang, H. Hu, and Y . Wei.\nDeformable convolutional networks. In Proc. IEEE Interna-\ntional Conference on Computer Vision (ICCV) , pages 764–\n773, 2017. 5\n[11] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nFei. ImageNet: A large-scale hierarchical image database.\nInProc. IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , pages 248–255. Ieee, 2009. 6\n[12] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and\nA. Zisserman. The pascal visual object classes (voc) chal-\nlenge. International Journal of Computer Vision , 88(2):303–\n338, 2010. 6\n[13] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-\nture hierarchies for accurate object detection and semantic\nsegmentation. In Proc. IEEE Conference on Computer Vi-\nsion and Pattern Recognition (CVPR) , pages 580–587, 2014.\n2\n[14] Y . Guan, W. Chen, X. Liang, Z. Ding, and Q. Peng. Easy\nmatting-a stroke based approach for continuous image mat-\nting. Computer Graphics Forum , 25(3):567–576, 2006. 2\n[15] K. He, C. Rhemann, C. Rother, X. Tang, and J. Sun. A global\nsampling method for alpha matting. In Proc. IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR) ,\npages 2049–2056. IEEE, 2011. 2\n\n[16] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into\nrectiﬁers: Surpassing human-level performance on imagenet\nclassiﬁcation. In Proc. IEEE International Conference on\nComputer Vision (ICCV) , pages 1026–1034, 2015. 6\n[17] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition. In Proc. IEEE conference on Com-\nputer Vision and Pattern Recognition (CVPR) , pages 770–\n778, 2016. 7\n[18] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,\nT. Weyand, M. Andreetto, and H. Adam. Mobilenets: Efﬁ-\ncient convolutional neural networks for mobile vision appli-\ncations. arXiv , 2017. 8\n[19] J. Hu, M. Ozay, Y . Zhang, and T. Okatani. Revisiting sin-\ngle image depth estimation: toward higher resolution maps\nwith accurate object boundaries. In Proc. IEEE Winter Con-\nference on Applications of Computer Vision (WACV) , pages\n1043–1051. IEEE, 2019. 8\n[20] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation net-\nworks. In Proc. IEEE Conference on Computer Vision and\nPattern Recognition , pages 7132–7141, 2018. 6\n[21] M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial\ntransformer networks. In Advances in Neural Information\nProcessing Systems (NIPS) , pages 2017–2025, 2015. 5\n[22] X. Jia, B. De Brabandere, T. Tuytelaars, and L. V . Gool. Dy-\nnamic ﬁlter networks. In Advances in Neural Information\nProcessing Systems (NIPS) , pages 667–675, 2016. 5\n[23] D. P. Kingma and J. Ba. Adam: A method for stochastic\noptimization. In Proc. International Conference on Learning\nRepresentations (ICLR) , 2015. 6\n[24] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis.\nThe case for learned index structures. In Proc. International\nConference on Management of Data , pages 489–504. ACM,\n2018. 2\n[25] A. Krizhevsky and G. Hinton. Learning multiple layers of\nfeatures from tiny images. Technical report, Citeseer, 2009.\n8\n[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet\nclassiﬁcation with deep convolutional neural networks. In\nAdvances in Neural Information Processing Systems (NIPS) ,\npages 1097–1105, 2012. 2\n[27] Y . LeCun, L. Bottou, Y . Bengio, P. Haffner, et al. Gradient-\nbased learning applied to document recognition. Proceed-\nings of the IEEE , 86(11):2278–2324, 1998. 8\n[28] P. Lee and Y . Wu. Nonlocal matting. In Proc. IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR) ,\npages 2193–2200. IEEE, 2011. 2\n[29] A. Levin, D. Lischinski, and Y . Weiss. A closed-form solu-\ntion to natural image matting. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , 30(2):228–242, 2008. 2,\n8\n[30] G. Lin, A. Milan, C. Shen, and I. Reid. ReﬁneNet: Multi-\npath reﬁnement networks for high-resolution semantic seg-\nmentation. In Proc. IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) , pages 1925–1934, 2017.\n1, 2, 7\n[31] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-\nmanan, P. Doll ´ar, and C. L. Zitnick. Microsoft coco: Com-mon objects in context. In Proc. European Conference on\nComputer Vision (ECCV) , pages 740–755. Springer, 2014. 6\n[32] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional\nnetworks for semantic segmentation. In Proc. IEEE confer-\nence on Computer Vision and Pattern Recognition (CVPR) ,\npages 3431–3440, 2015. 1, 2, 3\n[33] S. Lutz, K. Amplianitis, and A. Smolic. AlphaGAN: Gen-\nerative adversarial networks for natural image matting. In\nProc. British Machince Vision Conference (BMVC) , 2018. 9\n[34] V . Mnih, N. Heess, A. Graves, et al. Recurrent models of vi-\nsual attention. In Advances in Neural Information Processing\nSystems (NIPS) , pages 2204–2212, 2014. 4, 6\n[35] C. Osendorfer, H. Soyer, and P. Van Der Smagt. Image super-\nresolution with fast approximate convolutional sparse cod-\ning. In Proc. International Conference on Neural Informa-\ntion Processing (ICONIP) , pages 250–257. Springer, 2014.\n2\n[36] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-\nVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Au-\ntomatic differentiation in pytorch. In Advances in Neural\nInformation Processing Systems Workshops (NIPSW) , 2017.\n6\n[37] C. Rhemann, C. Rother, J. Wang, M. Gelautz, P. Kohli, and\nP. Rott. A perceptually motivated online benchmark for im-\nage matting. In Proc. IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) , pages 1826–1833. IEEE,\n2009. 6, 7\n[38] O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolu-\ntional networks for biomedical image segmentation. In Proc.\nInternational Conference on Medical Image Computing and\nComputer-Assisted Intervention (MICCAI) , pages 234–241.\nSpringer, 2015. 7\n[39] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C.\nChen. Mobilenetv2: Inverted residuals and linear bottle-\nnecks. In Proc. IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) , pages 4510–4520, 2018. 2, 6\n[40] X. Shen, X. Tao, H. Gao, C. Zhou, and J. Jia. Deep automatic\nportrait matting. In Proc. European Conference on Computer\nVision (ECCV) , pages 92–107. Springer, 2016. 2\n[41] W. Shi, J. Caballero, F. Husz ´ar, J. Totz, A. P. Aitken,\nR. Bishop, D. Rueckert, and Z. Wang. Real-time single im-\nage and video super-resolution using an efﬁcient sub-pixel\nconvolutional neural network. In Proc. IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) , pages\n1874–1883, 2016. 1, 2, 3\n[42] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor seg-\nmentation and support inference from rgbd images. In Proc.\nEuropean Conference on Computer Vision (ECCV) , pages\n746–760. Springer, 2012. 8\n[43] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. In Proc. Inter-\nnational Conference on Learning Representations (ICLR) ,\n2014. 8\n[44] S. Song, S. P. Lichtenberg, and J. Xiao. SUN RGB-D:\nA RGB-D scene understanding benchmark suite. In Proc.\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR) , pages 567–576, 2015. 8\n\n[45] J. Sun, J. Jia, C.-K. Tang, and H.-Y . Shum. Poisson matting.\nACM Transactions on Graphics , 23(3):315–321, 2004. 2\n[46] F. Wang, M. Jiang, C. Qian, S. Yang, C. Li, H. Zhang,\nX. Wang, and X. Tang. Residual attention network for im-\nage classiﬁcation. In Proc. IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) , pages 3156–3164,\n2017. 6\n[47] Y . Wang, Y . Niu, P. Duan, J. Lin, and Y . Zheng. Deep prop-\nagation based image matting. In Proc. International Joint\nConferences on Artiﬁcial Intelligence (IJCAI) , pages 999–\n1066, 2018. 2\n[48] S. Woo, J. Park, J.-Y . Lee, and I. So Kweon. CBAM: Con-\nvolutional block attention module. In Proc. European Con-\nference on Computer Vision (ECCV) , pages 3–19, 2018. 6\n[49] N. Xu, B. Price, S. Cohen, and T. Huang. Deep image mat-\nting. In Proc. IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) , pages 2970–2979, 2017. 1, 2,\n3, 6, 7, 8, 9\n[50] M. D. Zeiler and R. Fergus. Visualizing and understanding\nconvolutional networks. In Proc. European Conference on\nComputer Vision (ECCV) , pages 818–833. Springer, 2014.\n1, 2",
  "textLength": 47731
}