{
  "paperId": "7c4145425972a208d472e69b318d0b1b8cf21cdf",
  "title": "GraphBLAS on the Edge: Anonymized High Performance Streaming of Network Traffic",
  "pdfPath": "7c4145425972a208d472e69b318d0b1b8cf21cdf.pdf",
  "text": "GraphBLAS on the Edge:\nAnonymized High Performance Streaming of\nNetwork Trafﬁc\nMichael Jones1, Jeremy Kepner1, Daniel Andersen2, Aydın Buluc ¸3, Chansup Byun1, K Claffy2, Timothy Davis4,\nWilliam Arcand1, Jonathan Bernays1, David Bestor1, William Bergeron1, Vijay Gadepally1, Micheal Houle1,\nMatthew Hubbell1, Hayden Jananthan1, Anna Klein1, Chad Meiners1, Lauren Milechin1, Julie Mullen1,\nSandeep Pisharody1, Andrew Prout1, Albert Reuther1, Antonio Rosa1, Siddharth Samsi1,\nJon Sreekanth5, Doug Stetson1, Charles Yee1, Peter Michaleas1\n1MIT,2CAIDA,3LBNL,4Texas A&M,5Accolade Technology\nAbstract —Long range detection is a cornerstone of defense in\nmany operating domains (land, sea, undersea, air, space, ..,). In\nthe cyber domain, long range detection requires the analysis\nof signiﬁcant network trafﬁc from a variety of observatories\nand outposts. Construction of anonymized hypersparse trafﬁc\nmatrices on edge network devices can be a key enabler by\nproviding signiﬁcant data compression in a rapidly analyzable\nformat that protects privacy. GraphBLAS is ideally suited for\nboth constructing and analyzing anonymized hypersparse trafﬁc\nmatrices. The performance of GraphBLAS on an Accolade\nTechnologies edge network device is demonstrated on a near\nworse case trafﬁc scenario using a continuous stream of CAIDA\nTelescope darknet packets. The performance for varying numbers\nof trafﬁc buffers, threads, and processor cores is explored.\nAnonymized hypersparse trafﬁc matrices can be constructed at\na rate of over 50,000,000 packets per second; exceeding a typical\n400 Gigabit network link. This performance demonstrates that\nanonymized hypersparse trafﬁc matrices are readily computable\non edge network devices with minimal compute resources and\ncan be a viable data product for such devices.\nIndex Terms —Internet defense, packet capture, streaming\ngraphs, hypersparse matrices\nI. I NTRODUCTION\nThe Internet has become as essential as land, sea, air, and\nspace for enabling activities as diverse as commerce, educa-\ntion, health, and entertainment [1], [2]. Long range detection\nhas been a cornerstone of defense in many operating domains\nsince anitquity [3]–[10]. In the cyber domain, observatories\nand outposts have been constructed to gather data on Internet\ntrafﬁc and provide a starting point for exploring long range\ndetection [11]–[17] (see Figure 1). The largest public Internet\nThis material is based upon work supported by the Under Secretary of\nDefense for Research and Engineering under Air Force Contract No. FA8702-\n15-D-0001, National Science Foundation CCF-1533644, and United States Air\nForce Research Laboratory and Artiﬁcial Intelligence Accelerator Cooperative\nAgreement Number FA8750-19-2-1000. Any opinions, ﬁndings, conclusions\nor recommendations expressed in this material are those of the author(s) and\ndo not necessarily reﬂect the views of the Under Secretary of Defense for\nResearch and Engineering, the National Science Foundation, or the United\nStates Air Force. The U.S. Government is authorized to reproduce and\ndistribute reprints for Government purposes notwithstanding any copyright\nnotation herein.observatory is the Center for Applied Internet Data Analysis\n(CAIDA) Telescope that operates a variety of sensors including\na continuous stream of packets from an unsolicited darkspace\nrepresenting approximately 1/256 of the Internet [18]–[21].\nIn general, long range detection requires the analysis of\nsigniﬁcant network trafﬁc from a variety of observatories and\noutposts [22], [23].\nThe data volumes, processing requirements, and privacy\nconcerns of analyzing a signiﬁcant fraction of the Internet\nhave been prohibitive. The North American Internet generates\nbillions of non-video Internet packets each second [1], [2].\nThe GraphBLAS standard provides signiﬁcant performance\nand compression capabilities which improve the feasibility\nof analyzing these volumes of data [24]–[38]. Speciﬁcally,\nthe GraphBLAS is ideally suited for both constructing and\nanalyzing anonymized hypersparse trafﬁc matrices. Prior work\nwith the GraphBLAS has demonstrated rates of 75 billion\npackets per second (pps) [39], while achieving compressions\nof 1 bit per packet [17], and enabling the analysis of the\nlargest publicly available historical archives with over 40\ntrillion packets [40]. Analysis of anonymized hypersparse\ntrafﬁc matrices from a variety of sources has revealed power-\nlaw distributions [41], [42], novel scaling relations [17], [40],\nand inspired new models of network trafﬁc [43].\nGraphBLAS anonymized hypersparse trafﬁc matrices rep-\nresent one set of design choices for analyzing network trafﬁc.\nSpeciﬁcally, the use case requiring some data on all packets\n(no down-sampling), high performance, high compression,\nmatrix-based analysis, anonymization, and open standards.\nThere are a wide range of alternative graph/network anal-\nysis technologies and many good implementations achieve\nperformance close to the limits of the underlying computing\nhardware [44]–[54]. Likewise, there are many network analysis\ntools that focus on providing a rich interface to the full\ndiversity of data found in network trafﬁc [55]–[57]. Each of\nthese technologies has appropriate use cases in the broad ﬁeld\nof Internet trafﬁc analysis.\nSending large volumes of raw Internet trafﬁc to a centralarXiv:2203.13934v2  [cs.NI]  5 Sep 2022\n\n\u000b\u0005\u0006\u0004\n\n\b\f\t\u0002\u0004\n\u0003\u0004\n\u000b\u0005\u0007\u0001\u000b\u0005\b\u0007\n\n\u0016\u0014\n\t\u0013\u0015\u000e\u0014\u0013\f\u0012\n\u000b\u0014\f\u000f\u000f\u0011\r\u000b\u0010\u000e\u0011\u0014\n\t\u0013\u0015\u000e\u0014\u0013\f\u0012\n\u000b\u0014\f\u000f\u000f\u0011\r\u0011 1!-.\u001d-4\u0001\u001a\u001d'&%)#\u0001/*\u0001\u001b.\n\u001b.\u0001\u001a\u001d'&%)#\u0001/*\u0001\u001d 1!-.\u001d-4\u0001\n\t\u0006 \u0016*16/\u0001 \u001d-&\u0002\u001f'\u001d..\u0001\u0012\u0003\u0007\u001e'0!\u0001#\u001d/!2\u001d4\u0001\n\u00025\r\u00014!\u001d-.\u0005\u00015\t\b\u001a\u0001+\u001d\u001f&!/.\u0003\n\n\u0006 \u0018\u0011\u001c\u0017\u0001#-\u001d4\u0001/-0)&\u0001\u00025\r\u0001\u00014!\u001d-.\u0005\u00015\r\b\u0012\u0001\n+\u001d\u001f&!/.\u0003\n\u000b\u0006 \u0013\u0011\u0017\u0014\u0011\u0001#-\u001d4\u0001/-0)&\u0001\u00025\r\u0001\u00014!\u001d-.\u0005\u00015\r\b\u0012\u0001\n+\u001d\u001f&!/.\n\f\u0006 \u0013\u0011\u0017\u0014\u0011\u0001\u0015,0%)*3\u0001#-\u001d4\u0001/-0)&\u0001\u00025\t\b\b\u0001\n\u0016%#\u0015\u0003\n\r\u0006 \u0013\u0011\u0017\u0014\u0011\u0001 \u001d-&\u0002\u001f'\u001d..\u0001\u0011\u0003\u0001#\u001d/!2\u001d4\u0001\u0002\r\u0004\u0001\n4!\u001d-.\u0005\u00015\t\b\b\u001a\u0001+\u001d\u001f&!/. \u0003\n\u000e\u0006 \u0016-!4)*%.!#\u001d/!2\u001d4\u0001\u00025\f\b\b\u0001\u001d\u001f/%1!\u0001\n$*)!4+*/.\u0003\n\u000f\u0006 \u0016'*\u001e\u001d'\u0001\u00134\u001e!-\u0001\u0011''% \u001d)\u001f!\u0001#\u001d/!2\u001d4\u0001\u0002\u0017*\u001a\u0001\n$*)!4+*/\u0001\"\u001d-(\u0003\n\u0010\u0006 \u0019$\u001d *2.!-1!-#\u001d/!2\u001d4\u0001\u00025\t\b\b\u0018\u0001\n.%)&$*'! \u001e*/)!/.\u0003\u0001\u0001\u0002\u0003\u0004\u0005\u0006\n\b\u0007Fig. 1. Internet Observatories and Outposts. Trafﬁc matrix view of the Internet depicting selected observatories and outposts and their notional proximity\nto various types of network trafﬁc [11]–[17].\nlocation to construct anonymized hypersparse trafﬁc matrices\nis prohibitive. To meet the goal of providing some data on\nall packets without down-sampling requires constructing the\nanonymized hypersparse trafﬁc matrices in the network itself\nin order to realize the full data compression beneﬁts. The goal\nof this paper is to explore the viability of this approach by\nmeasuring the performance of GraphBLAS on an on edge\nnetwork device. The performance is measured on a near worse\ncase trafﬁc scenario using a continuous stream of CAIDA\nTelescope darknet packets (mostly botnets and scanners) which\nhave an irregular distribution and almost no data payload (i.e.,\nall header).\nThe outline of the rest of the paper is as follows. First, the\nCAIDA Telescope test data and some basic network quantities\nare deﬁned in terms of trafﬁc matrices. Next, the anonymized\nhypersparse trafﬁc matrix pipeline is described followed by\na description of the experimental setup and implementation.\nFinally, the results, conclusions, and directions for further\nwork are presented.\nII. T ESTDATA AND TRAFFIC MATRICES\nThe test data is drawn from the CAIDA Telescope darknet\npackets (mostly botnets and scanners) and is a near worse\ncase with a highly irregular distribution and almost no data\npayload (i.e., all header). The CAIDA Telescope monitors the\ntrafﬁc into and out of a set of network addresses providing a\nnatural observation point of network trafﬁc. These data can be\nviewed as a trafﬁc matrix where each row is a source and each\ncolumn is a destination. The CAIDA Telescope trafﬁc matrix\ncan be partitioned into four quadrants (see Figure 2). These\nquadrants represent different ﬂows between nodes internal and\nexternal to the set of monitored addresses. Because the CAIDA\nTelescope network addresses are a darkspace, only the upper\nleft (external!internal) quadrant will have data.\nInternet data must be handled with care, and CAIDA has\npioneered trusted data sharing best practices that combine\nanonymizing source and destinations using CryptoPAN [58]\nwith data sharing agreements. These data sharing best practices\nsourcesdestinationssourcesdestinationssparse traffic matrix A\ninternalàexternalinternalàinternalexternalàexternalexternalàinternal\ninternal                       externalinternal                       externalFig. 2. Network trafﬁc matrix. The trafﬁc matrix can be divided into\nquadrants separating internal and external trafﬁc. The CAIDA Telescope\nmonitors a darkspace, so only the upper left (external !internal) quadrant\nwill have data.\nare the basis of the architecture presented here and include the\nfollowing principles [22]\n\u000fData is made available in curated repositories\n\u000fUsing standard anonymization methods where needed:\nhashing, sampling, and/or simulation\n\u000fRegistration with a repository and demonstration of le-\ngitimate research need\n\u000fRecipients legally agree to neither repost a corpus nor\ndeanonymize data\n\u000fRecipients can publish analysis and data examples nec-\nessary to review research\n\u000fRecipients agree to cite the repository and provide pub-\nlications back to the repository\n\u000fRepositories can curate enriched products developed by\nresearchers\nA primary beneﬁt of constructing anonymized hypersparse\n\nvalidsource packetwindowNV=217,218,…,227(time window)validdestination packetwindowNV=217,218,…,227(time window)source packets(packets from a source)\ndestination packets(packets to a destination)uniquedestinationsuniquesourcesuniquelinkssourcefan-out\ndestinationfan-inlinkpacketsFig. 3. Streaming network trafﬁc quantities. Internet trafﬁc streams of\nNVvalid packets are divided into a variety of quantities for analysis: source\npackets, source fan-out, unique source-destination pair packets (or links),\ndestination fan-in, and destination packets.\nTABLE I\nNETWORK QUANTITIES FROM TRAFFIC MATRICES\nFormulas for computing network quantities from trafﬁc matrix Atat time tin\nboth summation and matrix notation. 1is a column vector of all 1’s,Tis the\ntranspose operation, and j j0is the zero-norm that sets each nonzero value of\nits argument to 1 [59]. These formulas are unaffected by matrix permutations\nand will work on anonymized data.\nAggregate Summation Matrix\nProperty Notation Notation\nValid packets NVP\niP\njAt(i; j)1TAt1\nUnique linksP\niP\njjAt(i; j)j01TjAtj01\nLink packets from itoj At(i; j) At\nMax link packets ( dmax) max ijAt(i; j) max( At)\nUnique sourcesP\nijP\njAt(i; j)j01TjAt1j0\nPackets from source iP\njAt(i; j) At1\nMax source packets ( dmax) max iP\njAt(i; j) max( At1)\nSource fan-out from iP\njjAt(i; j)j0jAtj01\nMax source fan-out ( dmax) max iP\njjAt(i; j)j0max(jAtj01)\nUnique destinationsP\njjP\niAt(i; j)j0j1TAtj01\nDestination packets to jP\niAt(i; j)1TjAtj0\nMax destination packets ( dmax)max jP\niAt(i; j) max( 1TjAtj0)\nDestination fan-in to jP\nijAt(i; j)j01TAt\nMax destination fan-in ( dmax) max jP\nijAt(i; j)j0max( 1TAt)\ntrafﬁc matrices with the GraphBLAS is the efﬁcient com-\nputation of a wide range of network quantities via matrix\nmathematics. Figure 3 illustrates essential quantities found\nin all streaming dynamic networks. These quantities are all\ncomputable from anonymized trafﬁc matrices created from the\nsource and destinations found in Internet packet headers.\nThe network quantities depicted in Figure 3 are computable\nfrom anonymized origin-destination trafﬁc matrices that are\nwidely used to represent network trafﬁc [60]–[63]. It is com-\nmon to ﬁlter the packets down to a valid set for any particular\nanalysis. Such ﬁlters may limit particular sources, destinations,\nprotocols, and time windows. To reduce statistical ﬂuctuations,\nthe streaming data should be partitioned so that for any chosen\ntime window all data sets have the same number of valid\npackets [64]. At a given time t,NVconsecutive valid packets\nare aggregated from the trafﬁc into a hypersparse matrix At,\nwhereAt(i; j)is the number of valid packets between the\nsource iand destination j. The sum of all the entries in Atisequal to NVX\ni;jAt(i; j) =NV\nConstant packet, variable time samples simplify the statistical\nanalysis of the heavy-tail distributions commonly found in\nnetwork trafﬁc quantities [41], [42], [65]. All the network\nquantities depicted in Figure 3 can be readily computed from\nAtusing the formulas listed in Table I. Because matrix opera-\ntions are generally invariant to permutation (reordering of the\nrows and columns), these quantities can readily be computed\nfrom anonymized data. Furthermore, the anonymized data can\nbe analyzed by subranges of IPs using simple matrix multi-\nplication. For a given subrange represented by an anonymized\nhypersparse diagonal matrix Ar, where Ar(i; i) = 1 implies\nsource/destination iis in the range, the trafﬁc within the sub-\nrange can be computed via: ArAtAr. Likewise, for additional\nprivacy guarantees that can be implemented at the edge, the\nsame method can be used to exclude a range of data from the\ntrafﬁc matrix\nAt\u0000ArAtAr\nThe contiguous nature of these data allows the exploration\nof a wide range of packet windows from NV= 217(sub-\nsecond) to NV= 227(minutes), providing a unique view into\nhow network quantities depend upon time. These observations\nprovide new insights into normal network background trafﬁc\nthat could be used for anomaly detection, AI feature engineer-\ning, polystore index learning, and testing theoretical models of\nstreaming networks [66]–[68].\nNetwork trafﬁc is dynamic and exhibits varying behavior\non a wide range of time scales. A given packet window\nsizeNVwill be sensitive to phenomena on its corresponding\ntimescale. Determining how network quantities scale with NV\nprovides insight into the temporal behavior of network trafﬁc.\nEfﬁcient computation of network quantities on multiple time\nscales can be achieved by hierarchically aggregating data in\ndifferent time windows [64]. Figure 4 illustrates a binary\naggregation of different streaming trafﬁc matrices. Computing\neach quantity at each hierarchy level eliminates redundant\ncomputations that would be performed if each packet window\nwas computed separately. Hierarchy also ensures that most\ncomputations are performed on smaller matrices residing in\nfaster memory. Correlations among the matrices mean that\nadding two matrices each with NVentries results in a matrix\nwith fewer than 2NVentries, reducing the relative number of\noperations as the matrices grow.\nOne of the important capabilities of the SuiteSparse Graph-\nBLAS library is direct support of hypersparse matrices where\nthe number of nonzero entries is signiﬁcantly less than either\ndimensions of the matrix. If the packet source and destination\nidentiﬁers are drawn from a large numeric range, such as those\nused in the Internet protocol, then a hypersparse representation\nofAteliminates the need to keep track of additional indices\nand can signiﬁcantly accelerate the computations [39].\n\nstream of trafficmatricesNV=217sourcessparsetrafficmatrixAt:t+TNV=218sourcessparsetrafficmatrixAt:t+2T++++\nNV=219sourcessparsetrafficmatrixAt:t+4T++sourcesdestinations\nsourcesdestinations\nsourcesdestinationsFig. 4. Multi-temporal streaming trafﬁc matrices. Efﬁcient computation of\nnetwork quantities on multiple time scales can be achieved by hierarchically\naggregating data in different time windows.\nIII. H YPERSPARSE MATRIX PIPELINE\nThe aforementioned analysis goals set the requirements for\nthe GraphBLAS hypersparse trafﬁc matrix pipeline. Speciﬁ-\ncally, the compression beneﬁts are maximized if the Graph-\nBLAS hypersparse trafﬁc matrix is constructed in the network\nas close to the network trafﬁc as possible as this minimizes the\namount of data that needs to be sent over the network. In addi-\ntion, collection at the network source allows the data owner to\nconstruct and own the anonymization scheme and only share\nanonymized data under trusted data sharing agreements with\nthe parties tasked with analyzing the data [69].\nThe ﬁrst step in the GraphBLAS hypersparse trafﬁc matrix\npipeline is to capture a packet, discard the data payload, and\nextract the source and destination Internet Protocol (IP) ad-\ndresses (Figure 5). For the purposes of the current performance\ntesting, only IPv4 packets are used which are stored as 32\nbit unsigned integers. Collections of NV= 217consecutive\npackets are then each anonymized using a cryptoPAN gener-\nated anonymization table. The resulting anonymized source\nand destination IPs are then used to construct a 232\u0002232\nhypersparse GraphBLAS matrix. 64 consecutive hypersparse\nGraphBLAS matrices are each serialized in compressed sparse\nrows (CSR) format with LZ4 compression and saved to a\nUNIX TAR ﬁle (Figure 6). The TAR ﬁles can be further\ncompressed using other compressing methods (if desired) and\nthen transmitted to the appropriate parties tasked for analysis.\nFor example, standard gzip compression reduces ﬁle size by\n40% but also reduces performance by 80%.\nIV. I MPLEMENTATION\nEffective implementation of the GraphBLAS pipeline re-\nquires that the anonymization, creation, and saving of the\nresulting ﬁles can keep up with the data rates of typical high\nbandwidth links. To measure this performance two Accolade\nTechnology ANIC-200Kq dual port 100 gigabit ﬂow classi-\nﬁcation and shunting adapters were installed into the PCIe\nslots of two HP Proliant DL360 G9 servers (Figure 7). These\nservers were connected via a Mellanox 40 gigabit network\nconnection. ANIC-200Kq cards are capable of a wide range\nof analysis techniques, this experiment only used their data\ntransmission, shunting, and buffering capabilities.\nThe C implementation of the GraphBLAS hypersparse\ntrafﬁc matrix pipeline shown in Figures 5 and 6 was run ondual Intel Xeon E5-2683 processors in the receiver server. The\nAccolade card on the receiver server collects packets in ring\nbuffers, the number of which can be set at initialization. Using\nC pthreads [70] an Accolade worker thread is assigned to each\nAccolade hardware ring. Within each Accolade worker thread,\na block of 223IPv4 packets are collected and a GraphBLAS\nworker thread is spawned to process the block in subblocks of\n217packets. Each sublock is anonymized using a cryptoPAN\ngenerated anonymization vector and the resulting anonymized\nsources and destinations are used to construct a GraphBLAS\nmatrix. The matrix is the serialized and appended to a TAR\nbuffer, which is saved to a ﬁle after all 64 subblocks have been\nprocessed. A more detailed outline of the code is as follows:\nMain Thread\n\u000fSet number of Accolade hardware rings\n\u000fLoad 232entry IPv4 anonymization table\n\u000fFor each Accolade hardware ring\n–Launch Accolade Worker thread\nAccolade Worker\n\u000fCreate libpcap handle to Accolade device\n\u000fAllocate 64MB buffer for packet processing\n\u000fFor each packet\n–Retrieve packet from Accolade device buffers\n–Append source and destination IP addresses to buffer\n–If buffer has 223packets\n\u000eLaunch GraphBLAS Worker thread with pointer to\npacket buffer\n\u000eAllocate new 64MB buffer for packet processing\nGraphBLAS Worker\n\u000fInitialize GraphBLAS\n\u000fFor each subblock of 217packets\n–Create new GraphBLAS matrix\n–Create row, column and value vectors\n–For each packet in subblock\n\u000eLookup in anonymization table\n\u000eInsert into row, column and value vectors\n–Build GraphBLAS matrix from row, column and value\nvectors; summing duplicate entries\n–Serialize and compress GraphBLAS matrix\n–Append to TAR buffer\n\u000fWrite TAR buffer to ﬁle\nV. R ESULTS\nUsing the experimental setup shown in Figure 7, a num-\nber of performance experiments were conducted. In these\nexperiments the sender server would load 100\u0002223CAIDA\nTelescope packets into the sender Accolade card and sends\nthe packets at deﬁned rate over the network to the receiver\nserver where the receiver Accolade card loads the packets\ninto its hardware rings. Likewise, on the receiver server the\nGraphBLAS hypersparse trafﬁc matrix pipeline described in\nthe previous section was executed. The cryptoPAN anonymiza-\ntion table was created ofﬂine, stored, and loaded at startup. The\nreason for this is that the single core cryptoPAN performance\nis approximately 700,000 IP address per second. The 232entry\ncryptoPAN anonymization lookup table dramatically speeds\nup the performance. Generation of the table is readily run in\nparallel and can be generated in a few seconds (if desired).\n\nVersionIHLService TypeECNTotal LengthIdentificationFlagsFragment OffsetHeader ChecksumTransport ProtocolTime to LiveSource IP AddressDestination IP AddressOptions + PaddingBit 0Bit 31IP Header(24 Bytes)\n45 00 01 13 00 00 40 00 40 06 18 46 c0 a8 c8 1a34 3d 64 9fc5 2e 01 bb df eac7 4f dc 89 d2 c580 18 08 00 22 96 00 00 01 01 08 0a 49 67 5c 0a86 23 3f 61 17 03 03 00 da 14 20 61 3a df eea7ff ba29 05 86 7a c7 76 d1 de 2a a3 4e ae fa cabb fd01 71 fc 8c 93 de ab 20 7a b5 8d 81 a2 c248 69 0b 35 f5 c1 8f c9 31 37 4b 4e 0f 97 44 ae4e b3 f5 11 dd 0e 0e 49 4c 95 79 77 c9 a1 b3 7ae5 9c 8b 88 80 38 4a 0f a5 a5 c9 67 cf7e b2 594c 0c 8f b0 eaa4 38 e0 e9 de 1c d4 f5 80 f1 d5a8 db95 04 a0 5f 50 d6 6e 97 9b 9f dd 82 14 26a8 c8 fe47 02 05 2d 33 28 37 87 3e 9c e9 f4 d565 84 45 78 e0 23 3f 34 94 2e 99 3c d5 05 ac 2565 6b 78 11 fd7b 08 a1 6e f3 47 62 3a e7 8a 435a 77 ef6a 88 d0 46 a9 dc 19 34 05 4e 69 b2 4056 a9 8f f5 d4 d6 21 e0 16 77 c9 35 a9 d0 20 4defe2 d0 8f f4 15 7c 3a f6 d2 93 aa 65 34 be b4a7 d0 7aSample PacketNetwork Packet Format\nSource IP AddressDestination IP Address192.168.200.2652.61.100.159PayloadFig. 5. Network Packet Description. A network packet consists of a header and a payload. To avoid downsampling and minimize privacy concerns only\nthe source and destination are selected.\nSrcIPDstIPTimestampSource IPDestination IPt0Source IP1Destination IP1t1Source IP2Destination IP2t2Source IP3Destination IP3t3Source IP4Destination IP4t4Source IP5Destination IP5Bucket of NVpacketsAsourcesdestinationsSparse Traffic Matrix1234232Anonymization Table\nFiles\nFig. 6. Anonymized Hypersparse Matrix Pipeline. Continuous sequences of NV= 217packets are extracted from packet headers, anonymized, formed\ninto a GraphBLAS hypersparse matrix, serialized, and saved in groups of 64 GraphBLAS matrices to a UNIX TAR ﬁle.\nPCIeGen 3.0 x16\nIntel Xeon E5-2683 QSFP28QSFP28Mellanox 40GBASE-CR4 DACHP ProLiant DL360 G9\nTransmit Direct MemoryAccess (TXDMA)\nIntel Xeon E5-2683 QSFP28QSFP28HP ProLiant DL360 G9SenderReceiver\nFig. 7. Experimental Setup. Test system consisted of two compute nodes\neach with an Accolade card connected over a network. The sender node reads\nCAIDA Telescope packet data into memory and then the Accolode card sends\nthe data directly from memory over the network to the receiver node. The\nAccolade card on the receiver takes the data off the network, places the data\nin a hardware ring buffer, and makes the data available to be processed by\nthe receiver processor.\n05101520Packets per SecondNumber of Rings100M80M60M40M20M0 \nEffective Bandwidth1000Gb800Gb600Gb400Gb200Gb0 Max InputFig. 8. Performance Results. Packets per second processed versus the\nnumber of hardware rings used. The packet performance can be converted\ninto an estimated equivalent bandwidth using a representative packets size of\n10,000 bits per packet (see right vertical axis)\nThe rate of packets being sent was adjusted to achieve the\nmaximum rate without dropped packets, which indicated that\nthe GraphBLAS hypersparse trafﬁc matrix pipeline was able\nto keep up with the incoming trafﬁc. The number of hardware\nrings were varied which increase the number of threads/cores\n\n02468101214161820\nNumber of Rings051015202530Processors Usedmax(top)\nperf\nmax(ps)Fig. 9. Processors Used. Processors used versus the number of hardware rings\nas determined by the max load of the Linux topcommand, average load of\nthe Linux perf command, and the max load of the Linux pscommand.\nbeing used.\nCAIDA Telescope data are a near worse case scenario\nbecause almost all of the packets have no payload and very\nfew packets use the same source and destination connection\nso the resulting hypersparse trafﬁc matrices have very few\nentries that are greater than 1. An advantage of the few\npayloads in the CAIDA Telescope data is that it allows the\nemulation of packet streams that are representative of much\nhigher bandwidth networks than the current experimental setup\nis capable of.\nFigure 8 shows the performance in terms packets per second\nversus the number of hardware rings used. GraphBLAS matrix\nconstruction is highly cache sensitive due to the underlying\nindex sorting required. The performance of a single ring using\na few processing threads/cores is over 50M packets per second.\nThe performance increases signiﬁcantly using two rings, and\nthen drops with 3 rings because of cache effects. Using 16\nrings makes up for the cache effects with increased parallelism\nand the performance reaches the maximum number of packets\nthe sender server can send to the receiver (approximately 88M\npackets per second). The packet performance can be converted\ninto an estimated equivalent bandwidth using a representative\npacket size of 10,000 bits per packet (see Figure 8 right vertical\naxis). The performance measurements indicate that a standard\nserver is a capable of constructing anonymized hypersparse\ntrafﬁc matrices at a rate above that corresponding to a typical\n400 Gigabit network link.\nFigure 9 shows the processors used as computed by the max\nload of the Linux topcommand, average load of the Linux\nperf command, and the max load of the Linux pscommand.\ntopis a point in time measurement of utilization at the precise\nmoment of polling and was sampled at 1 second intervals. perf\nis an estimated average of the processor load throughout the\nprocess lifetime. psreports percentage of time spent runningduring the entire lifetime of the process and was sampled at 1\nsecond intervals. Combined these results indicate that the peak\nperformance achieved using 2 rings (see Figure 8) required\nonly a handful of cores.\nVI. C ONCLUSIONS AND FUTURE WORK\nFor many operating domains (land, sea, undersea, air, space,\n..,) long range detection is a cornerstone of defense. Long\nrange detection in the cyber domain requires signiﬁcant net-\nwork trafﬁc to be analyzed from a variety of observatories\nand outposts. Construction of anonymized hypersparse trafﬁc\nmatrices on edge network devices can be a key enabler by\nproviding signiﬁcant data compression in a rapidly analyz-\nable format that protects privacy. Constructing and analyzing\nanonymized hypersparse trafﬁc matrices are operations ideally\nsuited to the GraphBLAS high performance library. Using an\nAccolade Technologies edge network device the performance\nof the GraphBLAS is demonstrated on a near worse case\ntrafﬁc scenario using a continuous stream of CAIDA Telescope\ndarknet packets. The performance was explored by varying the\nnumber of trafﬁc ring buffers, which are proportional to the\nnumber of threads and processor cores used. Rates of over\n50,000,000 packets per second for constructing anonymized\nhypersparse trafﬁc matrices were achieved which exceeds a\ntypical 400 Gigabit network link.\nThis performance demonstrates that anonymized hyper-\nsparse trafﬁc matrices are readily computable on edge network\ndevices with minimal compute resources and can be a viable\ndata product for such devices. This work suggests a variety of\nfuture directions that could be pursued (1) exploring additional\nnetwork cards; (2) develpoing the appropriate key management\narchitecture for multiple observatories and outposts; (3) analy-\nsis of spatial temporal patterns in anonymized trafﬁc matrices\nto identify adversarial activities; (4) cross-correlation of data\nfrom different observatories and outposts; (5) development\nof AI algorithms for classiﬁcation of background trafﬁc; (6)\ncreation of underlying models of trafﬁc.\nACKNOWLEDGMENTS\nThe authors wish to acknowledge the following individu-\nals for their contributions and support: Bob Bond, Stephen\nBuckley, Ronisha Carter, Cary Conrad, Alan Edelman, Tucker\nHamilton, Jeff Gottschalk, Nathan Frey, Chris Hill, Mike\nKanaan, Tim Kraska, Andrew Morris, Charles Leiserson, Dave\nMartinez, Mimi McClure, Joseph McDonald, Sandy Pentland,\nChristian Prothmann, John Radovan, Steve Rejto, Daniela Rus,\nMatthew Weiss, Marc Zissman.\nREFERENCES\n[1] “ Cisco Visual Networking Index: Forecast and Trends .”\nhttps://newsroom.cisco.com/press-release-content?articleId=1955935.\n[2] “ Cisco Visual Networking Index: Forecast and Trends, 2018–2023 .”\nhttps://www.cisco.com/c/en/us/solutions/collateral/executive-\nperspectives/annual-internet-report/white-paper-c11-741490.html.\n[3] W. P. Delaney, Perspectives on Defense Systems Analysis . MIT Press,\n2015.\n\n[4] S. Topouzi, A. Sarris, Y . Pikoulas, S. Mertikas, X. Frantzis, and\nA. Giourou, “Ancient mantineia’s defence network reconsidered through\na gis approach,” BAR INTERNATIONAL SERIES , vol. 1016, pp. 559–\n566, 2002.\n[5] Y . Shu and Y . He, “Research on the historical and cultural value of and\nprotection strategy for rammed earth watchtower houses in chongqing,\nchina,” Built Heritage , vol. 5, no. 1, pp. 1–16, 2021.\n[6] R. Cacciotti, “The ’guardian of the pontiﬁcal state’: structural assessment\nof a damaged coastal watchtower in south lazio,” Master’s thesis,\nUniversidade do Minho, 2010.\n[7] R. A. Watson-Watt, Three Steps to Victory: A Personal Account by\nRadar’s Greatest Pioneer . London: Odhams Press, 1957.\n[8] W. P. Delaney, “Air defense of the united states: Strategic missions and\nmodern technology,” International Security , vol. 15, no. 1, pp. 181–211,\n1990.\n[9] J. Geul, E. Mooij, and R. Noomen, “Modelling and assessment of the\ncurrent and future space surveillance network,” 7th ECSD , 2017.\n[10] K. W. O’Haver, C. K. Barker, G. D. Dockery, and J. D. Huffaker, “Radar\ndevelopment for air and missile defense,” Johns Hopkins APL Tech.\nDigest , vol. 34, no. 2, pp. 140–153, 2018.\n[11] “ CAIDA Anonymized Internet Traces Dataset (April 2008 - January\n2019) .” https://www.caida.org/catalog/datasets/passive dataset/.\n[12] “ UCSD Network Telescope .” https://www.caida.org/projects/network telescope/.\n[13] “ Global Cyber Alliance .” https://www.globalcyberalliance.org/.\n[14] “ Greynoise .” https://greynoise.io/.\n[15] “ MAWI Working Group Trafﬁc Archive .” http://mawi.wide.ad.jp/mawi/.\n[16] “ Shadowserver Foundation .” https://www.shadowserver.org/.\n[17] J. Kepner, C. Meiners, C. Byun, S. McGuire, T. Davis, W. Arcand,\nJ. Bernays, D. Bestor, W. Bergeron, V . Gadepally, R. Harnasch,\nM. Hubbell, M. Houle, M. Jones, A. Kirby, A. Klein, L. Milechin,\nJ. Mullen, A. Prout, A. Reuther, A. Rosa, S. Samsi, D. Stetson, A. Tse,\nC. Yee, and P. Michaleas, “Multi-temporal analysis and scaling relations\nof 100,000,000,000 network packets,” in 2020 IEEE High Performance\nExtreme Computing Conference (HPEC) , pp. 1–6, 2020.\n[18] K. Claffy, “Measuring the internet,” IEEE Internet Computing , vol. 4,\nno. 1, pp. 73–75, 2000.\n[19] B. Li, J. Springer, G. Bebis, and M. H. Gunes, “A survey of network ﬂow\napplications,” Journal of Network and Computer Applications , vol. 36,\nno. 2, pp. 567–581, 2013.\n[20] M. Rabinovich and M. Allman, “Measuring the internet,” IEEE Internet\nComputing , vol. 20, no. 4, pp. 6–8, 2016.\n[21] k. claffy and D. Clark, “Workshop on internet economics (wie 2019)\nreport,” SIGCOMM Comput. Commun. Rev. , vol. 50, p. 53–59, May\n2020.\n[22] J. Kepner, J. Bernays, S. Buckley, K. Cho, C. Conrad, L. Daigle,\nK. Erhardt, V . Gadepally, B. Greene, M. Jones, R. Knake, B. Maggs,\nP. Michaleas, C. Meiners, A. Morris, A. Pentland, S. Pisharody,\nS. Powazek, A. Prout, P. Reiner, K. Suzuki, K. Takhashi, T. Tauber,\nL. Walker, and D. Stetson, “Zero botnets: An observe-pursue-counter\napproach.” Belfer Center Reports, 6 2021.\n[23] S. Weed, “Beyond zero trust: Reclaiming blue cyberspace,” Master’s\nthesis, United States Army War College, 2022.\n[24] J. Kepner and J. Gilbert, Graph algorithms in the language of linear\nalgebra . SIAM, 2011.\n[25] J. Kepner, D. Bader, A. Buluc ¸, J. Gilbert, T. Mattson, and H. Mey-\nerhenke, “Graphs, matrices, and the graphblas: Seven good reasons,”\nProcedia Computer Science , vol. 51, pp. 2453–2462, 2015.\n[26] J. Kepner, P. Aaltonen, D. Bader, A. Buluc ¸, F. Franchetti, J. Gilbert,\nD. Hutchison, M. Kumar, A. Lumsdaine, H. Meyerhenke, S. McMillan,\nC. Yang, J. D. Owens, M. Zalewski, T. Mattson, and J. Moreira, “Math-\nematical foundations of the graphblas,” in 2016 IEEE High Performance\nExtreme Computing Conference (HPEC) , pp. 1–9, 2016.\n[27] A. Buluc ¸, T. Mattson, S. McMillan, J. Moreira, and C. Yang, “Design\nof the graphblas api for c,” in 2017 IEEE International Parallel and\nDistributed Processing Symposium Workshops (IPDPSW) , pp. 643–652,\n2017.\n[28] J. Kepner, M. Kumar, J. Moreira, P. Pattnaik, M. Serrano, and H. Tufo,\n“Enabling massive deep neural networks with the graphblas,” in 2017\nIEEE High Performance Extreme Computing Conference (HPEC) ,\npp. 1–10, IEEE, 2017.\n[29] C. Yang, A. Buluc ¸, and J. D. Owens, “Implementing push-pull efﬁciently\nin graphblas,” in Proceedings of the 47th International Conference on\nParallel Processing , pp. 1–11, 2018.[30] T. A. Davis, “Algorithm 1000: Suitesparse:graphblas: Graph algorithms\nin the language of sparse linear algebra,” ACM Trans. Math. Softw. ,\nvol. 45, Dec. 2019.\n[31] J. Kepner and H. Jananthan, Mathematics of big data: Spreadsheets,\ndatabases, matrices, and graphs . MIT Press, 2018.\n[32] T. A. Davis, “Algorithm 1000: Suitesparse: Graphblas: Graph algorithms\nin the language of sparse linear algebra,” ACM Transactions on Mathe-\nmatical Software (TOMS) , vol. 45, no. 4, pp. 1–25, 2019.\n[33] T. Mattson, T. A. Davis, M. Kumar, A. Buluc, S. McMillan, J. Moreira,\nand C. Yang, “Lagraph: A community effort to collect graph algorithms\nbuilt on top of the graphblas,” in 2019 IEEE International Parallel and\nDistributed Processing Symposium Workshops (IPDPSW) , pp. 276–284,\nIEEE, 2019.\n[34] P. Cailliau, T. Davis, V . Gadepally, J. Kepner, R. Lipman, J. Lovitz, and\nK. Ouaknine, “Redisgraph graphblas enabled graph database,” in 2019\nIEEE International Parallel and Distributed Processing Symposium\nWorkshops (IPDPSW) , pp. 285–286, IEEE, 2019.\n[35] T. A. Davis, M. Aznaveh, and S. Kolodziej, “Write quick, run fast:\nSparse deep neural network in 20 minutes of development time via\nsuitesparse: Graphblas,” in 2019 IEEE High Performance Extreme\nComputing Conference (HPEC) , pp. 1–6, IEEE, 2019.\n[36] M. Aznaveh, J. Chen, T. A. Davis, B. Hegyi, S. P. Kolodziej, T. G.\nMattson, and G. Sz ´arnyas, “Parallel graphblas with openmp,” in 2020\nProceedings of the SIAM Workshop on Combinatorial Scientiﬁc Com-\nputing , pp. 138–148, SIAM, 2020.\n[37] B. Brock, A. Buluc ¸, T. G. Mattson, S. McMillan, and J. E. Moreira,\n“Introduction to graphblas 2.0,” in 2021 IEEE International Parallel and\nDistributed Processing Symposium Workshops (IPDPSW) , pp. 253–262,\nIEEE, 2021.\n[38] M. Pelletier, W. Kimmerer, T. A. Davis, and T. G. Mattson, “The\ngraphblas in julia and python: the pagerank and triangle centralities,” in\n2021 IEEE High Performance Extreme Computing Conference (HPEC) ,\npp. 1–7, 2021.\n[39] J. Kepner, T. Davis, C. Byun, W. Arcand, D. Bestor, W. Bergeron,\nV . Gadepally, M. Hubbell, M. Houle, M. Jones, A. Klein, P. Michaleas,\nL. Milechin, J. Mullen, A. Prout, A. Rosa, S. Samsi, C. Yee, and\nA. Reuther, “75,000,000,000 streaming inserts/second using hierarchical\nhypersparse graphblas matrices,” in 2020 IEEE International Parallel\nand Distributed Processing Symposium Workshops (IPDPSW) , pp. 207–\n210, 2020.\n[40] J. Kepner, M. Jones, D. Andersen, A. Buluc ¸, C. Byun, K. Claffy,\nT. Davis, W. Arcand, J. Bernays, D. Bestor, W. Bergeron, V . Gadepally,\nM. Houle, M. Hubbell, A. Klein, C. Meiners, L. Milechin, J. Mullen,\nS. Pisharody, A. Prout, A. Reuther, A. Rosa, S. Samsi, D. Stetson,\nA. Tse, C. Yee, and P. Michaleas, “Spatial temporal analysis of\n40,000,000,000,000 internet darkspace packets,” in 2021 IEEE High\nPerformance Extreme Computing Conference (HPEC) , pp. 1–8, 2021.\n[41] J. Kepner, K. Cho, K. Claffy, V . Gadepally, P. Michaleas, and\nL. Milechin, “Hypersparse neural network analysis of large-scale internet\ntrafﬁc,” in 2019 IEEE High Performance Extreme Computing Confer-\nence (HPEC) , pp. 1–11, 2019.\n[42] J. Kepner, K. Cho, K. Claffy, V . Gadepally, S. McGuire, L. Milechin,\nW. Arcand, D. Bestor, W. Bergeron, C. Byun, M. Hubbell, M. Houle,\nM. Jones, A. Prout, A. Reuther, A. Rosa, S. Samsi, C. Yee, and\nP. Michaleas, “New phenomena in large-scale internet trafﬁc,” in Mas-\nsive Graph Analytics (D. Bader, ed.), pp. 1–53, Chapman and Hall/CRC,\n2022.\n[43] P. Devlin, J. Kepner, A. Luo, and E. Meger, “Hybrid power-law models\nof network trafﬁc,” arXiv preprint arXiv:2103.15928 , 2021.\n[44] A. Tumeo, O. Villa, and D. Sciuto, “Efﬁcient pattern matching on\ngpus for intrusion detection systems,” in Proceedings of the 7th ACM\nInternational Conference on Computing Frontiers , CF ’10, (New York,\nNY , USA), p. 87–88, Association for Computing Machinery, 2010.\n[45] M. Kumar, W. P. Horn, J. Kepner, J. E. Moreira, and P. Pattnaik,\n“Ibm power9 and cognitive computing,” IBM Journal of Research and\nDevelopment , vol. 62, no. 4/5, pp. 10–1, 2018.\n[46] J. Ezick, T. Henretty, M. Baskaran, R. Lethin, J. Feo, T.-C. Tuan, C. Co-\nley, L. Leonard, R. Agrawal, B. Parsons, and W. Glodek, “Combining\ntensor decompositions and graph analytics to provide cyber situational\nawareness at hpc scale,” in 2019 IEEE High Performance Extreme\nComputing Conference (HPEC) , pp. 1–7, 2019.\n[47] P. Gera, H. Kim, P. Sao, H. Kim, and D. Bader, “Traversing large graphs\non gpus with uniﬁed memory,” Proceedings of the VLDB Endowment ,\nvol. 13, no. 7, pp. 1119–1133, 2020.\n\n[48] A. Azad, M. M. Aznaveh, S. Beamer, M. Blanco, J. Chen,\nL. D’Alessandro, R. Dathathri, T. Davis, K. Deweese, J. Firoz, H. A.\nGabb, G. Gill, B. Hegyi, S. Kolodziej, T. M. Low, A. Lumsdaine,\nT. Manlaibaatar, T. G. Mattson, S. McMillan, R. Peri, K. Pingali,\nU. Sridhar, G. Szarnyas, Y . Zhang, and Y . Zhang, “Evaluation of\ngraph analytics frameworks using the gap benchmark suite,” in 2020\nIEEE International Symposium on Workload Characterization (IISWC) ,\npp. 216–227, 2020.\n[49] Z. Du, O. A. Rodriguez, J. Patchett, and D. A. Bader, “Interactive graph\nstream analytics in arkouda,” Algorithms , vol. 14, no. 8, p. 221, 2021.\n[50] S. Acer, A. Azad, E. G. Boman, A. Buluc ¸, K. D. Devine, S. Ferdous,\nN. Gawande, S. Ghosh, M. Halappanavar, A. Kalyanaraman, A. Khan,\nM. Minutoli, A. Pothen, S. Rajamanickam, O. Selvitopi, N. R. Tal-\nlent, and A. Tumeo, “Exagraph: Graph and combinatorial methods\nfor enabling exascale applications,” The International Journal of High\nPerformance Computing Applications , vol. 35, no. 6, pp. 553–571, 2021.\n[51] M. P. Blanco, S. McMillan, and T. M. Low, “Delayed asynchronous\niterative graph algorithms,” in 2021 IEEE High Performance Extreme\nComputing Conference (HPEC) , pp. 1–7, IEEE, 2021.\n[52] N. K. Ahmed, N. Dufﬁeld, and R. A. Rossi, “Online sampling of\ntemporal networks,” ACM Transactions on Knowledge Discovery from\nData (TKDD) , vol. 15, no. 4, pp. 1–27, 2021.\n[53] A. Azad, O. Selvitopi, M. T. Hussain, J. R. Gilbert, and A. Buluc ¸, “Com-\nbinatorial blas 2.0: Scaling combinatorial algorithms on distributed-\nmemory systems,” IEEE Transactions on Parallel and Distributed Sys-\ntems, vol. 33, no. 4, pp. 989–1001, 2021.\n[54] D. Koutra, “The power of summarization in graph mining and learning:\nsmaller data, faster methods, more interpretability,” Proceedings of the\nVLDB Endowment , vol. 14, no. 13, pp. 3416–3416, 2021.\n[55] R. Hofstede, P. ˇCeleda, B. Trammell, I. Drago, R. Sadre, A. Sperotto,\nand A. Pras, “Flow monitoring explained: From packet capture to data\nanalysis with netﬂow and ipﬁx,” IEEE Communications Surveys &\nTutorials , vol. 16, no. 4, pp. 2037–2064, 2014.\n[56] R. Sommer, “Bro: An open source network intrusion detection system,”\nSecurity, E-learning, E-Services, 17. DFN-Arbeitstagung ¨uber Kommu-\nnikationsnetze , 2003.\n[57] P. Lucente, “pmacct: steps forward interface counters,” Tech. Rep. , 2008.\n[58] J. Fan, J. Xu, M. H. Ammar, and S. B. Moon, “Preﬁx-preserving ip\naddress anonymization: measurement-based security evaluation and a\nnew cryptography-based scheme,” Computer Networks , vol. 46, no. 2,\npp. 253–272, 2004.\n[59] J. Karvanen and A. Cichocki, “Measuring sparseness of noisy signals,”\nin4th International Symposium on Independent Component Analysis\nand Blind Signal Separation , pp. 125–130, 2003.\n[60] A. Soule, A. Nucci, R. Cruz, E. Leonardi, and N. Taft, “How to\nidentify and estimate the largest trafﬁc matrix elements in a dynamic\nenvironment,” in ACM SIGMETRICS Performance Evaluation Review ,\nvol. 32, pp. 73–84, ACM, 2004.\n[61] Y . Zhang, M. Roughan, C. Lund, and D. L. Donoho, “Estimating point-\nto-point and point-to-multipoint trafﬁc matrices: an information-theoretic\napproach,” IEEE/ACM Transactions on Networking (TON) , vol. 13,\nno. 5, pp. 947–960, 2005.\n[62] P. J. Mucha, T. Richardson, K. Macon, M. A. Porter, and J.-P. Onnela,\n“Community structure in time-dependent, multiscale, and multiplex\nnetworks,” science , vol. 328, no. 5980, pp. 876–878, 2010.\n[63] P. Tune, M. Roughan, H. Haddadi, and O. Bonaventure, “Internet trafﬁc\nmatrices: A primer,” Recent Advances in Networking , vol. 1, pp. 1–56,\n2013.\n[64] J. Kepner, V . Gadepally, L. Milechin, S. Samsi, W. Arcand, D. Bestor,\nW. Bergeron, C. Byun, M. Hubbell, M. Houle, M. Jones, A. Klein,\nP. Michaleas, J. Mullen, A. Prout, A. Rosa, C. Yee, and A. Reuther,\n“Streaming 1.9 billion hypersparse network updates per second with\nd4m,” in 2019 IEEE High Performance Extreme Computing Conference\n(HPEC) , pp. 1–6, 2019.\n[65] J. Nair, A. Wierman, and B. Zwart, “The fundamentals of heavy tails:\nProperties, emergence, and estimation,” Preprint, California Institute of\nTechnology , 2020.\n[66] A. J. Elmore, J. Duggan, M. Stonebraker, M. Balazinska, U. Cetintemel,\nV . Gadepally, J. Heer, B. Howe, J. Kepner, T. Kraska, et al. , “A\ndemonstration of the bigdawg polystore system,” Proceedings of the\nVLDB Endowment , vol. 8, no. 12, p. 1908, 2015.\n[67] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The case\nfor learned index structures,” in Proceedings of the 2018 InternationalConference on Management of Data , SIGMOD 18, (New York, NY ,\nUSA), pp. 489–504, Association for Computing Machinery, 2018.\n[68] E. H. Do and V . N. Gadepally, “Classifying anomalies for network\nsecurity,” in ICASSP 2020 - 2020 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) , pp. 2907–2911,\n2020.\n[69] S. Pisharody, J. Bernays, V . Gadepally, M. Jones, J. Kepner, C. Meiners,\nP. Michaleas, A. Tse, and D. Stetson, “Realizing forward defense in the\ncyber domain,” in 2021 IEEE High Performance Extreme Computing\nConference (HPEC) , pp. 1–7, IEEE, 2021.\n[70] B. Nichols, D. Buttlar, and J. P. Farrell, Pthreads programming . O’Reilly\n& Associates, Inc., 1996.",
  "textLength": 42853
}