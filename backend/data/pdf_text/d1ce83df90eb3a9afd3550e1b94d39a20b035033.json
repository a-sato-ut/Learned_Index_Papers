{
  "paperId": "d1ce83df90eb3a9afd3550e1b94d39a20b035033",
  "title": "Learned Indexes with Distribution Smoothing via Virtual Points",
  "pdfPath": "d1ce83df90eb3a9afd3550e1b94d39a20b035033.pdf",
  "text": "Learned Indexes with Distribution Smoothing via Virtual Points\nKasun Amarasinghe\nThe University of Melbourne\nMelbourne, Australia\nkasun.amarasinghe@student.unimelb.edu.auFarhana Choudhury\nThe University of Melbourne\nMelbourne, Australia\nfarhana.choudhury@unimelb.edu.au\nJianzhong Qi\nThe University of Melbourne\nMelbourne, Australia\njianzhong.qi@unimelb.edu.auJames Bailey\nThe University of Melbourne\nMelbourne, Australia\nbaileyj@unimelb.edu.au\nAbstract\nRecent research on learned indexes has created a new perspective\nfor indexes as models that map keys to their respective storage\nlocations. These learned indexes are created to approximate the\ncumulative distribution function of the key set, where using only\na single model may have limited accuracy. To overcome this lim-\nitation, a typical method is to use multiple models, arranged in a\nhierarchical manner, where the query performance depends on two\naspects: (i) traversal time to find the correct model and (ii) search\ntime to find the key in the selected model. Such a method may cause\nsome key space regions that are difficult to model to be placed at\ndeeper levels in the hierarchy. To address this issue, we propose\nan alternative method that modifies the key space as opposed to\nany structural or model modifications. This is achieved through\nmaking the key set more learnable (i.e., smoothing the distribution)\nby inserting virtual points. Furthermore, we develop an algorithm\nnamed CSV to integrate our virtual point insertion method into\nexisting learned indexes, reducing both their traversal and search\ntime. We implement CSV on state-of-the-art learned indexes and\nevaluate them on real-world datasets. Extensive experimental re-\nsults show significant query performance improvement for the keys\nin deeper levels of the index structures at a low storage cost.\nCCS Concepts\nâ€¢Information systems â†’Data access methods .\nKeywords\nLearned indexes, Distribution smoothing, Index optimisation\nACM Reference Format:\nKasun Amarasinghe, Farhana Choudhury, Jianzhong Qi, and James Bailey.\n2025. Learned Indexes with Distribution Smoothing via Virtual Points\n1 Introduction\nLearned indexes [ 12] have reported strong query performance and\nare attracting much attention from both the academia and industry\nin recent years. The core idea of learned indexes is that an index\nstructure can be seen as a mapping function ğ‘“(Â·)from a search key\nArvix, Oct, 2025\nÂ©2025 Copyright held by the owner/author(s).\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM\nhttps://doi.org/XXXXXXX.XXXXXXX\nFacebook Covid OSM Genome\n1 2 3 4 5 6 7\nLevel100200Average runtime\n per query (ns)\nFigure 1: Query time at each level of the LIPP index for four\nreal datasets, each with 200 million keys.\nğ‘˜ğ‘–to the storage location (i.e., the rank ğ‘Ÿğ‘ğ‘›ğ‘˜(ğ‘˜ğ‘–)) of the correspond-\ning data record: ğ‘Ÿğ‘ğ‘›ğ‘˜(ğ‘˜ğ‘–)â‰ˆğ‘“(ğ‘˜ğ‘–). The mapping function (a.k.a.\nindexing function ) is learned and approximated by machine learn-\ning algorithms (models). To enable the learning, a storage ordering\nneeds to be established. Typically, an ascending order based on the\nsearch keys is used, such that the mapping function is effectively\nthecumulative distribution function (CDF) of the search keys.\nDifferent learned indexes have been proposed [ 2â€“4,6,9,13â€“\n15,19,22,23,27,29,31,33,37], with a common theme to design\nindexing functions and structures that enable better approximation\nof the CDF, since approximation errors translate to mapping errors\n(i.e., difference between ğ‘Ÿğ‘ğ‘›ğ‘˜(ğ‘˜ğ‘–)andğ‘“(ğ‘˜ğ‘–)) and hence extra search\ncosts to examine data records at around ğ‘“(ğ‘˜ğ‘–)and recover from the\nerrors. However, such approaches mean the use of either complex\nindexing functions (e.g., splines [ 10,21]) or piece-wise functions\nwith many segments [ 6,7,14], both of which could lead to sub-\noptimal query efficiency. This is illustrated by Fig. 1 with LIPP [ 33]\nâ€“ one of the latest learned indexes. The index has a hierarchical\nstructure built in a top-down manner. When an index model (i.e., a\nnode in the index) cannot achieve an overall low mapping error for\nall keys assigned to it for indexing, sub-index models are created\nrecursively as a hierarchy, to accommodate for the â€œmore difficult to\nlearnâ€ keys. As Fig. 1 shows, keys indexed in deeper levels (higher\nlevels in the figure) reported higher query times on average, on all\nfour datasets.\nIn this paper, we approach the problem from an alternate per-\nspective â€“ we adjust the CDF such that it becomes easier to be\napproximated by the indexing functions, to achieve lower approxi-\nmation errors and higher query efficiency.\nOur core idea is to add virtual points to â€œsmoothâ€ the CDF of a\ndataset. Take Fig. 2a as an example, where each black dot representsarXiv:2408.06134v3  [cs.DB]  15 Dec 2024\n\nArvix, Oct, 2025 Kasun Amarasinghe, Farhana Choudhury, Jianzhong Qi, and James Bailey\n051015202530\nKeys0246810121416Rank\nk1k2Loss value: 8.33\n(a) Before smoothing\n051015202530\nKeys0246810121416Rankkv1kv2\nkv3\nkv4kv5\nk1k2Loss value: 2.04 (b) After smoothing\nFigure 2: Indexing data points (keys) with CDF smoothing.\na data point (i.e., its search key). Approximating the CDF of the\ndataset with a linear function can result in a large approximation\nerror (and hence high search costs at query time) for keys ğ‘˜1and\nğ‘˜2. We sum up the squared prediction error of every point:\nLğ‘“(ğ¾)=ğ‘›âˆ‘ï¸\nğ‘–=1\u0000ğ‘“(ğ‘˜ğ‘–)âˆ’ğ‘Ÿğ‘ğ‘›ğ‘˜(ğ‘˜ğ‘–)\u00012, (1)\nwhereğ¾denotes the set of keys and ğ‘›is its size,ğ‘˜ğ‘–âˆˆğ¾is a key,\nğ‘Ÿğ‘ğ‘›ğ‘˜(ğ‘˜ğ‘–)is its rank, and ğ‘“(Â·)is the indexing function. We refer to\nLğ‘“(ğ¾)as the loss function , for which we use the sum of squared\nerrors (SSE). In this case, Lğ‘“(ğ¾)=8.33â€“ a large value ofLğ‘“(ğ¾)\nsuggests worse prediction accuracy using ğ‘“for search key mapping\nand hence higher query times.\nAs Fig. 2b shows, we can add virtual points ğ‘‰={ğ‘˜ğ‘£1,ğ‘˜ğ‘£2,...,ğ‘˜ğ‘£5}\nrepresented by the red hollow dots. Here, we assume a smoothing\nbudget of0.5ğ‘›=5, i.e., 5 virtual points are allowed. Now the orig-\ninal data points are spread out, and the CDF of the (original and\nvirtual) points is closer to a straight line. We refit the points with\na new indexing function ğ‘“â€², with the loss function value Lğ‘“â€²(ğ¾)\nbeing reduced to 2.04 (and Lğ‘“â€²(ğ¾âˆªğ‘‰)= 2.29).\nWe show that, given a smoothing budget that constrains the\nadditional space costs, finding the optimal placement of the vir-\ntual points to minimise the loss function Lğ‘“â€²(ğ¾)is NP-hard. We\nthen propose approximation solutions for two generic scenarios:\n(1) smoothing the CDF for index learning with a single indexing\nfunction and (2) smoothing the CDF for a hierarchy of indexing\nfunctions, which is a common structure for existing learned in-\ndexes. We focus on linear indexing functions for their efficiency,\nalthough CDF smoothing can naturally extend to more complex\n(e.g., quadratic) functions.\nWe propose an algorithm named CDFsmoothing via virtual points\n(CSV) to smooth the CDF for optimising hierarchical learned in-\ndexes, with the aim to reduce the overall height of the structures as\nwell as the prediction errors of each indexing function, and hence\nthe query costs. This is performed by collecting sub-trees of the\nhierarchical structure and smoothing the CDF of the keys in them.\nAs a result, the keys could now be placed into a single node due to\nthe higher learnability, provided it surpasses a cost model threshold\nvalue. Here, the cost model is used to balance the reduction in index\ntraversal time and the potential increase in the leaf-node search\ntime due to the increase of keys in a node.\nIt is important to note that our aim is notto propose yet another\nlearned index but rather a technique that can be integrated with\nexisting or emerging hierarchical learned indexes to optimise theirquery efficiency with controllable extra space. To show the appli-\ncability of our CSV algorithm, we integrate it with three recent\nlearned indexes ALEX [ 2], LIPP [ 33], and SALI [ 9], which are the\nstate-of-the-art (SOTA).\nTo summarize, this paper makes the following contributions:\n(1) We propose a key space transformation technique using CDF\nsmoothing via inserting virtual points to enhance index learnability.\n(2) We propose an efficient algorithm named CSV to integrate\nthe CDF smoothing technique with hierarchical learned indexes to\nimprove the query performance, with a controllable space overhead.\n(3) We integrate CSV with three learned indexes ALEX, LIPP,\nand SALI, and we conduct experiments with four real datasets. The\nexperimental results show that the learned indexes powered by\nCSV manage to promote up to 60% of the keys in lower levels to\nupper levels, resulting in up to 34% improvement of their query\ntime, with less than 15% increase to the storage space overhead.\n2 Related Work\nWe first review learned indexes in general. Then, we focus on studies\naddressing complex distributions, which share a similar goal with\nus. We also cover a technique called the poisoning attacks, which\nmotivates our CDF smoothing technique.\n2.1 Learned Indexes\nLearned indexes are a trending topic in the database community [ 2,\n4,6,10,12,13,19,33,35,37]. Their key idea is to treat indexes\nas functions that map a search key to the storage position of the\ncorresponding data object, which can be learned with machine\nlearning models. A common approach is to lay out the data objects\nby ascending order of their search keys, such that the indexing\nfunctions are effectively (approximations of) CDFs of the keys.\nTo index large datasets, multiple indexing functions are used,\ntypically organized in a hierarchical structure like a B-tree. The\nlookup performance of such a structure is then dominated by two\nsteps: (1) the traversal time to find the leaf-node (every node cor-\nresponds to an indexing function) indexing the search key, and\n(2) the search time within the selected leaf-node ( leaf-node search\ntime hereafter to distinguish from the traversal time) to locate the\ntarget data object, as the indexing functions have errors and may\nnot produce the exact storage position of the search target [ 28,32].\nIt is a challenge to balance the query costs from the two steps\nabove. While a deeper structure with more indexing functions may\nfit the data distribution better and have lower leaf-node search\ntimes, it may also have higher traversal times and larger index\nsizes [ 8,34]. Some studies impose a maximum error bound on\nthe indexing functions to reduce the leaf-node search times [ 6,7],\nalso at the cost of more indexing functions. Another approach\nis to use more complex indexing functions (as opposed to linear\nones) [ 10,12,29], e.g., splines, which could better fit the CDFs.\nThe issue with this approach is the higher inference time for the\nfunction, and hence higher query and insertion times [28].\nThese studies design structures and indexing functions to better\nfit the data distribution. We address the challenge from an alternate\nperspective, i.e., we adjust the data distribution such that it is easier\nto be fitted by the indexing functions.\n\nLearned Indexes with Distribution Smoothing via Virtual Points Arvix, Oct, 2025\n2.2 Addressing Complex Data Distributions\nTo better index CDFs of complex data distributions, there are two\ncommon approaches. One is to use more complex functions such\nas splines and piece-wise linear regression models [ 6,10]. The\nother is to use better data partitioning strategies for easier CDF\nlearning over each partition, such as by CARMI [ 36] and EWALI [ 17].\nAnother study, LER [ 5], uses logarithmic error-based loss functions\n(instead of the more commonly used least squared error-based) to\nimprove the learning of index models that better fit the CDF.\nA latest development, SALI [ 9], identifies the most frequently\naccessed nodes via probability models given a query workload.\nThe corresponding sub-trees are flattened using a segmentation\napproach, similar to the PGM index [ 6], to reduce their traversal\ntime. However, this leads to an additional search step for queries,\nas we need to find the correct node from the flattened structure.\nA couple of studies [ 16,34] transform the input key set into a\nmore uniform distribution to improve the CDF learnability. The\nNFL index [ 34] transforms the key distribution using a numerical\nnormalizing flow that transforms a latent distribution to a new\ndistribution via generative models. The distribution transformation\nintroduces overheads, while queries also need to be transformed\nto use the index. Further, the transformation may increase the\ntail conflict degree for certain distributions, making it unsuitable\nin those instances. The gap insertion [16] technique inserts gaps\nbetween the keys (i.e., storage positions of the corresponding data\nobjects) to straighten the CDF of the keys, thereby improving its\nlearnability. However, this is performed by manipulating the rank\nof each key, and as a result, multiple keys could be given the same\nposition. An extra array is used to house such conflicting keys,\nwhich in turn introduces search overheads to locate the correct key.\nThis method leads to a heavy storage space increase of up to 87%.\nSeveral learned indexes [ 2,19,33] leave gaps in their storage\nstructure (i.e., gapped arrays). While their purpose is to accommo-\ndate data insertions, a side effect is changing the data distribution,\nwhich is what we do. A core difference to note is that, they do not\nconsider minimizing the indexesâ€™ model prediction errors when\nadding gaps, in contrast to our approach which does.\n2.3 Poisoning CDFs\nOur idea of adjusting data distribution to fit the indexing functions\nis rooted from data poisoning â€“ a process of manipulating the train-\ning data to change the results from a predictive model [ 11]. Data\npoisoning has been introduced into learned indexes to poison the\nindexing functions and negatively impact their capability to approx-\nimate the CDFs [ 11]. The main goal of this process is to identify\nnew points to include into the original key set that would cause\nthe maximum increase to the loss function value (i.e., the SSE).\nMotivated by the poisoning technique, we propose a technique\nthat smooths the data distribution by adding virtual points, to obtain\nCDFs that are easier to be approximated by indexing functions\n(models), hence leading to a structure with higher query efficiency.\nSince the models are built with virtual points that can be used\nto host data insertions, a side benefit of our structure is that it is\nmore resilient against data insertions. Table 1 highlights the key\ndifference between our technique CSV, NFL, and gap insertion (GI).Table 1: Comparison with Existing Works\nCSV NFL [34] GI [16]\nNo extra transformation at query time âœ“ âœ— âœ“\nLow storage overhead âœ“ âœ“ âœ—\nIntegrable into other learned indexes âœ“ âœ“ âœ—\nRobust across different distributions âœ“ âœ— âœ“\n3 Preliminaries\nProblem statement. Consider a dataset ğ·ofğ‘›data records, where\neach record is associated with a one-dimensional value as its index\nkey. Letğ¾be the list of all index keys associated with ğ·, sorted in\nascending order.\nSuppose that the index keys have been partitioned and indexed\nby a setFofğ‘šindexing functions. Each indexing function ğ‘“ğ‘–âˆˆF\nhas some prediction error (a.k.a â€œlossâ€) for a key ğ‘˜âˆˆğ¾ğ‘–indexed\nby it. Here, ğ¾ğ‘–âŠ‚ğ¾refers to the subset of keys indexed by ğ‘“ğ‘–. The\nloss refers to the squared difference between the predicted index\npositionğ‘“ğ‘–(ğ‘˜)and the rank of ğ‘˜inğ¾, i.e.,ğ‘Ÿğ‘ğ‘›ğ‘˜(ğ‘˜). The sum of\nsquared errors (SSE) is one of the most commonly used metric to\nrepresent loss in the existing studies of learned indexes. Let LFbe\nthetotal sum of squared errors of all indexing functions in F:\nLF(ğ¾)=ğ‘šâˆ‘ï¸\nğ‘–=1âˆ‘ï¸\nğ‘˜âˆˆğ¾ğ‘–\u0000ğ‘“ğ‘–(ğ‘˜)âˆ’ğ‘Ÿğ‘ğ‘›ğ‘˜(ğ‘˜)\u00012. (2)\nEquation 2 is the loss function of our optimisation problem. We\naim to insert values ( virtual points ) intoğ¾while keeping it sorted,\nsuch thatLF(ğ¾)is minimised, i.e., to smooth the CDF ofğ¾.\nA naive optimal smoothing scheme is to insert as many virtual\npoints as needed such that every point ğ‘˜âˆˆğ¾ğ‘–lies at theğ‘“ğ‘–(ğ‘˜)-th\nposition (i.e., ğ‘Ÿğ‘ğ‘›ğ‘˜(ğ‘˜)=ğ‘“ğ‘–(ğ‘˜)) in the list (assuming unique integer\nkeys). This way, the loss becomes zero after smoothing. In reality,\nthis smoothing scheme is infeasible, due to the non-uniqueness of\nthe keys inğ¾and the potentially high space cost. For example, if\nthe keys are 64-bit integers, it will take 264Ã—8bytesâ‰ˆ128exabyte\n(i.e., 128Ã—106TB) to achieve such an index key layout.\nTo balance between the space overhead and the smoothness of\nthe CDF with inserted points, we consider a â€œsmoothing budgetâ€ ğœ†,\ni.e., the number of virtual points allowed to be inserted, such that\nthe loss is minimised given the constraint of ğœ†. We assume ğœ†=ğ›¼Â·ğ‘›\nwhere the smoothing threshold ğ›¼is in(0,1), to retain a linear space\noverhead. Formally, we aim to solve the following problem:\nDefinition 1. [Learned index smoothing ] Given a list of index\nkeysğ¾sorted in ascending order and partitioned into ğ‘šsegments,\neach of which is indexed by an indexing function ğ‘“ğ‘–âˆˆF, the learned\nindex smoothing problem aims to insert a set ğ‘‰(|ğ‘‰|â‰¤ğœ†) of virtual\npoints intoğ¾while keeping ğ¾in order, such that the loss as defined\nby Equation 2 is minimised.\nWe consider linear indexing functions as they are used in most\nexisting learned indexes. To simplify the discussion, we use integer\nindex keys, while our techniques also apply to real number index\nkeys when they can be scaled up to become integers.\n\nArvix, Oct, 2025 Kasun Amarasinghe, Farhana Choudhury, Jianzhong Qi, and James Bailey\nNP-hardness analysis. Solving the exact CDF smoothing prob-\nlem is NP-hard as it can be reduced from the Knapsack problem,\nwhich is a known NP-hard problem.\nLemma 3.1. Learned index CDF smoothing is NP-hard.\nProof. We reduce from the Knapsack problem which is NP-hard.\nThe Knapsack problem considers a set of items ğ‘†. Each itemğ‘ âˆˆğ‘†\nhas a value ğ‘ğ‘ and a weight ğ‘¤ğ‘ . The objective is to determine the\nsubsetğ´âŠ†ğ‘†that maximises the total value of the items in ğ´while\nthe total weight of the items is less than a given limit ğ‘¡.\nCDF smoothing considers a key set ğ¾of sizeğ‘›. We aim to find a\nsubsetğ‘‰(virtual points ğ‘˜ğ‘£ğ‘–) of at most size ğœ†from a candidate set\nğ¶that would minimise the loss L(which implies the maximisation\nof loss reduction from the loss without CDF smoothing). Naively,\nthe setğ¶can be formed by considering ğœ†virtual point candidates\nbetween every two adjacent keys in ğ¾, i.e.,|ğ¶|â‰¤ğœ†Â·(ğ‘›âˆ’1).\nTo reduce the Knapsack problem to our CDF smoothing problem,\nthe setğ‘†of items is mapped to the candidate set ğ¶of virtual points.\nWe set the weight of every item (a candidate virtual point) to 1 and\nletğ‘¡be our target number of virtual points to be added, i.e., ğœ†. The\nvalue of an item, ğ‘ğ‘ , is mapped to the loss reduction contributed\nby the corresponding virtual point. Maximising the values of the\nitems in subset ğ´is mapped to maximising the total loss reduction\nof the virtual points in ğ‘‰. Due to the nature of our problem, the\ntotal loss reduction when multiple virtual points are added together\nvaries from the sum of the loss reduction when the virtual points\nare added individually. This can be represented as:\n|âˆ‘ï¸\nğ‘ âˆˆğ´ğ‘ğ‘ |=ğ‘Ÿâˆ‘ï¸\nğ‘ âˆˆğ´|ğ‘ğ‘ |, (3)\nwhere|ğ‘ğ‘ |is the magnitude of the value of an item (a virtual point)\nandğ‘ŸâˆˆRis a parameter. Here, ğ‘Ÿis deterministic since it could\nbe calculated based on Equation 1. As such, the total value of the\ncombined items can be transformed to the sum of the individual\nitem values, and maximising the latter for the Knapsack problem\ncan be mapped to maximising the former for our problem.\nThe transformation between the two problems can be done in\npolynomial time since there is a one-to-one mapping between them.\nThus, when our problem is solved, the Knapsack problem is also\nsolved in polynomial time. As such, our problem is NP-hard. â–¡\nDue to the computational complexity in finding an exact optimal\nsolution for the learned index smoothing problem over a large set\nof keys, next, we consider two practical variants of the problem\nand propose highly effective heuristic solutions: (1) smoothing the\nCDF for the subset of keys ğ¾ğ‘–indexed by an indexing function ğ‘“ğ‘–\n(Section 4); and (2) smoothing the CDFs for all ğ‘šsubsetsğ¾ğ‘–when\nthey are indexed under a hierarchical learned index (Section 5).\n4 CDF Smoothing for a Single Linear Model\nWe first consider a single indexing function over a segment of keys\nğ¾ğ‘–. Given the smoothing budget ğœ†, our optimisation goal is:\nargminğ‘‰ğ‘–,ğ‘¤,ğ‘Lğ‘“ğ‘¤,ğ‘(ğ¾ğ‘–âˆªğ‘‰ğ‘–), ğ‘ .ğ‘¡.|ğ‘‰ğ‘–|â‰¤ğœ† (4)\nThis optimisation goal varies from Equation 2. Importantly, we now\nallow the slope ( ğ‘¤) and the intercept ( ğ‘) parameters of the indexing\nfunctionğ‘“to be refitted based on the keys ğ¾ğ‘–(with adjusted ranks)\nand the inserted virtual points ğ‘‰ğ‘–, rather than just inserting virtualpoints to adjust ğ‘Ÿğ‘ğ‘›ğ‘˜(ğ‘˜)forğ‘˜âˆˆğ¾ğ‘–to fitğ‘“(ğ‘˜)of the original index-\ning function ğ‘“. This way, intuitively, we could achieve a lower loss\nwith fewer virtual point insertions (hence reducing space overhead),\nas opposed to the naive optimal smoothing scheme described above\nthat simply spreads ğ¾ğ‘–to fitğ‘Ÿğ‘ğ‘›ğ‘˜(ğ‘˜)to a givenğ‘“(ğ‘˜).\nIn Equation 4, we include ğ‘‰ğ‘–in the loss calculation, such that\nthe storage space allocated to the virtual points can be used to\naccumulate data insertions, with minimized prediction errors when\nquerying the inserted data points.\nChallenges. Allowing to refit the slope and the intercept of\nthe indexing function makes the optimisation problem even more\ndifficult, as now the loss reduction brought by inserting a virtual\npoint depends further on the other virtual points to be inserted. To\nsolve the problem, a simple greedy heuristic is to iteratively select\nthe best remaining candidate virtual point that leads to the largest\nloss reduction and to refit the indexing function after each virtual\npoint selection. This process is repeated for ğœ†iterations to identify\nallğœ†virtual points to be inserted.\nThere are complexity issues with this simple heuristic.\nChallenge 1: Large size of the candidate set. Given keys ğ¾ğ‘–, the\ncandidate virtual points can be any key value in (min{ğ¾ğ‘–},max{ğ¾ğ‘–})\n(detailed in Section 4.2), which can be a large range in real datasets.\nChallenge 2: Repeated loss calculations. For each candidate vir-\ntual pointğ‘˜ğ‘£, we need to recalculate Lğ‘“(ğ¾ğ‘–âˆªğ‘‰ğ‘–), whereğ‘‰ğ‘–includes\nğ‘˜ğ‘£, to help select the best candidate virtual point that leads to the\nlargest loss reduction, which takes ğ‘‚(|ğ¾ğ‘–|+ğœ†)time. Forğœ†iterations\nandğ‘‚(ğ‘)candidate virtual points to be examined per iteration,\noverall, the greedy solution above takes ğ‘‚((|ğ¾ğ‘–|+ğœ†)Â·ğœ†Â·ğ‘)time.\nTo overcome these challenges, we present an efficient solution\nbelow to reduce the overall time complexity through (1) reducing\nthe number of candidate virtual points ğ‘and (2) the time cost to\ncalculate the loss for each candidate virtual point.\nOur solution. Our solution takes three steps:\nStep 1: We propose an effective approach to reduce the number\nof candidate virtual points. The idea is that, if there are consecutive\ncandidate virtual points, we only need to consider the point among\nthese consecutive points that minimises the loss function, which\ncan be identified utilising the first-order partial derivative of the\nloss function (addressing Challenge 1, detailed in Section 4.2).\nStep 2: We propose an efficient algorithm to calculate the loss.\nThe core idea is to calculate the loss incrementally, and to reuse\npart of the calculation from the previous iteration, as only one new\ncandidate virtual point is included into the calculation each time\n(addressing Challenge 2, detailed in Section 4.1).\nStep 3: Finally, from the reduced set of candidate virtual points,\nand with efficient loss calculation, we present an efficient algorithm\nto compute the best subset of virtual points of size ğœ†(Section 4.3).\nA running example on finding the best virtual point is shown\nin Fig. 3, which corresponds to the keys in Fig. 2a. In Fig. 3, each\nhollow dot represents a candidate virtual point. Its ğ‘¦-value repre-\nsents the loss (i.e., SSE) if the virtual point is included into the key\nset. Adjacent hollow dots are linked together, forming a segment of\ncandidate virtual point values. For example, the segment formed by\n21 to 25 is between index keys 20 and 26 (the index keys themselves\narenotconsidered to be candidate virtual points; they correspond\nto the gaps between the curve segments in Fig. 3). The goal is to\n\nLearned Indexes with Distribution Smoothing via Virtual Points Arvix, Oct, 2025\n5 10 15 20 25 30\nVirtual point values68101214Loss function valuekv1Original key set\nloss value\nFigure 3: Loss function (SSE) value corresponding to different\ninsertion positions for a virtual point.\nsearch for the virtual point with the smallest loss value. In the given\nexample, point 23 is the search target.\nIn what follows, we present the algorithm to efficiently calculate\nthe loss first (Section 4.1), based on which the strategy to reduce the\ncandidate virtual points is described (Section 4.2), and our overall al-\ngorithm to compute the best ğœ†virtual points is detailed (Section 4.3).\n4.1 Efficient Loss Calculation and Indexing\nFunction Refitting\nWe start with efficiently calculating the loss for each candidate\nvirtual point, where the idea is to reuse the calculations as much as\npossible. For the rest of Section 4, we abuse the notation slightly\nand useğ¾instead ofğ¾ğ‘–to denote the segment of keys for which\nCDF smoothing is to be done, since the discussion only concerns\na single segment. Similarly, we use ğ‘“instead ofğ‘“ğ‘–to denote the\nindexing function,Linstead ofLğ‘–to denote the loss function, and\nğ‘‰insteadğ‘‰ğ‘–to denote the candidate virtual points for the segment.\nLet the rank of a key ğ‘˜ğ‘–beğ‘¦ğ‘–(assuming that the ranks start\nfrom 0) for the ğ‘›keys inğ¾, i.e.,ğ‘¦ğ‘–=ğ‘Ÿğ‘ğ‘›ğ‘˜(ğ‘˜ğ‘–). First, consider a\nvirtual point ğ‘˜ğ‘£with its rank (after inserted into ğ¾) beingğ‘¦ğ‘£(we\nexplain the case for multiple virtual points later). The loss for ğ¾\nandğ‘‰={ğ‘˜ğ‘£}can be calculated as follows, where ğ‘¤andğ‘are the\nparameters of the indexing function ğ‘“:\nL({ğ¾âˆªğ‘‰})=ğ‘›âˆ‘ï¸\nğ‘–=1(ğ‘¤ğ‘˜ğ‘–+ğ‘âˆ’ğ‘¦ğ‘–)2+(ğ‘¤ğ‘˜ğ‘£+ğ‘âˆ’ğ‘¦ğ‘£)2(5)\nIt is computationally expensive to calculate the loss for each\ncandidate virtual point using Equation 5, as ğ‘¤andğ‘can change\nwhen the indexing function ğ‘“is refitted for the candidate virtual\npoint, which is governed by the following equations (derived from\nthe first-order partial derivatives of Equation 5):\nğ‘¤=Ãğ‘›+1\nğ‘–=1(ğ‘˜ğ‘–âˆ’Â¯ğ‘˜ğ‘£)(ğ‘¦ğ‘–âˆ’Â¯ğ‘¦ğ‘£)\nÃğ‘›+1\nğ‘–=1(ğ‘˜ğ‘–âˆ’Â¯ğ‘˜ğ‘£)2, (6)\nğ‘=Â¯ğ‘¦ğ‘£âˆ’ğ‘¤Â¯ğ‘˜ğ‘£. (7)\nHere, Â¯ğ‘˜ğ‘£and Â¯ğ‘¦ğ‘£are the mean of the key set (i.e., ğ¾âˆªğ‘‰) and the\nrank set after inserting the virtual point (ğ‘˜ğ‘£,ğ‘¦ğ‘£), respectively. They\ncan be computed by Equations 8 and 9 as follows:\nÂ¯ğ‘˜ğ‘£=Ãğ‘›\nğ‘–=1ğ‘˜ğ‘–+ğ‘˜ğ‘£\nğ‘›+1, (8)Â¯ğ‘¦ğ‘£=Ãğ‘›\nğ‘–=1ğ‘¦ğ‘–+ğ‘›\nğ‘›+1. (9)\nNext, we rewrite Equation 5 such that the candidate virtual point\nğ‘˜ğ‘£is separated from the values of ğ¾. This enables us to calculate the\nterms in the equation related to ğ¾separately and then reuse their\nvalues for different candidate virtual points. This would reduce the\ntime when computing the loss for different candidate virtual points.\nL({ğ¾âˆªğ‘‰})=ğ‘¤2ğ‘›âˆ‘ï¸\nğ‘–=1ğ‘˜2\nğ‘–+2ğ‘¤ğ‘ğ‘› Â¯ğ‘˜\nâˆ’2ğ‘¤ğ‘›âˆ‘ï¸\nğ‘–=1ğ‘˜ğ‘–ğ‘¦ğ‘–+ğ‘›ğ‘2âˆ’2ğ‘›ğ‘Â¯ğ‘¦\n+ğ‘›âˆ‘ï¸\nğ‘–=1ğ‘¦2\nğ‘œğ‘Ÿğ‘–ğ‘”ğ‘–ğ‘›ğ‘ğ‘™ğ‘–+ğ‘›2âˆ’ğ‘¦2\nğ‘£+(ğ‘¤ğ‘˜ğ‘£+ğ‘âˆ’ğ‘¦ğ‘£)2,(10)\nwhere\nğ‘›âˆ‘ï¸\nğ‘–=1ğ‘¦ğ‘–=ğ‘›âˆ‘ï¸\nğ‘–=1ğ‘¦ğ‘œğ‘Ÿğ‘–ğ‘”ğ‘–ğ‘›ğ‘ğ‘™ğ‘–+ğ‘›âˆ’ğ‘¦ğ‘£, (11)\nÂ¯ğ‘¦=Ãğ‘›\nğ‘–=1ğ‘¦ğ‘–\nğ‘›, (12)\nÂ¯ğ‘˜=Ãğ‘›\nğ‘–=1ğ‘˜ğ‘–\nğ‘›, (13)\nğ‘›âˆ‘ï¸\nğ‘–=1ğ‘˜ğ‘–ğ‘¦ğ‘–=ğ‘›âˆ‘ï¸\nğ‘–=1ğ‘˜ğ‘–ğ‘¦ğ‘œğ‘Ÿğ‘–ğ‘”ğ‘–ğ‘›ğ‘ğ‘™ğ‘–+ğ‘›âˆ‘ï¸\nğ‘–=ğ‘¦ğ‘£ğ‘˜ğ‘–. (14)\nIn the equations, Â¯ğ‘˜and Â¯ğ‘¦are the mean of the key set ğ¾and the\ncorresponding rank set before inserting the virtual point (ğ‘˜ğ‘£,ğ‘¦ğ‘£),\nrespectively; ğ‘¦ğ‘œğ‘Ÿğ‘–ğ‘”ğ‘–ğ‘›ğ‘ğ‘™ğ‘–refers to the rank of key ğ‘˜ğ‘–prior to inserting\nthe virtual point.\nGiven the rewritten loss, Equations 15 and 16 are derived by\nseparating the terms related to the candidate virtual point from\nthose related to the original key set ğ¾, which enable more efficient\ncomputation of ğ‘¤andğ‘.\nğ‘¤=(Ãğ‘›\nğ‘–=1ğ‘˜ğ‘–ğ‘¦ğ‘–+ğ‘˜ğ‘£ğ‘¦ğ‘£)âˆ’(ğ‘›+1)Â¯ğ‘˜ğ‘£Â¯ğ‘¦ğ‘£\n(Ãğ‘›\nğ‘–=1ğ‘˜2\nğ‘–+ğ‘˜2ğ‘£)âˆ’(ğ‘›+1)Â¯ğ‘˜2ğ‘£, (15)\nğ‘=Â¯ğ‘¦ğ‘£âˆ’ğ‘¤Â¯ğ‘˜ğ‘£. (16)\nAdjustment for multiple virtual points. In the case of insert-\ningğœ†virtual points, after inserting one virtual point (ğ‘˜ğ‘£1,ğ‘¦ğ‘£1), to\nfind the next virtual point (ğ‘˜ğ‘£2,ğ‘¦ğ‘£2), the original key set terms will\nbe adjusted to include the newly added virtual point (ğ‘˜ğ‘£1,ğ‘¦ğ‘£1). As\na result, the loss for the next candidate virtual point ğ‘˜ğ‘£2can also\nbe efficiently calculated by considering only the changes induced\nbyğ‘˜ğ‘£2and its corresponding ğ‘¦ğ‘£2.\nIn the derived equations above, the terms are separated based\non whether they belong to the original key set or dependent on\nthe candidate virtual key to be inserted. Doing so enables efficient\ncalculations of the loss by reusing the terms of the original key set\nafter calculating them just once.\n\nArvix, Oct, 2025 Kasun Amarasinghe, Farhana Choudhury, Jianzhong Qi, and James Bailey\n4.2 Filtering Virtual Point Candidates\nNext, we present an efficient approach to identify the candidate\nvirtual points that can potentially reduce the loss, thus providing a\nmuch smaller set of candidate virtual points to consider. This step is\nimportant because while using the equations above helps improve\nthe efficiency of processing one candidate virtual point, the number\nof candidate virtual points to be processed has a multiplicative\nimpact to the overall algorithm time efficiency.\nTo reduce the search space for the candidate virtual points, we\nbound it in(min{ğ¾},max{ğ¾}). This is because any virtual points\nadded prior to min{ğ¾}would cause all keysâ€™ ranks to increase at\nthe same time, while adding virtual points after max{ğ¾}would\nnot impact any keyâ€™s rank. As such, neither would help achieve a\nbetter-fitted indexing function. We also skip the index keys already\ninğ¾, such that our solution can be compatible with learned indexes\nthat do not support duplicate keys [28].\nBelow, we present an approach based on the derivative of the loss\nfunction to further reduce the set of candidate virtual points. Our\nidea is illustrated using Fig. 4, which plots the partial derivative of\nthe loss function with respect to a candidate virtual point ğ‘˜ğ‘£. Each\nsub-sequence (depicted as lines or dots) corresponds to the partial\nderivative of the loss of a sub-sequence of key values of a candidate\nvirtual point (which can also be seen as a sub-sequence of candidate\nvirtual points). The sub-sequences that cross the zero-value line\n(i.e., theğ‘¥-axis) imply that there is a minimal loss point within the\nsub-sequence. Otherwise, the minimal point for the sub-sequence\nmust be at one of two endpoints of the sub-sequence, since each of\nsuch sub-sequences has been shown to be convex [ 11]. Intuitively,\nthis convex property is because the loss function is a summation\nofğ‘›quadratic terms ((ğ‘¤ğ‘˜ğ‘–+ğ‘âˆ’ğ‘¦ğ‘–)2) and only one non-quadratic\nterm ((ğ‘¤ğ‘˜ğ‘£+ğ‘âˆ’ğ‘¦ğ‘£)2), and theğ‘¤ğ‘˜ğ‘£term is nonlinear variable. We\nexploit this property to streamline the selection of candidate virtual\npoints, i.e., to select the best virtual point from each sub-sequence.\nFollowing the idea above, we propose to filter the candidate\nvirtual points as follows:\n(1)For each sub-sequence of candidate points (that is, where\nthe candidate virtual point values are continuous), if the\nlength of the sub-sequence is greater than 2, there can be\na candidate virtual point within the sub-sequences with a\nlocal minima of the loss. We compute the partial derivative\nof the loss function, which includes the previously added\nvirtual points (L({ğ¾âˆªğ‘‰})shown in Equation 5) with respect\nto candidate virtual point ğ‘˜ğ‘£, i.e.,L({ğ¾âˆªğ‘‰})â€²of the two\nendpoints of such sub-sequence (we present efficient ways\nto compute the partial derivative of a candidate later).\nâ€¢If the sign ofL({ğ¾âˆªğ‘‰})â€²of the two endpoints are the\nsame, it means that there is no point with the local minima\nwithin this sub-sequence (i.e.â€ the sub-sequence does not\ncross the zero value in ğ‘¦-axis as shown in Fig. 4). In that\ncase, we only need to consider the two endpoints of the\nsub-sequence as the candidate virtual points, and we can\nsafely discard all the candidate virtual points in between.\nâ€¢Otherwise, if the sign of L({ğ¾âˆªğ‘‰})â€²of the two endpoints\nare opposite, it means that there is a point with a local\nminima within the sub-sequence. The minimal point can\nbe calculated by using the two partial derivative values\n5 10 15 20 25 30\nVirtual point values1.0\n0.5\n0.00.51.0First partial derivativekv1Figure 4: First-order partial derivatives of the loss (Equa-\ntion 17) with respect to the key value of a virtual point ğ‘˜ğ‘£\nto find their intersection with the x-axis. As this minimal\npoint is guaranteed to have a smaller loss than all the other\npoints in that sub-sequence, only the point is considered\nas a candidate virtual point. All other points in the sub-\nsequence can be safely discarded.\n(2)If the length of the sub-sequence is less than or equal to 2,\nwe need to consider all points in the sub-sequence as the\ncandidate virtual points.\nEfficient computation of the first-order derivative of the\nloss function value. Similar to the computation of the loss func-\ntion, we present an efficient way to calculate the first-order partial\nderivative of the loss function with respect to candidate virtual\npointğ‘˜ğ‘£, whereğ‘˜ğ‘£and its related terms are separated from the\nother terms to enable reusing the terms that require information\nfromğ¾only. This is achieved by the following equation:\nL({ğ¾âˆªğ‘‰})â€²=2(ğ‘¤â€²(ğ‘¤ğ‘›âˆ‘ï¸\nğ‘–=1ğ‘˜2\nğ‘–+ğ‘›ğ‘Â¯ğ‘˜âˆ’ğ‘›âˆ‘ï¸\nğ‘–=1ğ‘˜ğ‘–ğ‘¦ğ‘–)+\nğ‘›ğ‘â€²(ğ‘¤Â¯ğ‘˜+ğ‘âˆ’Â¯ğ‘¦)+(ğ‘¤ğ‘˜ğ‘£+ğ‘âˆ’ğ‘¦ğ‘£)(ğ‘¤â€²ğ‘˜ğ‘£+ğ‘¤+ğ‘â€²)).(17)\nHere,ğ‘¤â€²andğ‘â€²refer to the partial derivatives of ğ‘¤andğ‘with\nrespect toğ‘˜ğ‘£, respectively. They can be computed by Equations 18\nand 19 as follows.\nğ‘¤â€²=ğ´(ğ‘›(ğ‘¦ğ‘£âˆ’Â¯ğ‘¦))âˆ’ğµ(2ğ‘›(ğ‘˜ğ‘£âˆ’Â¯ğ‘˜))\nğ´2, (18)\nğ‘â€²=âˆ’(ğ‘¤+(ğ‘›+1)Â¯ğ‘˜ğ‘£ğ‘¤â€²)\nğ‘›+1, (19)\nğ´=(ğ‘›+1)(ğ‘›âˆ‘ï¸\nğ‘–=1ğ‘˜2\nğ‘–+ğ‘˜2\nğ‘£)âˆ’((ğ‘›+1)Â¯ğ‘˜ğ‘£)2, (20)\nğµ=(ğ‘›+1)(ğ‘›âˆ‘ï¸\nğ‘–=1ğ‘˜ğ‘–ğ‘¦ğ‘–+ğ‘˜ğ‘£ğ‘¦ğ‘£)âˆ’(ğ‘›+1)2Â¯ğ‘˜ğ‘£Â¯ğ‘¦ğ‘£. (21)\nHere,ğ´andğµare intermediary for computing the partial deriva-\ntives.\n4.3 Algorithm for Inserting ğœ†Virtual Points\nAfter filtering the candidate virtual points, among the remaining\nones, we present an efficient algorithm to find the best subset of\ncandidate virtual points of size ğœ†.\nWhen there are ğœ†virtual points to insert, the optimal solution\nwould require computing the loss for every size- ğœ†subset of the\ncandidate virtual points in the range of (min{ğ¾},max{ğ¾}). If there\n\nLearned Indexes with Distribution Smoothing via Virtual Points Arvix, Oct, 2025\nareğ‘possible insertion positions for the virtual points, the time\ncomplexity will be ğ‘‚(ğ‘ğ¶ğœ†Â·ğ‘›Â·ğ‘), whereğ‘ğ¶ğœ†is the combination of\nevery size-ğœ†subset from ğ‘. As this will be prohibitively expensive\nfor a large dataset, we propose a greedy algorithm that inserts\nindividual virtual points iteratively.\nOur core idea is to identify the virtual point that would min-\nimise the loss for each sub-sequence (i.e., local minima for the\nsub-sequences) and select the one that reduces the loss the most\n(i.e., global minimum). This process needs to be performed ğœ†times.\nThe algorithm for CDF smoothing by inserting ğœ†virtual points is\nsummarised in Algorithm 1 and described below.\nAlgorithm 1 CDF_smoothing\nRequire: Key set:ğ¾, loss function with new virtual point: L(ğ¾âˆªğ‘˜ğ‘£),\nsmoothing threshold: ğ›¼\n1:ğ‘ˆ,ğ¶,ğ‘‰=[]\n2:ğº=[],ğ‘€=[] âŠ²Arrays of point pairs\n3:Lâ€²(ğ¾âˆªğ‘˜ğ‘£)=ğœ•ğ¿(ğ¾âˆªğ‘˜ğ‘£)\nğœ•ğ‘˜ğ‘£,ğœ†=ğ›¼Â·ğ¾.ğ‘ ğ‘–ğ‘§ğ‘’ ,Lğ‘ğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘œğ‘¢ğ‘  =L(ğ¾âˆªâˆ…)\n4:Find the endpoint pairs, ğ¸, for each sub-sequence\n5:whileğ‘‰.ğ‘ ğ‘–ğ‘§ğ‘’ <ğœ†do\n6: forğ‘–from 1 toğ¸.ğ‘ ğ‘–ğ‘§ğ‘’ do âŠ²Separate sub-sequences\n7: ifğ¸[ğ‘–].ğ‘ ğ‘’ğ‘ğ‘œğ‘›ğ‘‘ -ğ¸[ğ‘–].ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡â‰¤1then\n8: Appendğ¸[ğ‘–].ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡ andğ¸[ğ‘–].ğ‘ ğ‘’ğ‘ğ‘œğ‘›ğ‘‘ toğ¶\n9: else âŠ²There are more than 2 points\n10: Appendğ¸[ğ‘–].ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡ toğº.ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡ andğ¸[ğ‘–].ğ‘ ğ‘’ğ‘ğ‘œğ‘›ğ‘‘ to\nğº.ğ‘ ğ‘’ğ‘ğ‘œğ‘›ğ‘‘\n11: end if\n12: end for\n13: forğ‘–from 1 toğº.ğ‘ ğ‘–ğ‘§ğ‘’ do âŠ²Calculate the partial derivatives\n14: ifLâ€²(ğ¾âˆªğº[ğ‘–].ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡)Â·Lâ€²(ğ¾âˆªğº[ğ‘–].ğ‘ ğ‘’ğ‘ğ‘œğ‘›ğ‘‘)<0then\n15: Appendğº[ğ‘–].ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡ toğ‘€.ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡ andğº[ğ‘–].ğ‘ ğ‘’ğ‘ğ‘œğ‘›ğ‘‘ to\nğ‘€.ğ‘ ğ‘’ğ‘ğ‘œğ‘›ğ‘‘\n16: else\n17: Appendğº[ğ‘–].ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡ andğº[ğ‘–].ğ‘ ğ‘’ğ‘ğ‘œğ‘›ğ‘‘ toğ¶\n18: end if\n19: end for\n20: forğ‘–from 1 toğ‘€.ğ‘ ğ‘–ğ‘§ğ‘’ do âŠ²Calculate minimum point\n21: Appendğ‘šğ‘–ğ‘›ğ‘–ğ‘šğ‘¢ğ‘š _ğ‘ğ‘œğ‘–ğ‘›ğ‘¡(ğ‘€[ğ‘–].ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡ ,ğ‘€[ğ‘–].ğ‘ ğ‘’ğ‘ğ‘œğ‘›ğ‘‘)toğ¶\n22: end for\n23: forğ‘–from 1 toğ¶.ğ‘ ğ‘–ğ‘§ğ‘’ do âŠ²Calculate loss value\n24:ğ‘ˆ[ğ‘–]=L(ğ¾âˆªğ¶[ğ‘–])\n25: end for\n26: Find indexğ‘–of minimumL\n27: ifLğ‘ğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘œğ‘¢ğ‘ â‰¤ğ‘ˆ[ğ‘–]then\n28: break\n29: end if\n30: Appendğ¶[ğ‘–]toğ‘‰, Appendğ¶[ğ‘–]toğ¾,Lğ‘ğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘œğ‘¢ğ‘  =ğ‘ˆ[ğ‘–]\n31:end while\n32:return C\nThe algorithm. Our algorithm takes as input a key set ğ¾and a\nsmoothing threshold ğ›¼(or a smoothing budget ğœ†=ğ›¼ğ‘›). We useğº\nto denote a set that stores the potential sub-sequences, where there\ncan be a candidate virtual point within the sub-sequences with a\nlocal minima of the loss. The candidate virtual points contributing\nthe local minima are stored in an array ğ‘€, whileğ¶stores the set of\ncandidate keys for the virtual points. We use ğ‘ˆto hold the loss for\neach candidate virtual point and vector ğ‘‰to store the final optimal\nvirtual points.First, the algorithm identifies the sub-sequences of candidate\nvirtual points that could have their minimal loss at the endpoints\nor in-between the sequence. This is shown in Lines 4 to 12. If there\nare more than two points in a sub-sequence, the candidate virtual\npoint with the minimal loss can be within that sub-sequence. As\nsuch, the two endpoints of the sub-sequence are saved in array ğº\nfor calculating the partial derivatives.\nAfterwards, in Lines 13 to 22, the partial derivative of the loss\nfunction with respect to the candidate virtual points is calculated for\nall point pairs in ğºusing the equations derived above. If the signs of\nthe partial derivatives corresponding to the two endpoints of a sub-\nsequence are different (i.e., on opposite sides of the ğ‘¥-axis), the two\nendpoints are added to array ğ‘€for calculating the minimum point.\nAs shown in Fig. 4, for the sub-sequences that contain candidate\nvirtual points with minimal loss, the partial derivatives of the end\npoints will appear on the two sides of the ğ‘¥-axis. These minimal\npoints are added to array ğ¶after they are calculated. If the partial\nderivatives of the two endpoints are on the same side of the ğ‘¥-axis,\nthe minima is at one of the endpoints, as such the two end points\nare added to ğ¶.\nFinally, Lines 23 to 31 compute the loss for each candidate virtual\npoint inğ¶and select the point with the minimum loss, as long as the\nnew loss is smaller than the existing loss obtained so far over ğ¾and\nany previously inserted virtual points. This process is repeated until\nat mostğœ†virtual points are inserted, or when the loss is not reduced\nany further. When the algorithm terminates, the final virtual points\ninğ‘‰are returned.\nComplexity analysis. Our proposed CDF smoothing algorithm\nreduces the computation of the loss over ğ¾to just once, which takes\nğ‘‚(ğ‘›)time. This process is repeated to find ğœ†optimal candidate\nvirtual points. However, there is no need to recalculate the loss\nfunction after adding a virtual point, as we could treat the key\nset with the previous virtual point inserted as the new original or\nbase key set for a constant time calculation. Thereby, giving a time\ncomplexity of ğ‘‚(ğœ†+ğ‘›).\n5 CDF Smoothing for Hierarchical Indexes\nIn this section, we present the CDF smoothing to a hierarchical\nlearned index to improve the performance of queried keys. A direct\napplication to individual nodes would help reduce leaf-node search\ntime by improving the learnability of the models but fail to address\ntraversal time. Therefore, a method for addressing both traversal\nand leaf-node search is required. As such we present CSV to smooth\nsegments of the CDF for different sub-trees in the hierarchical\nstructure of a learned index in order to merge and reduce the overall\nstructure height. A major challenge is the balancing between the\nimprovement of traversal time due to the reduction of the index\nheight and the increase in leaf-node search time due to more keys\nbeing merged into single nodes. To address this, we present a cost\nmodel that takes both of these factors into consideration.\nThe core idea is to start from the bottom most level of the index\nthat contains parent nodes of leaf-nodes and select those nodes.\nThen for each of these parents nodes, the keys in the node and its\nchild nodes are collected, which are then subjected to smoothing\nusing Algorithm 1. If the minimum cost threshold is satisfied (more\ndetails below), then the sub-tree and the node are reconstructed to\n\nArvix, Oct, 2025 Kasun Amarasinghe, Farhana Choudhury, Jianzhong Qi, and James Bailey\nAlgorithm 2 CSV\nRequire: Nodes with sub trees : ğ‘ğ‘œğ‘‘ğ‘’ğ‘  , smoothing threshold : ğ›¼, cost\nthreshold :ğ‘\n1:ğ‘ğ‘œğ‘‘ğ‘’ğ‘  =[]\n2:ğ‘˜ğ‘’ğ‘¦ğ‘ ğ‘’ğ‘¡ =[]\n3:ğ‘˜ğ‘’ğ‘¦ğ‘ ğ‘’ğ‘¡ _ğ‘ ğ‘šğ‘œğ‘œğ‘¡â„ =[]\n4:ğ‘šğ‘ğ‘¥ _ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™â†maximum level of index with sub trees\n5:ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ _ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™â†ğ‘šğ‘ğ‘¥ _ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™\n6:whileğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ _ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ >1do\n7:ğ‘ğ‘œğ‘‘ğ‘’ğ‘ â†all nodes with sub trees\n8: forğ‘–from 1 toğ‘ğ‘œğ‘‘ğ‘’ğ‘ .ğ‘ ğ‘–ğ‘§ğ‘’ do\n9:ğ‘˜ğ‘’ğ‘¦ğ‘ ğ‘’ğ‘¡â†collect all keys in the node and its sub tree\n10:ğ‘˜ğ‘’ğ‘¦ğ‘ ğ‘’ğ‘¡ _ğ‘ ğ‘šğ‘œğ‘œğ‘¡â„â†ğ¶ğ·ğ¹ _ğ‘ ğ‘šğ‘œğ‘œğ‘¡â„ğ‘–ğ‘›ğ‘”(ğ‘˜ğ‘’ğ‘¦ğ‘ ğ‘’ğ‘¡,ğ›¼) âŠ²Using\nAlgorithm 1\n11: ifğ‘ğ‘œğ‘ ğ‘¡ <ğ‘then\n12: Reconstruct the sub-tree and node with ğ‘˜ğ‘’ğ‘¦ğ‘ ğ‘’ğ‘¡ _ğ‘ ğ‘šğ‘œğ‘œğ‘¡â„\n13: end if\n14: end for\n15:ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ _ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™â†ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ _ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™âˆ’1\n16:end while\nmerge the collected nodes. The merging is performed by creating\na new leaf-node in place of the parent node and placing the keys\nfrom the collected nodes. By doing so, more keys would be placed\nin upper level nodes of the index as the indexing functions of\nthese nodes would be improved by the CDF smoothing, but the\ncost models would limit the number of keys as to not offset the\nperformance gain by the increase in the leaf-node search time.\nFurther details regarding this is given in Section 5.1. This process\nis performed until the root node depicted as level 1 is reached, thus\nreducing the loss (L).\nThis process is applied to a constructed learned index structure\nas the purpose of the method is to enhance the structure of the index.\nFurther, it is computationally expensive to perform the smoothing\noperation on the full key set. As such it is more reasonable to handle\nsubsets of the key set in the constructed hierarchical index. For this\npurpose, unbalanced learned index structures are better suited as\nit gives the ability to reduce the height of taller branches without\naffecting the rest.\nThe algorithm for CDF smoothing of a hierarchical learned index\nstructure is given in Algorithm 2 and described below. First the algo-\nrithm starting from the maximum level of the index, and identifies\nall nodes with sub-trees, which is shown in Lines 5-7. Afterwards,\nas shown in Lines 8-14, each of the nodeâ€™s and its sub-treeâ€™s keys\nare collected and subjected to the CDF smoothing. Provided that\nthey meet the minimum cost threshold selected, the node and its\nsub-tree are reconstructed to promote as many keys to upper levels\nas possible. This process is iteratively performed in a bottom up\nmanner for other levels of the index.\n5.1 Cost Conditions\nFor indexes that does not contain any searching component such\nas LIPP and SALI, their loss function values can be taken as the cost\nconditions. This is because if the new model could hold more keys\nthan before, then it does not have any other component (that is,\nleaf-node search time) that would negatively affect the performance.\nHowever, for the indexes with leaf-node search components likeALEX, there must be a trade-off between the increase of leaf-node\nsearch time over the reduction of traversal time. The reason is,\nintroducing new keys into the node would require more time to\nlocate the key. For this purpose, we develop the following cost\nmodel, where reconstruction is performed only if the cost is less\nthan a specified threshold value, ğ‘.\nğ‘ğ‘œğ‘ ğ‘¡=ğ‘ ğ‘’ğ‘ğ‘Ÿğ‘â„ _ğ‘ğ‘œğ‘›ğ‘ ğ‘¡ğ‘ğ‘›ğ‘¡Ã—ğ‘’ğ‘¥ğ‘ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ _ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ _ğ‘œğ‘“_ğ‘ ğ‘’ğ‘ğ‘Ÿğ‘â„ğ‘’ğ‘ \n+ğ‘¡ğ‘Ÿğ‘ğ‘£ğ‘’ğ‘Ÿğ‘ ğ‘ğ‘™ _ğ‘ğ‘œğ‘›ğ‘ ğ‘¡ğ‘ğ‘›ğ‘¡Ã—ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ _ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™(22)\nTo make the implementation hardware independent, the con-\nstants can be measured by sampling queries to measure the time\nspent per leaf-node search for the case of ğ‘ ğ‘’ğ‘ğ‘Ÿğ‘â„ _ğ‘ğ‘œğ‘›ğ‘ ğ‘¡ğ‘ğ‘›ğ‘¡ and the\ntraversal time spent per level for ğ‘¡ğ‘Ÿğ‘ğ‘£ğ‘’ğ‘Ÿğ‘ ğ‘ğ‘™ _ğ‘ğ‘œğ‘›ğ‘ ğ‘¡ğ‘ğ‘›ğ‘¡ . Theğ‘’ğ‘¥ğ‘ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘\n_ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ _ğ‘œğ‘“_ğ‘ ğ‘’ğ‘ğ‘Ÿğ‘â„ğ‘’ğ‘  can be calculated via the inbuilt function in\nALEX that uses the ğ‘™ğ‘œğ‘”2error to estimate it. Considering the cost\nmodel depicts the expected query time for the node, the cost thresh-\nold,ğ‘should be set below 0 to identify an improvement. Setting a\nlower value would result in fewer keys being able to be promoted\nto upper levels but the expected query time improvement will be\ngreater.\nComplexity analysis. For a key set of size ğ‘›, with a smoothing\nbudget ofğœ†and an index structure with ğ‘šnon-leaf nodes, the\ncomplexity for the developed algorithm can be calculated as follows.\nThe complexity for ğ‘›ğ‘œğ‘‘ğ‘’ğ‘–withğ‘›ğ‘–keys and a smoothing budget of ğœ†ğ‘–\nisğ‘‚(ğœ†ğ‘–+ğ‘›ğ‘–). Similarly, for the ğ‘šnodes, we would get a complexity\nofğ‘‚(ğœ†1+ğ‘›1+ğœ†2+ğ‘›2+Â·Â·Â·+ğœ†ğ‘š+ğ‘›ğ‘š). This can be simplified to\nğ‘‚(ğœ†+ğ‘›).\nChoice of smoothing threshold. Increasing the smoothing\nthreshold would make the algorithm insert more virtual points,\nthus reducing the loss function value of the newly fitted model\neven more. As a result these indexing functions would be able to\naccommodate more keys which were originally in lower levels of\nthe index and improve their query time. However, that is a trade-\noff between the query time improvement and the higher space\ncost for increasing this threshold. Further, key sets that cause the\noriginal index structure to construct poorly should benefit more\nfrom a higher smoothing threshold as shown in the Experimental\nresults 6.2.1.\n5.2 Approximation Quality Analysis\nThe effectiveness of our proposed greedy method of iteratively\nidentifying the ğœ†virtual points as opposed to the exhaustive manner\nof comparing all ğœ†subsets, is demonstrated via experimentation in\nthis section.\nThe key set of 10 keys given in Fig. 2 was subjected to CDF\nsmoothing with a smoothing threshold ( ğ›¼) of 0.5 (smoothing bud-\nget of 5) via both methods. The results are shown in Table 2. Here,\nthe greedy method improves the loss by 72.34%, while the exhaus-\ntive method improves it by 74.44%. However, the time taken by\nthe exhaustive method is nearly 3 orders of magnitude more than\nthe greedy method. This results show that the effectiveness of the\ngreedy method is similar to the exhaustive method, and the exhaus-\ntive method is impractical to use in real datasets.\n\nLearned Indexes with Distribution Smoothing via Virtual Points Arvix, Oct, 2025\n6 Experimental results\nNext, we report experimental results. The implementation of our\nevaluation framework is based on an existing benchmark [ 1] imple-\nmentation. All experiments were run on an Ubuntu 20.04.5 virtual\nmachine with an AMD EPYC 7763 64-Core CPU and 128 GB RAM.\n6.1 Experimental Settings\nCompetitors. To show the general applicability of our proposed\ntechniques, we integrate CSV with recent learned indexes, includ-\ningALEX [2],LIPP [33], and SALI [9] (SOTA). These three indexes\nwere chosen because they are the latest and among the most widely\nused benchmark learned indexes. They have reported strong em-\npirical performance, outperforming both traditional indexes such\nas the B+-tree and learned ones [ 28] such as the PGM index [ 6],\nXIndex [ 29], and FINEdex [ 14]. For simplicity, we do not repeat the\ncomparison results with these other indexes.\nDatasets. We run experiments with four datasets from two\nbenchmark works [ 20,32]: (1) Facebook contains 200 million in-\nteger Facebook user IDs [ 30]; (2) Covid contains 200 million inte-\nger tweet IDs randomly sampled from tweets tagged with â€œCovid-\n19â€ [ 18]; (3) OSM contains 200 million locations randomly sam-\npled from OpenStreetMap and represented as Google S2 [ 26] cell\nIDs [ 24]; and (4) Genome contains 200 million entries of loci pairs in\nhuman chromosomes represented as integers [ 25]. For all datasets,\nduplicate keys were removed to suit LIPP and SALIâ€™s requirements.\nOut of the four datasets, OSM and Genome are considered more\ndifficult for learned indexes [ 32] (hard datasets), while Facebook\nand Covid are easier (easy datasets). To illustrate this, the CDFs of\nthe full datasets are plotted in Figs. 5a to 5d. All datasets except OSM\nhave almost globally linear CDFs. Zooming in the CDFs shows that\nthere is more variability in the local distribution patterns, as shown\nin Figs. 5e to 5h (each showing from the 100 million-th data point\nto the next thousand data points). Except for Covid, all datasets\ndeviate from linear CDFs at local level, especially Genome.\n(a) Facebook\n (b) Covid\n (c) OSM\n (d) Genome\n(e) Facebook\n(zoomed-in)\n(f) Covid\n(zoomed-in)\n(g) OSM\n(zoomed-in)\n(h) Genome\n(zoomed-in)\nFigure 5: CDFs of the datasets\nWorkloads. We use the following two types of workloads:\nTable 2: Approximation Quality Results\nExhaustive CSV Original\nLoss 2.118 2.293 8.327\nTime (ns) 140,656,167 424,667 N/A(1)Read-only workload. The learned indexes ALEX, LIPP, and\nSALI are constructed over the full datasets. Afterwards, our CSV\nalgorithm is applied to optimise their structures. Then, the queries\n(detailed below) are run.\n(2)Read-write workload. The learned indexes are constructed\nover a random half of each dataset and then CSV is applied. The\nother half of the dataset is inserted in random batches of size 0.1ğ‘›.\nQueries are run after each batch insertion without using CSV again\nfor each batch.\nQueries. Considering the main objective of the developed method\nis to improve the performance of keys in lower levels, the experi-\nments are focused on them. Specifically, we report results for the\npromoted data , which includes every key that has been promoted\nto upper levels in the index by our algorithm.\nParameters. We vary the smoothing threshold, ğ›¼, from 0.05 to\n0.8, with a default value of 0.1. To show the scalability of our algo-\nrithm by varying the dataset size, the original datasets were down\nsampled by eliminating every ğ‘—-th key from the sorted datasets in\norder to remove ğ‘›/ğ‘—data points and create smaller datasets of size\n12.5 million, 25 million, 50 million, and 100 million, respectively.\nThe default datasets are the original ones with 200 million points.\nFor each queried key, the query time was recorded by repeating\nthe query 100 times and taking its average.\nFor LIPP and SALI, they can create nodes that are indexing only\na few keys [ 33]. For these two indexes, CSV is run starting at the\nsecond level of the index structures, such that each smoothing step\ncan benefit more points. This is not an issue for ALEX, and CSV is\nrun starting at the bottom level. Further, since the query times of\nthe keys in the top two levels of the index structures are very close,\nCSV stops at the second level from the top (i.e., the root).\nEvaluation metrics. We report: (1) the total query times\nsaved by the CSV-enhanced indexed compared with those of the\noriginal indexes; (2) the query time improvement (%) , which\nis the relative difference between the average query time (over\nall queried keys) of the CSV-enhanced indexes and that of the\noriginal ones; (3) the promoted data (%) , which is the percentage\nof keys promoted to upper levels in the index structure among all\nkeys that can be promoted (i.e., keys at levels 3 or below of the\noriginal indexes); (4) the storage space increase (%) , which is the\nrelative index size overhead of the CSV-enhanced indexes compared\nwith the original ones; (5) the node reduction (%) , which is the\npercentage of nodes reduced by the CSV-enhanced indexes over\nthe original ones; and (6) the insert time increase (%) , which is\nrelative increase in the average time per insertion required by the\nCSV-enhanced indexes compared to the original ones.\n6.2 Results on Read-only Workloads\n6.2.1 Impact of Smoothing Threshold. We vary the smoothing\nthreshold from 0.05 to 0.8 to quantify its impact.\nQuery time (for promoted data). Here, we report the query\ntime improvement by CSV for the â€˜promoted keysâ€™ (i.e., the keys\nthat is promoted to an upper level of the index by CSV), compared\nto the original index. We depict the total time saved due to the\nmethod in Fig. 6. The general trend is that adding more virtual\npoints (i.e., increasing the smoothing budget, ğ›¼) saves more query\ntimes. LIPP and SALI tend to perform quite similarly due to SALI\n\nArvix, Oct, 2025 Kasun Amarasinghe, Farhana Choudhury, Jianzhong Qi, and James Bailey\nFacebook Covid OSM Genome\n0.05 0.1 0.2 0.4 0.8\n0.01.53.04.5T otal time \nsaved (ns)Ã—108\n(a) LIPP\n0.05 0.1 0.2 0.4 0.8\n0.02.55.07.5T otal time \nsaved (ns)Ã—108 (b) SALI\n0.05 0.1 0.2 0.4 0.8\n0246T otal time \nsaved (ns)Ã—108 (c) ALEX\nFigure 6: Total time saved vs. smoothing threshold ğ›¼\n0.05 0.1 0.2 0.4 0.8\n0102030Query time \n improvement (%)\n(a) LIPP\n0.05 0.1 0.2 0.4 0.8\n051015Query time \n improvement (%) (b) SALI\n0.05 0.1 0.2 0.4 0.8\n051015Query time \n improvement (%) (c) ALEX\nFigure 7: Query time improvement vs. smoothing threshold ğ›¼\nusing LIPP as the base index. For LIPP and SALI indexes, the easy to\nlearn datasets (Facebook and Covid) stabilise after a certain number\nof virtual points are inserted. This is because the original datasetsâ€™\nCDFs are already quite linear. The same pattern was not observed\nfor ALEX, this is because ALEX has an additional leaf-node search\nstep not required by LIPP and SALI (CSV forms larger nodes that\ncould lead to longer leaf-node search times).\nFig. 7 further reports the average query improvements as a per-\ncentage against the original index structure over the promoted data.\nIt shows that applying CSV yields a query time improvement of up\nto 34%, with stronger benefits observed over the two SOTA index\nstructures LIPP and SALI. Smaller performance gain is observed\nover ALEX due to its leaf-node search process. It is important to\nnote that CSV still yields consistent query time improvements in\nthis case. As ğ›¼increases, the relative query time improvements for\nALEX also increases, as the leaf-node search efficiency is improved\ndue to increased accuracy of the refitted indexing functions. Since\nthere is no such searching in LIPP and SALIâ€™s query process, their\nquery performance is stagnant and the improvement represents\nthe reduction in the index traversal time for query processing.\nSize of the promoted data. Figs. 8a, 8d, and 8g show the percent-\nage of keys promoted. For the Facebook dataset, CSV can promote\naround 60% of all promotable data (i.e., keys at level 3 or below\nof the original index), while for the Covid dataset, CSV promotes\naround 30% of the promotable data. For the harder to learn datasets,\nOSM and Genome, CSV also manages to promote up to 27% and 57%\nof the promotable data, respectively. The datasets with the most\npromoted data are again different for ALEX, due to its structural dif-\nference. Even though OSM and Genome report lower percentages\nof promoted data, the actual number of promoted keys is higher.\nThis is because they have much more promotable data (i.e., more\nkeys in lower levels). Overall, as the smoothing threshold increases,more keys get promoted to upper levels. This is consistent with\nthe theoretical analysis as adding more virtual points would allow\nmore keys to be placed in nodes in upper levels. The difficult to\nlearn key sets (OSM and Genome) demonstrate this property the\nmost. This can also be matched with Fig. 6, where the total time\nsaved for OSM and Genome is higher due to higher number of data\nbeing promoted for those datasets, compared to the other datasets.\nIndex size. Due to the addition of virtual points, we expect\nthe storage space consumption to increase. This is reflected in\nFigs. 8b, 8e, and 8h. In most cases, less than 10% of additional\nstorage space is required by the CSV-enhanced indexes compared\nto the original structures, and in the worst case, less than 31%. The\nspace cost overhead is proportional to ğ›¼, which is also intuitive.\nThe storage space increase is balanced by the removal of unnec-\nessary nodes, whose data is promoted to higher levels. Figs. 8c, 8f,\nand 8i report the node reductions achieved by CSV, which are given\nas the percentage of nodes that are removed relative to the number\nof nodes at levels 3 or lower of the original indexes. They follow\nsimilar patterns to the percentage of promoted data as expected.\nPre-processing time for CSV. The times taken to run CSV to\noptimise the learned index structures are summarised in Tables 3\nand 4 for LIPP and ALEX. As SALI is based on LIPP, where CSV\nreports a similar performance, we omit its results for brevity.\nCSV takes more time to run as ğ›¼grows, which is consistent\nwith our time complexity analysis. The algorithm running times\nvary across different datasets, again because the datasets have dif-\nferent difficulties in index learning. While the running times of\nCSV may seem quite large under certain settings, these are one-off\npre-processing costs that can be amortised by the improvements\nin query time. One may further mitigate the impact of the extra\nconstruction times by using the original indexes for queries while\n\nLearned Indexes with Distribution Smoothing via Virtual Points Arvix, Oct, 2025\n0.05 0.1 0.2 0.4 0.8\n0204060Promoted \ndata (%)\n(a) Percentage of promoted\ndata (LIPP)\n0.05 0.1 0.2 0.4 0.8\n0153045Storage \nincrease (%)(b) Percentage of storage\nspace increase (LIPP)\n0.05 0.1 0.2 0.4 0.8\n0204060Node \nreduction (%)(c) Percentage of index\nmodel reduction (LIPP)\n0.05 0.1 0.2 0.4 0.8\n0204060Promoted \ndata (%)\n(d) Percentage of promoted\ndata (SALI)\n0.05 0.1 0.2 0.4 0.8\n0153045Storage \nincrease (%)(e) Percentage of storage\nspace increase (SALI)\n0.05 0.1 0.2 0.4 0.8\n0204060Node \nreduction (%)(f) Percentage of index\nmodel reduction (SALI)\n0.05 0.1 0.2 0.4 0.8\n0255075Promoted \ndata (%)\n(g) Percentage of promoted\ndata (ALEX)\n0.05 0.1 0.2 0.4 0.8\n04812Storage \nincrease (%)(h) Percentage of storage\nspace increase (ALEX)\n0.05 0.1 0.2 0.4 0.8\n04812Node \nreduction (%)(i) Percentage of index\nmodel reduction (ALEX)\nFigure 8: Space cost vs. smoothing threshold ğ›¼\nTable 3: CSV Pre-processing Time (s) for LIPP\nğ›¼ 0.05 0.1 0.2 0.4 0.8\nFacebook 589 1,194 1,859 2,106 2,228\nCovid 304 337 343 337 336\nOSM 1,217 2,329 4,495 7,983 13,019\nGenome 1,155 2,174 4,616 9,316 15,709\nTable 4: CSV Pre-processing Time (s) for ALEX\nğ›¼ 0.05 0.1 0.2 0.4 0.8\nFacebook 247 889 4,123 17,508 48,737\nCovid 609 1,423 2,795 4,463 4,955\nOSM 988 2,297 9,526 33,097 81,620\nGenome 1,356 2,902 6,253 8,854 9,777\nconstructing a parallel index structure with CSV. Once the CSV-\noptimised structure is ready, it is switched on for query processing.\n6.2.2 Impact of Dataset Cardinality. To demonstrate the scalability\nof CSV against the dataset cardinality, we repeat the experiments on\ndatasets of 12.5 million to 200 million data points. Fig. 9 shows thetotal query times saved by applying CSV. For all datasets, the times\nsaved grow with the dataset cardinality, with faster growth being\nobserved on the easier datasets (Facebook and Covid) grows faster.\nThis is because there are not many keys in the lower levels for\nthese datasets when the dataset cardinality is small. These results\nconfirm the scalability of CSV towards dataset cardinality.\n6.3 Results on Read-write Workloads\nDue to the similar trends between LIPP and SALI indexes, SALI is\nomitted from the results below for brevity. The default smoothing\nthreshold of 0.1 was used for these experiments.\nQuery time (promoted data). Figs. 10a and 10d show the total\nquery times saved by CSV for LIPP and ALEX, respectively, com-\npared to the original index structures, as more batches of data are\ninserted (recall that each batch consists of 0.1ğ‘›data points). Here,\nthe query times saved are decreasing slightly as more data points\nare inserted for LIPP, because the inserted data points have a higher\nchance of colliding with the promoted data points which are now\nin the upper levels, compared to when they are in lower levels as\nin the original index structure. For ALEX, the trend is quite similar\nexcept for on the OSM dataset, where there are two drops after\none and three insertion batches (i.e., 0.1ğ‘›and0.3ğ‘›data points are\ninserted). This is because the original index structureâ€™s query times\nhappen to be slightly lower in these two cases.\n\nArvix, Oct, 2025 Kasun Amarasinghe, Farhana Choudhury, Jianzhong Qi, and James Bailey\n12.5M 25M 50M 100M 200M\nDataset size0.00.51.01.5T otal time \nsaved (ns)Ã—108\n(a) LIPP\n12.5M 25M 50M 100M 200M\nDataset size0123T otal time \nsaved (ns)Ã—108 (b) SALI\n12.5M 25M 50M 100M 200M\nDataset size0246T otal time \nsaved (ns)Ã—107 (c) ALEX\nFigure 9: Total time saved vs. dataset cardinality\n00.1 0.2 0.3 0.4 0.5\nData insertion (Ã—n)0.00.40.81.2T otal time \nsaved (ns)Ã—108\n(a) Total query time saved (LIPP)\n00.1 0.2 0.3 0.4 0.5\nData insertion (Ã—n)036912Storage \nincrease (%) (b) Storage increase (LIPP)\n0.1 0.2 0.3 0.4 0.5\nData insertion (Ã—n)8\n08Insert \ntime (%) (c) Insertion time increase (LIPP)\n00.1 0.2 0.3 0.4 0.5\nData insertion (Ã—n)0.01.53.04.5T otal time \nsaved (ns)Ã—107\n(d) Total query time saved (ALEX)\n00.1 0.2 0.3 0.4 0.5\nData insertion (Ã—n)0.50\n0.25\n0.000.250.50Storage \nincrease (%) (e) Storage increase (ALEX)\n0.1 0.2 0.3 0.4 0.5\nData insertion (Ã—n)30\n15\n01530Insert \ntime (%) (f) Insertion time increase (ALEX)\nFigure 10: Performance results vs. data insertions\nIndex size. The index size overhead decreases after each batch\nof insertions, as shown in Figs. 10b and 10e. This is because the\ninitial gaps left by the virtual points are gradually filled up by\nthe inserted points, hence improving the overall space utilisation.\nAgain, the index size overhead is at or below 10%, emphasising the\nspace efficiency of CSV. For ALEX index, the storage increase is\nalmost negligible ( <0.5%). In some cases, the storage size of the\nCSV-enhanced ALEX is even lower than the original index, as the\noriginal ALEX index may need to create more new nodes to host\nthe insertions which outweighs the space overhead of CSV.\nInsertion time. Figs. 10c and 10f show the average insertion\ntimes of the CSV-enhanced indexes relative to those of the original\nindexes. Using CSV helps improve the insertion times in some\ncases because the gaps left by the virtual points are reused for\ninsertions. CSV could also lead to higher insertion times in other\ncases. This could be attributed to the fact that there are more keys\nat the upper levels of the CSV-enhanced indexes which may lead to\nmore collisions with the insertions, which requires new index node\ncreation. Overall, the insertion times of the CSV-enhanced indexes\nare on par to the original indexes.7 Conclusion\nWe addressed the issue of index learning over data of complex\ndistributions by a CDF smoothing technique to modify the key set,\ninstead of developing yet another indexing function or structure.\nWe proposed an algorithm named CSV to utilize this technique\non existing hierarchical learned index structures, to improve the\nquery time for the keys in lower levels of these index structures.\nThe proposed algorithm is implemented on three recent learned\nindexes, which are evaluated on real-world datasets. The results\nshow significant query performance improvements, i.e., up to 34%,\nwith a controllable and low storage space overhead.\nReferences\n[1]Matthias Bachfischer, Renata Borovica-Gajic, and Benjamin IP Rubinstein.\n2022. Testing the Robustness of Learned Index Structures. arXiv preprint\narXiv:2207.11575 (2022). https://doi.org/10.48550/arXiv.2207.11575\n[2]Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\nDavid Lomet, and Tim Kraska. 2020. ALEX: An Updatable Adaptive Learned\nIndex. In SIGMOD . 969â€“984.\n[3]Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. 2020.\nTsunami: A learned multi-dimensional index for correlated data and skewed\nworkloads. Proceedings of the VLDB Endowment 14, 2 (2020), 74â€“86.\n[4]Yuquan Ding, Xujian Zhao, and Peiquan Jin. 2022. An Error-Bounded Space-\nEfficient Hybrid Learned Index with High Lookup Performance. In DEXA .\n216â€“228.\n\nLearned Indexes with Distribution Smoothing via Virtual Points Arvix, Oct, 2025\n[5]Martin Eppert, Philipp Fent, and Thomas Neumann. 2021. A Tailored Regression\nfor Learned Indexes: Logarithmic Error Regression. In aiDM . 9â€“15.\n[6]Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-index: A fully-dynamic\ncompressed learned index with provable worst-case bounds. Proceedings of the\nVLDB Endowment 13, 8 (2020), 1162â€“1175.\n[7]Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and Tim\nKraska. 2019. FITing-Tree: A Data-aware Index Structure. In SIGMOD . 1189â€“1206.\n[8]Jiake Ge, Boyu Shi, Yanfeng Chai, Yuanhui Luo, Yunda Guo, Yinxuan He, and\nYunpeng Chai. 2023. Cutting Learned Index into Pieces: An In-depth Inquiry\ninto Updatable Learned Indexes. In ICDE . 315â€“327.\n[9]Jiake Ge, Huanchen Zhang, Boyu Shi, Yuanhui Luo, Yunda Guo, Yunpeng Chai,\nYuxing Chen, and Anqun Pan. 2023. SALI: A Scalable Adaptive Learned Index\nFramework based on Probability Models. Proceedings of the ACM on Management\nof Data 1, 4 (2023), 258:1â€“258:25.\n[10] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2020. RadixSpline: a single-pass learned\nindex. In aiDM . 5:1â€“5:5.\n[11] Evgenios M. Kornaropoulos, Silei Ren, and Roberto Tamassia. 2022. The Price of\nTailoring the Index to Your Data: Poisoning Attacks on Learned Index Structures.\nInSIGMOD . 1331â€“1344.\n[12] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In SIGMOD . 489â€“504.\n[13] Hai Lan, Zhifeng Bao, J Shane Culpepper, Renata Borovica-Gajic, and Yu Dong.\n2023. A Simple Yet High-Performing On-disk Learned Index: Can We Have Our\nCake and Eat it Too? arXiv preprint arXiv:2306.02604 (2023). https://doi.org/10.\n48550/arXiv.2306.02604\n[14] Pengfei Li, Yu Hua, Jingnan Jia, and Pengfei Zuo. 2021. FINEdex: A fine-grained\nlearned index scheme for scalable and concurrent memory systems. Proceedings\nof the VLDB Endowment 15, 2 (2021), 321â€“334.\n[15] Pengfei Li, Hua Lu, Qian Zheng, Long Yang, and Gang Pan. 2020. LISA: A Learned\nIndex Structure for Spatial Data. In SIGMOD . 2119â€“2133.\n[16] Yaliang Li, Daoyuan Chen, Bolin Ding, Kai Zeng, and Jingren Zhou. 2021. A\npluggable learned index method via sampling and gap insertion. arXiv preprint\narXiv:2101.00808 (2021). https://doi.org/10.48550/arXiv.2101.00808\n[17] Li Liu, Chunhua Li, Zhou Zhang, Yuhan Liu, Ke Zhou, and Ji Zhang. 2023. A\nData-aware Learned Index Scheme for Efficient Writes. In ICPP . 28:1â€“28:11.\n[18] Christian E Lopez and Caleb Gallemore. 2021. An augmented multilingual Twitter\ndataset for studying the COVID-19 infodemic. Social Network Analysis and Mining\n11, 1 (2021), 102.\n[19] Baotong Lu, Jialin Ding, Eric Lo, Umar Farooq Minhas, and Tianzheng Wang. 2021.\nAPEX: a high-performance learned index on persistent memory. Proceedings of\nthe VLDB Endowment 15, 3 (2021), 597â€“610.\n[20] Ryan Marcus, Andreas Kipf, Alexander van Renen, Mihail Stoian, Sanchit Misra,\nAlfons Kemper, Thomas Neumann, and Tim Kraska. 2020. Benchmarking learned\nindexes. Proceedings of the VLDB Endowment 14, 1 (2020), 1â€“13.\n[21] Mayank Mishra and Rekha Singhal. 2021. RUSLI: Real-time Updatable Spline\nLearned Index. In aiDM . 1â€“8.\n[22] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020. Learn-\ning Multi-Dimensional Indexes. In SIGMOD . 985â€“1000.\n[23] Sachith Pai, Michael Mathioudakis, and Yanhao Wang. 2024. WaZI: A Learned\nand Workload-aware Z-Index. In EDBT . 559â€“571.\n[24] Varun Pandey, Andreas Kipf, Thomas Neumann, and Alfons Kemper. 2018. How\ngood are modern spatial analytics systems? Proceedings of the VLDB Endowment\n11, 11 (2018), 1661â€“1673.\n[25] Suhas SP Rao, Miriam H Huntley, Neva C Durand, Elena K Stamenova, Ivan D\nBochkov, James T Robinson, Adrian L Sanborn, Ido Machol, Arina D Omer, Eric S\nLander, et al .2014. A 3D map of the human genome at kilobase resolution reveals\nprinciples of chromatin looping. Cell159, 7 (2014), 1665â€“1680.\n[26] S2Geometry. 2024. The S2 Geometry Library . http://s2geometry.io/\n[27] Yufan Sheng, Xin Cao, Yixiang Fang, Kaiqi Zhao, Jianzhong Qi, Gao Cong, and\nWenjie Zhang. 2023. WISK: A Workload-aware Learned Index for Spatial Keyword\nQueries. Proceedings of the ACM on Management of Data 1, 2 (2023), 187:1â€“187:27.\n[28] Zhaoyan Sun, Xuanhe Zhou, and Guoliang Li. 2023. Learned Index: A Com-\nprehensive Experimental Evaluation. Proceedings of the VLDB Endowment 16, 8\n(2023), 1992â€“2004.\n[29] Chuzhe Tang, Youyun Wang, Zhiyuan Dong, Gansen Hu, Zhaoguo Wang, Minjie\nWang, and Haibo Chen. 2020. XIndex: A scalable learned index for multicore\ndata storage. In PPoPP . 308â€“320.\n[30] Peter Van Sandt, Yannis Chronis, and Jignesh M. Patel. 2019. Efficiently Searching\nIn-Memory Sorted Arrays: Revenge of the Interpolation Search?. In SIGMOD .\n36â€“53.\n[31] Zhonghua Wang, Chen Ding, Fengguang Song, Kai Lu, Jiguang Wan, Zhihu Tan,\nChangsheng Xie, and Guokuan Li. 2024. WIPE: A Write-Optimized Learned Index\nfor Persistent Memory. ACM Transactions on Architecture and Code Optimization,\n21, 2 (2024), 22:1â€“22:25.\n[32] Chaichon Wongkham, Baotong Lu, Chris Liu, Zhicong Zhong, Eric Lo, and\nTianzheng Wang. 2022. Are updatable learned indexes ready? Proceedings of the\nVLDB Endowment 15, 11 (2022), 3004â€“3017.[33] Jiacheng Wu, Yong Zhang, Shimin Chen, Jin Wang, Yu Chen, and Chunxiao Xing.\n2021. Updatable learned index with precise positions. Proceedings of the VLDB\nEndowment 14, 8 (2021), 1276â€“1288.\n[34] Shangyu Wu, Yufei Cui, Jinghuan Yu, Xuan Sun, Tei-Wei Kuo, and Chun Jason\nXue. 2022. NFL: Robust learned index via distribution transformation. Proceedings\nof the VLDB Endowment 15, 10 (2022), 2188â€“2200.\n[35] Guang Yang, Liang Liang, Ali Hadian, and Thomas Heinis. 2023. FLIRT: A Fast\nLearned Index for Rolling Time frames.. In EDBT . 234â€“246.\n[36] Jiaoyi Zhang and Yihan Gao. 2022. CARMI: A cache-aware learned index with a\ncost-based construction algorithm. Proceedings of the VLDB Endowment 15, 11\n(2022), 2679â€“2691.\n[37] Zhou Zhang, Pei-Quan Jin, Xiao-Liang Wang, Yan-Qi Lv, Shou-Hong Wan, and\nXi-Ke Xie. 2021. COLIN: A cache-conscious dynamic learned index with high\nread/write performance. Journal of Computer Science and Technology 36 (2021),\n721â€“740.",
  "textLength": 69132
}