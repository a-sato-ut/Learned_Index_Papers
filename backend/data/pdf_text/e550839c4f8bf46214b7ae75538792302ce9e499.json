{
  "paperId": "e550839c4f8bf46214b7ae75538792302ce9e499",
  "title": "Tao: A Learning Framework for Adaptive Nearest Neighbor Search using Static Features Only",
  "pdfPath": "e550839c4f8bf46214b7ae75538792302ce9e499.pdf",
  "text": "Tao: A Learning Framework for Adaptive Nearest\nNeighbor Search using Static Features Only\nKaixiang Yang\u0003Hongya Wang\u0003Bo Xu\u0003; Wei WangyYingyuan XiaozMing Du\u0003Junfeng Zhou\u0003\n\u0003School of Computer Science and Technology, Donghua University, China\nySchool of Computer Science and Engineering, University of New South Wales, Australia\nzSchool of Computer Science and Technology, Tianjin University of Technology, Tianjin, China\nykx@mail.dhu.edu.cn, fhywang, xubo, duming, zhoujf g@dhu.edu.cn, weiw@cse.unsw.edu.au, yyxiao@tjut.edu.cn\nAbstract —Approximate nearest neighbor (ANN) search is\na fundamental problem in areas such as data management,\ninformation retrieval and machine learning. Recently, Li et\nal. proposed a learned approach named AdaptNN to support\nadaptive ANN query processing. In the middle of query execution,\nAdaptNN collects a number of runtime features and predicts\ntermination condition for each individual query, by which bet-\nter end-to-end latency is attained. Despite its efﬁciency, using\nruntime features complicates the learning process and leads to\nperformance degradation.\nRadically different from AdaptNN , we argue that it is promis-\ning to predict termination condition before query exetution.\nParticularly, we developed Tao, a general learning framework for\nTerminating ANN queries A daptively using O nly static features.\nUpon the arrival of a query, Tao ﬁrst maps the query to a\nlocal intrinsic dimension (LID) number, and then predicts the\ntermination condition using LID instead of runtime features.\nBy decoupling prediction procedure from query execution, Tao\neliminates the laborious feature selection process involved in\nAdaptNN . Besides, two design principles are formulated to guide\nthe application of Tao and improve the explainability of the\nprediction model. We integrate two state-of-the-art indexing\napproaches, i.e., IMI and HNSW, into Tao, and evaluate the\nperformance over several million to billion-scale datasets. Ex-\nperimental results show that, in addition to its simplicity and\ngenerality , Tao achieves up to 2.69x speedup even compared to\nits counterpart, at the same high accuracy targets.\nIndex Terms —approximate nearest neighbor search, local in-\ntrinsic dimension, neural networks\nI. I NTRODUCTION\nNearest neighbor search is a fundamental problem in do-\nmains such as large-scale image search and information re-\ntrieval [38], [39], recommendation [13], entity resolution [22],\nand sequence matching [6]. Constrained by the curse of dimen-\nsionality, exact nearest neighbor search becomes unaffordable\nfor the rapidly increasing amount of unstructured data (im-\nages, documents and video clips etc) [57]. As a workaround,\napproximate nearest neighbor (ANN) search is widely used to\nprovide an appropriate tradeoff between accuracy and latency.\nCurrently, the quantization-based and graph-based ap-\nproaches are two mainstream ANN search paradigms [43].\nVector quantization shrinks database vectors into compact\ncodes based on various quantization methods, and reduces\ncomputation latency and memory requirements by nearly an\norder of magnitude [27]. Graph-based approaches [41] ﬁrst\nHongya Wang is the corresponding author.builds a graph to capture proximity among data points and\nthen performs ANN search by traversing graphs in a greedy\nor heuristic fashion. A number of empirical studies show that\ngraph-based approaches own most appealing tradeoff between\naccuracy and search cost [56].\nBoth quantization-based and graph-based approaches use\nﬁxed conﬁgurations that impose the same termination con-\ndition (e.g., the number of candidates to examine) for all\nqueries, which penalizes “easy” queries and incurs higher-\nthan-necessary running time. To address this issue, Li et\nal. proposed AdaptNN that predicts and then applies termi-\nnation condition for each individual query [35]. Figure 1(a)\nillustrates the workﬂow of AdpatNN . After receiving a query,\nAdpatNN ﬁrst invokes a speciﬁc ANN algorithm to execute\nthe query for a while, then collects a number of runtime\nfeatures and predicts the termination condition based on them.\nTaking the estimated termination condition as the additional\ninput, the ANN algorithm continues to run until the condition\nis met, and outputs the query results ﬁnally.\nHowever, AdaptNN has several limitations despite its efﬁ-\nciency:\n\u000fFirstly, the runtime features require heavy manual en-\ngineering, that is, these hand-crafted features need to\nbe designed for each algorithms individually and no\nsystematic design strategy is available. For example, it\nuses six input features for the HNSW index whereas\nadopts 14 runtime statistics for the IMI index. Hence, it is\nnot easy to adapt AdaptNN to other indexing approaches\nthat are not discussed in [35].\n\u000fSecondly, the end-to-end latency is sensitive to the time\ninterval between when AdaptNN makes the prediction\n(dubbed as prediciton time ) and when the query pro-\ncessing starts. [35] provides no principled method to set\nthe prediction time. As will be discussed in detail in\nSection III-B, the optimal prediction times depend on\nspeciﬁc algorithms and datasets, and the tuning procedure\nis very tedious. In view of this, AdaptNN simply sets\nit to some ﬁxed default value, leading to undesirable\nperformance degradation.\nBoth of the above-mentioned limitations are rooted in\nthe belief that only the runtime features provide sufﬁcient\nprediction power to the termination condition. In this paper, wearXiv:2110.00696v1  [cs.DB]  2 Oct 2021\n\nQuery\nPrediction\nModel 1LID ANN Search Output\nfeatures OutputANN Search\n----------------\nPhase 1Feature\nCollectionQuery\nPrediction\nModel 2\nPrediction\nModelANN Search\n----------------\nPhase 2TCTC(a)Workﬂow of AdaptNN\nQuery LIDANN Search Output\nfeatures OutputANN Search\n----------------\nPhase 1Feature\nCollectionQuery\nPrediction\nModel\nPrediction\nModelANN Search\n----------------\nPhase 2TCTCVector to LID \nMapping\n(b)Workﬂow of Tao\nFigure 1: Illustrations of Tao andAdaptNN\nchallenge this belief and propose a simple and principled static\nfeature to perform the prediction accurately. Speciﬁcally, we\npropose Tao, a general learning framework for T erminating\nANN queries A daptively using O nly static features. Fig-\nure 1(b) illustrates the workﬂow of Tao. Upon the arrival of a\nquery, Tao ﬁrst maps the query to a local intrinsic dimension\n(LID) number, and then predicts termination condition using\nLID only before query execution. The speciﬁc ANN search\nalgorithm takes termination conditions as inputs and answers\nthe query until the condition is met.\nAs such, Tao has some advantages worth mentioning:\n\u000fTao decouples the prediction procedure from query ex-\necution and eliminates the requirement for tuning the\nprediction time, making it simple to use in practice.\n\u000fTao employs query vectors and the associated LID,\ninstead of a set of hand-crafted runtime features, as\nmodel inputs to accomplish the goal of adaptive query\nprocessing, making it general enough to accommodate\nnew ANN search algorithms\n\u000fTao improves the explainability of the prediction model\nby revealing the correlation between LID and search cost\nand formulating guidelines to use Tao, which is a highly\ndesirable feature in AI-powered applications nowadays.\nTo demonstrate the aforementioned advantages, we instan-\ntiated two instances of our framework based on two state-\nof-the-art indexing approaches (HNSW [41] and IMI [3]).\nSimilar with [35], we implement them over the Faiss similarity\nsearch library [29]. Comprehensive experiments are conducted\nto evaluate the end-to-end performance on a collection of real\ndatasets, including two billion-scale datasets Deep1B [4] and\nSift1B [28]. Empirical results demonstrate that, in addition to\nits simplicity and generality, Tao offers up to 2.69x speedup\ncompared with the runtime prediction strategy.\nThe contributions of the paper are summarized as follows:\n(1) We identify the limitations of the existing approach.\n(2) We propose and develop a general learning framework\nfor adaptive ANN search using only static features. (3) We\nconduct extensive experiments on various datasets to verify\nthe effectiveness and efﬁciency of our framework.\nRoadmap. Section 2 presents the necessary preliminaries.\nSection 3 gives a brief review of AdaptNN and discusses its\nlimitations. Section 4 demonstrates the feasibility of predicting\nwith static features. Section 5 sketches the workﬂow of Tao.\nSection 6 describes the experimental methodology and reportsthe results. Section 7 reviews the related work, and Section 8\nconcludes the paper.\nII. P RELIMINARIES\nIn this section, we present necessary preliminaries for ANN\nsearch and relevant indexing algorithms used in this paper.\nA. ANN Search Problem\nNearest neighbor (NN) search in high dimensional spaces\nis a challenging problem that has been studied extensively in\nthe past two decades. Formally, let D=fo1;\u0001\u0001\u0001;ong\u001aRd\nbe a ﬁnite set of nvectors. Given query q2Rd,kNN search\nreturnskresultsoi(1\u0014i\u0014k), whereoiis thei-th nearest\nneighbor of q. In this paper we use the Euclidean distance\nas the distance function to measure (dis)similarity between\nvectors.\nAs database sizes reach millions or billions of entries, and\nthe vector dimension grows into the hundreds, any indexing\nmethod is shown to be reduced to a linear scan of the whole\ndataset, which is prohibitively expensive in practice [28].\nTherefore, practitioners often resort to approximate nearest\nneighbor search. Instead of identifying exact NNs of a query\nsurely, ANN search sometimes may return neighbors that are\nonly close enough to the query. By trading precision for\nefﬁciency, ANN search provides better latency than the exact\nversion.\nThere is a large amount of signiﬁcant literature on algo-\nrithms for approximate nearest neighbor search, which are\nroughly divided into four categories: tree-structure based ap-\nproaches, hashing-based approaches, quantization-based ap-\nproaches and graph-based approaches. In this paper, we focus\non quantization-based and graph-based methods due to their\nappealing performance and popularity in real-life large scale\napplications [55].\nB. Product Quantization and Inverted Multi-Index\nProduct quantization (PQ) algorithm is a lossy compression\nquantization algorithm that is developed on the basis of vector\nquantization [27]. By quantizing the vectors of the dataset,\nit can effectively reduce the storage needed to store the\noriginal vectors of the dataset. To the best of our knowledge,\nquantization-based method is the only one that can support\nbillion-scale datasets on a commodity computer. However, the\nsearch is exhaustive for the original PQ – all database vectors\nmust be evaluated.\n\nTo avoid exhaustive search, IVFADC [27] uses two quanti-\nzation levels to organize large datasets. Particularly, IVFADC\ngroups database vectors into different clusters. When building\nthe index, a list of cluster centroids is trained by K-means\nclustering, and each database vector is assigned to the cluster\nwith the closest centroid. During searching, the index ﬁrst\ncomputes the distances between the query and all cluster\ncentroids, then evaluates database vectors belonging to the ﬁrst\nnprobe nearest clusters, where nprobe is the hyperparameter\nthat should be determined apriori .\nA number of variants of IVFADC are proposed, among\nwhich Inverted Multi-Index (IMI) [3] offers the state-of-\nthe-art performance thanks to its efﬁcient space-partitioning\nstrategy. To be speciﬁc, IMI decomposes the vectors into\nseveral subspaces and trains separate a list of centroids in each\nsubspace, which leads to a ﬁne-grained space partition.\nFor example, Figure 2 illustrates how IMI generates two\nproduct codebooks C1andC2for different halves of the vector\nat the ﬁrst level. Each codebook contains 5sub-codewords,\nleading to 52clusters in total. By subtracting a vector to\nthe corresponding cluster centroids, one obtains the residuals\nfor the second quantization level. Similar to IVFADC, IMI\nsearches the same ﬁxed number of nprobe nearest clusters for\nall queries.\n··\n·····\n·· ··· ···\n· ············\n·······\n····\n···\n··\n······\n···\n·················\n···\n····\n··\n······\n·······\n· ··\n··\n···············\n····\n···\n··\n······\n···\n·····\n··\n··· · ······\n···\n····\n····\n····\n·C1=5\nC2=5\nFigure 2: Illustration of IMI (black and red dots represent the\ntraining vectors and centroids respectively).\nC. Graph-based Methods\nGraph-based methods such as hierarchical navigable small\nworlds (HNSW) are currently one of the most efﬁcient ANN\nsearch paradigms [41]. HNSW employs hierarchical network\nstructures to organized vectors, and such networks are essen-\ntially approximate kNN-graphs [54]. To process a query, the\ngraph is traversed starting from the entry point using beam\nsearch, and the parameter efSearch is used to control the hops\nof graph traversal. Greater efSearch is, more accurate answers\nwill be.\nFor almost all graph-based methods, the ANN search proce-\ndure is based on the same principle as illustrated in Figure 3.\nFor a query q, start at an initial vertex chosen arbitrarily or\nusing some sophisticated selection rule, say p. Moves along\nan edge to the adjacent vertex with minimum distance to q.\nRepeat this step until the current element vis closer toqthan\nall its neighbors, and then report vas the NN of q.\npqvFigure 3: The search procedure of graph-based algorithms.\nIII. M OTIVATIONS\nA. Brief Review of AdaptNN\nMost existing ANN approaches use ﬁxed conﬁgurations\nthat apply the same termination condition to all queries.\nLi et al. conducted an empirical study over three datasets\n(Deep/Sift/Gist)1using IMI and HNSW implementations in\nthe Faiss library [35]. They observed that, due to the in-\ndex structures and the vector distributions, the number of\ndatabase vectors that must be searched to ﬁnd the ground-\ntruth nearest neighbor varies signiﬁcantly among queries.\nTake HNSW as an example, 80% of queries (dubbed “easy\nqueries”) only need to perform at most 547/481/1260 distance\nevaluations for Deep10M/Sift10M/Gist, respectively, while the\nother 20% queries (dubbed “hard queries”) require up to\n88696/16618/118277 distance evaluations, respectively.\nFixed conﬁgurations lead to high average latency because\neasy queries are forced to conduct a lengthy search as the\namount required to deliver reasonable performance for hard\nqueries. To address this issue, Li et al. propose to predict dif-\nferent termination condition for each individual query, making\neasy queries do less computation than hard ones, thus reducing\nthe end-to-end average latency dramatically.\nOne central proposition in AdaptNN is that static features\nalone (e.g., the query vectors themselves) cannot offer good\nprediction power. To realize highly accurate predictions, it\nelaborately identiﬁes a set of runtime features of the interme-\ndiate search results, and it adopts gradient boosting decision\ntrees to predict the minimum amount of search to perform. As\nan example, AdaptNN uses the distances between the query\nand its 1 stand 10 thneighbors as two runtime features for\nHNSW.\nFigure 1(a) depicts the workﬂow of AdaptNN . When\nreceiving a query, AdaptNN ﬁrst invokes the ANN search\nalgorithm and performs a ﬁxed amount of search to obtain the\nintermediate search results. Thereafter, AdaptNN computes\nthe runtime features, coalesces them with static features, and\npredicts the termination condition using gradient boosting\ndecision trees. The termination condition is passed down to\nthe ANN search algorithm, which continues to run until the\ncondition is met. Experimental results show that adaptive\nquery processing based on runtime prediction consistently\nreduces the average end-to-end latency.\n1The detailed description of these datasets are given in Section VI-A\n\nTable I: Prediction time vs. Search cost (prediction times are measured in terms of the recall already obtained and the optimal\nsearch costs are marked in bold)\nSearch Cost\nPrediction Time Deep10M Sift10M Gist ImageNet Msong Trevi Glove\n0.5 18,483 11,017 37,334 71,897 5,337 30,621 60,190\n0.6 22,821 9,453 34,072 86,030 4,521 38,213 67,465\n0.7 18,882 10,481 34,508 87,176 8,818 22,895 66,905\n0.8 17,594 16,787 44,390 78,641 7,182 22,891 66,406\n0.9 17,033 10,616 47,442 122,697 7,090 28,573 108,983\nB. Limitations\nAdaptNN , however, faces a number of limitations. Firstly,\nthe input features are chosen in an ad-hoc fashion and one has\nto try different hand-crafted features for different algorithms.\nFor example, HNSW uses six input features whereas IMI\nadopts 14 runtime statistics to predict the minimum amount of\nsearch, respectively. While the importance of different features\nare evaluated using the per-feature gain stats from gradient\nboosting decision tree models (the importance of a feature\nis proportional to the total error reduction contributed by the\nfeature), this only alleviate the feature engineering efforts and\ndoes not address issues such as correlation among features.\nHand-crafted feature engineering impedes the application of\nAdaptNN to other ANN search algorithms not addressed\nin [35].\nSecondly, as noted in [35], if AdaptNN searches less\nbefore the feature generation, the intermediate result features\nmay provide less information gain, reducing the prediction\naccuracy. If we search more before the feature generation, all\nqueries must search more, increasing the end-to-end average\nlatency. As will be discussed shortly, our preliminary experi-\nments show that the end-to-end latency is quite sensitive to the\nprediction time. Unfortunately, again, AdaptNN provides no\nprincipled method or guideline to determine when to predict\nin runtime.\nThe pragmatic (currently practiced) way to choose the\nprediction time adopted by AdaptNN is as follows. Given a\ndataset, manually choose a set of accuracy targets, say 0:5\u00000:9\nwith a step of 0:1, and then perform binary search to obtain\nthe corresponding search costs ( nprobe orefSearch ) for all\naccuracy targets. To determine the optimal one, one has to\nrunAdaptNN using different prediction times over training\ndatasets and pick the one with the least average latency. Since\nthe tuning process is tedious, time-consuming and lossy in\nnature, AdaptNN simply uses a ﬁxed prediction time, i.e.,\nthe time instant in which 0.8 recall is reached for all datasets\nby default in practice [35].\nUnsurprisingly, we found that the best prediction time is\nobviously data-dependent. Table I lists the total number of\npoints examined (search cost) under different prediction times\n(recall already obtained) for a collection of datasets2using\nAdaptNN with the HNSW method. As one can see 1) the\noptimal prediction time vary across different datasets and, 2)\n2The detailed description of these datasets are given in Section VI-Achoosing inappropriate prediction time will cause signiﬁcant\nperformance degradation. For example, the maximum search\ncost is twice as much as the minimum for Msong.\nThe limitations of AdaptNN motivate us to look for a radi-\ncally different way to perform adaptive ANN query processing.\nIt is claimed in [35] that “ static features such as the query\nvector itself are not sufﬁcient to predict this termination con-\ndition ”. Counter-intuitively, however, we observe that query\nvectors themselves, with the help of an intermediate feature\nlocal intrinsic dimension , are sufﬁcient to fulﬁll this goal.\nNext, we will present the core ideas and workﬂow of Tao.\nIV. P REDICTION WITH STATIC FEATURES MADE POSSIBLE\nIn this section, we introduce the core notion of our method,\ni.e., local intrinsic dimension ﬁrst, and then explore the\nfeasibility of predicting LID using a regression model. The\ncorrelation between LID and search cost is examined based\non two representative indexing schemes, i.e., IMI and HNSW.\nA. Local Intrinsic Dimension\nMany learning tasks involve data represented as vectors\nof dimension d. For example, an image representation is an\nembedding function that transforms the raw pixel represen-\ntation of the image to a point in a high-dimensional vector\nspace. While data are embedded in the space Rd, this does\nnot necessarily mean that its intrinsic dimension (ID) is d.\nThe intrinsic dimensionality of a representation refers to\nthe minimum number of parameters (or degrees of freedom)\nnecessary to capture the entire information present in the\nrepresentation [8]. Equivalently, it refers to the dimensionality\nof them-dimensional manifold Membedded within the d-\ndimensional ambient (representation) space where m\u0014d.\nID has wide applications in many machine learning and\ndata mining contexts. For example, most dimension reduction\ntechniques require that a target dimension be provided by\nthe user. Ideally, the supplied dimension should depend on\nthe intrinsic dimensionality of the data. This has served to\nmotivate the development of models of ID, as well as accurate\nestimators.\nOver the past few decades, many practical models of the\nintrinsic dimensionality of datasets have been proposed. Exam-\nples include the Principal Component Analysis and its variants\n[7], as well as several manifold learning techniques [31].\nTopological approaches to ID estimate the basis dimension\nof the tangent space of the data manifold from local samples.\n\nFractal methods such as the Correlation Dimension estimate an\nintrinsic dimension from the space-ﬁlling capacity of the data\n[9]. Graph-based methods use the k-nearest neighbors graph\nalong with density in order to estimate ID [11].\nThe aforementioned intrinsic dimensionality measures can\nbe described as ‘global’, in that they consider the dimension-\nality of a given set as a whole, without any individual object\nbeing given a special role. In other words, all vectors share\nthe same intrinsic dimension. In contrast, ‘local’ ID measures\nare deﬁned as those that involve only the k-nearest neighbor\ndistances of a speciﬁc location in the space.\nSeveral local intrinsic dimensionality models have been\nproposed recently, such as the expansion dimension [30], the\ngeneralized expansion dimension [25], the minimum neighbor\ndistance [45], and local continuous intrinsic dimension (LID)\n[24]. In this paper we focus on LID, which is deﬁned formally\nas follows:\nDeﬁnition 1: ( [24].) Given an absolutely continuous random\ndistance variable X, for any distance threshold xsuch that the\ncumulative density function FX(x)>0, the local continuous\nintrinsic dimension of Xat distancexis given by\nLIDX(x)4= lim\n\u000f!0+lnFX((1 +\u000f)x\u0000lnFX(x))\nln (1 +\u000f)\nwherever the limit exists.\nLID quantiﬁes intrinsic dimension in terms of the rate at\nwhich the number of encountered objects grows as the con-\nsidered range of distances expands from a reference location.\nEstimates of local ID constitute a measure of the complexity\nof data and LID could give researchers and practitioners more\ninsight into the nature of their data, and therefore help them\nimprove the efﬁciency and efﬁcacy of their applications [47].\nAmsaleg et al. studied several estimators of LID using\nmaximum likelihood estimation (MLE), the method of mo-\nment, probability weighted moment and regularly varying\nfunctions [1]. Experimental results show that the performance\nof different estimators to be largely in agreement with one\nanother, and faster initial convergence favors the choice of\nMLE for applications where the number of available query-\nto-neighbor distances is limited, or where time complexity is\nan issue.\nAssume that we are given a sequence x1,...,xnof obser-\nvations of a random distance variable Xwith support [0, w) in\nascending order, that is, x1\u0014x2\u0014···\u0014xn. Then, the MLE\nestimator of LID can be calculated as\n[LIDX4= \n1\nkkX\ni=1ln\u0012w\nxi\u0013!\u00001\n(1)\nB. Vector to LID Mapping\nBy deﬁnition, the LID number of a vector qis closed\nrelated to the distribution of its near neighbors, which suggests\nthat LID might be a promising indicator for the difﬁculty in\nﬁndingkNN ofq. Unfortunately, for an unsee query, there\nis no way to calculate the LID value without knowing its\n1.23.45.57.49.211.112.714.415.616.917.80\n246810121416182022024681012141618Predicted LIDR\neal LID(a)Deep10M\n3.55.47.59.411.312.914.515.816.82\n468101214161820024681012141618Predicted LIDR\neal LID (b)Sift10M\nFigure 4: Real vs. Predicted LID on Deep10M and Sift10M.\nTable II: MAE, RMSE and R2Score for different datasets.\nMAE RMSE R2Score\nDeep10M 1.15 1.88 0.68\nSift10M 0.65 0.89 0.89\nGist 1.24 1.72 0.89\nImageNet 0.77 1.07 0.91\nGlove 1.80 2.48 0.88\nMSong 0.67 1.22 0.80\nTrevi 3.17 4.41 0.82\nkNN. To circumvent this dilemma, we explore the possibility\nof predicting LID for unseen vectors using neural networks,\nconsidering they are extremely good at capturing complex\nrelationships in high dimensional spaces [33].\nSuppose datasetDis a sample drawn from a population\nthat follows some unknown probability distribution X. For\nany pointo\u0018 X , we can calculate the LID estimate of o\nusing Equation (1), where xiis the distance between oand\nitsi-th NN inD. For unseen vectors, assume there exists a\nfunctionf(\u0001)that relates any sample vector drawn from X\nto a single LID value. The universal approximation theorem\ntells us that a neural network can approximate f(\u0001), given that\nf(\u0001)is continuous on a compact set and the neural network\nhas sufﬁciently many hidden neurons with activation functions\n[12], [23].\nVerifying the continuity of f(\u0001)analytically is impossible\nbecause we do not even have an closed-form expression for\nf(\u0001). To this end, we demonstrate the feasibility by empirically\nevaluating the approximation errors on several real datasets.\nParticularly, we trained an MLP network with two hidden\nfully-connected layers, using 200 neurons per layer (i.e., 200\nwidth) and ReLU activation functions. The inputs to the neural\nnetwork are exclusively the original vectors and the outputs\nare the predicted LID values. We use the training vectors listed\nin Table III to generate training data and the query vectors to\ngenerate testing data.\nTable II summarizes the statistics of common evaluation\nmetrics for regression models. For mean absolute error (MAE)\nand RMSE, lower is better. For R2 scores, higher is better. As\nwe can see from the table, we can reliably estimate the LID\nof query vectors using our regression model. E.g., the MAE\nof the prediction is less than 1.24 in 6 out of the 8 datasets.\nWe formulate this observation as Property 1:\nProperty 1: Given a datasetD, the LID values of high-\ndimensional vectors in Dcan be estimated using a practical\nregression model with small approximation error.\n\nWe further illustrate the real and the predicted LID values in\nFigure 4 over Deep10M and Sift10M. The x-axis represents\nthe LID numbers of vectors calculated using Equation (1),\nwhich are partitioned into bins of size 2. The average of\npredicted LID values in each bin are marked on the top of\nbars. As we can see, the estimates match rather well with the\nreal values for small LID ( 0\u000014for Deep10M and 2\u000016\nfor Sift10M). For large LID, the regression model somewhat\nunderestimates the target values, mainly due to the sparsity\nof training vectors with large LID numbers. Please note that\nsimilar trends are observed for other datasets and we do not\nreport them here due to space limitation.\nWe also adopt gradient boosting decision tree models (using\nthe LightGBM library [32]) to perform the vector to LID\nmapping, which achieves similar results with MLP. This sug-\ngests that there exists inherent correlation between vectors and\nLID for all real datasets that we have experimented with, and\nsuch correlation may be model independent to some extent.\nWe conjecture that the reason might be these datasets, while\nrepresented in high-dimensional spaces, inherently resides on\nsome low-dimensional manifolds [2], [21]. As a result, the\nrelation between vectors and LID can be easily captured using\na reasonable regression model. We believe that more in-depth\nanalysis of this correlation deserves further study on its own\nright, and is thus out of the scope of this paper.\n5.86.126.77.27.78.38.79.39.910.92\n46810121416182022024681012LOG(Min Amount of Search)R\neal LID\n(a)Deep10M\n5.86.26.87.27.88.48.99.69.62\n468101214161820024681012LOG(Min Amount of Search)R\neal LID (b)Sift10M\nFigure 5: Search cost vs. LID for HNSW.\n12k14k16k22k23k29k41k55k88k108k138k0\n246810121416182022020000400006000080000100000120000140000Min Amount of SearchR\neal LID\n(a)Deep1B\n3.4k6.9k6k6.4k7.9k11k18k22k47k2\n46810121416182001000020000300004000050000Min Amount of SearchR\neal LID (b)Sift1B\nFigure 6: Search cost vs. LID for IMI.\nC. Correlation between LID and Search Cost\nLID is aimed to quantify the local intrinsic dimensionality of\na feature space exclusively in terms of the distance distribution\nof neighbors of a speciﬁc location. It is well known that\nthe dimensionality of the space that the point resides in has\na signiﬁcant impact on the difﬁculty of NN search [20].\nIntuitively, the overhead of NN search is probably correlated\nwith the LID of query vectors.The way to measure search cost varies over different in-\ndexing methods. For IMI, the amount of search is represented\nby the number of nearest clusters that need to be examined\n(nprobe ). For HNSW, we use the number of distance evalu-\nations to represent the amount of search. This is because 1)\nThe distance evaluation between query and database vector is\na time-consuming task; 2) The number of distance evaluations\nﬂuctuates greatly even with the same number of hops in the\ngraph ( efsearch ).\nFigure 5 illustrates the mean minimum search cost to ﬁnd\nthe ground truth nearest neighbor for query vectors with\ndifferent LID using HNSW. The bin size is 2 and the search\ncost is averaged over each bin. A base 2 logarithmic scale\nis used for y-axis. As one can see, for both Deep10M and\nSift10M, the search cost increases roughly exponentially as\nLID grows, suggesting that LID is a promising static feature\nto predict the termination condition.\nFigure 6 shows the histogram of the mean minimum number\nof centroids examined to ﬁnd the ground truth nearest neighbor\nusing IMI. The LID bin size is set to 2 and the search cost\nis averaged over each bin. As with HNSW, there exists a\nstrong correlation between LID and the number of centroids\nexamined. It is worth mentioning that IMI and HNSW ex-\nhibit similar trends for the remaining datasets that have been\nexperimented with, which are not reported here due to space\nlimitation. We highlight such correlations as Property 2:\nProperty 2: Given an ANN search algorithm Aand a dataset\nD, the LID values of query vectors are positively correlated\nwith the minimum amount of search to identify the true nearest\nneighbors inDbyA.\nD. Remarks\nAs discussed above, Property 1 is data dependent and Prop-\nerty 2 depends on both data and algorithms. Given an ANN\nsearch algorithm and a dataset, it is possible to accomplish\nthe goal of adaptive query processing as long as these two\nproperties hold. Such simple guidelines turn the learning black\nbox transparent to users to some extent, eliminate the hand-\ncrafted feature selection process and ease the applications of\nTao to other ANN algorithms and datasets.\nV. A DAPTIVE ANN S EARCH WITH TA O\nIn this section, we describe the prediction pipeline of the\nproposed learning framework and how ANN search algo-\nrithms, exempliﬁed by IMI and HNSW, are integrated into\nTao.\nThe general workﬂow. Our prediction pipeline consists of\ntwo phases as illustrated in Figure 1. In Phase 1, the regression\nmodel takes query vectors as inputs and outputs predicted LID\nnumbers. In Phase 2, the other regression model accepts a LID\nvalue and reports a numerical indicator, suggesting how much\nsearch should be done for the query.\nThe inputs. Instead of combining a number of hand-crafted\nstatic and runtime features as inputs, Tao needs only a single\nindependent variable, that is, the query vector itself.\n\nThe output. For each query, we expect to predict the\nminimum amount of search to obtain the ground truth nearest\nneighbor. Different indexing approaches may have different\nmetrics to quantify the search cost, but what we need is often\na numerical value which is proportional to the search latency.\nModel Selection and Training. According to the two\nproperties we formulated, Tao is actually model independent,\nmeaning that any reasonable regression model can be used\nto fulﬁll the vector !LID!termination-condition prediction\nframework. In this paper, we choose two popular models,\ni.e., MLP and gradient boosting decision trees, to show the\neffectiveness of Tao. For MLP, we employ two distinct neural\nnetworks to fulﬁll the predictive tasks in different phases,\nwhere the standard feed forward structure with two fully-\nconnected hidden layers is adopted and the ReLU activation\nfunction is used across all layers. The parameters of these two\nneural networks will be discussed in detail in Section VI-B.\nFor the second one, we build and train two distinct gradient\nboosting decision tree models using the LightGBM library [32]\nto accomplish the predictive tasks needed by Tao. Since the\nprediction performance of LightGBM is similar to that of\nMLP, we only report the experimental results using MLP in\nthis paper due to space limitation.\nWe use the training vectors in Table III to generate train-\ning/validation data and use the query vectors to generate\ntesting data. Each vector generates one row of data which\nincludes both the output target value and the true LID value.\nTo obtain output target values, we need to ﬁrst perform an\nexhaustive search to ﬁnd the ground truth nearest neighbor(s),\nand then ﬁnd the minimum amount of search to reach (one\nof) it. To calculate LID, we identify the top-1000 neighbors\nof each vector in the training set and compute LID values\nusing Equation (1). We trained the two neural networks\nindependently, i.e., no joint learning techniques are used. For\nthe ﬁrst one, we choose a ﬁxed number of training epochs\n(200) and batch sizes varying from 200 to 1000, depending\non the sizes of training sets. For the second one, a smaller\nnumber of training epochs is used (20) and batch sizes are the\nsame as the ﬁrst one.\nIntegration with IMI and HNSW. The integration of Tao\nwith the existing indexing algorithms such as IMI and HNSW\nis noninvasive, in that very few modiﬁcations have to be done\nwith them. Particularly, since Tao decouples the prediction\nmodels from query execution, we only need to change the\ntermination conditions in the code bases, involving less than\n5 lines of the core codes for IMI and HNSW.\nFor IMI, the number of nearest clusters to search equals\nMax(thresh ,multiplier *2TC), where the thresh equals the max-\nimum target value in the training data and TCis the predicted\nsearch cost. For HNSW, the number of distance evaluations\nequals Max(thresh ,multiplier *2TC). Similar to [35], multiplier\nis the scale factor needs to be tuned in order to achieve given\naccuracy target.\nAlgorithm 1 sketches how Tao is integrated with HNSW.Algorithm 1 Tao with HNSW\nInput: Query vector: q,\nEntry point: ep,\nNumber of nearest neighbors to return: k,\nCandidate queue: cq,\nMaxheap list: w\nOutput:knearest neighbors to q\ntc predict with Tao\nndis 0//number of distance comparisons\ncq:pushback(ep)\nwhilejcqj>0do\nv=cq:top ()\ncq:pop ()\nforv:neighbors do\ncalcultate distances and update cq;w\nndis + +\nend for\nifndis>tc then\nbreak\nend if\nend while\nreturnknearest neighbors to qinw\nVI. E VALUATION\nIn this section, we evaluate the performance of Tao by\nimplementing it in the Faiss ANN search library (CPU version)\n[35] similar to AdaptNN , which makes the comparison fair\nand reasonable. All experiments are carried out on a server\nwith E5-2620 v4@2.10GHz CPU and 256GB memory.\nA. Datasets\nWe used seven publicly available real datasets, which are\nof different data dimensions, cardinalities and types. Table III\nlists the statistics of datasets we have experimented with. The\nsizes of these datasets range from millions to one billion.\nTable III: Dataset Summary\nDataset Dim. Base Training Query Type\nDeep 96 10M,1B 1M 10,000 Image\nSift 128 10M,1B 1M 10,000 Image\nGist 960 1M 0.5M 1,000 Image\nImageNet 150 2.34M 200,000 10,000 Image\nMsong 420 9.22M 100,000 10,000 Audio\nTrevi 4096 1M 20,000 1,000 Image\nGlove 100 1.19M 100,000 10,000 Text\n\u000fDeep1B3consist of 1 billion contains deep neural codes\nof natural images obtained from the activations of a\nconvolutional neural network.\n\u000fDeep10M4is a subset of Deep1B.\n\u000fSift1B5consist of 1 billion 128-dim SIFT vectors.\n3https://github.com/facebookresearch/faiss/tree/master/\n4https://github.com/facebookresearch/faiss/tree/master/\n5http://corpus-texmex.irisa.fr/\n\n\u000fSift10M6consists of 10 million 128-dim SIFT vectors.\n\u000fGist7is an image dataset which contains about 1 million\ndata points.\n\u000fImageNet8contains about 2.4 million data points with\n150 dimensions dense SIFT features.\n\u000fMsong9is a collection of audio features and metadata\nfor millions of contemporary popular music tracks.\n\u000fTrevi10consists of around 1,000,000 4096- dvectors.\n\u000fGlove11contains 1.2 million 100- dword feature vectors\nextracted from Tweets.\nFor ImageNet/Msong/Trevi/Glove datasets, due to these\ndatasets do not have a training set or the query set is small, so\nwe split a subset of the database as the training set or query\nset. Taking ImageNet as an example, we split the ﬁrst 200,000\nof the database into a training set, then we split 200,001 to\n210,000 of the database into a query set, and the rest as the\nnew ImageNet base set.\nB. Benchmark Methods\n\u000fFixed Conﬁguration. The implementations of IMI and\nHNSW in the Faiss ANN search library (CPU version)\n[35] is used. Parameters to control the answer quality,\nthat is, efSearch (HNSW) and nprobe (IMI), are tuned\nto achieve the given target accuracy, and ﬁxed for all\nqueries.\n\u000fAdaptNN .We use the training and parameter tuning\nmethods described in [35] to obtain the optimal prediction\ntime, which is both algorithm and dataset dependent.\nIn other words, we report the best end-to-end search\nperformance that AdaptNN can deliver for any given\nalgorithm and dataset.\n\u000fTao.Tao employs two neural networks as the regression\nmodels. Both neural networks employ two fully con-\nnected layers with ReLU activation functions. For the ﬁrst\none, 200 neurons are used for each fully connected layer.\nThe second neural network uses 10 neurons for each fully\nconnected layer.\n\u000fVector Only (VO). VO combines the two neural net-\nworks of Tao into one single MLP with ﬁve hidden\nlayers. To make fair comparison, the structure of VO is\nthe same as those of Tao, i.e., ﬁve hidden layers are fully\nconnected and the number of neurons in each layer are set\nto 200, 200, 1, 10, 10 respectively. The only difference\nbetween VO and Tao is the training process – LID is\nnot used explicitly in VO and the input and output of the\nneural network of VO are query vectors and the amount\nof search to ﬁnd the true NN, respectively.\nC. Performance Metrics\nThe end-to-end latency and accuracy are two important\nmetrics for evaluating ANN algorithms. To compare the per-\n6https://archive.ics.uci.edu/ml/datasets/SIFT10M\n7http://corpus-texmex.irisa.fr/\n8http://cloudcv.org/objdetect/\n9http://www.ifs.tuwien.ac.at/mir/msd/download.html\n10http://phototour.cs.washington.edu/patches/default.htm\n11http://nlp.stanford.edu/projects/glove/\nDeep1BS ift1B14166425610244096Latency(ms)D\nataset IMI \nAdaptNN \nTaoFigure 7: Latency for plain IMI, AdaptNN andTao.\nformance of the baselines and proposed approach, we perform\ncontrolled experiments to keep the accuracy achieved by all\napproaches at the same level, and then to compare the average\nlatency. Given an accuracy target, we perform binary search to\nﬁnd the minimum average latency for the ﬁxed conﬁguration\nbaseline. For AdaptNN andTao, we multiply the predicted\nsearch cost with a coefﬁcient (tuning knob) to reach this\ndesired accuracy. Then we compare their performance at each\naccuracy target, where the prediction overhead is included in\nthe end-to-end latency.\nFor the accuracy target, we use recall @1 (the fraction of\nqueries where the top-1 nearest neighbor returned from search\nis (one of) the ground truth nearest neighbor) for HNSW.\nFor IMI, we use recall @100 (the fraction of queries where\nthe top-100 nearest neighbors returned from search include\n(one of) the ground truth nearest neighbor) as the accuracy\ntarget since it’s challenging for quantization-based approaches\nto reach high recall @1. We measure the average latency in the\nsingle-threaded setting as in previous work [35].\nD. Prediction Overhead\nSince the parameters of neural networks are ﬁxed for\nall datasets, the model size is a constant (560KB) in our\nexperiment setting as well, which is similar to those of\nAdaptNN (274 KB to 310 KB). When making prediction,\nit takes around 65us using the Keras framework for Tao (7us\nto 47us for AdaptNN ). While the prediction overhead is very\nsmall already, a signiﬁcant reduction to a few nano seconds\nis possible if one uses a plain C++ implementation [33]. We\nleave this as future work since it is not a dominant factor in\nthe end-to-end latency.\nE. Empirical evaluation with IMI\nFigure 7 compares the average end-to-end latency for plain\nIMI, AdaptNN andTao. We choose IMI index with OPQ\ncompression as the baseline, which is one of the state-of-the-\nart approaches for billion-scale ANN search. We build IMI\nindex with (214)2= 268,435,456 clusters.\nAll three methods achieve the highest accuracy, i.e., 0.995\nfor both Deep1B and Sift1B. We stop at these recall targets\nbecause it takes too long to reach 1.0 recall for billion-scale\ndatabase. Overall, our approach provides up to 1.16x and 2.69x\nspeedup on Sift1B and Deep1B, respectively.\n\nTable IV: Latency for IMI, AdpatNN andTao.\nDataset Recall IMI AdpatNN Tao Reduction over IMI Reduction over AdpatNN\nDeep1B0.95 39.84 ms 33.69 ms 16.18 ms 59% 52%\n0.96 51.21 ms 36.65 ms 17.77 ms 65% 52%\n0.97 69.22 ms 44.85 ms 21.08 ms 70% 53%\n0.98 95.90 ms 69.09 ms 26.68 ms 72% 61%\n0.99 195.45 ms 117.94 ms 39.61 ms 80% 66%\n0.995 275.20 ms 127.40 ms 47.39 ms 83% 63%\nSift1B0.95 38.69 ms 35.53 ms 30.93 ms 20% 13%\n0.96 46.28 ms 39.98 ms 32.27 ms 30% 19%\n0.97 57.59 ms 46.63 ms 39.13 ms 32% 16%\n0.98 70.78 ms 62.79 ms 50.71 ms 28% 19%\n0.99 117.68 ms 98.30 ms 69.49 ms 41% 29%\n0.995 181.21 ms 100.73 ms 86.71 ms 52% 14%\nDeepSiftGistImageNetGloveMsongTrevi14166425610244096Latency(ms)D\nataset HNSW \nAdaptNN \nTao\nFigure 8: Latency for plain HNSW, AdaptNN andTao.\n30649913818724933246573823001632641282565121024204840968192163843276865536Search CostA\nverage Minimum Search Cost HNSW \nOracle \nTao\nFigure 9: Comparison of search cost for Deep10M.\nTo see the big picture, Table IV lists the latencies for three\nmethods at recall @100 targets between 0.95 and 0.995. As\nthe target accuracy increases, the gain of Tao overAdaptNN\nbecomes more and more signiﬁcant and seems to saturate at\nthe highest recall. The reasons may be two-folds: 1) LID is\nmore informative than features collected by AdaptNN ; 2)Tao\nuse only the training data obtained with 1.0 recall whereas\nAdaptNN is trained at a number of accuracy targets ranging\nfrom 0.95 from 0.995. It is worth noting that Tao does not\ntake advantage of any runtime feature as AdaptNN does, and\nstill outperforms its counterpart with the optimal conﬁguration.Table V: Ablation study over Sift10M.\nRecall HNSW Tao Real LID\n0.95 0.82 ms 0.79 ms 0.69 ms\n0.96 1.01 ms 0.95 ms 0.75 ms\n0.97 1.08 ms 1.02 ms 0.83 ms\n0.98 1.12 ms 1.05 ms 0.95 ms\n0.99 1.36 ms 1.28 ms 1.24 ms\n1.0 17.48 ms 4.28 ms 3.81 ms\nF . Empirical evaluation with HNSW\nFigure 8 plots the average end-to-end latencies by compar-\ning plain HNSW, AdaptNN andTao for highest recalls we\nhave reached. Because of the graph connectivity issue, it is\nunable to ﬁnd the nearest neighbor for a few queries in the\nreasonable time budget. Hence, for different datasets we stop\nat different recall targets. Overall, Tao provides consistent\nspeedup over AdaptNN from 1.05x to 1.2x for recall @1\nmeasure.\nTable VIII presents the detailed numbers for different ac-\ncuracy targets. As one can see, the performance gaps among\nthree methods are less signiﬁcant for a relatively low recall,\nsay 0.9 for Glove and 0.95 for the other datasets. With the\nincrease of target recall, both AdaptNN andTao perform\nbetter than the original algorithms thanks to their ability to\ndistinguish ‘easy’ queries from ‘hard’ ones.\nTable V lists the results of ablation study over Sift10M –\nthe performance of Tao after removing the ﬁrst regression\nmodel. We manually calculate LIDs using Equation (1) for all\nqueries, and pass them to the second neural network. As one\ncan see, the results are only slightly inferior to those using the\npredicted LID, which demonstrates the efﬁcacy of the vectors-\nto-LID mapping.\nFigure 9 depicts the average minimum search cost (obtained\nby the Oracle), the overhead incurred by Tao and ﬁxed con-\nﬁgurations in a semi-log plot over Deep10M. By partitioning\n10,000 queries, sorted in ascending order of the minimum\nsearch cost, into 10 bins evenly, we compute the average of\nminimum search cost in each bin. As we can see, there exists\nsigniﬁcant variation in the optimal search cost. As discussed\nbefore, HNSW with ﬁxed conﬁguration cannot utilize this fact,\n\nDeep1BS ift1B14166425610244096Latency(ms)D\nataset IMI \nVector Only \nTaoFigure 10: Latency for plain IMI, Vector Only and Tao.\nDeepSiftGistImageNetGloveMsongTrevi14166425610244096Latency(ms)D\nataset HNSW \nVector Only \nTao\nFigure 11: Latency for plain HNSW, Vector only and Tao.\nthus lead to the largest latency. Tao, instead, is equipped with\nthe power to assign smaller search steps for ‘easy’ queries,\nby which better performance is obtained. Note that the room\nfor optimization is still giant since what Tao predict is still\nfar away from the Oracle, calling for more efﬁcient features\nand/or prediction models to shrink this gap.\nG. Comparison between Tao and Vector Only Method\nFigure 10 compares the latency of IMI, Tao and VO at\nthe highest recall we achieved, and Table VI lists the detailed\nnumbers including the results of AdaptNN . As we can see:\n1) by leveraging the information drawn from query vectors,\nVO performs better than the plain IMI; 2) without using any\nruntime features, VO is inferior to AdaptNN , validating the\nclaim made in [35]; 3) LID does matter and helps to obtain the\nbest latency among all approaches. The experimental results\nof HNSW are in agreement with IMI as shown in Figure 11\nand Table VII. One interesting observation made is that, in\nsome cases, the latency of VO is even worse than the original\nHNSW, which is caused by incorrect prediction.\nThis set of experimental results indicates that 1) the neural\nnetwork of VO cannot learn the correlation between query\nvectors and the amount of NN search without the help of LID;\n2) the explicit use of LID not only delivers better performance\nand makes Tao easy to use in practice, it also makes the black\nbox (learning process) more explainable for users compared\nwith AdaptNN and Vector Only method. This advantage\nis highly desirable nowadays since explainability is key for\nusers to understand conclusions and recommendations of the\nprediction model.VII. R ELATED WORK\nA large number of ANN search algorithms are available\nin literature, making this section cannot be exhaustive due\nto space limitation. The latest benchmarks [36] show that no\nalgorithm dominates the others in all scenarios and each index\ncomes with different tradeoffs in performance, accuracy and\nspace overhead.\nA. Hashing-based approaches\nFor high-dimensional approximate search, the well-known\nindexing method is locality sensitive hashing (LSH) [20].\nThe main idea is to use a family of locality-sensitive hash\nfunctions to hash nearby data points into the same bucket.\nAfter the query point goes through the same hash functions, it\nwill get the corresponding bucket number, and only compare\nthe distance between the point in the bucket and the query\npoint. In the end, the kapproximate nearest neighbor results\nthat are closest to the query point will be returned. In recent\ntwo decades, many LSH-based variants have been proposed,\nsuch as E2LSH [14], QALSH [26], Multi-Probe LSH [40],\nBayesLSH [46].\nE2LSH, the classical LSH implementations for `2norm,\ncannot solve c-ANN search problem directly. In practice,\none has to either assume there exists a “magical’ radius r,\nwhich can lead to arbitrarily bad outputs, or uses multiple\nhashing tables tailored for different radii, which may lead to\nprohibitively large space consumption in indexing. To reduce\nthe storage cost, LSB-Forest [51] and C2LSH [18] use the so-\ncalled virtual rehashing technique, implicitly or explicitly, to\navoid building physical hash tables for each search radius.\nBased on the idea of query-aware hashing, the two state-of-\nthe-art algorithms QALSH [26] and SRS [49] further improve\nthe efﬁciency over C2LSH by using different index structures\nand search methods, respectively. SRS reduces the ANN\nsearch problem in a d-dimensional space into the range query\nin anm-dimensional projection space (typically m= 6 ),\nand usesR-tree to fulﬁll this purpose. Recently, Lv et al.\nproposed VHP, which achieves better efﬁciency by ingeniously\nrestricting the search space to be the intersection of those of\nQALSH and SRS. All of the aforementioned LSH algorithms\nprovide probability guarantees on the result quality (recall\nand/or precision).\nOther LSH extensions such as Multi-probe LSH [40], SK-\nLSH [37], LSH-forest [5] and Selective hashing [19] use\nheuristics to access more plausible buckets or re-organize\ndatasets, and do not ensure any LSH-like theoretical guarantee.\nA recent paper discussed how to select better hash functions\nusing a deep learning approach [50].\nB. Quantization-based approaches\nThe most inﬂuential vector quantization for ANN search is\nProduct Quantization (PQ) [27]. It seeks to perform a similar\ndimension reduction to hashing algorithms, but in a way that\nbetter retains information about the relative distances between\npoints in the original vector space. Formally, a quantizer is a\nfunctionqmapping ad-dimensional vector x2Rdto a vector\n\nTable VI: Latency for plain IMI, Vector Only and Tao.\nDataset Recall IMI AdaptNN Vector Only Tao\nDeep1B 0.995 275.20 ms 127.40 ms 260.29 ms 47.39 ms\nSift1B 0.995 181.21 ms 100.73 ms 108.58 ms 86.71 ms\nTable VII: Latency for plain HNSW, Vector Only and Tao.\nDataset Recall HNSW AdaptNN Vector Only Tao\nDeep10M 0.999 19.47 ms 5.88 ms 48.21 ms 4.87 ms\nSift10M 1.0 17.48 ms 5.41 ms 40.11 ms 4.28 ms\nGist 0.999 264.12 ms 40.93 ms 314.35 ms 34.32 ms\nImageNet 0.998 303.34 ms 36.46 ms 24.52 ms 26.56 ms\nGlove 0.9668 1486.82 ms 65.24 ms 76.07 ms 61.46 ms\nMSong 1.0 5.48 ms 4.26 ms 4.67 ms 4.04 ms\nTrevi 0.997 45.85 ms 19.41 ms 67.51 ms 18.54 ms\nq(x)2C=fci;i2Ig , where the index set Iis assumed\nto be ﬁnite:I= 0:::k\u00001. The reproduction values ciare\ncalled centroids. The set Viof vectors mapped to given index\niis referred to as a cell, and deﬁned as\nVi,\b\nx2RD:q(x) =ci\t\nThekcells of a quantizer form a partition of Rd. So all\nthe vectors lying in the same cell Viare reconstructed by\nthe same centroid ci. Due to the huge number of samples\nrequired and the complexity of learning the quantizer, PQ uses\nmdistinct quantizers to quantize the subvectors separately.\nAn input vector will be divided into mdistinct subvectors uj,\n1\u0014j\u0014m. The dimension of each subvector is d\u0003=d=m .\nAn input vector xis mapped as follows:\nx1; : : : ; x d\u0003| {z }\nu1(x);\u0001\u0001\u0001; xd\u0000d\u0003+1; : : : ; x d| {z }\num(x)!q1(u1(x)); : : : ; q m(um(x))\nwhereqjis a low-complexity quantizer associated with the\njthsubvector. And the codebook is deﬁned as the Cartesian\nproduct,\nC=C1\u0002:::\u0002Cm\nand a centroid of this set is the concatenation of centroids\nof the m subquantizers. All subquantizers have the same\nﬁnite number k\u0003of reproduction values, the total number of\ncentroids is k= (k\u0003)m.\nPQ offers three attractive properties: (1) PQ compresses an\ninput vector into a short code (e.g., 64-bits), which enables\nit to handle typically one billion data points in memory;\n(2) the approximate distance between a raw vector and a\ncompressed PQ code is computed efﬁciently (the so-called\nasymmetric distance computation (ADC) and the symmetric\ndistance computation (SDC)), which is a good estimation of\nthe original Euclidean distance; and (3) the data structure and\ncoding algorithm are simple, which allow it to hybridize with\nother indexing structures.Original PQ requires examining all vectors during ANN\nsearch. To handle billion-scale datasets, advanced indexing\nstructures such as IMI and IVF are developed [3], [27].\nC. Graph-based approaches\nThe vast majority of graph-based indexing schemes are\ndesigned based on three types of theoretical graph models, that\nis, Delaunay Graphs [34], Relative Neighbor Graphs [52] and\nK-Nearest Neighbor Graphs [15], [42]. In practice, however,\nmost search graphs are essentially approximate K-nearest\nneighbor graphs, which mainly distinguish oneself from the\nother in the edge selection policies.\nInspired by HNSW, several follow-up projects aim to\nimprove the proximity graph-based approach. Douze et al.\npropose to combine HNSW with quantization [16]. Navigating\nSpreading-out Graph (NSG) aims to reduce the graph edge\ndensity while keeping the search accuracy [17]. SPTAG com-\nbines the IVF index and proximity graph for distributed ANN\nsearch [10]. As with HNSW, all of these proximity graph\nvariants employ ﬁxed conﬁgurations to perform a ﬁxed amount\nof graph traversal for all queries. Interested readers are referred\nto a recent survey for more detailed discussion [56], where\nuseful recommendations and guidelines are given for users to\nchoose appropriate graph-based algorithms.\nIn this subsection, we would like to highlight a few studies\nfocusing on external memory graph-based indices, which may\nbe of great interests to database researchers. This line of\nresearch is imperative since the sizes of datasets can easily\nexceeds capacity of available main memory in a commodity\nserver as more and more vectors are produced with the\nubiquity of machine learning techniques.\nTo the best of our knowledge, Bin Wang et al. ﬁrst presented\nideas to explore slow storage to achieve billion-scale ANNS in\na single machine [53]. HM-ANN maps the hierrchical design\nof the graph-based ANNS with memory heterogeneity in HM,\nthus most accesses will happen in upper layer stored in fast\nmemory to avoid expensive accesses in slow memory without\nsacriﬁcing accuracy [44]. Vamana fully analyzes the construc-\ntion details of HNSW and NSG, based on which Vamana uses\nthe random neighbor selection strategy of HNSW to increase\n\nTable VIII: Latency for HNSW, AdpatNN andTao.\nDataset Recall HNSW AdaptNN Tao Reduction over HNSW Reduction over AdpatNN\nDeep10M0.95 0.77 ms 0.69 ms 0.76 ms 1% /\n0.96 0.82 ms 0.77 ms 0.79 ms 4% /\n0.97 1.01 ms 0.91 ms 0.95 ms 6% /\n0.98 1.27 ms 1.15 ms 1.21 ms 6% /\n0.99 1.77 ms 1.45 ms 1.53 ms 14% /\n0.999 19.47 ms 5.88 ms 4.87 ms 75% 17%\nSift10M0.95 0.82 ms 0.73 ms 0.79 ms 4% /\n0.96 1.01 ms 0.89 ms 0.95 ms 5% /\n0.97 1.08 ms 0.96 ms 1.02 ms 6% /\n0.98 1.12 ms 1.03 ms 1.05 ms 6% /\n0.99 1.36 ms 1.24 ms 1.28 ms 6% /\n1.0 17.48 ms 5.41 ms 4.28 ms 76% 21%\nGist0.95 3.90 ms 3.57 ms 3.69 ms 5% /\n0.96 4.86 ms 4.23 ms 4.56 ms 6% /\n0.97 6.17 ms 5.41 ms 5.71 ms 7% /\n0.98 7.66 ms 7.06 ms 7.13 ms 7% /\n0.99 14.40 ms 10.92 ms 10.23 ms 29% 6%\n0.999 264.12 ms 40.93 ms 34.32 ms 87% 16%\nImageNet0.95 1.23 ms 0.66 ms 0.86 ms 30% /\n0.96 1.45 ms 0.76 ms 0.93 ms 36% /\n0.97 1.79 ms 0.91 ms 1.14 ms 36% /\n0.98 2.72 ms 1.28 ms 1.46 ms 46% /\n0.99 3.82 ms 2.70 ms 2.36 ms 38% 13%\n0.998 303.34 ms 36.46 ms 26.56 ms 91% 27%\nGlove0.9 5.08 ms 3.79 ms 4.49 ms 12% /\n0.95 94.93 ms 13.10 ms 16.09 ms 83% /\n0.96 335.25 ms 23.01 ms 22.12 ms 93% 4%\n0.9668 1486.82 ms 65.24 ms 61.46 ms 96% 6%\nMsong0.95 0.60 ms 0.53 ms 0.58 ms 3% /\n0.96 0.78 ms 0.67 ms 0.74 ms 5% /\n0.97 0.89 ms 0.73 ms 0.76 ms 15% /\n0.98 1.01 ms 0.65 ms 0.83 ms 17% /\n0.99 1.56 ms 0.82 ms 1.06 ms 32% /\n1.0 5.48 ms 4.26 ms 4.04 ms 26% 5%\nTrevi0.95 2.20 ms 2.03 ms 1.77 ms 20% /\n0.96 2.66 ms 2.01 ms 2.09 ms 21% /\n0.97 3.81 ms 2.64 ms 2.88 ms 24% /\n0.98 6.07 ms 3.76 ms 4.18 ms 31% /\n0.99 8.99 ms 5.33 ms 6.23 ms 31% /\n0.997 45.85 ms 19.41 ms 18.54 ms 60% 4%\nthe ﬂexibility of edge selection. As a result, it is able to\nconstruct a high recall SSD index on a single server of 64G\n[48]. Minjia Zhang et al. presented an ANN solution ZOOM\nthat uses SSD to employ a multi-view approach. Through the\nefﬁcient routing and optimized distance computation, ZOOM\ngreatly enhance the effectiveness and efﬁciency while attaining\nequal or higher accuracy [58].\nVIII. C ONCLUSION AND FUTURE WORK\nIn this paper, we propose Tao, a general learning framework\nfor adaptive ANN search in high dimensional spaces. Using\nLID as an intermediate feature, Tao decouples the prediction\nphase from query processing, and eliminates the laborious\nparameter tuning work involved in AdaptNN . Experimental\nresults show that Tao achieves comparable or even better\nperformance compared to AdaptNN . A few interesting prob-lems are worth further investigation. Tao unveil the hidden\ncorrelation between query vectors and their LID values, which\nmight deserve further study on its own right. Given a speciﬁc\nANN search algorithm, how LID affects the difﬁculty of ANN\nsearch in high-dimensional spaces is also an open problem of\nboth theoretical practical importance.\nACKNOWLEDGMENT\nThis work is supported partially by the Fundamental Re-\nsearch Funds for the Central Universities under grant number\n(No:2232021A-08), NSF of Xinjiang Key Laboratory under\ngrant number (No:2019D04024), Tianjin “Project + Team”\nKey Training Project under grant number (No:XC202022).\nWei Wang were supported by ARC DPs 170103710 and\n180103411, and D2DCRC DC25002 and DC25003.\n\nREFERENCES\n[1] Laurent Amsaleg, Oussama Chelly, Teddy Furon, St ´ephane Girard,\nMichael E. Houle, Ken-ichi Kawarabayashi, and Michael Nett. Esti-\nmating local intrinsic dimensionality. In SIGKDD , pages 29–38, 2015.\n[2] Peter Andras. Function approximation using combined unsupervised\nand supervised learning. TNNLS , 25(2):300–315, 2014.\n[3] Artem Babenko and Victor Lempitsky. The inverted multi-index. TPAMI ,\n37(6):1247–1260, 2014.\n[4] Artem Babenko and Victor Lempitsky. Efﬁcient indexing of billion-scale\ndatasets of deep descriptors. In CVPR , pages 2055–2063, 2016.\n[5] Mayank Bawa, Tyson Condie, and Prasanna Ganesan. LSH forest: self-\ntuning indexes for similarity search. In WWW , pages 651–660, 2005.\n[6] Konstantin Berlin, Sergey Koren, Chen-Shan Chin, James P Drake,\nJane M Landolin, and Adam M Phillippy. Assembling large genomes\nwith single-molecule sequencing and locality-sensitive hashing. Nature\nbiotechnology , 33(6):623–630, 2015.\n[7] Charles Bouveyron, Gilles Celeux, and St ´ephane Girard. Intrinsic\ndimension estimation by maximum likelihood in isotropic probabilistic\nPCA. PRL, 32(14):1706–1713, 2011.\n[8] Francesco Camastra and Antonino Staiano. Intrinsic dimension estima-\ntion: Advances and open problems. ISCI, 328:26–41, 2016.\n[9] Francesco Camastra and Alessandro Vinciarelli. Estimating the intrinsic\ndimension of data with a fractal-based method. TPAMI , 24, 2002.\n[10] Qi Chen, Haidong Wang, Mingqin Li, Gang Ren, Scarlett Li, Jeffery\nZhu, Jason Li, Chuanjie Liu, Lintao Zhang, and Jingdong Wang. SPTAG:\nA library for fast approximate nearest neighbor search , 2018.\n[11] Jose A. Costa and Alfred O. Hero III. Entropic graphs for intrinsic\ndimension estimation in manifold learning. In ISIT, page 466, 2004.\n[12] George Cybenko. Approximation by superpositions of a sigmoidal\nfunction. MCSS , 2(4):303–314, 1989.\n[13] Abhinandan Das, Mayur Datar, Ashutosh Garg, and Shyamsundar\nRajaram. Google news personalization: scalable online collaborative\nﬁltering. In WWW , pages 271–280, 2007.\n[14] Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S. Mirrokni.\nLocality-sensitive hashing scheme based on p-stable distributions. In\nACM , pages 253–262, 2004.\n[15] Wei Dong, Moses Charikar, and Kai Li. Efﬁcient k-nearest neighbor\ngraph construction for generic similarity measures. In WWW , pages\n577–586, 2011.\n[16] Matthijs Douze, Alexandre Sablayrolles, and Herv ´e J´egou. Link and\ncode: Fast indexing with graphs and compact regression codes. In CVPR ,\npages 3646–3654, 2018.\n[17] Cong Fu, Chao Xiang, Changxu Wang, and Deng Cai. Fast approxi-\nmate nearest neighbor search with the navigating spreading-out graph.\nPVLDB , 12(5):461–474, 2019.\n[18] Junhao Gan, Jianlin Feng, Qiong Fang, and Wilfred Ng. Locality-\nsensitive hashing scheme based on dynamic collision counting. In\nSIGMOD , pages 541–552, 2012.\n[19] Jinyang Gao, H. V . Jagadish, Beng Chin Ooi, and Sheng Wang. Selective\nhashing: Closing the gap between radius search and k-nn search. In\nSIGKDD , pages 349–358, 2015.\n[20] Aristides Gionis, Piotr Indyk, and Rajeev Motwani. Similarity search in\nhigh dimensions via hashing. In VLDB , pages 518–529, 1999.\n[21] S. Haykin. Neural networks and learning machines. Pearson Schweiz\nAg, 2008.\n[22] Johannes Hoffart, Stephan Seufert, Dat Ba Nguyen, Martin Theobald,\nand Gerhard Weikum. KORE: keyphrase overlap relatedness for entity\ndisambiguation. In CIKM , pages 545–554, 2012.\n[23] Kurt Hornik. Approximation capabilities of multilayer feedforward\nnetworks. Neural Networks , 4(2):251–257, 1991.\n[24] Michael E. Houle. Dimensionality, discriminability, density and distance\ndistributions. In ICDM , pages 468–473, 2013.\n[25] Michael E. Houle, Hisashi Kashima, and Michael Nett. Generalized\nexpansion dimension. In ICDM , pages 587–594, 2012.\n[26] Qiang Huang, Jianlin Feng, Yikai Zhang, Qiong Fang, and Wilfred Ng.\nQuery-aware locality-sensitive hashing for approximate nearest neighbor\nsearch. PVLDB , 9(1):1–12, 2015.\n[27] Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantiza-\ntion for nearest neighbor search. TPAMI , 33:117–128, 2010.\n[28] Herv ´e J´egou, Romain Tavenard, Matthijs Douze, and Laurent Amsaleg.\nSearching in one billion vectors: re-rank with source coding. In ICASSP ,\npages 861–864, 2011.[29] Jeff Johnson, Matthijs Douze, and Herv ´e J´egou. Billion-scale similarity\nsearch with gpus. IEEE Trans. Big Data , 7(3):535–547, 2021.\n[30] David R. Karger and Matthias Ruhl. Finding nearest neighbors in\ngrowth-restricted metrics. In STOC , pages 741–750, 2002.\n[31] Juha Karhunen and Jyrki Joutsensalo. Representation and separation of\nsignals using nonlinear PCA type learning. NN, 7, 1994.\n[32] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong\nMa, Qiwei Ye, and Tie-Yan Liu. Lightgbm: A highly efﬁcient gradient\nboosting decision tree. In NIPS , pages 3146–3154, 2017.\n[33] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis\nPolyzotis. The case for learned index structures. In SIGMOD , pages\n489–504, 2018.\n[34] D. T. Lee and Bruce J. Schachter. Two algorithms for constructing a\ndelaunay triangulation. IJPP , 9(3):219–242, 1980.\n[35] Conglong Li, Minjia Zhang, David G. Andersen, and Yuxiong He.\nImproving approximate nearest neighbor search through learned adaptive\nearly termination. In SIGMOD , pages 2539–2554, 2020.\n[36] Wen Li, Ying Zhang, Yifang Sun, Wei Wang, Mingjie Li, Wenjie\nZhang, and Xuemin Lin. Approximate nearest neighbor search on high\ndimensional data - experiments, analyses, and improvement. IEEE Trans.\nKnowl. Data Eng. , 32(8):1475–1488, 2020.\n[37] Yingfan Liu, Jiangtao Cui, Zi Huang, Hui Li, and Heng Tao Shen.\nSK-LSH: an efﬁcient index structure for approximate nearest neighbor\nsearch. PVLDB , 7(9):745–756, 2014.\n[38] Kejing Lu, Hongya Wang, Wei Wang, and Mineichi Kudo. VHP:\napproximate nearest neighbor search via virtual hypersphere partitioning.\nVLDB , 13(9):1443–1455, 2020.\n[39] Qin Lv, Moses Charikar, and Kai Li. Image similarity search with\ncompact data structures. In CIKM , pages 208–217, 2004.\n[40] Qin Lv, William Josephson, Zhe Wang, Moses Charikar, and Kai Li.\nMulti-probe LSH: efﬁcient indexing for high-dimensional similarity\nsearch. In VLDB , pages 950–961, 2007.\n[41] Yury A Malkov and Dmitry A Yashunin. Efﬁcient and robust approxi-\nmate nearest neighbor search using hierarchical navigable small world\ngraphs. TPAMI , 42(4):824–836, 2018.\n[42] Rodrigo Paredes, Edgar Ch ´avez, Karina Figueroa, and Gonzalo Navarro.\nPractical construction of k-nearest neighbor graphs in metric spaces. In\nWEA , volume 4007, pages 85–97, 2006.\n[43] Jianbin Qin, Wei Wang, Chuan Xiao, and Ying Zhang. Similarity query\nprocessing for high-dimensional data. VLDB , 13(12):3437–3440, 2020.\n[44] Jie Ren, Minjia Zhang, and Dong Li. HM-ANN: efﬁcient billion-point\nnearest neighbor search on heterogeneous memory. In NIPS , 2020.\n[45] Alessandro Rozza, Gabriele Lombardi, Claudio Ceruti, Elena Casiraghi,\nand Paola Campadelli. Novel high intrinsic dimensionality estimators.\nMach. Learn. , 89(1-2):37–65, 2012.\n[46] Venu Satuluri and Srinivasan Parthasarathy. Bayesian locality sensitive\nhashing for fast similarity search. PVLDB , 5(5):430–441, 2012.\n[47] Uri Shaft and Raghu Ramakrishnan. Theory of nearest neighbors\nindexability. TODS , 31(3):814–838, 2006.\n[48] Suhas Jayaram Subramanya, Fnu Devvrit, Harsha Vardhan Simhadri,\nRavishankar Krishnaswamy, and Rohan Kadekodi. Rand-nsg: Fast\naccurate billion-point nearest neighbor search on a single node. In NIPS ,\npages 13748–13758, 2019.\n[49] Yifang Sun, Wei Wang, Jianbin Qin, Ying Zhang, and Xuemin Lin. SRS:\nsolving c-approximate nearest neighbor queries in high dimensional\neuclidean space with a tiny index. PVLDB , pages 1–12, 2014.\n[50] Xiu Tang, Sai Wu, Gang Chen, Jinyang Gao, Wei Cao, and Zhifei Pang.\nA learning to tune framework for LSH. In ICDE , pages 2201–2206,\n2021.\n[51] Yufei Tao, Ke Yi, Cheng Sheng, and Panos Kalnis. Efﬁcient and accurate\nnearest neighbor and closest pair search in high-dimensional space.\nTODS , 35(3):20:1–20:46, 2010.\n[52] Godfried T. Toussaint. The relative neighbourhood graph of a ﬁnite\nplanar set. PR, 12, 1980.\n[53] Bin Wang, Bo Wu, Dong Li, Xipeng Shen, Weikuan Yu, Yizheng\nJiao, and Jeffrey S. Vetter. Exploring hybrid memory for GPU energy\nefﬁciency through software-hardware co-design. In PACT , pages 93–\n102, 2013.\n[54] Hongya Wang, Zhizheng Wang, Wei Wang, Yingyuan Xiao, Zeng Zhao,\nand Kaixiang Yang. A note on graph-based nearest neighbor search.\nCoRR , abs/2012.11083, 2020.\n[55] Jianguo Wang, Xiaomeng Yi, Rentong Guo, Hai Jin, Peng Xu, Shengjun\nLi, Xiangyu Wang, Xiangzhou Guo, Chengming Li, Xiaohai Xu, Kun\nYu, Yuxing Yuan, Yinghao Zou, Jiquan Long, Yudong Cai, Zhenxiang\n\nLi, Zhifeng Zhang, Yihua Mo, Jun Gu, Ruiyi Jiang, Yi Wei, and Charles\nXie. Milvus: A purpose-built vector data management system. In\nSIGMOD , pages 2614–2627, 2021.\n[56] Mengzhao Wang, Xiaoliang Xu, Qiang Yue, and Yuxiang Wang. A\ncomprehensive survey and experimental comparison of graph-based\napproximate nearest neighbor search. CoRR , abs/2101.12631, 2021.\n[57] Roger Weber, Hans-J ¨org Schek, and Stephen Blott. A quantitative\nanalysis and performance study for similarity-search methods in high-\ndimensional spaces. In VLDB , pages 194–205, 1998.\n[58] Minjia Zhang and Yuxiong He. Zoom: Ssd-based vector search for\noptimizing accuracy, latency and memory. CoRR , abs/1809.04067, 2018.",
  "textLength": 67065
}