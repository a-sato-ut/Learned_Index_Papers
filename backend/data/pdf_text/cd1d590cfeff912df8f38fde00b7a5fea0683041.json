{
  "paperId": "cd1d590cfeff912df8f38fde00b7a5fea0683041",
  "title": "Parallel External Sorting of ASCII Records Using Learned Models",
  "pdfPath": "cd1d590cfeff912df8f38fde00b7a5fea0683041.pdf",
  "text": "Parallel External Sorting of ASCII Records Using Learned Models\nAni Kristo\nBrown University\nani@brown.eduTim Kraska\nMIT\nkraska@mit.edu\nABSTRACT\nExternal sorting is at the core of many operations in large-scale\ndatabase systems, such as ordering and aggregation queries for large\nresult sets, building indexes, sort-merge joins, duplicate removal,\nsharding, and record clustering. Unlike in-memory sorting, these\nalgorithms need to work together with the OS and the filesystem\nto efficiently utilize system resources and minimize disk I/O.\nIn this paper we describe ELSAR: a parallel external sorting\nalgorithm that uses an innovative paradigm based on a learned data\ndistribution model. The algorithm leverages the model to arrange\nthe input records into mutually exclusive, monotonic, and equi-\ndepth partitions that, once sorted, can simply be concatenated to\nform the output. This method completely eliminates the need for\nmulti-way file merging, which is typically used in external sorting.\nWe present thorough benchmarks for uniform and skewed datasets\nin various storage media, where we measure the sorting rates, size\nscalability, and energy efficiency of ELSAR and other sorting algo-\nrithms. We observe that ELSAR has up to 1.65Ã—higher sorting rates\nthan the next-best external sort (Nsort) on SSD drives and 5.31Ã—\nhigher than the GNU coreutilsâ€™ sort utility on IntelÂ®Optaneâ„¢non-\nvolatile memory. In addition, ELSAR supersedes the current winner\nof the SortBenchmark for the most energy-efficient external string\nsorting algorithm by an impressive margin of 41%. These results\nreinforce the premise that novel learning-enhanced algorithms can\nprovide remarkable performance benefits over traditional ones.\n1 INTRODUCTION\nExternal sorting is at the core of many operations in large-scale\ndatabase systems. In relational databases, it is most obviously used\ninORDER_BY , and GROUP_BY queries for large result sets[ 23,32].\nHowever, it is also part of internal operations, such as building\nindexes[ 22,31], sort-merge joins[ 2], duplicate removal, as well\nas sharding and record clustering[ 33]. External sorting is also a\nfundamental piece of search engines and other document retrieval\nsystems that use inverted files to map records to their locations\non disk[ 39]. Moreover, it is used in constraint-based systems that\nwork with limited memory and have to rely on persistent storage\nfor their tasks[ 26]. Hence, improving the speed of external sorting\ncan have a profound impact on large-scale data processing systems.\nThe major bottleneck of external sorting is the latency associated\nwith disk accesses, which are an order of magnitude slower than\nDRAM memory. Unlike in-memory sorting, these algorithms need\nto work together with the operating system and the file system\nto efficiently utilize system resources and minimize disk I/O. The\nmajority of external sorting algorithms[ 7,13,37,44] that have\nappeared in leading sorting benchmarks follow the paradigm of\nExternal Mergesort [3]. Conceptually, these methods first divide the\ninput into small files such that they are small enough to fit entirely\nin memory. Then, these intermediate files are sorted separately and\nin parallel using Quicksort or alternative in-memory sorts. Finally,the sorted files are merged using a heap containing the smallest key\nfrom each of them. The algorithm continuously appends records to\nthe output file by popping the root of the heap.\nIn contrast, this paper describes a parallel external sorting al-\ngorithm (ELSAR), which uses an innovative paradigm based on\nlearned data distribution models. Unlike External Mergesort, it uses\na partition-and-concatenate approach. ELSAR first builds a model\nthat estimates the input distribution based on a sample of keys.\nThen, it uses this model to arrange the input records in mutually\nexclusive, monotonic, and equi-depth partitions. In this way, the\nmodel eliminates the need for merging, and ELSAR only needs to\nsequentially concatenate the partitions, which is much faster than\na multi-way file merge. While conceptually similar to Radix Sort,\nmodel-based partitioning is less sensitive to skew and produces\nbalanced loads for the worker threads.\nFor in-memory sorting, ELSAR uses the LearnedSort algorithm[ 16],\nwhich has shown high performance in numerous data sets with\nvarious distributions. Note that simply plugging in LearnedSort\nas the internal routine in the External Mergesort framework will\nnot considerably affect performance since external sorting is disk\nI/O-bound. The algorithm spends most of the execution time per-\nforming read/write operations from and to disk; therefore, only\nI/O-related improvements can make a significant impact on the\nalgorithmâ€™s throughput. Our design goals for ELSAR were to have\nlock-free concurrency, load balance, and optimized file I/O.\nCurrently, ELSAR supports sorting of ASCII strings as those\nare still the most common data types and multiple systems only\nsupport ASCII characters natively[ 19,21,42]. We do not yet sup-\nport Unicode as it requires special handling functions1, though we\nconsider extending ELSAR to Unicode in future work.\nTo evaluate ELSAR, we measure its sorting rates in various ma-\nchines, data distributions, and input sizes. We use desktop and\nserver-grade machines containing multiple storage technologies\n(i.e., HDD, SSD, and PMem) with varying I/O bandwidths and in-\nterfaces (SATA, PCIe, and DDR-T). We also tested ELSARâ€™s per-\nformance for terabyte-scale uniform and skewed data sets that\nare up to 40Ã—larger than memory capacity. Our implementation\nshowed notable performance gains over existing external sorting\nalgorithms - up to 1.65Ã—higher sorting rates than the next-best ex-\nternal sort (Nsort) on SSD drives and 5.31Ã—than the GNU coreutilsâ€™\nsort utility on IntelÂ®Optaneâ„¢non-volatile memory. These results\nwere consistent among the spectrum of our evaluations, reinforcing\nthe premise that novel learning-enhanced algorithms can provide\nextraordinary performance benefits to traditional algorithms.\nFinally, we also evaluated ELSAR using the SortBenchmarkâ€™s\nJouleSort metric, which assesses the energy efficiency of external\nstring sorting algorithms[ 36]. As of this publication, ELSAR out-\nperforms the current winner by 41%; an arguably impressive result\n1For example, there is no definition for how to sort Unicode strings because the lexical\norder is language-dependent[14](e.g., Ã¶<z in German, but Ã¶>z in Swedish).arXiv:2305.05671v1  [cs.DB]  8 May 2023\n\nAni Kristo and Tim Kraska\nthat we have officially submitted to the SortBenchmark committee\nto be considered for the next official ranking.\nIn the remainder of this paper we:\n(1)Introduce a new parallel external sorting algorithm for ASCII\nrecords that uses learned models,\n(2)Describe our method for representing ASCII strings onto a\nnumerical space for modeling their distribution and empiri-\ncal CDF,\n(3) Analyze the complexity of this new sorting algorithm, and\n(4)Present benchmarks that showcase the performance of EL-\nSAR in various hardware compositions\n(5)Discuss the strengths of ELSAR with respect to other external\nsorting algorithms.\nThe following section gives some background on existing ap-\nproaches for external sorting, their variations, and their differences.\n2 BACKGROUND & EXISTING APPROACHES\nExternal sorting algorithms are typically implemented in two phases:\n1) The Run Creation phase and 2) the Merge phase [ 7,13,20,30,37,\n41,44]. In the Run Creation phase, the sorting algorithm divides\nthe input file into several chunks, which are also called runs. This\nis usually done in a parallel fashion to speed up the file reading per-\nformance. The objective of this phase is to create intermediate files\ncontaining runs that are small enough to fit entirely in memory. At\nthe end of the first phase, each intermediate file is sorted using an in-\nmemory sorting algorithm, most commonly Quicksort[ 7,20,30,37].\nSeveral runs can be sorted in parallel depending on available mem-\nory since there are no dependencies between them.\nIn the second phase, the sorted runs are merged onto the output\nfile similar to the in-memory Mergesort algorithm. This algorithmic\ntemplate is called External Mergesort and is the one used by the\nsort utility in GNU Coreutils[7].\n2.1 Variations in the implementation of the\nExternal Mergesort\nThere are many variations of External Mergesort that aim to in-\ncrease the algorithmâ€™s performance, such as the choice of the in-\nmemory sorting routine, I/O operations scheduling, and merge\nstrategies. Table 1 summarizes the approaches discussed below.\nIn-memory sorting variations. The most trivial variation is the\nchoice of the in-memory sorting routine used to sort the created\nruns. For instance, some algorithms use Timsort, a hybrid of the\ninternal Mergesort algorithm and Insertion Sort. The rationale is\nthat Timsort leverages the naturally occurring patterns in the input\ndata and has better performance than Quicksort for real-world\ndatasets[ 27]. Timsort first finds pre-sorted sequences of elements\nin the input and merges them to create a total order. When it cannot\nfind large enough sequences (typically greater than 32 elements), it\nemploys Insertion Sort to form sorted sequences from consecutive\nout-of-order elements. Timsort is used as an internal sorting routine\nby algorithms like NADSort[44] and SparkSort[45].\nI/O scheduling variations. External sorting algorithms are typi-\ncally disk I/O-bound, so compute-related improvements, like the\nchoice of the internal sorting routine, do not help much with themore significant issue. The most successful external sorting strate-\ngies focus on 1) minimizing the number of I/O operations and 2)\nhiding I/O latencies by overlapping them with as much computation\nas possible. For example, KioxiaSort, one of the current winners of\nthe SortBenchmark[ 37], addresses this issue by adopting a pipeline\napproach. Using mutual exclusions, it overlaps the read operations\nof one-third of the threads with the sortoperations of the second\nthird of the threads and write operations of the final third of the\nthreads[ 37]. A similar strategy is also implemented by FuxiSort[ 43]\nand DEMSort[ 34], previous winners of the same benchmark. Be-\nsides overlapping, these algorithms also reduce the amount of I/O\nby generating smaller intermediate files that do not contain the\nrecord payloads. Instead, the algorithm only writes out the record\nkeys and indices from the original input file[37].\nMerge strategy variations. On the other hand, there are a few\nprominent approaches to how the intermediate files are merged\nin the second phase of the External Mergesort algorithm. The first\nstrategy is to perform a multi-way external merge, in which the\nalgorithm maintains a heap of the smallest key from each file and\nwrites the element at the root of the heap to the output. However,\nthis method is limited by how many elements can be kept in mem-\nory, restricting the number of intermediate files that the algorithm\ncan use and the degree of parallelism of the first phase. In addition,\nelement insertions to the heap become expensive due to the large\nheap size. Moreover, this merging routine can only be performed\nsequentially, thus becoming the algorithmâ€™s bottleneck.\nAlternatively, the Merge phase can avoid using a heap by per-\nforming hierarchical 2-way external merging (like a tournament\ntree). Not only does this method reduce the memory footprint of\nthis phase, but it also enables the lower level of the external merging\nto be done in parallel. However, this comes at increased disk I/O\ncost, additional intermediate files, and the need for synchronization\nprimitives (i.e., mutexes), which are costly.\nTherefore, the third and best approach is to have a hybrid merg-\ning routine: a hierarchical multi-way merge that combines the\nbenefits of having smaller heaps and parallelism. This method has\nthe smaller cost of using only a few additional intermediate files for\nthe merging routine and fewer synchronization variables among\nthe hierarchy levels. KioxiaSort, for example, uses two-stage merg-\ning, where the first stage performs six 200-way merges in parallel,\nand the second stage performs a 6-way merge to the output file[ 37].\n2.2 Nsort\nNsort is an interesting external sorting algorithm that is widely used\nand displays very high sorting throughput[ 25]. Nsort is employed\nas the software layer of multiple SortBenchmark winners like\nRezSort[ 35], TaichiSort[ 18], NTOSort[ 6], EcoSort[ 1], FlashSort[ 4],\nand FAWNsort[ 28]. Therefore, it is very intriguing to investigate\nthe successful recipe that has made SortBenchmark winners since\n2007. The authors mention that it has â€œsophisticated buffer manage-\nment to overlap computation and I/Oâ€ and that â€œit pays particular\nattention to processor cache locality to make the best use of fast\nmicroprocessorsâ€[ 24]. However, its source code remains propri-\netary at the time that we write this paper, and we do not possess\nany further insight into this algorithm.\n\nParallel External Sorting of ASCII Records Using Learned Models\nTable 1: Summary of the characteristics of some external sorting algorithms.\nAlgorithmâ€™s name Framework type In-memory sort Merging strategy Known for\nUnix sort External Merge-Sort Mergesort Binary heap Default filesort in Linux\nNsort Unknown Unknown Unknown Many winning SortBenchmark entries\nKioxia Sort External Merge-Sort Quicksort Hierarchical k-way Current winner of JouleSort\nMySQL filesort External Merge-Sort Introsort Hierarchical k-way Variation of Ext MS used in MySQL\nPostgres tuplesort External Merge-Sort Quicksort K-way Variation of Ext MS used in Postgres\nSQLite vdbesort External Merge-Sort Mergesort Hierarchical k-way Variation of Ext MS used in SQLite\nELSAR Partition & Concatenate LearnedSort N/A The algorithm described in this paper\nEven though there exist numerous external sorting algorithms, it\nis clear that none of them move away from the Run-Merge paradigm.\nIn fact, there is very little innovation in the algorithmic space, with\nmost of the efforts being spent on building hardware setups that\nexploit modern high-speed block storage, processors, and network\nadapters. In the next section, we look at a different sorting approach\nthat completely eliminates the need for a Merge phase due to the\nbenefits of using a learned CDF model.\n3 SORTING LARGER-THAN-MEMORY FILES\nELSAR is a learning-enhanced, data distribution-based, external\nsorting algorithm that sorts ASCII datasets by leveraging small,\nhighly accurate, and fast ML models. The algorithm combines vari-\nous techniques to achieve high sorting rates, such as sample-based\ndistribution learning, numerical embedding for ASCII keys, and\nparallel, buffered and lock-free file I/O. The design of ELSAR incor-\nporates I/O optimizations that avoid bottlenecks while maximizing\nutilization of the memory and available hardware parallelism. The\ncentral idea of ELSAR is organizing input records in mutually ex-\nclusive, monotonic, and equi-depth partitions that, once sorted, can\nsimply be concatenated to form the output file. In order to perform\nthat task fast and with good accuracy, ELSAR uses a CDF model,\nwhich approximates the empirical distribution function of the input\ndataset. Using this model, the algorithm can infer the rank of each\nrecord in the input file by simply looking at its key and not making\nany comparisons with the other records. This approach provides\nbetter partition size uniformity than a radix-based partitioning\nscheme, which is sensitive to skew.\nA diagram of ELSAR is shown in Figure 1, and the sections below\ndescribe the sorting procedure in detail.\n3.1 Model training\nThe CDF model is arguably the central component of ELSAR. The\nmodel is trained on a small sample from the input data ( ğ‘‹) and uses\na Recursive Model Index (RMI) architecture, as described in [ 16],\nand [ 15]. The sample is picked uniformly at random from the first\nbatch read by thread ğ‘‡0. Its size must be large enough to enable the\nmodel to learn well the boundaries of the partitions while not being\nso large that it makes the model training an expensive operation.\nIn [16] we have observed that the CDF model reaches an adequate\naccuracy for the partitioning task quickly. Hence this approach\nworks very well even with small samples. In our implementation\nof ELSAR, we empirically observed that a sample of 1% of the data\nsatisfies these constraints.\n...Partitioning1\n2\n......T1T2Tr\nCDF\nmodelCDF\nmodelCDF\nmodel\nAppend\nSortAppend\nSortAppend\nSort\noutput...\ninput\nP1P1F1\nP1F2\nP1FrP2F1\nP2F2\nP2FrPfF1\nPfF2\nPfFr\nP2PfSorting...\n......(a)\n(b)\n(c)\n(d)\n(e)Figure 1: The architecture of ELSAR. During partitioning,\nğ‘Ÿparallel threads read different stripes of the input file in\nbatches. Each record is placed into ğ‘“thread-local partition\nfragments, then flushed out to intermediary files. Partitions\nare composed of the contents of the corresponding fragment\nfiles. They are sorted in memory using LearnedSort[16] and\nwritten directly to the corresponding offset in the output\nfile.\nOn the other hand, the RMI structure is an acyclic graph arrange-\nment of linear regression models that acts as an expert system.\nStarting from the root, the record key from the input ( ğ‘¥âˆˆğ‘‹) tra-\nverses internal nodes, which recursively pick another linear model\nthat will be more â€œspecializedâ€ for a subset of values where the\ncurrent key belongs. When the key reaches the leaf nodes, the\n\nAni Kristo and Tim Kraska\noutput is a predicted CDF value ğ‘¦âˆˆ [0,1]that corresponds to\nğ‘¦=ğ‘ƒ(ğ‘‹â‰¤ğ‘¥), or the percentile rank of the record among the\nestimated population.\nThis CDF model combines the benefits of having good accuracy\nwith a fast training and inference time. In some sense, the model\nacts like an order-preserving hash function or a radix partitioner.\nHowever, unlike these, the CDF model absorbs input skew much\nbetter and produces uniformly-sized partitions, which is critical for\nload balancing the sorter threads[ 16]. This is because the training\nprocedure assigns high-density domain areas to more nodes in the\nRMI, hence spreading out the skew onto more evenly-distributed\nbuckets, ideally acting as equi-depth histogram bins.\nIt is important to note that we are interested in modeling the\nempirical distribution, i.e., the distribution of the observed data,\nrather than the theoretical one. The difference is that the observed\nsample does not follow a smooth distribution that could be modeled\nwith just a few linear models. The data will behave like a step\nfunction at a fine scale, with more structure, noise, and irregularities.\nThat is why it is necessary to use a more complex architecture to\ncapture the empirical distributionâ€™s subtle characteristics.\nâ€œThe Case for Learned Index Structuresâ€[ 15] provides a detailed\nexplanation of the architecture and the CDF modelâ€™s training algo-\nrithm, inference, and further analysis.\n3.2 Input processing\nAfter the training has been completed, the algorithm spawns ğ‘Ÿ\nthreads, each responsible for reading a specific, non-overlapping\nrange of records from the input file (Fig 1a). In our implementa-\ntion, we used OpenMP threads due to (1) their better portability\nthan pthreads, (2) thread pooling, which minimizes thread setup\nand teardown costs, (3) automatic scheduling, and (4) CPU affinity.\nEach thread reads the input records (key and payload) in batches.\nThis technique aims to utilize as much sequential I/O as possible to\ntake advantage of the higher reading speeds than random access\nI/O. Nevertheless, in any sorting procedure, record shuffling has\nunpredictable patterns that make it almost impossible to perform\nsequential reads and writes. In another attempt to speed up reading\ntimes, we could use asynchronous threads (or co-routines) to read\nahead of the next batch while the reader threads are processing\nthe current batch. However, we observed that this extension did\nnot improve the performance due to the synchronization overhead\nand the diminished parallelism. In addition, we also did not ob-\nserve significant performance improvements from using direct I/O,\ntherefore the algorithm relies on libc â€™s buffered file utilities.\n3.3 Partitioning\nNext, the algorithm processes the keys in the batch through the\ntrained CDF model, which predicts the ranks of the records among\nall records in the input (Fig 1b). Each reader thread ğ‘‡ğ‘–has their\nown read-only copy of the model and maintains a set of ğ‘“thread-\nlocal partition fragments that act like equi-depth histogram bins\n(i.e.,ğ‘ƒ1ğ¹ğ‘–,ğ‘ƒ2ğ¹ğ‘–,...,ğ‘ƒ ğ‘“ğ¹ğ‘–). Using the predictions from the model, the\nthreads place the records in respective fragments. For the ğ‘—thfrag-\nmentğ‘ƒğ‘—ğ¹ğ‘–in theğ‘–threader thread, the algorithm maintains the\nfollowing monotonicity invariant:{ğ‘¥â‰¤ğ‘¦|âˆ€ğ‘¥âˆˆğ‘ƒğ‘—ğ¹ğ‘–,âˆ€ğ‘¦âˆˆğ‘ƒğ‘—+1ğ¹ğ‘–,\nğ‘–âˆˆ[1,ğ‘“],ğ‘—âˆˆ[1,ğ‘“âˆ’1]}(1)\nOnce the first batch processing is done, the threads flush their par-\ntition fragments to temporary files. For future batches, the contents\nof the fragments are simply appended to the existing and corre-\nsponding temporary files. Note that each reader thread maintains its\nthread-local partition fragments instead of directly appending them\nto a big partition file. This avoids using mutexes and locks, which\nare costly. On the other hand, the algorithm only uses pointers to\nthe records during this process to reduce memory copying and mov-\ning operations, especially for long strings. Finally, since the reader\nthreads have mutually disjoint working sets, we can perform file\nI/O with the non-locking versions of the read and write functions\n(i.e.,fread_unlocked() andfwrite_unlocked() ). These omit the\nfile pointerâ€™s lock check and are faster.\nWe compared this partitioning approach with a radix-based one\non large skewed datasets and observed that the CDF model provides\nbetter uniformity. Radix-based partitioning looks at the most signif-\nicant bytes in the key and converts them to indices of the partition\nfiles. This approach is similar to building an equi-width histogram,\nwhere each bin corresponds to a fixed interval of the key domain\nthat has equal width to the other bins. In contrast, the model-based\npartitioning used in ELSAR produces equi-depth partitions with\nvariable-sized key intervals but evenly-sized bins. Based on our\nexperiments, our approach reduced the partition size variance by\n23% compared to the radix approach. This will provide a better load\nbalance for the sorter threads that will process the partition files in\nthe next stage.\n3.4 Sorting\nAfter the entire input file has been processed, ELSAR spawns ğ‘ \nthreads, each responsible for sorting and flushing one partition.\nThe number ğ‘ is calculated as the maximum number of partitions\nthat can be in memory simultaneously. Note that the number of\npartitions is chosen such that no single partition exceeds the mem-\nory capacity. Furthermore, this allows for several partitions to be\nin memory simultaneously, thus enabling parallel sorting.\nThe sorter threads read all the fragment files that belong to the\nlogical partition that they are assigned to and append their records\ninto a single large buffer (Fig 1c).\nThen the algorithm calls LearnedSort as an in-memory sorting\nroutine (Fig 1d). This is an excellent choice because it has the high-\nest in-memory sorting rates compared to many modern sorting\nalgorithms[ 17]. It implements various optimization related to CPU\ncache utilization, handling skewed inputs, and avoiding perfor-\nmance degradation on high-duplicate datasets by using an early\ntermination strategy[17].\n3.5 Writing to output\nFinally, after the partition contents have been sorted, they are con-\ncatenated sequentially with the neighboring partitionsâ€™ records to\nform a single continuous output file (Fig 1e). Each sorter thread\nmaintains an open descriptor for the output file, and, for each parti-\ntionğ‘ƒğ‘–that it will flush, it seeks to the offset location pre-calculated\n\nParallel External Sorting of ASCII Records Using Learned Models\nas the sum of the sizes of the partitions ğ‘ƒ1..ğ‘ƒğ‘–âˆ’1:\nğ‘¤ğ‘Ÿğ‘–ğ‘¡ğ‘’ _ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡ =ğ‘–âˆ’1âˆ‘ï¸\nğ‘˜=1ğ‘™ğ‘’ğ‘›(ğ‘ƒğ‘˜)\nSince the sorted buffer contains only keys and pointers to the\nrecords, it is impossible to flush the entire buffer in one sequential\ncall. Therefore, the thread first coalesces the records in batches\nby dereferencing the pointers in sorted order. Then, it performs a\nbuffered, sequential write of the coalesced buffer (typically 100KB),\nthus optimizing the write performance.\n4 ENCODING ASCII RECORDS\nIn order to learn the distribution of the input, the ASCII records have\nto be projected onto a numerical space on which the CDF model can\nuse linear regression to train its individual nodes. Therefore, the\nalgorithm operates on the keyâ€™s numerical encoding and a pointer\nto the record while in memory.\nAssuming the keys have a fixed size, the encodings are calculated\nusing the binary values of each character in the key represented\nas base-95 numbers since printable ASCII characters have codes\nbetween 32 and 127. The unprintable ASCII characters (0-31) are\ncontrol codes intended to provide meta-information for peripherals\n(e.g., printers), so they are not of interest in sorting. Therefore,\nencoding of a character in position ğ‘¥ğ‘–of the key of length ğ‘™(1â‰¤\nğ‘–â‰¤ğ‘™) is(ASCII(ğ‘¥ğ‘–)âˆ’32)Ã—95ğ‘™âˆ’ğ‘–. Then, the numerical encoding of\nthe entire key is:\nğ‘™âˆ‘ï¸\nğ‘–=1\u0010\n(ASCII(ğ‘¥ğ‘–)âˆ’32)Ã—95ğ‘™âˆ’ğ‘–\u0011\nIf the input contains variable-sized keys, then ğ‘™is set to the maxi-\nmum length observed ( ğ‘™=max(ğ‘™ğ‘’ğ‘›(ğ‘¥1),...,ğ‘™ğ‘’ğ‘›(ğ‘¥ğ‘›)), and ASCII(ğ‘¥ğ‘–)=\n0forğ‘–â‰¥ğ‘™ğ‘’ğ‘›(ğ‘¥).\nWith a 64-bit primitive type, we can encode up to the ninth byte\nof the key, since log2Ãğ‘™\nğ‘–=195ğ‘™âˆ’ğ‘–+1is59.14forğ‘™=9and65.71for\nğ‘™=10. If the record keys are longer than nine bytes, this encoding\nscheme will not be able to capture the rest of the bytes numerically.\nThis only violates the invariant in Eq. 1 if we use more than 6.37Â·1017\npartitions and the keys are at least 10 bytes, meaning that we are\nsorting inputs of at least 6369.5 Petabytes. In all other cases, this\nwill not affect the correctness of the sorting algorithm. This is\nbecause the in-memory LearnedSort routine has a touch-up step\nthat performs last-mile sorting on the rest of the key using pair-\nwise key comparisons with the strncmp() function. In addition,\nthis touch-up step also covers prediction inaccuracies from the CDF\nmodel. This step uses Insertion Sort, which works in almost linear\ntime for nearly-sorted arrays[16].\n5 IMPLEMENTATION\nThe complete pseudocode of the ELSAR algorithm is shown in\nAlgorithm 1. The algorithmâ€™s inputs are ğ¼- the input file name,\nğ‘‚- the output file name, ğµ- the batch size, ğ‘“- the number of\npartitions/fragments, ğ‘Ÿ- the number of processors, and ğ‘€- the\navailable memory in the system.\nIn line 1 , the algorithm creates a sparse output file that occupies\nprecisely as many bytes as the input file ğ¼.In line 2 , the algorithm calls a model training function that\nsamples the input file and returns a trained CDF model (i.e., ğ¹ğ‘‹(ğ‘¥)).\nIn line 3 , the algorithm initializes a 2D matrix ( ğ‘ŸÃ—ğ‘“) of dynamically-\nsized arrays (vectors). This matrix will store the records read in\nmemory organized by the reader thread on the first dimension and\nthe partition fragment on the second one.\nIn line 4 , the algorithm initializes a 2D matrix ( ğ‘ŸÃ—ğ‘“) of tempo-\nrary files that correspond to the records collected in ğ·.\nIn line 5 , the algorithm creates a size vector that will keep track\nof the number of records assigned to each partition.\nIn lines 6-20 , the algorithm starts by spawning ğ‘Ÿreader threads,\neach opening the input file at different offsets ( ğ‘œ) for parallel reading.\nNote that each thread will be responsible for reading |ğ¼|/ğ‘Ÿrecords\nfrom the input file. Each reader thread ğ‘–reads the records that it\nis responsible for in batches ( C) of sizeğµwhile keeping track of\nthe number of bytes read so far ( ğ‘). Then, for every record in each\nbatch, it uses the ğ¹ğ‘‹(ğ‘¥)predictor to estimate the rank of the key ( ğ‘)\nand place the record in the predicted fragment ( Dğ‘–,ğ‘). For this call,\nthe algorithm encodes the ASCII keys using the method described\nin the section above. Once a record has been placed in a fragment,\nthe corresponding partitionâ€™s size (i.e., ğ‘†ğ‘) is incremented.\nIn line 21 , the algorithm transitions to the second stage, where\nit starts by calculating ğ‘ - the number of sorter threads to be used.\nRecall that the value of ğ‘ is the maximum number of consecutive\npartitions that can be completely held in memory simultaneously.\nIn lines 22-31 , the algorithm spawns ğ‘ working threads, which\nwill process all ğ‘“partitions. Therefore, each sorter thread will be\nresponsible for ğ‘“/ğ‘ partition files. Here the algorithm also initializes\nan in-memory buffer that will contain all the records belonging to\nthese partitions that are, up to now, separated onto ğ‘Ÿtemporary\nfragment files from earlier. Each sorter thread ğ‘–reads the content\nof the fragments belonging to the partition that they are processing\n(Fğ‘—,ğ‘–) and gathers their contents onto a single in-memory buffer for\nsorting ( P). After each fragment is fully read into memory, its tem-\nporary file is closed, immediately signaling the OS to remove the\nfile and free up memory. Then, the sorter invokes the LearnedSort\ninternal sorting routine to sort the partition contents. This call will\ninternally re-encode the keys of each record to a numerical repre-\nsentation and use it to sort the keys as described in [ 17]. Finally, the\nsorter will flush the entire partition to its respective offset ğ‘œof the\noutput file, such that ğ‘œis the sum of sizes of all partitions [0,ğ‘–âˆ’1].\nThis parallel writing routine is simply an in-order concatenation\nof the partitions. Once every sorter thread has processed its cor-\nresponding partitions, the algorithm exits, and the output file is\nfully-populated with sorted records.\n6 COMPUTATION COMPLEXITY\nThis section provides a Work-Span analysis of ELSAR with respect\nto the input size ( ğ‘›). The work-span framework in a Parallel Ran-\ndom Access Machine (PRAM)[ 12] model is similar to the complexity\nanalysis of sequential algorithms. However, it differs in that the\ncomputation is treated as a directed task dependency graph. The\nterm work refers to the total number of operations executed by\nthe algorithm, which represents a lower bound of the worst-case\nsequential computation complexity. On the other hand, span rep-\nresents the maximum number of sequential operations (i.e., the\n\nAni Kristo and Tim Kraska\nAlgorithm 1: The pseudocode of ELSAR\nInput:ğ¼- Input file name\nInput:ğ‘‚- Output file name\nInput:ğ‘€- available memory\nInput:ğ‘Ÿ- number of processors\nInput:ğ‘“- number of partitions\nInput:ğµ- reading batch size\n1ğ‘œğ‘¢ğ‘¡â†ğ‘“ğ‘ğ‘Ÿğ‘’ğ‘ğ‘¡ğ‘’(ğ‘‚,|ğ¼|)\n2ğ¹ğ‘‹â†ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› _ğ¶ğ·ğ¹(ğ¼)\n3Dğ‘ŸÃ—ğ‘“â†ğ‘£ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿ()\n4Fğ‘ŸÃ—ğ‘“â†ğ‘¡ğ‘šğ‘ğ‘“ğ‘–ğ‘™ğ‘’()\n5S1Ã—ğ‘“â†{0}\n6forğ‘–âˆˆ[0,ğ‘Ÿ)do in parallel\n7ğ‘œâ†ğ‘–Â·|ğ¼|/ğ‘Ÿ\n8ğ‘–ğ‘›â†ğ‘“ğ‘œğ‘ğ‘’ğ‘›ğ‘ğ‘¡(ğ¼,â€œğ‘Ÿğ‘â€,ğ‘œ)\n9ğ‘â†0\n10 whileğ‘<|ğ¼|/ğ‘Ÿdo\n11 Câ†ğ‘“ğ‘Ÿğ‘’ğ‘ğ‘‘(ğ‘–ğ‘›,ğµ)\n12ğ‘â†ğ‘+ğµ\n13 forğ‘¥âˆˆCdo\n14 ğ‘â†ğ¹ğ‘‹(ğ‘’ğ‘›ğ‘ğ‘œğ‘‘ğ‘’(ğ‘¥))\n15 Dğ‘–,ğ‘â†Dğ‘–,ğ‘âˆª{ğ‘¥}\n16 Sğ‘â†Sğ‘+1\n17 forğ‘—âˆˆ[0,ğ‘“)do\n18 ğ‘“ğ‘¤ğ‘Ÿğ‘–ğ‘¡ğ‘’(Fğ‘–,ğ‘—,Dğ‘–,ğ‘—)\n19 ğ‘ğ‘™ğ‘’ğ‘ğ‘Ÿ(Dğ‘–,ğ‘—)\n20ğ‘“ğ‘ğ‘™ğ‘œğ‘ ğ‘’(ğ‘–ğ‘›)\n21ğ‘ â†{ğ‘¥|Ãğ‘¥\nğ‘–=0Sğ‘–â‰¤ğ‘€<Ãğ‘¥+1\nğ‘–=0Sğ‘–}\n22forğ‘–âˆˆ[0,ğ‘“)do inğ‘ parallel threads\n23 Pâ†{}\n24 forğ‘—âˆˆ[0,ğ‘Ÿ)do\n25 Pâ†Pâˆª{ğ‘“ğ‘Ÿğ‘’ğ‘ğ‘‘(Fğ‘—,ğ‘–)}\n26ğ‘“ğ‘ğ‘™ğ‘œğ‘ ğ‘’(Fğ‘—,ğ‘–)\n27ğ¿ğ‘’ğ‘ğ‘Ÿğ‘›ğ‘’ğ‘‘ğ‘†ğ‘œğ‘Ÿğ‘¡(P)\n28ğ‘œâ†Ãğ‘–\nğ‘™=0ğ‘†ğ‘™\n29ğ‘œğ‘¢ğ‘¡â†ğ‘“ğ‘œğ‘ğ‘’ğ‘›ğ‘ğ‘¡(ğ‘‚,â€œğ‘¤ğ‘â€,ğ‘œ)\n30ğ‘“ğ‘¤ğ‘Ÿğ‘–ğ‘¡ğ‘’(ğ‘œğ‘¢ğ‘¡,P)\n31ğ‘“ğ‘ğ‘™ğ‘œğ‘ ğ‘’(ğ‘œğ‘¢ğ‘¡)\ncritical path) that cannot otherwise be shortened due to data de-\npendencies while assuming unbounded parallelism[3, 12].\nThe algorithm starts by allocating enough space in the disk for\nthe output file, equal to the input size. If the underlying filesystem\nsupports sparse files, this is an ğ‘‚(1)operation (i.e., XFS, EXT4,\nNTFS, APFS.). Otherwise, this operation is linear w.r.t. the input\nsize. Next, ELSAR takes a small sample from the input and trains\nthe CDF model. Since the sample size is capped at 10M for inputs\nlarger than 1B, this is a constant operation asymptotically.\nThen, the algorithm switches from sequential to parallel mode\nduring the input reading phase. The algorithm uses ğ‘Ÿthreads that\nread equal-sized ranges from the input (i.e., each is responsibleforğ‘›/ğ‘Ÿrecords) and shuffle them into the corresponding partition\nfragments. Since the CDF prediction is made in constant time using\nthe RMI model, the partitioning phase has ğ‘‚(ğ‘›/ğ‘Ÿ)span andğ‘‚(ğ‘›)\nwork. In this case, we assume that the storage device has random\nread/write access and that the seek time is constant. Note that this\nwould not be the case in spinning disks; for example, the work for\nthe parallel read operation would increase to ğ‘‚(ğ‘›(ğ‘Ÿâˆ’1)/2)due to\nthe non-constant seek time.\nAfter the records in each reader threadâ€™s batch have been placed\ninto the predicted partition fragments, fragments across threads\nbelonging to the same partition are appended to the corresponding\npartition file. There are ğ‘“partition files in total, and the operation\nis done inğ‘ parallel threads, where ğ‘ is the maximum number of\npartitions that can be fully kept in memory at the same time. Each\npartition file will contain, on average, ğ‘›/ğ‘“records, and each of the ğ‘ \nsorter threads will be responsible for ğ‘“/ğ‘ partition files on average.\nTherefore, the span of this operation is ğ‘‚(ğ‘›/ğ‘ ), and work is ğ‘‚(ğ‘›).\nOnce each partition file has been completed, the sorter threads\ninvoke LearnedSort, whose complexity is linear with respect to the\ninput size in the average case[ 16,17]. Therefore, the span of this\noperation is ğ‘‚(ğ‘›/ğ‘ )and work is ğ‘‚(ğ‘›).\nFinally, each sorter thread flushes the sorted partition contents to\na specific offset of the output file. Again, assuming that the storage\ndevice has random write access, the seek time would be constant,\nand the span of this operation would be ğ‘‚(ğ‘›/ğ‘ )and workğ‘‚(ğ‘›).\nOtherwise, the seek time for each sorter would be ğ‘‚(ğ‘›(ğ‘“âˆ’1)/(2ğ‘ )).\nIn total, the span is ğ‘‚(ğ‘›/ğ‘Ÿ+3ğ‘›/ğ‘ )and work is ğ‘‚(ğ‘›).\n7 BENCHMARKS\nIn this section, we will evaluate the performance of ELSAR against\nother popular external sorts on ASCII datasets and various ma-\nchine configurations. First, we describe the benchmark data and\nthe hardware used for the experiments.\n7.1 Experimental Setup\nBaselines. For the following experiments, we will compare the\nsorting rates of ELSAR with GNU coreutilsâ€™ sort utility (a.k.a Unix\nsort)[ 7] and Nsort[ 25]. Unix sort is an External Mergesort and it\nrepresents the most popular external sorting paradigm (i.e., Run Cre-\nation & Merging) that is used by numerous DBMS like MySQL[ 20],\nPostgres[ 30], and SQLite[ 41]. On the other hand, Nsort is the high-\nest performing external ASCII sort that we are aware of and is\npublicly available. Note that, based on the results displayed on the\nSortBechmark website, KioxiaSort is also a very competitive algo-\nrithm. However, despite multiple attempts, we have been unable to\nobtain the code from its authors.\nData. We perform the evaluations on single-file datasets contain-\ning printable ASCII strings of fixed key and payload sizes. The key\nsize is 10 bytes, whereas the payload is 90 bytes, hence allowing for\n9510possible key values. We show results on two datasets: skewed\nand not skewed, which are generated using the gensort utility[ 40].\nFor the uniformly-distributed input, gensort produces each key\ncharacter independently and with equal probability. Whereas for\nthe skewed case, it first generates non-skewed records, then modi-\nfies the dataset to inject some skew. For this, it maintains a table\nof 128 6-byte entries, and for each record with index ğ‘Ÿğ‘’ğ‘_ğ‘–ğ‘‘ğ‘¥, it\n\nParallel External Sorting of ASCII Records Using Learned Models\nTable 2: Key statistics of the machines used for performance evaluation of ELSAR.\nCPU Model CPU freq SMT threads Disk Model Disk Type Interface Memory\nMcGraw IntelÂ®XeonÂ®Gold 6230 2.1GHz 40IntelÂ®SSDSC2KB03 SSD SATA III\n256 GB IntelÂ®SSDPE2KX010T8 SSD NVMe\nSeagate ST4000NM0115-1YZ HDD SATA III\nIntelÂ®Optaneâ„¢100 Series PMem DDR-T\nAurora IntelÂ®Coreâ„¢i5-12600K 3.7GHz 16 (12P + 4E) 4Ã—WD_BLACK SN850 (2TB) SSD M.2 NVMe 32GB\nsubstitutes the most significant bytes of the key with the table entry\nat indexğ‘¡ğ‘ğ‘ğ‘™ğ‘’ _ğ‘–ğ‘‘ğ‘¥=log2(ğ‘Ÿğ‘’ğ‘_ğ‘–ğ‘‘ğ‘¥)mod 128 .\nMachines. We use two different Linux machines, one server-\ngrade, and one desktop-grade machine. The server-grade machine,\nMcGraw, is connected to different types of storage devices, i.e.,\nHDD, SATA SSD, NVMe SSD, and IntelÂ®Optaneâ„¢persistent mem-\nory. This makes it interesting to compare the performance of the\nsorting algorithms on disks of various bandwidths and characteris-\ntics. This machine contains an IntelÂ®XeonÂ®Gold 6230 processor\nwith massive parallelism (40 threads) but not the highest clock\nspeeds. On the other hand, Aurora, the desktop machine, contains\nan IntelÂ®Coreâ„¢i5-12600K processor, which can achieve much\nhigher clock speeds. This processor has a hybrid architecture with\nsix high-performing cores and four power-efficient ones. Aurora\ncontains 4 2TB NVMe SSDs, each with an M.2 form factor directly\nconnecting to the motherboard slots. Given that this machine has\n32 GB of RAM and 8TB of disk space, it allows us to perform scal-\nability experiments, where we can measure the performance of\nthe sorting algorithms on datasets up to 40x the memory capacity.\nTable 2 summarizes key specifications that influence sorting rates.\nMethodology. We sort the records based on the ASCII binary\norder, as determined by the strncmp(3) ormemcmp(3) functions in\nthe GNU C library[ 8]. It is important to note that all the experiments\naccess disk data only via the filesystem mode, hence leveraging\nbuffered file I/O mechanisms of the OS. All measurements use the\nmaximum number of available threads in the system: 40 for the\nexperiments in the McGraw machine and 16 for the ones in the\nAurora machine. For each measurement, (1) we empty the system\nbuffer, (2) record the elapsed time via the Unix time utility which\nincludes the process setup and shutdown times, and (3) verify the\nsortedness and checksum of the output file using valsort [40]. We\nreport the mean of five consecutive runs of each algorithm.\n7.2 Different Storage Types\nAs we mentioned earlier, the disk I/O speeds heavily influence\nexternal sorting algorithms, as they make up most of the execution\ntime. Therefore, we start by looking at the sortersâ€™ performance on\nthe McGraw machine, which is connected to four different types of\nstorage: SATA HDD, SATA SSD, NVMeâ„¢SSD, and IntelÂ®Optaneâ„¢\nDC Persistent Memory. The results are shown in Figure 2. We\nshow the disk bandwidth in the dashed red line for each storage\ndevice. The bandwidth is calculated by reading the input file and\nimmediately writing it back to the same disk using the same numberof threads as the sorting algorithms. In this section, we only discuss\nthe non-skewed dataset.\nIn the case of the spinning hard drive, the bandwidths are very\nlow, and ELSARâ€™s sorting rate is 70 MB/s. This is lower than Nsortâ€™s\n(110 MB/s) and approximately equal to Unix sort (60 MB/s). This\nis a result of ELSAR heavily depending on disk seeks to maximize\nthe threadsâ€™ parallelism and minimize their co-dependence. Unlike\nSSDs, the file-seeking operations in hard disks are not constant\nwith respect to the seek offset.\nOn the other hand, in the SATA SSD, ELSAR sorts at 380 MB/s,\nwhereas Nsort at 230 MB/s, which is 39% lower. Unix sortâ€™s rate is\n130 MB/s, which is 66% lower than ELSAR. ELSARâ€™s throughput is\nonly 10% lower than the diskâ€™s bandwidth.\nAs expected, for the NVMeâ„¢disk, all three algorithms have\nhigher throughputs. ELSAR gets a 2.35 Ã—boost, Nsort 3.35Ã—, whereas\nUnix sort only a 1.41Ã—. The sorting rate of ELSAR is 900 MB/s, which\nis 18% higher than the next-best algorithm - Nsort (760 MB/s), and,\nagain, only 7% lower than the diskâ€™s bandwidth.\nAn exciting part of this experiment is the performance of these al-\ngorithms on IntelÂ®Optaneâ„¢DC Persistent Memory (PMem), which\nis a non-volatile memory technology that enables memory-like per-\nformance at storage-like capacity and costs[ 11]. The Optaneâ„¢de-\nvices are connected to the motherboard via the DIMM slots, hence\napproximating memory bus speeds for reads and writes. However,\nsince they are non-volatile media, the technology still does not\noperate as fast as DRAM speeds, placing it between DRAM and\nSSD in the performance hierarchy[ 38]. Nevertheless, ELSAR still\nmaintains its lead, sorting 13% faster than on NVMeâ„¢and outper-\nforming Nsort by 9%. In this case, ELSARâ€™s gap with respect to the\nreference line is larger because we have not tailored our code to\nuse the libpmem.h library that contains intrinsics for accessing the\nOptaneâ„¢drives[29].\nThe final storage type is a RAMdisk, a memory-mounted file\nsystem (e.g., ramfs, tmpfs) that simulates a nearly zero-latency\ndisk. This is an interesting experiment because we demonstrate\nhow these algorithms behave when they are not disk I/O-bound\nanymore. Storage media will keep getting faster over time, and this\nevaluation approximates the sorting rates in high-speed disks. In\nthis case, ELSARâ€™s sorting rate jumps to 1.43 GB/s, being 20% higher\nthan Nsort, about 7.53Ã—higher than Unix sort, and 22% lower than\nthe RAMâ€™s bandwidth.\n7.3 Skewed datasets\nIn addition to the performance on different storage types, Figure 2\nalso compares the sorting rates on skewed datasets for each disk\n\nAni Kristo and Tim Kraska\nNot skewed Skewed0.000.250.500.751.001.251.501.752.00Sorting rate (GB/s)\n0.07 0.08 0.11 0.120.06 0.07SATA HDD\nDisk BW\nELSAR\nNsort\nUnix sort\nNot skewed Skewed0.38 0.38\n0.23 0.23\n0.13 0.13SATA SSD\nNot skewed Skewed0.90 0.88\n0.76\n0.60\n0.18 0.18NVMe SSD\nNot skewed Skewed0.000.250.500.751.001.251.501.752.00Sorting rate (GB/s)1.010.92 0.92\n0.81\n0.19 0.19PMem\nNot skewed Skewed1.431.58\n1.15 1.16\n0.19 0.19RAMdisk\nFigure 2: The sorting rates of ELSAR and other baselines for different storage types belonging to the McGraw machine (higher\nis better). This graph also shows the performance on both uniform and skewed datasets. The reference line (dashed) represents\nthe bandwidth of the disk, measured as the time to fully read the input file and immediately write it out in the same directory.\nAll these measurements are performed using the maximum amount of parallelism available on the hardware.\ntype. The skewed datasets were produced using the gensort [40]\nutility with the -soption. This produces various â€œspikesâ€ in the\ndata histogram, increasing the variance in the size of the histogram\nbins (see Fig. 3). In the case of the skewed dataset, the standard\ndeviation of the bin sizes increases from 0.14% of the mean bin size\nto 65.65% of the mean bin size ( âˆ¼468Ã—higher).\nFigure 2 shows that, while the sorting rates are slightly affected\nby the skew, there is no severe performance degradation and ELSAR\nis able to absorb the skew. The reason for this is twofold: 1) The\nCDF model predictions create equi-depth partitions, as described in\n[16], since the partitions do not have fixed value ranges, and 2) even\nif some partitions are larger than other ones, and the sorting time\nwould take longer, OpenMPâ€™s dynamic thread scheduling manages\nto balance the load by assigning the under-worked threads more\npartitions to work on.\nThe results show that, for storage devices, ELSARâ€™s rate drops\nby an average of 3%, and Nsortâ€™s by 11%. The performance on hard\ndisks improves by 5% for both algorithms due to reduced randomseeks. In the case of the RAM disk, since the algorithms are not\ndisk-bound anymore, ELSARâ€™s sorting rate is 10% higher than the\nnon-skewed case. This is because the input, output, and temporary\nfiles are all in memory, and repeated access to the same few files\nleverages page cache locality.\n7.4 Data Scalability\nNext, we look at how these algorithms scale with increasing input\nsizes. Therefore, we compare the sorting rates of ELSAR for inputs\nsizes starting from 5Ã—and up to 40Ã—the size of physical memory.\nFigure 4 shows the collected results.\nWe performed these measurements on the Aurora machine, us-\ning 10 hardware threads. The disks are M.2 NVMeâ„¢SSDs that can\nreach sequential read speeds up to 7.00 GB/s[ 5]. Based on our mea-\nsurement, as described in the section above, a parallel, complete\ninput read and immediate write to these disks results in a through-\nput of 2.93 GB/s. This represents an almost 3Ã—higher throughput\n\nParallel External Sorting of ASCII Records Using Learned Models\n0 200 400 600 800 1000\nBins0.1%0.2%0.3%0.4%0.5%0.6%FrequencyNon skewed distribution\nSkewed distribution\nFigure 3: A histogram of the skewed and non-skewed\ndatasets produced from gensort . The histogram contains\n1000 bins (x-axis), and the frequency of items is shown on\nthe y-axis, and it is calculated as the ratio of the binâ€™s size\nto the input size. In the case of the skewed dataset, certain\nhistogram bins can be up to 6Ã—larger than the mean. Note\nthat there are smaller â€œspikesâ€ in the bin sizes, but they are\nnot visible due to the scale of the y-axis. The standard de-\nviation of the bin sizes for the non-skewed dataset is 0.14%\nof the mean bin size. For the skewed dataset, the standard\ndeviation of the bin sizes is 65.65% of the mean bin size.\n5x 10x 15x 20x 25x 30x 35x 40x\nRatio of input size to memory capacity0.000.250.500.751.001.251.501.75Sorting rate (GB/s)ELSAR\nNsort\nUnix sort\nFigure 4: The scalability experiment showing the perfor-\nmance of ELSAR and other baselines as the input size gets\nprogressively larger (higher is better). The evaluation is per-\nformed on the Aurora machine, on the non-skewed datasets\nvarying from 150 GB and up to 1.2 TB. The average disk\nbandwidth on the Aurora machine is 2.93 GB/s.\nfrom the NVMeâ„¢disks used by the McGraw machines, hence the\ndifference in observed speeds of the algorithms in Figures 2 and 4.\nWe observed that the average sorting rate of ELSAR is 1.66 GB/s,\nNsortâ€™s is 1.48 GB/s, and Unix sortâ€™s is 0.21 GB/s. At each increment\nof5Ã—(i.e., 150 GB), ELSAR slows down by 5% on average, whereas\nNsort 8%. Thus, when the input size is 40Ã—that of the memory\ncapacity, Nsortâ€™s throughput has dropped by 45% compared to the\nstarting point, while ELSARâ€™s only by 28%. Finally, Unix sortâ€™s\nperformance is relatively steady, only dropping, on average, by 1%\nat each step and 4% in total.\nFAWNSort (Nsort) \n 2012NTOSort (Nsort) \n 2013KioxiaSort \n 2019Nsort on Aurora  \n 2022ELSAR on Aurora \n 2022050000100000150000200000Energy consumption (J)229 kJ\n168 kJ\n89 kJ\n70 kJ\n63 kJFigure 5: The energy efficiency of ELSAR and Nsort on the\nAurora machine, and the past three winners of the Sort-\nBenchmark JouleSort category dating back up to 2012 (lower\nis better). ELSARâ€™s total energy consumption on the Aurora\nmachine is 63 kJ, which is 41% lower than the current bench-\nmark winner - KioxiaSort[37] at 89 kJ. KioxiaSort is an Ex-\nternal Mergesort algorithm. The FAWNSort and NTOSort\nsubmissions both use Nsort but on different hardware. We\nshow the energy consumption of Nsort on our hardware (Au-\nrora) as well for comparison.\n7.5 Energy efficiency\nAnother perspective for evaluating external sorting algorithms is\ntheir energy efficiency. Energy consumption is calculated as the\nproduct of the total execution time and the systemâ€™s average power.\nFor example, a sorting algorithm that runs on a system whose CPU\nhas a massive number of cores to boost parallelism or connected to\nmany fast disks to increase I/O bandwidth may perform extremely\nfast. However, its power consumption is also very high. For exam-\nple, the PMem drive used in the McGraw machine has 1.4Ã—faster\nperformance than the NVMeâ„¢SSD, but its average power rating is\n3Ã—higher[ 9,10]. In contrast, the algorithm might be slower on a\ndesktop system with a limited number of CPU cores and disks, but\nits overall power is low. For example, typical desktop-grade proces-\nsors, such as the IntelÂ®Coreâ„¢i5 with ten cores, has a base TDP\nof 65 W, which is 1.92Ã—lower than that of the IntelÂ®XeonÂ®Gold\n6230 processor used in the McGraw machine, which has 20 cores.\nTherefore, the energy consumption metric is an excellent choice to\ncompare external sorting algorithms on different hardware because\nit incorporates both the time and system resources via the power\ndissipation measure. In addition, we do not possess the source code\nor binaries of the sorting algorithms in Figure 5, so we can only\ncompare their reported energy readings.\nThe SortBenchmark has a particular category - JouleSort[ 36],\ndedicated to submissions for the most energy-efficient algorithm.\nThe importance of energy efficiency is associated with lower busi-\nness costs for providing power to the computer systems running\nthese processes, lower costs for building adequate cooling infras-\ntructure, and environmental and sustainability concerns. Each year,\nthe benchmark accepts submissions that have been able to sort\n\nAni Kristo and Tim Kraska\n1TB of 100-byte ASCII records with 10-byte keys while consum-\ning less energy than the existing record. The current winner of\nthe benchmark is a 2019 submission named KioxiaSort[ 37], which\nperforms the task with 89 kJ of energy. Since the metric is energy\nconsumption and not only execution time, entries are allowed to\nuse any hardware configuration, with the majority using desktop-\ngrade machines[ 6,28,34,37]. Other SortBenchmark categories like\nGraySort and CloudSort contain submissions from very large cloud\nproviders like Tencent[ 13] or Alibaba[ 44], who use high-speed net-\nworks of several hundred virtual machines to sort 100 TB of records.\nWhile we would have loved to participate in those benchmarks as\nwell, it appears that cloud pricing and the ability to have access\nto huge amounts of servers are more critical in those categories\nthan the actual sorting techniques used. Figure 5 summarizes the\nenergy consumption of the last three winners of the SortBenchmark\nJouleSort category, as well as the energy consumption of Nsort and\nELSAR on the Aurora machine, the machine we purposefully build\nto be energy efficient for sorting.\nIn Figure 5, the first bar belongs to FAWNSort (2012), which uses\nNsort on a system with IntelÂ®Coreâ„¢i7 with 8 GB of RAM and 16\nSSDs[ 28]. The second bar belongs to NTOSort (2013), which also\nuses Nsort, and their system also consists of a newer-generation\ni7 processor and 16 faster SSDs[ 6]. The third one is KioxiaSort\n(2019), which implements its version of an External Mergesort on a\nsystem with an IntelÂ®Coreâ„¢i9 processor with 64 GB of RAM and\n8 RAID-0 NVMeâ„¢SSDs. Finally, the last two bars show the energy\nconsumption of Nsort and ELSAR on our Aurora machine.\nELSAR on the Aurora machine consumes 62,912 J, which is 41%\nlower than the current winner of the benchmark. This sets a new\ncandidate for the record for the lowest energy consumption for the\nbenchmark task by a significant margin.\nFor reference, we also show the energy consumption of Nsort\non Aurora, which is the same software used by FAWNSort and\nNTOSort, but on our hardware. We included the evaluation of Nsort\non our machine to emphasize that the performance improvements\nseen by ELSAR come as a result of the algorithmic design and not\njust purely from the hardware used. Nsort uses 69,772 J, which is\n11% larger than ELSARâ€™s energy consumption.\n7.6 A breakdown of the algorithmic portions\nWe analyze ELSAR even further by breaking down how much time\nthe algorithm spends on each algorithm phase. In addition, we also\nlook at the breakdown of energy consumption for each portion.\nThe results are shown in Figure 6.\nThe training procedure used to partition records is the smallest\nportion of the algorithm, taking <1% of the total execution time\nand energy consumption. On the other hand, the biggest portion\nis the record partitioning (23.5% of the total time and 23.8% of the\ntotal energy). This is when the algorithm splits the input records\nonto different fragment files based on the CDF predictions. It is the\nmost expensive operation of the algorithm as the records are not\nin any particular order, and they incur random writing patterns.\nIn addition, record coalescing takes up roughly 7% of the time\nand energy. We added this optimization to speed up the time it takes\nthe algorithm to flush the records to the output file by making the\nwrites in sequential batches. While 7% is not insignificant, the total\n0% 20% 40% 60% 80% 100%\nAlgorithm portionsTimeEnergy\nTrain\n0.2%Train\n0.1%\nLoad\n17.5%Load\n15.4%\nPartition\n23.5%Partition\n23.8%\nCombine\n19.4%Combine\n18.3%\nSort\n15.3%Sort\n15.1%\nCoalesce\n6.4%Coalesce\n6.8%\nFlush\n17.6%Flush\n20.5%Figure 6: The breakdown of the algorithmic portions of EL-\nSAR in terms of execution time and energy consumption.\nThese measurements were performed on the Aurora ma-\nchine. The energy consumption of each portion was calcu-\nlated as the number of seconds consumed by it multiplied by\nthe average power consumed by the algorithm up until then.\nThe training time and energy are not visible, representing\nless than 1% (in orange for contrast).\nELSAR Nsort Unix sort01234Total I/O bytes1e11\n+17%+89%(a)\nELSAR Nsort Unix sort0%20%40%60%80%Portion of time spent in I/O+32%+347%(b)\nFigure 7: The I/O performance of ELSAR and other baseline\nalgorithms. In (a), we show the I/O size of each algorithm, de-\nfined as the total number of bytes written and read to/from\ndisk (lower is better). In (b), we show the portion of the time\nthat these algorithms spend performing read and write sys-\ntem calls.\ntime taken by coalescing and flushing ( âˆ¼24%) is still less than how\nmuch it would take to perform the flushing without coalescing.\n7.7 I/O performance\nFinally, we also dissect the I/O performance of ELSAR, Nsort, and\nUnix sort by looking at the I/O workload of each algorithm and\nthe portion of execution time they spend doing I/O. This analysis\nallows us to quantify how much these algorithms are disk I/O-\nbound, which is the major bottleneck of external sorting. In order\nto provide this metric, we use the strace Linux utility to log the\nsize of the read andwrite system calls of each algorithm and their\nchildren processes (i.e., threads). We perform this evaluation on the\nAurora machine with an input of size 100GB.\nIn Figure 7(a), we show the I/O load of each algorithm, which we\ndefine as the total number of bytes written to and read from disk,\nas reported by the strace tool. Relative to ELSAR, Nsortâ€™s I/O load\nis 17% larger, and Unix sortâ€™s is 89% larger.\n\nParallel External Sorting of ASCII Records Using Learned Models\nOn the other hand, in Figure 7(b), we show the portion of the exe-\ncution time that these algorithms spend performing I/O operations\non disk. ELSAR spends 17% of its total execution time performing\nI/O, whereas Nsort spends 23% of its time in I/O, which is 32% longer\nthan ELSAR. Finally, for Unix sort, the I/O time is 76% of its total\nexecution time, which is almost 3.5Ã—higher than ELSARâ€™s.\nThese results show that ELSAR spends less time performing I/O\ndue to a smaller load, leading to better overall performance.\n8 CONCLUSION\nThis paper introduced a novel external sorting algorithm, ELSAR,\nwhich leverages learned data distribution models. Unlike the exter-\nnal merge-sort paradigm, ELSAR partitions the input in a mutually\nexclusive, equi-depth, and monotonically increasing way, eliminat-\ning the need for a file merging routine. Instead, it performs a simple\nfile concatenation, which is significantly faster. We also showed\nmultiple evaluations of ELSAR and existing external sorting algo-\nrithms on different machines, storage media, distributions, and in-\nput sizes. We consistently observed higher sorting rates for ELSAR\nthan other sorting algorithms. Furthermore, ELSAR superseded\nthe SortBenchmark JouleSort record for the most energy-efficient\nsorting algorithm by 41%.\nNevertheless, this work only represents the initial results of\nthis sorting algorithm. In the future, we intend to make ELSAR a\nhigh-performing distributed sorting algorithm that can work with\ndatasets in the order of hundreds of terabytes and also supports\nUnicode characters.\nREFERENCES\n[1]A. Beckmann, U. Meyer, P. Sanders, and J. Singler. 2010. Energy-Efficient Sorting\nusing Solid State Disks. http://sortbenchmark.org/ecosort_2010_May_15.pdf\n[2]Mike W. Blasgen and Kapali P. Eswaran. 1977. Storage and access in relational\ndata bases. IBM Systems Journal 16, 4 (1977), 363â€“377.\n[3]Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein.\n2009. Introduction to Algorithms, Third Edition (3rd ed.). The MIT Press, Cam-\nbridge, Massachusetts.\n[4]J. D. Davis and S. Rivoire. 2010. FlashSort: JouleSort Benchmark Entry and 2010\nDaytona 10 GB class. http://sortbenchmark.org/flashsort_2010_Jan_01.pdf\n[5]Western Digital. 2021. WD_BLACK SN850 NVMeâ„¢SSD 2TB with Heatsink\nspecifications. https://www.westerndigital.com/products/internal-drives/wd-\nblack-sn850-nvme-ssd#WDS200T1XHE\n[6] A. Ebert. 2013. NTO Sort. http://sortbenchmark.org/NTOSort2013.pdf\n[7]GNU. 2022. Free Software Foundation, Inc., GNU coreutils/sort. https://github.\ncom/coreutils/coreutils/blob/master/src/sort.c\n[8]GNU. 2022. The GNU C Library. https://www.gnu.org/software/libc/manual/\nhtml_mono/libc.html\n[9]IntelÂ®. 2018. SSD DC P4510 Series. https://ark.intel.com/content/www/us/en/\nark/products/122573/intel-ssd-dc-p4510-series-1-0tb-2-5in-pcie-3-1-x4-3d2-\ntlc.html\n[10] IntelÂ®. 2019. Optaneâ„¢Persistent Memory 200 Series Brief. https:\n//www.intel.com/content/www/us/en/products/docs/memory-storage/optane-\npersistent-memory/optane-persistent-memory-200-series-brief.html\n[11] IntelÂ®. 2019. Optaneâ„¢technology. https://www.intel.com/content/www/us/\nen/architecture-and-technology/intel-optane-technology.html\n[12] J. JÃ¡JÃ¡. 1992. An Introduction to Parallel Algorithms . Addison-Wesley Pub-\nlishing Company, Reading, MA, USA. https://books.google.com/books?id=\nUom6wAEACAAJ\n[13] J. Jiang, L. Zheng, J. Pu, X. Cheng, C. Zhao, M. Nutter, and J. D. Schaub. 2016.\nTencent Sort. http://sortbenchmark.org/TencentSort2016.pdf\n[14] Ken Whistler and Markus Scherer. 2021. UNICODE COLLATION ALGORITHM .\nStandard. Unicode. https://unicode.org/reports/tr10/\n[15] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In Proceedings of the 2018 international\nconference on management of data . Association for Computing Machinery, New\nYork, NY, USA, 489â€“504.\n[16] Ani Kristo, Kapil Vaidya, Ugur Ã‡etintemel, Sanchit Misra, and Tim Kraska. 2020.\nThe case for a learned sorting algorithm. In Proceedings of the 2020 ACM SIGMODInternational Conference on Management of Data . Association for Computing\nMachinery, New York, NY, USA, 1001â€“1016.\n[17] Ani Kristo, Kapil Vaidya, and Tim Kraska. 2021. Defeating duplicates: A re-\ndesign of the LearnedSort algorithm. https://doi.org/10.48550/arXiv.2107.03290)\narXiv:arXiv:2107.03290\n[18] M. Liu, K. Zhang andS. Peter, and A. Krishnamurthy. 2019. TaichiSort: Energy-\nefficient Sorting of 1TB with NVMe and Coffee Lake. http://sortbenchmark.org/\nTaichiSort2019.pdf\n[19] P. Mockapetris. 1987. Domain names - implementation and specification . Technical\nReport 1035. DNS. https://doi.org/10.17487/RFC1035\n[20] MySQL. 2021. Oracle, MySQL filesort.cc. ver 8. https://github.com/mysql/mysql-\nserver/blob/8.0/sql/filesort.cc\n[21] MySQL. 2022. 5.7 Reference Manual :: Unicode Support. https://dev.mysql.com/\ndoc/refman/5.7/en/charset-unicode.html\n[22] MySQL. 2022. Index Creation. https://github.com/mysql/mysql-\nserver/blob/7ed30a748964c009d4909cb8b4b22036ebdef239/mysql-test/r/\nsingle_delete_update.result\n[23] MySQL. 2022. ORDER BY clause. https://github.com/mysql/mysql-\nserver/blob/7ed30a748964c009d4909cb8b4b22036ebdef239/mysql-test/t/single_\ndelete_update.test\n[24] Chris Nyberg, Jim Gray, and Charles Koester. 2004. A minute with Nsort on a\n32P NEC Windows Itanium2 server.\n[25] C. Nyberg, C. Koester, and J. Gray. 2000. Nsort: a Parallel Sorting Program for\nNUMA and SMP Machines. http://www.ordinal.com/NsortPara.pdf\n[26] Hwee Hwa Pang, Michael J Carey, and Miron Livny. 1993. Memory-adaptive\nexternal sorting . Technical Report. University of Wisconsin-Madison Department\nof Computer Sciences.\n[27] T. Peters. 2022. Timsort. https://svn.python.org/projects/python/trunk/Objects/\nlistsort.txt\n[28] P. Pillai, M. Kaminsky, M. Kozuch, and D. Andersen. 2012. FAWNSort: Energy-\nefficient Sorting of 10GB and 100GB and 1TB. http://sortbenchmark.org/\nfawnsort-joulesort-2012.pdf\n[29] PMDK. 2022. The libpmem library. https://pmem.io/pmdk/libpmem/\n[30] Postgres. 2021. PostgreSQL, tuplesort.c. https://github.com/postgres/postgres/\nblob/166f94377c886516ca986ef8a623cd2e854fe911/src/backend/utils/sort/\ntuplesort.c\n[31] Postgres. 2022. Hash & B-Tree Index Creation. https://github.com/postgres/\npostgres/blob/40af10b571bdabbab06839d090083c9f9c2091d2/src/include/utils/\ntuplesort.h\n[32] Postgres. 2022. ORDER BY clause. https://github.com/postgres/postgres/blob/\nc30f54ad732ca5c8762bb68bbe0f51de9137dd72/src/test/regress/sql/tuplesort.sql\n[33] Postgres. 2022. Relation Clustering source code. https://github.com/postgres/\npostgres/blob/4036bcbbb92839f20ed6d0db418996954dd66470/src/backend/\ncommands/cluster.c\n[34] M. Rahn, P. Sanders, J. Singler, and T. Kieritz. 2009. DEMSort â€” Distributed\nExternal Memory Sort. http://sortbenchmark.org/demsort.pdf\n[35] Waleed Reda and Dejan KostiÄ‡. 2021. RezSort: Sorting 1TB using Energy-efficient\nNVMe SSDs. http://sortbenchmark.org/RezSort2021.pdf\n[36] Suzanne Rivoire, Mehul A. Shah, Parthasarathy Ranganathan, and Christos\nKozyrakis. 2007. JouleSort: A Balanced Energy-Efficiency Benchmark. In Pro-\nceedings of the 2007 ACM SIGMOD International Conference on Management of\nData (Beijing, China) (SIGMOD â€™07) . Association for Computing Machinery, New\nYork, NY, USA, 365â€“376. https://doi.org/10.1145/1247480.1247522\n[37] S. Sano, Z. Mahmoud, and T. Suzuki. 2019. KioxiaSort: Sorting 1TB by 89K Joules.\nhttp://sortbenchmark.org/KioxiaSort2019.pdf\n[38] Anil Shanbhag, Nesime Tatbul, David Cohen, and Samuel Madden. 2020. Large-\nScale in-Memory Analytics on Intel<sup> Â®</sup> Optane<sup> â„¢</sup> DC\nPersistent Memory. In Proceedings of the 16th International Workshop on Data\nManagement on New Hardware (Portland, Oregon) (DaMoN â€™20) . Association for\nComputing Machinery, New York, NY, USA, Article 4, 8 pages. https://doi.org/\n10.1145/3399666.3399933\n[39] Muthian Sivathanu, Saurabh Goyal, and Rajiv Mathews. 2018. Updating inverted\nindices. US Patent 10,073,874.\n[40] SortBenchmark. 2022. gensort Data Generator. http://www.ordinal.com/gensort.\nhtml\n[41] SQLite. 2011. vdbesort.c. https://sqlite.org/src/file?name=src/vdbesort.c&ci=\ntrunk\n[42] The PHP Group. 2022. PHP Strings: Manual. https://www.php.net/manual/en/\nlanguage.types.string.php\n[43] J. Wang, Y. Wu, H. Cai, Z. Tang, Z. Lv, B. Lu, Y. Tao, C. Li, J. Zhou, and H. Tang.\n2015. FuxiSort. http://sortbenchmark.org/FuxiSort2015.pdf\n[44] Q. Wang, R. Gu, Y. Huang, R. Xin, W. Wu, J. Song, and J. Xia. 2016. NADSort.\nhttp://sortbenchmark.org/NADSort2016.pdf\n[45] R. Xin, P. Deyhim, A. Ghodsi, X. Meng, and M. Zaharia. 2014. GraySort on Apache\nSpark by Databricks. http://sortbenchmark.org/ApacheSpark2014.pdf\nOptimization Notice: Software and workloads used in performance tests\nmay have been optimized for performance only on Intel microprocessors.\n\nAni Kristo and Tim Kraska\nPerformance tests, such as SYSmark and MobileMark, are measured using\nspecific computer systems, components, software, operations and functions.\nAny change to any of those factors may cause the results to vary. You\nshould consult other information and performance tests to assist you infully evaluating your contemplated purchases, including the performance\nof that product when combined with other products. For more information\ngo to http://www.intel.com/performance. Intel, Xeon, and Intel Xeon Phi\nare trademarks of Intel Corporation in the U.S. and/or other countries.",
  "textLength": 63352
}