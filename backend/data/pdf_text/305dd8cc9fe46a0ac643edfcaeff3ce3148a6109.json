{
  "paperId": "305dd8cc9fe46a0ac643edfcaeff3ce3148a6109",
  "title": "Evaluating Learned Indexes for External-Memory Joins",
  "pdfPath": "305dd8cc9fe46a0ac643edfcaeff3ce3148a6109.pdf",
  "text": "Evaluating Learned Indexes for External Memory Joins\nYuvaraj Chesetti∗Prashant Pandey†\nAbstract\nJoins are among the most time-consuming and data-intensive\noperations in relational query processing. Much research effort\nhas been applied to the optimization of join processing due\nto their frequent execution. Recent studies have shown that\nCDF-based learned models can create smaller and faster indexes,\naccelerating in-memory joins. However, their effectiveness for\nexternal-memory joins, which are crucial for large-scale databases,\nremains underexplored.\nThis paper evaluates the impact of learned indexes on\nexternal-memory joins for both sorted and unsorted data. We\ncompare learned index-based joins against traditional join\nmethods such as hash joins, sort joins, and indexed nested-loop\njoins on real-world and simulated datasets. Additionally, we\nanalyze learned index-based joins across multiple dimensions,\nincluding storage device types, data sorting, parallelism,\nconstrained memory environments, and varying model error. The\ndetailed evaluation enables us to determine the most appropriate\nlearned index to employ for external-memory joins.\nOur experiments reveal that, unlike in-memory settings,\nlearned indexes in external-memory joins can trade off accuracy\nfor space without significantly degrading performance. While\nlearned indexes provide smaller index sizes and faster lookups,\nthey perform similarly to B-trees in external-memory joins since\nthe total amount of I/O, which dominates runtime, remains\nunchanged. Additionally, the construction times of learned\nindexes are ∼1000×longer, and although they are 2–4 ×smaller\nthan the internal nodes of a B-tree, these nodes only represent\n0.4%–1% of the data size and typically fit in main memory.\n1 Introduction\nThe join operation is a fundamental operation in database\nmanagement systems. Data normalization spreads data\nacross multiple relational tables, and the join operation\nenables combining tuples from two or more relations based on\na common attribute. Implementing join efficiently is challeng-\ning as it involves iterating through the tuples across multiple\nrelations, incurring large amounts of disk I/Os. The join\noperation is often critical for the overall database performance\nas it is expensive and frequently executed. This is especially\ntrue for online analytics processing (OLAP) systems where\nthe data is static and large-scale multidimensional analytical\nqueries are frequent. This has resulted in extensive research\non optimizing joins in recent decades [ 4,6,7,10,31,29,21].\nRecently, machine learning has significantly influenced the\nautomation of fundamental database functions and design\nchoices. Specifically, researchers have shown that indexes\nbased on learned models that approximate the cumulative\ndistribution function (CDF) [20]—which is effectively the\nrank function for items in a dataset —- are faster and\nsmaller than traditional indexing data structures [ 25]. These\nlearned structures and algorithms often outperform their\ntraditional counterparts because they can accurately capture\ndata trends and optimize performance for specific instances.\n∗Northeastern University.\n†Northeastern University.A recent survey [ 3] identifies close to 100 proposed learned\nindexes in the last five years.\nA recent benchmarking study [25] demonstrates that\nlearned index structures such as the PGM-Index [14],\nRMI [ 20] and RadixSpline [ 18] can deliver superior lookup\nperformance on sorted data compared to traditional indexes.\nKristo et al. [22] show that learned models can speed\nup sorting in main memory for integer keys. Sabek and\nKraska [29] demonstrate that learned models can improve\nin-memory indexed nested loop joins by 5 −25% and can\nimprove the partitioning step in parallel hash and sort-joins.\nWhile recent work on in-memory benchmarks has shown\npromising results, the benefits of learned indexes in the ex-\nternal memory settings do not appear to be as clear. Lan et\nal. [23] adapt multiple updatable learned indexes to external\nmemory and evaluate performance on read, scan, and write\nworkloads. They observe that in their current iterations,\nnone of the updatable learned indexes are competitive with\ntraditional B-trees. Zhang et al. [35] identify that directly\nextending learned indexes to disk environments often re-\nsults in suboptimal performance compared to traditional\nB-trees. However, they show that employing optimizations\nsuch as leaf-page fetching strategies, prediction granularity,\nand adapting for disk characteristics can help bridge this gap.\nExisting benchmarking studies involving learned indexes\nhave not focused on external memory database joins, which\nare crucial for large-scale databases. External-memory joins\noffer vastly different trade-offs and challenges compared to\nin-memory joins. For relations stored in external memory,\nthe dominating cost in performing the join is the I/O cost.\nRandom I/Os are more expensive compared to sequential\nones, resulting in different trade-off choices [34, 17, 27].\nAdditionally, different storage devices such as hard disks and\nSSDs offer different I/O bandwidths and latencies, leading to\na different choice of indexing parameters to achieve optimal\nperformance. Therefore, the insights gained from employing\nlearned models for accelerating in-memory joins are relevant\nbut not directly applicable in the context of external memory.\nThis paper. We investigate the applicability of learned\nindexes for external-memory joins, a common operation\nin large-scale databases. Through an extensive empirical\nevaluation, we assess whether the performance advantages\nof learned indexes in in-memory settings extend to\nexternal-memory environments.\nTo utilize learned indexes for external-memory joins\n(learned index-based join), we replace the traditional index\nin the indexed nested-loop join with a learned index and\nintroduce optimizations tailored for external-memory settings\nto enhance performance. This approach aligns with the\nmethodology used by Sabek and Kraska [ 29] in their study\non in-memory learned joins. We conduct a comprehensive\nCopyright ©2025 by SIAM\nUnauthorized reproduction of this article is prohibitedarXiv:2407.00590v3  [cs.DB]  23 May 2025\n\nempirical evaluation, comparing learned index-based join\nwith various traditional external-memory join algorithms.\nOur study results in the following key contributions:\n•We evaluate the performance of the indexed-nested loop\njoin using learned indexes on real-world and synthetic\nsorted datasets.\n•We evaluate learned indexes across a range of parameter\nsettings, including table-size ratios (selectivity), concur-\nrency, model accuracy, and types of storage devices (SSDs\nand HDDs).\n•We evaluate the performance of learned indexes for\njoins on unsorted data to produce a sorted output by\npartitioning the data using the learned CDF model and\nevaluate its performance under memory pressure.\n•We analyze experimental results and make several key\nobservations that will benefit future work in incorporating\nlearned indexes as part of the query execution engine in\nlarge-scale databases.\nKey Takeaways. Here we list the key takeaways from our\nexperimental evaluation:\n•For external-memory joins, learned indexes can benefit\nfrom using higher errors, and in turn achieve lower memory\nusage without loss in join performance. Using a larger\nerror window size for the learned model helps increase disk-\nbandwidth utilization for learned indexes when the join\nno longer does sequential I/Os, which can happen when\nthe sizes of two sorted tables being joined differ by a large\nfactor. This enables smaller indexes and faster operations.\n•While learned indexes provide smaller index sizes and\nfaster lookups, they do not reduce total I/O costs,\nleading to performance that is comparable to B-trees in\nexternal-memory joins.\n•On SSDs, learned indexes improve the time to perform\na join on sorted data compared to B-trees by 1 .2−1.6×\nwhen the input tables must be completely scanned. The\nimprovement in performance on SSDs comes from faster\nCPU performance in querying for items.\n•Learned indexes scale similarly to B-trees with increasing\nthreads, but they become I/O-bound at high levels of\nparallelism.\n•Learned indexes have an order-of-magnitude higher\nbuild time than B-trees when built on the entire dataset.\nSampling keys to build the learned index enables faster\nindex construction without a noticeable impact on the\njoin performance. However, even with sampling, the\nlearned index build time is 10 ×larger than B-trees.\n2 Background & Related work\nIn this section, we briefly describe the various learned\nindex designs and their usage in database applications.\nWe highlight the key insights and unanswered questions\npertaining from these studies.\nKraska et al. [20] showed that machine learning models\ncan help reduce the memory footprint and increase the\nperformance of traditional indexing data structures such as\nB-trees, hash tables, and filters. More specific to databases,\nC.D.F \nApproximation 01\n0Learned \nIndex Sorted List \nLast level models Inner models Figure 1: Modeling the CDF using piecewise linear approx-\nimation and indexing into the individual linear models.\nthere is considerable interest in designing learned indexes\nfor sorted data that deliver better performance with lowered\nspace usage than B-trees that are commonly used to\nimplement database indexes.\nVarious learned index designs have been proposed, includ-\ning read-only indexes such as the Recursive model index\n(RMI) [ 20], RadixSpline [ 18] and Piecewise geometric model\n(PGM) index [14] and updatable learned indexes such as\nALEX [ 12], FITing Tree [ 15] and LIPP [ 33] as replacements\nfor B-trees in static and dynamic workloads, respectively.\nFor a more detailed history of the work in this area, please\nrefer to [3] which extensively surveys learned indexes.\nThere has also been considerable research showing the\napplicability of learned models in other applications. Recent\nworks include improvements to fundamental algorithms such\nas sorting and joins [ 22,29], range index designs [ 12,33,14],\nquery performance in log-structured merge-trees [11, 1],\nmulti-dimensional and spatial indexes [ 24,36] and genome\nsequencing [19, 16].\n2.1 Learned indexes In this section, we briefly describe\nvarious types of learned indexes.\nRange indexes are CDF models. The key insight\nthat ties machine learning techniques and deterministic\nproblems such as indexing is that all range indexes model\nthe cumulative distribution function (CDF) of the data they\nindex [ 20]. The cumulative distribution function of a list is\na function F(x) that maps the probability that xis greater\nthan an item picked randomly from that list L. When the\nCDF value for an item xis scaled by the number of items\nin the list, it returns the position where xwould fit in that\nlist, i.e. pos(x)=F(x).|L|, where |L|is the size of the list.\nCDF-based models. Learned index implementations follow\na common design trend in modelling the CDF. They often\nemploy regression techniques (as opposed to deep neural\nnetworks) to model the CDF. As a single model is practically\nnot enough to model the entire CDF, the CDF is modelled\nin a piecewise manner with a hierarchical structure indexing\nindividual piecewise models. This is illustrated in Figure 1.\nThe CDF model only approximates the position of a key in\nthe list, i.e., it cannot accurately determine the position of the\nkey. However, this is not a significant issue in practice as the\nmodel error is usually bounded and a local last-mile search\ncan be performed to return the exact answer to a query (Fig-\nure 2). The model error is a training parameter that trades\naccuracy for space usage, i.e., high-accuracy models require\nCopyright ©2025 by SIAM\nUnauthorized reproduction of this article is prohibited\n\nq\nqlast-mile \nsearch window Figure 2: Query path using a learned index. The inner levels\ndirect the query to the approximate location in the dataset.\nA last-mile search is performed to find the query key.\nmore space but a smaller last-mile search and vice versa.\n2.2 Learned index designs Multiple learned index\ndesigns have been proposed, each using a unique modelling\ntechnique and data layout to implement the index. Below,\nwe go over a few learned indexes that we use as candidates\nin our external-memory join implementation.\nRecursive model index (RMI). RMI [ 20] builds a hierar-\nchy of models on the dataset given a specification by the user.\nThe user specification provides various parameters, such as\nthe type of models and learning techniques to use, the maxi-\nmum branching factor of each level, and the maximum size of\nthe final model. To help with tuning, the RMI implementa-\ntion provides an optimizer that performs a grid search of RMI\nmodels against the space/performance curve, allowing users to\nchoose the RMI configuration appropriate for their use case.\nRadixSpline. RadixSpline [18] is a single-pass learned\nindex that uses a linear spline to model the CDF and\nemploys a radix table to speed up the lookup of spline\npoints. To perform a key search, the radix table is first\nconsulted to return a range of linear spline segments. The\nreturned linear spline segments are then used to find the\nerror-bounded position of the lookup key. The RadixSpline\nis parametrizable in two parameters, the error-bound of the\nlinear spline model and the number of the bits to use for\nthe radix table. A larger error bound reduces the number of\nlinear spline segments used to model the CDF but increases\nthe last-mile search window. Similarly, using more bits for\nthe radix table speeds up finding the correct linear spline\nmodel for a query at the cost of a larger radix table.\nPiecewise geometric model (PGM) index. The PGM\nindex [14] is a learned index that is similar to the RMI\nin that it builds a hierarchical structure of models. Each\nlevel is an error-bounded linear regression model built using\npiecewise linear approximation (PLA), i.e., the model is a\npiecewise function where each component is a linear function.\nThe model can be visualized as a list of line segments that\napproximate a curve. The lowest level model approximates\nthe CDF curve of the data, while higher levels are PLA\nmodels approximating the CDF curve of line-segment end\npoints of the next level. Similar to the RadixSpline, the\nPGM is configurable by two parameters, the error bound\nand the maximum height of the index.\nAdaptive Learned Index (ALEX). The aforementionedlearned indexes are read-only indexes. ALEX [12] is an\nupdatable learned index that augments the traditional\nB-tree nodes with learned models. ALEX maintains the\ninvariant that the data inside a node follows the distribution\nof the learned model associated with the node. ALEX uses\na cost model to help with decisions related to rebalancing\nthe tree when new items are inserted or deleted. While\nALEX is implemented as an in-memory data structure, we\nuse the implementation adapted for disks by Lan et al. [ 23].\n2.3 Related work In this section, we describe related\nresearch work evaluating learned indexes.\nBenchmarks for learned indexes. The Search on Sorted\nDatasets (SOSD) benchmark [25] evaluates the lookup\nperformance of various learned indexes in main memory on\nsorted data. Lan et al. [ 23] adapt multiple updatable learned\nindexes to external memory and evaluate performance on\nread, scan, and write workloads. They observe that in their\ncurrent iterations, none of the updatable learned indexes\nare competitive with B-trees on all workloads.\nLearned indexes for in-memory joins and sorting.\nSabek and Kraska [ 29] show that learned indexes can improve\nthe performance of in-memory joins. They show that learned\nindexes must be carefully adapted for each of the three main\ntypes of joins — hash, index-nested loop, and sort join — and\nusing them as black-box replacements for traditional indexes\nis not optimal. For instance, they observe that index-nested\nloop joins benefit from removing the last-mile search by redis-\ntributing elements according to the learned model predictions.\nAs another example, hash and sort join benefit from using\nthe sampled CDF to uniformly partition the workload across\ndifferent cores. Similarly, Kristo et al. [ 22] show that utilizing\nCDF approximations to recursively partition unsorted data\ninto buckets results in a sorting algorithm that is 1 .49−5×\nfaster than various state-of-the-art sorting algorithms.\n3 Approach and Analysis\nIn this section, we describe and analyze the usage of learned\nindexes for external-memory joins. We describe the overall\napproach of the learned index-based join and then describe\nspecific optimizations. Finally, we analyze the performance\nin I/O cost using the affine memory model [8].\n3.1 Approach We use a learned index to support the\nindexed nested-loop join. During the join, keys from the\nsmaller table are streamed, and each key is looked up in the\nlearned index of the larger table to determine if it contributes\nto the join result (see Figure 3). Since the join is performed\nusing indexes, the resulting output is naturally sorted.\nFor sorted tables, the learned index-based join uses two\noptimizations. The first optimization reduces the overall\ncost of the learned index look-up. The second optimization\nminimizes the size of the last mile search. We describe both\noptimizations below.\nOptimization 1. The first optimization is to avoid traversing\nthe entire height of a learned index for each query. Typically, a\nlearned index query needs to start from the root and traverses\nCopyright ©2025 by SIAM\nUnauthorized reproduction of this article is prohibited\n\nTable R \n(Disk) \nTable S \n(Disk) \nLearned Index \n(Table S) \nJoin output \n(R ⨝S, Disk) \nMain Memory Last-mile \nsearch \nFigure 3: Illustration of the learned index-based join: Tables RandS(with Sbeing the larger table) reside on disk,\nwhile the learned index for Sis kept in memory. Pages of table Rare sequentially loaded into memory, and for each\nkey, the learned index predicts the corresponding page in Sto perform the final lookup.\ndown to the leaf that contains the model assigned to handle\nqueries for the query key (Figure 2). However, we can take\nadvantage of the fact that queries increase monotonically\nduring the join operation to reduce the query cost. Instead\nof traversing the entire height of the learned index for each\nquery, we use a leaf node iterator that traverses the breadth\nof the last level. This iterator will advance to the next model\nwhen the query (which is the candidate join key) exceeds\nthe range assigned to the model of the current leaf.\nOptimization 2. The second optimization also takes\nadvantage of the fact that queries during a join on sorted\ndata occur in monotonically increasing order. In other words,\nthe search window only advances forward. For example, if\nthe lower bound for key Kiis atLi, and the learned index\nreturned a search window [ Lj,Hj] for key Kjwhere j >i,\nthen the last-mile search for key ( Kj) can be constrained\nto [MAX( Li,Lj), Hj].\n3.2 Analysis We analyze the cost of using a learned\nindex for indexed nested loop joins.\nSetup. We perform the join on two input tables R, S\ncontaining |R|and|S|number of keys respectively. We\nwill assume that the tables are large enough such that their\ncombined size exceeds main memory, and that a learned\nindex for Shas already been built offline. We will also\nassume that the learned index fits in memory. Without loss\nof generality, we will assume that |R|<|S|. For a query key\nq, the learned index of table Swill return a search window\nfor where qmight exist. The size of the window will never\nexceed ϵ, the maximum error bound of the learned index,\nand is a training parameter for the learned index.\nThe affine model. We analyze the performance of the\nlearned index-based join in the affine model [ 8]. Traditionally,\nthe disk-access machine (DAM) [2] model has been used\nto evaluate the performance of external-memory algorithms\nand data structures in terms of the number of I/O transfers\nin the memory hierarchy. However, the DAM model does\nnot assign a cost to each I/O. On HDDs, it does not model\nthe faster speeds of sequential I/O versus random I/O. On\nSSDs, it does not model internal device parallelism or the\nincremental cost of larger I/Os.\nThe affine model [28, 5, 8] makes small refinements tothe DAM model, but yields a surprisingly large improvement\nin predictability without sacrificing ease of use. The affine\nexplicitly account for seeks (in spinning disks) by modelling\nthe cost of an I/O of kwords as 1+ αk, where α≪1 is a\nhardware parameter.\nI/O cost analysis. We will divide the I/O cost of the\nlearned index into two components, one for reading R, and\nthe other for reading S. Assuming Ris read in blocks of size\nB, the block transfer size - the cost of reading Ris|R|\nB. For\nS, we assume that the learned index-based join will perform\na single I/O of ϵwords for each query. The cost of I/O is\nthen O(|R|(1+αϵ\nB)), where αis the hardware parameter\naccording to the affine model. This cost is also bounded by\nO(|S|\nϵ(1+αϵ\nB)) as we do not read any item in Smore than\nonce. Hence, the I/O cost for reading SisO(min(|R|,|S|\nϵ)(1+\nαϵ\nB)). The overall I/O cost can now be summarized as\n(3.1) O\u0012|R|\nB+min( |R|,|S|\nϵ)·(1+αϵ\nB)\u0013\nIn the case of |R|≤|S|\nϵ, this analysis shows that increasing\nthe size of the last-mile search window by building less accu-\nrate learned indexes does not significantly increase the overall\nI/O cost as α<< 1. When the table sizes are very similar\n(|R|≥|S|\nϵ), the I/O cost of the learned index is essentially\nthe same as linearly scanning both tables. In this case, the\naffine model predicts that increasing ϵactually decreases the\nI/O cost - we perform fewer but larger I/Os to completely\nscanS. Note that this analysis also holds for unsorted tables,\nthe only difference being that the indexes are unclustered.\nIndex size analysis. While there are no tight bounds\nproven for the size of learned indexes, empirically they\noccupy less space than B-trees. Analyzing the size of\nlearned indexes is not straightforward as it depends on the\ndistribution of the data, the specific design of the learned\nindex, and the training parameters used to build the learned\nindex. The most critical training parameter common to\nall learned index designs is the size of the last-mile search\nwindow. Ferragina et al. [13] find that when the gaps\nbetween elements follow a distribution with finite mean and\nvariance, the size of the learned index is O(n/ϵ2), where ϵis\nthe size of the search window and nis the number of keys.\nCopyright ©2025 by SIAM\nUnauthorized reproduction of this article is prohibited\n\nIn general, we can assume that learned indexes occupy\nless space than B-trees, and that the space decreases by\nmore than a linear factor with the accuracy of the learned\nindex. This is in contrast to B-trees whose size decreases\nlinearly with the node size. We will assume that the size\nof the learned index is O(n/f(ϵ)), where fis a function that\nis polynomially greater than a linear function.\n3.3 Takeaways We now compare the cost of using\nthe learned index compared to ithe B-tree in the indexed\nnested-loop join (INLJ). We then compare the learned\nindex-based join proposed above with the sort join. We\ncompare these in terms of I/O cost, CPU cost, and index size.\nLearned index vs B-trees in INLJ. Both indexes have the\nsame I/O cost for the same search window size ( ϵfor learned\nindexes, node size for B-trees). Increasing ϵor the node size in-\ncreases the I/O cost marginally ( α<< 1) for both the indexes.\nHowever, learned indexes occupy less memory and are faster\nto query compared to B-trees. The learned index also has\na better memory performance trade-off curve. Although the\nsize of both the learned index and B-tree can be reduced by in-\ncreasing the size of the search window, the size of the learned\nindex decreases by more than a linear factor of ϵcompared to\nthe B-tree size that only decreases linearly with the node size.\nLearned index-based INLJ vs sort join. The comparison\nbetween the sort join and learned index-based join is similar\nto comparing the sort join with INLJ. The I/O cost for both\nmethods depends on the size of the input tables. When the\ntables are of similar size, the I/O cost of both the join methods\nare equal. In this case, the differentiating factor of both the\njoin methods is the CPU cost. The cost of the sort join is one\nkey comparison per element from the larger table, resulting in\na total CPU cost of O(|S|). For the index-based approaches,\nthe CPU cost is one index lookup per element from the smaller\ntable, leading to a total CPU cost of O(|R|·log|S|). When\ntable sizes vary by a factor of more than ϵ(that is, |R|≤|S|\nϵ),\nthe learned index-based join, similar to the index nested-loop\njoin, costs less I/O and CPU compared to the sort join.\n4 Implementation\nWe now describe our implementation of join methods. We\nfirst describe how our tables are stored on disk, followed\nby the join and merge implementations. We then describe\nour join implementation for unsorted data. In both cases,\nthe tables are stored on disk.\n4.1 Tables While fully featured database systems will\nuse more complex data storage layouts and robust page man-\nagement strategies, we intentionally choose a very simplistic\nlayout to focus on the effects of learned indexes for joins.\nStorage Layout. A table is a list of key-value tuples. The\nkeys and values are of a fixed size of 8 byte keys and 8\nbyte values. The keys are distinct in a table. Tables are\nstored as a dense sorted array on disk in a single file. The\nfirst 16 bytes of the file are reserved for a header to store\nthe number and size of the key-value tuples, followed bythe key-value tuples themselves. We do not perform any\ncompression of the actual key-value tuples.\nReading and writing tables. Tables are logically divided\ninto blocks, each 4096 bytes in size. To read a key-value\ntuple from the table, the block corresponding to that key\nmust be loaded into memory. The blocks are stored in an\ninternal buffer that holds a contiguous set of blocks in main\nmemory. We configure the internal buffer size to hold enough\nblocks so that the search window of the learned index can\nbe fully held in memory. If a key that we are searching for\nis already in the internal buffer, we immediately return the\nkey. Otherwise, the set of contiguous blocks starting with the\nblock containing the key is loaded into memory. Similarly,\nwhen writing the output join table, we store keys in an\ninternal buffer and flush them to disk once the buffer is full.\n4.2 Indexes Indexes are built offline and serialized to\ndisk. When used for a join, the indexes are loaded from disk\nand stored fully in main memory.\nIndex API. Given a query for a key, all indexes (learned\nor B-trees) return a search window whose size is bounded,\nrepresenting the range in which the table contains the lower\nbound of the query key, which is the largest key in the\ntable that is greater than or equal to the query key. More\nformally, a table Tis an array of [ K1,K2,...K N], such that\nKi>Ki−1. The index is a function IT(x) that returns the\npair ( lo,hi) such that Klo≥x≤Khiand (hi−lo)≤ϵ, where\nϵis the error bound of the learned index.\nB-tree index. We use the interpretation defined in [20],\nwhere the inner nodes of the B-tree are interpreted as a\nlearned model and the leaf nodes are the search window\nranges that these learned models ultimately return. When\ninterpreted this way, the B-tree can be decomposed into\ntwo distinct entities - a learned index (inner nodes) and the\ndata (leaf nodes). For the sorted tables, we only create the\nlearned index part of the B-tree by evenly sampling keys\n(these are keys that would have been the first key of their\nleaf nodes) and insert them into a B-tree.\nLearned indexes. We consider various learned index\nimplementations (as described in Section 5.1) in our\nevaluation. We ensure that all learned indexes use the\nsame search window size (except RMI, where this is not\nconfigurable). We discuss the exact configuration used for\neach learned index in the experimental setup (Section 5.1).\nConstructing learned indexes using sampling.\nConstructing learned indexes on sorted arrays can be\nexpensive due to the computational cost of modeling the data\ndistribution. In contrast, building a B-tree is more efficient,\nas pivots can be directly selected from the sorted array.\nTo reduce the cost of learned index construction, we sample\nevery kthelement from the array and learn the distribution\nover this subset. We then build a learned index on the sam-\npled array with error ϵ′. A query returning the interval [ lo,hi]\nin the sampled array maps to the interval [ k·lo, k·hi] in the\nfull array. This effectively gives a learned index on the original\narray with error ϵ=k·ϵ′, constructed using only n/kelements.\nCopyright ©2025 by SIAM\nUnauthorized reproduction of this article is prohibited\n\nPass 1:  Sample, \nsort and train \nlearned index \nPass 2: Store leaf \nnodes of unclustered \nindex on disk Unsorted input table  (Disk) \nUnclustered index leaf nodes (Disk) \nMain Memory \nLearned \nindex Figure 4: An unclustered index using a learned index is built in two passes: the first pass samples the data to train\nthe learned index, and the second pass uses the index to assign each data item to its predicted position.\nLearned Index Lookup. Using learned indexes to answer\nlower bound queries is a two-step process. First, a lookup\nis performed on the index for the query key Qin table T.\nThe index will then return a range [ lo,hi], representing the\nrange containing the actual lower bound. The second step\nis to perform a last mile search in the table to find the exact\nlower bound in the table.\nLast-mile search. To perform the actual last-mile search,\nwe use the branchless implementation of the binary search,\nresulting in a fast and efficient last-mile search. The branch-\nless search algorithm takes advantage of the conditional\nmove (CMOV) instruction to generate assembly code with\nno branches [ 30]. We first check if the search range returned\nby the index is partially loaded in our internal buffer. If\nthe lower bound is not found or the internal buffer contains\nblocks that do not overlap with the search range, we load the\nrequired contiguous set of blocks from the disk into memory.\n4.3 Join on sorted tables We now describe implemen-\ntation details of the various join implementations. The join\noperation will output the common keys from the input tables\n(R,S) as its own table to disk. We compare the indexed\nnested loop join with B-tree and learned index-based join,\nagainst each other and also with other join methods such\nas hash join and sort join.\nIndexed nested loop join. Similar to the hash join, we\nstart first by streaming all keys of R. For each key of R,\nwe query the index of Swhich will return a search window.\nWe then do a last-mile search inside S[lo,hi], outputting\nthe key to the final table if it is present.\nSort join. IfR,Sare already sorted, we skip the sort phase\nand proceed directly to the merge phase. We use a standard\ntwo-pointer approach to find the common keys. We initialize\ntwo iterators, comparing the iterator heads and advancing\nthe iterator with the smaller key. If the keys are equal, the\nkey is added to the output join table. The join ends if any of\nthe iterators reaches the end of its table and cannot advance.\nHash join. We first iterate over all keys of Rand\ninsert them into an in-memory hash table HR. We use\nstd::unordered mapas our in-memory hash table. We\nthen iterate over all the elements from S, adding the key\nto the output if it exists in HR.4.4 Join on unsorted tables We now describe the\nimplementation details of the join methods for unsorted\ntables. These methods will output the result of the join in\nsorted order. Here we only implement two variants of the\nindexed nested loop join, B-tree based and learned index\nbased. We build unclustered indexes for both input tables\nas part of the join.\nB-tree index. We first build a complete B-tree for each\ninput table by sequentially inserting keys along with a\npointer to its location in the table. We then perform a range\nscan on the B-tree of the smaller table to stream the keys\nin sorted order, checking if each key exists in the large table\nby querying the B-tree of the larger table. The keys which\nare common to both input tables are added to join table.\nLearned index. To approximate the data distribution,\nwe build a learned index on a set of sampled keys from the\nunsorted table. The learned index on the sampled subset\nacts as a model for the distribution of data in the entire\ndataset, as uniform sampling captures the overall data\ndistribution and also avoids the need to fully sort the data.\nWe use the approximate rank of a key in the sampled\nlearned index to partition all keys into the desired number\nof partitions. Each key kis assigned to the partition\nˆRk\nNP, where ˆRkis the approximate rank of k,Nis the\nnumber of keys in the table, and Pis the desired number\nof partitions. We choose Pso that the average partition\nfits into a small constant number of pages. In practice, if\nthe keys were randomly sampled uniformly and the learned\nindex is reasonably accurate, the partition sizes have low\nvariance. As long as the ˆRkreturned by the learned index\nis monotonic, the partitions will be disjoint. The partitions\nare essentially leaf nodes of an unclustered index, and the\npartitioning process is illustrated in Figure 4.\nPartitioning the data according to the position returned\nby the CDF model is an idea that has been explored by\nKristo et al. [ 22] in the context of in-memory sorting, where\nthe data is sorted by recursively partitioning the data using\nthe learned index. Similarly, Abu-Libdeh et al. [1] build\ntables that are sorted into blocks according to the position\npredicted by the learned index. We adapt this idea for joins\non external memory by only partitioning the data once and\nnot sorting the data inside the partitions. Our partitioning\nmethod uses only two passes on the data, one to sample\nCopyright ©2025 by SIAM\nUnauthorized reproduction of this article is prohibited\n\nthe keys and build the CDF and the other to assign the\nkey to a bucket according to the built model.\nAfter partitioning keys into buckets, we perform the\nindexed nested-loop join. We store the learned index and\nthe partition map of both tables in memory. We sequentially\nload each bucket of Rinto memory and process the keys\nin sorted order. For each key, we query the index of Sand\nload the corresponding bucket into memory. The key is\nadded to the output if it is part of both tables.\n5 Evaluation\nDataset Size Key count Description\nFB 3.2 GB 200000000 Facebook user ids\nWiki 1.44 GB 90437011 Wikipedia edit timestamps\nOSM 12.8 GB 800000000 OpenStreetMap locations\nBooks 12.8 GB 800000000 Amazon book popularity data.\nudense 3.2 GB 200000000 sequential integers\nusparse 3.2 GB 200000000 Uniform sparse distribution\nnormal 3.2 GB 200000000 Normal distribution\nlognormal 3.2 GB 200000000 Lognormal distribution\nTable 1: Summary of datasets\nIn this section, we evaluate the usage of learned indexes\nfor external-memory joins against traditional join algorithms.\nFor learned index-based join, we employ the learned model to\nreplace the index in indexed nested loop joins. For all indexes,\nwe keep the leaf nodes on disk and the non-leaf nodes in\nmain memory. First, we evaluate various learned indexes\non construction time, in-memory space requirements and\njoin performance when used as part of learned index-based\njoin. We then pick the most appropriate learned index and\ncompare the learned index-based join against traditional\nindexed nested loop joins, sort join (SJ) and hash join (HJ).\nWe evaluate the performance of the learned index-based\njoin against traditional join algorithms across multiple\ndimensions: (1) storage device types (HDD/SSD), (2) data\nordering (sorted/unsorted), (3) concurrency, (4) constrained\nmemory settings and (5) trade off between error window\nsize and number of threads.\nAll benchmarking source code and datasets\nused in our evaluation are available at https:\n//github.com/saltsystemslab/learnedjoindiskexp .\n5.1 Experimental Setup In this section, we describe\nthe experimental setup we use for our evaluation.\nEnvironment. We run our experiments on an Intel(R)\nCore(TM) i7-8700 CPU @ 3.20GHz with 1 NUMA node with\n12 cores with a single 12MB L3 cache with 32GB of RAM. We\nrun our experiments on both SSD and HDD. The SSD model\nused in our experiments is a 512GB SanDisk SD9SB8W5,\nwhile the HDD is 2TB TOSHIBA DT01ACA200.\nDatasets. We use real-world and synthetic datasets used in\nthe unified benchmarking paper by Marcus et al. [ 25].Table 1\nsummarizes various datasets.\n•Real-world datasets: Books is a collection of 800 mil-lion keys representing book popularity data from amazon.\nWiki is a collection of 90 million wikipedia edit timestamps.\nOSM is a collection of 800 million OpenStreetMap loca-\ntions. FBis a collection of 200 million Facebook user ids.\n•Synthetic datasets: All synthetic datasets contain 200M\nitems generated from a universe of 64-bit unsigned integers.\nusparse represents a dataset of integers picked uniform\nrandomly, while udense represents a dense distribution\nof sequential keys. normal andlognormal represent\ndata distributions that follow normal and lognormal\ndistributions, respectively.\nJoin processing. Each join operation is invoked as a\nnew process. Our join evaluation setup for static tables\nis similar to previous join studies [4, 6, 7, 10, 31, 29, 21].\nThe indexes are constructed offline, stored on disk, and\nloaded into memory before starting the join operation. For\nmultithreaded experiments, the input tables are partitioned\nevenly across threads and work is distributed evenly. All\noperations are performed with cold cache by dropping the\noperating system page caches, and the input files are read\nusing ODIRECT to ensure that all data is always retrieved\nfrom disk. We do not use ODIRECT for writes as the\noutput cost for all join methods is the same and increases\nthe duration of the experiment. To constrain the memory\nduring join process, we employ CGroupsV2 utility.\n5.2 Evaluating learned indexes for joins on disk\nIn this section, we evaluate the performance of PGM,\nRadixSpline, RMI, and ALEX against B-trees for index\nconstruction, query latency, and in-memory index space.\nWe use the insights from this evaluation to identify the\nappropriate learned index to employ for external-memory\njoins on sorted and unsorted data.\nOur evaluation of learned indexes extends the SOSD\nbenchmark [ 25] for disk-based evaluation and performance\non the join operation. Furthermore, our evaluation extends\nthe disk-based benchmarking study [23] by evaluating the\nlearned indexes on external memory joins.\nIndexes. We use the B-tree index for implementing the index\nnested-loop join. We use the STX-BTree v0.9 [ 9] library as\nour B-tree implementation. For each of the learned indexes\n(PGM [ 32], RadixSpline [ 18] and the recursive model index\n(RMI) [ 20]), we use the reference implementations provided by\nthe authors. For ALEX [ 12], we use the disk-based implemen-\ntation used in the study by Lan et al. [ 23], which evaluates\nupdatable learned indexes in external memory. The specific\nconfiguration used for building each index is described below:\n•B-tree : B-tree node size is fixed to 4K bytes. To make\nthe comparison fair with static learned indexes, the B-tree\nis built by bulk loading the keys. Furthermore, we build\nthe B-tree on every 256thkey in the dataset. The leaf\nnodes map their keys to the block they come from in\nthe dataset, where a block is a contiguous set of 256 keys.\nThus, the leaf nodes of a B-tree return a search window of\nsize 256, similar to how a learned index returns a search\nwindow for the last-mile search.\nCopyright ©2025 by SIAM\nUnauthorized reproduction of this article is prohibited\n\nB-Tree PGM PGM(Sampled) RadixSpline RMI ALEX\nFB Wiki OSM Books10−1101103Real\nDatasets\nSize(MB)Index size\nFB Wiki OSM Books10−3100103Time(sec)Index construction time\nFB Wiki OSM Books102103Time(sec)Self join time\nudense usparselognormal normal10−5101107Synthetic\nDatasets\nSize(MB)\nudense usparselognormal normal10−2101104Time(sec)\nudense usparselognormal normal102103Time(sec)Figure 5: Index size, build time and performance of learned indexes and B-tree. Index Size is the space used by the\nindex in main memory. ALEX did not finish building the index using 32GB of RAM for OSM/Books datasets.\n•Piecewise geometric model (PGM ): The PGM index\nis built with an error window of 256 with a single level.\n•Piecewise geometric model (PGM) sampled : A\nversion of the PGM index that is built on every 128thkey\nwith a maximum error of 2. The search window returned\nby this index is then scaled up to get the search window\nin the dataset.\n•RadixSpline (RS) : For each dataset, we choose the\nPareto-optimal index configuration of the RadixSpline\nas evaluated by the SOSD benchmark [ 25]. The Pareto-\noptimal index is an index configuration for which no other\nindex with lower memory usage has better performance.\nRadix bits range from 16 to 28 across the various datasets.\nWe also set the RadixSpline maximum error to 256.\n•Recursive model index : For each dataset, we use the\nRMI model configuration from the SOSD benchmark\n[25] with hyperparameters tuned using CDFShop [26].\nSimilar to the RadixSpline, we choose the Pareto-optimal\nindex for each dataset.\n•ALEX : We use the disk-based implementation provided\nby Lan et al. [23] to evaluate the performance of\nupdatable learned indexes on disk. The data nodes are\nstored as a contiguous file on disk, while the inner nodes\nare stored in main memory.\nSetup. The construction time is measured as the time to\nbuild the index over the set of keys. We load all the keys\ninto memory before building the index to avoid counting the\ndisk I/O cost during index construction. All indexes, except\nRMI, can be built in a single pass by streaming keys from\nthe disk. The query performance is evaluated by measuring\nthe time taken to perform the index nested-loop self-join\nusing the index. All indexes are loaded into main memory\nwhile the data is kept on disk and loaded one page at a time.The index evaluation experiments are constructed on flash\nstorage with the operating system page caches flushed before\nthe start of each experiment. Figure 5 plots the construction\ntime, index size, and the time to complete the self join on\nall datasets using each index.\nIndex size. For real-world datasets, PGM indexes (both\nsampled and full) have the lowest memory footprint among\nall learned indexes, being 4 ×smaller than the B-tree. The\nRadixSpline and RMI are an order of magnitude (30 −80×)\nlarger than B-trees. ALEX was unable to construct the OSM\nandBooks datasets as it ran out of memory. For synthetic\ndatasets, PGM indexes were an order of magnitude smaller\nin size compared to B-trees. The RMI and RadixSpline are\nnot able to model the usparse andlognormal distributions\nwell and needed several orders of magnitude of space to do so.\nIndex construction time. All learned indexes take at least\n4 orders of magnitude longer to build than B-trees. Con-\nstructing the PGM index with sampling reduces the duration\nby roughly two orders of magnitude. The higher construction\ntimes of learned indexes can be reduced by sampling without\nsacrificing query performance. As the time to construct the\nPGM and RadixSpline indexes grows linearly with the size\nof the dataset, evenly sampling keys to construct the index\nreduces the number of keys used to train the index while still\neffectively capturing the distribution of the data. The PGM\nindex built on the sampled data has performance identical to\nthat of the index built on the entire dataset. Additionally, the\nPGM index is also simpler to construct, requiring only a single\nparameter (the maximum error), compared to the RMI and\nRadixSpline, which require tuning multiple parameters to find\nthe pareto-optimal configuration for space and performance.\nDespite reducing the construction time using sampling,\nlearned indexes are slower to construct than B-trees. The\nCopyright ©2025 by SIAM\nUnauthorized reproduction of this article is prohibited\n\nB-tree construction is extremely fast when built using bulk\nloading because the B-tree only needs to perform fixed\nmemory allocations and data copying. On the other hand,\nlearned indexes need to perform more complex processing to\nmodel the distribution leading to increased construction time.\nJoin performance. Across all datasets, we find that the\nPGM index performs the most consistently, being 1 .1−1.7×\nfaster than the B-tree. As disk I/O is the bottleneck, all\nlearned indexes (except ALEX) performed very similarly.\nFor ALEX, our results are consistent with the disk-resident\nlearned index study by Lan et al. [ 23], which showed that\nthe read performance of ALEX on disk is not competitive\nin read-heavy workloads.\nTakeaways. Overall, the PGM index with sampling offers\nthe best tradeoff in terms of query latency, construction\ntime, and space usage. Using sampling, the join can be\nsped up by 1 .1−1.7×compared to B-trees and also uses\n4×less space. Although the PGM index takes 10 ×longer\nto build compared to the B-tree, this is often an acceptable\ntradeoff in large-scale analytics systems where the indexes\nare built offline and are used several times to perform fast\njoins. Therefore, we employ sampled PGM index in\nour implementation of the learned index-based join.\n5.3 Join methods on sorted data In this section, we\ncompare the single-threaded performance of the hash join\n(HJ), indexed nested-loop join (INLJ), sort join (SJ), and\nlearned index-based join on sorted tables stored on disk\n(both HDD and SSD) across varying table size ratios.\nSetup. We use the sampled PGM index as the index for\nthe learned index-based join based on the analysis presented\nin Section 5.2. The indexed nested-loop join uses a B-tree\nas the index. The sorting phase of sort join is skipped as\nthe data is already sorted on disk. The hash join uses STL\nstd::unordered mapas the hash table of the smaller table.\nAll indexes and hash tables are loaded in memory before\nstarting the join, while the data is streamed from disk in\npages using file I/O. The join time experiments on SSD and\nHDD is plotted in Figure 6 and Figure 7, respectively.\nJoin selectivity. We employ different table ratios to evaluate\njoin algorithms for different selectivity values. A table size\nratio of 1 is a self-join. For other table size ratios (10, 100, and\n1000), we sample a fraction of the keys uniformly randomly\nfrom the table to create the smaller table for the join.\nIndex for indexed-nested loop join. We employ sampled\nPGM index in our implementation of the learned index-\nbased join. This is based on the conclusions drawn from an\nextensive study of learned indexes for construction time, size,\nand query time detailed in Section 5.2. A sampled version of\nthe PGM index is built on every 128thkey with a max error\nof 2. The search window returned by this index is scaled up\nto get the actual search window in the dataset. We use the\nB-tree index to implement the index-nested loop join. We use\nthe STX-BTree v0.9 [ 9] library as our B-tree implementation.\nJoin method evaluation on SSDs. The learnedindex-based join is faster by 1 .2−1.6×compared to the\nindexed nested-loop join with B-tree when the table size ratio\nlies between 1 and 100. The learned index-based join is also\nfaster compared to the sort-join(1 .1−1.4×) when the table\nsize ratio is between 10 and 100. When the table size ratio is\nbetween 1 and 100, the I/O cost is identical for all methods\nas items from the table are fetched in blocks of size 4KB\nthat contain 256 items. Thus, every block is expected to\ncontain a join key. At higher table size ratios (such as 1000),\nboth indexed-based joins, learned and B-tree based, perform\nrandom I/Os on the inner table. In our tests, the learned\nindex-based join performed slightly worse than the B-tree\nbased index nested-loop join by about (1 .1−1.6×). However,\nthe performance is still very similar in absolute terms due to\nthe small output size. In cases where both tables have\nto be scanned completely, the learned index-based\njoin offers better performance compared to the\nB-tree by 1.2−1.6×, and the sort join by 1.1−1.4×\n(except when both input tables are of similar size) .\nJoin method evaluation with HDDs. On hard disks,\nthe performance of the B-tree and learned index-based join\nwas similar across all table-size ratios and datasets. Both the\nindex-based methods were also faster than the sort-join except\nfor when the table sizes were equal. Indexed-based meth-\nods have similar performance on HDDs and are faster\nthan sort-join except for when table sizes are similar.\nTakeaways. Learned indexes offer performance mostly\nsimilar to the traditional B-tree-based index nested-loop\njoin in external memory settings. The PGM index is much\nsmaller in size compared to the B-tree. However, that does\nnot result in improved performance as the join operation\nis bounded by the disk I/O. Using learned indexes for join\ndoes not help in reducing the total I/O. This is especially\ntrue when there is enough working memory to store the\nB-tree. This is unlike in main-memory joins where learned\nindexes can help speed up join performance [29].\n5.4 Join methods on unsorted data In this section,\nwe compare the single-threaded performance of various\njoins on unsorted tables to produce a sorted join output in\nexternal memory.\nSetup. We generate input tables for a dataset by shuffling\nkeys from the FB dataset and storing them on disk. We\ncompute the join using an indexed nested-loop join using\nunclustered indexes (B-tree, PGM) on the keys. The index\nstores keys and a pointer to its table entry. We use the\npointer to fetch the associated value of a join key from\nthe table on disk. We run the test under different memory\nconstraints using CGroupsV2 to limit the amount of memory\nthat a process is allowed to use and plot the time to complete\nthe join for the B-tree and PGM index in Section 5.3.\nSimilar to joins on sorted data, we test for different table\nsize ratios of the input tables. We summarize the index\nconstruction and implementation of the join for each index.\n•B-tree : We build the B-tree by streaming keys from disk.\nThe leaf nodes of the B-tree are stored on disk, while the\nCopyright ©2025 by SIAM\nUnauthorized reproduction of this article is prohibited\n\nPGM (sampled) join Sort join Indexed nested loop join Hash join\n050100150200\n100\n101\n102\n103\nTable ratio (S/R)Runtime (secs)FB\n0204060\n100\n101\n102\n103\nTable ratio (S/R)Wiki\n0200400600\n100\n101\n102\n103\nTable ratio (S/R)OSM\n0200400600\n100\n101\n102\n103\nTable ratio (S/R)Books\n050100150200\n100\n101\n102\n103\nTable ratio (S/R)Runtime (secs)udense\n050100150200\n100\n101\n102\n103\nTable ratio (S/R)usparse\n050100150200\n100\n101\n102\n103\nTable ratio (S/R)normal\n050100150200\n100\n101\n102\n103\nTable ratio (S/R)lognormalFigure 6: Performance of various join methods for sorted tables on flash-based storage devices (SSD).\n0100200\n100\n101\n102\n103\nTable ratio (S/R)Runtime (secs)FB\n050100\n100\n101\n102\n103\nTable ratio (S/R)Wiki\n05001,000\n100\n101\n102\n103\nTable ratio (S/R)OSM\n02004006008001,000\n100\n101\n102\n103\nTable ratio (S/R)Books\n0100200\n100\n101\n102\n103\nTable ratio (S/R)Runtime (secs)udense\n0100200\n100\n101\n102\n103\nTable ratio (S/R)usparse\n0100200\n100\n101\n102\n103\nTable ratio (S/R)normal\n0100200\n100\n101\n102\n103\nTable ratio (S/R)lognormal\nFigure 7: Performance of various join methods for sorted tables on hard disks (HDD).\nintermediate nodes are held in memory. Nodes are 4KB\nin size. To compute the join, we scan the keys from the\nsmaller table and for each key perform a lookup in the\nB-tree of the larger table.\n•PGM Index : As the tables are not sorted, we partition\nthe data into disjoint ranges with the help of a PGM index\nbuilt on a sampled subset (1%) of the dataset. The rank\nof a key according to the sampled PGM index is used\nto determine its partition. As the rank returned is only\nan approximation, it is not necessary that partitions areequally sized. The more accurate the learned index, the\nless the variance in the size of the partitions. We set the\nexpected partition size to be that of a single page (4KB).\nPartitions are flushed to disk 8 keys at a time to improve\nwrite efficiency. The index and partition map for each table\nare stored in main memory, and during the join only a single\npartition per table is kept in memory. To compute the\njoin, we sequentially process the partitions of the smaller\ntable. For each key in the partition, we use the PGM\nindex of the larger table to determine the corresponding\nCopyright ©2025 by SIAM\nUnauthorized reproduction of this article is prohibited\n\nPGM(sampled) Indexed nested loop join\n102103104\n100\n101\n102\n103\nTable ratio (S/R)Runtime(sec)32 GB\n102103104\n100\n101\n102\n103\nTable ratio (S/R)2 GB(a) Join duration on unsorted data after index creation under\ndifferent memory constaints using the FB dataset.\nIndex creation (sec)\nMemory Limit B-tree PGM\n2GB 10777.06 2993.49\n32GB 1406.33 411.28\n(b) Time to create the index on unsorted data under memory\nconstraints using the FB dataset.\nFigure 8: Join performance on unsorted data\npartition in the larger table. Our approach for learned\njoins on unsorted data is based on learned sorting [22].\nResults summary. The performance of different join\napproaches is largely similar. Joins on unsorted data incur\nmore I/O operations than those on sorted tables. Since\nI/O cost dominates the join performance, faster queries\nof the learned index-based join do not yield significant\nimprovements. The PGM index also performs similarly to\nthe B-tree when memory is constrained using CGroupsV2 .\nAlthough the PGM index is significantly smaller at 3MB\ncompared to 12MB for B-tree, this 75% reduction in index\nsize is insufficient to reduce the I/O performed to swap pages\nto disk due to constrained memory. However, partitioning\nthe data to partially sort the data using the PGM index\nis faster (3 −3.5×) compared to constructing a B-tree with\nrandom inserts. This is due to the higher write amplification\nof B-tree to keep items sorted under random inserts.\nTakeaways. Although partitioning and constructing an\nunclustered index for a unsorted table using the CDF model\nis upto 3 .5×faster compared to building the B-tree index,\nthe join itself performs similar. The benefits of smaller\nindexes and faster queries are not apparent, as the memory\nsavings of the learned index-based join are only over the inner\nnodes of B-tree, which is tiny compared to the space required\nto store the dataset. The benefits of smaller indexes and\nfaster queries on the join itself are not apparent even\nwhen operating under constrained memory settings .\n5.5 Multithreading In this section, we evaluate the\neffect of scaling up external-memory join methods using\nmultiple threads. We will further study the performanceIndex size (MB)\nDataset ϵ=256 ϵ=2048 ϵ=4096\nFB 3.1623 0.7314 0.0910\nWiki 0.1121 0.0507 0.0064\nOSM 8.6735 2.4492 0.6454\nBooks 3.140 0.092 0.024\nudense 48 KB 48 KB 48 KB\nusparse 0.0505 0.0034 0.0002\nnormal 0.0109 0.0054 0.0027\nlognormal 0.0152 0.0076 0.0038\nTable 2: Index size of PGM index as search window size varies.\ntradeoff between search window size and number of threads.\nSetup. We partition the smaller table into equal size\npartitions based on the number of threads and assign a single\npartition to each thread. We build an index on the larger table\nand query it for keys from the smaller table to find a match\nfor the join. Each thread writes its output to a separate file on\ndisk. The threads are synchronized to block until all threads\nfinish writing their join output. Once all threads are finished,\nwe merge the output file for the final join output. To do this,\neach thread computes the offset of where its output lies in the\nmerged output and writes it to the final join output. We test\nfor thread sizes of 1, 2, 4, 6, and 8. We compare the learned\nindex-based join with the indexed nested-loop join with B-\ntree, hash join, and sort join and plot the results in Figure 9.\nResults summary. Adding more threads makes the join\noperation more I/O bound. Both the learned index-based join\nand B-tree indexed joins scale linearly with increasing threads\nbefore becoming I/O bound at some point. For example,\nwhen the table size ratio is 100, the learned index-based join\nbecomes completely I/O bound with 4 threads. At this point,\nadding more threads does not improve the overall process as\nthe join is I/O bound. The performance of sort join does not\nscale with more threads as it is already I/O bound. The hash\njoin is mostly CPU bound and almost linearly scales with\nincreasing threads all the way up to 8 threads. The runtime\ndoes not include the time to build the hash table. Thus, the\ntime for hash join measurement avoids the time to perform\nI/O on the smaller table. This makes the hash join less I/O\nbound compared to the other joins. Note that the hash join\nuses much more memory compared to indexed and sort joins\nas it stores a hash table of size O(|R|) in memory. It is only\nincluded in the evaluation only as a baseline for comparison.\nTakeaway. The conclusions made in the previous section\nregarding which join method to use at different table\nratios hold true even for multithreaded join processing.\nLearned indexes scale up with more threads similarly\ncompared to other join methods.\n5.6 Error window size analysis In this section, we\nstudy the effect of the size of the error window on the join\nperformance with an increasing number of threads. We also\nstudy how the build time and index size varies as the error\nCopyright ©2025 by SIAM\nUnauthorized reproduction of this article is prohibited\n\nPGM (sampled) join Sort join Indexed nested loop join Hash join\n12 4 6 8050100150\nNum threadsRuntime (secs)|S|/|R|=1\n12 4 6 8020406080\nNum threads|S|/|R|=10\n12 4 6 8020406080\nNum threads|S|/|R|=100\n12 4 6 802040\nNum threads|S|/|R|=1000Figure 9: Performance of join methods as the number of threads increase (FB dataset)\n1T=1 2T=2 4T=4 6T=6 8T=8\n2561024 4096050100150\n1 11222 4 446 6 6 8 8 8\nSearch window sizeRuntime (secs)Table ratio (S/R=1)\n2561024 4096010203040\n1\n1122\n24\n446\n6688 8\nSearch window sizeTable ratio (S/R=10)\n2561024 409601020301\n1 1 2\n224\n4 46\n668\n8 8\nSearch window sizeTable ratio (S/R=100)\n2561024 40960204011\n1222 4 4 4 6 6 6 8 8 8\nSearch window sizeTable ratio (S/R=1000)\nFigure 10: Performance of learned index-based join with various search window sizes (FB dataset)\nwindow changes.\nSetup. We use the sampled PGM index with error window\nsizes of 256, 1024 and 4096. The results of this experiment\nare plotted in Figure 10, while Table 2 shows the index size of\nthe sampled PGM. We increase the page fetch size of a single\nI/O call to match the size of the error window. Note that this\ndoes not change the total number of bytes fetched from disk,\nonly the number of I/O calls performed to fetch those bytes.\nResult Summary. For the sampled PGM index, the\nindex size is reduced by a factor of 30 −130×when the\nerror window size is increased from 256 to 2048 and 4096\nrespectively. The index build time is independent of the\nindex error window size and depends only on the sampling\nrate (which is fixed at 128 in this case).\nFigure 10 shows that the performance of the PGM index\nremains consistent with increasing the size of the search\nwindow. We perform I/Os of larger block sizes to ensure that\nwe perform no more than a single I/O call per query. For\nlower table size ratios, performance remains consistent with\nincreasing the search window size from 256 to 4096. At these\ntable size ratios, the join disk access pattern is sequential.\nThus, requesting larger I/O block sizes across a varying num-\nber of threads does not have a significant effect on overall\nperformance. As the table size ratios increase, the join access\npattern is no longer sequential. When run with a low number\nof threads, a larger search window and I/O block fetch sizes\nlead to higher disk bandwidth utilization, resulting in disk\nsaturation and consequently better performance. With a highnumber of threads, the disk utilization is already high and per-\nforming larger I/O block fetches has no effect on performance.\n6 Conclusion\nThis study presents an extensive evaluation of learned indexes\nfor external-memory joins, analyzing their impact across\nvaroius database configurations. Unlike the main-memory\nsetting, where learned indexes provide clear advantages, our\nfindings indicate that their benefits in external-memory joins\nare less pronounced due to I/O dominance.\nWhile learned indexes offer smaller index sizes and faster\nlookups, they do not reduce the total I/O costs, resulting\nin similar performance to B-trees-based joins in most cases.\nHowever, by tuning parameters such as search window size\nand error bounds, learned indexes can achieve comparable or\nslightly better performance in specific workloads, particularly\non SSDs. The significant index construction overhead (up to\n1000×slower than B-trees) further limits their practicality\nfor dynamic workloads, but remains acceptable in offline\nanalytics scenarios.\nOur results suggest that practitioners must carefully\nassess I/O constraints when integrating learned indexes into\ndatabase engines.\nAcknowledgments\nThis research is funded in part by NSF grant OAC 2339521\nand 2517201.\nCopyright ©2025 by SIAM\nUnauthorized reproduction of this article is prohibited\n\nReferences\n[1]Hussam Abu-Libdeh, Deniz Altinb¨ uken, Alex Beutel, Ed H.\nChi, Lyric Doshi, Tim Kraska, Xiaozhou Li, Andy Ly, and\nChristopher Olston. Learned indexes for a google-scale\ndisk-based database. CoRR , abs/2012.12501, 2020.\n[2]Alok Aggarwal and Jeffrey Scott Vitter. The input/output\ncomplexity of sorting and related problems. Commun.\nACM , 31(9):1116–1127, 1988.\n[3]Abdullah Al-Mamun, Hao Wu, Qiyang He, Jianguo Wang,\nand Walid G. Aref. A survey of learned indexes for the\nmulti-dimensional space. CoRR , abs/2403.06456, 2024.\n[4] Martina-Cezara Albutiu, Alfons Kemper, and Thomas\nNeumann. Massively parallel sort-merge joins in main\nmemory multi-core database systems. Proc. VLDB Endow. ,\n5(10):1064–1075, 2012.\n[5] Matthew Andrews, Michael A. Bender, and Lisa Zhang.\nNew algorithms for the disk scheduling problem. In 37th\nAnnual Symposium on Foundations of Computer Science,\nFOCS ’96, Burlington, Vermont, USA, 14-16 October, 1996 ,\npages 550–559. IEEE Computer Society, 1996.\n[6] Cagri Balkesen, Gustavo Alonso, Jens Teubner, and\nM. Tamer ¨Ozsu. Multi-core, main-memory joins: Sort vs.\nhash revisited. Proc. VLDB Endow. , 7(1):85–96, 2013.\n[7] Maximilian Bandle, Jana Giceva, and Thomas Neumann.\nTo partition, or not to partition, that is the join question in\na real system. In Guoliang Li, Zhanhuai Li, Stratos Idreos,\nand Divesh Srivastava, editors, SIGMOD ’21: International\nConference on Management of Data, Virtual Event, China,\nJune 20-25, 2021 , pages 168–180. ACM, 2021.\n[8] Michael A. Bender, Alex Conway, Mart´ ın Farach-Colton,\nWilliam Jannen, Yizheng Jiao, Rob Johnson, Eric Knorr,\nSara Mcallister, Nirjhar Mukherjee, Prashant Pandey, Don-\nald E. Porter, Jun Yuan, and Yang Zhan. External-memory\ndictionaries in the affine and pdam models. ACM Trans.\nParallel Comput. , 8(3), sep 2021.\n[9] Timo Bingmann. STX B+ Tree C++ Template Classes\nv0.9, November 2023.\n[10] Spyros Blanas, Yinan Li, and Jignesh M. Patel. Design\nand evaluation of main memory hash join algorithms\nfor multi-core cpus. In Timos K. Sellis, Ren´ ee J. Miller,\nAnastasios Kementsietsidis, and Yannis Velegrakis, editors,\nProceedings of the ACM SIGMOD International Conference\non Management of Data, SIGMOD 2011, Athens, Greece,\nJune 12-16, 2011 , pages 37–48. ACM, 2011.\n[11] Yifan Dai, Yien Xu, Aishwarya Ganesan, Ramnatthan\nAlagappan, Brian Kroth, Andrea C. Arpaci-Dusseau, and\nRemzi H. Arpaci-Dusseau. From wisckey to bourbon:\nA learned index for log-structured merge trees. In 14th\nUSENIX Symposium on Operating Systems Design and\nImplementation, OSDI 2020, Virtual Event, November 4-6,\n2020, pages 155–171. USENIX Association, 2020.\n[12]Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaey-\noung Do, Yinan Li, Hantian Zhang, Badrish Chandramouli,\nJohannes Gehrke, Donald Kossmann, David Lomet, and\nTim Kraska. ALEX: An Updatable Adaptive Learned Index.\nInProceedings of the 2020 ACM SIGMOD International\nConference on Management of Data , SIGMOD ’20, pages\n969–984, New York, NY, USA, May 2020. Association for\nComputing Machinery.[13] Paolo Ferragina, Fabrizio Lillo, and Giorgio Vinciguerra.\nOn the performance of learned data structures. Theoretical\nComputer Science , 871:107–120, June 2021.\n[14]Paolo Ferragina and Giorgio Vinciguerra. The PGM-index:\na fully-dynamic compressed learned index with provable\nworst-case bounds. Proceedings of the VLDB Endowment ,\n13(8):1162–1175, April 2020.\n[15] Alex Galakatos, Michael Markovitch, Carsten Binnig,\nRodrigo Fonseca, and Tim Kraska. FITing-Tree: A\nData-aware Index Structure. In Proceedings of the 2019\nInternational Conference on Management of Data , SIGMOD\n’19, pages 1189–1206, New York, NY, USA, June 2019.\nAssociation for Computing Machinery.\n[16] Darryl Ho, Jialin Ding, Sanchit Misra, Nesime Tatbul,\nVikram Nathan, Vasimuddin Md, and Tim Kraska.\nLISA: towards learned DNA sequence search. CoRR ,\nabs/1910.04728, 2019.\n[17]William Jannen, Jun Yuan, Yang Zhan, Amogh Akshintala,\nJohn Esmet, Yizheng Jiao, Ankur Mittal, Prashant Pandey,\nPhaneendra Reddy, Leif Walsh, Michael A. Bender, Martin\nFarach-Colton, Rob Johnson, Bradley C. Kuszmaul, and\nDonald E. Porter. Betrfs: A right-optimized write-optimized\nfile system. In Jiri Schindler and Erez Zadok, editors, Pro-\nceedings of the 13th USENIX Conference on File and Storage\nTechnologies, FAST 2015, Santa Clara, CA, USA, February\n16-19, 2015 , pages 301–315. USENIX Association, 2015.\n[18]Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail\nStoian, Alfons Kemper, Tim Kraska, and Thomas Neumann.\nRadixSpline: a single-pass learned index. In Proceedings\nof the Third International Workshop on Exploiting Artificial\nIntelligence Techniques for Data Management , aiDM ’20,\npages 1–5, New York, NY, USA, June 2020. Association\nfor Computing Machinery.\n[19]Melanie Kirsche, Arun Das, and Michael C. Schatz. Sapling:\naccelerating suffix array queries with learned data models.\nBioinform. , 37(6):744–749, 2021.\n[20] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and\nNeoklis Polyzotis. The Case for Learned Index Structures.\nInProceedings of the 2018 International Conference on Man-\nagement of Data , SIGMOD ’18, pages 489–504, New York,\nNY, USA, May 2018. Association for Computing Machinery.\n[21] Simeon Krastnikov, Florian Kerschbaum, and Douglas\nStebila. Efficient oblivious database joins. Proc. VLDB\nEndow. , 13(11):2132–2145, 2020.\n[22] Ani Kristo, Kapil Vaidya, Ugur C ¸etintemel, Sanchit\nMisra, and Tim Kraska. The case for a learned sorting\nalgorithm. In David Maier, Rachel Pottinger, AnHai Doan,\nWang-Chiew Tan, Abdussalam Alawini, and Hung Q. Ngo,\neditors, Proceedings of the 2020 International Conference\non Management of Data, SIGMOD Conference 2020, online\nconference [Portland, OR, USA], June 14-19, 2020 , pages\n1001–1016. ACM, 2020.\n[23] Hai Lan, Zhifeng Bao, J. Shane Culpepper, and Re-\nnata Borovica-Gajic. Updatable Learned Indexes Meet\nDisk-Resident DBMS - From Evaluations to Design\nChoices. Proceedings of the ACM on Management of Data ,\n1(2):139:1–139:22, June 2023.\n[24]Pengfei Li, Hua Lu, Qian Zheng, Long Yang, and Gang Pan.\nLISA: A learned index structure for spatial data. In David\nMaier, Rachel Pottinger, AnHai Doan, Wang-Chiew Tan, Ab-\nCopyright ©2025 by SIAM\nUnauthorized reproduction of this article is prohibited\n\ndussalam Alawini, and Hung Q. Ngo, editors, Proceedings of\nthe 2020 International Conference on Management of Data,\nSIGMOD Conference 2020, online conference [Portland, OR,\nUSA], June 14-19, 2020 , pages 2119–2133. ACM, 2020.\n[25]Ryan Marcus, Andreas Kipf, Alexander van Renen, Mihail\nStoian, Sanchit Misra, Alfons Kemper, Thomas Neumann,\nand Tim Kraska. Benchmarking learned indexes. Proc.\nVLDB Endow. , 14(1):1–13, 2020.\n[26] Ryan Marcus, Emily Zhang, and Tim Kraska. Cdfshop:\nExploring and optimizing learned index structures. In David\nMaier, Rachel Pottinger, AnHai Doan, Wang-Chiew Tan, Ab-\ndussalam Alawini, and Hung Q. Ngo, editors, Proceedings of\nthe 2020 International Conference on Management of Data,\nSIGMOD Conference 2020, online conference [Portland, OR,\nUSA], June 14-19, 2020 , pages 2789–2792. ACM, 2020.\n[27] Prashant Pandey, Shikha Singh, Michael A. Bender,\nJonathan W. Berry, Martin Farach-Colton, Rob Johnson,\nThomas M. Kroeger, and Cynthia A. Phillips. Timely report-\ning of heavy hitters using external memory. In David Maier,\nRachel Pottinger, AnHai Doan, Wang-Chiew Tan, Abdus-\nsalam Alawini, and Hung Q. Ngo, editors, Proceedings of\nthe 2020 International Conference on Management of Data,\nSIGMOD Conference 2020, online conference [Portland, OR,\nUSA], June 14-19, 2020 , pages 1431–1446. ACM, 2020.\n[28]Chris Ruemmler and John Wilkes. An introduction to disk\ndrive modeling. Computer , 27(3):17–28, 1994.\n[29] Ibrahim Sabek and Tim Kraska. The case for learned\nin-memory joins. Proc. VLDB Endow. , 16(7):1749–1762,\nmar 2023.[30]Malte Skarupke. Beautiful Branchless Binary Search, 2024.\n[31] Lasse Thostrup, Gloria Doci, Nils Boeschen, Manisha\nLuthra, and Carsten Binnig. Distributed GPU joins on\nfast rdma-capable networks. Proc. ACM Manag. Data ,\n1(1):29:1–29:26, 2023.\n[32]Giorgio Vinciguerra. gvinciguerra/PGM-index, November\n2023. original-date: 2019-10-18T11:48:12Z.\n[33] Jiacheng Wu, Yong Zhang, Shimin Chen, Jin Wang,\nYu Chen, and Chunxiao Xing. Updatable learned index with\nprecise positions. Proceedings of the VLDB Endowment ,\n14(8):1276–1288, April 2021.\n[34]Jun Yuan, Yang Zhan, William Jannen, Prashant Pandey,\nAmogh Akshintala, Kanchan Chandnani, Pooja Deo, Zar-\ndosht Kasheff, Leif Walsh, Michael A. Bender, Martin Farach-\nColton, Rob Johnson, Bradley C. Kuszmaul, and Donald E.\nPorter. Writes wrought right, and other adventures in file sys-\ntem optimization. ACM Trans. Storage , 13(1):3:1–3:26, 2017.\n[35] Jiaoyi Zhang, Kai Su, and Huanchen Zhang. Making\nin-memory learned indexes efficient on disk. Proc. ACM\nManag. Data , 2(3):151, 2024.\n[36]Songnian Zhang, Suprio Ray, Rongxing Lu, and Yandong\nZheng. SPRIG: A learned spatial index for range and knn\nqueries. In Erik Hoel, Dev Oliver, Raymond Chi-Wing\nWong, and Ahmed Eldawy, editors, Proceedings of the\n17th International Symposium on Spatial and Temporal\nDatabases, SSTD 2021, Virtual Event, USA, August 23-25,\n2021, pages 96–105. ACM, 2021.\nCopyright ©2025 by SIAM\nUnauthorized reproduction of this article is prohibited",
  "textLength": 70905
}