{
  "paperId": "0b1d25ceaf9cfafffe98d67ed0a58dadc20ed656",
  "title": "A Computational Approach to Packet Classification",
  "pdfPath": "0b1d25ceaf9cfafffe98d67ed0a58dadc20ed656.pdf",
  "text": "arXiv:2002.07584v2  [cs.DC]  13 Jul 2020A ComputationalApproachtoPacket Classification\nAlon Rashelbach\nTechnion\nalonrs@campus.technion.ac.ilOri Rottenstreich\nTechnion\nor@technion.ac.ilMark Silberstein\nTechnion\nmark@ee.technion.ac.il\nABSTRACT\nMulti-ﬁeldpacket classiﬁcationis a crucial component in m odern\nsoftware-deﬁned data center networks. To achieve high thro ugh-\nputandlowlatency,state-of-the-artalgorithmsstriveto ﬁttherule\nlookup data structures into on-die caches; however, they do not\nscalewell with thenumberof rules.\nWepresentanovel approach, NuevoMatch ,whichimprovesthe\nmemory scalingof existing methods.Anew data structure, Range\nQueryRecursive ModelIndex (RQ-RMI),is thekeycomponentthat\nenablesNuevoMatchtoreplacemostoftheaccessestomainme m-\nory with model inference computations. We describe an eﬃcie nt\ntrainingalgorithmthatguaranteesthecorrectnessoftheR Q-RMI-\nbasedclassiﬁcation.TheuseofRQ-RMIallowstherulestobe com-\npressed into model weights that ﬁt into the hardware cache. F ur-\nther,ittakesadvantage ofthegrowing supportforfastneur alnet-\nworkprocessinginmodernCPUs,suchaswidevectorinstruct ions,\nachieving a rateof tens ofnanoseconds perlookup.\nOur evaluation using 500K multi-ﬁeld rules from the standar d\nClassBench benchmarkshowsageometricmeancompressionfa c-\ntor of 4.9×, 8×, and 82×, and average performance improvement\nof 2.4×, 2.6×, and 1.6×in throughput compared to CutSplit, Neu-\nroCuts,and TupleMerge,all state-of-the-art algorithms1.\n1 INTRODUCTION\nPacket classiﬁcation is a cornerstoneof packet-switched networks.\nNetwork functions such as switches use a set of rulesthat deter-\nminewhichactiontheyshouldtakeforeachincomingpacket. The\nrules originate in higher-level domains, such as routing, Q uality\nof Service, orsecuritypolicies.Theymatch thepackets’ me tadata,\ne.g., the destination IP-address and/or the transport prot ocol. If\nmultiplerules match,therulewiththehighest priorityis used.\nPacket classiﬁcation algorithms have been studied extensi vely.\nThere are two main classes: those that rely on Ternary Conten t\nAddressable Memory (TCAM) hardware [13, 20, 23, 28, 37], and\nthosethatareimplementedinsoftware [3,8,21,22,34,36,4 1,44].\nIn this work, we focus on software-only algorithms that can b e\ndeployed invirtualnetwork functions,suchasforwarders o rACL\nﬁrewalls, running oncommodityX86servers.\nSoftware algorithms fall into two major categories: decisi on-\ntreebased[8,21,22,34,41,44]andhash-based[3,36].Thef ormer\nusedecisiontreesforindexingandmatchingtherules,wher easthe\nlatterperformlookupviahash-tablesbyhashingtherule’s preﬁxes.\nOthermethodsforpacketclassiﬁcation[7,38]arelesscomm onas\nthey either requiretoomuchmemoryorare tooslow.\nA key to achieving high classiﬁcation performance in modern\nCPUsistoensurethattheclassiﬁerﬁtsintotheCPUon-dieca che.\nWhen the classiﬁer is too large, the lookup involves high-la tency\n1Thisworkdoes not raiseanyethical issues.Incoming\nPacketIndependent Set\nRQ-RMI\n(CPUCache)iSet\nRules\n(DRAM)predicted\nindexcandidate\nrule\nRemainderSet\nExternal\nClassiﬁer\n(CPUCache)Remainder\nRules\n(DRAM)indexescandidate\nruleSelector\nAction\nFigure 1: NuevoMatch overview. The rules are divided into\nIndependent Sets indexed by RQ-RMIs and the Remainder\nSetindexedby anyclassiﬁer.OneRQ-RMIpredictsthestor-\nage index of the matching rule. The Selector chooses the\nhighest-prioritymatchingrule.\nmemory accesses, which stall the CPU, as the data-dependent ac-\ncesspatternduringthelookupimpedeshardwareprefetchin g.Un-\nfortunately, as the number of rules grows, it becomes diﬃcul t to\nmaintain the classiﬁer in the cache. In particular, in decis ion-tree\nmethods,rulesareoftenreplicatedamongmultipleleaveso fthede-\ncisiontree,inﬂatingitsmemoryfootprintandaﬀectingsca lability.\nConsequently,recentapproaches,notablyCutSplit[21]an dNeuro-\nCuts[22],seek toreducerulereplicationtoachieve better scaling.\nHowever,theystill failtoscale tolargerule-sets,whichinmodern\ndata centers may reach hundreds of thousands of rules [6]. Ha sh-\nbased techniques also suﬀer from poorscaling, as adding rul es in-\ncreases thenumberof hash-tables and their size.\nWe propose a novel approach to packet classiﬁcation, Nuevo-\nMatch, whichcompresses the rule-set index dramatically to ﬁt it\nentirely into the upper levels of the CPU cache (L1/L2) even f or\nlarge 500K rule-sets. We introduce a novel Range Query Recursive\nModelIndex (RQ-RMI)model,andtrainitto learntherules’match-\ning sets, turning rule matching into neural network inference . We\nshow that RQ-RMI achieves out-of-L1-cache execution by red uc-\ning the memory footprint on average by 4.9 ×, 8×, and 82×com-\nparedtorecentCutSplit[21],NeuroCuts[22],andTupleMer ge[3]\non the standard ClassBench [39] benchmarks, and up to 29 ×for\nreal forwardingrule-sets.\nTo the best of our knowledge, NuevoMatch is the ﬁrst to per-\nform packet classiﬁcation using trained neural network mod els.\nNeuroCuts also uses neural nets, but it applies them for opti miz-\ning the decision tree parameters during the oﬄinetree construc-\ntionphase;theirrulematchingstillusestraditional(opt imized)de-\ncision trees. In contrast, NuevoMatch performs classiﬁcat ion via\nRQ-RMIs, which are more space-eﬃcient than decision trees o r\nhash-tables, improving scalabilitybyanorder ofmagnitud e.\n\nNuevoMatch transforms the packet classiﬁcation task from\nmemory- to compute-bound. This design is appealing because it\nis likely to scale well in the future, with rapid advances in h ard-\nwareaccelerationof neural network inference [11,19,29]. On the\notherhand,theperformanceofbothdecisiontreesandhash- tables\nis inherently limited because of the poorscaling of DRAM acc ess\nlatency and CPU on-die cache sizes (e.g., 1 .5×over ﬁve years for\nL1 in Intel’s CPUs).\nNuevoMatch builds ontherecent work on learnedindexes [18],\nwhich applies a Recursive Model Index (RMI) model to indexin g\nkey-value pairs. The values are stored in an array, and the RM I\nistrainedtolearn themappingfunctionbetween thekeys and the\nindexesoftheirvaluesinthearray.Themodelisusedto predictthe\nindex given the key. When applied to databases [18], RMI boos ts\nperformancebycompressing theindexes toﬁt inCPUcaches.\nUnfortunately, RMI is not directly applicable for packet cl assi-\nﬁcation. First, a key (packet ﬁeld) may not have an exact matc h-\ning value, but match a rule range , whereas RMI can learn only\nexact key-index pairs. This is a fundamental property of RMI : it\nguaranteescorrectnessonlyforthekeys usedduringtraini ng,but\nprovides no such guarantees for non-existing keys ([18], Se ction\n3.4). Thus, for range matching it requires enumeration of al l pos-\nsible keys in the range, making it too slow. Second, the match is\nevaluated over multiplepacket ﬁelds, requiring lookupin a multi-\ndimensional space. Unfortunately, multi-dimensional RMI [17] re-\nquiresthattheinputbeﬂattenedintoonedimension,whichi nthe\npresenceofwildcardsresultsinanexponentialblowupofth einput\ndomain,makingittoolargetolearnforcompactmodels.Fina lly,a\nkey may match multiplerules, with the highest priority one u sed\nas output,whereas RMI retrieves onlya single index for each key.\nNuevoMatch successfullysolves thesechallenges.\nRQ-RMI . We design a novel model which can match keys to\nranges, with an eﬃcient training algorithm that does not req uire\nexhaustive key enumeration to learn the ranges. The trainin g\nstrives to minimize the prediction error of the index, while main-\ntaining a small model size. We show that the models can store i n-\ndicesof500KClassBenchrulesin35KB(§5.2.1).Weprovetha tour\nalgorithm guaranteesrangelookupcorrectness (§3.3).\nMulti-ﬁeld packet classiﬁcation . To enable multi-ﬁeld match-\ning with overlapping ranges, therule-set is splitinto inde pendent\nsetswithnon-overlappingranges,called iSets,eachassociatedwith\na single ﬁeld and indexed with its own RQ-RMI model. The iSet\npartitioning(§3.6)strivestocover therule-setwithasfe w iSetsas\npossible, discarding those that are toosmall. The remainderset of\ntherules notcovered bylargeiSets is indexed via existing c lassiﬁ-\ncationtechniques.Inpractice,therulesintheremainderc onstitute\nasmallfractioninrepresentativerule-sets,sotheremain derindex\nﬁts intoa fast cachetogether with theRQ-RMIs.\nFigure1summarizesthecompleteclassiﬁcationﬂow.Theque ry\noftheRQ-RMImodelsproducesthehintsforthesecondarysea rch\nthatselectsonematchingruleperiSet.Thevalidationstag eselects\nthe candidates with a positive match across all the ﬁelds, an d a\nselectorchooses thehighest prioritymatchingrule.\nConceptually,NuevoMatch canbeseenasan accelerator forex-\nistingpacketclassiﬁcationtechniquesandthuscomplemen tsthem.\nIn particular, the RQ-RMI model is best used for indexing rul es\nwith high value diversity that can be partitioned into fewer iSets.WeshowthattheiSetconstructionalgorithmiseﬀectivefor select-\ning the rules that can be indexed via RQ-RMI, leaving the rest in\nthe remainder (§5.3.1). The performance beneﬁts of NuevoMa tch\nbecomeevident whenitindexes morethan25%oftherules.Sin ce\nthe remainder is only a fraction of the original rule-set, it can be\nindexed eﬃciently with smaller decision-trees/hash-tabl es or will\nﬁtsmallerTCAMs.\nOur experiments2show that NuevoMatch outperforms all the\nstate-of-the-artalgorithmsonsynthetic andreal-liferu le-sets.For\nexample,itisfasterthanCutSplit,NeuroCuts,andTupleMe rge,by\n2.7×,4.4×and 2.6×inlatencyand2.4×,2.6×,and1.6×inthrough-\nput respectively, averaged over 12 rule-sets of 500K ClassB ench-\ngenerated rules, and by 7 .5×in latency and 3.5×in throughput\nvs. TupleMerge for the real-world Stanford backbone forwar ding\nrule-set.\nNuevoMatch supports rule updates by removing the updated\nrules from the RQ-RMI and adding them to the remainder set in-\ndexedbyanotheralgorithmthatsupportsfastupdates,e.g. , Tuple-\nMerge. This approach requires periodic retraining to maint ain a\nsmallremainderset;henceitdoesnotyetsupportmorethana few\nthousands of updates (§3.9). The algorithmic solutions to d irectly\nupdateRQ-RMI are deferred forfuturework.\nInsummary,ourcontributionsare asfollows.\n•We present an novel RQ-RMI model and a training technique\nfor learning packet classiﬁcationrules.\n•We demonstrate the application of RQ-RMI to multi-ﬁeld\npacket classiﬁcation.\n•NuevoMatchoutperformsexistingtechniquesintermsofmem -\noryfootprint,latency,andthroughputonchallengingrule -sets\nwithupto500Krules,compressingthemtoﬁtintosmallcache s\nof modernprocessors.\n2 BACKGROUND\nThis section describes the packet classiﬁcation problem an d sur-\nveys existing solutions.\n2.1 Classiﬁcation algorithms\nPacket classiﬁcation is the process of locating a single rul e that is\nsatisﬁed by an input packet among a set of rules. A rule contai ns\na few ﬁelds inthe packet’s metadata.Wildcards deﬁne ranges,i.e.,\nthey match multiple values. Ranges may overlap with each oth er,\ni.e., apacketmaymatchseveral rules,butonlytheonehavin gthe\nhighestpriorityisselected.Figure2illustratesaclassi ﬁerwithtwo\nﬁelds and ﬁve overlapping matching rules. An incoming packe t\nmatchestworules( R3,R4),butR3isusedasithasahigherpriority.\nPacket classiﬁcation performance becomes diﬃcult to scale as\nthenumberofrulesandthenumberofmatchingﬁeldsgrow.The re-\nfore, it has received renewed interest with increased compl exity\nof software-deﬁned data center networks, featuring hundre ds of\nthousands of rules per virtual network function [5] and tens of\nmatchingﬁelds (upto41in OpenFlow 1.4 [27]).\nDecisionTreeAlgorithms. Therulesareviewedashyper-cubes\nandpacketsaspointsinamulti-dimensionalspace.Theaxes ofthe\nrulespace representdiﬀerentﬁeldsandholdnon-negativeintegers.\n2Thesourcecode of NuevoMatchisavailablein[31].\n\nIPv4 Address Port Priority Action\nR010.10.*.* 10-18 1(highest) a1\nR110.10.1.* 15-25 2 a2\nR2 10.*.*.* 5-8 3 a3\nR310.10.3.* 7-20 4 a4\nR410.10.3.100 19 5(lowest) a5\nIncoming packet\n10.10.3.100:19Action to take\na4\nFigure2:Packetclassiﬁcationwithtwoﬁelds:IPaddressan d\nport.\nArecursivepartitioningtechniquedivides therulespacei ntosub-\nsets with at most binthrules. Thus, to match a rule, a tree traver-\nsal ﬁnds thesmallest subset fora given packet, whilea secon dary\nsearch scans over thesubset’s rulestoselect thebest match .\nUnfortunately, a rule replication problem may hinder perfor-\nmance in larger rule-sets by dramatically increasing the tr ee’s\nmemory footprint when a rule spans several subspaces. Early\nworks, such as HiCuts [8] and HyperCuts [34] both suﬀer from\nthisissue.MorerecentEﬃCuts[41]andCutSplit[21],sugge stthat\nthe rule set should be split into groups of rules that share si milar\nproperties and generate a separate decision-tree for each. Neuro-\nCuts[22],themostrecentworkinthisdomain,usesreinforc ement\nlearningforoptimizingdecisiontreeparameterstoreduce itsmem-\noryfootprint,orthenumberofmemoryaccesses duringtrave rsal,\nbyeﬃciently exploring alargetreeconﬁgurationspace.\nHash-BasedAlgorithms. TupleSpaceSearch[36]andrecentTu-\npleMerge [3] partition the rule-set into subsets according to the\nnumber of preﬁx bits in each ﬁeld. As all rules of a subset have\nthe same number of preﬁx bits, they can act as keys in a hash ta-\nble.Theclassiﬁcationisperformedbyextractingthepreﬁx bits,in\nall ﬁelds, of an incoming packet, and checking all hash-tabl es for\nmatchingcandidates.Asecondarysearcheliminatesfalse- positive\nresults and selects therulewiththehighest priority.\nHash-based techniques are eﬀective in an online classiﬁcation\nproblemwithfrequentruleupdates,whereasdecisiontrees arenot.\nHowever, decision trees have been traditionally considere d faster\nin classiﬁcation. Nevertheless, the recent TupleMerge has h-based\nalgorithm closes thegap and achieves high classiﬁcation th rough-\nputwhilesupportinghigh performanceupdates.\n2.2 Poorperformancewithlargerule-sets\nThepacketclassiﬁcationperformanceofalltheexistingte chniques\ndoesnotscalewellwiththenumberofrules.Thishappensbec ause\ntheirindexingstructuresspilloutofthefastL1/L2CPUcac hesinto\nL3 or DRAM. Indeed, as we show in our experiments (§5), Tuple-\nMerge and NeuroCuts exceed the 1MB L2 cache with 100K rules\nand CutSplit with 500K rules. However, keeping the entireindex-\ningstructureinfastcachesiscriticalforperformance.Th einherent\nlack of access locality in hash and tree data structures, com bined\nwith the data-dependent nature of the accesses, make hardwa re\nprefetchersineﬀectiveforhidingmemoryaccesslatency.T hus,the\nperformanceof alllookupsdropsdramatically.m0,0(x) s0\nm1,0(x) m1,1(x) m1,W1−1(x) s1\nMoreStages\nmn−1,0(x) mn−1,Wn−1−1(x) sn−1\n/v.alt0/v.alt1/v.alt2 /v.alt|I|−1 ValuesStage\nFigure3: RMI modelstructureandinference[18].\nThe performance drop is signiﬁcant even when the data struc-\ntures ﬁt in the L3 cache. This cache is shared among all the cor es,\nwhereasL1andL2cachesareper-core.Thus,L3isnotonlyslo wer\n(up to 90 cycles in recent X86 CPUs), but also suﬀers from cach e\ncontention,e.g.,whenanothercorerunsacache-demanding work-\nload and causes cache trashing. We observe the eﬀects of L3 co n-\ntentionin§5.2.1.\nNuevoMatch aims to provide more space eﬃcient representa-\ntionof theruleindex toscaletolargerule-sets.\n3 NUEVOMATCH CONSTRUCTION\nWe ﬁrst explain the RMI model for learned indexes which we use\nasthebasis,explainitslimitations,andthenshowoursolu tionthat\novercomes them.\n3.1 Recursive ModelIndex\nKraska etal.[18]suggest using machine-learning modelsfo r stor-\ningkey-valuepairsinsteadofconventionaldatastructure ssuchas\nB-trees or hash tables. The values are stored in a value array , and\naRecursive ModelIndex (RMI)is usedtoretrieve thevaluegiven a\nkey.Speciﬁcally,RMI predictstheindex ofthecorrespondingvalue\nin the value array using a model that learnedthe underlying key-\nindex mapping function.\nThe main insight is that any index structure can be expressed\nas a continuous monotonically increasing function /y.alt=h(x):\n[0,1]/mapsto→[0,1],wherexis a key scaled down uniformly into [0,1],\nand/y.altistheindex oftherespectivevalueinthevaluearrayscaled\ndownuniformlyinto [0,1].RMIistrainedtolearn h(x).Theresult-\ninglearnedindexmodel /hatwideh(x)performslookupsintwophases:ﬁrst\nit computes the predicted index/hatwide/y.alt=/hatwideh(ke/y.alt), and then performs a\nsecondary search in the array, in the vicinity ϵof the predicted in-\ndex, where ϵis themaximum index prediction error of the model,\nnamely|/hatwideh(ke/y.alt)−h(ke/y.alt)|≤ϵ.\nModelstructure. RMIis a hierarchical modelmadeofseveral ( n)\nstages (Figure3).Eachstage iincludesWisubmodels mi,j,j<Wi,\nwhereWiis thestage width .The ﬁrststage has a single submodel.\nEach successive stage has a larger width. The submodels in ea ch\nstage are trained on a progressively smaller subset of the in put\nkeys,reﬁningthe index prediction toward the submodels in the\nleaves. Thus, each key-index pair is learned by one submodel at\neachstage,withtheleafsubmodelproducingtheindexpredi ction.\nRMIisagenericstructure;avarietyofmachinelearningmod els\nor data structures can be used as submodels, such as regressi on\n\nmodelsorB-trees. Thetypeofthesubmodels,thenumberofst ages\nand thewidth ofeach stageareconﬁgured priortotraining.\nTraining. Trainingis performedstagebystage.\nFirst stage. The submodel in stage m0,0is trained on the whole\ndata set. Then, the input key-index pairs are split into W1dis-\njoint subsets. The input partitioning is performed as follo ws.\nFor each input key-index pair {ke/y.alt:idx}we compute the sub-\nmodel prediction /hatwidej=m0,0(ke/y.alt), satisfying /hatwidej∈[0,1). The output\n/hatwidejisusedtoobtain j=⌊/hatwidej·W1⌋which istheindex ofthesubmodel\nin stage 1, m1,j, to be used for learning {ke/y.alt:idx}. We call the\nsubset of the input to be learned by model mi,jas model input\nresponsibility domain Ri,j, or responsibility for short. R0,0is the\nwholeinput.\nInternal stages. The submodels in stage i,mi,j, are trained on\nthekeys in Ri,j(j<Wi).Aftertraining, theresponsibilities ofthe\nsubmodels in stage i+1 are computed,and the process continues\nuntil thelaststage.\nLast stage. The submodels of the last stage must predict the ac-\ntualindexofthematchingvalueinthevaluearray.However, asub-\nmodelmayhaveapredictionerror.Therefore,RMIusesthemo del\nprediction as a hint. The matching value is found by searching in\nthe value array in the vicinity of the predicted index, as deﬁ ned\nby the maximum error bound ϵof the model. Note that ϵshould\nbe valid for allinput key-index pairs. To compute ϵ, RMI exhaus-\ntively computes the submodel prediction foreach inputkey in its\nresponsibility.Submodelswithahigherrorboundareretra inedor\nconverted toB-trees.\nInference. Given a key, we iteratively evaluate each submodel\nstageafterstage,from m0,0.Weusethepredictioninstage i−1to\nselect a submodelin stage i,untilwe reach thelast stage. Thelast\nselectedsubmodelpredictstheindexinthevaluearray.Thi sindex\n/hatwideidetermines therangeforthesecondarysearch inthevaluear ray\nthat spans [ /hatwidei−ϵ,/hatwidei+ϵ].\n3.2 RMIlimitations\nDirect applicationofRMItoindexing packetclassiﬁcation rulesis\nnot possibleforthefollowingreasons:\nNo support for range matching3.RMI allows only an exact\nmatch for a given key, whereas packet classiﬁcation require s re-\ntrieving rules with matching rangesas deﬁned by wildcards. This\nproblemisfundamental:RMIexhaustivelyenumerates allthekeys\nin all the ranges to calculate the submodel responsibility a nd the\nmaximum model prediction error (see the underlined parts of the\ntrainingalgorithm).Inotherwords,allthevaluesinthera ngemust\nbe materialized into key-index pairs for RMI to learn them, s ince\nRMIdoes not guarantee correct lookup for keys not used in train-\ning[18].Theoriginalpapersketchesafewpossiblesolutions, how-\never, they either rely onmodelmonotonicity(whilewedo not )or\nusesmarter yet still expensive enumeration techniques.\nSlow multi-dimensionalindexing. RMI isineﬀective formulti-\ndimensional indexes because the proposed solution [17] lea ds to\n3The RMI paper also uses the term range index while applying RMI to range index\ndata structures (i.e., B-trees) that can quickly retrieve a ll stored keys in a requested\nnumerical range. Our work is fundamentally diﬀerent: given a key it retrieves the\nindex of itsmatchingrange .xM(x),⌊M(x)·4⌋1,4\n0.75,3\n0.5,2\n0.25,1\n0,0t0 t1t2t3t4\nFigure 4: Transition inputs ( t0,...,t4) for a piece-wise linear\nfunctionwith theoutputdomain=4.\ngeneratinganexponentialnumberofrulesinthepresenceof wild-\ncards. For example, a single rule with wildcards in destinat ion IP\n(0.0.0.*),port(10-100),andprotocol(TCP/UDP)resultsi n46,592dis-\ntinctkey-index pairs.Sincetheinputdomainbecomestoola rge,it\nrequires a largemodelthat exceeds theCPUcache.\nIn the following we outline the solutions to these challenge s\nWe ﬁrst discuss Range-Query RMI (RQ-RMI), which extends RMI\nto perform range-value queries in a one-dimensional index where\nranges donotoverlap(§3.3-§3.5).WethenshowhowtoapplyR Q-\nRMIinmulti-dimensional index spacewith overlaps (§3.6-§ 3.7).\n3.3 One-dimensional RQ-RMI\nWe ﬁrst seek to ﬁnd a way to perform range matching over a set\nofnon-overlapping ranges in onedimension.\nThereare twobasic ideas:\nSampling. Eachsubmodel mi,jistrainedbygeneratingauniform\nsample of key-index pairs from input ranges in its responsib ility.\nThesamples are generated on-the-ﬂy foreach submodel(§3.5 .4).\nAnalytical error bound estimation for ranges. We eliminate\ntheRMI’s requirement forexhaustive key-value enumeratio n dur-\ningtraining bymaking thefollowingobservation: ifa submodelis\napiece-wiselinearfunction,theworst-caseerrorbound ϵcanbecom-\nputedanalytically ,thereby enabling eﬃcient learning of ranges.\nThe intuition behind this observation is illustrated in Fig ure 4.\nIt shows the graph of some piece-wise linear function which r ep-\nresents a submodel Mwhose outputs are quantized into integers\nin[0,4),i.e.,Mpredictstheindex inanarrayofsize4.Wecallthe\ninputsforwhichthisfunctionchanges itsquantizedoutput transi-\ntion inputs ti∈T. In turn, transition inputs determine the region\nof inputs with the samequantized output.Therefore, given an in-\nput range in the model’s responsibility, to compute the mode l’s\nmaximum prediction error for any key in that range, it suﬃces to\nevaluate the prediction error in the transition inputs that fall in\nthe range. We describe the training algorithm that relies on these\nobservationsinSection3.4.Wenowprovideamoreformaldes crip-\ntion,butleavemost oftheproofsintheAppendix.\n3.4 Usinga neural networkas asubmodel\nWe choose to use a 3-layer fully-connected neural network (N N)\nwith a single hidden layer and ReLU activation A.Such NNs have\nbeen suggested in the original RMI paper [18]; however, they did\nnot leverage their properties for accelerating error bound compu-\ntations.\nWedenotea submodelas mi,j,and deﬁneit as follows.\n\nCompute\ntransition\ninputsGenerate\ndatasetw/\nssamplesTrain\nsubmodelCompute\nerror\nbounds\nSubmodels w/error >threshold. s←2s\nFigure 5: The submodel training process. The additional\nphase for training submodels in the leaves is depicted with\ndashedlines.\nDeﬁnition 3.1 (RQ-RMI submodel). Denote the output of a 3-\nlayer fully-connected neuralnetwork as:\nNi,j(x)=A/parenleftbigx·w1+b1/parenrightbig×w2+b2\nwherexis a scalar input, w1,b1are the weight and bias row-\nvectorsforlayer1(hiddenlayer),and w2,b2aretheweightcolumn-\nvectorand biasscalarforlayer 2.Notethat Ni,j(x)is ascalar.The\nReLU function Aapplies a function aoneach element of an input\nvector:\na(x)=/braceleftBigg\nx x≥0\n0x<0.\nThesubmodeloutput,denoted Mi,j(x),is deﬁned asfollows:\nMi,j(x)=H/parenleftbigNi,j(x)/parenrightbig\nwhereH(x)trimstheoutputdomaintobein [0,1).\nC/o.sc/r.sc/o.sc/l.sc/l.sc/a.sc/r.sc/y.sc 3.2. Mi,j(x)isa piece-wiselinearfunction.\n3.5 RQ-RMItraining\nWe use Corollary 3.2 to compute the transition inputs and the re-\nsponsibilityofthesubmodels.Weprovideasimpliﬁeddescr iption;\nseeAppendix forthepreciseexplanation.\n3.5.1 Overview. RQ-RMI training is similar to RMI’s. It is per-\nformed stageby stage. Figure 5 illustrates thetraining pro cess for\none stage. We start by training the single submodel in the ﬁrs t\nstage using the entire input domain. Next, we calculate its t ran-\nsition inputs (§3.5.2) and use them to ﬁnd the responsibilit ies of\nthesubmodelsinthefollowingstage(§3.5.3).Weproceedby train-\ning submodels in the subsequent stage using designated data sets\nwe generate based on the submodels’ responsibilities (§3.5 .4). We\nrepeat this process until all submodels in all internal stag es are\ntrained. For the submodels in the leaves (last stage), there is an\nadditional phase (dashed lines in Figure 5). After training , we cal-\nculate their error bounds and retrain the submodels that do n ot\nsatisfy a predeﬁned errorthreshold (§3.5.6).\n3.5.2 Computing transition inputs. Given a trained submodel\nmwe can analytically ﬁnd all its linear regions, and respecti vely\nthe inputs delimiting them, which we call trigger inputs /afii10069.itall. For\nall inputs in the region [/afii10069.itall,/afii10069.itall+1], the model function, denoted as\nM(x),islinearbyconstruction.Ontheotherhand,theuniformou t-\nput quantizationdeﬁnes a step-wise function Q=⌊M(x)·W⌋/W,\nwhereWis the size of the quantized output domain (Figure 4).\nThus, for each input region [/afii10069.itall,/afii10069.itall+1], the set of transition inputs\ntl∈Tare thosewhere M(x)andQintersect.3.5.3 Computingtheresponsibilitiesofsubmodelsinthefo llow-\ning stage. Given a trained submodel mi,jin an internal stage i,\nwe say that it mapsake/y.altto a submodel mi+1,k,k<Wi+1, if\n⌊Mi,j(ke/y.alt)·Wi+1⌋=k. As discussed informally earlier, the re-\nsponsibility Ri+1,kofmi+1,kis deﬁnedas alltheinputswhich are\nmapped by submodels in stage itomi+1,k. In other words, the\ntrainedsubmodelsatstage ideﬁnetheresponsibilityofuntrained\nsubmodelsat stage i+1.\nKnowing theresponsibility of a submodelis crucial,as it de ter-\nmines the subset of the inputs used to train the submodel. RMI\nexhaustively evaluates all the inputs, which is ineﬃcient. Instead,\nwe compute Ri+1,kusing the transition inputs of mi,j. In the fol-\nlowing, we assume for clarity that Ri,jis contiguous, and mi,jis\ntheonlysubmodelatstage i.\nWe compute Ri+1,kby observing that it is composed of all the\ninputsintheregions (tl,tl+1)thatmaptosubmodel mi+1,k,where\ntl∈Ti,jare transition inputs of mi,j. By construction, the inputs\nin the region between two adjacent transition points map to t he\nsameoutput.Then,itsuﬃcestocomputetheoutputof mi,jforits\ntransition points, and choose the respective input ranges t hat are\nmappedto mi+1,k.\n3.5.4 Training a submodel with ranges using sampling. Up to\nthispoint,weusedonlykey-index pairsasmodelinputs.Now we\nfocusontrainingoninput ranges.Arangecanberepresentedasall\nthekeys thatfallintotherange,allassociatedwiththe sameindex\noftherespectiverule.Forexample,10.1.1.0-10.1.1.255i ncludes256\nkeys.Ourgoalistotrainamodelsuchthatgivenakeyinthera nge,\nthe model predicts the correct index. Enumerating all the ke ys in\ntheranges is ineﬃcient. Instead,we usesamplingas follows .\nWegeneratethetrainingkey-indexpairsbyuniformlysampl ing\nthe submodel’s responsibility. We start with a low sampling fre-\nquency. A sample is included in the training set if there is an in-\nputrulerangethatmatchesthesampledkey.Thus,thenumber of\nsamples per input range is proportional to its relative size in the\nsubmodel’sresponsibility.Notethatsomeinputranges(or individ-\nual keys) might not be sampled at all. Nevertheless, they wil l be\nmatchedcorrectlyas weexplain further.\n3.5.5 Submodeltraining. Wetrainsubmodelsonthegenerated\ndatasets using supervised learning and Adam optimizer [14] with\namean squarederror lossfunction.\n3.5.6 Computingerrorbounds. Givenatrainedsubmodelinthe\nlast stage, we compute the prediction error bound for all inputs\ninitsresponsibility byevaluatingthesubmodelonitstransitionin-\nputs.Thepredictionerroriscomputedalsofortheinputsth atwere\nnotnecessarilysampled,guaranteeingmatchcorrectness. Iftheer-\nror is too large, we doublethe number of samples, regenerate the\nkey-index pairs,and retrain thesubmodel.Training contin ues un-\ntil thetarget error boundis attained orafter a predeﬁned nu mber\nof attempts. If training does not converge, the target error bound\nmaybeincreasedbytheoperator.Theerrorbounddetermines the\nsearchdistanceofthesecondarysearch ;hencealargerboundcauses\nlowersystem performance. Weevaluatethis tradeoﬀlater(§ 5.3.4).\n\nIP\nAddressPort Number\nR0r0\n1R1\nr1\n0R2r2\n1R3\nr3\n0r4\n1R4\nFigure 6: Rules from Figure 2 are split into two iSets:\n{R0,R2,R4}(byport),and{R1,R3}(byIP).\n3.6 Handlingmulti-dimensional queries with\nrangeoverlaps\nNuevoMatch supportsoverlapped ranges and matching over mu l-\ntipledimensions,i.e.,packetﬁelds,bycombiningtwosimp leideas:\npartitioning therule-set into disjoint independent sets ( iSets), and\nperformingmulti-ﬁeldvalidationofeachrule.Inthefollo wing,we\nusetheterms dimension and ﬁeld interchangeably.\nPartitioning. Each iSet contains rules that do not overlap in one\nspeciﬁcdimension .Werefertothe coverageofaniSetasthefraction\noftherulesitholdsoutofthoseintheinput.OneiSetmaycov erall\ntherulesiftheydonotoverlapinatleastonedimension, whe reas\nthe same dimension with many overlapping ranges may require\nmultipleiSets.Figure6showstheiSets fortherulesfromFi gure2.\nEachiSetisindexedbyoneRQ-RMI.Thus,toﬁndthematchtoa\nquerywithmultipleﬁelds,wequeryallRQ-RMIs(inparallel ),each\nover the ﬁeld on which it was trained. Then, the highest prior ity\nresult is selectedas theoutput.\nEach iSet adds to the total memory consumption and compu-\ntational requirements of NuevoMatch. Therefore, we introd uce a\nheuristicthatstrivestoﬁndthesmallestnumberofiSetsth atcover\nthelargest partof therule-set (§3.6.1).\nMulti-ﬁeld validation. Since an RQ-RMI builds an index of the\nrules over a single ﬁeld, it might retrieve a rule which does n ot\nmatch against other ﬁelds. Hence, each rule returned by an RQ -\nRMI is validated across all ﬁelds. This enables NuevoMatch t o\navoid indexing all dimensions, yet obtaincorrectresults.\n3.6.1 iSet partitioning. We introduce a greedy heuristic that\nrepetitively constructs the largest iSet from the input rul es, pro-\nducingagroupofiSets.ToﬁndthelargestiSetoveronedimen sion,\nweuseaclassical intervalschedulingmaximization algorithm[15].\nThe algorithm sorts the ranges by their upper bounds,and rep eti-\ntivelypickstherangewiththesmallestupperboundthatdoe snot\noverlap previouslyselectedranges.\nWe apply the algorithm to ﬁnd the largest iSet in each ﬁeld.\nThen we greedily choose the largest iSet among all the ﬁelds a nd\nremove its rules from the input set. We continue until exhaus ting\ntheinput.Thisheuristicissub-optimalbutquiteeﬃcient. Weplan\ntoimprove it infuturework.\nHaving a larger number of ﬁelds in a rule-set might help im-\nprove coverage. For example, if the rules that overlap in one ﬁeld\ndonotoverlapinanotherandviceversa,twoiSetscoverthew hole\nrule-set,requiring moreiSets foreach ﬁeldin isolation.3.7 Remainder set and external classiﬁers\nReal-worldrule-setsmayrequiremanyiSetsforfullcovera ge,with\nasingleruleperiSetintheextremecases.UsingseparateRQ -RMIs\nforsuchiSetswillhinderperformance.Therefore,wemerge small\niSetsintoasingle remainderset .Therulesintheremainder setare\nindexed using an externalclassiﬁer. Each query is performed on\nboththeRQ-RMI and theexternal classiﬁer.\nInessence,NuevoMatchservesasan accelerator fortheexternal\nclassiﬁer.Indeed,ifrule-setsarecoveredusingafewlarg eiSets,the\nexternal classiﬁer needs to index a small remainder set that often\nﬁtsinto faster memory,soit canbevery fast.\nTwoprimaryfactorsdeterminetheend-to-endperformance: (1)\nthe number of iSets required for high coverage (depends on th e\nrule-set), and; (2) the number of iSets for achieving high pe rfor-\nmance(set byanoperator).\nOur evaluation (§5.3.1) shows that most of the evaluated rul e-\nsets can be covered with high coverage above 90% with only 2-3\niSets. This is enough to accelerate the external classiﬁer, as is evi-\ndent from the performance results. On the other hand, the cho ice\nofthenumberofiSetsdependsontheexternalclassiﬁerprop erties,\nin particular, its sensitivity to memory footprint. We anal yze this\ntradeoﬀin§5.3.\nWorst-caseinputs. Somerule-setscannotachievegoodcoverage\nwith only a few iSets. For example, a rule-set with a single ﬁe ld\nwhoseranges overlap requires toomany iSets tobecovered.\nTo obtain a better intuition about the origins of worst-case in-\nputs,weconsider thenotionof rule-setdiversity forrule-setswith\nexactmatches.Rule-setdiversityinaﬁeldisthenumberofu nique\nvaluesinitacrosstherule-set,dividedbythetotalnumber ofrules.\nTherule-setdiversityisanupperboundonthefractionofru lesinthe\nlargest iSet of that ﬁeld . In other words, low diversity implies that\nusingtheﬁeld foriSet partitioningwouldresult inpoorcov erage.\nWe can also identify challenging rule-sets with ranges. We d e-\nﬁnerule-set centrality as the maximal number of rules that each\npairofthemoverlap(theyallshareapointinamulti-dimens ional\nspace).Therule-setcentralityisalowerboundonthenumberofiSet s\nrequiredfor full coverage .\nThe diversity and centrality metrics can indicate the poten tial\nofNuevoMatchtoacceleratetheclassiﬁcationofarule-set .Onthe\npositive side, our iSet partitioning algorithm is eﬀective at segre-\ngatingtherulesthatcannotbecoveredwellfromtherulesth atcan,\nthereby accelerating the remainder classiﬁer as much as pos sible\nfora given rule-set. Weanalyze thispropertyin §5.3.3.\n3.8 Puttingitalltogether\nWebrieﬂy summarizeallthesteps ofNuevoMatch.\nTraining\n(1) Partitiontheinputinto iSets and a remainder set\n(2) TrainoneRQ-RMI oneach iSet\n(3) Constructanexternal classiﬁer fortheremainder set\nLookup\n(1) Query alltheRQ-RMIs\n(2) Query theexternal classiﬁer\n(3) Collectalltheoutputs,returnthehighest-priority ru le\n\nτ 2τ 3τ 4τTimeThroughput Fasttraining Long training\nFigure 7: Updates impact on Throughput over time. An up-\nperbound (ingreen)isfor zero trainingtime.\n3.9 RuleUpdates\nWe explain how NuevoMatch can support updates with a limited\nperformancedegradation.\nFirstly, an external classiﬁer used for the remainder must s up-\nportupdates.Amongtheevaluatedexternalclassiﬁersonly Tuple-\nMerge is designed forfast updates.\nSecondly, we distinguish four types of updates: (i)a change in\ntherule action ;(ii)ruledeletion (iii) rulematching set change ;(iv)\nruleaddition.\nThe ﬁrst two types of updates are supported without perfor-\nmance degradation, and require a lookup followed by an updat e\nin the value array. However, if an updatemodiﬁes a rule’s mat ch-\ning set or adds a new rule, it might require modiﬁcations to th e\nRQ-RMI model.We currently do not know an algorithmic way to\nupdate RQ-RMI without retraining; therefore, an updated ru le is\nalways added totheremainder set.\nUnfortunately, this design leads to gradual performance de gra-\ndation,asupdatesarelikelytoincreasetheremainderset. Accord-\ningly, the model is retrained on the updated rule-set, eithe r peri-\nodicallyorwhenalargeperformancedegradationis detecte d.Up-\ndates occurringwhile retraining are accommodatedin the fo llow-\ning batch ofupdates.\nEstimatingsustainedupdaterate. Letrandubethetotalnum-\nber of rules and the number of updatesthat move a ruletothe re -\nmainder,respectively; ucanbesmallerthantherealrateofruleup-\ndates.Weassumethattheupdatesareindependent andunifor mly\ndistributedamong the rrules. For each ruleupdate,a ruleis mod-\niﬁed w.p. (with probability)1\nr. Thus a rule is not modiﬁed in any\nof theupdatesw.p. (1−1\nr)u≈e−u/r.Theexpectednumber of un-\nmodiﬁedrulesis r·(1−1\nr)u≈r·e−u/r.Throughputbehaves asa\nweightedaveragebetweenthatofNuevoMatchandtheremaind er\nimplementation, based onthenumberof rulesin each.\nFigure 7 illustrates the throughput over time for diﬀerent r e-\ntraining rates given a certain update rate. If retraining is invoked\neveryτtime units, the slower the training process, the worse the\nperformancedegradation.\nWith these update estimates, using the measured speedup as a\nfunctionofthefractionoftheremainder(§5.3.3),NuevoMa tchcan\nsustain up to 4k updates per second for 500K rule-sets, yield ingabouthalfthespeedupoftheupdate-freecase,assumingami nute-\nlongtraining.Theseresultsindicatetheneedforspeeding uptrain-\ning, but weconjecture there might be a moreeﬃcient wayto per -\nform updates directly in RQ-RMI without complete re-traini ng of\nallsubmodels.Accelerating updatesis leftfor futurework .\n4 IMPLEMENTATION DETAILS\nRQ-RMI structure. The number of stages and the width of each\nstage depend on the number of rules to index. We increase the\nwidthof thelast stagefrom 16for 10Krulestoas muchas 512fo r\n500K.SeeTable4intheAppendix.\nSubmodelstructure. Eachsubmodelis a fullyconnected 3-layer\nneuralnetwith1input,1output,and8neuronsinthehiddenl ayer\nwithReLUactivation.Thisstructureaﬀordsaneﬃcientvect orized\nimplementation(see below).\nTraining. WeuseTensorFlow[1]totraineachsubmodelonaCPU.\nTrainingasubmodelrequiresafewseconds,butthewholeRQ- RMI\nmaytakeuptoafewminutes(see§5.3.4).Webelieve,however ,that\namuchfastertrainingtimecouldbeachieved withmoreoptim iza-\ntions, i.e., replacing TensorFlow (known for its poor perfo rmance\nonsmall models).Weleave itforfuturework.\niSet partitioning. We implement the iSet partitioning algorithm\nusingPython.Thepartitioningtakes atmostafew seconds an dis\nnegligible comparedtoRQ-RMI training time.\nInference and secondary search. We implement RQ-RMI infer-\nence in C++. For each iSet we sort the rules by the value of the\nrespective ﬁeld to optimize the secondary search. To reduce the\nnumber of memory accesses, we pack multiple ﬁeld values from\ndiﬀerent rulesin thesamecacheline.\nHandling long ﬁelds. Both iSet partitioning algorithms and RQ-\nRMI models map the inputs into single-precision ﬂoating-po int\nnumbers.This allows thepacking of morescalars invector op era-\ntions, resulting in faster inference. While enough for 32-b it ﬁelds,\ndoing so might cause poor performance for ﬁelds of 64-bits an d\n128-bits.\nWe compared two solutions: (1) splitting the ﬁelds into 32-b it\nparts and treating each as a distinct ﬁeld, and (2) using a sin gle-\nprecision ﬂoating-point to express long ﬁelds. The two show ed\nsimilar results for iSet partitioning with MAC addresses, w hile\nwith IPv6, splitting into multiple ﬁelds worked better. Not e that\nboth the secondary search and the validation phases are not a f-\nfectedbecausetherules arestoredwith theoriginal ﬁelds.\nVectorization. We accelerate the inference by using wide CPU\nvectorinstructions.Speciﬁcally,with8neuronsinthehid denlayer\nofeach submodel,computingthepredictioninvolves a handf ul of\nvectorinstructions.Validationis also vectorized.\nTable 1 shows the eﬀectiveness of vectorization. The use of\nwiderunitsspeedsupinference,highlightingthepotentia lforscal-\ningNuevoMatch infutureCPUs.\nParallelization. NuevoMatch lends itself to parallel execution\nwhereiSetsandtheremainderclassiﬁerruninparallelondi ﬀerent\nCPUcores.Thesystemreceivesthepacketsandenqueueseach for\n\nTable 1: Submodel acceleration via vectorization. Methods\nare annotated with the number of ﬂoats per single instruc-\ntion.\nInstruction set (width) Serial(1) SSE(4) AVX(8)\nInferenceTime(ns) 126 62 49\nexecution into the worker threads. The threads are statical ly allo-\ncatedtorunRQ-RMIortheexternalclassiﬁerwithabalanced load\nbetween thecores.\nNote that since RQ-RMI are small and ﬁt in L1, running them\non a separate core enables L1-cache-resident executions ev en if\ntheremainder classiﬁerislarge.Suchaneﬃcientcacheutil ization\ncouldnothavebeenachievedwithotherclassiﬁersrunningo ntwo\ncores.\nEarlytermination. Onedrawbackoftheparallelimplementation\nis that the slowest thread determines the execution time. Ou r ex-\nperiments show that the remainder classiﬁer is the slowest o ne.\nIt holds only a small fraction of the rules, so it returns an em pty\nset for most of the queries, which in turn leads to the worst-c ase\nlookuptime. In TupleMerge, forexample, a querywhich does n ot\nﬁndanymatchingrulesresultsinasearchoveralltables,wh ereas\nin theaverage casesometables areskipped.\nInstead,wequerytheremainder afterobtainingtheresultsfrom\nthe iSets, and terminate the search when we determine that th e\ntarget ruleis not intheremainder.\nTo achieve that, we make minor changes to existing classiﬁca -\ntiontechniques. Speciﬁcally,indecision-tree algorithm s, westore\nineachnodethemaximumpriorityofallthesub-treerules.W hen-\never we encounter a maximum priority that is lower than that\nfoundintheiSets,weterminatethetree-walk.Thechanges t othe\nhash-based algorithmsare similar.\nWe callthis optimization earlytermination .With this optimiza-\ntion, both the iSets and the remainder are queried on the same\ncore. While a parallel implementation is possible, it incur s higher\nsynchronization overheads amongthreads.\n5 EVALUATION\nIn theevaluation,we pursuedthefollowinggoals.\n(1) Comparison of NuevoMatch with the state-of-the-art alg o-\nrithmsTupleMerge[3],CutSplit[21],and NeuroCuts[22];\n(2) Systematic analysis of the performance characteristic s, in-\ncluding coverage in challenging data sets, the eﬀect of RQ-\nRMIerror bound,and training time.\n5.1 Methodology\nWe ran the experiments on Intel Xeon Silver 4116 @ 2.1 GHz\nwith 12 cores, 32KB L1, 1024KB L2, and 16MB L3 caches, running\nUbuntu 16.04(Linux kernel 4.4.0). Wedisablepower managem ent\nfor stablemeasurements.\nEvaluatedconﬁgurations. CutSplit ( /c.sc/s.sc)issetwith binth=8,as\nsuggested in [21].\nForNeuroCuts( /n.sc/c.sc),weperformedahyperparametersweepand\nselected the best classiﬁer per rule-set. As recommended in [22],wefocusedonbothtop-modepartitioningandrewardscaling .We\nranthesearchonthree12-coreIntelmachines,allocatings ixhours\nper conﬁguration to converge. In total, we ran /n.sc/c.sctraining for up\nto 36 hours per rule-set. In addition, we developed a C++ impl e-\nmentation of /n.sc/c.scfor faster evaluation of the generated classiﬁers,\nmuchfaster thantheauthors’Python-based prototype.\nTupleMerge( /t.sc/m.sc)isusedwiththeversionthatsupportsupdates\nwithcollision-limit=40,as suggested in[3].\nNuevoMatch ( /n.sc/m.sc) was trained with a maximum error thresh-\noldof 64. Wepresent the analysis of thesensitivity to thech osen\nparameters and training times in§5.3.2.\nMulti-core implementation. We run a parallel implementation\non two cores. NuevoMatch allocates one core for the remainde r\ncomputationsandthesecondfortheRQ-RMIs.For /c.sc/s.sc,/n.sc/c.sc,and/t.sc/m.sc,\nwe ran two instances of the algorithm in parallel on two cores\nusing two threads (i.e., no duplication of the rules), split ting the\ninput equally between the cores. We discarded iSets with cov er-\nage below 25% for comparisons against /c.sc/s.scand/n.sc/c.sc, and below 5%\nfor comparisons against /t.sc/m.sc. We used batches of 128 packets to\namortize the synchronization overheads. Thus, these algor ithms\nachieve almost linear scaling and the highest possiblethro ughput\nwithperfectload-balancing betweenthecores.\nSingle-core implementation. We used a single core to measure\nthe performance of NuevoMatch with the early termination op ti-\nmization.For /n.sc/m.sc,wediscarded iSets withcoverage below25%.\n5.1.1 Packettracesandrule-sets. Forevaluating eachclassiﬁer,\nwegeneratedtraceswith700Kpackets.Weprocessedeachtra ce6\ntimes, using the ﬁrst ﬁve as warmup and measuring the last. We\nreporttheaverage of15measurements.\nUniformtraﬃc. Wegeneratetracesthataccessallmatchingrules\nuniformlytoevaluatetheworst-casememoryaccess pattern .\nSkewed traﬃc. For each rule-set we generate traces that follow\nZipfdistributionwithfourdiﬀerentskewparameters,acco rdingto\ntheamountoftraﬃcthataccountsforthe3%mostfrequentﬂow s\n(e.g., 80% of the traﬃc accounts for the 3% most frequent ﬂows ).\nThisisrepresentativeofrealtraﬃc,ashasbeenshowninpre vious\nworks [13,33].\nAdditionally,we usea real CAIDA tracefrom theEquinixdata -\ncenter in Chicago [2]. As CAIDA does not publish the rules use d\nto process the packets, we modify the packet headers in the tr ace\ntomatcheach evaluatedrule-setasfollows.Foreachrule,w egen-\nerate one matching ﬁve-tuple. Then, for each packet in CAIDA ,\nwe replace the original ﬁve-tuple with a random ﬁve-tuple ge n-\nerated from the rule-set, while maintaining a consistent ma pping\nbetweentheoriginalandthegenerated one.Notethattherul e-set\naccess localityof thegenerated traceis thesame oras high a s the\noriginal trace.\nClassBench rules. ClassBench [39] is a standard benchmark\nbroadlyusedforevaluatingpacketclassiﬁcationalgorith ms[3,16,\n21,22,28,41,44].Itcreatesrule-setsthatcorrespondtot heruledis-\ntributionofthreediﬀerentapplications:AccessControlL ist(ACL),\nFirewall (FW), and IP Chain (IPC). We created rule-sets of si zes\n500K,100K,10K,and1K,eachwith12distinctapplications, allwith\n5-ﬁeldrules:sourceanddestinationIP,sourceanddestina tionport,\nand protocol.\n\n1 2 3 4 5 6 7 8 9 10 11 12 GM1 2 3 4 5 6 7 8 9 10 11 12 GM02468\n100K Classiﬁers 500K ClassiﬁersLatency\nSpeedup\nNuevoMatch w/CutSplit NuevoMatch w/NeuroCuts NuevoMatch w/TupleMerge1 2 3 4 5 6 7 8 9 10 11 12 GM1 2 3 4 5 6 7 8 9 10 11 12 GM01234\n100K Classiﬁers 500K ClassiﬁersThroughput\nSpeedup\nFigure8:ClassBench:NuevoMatchvs.CutSplit,NeuroCuts, andTupleMerge,usingtwoCPUcores.(Seerule-setintheApp en-\ndix.)\nReal-world rules. WeusedtheStanfordBackbonedatasetwhich\ncontains a large enterprise network conﬁguration [46]. The re are\nfour IP forwarding rule-sets with roughly 180K single-ﬁeld rules\neach (i.e., destinationIP address).\n5.2 End-to-endperformance\nForfaircomparison,NuevoMatchused thesamealgorithmforboth\nthe remainder classiﬁer and the baseline. For example, we ev al-\nuated the speedup produced by NuevoMatch over /c.sc/s.scwhile also\nusing/c.sc/s.sctoindex theremainder set.\nWe present the results for random packet traces, followed by\nskewed and CAIDA traces.\nLargerule-sets:ClassBench:multi-core. Figure8showsthat,in\nthelargestrule-sets(500K),theparallelimplementation ofNuevo-\nMatch achieves a geometric mean factor of 2.7 ×, 4.4×, and 2.6×\nlower latency and 1.3 ×, 2.2×, and 1.2×higher throughput over\n/c.sc/s.sc,/n.sc/c.sc, and/t.sc/m.sc, respectively. For the classiﬁers with 100K rules,\nthegains arelower butstillsigniﬁcant: 2.0 ×,3.6×,and 2.6×lower\nlatency and 1.0×, 1.7×, and 1.2×higher throughput over /c.sc/s.sc,/n.sc/c.sc,\nand/t.sc/m.sc,respectively.Theperformancevariesamongrule-sets,i. e.,\nsomeclassiﬁers are upto1.8 ×faster than /c.sc/s.scfor 100˙Kinputs.\nLarge rule-sets: ClassBench: single core. Figure 9 shows the\nthroughput speedup of /n.sc/m.sccompared to /c.sc/s.sc,/n.sc/c.sc, and/t.sc/m.sc. For 500K\nrule-sets, NuevoMatch achieves a geometric mean improveme nt\nof 2.4×, 2.6×, and 1.6×in throughput compared to /c.sc/s.sc,/n.sc/c.sc, and\n/t.sc/m.sc,respectively.Forthesinglecoreexecutionthelatencyan d the\nthroughputspeedupsare thesame.\nLarge rule-sets: Stanford backbone: multi-core. Figure 10\nshowsthespeedupof /n.sc/m.scover/t.sc/m.scforthereal-worldStanfordback-\nbonedatasetwith4rule-sets. /n.sc/m.scachieves 3.5×higherthroughput\nand 7.5×lower latency over /t.sc/m.sconallfourrule-sets.\nSmallrule-sets:multi-core. Forrule-setswith1Kand10Krules,\nNuevoMatch results in the same or lower throughput, and 2.2 ×\nand 1.9×on average better latency compared to /c.sc/s.scand/t.sc/m.sc. The\nlowerspeedupisexpected,asboth /c.sc/s.scand/t.sc/m.scﬁtintoL1(§5.2.1),so\n/n.sc/m.scdoesnotbeneﬁtfromreducedmemoryfootprint,whileadding\ncomputationaloverheads. See Appendixforthedetailedcha rt.1 2 3 4123·106\n3.51×3.49×3.40×3.56×Throughput\n(pps)\nTupleMerge NuevoMatch w/TupleMerge1 2 3 402004007.51×7.84×7.59×7.47×\nLatency\n(µs)\nFigure 10: End-to-end performance on real Stanford back-\nbonedatasets.\nThe/c.sc/s.scresults are averaged over three rule-sets of 1K and six\nrule-sets for 10K. In the remaining rule-sets, NuevoMatch d id not\nproducelarge-enoughiSetstoacceleratetheremainder.No te,how-\never, that it promptly identiﬁes the rule-sets expected to b e slow\nand fallsback totheoriginal classiﬁer.\nThe source of speedups. The ability to compress the rule-set to\nﬁtintofastermemorywhileretainingfastlookupisthekeyf actor\nunderlyingtheperformancebeneﬁtsofNuevoMatch.Toillus trate\nit,we take a closer lookat the performance. We evaluate /t.sc/m.scwith\nandwithout /n.sc/m.scaccelerationasafunctionofitsmemoryfootprint\nonClassBench-generated 1K,10K,100Kand 500Krule-sets fo r one\napplication(ACL).\nFigure 11 shows that the performance of /t.sc/m.scdegrades as the\nnumberofrulesgrows,causingthehashtablestospilloutof L1and\nL2caches. /n.sc/m.sccompressesalargepartoftherule-set(seecoverage\nannotations), thereby making the remainder index small eno ugh\ntoﬁtintheL1 cache, and gaining back thethroughputequival ent\nto/t.sc/m.sc’sonsmallrule-sets.\nClassBench: Skewed traﬃc. Figure 12 shows the evaluation of\ntheearlyterminationimplementationonskewedpackettrac es.We\nreportthe throughputspeedupof /n.sc/m.sccompared to /c.sc/s.scand/t.sc/m.sc;the\nresultsfor /n.sc/c.scaresimilar tothoseof /c.sc/s.sc.\nWe perform 6000 experiments using 25 traces per rule-set: ﬁv e\ntraces per Zipf distribution plus ﬁve modiﬁed CAIDA traces. We\nevaluateovertwelve500Krule-setsandreportthegeometri cmean.\nAdditionally, we evaluate CAIDA traces in two settings. Fir st, the\n\n1 2 3 4 5 6 7 8 9 10 11 12 GM1 2 3 4 5 6 7 8 9 10 11 12 GM01234100K Classiﬁers 500K ClassiﬁersThroughput\nSpeedup\nNuevoMatch w/CutSplit NuevoMatch w/NeuroCuts NuevoMatch w/TupleMerge\nFigure9: ClassBench:NuevoMatch vs. CutSplit,NeuroCuts, andTupleMerge,using asingle CPUcore.\n10310410510623456·106\nL2Size (1MB) L1Size (32KB)\n14.85:15.6\n25%\n192.0:192.8\n6%2.9:21.3\n99%7.9:46.1\n99%19.5KB\n205.0KB\n2MB\n10MB\nNumberof RulesThroughput (pps)\nTupleMerge NuevoMatch w/TupleMerge\nFigure 11: Throughput vs. number of rules for TupleMerge\nand NuevoMatch. Annotations are coverage (%) and index\nmemorysizeinKB (remainder: total).\nclassiﬁer runs with access to the entire 16MB of the L3 cache ( de-\nnoted as CAIDA). Second, the classiﬁer use of L3 is restricte d to\n1.5˙MB via Intel’s Cache Allocation Technology, emulating mult i-\ntenant setting(denoted as CAIDA*).\nNuevoMatch issigniﬁcantly fasterthan /c.sc/s.sc,butitsbeneﬁtsover\n/t.sc/m.scdiminish for workloads with higher skews. Yet, the speedups\nare morepronouncedunder smaller L3allocation.\nOverall, weobserve lower speedupsfortheskewed traﬃcthan\nfor the random trace. This is not surprising, as skewed trace s in-\nduce a higher cache hit rate for all the methods, which in turn\nreduces the performance gains of /n.sc/m.scover both /c.sc/s.scand/t.sc/m.sc, simi-\nlar to the case of small rule-sets. Nevertheless, it is worth noting\nthatclassiﬁcationalgorithmsareusuallyappliedalongsi decaching\nmechanismsthatcatchthepackets’temporallocality.Fori nstance,\nOpen vSwitch applies caching for most frequently used rules . It\ninvokes Tuple Space Search upon cache misses [30]. Therefor e, if\nNuevoMatchisappliedatthisstage,weexpectittoyieldthe perfor-\nmancegainsequivalenttothosereportedforunskewedworkl oads.\nOpen vSwitch integrationis thegoal ofourongoing work.\n5.2.1 Memory footprint comparison. Figure 13 compares the\nmemoryfootprintoftheclassiﬁerswithoutandwithNuevoMa tch\n(thetworight-mostbarsineachbarcluster).Weusethesame num-\nber of iSets as in the end-to-end experiments. Note that a sma ller\nfootprintalonedoes notnecessarily lead tohigher perform anceif\nmoreiSetsareused.Therefore,theresultsshouldbeconsid eredin\nconjunction withtheend-to-end performance.\nThe memory footprint includes only the index data structure s\nbut not the rules themselves. In particular, the memory foot printZipf 80%\n(α=1.05)Zipf 85%\n(α=1.10)Zipf 90%\n(α=1.15)Zipf 95%\n(α=1.25)CAIDA CAIDA*0.511.522.5\n2.06×\n1.14×1.95×\n1.06×1.84×\n0.99×1.62×\n0.89×1.79×\n1.05×2.26×\n1.16×Throughput\nSpeedup\nNuevoMatch w/CutSplit NuevoMatch w/TupleMerge\nFigure 12: ClassBench: NuevoMatch vs. CutSplit and Tuple-\nMergewith skewedtraﬃc.\nforNuevoMatchincludesboththeRQ-RMImodelsandtheremai n-\nderclassiﬁer.Eachbaris theaverage ofallthe12applicati onrule-\nsets ofthesamesize.\nFor/n.sc/m.scweshowboththeremainderindexsize(middlebar)and\nthe total RQ-RMI size (right-most bar). Note that due to the l oga-\nrithmicscaleoftheYaxis,theactualratiobewteenthetwoi smuch\nhigherthanitmightseem.Forexample,theremainder for10K /t.sc/m.sc\nisalmost100×thesizeoftheRQ-RMI.Notealsothatsincewerun\n/n.sc/m.scon two cores, both RQ-RMI and the remainder classiﬁer use\ntheir ownCPUcaches.\nOverall, NuevoMatch enables dramatic compression of the\nmemory footprint, in particular for 500K rule-sets, with 4. 9×, 8×,\nand 82×onaverage over /c.sc/s.sc,/n.sc/c.scand/t.sc/m.screspectively.\nThegraphexplainswelltheend-to-endperformanceresults .For\n1K rule-sets, the original classiﬁers ﬁt into the L1 cache, s o/n.sc/m.scis\nnot eﬀective. For 10K sets, even though the remainder index ﬁ ts\nin L1, the ratio between L1 and L2 performance is insuﬃcient t o\ncovertheRQ-RMI overheads.For100K,thesituationissimil arfor\n/c.sc/s.sc;however, for /n.sc/c.sc,theremainder ﬁts in L1,whereas theoriginal\n/n.sc/c.scspills to L3. For /t.sc/m.sc, the remainder is already in L2, yielding a\nloweroverallspeedupcomparedto /n.sc/c.sc.Last,for500Krule-sets,all\nthe original classiﬁers spill to L3, whereas the remainder ﬁ ts well\ninL2,yielding clear performanceimprovements.\nPerformance under L3 cache contention. The small memory\nfootprintof /n.sc/m.scplays animportantroleeven when therule-index\nﬁtsintheL3cache(16MBinourmachine). L3issharedamongal l\ntheCPUcores;therefore,cachecontentionisnotrare,inpa rticular\nin data centers. /n.sc/m.screduces the eﬀects of L3 cache contention on\npacket classiﬁcation performance. In the experiment we use the\n500˙Krule-set(1)andcomparetheperformanceof /c.sc/s.scand/n.sc/m.sc(with\n/c.sc/s.sc)whilelimitingtheL3to1.5MB. /c.sc/s.scloseshalfofitsperformance,\nwhereas/n.sc/m.scslows downby30%,increasing theoriginal speedup.\n\n1K 10K 100K 500K102103104105106107\n32KBL1Cache Size1024KBL2Cache Size\nNumber of rulesSize (Bytes)\nNuevoMatch: Remainder CutSplit NeuroCuts\nNuevoMatch: iSets TupleMerge\nFigure 13: Memory size for CutSplit, NeuroCuts, Tuple-\nMerge vs. NuevoMatch with them indexing the remainder.\nEach bar isa geometricmeanof 12 applications.\nTable 2: iSetcoverage.\n1iSet 2iSets 3 iSets 4iSets\n1K20.2±18.6 28.9±22.3 34.6±25.6 38.7±27.2\n10K45.1±31.6 59.6±38.9 62.6±37.1 65.1±35.7\n100K 80.0±14.5 96.5±8.3 98.1±4.8 98.8±2.7\n500K 84.2±10.5 98.8±1.5 99.4±0.6 99.7±0.2\n183,376 57.8 91.6 96.5 98.2\n0 1 2 3 4 5 602004006008001,000\nNumber ofiSetsTime (ns)\nCoverage(%)\nRemainder Secondary Search Validation\nInference Coverage\nFigure 14: Coverage and execution time breakdown of\nNuevoMatch vs.varyingnumberof iSets.\n5.3 Performanceanalysis\n5.3.1 iSet coverage. Table 2 shows the cumulative coverage\nachievedwithupto4iSetsaveragedover12rule-sets(Class Bench)\nof thesame size. The coverage of smaller rule-sets is worseo nav-\nerage, butimproves withthesizeof therule-set.\nThelastrowshowsarepresentativeresultfortheStanfordb ack-\nbone rule-set (the other three diﬀer within 1%). Two iSets ar e\nenoughtoachieve90%coverageandthreeareneededfor95%.T his\ndata set diﬀers from ClassBench in that it contains only one ﬁ eld,\nproviding fewer opportunitiesforiSet partitioning.\n5.3.2 ImpactofthenumberofiSets. Weseektounderstandthe\ntradeoﬀbetweentheiSetcoverageoftherule-setandthecom puta-\ntional overheads of adding more RQ-RMI. All computations we re\nperformed on a single core to obtain the latency breakdown. W e\nuse/c.sc/s.scfor indexing theremainder.Table 3: Throughput and a single iSet coverage vs. the frac-\ntionof low-diversity rulesina 500K rule-set.\n% Lowdiversityrules % Coverage Speedup (throughput)\n70% 25% 1.07 ×\n50% 50% 1.14 ×\n30% 70% 1.60 ×\nFigure 14 shows the geometric mean of the coverage and the\nruntime breakdown over 12 rule-sets of 500K. The breakdown\nincludes the runtime of the remainder classiﬁer, validatio n, sec-\nondary search, and RQ-RMI inference. Zero iSets implies tha t/c.sc/s.sc\nwas run alone. Adding more iSets shows diminishing returns b e-\ncauseoftheircomputeoverhead,whichisnotcompensatedby the\nremainder runtimeimprovements becausethecoverageis alr eady\nsaturated to almost 100%. Using one or two iSets shows the bes t\ntrade-oﬀ. /n.sc/c.scshows similar results.\n/t.sc/m.scbehaved diﬀerently (not shown). /t.sc/m.scoccupies much more\nmemorythan /c.sc/s.sc;therefore,usingmoreiSetstoachievehighercov-\nerageallowedustofurtherspeeduptheremainderbyﬁttingi tinto\nanupperlevel cache. Thus,4 iSets showed thebest conﬁgurat ion.\nWenotethattheruntimeissplitnearlyequallybetweenmode l\ninference and validation (which are compute-bound parts), and\nthesecondary search and the remainder computations(which are\nmemory-bound). We expect the compute performance of future\nprocessors to scale better than their cache capacity and mem ory\naccess latency. Therefore, we believe /n.sc/m.scwill provide better scal-\ningthan memory-boundstate-of-the-art classiﬁers.\n5.3.3 Partitioning eﬀectiveness. We seek to understand how\nlowdiversityrule-setsaﬀectNuevoMatch.Toanalyzethat, wesyn-\nthetically generated a large rule-set as a Cartesian produc t of a\nsmallnumberofvaluesperﬁeld(noranges).Weblendedthemi nto\na500KClassBenchrule-set,replacingrandomlyselectedru leswith\nthosefromtheCartesianproduct,whilekeeping thetotalnu mber\nofrules thesame.\nTable 3 shows the coverage and the speedup over /t.sc/m.scon the\nresulting mixed rule-sets for diﬀerent fractions of low-di versity\nrules. The partitioning algorithm successfully segregate s the low-\ndiversity rules the best, achieving the coverage inversely propor-\ntional to their fraction in the rule-set. Note that NuevoMat ch be-\ncomeseﬀectivewhenitoﬄoadstheprocessingofabout25%oft he\nrules.\n5.3.4 Training time and secondary search range. RQ-RMIs are\ntrained to minimize the predictionerror boundto achieve a s mall\nsecondarysearchdistance.Recallthatasecondarysearchi nvolves\na binary search within the error bound, where each rule is val i-\ndatedtomatchalltheﬁelds.\nThetradeoﬀbetweentrainingtimeandsecondarysearchperf or-\nmance is not trivial. A larger search distance enables faste r train-\ning but slows down the secondary search. A smaller search dis -\ntanceresults in a faster search butslows down thetraining. In ex-\ntreme cases, the training does not converge, since a higher p reci-\nsionmightrequirelargersubmodels.However,increasing t hesize\nof the submodels leads to a larger memory footprint and longe r\ncomputations.\n\n64 128 256 512 1024010203040\nSearch DistanceBoundsTraining\nTime\n(minutes)500Krules\n100Krules\n10Krules\nFigure 15: RQ-RMI training time in minutes vs. maximum\nsearch rangebound.\nFigure 15 shows the average end-to-end training time in min-\nutes of 500 models as a function of the secondary search dista nce\nand the rule-set size. The measurements include all trainin g itera-\ntions as described in §3.5. As mentioned (§4), our training i mple-\nmentation can be dramatically accelerated, so the results h ere in-\ndicatethegeneral trend.\nTrainingwiththeboundof64isexpensive,butisitreallyne ces-\nsary?Toanswer,weevaluatetheperformanceimpactofthese arch\ndistance on the secondary search time. We measure 40 ˙ns for re-\ntrievingarulewithapreciseprediction(nosearch).For64 ,128and\n256 distances thesearch timevaries between 75 to 80 ˙ns thanks to\nthebinarysearch.Last,itturnsoutthatthe actualsearch distance\nfromthepredictedindexisoftenmuchsmallerthantheworst -case\noneenforcedintraining.Ouranalysisshowsthatinpractic e,train-\ningwitharelativelylargeboundof128leadsto80%oftheloo kups\nwitha search distanceof 64,and 60%with 32.\nWe conclude that training with larger bounds is likely to hav e\naminoreﬀectontheend-to-endperformance,butsigniﬁcant lyac-\ncelerate training. This property is important to support mo re fre-\nquent retraining and faster updates(§3.9).\n5.3.5 Performancewithmorefields. Addingﬁeldstoanexisting\nclassiﬁer will not harm its coverage, so it will not aﬀect the RQ-\nRMIperformance.Nonetheless,moreﬁeldswillincreaseval idation\ntime.\nUnfortunately,wedidnotﬁndpublicrule-setsthathaveala rge\nnumber of ﬁelds. Thus, we ran a microbenchmark by increasing\nthe number of ﬁelds and measuring the validation stage perfo r-\nmance. As expected,we observed almost linear growth in the v al-\nidationtime, from 25nsfor oneﬁeldto180ns for40ﬁelds.\n6 RELATED WORK\nHardware-based classiﬁers. Hardware-based solutions for clas-\nsiﬁcationsuchasTCAMsandFPGAsachieveaveryhighthrough -\nput [6, 35]. Consequently, many software algorithms take ad -\nvantage of them, further improving classiﬁcation performa nce\n[13, 20, 23, 24, 28, 32, 37]. Our work is complementary, but ca n\nbe used to improve scaling of these solutions. For example, i f the\noriginal classiﬁer requiredlargeTCAMs,theremainder set would\nﬁta much smallerTCAM.\nGPUsforclassiﬁcation. AcceleratingclassiﬁcationonGPUswas\nsuggested by numerous works. PacketShader [10] uses GPU for\npacket forwarding and provides integration with Open vSwit ch.\nHowever,packetforwardingisasingle-dimensional proble m,soitiseasierthanmulti-ﬁeldclassiﬁcation[9].Varvelloetal .[42]imple-\nmented various packet classiﬁcation algorithms in GPUs, in clud-\ninglinear search,TupleSpaceSearch,andbloomsearch.Non ethe-\nless, these techniques suﬀer from poor scalability for larg e classi-\nﬁerswith wildcardrules,which NuevoMatch aims toalleviat e.\nMLtechniquesfornetworking. RecentworkssuggestusingML\ntechniquesfor solvingnetworking problems,such asTCPcon ges-\ntioncontrol[4,12,45],resourcemanagement [25],quality ofexpe-\nrience in video streaming [26, 43], routing [40], and decisi on tree\noptimizationforpacketclassiﬁcation[22].NuevoMatchis diﬀerent\nin that it uses an ML technique for building space-eﬃcient re pre-\nsentations of therulesthatﬁtin theCPUcache.\n7 CONCLUSIONS\nWehavepresentedNuevoMatch,theﬁrstpacketclassiﬁcatio ntech-\nnique that uses Range-Query RMI machine learning model for ac-\ncelerating packetclassiﬁcation.Wehaveshownaneﬃcientwayof\ntrainingRQ-RMI models,makingthem learnthematchingrang es\nof large rule-sets, via sampling and analytical error bound com-\nputations. We demonstrated the application of RQ-RMI to mul ti-\nﬁeldpacketclassiﬁcationusingrule-setpartitioning.We evaluated\nNuevoMatch onsynthetic and real-world rule-sets and conﬁr med\nitsbeneﬁts for largerule-sets over state-of-the-art tech niques.\nNuevoMatch introduces a new point in the design space of\npacket classiﬁcation algorithms and opens up new ways to sca le\nit on commodity processors. We believe that its compute-bou nd\nnature and the use of neural networks will enable further sca ling\nwithfutureCPUgenerations,whichwillfeaturepowerfulco mpute\ncapabilities targeting faster execution of neural network -related\ncomputations.\n8 ACKNOWLEDGEMENTS\nWethanktheanonymousreviewersofSIGCOMM’20andourshep-\nherd Minlan Yu for their helpful comments and feedback. We\nwouldalsoliketothankIsaacKeslassyandLeonidRyzhykfor their\nfeedbackontheearlydraft ofthepaper.\nThiswork was partiallysupportedbytheTechnionHiroshi Fu -\njiwaraCyberSecurityResearchCenterandtheIsraelNation alCy-\nber Directorate, by the Alon fellowship and by the Taub Famil y\nFoundation. We gratefully acknowledge support from Israel Sci-\nenceFoundation(Grant1027/18)andIsraeliInnovationAut hority.\nREFERENCES\n[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, A ndy Davis, Jeﬀrey\nDean, Matthieu Devin, SanjayGhemawat, GeoﬀreyIrving, Mic hael Isard,Man-\njunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, D erek G. Murray,\nBenoitSteiner,PaulTucker,VijayVasudevan,PeteWarden, MartinWicke,Yuan\nYu,andXiaoqiangZheng.2016. TensorFlow:ASystemforLarg e-ScaleMachine\nLearning.In USENIXOSDI .\n[2] CAIDA. [n.d.]. The CAIDA UCSD Anonymized Internet Traces 2019 . Retrieved\nJune 15,2020 fromhttp://www.caida.org/data/passive/pa ssive_dataset.xml\n[3] JamesDaly,ValerioBruschi,LeonardoLinguaglossa,Sa lvatorePontarelli,Dario\nRossi, Jerome Tollet, Eric Torng, and Andrew Yourtchenko. 2 019. TupleMerge:\nFast Software Packet Processing for online Packet Classiﬁc ation.IEEE/ACM\nTransactionson Networking(TON) 27,4 (2019), 1417–1431.\n[4] Mo Dong, Tong Meng, DoronZarchy,Engin Arslan, YossiGil ad, Brighten God-\nfrey,andMichaelSchapira.2018.PCCVivace:Online-Learn ingCongestionCon-\ntrol. InUSENIXNSDI .\n[5] Daniel Firestone. 2017. VFP: A Virtual Switch Platform f or Host SDN in the\nPublic Cloud.In USENIXNSDI .\n\n[6] Daniel Firestone, Andrew Putnam, Sambrama Mundkur, Der ek Chiou, Alireza\nDabagh, Mike Andrewartha, Hari Angepat, Vivek Bhanu, Adria n M. Caulﬁeld,\nEric S. Chung, Harish Kumar Chandrappa, Somesh Chaturmohta , Matt\nHumphrey,JackLavier,NormanLam,FengfenLiu,KalinOvtch arov,JituPadhye,\nGautham Popuri, ShacharRaindel, TejasSapre, MarkShaw, Ga brielSilva, Mad-\nhan Sivakumar,Nisheeth Srivastava,Anshuman Verma, Qasim Zuhair,Deepak\nBansal, Doug Burger, KushagraVaid,David A. Maltz, and Albe rt G. Greenberg.\n2018.AzureAcceleratedNetworking:SmartNICsinthePubli cCloud.In USENIX\nNSDI.\n[7] PankajGuptaandNickMcKeown.1999. PacketClassiﬁcati ononMultipleFields.\nInACMSIGCOMM .\n[8] Pankaj Gupta and Nick McKeown. 2000. Classifying Packet s with Hierarchical\nIntelligent Cuttings. IEEEMicro 20, 1(2000), 34–41.\n[9] Pankaj Gupta and Nick McKeown. 2001. Algorithms for Pack et Classiﬁcation.\nIEEENetwork 15, 2(2001), 24–32.\n[10] Sangjin Han, Keon Jang, KyoungSoo Park, and Sue Moon. 20 10. PacketShader:\nA GPU-acceleratedsoftwarerouter.In ACMSIGCOMM .\n[11] Intel. 2019. Intel Nervana Neural Network Processors . Retrieved September 25,\n2019 fromhttps://www.intel.ai/nervana-nnp/\n[12] Nathan Jay, Noga H. Rotman, Philip Brighten Godfrey, Mi chael Schapira, and\nAviv Tamar.2018. Internet Congestion Control viaDeep Rein forcement Learn-\ning.arXiv preprint arXiv:1810.03259 (2018).\n[13] Naga Praveen Katta, Omid Alipourfard, Jennifer Rexfor d, and David Walker.\n2016. CacheFlow: Dependency-Aware Rule-Caching for Softw are-Deﬁned Net-\nworks.In ACMSOSR .\n[14] Diederik P Kingma and Jimmy Ba. 2014. Adam: A Method for S tochastic Opti-\nmization. arXiv:1412.6980 (2014).\n[15] Jon M. Kleinberg and Éva Tardos. 2006. Algorithm Design . Addison-Wesley,\n116–125.\n[16] KirillKogan,SergeyNikolenko,OriRottenstreich,Wi lliamCulhane,andPatrick\nEugster.2014. SAX-PAC(Scalableandexpressivepacketcla ssiﬁcation).In ACM\nSIGCOMM .\n[17] Tim Kraska, Mohammad Alizadeh, Alex Beutel, Ed H. Chi, J ialin Ding, Ani\nKristo, Guillaume Leclerc, Samuel Madden, Hongzi Mao, and V ikram Nathan.\n2019. SageDB:A LearnedDatabaseSystem.\n[18] Tim Kraska, Alex Beutel, Ed H. Chi, Jeﬀrey Dean, and Neok lis Polyzotis. 2018.\nThe CaseforLearned Index Structures.In ACMSIGMOD .\n[19] Habana Labs.2019. Habana AI Processors . Retrieved September 25, 2019 from\nhttps://habana.ai/product\n[20] Karthik Lakshminarayanan, Anand Rangarajan, and Srin ivasan Venkatachary.\n2005. Algorithms for Advanced Packet Classiﬁcation with Te rnary CAMs. In\nACMSIGCOMM .\n[21] Wenjun Li, Xianfeng Li, Hui Li, and Gaogang Xie. 2018. Cu tSplit: A Decision-\nTreeCombiningCuttingandSplittingforScalablePacketCl assiﬁcation.In IEEE\nINFOCOM .\n[22] EricLiang,HangZhu,XinJin,andIonStoica.2019. Neur alPacketClassiﬁcation.\nInACMSIGCOMM .\n[23] Alex X Liu, Chad R Meiners, and Yun Zhou. 2008. All-Match Based Complete\nRedundancy Removalfor PacketClassiﬁersinTCAMs.In IEEEINFOCOM .\n[24] Yadi Ma and Suman Banerjee. 2012. A Smart Pre-classiﬁer to Reduce Power\nConsumption of TCAMs for Multi-dimensional Packet Classiﬁ cation. In ACM\nSIGCOMM .\n[25] Hongzi Mao,MohammadAlizadeh,IshaiMenache,andSrik anthKandula.2016.\nResourceManagement with Deep Reinforcement Learning. In ACM SIGCOMM\nHotNetsWorkshop .\n[26] Hongzi Mao, Ravi Netravali, and Mohammad Alizadeh. 201 7. Neural Adaptive\nVideo Streamingwith Pensieve.In ACMSIGCOMM .\n[27] Nick McKeown, Tom Anderson, Hari Balakrishnan, Guru Pa rulkar, Larry Pe-\nterson, Jennifer Rexford, Scott Shenker, and Jonathan Turn er.2008. OpenFlow:\nEnabling Innovation in Campus Networks. ACM SIGCOMM CCR 38, 2 (2008),\n69–74.\n[28] NinaNarodytska,Leonid Ryzhyk,IgorGanichev,andSon er Sevinc.2019. BDD-\nBased Algorithms for Packet Classiﬁcation. In Formal Methods in Computer\nAided DesignFMCAD .\n[29] Nvidia. 2019. Nvidia Deep Learning Inference Plat-\nform. Retrieved September 25, 2019 from\nhttps://www.nvidia.com/en-us/deep-learning-ai/solut ions/inference-platform/\n[30] BenPfaﬀ,JustinPettit,TeemuKoponen, EthanJackson, Andy Zhou,JarnoRaja-\nhalme, Jesse Gross, Alex Wang, Joe Stringer, Pravin Shelar, Keith Amidon, and\nMartin Casado. 2015. The Design and Implementation of Open v Switch. In\nUSENIXNSDI .\n[31] Alon Rashelbach.2020. NeuvoMatch source code . Retrieved June 21, 2020 from\nhttps://github.com/acsl-technion/nuevomatch\n[32] Ori Rottenstreich and János Tapolcai. 2015. Lossy Comp ressionof Packet Clas-\nsiﬁers.In ACM/IEEEANCS .\n[33] Nadi Sarrar,Steve Uhlig, Anja Feldmann, Rob Sherwood, and Xin Huang. 2012.\nLeveraging Zipf’s law for traﬃc oﬄoading. Computer Communication Review42,1 (2012), 16–22.\n[34] Sumeet Singh, Florin Baboescu, George Varghese, and Ji a Wang. 2003. Packet\nClassiﬁcationUsing Multidimensional Cutting. In ACMSIGCOMM .\n[35] EdSpitznagel,DavidETaylor,andJonathanSTurner.20 03. Packetclassiﬁcation\nusing extended TCAMs.In IEEEICNP .\n[36] VenkatacharySrinivasan,SubhashSuri,andGeorgeVar ghese.1999.PacketClas-\nsiﬁcationUsing TupleSpace Search.In ACMSIGCOMM .\n[37] DavidETaylor.2005.SurveyandTaxonomyofPacketClas siﬁcationTechniques.\nACMComputingSurveys (CSUR) 37, 3(2005), 238–275.\n[38] DavidE.TaylorandJonathanS.Turner.2005.Scalablep acketclassiﬁcationusing\ndistributed crossproducingofﬁeld labels.In IEEEINFOCOM .\n[39] DavidETaylorandJonathanSTurner.2007.Classbench: APacketClassiﬁcation\nBenchmark. IEEE/ACMTransactionsonNetworking(TON) 15,3(2007),499–511.\n[40] AsafValadarsky,MichaelSchapira,DafnaShahaf,and A vivTamar.2017. Learn-\ning to Route with Deep RL.In NIPS Deep ReinforcementLearning Symposium .\n[41] Balajee Vamanan,Gwendolyn Voskuilen, and T. N. Vijayk umar.2010. EﬃCuts:\nOptimizing Packet Classiﬁcation for Memory and Throughput . InACM SIG-\nCOMM.\n[42] Matteo Varvello,RafaelLaufer,Feixiong Zhang,andT. V.Lakshman.2016. Mul-\ntilayer Packet Classiﬁcationwith Graphics Processing Uni ts.IEEE/ACMTrans-\nactionson Networking(TON) 24,5 (2016), 2728–2741.\n[43] HyunhoYeo,YoungmokJung,JaehongKim,JinwooShin,an dDongsuHan.2018.\nNeuralAdaptive Content-awareInternet Video Delivery.In USENIXOSDI .\n[44] Sorrachai Yingchareonthawornchai, James Daly, Alex X . Liu, and Eric Torng.\n2018. ASorted-PartitioningApproachtoFastandScalableD ynamicPacketClas-\nsiﬁcation. IEEE/ACMTransactionsonNetworking(TON) 26,4(2018),1907–1920.\n[45] Yasir Zaki, Thomas Pötsch, Jay Chen, Lakshminarayanan Subramanian, and\nCarmelitaGörg. 2015. Adaptive Congestion Control for Unpr edictable Cellular\nNetworks.In ACMSIGCOMM .\n[46] Hongyi Zeng, Peyman Kazemian, George Varghese, and Nic k McKeown. 2012.\nAutomatic Test PacketGeneration. In ACMCoNEXT .\nAppendices are supporting material that has not been peer-\nreviewed.\nA RQ-RMICORRECTNESS\nA.1 Responsibilityofa submodel\nDenote the input domain of an RQ-RMI model as D⊂Rand its\nnumberof stages as n.\nT/h.sc/e.sc/o.sc/r.sc/e.sc/m.sc A.1 (R/e.sc/s.sc/p.sc/o.sc/n.sc/s.sc/i.sc/b.sc/i.sc/l.sc/i.sc/t.sc/y.sc T/h.sc/e.sc/o.sc/r.sc/e.sc/m.sc). Letsibe a trained\nstage such that i<n−1. The responsibilities of submodels in si+1\ncan be calculated by evaluating a ﬁnite set of inputs over the stage\nsi.\nThe intuition behind Theorem A.1 is based on Corollary 3.2,\nnamelythatsubmodelsoutputpiecewiselinearfunctions.P roving\nitrequires someadditional deﬁnitions.\nDeﬁnition A.2 (Stage Output). The outputof stage siis deﬁned\nforx∈DasSi(x)=Mi,fi(x)(x)wherefi(x)is the index of the\nsubmodelin sithatis responsiblefor input x, and deﬁned as\nfi(x)=/braceleftBigg\n0 i=0/floorleftbig\nSi−1(x)·Wi/floorrightbig\ni={1,2,...,n−1}\nDeﬁnitionA.3(SubmodelResponsibility). Theresponsibilityofa\nsubmodel mi,jis deﬁned as\nRi,j=/braceleftBigg\nD i=0/braceleftbig\nx/barex/barexfi(x)=j/bracerightbig\ni={1,2,...,n−1}\nNotethat theresponsibilities of every twosubmodels in the same\nstagearedisjoint.\nDeﬁnition A.4 (Left and Right Slopes). For a range R, if points\nminx∈Rxormaxx∈Rxaredeﬁned,werefertothemasthebound-\nariesoftherange.Forallotherpoints,werefertoasintern alpoints\nof the range. For a piecewise linear function deﬁned over som e\n\nxMi,j(x)\n1\n0.75\n0.5\n0.25\n0/afii10069.ital0/afii10069.ital1/afii10069.ital2/afii10069.ital3/afii10069.ital4/afii10069.ital5\n(a)Trigger Inputs Gi,jxMi,j(x),/floorleftbig\nMi,j(x)·Wi+1/floorrightbig\n1,4\n0.75,3\n0.5,2\n0.25,1\n0,0t0t1t2t3t4\n(b)Transition Inputs Ti,j\nFigure 16: Illustration of the trigger inputs ( /afii10069.ital0,...,/afii10069.ital5) and\ntransitioninputs( t0,...,t4)forgraph Mi,j(x)ofsubmodel mi,j.\nNote that Wi+1, namely the number of submodels in stage\ni+1,aﬀectsthetransition inputsof mi,jandequals 4.\nrangeR, for every internal point x∈R, there exists δ>0 such\nthat the function is linear in each of (x−δ,x),(x,x+δ). Accord-\ningly, we can refer to the left slope and the right slope of a po int,\ndeﬁned as thoseofthetwolinear functions.\nDeﬁnition A.5 (TriggerInputs). We say that an input /afii10069.ital∈Dis a\ntriggerinput ofa submodel mi,jifoneofthefollowingholds: (i)/afii10069.ital\nisaboundarypointof D(namely,/afii10069.ital=min/y.alt∈D/y.altor/afii10069.ital=max/y.alt∈D/y.alt).\n(ii)/afii10069.italisaninternalpointof Dandtheleftandrightslopesof Mi,j(/afii10069.ital)\ndiﬀer.\nDeﬁnitionA.6(TransitionInputs). Wesaythataninput t∈Disa\ntransitioninput ofasubmodel mi,jifitchangessubmodelselection\ninthefollowingstage.Formally,thereexists ϵ>0suchthatforall\n0<δ<ϵ:\n/floorleftbig\nMi,j(t−δ)·Wi+1/floorrightbig\n/nequal/floorleftbig\nMi,j(t+δ)·Wi+1/floorrightbig\nDeﬁnition A.7 (The function Bi(x)).We deﬁne the function Bi\nfori∈{0,1,...,n−1}.Biisastaircasefunctionofvalues [0,Wi+1−\n1],and deﬁned as Bi(x)=⌊x·Wi+1⌋forx∈[0,1).\nForasubmodel mi,j,wetermthesetofitstriggerinputsas Gi,j\nandthesetofitstransitioninputsas Ti,j.SeeFigure16forillustra-\ntion. From submodeldeﬁnition and Corollary3.2, we can tell that\na submodel’s ReLU operations determine its trigger inputs. Con-\nsequently, any set of trigger inputs is ﬁnite and can be calcu lated\nusing a few linear equations. Nonetheless, calculating the transi-\ntion inputs of a submodel is not straightforward. We show a fa st\nand eﬃcient wayfor doingsoin thefollowinglemma:\nL/e.sc/m.sc/m.sc/a.sc A.8. Letmi,jbe an RQ-RMI submodel,and a<b∈Gi,j\ntwo adjacent trigger inputs of mi,j. Then the set S=[a,b]∩Ti,jis\nﬁniteand canbecalculated usingtheinputs aandbalone.\nP/r.sc/o.sc/o.sc/f.sc.Wedividetheconstructionof Stotwosubsets S=S0∪\nS1. First we handle S0. For each x∈{a,b},x∈S0if and only if\nthere exists ϵ>0 suchthat forall0 <δ<ϵ:\nBi/parenleftbigMi,j(x−δ)/parenrightbig/nequalBi/parenleftbigMi,j(x+δ)/parenrightbig\nNow to S1. Without loss of generality, Mi,j(a) ≤Mi,j(b).\nFrom Corollary 3.2 and Deﬁnition A.5, Mi,jis linear in[a,b]. If\nBi(Mi,j(a))=Bi(Mi,j(b)), thenS1=∅. Otherwise, Mi,j(a)/nequal\nMi,j(b).Bi(x)outputs discrete values between Bi(Mi,j(a))andBi(Mi,j(b))for allx∈(a,b). Denote this ﬁnite set of discrete val-\nues asM. For any /y.alt∈Mthere exists a value d∈(a,b]such that\nMi,j(d)·Wi+1=/y.alt.Bythelinearityof Mi,jin[a,b]:\nd=/parenleftBig/y.alt\nWi+1−Mi,j(a)/parenrightBig\n·b−a\nMi,j(b)−Mi,j(a)+a\nWeconstruct S1asfollows:\nS1=/braceleftbigg/parenleftBig/y.alt\nWi+1−Mi,j(a)/parenrightBig\n·b−a\nMi,j(b)−Mi,j(a)+a/barex/barex/barex∀/y.alt∈M/bracerightbigg\n/square\nC/o.sc/r.sc/o.sc/l.sc/l.sc/a.sc/r.sc/y.scA.9. Thesetoftransitioninputs Ti,jcanbecalculated\nusingGi,janditssizeisboundedsuch that |Ti,j|≤Wi+1·|Gi,j|.\nNot all transition inputs of all submodels are reachable, as\nsome exist outside of their corresponding submodel’s respo nsibil-\nity.Therefore, wedeﬁnethesetofreachabletransitioninp utsofa\nstagesias thetransitionset of astage:\nDeﬁnition A.10 (Transition Set). Thetransition set Uiof a stage\nsiis anordered set,deﬁned as:\nUi={min(D)}∪{Wi−1/uniondisplay.1\nj=0Ti,j∩Ri,j}∪{max(D)}\nThe proof of Theorem A.1 directly follows from the next two\nlemmas:\nL/e.sc/m.sc/m.sc/a.scA.11. Letsi,si+1be two adjacent stages. For any two ad-\njacent values u0<u1∈Uithere exists a submodel mi+1,jsuch that\nSi+1(x)ispiecewiselinearandequalto Mi+1,j(x)forallx∈(u0,u1).\nP/r.sc/o.sc/o.sc/f.sc.We show that there exists a submodel mi+1,jsuch that\nanyx∈(u0,u1)satisﬁesx∈Ri+1,j,which implies fi+1(x)=jand\nsoSi+1(x)=Mi+1,j(x). By Corollary 3.2, Si+1is piecewise linear\nforallx∈(u0,u1).\nLetx</y.alt∈ (u0,u1). Assume by contradiction there exist\ntwo submodels mi+1,j0andmi+1,j1such that x∈Ri+1,j0and\n/y.alt∈Ri+1,j1. From Deﬁnition A.3, fi+1(x)/nequalfi+1(/y.alt)implies\nBi(Si(x))/nequalBi(Si(/y.alt)). Thus, there exists an input z∈(x,/y.alt]and\nϵ>0such thatforall 0 <δ<ϵ:\nBi/parenleftbigSi(z−δ)/parenrightbig/nequalBi/parenleftbigSi(z+δ)/parenrightbig\nSinceSiconsists of the outputs of submodels in si, there exists a\nsubmodel mi,ksuch that Si(z)=Mi,k(z). Therefore, z∈Ti,kand\nz∈Ri,k, which means z∈Ui, in contradiction to deﬁnition of u0\nandu1. /square\nL/e.sc/m.sc/m.sc/a.scA.12. LetsibeanRQ-RMIstagesuchthat i∈{0,1,...,n−\n2}.Thefunction fi+1deﬁnedoverthespace Dcanbecalculatedusing\ntheinputs UioverSi.\nP/r.sc/o.sc/o.sc/f.sc.Letu0<u1∈Uibe two adjacent values. By Lemma\nA.11thereexistsasubmodel mi+1,jsuchthat Si+1(x)=Mi,j(x)for\nallx∈(u0,u1).FromDeﬁnitionA.2, fi+1(x)=jforallx∈(u0,u1).\nBy calculating Bi(Si(u0))andBi(Si(u1)),fi+1(x)is known for all\nx∈ [u0,u1]. Since min{D} ∈Uiandmax{D} ∈Ui,fi+1(x)is\nknown forall x∈D. /square\n\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 240123456\n1KRules 10KRulesLatency\nSpeedup\nNuevoMatchw/ CutSplit NuevoMatchw/ TupleMerge1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 2400.511.52\n1KRules 10KRules\nThroughput\nSpeedup\nFigure17:Adetailedversionofend-to-endperformancefor smallrule-sets.SpeedupinthroughputandlatencyofNuevoMatch\nagainststand-aloneversionsof CutSplitand TupleMerge.C lassiﬁerswith no validiSetsarenot displayed.\nA.2 Submodel prediction error\nT/h.sc/e.sc/o.sc/r.sc/e.sc/m.scA.13(S/u.sc/b.sc/m.sc/o.sc/d.sc/e.sc/l.scP/r.sc/e.sc/d.sc/i.sc/c.sc/t.sc/i.sc/o.sc/n.scE/r.sc/r.sc/o.sc/r.sc). Letsn−1bethe\nlaststageofanRQ-RMImodel.Themaximumpredictionerroro fany\nsubmodel in sn−1can be calculated using a ﬁnite set of inputs over\nthe stagesn−1.\nTheintuitionbehindTheoremA.13istoaddressthesetofran ge-\nvaluepairs as anadditional,virtual,stagein themodel.\nDeﬁnition A.14 (Range-Value Pair). A range-value pair /a\\}⌊ra⌋ketle{tr,/v.alt/a\\}⌊ra⌋ketri}htis\ndeﬁnedsuchthat risanintervalin Dand/v.alt∈{0,1,2,...}isunique\ntothat pair.\nWetermWnthenumberofrange-valuepairsanRQ-RMImodel\nshouldindex.Similartothedeﬁnitionsforsubmodels,weex tendfi\nsuchthat fn(x)=⌊Sn−1(x)·Wn⌋,andsaythattheresponsibility Rp\nofapairp=/a\\}⌊ra⌋ketle{tr,/v.alt/a\\}⌊ra⌋ketri}htisthesetofinputs{x|fn(x)=/v.alt}.Consequently,\nwe make the following two observations. First, all inputs in the\nranger\\Rpshouldhave reached pbutdid not.Second, allinputs\nin therange Rp\\rdidreach pbutshouldnot.\nDeﬁnition A.15 (Misclassiﬁed Pair Set). Letmbe a submodel in\nsn−1witharesponsibility Rm.DenotePmasthesetofallpairssuch\nthatapair p=/a\\}⌊ra⌋ketle{tr,/v.alt/a\\}⌊ra⌋ketri}ht∈Pmsatisﬁes(r\\Rp)∪(Rp\\r)∩Rm/nequal∅. In\nother words, Pmholds all pairs that were misclassiﬁed by m, and\ntermed the misclassiﬁedpairset ofm.\nDeﬁnition A.16 (Maximum Prediction Error). Letmbe a sub-\nmodel in sn−1with a responsibility Rmand a misclassiﬁed pair\nsetPm.Themaximumpredictionerror of mis deﬁned as:\nmax/braceleftbig\n|fn(x)−/v.alt|/barex/barex/a\\}⌊ra⌋ketle{tr,/v.alt/a\\}⌊ra⌋ketri}ht∈Pm,x∈Rm/bracerightbig\nL/e.sc/m.sc/m.sc/a.sc A.17. The misclassiﬁedpairsets of all submodelsin sn−1\ncan becalculatedusing Un−1overSn−1.\nP/r.sc/o.sc/o.sc/f.sc.Letq0<q1be two adjacent values in Un−1. From\nLemma A.11 there exists a single submodel mn−1,j,j∈Wn−1s.t\nSn−1(x)=Mn−1,j(x)forallx∈(q0,q1).Hence,usingCorollary3.2,\nSn−1is linear in(q0,q1). Therefore, the values of Sn−1in[q0,q1]\ncan be calculated using q0andq1alone. Consequently, according\nto the deﬁnitions of fnand the responsibility of a pair, the set of\npairsPjwithresponsibilitiesin [q0,q1]canalsobecalculatedusingq0andq1.Calculatingtheresponsibilitiesofallpairsisperforme d\nbyrepeatingtheprocess forany twoadjacent pointsin Un−1.\nAt this point, as we know Rpfor allp=/a\\}⌊ra⌋ketle{tr,/v.alt/a\\}⌊ra⌋ketri}ht, calculating the\nset(r\\Rp)∪(Rp\\r)istrivial.Acquiringtheresponsibilityofany\nsubmodel in sn−1using Theorem A.1 enables us to calculate its\nmisclassiﬁed pairset immediately. /square\nProof of TheoremA.13\nP/r.sc/o.sc/o.sc/f.sc.Letmbe a submodel in sn−1with a responsibility Rm.\nForsimplicity,weaddressthecasewhere Rmisacontinuousrange.\nExtensiontothegeneralcaseispossiblebyrepeatingthepr ooffor\nanycontinuous range in Rm.\nDenotethesubmodel’sﬁnitesetoftriggerinputsas Gm.Deﬁne\nthesetQas follows:\nQ=minRm∪(Gm∩Rm)∪maxRm\nLetq0<q1be twoadjacent values in Q.From the deﬁnition of\ntrigger inputs, moutputs a linear function in [q0,q1]. Hence, the\nsetofvalues S0={fn(x)|x∈[q0,q1]}canbecalculatedusingonly\nq0andq1overSn−1. From Lemma A.17, the misclassiﬁed pair set\nPmcan becalculatedusingtheﬁniteset Un−1.Denote theset\nˆP0={/a\\}⌊ra⌋ketle{tr,/v.alt/a\\}⌊ra⌋ketri}ht|/a\\}⌊ra⌋ketle{tr,/v.alt/a\\}⌊ra⌋ketri}ht∈Pm,r∩[q0,q1]/nequal∅}\nCalculating max{s−/v.alt|s∈S0,/a\\}⌊ra⌋ketle{tr,/v.alt/a\\}⌊ra⌋ketri}ht∈ˆP0}yields the maximum\nerror ofmin[q0,q1]. Repeating the process for any two adjacent\npointsin Qyields themaximum errorof mfor allRm./square\nRule-setnames inFigures8and17,byorder:ACL1,ACL2,ACL3,\nACL4, ACL5, FW1,FW2,FW3,FW4,FW5,IPC1,IPC2.\nTable 4: RQ-RMI conﬁgurations for diﬀerent input rule-set\nsizes.\n#Rules #Stages Stage Widths\nLessthan 1032 [1,4]\n103to1043 [1,4,16]\n104to1053 [1,4,128]\nMorethan 1053 [1,8,256]or [1,8,512]",
  "textLength": 81298
}