{
  "paperId": "a82a1f77923c8d8659ac2bc9c00689e61df2d8e5",
  "title": "No DBA? No Regret! Multi-Armed Bandits for Index Tuning of Analytical and HTAP Workloads With Provable Guarantees",
  "pdfPath": "a82a1f77923c8d8659ac2bc9c00689e61df2d8e5.pdf",
  "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1\nNo DBA? No regret! Multi-armed bandits for\nindex tuning of analytical and HTAP workloads\nwith provable guarantees\nR. Malinga Perera, Bastian Oetomo, Benjamin I. P . Rubinstein, and Renata Borovica-Gajic\nAbstract —Automating physical database design has remained a long-term interest in database research due to substantial\nperformance gains afforded by optimised structures. Despite significant progress, a majority of today’s commercial solutions are highly\nmanual, requiring offline invocation by database administrators (DBAs). This status quo is untenable: identifying representative static\nworkloads is no longer realistic; and physical design tools remain susceptible to the query optimiser’s cost misestimates. Furthermore,\nmodern application environments like hybrid transactional and analytical processing (HTAP) systems render analytical modelling next\nto impossible. We propose a self-driving approach to online index selection that does not depend on the DBA and query optimiser, and\ninstead learns the benefits of viable structures through strategic exploration and direct performance observation. We view the problem\nas one of sequential decision making under uncertainty, specifically within the bandit learning setting. Multi-armed bandits balance\nexploration and exploitation to provably guarantee average performance that converges to policies that are optimal with perfect\nhindsight. Our comprehensive empirical evaluation against a state-of-the-art commercial tuning tool demonstrates up to 75% speed-up\nin analytical processing environments and 59% speed-up in HTAP environments. Lastly, our bandit framework outperforms a Monte\nCarlo tree search (MCTS)-based database optimiser, providing up to 24% speed-up.\nIndex Terms —Physical Design Tuning, Index Tuning, HTAP , Multi-Armed Bandits, Reinforcement Learning.\n✦\n1 I NTRODUCTION\nWith the growing complexity and variability of database\napplications and their hosting platforms (e.g., multi-tenant\ncloud environments), automated physical design tuning,\nparticularly automated index selection, has re-emerged as a\ncontemporary challenge for database management systems.\nMost database vendors offer automated tools for physical\ndesign tuning within their product suites [1], [2], [3]. Such\ntools form an integral part of broader efforts toward fully\nautomated database management systems which aim to:\na) decrease database administration costs and thus total\ncosts of ownership [4], [5]; b) help non-experts use database\nsystems; and c) facilitate hosting of databases on dynamic\nenvironments such as cloud-based services [6], [7], [8], [9].\nMost physical design tools take an off-line approach, where\nDBAs must decide when to invoke the tool and what rep-\nresentative training workload to provide [10]. Where online\nsolutions are provided [8], [11], [12], [13], questions remain:\nHow can tools generalise beyond queries seen to dynamic\nad-hoc workloads, where queries are unpredictable and\nnon-stationary? And importantly, is the quality of proposed\ndesigns in any way guaranteed?\nModern analytics workloads are dynamic in nature with\nad-hoc queries common [14], e.g., data exploration work-\nloads adapt to past query responses [15]. Such ad-hoc work-\nloads hinder automated tuning since: a) inputting represen-\n•R. M. Perera, Bastian Oetomo, Benjamin I.P . Rubinstein and Renata\nBorovica Gajic are with University of Melbourne, Australia.\nE-mail: {warnakula,bastian.oetomo,benjamin.rubinstein,\nrenata.borovica }@unimelb.edu.au.\nManuscript received April 19, 2005; revised August 26, 2015.tative information to design tools is infeasible under time-\nevolving workloads; and b) reacting too quickly to changes\nmay result in performance variability, where indices are\ncontinuously dropped and created. Any robust automated\nphysical design solution must address such challenges [11].\nThe situation is further aggravated in HTAP envi-\nronments, that consist of online transaction processing\n(OLTP) and online analytical processing (OLAP) workloads.\nWhile indices provide (primarily) positive benefits to OLAP\nqueries, they hinder the OLTP performance due to the\nadditional index maintenance overhead. Furthermore, in\ndynamic settings, workload composition (i.e., analytical to\ntransactional ratio) can vary over time, making it even more\nchallenging to identify useful indices that boost overall\nworkload performance.\nTo compare alternative physical design structures, au-\ntomated design tools use a cost model employed by the\nquery optimiser, typically exposed through a “what-if” in-\nterface [16], as the sole source of truth. However such cost\nmodels make inappropriate assumptions about data charac-\nteristics [17], [18]: commercial DBMSs often assume attribute\nvalue independence and uniform data distributions when\nsufficient statistics are unavailable [18], [19], [20]. As a\nresult, estimated benefits of proposed designs may diverge\nsignificantly from actual workload performance [8], [9],\n[20], [21], [22]. Even with more complex data distribution\nstatistics such as single- and multi-column histograms, the\nissue remains for complex workloads [20]. Moreover, data\nadditions and updates in HTAP environments continuously\ninvalidate statistics, compounding the effect of the optimiser\nmisestimates. Keeping statistics up-to-date in such a setting\nrequires extra effort.This article has been accepted for publication in IEEE Transactions on Knowledge and Data Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TKDE.2023.3271664\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2\nIn this paper, we demonstrate that even in ad-hoc\nenvironments where queries are unpredictable, there are\nopportunities for index optimisation. We argue that the\nproblem of online index selection under ad-hoc, analytical\nand HTAP workloads can be efficiently formulated within\nthe multi-armed bandit (MAB) learning setting—a tractable\nform of Markov decision process. MABs take arms or ac-\ntions (selecting indices) to maximise cumulative rewards,\ntrading off exploration of untried actions with exploitation\nof actions that maximise rewards observed so far (see Fig-\nure 1). MABs permit learning from observations of actual\nperformance, and need not rely on potentially misspecified\ncost models. Unlike initial efforts with applying learning for\nphysical design, e.g., more general forms of reinforcement\nlearning [23], bandits offer regret bounds that guarantee the\nfitness of dynamically-proposed indices [24]. In critical pro-\nduction environments, the uncertainties of online learned\nsolutions can create doubts in a DBA’s mind, making safety\nguarantees critical.\nThe key contributions of the paper can be summarised\nas:\n•We model index tuning as a multi-armed bandit,\nproposing design choices that lead to a practical,\ncompetitive solution.\n•Our proposed design achieves a worst-case safety\nguarantee against any optimal fixed policy, as a con-\nsequence of a corrected regret analysis of the C2UCB\nbandit.\n•We introduce a new bandit flavour that extends the\nexisting contextual and combinatorial bandit where\nstructured rewards are observed for each arm, pro-\nviding additional feedback for the bandit. This ban-\ndit variation enjoys a superior regret bound com-\npared to the C2UCB bandit.\n•Our comprehensive experiments demonstrate\nMAB’s superiority over a state-of-the-art commercial\nphysical design tool and a deep reinforcement\nlearning agent, with up to 75% speed-up in the\nformer and 58% speed-up in the latter case, under\ndynamic, analytical workloads.\n•We showcase MAB’s ability to perform in complex\nHTAP environments, which are notoriously chal-\nlenging for index tuning, delivering up to 59% and\n24% speed-up over the state-of-the-art commercial\ndesign tool and Monte Carlo tree search (MCTS)-\nbased database optimiser, respectively.\n2 P ROBLEM FORMULATION\nThe goal of the online database index selection problem is to\nchoose a set of indices (referred to as a configuration ) that\nminimises the total running time of a workload sequence\nwithin a given memory budget. Neither the workload se-\nquence, nor system run times, are known in advance.\nWe adopt the problem definition of [13]. Let the workload\nW= (w1, w2, . . . , w T)be a sequence of mini-workloads\n(e.g., a sequence of individual statements), Ithe set of\nsecondary indices ,Cmem(s)represent the memory space re-\nquired to materialise a configuration s⊆I, and S=\n{s⊆I|Cmem(s)≤M} ⊆ 2Ibe the class of index con-\nfigurations feasible within our total memory allowance M.Our goal is to propose a configuration sequence S=\n(s0, s1, . . . , s T), with st∈ S as the configuration in round t\nands0=∅as the starting configuration, which minimises\nthetotal workload time Ctot(W, S)defined as:\nCtot(W, S) =TX\nt=1Crec(t) +Ccre(st−1, st) +Cexc(wt, st).\nHere Crec(t)refers to the recommendation time in round\nt(defined as running time of the recommendation tool)\nandCcre(st−1, st)refers to the incremental index creation\ntime in transitioning from configuration st−1tost. Finally,\nCexc(wt, st)denotes the execution time of mini-workload\nwtunder the configuration st, namely the sum of response\ntimes of individual statements.\nAt round t, the system:\n1) Chooses a set of indices st∈ S in preparation for\nupcoming workload wt, without direct access to wt.\nstonly depends on observation of historical work-\nloads ( w1, . . . , w t−1), corresponding sets of chosen\nindices, and resulting performance;\n2) Materialises the indices in stwhich do not exist yet,\nthat is, all indices in the set difference st\\st−1; and\n3) Receives workload wt, executes all the statements\ntherein, and measures elapsed time of each individ-\nual statements and each operator in the correspond-\ning query plan.\n3 C ONTEXTUAL COMBINATORIAL BANDITS\nIn this paper, we argue that online index selection\ncan be successfully addressed using multi-armed bandits\n(MABs) from statistical machine learning, where each\narm corresponds to an index. We first present nec-\nessary background on MABs, outlining the essential\nproperties that we exploit in our work (i.e., ban-\ndit context and combinatorial arms) to converge to\nhighly performant index configurations.\nWe use the following notation. We denote non-scalar val-\nues with boldface: lowercase for (by default column) vectors\nand uppercase for matrices. We also write [k] ={1,2, . . . , k }\nfork∈N, and denote the transpose of a matrix or a vector\nwith a prime.\nThe contextual combinatorial bandit setting under semi-\nbandit feedback involves repeated selections from kpossible\nactions, over rounds t= 1,2, . . ., in which the MAB:\n1) Observes a context feature vector (possibly random\nor adversarially chosen) of each action or armi∈[k],\ndenoted as Xt={xt(i)}i∈[k], forxt(i)∈Rd, along\nwith their costs, ci;\n2) Selects or pulls a set of arms (referred to as super arm )\nst∈ St, where we restrict the class of possible super\narmsSt⊆ S′\nt=\bs⊆[k]\f\fP\ni∈sci≤M\t⊆2[k];\nand\n3) For each it∈st, observes random scores rt(it)\ndrawn from fixed but unknown arm distribution\nwhich depends solely on the arm itand its context\nxt(it), whose true expected values are contained in\nthe unknown variable r⋆\nt={E[rt(i)]}i∈[k].This article has been accepted for publication in IEEE Transactions on Knowledge and Data Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TKDE.2023.3271664\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3\nSELECT A.C1 FROM A\nWHERE A.C2 = 5 AND \nA.C3 = 6\nTable A(C1, C2, C3)\nTable B(C4, C5)0 0.1 1 0 0\n0 1 0.1 0 00 0 1 0 00 1 0 0 0\nPart 1 Part 2\n0 0.1 0\n0 0.1 0\n1 0.5 0\n1 0.5 0(3) Identify Arms\n(7) Creation time, Execution time w/ Index0 5 0.5 0 0 5 -5 0Shared Weight ( θ)\n(8) 10sec gain, 20sec creation time, \n30MB size\n(2) Query details & \nExecution time w/o IndexArms\n(5) Materialize IX6IX1 (C2)\nIX2 (C3)\nIX5 (C3, C2, C1)\nIX6 (C2, C3, C1)C1 C2 C3 C4 C5 D1 D2 D3C1 C2 C3 C4 C5 D1 D2 D3\n(4) Arm SelectedFirst Round Second Round\n(1) New \nQuery (6) Returning\nQueryContext (x)\nFig. 1. An abstract view of the proposed bandit learning-based framework for online index selection.\nRemark 1. The contextual combinatorial bandit setting is\na special case of a Markov decision process, which is\nsolved in general by reinforcement learning (RL). The\nkey difference is that in bandits, state transition is not\naffected by MAB actions, only rewards are. States (ob-\nserved via contexts) arrive arbitrarily. This simplicity\nadmits theoretical guarantees for practical MAB learn-\ners, where state-of-the-art RL agents regularly have\nnone. When playing in a bandit setting, in practice MAB\nlearners may converge faster than their (typically over\nparametrised) RL cousins.\nA MAB’s goal is to maximise the cumulative expected\nrewardP\ntE[Rt(st)] =P\ntg(st,r⋆\nt,Xt)for a known func-\ntiong. This function gneed not be a simple summation of\nall the scores, however is typically assumed to be monotonic\nand Lipschitz smooth in the arm scores.\nDefinition 1. Amonotonic function g(s,r,X)is non-\ndecreasing in r: for all s,X, ifr⪯r′then g(s,r,X)≤\ng(s,r′,X).\nDefinition 2. Function g(s,r,X)isC-Lipschitz (uniformly) in\nr, if|g(s,r,X)−g(s,r′,X)| ≤C· ∥r−r′∥2, for all r,r′,\nX, s.\nThe core challenge in this problem is that the expected\nscores for all arms i∈[k]are unknown. Refinement of a\nbandit learner’s approximation for arm iisgenerally only\npossible by including arm iin the super arm, as the score\nfor arm iis not observable when iis not played. This\nsuggests solutions that balance exploration and exploitation .\nEven though at first glance it may seem that each arm\nneeds to be explored at least once, placing practical limits\non large numbers of arms, there is a remedy to this as will\nbe discussed shortly.\nThe C2UCB algorithm. Used to solve the contextual\ncombinatorial bandit problem, the C2UCB Algorithm [24]\nmodels the arms’ scores as linearly dependent on their con-\ntexts: rt(i) =θ′xt(i) +εt(i)for unknown zero-mean (sub-\ngaussian) random variable εt, unknown but fixed parameter\nθ∈Rd, and known context xt(i). It is crucial to notice\nthe implication that, all learned knowledge is contained\nin estimates of θ, which is shared between all arms,obviating the need to explore each arm . Estimation of θcan\nbe achieved using ridge regression, with |st|new data points\n{(xt(i), rt(i))}i∈stavailable at round t, further accelerating\nthe convergence rate of the estimator ˆθ, over observing only\none example as might be na ¨ıvely assumed.\nPoint estimates on the expected scores can be made\nwith ¯rt(i) = ˆθ′\ntxt(i), where ˆθtare trained coefficients of\na ridge regression on observed rewards against contexts.\nHowever, this quantity is oblivious to the variance in the\nscore estimation. Intuitively, to balance out the exploration\nand exploitation, it is desirable to add an exploration boost\nto the arms whose score we are less sure of (i.e., greater\nestimate variance). This suggests that the upper confidence\nbound (UCB) should be used, in place of the expected value,\nand which can be calculated [25] as:\nˆrt(i) = ˆθ′\ntxt(i) +αtq\nxt(i)′V−1\nt−1xt(i), (1)\nwhere αt>0is the exploration boost factor, and Vt−1is\nthe positive-definite d×dscatter matrix of contexts for the\nchosen arms up to and including round t−1. The first term\nofˆrt(i)corresponds to arm i’s immediate reward, whereas\nits second term corresponds to its exploration boost, as its\nvalue is larger when the arm is sensitive to the context\nelements we are less confident of (i.e., the underexplored\ncontext dimension). Hence, by using ˆrt(i)in place of ¯rt(i),\narms with contexts lying in the underexplored regions of\ncontext space are more likely to be chosen, as higher scores\nyield higher g, assuming that gis monotonic increasing in\nthe arm rewards.\nIdeally, the super arm st∈ S tis chosen such that\ng(st,ˆrt,Xt)is maximised. However, it is sometimes com-\nputationally expensive to find such super arms. In such\ncases, it is often good enough to obtain a solution\nvia some approximation algorithm where g(ˆrt,Xt, st)is\nnear maximum. With this criterion in mind, we now define\nanα-approximation oracle .\nDefinition 3. Anα-approximation oracle is an algorithm A\nthat outputs a super arm s=A(r,X)with guarantee\ng(s,r,X)≥α·max sg(s,r,X), for some α∈[0,1]and\ngiven input randX.This article has been accepted for publication in IEEE Transactions on Knowledge and Data Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TKDE.2023.3271664\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4\nAlgorithm 1 The C2UCB Algorithm\n1:Input: λ, α1, . . . , α T\n2:Initialize V0←λId,b0←0d\n3:fort←1, . . . , T do\n4: Observe St\n5: ˆθt←V−1\nt−1bt−1 ▷estimate via ridge regression\n6: fori∈[k]do\n7: Observe context xt(i)\n8: ˆrt(i)←ˆθ′\ntxt(i) +αtq\nxt(i)′V−1\nt−1xt(i)\n9: end for\n10: st← A(ˆrt,Xt) ▷using α-approximation oracle\n11: Playstand observe rt(i)for all i∈st\n12: Vt←Vt−1+P\ni∈stxt(i)xt(i)′▷regression update\n13: bt←bt−1+P\ni∈strt(i)xt(i)▷regression update\n14:end for\nNote that knapsack-constrained submodular\nprograms are efficiently solved by the greedy algorithm\n(iteratively select a remaining cost-feasible arm with highest\navailable score) with α= 1−1/e. C2UCB is detailed in\nAlgorithm 1.\nThe performance of a bandit algorithm is usually\nmeasured by its cumulative regret , defined as the to-\ntal expected difference between the reward of the cho-\nsen super arm E[Rt(st)]and an optimal super arm\nmax s∈StE[Rt(s)]over Trounds. Such a metric is unfair\nto C2UCB since its performance depends on the oracle’s\nperformance. This suggests assessing C2UCB’s performance\nwith a metric using the oracle’s performance guarantee as\nits measuring stick, as follows.\nDefinition 4. Letsbe a super arm returned by an α-\napproximation oracle as a part of the bandit algorithm,\nandr⋆\ntbe a vector containing each arms’ true expected\nscores. Then cumulative α-regret is the sum of expected\ninstantaneous regret, Regα\nt=α·max sg(s,r⋆\nt,Xt)−\ng(st,r⋆\nt,Xt).\nWhen gis assumed to be monotonic and Lipschitz\ncontinuous, [24] claimed that C2UCB enjoys ˜O(√\nT)α-\nregret. We have corrected an error in the original proof,\nas seen in Appendix, confirming the ˜O(√\nT)α-regret. This\nexpression is sub-linear in T, implying that the per-round\naverage cumulative regret approaches zero after sufficiently\nmany rounds. Consequently, online index selection based\non C2UCB comes endowed with a safety guarantee on worst-\ncase performance: selections become at least as good as an\nα-optimal policy (with perfect access to true scores); and\npotentially much better than any fixed policy .\n4 MAB FOR ONLINE INDEX SELECTION\nPerformant bandit learning for online index tuning de-\nmands arms covering important actions and no more, re-\nwards that are observable and for which regret bounds are\nmeaningful, and contexts and oracle that are efficiently com-\nputable and predictive of rewards. Each workload statement\nis monitored for characteristics such as running time, query\npredicates, payload, etc. (see Figure 1). These observations\nfeed into generation of relevant arms and their contexts. The\nlearner selects a desired configuration which is materialised.For returning statements, the system identifies benefits of\nthe materialised indices, which are then shaped into the\nreward signal for learning.\nDynamic arms from workload predicates. Instead of\nenumerating all column combinations, relevant arms (in-\ndices) may be generated based on queries: combinations\nand permutations of query predicates (including join pred-\nicates), with and without inclusion of payload attributes\nfrom the selection clause. Such workload-based arm gen-\neration drastically reduces the action space, and exploits\nnatural skewness of real-life workloads that focus on small\nsubsets of attributes over full tables [15]. Workload-based\narm generation is only viable due to dynamic arm addition\n(reflecting a dynamic action space) and is allowed by the\nbandit setting: we may define the set of feasible arms for\neach round at its start.\nContext engineering. Effective contexts are predictive of\nrewards, efficiently computable, and promote generalisation\nto previously unseen workloads and arms. We form our\ncontext in two parts (see Figure 1).\nContext Part 1: Indexed column prefix. We encode one\ncontext component per column. However unlike a bag-of-\nwords or one-hot representation appropriate for text, simi-\nlarity of arms depends on having similar column prefixes;\ncommon index columns is insufficient. This reflects a novel\nbandit learning aspect of the problem. A context component\nhas value m−jwhere jis the corresponding column’s\nposition in the index, provided that the column is included in\nthe index and part of the workload. We experimented with\nvalues 1, 2, 10 and 100 for m, where 1 represents the one-\nhot encoding. We observe that smaller values (i.e., 1 and\n2) do not provide sufficient differentiation between arms,\nwhile the larger values (i.e., 100) only differentiate based\non the first column (there is insufficient representation for\nthe rest of the columns). Thus we set mto 10. The value\nis set to 0 otherwise, including if its presence only covers\nthe payload. Unlike a simple one hot encoding, this context\nenables the bandit to differentiate between arms with the\nsame set of columns but different ordering, and reward the\ncolumns differently based on their position in the index.\nExample 1. Under the simplest workload (single query) in\nFigure 1, our system generates six arms: four using dif-\nferent combinations and permutations of the predicates,\ntwo including the payload (covering indices). Index IX5\nincludes column C1, but the context for C1 is valued as\n0, as this column is considered only due to the query\npayload.\nContext Part 2: Derived statistical information. We represent\nstatistical and derived information about the arms and\nworkload, details available during statement execution, and\nsufficient statistics for unbiased estimates. This statistical\ninformation includes: a Boolean indicating a covering index,\nthe estimated size of the index divided by the database size\n(if not materialised already, 0 otherwise), and the number\nof times the optimiser has picked this arm in recent rounds.\nThis is shown in Figure 1 under D1, D2 and D3, respec-\ntively. Results showed very low sensitivity to the number of\nrounds considered for the D3 feature, however making it 0\nleads to slight increases in creation times.This article has been accepted for publication in IEEE Transactions on Knowledge and Data Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TKDE.2023.3271664\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5\nReward shaping. As the goal of physical design tuning\ntools is to minimise end-to-end workload time, we incorpo-\nrate index creation time and statement execution time into\nthe reward for a workload. We omit index recommenda-\ntion time, as it is independent of arm selection. However,\nwe measure and report recommendation time of the MAB\nalgorithm in our experiments. Recall that MAB depends\nonly on observed execution statistics from implemented\nconfigurations and generalisation of the learned knowledge\nto unseen arms thereafter. Unlike OLAP workloads, un-\nder HTAP workloads, the statement execution time can be\nnegatively impacted by the index maintenance operations,\nnecessitating its inclusion in the reward.\nThe implementation of the reward for an arm includes\nthe execution time as a gainGt(i, wt, st)for a workload wt\nby each arm iunder configuration st. Indices can impact\nthe execution time in multiple ways. We split the execution\ntime gain into three components: a) data scan gains ( Gds\nt),\nb) index maintenance gains ( Gim\nt, usually a negative value),\nand c) other areas of the query plan which can be difficult\nto attribute to a single index (unclaimed gains) ( Gun\nt).\nGt(i, wt, st)\n=Gds\nt(i, wt, st) +Gim\nt(i, wt, st) +Gun\nt(i, wt, st).\nData scan gains : The data scan gain by index ifor query qis\ndefined as:\nGds\nt(i,{q}, st)\n= [Ctab(τ(i), q,∅)−Ctab(τ(i), q,{i})] 1U(s,q)(i),\nwhere U(s, q)denotes the list of indices used by the\noptimiser in query qunder a given configuration s.\nCtab(τ(i), q,∅)represents the full table scan time for table\nτ(i)and query q, where τ(i)is the table which ibelongs\nto.1\nIndex maintenance gains : Index maintenance operations\ncan take different forms based on the number of rows up-\ndated. The optimiser typically opts for row-wise updates for\na small number of rows and index-wise updates otherwise.\nIn the second case, we can easily capture the maintenance\ngain of an index as each index is updated separately. This\nis however not straightforward in the case of row-wise\nupdates, where all indices are bulk updated for each row.\nOn these occasions, we compute the total maintenance\ngain ( Gim\nt(V(s, q),{q}, st)) for all secondary indices that\nrequire maintenance due to a statement qunder a given\nconfiguration sand equally divide it among the updated\nindices. V(s, q)represents the set of indices updated under\nconfiguration sby the statement q.\nGim\nt(V(s, q),{q}, st) = [Cim(q,∅)−Cim(q, s)].\nwhere Cim(q,∅)andCim(q, s)represent the index main-\ntenance time without secondary indices and index mainte-\nnance time under configuration s, respectively.\nGim\nt(i,{q}, st)\n=\u0002Gim\nt(V(s, q),{q}, st)/|V(s, q)|\u00031V(s,q)(i).\n1. Due to the reactive nature of multi-armed bandits, we mostly\nobserve a full table scan time for each table τ(i)and query q. When\nwe do not observe this, we estimate it with the maximum secondary\nindex scan/seek time.Unclaimed gains : Sometimes the impact of the indices can\nbe indirect. For example, while introducing a new index can\nspeed up the data scan, it can require sorting, which can\nbe costly. Therefore, when the overall gain results in a per-\nformance regression, MAB needs to take corrective actions\nto trigger a different query plan. These gains are captured\nunder Gun\nt.Gun\ntfor a statement is computed by subtracting\nall the other gains (data scan and index maintenance) from\nthe total gain ( Gto\nt). The total gain ( Gto\nt) can be obtained\nby using statement running times before and after index\ncreation.\nGto\nt({q}, st) = [Cto(q,∅)−Cto(q, st)].\nThen we equally divide this cost among participating\nindices ( U(s, q)∪ V(s, q)).\nThe gain for a workload relates to the gain for individual\nstatement by:\nGt(i, wt, st) =X\nq∈wtGt(i,{q}, st).\nBy this definition, gain Gt(i, wt, st)will be 0ifiis not used\nby the optimiser in the current round tand can be negative\nif the index creation leads to a performance regression or if\nthe index incurs a maintenance cost.2Creation time of iis\ntaken as a negative reward, only if iis materialised in round\nt, and is 0otherwise:\nrt(i) =Gt(i, wt, st)−Ccre(st−1,{i}).\nMinimising the end-to-end workload time, or rather, max-\nimising the end-to-end workload time gained, is the goal\nof the bandit. As defined earlier, the total workload time\nCtotis the sum of execution ,recommendation and creation\ntimes accumulated over rounds. As such, minimising each\nround’s summand is an equivalent problem. Modifying the\nexecution time to the time gain while ignoring the recom-\nmendation time yields per-round super arm reward of:\nRt(st) =Cexc(wt,∅)−[Cexc(wt, st) +Ccre(st−1, st)]\n≈X\ni∈stGt(i, wt, st)−X\ni∈stCcre(st−1,{i})\n=X\ni∈strt(i).\nSelection of the execution plan depends on the query op-\ntimiser, and as noted, the query optimiser may resolve\nto a sub-optimal query plan. As we show, the bandit is\nnonetheless resilient as it can quickly recover from any\nsuch performance regressions. Observed execution times\nencapsulate real-world effects, e.g., the interaction between\nstatements, application properties, run-time parameters, etc.\nHowever, concurrent environments might require modi-\nfying the reward design based on specific performance\ntargets (e.g., removing index creation time, or considering\ntotal workload run times over query run times). Since the\nend-to-end workload time includes the index creation and\nstatement execution times, we are indirectly optimising for\nboth efficiency and the quality of recommendations.\n2. The optimiser cost model does not have to agree that MAB choices\nare optimal. The recommended indices will still be used if the optimiser\nestimates that recommended MAB indices will provide a positive gain\nover a full table scan.This article has been accepted for publication in IEEE Transactions on Knowledge and Data Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TKDE.2023.3271664\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6\nAlgorithm 2 MAB Simulation for Index Tuning\n1:QS←QueryStore () ▷keeps query information\n2:C2UCB←InitialiseBandit () ▷A1, L 1-2\n3:while (TRUE) do\n4: queries ←getLastRoundWorkload ()\n5: for all queries do\n6: if(isNewTemplate) then\n7: QS.add (query )\n8: else\n9: QS.update (query )\n10: end if\n11: end for\n12: QoI←QS.getQoI () ▷get queries of interest\n13: arms←generateArms (QoI)\n14: X←generateContext (arms, QoI )\n15: st←C2UCB.recommend (arms, X ) ▷A1, L 4-10\n16: Ccre←materialise (st)\n17: Cexc←executeCurrentWorkload ()\n18: C2UCB.updateWeights (Ccre, Cexc)▷A1, L 12-13\n19:end while\nA greedy oracle for super-arm selection. Recall that\nC2UCB leverages a near-optimal oracle to select a su-\nper arm, based on individual arm scores [24]. As a sum\nof individual arm rewards, our super-arm reward has a\n(sub)modular objective function and (as easily proven) ex-\nhibits monotonicity and Lipschitz continuity. Approximate\nsolutions to maximise submodular (diminishing returns)\nobjective functions can be obtained with greedy oracles\nthat are efficient and near-optimal [26]. Our implementation\nuses such an oracle combined with filtering to encourage\ndiversity.\nInitially, arms with negative scores are pruned. Then\narm selection and filtering steps alternate, until the memory\nbudget is reached. In the selection step, an arm is selected\ngreedily based on individual scores. The filtering step filters\nout arms that are no longer viable under the remaining\nmemory budget, or those that are already covered by the\nselected arms based on prefix matching. If a covering index\nis selected for a query, all other arms generated for that\nquery will be filtered out. Note that filtering is a temporary\nprocess that only impacts the current round.\nBandit learning algorithm. Algorithm 2 shows the\nMAB algorithm, which wraps Algorithm 1 and handles the\ndomain specific aspects of the implementation. We have\ndivided Algorithm 1 into three main parts, initialisation\n(lines 1-2), arm recommendation (lines 4-10) and weight\nvector update (lines 12-13). These segments are utilised\nin Algorithm 2 as C2UCB function calls. After initialising\nthe bandit, Algorithm 2 summarises workload information\nusing templates; these track frequency, average selectivity,\nfirst seen and last seen times of the statements which help to\ngenerate the best set of arms per round (i.e., QoI). The con-\ntext is updated after each round based on the workload and\nselected set of arms. The bandit then selects the round’s set\nof arms, forming a configuration to be materialised within\nthe database. The reward will then be calculated based on\nobserved execution statistics on a new set of statements,\nand will be used to update the shared weight. To support\nshifting workloads, where users’ interests change over time,the learner may forget learned knowledge depending on\nthe workload shift intensity (i.e., the number of newly\nintroduced statement templates).\nIn our implementation, we perform bandit updates sep-\narately for creation time reward and execution time re-\nward (line 13 of Algorithm 1). At the creation cost update,\nwe temporarily make all context features 0 except for the\ncontext feature that is responsible for the index size. This\ncan be viewed as an innovation of independent interest to\nthe bandit community where we decompose the reward\ninto multiple components and want to direct each reward\ncomponent feedback to a subset of the features. We coin\nthe term focused update in reference to this approach. This\nidea invites a new flavour of bandits elaborated in the next\nsection.\n5 C ONTEXTUAL COMBINATORIAL BANDIT WITH\nSTRUCTURED REWARDS\nWhen rewards can be decomposed into component rewards\nunder two key conditions, we hypothesise that a focused\nupdate can result in faster convergence: (i) when each reward\ncomponent is directly related to a small subset of context\nfeatures we create lower dimensional supervised learning\nproblems; and (ii) when each reward component is directly\nobserved we offer more opportunities for bandit feedback.\nUnder focused updates we use each component of the\nreward to learn a part of the weight vector (see Figure 2).\nIndeed for this structured setting we modify the proof of\nC2UCB to arrive at a tighter regret bound by a factor of\n1/√nf, where nfis the number of reward components (i.e.,\nobserved number of examples per round).\nOur approach to structured rewards is by a reduction\nto C2UCB. We modify the C2UCB’s formulation as if two\nexamples are observed for pulled arm iin round twith\nrespective rewards ¯rt,1(i)and¯rt,2(i). Throughout both (sub\nround) observations, the overall arm reward function is\nfixed as rt(i) = ¯rt,1(i) + ¯rt,2(i). This permits learning at\na faster rate, while still coordinating an overall arm reward\nestimate.\nx1 x2 x3 x4 x5 x6 x7 x8\nx1 x2 x3 x4 x5 x6 x7 x8rt(i)   =  rt, 1(i)      +  rt, 2(i) Normal \nUpdate\nFocused\nUpdatext(i)\nxt, 1(i) xt, 2(i) ~ ~\nFig. 2. Regular contextual updates vs focused update.\nExample 2. In the bandit setting of this paper, we are mo-\ntivated by the desire for context part 1 to be completely\nresponsible for execution cost gains—we can learn the\nnegative weight from index creation cost directly into\npart 2’s index size feature. This enables us to switch\noff the creation cost overhead for already created arms\nby simply setting the index size context feature to zero.This article has been accepted for publication in IEEE Transactions on Knowledge and Data Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TKDE.2023.3271664\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7\nThis ability to use domain knowledge to tailor reward\nfeedback to a subset of context features is a powerful\nbenefit of structured rewards.\nLet¯xt,f(i)∈Rdbe the fthcontext of arm iat round t,\nsuch that rt(i) =θ′\n⋆xt(i) +εt=θ′\n⋆(¯xt,1(i) +¯xt,2(i)) +\nεt,1+εt,2for two independent zero-mean (subgaussian)\nnoise random variables εt,1andεt,2with equal variance3.\nTo observe the benefit of the focused update, we further\nassume that εt,1andεt,2are eachR√\n2-subgaussian, which\nmakes εtR-subgaussian.4It should be noted that even\nthough we have assumed that the overall context is the\nsum of the sub-contexts, Equation (1) from Section 3 still\nholds since the equation is oblivious to how the overall con-\ntext is obtained from the sub-contexts. We further assume\ncomplementary sparse sub-contexts: overall context xt(i)\nis the concatenation of ˜xt,1(i)∈Rd1and ˜xt,2(i)∈Rd2,\nwhere d=d1+d2and ¯xt,1(i)′=\u0002˜xt,1(i)′01×d2\u0003\nand\n¯xt,2(i)′=\u000201×d1˜xt,2(i)′\u0003.\nTo maintain optimal least squares learning given two\nobservations per round with equal variances, at the end of\nround twe generalise our matrix Vtandbtupdates to:\nVt=λI+tX\nτ=12X\nf=1X\ni∈Sτ¯xτ,f(i)¯xτ,f(i)′\nbt=tX\nτ=12X\nf=1X\ni∈Sτ¯xτ,f(i)¯rτ,f(i).\nNotice that this is different from C2UCB’s definition of Vt\nandbt, and hence a new regret analysis is warranted.\nWe exploit the fact that Theorem 4.2 in [24] holds regard-\nless of the super arm St. Therefore, solely for the purpose\nof modifying the aforementioned theorem, we re-index the\ncontext and rewards such that ¯xt,f(i) = ¯xt(i+kf)and\n¯rt,f(i) = ¯rt(i+kf), and we construct the effective super\narmS′\nt={i′:i′=i+kf, i∈St, f∈ {0,1}}. As such, our\ndefinition of Vtandbtcan now be rewritten as:\nVt=λI+tX\nτ=1X\ni∈S′τ¯xτ(i)¯xτ(i)′\nbt=tX\nτ=1X\ni∈S′τ¯xτ(i)¯rτ(i),\nwhich is syntactically the same as the definition given in\n[24]. We need to be more careful in concluding the theorem,\nhowever, since it contains an intermediate step involving\n3. Equivariance is without loss of generality. If the two variances\nwere different, the expressions for Vtandbtwould be different.\nThe data with the less variance would be prioritised via larger\nweight: Vt=λI+Pt\nτ=1P2\nf=1P\ni∈Sτ(σ1σ2\nσf)2¯xτ,f(i)¯xτ,f(i)′and\nbt=Pt\nτ=1P2\nf=1P\ni∈Sτ(σ1σ2\nσf)2¯xτ,f(i)¯rτ,f(i). The hyperparameter\nλwould need different adjustments since λ=σ2\n1σ2\n2/γ2. The value of\nγ2stays the same, serving as the variance for the prior of θ.\n4. For independent r.v.’s X R 1-subgaussian and Y R 2-subgaussian,\nX+Ymust beq\nR2\n1+R2\n2-subgaussian.det(Vt)as defined in [27]. Assuming that ∥xt(i)∥ ≤1, we\nbound det(Vt)as follows:\ndet(Vt)≤\u0012tr(Vt)\nd\u0013d\n= tr(λId+Pt\nτ=1P\ni∈S′τ¯xτ(i)¯xτ(i)′)\nd!d\n= tr(λId) +Pt\nτ=1P\ni∈S′τtr(¯xτ(i)¯xτ(i)′)\nd!d\n= λd+Pt\nτ=1P\ni∈S′\nτ∥¯xτ(i)∥2\n2\nd!d\n= \nλd+Pt\nτ=1P\ni∈Sτ\u0000∥¯xτ,1(i)∥2\n2+∥¯xτ,2(i)∥2\n2\u0001\nd!d\n= \nλd+Pt\nτ=1P\ni∈Sτ\u0000∥˜xτ,1(i)∥2\n2+∥˜xτ,2(i)∥2\n2\u0001\nd!d\n= \nλd+Pt\nτ=1P\ni∈Sτ∥xτ(i)∥2\n2\nd!d\n≤ \nλd+Pt\nτ=1P\ni∈Sτ1\nd!d\n≤\u0012λd+tk\nd\u0013d\n,\nwhere we have used the AM-GM Inequality for the first\ninequality and the fact that xτ(i)′=\u0002˜xτ,1(i)′˜xτ,2(i)′\u0003\nto\narrive at the last equality. Finally, using the fact that Vt−1⪯\nVt, that the noise isR√\n2-subgaussian and Theorem 2 from\n[27], Theorem 4.2 from [24] becomes:\n∥ˆθt−θ⋆∥Vt−1≤ ∥ˆθt−θ⋆∥Vt\n≤R√\n2s\n2 log\u0012det(Vt)1/2\nδdet(λId)1/2\u0013\n+λ1/2S\n≤R√\n2s\ndlog\u00121 +tk/λ\nδ\u0013\n+λ1/2S ,\nwith probability at least 1−δ, which is the same as that in\n[24], with the exception of the definition of Vt.\nConveniently, Lemma 4.1 from [24] is written in terms\nofVt−1andαt, thus the proof follows exactly besides the\nchoice of αt, rewritten below for convenience:\nLemma 1. Ifαt=R√\n2r\ndlog\u0010\n1+tk/λ\nδ\u0011\n+λ1/2S, then we have\n0≤ˆrt(i)−r⋆\nt(i)≤2αt∥xt(i)∥V−1\nt−1\nholds simultaneously for all t≥0andi∈[k]with\nprobability at least 1−δ.\nWe provide the correction of the proof of Lemma 4.2\nfrom [24] in Appendix. This proof can be used by changingThis article has been accepted for publication in IEEE Transactions on Knowledge and Data Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TKDE.2023.3271664\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8\nthe definition of the matrix XTinto:\nX′\nT=\n¯x′\nT,1(s(1,T))\n...\n¯x′\nT,1(s(|ST|,T))\n¯x′\nT,2(s(1,T))\n...\n¯x′\nT,2(s(|ST|,T)).\n.\nThen we rewrite:\ndet(VT) = det\nV+TX\nt=12X\nf=1X\ni∈st¯xt,f(i)¯xt,f(i)′\n\n= det\nV+T−1X\nt=12X\nf=1X\ni∈st¯xt,f(i)¯xt,f(i)′+\n2X\nf=1X\ni∈sT¯xT,f(i)¯xT,f(i)′\n\n= det ( VT−1+XTX′\nT),\nand the rest of the proof follows very similarly, with slight\ndifference in the dimension of XT, changing from |sT|into\n2|sT|. Finally, the third last equality requires us to find the\ntrace of the matrix of interest, which is tr(X′\nTV−1\nT−1XT) =P2\nf=1P\ni∈sTxT,f(i)V−1\nT−1xT,f(i), which in turn gives us\nour new determinant inequality:\ndet(VT)≥det(VT−1)\n1 +2X\nf=1X\ni∈sT∥xT,f(i)∥2\nV−1\nT−1\n.\nTherefore, we have our modification of Lemma 4.2 of [24]\n(Lemma 1 in Appendix) as follows:\nLemma 2. LetV∈Rd×dbe a positive definite matrix, st⊆\n{1,···, k}where |st| ≤ℓfort= 1,2, . . ., and VT=\nV+PT\nt=1P2\nf=1P\ni∈st¯xt,f(i)¯xt,f(i)′. Then, if ∀t, i λ≥\nℓand||xt(i)||2≤1for concatenated context xt(i) =\n¯xt,1(i) +¯xt,2(i)where ¯xt,1(i)′=\u0002˜xt,1(i)′01×d2\u0003\nand\n¯xt,2(i)′=\u000201×d1˜xt,2(i)′\u0003\n, we have\nTX\nt=1X\ni∈st∥xt(i)∥2\nV−1\nt−1=2X\nf=1TX\nt=1X\ni∈st∥xt,f(i)∥2\nV−1\nt−1\n≤2 log det VT−2 log det V\n≤2dlog((tr( V) +Tℓ)/d)−\n2 log det V.\nSince there is no modification to the objective function,\nand since all the theorems and lemma required to arrive at\nthe final regret bound are the same, the regret bound for the\nmodified C2UCB stays the same, as stated in [24]:\nTX\nt=1Regα\nt≤CR√\n2s\n8Tdlog\u0012\n1 +Tk\ndλ\u0013\n·\n s\ndlog\u00121 +Tk/λ\nδ\u0013\n+√\nλS!\n.Notice that in cases where there are nfexamples per arm\nin each round instead of only two, the regret will generalise\ninto:\nTX\nt=1Regα\nt≤CR√nfs\n8Tdlog\u0012\n1 +Tk\ndλ\u0013\n·\n s\ndlog\u00121 +Tk/λ\nδ\u0013\n+√\nλS!\n,\nwhich has a factor of1√nf,nf∈Ncompared to the original\nC2UCB where nf= 1.\n6 E XPERIMENTAL METHODOLOGY\nWe evaluate our MAB framework across a range of widely\nused analytical and HTAP industrial benchmarks, compar-\ning it to a state-of-the-art physical design tool shipped with\na commercial database product referred to as the Physical\nDesign Tool (PDTool). This is a mature product, proven to\noutperform other physical design tools available on the mar-\nket [21], [28]. As a representative of the most recent studies\nthat successfully use Monte Carlo tree search (MCTS) to\ntune indices [29], [30], [31], we test our framework against a\ndatabase optimiser that can tune indices using MCTS, called\nUDO [29]. UDO is originally designed to work with ana-\nlytical queries only, which we extend to work with HTAP\nworkloads (we refer to this baseline as MCTS hereafter).\nBenchmarks. For HTAP performance testing we use\nCH-BenCHmark [32], [33], TPC-H benchmark (with uni-\nform distribution) [34] and TPC-H Skew benchmark [35]\nwith Zipfian factor 4. CH-BenCHmark provides a com-\nplex mixed workload, combining TPC-C [36] and TPC-H\nbenchmarks. The CH-BenCHmark schema comprises an un-\nmodified TPC-C schema and three tables (Supplier, Region,\nNation) from TPC-H. Its workload is composed of TPC-C\ntransactional workload and modified 22 TPC-H [34] queries\nadapted to the CH-BenCHmark schema.\nWhile CH-BenCHmark provides a uniform dataset, we\nare unaware of any HTAP benchmarks with skewed data\ngeneration. While there are some OLTP benchmarks with\nskewed datasets [33], they do not provide the required\nlevel of OLAP complexity for the index selection problem\nto be interesting. Due to the limitations of existing bench-\nmarks, we decided to extend the TPC-H skew benchmark\nto include INSERT, DELETE and UPDATE statements to\nmimic a skewed HTAP benchmark [37]. The TPC-H skew\ndata generation tool already provides the functionality to\ngenerate data for inserts and deletes. In our extension, we\nadditionally perform updates on existing records using the\nsame generated data. To highlight the impact of skewness\non the overall performance, we also report comparable\nHTAP results on the original TPC-H database.\nFor analytical experiments, we use five publicly available\nbenchmarks: TPC-DS [38], a complex benchmark resulting\nin a large number of candidate configurations; SSB [39]\nwith easily achievable index benefits; Join Order Bench-\nmark (JOB) with IMDb dataset (a real-world dataset) [18]\n(henceforth referred to as IMDb), a challenging workload\nfor index recommendations with index overuse leading to\nperformance regressions; and finally, TPC-H and TPC-H\nskew benchmarks.This article has been accepted for publication in IEEE Transactions on Knowledge and Data Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TKDE.2023.3271664\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9\nUnless stated otherwise, all experiments use scale fac-\ntor (SF) 10, resulting in approximately 10GB of data per\nworkload, except in the case of the IMDb dataset, which\nhas a fixed size of 6GB.5We consider two broad types of\nworkloads, allowing us to compare different aspects of the\nrecommendation process:\n1) Static : The workload sequence is known in advance,\nand repeating over time (modelling workloads used\nfor reporting purposes). In the absence of dynamic\nenvironment complexities, this simpler setting al-\nlows us to single-out the effectiveness (the ability\nto find a good configuration) and the efficiency (the\nsearch overhead) of the MAB search strategy.\n2) Dynamic : The region of interest shifts over time\nfrom one group of queries to another (modelling\ndata exploration). For HTAP experiments, interest\nshifts through workloads with different transaction\nand analytical compositions, ranging from fully an-\nalytical workloads to transaction heavy workloads.\nDynamic workloads are used to evaluate adoption\nspeed, cost of exploration and memory efficiency in\ndynamic environments.\nAcross experiments, each group of templates is invoked\nover rounds, producing different instances. For static exper-\niments, we invoke the PDTool at the start of the second\nround giving the first round workload as the represen-\ntative workload. For dynamic workloads, we invoke the\nPDTool soon after the workload shift since this workload\nwill become representative of future rounds. This setting is\nsomewhat unrealistic and favourable for PDTool, since in\nreal-life the PDTool will seldom truly have knowledge of\nthe representative workload (i.e., what is yet to arrive in the\nfuture), advantaging the PDTool in our experiments. How-\never, it presents a viable comparison against the workload-\noblivious MAB. Bandits do not use any workload informa-\ntion ahead of time, but instead observe a workload sequence\nand react accordingly.\nPhysical design tuning parameters. Both PDTool and\nMAB are given a memory budget approximately equal to\nthe size of the data (1x; 10GB for SF 10 datasets and 6GB for\nIMDb dataset) for the creation of secondary indices. We have\nexperimented with different memory budgets ranging from\n0.25x to 2x (since benefits of additional memory seem to\ndiminish beyond a 2x limit) under TPC-H and TPC-H skew\nbenchmarks, and observed the same patterns throughout\nthat range.6We have naturally picked the middle of the\nactive region (1x) as our default memory budget. All these\nworkloads come with original primary and foreign keys that\ninfluence the choice of indices. We grant the aforementioned\nmemory budget on top of this.\nIn search of the best possible design, we do not constrain\nthe running time of PDTool.All proposed indices are mate-\nrialised and workload invoked over the same commercial\nDBMS in both cases (MAB and PDTool).\n5. CH-BenCHmark does not scale with the SF parameter like most of\nthe other benchmarks we use. It uses a number of warehouses (similar\nto the TPC-C benchmark) as a scaling parameter. For our experiments,\nwe use 137 warehouses which generates approximately a 10GB dataset.\n6. Both tools converge to the same execution cost by the final round,\nwhen enough memory was given to fit the entire useful configuration.\nStatic\n02000400060008000100001200014000\nNoIndex\nPDTool\nMAB\nMCTS\nNoIndex\nPDTool\nMAB\nMCTS\nNoIndex\nPDTool\nMAB\nMCTS\nNoIndex\nPDTool\nMAB\nMCTS\nNoIndex\nPDTool\nMAB\nMCTS\nNoIndex\nPDTool\nMAB\nMCTS\n0:1 1:1 2:1 3:1 4:1 5:1Total Workload Time (sec)\nTransactional to Analytical Ratio (TAR)Fig. 3. MAB vs. PDTool vs. MCTS total workload time under CH-\nBenCHmark for static workloads with a range of different TARs\nHardware. All experiments are performed on a server\nequipped with 2x 24 Core Xeon Platinum 8260 at 2.4GHz,\n1.1TB RAM, and 50TB disk (10K RPM) running Windows\nServer 2016. We report cold runs, clearing database buffer\ncaches prior to every query execution.\n7 E XPERIMENTAL RESULTS\nThis section reports empirical comparisons of MAB against\nPDTool and MCTS on HTAP workloads and summarises\nresults under analytical workloads, reported earlier [40].\nWe report the total workload time broken down by recom-\nmendation, index creation, and workload execution times.\nTotal workload time captures all the costs incurred from\nthe start of the experiment to the end. Index deletion cost\nis negligible compared to creation and execution costs and\ndoes not have an observable growth with index size.7There-\nfore, we ignore the index deletion cost. For HTAP experi-\nments, execution time is further divided into analytical and\ntransactional components. In addition, we present original\nstatement times without any secondary indices (denoted\nas NoIndex). We present summary graphs with total end-\nto-end workload time and convergence graphs with to-\ntal workload and execution times per round. Finally, we\npresent results against a well-tuned reinforcement learning\nagent.\n7.1 MAB vs PDTool Under HTAP Workloads\n7.1.1 Static HTAP Workloads\nTo illustrate the impact of transactional statements, we use\na series of CH-BenCHmark static experiments varying the\ntransactional to analytical ratio. In each of these experi-\nments, we keep the analytical component constant with\n22 adapted TPC-H queries ( a set of analytical queries ) while\nchanging the size of the transactional component.\nThe transactional component of the workload is com-\nposed of 5 transactions (new-order, payment, order-status,\ndelivery and stock-level) with a pre-specified transaction\nmixture (44%, 44%, 4%, 4% and 4%, respectively). The small-\nest transactional workload adhering to the specified TPC-C\ntransaction mixture comprises 11 new order transactions, 11\n7. When tested with indices of different sizes and complexities, we\nobserved sub-millisecond deletion costs in all cases.This article has been accepted for publication in IEEE Transactions on Knowledge and Data Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TKDE.2023.3271664\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10\nWith MCTS\n0100020003000400050006000\n0 5 10 15 20 25Total Time Per Round (sec)\nRound NumberPDTool\nMAB\nNoIndex\nMCTS\n050100150200250300350\n0 5 10 15 20 25Total Exec. Time Per Round (sec)\nRound NumberPDTool\nMAB\nNoIndex\nMCTS\n(a)\nWith MCTS\n0100020003000400050006000\n0 5 10 15 20 25Total Time Per Round (sec)\nRound NumberPDTool\nMAB\nNoIndex\nMCTS\n050100150200250300350\n0 5 10 15 20 25Total Exec. Time Per Round (sec)\nRound NumberPDTool\nMAB\nNoIndex\nMCTS(b)\nFig. 4. MAB vs. PDTool vs MCTS convergence under CH-BenCHmark\nforstatic workloads with 3:1 TAR: (a) End-to-end workload time, (b) Total\nexecution time.\n050100150200250300350\n0 5 10 15 20 25Analytical Exec. Time Per Round (sec)\nRound NumberPDTool\nMAB\nNoIndex\nMCTS\n020406080100120\n0 5 10 15 20 25Trans. Exec. Time Per Round (sec)\nRound NumberPDTool\nMAB\nNoIndex\nMCTS\n(a)\n050100150200250300350\n0 5 10 15 20 25Analytical Exec. Time Per Round (sec)\nRound NumberPDTool\nMAB\nNoIndex\nMCTS\n020406080100120\n0 5 10 15 20 25Trans. Exec. Time Per Round (sec)\nRound NumberPDTool\nMAB\nNoIndex\nMCTS (b)\nFig. 5. MAB vs. PDTool vs MCTS convergence under CH-BenCHmark\nforstatic workloads with 3:1 TAR: (a) Transactional Execution cost, (b)\nAnalytical Execution cost.\npayment transactions, an order-status transaction, a delivery\ntransaction and a stock-level transaction (approximately 650\nstatements). This smallest transactional workload is hence-\nforth referred to as a set of transactional statements . We define\nthe transactional to analytical ratio (henceforth referred to\nasTAR ) as the ratio between transactional and analytical\nstatement sets. As an example, 5:1 TAR is composed of one\nanalytical set (22 TPC-H queries) and 5 transactional sets\n(i.e., 55 new order transactions, 55 payment transactions,\n5 order-status transactions, 5 delivery transactions and 5\nstock-level transactions, resulting in approximately 3300\ntransactional statements per round).\nAs evident from Figure 3, in transaction-heavy work-\nloads, MAB performs much better than PDTool providing\nup to 51% speed-up (5:1) in total workload time, whereas\nPDTool performs better in the fully analytical workload\n(0:1) providing up to 8% speed-up. Static workloads over\nuniform datasets are the best case for offline physical design\ntools, as a pre-determined workload sequence may perfectly\nrepresent future statements. We will look into analytical\nworkloads in more detail in Section 7.3. At 1:1 TAR, the\nfirst ratio that introduces the transactional component, MAB\nstarts to take the lead providing a 4% performance gain\nin total workload time. MAB reaches the same last round\nexecution time as PDTool; however, due to better execution\ntimes in early rounds, PDTool provides 7.7% total execution\ntime speed up over MAB. From 2:1 TAR onwards, MABdominates the PDTool providing 26%, 47%, 51% and 51%\ntotal workload time speed-up over PDTool, under 2:1, 3:1,\n4:1, 5:1 TARs, respectively. PDTool struggles to perform\nbetter than NoIndex from 3:1 TAR onwards due to the heavy\nrecommendation costs incurred by PDTool, yet PDTool is\nstill superior to NoIndex in execution cost.\nThe MCTS-based approach performs much better than\nNoIndex but records lower performance in total workload\ncost compared to the PDTool and MAB. MAB managed to\noutperform MCTS by 21.7%, 16.3%, 18.7%, 20%, 23.7% and\n24.1% under 0:1, 1:1, 2:1, 3:1, 4:1 and 5:1 TARs, respectively.\nFurthermore, MCTS requires longer training outside the\ntotal workload time (e.g., MCTS used 1h on average for\ntraining across the experiments). On the downside, MCTS\naction space grows like O(2k)where k is the number of\narms, which limits its candidate indices to unique column\nsubsets and not the permutation of those columns.\nTo further understand the results, we dive into the 3:1\nTAR experiment, which provides a good balance of trans-\nactional and analytical statements to demonstrate the im-\nportance of both analytical gain and transactional overhead.\nAs shown in the convergence graphs in Figure 4(b), MAB\nconverges to a better configuration providing 29.4% and\n42.8% faster execution time by the last round compared\nto PDTool and MCTS, respectively. Figure 4 also explains\nPDTool’s higher total workload time compared to NoIndex.\nWhile PDTool consumes a high recommendation time in the\nfirst round, it results in a better per round total workload\nand execution times than NoIndex.\nBalancing the configuration fitness between transac-\ntional and analytical workloads is the prime concern of\nindex tuning in HTAP environments. How tools achieve\nthis balance can be better understood by breaking the ex-\necution cost into analytical and transactional components.\nAs Figure 5 demonstrates, MAB configuration provides\nbetter execution time for both analytical and transactional\nworkload components compared to both PDTool and MCTS.\nMAB provides 19.9% and 42.5% better execution times by\nthe 25thround for analytical and transactional workloads,\nrespectively, compared to the second best option (MCTS in\nthe transactional and PDTool in the analytical cost). In initial\nrounds, MAB performance is inferior to PDTool in trans-\nactional execution time, but it quickly learns the negative\nimpact of indices on the transactional workload. By the 4th\nround, MAB surpasses PDTool in transactional execution\ntime by dropping indices with negative rewards. While\nremoving the unnecessary indices, MAB makes sure not to\nimpact the analytical execution times by keeping the high\nreward indices intact. MAB performs several configuration\nchanges in rounds 15–17, which results in a sudden oscilla-\ntion in transactional execution time, but these configuration\nchanges allow the bandit to find a superior configuration\nin both analytical and transactional execution costs. There\nis some variability in transactional execution costs even\nwith the same number of transactions in each round, as\nthe number of statements per round can be different (e.g.,\ndifferent new orders can have a different number of items in\na transaction, leading to a different number of statements).\nMCTS performance under the static experiments are less\nsatisfactory due to its limitations in the action space and\nlonger training times. These issues will be compoundedThis article has been accepted for publication in IEEE Transactions on Knowledge and Data Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TKDE.2023.3271664\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11\nTABLE 1\nHTAP: Total workload time breakdown for HTAP workloads (in min): the best choice is in bold text.\nWorkload Recommendation Index Creation Execution Analytical Transactional Total\nMAB PDT MAB PDT MAB PDT MAB PDT MAB PDT MAB PDT\nCH 0.1 79.15 6.67 1.86 76.6 76.72 55.02 48.74 21.58 27.98 83.37 157.73\nTPC-H 0.14 14.09 9.41 6.43 87.41 81.82 57.42 53.4 29.99 28.42 96.96 102.35\nTPC-H Sk. 0.15 13.92 14.79 12.74 77.29 102.29 37.19 65.45 40.11 36.84 92.22 128.94\nStatic\n020004000600080001000012000\nNoIndex\nPDTool\nMAB\nNoIndex\nPDTool\nMAB\nTPC-H (HTAP) TPC-H Skew (HTAP)Total Workload Time (sec)\nBenchmark\nFig. 6. MAB vs. PDTool vs. NoIndex total end-to-end workload time for\nstatic TCP-H and TPC-H skew HTAP workloads.\nwith dynamic workloads which would require multiple\ntraining sessions. Therefore, in the remaining experiments,\nwe compare MAB against PDTool (the strongest competi-\ntor).\nFurther analysis of additional RL approaches under\nanalytical workloads can be found in [40]. Those experi-\nments demonstrate that deep RL’s randomised exploration\nof the vast state-action space and challenging hyperparam-\neter tuning contributes to the solution volatility, whereas\nMAB typically provides better convergence and simpler\nimplementation.\n7.1.2 The Impact of Data Skew in HTAP Workloads\nWe now experiment with TPC-H and TPC-H Skew HTAP\nworkloads to demonstrate the impact of the addition of\ntransactional statements to well-known OLAP benchmarks.\nWe experiment with a similar number of transactional\nstatements as in the 3:1 TAR CH-BenCHmark experiment.\nThe OLTP part of the workload is composed of 6 templates\n(two insert templates, two delete templates and two update\ntemplates). We use the original data generation tools to\ngenerate the INSERT, DELETE and UPDATE statements for\nORDER and LINEITEM tables. The transactional workload\nused here is less complex than CH-BenCHmark but suffi-\ncient to demonstrate the impact of HTAP workloads on the\noverall performance.\nAs shown in Figure 6, MAB performs better in both TPC-\nH and TPC-H skew HTAP workloads. Interestingly, MAB\nachieves 5.2% better total workload time under the TPC-H\nbenchmark, which is usually favourable to PDTool. MAB\nconverges to a similar performant configuration, comparing\nthe final round execution cost. However, with MAB’s longer\nexecution times in the first few rounds, PDTool achieves a\n6% better total execution time. Due to the higher recom-\nmendation time of PDTool, it has a higher total workload\nDynamic\n505005000\n0 20 40 60 80 100 120 140 160 180 200 220Total Time Per Round (sec)\nRound NumberPDTool\nMABFig. 7. MAB vs. PDTool total-workload time convergence under CH-\nBenCHmark for dynamic workloads with different transaction levels (log\ny-axis)\n050100150200250300350\n0 20 40 60 80 100 120 140 160 180 200 220Total Exec. Time Per Round (sec)\nRound NumberPDTool\nMAB\nFig. 8. MAB vs. PDTool total execution time convergence under CH-\nBenCHmark for dynamic workloads with different transaction levels\ntime. In TPC-H Skew, MAB dominates the PDTool across\nall dimensions, having 28.4% better total workload time and\n24% better execution time. All HTAP results are summarised\nin Table 1.\n7.1.3 Dynamic HTAP Workloads\nThis experiment gradually increases and decreases the\ntransactional workload component over the rounds. We\nstart with 0:1 TAR, which is purely analytical, and then we\nadd transactional workload sets one by one till we reach\n5:1 TAR. Afterwards, we gradually reduce the transactional\nworkload sets one by one to reach 0:1 TAR again. We run 20\nrounds in each TAR.\nAfter each workload change, PDTool is invoked with\nthe new workload from the previous round, which is a\ngood representation of the next 19 rounds. It is essential\nto provide the workload from at least one complete round\nbecause PDTool considers the transactional to analytical ra-\ntio when making recommendations. However, as observable\nfrom Figure 7, each invocation of PDTool takes a substantial\namount of time for larger workloads in higher transaction\nlevels.This article has been accepted for publication in IEEE Transactions on Knowledge and Data Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TKDE.2023.3271664\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12\n050100150200250300\n0 20 40 60 80 100 120 140 160 180 200 220Analytical Exec. Time Per Round (sec)\nRound NumberPDTool\nMAB\nFig. 9. MAB vs. PDTool analytical execution time convergence under\nCH-BenCHmark for dynamic workloads with different transaction levels\n020406080100120140160180\n0 20 40 60 80 100 120 140 160 180 200 220Trans. Exec. Time Per Round (sec)\nRound NumberPDTool\nMAB\nFig. 10. MAB vs. PDTool transactional execution time convergence\nunder CH-BenCHmark for dynamic workloads with different transaction\nlevels\nMAB performs most of the configuration changes at the\nstart of the experiment and then after the first workload\nchange (0:1 →1:1). Understandably, MAB needs to explore\nextensively at the start of the experiment. However, a similar\nlevel of exploration for the first workload change might not\nbe as intuitive, given that we do not see such exploration\nfrom MAB for the rest of the experiment. In round 21,\nMAB is exposed to transactional statements for the first\ntime in this experiment. As a result, MAB performs more\nexploration and learns the negative impact of indices in\nthese rounds and thus performs much better after round\n40. Ultimately, MAB provides a 57.8% speed up in the total\nworkload time compared to PDTool. A significant portion of\nthis speed-up is attributed to MAB’s lower recommendation\ncost compared to PDTool.\nTo compare the different configurations proposed by the\ntools, we plot execution time over the rounds in Figure 8.\nAfter the first two transaction levels (i.e., after round 40),\nMAB always manages to lock into a superior configuration\nproviding faster execution time. As a result, MAB provides\nan 11% speed-up in total execution cost.\nIn the entire experiment, we go through the same TAR\ntwo times (except for 5:1 TAR), which results in similar\nworkloads. However, from Figure 8, it is noticeable that\nPDTool reaches higher execution costs in the descending\npart of the experiment (after round 120) compared to the\nascending part of the experiment (rounds 1 to 120). Con-\nfigurations proposed by the PDTool depend on the existing\nsecondary indices present in the system and the underly-\ning data. Continuous additions and deletions change the\nunderlying data, partially invalidating the statistics usedby the optimiser. Furthermore, at each PDTool invocation,\nthe system has a different starting set of secondary indices,\nimpacting PDTool’s recommendations. Therefore the recom-\nmendations proposed for similar workloads in ascending\nand descending parts of the graph are different. For exam-\nple, rounds 80–100 and 120–140 run a 4:1 TAR workload,\nwhereas PDTool proposes two very different configurations\nfor these two sections. While it proposes only 18 indices at\nround 81, 27 are proposed in round 121, with only 11 indices\nbeing shared across 2 configurations. On the other hand,\nMAB converges quickly in the later experiment rounds,\ntaking advantage of the already obtained knowledge.\nTo further analyse MAB’s gain in the dynamic exper-\niment, we need to break down the execution time into\nanalytical and transactional components. As one can ob-\nserve from Figures 9 and 10, MAB obtains the gain mainly\nfrom the transactional workload. MAB provides 4.5% better\nanalytical execution cost and 22.6% better transactional ex-\necution cost compared to the PDTool. As observable from\nFigure 9, MAB is obtaining a noticeable analytical gain in\n2:1 and 3:1 TARs. In the analytical heavy workloads (0:1,\n1:1 TARs), PDTool records a better or similar analytical\nexecution time. MAB opts for the transactional friendly con-\nfiguration for transactional heavy workloads (4:1, 5:1 TARs),\nreducing thereby the analytical execution time gain. On the\ntransactional end, MAB leads in almost all workloads. As\nexpected, transactional execution cost gain increases for the\ntransactional heavy workloads.\nImpact of data skewness on dynamic HTAP workloads:\nThis section explores the compound effect of data skewness\nand dynamic workloads. We experiment with a dynamic\nHTAP TPC-H skew workload to demonstrate this effect.\nWe run a shifting workload with different TARs similar to\nthe dynamic ch-BenCHmark, where each shift runs for 15\nrounds. There are three shifts (0.5 ×, 1×, 2×TARs) with\n45 rounds in total. Here 1 ×represents the TAR used in\nthe static TPC-H skew HTAP experiment. However, in this\nexperiment, we shift the analytical workload as well. All\nanalytical templates are divided into three almost equal\nsize sets and used one set per shift. In this experiment,\nMAB provided an 82.51% gain in total workload time and a\n22.77% gain in total execution time.\n7.1.4 Space Savings Under HTAP Workloads\nIn the case of index tuning of HTAP workloads, more\nindices can result in higher running time for transactional\ncomponents of the workload. Consequently, a minimal in-\ndex set can be optimal for a transaction-heavy workload.\nWhile such a configuration might be suboptimal for the\nanalytical component of the workload, a minimal config-\nuration can result in a better total workload execution\ntime due to the significant savings obtained from avoiding\nindex maintenance activities stemming from transactional\nstatements. Our experiments observed that PDTool usually\nexploits the entire given memory budget, resulting in a\nhigher transactional execution cost.8\n8. We have observed that PDTool sometimes goes over the given\nbudget due to errors in index size estimations. On the other hand, MAB\ninitially estimates the index size based on the statistics and corrects the\nestimate after indices are materialised for the first time and therefore\ndoes not suffer from the same issue.This article has been accepted for publication in IEEE Transactions on Knowledge and Data Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TKDE.2023.3271664\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13\nMemory\n020004000600080001000012000\n0 5 10 15 20 25Memory (MB)\nRound NumberPDTool\nMAB\n050100150200250300350400450\n0 5 10 15 20 25Total Exec. Time Per Round (sec)\nRound NumberPDTool\nMAB\n(a)\nMemory\n020004000600080001000012000\n0 5 10 15 20 25Memory (MB)\nRound NumberPDTool\nMAB\n050100150200250300350400450\n0 5 10 15 20 25Total Exec. Time Per Round (sec)\nRound NumberPDTool\nMAB (b)\nFig. 11. MAB vs. PDTool convergence under CH-BenCHmark for static\nworkloads with 5:1 TAR: (a) Memory use, (b) Total execution time.\n020406080100120140160180\n0 5 10 15 20 25Trans. Exec. Time Per Round (sec)\nRound NumberPDTool\nMAB\n050100150200250300350400\n0 5 10 15 20 25Analytical Exec. Time Per Round (sec)\nRound NumberPDTool\nMAB\n(a)\n020406080100120140160180\n0 5 10 15 20 25Trans. Exec. Time Per Round (sec)\nRound NumberPDTool\nMAB\n050100150200250300350400\n0 5 10 15 20 25Analytical Exec. Time Per Round (sec)\nRound NumberPDTool\nMAB (b)\nFig. 12. MAB vs. PDTool convergence under CH-BenCHmark for static\nworkloads with 5:1 TAR: (a) Transactional Execution cost, (b) Analytical\nExecution cost.\nOn the flip side, MAB learns the negative impact of\nindices on transactional statements and dynamically adjusts\nthe configuration. However, this behaviour was not visible\nwith fully analytical workloads. The index size context\nfeature typically carries a negative weight due to negative\nrewards from index creation operations and forces the ban-\ndit to choose the smaller arms that provide the best gains in\nexecution cost.\nAt 5:1 TAR, MAB provides a configuration that yields\nan 83% memory saving while achieving an 8.8% execution\ntime gain by the last round (see Figure 11). This execution\ncost gain is smaller than the gain we observed under trans-\naction level 3, as the usefulness of indices reduces when the\nworkload becomes transactions heavy.\nWhile it might be counter-intuitive for an index tuning\ntool to use less memory to provide better performing con-\nfigurations, it can be easily understood by observing the\nanalytical and transactional execution times of both tools by\nthe last round (see Figure 12). Comparing the last round\nconfigurations of both tools, PDTool creates an analytical\nfriendly configuration that provides a 27% speed-up in an-\nalytical execution time (around 40 second gain per round).\nOn the other hand, MAB locks into a smaller configuration\nthat is more suitable for transactional statements providing\na 60% speed-up in transactional execution cost (around 60\nsecond gain per round). Ultimately MAB provides an 8.8%\nspeed-up in total execution time while offering a significantmemory saving.\n7.1.5 Impact of restricted recommendation times under\nHTAP workloads\nIn all experiments, we run with unrestricted running\ntimes for PDTool in search of the best recommendation\nquality. Some tuning tools, like PDTool, can restrict the\ntuning session to a time provided by the user [41]. While\nthis can negatively impact the recommendation quality (and\nin turn execution time), this restriction can reduce the total\nworkload time.\nTo shed some light on this issue, we experiment with\nCH-BanCHmark 3:1 TAR workload providing different tun-\ning times to the PDTool (See Table 2). We notice an apparent\nincrease in execution time when we reduce the recommen-\ndation time, impacting the total workload time in the long\nrun. However, in our 25-round experiments, the reduction in\nrecommendation time leads to better total workload times\nfor the PDTool. Nevertheless, MAB’s total workload time\nremains superior.\nAs observable in Table 2, when given a tuning time less\nthan the minimum, PDTool will end without providing any\nrecommendations. Unfortunately, identifying the minimum\ntime requirement for the tuning session is next to impossible\n(other than through the trial and error). For example, the\n3:1 TAR experiment gives the first recommendation at 30\nminutes, whereas under the 5:1 TAR workload, PDTool took\n60 minutes.\n7.1.6 Impact on Recommendation Quality of Including In-\ndex Maintenance Time in Rewards\nThis section tests the impact of including the index\nmaintenance time in the reward. We experiment against the\nOLAP version of the bandit, which only considers the index\ncreation time and query execution time. While the OLAP\nversion focuses on improving the data scan gains entirely,\nthe HTAP version tries to balance the gain from the data\nscans and the negative impact of the index maintenance. We\nrun a CH-BenCHmark 5:1 TAR workload for 25 rounds. In\nthis experiment, we notice that the OLAP version provides a\n5.33-minute gain in analytical execution time, while causing\na 13.02-minute loss in transactional execution time. Overall,\nthe HTAP version provides a 7.68-minute gain in total\nexecution time. This experiment shows that HTAP reaches\na better balance by considering both data scan gains and\nindex maintenance overheads.\nTABLE 2\nHTAP: Total workload time breakdown for CH-BenCHmark 3:1 TAR\nworkloads (in min)\nComponent Recommend Creation Execution Total\nMAB 0.1 6.67 76.6 83.37\nPDT (15 mins) No Recommendations\nPDT (30 mins) 30.64 2.34 84.74 117.72\nPDT (60 mins) 60.67 2.11 81.17 143.95\nPDT (original) 79.15 1.86 76.72 157.73This article has been accepted for publication in IEEE Transactions on Knowledge and Data Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TKDE.2023.3271664\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 14\nTABLE 3\nTotal time breakdown for analytical workloads (in min): the best choice is in bold text.\nWorkloadRecommendation Creation Execution Total\nPDTool (#) MAB PDTool MAB PDTool MAB PDTool MABStaticSSB 0.34 (0.34) 0.02 0.95 1.86 12.9 13.15 14.19 15.03\nTPC-H 0.6 (0.6) 0.08 2.45 5.66 46.35 55.64 49.4 61.38\nTPC-H Sk. 0.58 (0.58) 0.11 8.37 19.82 54.17 32.06 63.12 51.99\nTPC-DS 44.86 (44.86) 1.53 1.45 5.94 302.63 242.15 348.94 249.62\nIMDB 0.34 (0.34) 0.31 1.1 1.3 11.01 9.42 12.41 11.03DynamicSSB 1.28 (0.32) 0.05 1.5 2.21 5.42 5.69 8.2 7.95\nTPC-H 1.55 (0.32) 0.12 9.36 9.74 26.35 25.14 37.25 35\nTPC-H Sk. 1.65 (0.41) 0.16 14.98 20.96 85.49 21.44 102.11 42.56\nTPC-DS 11.13 (2.78) 1.66 6.08 16.48 187.08 155.65 204.29 173.79\nIMDB 3.09 (0.77) 0.29 1.59 2.24 11.21 7.93 15.89 10.46RandomSSB 2.83 (0.57) 0.02 1.77 2.37 26.59 16.83 30.85 19.22\nTPC-H 7.55 (1.51) 0.08 14.68 7.06 84.14 80.43 106.37 87.57\nTPC-H Sk. 3.3 (0.66) 0.08 31.74 34.68 48.71 39.44 83.75 74.2\nTPC-DS 310.22 (62.04) 1.4 8.23 19.81 323.57 227.02 642.01 248.24\nIMDB 14.74 (2.94) 0.28 2.72 1.14 48.55 14.47 66.01 15.89\n# The average time of a single PDTool invocationTABLE 4\nTotal end-to-end workload time for\nstatic analytical workloads under\ndifferent database sizes (in min)\nWorkload SF PDTool MAB\nTPC-H1 2.02 2.03\n10 49.4 61.38\n100 891.01 793.40\nTPC-H\nSkew1 4.17 3.83\n10 63.12 51.99\n100 2640.64 1219.33\nTABLE 5\nTotal time breakdown for analytical TPC-H Skew workloads under\ndifferent round sizes (in min)\nRound size Rec. Creation Execution Total\nSingle Query 1.11 27.77 30.16 59.04\n0.5x 0.13 22.39 30.39 52.92\n1x 0.11 19.82 32.06 51.99\n2x 0.08 12.66 43.53 56.27\n7.2 MAB vs PDTool Under Analytical Workloads\nIn addition to the static and dynamic workloads, we incor-\nporate random workloads for analytical testing.9Random\nexperiments test the delicate balance between swift and\ncareful adaptation under returning workloads, which can\nlead to unwanted index oscillations. As presented in Table 3,\nunder all three analytical settings (static, dynamic, and ran-\ndom, MAB achieves a faster total workload time, except for\nSSB and TPC-H static analytical workloads. Consistent with\nthe HTAP experiments, fully analytical workloads on uni-\nform datasets work as the best case for offline tuning tools.\nHowever, when underlying data is skewed or dynamic, rec-\nommendations based on a pre-determined workload alone\ncan have unfavourable outcomes.\nThe main experiments used skewed datasets with Zip-\nfian factor 4 (and uniform datasets with Zipfian factor 0).\nTo further investigate the impact of the degree of data skew,\nwe experiment with different Zipfian factors ranging from\n1 to 3 with analytical workloads. As shown in Figure 13\nunder Zipfian factors 2 and 3, MAB demonstrates over 51%\nand 58% performance gain against PDTool, respectively.\nWhereas under Zipfian factor 1, PDTool outperforms MAB\nby 16%. PDTool missing the index on Orders.O custkey\nappears to be more costly with Zipfian factors 2 and 3,\nmainly affecting Q22. An in-depth analysis of the solution\nfitness on analytical workloads can be found in [40].\n7.2.1 The Impact of Database Size\nTo examine the impact of database size, we run TPC-H\nuniform and TPC-H Skew benchmarks with static analytical\nworkloads on SF 1, 10 and 100 databases. Under SF 10, MAB\n9. A query sequence is chosen entirely at random (modelling more\ndynamic settings, such as cloud services)performs better in the case of TPC-H Skew and PDTool\nperforms better on TPC-H (see Table 4). The impact of\nsub-optimal index choices is even more evident for larger\ndatabases, leading to a huge gap between total workload\ntimes of MAB and PDTool for TPC-H Skew (44 hours in the\nformer vs 20 hours in the latter case). In TPC-H, PDTool\nresults in a higher total workload time (14.8 hours vs. 13.2\nhours for MAB). This is mainly due to sub-optimal opti-\nmiser decisions, where the optimiser favours the usage of\nindices (coupled with nested loops joins) when alternative\nplans would be a better option. For instance, under the\nrecommended indices from PDTool, some instances of Q5\nrun longer than 8 minutes (using index nested loops join),\nwhereas others finish in 1.5 minutes (using a plan based\non hash joins). We notice that, with larger database sizes,\nexecution time dominates contributing more than 91% to the\ntotal workload time. We observe faster and more accurate\nconvergence of MAB under larger databases, due to a clear\ndifference between rewards for different arms, highlighting\nMAB’s excellent potential for larger databases.\n7.2.2 Hypothetical Index Creation vs Actual Index Creation\nManaging the exploration-exploitation balance under a\nlarge number of candidate indices, with an enormous num-\nber of combinatorial choices, is non trivial. PDTool explores\nusing the “what-if” analysis, which comes under the tool’s\nrecommendation time, whereas MAB explores using index\ncreations.\nComparing the total of recommendation and index cre-\nation times (henceforth referred to as exploration cost ) be-\ntween MAB and PDTool presents a clear picture about these\ntwo exploration methods. From Table 3 we can observe\nthat, in most cases (9 out of 15) MAB archives a better\nexploration cost compared to PDTool when running analyt-\nical workloads. However when the workload is small (e.g.,\ndynamic shifting) PDTool tends to perform better. TPC-DS,\nwith the highest number of candidate indices among these\nbenchmarks (over 3200 indices), provides a great test case\nfor exploration efficiency. Under TPC-DS, MAB exploration\ncost is significantly lower in shifting and random settings,\nand marginally higher in the static setting. Despite the effi-\ncient exploration, MAB does not sacrifice recommendation\nquality in any way (achieving faster execution times in 12This article has been accepted for publication in IEEE Transactions on Knowledge and Data Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TKDE.2023.3271664\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 15\n0500010000150002000025000\nPDTool\nMAB\nPDTool\nMAB\nPDTool\nMAB\nPDTool\nMAB\nPDTool\nMAB\n0 1 2 3 4Total Workload Time (sec)\nDatabase Skewness (z)Recommendation\nIndex Creation\nExecution\nFig. 13. MAB vs. PDTool total end-to-end workload time under TPC-H\nskew static analytical workloads with different Zipfian factors (z)\nout of 15 cases, with significantly faster execution across all\nTPC-DS experiments).\nThis efficient exploration is promoted by the linear\nreward-context relationship along with C2UCB’s weight\nsharing (Section 3), resulting in a small number of param-\neters to learn. An arm’s identity becomes irrelevant and\ncontext (Section 4) becomes the sole determining factor of\neach arm’s expected score, which allows MAB to predict the\nUCB of a newly arriving arm with known context without\ntrying it even once .\n7.2.3 The Impact of Round Size\nIn the original TPC-H Skew static analytical experiment (1x),\neach bandit round includes all the benchmark templates (22\nqueries). To analyse the impact of the round size (bandit\ninvocation frequency), we conduct experiments with single-\nquery (1 query), 0.5x (11 queries) and 2x (44 queries) round\nsizes using the TPC-H Skew analytical workload. All three\nround sizes converge to the same performant configurations\nby the last round. We observe a faster convergence with\nsmall round sizes, resulting in lower execution costs in\nthe first few rounds. While the execution cost gain from\n1x to 0.5x is noticeable, dividing the round further (single\nquery) does not provide a considerable benefit compared\nto the added creation and recommendation overhead. With\nlarger round sizes, we observe lower creation costs due to\nless frequent bandit updates (see Table 5). MAB performs\nbetter under all round-sizes compared to PDTool. A DBA\ncan decide on the round size (bandit invocation frequency)\nbased on the application and DBA’s primary goal (faster\nconvergence vs lower creation cost). We leave auto-tuning\nof this parameter as an interesting future work avenue.\n7.2.4 The Impact of Focused Updates\nThis section analyses the impact of focused updates on the\nconvergence speed. As shown in Figure 14, under both TPC-\nH and TPC-H Skew analytical workloads, there is a clear im-\nprovement in convergence speed. Faster convergence results\nin 17% and 20% gain in total execution time and 21% and\n28% gain in total workload time under TPC-H and TPC-H\nskew benchmarks with focused updates, respectively.\n0100200300400500600700\n0 5 10 15 20 25Total Time Per Round (sec)\nRound NumberMAB w/o focus\nMAB w/ focus\n050100150200250300350400450\n0 5 10 15 20 25Total Time Per Round (sec)\nRound NumberMAB w/o focus\nMAB w/ focus(a)\n050100150200250300350400450\n0 510 15 20 25 30Total Time Per Round (sec)\nRound NumberMAB w/o focus\nMAB w/ focus\n0100200300400500600700\n0 5 10 15 20 25Total Time Per Round (sec)\nRound NumberMAB w/o focus\nMAB w/ focus (b)\nFig. 14. MAB end-to-end workload time convergence with and without\nfocus update for static analytical workloads: (a) TPC-H, (b) TPC-H Skew\n7.3 Experimental Results Summary\nThis section summarises the experimental results. In an\nexperiment against a uniform dataset and 6 TARs, MAB\nshowed a 28.9% and 20.8% gain on average compared to\nPDTool and MCTS, respectively. Diving deeper into the 3:1\nTAR, we noticed that MAB provides improvements in both\ntransactional execution cost and analytical execution cost.\nExecution cost gain from MAB peaks with more balanced\nTARs. With workloads that are at the extreme ends of the\nspectrum (either purely analytical or purely transactional\nqueries), both PDTool and MAB provide similar execu-\ntion times. However, large PDTool recommendation times\nwere observed with transactional heavy workloads. Besides\nbetter performance, we noticed that MAB also provides\nremarkable memory saving with transactional heavy work-\nloads (up to 83%).\nUsing learned knowledge, MAB performed much bet-\nter than PDTool in dynamic experiments. On the other\nhand, PDTool’s performance degraded in the dynamic set-\nting when index recommendations had to consider existing\nindices. Furthermore, we demonstrated the superiority of\nMAB-based PDS tuning over different bandit update fre-\nquencies and with large databases. Finally, we demonstrated\nthat under analytical workloads, MAB outperforms PDTool\nin 13/15 experiments, showcasing the robustness of the\nMAB framework.\n8 R ELATED WORK\nHTAP . HTAP workloads are composed of online transaction\nprocessing (OLTP) workloads and online analytical pro-\ncessing (OLAP) workloads. While most existing analytical\nsystems depend on data pipelines writing to a separate\ndata warehouse for OLAP queries, such an approach lim-\nits the users from running analytics on fresh operational\ndata. Research has targeted hybrid environments that can\ncater to OLTP statements and OLAP queries. The last few\nyears have witnessed the emergence of HTAP focused\ndatabase architectures, platforms and databases [42], [43],\n[44], [45], [46], [47], [48], commercial tools [49], [50], [51], and\nbenchmarks [32], [33], [52]. This rapid growth of research\nand commercial interest in HTAP environments highlights\nan important point of efficiently processing analytical and\ntransactional statements over the same dataset.This article has been accepted for publication in IEEE Transactions on Knowledge and Data Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TKDE.2023.3271664\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 16\nAutomated physical design tuning. Most commercial\nDBMS vendors nowadays offer physical design tools in their\nproducts [1], [2], [3]. These tools rely heavily on the query\noptimiser to compare benefits of different design structures\nwithout materialisation [16]. Such an approach is ineffective\nwhen base data statistics are unavailable, skewed, or change\ndynamically [10]. In these dynamic environments, the prob-\nlem of physical design is aggravated: a) deciding when to\ncall a tuning process is not straightforward; and b) deciding\nwhat is a representative training workload is a challenge.\nOnline physical design tuning. Several research\ngroups have recognised these problems and have offered\nlightweight solutions to physical design tuning [11], [12],\n[13]. While such solutions are more flexible and need not\nknow the workload in advance, they are typically limited\nin terms of applicability to new unknown workloads (gen-\neralisation beyond past), and do not come with theoret-\nical guarantees that extend to actual runtime conditions.\nMoreover, by giving the optimiser a central role, the tools\nremain susceptible to its mistakes [9]. [8] extends [1] with\nthe use of additional components, in a narrowed scope of\nindex selection to mimic an online tool. This takes corrective\nactions against the optimiser mistakes through a validation\nprocess.\nAdaptive and learning indices. Another dimension of\nonline physical design tuning is database cracking and\nadaptive indexing that smooth the creation cost of indices\nby piggybacking on query execution [53], [54]. Recent efforts\nhave gone a step further and proposed replacing data struc-\ntures with learned models that are smaller in size and faster\nto query [55], [56]. Such approaches are complementary\nto our efforts: once the data structures (or models) are\nmaterialised inside a DBMS, the MAB framework can be\nused to automate the decision making as to which data\nstructure should be used to speed-up query analysis.\nLearning approaches to optimisation and tuning.\nRecent years have witnessed new machine learning ap-\nproaches to automate decision-making processes within\ndatabases. For instance, reinforcement learning approaches\nhave been used for query optimisation and join order-\ning [57], [58], [59], [60]. In [9], regression has been used to\nsuccessfully mitigate the optimiser’s cost misestimates as a\npath toward more robust index selection. [9] shows promis-\ning results when avoiding query regressions. However, this\nclassifier incurs up to 10% recommendation time, impacting\nrecommendation cost in all cases, especially where recom-\nmendation cost already dominates the cost for PDTool (e.g.,\nTPC-DS, IMDb).\nWhen it comes to tuning, the closest approaches employ\nvariants of RL for index selection or partitioning [23], [30],\n[31], [61], [62] or configuration tuning [5], [29]. [62] describes\nRL-based index selection, which depends solely on the\nrecommendation tool for query-level recommendations and\nis affected by decision combinatorial explosion, both issues\naddressed in our work. Unlike its more general counter-\npart (RL), MABs have advantages of faster convergences,\nsimpler implementation, and theoretical guarantees [40].\nThere has also been recent interest in using bandits for\ndatabase tasks such as monitoring, query optimisation and\njoin ordering [63], [64], [65].\nUse of learned cost/cardinality estimators and costmodels. Learned cost estimators and models [66], [67] allow\naccurate and faster cost estimations and provide better\nexecution plans. Better estimations and models can be par-\nticularly beneficial for avoiding estimation errors in offline\noptimiser-based tools like PDTool. Even learned systems\nlike our MAB system can benefit from such cost models\nto avoid the cold start problem [68]. These learned models\nand estimations will require using ‘optimiser hints’, forcing\nthe optimiser to use a different query plan than its original\nchoice. However, when used externally with commercial\nsystems, flexibility in optimiser hints will be limited. Fur-\nthermore, such learned solutions suffer from long training\ntimes [66], which will be problematic given that PDTool\nalready suffers from long recommendation times.\nWorkload compression. Large complex workloads have\nbeen a challenge for index tuning tools. This is visible from\nhigh recommendation times in PDTool and high creation\ntimes in HMAB under CH-BenCHmark. Workload com-\npression [69], [70], [71] can alleviate these challenges by\nefficiently identifying a small subset of queries that can be\nused for index tuning.\nWorkload forecasting. Both PDtool and MAB have lim-\nited visibility into the future. Understanding what a future\nworkload might look like would allow these tools to pro-\nvide better recommendations. We acknowledge the progress\nin workload forecasting [72] as a complementary research\ndirection to PDS tuning.\n9 C ONCLUSIONS\nThis paper develops a multi-armed bandit learning frame-\nwork for online index selection. This framework does not\ndepend on the DBA and the (error-prone) query optimiser\nfor index selection and learns the benefits of indices through\nstrategic exploration and observation. We justify our choice\nof MAB over general reinforcement learning for online\nindex tuning, comparing MAB against DDQN, a popular\nRL algorithm based on deep neural networks, demonstrat-\ning significantly faster convergence of the MAB. Further-\nmore, our extensive experimental evaluation demonstrates\nadvantages of MAB over an existing commercial physical\ndesign tool (up to 75% speed up, and 23% on average), and\nexemplifies robustness to data skew, unpredictable ad-hoc\nworkloads and complex HTAP environments.\nACKNOWLEDGMENTS\nWe gratefully acknowledge support from the Australian\nResearch Council Discovery Project DP220102269 as well as\nDiscovery Early Career Researcher Award DE230100366.\nREFERENCES\n[1] S. Agrawal, S. Chaudhuri, L. Koll ´ar, A. P . Marathe, V . R.\nNarasayya, and M. Syamala, “Database tuning advisor for Mi-\ncrosoft SQL Server 2005,” in VLDB , 2004, pp. 1110–1121.\n[2] D. C. Zilio, J. Rao, S. Lightstone, G. M. Lohman, A. J. Storm,\nC. Garcia-Arellano, and S. Fadden, “DB2 design advisor: Inte-\ngrated automatic physical database design,” in VLDB , 2004, pp.\n1087–1097.\n[3] B. Dageville, D. Das, K. Dias, K. Yagoub, M. Za ¨ıt, and M. Ziauddin,\n“Automatic SQL tuning in oracle 10g,” in VLDB , 2004, pp. 1098–\n1109.This article has been accepted for publication in IEEE Transactions on Knowledge and Data Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TKDE.2023.3271664\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 17\n[4] D. Zilio, S. Lightstone, K. Lyons, and G. Lohman, “Self-managing\ntechnology in IBM DB2 universal database,” in ACM CIKM , 2001,\npp. 541–543.\n[5] A. Pavlo, G. Angulo, J. Arulraj, H. Lin, J. Lin, L. Ma, P . Menon,\nT. C. Mowry, M. Perron, I. Quah et al. , “Self-driving database\nmanagement systems,” in CIDR , 2017.\n[6] C. Curino, E. P . C. Jones, R. A. Popa, N. Malviya, E. Wu, S. Mad-\nden, H. Balakrishnan, and N. Zeldovich, “Relational cloud: a\ndatabase service for the cloud,” in CIDR , 2011, pp. 235–240.\n[7] V . R. Narasayya, S. Das, M. Syamala, B. Chandramouli, and\nS. Chaudhuri, “SQLVM: Performance isolation in multi-tenant\nrelational database-as-a-service,” in CIDR , 2013.\n[8] S. Das, M. Grbic, I. Ilic, I. Jovandic, A. Jovanovic, V . R. Narasayya,\nM. Radulovic, M. Stikic, G. Xu, and S. Chaudhuri, “Automatically\nindexing millions of databases in microsoft azure sql database,” in\nSIGMOD , 2019, pp. 666–679.\n[9] B. Ding, S. Das, R. Marcus, W. Wu, S. Chaudhuri, and V . R.\nNarasayya, “AI Meets AI: leveraging query executions to improve\nindex recommendations,” in SIGMOD , 2019.\n[10] S. Chaudhuri and V . Narasayya, “Self-tuning database systems: A\ndecade of progress,” in VLDB , 2007, pp. 3–14.\n[11] K. Schnaitter, S. Abiteboul, T. Milo, and N. Polyzotis, “On-Line\nindex selection for shifting workloads,” in ICDEW , 2007, pp. 459–\n468.\n[12] K.-U. Sattler, I. Geist, and E. Schallehn, “QUIET: Continuous\nquerydriven index tuning,” in VLDB . Elsevier, 2003, pp. 1129–\n1132.\n[13] N. Bruno and S. Chaudhuri, “An online approach to physical\ndesign tuning,” in ICDE , 2007, pp. 826–835.\n[14] M. L. Kersten, S. Idreos, S. Manegold, and E. Liarou, “The re-\nsearcher’s guide to the data deluge: Querying a scientific database\nin just a few seconds,” VLDB , vol. 4, no. 12, pp. 1474–1477, 2011.\n[15] I. Alagiannis, R. Borovica, M. Branco, S. Idreos, and A. Ailamaki,\n“NoDB: Efficient query execution on raw data files,” in SIGMOD ,\n2012, pp. 241–252.\n[16] S. Chaudhuri and V . Narasayya, “AutoAdmin “what-if”; index\nanalysis utility,” in SIGMOD , 1998, pp. 367–378.\n[17] S. Christodoulakis, “Implications of certain assumptions in\ndatabase performance evaluation,” TODS , vol. 9, no. 2, pp. 163–\n186, 1984.\n[18] V . Leis, A. Gubichev, A. Mirchev, P . Boncz, A. Kemper, and\nT. Neumann, “How good are query optimizers, really?” VLDB ,\nvol. 9, no. 3, pp. 204–215, Nov. 2015.\n[19] A. Aboulnaga and S. Chaudhuri, “Self-tuning histograms: Build-\ning histograms without looking at data,” ACM SIGMOD Record ,\nvol. 28, no. 2, pp. 181–192, 1999.\n[20] R. Borovica-Gajic, S. Idreos, A. Ailamaki, M. Zukowski, and\nC. Fraser, “Smooth scan: Robust access path selection without\ncardinality estimation,” The VLDB Journal , vol. 27, no. 4, pp. 521–\n545, 2018.\n[21] R. Borovica, I. Alagiannis, and A. Ailamaki, “Automated physical\ndesigners: What you see is (not) what you get,” in DBTest , 2012,\np. 9.\n[22] K. E. Gebaly and A. Aboulnaga, “Robustness in automatic physi-\ncal database design.” in EDBT , vol. 261. ACM, 2008, pp. 145–156.\n[23] A. Sharma, F. M. Schuhknecht, and J. Dittrich, “The case for au-\ntomatic database administration using deep reinforcement learn-\ning,” 2018, unpublished.\n[24] L. Qin, S. Chen, and X. Zhu, “Contextual combinatorial bandit and\nits application on diversified online recommendation,” in SDM ,\n2014, pp. 461–469.\n[25] L. Li, W. Chu, J. Langford, and R. E. Schapire, “A contextual-\nbandit approach to personalized news article recommendation,”\ninWWW , 2010, pp. 661–670.\n[26] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher, “An analysis\nof approximations for maximizing submodular set functions–i,”\nMathematical programming , vol. 14, no. 1, pp. 265–294, 1978.\n[27] Y. Abbasi-Yadkori, D. P ´al, and C. Szepesv ´ari, “Improved algo-\nrithms for linear stochastic bandits,” in NIPS , vol. 11, 2011, pp.\n2312–2320.\n[28] J. Kossmann, S. Halfpap, M. Jankrift, and R. Schlosser, “Magic\nmirror in my hand, which is the best in the land? an experimental\nevaluation of index selection algorithms,” VLDB , vol. 13, no. 12,\npp. 2382–2395, 2020.\n[29] J. Wang, I. Trummer, and D. Basu, “Udo: universal database\noptimization using reinforcement learning,” VLDB , vol. 14, no. 13,\npp. 3402–3414, 2021.[30] W. Wu, C. Wang, T. Siddiqui, J. Wang, V . Narasayya,\nS. Chaudhuri, and P . A. Bernstein, “Budget-aware index\ntuning with reinforcement learning,” in Proceedings of\nthe 2022 International Conference on Management of Data ,\nser. SIGMOD ’22. New York, NY, USA: Association for\nComputing Machinery, 2022, p. 1528–1541. [Online]. Available:\nhttps://doi.org/10.1145/3514221.3526128\n[31] X. Zhou, L. Liu, W. Li, L. Jin, S. Li, T. Wang, and J. Feng, “Au-\ntoindex: An incremental index management system for dynamic\nworkloads,” in 2022 IEEE 38th International Conference on Data\nEngineering (ICDE) , 2022, pp. 2196–2208.\n[32] R. Cole, F. Funke, L. Giakoumakis, W. Guy, A. Kemper,\nS. Krompass, H. Kuno, R. Nambiar, T. Neumann, M. Poess et al. ,\n“The mixed workload ch-benchmark,” in DBTest , 2011, pp. 1–6.\n[33] D. E. Difallah, A. Pavlo, C. Curino, and P . Cudre-Mauroux,\n“Oltp-bench: An extensible testbed for benchmarking relational\ndatabases,” VLDB , vol. 7, no. 4, pp. 277–288, 2013.\n[34] TPC, “TPC-H benchmark,” http://www.tpc.org/tpch/.\n[35] Microsoft, “TPC-H skew bench-\nmark,” https://www.microsoft.com/en-\nus/download/details.aspx?id=52430.\n[36] TPC, “TPC-C benchmark,” http://www.tpc.org/tpcc/.\n[37] R. M. Perera, “Tpc-h and tpc-h skew htap workload,”\nhttps://github.com/malingaperera/TPC HHTAP , 2021.\n[38] R. O. Nambiar and M. Poess, “The making of tpc-ds,” in VLDB .\nVLDB Endowment, 2006, p. 1049–1058.\n[39] P . O. Neil, B. O. Neil, and X. Chen, “Star schema benchmark,”\n2009, unpublished.\n[40] R. M. Perera, B. Oetomo, B. I. Rubinstein, and R. Borovica-Gajic,\n“Dba bandits: Self-driving index tuning under ad-hoc, analytical\nworkloads with safety guarantees,” in ICDE . IEEE, 2021, pp.\n600–611.\n[41] S. Chaudhuri and V . R. Narasayya, “Anytime\nalgorithm of database tuning advisor for mi-\ncrosoft sql server.” https://www:microsoft:com/en-\nus/research/publication/anytime-algorithm-of-databasetuning-\nadvisor-for-microsoft-sql-server, 2020.\n[42] R. Appuswamy, M. Karpathiotakis, D. Porobic, and A. Ailamaki,\n“The case for heterogeneous htap,” in CIDR , 2017.\n[43] J. Arulraj, A. Pavlo, and P . Menon, “Bridging the archipelago\nbetween row-stores and column-stores for hybrid workloads,” in\nSIGMOD , 2016, p. 583–598.\n[44] M. Athanassoulis, K. S. Bøgh, and S. Idreos, “Optimal column\nlayout for hybrid workloads,” VLDB , vol. 12, no. 13, p. 2393–2407,\nSep. 2019.\n[45] D. Makreshanski, J. Giceva, C. Barthels, and G. Alonso, “Batchdb:\nEfficient isolated execution of hybrid oltp+olap workloads for\ninteractive applications,” in SIGMOD . Association for Computing\nMachinery, 2017, p. 37–50.\n[46] D. Huang, Q. Liu, Q. Cui, Z. Fang, X. Ma, F. Xu, L. Shen, L. Tang,\nY. Zhou, M. Huang, W. Wei, C. Liu, J. Zhang, J. Li, X. Wu, L. Song,\nR. Sun, S. Yu, L. Zhao, N. Cameron, L. Pei, and X. Tang, “Tidb:\nA raft-based htap database,” VLDB , vol. 13, no. 12, p. 3072–3084,\nAug. 2020.\n[47] J. Ramnarayan, B. Mozafari, S. Wale, S. Menon, N. Kumar,\nH. Bhanawat, S. Chakraborty, Y. Mahajan, R. Mishra, and K. Bach-\nhav, “Snappydata: A hybrid transactional analytical store built on\nspark,” in SIGMOD , 2016, p. 2153–2156.\n[48] A. Kemper and T. Neumann, “Hyper: A hybrid oltp olap main\nmemory database system based on virtual memory snapshots,” in\nICDE , 2011, pp. 195–206.\n[49] T. Lahiri, S. Chavan, M. Colgan, D. Das, A. Ganesh, M. Gleeson,\nS. Hase, A. Holloway, J. Kamp, T.-H. Lee, J. Loaiza, N. Mac-\nnaughton, V . Marwah, N. Mukherjee, A. Mullick, S. Muthulingam,\nV . Raja, M. Roth, E. Soylemez, and M. Zait, “Oracle database in-\nmemory: A dual format in-memory database,” in ICDE , 2015, pp.\n1253–1258.\n[50] P .-r. Larson, A. Birka, E. N. Hanson, W. Huang, M. Nowakiewicz,\nand V . Papadimos, “Real-time analytical processing with sql\nserver,” VLDB , vol. 8, no. 12, p. 1740–1751, Aug. 2015.\n[51] J. Yang, I. Rae, J. Xu, J. Shute, Z. Yuan, K. Lau, Q. Zeng, X. Zhao,\nJ. Ma, Z. Chen, Y. Gao, Q. Dong, J. Zhou, J. Wood, G. Graefe,\nJ. Naughton, and J. Cieslewicz, “F1 lightning: Htap as a service,”\nVLDB , vol. 13, no. 12, p. 3313–3325, Aug. 2020.\n[52] F. Coelho, J. a. Paulo, R. Vilac ¸a, J. Pereira, and R. Oliveira, “Htap-\nbench: Hybrid transactional and analytical processing bench-\nmark,” in ICPE , 2017, p. 293–304.This article has been accepted for publication in IEEE Transactions on Knowledge and Data Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TKDE.2023.3271664\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 18\n[53] S. Idreos, M. L. Kersten, and S. Manegold, “Database cracking,” in\nCIDR , 2007, pp. 68–78.\n[54] G. Graefe and H. A. Kuno, “Self-selecting, self-tuning, incremen-\ntally optimized indexes.” in EDBT , 2010, pp. 371–381.\n[55] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The\ncase for learned index structures,” in SIGMOD , 2018, pp. 489–504.\n[56] A. Galakatos, M. Markovitch, C. Binnig, R. Fonseca, and T. Kraska,\n“Fiting-tree: A data-aware index structure,” in SIGMOD , 2019.\n[57] T. Kaftan, M. Balazinska, A. Cheung, and J. Gehrke, “Cuttlefish:\nA lightweight primitive for adaptive query processing,” 2018,\nunpublished.\n[58] I. Trummer, S. Moseley, D. Maram, S. Jo, and J. Antonakakis,\n“SkinnerDB: Regret-bounded query evaluation via reinforcement\nlearning,” VLDB , vol. 11, no. 12, pp. 2074–2077, 2018.\n[59] A. Kipf, T. Kipf, B. Radke, V . Leis, P . A. Boncz, and A. Kem-\nper, “Learned cardinalities: Estimating correlated joins with deep\nlearning,” in CIDR , 2019.\n[60] R. Marcus and O. Papaemmanouil, “Towards a hands-free query\noptimizer through deep learning,” in CIDR , 2019.\n[61] B. Hilprecht, C. Binnig, and U. R ¨ohm, “Towards learning a parti-\ntioning advisor with deep reinforcement learning,” in aiDM , 2019.\n[62] D. Basu, Q. Lin, W. Chen, H. T. Vo, Z. Yuan, P . Senellart, and\nS. Bressan, “Regularized cost-model oblivious database tuning\nwith reinforcement learning,” in TLDKS . Springer, 2016, pp. 96–\n132.\n[63] H. Grushka-Cohen, O. Biller, O. Sofer, L. Rokach, and B. Shapira,\n“Using bandits for effective database activity monitoring,” in\nP AKDD , H. W. Lauw, R. C.-W. Wong, A. Ntoulas, E.-P . Lim, S.-\nK. Ng, and S. J. Pan, Eds., 2020, pp. 701–713.\n[64] R. Marcus, P . Negi, H. Mao, N. Tatbul, M. Alizadeh, and T. Kraska,\n“Bao: Making learned query optimization practical,” in SIGMOD ,\n2020.\n[65] V . Ghadakchi, M. Xie, and A. Termehchy, “Bandit join: Preliminary\nresults,” in aiDM-SIGMOD , 2020.\n[66] H. Lan, Z. Bao, and Y. Peng, “A survey on advancing the dbms\nquery optimizer: Cardinality estimation, cost model, and plan\nenumeration,” Data Science and Engineering , vol. 6, pp. 86–101,\n2021.\n[67] G. Li, X. Zhou, and L. Cao, AI Meets Database:\nAI4DB and DB4AI . New York, NY, USA: Association for\nComputing Machinery, 2021, p. 2859–2866. [Online]. Available:\nhttps://doi.org/10.1145/3448016.3457542\n[68] B. Oetomo, R. M. Perera, R. Borovica-Gajic, and B. I. Rubinstein,\n“Cutting to the chase with warm-start contextual bandits,” in 2021\nIEEE International Conference on Data Mining (ICDM) . IEEE, 2021,\npp. 459–468.\n[69] S. Chaudhuri, A. K. Gupta, and V . Narasayya, “Compressing sql\nworkloads,” in Proceedings of the 2002 ACM SIGMOD international\nconference on Management of data , 2002, pp. 488–499.\n[70] S. Deep, A. Gruenheid, P . Koutris, J. F. Naughton, and S. Viglas,\n“Comprehensive and efficient workload compression,” Proc.\nVLDB Endow. , vol. 14, no. 3, pp. 418–430, 2020. [Online]. Available:\nhttp://www.vldb.org/pvldb/vol14/p418-deep.pdf\n[71] T. Siddiqui, S. Jo, W. Wu, C. Wang, V . Narasayya, and\nS. Chaudhuri, “Isum: Efficiently compressing large and complex\nworkloads for scalable index tuning,” in Proceedings of\nthe 2022 International Conference on Management of Data ,\nser. SIGMOD ’22. New York, NY, USA: Association for\nComputing Machinery, 2022, p. 660–673. [Online]. Available:\nhttps://doi.org/10.1145/3514221.3526152\n[72] L. Ma, D. Van Aken, A. Hefny, G. Mezerhane, A. Pavlo, and\nG. J. Gordon, “Query-based workload forecasting for self-driving\ndatabase management systems,” in SIGMOD , 2018, pp. 631–645.R. Malinga Perera Malinga Perera is a PhD candidate in the School\nof Computing and Information Systems at the University of Melbourne.\nHis research interests include machine learning and databases. He was\npart of the team designing multiple large scale big-data systems which\nwon national-level awards in Sri Lanka (1st runner up ‘Best Technology\nor Framework Innovation’ in SLASSCOM Innovation Awards 2019). He\ncompleted his bachelor’s in the University of Moratuwa, Sri Lanka (CS\nand Eng).\nBastian Oetomo Bastian Oetomo is a PhD candidate in the School of\nComputing and Information Systems at the University of Melbourne. His\nresearch is on multi-armed bandit applied to databases. He completed\nhis DMathSc (Applied Mathematics), BSc (Mechanical Systems) and\nMEng (Electrical) at the University of Melbourne. During his studies, he\nreceived multiple student awards for his performance.\nBenjamin I. P. Rubinstein Benjamin Rubinstein is a Professor in the\nSchool of Computing and Information Systems, at the University of\nMelbourne. His research interests span machine learning, security &\nprivacy, and databases. He has been part of teams that have: analysed\nprivacy of products at the Australian Bureau of Statistics, the financial\nindustry, and Transport for NSW; robustness of translation systems to\ndata poisoning attacks with Meta; helped identify and plug side-channel\nattacks against the Firefox browser; deanonymised Victorian Myki trans-\nport data and an unprecedented Australian Medicare data release;\ndeveloped scalable Bayesian approaches to record linkage tested by\nU.S. Census; and shipped production systems for entity resolution in\nBing and the Xbox360. Rubinstein completed a BSc (Pure Maths), BEng\n(Software Hons.), MCompSci (Research) at the University of Melbourne,\nand a PhD (CS) at UC Berkeley in 2010.\nRenata Borovica-Gajic Renata Borovica-Gajic holds a position of Se-\nnior Lecturer in Data Analytics in the School of Computing and Informa-\ntion Systems at The University of Melbourne. Dr Borovica-Gajic received\nher Ph.D. degree in Computer Science from Swiss Federal Institute\nof Technology in Lausanne (EPFL), Switzerland. Renata’s research\nfocuses on solving data management problems when storing, accessing\nand processing massive data sets, enabling faster, more predictable,\nand cheaper data analysis as a result. She is also interested in the topics\nof scientific data management, data exploration, query optimization,\nphysical database design, and hardware-software co-design. Her work\nhas repeatedly appeared in premier data management outlets, including\nSIGMOD, VLDB, ICDE and VLDB Journal, and she is a recipient of\nthe SIGMOD 2022 Test-of-Time Award as well as Australian Research\nCouncil (ARC) DECRA Fellowship.This article has been accepted for publication in IEEE Transactions on Knowledge and Data Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TKDE.2023.3271664\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
  "textLength": 108215
}