{
  "paperId": "837840c29b9cef4df596b1bceaba9815606125a1",
  "title": "Fast Partitioned Learned Bloom Filter",
  "pdfPath": "837840c29b9cef4df596b1bceaba9815606125a1.pdf",
  "text": "Fast Partitioned Learned Bloom Filter\nAtsuki Sato Yusuke Matsui\nThe University of Tokyo\nTokyo, Japan\na_sato@hal.t.u-tokyo.ac.jp matsui@hal.t.u-tokyo.ac.jp\nAbstract\nA Bloom filter is a memory-efficient data structure for approximate membership\nqueries used in numerous fields of computer science. Recently, learned Bloom\nfilters that achieve better memory efficiency using machine learning models have\nattracted attention. One such filter, the partitioned learned Bloom filter (PLBF),\nachieves excellent memory efficiency. However, PLBF requires a O(N3k)time\ncomplexity to construct the data structure, where Nandkare the hyperparameters\nof PLBF. One can improve memory efficiency by increasing N, but the construction\ntime becomes extremely long. Thus, we propose two methods that can reduce the\nconstruction time while maintaining the memory efficiency of PLBF. First, we\npropose fast PLBF, which can construct the same data structure as PLBF with a\nsmaller time complexity O(N2k). Second, we propose fast PLBF++, which can\nconstruct the data structure with even smaller time complexity O(NklogN+Nk2).\nFast PLBF++ does not necessarily construct the same data structure as PLBF. Still,\nit is almost as memory efficient as PLBF, and it is proved that fast PLBF++ has\nthe same data structure as PLBF when the distribution satisfies a certain constraint.\nOur experimental results from real-world datasets show that (i) fast PLBF and\nfast PLBF++ can construct the data structure up to 233 and 761 times faster than\nPLBF, (ii) fast PLBF can achieve the same memory efficiency as PLBF, and (iii)\nfast PLBF++ can achieve almost the same memory efficiency as PLBF. The codes\nare available at https://github.com/atsukisato/FastPLBF .\n1 Introduction\nMembership query is a problem of determining whether a given query qis contained within a set\nS. Membership query is widely used in numerous areas, including networks and databases. One\ncan correctly answer the membership query by keeping the set Sand checking whether it contains q.\nHowever, this approach is memory intensive because we need to maintain S.\nA Bloom filter [ 1] is a memory-efficient data structure that answers approximate membership queries.\nA Bloom filter uses hash functions to compress the set into a bitstring and then answers the queries\nusing the bitstring. When a Bloom filter answers qâˆˆ S, it can be wrong (false positives can\narise); when it answers q /âˆˆ S, it is always correct (no false negatives arise). A Bloom filter\nhas been incorporated into numerous systems, such as networks [ 2,3,4], databases [ 5,6,7], and\ncryptocurrencies [8, 9].\nA traditional Bloom filter does not care about the distribution of the set or the queries. Therefore, even\nif the distribution has a characteristic structure, a Bloom filter cannot take advantage of it. Kraska et\nal. proposed learned Bloom filters (LBFs), which utilize the distribution structure using a machine\nlearning model [ 10]. LBF achieves a better trade-off between memory usage and false positive rate\n(FPR) than the original Bloom filter. Since then, several studies have been conducted on various\nLBFs [11, 12, 13].\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).arXiv:2306.02846v3  [cs.DS]  28 Oct 2023\n\nğ‘“!ğ‘“\"ğ‘“#ğ‘¡\"ğ‘¡#$!ğ‘¡!â€¦â€¦Densityscoreğ‘¡#=1ğ‘¡%=0keysnon-keys\nFPR\nBF!BF\"BF#â€¦Figure 1: PLBF partitions the score\nspace into kregions and assigns\nbackup Bloom filters with different\nFPRs to each region.\n0.0050.0020.008â€¦â€¦â€¦â€¦â€¦â€¦â€¦0.2\n0.10.020.05â€¦â€¦â€¦â€¦â€¦â€¦â€¦0.001\n12ğ‘–ğ‘ğº!ğ‘”\"ğ‘¡#ğ‘¡$%!ğ‘¡!â€¦3ğ‘¡&=0ğ‘¡$=1â„\"ğº#ğº$ğ»!ğ»#ğ»$ğ‘“!ğ‘“#ğ‘“$â€¦FPRBF!BF#BF$â€¦Figure 2: PLBF divides the score space into Nsegments\nand then clusters the Nsegments into kregions. PLBF\nuses dynamic programming to find the optimal way to\ncluster segments into regions.\nPartitioned learned Bloom filter (PLBF) [ 14] is a variant of the LBF. PLBF effectively uses the\ndistribution and is currently one of the most memory-efficient LBFs. To construct PLBF, a machine\nlearning model is trained to predict whether the input is included in the set. For a given element x,\nthe machine learning model outputs a score s(x)âˆˆ[0,1], indicating the probability that Sincludes\nx. PLBF divides the score space [0,1]intoNequal segments and then appropriately clusters the\nNsegments into k(< N)regions using dynamic programming (DP). Then, kbackup Bloom filters\nwith different FPRs are assigned to each region. We can obtain a better (closer to optimal) data\nstructure with a larger N. However, PLBF builds DP tables O(N)times, and each DP table requires\nO(N2k)time complexity to build, so the total time complexity amounts to O(N3k). Therefore, the\nconstruction time increases rapidly as Nincreases.\nWe propose fast PLBF, which is constructed much faster than PLBF. Fast PLBF constructs the same\ndata structure as PLBF but with O(N2k)time complexity by omitting the redundant construction of\nDP tables. Furthermore, we propose fast PLBF++, which can be constructed faster than fast PLBF,\nwith a time complexity of O(NklogN+Nk2). Fast PLBF++ accelerates DP table construction by\ntaking advantage of a characteristic that DP tables often have. We proved that fast PLBF++ constructs\nthe same data structure as PLBF when the probability of xâˆˆ Sand the score s(x)are ideally well\ncorrelated. Our contributions can be summarized as follows.\nâ€¢We propose fast PLBF, which can construct the same data structure as PLBF with less time\ncomplexity.\nâ€¢We propose fast PLBF++, which can construct the data structure with even less time\ncomplexity than fast PLBF. Fast PLBF++ constructs the same data structure as PLBF when\nthe probability of xâˆˆ S and the score s(x)are ideally well correlated.\nâ€¢Experimental results show that fast PLBF can construct the data structure up to 233 times\nfaster than PLBF and achieves the same memory efficiency as PLBF.\nâ€¢Experimental results show that fast PLBF++ can construct the data structure up to 761 times\nfaster than PLBF and achieves almost the same memory efficiency as PLBF.\n2 Preliminaries: PLBF\nFirst, we define terms to describe PLBF [ 14]. LetSbe a set of elements for which the Bloom filter is\nto be built, and let Qbe the set of elements not included in Sthat is used when constructing PLBF\n(S âˆ©Q =âˆ…). The elements included in Sare called keys, and those not included are called non-keys.\nTo build PLBF, a machine learning model is trained to predict whether a given element xis included\nin the set SorQ. For a given element x, the machine learning model outputs a score s(x)âˆˆ[0,1].\nThe score s(x)indicates â€œhow likely is xto be included in the set S.â€\nNext, we explain the design of PLBF. PLBF partitions the score space [0,1]intokregions and assigns\nbackup Bloom filters with different FPRs to each region (Figure 1). Given a target overall FPR,\n2\n\nFâˆˆ(0,1), we optimize tâˆˆRk+1andfâˆˆRkto minimize the total memory usage. Here, tis a\nvector of thresholds for partitioning the score space into kregions, and fis a vector of FPRs for each\nregion, satisfying t0= 0< t1<Â·Â·Â·< tk= 1and0< fiâ‰¤1 (i= 1. . . k).\nNext, we explain how PLBF finds the optimal tandf. PLBF divides the score space [0,1]intoN(>\nk)segments and then finds the optimal tandfusing DP. Deciding how to cluster Nsegments into k\nconsecutive regions corresponds to determining the threshold t(Figure 2). We denote the probabilities\nthat the key and non-key scores are contained in the i-thsegment bygiandhi, respectively. After\nwe cluster the segments (i.e., determine the thresholds t), we denote the probabilities that the key and\nnon-key scores are contained in the i-thregion byGiandHi, respectively (e.g., g1+g2+g3=G1\nin Figure 2).\nWe can find the thresholds tthat minimize memory usage by solving the following problem for each\nj=k . . . N (see the appendix for details); we find a way to cluster the 1st to(jâˆ’1)-th segments\nintokâˆ’1regions while maximizing\nkâˆ’1X\ni=1Gilog2\u0012Gi\nHi\u0013\n. (1)\nPLBF solves this problem by building a jÃ—kDP table DPj\nKL[p][q](p= 0. . . jâˆ’1andq= 0. . . kâˆ’1)\nfor each j.DPj\nKL[p][q]denotes the maximum value ofPq\ni=1Gilog2\u0010\nGi\nHi\u0011\none can get when you\ncluster the 1st top-th segments into qregions. To construct PLBF, one must find a clustering method\nthat achieves DPj\nKL[jâˆ’1][kâˆ’1].DPj\nKLcan be computed recursively as follows:\nDPj\nKL[p][q] =ï£±\nï£´ï£´ï£²\nï£´ï£´ï£³0 ( p= 0âˆ§q= 0)\nâˆ’âˆ ((p= 0âˆ§q >0)âˆ¨(p >0âˆ§q= 0))\nmax\ni=1...p\u0010\nDPj\nKL[iâˆ’1][qâˆ’1] +dKL(i, p)\u0011\n(else) ,\n(2)\nwhere the function dKL(il, ir)is the following function defined for integers ilandirsatisfying\n1â‰¤ilâ‰¤irâ‰¤N:\ndKL(il, ir) = irX\ni=ilgi!\nlog2 Pir\ni=ilgiPir\ni=ilhi!\n. (3)\nThe time complexity to construct this DP table is O(j2k). Then, by tracing the recorded transitions\nbackward from DPj\nKL[jâˆ’1][kâˆ’1], we obtain the best clustering with a time complexity of O(k). As\nthe DP table is constructed for each j=k . . . N , the overall complexity is O(N3k). The pseudo-code\nfor PLBF construction is provided in the appendix.\nWe can divide the score space more finely with a larger Nand thus obtain a near-optimal t. However,\nthe time complexity increases rapidly with increasing N.\n3 Fast PLBF\nWe propose fast PLBF, which constructs the same data structure as PLBF more quickly than PLBF\nby omitting the redundant construction of DP tables. Fast PLBF uses the same design as PLBF and\nfinds the best clustering (i.e., t) and FPRs (i.e., f) to minimize memory usage.\nPLBF constructs a DP table for each j=k . . . N . We found that this computation is redundant and\nthat we can also use the last DP table DPN\nKLforj=k . . . N âˆ’1. This is because the maximum\nvalue ofPkâˆ’1\ni=1Gilog2\u0010\nGi\nHi\u0011\nwhen clustering the 1st to(jâˆ’1)-th segments into kâˆ’1regions is\nequal to DPN\nKL[jâˆ’1][kâˆ’1]. We can obtain the best clustering by tracing the transitions backward\nfrom DPN\nKL[jâˆ’1][kâˆ’1]. The time complexity of tracing the transitions is O(k), which is faster\nthan constructing the DP table.\nThis is a simple method, but it is a method that only becomes apparent after some organization on the\noptimization problem of PLBF. PLBF solves Nâˆ’k+ 1optimization problems separately, i.e., for\neach of j=k, . . . , N , the problem of â€œfinding the optimal tandfwhen the k-th region consists of\n3\n\nğ‘0123â€¦ğ‘âˆ’1DP!\"ğ‘[ğ‘âˆ’1]DP!\"ğ‘[ğ‘](a)\nğ‘0123â€¦ğ‘âˆ’1DP!\"ğ‘[ğ‘âˆ’1]DP!\"ğ‘[ğ‘] (b)\nFigure 3: The cases (a) where there is no â€œcrossingâ€ of the recorded transitions when computing\nDPKL[p][q] (p= 1. . . Nâˆ’1)from DPKL[p][qâˆ’1] (p= 0. . . Nâˆ’2)and (b) where there is. The\narrows indicate which transition took the maximum value. Empirically, the probability of â€œcrossingâ€\nis small.\nğ‘–123â€¦DP!\"ğ‘–âˆ’1[ğ‘âˆ’1]\nDP!\"ğ‘[ğ‘]ğ‘123â€¦\nâˆ’âˆâˆ’âˆâˆ’âˆâˆ’âˆâˆ’âˆâˆ’âˆâˆ’âˆâˆ’âˆâˆ’âˆâˆ’âˆğ´=maxğ‘âˆ’1\nğ‘âˆ’1\nFigure 4: Computing DPKL[p][q] (p= 1. . . Nâˆ’1)from\nDPKL[p][qâˆ’1] (p= 0. . . Nâˆ’2)via the matrix A. The\ncomputation is the same as solving the matrix problem\nfor matrix A. When the score distribution is ideal ,Ais a\nmonotone matrix .\n72415426535897932341max\n7694Figure 5: Example of a matrix prob-\nlemfor a4Ã—5monotone matrix . The\nposition of the maximum value in\neach row (filled in gray) is moved to\nthe â€œlower rightâ€.\nthej, . . . , N th segmentsâ€ is solved separately. Fast PLBF, on the other hand, solves the problems for\nj=k, . . . , N âˆ’1faster by reusing the computations in the problem for j=N. The reorganization\nof the problem in the appendix makes it clear that this reuse does not change the answer.\nThe pseudo-code for fast PLBF construction is provided in the appendix. The time complexity of\nbuilding DPN\nKLisO(N2k), and the worst-case complexity of subsequent computations is O(Nk2).\nBecause N > k , the total complexity is O(N2k), which is faster than O(N3k)for PLBF, although\nfast PLBF constructs the same data structure as PLBF.\nFast PLBF extends the usability of PLBF. In any application of PLBF, fast PLBF can be used instead\nof PLBF because fast PLBF can be constructed quickly without losing the accuracy of PLBF. Fast\nPLBF has applications in a wide range of computing areas [ 15,5], and is significantly superior\nin applications where construction is frequently performed. Fast PLBF also has the advantage of\nsimplifying hyperparameter settings. When using PLBF, the hyperparameters Nandkmust be\ncarefully determined, considering the trade-off between accuracy and construction speed. With\nfast PLBF, on the other hand, it is easy to determine the appropriate hyperparameters because the\nconstruction time is short enough, even if Nandkare set somewhat large.\n4 Fast PLBF++\nWe propose fast PLBF++, which can be constructed even faster than fast PLBF. Fast PLBF++\naccelerates the construction of the DP table DPN\nKLby taking advantage of a characteristic that DP\ntables often have. When the transitions recorded in computing DPN\nKL[p][q] (p= 1. . . Nâˆ’1)from\nDPN\nKL[p][qâˆ’1] (p= 0. . . Nâˆ’2)are represented by arrows, as in Figure 3, we find that the arrows\nrarely â€œcrossâ€ (at locations other than endpoints). In other words, the transitions tend to have few\nintersections (Figure 3(a)) rather than many (Figure 3(b)). Fast PLBF++ takes advantage of this\ncharacteristic to construct the DP table with less complexity O(NklogN), thereby reducing the total\nconstruction complexity to O(NklogN+Nk2).\n4\n\n3271435(a)\n3271435 (b)\n239327143532461 (c)\nFigure 6: A divide-and-conquer algorithm for\nsolving a matrix problem for a monotone ma-\ntrix: (a) An exhaustive search is performed on\nthe middle row, and (b) the result is used to nar-\nrow down the search area. (c) This is repeated\nrecursively.\nğ‘+1ğ‘¡!\"#ğ‘–12ğ‘ğ‘”$â„$ğ‘¡!\"#ğ‘–12ğ‘ğ‘”$â„$(a)\nğ‘”!â„!\nâ„!\nğ‘+1ğ‘¡\"#$ğ‘–12ğ‘ğ‘¡\"#$ğ‘–12ğ‘ğ‘”! (b)\nFigure 7: When the number of regions is fixed at\nqand the number of segments is increased by 1,\nthe optimal tqâˆ’1remains unchanged or increases\nifAis amonotone matrix (a). When Ais not\namonotone matrix (b), the optimal tqâˆ’1may\ndecrease.\nFirst, we define the terms to describe fast PLBF++. For simplicity, DPN\nKLis denoted as DPKLin this\nsection. The (Nâˆ’1)Ã—(Nâˆ’1)matrix Ais defined as follows:\nApi=\u001aâˆ’âˆ (i=p+ 1, p+ 2, . . . , N âˆ’1)\nDPKL[iâˆ’1][qâˆ’1] +dKL(i, p) (else) .(4)\nThen, from the definition of DPKL,\nDPKL[p][q] = max\ni=1...Nâˆ’1Api. (5)\nThe matrix Arepresents the intermediate calculations involved in determining DPKL[p][q]from\nDPKL[p][qâˆ’1](Figure 4).\nFollowing Aggarwal et al. [16], we define the monotone matrix andmatrix problem as follows.\nDefinition 4.1. LetBbe an nÃ—mreal matrix, and we define a function J:{1. . . n} â†’ { 1. . . m},\nwhere J(i)is the jâˆˆ {1. . . m}such that Bijis the maximum value of the i-th row of B. If\nthere is more than one such j, letJ(i)be the smallest. A matrix Bis called a monotone matrix if\nJ(i1)â‰¤J(i2)for any i1andi2that satisfy 1â‰¤i1< i2â‰¤n. Finding the maximum value of each\nrow of a matrix is called a matrix problem .\nAn example of a matrix problem for a monotone matrix is shown in Figure 5. Solving the matrix\nproblem for a general nÃ—mmatrix requires O(nm)time complexity because all matrix values must\nbe checked. Meanwhile, if the matrix is known to be a monotone matrix , the matrix problem for\nthis matrix can be solved with a time complexity of O(n+mlogn)using the divide-and-conquer\nalgorithm [ 16]. Here, the exhaustive search for the middle row and the refinement of the search range\nare repeated recursively (Figure 6).\nWe also define an ideal score distribution as follows.\nDefinition 4.2. A score distribution is ideal if the following holds:\ng1\nh1â‰¤g2\nh2â‰¤ Â·Â·Â· â‰¤gN\nhN. (6)\nAnideal score distribution implies that the probability of xâˆˆ Sand the score s(x)are ideally well\ncorrelated. In other words, an ideal score distribution means that the machine learning model learns\nthe distribution ideally.\nâ€œFew crossing transitionsâ€ in Figure 3 indicates that Ais amonotone matrix or is close to it. It is\nsomewhat intuitive that Ais amonotone matrix or is close to it. This is because the fact that Ais\namonotone matrix implies that the optimal tqâˆ’1does not decrease when the number of regions is\nfixed at qand the number of segments increases by 1(Figure 7). It is intuitively more likely that tqâˆ’1\nremains unchanged or increases, as in Figure 7(a), than that tqâˆ’1decreases, as in Figure 7(b). Fast\nPLBF++ takes advantage of this insight to rapidly construct DP tables.\nFrom Equation (5), determining DPKL[p][q] (p= 1. . . Nâˆ’1)from DPKL[p][qâˆ’1] (p= 0. . . Nâˆ’2)\nis equivalent to solving the matrix problem of matrix A. When Ais amonotone matrix , the divide-and-\nconquer algorithm can solve this problem with O(NlogN)time complexity. (The same algorithm\n5\n\n0.0 0.2 0.4 0.6 0.8 1.0\nScore103104105CountMalicious (key)\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104105CountBenign (non-key)(a) Malicious URLs Dataset\n0.0 0.2 0.4 0.6 0.8 1.0\nScore103104105CountMalicious (key)\n0.0 0.2 0.4 0.6 0.8 1.0\nScore103104105CountBenign (non-key) (b) EMBER Dataset\nFigure 8: Histograms of the score distributions of keys and non-keys.\n0 200 400 600 800 1000\ni101\n100101102gi/hi\n(a) Malicious URLs Dataset\n0 200 400 600 800 1000\ni103\n102\n101\n100101102103gi/hi (b) EMBER Dataset\nFigure 9: Ratio of keys to non-keys.\ncan obtain a not necessarily correct solution even if Ais not a monotone matrix .) By computing\nDPKL[p][q]with this algorithm sequentially for q= 1. . . kâˆ’1, we can construct a DP table with\na time complexity of O(NklogN). The worst-case complexity of the subsequent calculations is\nO(Nk2), so the total complexity is O(NklogN+Nk2). This is the fast PLBF++ construction\nalgorithm, and this is faster than fast PLBF, which requires O(N2k)computations.\nFast PLBF++ does not necessarily have the same data structure as PLBF because Ais not necessarily\namonotone matrix . However, as the following theorem shows, we can prove that Ais amonotone\nmatrix under certain conditions.\nTheorem 4.3. If the score distribution is ideal, Ais a monotone matrix.\nThe proof is given in the appendix. When the distribution is not ideal , matrix Ais not necessarily\namonotone matrix , but as mentioned above, it is somewhat intuitive that Ais close to a monotone\nmatrix . In addition, as will be shown in the next section, experiment results from real-world datasets\nwhose distribution is not ideal show that fast PLBF++ is almost as memory efficient as PLBF.\n5 Experiments\nThis section evaluates the experimental performance of fast PLBF and fast PLBF++. We compared\nthe performances of fast PLBF and fast PLBF++ with four baselines: Bloom filter [ 1], Ada-BF [ 11],\nsandwiched learned Bloom filter (sandwiched LBF) [ 12], and PLBF [ 14]. Similar to PLBF, Ada-BF\nis an LBF that partitions the score space into several regions and assigns different FPRs to each\nregion. However, Ada-BF relies heavily on heuristics for clustering and assigning FPRs. Sandwiched\nLBF is an LBF that â€œsandwichesâ€ a machine learning model with two Bloom filters. This achieves\nbetter memory efficiency than the original LBF by optimizing the size of two Bloom filters.\nTo facilitate the comparison of different methods or hyperparameters results, we have slightly modified\nthe original PLBF framework. The original PLBF was designed to minimize memory usage under\nthe condition of a given false positive rate. However, this approach makes it difficult to compare\nthe results of different methods or hyperparameters. This is because both the false positive rate at\ntest time and the memory usage vary depending on the method and hyperparameters, which often\nmakes it difficult to determine the superiority of the results. Therefore, in our experiments, we used a\nframework where the expected false positive rate is minimized under the condition of memory usage.\nThis approach makes it easy to obtain two results with the same memory usage and compare them by\n6\n\nInsert keys to Bloom filter Train machine learning model Find optimal parameters\n100150200\nBloom filter Ada-BF Sandwiched\nLBFPLBF Fast PLBF\n(Proposed)   Fast PLBF++\n   (Proposed)024Construction Time [s](a) Malicious URLs Dataset\n100150200250\nBloom filter Ada-BF Sandwiched\nLBFPLBF Fast PLBF\n(Proposed)   Fast PLBF++\n   (Proposed)0.02.55.07.5Construction Time [s] (b) EMBER Dataset\nFigure 10: Construction time.\nthe false positive rate at test time. See the appendix for more information on how this framework\nmodification will change the construction method of PLBFs.\nDatasets : We evaluated the algorithms using the following two datasets.\nâ€¢Malicious URLs Dataset : As in previous papers [ 11,14], we used Malicious URLs Dataset\n[17]. The URLs dataset comprises 223,088 malicious and 428,118 benign URLs. We\nextracted 20 lexical features such as URL length, use of shortening, number of special\ncharacters, etc. We used all malicious URLs and 342,482 (80%) benign URLs as the training\nset, and the remaining benign URLs as the test set.\nâ€¢EMBER Dataset : We used the EMBER dataset [ 18] as in the PLBF research. The dataset\nconsists of 300,000 malicious and 400,000 benign files, along with the features of each file.\nWe used all malicious files and 300,000 (75%) benign files as the train set and the remaining\nbenign files as the test set.\nWhile any model can be used for the classifier, we used LightGBM [ 19] because of its speed in\ntraining and inference, as well as its memory efficiency and accuracy. The sizes of the machine\nlearning model for the URLs and EMBER datasets are 312 Kb and 1.19 Mb, respectively. The training\ntime of the machine learning model for the URLs and EMBER datasets is 1.09 and 2.71 seconds,\nrespectively. The memory usage of LBF is the total memory usage of the backup Bloom filters and\nthe machine learning model. Figure 8 shows a histogram of each datasetâ€™s score distributions of keys\nand non-keys. We can see that the frequency of keys increases and that of non-keys decreases as the\nscore increases. In addition, Figure 9 plots gi/hi(i= 1. . . N )when N= 1,000. We can see that\ngi/hitends to increase as iincreases, but the increase is not monotonic (i.e., the score distribution is\nnotideal ).\n5.1 Construction time\nWe compared the construction times of fast PLBF and fast PLBF++ with those of existing methods.\nFollowing the experiments in the PLBF paper, hyperparameters for PLBF, fast PLBF, and fast PLBF++\nwere set to N= 1,000andk= 5.\nFigure 10 shows the construction time for each method. The construction time for learned Bloom\nfilters includes not only the time to insert keys into the Bloom filters but also the time to train the\nmachine learning model and the time to compute the optimal parameters ( tandfin the case of\nPLBF).\nAda-BF and sandwiched LBF use heuristics to find the optimal parameters, so they have shorter\nconstruction times than PLBF but have worse accuracy. PLBF has better accuracy but takes more\nthan 3 minutes to find the optimal tandf. On the other hand, our fast PLBF and fast PLBF++ take\nless than 2 seconds. As a result, Fast PLBF constructs 50.8 and 34.3 times faster than PLBF, and\nfast PLBF++ constructs 63.1 and 39.3 times faster than PLBF for the URLs and EMBER datasets,\nrespectively. This is about the same construction time as sandwiched LBF, which relies heavily on\nheuristics and, as we will see in the next section, is much less accurate than PLBF.\n7\n\nBloom filter Ada-BF Sandwiched LBF PLBF Fast PLBF (Proposed) Fast PLBF++ (Proposed)\n0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0\nTotal Memory Usage [Mbit]  103\n102\n101\nFalse Positive Rate\n(a) Malicious URLs Dataset\n1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0\nTotal Memory Usage [Mbit]  103\n102\n101\nFalse Positive Rate\n (b) EMBER Dataset\nFigure 11: Trade-off between memory usage and FPR.\nPLBF Fast PLBF (Proposed) Fast PLBF++ (Proposed)\n(a) Malicious URLs Dataset\n (b) EMBER Dataset\nFigure 12: Ablation study for hyper-parameter N.\n5.2 Memory Usage and FPR\nWe compared the trade-off between memory usage and FPR for fast PLBF and fast PLBF++ with\nBloom filter, Ada-BF, sandwiched LBF, and PLBF. Following the experiments in the PLBF paper,\nhyperparameters for PLBF, fast PLBF, and fast PLBF++ were always set to N= 1,000andk= 5.\nFigure 11 shows each methodâ€™s trade-off between memory usage and FPR. PLBF, fast PLBF, and fast\nPLBF++ have better Pareto curves than the other methods for all datasets. Fast PLBF constructs the\nsame data structure as PLBF in all cases, so it has exactly the same accuracy as PLBF. Fast PLBF++\nachieves almost the same accuracy as PLBF. Fast PLBF++ has up to 1.0019 and 1.000083 times\nhigher false positive rates than PLBF for the URLs and EMBER datasets, respectively.\n5.3 Ablation study for hyper-parameters\nThe parameters of the PLBFs are memory size, N, and k. The memory size is specified by the user,\nandNandkare hyperparameters that are determined by balancing construction time and accuracy.\nIn the previous sections, we set Nto 1,000 and kto 5, following the original paper on PLBF. In this\nsection, we perform ablation studies for these hyperparameters to confirm that our proposed methods\ncan construct accurate data structures quickly, no matter what hyperparameter settings are used. We\nalso confirm that the accuracy tends to be better and the construction time increases as Nandkare\nincreased, and that the construction time of the proposed methods increases much slower than PLBF.\nFigure 12 shows the construction time and false positive rate with various Nwhile the memory usage\nof the backup Bloom filters is fixed at 500 Kb and kis fixed at 5. For all three PLBFs, the false\npositive rate tends to decrease as Nincreases. (Note that this is the false positive rate on test data,\nso it does not necessarily decrease monotonically. The appendix shows that the expected value of\nthe false positive rate calculated using training data decreases monotonically as Nincreases.) Also,\nasNincreases, the PLBF construction time increases rapidly, but the fast PLBF construction time\nincreases much more slowly than that, and for fast PLBF++, the construction time changes little. This\nis because the construction time of PLBF is asymptotically proportional to N3, while that of fast\nPLBF and fast PLBF++ is proportional to N2andNlogN, respectively. The experimental results\n8\n\nPLBF Fast PLBF (Proposed) Fast PLBF++ (Proposed)\n(a) Malicious URLs Dataset\n (b) EMBER Dataset\nFigure 13: Ablation study for hyper-parameter k.\nshow that the two proposed methods can achieve high accuracy without significantly changing the\nconstruction time with large N.\nFigure 13 shows the construction time and false positive rate with various kwhile the backup bloom\nfilter memory usage is fixed at 500 Kb and Nis fixed at 1,000. For all three PLBFs, the false positive\nrate tends to decrease as kincreases. For the EMBER dataset, the false positive rate stops decreasing\nat about k= 20 , while for the URLs dataset, it continues to decrease even at about k= 500 . (Just as\nin the case of experiments with varying N, this decrease is not necessarily monotonic.) In addition,\nthe construction times of all three PLBFs increase proportionally to k, but fast PLBF has a much\nshorter construction time than PLBF, and fast PLBF++ has an even shorter construction time than fast\nPLBF. When k= 50 , fast PLBF constructs 233 and 199 times faster than PLBF, and fast PLBF++\nconstructs 761 and 500 times faster than PLBF for the URLs and EMBER datasets, respectively.\nThe experimental results indicate that by increasing k, the two proposed methods can achieve high\naccuracy without significantly affecting the construction time.\n6 Related Work\nApproximate membership query is a query that asks whether the query qis contained in the set S\nwhile allowing for false positives with a small probability Îµ. One can prove that at least |S|log2\u00001\nÎµ\u0001\nbits of memory must be used to answer the approximate membership query if the elements in the set\nor queries are selected with equal probability from the universal set [20].\nA Bloom filter [ 1] is one of the most basic data structures for approximate membership queries. It\ncompresses the set Sinto a bit string using hash functions. This bit string and the hash functions are\nthen used to answer the queries. To achieve an FPR of Îµ, a Bloom filter requires |S|log2\u00001\nÎµ\u0001\nlog2e\nbits of memory. This is log2etimes the theoretical lower bound.\nVarious derivatives have been proposed that achieve better memory efficiency than the original Bloom\nfilter. The cuckoo filter [ 21] is more memory efficient than the original Bloom filter and supports\ndynamic addition and removal of elements. Pagh et al. [ 22] proposed a replacement for Bloom filter\nthat achieves a theoretical lower bound. Various other memory-efficient filters exist, including the\nvacuum filter [ 23], xor filter [ 24], and ribbon filter [ 25]. However, these derivatives do not consider\nthe structure of the distribution and thus cannot take advantage of it.\nKraska et al.[ 10] proposed using a machine learning model as a prefilter of a backup Bloom filter. Ada-\nBF [11] extended this design and proposed to exploit the scores output by the machine learning model.\nPLBF [ 14] uses a design similar to that of Ada-BF but introduces fewer heuristics for optimization\nthan Ada-BF. Mitzenmacher [ 12] proposed an LBF that â€œsandwichesâ€ a machine learning model with\ntwo Bloom filters. This achieves better memory efficiency than the original LBF but can actually be\ninterpreted as a special case of PLBF.\n7 Limitation and Future Work\nAs explained in Section 4, it is somewhat intuitive that Ais a monotone matrix or close to it, so it is\nalso intuitive that fast PLBF++ achieves accuracy close to PLBF. Experimental results on the URLs\nand EMBER datasets also suggest that fast PLBF++ achieves almost the same accuracy as PLBF,\n9\n\neven when the score distribution is not ideal . This experimental rule is further supported by the\nresults of the artificial data experiments described in the appendix. However, there is no theoretical\nsupport for the accuracy of fast PLBF++. Theoretical support for how fast PLBF++ accuracy may\ndegrade relative to PLBF is a future issue.\nBesides, it is possible to consider faster methods by making stronger assumptions than fast PLBF++.\nFast PLBF++ assumes the monotonicity of matrix A. We adopted this assumption because matrix\nAis proved to be monotone under intuitive assumptions about the data distribution. However, by\nassuming stronger assumptions about matrix A, the computational complexity of the construction\ncould be further reduced. For example, if matrix Aistotally monotone , the matrix problem for matrix\nAcan be solved in O(N)using algorithms such as [ 16,26]. Using such existing DP algorithms is a\npromising direction toward even faster or more accurate methods, and is a future work.\n8 Conclusion\nPLBF is an outstanding LBF that can effectively utilize the distribution of the set and queries captured\nby a machine learning model. However, PLBF is computationally expensive to construct. We\nproposed fast PLBF and fast PLBF++ to solve this problem. Fast PLBF is superior to PLBF because\nfast PLBF constructs exactly the same data structure as PLBF but does so faster. Fast PLBF++ is\neven faster than fast PLBF and achieves almost the same accuracy as PLBF and fast PLBF. These\nproposed methods have greatly expanded the range of applications of PLBF.\nAcknowledgments and Disclosure of Funding\nWe thank the anonymous reviewers for their constructive comments. This work was supported by\nJST AIP Acceleration Research JPMJCR23U2, Japan.\nReferences\n[1]Burton H Bloom. Space/time trade-offs in hash coding with allowable errors. Communications\nof the ACM , 13(7):422â€“426, 1970.\n[2]Andrei Broder and Michael Mitzenmacher. Network applications of bloom filters: A survey.\nInternet mathematics , 1(4):485â€“509, 2004.\n[3]Sasu Tarkoma, Christian Esteve Rothenberg, and Eemil Lagerspetz. Theory and practice of\nbloom filters for distributed systems. IEEE Communications Surveys & Tutorials , 14(1):131â€“\n155, 2011.\n[4]Shahabeddin Geravand and Mahmood Ahmadi. Bloom filter applications in network security:\nA state-of-the-art survey. Computer Networks , 57(18):4047â€“4064, 2013.\n[5]Fay Chang, Jeffrey Dean, Sanjay Ghemawat, Wilson C Hsieh, Deborah A Wallach, Mike\nBurrows, Tushar Chandra, Andrew Fikes, and Robert E Gruber. Bigtable: A distributed storage\nsystem for structured data. ACM Transactions on Computer Systems (TOCS) , 26(2):1â€“26, 2008.\n[6]Michael T Goodrich and Michael Mitzenmacher. Invertible bloom lookup tables. In Allerton\nConference on Communication, Control, and Computing (Allerton) , 2011.\n[7]Guanlin Lu, Young Jin Nam, and David HC Du. Bloomstore: Bloom-filter based memory-\nefficient key-value store for indexing of data deduplication on flash. In IEEE Symposium on\nMass Storage Systems and Technologies (MSST) , 2012.\n[8]Mike Hearn and Matt Corallo. Bips: Connection bloom filtering, 2012. URL\nhttps://github.com/bitcoin/bips/blob/master/bip-0037 [Online; accessed 22-December-2022] ,\n2012.\n[9]Jongbeen Han, Mansub Song, Hyeonsang Eom, and Yongseok Son. An efficient multi-signature\nwallet in blockchain using bloom filter. In Proceedings of the ACM Symposium on Applied\nComputing , 2021.\n10\n\n[10] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned\nindex structures. In Proceedings of the International Conference on Management of Data , 2018.\n[11] Zhenwei Dai and Anshumali Shrivastava. Adaptive learned bloom filter (ada-bf): Efficient\nutilization of the classifier with application to real-time information filtering on the web. In\nAdvances in Neural Information Processing Systems , 2020.\n[12] Michael Mitzenmacher. A model for learned bloom filters and optimizing by sandwiching. In\nAdvances in Neural Information Processing Systems , 2018.\n[13] Jack Rae, Sergey Bartunov, and Timothy Lillicrap. Meta-learning neural bloom filters. In\nInternational Conference on Machine Learning , 2019.\n[14] Kapil Vaidya, Eric Knorr, Michael Mitzenmacher, and Tim Kraska. Partitioned learned bloom\nfilters. In International Conference on Learning Representations , 2021.\n[15] Yi-Hsuan Feng, Nen-Fu Huang, and Chia-Hsiang Chen. An efficient caching mechanism for\nnetwork-based url filtering by multi-level counting bloom filters. In 2011 IEEE International\nConference on Communications (ICC) , 2011.\n[16] Alok Aggarwal, Maria M Klawe, Shlomo Moran, Peter Shor, and Robert Wilber. Geometric\napplications of a matrix-searching algorithm. Algorithmica , 2(1):195â€“208, 1987.\n[17] Manu Siddhartha. Malicious urls dataset | kaggle. URL\nhttps://www.kaggle.com/datasets/sid321axn/malicious-urls-dataset [Online; accessed\n22-December-2022] , 2021.\n[18] Hyrum S Anderson and Phil Roth. Ember: an open dataset for training static pe malware\nmachine learning models. arXiv preprint arXiv:1804.04637 , 2018.\n[19] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and\nTie-Yan Liu. Lightgbm: A highly efficient gradient boosting decision tree. In Advances in\nNeural Information Processing Systems , 2017.\n[20] Larry Carter, Robert Floyd, John Gill, George Markowsky, and Mark Wegman. Exact and ap-\nproximate membership testers. In Proceedings of the ACM symposium on Theory of computing ,\n1978.\n[21] Bin Fan, Dave G Andersen, Michael Kaminsky, and Michael D Mitzenmacher. Cuckoo filter:\nPractically better than bloom. In Proceedings of the ACM International Conference on emerging\nNetworking Experiments and Technologies , 2014.\n[22] Anna Pagh, Rasmus Pagh, and S Srinivasa Rao. An optimal bloom filter replacement. In\nProceedings of the ACM-SIAM Symposium on Discrete Algorithms , 2005.\n[23] Minmei Wang, Mingxun Zhou, Shouqian Shi, and Chen Qian. Vacuum filters: More space-\nefficient and faster replacement for bloom and cuckoo filters. Proceedings of the VLDB\nEndowment , 13(2):197â€“210, 2019.\n[24] Thomas Mueller Graf and Daniel Lemire. Xor filters: Faster and smaller than bloom and cuckoo\nfilters. Journal of Experimental Algorithmics (JEA) , 25:1â€“16, 2020.\n[25] Peter C. Dillinger and Stefan Walzer. Ribbon filter: practically smaller than bloom and xor.\narXiv preprint arXiv:2103.02515 , 2021.\n[26] Zvi Galil and Kunsoo Park. A linear-time algorithm for concave one-dimensional dynamic\nprogramming. Information Processing Letters , 33(6):309â€“311, 1989.\n[27] Johan Ludwig William Valdemar Jensen. Sur les fonctions convexes et les inÃ©galitÃ©s entre les\nvaleurs moyennes. Acta mathematica , 30(1):175â€“193, 1906.\n11\n\nAppendices\nAppendix A details the optimization problem designed in the original PLBF paper [ 14] and its solution.\nIt includes a comprehensive analysis of the optimization problem and a detailed description of how\nPLBFâ€™s method leads (under certain assumptions) to a solution to the optimization problem, which\nare not provided in [ 14]. Appendix B presents details of PLBF and the fast PLBF algorithm along\nwith pseudo-code. Appendix C gives the proof of Theorem 4.3, a theorem about the accuracy of fast\nPLBF++. In Appendix D, we explain the trivial modifications to the PLBF framework that we made\nfor the experiments. Appendix E describes a detailed ablation study of the PLBFs hyperparameters,\nNandk. Appendix F describes the experiments on the simplified solution method presented in the\nPLBF paper. Appendix G describes experiments with artificial datasets to evaluate the experimental\nperformance of fast PLBF++ in detail.\nAppendix A Solution of the optimization problems\nIn the original PLBF paper [ 14], the analysis with mathematical expressions was conducted only for\nthe relaxed problem, while no analysis utilizing mathematical expressions was performed for the\ngeneral problem. Consequently, it was unclear which calculations were redundant, leading to the\nrepetition of constructing similar DP tables. Hence, this appendix provides a comprehensive analysis\nof the general problem. It presents the optimal solution and the corresponding value of the objective\nfunction using mathematical expressions. This analysis uncovers redundant calculations, enabling\nthe derivation of fast PLBF.\nThe optimization problem designed by PLBF [14] can be written as follows:\nminimize\nf,tkX\ni=1c|S|Gilog2\u00121\nfi\u0013\nsubject tokX\ni=1Hifiâ‰¤F\nt0= 0< t1< t2<Â·Â·Â·< tk= 1\nfiâ‰¤1 ( i= 1. . . k).(7)\nThe objective function represents the total memory usage of the backup Bloom filters. The first\nconstraint equation represents the condition that the expected overall FPR is below F.câ‰¥1is a\nconstant determined by the type of backup Bloom filters. GandHare determined by t. Once tis\nfixed, we can solve this optimization problem to find the optimal fand the minimum value of the\nobjective function. In the original PLBF paper [ 14], the condition fiâ‰¤1 (i= 1. . . k)was relaxed;\nhowever, here we conduct the analysis without relaxation. We assume Gi>0, Hi>0 (i= 1. . . k)\nand|S|>0.\nThe Lagrange function is defined using the Lagrange multipliers ÂµâˆˆRkandÎ½âˆˆR, as follows:\nL(f,Âµ, Î½) =kX\ni=1c|S|Gilog2\u00121\nfi\u0013\n+kX\ni=1Âµi(fiâˆ’1) +Î½ kX\ni=1Hifiâˆ’F!\n. (8)\nFrom the KKT condition, there exist Â¯ÂµandÂ¯Î½in the local optimal solution Â¯fof this optimization\nproblem, and the following holds:\nâˆ‚L\nâˆ‚fi(Â¯f,Â¯Âµ,Â¯Î½) = 0 ( i= 1. . . k) (9)\nÂ¯fiâˆ’1â‰¤0,Â¯Âµiâ‰¥0,Â¯Âµi(Â¯fiâˆ’1) = 0 ( i= 1. . . k) (10)\nkX\ni=1HiÂ¯fiâˆ’Fâ‰¤0,Â¯Î½â‰¥0,Â¯Î½ kX\ni=1HiÂ¯fiâˆ’F!\n= 0. (11)\nWe define If=1andIf<1asIf=1={i|Â¯fi= 1}andIf<1={i|Â¯fi<1}.If=1âˆªIf<1={1. . . k}\nandIf=1âˆ© If<1=âˆ…are satisfied. Furthermore, we assume that If<1Ì¸=âˆ…. This means that there\nis at least one region that uses a backup Bloom filter with an FPR smaller than 1.\n12\n\nBy introducing If=1andIf<1, Equations (9, 10, 11) can be organized as follows:\nc|S|Gi= Â¯Âµi+ Â¯Î½Hi,Â¯Âµiâ‰¥0,Â¯fi= 1 ( iâˆˆ If=1) (12)\nc|S|Gi=Â¯fiÂ¯Î½Hi,Â¯Âµi= 0,Â¯fi<1 ( iâˆˆ If<1) (13)\nX\niâˆˆIf=1Hi+X\niâˆˆIf<1HiÂ¯fiâˆ’Fâ‰¤0,Â¯Î½â‰¥0,Â¯Î½ï£«\nï£­X\niâˆˆIf=1Hi+X\niâˆˆIf<1HiÂ¯fiâˆ’Fï£¶\nï£¸= 0.(14)\nHere, we get Â¯Î½ >0. This is because if we assume Â¯Î½= 0, Equation (13) implies that Gi= 0 for\niâˆˆ If<1(Ì¸=âˆ…), which contradicts the assumption that Gi>0. From Â¯Î½ >0and Equation (13),\nÂ¯fi=c|S|\nÂ¯Î½Gi\nHi(iâˆˆ If<1). (15)\nFrom Equations (14, 15) and Â¯Î½ >0,\nX\niâˆˆIf=1Hi+X\niâˆˆIf<1Hic|S|\nÂ¯Î½Gi\nHiâˆ’F= 0 (16)\nc|S|\nÂ¯Î½=Fâˆ’P\niâˆˆIf=1HiP\niâˆˆIf<1Gi. (17)\nSubstituting Equations (15, 17) into the objective function of the optimization problem (7), we obtain\nkX\ni=1c|S|Gilog2\u00121\nÂ¯fi\u0013\n=X\niâˆˆIf<1âˆ’c|S|Gilog2 \nFâˆ’P\njâˆˆIf=1HjP\njâˆˆIf<1GjGi\nHi!\n(18)\n=c|S|(1âˆ’Gf=1) log2\u00121âˆ’Gf=1\nFâˆ’Hf=1\u0013\nâˆ’c|S|X\niâˆˆIf<1Gilog2\u0012Gi\nHi\u0013\n.\n(19)\nWe define Gf=1andHf=1asGf=1=P\niâˆˆIf=1GiandHf=1=P\niâˆˆIf=1Hi, respectively.\nPLBF makes the following assumption:\nAssumption A.1. For optimal tandf,If=1=âˆ…orIf=1={k}.\nIn other words, PLBF assumes that there is at most one region for which f= 1, and if there is one, it\nis the region with the highest score. PLBF then tries all possible thresholds tkâˆ’1(under the division\nof the score space).\nIn the following, we discuss the optimal fandtunder the assumption that the j-th to N-th segments\nare clustered as the k-th region ( jâˆˆ {k . . . N }).\nIn the case of If=1=âˆ…, because Gf=1=Hf=1= 0, Equation (19) becomes\nc|S|log2\u00121\nF\u0013\nâˆ’c|S|Gklog2\u0012Gk\nHk\u0013\nâˆ’c|S|kâˆ’1X\ni=1Gilog2\u0012Gi\nHi\u0013\n. (20)\nMeanwhile, in the case of If=1={k}, Equation (19) becomes\nc|S|(1âˆ’Gk) log2\u00121âˆ’Gk\nFâˆ’Hk\u0013\nâˆ’c|S|kâˆ’1X\ni=1Gilog2\u0012Gi\nHi\u0013\n. (21)\nIn both cases, the terms other than the last one are constants under the assumption that the j-th to N-th\nsegments are clustered as the k-th region. Therefore, the value of the objective function is minimized\nwhen the 1st to(jâˆ’1)-th segments are clustered in a way that maximizesPkâˆ’1\ni=1Gilog2\u0010\nGi\nHi\u0011\n. Thus,\nthe problem of finding tthat minimizes memory usage can be reduced to the following problem: for\neachj=k . . . N , we find a way to cluster the 1st to(jâˆ’1)-th segments into kâˆ’1regions that\nmaximizesPkâˆ’1\ni=1Gilog2\u0010\nGi\nHi\u0011\n.\n13\n\nAlgorithm 1 PLBF [14]\nInput:\ngâˆˆRN: probabilities that the keys are contained in each segment\nhâˆˆRN: probabilities that the non-keys are contained in each segment\nFâˆˆ(0,1): target overall FPR\nkâˆˆN: number of regions\nOutput:\ntâˆˆRk+1: threshold boundaries of each region\nfâˆˆRk: FPRs of each region\nAlgorithm:\nTHRES MAXDIVDP(g,h, j, k):\nconstructs DPj\nKLand calculates the optimal thresholds by tracing the transitions backward\nfrom DPj\nKL[jâˆ’1][kâˆ’1]\nOPTIMAL FPR(g,h,t, F, k ):\nreturns the optimal FPRs for each region under the given thresholds\nSPACE USED(g,h,t,f):\nreturns the space size used by the backup Bloom filters for the given thresholds and FPRs\nMinSpaceUsed â† âˆ\ntbestâ†None\nfbestâ†None\nforj=k . . . N\ntâ†THRES MAXDIVDP(g,h, j, k)\nfâ†OPTIMAL FPR(g,h,t, F, k )\nifMinSpaceUsed >SPACE USED(g,h,t,f)then\nMinSpaceUsed â†SPACE USED(g,h,t,f)\ntbestâ†t\nfbestâ†f\nreturn tbest,fbest\nAppendix B Algorithm details\nIn this appendix, we explain the algorithms for PLBF and the proposed method using pseudo-code in\ndetail. This will help to clarify how each method differs from the others.\nFirst, we show the PLBF algorithm in Algorithm 1. For details of OPTIMAL FPR andSPACE USED,\nplease refer to Appendix B and Equation (2) in the original PLBF paper [ 14], respectively. The\nworst case time complexities of OPTIMAL FPR andSPACE USED areO(k2)andO(k), respectively.\nAs the DP table is constructed with a time complexity of O(j2k)for each j=k . . . N , the overall\ncomplexity is O(N3k).\nNext, we show the fast PLBF algorithm in Algorithm 2. The time complexity of building DPN\nKLis\nO(N2k), and the worst-case complexity of subsequent computations is O(Nk2). Because N > k , the\ntotal complexity is O(N2k), which is faster than O(N3k)for PLBF, although fast PLBF constructs\nthe same data structure as PLBF.\nFinally, we describe the fast PLBF++ algorithm. The fast PLBF++ algorithm is nearly identical to\nfast PLBF, but it calculates approximated DPN\nKLwith a computational complexity of O(NklogN).\nSince the remaining calculations are unchanged from fast PLBF, the overall computational complexity\nisO(NklogN+Nk2).\nAppendix C Proof of Theorem 4.3\nIn this appendix, we present a proof of Theorem 4.3. It demonstrates that fast PLBF++ can attain the\nsame accuracy as PLBF and fast PLBF under certain conditions.\nThe following lemma holds.\n14\n\nAlgorithm 2 Fast PLBF\nAlgorithm:\nMAXDIVDP(g,h, N, k ):\nconstructs DPN\nKL\nTHRES MAXDIV(DPN\nKL, j, k):\ncalculates the optimal thresholds by tracing the transitions backward from DPN\nKL[jâˆ’1][kâˆ’1]\nMinSpaceUsed â† âˆ\ntbestâ†None\nfbestâ†None\nDPN\nKLâ†MAXDIVDP(g,h, N, k )\nforj=k . . . N\ntâ†THRES MAXDIV(DPN\nKL, j, k)\nfâ†OPTIMAL FPR(g,h,t, F, k )\nifMinSpaceUsed >SPACE USED(g,h,t,f)then\nMinSpaceUsed â†SPACE USED(g,h,t,f)\ntbestâ†t\nfbestâ†f\nreturn tbest,fbest\nLemma C.1. Letu1,u2,v1, and v2be real numbers satisfying\n0< u 1< u 2 (22)\n0< v1< v2 (23)\nu1\nv1â‰¥u2\nv2. (24)\nFurthermore, we define the function D(x, y)for real numbers x >0andy >0as follows:\nD(x, y) =\u001a\n(u1+x) log2\u0012u1+x\nv1+y\u0013\nâˆ’u1log2\u0012u1\nv1\u0013\u001b\nâˆ’\u001a\n(u2+x) log2\u0012u2+x\nv2+y\u0013\nâˆ’u2log2\u0012u2\nv2\u0013\u001b\n.\n(25)\nIfx\nyâ‰¥u1\nv1holds, then D(x, y)â‰¥0.\nProof.\nâˆ‚D\nâˆ‚x= log2\u0012u1+x\nv1+y\u0013\nâˆ’log2\u0012u2+x\nv2+y\u0013\n(26)\nâˆ‚D\nâˆ‚y=âˆ’u1+x\nv1+y+u2+x\nv2+y. (27)\nFrom Equations (22, 23, 24), when 0< x,0< y, andx\nyâ‰¥u1\nv1, we obtain\nu1+x\nv1+yâ‰¥u2+x\nv2+y. (28)\nTherefore,\nâˆ‚D\nâˆ‚xâ‰¥0 (29)\nâˆ‚D\nâˆ‚yâ‰¤0. (30)\n15\n\nThus,\ninf\n0<x,0<y,x\nyâ‰¥u1\nv1D(x, y)â‰¥ inf\n0<x,0<y,x\ny=u1\nv1D(x, y) (31)\n= inf\nz>0D(zu1, zv1) (32)\n= inf\nz>0\u001a\nzu1log2\u0012zu1\nzv1\u0013\n+u2log2\u0012u2\nv2\u0013\nâˆ’(zu1+u2) log2\u0012zu1+u1\nzv1+v1\u0013\u001b\n(33)\nâ‰¥0. (34)\nThe transformation to Equation (34) is due to Jensenâ€™s inequality [ 27]. This shows that ifx\nyâ‰¥u1\nv1,\nthenD(x, y)â‰¥0.\nProof. We prove Theorem 4.3 by contradiction.\nAssume that the (Nâˆ’1)Ã—(Nâˆ’1)matrix Ais not a monotone matrix . That is, we assume that\nthere exists p(1â‰¤pâ‰¤Nâˆ’2) such that J(p)> J(p+ 1) . LetJ(i)be the smallest jsuch that Aij\nequals the maximum of the i-th row of A. We define a:=J(p)andaâ€²:=J(p+ 1) (a > aâ€²from the\nassumption for contradiction).\nFrom the definitions of AandJ(i), we obtain the following:\nDPKL[p][q] = DP KL[aâˆ’1][qâˆ’1] +dKL(a, p) (35)\nDPKL[p+ 1][q] = DP KL[aâ€²âˆ’1][qâˆ’1] +dKL(aâ€², p+ 1) (36)\nDPKL[p][q]>DPKL[aâ€²âˆ’1][qâˆ’1] +dKL(aâ€², p) (37)\nDPKL[p+ 1][q]â‰¥DPKL[aâˆ’1][qâˆ’1] +dKL(a, p+ 1). (38)\nUsing Equations (35, 36, 38), we evaluate the right side of Equation (37) minus the left side of\nEquation (37).\nDPKL[aâ€²âˆ’1][qâˆ’1] +dKL(aâ€², p)âˆ’DPKL[p][q] (39)\n={DPKL[p+ 1][q]âˆ’dKL(aâ€², p+ 1)}+dKL(aâ€², p)âˆ’ {DPKL[aâˆ’1][qâˆ’1] +dKL(a, p)}(40)\n={DPKL[p+ 1][q]âˆ’DPKL[aâˆ’1][qâˆ’1]}+dKL(aâ€², p)âˆ’dKL(aâ€², p+ 1)âˆ’dKL(a, p) (41)\nâ‰¥dKL(a, p+ 1) + dKL(aâ€², p)âˆ’dKL(aâ€², p+ 1)âˆ’dKL(a, p) (42)\n={dKL(a, p+ 1)âˆ’dKL(a, p)} âˆ’ { dKL(aâ€², p+ 1)âˆ’dKL(aâ€², p)}. (43)\nEquations (35, 36) are used for the transformation to Equation (40), and Equation (42) is used for the\ntransformation to Equation (38).\nWe define u1,u2,v1,v2,x, and yas follows:\nu1:=pX\ni=agi, u2:=pX\ni=aâ€²gi, x:=gp+1,\nv1:=pX\ni=ahi, v2:=pX\ni=aâ€²hi, y:=hp+1.(44)\nFrom the conditions of ideal score distribution andaâ€²< a, the followings hold:\n0< u 1< u 2 (45)\n0< v1< v2 (46)\nx\nyâ‰¥u1\nv1â‰¥u2\nv2(47)\n0< x, y. (48)\nTherefore, from Lemma C.1,\u001a\n(u1+x) log2\u0012u1+x\nv1+y\u0013\nâˆ’u1log2\u0012u1\nv1\u0013\u001b\nâˆ’\u001a\n(u2+x) log2\u0012u2+x\nv2+y\u0013\nâˆ’u2log2\u0012u2\nv2\u0013\u001b\n(49)\n={dKL(a, p+ 1)âˆ’dKL(a, p)} âˆ’ { dKL(aâ€², p+ 1)âˆ’dKL(aâ€², p)} (50)\nâ‰¥0. (51)\n16\n\nAlgorithm 3 OptimalFPR_F (Algorithm 1 in [14])\nInput:\nGâˆˆRN: probabilities that the keys are contained in each region\nHâˆˆRN: probabilities that the non-keys are contained in each region\nFâˆˆ(0,1): target overall FPR\nkâˆˆN: number of regions\nOutput:\nfâˆˆRk: FPRs of each region\nfori= 1. . . k\nfiâ†GiÂ·F\nHi\nwhileâˆƒi:fi>1\nfori= 1. . . k\niffi>1then\nfiâ†1\nGsumâ†0\nHsumâ†0\nfori= 1. . . k\niffi= 1then\nGsumâ†Gsum+Gi\nHsumâ†Hsum+Hi\nfori= 1. . . k\niffi<1then\nfiâ†GiÂ·(Fâˆ’Hsum)\nHiÂ·(1âˆ’Gsum)\nreturn f\nThen, from Equations (39 . . . 43), we obtain\nDPKL[aâ€²âˆ’1][qâˆ’1] +dKL(aâ€², p)âˆ’DPKL[p][q]â‰¥0. (52)\nHowever, this contradicts Equation (37). Hence, Theorem 4.3 is proved.\nAppendix D Modification of the PLBF framework\nIn this appendix, we describe the framework modifications we made to PLBF in our experiments.\nIn the original PLBF paper [14], the optimization problem was designed to minimize the amount of\nmemory usage under a given target false positive rate (Equation (7)). However, this framework makes\nit difficult to compare the results of different methods and hyperparameters. Therefore, we designed\nthe following optimization problem, which minimizes the expected false positive rate under a given\nmemory usage condition:\nminimize\nf,tkX\ni=1Hifi\nsubject tokX\ni=1c|S|Gilog2\u00121\nfi\u0013\nâ‰¤M\nt0= 0< t1< t2<Â·Â·Â·< tk= 1\nfiâ‰¤1 ( i= 1. . . k),(53)\nwhere Mis a parameter that is set by the user to determine the upper bound of memory usage and is\nset by the user.\nAnalyzing as in Appendix A, we find that in order to find the optimal thresholds t, we need to find a\nway to cluster the 1st to(jâˆ’1)-th segments into kâˆ’1regions that maximizesPkâˆ’1\ni=1Gilog2\u0010\nGi\nHi\u0011\nfor each j=k . . . N . We can compute this using the same DP algorithm as in the original framework.\n17\n\nAlgorithm 4 OptimalFPR_M\nInput:\nGâˆˆRN: probabilities that the keys are contained in each region\nHâˆˆRN: probabilities that the non-keys are contained in each region\nMâˆˆR: upper bound of the total memory usage of backup Bloom filters\nkâˆˆN: number of regions\nOutput:\nfâˆˆRk: FPRs of each region\nKsumâ†0\nfori= 1. . . k\nKsumâ†Ksum+Gilog2\u0010\nGi\nHi\u0011\nÎ²â†M+c|S|Ksum\nc|S|\nfori= 1. . . k\nfiâ†2âˆ’Î²Gi\nHi\nwhileâˆƒi:fi>1\nfori= 1. . . k\niffi>1then\nfiâ†1\nGsumâ†0\nHsumâ†0\nKsumâ†0\nfori= 1. . . k\niffi= 1then\nGsumâ†Gsum+Gi\nHsumâ†Hsum+Hi\nelse\nKsumâ†Ksum+Gilog2\u0010\nGi\nHi\u0011\nÎ²â†M+c|S|Ksum\nc|S|(1âˆ’Gsum)\nfori= 1. . . k\niffi<1then\nfiâ†2âˆ’Î²Gi\nHi\nreturn f\nAlso similar to Appendix A, introducing If=1andIf<1for analysis, it follows that the optimal false\npositive rates Â¯fis\nÂ¯fi= 2âˆ’Î²Gi\nHi(iâˆˆ If<1), (54)\nwhere\nÎ²=M+c|S|P\niâˆˆIf<1Gilog2\u0010\nGi\nHi\u0011\nc|S|(1âˆ’Gf=1). (55)\nIn the original framework, the sets If=1andIf<1are obtained by repeatedly solving the relaxed\nproblem and setting fito 1 for regions where fi>1(for details of this algorithm, please refer to\nthe Section 3.3.3 in the original PLBF paper [ 14]). In our framework, we can find the sets If=1and\nIf<1in the same way.\nThe pseudo-code for OPTIMAL FPR_F in the original PLBF paper [ 14] and OPTIMAL FPR_M , which\nis conditioned on the memory usage M, is shown in Algorithm 3 and Algorithm 4. For both functions,\nthe best-case complexity is O(k), and the worst-case complexity is O(k2).\nAlso, the fast PLBF algorithm for the case conditioned by memory usage is shown in Algorithm 5.\nIt is basically the same as the original PLBF Algorithm 2, but instead of using the SPACE USED\nfunction, the EXPECTED FPR function is used here. Then, the algorithm selects the case when the\nexpected overall false positive rate calculated using the training data is minimized.\n18\n\nAlgorithm 5 Fast PLBF conditioned by memory usage\nAlgorithm:\nMAXDIVDP(g,h, N, k ):\nconstructs DPN\nKL\nTHRES MAXDIV(DPN\nKL, j, k):\ncalculates the optimal thresholds by tracing the transitions backward from DPN\nKL[jâˆ’1][kâˆ’1]\nOPTIMAL FPR_M (g,h,t, M, k ):\nreturns the optimal FPRs for each region under the given thresholds and the memory usage\nEXPECTED FPR(g,h,t,f):\nreturns the expected overall false positive rate for the given thresholds and FPRs\nMinExpectedFPR â† âˆ\ntbestâ†None\nfbestâ†None\nDPN\nKLâ†MAXDIVDP(g,h, N, k )\nforj=k . . . N\ntâ†THRES MAXDIV(DPN\nKL, j, k)\nfâ†OPTIMAL FPR_M (g,h,t, M, k )\nifMinExpectedFPR >EXPECTED FPR(g,h,t,f)then\nMinExpectedFPR â†EXPECTED FPR(g,h,t,f)\ntbestâ†t\nfbestâ†f\nreturn tbest,fbest\nAppendix E Additional ablation study for hyperparameters\nThis appendix details the ablation studies for the hyperparameters Nandk. In this appendix,\nwe discuss the results of the ablation studies that were not presented in Section 5.3. The results\nconfirm that the expected false positive rate decreases monotonically as the hyperparameters Nandk\nincrease. We also confirm that the proposed methods are superior to PLBF in terms of hyperparameter\ndetermination.\nFirst, we confirm that the expected false positive rate decreases monotonically as Norkincreases.\nThe expected false positive rate is computed using training data (objective function in Equation (53)).\nFigure 14 shows the expected false positive rate at various Nandk. The ablation study for Nis done\nwithkfixed to 5. The ablation study for kis done with Nfixed to 1,000.\nThe expected false positive rate decreases monotonically as Norkincreases for all datasets and\nmethods. However, as observed in Section 5.3, the false positive rate at test time does not necessarily\ndecrease monotonically as Norkincreases. Also, the construction time increases as Norkincreases.\nTherefore, it is necessary to set appropriate Nandkwhen practically using PLBFs.\nTherefore, we next performed a comprehensive experiment at various Nandk. Figure 15 shows\nthe construction time and false positive rate for each method when the memory usage of the backup\nbloom filter is fixed at 500 Kb, Nis set to 8,16, . . . , 1024 , andkis set to 3,5,10,and20. The curves\nconnect the same kand different Nresults.\nThe results show that the proposed methods are superior to PLBF because it is easier to set hyper-\nparameters to construct accurate data structures in a shorter time. PLBF can achieve the same level\nof accuracy with the same construction time as fast PLBF and fast PLBF++ if the hyperparameters\nNandkare set well. However, it is not apparent how Nandkshould be set when applying the\nalgorithm to unseen data. If Norkis set too large, the construction time will be very long, and if N\norkis set too small, the false positive rate will be large. The proposed methods can construct the\ndata structure quickly even if the hyperparameters Nandkare too large. Therefore, by using the\nproposed methods instead of PLBF and setting Nandkrelatively large, it is possible to quickly and\nreliably construct a data structure with reasonable accuracy.\nFurthermore, we performed an ablation study using fast PLBF to obtain guidelines for set-\nting the hyperparameters Nandk(Figure 16). We observed false positive rates for N=\n19\n\nPLBF Fast PLBF (Proposed) Fast PLBF++ (Proposed)\n(a) Malicious URLs Dataset.\n(b) EMBER Dataset.\nFigure 14: Expected false positive rate computed with training data at various Nork. The memory\nusage of the backup bloom filter is 500 Kb. The ablation studies for Nare done with kfixed to 5 (left\nfigures). The ablation studies for kare done with Nfixed to 1,000 (right figures).\n20,50,100,200,300, . . . , 1500 andk= 50 ,100,200,300, . . . , 1400 withk < N . For the Mali-\ncious URLs Dataset, the false positive rate was lowest when N= 1300 andk= 700 (in this case,\nthe fast PLBF construction is 333 times faster than PLBF). And for the Ember dataset, the false\npositive rate was lowest when N= 500 andk= 50 (in this case, the fast PLBF construction is\n45 times faster than PLBF). The Malicious URLs Dataset showed an almost monotonous decrease\nin the false positive rate as Nandkincreased, while the Ember Dataset showed almost no such\ntrend. This is thought to be due to a kind of overlearning that occurs as Nandkare increased in\nthe Ember Dataset. Although a method for determining the appropriate Nandkhas not yet been\nestablished, we suggest setting Nâˆ¼1000 andkâˆ¼100. This is because the false positive rate is\nconsidered to be small enough at this level, and the proposed method can be constructed in a few tens\nof seconds. In any case, the proposed method is useful because it is faster than the original PLBF for\nany hyperparameters Nandk.\nAppendix F Experiments on PLBF solution to relaxed problem\nThe PLBF paper [ 14] proposes two methods: a simple method and a complete method. In this\nappendix, we conducted experiments on the simple method and evaluated the difference in accuracy\ncompared to the complete method. The results revealed that, in certain cases, this method exhibited\nsignificantly lower accuracy than the complete method. It was then confirmed that fast PLBF and fast\nPLBF++ are superior to the simple method in terms of the trade-off between construction time and\naccuracy.\nThe simpler method, described in section 3.3.3 of [ 14], does not consider the condition that fiâ‰¤\n1 (i= 1. . . k)when finding the optimal t. In other words, this method solves the relaxed problem\n20\n\nPLBF Fast PLBF (Proposed) Fast PLBF++ (Proposed)\nğ‘˜=3ğ‘˜=3ğ‘˜=3ğ‘˜=5ğ‘˜=10ğ‘˜=20ğ‘˜=5ğ‘˜=5ğ‘˜=20ğ‘˜=10ğ‘˜=20ğ‘˜=10(a) Malicious URLs Dataset\nğ‘˜=3ğ‘˜=3ğ‘˜=3ğ‘˜=5ğ‘˜=10ğ‘˜=20ğ‘˜=5ğ‘˜=5ğ‘˜=20ğ‘˜=10ğ‘˜=20ğ‘˜=10\n(b) EMBER Dataset\nFigure 15: Construction time and FPR for various Nandk. The curves connect the same kand\ndifferent Nresults. The memory usage of the backup bloom filter is 500 Kb.\nthat does not consider the condition fiâ‰¤1 (i= 1. . . k). The complete method uses the first\nmethod repeatedly as a subroutine to solve the general problem and find the optimal t. Here, the time\ncomplexity of the method for the relaxed problem is O(N2k), and that of the method for the general\nproblem is O(N3k).\nWe experimented with the simpler method. Figure 17 shows the construction time (the hyperpa-\nrameters for PLBFs were set to N= 1,000andk= 5). The PLBF for the relaxed problem has\nabout the same construction time as our proposed fast PLBF. This is because the time complexity for\nconstructing the two methods is O(N2k)for both. Figure 18 shows the trade-off between memory\nusage and FPR. On the URL dataset, the method for the relaxed problem is almost as accurate as\nthe method for the general problem. At most, it has a false positive rate of only 1.06 times greater.\nOn the other hand, on the EMBER dataset, the method for the relaxed problem is significantly less\naccurate than the method for the general problem, especially when memory usage is small. It has up\nto 1.48 times higher false positive rate than the complete method.\nIn summary, the method for the relaxed problem can construct the data structure faster than the\nmethod for the general problem, but the accuracy can be significantly worse, especially when memory\nusage is small. Our proposed fast PLBF has the same accuracy as the complete PLBF in almost the\nsame construction time as the method for the relaxed problem. Our fast PLBF++ can construct the\ndata structure in even less time than the method for the relaxed problem, and the accuracy degradation\nis relatively small.\n21\n\n50\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n1100\n1200\n1300\n1400\n1500\nN205010020030040050060070080090010001100120013001400k\n0.01240.01260.01280.01300.01320.0134\nFalse Positive Rate\n(a) Malicious URLs Dataset\n50\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n1100\n1200\n1300\n1400\n1500\nN205010020030040050060070080090010001100120013001400k\n0.00830.00840.00850.00860.0087\nFalse Positive Rate\n (b) EMBER Dataset\nFigure 16: FPR for N= 50,100,200,300, . . . , 1400 andk= 20,50,100,200,300, . . . , 1500 with\nk < N .\nAppendix G Experiments with artificial datasets\nWe experimented with artificial datasets. Fast PLBF++ is faster to construct than fast PLBF. However,\nfast PLBF++ is not theoretically guaranteed to be as accurate as PLBF, except when the score\ndistribution is â€œideally monotonic.â€ Therefore, we evaluated the accuracy of fast PLBF++ by creating\na variety of datasets, ranging from data with monotonicity to data with little monotonicity. The results\nshow that the smaller the monotonicity of the score distribution, the larger the difference in accuracy\nbetween fast PLBF++ and PLBF. We also observed that in most cases, the false positive rate for fast\nPLBF++ is within 1.1 times that of PLBF, but in cases where there is little monotonicity in the score\ndistribution, the false positive rate of PLBF++ can be up to 1.85 times that of PLBF.\nHere, we explain the process of creating an artificial dataset, which consists of two steps. First,\nas in the original PLBF paper [ 14], the key and non-key score distribution is generated using the\nZipfian distribution. Figure 19(a) shows a histogram of the distribution of key and non-key scores at\nthis time, and Figure 20(a) shows gi/hi(i= 1. . . N )when N= 1,000. This score distribution is\nideally monotonic. Next, we perform swaps to add non-monotonicity to the score distribution. Swap\nrefers to changing the scores of the elements in the two adjacent segments so that the number of\nkeys and non-keys in the two segments are swapped. Namely, an integer iis randomly selected from\n{1,2, . . . , N âˆ’1}, and the scores of elements in the i-th segment are changed so that they are included\nin the (i+ 1) -th segment, and the scores of elements in the (i+ 1) -th segment are changed to include\nthem in the i-th segment. Figures 19(b) to 19(i) and Figures 20(b) to 20(i) show the histograms of the\nscore distribution and gi/hi(i= 1. . . N )for10,102, . . . , 108swaps, respectively. It can be seen\nthat as the number of swaps increases, the score distribution becomes more non-monotonic. For each\ncase of the number of swaps, 10 different datasets were created using 10 different seeds.\nFigure 21 shows the accuracy of each method for each number of swaps, with the seed set to 0.\nHyperparameters for PLBFs are set to N= 1,000andk= 5. It can be seen that fast PLBF and fast\nPLBF++ achieve better Pareto curves than the other methods for all datasets. It can also be seen that\nthe higher the number of swaps, the more often there is a difference between the accuracy of fast\nPLBF++ and fast PLBF.\nFigure 22 shows the difference in accuracy between fast PLBF++ and fast PLBF for each swap count.\nHere, the â€œrelative false positive rateâ€ is the false positive rate of fast PLBF++ divided by that of\nPLBF constructed with the same hyperparameters and conditions (note that fast PLBF constructs the\nsame data structure as PLBF, so the false positive rate of fast PLBF is the same as that of PLBF). We\ncreated 10 different datasets for each swap count and conducted 6 experiments for each dataset and\nmethod with a memory usage of 0.25Mb, 0.5Mb, 0.75Mb, 1.0Mb, 1.25Mb, and 1.5Mb. Namely, we\ncompared the false positive rates of fast PLBF and fast PLBF++ under 60 conditions for each swap\ncount. The result shows that fast PLBF++ consistently achieves the same accuracy as PLBF in a total\nof240experiments where the number of swaps is 103or less. It also shows that in the experiments\nwhere the number of swaps is 107or less, the false positive rate of fast PLBF++ is less than 1.1 times\n22\n\nInsert keys to Bloom filter Train machine learning model Find optimal parameters\n100200\nPLBF PLBF\n(for relaxed problem)Fast PLBF\n(Proposed)   Fast PLBF++\n   (Proposed)024Construction Time [s](a) Malicious URLs Dataset\n100200\nPLBF PLBF\n(for relaxed problem)Fast PLBF\n(Proposed)   Fast PLBF++\n   (Proposed)0.02.55.07.5Construction Time [s]\n(b) EMBER Dataset\nFigure 17: Construction time. The memory usage of the backup bloom filter is 500 Kb, and the\nhyperparameters for PLBFs are N= 1,000andk= 5.\nthat for PLBF, except for 14 cases. However, in the cases of 108swap counts (where there is almost\nno monotonicity in the score distribution), the false positive rate for fast PLBF++ is up to 1.85 times\nthat for PLBF.\n23\n\nBloom filter\nPLBF (for relaxed problem)Ada-BF\nFast PLBF (Proposed)Sandwiched LBF\nFast PLBF++ (Proposed)PLBF\n0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0\nTotal Memory Usage [Mbit]  103\n102\n101\nFalse Positive Rate\n(a) Malicious URLs Dataset\n1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0\nTotal Memory Usage [Mbit]  103\n102\n101\nFalse Positive Rate\n(b) EMBER Dataset\nFigure 18: Trade-off between memory usage and FPR. The hyperparameters for PLBFs are N=\n1,000andk= 5.\n24\n\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104CountPositive\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104CountNegative(a)0swaps (score distribution is ideally monotonic)\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104CountPositive\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104CountNegative (b)10swaps\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104CountPositive\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104CountNegative\n(c)102swaps\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104CountPositive\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104CountNegative (d)103swaps\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104CountPositive\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104CountNegative\n(e)104swaps\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104CountPositive\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104CountNegative (f)105swaps\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104CountPositive\n0.0 0.2 0.4 0.6 0.8 1.0\nScore102103104CountNegative\n(g)106swaps\n0.0 0.2 0.4 0.6 0.8 1.0\nScore103104CountPositive\n0.0 0.2 0.4 0.6 0.8 1.0\nScore103104CountNegative (h)107swaps\n0.0 0.2 0.4 0.6 0.8 1.0\nScore103104CountPositive\n0.0 0.2 0.4 0.6 0.8 1.0\nScore103104CountNegative\n(i)108swaps\nFigure 19: Score distribution histograms of artificial datasets. All figures are with the seed set to 0.\n25\n\n0 200 400 600 800 1000\ni103\n102\n101\n100101102103gi/hi(a)0swaps (score distribution is ideally monotonic)\n0 200 400 600 800 1000\ni103\n102\n101\n100101102103gi/hi (b)10swaps\n0 200 400 600 800 1000\ni103\n102\n101\n100101102103gi/hi\n(c)102swaps\n0 200 400 600 800 1000\ni103\n102\n101\n100101102103gi/hi (d)103swaps\n0 200 400 600 800 1000\ni103\n102\n101\n100101102103gi/hi\n(e)104swaps\n0 200 400 600 800 1000\ni103\n102\n101\n100101102103gi/hi (f)105swaps\n0 200 400 600 800 1000\ni103\n102\n101\n100101102103gi/hi\n(g)106swaps\n0 200 400 600 800 1000\ni103\n102\n101\n100101102103gi/hi (h)107swaps\n0 200 400 600 800 1000\ni103\n102\n101\n100101102103gi/hi\n(i)108swaps\nFigure 20: Ratio of key to non-key of artificial datasets. All figures are with the seed set to 0.\n26\n\nBloom filter\nFast PLBF (Proposed)Ada-BF\nFast PLBF++ (Proposed)Sandwiched LBF\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\nTotal Memory Usage [Mbit]  104\n103\n102\n101\nFalse Positive Rate\n(a)0swaps (score distribution is ideally monotonic)\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\nTotal Memory Usage [Mbit]  104\n103\n102\n101\nFalse Positive Rate\n (b)10swaps\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\nTotal Memory Usage [Mbit]  104\n103\n102\n101\nFalse Positive Rate\n(c)102swaps\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\nTotal Memory Usage [Mbit]  104\n103\n102\n101\nFalse Positive Rate\n (d)103swaps\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\nTotal Memory Usage [Mbit]  104\n103\n102\n101\nFalse Positive Rate\n(e)104swaps\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\nTotal Memory Usage [Mbit]  104\n103\n102\n101\nFalse Positive Rate\n (f)105swaps\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\nTotal Memory Usage [Mbit]  104\n103\n102\n101\nFalse Positive Rate\n(g)106swaps\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\nTotal Memory Usage [Mbit]  104\n103\n102\n101\nFalse Positive Rate\n (h)107swaps\n0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6\nTotal Memory Usage [Mbit]  104\n103\n102\n101\nFalse Positive Rate\n(i)108swaps\nFigure 21: Trade-off between memory usage and FPR for artificial datasets. All figures are with the\nseed set to 0. The hyperparameters for PLBFs are N= 1,000andk= 5.\n27\n\n0101102103104105106107108\nNumber of Swaps1.01.21.41.61.8Relative False Positive Rate\nFigure 22: Distribution of the â€œrelative false positive rateâ€ of fast PLBF++ for each swap count.\nThe â€œrelative false positive rateâ€ is the false positive rate of fast PLBF++ divided by that of PLBF\nconstructed with the same hyperparameters and conditions.\n28",
  "textLength": 66040
}