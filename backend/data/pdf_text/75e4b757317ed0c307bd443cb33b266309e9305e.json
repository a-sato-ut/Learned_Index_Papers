{
  "paperId": "75e4b757317ed0c307bd443cb33b266309e9305e",
  "title": "Deep learning, transparency, and trust in human robot teamwork",
  "pdfPath": "75e4b757317ed0c307bd443cb33b266309e9305e.pdf",
  "text": "Deep Learning, Transparency and Trust in Human Robot\nTeamwork\nMichael Lewis, Huao Li\nUniversity of Pittsburgh, School of Computing and Information\nKatia Sycara\nCarnegie Mellon University, Robotics Institute\nAbstract\nFor Autonomous AI systems to be accepted and trusted, the users should be able to\nunderstand the reasoning process of the system (i.e., the system should be transpar-\nent). Robotics presents unique programming difﬁculties in that systems need to map\nfrom complicated sensor inputs such as camera feeds and laser scans to outputs such\nas joint angles and velocities. Advances in Deep Neural Networks are now making it\npossible to replace laborious handcrafted features and control code by learning control\npolicies directly from high dimensional sensor inputs. Because Atari games, where\nthese capabilities were ﬁrst demonstrated, replicate the robotics problem they are ideal\nfor investigating how humans might come to understand and interact with agents who\nhave not been explicitly programmed. We present computational and human results for\nmaking DRLN more transparent using object saliency visualizations of internal states\nand test the effectiveness of expressing saliency through teleological verbal explana-\ntions.\n1. Introduction\nTeamwork is a set of interrelated reasoning, actions and behaviors of each team\nmember that adaptively combine to fulﬁll shared team goals (Morgan et al., 1986). Ex-\nperimental evidence from high performance human teams resulted in a set of drivers of\nteam effectiveness (Salas et al., 2005, 2008). These drivers are: team leadership ,mu-\ntual performance monitoring ,backup behaviors (ability to anticipate other team mem-\nbers’ needs and shift work and cognitive workload to achieve appropriate balance),\nadaptability (ability to adjust strategies and actions based on dynamic changes in mis-\nsion and environment), team orientation (taking other’s behavior into account during\ngroup interactions and belief on team goals over individual goals), shared mental mod-\nels(organizing knowledge structure of the relationship between tasks and how the team\nwill perform them), closed loop communication (reliable exchange of information) and\nﬁnally and most crucially mutual trust (the shared belief that team members will per-\nform their roles and protect the interests of their teammates). As technology enables\nPreprint submitted to Elsevier May 25, 2020\n\nincreased machine autonomy, human-machine teaming could acquire the same charac-\nteristics as human-human teaming.\nBesides being an important ingredient of teamwork, trust has been found to be im-\nportant in human use of automation: people tend to rely on automation they trust and\nnot use automation they do not trust. This has generated sustained interest in concep-\ntualizations of trust and its relation to human interaction with automation. Trust has\nbeen deﬁned by Mayer et al. (1995) as “ The willingness of a party to be vulnerable\nto the actions of another party based on the expectation that the other will perform\na particular action important to the trustor, irrespective of the ability to monitor or\ncontrol that party. ” In the context of trust in automation the above deﬁnition has typi-\ncally been interpreted as a human’s willingness to rely on automation to perform some\ntask. Research in human interaction with automation, which we believe is also valid\nfor human interaction with autonomy, has found that the human may fail to use the\nautomation when it would be advantageous, called disuse or under-reliance , fail to\nmonitor it properly when in use, or accept its recommendations and actions when inap-\npropriate, called misuse or over-reliance (Lyons & Stokes, 2012). Lee & Moray (1992,\n1994); Muir & Moray (2013); Lewandowsky et al. (2000) have shown that trust to-\nwards automation can mediate reliance. Additionally, operator trust has been found to\nvary dynamically , accumulating over periods of successful performance, then declin-\ning sharply when failures or poor performance are encountered (Lee & Moray, 1994;\nGao & Lee, 2006; Xu & Dudek, 2015).\nIn social robots, incorrect trust calibration can lead to extreme overtrust as demon-\nstrated in Robinette et al. (2016) where participants followed the directions of a demon-\nstrably dysfunctional robot to evacuate a smoke ﬁlled room with detrimental results,\nincluding getting directed into closets rather than following exit signs. In other stud-\nies reviewed by Schaefer (2013) trust in social robots increased with matches between\nrobot appearance and user expectations and were generally higher for less socially\ncompetent robots.\nAnother characteristic that contributes to both teamwork effectiveness and trust is\ntransparency . Systems that are more transparent in conveying their reasoning should\nbe more trusted, since they would be more easily understood by their users (Simpson\net al., 1995; Sycara et al., 1998; Lewis, 1998; Lewis & Heidorn, 1991) and hopefully\ntheir users would be able to better judge their capabilities, thus improving their trust\ncalibration. Additionally transparency in a human-robot context can be viewed as a\nmethod to establish shared intent and shared awareness between a human and a ma-\nchine (Lyons, 2013). Although there are multiple deﬁnitions of agent transparency\n(Chen et al., 2014; Lyons & Havig, 2014), we use, with minor variation, the deﬁni-\ntion proposed by Chen et al. (2014): ”Agent transparency is the quality of an interface\n(e.g., visual, linguistic) pertaining to its abilities to afford an operator’s comprehension\nabout an intelligent agent’s intent, performance, future plans, and reasoning process”.\nThe goal of transparency is not to relay all of a system’s capabilities, behaviors, and\ndecision-making rationale to the human. Ideally, agents should relay clear and efﬁcient\ninformation as succinctly as possible, thus enabling the human to maintain a proper\nunderstanding of the system in its tasking environment. The human factors and com-\nputational literature (Kim, 2015; Mercado et al., 2016; Ribeiro et al., 2016) has pointed\nout the need for system transparency as a way to increase trust in the system.\n2\n\nAs agents become more sophisticated and independent via learning and interaction,\nit is critical for their human counterparts to understand their behaviors, the reasoning\nprocess behind those behaviors, and the expected outcomes to properly calibrate their\ntrust in the systems and make appropriate decisions (de Visser et al., 2014; Lee & See,\n2004; Mercado et al., 2016). When interacting with autonomous intelligent agents,\npeople tend to regard them as intentional individuals and explain their behaviors in\nterms of interpersonal relationships (de Graaf & Malle, 2017). That requires an ex-\nplainable agent to clarify its actions by offering reasons of beliefs, desires, and inten-\ntions (Langley et al., 2017). Indeed, past studies have shown that humans sometimes\nquestion the accuracy and effectiveness of agents’ actions due to the human’s difﬁcul-\nties understanding the state/status of the agent (Bitan & Meyer, 2007; Seppelt & Lee,\n2007; Stanton et al., 2007) and the rationales behind the behaviors (Linegang et al.,\n2006).\nIn recent years automation and autonomous system have increasingly relied on Ma-\nchine Learning. ML systems are typically used for two broad types of problems. First,\nthey are used for classiﬁcation, which relies on supervisory methods with ground truth\ngiven by labelled data, and produces a judgment as to whether an input belongs to a\nparticular class. Such systems have become ubiquitous in almost all areas of human\nendeavor, such as Web services, health care, education, insurance, law enforcement and\ndefense (Lipron, 2013). Machine learning algorithms make important decisions in our\ninteractions inﬂuencing the news we see, our ﬁnances (who gets a loan, or a particular\nline of credit), careers (algorithms often ﬁlter job applications). Courts have employed\nalgorithms to predict the probability that an individual relapses into criminal behav-\nior (Choudlechova, 2016). Neural Networks used in classiﬁcation have revolutionized\ncomputer vision and natural language understanding. Second, ML systems solve se-\nquential decision making problems that use mainly unsupervised methods to produce\na series of decisions that would give an optimal reward. These systems work using\nDeep Neural Networks for Reinforcement Learning (DRL) where an agent explores\nthe space of possible strategies in an environment and receives feedback (positive or\nnegative) on the outcome of choices it makes. Given a particular domain, exploration\nallows the agent to form a strategy, called policy, that allows it to generate and follow\na sequence of actions to maximize its payoff (see section 4.1 for related work in this\narea). The last few years have witnessed a rapid growth of research and interest in the\ndomain of deep Reinforcement Learning (RL) due to the signiﬁcant progress in solving\nRL problems (Arulkumaran et al., 2017). Deep RL has been applied to a wide variety\nof disciplines ranging from game playing, to robotics to dialogue systems (Silver et al.,\n2017; Mnih et al., 2015b; Levine et al., 2016; Kraska et al., 2018; Williams et al., 2017;\nChoi et al., 2017).\nSelf-driving cars that are poised to be deployed in the near future increasingly em-\nploy AI-based object recognition software, and military autonomous systems may be\ncalled to make decisions that cost civilian lives. These systems are more opaque than\ntheir predecessors since they rely almost exclusively on learning form data in order\nto shape their conclusions and behavior, as opposed to model-based systems that are\nmore understandable and often have algorithms that provide formal guarantees of per-\nformance. The inner logic and reasoning of DRL systems are opaque and difﬁcult to\nbe understood even by their own designers.\n3\n\nThe opacity of these systems is becoming increasingly problematic and therefore\nthere is increasing clamor for transparency in the machine learning community as well\n(Kim, 2015; Ribeiro et al., 2016). This need for system transparency has resonated\nwith policy makers as well. For example, new regulations in the European Union\npropose that individuals affected by algorithmic decisions have a right to explanation\n(Goodman & Flaxman, 2016).\nIn this chapter, we discuss the intertwined notions of trust and transparency in the\ncontext of human-agent teamwork. Here agents can be autonomous systems that en-\ngage in sequential decision making processes and control, can be computational pro-\ncesses that comprise robotic components, such as a vision system for the robot, or\nprovide information and suggestions to humans. We will present a review of trust and\ntransparency in the human factors literature emphasizing the need to consider trans-\nparency in all dimensions of the trust construct, and then we will turn our attention to\nthe challenges of transparency for Deep Learning algorithms. We will then present our\nown work on transparency for Deep Reinforcement Learning, followed by conclusions\nand open research problems.\n2. Factors Affecting Trust in Automation\nIt is generally agreed in the psychological literature that trust is best conceptualized\nas amultidimensional psychological attitude involving beliefs and expectations about\nthe trustee’s trustworthiness derived from experience and interactions with the trustee\n(Jones & George, 1998). Although in the literature, the number and concepts in the\ntrust dimensions vary Jian et al. (2000); Madsen & Gregor (2000), Lee & See (2004)\npoint out a broad consensus on 3 dimensions they label Purpose (what the automation\nis supposed to do), Process (how automation goes about fulﬁlling its purpose) and\nPerformance (actual performance). Moreover, it has been found that trust is dynamic\n(Lee & Moray, 1994; Lee & See, 2004; Nam et al., 2017, 2019) and a human’s history\nof interaction with automation affects future behavior indirectly through changes in\ntrust.\nThe most important factors that affect trust in automation are:\n(a)System reliability: Prior literature has provided empirical evidence that as au-\ntomation reliability declines, human trust declines and vice versa (Hancock et al., 2011;\nMoray et al., 2000; Moray & Inagaki, 1999; Riley, 1994; Parasuraman & Manzey,\n2010). Additional studies have shown that imperfect (unreliable) automation can have\nadverse effects on reliance and compliance (Dixon & Parasuraman, 2006; Meyer, 2004),\nand overall system performance de Visser et al. (2006); Dzindolet et al. (2003). Moray\net al. (2000) showed that declining system reliability can lead to systematic decline in\ntrust and trust expectations, and most crucially, these changes can be measured over\ntime.\n(b)System predictability: Research has shown that when people have prior knowl-\nedge of faults, these faults do not necessarily diminish trust in the system (Riley, 1994;\nLewandowsky et al., 2000), possibly due to the fact that knowing how the automa-\ntion may fail reduces the uncertainty and consequent risk associated with use of the\nautomation.\n4\n\n(c)System intelligibility and transparency: Prior research on trust in automation\nfound that providing human operators with information related to the reliability of an\nautomated tool promoted more optimal reliance strategies on the tool (Lyons & Havig,\n2014). Further, information related to the limitations of an automated tool aids in trust\nrecovery following errors of the automation (Choudlechova, 2016). Given that reliabil-\nity of performance is the biggest determiner of trust (Hancock et al., 2011),providing\nadditional information about performance such as knowledge of results Beck et al.\n(2007); Dzindolet et al. (2002) or conﬁdence judgements Dadashi et al. (2013) have\nalso been widely used to increase trust through greater transparency. The Human Fac-\ntors literature has paid less attention to the Purpose and Process components of trust,\nmost frequently assuming Purpose to be evident. Attention to Process has concentrated\nmostly on developing displays for mission-based systems to show the state of the sys-\ntem in more transparent ways to allow the human to understand the system (Wang\net al., 2016) or to improve trust calibration (Lyons et al., 2016).\n(d)Level of Automation: Another factor that may affect trust in the system is its\nlevel of automation (i.e., the degree to which the system acts on its own). Since higher\nlevels of automation are more complex, thus potentially more opaque to the operator,\nhigher levels of automation are frequently (Calhoun et al., 2009; Amato et al., 2011;\nNam et al., 2019; Kira & Potter, 2009) found to engender less trust. To overcome this\ndeﬁcit transparency becomes more important when the system is more autonomous.\n(Oh et al., 2015) In summary, prior literature indicates that a high level of autonomy\nmay cause the operator to undertrust the system. These implications are echoed in the\ndistrust and clamor for transparency for systems based on Deep Neural Networks.\n3. Trust and Transparency in Human-Autonomy Teaming\nSince transparency is an important ingredient of trust, and since trust is multi-\ndimensional, we believe that work on transparency should span all trust dimensions,\nin particular purpose, process and performance. Additionally, since factors such as De-\ngree of Autonomy modulates trust, its inﬂuence should also be studied. Transparency\ncan be viewed as the degree to which automation conveys the basis of its behavior.\nParasuraman et al. (2000) proposed a model characterizing automation as a sequence\nof stages which were reduced to two by Wickens (2018): 1) situation assessment and\n2) action choice and execution. Transparency effects can usefully be organized by these\nstages of autonomy Wickens (2018) and dimensions of trust Chen et al. (2014).\nFigure 1 presents constituents of transparency that we brieﬂy discuss in this section\nin the context of the trust dimensions of performance, purpose and process (also de-\npicted in the ﬁgure). Additionally, the ﬁgure sketches our explanation model for DRL\nagents and shows its relationship with transparency and trust.\nTransparency of performance (execution stage) such as system reliability is of-\nten assumed to be directly observable to users yet may be perceived inaccurately. Biros\net al. (2004), for example, found trust and reliance to be inﬂuenced by cover stories.\nOther studies (Beck et al., 2007) have found supplying knowledge of results to lead to\nbetter trust calibration and improved reliance. Annotating decisions with conﬁdence\njudgments is another widely used technique for providing greater insight into system\nperformance. This has been done in a variety of ways (e.g., by providing probabilities\n5\n\nState(Images)State(Images)State(Images)ClassifierCNNClassified Concepts(e.g., Objects)State feature vectorDNN!\"!#!$…Learned DRL Model•Explainability•Interpretability•Auditability•IntrospectionTransparency\n…Explanation Model\nObject-Saliency Maps [Sec 6.1]Verbal Explanations  [Sec 6.2]Domain Model / Background Knowledge\n•Process•Purpose•PerformanceStateRewardActionsQ-valueTrustFigure 1: A block diagram of the visual and verbal explanations offered by our model of DRL agents and its\nrelationship with transparency and trust. The model is reported in Section 5.\nof success) (Lyons et al., 2016; Wang et al., 2016; Dadashi et al., 2013), conﬁdence in\ndetection (Dadashi et al., 2013) or expected reward from selected robot action (Wang\net al., 2016).\nTransparency of purpose: For pre-programmed automation the system’s purpose\nis static and known a priori (purpose transparency was baked in) to the user. As systems\nbecome increasingly autonomous and thus learning and adaptive, their intent/purpose\nmay change over time and may depend on environmental and social factors. Therefore,\nthe system’s intent must be communicated to the human to help the human understand\nand anticipate system activities. To do this, the system must be able to introspect , (i.e.,\nbe aware of its own goals, preferences and what factors bring change to them). For\nexample, a system might change its intent in response to a perceived change in intent\nof the human with whom the system is teaming.\nIn experiments with humans, the system’s purpose is almost always informally\nconveyed through instructions or the demand characteristics of an experiment, however,\nwhen explicitly manipulated, as in Sadler et al. (2016) effects on trust and reliance were\nfound. More commonly, experiments involving transparency of automation do not\ndiscriminate between stages of automation or form of explanation. They instead mix\ncategories often in an additive manner such as Verberne et al. (2012) which started with\nan unsupported recommendation in the control condition (execution stage), increased\ntransparency of performance by conveying expected risk in the second condition, and\nadded an explanation (situation assessment stage) involving process in the third.\nTransparency of process (situation assessment stage): While trust and the de-\ncision to rely on or comply with an autonomous agent may depend on knowledge of\nits purpose or reliability, the reciprocal interactions needed for teamwork depend cru-\ncially on the ability to predict and coordinate behavior (i.e., knowledge of process). In\nthe current human factors literature, transparency of process involves conveying infor-\n6\n\nmation to the human about the system’s decision making process to support operator\nsituation awareness (Chen et al., 2014; Lee & See, 2004). Increased information, such\nas explanations by an agent, may have a complicated effect on operators’ workload.\nAlthough a human may need to process more information from an agent with high\ntransparency, this information might be desired in order to achieve better performance.\nTherefore, additional information may help operators to understand the intentions and\nbehaviors of an agent and correct their expectation and trust in it without increasing\nworkload (Chen & Barnes, 2014). Recent work has conﬁrmed the above argument in\nmulti-unmanned-vehicle control task scenarios (Mercado et al., 2016).\nThe reasoning (transparency of process) of pre-programmed automation, though\nnot necessarily known to the user a priori, was easier to convey. For example, a rule-\nbased system can display its set of rules to allow a human to see its reasoning. A\nvery early example of this was Teiresias (Davis, 1978) that explained its reasoning by\nshowing the rule chain that led to its conclusions. Even for more sophisticated systems\nthat may use (e.g., Bayesian processes, or decision trees), the system’s reasoning is\nrelatively easy to convey.\nThe broad use of purely data-driven AI/ML techniques raises additional challenges\nthat necessitate a more reﬁned look at the concept of transparency and trust. If the\ndata is biased, the system may come to the wrong conclusions although its algorithm\noperates correctly. In a well known example of such behavior, the data used by an\nimage classiﬁcation system inadvertently included images of wolves only in snowy\nbackgrounds (Ribeiro et al., 2016). When the system was given an image of a wolf in\na non-snowy background, it misclassiﬁed it as a dog. Only after the system designers\nsearched for the cause, they discovered it was due not to a faulty algorithm but to biased\ndata. In this particular case, recognizing the mistake was easy (ﬁnding the cause was\nnot so easy), however in other situation (e.g., system decisions on recidivism or ex-\ntending credit), even recognition of faulty system conclusions is not easy. Besides data\nbiases, adversarial data manipulation is also a major source of concern. Convolutional\nNeural Networks (CNNs), used for image classiﬁcation, are susceptible to adversarial\nmanipulation that causes them to misclassify images that seem only imperceptibly per-\nturbed to a human (Szegedy et al., 2013). Their paper shows examples of images that\nwere slightly perturbed by adversarial noise (imperceptible to the human eye) including\na yellow school bus and a white dog that were both misclassiﬁed as ostriches!. Thus,\nfor these systems to be trusted, they should be transparent in terms of being auditable\nas to possible biases in the data and robust to adversarial manipulation. This notion of\nauditability as a characteristic of transparency has also been found in the human factors\nliterature where increases in compliance was found from providing means of verifying\nautomation decisions (Bliss et al., 1996). Making progress towards discovering and\ncorrecting biases and also discovering and protecting systems from adversarial data\nmanipulation are open research problems in the AI/ML community.\nBesides auditability of decisions, transparency can also be considered in terms of\ninterpetability andexplainability . Interpretability enables understanding of the me-\nchanics of the algorithm, whereas explainability enables producing post hoc explana-\ntions or predictions of the model without necessarily elucidating the mechanisms by\nwhich the model works (Lipton, 2016). Both interpetabilty and explainability, like\ntrust and transparency, also lack formal deﬁnitions and characterizations. There is in-\n7\n\ncreasing interest in the AI community of providing those and developing techniques for\nunderstanding them. In section 5, we present our own work on transparent/explainable\nDeep Reinforcement Learning using object-based saliencey maps.\n3.1. Types of Explanations\nExplanations are conventionally categorized as:\n\u000fTeleological or functional (WHY): Explanations for transparency may be tele-\nological (why) which may be incomplete, conveying simply relevant features\nof the situation contributing to an automation decision (Wright et al., 2016) or\ncomplete in supplying both features and logic behind the decision. Teleological\nexplanations are preferred by humans (Lombrozo, 2006) who are strongly pre-\ndisposed to attribute causality (Thines, 1991) even in its absence. We have strong\npredispositions to attribute causality as shown in Michotte’s classic experiments\nreported by Thines (1991) in which subjects interpreted the motions of abstract\ngeometric objects as elastic collisions or animate behaviors such as chasing or\nleading depending on timing and proximity of objects. Explanations of more\nabstract behavior typically appeal to causes as well, with knowledge of general\npatterns constraining which causes are judged probable and relevant Lombrozo\n(2006) in much the way timing and proximity do for perception. When explana-\ntions are judged for quality the presence of a general pattern is typically preferred\nto probability judgments alone while this pattern is required to be both general\nand relevant (appropriate to the actors and event). So, for example, an expla-\nnation that ”a robot turned left to avoid a collision” would be preferable to an\nexplanation that the turn was taken to increase the robot’s long term probability\nof reaching a goal, although both might be correct. Similarly, citing the presence\nof a wall (often present preceding turns) or distance from door (a serendipitous\nfeature of the robot’s history) as cause would fail the probable and relevant test\nsince walls were often present when turns did not occur (not relevant) and dis-\ntance from door does not reference a humanly known general pattern for evoking\nturns (not probable). Moreover, this explanation implicitly includes the the do-\nmain knowledge that collisions are undesirable, that the robot had learned to\navoid collisions, and that collision is threatened by a trajectory leading the robot\ninto the wall.\n\u000fEfﬁcient or mechanistic (HOW): the primary source of an action “the robot’s\nprogramming caused it to turn”. Although this explanation has implicit the state\nof the world at the time of the turn, it is not relevant or useful to the human.\nMechanistic explanations can drill down by chaining together a series of inter-\nmediate steps so the answer might be elaborated to include that what the robot\nsensed caused its programming to produce an observed preference value leading\nit to turn.\n\u000fFormal or constitutive (WHAT): expressing a necessary aspect of an object or\nevent or conveying part-whole relationships. For example, the What explanation\nof the robot taking a turn may be, ”The robot took 5 steps forward, turned its\norientation to the left, took 5 steps forward”.\n8\n\nFrom this perspective teamwork involving robots trained through DRL presents a\ndilemma. An understanding of process needed to predict behavior, remains hidden\nwithin the layers of the network. The accuracy of our predictions of robot actions\ntherefore, depend on the correspondence between our own attributions of causality\nand the policies the robot has learned. As summarized by Lombrozo (2006) these\nattributions are predominantly based on probability and relevance. In the case of the\ngame Ms. Pac-Man, for example, relevance would revolve around the rules of the game\nin which Pac-Man gains points by eating pellets and avoiding ghosts. The DRL player,\nhowever, does not beneﬁt from predeﬁned relevance and learns from scratch to ﬁnd the\noptimal policy through trial and error. Therefore, while the resulting policy has been\ntrained to win the game it will not necessarily choose the same actions as a human,\nmaking attribution difﬁcult. Recently, the AI community has studied the effects of\nincluding domain knowledge in the form of relevant features. Although this may limit\ngenerality of learning, since some features may be predetermined, it may help system\nexplainability.\nIn the context of teamwork, not only should the system be transparent to the user,\nbut also the human should be transparent to the system . In other words, the system\nshould be able to make inferences about human intent (purpose), human beliefs and\nhow these beliefs may lead to actions (process) prediction of human actions (perfor-\nmance). Such understanding on the part of the system is equivalent to the system\nformulating a Theory of Mind of the human that would allow it to adapt to human’s be-\nhavior thus improving teamwork performance. Such a Theory of Mind will allow the\nagent to better understand why a human may be taking a particular action, understand\nwhen the human may have false beliefs (e.g., due to lack of information to environment\nchanges) and inform the human of missing information that the agent may have, so as\nto correct the human’s false beliefs.\n4. Background on RL and Deep RL\nReinforcement learning solves the sequential decision making problems by learn-\ning from experience. In Reinforcement Learning (RL), an agent interacts with an envi-\nronment eover discrete time steps and receives feedback (rewards) on the outcome of\nchoices is makes. Given a state, the agent selects actions in order to maximize future\nrewards. In the RL setting the problem can be modelled as a Markov Decision Process\n(MDP) represented by the 5-tuple (S;A;T;R;g), where Sis the state space, Ais the\naction space, T(s0js;a)is the state transition probability function, R(s;a)is the reward\nfunction and g2[0;1]is the discount factor. Due to stochasticity, a policy p:S!A\nmaps every state to a distribution over actions. In the time step t, the agent receives a\nstate st2Sand selects an action at2Aaccording to its policy p, where SandAdenote\nthe sets of all possible states and actions respectively. After executing the action, the\nagent receives a scalar reward rtand enters the next state st+1.\nThe goal of the agent is to choose actions to maximize its rewards over time. In\nother words, the action selection implicitly considers the future rewards. The total\ndiscounted return is deﬁned as Rt=å¥\nt=tgt\u0000trtwhere g2[0;1]is a discount factor\nthat trades-off the importance of recent and future rewards.\n9\n\nPolicy based methods directly model the policy (Williams, 1992), while in value-\nbased RL methods, the action value (a.k.a., Q-value) is commonly estimated by a func-\ntion approximator, such as a deep neural network (Mnih et al., 2015a). The actor-critic\nSutton & Barto (1998) architecture is a combination of value-based and policy-based\nmethods.\nThe value function Vp(st)is the expected discounted sum of rewards by follow-\ning policy pfrom state stat time t,Vp(st) =E[åT\ni=0girt+i]. Similarly, the Q-value\n(action-value) Qp(st;a)is the expected return starting from state st, taking action aand\nthen following p. The Q-value function can be recursively estimated using the Bell-\nman equation Qp(st;a) =E[rt+gmax a0Q(st+1;a0)]andp\u0003is the optimal policy which\nachieves the highest Qp(st;a)over all policies p.\nIn the reinforcement learning community the action value function Q is computed\nvia a typically linear function approximator, but in Deep Reinforcement learning, the\nfunction approximator is a (nonlinear) neural network. Deep neural networks have\nbeen recently applied in reinforcement learning (RL) to achieve human-level control\npolicies in various challenging domains. The rich representations given by a deep\nneural network improve the efﬁciency of reinforcement learning (RL) at the expense\nof requiring a vast amount of training data. If a high ﬁdelity simulator is available\n(e.g., Zhu et al. (2016) which describes the appearance of the real-world as closely as\npossible), such data can be easily generated.\nThe advantages of deep RL are (a) the features do not need to be hand-crafted but\nare learned during training, (b) deep neural networks have shown superior performance\nin many challenging domains, and (c) the algorithm is model-free (i.e., it solves the\nreinforcement learning task directly using sample data without explicitly estimating\nthe reward and transition dynamics).\nThe challenges of this approach are: (a) large amounts of data are needed, (b) RL\nwith nonlinear function approximator as a neural network could be unstable (or even\ndiverge). This is due to different causes, such as the correlation present in the sequence\nof observations, the fact that small updates to the value function may signiﬁcantly\nchange the policy, and the existence of correlations between action values (Q) and the\ntarget reward values\nDRL differs from image classiﬁcation in a number of ways: In classiﬁcation, for\neach image, the system has to predict its label whereas for DRL the system must learn\na function that maps state-value pairs to Q-values that capture the dynamics of the\nsystem over time. If the state in DRL is a single image, the dynamics of the system are\nnot captured. Therefore, a state must be represented by a sequence of images . Current\nworks on DQNs for Atari games use only 4 previous screens as input to learn the reward\nfor each state.\nAlthough Deep RL is much more challenging than classiﬁcation, in the past few\nyears a new variant of Deep RL has been developed and tested mainly on Atari games\n(Mnih et al., 2013) where the states are the input screens of the game. The agent\nchooses an action from the possible control actions (e.g., up/down/left/right/). After\nthat, the agent receives a reward (how much the score increases or decreases) and the\nnext image input. The deep RL agent, only reasoning on the image pixels and the game\nscores show performance comparable or higher to an expert human player (Mnih et al.,\n2015b). However, producing explanations has been out of scope for Deep RL until\n10\n\nrecently but interest in interpetability and explainability is increasing (Lipton, 2016).\nDeveloping methods to enable autonomous agents to be transparent is very chal-\nlenging, because ease of transparency seems to be inversely proportional to agent so-\nphistication. However, DNNs are extremely opaque (i.e., they cannot produce human\nunderstandable accounts of their reasoning processes or explanations). Therefore, there\nis a clear need for deep RL agents to dynamically and automatically offer explanations\nthat users can understand and act upon.\n4.1. Related Work on Computational Models\nThe literature on deep reinforcement learning is fast increasing. Here we brieﬂy\nreview work that is most relevant to this chapter. Multiple deep RL algorithms have\nbeen developed to incorporate both on-policy RL such as Sarsa (Rummery & Niran-\njan, 1994), actor-critic methods (Sutton & Barto, 1998) and off-policy RL such as Q-\nlearning (Watkins, 1989), or combination of RL and experience replay memory (Mnih\net al., 2015a; Riedmiller, 2005). A parallel RL paradigm (van Hasselt et al., 2015) has\nalso been proposed to reduce the heavy reliance of deep RL algorithms on specialized\nhardware or distributed architectures. The deep Q-network (DQN) model proposed\nin Mnih et al. (2015a) combines Q-learning with a ﬂexible deep neural network. More\nspeciﬁcally, recent work has found outstanding performance of deep reinforcement\nlearning models on Atari 2600 games using only raw pixels to make game control de-\ncisions (Mnih et al., 2015a). DQN can reach human-level performance on many of\nAtari 2600 games. However, DQN suffers from substantial over-estimation in some\ngames. van Hasselt et al. (2015) thus proposes Double Q-learning algorithm that can\nbe generalized to work with large-scale function approximation. A dueling network\narchitecture (Wang et al., 2015) has been proposed to decouple the state-action values\ninto state values and action values. The experiments of Mnih et al. (2016) show that\nthe actor-critic (A3C) method surpasses the current state-of-the-art in the Atari game\ndomain. In contrast to Q-learning, A3C is a policy-based model that learns a network\naction policy. However, for game settings with many objects where each object has\na different role in reward computation, A3C does not perform very well. Therefore,\nLample & Chaplot (2016) propose a method that augments performance of reinforce-\nment learning by exploiting game feature information.\nIn human-robot interaction and teaming, robots and humans must adapt to one an-\nother. This requires the robots to maintain a computational cognitive model of their\nhuman co-workers in the task environment. There is a body of work on making intel-\nligent robots able to 1) adapt to physical human behaviors (Liu et al., 2016), 2) infer\nhuman’s intent (Hadﬁeld-Menell et al., 2016; Dorsa Sadigh et al., 2017) and 3) shape\nthe way how humans reason about robots (Huang et al., 2019; Zhou et al., 2017; Pez-\nzulo et al., 2013). Previous research assumes the human is a perfect collaborator and\nuses Bayesian inference to predict the human’s next goal in order for the robot to adapt\naccordingly (Liu et al., 2016). Subsequent work by Hadﬁeld-Menell et al. (2016) ex-\ntends the scope to situations where robots do not know the human operators’ reward\nfunction and need to learn it over the course of interaction. Inverse Reinforcement\nLearning (Ng et al., 2000; Ramachandran & Amir, 2007) approaches this problem\nfrom a passive learning perspective like learning from demonstrations ofﬂine, while in\nCooperative Inverse Reinforcement Learning (Hadﬁeld-Menell et al., 2016) the human\n11\n\nhelps the robot learn by making his/her behavior more transparent. Alternately, by\nmaintaining a model of human mental states, the robot may take informative actions to\ncommunicate its goals (Pezzulo et al., 2013), reward functions (Huang et al., 2019), or\nconﬁdence levels (Zhou et al., 2017) to humans.\nIdeally, we would like to develop techniques for DRL explanations that explain (a)\nwhy an action was taken (teleology), (b) why this action was taken as opposed to other\nones (counterfactuals), and (c) why a sequence of actions was taken (plans).\n5. Object-Saliency Based Explainability in Deep RL\nMost current work around interpretability in deep learning is based on local ex-\nplanations (i.e., explaining network predictions for speciﬁc input examples (Lipton,\n2016)). Saliency maps are used to generate local explanations. Saliency maps gener-\nally use gradient-like information to identify salient parts of the image and highlight\nimportant regions of the input that inﬂuence the output of the neural network. Zahavy\net al. (2016) use the Jacobian of the network to compute saliency maps on a Q-value\nnetwork. Perturbation based saliency maps using a continuous mask across the image\nand also using object segmentation based masks have been studied in the context of\ndeep-RL (Greydanus et al., 2017). Our object-based saliency method (Iyer et al., 2018;\nLi et al., 2017) that we present in 5.1 belongs to this category.\nIn contrast, global explanations attempt to understand the mapping learned by a\nneural network regardless of the input. There are additional difﬁculties with global\nexplanations since there are problems of generalization and memorization. Recent\nﬁndings suggest that deep RL agents can easily memorize large amounts of training\ndata with drastically varying test performance and are vulnerable to adversarial attacks\n(Zhang et al., 2018a,b; Huang et al., 2017). We have explored interpetability using\nglobal explanations in Annasamy & Sycara (2019). Our method aims to understand\naspects of the input space (images) that are captured in the latent space across inputs.\nIn particular, given a particular action (e.g., Pac-Man goes left) and expected returns\n(e.g., rewards of 50 points), our method tries to understand visually which would be the\nfeatures of the corresponding states. This could help in implicitly identifying underly-\ning relations between the entities in these states, (e.g., the network might have learned\nthat presence of a ghost in the vicinity of Pac-Man would be a had thing that must be\navoided). However, our results suggest that the features extracted by the convolutional\nlayers are extremely shallow and can easily overﬁt to trajectories seen during training,\nrather than generalize to trajectories that would be in conformance with real relations\namong entities.\nIn the following section we present a short description of our recent work on ex-\nplainability in DRL, using the game of Pac-Man. In this work, we have explored bot\nvisualization and text as means of producing local explanations of agent (Pac-Man)\nbehavior.\n5.1. Visual Explanation\nIn Atari games, the DRL network is supposed to implicitly learn all relevant fea-\ntures that capture the agent’s reasoning and behavior. However, this does not aid ex-\nplainability to humans. Since humans recognize objects, our idea was to enhance the\n12\n\nneural network with object recognition ability as a way to get a handle on relevance of\ndifferent objects for the DRL agent’s decision making. We ﬁrst used template match-\ning, a computer vision technique (Brunelli, 2009) to recognize objects in images. The\ntechnique works by taking a template image (the patch) and sliding it through a source\nimage (up to down, left to right) and calculating the current source image similarity to\nthe template image. After object recognition, we used object channels to incorporate\nfeatures of objects in the input images to the DRL network.\nThe object channels as well as the original image are given to the network as input.\nThe network outputs (predicts) Q-values for each action. This method can be used\nincorporated into different existing deep reinforcement learning algorithms, such as\nDQN or ACC.\n(a) Screenshot of\nthe State\n(b) Pixel Saliency\nMap\n(c) Object\nSaliency Map\nFigure 2: An example of original state, corresponding pixel saliency map and object saliency map produced\nby a double DQN agent in the game “Ms. Pac-Man.”\nOur method to provide transparency for Deep Neural Networks is called object\nsaliency maps . A saliency map highlights regions of the input that, if changed, would\nmost inﬂuence the output (Simonyan et al., 2013). Saliency maps is an ex post facto\nlocal explanation method. This means that after the network has been trained, query\nimages can be input that may help the user predict what the network was paying atten-\ntion to (ie. ”explain) so as to help the user understand the system’s reasoning and also\npossibly predict the system’s next decision. Object saliency maps provide visualization\nof the decisions made by RL agents. These visualizations aim to be intelligible to hu-\nmans. To generate intelligible visualizations that would help with explanations of DQN\nagent behaviors, we need to determine which pixels the model pays attention to when\nmaking a decision (Simonyan et al., 2013). Another interpretation of computing pixel\nsaliency is that the value of the derivative indicates which pixels need to be changed\nthe least to affect the Q-value.\nHowever, pixel-level representations are not intelligible to people. Figure 2(a)\nshows a screenshot from the game Ms.Pac-Man. Figure 2(b) is the corresponding pixel\nsaliency map produced by an agent trained with the Double DQN(DDQN) model. The\nagent chooses to go right in this situation. Although we can get some intuition of\nwhich area the deep RL agent is looking at to make the decision, it is not clear what\nobjects the agent is looking at and why it chooses to move right. On the other hand,\nFigure 2 makes more intelligible the objects that the DQN is paying attention to. To\nunderstand the inﬂuence of objects on agent decisions, we need to rank the objects\n13\n\nin a state sbased on their effect on Q(s;a). In the game of Pac-Man, there are static\npellets/dots that Pac-Man eats to get points. There are ghosts that chase and eat the\nPac-Man, which ﬁnishes the game and gives a large number of negative points. There\nare super-pellets and cherries that appear dynamically, and if Pac-Man eats them then\nit gets more points. Moreover, if Pac-Man eats a cherry then, the ghosts become edible\nfor some time, so if the Pac-Man manages to eat an edible ghosts it gets a very high\nreward (many points).\nFor each object Ofound in s, we mask the object with background color to form a\nnew state soas if the object does not appear in this new state. We calculate the Q-values\nfor both states, and the difference w of the Q-values actually represents the inﬂuence\nof this object on Q(s;a). So, if w is positive the object has a positive inﬂuence which\nmeans the the object gives positive future reward to the agent (the positive objects are\nshown in dark in the saliency maps). Negative wrepresents “bad” object since after we\nremove the object, the Q-value gets improved.\nFigure 2(c) shows an example of the object saliency map that clearly shows which\nobjects the model is paying attention to and the relative importance (via shading) of\neach object.\n5.1.1. Human Experiments\nIn order to test whether the object saliency map visualization can help humans un-\nderstand the learned behavior of Pac-Man, we performed an initial set of experiments.\nThe goals of the experiment were to: 1) test whether object saliency maps contain\nenough information to allow humans to match them with corresponding game scenar-\nios, 2) test whether participants could use object saliency maps to generate reasonable\nexplanations of the behavior of the Pac-Man and 3) test whether object saliency maps\nallow participants to correctly predict the Pac-Man’s next action. This requires a deeper\ncausal understanding of what may inﬂuence the Pac-Man in his decisions.\n(a) Screen-shots\n (b) Object Saliency Maps\nFigure 3: An example of the stimulus materials participants saw on trial 9 of the prediction task. 75%\nparticipants in the screen-shot group thought Pac-Man would go left to eat the cherry at the left side. 60%\nparticipants in the object saliency maps group predicted the Pac-Man would keep going down for the dark\nelements (the pellets) below.\nExperiments were conducted in a graduate and undergraduate Human Factors class.\nThe forty participants were approximately equally divided by gender and between 20-\n29 years old. The Matching and Prediction tasks were presented sequentially over a\nperiod of 30-45 minutes.\nMatching Task In each trial, the participants are shown twice, a 5-second video\nclip of Pac-Man gameplay generated by O-DDQN. During the video clip, Pac-Man de-\n14\n\ncides and takes particular actions. The last decision made produces the crucial move-\nment of the clip (e.g., Pac-Man moves right), with the clip ending just after the crucial\nmovement. Three frames from the object saliency map are then shown to participants\n(see Fig. 3(b)). The center frame is the frame where the Pac-Man makes the cru-\ncial decision and the other two are frames from before and after that moment. In the\ntask, participants are asked to judge whether the saliency maps accurately represent the\nvideo they just saw. In the matching cases, the saliency maps indeed were generated\nfrom the video clip the participant saw. In the non-matching cases, the three saliency\nmap frames were generated from a different video clip. In distractor/non-matching\nclips, the Pac-Man occupies the same area of map as in the target video, but makes\ndifferent movements. This is done to avoid the case where the participants solely focus\non the location of the Pac-Man as a matching criterion, disregarding the movements\nand environmental factors.\nFollowing the match decision, if the participants’ answer is ”match”, they are asked\nto give an explanation for the Pac-Man’s movements based on the video and saliency\nmaps. In other words, participants are asked to provide a teleological explanation ex-\nplaining ‘why’ Pac-Man acted as she did. For example, ”Pac-Man moved up to eat\nmore energy pellets while avoiding the ghost coming from below.”\nThe matching task consisted of 2 training trials and 20 test trials, half (10 trials)\npresenting matched video and saliency maps, the other half presenting non-matched\npairs in a single randomly ordered sequence. Dependent variables were correctness of\nmatches and agreement between explanations and saliency maps.\nPrediction Task In each trial, the participants are shown a video clip not used in\nthe matching task. Each clip ends at the point where the Pac-Man must choose a crucial\nmove. The participants are divided equally into two experimental conditions. In the\nscreen-shot condition, after the video clip, participants see 3 actual screen-shots from\nthe video ending before the crucial move is taken. In the object saliency map condition,\nthe participants see three object saliency map frames (corresponding to the screen-shot\nframes) after viewing the video clip (see Fig. 3). At the decision point in the third\nframe Pac-Man’s choices (up, down, left, right) may be limited by barriers indicated on\nthe response forms. Participants are asked to predict Pac-Man’s movement among the\nfeasible directions based on the three previous frames (screenshots or saliency maps),\nand then give an explanation for their prediction which includes their judgment as\nto which elements of the game inﬂuenced the Pac-Man’s decision (indicating these\nelements by circling them on a hardcopy of the screenshot or saliency map), and explain\nwhy Pac-Man made that decision.\nThe prediction task consisted of 2 training trials and 10 test trials. Each participant\nwas assigned to either the screenshot group or the saliency map group. Dependent vari-\nables include whether predictions were correct, and whether explanations were consis-\ntent with the saliency maps.\nResults The average matching accuracy of the participants was 61 :0% (SD=14:0%).\nA learning effect was found with participants having higher accuracy (65 :5%) in the\nlast half of the trials than the ﬁrst half (56 :5%) ( t(39) =3:10;p=0:04). Comparing\nhit and false alarm rates, participants reported more ”matches” when the video and im-\nage stimulus matched ( t(18) =2:91;p<0:001). If the 40 participants are treated as a\nbinary classiﬁer and the percentage of their answers as an output score, a receiver oper-\n15\n\nFigure 4: ROC curve of the matching task, AUC =\n0.81.\nFigure 5: The mean accuracy of participants in each\ntest cases of the prediction task. Error bars are one\nStandard Error from Means.\nating characteristic (ROC) curve (Fawcett, 2006) can be plotted for true positive rates\nversus false positive rates across a range of threshold parameters (as Fig. 4 shows).\nThe area under the curve is 0.81 which indicates a good classiﬁcation between match-\ning and non-matching situations. In summary, human participants were able to link the\nobject saliency maps with the game scenarios.\nFor the more difﬁcult prediction task, there was no signiﬁcant difference in accu-\nracy between the object saliency map group (58 :0%\u000612:8%) and the control group(56 :5%\u0006\n10:4%). However, the main effect of trials ( F(9;342) =11:18,p<0:001) and the in-\nteraction between trials and groups ( F(9;342) =2:72,p=0:005) were both highly\nsigniﬁcant suggesting that characteristics of the trials had a strong inﬂuence on perfor-\nmance. Thus we conducted a simple effect analysis to examine differences among the\n10 test scenarios (see Fig. 5). Results show that the screen-shot group has high pre-\ndictive accuracy in test 2 ( p=0:027), while the object saliency map group has higher\naccuracy in tests 3 and 9 ( p=0:007;p=0:025). Those three trials can help provide\na deeper insight into the mechanism of how object saliency maps could help humans\nunderstand Pac-Man’s learned behavior.\nTrial 9 provides a good example (see Fig. 3). The Pac-Man goes down and faces a\ndilemma whether to turn left or keep going down. 60% participants who saw the object\nsaliency maps predicted Pac-Man would continue going down, and objects circled and\nexplanations focused on the dark elements or dots below. In contrast, 75% participants\nin the screen-shot group predicted Pac-Man would go left, and all except one of their\nexplanations mentioned the cherry at the left side. In the scenarios generated by O-\nDDQN, the Pac-Man did go down for the dots. Trial 9 is a typical case in which\nthere are multiple inﬂuencing elements and it is hard for humans to predict Pac-Man’s\nbehavior based on information from the game screen and their own knowledge of the\nrules and ideas about gameplay. However, displaying object saliency enables us to\ndirectly identify those objects affecting the program’s decision. In other situations\nwhen the Pac-Man may make what we judge to be suboptimal choices (e.g., the Pac-\nMan chose a wrong direction and was eaten by a ghost), an object saliency map could\nbe crucial to helping users and system developers understand some of the rationale\nbehind such behaviors and the saliency map can be used as a debugging tool.\n16\n\n5.2. Natural Language Explanation\nTo present information from object saliency maps in a form more consistent with\nhuman reasoning we developed algorithms to generate relevance-sensitive textual ex-\nplanations for DRL networks. We developed focused verbal explanation models of\nthe DRL system in which verbal explanations referred to objects expected to be most\nimportant in inﬂuencing the agent’s selection of next action. Prior work in generating\nexplanations for RL systems has been limited with direct translation of policies (Hayes\n& Shah, 2017) restricted to simple cases while more complex environments such as\nAtari games (Ehsan et al., 2017) have relied on human attributions. Our approach is\nintermediate basing explanation on internal information but restricting its expression\nto a teleological form with relations satisfying human criteria for relevance. Our initial\nrule-based model was constructed based on prior knowledge of the Ms. Pac-Man game\nand its rules and consisted of a collection of allowable expressions. Object salience\nand valence were used to match information encoded in a saliency map with a verbal\nexplanation. Although the rule-based model is capable of generating reasonable expla-\nnations, it lacks the generalizability and ﬂexibility that a neural network might provide\nin addressing unexpected situations. To overcome the limitations of the rule-based\nmodel, a learning model was also developed in order to : 1) be more generalizable in\nterms of game episodes 2) be more tolerant to input noise 3) to provide explanation\nfor future states and actions, which could help the user predict and plan. Game image,\nagent position map, and object saliency map act as the input for both models. Data\ngenerated by the rule-based model was employed to train the learning model , which\nconsisted of two parts: an encoder for feature extraction, and a decoder for generating\nthe explanation in natural language using an attention mechanism. The challenge for\nthe learning model lies in extracting distinguishable features from DRL systems, es-\npecially from images with high structural similarity, such as a game image with ﬁxed\nboard or ﬁxed map, which can not be fully solved by current networks.\n5.2.1. Rule-based Verbal Explanation Model\nThe input of the rule-based model is game images and corresponding object saliency\nmaps. The output is a verbal explanation for the given game state. The pre-deﬁned rules\nand workﬂow used in this model were designed based on literature and a previous user\nstudy (Iyer et al., 2018). The design of the rule-based system was based on a number\nof assumptions. (i) Object priority : Since different objects in the game give Pac-Man\ndifferent rewards (e.g., being eaten by the ghost means Packamn dies, eating a ghost\nwhen it becomes edible gives high positive reward), the explanation should focus on\nhigh-value objects (positive or negative). From Pakman’s rules the value of objects is\nghost, edible ghost, cherry, pellet, and dot. (ii) Attention area : User tests conﬁrmed\nthat participants only consider a limited area relatively close to Pac-Man when ex-\nplaining its actions. this is reasonable since the Pac-Man and ghosts move only one\nstep at a time and the rest of the objects are static, with the exception of cherries that\nappear and disappear dynamically. Therefore we consider only a limited area of atten-\ntion with limited number of objects for the verbal explanation. (iii) Action accordance\nThe expected action of Pac-Man should be approaching beneﬁcial objects and avoid-\ning ghosts. However, there are situations where Pac-Man has to leave beneﬁcial object\n17\n\nin order to avoid ghost or Pac-Man has to approach ghost in order to chase beneﬁcial\nobjects. The action of Pac-Man is divided into two classes, considering consistent or\ninconsistent with expectations. Class #1 means the action of Pac-Man is in accordance\nwith the expectation. Class #2 means the action is in contrast with the expectation.\n(iv)Language style To make the explanation more natural we created sentences that\ndescribe the Pac-Man’s actions (moving directions) followed by the objects that moti-\nvate the current action. Relative coordination is employed to indicate the position of\nobjects with relation to Pac-Man. An explanation template was designed accordingly.\nFor example, a typical sentence is ”The Pac-Man moves up to eat the dot above her”.\n5.2.2. Learning-based Verbal Explanation Model\nThe learning model consists of two stages. The ﬁrst stage is image processing in\norder to get the game image feature map. The second stage generates verbal content\nbased on the image process result. The Image encoder encodes game image, Pac-Man\nposition map, and object saliency map for the verbal decoder. The three input chan-\nnels provide information from different aspects: Game image provides environment\ninformation. Pac-Man position map contains Pac-Man location information and the\nobject saliency map provides the saliency weight of each object. The Verbal decoder\ngenerates a verbal description consistent with the image encoder output. A sequence\ngeneration model generates verbal explanations verbatim. An attention mechanism\nthen selects the most salient output from the encoder. To capture dynamic game infor-\nmation ﬁve previous frames of the game image, ﬁve previous frames of the Pac-Man\nposition map, and a frame of the object saliency map serve as input. A verbal explana-\ntion derived from the game image is generated as output.\n5.2.3. Experimental Results\nIn this part, both the verbal explanation of rule-base model and learning model are\nquantitatively evaluated.\nFigure 6: Subjects satisfaction score.\n Figure 7: The mean accuracy of participants.\nUser tests. User tests were conducted in our laboratory and through Mechanical Turk\nto validate the appropriateness and effectiveness of both the rule-based and learning-\nbased explanation generation models.\n18\n\nExperimental design. The matching and prediction tasks were conducted in a labora-\ntory environment with 17 paid participants recruited from the University of Pittsburgh\ncommunity. The Task settings and game episodes were identical to those used in the\nearlier evaluation of object-saliency maps, except some stimulus materials were re-\nplaced with natural language explanations generated by the rule-based model.\nThe second task to evaluate the acceptability of DNN generated explanations was\nconducted online using Amazon Mechanical Turk. The online questionnaire consisted\nof an introduction section and two evaluation tasks. The introduction contained ba-\nsic background knowledge about the Ms. Pac-Man game, object saliency maps and\nthe language generation model. On each trial participants reported their satisfaction\nwith an explanation presented either visually (saliency map) or verbally (rule-based\nor learning-based). In the visual evaluation trials, a game screen-shot and the corre-\nsponding object saliency map were presented. In the verbal evaluation trials, a natural\nlanguage explanation generated by either the rule-based or learning based model was\ngiven in addition to the two images. Responses were collected on a Likert scale ranging\nfrom 1 (strongly disagree) to 7 (strongly agree).\nThe 10 trials on each task contained randomly selected scenarios from the game\nepisodes played by DRL. The sequence of two tasks and trials in each task was ran-\ndomized to counterbalance the learning effect. For the rule-based and learning models,\ntests were conducted separately on different groups of participants to avoid interfer-\nence. The questionnaire was deployed on Qualtircs.com for public access.\nResults. The performance of matching and prediction participants was compared di-\nrectly with the historical data from the previous experiment. There was no signiﬁcant\ndifference between natural language explanation and the object-saliency visualization.\nFor the online questionnaire, 150 samples are kept after removing abnormal data.\nFor the rule-based model, the average rating of verbal and visual tasks were 5 :11\u0006\n0:07 (Mean\u0006Standard Error) and 4 :89\u00060:09, respectively. Paired T-test showed that\nthe rule-based verbal explanations received signiﬁcant higher subjective ratings than\nobject saliency maps, t(74) =2:989;p=:004. For the learning model, a similar pattern\nappears suggesting that learning-based verbal explanations (5 :08\u00060:10) are better than\no-saliency maps (4 :93\u00060:10) in terms of users’ satisfaction, t(74) =2:020;p=:047.\nThe results are shown in ﬁgures 6 and 7.\nOur results indicate that verbal explanations consistent with human preferences\nfor teleological explanation are found more satisfactory than visual saliency maps that\ndo not respect this preference. However, in terms of matching or prediction accuracy\nthere were no signiﬁcant differences between the natural language explanations and the\nsaliency maps from which they were derived.\n6. Conclusions\nIn this chapter we present issues involving trust and transparency arising in in hu-\nman interactions with autonomy that uses AI and Machine Learning algorithms. Sec-\ntions 1-3 review literature on trust and transparency for conventional automation in\n19\n\norder to extrapolate to what may be expected as we move from systems which auto-\nmate a relatively narrow range of actions to autonomous agents/robots with substan-\ntially larger action spaces. We argue that as the degree of automation (extent to which\nsystem output is controlled by machine rather than human) increases, transparency of\nthe automation usually decreases. Adopting Wickens (2018) simpliﬁed two stage (in-\nput/output) model of automation we argue that transparency (added information) at the\ninput stage contributes more to making a system predictable than transparency at the\noutput stage. So, a display showing the proximity of targets, for example, would be\na greater help in predicting the behavior of an automated weapon than assurance or\nexperience that the weapon is 90% effective although either might lead to a decision to\nrely on the automation. A convergent literature on explanation from psychology sug-\ngests that humans have a strong preference for teleological (causal) explanations and\nare more likely to use such models to guide their actions. From these observations we\nsuggest that an AI/robot’s ability to provide or support teleological explanations of its\nbehavior will likely be crucial to to effective human-robot interaction and teaming.\nThe remainder of the chapter is devoted to examining the consequences for inter-\nacting with Deep Reinforcement Learning systems which use very high dimensional,\nnonlinear embeddings to generate their behavior. While DRL produces highly effec-\ntive performers achieving equal or better than human performance they are opaque and\noften make choices bafﬂing to humans. In a series of studies we examine mechanisms\nwhich might make DRL behavior more transparent. The ﬁrst study supports trans-\nparency by making the input used by the system visible to the user in a manner similar\nto the automated weapon’s display of target proximity. While participants were able to\nassociate saliency maps with corresponding screen shots, their ability to predict Pac-\nMan’s next action based on the most inﬂuential objects/regions in the display was not\nsigniﬁcantly greater than chance. A trial by trial examination shows that while saliency\nmaps improved predictions substantially in some cases in others it depressed them.\nThe second set of experiments addressed the question of whether constraining ex-\nplanations to teleological form could make them more effective and usable. To gen-\nerate explanations, objects with high salience and their valence were matched against\nhypothesized rules governing Pac-Man behavior. So, for example, in a saliency map\nin which pellets below the Pac-Man had the highest salience and positive valence the\nsituation might be re-expressed as, ’the Pac-Man is attracted by the pellets below’. The\nverbal teleological descriptions of saliency map contents performed no better than the\nmaps themselves on the matching and prediction tasks. Because many of the collected\nepisodes of DRL gameplay could not be translated by matching to rules, a second DRL\nnetwork was trained using the matches as labeled examples. Mechanical Turk workers\nrated explanations generated by this DRL as satisfactory as those generated by match-\ning to rules and felt both to be more interpretable than the saliency maps. Performance\non the matching and prediction tasks, however, did not vary across conditions. These\nexperiments suggest that while DRL networks learn to play games such as Pac-Man\nwith a high level of skill, what they have learned and how they play may seem quite\nalien to a human observer. When provided with a series of screen shots or saliency\nmaps our participants readily attributed desires such as eating pellets or avoiding ghosts\nto the Pac-Man yet predicting actions on this basis worked no better than chance. If we\nare to take advantage of the strength of DRL performance in human-robot interaction\n20\n\nor human-autonomy teaming this gulf between how we view problems and how they\ncome to be solved by a learner with massive experience but none of our knowledge\nmust be bridged. Until then we may come to trust DRL systems based on performance\nalone but won’t be able to predict their actions or realize when they are wrong.\nAs the development and penetration of these systems into society increases, and\nas vulnerabilities of these opaque systems are identiﬁed, there is a tremendous need\nfor (a) formulating rigorous deﬁnitions of transparency, (b) identifying dimensions of\ntransparency and algorithms for making those dimensions operational to humans, and\n(c) studying their effects in human autonomy teaming. There is also an imperative to\nstudy the transparency of these systems in the broader societal context.\nReferences\nAmato, F., Felici, M., Lanzi, P., Lotti, G., Save, L., & Tedeschi, A. (2011). Trust\nobservations in validation exercises. (pp. 216–223).\nAnnasamy, R., & Sycara, K. (2019). Towards better interpretability in deep q-networks.\nInInternational Conference on Artiﬁcial Intelligence (AAAI) . AAAI.\nArulkumaran, K., Deisenroth, M. P., Brundage, M., & Bharath, A. A. (2017). A brief\nsurvey of deep reinforcement learning. arXiv preprint arXiv:1708.05866 , .\nBeck, H. P., Dzindolet, M. T., & Pierce, L. G. (2007). Automation usage decisions:\nControlling intent and appraisal errors in a target detection task. Human Factors ,49,\n429–437.\nBiros, D. P., Daly, M., & Gunsch, G. (2004). The inﬂuence of task load and automation\ntrust on deception detection. Group Decision and Negotiation ,13, 173–189.\nBitan, Y ., & Meyer, J. (2007). Self-initiated and respondent actions in a simulated\ncontrol task. Ergonomics ,50, 763–788.\nBliss, J. P., Jeans, S. M., & Prioux, H. J. (1996). Dual-task performance as a function of\nindividual alarm validity and alarm system reliability information. In Proceedings\nof the Human Factors and Ergonomics Society Annual Meeting (pp. 1237–1241).\nSAGE Publications Sage CA: Los Angeles, CA volume 40.\nBrunelli, R. (2009). Template Matching Techniques in Computer Vision: Theory and\nPractice . Wiley Publishing.\nCalhoun, G. L., Draper, M. H., & Ruff, H. A. (2009). Effect of level of automation\non unmanned aerial vehicle routing task. Proceedings of the Human Factors and\nErgonomics Society Annual Meeting ,53, 197–201.\nChen, J. Y ., & Barnes, M. J. (2014). Human–agent teaming for multirobot control: A\nreview of human factors issues. IEEE Transactions on Human-Machine Systems ,\n44, 13–29.\n21\n\nChen, J. Y ., Procci, K., Boyce, M., Wright, J., Garcia, A., & Barnes, M. (2014). Sit-\nuation awareness-based agent transparency . Technical Report Army research lab\nAberdeen proving ground MD human research and engineering . . . .\nChoi, E., Hewlett, D., Uszkoreit, J., Polosukhin, I., Lacoste, A., & Berant, J. (2017).\nCoarse-to-ﬁne question answering for long documents. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers) (pp. 209–220). volume 1.\nChoudlechova, A. (2016). Fair predictions aith disparate impact: A study of bias in\nrecidivism prediction instruments. In arXiv:1610.07524 .\nDadashi, N., Stedmon, A. W., & Pridmore, T. P. (2013). Semi-automated cctv surveil-\nlance: The effects of system conﬁdence, system accuracy and task complexity on\noperator vigilance, reliance and workload. Applied Ergonomics ,44, 730–738.\nDavis, R. (1978). Pattern-directed inference systems. chapter Model-directed learning\nof production rules. New York: Academic Press.\nDixon, S., & Parasuraman, R. (2006). Automation reliability in unmanned aerial vehi-\ncle control: A reliance-compliance model of automation dependence in high work-\nload. Human Factors ,48, 474–486.\nDorsa Sadigh, A. D. D., Sastry, S., & Seshia, S. A. (2017). Active preference-based\nlearning of reward functions. In Robotics: Science and Systems (RSS) .\nDzindolet, M., Peterson, S., Pomranky, R., Pierce, L., & Beck, H. (2003). The role of\ntrust in automation reliance. International Journal of Human-Computer Studies ,58,\n697–718.\nDzindolet, M. T., Pierce, L. G., Beck, H. P., & Dawe, L. A. (2002). The perceived\nutility of human and automated aids in a visual detection task. Human Factors: The\nJournal of the Human Factors and Ergonomics Society ,44, 79–94.\nEhsan, U., Harrison, B., Chan, L., & Riedl, M. O. (2017). Rationalization: A neural\nmachine translation approach to generating natural language explanations. arXiv\npreprint arXiv:1702.07826 , .\nFawcett, T. (2006). An introduction to roc analysis. Pattern Recogn. Lett. ,\n27, 861–874. URL: http://dx.doi.org/10.1016/j.patrec.2005.10.010 .\ndoi:10.1016/j.patrec.2005.10.010.\nGao, J., & Lee, J. D. (2006). Extending the decision ﬁeld theory to model operators’\nreliance on automation in supervisory control situations. Trans. Sys. Man Cyber.\nPart A ,36, 943–959.\nGoodman, B., & Flaxman, S. (2016). European union regulations on algorithmic deci-\nsion making and ”a right to explanation”. In arXiv:1606.08813 .\n22\n\nde Graaf, M., & Malle, B. F. (2017). How people explain action (and autonomous\nintelligent systems should too). In AAAI Fall Symposium on Artiﬁcial Intelligence\nfor Human-Robot Interaction .\nGreydanus, S., Koul, A., Dodge, J., & Fern, A. (2017). Visualizing and understanding\natari agents. arXiv preprint arXiv:1711.00138 , .\nHadﬁeld-Menell, D., Russell, S. J., Abbeel, P., & Dragan, A. (2016). Cooperative in-\nverse reinforcement learning. In Advances in neural information processing systems\n(pp. 3909–3917).\nHancock, P. A., Deborah, B., Schaefer, K., Chen, J., de Visser, E., & Parasuraman, R.\n(2011). A meta analysis of factors affecting trust in human robot interaction. Human\nFactors ,53, 517–527.\nvan Hasselt, H., Guez, A., & Silver, D. (2015). Deep reinforce-\nment learning with double q-learning. CoRR ,abs/1509.06461 . URL:\nhttp://arxiv.org/abs/1509.06461 .\nHayes, B., & Shah, J. A. (2017). Improving robot controller transparency through au-\ntonomous policy explanation. In Proceedings of the 2017 ACM/IEEE international\nconference on human-robot interaction (pp. 303–312). ACM.\nHuang, S., Papernot, N., Goodfellow, I., Duan, Y ., & Abbeel, P. (2017). Adversarial\nattacks on neural network policies. arXiv preprint arXiv:1702.02284 , .\nHuang, S. H., Held, D., Abbeel, P., & Dragan, A. D. (2019). Enabling robots to com-\nmunicate their objectives. Autonomous Robots ,43, 309–326.\nIyer, R., Li, Y ., Li, H., Lewis, M., Sundar, R., & Sycara, K. (2018). Transparency and\nexplanation in deep reinforcement learning neural networks. In Proceedings of the\nAAAI/ACM Conference on Artiﬁcial Intelligence, Ethics, and Society , .\nJian, J., Bisantz, A., & Drury, C. (2000). Foundations for an empirically determined\nscale of trust in automated systems. International Journal of Cognitive Ergonomics ,\n4, 53–71.\nJones, G., & George, J. (1998). the experience and evolution of trust: Implications for\ncooperation and teamwork. Academy of Management Review ,23, 531–546.\nKim, B. (2015). Interactive and interpretable machine learning models for human\nmachine collaboration . Ph.D. thesis Massachusetts Institute of Technology.\nKira, Z., & Potter, M. A. (2009). Exerting human control over decentralized robot\nswarms. In Autonomous Robots and Agents, 2009. ICARA 2009. 4th International\nConference on (pp. 566–571). IEEE.\nKraska, T., Beutel, A., Chi, E. H., Dean, J., & Polyzotis, N. (2018). The case for\nlearned index structures. In Proceedings of the 2018 International Conference on\nManagement of Data (pp. 489–504). ACM.\n23\n\nLample, G., & Chaplot, D. S. (2016). Playing fps games with deep reinforcement\nlearning. arXiv preprint arXiv:1609.05521 , .\nLangley, P., Meadows, B., Sridharan, M., & Choi, D. (2017). Explainable agency for\nintelligent autonomous systems. In AAAI (pp. 4762–4764).\nLee, J., & Moray, N. (1992). Trust, control strategies and allocation of function in\nhuman-machine systems. Ergonomics ,35.\nLee, J., & Moray, N. (1994). Trust, self conﬁdence and operator’s adaptation to au-\ntomation. International Journal of Human-Computer Studies ,40, 153–184.\nLee, J., & See, K. (2004). Trust in automation: Designing for appropriate reliance.\nHuman Factors ,46, 50–80.\nLevine, S., Finn, C., Darrell, T., & Abbeel, P. (2016). End-to-end training of deep\nvisuomotor policies. The Journal of Machine Learning Research ,17, 1334–1373.\nLewandowsky, S., Mudy, M., & Tan, G. (2000). The dynamics of trust: Comparing\nhumans to automation. Journal of Experimental Psychology: Applied ,6, 104–123.\nLewis, C. M., & Heidorn, P. B. (1991). Identifying tacit strategies in aircraft maneu-\nvers. IEEE Transactions on Systems, Man, and Cybernetics ,21, 1560–1571.\nLewis, M. (1998). Designing for human-agent interaction. AI Magazine ,19, 67.\nLi, Y ., Sycara, K. P., & Iyer, R. (2017). Object-sensitive deep reinforce-\nment learning. In GCAI 2017, 3rd Global Conference on Artiﬁcial In-\ntelligence, Miami, FL, USA, 18-22 October 2017. (pp. 20–35). URL:\nhttp://www.easychair.org/publications/paper/h9zx .\nLinegang, M. P., Stoner, H. A., Patterson, M. J., Seppelt, B. D., Hoffman, J. D., Crit-\ntendon, Z. B., & Lee, J. D. (2006). Human-automation collaboration in dynamic\nmission planning: A challenge requiring an ecological approach. In Proceedings\nof the Human Factors and Ergonomics Society Annual Meeting (pp. 2482–2486).\nSAGE Publications Sage CA: Los Angeles, CA volume 50.\nLipron, Z. (2013). The mythos of model interpretability. In arXiv:1606.03490v3 .\nLipton, Z. C. (2016). The mythos of model interpretability. arXiv preprint\narXiv:1606.03490 , .\nLiu, C., Hamrick, J. B., Fisac, J. F., Dragan, A. D., Hedrick, J. K., Sastry, S. S., & Grif-\nﬁths, T. L. (2016). Goal inference improves objective and perceived performance in\nhuman-robot collaboration. In Proceedings of the 2016 international conference on\nautonomous agents & multiagent systems (pp. 940–948). International Foundation\nfor Autonomous Agents and Multiagent Systems.\nLombrozo, T. (2006). The structure and function of explanations. Trends in cognitive\nsciences ,10, 464–470.\n24\n\nLyons, J., & Stokes, C. (2012). Human-human reliance in the context of automation.\nHuman Factors ,54, 112–121.\nLyons, J. B. (2013). Being transparent about transparency: A model for human-robot\ninteraction. In 2013 AAAI Spring Symposium Series .\nLyons, J. B., & Havig, P. R. (2014). Transparency in a human-machine context: ap-\nproaches for fostering shared awareness/intent. In International Conference on Vir-\ntual, Augmented and Mixed Reality (pp. 181–190). Springer.\nLyons, J. B., Koltai, K. S., Ho, N. T., Johnson, W. B., Smith, D. E., & Shively, R. J.\n(2016). Engineering trust in complex automated systems. ergonomics in design ,24,\n13–17.\nMadsen, M., & Gregor, S. (2000). Measuring human-computer trust. In 11th aus-\ntralasian conference on information systems (pp. 6–8). Citeseer volume 53.\nMayer, R. C., Davis, J. H., & Schoorman, F. D. (1995). An integrative model of\norganizational trust. Academy of management review ,20, 709–734.\nMercado, J. E., Rupp, M. A., Chen, J. Y ., Barnes, M. J., Barber, D., & Procci, K. (2016).\nIntelligent agent transparency in human–agent teaming for multi-uxv management.\nHuman factors ,58, 401–415.\nMeyer, J. (2004). Conceptual issues in the study of dynamic hazard warnings. Human\nFactors ,46, 196–204.\nMnih, V ., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T. P., Harley, T., Silver, D., &\nKavukcuoglu, K. (2016). Asynchronous methods for deep reinforcement learning.\nCoRR ,abs/1602.01783 . URL: http://arxiv.org/abs/1602.01783 .\nMnih, V ., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D.,\n& Riedmiller, M. (2013). Playing atari with deep reinforcement learning. arXiv\npreprint arXiv:1312.5602 , .\nMnih, V ., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G.,\nGraves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie,\nC., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., &\nHassabis, D. (2015a). Human-level control through deep reinforcement learning.\nNature ,518, 529–533. URL: http://dx.doi.org/10.1038/nature14236 .\nMnih, V ., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G.,\nGraves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G. et al. (2015b). Human-\nlevel control through deep reinforcement learning. Nature ,518, 529.\nMoray, N., & Inagaki, T. (1999). Laboratory studies of trust between humans and\nmachines in automated systems. Transactions of the Institute of Measurement and\nControl ,21, 203–211.\n25\n\nMoray, N., Inagaki, T., & Ito, M. (2000). Adaptive automation, trust, and self-\nconﬁdence in fault management of time-critical tasks. Journal of Experimental Psy-\nchology: Applied ,6, 44–58.\nMorgan, B. B., Glickman, A. S., Woodard, E. A., Blaiwes, A. S., Salas, E., Camp-\nbell, W. J., Miller, D. L., Montero, R. C., & Zimmer, S. (1986). Measurement of\nteam behaviors in a Navy training environment . Old Dominion University Research\nFoundation.\nMuir, B., & Moray, N. (2013). Trust in automation: 2. experimental studies of trust\nand human intervention in a process control simulation. Ergonomics ,39, 429–460.\nNam, C., Walker, P., Lewis, M., & Sycara, K. (2017). Predicting trust in human control\nof swarms via inverse reinforcement learning. In 2017 26th IEEE International\nSymposium on Robot and Human Interactive Communication (RO-MAN) (pp. 528–\n533). IEEE.\nNam, C., Walker, P., Li, H., Lewis, M., & Sycara, K. (2019). Models of trust in human\ncontrol of swarms with varied levels of autonomy. IEEE Transactions on Human-\nMachine Systems , .\nNg, A. Y ., Russell, S. J. et al. (2000). Algorithms for inverse reinforcement learning.\nInIcml (p. 2). volume 1.\nOh, J., Suppe, A., Duvallet, F., Boularias, A., Vinokurov, J., Navarro-Serment, L.,\nRomero, O., Dean, R., Lebiere, C., Hebert, M., & Stentz, A. (2015). Toward mobile\nrobots reasoning like humans. In AAAI Conference on Artiﬁcial Intelligence (AAAI) .\nParasuraman, R., & Manzey, D. (2010). Compacency and bias in human use of au-\ntomation:an attentional integration. Human Factors ,52, 381–410.\nParasuraman, R., Sheridan, T., & C, W. (2000). A model of types and levels of hu-\nman interaction with automation. IEEE Transactions on SMC, Part A: Systems and\nHumans ,30, 286–297.\nPezzulo, G., Donnarumma, F., & Dindo, H. (2013). Human sensorimotor communica-\ntion: A theory of signaling in online social interactions. PloS one ,8, e79876.\nRamachandran, D., & Amir, E. (2007). Bayesian inverse reinforcement learning. In\nIJCAI (pp. 2586–2591). volume 7.\nRibeiro, M. T., Singh, S., & Guestrin, C. (2016). Why should i trust you?: Explaining\nthe predictions of any classiﬁer. In Proceedings of the 22nd ACM SIGKDD interna-\ntional conference on knowledge discovery and data mining (pp. 1135–1144). ACM.\nRiedmiller, M. (2005). Neural ﬁtted q iteration–ﬁrst experiences with a data efﬁcient\nneural reinforcement learning method. In European Conference on Machine Learn-\ning(pp. 317–328). Springer.\nRiley, V . A. (1994). Human use of automation . Ph.D. thesis University of Minneapolis.\n26\n\nRobinette, P., Li, W., Allen, R., Howard, A. M., & Wagner, A. R. (2016). Overtrust of\nrobots in emergency evacuation scenarios. In ACM/IEEE International Conference\non Human-Robot Interaction (pp. 101–108).\nRummery, G. A., & Niranjan, M. (1994). On-line Q-learning using connectionist\nsystems . University of Cambridge, Department of Engineering.\nSadler, G., Battiste, H., Ho, N., Hoffmann, L., Johnson, W., Shively, R., Lyons, J.,\n& Smith, D. (2016). Effects of transparency on pilot trust and agreement in the\nautonomous constrained ﬂight planner. In 2016 IEEE/AIAA 35th Digital Avionics\nSystems Conference (DASC) (pp. 1–9). IEEE.\nSalas, E., Cooke, N. J., & Rosen, M. A. (2008). On teams, teamwork, and team perfor-\nmance: Discoveries and developments. Human factors ,50, 540–547.\nSalas, E., Sims, D. E., & Burke, C. S. (2005). Is there a “big ﬁve” in teamwork? Small\ngroup research ,36, 555–599.\nSchaefer, K. E. (2013). The perception and measurement of human-robot trust . Ph.D.\nthesis University of Central Florida Orlando, Florida.\nSeppelt, B. D., & Lee, J. D. (2007). Making adaptive cruise control (acc) limits visible.\nInternational journal of human-computer studies ,65, 192–205.\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanc-\ntot, M., Sifre, L., Kumaran, D., Graepel, T. et al. (2017). Mastering chess and\nshogi by self-play with a general reinforcement learning algorithm. arXiv preprint\narXiv:1712.01815 , .\nSimonyan, K., Vedaldi, A., & Zisserman, A. (2013). Deep inside convolutional net-\nworks: Visualising image classiﬁcation models and saliency maps. arXiv preprint\narXiv:1312.6034 , .\nSimpson, A., Brander, G., & Portsdown, D. (1995). Seaworthy trust: Conﬁdence in\nautomated data fusion. The Human-Electronic Crew: Can we Trust the Team , (pp.\n77–81).\nStanton, N. A., Young, M. S., & Walker, G. H. (2007). The psychology of driving au-\ntomation: a discussion with professor don norman. International journal of vehicle\ndesign ,45, 289–306.\nSutton, R. S., & Barto, A. G. (1998). Reinforcement learning: An introduction vol-\nume 1. MIT press Cambridge.\nSycara, K. P., Lewis, M., Lenox, T., & Roberts, L. (1998). Calibrating trust to integrate\nintelligent agents into human teams. In System Sciences, 1998., Proceedings of the\nThirty-First Hawaii International Conference on (pp. 263–268). IEEE volume 1.\nSzegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., & Fergus,\nR. (2013). Intiriguing properties of neural networks. In arXiv:1312.6199 .\n27\n\nThines, C. A. . B. G. E., G. (1991). Michotte’s experimental phenomenology of per-\nception . Hillsdale, NJ: Erlbaum.\nVerberne, F. M., Ham, J., & Midden, C. J. (2012). Trust in smart systems: Sharing\ndriving goals and giving information to increase trustworthiness and acceptability of\nsmart systems in cars. Human factors ,54, 799–810.\nde Visser, E., Parasuraman, R., Freedy, A., Freedy, E., & Weltman, G. (2006). A\ncomprehensive methodology for assessing human-robot team performance for use\nin training and simulation. In Proceedings of the Human Factors and Ergonomics\nSociety 50th Annual Meeting (pp. 2639–2643). HFES.\nde Visser, E. J., Cohen, M., Freedy, A., & Parasuraman, R. (2014). A design method-\nology for trust cue calibration in cognitive agents. In International Conference on\nVirtual, Augmented and Mixed Reality (pp. 251–262). Springer.\nWang, N., Pynadath, D. V ., & Hill, S. G. (2016). Trust calibration within a human-robot\nteam: Comparing automatically generated explanations. In The Eleventh ACM/IEEE\nInternational Conference on Human Robot Interaction (pp. 109–116). IEEE Press.\nWang, Z., Schaul, T., Hessel, M., Van Hasselt, H., Lanctot, M., & De Freitas, N.\n(2015). Dueling network architectures for deep reinforcement learning. arXiv\npreprint arXiv:1511.06581 , .\nWatkins, C. J. C. H. (1989). Learning from delayed rewards . Ph.D. thesis University\nof Cambridge England.\nWickens, C. (2018). Automation stages & levels, 20 years after. Journal of Cognitive\nEngineering and Decision Making ,12, 35–41.\nWilliams, J. D., Asadi, K., & Zweig, G. (2017). Hybrid code networks: practical\nand efﬁcient end-to-end dialog control with supervised and reinforcement learning.\narXiv preprint arXiv:1702.03274 , .\nWilliams, R. J. (1992). Simple statistical gradient-following algorithms for connec-\ntionist reinforcement learning. Machine learning ,8, 229–256.\nWright, J. L., Chen, J. Y ., Barnes, M. J., & Hancock, P. A. (2016). The effect of agent\nreasoning transparency on automation bias: An analysis of response performance. In\nInternational Conference on Virtual, Augmented and Mixed Reality (pp. 465–477).\nSpringer.\nXu, A., & Dudek, G. (2015). Optimo: Online probabilistic trust inference model for\nasymmetric human-robot collaborations. In Conference on Human Robot Interaction\n(pp. 221–228). ACM.\nZahavy, T., Ben-Zrihem, N., & Mannor, S. (2016). Graying the black box: Under-\nstanding dqns. In International Conference on Machine Learning (pp. 1899–1908).\n28\n\nZhang, A., Ballas, N., & Pineau, J. (2018a). A dissection of overﬁtting and gener-\nalization in continuous reinforcement learning. arXiv preprint arXiv:1806.07937 ,\n.\nZhang, C., Vinyals, O., Munos, R., & Bengio, S. (2018b). A study on overﬁtting in\ndeep reinforcement learning. arXiv preprint arXiv:1804.06893 , .\nZhou, A., Hadﬁeld-Menell, D., Nagabandi, A., & Dragan, A. D. (2017). Expressive\nrobot motion timing. In Proceedings of the 2017 ACM/IEEE International Confer-\nence on Human-Robot Interaction (pp. 22–31). ACM.\nZhu, Y ., Mottaghi, R., Kolve, E., Lim, J. J., Gupta, A., Fei-Fei, L., & Farhadi, A.\n(2016). Target-driven visual navigation in indoor scenes using deep reinforcement\nlearning. arXiv preprint arXiv:1609.05143 , .\n29",
  "textLength": 86384
}