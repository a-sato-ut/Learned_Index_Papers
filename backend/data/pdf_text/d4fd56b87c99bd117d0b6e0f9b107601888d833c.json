{
  "paperId": "d4fd56b87c99bd117d0b6e0f9b107601888d833c",
  "title": "On the Costs and Benefits of Learned Indexing for Dynamic High-Dimensional Data: Extended Version",
  "pdfPath": "d4fd56b87c99bd117d0b6e0f9b107601888d833c.pdf",
  "text": "On the Costs and Benefits of Learned Indexing\nfor Dynamic High-Dimensional Data: Extended\nVersion\nTerézia Slanináková1,2[0000−0003−0502−1145], Jaroslav\nOlha1[0000−0003−1824−468X], David Procházka1[0009−0000−2765−8329], Matej\nAntol1,2[0000−0002−1380−5647], and Vlastislav Dohnal1[0000−0001−7768−7435]\n1Faculty of Informatics, Masaryk University, Brno, Czechia\n2Institute of Computer Science, Masaryk University, Brno, Czechia\nAbstract. One of the main challenges within the growing research area\nof learned indexing is the lack of adaptability to dynamically expanding\ndatasets. This paper explores the dynamization of a static learned index\nfor complex data through operations such as node splitting and broaden-\ning, enabling efficient adaptation to new data. Furthermore, we evaluate\nthe trade-offs between static and dynamic approaches by introducing an\namortized cost model to assess query performance in tandem with the\nbuild costs of the index structure, enabling experimental determination\nof when a dynamic learned index outperforms its static counterpart. We\napply the dynamization method to a static learned index and demon-\nstrate that its superior scaling quickly surpasses the static implementa-\ntion in terms of overall costs as the database grows. This is an extended\nversion of the paper presented at DAWAK 2025.\nKeywords: Learnedindexing ·Dynamization ·Dynamicdatasets ·k-NN\nsearch ·ANN search\n1 Introduction\nThe problem of adapting to dynamically expanding datasets remains a challenge\nin many indexing approaches. For instance, many recent advances in indexing\ninvolve machine learning models, leading to the emergence of a new specialized\nfield of study called learned indexing . Once trained, machine learning models\ntypically cannot be updated with new data or classification categories without\nlosing prior knowledge, often requiring a full retraining instead.\nDespite this limitation, learned indexing has proven successful at indexing\nstructured, low-dimensional datasets [6,15,38], and continues to gain traction in\ncomplex data indexing and retrieval [1,8,13,16,17]. However, when it comes to\nindexing complex, high-dimensional data, the models also tend to become more\ncomplex [17].\nIt is reasonable to assume that learned models are on their way to modeling\ncomplex data distributions more efficiently than traditional indexing methods,arXiv:2507.05865v1  [cs.IR]  8 Jul 2025\n\n2 T. Slanináková et al.\nraising the question of the future role of indexing experts in such a transition.\nOne option is for indexing experts to develop machine learning expertise and\ndesign models that better support dynamic learning and effectively manage com-\nplex data. Another, more feasible option – though not mutually exclusive – is to\naddress the indexing questions separately from the core machine learning chal-\nlenges, making improvements to learned indexing methods at a higher level of\nabstraction. This approach would be analogous to how indexing research rarely\nconcerns itself with the physical characteristics of storage media, leaving those\nlower levels of abstraction to storage systems engineers.\nThis paper is focused on the latter approach, proposing a dynamization\nmethod capable of transforming a static learned indexing structure for complex,\nhigh-dimensional data into a dynamic one. This is achieved through general-\nized node-splitting and node-broadening operations, along with a set of simple\nrules for their application. Even though it was developed with a learned index-\ning use case in mind, this approach is general enough to be applicable to any\npartition-based indexing technique that struggles with dynamization.\nA key challenge, however, is the evaluation of the costs and benefits of such\na dynamization, since it can be difficult to compare an index with high upfront\nconstruction costs to one where those costs are distributed more evenly over the\nlifetime of the database.\nThus, we compare the static and dynamic approaches in terms of their amor-\ntized costs. We do this by establishing several indexing scenarios, and defining\nthem in a way that allows us to quantify the amortized build costs of an aver-\nage query, in addition to its search costs. This results in a single amortized cost\nmetric that can be used to evaluate, in a very clear and straightforward manner,\nwhen dynamization is appropriate and when it might be counter-productive.\n2 Related Work\nThis paper focuses on dynamic learned indexing of complex data – while re-\nsearch specifically on this topic is limited, both learned indexing and dynamic\nindexing have been extensively studied as separate topics. Additionally, the idea\nof dynamizing static indexes in an index-agnostic manner has been explored in\nprior research, which we also review in this section.\n2.1 Learned Indexing\nLearned indexing has emerged as a novel approach to indexing, based on the\npremise that indexes can be viewed as models [15]. While there is a growing\nbody of research on using such indexes for one-dimensional datasets [32], the\nstraightforward idea of learning the cumulative distribution function is not di-\nrectly applicable to high-dimensional data. Most of the existing learned indexes\nfor high-dimensional data either create their own clustering [8,13,18,31] or learn\nan existing one [1,17]. The limitation of such approaches is that they are de-\nsigned to index static datasets only. They do not address the dynamic indexing\n\nLearned Indexing for Dynamic High-Dimensional Data 3\nscenario, aside from resorting to a complete rebuild of the index [17] without\nany guiding policy, essentially relying on the user to trigger the reconstruction\nprocess manually.\nInsert Operation After a certain point, the accuracy of a learned index will\ninevitably deteriorate due to the continuous insertion of new objects that exhibit\ndistribution shifts, or deletion of existing objects. Adapting a trained model to\nnew data requires fine-tuning, which can overwrite parameters critical for han-\ndling previously indexed data. In neural networks, this phenomenon is known\nas catastrophic forgetting [14,24], where optimizing for new tasks degrades per-\nformance on earlier ones. A subfield of machine learning called continual learn-\ning [35] studies methods that allow models to adapt to situations where the\nlearned knowledge is to be adjusted throughout the model’s existence. Such\nmethods attempt to balance the trade-off between learning plasticity, i.e., how\nwell the new task is learned, and memory stability, i.e., how well the model\nremembers previous knowledge.\nSelecting the Model Architecture Choosing the optimal model architecture\nrequiresmachinelearningexpertiseandcanbecomputationallyexpensiveaswell\nas time-consuming. In this paper, we do not focus on the internal model tuning.\nHowever, we presume that an automated search for a model architecture [9,28]\nwould lead to further performance improvements.\n2.2 Dynamicity in Traditional Indexes\nThe feasibility of dynamizing an index structure depends on the index type –\nwhile some naturally accommodate dynamic operations, others present inherent\nchallenges.\nTree-based Tree-based methods hierarchically divide the search space so that\neach node covers a given sub-space, thereby increasing the specificity of the\nsearch with deeper tree levels. While some tree indexes, particularly those based\non a pivoting mechanism [7,40] are inherently dynamic, others [5,25] either pre-\nsume a static use case or require a balanced structure for optimal performance,\nrequiring a re-balancing operation to prevent the influx of new data from de-\ngrading the performance.\nGraph-based Graph-based indexes [36] are considered the state of the art in\napproximatenearestneighborsearch[2].Conceptualizedasproximitygraphs[11,\n23], they approximate their theoretical counterparts [10,34], reducing building\ncosts and accelerating search by omitting non-relevant edges. HNSW [22], the\nmostresearchedindexinthiscategory,isincorporatedintoawiderangeofvector\n\n4 T. Slanináková et al.\ndatabases [27] and is dynamic by design due to its incremental building proce-\ndure. However, it lacks a hard delete operation, instead relying on so-called lazy\ndeletion, which depends on the eventual garbage collection of tagged objects.3\nLSH-based Most LSH-based methods [12,19,21] are built for static use and\nthus fail to efficiently accommodate dynamic data loads. Adding new data to a\ncreated LSH typically requires recalculating the hash functions and updating the\nindex. Two notable works attempt to extend the static nature of LSH to dynamic\nuse cases: LSH forest [3] through the use of a multiple prefix tree structure and\nPLSH [33] by employing an insert-optimized hash table.\n2.3 Dynamization Frameworks\nThe central theme of this paper is the transformation of a static index into a dy-\nnamic one – dynamization – through the use of simple, index-agnostic operators.\nThe goal of dynamization, as proposed in the original Bentley-Saxe method [4],\nis to abstract away from the details of particular indexing solutions by integrat-\ning procedures for insertion and deletion as well as transforming a single index\ninto a series of indexes of progressively larger sizes. Although this method has\nbeen applied in numerous use cases [26,29,39], it struggles with deletion perfor-\nmance, limited support for various query types and a lack of generality beyond\ndecomposable search problems. Recent work [30] addressed the shortcomings of\nBentley-Saxe by proposing a more general dynamization framework, paving the\nway for modern, robust index dynamization.\nAsopposedto[30],thedynamizationmethodpresentedinthispaperisnotan\nextensiveready-to-useframework.Instead,weintroduceaminimalisticapproach\nconsisting of two extension operations and a basic set of restructuring policies\nthat can be implemented into any partitioning-based index. While comprehen-\nsive frameworks are suitable for complex dynamization needs (e.g., performance\ntuning), in this paper, we show that many use cases can be addressed with our\nlightweight approach.\n3 Methods\nThe static index we have chosen for dynamization is a hierarchical learned index\ncalled the Learned Metric Index (LMI) [1]. The model was first conceptualized\nas a tree structure composed of learned models, where the root node is a model\ntrained on all the data, with a pre-defined number of classification categories\ncorresponding to its children. The child nodes are either leaf nodes, i.e., buckets\ncontaining the given subset of the data, or inner nodes, which correspond to\nanother learned model – this model once again partitions its given subset of\n3https://github.com/nmslib/hnswlib/issues/4\n\nLearned Indexing for Dynamic High-Dimensional Data 5\nDEEPEN((0,0), 3)\n(0,0) (0,1) (0,2)(0,)(0,)\n(0,0) (0,1) (0,2)\n(0,0,0) (0,0,1) (0,0,2)\nFig. 1.Overview of the deepening operation.\ndata into a pre-defined number of classes.4Once such an index is built, it can be\nused to resolve search queries by recursively classifying the query objects until\na certain number of leaf nodes has been reached.\n3.1 Dynamized Learned Index\nFor a tree-like structure to become dynamic, we need to define mechanisms for\nadaptive expansion of its nodes. For a node nwhich has reached its defined\ncapacity (thereby requiring an update), we describe two basic extension mecha-\nnisms: node split (deepening) and node expansion (broadening).\nDeepening is defined on a leaf node – upon reaching maximum capacity, the\nnode is transformed into an inner node, and its objects are dispersed into newly\ncreated child nodes, as illustrated in Figure 1. In the context of a learned index,\ndeepening would be equivalent to creating a new model and training on the\nnode’s objects with a given target number of child nodes – see Algorithm 1.\nWhile deepening triggers vertical growth, broadening extends the index hor-\nizontally to avoid an unnecessarily deep structure. Broadening is defined as the\nre-creation of a node (either inner or leaf) from scratch with its current objects.\nIn learned indexing, this involves re-partitioning and retraining of the learned\nmodel with all the relevant objects (potentially also objects on the grandchil-\ndren’s level). This operation is fully described in Algorithm 2 and visualized in\nFigure 2. Note that while the complete recreation of a node may not strictly\nmeet the definition of \"broadening\", as the original node technically ceased to\nexist, in the context of learned indexing, adding new categories to a model after\nit has been built would lead to performance deterioration due to catastrophic\nforgetting (as mentioned in Section 2).\n4Specifically, in the index we dynamized, the single predictive unit is a Multi-Layer\nPerceptron (MLP) with one hidden layer of 128 neurons. Prior to training the model,\neach data point was assigned a category by the K-Means clustering algorithm – the\nMLP was then trained with a classification objective in a supervised way.\n\n6 T. Slanináková et al.\nAlgorithm 1: Deepen\nInput:structure S, a data node n, # of new children n_child, distance\nfunction d\nOutput: modified structure S\nlabels = cluster(n.objects, n_child, d)\nmodel, positions = Model(n.objects, labels, n_child)\nnew_n = InnerNode(n.pos, model)\nS.insert_node(new_n)\nS.insert_child_nodes(n_child, new_n.pos, n.objects, positions)\ndelete_nodes([n])\nS.check_consistency()\nreturnS;\nBROADEN((0, ), 5)\n(0,0) (0,1) (0,2)(0,)\n(0,1) (0,2) (0,3)(0,)\n(0,4) (0,0)\nFig. 2.Overview of the broadening operation.\nTheshorten operation addresses the removal of severely underpopulated\nnodes. This involves deleting the node and reinserting its objects into the in-\ndex by removing the corresponding output neuron and its connections from\nthe learned model (MLP) – see Algorithm 3 and Figure 3. Unlike adding a\nneuron (which requires global retraining and risks catastrophic forgetting), this\nlocalized operation allows well-populated categories to redistribute the obsolete\ncategory’sdatapoints,eliminatingitsdecisionboundary.Whileeffectiveforshal-\nlower MLPs and underpopulated categories, deeper networks or significant data\ndistribution shifts may necessitate fine-tuning to maintain model performance.\nPolicies that trigger the structural changes. After defining the three\nbasic operations for a structural update (deepen, broaden, and shorten), the\nnext step is to establish policies to invoke them. First, we define minimum and\nmaximum bounds on the leaf node capacity to impose limits on the costs of\nsequential search. Similarly, these bounds are defined on the inner nodes for\noptimal discriminative power of a single model. We implement a policy of de-\ntecting and resolving any violations of these bounds – in case of a leaf node\nhaving fewer than 5 objects ( underflow ), we invoke the shortenoperation. To\nmanageoverflow, the structure ensures that the average occupancy of leaf nodes\nremains below 1000. When this bound is violated, we invoke node extension, al-\nternating between deepen and broaden operations to maintain a shallow index.\nSpecifically, a node will be deepened (split) until the index reaches a maximum\ndepth of two levels. After this depth is reached, the broaden operation is invoked\n\nLearned Indexing for Dynamic High-Dimensional Data 7\nAlgorithm 2: Broaden\nInput:structure S, an inner node n, # of new children n_child, distance\nfunction d\nOutput: modified structure S\ndata_n, inner_n = S.find_nodes_in_subtree(n.pos)\nobjects = collect_objects(data_n)\nlabels = cluster(objects, n_child, d)\nmodel, positions = Model(n.objects, labels, n_child)\nn.model = model\nnew_n = InnerNode(n.pos, model)\nS.insert_child_nodes(n_child, new_n.pos, objects, positions)\ndelete_nodes(data_n + [inner_n])\nS.check_consistency()\nreturnS;\nSHOR TEN((0,0))\n(0,0) (0,1) (0,2)(0,)\n(0,1) (0,2)(0,)\nFig. 3.Overview of the shortening operation.\non the appropriate node to accommodate further growth, re-partitioning, and\nretraining of the learned model.\n3.2 Evaluation Baseline\nA dynamized index is not necessarily an improvement in terms of total costs\n– while it should theoretically scale better with growing datasets, it introduces\nadditional overhead not present in the static version. To determine the condi-\ntions under which dynamization is worthwhile, the dynamized index should be\nevaluated against its static counterpart.\nOne way to do this is to simply keep inserting objects into the static index\nand compare it with the growing adaptive version. This will necessarily lead to\ndeterioration of the quality of the query results in the static index, but the static\nversion will only require a single build, likely leading to lower overall build costs.\nIn the experimental evaluation, we will refer to this as the No rebuild baseline.\nAnother way is to take advantage of the fact that any static index can be\nmade somewhat dynamic by rebuilding it in its entirety after a certain threshold\nof new objects, thus allowing it to adapt to new data at the cost of additional\nbuild costs – we will refer to this as the Naive rebuild baseline. Implementing\nthe Naive rebuild method involves the selection of a single parameter which\n\n8 T. Slanináková et al.\nAlgorithm 3: Shorten\nInput:structure S, an array of data nodes arrn\nOutput: modified structure S\nInitialize empty array A\nfor each item ninarrndo\nparent = n.get_parent()\nparent.remove_child(n.pos)\nA.add(n.objects)\nend\nS.insert( A); S.delete_nodes( arrn);returnS;\ndetermines how often the index is discarded and rebuilt from scratch. Let us call\nthis parameter the rebuild interval , and define it in terms of the number of new\nobjects that can be added before the index is rebuilt (e.g., a rebuild interval of\n10000 means that after we build the index, the next 9999 objects are simply\nadded to the structure as is, and the 10000th object triggers a full rebuild). In\nthe next section, we will discuss the selection and optimization of this parameter.\n3.3 Amortized Cost Model\nThe typical problem when evaluating static and dynamic indexes is the fact\nthat their costs are distributed very differently – some indexes trade good query\nperformance for high upfront costs of building a well-performing index, while\nothers may be able to start answering queries quickly by starting with an im-\nperfect structure that adapts over time. Thus, if build time is included in the\nevaluation, the static index is greatly disadvantaged at the beginning, as it takes\na long time to even answer the first query. If the build time is excluded, however,\nvarious adaptive methods may be punished for not taking the time to build the\nperfect structure ahead of time.\nA typical approach in research evaluation is to present two sets of plots,\ncomparing the build costs and search costs side by side (for example in [16]).\nWhile this can help identify outliers or unusual behavior (such as an index with\nexorbitant build costs for the sake of minor query improvement), it does not\nprovide a clear overall picture of the actual trade-off between build and search\ncosts. The information is dispersed across two separate sets of plots, often on\ndifferent scales, making it hard to assess directly.\nAn ideal evaluation metric should combine the search costs and build costs\ninto a single objective that can be compared on a per-query basis regardless\nof how dynamic the method is. A previous work evaluated the total cost of\nexecuting a fixed number of queries (including the build costs) [37] – however,\nthis approach assumes a read-only use case, with all the data provided upfront\nrather than being inserted dynamically as the queries arrive. While this metric\ncan be appropriate for read-dominated workloads, it may not fully capture the\nperformanceofsystemsthataresubjecttofrequentupdatesordynamicchanges.\n\nLearned Indexing for Dynamic High-Dimensional Data 9\nTo explore how such systems perform in more dynamic settings, let us define\na new property of the system: the relative number of queries per inserted object.\nWe will call this the querying frequency , and define it simply as#queries\n#new_objects. We\nwill assume querying frequency to be an inherent property of the given indexing\nscenario, which does not change during the lifetime of the database. This allows\nus to relate the number of new objects added to the database (which dictate the\nrebuild costs of a dynamic structure) with the number of queries performed on\nthe database (which determine the amortization).\nThe only other factor that determines whether structural adjustments are\nworth their costs is how accurate the queries need to be – the target recall . If\nlower query recall is sufficient, deterioration of the structure is less problematic,\nand rebuilds are not as worthwhile. If high recall is needed, the index needs to\nbe maintained in peak condition via more frequent rebuilds – deterioration of\nquery quality is more punishing, so even more costly rebuilds are justified.\nOnce we determine the querying frequency of our indexing scenario and the\ndesired (average) recall of our queries, we can infer the amortized search cost as:\nAC=SC+BC\nRI∗QF\nwhere ACis the amortized cost, SCis the search cost of a single query (i.e., how\nmany seconds it takes for an average query to achieve the target recall), RIis\nthe rebuild interval of the given index (in terms of the average number of new\nobjects that trigger a rebuild), BCis its build cost (in seconds), and QFis the\nquerying frequency of the given indexing scenario (in terms of queries per insert).\nIn other words, if an index is rebuilt after adding 1K new objects, and the\nquerying frequency is 100 queries per new object, then one build of the index will\nlast for 100K queries. Therefore, the amortized cost of a query should include\n1\n100000of the index build costs in addition to the immediate search costs.\nThis allows methods with infrequent but costly rebuilds to be directly com-\npared with methods that perform gradual, less extensive updates.\nOptimal Rebuild Interval The concept of amortized cost can also be used\nto optimize the Naive rebuild baseline method described earlier – if we can\ndetermine the indexing scenario ahead of time (i.e., estimate the typical number\nof new queries per insert and decide on the desired recall of the database), we\ncan optimize when a full rebuild of the structure should be triggered.\nThe optimal rebuild interval must balance the cost of rebuilding the entire\nstructure against the increasing search costs of the average query caused by the\ngradual deterioration of the index. As objects are added without rebuilding the\nindex, the per-query build costs decline over time, since there are more queries\nto divide the original build cost. On the other hand, the per-query search costs\nincrease over time as the index deteriorates due to a lack of a structural update.\nThus, the sum of these two components inevitably creates a single optimum,\nrepresenting the ideal rebuild interval with the perfect balance of search and\nbuild costs that minimizes the amortized cost per query. Figure 4 illustrates\n\n10 T. Slanináková et al.\nhow the amortized cost per query relates to the rebuild interval in a particular\nindexing scenario, derived from one of the experimental cases discussed in the\nnext section.\nFig. 4.The amortized cost of a Naive rebuild baseline at different rebuild intervals\n(setup with 1 query per new object and a target recall of 0.5).\nBy definition, the rebuild interval can only be optimized for a specific sce-\nnario, that is, a single combination of query frequency and target recall. If these\nproperties cannot be accurately determined in advance, or if they change over\ntime, the performance of this approach may end up being far from optimal.\nThe penalty for a suboptimal selection of the rebuild interval parameter will be\nevaluated in the next section.\n4 Experiments\nForourexperimentalevaluation,weuseSIFTdescriptors[20],oneofthedatasets\nused in ANN-benchmarks [2]. This dataset consists of 1 million objects of 128\ndimensions with the Euclidean distance metric, and additionally contains 10K\nqueries in a 30-NN setup for experimental evaluation.\nThe experiments were conducted using Python 3.8 with PyTorch 1.8 on a\nmachine with AMD EPYC 7261 8-Core Processor and 50 GiB of RAM. GPU\nacceleration was not used.\nSince the amortized cost metric calculation depends on two variables – the\nnumber of queries per inserted object (QPI) andtarget recall (TR) – the eval-\nuation considers scenarios with two extreme settings of either variable, for a\ntotal of 4 scenarios. We set a high querying frequency at 100 queries per insert\n(corresponding, for instance, to a typical social media feed), and low querying\nfrequency at 1 query per insert (corresponding to a monitoring service or a mes-\nsaging system). For target recall, we set the extremes at 0.9 (high) and 0.5 (low)\n\nLearned Indexing for Dynamic High-Dimensional Data 11\n– in the context of a 30-NN query, this means that we require the database to\nreturn, on average, at least 27 objects at high target recall and 15 objects at low\ntarget recall.\nAs described in Section 3, we compare the dynamized version of the learned\nindexagainsttwobaselines–the Naive rebuild baseline,whichrebuildstheentire\nindex periodically based on a pre-defined rebuild interval, and the No rebuild\nbaseline, which simply stores incoming objects without adjusting its structure\nuntil it runs out of experimental data to process. We performed the experiment\nwith various database sizes, ranging from 100K to 900K. For the baselines, this\nmeans that the database was first built using the given number of initial objects,\nand then new objects were added – in the case of the Naive rebuild baseline, the\nobjects were added until a rebuild was triggered, in the No rebuild baseline, they\nwere added until all 1M experimental data objects were indexed. The dynamized\nindexwasbuiltgraduallystartingwithnoobjects,anditsamortizedperformance\nwas simply evaluated after every 100K objects as the index grew and adapted.\nSincethe Naive rebuild baselineissensitivetotheselectionofrebuildinterval,\nwe optimized this parameter for each of the 4 experimental scenarios, resulting\nin 4 different performance curves of this method. As a result, in each evaluated\nscenario, one of the Naive rebuild baselines shows the optimal solution (since\nthe rebuild frequency is set up perfectly for that scenario), and the other Naive\nrebuild baselines show the penalty for parameterizing the method incorrectly.\nAs for the No rebuild baseline, since the index is only built once (using\nthe initial number of objects), the build costs are as low as possible for the\ngiven initial database size, but the quality of queries will keep deteriorating\ntowards exhaustive search in the limit. Due to the size of our experimental\ndataset (1M objects), we can only simulate smaller and smaller increments of\nthis deterioration as we use more objects to build the initial database. In other\nwords, when the initial database size is 100K, the plotted results show how much\nqueries deteriorate if database size increases by 900% with no rebuild; when we\nstart with 900K objects, the plot shows how much performance deteriorates if we\nadd∼11% additional objects, as there are no more objects to insert. Thus, when\nplotting the amortized cost, the results of this baseline keep converging towards\nthe dynamized methods, not due to improved performance of the method, but\nbecause the fixed size of the experimental dataset limits the ability to model\nlarger-scale growth.\nNote that the static index used in the Naive rebuild andNo rebuild baselines\nis a single-level structure, implemented as a single MLP, and is parameterized to\nholdanaverageof1Kobjectsperbucket.Additionally,notethatonlyinsertionof\nnew objects is considered – deletion and re-balancing can also be implemented\nin a straightforward manner (as shown in Section 3.1), but the experimental\nevaluation of such operations is beyond the scope of this paper.\nFigures 5-8 show how amortized costs scale with the database size for each\ncombination of high/low querying frequency and high/low target recall. Keep\nin mind that the \"Initial database size\" label only holds true for the baseline\n\n12 T. Slanináková et al.\nFig. 5.Amortized costs of the dynamized index and various baselines in the high\nintensity—high target recall scenario (100 queries per insert, target recall of 0.9).\nFig. 6.Amortized costs of the dynamized index and various baselines in the high\nintensity—low target recall scenario (100 queries per insert, target recall of 0.5).\nFig. 7.Amortized costs of the dynamized index and various baselines in the low\nintensity—high target recall scenario (1 query per insert, target recall of 0.9).\n\nLearned Indexing for Dynamic High-Dimensional Data 13\nFig. 8.Amortized costs of the dynamized index and various baselines in the low\nintensity—low target recall scenario (1 query per insert, target recall of 0.5).\nmethods – the dynamized index always has an initial database size of 0, and it\nis merely evaluated at various size thresholds as it grows.\nOur first observation is that the plots communicate the overall costs of a\ngiven indexing approach in a very transparent manner, making it clear which\nmethods perform best under which conditions, and how this is affected by the\ngrowing datasets. As expected, the amortized costs of the Naive rebuild andNo\nrebuildbaselines keep increasing with database size – as a result, the dynamized\nmethod inevitably performs better on larger datasets.\nIn scenarios with higher querying intensity (100 queries per object), the var-\nious rebuild intervals of the Naive rebuild method do not seem to make much of\na difference. In lower intensity scenarios, however, the method with the most fre-\nquent rebuilds (parameterized for high intensity scenarios) is severely punished,\nusually performing worse than a method that performs no rebuilds at all. The\nreason for this can be inferred from Figure 4 – if the rebuild interval is too small,\nthe penalty in terms of amortized costs is much steeper than if the interval is too\nlarge. Aside from this extreme case, the parameterization of the Naive rebuild\nmethod is not as impactful as we had expected – the choice between the naive\nand dynamized approaches is usually much more important than the choice of\nthe rebuild interval between different Naive rebuild variants.\n5 In Conclusion\nWe introduced a method for dynamizing a static index, using a streamlined\nset of policies and operations to enable adaptive expansion. This approach was\nused to transform a previously static learned index for complex datasets into a\ndynamic one. In addition, we introduced an amortized cost model to evaluate\nthe benefits of the dynamic version against its static counterpart in various\nscenarios. As expected, the results show that the dynamized index scales very\n\n14 T. Slanináková et al.\nwell with database size in all scenarios, and while its overhead may not be\njustified for smaller databases, the dynamization quickly proves advantageous as\nthe database grows.\nTaken together, our dynamization approach offers a straightforward way to\ntransition from static to dynamic learned indexes, significantly broadening their\napplicability in complex data, while the evaluation method provides a practical\nway to assess the trade-offs of dynamization and identify scenarios where its\nbenefits outweigh its costs.\n\nLearned Indexing for Dynamic High-Dimensional Data 15\nReferences\n1. Antol, M., Olha, J., Slanináková, T., Dohnal, V.: Learned metric index – proposi-\ntion of learned indexing for unstructured data. Inf. Systems 100, 101774 (2021)\n2. Aumüller, M., Bernhardsson, E., Faithfull, A.: Ann-benchmarks: A benchmarking\ntool for approximate nearest neighbor algorithms. Inf. Systems 87, 101374 (2020)\n3. Bawa, M., Condie, T., Ganesan, P.: Lsh forest: self-tuning indexes for similarity\nsearch. In: Proceedings of the 14th international conference on World Wide Web.\npp. 651–660 (2005)\n4. Bentley, J.L., Saxe, J.B.: Decomposable searching problems i. static-to-dynamic\ntransformation. Journal of Algorithms 1(4), 301–358 (1980)\n5. Bernhardsson, E.: Annoy at github. GitHub. Repéré à https://github.\ncom/spotify/annoy (2015)\n6. Ding, J., Minhas, U.F., Yu, J., Wang, C., Do, J., Li, Y., Zhang, H., Chandramouli,\nB., Gehrke, J., Kossmann, D., et al.: Alex: an updatable adaptive learned index.\nIn: Proc. ACM SIGMOD 2020. pp. 969–984 (2020)\n7. Dohnal, V., Gennaro, C., Savino, P., Zezula, P.: D-index: Distance searching index\nfor metric data sets. Multimedia Tools Appl. 21(1), 9–33 (Sep 2003)\n8. Dong, Y., Indyk, P., Razenshteyn, I.P., Wagner, T.: Learning space partitions for\nnearest neighbor search. ICLR (2020)\n9. Elsken, T., Metzen, J.H., Hutter, F.: Neural architecture search: A survey. Journal\nof Machine Learning Research 20(55), 1–21 (2019)\n10. Fortune, S.: Voronoi diagrams and delaunay triangulations. In: Handbook of dis-\ncrete and computational geometry, pp. 705–721. Chapman and Hall/CRC (2017)\n11. Fu, C., Xiang, C., Wang, C., Cai, D.: Fast approximate nearest neighbor search\nwith the navigating spreading-out graph. Proc. VLDB Endow. 12(5), 461–474\n(2019)\n12. Gionis, A., Indyk, P., Motwani, R., et al.: Similarity search in high dimensions via\nhashing. In: Vldb. vol. 99, pp. 518–529 (1999)\n13. Gupta, G., Medini, T., Shrivastava, A., Smola, A.J.: Bliss: A billion scale index us-\ningiterativere-partitioning.In:Proceedingsofthe28thACMSIGKDDConference\non Knowledge Discovery and Data Mining. pp. 486–495 (2022)\n14. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu,\nA.A.,Milan,K.,Quan,J.,Ramalho,T.,Grabska-Barwinska,A.,etal.:Overcoming\ncatastrophic forgetting in neural networks. Proceedings of the national academy of\nsciences 114(13), 3521–3526 (2017)\n15. Kraska,T.,Beutel,A.,Chi,E.H.,Dean,J.,Polyzotis,N.:Thecaseforlearnedindex\nstructures. In: Proceedings of the 2018 international conference on management of\ndata. pp. 489–504 (2018)\n16. Lampropoulos, K., Zardbani, F., Mamoulis, N., Karras, P.: Adaptive indexing in\nhigh-dimensional metric spaces. Proc. VLDB Endow. (2023)\n17. Li, L., Han, A., Cui, X., Wu, B.: Flex: A fast and light-weight learned index for\nknn search in high-dimensional space. Information Sciences 669, 120546 (2024)\n18. Li,W.,Feng,C.,Lian,D.,Xie,Y.,Liu,H.,Ge,Y.,Chen,E.:Learningbalancedtree\nindexes for large-scale vector retrieval. In: Proceedings of the 29th ACM SIGKDD\nConference on Knowledge Discovery and Data Mining. pp. 1353–1362 (2023)\n19. Liu, Y., Cui, J., Huang, Z., Li, H., Shen, H.T.: Sk-lsh: an efficient index structure\nforapproximatenearestneighborsearch.Proc.VLDBEndow. 7(9),745–756(2014)\n20. Lowe, D.G.: Distinctive image features from scale-invariant keypoints. Interna-\ntional journal of computer vision 60, 91–110 (2004)\n\n16 T. Slanináková et al.\n21. Lv, Q., Josephson, W., Wang, Z., Charikar, M., Li, K.: Intelligent probing for\nlocality sensitive hashing: Multi-probe lsh and beyond. Proc. VLDB Endow. (2017)\n22. Malkov, Y.A., Yashunin, D.A.: Efficient and robust approximate nearest neighbor\nsearch using hierarchical navigable small world graphs. IEEE TPAMI 42(4), 824–\n836 (2018)\n23. Malkov, Y., Ponomarenko, A., Logvinov, A., Krylov, V.: Approximate nearest\nneighbor algorithm based on navigable small world graphs. Inf. Systems 45, 61–68\n(2014)\n24. McCloskey, M., Cohen, N.J.: Catastrophic interference in connectionist networks:\nThesequentiallearningproblem.In:Psychologyoflearningandmotivation,vol.24,\npp. 109–165. Elsevier (1989)\n25. Muja, M., Lowe, D.G.: Scalable nearest neighbor algorithms for high dimensional\ndata. IEEE TPAMI 36(11), 2227–2240 (2014)\n26. Naidan, B., Hetland, M.L.: Static-to-dynamic transformation for metric indexing\nstructures. In: International Conference on Similarity Search and Applications. pp.\n101–115. Springer (2012)\n27. Pan, J.J., Wang, J., Li, G.: Survey of vector database management systems. The\nVLDB Journal 33(5), 1591–1615 (2024)\n28. Ren, P., Xiao, Y., Chang, X., Huang, P.Y., Li, Z., Chen, X., Wang, X.: A com-\nprehensive survey of neural architecture search: Challenges and solutions. ACM\nComputing Surveys (CSUR) 54(4), 1–34 (2021)\n29. Rumbaugh, D.B., Xie, D.: Practical dynamic extension for sampling indexes. Pro-\nceedings of the ACM on Management of Data 1(4), 1–26 (2023)\n30. Rumbaugh, D.B., Xie, D., Zhao, Z.: Towards systematic index dynamization. Proc.\nVLDB Endow. 17(11), 2867–2879 (2024)\n31. Slanináková, T., Antol, M., Olha, J., Kaňa, V., Dohnal, V.: Data-driven learned\nmetric index: an unsupervised approach. In: International Conference on Similarity\nSearch and Applications. pp. 81–94. Springer (2021)\n32. Sun,Z.,Zhou,X.,Li,G.:Learnedindex:Acomprehensiveexperimentalevaluation.\nProc. VLDB Endow. 16(8), 1992–2004 (2023)\n33. Sundaram, N., Turmukhametova, A., Satish, N., Mostak, T., Indyk, P., Madden,\nS., Dubey, P.: Streaming similarity search over one billion tweets using parallel\nlocality-sensitive hashing. Proc. VLDB Endow. 6(14), 1930–1941 (2013)\n34. Toussaint, G.T.: The relative neighbourhood graph of a finite planar set. Pattern\nrecognition 12(4), 261–268 (1980)\n35. Wang,L.,Zhang,X.,Su,H.,Zhu,J.:Acomprehensivesurveyofcontinuallearning:\ntheory, method and application. IEEE TPAMI (2024)\n36. Wang, M., Xu, X., Yue, Q., Wang, Y.: A comprehensive survey and experimen-\ntal comparison of graph-based approximate nearest neighbor search. Proc. VLDB\nEndow. 14(11), 1964–1978 (2021)\n37. Wei, J., Peng, B., Lee, X., Palpanas, T.: Det-lsh: A locality-sensitive hashing\nscheme with dynamic encoding tree for approximate nearest neighbor search.\nProc. VLDB Endow. 17(9), 2241–2254 (May 2024). https://doi.org/10.14778/\n3665844.3665854 ,https://doi.org/10.14778/3665844.3665854\n38. Wu, J., Zhang, Y., Chen, S., Wang, J., Chen, Y., Xing, C.: Updatable learned\nindex with precise positions. Proc. VLDB Endow. 14(8), 1276–1288 (2021)\n39. Xie, D., Phillips, J.M., Matheny, M., Li, F.: Spatial independent range sampling.\nIn: Proceedings of the 2021 International Conference on Management of Data. pp.\n2023–2035 (2021)\n40. Yianilos, P.N.: Data structures and algorithms for nearest neighbor search in gen-\neral metric spaces. In: Soda. vol. 93, pp. 311–21 (1993)",
  "textLength": 38152
}