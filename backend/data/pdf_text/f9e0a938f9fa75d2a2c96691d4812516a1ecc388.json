{
  "paperId": "f9e0a938f9fa75d2a2c96691d4812516a1ecc388",
  "title": "Density-optimized Intersection-free Mapping and Matrix Multiplication for Join-Project Operations",
  "pdfPath": "f9e0a938f9fa75d2a2c96691d4812516a1ecc388.pdf",
  "text": "Density-optimized Intersection-free Mapping and Matrix\nMultiplication for Join-Project Operations (extended version)\nZichun Huang, Shimin Chen *\nSKL of Computer Architecture, ICT, CAS\nUniversity of Chinese Academy of Sciences\nABSTRACT\nA Join-Project operation is a join operation followed by a duplicate\neliminating projection operation. It is used in a large variety of\napplications, including entity matching, set analytics, and graph\nanalytics. Previous work proposes a hybrid design that exploits the\nclassical solution (i.e., join and deduplication), and MM (matrix\nmultiplication) to process the sparse and the dense portions of the\ninput data, respectively. However, we observe three problems in\nthe state-of-the-art solution: 1) The outputs of the sparse and dense\nportions overlap, requiring an extra deduplication step; 2) Its table-\nto-matrix transformation makes an over-simplified assumption of the\nattribute values; and 3) There is a mismatch between the employed\nMM in BLAS packages and the characteristics of the Join-Project\noperation.\nIn this paper, we propose DIM3, an optimized algorithm for the\nJoin-Project operation. To address 1), we propose an intersection-free\npartition method to completely remove the final deduplication step.\nFor 2), we develop an optimized design for mapping attribute values\nto natural numbers. For 3), we propose DenseEC and SparseBMM\nalgorithms to exploit the structure of Join-Project for better effi-\nciency. Moreover, we extend DIM3to consider partial result caching\nand support Join- ğ‘œğ‘queries, including Join-Aggregate and MJP\n(Multi-way Joins with Projection). Experimental results using both\nreal-world and synthetic data sets show that DIM3outperforms pre-\nvious Join-Project solutions by a factor of 2.3 Ã—-18Ã—. Compared to\nRDBMSs, DIM3achieves orders of magnitude speedups.\n1 INTRODUCTION\nA Join-Project operation is a join operation followed by a duplicate\neliminating projection operation [ 1]. Given two tables ğ‘…(ğ‘¥,ğ‘¦)and\nğ‘†(ğ‘§,ğ‘¦), the Join-Project operation can be written as follows:\nÎ ğ‘¥,ğ‘§(ğ‘…(ğ‘¥,ğ‘¦)Zğ‘¦ğ‘†(ğ‘§,ğ‘¦)) (1)\nIt joinsğ‘…andğ‘†withğ‘¦as the join key, then projects and deduplicates\n(ğ‘¥,ğ‘§)tuples. Î denotes the duplicate eliminating projection.\nExample : In the HetRec2011 data set [ 7],U(userID, bookID,\ntagID) table contains tags given by users to books that they read,\nandB(bookID, tagID, weight) table records books and their possible\ntags with weights. The following SQL query recommends books to\nusers based on the tags recorded in usersâ€™ reading history:\nSELECT DISTINCT U.userID, B.bookID\nFROM U, B\nWHERE U.tagID = B.tagID\nThis Join-Project operation joins U and B on tagID, then projects\n*Shimin Chen is the corresponding author.\nOur codes are available at https://github.com/schencoding/JoinProject-DIM3 .and deduplicates using SELECT DISTINCT. The results can be\nstored by the application for quick user-specific recommendations.\nThe Join-Project operation is used in a large variety of appli-\ncations [ 10], including entity matching, set analytics, and graph\nanalytics. The above is an example of entity matching. Similar ex-\namples include finding users who have seen the same movies in the\nMovieLens data set [ 15], and discovering co-authors in the DBLP\ndata set [43]. Moreover, if tuple (ğ‘¥,ğ‘¦)represents that set ğ‘¥contains\nelementğ‘¦, then the Join-Project operation using ğ‘¦as the join key ob-\ntains all the pairs of sets that intersect with each other. Furthermore,\nif we interpret tuple (ğ‘¥,ğ‘¦)as an edge between two vertices ğ‘¥andğ‘¦\nin a graph, then the Join-Project operation can be used to compute\nall pairs of vertices that are indirectly connected.\n1.1 Previous Solutions\nClassical Solution. The classical solution to compute the Join-\nProject in RDBMSs is to first perform the join operation [ 2,23,33],\nthen deduplicate the projected join results using hash tables [ 24]\nor other types of indices [ 14,17,25]. The time complexity of the\nclassical solution is Î˜(|ğ‘…|+|ğ‘†|+|ğ‘‚ğ‘ˆğ‘‡ğ½|), where|ğ‘…|,|ğ‘†|, and|ğ‘‚ğ‘ˆğ‘‡ğ½|\ndenote the sizes of input table ğ‘…, tableğ‘†, and the join results before\ndeduplication, respectively. This cost is reasonable when the number\nof duplicates is low. However, the solution is less efficient when\n|ğ‘‚ğ‘ˆğ‘‡ğ½|is much larger than the size |ğ‘‚ğ‘ˆğ‘‡ğ‘ƒ|of the final results after\ndeduplication. For example, |ğ‘‚ğ‘ˆğ‘‡ğ½|is 3.7x as large as |ğ‘‚ğ‘ˆğ‘‡ğ‘ƒ|in\nthe HetRec2011 example, while |ğ‘‚ğ‘ˆğ‘‡ğ½|is 24x larger than|ğ‘‚ğ‘ˆğ‘‡ğ‘ƒ|\nin the MovieLens data set. Let |ğ‘‹|,|ğ‘Œ|and|ğ‘|denote the number of\ndistinct values in column ğ‘¥,ğ‘¦, andğ‘§in Eqn 1, respectively. Consider\nthe case where|ğ‘‹|=|ğ‘Œ|=|ğ‘|=ğ‘›.|ğ‘‚ğ‘ˆğ‘‡ğ½|can beO(ğ‘›3)in the worst\ncase, while|ğ‘‚ğ‘ˆğ‘‡ğ‘ƒ|is onlyO(ğ‘›2). In other words, the classical solu-\ntion can spend a lot of time generating the |ğ‘‚ğ‘ˆğ‘‡ğ½|join results and\nthen processing them to remove a large number of duplicates.\nMatrix Multiplication. Alternatively, the Join-Project operation\ncan be computed using MM. The basic idea is to represent ta-\nblesğ‘…(ğ‘¥,ğ‘¦)andğ‘†(ğ‘§,ğ‘¦)as two matrices Rğ‘¥Ã—ğ‘¦andSğ‘¦Ã—ğ‘§. Specif-\nically, Rğ‘¥ğ‘–,ğ‘¦ğ‘˜=1if and only if tuple (ğ‘¥ğ‘–,ğ‘¦ğ‘˜)âˆˆğ‘…(similarly for\nğ‘†). Then the multiplication of Rğ‘¥Ã—ğ‘¦andSğ‘¦Ã—ğ‘§gives matrix Cğ‘¥Ã—ğ‘§,\nwhere Cğ‘¥ğ‘–,ğ‘§ğ‘—=Ã|ğ‘Œ|\nğ‘˜=1Rğ‘¥ğ‘–,ğ‘¦ğ‘˜Sğ‘¦ğ‘˜,ğ‘§ğ‘—. A non-zero element Cğ‘¥ğ‘–,ğ‘§ğ‘—>0\nin the matrix corresponds to a tuple (ğ‘¥ğ‘–,ğ‘§ğ‘—)in the final output of\nthe Join-Project operation. Compared to the classical solution, MM\nperforms the join and the deduplication together. There are efficient\nMM implementations in BLAS (Basic Linear Algebra Subprograms)\npackages with advanced techniques [ 9,26,37]. Moreover, there\nare sub-cubic MM algorithms in theory. The best known is the\nCoppersmith-Winograd algorithm with O(ğ‘›2.373)complexity [12].\nHybrid Solution. Recent studies [ 1,10] combine the classical solu-\ntion and the MM solution based on the observation that the classicalarXiv:2206.04995v1  [cs.DB]  10 Jun 2022\n\nFigure 1: Breakdown of DHK run time.\nsolution performs better when the data is sparse, while MM per-\nforms better when the data is dense. Amossen et al. [ 1] propose an\nalgorithm to partition the data into dense and sparse parts according\nto degrees of ğ‘¥,ğ‘¦, andğ‘§. Then dense MM is employed for the dense\nparts and the classical solution is used for the remaining parts.\nDeep, Hu, and Koutris [ 10] correct errors in the cost analysis\nof [1] and implement the algorithm for experimental comparison.\nWe call this state-of-the-art algorithm DHK . DHK has been shown\nto significantly outperform the classical solution. However, the fol-\nlowing problems reduce the efficiency and practicality of DHK:\nâ€¢Overlapping outputs between sparse and dense parts : The Join-\nProject results computed from the sparse and from the dense\nparts can overlap. Consequently, a final deduplication step\nis necessary to deduplicate the overlapping results. Figure 1\nshows the breakdown of the run time of DHK for five data\nsets (cf. Section 5). For all these data sets, DHK partitions the\ninput data into dense and sparse parts. It performs the final\ndeduplication step, incurring significant overhead.\nâ€¢Over-simplified assumption for table-to-matrix transformation :\nThe DHK implementation assumes that the input columns ğ‘¥,\nğ‘¦, andğ‘§contain consecutive natural numbers starting from\n0. Thus, it directly uses their values as row or column ids in\nthe matrices. However, in reality, database attribute values are\nrarely natural numbers. A step is missing: mapping values of\ninput columns to consecutive natural numbers.\nâ€¢Caveats of MM Implementation : DHK invokes MM in BLAS\npackages (e.g., Intel MKL [ 42]) as a black box. On the one\nhand, BLAS packages typically implement the O(ğ‘›3)MM\nalgorithm. It would be interesting to investigate sub-cubic MM\nalgorithms. On the other hand, the MM invocation as a black\nbox cannot exploit the characteristics of the Join-Project oper-\nations for performance improvement.\n1.2 Our Solution: DIM3\nIn this paper, we propose an efficient and practical Join-Project\nalgorithm, DIM3(Density-optimized Intersection-free Mapping and\nMatrix M ultiplication). We address the above problems as follows:\nâ€¢Intersection-free partitioning : We propose a novel partitioning\nmethod that divides matrix Sğ‘¦Ã—ğ‘§into subsets of rows based on\nthe density of ğ‘§ğ‘—rows and then chooses different evaluation\nstrategies for the dense rows and the sparse rows. Since the\nresults(ğ‘¥ğ‘–,ğ‘§ğ‘—)of the two parts have different ğ‘§ğ‘—s, this method\nis guaranteed to be intersection-free. Hence, DIM3completely\nremoves the final deduplication step required by DHK.â€¢Optimized mapping design : We investigate the design of the\nmapping step in DIM3. First, we identify cases (e.g., auto in-\ncrement, dictionary encoding) where columns contain roughly\nnatural numbers and thus the mapping step can be skipped. Sec-\nond, we exploit the mapping of the shared join key columns to\nperform a semi-join like optimization to discard tuples with-\nout matches. Finally, we design a cache-optimized hash-based\nalgorithm to efficiently compute the mappings.\nâ€¢Optimized MM algorithms : We obtain and evaluate an imple-\nmentation of the sub-cubic Strassen algorithm [ 40] in a re-\ncent study [ 3]. However, our results show that it cannot beat\nIntel MKL (cf. Section 5.2). Therefore, we focus on O(ğ‘›3)\nalgorithms in DIM3. For the dense rows, we observe that the\ncomputation Cğ‘¥ğ‘–,ğ‘§ğ‘—=Ã|ğ‘Œ|\nğ‘˜=1Rğ‘¥ğ‘–,ğ‘¦ğ‘˜Sğ‘¦ğ‘˜,ğ‘§ğ‘—can stop early as soon\nas there is a non-zero Rğ‘¥ğ‘–,ğ‘¦ğ‘˜Sğ‘¦ğ‘˜,ğ‘§ğ‘—. We design the DenseEC\n(Dense MM with Early stopping Checking) algorithm. For\nthe sparse rows, we design the SparseBMM (Sparse Boolean\nMM) algorithm that leverages the CSR (Compressed Sparse\nRow) [ 22] format of matrix Sğ‘¦Ã—ğ‘§as a hash table on the join key\nğ‘¦with NO hash conflicts. We also introduce a way to reduce\nthe cost for initializing the deduplication vector.\nIn addition to addressing the three problems, we extend the Join-\nProject solution in the following two directions:\nâ€¢Partial Result Caching : We observe that in real-world data sets,\ncomputing results for different ğ‘§ğ‘—values often take widely\ndifferent amounts of time. This motivates us to investigate\nif caching results for a subset of ğ‘§ğ‘—values is profitable. We\nconsider caching either the original or the complement set\nofğ‘¥â€™s given ağ‘§ğ‘—. The caching decision for ğ‘§ğ‘—is based on a\nscore computed from the computation time and the required\nresult space. Our experiments show that partial result caching\ncan effectively reduce the Join-Project computation time with\nreasonable space cost for most data sets.\nâ€¢Support for Join-Op Queries : We discuss different types of re-\nlational operations ğ‘œğ‘, and investigate whether we can lever-\nage the Join-Project algorithm to support Join- ğ‘œğ‘queries. We\nstudy two interesting ğ‘œğ‘s in depth: 1) For Join-Aggregate\nqueries [ 18], we show that DIM3can be applied with simple\nmodifications; and 2) For MJP (Multi-Way Joins with Projec-\ntion) queries, a.k.a. conjunctive queries with projection [ 11],\nwe develop a dynamic programming algorithm to find the op-\ntimal query plan that considers pushing down deduplication\noperations to after the join operations.\n1.3 Contributions\nThe main contributions of this paper is threefold. First, we propose\nDIM3with intersection-free partitioning, optimized mapping, and\nDenseEC and SparseBMM algorithms to address the three problems\nin the state-of-the-art solution (cf. Section 2). Second, we propose\npartial result caching for the Join-Project algorithm (cf. Section 3)\nand generalize DIM3to support Join- ğ‘œğ‘queries (cf. Section 4).\nThird, we perform extensive experimental evaluation using both real-\nworld and synthetic data sets (cf. Section 5). Experimental results\nshow that DIM3outperforms previous Join-Project solutions by\na factor of 2.3Ã—-18Ã—. Compared to commercial and open-source\nRDBMSs, DIM3achieves orders of magnitude speedups.\n2\n\n2 DIM3FOR JOIN-PROJECT\nIn this section, we first overview the DIM3algorithm in Section 2.1.\nThen, we explain the components of the algorithm in detail. Specif-\nically, we present the mapping phase in Section 2.2, intersection-\nfree partitioning in Section 2.3, SparseBMM in Section 2.4, and\nDenseEC in Section 2.5. We analyze the algorithm and describe\nthe strategy selection criteria in Section 2.6. Finally, we analyze the\ntime complexities of the solutions in Section 2.7.\n2.1 Overview\nThe DIM3algorithm is depicted in Figure 2 and listed in Algorithm 1.\nWe perform the Join-Project operation on two tables ğ‘…(ğ‘¥,ğ‘¦)and\nğ‘†(ğ‘§,ğ‘¦). Without loss of generality, suppose ğ‘…is the larger table and\nğ‘†is the smaller table.\nDIM3begins by selecting either the classical or the hybrid solu-\ntion for evaluating the Join-Project operation. This allows completely\navoiding the mapping step, which can have significant cost for highly\nsparse data. DHK [ 10] uses a rule-of-thumb condition to choose the\nclassical solution. In comparison, our strategy selection function\nğ‘“1makes better decisions based on the estimated run times of the\nclassical and the hybrid solutions (cf. Section 2.6 and 5.2).\nIn the hybrid strategy, the first step is to map columns ğ‘¥,ğ‘¦, and\nğ‘§to consecutive natural numbers. In this way, an (ğ‘¥,ğ‘¦)tuple in\nğ‘…(similarly(ğ‘¦,ğ‘§)tuple inğ‘†) can be converted to an element at\nrowğ‘¥and column ğ‘¦in matrix Rğ‘¥Ã—ğ‘¦. DHK [ 10] makes the over-\nsimplified assumption that the input columns contain consecutive\nnatural numbers. We delve into the design of the mapping step to\nprovide general-purpose support for all attribute types.\nThe second step converts table ğ‘…to CSR [ 22] format, then parti-\ntionsğ‘†toğ‘†ğ‘ ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’ andğ‘†ğ‘‘ğ‘’ğ‘›ğ‘ ğ‘’ . We propose an intersection-free parti-\ntion method so that the Join-Project results of ğ‘†ğ‘ ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’ andğ‘†ğ‘‘ğ‘’ğ‘›ğ‘ ğ‘’ can\nbe simply combined without the final deduplication step required by\nDHK [ 10]. The partition method uses function ğ‘“2to decide which ğ‘§\nrow is dense. ğ‘“2will be described in Section 2.6.\nThe third step designs SparseBMM and DenseEC algorithms for\nprocessing the sparse and dense data, respectively. SparseBMM com-\nputes the Join-Project ğ‘…ğ‘’ğ‘ ğ‘¢ğ‘™ğ‘¡ğ‘ ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’ onğ‘…ğ¶ğ‘†ğ‘…andğ‘†ğ‘ ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’ . DenseEC\nmultipliesğ‘…ğ¶ğ‘†ğ‘… withğ‘†ğ‘‘ğ‘’ğ‘›ğ‘ ğ‘’ to obtainğ‘…ğ‘’ğ‘ ğ‘¢ğ‘™ğ‘¡ğ‘‘ğ‘’ğ‘›ğ‘ ğ‘’ . In DenseEC,\nfunctionğ‘“3is used to choose the best method for intersecting two\nbitmaps. The computation of ğ‘“3will be discussed in Section 2.6.\nThe final step is to merge ğ‘…ğ‘’ğ‘ ğ‘¢ğ‘™ğ‘¡ğ‘€ğ‘€andğ‘…ğ‘’ğ‘ ğ‘¢ğ‘™ğ‘¡ğ¸ğ¶to obtain the\nfinal results. Because of intersection-free partitioning, there is no\nneed to perform an extra deduplication step.\n2.2 Mapping\nThe mapping step maps columns ğ‘¥,ğ‘¦, andğ‘§to consecutive natural\nnumbers. This can be achieved with a baseline hash-based algorithm.\nGiven a column, the algorithm looks up the values of the column\none by one in a hash table. If a value ğ‘£does not exist in the hash\ntable, it inserts ( ğ‘£, the next consecutive number) into the hash table.\nAs a result, every distinct value in the column is assigned a natural\nnumber. We can repeat this algorithm for ğ‘¥,ğ‘¦, andğ‘§.\nThis mapping algorithm can be costly. It creates three hash tables\nforğ‘¥,ğ‘¦, andğ‘§, and performs up to 2(|ğ‘…|+|ğ‘†|)hash table lookups\nand/or inserts. Compared to the hash join of ğ‘…(ğ‘¥,ğ‘¦)andğ‘†(ğ‘§,ğ‘¦),\nwhich performs|ğ‘…|+|ğ‘†|hash table accesses, the mapping algorithm\nFigure 2: DIM3for Join-Project.\nAlgorithm 1: DIM3\nInput: Tableğ‘…(ğ‘¥,ğ‘¦)and Tableğ‘†(ğ‘§,ğ‘¦)\nOutput: List of result tuples (ğ‘¥,ğ‘§)\n1Estimateğ‘‚ğ‘ˆğ‘‡ğ½;\n2ifğ‘“1(|ğ‘…|,|ğ‘†|,|ğ‘‚ğ‘ˆğ‘‡ğ½|)>0then /* Classical is better */\n3 return Use classical solution;\n4Mapping column values to consecutive natural numbers;\n5ğ‘…ğ¶ğ‘†ğ‘…â†Create CSR for ğ‘…(ğ‘¥,ğ‘¦);\n6Intersection-free partition ğ‘†byğ‘†.ğ‘§; /* dense ifğ‘“2(ğ‘§)>0 */\n7ğ‘†ğ‘ ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’â†Save the sparse part of ğ‘†as CSR;\n8ğ‘†ğ‘‘ğ‘’ğ‘›ğ‘ ğ‘’â†Save the dense part of ğ‘†as Bitmap array;\n9ğ‘…ğ‘’ğ‘ ğ‘¢ğ‘™ğ‘¡ğ‘ ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’â†SparseBMM( ğ‘…ğ¶ğ‘†ğ‘…,ğ‘†ğ‘ ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’ );\n10ğ‘…ğ‘’ğ‘ ğ‘¢ğ‘™ğ‘¡ğ‘‘ğ‘’ğ‘›ğ‘ ğ‘’â†DenseEC(ğ‘…ğ¶ğ‘†ğ‘…,ğ‘†ğ‘‘ğ‘’ğ‘›ğ‘ ğ‘’ );\n11returnğ‘…ğ‘’ğ‘ ğ‘¢ğ‘™ğ‘¡ğ‘ ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’âˆªğ‘…ğ‘’ğ‘ ğ‘¢ğ‘™ğ‘¡ğ‘‘ğ‘’ğ‘›ğ‘ ğ‘’ ;\npays twice as much cost for hash table visits. This can be a significant\nadditional overhead for the Join-Project algorithm when the join\nresult size is not much larger than the input sizes.\nIn the following, we consider three opportunities to optimize the\nbaseline algorithm. Then, we extend the mapping to support a wider\nrange of Join-Project operations.\nOptimization 1: Skip Mapping. First of all, DIM3chooses the clas-\nsical solution for highly sparse data sets as shown in Figure 2. This\ncompletely avoids the mapping step. Second, it is possible to skip\nmapping for columns that already contain natural numbers. There are\ntwo common cases in database systems. (i) Columns declared with\nauto increment (e.g., AUTO_INCREMENT in Oracle, IDENTITY in\nSQLServer and DB2, SERIAL in PostgreSQL, AUTOINCREMENT\nin MySQL) contain consecutive natural numbers. (ii) String columns\ncan be encoded by dictionary encoding [ 21] and stored as natural\nnumbers in database systems (e.g., SAP HANA and MonetDB).\nOptimization 2: Reduce Computation with Join Key Mapping.\nThe naÃ¯ve way to map ğ‘¦is to useğ‘….ğ‘¦âˆªğ‘†.ğ‘¦as the mapping input.\nWe observe that only ğ‘….ğ‘¦âˆ©ğ‘†.ğ‘¦contributes to the equality join re-\nsults. Therefore, we can employ a semi-join like idea, and optimize\nthe mapping of ğ‘¦as follows. Since ğ‘†is the smaller table, we first\ncompute the mapping of ğ‘†.ğ‘¦using a hash table. Then we map tuples\ninğ‘…using the same hash table. If ğ‘¦ğ‘˜âˆˆğ‘….ğ‘¦butğ‘¦ğ‘˜âˆ‰ğ‘†.ğ‘¦, then the\ncorresponding ğ‘…tuple can be safely discarded because it does not\nhave any matches in ğ‘†. Note that we choose not to pay the cost of\nre-scanning ğ‘†to removeğ‘†tuples with ğ‘¦ğ‘˜âˆˆğ‘†.ğ‘¦butğ‘¦ğ‘˜âˆ‰ğ‘….ğ‘¦. As\nshown in Figure 2, the first and third ğ‘…tuples are filtered out.\n3\n\nFigure 3: Comparing the partition methods in DHK [10] and\nDIM3. (A tuple in a table is displayed as an edge in graphs and\nan element in matrices. Red: dense, blue: sparse)\nOptimization 3: Optimize Hash Table Performance. When a hash\ntable is larger than the CPU cache, hash table accesses result in ex-\npensive random memory accesses with poor CPU cache behavior.\nA hash table visit may probe multiple locations, and dereference\npointers (e.g., in the case of chained hash table), incurring significant\noverhead. Therefore, we employ the following designs to improve\nthe hash table performance in the mapping algorithm. First, we es-\ntimate the hash table size for a column (e.g., based on statistics\nof the number of distinct keys). If the size exceeds the last-level\nCPU cache, we employ cache partitioning. We use the last ğ‘˜bits of\nthe hash value to divide the data into 2ğ‘˜partitions so that the hash\ntable of each partition fits into the last-level CPU cache. Then, we\ncompute the mapping for each partition. Second, we employ a linear\nprobing hash table design to avoid pointer dereference. We tune the\nnumber of slots and the maximum probing distance to reduce the\ncost of hash table accesses. If no available slots are found for a given\ncolumn value, we employ a stash hash table (Flat_hash_map [ 39] in\nour implementation) to store the overflow data.\nSupporting Wider Range of Join-Project Operations. We can\nmap not only single attribute but also multiple attributes to consecu-\ntive numbers. For instance,\nÎ ğ‘,ğ‘,ğ‘,ğ‘‘,ğ‘’(ğ‘…(ğ‘,ğ‘,ğ‘,ğ‘‘)Zğ‘,ğ‘‘ğ‘†(ğ‘,ğ‘‘,ğ‘’,ğ‘“))\ncan be treated as\nÎ ğ‘¥,ğ‘§(ğ‘…(ğ‘¥,ğ‘¦)Zğ‘¦ğ‘†(ğ‘§,ğ‘¦))\nwhereğ‘….ğ‘¥={ğ‘,ğ‘},ğ‘†.ğ‘§={ğ‘,ğ‘‘,ğ‘’}, andğ‘….ğ‘¦=ğ‘†.ğ‘¦={ğ‘,ğ‘‘}. In this\nway, we can support any combinations of join keys and projection\ncolumns, including operations that contain the join key in the output.\n2.3 Intersection-Free Partitioning\nFigure 3 compares the partition methods of DHK [ 10] and DIM3.\nDHK makes separate decisions on ğ‘¥,ğ‘¦andğ‘§according to their\ndegrees (i.e. the number of tuples of the same attribute value). An (ğ‘¥,\nğ‘¦)tuple is added to the dense part only if both its ğ‘¥andğ‘¦attributes\nare considered as dense (shown as red color in the figure).\nIn comparison, we propose an intersection-free partition method1\nas shown in Figure 3. It examines table ğ‘†and ensure that all tuples\nwith the same ğ‘†.ğ‘§value are in the same partition. Note that table\nğ‘…will not be partitioned. Essentially, we partition matrix ğ‘†by its\ncolumns. Our partitioning method has the following benefits.\nFirst, the Join-Project results generated by the sparse and dense\nparts do not intersect. Given a result (ğ‘¥,ğ‘§), ifğ‘§is judged as dense,\n1Previous work proposes an intersection-free partition method in the context of query\nenumeration algorithms [ 11]. It divides the data into two sets with roughly equal number\nof join results. The method optimizes enumeration delays rather than end-to-end query\nrun times. It is not directly applicable to the Join-Project operation.this result must be generated by the dense part. Otherwise, it is\ngenerated by the sparse part. In contrast, this property does not hold\nin DHK. As shown in Figure 3, the result (ğ‘¥2,ğ‘§2)is generated both\nin the dense part by joining (ğ‘¥2,ğ‘¦1)and(ğ‘¦1,ğ‘§2), and in the sparse\npart by joining(ğ‘¥2,ğ‘¦3)and(ğ‘¦3,ğ‘§2). Therefore, while DHK must\ndeduplicate the results from the two parts, DIM3can eliminate this\nfinal deduplication step.\nSecond, DIM3may apply dense MM to more tuples. DHK con-\nsiders a tuple as dense only if both its attributes are judged as dense.\nIn comparison, DIM3makes the partitioning decision based solely\nonğ‘†.ğ‘§. Since its dense criteria tend to be more flexible, DIM3can\nemploy dense MM under more scenarios, as illustrated in Figure 3.\nThird, the selection based on ğ‘§simplifies the Join-Project com-\nputation. In DHK, it is costly (spatially) to record the per-tuple\ndensity decisions. Therefore, DHK does not save them. Instead,\nwhen processing the sparse part, DHK uses the degree thresholds\nto re-compute whether a tuple is dense and should be skipped. In\ncomparison, DIM3avoids this complexity. As shown in Figure 2,\ntableğ‘†is divided into the sparse and the dense parts based on ğ‘†.ğ‘§.\nHence, there is no need to re-evaluate the density criteria any more.\nFinally, the cost for computing the partition decision in DIM3\nis lower compared to DHK. DHK makes |ğ‘…|+|ğ‘†|decisions on all\ninput tuples. In comparison, DIM3makes|ğ‘|decisions on ğ‘†.ğ‘§. The\nnumber of decisions to make is much smaller. Consider the case\nwhere|ğ‘‹|=|ğ‘Œ|=|ğ‘|=ğ‘›. In the worst case, the cost is O(ğ‘›2)in DHK,\nbut onlyO(ğ‘›)in DIM3. Moreover, DHK performs binary search to\ndetermine the density threshold, which incurs additional cost.\n2.4 SparseBMM\nThe classical hash-based solution is often used to process the sparse\npart of the data. We propose a SparseBMM algorithm with two main\noptimization techniques, as shown in Algorithm 2.\nFirst, we observe that hash table accesses are often one main\ncost of the classical hash-based computation. Interestingly, since the\ncolumn values are mapped to natural numbers, the CSR (Compressed\nSparse Row) [ 22] format of matrix Sğ‘¦Ã—ğ‘§is essentially a hash table\non the join key ğ‘¦with NO hash conflicts. The original CSR structure\nconsists of three arrays: ğ‘‰ğ‘ğ‘™[],ğ¶ğ‘œğ‘™[], andğ‘…ğ‘œğ‘¤ğ‘ƒğ‘¡ğ‘Ÿ[].ğ‘‰ğ‘ğ‘™[]and\nğ¶ğ‘œğ‘™[]contain the value and the column index of non-zero elements\nin the matrix, respectively. ğ‘…ğ‘œğ‘¤ğ‘ƒğ‘¡ğ‘Ÿ[]points to the row starts in ğ‘‰ğ‘ğ‘™[]\nandğ¶ğ‘œğ‘™[]. In the case of Join-Project, ğ‘‰ğ‘ğ‘™[]contains all 1â€™s and\ncan be omitted. Therefore, we have two arrays ğ¶ğ‘œğ‘™[]andğ‘…ğ‘œğ‘¤ğ‘ƒğ‘¡ğ‘Ÿ[].\nWe employ matrix Sğ‘¦Ã—ğ‘§as the hash table. Given ğ‘¦ğ‘˜, we locate\nall non-zero element Sğ‘¦ğ‘˜,ğ‘§ğ‘—by visiting entries ğ¶ğ‘œğ‘™[ğ‘…ğ‘œğ‘¤ğ‘ƒğ‘¡ğ‘Ÿ[ğ‘¦ğ‘˜]]..\nğ¶ğ‘œğ‘™[ğ‘…ğ‘œğ‘¤ğ‘ƒğ‘¡ğ‘Ÿ[ğ‘¦ğ‘˜+1]-1]. In fact, ğ‘…ğ‘œğ‘¤ğ‘ƒğ‘¡ğ‘Ÿ serves as the hash bucket\nheader, and ğ¶ğ‘œğ‘™[ğ‘…ğ‘œğ‘¤ğ‘ƒğ‘¡ğ‘Ÿ[ğ‘¦ğ‘˜]]..ğ¶ğ‘œğ‘™[ğ‘…ğ‘œğ‘¤ğ‘ƒğ‘¡ğ‘Ÿ[ğ‘¦ğ‘˜+1]-1] contain the\nhash entries in bucket ğ‘¦ğ‘˜. In this way, we avoid the hash function\ncomputation and hash conflicts in common hash table designs.\nSecond, DHK performs deduplication using a ğ‘§-vector. For each\nğ‘¥ğ‘–, it initializes the ğ‘§-vector to all zeros. Then, it checks all (ğ‘¥ğ‘–,\nğ‘¦ğ‘˜)s to compute the join results. For every result (ğ‘¥ğ‘–,ğ‘§ğ‘—), DHK\nincrements the corresponding element in the ğ‘§-vector. Hence, the\nnon-zero elements in the ğ‘§-vector indicate the deduplicated Join-\nProject results. However, the initialization cost is O(|ğ‘|), while the\nnumber of non-zero ğ‘§â€™s can be small for the sparse part of the data.\nConsequently, the initialization of the ğ‘§-vector is often a main cost of\n4\n\nAlgorithm 2: SparseBMM (for sparse data).\nInput: CSR-storedğ‘…ğ¶ğ‘†ğ‘… and CSR-stored ğ‘†ğ‘ ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’\nOutput: List of result tuples (ğ‘¥,ğ‘§)\n1SPA[0..|ğ‘|-1]=âˆ’1;\n2forğ‘ğ‘¢ğ‘Ÿğ‘¥â†0to|ğ‘‹|do\n3 foreachğ‘ğ‘¢ğ‘Ÿğ‘¦related toğ‘ğ‘¢ğ‘Ÿğ‘¥inğ‘…ğ¶ğ‘†ğ‘… do\n4 foreachğ‘ğ‘¢ğ‘Ÿğ‘§related toğ‘ğ‘¢ğ‘Ÿğ‘¦inğ‘†ğ‘ ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’ do\n5 ifSPA[ğ‘ğ‘¢ğ‘Ÿğ‘§]!=ğ‘ğ‘¢ğ‘Ÿğ‘¥then\n6 SPA[ğ‘ğ‘¢ğ‘Ÿğ‘§]=ğ‘ğ‘¢ğ‘Ÿğ‘¥;\n7 Result.append((ğ‘ğ‘¢ğ‘Ÿğ‘¥,ğ‘ğ‘¢ğ‘Ÿğ‘§));\n8return Result;\nthe deduplication computation. We remove this per- ğ‘¥initialization\ncost with a monotonically increasing flag for different ğ‘¥. As shown\nin Algorithm 2, the ğ‘†ğ‘ƒğ´ array is the ğ‘§-vector. We initialize the\nğ‘†ğ‘ƒğ´ array only once before any computation. The ğ‘ğ‘¢ğ‘Ÿğ‘¦andğ‘ğ‘¢ğ‘Ÿğ‘§\nloops (Line 3â€“7) compute the join results for the given ğ‘ğ‘¢ğ‘Ÿğ‘¥. For a\nnewly computed join result (ğ‘ğ‘¢ğ‘Ÿğ‘¥,ğ‘ğ‘¢ğ‘Ÿğ‘§), we setğ‘†ğ‘ƒğ´[ğ‘ğ‘¢ğ‘Ÿğ‘§] toğ‘ğ‘¢ğ‘Ÿğ‘¥.\nIf there are multiple duplicate (ğ‘ğ‘¢ğ‘Ÿğ‘¥,ğ‘ğ‘¢ğ‘Ÿğ‘§),ğ‘†ğ‘ƒğ´[ğ‘ğ‘¢ğ‘Ÿğ‘§] is set only\nonce for the first instance. In this way, for the next ğ‘ğ‘¢ğ‘Ÿğ‘¥+1, the\nprevious content of ğ‘†ğ‘ƒğ´ isautomatically invalid. This saves the cost\nof initializing ğ‘†ğ‘ƒğ´ in everyğ‘ğ‘¢ğ‘Ÿğ‘¥loop iteration.\nWe consider the time and space complexity of SparseBMM.\nLine 5 of Algorithm 2 runs |ğ‘‚ğ‘ˆğ‘‡ğ½|times. Thus, the time complexity\nisÎ˜(|ğ‘…|+|ğ‘†|+|ğ‘‚ğ‘ˆğ‘‡ğ½|). While this is the same as the classical hash-\nbased solution, SparseBMM reduces the constant factor, accelerating\nhash table visits and deduplication. Moreover, SparseBMM requires\nÎ˜(|ğ‘…|+|ğ‘†|+|ğ‘|)space if the final output is consumed by upper\nlevel operators. While hash-based deduplication requires Î˜(|ğ‘‚ğ‘ˆğ‘‡ğ‘ƒ|)\nspace for the hash table, SparseBMM allocates only Î˜(|ğ‘|)space\nforğ‘†ğ‘ƒğ´, which is often much smaller than Î˜(|ğ‘‚ğ‘ˆğ‘‡ğ‘ƒ|).\n2.5 DenseEC\nTo compute Cğ‘¥ğ‘–,ğ‘§ğ‘—=Ã|ğ‘Œ|\nğ‘˜=1Rğ‘¥ğ‘–,ğ‘¦ğ‘˜Sğ‘¦ğ‘˜,ğ‘§ğ‘—, standard dense MM enu-\nmerates all pairs of Rğ‘¥ğ‘–,ğ‘¦ğ‘˜andSğ‘¦ğ‘˜,ğ‘§ğ‘—. We observe that the compu-\ntation can stop early as soon as there is a non-zero Rğ‘¥ğ‘–,ğ‘¦ğ‘˜Sğ‘¦ğ‘˜,ğ‘§ğ‘—.\nTherefore, we propose a DenseEC algorithm to leverage this obser-\nvation, as listed in Algorithm 3.\nIn Algorithm 3, the first two for-loops enumerate all the pairs of\nğ‘….ğ‘¥andğ‘†.ğ‘§. Line 4â€“11 use one of two methods to check if there\nis any common ğ‘¦betweenğµğ‘–ğ‘¡ğ‘šğ‘ğ‘ğ‘¥andğµğ‘–ğ‘¡ğ‘šğ‘ğ‘ğ‘§. The first method\nuses SIMD to compute the bit-wise AND ofğµğ‘–ğ‘¡ğ‘šğ‘ğ‘ğ‘¥andğµğ‘–ğ‘¡ğ‘šğ‘ğ‘ğ‘§\n(e.g., using _mm256_testz_si256). The second method examines\neachğ‘¦related toğ‘ğ‘¢ğ‘Ÿğ‘¥inğµğ‘–ğ‘¡ğ‘šğ‘ğ‘ğ‘§using a random memory access. If\nany common ğ‘¦is found, both methods stop early. Generally speaking,\nthe SIMD method performs better when the ğ‘ğ‘¢ğ‘Ÿğ‘¥row has a large\nnumber ofğ‘¦. We determine which method to use with function ğ‘“3,\nwhich will be discussed in Section 2.6.\nApart from the time saving due to early stopping, DenseEC saves\nmemory space compared to dense MM. DenseEC represents every\nelement as a single bit rather than a 4-byte integer or floating point\nvalue in BLAS packages. Moreover, a dense MM invocation would\nrequire space to save the temporary output matrix. In comparison,\nDenseEC never generates the output matrix.Algorithm 3: DenseEC (for dense data).\nInput: CSR-storedğ‘…ğ¶ğ‘†ğ‘… and bitmap array ğ‘†ğ‘‘ğ‘’ğ‘›ğ‘ ğ‘’\nOutput: List of result tuples (ğ‘¥,ğ‘§)\n1forğ‘ğ‘¢ğ‘Ÿğ‘¥â†0to|ğ‘‹|do\n2ğµğ‘–ğ‘¡ğ‘šğ‘ğ‘ğ‘¥â†Save theğ‘¦in theğ‘ğ‘¢ğ‘Ÿğ‘¥row as bitmap;\n3 foreachğ‘ğ‘¢ğ‘Ÿğ‘§inğ‘†ğ‘‘ğ‘’ğ‘›ğ‘ ğ‘’ do\n4 ifğ‘“3(ğ‘šğ‘¥,ğ‘šğ‘§,|ğ‘Œ|)>0then /* SIMD is better */\n5 ifSIMD_AND( ğµğ‘–ğ‘¡ğ‘šğ‘ğ‘ğ‘¥,ğµğ‘–ğ‘¡ğ‘šğ‘ğ‘ğ‘§) not all 0 then\n6 Result.append((ğ‘ğ‘¢ğ‘Ÿğ‘¥,ğ‘ğ‘¢ğ‘Ÿğ‘§));\n7 else\n8 foreachğ‘¦related toğ‘ğ‘¢ğ‘Ÿğ‘¥inğ‘…ğ¶ğ‘†ğ‘… do\n9 iftheğ‘¦th bit inğµğ‘–ğ‘¡ğ‘šğ‘ğ‘ğ‘§== 1 then\n10 Result.append((ğ‘ğ‘¢ğ‘Ÿğ‘¥,ğ‘ğ‘¢ğ‘Ÿğ‘§));\n11 Break;\n12return Result;\n2.6 Evaluation Path Section Functions\nWe compute the three functions used in the DIM3algorithm to select\ndifferent evaluation paths. Table 1 lists the symbols used in this\nsubsection. All the listed parameters can be measured in advance.\nStrategy Selection ( ğ‘“1).DHK chooses the classical solution using\na rule-of-thumb condition: |ğ‘‚ğ‘ˆğ‘‡ğ½| â‰¤ 20Â·ğ‘, whereğ‘=|ğ‘…|=\n|ğ‘†|. In comparison, DIM3determines whether to use the classical\nsolution or the hybrid solution with function ğ‘“1. As the classical\nsolution is beneficial only when the data is sparse, we compare\nmapping+SparseBMM and the classical solution to compute ğ‘“1.\nWhenğ‘“1>0, the classical solution is faster and will be chosen.\nğ‘“1(|ğ‘…|,|ğ‘†|,|ğ‘‚ğ‘ˆğ‘‡ğ½|)=(ğ‘‡ğ‘šğ‘ğ‘ğ‘ğ‘–ğ‘›ğ‘”+ğ‘‡ğ‘†ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’ğµğ‘€ğ‘€)âˆ’(ğ‘‡ğ‘—ğ‘œğ‘–ğ‘›+ğ‘‡ğ‘‘ğ‘’ğ‘‘ğ‘¢ğ‘)\n=ğ‘‡ğ‘šğ‘ğ‘ğ‘ğ‘–ğ‘›ğ‘”+(ğ‘‡ğ‘†ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’ğµğ‘€ğ‘€âˆ’ğ‘‡ğ‘—ğ‘œğ‘–ğ‘›)âˆ’ğ‘‡ğ‘‘ğ‘’ğ‘‘ğ‘¢ğ‘\nâ‰ˆ2(|ğ‘…|+|ğ‘†|)ğ‘¡ğ‘šğ‘ğ‘+|ğ‘‚ğ‘ˆğ‘‡ğ½|ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘‘ğ‘…ğ‘Šâˆ’|ğ‘‚ğ‘ˆğ‘‡ğ½|ğ‘¡â„ğ‘ğ‘ â„ (2)\nHere,ğ‘‡ğ‘šğ‘ğ‘ğ‘ğ‘–ğ‘›ğ‘” =2(|ğ‘…|+|ğ‘†|)ğ‘¡ğ‘šğ‘ğ‘ from Section 2.2. To estimate\nğ‘‡ğ‘†ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’ğµğ‘€ğ‘€âˆ’ğ‘‡ğ‘—ğ‘œğ‘–ğ‘›, we see that generating join results with CSR\nin SparseBMM has smaller or similar cost compared to hash joins.\nHence, the difference is mainly the deduplication cost of SparseBMM.\nThis is|ğ‘‚ğ‘ˆğ‘‡ğ½|âˆ—ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘‘ğ‘…ğ‘Š because SparseBMM performs a ran-\ndom access to the ğ‘†ğ‘ƒğ´ array per join result. Finally, ğ‘‡ğ‘‘ğ‘’ğ‘‘ğ‘¢ğ‘ =\n|ğ‘‚ğ‘ˆğ‘‡ğ½|ğ‘¡â„ğ‘ğ‘ â„ because the classical solution performs a hash table\naccess for deduplicating every join output tuple.\nBitmap Comparison in DenseEC ( ğ‘“3).Line 4 of Algorithm 3 uses\nfunctionğ‘“3to choose the SIMD or non-SIMD method for comparing\nğµğ‘–ğ‘¡ğ‘šğ‘ğ‘ğ‘¥andğµğ‘–ğ‘¡ğ‘šğ‘ğ‘ğ‘§. The bitmaps have|ğ‘Œ|bits. Suppose there are\nğ‘šğ‘¥andğ‘šğ‘§1â€™s inğµğ‘–ğ‘¡ğ‘šğ‘ğ‘ğ‘¥andğµğ‘–ğ‘¡ğ‘šğ‘ğ‘ğ‘§, respectively.\nIn the non-SIMD method, the comparison stops as soon as a check\nhits a set bit in ğµğ‘–ğ‘¡ğ‘šğ‘ğ‘ğ‘§. The probability that the check hits a set bit\nisğ‘ğ‘ =ğ‘šğ‘§\n|ğ‘Œ|. The number of checks follows a geometric distribution\nTable 1: Symbols used in Section 2.6.\nSymbol Description\nğ‘¡ğ‘ ğ‘’ğ‘ğ‘… time for sequential memory read\nğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘‘ğ‘… time for random memory read\nğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘‘ğ‘…ğ‘Š time for random memory read-modify-write\nğ‘¡â„ğ‘ğ‘ â„ time of a lookup or insertion to plain hash table\nğ‘¡ğ‘šğ‘ğ‘ time to access the optimized hash table for mapping\nğ‘¡ğ¸ğ¶ğ‘  time of a non-SIMD comparison in DenseEC\nğ‘¡ğ¸ğ¶ğ‘‘ time of a SIMD comparison in DenseEC\n5\n\n1 2 3 4 5 6\nJoin result size (|OUTJ|) 107100101102Running time (s)Classical\nMapping+SparseBMM\nClassical(estimated)\nMapping+SparseBMM(estimated)(a)ğ‘“1for strategy selection\n0 0.5 1 1.5 2 2.5\n10-3100101102\nSIMD\nnonSIMD\nSIMD(estimated)\nnonSIMD(estimated) (b)ğ‘“3for selecting bitmap comparison method\n0 0.1 0.2 0.3 0.4 0.5102103104105\nDense\nSparse\nDense(estimated)\nSparse(estimated) (c)ğ‘“2for deciding column density\nFigure 4: Effectiveness of the three functions for choosing evaluation paths in DIM3.\nwith probability ğ‘ğ‘ , and the method performs at most ğ‘šğ‘¥checks.\nHence, the expected number of checks is calculated as:\nğ¶â„ğ‘’ğ‘ğ‘˜ğ‘›ğ‘œğ‘›ğ‘ ğ‘–ğ‘šğ‘‘ =ğ‘šğ‘¥âˆ‘ï¸\nğ‘–=1ğ‘–(1âˆ’ğ‘ğ‘ )ğ‘–âˆ’1ğ‘ğ‘ +ğ‘šğ‘¥(1âˆ’ğ‘ğ‘ )ğ‘šğ‘¥=1âˆ’(1âˆ’ğ‘ğ‘ )ğ‘šğ‘¥\nğ‘ğ‘ \nIn the SIMD method, every SIMD comparison checks 256 bits\nin the two bitmaps. When the bits at the same position in the two\nbitmaps are both set, the comparison returns true and the process\nstops. The probability that an SIMD comparison returns true is\nğ‘ğ‘‘=1âˆ’(1âˆ’ğ‘šğ‘¥ğ‘šğ‘§\n|ğ‘Œ|2)256. The number of SIMD checks follows a\ngeometric distribution with probability ğ‘ğ‘‘, and the method performs\nup to|ğ‘Œ|\n256checks. Hence, the expected ğ¶â„ğ‘’ğ‘ğ‘˜ğ‘ ğ‘–ğ‘šğ‘‘=1âˆ’(1âˆ’ğ‘ğ‘‘)|ğ‘Œ|\n256\nğ‘ğ‘‘.\nThen, we can compute ğ‘“3as follows. When ğ‘“3>0, the SIMD\nmethod is preferred. Otherwise, the non-SIMD method is selected.\nğ‘“3(ğ‘šğ‘¥,ğ‘šğ‘§,|ğ‘Œ|)=ğ¶â„ğ‘’ğ‘ğ‘˜ğ‘›ğ‘œğ‘›ğ‘ ğ‘–ğ‘šğ‘‘ğ‘¡ğ¸ğ¶ğ‘ âˆ’ğ¶â„ğ‘’ğ‘ğ‘˜ğ‘ ğ‘–ğ‘šğ‘‘ğ‘¡ğ¸ğ¶ğ‘‘ (3)\nWe can reduce the computation overhead of ğ‘“3as follows. Note that\nğ‘šğ‘¥and|ğ‘Œ|are constants in the for-loop at line 3 of Algorithm 3.\nğ‘“3(ğ‘šğ‘¥,ğ‘šğ‘§,|ğ‘Œ|)is monotonically increasing with ğ‘šğ‘§. Therefore, an\noptimization is to use binary search to find the threshold value of\nğ‘šğ‘§ğ‘¡so thatğ‘“3(ğ‘šğ‘¥,ğ‘šğ‘§ğ‘¡,|ğ‘Œ|)=0. Then, we can select the SIMD\nmethod ifğ‘šğ‘§>ğ‘šğ‘§ğ‘¡. Moreover, we find that multiple ğ‘….ğ‘¥â€™s can\nshare the same ğ‘šğ‘¥. Sinceğ‘šğ‘§ğ‘¡is determined for a given ğ‘šğ‘¥, we can\ncache the pairs of ğ‘šğ‘¥andğ‘šğ‘§ğ‘¡, then reuse the computed ğ‘šğ‘§ğ‘¡to avoid\nredundant binary searches. In this way, the time for computing all ğ‘“3\nthresholds is at most 1.15ms for the real-world data sets in Section 5,\nwhich is less than 0.5% of the total run time.\nDense vs. Sparse Partitions ( ğ‘“2).At line 6 in DIM3(Algorithm 1),\nintersection-free partitioning uses function ğ‘“2to determine if ğ‘†.ğ‘§\ncolumn is dense or sparse.\nFor column ğ‘§, the number of join results can be estimated as\nğ‘šğ‘§\n|ğ‘†||ğ‘‚ğ‘ˆğ‘‡ğ½|, whereğ‘šğ‘§is the number of ğ‘†tuples in column ğ‘§. We\ndenote this value as ğ‘‚ğ‘ˆğ‘‡ğ½,ğ‘§. We estimate the cost of processing\ncolumnğ‘§using either SparseBMM and denseEC.\nThe cost of SparseBMM for ğ‘§is computed as follows:\nğ‘‡ğ‘ ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’ğµğ‘€ğ‘€ =(2|ğ‘‹|+|ğ‘…|)ğ‘¡ğ‘ ğ‘’ğ‘ğ‘…+2|ğ‘…|ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘‘ğ‘…\n|ğ‘|+ğ‘‚ğ‘ˆğ‘‡ğ½,ğ‘§(ğ‘¡ğ‘ ğ‘’ğ‘ğ‘…+ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘‘ğ‘…ğ‘Š)\nThe first component computes the cost of the two for-loops at Line 2â€“\n3 amortized to one of |ğ‘|columns. The second component estimates\nthe cost of Line 4â€“7. For DenseEC, we know that the cost for each\npair ofğ‘¥andğ‘§from the above. Then we can sum this up to obtain\nğ‘‡ğ‘‘ğ‘’ğ‘›ğ‘ ğ‘’ğ¸ğ¶ =Ã|ğ‘‹|\nğ‘–=1ğ‘šğ‘–ğ‘›(ğ¶â„ğ‘’ğ‘ğ‘˜ğ‘›ğ‘œğ‘›ğ‘ ğ‘–ğ‘šğ‘‘ğ‘¡ğ¸ğ¶ğ‘ ,ğ¶â„ğ‘’ğ‘ğ‘˜ğ‘ ğ‘–ğ‘šğ‘‘ğ‘¡ğ¸ğ¶ğ‘‘).\nFinally, we can compute ğ‘“2as follows. When ğ‘“2>0, we consider\ncolumnğ‘§as dense.\nğ‘“2=ğ‘‡ğ‘ ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’ğµğ‘€ğ‘€âˆ’ğ‘‡ğ‘‘ğ‘’ğ‘›ğ‘ ğ‘’ğ¸ğ¶ (4)Effectiveness of the Functions. Figure 4(a) compares the measured\nrunning time and the estimated running time of the classical solution\nand mapping+SparseBMM. In this experiment, we generate random\ndata with column values in the range of 0 to 107. We vary|ğ‘…|=|ğ‘†|\nfrom 5 million to 60 million. The X-axis shows the join result size.\nThe Y-axis reports the running time. From the figure, we see that the\nestimated times show consistent trends compared to the measured\ntimes. Function ğ‘“1models the intersection of the curves for the two\nalgorithms. We see that the intersection points of the measured and\nestimated time curves are very close, showing the effectiveness of ğ‘“1\nfor strategy selection.\nFigure 4(b) compares the measured running time and the esti-\nmated running time of the SIMD and non-SIMD methods. From the\nfigure, we see that (i) the estimated values are always smaller than\nthe measured values because certain runtime overheads, such as the\nfor-loop, are not taken into consideration; (ii) the estimated curves\nshow similar trends as the measured curves; and (iii) the intersection\npoints of the two sets of curves are very close.\nFigure 4(c) compares the measured running time and the esti-\nmated running time of SparseBMM and DenseEC. From the figure,\nwe see that (i) the estimated curves show similar trends as the mea-\nsured curves; and (ii) the intersection points of the two sets of curves\nare close. This demonstrates the effectiveness of using function ğ‘“2\nfor intersection-free partitioning.\n2.7 Time Complexity Analysis\nBased on the above discussion in Section 2.6, we have the following:\nğ¶ğ‘œğ‘šğ‘ğ‘™ğ‘’ğ‘¥ğ‘–ğ‘¡ğ‘¦ ğ‘†ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’ğµğ‘€ğ‘€ =Î˜(|ğ‘…|+|ğ‘†|+|ğ‘‚ğ‘ˆğ‘‡ğ½|)\nğ¶ğ‘œğ‘šğ‘ğ‘™ğ‘’ğ‘¥ğ‘–ğ‘¡ğ‘¦ ğ·ğ‘’ğ‘›ğ‘ ğ‘’ğ¸ğ¶ğ‘›ğ‘œğ‘›ğ‘ ğ‘–ğ‘šğ‘‘=Î˜(|ğ‘‹|âˆ‘ï¸\nğ‘–=1|ğ‘|âˆ‘ï¸\nğ‘—=1ğ‘‡ğ¶â„ğ‘’ğ‘ğ‘˜ _ğ‘›ğ‘œğ‘›ğ‘ ğ‘–ğ‘šğ‘‘(ğ‘¥ğ‘–,ğ‘§ğ‘—))\nğ¶ğ‘œğ‘šğ‘ğ‘™ğ‘’ğ‘¥ğ‘–ğ‘¡ğ‘¦ ğ·ğ‘’ğ‘›ğ‘ ğ‘’ğ¸ğ¶ğ‘ ğ‘–ğ‘šğ‘‘=Î˜(|ğ‘‹|âˆ‘ï¸\nğ‘–=1|ğ‘|âˆ‘ï¸\nğ‘—=1ğ‘‡ğ¶â„ğ‘’ğ‘ğ‘˜ _ğ‘ ğ‘–ğ‘šğ‘‘(ğ‘¥ğ‘–,ğ‘§ğ‘—))\nNote thatğ‘‡ğ¶â„ğ‘’ğ‘ğ‘˜ _ğ‘›ğ‘œğ‘ ğ‘–ğ‘šğ‘‘(ğ‘¥ğ‘–,ğ‘§ğ‘—)andğ‘‡ğ¶â„ğ‘’ğ‘ğ‘˜ _ğ‘ ğ‘–ğ‘šğ‘‘(ğ‘¥ğ‘–,ğ‘§ğ‘—)are the ac-\ntual numbers of checks given (ğ‘¥ğ‘–,ğ‘§ğ‘—), which are determined by the\ninput data distribution.\nWorst-Case Costs. We compute the worst-case costs. Suppose that\ninput tables ğ‘…andğ‘†do not have duplicates. Then, for SparseBMM,\nit is clear that|ğ‘‚ğ‘ˆğ‘‡ğ½|<|ğ‘‹||ğ‘Œ||ğ‘|. For DenseEC, early stopping\nnever happens in the worst cases. Hence, ğ‘‡ğ¶â„ğ‘’ğ‘ğ‘˜ _ğ‘›ğ‘œğ‘›ğ‘ ğ‘–ğ‘šğ‘‘(ğ‘¥ğ‘–,ğ‘§ğ‘—)=\nÎ˜(ğ‘šğ‘¥ğ‘–)andğ‘‡ğ¶â„ğ‘’ğ‘ğ‘˜ _ğ‘ ğ‘–ğ‘šğ‘‘(ğ‘¥ğ‘–,ğ‘§ğ‘—)=Î˜(|ğ‘Œ|\nğ‘†ğ¼ğ‘€ğ· _ğ‘ğ‘–ğ‘¡_ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„).\n6\n\nTherefore, the worst case time complexities are as follows:\nğ¶ğ‘œğ‘šğ‘ğ‘™ğ‘’ğ‘¥ğ‘–ğ‘¡ğ‘¦ ğ‘†ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’ğµğ‘€ğ‘€ =O(|ğ‘‹||ğ‘Œ||ğ‘|)\nğ¶ğ‘œğ‘šğ‘ğ‘™ğ‘’ğ‘¥ğ‘–ğ‘¡ğ‘¦ ğ·ğ‘’ğ‘›ğ‘ ğ‘’ğ¸ğ¶ğ‘›ğ‘œğ‘›ğ‘ ğ‘–ğ‘šğ‘‘=O(|ğ‘…||ğ‘|)\nğ¶ğ‘œğ‘šğ‘ğ‘™ğ‘’ğ‘¥ğ‘–ğ‘¡ğ‘¦ ğ·ğ‘’ğ‘›ğ‘ ğ‘’ğ¸ğ¶ğ‘ ğ‘–ğ‘šğ‘‘=O(|ğ‘‹||ğ‘Œ||ğ‘|\nğ‘†ğ¼ğ‘€ğ· _ğ‘ğ‘–ğ‘¡_ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„)\nAverage-Case Costs. It is difficult to compute the precise formula\nfor average-case costs because the time complexities are affected by\nthe path selection decisions and the density of the input tables. In\nthe following, we analyze the costs under a simplifying assumption\nof uniform density distribution in the input tables.\nWe assume that the non-zeros in matrices Rğ‘¥,ğ‘¦andSğ‘¦,ğ‘§are uni-\nformly randomly distributed, and the densities (probability) of non-\nzero values are ğ›¼andğ›½, respectively. Then, we have |ğ‘‚ğ‘ˆğ‘‡ğ½|=\nğ›¼ğ›½|ğ‘‹||ğ‘Œ||ğ‘|,ğ‘šğ‘¥=ğ›¼|ğ‘Œ|andğ‘šğ‘§=ğ›½|ğ‘Œ|. We also assume that\n|ğ‘‚ğ‘ˆğ‘‡ğ½|>ğ‘šğ‘ğ‘¥(|ğ‘…|,|ğ‘†|).\nSince all the rows and columns of the input data are of the same\ndensity,ğ‘“2considers the entire ğ‘†as dense or sparse. Similarly, ğ‘“3\nselects the same execution path for all pairs of (ğ‘¥,ğ‘§). That is, all data\nwill be processed by the same algorithm. We can use the expected\nnumbers of checks ( ğ¶â„ğ‘’ğ‘ğ‘˜ğ‘›ğ‘œğ‘›ğ‘ ğ‘–ğ‘šğ‘‘ andğ¶â„ğ‘’ğ‘ğ‘˜ğ‘ ğ‘–ğ‘šğ‘‘) computed in the\nprevious subsection to estimate ğ‘‡ğ¶â„ğ‘’ğ‘ğ‘˜ _ğ‘›ğ‘œğ‘ ğ‘–ğ‘šğ‘‘ andğ‘‡ğ¶â„ğ‘’ğ‘ğ‘˜ _ğ‘ ğ‘–ğ‘šğ‘‘. The\nabove formula can be rewritten as follows:\nğ¶ğ‘œğ‘šğ‘ğ‘™ğ‘’ğ‘¥ğ‘–ğ‘¡ğ‘¦ ğ‘†ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’ğµğ‘€ğ‘€ =Î˜(ğ›¼ğ›½|ğ‘‹||ğ‘Œ||ğ‘|)\nğ¶ğ‘œğ‘šğ‘ğ‘™ğ‘’ğ‘¥ğ‘–ğ‘¡ğ‘¦ ğ·ğ‘’ğ‘›ğ‘ ğ‘’ğ¸ğ¶ğ‘›ğ‘œğ‘›ğ‘ ğ‘–ğ‘šğ‘‘=Î˜(1âˆ’(1âˆ’ğ›½)ğ›¼|ğ‘Œ|\nğ›½|ğ‘‹||ğ‘|)\nğ¶ğ‘œğ‘šğ‘ğ‘™ğ‘’ğ‘¥ğ‘–ğ‘¡ğ‘¦ ğ·ğ‘’ğ‘›ğ‘ ğ‘’ğ¸ğ¶ğ‘ ğ‘–ğ‘šğ‘‘=Î˜(1âˆ’(1âˆ’ğ›¼ğ›½)|ğ‘Œ|\n1âˆ’(1âˆ’ğ›¼ğ›½)ğ‘†ğ¼ğ‘€ğ· _ğ‘ğ‘–ğ‘¡_ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„|ğ‘‹||ğ‘|)\nThe classical hash solution shares the same complexity with\nSparseBMM (i.e., Î˜(ğ›¼ğ›½|ğ‘‹||ğ‘Œ||ğ‘|)).\nThe complexity of dense MM is Î˜(|ğ‘‹||ğ‘Œ||ğ‘|ğ›¾ğœ”âˆ’3), whereğ›¾=\nğ‘šğ‘–ğ‘›{|ğ‘‹|,|ğ‘Œ|,|ğ‘|}[10]. Though the best known ğœ”value can be\n2.373 [ 12], these theoretical MM algorithms are often considered\nimpractical [ 29,35]. And in the widely used packages for MM (e.g.\nIntel MKL), ğœ”=3.\nEstimating the Costs of Two Special Cases. We consider two\nspecific cases to better understand the average-case costs. First,\nğ›¼=ğ›½=1. That is, the input matrices are full. In this very dense\ncase, the time complexity of SparseBMM is Î˜(|ğ‘‹||ğ‘Œ||ğ‘|), while\nthe costs of DenseEC ğ‘›ğ‘œğ‘›ğ‘ ğ‘–ğ‘šğ‘‘ and DenseEC ğ‘ ğ‘–ğ‘šğ‘‘ are both Î˜(|ğ‘‹||ğ‘|).\nThanks to the early stopping technique, DenseEC is significantly\nfaster than the other algorithms (including Dense MM).\nSecond,ğ›¼=ğ›½=1\n|ğ‘Œ|. This is a very sparse case. The complex-\nity of SparseBMM is Î˜(|ğ‘‹||ğ‘|\n|ğ‘Œ|). No-simd will be chosen for the\nvery sparse case. The complexity of DenseEC ğ‘›ğ‘œğ‘›ğ‘ ğ‘–ğ‘šğ‘‘ isÎ˜(|ğ‘‹||ğ‘|).\nTherefore, SparseBMM is the best solution.\n3 PARTIAL RESULT CACHING\nWe observe that computing Join-Project results for different ğ‘§values\ncan take widely different amounts of time for real-world data sets.\nFigure 5 depicts the CDF (Cumulative Distribution Function) of\n0% 25% 50% 75% 100%\nPercentage of z0.00.20.40.60.81.0CDF(a) HetRec data set.\n0% 25% 50% 75% 100%\nPercentage of z0.00.20.40.60.81.0CDF (b) MV2 data set.\nFigure 5: CDF of computation cost.\ncomputation cost for two representative real-world data sets. In\nHetRec, the top 1% of ğ‘§values take 60% of the total time. In MV2,\nthe top 10% of ğ‘§values contribute to 23% of the total cost. This\nmotivates us to study caching the results of a subset of ğ‘§values\nto improve Join-Project performance. Note that our technique is\ndifferent from materialized views [ 28] or query result cache [ 27],\nwhere the full query results are cached.\nDIM3with Cached Partial Results. We choose to cache results\nbased onğ‘§values because the intersection-free partitioning method\ndivides table ğ‘†according to the ğ‘†.ğ‘§values. Hence, it is easy to\nintegrate the cached partial results into the DIM3algorithm.\nSupposeğ‘ğ‘ğ‘ğ‘â„ğ‘’ğ‘‘ is the subset of ğ‘†.ğ‘§, whose Join-Project re-\nsults are cached. That is, the cached partial results are ğ‘…ğ‘ğ‘ğ‘â„ğ‘’ğ‘‘ =\n{(ğ‘¥,ğ‘§)|(ğ‘¥,ğ‘§)âˆˆÎ ğ‘¥,ğ‘§(ğ‘…(ğ‘¥,ğ‘¦)Zğ‘¦ğ‘†(ğ‘§,ğ‘¦))âˆ§ğ‘§âˆˆğ‘ğ‘ğ‘ğ‘â„ğ‘’ğ‘‘}. In Figure 2,\nwe can simply omit any ğ‘§âˆˆğ‘ğ‘ğ‘ğ‘â„ğ‘’ğ‘‘ when generating ğ‘†ğ‘ ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’ and\nğ‘†ğ‘‘ğ‘’ğ‘›ğ‘ ğ‘’ , then keep the other steps of DIM3unchanged. Finally, we\noutputğ‘…ğ‘ğ‘ğ‘â„ğ‘’ğ‘‘ in addition to the results computed by DIM3.\nCaching Score. Given a caching space budget ğµfor a Join-Project\nquery, we rank ğ‘§values for caching with the following score:\nğ¶ğ‘ğ‘â„ğ‘’ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’(ğ‘§)=ğ‘ğ‘œğ‘ ğ‘¡(ğ‘§)\nğ‘ ğ‘ğ‘ğ‘ğ‘’(ğ‘§)\nğ‘ ğ‘ğ‘ğ‘ğ‘’(ğ‘§)is the cache space required to store the (ğ‘¥,ğ‘§)results for\nthe givenğ‘§. The cache content is (ğ‘§,ğ‘ ğ‘–ğ‘§ğ‘’,ğ‘¥ 1,ğ‘¥2,...,ğ‘¥ğ‘˜). Whenğ‘˜is\nsmall,ğ‘ ğ‘–ğ‘§ğ‘’=ğ‘˜>0, and we cache the original results. When ğ‘˜>0.5|ğ‘‹|,\nwe store the complement set of ğ‘¥â€™s to save space, and set ğ‘ ğ‘–ğ‘§ğ‘’=\nğ‘˜âˆ’|ğ‘‹|<0. Hence,ğ‘ ğ‘ğ‘ğ‘ğ‘’(ğ‘§)=2+ğ‘šğ‘–ğ‘›(ğ‘˜,|ğ‘‹|âˆ’ğ‘˜).\nğ‘ğ‘œğ‘ ğ‘¡(ğ‘§)is the time for computing (ğ‘¥,ğ‘§)results for the given ğ‘§.\nThis is the benefit from caching ğ‘§. Since the processing related to ğ‘§\nis distributed into many inner loops in SparseBMM and DenseEC,\nwe cannot directly measure ğ‘ğ‘œğ‘ ğ‘¡(ğ‘§). Instead, we collect statistics and\nestimate the cost as follows:\nğ‘ğ‘œğ‘ ğ‘¡(ğ‘§)=ğ‘›ğ‘ ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’(ğ‘¡ğ‘ ğ‘’ğ‘ğ‘…+ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘‘ğ‘…ğ‘Š)+ğ‘›ğ‘ ğ‘–ğ‘šğ‘‘ğ‘¡ğ¸ğ¶ğ‘‘+ğ‘›ğ‘›ğ‘œğ‘›ğ‘ ğ‘–ğ‘šğ‘‘ğ‘¡ğ¸ğ¶ğ‘ \nwhereğ‘›ğ‘ ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’ is the number of times that ğ‘§is checked in the inner\nloop of the SparseBMM algorithm, ğ‘›ğ‘ ğ‘–ğ‘šğ‘‘ andğ‘›ğ‘›ğ‘œğ‘›ğ‘ ğ‘–ğ‘šğ‘‘ are the\nnumber of times that ğ‘§is encountered in the SIMD and non-SIMD\npart of the inner loop of the DenseEC algorithm, respectively.\nğ¶ğ‘ğ‘â„ğ‘’ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’(ğ‘§)shows the benefit per unit space for caching ğ‘§.\nThe higher the ğ‘ğ‘œğ‘ ğ‘¡(ğ‘§), the lower the ğ‘ ğ‘ğ‘ğ‘ğ‘’(ğ‘§), the more beneficial\nto cacheğ‘§. Interestingly, a ğ‘§value with high ğ‘ğ‘œğ‘ ğ‘¡(ğ‘§)may produce a\nlarge number of results. This actually leads to small ğ‘ ğ‘ğ‘ğ‘ğ‘’(ğ‘§)when\nthe complement result set is saved.\nPractical Considerations. We discuss several practical issues for\nusing the partial result caching. First, database users can enable\nJoin-Project caching dynamically. We see in experiments that the\nstatistics collection and computation for ğ‘ğ‘œğ‘ ğ‘¡(ğ‘§)andğ‘ ğ‘ğ‘ğ‘ğ‘’(ğ‘§)do not\n7\n\nincur significant overhead for DIM3. Hence, after caching is enabled,\nthe first run of DIM3can compute ğ¶ğ‘ğ‘â„ğ‘’ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’(ğ‘§)and populate the\npartial result cache. Then, subsequent runs of DIM3on the same\ntables can leverage the cached results to improve performance. Sec-\nond, when the underlying tables are modified by insert/delete/update,\nwe can simply invalidate the cache and let the next run of DIM3to\nre-populate the result cache. Note that updating the cached results\n(similar to the view maintenance problem [ 4]) is beyond the scope\nof this work. Finally, we can use the cached result to support filter\npredicates on ğ‘¥and/orğ‘§. However, if there are filter predicates on ğ‘¦,\nwe have to run DIM3without the cached result.\n4 SUPPORT FOR JOIN-OP QUERY TYPES\nIn the above, we focus on the Join-Project operation. Join-Project\nis a special case of Join- ğ‘œğ‘queries, where ğ‘œğ‘is a common SQL\noperation. An interesting question arises: Is it possible to apply DIM3\nto Join-ğ‘œğ‘queries in general? We consider ğ‘œğ‘other than projection\nin the following:\nâ€¢Group-by Aggregation : The group-by operation inherently re-\nmoves duplicates. A join followed by a group-by aggregation\noperation, which we call Join-Aggregate for simplicity, im-\nplicitly performs a Join-Project operation. We describe how to\nextend DIM3to support Join-Aggregate in Section 4.1.\nâ€¢Join: Multiple join operations are followed by a duplicate elimi-\nnating projection. One strategy to evaluate this type of queries\nis to employ Join-Project to deduplicate the results of the last\njoin operation. But can we do better? We consider how and\nwhen to push the deduplication down in the query plan of MJP\nin Section 4.2.\nâ€¢Selection or Sorting : As Join-Selection or Join-Sorting do not\nrequire deduplication, there is no need to employ Join-Project\nfor these types of queries in general. However, in special cases\nwhere there are too many duplicates, an alternative evaluation\nstrategy can be more efficient. We compute Join-Project and\nkeep the duplicate count for each generated join result tuple.\nThen, the selection or sorting operation processes the much\nsmaller, deduplicated join result, thereby achieving better per-\nformance. Finally, we output the correct number of duplicates\nbased on the per-tuple duplicate counts.\nâ€¢Intersection/Difference/Union : First, the computation of intersec-\ntion is similar to a join operation. Hence, for Join-Intersection,\nwe can employ MJP. Second, set difference can be evaluated\nas a left-outer join followed by deduplication. We can modify\nDIM3to compute Outer-Join-Project for Join-Difference. To\nsupport outer-joins, DIM3can be extended with a bitmap for\nğ‘¥(ğ‘§). It sets a bit if the corresponding ğ‘¥row (ğ‘§column) has\ngenerated join results. In this way, the modified DIM3can com-\nputeğ‘¥(ğ‘§) with no matches for outer-joins. Finally, Join-Union\nrequires the deduplication of the outputs from two joins. We\ncan push the deduplication operation down in the query plan,\nand apply similar considerations as in Section 4.2.\n4.1 Join-Aggregate\nOur DIM3algorithm can be applied to Join-Aggregate operations\nwith slight modifications. Without loss of generality, we divide Join-\nAggregate operations into two categories based on the group byattributes: i) group-by attributes are from both tables, and ii) group-\nby attributes come from one table.\nGroup-by Attributes from Both Tables. For instance, given tables\nğ‘…(ğ‘¥,ğ‘¦,ğ‘£)andğ‘†(ğ‘§,ğ‘¦,ğ‘¢), we want to compute\nğ‘¥,ğ‘§Gğ‘ğ‘”ğ‘”ğ‘Ÿğ‘’ğ‘”ğ‘ğ‘¡ğ‘’(ğ‘“(ğ‘….ğ‘£,ğ‘†.ğ‘£))(ğ‘…(ğ‘¥,ğ‘¦,ğ‘£)Zğ‘¦ğ‘†(ğ‘§,ğ‘¦,ğ‘£))\nThis task is similar to the original Join-Project operation. The main\ndifference is that it computes ğ‘ğ‘”ğ‘”ğ‘Ÿğ‘’ğ‘”ğ‘ğ‘¡ğ‘’(ğ‘“(ğ‘….ğ‘£,ğ‘†.ğ‘¢))on the join\nresults with the same (ğ‘¥,ğ‘§)rather than deduplicating the results.\nWe modify DIM3to support this task. In SparseBMM, we change\ntheğ‘†ğ‘ƒğ´ array to contain the aggregate for each ğ‘ğ‘¢ğ‘Ÿğ‘§.ğ‘†ğ‘ƒğ´ is initial-\nized in each outer-loop iteration. Line 5â€“7 is modified to accumulate\nthe aggregate for group (ğ‘ğ‘¢ğ‘Ÿğ‘¥,ğ‘ğ‘¢ğ‘Ÿğ‘§). In DenseEC, bitmaps cannot\nsupport the aggregates. Therefore, we use a plain dense MM, while\ncomputing aggregates for each pair of (ğ‘ğ‘¢ğ‘Ÿğ‘¥,ğ‘ğ‘¢ğ‘Ÿğ‘§).\nGroup-by Attributes from One Table. Given tables ğ‘…(ğ‘¥,ğ‘¦)and\nğ‘†(ğ‘§,ğ‘¦), we want to compute the following query. If the group-by\nattribute is from ğ‘†, we switch table ğ‘…andğ‘†.\nğ‘¥Gğ‘ğ‘”ğ‘”ğ‘Ÿ(ğ‘§)(ğ‘…(ğ‘¥,ğ‘¦)Zğ‘¦ğ‘†(ğ‘§,ğ‘¦))\nWe can rewrite the query as follows:\nğ‘¥Gğ‘ğ‘”ğ‘”ğ‘Ÿ â€(ğ‘§â€²)(ğ‘…(ğ‘¥,ğ‘¦)Zğ‘¦(ğ‘¦Gğ‘ğ‘”ğ‘”ğ‘Ÿâ€²(ğ‘§)(ğ‘†(ğ‘§,ğ‘¦))))\nWe first compute the group-by aggregate on ğ‘†withğ‘¦as the group-\nby key. Ifğ‘ğ‘”ğ‘”ğ‘Ÿ isğ‘ ğ‘¢ğ‘š,ğ‘šğ‘–ğ‘›, orğ‘šğ‘ğ‘¥, thenğ‘ğ‘”ğ‘”ğ‘Ÿâ€²andğ‘ğ‘”ğ‘”ğ‘Ÿ â€are the\nsame. Forğ‘ğ‘œğ‘¢ğ‘›ğ‘¡ ,ğ‘ğ‘”ğ‘”ğ‘Ÿâ€²isğ‘ğ‘œğ‘¢ğ‘›ğ‘¡ andğ‘ğ‘”ğ‘”ğ‘Ÿ â€isğ‘ ğ‘¢ğ‘š. Forğ‘ğ‘£ğ‘”, theğ‘ğ‘”ğ‘”ğ‘Ÿâ€²\nconsists of both ğ‘ ğ‘¢ğ‘š andğ‘ğ‘œğ‘¢ğ‘›ğ‘¡ . Thenğ‘ğ‘”ğ‘”ğ‘Ÿ â€accumulates the two\ncomponents, and finally computes a division to obtain the ğ‘ğ‘£ğ‘”. We\ndenote the resulting table as ğ‘†ğº(ğ‘¦,ğ‘§â€²), whereğ‘§â€²is the intermediate\naggregate value(s) for ğ‘¦. Note that|ğ‘†ğº|=|ğ‘|is often much smaller\nthan|ğ‘†|.ğ‘†ğºis highly sparse because there is no duplicate ğ‘¦â€™s in the\ntable. Therefore, we employ the classical hash-based algorithm to\njoinğ‘…andğ‘†ğºthen compute the final group-by aggregates.\n4.2 MJP (Multi-Way Joins with Projection)\nSo far, we have been focusing on Join-Project on twotables. In this\nsubsection, we study deduplication on the results of joining multiple\ntables. This is also called conjunctive queries with projection [ 11].\nFor example, the line join projection with ğ‘›tables is expressed as:\nÎ ğ‘¥1,ğ‘¥ğ‘›+1(ğ‘…1(ğ‘¥1,ğ‘¥2)Zğ‘¥2ğ‘…2(ğ‘¥2,ğ‘¥3)Zğ‘¥3...Zğ‘¥ğ‘›ğ‘…ğ‘›(ğ‘¥ğ‘›,ğ‘¥ğ‘›+1))\nFigure 6 illustrates the query plan tree for evaluating a MJP query.\nThe deduplication operation ( Î ) can be pushed down to after each\njoin operation. While it incurs extra overhead, deduplication reduces\nthe intermediate result size, thereby reducing the cost of subsequent\njoin operations. There are two baseline execution plans. The first\nplan computes all the joins and then deduplicate the final join results.\nThe second plan deduplicates the results immediately after each join.\nHowever, it is easy to construct cases to show neither plan is optimal.\nTherefore, we need to judiciously place the deduplication operations\ninto the query plan tree.\nIn the following, we develop a DP (Dynamic Programming) al-\ngorithm to find the optimal query plan. To limit the scope of our\ninvestigation, we make the following assumptions.\n(1)The join order is given by the query optimizer. We focus on\nthe deduplication placement problem. For ease of presenta-\ntion, we number the tables in the join order as ğ‘…1, ...,ğ‘…ğ‘˜.\n8\n\nFigure 6: The deduplication placement problem.\nThe problem of optimizing both join order and deduplication\nplacement is beyond the scope of this work.\n(2)The query plan is in the form of a left-deep tree [ 20,38].\nLeft-deep or right-deep trees are widely used in RDBMSs\nto process multi-way joins. If a solution produces left-deep\ntrees, it is easy to modify it to support right-deep trees.\n(3)Estimated|ğ‘‚ğ‘ˆğ‘‡ğ½ğ‘–|and|ğ‘‚ğ‘ˆğ‘‡ğ‘ƒğ‘–|, whereğ‘–=2,...,ğ‘˜, are avail-\nable. (Please refer to recent work on cardinality esitmation\nfor details [ 6,16,30,36].) Here,|ğ‘‚ğ‘ˆğ‘‡ğ½ğ‘–|=|ğ‘…1ZÂ·Â·Â·Zğ‘…ğ‘–|\ndenotes the size of the intermediate results after joining the\nfirstğ‘–tables without deduplication. |ğ‘‚ğ‘ˆğ‘‡ğ‘ƒğ‘–|=|Î (ğ‘‚ğ‘ˆğ‘‡ğ½ğ‘–)|de-\nnotes the size of the intermediate results after deduplication.\nFor simplicity, we define |ğ‘‚ğ‘ˆğ‘‡ğ‘ƒ1|=|ğ‘‚ğ‘ˆğ‘‡ğ½1|=|ğ‘…1|.\n(4)Adding deduplication to the ğ‘–-th join reduces the final join\nresult by a factor of|ğ‘‚ğ‘ˆğ‘‡ğ‘ƒğ‘–|\n|ğ‘‚ğ‘ˆğ‘‡ğ½ğ‘–|. That is,|Î (ğ‘…1ZÂ·Â·Â·Zğ‘…ğ‘–)Z\nÂ·Â·Â·Zğ‘…ğ‘˜|â‰ˆ|ğ‘‚ğ‘ˆğ‘‡ğ‘ƒğ‘–|\n|ğ‘‚ğ‘ˆğ‘‡ğ½ğ‘–||ğ‘‚ğ‘ˆğ‘‡ğ½ğ‘˜|. This is reasonable since the\njoin input size to the (ğ‘–+1)-th join is reduced by a factor\nof|ğ‘‚ğ‘ˆğ‘‡ğ‘ƒğ‘–|\n|ğ‘‚ğ‘ˆğ‘‡ğ½ğ‘–|. This assumption simplifies the estimation of the\nfinal result size after inserting a deduplication operation. More\nprecise estimation requires collecting more statistics, and may\nincur much higher cost.\nWe useğ·ğ‘ƒğ‘–to denote the optimal time for computing Î (ğ‘…1Z\nÂ·Â·Â·Zğ‘…ğ‘–). We observe that if we already add deduplication to the\nğ‘–-th join, the deduplicated result ğ‘‚ğ‘ˆğ‘‡ğ‘ƒğ‘–does not change if more\ndeduplication operations are added to the joins before the ğ‘–-th join.\nTherefore, we have the following equations.\nğ·ğ‘ƒğ‘–=\u001a0, ğ‘– =1\nminğ‘–âˆ’1\nğ‘—=1(ğ·ğ‘ƒğ‘—+Ãğ‘–âˆ’1\nâ„=ğ‘—+1ğ½ğ‘œğ‘–ğ‘›ğ¶ğ‘œğ‘ ğ‘¡â„+ğ½ğ‘ƒğ¶ğ‘œğ‘ ğ‘¡ğ‘–), ğ‘–>1\nThe formula inside ğ‘šğ‘–ğ‘› computes the case where a deduplication is\nadded to the ğ‘—-th join and there is no deduplication from the (ğ‘—+1)-th\nto(ğ‘–âˆ’1)-th join. The total cost of this case has three components: (i)\nğ·ğ‘ƒğ‘—, which is the optimal time for computing ğ‘‚ğ‘ˆğ‘‡ğ‘ƒğ‘—; (ii) the cost\nof the subsequent joins, i.e., the (ğ‘—+1)-th to(ğ‘–âˆ’1)-th join; and (iii)\nthe cost of the final join project operation. Note that when ğ‘—=1, there\nis no deduplication before the ğ‘–-th join.\nBased on assumption (3), we estimate the input table size for\ncomponent (ii) and (iii). Specifically, ğ½ğ‘œğ‘–ğ‘›ğ¶ğ‘œğ‘ ğ‘¡â„is the cost of joining\nÎ (ğ‘…1ZÂ·Â·Â·Zğ‘…ğ‘—)ZÂ·Â·Â·Zğ‘…â„âˆ’1andğ‘…â„. The size of the former\nis estimated as|ğ‘‚ğ‘ˆğ‘‡ğ‘ƒğ‘—|\n|ğ‘‚ğ‘ˆğ‘‡ğ½ğ‘—||ğ‘‚ğ‘ˆğ‘‡ğ½â„âˆ’1|. Moreover, ğ½ğ‘ƒğ¶ğ‘œğ‘ ğ‘¡ğ‘–is the cost\nof the Join-Project operation at the ğ‘–-th join. The sizes of the two\ninput tables to the Join-Project operation are|ğ‘‚ğ‘ˆğ‘‡ğ‘ƒğ‘—|\n|ğ‘‚ğ‘ˆğ‘‡ğ½ğ‘—||ğ‘‚ğ‘ˆğ‘‡ğ½ğ‘–âˆ’1|and\n|ğ‘…ğ‘–|. The intermediate join result in the Join-Project operation can\nbe estimated as|ğ‘‚ğ‘ˆğ‘‡ğ‘ƒğ‘—|\n|ğ‘‚ğ‘ˆğ‘‡ğ½ğ‘—||ğ‘‚ğ‘ˆğ‘‡ğ½ğ‘–|and the final output size is |ğ‘‚ğ‘ˆğ‘‡ğ‘ƒğ‘–|.Given the sizes of the input tables, the intermediate join result, and\nthe final output, we can use the formulas in Section 2.6 to estimate\nthe cost of the Join-Project operation.\nFor a MJP query on ğ‘›tables, the path to get ğ·ğ‘ƒğ‘›in the DP process\ngives the optimal deduplication placement. The time complexity of\nthis algorithm is Î˜(ğ‘›3). Asğ‘›is often not large, the overhead of this\nDP algorithm is small.\n5 PERFORMANCE EV ALUATION\nIn this section, we evaluate the performance of our proposed solu-\ntions using both real-world and synthetic data sets.\n5.1 Experimental Setup\nMachine Configuration. All experiments are performed on a ma-\nchine with Intel Core i7-9700 CPU (3.00GHz, Turbo Boost 4.70\nGHz, 8 cores/8 threads, 12MB last level cache) and 32 GB RAM,\nrunning Ubuntu 18.04.5 LTS with Linux 5.4.0-81 kernel. All code is\nwritten in C/C++ and compiled with g++ 7.5.0 using â€“std=c++11 ,\n-O3, and -mavx flags. MKL (Intel Math Kernel Library) is used in\nDHK. By default, we run single-threaded experiments. In the parallel\nexperiments, we use OpenMP for parallelization.\nDatasets. We use six real-world datasets with different input size,\n|ğ‘‚ğ‘ˆğ‘‡ğ½|, and|ğ‘‚ğ‘ˆğ‘‡ğ‘ƒ|in our experiments, as shown in Table 2. Ama-\nzon [ 31] data set records the frequently co-purchased products on\nAmazon website. The Join-Project queries help to find potential\nproducts that can be co-purchased. Slashdot [ 32] is a technology-\nrelated news website. The data set contains friend/foe links between\nusers of Slashdot. The query computes the indirectly connected pairs\nof users. The data set and the query of HetRec [ 7] follow the motivat-\ning example in Section 1. MV1 and MV2 are two MovieLens [ 15]\ndata sets. They contain user ratings for movies. Similarly, Jokes [ 13]\ncontains user ratings for jokes. The Join-Project queries on Movie-\nLens and Jokes data sets find users that have rated the same objects.\nFriendster [ 44] is a social network dataset that contains the retweet\nrelationship between users. We use this data set to study MJP, which\ncomputes the multi-hop connections between users.\nWe also conduct experiments on the TPC-H data set with SF=10.\nA representative query joins LineItem andOrders , then projects on\nCUSTKEY andSUPPKEY to attain the purchase relationship be-\ntween customers and suppliers. However, since this join is a primary-\nforeign key join,|ğ‘‚ğ‘ˆğ‘‡ğ½|is similar to the size of LineItem . Both\nDIM3and DHK choose the classical solution, showing the same\nperformance. Hence, we do not study TPC-H further.\nIn contrast,|ğ‘‚ğ‘ˆğ‘‡ğ½|and|ğ‘‚ğ‘ˆğ‘‡ğ‘ƒ|in the other data sets are much\nlarger than their input sizes. The higher the ğ‘ğ‘›ğ‘§, the denser the\ndataset, and thus the more intermediate join results. As we do not\nassume any special sort order in the input tables of Join-Project, we\nrandomly shuffle the data sets before the experiments.\nSolutions to Compare. We compare two categories of solutions:\nstand-alone Join-Project algorithms, and full-fledged RDBMSs.\nWe compare the following stand-alone implementations. (1) Clas-\nsical : The classical solution performs the Radix join [ 2] then a\nhash-based deduplication using a flat_hash_map [ 39]. (2) MKL : The\nstate-of-the-art dense MM in Intel MKL is used to evaluate Join-\nProject. We implement the baseline mapping algorithm as described\n9\n\nTable 2: Real-world data sets used in experiments.\nData set|ğ‘…| |ğ‘†|ğ‘ğ‘›ğ‘§(ğ‘…)ğ‘ğ‘›ğ‘§(ğ‘†) |ğ‘‚ğ‘ˆğ‘‡ğ½| |ğ‘‚ğ‘ˆğ‘‡ğ‘ƒ|\nAmazon 1.2M 1.2M 0.0018% 0.0018% 14M 11M\nSlashdot 905K 905K 0.02% 0.02% 118M 81M\nHetRec 487K 438K 0.02% 0.03% 125M 34M\nMV1 500K 500K 2.2% 2.2% 204M 29M\nMV2 1M 1M 4.5% 4.5% 816M 35M\nJokes 617K 617K 25% 25% 10B 622M\nFriendster 1.8B 4.2Ã—10âˆ’7â€“ â€“\nNote:ğ‘ğ‘›ğ‘§(ğ‘€)is the percentage of non-zero elements in matrix ğ‘€.\nin Section 2.2. (3) DHK : We obtained the DHK code from the au-\nthors of the DHK paper [ 10]. Algorithm 3 in DHK chooses the\nclassical solution when |ğ‘‚ğ‘ˆğ‘‡ğ½| â‰¤ 20Â·ğ‘, whereğ‘=|ğ‘…|=|ğ‘†|.\nWe find this feature is missing in the DHK code, and implement\nthe feature. The code assumes the input contains consecutive natu-\nral numbers. Hence, we add the baseline mapping algorithm. The\ncode leaves the final deduplication unimplemented. Thus, we add\na deduplication step that checks every result (ğ‘¥,ğ‘§)from the sparse\npart against the matrix Cğ‘¥Ã—ğ‘§computed in the dense part. (4) DIM3:\nOur proposed solution follows the description in Section 2. We also\nstudy the individual components of DIM3. For the MM component,\nwe compare DenseEC, SparseBMM, MM in MKL, and a sub-cubic\nMM [3] based on the Strassen algorithm [40].\nIn addition to the stand-alone algorithms, we compare DIM3\nwith four full-fledged RDBMSs. (5) DBMSX : one of the best per-\nforming commercial RDBMSs. (6) PostgreSQL version 13.3 and (7)\nMariaDB version 10.5.11: two popular open-source RDBMSs. (8)\nMonetDB version 11.39.17: a representative analytical main memory\nRDBMS. We set the configuration parameters of the RDBMSs to\nensure that they make full use of the memory. For each experiment,\nwe run the same query on the target data set twice. The first run\nwarms up the RDBMSs so that the input tables are loaded into main\nmemory. Then we measure the performance of the second run.\nOur comparison between DIM3and RDBMSs is meaningful.\nOne concern is that RDBMSs pay additional cost, including socket\ncommunication, SQL parsing, query optimization. (Note that we\ncompute a final count to avoid returning a large number of query\nresults from RDBMSs.) We quantify this additional cost by mea-\nsuring the execution time of the same queries on a very small data\nset. The result is 1â€“3ms. In comparison, our reported run times on\nRDBMSs are from 7s to about 1 hour. Hence, the extra cost of 1â€“3ms\nis negligible. Our reported run times indeed correspond to the cost\nof Join-Project query processing in RDBMSs.\nUnless otherwise noted, we focus on single-threaded performance\nin the experiments. The scalability results report multi-threaded\nperformance to show that our proposed algorithms are amenable to\nparallelization. Partial result caching is disable by default. Every\nexperiment is run five times. We report the average of the five runs.\n5.2 Evaluation for Join-Project Operations\nPerformance of Stand-alone Solutions on Real-World Data Sets.\nFigure 7(a) compares DIM3with stand-alone Join-Project algorithms\non real-world data sets. We see that DIM3achieves the best per-\nformance among all stand-alone algorithms with or without result\nmaterialization. Compared to DHK , the state-of-the-art algorithm,\nDIM3achieves 2.3Ã—-18Ã—improvements. For all the six data sets,\nDIM3chooses the hybrid strategy. In contrast, for Amazon, DHK\n(a) Comparison with stand-alone Join-Project algorithms.\nAmazon Slashdot HetRec MV1 MV2 Jokes100101102103Running time (s)\nDBMSX PostgreSQL MariaDB MonetDB DIM312 27 24 48 506 >2385\n(b) Comparison with RDBMSs.\nFigure 7: Join-Project on real-world datasets. (By default, the\nalgorithms count then discard the result tuples. Subscript â€™Mâ€™\ndenotes a version of an algorithm that materializes the result\ntuples in memory. â€™Xâ€™ shows when MKL/Classical runs out of\nmemory and fails. We label speedups of DIM3over DHK in (a),\nand over the best RDBMS solution in (b).)\nchooses the classical solution. The speedup of DIM3over DHK\nfor Amazon shows that our strategy selection function ğ‘“1is more\naccurate than the rule-of-thumb condition of DHK.\nMoreover, DIM3has smallest memory footprints among all algo-\nrithms, as shown in Table 3. We use â€™ /usr/bin/time -v â€™ to measure the\npeak memory usage (i.e., maximum resident set size). Typically, the\nfinal Join-Project results will be consumed by upper-level operators,\nand therefore we do not allocate space for storing the final results.\nCompared to Classical , when the data sets are dense, DIM3saves\n80%â€“99% memory because it chooses the hybrid strategy and saves\nthe space required by hash-based deduplication in Classical . The\nmemory usage of Gemm(MKL) is mainly determined by the matrix\nsizes. MKL runs out of memory for the two sparsest data sets (i.e.,\nAmazon and Slashdot). When the datasets are dense, DIM3saves\n68%â€“99% memory of Gemm(MKL) because DenseEC saves the\nspace of the output matrix, and uses one bit per element rather than\n4-byte integers in MKL. As for DHK , one main source of its memory\nusage is the input and output matrices for dense MM invocation. In\ncomparison, DenseEC significantly saves this memory.\nComparison with RDBMSs on Real-World Data Sets. Figure 7(b)\ncompares DIM3with RDBMSs. We see that DIM3outperforms all\nthe RDBMSs. For data sets with |ğ‘‚ğ‘ˆğ‘‡ğ½|â‰«|ğ‘…|+|ğ‘†|, DIM3achieves\none to three orders of magnitudes of speedups.\nWe examine the query plans generated by the RDBMSs and see\nthat they all essentially employ the Classical solution, i.e., a join\nfollowed by a deduplication of the intermediate results. Among the\nRDBMSs, MonetDB is the best when memory is sufficient. However,\nits performance drops sharply when the required space exceeds the\nmemory size (for MV2 and Jokes). In the case of Jokes, all the\nRDBMSs fail to perform the deduplication operation entirely in\nmemory. The queries take much longer time because of the disk\nI/Os for storing and accessing the intermediate results. We stop the\n10\n\nTable 3: Memory usage of stand-alone Join-Project algorithms.\nAmazon Slashdot HetRec MV1 MV2 Jokes\nDIM386MB 64MB 32MB 39MB 72MB 47MB\nDHK 426MB 70MB 168MB 68MB 132MB 2.4GB\nGemm(MKL) >32GB >32GB 7.1GB 211MB 227MB 2.4GB\nClassical 426MB 3.0GB 1.5GB 789MB 1.5GB 24GB\nSlashdot HetRec MV1 MV2 Jokes0.51Running time (s)DIM3\nDHK4.5s\n(a) Performance with 8 threads.\n1 2 3 4 5 6 7 8\n#Thread123456SpeedupDIM3\nDHK\n(b) Scalability on the HetRec data set.\n1 2 3 4 5 6 7 8\n#Thread123456SpeedupDIM3\nDHK (c) Scalability on the MV1 data set.\nFigure 8: Multi-threaded performance.\nqueries when they took longer than 1 hour. The figure reports the\nlower bound (i.e., 3600s) for RDBMSs on the Jokes data set.\nAs RDBMSs implement the classical solution, we focus on the\ncomparison with stand-alone solutions in the rest of this subsection.\nScalability. We implement the multi-threaded DIM3with OpenMP\nby modifying around 20 lines of code. Specifically, we parallelize the\nouter for-loop in SparseBMM and DenseEC. Since every outer loop\niteration focuses on a specific ğ‘¥, there is no dependence or contention\namong the execution of different outer-loop iterations. While more\ninvolved changes may better parallelize the algorithm, we find that\nsuch simple modifications can already achieve promising results.\nWe compare DIM3with DHK using 8 threads in Figure 8(a). We\nsee that DIM3achieves 2.6Ã—-20Ã—speedups over DHK. Figure 8(b)â€“\n(c) show the scalability of the two algorithms. The Y-axis reports\nthe speedup compared to the single-threaded execution of the same\nalgorithm. We see that DIM3shows good scalability as the number\nof threads increases from 1 to 8.\nSensitivity Analysis with Synthetic Data Sets. We use synthetic\ndata sets to investigate the Join-Project performance for a wide range\nof situations. The default parameters in these experiments are as\nfollows. All input columns are generated with uniform distributions.\n|ğ‘…|=|ğ‘†|=106, and|ğ‘‹|=|ğ‘Œ|=|ğ‘|=ğ‘›=104. We study the\nperformance impact varying ğ‘›, data skews, and selectivity in Figure 9.\nOverall, we see that DIM3performs the best in all the cases.\n(1) Varying ğ‘›. Figure 9(a) varies ğ‘›, the number of distinct ğ‘¥/ğ‘¦/ğ‘§,\nwhile keeping the number of non-zero elements (i.e. |ğ‘…|and|ğ‘†|)\nfixed. Asğ‘›increases, the input data sets become more and more\nsparse, and|ğ‘‚ğ‘ˆğ‘‡ğ½|decreases from 109to106.Classical runs faster\nfor more sparse data sets. Gemm(MKL) sees a sharp increase of run\ntime as the matrix dimensions (i.e., ğ‘›) increase. DIM3wisely selects\nevaluation strategies based on data density. It employs DenseEC for\nall data when ğ‘›<104, SparseBMM when ğ‘›âˆˆ[104,4Ã—105], and the\nclassical solution when ğ‘›>4Ã—105. In this way, DIM3achieves the\nbest performance in all cases.\nDHK has good performance in the middle range in Figure 9(a) and\nswitches to Classical in the higher range. However, its performance\nis worse than Gemm(MKL) in the lower range. To understand the\nreason, we look into the DHK code and find that DHK restricts the\n103104105106\n|X|, |Y|, |Z|0.00.51.01.52.02.53.0Running time (s)DIM3\nDHK\nGemm(MKL)\nClassical(a) Varyingğ‘›(|ğ‘‹|=|ğ‘Œ|=|ğ‘|=ğ‘›).\n0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Running time (s)DIM3\nDHK\nGemm(MKL) >8s\nClassical > 2.8s (b) Zipf onğ‘….ğ‘¥andğ‘†.ğ‘§.\n0.0 0.2 0.4 0.6 0.8 1.0051015Running time (s)DIM3\nDHK\nGemm(MKL)\nClassical\n(c) Zipf onğ‘….ğ‘¦andğ‘†.ğ‘¦.\n1 2 3 4 5 6 7 8 9 10\n|R| 106020406080100Running time (s)DIM3\nDHK\nGemm(MKL)\nClassical (d) R-mat.\n0 0.2 0.4 0.6 0.8 1\nSelectivity0.00.51.01.52.0Running time (s)DIM3\nDHK\nGemm(MKL)\nClassical\n(e) Varying selectivity of ğ‘….ğ‘¥andğ‘†.ğ‘§.\n0.00 0.02 0.04 0.06 0.08 0.10\nSelectivity0.000.020.040.060.080.10Running time (s)DIM3\nDHK\nGemm(MKL)\nClassical (f) Zoom in to bottom left of (e).\nFigure 9: Sensitivity analysis of DIM3.\ndegree thresholds (i.e., Î”1andÎ”2[10]) to have Î”2=|ğ‘…|Î”1\n|ğ‘‚ğ‘ˆğ‘‡ğ‘ƒ|in order\nto reduce the threshold search space. Unfortunately, this narrows the\nrange of considered thresholds. For the lower range when the data is\nvery dense, DHK fails to put all data into the dense part, and thus\nperforms worse than Gemm(MKL) . In comparison, DIM3does not\nsuffer from this problems. DIM3makes the partition decision based\nonğ‘§, which is simple and less error-prone to compute.\n(2) Varying data skews . In Figure 9(b), we generate ğ‘….ğ‘¥andğ‘†.ğ‘§\nusing the Zipf distribution, and vary the parameter ğ›¼from 0 to\n1. Asğ‘….ğ‘¥andğ‘†.ğ‘§become more skewed, |ğ‘‚ğ‘ˆğ‘‡ğ½|stays around 108,\nwhile the deduplicated result size |ğ‘‚ğ‘ˆğ‘‡ğ‘ƒ|decreases from 6Ã—107to\n1.4Ã—107. DIM3partitions the more skewed ğ‘§â€™s into the dense part\nasğ›¼increases, and achieves a speedup of âˆ¼1.5Ã—compared to DHK .\nIn Figure 9(c), we generate the join attribute, ğ‘….ğ‘¦andğ‘†.ğ‘¦, using\nZipf distribution, and vary ğ›¼from 0 to 1. As ğ‘….ğ‘¦andğ‘†.ğ‘¦become\nmore skewed, the number of intermediate join results ( |ğ‘‚ğ‘ˆğ‘‡ğ½|) in-\ncreases sharply from 108to4.3Ã—109, and the data sets become\nincreasingly dense. Classical runs slower as the data sets become\ndenser. Gemm(MKL) sees a flat curve because the matrix dimensions\nare the same. DHK suffers from the threshold search space problem\nfor very dense data sets. DIM3achieves the best performance in\nall cases. There is a dip in the DIM3curve atğ›¼=1 because early\nstopping effectively reduces DenseEC computation in this case.\nIn Figure 9(d), we use R-mat [ 8] to generate skew data so that the\ntwo columns in a table are correlated. We use the R-mat parameters\nin Graph500 [ 34] (i.e.,ğ‘=0.57,ğ‘=0.19,ğ‘=0.19, andğ‘‘=0.05).\nWe generate a directed graph with ğ‘›vertices and|ğ‘…|edges. The Join-\nProject finds 2-hop paths in the graph. As R-mat requires ğ‘›to be a\npower of 2, we set ğ‘›=214, which is close to the default 104. We vary\n|ğ‘…|from 106to107. From Figure 9(d), we see that DIM3performs\nsignificantly better than the other algorithms. As |ğ‘…|increases, the\n11\n\n103104105106\n|X|, |Y|, |Z|0.000.050.100.150.20Running time (s)Optimized Mapping\nflat_hash_map\nstd::unordered_map(a) Mapping step varying ğ‘›.\n1 2 3 4 5 6 7 8 9 10\n|R|, |S| 1060.00.51.01.52.02.53.03.5Running time (s)Optimized Mapping\nflat_hash_map\nstd::unordered_map (b) Mapping step varying |ğ‘…|=|ğ‘†|.\n1000 2000 3000 4000 5000 6000\n|X|, |Y|, |Z|0.00.51.01.52.0Running time (s)DenseEC\nGemm(MKL)\nSTRASSEN\nSparseBMM\n(c) DenseEC varying ğ‘›.\n103104105106\n|X|, |Y|, |Z|0.00.51.01.52.02.53.0Running time (s)SparseBMM\nClassical\nSpmm(MKL) (d) SparseBMM varying ğ‘›.\nFigure 10: Effectiveness of DIM3Components.\ndata set becomes increasingly dense. Classical ,Gemm(MKL) , and\nDHK curves show similar trends as in Figure 9(c).\n(3) Varying selectivity . In Figure 9(e), we consider the cases\nwhere there are filtering predicates on both ğ‘….ğ‘¥andğ‘†.ğ‘§. We vary\ntheir selectivity at the same time from 0 to 1. Figure 9(f) zooms in to\nthe bottom left of the Figure 9(e). As the selectivity decreases, both\nthe input sizes and the output size decrease. Hence, all the algorithms\nrun faster. From the figure, we see that DIM3performs the best in all\ncases. Note that when the selectivity is very small (less than 0.03),\nDIM3switches to the classical solution. When the selectivity is less\nor equal than 0.2, DHK switches to the classical solution.\nEffectiveness of DIM3Components. In the following, we use syn-\nthetic data sets with the same default parameters as the above.\n(1) Mapping step . We compare our optimized mapping solution\nwith the baseline using std::unordered_map, and an improved base-\nline using flat_hash_map. In Figure 10(a), we fix |ğ‘…|=|ğ‘†|=106while\nvaryingğ‘›from 103to106. The hash table size increases as ğ‘›. When\nğ‘›<4Ã—105, the hash table fits into the L3 cache, and thus we\ndo not perform cache partitioning. Our solution has similar perfor-\nmance compared to flat_hash_map. When ğ‘›â‰¥4Ã—105, we use cache\npartitioning to reduce expensive CPU cache misses, thereby signifi-\ncantly out-performing flat_hash_map. As for std::unordered_map, it\nis much slower because of chained hashing.\nIn Figure 10(b), we fix ğ‘›=106while varying|ğ‘…|=|ğ‘†|from 106to\n107. The size of the hash table is fixed. The number of hash visits\nincreases. Hence, we see the increasing trends for all curves. Overall,\nour optimized mapping solution performs the best.\nMoreover, we compare DIM3(with optimized mapping) and\nDIM3with baseline std::unordered_map for the Amazon data set.\nThe mapping step takes 54.9% of the Join-Project run time for\nDIM3with baseline mapping. Replacing the baseline mapping with\noptimized mapping, DIM3achieves an improvement of 1.5x.\n(2) DenseEC . In Figure 10(c), we vary ğ‘›from 1000 to 6000,\nwhere the data sets are relatively dense and thus dense MM makes\nsense. We compare DenseEC with dense MM implementations.\nWe investigate sub-cubic fast MM algorithms. While the best\nknownO(ğ‘›2.373)algorithm [ 12] is considered impractical because\nof its huge constant factors [ 29,35], recent work aims to make\nthe Strassen algorithm practical [ 3]. We obtain and evaluate this\n0% 20% 40% 60% 80% 100%\nCache Percentage0.00.20.40.60.81.0Run time (normalized)Slashdot\nHetrec\nMV1\nMV2\nJokes(a) Run time on real-world data sets.\n0% 20% 40% 60% 80% 100%\nCache Percentage0.00.10.20.30.40.50.6Space (normalized)Slashdot\nHetrec\nMV1\nMV2\nJokes (b) Space cost on real-world data sets.\n0% 20% 40% 60% 80% 100%\nCache Percentage0.00.20.40.60.81.0Run time (normalized)Zipf-0\nZipf-0.25Zipf-0.50\nZipf-0.75Zipf-1.0\n(c) Run time on Zipf data.\n0% 20% 40% 60% 80% 100%\nCache Percentage0.00.20.40.60.81.0Run time (normalized)|R|=106\n|R|=3 106|R|=6 106\n|R|=107 (d) Run time on R-mat data.\nFigure 11: Partial result caching. ( â˜…shows the point where\ncached result size =input data size.)\nSTRASSEN implementation [ 3]. Our initial results show that it is\nslower than Intel MKL. One potential reason is that the original\nSTRASSEN supports only floating point MM, while the MKL run\nuses integer MM. Since the STRASSEN code calls BLAS as the\nunderlying MM sub-routine, it is straight-forward to modify the code\nto call MKLâ€™s integer MM. The resulting integer STRASSEN indeed\nruns slightly faster, but it is still slower than MKL. Figure 10(c)\nshows the performance of the improved integer STRASSEN .\nOverall, DenseEC out-performs dense MM implementations be-\ncause early stopping can effectively reduce computation cost.\n(3) SparseBMM . In Figure 10(d), we vary ğ‘›from 103to106.\nThe generated data sets become more and more sparse, and there-\nfore sparse MM and Classical are more suitable. We compare\nSparseBMM with Classical and MKLâ€™s sparse MM. From the figure,\nwe see that SparseBMM performs the best.\nWe also plot the SparseBMM curve in Figure 10(c). The crossing\npoint of SparseBMM and DenseEC is around 4000. This shows why\nDIM3chooses between DenseEC and SparseBMM with function ğ‘“2.\n5.3 Evaluation for Partial Result Caching\nWe evaluate partial result caching for DIM3using both real-world\nand synthetic data sets, as shown in Figure 11. The synthetic data\nsets use the same default parameters as in Section 5.2. Our solution\nretrieves cached partial results and performs JoinProject between ğ‘…\nand{(ğ‘§,ğ‘¦)|(ğ‘§,ğ‘¦)âˆˆğ‘†âˆ§ğ‘§âˆ‰ğ‘ğ‘ğ‘ğ‘â„ğ‘’ğ‘‘}. The X-axis varies the percent-\nage ofğ‘§values that are cached. The Y-axis reports either the run\ntime normalized to DIM3without caching, or the cached result size\nnormalized to the size of the total JoinProject results.\nFigure 11(a) and 11(b) show the run time and the cached result\nsize on real-world data sets, respectively. For Slashdot and HetRec,\nthe computation time distributions for ğ‘§values are very skewed.\nThe space cost is relatively high because most results are cached\nby their original values. Caching a relatively small percentage (e.g.,\n20%) ofğ‘§values can significantly speedup DIM3. On the other hand,\nthe denser data sets (i.e., MV1, MV2, and Jokes) see less skewed\ncomputation time distributions for ğ‘§values. As the number of results\nfor the same ğ‘§is often large, our technique to cache the complement\nset of result vectors can drastically reduce the cached result size. For\n12\n\nexample, the space at 100% (i.e., for caching all results) is only 12%,\n6%, and less than 1% of the total result sizes for MV1, MV2, and\nJokes, respectively. Thus, we can use a small amount of cache space\nto achieve significant speedups. Overall, if we cache either 20% of\nğ‘§values or up to the input data size, partial result caching achieves\n3.3xâ€“20x improvements over DIM3without caching.\nFigure 11(c) shows the run time on synthetic data sets where ğ‘….ğ‘¥\nandğ‘†.ğ‘§follow Zipf distribution. When Zipf parameter ğ›¼=0, the\ndata is uniformly distributed, and the benefits of caching is low. As\nğ›¼increases from 0.25 to 1, the computation time distribution for ğ‘§\nvalues becomes more skewed and caching is more helpful. Caching\nonly 10% of ğ‘§values reduces the run time by 13% to 57%.\nFigure 11(d) shows the run time on R-mat data varying the input\ntable size|ğ‘…|from 106to107. If we limit the cache space to the\nsize of the input table, partial result caching reduces the run time of\nDIM3by 25%â€“67%.\n5.4 Evaluation for Join-OP Query Types\nJoin-Aggregate. Figure 12(a) compares the Join-Aggregate perfor-\nmance of DIM3with RDBMSs and the stand-alone hash-based\nsolution on MV1, MV2, and Jokes. We focus on the more complex\ncase where the group-by attributes are from both tables. As described\nin Section 4.1, if the group-by attributes are from one table, we can\nrewrite the query and employ the classical solution for efficient eval-\nuation. We compute aggregates on the ratings for the MovieLens\nand Jokes data sets. (We omit Slashdot and HetRec in this set of\nexperiments because they do not have natural value columns to ex-\npress the aggregation.) We see that DIM3performs the best among\nall solutions. Compared with the hash-based solution, DIM3obtains\na speedup of 18â€“24 Ã—. Compared with the fastest RDBMS solution,\nDIM3obtains a speedup of as least 30 Ã—.\nMJP. We evaluate the effectiveness of our proposed DP algorithm.\nWe compare three DIM3variants with RDBMSs: 1) DIM3(DP) runs\nDP to find the optimal query plan; 2) DIM3(EagerDedup) places a\ndeduplication after every join; and 3) DIM3(LazyDedup) performs\nonly one deduplication after all the joins.\nIn this set of experiments, we use the Friendster data set [ 44] and\ncompute multi-hop connections between users. Since the original\nFriendster data set is large and very sparse, as shown in Table 2, we\nconstruct subsets of the data set using a parameter ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’ . Given a\nğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’ , we filter out any tuples whose attributes are both larger than\nğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’ . From the remaining tuples, we randomly extracted 10 tables,\neach with 106tuples, then perform a 10-way join with projection. We\nsetğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’ =5Ã—105,7Ã—105or9Ã—105. Note that the larger the ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’ ,\nthe sparser the input tables. For the three sub data sets, the final\noutput size|ğ‘‚ğ‘ˆğ‘‡ğ‘ƒ|is 46M, 18M, and 8M, and DIM3(DP) decides\nto place 4, 3, and 2 deduplication operations, respectively.\nAs shown in Figure 12(b), all DIM3variants run faster than\nRDBMSs because of the better Join-Project performance of DIM3.\nCompared with the hash-based solution with EagerDedup strategy,\nDIM3(DP) obtains a speedup of 4.9â€“7.4 Ã—. Whenğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’ =5Ã—105,\nDIM3(DP) gains a 10x speedup compared to LazyDedup . When\nğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’ =7Ã—105,DPare 11% and 39% better than EagerDedup\nandLazyDedup , respectively. When ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’ =9Ã—105,DPis 1.9x as\nfast as EagerDedup . However, DPis slightly (i.e., 5%) slower than\nLazyDedup . In this case, DPplaces a deduplication after the first\nMV1 MV2 Jokes100101102103Running time (s)\nDBMSX PostgreSQL MariaDB MonetDB Hash DIM3X(a) Join-Aggregate. ( ğ‘¥,ğ‘§Gğ‘ğ‘£ğ‘”(|ğ‘….ğ‘£âˆ’ğ‘†.ğ‘£|)(ğ‘…(ğ‘¥,ğ‘¦,ğ‘£)Zğ‘¦ğ‘†(ğ‘§,ğ‘¦,ğ‘£)))\nRange=5 105Range=7 105Range=9 105100101102Running time (s)\nDBMSX PostgreSQL MariaDB MonetDB\nHash(EagerDedup) DIM3(EagerDedup) DIM3(LazyDedup) DIM3(DP)\n(b) MJP.\nFigure 12: Evaluation for Join- ğ‘œğ‘query types.( Hash : stand-\nalone implementation using flat_hash_map for join, aggrega-\ntion, and deduplication. â€™Xâ€™: Hash runs out of memory and\nfails.)\njoin. The hope is that the amount of computation in subsequent\njoins will be proportionally reduced, but the actual reduction is not\nas significant. Overall, we see that DIM3(DP) performs the best\namong the three variants. This confirms the effectiveness of the DP\nalgorithm for finding the optimal plan for evaluating MJP.\n6 CONCLUSION AND FUTURE WORK\nIn this paper, we propose DIM3that combines intersection-free\npartitioning, optimized mapping, and DenseEC and SparseBMM\nalgorithms to improve the state-of-the-art DHK solution. Moreover,\nwe investigate partial result caching and extend DIM3to efficiently\ncompute Join- ğ‘œğ‘queries. Our results show that DIM3is a promising\nsolution for the widely used Join-Project operation.\nSeveral promising future directions remain to be explored. The\nfirst is out-of-core computation. When the input table ğ‘…andğ‘†are\ntoo large to fit in the allocated memory, one way is to perform I/O\npartitioning for ğ‘…andğ‘†according to ğ‘….ğ‘¥andğ‘†.ğ‘§, respectively. Then,\nwe load each pair of ğ‘….ğ‘¥partition and ğ‘†.ğ‘§partition into memory\nand employ DIM3to compute the results. An alternative way is\nto partition the tables according to the join key ğ‘¦. However, a final\ndeduplication step is necessary because the intermediate results from\ndifferent partitions may contain duplicates.\nThe second direction is to study caching for multiple queries\nwhen partial result caching is enabled. In addition to traditional con-\nsiderations, such as query access statistics, the cache space allocated\nto a Join-Project query becomes a tunable parameter. This adds a\nnew dimension in the design of the caching strategy.\nLast but not least, it is interesting to exploit new hardware to accel-\nerate database operations. For example, GPUs (Graphics Processing\nUnits) [ 5] and TPUs (Tensor Core Units) [ 19] can significantly ac-\ncelerate matrix multiplication in Join-Project. NVM (Non-V olatile\nMemory) provides an interesting combination of persistence, capac-\nity, and performance, which can be used to speed up queries with\nhuge memory consumption [41].\n13\n\nACKNOWLEDGMENTS\nThis work is partially supported by Natural Science Foundation of\nChina (62172390). Shimin Chen is the corresponding author.\nREFERENCES\n[1]Rasmus Resen Amossen and Rasmus Pagh. 2009. Faster join-projects and sparse\nmatrix multiplications. In Proceedings of the 12th International Conference on\nDatabase Theory . 121â€“126.\n[2]Cagri Balkesen, Gustavo Alonso, Jens Teubner, and M Tamer Ã–zsu. 2013. Multi-\ncore, main-memory joins: Sort vs. hash revisited. Proceedings of the VLDB\nEndowment 7, 1 (2013), 85â€“96.\n[3]Austin R Benson and Grey Ballard. 2015. A framework for practical parallel fast\nmatrix multiplication. ACM SIGPLAN Notices 50, 8 (2015), 42â€“53.\n[4]JosÃ© A. Blakeley, Per-Ã…ke Larson, and Frank Wm. Tompa. 1986. Efficiently\nUpdating Materialized Views. In Proceedings of the 1986 ACM SIGMOD Interna-\ntional Conference on Management of Data, Washington, DC, USA, May 28-30,\n1986 . ACM Press, 61â€“71.\n[5]Sebastian BreÃŸ, Max Heimel, Norbert Siegmund, Ladjel Bellatreche, and Gunter\nSaake. 2014. Gpu-accelerated database systems: Survey and open challenges. In\nTransactions on Large-Scale Data-and Knowledge-Centered Systems XV . Springer,\n1â€“35.\n[6]Walter Cai, Magdalena Balazinska, and Dan Suciu. 2019. Pessimistic cardinality\nestimation: Tighter upper bounds for intermediate join cardinalities. In Proceed-\nings of the 2019 International Conference on Management of Data . 18â€“35.\n[7]IvÃ¡n Cantador, Peter Brusilovsky, and Tsvi Kuflik. 2011. Second workshop on\ninformation heterogeneity and fusion in recommender systems (HetRec2011). In\nProceedings of the fifth ACM conference on Recommender systems . 387â€“388.\n[8]Deepayan Chakrabarti, Yiping Zhan, and Christos Faloutsos. 2004. R-MAT: A\nrecursive model for graph mining. In Proceedings of the 2004 SIAM International\nConference on Data Mining . SIAM, 442â€“446.\n[9]Steven Dalton, Luke Olson, and Nathan Bell. 2015. Optimizing sparse ma-\ntrixâ€”matrix multiplication for the gpu. ACM Transactions on Mathematical\nSoftware (TOMS) 41, 4 (2015), 1â€“20.\n[10] Shaleen Deep, Xiao Hu, and Paraschos Koutris. 2020. Fast join project query\nevaluation using matrix multiplication. In Proceedings of the 2020 ACM SIGMOD\nInternational Conference on Management of Data . 1213â€“1223.\n[11] Shaleen Deep, Xiao Hu, and Paraschos Koutris. 2021. Enumeration Algorithms\nfor Conjunctive Queries with Projection. In 24th International Conference on\nDatabase Theory, ICDT 2021, March 23-26, 2021, Nicosia, Cyprus (LIPIcs) ,\nKe Yi and Zhewei Wei (Eds.), V ol. 186. Schloss Dagstuhl - Leibniz-Zentrum fÃ¼r\nInformatik, 14:1â€“14:17.\n[12] FranÃ§ois Le Gall and Florent Urrutia. 2018. Improved rectangular matrix multi-\nplication using powers of the Coppersmith-Winograd tensor. In Proceedings of\nthe Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms . SIAM,\n1029â€“1046.\n[13] Ken Goldberg, Theresa Roeder, Dhruv Gupta, and Chris Perkins. 2001. Eigentaste:\nA constant time collaborative filtering algorithm. information retrieval 4, 2 (2001),\n133â€“151.\n[14] Goetz Graefe and Harumi Kuno. 2011. Modern B-tree techniques. In 2011 IEEE\n27th International Conference on Data Engineering . IEEE, 1370â€“1373.\n[15] F Maxwell Harper and Joseph A Konstan. 2015. The movielens datasets: History\nand context. Acm transactions on interactive intelligent systems (tiis) 5, 4 (2015),\n1â€“19.\n[16] Axel Hertzschuch, Claudio Hartmann, Dirk Habich, and Wolfgang Lehner. 2021.\nSimplicity Done Right for Join Ordering.. In CIDR .\n[17] VanPhi Ho and Dong-Joo Park. 2016. A survey of the-state-of-the-art b-tree\nindex on flash memory. International Journal of Software Engineering and Its\nApplications 10, 4 (2016), 173â€“188.\n[18] Xiao Hu and Ke Yi. 2020. Parallel Algorithms for Sparse Matrix Multiplication\nand Join-Aggregate Queries. In Proceedings of the 39th ACM SIGMOD-SIGACT-\nSIGAI Symposium on Principles of Database Systems . 411â€“425.\n[19] Yu-Ching Hu, Yuliang Li, and Hung-Wei Tseng. 2021. TCUDB: Accelerating\nDatabase with Tensor Processors. arXiv preprint arXiv:2112.07552 (2021).\n[20] Yannis E Ioannidis and Younkyung Cha Kang. 1991. Left-deep vs. bushy trees:\nAn analysis of strategy spaces and its implications for query optimization. In\nProceedings of the 1991 ACM SIGMOD international conference on Management\nof data . 168â€“177.\n[21] Shunsuke Kanda, Kazuhiro Morita, and Masao Fuketa. 2017. Practical string\ndictionary compression using string dictionary encoding. In 2017 International\nConference on Big Data Innovations and Applications (Innovate-Data) . IEEE,\n1â€“8.\n[22] Jeremy Kepner and John Gilbert. 2011. Graph algorithms in the language of\nlinear algebra . SIAM.\n[23] Changkyu Kim, Tim Kaldewey, Victor W Lee, Eric Sedlar, Anthony D Nguyen,\nNadathur Satish, Jatin Chhugani, Andrea Di Blas, and Pradeep Dubey. 2009.Sort vs. hash revisited: Fast join implementation on modern multi-core CPUs.\nProceedings of the VLDB Endowment 2, 2 (2009), 1378â€“1389.\n[24] Onur Kocberber, Boris Grot, Javier Picorel, Babak Falsafi, Kevin Lim, and\nParthasarathy Ranganathan. 2013. Meet the walkers accelerating index traver-\nsals for in-memory databases. In 2013 46th Annual IEEE/ACM International\nSymposium on Microarchitecture (MICRO) . IEEE, 468â€“479.\n[25] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe case for learned index structures. In Proceedings of the 2018 International\nConference on Management of Data . 489â€“504.\n[26] Jakub Kurzak, Wesley Alvaro, and Jack Dongarra. 2009. Optimizing matrix\nmultiplication for a short-vector SIMD architectureâ€“CELL processor. Parallel\nComput. 35, 3 (2009), 138â€“150.\n[27] Per-Ã…ke Larson, Jonathan Goldstein, and Jingren Zhou. 2004. MTCache: Trans-\nparent Mid-Tier Database Caching in SQL Server. In Proceedings of the 20th\nInternational Conference on Data Engineering, ICDE 2004, 30 March - 2 April\n2004, Boston, MA, USA . IEEE Computer Society, 177â€“188.\n[28] Per-Ã…ke Larson and H. Z. Yang. 1985. Computing Queries from Derived Relations.\nInVLDBâ€™85, Proceedings of 11th International Conference on Very Large Data\nBases, August 21-23, 1985, Stockholm, Sweden , Alain Pirotte and Yannis Vassiliou\n(Eds.). Morgan Kaufmann, 259â€“269.\n[29] FranÃ§ois Le Gall. 2012. Faster algorithms for rectangular matrix multiplication.\nIn2012 IEEE 53rd annual symposium on foundations of computer science . IEEE,\n514â€“523.\n[30] Viktor Leis, Bernhard Radke, Andrey Gubichev, Alfons Kemper, and Thomas\nNeumann. 2017. Cardinality Estimation Done Right: Index-Based Join Sampling..\nInCidr.\n[31] Jure Leskovec, Lada A. Adamic, and Bernardo A. Huberman. 2007. The dynamics\nof viral marketing. ACM Trans. Web 1, 1 (2007), 5. https://doi.org/10.1145/\n1232722.1232727\n[32] Jure Leskovec, Kevin J Lang, Anirban Dasgupta, and Michael W Mahoney. 2009.\nCommunity structure in large networks: Natural cluster sizes and the absence of\nlarge well-defined clusters. Internet Mathematics 6, 1 (2009), 29â€“123.\n[33] Priti Mishra and Margaret H Eich. 1992. Join processing in relational databases.\nACM Computing Surveys (CSUR) 24, 1 (1992), 63â€“113.\n[34] Richard C Murphy, Kyle B Wheeler, Brian W Barrett, and James A Ang. 2010.\nIntroducing the graph 500. Cray Users Group (CUG) 19 (2010), 45â€“74.\n[35] Marco Pegoraro, Merih Seran Uysal, and Wil MP van der Aalst. 2020. Efficient\ntime and space representation of uncertain event data. Algorithms 13, 11 (2020),\n285.\n[36] Yuan Qiu, Yilei Wang, Ke Yi, Feifei Li, Bin Wu, and Chaoqun Zhan. 2021.\nWeighted Distinct Sampling: Cardinality Estimation for SPJ Queries. In Proceed-\nings of the 2021 International Conference on Management of Data . 1465â€“1477.\n[37] Erik Saule, Kamer Kaya, and Ãœmit V Ã‡atalyÃ¼rek. 2013. Performance evaluation of\nsparse matrix multiplication kernels on intel xeon phi. In International Conference\non Parallel Processing and Applied Mathematics . Springer, 559â€“570.\n[38] Donovan A Schneider and David J DeWitt. 1990. Tradeoffs in processing complex\njoin queries via hashing in multiprocessor database machines . University of\nWisconsin-Madison. Computer Sciences Department.\n[39] Malte Skarupke. 2017. I Wrote The Fastest Hashtable.\nhttps://probablydance.com/2017/02/26/i-wrote-the-fastest-hashtable/.\n[40] V olker Strassen. 1969. Gaussian elimination is not optimal. Numer. Math. 13\n(1969), 354â€“356.\n[41] Alexander van Renen, Viktor Leis, Alfons Kemper, Thomas Neumann, Takushi\nHashida, Kazuichi Oe, Yoshiyasu Doi, Lilian Harada, and Mitsuru Sato. 2018.\nManaging non-volatile memory in database systems. In Proceedings of the 2018\nInternational Conference on Management of Data . 1541â€“1555.\n[42] Endong Wang, Qing Zhang, Bo Shen, Guangyong Zhang, Xiaowei Lu, Qing\nWu, and Yajuan Wang. 2014. Intel math kernel library. In High-Performance\nComputing on the IntelÂ® Xeon Phiâ„¢ . Springer, 167â€“188.\n[43] Jaewon Yang and Jure Leskovec. 2015. Defining and evaluating network commu-\nnities based on ground-truth. Knowledge and Information Systems 42, 1 (2015),\n181â€“213.\n[44] Jaewon Yang and Jure Leskovec. 2015. Defining and evaluating network commu-\nnities based on ground-truth. Knowledge and Information Systems 42, 1 (2015),\n181â€“213.\n14",
  "textLength": 84592
}