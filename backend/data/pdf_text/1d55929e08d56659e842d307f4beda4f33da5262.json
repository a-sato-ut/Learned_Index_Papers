{
  "paperId": "1d55929e08d56659e842d307f4beda4f33da5262",
  "title": "Uniform Convergence Bounds for Codec Selection",
  "pdfPath": "1d55929e08d56659e842d307f4beda4f33da5262.pdf",
  "text": "Uniform Convergence Bounds for Codec Selection\nClayton Sanford, Cyrus Cousins, Eli Upfal\nDecember 20, 2018\nAbstract\nWe frame the problem of selecting an optimal audio encoding scheme as a supervised learning task.\nThrough uniform convergence theory, we guarantee approximately optimal codec selection while controlling\nfor selection bias. We present rigorous statistical guarantees for the codec selection problem that hold for\narbitrary distributions over audio sequences and for arbitrary quality metrics. Our techniques can thus\nbalance sound quality and compression ratio, and use audio samples from the distribution to select a\ncodec that performs well on that particular type of data. The applications of our technique are immense,\nas it can be used to optimize for quality and bandwidth usage of streaming and other digital media, while\nsigniﬁcantly outperforming approaches that apply a ﬁxed codec to all data sources.\n1 Introduction\nLarge amounts of digital media are downloaded and streamed on the web by a growing number of users, often\nacross long distances and over low-bandwidth mobile connections. Transmitting audio and video signals with\nperfect ﬁdelity is generally prohibitively expensive, so often lossy encoder-decoders (codecs) are employed,\nwhich compress media to a much smaller size, but only approximate the original signal. Depending on the\nintended use of the data, diﬀerent types of signal degradation may or may not be acceptable, and no-free-lunch\nanalysis tells us that no codec will be optimal for all data distributions or all notions of quality. In this paper,\nwe analyze the problem of statistically signiﬁcant codec selection , where the goal is to, given a set of codecs, a\nnotion of quality, and a sample of media sequences, select a codec that with high probability , isapproximately\noptimalover the distribution from which the media sequences were drawn.\nWe allow for arbitrary objective quality metrics, which for example may consider both compression ratios\nand perceptual quality metrics. We want to select the optimal codec for a particular data distribution with\nrespect to some objective, thus both the data distribution and the objective inﬂuence this choice. For example,\na speech codec could have high quality and low size on speech, but poor quality on music, while a music\ncodec may produce larger encodings, but with better quality on music. Depending on our choice of metric, we\nmay ﬁnd that the speech codec is optimal on speech data, wheras the music codec is optimal for music data.\nWe do not assume knowledge of the distribution from which sequences are drawn, requiring only a sample\nfrom this distribution. Under this framing, codec selection becomes a supervised learning problem , as we are\ngiven data, and tasked with selecting a codec that performs well over the data distribution. When selecting\nbetween a family Hof codecs, multiple hypothesis testing issues arise. We control these issues through\nuniform convergence theory , through which we guarantee that with high probability, the empirical performance\nof all codecs over our sample simultaneously approximates their performance over the data distribution.\nSpeciﬁcally, we guarantee that with high probability, the selected codec ^hperforms approximately as well on\naverage as any h2Hover the distribution. We also handle more complicated constrained queries, which seek\nthe optimal codec that satisﬁes some property, for instance the highest-quality codec that achieves a 50%\ncompression ratio or better.\nWe propose two general meta-algorithms that probabilistically bound the objective function and ﬁnd\nthis optimal encoding function by process of elimination. Each may be instantiated with a particular type\nof uniform convergence bound, which we describe in detail for bounds based on the empirical maximum\ndiscrepancy [Bartlett et al., 2002] (both under a boundedness assumption, and in an unbounded setting,\nwith a novel asymptotic uniform convergence tail bound), as well as several types of union bound . Our ﬁrst\nmeta algorithm is Global Sampling , which bounds the means of each criteria and selects a scheme if its\n1arXiv:1812.07568v1  [cs.SD]  18 Dec 2018\n\nobjective is optimal with high conﬁdence — that is, if its conﬁdence interval does not overlap with that of\nany other scheme. However, this algorithm can be wasteful of computation and data, as it applies all codecs\nto all samples. We also introduce the Progressive Sampling with Pruning (PSP) algorithm, which adaptively\nprunes provably suboptimal codecs over time, and terminates when a desired approximation threshold is met,\nthus reducing computation and data consumption. Pruning also improves the statistical performance, as\nPSP employs uniform guarantees over subsets of the original codec family, and thus obtains the beneﬁts of\nlocalization [Bartlett et al., 2005].\nThe bounds and algorithms presented here encourage customizable and resource-eﬃcient ways to make a\nstatistically conﬁdent data-driven decision that is appropriate to the task at hand. They allow users to ensure\nselection of an optimal encoding scheme given a wide range of preferences and to navigate the statistical\ntrade-oﬀs of encoding. Because storage of data sequences on disk space and personal devices are often more\nlimited than server computation, users ensure that their restrictions on storage space and reconstruction\nquality are addressed with statistical conﬁdence. Further, our progressive sampling algorithm avoids wasted\ncomputation time, and users can tweak the conﬁdence parameter to balance the trade-oﬀ between conﬁdence\nand size of training sample.\nWe compare the eﬀectiveness of union and uniform convergence bounds at establishing tight bounds and\npruning codecs with these algorithms. We examine both traditional ﬁnite-sample uniform convergence bounds\nand novel asymptotic uniform convergence bounds bounds, which bound variances and leverage central limit\ntheorems (classical and Martingale [Brown et al., 1971]). Using a dataset of segments from audio books\nwith a variety of MP3 encoding schemes, we ﬁnd that the asymptotic bounds outperform the exact bounds.\nBecause we use few encoding schemes, the union bounds produce tighter bounds experimentally; however,\nthe uniform-convergence bounds will outperform the union bounds with a larger class of codecs.\nIn summary, the contributions of our paper are as follows:\n1. We frame codec selection as asupervised learning problem .\n2.We apply uniform converge theory to obtain generalization bounds for the codec selection problem, which\nwe contrast with union bounds.\n3.We show novel asymptotic uniform convergence bounds for unbounded variables under a ﬁnite variance\nassumption.\n4.We deﬁne the progressive sampling with pruning algorithm for adaptive codec search , with global general-\nization guarantees.\n5.We present experiments showing that our asymptotic bounds outperform the ﬁnite-sample bounds. Our\nexperiments use an insuﬃcient number of codecs for uniform convergence bounds to outperform union bounds,\nthough we argue that this trend reverses for larger codec families.\n2 Background\nHere we present relevant background in statistics and uniform convergence theory, as well as background on\naudio compression.\n2.1 Uniform Converge and the Empirical Maximum Discrepancy\nA core goal of uniform convergence theory is to obtain bounds on the expectation and tails of the supremum\ndeviation , deﬁned as\nsup\nh2HE\nx0sD[h(x0)]\u0000^E[h(x)]\ngiven distribution DoverX, samplexsDm, and function family H\u0012X! R. Related quantities, such as\nthe supremum over absolute diﬀerences between true and empirical expectation, are also of interest. In the\ncodec selection domain, we aim to estimate the means of certain criteria by computing intervals that contain\nthe true mean with high probability. Bounding on the diﬀerence between true and empirical means allow\nus to be certain that what works well empirically will with high probability work approximately as well on\nsimilarly distributed data.\nClassical asymptotic results abound and are routinely used in applied statistics. These results, such as\nthe Glivenko-Cantelli theorem [Cantelli, 1933, Glivenko, 1933], hold asymptotically as the sample size tends\n2\n\nto inﬁnity. The conclusions we may draw from ﬁnite-sample results, such as the DKW inequality [Dvoretzky\net al., 1956, Massart, 1990], which may be viewed as a special-case of the Vapnik-Chervonenkis [Chervonenkis\nand Vapnik, 1971] inequality hold for ﬁnite-samples. Because asymptotically negligible terms cannot be\nneglected, asymptotic results are often substantially harder to prove. However, these results are generally far\nmore valuable to scientiﬁc and empirical inquiry, as they have the potential to yield \u000f-\u000eprobabilistic bounds\nforﬁnite samples . Early results — such as VC-dimension — were distribution-free , meaning the bounds\napplied for anyDoverX. Because distribution-free bounds are necessarily worst-case, more recent work\nhas favored distribution-dependent anddata-dependent bounds, which may depend on Dor properties ofD\nestimated through x.\nBartlett et al. [2002] introduced Empirical Maximum Discrepancy (EMD) to obtain uniform convergence\nbounds, which are similar to the better-known Rademacher Complexity (RC) [Koltchinskii, 2001, Bartlett and\nMendelson, 2002] bounds. Both of these sample complexity measurements are distribution-dependent. While\nthey are primarily used for obtaining generalization bounds forsupervised learning in the statistical learning\ntheory literature, they have also been applied to unsupervised learning andsampling problems [Riondato and\nUpfal, 2015, 2018].\nDeﬁnition 2.1 (Empirical Maximum Discrepancy [EMD]) .Given a sample x2Xm, where 2jm, and a\nfunction familyH\u0012X! R, the EMD is deﬁned as\n~Dm(H;x):= sup\nh2H1\nmmX\ni=1(\u00001)ih(xi)\nWe use the EMD generalization bounds of Bartlett et al. [2002] to bound the diﬀerence between observed\nand true errors, which we use in an online manner in our algorithm to bound the values of various criteria of\nour encoding schemes.\nTheorem 2.1 (Finite-Sample EMD Uniform Convergence Bounds) .LetHbe a set of functions representing\nthe errors of hypotheses such that h:X ! [0;1],8h2H, whereXrepresents the feature space, and let\nxsDm, and\u000e2(0;1). With probability at least 1\u0000\u000e,\nsup\nh2HE\nxsD[h(x)]\u00001\nmmX\ni=1h(xi)\u00142~Dm(F;x) + 3s\nln(1\n\u000e)\n2m:\nFurthermore, also with probability at least 1\u0000\u000e,\nsup\nh2H\f\f\f\f\fE\nxsD[h(x)]\u00001\nmmX\ni=1h(xi)\f\f\f\f\f\u00142~Dm(F;x) + 3s\nln(2\n\u000e)\n2m:\nThese results are trivial consequences of a symmetrization inequality and McDiarmid’s ﬁnite diﬀerence\ninequality [McDiarmid, 1989].\nSo far, we have made the boundedness assumption, where values must lie on [0;1]. Through linear\ntransformation, this is suﬃcient to handle any bounded interval, which through Hoeﬀding’s inequality or\nMcDiarmid’s inequality results in ﬁnite-sample tail bounds. However, often we can’t assume values or\nbounded, and even if they are, it may be the case that they are concentrated within a tiny region of their\ninterval, and consequently we obtain extremely wide conﬁdence intervals. We address both of these issues\nwith our asymptotic uniform convergence bounds, presented in section 3.\n2.2 On Union Bounds\nWe now present analogues to the above uniform convergence bounds, where the supremum deviation is\nbounded with a union bound over applications of Hoeﬀding’s inequality [Hoeﬀding, 1963] (for bounded\nrandom variables) or a Gaussian Chernoﬀ bound (under an asymptotic normality assumption).\nTheorem 2.2 (Finite-Sample Hoeﬀding Union Bound) .SupposeH\u0012X! [0;1]. The following holds with\nprobability at least 1\u0000\u000eoverxsDm:\n3\n\nsup\nh2H\f\f\f\f\fE\nx0sD[h(x0)]\u00001\nmmX\ni=1h(xi)\f\f\f\f\f\u0014vuutln\u0010\n2jHj\n\u000e\u0011\n2m\nThis result is a straightforward consequence of Hoeﬀding’s inequality for each h2H, which simultaneously\nhold due to a union bound.\nTheorem 2.3 (Asymptotic Gaussian-Chernoﬀ Union Bound) .SupposeH\u0012X! R, andxsDm: Suppose\nalso that Vx0sD[h(x0)]is ﬁnite for all h2H. Now, take ^\u001bto be the maximum empirical variance among all\nh2Hoverx. The following holds, asymptotically in m, with probability at least 1\u0000\u000eover choice of x:\nsup\nh2H\f\f\f\f\fE\nx0sD[h(x0)]\u00001\nmmX\ni=1h(xi)\f\f\f\f\f\u00142^\u001bvuutln\u0010\n2jHj\n\u000e\u0011\n2m\nThis result is a trivial consequence of a Gaussian Chernoﬀ bound for each h2H, which hold together by\nunion bound. Naturally, one may wonder whether the uniform convergence bounds are tighter than the union\nbounds. We expect the EMD bounds to outperform the union bounds when bounding the generalization\nof large hypothesis classes because the union bounds scale with increases in jHj. Furthermore, the union\nbounds cannot handle cases with inﬁnitely-sized codec classes due to their dependence on jHj, which may\noccur if we aim to select an encoding function with a continuous parameter. The following theorem shows\nthat EMD bounds may also be superior for suﬃciently large sample sizes.\nTheorem 2.4. The following performance results about the Hoeﬀding-Union bound of theorem 2.2 hold:\n1.The Hoeﬀding bound will always outperform the McDiarmid ﬁnite-sample EMD bound of Theorem 2.1\nif:\njHj\u0014\u00122\n\u000e\u00138\n2. It always outperforms the asymptotic EMD bound of Theorem 3.1 if:\njHj\u0014\u00122\n\u000e\u0013\u001b2(2+2p\n2)2\n\u0001\u000e\n3\nwhere\u001bis deﬁned as is in Theorem 3.1.\nProof.To see this, note that the EMD term of the EMD bound is always non-negative, and thus if its\nMcDiarmid term exceeds the Hoeﬀding term of the union bound, the Hoeﬀding union bound is superior.\n1. The McDiarmid term of the ﬁnite-sample EMD bound is larger when:\nvuutln\u0010\n2jHj\n\u000e\u0011\n2m\u00143s\nln\u00002\n\u000e\u0001\n2m\nSolving forjHjobtains the desired result.\n2. Here, McDiarmid term of the asymptotic EMD bound is always greater when:\nvuutln\u0010\n2jHj\n\u000e\u0011\n2m\u0014\u001b(2 + 2p\n2)s\nln\u00003\n\u000e\u0001\n2m\nAgain, a simple algebraic manipulation is suﬃcient to obtain the desired inequality.\n4\n\n2.3 Codecs and Compression\nAcodec— a combination of the words “coder” and “decoder” — is an algorithm that encodes a sequence to a\ndiﬀerent data format and decodes it back. Codecs are applicable to domains like audio, video, and they cover\na variety of purposes:\n\u000fCompression codecs reduce the memory needed to store a sequence while maintaining most of its the its\ndata. We evaluate the success of a compression scheme metrics such as the amount of memory needed\nto store compressed sequences and the similarity between the original sequence and its decompressed\ncounterpart. Lossless compression schemes perfectly recall the original sequences when decoded, and\nlossyschemes create encodings that require less memory while losing some of the original sequence’s\ndata.\n\u000fError-correcting codecs add redundancy to a sequence to protect its contents in the event that bits are\ncorrupted. Eﬀective error-correcting encoding schemes have relatively small encodings and minimize\ndamage to the original data when subject to random noise.\n\u000fEncryption codecs obfuscate a sequence to prevent a malicious adversary from obtaining an unobfuscated\nrecord. Creators of encryption schemes aim to minimize the probability that the original sequence can\nbe obtained from the obfuscated form, while ensuring that the algorithm quickly and without intensive\ndata needs.\nWhile we apply our algorithms to audio compression codecs in this paper, our codec selection algorithms\naccommodate other types of codecs with diﬀerent success criteria. We speciﬁcally compare various MP3\nencoding schemes, which are lossy compression algorithms that vary dramatically in the amount of data\nneeded for compressed ﬁles.\n2.4 Codec Selection Problem\nThe codec selection problem involves choosing an encoding scheme for a domain like audio, video, or images\nthat meets certain criteria and maximizes an objective dependent on those criteria. Speciﬁcally, we examine\nthe audio domain and create a sampling procedure that determines which audio compression scheme performs\nbest with respect to measurements like the size of the compressed data sequences and the similarity between\nthe original sequence and the decompressed version.\nThis problem is signiﬁcant because an optimal compression algorithm should produce a similar sequence\nto the original and compress to a very small size. These two goals are at odds. All lossless compression\nschemes are fundamentally limited in compressibility by entropy bounds; therefore, no algorithm can strictly\ndominate this optimal compression scheme with respect to both decompressed similarity and compression\nratio. Furthermore, the more approximation that is allowed, the better the possible rate of compression, but\nquality is negatively impacted as well. We conclude that “there is no free lunch” with compression algorithms;\na user’s relative preferences for ﬁdelity, compressibility, and other measures favor diﬀerent compression\nalgorithms.\nGupta and Roughgarden [2017] examined this kind of problem from a PAC-learning approach. They\nbounded the performance of diﬀerent algorithms using bounds based on pseudo-dimension, which generalizes\nthe VC-dimension and associated bounds from binary classiﬁcation toregression .\nTheorem 2.5 (Pseudodimension Bounds) .LetHbe a ﬁnite class of functions with domain Xand range in\n[0;H]. Then, there exists constant csuch that for all distributions DoverX,\u000e2(0;1], andm2Z+:\nPrx1;:::;xm D2\n4\f\f\f\f\f1\nmmX\ni=1h(xi)\u0000E\nxsD[h(x)]\f\f\f\f\f<Hvuutc\nmln \njHj1\nln 2\n\u000e!3\n5\u00151\u0000\u000e\nWe build on their model by replacing their distribution-free pseudo-dimension bounds with data-dependent\nEMD bounds. Rather than simply ﬁnding a function that outperforms others with high conﬁdence, we further\nexpanded their model by also seeking functions that must meet certain constraints. For example, in the\naudio domain, we might seek a compression algorithm that minimizes the amount of memory needed for the\n5\n\ncompression sequence while requiring that a compressed and decompressed sequence is suﬃciently similar to\nthe original sequence.\n2.5 Perceptual Audio Models\nWe apply our framework to the the speciﬁc codec selection problem of audio compression, which requires\nthat we select a notion of reconstruction error. While a normalized square error is mathematically convenient\nand sensible for comparing two vectors, other measures of distance are better tailored to audio data. Because\naudio ﬁles are almost always intended for human listening, we redeﬁne success for denoising algorithms to be\nhow much an decompressed sequences diﬀers from the original sequence according to a human listener . To\nmeasure this, we employ the techniques of perceptual audio models , which attempt to measure how clearly\nhumans hear diﬀerent sounds.\nA central idea in perceptual models is noise masking , which occurs when the clean audio signals dominate\nthe added noise from the algorithm in terms of human perception; that is, listeners only hear the certain\nstrong frequencies that drown out other similar frequencies [Jayant et al., 1993]. Some perceptual models are\nsubjective , meaning that they measure audio quality based on human ratings of sound quality, where listeners\nassess how much a modiﬁed signal resembles the reference signals. Others, like PEAQ (perceptual evaluation\nof audio quality) are objective, and are based more directly on psychoacoustic principles without relying on\nhuman-supplied data[Thiede et al., 2000].\nIn this paper, we use an objective measure for how much compression perturbs each given sound. This\nmethod serves as a stand-in for human listeners; we could replace PEAQ with a human listener who rates\ntwo sounds by how similar they sound between 0 and 1. Given a better measurement of perceptual audio\nquality than PEAQ — whether algorithmic or human-dependent — we could incorporate it in our algorithms\nas a quantity to optimize; our algorithm adapts to the provided perceptual quality metric.\n3 A Novel Scale-Sensitive Uniform Convergence Theorem for Un-\nbounded Random Variables\nWhile theorem 2.1 is appropriate when the range of values is known, this is not always the case, and even\nwhen it is known, the result is quite loose when most of the probability mass is concentrated in a relatively\nsmall portion of the range. We now present a scale-sensitive asymptotic uniform convergence bound that\nassumes only ﬁnite variance of each h2H. Our bound is based on analysis of the variance of the EMD.\nComputing variances of suprema of empirical processses is a rather subtle topic (the interested reader is\nreferred to [Boucheron et al., 2013]), but for our purposes, it suﬃces to understand the weak variance , which\ncomes out to\n1\nmE\nx\"\nsup\nh2H1\nmmX\ni=1(h(xi)\u0000E\nx0sD[h(x0)])2#\n;\nand thewimpy variance , which commutes the supremum, and comes out to\n1\nmsup\nh2HV\nx0sD[h(x0)]:\nBy the Efron-Stein inequality, their sum upper-bounds the variance of the EMD, and both have the same\nplugin estimator, which we use to obtain asymptotic bounds without a priori variance knowledge.\nTheorem 3.1 (Asymptotic EMD Uniform Convergence Bounds) .SupposeH\u0012X! RwithjHj<1, and\nthatVx[h(x)]exists8h2H, and takexsDm. Now, take either true variance bound\n\u001b2:=E\nx\"\nsup\nh2H1\nmmX\ni=1(h(xi)\u0000E\nx0sD[h(x0)])2#\n+ sup\nh2HV\nx0sD[h(x0)] 2 sup\nh2HV\nx0sD[h(x0)];\norplugin variance bound estimate:\n\u001b2:= 2 sup\nh2H^V[h(x)]:\nThe following then hold (asymptotically w.r.t. sample size m):\n6\n\n1.sup\nh2HE\nx0sD[h(x0)]\u0000^E[h(x)]and ~Dm(H;x) mGaussian with variance \u00142\nm\u001b2.\n2.P0\n@sup\nh2HE\nx0sD[h(x0)]\u0000^E[h(x)]\u0015p\n2~Dm(H;x) +\u001b(2 + 2p\n2)s\nlog(2\n\u000e)\n2m1\nA.m\u000e.\n3.P0\n@sup\nh2H\f\f\f\fE\nx0sD[h(x0)]\u0000^E[h(x)]\f\f\f\f\u0015p\n2~Dm(H;x) +\u001b(2 + 2p\n2)s\nlog(3\n\u000e)\n2m1\nA.m\u000e.\nHere 1 shows that the quantities of interest (the supremum deviation and the EMD) are asymptotically\nGaussian, and computes their variance. This is not entirely obvious, as due to the suprema in both quantities,\nthe classical central limit theorem does not apply. 2 then exploits this asymptotic normality to obtain a\n1-tailed asymptotic conﬁdence interval via Gaussian Chernoﬀ bounds, and 3 is the 2-tailed bound, which\nrequires a conﬁdence interval wider by a factor ofq\nlog(3\n\u000e)=log(2\n\u000e).\nThis result should be compared with the ﬁnite-sample bounded-diﬀerence results of theorem 2.1. In that\ncase,\u001b2\u00141(due to boundedness). To isolate the eﬀects of changing the concentration inequality from the\neﬀects of the 2factor being asymptotically replaced by ap\n2, we use 2~Dm(H;x)instead ofp\n2~Dm(H;x),\nwhich yields a concentration inequality term of \u001b6p\n2q\nlog(2\n\u000e)\n2m, as opposed to the \u001b3q\nlog(1\n\u000e)\n2m. Thus here,\nwithout making a boundedness assumption, we obtain a result that diﬀers only in the constants. Often\n\u001b2\u001c1, in which case the asymptotic bound is tighter, and the asymptotic result also applies in situations\nwhere values are unbounded, thus it is applicable in a far broader domain.\n4 Codec Selection Algorithms\nWe present algorithms that determine the optimal codec in Hthat satisﬁes the constraints. Both algorithms\nrely on placing probabilistic bounds on the means for each criterion to obtain intervals where each mean lie\nwith high probability. If an intervals lies within the constraint space, we conclude that the corresponding\nfunction is satisfactory. If the conﬁdence intervals for two objectives do not overlap, then we can conclude\nthat the greater objective is less optimal with high probability.\nThe Global Sampling (GS) algorithm in Section 4.2 creates those intervals by encoding all samples with\neach function. The Progressive Sampling with Pruning (PSP) algorithm in Section 4.3 diﬀers by ﬁnding the\nintervals with only some of the samples and disqualifying suboptimal or unsatisfactory functions in an online\nmanner. The two algorithms use the same bounds to construct conﬁdence intervals, but they take diﬀerent\nsampling approaches. The GS algorithm processes non-adaptively because it encodes and decodes all samples\nregardless. The PSP eliminates codecs that are suﬃciently unlikely of being optimal and chooses samples\nadaptively from remaining codecs. Adaptive selection has the advantage of avoiding wasted time o samples\nwith clearly sub-optimal codecs.\nThese algorithms diﬀer from multi-armed bandit approaches because they apply every sample to each\ncodec. Bandit-based algorithms must draw each sample i.i.d., which means that samples cannot be reused on\ndiﬀerent codecs and requires a much larger data source. Moreover, bandit bounds do not take advantage of\nuniform convergence properties, so we expect them to perform worse than our algorithms with EMD bounds\nwhen there is a large class of codecs.\nFirst, we introduce the notation for the model that is used for both algorithms.\n4.1 Problem Formulation\nFor our model, the encoding schemes are represented as a class of functions Hwithh:X!YwhereX\nis the set of input objects to encode and Yis a representation of the encoding along with useful metadata\nabout the transformation. We also have a set of criteria Cthat describes the performance of the function\nwithc:X\u0002Y! [0;1]forc2C. In the audio compression domain, Hcontains compression schemes like\nMP3, whileCcontains measurements like the ratio of the size of the original data sequence to the size of the\ncompressed sequence.\n7\n\nWe measure the success of an encoding with an objective V:X\u0002Y! Rsuch thatV(x;y)is a linear\ncombination of c(x;y)forc2C. Thus, we can also express Vas a function of a vector of criterion values:\nV(u)for where each element of ucorresponds to some c(x;h(x))forc2C. We also seek functions that satisfy\nsome set convex combination of linear inequality constraints in terms of the criteria: let W\u0012Ybe the region\nof the function’s range such that those constraints are satisﬁed. Thus, we seek h\u00032Hsuch that with high\nprobability for xdrawn fromXover some distribution and for all h2H,V(x;h\u0003(x))\u0014V(x;h(x)) +\u000fand\nh\u0003(x)2W. Because two functions could be arbitrarily similar, we can only guarantee that our function’s\nobjective mean is no more than \u000fworse than the optimal with high probability.\nFor simplicity, deﬁne c\u000ehsuch that (c\u000eh)(x) =c(x;h(x))forc2C;h2H. In addition, we let\nc\u000eH=fc\u000eh:h2Hgrepresent a function class for corresponding to each criterion.\n4.2 Global Sampling Algorithm\nThe global sampling algorithm encodes every sample in S\u0012Xwith each codec in H. It computes an empirical\nmean ^eh;cand a conﬁdence interval ^Eh;cfor the means of each criterion c2Cwith each codec h2H. Based\non those bounds, we can determine which hypotheses reside within the constraint space with high probability,\nand we also ﬁnd similar means and bounds for the objective for each codec. We assume an objective function\nVthat is a nonnegative weighted sum of criteria (occasionally writing V\u0001cfor criteria vector to specify this),\nand estimate the optimal h2Hw.r.t. objective V. We also allow for a constraint space W, in which case we\nestimate the optimal codec known to satisfy H, as well as some auxilliary information used to quantify the\nquality of the estimation.\nWhen assessing a whether a codec meets the speciﬁed constraints with conﬁdence on a set of samples,\nthere are three possible conclusions: with conﬁdence, the codec meets the constraints; with conﬁdence, the\ncodec does notmeet the constraints; or neither conclusion can be mode conﬁdently. This poses a question;\nif the algorithm is inconclusive about the constraint satisﬁability of the codec with the smallest empirical\nmean, should report that codec as the optimal scheme? This motivates a need for liberalandconservative\nselections, where codecs with undetermined constraint satisfaction are eligible to be optimal according to\nliberal selection, but not for conservative. This distinction is further necessary because some criterion’s\nsatisﬁability may be impossible to verify. If its true mean lies on the constraint boundary, the conﬁdence\ninterval will never lie entirely inside or outside of the feasible region, and conﬁdence is impossible regardless\nof the number of samples.\nAlgorithm 1 GlobalSampling (S;H;C;V;W;\u000e)\n1:Input:samplesS, codec familyH, criteriaC, objective V, constraint space W, failure probability\n\u000e2(0;1)\n2:Output: liberal and conservative codec candidate sets ^hL;^hC\u0012H, criteria estimates ^e^h;c, criteria error\nbounds\u000f\n3:forc2C;h2Hdo\n4: ^eh;c 1\njSjP\nx2Sc(x;h(x))\n5:dc ~DjSj(c\u000eH;S)\n6:\u0011 3p\nln(2jCj=\u000e)=2jSj\n7:\u000fc 2dc+\u0011\n8: ^Eh;c [^eh;c\u0000\u000fc;^eh;c+\u000fc]\\[0;1]\n9:^HL fh2H:^Eh;\u0001\\W6 =;g .Set of all codecs that may satisfy W\n10:^HC fh2H:^Eh;\u0001\u0012Wg .Set of all codecs that satisfy Ww.h.p.\n11:^hL fh2^HL: infc2^Eh;:V(c)\u0014infh02^HLsupc2^Eh0;\u0001V(c)g .Select near-optimal liberal codecs\n12:^hC fh2^HC: infc2^Eh;:V(c)\u0014infh02^HCsupc2^Eh0;\u0001V(c)g.Select near-optimal conservative codecs\n13:return (^hL;^hC;^e;\u000f)\nWe obtain basic theoretical guarantees about the outcomes of the algorithm. Note that the algorithm\nuses the EMD bounds from Theorem 2.1 to deﬁne the conﬁdence intervals. Those bounds can be replaced\n8\n\nwith the bounds for conﬁdence intervals without aﬀecting the rest of the algorithm or the theoretical results\nof Theorem 4.1.\nTheorem 4.1. Suppose we run GlobalSampling (S;H;C;V;W;\u000e)and obtain (^hL;^hC;^e\u0001;\u0001;\u000f), withSdrawn\ni.i.d. fromD. TakeHW=fh2H :ExsD[C(x;h(x))]2Wgto be the subset of Hthat satisﬁesWin\nexpectation. The following hold (simultaneously) with probability at least 1\u0000\u000eover choice of S:\n1. IfHWis nonempty, then ^hLis nonempty.\n2. The set of conservative hypotheses satisﬁes ^hC\u0012HW.\n3. Each estimated mean criterion value for each codec c2Cis approximately correct, satisfying\nsup\nh2H\f\f\f\fE\nxsD[c(x;h(x))]\u0000^eh;c\f\f\f\f\u0014\u000fc:\n4. Each estimated objective value is approximately correct, satisfying\nsup\nh2H\f\f\f\fE\nxsD[V(C(x;h(x)))]\u0000^eh;c\f\f\f\f\u0014V\u0001\u000f:\n5.Assume ^hLand^hCare nonempty for the lower and upper bounds, respectively. We then bound the true\nobjective of the optimal h\u00032HWin terms of our empirical estimates as\ninf\nh2^hLV(^eh;\u0001\u0000\u000f)\u0014inf\nh\u00032HWE\nxsD[V(C(x;h\u0003(x)))]\u0014inf\nh2^hCV(^eh;\u0001+\u000f):\nProof.To show each part of the theorem, we ﬁrst note that the conclusion of theorem 2.1 holds by union\nbound for each criterion with probability at least 1\u0000\u000e. This immediately implies 3, which itself directly\nimplies 4.\nTo see 1, consider that the conﬁdence intervals about the empirical estimate of each criterion for each\nhypothesis inHcontain their true values. Consequently, any codec that satisﬁes Wwill be a member of\n^HL, and thus ^hwill be nonempty. Similarly, to see 2, ^HCis the set of hypothesis such that any possible\nconﬁguration of criterion values (within their conﬁdence intervals) satisfy W, thus all conservative codecs\n(members of ^hC) must satisfyW.\nFinally, to see 5, note that the set Hn ^HLcontains only hypotheses that are known not to satisfy W\n(with probability at least 1\u0000\u000e), thus we may ignore them, and consider only ^HLwhen lower-bounding the\ntrue optimal objective. Similarly, when upper-bounding the true objective, we must consider only codecs\nknown to satisfy W(as a codec that does not satisfy Wcould have a lower objective), thus we consider ^hC\ninstead of ^hL.\n1 characterizes situations in which we will obtain a liberal codec estimate, namely, so long as a valid\ncodec exists, we will obtain a liberal estimate with high probability. 2 tells us that the true criterion values\nassociated with conservative hypotheses always satisfy the constraints W: while it is a simple matter to\nguarantee that empirical criteria estimates satisfy W, this is subtler but more useful property. 3 and 4\ncharacterize the quality of our estimated criteria and objective values (showing they uniformly approximate\nthe true values). Finally 4 characterizes the true optimal objective value of all codecs that satisfy W, which\nis rather subtle, as we accomplish this without actually computing the optimal codec or knowing exactly\nwhich codecs satisfy W.\n4.3 Progressive Sampling with Pruning Algorithm\nOurProgressive Sampling with Pruning (PSP) procedure samples data sequences to the codec scheme from\nthe distribution of sequences in batches of increasing size. It is based on the progressive sampling method\nintroduced by [Elomaa and Kääriäinen, 2002], and later applied to unsupervised learning by [Riondato and\nUpfal, 2015, 2016]. Our technique diﬀers in that we use the results of the progressive sampling to eliminate\nfrom consideration, or prune, functions whose results will be insuﬃcient with high conﬁdence. Based on the\n9\n\nresults of each batch, we empirically estimate the means of c(x;h(x))for criterion cand hypothesis hover\nsamplesx. We bound the means with high probability with Rademacher complexity and prune functions\nwhich (1) with high conﬁdence lie outside of Wor (2) with high conﬁdence has a greater mean value of\nV(x;h(x))thanV(x;h0(x))for some other f0that is inWwith high conﬁdence.\nThe key advantage of pruning is that the statistical power of the technique is only minimally impacted by\npoorly performing functions, as these are quickly pruned, and additionally minimal computation time is spent\non these functions. Intuitively, this is similar to the idea of local Rademacher complexity , where localized\nfunction families [Bartlett et al., 2005] are centered around the optimal function in a family.\nThe algorithm is parameterized by \u000eand\u000f, whose meanings are analogous to those in PAC-Learning\nframework [Valiant, 1984]. \u000erepresents our level of certainty in each bound. That is, we require that each\nbound holds with probability 1\u0000\u000e.\u000frepresents the tightness of our bound. That is if the algorithm\nterminates, we guarantee an \u000f-optimal codec with probability at least 1\u0000\u000e. We choose any values of \u000eand\u000f,\nand the algorithm uses no more than twice as many samples as the minimum needed for suﬃciently tight\nEMD bounds.\nWe repeat this process over batches that double in size after each iteration. Because each batch has more\nsamples than the previous one, it can place tighter bounds than the preceding ones can, thus allowing it\nto be more conﬁdent in its empirical means. We then disqualify more functions because we conclude with\nconﬁdence that certain functions outperform other functions due to the shrinking conﬁdence intervals. This\nmeans that fewer functions need to be run on each subsequent batch of inputs, which limits the number of\nunnecessary encoding steps. The algorithm terminates when there is exactly one remaining function that\nsatisﬁes the constraints, when it is shown that no function satisﬁes the constraints, or when no more samples\nare available. Therefore, the algorithm ﬁnds the function with the smallest objective subject to the constraints\nwith high conﬁdence. We give pseudo-code for the algorithm in an attached ﬁgure:\nAlgorithm 2 PSP(S;s0;H;C;V;W;\u000f;\u000e)\n1:Input:samplesS, initial batch size s0, codec classH, criterion setC, objectiveV, constraint space W,\nconﬁdence\u000f2f0;1g, failure probability \u000e2f0;1g\n2:Output: liberal and conservative codec sets ^hL;^hC\u0012H, empirical criteria estimates ^e^h;c, criteria\nconﬁdence intervals ^E^h;c\n3:^Eh;c [0;1]for allh2H;c2C\n4:n blog2(jSj\ns0+ 1)c .Maximum number of iterations\n5:H1 H .Begin by considering all codecs\n6:fori2f1;:::;ngdo\n7:LetSibe2i\u00001\u0001s0unused samples from S\n8:\u0011i 3p\nln(2njCj=\u000e)=2jSij .McDiarmid’s inequality term\n9:forc2C;h2Hdo\n10: ^eh;c 1\njSijP\nx2Sic(x;h(x))\n11:dc;i ~D(c\u000eH;Si)\n12: ^Eh;c ^Eh;c\\[^eh;c\u00002dc;i\u0000\u0011i;^eh;c+ 2dc;i+\u0011i] .Compute EMD conﬁdence intervals\n13: ^hL fh2Hi: infc2^Eh;:V(c)\u0014infh02^HLsupc2^Eh0;\u0001V(c)^Eh;\u0001\\W6 =;g\n14: ^hC fh2Hi: infc2^Eh;:V(c)\u0014infh02^HCsupc2^Eh0;\u0001V(c)^Eh;\u0001\u0012Wg\n15:Hi+1 fh2Hi:^Eh;\u0001\\W6 =;g .Prune codecs that provably violate W\n16:if^hC6=;then\n17:Hi+1 fh2Hi+1: infc2^Eh;\u0001V(c)\u0014infh02^hCsupc2^Eh0;\u0001V(c)g.Prune suboptimal codecs\n18:ifjHi+1j= 1_infh2^hCsupc2^Eh;\u0001V(c)\u0014infh02^hLinfc2^Eh0;\u0001V(c) +\u000f_i=nthen\n19: return (^hL;^hC;^e;^E).Terminate if 1 codec remains, \u000f-optimality is reached, or Sis exhausted\nBased on the algorithm, we obtain analogous guarantees to theorem 4.1.\nTheorem 4.2 (Guarantees of the PSP Algorithm) .Suppose we run PSP(S;s0;H;C;V;W;\u000f;\u000e), forSdrawn\ni.i.d. fromDand obtain (^hL;^hC;^e^h;:;^E^h;:;\u000f). TakeHW=fh2H:ExsD[C(x;h(x))]2Wgto be the subset\n10\n\nofHthat satisﬁesWin expectation. Then, the following hold with probability 1\u0000\u000e(simultaneously) over\nchoice ofS:\n1. IfHWis nonempty, then ^hLis nonempty.\n2. Each conservative hypothesis h2^hCsatisﬁesh2HW.\n3. The true mean of each criteria and codec lies within our conﬁdence rectangle, satisfying\nE\nx[c(x;h(x))]2^Eh;c8c2C;h2H:\n4. The true objective of each codec lies within our conﬁdence interval for the objective, satisfying\nE\nx[V(C(x;h(x)))]2V(^Eh;:)8h2H:\n5.Assume ^hLand^hCare nonempty for the lower and upper bounds, respectively. We then bound the true\nobjective of the optimal h\u00032HWin terms of our empirical estimates as\ninf\nh2^hLmin(V(^Eh;\u0001))\u0014inf\nh\u00032HWE\nxsD[V(C(x;h\u0003(x)))]\u0014inf\nh2^hCmax(V(^Eh;\u0001)):\nProof.Proof of this result is essentially identical to that for global sampling, except that we now require\nthe EMD bounds to hold simultaneously (by union bound) for each iteration of the progressive sampling.\nNote that every pruned function provably violates Wor is provably suboptimal, thus removing them never\nremoves a candidate for ^hLor^hC, and indeed only allows us to obtain tighter bounds for each criterion,\nreﬁning our ﬁnal estimated values and selected functions.\nWhile the two algorithms are structured similarly, with additive approximation \u000f, error probability \u000e, and\nsample size m, each can be modiﬁed slightly to obtain a suﬃciently large value for one given values for the\nother two.\n\u000fThe global sampling algorithms naturally works with a ﬁxed set of data points where the accuracy can\nbe determined: given mand\u000e, this algorithm can easily return a satisfying \u000fvalue.\n\u000fThe progressive sampling algorithm is more intuitive when we aim to only use as much data as is\nnecessary. Given \u000fand\u000e, PSP uses roughly no more than twice as many samples required for the \u000f-\u000e\nbound, as at this point, its sample size equals that we would use for global sampling. Often PSP requires\nsigniﬁcantly less data, as it can terminate early, and pruning can lead to much tighter conﬁdence\nintervals.\n5 Experimental Results\n5.1 Implementation Details\nWe implemented both the Global Sampling algorithm and the Progressing Sampling with Pruning algorithm\nin Java. Our codebase is suﬃciently general to apply then algorithm to a wide range of domains; one simply\nneeds to implement the appropriate codecs and criteria.\nWe applied the algorithm to the audio codec selection domain. The function class Hcorresponds to\ncompression algorithms. For this example, we choose between LAME MP3 encoders of diﬀerent variable\nbit-rates — each one has a rating between V0 and V9 representing the how many bits are used to encode\nsegments of audio [Hegemann et al., 2017]. V0 has the highest bit-rate, which means it features the highest\nquality sound yet also reduces the ﬁle size the least; V9 has the lowest bit-rate and thus has the lowest quality\nand the smallest output ﬁles.\nWe can also compare variable bit-rate (VBR) codec schema with constant bit-rate (CBR) and average\nbit-rate (ABR) schema. VBR schemes dynamically choose how many bits to compress, which tends to give\nthe best results because more bits can be used to compress complex segments of audio than simple segments.\n11\n\nFigure 1: A scatter-plot of scaled compression ratios versus PEAQ divergences for music segments when\nencoded by a wide range of codecs.\nCBR algorithms use the same amount of bits for each segment, which means that complex segments may lose\nkey sounds, and simple segments may have too much redundancy; however, CBR schema guarantees an exact\ncompression ratio. ABR is a compromise between the two that aims for a certain compression ratio, but can\ndedicate more resources to compressing complex portions of the audio track, while dedicating fewer resources\nto easily-compressed simple regions.\nWe considered several kinds of criteria for C, which are meant to measure qualities of the compression\nalgorithms that users would care about:\n\u000fPEAQ Objective Diﬀerence (c1) is anobjective perceptual audio model because it uses on computational\nmodels of the ear to how much two audio sequences diﬀer in perception. We discuss these models in Section\n2.5. We compute PEAQ using the GSTPEAQ codebase created by Holters and Zölzer [2015].\n\u000fRoot Mean Squared Error (c2) treats the two audio sequences as vectors of numbers in [\u00001;1]and computes\na normalized L2-distance between the two. This criterion examines similarities in the ﬁle representation while\nneglecting the perceptual diﬀerences.\n\u000fCompression Ratio (c4) represents the ratio of size of the compressed audio sequence to the size of the\noriginal audio sequence. Smaller values indicate that a compression algorithm is eﬀective at reducing the size\nof a given audio ﬁle.\n\u000fCompression Time (c5) is the time in seconds needed for the compression algorithm to compress the audio\nsequence. Because all criteria must output values in [0;1], we actually take the minimum of the compression\ntime and 1 — this is a reasonable assumption because all compression schemes we have observed so far take\nmuch less time than one second.\n\u000fDecompression Time (c6) is the same as Compression Time, except that it measures the amount of time\nneeded to decompress the ﬁle back to WAV format.\nTovisualizetrendsinthesecriteriaforthesecodecfunctions, wecomparePEAQdivergenceandcompression\nratio in Figure 1 on music ﬁles. Each point represents those criteria values for a sample encoded with a given\nfunction. We observe an inverse relationship with respect to compression ratio and PEAQ values. This makes\nsense because using more bits to encode a ﬁle leads to a smaller diﬀerence in sound between the original\nsequence and the decompressed sequence. Because they have ﬁxed bit-rates, the CBR schemes have constant\ncompression ratios, but vary widely in sound ﬁdelity. The most accurate CBR schemes (256 and 320 bits)\nhave zero perceived diﬀerence between the input and output ﬁles, but they have very high compression ratios.\n12\n\nSince we consider objectives that balance PEAQ and CR as linear combinations of the two terms, we can\nvisualize the objective on the plot as the line through the origin deﬁned by vector corresponding to the\ncombination. The optimal scheme will be the one whose mean is closest to the origin when projected onto\nthat line.\nFrom the criteria introduced above, we construct objectives Vthat are linear combinations of these\ncriteria. We run the the algorithm with each objective and observe how diﬀerent compression functions are\nselected based on our stated preferences. We intuitively want to limit the compression ratio and changes to\nthe sound. Future users could modify this objective to be other combinations of criteria depending on what\nthe listener values in a compression algorithm. Our objectives are as follows:\n\u000fRoot Mean Squared Error: V1(h(x)) :=c2(h(x)).\n\u000fPEAQ divergence: V2(h(x)) :=c1(h(x)).\n\u000fCompression ratio: V3(h(x)) :=c4(h(x)).\n\u000fCombination of PEAQ and CR: V4(h(x)) :=1\n3c1(h(x)) +2\n3c4(h(x))\nIn addition, we can apply conﬁdence intervals to the variance of criterion c. This can be useful in the\naudio domain because we might insist on a particularly dependable encoding function by requiring that the\ncompression ratio have little variation. Because Vz[c(h(z))]=E\u0002\nc(h(z))2\u0003\n\u0000E[c(h(z))]2, we can obtain\nconﬁdence intervals for c2andcand combine their intervals. We can then ﬁnd a suitable conﬁdence interval\nfor the variance with that holds with at least the same probability as the two other conﬁdence intervals:\n^EV[c];h\u0012[min ^Ec2;h\u0000max ^E2\nc;h;max ^Ec2;h\u0000min^E2\nc;h]\nVariances are natural criteria to incorporate in constraints as a way to control extreme values. Although we\ndo not demonstrate any constraint examples here, our framework easily permits these tests.\nTo test the model, we run it on ten-second clips from open source audio books on LibriVox[McGuire]. We\ncollected over 16000 samples in that manner. When running the progressive sampling algorithm, we started\nby using 25 samples in the ﬁrst round and doubling that quantity until terminating after ninth samples\nwere used on the eighth round. Notably, our algorithm often fails to isolate the best encoding function with\nEMD-based bounds — that would require more data. However, it succeeds in creating tight conﬁdence\nintervals and in using those intervals to eliminate the worst function given our objective.\nThe tests that follow demonstrate the versatility of our selection algorithm by providing showing the\ndiﬀerences in algorithmic behavior by changing diﬀerent variables. Because the rigorous algorithm converges\nslowly for a relatively small number of samples, we set \u000eand\u000fin order to cleanly observe the behavior of the\nalgorithm. Unless otherwise stated, assume that \u000f= 0:05and\u000e= 0:01for each test.\n5.2 Global Sampling Experiments\nWe tested the Global Sampling algorithm to compare the convergence rates of each bound. Table 1 contains\nthe results for the algorithm with our Finite-Sample EMD bounds (theorem 2.1), our Asymptotic EMD bounds\n(theorem 3.1), the standard Hoeﬀding+Union bound that dominates Roughgarden’s pseudo-dimension bounds\n(theorem 2.2), and asymptotic Gaussian-Chernoﬀ bounds (theorem 3.1). We use \u001916,000 10-second samples\nfrom audio books, test on nine variable bit-rate MP3 encoding schemes and four constant bit-rate MP3s,\nand use only the PEAQ divergence and Compression Ratio criteria. We track convergence by measuring\nthe width of the conﬁdence interval for the objective function of the optimal scheme and how many total\ncompression schemes remain under consideration when the algorithm ﬁnishes; the algorithm only concludes\nthat a codec is optimal with at least probability 1\u0000\u000e= 0:99if the number of remaining schemes is 1.\nNote that the approximate bounds converge much faster than the ﬁnite-sample bounds, with much smaller\nintervals than the EMD and Hoeﬀding bounds, as the latter are based on bounded diﬀerences. Because\nPEAQ divergence and Compression Ratio are at odds (i.e. improvements to audio ﬁdelity require additional\ndata for the compression), it is unsurprising that objective V4is the most diﬃcult to separate.\n13\n\nObjective Optimal CodecFinite-Sample EMD Asymptotic EMD Hoeﬀding Union Gaussian-Chernoﬀ Union\nWidthj^Hj Widthj^HjWidthj^HjWidthj^Hj\nV2(PEAQ) MB3 VBR V1 0.082 3 0.018 10.032 10.007 1\nV3(CR) MB3 VBR V9 0.081 1 0.005 10.032 10.006 1\nV4(PEAQ & CR) MB3 VBR V5 0.082 10 0.009 50.032 100.003 3\nTable 1: A comparison of experimental results when the Global Sampling algorithm is applied to objectives\nV2,V3, andV4with four types of bounds. Width denotes the width of each conﬁdence interval, and j^Hj\ndenotes the number of members of Hthat GS concludes could possibly be optimal (i.e. their conﬁdence\nintervals intersect those of the estimated optimal codec).\nFigure 2: A demonstration of the PSP algorithm with objective V3and Finite-Sample EMD bounds.\n5.3 Progressive Sampling Experiments\nLikewiththeGlobalSamplingalgorithm, wecomparetheeﬀectivenessofdiﬀerentboundsusingtheProgressive\nSampling with Pruning algorithm under various objectives. Because the Progressive Sampling approach is\niterative, we visualize the results as plots of the conﬁdence intervals of the objectives for each iteration. Each\nencoding scheme has a series of intervals for each iteration where it is active; once a scheme is pruned, its\nintervals are no longer displayed. The plot in Figure 2 demonstrates visually how the algorithm prunes codecs\nand shrinks interval sizes with the Finite-Sample EMD bounds.\nBecause the algorithm guarantees the true objective mean of the selected codec to be no more than \u000f\ngreater than that of the optimal codec with probability \u000e, the algorithm may terminate early when the width\nof the interval of the empirically-optimal scheme is smaller than \u000f. Note that Figure 3 shows all nine iterations\nof interval-tightening, regardless of when the algorithm actually terminates.\nWe apply the PSP algorithm to the PEAQ divergence objective in ﬁg. 3 (top left). Once again, we have\n16,000 10-second audio book samples, thirteen variants of VBR and CBR MP3, and only PEAQ divergence\nand Compression Ratio criteria. The approximate Asymptotic EMD bounds and Guassian-Chernoﬀ converge\nquickly to the optimal VBR V1 MP3 after seven and four iterations respectively. The others fail to terminate,\nbut the Hoeﬀding union bounds prune all but two functions while Finite-Sample EMD leaves seven.\nFigure 3 (top right) displays the results of the same experiment applied to the Compression Ratio objective.\nBecause this criterion more evenly dispersed on the interval [0;1]than PEAQ divergence, codecs tend to\nbe pruned quicker as the conﬁdence intervals shrink. Here, VBR V9 MP3 is optimal, and both Asymptotic\nEMD, Gaussian-Chernoﬀ, and Hoeﬀding+Union converge, with the approximate bounds again converging\nnearly instantaneously. The Finite-Sample EMD bounds also prune more eﬀectively, leaving four codecs.\nFinally, we apply the algorithm to the combined PEAQ Divergence and Compression Ratio objective\n14\n\nFigure 3: The results of PSP algorithm with objective\nV2(PEAQ divergence, top left) and V3(Compression\nRatio, top right), and V4(PEAQ and Compression\nRatio, bottom left) with the all four bounds. The\nobjective conﬁdence intervals are shown as semitrans-\nparent solid regions, and the estimated mean objective\nvalues are plotted as solid lines. The number of codecs\nremaining after each progressive sampling iteration is\nplotted with dashed lines.\nin ﬁg. 3 (bottom left). Because PEAQ divergence and Compression Ratio are contradictory, objective\nintervals tend to be much harder to separate. The algorithm only terminates for Asymptotic EMD and\nGaussian-Chernoﬀ after six and three iterations respectively. Note that VBR V5 MP3 is optimal, which\nindicates that we’re able to successfully choose a “moderate” codec that balances the interests of high audio\nquality and low compression ratio. Finite-Sample EMD and Hoeﬀding Union each have ten remaining codecs.\nIt struggles to eliminate codecs as eﬃciently in this case because the conﬂicts between the two criteria clusters\nthe combined means together.\n5.4 Analysis\nAcross the Progressive Sampling and Global Sampling experiments with diﬀerent objectives, we ﬁnd that the\nGaussian-Chernoﬀ bounds tend to be the most eﬀective, followed by the Asymptotic EMD, Hoeﬀding+Union,\nand Finite-Sample EMD bounds, in that order. It makes logical sense that the two approximate bounds would\nout-perform their more rigorous counterparts. While the uniform convergence bounds tended to underperform\nwhen compared to the union bounds, this is largely due to the small number of compression schemes; because\nthe Hoeﬀding+Union and Gaussian-Chernoﬀ bounds logarithmically scale with the number of compression\nschemesjHj, we expect the uniform convergence bounds to perform better in cases with much more than 13\ncodecs.\nFurthermore, changing the objective has a signiﬁcant impact on the choice of codec. MP3 VBR V1 is\nconsistently the best codec when optimizing for audio quality, MP3 VBR V9 dominates other codecs for\ncompression ratio optimization, and MP3 VBR V5 wins under a combination of the two. This demonstrates\nour algorithms allow users to choose a codec that ﬁts their explicit preferences that performs near optimally\nwith high probability.\n6 Discussion and Open Questions\nThe PSP algorithm provides a clean application of uniform convergence bounds that beneﬁts from locality, by\npruning functions that are provably suboptimal. This adaptive nature also leads to improved computational\nand sample eﬃciency, as codecs are applied fewer times, and the algorithm terminates before exhausting\n15\n\nits data if it can compute the optimal codec to within a given tolerance. We proved theoretical results that\nensure the convergence of our algorithm. We are interested in exploring other options for asymptotic bounds\nwith the goal of obtaining more stable bounds with more provable guarantees. We showed that the framework\ncan be successfully applied to the domain of audio compression. However, no special characteristics of audio\ndata are used here, so we could easily extend this to other function selection tasks.\nWhile we make theoretical arguments about the dominance of bounds based on uniform convergence\ntheory over union bounds, our experiments currently use families of codecs that are too small to exploit those\nadvantages. In future experimental work, we intend to work on incorporating more codecs into the model to\nenable the EMD bounds the eclipse the union bounds in their ability to prune candidate codecs. Further, we\nintend to run experiments on samples from diﬀerent distributions — such as music ﬁles — to demonstrate\nthat the optimal encoding scheme is highly distribution dependent and thus well-suited to a learning problem.\nBecause our EMD bounds outperform union bounds when there are a large number of codecs, we think\nthat this algorithm would ﬁnd a natural application in selecting combinations of encoding schemes. For\nexample, suppose that we wanted to both compress and error-correct a data sequence, and we are unsure\nabout how pairs of encoding schemes will interact with each other. We can use our sampling algorithms to\nencode each sequence with each pair of schemes to determine which combination of algorithms optimizes an\nobjective based on compression ratio, reconstruction error, and susceptibility to noise. The large number of\npaired codecs will more starkly highlight the weaknesses of union bounds in this application.\nWe also hope to incorporate a broader range of criteria; in particular, we want to integrate criteria which\nsynthesizes a range of feedback. For example, in the audio domain the similarity between two sequences\nis often measured subjectively by human listener-provided ratings. Because perceptual abilities varies by\nlisteners, we want to evaluate the listeners as well as the audio ﬁles. To do so, we can regard each data point\nas the pairing of a ﬁle and a listener, rather than just a ﬁle. We need a diﬀerent method of complexity that\nallows samples to drawn with i.i.d.\nOne weakness of our techniques is that they only identify a codec that is optimal on average . This\ncan be partially mitigated by constructing objectives using variances or higher moments, which can yield\nChernoﬀ-like bounds [Philips and Nelson, 1995]. However, an alternative strategy would be to uniformly\nlearn the cumulative distribution function of each criterion and each codec, in addition to expectations. Our\nmethods can easily be extended to this case, and we would then be able to select criteria or induce additional\nconstraints based on provable tail bounds. This complements our existing framework, and mitigates the\nchances of selecting a codec that works well on average but occasionally performs extremely poorly.\nOur data-driven approach to codec-selection is in line with many trends in machine learning and databases.\nWe replace ﬁxed codecs with learned codecs, in much the same way that autoML systems replace ﬁxed\nmodels with learned pipelines [Hutter et al., 2011], and generative adversarial networks [Goodfellow et al.,\n2014] replace ﬁxed loss functions with learned discriminative loss functions. Similarly, database systems have\nrecently seen great improvements to query prediction and latency by replacing static indices with learned\nindices [Kraska et al., 2018].\nAcknowledgments\nThis work was partially supported by NSF award IIS 1813444 and DARPA/US Army grant W911NF-16-1-0553.\nReferences\nPeter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results.\nJournal of Machine Learning Research , 3(Nov):463–482, 2002.\nPeter L. Bartlett, Stáphane Boucheron, and Gábor Lugosi. Model selection and error estimation. Machine Learning ,\n48(1):85–113, 2002.\nPeter L Bartlett, Olivier Bousquet, Shahar Mendelson, et al. Local rademacher complexities. The Annals of Statistics ,\n33(4):1497–1537, 2005.\nGeorge Bennett. Probability inequalities for the sum of independent random variables. Journal of the American\nStatistical Association , 57(297):33–45, 1962.\nStéphane Boucheron, Gábor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymptotic theory of\nindependence . Oxford university press, 2013.\n16\n\nBruce M Brown et al. Martingale central limit theorems. The Annals of Mathematical Statistics , 42(1):59–66, 1971.\nFrancesco Paolo Cantelli. Sulla determinazione empirica delle leggi di probabilita. Giorn. Ist. Ital. Attuari , 4(421-424),\n1933.\nAlexey Chervonenkis and Vladimir Vapnik. On the uniform convergence of relative frequencies of events to their\nprobabilities. Theory of Probability and its Applications , 16:264–280, 1971.\nAryeh Dvoretzky, Jack Kiefer, and Jacob Wolfowitz. Asymptotic minimax character of the sample distribution function\nand of the classical multinomial estimator. The Annals of Mathematical Statistics , pages 642–669, 1956.\nBradley Efron and Charles Stein. The jackknife estimate of variance. The Annals of Statistics , pages 586–596, 1981.\nTapio Elomaa and Matti Kääriäinen. Progressive rademacher sampling. In AAAI/IAAI , 2002.\nVL Glivenko. Sulla determinazione empirica delle leggi di probabilita. Gion. Ist. Ital. Attauri. , 4:92–99, 1933.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville,\nand Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems , pages\n2672–2680, 2014.\nRishi Gupta and Tim Roughgarden. A PAC approach to application-speciﬁc algorithm selection. SIAM Journal on\nComputing , 46(3):992–1017, 2017.\nRobert Hegemann, Alexander Leidinger, and Rogério Brito. LAME MP3 encoder, 2017. URL http://lame.\nsourceforge.net/ .\nWassily Hoeﬀding. Probability inequalities for sums of bounded random variables. Journal of the American statistical\nassociation , 58(301):13–30, 1963.\nMartin Holters and Udo Zölzer. GSTPEAQ – an open source implementation of the PEAQ algorithm. Procedings of\nthe International Conference on Digital Audio Eﬀects , 18, 2015.\nFrank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential model-based optimization for general algorithm\nconﬁguration. In International Conference on Learning and Intelligent Optimization , pages 507–523. Springer, 2011.\nNikil Jayant, James Johnston, and Robert Safranek. Signal compression based on models of human perception.\nProceedings of the IEEE , 81(10):1385–1422, 1993.\nVladimir Koltchinskii. Rademacher penalties and structural risk minimization. IEEE Transactions on Information\nTheory, 47(5):1902–1914, 2001.\nTim Kraska, Alex Beutel, Ed H Chi, Jeﬀrey Dean, and Neoklis Polyzotis. The case for learned index structures. In\nProceedings of the 2018 International Conference on Management of Data , pages 489–504. ACM, 2018.\nPascal Massart. The tight constant in the dvoretzky-kiefer-wolfowitz inequality. The annals of Probability , pages\n1269–1283, 1990.\nColin McDiarmid. On the method of bounded diﬀerences. Surveys in combinatorics , 141(1):148–188, 1989.\nHugh McGuire. LibriVox: Free public domain audiobooks. URL https://librivox.org/ .\nThomas K Philips and Randolph Nelson. The moment bound is tighter than chernoﬀ’s bound for positive tail\nprobabilities. The American Statistician , 49(2):175–178, 1995.\nMatteo Riondato and Eli Upfal. Mining frequent itemsets through progressive sampling with rademacher averages. In\nProceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages\n1005–1014. ACM, 2015.\nMatteo Riondato and Eli Upfal. Abra: Approximating betweenness centrality in static and dynamic graphs with\nrademacher averages. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery\nand Data Mining , pages 1145–1154. ACM, 2016.\nMatteo Riondato and Eli Upfal. Abra: Approximating betweenness centrality in static and dynamic graphs with\nrademacher averages. ACM Transactions on Knowledge Discovery from Data (TKDD) , 12(5):61, 2018.\nThilo Thiede, William C. Treurniet, Roland Bitto, Christian Schmidmer, Thomas Sporer, John G. Beerends, Catherine\nColomes, Michael Keyhl, Gerhard Stoll, Karlheinz Brandenburg, and Bernhard Feiten. PEAQ—the ITU standard\nfor objective measurement of perceived audio quality. Journal of the Audio Engineering Society , 48(1):3–29, 2000.\nLeslie G. Valiant. A theory of the learnable. Communications of the ACM , 27(11):1134–1142, 1984.\n17\n\nA Proofs\nA.1 Asymptotic EMD Generalization Bounds\nTheorem 3.1 (Asymptotic EMD Uniform Convergence Bounds) .SupposeH\u0012X! RwithjHj<1, and\nthatVx[h(x)]exists8h2H, and takexsDm. Now, take either true variance bound\n\u001b2:=E\nx\"\nsup\nh2H1\nmmX\ni=1(h(xi)\u0000E\nx0sD[h(x0)])2#\n+ sup\nh2HV\nx0sD[h(x0)] 2 sup\nh2HV\nx0sD[h(x0)];\norplugin variance bound estimate:\n\u001b2:= 2 sup\nh2H^V[h(x)]:\nThe following then hold (asymptotically w.r.t. sample size m):\n1.sup\nh2HE\nx0sD[h(x0)]\u0000^E[h(x)]and ~Dm(H;x) mGaussian with variance \u00142\nm\u001b2.\n2.P0\n@sup\nh2HE\nx0sD[h(x0)]\u0000^E[h(x)]\u0015p\n2~Dm(H;x) +\u001b(2 + 2p\n2)s\nlog(2\n\u000e)\n2m1\nA.m\u000e.\n3.P0\n@sup\nh2H\f\f\f\fE\nx0sD[h(x0)]\u0000^E[h(x)]\f\f\f\f\u0015p\n2~Dm(H;x) +\u001b(2 + 2p\n2)s\nlog(3\n\u000e)\n2m1\nA.m\u000e.\nProof.We ﬁrst show 1, which follows via a complicated argument regarding the central limit theorem,\nthe Martingale central limit theorem, and the Efron-Stein inequality. 2 and 3 then follows from 1 and an\nasymptotic variant of the EMD symmetrization inequality.\nWe now show 1. We explicitly bound the variance of the EMD; the argument holds as well, mutatis\nmutandis, for the supremum deviation.\nWe ﬁrst use a centering trick on the EMD, which ultimately results in tighter bounds by replacing raw\nvariances with centeralized variances. The autocenteredness property of the EMD states that ~Dm(H;x) =\n~Dm(H0;x), whereH0:=fh0(x):=h(x) +c(h)jh2Hgis equal toH, except each function is oﬀset by an\narbitrary constant c(h)2R. Now, takeH0:=fh0(x):=h(x)\u0000Ex0sD[h(x0)]jh2Hgto be the centralized\nversion ofH. As ~Dm(H;x) =~Dm(H0;x)for anyx2Xm, we may conclude that their variances are also the\nsame. The supremum deviation needs no such centering trick, as it is already a centered empirical process.\nWe now use a standard bound on the variance of suprema of empirical processes, derived from the\nEfron-Stein inequality [Efron and Stein, 1981], given as Theorem 11.1 of [Boucheron et al., 2013]. The weak\nvariance \u00062\nweak(as deﬁned in [Boucheron et al., 2013]) of the EMD processes is by deﬁnition:\n\u00062\nweak:=E\nxsDm\"\nsup\nh02H0mX\ni=1\u0012(\u00001)i\nmh0(xi)\u00132#\n=E\nxsDm\"\nsup\nh02H0mX\ni=11\nm2((\u00001)i)2h02(xi)#\n=1\nmE\nxsDm\"\nsup\nh02H01\nmmX\ni=1h02(xi)#\n=1\nmE\nxsDm\"\nsup\nh2H1\nmmX\ni=1(h(xi)\u0000E\nx0sD[h(x0)])2#\nThis quantity is1\nmtimes the expected largest centered empirical variance estimate (without Bessel’s\ncorrection, though asymptotically it matters not) with respect to a sample of size m. By similar logic, it can\n18\n\neasily be shown that the wimpy variance \u001b2\nwimpyin which the supremum and the expectation are commuted,\nis\u001b2\nwimpy:= suph2HVx0[h(x0)]. This quantity is far more convenient than the weak variance, as it is an\nelementary statistical quantity, well-studied and well-understood, and can reasonably be assumed. Fortunately,\nwe can argue that for ﬁnite Hand under the bounded variance assumption, they are asymptotically equivalent,\nas each variance estimate converges to the true variance.\nWe thus conclude that, asymptotically, \u00062\nweak =\u001b2\nwimpy =1\nmsuph2HVx0sD[h(x0)], and the plugin-\nestimator for each, which is an unbiased estimator for \u00062, but is upward-biased for \u001b2, issuph2H^V[h(x)]\n(with Bessel’s correction). Now, by Theorem 11.1 of [Boucheron et al., 2013], we bound the variance of the\nEMD (and the supremum deviation) as \u001b2= \u00062\nweak+\u001b2\nwimpy.\nThe ﬁnal part of the argument is that ~Dm(H;x)is Gaussian distributed. This result holds by the\nMartingale central limit theorem, noting that m~Dm(H;x)forms a submartingale, while mq\nm\nm+1~Dm(H;x)\nforms a supermartingale, each with variances that sum to 1. A similar argument holds for the supremum\ndeviation.\nWe now show 2 and 3. We require an tighter asymptotic form of the standard EMD symmetrization\ninequality than the standard ﬁnite-sample bound; behold, taking x;x0sDm, and letting x\u000ex0be their\nconcatenation:\nE\nx\u0014\nsup\nh2HE\nx0sD[h(x0)]\u0000^E[h(x)]\u0015\n=E\nx\u0014\nsup\nh2HE\nx0\u0002^E[h(x0)]\u0003\n\u0000^E[h(x)]\u0015\nLinearity\n\u0014E\nx;x0\u0014\nsup\nh2H^E[h(x0)]\u0000^E[h(x)]\u0015\nJensen’s Inequality\n=E\nx;x0h\n~D2m(H;x\u000ex0)i\nDefinition of ~D\n.m1p\n2E\nxsDmh\n~Dm(H;x)i\nEMD Asymptotics\nThe ﬁnal step holds, as for ﬁnite Hwith bounded variance, by Massart’s lemma, ExsDmh\n~D2m(H;x)i\n\u0014\nRm(H;x)\u0014suph2Hkh(x)k2p\nlog(2jHj)\nm2O\u0010\n1pm\u0011\n.\nWe are now ready to apply the Chernoﬀ bounds to obtain desiderata 2 and 3. First note that in the\nbounded-diﬀerence EMD tail bounds, we were able to replace the standard double-usage of McDiarmid’s\ninequality with a single usage, improving the log(2\n\u000e)with log(1\n\u000e). Here, a more sophisticated argument may be\nable to do the same (the EMD and supremum deviation should be correlated, thus their diﬀerence should have\nlower variance than if they were independent), but for simplicity and to require fewer asymptotic assumptions\nto hold, we instead use a union bound and two applications of the Gaussian Chernoﬀ bound.\nThe Gaussian Chernoﬀ bound states that P\u0000\nN(\u0016;\u001b2)\u0015\u0016+\u000f\u0001\n\u0014e\u0000\u000f2\n2\u001b2. Applying this to both the\nsupremum deviation and the EMD, bounding the supremum deviation upper-tail and the EMD lower-tail,\nand combining the results via union bound, we obtain\nP\u0012\nsup\nh2HE\nx0[h(x0)]\u0000^E[h(x)]\u0014p\n2~Dm(H;x) + (1 +p\n2)\u000f\u0013\n\u00142e\u0000m\u000f2\n2\u001b2:\nNow, to show 2, we simply take \u000e=P\u0010\nsuph2HEx0[h(x0)]\u0000^E[h(x)]\u0014p\n2~Dm(H;x) + (1 +p\n2)\u000f\u0011\n, and\ncompute the minimal \u000ffor which we may guarantee the statement holds with probability at least 1\u0000\u000e.\n3 follows via much the same logic, except we now use a 2-tailed bound on the supremum deviation (a\n1-tailed bound suﬃces for the EMD), and the 3 tails result in log(2\n\u000e)increasing to log(3\n\u000e).\nThese asymptotic bounds can be converted to ﬁnite-sample bounds by using the true variance bound\ninstead of the plugin estimate variance bound, replacingp\n2~Dm(H;x)with 2~Dm(H;x), and replacing the\nGaussian Chernoﬀ bound with Chebyshev’s inequality. Unfortunately, the ﬁrst summand (the expected\n19\n\nvariance empirical of the centralized hypothesis class) is quite complicated, and not usually a reasonable\nquantity to bound a priori, though the second (the maximum true variance of the hypothesis class) is\nwell-understood, and assuming a bound is quite reasonable. Furthermore, the weak polynomial bounds of\nChebyshev’s inequality, while reasonable for large \u000e, become prohibitively loose when high-probability tail\nbounds are desired, though these can be improved by additionally assuming suph2H;x;x02Xh(x)\u0000h(x0)\u0014c\nand applying Bennett’s inequality [Bennett, 1962], essentially yielding a hybrid of Theorems 2.1 & 3.1.\n20",
  "textLength": 66559
}