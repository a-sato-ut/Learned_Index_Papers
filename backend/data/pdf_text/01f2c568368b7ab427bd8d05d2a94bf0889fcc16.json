{
  "paperId": "01f2c568368b7ab427bd8d05d2a94bf0889fcc16",
  "title": "Learned Sorted Table Search and Static Indexes in Small-Space Data Models",
  "pdfPath": "01f2c568368b7ab427bd8d05d2a94bf0889fcc16.pdf",
  "text": "Citation: Domenico, A.;\nGiancarlo, R.; Lo Bosco, G. Learned\nSorted Table Search and Static\nIndexes in Small-Space Data Models.\nData 2023 ,8, 56. https://doi.org/\n10.3390/data8030056\nAcademic Editors: Donatella Merlini\nand Irene Finocchi\nReceived: 20 January 2023\nRevised: 22 February 2023\nAccepted: 23 February 2023\nPublished: 3 March 2023\nCopyright: © 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\ndata\nArticle\nLearned Sorted Table Search and Static Indexes in Small-Space\nData Models†\nDomenico Amato\n , Raffaele Giancarlo *\n and Giosué Lo Bosco\nDipartimento di Matematica e Informatica, Universitá degli Studi di Palermo, 90123 Palermo, Italy;\n*Correspondence: raffaele.giancarlo@unipa.it\n†An extended abstract related to this paper has been presented at the 20th International Conference of the Italian\nAssociation for Artiﬁcial Intelligence (AixIA 2021), Milano, Italy, 1–3 December 2021. The proceedings are\nLecture Notes in Computer Science Vol. 13196, by Springer.\nAbstract: Machine-learning techniques, properly combined with data structures, have resulted in\nLearned Static Indexes, innovative and powerful tools that speed up Binary Searches with the use of\nadditional space with respect to the table being searched into. Such space is devoted to the machine-\nlearning models. Although in their infancy, these are methodologically and practically important,\ndue to the pervasiveness of Sorted Table Search procedures. In modern applications, model space\nis a key factor, and a major open question concerning this area is to assess to what extent one can\nenjoy the speeding up of Binary Searches achieved by Learned Indexes while using constant or nearly\nconstant-space models. In this paper, we investigate the mentioned question by (a) introducing two\nnew models, i.e., the Learned k-ary Search Model and the Synoptic Recursive Model Index; and\n(b) systematically exploring the time–space trade-offs of a hierarchy of existing models, i.e., the ones\nin the reference software platform Searching on Sorted Data , together with the new ones proposed\nhere. We document a novel and rather complex time–space trade-off picture, which is informative for\nusers as well as designers of Learned Indexing data structures. By adhering to and extending the\ncurrent benchmarking methodology, we experimentally show that the Learned k-ary Search Model is\ncompetitive in time with respect to Binary Search in constant additional space. Our second model,\ntogether with the bi-criteria Piece-wise Geometric Model Index, can achieve speeding up of Binary\nSearch with a model space of 0.05% more than the one taken by the table, thereby, being competitive\nin terms of the time–space trade-off with existing proposals. The Synoptic Recursive Model Index\nand the bi-criteria Piece-wise Geometric Model complement each other quite well across the various\nlevels of the internal memory hierarchy. Finally, our ﬁndings stimulate research in this area since\nthey highlight the need for further studies regarding the time–space relation in Learned Indexes.\nKeywords: sorted table search; database management; learned indexes; machine learning\n1. Introduction\nFundamental data structures, such as Hash Tables and Balanced Search Trees, are\nnot only very useful in a broad range of application domains but are also so fundamental\nfor computer science to be part of widely adopted textbooks in the discipline, e.g., [ 1].\nWith the aim of obtaining time and space performance improvements, an emerging trend\nis to combine machine-learning techniques with data-structure techniques. This new\nresearch area goes under the name of Learned Data Structures , and it was initiated in 2018\nbyKraska et al. [2] . In particular, in this paper, Learned Data Structures were used mainly\nfor the case of searching in sorted sets. This particular problem can be solved in classic\nalgorithmics by using a well-known and optimal routine, i.e., Binary Search [ 3,4], or more\nsophisticated data structures, e.g., classic indexes, such as B-Trees [ 5]. Usually, the classic\napproaches consider all of the element positions in a sorted list as possible candidates to be\nanswers to a search query.\nData 2023 ,8, 56. https://doi.org/10.3390/data8030056 https://www.mdpi.com/journal/data\n\nData 2023 ,8, 56 2 of 33\nSuch an initial list is then reﬁned in at most O(logn)iterations, where nis the size of\nthe sorted set. The main novelty in the Learned Data Structures paradigm is the use of a\nmachine-learning model trained over the elements of a sorted set that can learn the dataset\ndistribution. This model uses such knowledge to make a prediction of the position of the\nquery element in the sorted set. The prediction may be inaccurate, and thus the model\nreturns an interval to search that accounts for prediction errors.\nAs a consequence, the output of the model is an interval of positions to search. The\nbetter the model, the smaller the interval. The ﬁnal search stage on the reduced table\npositions interval is performed via Binary Search. This particular kind of Learned Data\nStructure is referred to as Learned Index and is the main object of this research. In what\nfollows, in order to place our contributions in the proper context, we provide a brief\nliterature review, followed by a presentation of our contributions to this novel area, together\nwith a road-map of the paper in which we also highlight where it has been expanded with\nrespect to [6].\n1.1. Literature Review\nAlthough the Learned Data Structures research ﬁeld is a very novel one, it has already\nbeen extensively studied in the literature [ 7–9]. In what follows, we mention the main\nmethods that can be useful for a better comprehension of the contributions provided in\nthis paper. To this end, the most signiﬁcant Learned Indexes are presented, with speciﬁc\nreference to their training procedures and relative benchmarking studies. Moreover, exam-\nples of real-world applications of Learned Indexes are provided, the important aspect of\ntime/space correlation is highlighted, and, for completeness, examples of other Learned\nData Structures different from Learned Indexes are given. However, the presentation is\nintended to be synoptic, since the interested reader can ﬁnd details in the papers that are\nmentioned, including a recent review on the subject [7].\n1.1.1. Core Methods and Benchmarking Platform\nThe Recursive Model Index [ 2] (RMI ) is the ﬁrst Learned Index proposal. It is a\nhierarchical model that estimates the distribution of the data via a top-down approach. It\ncan be considered as a tree-like structure, where the nodes are generic models, ranging\nfrom neural-network models [ 10] to simple linear or polynomial-regression models [ 2].\nGiven a query element, the internal nodes at each level identify the index of the next model\n(node) to use in the hierarchy. Finally, these provide a reduced interval to search into.\nThe tree structure of the RMI is characterized by the number of levels, the number\nof nodes for each level and the kind of models adopted at each node. As a consequence,\ntheRMI depends on a consistent number of hyper-parameters, whose estimation can be a\nserious issue in real-world contexts as highlighted by Maltry et al. [ 11]. To overcome these\ndifﬁculties, Marcus et al. provided a platform, referred to as CDFShop [12], that can be\nused to generate the code of a speciﬁc RMI , given an input dataset and speciﬁc values of\nits hyper-parameters. In addition, given an input dataset, the platform can provide up to\ntenRMI s.\nFollowing the seminal proposal of the RMI , various new versions of Learned Indexes\nwere designed. This is the case of the Piece-wise Geometric Model Index [ 13] (PGM ) that\nestimates the data distribution in a bottom-up fashion by a piece-wise linear approximation\nalgorithm [ 14]. Differently from the RMI , it is based on only one hyper-parameter e, which\nrepresents the maximum error admitted for the index prediction. Note that, despite the\nvalue of eguaranteeing an upper bound on the search time, it does not provide any bound\nsuggestion on the additional space used by any Learned Index with respect to the size of\nthe input data.\nThe FITing-Tree model by Kraska et al. [ 15] was designed to overcome the mentioned\nspace issue. It is an extension of the PGM using the maximum number of approximation\nsegments as an additional parameter so that it is possible to compute the maximum\nadditional space used by this model. Although characterized by this new space-bound\n\nData 2023 ,8, 56 3 of 33\nproperty, it is not considered in this study because of its poor performance in terms of the\nquery time with respect to other Learned Indexes, as remarked upon in the literature [16].\nThe Radix Spline index [ 17] (RS) is another example of a bottom-up approach to\nLearned Indexing that, in a different manner to the PGM , estimates the distribution through\na spline curve [ 18]. As for the FITing-Tree model, search time and space can be controlled\nthrough two hyper-parameters, i.e., the maximum error eand the number of bits needed to\nindex the spline points. However, we anticipate that such control of space is rather poor as\ndocumented by our experiments.\nExcept for the PGM , all the Learned Indexes mentioned so far are static and need to\nbe rebuilt in the case the input dataset changes. Such a reconstruction could affect seriously\nthe Learned Index performances, and thus a new class of indexes referred to as dynamic,\nwas proposed. This is the case of the Adaptive Learned Index [ 19] (ALEX ), which provides\na Dynamic Learned Index via an extension of the RMI .\nDue to the high number of Learned Index proposals, it is evident that it is necessary\nto determine the strengths and weaknesses of each method. To this end, Marcus et al. [ 9]\nprovided an exhaustive benchmarking study of the main Learned Indexes on real datasets,\nsupported by the development of a software platform referred to as Searching on Sorted\nData [16] (SOSD ). The mentioned study approaches the question by considering only\nBinary Search as the ﬁnal level of Learned Indexing.\nAdditional pros/cons studies are available at [ 6,20] also considering different types of\nsearch procedures, such as Uniform Binary Search and k-ary Search. However, it is evident\nthat no clear winner emerges, across the many datasets and search routines used for\nexperimentation. It is also evident that, as also summarized in a web platform [ 21], in most\ncases, the best-performing indexes are the RMI ,PGM and RS. As a consequence, these\nthree Learned Indexes are the ones considered in this paper as a baseline to compare against.\n1.1.2. Applications\nClassic Indexes are widely used in a variety of real-world contexts, such as databases [ 22]\nand search engines [ 23]. As a consequence, Learned Indexes can also make improvements\nin various related applications. In particular, they are widely used for databases, pro-\nviding new challenges and opportunities [ 24], such as the development of the so-called\nLearned Databases [25]. They have also been applied in speciﬁc kinds of databases, such as\nspatial [26,27] and biological [ 28]. Finally, another very recent application is the develop-\nment of frameworks for optimizing database queries [29–32].\nOnce we have outlined, at a high level, the range of applications for Learned Indexes,\nit is appropriate to present a well-documented topic regarding the time/space trade-off\nof Binary Search: Static In-Memory Databases [ 22]. The purpose is to illustrate the\nimportance and peculiarity of the time/space trade-off when dealing with Sorted Table\nSearch Procedures since Static Learned Indexes speed up those routines using more space.\nAs well-presented by Rao and Ross [ 22], the deployment of large databases in main memory\nis now possible and highly recommended since the query time greatly beneﬁts from the\ndata being in main memory.\nGiven that Binary Search has a logarithmic time performance and requires no addi-\ntional space with respect to the Sorted Table, i.e., the database in main memory, Rao and\nRoss investigated the use of additional space in order to obtain indexing data structures\nthat can provide faster query times with respect to Binary Search. They proposed the CSS\nTree. Learned Indexes improved the query time of this data structure using less space with\nrespect to it [13].\nAnalogous results hold for many other static indexing data structures. Therefore, the\nproper formulation of a time/space trade-off in this context is to take constant additional\nspace with respect to the table as the space baseline and the query time of Binary Search\nas the time baseline. Then, as investigated by Rao and Ross, it is natural to ask if, by\nusing more space, we can design indexes that have a query time faster than Binary Search.\nLearned Indexes are the most recent and effective answer to this question.\n\nData 2023 ,8, 56 4 of 33\n1.1.3. The Emergence of a New Role of Machine Learning for Data Processing and Analysis\nAnalogously to Learned Indexes, many methods can beneﬁt from the combined\napproach of machine learning and classic data structures. An example that has been\nextensively discussed in the literature is the case of Bloom Filters [ 33], whose learned\nversion was introduced by Kraska et al. [ 2], extended with several variants in [ 34–36] and\nanalyzed in more depth by Fumagalli et al. [ 37]. Other examples are the learned versions of\nHash Functions [ 2,38], Binary Trees [ 39], Rank/Select Dictionaries [ 40], Sufﬁx Arrays [ 41]\nand String Dictionaries [ 42]. However, the importance of using a learning phase to improve\nthe performance of a classic algorithm has not been limited only to those concerned with\nsearching in sorted sets but has also, recently, been used for caching, scheduling and\ncounting with data streams [8] and in the speciﬁc case of sorting operations [43].\nDue to the line of research outlined thus far, a new role has emerged for machine\nlearning in data processing and management. Indeed, it is well-established that machine-\nlearning techniques have a broad spectrum of applications in data-driven domains as well\nexempliﬁed in [ 44,45]. However, how to leverage those techniques to obtain improvements\nin the time and/or space performance of data-processing procedures that can be seen as\nbelonging to the area of data structures and databases has been overlooked. Learned Data\nStructures and algorithms open the way to the exploration of machine-learning techniques\nto design pre-processors to boost the performance of data structures and algorithms. The\nimplications for applications are natural and real as discussed in [ 46], including a better use\nof time and space and the ability to process larger amounts of data with fewer resources.\nDatabase systems, which are essential to any kind of large data-analysis task, can beneﬁt\nthe most from this new role of machine learning [47].\n1.2. Our Contributions to Learned Indexing\nAs we mentioned, all Learned Index proposals offer some kind of time/space trade-off\nwith respect to Binary Search. In turn, the abundance of those new data structures gives\nrise to many options regarding the use of space and time and which one to pick is speciﬁc\nto the application that one has in mind and the resources one has available. Unfortunately,\nthis aspect regarding the time/space trade-off of Learned Data Structures has not been\ninvestigated in depth and rigorously following the methodology coming from classic data\nstructures [ 3]. In particular, given that the intent is to speed up Binary Search with the use\nof additional space with respect to the input Sorted Table, we are missing an assessment\nof how effective constant-space models would be at speeding up Sorted Table Search\nprocedures. In summary, two related fundamental questions have been overlooked and are\nstated here:\n• How space-demanding should a predictive model be in order to speed up Sorted\nTable Search procedures.\n• To what extent can one enjoy the speeding up of the search procedures provided by\nLearned Indexes with respect to the additional space one needs to use.\nIt is relevant to state that, given the previous work that we mentioned motivating the\nuse of more space with respect to Binary Search in order to obtain better query times, the two\nrelated questions above are methodologically important since a systematic and coherent\nstudy of whether we can obtain Learned Indexes with small-space occupancy, i.e, close to\nconstant, such as a classic Binary Search, with the characteristic of being able to speed up\nSorted Table Search procedures, are not available. Moreover, such a characterization has\nimportant practical implications as discussed in [13,22].\n1.3. Road Map of the Paper\nThis paper considerably expands the presentation and the results contained in [ 6]. In\nparticular, we describe our research in full and provide details with an in-depth analysis.\nWe also include material in an appendix. We provide a road map of the paper and our\ncontributions with respect to the state of the art. Moreover, noting that the Introduction\n\nData 2023 ,8, 56 5 of 33\nwas considerably expanded with respect to the conference version [ 6], we also highlight the\nadditions that we provide in the remaining sections with regard to the conference paper [ 6].\nSection 2 is dedicated to a formal deﬁnition of the search on sorted data problem with\nan outline of the classic algorithmic solution via Binary Search. Then, we provide and\ndiscuss a very simple approach to learning from data to speed up searching in sorted tables.\nMoreover, we propose a classiﬁcation of Learned Indexes that includes two new ones as\nwell as some that are leaders in the literature, i.e., RMI ,RSand PGM . In particular, the\nﬁrst new model, referred to as Learned k-ary Search ( KO-US ), uses constant space, while\nthe other new model, referred to as Synoptic RMI (SY-RMI ), uses a user-deﬁned amount\nof space.\nThe two models were introduced for the ﬁrst time in [ 6], where they also presented\nan outline of their evaluation. As stated, this paper is an extended version of a conference\npaper [ 6] and provides the full spectrum of our experiments and evaluations. In regard to\nthe conference paper [ 6], Section 2 here accounts for Sections 2, 3.1 and 3.2 in that paper,\nwith very few additions in detail.\nSection 3 provides our experimental methodology, which extends the one recom-\nmended in the benchmarking study by Marcus et al. [ 9]. In particular, in order to provide\nan evaluation of how Learned Indexes perform when the input table ﬁts the different levels\nof the internal memory hierarchy, we extended the datasets used in the benchmarking\nstudy. This is another methodologically important contribution of this scientiﬁc research.\nIn regard to the mentioned conference paper [ 6], Section 3 accounts for Section 3.4 of that\npaper, which is now considerably extended in terms of experimental details, and we also\ninclude material presented in Appendixes A.2 and A.4.\nSection 4 describes and analyses the training phases of the two novel models. In\nparticular, we focus on how the Synoptic RMI is able to learn, in small space, key features of a\nvariety of real datasets for the purpose of prediction. Moreover, we report useful indications,\noverlooked so far in the literature, for Learned Index designers and practitioners about\nmodel training across different memory levels, shedding additional light on the training\nphase of the RSand the PGM . It is useful to recall that the RSwas shown to be faster to\ntrain than the PGM only on large datasets [ 17]. Here, we show that, on small datasets,\nthis is no longer the case. In regard to the conference paper [ 6], this section accounts for\nSection 4 in that paper, which is now considerably extended in terms of the description of\nthe training phase experiments with a full discussion of the obtained results.\nSection 5 describes and analyzes the Learned Indexes query phase, providing the main\ncontributions of this paper. In particular, concerning the additional space, we analyze two\npossible cases: constant or nearly constant and parametric.\nFor the case of constant space, our main contribution is the study of the performance\nof the Learned k-ary Search Model in comparison with a Cubic Regression Model and\nBinary Search alone. Indeed, we anticipated that the Learned k-ary Search Model would\nperform better than the Binary Search alone and the Cubic Model, except in the case when\nthe dataset distribution was very complex to approximate. This issue represents the main\nweakness of constant-space models. In addition, the Learned k-ary Search Model was\ncompared with a top performing Binary-Search routine that uses a layout other than sorted,\ni.e., the Eytzinger Layout [ 23]. Our ﬁndings provide evidence that the Eytzinger Layout,\nwhen possible to use, is always competitive with respect to all the models with constant or\nnearly constant space—even the Learned k-ary Search. Unfortunately, as indicated in what\nfollows, such a layout cannot be used within the current Learned-Indexing paradigm.\nFor the case of parametric space, we provide a conﬁrmation and an extension of the\nﬁndings provided in the benchmarking study by Marcus et al. [ 9]. Indeed, the new models\nintroduced in the study, i.e., the Synoptic RMI and the bi-criteria PGM , perform better\nthan the Binary Search alone, across all the datasets and memory levels, using very small\nadditional space with respect to the input table. Moreover, even the most complex models,\nexcluding the RSon the lower memory levels, achieve very good performance considering\na bound of at most 10% of additional space.\n\nData 2023 ,8, 56 6 of 33\nWe also investigate the time and space relationships of parametric models, showing\nthat, while their query times can differ by constant factors, the corresponding spaces\ncan disagree by several orders of magnitude. The main ﬁnding is that space seems to\nbe the real key to the efﬁciency of a model. This provides additional insight into the\ntime/space relationship of Learned Indexes, with respect to what is known in the literature.\nOur analysis also provides useful guidelines to practitioners interested in using Learned\nIndexes. The software and datasets used for this research are available at [48].\nIn regard to the conference paper [ 6], Section 5 accounts for Sections 5 and 6 of the\nmentioned paper, which now presents the full set of experiments on all datasets together\nwith an in-depth analysis of the results. Some of the material relevant to this section is in\nAppendix A.5.\n2. Learning from a Static Sorted Set to Speed Up Searching\nConsider a sorted table Aofnkeys, taken from a universe U. It is well-known that\nSorted Table Search can be phrased as the Predecessor Search Problem: for a given query\nelement x, return the A[j]such that A[j]\u0014x<A[j+1]. With reference to such a problem,\nin the following, we describe the classic solutions in the literature and how to transform\nthem into learning-prediction ones.\n2.1. Solution with a Sorted Search Routine\nIt is well-known in algorithmics [ 1,3,4,49] that the Predecessor Search Problem can be\nsolved with Sorted Table Search routines, such as Binary and Interpolation Search. For the\naim of this paper and according to the benchmarking study, we use the C++ lower_bound\nroutine, denoted as BSand informally referred to as Standard. In addition to this method,\nwe use the best routines that came out of the study by Khuong and Morin [ 23], i.e., Uniform\nBinary Search [3], denoted as US, and Eytzinger Layout Search, denoted as EB.\nFor the convenience of the reader, details about all the above-mentioned search proce-\ndures are given in Appendix A.1. We anticipate that other routines could be considered\nin this study, such as Interpolation Search or its variant TIP [ 50]; however, the extensive\nexperiments conducted in [ 51] show that they are not competitive in the Learned Indexing\nscenario. Therefore, in order to keep this paper focused on relevant contributions, they are\nomitted here.\n2.2. Learning from Data to Speed Up Sorted Table Search: A Simple View with an Example\nKraska et al. [ 2] proposed an approach that transforms the Predecessor Search problem\ninto a learning-prediction one. With reference to Figure 1, the model learned from the data\nis used as a predictor of where a query element may be in the table. To ﬁx ideas, Binary\nSearch is then performed only on the interval returned by the model.\nQuery Element\n{1\n5\n11\n14\n58\n59\n60\n97\n100\n101Model\nFigure 1. A general paradigm of Learned Searching in a Sorted Table [9] . The model is trained on\nthe data in the table. Then, given a query element, it is used to predict the interval in the table of\nwhere to search (included in brackets in the ﬁgure).\nWe now outline the simplest technique that can be used to build a model for Aand pro-\nvide an example. It relies on Linear Regression, with Mean Square Error Minimization [ 52].\nWe start with the example. Consider Figure 2 and the table Ain the caption.\n\nData 2023 ,8, 56 7 of 33\n• Ingredient One of Learned Indexing: The Cumulative Distribution Function of a\nSorted Table . With reference to Figure 2a, we can plot the elements of Ain a graph,\nwhere the abscissa reports the value of the elements in the table and the ordinates\nare their corresponding ranks. The result of the plot is reminiscent of a discrete\nCumulative Distribution Function that underlines the table. The speciﬁc construction\nexempliﬁed here can be generalized to any sorted table as discussed in Marcus et\nal. [9]. In the literature, for a given table, such a discrete curve is referenced as CDF .\n• Ingredient Two of Learned Indexing: A Model for the CDF . Now, it is essential to\ntransform the discrete CDF into a continuous curve. The simplest way to do this\nis to ﬁt a straight line of equation F(x) = ax+bto the CDF (this process is shown\nin Figure 2b). In this example, we use Linear Regression with Mean Square Error\nMinimization in order to obtain aand b. They are 0.01 and 0.85, respectively.\n• Ingredient Three of Learned Indexing: The Model Error Correction . Since Fis an\napproximation of the ranks of the elements in the table, in applying it to an element\nin order to predict its rank, we may produce an error e. With reference to Figure 2c,\napplying the model to the element 398, we obtain a predicted rank of 4.68, instead of\n7, which is the real rank. Thus, the error made by the model F(x) =0.01\u0002x+0.85\non this element is e=7\u0000d4.68e=2. Therefore, in order to use the equation Fto\npredict where an element xis in the table, we must correct for this error. Indeed,\nwe consider the maximum error ecomputed as the maximum distance between the\nreal rank of the elements in the table and the corresponding rank predicted by the\nmodel. The maximum error eis used to set the search interval of an element xto be\n[F(x)\u0000e,F(x) +e]. In the example we are discussing, eis 3.\nMore in general, in order to perform a query, the model is consulted, and an interval\nin which to search is returned. Then, Binary Search is performed on that interval. Different\nmodels may use different schemes to determine the required range as outlined in Section 2.3.\nThe reader interested in a rigorous presentation of those ideas can consult Marcus et al. [ 12].\nIn this paper, we characterize the accuracy in the prediction of a model via the reduction\nfactor : the percentage of the table that is no longer considered for searching after the\nprediction of a rank.\nRegarding the diversity across models to determine the search interval and in order to\nplace all models on par, we empirically estimate the reduction factor of a model. With the\nuse of the model and over a batch of queries, we determine the length of the interval to\nsearch into for each query. Based on this, we can immediately compute the reduction factor\nfor that query. Then, we take the average of those reduction factors over the entire set of\nqueries as the reduction factor of the model for the given table.\n(a)\n••  ••••••••CDF\n10\n8\n2Positions\n0\n0 200 400 600 8006\n4\nValues (b)\n••  10 \n8 \n6 \n4\n2\nValues\nPositions ••••••••\n0\n0 200 400 600 800CDF with linear approximation (c)\n10 \n8 \n2\nValues\nPositions•\n0\n0 200 398 600 800Prediction with linear model\n• 4.76 e\nFigure 2. The Process of Learning a Simple Model via Linear Regression. Let table A be\n[47, 105, 140, 289, 316, 358, 386, 398, 819, 939 ]. (a) the empirical CDF ofA; (b) the line (in orange)\nassociated with a linear model obtained via Linear Regression; and ( c) the error emade by the model\nin predicting the query element 398.\n2.3. A Classiﬁcation of Learned Indexing Models\nWith the exception of the Eytzinger Binary Search, all procedures mentioned in Section 2.1\nhave a natural learned version. Indeed, all models currently known in the literature naturally\nfit sorted table layouts for the final search stage; however, for that purpose, array layouts other\nthan sorted or more complex data structures cannot be used. Given a learned version of the\ntwo mentioned procedures, the time and space performances depend critically on the model\n\nData 2023 ,8, 56 8 of 33\nused to predict the interval to search into. Here, we propose a classification of models that\ncomprises four classes.\nThe ﬁrst two classes, shown in Figure 3, consist of models that use constant space,\nwhile the other two, shown in Figure 4, consist of models that use space as a function of\nsome model parameters. For each of them, the reduction factor is determined as described\nin Section 2. Moreover, as already indicated, the Learned k-ary Search and the Synoptic\nRMI models are new and ﬁt quite naturally in the classiﬁcation that we present.\n(a)\n{1\n5\n11\n14\n58\n59\n60\n97\n100\n101Cubic (b)\nCubic Linear Quadratic1511 14 596097100 58\nFigure 3. Examples of various Learned Indexes that use constant space . (a) An Atomic Model,\nwhere the box cubic means that the CDF of the entire dataset is estimated by a cubic function via\nregression, in analogy with the linear approximation exempliﬁed in Figure 2. ( b) An example of a\nKO-US , with k=3. The top part divides the table into three segments, and it is used to determine\nthe model to pick at the second stage. Each box indicates which atomic model is used for prediction\non the relevant portion of the table.\nLinear\nCubic1 2Cubic Cubicn.......Linear\n1 Linear1 2Quadraticb C u b ic\n151114 60{{ {\n58 578 97100100 590630 59\n{{{\n{{\nkey:1 Model:f key:58 Model:f key:97 Model:fkey:58 Model:fkey:1 Model:f\n12 345\n1\n51 97100 11 14585960 1019758\n0123 4 5 6 7\nkey\nIndex\nFigure 4. Examples of various Learned Indexes that use space in functions of parameters (see\nalso [ 9]). (Left) An example of an RMI with two layers and branching factor equal to b. The top box\nindicates that the lower models are selected via a linear function. As for the leaf boxes, each indicates\nwhich Atomic Model is used for prediction on the relevant portion of the table. ( Center ) An example\nof aPGM Index. At the bottom, the table is divided into three parts. A new table is thus constructed,\nand the process is iterated. ( Right ) An example of an RSIndex. At the top are the buckets where\nelements fall based on their three most signiﬁcant digits. At the bottom, a linear spline approximating\ntheCDF of the data is shown, including suitably chosen spline points. Each bucket points to a spline\npoint so that, if a query element falls in a bucket (say six), the search interval is limited by the spline\npoints pointed to by that bucket and the one preceding it (ﬁve in our case).\n2.3.1. Atomic Models: One Level and No Branching Factor\n• Simple Regression [52]. We use linear, quadratic and cubic regression models. Each\ncan be thought of as an Atomic Model in the sense that it cannot be divided into\n“sub-models\". Figure 3a provides an example. We report that the most appropriate\nregression model in terms of the query times and a reduction factor is the cubic\n\nData 2023 ,8, 56 9 of 33\none. We omit those results for brevity and to keep our contribution focused on the\nimportant ﬁndings. However, they can be found in [ 51]. For this reason, the cubic\nmodel, indicated in the rest of the manuscript by C, is the only one that is included in\nwhat follows.\n2.3.2. A Two-Level Hybrid Model with a Constant Branching Factor\n• KO-US: Learned k-ary Search . This model partitions the table into a ﬁxed number\nof segments, bounded by a small constant, i.e., at most 20 in this study, in analogy\nwith a single iteration of the k-ary Search routine [ 53,54]. An example is provided\nin Figure 3b. For each segment, Atomic Models are computed to approximate the\nCDF of the table elements in that segment. Finally, the model that guarantees the best\nreduction factor is assigned to each segment. As for the prediction, a sequential search\nis performed for the second level segment to pick and the corresponding model is\nused for the prediction, followed by Uniform Binary Search, since it is superior to\nthe Standard one (data not reported and available upon request). We anticipate that,\nfor the experiments conducted in this study, kis chosen in the interval [3, 20]. For\nconciseness, only results for the model with k=15are reported, since it is the value\nwith the best performance in terms of query time (data not reported and available\nupon request). Accordingly, from now on, KO-US indicates the model with k=15.\n2.3.3. Two-Level RMIs with Parametric Branching Factor\n• Heuristically Optimized RMIs. Informally, an RMI is a multi-level, directed graph,\nwith Atomic Models at its nodes. When searching for a given key and starting with the\nﬁrst level, a prediction at each level identiﬁes the model of the next level to use for the\nnext prediction. This process continues until a ﬁnal level model is reached. This latter\nis used to predict the table interval to search into. As indicated in the benchmarking\nstudy, in most applications, a generic RMI with two layers, a tree-like structure and a\nbranching factor bsufﬁces. An example is provided in Figure 4 on the left. It is to be\nnoted that the Atomic Models are RMI s. Moreover, the difference between Learned\nk-ary Search and RMI s is that the ﬁrst level in the former partitions the table, while\nthat same level in the latter partitions the universe of the elements.\nFollowing the benchmarking study and for a given table, we use two-layer RMI s\nthat we obtain using the optimization software provided in CDFShop , which returns\nup to ten versions of the generic RMI for a given input table. For each model, the\noptimization software picks an appropriate branching factor and the type of regression\nto use within each part of the model—the latter quantities being the parameters\nthat control the precision of its prediction as well as its space occupancy. It is also\nto be remarked, as indicated in [ 12], that the optimization process provides only\napproximations to the real optimum and is heuristic in nature with no theoretic\napproximation performance guarantees. The problem of ﬁnding an optimal model in\npolynomial time is open.\n• SY-RMI: A Synoptic RMI. For a given set of tables of approximately the same size,\nwe use CDFShop as above to obtain a set of models (at most 10 for each table). For\nthe entire set of models thus obtained and each model in it, we compute the ratio\n(branching factor)/(model space), and we take the median of those ratios as a measure\nof the branching factor per unit of model space, denoted as UB. Among the RMI s\nreturned by CDFShop , we pick the relative majority winner, i.e., the one that provides\nthe best query time, averaged over a set of simulations. When one uses such a model\non tables of approximately the same size as the ones used as input to CDFShop ,\nwe set the branching factor to be a multiple of UB, which depends on how much\nspace the model is expected to use relative to the input table size. This model can\nbe intuitively considered as the one that best summarizes the output of CDFShop\nin terms of the query time for the given set of tables. The ﬁnal model is informally\nreferred to as Synoptic.\n\nData 2023 ,8, 56 10 of 33\n2.3.4. CDF Approximation-Controlled Models\n• PGM [13]. This is also a multi-stage model, built bottom-up and queried top-down. It\nuses a user-deﬁned approximation parameter e, which controls the prediction error at\neach stage. With reference to Figure 4 in the center, the table is subdivided into three\npieces. A prediction in each piece can be provided via a linear model guaranteeing\nan error of e. A new table is formed by selecting the minimum values in each of the\nthree pieces. This new table is possibly again partitioned into pieces, in which a linear\nmodel can make a prediction within the given error.\nThe process is iterated until only one linear model sufﬁces, as in the case in the\nﬁgure. A query is processed via a series of predictions, starting at the root of the tree.\nFurthermore, in this case, for a given table, at most ten models were built as prescribed\nin the benchmarking study with the use of the parameters, software and methods\nprovided there, i.e, SOSD . It is to be noted that the PGM index, in its bi-criteria\nversion, is able to return the best query time index, within the given amount of space\nthat the model is supposed to use. Experiments are also performed with this version of\nthePGM , denoted for brevity as B-PGM . The interested reader can ﬁnd a discussion\nregarding more variants of this PGM version in [51].\n• RS[17]. This is a two-stage model. It also uses a user-deﬁned approximation parame-\ntere. With reference to Figure 4 on the right, a spline curve approximating the CDF of\nthe data is built. Then, the radix table is used to identify spline points to use to reﬁne\nthe search interval. Furthermore, in this case, we performed the training as described\nin the benchmarking study.\nIn what follows, for ease of reference, models in the first two classes are referred to as\nconstant-space models, while the ones in the remaining classes are parametric-space models.\n3. Experimental Methodology\nOur experimental setup closely follows the one outlined in the already mentioned\nbenchmarking study by Marcus et al. [ 9]. Since an intent of this study is to gain deeper\ninsights regarding the circumstances in which learned versions of Sorted Table Search\nprocedure and indexes are proﬁtable in small additional space with respect to the one\ntaken by the input table, across the main memory hierarchy, we derive our own benchmark\ndatasets from the ones in the study by Marcus et al. [9].\n3.1. Hardware\nAll the experiments were performed on a workstation equipped with an Intel Core\ni7-8700 3.2GHz CPU with three levels of cache memory: (a) 64 kb of L1cache; (b) 256 kb\nofL2cache; and (c) 12 Mb of shared L3cache. The total amount of system memory is\n32 Gbytes of DDR4. The operating system is Ubuntu LTS 20.04.\n3.2. Datasets\nThe same real datasets of the benchmarking study are used. In particular, attention is\nrestricted to integers only, each represented with 64 bits unless otherwise speciﬁed. For\nthe convenience of the reader, a list of those datasets, with an outline of their content, is\nprovided next.\n• amzn : book popularity data from Amazon. Each key represents the popularity of\na particular book. Although two versions of this dataset, i.e., 32-bit and 64-bit, are\nused in the benchmarking, no particular differences were observed in the results of\nour experiments, and, for this reason, we report only those for the 64-bit dataset. The\ninterested reader can ﬁnd the results for the 32-bit version in [51].\n• face: randomly sampled Facebook user IDs. Each key uniquely identiﬁes a user.\n• osm : cell IDs from Open Street Map. Each key represents an embedded location.\n• wiki : timestamps of edits from Wikipedia. Each key represents the time an edit\nwas committed.\n\nData 2023 ,8, 56 11 of 33\nMoreover, for the purpose of this research, as already mentioned above, additional\ndatasets were extracted from the ones just mentioned. For each of those datasets, three new\nones were obtained in order to ﬁt each lower level of the internal memory hierarchy. In\nparticular, each new dataset was obtained by sampling the original one so that the CDF was\nsimilar to the original one. The interested reader can ﬁnd more details of this extraction\nprocedure in Appendix A.2. Letting nbe the number of elements in a table, for the computer\narchitecture that was used, the details of the generated tables are the following.\n\u000f Fitting in L1 cache: cache size 64 Kb. Therefore, n=3.7Kwas chosen.\n\u000f Fitting in L2 cache: cache size 256 Kb. Therefore, n=31.5Kwas chosen.\n\u000f Fitting in L3 cache: cache size 8 Mb. Therefore, n=750Kwas chosen.\n\u000f Fitting in PC main memory (L4): memory size 32 Gb. Therefore, n=200Mwas\nchosen, i.e., the entire dataset.\nThe rationale for the choice of those datasets, in particular the ones coming from the\nbenchmarking study, is that they provide different Empirical CDFs , as shown in Figure 5a,\nand this allows us to measure the performance of Learned Indexes considering different\npossible characteristics of real-world data. It is to be noted that the face dataset is somewhat\nspecial. Indeed, the shape of its CDF (see Figure 5a) is determined by 21 outliers at the end\nof the table: all the elements of that dataset, up to the ﬁrst outlier, have essentially the same\ndistance between consecutive elements—that is, they are all in a straight line.\nThis regularity breaks with the ﬁrst outlier that, together with the other ones, does\nnot follow such a nice pattern. For lower memory levels, the CDF of the corresponding\nface datasets becomes a straight line, as exempliﬁed in Figure 5b for the L3memory level.\nAs for the remaining datasets, their smaller versions closely follow the CDF of the largest\ndatasets as again exempliﬁed in Figure 5b for the L3memory level.\n(a)\n0 2 4 6 80.00.51.01.52.0amzn\n0.0 0.2 0.4 0.6 0.8 1.00.00.51.01.52.0face\n0.0 0.5 1.00.00.51.01.52.0osm\n1.00 1.05 1.10 1.15 1.20 0.00.51.01.52.0wikix 108x 108\nx 108x 108x 1018x 1019\nx 10x 10919 (b)\n0 2 4 6 8\nx 100246amzn\n0 2 4 6 80face\n0.0 0.5 1.00osm\n1.0 1.1 1.20wiki246\n246\n246\n246\n24618x 105x 105\nx 105x 105x 1010\nx 1019\nx 109\nFigure 5. The CDF of the main datasets . For each dataset coming from the benchmarking study, the\nvalue of each of its elements is reported on the abscissa and its position on the ordinate. In particular,\nFigure ( a) is referred to the L4memory level, while ( b) toL3.\nAs for query dataset generation, for each of the tables built as described above, we\nextract uniformly and at random (with replacement) from the universe U, a total of two\nmillion elements, 50% of which are present and 50% absent, in each table. For coherence, all\nof the query experiments were performed within SOSD , suitably modiﬁed to accommodate\nall Learned Indexes used in this research. The query time that we report is an average taken\non a batch of two million queries executed by a search routine or a Learned Index.\nThis is essential for Learned Indexes: a measure of a single query performance would\nbe unreliable [ 55], and in fact SOSD does not allow it, while the method we chose is\ncompliant with the literature [ 9]. Such a limitation makes it unreliable to measure certain\nrelevant performance parameters of a Learned Index, such as, for each query, the amount\nof time spent for prediction and the amount of time spent for searching.\nIn fact, to the best of our knowledge, none of the papers reporting on Learned Indexing\nprovided such a breakdown. Rather they concentrated on the accuracy of the predictions.\nFor completeness, we mention that the query time estimates adopted in the current state of\n\nData 2023 ,8, 56 12 of 33\nthe art and followed here, are accurate—that is, the processing time of a batch of queries\nis subject to very little standard deviation when averaged over independent executions.\nAlthough not perceived as essential in previous work, we provide a highlight of such an\nassessment using the new and the RMI ,PGM and RSmodels on the amzn datasets. The\nresults are reported in Table A8 of Appendix A.4. Finally, in terms of theoretic worst-case\nanalysis, the prediction for the RMI s used here takes O(1)time and O(logn)time for the\nPGM and the RS.\n4. Training of the Novel Models: Analysis and Insights into Model Training\nWe now focus on the training phase of the novel models, and we compare their\nperformance with the literature standards included in this research. In order to assess\nhow well a Learned Index model can be trained, three indicators are important: the time\nrequired for learning, the reduction factor that one obtains and the time needed to perform\nthe prediction. A quantiﬁcation of the ﬁrst parameter is provided and discussed here. The\nother two indicators are strongly dependent on each other, with the reduction factor being\nrelated to space. In turn, those two indicators affect the query time. Therefore, they are\nbest discussed in Section 5. We anticipate that our analysis of the training time performed\nhere provides useful and novel insights into model training for Learned Indexing. All the\ntraining experiments were performed on the datasets mentioned in Section 3.2 across all\ninternal memory levels.\n4.1. Mining SOSD Output for the Synoptic RMI\nAs anticipated in Section 2.3, in order to set the levels and UBof the Synoptic RMI ,\nit is necessary to process the output of SOSD for each dataset and memory level. Indeed,\nas described in Section 2.3, once we set a space budget for the model, the corresponding\nbranching factor was computed by multiplying it by UB. In particular, we computed three\nversions of a Synoptic RMI using a percentage of space of 0.05%, 0.7% and 2% with respect\nto the input table size.\nWith regard to the layers choice, the simulation to identify the relative majority RMI s\nwas performed on query datasets extracted as described in the previous Section but using\nonly 1%of the number of query elements speciﬁed there. The statistics regarding the results\nof such a simulation are summarized in Figure 6. In particular, for each memory level, we\nreport the computed UB. Furthermore, limited to the top layer of an RMI , we also report\nthe models associated with the best ones. The time it took to identify the proper Synoptic\nRMI (average time per element, over all the RMI s returned by CDFShop , denoted as the\nmining time) is also reported, together with the same time required to obtain the output\nofCDFShop .\nAs is evident, the mining time is comparable with the performance of CDFShop . It is\nalso evident from that ﬁgure that the variety of best-performing models represents various\nchallenges for the learning of the CDF of real datasets. Therefore, given such a variety, it\nis far from obvious that the median UBis the same for each memory level. Moreover, the\nrelative majority model is also the same across memory levels, i.e., linear spline, with linear\nmodels for each segment of the second layer.\n\nData 2023 ,8, 56 13 of 33\nlinear_spline robust_linearradix22cubic050100%L1\nMining Time:        \nCDF Shop Time:\nUB: \nradix\nlinear_splineradix18050100%\nlinear_splinelinearradix18\nTop Layer050100%Top Layer  L3\nradix18\nlinear_spline robust_linearradix22\nTop Layer050100%-62.8x10-61.44x10\n-24.2x10L2\nMining Time:        \nCDF Shop Time:\nUB: -66.6x10-66.28x10\n-24.2x10        2.8x10-6\n1.44x10-6Mining Time:\nCDF Shop Ti-2me:\nUB: 4.2x10\n        1.1x10-6Mining Time:\nCDF Shop Ti-2me:\nUB: 4.2x10        7.5x10-7Top Layer L4\n        1.1x10-7Mining Time:\nCDF Shop Ti-2me:\nUB: 4.2x10        2.12x10-7\nFigure 6. Time and UBfor the identiﬁcation of the Synoptic RMIs. For each memory level, only\nthe top layer of the various models is indicated in the abscissa, while the ordinate indicates the\nnumber of times, in percentage, that the given model is the best in terms of the query performance\non a table. On top of each histogram, we report the branching factor per unit of space as well as the\nmining time (in seconds) to build the synoptic models. For comparison, we also report the same time\nspent in obtaining the output of CDFShop .\n4.2. Training Time Comparison Between Novel Models and the State of the Art\nIn what follows, we divide the training-time comparison into two groups: constant-\nand parametric-space models. For the ﬁrst group, we consider the new model and only the\nCubic Atomic Model, excluding the linear and quadratic ones for the reasons mentioned\nearlier in this paper. For the Cubic Model, the training time on a given dataset is due to\nthe computation of its parameters via polynomial regression. As for the Learned k-ary\nSearch Model, its training consists of partitioning the table into ksegments. Then, for each\nsegment, Atomic Models are used to approximate the local CDF of the elements belonging\nto that segment, and, among them, the model with the best reduction factor is chosen.\nFor each dataset and each memory level, the resulting training times are reported in\nTable 1 and Tables A2–A4 of Appendix A.3. As expected, the Learned k-ary Search Model\nis slower than the Cubic Atomic Model; however, the important fact is that the slowdown\nis due to constant multiplicative factors rather than being of an order of magnitude—that\nis, the slow-down is tolerable. Another additional and counter-intuitive ﬁnding is that\nthe training time of both models, on average, is better for the cases of large datasets with\nrespect to smaller ones. We analyzed the training code in order to obtain insight into such a\nfact. We found that the cost of the matrix products involved in the training computation\nof both models depends on the size of the involved operands. As the size of the dataset\ngrows, such a cost is amortized on a larger and larger number of elements.\n\nData 2023 ,8, 56 14 of 33\nTable 1. Constant-space model training time for the L4 tables . The ﬁrst column indicates the\ndatasets. The remaining columns indicate the model used for the learning phase. Each entry reports\nthe training time in seconds and per element.\nKO-US C\namzn 3.7\u000210\u000081.4\u000210\u00008\nface 3.6\u000210\u000081.4\u000210\u00008\nosm 3.6\u000210\u000081.4\u000210\u00008\nwiki 3.6\u000210\u000081.4\u000210\u00008\nRegarding the second group, we consider the new model and the ones described in\nSection 2.3, i.e., RMI ,PGM and RS. The training time was computed using two different\nplatforms: CDFShop in the case of the Synoptic RMI and RMI as well as SOSD forPGM\nand RS. It is useful to recall that, in the case of the state-of-the-art models, the result of a\nsingle execution of those two platforms returns a batch of up to ten models, and thus the\nreported times refer to the execution of the entire learning suite—that is, the training of\nthose models consists of a batch of model instances from which a user can choose.\nOn the other hand, for the Synoptic RMI , this refers to the training of a single RMI\nwith a given branching factor and layer composition. For each dataset and each memory\nlevel, the results are reported in Table 2 and Tables A5–A7 of Appendix A.3. The time\nneeded to train the Synoptic RMI is comparable to the one needed to train a batch of\nRMI models. This latter, as already known, is worse than the time to train a batch of RS\norPGM models. Such results are not considered problematic for the deployment of the\nRMI in application contexts, and the training time of the Synoptic RMI is in-line with the\nmentioned literature standards. For completeness, we mention that the reason for which\nthe time needed to train a unique Synoptic RMI model is very close to the training of a\nbatch on RMI models is due to the library start-up overhead time.\nSuch a time is mitigated for the case of the training of a batch of models, while it\nbecomes dominant in training a single model. Fortunately, the CDFShop or the SOSD\ntraining executions are a “one-time-only\" processes, in which the output can then be reused\nover and over again, suggesting that this overhead time is of little relevance for the case of\na production environment.\nTable 2. Parametric-model training time for the L4 tables . The ﬁrst column indicates the datasets.\nThe remaining columns indicate the model used for the learning phase. In particular, each entry\nreports the time to train the Synoptic RMI and an entire batch of models via the CDFShop and the\nSOSD libraries as speciﬁed in the main text. The time is in seconds and per element.\nCDFShop\nSY-RMI 2%CDFShop RMI SOSD RS SOSD PGM\namzn 1.1\u000210\u000062.2\u000210\u000062.1\u000210\u000075.0\u000210\u00007\nface 1.3\u000210\u000062.5\u000210\u000062.1\u000210\u000076.5\u000210\u00007\nosm 1.2\u000210\u000062.5\u000210\u000062.2\u000210\u000077.4\u000210\u00006\nwiki 1.1\u000210\u000062.2\u000210\u000061.9\u000210\u000074.1\u000210\u00007\n4.3. Insights into the Training Time of the RSandPGM Models\nAnother important contribution that this research provides is a more reﬁned assess-\nment of the relation between the RSand PGM indexes, in terms of the training time. In\nTable 3, for each dataset and memory level, we report the training times of those two\nindexes. As discussed in [ 17], those two Learned Indexes can both be built in one pass\nover the input with important implications: one being that they can be trained faster than\ntheRMI s—even one order of magnitude sped up. However, in that study as well as\n\nData 2023 ,8, 56 15 of 33\nin the benchmarking one, the RSwas reported as superior to the PGM in terms of the\ntraining time.\nIt is to be noted that the datasets that they used are the largest ones in this study. With\nreference to Table 3, our experiments conﬁrm such a ﬁnding. On the other hand, the PGM\nis more effective in terms of the training time across the lower memory hierarchy. The\nreason may be the following. Those two indexes both use streaming procedures in order to\napproximate the CDF of the input dataset within a parameter evia the use of straight-line\nsegments that partition the universe. The main difference between the two is that the\nlatter ﬁnds an optimal partition, determined via a well-known algorithm (see references\nin [13]), while the former ﬁnds a partition that approximates the optimal as described\nin [18]. Such an approximation algorithm is supposed to be faster than the optimal one;\nhowever, apparently, this speed pays off on large datasets.\nTable 3. Comparison between the RS and PGM training times . For each dataset and memory level,\nwe report the training times for the RSand PGM models in seconds. Panel (a) refers to memory\nlevels L1and L2, while Panel (b) refers to L3and L4.\nPanel (a)\nL1 L2\nSOSD RS SOSD PGM SOSD RS SOSD PGM\namzn 3.5\u000210\u000065.0\u000210\u000073.5\u000210\u000075.0\u000210\u00008\nface 1.1\u000210\u000063.9\u000210\u000071.1\u000210\u000073.9\u000210\u00008\nosm 6.9\u000210\u000064.0\u000210\u000076.9\u000210\u000074.0\u000210\u00008\nwiki 1.0\u000210\u000053.7\u000210\u000071.0\u000210\u000063.7\u000210\u00008\nPanel (b)\nL3 L4\nSOSD RS SOSD PGM SOSD RS SOSD PGM\namzn 2.4\u000210\u000083.4\u000210\u000082.1\u000210\u000075.0\u000210\u00007\nface 1.4\u000210\u000082.4\u000210\u000082.1\u000210\u000076.5\u000210\u00007\nosm 3.5\u000210\u000083.8\u000210\u000082.2\u000210\u000077.4\u000210\u00007\nwiki 5.1\u000210\u000083.7\u000210\u000081.9\u000210\u000074.1\u000210\u00007\n5. Query Experiments\nThe query experiments were performed using all the methods described in Sections 2.1\nand 2.3. The query datasets were generated as described in Section 3.2, and the models\nwere trained as described in Section 4. Following that section, we divided the presentation\nof the query experiments and the relative discussion into two groups. For both groups,\nfor conciseness, we report here only the experiments for the amzn and the osm datasets\nsince they are representative of two different levels of difﬁculty in learning their CDF s. The\nresults regarding the other datasets are reported in Appendix A.5.\n5.1. Constant-Space Models\nThe results of the experiments for this group of models are reported in Figures 7\nand 8 for the amzn and the osm datasets, respectively, and in Figures A2 and A3 of the\nAppendix A.5, for the remaining ones. In those ﬁgures, only the query time for Uniform\nBinary Search is reported, since the results are analogous to the ones obtained by using\nthe Standard routine. In addition, the query time for the Eytzinger Binary Search is\nalso reported as a useful baseline due to its superiority among the classic routines that\ntake constant additional space with respect to the size of the input table as discussed\nin [23]. From the mentioned ﬁgures, it is evident that the query performance of each model\n\nData 2023 ,8, 56 16 of 33\nconsidered here is highly inﬂuenced by how difﬁcult to learn the CDF of the input table is\nas explained next.\n• The Cubic Model achieved a high reduction factor, i.e., \u001999%, on the versions of the\nface dataset for the ﬁrst three levels of the internal memory hierarchy, and it was\nalso the best performing, even compared to the Eytzinger Layout routine. This is a\nquite remarkable achievement; however, the involved datasets had an almost uniform\nCDF , while a few outliers disrupt this uniformity on the L4version of that dataset (see\nFigure 5 and the discussion regarding the face dataset in Section 3.2).\n• The Learned k-ary Search Model achieved a high reduction factor on all versions of\ntheamzn and the wiki datasets, i.e.,\u001999.73 and was faster than the Uniform Binary\nSearch and the Cubic Model. Those datasets have a regular CDF across all the internal\nmemory levels. It is to be noted that the Eytzinger Layout routine is competitive with\nthe Learned k-ary Search Model.\n• No constant space Learned Model won on the difﬁcult-to-learn dataset. The osm\ndataset has a difﬁcult-to-learn CDF (see Figure 5), and such a characteristic is preserved\nacross the internal memory levels. The Learned k-ary Search Model achieved a\nrespectable reduction factor, i.e., \u001998%, but no speed increase with respect to Uniform\nBinary Search. In order to obtain insights into such a counter-intuitive behavior, we\nperformed an additional experiment.\nFor each representative dataset and as far as the Learned k-ary Search Model is\nconcerned, we computed two kinds of reduction factors: the ﬁrst was the global one,\nachieved considering the size of the entire table, while the second was the local one,\ncomputed as the average among the reduction factors of each segment. Those results\nare reported in Table 4. For the osm dataset, it is evident that the local reduction\nfactors are consistently lower than the global ones, highlighting that its CDF is also\nlocally difﬁcult to approximate, which, in turn, implies an ineffective use of the local\nprediction for the Learned k-ary Search, resulting in poor time performance. Finally, it\nis to be noted that the Eytzinger Layout routine was the best performing.\nIn conclusion, in applications where there is a constant space constraint with respect\nto the input table and where a layout other than sorted can be used, then the Eytzinger\nBinary Search is still the best choice unless the CDF of the input dataset is particularly\neasy to approximate. If such a layout cannot be afforded, the best choice is the use of a\nconstant-space model, in particular the Learned k-ary Search Model only for datasets with\naCDF that is simple to approximate, otherwise the use of Uniform Binary Search alone\nis indicated.\nOur research extends the results in [ 23] regarding the Eytzinger Binary Search routine:\neven compared to Learned Indexes that use constant space, it still results as competitive\nand many times superior to the others.\nTable 4. Global and local reduction factors . For the two representative datasets, i.e., amzn and osm ,\nand for each memory level, in each entry, we report the global reduction factor (left) and the local\none (right).\namzn osm\nL1 99.94\u000099.48 98.12 \u000086.70\nL2 99.98\u000099.56 98.07 \u000086.57\nL3 99.98\u000099.53 97.98 \u000086.43\nL4 99.98\u000099.54 98.03 \u000086.57\n\nData 2023 ,8, 56 17 of 33\nNM-0% C-95.98% K0-US-99.94%00.20.40.60.811.2\n10-7 L1\nNM-0% C-96.02% K0-US-99.98%00.20.40.60.811.21.410-7 L2\nNM-0% C-95.94% K0-US-99.98%00.511.522.510-7 L3\nNM-0% C-96.03% K0-US-99.98%012345678\n10-7 L4US\nEB\nFigure 7. Constant-space-model query times for the amzn dataset. For each memory level, the blue\nbar reports the average query time in seconds of Uniform Binary Search using, from left to right, no\nmodel, Cubic model and KO-US . In addition, we report the average query time also for the Eytzinger\nBinary Search in the orange bar.\nNM-0% C-22.84% K0-US-98.12%00.20.40.60.811.21.410-7 L1\nNM-0% C-21.19% K0-US-98.07%00.511.5210-7 L2\nNM-0% C-14.50% K0-US-97.98%00.511.522.5310-7 L3\nNM-0% C-22.70% K0-US-98.03%00.20.40.60.8110-6 L4US\nEB\nFigure 8. Constant-space-model query times for the osm dataset. The ﬁgure legend is as in Figure 7.\n\nData 2023 ,8, 56 18 of 33\n5.2. Parametric-Space Models\nFor the convenience of the reader, we recall that the model classes involved are: RMI ,\nRS,PGM , the Synoptic RMI and the bi-criteria PGM , which are trained on the input\ndatasets (see Section 3.2) as reported in Section 4. The batch of queries used here was\nobtained as described in Section 3.2 and, for the query time, we took the average per\nelement as speciﬁed in that section. For each of the ﬁrst three model classes, we considered,\namong the trained models, the fastest in terms of the query time and those taking less than\n10% of space with respect to the the space taken by the input table.\nFor the other two model classes, we considered three increasing bounds on space, i.e.,\n0.05% ,0.7% and 2%, with respect to the space of the table alone. Moreover, as a measure\nof the Learned Indexes speed up, we also report the query time of the Uniform Binary\nSearch. The results of the corresponding experiments are reported in Figures 9 and 10 for\ntheamzn and osm datasets, respectively, and in Figures A4 and A5 of the Appendix A.5\nfor the remaining ones.\nAn interesting ﬁnding is that both the Synoptic RMI and the bi-criteria PGM per-\nformed better than Uniform Binary Search across datasets and memory levels using very\nlittle additional space—that is, one can enjoy the speed of Learned Indexes with a very\nsmall space penalty. Moreover, it is important to note that, except for the L1memory level,\nthe space of those two models is very close to the user-deﬁned bound. Furthermore, in\nterms of query performances, such two models seem to be complementary. In fact, the\nbi-criteria PGM performed better on the L1and L4memory levels, while the Synoptic RMI\nperformed better on the remaining ones. This complementary and effective control of space\nmakes these two models quite useful in practice.\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%024681012        L1\n7.27%0.93% 0.13%0.29% 0.60%0.85% 0.60%2.05%\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%02468101214       L2\n2.44%0.90%0.27% 0.04%0.06% 0.15%0.75% 0.15%2.01%\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%0510152025       L3\n6.55%4.20%0.15% 0.03%0.05% 0.03%0.74% 0.03%2.00%\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%010203040506070       L4\n6.29%3.35%2.82% 0.05%0.05% 0.69%0.74% 0.96%2.00%RMI\nRS\nPGM\nB-PGM\nSY-RMI\nUS10-810-8\n10-810-8\nFigure 9. Query times for the amzn dataset on Learned Indexes in small space. The methods are\nthe ones in the legend (the middle of the four panels; the notation is as in the main text, and each\nmethod has a distinct color). For each memory level, the abscissa reports methods grouped by space\noccupancy as speciﬁed in the main text. When no model in a class output by SOSD took at most 10%\nof additional space, that class was left absent. The ordinate reports the average query time in seconds,\nwith Uniform Binary Search executed in SOSD as a baseline (horizontal lines).\n\nData 2023 ,8, 56 19 of 33\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%024681012    L1\n8.40% 0.13%0.29% 0.60%0.85% 1.73%2.05%\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%02468101214 L2\n4.88%9.88%7.56% 0.04%0.06% 0.67%0.75% 2.07%2.01%\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%051015202530    L3\n1.64%1.78%6.50% 0.05%0.05% 0.69%0.74% 1.77%2.00%\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%010203040506070    L4\n6.29%7.95%7.43% 0.05%0.05% 0.70%0.74% 1.74%2.00%RMI\nRS\nPGM\nB-PGM\nSY-RMI\nUS10-810-8\n10-810-8\nFigure 10. Query times for the osm dataset on Learned Indexes in small space. The ﬁgure legend\nis as in Figure 9.\nIn addition to those ﬁndings, our research provides some more insights into the time–\nspace relation in Learned Indexes, thereby, extending the results of the benchmark study as\nwe now discuss.\n• Space Constraints and the Models Provided by SOSD. We ﬁxed a rather small space\nbudget, i.e., at most 10% of additional space in order for a model returned by SOSD\nto be considered. The RSIndex was not competitive with respect to the other Learned\nIndexes. Those latter consistently use less space and time across datasets and memory\nlevels. As for the RMI s coming out of SOSD , they were not able to operate in a small\nspace at the L1memory level. On the other memory levels, they were competitive\nwith respect to the bi-criteria PGM and the Synoptic RMI ; however, they required\nmore space with respect to them.\n• Space, Time and Accuracy of Models. As stated in the benchmarking study, a com-\nmon view of Learned Indexing Data Structures is as a CDF lossy compressor; see\nalso [ 2,13]. In this view, the quality of a Learned Index can be judged by the size of the\nstructure and its reduction factor. In that study, it was also argued that this view does\nnot provide an accurate selection criterion for Learned Indexes. Indeed, it may very\nwell be that an index structure with an excellent reduction factor takes a long time to\nproduce a search bound, while an index structure with a worse reduction factor that\nquickly generates an accurate search bound may be of better use. In the benchmarking\nstudy, they also provided evidence that the space–time trade-off is the key factor in\ndetermining which model to choose.\nOur contribution is to provide additional results supporting those ﬁndings. To this\nend, we conducted several experiments, whose results are reported in Tables 5 and 6\nand Tables A9 and A10 in the Appendix A.5. In these Tables, for each dataset, we\nreport a synopsis of three parameters, i.e., the query time and space used in addition\n\nData 2023 ,8, 56 20 of 33\nby the model and reduction factor, across all datasets and memory levels. In particular,\nfor each dataset, we compare the best-performing model with all the ones that use\nsmall space, taking, for each parameter, the ratio of the model/best model. The ratio\nvalues are reported from the second row of the table, and the ﬁrst row shows the\naverage values of the parameters for the best model.\nFirst, it is useful to note that, even in a small-space model, it is possible to obtain a\ngood, if not nearly perfect, prediction (i.e., a very high reduction factor). However,\nprediction power is somewhat marginal to assess performance. Indeed, across memory\nlevels, we see a space classiﬁcation of model conﬁgurations. The most striking feature\nof this classiﬁcation is that the gain in query time between the best model and the\nothers is within small constant factors, while the difference in space occupancy may\nbe, in most cases, several orders of magnitude different—that is, space is the key\nto efﬁciency.\nTable 5. A synoptic table of space, time and accuracy of the models on the amzn dataset . For each\nmemory level, we report, in the ﬁrst row, the best performing method for that memory level. The\ncolumns named time, space and reduction factor indicate, for this best model, the average query time\nin seconds, the average additional space used in Kb and the average of the empirical reduction factor.\nIn the second row, we report the versions of the RMI ,RS,PGM and Synoptic RMI models that use\nthe least space. In particular, the number next to the models represents, in percentage, the bound on\nthe used space with respect to the input dataset. The columns indicate the ratio of the model/best\nmodel of the time, space and reduction factor.\nL1\nTime Space Reduction Factor\nBest RMI 1.89\u000210\u000083.09 99.84\nB-PGM 0.05 4.03 1.30\u000210\u000022.50\u000210\u00001\nSY-RMI 0.05 3.77 2.85\u000210\u000021.75\u000210\u00001\nRS<10 2.58 7.06\u000210\u000019.29\u000210\u00001\nBest RMI 1.00 1.00 1.00\nL2\nTime Space Reduction Factor\nBest RMI 2.51\u000210\u000086.16 99.97\nB-PGM 0.05 3.78 1.62\u000210\u000029.16\u000210\u00001\nSY-RMI 0.05 3.74 2.60\u000210\u000026.44\u000210\u00001\nBest RS<10 2.38 3.68\u000210\u000019.92\u000210\u00001\nBest RMI 1.00 1.00 1.00\nL3\nTime Space Reduction Factor\nBest RMI 4.70\u000210\u000086.29\u0002103100.00\nB-PGM 0.05 2.07 3.05\u000210\u000041.00\nSY-RMI 0.05 1.49 4.79\u000210\u000049.99\u000210\u00001\nRS<10 1.59 4.00\u000210\u000021.00\nRMI<10 1.03 6.25\u000210\u000021.00\n\nData 2023 ,8, 56 21 of 33\nTable 5. Cont.\nL4\nTime Space Reduction Factor\nBest RMI 1.51\u000210\u000072.01\u0002105100.00\nB-PGM 0.05 1.85 3.93\u000210\u000031.00\nSY-RMI 0.05 1.18 3.97\u000210\u000031.00\nBest RS 1.19 7.16\u000210\u000021.00\nRMI<10 1.03 5.00\u000210\u000011.00\nTable 6. A synoptic table of space, time and accuracy of the models on the osm dataset . The legend\nis as in Table 5.\nL1\nTime Space Reduction Factor\nBest RMI 2.72\u000210\u000081.15\u000210399.87\nB-PGM 0.05 2.31 3.49\u000210\u000051.74\u000210\u00001\nSY-RMI 0.05 2.60 7.67\u000210\u000052.30\u000210\u00001\nBest RMI 1.00 1.00 1.00\nBest RS 1.19 4.33 \u000210 9.99\u000210\u00001\nL2\nTime Space Reduction Factor\nBest RMI 3.93\u000210\u000081.84\u000210399.97\nB-PGM 0.05 2.68 5.45\u000210\u000057.75\u000210\u00001\nSY-RMI 0.05 3.11 8.72\u000210\u000057.24\u000210\u00001\nRMI<10 1.73 6.71\u000210\u000039.87\u000210\u00001\nRS<10 1.93 1.36\u000210\u000029.79\u000210\u00001\nL3\nTime Space Reduction Factor\nBest RS 7.06\u000210\u000084.63\u0002104100.00\nB-PGM 0.05 2.40 6.22\u000210\u000059.98\u000210\u00001\nSY-RMI 0.05 2.63 6.52\u000210\u000059.31\u000210\u00001\nRMI<10 1.75 2.12\u000210\u000039.97\u000210\u00001\nRS<10 1.55 2.31\u000210\u000031.00\nL4\nTime Space Reduction Factor\nBest RS 2.04\u000210\u000075.08\u0002105100.00\nSY-RMI 0.05 2.52 1.57\u000210\u000039.99\u000210\u00001\nB-PGM 0.05 2.03 1.59\u000210\u000031.00\nRMI<10 1.18 1.98\u000210\u000011.00\nRS<10 1.05 2.50\u000210\u000011.00\n6. Conclusions and Future Directions\nIn this research, we provided a systematic experimental analysis regarding the ability\nof Learned Model Indexes to perform better than Binary Search in small space. This is\n\nData 2023 ,8, 56 22 of 33\nthe ﬁrst step forward in the full characterization of the time/space trade-off spectrum\nof Learned Indexes, with respect to Sorted Table Search routines that use constant addi-\ntional space.\nIn particular, in regard to the ﬁrst question stated in Section 1.2, i.e., how space-\ndemanding should be a predictive model in order to speed up Sorted Table Search proce-\ndures, we show that constant-space models may grant such a speeding up with respect to\nclassic versions of Binary Search unless the data CDF to be learned is complex. In addition,\nwe also show that models using a small percentage of additional space with respect to the\nSorted Table guarantee consistent speed ups of sorted layout Binary Search procedures\nacross Learned Indexes and datasets with different levels of CDF complexity to learn.\nIn regard to the second question, i.e., to what extent one can enjoy the speed up of\nthe search procedures provided by Learned Indexes with respect to the additional space\none needs to use, our experiments bring to light the existence of a large gap between the\nbest-performing methods and the others that we considered and that operate in small space.\nIndeed, the query time performance of the latter with respect to the former is bounded by\nsmall constants, while the space usage may differ even by ﬁve orders of magnitude.\nThese ﬁndings bring to light the acute need to investigate the existence of small-space\nmodels that should close the time gap mentioned earlier. Another important aspect, with\npotential practical impacts, is to devise models that can work on layouts other than Sorted,\ni.e., Eytzinger. Indeed, since the Eytzinger layout is consistently faster than the sorted\nones [ 23], it would be of interest to provide models that take advantage of this layout rather\nthan the sorted ones.\nAuthor Contributions: D.A., R.G. and G.L.B. contributed equally to the design of the algorithms\nand the analysis of the experiments. D.A. developed most of the code and performed most of the\nexperiments. R.G. wrote a substantial part of the paper, with substantial contributions by D.A. and\nG.L.B. to the presentation of the results in graphic and tabular form. All authors have read and agreed\nto the published version of the manuscript.\nFunding: This research was funded in part by the MIUR Project of National Relevance 2017WR7SHH\n“Multicriteria Data Structures and Algorithms: from compressed to learned indexes, and beyond”.\nAdditional support to RG was granted by Project INdAM—GNCS “Modellizazzione ed analisi di big\nknowledge graphs per la risoluzione di problemi in ambito medico e web”.\nData Availability Statement: All datasets are publicly available. The largest ones are accessible as\nstated in [9]. The datasets generated for this research are available at [56].\nConﬂicts of Interest: The authors declare no conﬂict of interest.\nAbbreviations\nThe following abbreviations are used in this manuscript:\nCDF Cumulative Distribution Function\nRMI Recursive Model Index\nPGM Piece-wise Geometric Model Index\nRS Radix Spline index\nALEX Adaptive Learned index\nSOSD Searching on Sorted Data\nKO-US Learned k-ary Search\nSY-RMI Synoptic RMI\nBS lower_bound search routine\nUS Uniform Binary Search\nEB Eytzinger Layout Search\n\nData 2023 ,8, 56 23 of 33\nC Cubic regression model\nB-PGM Bicriteria Piece-wise Geometric Model Index\nL1 cache of size 64kb\nL2 cache of size 256kb\nL3 cache of size 12Mb\nL4 memory size 32Gb\namzn the Amazon dataset\nface the Facebook dataset\nosm the OpenStreetMap dataset\nwiki the Wikipedia dataset\nAppendix A. Methods and Results: Additional Material\nAppendix A.1. Binary Search and Its Variants\nWith reference to the routines mentioned in the main text (Section 2.1) and following\nthe research in [23], we provide more details about two kind of layouts.\n1 Sorted. We use two versions of Binary Search for this layout. The template of the\nlower_bound routine is provided in Algorithm A1, while the Uniform Binary Search\nimplementation is given in Algorithm A2. In particular, this implementation of Binary\nSearch is as found in [23].\n2 Eytzinger Layout [ 23]. The sorted table is now seen as stored in a virtual complete\nbalanced binary search tree. Such a tree is laid out in Breadth-First Search order in\nan array. An example is provided in Figure A1. The implementation is reported in\nAlgorithm A3.\nAlgorithm A1 lower_bound Template.\n1:ForwardIterator lower_bound (ForwardIterator ﬁrst, ForwardIterator last, const T&\nval){\n2:ForwardIterator it;\n3:iterator_traits <ForwardIterator >::difference_type count, step;\n4:count = distance(ﬁrst,last);\n5:while (count >0){\n6: it = ﬁrst; step=count/2; advance (it,step);\n7: if (*it<val){\n8: ﬁrst=++it;\n9: count-=step+1;\n10: }\n11: else count=step;\n12: }\n13: return ﬁrst;\n14:}\n5\n79\n1113\n15 316\n1819\n2021\n25\n22 27\n20 16 9 215131925371115 18 22 27\nFigure A1. An example of Eyzinger layout of a table with 15 elements. Nodes with the same color\nin the tree are contiguous in the array. See also [23].\n\nData 2023 ,8, 56 24 of 33\nAlgorithm A2 Implementation of Uniform Binary Search. The code is as in [ 23] (see\nalso [3,54]).\n1:int UniformBinarySearch(int *A, int x, int left, int right){\n2:const int *base = A;\n3:int n = right;\n4:while (n >1) {\n5: const int half = n / 2;\n6: __builtin_prefetch(base + half/2, 0, 0);\n7: __builtin_prefetch(base + half + half/2, 0, 0);\n8: base = (base[half] <x) ? &base[half] : base;\n9: n -= half;\n10: }\n11: return (*base <x) + base - A;\n12:}\nAlgorithm A3 Uniform Binary Search with Eytzinger layout. The code is as in [23].\nint EytzingerLayoutSearch(int *A, int x, int left, int right){\nint i = 0;\n3:int n = right;\nwhile (i <n){\n__builtin_prefetch(A+(multiplier*i + offset));\n6: i = (x<=A[i]) ? (2*i + 1) : (2*i + 2);\n}\nint j = (i+1) >> __builtin_ffs(\u0018(i+1));\n9:return (j == 0) ? n : j-1;\n}\nAppendix A.2. Datasets: Details\nWith reference to the datasets mentioned in the main text (Section 3.2), we produce\nsorted tables of varying sizes so that each ﬁts into a level of the internal memory hierarchy\nas follows. Letting nbe the number of elements in a table, for the computer architecture we\nare using, the details of the tables we generate are as follows.\n\u000f Fitting in L1 cache: cache size 64 Kb. Therefore, we choose n=3.7K. For each\ndataset, the table corresponding to this type is denoted with the preﬁx L1, e.g.,\nL1_amzn , when needed. For each dataset, in order to obtain a CDF that resem-\nbles one of the original tables, we proceed as follows. Concentrating on amzn , since\nfor the other datasets the procedure is analogous, we extract uniformly and at random\na sample of the data of the required size. For each sample, we compute its CDF . Then,\nwe use the Kolmogorov–Smirnov test [ 57] in order to assess whether the CDF of the\nsample is different than the amzn CDF .\nIf the test returns that we cannot exclude such a possibility, we compute the PDF of\nthe sample and compute its KL divergence [ 58] from the PDF of amzn . We repeat\nsuch a process 100 times and, for our experiments, we use the sample dataset with the\nsmallest KL divergence.\n\u000f Fitting in L2 cache: cache size 256 Kb. Therefore, we choose n=31.5K. For each\ndataset, the table corresponding to this type is denoted with the preﬁx L2, when\nneeded. For each dataset, the generation procedure is the same as the one of the\nL1dataset.\n\u000f Fitting in L3 cache: cache size 8 Mb. Therefore, we choose n=750K. For each\ndataset, the table corresponding to this type is denoted with the preﬁx L3, when\nneeded. For each dataset, the generation procedure is the same as the one of the\nL1dataset.\n\nData 2023 ,8, 56 25 of 33\n\u000f Fitting in PC main memory: memory size 32 Gb. Therefore, we choose n=200M,\ni.e., the entire dataset. For each dataset, the table corresponding to this type is denoted\nwith the preﬁx L4.\nFor completeness, the results of the Kolmogorov–Smirnov Test as well as KL diver-\ngence computation are reported in Table A1. For each memory level (ﬁrst row) and each\noriginal dataset (ﬁrst column), we report the percentage of times in which the Kolmogorov–\nSmirnov test failed to report a difference between the CDF s of the dataset extracted uni-\nformly and at random from the original one and this latter, over 100 extractions. Moreover,\nthe KL divergence between the PDFs of the chosen generated dataset and the original one\nis also reported. From these results, it is evident that the PDF of the original datasets is\nquite close to the one of the extracted datasets.\nTable A1. Results of the Kolmogorov–Smirnov test . For each memory level and each dataset, the\nresults of the Kolmogorov–Smirnov test ( %succ columns) and of the KL divergence computation\n(KLdiv columns) are reported.\nL1 L2 L3\nDatasets %succ KLdiv %succ KLdiv %succ KLdiv\namzn 1009.54\u000210\u00006\u0006\n7.27\u000210\u000014 1007.88\u000210\u00005\u0006\n7.97\u000210\u000013 1001.88\u000210\u00003\u0006\n1.52\u000210\u000011\nface 1001.98\u000210\u00005\u0006\n1.00\u000210\u000012 1007.98\u000210\u00005\u0006\n4.43\u000210\u000013 1001.88\u000210\u00003\u0006\n1.24\u000210\u000011\nosm 1009.38\u000210\u00006\u0006\n4.51\u000210\u000014 1007.88\u000210\u00005\u0006\n3.46\u000210\u000013 1001.88\u000210\u00003\u0006\n9.55\u000210\u000012\nwiki 1009.47\u000210\u00006\u0006\n5.27\u000210\u000014 1007.87\u000210\u00005\u0006\n5.64\u000210\u000013 1001.88\u000210\u00003\u0006\n1.25\u000210\u000011\nAppendix A.3. Training of the Novel Models: Analysis and Insights into Model\nTraining—Additional Results\nFollowing the same approach as used in Section 4 of the main text, we divide the\ntraining time analysis into two groups: constant- and parametric-space models.\n• Tables A2–A4 report the experiments concerning the constant-space models for the\ndatasets L1,L2and L3.\n• Tables A5–A7 report the experiments concerning the parametric-space models for the\ndatasets L1,L2and L3.\nTable A2. Constant-space model training time for L1 Tables . The ﬁrst column indicates the datasets.\nThe remaining columns indicate the models used for the learning phase. Each entry reports the\ntraining time in seconds and per element.\nKO-US C\namzn 5.3\u000210\u000071.0\u000210\u00007\nface 5.5\u000210\u000078.5\u000210\u00008\nosm 4.6\u000210\u000079.9\u000210\u00008\nwiki 9.0\u000210\u000077.9\u000210\u00008\n\nData 2023 ,8, 56 26 of 33\nTable A3. Constant-space model training time for L2 Tables . The table legend is as in Table A2.\nKO-BFS C\namzn 1.8\u000210\u000073.1\u000210\u00007\nface 1.0\u000210\u000072.8\u000210\u00007\nosm 1.2\u000210\u000072.9\u000210\u00007\nwiki 1.0\u000210\u000072.7\u000210\u00007\nTable A4. Constant-space model training time for L3 Tables . The table legend is as in Table A2.\nKO-US C\namzn 6.3\u000210\u000081.9\u000210\u00008\nface 3.9\u000210\u000081.9\u000210\u00008\nosm 4.4\u000210\u000082.0\u000210\u00008\nwiki 4.1\u000210\u000081.9\u000210\u00008\nTable A5. Parametric-space model training time for L1 Tables . The ﬁrst column indicates the\ndatasets. The remaining columns indicate the models used for the learning phase. In particular,\neach entry reports the time used by the CDFShop and SOSD libraries to train the entire batch of\nparametric models in seconds and per element.\nCDFShop\nSY-RMI 2%CDFShop RMI SOSD RS SOSD PGM\namzn 5.2\u000210\u000065.6\u000210\u000063.5\u000210\u000065.0\u000210\u00007\nface 4.1\u000210\u000064.6\u000210\u000061.1\u000210\u000063.9\u000210\u00007\nosm 2.8\u000210\u000042.9\u000210\u000046.9\u000210\u000064.0\u000210\u00007\nwiki 7.8\u000210\u000069.3\u000210\u000061.0\u000210\u000053.7\u000210\u00007\nTable A6. Parametric-space model training time for L2 Tables . The table legend is as in Table A5.\nCDFShop\nSY-RMI 2%CDFShop RMI SOSD RS SOSD PGM\namzn 5.2\u000210\u000065.6\u000210\u000073.5\u000210\u000075.0\u000210\u00008\nface 4.1\u000210\u000064.6\u000210\u000071.1\u000210\u000073.9\u000210\u00008\nosm 2.8\u000210\u000042.9\u000210\u000056.9\u000210\u000074.0\u000210\u00008\nwiki 7.8\u000210\u000069.3\u000210\u000071.0\u000210\u000063.7\u000210\u00008\nTable A7. Parametric-space model training time for L3 Tables . The table legend is as in Table A5.\nCDFShop\nSY-RMI 2%CDFShop RMI SOSD RS SOSD PGM\namzn 1.5\u000210\u000061.3\u000210\u000072.4\u000210\u000083.4\u000210\u00008\nface 1.5\u000210\u000051.6\u000210\u000061.4\u000210\u000082.4\u000210\u00008\nosm 1.2\u000210\u000051.3\u000210\u000063.5\u000210\u000083.8\u000210\u00008\nwiki 2.3\u000210\u000062.2\u000210\u000075.1\u000210\u000083.7\u000210\u00008\n\nData 2023 ,8, 56 27 of 33\nAppendix A.4. Accuracy of Query Time Evaluation\nWith reference to Section 3.2 of the main text, we provide a highlight that the processing\ntime for a batch of queries, over independent executions, is stable. In particular, we\nconcentrate on the amzn datasets and on the Learned k-are Search, the Synoptic RMI ,\ntheRMI , the PGM and the RSmodels. A query batch of 2 million elements, obtained as\nspeciﬁed in Section 3.2 of the main text, is processed 10 times. In Table A8, we report the\naverage batch-query time processing (with the standard deviation), which is indeed low.\nTable A8. Batch-query time processing over independent executions. The ﬁrst row indicates the\nmodel, while the memory level is indicated by the ﬁrst column. Each entry in the table indicates\nthe average time (in seconds) to execute 10 times the same batch of queries together with the\ncorresponding standard deviation.\nRMI PGM RS SY-RMI 0.05 KO-US\nL12.35\u000210\u00002\u0006\n5.56\u000210\u000043.27\u000210\u00002\u0006\n1.49\u000210\u000032.61\u000210\u00002\u0006\n3.69\u000210\u000042.34\u000210\u00002\u0006\n3.69\u000210\u000042.90\u000210\u00001\u0006\n1.1\u000210\u00002\nL23.02\u000210\u00002\u0006\n4.20\u000210\u000043.88\u000210\u00002\u0006\n9.93\u000210\u000043.44\u000210\u00002\u0006\n2.89\u000210\u000043.02\u000210\u00002\u0006\n4.20\u000210\u000042.99\u000210\u00001\u0006\n8.04\u000210\u00003\nL36.78\u000210\u00002\u0006\n1.02\u000210\u000037.16\u000210\u00002\u0006\n1.31\u000210\u000038.33\u000210\u00002\u0006\n1.40\u000210\u000036.78\u000210\u00002\u0006\n1.02\u000210\u000033.81\u000210\u00001\u0006\n1.09\u000210\u00002\nL41.66\u000210\u00001\u0006\n3.93\u000210\u000031.67\u000210\u00001\u0006\n1.90\u000210\u000031.62\u000210\u00001\u0006\n1.53\u000210\u000031.66\u000210\u00001\u0006\n3.94\u000210\u000037.37\u000210\u00001\u0006\n5.40\u000210\u00003\nAppendix A.5. Query Experiments—Additional Results\nIn this section, we report the experiments described and discussed in Section 5 of the\nmain text for the face and wiki datasets.\n• Figures A2 and A3 report the experiments concerning the constant-space models as\nin Section 5.1.\n• Figures A4 and A5 report the experiments concerning the parametric-space models\nas in Section 5.2.\n• Tables A9 and A10 report a synopsis of three parameters, i.e., the query time, space\nused in addition by the model and reduction factor as described in Section 5.2.\n\nData 2023 ,8, 56 28 of 33\nNM-0% C-99.68% K0-US-99.95%00.20.40.60.81\n10-7 L1\nNM-0% C-99.67% K0-US-99.96%00.20.40.60.811.21.410-7 L2\nNM-0% C-99.67% K0-US-99.96%00.511.522.510-7 L3\nNM-0% C-0.00% K0-US-99.52%0123456710-7 L4US\nEB\nFigure A2. Constant-space-model query times for the face dataset . For each memory level, the blue\nbar reports the average query time in seconds of Uniform Binary Search using, from left to right,\nno model, the Cubic model and KO-US . In addition, we also report the average query time for the\nEytzinger Binary Search in the orange bar.\nNM-0% C-43.56% K0-US-99.85%00.20.40.60.811.21.410-7 L1\nNM-0% C-31.99% K0-US-99.76%00.20.40.60.811.21.41.6\n10-7 L2\nNM-0% C-30.93% K0-US-99.73%00.511.522.510-7 L3\nNM-0% C-30.54% K0-US-99.73%00.20.40.60.8110-6 L4US\nEB\nFigure A3. Constant-space-model query times for the wiki dataset . The legend is as in Figure A2.\n\nData 2023 ,8, 56 29 of 33\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%024681012    L1\n0.13% 0.13%0.29% 0.13%0.85% 0.13%2.05%\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%02468101214    L2\n9.76%0.40% 0.04%0.06% 0.06%0.75% 0.06%2.01%\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%0510152025    L3\n0.41%9.89%5.24% 0.04%0.05% 0.51%0.74% 0.51%2.00%\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%010203040506070    L4\n6.29%0.25%2.74% 0.05%0.05% 0.70%0.74% 1.61%2.00%RMI\nRS\nPGM\nB-PGM\nSY-RMI\nUS10-810-8\n10-810-8\n10-8\nFigure A4. Query times for the face dataset on Learned Indexes in small space. The methods\nare the ones in the legend (middle of the four panels, the notation is as in the main text and each\nmethod has a distinct colour). For each memory level, the abscissa reports methods grouped by space\noccupancy, as speciﬁed in the main text. When no model in a class output by SOSD takes at most\n10% of additional space, that class is absent. The ordinate reports the average query time in seconds,\nwith Uniform Binary Search executed in SOSD as baseline (horizontal lines).\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%024681012    L1\n1.13% 0.13%0.29% 0.60%0.85% 0.60%2.05%\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%02468101214    L2\n9.76%3.03%1.00% 0.04%0.06% 0.21%0.75% 0.21%2.01%\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%0510152025    L3\n6.55%0.50%1.45% 0.05%0.05% 0.63%0.74% 0.63%2.00%\nSOSD<=10% Bound 0.05% Bound 0.7% Bound 2%010203040506070    L4\n0.79%7.82%0.89% 0.05%0.05% 0.29%0.74% 0.29%2.00%RMI\nRS\nPGM\nB-PGM\nSY-RMI\nUS10-810-810-810-8\n10-810-810-810-8\nFigure A5. Query times for the wiki dataset on Learned Indexes in small space. The ﬁgure legend\nis as in Figure A4.\n\nData 2023 ,8, 56 30 of 33\nTable A9. A synoptic table of space, time and accuracy of the models on the face dataset . For each\nmemory level, we report, in the ﬁrst row, the best performing method for that memory level. The\ncolumns named time, space and reduction factor indicate, for this best model, the average query time\nin seconds, the average additional space used and the average of the empirical reduction factor. From\nthe second row, we report the versions of the RMI ,RS,PGM and Synoptic RMI models that use the\nleast space. In particular, the numbers next to the models represent the percentage of the used space\nwith respect to the input dataset. The columns indicate the ratio of the model/best model of the time,\nspace and reduction factor.\nL1\nTime Space Reduction Factor\nBest PGM 2.26\u000210\u000084.00\u000210\u0000299.52\nBest PGM 1.00 1.00 1.00\nSY-RMI 0.05 2.33 2.20 6.30\u000210\u00001\nBest RMI 1.16 7.72\u00021011.00\nBest RS 1.17 5.90\u00021041.00\nL2\nTime Space Reduction Factor\nBest RMI 3.02\u000210\u000081.23\u000210 99.98\nB-PGM 0.05 1.89 8.13\u000210\u000039.98\u000210\u00001\nSY-RMI 0.05 1.89 1.30\u000210\u000029.40\u000210\u00001\nBest RMI 1.00 1.00 1.00\nBest RS 1.10 1.92\u00021021.00\nL3\nTime Space Reduction Factor\nBest RMI 6.11\u000210\u000087.86\u0002102100.00\nB-PGM 0.05 1.57 3.33\u000210\u000031.00\nRMI<10 1.19 3.13\u000210\u000021.00\nSY-RMI 0.7 1.22 5.62\u0002\u000210\u000021.00\nRS<10 1.53 7.54\u000210\u000011.00\nL4\nTime Space Reduction Factor\nBest RMI 1.80\u000210\u000072.01\u0002105100.00\nSY-RMI 0.05 3.74 3.97\u000210\u000031.32\u000210\u00002\nB-PGM 0.05 2.14 3.98\u000210\u000031.00\nBest RS 2.21 3.96\u000210\u000021.00\nRMI<10 1.06 5.00\u000210\u000011.00\n\nData 2023 ,8, 56 31 of 33\nTable A10. A synoptic table of space, time and accuracy of the models on the wiki dataset . The\nlegend is as in Table A9.\nL1\nTime Space Reduction Factor\nBest RMI 2.55\u000210\u000083.09 99.84\nB-PGM 0.05 2.50 1.30\u000210\u000022.06\u000210\u00001\nSY-RMI 0.05 2.24 2.85\u000210\u000028.52\u000210\u00001\nBest RMI 1.00 1.00 1.00\nBest RS 1.70 2.40 9.77\u000210\u00001\nL2\nTime Space Reduction Factor\nBest RMI 3.32\u000210\u000089.83\u000210199.98\nB-PGM 0.05 2.66 1.02\u000210\u000039.26\u000210\u00001\nSY-RMI 0.05 2.59 1.63\u000210\u000039.57\u000210\u00001\nBest RS 1.60 7.77\u000210\u000029.97\u000210\u00001\nRMI<10 1.05 2.50\u000210\u000011.00\nL3\nTime Space Reduction Factor\nBest RMI 5.16\u000210\u000087.86\u0002102100.00\nB-PGM 0.05 2.26 3.76\u000210\u000031.00\nSY-RMI 0.05 2.01 3.83\u000210\u000031.00\nBest RS 1.74 3.82\u000210\u000021.00\nRMI<10 1.14 5.00\u000210\u000011.00\nL4\nTime Space Reduction Factor\nSY-RMI 2 1.61\u000210\u000073.20\u0002104100.00\nSY-RMI 0.05 1.39 2.50\u000210\u000021.00\nB-PGM 0.05 1.82 2.53\u000210\u000021.00\nBest RS 1.30 4.97\u000210\u000011.00\nRMI<10 1.02 7.86\u000210\u000011.00\nReferences\n1. Cormen, T.H.; Leiserson, C.E.; Rivest, R.L.; Stein, C. Introduction to Algorithms , 3rd ed.; The MIT Press: Cambridge, MA, USA, 2009.\n2. Kraska, T.; Beutel, A.; Chi, E.H.; Dean, J.; Polyzotis, N. The case for learned index structures. In Proceedings of the 2018\nInternational Conference on Management of Data, Houston, TX, USA, 10–15 June 2018; pp. 489–504.\n3. Knuth, D.E. The Art of Computer Programming, Volume 3 (Sorting and Searching) ; Addison-Wesley:Boston, MA, USA, 1973; Volume 3,\npp. 481–489.\n4. Aho, A.V .; Hopcroft, J.E.; Ullman, J.D. The Design and Analysis of Computer Algorithms ; Addison Wesley: Boston, MA, USA, 1974.\n5. Comer, D. Ubiquitous B-Tree. ACM Comput. Surv. CSUR 1979 ,11, 121–137. [CrossRef]\n6. Amato, D.; Lo Bosco, G.; Giancarlo, R. Learned Sorted Table Search and Static Indexes in Small Model Space. In Proceedings\nof the AIxIA 2021—Advances in Artiﬁcial Intelligence: 20th International Conference of the Italian Association for Artiﬁcial\nIntelligence, Virtual, 1–3 December 2021; Revised Selected Papers; Springer: Berlin/Heidelberg, Germany, 2021; pp. 462–477.\n[CrossRef]\n7. Ferragina, P .; Vinciguerra, G. Learned data structures. In Recent Trends in Learning from Data ; Springer International Publishing:\nBerlin/Heidelberg, Germany, 2020; pp. 5–41. [CrossRef]\n8. Mitzenmacher, M.; Vassilvitskii, S. Algorithms with Predictions. Commun. ACM 2022 ,65, 33–35. [CrossRef]\n\nData 2023 ,8, 56 32 of 33\n9. Marcus, R.; Kipf, A.; van Renen, A.; Stoian, M.; Misra, S.; Kemper, A.; Neumann, T.; Kraska, T. Benchmarking Learned Indexes.\nProc. VLDB Endow. 2020 ,14, 1–13. [CrossRef]\n10. Amato, D.; Lo Bosco, G.; Giancarlo, R. On the Suitability of Neural Networks as Building Blocks for the Design of Efﬁcient\nLearned Indexes. In Proceedings of the Engineering Applications of Neural Networks, Crete, Greece, 17–20 June 2022; Iliadis, L.,\nJayne, C., Tefas, A., Pimenidis, E., Eds.; Springer International Publishing: Cham, Switzerland, 2022; pp. 115–127.\n11. Maltry, M.; Dittrich, J. A critical analysis of recursive model indexes. Proc.VLDB Endow. 2022 ,15, 1079–1091. [CrossRef]\n12. Marcus, R.; Zhang, E.; Kraska, T. CDFShop: Exploring and optimizing learned index structures. In Proceedings of the 2020 ACM\nSIGMOD International Conference on Management of Data, SIGMOD’20, Portland, OR, USA, 14–19 June 2020; pp. 2789–2792.\n13. Ferragina, P .; Vinciguerra, G. The PGM-index: A fully-dynamic compressed learned index with provable worst-case bounds.\nPVLDB 2020 ,13, 1162–1175. [CrossRef]\n14. Chen, D.Z.; Wang, H. Approximating Points by a Piecewise Linear Function. Algorithmica 2012 ,66, 682–713. [CrossRef]\n15. Galakatos, A.; Markovitch, M.; Binnig, C.; Fonseca, R.; Kraska, T. FITing-Tree: A data-aware index structure. In Proceedings of\nthe 2019 International Conference on Management of Data, Amsterdam, The Netherlands, 30 June–5 July 2019; Association for\nComputing Machinery: New York, NY, USA, 2019; SIGMOD ‘19, pp. 1189–1206. [CrossRef]\n16. Kipf, A.; Marcus, R.; van Renen, A.; Stoian, M.; Kemper, A.; Kraska, T.; Neumann, T. SOSD: A benchmark for learned indexes. In\nProceedings of the ML for Systems at NeurIPS, MLForSystems @ NeurIPS’19, Vancouver, BC, USA, 12–14 December 2019.\n17. Kipf, A.; Marcus, R.; van Renen, A.; Stoian, M.; Kemper, A.; Kraska, T.; Neumann, T. RadixSpline: A single-pass learned index. In\nProceedings of the Third International Workshop on Exploiting Artiﬁcial Intelligence Techniques for Data Management, Portland,\nOR, USA, 14–20 June 2020; Association for Computing Machinery: New York, NY, USA, 2020; pp. 1–5.\n18. Neumann, T.; Michel, S. Smooth Interpolating Histograms with Error Guarantees. In Proceedings of the Sharing Data, Information\nand Knowledge, Cardiff, UK, 7–10 July 2008. [CrossRef]\n19. Ding, J.; Minhas, U.F.; Yu, J.; Wang, C.; Do, J.; Li, Y.; Zhang, H.; Chandramouli, B.; Gehrke, J.; Kossmann, D.; et al. ALEX: An\nUpdatable Adaptive Learned Index. In Proceedings of the 2020 ACM SIGMOD International Conference on Management of\nData, Portland, OR, USA, 14–19 June 2020; Association for Computing Machinery: New York, NY, USA, 2020; SIGMOD’20,\npp. 969–984. [CrossRef]\n20. Amato, D.; Lo Bosco, G.; Giancarlo, R. Standard versus uniform binary search and their variants in learned static indexing: The\ncase of the searching on sorted data benchmarking software platform. Softw. Pract. Exp. 2023 ,53, 318–346.\n21. Kipf, A.; Marcus, R.; van Renen, A.; Stoian, M.; Kemper A.; Kraska, T.; Neumann, T. SOSD Leaderboard. Available online:\nhttps://learnedsystems.github.io/SOSDLeaderboard/leaderboard/ (accessed on 5 June 2022).\n22. Rao, J.; Ross, K.A. Cache conscious indexing for decision-support in main memory. In Proceedings of the 25th International\nConference on Very Large Data, Edinburgh, Scotland, UK, 7–10 September 1999; pp 78–89. Morgan Kaufmann Publishers Inc.\n23. Khuong, P .; Morin, P . Array layouts for comparison-based searching. J. Exp. Algorithmics 2017 ,22, 1.3:1–1.3:39. [CrossRef]\n24. Wang, W.; Zhang, M.; Chen, G.; Jagadish, H.V .; Ooi, B.C.; Tan, K. Database Meets Deep Learning: Challenges and Opportunities.\nSIGMOD Rec. 2016 ,45, 17–22. [CrossRef]\n25. Kraska, T.; Alizadeh, M.; Beutel, A.; Chi, E.H.; Ding, J.; Kristo, A.; Leclerc, G.; Madden, S.; Mao, H.; Nathan, V . Sagedb: A Learned\nDatabase System. In Proceedings of the CIDR 2019-9th Biennial Conference on Innovative Data Systems Research, Asilomar, CA,\nUSA, 13–16 January 2019.\n26. Li, P .; Lu, H.; Zheng, Q.; Yang, L.; Pan, G. LISA: A Learned Index Structure for Spatial Data. In Proceedings of the 2020 ACM\nSIGMOD International Conference on Management of Data, Portland, OR, USA, 14–19 June 2020; Association for Computing\nMachinery: New York, NY, USA, 2020; SIGMOD’20, pp. 2119–2133. [CrossRef]\n27. Wang, H.; Fu, X.; Xu, J.; Lu, H. Learned Index for Spatial Queries. In Proceedings of the 2019 20th IEEE International Conference\non Mobile Data Management (MDM), Hong Kong, China, 10–13 June 2019; pp. 569–574. [CrossRef]\n28. Ol’ha, J.; Slanináková, T.; Gendiar, M.; Antol, M.; Dohnal, V . Learned Indexing in Proteins: Substituting Complex Distance\nCalculations with Embedding and Clustering Techniques. arXiv 2022 , arXiv:2208.08910.\n29. Marcus, R.; Negi, P .; Mao, H.; Zhang, C.; Alizadeh, M.; Kraska, T.; Papaemmanouil, O.; Tatbul, N. Neo: A Learned Query\nOptimizer. Proc. VLDB Endow. 2019 ,12, 1705–1718. [CrossRef]\n30. Zhang, M.; Wang, H. LAQP: Learning-based approximate query processing. Inf. Sci. 2021 ,546, 1113–1134. [CrossRef]\n31. Marcus, R.; Negi, P .; Mao, H.; Tatbul, N.; Alizadeh, M.; Kraska, T. Bao: Making Learned Query Optimization Practical. SIGMOD\nRec. 2022 ,51, 6–13. [CrossRef]\n32. Mikhaylov, A.; Mazyavkina, N.S.; Salnikov, M.; Troﬁmov, I.; Qiang, F.; Burnaev, E. Learned Query Optimizers: Evaluation and\nImprovement. IEEE Access 2022 ,10, 75205–75218. [CrossRef]\n33. Bloom, B.H. Space/Time Trade-Offs in Hash Coding with Allowable Errors. Commun. ACM 1970 ,13, 422–426. [CrossRef]\n34. Mitzenmacher, M. A model for learned bloom ﬁlters and optimizing by sandwiching. In Proceedings of the Advances in Neural\nInformation Processing Systems, Montreal, QC, Canada, 2–8 December 2018; Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,\nCesa-Bianchi, N., Garnett, R., Eds.; Curran Associates, Inc.: Red Hook, NY, USA, 2018; Volume 31.\n35. Vaidya, K.; Knorr, E.; Kraska, T.; Mitzenmacher, M. Partitioned Learned Bloom Filter. arXiv 2020 ,arXiv:2006.03176 .\n36. Dai, Z.; Shrivastava, A. Adaptive learned bloom ﬁlter (ada-bf): Efﬁcient utilization of the classiﬁer with application to real-time\ninformation ﬁltering on the web. Adv. Neural Inf. Process. Syst. 2020 ,33, 11700–11710.\n\nData 2023 ,8, 56 33 of 33\n37. Fumagalli, G.; Raimondi, D.; Giancarlo, R.; Malchiodi, D.; Frasca, M. On the Choice of General Purpose Classiﬁers in Learned\nBloom Filters: An Initial Analysis within Basic Filters. In Proceedings of the 11th International Conference on Pattern Recognition\nApplications and Methods (ICPRAM), Online, 3–5 February 2022; pp. 675–682.\n38. Singh, A.; Gupta, S. Learning to hash: A comprehensive survey of deep learning-based hashing methods. Knowl. Inf. Syst. 2022 ,\n64, 2565–2597. [CrossRef]\n39. Lin, H.; Luo, T.; Woodruff, D. Learning Augmented Binary Search Trees. In Proceedings of the 39th International Conference on\nMachine Learning, Baltimore, MD, USA, 17–23 July 2022; Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., Sabato, S.,\nEds.; Volume 162, pp. 13431–13440.\n40. Boffa, A.; Ferragina, P .; Vinciguerra, G. A “learned” approach to quicken and compress rank/select dictionaries. In Proceedings\nof the SIAM Symposium on Algorithm Engineering and Experiments (ALENEX), Alexandria, VA, USA, 10–11 January 2021.\n41. Kirsche, M.; Das, A.; Schatz, M.C. Sapling: Accelerating sufﬁx array queries with learned data models. Bioinformatics 2020 ,\n37, 744–749.\n42. Boffa, A.; Ferragina, P .; Tosoni, F.; Vinciguerra, G. Compressed string dictionaries via data-aware subtrie compaction. In\nProceedings of the 29th International Symposium on String Processing and Information Retrieval (SPIRE), Concepcion, Chile,\n8–10 November 2022; pp. 233–249. [CrossRef]\n43. Kristo, A.; Vaidya, K.; Çetintemel, U.; Misra, S.; Kraska, T. The Case for a Learned Sorting Algorithm. In Proceedings of the\n2020 ACM SIGMOD International Conference on Management of Data, Portland, OR, USA, 14–19 June 2020; Association for\nComputing Machinery: New York, NY, USA, 2020; SIGMOD’20, pp. 1001–1016. [CrossRef]\n44. Bishop, C.M. Neural Networks for Pattern Recognition ; Oxford University Press, Inc.: New York, NY, USA, 1995.\n45. Goodfellow, I.; Bengio, Y.; Courville, A. Deep Learning ; The MIT Press: Cambridge, MA, USA, 2016.\n46. Kraska, T. Towards Instance-Optimized Data Systems. Proc. VLDB Endow. 2021 ,14, 3222–3232. [CrossRef]\n47. Abadi, D.; Ailamaki, A.; Andersen, D.; Bailis, P .; Balazinska, M.; Bernstein, P .A.; Boncz, P .; Chaudhuri, S.; Cheung, A.; Doan, A.;\net al. The Seattle Report on Database Research. Commun. ACM 2022 ,65, 72–79. [CrossRef]\n48. Available online: https://github.com/globosco/A-learned-sorted-table-search-library (accessed on 5 June 2022).\n49. Peterson, W.W. Addressing for random-access storage. IBM J. Res. Dev. 1957 ,1, 130–146. [CrossRef]\n50. Van Sandt, P .; Chronis, Y.; Patel, J.M. Efﬁciently searching in-memory sorted arrays: Revenge of the interpolation search? In\nProceedings of the 2019 International Conference on Management of Data, Amsterdam, The Netherlands, 30 June–5 July 2019;\nACM: New York, NY, USA, 2019; SIGMOD’19, pp. 36–53.\n51. Amato, D. A Tour of Learned Static Sorted Sets Dictionaries: From Speciﬁc to Generic with an Experimental Performance\nAnalysis. Ph.D. Thesis, University of Palermo, Palermo, Sicily, Italy, 2022.\n52. Freedman, D. Statistical Models: Theory and Practice ; Cambridge University Press: Cambridge, UK, 2005.\n53. Schlegel, B.; Gemulla, R.; Lehner, W. K-Ary Search on Modern Processors. In Proceedings of the Fifth International Workshop on\nData Management on New Hardware, Providence, RI, USA, 28 June 2009; Association for Computing Machinery: New York, NY,\nUSA, 2009; DaMoN’09, pp. 52–60. [CrossRef]\n54. Schulz, L.; Broneske, D.; Saake, G. An eight-dimensional systematic evaluation of optimized search algorithms on modern\nprocessors. Proc. VLDB Endow. 2018 ,11, 1550–1562. [CrossRef]\n55. Kipf, A. (MIT Data Systems Group, Massachusetts Institute of Technology , Cambridge, MA, USA). Personal communication, 2021.\n56. Available online: https://osf.io/qtyu7/?view_only=b48e6cc6e01b441383b26b81588090ec (accessed on 5 June 2022).\n57. Smirnov, N.V . Estimate of deviation between empirical distribution functions in two independent samples. Bull. Mosc. Univ.\n1939 ,2, 3–16.\n58. Kullback, S. Information Theory and Statistics ; Dover Publications: New York, NY, USA, 1968.\nDisclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual\nauthor(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to\npeople or property resulting from any ideas, methods, instructions or products referred to in the content.",
  "textLength": 99908
}