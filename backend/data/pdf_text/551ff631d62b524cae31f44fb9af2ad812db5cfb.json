{
  "paperId": "551ff631d62b524cae31f44fb9af2ad812db5cfb",
  "title": "Stage: Query Execution Time Prediction in Amazon Redshift",
  "pdfPath": "551ff631d62b524cae31f44fb9af2ad812db5cfb.pdf",
  "text": "MIT Open Access Articles\nStage: Query Execution Time Prediction in Amazon Redshift\nThe MIT Faculty has made this article openly available. Please share\nhow this access benefits you. Your story matters.\nCitation: Wu, Ziniu, Marcus, Ryan, Liu, Zhengchun, Negi, Parimarjan, Nathan, Vikram et al. 2024. \n\"Stage: Query Execution Time Prediction in Amazon Redshift.\"\nAs Published: 10.1145/3626246.3653391\nPublisher: ACM|Companion of the 2024 International Conference on Management of Data\nPersistent URL: https://hdl.handle.net/1721.1/155774\nVersion: Final published version: final published article, as it appeared in a journal, conference \nproceedings, or other formally published context\nTerms of use: Creative Commons Attribution\n\n\nStage: Query Execution Time Prediction in Amazon Redshift\nZiniu Wu‚Ä†\nziniuw@mit.edu\nMassachusetts Institute of Technology\nCambridge, USARyan Marcus‚Ä†\nrcmarcus@seas.upenn.edu\nUniversity of Pennsylvania\nPhiladelphia, USAZhengchun Liu\nzcl@amazon.com\nAmazon Web Services\nSanta Clara, USA\nParimarjan Negi‚Ä†\npnegi@mit.edu\nMassachusetts Institute of Technology\nCambridge, USAVikram Nathan\nvrnathan@amazon.com\nAmazon Web Services\nBoston, USAPascal Pfeil\npfeip@amazon.de\nAmazon Web Services\nMunich, Germany\nGaurav Saxena\ngssaxena@amazon.com\nAmazon Web Services\nEast Palo Alto, USAMohammad Rahman\nrerahman@amazon.com\nAmazon Web Services\nEast Palo Alto, USABalakrishnan Narayanaswamy\nmuralibn@amazon.com\nAmazon Web Services\nSanta Clara, USA\nTim Kraska‚Ä†\nkraska@mit.edu\nMassachusetts Institute of\nTechnology, Amazon Web Services\nCambridge, USA\nABSTRACT\nQuery performance (e.g., execution time) prediction is a critical com-\nponent of modern DBMSes. As a pioneering cloud data warehouse,\nAmazon Redshift relies on an accurate execution time prediction\nfor many downstream tasks, ranging from high-level optimizations,\nsuch as automatically creating materialized views, to low-level\ntasks on the critical path of query execution, such as admission,\nscheduling, and execution resource control. Unfortunately, many\nexisting execution time prediction techniques, including those used\nin Redshift, suffer from cold start issues, inaccurate estimation, and\nare not robust against workload/data changes.\nIn this paper, we propose a novel hierarchical execution time\npredictor: the Stage predictor. The Stage predictor is designed to\nleverage the unique characteristics and challenges faced by Redshift.\nThe Stage predictor consists of three model states: an execution time\ncache, a lightweight local model optimized for a specific DB instance\nwith uncertainty measurement, and a complex global model that is\ntransferable across all instances in Redshift. We design a systematic\napproach to use these models that best leverages optimality (cache),\ninstance-optimization (local model), and transferable knowledge\nabout Redshift (global model). Experimentally, we show that the\nStage predictor makes more accurate and robust predictions while\nmaintaining a practical inference latency and memory overhead.\nThis work is licensed under a Creative Commons Attribution\nInternational 4.0 License.\nSIGMOD-Companion ‚Äô24, June 9‚Äì15, 2024, Santiago, AA, Chile\n¬©2024 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0422-2/24/06.\nhttps://doi.org/10.1145/3626246.3653391Overall, the Stage predictor can improve the average query execu-\ntion latency by 20%on these instances compared to the prior query\nperformance predictor in Redshift.\nCCS CONCEPTS\n‚Ä¢Information systems ‚ÜíDatabase performance evaluation;\nRelational database model .\nKEYWORDS\nQuery performance prediction, AWS Redshift\nACM Reference Format:\nZiniu Wu, Ryan Marcus, Zhengchun Liu, Parimarjan Negi, Vikram Nathan,\nPascal Pfeil, Gaurav Saxena, Mohammad Rahman, Balakrishnan (Murali)\nNarayanaswamy, Tim Kraska . 2024. Stage: Query Execution Time Prediction\nin Amazon Redshift. In Companion of the 2024 International Conference on\nManagement of Data (SIGMOD-Companion ‚Äô24), June 9‚Äì15, 2024, Santiago,\nAA, Chile. ACM, New York, NY, USA, 15 pages. https://doi.org/10.1145/\n3626246.3653391\n1 INTRODUCTION\nPredicting the execution time (exec-time) of a query before actually\nexecuting the query is a crucial component for a number of tasks\nin intelligent cloud DBMSes, such as query optimization [ 21,52],\nworkload scheduling [ 46,57], admission control [ 55], resource man-\nagement [7, 29, 53], and maintaining SLAs [9, 34].\nAmazon Redshift, a pioneering cloud data warehouse, relies on\nexec-time prediction for many downstream tasks, ranging from\nhigh-level optimizations (e.g., automatically creating materialized\nviews [ 5]) to low-level tasks on the critical path of query execution\n‚Ä†Work conducted while these authors are affiliated with Amazon Web Services.\n280\n\nSIGMOD-Companion ‚Äô24, June 9‚Äì15, 2024, Santiago, AA, Chile Ziniu Wu et al.\n\b\u0007\b\b\u0007\n\b\u0007\f\b\u0007\u000e\b\u0007\u000f\t\u0007\b\u0004\u0003!%\u0017\"\u001b\u0017#\u0003%\u001e\u001b!%\u0017\u0003'\u001b$\u001a\u001b\u001e\u0003\u0014\u0003\u0016\u0014(\b\u0007\b\b\u0007\n\b\u0007\f\b\u0007\u000e\b\u0007\u000f\t\u0007\b\u0012\u0017\"\u0015\u0017\u001e$\u001b\u001d\u0017\u0003\u0005\u0015\u001d%#$\u0017\"\u0006\f\b\u0004\u0003\u001f\u0018\u0003\u0013\u0017\u0016#\u001a\u001b\u0018$\u0003\u0015\u001d%#$\u0017\"#\u0003\u001a\u0014&\u0017\u0003'\u001f\"\u001c\u001d\u001f\u0014\u0016#\u0003\u0015\u001f\u001e#\u001b#$\u001b\u001e\u0019\u0003\u001f\u0018\u0003\u0010\u0003\r\b\u0004\u0003%\u001e\u001b!%\u0017\u0003\u0016\u0014\u001b\u001d(\u0003!%\u0017\"\u001b\u0017#\n\u0011\u001e\u001d(\u0003\t\u000b\u0004\u0003\u001f\u0018\u0003\u0013\u0017\u0016#\u001a\u001b\u0018$\u0003\u0015\u001d%#$\u0017\"#\u0003\u001a\u0014&\u0017\u0003\u001e\u001f\u0003\"\u0017 \u0017\u0014$\u001b\u001e\u0019\u0003!%\u0017\"\u001b\u0017#\u0003\u0005'\u001b$\u001a\u001b\u001e\u0003\u0014\u0003\u0016\u0014(\u0006\t\b\t\t\b\u000b\t\b\r\t\b\u000f\u0011\u0014\"\u0017\u001d\u0015%\u0003\u0005\u001c!\u0006\b\u0007\b\b\u0007\n\b\u0007\f\b\u0007\u000e\b\u0007\u0010\t\u0007\b\u0012\u0017 \u0015\u0017\u001d\"\u001a\u001b\u0017\u0003\u0005\u001f#\u0017 %\u0006\u000e\b\u0004\u0003\u001e\u0018\u0003\u0013\u0017\u0016!\u0019\u001a\u0018\"\u0003\u001f#\u0017 \u001a\u0017!\u0003\u0017$\u0017\u0015#\"\u0017\u0003\u001a\u001d\u0003#\u001d\u0016\u0017 \u0003\t\b\b\u001c!No. of clusters (%)02040608010040% of Redshift clusters have > 50% unique daily queries  Only 13% of clusters have no repeating queries020406080100No. of queries (%)020406080100No. of unique queries within a day (%)101103105107Query latency (ms)40% of Redshift queriesexecute in under 100ms \n(a) Distribution of clusters by the\n% of queries that were unique\nwithin a day (not repeated).\n\b\u0007\b\b\u0007\n\b\u0007\f\b\u0007\u000e\b\u0007\u000f\t\u0007\b\u0004\u0003!%\u0017\"\u001b\u0017#\u0003%\u001e\u001b!%\u0017\u0003'\u001b$\u001a\u001b\u001e\u0003\u0014\u0003\u0016\u0014(\b\u0007\b\b\u0007\n\b\u0007\f\b\u0007\u000e\b\u0007\u000f\t\u0007\b\u0012\u0017\"\u0015\u0017\u001e$\u001b\u001d\u0017\u0003\u0005\u0015\u001d%#$\u0017\"\u0006\f\b\u0004\u0003\u001f\u0018\u0003\u0013\u0017\u0016#\u001a\u001b\u0018$\u0003\u0015\u001d%#$\u0017\"#\u0003\u001a\u0014&\u0017\u0003'\u001f\"\u001c\u001d\u001f\u0014\u0016#\u0003\u0015\u001f\u001e#\u001b#$\u001b\u001e\u0019\u0003\u001f\u0018\u0003\u0010\u0003\r\b\u0004\u0003%\u001e\u001b!%\u0017\u0003\u0016\u0014\u001b\u001d(\u0003!%\u0017\"\u001b\u0017#\n\u0011\u001e\u001d(\u0003\t\u000b\u0004\u0003\u001f\u0018\u0003\u0013\u0017\u0016#\u001a\u001b\u0018$\u0003\u0015\u001d%#$\u0017\"#\u0003\u001a\u0014&\u0017\u0003\u001e\u001f\u0003\"\u0017 \u0017\u0014$\u001b\u001e\u0019\u0003!%\u0017\"\u001b\u0017#\u0003\u0005'\u001b$\u001a\u001b\u001e\u0003\u0014\u0003\u0016\u0014(\u0006\t\b\t\t\b\u000b\t\b\r\t\b\u000f\u0011\u0014\"\u0017\u001d\u0015%\u0003\u0005\u001c!\u0006\b\u0007\b\b\u0007\n\b\u0007\f\b\u0007\u000e\b\u0007\u0010\t\u0007\b\u0012\u0017 \u0015\u0017\u001d\"\u001a\u001b\u0017\u0003\u0005\u001f#\u0017 %\u0006\u000e\b\u0004\u0003\u001e\u0018\u0003\u0013\u0017\u0016!\u0019\u001a\u0018\"\u0003\u001f#\u0017 \u001a\u0017!\u0003\u0017$\u0017\u0015#\"\u0017\u0003\u001a\u001d\u0003#\u001d\u0016\u0017 \u0003\t\b\b\u001c!No. of clusters (%)02040608010040% of Redshift clusters have > 50% unique daily queries  Only 13% of clusters have no repeating queries020406080100No. of queries (%)020406080100No. of unique queries within a day (%)101103105107Query latency (ms)40% of Redshift queriesexecute in under 100ms (b) Distribution of query latency\nacross the Redshift fleet (0.01% to\n99.99% shown).\nFigure 1: Distribution statistics from the Redshift fleet\n(e.g. admission, scheduling and execution resource control inside\nits workload manager [ 50]). For example, the workload manager in\nRedshift separates queries into ‚Äúshort-running‚Äù and ‚Äúlong-running‚Äù\nqueues based on estimated exec-time. The short-running query\nqueue has its own dedicated resources and unique optimizations to\nmeet users‚Äô expectations of fast execution. If a long-running query\nis erroneously placed into the short-running queue by the exec-time\npredictor, the long-running query can cause head-of-line blocking,\nsignificantly delaying the execution of short-running queries in\nthe queue. Conversely, if a short query is wrongly placed into the\nlong-running queue, it may queue up for minutes before execution.\nBoth cases can severely affect the overall query performance on a\ncluster and the user experience of Redshift.\nThe prior exec-time predictor inside Amazon Redshift (AutoWLM\npredictor [ 50]) uses an instance-optimized XGBoost model [ 8]\ntrained on each customer‚Äôs database cluster, using each cluster‚Äôs\nexecuted queries. This model is very lightweight to ensure negligi-\nble inference latency and memory overhead on the critical path of\nquery execution. However, the AutoWLM predictor has the follow-\ning downsides. First, due to its lightweight nature and simplified\nquery featurization techniques, it can produce inaccurate estima-\ntions. Second, whenever the customers‚Äô data or query workload\nchanges, it can provide unreliable predictions until the predictor‚Äôs\ntraining set ‚Äúcatches up‚Äù with the change. Third, the AutoWLM pre-\ndictor requires a sufficient amount of executed queries as training\nexamples, which may not be available for a new instance, and thus,\nit performs poorly in cold start scenarios.\nWe make two key observations about the Amazon Redshift fleet\nthat motivate the design of our new exec-time predictor. First, most\nqueries executed on Amazon Redshift are low latency queries. Many\nqueries execute in just a few milliseconds (as shown in Figure 1b).\nTherefore, naively applying the advanced exec-time predictors in\nrecent literature [ 21,35,52], which have inference time on the\norder of 50ms to 500ms, on the critical path will result in more\ntime being spent on prediction than on actual query execution.\nThus, despite the superior estimation accuracy of these modern\ntechniques, their inference latency overhead is not affordable for\nmany Redshift queries. Second, Amazon Redshift customers tend\nto issue repeating queries. On average, more than 60%of the queries\nexecuted within Amazon Redshift have been executed within 24\nhours of the execution of an identical query (as shown in Figure 1a).1\n1These queries are exactly repeated, both in terms of SQL and parameter values, but\nthe data in the database may have changed in the meantime.\n123Exec-time cache (memorized)Local model (instance-optimized)Global model (transferrable)Stage predictor hierarchiesHave we seen‚Ä¶this exact query before?a similar query on this instance?anything like this on all instances?Figure 2: The key components and ideas of Stage predictor.\nTo address the challenges of cold-start prediction, inference time,\nand reliable estimation, we implemented a novel hierarchical exec-\ntime predictor (Stage ) with three stages of models illustrated in\nFigure 2: (1) a local exec-time cache, which simply memorizes the\nlatency of recently executed queries and predicts that latency when\nthe exact query is submitted again, (2) a local lightweight exec-time\npredictor with uncertainty measurement that is instance-optimized\nto each Redshift customer, and (3) a complex global predictor that is\ntransferable across all instances in Redshift. When a new query ùëÑ\narrives, Stage predictor will first look up ùëÑinexec-time cache and di-\nrectly return its prediction based on previously observed exec-time\nifùëÑis present in the cache. If a query ùëÑmisses the cache, Stage pre-\ndictor will use a lightweight local exec-time predictor (local model )\nto predict its exec-time and an uncertainty measurement of the\nprediction. The local model utilizes a Bayesian ensemble of light-\nweight XGBoost models [ 31] that can provide a query exec-time\nprediction and a reliable uncertainty measure associated with the\nprediction with a very low inference latency. While the local model\nnever learns a fully generalizable model of query performance, it\ncan accurately predict queries similar to the past-seen queries. Thus,\nit can be thought of as a ‚Äúfuzzy cache‚Äù. The prediction uncertainty\ncan be high whenever the local model does not have enough train-\ning examples or the input query is very different from previously\nseen queries. In this situation, Stage predictor will use the global\nmodel. Inspired by the recent advance in zero-shot cost model [ 21],\nwe design our global model as a graph neural network [ 51,61] that\ntakes a physical execution plan of a query as input to predict its\nexec-time. There exist tens of thousands of instances in Redshift\nwith diversified workloads. Thus, we train a single global model\non a diverse set of instances to distill the transferable knowledge\nof exec-time prediction across various instances. As a result, it\nis capable of accurately and robustly predicting the exec-time of\nqueries on unseen clusters. The global model will have a non-trivial\ninference latency (up to 100ms). Therefore, when used on a critical\npath of query execution, it will only be used when the local model\nis uncertain about its prediction and believes the query‚Äôs exec-time\nto be longer than a couple of seconds. Because the global model is\nrarely used, the additional inference overhead is amortized out.\nWe simulate Stage predictor inside the workload manager [ 50]\nof Amazon Redshift in an actual production environment. We con-\nduct end-to-end evaluations on the 100 most billed instances in the\nmonth of July 2023 for each of three regions: ‚Äòus-east-1‚Äô, ‚Äòus-west-2‚Äô,\nand ‚Äòeu-west-1‚Äô. The results show that Stage predictor can improve\nthe average query execution latency by 20%on these instances com-\npared to the prior exec-time predictor in Redshift. In addition, we\nconduct thorough ablation studies to demonstrate the performance\nand reliability of each component of Stage predictor.\nTo the best of our knowledge, Stage predictor is the first to apply\nthe idea of a hierarchy of models in exec-time prediction or similar\ntasks in DBMS. We believe that Stage predictor points out a way\n281\n\nStage: Query Execution Time Prediction in Amazon Redshift SIGMOD-Companion ‚Äô24, June 9‚Äì15, 2024, Santiago, AA, Chile\nto practically integrate expensive machine learning models on the\ncritical path of customer-facing production systems. We list the\nmain contributions of this paper as follows:\n‚Ä¢We describe the use cases and unique challenges of the exec-\ntime predictor in Redshift (Section 2 and Section 3).\n‚Ä¢We design a Stage predictor framework with hierarchical\ncomponents for exec-time prediction (Section 4).\n‚Ä¢We show a comprehensive evaluation of production data to\nshowcase the advantages of Stage predictor (Section 5).\n‚Ä¢We summarize the lessons learned and point out important\nresearch questions (Section 6).\n2 BACKGROUND\nIn this section, we first give an overview of query processing in\nAmazon Redshift, and then we provide a brief survey of related\nworks on query exec-time prediction.\n2.1 Execution time predictor in Redshift\nFigure 3 illustrates the lifetime of user-issued queries inside Red-\nshift. The queries will first go through a parser and query optimizer\nto derive their physical execution plans. Then, the exec-time predic-\ntor will take these plans as input and predict their exec-time. Based\non their predictions, the workload manager will make a series of\nchoices to determine their execution strategy and resource alloca-\ntions (see [ 50] for an overview). Finally, the workload manager will\nsend the queries out for actual execution.\nImportance of exec-time predictor. The predicted exec-time is a\ncritical component of the workload manager‚Äôs decision-making\nchoices. Based on the prediction, the admission controller in the\nworkload manager will decide whether a query should wait in\nthe queue, be pushed to a special short query queue, be executed\non the user‚Äôs main cluster, or be sent to a concurrency scaling\ncluster. For queries waiting in a queue, each query‚Äôs priority is\ndetermined by the predicted exec-time (short queries execute first).\nIf the workload manager decides to start up a new concurrency\nscaling cluster to process an incoming query, the optimal cluster size\nwill be chosen based on the predicted exec-time on the candidate\ncluster sizes. Therefore, the accuracy of exec-time predictor directly\naffects query performance in Redshift. For example, if exec-time\npredictor is inaccurate, it can mistake a long-running query as short-\nrunning. This long-running query can block the execution of other\nshort-running queries in the queue, thus severely degrading the\noverall query latency. Conversely, if a shorting-running is mistaken\nas long-running, it may queue up for minutes before execution.\nIn addition to the usage of exec-time predictor on the critical\npath of query execution, it is also used in several other high-level\noptimization tasks. For example, automatic materialized view cre-\nation in Redshift [ 5] uses the query optimizer to regenerate queries‚Äô\nexecution plans as if certain materialized view exists and then uses\nthe exec-time predictor to estimate the performance of these plans\nto determine the benefits of building such materialized view.\nThe prior AutoWLM exec-time predictor in Redshift. Here, we\nsummarize the current status of the exec-time predictor in Redshift\nas described in prior work [ 50], the AutoWLM predictor. First, the\nAutoWLM predictor takes a physical execution plan of a query as\nLeader NodeParserQueryOptimizerUser Query\nExecutionCompute NodesWorkloadManagerPriorityAssignerAdmissionControllerResourceManager\nüîçDB Stats\nExec-timePredictor\nü§ñ Figure 3: Queries‚Äô lifetime inside Redshift.\ninput and flattens it into a vector. Then, a lightweight XGBoost\nmodel [ 8] is used to predict the query‚Äôs exec-time. As queries are\nexecuted in each instance, their feature vector and observed exec-\ntime are added to the XGBoost model‚Äôs training set.\nThe AutoWLM predictor is lightweight to ensure negligible in-\nference latency and memory overhead on the critical path of query\nexecution. However, due to its lightweight nature and simplified\nquery functionalization techniques, it can sometimes produce in-\naccurate estimations. Worse yet, some customers‚Äô data and query\nworkload change quickly, thus making the predictions unreliable.\nIn addition, the AutoWLM predictor requires sufficient executed\nqueries as training examples, which may not be available for a\nnew instance (and hence a cold-start problem). Moreover, many\ndownstream tasks require not just an estimate of exec-time but also\na confidence interval around that estimate for robust optimizations.\nFor example, the automatic materialized view creation and cluster\nscaling model in Redshift need a confidence interval to ensure good\nworst-case behavior of the changes in the cluster. The AutoWLM\npredictor provides these confidence intervals using simple global\nstatistics, which leaves room for improvement.\n2.2 Related Works\nHere, we give a brief overview of previous results in the areas of\nuncertainty quantification and query performance prediction.\nUncertainty of XGBoost models. XGBoost is a scalable approach\nfor building gradient boosting tree models, which achieved state-of-\nart performance in a wide range of tasks [ 8,45,54,67]. Recent work\nproposed a Bayesian ensemble of gradient boosting tree models to\nestimate the uncertainty of model prediction [ 31]. We abuse the\nterm XGBoost models to refer to gradient-boosting tree models\nfor easier understanding. In a nutshell, this approach separates the\nuncertainty into model and data uncertainty. It trains the XGBoost\nmodels with a probabilistic likelihood loss function [ 48]. Thereafter,\ninstead of predicting a single name, the XGBoost models will output\na meanùúáand variance ùúéfor its prediction, where ùúáis the model\nprediction and ùúécaptures the data uncertainty. The Bayesian en-\nsemble of XGBoost models independently trains several XGBoost\n282\n\nSIGMOD-Companion ‚Äô24, June 9‚Äì15, 2024, Santiago, AA, Chile Ziniu Wu et al.\nmodels, denoted as ùëÄ1,...,ùëÄùëò, each of which will produce a ùúáùëñand\nùúéùëñ. The variance of all output means ùúá1,...,ùúáùëòcaptures the model\nuncertainty. Finally, the total uncertainty of prediction is the sum\nof model uncertainty and data uncertainty. Stage predictor adapts\nthis approach to build an instance-optimized local predictor. The\ndetails will be discussed in Section 4.3.\nInstance-optimized exec-time predictor. Traditional exec-time pre-\ndictors generally use hand-derived heuristics and statistical models\nto understand the relational operators [ 1,15,27,62]. There has\nbeen a line of work using machine learning models to predict the\nexec-time of a query with superior accuracy over their traditional\ncounterparts. In general, they featurize the logical or physical query\nplan as a graph and train graph neural networks to map the query\nplan to its exec-time [ 32,33,35,52,64,68]. One common drawback\nof these approaches is their high inference latency, preventing Red-\nshift from integrating them on the critical path of query execution.\nMany Redshift queries execute in just a few milliseconds, and the\ninference latency of these methods surpasses a large proportion of\nqueries‚Äô actual execution time.\nZero-shot exec-time predictor. In contrast to instance-optimization,\nzero-shot exec-time predictor [ 21] proposed to train one model over\na diverse set of DB instances, and it can be directly used to predict\nthe query exec-time on arbitrary unseen DB instances. Specifically,\nthe zero-shot model gathers data-specific statistics from each DB\ninstance, such as the number of tuples/columns/pages of each table.\nThen, it embeds these statistics into a physical query execution\nplan to predict its exec-time. After a heavy offline training process,\nthe zero-shot model understands the data-independent knowledge\nabout the system and is transferrable to unseen DB instances. Stage\npredictor adapts this approach to build an instance-optimized global\npredictor inside Redshift. The details will be discussed in Section 4.4.\nMachine Learning for Databases. In addition to exec-time pre-\ndiction, machine learning has a large impact on optimizing and\nmanaging modern database systems. Machine learning systems help\nenable more effective and automated workload management [ 34,\n50,66], index recommendation [ 12], and configuration tuning [ 2].\nMachine learning algorithms help build more fine-grained and\ninstance-optimized sub-components embedded in existing DBM-\nSes, such cardinality estimation [ 22,42,63,65,69], learned query\noptimization [ 32,33,56,60,64], learned indexes [ 23,25,41], and\nlearned storage layouts [11, 13].\n3 DESIGN PRINCIPLES\nRedshift‚Äôs query predictor has several design constraints, some\nlikely to apply to other database engines, while others may be\nunique to Redshift. Here, we outline the three most important\ndesign principles behind our Stage exec-time predictor.\nFirst, like many OLAP databases, a large number of queries seen\nby Redshift are repeated queries (e.g., dashboard refreshes) ‚Äì tak-\ning advantage of this repetition is critical to correctly predicting\nlatency for the majority of queries across the Redshift fleet. Second,\nstandard point predictions (i.e., mean estimates) are insufficient\nfor the predictor‚Äôs downstream tasks; we require reasonable con-\nfidence bounds around each prediction to guarantee worst-case\nperformance. Third, the inference time of models on the criticalpath of query execution must be fast since a large portion of Red-\nshift queries execute in only a few milliseconds. Thus, we set out to\ndesign a predictor with inference time on the order of microseconds.\nRepeated queries. Many customers use Redshift for analytics\ntasks like dashboarding or report generation. As a result, identical\nqueries are often repeatedly issued to Redshift. Figure 1a shows the\ndistribution of the percentage of daily unique queries across the\nRedshift fleet: a ‚Äúdaily unique‚Äù query is a query sent to Redshift\nwithout an identical query being issued within the last 24 hours.\nDaily unique queries are thus a good lower bound for the number\nof repeating queries that Redshift sees, as monthly or weekly re-\nports will not appear as daily unique queries. We observe that past\nperformance of a query is a strong predictor of the same query‚Äôs\nperformance later that day, since data distributions are normally\nstatic day-by-day (distribution shifts do occur, but normally not\nwithin 24 hours). Therefore, we want to design our predictor to\ntake advantage of these repeating queries. This motivates the first\n‚Äúcaching‚Äù stage of our predictive model, discussed in Section 4.2.\nHigh-confidence predictions. Many off-the-shelf machine learn-\ning models (e.g., [ 47]) give predictions as point estimates, or ap-\nproximations of the mean. However, Redshift uses predictions in\na number of downstream tasks, including query scheduling and\ncluster sizing, so error bounds are essential for ensuring the entire\nsystem maintains good worst-case behavior. Error bounds are espe-\ncially important for deciding when to dedicate more inference time\nto make a more accurate prediction, which we discuss in Section 4.3.\nLow inference latency. Since Redshift needs to estimate the exec-\ntime for every issued query, it is important that the model‚Äôs in-\nference procedure is efficient. Figure 1b shows the distribution of\nquery latency across the Redshift fleet. Most Redshift queries exe-\ncute in under 100ùëöùë†. This rules out exclusively using some modern\nadvanced models, which could have inference times as high as\n100ùëöùë†[21,35,52] (higher than the total query latency for 40% of\nthe queries!). For Redshift, our new Stage predictor only uses an ex-\npensive neural network model when we have high confidence that a\nquery will be long (details in Section 4.4). In this case, the additional\ninference time is a trivial portion of the overall exec-time.\n4 STAGE PREDICTOR\nTo meet the specific needs of Redshift, we design the Stage exec-time\npredictor to work in stages. The first stage of the model, the exec-\ntime cache (Section 4.2), remembers the recently executed queries.\nWhen an incoming query matches a past query, the exec-time cache\nmakes a prediction. If a match is not found, the query proceeds\nto the next stage, the local model (Section 4.3). The local model is\ninstance-optimized to each user‚Äôs clusters (i.e., trained per cluster).\nWhile the local model never learns a fully generalizable model of\nquery performance, it can accurately predict queries that are slight\nmodifications of past-seen queries. Thus, it can be thought of as\na ‚Äúfuzzy cache‚Äù. When it cannot make a confident prediction, the\nquery proceeds to the final stage, the global model (Section 4.4). The\nglobal model is a state-of-the-art graph convolution neural network\ntrained across the Redshift fleet.\n283\n\nStage: Query Execution Time Prediction in Amazon Redshift SIGMOD-Companion ‚Äô24, June 9‚Äì15, 2024, Santiago, AA, Chile\nExec-time cacheIf seen \n‚úÖOutput: exec-time predictionIf not        \n‚ùåLocal XGBoostBayesian ensemble If short or  certain \n‚úÖ If not  \n‚ùåGlobal GCN exec-time predictorPhysicalPlanVector Rep.User query\nüîçQueryOptimizer\nü§ñ \nDB Stats\nFigure 4: Workflow overview of the Stage predictor.\n4.1 Overview\nWe present an overview of Stage predictor‚Äôs workflow in Figure 4.\nAs discussed in Section 2, a user‚Äôs query Qwill go through the parser\nand query optimizer to derive its physical execution plan before\narriving at the exec-time predictor. Stage predictor first flattens\nthe physical plan of Qinto a 33-dimensional vector. The hashed\nvector is checked against the exec-time cache, which records the\nobserved queries and their actual exec-time in the past. If Qis in\nthe cache, we return the prediction based on its observed exec-time\n(within the Redshift, we find that 60% of the queries will find a\nmatch in the cache). When Qmisses the cache, we send Q‚Äôs vector\nrepresentation to the local model that is trained on executed queries\non this instance, which will output a prediction and uncertainty\nassociated with it. If the local model thinks thatQis short-running,\nor the local model is highly confident about its prediction, Stage\npredictor directly returns the local model‚Äôs prediction.\nFinally, when the local model is uncertain aboutQ, the state-of-\nthe-art graph convolution neural network (global model ) is used.\nTheglobal model takes the physical plan of Qas input and under-\nstands the complicated interactions between operator nodes in this\nplan tree. A single global model is trained on all executed queries\nfrom a diverse set of instances and is shared across the entire Ama-\nzon Redshift fleet. As a result, the global model is more robust and\naccurate on the queries that the local model is uncertain about. The\ndownside to the global model is that the global model has a high\ninference time (e.g., up to 100ms). However, since it is only used\nwhen local model is uncertain and thinks the query is long-running,\nit is rarely used, so the additional time is amortized out.\nAdvantages. TheStage predictor designs a systematic approach\nto use these models that best leverages the optimality (exec-time\ncache ), instance-optimization (local model ), and transferrable knowl-\nedge about Redshift (global model ). By combining the merits of all\nthese models with different characteristics, the Stage predictor is\nable to reliably achieve high prediction accuracy at a (amortized)\nnegligible inference latency. Conceptually, the Stage predictor ef-\nfectively addresses the downside of the prior AutoWLM predictor\ninside Redshift. In particular, the Stage predictor is able to sig-\nnificantly improve prediction accuracy without adding too much\ninference latency. In addition, the Stage predictor has reliable per-\nformance, especially when used on a new instance with insufficient\ntraining queries or instances with changing data and query work-\nload. For those instances, the uncertainty of local model ‚Äôs prediction\nwill be high, and we can rely on the more robust global model. Atlast, the Stage predictor can provide probabilistic distribution or\nconfidence interval for the predicted exec-time to enable robust\ndecisions making of many downstream tasks.\n4.2 Exec-time Cache\nTheexec-time cache is able to output near-optimal prediction with\nnear-zero inference latency for repeating queries that were recently\nobserved (this is 60%of queries on average across the Redshift fleet).\nIn the following, we first explain the cache‚Äôs keys and values, the\nprocedure to make predictions, and the eviction policy of entries\nwhen the cache is full. Then, we discuss several optimizations.\nCache keys and values. Similar to AutoWLM predictor [ 50], the\nfirst step of the Stage predictor is to flatten a physical plan tree as\na vector. We traverse the plan tree, collect operator nodes of the\nsame type, and sum up their estimated cost and cardinality. We\nalso add features such as query type (e.g., SELECT ,DELETE ) and end\nup with an ùëõ-dimensional vector representation of the physical\nplan tree. The exec-time cache uses this vector for each query as\nkey and maps the actual exec-time of this query after execution as\nvalues. For the same query that executes multiple times, the cache\nstores all their observed exec-times as values. It is worth noticing\nthat in some very rare cases, two different plan trees may result\nin the same vector representation, which means that the exec-time\ncache cannot distinguish them. However, in those cases, their query\nplans should be very close to each other, and thus, we assume their\nexec-times should be similar as well.\nExec-time prediction. Whenever a new query arrives that matches\na cached key, the exec-time cache is able to predict its exec-time\nbased on its observed exec-times ùë°1,...,ùë°ùëò. Since variance exists in\nthese observed exec-times due to different system loads when the\nsame query is being executed, one might think to use the mean ùúáof\nthese exec-times as a prediction to increase prediction robustness.\nHowever, since the underlying table stats of Redshift may not be\nup-to-date, the same query executed at different time may access\nslightly different data, leading to different exec-times. In this case,\nùúáwill contain outdated exec-times, and the most recently observed\nexec-timeùë°ùëòcaptures the freshness of data. Therefore, we design a\nsimple heuristic to predict the exec-time: ùúá√óùõº+ùë°ùëò√ó(1‚àíùõº). This\nheuristic can capture both the robustness and the freshness of data.\nThe value of ùõºis a hyperparameter that balances the average exec-\ntime against the most recently observed exec-time. Empirically,\nùõº=0.8works well for the Redshift fleet. In the future, we plan to\ndesign more principled approaches for prediction based on observed\nexec-time, such as time series prediction.\nEviction policy. In order to maintain efficient memory usage and\nfast look-up speed, we need to ensure the number of cached queries\ndoes not grow unbounded. Therefore, whenever the number of\ncached queries surpasses a certain threshold, exec-time cache will\nevict the least updated queries from the cache. In practice, this can\nbe implemented by maintaining a sorted list of dates for each query\nat which the most recently observed exec-time is collected and\nremoving the query with the oldest date from the exec-time cache.\nOptimization 1: hash value replacement. As described above, the\nhash table stores query feature vectors as keys, so whenever an\nincoming query arrives, its entire query feature vector needs to\nbe compared element-by-element with the cached vectors. We can\n284\n\nSIGMOD-Companion ‚Äô24, June 9‚Äì15, 2024, Santiago, AA, Chile Ziniu Wu et al.\noptimize this vector-vector comparison by storing the hash value of\nthe feature vector as the key. This, in theory, may lead to collisions,\nas any query feature vectors with matching hash values will be\ntreated as identical, but we observed zero hash collision for all\nqueries in the top 200 instances in the Amazon Redshift fleet. This\noptimization removes the costly vector-vector comparison and\nsignificantly reduces the size of the hash table keys.\nOptimization 2: running mean and variance. As described above,\nthe values in the hash table are lists of past query latencies. This\ngives us some design flexibility, as we can compute any summary\nstatistic we want from the history (e.g., mean, median, quantiles).\nHowever, if we know we are only interested in the mean, variance,\nand the most recently observed exec-time, we can replace each\nquery history with a running mean and variance. Using Welford‚Äôs\nalgorithm [ 58], this only requires storing 4 values per hash table\nentry, reducing both the in-memory size of the cache and the cache‚Äôs\nlookup time (due to reading fewer in-memory values).\n4.3 Instance-optimized local model\nIn this section, we first describe the implementation of our local\nmodel, and then conceptually justify the design choices for local\nmodel in Redshift and compare it with other potential alternative\ndesigns. At last, we explain how to leverage the exec-time cache to\nbuild an effective and efficient training pipeline for local model.\nBayesian ensemble of XGBoost models. As mentioned in Sec-\ntion 2.2, we adapt and implement the Bayesian ensemble of XGBoost\nmodels [ 31] as our instance-optimized local model with a reliable\nuncertainty measurement. Recall that this ensemble independently\nlearnsùêæXGBoost models, each of which takes the 33-dimension\nvector representation of query as input and estimates a mean ùúáùëò\nand variance ùúé2\nùëòof a query‚Äôs exec-time. The final prediction of\nexec-time ÀÜùë¶is given by the average of each model‚Äôs prediction in\nEquation 1. The total uncertainty V[ÀÜùë¶](i.e., variance of prediction)\nof this prediction ÀÜùë¶is a summation of estimated model uncertainty\nand estimated data uncertainty as shown in Equation 2.\nÀÜùë¶=1\nùêæùêæ‚àëÔ∏Å\nùëò=1ùúáùëò (1)\nV[ÀÜùë¶]\n|{z}\nPrediction uncertainty=1\nùêæùêæ‚àëÔ∏Å\nùëò=1(ÀÜùë¶‚àíùúáùëò)2\n|             {z             }\nModel uncertainty+1\nùêæùêæ‚àëÔ∏Å\nùëò=1ùúé2\nùëò\n|    {z    }\nData uncertainty(2)\nJustification of the local model‚Äôs design choices. The model un-\ncertainty is estimated as the variance of each XGBoost model‚Äôs\nprediction. Since each model is independently trained, when local\nmodel does not have enough training data or if the incoming query\nis different from the training queries, the models will have diverse\ninterpretations of this query. Thus, the variance of their prediction\nwill be high, and the global model could come to the rescue.\nThe estimated data uncertainty can capture the inherent nois-\niness in the labels and training features themselves. In Redshift,\nthe same query executed at a different time can have different\nexec-times (i.e. noisiness in labels) due to different system loads\nand concurrency state. Meanwhile, the input to local model alsocontains high noise. Specifically, the 33-dimensional vector feature\ndoes not fully capture all information of the physical query plan\ntree (e.g., tree structure, missing node types, and Redshift‚Äôs cardi-\nnality estimation error). When a query plan is complicated with\nmany joins, the vector feature tends to be less representative and\nthelocal model will have a high data uncertainty. In this case, the\nglobal model will take the entire physical execution plan as input\nand will have a better performance.\nTherefore, using the Bayesian ensemble of XGBoost models as\nthelocal model captures two sources of uncertainty that could result\nin high prediction errors in Redshift. There exists a line of works in\nthe machine learning domain for quantifying the prediction uncer-\ntainty, which are less optimal to apply inside Redshift. Specifically,\nuncertainty measurement using deep learning models [ 16,26,59]\nare not practical in Redshift due to their large inference latency.\nThe popular lightweight alternatives for uncertainty measurement\nnormally only focus on one source of uncertainty. For example,\nuncertainty in random forest regression [ 10,39], quantile regres-\nsion forests [ 36], and one-class support vector machine for outlier\ndetection [ 3,28] mainly focus on quantifying the model uncertainty\nbut not the data uncertainty. Whereas, another line of works on\nprobabilistic prediction [ 14,18,44,49] and probabilistic program-\nming [ 6,19] mainly focus on understanding the uncertainty in data\nitself rather than quantifying the model uncertainty.\nIt is worth noting that using the Bayesian ensemble of XGBoost\nmodels as local model in Redshift also involves minimal engineering\neffort since the prior AutoWLM predictor inside Redshift already\nuses XGBoost. In order to build the new local model, we only need to\nchange the loss function of the XGBoost model and independently\ntrain multiple such models.\nLocal model training optimization. The training process of the\nlocal model needs to maintain a diverse set of training queries to\ntrain effective local models. At the same time, it also needs to ensure\na low memory and computation overhead of training because the\ntraining process is conducted locally on the customers‚Äô database\nclusters. Therefore, we tailor the training process based on the\nunique characteristics of Redshift queries.\nWe collect the observed features and latency of executed queries\ninto a training query pool. Naively storing every query execution\nresult in the training pool has the following three issues: (1) the size\nof the training pool would grow unbounded, (2) the training pool\nwould become ‚Äúpolluted‚Äù with repetitive queries that the exec-time\ncache will take care of anyway, and (3) the training pool will have\nmore short queries than long queries, skewing prediction accuracy\nfor longer (and often more important) queries.\nBounding the size. To resolve the first issue of the training pool,\nwe cap the total number of queries in the training query pool.\nWhenever the number of queries exceeds a certain threshold, the\ntraining pool will evict the oldest observed queries.\nDealing with repeats. Recall that a large amount of queries in\nRedshift repeat themselves, which will significantly reduce the\ndiversity of queries in the training pool. Besides, these repeating\nqueries will be captured by the exec-time cache, so overfitting these\nqueries may degrade local model ‚Äôs generalizability to other queries.\nTherefore, we deduplicate the repeating queries in the training pool.\n285\n\nStage: Query Execution Time Prediction in Amazon Redshift SIGMOD-Companion ‚Äô24, June 9‚Äì15, 2024, Santiago, AA, Chile\nType: AggregateCost: 1671.5Cardinality: 80Width: 16S3 format: NullTable rows: NullDistributed hash joinDistributed hash joinHash1. Node embedding\n2. Graph convolution     message passing3. Exec-time prediction\n+\n15.3s\nSequentialScanSequentialScanSequentialScan\nFigure 5: Global model featurization and architecture.\nWe leverage the exec-time cache to implement this data dedupli-\ncation efficiently. Specifically, for each executed query, we hash\nits observed features and check against the exec-time cache. If the\nquery misses the cache, we add it to the training pool.\nDuration diversity. Next we address the third issue, which is that\nthe distribution of query latencies is skewed. Most of the Redshift\nqueries execute in less than a couple of seconds, so our training pool\ncan be filled by short-running queries. In this case, the local model\nwill have catastrophic performance for longer-running queries.\nTherefore, we partition the training pool into several query exec-\ntime buckets (e.g., 0‚àí10ùë†,10‚àí60ùë†, and 60ùë†+) and assign a cap for\neach bucket to ensure the query diversity in the training pool.\n4.4 Transferrable global model\nInspired by recent work in zero-shot cost model [ 21], we design\nthe instance-independent featurization of Redshift query plans,\nallowing us to map query plans from various customers‚Äô instances\nto a unified space. As a result, we can collect a diverse set of training\nqueries from a large amount of Redshift instances to jointly train\noneglobal model that is able to make robust predictions for all\ninstances, including the unseen instances. The global model uses a\ngraph convolutional network (GCN) [ 24] architecture to understand\nthe query plan of Redshift and map it to its exec-time.\nQuery plan featurization. We run the fleet sweep to gather the\nlogs (i.e., STL_EXPLAIN table) on the physical execution plans of\nexecuted queries from the customers‚Äô Redshift instances. Then,\nwe parse the information from the logs into a tree data structure\nrepresenting each query plan as shown in Figure 5. Each node\nin the tree represents a physical operator (e.g., ‚Äúsequential scan‚Äù,\n‚Äúhash‚Äù, ‚Äúmaterialize‚Äù, ‚Äúdistributed hash join‚Äù, ‚Äúaggregate‚Äù, ‚Äúorder by‚Äù),\nand we featurize it as its operator type, estimated cost, estimated\ncardinality, tuple width, S3 table format (e.g., ‚ÄúParquet‚Äù, ‚ÄúOpenCSV‚Äù,\n‚ÄúText‚Äù or ‚ÄúLocal‚Äù if the table is stored locally), and number of rows\nin the table. An example of node features is shown in red in Figure 5.\nIt is worth noticing that 90 unique operator types exist in Redshift,\nso we represent the node operator type as a 90-bit one-hot vector.\nFurthermore, we set the S3 table format and table rows features to‚ÄúNull‚Äù if the operator is not directly operating on a base table (e.g.,\nnot a scan operation).\nModel architecture. Our global exec-time predictor contains three\ncomponents: node embedding, graph convolution message passing,\nand final exec-time prediction. First, the features of all nodes are\nembedded with a multi-layer perceptron (MLP) to a feature vector.\nOne example is shown in blue color in Figure 5. Then, we use a\nGCN model to perform message passing between nodes to aggre-\ngate information and understand operator interactions in Redshift.\nSome message passing directions are shown in green in Figure 5.\nSpecifically, GCN consists of several layers of message passing. In\nthe first layer of GCN, each node combines its own embedded node\nfeatures with those of its children and transforms them into a new\nnode feature. This feature combination process is controlled by\nlearnable weights. The following GCN layers will repeat the same\nprocess on the transformed features of each node from the previous\nlayer. After several GCN layers, information on all nodes will be\naggregated to the root node, and a vector representation of the\nentire query plan will be outputted. The GCN message passing is\ncapable of understanding the complex operator dependencies and\ninteractions, as shown by the zero-shot cost model [ 21]. At last, the\nfinal vector representation output by GCN will be concatenated\nwith a system feature vector, which includes a summarization of\nthe query plan, Redshift instance type, number of Redshift nodes,\nmemory size, and number of concurrent queries. The system fea-\nture vector contains factors that may affect query exec-time other\nthan the query execution plan itself. The concatenated feature will\nbe sent to an MLP to estimate the exec-time of this query.\nOur global GCN model is trained on a diverse set of hundreds of\nRedshift instances, each with more than 10,000 queries.\n5 EXPERIMENTAL EVALUATION\nIn this section, we evaluate the performance of the Stage predictor\nand compare it against the prior AutoWLM predictor in Redshift\non real-world data. We first explain the experimental setting in\nSection 5.1 and then evaluate the following questions:\n‚Ä¢End-to-end (Section 5.2): How much practical gain can the\nStage predictor achieve in terms of improving end-to-end\nquery execution latency in Redshift?\n‚Ä¢Accuracy (Section 5.3): How accurate is the Stage predic-\ntor? What is its model size and inference latency?\n‚Ä¢Ablation (Section 5.4): How accurate and robust is each\nhierarchy of Stage predictor individually?\n5.1 Experimental Settings\nReal-world workloads. Since we are primarily concerned with the\nperformance of the Stage predictor on the Redshift fleet, we evaluate\ntheStage predictor on query logs of real Redshift customers. We\nselect the top 100 most-billed instances in the month of July 2023\nfrom Redshift in each of the three regions: ‚Äòus-east-1‚Äô, ‚Äòus-west-\n2‚Äô, and ‚Äòeu-west-1‚Äô. We select all user-executed queries on these\ninstances from ‚ÄòJuly 28th, 2023‚Äô to ‚ÄòAugust 11th, 2023‚Äô, resulting\nin a total of roughly 30 million queries. Unless specifically stated\notherwise, all our experiments are conducted on these queries.\n286\n\nSIGMOD-Companion ‚Äô24, June 9‚Äì15, 2024, Santiago, AA, Chile Ziniu Wu et al.\nLocal environment. We train and evaluate the performance of\nourStage predictor and the baseline (i.e., AutoWLM predictor in\nRedshift) on one AWS ‚Äòm5.4xlarge‚Äô machine with 64 GB memory, 16\nvCPUs, and Intel Xeon Platinum 8175 processor. It is worth noting\nthat the offline training of our global model is conducted on two\nAWS ‚Äòp3.8xlarge‚Äô machines, each with 244 GB memory, 32 vCPUs,\nand 4 Nvidia tesla v100 GPU. We only leverage GPU to accelerate\nthe training of the global model, not the cache or local model.\nModel training. We simulate the exact training and testing pro-\ncedure as in real Redshift deployment to evaluate the performance\nofStage predictor and the baseline. Specifically, on each cluster, we\nreplay all the queries sequentially based on their logged execution\nstart time. In addition, we randomly sample 100 training instances\nand use three weeks of user-executed queries on all instances to\ntrain our global model. These training instances do not overlap with\nthe evaluation instances. Unless specified otherwise, all predictions\nused for evaluation are derived from the aforementioned procedure.\nHyper-parameters. Our Stage predictor contains a set of hyper-\nparameters that are relatively easy to tune. We describe each of\nthe hyper-parameters and its value as follows. Each value was de-\ntermined via tuning on data prior to our test workload (i.e., data\nused to select hyperparameters is completely separate from the\nevaluation dataset). For the exec-time cache, we set the cache size\nto 2,000 (i.e., it can only keep 2,000 unique queries before eviction).\nFor the local model, we used the CatBoost Python package [ 48] to\ntrain 10XGBoost models independently. For each XGBoost model,\nwe set the number of estimators as 200, the max depth as 6, and the\nnumber of parallel trees as 1. When training the XGBoost models,\nwe randomly choose 20%of the training data as a validation set for\nearly stopping to prevent overfitting. We note that the AutoWLM\npredictor baseline in Redshift uses exactly the same hyperparam-\neters for the XGBoost model. The differences between our local\nmodel and the baseline are (1) we train 10 models instead of one; (2)\nwe use a log-likelihood loss function instead of the mean absolute\nerror as used by the baseline. Both changes are necessary to provide\nan uncertainty measure of prediction. For global model, we use a\ndirected GCN with the hidden dimension size 512, 8 layers of graph\nconvolution, and 0.2weight dropout ratio.\n5.2 End-to-end Evaluation in Redshift\nThe most straightforward and important approach to evaluate the\neffectiveness of Stage predictor is to test how much it can improve\nthe end-to-end query execution latency inside Redshift. It is worth\nnoticing that query latency is different from query exec-time ‚Äî\nlatency includes the scheduling time and wait time of a query,\nwhereas exec-time excludes those factors.\nEnd-to-end simulation. To evaluate the impact of the Stage predic-\ntor on end-to-end performance, we simulate the Redshift workload\nmanager [ 50] using the Redshift team‚Äôs internal tools. The simula-\ntor replays an existing user workload using the Stage predictor. The\nsimulator then computes the expected query latency of each query\nin the workload. More accurate exec-time predictions will cause the\nworkload manager to make better scheduling decisions and thus\nimprove query latency. It is worth noticing that in this simulation,\nexec-time prediction accuracy only affects query wait time but not\n44.4%20.3%0.0%0.0%16.4%59.8%0.0%14.9%54.5%Figure 6: End-to-end performance in terms of query latency\nof different exec-time predictors inside Redshift. We listed\nthe percentage improvement over the AutoWLM predictor.\nactual query exec-time: that is, the query execution time is assumed\nto be identical to when the query was actually executed by the cus-\ntomer. This could lead to some simulation inaccuracies since the\nworkload manager‚Äôs decisions can impact query execution time (i.e.,\nresource allocation). However, we have verified through various\nother experiments conducted internally by Redshift teams on the\nworkload manager that improvements in the simulation results\naccurately reflect actual query execution latency in production.\nWe chose the workload manager simulation as our end-to-end\nevaluation for two reasons. First, we cannot directly compare the\nStage predictor with the AutoWLM predictor in real production\nenvironments. This is because users execute their queries once and\nonce only, so the workload manager either uses the Stage predictor\nor the AutoWLM predictor to get the actual query execution latency.\nCounterfactually ‚Äúreplaying‚Äù the workload via a simulator is the\nonly way to measure possible improvements. Second, other tasks\nexist in Redshift as sub-routines (e.g., automatic materialized view\ncreation). However, those tasks are not on the critical path of query\nexecution, so they can only indirectly reflect the query execution\nlatency changes. Thus, we cannot explicitly evaluate the benefit\nofStage predictor on those tasks. It is worth noticing that part\nof the Stage predictor (exec-time cache andlocal model ) is already\ndeployed in production. Due to some observed regression in the\naccuracy of global model (see Section 5.4), we are exploring a more\nrobust global model before deploying it in Redshift.\nPerformance comparison. We conduct the simulation experiment\nusing three exec-time predictors: Stage predictor, the AutoWLM\npredictor, and the actual exec-time (Optimal ). The Optimal feeds\nthe observed exec-time to the workload manager, representing the\noptimal performance an exec-time predictor can ever achieve.\nThe overall end-to-end query execution latency on all top 100\nmost-billed instances in three regions (roughly 30 million queries)\nare shown in Figure 6. We observe that the Stage predictor signifi-\ncantly improves over the AutoWLM predictor: the 20.3%,16.4%, and\n14.9%query execution latency improvement on average, median,\nand tail, respectively. This improvement purely results from a more\naccurate exec-time predictor. However, we observe that Optimal\nhas a significantly better performance than Stage predictor: 44.4%,\n287\n\nStage: Query Execution Time Prediction in Amazon Redshift SIGMOD-Companion ‚Äô24, June 9‚Äì15, 2024, Santiago, AA, Chile\n0 25 50 75 100\nClusters (%)-40-20020406080100Mean Latency Improvement (%)Stage Predictor\nOptimal\nFigure 7: End-to-end query latency improvement over Au-\ntoWLM predictor on each top instance. We sort the instances\nbased on the improvement the optimal predictor achieves.\n59.8%, and 54.5%query execution latency improvement on average,\nmedian, and tail, respectively. This suggests that there still exists a\nlarge room for improvement, and further improving the accuracy\nof exec-time predictor in Redshift can be fruitful.\nIn addition, we analyze the average query latency improvement\nof the Stage predictor over the AutoWLM predictor on each instance\nin Figure 7. For comparison, we also plot the optimal predictor‚Äôs\nimprovement and sort the instances based on this value. We can\nsee that the Stage predictor is able to improve the average query\nlatency for most of the instances. However, there are regressions:\non less than 10% of the instances, Stage predictor actually does\nworse than the AutoWLM predictor. There are several possible\nexplanations. First, the AutoWLM predictor does occasionally make\nbetter predictions than the Stage predictor on a small portion of\nqueries, which could have an impact on the workload manager.\nSecond, both the Stage predictor and the AutoWLM predictor have\nerroneous predictions, as detailed in Section 5.3, and there is an\nasymmetry in prediction errors. For example, for a query with\nexec-time of 30s, the Stage predictor may predict it to be 5s, thus\nsending it to the short-running queue, and the AutoWLM predictor\nmay make a worse prediction of 900s, thus sending it correctly to\nthe long-running queue. In this scenario, although Stage predictor\nis more accurate, it makes the wrong decision. Third, due to the\nalgorithmic design of workload manager [ 50], there will be edge\ncases when perfect prediction does not lead to the best end-to-\nend query latency. This also explains why the Stage predictor can\nsometimes outperform the optimal predictor.\n5.3 Stage Predictor Accuracy\nWe show the prediction accuracy of the Stage predictor and the\nAutoWLM predictor on all top 100 most billed instances from three\nregions in Redshift, with a total of 27,441,359queries. The accu-\nracy is evaluated using absolute error, that is, |actual exec-time ‚àí\npredicted exec-time | in seconds. We show the accuracy comparison\nin Figure 8 and report the details of mean (MAE), median (p50-AE),\nand tail (p90-AE) absolute error of these queries in Table 1. Stage\npredictor is able to achieve a median absolute error of 0.67, sug-\ngesting that for 50%of the query, Stage prediction is within 0.67ùë†of\nMean Absolute Error (s)Query execution time rangeFigure 8: Prediction accuracy of stage predictor compared to\nthe AutoWLM predictor in Redshift.\nQuer\ny Exec-time #\nQueriesStage\npredictor AutoWLM predictor\nMAE\nP50-AE P90-AE MAE\nP50-AE P90-AE\nO\nverall 27,441,359 7.76\n0.67 9.39 17.87\n2.03 23.68\n0s\n‚Äì 10s 22,015,851 3.74\n0.31 7.43 9.04\n1.11 14.44\n10s\n‚Äì 60s 5,085,965 8.53\n2.60 13.68 32.83\n10.02 51.34\n60s\n‚Äì 120s 163,913 50.11\n24.15 85.00 91.63\n31.04 113.8\n120s\n‚Äì 300s 83,590 126.4\n70.46 206.4 181.9\n84.55 255.4\n300s+ 92,041 744.4\n235.7 1496 990.1\n289.7 1922\nTable 1: Prediction accuracy (absolute error in seconds) of\nstage predictor and the AutoWLM predictor.\nQuer\ny Exec-time #\nQueriesStage\npredictor AutoWLM predictor\nMQE\nP50-QE P90-QE MQE\nP50-QE P90-QE\nO\nverall 27,441,359 54.57\n1.60 19.00 171.8\n4.08 135.7\n0s\n‚Äì 10s 22,015,851 43.87\n1.92 26.25 97.41\n6.38 173.1\n10s\n‚Äì 60s 5,085,965 71.66\n1.18 2.16 441.4\n1.77 6.39\n60s\n‚Äì 120s 163,913 251.9\n1.38 4.57 633.2\n1.51 4.97\n120s\n‚Äì 300s 83,590 307.4\n1.59 5.83 548.1\n1.71 6.61\n300s+ 92,041 1084\n1.48 6.12 1922\n1.58 10.00\nTable 2: Prediction accuracy (in Q-error) of stage predictor\ncompared to the AutoWLM predictor.\nactual execution time. Overall, Stage predictor achieves more than\n2x more accurate prediction than the AutoWLM predictor.\nTo dive deep into the results, we provide a detailed accuracy\ncomparison on queries with different exec-time ranges in Table 1. It\nis very important to analyze the prediction performance of queries\nwith different exec-time ranges because the workload manager of\nRedshift schedules queries into execution queues and assigns prior-\nity according to the predicted exec-time. Specifically, we see that\nStage predictor is able to achieve more than 3ùë•better prediction\non queries with less than 60s exec-time. We additionally provide\nthe same table on another widely-used metric for relative error: Q-\nError [ 40], that is,ùëöùëéùë•{ùëùùëüùëíùëëùëñùëêùë°ùëíùëë/ùë°ùëüùë¢ùëí,ùë°ùëüùë¢ùëí/ùëùùëüùëíùëëùëñùëêùë°ùëíùëë}in Table 2.\nThe minimal value Q-Error can take is 1, and closer to 1 implies\na more accurate prediction. We observe a roughly similar pattern\nthat the Stage predictor significantly outperforms the AutoWLM\npredictor on 60s exec-time. However, it achieves mild improve-\nment on the queries with more than 60s exec-time, possibly due\nto the following reasons. First, the distribution of query exec-time\nis heavily skewed and only 1%of the queries execute longer than\n60s. Therefore, both Stage predictor and the AutoWLM predictor\ndon‚Äôt have enough training data on the long-running queries and\nyield worse performance. Second, the long-running queries are\ninherently more difficult to predict because of a larger noisiness\n288\n\nSIGMOD-Companion ‚Äô24, June 9‚Äì15, 2024, Santiago, AA, Chile Ziniu Wu et al.\nAverage memory usageAverage inference latency\nFigure 9: Average inference latency and average memory\nusage overhead of different exec-time predictors.\nin the label. We observe in Redshift that the exec-time of the same\nquery repeatedly executed several times can range from just tens of\nseconds to several hundred seconds. This scenario is largely due to\ndifferent system loads, cache states, and the number of concurrency\nqueries in Redshift during the query execution. Unfortunately, our\nexec-time predictor cannot yet take that into account. In future\nwork, we plan to design better exec-time predictors that can take\nsystem statistics into account to explain the variance in labels.\nIn addition to accuracy, we provide the average inference la-\ntency and memory usage overhead of Stage and AutoWLM pre-\ndictor along with each component of Stage predictor in Figure 9.\nIt is worth noticing that since different instances of Redshift are\nlikely to have different hardware types, the numbers in Figure 9 are\nrough estimations rather than actually measured inference latency\nand memory usage. Although Stage predictor (red dot) has a larger\ninference latency and memory overhead than the AutoWLM predic-\ntor (blue square), it is still within a practical range: sub-millisecond\ninference latency, a few hundred kb memory usages. Specifically,\ntheexec-time cache (yellow dot) is able to make an inference in\njust a couple of microseconds. The local model (green dot) trains 10\nXGBoost models as opposed to one in AutoWLM predictor, so it is\ngenerally 10ùë•larger and slower to make inferences than AutoWLM\npredictor. The global model (purple dot) is a deep-learning-based\nmodel, which is roughly two of a magnitude larger than other pre-\ndictors. However, since the deep learning model is rarely used (3%\nof the time), its inference latency is amortized out. Furthermore,\nwe do not include the memory overhead of global model into Stage\npredictor because it will eventually be deployed as a serverless\nLambda function that every Redshift instance can invoke to avoid\nlocal memory and CPU overhead.\n5.4 Ablation study\nIn this section, we first provide detailed accuracy analysis on each\ncomponent of Stage predictor: exec-time cache, local model, and\nglobal model. Then, we study how reliable is the uncertainty mea-\nsure of our local model.\nAccuracy of exec-time cache. We find that 16,963,658out of the\n27,441,359queries in these instances (61 .8%) repeat themselves and\ncan be directly predicted by exec-time cache. As shown in Table 3,\nexec-time cache overall achieves a significantly better prediction ac-\ncuracy than the AutoWLM predictor. The advantages are apparent\nsince the AutoWLM predictor (XGBoost model) is trained on theQuer\ny Exec-time #\nQueriesExe\nc-time cache AutoWLM predictor\nMAE\nP50-AE P90-AE MAE\nP50-AE P90-AE\nO\nverall 16,963,658 4.83\n0.56 4.66 15.04\n3.00 20.35\n0s\n‚Äì 10s 12,616,915 1.82\n0.16 2.74 7.40\n1.20 15.96\n10s\n‚Äì 60s 4,212,128 4.55\n2.27 6.61 23.15\n7.57 24.26\n60s\n‚Äì 120s 74,604 30.87\n9.80 67.93 43.88\n23.29 77.2\n120s\n‚Äì 300s 27,185 115.8\n72.78 197.8 117.4\n88.13 205.6\n300s+ 32,826 764.3\n193.4 1524 1046\n284.4 2045\nTable 3: Prediction accuracy (absolute error in seconds) of\nexec-time cache and AutoWLM predictor in Redshift.\nQuer\ny Exec-time #\nQueriesLo\ncal model AutoWLM predictor\nMAE\nP50-AE P90-AE MAE\nP50-AE P90-AE\nO\nverall 10,477,701 21.48\n4.16 34.88 19.06\n4.32 29.27\n0s\n‚Äì 10s 9,398,936 13.76\n3.27 30.88 10.94\n3.60 23.41\n10s\n‚Äì 60s 873,837 35.63\n16.47 87.06 32.63\n12.90 54.36\n60s\n‚Äì 120s 89,309 77.69\n37.61 137.6 69.65\n36.04 93.40\n120s\n‚Äì 300s 56,405 140.1\n72.03 230.5 120.7\n76.75 195.1\n300s+ 59,215 840.3\n267.6 1652 852.3\n276.0 1729\nTable 4: Prediction accuracy (absolute error in seconds) of\nthe local model and AutoWLM predictor in Redshift.\nexecuted queries‚Äô exec-time as ground truth, which is captured in\nthe cache. Therefore, in theory, the locally trained model can never\noutperform the exec-time cache. However, it is worth noticing that\nexec-time cache does make significant errors (in terms of absolute\nerror) because these repeating queries are executed at different sys-\ntem loads, buffer pool states, and concurrency conditions, making\nit extremely hard to predict the exec-time at a different state accu-\nrately. We find roughly the same pattern for prediction accuracy\ncomparison in terms of Q-error, so we omit all results on Q-error\ndue to space limitations.\nAccuracy of local model. We evaluate and compare the perfor-\nmance of the local model to the AutoWLM predictor in Redshift\non the 10,477,701out of the 27,441,359queries that miss the exec-\ntime cache (38.2%). Recall that there are only two differences be-\ntween local model and AutoWLM predictor: 1) local model inde-\npendently trains 10XGBoost model whereas AutoWLM predictor\nonly uses one; 2) the XGBoost model in local model is trained with\nlog-likelihood loss whereas AutoWLM predictor is trained with the\nabsolute error. We observe in Table 4 that local model is slightly\nworse than the AutoWLM predictor because the AutoWLM predic-\ntor is directly trained to optimize the absolute error, which is the\nevaluation metric. As a future work, we plan to lower the gap in\nperformance difference between the two by adding an XGBoost\nmodel trained with absolute error into the Bayesian ensemble of\nXGBoost models in local model.\nAccuracy of global model. The global model is trained on a di-\nverse set of instances and evaluated on the top-billed instances with\nunseen queries. We first compare the performance of global model\nagainst the local model on all queries that miss the cache in Ta-\nble 5. We observe that thelocal model has a better performance\nthan global model, especially on long-running queries. This was\nsurprising to us because it runs against the common wisdom that\n‚Äúmore data makes a better model‚Äù: in this case, the global model is\ntrained on significantly more data than the local model. However,\nthelocal model ‚Äôs data is much closer in distribution to the test data,\n289\n\nStage: Query Execution Time Prediction in Amazon Redshift SIGMOD-Companion ‚Äô24, June 9‚Äì15, 2024, Santiago, AA, Chile\nQuer\ny Exec-time #\nQueriesGlobal\nmodel Local model\nMAE\nP50-AE P90-AE MAE\nP50-AE P90-AE\nO\nverall 10,477,701 23.82\n6.42 29.53 21.48\n4.16 34.88\n0s\n‚Äì 10s 9,398,936 12.61\n8.48 20.96 13.76\n3.27 30.88\n10s\n‚Äì 60s 873,837 39.42\n17.92 67.58 35.63\n16.47 87.06\n60s\n‚Äì 120s 89,309 166.1\n111.9 204.6 77.69\n37.61 137.6\n120s\n‚Äì 300s 56,405 366.5\n243.9 519.9 140.1\n72.03 230.5\n300s+ 59,215 1763\n701.5 3540 840.3\n267.6 1652\nTable 5: Prediction accuracy (absolute error in seconds) of\nthe global model compared to the local model on all queries\nthat miss the exec-time cache.\nQuer\ny Exec-time #\nQueriesGlobal\nmodel Local model\nMAE\nP50-AE P90-AE MAE\nP50-AE P90-AE\nO\nverall 361,752 134.8\n10.09 164.1 164.7\n25.21 196.8\n0s\n‚Äì 10s 167,617 9.45\n3.90 20.37 50.77\n18.82 113.4\n10s\n‚Äì 60s 91,714 33.31\n48.46 67.58 62.12\n20.14 113.0\n60s\n‚Äì 120s 37,539 64.72\n35.78 92.89 82.03\n34.27 151.2\n120s\n‚Äì 300s 31,508 230.7\n90.51 309.9 235.5\n82.14 326.6\n300s+ 33,343 1033\n423.8 2011 1046\n391.9 1712\nTable 6: Prediction accuracy (absolute error in seconds) of\nglobal model compared to local model on uncertain queries.\nand the local model is able to win out. While there are some specific\ndatabases where the global model outperforms the local model, the\noverall trend favors the local model, which might be evidence that,\nin the context of query performance prediction, better data beats\nbigger data. In our opinion, this casts serious doubt on the premise\nthat cloud database operators can train effective ‚Äúcross-customer‚Äù\nmodels that are better than instance-optimized models.\nOne possible explanation for the relatively poor performance\nof the global model is a lack of model capacity: could a sufficiently\nlarge model learn the latent information hidden in each database\ninstance with enough training data? While we are unable to an-\nswer this question conclusively, we did find several examples of\nnearly identical query plans with nearly identical cost and cardi-\nnality estimates from different customers with drastically different\nperformances. No amount of data can resolve this issue, as there are\ntwo nearly identical training inputs with wildly different desired\noutputs. Thus, database-specific features may be needed for a global\nmodel to learn to differentiate between these pairs.\nFor the context of this work, we primarily care about the per-\nformance of the global model when the local model is uncertain\nand thinks the query is long-running. We evaluate those queries in\nTable 6. In this scenario, global model is able to achieve a better\nresult than the local model. This suggests that global model is able\nto provide more robust and reliable prediction whenever the local\nmodel is uncertain. However, we barely observe improvement on\nlong-running queries (larger than 120s) over local model because\nthose queries are very sparse in each instance and may contain\ninstance-specific characteristics that the global model cannot under-\nstand. Overall, we do see a significant performance drop for local\nmodel itself, shifting from testing on all queries to only uncertain\nones. This implies that our uncertainty measurement for local model\nis reliable. We provide a more exhaustive analysis of the reliability\nof this uncertainty measurement as follows.\nUncertainty measurement in local model. We use the well-\nestablished scoring rule: prediction-rejection ratio (PRR) [ 30,31] to\nPrediction uncertaintyProportion of queriesCumulative AE / Total AEAbsolute prediction errorPrediction uncertainty\nCumulated AE / Total AEProportion of queriesOracle Uncertainty Random Figure 10: Illustration of PRR calculation on queries from\nan example instance, whose PPR score is 0.9.\nPRR ScoreNumber of instancesPRR ScoreAverageMedian\nFigure 11: Uncertainty quality of local model (prediction\nrejection ratio) distribution for all top instances.\nevaluate how good is the uncertainty measurement of local model.\nPRR quantifies the rank correlation between the predicted uncer-\ntainty and the observed prediction error for each query in one\ninstance. To better explain how PRR works, we provide an example\nin Figure 10. On the left of Figure 10, we show 2000 testing queries\nfrom one instance and plot their uncertainty estimated by the lo-\ncal model on the x-axis against the observed absolute estimation\nerror on the y-axis. We can see a statistically significant positive\nrelation between these two values. On the right of Figure 10, we\nshow the calculation of PRR for this instance. Specifically, we first\nsort/rank all queries based on their observed absolute estimation\nerror (‚ÄúOracle‚Äù) in descending order and plot the proportion of\ncumulative error (i.e., cumulative error/total error) as shown in\nthe red curve in this figure. Then, we sort/rank all queries based\non their prediction uncertainty in the blue curve and randomly\nsort all queries as in the black curve. Ideally, if our uncertainty has\na perfect correlation with the actual error, the blue curve should\noverlap with the red curve. Therefore, the ‚Äúcloser‚Äù blue curve is\nto the red curve, the more reliable our prediction uncertainty is.\nWe can compute the area under the curve (AUC) between red and\nblack curves as (AUC_Oracle) and the AUC between blue and black\ncurves as (AUC_Stage). The PRR score is quantitatively defined as\nthe ratio AUC_Stage/AUC_Oracle, between 0 and 1.\nWe calculate the overall PRR score for all top instances and plot\ntheir distribution in Figure 11. We can see that 30%of the instance\nhas a PRR score close to 1, which suggests that our uncertainty\nmeasurement can perfectly capture estimation error for these in-\nstances. We have a median PRR score of 0.9, which is the same\nas the example in Figure 10. However, in some instances, the PRR\nscore is very low, generally because of insufficient training queries.\n290\n\nSIGMOD-Companion ‚Äô24, June 9‚Äì15, 2024, Santiago, AA, Chile Ziniu Wu et al.\n6 LESSONS LEARNED AND POTENTIAL\nFUTURE RESEARCH DIRECTIONS\nThroughout the process of building Stage predictor in Redshift, we\nlearned several important lessons that could benefit the research\ncommunity. In the following, we describe these lessons and list\npotential research directions that could be valuable to the industry.\n6.1 Applying Stage predictor in other tasks\nApart from improving the workload manager in Redshift, we believe\ntheStage predictor enables new solutions for many tasks in smart\nDBMSes, including query optimization and hypothetical reasoning.\nQuery optimization. Recently, many ML-based solutions have\nbeen proposed for query optimization [ 33,64]. Although these ML-\nbased techniques can select join-order and physical operators more\naccurately than the traditional optimizers, their inference latency\ncan get up to several hundred milliseconds. Thus, this overhead\nwill be impractical to optimize short-running queries. To practi-\ncally integrate ML-based optimizers into real systems, they can\nuseStage predictor as a sub-routine. Specifically, a query could\nfirst be optimized by the default query optimizer inside a system.\nThen, the query plan is fed into Stage predictor to estimate its exec-\ntime, and the system will use the expensive ML-based optimizer to\nre-optimize the plan only if it is long-running.\nAnswering ‚Äúwhat-if‚Äù questions. Hypothetical reasoning is a cru-\ncial element of many database decision-making tasks, including\nprovenance updates, view manipulation, knob tuning, and auto-\nmatic cluster scaling [ 4,38,43]. Hypothetical reasoning allows DB\nadministrators and users to test database assumptions by asking\n‚Äúwhat-if‚Äù questions, such as ‚Äúwhat will the performance of existing\nqueries be if an index on column X is created?‚Äù, ‚Äúwhat if the data\nsize increases by 5x?‚Äù, ‚Äúwhat if the cluster adds 3 nodes?‚Äù. Answer-\ning ‚Äúwhat-if‚Äù questions is very difficult. For example, learning a\nquery performance predictor on the executed workloads of a data-\nbase cannot accurately estimate the performance under ‚Äúwhat-if‚Äù\nscenarios because the model does not observe any training data\nunder such hypothetical scenarios. Therefore, existing methods\nmainly rely on casual inference to answer these questions [ 17,37].\nIn theory, the global model of Stage predictor could provide more\naccurate and more fine-grained answers to these ‚Äúwhat-if‚Äù ques-\ntions. Since the transferrable global model distills the knowledge\nof a DBMS, it will observe these ‚Äúwhat-if‚Äù scenarios happening on\nother similar databases. Thus, it can leverage the observation on\nother databases to accurately predict the query performance under\n‚Äúwhat-if‚Äù scenarios of the current database.\n6.2 Hierarchical models\nWhen designing a practical exec-time predictor for Redshift, we\nfound that although a plethora of ML-based predictors can provide\nmore accurate exec-time prediction, their inference overhead is too\nlarge to be deployed on the critical path of Redshift. We believe\nthis problem generally exists in the database research community\nbeyond exec-time prediction and Redshift.\nMost ML models naturally present a trade-off between accuracy\nand model size/inference latency that more accurate models tend to\nbe more expensive. Thus, when sophisticated ML-based solutionsare adopted to solve existing database problems on the critical path\nof query execution, they will inevitably incur a non-trivial overhead.\nThis overhead may be unaffordable for short-running queries. We\nbelieve the hierarchical model solutions, similar to the Stage predic-\ntor, could enable a practical adoption of ML-based solutions to best\nleverage their accuracy with an affordable inference overhead. To\nthe best of our knowledge, there does not exist other works in the\ndatabase community that use the idea of hierarchical models. In the\nfollowing, we provide a detailed example of cardinality estimation,\nwhich is on the critical path of query execution.\nCardinality estimation is crucial for query optimization. Due to\nits challenging nature, sophisticated ML-based solutions [ 22,65,69]\nhave been proposed to improve the accuracy of their traditional\ncounterpart. Their inference latency varies from a couple of millisec-\nonds to a hundred milliseconds [ 20], which will not be affordable for\nshort-running queries. A hierarchy of several cardinality estimators\nwith different accuracy/overhead trade-offs could enable practical\nintegration of ML-based solutions in real systems. Specifically, the\nqueries will first be fed into cheap estimators and more expensive\nestimators will be invoked only if the previous cheaper estimator\nis uncertain about its prediction. Therefore, the inference overhead\nof the expensive estimators can be amortized out.\n6.3 Environment factors in exec-time prediction\nInside Redshift, we found that the same query in the same clus-\nter can sometimes have very different exec-times ranging from a\ncouple of seconds to several minutes, even hours because of dif-\nferent environment factors at the time of execution. These factors\ninclude memory and CPU utilizations that directly affect the query\nexec-time. For example, if 90%of memory has been used by other\njobs in the clusters, a query may spill its intermediate results to\ndisk, incurring a large additional cost. However, simply adding the\nmemory and CPU utilizations at the time of execution into the\nfeature of predictor is unlikely to provide better prediction because\nthey can vary throughout the execution of a query.\nFurthermore, there exist other environment factors, such as\ncache effect and buffer pool state that are not trivial to featurize in\nStage predictor or any other exec-time predictors. Specifically, the\nrecently accessed pages will be cached which can greatly speed up\nthe following queries touching the same pages.\nWe believe designing exec-time predictors that can accurately\ntake these environment factors into consideration can further im-\nprove the prediction accuracy.\n7 CONCLUSIONS\nWe have presented Stage, a novel hierarchical query performance\npredictor custom-tailored to Amazon Redshift‚Äôs specific require-\nments. The Stage predictor provides fast and robust query per-\nformance predictions by taking advantage of the repetitive na-\nture of analytic workloads, remembering the latency of common,\nfrequently-issued queries, and using two different machine learn-\ning models for similar and novel queries, respectively. We showed\nthat Stage predictor improves the average query latency by 20%\nwhen compared to Redshift‚Äôs prior exec-time predictor. Based on\nthe lessons learned from building Stage predictor, we pointed out a\nlist of research directions that could be fruitful.\n291\n\nStage: Query Execution Time Prediction in Amazon Redshift SIGMOD-Companion ‚Äô24, June 9‚Äì15, 2024, Santiago, AA, Chile\nREFERENCES\n[1]Mert Akdere, Ugur √áetintemel, Matteo Riondato, Eli Upfal, and Stanley B. Zdonik.\n2012. Learning-based Query Performance Modeling and Prediction. In IEEE 28th\nInternational Conference on Data Engineering (ICDE 2012), Washington, DC, USA\n(Arlington, Virginia), 1-5 April, 2012, Anastasios Kementsietsidis and Marcos\nAntonio Vaz Salles (Eds.). IEEE Computer Society, 390‚Äì401. https://doi.org/10.\n1109/ICDE.2012.64\n[2]Dana Van Aken, Andrew Pavlo, Geoffrey J. Gordon, and Bohan Zhang. 2017.\nAutomatic Database Management System Tuning Through Large-scale Machine\nLearning. In Proceedings of the 2017 ACM International Conference on Management\nof Data, SIGMOD Conference 2017, Chicago, IL, USA, May 14-19, 2017, Semih\nSalihoglu, Wenchao Zhou, Rada Chirkova, Jun Yang, and Dan Suciu (Eds.). ACM,\n1009‚Äì1024. https://doi.org/10.1145/3035918.3064029\n[3]Mennatallah Amer, Markus Goldstein, and Slim Abdennadher. 2013. Enhanc-\ning one-class support vector machines for unsupervised anomaly detection. In\nProceedings of the ACM SIGKDD workshop on outlier detection and description.\n8‚Äì15.\n[4]Bahareh Sadat Arab and Boris Glavic. 2017. Answering Historical What-if Queries\nwith Provenance, Reenactment, and Symbolic Execution. In 9th USENIX Workshop\non the Theory and Practice of Provenance, TaPP 2017, Seattle, WA, USA, June 23,\n2017, Adam Bates and Bill Howe (Eds.). USENIX Association. https://www.\nusenix.org/conference/tapp17/workshop-program/presentation/arab\n[5]Nikos Armenatzoglou, Sanuj Basu, Naga Bhanoori, Mengchu Cai, Naresh\nChainani, Kiran Chinta, Venkatraman Govindaraju, Todd J. Green, Monish Gupta,\nSebastian Hillig, Eric Hotinger, Yan Leshinksy, Jintian Liang, Michael McCreedy,\nFabian Nagel, Ippokratis Pandis, Panos Parchas, Rahul Pathak, Orestis Polychro-\nniou, Foyzur Rahman, Gaurav Saxena, Gokul Soundararajan, Sriram Subramanian,\nand Doug Terry. 2022. Amazon Redshift Re-invented. In SIGMOD ‚Äô22: Interna-\ntional Conference on Management of Data, Philadelphia, PA, USA, June 12 - 17, 2022,\nZachary G. Ives, Angela Bonifati, and Amr El Abbadi (Eds.). ACM, 2205‚Äì2217.\nhttps://doi.org/10.1145/3514221.3526045\n[6]Eli Bingham, Jonathan P Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Prad-\nhan, Theofanis Karaletsos, Rohit Singh, Paul Szerlip, Paul Horsfall, and Noah D\nGoodman. 2019. Pyro: Deep universal probabilistic programming. The Journal of\nMachine Learning Research 20, 1 (2019), 973‚Äì978.\n[7]Jing Chen, Tiantian Du, and Gongyi Xiao. 2021. A multi-objective optimization for\nresource allocation of emergent demands in cloud computing. J. Cloud Comput.\n10, 1 (2021), 20. https://doi.org/10.1186/s13677-021-00237-7\n[8]Tianqi Chen and Carlos Guestrin. 2016. XGBoost: A Scalable Tree Boosting\nSystem. In Proceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17,\n2016, Balaji Krishnapuram, Mohak Shah, Alexander J. Smola, Charu C. Aggarwal,\nDou Shen, and Rajeev Rastogi (Eds.). ACM, 785‚Äì794. https://doi.org/10.1145/\n2939672.2939785\n[9]Yun Chi, Hyun Jin Moon, Hakan Hacig√ºm√ºs, and Jun‚Äôichi Tatemura. 2011. SLA-\ntree: a framework for efficiently supporting SLA-based decisions in cloud comput-\ning. In EDBT 2011, 14th International Conference on Extending Database Technology,\nUppsala, Sweden, March 21-24, 2011, Proceedings, Anastasia Ailamaki, Sihem Amer-\nYahia, Jignesh M. Patel, Tore Risch, Pierre Senellart, and Julia Stoyanovich (Eds.).\nACM, 129‚Äì140. https://doi.org/10.1145/1951365.1951383\n[10] John W Coulston, Christine E Blinn, Valerie A Thomas, and Randolph H Wynne.\n2016. Approximating prediction uncertainty for random forest regression models.\nPhotogrammetric Engineering & Remote Sensing 82, 3 (2016), 189‚Äì197.\n[11] Carlo Curino, Yang Zhang, Evan P. C. Jones, and Samuel Madden. 2010. Schism:\na Workload-Driven Approach to Database Replication and Partitioning. Proc.\nVLDB Endow. 3, 1 (2010), 48‚Äì57. https://doi.org/10.14778/1920841.1920853\n[12] Bailu Ding, Sudipto Das, Ryan Marcus, Wentao Wu, Surajit Chaudhuri, and\nVivek R. Narasayya. 2019. AI Meets AI: Leveraging Query Executions to Improve\nIndex Recommendations. In Proceedings of the 2019 International Conference on\nManagement of Data, SIGMOD Conference 2019, Amsterdam, The Netherlands, June\n30 - July 5, 2019, Peter A. Boncz, Stefan Manegold, Anastasia Ailamaki, Amol\nDeshpande, and Tim Kraska (Eds.). ACM, 1241‚Äì1258. https://doi.org/10.1145/\n3299869.3324957\n[13] Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. 2020.\nTsunami: A Learned Multi-dimensional Index for Correlated Data and Skewed\nWorkloads. Proc. VLDB Endow. 14, 2 (2020), 74‚Äì86. https://doi.org/10.14778/\n3425879.3425880\n[14] Tony Duan, Anand Avati, Daisy Yi Ding, Khanh K. Thai, Sanjay Basu, An-\ndrew Y. Ng, and Alejandro Schuler. 2020. NGBoost: Natural Gradient Boost-\ning for Probabilistic Prediction. In Proceedings of the 37th International Con-\nference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event (Pro-\nceedings of Machine Learning Research, Vol. 119). PMLR, 2690‚Äì2700. http:\n//proceedings.mlr.press/v119/duan20a.html\n[15] Jennie Duggan, Olga Papaemmanouil, Ugur √áetintemel, and Eli Upfal. 2014. Con-\ntender: A Resource Modeling Approach for Concurrent Query Performance\nPrediction. In Proceedings of the 17th International Conference on Extending\nDatabase Technology, EDBT 2014, Athens, Greece, March 24-28, 2014, SihemAmer-Yahia, Vassilis Christophides, Anastasios Kementsietsidis, Minos N. Garo-\nfalakis, Stratos Idreos, and Vincent Leroy (Eds.). OpenProceedings.org, 109‚Äì120.\nhttps://doi.org/10.5441/002/EDBT.2014.11\n[16] Yarin Gal and Zoubin Ghahramani. 2016. Dropout as a Bayesian Approximation:\nRepresenting Model Uncertainty in Deep Learning. In Proceedings of the 33nd\nInternational Conference on Machine Learning, ICML 2016, New York City, NY,\nUSA, June 19-24, 2016 (JMLR Workshop and Conference Proceedings, Vol. 48),\nMaria-Florina Balcan and Kilian Q. Weinberger (Eds.). JMLR.org, 1050‚Äì1059.\nhttp://proceedings.mlr.press/v48/gal16.html\n[17] Sainyam Galhotra, Amir Gilad, Sudeepa Roy, and Babak Salimi. 2022. HypeR:\nHypothetical Reasoning With What-If and How-To Queries Using a Probabilistic\nCausal Approach. In SIGMOD ‚Äô22: International Conference on Management of\nData, Philadelphia, PA, USA, June 12 - 17, 2022, Zachary G. Ives, Angela Bonifati,\nand Amr El Abbadi (Eds.). ACM, 1598‚Äì1611. https://doi.org/10.1145/3514221.\n3526149\n[18] Tilmann Gneiting and Matthias Katzfuss. 2014. Probabilistic forecasting. Annual\nReview of Statistics and Its Application 1 (2014), 125‚Äì151.\n[19] Andrew D Gordon, Thomas A Henzinger, Aditya V Nori, and Sriram K Rajamani.\n2014. Probabilistic programming. In Future of Software Engineering Proceedings.\n167‚Äì181.\n[20] Yuxing Han, Ziniu Wu, Peizhi Wu, Rong Zhu, Jingyi Yang, Liang Wei Tan, Kai\nZeng, Gao Cong, Yanzhao Qin, Andreas Pfadler, Zhengping Qian, Jingren Zhou,\nJiangneng Li, and Bin Cui. 2021. Cardinality Estimation in DBMS: A Com-\nprehensive Benchmark Evaluation. Proc. VLDB Endow. 15, 4 (2021), 752‚Äì765.\nhttps://doi.org/10.14778/3503585.3503586\n[21] Benjamin Hilprecht and Carsten Binnig. 2022. Zero-Shot Cost Models for Out-\nof-the-box Learned Cost Prediction. Proc. VLDB Endow. 15, 11 (2022), 2361‚Äì2374.\nhttps://www.vldb.org/pvldb/vol15/p2361-hilprecht.pdf\n[22] Benjamin Hilprecht, Andreas Schmidt, Moritz Kulessa, Alejandro Molina, Kristian\nKersting, and Carsten Binnig. 2020. DeepDB: Learn from Data, not from Queries!\nProc. VLDB Endow. 13, 7 (2020), 992‚Äì1005. https://doi.org/10.14778/3384345.\n3384349\n[23] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2020. RadixSpline: a single-pass learned\nindex. In Proceedings of the Third International Workshop on Exploiting Artificial\nIntelligence Techniques for Data Management, aiDM@SIGMOD 2020, Portland,\nOregon, USA, June 19, 2020, Rajesh Bordawekar, Oded Shmueli, Nesime Tatbul,\nand Tin Kam Ho (Eds.). ACM, 5:1‚Äì5:5. https://doi.org/10.1145/3401071.3401659\n[24] Thomas N. Kipf and Max Welling. 2016. Semi-Supervised Classification with\nGraph Convolutional Networks. (2016). https://doi.org/10.48550/ARXIV.1609.\n02907 Publisher: arXiv Version Number: 4.\n[25] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In Proceedings of the 2018 International\nConference on Management of Data, SIGMOD Conference 2018, Houston, TX, USA,\nJune 10-15, 2018, Gautam Das, Christopher M. Jermaine, and Philip A. Bernstein\n(Eds.). ACM, 489‚Äì504. https://doi.org/10.1145/3183713.3196909\n[26] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. 2017. Simple\nand Scalable Predictive Uncertainty Estimation using Deep Ensembles. In Ad-\nvances in Neural Information Processing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, Is-\nabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus,\nS. V. N. Vishwanathan, and Roman Garnett (Eds.). 6402‚Äì6413. https://proceedings.\nneurips.cc/paper/2017/hash/9ef2ed4b7fd2c810847ffa5fa85bce38-Abstract.html\n[27] Jiexing Li, Arnd Christian K√∂nig, Vivek R. Narasayya, and Surajit Chaudhuri. 2012.\nRobust Estimation of Resource Consumption for SQL Queries using Statistical\nTechniques. Proc. VLDB Endow. 5, 11 (2012), 1555‚Äì1566. https://doi.org/10.14778/\n2350229.2350269\n[28] Kun-Lun Li, Hou-Kuan Huang, Sheng-Feng Tian, and Wei Xu. 2003. Improving\none-class SVM for anomaly detection. In Proceedings of the 2003 international\nconference on machine learning and cybernetics (IEEE Cat. No. 03EX693), Vol. 5.\nIEEE, 3077‚Äì3081.\n[29] Chenghao Lyu, Qi Fan, Fei Song, Arnab Sinha, Yanlei Diao, Wei Chen, Li Ma, Yihui\nFeng, Yaliang Li, Kai Zeng, and Jingren Zhou. 2022. Fine-Grained Modeling and\nOptimization for Intelligent Resource Management in Big Data Processing. Proc.\nVLDB Endow. 15, 11 (2022), 3098‚Äì3111. https://www.vldb.org/pvldb/vol15/p3098-\nlyu.pdf\n[30] Andrey Malinin, Bruno Mlodozeniec, and Mark J. F. Gales. 2020. Ensemble\nDistribution Distillation. In 8th International Conference on Learning Represen-\ntations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.\nhttps://openreview.net/forum?id=BygSP6Vtvr\n[31] Andrey Malinin, Liudmila Prokhorenkova, and Aleksei Ustimenko. 2021. Un-\ncertainty in Gradient Boosting via Ensembles. In 9th International Conference\non Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.\nOpenReview.net. https://openreview.net/forum?id=1Jv6b0Zq3qi\n[32] Ryan Marcus, Parimarjan Negi, Hongzi Mao, Nesime Tatbul, Mohammad Al-\nizadeh, and Tim Kraska. 2021. Bao: Making Learned Query Optimization Practical.\nInProceedings of the 2021 International Conference on Management of Data (SIG-\nMOD ‚Äô21). China. https://doi.org/10.1145/3448016.3452838 Award: ‚Äôbest paper\n292\n\nSIGMOD-Companion ‚Äô24, June 9‚Äì15, 2024, Santiago, AA, Chile Ziniu Wu et al.\naward‚Äô.\n[33] Ryan Marcus, Parimarjan Negi, Hongzi Mao, Chi Zhang, Mohammad Alizadeh,\nTim Kraska, Olga Papaemmanouil, and Nesime Tatbul. 2019. Neo: A Learned\nQuery Optimizer. PVLDB 12, 11 (2019), 1705‚Äì1718.\n[34] Ryan Marcus and Olga Papaemmanouil. 2016. WiSeDB: A Learning-based Work-\nload Management Advisor for Cloud Databases. Proc. VLDB Endow. 9, 10 (2016),\n780‚Äì791. https://doi.org/10.14778/2977797.2977804\n[35] Ryan Marcus and Olga Papaemmanouil. 2019. Plan-Structured Deep Neural\nNetwork Models for Query Performance Prediction. Proc. VLDB Endow. 12, 11\n(2019), 1733‚Äì1746. https://doi.org/10.14778/3342263.3342646\n[36] Nicolai Meinshausen. 2006. Quantile Regression Forests. J. Mach. Learn. Res. 7\n(2006), 983‚Äì999. http://jmlr.org/papers/v7/meinshausen06a.html\n[37] Alexandra Meliou, Wolfgang Gatterbauer, Katherine F. Moore, and Dan Suciu.\n2010. WHY SO? or WHY NO? Functional Causality for Explaining Query Answers.\nInProceedings of the Fourth International VLDB workshop on Management of\nUncertain Data (MUD 2010) in conjunction with VLDB 2010, Singapore, September\n13, 2010 (CTIT Workshop Proceedings Series, Vol. WP10-04), Ander de Keijzer and\nMaurice van Keulen (Eds.). Centre for Telematics and Information Technology\n(CTIT), University of Twente, The Netherlands, 3‚Äì17. http://ewi1276.ewi.utwente.\nnl:3000/papers/MUD2010_whyso.pdf\n[38] Alexandra Meliou and Dan Suciu. 2012. Tiresias: the database oracle for how-to\nqueries. In Proceedings of the ACM SIGMOD International Conference on Man-\nagement of Data, SIGMOD 2012, Scottsdale, AZ, USA, May 20-24, 2012, K. Sel√ßuk\nCandan, Yi Chen, Richard T. Snodgrass, Luis Gravano, and Ariel Fuxman (Eds.).\nACM, 337‚Äì348. https://doi.org/10.1145/2213836.2213875\n[39] Lucas Mentch and Giles Hooker. 2016. Quantifying Uncertainty in Random\nForests via Confidence Intervals and Hypothesis Tests. J. Mach. Learn. Res. 17\n(2016), 26:1‚Äì26:41. http://jmlr.org/papers/v17/14-168.html\n[40] Guido Moerkotte, Thomas Neumann, and Gabriele Steidl. 2009. Preventing Bad\nPlans by Bounding the Impact of Cardinality Estimation Errors. PVLDB 2, 1\n(2009), 982‚Äì993. https://doi.org/10.14778/1687627.1687738\n[41] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020. Learn-\ning Multi-Dimensional Indexes. In Proceedings of the 2020 International Conference\non Management of Data, SIGMOD Conference 2020, online conference [Portland,\nOR, USA], June 14-19, 2020, David Maier, Rachel Pottinger, AnHai Doan, Wang-\nChiew Tan, Abdussalam Alawini, and Hung Q. Ngo (Eds.). ACM, 985‚Äì1000.\nhttps://doi.org/10.1145/3318464.3380579\n[42] Parimarjan Negi, Ziniu Wu, Andreas Kipf, Nesime Tatbul, Ryan Marcus, Sam\nMadden, Tim Kraska, and Mohammad Alizadeh. 2023. Robust Query Driven\nCardinality Estimation under Changing Workloads. Proc. VLDB Endow. 16, 6\n(2023), 1520‚Äì1533. https://doi.org/10.14778/3583140.3583164\n[43] Susana Nieva, Fernando S√°enz-P√©rez, and Jaime S√°nchez-Hern√°ndez. 2020. HR-\nSQL: Extending SQL with hypothetical reasoning and improved recursion for\ncurrent database systems. Inf. Comput. 271 (2020), 104485. https://doi.org/10.\n1016/J.IC.2019.104485\n[44] Jakub Nowotarski and Rafa≈Ç Weron. 2018. Recent advances in electricity price\nforecasting: A review of probabilistic forecasting. Renewable and Sustainable\nEnergy Reviews 81 (2018), 1548‚Äì1568.\n[45] Adeola Ogunleye and Qing-Guo Wang. 2020. XGBoost Model for Chronic Kidney\nDisease Diagnosis. IEEE ACM Trans. Comput. Biol. Bioinform. 17, 6 (2020), 2131‚Äì\n2140. https://doi.org/10.1109/TCBB.2019.2911071\n[46] Jignesh M. Patel, Harshad Deshmukh, Jianqiao Zhu, Navneet Potti, Zuyu Zhang,\nMarc Spehlmann, Hakan Memisoglu, and Saket Saurabh. 2018. Quickstep: A\nData Platform Based on the Scaling-Up Approach. Proc. VLDB Endow. 11, 6 (2018),\n663‚Äì676. https://doi.org/10.14778/3184470.3184471\n[47] Fabian Pedregosa, Ga√´l Varoquaux, Alexandre Gramfort, Vincent Michel,\nBertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron\nWeiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau,\nMatthieu Brucher, Matthieu Perrot, and √âdouard Duchesnay. 2011. Scikit-learn:\nMachine Learning in Python. J. Mach. Learn. Res. 12 (Nov. 2011), 2825‚Äì2830.\nhttp://dl.acm.org/citation.cfm?id=1953048.2078195\n[48] Liudmila Ostroumova Prokhorenkova, Gleb Gusev, Aleksandr Vorobev,\nAnna Veronika Dorogush, and Andrey Gulin. 2018. CatBoost: unbiased boost-\ning with categorical features. In Advances in Neural Information Processing\nSystems 31: Annual Conference on Neural Information Processing Systems 2018,\nNeurIPS 2018, December 3-8, 2018, Montr√©al, Canada, Samy Bengio, Hanna M.\nWallach, Hugo Larochelle, Kristen Grauman, Nicol√≤ Cesa-Bianchi, and Roman\nGarnett (Eds.). 6639‚Äì6649. https://proceedings.neurips.cc/paper/2018/hash/\n14491b756b3a51daac41c24863285549-Abstract.html\n[49] Harry V Roberts. 1965. Probabilistic prediction. J. Amer. Statist. Assoc. 60, 309\n(1965), 50‚Äì62.\n[50] Gaurav Saxena, Mohammad Rahman, Naresh Chainani, Chunbin Lin, George\nCaragea, Fahim Chowdhury, Ryan Marcus, Tim Kraska, Ippokratis Pandis, and\nBalakrishnan (Murali) Narayanaswamy. 2023. Auto-WLM: Machine Learning\nEnhanced Workload Management in Amazon Redshift. In Companion of the 2023\nInternational Conference on Management of Data, SIGMOD/PODS 2023, Seattle,\nWA, USA, June 18-23, 2023 , Sudipto Das, Ippokratis Pandis, K. Sel√ßuk Candan,and Sihem Amer-Yahia (Eds.). ACM, 225‚Äì237. https://doi.org/10.1145/3555041.\n3589677\n[51] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele\nMonfardini. 2008. The graph neural network model. IEEE transactions on neural\nnetworks 20, 1 (2008), 61‚Äì80.\n[52] Ji Sun and Guoliang Li. 2019. An End-to-End Learning-based Cost Estimator. Proc.\nVLDB Endow. 13, 3 (2019), 307‚Äì319. https://doi.org/10.14778/3368289.3368296\n[53] Rebecca Taft, Willis Lang, Jennie Duggan, Aaron J. Elmore, Michael Stone-\nbraker, and David J. DeWitt. 2016. STeP: Scalable Tenant Placement for Man-\naging Database-as-a-Service Deployments. In Proceedings of the Seventh ACM\nSymposium on Cloud Computing, Santa Clara, CA, USA, October 5-7, 2016, Mar-\ncos K. Aguilera, Brian Cooper, and Yanlei Diao (Eds.). ACM, 388‚Äì400. https:\n//doi.org/10.1145/2987550.2987575\n[54] Laurent Torlay, Marcela Perrone-Bertolotti, Elizabeth Thomas, and Monica Baciu.\n2017. Machine learning-XGBoost analysis of language networks to classify\npatients with epilepsy. Brain Informatics 4, 3 (2017), 159‚Äì169. https://doi.org/10.\n1007/S40708-017-0065-7\n[55] Sean Tozer, Tim Brecht, and Ashraf Aboulnaga. 2010. Q-Cop: Avoiding bad query\nmixes to minimize client timeouts under heavy loads. In Proceedings of the 26th\nInternational Conference on Data Engineering, ICDE 2010, March 1-6, 2010, Long\nBeach, California, USA, Feifei Li, Mirella M. Moro, Shahram Ghandeharizadeh,\nJayant R. Haritsa, Gerhard Weikum, Michael J. Carey, Fabio Casati, Edward Y.\nChang, Ioana Manolescu, Sharad Mehrotra, Umeshwar Dayal, and Vassilis J.\nTsotras (Eds.). IEEE Computer Society, 397‚Äì408. https://doi.org/10.1109/ICDE.\n2010.5447850\n[56] Immanuel Trummer, Samuel Moseley, Deepak Maram, Saehan Jo, and Joseph\nAntonakakis. 2018. SkinnerDB: Regret-bounded Query Evaluation via Rein-\nforcement Learning. PVLDB 11, 12 (2018), 2074‚Äì2077. https://doi.org/10.14778/\n3229863.3236263\n[57] Benjamin Wagner, Andr√© Kohn, and Thomas Neumann. 2021. Self-Tuning Query\nScheduling for Analytical Workloads. In SIGMOD ‚Äô21: International Conference\non Management of Data, Virtual Event, China, June 20-25, 2021 , Guoliang Li,\nZhanhuai Li, Stratos Idreos, and Divesh Srivastava (Eds.). ACM, 1879‚Äì1891. https:\n//doi.org/10.1145/3448016.3457260\n[58] B. P. Welford. 1962. Note on a Method for Calculating Cor-\nrected Sums of Squares and Products. Technometrics 4, 3\n(1962), 419‚Äì420. https://doi.org/10.1080/00401706.1962.10490022\narXiv:https://www.tandfonline.com/doi/pdf/10.1080/00401706.1962.10490022\n[59] Andrew Gordon Wilson and Pavel Izmailov. 2020. Bayesian Deep Learn-\ning and a Probabilistic Perspective of Generalization. In Advances in Neu-\nral Information Processing Systems 33: Annual Conference on Neural Infor-\nmation Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual,\nHugo Larochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,\nand Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/paper/2020/hash/\n322f62469c5e3c7dc3e58f5a4d1ea399-Abstract.html\n[60] Lucas Woltmann, Jerome Thiessat, Claudio Hartmann, Dirk Habich, and Wolf-\ngang Lehner. 2023. FASTgres: Making Learned Query Optimizer Hinting Ef-\nfective. Proceedings of the VLDB Endowment 16, 11 (Aug. 2023), 3310‚Äì3322.\nhttps://doi.org/10.14778/3611479.3611528\n[61] Fei Wu, Xiao-Yuan Jing, Pengfei Wei, Chao Lan, Yimu Ji, Guo-Ping Jiang, and\nQinghua Huang. 2022. Semi-supervised multi-view graph convolutional networks\nwith application to webpage classification. Inf. Sci. 591 (2022), 142‚Äì154. https:\n//doi.org/10.1016/J.INS.2022.01.013\n[62] Wentao Wu, Yun Chi, Shenghuo Zhu, Jun‚Äôichi Tatemura, Hakan Hacig√ºm√ºs,\nand Jeffrey F. Naughton. 2013. Predicting query execution time: Are optimizer\ncost models really unusable?. In 29th IEEE International Conference on Data\nEngineering, ICDE 2013, Brisbane, Australia, April 8-12, 2013, Christian S. Jensen,\nChristopher M. Jermaine, and Xiaofang Zhou (Eds.). IEEE Computer Society,\n1081‚Äì1092. https://doi.org/10.1109/ICDE.2013.6544899\n[63] Ziniu Wu, Parimarjan Negi, Mohammad Alizadeh, Tim Kraska, and Samuel\nMadden. 2023. FactorJoin: A New Cardinality Estimation Framework for Join\nQueries. Proc. ACM Manag. Data 1, 1 (2023), 41:1‚Äì41:27. https://doi.org/10.1145/\n3588721\n[64] Zongheng Yang, Wei-Lin Chiang, Sifei Luan, Gautam Mittal, Michael Luo, and\nIon Stoica. 2022. Balsa: Learning a Query Optimizer Without Expert Demon-\nstrations. In Proceedings of the 2022 International Conference on Management of\nData (SIGMOD ‚Äô22). Association for Computing Machinery, New York, NY, USA,\n931‚Äì944. https://doi.org/10.1145/3514221.3517885\n[65] Zongheng Yang, Amog Kamsetty, Sifei Luan, Eric Liang, Yan Duan, Xi Chen, and\nIon Stoica. 2020. NeuroCard: One Cardinality Estimator for All Tables. Proc.\nVLDB Endow. 14, 1 (2020), 61‚Äì73. https://doi.org/10.14778/3421424.3421432\n[66] Chi Zhang, Ryan Marcus, Anat Kleiman, and Olga Papaemmanoui l. 2020. Buffer\nPool Aware Query Scheduling via Deep Reinforc ement Learning. In 2nd In-\nternational Workshop on Applied AI for Datab ase Systems and Applications\n(AIDB@VLDB ‚Äô20), Bingsheng He, Berthold Reinwald, and Yingjun Wu (Eds.).\nTokyo, Japan. https://drive.google.com/file/d/1trNYAcQ3S71SHu5dbtkBR2hjcK-\nVWFSx/view?usp=sharing\n293\n\nStage: Query Execution Time Prediction in Amazon Redshift SIGMOD-Companion ‚Äô24, June 9‚Äì15, 2024, Santiago, AA, Chile\n[67] Dahai Zhang, Liyang Qian, Baijin Mao, Can Huang, Bin Huang, and Yulin Si.\n2018. A Data-Driven Design for Fault Detection of Wind Turbines Using Random\nForests and XGboost. IEEE Access 6 (2018), 21020‚Äì21031. https://doi.org/10.1109/\nACCESS.2018.2818678\n[68] Xuanhe Zhou, Ji Sun, Guoliang Li, and Jianhua Feng. 2020. Query performance\nprediction for concurrent queries using graph embedding. Proceedings of theVLDB Endowment 13, 9 (2020), 1416‚Äì1428.\n[69] Rong Zhu, Ziniu Wu, Yuxing Han, Kai Zeng, Andreas Pfadler, Zhengping Qian,\nJingren Zhou, and Bin Cui. 2021. FLAT: Fast, Lightweight and Accurate Method\nfor Cardinality Estimation. Proc. VLDB Endow. 14, 9 (2021), 1489‚Äì1502. https:\n//doi.org/10.14778/3461535.3461539\n294",
  "textLength": 97740
}