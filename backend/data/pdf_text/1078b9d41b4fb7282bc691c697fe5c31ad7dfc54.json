{
  "paperId": "1078b9d41b4fb7282bc691c697fe5c31ad7dfc54",
  "title": "LearnedFTL: A Learning-Based Page-Level FTL for Reducing Double Reads in Flash-Based SSDs",
  "pdfPath": "1078b9d41b4fb7282bc691c697fe5c31ad7dfc54.pdf",
  "text": "2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA)\nLearnedFTL: A Learning-based Page-level FTL for\nReducing Double Reads in Flash-based SSDs\nShengzhe Wang†, Zihang Lin†, Suzhen Wu†, Hong Jiang‡, Jie Zhang∗and Bo Mao†\n†Xiamen Key Laboratory of Intelligent Storage and Computing, School of Informatics,\nXiamen University, Xiamen, Fujian, China\n‡Computer Science and Engineering Department, University of Texas at Arlington, Arlington, TX, USA\n∗School of Computer Science, Peking University, Beijing, China\nCorresponding Author: Suzhen Wu (suzhen@xmu.edu.cn) and Bo Mao (maobo@xmu.edu.cn)\nAbstract —We present LearnedFTL, a new on-demand page-\nlevel flash translation layer (FTL) design, which employs learned\nindexes to improve the address translation efficiency of flash-\nbased SSDs. The first of its kind, it reduces the number of double\nreads induced by address translation in random read accesses.\nLearnedFTL proposes three key techniques: an in-place-update\nlinear model to build learned indexes efficiently, a virtual PPN\nrepresentation to obtain contiguous PPNs for sorted LPNs, and\na group-based allocation and model training via GC/rewrite\nstrategy to reduce the training overhead. By tightly integrating\nthe aforementioned key techniques, LearnedFTL considerably\nspeeds up address translation while reducing the number of flash\nread accesses caused by the address translation. Our extensive\nexperiments on a FEMU-based prototype show that LearnedFTL\ncan reduce up to 55.5% address translation-induced double reads.\nAs a result, LearnedFTL reduces the P99 tail latency by 2.9 ×\n∼12.2×with an average of 5.5 ×and 8.2 ×compared to the\nstate-of-the-art TPFTL and LeaFTL schemes, respectively.\nI. I NTRODUCTION\nWith the emergence of 3D NAND and NVMe tech-\nniques [11], [27], [29], the capacity and performance of flash-\nbased SSDs have increased significantly. Accordingly, the\nmapping table size for those using page-level Flash Translation\nLayer (FTL) cannot be practically deployed in SSDs since\nit consumes a prohibitively large memory. Moreover, storing\nthe mapping table in SSDs causes the double-read problem\nof constantly accessing the flash for the address translation,\nwhich directly degrades the read performance of SSDs.\nTo solve this problem, the mainstream demand-based FTL\n(DFTL) proposes a mapping cache method [4], [10], [41].\nSpecifically, to reduce the memory overhead of the page-level\nmapping table, DFTL exploits the locality of workloads and\nsets a small-sized mapping cache in SSD internal memory\nto cache frequently accessed mappings. If an LPA cannot be\nfound in this mapping cache, DFTL needs an extra flash read\nto fetch the corresponding PPA, followed by another flash\nread to access the data. This phenomenon is referred to as\ndouble reads . Since the mapping cache can capture locality\nwell, DFTL generates only a small number of double reads\nunder workloads with high locality, which in turn has minor\nimpacts on SSD performance.\nHowever, the double-read problem in DFTL becomes very\nsevere under random reads. Since there is no obvious data\nlocality in random reads, and the LPNs of continuous requestsmay separate far away, it is difficult for the mapping cache to\nbuffer the mappings in need. As a result, almost all read requests\nrequire double reads [10], [41], which incurs poor random\nread performance as also validated in the subsection II-A.\nUnfortunately, in many SSD-based modern applications, the\nproportion of random access is gradually increasing and even\noccupies a dominant position. The ability to handle random\naccess becomes critical for FTL.\nHow to index the most mappings in cache-sized DRAM\nmemory is the key to increasing the hit rate of random reads.\nRecent studies on learned index [6], [28], [34], [37] have\ndemonstrated the feasibility of realizing this goal. Learned\nindex builds machine learning models based on the key-position\nmappings. With these models, the learned index can index\nhundreds of mappings with a few parameters. Ideally, by\nadopting the learned index to all LPN-PPN mappings, one\ncan calculate the PPN of an LPN directly from learned models\nwithout double reads.\nUnfortunately, the limitations of the learned index pose\nseveral challenges. First, the accuracy of the learned index\ncannot reach 100%. When the PPN prediction is incorrect, the\nneighboring flash pages to the predicted PPN need to be probed\nin order to identify the correct PPN. However, this process\nresults in degraded performance. Second, learned indexes\nrequire assigning consecutive PPNs to consecutive LPNs, which\nconflicts with the internal access parallelism of SSDs. Third, the\ncomplex model training of learned indexes will cause additional\nperformance and space overheads. Specifically, training a\nlearned index needs multiple time-consuming computational\noperations on the critical write path. Moreover, training random\nwrite requests requires more space to store the learning models.\nTo tackle these challenges, we propose LearnedFTL, a\nlearning-based page-level FTL. LearnedFTL combines the\nlearned indexes with the existing demand-based page-level\nFTL scheme, TPFTL [41], where TPFTL handles the locality\nworkloads, and learned indexes handle the random workloads.\nEach learned index in LearnedFTL is a piece-wise linear\nmodel with adjustable parameters, called the in-place-update\nlinear model. To ensure the accuracy of the model prediction,\nLearnedFTL equips each in-place-update linear model with a\nbitmap filter where each bit indicates whether the prediction\nof an LPN is accurate. To obtain contiguous PPNs for sortedarXiv:2303.13226v2  [cs.AR]  25 Apr 2024\n\nCaches PART of the Models\nTransPage:225\nlog-structured\nlearned segmentsL0\nL1\nL2GTD\nLPN\n0-255\n256-511\n…TPPN\n225\n…223Model Cache\nLPN Segment\n0-255\n… …4096 -4607 ModelModel\nTransPage:225\nlog-structured\nmapping tableL0\nL1\nL2OOBData Page\nPPN:107\nError IntervalData Page\nPPN:106\nOOB\nCRBCMT\nLPN PPN\n15 104\n1005 127\n… …\nLPN LPN\n0 127\n1 130TransPage:225\nLPN PPN\n1005 127\n… …DRAM\nFLASH\nData Page\nPPN:107Data Page\nPPN:107Data Page\nPPN:127GTD\nLPN\n224\n225\n… …512-1023Caches PART of the Mappings\nTPPN\n0-511Caches PART of the Mappings\nGTD stores ALL Models\nCMT\nLPN PPN\n15 104\n1005 107\n… …LPN Model\n… …TPPN\n…GTD\n0-511\n511-1023 225223\nData Page\nPPN:107Data Page\nPPN:127LPN LPN\n0 127\n……LPN PPN\n0 127\n……TransPage:225\n(a) DFTL/TPFTL (b)LeaFTL (c)LearnedFTLModel Hits Eliminate  Double Reads Model Hits Still Trigger Double Reads①Always Accurate PPN①\n②Double ReadAccurate ModelApproximate Model①Inaccurate PPN\n②Double ReadAccurate Model\n* 1 Model PerTransPage\n** Only exists in GTDModel Replacement\nModel TrainingFig. 1: Representative mapping schemes in flash-based SSDs.\nLPNs with SSD’s internal parallelism, LearnedFTL proposes a\nvirtual PPN representation to convert the incontiguous PPNs\nto sequential ones. To reduce the space overhead of model\ntraining under random writes, a group-based allocation strategy\nis proposed to replace the current dynamic allocation strategy.\nLastly, LearnedFTL proposes two model training strategies to\nminimize the performance overhead: first, initializing the model\nbased on sequential write requests, and second, training the\nmodel through garbage collection. Overall, this paper makes\nthe following contributions:\n•Our experimental analysis of the random reads in flash-\nbased SSDs reveals that the address-translation-induced\ndouble-read accesses to flash storage are the root cause\nof SSD’s poor random-read performance.\n•We propose a novel FTL design, LearnedFTL, to im-\nprove the read performance of SSDs. The innovation\nof LearnedFTL lies in its ability to effectively combine\nLearned Index and address mapping in FTL that maxi-\nmizes performance while minimizing modifications. By\nincorporating learned indexes into the currently popular\ndemand-based FTL, LearnedFTL can enhance random\nread performance without compromising the ability to\nhandle workloads with locality. To the best of our\nknowledge, LearnedFTL is the first FTL design that\noptimizes for random read performance.\n•We propose several optimizations in LearnedFTL to\nfacilitate the learned indexes, including an in-place-update\nlinear model equipped with a bitmap filter to guarantee the\naccuracy of predictions, a virtual PPN representation to\nconvert the unordered PPNs from different parallel units\ninto contiguous ones, a group-based allocation strategy and\ntwo model training strategies to reduce training overheads.\n•We implement the prototype of LearnedFTL on the SSD\nemulator FEMU [23]. The extensive evaluations validate\nthe efficacy of LearnedFTL over the state-of-the-art FTLs.II. B ACKGROUND AND MOTIVATION\nA. Demand-based Page-Level FTLs\nAddress translation is a vital function in FTL, which searches\nthe physical addresses of flash memory for incoming requests.\nThere exist several mapping schemes such as page-level\nmapping, block-level mapping, and hybrid mapping [1], [21],\n[22]. Since the flash page is the basic unit for read/write\noperations, page-level mapping can handle requests flexibly\nand performs well. However, this fine-grained mapping scheme\nrequires a huge DRAM memory to accommodate its mapping\ntable. For example, suppose an SSD has a 10TB capacity with a\n4KB page size, and each entry of the LPN-PPN mapping is 8B,\nthe SSD requires 20GB of DRAM memory to store the LPN-\nPPN mapping of 2.5 billion entries. This huge DRAM memory\nconsumption, unfortunately, is impractical for enterprise SSDs.\nBlock-level mapping [16] and hybrid mapping [21], [22]\naddressed the space issue by compressing the mapping table.\nIn these schemes, the address mappings are organized at the\ngranularity of a flash block, leading to a significantly lower\nmapping space overhead. However, block-level mapping has a\nlimitation, that is, data stored in a block must have contiguous\nLPNs. Flash pages can only be written to a fixed location in\nthe flash block. Consequently, these mapping schemes exhibit\npoor writing performance.\nTo strike a good balance between write performance and\nDRAM memory, demand-based page-level FTLs (DFTL) [10]\nis proposed. Specifically, DFTL uses a selective cache solu-\ntion to only buffer frequently accessed mappings into SSD\nmemory to exploit workloads’ temporal locality, thus reducing\nmemory usage without compromising performance. Figure 1(a)\nillustrates the general structure of DFTL. It stores the whole\nmapping table in multiple flash pages, called translation\npages . DFTL contains two data structures in SSD memory.\nCached Mapping Table (CMT) stores mapping information\nfor frequently accessed flash pages while Global Translation\nDirectory (GTD) records the physical location of translation\n\n0400800120016002000\n1 16 32 64Bandwidth (MB/s)\nThreads图表标题\nseqread\nrandread(a) Read throughput\n020406080100\n1 16 32 64CMT Hit Ratio (%)\nThreads图表标题\nSeqRead RandRead\n0.1 1.9 5.2425.9\n020406080100\n3 10 30 50CMT Hit Ratio (%)\nCMT space ratio (%)RandRead\nRandRead (b) CMT hit ratio\nFig. 2: The performance of a FEMU-emulated SSD under\nsequential reads and random reads.\npages in flash memory. For requests that miss from CMT, DFTL\nimposes a high miss penalty. In particular, the SSD controller\nneeds an additional flash read to fetch the missing mapping\nfrom the translation page. A read request may generate two\nflash reads for data and metadata, which is called double reads .\nSeveral demand-based page-level FTLs have been proposed\nto address double reads by exploiting workload locality\ncharacteristics. Examples include TPFTL [41], HCFTL [5],\nand ZFTL [36]. Among them, TPFTL is a well-known FTL\nthat utilizes both temporal and spatial locality. It proposes\na workload-adaptive loading policy to prefetch mappings to\nCMT based on the request length. This approach improves the\nhit ratio of CMT and significantly alleviates double reads in\nworkloads with a strong locality.\nB. Performance Impact of Double Reads\nDespite the effectiveness of demand-based page-level FTLs\nin reducing memory usage, their efficiency is limited to\nworkloads with high locality. This becomes problematic in\nsome modern applications with random accesses. Therefore, the\nability of FTL to solve double reads under random workloads\nis of great importance.\nTo investigate whether demand-based FTLs can handle\nrandom workloads, we evaluate the sequential and random\nread performance of TPFTL driven by FIO [14] stress testing\ntool. As shown in Figure 2(a), regardless of the variation\nin the number of threads, the performance of random reads\nconsistently falls short compared to sequential reads (i.e., up to\n60% degradation). Figure 2(b) shows the CMT hit ratio under\ndifferent threads. Under random reads, although TPFTL adopts\na prefetching strategy, it can only predictively prefetch PPNs\nnear one PPN. However, the two following requests in random\nreads may be far apart. As a result, the prefetching strategy\nbecomes ineffective, incurring a very low CMT hit ratio.\nIncreasing the size of the CMT is a straightforward solution\nto improve the random read performance. However, this\napproach remains ineffective due to cache contention. Figure 3\nillustrates the changes in the CMT hit ratio of TPFTL when\nincreasing CMT space. Even when the CMT space expands to\n50% of the total page mappings, the hit ratio only improves\nslightly to 25.9%. It is clear that contention for the CMT will\nexist unless the CMT can accommodate the majority of the\n0.01 0.1 1.9 5.2425.9\n0255075100\n0.1 3 10 30 50CMT hit ratio (%)\nCMT space ratio (%)RandRead SeqReadFig. 3: The hit ratio of TPFTL under different CMT space.\nkey (sorted)CDFpos (sorted)\nkey (sorted)CDFpos (sorted)\nerror band\nkey (sorted)CDFpos (sorted)\nModel step ① step ②\nFig. 4: The workflow of the learned index.\nmappings. Consequently, regardless of the practical capacity of\nthe CMT, the prefetched mappings will be frequently replaced,\nleading to a low CMT hit ratio.\nThe above experiments and analysis demonstrate that the\nselective cache solution of demand-based FTL cannot handle\nrandom reads. Since a random read request may access any\nLPN in the entire address space, an efficient solution is to place\nas many mappings as possible in the small capacity of SSD\nmemory. The mapping table compression scheme mentioned\nearlier is the only approach that meets these criteria. However,\nthis approach adversely impacts SSD write performance. Thus,\nto improve the random read performance, we need a new\nsolution to compress the mapping table without degrading SSD\nwrite performance.\nC. Learned Index and LeaFTL\nLearned Index. Recent studies on learned index [6], [7], [8],\n[25], [26], [34] have demonstrated its potential for compressing\nthe mapping table since it has a high compression rate and\nlow write limit. The learned index only requires the LPN-PPN\nmappings to be ordered and builds lightweight models for\nkey-position mappings. A model with several parameters can\ncalculate hundreds of data locations, thus reducing memory\nconsumption. Figure 4 illustrates the workflow of the learned\nindex. Building a learned index model only requires two steps:\ntraining an approximate model (usually linear models) over\nthe key-position mappings and identifying the maximum error\nbetween the fitted model and the actual values. With the simple\nmodel and maximum error, the needed value can be found in\nthe error interval [ y−error ,y+error ], where yis the predicted\nposition by the approximate model.\nLeaFTL [33]. Ideally, if the learned index could completely\nreplace the existing mapping table structure and index all\nmappings in memory, the double-read problem could be\nsolved. Recently, LeaFTL has taken this approach. The primary\nmotivation behind LeaFTL is to utilize the learned indexes to\n\nTransPage:225\nL0\nL1\nL2\nData Page\nPPN:107Data Page\nPPN: 106\nOOBModel Cache\nLPN\n0-256②model read\n③data accesscache miss\nDRAM FLASHSegment\n①translation readFig. 5: The workflow of triple reads in LeaFTL [33].\nreplace the current mapping table, thereby reducing the DRAM\nmemory overhead to store the mapping table.\nFigure 1(b) illustrates the structure of LeaFTL. LeaFTL\nuses a learned segment design for learned indexes, and each\nlearned segment has four parameters [S, K, L, I] , expressed as\na model PPN =LPN∗K+I,LPN∈[S,S+L]. In LeaFTL’s\nconfiguration, one learned segment can index up to 256\nmappings. For learned segments that are not 100% accurate\n(denoted as approximate segments), LeaFTL conceals the error\ninterval in the Out Of Band (OOB) area of each flash page.\nWhen the model predicts a wrong PPN, LeaFTL reads the\nerror interval from the OOB of the mispredicted flash page\nand finds the correct PPN, then LeaFTL can read the correct\nPPN to access data. With this approach, each misprediction\nrequires 2 flash reads.\nSince the learned index cannot be updated unless retrained,\nLeaFTL adopts the idea of a Log-Structured Merge-tree to\nensure the timeliness of the learned segments. LeaFTL allocates\na small area in SSD internal memory, called data buffer , to\nbuffer newly written data (up to 2048 pages). When the data\nbuffer is full, LeaFTL sorts all data by their LPNs and then\nwrites them to flash pages of continuous PPNs. After that,\nLeaFTL groups these mappings according to the translation\npages they belong, and each group trains a newly learned\nsegment. Then all the learned segments are flushed to the\ncorresponding translation pages. In each translation page, the\nlearned segments are organized in a log-structured mapping\ntable (LSMT). The newly created segment is inserted into\nthe top layer. If one layer has overlapped segment, LeaFTL\nwill migrate the old segment to the next layer. Since the log-\nstructured design brings space amplification (In our evaluations,\nLSMT can only reduce the space to 10%-15% of the original\nmapping table, which is still too large to be fully stored in\nmemory), LeaFTL continues to use the idea of CMT and only\ncaches the most frequently used learned segments into memory.\nD. Challenges in Learned Indexes/LeaFTL\nWhile LeaFTL can reduce the size of the mapping table\nby several times, it, unfortunately, fails in improving the read\nperformance. We take an in-depth analysis and observe multiple\nkey challenges in Learned Index/LeaFTL.\nChallenge #1: Accuracy of learned indexes. The accuracy\nof learned indexes directly determines the efficiency of ad-\ndress translation. LeaFTL is a purely learned index based\naddress translation scheme and replaces the mapping cache\n0.7121\n00.20.40.60.811.2\nLeaFTL TPFTLNormalized Throughputfio-randr\n52%43%\n0%50%100%\nLeaFTL图表标题\ntriple double single(a) Random read\n0.83121\n00.20.40.60.811.2\nLeaFTL TPFTLNormalized Throughputfio-randr\n52%43%\n0%50%100%\nLeaFTL图表标题\ntriple double single (b) Multi-read count statistics\nFig. 6: The performance results under random reads.\nof DFTL/TPFTL with a model cache. Thus, mispredictions\nof learned indexes will bring double reads (one for error\ninterval in OOB and one for data) in LeaFTL. LeaFTL uses a\nlinear regression model [25], [28], [34] that can only express\nPPN=LPN*K+B . As a result, if the LPN-PPNs in the model\nbuffer are not linear, part of the requests may experience double\nreads.\nMoreover, LeaFTL even causes triple reads owing to its\nmodel cache design. Figure 5 illustrates the workflow of triple\nreads in LeaFTL. When an LPN fails to hit any model in\nthe model cache, it initiates a translation read to find the\ncorresponding model from NAND flash. However, as the model\nin LeaFTL is an approximate one, the predicted PPN may be\nwrong. After sending a second flash read to access the wrong\nflash page, this request has to find the correct PPN via the error\ninterval stored in OOB. Finally, this request reads the correct\nPPN to access data with a third flash read. The workflow of\ntriple reads indicates that the miss penalty in LeaFTL is much\nhigher than double reads in DFTL.\nConsidering the fact that the accuracy of learned indexes\ncannot reach 100%, the problems of double reads and triple\nreads will have a significant impact on the performance of\nLeaFTL. Figure 6(a) illustrates the normalized throughput of\nTPFTL and LeaFTL under FIO [14] random reads. LeaFTL\nexhibits a 29% lower throughput compared to TPFTL. Fig-\nure 6(b) shows the fraction of double reads and triple reads\nduring random reads. Triple reads and double reads account\nfor 43% and 52%, respectively. These results demonstrate that\nthe double reads and triple reads make LeaFTL completely\nunable to handle random reads.\nBesides affecting random workload, double and triple reads\nalso have a negative effect on workloads with high locality.\nFigure 7(a) shows the performance comparison between\nTPFTL and LeaFTL under three Filebench [9] workloads. The\nperformance of LeaFTL is equal to or even worse than that of\nTPFTL. Figure 7(b) shows the cache and model hit ratio under\nwebserver workload (read-intensive). In this context, the cache\nhit ratio of LeaFTL simply indicates that the model cache\ncontains the corresponding model for the queried LPN, and it\ndoes not mean that the correct PPN has been calculated. Due\nto the space efficiency of learned indexes, the corresponding\nmodel of an LPN can be easily found in the model cache,\nmaking the model cache hit ratio high. However, there are\n\n00.511.5\nfileserver webserver varmailNormalized Perf.图表标题\nLeaFTL\nTPFTL\n020406080100\nLeaFTL TPFTLHit Ratio (%)CMT model(a) Throughput\n00.511.5\nfileserver webserver varmailNormalized Perf.图表标题\nLeaFTL\nTPFTL\n020406080100\nLeaFTL TPFTLHit Ratio (%)cache model (b) Hit ratio in webserver\nFig. 7: The performance of TPFTL and LeaFTL under\nworkloads with high locality.\ninstances where models experience mispredictions, leading\nto a significant number of requests requiring double reads.\nConsequently, the proportion of LPNs that are successfully hit\nin the model cache and accurately predicted by the model\nis significantly lower than the LPNs hit in the CMT of\nTPFTL. Therefore, TPFTL performs much better than LeaFTL\nunder workloads with high locality. This experiment indicates\nthat when dealing with locality-based workloads, using direct\nmapping in the cache is more reliable and efficient than using\nmodels.\nThe above analysis and experiments all indicate that handling\nmispredictions caused by incompletely accurate learned index\nmodels directly affects SSD performance.\nChallenge #2: Conflict between the linear model and access\nparallelism. A key requirement for training the linear model in\nlearned indexes is sorted LPN-PPN mappings. In LeaFTL, after\nthe model buffer is sorted with LPNs, LeaFTL needs to allocate\ncontiguous PPNs for these LPNs. However, modern SSDs are\nhighly dependent on internal parallelism so that multiple flash\nblocks can be accessed simultaneously across separate flash\nchips [13], [39]. To be specific, when a set of LPNs needs\nto be written to an SSD, these LPNs are written to different\nparallel units (channels, chips, dies, and planes). Since the\nparallel units belong to a high hierarchical structure, the PPNs\nin different parallel units may be far apart. Therefore, assigning\ncontiguous PPNs for sorted LPNs is hard in the parallel writing\nstrategy.\nChallenge #3: High training overhead. In LeaFTL, model\ntraining is performed on the critical write path. It brings two\noverheads: (1) Performance overhead: The model training\nincludes sorting, parameters fitting, and compaction, which\nis time-consuming. These operations performed on the critical\nwrite path will directly affect write performance. (2) Space\noverhead: The space overhead happens in random writes. The\nLPNs of adjacent write requests are dramatically separated.\nIt is difficult for the model buffer to gather these LPNs in\nLeaFTL. In the worst case, each LPN-PPN mapping in the\nmodel buffer becomes an individual learned segment, leading\nto a huge space overhead.\nTo sum up, recent advances in the learned index have\nshown that it can achieve significantly faster lookup speed\nand index space savings. Motivated by the urgent need to\nresolve the double-read problem caused by random reads in\nflash-based SSDs, along with the challenges learned from\nPPN\nLPN off0off1off2 offnPPN=(LPN-SLPN) *kn+bn\n…… 0 001 1………bn\nb1\nb0k0k1kn\nbitmap filter 0 0 1 1 0Model\nModel\n<k, b, off>[N]\nbitmap[M]GTD\nSLPNFig. 8: The structure of an in-place-update model in GTD.\nlearned indexes/LeaFTL, we propose LearnedFTL, which\nutilizes lightweight learned index models in the existing on-\ndemand page-level FTL (TPFTL) to enhance the random-read\nperformance of flash-based SSDs.\nIII. D ESIGN\nA. LearnedFTL Overview\nThe main idea of LearnedFTL is combining the learned\nindex with demand-based FTL, where the demand-based\nmapping scheme handles locality-based access patterns and\nlearned indexes handle random access patterns. This design\nallows LearnedFTL to serve all types of workloads efficiently.\nFigure 1(c) illustrates the system overview of LearnedFTL.\nIn LearnedFTL, each request first checks the CMT. If the\nCMT fails, LearnedFTL queries the corresponding GTD entry\nand uses the learned index model to predict the PPN. If the\nprediction is correct, LearnedFTL accesses the predicted PPN\ndirectly, thus eliminating the flash double-read operation. If\nthe prediction is inaccurate, LearnedFTL accesses the data by\nusing the original flash double-read method in TPFTL.\nEach model in the GTD is called an in-place-update linear\nmodel . Each in-place-update model is a piece-wise linear\nmodel, and each linear model has adjustable parameters. To\nguarantee the accuracy of the model predictions ( Challenge\n#1), each in-place-update linear model is equipped with a\nbitmap filter, which indicates whether the prediction of a\ncertain LPN is accurate, thus reducing the cost of inaccurate\npredictions. To obtain contiguous PPNs for sorted LPNs\n(Challenge #2 ), LearnedFTL proposes the virtual PPN (VPPN)\nrepresentation to convert PPNs from different parallel units into\nsequential ones. To reduce the space overhead and performance\noverhead of model training under random writes ( Challenge\n#3), LearnedFTL proposes a group-based allocation to bring\nLPNs belonging to the same GTD entry together and proposes\ntwo model training strategies, including a computation-free\nsequential initialization and a model training via GC/rewrite\nstrategy.\nB. In-Place-Update Linear Model\nThe model layer in GTD is the most critical component\nin LearnedFTL, as it determines the efficiency of the entire\naddress mapping process. Figure 8 illustrates the structure of\nthein-place-update linear model used in LearnedFTL. Since\n\nRead LPNreq1Read LPNreq2 CMT\nLPN PPN\n15 104GTD\nModel LPN Trans\n228 0-511VPPNreq1 ModelTransPage\nLPN PPN\n15 104Data PageDouble read\nPPNreq1PPNreq2\nFAIL\nLPNreq1 LPNreq2…… 0010 1Bitmap FilterData PageFig. 9: The workflow of bitmap filter.\neach model is attached to a GTD entry, each model is only\nused to predict the mappings of the LPN range represented\nby its attached GTD entry. An in-place update linear model\nis a piece-wise linear regression model (PLR model), and it\nconsists of two parts: a parameter array <k,b, off >[N] and\nabitmap filter .\nIn the parameter array, Each <k,b,off >represents a linear\nmodel, including intercept ( b), slope ( k), and the offset ( off)\nfrom this PLR model’s starting LPN. Given a certain LPN,\nthe offset ( offx) from the starting LPN is calculated first,\nand then LearnedFTL queries the corresponding linear model\n<kn,bn,offn>based on the offx. The PPN can be predicted\nusing the y=kn×(LPN−LPN start)+bn.\nA bitmap filter is a bitmap, and each bit in the bitmap is\nassociated with an LPN, representing whether an LPN can be\naccurately predicted ( 1means accurate, 0means inaccurate).\nThe bitmap is updated during model training (detailed in\nSection III-E ). With the bitmap filter, the in-place-update linear\nmodel offers two significant benefits over the traditional learned\nindexes:\n(1)Accurate predictions . The bitmap filter can mark which\nLPNs can make accurate predictions, assisting models to make\nonly accurate predictions. Figure 9 illustrates the two different\ninstances of the bitmap filter. For a request with an LPN req1\nthat needs to use the model to predict the PPN, LearnedFTL\nfirst checks the corresponding bit in the bitmap and finds the\nbit is 1. Then LearnedFTL will perform model prediction to\ngenerate the true PPN req1. Since this prediction is marked as\naccurate, LearnedFTL directly uses this PPN req1to access data.\nFor another request with LPN req2whose corresponding bit is\n0, LearnedFTL will perform a double read for this LPN and\nnot use the model to make predictions. With the bitmap filter,\nLearnedFTL can make only the correct model predictions and\navoid miss penalty caused by wrong model predictions.\n(2)The model parameters can be updated as needed . The\nbitmap filter offers the ability to control each LPN, making\nin-place update of the model possible. Figure 10 shows the\nPPN\nLPN off1off2\n0 001 0b2\nb1k1k2\nbitmap filter 0 01 0PPN\nLPN off1off2’\n1 111 1b2\nb1’k1’k2\n0 01 0①modify the k1and b1\n②modify the off2\nof model2\n③update the bitmapmodel1model2\nmodel1’Fig. 10: The workflow of model in-place update.\nworkflow of model in-place update, when we retrain the LPN-\nPPN mapping for model 1(with k1andb1), we can directly\nupdate the original model in-place. The model in-place update\nfirst modifies the slope k1and then intercepts b1to the newly\ncalculated value k′\n1and b′\n1. Since the range of new model′\n1\nand the range of model 2have conflict, the o f f 2ofmodel 2\nshould be increased until it does not conflict with the new\nmodel′\n1. Finally, the bitmap is updated based on the accuracy\nof the newly generated model. With the in-place-update ability,\nan in-place update linear model can always maintain a fixed\nspace overhead, avoiding the need for space compaction like\nthe LSMT in LeaFTL.\nThe data consistency of the in-place-update linear model\nis guaranteed upon each update. Specifically, for each write\nrequest with an LPN, LearnedFTL first checks if the corre-\nsponding bit of this LPN in the bitmap is 1. If so, LearnedFTL\nwill set this bit to 0to prevent the model from making wrong\npredictions.\nSince persisting the models to flash upon each update will\nbring additional writing overhead, the models are saved to flash\nfollows the GTD saving procedure as the TPFTL and DFTL\nhandle. During a normal power-off, the models are saved in a\nflash area alongside GTD. This allows us to easily retrieve and\nuse the stored models when the device reboots. In the event\nof a power failure, GTD is rebuilt by scanning all translation\npages. Models can also be reconstructed from the mapping\ninformation within these translation pages, similar to TPFTL\nand DFTL. The reconstruction won’t take much time since\nthe time overhead for model training is minimal, as shown in\nFigure 15.\nC. Virtual PPN Representation\nWe propose virtual PPN representation to address the\nproblem of non-contiguous PPNs caused by SSD internal\nparallelism. During model training of learned indexes, it is\nimportant to allocate contiguous PPNs for contiguous LPNs.\nHowever, the pages with consecutive LPNs may be written back\nto different flash chips, leading to non-contiguous PPNs. To\ntackle this problem, LearnedFTL uses a VPPN representation\nto transform the non-contiguous PPNs scattered across different\nchips into contiguous ones. Figure 11 shows the translation\nprinciple from PPN to virtual PPN. Since the total number of\nphysical flash pages is fixed in an SSD, the PPN is formed\nin such a way that it represents the hierarchical tree structure\n\nchn\n8 8 1 256 512lun pl blk pg\nchn lun pl blk pgTotal: =8388608 ××××\nTotal:Real:\n256 512 1 8 8=8388608 ××××Exchange\nSame\nVirtual:Fig. 11: The principle of virtual PPN translation.\nchn❶❷❸❹❺\nlun pl blk pg\nchn❶❷❸❹❺\nlun pl blk pgTrue PPN\nVirtual PPNLPN\nLPN4 5 1 64 127 =5013631 1001\n5 5 1 64 127 =6062207 1002\n6 5 1 64 127 =7110783 1003\n4 5 1 64 127 =2105388 1001\n5 5 1 64 127 =2105389 1002\n6 5 1 64 127 =2105390 1003\nFig. 12: An example of PPN-to-VPPN translation.\nof an SSD by the concatenation of address fields representing\ndifferent levels of the hierarchy from the highest (channel) to\nthe lowest (page) granularity. Because of the commutative law\nof multiplication, the order of these address fields in PPN can\nbe changed to obey the allocation order. Thus, each physical\npage retains its unique number, and the new page number will\nbecome contiguous according to the allocation order.\nFigure 12 gives an example of the PPN-to-VPPN translation.\nIn LearnedFTL, the allocation order is channel, chip, plane,\npage, and block , which is the fastest allocation order based on\nthe previous study [13]. For requests with LPNs 1001, 1002,\n1003 that are already written to flash-based SSD, their PPNs\nare5013631, 6062207, 7110783 , which are not contiguous.\nHowever, after the PPN-to-VPPN translation by changing the\norder of the fields in the address appropriately, LearnedFTL\nobtains contiguous VPPNs 2105388, 2015389, 2105390 for\nthese LPNs.\nThe virtual PPN representation allows LearnedFTL to\ngenerate contiguous VPPNs for model training when valid\npages are written to the flash-based SSDs concurrently. Since\nthe training model is built based on LPN-VPPN mappings, the\npredicted VPPN needs to be translated back to PPN to obtain\nthe physical flash page.\nD. Group-based Allocation Strategy\nSince random writes generate requests of non-contiguous\nLPNs, it’s non-trivial to group them together and create a\nlearned index during writes. Fortunately, garbage collection\nprovides the opportunity to rearrange PPNs. Specifically, during\nGC, LearnedFTL can rearrange one GTD entry’s PPNs to\nconsecutive PPNs and then train models over these newly\narranged PPNs.\nLPN TPPN\n0-511\n511-1023224\n225\n… …Global Translation Directory (GTD)\nModel\n…\n…\n…\n289 …\n295 …4096 -4607\n4608 -5119blk1\nFLASHGroup 0\nGroup 4blk2\nblk110 blk111blk3 blk4\nDRAM......newFig. 13: An example of group-based allocation.\nHowever, the current dynamic allocation strategy used by\nLeaFTL and TPFTL makes PPN rearrangement difficult. This\nis because when allocating a flash page for a PPN, dynamic\nallocation will select the least busy flash chip to allocate pages\nfor optimal parallelism and write efficiency. As a result, the\nPPNs of a GTD entry will be scattered across various locations.\nWhen building a learned model over this GTD entry via GC,\nLearnedFTL needs to collect the valid pages across multiple\nflash blocks, and these blocks also may contain PPNs belonging\nto other GTD entries. As a result, the GC process generates\nfrequent data movement, which significantly increases the\ncomplexity and overhead of the model training process.\nTo address this PPN rearrangement issue, we propose a\ngroup-based allocation strategy . The basic idea is to divide\nGTD into groups of consecutive entries, referred to as GTD\nentry group . Each group is allocated an exact number of\ncontiguous flash blocks to accommodate all the LPNs of the\ngroup. When the flash blocks allocated to a GTD entry group\nare full, these used flash blocks are replaced by the same\nnumber of contiguous empty flash blocks. When there are no\nempty flash blocks or the cumulative number of flash blocks\nallocated to this GTD entry group reaches a threshold, GC\nis performed on the GTD entry group with the most invalid\ndata pages. During GC, LearnedFTL reclaims data blocks by\nrelocating the valid data pages and retrains the learned models\nfor all GTD entries in this group.\nFigure 13 illustrates an example of group-based allocation.\nIn this instance, for the convenience of presentation, each GTD\nentry group contains two entries and needs two contiguous\nflash blocks to accommodate all its LPNs. Therefore, LPNs\n0-1023 belong to group 0 and LPNs 4096-5119 belong to\ngroup 4. When a request for data with LPN belonging to group\n0 arrives, two contiguous blocks, blk1 andblk2, are allocated\nto group 0. When a request for data with LPN belonging to\ngroup 4 arrives, another two contiguous blocks, blk110 and\nblk111 , are allocated to group 4 to accommodate the required\ndata pages. When group 0 has no free physical pages, another\ntwo contiguous blocks, blk3 and blk4, are allocated to this\ngroup. If group 0 is selected for garbage collection, all four\nblocks are collected directly.\nA serious write-amplification concern arises with this group-\nbased allocation strategy: when all GTD entry groups have\nbeen written at least once, a few hot GTD entry groups have\nbeen written frequently, which causes huge write amplification.\n\nTo solve the problem, a global counter is associated with\neach GTD entry group to identify the hot groups by counting\navailable free pages in the group.\nTo address the situation where hot GTD entry groups have\nlimited or no free pages, LearnedFTL employs an opportunistic\ncross-group allocation strategy. This approach allows these hot\ngroups to utilize the available free-page spaces within flash\nblocks belonging to ”cold” GTD entry groups that have an\nabundance of free pages and untrained models. By encroaching\ninto the free-page spaces of the cold groups, LearnedFTL\neffectively avoids or delays the need for GC operations. Once\nthe amount of encroachment reaches a specific threshold, GC\nis triggered for both the encroaching (hot) group and the\nencroached (cold) group. Subsequently, their respective models\nundergo retraining and training processes. Consequently, this\nopportunistic cross-group allocation approach not only reduces\nthe frequency of GC and the write amplification caused by GC\noperations in hot groups but also ensures the early training of\nmodels in cold groups.\nE. Model Training\nTo ensure the timeliness of the in-place-update model,\nLearnedFTL uses two model training strategies. One is se-\nquential initialization, which is used to initialize the model\nthrough sequential write requests during data writing. The other\nis model training via GC, which is used to train the model\nduring garbage collection to achieve higher accuracy.\n1) Sequential Initialization: The main idea of sequential\ninitialization is to update the learned index model in place based\non sequential write requests. In many workloads, the I/O size\nof each request may range from several to tens of flash pages.\nWhen assigning contiguous PPNs for each I/O request, these\nLPN-PPN mappings can be seen as a y=xmodel. Therefore,\nwe can use these y=x models to update the corresponding\nin-place-update linear model. For each write request, there are\nfour steps in sequential initialization:\n①Obtaining contiguous PPNs. LearnedFTL first writes the\ndata of this request to the flash memory and obtains contiguous\nPPNs. After obtaining contiguous PPN, each LPN must check\nwhether the corresponding bit in the bitmap is ‘1’. If it is,\nLearnedFTL updates it to ‘0’.\n②Generate the linear model. LearnedFTL builds a y=x\nmodel on these LPN-PPN mappings. Then LearnedFTL obtains\nthe model’s starting LPN ( LPN start), ending LPN ( LPN end), and\nlength (L).\n③Check corresponding model. LearnedFTL locates the\ncorresponding model with <k,b,off>in GTD by LPN start\nandLPN end. After that, LearnedFTL calculates the length Lold\nof the existing model through the corresponding bitmap.\n④Update the model. IfLoldis smaller than L, LearnedFTL\nperforms in-place-update to replace the existing linear model\nwith the newly generated linear model.\n2) Model Training via GC: Since only long write requests\nwill perform sequential initialization, LearnedFTL also pro-\nposes model training via GC to obtain a more comprehensive\nand accurate model. With the help of group-based allocation,when LearnedFTL performs garbage collection, all valid pages\nof one GTD entry group can be collected and trained. When\na GTD entry group needs to perform GC, the whole model\ntraining process via GC is divided into four steps:\n①Regulate valid mappings. First, LearnedFTL reads all\nthe translation pages of this GTD entry group and only keeps\nthe valid translations in memory. Then LearnedFTL sorts the\nvalid translations by their LPNs to make them ordered.\n②Write valid pages back and obtain PPNs. After\nregulating the valid LPNs, LearnedFTL allocates another group\nof flash blocks to this GTD entry group, then writes the valid\npages back to the newly allocated flash blocks to get contiguous\nPPNs.\n③Train the learned model. In this step, each GTD entry\nin this group will train its in-place-update linear model. For\neach GTD entry, calculate the offset of PPNs/LPNs from this\nGTD entry’s starting PPN/LPN. Then, perform greedy linear\nregression to fit to get the <k, b, off >parameters array.\n④Evaluate the model. After training the models, Learned\nFTL will evaluate the model and update the bitmap filter.\nDuring this process, each LPN will be inputted into the model.\nIf the predicted PPN is accurate, the corresponding bit will be\nset to ‘1’.\n3) Model Training via Rewrite: For some scenarios where\nGC rarely happens, the model training can be integrated into\nthe SSD rewrite process [3], [30]. The rewrite is a widely used\nreliability mechanism to reduce retention errors in modern\nSSDs by periodically reading, correcting, and reprogramming\nthe flash memory. Rewrite happens frequently and is the most\nsignificant factor for write amplification [30]. During SSD\nrewrite, LPNs of flash pages can be sorted in order so that\nthese pages are written back in contiguous PPNs, which then\nenables a model to be built and trained on them by LearnedFTL.\nF . Cost Analysis\nThough LearnedFTL introduces multiple new components\nto apply the learned index in the FTL, it only introduces minor\ncomputational overhead, illustrated as follows.\n(1) Write : For each write request, LearnedFTL incurs two\nadditional operations, one is the bitmap check operation\n(Section III-B ) to maintain the consistency of the model,\nthe other is the sequential initialization (Section III-E 1).\nBoth operations are performed in memory, and there are no\ncalculation operations. Thus, the overhead can be ignored.\n(2) Read : For each read request with an LPN, LearnedFTL\nincurs two additional operations when this LPN cannot hit in\nthe CMT. The first operation is a bitmap check to check if\nthis LPN can predict a real PPN. The second operation is a\nmodel prediction when the bitmap check is true. For these\nLPNs, LearnedFTL will use the model to predict the real PPN\ninstead of an extra flash read. The model prediction includes\ncalculating the VPPN with the y=kx+b model and translating\nthe predicted VPPN to PPN.\n(3) GC : The model training incurs two computational\noverheads during the GC period. The first one is sorting all\nthe LPNs within each GTD entry (Step ①in Section III-E 2).\n\n0400800120016002000\nRandRead SeqRead RandWrite SeqWriteThroughput (MB/s)图表标题\nDFTL\nTPFTL\nLeaFTL\nLearnedFTL\nideal(a) Throughput\n020406080100\nDTP LF LD IHit Ratio (%)图表标题\nModel hit\nCMT hit\nRandReadD TP LF LD I \nSeqRead (b) Model and CMT hit ratio\n1.35 1.251.451.28\n1.07\n02468\nRandWrite SeqWriteWrite Amplification图表标题\nDFTL\nTPFTL\nLeaFTL\nLearnedFTL\nideal (c) Write amplification\nFig. 14: The FIO performance under 64 threads (D: DFTL, TP: TPFTL, LF: LeaFTL, LD: LearnedFTL, I: ideal FTL).\nThe second one is training each GTD entry’s model (Step ③\nin Section III-E2).\nOur experiments in Section IV-C have detailed evaluations\nto quantitatively analyze these overheads.\nIV. P ERFORMANCE EVALUATION\nA. Implementation and Experiment Setup\nExperiment Setup : The experiments are conducted on\nFEMU [23]. FEMU is a QEMU-based and DRAM-backed\nSSD emulator that is widely used in recent studies [12], [24],\n[42]. It runs in a machine with two Intel(R) Xeon(R) Gold\n5318Y 2.10GHz CPUs and 128GB DRAM. The operating\nsystem is Linux with kernel version 5.4.0. The emulated SSD\nis configured with 32GB logical capacity plus 2GB over-\nprovisioning space and has 64 parallel chips (8 channels and\n8 ways per channel). Each flash chip has 256 flash blocks and\neach flash block has 512 flash pages. The size of a flash page\nis set to 4KB. The latency of NVMe SSD is 40 µs for NAND\nread, 200 µs for NAND write, and 2ms for NAND erase, which\nare the default settings in FEMU and widely used in the recent\nflash-based studies [12], [23], [24]. Since the SSD rewrite for\nretention errors is not implemented in FEMU [30], we only\ntrain models in GC.\nLearnedFTL is compared against three representative page-\nlevel FTL designs, DFTL [10], LeaFTL [33], and TPFTL [41].\nWe also use full-page mapping as a control (denoted as ideal ,\nwhich is considered a performance upper bound). In the\nexperiments, we use both FIO benchmark [14] and real-world\napplications/traces to evaluate different FTL designs.\nPrototype implementation : We implement LearnedFTL\nby modifying the blackbox mode of the FEMU based on\nthe TPFTL scheme. According to the allocation strategy and\ninternal parallelism of the SSDs, we group each 64 consecutive\nGTD entries as a GTD entry group . Since the size of a flash\npage is 4KB and each translation page has 512 LPN-PPN\nmappings, the GTD has 16384 entries. Each GTD entry group\nis allocated 64 flash blocks at a time, one for each of the 64\ntranslation pages. For parameter setting in the piecewise linear\nmodel, 8 pieces are set by default.\nSince the previous demand-based FTLs, such as DFTL and\nTPFTL, and the recent LeaFTL are all implemented on trace-\ndriven simulators, such as SSDsim [13] and Flashsim [18],\nwe incorporate them into the FEMU emulator according to\ntheir designs in the papers. We use FEMU’s default greedy\n0.65 0.38\n020406080100\nsorting training predictionTime overhead (μs)X86 ARM A72Fig. 15: The computing overhead of the training operations\nbetween ARM and X86 processors.\ndynamic allocation strategy for their allocation strategy. Since\nLeaFTL’s paper does not explain how the data in the model\nbuffer is written to the SSD, in this article, the data writing\nstrategy of LeaFTL will be consistent with that of TPFTL.\nBesides, we also added VPPN representation to LeaFTL to\nobtain continuous training data. We added and modified about\n5,000 LoC to implement these baselines and the LearnedFTL\nin FEMU. The source code of these prototype implementations\ncan be found in our Github repository [19].\nMemory consumption : Previous studies on demand-based\nFTLs usually set the capacity of CMT to about 3% of the total\nnumber of page mappings [10], [41]. For a fair comparison,\nwe set the capacity of LeaFTL’s model cache to have the same\nspace overhead as the CMT of DFTL/TPFTL. For LearnedFTL,\neach model in the GTD entry has two parameters, <k,b,off >[N]\nandbitmap . For <k,b,off >[N], both kandbare set to a 2B\nfloat value (float16), and offis set to a 2B integer value. For\nbitmap , each slot is a bit, and there are 512 bits in total. To sum\nup, an in-place-update linear model requires 128 Bytes. After\naggregating the space overhead of all the models, it can be\ncalculated that the total overhead of the models in LearnedFTL\nis approximately half of the CMT space overhead in TPFTL\nand DFTL. Therefore, we set the CMT size of LearnedFTL to\nonly accommodate 1.5% of the total number of mappings to\nmaintain the same memory overhead as other FTLs.\nController computing : Since LearnedFTL adds some addi-\ntional computing operations, it is necessary to correctly simulate\nthe computing power of the SSD controller. The mainstream\nSSD controller CPUs are ARM’s Cortex-A series and Cortex-R\nseries. we compared the time consumption of executing the\nadditional operations on the FEMU simulated CPU (X86) and\na low-end embedded processor (ARM Cortex-A72), and each\n\noperation is at the maximum complexity. Figure 15 shows that\nthe ARM A72 processor even performs better than the X86,\nwhich shows that we can use the X86 FEMU simulator to\nsimulate LearnedFTL’s computing power.\nB. FIO Benchmark\nWe use the FIO benchmark [14] to evaluate the performance\nof sequential writes, random writes, sequential reads, and\nrandom reads for different FTL designs. For each experiment,\nwe ran at least three times to get the average results.\n(1) Read : For random-read and sequential-read evaluations,\nwe first perform FIO random write and sequential write to\nwarm up the whole SSD. Data is continuously written until\nthe SSD is written over about 6 times to reach a stable state.\nSince LeaFTL cannot handle 4KB random writes, the I/O size\nin the warm-up is 512KB (128 flash pages), which allowed\nthe learned index of LeaFTL to be built normally. Then we\nperform a corresponding FIO read benchmark for evaluations.\nAll the evaluations use 4KB I/O size and psync I/O engine\nwith 64 threads.\nFigure 14(a) illustrates the throughput results for d ifferent\nFTL designs under different access patterns. For random read,\nLearnedFTL outperforms DFTL, TPFTL, and LeaFTL by 1.5 ×,\n1.4×, and 1.6 ×, respectively. For sequential read, LearnedFTL\noutperforms DFTL, TPFTL, and LeaFTL by 1.1 ×, 1.1×, and\n1.1×, respectively. Moreover, the performance of LearnedFTL\nis very close to that of the ideal FTL, achieving about 89.2%\nand 96.8% of the performance of the ideal FTL under random\nand sequential reads, respectively.\nTo explore the behind reasons, we also recorded the\npercentage of requests that hit the CMT and the learned models\nduring random and sequential reads. The ideal FTL is used as\na control which can be considered as an upper bound since its\nCMT has a hit ratio of 100% and infinite space. For LeaFTL,\nwe only count the situation that requires a single flash read\n(cache hit and model prediction is accurate), which is also\nmarked as a model hit.\nFigure 14(b) shows that the CMT hit ratios of DFTL and\nTPFTL designs are almost 0 under random reads. The reason\nis that random reads show no locality, which makes the cache\nreplacement policy fail to capture the access pattern. In LeaFTL,\nonly 5% of requests can perform a single flash read. Of the\nremaining requests, 43% were triple reads and 52% were\ndouble reads, so LeaFTL performs worst in random reads.\nBy contrast, all learned index models in LearnedFTL can be\nstored in SSD’s memory with 55.5% accuracy, which means\n55.5% extra flash translation reads can be reduced. Thus, it\ncan significantly improve the random-read performance over\nother FTL schemes.\nUnder sequential reads, LearnedFTL still outperforms DFTL,\nLeaFTL, and TPFTL. Since each thread competes for cache\nspace, DFTL, TPFTL, and LeaFTL only achieve 61%, 80%,\nand 76% hit ratios on CMT and models. By contrast, because\nall models of LearnedFTL can be stored in SSD memory,\nLearnedFTL can resolve contentions effectively. LPN misses\nin CMT can be hit in the model, and no cache replacement\n02468101214Frequency (#/s)\nTime (s)图表标题\nDFTL TPFTL LeaFTL LearnedFTL ideal\nRandom Sequential\nTime (s)Fig. 16: The GC frequency of all FTL designs under FIO\nrandom and sequential write benchmarks.\n0 10 20 100trainingsortingGC readGC writesGC erase\nTime (ms)time\n0123\nMAXPercentage of GC (%)图表标题\ntraining\nsort\n180 360 480\nFIO Write Time (s)\nFig. 17: The time overhead of sorting and training under\ndifferent running times of FIO random writes (MAX means\nalmost all pages are valid during GC).\nwill occur. As a result, LearnedFTL achieves a combined CMT-\nModel hit ratio of up to 90%, eliminating 90% of the LPN-PPN\ndouble reads. Thus, LearnedFTL achieves the best performance\namong all FTLs and approaches that of the ideal FTL, which\nis the upper bound.\n(2) Write : We perform FIO write for the random-write and\nsequential-write evaluations, and all the evaluations use 4KB\nI/O size and psync I/O engine with 64 threads.\nFigure 14(a) shows that under random writes, LearnedFTL\noutperforms other schemes by 1.2 ×to 1.4×, respectively,\nbecause of LearnedFTL’s group-based allocation strategy. With\nthe group-based allocation strategy, LearnedFTL selects one\nGTD entry group for each GC, only the translation pages of\nthis GTD entry group need to be updated. That is, a maximum\nof 64 translation pages are updated per GC. However, for the\ndynamic allocation used in other schemes, when the same\nnumber of data blocks are collected, the LPN range of flash\npages written back may be more than 64 translation pages,\nincurring additional write amplification.\nOwing to the spatial locality of sequential writes, Learned\nFTL performs almost the same as DFTL and TPFTL, by less\nthan 2%. Unlike the dynamic allocation strategy which selects\nthe blocks with the fewest valid pages in each GC, the group-\nbased allocation strategy performs GC on a group-by-group\nbasis, which may result in more valid pages being written back.\nFortunately, the opportunistic cross-group allocation allows\nthe hot GTD entry group in sequential writes to use free\npages of cold GTD entry groups, reducing the number of valid\npages being written back. Thus, LearnedFTL’s sequential write\nperformance is the same as other FTLs.\n\n0100200300Throughput (MB/S)图表标题\nw/o training&sorting\nw/ training&sorting\nTime (s)18036048073012001800(a) Write and GC\n1.005 1.0041 1\n00.511.5\nRandRead SeqReadNormalized Throughput图表标题\nLD ideal LD (b) Read\nFig. 18: LearnedFTL with and without additional computing\noperations (LD: LearnedFTL, ideal LD: ideal LearnedFTL.\nC. Overhead Analysis\nWe evaluate the overhead induced by additional operations\nthat LearnedFTL brings as mentioned in Section III-F.\n(1) GC frequency and write amplification : In LearnedFTL,\nmodel training happens in GC, and LearnedFTL proposes\ngroup-based allocation to assist model training. Therefore, the\nGC frequency and write amplification are critical indicators.\nFigure 16 illustrates the GC frequency of various FTLs in\nthe FIO write evaluations. Although the GC frequency of\nLearnedFTL fluctuates, the total number of GCs triggered under\nrandom writes and sequential writes of LearnedFTL (4188 and\n4285) are less than DFTL (4335 and 4572), LeaFTL(4395\nand 4473) and TPFTL (4335 and 4304). Figure 14(c) shows\nthat the write amplifications of DFTL and LeaFTL are larger\nthan LearneFTL in random writes because the group-based\nallocation requires fewer translation page writes. For sequential\nwrites, with the assistance of opportunistic cross-group allo-\ncation, the write amplification of LearnedFTL is comparable\nto other FTLs. In summary, our group-based allocation can\neffectively assist the model training without inducing additional\nGC and write amplifications.\n(2) Overhead of training and sorting : The model training\n(denoted as training) and LPNs-sorting (denoted as sorting)\nare two additional operations added to GC in LearnedFTL. In\nour implementation, we group 64 GTD entries into one group.\nDuring each GC, a maximum of 64 LPNs-sorting and model\ntraining operations will be triggered for each GTD entry group.\nFigure 15 shows each GTD entry needs about 50 µs for sorting\nand training in ARM Cortex-A72. The maximum additional\noverhead incurred by sorting and training is equivalent to about\n80 SSD reads (40 µs per read), which is negligible since GC\nfor each GTD entry group will incur tens of thousands of SSD\nreads and writes. Figure 17 shows that the time overhead of\nsorting and training only accounts for up to 3.2% of the GC\nexecution time.\nTo further explore whether they will introduce additional\nlatency, we compare the FIO random write performance of\nLearnedFTL with and without training and sorting operations.\nFigure 18(a) shows that their performance difference is nearly\nnegligible (less than 0.7%), further verifying that the computing\noverhead of training and sorting is minimal in LearnedFTL.\n(3) Overhead in read operations : Only LPNs that can be\ncorrectly predicted will perform model prediction (0.65µs\n012345\nreadrandom readseqNormalized  Throughput图表标题\nDFTL\nTPFTL\nLeaFTL\nLearnedFTL\nideal(a) Normalized throughput\n020406080100\nDTPLFLD IDTPLFLD I\nSeqReadHit Ratio (%)\nRandRead图表标题\nCMT hit\nModel hit (b) CMT and model hit ratio\nFig. 19: RocksDB performance with one thread (D: DFTL, TP:\nTPFTL, LF: LeaFTL LD: LearnedFTL, I: ideal FTL).\nTABLE I: Filebench configurations.\nName Fileset Feature Threads\nfileserver 225,000 ×128KB write heavy 50\nwebserver 825,000 ×16KB read heavy 64\nvarmail 475,000 ×16KB all read 64\nin Figure 15). This means there is no miss penalty in model\npredictions. Although there is no miss penalty, if the model\nprediction takes too long, it will reduce the advantage of\nreducing double reads. We implement the ideal LearnedFTL,\nwhich puts all mappings in memory. For ideal LearnedFTL,\neach time the bitmap check is yes, it can directly get the\nPPN through the mapping table without model prediction.\nFigure 18(b) shows that the FIO read performance gap between\nLearnedFTL and ideal LearnedFTL does not exceed 1%,\ndemonstrating that the model predictions are lightweight.\nD. Real-World Applications\nRocksDB : RocksDB [32] is a widely used LSM-Tree-based\nKV store designed to exploit the parallelism of flash-based\nSSDs. As we mentioned before, LSM-Trees can merge random\nwrites into sequential ones, but at the cost of relatively poor\nservices to random reads. We deploy RocksDB with EXT4\nfile system on top of each FTL design and use the dbbench\ntool of RocksDB with one thread, which is consistent with the\nprevious studies [17], [31]. To evaluate the read performance,\nwe first use the fillseq andoverwrite indbbench to write the\nDB to 80% full, then we perform readrandom andreadseq in\ndbbench to evaluate the read performance in RocksDB.\nIn terms of throughput, Figure 19(a) shows that LearnedFTL\noutperforms other FTLs by 1.3 × ∼ 1.4×in random reads.\nLearnedFTL also outperforms other FTLs by 1.02 × ∼ 4.0×in\nsequential reads. To better understand these results, Figure 19(b)\nshows the model and CMT hit ratios recorded in these\nevaluations. In a single-threaded environment, DFTL does\nnot exploit and thus fails to benefit from the spatial locality, so\nits CMT hit ratio is zero. TPFTL and LeaFTL can achieve an\n81% CMT hit ratio and 83% model hit ratio by exploiting the\nspatial locality. By contrast, since LearnedFTL exploits both\nthe spatial locality and the learned model, it achieves 0.3% and\n46% CMT hit ratio, 55% and 41% model hit ratio in random\nreads and sequential reads, respectively.\n\n0123\nfileserver webserver varmailNormalized Throughput图表标题\nDFTL TPFTL LeaFTL LearnedFTL idealFig. 20: The normalized throughput of Filebench.\nTABLE II: Workload characteristics of four traces.\nTraces # of I/O Average I/O size Read ratio\nWebSearch1 1,055,235 15.5KB 100%\nWebsearch2 1,200,964 15.3KB 99.98%\nWebsearch3 793,073 15.7KB 99.96%\nSystor17 1,253,423 10.25KB 61.6%\nFilebench : Filebench [9] is a highly flexible storage bench-\nmark. We select three workloads that are most widely used\nin previous studies [2], [12], [42]: fileserver (write heavy),\nwebserver (read heavy, less random write), and varmail\n(read:write=1:1). Their configurations, consistent with previous\nstudies [12], [42], are summarized in Table I.\nFigure 20 shows the normalized performance of the four\nFTLs. LearnedFTL outperforms other schemes by 1.1 ×to\n2.3×. As we mentioned in Challenge #1 in Section II-C, the\ninaccuracy of LeaFTL’s learned models makes LeaFTL still\nrequire many double reads under these workloads with high\nlocality. As a result, LeaFTL’s performance is lower than\nTPFTL and LearnedFTL. Since LearnedFTL preserves the\nCMT, so most requests with high locality can be hit directly\nthrough the CMT. In addition, the learned index models can\nalso make predictions for requests that cannot be hit in the\nCMT, further improving performance.\nE. Real-world Traces\nWe select four traces (three WebSearch traces and one Systor\ntrace) to evaluate the efficacy of different FTL designs. The\nthree WebSearch traces are read-intensive workloads that are\ngenerated from a popular search engine [35]. The Systor trace\nis the enterprise storage traffic on modern commercial office\nVDI for 28 days [20]. The four traces all have strong locality.\nFor these traces, we pick the busiest periods (20 minutes to 2\nhours). Since the WebSearch traces are relatively old, we scale\nup them to reflect modern SSD workloads [24]. The workload\ncharacteristics of the four traces are summarized in Table II.\nBefore replaying the four traces, we warm up the whole SSD\nto a steady state with the same warm-up method mentioned in\nSection IV-B . We choose TPFTL and LeaFTL as the baselines\nfor the tail latency evaluation.\nFigure 21 shows the P99 andP99.9 tail latencies of TPFTL,\nLearnedFTL, and ideal FTL driven by the four traces. Under\nthese four traces, compared to TPFTL, LearnedFTL reduces the\nP99 tail latency by 5.3 ×, 7.4×, 6.5×, and 2.9 ×, respectively,\nwith an average of 5.5 ×. Compared with LeaFTL, LearnedFTL\n0100200300400500600\nWS1 WS2 WS3 SystorTail Latency (ms)图表标题\nTPFTL LeaFTL LearnedFTL ideal\nP99\n02004006008001000\nWS1 WS2 WS3 SystorP999Fig. 21: The P99 and P999 tail latencies results under four\ntraces (WS# denotes WebSearch#).\n0.511.5\nWS1 WS2 WS3 SystorNormalized Energy CostTPFTL LeaFTL LearnedFTL Ideal\nFig. 22: The energy cost under four traces (WS# denotes\nWebSearch#).\nreduces the P99 tail latency by 7.8 ×, 12.2×, 9.7×, and 3.0 ×,\nrespectively, with an average of 8.2 ×. Moreover, compared\nwith TPFTL and LeaFTL, LearnedFT reduces the P99.9 tail\nlatency by up to 13.9 ×and 21.4 ×, respectively. LearnedFTL’s\ntail latency in WebSearch2 and WebSearch3 are extremely\nclose to that of the ideal FTL. Although TPFTL and LeaFTL\ncan maintain high CMT hit ratios or model hit ratios on\nworkloads with strong locality, sporadic double reads and triple\nreads still induce high tail latency. By contrast, LearnedFTL’s\nlearned model can further reduce these sporadic double reads\nby accurate PPN prediction, thus reducing tail latency.\nF . Energy Cost\nWe established a basic power/energy model based on\nNANDFlashSim [15] and conducted tests. Figure 22 provides\na comparison of energy consumption for each FTL in four real\ntraces. In the three WebSearch traces, LearnedFTL reduces\nenergy consumption by 1.09 ×to 1.2×than TPFTL and\nLeaFTL, respectively. They perform similar under Systor trace.\nThe reason is that the energy consumption of flash write and\nerase overwhelms that of flash read. In workloads that are not\nread-intensive, the reduction in reading energy consumption\nhas limited impact on the total energy consumption. In\nread-intensive workloads, LearnedFTL can reduce energy\nconsumption compared to other FTLs.\nV. D ISCUSSION\nLinearity in page-level mappings . One common concern\nis why simple linear models can effectively fit page-level\nmappings. In real-world workloads, write requests often consist\nof multiple consecutive LPNs. When these LPNs are written\nto consecutive PPNs, we can observe a linear relationship like\ny=x+b . Furthermore, GC can also contribute to the linearity.\n\nDuring the GC process, the FTL can collect LPNs, sort them,\nand write them back to consecutive physical pages, resulting\nin a linear relationship such as y=kx+b . The existence of this\nlinearity enables good fitting results of simple linear models.\nMoreover, these models are easy to train and require less\ntraining time. Consequently, employing linear models for fitting\npurposes emerges as the optimal choice.\nEfficiency in random access . The superior performance of\nLearnedFTL in random access comes from its unique learning\npattern. Unlike traditional machine learning methods that try\nto fit access patterns [38], [40], the learned index models in\nLearnedFTL fit the relationship between LPN-PPN, that is, the\nrelationship between data and actual locations. In this way,\nwhether it is random access or regular access, the model can\nstably calculate the PPN corresponding to the LPN, thereby\nsignificantly improving the performance of random access.\nModel’s Space Overhead . The space overhead of learned\nindex models is an additional consideration introduced in FTL.\nTherefore, how to further reduce the space consumption of the\nmodel is a future work. Since LearnedFTL uses rounding mode\nto calculate PPN and bitmap filter to ensure the accuracy of\npredictions, the computational precision requirement is not high.\nAs a result, it is possible to consider using lower-precision data\ntypes, such as Float8, for model parameters. Apart from that,\nwe can further reduce the space overhead by compressing each\nmodel’ bitmap through compression or encoding techniques.\nVI. C ONCLUSION\nWe propose LearnedFTL, a learning-based page-level FTL\ndesign, by exploiting some unique characteristics of both the\nflash device and the learned index. LearnedFTL uses an in-\nplace-update linear model to build learned indexes efficiently, a\nvirtual PPN representation to obtain contiguous PPNs for sorted\nLPNs, and a group-based allocation and model training via\nGC/rewrite strategy to reduce the training overhead. Our FEMU-\nbased prototype and extensive evaluations have validated\nthat LearnedFTL outperforms the state-of-the-art TPFTL and\nLeaFTL schemes.\nACKNOWLEDGEMENTS\nWe thank the anonymous reviewers and You Zhou from\nHuazhong University of Science and Technology for their\nvaluable feedback. This work is supported by the Na-\ntional Key Research and Development Program of China\n(Grant No. 2023YFB4502703), the National Natural Science\nFoundation of China under Grant No. U22A2027 and No.\n61972325, and Open Research Projects of Zhejiang Lab (No.\n2021DA0AM01/002). Dr. Zhang is supported in part by the\nNational Natural Science Foundation of China under Grant\nNo. 62332021 and the Fundamental Research Funds for the\nCentral Universities, Peking University.\nREFERENCES\n[1]R. Bez, E. Camerlenghi, A. Modelli, and A. Visconti, “Introduction to\nFlash Memory,” Proceedings of the IEEE , vol. 91, no. 4, pp. 489–502,\n2003.[2]M. Bjørling, A. Aghayev, H. Holmberg, A. Ramesh, D. Le Moal, G. R.\nGanger, and G. Amvrosiadis, “ZNS: Avoiding the Block Interface Tax\nfor Flash-based SSDs,” in Proceedings of the USENIX Annual Technical\nConference (USENIX ATC’21) , 2021, pp. 689–703.\n[3]Y . Cai, Y . Luo, E. F. Haratsch, K. Mai, and O. Mutlu, “Data retention in\nMLC NAND flash memory: Characterization, optimization, and recovery,”\ninProceedings of the 21st International Symposium on High Performance\nComputer Architecture (HPCA’15) , 2015, pp. 551–563.\n[4]S.-J. Chae, R. Mativenga, J.-Y . Paik, M. Attique, and T.-S. Chung,\n“DSFTL: An Efficient FTL for Flash Memory based Storage Systems,”\nElectronics , vol. 9, no. 1, p. 145, 2020.\n[5]H. Chen, C. Li, Y . Pan, M. Lyu, Y . Li, and Y . Xu, “HCFTL: A\nLocality-aware Page-level Flash Translation Layer,” in Proceedings of\nthe 2019 Design, Automation & Test in Europe Conference & Exhibition\n(DATE’19) , 2019, pp. 590–593.\n[6]J. Ding, U. F. Minhas, J. Yu, C. Wang, J. Do, Y . Li, H. Zhang,\nB. Chandramouli, J. Gehrke, D. Kossmann, and D. Lomet, “ALEX:\nAn Updatable Adaptive Learned Index,” in Proceedings of the 2020\nACM International Conference on Management of Data (SIGMOD’20) ,\n2020, pp. 969–984.\n[7]J. Ding, V . Nathan, M. Alizadeh, and T. Kraska, “Tsunami: A Learned\nMulti-dimensional Index for Correlated Data and Skewed Workloads,”\nProceedings of the VLDB Endowment , vol. 14, no. 2, pp. 74–86, 2020.\n[8]P. Ferragina and G. Vinciguerra, “The PGM-Index: a Fully-dynamic Com-\npressed Learned Index with Provable Worst-case Bounds,” Proceedings\nof the VLDB Endowment , vol. 13, no. 8, pp. 1162–1175, 2020.\n[9] Filebench, https://github.com/filebench/filebench, 2021.\n[10] A. Gupta, Y . Kim, and B. Urgaonkar, “DFTL: a Flash Translation\nLayer Employing Demand-Based Selective Caching of Page-Level\nAddress Mappings,” in Proceedings of the 14th International Conference\non Architectural Support for Programming Languages and Operating\nSystems (ASPLOS’09) , 2009, pp. 229–240.\n[11] S. Gupta, Y . Oh, L. Yan, M. Sutherland, A. Bhattacharjee, B. Falsafi,\nand P. Hsu, “AstriFlash: A Flash-Based System for Online Services,” in\nProceedings of the 29th International Symposium on High Performance\nComputer Architecture (HPCA’23) , 2023, pp. 81–93.\n[12] K. Han, H. Gwak, D. Shin, and J. Hwang, “ZNS+: Advanced Zoned\nNamespace Interface for Supporting In-storage Zone Compaction,” in\nProceedings of the 15th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI’21) , 2021, pp. 147–162.\n[13] Y . Hu, H. Jiang, D. Feng, L. Tian, H. Luo, and S. Zhang, “Performance\nImpact and Interplay of SSD Parallelism through Advanced Commands,\nAllocation Strategy and Data Granularity,” in Proceedings of the 25th\nInternational Conference on Supercomputing (ICS’11) , 2011, pp. 96–107.\n[14] Jens Axboe, Flexible I/O Tester, https://github.com/axboe/fio, 2022.\n[15] M. Jung, W. Choi, S. Gao, E. H. Wilson III, D. Donofrio, J. Shalf, and\nM. T. Kandemir, “Nandflashsim: High-fidelity, microarchitecture-aware\nnand flash memory simulation,” ACM Transactions on Storage (TOS) ,\nvol. 12, no. 2, pp. 1–32, 2016.\n[16] J.-U. Kang, H. Jo, J.-S. Kim, and J. Lee, “A superblock-based flash\ntranslation layer for NAND flash memory,” in Proceedings of the 6th ACM\n& IEEE International conference on Embedded Software (EMSOFT’06) ,\n2006, pp. 161–170.\n[17] S. Kannan, N. Bhat, A. Gavrilovska, A. Arpaci-Dusseau, and R. Arpaci-\nDusseau, “Redesigning LSMs for Nonvolatile Memory with NoveLSM,”\ninProceedings of the USENIX Annual Technical Conference (USENIX\nATC’18) , 2018, pp. 993–1005.\n[18] Y . Kim, B. Tauras, A. Gupta, and B. Urgaonkar, “Flashsim: A Simulator\nfor NAND flash-based Solid-State Drives,” in Proceedings of the 1st\nInternational Conference on Advances in System Simulation (SIMUL’09) ,\n2009, pp. 125–131.\n[19] LearnedFTL Github Homepage, https://github.com/astlxmu/LearnedFTL,\n2023.\n[20] C. Lee, T. Kumano, T. Matsuki, H. Endo, N. Fukumoto, and M. Sugawara,\n“Understanding Storage Traffic Characteristics on Enterprise Virtual\nDesktop Infrastructure,” in Proceedings of the 10th ACM International\nSystems and Storage Conference (Systor’17) , 2017, pp. 1–11.\n[21] S.-W. Lee, W.-K. Choi, and D.-J. Park, “FAST: An Efficient Flash\nTranslation Layer for Flash Memory,” in Proceedings of the International\nConference on Embedded and Ubiquitous Computing (EUC’06) , 2006,\npp. 879–887.\n[22] S. Lee, D. Shin, Y .-J. Kim, and J. Kim, “LAST: Locality-Aware Sector\nTranslation for NAND Flash Memory-based Storage Systems,” ACM\nSIGOPS Operating Systems Review , vol. 42, no. 6, pp. 36–42, 2008.\n\n[23] H. Li, M. Hao, M. H. Tong, S. Sundararaman, M. Bjørling, and H. S.\nGunawi, “The CASE of FEMU: Cheap, Accurate, Scalable and Extensible\nFlash Emulator,” in Proceedings of the 16th USENIX Conference on File\nand Storage Technologies (FAST’18) , 2018, pp. 83–90.\n[24] H. Li, M. L. Putra, R. Shi, X. Lin, G. R. Ganger, and H. S. Gunawi,\n“IODA: A Host/Device Co-Design for Strong Predictability Contract\non Modern Flash Storage,” in Proceedings of the 28th Symposium on\nOperating Systems Principles (SOSP’21) , 2021, pp. 263–279.\n[25] P. Li, Y . Hua, J. Jia, and P. Zuo, “FINEdex: a Fine-grained Learned Index\nScheme for Scalable and Concurrent Memory Systems,” Proceedings of\nthe VLDB Endowment , vol. 15, no. 2, pp. 321–334, 2021.\n[26] P. Li, Y . Hua, P. Zuo, Z. Chen, and J. Sheng, “ROLEX: A Scalable\nRDMA-oriented Learned Key-Value Store for Disaggregated Memory\nSystems,” in Proceedings of the 21st USENIX Conference on File and\nStorage Technologies (FAST’23) , February 2023, pp. 99–114.\n[27] C.-Y . Liu, Y . Lee, W. Choi, M. Jung, M. T. Kandemir, and C. Das, “GSSA:\nA Resource Allocation Scheme Customized for 3D NAND SSDs,” in\nProceedings of the 27th International Symposium on High Performance\nComputer Architecture (HPCA’21) , 2021, pp. 426–439.\n[28] B. Lu, J. Ding, E. Lo, U. F. Minhas, and T. Wang, “APEX: a High-\nPerformance Learned Index on Persistent Memory,” Proceedings of the\nVLDB Endowment , vol. 15, no. 5, pp. 597–610, 2021.\n[29] Y . Lv, L. Shi, Q. Li, C. Gao, Y . Song, L. Luo, and Y . Zhang, “MGC:\nMultiple-Gray-Code for 3D NAND Flash based High-Density SSDs,” in\nProceedings of the 29th International Symposium on High Performance\nComputer Architecture (HPCA’23) , 2023, pp. 122–136.\n[30] S. Maneas, K. Mahdaviani, T. Emami, and B. Schroeder, “Operational\nCharacteristics of SSDs in Enterprise Storage Systems: A Large-Scale\nField Study,” in Proceedings of the 20th USENIX Conference on File\nand Storage Technologies (FAST’22) , 2022, pp. 165–180.\n[31] P. Raju, R. Kadekodi, V . Chidambaram, and I. Abraham, “Pebblesdb:\nBuilding Key-Value Stores Using Fragmented Log-structured Merge\nTrees,” in Proceedings of the 26th Symposium on Operating Systems\nPrinciples (SOSP’17) , 2017, pp. 497–514.\n[32] RocksDB, https://github.com/facebook/rocksdb, 2022.\n[33] J. Sun, S. Li, Y . Sun, C. Sun, D. Vucinic, and J. Huang, “LeaFTL:A Learning-based Flash Translation Layer for Solid-State Drives,” in\nProceedings of the 28th ACM International Conference on Architectural\nSupport for Programming Languages and Operating Systems (ASP-\nLOS’23) , 2023, pp. 442–456.\n[34] K. Tim, B. Alex, C. E. H, D. Jeffrey, and P. Neoklis, “The Case For\nLearned Index Structures,” in Proceedings of the 2018 International\nConference on Management of Data (SIGMOD’18) , 2018, pp. 489–504.\n[35] UMass Trace Repository. University of Massachusetts Amherst, https:\n//traces.cs.umass.edu/index.php/Storage/Storage, 2014.\n[36] M. Wang, Y . Zhang, and W. Kang, “ZFTL: A Zone-based Flash\nTranslation Layer with a Two-tier Selective Caching Mechanism,” in\nProceedings of the 14th International Conference on Communication\nTechnology (ICCT’12) , 2012, pp. 578–588.\n[37] X. Wei, R. Chen, and H. Chen, “Fast RDMA-based Ordered Key-Value\nStore Using Remote Learned Cache,” in Proceedings of the 14th USENIX\nConference on Operating Systems Design and Implementation (OSDI’20) ,\n2020, pp. 117–135.\n[38] S. Yoo and D. Shin, “Reinforcement Learning-Based SLC Cache\nTechnique for Enhancing SSD Write Performance,” in Proceedings of\nthe 12th USENIX Workshop on Hot Topics in Storage and File Systems\n(HotStorage’20) , 2020.\n[39] J. Zhang, J. Shu, and Y . Lu, “ParaFS: A Log-structured File System to\nExploit the Internal Parallelism of Flash Devices,” in Proceedings of the\nUSENIX Annual Technical Conference (USENIX ATC’16) , June 2016,\npp. 89–100.\n[40] T. Zhang, Z. Cheng, and J. Li, “Reinforcement Learning-driven Address\nMapping and Caching for Flash-based Remote Sensing Image Processing,”\nJournal of Systems Architecture , vol. 98, no. c, pp. 374–387, 2019.\n[41] Y . Zhou, F. Wu, P. Huang, X. He, C. Xie, and J. Zhou, “An Efficient\nPage-level FTL to Optimize Address Translation in Flash Memory,” in\nProceedings of the 10th European Conference on Computer Systems\n(EuroSys’15) , 2015, pp. 1–16.\n[42] Y . Zhou, Q. Wu, F. Wu, H. Jiang, J. Zhou, and C. Xie, “Remap-SSD:\nSafely and Efficiently Exploiting SSD Address Remapping to Eliminate\nDuplicate Writes,” in Proceedings of the 19th USENIX Conference on\nFile and Storage Technologies (FAST’21) , 2021, pp. 187–202.",
  "textLength": 74617
}