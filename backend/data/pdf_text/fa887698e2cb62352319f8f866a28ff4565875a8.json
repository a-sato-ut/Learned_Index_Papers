{
  "paperId": "fa887698e2cb62352319f8f866a28ff4565875a8",
  "title": "Video Monitoring Queries",
  "pdfPath": "fa887698e2cb62352319f8f866a28ff4565875a8.pdf",
  "text": "1\nVideo Monitoring Queries\nNick Koudas, Raymond Li, Ioannis Xarchakos\nUniversity of Toronto\nfkoudas,raymond.li,xarchakos g@cs.toronto.edu\nAbstract —Recent advances in video processing utilizing deep\nlearning primitives achieved breakthroughs in fundamental prob-\nlems in video analysis such as frame classiﬁcation and object\ndetection enabling an array of new applications.\nIn this paper we study the problem of interactive declarative\nquery processing on video streams. In particular we introduce\na set of approximate ﬁlters to speed up queries that involve\nobjects of speciﬁc type (e.g., cars, trucks, etc.) on video frames\nwith associated spatial relationships among them (e.g., car left\nof truck). The resulting ﬁlters are able to assess quickly if the\nquery predicates are true to proceed with further analysis of\nthe frame or otherwise not consider the frame further avoiding\ncostly object detection operations.\nWe propose two classes of ﬁlters IC andOD, that adapt\nprinciples from deep image classiﬁcation and object detection.\nThe ﬁlters utilize extensible deep neural architectures and are\neasy to deploy and utilize. In addition, we propose statistical\nquery processing techniques to process aggregate queries in-\nvolving objects with spatial constraints on video streams and\ndemonstrate experimentally the resulting increased accuracy on\nthe resulting aggregate estimation.\nCombined these techniques constitute a robust set of video\nmonitoring query processing techniques. We demonstrate that\nthe application of the techniques proposed in conjunction with\ndeclarative queries on video streams can dramatically increase\nthe frame processing rate and speed up query processing by\nat least two orders of magnitude. We present the results of a\nthorough experimental study utilizing benchmark video data sets\nat scale demonstrating the performance beneﬁts and the practical\nrelevance of our proposals.\nI. I NTRODUCTION\nIn the last few years, Deep Learning (DL) [50], [32] has\nbecome a dominant artiﬁcial intelligence (AI) technology in\nindustry and academia. Although by no means a panacea for\neverything related to AI it has managed to revolutionize certain\nimportant practical applications such as machine translation,\nimage classiﬁcation, image understanding, video query an-\nswering and video analysis.\nVideo data abound; as of this writing 300 hours of video\nare uploaded on Youtube every minute. The abundance of\nmobile devices enabled video data capture en masse and\nas a result more video content is produced than can be\nconsumed by humans. This is especially true in surveillance\napplications. Thus, it is not surprising that a lot of research\nattention is being devoted to the development of techniques\nto analyze and understand video data in several communities.\nThe applications that will beneﬁt from advanced techniques to\nprocess and understand video content are numerous ranging\nfrom video surveillance and video monitoring applications, to\nnews production and autonomous driving.\nDeclarative query processing enabled accessible query in-\nterfaces to diverse data sources. In a similar token we wishto enable declarative query processing on streaming video\nsources to express certain types of video monitoring queries .\nRecent advances in computer vision utilizing deep learning\ndeliver sophisticated object classiﬁcation [31], [57], [59], [20]\nand detection algorithms [10], [12], [54], [19], [51], [52], [53].\nSuch algorithms can assess the presence of speciﬁc objects\nin an image, assess their properties (e.g. color, texture), their\nlocation relative to the frame coordinates as well as track\nan object from frame [30] to frame delivering impressive\naccuracy. Depending on their accuracy, state of the art object\ndetection techniques are far from real time [19]. However\ncurrent technology enables us to extract a schema from a\nvideo by applying video classiﬁcation/detection algorithms\nat the frame level. Such a schema would detail at the very\nminimum, each object present per frame, their class (e.g., car)\nany associated properties one is extracting from the object\n(e.g., color), the object coordinates relative to the frame. As\nsuch one can express numerous queries of interest over such\na schema.\nIn this paper we conduct research on declarative query pro-\ncessing over streaming video incorporating speciﬁc constraints\nbetween detected objects, extending prior work [25], [24],\n[23], [21]. In particular we focus on queries involving count\nandspatial constraints on objects detected in a frame, a topic\nnot addressed in prior art. Considering for example the image\nin Figure 1(a) we would like to be able to execute a query of\nthe form1:\nSELECT cameraID, frameID,\nC1 (F1 (vehBox1)) AS vehType1,\nC1 (F1 (vehbox2)) AS vehType2,\nC2 (F2 (vehBox1)) AS vehColor\nFROM (PROCESS inputVideo PRODUCE cameraID, frameID,\nvehBox1, vehBox2 USING VehDetector)\nWHERE vehType1 = car AND vehColor = red\nAND vehType2 = truck\nAND (ORDER(vehType1, vehType2) = RIGHT\nthat identiﬁes all frames in which a red car has a truck on\nits right. In the query syntax, Ciare classiﬁers for different\nobject types (vehicle types, color, etc) and Fiare features\nextracted from areas of a video frame in which objects are\nidentiﬁed (using vehDetector which is an object detection\nalgorithm). Naturally queries may involve more than two\nobjects. Numerous types of spatial constraints exist such as\nleft, right, above, below , as well as combinations thereof.\nCategorization of such constraints from the ﬁeld of spatial\ndatabases are readily applicable [44]. Our interest in not\nonly to capture constraints among objects but also constraints\nbetween objects and areas of the visible screen in the same\n1Adopting query syntax from [37]arXiv:2002.10537v1  [cs.DB]  24 Feb 2020\n\n2\nfashion (e.g., bicycle not in bike lane, where bike lane is\nidentiﬁed by a rectangle in the screen). We assume that the\nquery runs continuously and reports frames for which the\nquery predicates are true.\nObject detection algorithms have advanced substantially\nover the last few years [10], [12], [54], [19], [51], [52], [53].\nFrom a processing point of view if one could afford to execute\nstate of the art object detection and suitable classiﬁcation for\neach frame in real time, answering a query as the one above\nwould be relative easy. We would evaluate the predicates on\nsets of objects at each frame as dictated by the query aiming\nto determine whether they satisfy the query predicate. After\nthe objects on a frame have been identiﬁed along with their\nlocations and types as well as features, query evaluation would\nfollow by applying well established spatial query processing\ntechniques. In practice, such an approach would be slow as\ncurrently state of the art object detectors run at a few frames\nper second [54].\nAs a result we present a series of relatively inexpensive\nﬁlters, that can determine if a frame is a candidate to qualify\nin the query answer. As an example, if a frame only contains\none object ( count ﬁlter ) or if there is no red car or truck in the\nimage or there is no car right of a truck in the frame ( class\nlocation ﬁlter ), it is not a candidate to yield a query answer.\nWe fully process the frame with object detection algorithms\nonly if they pass suitably applied ﬁlters. Depending on the\nselectivity of the ﬁlters, one can skip frames and increase\nthe rate at which streaming video is processed in terms of\nframes per second. The proposed ﬁlters follow state of the art\nimage classiﬁcation and object detection methodologies and\nwe precisely quantify their accuracy and associated trade-offs.\nOur main focus in this paper is the introduction and study\nof the effectiveness of such ﬁlters. Placement of such ﬁlters\ninto the query plan and related optimizations are an important\nresearch direction towards query optimization in the presence\nof such ﬁlters. Such optimizations are the focus of future work.\n(a)\n (b)\nFig. 1: Example Video Frames: (a) Example Spatial constraints\n(b) Spatial constraints on a temporal dimension\nArmed with the ability to efﬁciently answer monitoring\nqueries involving spatial constraints, we embed it as a prim-\nitive to answer another important class of video monitoring\nqueries, namely video streaming aggregation queries involving\nspatial constraints. Consider for example Figure 1(b). It depicts\na car at the left of a stop sign. From a surveillance point of\nview we would like to determine if this event is true for more\nthan say 10 minutes. This may indicate that the car is parked\nand be ﬂagged as a possible violation of trafﬁc regulations. Weintroduce Monte Carlo based techniques to efﬁciently process\nsuch aggregation queries involving spatial constraints between\nobjects.\nIn this paper we focus on single static camera streaming\nvideo as this is the case prevalent in video surveillance\napplications. Moreover since our work concerns ﬁlters to\nconduct estimations regarding objects in video frames and\ntheir relationships, we focus on video streams in which\nobjects and their features of interest (e.g. shapes, colors)\nare clearly distinguishable on the screen for typical image\nresolutions. As such the surveillance applications of interest\nin this study consist of limited number of classes and frames\ncontaining small numbers of objects (e.g., multiples of tens\nof objects as in city intersections, building security, highway\nsegment surveillance etc) but not thousands of objects. Crowd\nmonitoring applications [58] in which frames may contain\nmultiple hundreds or thousands of objects (sports events,\ndemonstrations, political rallies, etc) are not a focus in this\nwork. Such use cases are equally important but require very\ndifferent approaches than those we propose herein. They are\nhowever important directions for future work.\nMore speciﬁcally in this work we make the following\ncontributions:\n\u000fWe present a series of ﬁlters that can quickly assess\nwhether a frame should be processed further given video\nmonitoring queries involving count and spatial constraints\non objects present in the frame. These include count-\nﬁlters (CF) that quickly determine the number of ob-\njects in a frame, class-count-ﬁlters (CCF) that quickly\ndetermine the number of objects on a speciﬁc class in\na frame and class-location-ﬁlters (CLF) that predict the\nspatial location of objects of a speciﬁc class in a frame\nenabling the evaluation of spatial relationships/constraints\nacross objects utilizing such predictions. In each case we\nevaluate the accuracy performance trade-offs of applying\nsuch ﬁlters in a query processing setting.\n\u000fWe present Monte Carlo techniques to process aggregate\nqueries involving spatial query predicates that effectively\nreduce the variance of the estimates. We present a\ngeneralization of the Monte Carlo based approach to\nqueries involving predicates among multiple objects and\ndemonstrate the performance/accuracy trade-offs of such\nan approach.\n\u000fWe present the results of a thorough experimental eval-\nuation utilizing real video streams with diverse charac-\nteristics and demonstrate the performance beneﬁts of our\nproposals when deployed in real application scenarios.\nThis paper is organized as follows: Section II presents our\nmain ﬁlter proposals. In section III we present variance\nreduction techniques for monitoring aggregates introducing\nmultiple control variates. Section IV presents our experimental\nstudy. Finally section V reviews related work and Section VI\nconcludes the paper.\nII. F ILTERING APPROACHES\nIn this section we outline our main ﬁltering proposals. We\nassume video streams with a set frames per second (fps) rate;\n\n3\nwe also assume access to each frame individually. Resolution\nof each image frame is ﬁxed and remains the same throughout\nthe stream. Our objective is to process each frame fast applying\nﬁlters and only activate expensive object detection algorithms\non a frame when there is high conﬁdence that it will belong\nto the answer set (i.e., satisﬁes the query), to make the ﬁnal\ndecision. We ﬁrst outline a set of ﬁlters which we refer to\nasImage Classiﬁcation (IC) inspired by image classiﬁcation\nalgorithms [31], [57], [59], [20] and then outline a set of\nﬁlters which we refer to as Object Detection (OD) inspired\nby object detection algorithms [10], [12], [54], [19], [51],\n[52], [53]2. The set of ﬁlters we propose are approximate\nand as such can yield both false positive and false negatives.\nWe will precisely quantify their accuracy in Section IV.\nFrom a query execution perspective multiple ﬁlters may be\napplicable on a single frame. In this paper we do not address\noptimization issues related to ﬁlter ordering. A body of work\nfrom stream databases is applicable for this, including [2],\n[37]. Our implementation of ﬁlters is utilizing standard deep\nnetwork architectures and are implemented as branches in\naccordance to prior work [22], [69].\nA. IC Filters\nPopular algorithms for image classiﬁcation [31], [57], [59],\n[20] train a sequence of convolution ﬁlters followed by fully\nconnected layers to derive a probability distribution over the\nclass of objects they are trained upon. During training the ﬁrst\nlayers tend to learn high level object features that progressively\nbecome more specialized as we move to deeper network layers\n[16]. Recent works [3], [7], [42], [43], [68] have demonstrated\nthat concepts from image classiﬁcation can aid also in object\nlocalization (identifying the location of an object in the image).\nWe demonstrate that information collected at convolution\nlayers assists in determining the location of speciﬁc object\ntypes in an image and detail how this observation can be\nutilized to deliver object counts as well as assess spatial\nconstraints between objects. A typical architecture for image\nclassiﬁcation consists of a number of convolution layers3.\nBefore the ﬁnal output layer it conducts global average pooling\non the convolution feature maps and uses them as features for\na softmax layer, delivering the ﬁnal classiﬁcation.\nAssume an image classiﬁcation architecture (e.g., VGG19\n[57]) withL= 19 convolution layers and consider the l\u0000-th\nlayer from the start. Let the lconvolution layer as in Figure\n2 havedfeature maps each of size g\u0002g. Global average\npooling provides the average of the activation for each feature\nmap. Subsequently a weighted sum of these values is used\nto generate the output. Let ak(i;j)represent the activation\nof location i;j,1\u0014i;j\u0014gat feature map k. Conducting\nglobal average pooling on map k(k2[1:::d]) we obtain\nAk=P\ni;jak(i;j)(we omit the normalization by g2to\nease notation). Given a class cthe softmax input (for object\nclassiﬁcation) would be Sc=P\nkwc\nkAkwherewc\nkis the\nweight for class cfor mapk. As a result wc\nkexpresses the\n2In this paper we assume familiarity with object classiﬁcation and object\ndetection approaches, architectures and associated terminology [11].\n3Many other architectures are possible\nConvolution ConvolutionConvolutionGAPd channels\n1 x 1 x d\ndn outputs\nRELU\nweights n classes\nClass activation\nMap for Class iLast Conv Layer\nbefore GAP\nX\nConvolutionX d\nweights for the ith class\nd channels( )Input Frame\nInput FrameFully\nConnected\nLayer\ngg\ng\ngscaled=Fig. 2: Object count prediction architecture. Utilize ﬁrst few\nlayers of an image classiﬁer (e.g., VGG19). The last feature\nmapfmfees into a fully connected later with ReLU providing\nobject counts per class. The weights of the i-th class for the\nfully connected layer combined with fmprovides ag\u0002ggrid\nfor the locations of objects of class i.\nimportance of Akfor classc. For the class score we have\nSc=P\nkwc\nkP\ni;jak(i;j) =P\ni;jP\nkwc\nkak(i;j). We refer\nto:\nMc(i;j) =X\nkwc\nkak(i;j) (1)\nas the class activation map of cwhere each element in the\nmap at location (i;j)isMc(i;j). In consequence Mc(i;j)\nsigniﬁes the importance of location (i;j)of the spatial map\nin the classiﬁcation of an image as class c. Scaling the spatial\nmap to the size of the image we can identify the image regions\nmost relevant to the image category, namely the regions that\nprompted the network to identify the speciﬁc class cas present\nin the image. Figure 3 presents an example. It showcases the\nclass activation maps (as a heatmap) for different images clas-\nsiﬁed as containing a human. It is evident that class activation\nmaps localize objects as they highlight object locations most\ninﬂuential in the classiﬁcation. We develop this observation\nfurther to yield effective ﬁlters for our purposes.\nOur approach consists of utilizing the class activation map\nfor each object class to count and localize objects. However\nwe improve accuracy by regulating the activation map for each\nclass via training. We adopt the ﬁrst ﬁve layers of VGG19 [57]\nnetwork pre-trained on ImageNet [31]. Experimental results\nhave demonstrated that the ﬁrst ﬁve layers provide a favourable\ntrade off between prediction accuracy and time performance\n4.\nThe ﬁfth layer is a feature map fmof sized\u0002g\u0002g, where\nd= 256 andg= 56 following the default parameters of the\nVGG network. We will adopt those, with the understanding\nthat they can be easily adjusted as part of the overall network\narchitecture. In our approach, the feature map is fed through\na global average pooling layer followed by a fully connected\n4Branching at layer 5 provides accuracy close to 90% requiring around 1ms\nto evaluate through the ﬁrst ﬁve layers. In contrast placing the branch at layer\n15, provides accuracy close to 92% increasing latency to 1.5ms. Clearly there\nis a trade-off between accuracy and processing speed which can be exploited\nas required\n\n4\nlayer with ReLU activation to output an ndimensional vector\nthat represents the count for each of the nclasses. The\narchitecture is as depicted in Figure 2 (top). In addition the\nnetwork outputs a class activation map Mc(i;j)of size 56\nx 56 for each class cproduced by the dot sum product of\nthe feature map fmand the weight vector wc\nk(k2[1:::d])\n(equation 1) of the fully connected layer, for each 1\u0014c\u0014n.\nThis map is up-scaled to the original image size producing\na heat map per class which localize the objects detected for\neach class. A cell of the map, corresponds to an image area.\nWe threshold the value of each cell of the map in order to\ndetect whether an object of the speciﬁc class is present in the\ncorresponding area of the image. That way objects of each\nclass are localized in each cell of the map, allowing us to\nevaluate spatial constraints between objects as well as between\nobjects belonging to speciﬁc classes.\nFig. 3: Sample Class Activation Maps: Localizing Objects\nexploring neural activation during classiﬁcation.\nDuring training we use the following multi-task loss to train\nthe network for performing both count and localization.\nL=X\nc2classesweightc\u0001(\u000b\u0001SmoothL 1(xc;^xc)+\f\u0001MSE (yc\u0000^yc))\n(2)\nWhereweightcis a weight for class c(computed as the\nfraction of frames in the training set containing class c),\u000band\n\fare parameters for balancing the loss between the two tasks\nandx;^x;y;^yare respectively the count prediction, the count\nlabel (ground truth), the class activation map and ground-truth\nobject location map. Notice that involving ^yin the training\nprocess regulates the network to understand object location\nas well. The training objective utilizes SmoothL 1[10]. The\nlabels for both count and object location maps (ground truth)\nare generated by Mask R-CNN. Speciﬁcally, the location map\nis produced by down-scaling the locations of the Mask R-CNN\nbounding boxes in the image to size 56\u000256for each class.\nWhen optimizing for localization, we ﬁx the weights of the\nfully connected layer and only back-propagated the error to the\nfeature layers of VGG. During training, we ﬁrst only optimize\ncount by setting parameter \fto zero, and after 5 epochs, set\n(\u000b;\f)to(1;10), and gradually decrease \fwhile keeping \u000b\nﬁxed at 1. We found that using this method, the network\nconverges much faster than optimizing both tasks from the\nstart. The training process is optimized with Adam Optimizer\n[26] with the only data transformation being normalization and\nrandom horizontal ﬂip.\nObtaining estimates for our various ﬁlters follows naturally\nfrom the network output. The IC\u0000CF (count of all objects)\nas well asIC\u0000CCF (class count ﬁlter. i.e., count of objects\nper class) is obtained from the output of the network as it\nprovides a vector with the count of the objects per class\ndetected in the frame. In addition IC\u0000CLF (class location\nFig. 4: Object Center prediction architecture. Utilize ﬁrst\nfew (e.g.,six) layers of an object detector (e.g., YOLOv2) and\nbranch out to the proposed network (with g=56) utilizing the\nfeatures for object prediction on a grid. Branching can take\nplace at later layers as well.\nﬁlter) are obtained by manipulating the class activation maps\nfor each object class directly. An activation map for class c\nafter thresholding is a binary vector of size g\u0002g(56\u000256in\nour case) indicating whether or not there is an object of class\ncin the corresponding area of the image. Spatial constraints\nbetween objects can be evaluated in a straightforward manner\nmanipulating the thresholded activation maps.\nB. OD Filters\nState of the art object detection algorithms typically work as\nfollows: they apply a series of convolution layers (each with\na speciﬁc size and number of feature maps) to extract suitable\nfeatures from an image frame, followed by the application of\na sequence of region proposals (boxes of certain width and\nheight) to determine the exact regions inside an image that\ncontain objects of a speciﬁc class. Although passing an image\nthrough convolution layers for feature extraction is relatively\nfast, the later steps of assessing object regions and object class\nare where the bulk of the computation is spend [19].\nWe observe that from an estimation point of view how-\never, being able to assess object count as well as spatial\nconstraints among objects, does not necessarily require precise\nidentiﬁcation of the entire objects. ICﬁlters utilized object\nlocation only via regulation during training. There is no\nother object location information on the network as object\nclassiﬁcation networks are not designed to deal with object\ndetection. We thus explore a hybrid approach. We utilize the\nsolid capabilities of object detection techniques to localize\nobjects of speciﬁc type in a frame, without the overhead to\ndiscover the object extend in the image. Discovery of the\nextend is the most time consuming part of object detection [13]\nand not required for estimation purposes. Thus, we propose to\nutilize an object detection network augmented with a branch to\nconduct object localization and class detection utilizing only\nthe ﬁrst few convolution layers.\nThe approach is summarized in ﬁgure 4. Our basic archi-\ntecture consists of a few convolution layers from an object\n\n5\ndetection framework. We adopt YOLOv2 [51], [52], [53] but\nany other framework will be suitable as well [10], [12], [54],\n[19]. YOLOv2 utilizes Darknet-19 with 19 convolution layers.\nOur approach is to pass an image only through kof such\nlayers extracting features that assist in the estimation of object\nlocations in an adjustable image grid of size g\u0002g. The features\nextracted after the image passes through klayers are utilized\nas input into a network created as a branch from the underlying\nobject detection architecture.\nThe parameters of the network at the branch are depicted in\nFigure 4. The network has three convolution layers followed\nby global average pooling and a fully connected layer that\nproduces the ﬁnal answer. The intuition for this architecture\nis to utilize the convolution layers learned for detection of\nobjects from the object detection network (YOLOv2 in this\ncase) but instead of proceeding with the exact detection of\nobject extents, branch to a network that uses the regularized\nheat-map of the ICapproach to pin point image frame grid\ncells that contain objects. We demonstrate that this improves\nthe detection and localization process, as the network is trained\nto recognize as well as localize objects from scratch.\nWe set the image frame input of YOLO to 448\u0002448pixels.\nThus, if we set the branch at layer eight ( k=8) its feature map is\na256\u000256\u000256vector. The branch network therefore produces\na per-class regression output for count and a 56\u000256(g=56)\ngrid. The three convolution layers of the branch network are\nof size 56\u000256\u0002512. The ﬁrstklayers of Darknet are thus\nshared between the task of YOLO detection and count/object\nlocation estimation.\nPrior to training, we use a state of the art object detection\nnetwork (Mask R-CNN [54]) to annotate the training set and\nasses ground truth. The object detection network is initialized\nwith pre-trained weights from MS-COCO [35]. The network\nis trained end-to-end minimizing the sum of the loss function\nof YOLOv2 [52] and the loss of the branch network which is\ndeﬁned as:\nLbranch =CX\nc=0[\u0015count \u0001SmoothL 1(countc;^countc)+\n\u0015grid\u00011\ng2(Aobj\nicg2X\ni=0\u0015obj\u0001(xc\ni\u0000^xc\ni)2+\nAnoobj\nicg2X\ni=0\u0015noobj \u0001(xc\ni\u0000^xc\ni)2)] (3)\nIn the equation, countcis the count prediction for the cth\nclass,xc\niis the prediction for an object of class cat theithcell.\nAobj\nicandAnoobj\nic are respectively the mask indicating whether\nor not an object of class cexists at grid location i. We sum\nover a subset of classes Cfrom MS COCO, and balance the\nloss for the counts and the loss from the grid with \u0015count and\n\u0015grid. In addition, we set \u0015noobj and\u0015objfor balancing the\nfalse positive and false negative loss when summing over the\ng2spatial locations of the grid.\nFor the case of YOLOv2 we use the training data as\nannotated by Mask R-CNN when computing the loss (that\nincludes both object types and object extents). For the case\nof the branch network, the labels for both count and object\n1414\n1024 1414\n512N\n1 1024\n1414\nGlobal\nAverage  \nPooling  Fig. 5: Object Counting Network Branch. Utilize ﬁrst few\n(e.g.,ﬁve) layers of an object detector (e.g., YOLOv2) and\nbranches out to this network utilizing the features for object\ncount prediction. Branching can take place in later layers as\nwell.\nLayer Filters Size Padding Activation\nConv1 1024 1 1 LeakyReLU\nConv2 512 3 1 LeakyReLU\nConv3 1024 1 0 LeakyReLU\nConv4 1024 1 3 LeakyReLU\nTABLE I: Network parameters for object count prediction\nlocation maps (ground truth) are generated by Mask R-CNN\nas well. Speciﬁcally, as in the case of ICﬁlters, the location\nmap is produced by down-scaling the locations of the Mask\nR-CNN bounding boxes in the image to the size of the grid,\nfor each class.\nSince the output of the branch network is exactly the same\nas in theICapproach, the estimates for the OD ﬁlters are\nproduced in exactly the same way by manipulating the output\nof the branch network which provides a vector with the counts\nof objects per class (for the case of OD\u0000CFandOD\u0000CCF\nﬁlters). Similarly OD\u0000CLF are obtained in the same way\nmanipulating the activation maps of each class.\n1) Object Count Classiﬁer: We present an alternative ap-\nproach in which the optimization objective is strictly to predict\nthe number of objects in the image frame. We follow the same\nmethodology as in Section II-B placing a network as a branch\nof an object detection architecture. The network utilizes the\nobject detection features learned by the k-th convolution layer\nas input. The features are max-pooled to ( F,f,f) where\nFis the number of ﬁlters in the k-th convolution layer and\nf\u0002fthe ﬁlter size. However the network is exclusively\ntrained to predict the number of objects in the frame and its\narchitecture is depicted in Figure 5 and Table I. We refer to\nthis ﬁlter as Object detection, count optimized classiﬁcation\nﬁlterOD\u0000COF .\nPrior to training, we are using Mask R-CNN to annotate\nthe training set. To produce the labels for training our model,\nwe obtain the number of objects for each frame detecting all\nobjects and counting them. The proposed network is trained\nend to end minimizing the loss function. The speciﬁcs of\ntraining and network parameters are detailed further in Section\nIV.\nIII. M ONITORING AGGREGATES\nReal time video streaming, like any streaming data source\n[27], provides opportunities for useful queries when one ac-\ncounts for the temporal dimension. Object presence along with\nspatial constraints offer numerous avenues to express temporal\n\n6\naggregate queries of interest. Revisiting the example of Figure\n1(b) one would wish to evaluate the following query5:\nSELECT cameraID, count(frameID),\nC1(F1(vehBox1)) AS vehType1,\nC3(F3(SignBox1)) AS SignType2,\nC2(F2(vehBox1)) AS vehColor\nFROM (PROCESS inputVideo\nPRODUCE cameraID, frameID, vehBox1\nUSING VehDetector, SignBox1 USING SignDetector)\nWHERE vehType1 = car AND vehColor = blue\nAND vehType2 = Stop-Sign\nAND (ORDER(vehType1,SignBox1)=RIGHT)\nWINDOW HOPING (SIZE 5000, ADVANCE BY 5000)\nseeking to report the number of frames in a batch window\nof 5000 frames for which a blue car has a stop sign on its\nright in a road surveillance scenario6. Similarly one could seek\nto report the average number of bicycles in a road segment\n(e.g. bike lane, identiﬁed by a bounding box in the frame) per\nhour. From a query evaluation perspective, one can utilize a\nsampling based approach evaluating each sampled frame for\nall objects present (applying a state of the art object detection\nalgorithm) and determining their spatial constraints to identify\nwhether they satisfy the query constraints. Essentially for each\nframe sampled, we determine a value (boolean or otherwise)\nthat can be utilized in conjunction with sampling based\napproximate query processing techniques [5], [45], [55], [4]\nto derive statistical estimates of interest (e.g., mean, variance\nestimates as well as obtain conﬁdence intervals for the query\nanswer).\nThe various ﬁlters introduced thus far, can also be utilized\nto improve query estimates and in particular reduce the sample\nstandard deviation of the required estimates. We discuss a\nwell known Monte Carlo technique called control variates\n(CV) [14] and its application to monitoring aggregate queries.\nMoreover, since our problem involves queries encompassing\nmultiple objects in a frame and their constraints, we demon-\nstrate howCVcan be extended in this case as well introducing\nMultiple Control Variates .\nLetYbe a random variable whose mean is to be estimated\nvia simulation and Xa random variable with an estimated\nmean\u0016X. In our trial the outcome Xiis calculated along\nwithYi. Further assume that the pairs (Xi;Yi)are i.i.d, then\ntheCV estimator ^YCVofE[Y]is^YCV=1\nnPn\ni=1(Yi\u0000Xi+\n\u0016X). It is well known [14] that the CV estimator is unbiased\nand consistent. The variance of the estimate is Var(^YCV) =\n1\nnVar(Y\u0000X+\u0016X) =1\nn(\u001b2\nY+\u001b2\nX\u00002\u001aXY\u001bX\u001bY), indicating\nthat theCV estimate will have lower variance when \u001b2\nX<\n2\u001aXY\u001bX\u001bY, where\u001aXYis the correlation coefﬁcient between\nYandX.\nTo get full advantage of the CV estimate, a parameter\n\fis introduced and optimized to minimize ^YCV, namely\n^YCV=^Y\u0000\f(^X\u0000\u0016X)with variance Var(^YCV) =\n1\nn(\u001b2\nY+\f2\u001b2\nX\u00002\f\u001aXY\u001bX\u001bY). Minimizing for \fwe get\n\f\u0003=\u001bY\n\u001bX\u001aXY=Cov(Y;X)\nVar(X). This reveals that the reduction\n5The query is simpliﬁed to ease notation. One has also to account for the\ntrackid assigned via object tracking [61] to each blue car identiﬁed as it\nenters and leaves the screen so to associate the aggregate to the same blue\ncar in the batch window.\n6At typical 30 frames per second (fps) one can deduce when the count is\nhigher than a threshold whether the car maybe parked.of variance depends on the correlation of the estimated vari-\nable (Y) and the control variate ( X). Substituting \f\u0003in the\nexpression of variance Var(^YCV(\f\u0003)) = (1\u0000\u001aXY)\u001b2\nY.\nIn practice Cov(X;Y)is never known. However we\ncan certainly estimate sample values from SXX =\n1\nn\u00001Pn\ni=1(Xi\u0000^X)2andSYX=1\nn\u00001Pn\ni=1(Xi\u0000^X)(Yi\u0000^Y),\nproviding an estimator \f\u0003=SXYS\u00001\nXX.\nCV’s blend in our application scenario naturally. Yis\nthe variable we like to estimate and Xis the result of the\napplication of one of the introduced ﬁlters. Since Xestimates\nYwe expect (provided the the ﬁlters are good estimators) that\nthe two variables would be highly correlated and as a result\nCV estimation would provide good results. For example, for\nthe case of estimating the average number of bicycles in a\nbike lane (where the bike lane is identiﬁed by a region on the\nscreen) over a time period, Yiis the result of the application\nof full object detection for objects falling inside the bike lane\nregion on a frame and Xiis the application of a CLF ﬁlter\non the frame. The two quantities are estimated by sampling a\nnumber of frames over the speciﬁed time period (e.g., one\nhour) applying standard sampling based techniques. In the\nestimation, we use as \u0016Xthe sample mean ^\u0016Xover the\nsampledXi’s. The estimated variance however will be tighter\nutilizing the CV estimates. Thus CV provides a way to trade\nimproved accuracy for small increase in processing time per\nsample, as in addition to applying the full object detection\nalgorithm we also have to apply the ﬁlter. Filter time per\nsample is a tiny fraction of the object detection time (see\nSection IV), making CV appealing. Similarly, in the case of\ncar next to a stop sign for more than 10 minutes query,Yiis\nthe answer of the application of full object detection followed\nby processing the identiﬁed objects to assess whether there\nis a car next to a stop sign in the frame and Xiis aCLF\nthat estimates the locations of cars and stop signs on a grid\nand assess the constraint. Given the frame per second rate of\nthe video, we can identify how many frames constitute the\ntime range of interest (e.g., 10 minutes); then use sampling\nto identify whether the predicate is true for the entire time\ninterval and the associated variance applying CV.\nA. Multiple Control Variates\nIn certain applications, aggregate monitoring queries can\nobtain reduced variance estimates by involving multiple con-\ntrol variates. For example consider a query inquiring for the\naverage number of bicycles and trucks entering simultaneously\nfrom the right and left of the image respectively in a trafﬁc\nmonitoring scenario. We demonstrate how it is possible to\ngeneralize control variates to more than one variables.\nLetZ= (Z1:::Zd)be a vector of control variates with\nestimated expected values \u0016Z= (\u00161:::\u0016d). In this case,\n^YCV(\f) =^Y\u0000\f(Z\u0000\u0016Z)is an unbiased estimator of \u0016Y[14].\nFollowing the case for a single control variate, the optimal\nvalue for\fif provided by \f\u0003=P\u00001\nZZP\nYZ, whereP\nZZ\nandP\nYZare the covariance matrix of Zand the vector of\ncovariances between (Y;Z). In practice the covariance matri-\nces will not be known in advance. Thus they are estimated\nusing samples, SZjZk=1\nn\u00001Pn\ni=1(Zj\ni\u0000^Zj)(Zk\ni\u0000^Zk)for\n\n7\nVideo Stream FramesSample Sample Sample\nFig. 6: A number of frames will be sampled, for each frame\nsuitable all suitable ﬁlters are applied and control variates is\ndeployed to estimate the aggregate.\nj;k2[1:::d]andSYZj=1\nn\u00001Pn\ni=1(Yi\u0000^Y)(Zj\ni\u0000^Zj),\nwithj2[1:::d]. Following the same methodology as in the\ncase of a single variate one can show that Var(^YCV(\f\u0003)) =\n(1\u0000R2)Var(^Y)whereR2=P0\nY ZP\u00001\nZZP\nY Z\n\u001b2\nYis the squared\nmultiple correlation coefﬁcient, which is a measure of variance\nof^Yexplained by Zas in regression analysis.\nFor example consider a query seeking to estimate the\nnumber of frames in which bicycles and trucks exist having\ndifferent spatial constraints each (e.g., objects exist in speciﬁc\nscreen areas) in frames having more than three cars. In this\ncase we would employ one CLF to obtain an estimate\nfor bicycles, an additional for trucks and one more for car\ncounts and utilize multiple control variates to obtain a reduced\nvariance estimate for this query. Figure 6 presents the overall\napproach. In each sampled frame, all applicable ﬁlters are\napplied to qualify the frame. In this case CLF for bicycles\nand trucks to assess the constraint as well as a count ﬁlter for\ncars. If the frame is qualiﬁed by the applicable ﬁlter, control\nvariates is utilized for the aggregate.\nIn section IV we will evaluate the ability of CV to reduce\nvariance for different types of aggregate monitoring queries.\nIV. E XPERIMENTAL EVALUATION\nWe now present the results of a detailed experimental eval-\nuation of the proposed approaches. We utilize three different\ndata sets namely, Coral ,Jackson (Jackson town square), and\nDetrac [62]. Of these data sets, Coral ,Jackson are video\nsequences shot from a single ﬁxed-angle camera, while Detrac\nis a trafﬁc data set composed of 100 distinct ﬁxed angle video\nsequences shot at different locations. In order to maintain the\nconsistency of our models, we annotate the three data sets\nusing the Mask R-CNN Detector.\nTheCoral data set is an 80 hour ﬁxed-angle video sequence\nshot at an aquarium. Similarly, Jackson is a 60 hour ﬁxed-\nangle sequence shot at a zoomed-in trafﬁc intersection. Finally,\nDetrac consists of 10 hours of ﬁxed-angle trafﬁc videos shot\nat various locations in China. We did not use the provided\ntraining set label, and instead annotated the data set with\nvarious classes of vehicles.\nTo evaluate our model, we partition the video sequences\nto create a training, validation, and test set for each data\nset. For Coral andJackson , we create our data set with the\nmp4 ﬁles published by NoScope [25]. The Detrac data set\ncontains 60 and 40 different sequences for training and testing\nrespectively. For the purpose of our experiment, we combine\nthe train and test set of the original data set and partition theDataset Train Size Test Size Obj/Frame std Classes\nCoral 52000 7215 8.7 5.1 Person\nJackson 14094 3000 1.2 0.5car (80%)\nPerson (20%)\nDetrac 55020 9971 15.8 9.8car (92%)\nbus (6%)\nTruck (2%)\nTABLE II: Datasets and their characteristics\nordered frames into train, validation, and test set with equal\nratios between sequences. Table II presents a description and\nkey characteristics of each data set.\nWith our experiments we seek to quantify the accuracy of\neach of the ﬁlters introduced as well as their effectiveness\nwhen applied in conjunction with declarative queries on video\nstreams. We also quantify the variance reduction resulting\nfrom the application of the proposed control variates approach\non aggregate queries involving various types of spatial predi-\ncates. Finally we demonstrate the effectiveness of our approach\nto identify unexpected objects on video streams, demonstrating\nimage frames ﬂagged as unexpected in sample videos.\nFor the case of ICﬁlters we utilize VGG19. Placing the\nbranch at layer ﬁve results in a grid of size 56\u000256. In\nthis case the time required to pass the frame through the\nnetwork and through the branch to derive our estimations\nis approximately 1.5ms/frame. Similarly for OD ﬁlters we\nutilize YOLOv2. Placing the branch at layer eight, results in\na grid size of 56\u000256as well. The time required to pass\na frame through the ﬁrst eight layers and also through the\nﬁlter is 1.9ms/frame. For our experiments we choose to place\nthe branches at layers as speciﬁed above in order to compare\nboth techniques on at 56\u000256grid. Placing the branch at\nhigher layers introduces a trade-off. Processing time per frame\nincreases (as we have to pass through more layers). At the\nsame time count accuracy increases (by a few percentage\npoints according to our experiments) for all techniques as\nhigher level features are discovered in later layers that improve\ncounts. However the grid size becomes smaller due to the\nnetwork architecture (e.g., 28\u000228or14\u000214depending on\nthe layer) that penalizes location accuracy (up to 8% lower\nacross all techniques than the results reported below according\nto our experiments). We note however that the results do not\nchange qualitatively than those reported below for a grid of\nsize56\u000256. In comparison the time required per frame by\nMask R-CNN is 200ms. Similarly for comparison, passing\na frame through the entire YOLOv2 requires 15ms. This\nprovides good localization accuracy for objects (approximately\n3-5% higher than those we report, across data sets for the case\nofOD\u0000CLF ) but results in poor counting accuracy as the\nYOLO network in itself is trained exclusively for location and\ndoes not incorporate counts.\nBoth networks are trained end to end utilizing the same\ndata sets, using the training and test sets as depicted in table\nII. Prior to training the model for IC, we ﬁrst set the loss\nfunction hyper-parameters using data from the training set (See\nSection II-A). For all three data sets, we use ADAM optimizer\nwith learning rate 10\u00004and exponential decay of 5\u000210\u00004,\nand exit training when the performance on the validation set\nbegins to drop. For the Coral andJackson data sets 10 epochs\n\n8\nFig. 7: Accuracy of object count ﬁlters on the three data sets\nof training were required to achieve optimal performance. The\nmore complex Detrac dataset with three classes requires 20\nepochs to converge. We train the model for OD techniques\nusing stochastic gradient descent with a exponential weight\ndecay of 5\u000210\u00004and momentum of 0:9. However, we chose\nto use a smaller learning rate of 10\u00004due to unstable gradients\nat the added branch. We train Coral andJackson for 4 epochs\nand the Detrac data set for 6 epochs. The hyper-parameters\nof the loss function were set manually based on the data from\nthe training set (see Section II-B). For OD techniques we\nthreshold the grid cell to determine the presence of an object\nusing a threshold of 0.2.\nWe implement our models with the PyTorch framework [46]\nand perform all experiments on a single Nvidia Titan XP GPU\nusing an HP desktop with an Intel Xeon Processor E5-2650\nv4, and 64GB of memory.\nA. Filter Accuracy\nIn this set of experiments we quantify the accuracy of the\nvarious proposed ﬁlters. Figure 7 presents the results of the\naccuracy of the OD\u0000COF ,IC\u0000CF andOD\u0000CF ﬁlters\nfor the three data sets. For a frame fletcfbe the number\nof objects in fand ^cfbe the ﬁlter estimate. Accuracy is\ndeﬁned as the fraction of frames for which ^cf=cf. We also\npresent two variants for each ﬁlter annotated with postﬁx 1\nand2to assess approximate counts. Filters OD\u0000COF\u00001\n(correspondingly IC\u0000CF\u00001,OD\u0000CF\u00001) quantify the\nfraction of frames ffor whichcf\u00001\u0014^cf\u0014cf+1. Similarly\nOD\u0000CF\u00002(correspondingly IC\u0000CF\u00002,OD\u0000CF\u00002)\nquantify the fraction of frames for which cf\u00002\u0014^cf\u0014cf+2.\nIt is evident that for the Coral data set all three techniques\nperform the same when estimating the exact count of objects\nin frames. Accuracy increases very quickly for all techniques\nforCF\u00001andCF\u00002ﬁlters. All techniques appear to equally\nunderestimate and overestimate the true count of objects. In\nthe case of this data set the average number of objects per\nframe is 8.5 and the variance 5.1. The situation is similar in\nthe case of the Jackson data set; even though the data set\nhas two classes of objects the average number of objects per\nframe as well as the variance across frames is low and all three\nestimation techniques present comparable results. In the case\nofDetrac the situation is a bit different. The average number of\nobjects per frame is higher as well as the variance. Moreoverthe data set has three classes and the most popular class (car)\nhas high variance in itself. In this case OD\u0000COF performs\npoorly; evidently utilizing the convolution features only for\ncount estimation is ineffective as the number of objects per\nframe increases. The other two techniques are competitive\nand become much better for CF\u00001andCF\u00002ﬁlters\n(approximate counts). The ICtechniques perform slightly\nbetter than the OD techniques for count estimation. Since\nthe network for ICis pre-trained on ImageNet to perform\nclassiﬁcation, the feature maps seem to provide dense and\ndiscriminate context of the objects which appears more suited\nfor transfer learning tasks such as counting. This is evident\nin Figure 11 which presents the count estimates per class\nfor the three data sets. For Coral (Figure 8) we observe that\nthe two techniques perform almost equally. In the case of the\nJackson data set (Figure 9) ICtechniques perform better when\nconsidering exact counts, but both techniques present similar\naccuracy for approximate counts (within count one and two).\nFinally the same trend is conﬁrmed also on the Detrac data\nset (Figure 10). It is worth noticing that the techniques present\nhigher accuracy for classes that are less popular. Even though\nthe less popular classes in the video stream are represented\nby less training examples, less popular objects in frames have\ntypically low counts in each frame. They thus constitute an\neasier estimation problem. In summary COF techniques do\nnot appear competitive for cases with larger number of objects\nper frame. Moreover ICandOD,CCF ﬁlters demonstrate\ncomparable accuracy, with IC\u0000CCF ﬁlters having a slight\nadvantage in exact counting.\nFigure 15 presents the results for the case of CLF ﬁlters.\nWe aim to explore how well the two techniques are able to\nestimate object locations. Figure 12 presents the results for the\ncase of the Coral data set. The y-axis measures the f1score\n7of the estimation. Each estimation takes place on a 56\u000256\ngrid in which we predict the presence of objects of a speciﬁc\nclass. For each prediction we quantify the true positives, false\npositives and false negatives and compute precision and recall\n(andf1measure) of object detection over all frames. OD\ntechniques demonstrate high accuracy localizing the exact grid\ncell that objects are located. As in the case of count estimation\nwe also present two variants of the ﬁlter annotated with postﬁx\n1and2. FiltersIC\u0000CLF\u00001andOD\u0000CLF\u00001assess\nthe localization of the grid cell prediction as correct if an\nobject of the same class as the predicted one, lies in a cell\nat Manhattan distance one from the predicted cell (any of the\nfour adjacent grid cells of the predicted one). Similarly for\nIC\u0000CLF\u00002andOD\u0000CLF\u00002ﬁlters, we expand the\ngrid to Manhattan distance two around the predicted cell. The\nintuition is that spatial constraints are still preserved if the\nprediction is ”close” (in cell distance) to the actual location\nof the object. We will evaluate the effectiveness and accuracy\nof these ﬁlters in sample queries. Overall OD localization\ntechniques perform much better as they are optimized for\n7Deﬁned as a function of true positives (tp: objects identiﬁed correctly\nin the validation set), false positives (fp: objects identiﬁed erroneously in\nthe validation set) and false negatives (fn: objects failed to identify in the\nvalidation set) compared to ground truth. In particular f1 =2\u0002p\u0002r\np+r, where\np=tp\ntp+fpandr=tp\ntp+fn\n\n9\nFig. 8: Coral Dataset\n Fig. 9: Jackson Dataset\n Fig. 10: Detrac Dataset\nFig. 11:CCF performance across data sets for varying number of classes\nFig. 12: Coral Dataset\n Fig. 13: Jackson Dataset\n Fig. 14: Detrac Dataset\nFig. 15:CLF performance across data sets for varying number of classes.\nobject location prediction. The network for OD ﬁlters is an\nobject detection network and thus the corresponding feature\nmaps provide more details regarding the spatial and location\nfeatures of the objects in the image.\nFor the Jackson data set (Figure 13) as well as Detrac\n(Figure 14) which have two and three classes respectively, OD\ntechniques demonstrate superior performance. We also observe\nthat less popular classes (as is the case of person in Jackson\nand Truck and Bus in Detrac ) have lower f1score and this\nis an artifact of training; real videos contain unbalanced (in\nrelative frequency of occurrence) object classes; less popular\nclasses in the video stream present less training examples in\nthe network and as a result the accuracy of location estimation\nis lower (unlike counts, predicting location is much harder).\nIn summary OD techniques provide superior accuracy for\nlocalization and seem highly suitable for evaluating spatial\nconstraints among objects in frames. At the same time they\nremain competitive for estimating object counts.\nWe also conducted an experiment comparing OD tech-\nniques to identify a constraint between two object classes (car\nleft of a bus), against a data set that is manually annotated to\nrecognize the constraint. Our results indicate that the proposed\nOD ﬁltering approach achieves superior accuracy, at 99%\nagainst a manually annotated data set. However the additional\nbeneﬁt of our approach is generality as we do not require\ntraining for any possible class of object and any possible\nconstraint among the objects.\nB. Query Results\nWe now present an evaluation of the effectiveness of these\nﬁlters in a query processing setting. We consider the followingqueries: For Coral data set: identify all frames with two people\n(q1) as well as identify all frames with two people in the lower\nleft quadrant of the frame ( q2). For Jackson data set, identify\nall frames with exactly one car and exactly one person ( q3),\nidentify all frames with at least one car and at least one person\n(q4), identify all frames with exactly one car and exactly one\nperson and the car left of the person ( q5). For Detrac , identify\nthe frames with exactly one car and exactly one bus ( q6) and\nidentify the frames with exactly one car and exactly one bus\nand the car to the left of the bus ( q7).\nFor these queries we progressively enable all applicable\nﬁlters and for the frames that pass each ﬁlter we evaluate\nthe entire frame with an object detector (Mask R-CNN) for\nthe ﬁnal evaluation. For the queries that involve only counts\nwe assess accuracy as the fraction of frames that are correctly\nidentiﬁed by our ﬁlters over the number of frames in which\nthe query predicates are true (ground truth). For the queries\nthat involve spatial constraints we report the f1measure for\neach query. For count estimation and class count estimation we\ndeployOD\u0000CCF ﬁlters and for evaluating spatial constraints\nwe deployOD\u0000CLF ﬁlters. We also evaluate each query in\na brute force manner annotating all frames with Mask R-CNN\nto determine the true frames in the query results. To run Coral\nthrough Mask R-CNN requires 5.2 hours, Jackson 3.89 hours\nandDectrac 7.14 hours. Table III presents the query results.\nIn the ﬁgure we present the most selective ﬁlter combinations\nthat yield 100% accuracy (with the exception of query q7in\nwhich the resulting accuracy is 93%).\nAs is evident from the table, the performance advantage\noffered by the ﬁlters proposed are very large. In most of\nthe cases, without loss in accuracy the ﬁlters offer orders\n\n10\nFilter q1q2q3q4q5q6q7\nOD-CCF - - 87.4 122.6 - - -\nOD-CCF-1 909.4 - - - - 367.6 -\nOD-CCF-1/OF-CLF - 427 - - - - -\nOD-CCF/OD-CLF-1 - - - - 67.6 - -\nOD-CCF-1/OD-CLF-2 - - - - - - 293.4*\nTABLE III: Execution times (sec) and ﬁlter combinations to\nachieve 100% accuracy ( q7accuracy is 93%).\nQuery Filter + Mask RCNN Variance Reduction\na1 201.6ms 48\na2 201.6ms 12\na3 202.2ms 38\na4 201.6ms 230\na5 202.2ms 89\nTABLE IV: Sample estimation queries, time per frame to\nenable applicable ﬁlters and Mask R-CNN execution and the\nresulting reduction in variance.\nof magnitude performance improvements, enabling running\ncomplex queries on video streams much faster than it was\npossible before.\nC. Aggregates\nWe now turn to evaluate correlated aggregates and their\napplicability to reduce the variance for aggregate estimation of\nqueries involving multiple objects as well as spatial constraints\namong them. We focus on ﬁve sample queries in all 3 data\nsets, namely in the Jackson data set, estimate the number of\nframes having a car on the lower right quadrant ( a1) as well\nas a query estimating the number of frames with a car on the\nleft of a person ( a2), for Detrac estimate the number of frames\nwith three objects and a car in the lower left quadrant and a bus\nin the upper left ( a3) and estimate the number of frames with\na car left of a bus ( a4). Finally for Coral estimate the number\nof frames with three people out of which at least two people\nare in the lower left quadrant ( a5). The results are depicted\nin Table IV. We execute the query sampling randomly from\nthe stream; each query is executed one hundred times and we\nreport averages. The straightforward way to do the estimation\nis to evaluate each sampled frame with Mask R-CNN and\nderive a statistical estimate of the results. Applying correlated\naggregates to enable the proposed ﬁlters to derive an estimate\nof the correlated aggregate and then apply Mask R-CNN. In\nthe ﬁnal estimation we utilize the correlated aggregate aiming\nto reduce the variance of the estimate (as per Section III). We\nreport the time required per frame for our ﬁlters (correlated\nvariables) followed by and including Mask R-CNN reporting\non the corresponding reduction in variance for each query\nestimate. The time to run Mask R-CNN in each frame is\n200ms. It is evident that variance is reduced substantially with\na small increase in the processing time per frame.\nV. R ELATED WORK\nRecently there has been increased interest in the application\nof Deep Learning techniques in data management [29], [39],\n[18], [6], [60]. NoScope [25] initiates work in surveillance\nvideo query processing. The authors address fast query pro-\ncessing on video streams focusing on frame classiﬁcationqueries, namely identify frames that contain certain classes\nof objects involved in the query. They train deep classiﬁers\nto recognize only speciﬁc objects, thus being able to utilize\nsmaller and faster networks. They demonstrate that ﬁltering\nfor speciﬁc query objects can improve query processing rate\nsigniﬁcantly with a small loss in accuracy. In subsequent work\n[23], [24] the authors introduce a query language for express-\ning certain types of queries on video streams. They also discuss\nsampling based techniques inspired by approximate query\nprocessing to answer certain types of approximate aggregate\nqueries on a video stream as well as use control variates\ntechniques from the literature [14], for a single variable to\nreduce variance of aggregates. Our work builds upon and\nextends these works by focusing on query processing taking\ninto account spatial constraints between objects in a frame,\na problem not addressed thus far , as well as demonstrating\nhow to adapt control variates for multiple variables since\nin our problem multiple objects are involved in a query\npossibly with constraints among them. Lu et. al., [36] present\nOptasia, a system that accepts input from multiple cameras and\napplies difference video query processing algorithms from the\nliterature to piece together a video query processing system.\nThe emphasis of their work is on system aspects such as\nparallelism of query plans based on number of cameras and\noperation complexity as well as parallelism to allow multiple\ntasks to process the feed of a single camera.\nStream query processing is a well established area in the\ndatabase community [27], [40]. Although the bulk of the\nwork focused on numerical and categorical streams, numerous\nconcepts invented apply in the streaming video domain [8],\n[38], [17], [1], [15], [34], [65], [66], [33], [49], [41], [63].\nIn particular work on operator ordering [2], [37] is highly\nrelevant when considering multiple ﬁlters to reduce the number\nof frames processed. We foresee a resurgence of research\ninterest in this areas taking the characteristics of video data\ninto account. Spatial database management [56] is another\nwell establish ﬁeld in data management from which numerous\nconcepts apply when modeling objects in an image and their\nrelationships, in the case of streaming video query processing.\nIn particular past work on topological relationships on spatial\nobjects [44] is readily applicable.\nApproximate queries have been well studied in the database\ncommunity [5], [45], [55], [4], [67], [48], [28], [9], [47].\nNumerous results for different types of queries exist with\nvarying degrees of accuracy guarantees. These results are\nreadily applicable to different types of queries of interest in a\nstreaming video query processing scenario.\nRecent results in the computer vision community have\nrevolutionized object classiﬁcation [31], [57], [59], [20] as\nwell as object detection [10], [12], [54], [19], [51], [52],\n[53]. Among object detection approaches the family of R-\nCNN [10], [12], [54], [19] papers achieves strong results, but\nthe area is still under rapid improvement. Our proposed OD\ntechniques inspired by object detection utilize the YOLOv2\n[51], [52], [53] architecture, but can easily adapt any detection\nframework since all are convolutional with main differences in\nthe way the actual objects are extracted (YOLOv2 uses ideas\nfrom R-CNN as well). Similarly our proposed ICtechniques\n\n11\nutilizing classiﬁcation are based on VGG19 [57] but can easily\nadapt any classiﬁcation framework. Several recent surveys,\nsummarize the results in the areas of object classiﬁcation\nand detection [13]. The properties of deep features learned\nduring training convolutional networks for localization have\nbeen utilized before [3], [7], [42], [43], [68]. Here we utilize\nthis observation for counting. Density map estimation (number\nof people present per pixel in an image) is a problem central in\ncrowd counting [58]. The main emphasis has been on images\ncontaining hundreds or thousands of objects (e.g., people,\nanimals, etc) with speciﬁc annotations (dot annotations). In\ncontrast we focus on applications with small number of objects\nper frame, training networks of limited size with emphasis\non performance, so the techniques can be easily applied\nalong with standard training methods, for query evaluation.\nMoreover, we are also addressing the problem of counting\nper object class, which is not the focus on crowd counting\napproaches. Our motivation stems from query processing as\nopposed to counting crowds.\nA system demonstration and a prototype system encompass-\ning the techniques and concepts introduced in this paper is\navailable elsewhere [64].\nVI. C ONCLUSIONS\nWe have presented a series of ﬁlters to estimate the number\nof objects in a frame, the number of objects of speciﬁc classes\nin a frame as well as to assess an estimate of the spatial\nposition of an object in a frame enabling us to reason about\nspatial constraints. These ﬁlters were evaluated for accuracy\nand we experimentally demonstrated using real video data\nsets that they attain good accuracy for counting and location\nestimation purposes. When applied in query scenarios over\nvideo streams, we demonstrated that they achieve dramatic\nspeedups by several orders of magnitude. We also presented\ntechniques to complement our video monitoring study, that\nreduce the variance of aggregate queries involving counting\nand spatial predicates.\nThis work opens numerous avenues for further study.\nDeclarative query languages and query processors for video\nstreams is largely an open research area. Studying query\noptimization issues in our framework is an important research\ndirection. Study of additional query types involving spatial and\ntemporal predicates is a natural extension. Finally extension of\nthe ﬁlters for crowd counting and estimation scenarios is also\nnecessary.\nREFERENCES\n[1] R. Ananthanarayanan, V . Basker, S. Das, A. Gupta, H. Jiang, T. Qiu,\nA. Reznichenko, D. Ryabkov, M. Singh, and S. Venkataraman. Photon:\nfault-tolerant and scalable joining of continuous data streams. In\nProceedings of the ACM SIGMOD International Conference on Man-\nagement of Data, SIGMOD 2013, New York, NY, USA, June 22-27, 2013 ,\npages 577–588, 2013.\n[2] B. Babcock, S. Babu, R. Motwani, and M. Datar. Chain: Operator\nscheduling for memory minimization in data stream systems. In\nProceedings of the 2003 ACM SIGMOD International Conference on\nManagement of Data , SIGMOD ’03, pages 253–264, New York, NY ,\nUSA, 2003. ACM.[3] L. Bazzani, A. Bergamo, D. Anguelov, and L. Torresani. Self-taught\nobject localization with deep networks. In 2016 IEEE Winter Conference\non Applications of Computer Vision, WACV 2016, Lake Placid, NY, USA,\nMarch 7-10, 2016 , pages 1–9, 2016.\n[4] S. Chaudhuri, G. Das, and V . Narasayya. Optimized stratiﬁed sampling\nfor approximate query processing. ACM Trans. Database Syst. , 32(2),\nJune 2007.\n[5] S. Chaudhuri, B. Ding, and S. Kandula. Approximate query processing:\nNo silver bullet. In Proceedings of the 2017 ACM International\nConference on Management of Data , SIGMOD ’17, pages 511–519,\nNew York, NY , USA, 2017. ACM.\n[6] Z. Cheng and N. Koudas. Nonlinear models over normalized data. In\n35th IEEE International Conference on Data Engineering, ICDE 2019,\nMacao, China, April 8-11, 2019 , pages 1574–1577, 2019.\n[7] R. G. Cinbis, J. J. Verbeek, and C. Schmid. Weakly supervised object\nlocalization with multi-fold multiple instance learning. IEEE Trans.\nPattern Anal. Mach. Intell. , 39(1):189–203, 2017.\n[8] M. Datar and R. Motwani. The sliding-window computation model\nand results. In Data Streams - Models and Algorithms , pages 149–167.\nDBLP, 2007.\n[9] A. Galakatos, A. Crotty, E. Zgraggen, C. Binnig, and T. Kraska. Revisit-\ning reuse for approximate query processing. PVLDB , 10(10):1142–1153,\n2017.\n[10] R. B. Girshick. Fast R-CNN. In 2015 IEEE International Conference on\nComputer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015 ,\npages 1440–1448, 2015.\n[11] R. B. Girshick. Visual recognition and beyond. Computer Vision and\nPattern Recognition , 2018.\n[12] R. B. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature\nhierarchies for accurate object detection and semantic segmentation. In\n2014 IEEE Conference on Computer Vision and Pattern Recognition,\nCVPR 2014, Columbus, OH, USA, June 23-28, 2014 , pages 580–587,\n2014.\n[13] R. B. Girshick, I. Kokkinos, I. Laptev, J. Malik, G. Papandreou,\nA. Vedaldi, X. Wang, S. Yan, and A. L. Yuille. Editorial- deep learning\nfor computer vision. Computer Vision and Image Understanding , 164:1–\n2, 2017.\n[14] P. Glasserman. Monte Carlo Methods in Financial Engineering .\nSpringer, New York, 2004.\n[15] L. Golab and M. T. ¨Ozsu. Processing sliding window multi-joins in\ncontinuous queries over data streams. In VLDB 2003, Proceedings of\n29th International Conference on Very Large Data Bases, September\n9-12, 2003, Berlin, Germany , pages 500–511, 2003.\n[16] I. J. Goodfellow, Y . Bengio, and A. C. Courville. Deep Learning .\nAdaptive computation and machine learning. MIT Press, 2016.\n[17] S. Guha, A. Meyerson, N. Mishra, R. Motwani, and L. O’Callaghan.\nClustering data streams: Theory and practice. IEEE Trans. Knowl. Data\nEng., 15(3):515–528, 2003.\n[18] S. Hasan, S. Thirumuruganathan, J. Augustine, N. Koudas, and G. Das.\nMulti-attribute selectivity estimation using deep learning. CoRR ,\nabs/1903.09999, 2019.\n[19] K. He, G. Gkioxari, P. Doll ´ar, and R. B. Girshick. Mask R-CNN. In\nIEEE International Conference on Computer Vision, ICCV 2017, Venice,\nItaly, October 22-29, 2017 , pages 2980–2988, 2017.\n[20] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image\nrecognition. In 2016 IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2016, Las Vegas, NV , USA, June 27-30, 2016 , pages\n770–778, 2016.\n[21] K. Hsieh, G. Ananthanarayanan, P. Bodik, S. Venkataraman, P. Bahl,\nM. Philipose, P. B. Gibbons, and O. Mutlu. Focus: Querying large video\ndatasets with low latency and low cost. In 13th USENIX Symposium\non Operating Systems Design and Implementation (OSDI 18) , pages\n269–286, Carlsbad, CA, Oct. 2018. USENIX Association.\n[22] W. Hu, Q. Zhuo, C. Zhang, and J. Li. Fast branch convolutional neural\nnetwork for trafﬁc sign recognition. IEEE Intelligent Transportation\nSystems Magazine , 9:114–126, 10 2017.\n[23] D. Kang, P. Bailis, and M. Zaharia. Blazeit: Fast exploratory video\nqueries using neural networks. In https://arxiv.org/abs/1805.01046 ,\n2018.\n[24] D. Kang, P. Bailis, and M. Zaharia. Challenges and opportunities in\ndnn-based video analytics: A demonstration of the blazeit video query\nengine. In CIDR 2019, 9th Biennial Conference on Innovative Data\nSystems Research, Asilomar, CA, USA, January 13-16, 2019, Online\nProceedings , 2019.\n[25] D. Kang, J. Emmons, F. Abuzaid, P. Bailis, and M. Zaharia. Noscope:\nOptimizing neural network queries over video at scale. Proc. VLDB\nEndow. , 10(11):1586–1597, Aug. 2017.\n\n12\n[26] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization.\nCoRR , abs/1412.6980, 2014.\n[27] N. Koudas and D. Srivastava. Data stream query processing: A tutorial.\nInProceedings of the 29th International Conference on Very Large Data\nBases - Volume 29 , VLDB ’03, pages 1149–1149. VLDB Endowment,\n2003.\n[28] T. Kraska. Approximate query processing for interactive data science. In\nProceedings of the 2017 ACM International Conference on Management\nof Data, SIGMOD Conference 2017, Chicago, IL, USA, May 14-19,\n2017 , page 525, 2017.\n[29] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for\nlearned index structures. In Proceedings of the 2018 International Con-\nference on Management of Data, SIGMOD Conference 2018, Houston,\nTX, USA, June 10-15, 2018 , pages 489–504, 2018.\n[30] S. Krebs, B. Duraisamy, and F. Flohr. A survey on leveraging deep neural\nnetworks for object tracking. In 20th IEEE International Conference\non Intelligent Transportation Systems, ITSC 2017, Yokohama, Japan,\nOctober 16-19, 2017 , pages 411–418, 2017.\n[31] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation\nwith deep convolutional neural networks. Commun. ACM , 60(6):84–90,\nMay 2017.\n[32] Y . LeCun, Y . Bengio, and G. E. Hinton. Deep learning. Nature ,\n521(7553):436–444, 2015.\n[33] J. Li, D. Maier, K. Tufte, V . Papadimos, and P. A. Tucker. No pane,\nno gain: efﬁcient evaluation of sliding-window aggregates over data\nstreams. SIGMOD Record , 34(1):39–44, 2005.\n[34] J. Li, D. Maier, K. Tufte, V . Papadimos, and P. A. Tucker. Semantics\nand evaluation techniques for window aggregates in data streams.\nInProceedings of the ACM SIGMOD International Conference on\nManagement of Data, Baltimore, Maryland, USA, June 14-16, 2005 ,\npages 311–322, 2005.\n[35] T. Lin, M. Maire, S. J. Belongie, L. D. Bourdev, R. B. Girshick, J. Hays,\nP. Perona, D. Ramanan, P. Doll ´ar, and C. L. Zitnick. Microsoft COCO:\ncommon objects in context. CoRR , abs/1405.0312, 2014.\n[36] Y . Lu, A. Chowdhery, and S. Kandula. Optasia: A relational platform\nfor efﬁcient large-scale video analytics. In Proceedings of the Seventh\nACM Symposium on Cloud Computing , SoCC ’16, pages 57–70, New\nYork, NY , USA, 2016. ACM.\n[37] Y . Lu, A. Chowdhery, S. Kandula, and S. Chaudhuri. Accelerating\nmachine learning inference with probabilistic predicates. In Proceedings\nof the 2018 International Conference on Management of Data , SIGMOD\n’18, pages 1493–1508, New York, NY , USA, 2018. ACM.\n[38] G. S. Manku and R. Motwani. Approximate frequency counts over data\nstreams. PVLDB , 5(12):1699, 2012.\n[39] R. Marcus and O. Papaemmanouil. Towards a hands-free query opti-\nmizer through deep learning. In CIDR 2019, 9th Biennial Conference on\nInnovative Data Systems Research, Asilomar, CA, USA, January 13-16,\n2019, Online Proceedings , 2019.\n[40] S. Muthukrishnan. Data streams: Algorithms and applications. Founda-\ntions and Trends in Theoretical Computer Science , 1(2), 2005.\n[41] K. Nagaraj, K. V . M. Naidu, R. Rastogi, and S. Satkin. Efﬁcient\naggregate computation over data streams. In Proceedings of the 24th\nInternational Conference on Data Engineering, ICDE 2008, April 7-12,\n2008, Canc ´un, Mexico , pages 1382–1384, 2008.\n[42] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Learning and transferring\nmid-level image representations using convolutional neural networks.\nIn2014 IEEE Conference on Computer Vision and Pattern Recognition,\nCVPR 2014, Columbus, OH, USA, June 23-28, 2014 , pages 1717–1724,\n2014.\n[43] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Is object localization for\nfree? - weakly-supervised learning with convolutional neural networks.\nInIEEE Conference on Computer Vision and Pattern Recognition, CVPR\n2015, Boston, MA, USA, June 7-12, 2015 , pages 685–694, 2015.\n[44] D. Papadias, T. Sellis, Y . Theodoridis, and M. J. Egenhofer. Topological\nrelations in the world of minimum bounding rectangles: A study with\nr-trees. In Proceedings of the 1995 ACM SIGMOD International\nConference on Management of Data , SIGMOD ’95, pages 92–103, New\nYork, NY , USA, 1995. ACM.\n[45] Y . Park, B. Mozafari, J. Sorenson, and J. Wang. Verdictdb: Univer-\nsalizing approximate query processing. In Proceedings of the 2018\nInternational Conference on Management of Data , SIGMOD ’18, pages\n1461–1476, New York, NY , USA, 2018. ACM.\n[46] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin,\nA. Desmaison, L. Antiga, and A. Lerer. Automatic differentiation in\npytorch. In NIPS-W , 2017.\n[47] J. Peng, D. Zhang, J. Wang, and J. Pei. AQP++: connecting approx-\nimate query processing with aggregate precomputation for interactiveanalytics. In Proceedings of the 2018 International Conference on\nManagement of Data, SIGMOD Conference 2018, Houston, TX, USA,\nJune 10-15, 2018 , pages 1477–1492, 2018.\n[48] N. Potti and J. M. Patel. DAQ: A new paradigm for approximate query\nprocessing. PVLDB , 8(9):898–909, 2015.\n[49] S. Qin, W. Qian, and A. Zhou. Approximately processing multi-\ngranularity aggregate queries over data streams. In Proceedings of the\n22nd International Conference on Data Engineering, ICDE 2006, 3-8\nApril 2006, Atlanta, GA, USA , page 67, 2006.\n[50] M. Ranzato, G. E. Hinton, and Y . LeCun. Guest editorial: Deep learning.\nInternational Journal of Computer Vision , 113(1):1–2, 2015.\n[51] J. Redmon, S. K. Divvala, R. B. Girshick, and A. Farhadi. You only\nlook once: Uniﬁed, real-time object detection. In 2016 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas,\nNV , USA, June 27-30, 2016 , pages 779–788, 2016.\n[52] J. Redmon and A. Farhadi. YOLO9000: better, faster, stronger. In 2017\nIEEE Conference on Computer Vision and Pattern Recognition, CVPR\n2017, Honolulu, HI, USA, July 21-26, 2017 , pages 6517–6525, 2017.\n[53] J. Redmon and A. Farhadi. Yolov3: An incremental improvement. CoRR ,\nabs/1804.02767, 2018.\n[54] S. Ren, K. He, R. B. Girshick, and J. Sun. Faster R-CNN: towards\nreal-time object detection with region proposal networks. IEEE Trans.\nPattern Anal. Mach. Intell. , 39(6):1137–1149, 2017.\n[55] F. Rusu and A. Dobra. Fast range-summable random variables for\nefﬁcient aggregate estimation. In Proceedings of the 2006 ACM\nSIGMOD International Conference on Management of Data , SIGMOD\n’06, pages 193–204, New York, NY , USA, 2006. ACM.\n[56] H. Samet. The Design and Analysis of Spatial Data Structures . Addison-\nWesley, 1990.\n[57] K. Simonyan and A. Zisserman. Very deep convolutional networks for\nlarge-scale image recognition. CoRR , abs/1409.1556, 2014.\n[58] V . Sindagi and V . M. Patel. A survey of recent advances in cnn-\nbased single image crowd counting and density estimation. CoRR ,\nabs/1707.01202, 2017.\n[59] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. E. Reed, D. Anguelov,\nD. Erhan, V . Vanhoucke, and A. Rabinovich. Going deeper with\nconvolutions. In IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015 , pages\n1–9, 2015.\n[60] S. Thirumuruganathan, S. Hasan, N. Koudas, and G. Das. Approximate\nquery processing using deep generative models. CoRR , abs/1903.10000,\n2019.\n[61] L. Wang, W. Ouyang, X. Wang, and H. Lu. Visual tracking with fully\nconvolutional networks. In 2015 IEEE International Conference on\nComputer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015 ,\npages 3119–3127, 2015.\n[62] L. Wen, D. Du, Z. Cai, Z. Lei, M. Chang, H. Qi, J. Lim, M. Yang,\nand S. Lyu. DETRAC: A new benchmark and protocol for multi-object\ntracking. CoRR , abs/1511.04136, 2015.\n[63] D. P. Woodruff. New algorithms for heavy hitters in data streams (invited\ntalk). In 19th International Conference on Database Theory, ICDT 2016,\nBordeaux, France, March 15-18, 2016 , pages 4:1–4:12, 2016.\n[64] I. Xarchakos and N. Koudas. Svq: Streaming video queries. In\nProceedings of ACM SIGMOD, Demo Track , 2019.\n[65] D. Zhang, D. Gunopulos, V . J. Tsotras, and B. Seeger. Temporal\naggregation over data streams using multiple granularities. In Advances\nin Database Technology - EDBT 2002, 8th International Conference on\nExtending Database Technology, Prague, Czech Republic, March 25-27,\nProceedings , pages 646–663, 2002.\n[66] R. Zhang, N. Koudas, B. C. Ooi, and D. Srivastava. Multiple ag-\ngregations over data streams. In Proceedings of the ACM SIGMOD\nInternational Conference on Management of Data, Baltimore, Maryland,\nUSA, June 14-16, 2005 , pages 299–310, 2005.\n[67] R. Zhang, N. Koudas, B. C. Ooi, D. Srivastava, and P. Zhou. Streaming\nmultiple aggregations using phantoms. VLDB J. , 19(4):557–583, 2010.\n[68] B. Zhou, A. Khosla, `A. Lapedriza, A. Oliva, and A. Torralba. Learning\ndeep features for discriminative localization. In 2016 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas,\nNV , USA, June 27-30, 2016 , pages 2921–2929, 2016.\n[69] X. Zhu and M. Bain. B-CNN: branch convolutional neural network for\nhierarchical classiﬁcation. CoRR , abs/1709.09890, 2017.",
  "textLength": 72922
}