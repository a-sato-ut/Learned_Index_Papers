{
  "paperId": "ff6da1881b3ec500aa753d3afc447626bc74d896",
  "title": "LAQP: Learning-based Approximate Query Processing",
  "pdfPath": "ff6da1881b3ec500aa753d3afc447626bc74d896.pdf",
  "text": "arXiv:2003.02446v1  [cs.DB]  5 Mar 2020LAQP: Learning-based Approximate Query Processing\nMeifan Zhang, Hongzhi Wang /AC1\nDepartment of Computer Science and Technology, Harbin Inst itute of Technology\nAbstract\nQuerying on big data is a challenging task due to the rapid growth of da ta\namount. Approximatequeryprocessing(AQP)isawaytomeetthe r equirement\nof fast response. In this paper, we propose a learning-based AQP method called\nthe LAQP. The LAQP builds an error model learned from the historica l queries\nto predict the sampling-based estimation error of each new query. It makes\na combination of the sampling-based AQP, the pre-computed aggre gations and\nthe learned error model to provide high-accurate query estimatio ns with a small\noﬀ-line sample. The experimental results indicate that our LAQP out performs\nthe sampling-based AQP, the pre-aggregation-based AQP and the most recent\nlearning-based AQP method.\nKeywords: Approximate Query Processing, Pre-computed aggregation,\nSampling, Machine Learning\n1. Introduction\nIt is a diﬃcult task to obtain the exact query answers on big data. Ev en\nthough suﬃcient hardware is available to conduct queries on big data , hours of\nresponse time is unacceptable to make real-time decisions [24, 27]. Ap proximate\nqueryprocessing(AQP)[7, 8] makesitpossible toeﬃciently obtainap proximate\nquery results. The AQP has been studied for a long time. Diﬀerent me thods\nmake diﬀerent trade-oﬀs among the accuracy, the response time , the space bud-\nget and the supported queries [21]. However, it is still challenging to a chieve a\nsatisfactory trade-oﬀ uniting all these aspects [27, 21].\nPreprint submitted to Journal of L ATEX Templates March 6, 2020\n\nExisting methods adopt two kinds of ideas. The ﬁrst one is the samplin g-\nbased AQP methods, i.e., the query results are estimated based on r andom\nsamples. Online sampling methods collect samples in the process of con duct-\ning each query, which will increase the response time accordingly [21]. The\noﬀ-line sample is collected before executing the queries, which leads t o fast re-\nsponse. However, a small sample may not be adequately to represe nt the entire\ndataset [7, 22, 15]. The second one is based on pre-computed syno pses or ag-\ngregations, which are computed before query processing and use d to estimate\nquery results. However, these methods can only support some sp ecial queries.\nThey cannot support queries as general as those supported by t he sampling\nmethods [21]. In addition, suﬃcient pre-computed aggregations will cost much\nspace [2]. A limited number of pre-computed aggregations or data cu bes are\ndiﬃcult to provide suﬃciently accurate estimation results, especially for the\nhigh-dimensional data.\nRecently, some methods make eﬀorts to adopt machine learning met hods to\nsolve the AQP problems [23, 26]. The main drawback of these methods is that\nthe query estimation based on the learned model cannot provide a p riori error\nguarantee as the sampling-based methods [23, 26, 31]. However, the error guar-\nantee is an important metric to measure the quality of the AQP estima tions [2].\nMost of the existing methods have some shortcomings in diﬀerent as pects\nincluding the low accuracy, unmeasured error, large space require ment, limited\nquery support, and low eﬃciency. Clearly, an ideal AQP approach ac hieves\nhigh accuracy of the estimations compared to the sampling-based A QP meth-\nods, provides error guarantee like the sampling-based method, co sts little space,\nsupports general queries and responds fast to the queries. Thu s, to support\ngeneral queries and provide the error guarantee, we take full ad vantage of the\nsampling-based method. Furthermore we also adopt the pre-comp uted aggre-\ngations to increase the accuracy according to the previous work A QP++ [27].\nThe AQP++ estimates the new query based on its ‘range-similar’ pre-\ncomputed query, whose predicate range is the most similar to the giv en query.\nIt estimates a new query qnewas follows.\n2\n\nAQP++:\nqnew=qold+(ˆqnew−ˆqold)\nThe estimation of qnewis the sum of two parts. The ﬁrst part is the exact\nresult of the old query qold. The second part is the diﬀerence between the\nnew query and the old one estimated by sampling. The ˆ qnewand the ˆqoldare\nthe estimations of qnewandqoldbased on sampling. The estimation accuracy\ndepends on the accuracy of the second part, since the ﬁrst part is a constant. If\ntheqnewandqoldhave similar predicate range, suggesting that, they are highly\ncorrelated, the new query estimated based on the old query is poss ible to be\nmore reliable than the sampling-based estimation ˆ qnew. Therefore, it estimates\na new query based on its ‘range-similar’ pre-computed query. That is the main\nidea of the AQP++.\nOur LAQP use a strategy diﬀerent from the AQP++ to ﬁnd a proper p re-\ncomputed query for each new query. In order to introduce the ide a of our\nmethod, we further reﬁne the second part of the above equation as follows.\nˆqnew−ˆqold= [qnew−Error(qnew)]+[qold−Error(qold)]\nIf (1) the estimation error of these two queries Error(qnew) =qnew−ˆqnew\nandError(qold) =qold−ˆqoldare close to each other, and (2) the ˆ qnewand the\nˆqoldare computed based on the same sample, estimating the new query b ased\non the old query is still reliable even though the range predicates of t he two\nqueries are irrelevant. We call thus old query the ‘error-similar’ que ry of the\nnew query.\nWe proved that is likely to provide a more accurate estimation for a ne w\nquery based on its ‘error-similar’ pre-computed query. Assuming t hat we have\na query log storing the true result of each per-computed query, t he precondition\nfor ﬁnding the ‘error-similar’ aggregation is to predict the sampling- based error\nfor each new query. We involve machine learning models in error predic tion,\nsince they are useful to predict the unknowns with the past obser vations. We\ncalculate the estimations of the pre-computed queries based on a ﬁ xed sample.\nIt is alsoeasyto compute the estimationerrors, sincewe knowthe t rue resultsof\n3\n\nthe pre-computed queries. Thus, a regression model can be learn ed by mapping\neach pre-computed query’s predicate to its estimation error which is computed\nbased on the ﬁxed sample. We describe that model as mapping each query to\nits sampling-based estimation error for brevity in the rest of the paper.\nIn this way, we unit the advantages of the sampling-based AQP meth od,\nthe pre-computed aggregations and the machine learning. At the s ame time,\nwe avoid the shortcomings of them including the high space cost of a s uﬃcient\nsample, and the lack of error guarantee for the predictions of a ma chine learning\nmodel.\nFor avoiding too much space cost, we use a small oﬀ-line sample to est imate\nall the queries. This sample is clearly insuﬃcient to provide an accurat e esti-\nmation. With the consideration that the error model learned from t he query\nlog can predict the sampling-based error of each query, we measur e the quality\nof the small sample with such a model. Meaning that, we can still give an ac-\ncurate estimation of the diﬀerence between a given query and a pre -computed\none according to their sampling-based estimations and errors. We p roved that\nthe accuracy of estimating their diﬀerence determines the accura cy of the ﬁnal\nestimation result. In this way, we can provide a suﬃciently accurate estimation\nwith only a small sample.\nFor supporting an error guarantee, we use a machine learning regr ession\nmodel, such as the SVM, the RandomForest, and the ANN, to predic t the\nsampling-based estimation error of a query instead of directly estim ating the\nquery result. The model can be tuned by mapping each pre-comput ed query to\nits sampling-based estimation error. In this way, the LAQP beneﬁts from the\nregression model to choose an ‘error-similar’ pre-computed quer y resulting in\nless error. Meanwhile, the estimation error can still be limited accord ing to the\nstatistical theorems. In addition, the fast prediction of the mode l will not cost\nmuch response time.\nThe framework of our LAQP is shown in Figure 1. We randomly choose a\nsmall sample from the dataset. Note that this is the only sample we us ed in our\nLAQP. We then compute the estimation of each query in the given que ry log\n4\n\nSample\n......\n...Query Log\nError Model\nerror-similar query\nFigure 1: The framework of LAQP\nbased on the small sample in order to measure the quality of the samp le. We\nstore the sampling-based estimations and the errors of estimation s along with\nthe results of queries in the log. We train a regression model mapping each\nquery to its sampling-based estimation error. When a new query arr ives, its\nsampling-based estimation error is predicted according to the erro r model. Its\n‘error-similar’ pre-computed query Qoptwill then be used to estimate the new\nquery. The ﬁnal query result is the sum of the chosen pre-comput ed query Ropt\nand the diﬀerence between the new query and the pre-computed o ne estimated\nby the same sample ( ˆRnew−ˆRopt). Since the diﬀerence is estimated based on\nthe sample, the estimation errorcan be limited according to statistic al theorems\nsuch as the Central Limit Theorem.\nWe make the following contributions in this paper.\n(1) Our ﬁrst contribution is the learning-based AQP method (LAQP) . We\nbuildaregressionmodellearnedfromthegivenquerylogtopredictt hesampling-\nbased estimation error of each new query. To the best of our know ledge, this is\nthe ﬁrst work making a combination of the sampling-based AQP metho d, the\npre-computed aggregations, and the regression model to increa se the accuracy,\nlimit estimation error, support general queries while occupying little s pace.\n(2) Our second contribution is the error analysis of the estimations and two\nextensionsoftheLAQPbeneﬁting fromthe diversiﬁcationandtheo ptimization.\nOur LAQP can provide an error guarantee for each estimation acco rding to\n5\n\nthe statistical theorems. The diversiﬁcation and optimization meth ods can be\napplied to our LAQP to improve its performance.\n(3) Our third contribution is the extensive experiments. We compar e the\nperformance of our method with some existing representative app roaches in-\ncluding a sampling-based AQP method, a pre-aggregation-based me thod and\na recent learning-based AQP method called DBEst. The experimenta l results\nindicate the advantages of our method over the existing ones.\n2. Related works\nThe sampling-based AQP methods are widely used, due to its eﬃciency and\nuniversality. Usually, there is a tradeoﬀ between the eﬃciency and t he accu-\nracy [30, 27, 21]. Samples in small size respond fast at the cost of re ducing the\naccuracy. Many works make eﬀorts to ﬁnd a representative samp le in a small\nsize to improve accuracy without reducing the eﬃciency [30, 3, 7]. Th e main\ndrawback of random sampling is that the accuracy decreases with t he variance\nof the aggregated attribute values. That is, random sampling cann ot provide\nsuﬃciently accurate estimations for the attributes with high-skew distribution.\nStratiﬁed sampling [1, 10, 7, 30, 3] is a way to solve this problem. The p roba-\nbility of stratiﬁed sampling is related to the importance or contributio n to the\naggregation result. However, the stratiﬁed sampling usually relies o n a prior\nknowledge of the distribution.\nMaking use of some pre-computed synopses and aggregationsis an other way\nto solve this problem. Synopses such as histograms [12], sketches [2 8], and\nwavelets[11] can be computed accordingto the query workloadbef ore executing\nqueries. Pre-computedaggregationsare alsobased on the given q ueryworkload.\nApproximate Pre-Aggregation[16] (APA) uses a random sample com bined with\na small set of statistics about the data to increase the accuracy. There are some\nmethodsstoringsomedatacubesaspre-aggregationstoimprove theaccuracy[9,\n25]. These method cannot support queries as general as those su pported by the\nsampling-based AQP methods. There are some other methods comb ining the\n6\n\npre-aggregationswithsampling[17, 22,18,19,27]. Theauthorsof reference [27]\nproposed the AQP++ estimating the query result based on the pre- computed\ndata cubes and the sampling-based AQP.\nMachine learning methods have been used in the ﬁeld of data process ing\nand data analysis in recent years [20, 31]. There are some new AQP m ethods\nadopt machine learning methods [23, 26]. In reference [23], the auth ors pro-\nposed an AQP method learning a density model and a regression mode l from\na small sample. Some researches use deep generative models to lear n the data\ndistribution and generate samples for the AQP [31]. The main problem of using\nmachine learning methods to approximate query result is that it curr ently does\nnot provide a priori error guarantee like the sampling-based AQP me thods.\nMost of the existing methods have some shortcomings in diﬀerent as pects\nincluding the low accuracy, limited query support, the storage of su ﬃcient sam-\nples or pre-computed information and the low eﬃciency. Our LAQP su pports\nmost of the typical queries supported by the sampling-based AQP m ethod. In\nthe meanwhile, it aims at increasing the accuracy with a small sample.\n3. Preliminaries\nIn this section, we introduce two representative AQP methods, i.e., the\nsampling-based AQP and the pre-aggregation-based AQP.\n3.1. Sampling-based AQP\nAssuming that there is an aggregation query in the following form.\nq: SELECT SUM(A) fromDWHERE C.\nA sampling-based AQP method ﬁrst randomly chooses a subset Sfrom the\ndatasetD. The query approximation including the estimation result and the\nconﬁdence interval are computed based on the query results on t he sample S\nand the Central Limit Theorem.\nEST(q) =|D|\n|S|SUM(SC(A))±λ/radicalBigg\nvar(SC(A))\n|S|\nTheSCmeans the tuples in the sample Smatching the predicate C.\n7\n\n3.2. Pre-aggregation-based AQP\nThe pre-aggregation-based AQP estimates a new query based on a pre-\ncomputed query. The ﬁnal estimation result est(q) is the sum of the pre-\ncomputedaggregation Pre(Q)ofquery Qandtheestimateddiﬀerence( EST(q)−\nEST(Q)).\nest(q) =Pre(Q)+(EST(q)−EST(Q)),\nwhereEST(q) andEST(Q) are the estimated results of the query qandQ\nbased on the method in Section 3.1.\n4. LAQP\nWe will introduce the frameworkof our LAQP in Section 4.1. In Section 4.2,\nwe analyze the estimation error.\n4.1. The framework\nIn this section, we will introduce the framework of our LAQP. LAQP r an-\ndomly chooses a small sample from the data, and estimates the quer ies in the\ngiven log based on the sample. We then train a regressionmodel from the query\nlog to predict the sampling-based error for the given query. For ea ch query, we\nsearch the log for its ‘error-similar’ pre-computed query resulting in less esti-\nmation error. At last, the query result is estimated according to th e sum of\nthe chosen pre-computed query result and the diﬀerence betwee n the new query\nand the pre-computed one. The estimation error can be limited acco rding to\nthe statistical theorems such as CLT, and Hoeﬀding bounds.\nAs introduced in Section 1, the major tasks of LAQP include providing the\nerror guarantee by statistics, saving the space cost by maintainin g a small oﬀ-\nline sample, and increasing the accuracy by making use of the pre-ag gregations\nand a regression model. We will introduce how our LAQP achieves thes e tasks\nin detail as follows.\nThe ﬁrst task is to measure the error of each estimation. Our idea o f the\nLAQP starts from the combination of the pre-computed aggregat ions and the\n8\n\nsampling-basedAQP, which estimates a new query accordingto a pre -computed\nquery result and the diﬀerence between the new query and the old o ne. The\naccuracy of the diﬀerence estimated by sampling determines the ac curacy of the\nquery estimation. The estimation error is limited according to the sta tistical\ntheorems.\nOur second task is to narrow down the space cost. We store a small oﬀ-line\nsample in the memory, since it costs little and responds fast. Howeve r, choosing\nagoodsamplein alimited sizeisadiﬃcult task. Ifthe distributionofthed atais\nskew or the number of dimensions increases to a large number, the d istribution\nof a small sample has little chance to be the same with that of the entir e data.\nInstead of ﬁnding a perfect sample, we would like to keep a small stat ic sample\nand measure the quality of that sample according to the pre-compu ted results\nin the query logs.\nThethirdtaskistoimprovetheaccuracy. Theaccuracyofthepre -aggregation-\nbased AQP method depends on the accuracy of estimating the diﬀer ence be-\ntween the new query and a proper pre-computed one as we mention ed before.\nWe come up with a new idea of choosing the ‘error-similar’ pre-comput ed query\ninstead of the ‘range-similar’ pre-computed one. We will prove that this idea\nleads to a more accurate estimation. The motivation of this idea is com posed of\ntwoparts. First, the assumptionthattheestimationerrorsoftw o‘range-similar’\nqueries are similar is not quite reliable for the high-skew data. Even th ough we\ncan ﬁnd a ‘range-similar’ pre-computed one, is that the optimal cho ice? There\nmay still exist another ‘error-similar’ pre-computed one resulting in the higher\naccuracy. Second, how to choose a proper one from a small numbe r of pre-\ncomputed queries whose predicate ranges are all not suﬃciently sim ilar to the\nnew query? The probability of choosing a suﬃciently similar predicate is not\nhigh, because of the increasing dimensions and the limited number of t he pre-\ncomputed queries. In that situation, the ‘error-similar’ query is po ssible to be\na good choice.\nThe remaining problem is how to compute the sampling-based estimatio n\nerror of a new query before execution. The previous method estim ates a new\n9\n\nquerybasedonthe‘range-similar’pre-computedquery,sincethe predicaterange\nis the only information ofthe new querywhile the sampling-basederro rof a new\nquery is unknown. We make use of a machine learning approach to solv e this\nproblem. The assumption of our method is that the pre-computed q ueries and\ntheir true results are available in the query log. We compute the estim ations of\nthe pre-computed queries in the given query log based on the same s ample, and\nlearn a regression model mapping each query to the diﬀerence betw een its true\nresult and its estimation. We call the diﬀerence the sampling-based e stimation\nerror. The predictive ability of the regressionmodel makes it possib le to predict\nthe estimation error of a large number of new queries while occupying a little\nspace. Furthermore, its ability of handling multi-dimensional data be neﬁts the\nmulti-dimensional AQP. In addition, predicting an error with a simple mo del is\neﬃcient, since the machine learning models are widely used in real-time d ecision\nmaking.\nAlgorithm 1 LAQP-ModelConstruction\nInput: QueryLog QL={[Q1,R1],[Q2,R2],...,[Qn,Rn]}, DATA D\nOutput: Sample S, Error model f\n1:S←Uniform Random Sample from D\n2:forQiinQLdo\n3:ˆRi=SAQP(Qi,S)\n4:end for\n5:Training Model f:Qi→Ri−ˆRi\nAlgorithm 2 LAQP-Estimation\nInput: QueryLog QL, Error model f, Sample S, New query q\nOutput: Estimation est\n1:PredictedError←f(q)\n2:opt= argmin i|(Ri−ˆRi)−PredictedError|\n3:est=Ropt+SAQP(q,S)−SAQP(Qopt,S)\nOur LAQP is composed of two parts, i.e., error model learning, and qu ery\n10\n\nPrice 120195200210250280\nCount 751015201565\nSample 411112\nTable 1: Information in an order list.\nestimation. Thepseudo-codeofthemareshowninAlgorithm1andAlg orithm2,\nrespectively.\nThe ﬁrst part is to construct the error model. It ﬁrst randomly ch oose a\nsmall sample Sfrom the dataset (Line 1). The sample is then used to com-\npute the estimation of each query Qin the QueryLog according to the simplest\nSAQP (Line 2-4). SAQP(Qi,S) denotes the approximate query result of Qi\nestimated with the sample Sbased on the method introduced in Section 3.1 .\nThe algorithm then trains an error model fmapping each query in the query\nlog to its estimation error (Line 5). In the implementation, we train on e model\nfor one kind of aggregation query. For example, the model of the s um queries\nlikeQ: select sum(X) from D, where lA≤A≤rAandlB≤B≤rBcan\nbe formed as fsum: (lA,rA,lB,rB)→sum(X)−ˆsum(X). For brevity, we\nuniformly represent the model as the form in the pseudo-code.\nThe second part is to estimate the result of a new query q. First, the error\nmodel predicts the error f(q) of the new query (Line 1). We regard a query\nQi, whose estimation error ( Ri−ˆRi) is the closest to the predicted error f(q)\nof the new query, as the baseline pre-computed aggregation (Line 2). The ﬁnal\nestimation result is calculated by summing the pre-computed result Roptand\nthe estimated diﬀerence SAQP(q,S)−SAQP(Qopt,S) between the new query\nqand the pre-computed Qj(Line 3).\nWe show the process of our algorithm with the following example.\nExample 4.1. Table 1 shows the PriceandCountof 200 items, and we choose\n10 items from the entire list to estimate the queries. Suppos e the following two\nqueries are in the query log:\n11\n\nRange TrueEst Error\nQ1[100,200]100120 +20\nQ2[201,300]10080−20\nQnew[190,270]6080+16(predicted)\nTable 2: Estimate new query based on the query log.\nQ1: select count(item) from order,\nwhere price between 100 and 200;\nQ2: select count(item) from order,\nwhere price between 201 and 300;\nThe true query results, the sampling-based estimated resul ts, and the esti-\nmation errors are shown in Table 2. Assuming that an error mod el has already\nbeen well tuned based on the information in the query log. Cur rently, the task\nis to estimate the following new query:\nQnew: select count(item) from order,\nwhere price between 190 and 270;\nWe can learn from the ﬁgure that the predicted error of the new query is\nmore similar to the error of Q1, suggesting that, estimating the result based on\nQ1is better than Q2. We compute the estimations based on Q1andQ2, and\ncompare them with the true result to verify the idea of our alg orithm.\nEstimation result based on Q1:R1= 100+80−120 = 60 .\nEstimation result based on Q2:R2= 100+80−80 = 100 .\nThe estimation based on Q1is more accurate, even though the predicate\nrange of the new query is more similar to Q2.\nAs shown in the example, estimating a query based on an error-similar pre-\ncomputed query is possible to provide a more accurate estimation. I n the next\nsection, we will prove that the estimation error of our LAQP method is limited.\n12\n\n4.2. Error Analysis\nIn this section, we describe the query estimation in LAQP and prove t hat\nthe estimation error is limited. At last, we discuss the impacts of the e stimation\naccuracy.\nWe ﬁrst deﬁne the estimation of a query q.\nDeﬁnition 1. The estimation of a query qbased on a pre-computed qiis deﬁned\nasest(q), and calculated with the following equation.\nest(q) =R(qi)+EST(q)−EST(qi) (1)\nThen, we prove that the estimation est(q) in Deﬁnition 1 is the unbiased\nestimation of query q.\nTheorem 1. est(q)is the unbiased estimation of R(q), i.e.,E[est(q)] =R(q).\nProof.\nE[est(q)] =E[R(qi)+EST(q)−EST(qi)] (2)\n=E[EST(q)]+E[R(qi)]−E[EST(qi)] (3)\nSinceEST(qi) is the estimated by sampling,\nE[R(qi)] =E[EST(qi)] (4)\nConsequently,\nE[est(q)] =E[EST(q)] =E[R(q)] =R(q) (5)\nThat is, est(q) is the unbiased estimation of R(q).\nThe estimation error R(q)−est(q) canbe bounded accordingto the following\ntheorem.\nTheorem 2. Pr[R(q)−est(q)> δ·R(q)]≤e−δ2·R(q)/2,δ∈(0,1).\n13\n\nProof.By the Chernoﬀ bound, for any δ∈(0,1),\nPr[est(q)<(1−δ)E[est(q)]]≤e−δ2·E[est(q)]]/2. (6)\nSinceE[est(q)] =R(q) according to Theorem 1,\nPr[R(q)−est(q)> δ·R(q)]≤e−δ2·R(q)/2.\nThat is, the estimation error R(q)−est(q) is limited.\nIn this way, the estimation error of each query based on our LAQP m ethod\nis limited.\nIn the above discussions, the qicould be any query in the query log Q.\nHowever, the selection of qialso inﬂuences the accuracy of the estimation. We\nwill show the inﬂuences of the accuracy in the following Theorem 3\nTheorem 3. min|R(q)−est(q)|= min|PredictionError (q)+(f(q)−Error(qi))|,\nwhere the model fmaps each query qiin the query log Qto its sampling-based\nestimation error. f:qi→Error(qi) =R(qi)−EST(qi).\nProof.Considering the prediction error of the model,\nf(q)+PredictionError (q) =R(q)−Est(q). (7)\nThe estimation error can be calculated as,\n|R(q)−est(q)|=|R(q)−[R(qi)+EST(q)−EST(qi)]|\n=|[R(q)−EST(q)]−[R(qi)−EST(qi)]|\n=|[f(q)+PredictionError (q)]−Error(qi)|\n=|PredictionError (q) +(f(q)−Error(qi))|\nThatis,theestimationaccuracydependsontwoparts. Theﬁrsto neisthemodel\naccuracy, the second is the similarity between the sampling-based e stimation\nerror of the chosen pre-computed query and the predicted erro r of the new\nquery.\n14\n\nThe prediction error mostly depends on the reliability of the training d ata.\nOnce the model is trained, the only thing to do is to ﬁnd a pre-comput ed query\nin the log whose sampling-based estimation error is the most similar to t he\npredicted error. Therefore, we ﬁnally deﬁne the estimator of our LAQP as\nfollows.\nDeﬁnition 2. The estimation of a query qbased on the LAQP is deﬁned as\nest(q), and computed according to the following equation.\nest(q) =R(qopt)+EST(q)−EST(qopt), (8)\nwhereqopt= argmin qi|f(q)−[R(qi)−EST(qi)]|.\nAs we discussed above, the estimation error of LAQP is limited, and th e es-\ntimation accuracy depends on both the model accuracy and the er ror-similarity\nbetween the chosen pre-computed query and the new query.\n4.3. Aggregation functions\nLAQPcansupportanytypicalaggregationfunctionssupportedb ythesampling-\nbased AQP method, such as the COUNT, SUM, AVG, STD and VAR. This\npoint can be proved easily similar to the Lemma 1 in the reference [27]. L AQP\nis able to provide an error guarantee for these aggregation funct ions according\nto the Theorem 2.\nSince the queries involving MAX and MIN are very sensitive to rare larg e\nor small values [2], the sampling-based estimations are not reliable for them.\nSimilarly, LAQP cannot provide the error guarantee for the queries involving\nthe MAX/MINaggregationswithout the explicitdistribution ofthe da ta. These\nqueriesdepend onthe rankorderofthe tuples ratherthantheir a ctualvalues[6].\nHowever, LAQP has more information besides the sample to rely on, s ince\nit learns the sampling-based error from the pre-computed queries . Therefore,\nit is possible to give better estimations of the MAX/MIN queries compa red\nwith the sampling-based method. The following theorems demonstra te that the\nestimations of the MAX/MIN queries based on the LAQP is possible to b e more\naccurate than the sampling-based estimation.\n15\n\nTheorem 4. The LAQP estimation est(q)of aMAXqueryqis more accurate\nthan the sampling-based estimation EST(q)as long as there exist a pre-computed\nqueryQthatError(Q)≤2·Error(q), whereError(Q)andError(q)are the\nsampling-based errors of Qandq, respectively.\nProof.The estimation of qbased on LAQP is R(Q) +EST(q)−EST(Q) ac-\ncording to the Theorem 1. Thus, the estimation error is:\n|R(q)−est(q)|=|R(q)−[R(Q)+EST(q)−EST(Q)]|\n=|[R(q)−EST(q)]−[R(Q)−EST(Q)]|\n=|Error(q)−Error(Q)|\nTheError(q) is the diﬀerence between the true result R(q) and its estima-\ntionEST(q) calculated based on the sample, i.e., Error(q) =R(q)−EST(q).\nIt is clearly that the sampling-based estimation EST(q) of aMAXquery\nis the under-estimation. Therefore, |Error(q)|=Error(q). Consequently,\n|Error(q)|≥|R(q)−est(q)|can be true on the assumption that Error(q)≥\n|Error(q)−Error(Q)|.\nIn the next step, we prove that Error(q)≥|Error(q)−Error(Q)|is true as\nlong asError(Q)≤2·Error(q). (1) If Error(q)≥Error(Q), the assumption\nis always true since Error(Q)≥0 for any MAXquery. (2) If Error(q)<\nError(Q), the assumption is true as long as1\n2·Error(Q)≤Error(q)<\nError(Q). Thus, the assumption Error(q)≥|Error(q)−Error(Q)|is true\nas long as1\n2·Error(Q)≤Error(q).\nTheorem 5. The LAQP estimation est(q)of aMINqueryqis more accurate\nthan the sampling-based estimation EST(q)as long as there exist a pre-computed\nqueryQthatError(Q)≥2·Error(q), whereError(Q)andError(q)are the\nsampling-based errors of Qandq, respectively.\nWe omit the proof of Theorem 5 since it can be proved in the same way\nas proving the Theorem 4. These theorems demonstrate that, as long as we\ncan ﬁnd a pre-computed query satisfying the assumption, the LAQ P is more\naccurate than the sampling-based AQP.\n16\n\n5. Extensions\nIn this section, we discuss the diversiﬁcation and optimization of LAQ P to\nimprove the performance furthermore.\n5.1. Diversiﬁcation\nIn the previous section, we have discussed training the error mode l based on\nthe given query logs. For such approach, we have two questions. A re all the\nqueries in the log suitable to be used to train the error model? Do we ne ed to\nstore all the queries that keep coming up?\nClearly, it is not a good idea to store all the processed queries and tr ain the\nerror model with them, due to its huge storage requirement. The s implest way\nto reduce the storage requirement is to limit the number of queries in the log.\nHowever, reducing the number of pre-computed queries will aﬀect the accuracy\nof the LAQP. We need to ﬁnd a proper subset of the log that perfor ms well\nwith a limited number of queries. As discussed in Section 4.2, the estima tion\naccuracy of LAQP depends on (1) the accuracy of the error mode l and (2) the\nsimilarity between the error of a pre-computed query and the pred icted error of\nthe given query. We consider to improve these two points by diversif ying the\ntraining data and the errors of a limited number of pre-computed qu eries.\nOn the one hand, the error model beneﬁts from the diversiﬁcation of the\ntraining data. The diversity of training data ensures that it can pro vide more\ndiscriminative information to the model [14]. Therefore, we consider increasing\nthe diversify of the training data in LAQP, i.e., the pre-computed que ries in\nthe log. On the other hand, the diversiﬁcation of the sampling-base d estimation\nerrorsis possible to improve the estimation accuracy. We show that diversifying\nthe sampling-based errors of the pre-computed queries is possible to reduce the\nmaximum error-diﬀerence between a query and its ‘error-similar’ pr e-computed\nquery. That diﬀerence determines the estimation accuracy as we d iscussed be-\nfore. Thus, the sampling-based estimation error should also be dive rsiﬁed.\nWe take the Max-Min [13] diversiﬁcation method as an example to show\nthat it is possible to reduce the estimation error. The Max-Min metho d aims\n17\n\n｛\n｛ MaxDis\n｛\n｛MinDiserror of q ｛\nmaxDis(q,L1) Q1Q2Q3\nQ'1 Q'2 Q'3Q'4Q4L1\nL2\nmaxDis(q',L2)\n｛\nerror of q' \nFigure 2: Diversiﬁcation\nat maximizing the minimum of the distance between any two elements in t he\ndiversiﬁed result. We use the following example to show that the maxim um of\nthe distance between the given query and its ‘error-similar’pre-co mputed query\ncan be reduced by the Max-Min diversiﬁcation method.\nExample 5.1. Suppose that the L1andL2in Figure 2 are two subsets of the\nquery log. The red points on the ﬁrst line are the sampling-ba sed errors of the\nfour queries Q1,Q2,Q3, andQ4inL1. The red points on the second line are\nthose errors in L2. Assuming that the ranges of the errors in L1andL2are the\nsame, and L2is the diversiﬁed result chosen from the query log, the minim um\ndistance (MinDis) of two errors in L2is larger than that of the distance in L1.\nThe max distance maxDis(q,L1)between the error of a query qand its nearest\nerror in L1is the half of the max distance (MaxDis) in L1. Since the MinDis\ninL2is larger than that of L1, the maximum of its MaxDis is smaller than that\nofL1, i.e.,maxDis(q,L1)> maxDis (q,L2).\nAs shown in the above example, the diversiﬁcation method is possible t o\nreduce the maximum of the diﬀerence between a query and its ‘error -similar’\npre-computed one in the log. The diﬀerence determines the accura cy of the\nquery estimation as we discussed before. Thus, diversifying the sa mpling-based\nerrors of the pre-computed queries is possible to reduce the estim ation error.\nTo discuss the diversiﬁcation strategy, we deﬁne the distance of t wo queries\nas follows.\nDeﬁnition 3. The distance of query QiandQj:\nDis(Qi,Qj) =/summationtext(li,x−lj,x)2+(ri,x−rj,x)2\nx∈d\n2d+(Error i−Error j)2,\n18\n\nwheredis the number of dimensions, the li,xandri,xmean the left and right\nrange boundaries of the Qiin thexth dimension, and the Error imeans the\nsampling-based estimation error of Qi.\nAccording to the deﬁnition of the distance between two queries, a v ariety\nof diversiﬁcation methods [13, 32] can be adopted to ﬁnd a set of qu eries with\nhigh diversity. The usage of the diversiﬁcation makes it possible to tu ne a good\nmodel with limited pre-computed aggregationsand increases the op portunity of\nﬁnding a suﬃciently similar pre-computed query for each new query. Note that\nthe normalization is required before the computation of the distanc e between\ntwo multi-dimensional queries in the implement. We make experiments t o test\nthe improvement of the LAQP beneﬁting from the diversiﬁcation in Se ction 6.6.\n5.2. Optimization\nThe ﬁnal estimation provided by our LAQP is based on an existing pre-\ncomputed query as discussed before. However, the pre-comput ed query chosen\naccording to the ‘error-similar’ strategy is not always quite reliable. What if\nthe situation that the model is not well tuned? What if there exists a pre-\ncomputed query whose predicate range is suﬃciently similar to the qu ery range\nof the new one? That is, the LAQP may fail to ﬁnd the true ‘error-sim ilar’\nquery based on the wrong prediction of the error model. We still hav e a chance\nto give an accurate estimation based on a ‘range-similar’ pre-compu ted query\nwhose predicate range is close to that of the new query. Therefor e, we consider\nto make a proper combination of our ‘error-similar’ strategy with th e previous\n‘range-similar’strategy to ﬁnd the optimal pre-computed query w hen the model\naccuracy is not satisfactory.\nWe involve the range-distance to the query-distance to ﬁnd the sim ilar pre-\ncomputed query. We put diﬀerent weights αandβon the error-distance and\nthe range-distance, and redeﬁne the distance of two queries as f ollows.\nDis(Qi,Qj) =α·EDis(Qi,Qj)+β·RDis(Qi,Qj) (9)\n19\n\nAlgorithm 3 Optimized-LAQP\nInput: QueryLog QL, DATA D, New Query q\nOutput: Sample S, Error model f, Estimation est\n1:f:Qi→Ri−ˆRi,Ri∈QL\n2:Error q←f(q)\n3:Dis(q,Qi) =α·EDis(q,Qi)+β·RDis(q,Qi)\n4:opt= argiminDis(q,Qi)\n5:est=Ropt+SAQP(q,S)−SAQP(Qopt,S)\nWe propose a new algorithm adopting the new distance in Equation 9 to ﬁnd\nthe nearest pre-computed query. The weights in Equation 9 show t he reliability\nof the error model. They can be computed by solving an optimization p roblem\nwhose object function is to minimize the error on the testing set.\nThe pseudo-code of our optimized-LAQP is shown in Algorithm 3. Lear ning\nthe error model is the ﬁrst step (Line 1). The way of choosing the o ptimal pre-\ncomputed query is based on the redesigned distance of two queries involving\nboth the error-distance( EDis) and the range-distance( RDis) (Line 3-4). The\nparameters αandβcanbeeitherdeterminedbytheusersortunedbyoptimizing\nthe accuracy.\nAs discussed in Section 4.2, the accuracy of an estimation is determin ed by\nthe diﬀerence between the sampling-based estimation error of the new query\nand the chosen pre-computed one, i.e., |errorq−erroropt|. Therefore we regard\nthe sum of that diﬀerence of each query in the Testset as the object function\nz. The problem of choosing the optimal weights αandβcan be solved by the\noptimization problem as follows.\n20\n\nminz=/summationdisplay\nq∈Test|errorq−erroropt(q,α,β)|2(10)\nopt(q,α,β) = argmin\nQi∈Trainα·EDis(q,Qi)+β·RDis(q,Qi) (11)\nEDis(Qi,Qj) =|errorQi−errorQj|2(12)\nRDis(Qi,Qj) =/summationtext(li,x−lj,x)2+(ri,x−rj,x)2\nx∈d\n2d(13)\ns.t. α+β= 1,0≤α≤1,0≤β≤1. (14)\nSuch an optimization problem could be solved by the approachessuch as the\nBrent’s method [5], and the weights αandβare obtained in the solution.We\nproved that the optimization improves the performance of the orig inal LAQP.\nTheorem 6. Ifα= 1, the accuracy of the optimized LAQP is the same with\nthe original LAQP. If α <1, the accuracy of the optimized LAQP is better than\nthe original LAQP on the testing set.\nProof.(1)Ifα= 1,Equation11issimpliﬁedas opt(q,1,0) = argminQi∈Train|errorq−\nerrorQi|, which is the same with the equation in Algorithm 2 (Line 2). Thus,\nthe optimized LAQP is the same with the original LAQP.\n(2) Ifα <1, and assuming that the optimized LAQP is worse than the\noriginal LAQP, the sum of the squared error on the testing set of t he opti-\nmized LAQP is higher than that of the original LAQP, i.e.,/summationtext\nq∈Test|errorq−\nerroropt(q,α,β)|2>/summationtext\nq∈Test|errorq−erroropt(q,1,0)|2. It contradicts the ob-\nject function of the optimization in Equation 10. Therefore, the as sumption\nis false. Thus, the optimized LAQP is better than the original LAQP in t his\nsituation.\nTherefore, the optimization improves the accuracy on the testing set. In-\nvolving the optimization only modiﬁes the way of choosing the pre-com puted\nquery for each new query, the estimation error is still limited accord ing to the\nTheorem 2 as we discussed in Section 4.2.\n21\n\n6. Experiments\nIn this section, we show the experimental results of the LAQP. We c ompare\nthe LAQP with the existing methods including the most typical and wide ly\nused sampling-based AQP method, the most recent method combinin g the pre-\ncomputed aggregations with sampling called AQP++, and the state-o f-the-art\nlearning-based AQP method called DBEst.\n6.1. Experimental Setup\nHardware and Library All the experiments were conducted on a laptop with an\nIntel Core i5 CPU with 2.60GHz clock frequency and 8GB of RAM. The e rror\nmodel implementation is based on the RandomForestRegressor in th e scikit-\nlearn library1.\nDatesets We use three real-life datasets for experiments.The POWER and WE-\nSAD are two largemulti-dimensional datasets. The distribution of th e attribute\n‘globalactivepower’ in POWER is a long-tailed distribution, while the distri-\nbution of each attribute in WESAD approximates a normal distributio n. We\nalso use the PM2.5 dataset adopted in the experiments of the DBEst for fair\ncomparisons.\n(1) The POWER dataset is the “Individual household electric power c on-\nsumption Data Set”2. This dataset contains 2,075,259 tuples and 9 attributes.\nWeuseasubsetofthisdatasetincludingsevennumericalattribute sand2,000,000\ntuples to conduct the experiments.\n(2) The WESAD (Wearable Stress and Aﬀect Detection) dataset [2 9] is a\nreal-life dataset. This dataset is a 16GB dataset containing 63 million r ecords.\nWe use eight attributes (CH1, CH2, CH3, CH4, CH5, CH6, CH7, CH8) from it\nto conduct our experiments.\n(3)ThePM2.5datasetisareal-lifehourlydatasetcontainingtheinfo rmation\nof the PM2.5 data of the US Embassy in Beijing3. There are 43,824 instances\n1https://scikit-learn.org/stable/\n2http://archive.ics.uci.edu/ml/datasets/Individual+h ousehold+electric+power+consumption\n3http://archive.ics.uci.edu/ml/datasets/Beijing+PM2. 5+Data\n22\n\nin this dataset.\nQueriesThe queries in our experiments include multi-dimensional predicates\nfor testing the inﬂuence of dimension on the performance. We make examples\nabout the queries adopted in the experiments.\nAn example of a one-dimensional query:\nQ1D:SELECT COUNT(pm2.5) from PM2.5,\nwherel≤PREC≤r.\nAn example of a three-dimensional query:\nQ3D:SELECT COUNT(CH1) from WESAD,\nwherel1≤CH1≤r1,l2≤CH2≤r2,l3≤CH3≤r3.\nWe introduce the generation of the queries in the experiments as fo llows.\n(1) The aggregated attribute in the queries for the PM2.5 dataset is ‘pm2.5’,\nand each predicate only involves one attribute ‘PREC’. The one-dimen sional\nqueries are generated by randomly choosing the range boundaries from the do-\nmain of the attribute ‘PREC’.\n(2)TheaggregatedattributeforthePOWERdatasetis‘global activepower’,\nand each predicate involves seven attributes. In order to avoid mo st of the\nmulti-dimensional query results to be zero, we limit the range to gene rate the\nboundariesin each dimension. The left boundary of the rangein each dimension\nis randomly chosen from the ﬁrst quarter of the entire value range of the cor-\nresponding attribute. Similarly, each right boundary is randomly cho sen from\nthe last quarter of the corresponding attribute range.\n(3) The aggregated attribute for the WESAD dataset is ‘CH1’, and e ach\npredicate involves eight attributes. The predicates are generate d in the same\nway as those generated for the POWER dataset.\nFor the following experiments, we generate the queries for each da taset. We\ntake a subset of the queries to form the query log, and estimate th e remaining\nqueries based on the LAQP method.\nError Metrics We use two error metrics to measure the accuracy of our LAQP\nmethod. In the following deﬁnitions, Q, ˆriandridenote the queries, the esti-\nmated query results and the true results, respectively.\n23\n\n●\n●\n● ●\n1.01.52.02.53.03.54.02.0×1052.5×1053.0×1053.5×105\nmax_depthMSE\n(a) MSE●●●●\n1.01.52.02.53.03.54.005101520\nmax_depthSpaceCost (KB)\n(b) SpaceCost\nFigure 3: The impact of the max depth of the RandomForest\nAverage Relative Error: ARE=1\n|Q|/summationtext\nqi∈Q|ˆri−ri|\nri\nMean Squared Error: MSE=1\n|Q|/summationtext\nqi∈Q|ˆri−ri|2\nImplementation Details All the experiments are conducted in Python 3.5. We\nusethe RandomForestRegressorinthe scikit-learnpackageto mo delthe relation\nbetween each pre-computed query and its sampling-based estimat ion error. A\nrandomforestisametaestimatorthatﬁtsanumberofdecisiontre eclassiﬁerson\nvarious sub-samples of the dataset and uses averaging to improve the predictive\naccuracy and control over-ﬁtting [4]. We simply adopted the Rando mForestRe-\ngressor since it is widely used and its parameters are easy to tune. W e set the\nparameter of the RandomForestRegressor maxdepth= 3. This parameter is\ndetermined by tuning. We use the ﬁrst dataset to test the perfor mance of the\nmodel with diﬀerent maxdepthand show the result in Figure 3.\nCompetitors Inthefollowingexperiments,wecomparetheperformanceofLAQP\nwith the most representative or the state-of-the-art AQP meth ods including the\nsimplestSAQP(thesampling-basedAQP),amodiﬁedversionoftheAQ P++[19]\nand the DBEst [23].\nSAQP: the simplest sampling-based AQP method as introduced in Sec-\ntion 3.1. It estimates the query result according to a sample random ly chosen\nfrom the entire dataset.\nAQP++: an AQP method combining the sampling-basedAQP method with\nthe pre-computed data cubes. In this experiment, we did not gene rate data\ncubes, butmodifytheAQP++methodtoregardthepre-computed aggregations\n24\n\nasthe pre-computedcubes. We stillfollowthe main ideaofthe origina lAQP++\nestimating a new query based on a pre-computed query and comput ing the\ndiﬀerence between the new query and the old one based on sampling. The\nreason for the modiﬁcation is that we want to test the performanc e the AQP\nmethods with a small number of pre-aggregations. However, if the BP-Cube in\nthe original AQP++ is adopted, each dimension can be only partitioned into\nvery few parts. In addition, the queries are not generated unifor mly in the data\nrange to avoid the query result to be zero. Thus, the queries may b e far from\nthe cells in the BP-Cube. However, since the queries and the pre-ag gregations\nare generated in the same way as we introduced before, the pre-a ggregationsare\nmore reliable than the BP-Cube. Thus, the modiﬁed version perform s better\nthan the original AQP++ in the following experiments. In the following ﬁ gures\nand experimental descriptions, the modiﬁed version is still called AQP ++ for\nthe sake of brevity.\nDBEst: a learning-based AQP method which learns a density model an d a\nregression model from a small sample of the data. We adopted the im plemen-\ntation of the DBEst provided by the authors in github4.\n6.2. Accuracy\nIn this section, we compare the accuracy of our LAQP method with t he\nother methods. The accuracy of each method is measured by both the MSE\nand ARE. We also test the inﬂuence of the query selectivity on the ac curacy.\nWe compare our LAQP with the SAQP and AQP++ for the multi-dimension al\nqueries. The comparisons involving the DBEst are only conducted on the one-\ndimensional queries due to the limitation of its implementation. We test their\nperformance for three aggregation functions Count,SumandAvg. The queries\nare generated as introduced before. To be fair, all the methods u se the same\noﬀ-line sample.\nEXP1: We compare the accuracy of our LAQP, SAQP and the AQP++ on the\n4https://github.com/qingzma/DBEstClient\n25\n\nLAQPAQP++SAQPC \u0000 \u0001 \u0002 \u0003 Sum A \u0004 \u0005\n1 \u0006\n\u0007 \b \t \n\u000b \f\n5\n\r \u000e\n7\nM \u000f \u0010\n(a) MSELAQPAQP++SAQP\u0011 \u0012 \u0013 \u0014 \u0015 Sum \u0016 \u0017 \u0018\n0 \u0019 \u001a\n\u001b \u001c \u001d\n\u001e \u001f  \n! \" #\n$ % &\n' ( )\n* + ,\n(b) ARE\nFigure 4: Accuracy Comparison on Dataset POWER\nLAQPAQP++SAQP- . / 2 3 Sum 4 6 8\n9 :\n;\n< =\n> ?\n@ B\nD E\nF G H\n(a) MSELAQPAQP++SAQPI J K L N Sum O P Q\nR S T U\nV W X Y\nZ [ \\ ]\n^ _ ` a\nb c d e\nf g h\n(b) ARE\nFigure 5: Accuracy Comparison on Dataset WESAD\nLAQPAQP++SAQPi j k l mn opqr Sum s t u\nvwx\nyz\n{\n|}\n~\n  \n(a) MSELAQPAQP++SAQP          Sum  \n   \n   \n   \n   \n   ¡ ¢\n£ ¤ ¥ ¦\n§ ¨ ©\n(b) ARE\nFigure 6: Accuracy Comparison on Dataset PM2.5\n26\n\ndataset POWER. The results are shown in Figure 4. The number of sa mples\nis 2,000. We adopted 800 and 100 queries in the log to train and test th e\nerror model of the LAQP, respectively. We compute the estimation error of 100\nnew queries to evaluate the accuracy. Each query includes a seven -dimensional\npredicate. The average selectivity of the queries is nearly 0.2%. The results are\nshown in Figure 4. We can learn from the ﬁgure that the our LAQP is mo re\naccurate than the other two methods. Since the sample size is much smaller\nthan the data size, the sample distribution is not suﬃciently similar to t hat of\nthe entire data. Therefore, the SAQP does not perform well in this experiment.\nEven though a large number of the pre-computed queries are adop ted, it is still\ndiﬃcult for the AQP++ to ﬁnd a similar pre-computed query for each n ew due\nto the high dimensions. As a comparison, LAQP does not suﬀer from t he small\nsample size and the multi-dimensional data. LAQP beneﬁts from the a bility\nof the error model to handle the multi-dimensional data. In addition , it is\neasier to ﬁnd a similar one-dimensional estimation error than ﬁnding a similar\nmulti-dimensional predicate range.\nEXP2: We compare the accuracy of our LAQP, SAQP and the AQP++ on the\ndataset WESAD. The results are shown in Figure 5. The number of sa mples is\n20K. We adopted 130 and 40 queries in the query log to train and test the error\nmodel of the LAQP, and we computed the estimation error of 30 new queries\nto evaluate the accuracy. Each query includes an eight-dimensiona l predicate.\nThe average selectivity of the queries is nearly 2%. In this experimen t, the\nSAQP performs better than the AQP++, since the sample is suﬃcient while\nthe number of the pre-computed queries is limited. Our LAQP is still mo re\naccurate than the other two methods.\nEXP3: We compare the accuracy of our LAQP, SAQP, AQP++ and DBEst on\nthe dataset PM2.5. The experimental results are shown in Figure 6. We ran-\ndomlychooseasamplewith thesamplingrateat1%fromtheentiredat aset. We\nonly use a query log including 200 one-dimensional queries in this exper iment,\nand compute the estimation error of 100 new queries to evaluate th e accuracy.\nThe queries in this experiment only include the one-dimensional predic ates due\n27\n\nto the limitation of the implementation of the DBEst. We can learn from the\nﬁgure that the accuracy of our LAQP outperforms the other met hods. The\nreason why our method outperforms the SAQP and AQP++ is similar to that\nin EXP1. The reason why LAQP outperforms the DBEst is that we use pre-\ncomputed queries to improve the accuracy, while the accuracy of t he DBEst\nmethod largely depends on the sample. When the sample size is much sm aller\nthan the data size, the regression model of DBEst based on the sa mple is unre-\nliable.\nEXP4: We test the inﬂuence of the query selectivity on the accuracy. Th e\nqueries in the experiments only contain one-dimensional predicates and two-\ndimensional predicates, since it is diﬃcult to generate high-dimension al queries\nwith a high selectivity. The sample size of these methods are 2,000, an d the\nnumber of the pre-computed queries is 200. The relative errors of the estima-\ntion results on the one-dimensional queries and the two-dimensiona l queries are\nshown in Figure 7 and Figure 8, respectively. We can learn from the ﬁg ures\nthat the relative errors of the estimations decrease with the selec tivity. In most\ncases, LAQP is more accurate than the other methods. The perfo rmance of\nthese three methods are similar on AVG queries. The reason is that t he vari-\nance of the aggregated attribute values is not high, meaning that t he results of\nthe AVG queries are similar. As a comparison, LAQP still has superiorit y to\nhandle the two-dimensional AVG queries as shown in Figure 8(c). In t his exper-\niment, the sampling rate is only 0.1% and only a small number of pre-com puted\nqueries are adopted in AQP++ and LAQP. Therefore, the distributio n of the\nsample is not suﬃciently similar to that of the entire dataset, and it is d iﬃ-\ncult to ﬁnd a pre-computed query similar to each new query. Our LAQ P is\nbetter than the SAQP since the sampling-based estimation errors a re learned\nfrom the pre-computed queries to improve the accuracy. Our LAQ P provides\nan opportunity to ﬁnd an ‘error-similar’pre-computed query, oth er than ﬁnding\na ‘range-similar’ pre-computed one. As the dimension increases, it w ill be more\ndiﬃcult to ﬁnd a pre-computed query with suﬃciently similar range. Th is point\nis also veriﬁed by the fact that the advantage of LAQP on the two-d imensional\n28\n\n●LAQP ■AQP++◆SAQP\n●\n●\n●\n●●●\n●●\n●●●●●\n●\n●●\n●●●●●●●●●●\n●\n●\n●●●●●●●●\n●●●●●●●●●●●●●\n●●\n●●\n●●●●●●■\n■■\n■■■\n■■\n■■■■■\n■■■■■■■■■■\n■\n■■\n■■■■■\n■■■■■■■\n■\n■■■■■■■■■■■■\n■■\n■■\n■■■■◆\n◆◆\n◆◆◆◆◆◆◆◆◆◆◆◆◆\n◆◆◆◆◆◆◆◆◆\n◆◆\n◆\n◆◆◆◆◆◆◆◆\n◆◆\n◆\n◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆ª « ¬ ­ ® ¯ ° ± ² ³ ´ µ ¶ · ¸ ¹ º »\n¼ ½ ¾ ¿ À\nÁ Â Ã Ä Å\nÆ Ç È É Ê\nË Ì Í Î Ï\nÐ Ñ Ò Ó Ô\nÕ\nÖ×Ø Ù Ú Û Ü Ý Þ ß à á\nâãä(a) COUNT●LAQP ■AQP++◆SAQP\n●\n●\n●●●●●\n●\n●●●●\n●\n●●●●●●●●●●\n●\n●●\n●\n●●●●●●●●\n●●\n●●\n●●●●●●●●●●●\n●●\n●●●●●\n●■\n■\n■■■■■\n■\n■■■■\n■■■■\n■\n■■■■■\n■\n■\n■■■■■■■■■■■\n■■\n■\n■■■■■■■■■■\n■\n■\n■■\n■■■\n■■\n■◆◆\n◆\n◆◆◆◆◆◆◆◆◆\n◆\n◆◆\n◆◆◆◆\n◆◆◆◆\n◆◆\n◆◆◆◆◆◆◆◆◆◆◆◆\n◆\n◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆å æ ç è é ê ë ì í î ï ð ñ ò ó ô õ ö\n÷ ø ù ú û\nü ý þ ÿ 0\n\u0000 \u0001 \u0002 \u0003 \u0004\n\u0005 \u0006 \u0007 \b \t\n\n \u000b \f \r \u000e\n1S\u000f \u0010 \u0011 \u0012 \u0013 \u0014 \u0015 \u0016 \u0017 \u0018\nA\u0019\u001a(b) SUM●LAQP ■AQP++◆SAQP\n●\n●●●●●●●●●\n●●●●\n●●\n●●●●●●●●●●●●●●\n●●\n●●●\n●●\n●●●\n●●●●\n●●●●●●●●●●● ●\n●■\n■■■■■■ ■\n■■\n■■■■\n■■\n■■■■■■■■■\n■■■\n■■■■■■\n■■\n■■■■■\n■■■ ■■■\n■■■■■\n■■\n■■■◆\n◆◆◆◆◆◆◆\n◆◆\n◆◆◆◆\n◆◆\n◆◆◆◆◆◆◆◆◆◆◆◆\n◆\n◆\n◆\n◆\n◆◆\n◆\n◆◆◆◆◆\n◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆\n◆\u001b \u001c \u001d \u001e \u001f  ! \" # $ % & ' ( ) * + ,\n- . / 2 3\n4 5 6 7 8\n9 : ; < =\n> ? @ B C\nD E F G HI J K L M N O P Q R T\nUVW(c) AVG\nFigure 7: The inﬂuence of selectivity on the accuracy (1D).\n●LAQP ■AQP++◆SAQP\n●●\n●●\n●●●●●●●\n●●\n●\n●●●●\n●●●●●\n●●\n●●●●●●\n●●\n●●●\n●\n●\n●●●●●●●\n●●●■\n■■■■\n■■\n■■\n■■■■■\n■■ ■■■■■■■■■■■■\n■\n■■■■■■■■\n■\n■\n■■■■\n■■■■■◆ ◆ ◆ ◆ ◆◆◆\n◆◆\n◆\n◆◆◆◆\n◆◆\n◆◆◆◆◆◆◆\n◆◆\n◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆ ◆X Y Z [ \\ ] ^ _ ` a b c d e f g h i j k l\nm n o p q\nr s t u v\nw x y z {\n| } ~  \n    \n         \n(a) COUNT●LAQP ■AQP++◆SAQP\n●\n●\n●●\n●●●\n●●\n●\n●●\n●●●●●●\n●●●●●●●\n●●●\n●●●\n●\n●●\n●\n●●●●●●●●●●●\n●■\n■\n■■\n■■\n■\n■■■■■\n■\n■■\n■■■\n■■■■■■■■\n■■\n■\n■■■■■■■■\n■\n■■■■■\n■■\n■■◆ ◆ ◆ ◆◆◆◆◆◆◆\n◆\n◆\n◆\n◆◆◆\n◆\n◆◆\n◆\n◆◆\n◆◆◆\n◆◆◆◆◆\n◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆             ¡ ¢ £\n¤ ¥ ¦ § ¨\n© ª « ¬ ­\n® ¯ ° ± ²\n³ ´ µ ¶ ·\n¸ ¹ º » ¼\n½¾¿ À Á Â Ã Ä Å Æ Ç È\nÉÊË(b) SUM●LAQP ■AQP++◆SAQP\n●●●●●●●●●●●●●●●●●●●●●●●\n●●●●●●●●●●\n●●●●●●\n●\n●●●\n●●●●●●●●●●\n●●●●●\n●●●●●\n●●●\n●●●●●●\n●●●\n●●●●●●●●●●●●●●●■■■■■■■■■■■■■■■■■■■■■■\n■\n■\n■■\n■■■■■■■\n■■\n■■■■■\n■■■\n■■■■■■■■■■■■■■■\n■■■■■\n■■■\n■■■■■■■\n■■■■■■■■■■■■■\n■■■■◆ ◆ ◆ ◆◆◆ ◆ ◆ ◆ ◆◆◆◆ ◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆\n◆\n◆\n◆◆◆◆◆◆◆\n◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆Ì Í Î Ï Ð Ñ Ò Ó Ô Õ Ö × Ø Ù Ú\nÛ Ü Ý Þ ß\nà á â ã ä\nå æ ç è é\nê ë ì í î\nï ð ñ ò ó\nôõö ÷ ø ù ú û ü ý þ ÿ\nA\u0000\u0001(c) AVG\nFigure 8: The inﬂuence of selectivity on the accuracy (2D).\nqueries is more signiﬁcant than that on the one-dimensional queries .\n6.3. Space Cost\nWe compare the inﬂuence of the space cost on the accuracy. The s pace cost\nof the SAQP is the sample size. The space cost of the AQP++ includes b oth\nthe sample size and the cost of the pre-computed aggregations. L AQP has an\nadditional cost of the error model compared with the AQP++. Since the space\ncost of the sample, the pre-computed aggregations, and the err or model are\nindependent of the data size, we did not evaluate the relation of the data size\nand the space cost. Instead, we focus on the inﬂuence of the spa ce cost on the\naccuracy.\nThis experiment was conducted on the POWER dataset. Each query in-\nvolves a seven-dimensional predicate. The number of the samples in the SAQP\nvaries from 1k to 5k. Since the space cost of LAQP and AQP++ include three\nand two parts, respectively, it is diﬃcult to make their space cost ab solutely the\n29\n\nMethod Sample Pre-Queries Model SpaceCost(KB)\nLAQP 1000 250 12KB 184\nLAQP 2000 250 12KB 296\nLAQP 2000 500 13KB 358\nLAQP 2000 800 14KB 430\nLAQP 2000 1000 14KB 534\nAQP++ 1000 250 172\nAQP++ 1000 500 232\nAQP++ 2000 500 344\nAQP++ 2000 800 416\nAQP++ 2000 1000 464\nSAQP 1000 112\nSAQP 2000 224\nSAQP 3000 336\nSAQP 4000 448\nSAQP 5000 560\nTable 3: Settings for the space cost experiment.\nsamewith that ofthe SAQP.We adopteddiﬀerent settings to ﬁnd th e space cost\nof LAQP and AQP++ in the range similar to that of the SAQP. The settin gs\n(the number of samples, the number of pre-computed queries, th e space cost\nof the error model, and the total space cost) of diﬀerent method s are shown in\nTable 3. The experimental results are shown in Figure 9.\nWe can learn from Figure 9 that both the MSE and ARE decrease with t he\nspace cost. This phenomenon is absolutely reasonable since more sa mples, more\npre-computed queries and more complex model lead to higher accur acy. The\nestimation error of our method is lower than that of the SAQP and AQ P++\nfor the most of the time when only a little space are provided. The dist ribution\nof a small sample has little chance to be similar to the entire data. Insu ﬃcient\npre-computed queriesalso makeit diﬃcult to ﬁnd a similar pre-comput ed query.\n30\n\n●LAQP■AQP++◆SAQP\n●\n●●● ●■■\n■■■◆\n◆\n◆\n◆◆2 \u0002 \u0003 3 \u0004 \u0005 4 \u0006 \u000750005000001.0×10\n61.5×10\n\b\nSpacecost (KB)MSE\n(a) MSE●LAQP■AQP++◆SAQP\n●\n●●● ●■\n■\n■\n■\n■◆\n◆\n◆\n◆◆\t \n \u000b \f \r \u000e \u000f \u0010 \u0011 500\n0 \u0012 \u0013\n\u0014 \u0015 \u0016\n\u0017 \u0018 \u0019\n\u001a \u001b \u001c\n\u001d \u001e \u001f\nSpacecost (KB)ARE\n(b) ARE\nFigure 9: The impact of the space cost\nHowever, the error model can learn the estimation error of a small sample while\noccupying little space. That is the reason why LAQP outperforms th e other\nmethods when the provided space is limited. However, when enough s amples\nand pre-computed queries are provided, LAQP will not be better th an the other\ntwo methods.\n6.4. Eﬃciency\nWe test the eﬃciency of LAQP, and compare our method with the exis ting\nmethods. We compare the average processing time of estimating 10 0 queries.\nWe also test the inﬂuence of the dimensions on the processing time. T he query\ntime of the LAQP includes the prediction, ﬁnding the nearest query, and the\nsampling-based estimation. The aggregation function in these expe riments is\nCOUNT. The pre-computation and the training time are oﬀ-line proce ss, and\nthey are not included in the following experiments.\nEXP1: We compare the query processing time of the LAQP, AQP++, SAQP\nand DBEst on the PM2.5 dataset. The sample includes 4K tuples. The n umber\nof the pre-computed queries is 100. The average processing time o f 100 one-\ndimensional queries are shown in Figure 10. We can learn that our LAQ P is\ncomparable to the other methods, even though the processing tim e of LAQP\nincludes three parts.\nEXP2: We compare the query processing time of the LAQP, AQP++, and\nSAQP on the POWER dataset. The query predicate varies from one d imension\nto seven dimensions. The sample in this experiment includes 20K tuples . We\n31\n\nP  ! \" # $ % & ' ( F ) * + , - . / 1 5 7 8 9 : ; < Sampling -basedestimationL = > ?AQP++SAQP D @ B C E\nG H×I J-K\nM N O Q R\nS T U V W\nX Y Z [ \\]^_`apbcdefghij time(s)\nFigure 10: Eﬃciency●\nk l m n■AQP++◆SAQP\n●●●●●●●\n■■■■■■■\n◆◆◆◆◆◆◆\n0o q r s t u v\nw xy z\n{ |} ~\n  \n  \n  \nDimensionQueryprocessingtime (s)\nFigure 11: The impact of dimension\ncomputetheprocessingtimeof100queriesforeachkindofaggreg ationfunction.\nThe experimental results are shown in Figure 11. The processing tim e of these\nthree methods all increaseswith the number of dimensions. The pro cessingtime\nof LAQP and AQP++is nearly two times of the SAQP. The reason is that both\nthe processing time of LAQP and AQP++ includes estimating the given q uery\nand a pre-computed query based on a sample. If the sampling-base d estimation\nof each pre-computed query is computed oﬀ-line and stored in memo ry, the\nquery time of them will be close to that of the SAQP.\n6.5. Other aggregation functions\nIn this section, we test the performance of LAQP on the other agg regation\nfunctionsincludingtheVAR,STD,MINandMAX.WecompareourLAQP with\nthe SAQP and AQP++ on the PM2.5 dataset. One hundred one-dimens ional\npre-computed queries for each kind of aggregationfunction are u sed to train the\nerror model. The result of this experiment is shown in Figure 12. The r esult of\nthe queries involving the MIN aggregation function is nearly 0 for mos t of the\ntime, suggesting that, even a little absolute error becomes a high re lative error.\nWe can learn from the ﬁgure that our LAQP has better performanc e for most\nof the time.\n32\n\nLAQPAQP++SAQP\nVar Std Min Max100100010\n10\n10\n\n  \n(a) MSELAQPAQP++SAQP\nVar Std Min Max\n \n \n \n \n \n  ¡ ¢\n(b) ARE\nFigure 12: Diﬀerent aggregation functions.\nLAQP£ ¤ ¥ ¦ § ¨ © ª « ¬ ­LAQP\nCount Sum ® ¯°10010\n±10\n²10\n³\n´ µ ¶\n(a) MSELAQP · ¸ ¹ º » ¼ ½ ¾ ¿ À ÁLAQP\nCount Sum Â Ã Ä\nÅ Æ Ç È\nÉ Ê Ë Ì\nÍ Î Ï Ð\nÑ Ò Ó Ô\nÕ Ö × Ø\nÙ Ú Û\n(b) ARE\nFigure 13: LAQP vs. DiversiﬁedLAQP\n6.6. The Beneﬁts from the Diversiﬁcation\nWe test the improvement of LAQP by diversiﬁcation. We use a random ly\nchosen query log including 200 one-dimensional queries and a diversiﬁ ed query\nlog including 200 queries to train the error model, respectively. The d iversiﬁca-\ntion method in this experiment is the MaxMin method, which greedily inse rts\nthe query maximizing the minimum distance to the existing queries into t he\ndiversiﬁed log. This experiment was conducted on the PM2.5 dataset . The\nperformance of these two situations are shown in Figure 13. The LA QP with\nthe diversiﬁed query log is called the ‘DiversiﬁedLAQP’ for short in the ﬁgure.\nThis ﬁgure indicates that the diversiﬁed query log improves the accu racy of the\nLAQP, which is coincident with the analysis in Section 5.1 .\n33\n\n●Ü Ý Þ ß à á â ã =1■ä å æ ç è é êë =ì\n●●●●●●●●●●●\n■\n■\n■■■■■■■■■\n0.0 í î ï ð ñ ò ó ô õ ö ÷ ø1.0\nù ú û\nü ý þ\nÿ 2 \u0000\n3 \u0001 \u0002\n\u0003 \u0004 \u0005\nαObjectFunction\n(a) Optimize αα=0α=optα=1\nCount Sum Avg0.05\n0 \u0006 \u0007 \b0.50\n1ARE\n(b) ARE\nFigure 14: Optimized LAQP\n6.7. The Beneﬁts from the Optimization\nWe test the improvement of LAQP by optimization. This experiment wa s\nconducted on the PM2.5 dataset. We modify the maxdepthof the Random-\nForest to form the error models with diﬀerent reliability. We randomly choose\n100 one-dimensional queries as the training set, and another 100 q ueries as the\ntesting set. We can learn the inﬂuence of the weight αon the Object Function\nin Equation 10 from the Figure 14(a). As discussed in Section 5.2, the lower\nthe result of the Object Function, the better performance on th e testing set.\nWhen the error model is not well tuned ( maxdepth= 1), the result of the Ob-\nject Functionincreaseswith α. Inthis situation, the range-similarpre-computed\nquery is more reliable than the error-similarone. The result of the Ob ject Func-\ntion decreases with the αwhen the error model is well tuned ( maxdepth= 2),\nsince the error-similar pre-computed query is more reliable. We also t est the\nperformance on diﬀerent kinds of aggregationfunctions. In this e xperiment, the\noptimization of the parameter αwas computed by the method ‘bounded’ in the\nscipy.optimize.minimize scalar. As shown in Figure 14(b), optimizing the value\nofαreally improves the accuracy.\n6.8. The summary of the experiments\nFrom above experiments, we have following conclusions.\n34\n\n1. The accuracy of our LAQP outperforms the SAQP, AQP++ and DB Est\nwhen the sample are limited in a small size.\n2. The superiority of LAQP to handle multi-dimensional data is evident .\n3. The eﬃciency of LAQP is comparable to other AQP methods.\n4. The performance of LAQP can be improved by diversiﬁcation and o pti-\nmization.\n5. The LAQP performs well for most typical aggregationfunctions including\nthe COUNT, SUM, AVG, VAR, STD, MIN and MAX.\n7. Conclusions\nIn this work, we proposed a learning-based AQP method. We make a c om-\nbination of a regression model, the sampling-based AQP and the pre- computed\naggregations to provide more accurate approximate query answe rs. Our LAQP\nsupports most of the typical queries supported by the sampling-b ased method.\nThe performance of LAQP can be improved by involving the diversiﬁca tion and\noptimization. The experimental results indicate that our method ou tperforms\nthe representative exiting methods including the sampling-based AQ P method,\nthe pre-computed aggregations based method, and the most rec ent learning-\nbased method. In this work, we learn the error model based on a giv en query\nlog. We will try to automatically generate the pre-computed synops es for AQP\nin our future study.\nReferences\n[1] Acharya, S., Gibbons, P.B., Poosala, V., 2000. Congressional\nsamples for approximate answering of group-by queries, in: Pro-\nceedings of the 2000 ACM SIGMOD International Conference on\nManagement of Data, May 16-18, 2000, Dallas, Texas, USA.,\npp. 487–498. URL: https://doi.org/10.1145/342009.335450 ,\ndoi:10.1145/342009.335450 .\n35\n\n[2] Agarwal, S., Milner, H., Kleiner, A., Talwalkar, A., Jordan, M.I.,\nMadden, S., Mozafari, B., Stoica, I., 2014. Knowing when\nyou’re wrong: building fast and reliable approximate query pro-\ncessing systems, in: International Conference on Management o f\nData, SIGMOD 2014, Snowbird, UT, USA, June 22-27, 2014,\npp. 481–492. URL: https://doi.org/10.1145/2588555.2593667 ,\ndoi:10.1145/2588555.2593667 .\n[3] Agarwal, S., Mozafari, B., Panda, A., Milner, H., Madden, S.,\nStoica, I., 2013. Blinkdb: queries with bounded errors and\nbounded response times on very large data, in: Eighth Eurosys\nConference 2013, EuroSys ’13, Prague, Czech Republic, April 14- 17,\n2013, pp. 29–42. URL: https://doi.org/10.1145/2465351.2465355 ,\ndoi:10.1145/2465351.2465355 .\n[4] Breiman, L., 2001. Random forests. Machine Learning 45,\n5–32. URL: https://doi.org/10.1023/A:1010933404324 ,\ndoi:10.1023/A:1010933404324 .\n[5] Brent, R.P., 2013. Algorithms for minimization without derivatives.\nCourier Corporation.\n[6] Chaudhuri, S., Das, G., Datar, M., Motwani, R., Narasayya,\nV.R., 2001. Overcoming limitations of sampling for aggrega-\ntion queries, in: Proceedings of the 17th International Confer-\nence on Data Engineering, April 2-6, 2001, Heidelberg, Germany,\npp. 534–542. URL: https://doi.org/10.1109/ICDE.2001.914867 ,\ndoi:10.1109/ICDE.2001.914867 .\n[7] Chaudhuri, S., Das, G., Narasayya, V.R., 2007. Optimized strati-\nﬁed sampling for approximate query processing. ACM Trans. Datab ase\nSyst. 32, 9. URL: https://doi.org/10.1145/1242524.1242526 ,\ndoi:10.1145/1242524.1242526 .\n36\n\n[8] Chaudhuri, S., Ding, B., Kandula, S., 2017. Approximate\nquery processing: No silver bullet, in: Proceedings of the 2017\nACM International Conference on Management of Data, SIG-\nMOD Conference 2017, Chicago, IL, USA, May 14-19, 2017, pp.\n511–519. URL: https://doi.org/10.1145/3035918.3056097 ,\ndoi:10.1145/3035918.3056097 .\n[9] Dyreson, C.E., 1996. Information retrieval from an incomplete d ata cube,\nin: VLDB’96, Proceedings of 22th International Conference on Ve ry Large\nData Bases, September 3-6, 1996, Mumbai (Bombay), India, pp. 532–543.\nURL:http://www.vldb.org/conf/1996/P532.PDF .\n[10] Ganti, V., Lee, M., Ramakrishnan, R., 2000. ICICLES: self-\ntuning samples for approximate query answering, in: VLDB 2000,\nProceedings of 26th International Conference on Very Large Da ta\nBases, September 10-14, 2000, Cairo, Egypt, pp. 176–187. URL :\nhttp://www.vldb.org/conf/2000/P176.pdf .\n[11] Garofalakis, M.N., Gibbons, P.B., 2002. Wavelet synopses with err or guar-\nantees, in: Proceedings of the 2002 ACM SIGMOD International Co n-\nference on Management of Data, Madison, Wisconsin, USA, June 3- 6,\n2002, pp. 476–487. URL: http://doi.acm.org/10.1145/564691.564746 ,\ndoi:10.1145/564691.564746 .\n[12] Gibbons, P.B., Matias, Y., Poosala, V., 2002. Fast incremen-\ntal maintenance of approximate histograms. ACM Trans. Databas e\nSyst. 27, 261–298. URL: http://doi.acm.org/10.1145/581751.581753 ,\ndoi:10.1145/581751.581753 .\n[13] Gollapudi, S., Sharma, A., 2009. An axiomatic approach for result diver-\nsiﬁcation, in: Proceedings of the 18th International Conference on World\nWide Web, WWW 2009, Madrid, Spain, April 20-24, 2009, pp. 381–390 .\n[14] Gong, Z., Zhong, P., Hu, W., 2019. Diversity\nin machine learning. IEEE Access 7, 64323–64350.\n37\n\nURL: https://doi.org/10.1109/ACCESS.2019.2917620 ,\ndoi:10.1109/ACCESS.2019.2917620 .\n[15] Inoue, T., Krishna, A., Gopalan, R.P., 2016. Approxi-\nmate query processing on high dimensionality database ta-\nbles using multidimensional cluster sampling view. JSW 11,\n80–93. URL: https://doi.org/10.17706/jsw.11.1.80-93 ,\ndoi:10.17706/jsw.11.1.80-93 .\n[16] Jermaine, C., 2003. Robust estimation with sampling and approxim ate\npre-aggregation, in: Proceedings of 29th International Confer ence on Very\nLargeDataBases,VLDB2003,Berlin, Germany,September9-12, 2003,pp.\n886–897. URL: http://www.vldb.org/conf/2003/papers/S26P03.pdf ,\ndoi:10.1016/B978-012722442-8/50083-5 .\n[17] Jermaine, C., Miller, R.J., 2000. Approximate query answering in hig h-\ndimensional data cubes, in: 2000 ACM SIGMOD Workshop on Researc h\nIssues in Data Mining and Knowledge Discovery, Dallas, Texas, USA, M ay\n14, 2000, pp. 31–36.\n[18] Kamat, N., Jayachandran, P., Tunga, K., Nandi, A., 2014. Distrib uted and\ninteractive cube exploration, in: IEEE 30th International Confer ence on\nData Engineering, Chicago, ICDE 2014, IL, USA, March 31 - April 4, 2014,\npp. 472–483. URL: https://doi.org/10.1109/ICDE.2014.6816674 ,\ndoi:10.1109/ICDE.2014.6816674 .\n[19] Kamat, N., Nandi, A., 2018. A session-based approach to fast- but-\napproximate interactive data cube exploration. TKDD 12, 9:1–9:26. URL:\nhttps://doi.org/10.1145/3070648 , doi:10.1145/3070648 .\n[20] Kraska, T., Beutel, A., Chi, E.H., Dean, J., Polyzotis, N.,\n2018. The case for learned index structures, in: Proceedings of\nthe 2018 International Conference on Management of Data, SIG -\nMOD Conference 2018, Houston, TX, USA, June 10-15, 2018,\n38\n\npp. 489–504. URL: https://doi.org/10.1145/3183713.3196909 ,\ndoi:10.1145/3183713.3196909 .\n[21] Li, K., Li, G., 2018. Approximate query processing: What\nis new and where to go? - A survey on approximate\nquery processing. Data Science and Engineering 3, 379–\n397. URL: https://doi.org/10.1007/s41019-018-0074-4 ,\ndoi:10.1007/s41019-018-0074-4 .\n[22] Li, X., Han, J., Yin, Z., Lee, J., Sun, Y., 2008. Sampling\ncube: a framework for statistical olap over sampling data, in: Pro-\nceedings of the ACM SIGMOD International Conference on Manage -\nment of Data, SIGMOD 2008, Vancouver, BC, Canada, June 10-12 ,\n2008, pp. 779–790. URL: https://doi.org/10.1145/1376616.1376695 ,\ndoi:10.1145/1376616.1376695 .\n[23] Ma, Q., Triantaﬁllou, P., 2019. Dbest: Revisiting approximate que ry\nprocessing engines with machine learning models, in: Proceedings of t he\n2019 International Conference on Management of Data, SIGMOD Con-\nference 2019, Amsterdam, The Netherlands, June 30 - July 5, 201 9.,\npp. 1553–1570. URL: https://doi.org/10.1145/3299869.3324958 ,\ndoi:10.1145/3299869.3324958 .\n[24] Mozafari, B., Niu, N., 2015. A handbook for building an ap-\nproximate query engine. IEEE Data Eng. Bull. 38, 3–29. URL:\nhttp://sites.computer.org/debull/A15sept/p3.pdf .\n[25] Mumick, I.S., Quass, D., Mumick, B.S., 1997. Maintenance\nof data cubes and summary tables in a warehouse, in: SIG-\nMOD 1997, Proceedings ACM SIGMOD International Conference\non Management of Data, May 13-15, 1997, Tucson, Arizona,\nUSA., pp. 100–111. URL: https://doi.org/10.1145/253260.253277 ,\ndoi:10.1145/253260.253277 .\n39\n\n[26] Olma, M., Papapetrou, O., Appuswamy, R., Ailamaki, A., 2019.\nTaster: Self-tuning, elastic and online approximate query pro-\ncessing, in: 35th IEEE International Conference on Data En-\ngineering, ICDE 2019, Macao, China, April 8-11, 2019, pp.\n482–493. URL: https://doi.org/10.1109/ICDE.2019.00050 ,\ndoi:10.1109/ICDE.2019.00050 .\n[27] Peng, J., Zhang, D., Wang, J., Pei, J., 2018. AQP++: con-\nnecting approximate query processing with aggregate precom-\nputation for interactive analytics, in: Proceedings of the 2018\nInternational Conference on Management of Data, SIGMOD\nConference 2018, Houston, TX, USA, June 10-15, 2018, pp.\n1477–1492. URL: https://doi.org/10.1145/3183713.3183747 ,\ndoi:10.1145/3183713.3183747 .\n[28] Roy, P., Khan, A., Alonso, G., 2016. Augmented sketch: Faster\nand more accurate stream processing, in: Proceedings of the 201 6\nInternational Conference on Management of Data, SIGMOD Conf er-\nence 2016, San Francisco, CA, USA, June 26 - July 01, 2016,\npp. 1449–1463. URL: http://doi.acm.org/10.1145/2882903.2882948 ,\ndoi:10.1145/2882903.2882948 .\n[29] Schmidt, P., Reiss, A., D¨ urichen, R., Marberger, C., Laerhoven , K.V.,\n2018. Introducing wesad, a multimodal dataset for wearable stre ss and\naﬀect detection, in: Proceedings of the 2018 on International Co nference\non Multimodal Interaction, ICMI 2018, Boulder, CO, USA, October 16-20,\n2018, pp. 400–408. URL: https://doi.org/10.1145/3242969.3242985 ,\ndoi:10.1145/3242969.3242985 .\n[30] Sidirourgos, L., Kersten, M.L., Boncz, P.A., 2011. Sciborq: Scien tiﬁc\ndata management with bounds on runtime and quality, in: CIDR 2011,\nFifth Biennial Conference on Innovative Data Systems Research, Asilomar,\n40\n\nCA, USA, January 9-12, 2011, Online Proceedings, pp. 296–301. U RL:\nhttp://cidrdb.org/cidr2011/Papers/CIDR11_Paper39.pd f.\n[31] Thirumuruganathan, S., Hasan, S., Koudas, N., Das, G., 2019.\nApproximate query processing using deep generative models.\nCoRR abs/1903.10000. URL: http://arxiv.org/abs/1903.10000 ,\narXiv:1903.10000 .\n[32] Vieira, M.R., Razente, H.L., Barioni, M.C.N., Hadjieleftheriou, M.,\nSrivastava, D., Jr., C.T., Tsotras, V.J., 2011. On query result di-\nversiﬁcation, in: Proceedings of the 27th International Confere nce on\nData Engineering, ICDE 2011, April 11-16, 2011, Hannover, Germ any,\npp. 1163–1174. URL: https://doi.org/10.1109/ICDE.2011.5767846 ,\ndoi:10.1109/ICDE.2011.5767846 .\n41\n\nLAQPAQP++SAQPV \t\n S \u000b\f M\r \u000e \u000f\u0010\u0011\n\u0012\u0013\u0014\n\u0015\u0016\u0017\u0018\n\u0019\u001a\n4\n\u001b\u001c5\n\u001d\u001e\n6\n\u001f !\n\n\nCount Sum AvgARE\n\n\nCount Sum\nL\" # $ D % & ' (\nAvg\n)*\n+,-.\n/55\n78\n9\n:;\n<\n=>\n? @\nAB C\n",
  "textLength": 70841
}