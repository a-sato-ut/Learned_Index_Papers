{
  "paperId": "a289100678e7d94af836d91cd48d7821ebc5b83d",
  "title": "Recommender Systems with Generative Retrieval",
  "pdfPath": "a289100678e7d94af836d91cd48d7821ebc5b83d.pdf",
  "text": "Recommender Systems with Generative Retrieval\nShashank Rajput⋆\nUniversity of Wisconsin-MadisonNikhil Mehta⋆\nGoogle DeepMindAnima Singh\nGoogle DeepMind\nRaghunandan Keshavan\nGoogleTrung Vu\nGoogleLukasz Heldt\nGoogleLichan Hong\nGoogle DeepMind\nYi Tay\nGoogle DeepMindVinh Q. Tran\nGoogleJonah Samost\nGoogleMaciej Kula\nGoogle DeepMind\nEd H. Chi\nGoogle DeepMindMaheswaran Sathiamoorthy\nGoogle DeepMind\nAbstract\nModern recommender systems perform large-scale retrieval by embedding queries\nand item candidates in the same unified space, followed by approximate nearest\nneighbor search to select top candidates given a query embedding. In this paper,\nwe propose a novel generative retrieval approach, where the retrieval model au-\ntoregressively decodes the identifiers of the target candidates. To that end, we\ncreate semantically meaningful tuple of codewords to serve as a Semantic ID for\neach item. Given Semantic IDs for items in a user session, a Transformer-based\nsequence-to-sequence model is trained to predict the Semantic ID of the next\nitem that the user will interact with. We show that recommender systems trained\nwith the proposed paradigm significantly outperform the current SOTA models on\nvarious datasets. In addition, we show that incorporating Semantic IDs into the\nsequence-to-sequence model enhances its ability to generalize, as evidenced by\nthe improved retrieval performance observed for items with no prior interaction\nhistory.\n1 Introduction\nOr an g e shoe s, Br and X \nA t o mi c I t em ID: 233 R e d shoe s, Br and Y \nA t o mi c I t em ID: 5 1 5 \nOr an g e shoe s, Br and Y \nA t o mi c I t em ID: 64 \nSem an t i c ID \nG ener a t o r \n(5, 23 , 55) (5, 25, 7 8) (5, 25, 55) I t em \nL oo kup \nG ener a t iv e \nR e tri e v al Us er His t o r y N e xt i t em \nNeurips Version \nFigure 1: Overview of the Transformer Index for GEnerative\nRecommenders (TIGER) framework. With TIGER, sequential\nrecommendation is expressed as a generative retrieval task by\nrepresenting each item as a tuple of discrete semantic tokens.Recommender systems help users discover\ncontent of interest and are ubiquitous in\nvarious recommendation domains such as\nvideos [ 4,43,9], apps [ 3], products [ 6,8],\nand music [ 18,19]. Modern recommender\nsystems adopt a retrieve-and-rank strategy,\nwhere a set of viable candidates are se-\nlected in the retrieval stage, which are then\nranked using a ranker model. Since the\nranker model works only on the candidates\nit receives, it is desired that the retrieval\nstage emits highly relevant candidates.\n⋆Equal contribution. Work done when SR was at Google.\nCorrespondence to rajput.shashank11@gmail.com, nikhilmehta@google.com, nlogn@google.com.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).arXiv:2305.05065v3  [cs.IR]  3 Nov 2023\n\nThere are standard and well-established methods for building retrieval models. Matrix factoriza-\ntion [ 19] learns query and candidate embeddings in the same space. In order to better capture the\nnon-linearities in the data, dual-encoder architectures [ 39] (i.e., one tower for the query and another\nfor the candidate) employing inner-product to embed the query and candidate embeddings in the same\nspace have become popular in recent years. To use these models during inference, an index that stores\nthe embeddings for all items is created using the candidate tower. For a given query, its embedding is\nobtained using the query tower, and an Approximate Nearest Neighbors (ANN) algorithm is used\nfor retrieval. In recent years, the dual encoders architectures have also been extended for sequential\nrecommendations [ 11,24,41,17,32,6,44] that explicitly take into account the order of user-item\ninteractions.\nWe propose a new paradigm of building generative retrieval models for sequential recommendation.\nInstead of traditional query-candidate matching approaches, our method uses an end-to-end generative\nmodel that predicts the candidate IDs directly. We propose to leverage the Transformer [ 36] memory\n(parameters) as an end-to-end index for retrieval in recommendation systems, reminiscent of Tay et al.\n[34] that used Transformer memory for document retrieval. We refer to our method as Transformer\nIndex for GEnerative Recommenders (TIGER). A high-level overview of TIGER is shown in Figure 1.\nTIGER is uniquely characterized by a novel semantic representation of items called \"Semantic ID\"\n– a sequence of tokens derived from each item’s content information. Concretely, given an item’s\ntext features, we use a pre-trained text encoder (e.g., SentenceT5 [ 27]) to generate dense content\nembeddings. A quantization scheme is then applied on the embedding of an item to form a set\nof ordered tokens/codewords, which we refer to as the Semantic ID of the item. Ultimately, these\nSemantic IDs are used to train the Transformer model on the sequential recommendation task.\nRepresenting items as a sequence of semantic tokens has many advantages. Training the transformer\nmemory on semantically meaningful data allows knowledge sharing across similar items. This allows\nus to dispense away with the atomic and random item Ids that have been previously used [ 33,42,11,8]\nas item features in recommendation models. With semantic token representations for items, the\nmodel is less prone to the inherent feedback loop [ 1,26,39] in recommendation systems, allowing\nthe model to generalize to newly added items to the corpus. Furthermore, using a sequence of tokens\nfor item representation helps alleviate the challenges associated with the scale of the item corpus; the\nnumber of items that can be represented using tokens is the product of the cardinality of each token\nin the sequence. Typically, the item corpus size can be in the order of billions and learning a unique\nembedding for each item can be memory-intensive. While random hashing-based techniques [ 16]\ncan be adopted to reduce the item representation space, in this work, we show that using semantically\nmeaningful tokens for item representation is an appealing alternative. The main contributions of this\nwork are summarized below:\n1.We propose TIGER, a novel generative retrieval-based recommendation framework that assigns\nSemantic IDs to each item, and trains a retrieval model to predict the Semantic ID of an item that\na given user may engage with.\n2.We show that TIGER outperforms existing SOTA recommender systems on multiple datasets as\nmeasured by recall and NDCG metrics.\n3.We find that this new paradigm of generative retrieval leads to two additional capabilities in\nsequential recommender systems: 1. Ability to recommend new and infrequent items, thus\nimproving cold-start recommendations, and 2. Ability to generate diverse recommendations\nusing a tunable parameter.\nPaper Overview. In Section 2, we provide a brief literature survey of recommender systems,\ngenerative retrieval, and the Semantic ID generation techniques we use in this paper. In Section 3,\nwe explain our proposed framework, and outline the various techniques we use for Semantic ID\ngeneration. We present the result of our experiments in Section 4, and conclude the paper in Section 5.\n2 Related Work\nSequential Recommenders. Using deep sequential models in recommender systems has developed\ninto a rich literature. GRU4REC [ 11] was the first to use GRU based RNNs for sequential recom-\nmendations. Li et al. [ 24] proposed Neural Attentive Session-based Recommendation (NARM),\nwhere an attention mechanism along with a GRU layer is used to track long term intent of the user.\n2\n\n(a) Semantic ID generation for items using\nquantization of content embeddings.\nBi dir e c t i o n al T r an sf o rmer E nc od er \nUser_5 t_ u5 \nI t em 5 1 5 \nSem. ID = (5, 25, 7 8) I t em 233 \nSem. ID = (5, 23 , 55) t_23 t_55 t_5 T o k en s T r an sf o rmer D e c od er \n<BOS> I t em 64 \nSem. ID = (5, 25, 55) \nI t em In t er a c t i o n His t o r y o f Us er 5 t_25 t_7 8 t_5 E nc od e d \nC o n t e xt t_25 t_5 \nt_5 <E OS> t_55 \nt_25 t_55 N e xt I t em \nNeurips Version (b) Transformer based encoder-decoder setup for building the\nsequence-to-sequence model used for generative retrieval.\nFigure 2: An overview of the modeling approach used in TIGER.\nAttRec [ 41] proposed by Zhang et al. used self-attention mechanism to model the user’s intent\nin the current session, and personalization is ensured by modeling user-item affinity with metric\nlearning. Concurrently, Kang et al. also proposed SASRec [ 17], which used self-attention similar to\ndecoder-only transformer models. Inspired by the success of masked language modeling in language\ntasks, BERT4Rec [ 32] and Transformers4Rec [ 6] utilize transformer models with masking strate-\ngies for sequential recommendation tasks. S3-Rec [ 44] goes beyond just masking by pre-training\non four self-supervised tasks to improve data representation. The models described above learn a\nhigh-dimensional embedding for each item and perform an ANN in a Maximum Inner Product Search\n(MIPS) space to predict the next item. In contrast, our proposed technique, TIGER, uses Generative\nRetrieval to directly predict the Semantic ID of the next item.\nP5 [8] fine-tunes a pre-trained large language models for multi-task recommender systems. The P5\nmodel relies on the LLM tokenizer (SentencePiece tokenizer [ 29]) to generate tokens from randomly-\nassigned item IDs. Whereas, we use Semantic ID representation of items thay are learned based on the\ncontent information of the items. In our experiments (Table 2), we demonstrate that recommendation\nsystems based on Semantic ID representation of items yield much better results than using random\ncodes.\nSemantic IDs. Hou et al. proposed VQ-Rec [ 12] to generate “codes” (analogous to Semantic IDs)\nusing content information for item representation. However, their focus is on building transferable\nrecommender systems, and do not use the codes in a generative manner for retrieval. While they\nalso use product quantization [ 15] to generate the codes, we use RQ-V AE to generate Semantic IDs,\nwhich leads to hierarchical representation of items (Section 4.2). In a concurrent work to us, Singh et\nal. [31] show that hierarchical Semantic IDs can be used to replace item IDs for ranking models in\nlarge scale recommender systems improves model generalization.\nGenerative Retrieval. While techniques for learning search indices have been proposed in the past\n[20], generative retrieval is a recently developed approach for document retrieval, where the task is to\nreturn a set of relevant documents from a database. Some examples include GENRE [ 5], DSI [ 34],\nNCI [ 37], and CGR [ 22]. A more detailed coverage of the related work is in Appendix A. To the best\nof our knowledge, we are the first to propose generative retrieval for recommendation systems using\nSemantic ID representation of items.\n3 Proposed Framework\nOur proposed framework consists of two stages:\n1.Semantic ID generation using content features. This involves encoding the item content features\nto embedding vectors and quantizing the embedding into a tuple of semantic codewords. The\nresulting tuple of codewords is referred to as the item’s Semantic ID.\n2.Training a generative recommender system on Semantic IDs. A Transformer model is trained on\nthe sequential recommendation task using sequences of Semantic IDs.\n3\n\nEmbedding \n+ + =DNN \nDecoder \n7 , 1  , 4)    (01234567codebook_1 \n01234567codebook_2 \n01234567codebook_3 \nSemantic codes Quantized representation \nd=2 \n= -3 2d=1 \n= -2 1d=0 \n= -1\nEmbedding \nDNN \nEncoder Residual Quantization \nNeurips Version Figure 3: RQ-V AE: In the figure, the vector output by the DNN Encoder, say r0(represented by the blue\nbar), is fed to the quantizer, which works iteratively. First, the closest vector to r0is found in the first level\ncodebook. Let this closest vector be ec0(represented by the red bar). Then, the residual error is computed as\nr1:=r0−ec0. This is fed into the second level of the quantizer, and the process is repeated: The closest vector\ntor1is found in the second level, say ec1(represented by the green bar), and then the second level residual\nerror is computed as r2=r1−e′\nc1. Then, the process is repeated for a third time on r2. The semantic codes\nare computed as the indices of ec0,ec1,andec2in their respective codebooks. In the example shown in the\nfigure, this results in the code (7,1,4).\n3.1 Semantic ID Generation\nIn this section, we describe the Semantic ID generation process for the items in the recommendation\ncorpus. We assume that each item has associated content features that capture useful semantic\ninformation ( e.g.titles or descriptions or images). Moreover, we assume that we have access to a\npre-trained content encoder to generate a semantic embedding x∈Rd. For example, general-purpose\npre-trained text encoders such as Sentence-T5 [ 27] and BERT [ 7] can be used to convert an item’s text\nfeatures to obtain a semantic embedding. The semantic embeddings are then quantized to generate a\nSemantic ID for each item. Figure 2a gives a high-level overview of the process.\nWe define a Semantic ID to be a tuple of codewords of length m. Each codeword in the tuple comes\nfrom a different codebook. The number of items that the Semantic IDs can represent uniquely is\nthus equal to the product of the codebook sizes. While different techniques to generate Semantic IDs\nresult in the IDs having different semantic properties, we want them to at least have the following\nproperty: Similar items (items with similar content features or whose semantic embeddings are close)\nshould have overlapping Semantic IDs. For example, an item with Semantic ID (10,21,35)should\nbe more similar to one with Semantic ID (10,21,40), than an item with ID (10,23,32). Next, we\ndiscuss the quantization schemes which we use for Semantic ID generation.\nRQ-V AE for Semantic IDs. Residual-Quantized Variational AutoEncoder (RQ-V AE) [ 40] is a\nmulti-level vector quantizer that applies quantization on residuals to generate a tuple of codewords\n(aka Semantic IDs). The Autoencoder is jointly trained by updating the quantization codebook and\nthe DNN encoder-decoder parameters. Fig. 3 illustrates the process of generating Semantic IDs\nthrough residual quantization.\nRQ-V AE first encodes the input xvia an encoder Eto learn a latent representation z:=E(x). At the\nzero-th level ( d= 0), the initial residual is simply defined as r0:=z. At each level d, we have a\ncodebook Cd:={ek}K\nk=1, where Kis the codebook size. Then, r0is quantized by mapping it to the\nnearest embedding from that level’s codebook. The index of the closest embedding ecdatd= 0, i.e.,\nc0= arg mini∥r0−ek∥, represents the zero-th codeword. For the next level d= 1, the residual is\ndefined as r1:=r0−ec0. Then, similar to the zero-th level, the code for the first level is computed\nby finding the embedding in the codebook for the first level which is nearest to r1. This process is\nrepeated recursively mtimes to get a tuple of mcodewords that represent the Semantic ID. This\nrecursive approach approximates the input from a coarse-to-fine granularity. Note that we chose to\nuse a separate codebook of size Kfor each of the mlevels, instead of using a single, mK-sized\ncodebook. This was done because the norm of residuals tends to decrease with increasing levels,\nhence allowing for different granularities for different levels.\n4\n\nOnce we have the Semantic ID (c0, . . . , c m−1), a quantized representation of zis computed as\nbz:=Pm−1\nd=0eci. Thenbzis passed to the decoder, which tries to recreate the input xusing\nbz. The RQ-V AE loss is defined as L(x) :=Lrecon+Lrqvae, where Lrecon :=∥x−bx∥2, and\nLrqvae:=Pm−1\nd=0∥sg[ri]−eci∥2+β∥ri−sg[eci]∥2. Herebxis the output of the decoder, and sgis\nthe stop-gradient operation [35]. This loss jointly trains the encoder, decoder, and the codebook.\nAs proposed in [ 40], to prevent RQ-V AE from a codebook collapse, where most of the input gets\nmapped to only a few codebook vectors, we use k-means clustering-based initialization for the\ncodebook. Specifically, we apply the k-means algorithm on the first training batch and use the\ncentroids as initialization.\nOther alternatives for quantization. A simple alternative to generating Semantic IDs is to use\nLocality Sensitive Hashing (LSH). We perform an ablation study in Subsection 4.2 where we find that\nRQ-V AE indeed works better than LSH. Another option is to use k-means clustering hierarchically\n[34], but it loses semantic meaning between different clusters [ 37]. We also tried VQ-V AE, and\nwhile it performs similarly to RQ-V AE for generating the candidates during retrieval, it loses the\nhierarchical nature of the IDs which confers many new capabilities that are discussed in Section 4.3.\nHandling Collisions. Depending on the distribution of semantic embeddings, the choice of codebook\nsize, and the length of codewords, semantic collisions can occur ( i.e., multiple items can map to the\nsame Semantic ID). To remove the collisions, we append an extra token at the end of the ordered\nsemantic codes to make them unique. For example, if two items share the Semantic ID (12,24,52),\nwe append additional tokens to differentiate them, representing the two items as (12,24,52,0)\nand(12,24,52,1). To detect collisions, we maintain a lookup table that maps Semantic IDs to\ncorresponding items. Note that collision detection and fixing is done only once after the RQ-V AE\nmodel is trained. Furthermore, since Semantic IDs are integer tuples, the lookup table is efficient in\nterms of storage in comparison to high dimensional embeddings.\n3.2 Generative Retrieval with Semantic IDs\nWe construct item sequences for every user by sorting chronologically the items they have interacted\nwith. Then, given a sequence of the form (item 1, . . . , itemn), the recommender system’s task is to pre-\ndict the next item itemn+1. We propose a generative approach that directly predicts the Semantic ID of\nthe next item. Formally, let (ci,0, . . . , c i,m−1)be the m-length Semantic ID for itemi. Then, we con-\nvert the item sequence to the sequence (c1,0, . . . , c 1,m−1, c2,0, . . . , c 2,m−1, . . . , c n,0, . . . , c n,m−1).\nThe sequence-to-sequence model is then trained to predict the Semantic ID of itemn+1, which is\n(cn+1,0, . . . , c n+1,m−1). Given the generative nature of our framework, it is possible that a generated\nSemantic ID from the decoder does not match an item in the recommendation corpus. However, as\nwe show in appendix (Fig. 6) the probability of such an event occurring is low. We further discuss\nhow such events can be handled in appendix E.\n4 Experiments\nDatasets. We evaluate the proposed framework on three public real-world benchmarks from the\nAmazon Product Reviews dataset [ 10], containing user reviews and item metadata from May 1996\nto July 2014. In particular, we use three categories of the Amazon Product Reviews dataset for the\nsequential recommendation task: “Beauty”, “Sports and Outdoors”, and “Toys and Games”. We\ndiscuss the dataset statistics and pre-processing in Appendix C.\nEvaluation Metrics. We use top-k Recall (Recall@K) and Normalized Discounted Cumulative Gain\n(NDCG@K) with K= 5,10to evaluate the recommendation performance.\nRQ-V AE Implementation Details. As discussed in section 3.1, RQ-V AE is used to quantize the\nsemantic embedding of an item. We use the pre-trained Sentence-T5 [ 27] model to obtain the\nsemantic embedding of each item in the dataset. In particular, we use item’s content features such\nas title, price, brand, and category to construct a sentence, which is then passed to the pre-trained\nSentence-T5 model to obtain the item’s semantic embedding of 768 dimension.\nThe RQ-V AE model consists of three components: a DNN encoder that encodes the input semantic\nembedding into a latent representation, residual quantizer which outputs a quantized representation,\nand a DNN decoder that decodes the quantized representation back to the semantic input embedding\n5\n\n(a) The ground-truth category distribution for\nall the items in the dataset colored by the\nvalue of the first codeword c1.\n(b) The category distributions for items having the Semantic ID\nas(c1,∗,∗), where c1∈ {1,2,3,4}. The categories are color-\ncoded based on the second semantic token c2.\nFigure 4: Qualitative study of RQ-V AE Semantic IDs (c1, c2, c3, c4)on the Amazon Beauty dataset. We show\nthat the ground-truth categories are distributed across different Semantic tokens. Moreover, the RQV AE semantic\nIDs form a hierarchy of items, where the first semantic token ( c1) corresponds to coarse-level category, while\nsecond/third semantic token ( c2/c3) correspond to fine-grained categories.\nspace. The encoder has three intermediate layers of size 512, 256 and 128 with ReLU activation,\nwith a final latent representation dimension of 32. To quantize this representation, three levels of\nresidual quantization is done. For each level, a codebook of cardinality 256is maintained, where\neach vector in the codebook has a dimension of 32. When computing the total loss, we use β= 0.25.\nThe RQ-V AE model is trained for 20k epochs to ensure high codebook usage ( ≥80%). We use\nAdagrad optimizer with a learning rate of 0.4 and a batch size of 1024. Upon training, we use the\nlearned encoder and the quantization component to generate a 3-tuple Semantic ID for each item. To\navoid multiple items being mapped to the same Semantic ID, we add a unique 4thcode for items that\nshare the same first three codewords, i.e.two items associated with a tuple (7, 1, 4) are assigned (7, 1,\n4, 0) and (7, 1, 4, 1) respectively (if there are no collisions, we still assign 0 as the fourth codeword).\nThis results in a unique Semantic ID of length 4 for each item in the recommendation corpus.\nSequence-to-Sequence Model Implementation Details. We use the open-sourced T5X frame-\nwork [ 28] to implement our transformer based encoder-decoder architecture. To allow the model to\nprocess the input for the sequential recommendation task, the vocabulary of the sequence-to-sequence\nmodel contains the tokens for each semantic codeword. In particular, the vocabulary contains 1024\n(256×4) tokens to represent items in the corpus. In addition to the semantic codewords for items,\nwe add user-specific tokens to the vocabulary. To keep the vocabulary size limited, we only add 2000\ntokens for user IDs. We use the Hashing Trick [ 38] to map the raw user ID to one of the 2000 user ID\ntokens. We construct the input sequence as the user Id token followed by the sequence of Semantic\nID tokens corresponding to a given user’s item interaction history. We found that adding user ID to\nthe input, allows the model to personalize the items retrieved.\nWe use 4 layers each for the transformer-based encoder and decoder models with 6 self-attention\nheads of dimension 64 in each layer. We used the ReLU activation function for all the layers. The\nMLP and the input dimension was set as 1024 and 128, respectively. We used a dropout of 0.1.\nOverall, the model has around 13 million parameters. We train this model for 200k steps for the\n“Beauty” and “Sports and Outdoors” dataset. Due to the smaller size of the “Toys and Games” dataset,\nit is trained only for 100k steps. We use a batch size of 256. The learning rate is 0.01for the first 10k\nsteps and then follows an inverse square root decay schedule.\n4.1 Performance on Sequential Recommendation\nIn this section, we compare our proposed framework for generative retrieval with the following\nsequential recommendation methods (which are described briefly in Appendix B): GRU4Rec [ 11],\nCaser [ 33], HGN [ 25], SASRec [ 17], BERT4Rec [ 32], FDSA [ 42], S3-Rec [ 44], and P5 [ 8]. Notably\n6\n\nTable 1: Performance comparison on sequential recommendation. The last row depicts % improvement with\nTIGER relative to the best baseline. Bold (underline) are used to denote the best (second-best) metric.\nMethodsSports and Outdoors Beauty Toys and Games\nRecall\n@5NDCG\n@5Recall\n@10NDCG\n@10Recall\n@5NDCG\n@5Recall\n@10NDCG\n@10Recall\n@5NDCG\n@5Recall\n@10NDCG\n@10\nP5 [8] 0.0061 0.0041 0.0095 0.0052 0.0163 0.0107 0.0254 0.0136 0.0070 0.0050 0.0121 0.0066\nCaser [33] 0.0116 0.0072 0.0194 0.0097 0.0205 0.0131 0.0347 0.0176 0.0166 0.0107 0.0270 0.0141\nHGN [25] 0.0189 0.0120 0.0313 0.0159 0.0325 0.0206 0.0512 0.0266 0.0321 0.0221 0.0497 0.0277\nGRU4Rec [11] 0.0129 0.0086 0.0204 0.0110 0.0164 0.0099 0.0283 0.0137 0.0097 0.0059 0.0176 0.0084\nBERT4Rec [32] 0.0115 0.0075 0.0191 0.0099 0.0203 0.0124 0.0347 0.0170 0.0116 0.0071 0.0203 0.0099\nFDSA [42] 0.0182 0.0122 0.0288 0.0156 0.0267 0.0163 0.0407 0.0208 0.0228 0.0140 0.0381 0.0189\nSASRec [17] 0.0233 0.0154 0.0350 0.0192 0.0387 0.0249 0.0605 0.0318 0.0463 0.0306 0.0675 0.0374\nS3-Rec [44] 0.0251 0.0161 0.0385 0.0204 0.0387 0.0244 0.0647 0.0327 0.0443 0.0294 0.0700 0.0376\nTIGER [Ours] 0.0264 0.0181 0.0400 0.0225 0.0454 0.0321 0.0648 0.0384 0.0521 0.0371 0.0712 0.0432\n+5.22% +12.55% +3.90% +10.29% +17.31% +29.04% +0.15% +17.43% +12.53% +21.24% +1.71% +14.97%\nall the baselines (except P5), learn a high-dimensional vector space using dual encoder, where the\nuser’s past item interactions and the candidate items are encoded as a high-dimensional representation\nand Maximum Inner Product Search (MIPS) is used to retrieve the next candidate item that the user\nwill potentially interact with. In contrast, our novel generative retrieval framework directly predicts\nthe item’s Semantic ID token-by-token using a sequence-to-sequence model.\nRecommendation Performance. We perform an extensive analysis of our proposed TIGER on\nthe sequential recommendation task and compare against the baselines above. The results for all\nbaselines, except P5, are taken from the publicly accessible results3made available by Zhou et al.\n[44]. For P5, we use the source code made available by the authors. However, for a fair comparison,\nwe updated the data pre-processing method to be consistent with the other baselines and our method.\nWe provide further details related to our changes in Appendix D.\nThe results are shown in Table 1. We observe that TIGER consistently outperforms the existing\nbaselines4. We see significant improvement across all the three benchmarks that we considered. In\nparticular, TIGER performs considerably better on the Beauty benchmark compared to the second-best\nbaseline with up to 29% improvement in NDCG@5 compared to SASRec and 17.3% improvement\nin Recall@5 compared to S3-Rec. Similarly on the Toys and Games dataset, TIGER is 21% and 15%\nbetter in NDCG@5 and NDCG@10, respectively.\n4.2 Item Representation\nIn this section, we analyze several important characteristics of RQ-V AE Semantic IDs. In particular,\nwe first perform a qualitative analysis to observe the hierarchical nature of Semantic IDs. Next, we\nevaluate the importance of our design choice of using RQ-V AE for quantization by contrasting the\nperformance with an alternative hashing-based quantization method. Finally, we perform an ablation\nto study the importance of using Semantic IDs by comparing TIGER with a sequence-to-sequence\nmodel that uses Random ID for item representation.\nQualitative Analysis. We analyze the RQ-V AE Semantic IDs learned for the Amazon Beauty dataset\nin Figure 4. For exposition, we set the number of RQ-V AE levels as 3 with a codebook size of 4, 16,\nand 256 respectively, i.e.for a given Semantic ID (c1, c2, c3)of an item, 0≤c1≤3,0≤c2≤15\nand0≤c3≤255. In Figure 4a, we annotate each item’s category using c1to visualize c1-specific\ncategories in the overall category distribution of the dataset. As shown in Figure 4a, c1captures the\nhigh-level category of the item. For instance, c1= 3contains most of the products related to “Hair”.\nSimilarly, majority of items with c1= 1are “Makeup” and “Skin” products for face, lips and eyes.\nWe also visualize the hierarchical nature of RQ-V AE Semantic IDs by fixing c1and visualizing the\ncategory distribution for all possible values of c2in Fig. 4b. We again found that the second codeword\nc2further categorizes the high-level semantics captured with c1into fine-grained categories. The\nhierarchical nature of Semantic IDs learned by RQ-V AE opens a wide-array of new capabilities\nwhich are discussed in Section 4.3. As opposed to existing recommendation systems that learn item\nembeddings based on random atomic IDs, TIGER uses Semantic IDs where semantically similar\n3https://github.com/aHuiWang/CIKM2020-S3Rec\n4We show in Table 9 that the standard error in the metrics for TIGER is insignificant.\n7\n\nTable 2: Ablation study for different ID generation techniques for generative retrieval. We show that RQ-V AE\nSemantic ID (SID) perform significantly better compared to hashing SIDs and Random IDs.\nMethodsSports and Outdoors Beauty Toys and Games\nRecall\n@5NDCG\n@5Recall\n@10NDCG\n@10Recall\n@5NDCG\n@5Recall\n@10NDCG\n@10Recall\n@5NDCG\n@5Recall\n@10NDCG\n@10\nRandom ID 0.007 0.005 0.0116 0.0063 0.0296 0.0205 0.0434 0.0250 0.0362 0.0270 0.0448 0.0298\nLSH SID 0.0215 0.0146 0.0321 0.0180 0.0379 0.0259 0.0533 0.0309 0.0412 0.0299 0.0566 0.0349\nRQ-V AE SID 0.0264 0.0181 0.0400 0.0225 0.0454 0.0321 0.0648 0.0384 0.0521 0.0371 0.0712 0.0432\n(a) Recall@K vs. K, ( ϵ= 0.1).\n (b) Recall@10 vs. ϵ.\nFigure 5: Performance in the cold-start retrieval setting.\nitems have overlapping codewords, which allows the model to effectively share knowledge from\nsemantically similar items in the dataset.\nHashing vs. RQ-V AE Semantic IDs. We study the importance of RQ-V AE in our framework by\ncomparing RQ-V AE against Locality Sensitive Hashing (LSH) [ 14,13,2] for Semantic ID generation.\nLSH is a popular hashing technique that can be easily adapted to work for our setting. To generate\nLSH Semantic IDs, we use hrandom hyperplanes w1, . . . , whto perform a random projection of the\nembedding vector xand compute the following binary vector: (1w⊤\n1x>0, . . . , 1w⊤\nhx>0). This vector\nis converted into an integer code as c0=Ph\ni=12i−11w⊤\nix>0. This process is repeated mtimes using\nan independent set of random hyperplanes, resulting in mcodewords (c0, c1, . . . , c m−1), which we\nrefer to as the LSH Semantic ID.\nIn Table 2, we compare the performance of LSH Semantic ID with our proposed RQ-V AE Semantic\nID. In this experiment, for LSH Semantic IDs, we used h= 8random hyperplanes and set m= 4to\nensure comparable cardinality with the RQ-V AE. The parameters for the hyperplanes are randomly\nsampled from a standard normal distribution, which ensures that the hyperplanes are spherically\nsymmetric. Our results show that RQ-V AE consistently outperforms LSH. This illustrates that\nlearning Semantic IDs via a non-linear, Deep Neural Network ( DNN ) architecture yields better\nquantization than using random projections, given the same content-based semantic embedding.\nRandom ID vs. Semantic ID. We also compare the importance of Semantic IDs in our generative\nretrieval recommender system. In particular, we compare randomly generated IDs with the Semantic\nIDs. To generate the Random ID baseline, we assign mrandom codewords to each item. A Random\nID of length mfor an item is simply (c1, . . . , c m), where ciis sampled uniformly at random from\n{1,2, . . . , K }. We set m= 4, and K= 255 for the Random ID baseline to make the cardinality\nsimilar to RQ-V AE Semantic IDs. A comparison of Random ID against RQ-V AE and LSH Semantic\nIDs is shown in Table 2. We see that Semantic IDs consistently outperform Random ID baseline,\nhighlighting the importance of leveraging content-based semantic information.\n4.3 New Capabilities\nWe describe two new capabilities that directly follow from our proposed generative retrieval frame-\nwork, namely cold-start recommendations and recommendation diversity. We refer to these capabil-\nities as “new” since existing sequential recommendation models (See the baselines in section 4.1)\ncannot be directly used to satisfy these real-world use cases. These capabilities result from a synergy\nbetween RQ-V AE based Semantic IDs and the generative retrieval approach of our framework. We\ndiscuss how TIGER is used in these settings in the following sections.\n8\n\nCold-Start Recommendation. In this section, we study the cold-start recommendation capability of\nour proposed framework. Due to the fast-changing nature of the real-world recommendation corpus,\nnew items are constantly introduced. Since newly added items lack user impressions in the training\ncorpus, existing recommendation models that use a random atomic ID for item representation fail\nto retrieve new items as potential candidates. In contrast, the TIGER framework can easily perform\ncold-start recommendations since it leverages item semantics when predicting the next item.\nFor this analysis, we consider the Beauty dataset from Amazon Reviews. To simulate newly added\nitems, we remove 5% of test items from the training data split. We refer to these removed items as\nunseen items . Removing the items from the training split ensures there is no data leakage with respect\nto the unseen items. As before, we use Semantic ID of length 4 to represent the items, where the first\n3 tokens are generated using RQ-V AE and the 4thtoken is used to ensure a unique ID exists for all\nthe seen items. We train the RQ-V AE quantizer and the sequence-to-sequence model on the training\nsplit. Once trained, we use the RQ-V AE model to generate the Semantic IDs for all the items in the\ndataset, including any unseen items in the item corpus.\nGiven a Semantic ID (c1, c2, c3, c4)predicted by the model, we retrieve the seen item having the same\ncorresponding ID. Note that by definition, each Semantic ID predicted by the model can match at\nmost one item in the training dataset. Additionally, unseen items having the same first three semantic\ntokens, i.e.(c1, c2, c3)are included to the list of retrieved candidates. Finally, when retrieving a set\nof top-K candidates, we introduce a hyperparameter ϵwhich specifies the maximum proportion of\nunseen items chosen by our framework.\nWe compare the performance of TIGER with the k-Nearest Neighbors (KNN) approach on the\ncold-start recommendation setting in Fig. 5. For KNN, we use the semantic representation space\nto perform the nearest-neighbor search. We refer to the KNN-based baseline as Semantic_KNN.\nFig. 5a shows that our framework with ϵ= 0.1consistently outperforms Semantic_KNN for\nall Recall@K metrics. In Fig. 5b, we provide a comparison between our method and Seman-\ntic_KNN for various values of ϵ. For all settings of ϵ≥0.1, our method outperforms the baseline.\nTable 3: The entropy of the category distribution predicted\nby the model for the Beauty dataset. A higher entropy\ncorresponds more diverse items predicted by the model.\nTemperature Entropy@10 Entropy@20 Entropy@50\nT = 1.0 0.76 1.14 1.70\nT = 1.5 1.14 1.52 2.06\nT = 2.0 1.38 1.76 2.28Recommendation diversity. While Recall\nand NDCG are the primary metrics used to\nevaluate a recommendation system, diversity\nof predictions is another critical objective of\ninterest. A recommender system with poor\ndiversity can be detrimental to the long-term\nengagement of users. Here, we discuss how\nour generative retrieval framework can be used\nto predict diverse items. We show that temperature-based sampling during the decoding process can\nbe effectively used to control the diversity of model predictions. While temperature-based sampling\ncan be applied to any existing recommendation model, TIGER allows sampling across various levels\nof hierarchy owing to the properties of RQ-V AE Semantic IDs. For instance, sampling the first token\nof the Semantic ID allows retrieving items from coarse-level categories, while sampling a token from\nsecond/third token allows sampling items within a category.\nTable 4: Recommendation diversity with temperature-based decoding.\nTarget Category Most-common Categories for top-10 predicted items\nT = 1.0 T = 2.0\nHair Styling Products Hair Styling Products Hair Styling Products, Hair Styling Tools, Skin Face\nTools Nail Tools Nail Tools Nail, Makeup Nails\nMakeup Nails Makeup Nails Makeup Nails, Skin Hands & Nails, Tools Nail\nSkin Eyes Skin Eyes Hair Relaxers, Skin Face, Hair Styling Products, Skin Eyes\nMakeup Face Tools Makeup Brushes,Makeup Face Tools Makeup Brushes, Makeup Face,Skin Face, Makeup Sets, Hair Styling Tools\nHair Loss Products Hair Loss Products,Skin Face, Skin Body Skin Face, Hair Loss Products, Hair Shampoos,Hair & Scalp Treatments, Hair Conditioners\nWe quantitatively measure the diversity of predictions using Entropy@K metric, where the entropy\nis calculated for the distribution of the ground-truth categories of the top-K items predicted by\nthe model. We report the Entropy@K for various temperature values in Table 3. We observe that\ntemperature-sampling in the decoding stage can be effectively used to increase the diversity in the\nground-truth categories of the items. We also perform a qualitative analysis in Table 4.\n9\n\nTable 5: Recall and NDCG metrics for different number layers.\nNumber of Layers Recall@5 NDCG@5 Recall@10 NDCG@10\n3 0.04499 0.03062 0.06699 0.03768\n4 0.0454 0.0321 0.0648 0.0384\n5 0.04633 0.03206 0.06596 0.03834\n4.4 Ablation Study\nWe measure the effect of varying the number of layers in the sequence-to-sequence model in Table 5.\nWe see that the metrics improve slightly as we make the network bigger. We also measure the effect\nof providing user information, the results for which are provided in Table 8 in the Appendix.\n4.5 Invalid IDs\nSince the model decodes the codewords of the target Semantic ID autoregressively, it is possible\nthat the model may predict invalid IDs (i.e., IDs that do not map to any item in the recommendation\ndataset). In our experiments, we used semantic IDs of length 4with each codeword having a\ncardinality of 256(i.e., codebook size = 256 for each level). The number of possible IDs spanned\nby this combination = 2564, which is approximately 4 trillion. On the other hand, the number of\nitems in the datasets we consider is 10K-20K (See Table 6). Even though the number of valid IDs is\nonly a fraction of all complete ID space, we observe that the model almost always predicts valid IDs.\nWe visualize the fraction of invalid IDs produced by TIGER as a function of the number of retrieved\nitems Kin Figure 6. For top-10 predictions, the fraction of invalid IDs varies from ∼0.1%−1.6%\nfor the three datasets. To counter the effect of invalid IDs and to always get top-10 valid IDs, we can\nincrease the beam size and filter the invalid IDs.\nIt is important to note that, despite generating invalid IDs, TIGER achieves state-of-the-art perfor-\nmance when compared to other popular methods used for sequential recommendations. One extension\nto handle invalid tokens could be to do prefix matching when invalid tokens are generated by the\nmodel. Prefix matching of Semantic IDs would allow retrieving items that have similar semantic\nmeaning as the tokens generated by the model. Given the hierarchical nature of our RQ-V AE tokens,\nprefix matching can be thought of as model predicting item category as opposed to the item index.\nNote that such an extension could improve the recall/NDCG metrics even further. We leave such an\nextension as a future work.\n5 Conclusion\nThis paper proposes a novel paradigm, called TIGER, to retrieve candidates in recommender systems\nusing a generative model. Underpinning this method is a novel semantic ID representation for items,\nwhich uses a hierarchical quantizer (RQ-V AE) on content embeddings to generate tokens that form\nthe semantic IDs. Our framework provides results in a model that can be used to train and serve\nwithout creating an index — the transformer memory acts as a semantic index for items. We note\nthat the cardinality of our embedding table does not grow linearly with the cardinality of item space,\nwhich works in our favor compared to systems that need to create large embedding tables during\ntraining or generate an index for every single item. Through experiments on three datasets, we show\nthat our model can achieve SOTA retrieval performance, while generalizing to new and unseen items.\nReferences\n[1]Himan Abdollahpouri, Masoud Mansoury, Robin Burke, and Bamshad Mobasher. The unfair-\nness of popularity bias in recommendation. arXiv preprint arXiv:1907.13286 , 2019.\n[2]Moses S Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings\nof the thiry-fourth annual ACM symposium on Theory of computing , pages 380–388, 2002.\n[3]Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye,\nGlen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. Wide & deep learning for\n10\n\nrecommender systems. In Proceedings of the 1st workshop on deep learning for recommender\nsystems , pages 7–10, 2016.\n[4]Paul Covington, Jay Adams, and Emre Sargin. Deep neural networks for youtube recommenda-\ntions. In Proceedings of the 10th ACM conference on recommender systems , pages 191–198,\n2016.\n[5]Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. Autoregressive entity\nretrieval. arXiv preprint arXiv:2010.00904 , 2020.\n[6]Gabriel de Souza Pereira Moreira, Sara Rabhi, Jeong Min Lee, Ronay Ak, and Even Oldridge.\nTransformers4rec: Bridging the gap between nlp and sequential/session-based recommendation.\nInFifteenth ACM Conference on Recommender Systems , pages 143–153, 2021.\n[7]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 ,\n2018.\n[8]Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. Recommendation\nas language processing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5).\narXiv preprint arXiv:2203.13366 , 2022.\n[9]Carlos A Gomez-Uribe and Neil Hunt. The netflix recommender system: Algorithms, business\nvalue, and innovation. ACM Transactions on Management Information Systems (TMIS) , 6(4):1–\n19, 2015.\n[10] Ruining He and Julian McAuley. Ups and downs: Modeling the visual evolution of fashion\ntrends with one-class collaborative filtering. In proceedings of the 25th international conference\non world wide web , pages 507–517, 2016.\n[11] Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. Session-based\nrecommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939 , 2015.\n[12] Yupeng Hou, Zhankui He, Julian McAuley, and Wayne Xin Zhao. Learning vector-quantized\nitem representation for transferable sequential recommenders. arXiv preprint arXiv:2210.12316 ,\n2022.\n[13] Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the\ncurse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of\ncomputing , pages 604–613, 1998.\n[14] Piotr Indyk, Rajeev Motwani, Prabhakar Raghavan, and Santosh Vempala. Locality-preserving\nhashing in multidimensional spaces. In Proceedings of the twenty-ninth annual ACM symposium\non Theory of computing , pages 618–625, 1997.\n[15] Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor\nsearch. IEEE transactions on pattern analysis and machine intelligence , 33(1):117–128, 2010.\n[16] Wang-Cheng Kang, Derek Zhiyuan Cheng, Tiansheng Yao, Xinyang Yi, Ting Chen, Lichan\nHong, and Ed H Chi. Learning to embed categorical features without embedding tables for\nrecommendation. arXiv preprint arXiv:2010.10784 , 2020.\n[17] Wang-Cheng Kang and Julian McAuley. Self-attentive sequential recommendation. In 2018\nIEEE international conference on data mining (ICDM) , pages 197–206. IEEE, 2018.\n[18] Dongmoon Kim, Kun-su Kim, Kyo-Hyun Park, Jee-Hyong Lee, and Keon Myung Lee. A music\nrecommendation system with a dynamic k-means clustering algorithm. In Sixth international\nconference on machine learning and applications (ICMLA 2007) , pages 399–403. IEEE, 2007.\n[19] Yehuda Koren, Robert Bell, and Chris V olinsky. Matrix factorization techniques for recom-\nmender systems. Computer , 42(8):30–37, 2009.\n[20] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned\nindex structures. In Proceedings of the 2018 international conference on management of data ,\npages 489–504, 2018.\n11\n\n[21] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive\nimage generation using residual quantization. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 11523–11532, 2022.\n[22] Hyunji Lee, Jaeyoung Kim, Hoyeon Chang, Hanseok Oh, Sohee Yang, Vlad Karpukhin, Yi Lu,\nand Minjoon Seo. Contextualized generative retrieval. arXiv preprint arXiv:2210.02068 , 2022.\n[23] Hyunji Lee, Sohee Yang, Hanseok Oh, and Minjoon Seo. Generative retrieval for long sequences.\narXiv preprint arXiv:2204.13596 , 2022.\n[24] Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, and Jun Ma. Neural attentive\nsession-based recommendation. In Proceedings of the 2017 ACM on Conference on Information\nand Knowledge Management , pages 1419–1428, 2017.\n[25] Chen Ma, Peng Kang, and Xue Liu. Hierarchical gating networks for sequential recommen-\ndation. In Proceedings of the 25th ACM SIGKDD international conference on knowledge\ndiscovery & data mining , pages 825–833, 2019.\n[26] Masoud Mansoury, Himan Abdollahpouri, Mykola Pechenizkiy, Bamshad Mobasher, and Robin\nBurke. Feedback loop and bias amplification in recommender systems, 2020.\n[27] Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, and\nYinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models.\nInFindings of the Association for Computational Linguistics: ACL 2022 , pages 1864–1874,\nDublin, Ireland, May 2022. Association for Computational Linguistics.\n[28] Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel\nAndor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor\nLewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini\nSoares, Haitang Hu, Sasha Tsvyashchenko, Aakanksha Chowdhery, Jasmijn Bastings, Jannis\nBulian, Xavier Garcia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, Jonathan H. Clark, Stephan\nLee, Dan Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten\nBosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan\nSaeta, Ryan Sepassi, Alexander Spiridonov, Joshua Newlan, and Andrea Gesmundo. Scaling\nup models and data with t5xandseqio .arXiv preprint arXiv:2203.17189 , 2022.\n[29] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\n[30] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare\nwords with subword units. In Proceedings of the 54th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers) , pages 1715–1725, Berlin, Germany,\nAugust 2016. Association for Computational Linguistics.\n[31] Anima Singh, Trung Vu, Raghunandan Keshavan, Nikhil Mehta, Xinyang Yi, Lichan Hong,\nLukasz Heldt, Li Wei, Ed Chi, and Maheswaran Sathiamoorthy. Better generalization with\nsemantic ids: A case study in ranking for recommendations. arXiv preprint arXiv:2306.08121 ,\n2023.\n[32] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. Bert4rec:\nSequential recommendation with bidirectional encoder representations from transformer. In Pro-\nceedings of the 28th ACM international conference on information and knowledge management ,\npages 1441–1450, 2019.\n[33] Jiaxi Tang and Ke Wang. Personalized top-n sequential recommendation via convolutional\nsequence embedding. In Proceedings of the eleventh ACM international conference on web\nsearch and data mining , pages 565–573, 2018.\n[34] Yi Tay, Vinh Q Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai\nHui, Zhe Zhao, Jai Gupta, et al. Transformer memory as a differentiable search index. arXiv\npreprint arXiv:2202.06991 , 2022.\n[35] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in\nneural information processing systems , 30, 2017.\n12\n\n[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems , 30, 2017.\n[37] Yujing Wang, Yingyan Hou, Haonan Wang, Ziming Miao, Shibin Wu, Hao Sun, Qi Chen,\nYuqing Xia, Chengmin Chi, Guoshuai Zhao, et al. A neural corpus indexer for document\nretrieval. arXiv preprint arXiv:2206.02743 , 2022.\n[38] Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, and Josh Attenberg. Feature\nhashing for large scale multitask learning. In Proceedings of the 26th annual international\nconference on machine learning , pages 1113–1120, 2009.\n[39] Xinyang Yi, Ji Yang, Lichan Hong, Derek Zhiyuan Cheng, Lukasz Heldt, Aditee Kumthekar,\nZhe Zhao, Li Wei, and Ed Chi. Sampling-bias-corrected neural modeling for large corpus item\nrecommendations. In Proceedings of the 13th ACM Conference on Recommender Systems ,\npages 269–277, 2019.\n[40] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi.\nSoundstream: An end-to-end neural audio codec. CoRR , abs/2107.03312, 2021.\n[41] Shuai Zhang, Yi Tay, Lina Yao, and Aixin Sun. Next item recommendation with self-attention.\narXiv preprint arXiv:1808.06414 , 2018.\n[42] Tingting Zhang, Pengpeng Zhao, Yanchi Liu, Victor S Sheng, Jiajie Xu, Deqing Wang, Guan-\nfeng Liu, and Xiaofang Zhou. Feature-level deeper self-attention network for sequential\nrecommendation. In IJCAI , pages 4320–4326, 2019.\n[43] Zhe Zhao, Lichan Hong, Li Wei, Jilin Chen, Aniruddh Nath, Shawn Andrews, Aditee Kumthekar,\nMaheswaran Sathiamoorthy, Xinyang Yi, and Ed Chi. Recommending what video to watch\nnext: a multitask ranking system. In Proceedings of the 13th ACM Conference on Recommender\nSystems , pages 43–51, 2019.\n[44] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan\nWang, and Ji-Rong Wen. S3-rec: Self-supervised learning for sequential recommendation with\nmutual information maximization. In Proceedings of the 29th ACM International Conference\non Information & Knowledge Management , pages 1893–1902, 2020.\n13\n\nA Related Work (cont.)\nGenerative Retrieval Document retrieval traditionally involved training a 2-tower model which\nmapped both queries and documents to the same high-dimensional vector space, followed by perform-\ning an ANN or MIPS for the query over all the documents to return the closest ones. This technique\npresents some disadvantages like having a large embedding table [ 22,23]. Generative retrieval is\na recently proposed technique that aims to fix some of the issues of the traditional approach by\nproducing token by token either the title, name, or the document id string of the document. Cao et\nal. [5] proposed GENRE for entity retrieval, which used a transformer-based architecture to return,\ntoken-by-token, the name of the entity referenced to in a given query. Tay et al. [ 34] proposed DSI\nfor document retrieval, which was the first system to assign structured semantic DocIDs to each\ndocument. Then given a query, the model autoregressively returned the DocID of the document\ntoken-by-token. The DSI work marks a paradigm shift in IR to generative retrieval approaches and is\nthe first successful application of an end-to-end Transformer for retrieval applications. Subsequently,\nLee et al. [ 23] show that generative document retrieval is useful even in the multi-hop setting, where\na complex query cannot be answered directly by a single document, and hence their model generates\nintermediate queries, in a chain-of-thought manner, to ultimately generate the output for the complex\nquery. Wang et al. [ 37] supplement the hierarchical k-means clustering based semantic DocIDs\nof Tay et al. [ 34] by proposing a new decoder architecture that specifically takes into account the\nprefixes in semantic DocIDs. In CGR [ 22], the authors propose a way to take advantage of both\nthe bi-encoder technique and the generative retrieval technique, by allowing the decoder, of their\nencoder-decoder-based model, to learn separate contextualized embeddings which store information\nabout documents intrinsically. To the best of our knowledge, we are the first to use generative\nSemantic IDs created using an auto-encoder (RQ-V AE [40, 21]) for retrieval models.\nVector Quantization. We refer to Vector Quantization as the process of converting a high-\ndimensional vector into a low-dimensional tuple of codewords. One of the most straightforward\ntechniques uses hierarchical clustering, such as the one used in [ 34], where clusters created in a\nparticular iteration are further partitioned into sub-clusters in the next iteration. An alternative popular\napproach is Vector-Quantized Variational AutoEncoder (VQ-V AE), which was introduced in [ 35] as\na way to encode natural images into a sequence of codes. The technique works by first passing the\ninput vector (or image) through an encoder that reduces the dimensionality. The smaller dimensional\nvector is partitioned and each partition is quantized separately, thus resulting in a sequence of codes:\none code per partition. These codes are then used by a decoder to recreate the original vector (or\nimage).\nRQ-V AE [ 40,21] applies residual quantization to the output of the encoder of VQ-V AE to achieve\na lower reconstruction error. We discuss this technique in more detail in Subsection 3.1. Locality\nSensitive Hashing (LSH) [ 14,13] is a popular technique used for clustering and approximate nearest\nneighbor search. The particular version that we use in this paper for clustering is SimHash [ 2], which\nuses random hyperplanes to create binary vectors which serve as hashes of the items. Because it has\nlow computational complexity and is scalable [ 13], we use this as a baseline technique for vector\nquantization.\nB Baselines\nBelow we briefly describe each of the baselines with which we compare TIGER\n•GRU4Rec [ 11] is the first RNN-based approach that uses a customized GRU for the sequential\nrecommendation task.\n•Caser [ 33] uses a CNN architecture for capturing high-order Markov Chains by applying horizon-\ntal and vertical convolutional operations for sequential recommendation.\n•HGN [ 25]: Hierarchical Gating Network captures the long-term and short-term user interests via\na new gating architecture.\n•SASRec [ 17]: Self-Attentive Sequential Recommendation uses a causal mask Transformer to\nmodel a user’s sequential interactions.\n•BERT4Rec [ 32]: BERT4Rec addresses the limitations of uni-directional architectures by using a\nbi-directional self-attention Transformer for the recommendation task.\n14\n\n•FDSA [ 42]: Feature-level Deeper Self-Attention Network incorporates item features in addition\nto the item embeddings as part of the input sequence in the Transformers.\n•S3-Rec [ 44]: Self-Supervised Learning for Sequential Recommendation proposes pre-training a\nbi-directional Transformer on self-supervision tasks to improve the sequential recommendation.\n•P5 [8]: P5 is a recent method that uses a pretrained Large Language Model (LLM) to unify\ndifferent recommendation tasks in a single model.\nC Dataset Statistics\nTable 6: Dataset statistics for the three real-world benchmarks.\nDataset # Users # Items Sequence Length\nMean Median\nBeauty 22,363 12,101 8.87 6\nSports and Outdoors 35,598 18,357 8.32 6\nToys and Games 19,412 11,924 8.63 6\nWe use three public benchmarks from the Amazon Product Reviews dataset [ 10], containing user\nreviews and item metadata from May 1996 to July 2014. We use three categories of the Amazon\nProduct Reviews dataset for the sequential recommendation task: “Beauty”, “Sports and Outdoors”,\nand “Toys and Games”. Table 6 summarizes the statistics of the datasets. We use users’ review history\nto create item sequences sorted by timestamp and filter out users with less than 5 reviews. Following\nthe standard evaluation protocol [ 17,8], we use the leave-one-out strategy for evaluation. For each\nitem sequence, the last item is used for testing, the item before the last is used for validation, and the\nrest is used for training. During training, we limit the number of items in a user’s history to 20.\nD Modifications to the P5 data preprocessing\nTable 7: Results for P5[8] with the standard pre-processing.\nMethodsSports and Outdoors Beauty Toys and Games\nRecall@5 NDCG@5 Recall@10 NDCG@10 Recall@5 NDCG@5 Recall@10 NDCG@10 Recall@5 NDCG@5 Recall@10 NDCG@10\nP5 0.0061 0.0041 0.0095 0.0052 0.0163 0.0107 0.0254 0.0136 0.0070 0.0050 0.0121 0.0066\nP5-ours 0.0107 0.0076 0.01458 0.0088 0.035 0.025 0.048 0.0298 0.018 0.013 0.0235 0.015\nThe P5 source code5pre-processes the Amazon dataset to first create sessions for each user, containing\nchronologically ordered list of items that the user reviewed. After creating these sessions, the original\nitem IDs from the dataset are remapped to integers 1,2,3, . . .6. Hence, the first item in the first\nsession gets an id of ‘1’, the second item, if not seen before, gets an id of ‘2’, and so on. Notably,\nthis pre-processing scheme is applied before creating training and testing splits. This results in the\ncreation of a sequential dataset where many sequences are of the form a, a+ 1, a+ 2, . . .. Given\nthat P5 uses Sentence Piece tokenizer [ 30] (See Section 4.1 in [ 8]), the test and train items in a user\nsession may share the sub-word and can lead to information leakage during inference.\nTo resolve the leakage issue, instead of assigning sequentially increasing integer ids to items, we\nassigned random integer IDs, and then created splits for training and evaluation. The rest of the code\nfor P5 was kept identical to the source code provided in the paper. The results for this dataset are\nreported in Table 7 as the row ‘P5’. We also implemented a version of P5 ourselves from scratch,\nand train the model on only sequential recommendation task. The results for our implementation are\ndepicted as ‘P5-ours’. We were also able to verify in our P5 implementation that using consecutive\ninteger sequences for the item IDs helped us get equivalent or better metrics than those reported in\nP5.\n15\n\nTable 8: The effect of providing user information to the recommender system\nRecall@5 NDCG@5 Recall@10 NDCG@10\nNo user information 0.04458 0.0302 0.06479 0.0367\nWith user id (reported in the paper) 0.0454 0.0321 0.0648 0.0384\nTable 9: The mean and stand error of the metrics for different dataset (computed using 3 runs with\ndifferent random seeds)\nDatasets Recall@5 NDCG@5 Recall@10 NDCG@10\nBeauty 0.0441 ±0.00069 0.0309 ±0.00062 0.0642 ±0.00092 0.0374 ±0.00061\nSports and Outdoors 0.0278 ±0.00069 0.0189 ±0.00043 0.0419 ±0.0010 0.0234 ±0.00048\nToys and Games 0.0518 ±0.00064 0.0375 ±0.00039 0.0698 ±0.0013 0.0433 ±0.00047\n1234567891011121314151617181920\nK0.000.050.100.150.200.250.300.35Invalid IDs (%)\n(a) Sports and Outdoors\n1234567891011121314151617181920\nK0.000.250.500.751.001.251.501.752.00Invalid IDs (%) (b) Beauty\n1234567891011121314151617181920\nK0123456Invalid IDs (%) (c) Toys and Games\nFigure 6: Percentage of invalid IDs when generating Semantic IDs using Beam search for various\nvalues of K. As shown, ∼0.3%−6%of the IDs are invalid when retrieving the top-20 items.\nE Discussion\nEffects of Semantic ID length and codebook size. We tried varying the Semantic ID length and\ncodebook size, such as having an ID consisting of 6codewords each from a codebook of size 64.\nWe noticed that the recommendation metrics for TIGER were robust to these changes. However,\nnote that the input sequence length increases with longer IDs (i.e., more codewords per item ID),\nwhich makes the computation more expensive for our transformer-based sequence-to-sequence model.\nScalability. To test the scalability of Semantic IDs, we ran the following experiment: We combined\nall the three datasets and generated Semantic IDs for the entire set of items from the three datasets.\nThen, we used these Semantic IDs for the recommendation task on the Beauty dataset. We compare\nthe results from this experiment with the original experiment where the Semantic IDs are generated\nonly from the Beauty dataset. The results are provided in Table 10. We see that there is only a small\ndecrease in performance here.\nInference cost. Despite the remarkable success of our model on the sequential recommendation\ntask, we note that our model can be more computationally expensive than ANN-based models during\ninference due to the use of beam search for autoregressive decoding. We emphasize that optimizing\nthe computational efficiency of TIGER was not the main objective of this work. Instead, our work\nopens up a new area of research: Recommender Systems based on Generative Retrieval . As part of\nfuture work, we will consider ways to make the model smaller or explore other ways of improving\nthe inference efficiency.\nMemory cost of lookup tables. We maintain two lookup hash tables for TIGER: an Item ID to\nSemantic ID table and a Semantic ID to Item ID table. Note that both of these tables are generated\nonly once and then frozen: they are generated after the training of the RQ-V AE-based Semantic\nID generation model, and after that, they are frozen for the training of the sequence-to-sequence\n5https://github.com/jeykigung/P5\n6https://github.com/jeykigung/P5/blob/0aaa3fd8366bb6e708c8b70708291f2b0ae90c82/preprocess/data_preprocess_amazon.ipynb\n16\n\nTable 10: Testing scalability by generating the Semantic IDs on the combined dataset vs generating\nthe Semantic IDs on only the Beauty dataset.\nRecall@5 NDCG@5 Recall@10 NDCG@10\nSemantic ID [Combined datasets] 0.04355 0.3047 0.06314 0.03676\nSemantic ID [Amazon Beauty] 0.0454 0.0321 0.0648 0.0384\ntransformer model. Each Semantic ID consists of a tuple of 4 integers, each of which are stored in 8\nbits, hence totalling to 32 bits per item. Each item is represented by an Item ID, stored as a 32 bit\ninteger. Thus, the size of each lookup table will be of the order of 64Nbits, where Nis the number\nof items in the dataset.\nMemory cost of embedding tables . TIGER uses much smaller embedding tables compared to\ntraditional recommender systems. This is because where traditional recommender systems store an\nembedding for each item, TIGER only stores an embedding for each semantic codeword. In our\nexperiments, we used 4 codewords each of cardinality 256 for Semantic ID representation, resulting\nin 1024 (256 ×4) embeddings. For traditional recommender systems, the number of embeddings is\nN, where Nis the number of items in the dataset. In our experiments, Nranged from 10K to 20K\ndepending on the dataset. Hence, the memory cost of TIGER’s embedding table is 1024d, where dis\nthe dimension of the embedding, whereas the memory cost for embedding lookup tables in traditional\nrecommendation systems is Nd.\n17",
  "textLength": 62886
}