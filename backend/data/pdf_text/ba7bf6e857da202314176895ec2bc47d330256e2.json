{
  "paperId": "ba7bf6e857da202314176895ec2bc47d330256e2",
  "title": "Learning to Optimize Join Queries With Deep Reinforcement Learning",
  "pdfPath": "ba7bf6e857da202314176895ec2bc47d330256e2.pdf",
  "text": "Learning to Optimize Join /Q_ueries With Deep\nReinforcement Learning\nSanjay Krishnan1;2, Zongheng Yang1, Ken Goldberg1, Joseph M. Hellerstein1, Ion Stoica1\n1RISELab, UC Berkeley2Computer Science, University of Chicago\nskr@cs.uchicago.edu fzongheng, goldberg, hellerstein, istoica g@berkeley.edu\nABSTRACT\nExhaustive enumeration of all possible join orders is o/f_ten\navoided, and most optimizers leverage heuristics to prune the\nsearch space. /T_he design and implementation of heuristics are\nwell-understood when the cost model is roughly linear, and\nwe /f_ind that these heuristics can be signi/f_icantly suboptimal\nwhen there are non-linearities in cost. Ideally, instead of\na /f_ixed heuristic, we would want a strategy to guide the\nsearch space in a more data-driven way—tailoring the search\nto a speci/f_ic dataset and query workload. Recognizing the\nlink between classical Dynamic Programming enumeration\nmethods and recent results in Reinforcement Learning (RL),\nwe propose a new method for learning optimized join search\nstrategies. We present our RL-based DQ optimizer, which\ncurrently optimizes select-project-join blocks. We implement\nthree versions of DQ to illustrate the ease of integration into\nexisting DBMSes: (1) A version built on top of Apache Calcite,\n(2) a version integrated into PostgreSQL, and (3) a version\nintegrated into SparkSQL. Our extensive evaluation shows\nthat DQ achieves plans with optimization costs and query\nexecution times competitive with the native query optimizer\nin each system, but can execute signi/f_icantly faster a/f_ter\nlearning (o/f_ten by orders of magnitude).\nACM Reference format:\nSanjay Krishnan1;2, Zongheng Yang1, Ken Goldberg1, Joseph\nM. Hellerstein1, Ion Stoica1\n1RISELab, UC Berkeley2Computer Science, University of\nChicago\nskr@cs.uchicago.edu fzongheng, goldberg, hellerstein, isto-\nicag@berkeley.edu\n. 2016. Learning to Optimize Join /Q_ueries With Deep Re-\ninforcement Learning. In Proceedings of ACM Conference,\nWashington, DC, USA, July 2017 (Conference’17), 19 pages.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are not\nmade or distributed for pro/f_it or commercial advantage and that copies bear\nthis notice and the full citation on the /f_irst page. Copyrights for components\nof this work owned by others than ACM must be honored. Abstracting with\ncredit is permi/t_ted. To copy otherwise, or republish, to post on servers or to\nredistribute to lists, requires prior speci/f_ic permission and/or a fee. Request\npermissions from permissions@acm.org.\nConference’17, Washington, DC, USA\n©2016 ACM. 978-x-xxxx-xxxx-x/YY/MM. . . $15.00\nDOI: 10.1145/nnnnnnn.nnnnnnnDOI: 10.1145/nnnnnnn.nnnnnnn\n1 INTRODUCTION\nJoin optimization has been studied for more than four\ndecades [ 44] and continues to be an active area of re-\nsearch [ 33,40,49]. /T_he problem’s combinatorial complexity\nleads to the ubiquitous use of heuristics . For example, clas-\nsical System R-style dynamic programs o/f_ten restrict their\nsearch space to certain shapes (e.g., “le/f_t-deep” plans). /Q_uery\noptimizers sometimes apply further heuristics to large join\nqueries using genetic [ 4] or randomized [ 40] algorithms. In\nedge cases, these heuristics can break down (by de/f_inition),\nwhich results in poor plans [29].\nIn light of recent advances in machine learning, a new\ntrend in database research explores replacing programmed\nheuristics with learned ones [ 11,25,26,32–34,37,41]. In-\nspired by these results, this paper explores the natural ques-\ntion of synthesizing dataset-speci/f_ic join search strategies\nusing learning. Assuming a given cost model and plan space,\ncan we optimize the search over all possible join plans for a\nparticular dataset? /T_he hope is to learn tailored search strate-\ngies from the outcomes of previous planning instances that\ndramatically reduce search time for future planning.\nOur key insight is that join ordering has a deep algorith-\nmic connection with Reinforcement Learning (RL) [ 47]. Join\nordering’s sequential structure is the same problem structure\nthat underpins RL. We exploit this algorithmic connection\nto embed RL deeply into a traditional query optimizer; any-\nwhere an enumeration algorithm is used, a policy learned\nfrom an RL algorithm can just as easily be applied. /T_his in-\nsight enables us to achieve two key bene/f_its. First, we can\nseamlessly integrate our solution into many optimizers with\nthe classical System R architecture. Second, we exploit the\nnested structure of the problem to dramatically reduce the\ntraining cost, as compared to previous proposals for a “learn-\ning optimizer”.\nTo be/t_ter understand the connection with RL, consider the\nclassical “bo/t_tom-up” dynamic programming solution to join\nordering. /T_he principle of optimality leads to an algorithm\nthat incrementally builds a plan from optimal subplans of\nsize two, size three, and so on. Enumerated subplans are\nmemoized in a lookup table, which is consulted to construct\na sequence of 1-step optimal decisions. Unfortunately, thearXiv:1808.03196v2  [cs.DB]  10 Jan 2019\n\nspace and time complexities of exact memoization can be\nprohibitive. Q-learning, an RL algorithm [ 47], relaxes the\nrequirement of exact memoization. Instead, it formulates\noptimal planning as a prediction problem: given the costs\nof previously enumerated subplans, which 1-step decision\nis most likely optimal? RL views the classic dynamic pro-\ngramming lookup table as a model—a data structure that\nsummarizes enumerated subplans and predicts the value of\nthe next decision. In concrete terms, Q-learning sets up a\nregression from the decision to join a particular pair of re-\nlations to the observed bene/f_it of making that join on past\ndata (i.e., impact on the /f_inal cost of the entire query plan).\nTo validate this insight, we built an RL-based optimizer DQ\nthat optimizes select-project-join blocks and performs join\nordering as well as physical operator selection. DQobserves\nthe planning results of previously executed queries and trains\nan RL model to improve future search. We implement three\nversions of DQto illustrate the ease of integration into exist-\ning DBMSes: (1) A standalone version built on top of Apache\nCalcite [ 2], (2) a version integrated with PostgreSQL [ 3], and\n(3) a version integrated with SparkSQL [ 7]. Deploying DQ\ninto existing production-grade systems (2) and (3) each re-\nquired changes of less than 300 lines of code and training\ndata could be collected through the normal operation of the\nDBMS with minimal overhead.\nOne might imagine that training such a model is ex-\ntremely data-intensive. While RL algorithms are indeed noto-\nriously data-ineﬃcient (typical RL se/t_tings, such as the Atari\ngames [ 38], require hundreds of thousands of training exam-\nples), we can exploit the optimal subplan structure speci/f_ic\nto join optimization to collect an abundance of high-quality\ntraining data. From a single query that passes through a na-\ntive optimizer, not only are the /f_inal plan and its total cost\ncollected as a training example, so are all of its subplans and,\nrecursively, everything inside the exact memoization table . For\ninstance, planning an 18-relation join query in TPC-DS (Q64)\nthrough a bushy optimizer can yield up to 600,000 training\ndata points thanks to DQ’s Q-learning formulation.\nWe thoroughly study this approach on two workloads:\nJoin Order Benchmark [ 29] and TPC-DS [ 5].DQsees sig-\nni/f_icant speedups in planning times (up to >200\u0002) rela-\ntive to dynamic programming enumeration while essentially\nmatching the execution times of optimal plans computed by\nthe native enumeration-based optimizers. /T_hese planning\nspeedups allow for broadening the plan space to include\nbushy plans and Cartesian products. In many cases, they\nlead to improved query execution times as well. DQis partic-\nularly useful under non-linear cost models such as memory\nlimits or materialization. On two simulated cost models with\nsigni/f_icant non-linearities, DQimproves on the plan quality\nof the next best heuristic over a set of 6 baselines by 1 :7\u0002and\n3\u0002. /T_hus, we show DQapproaches the optimization time\nFigure 1: We consider 3 cost models for the Join Order\nBenchmark: (1) one with inexpensive index lookups, (2) one\nwhere the only physical operator is a hybrid hash join with\nlimited memory, and (3) one that allows for the reuse of pre-\nviously built hash tables. /T_he /f_igure plots the cost subopti-\nmality w.r.t. optimal plans. /T_he classical le/f_t-deep dynamic\nprogram fails on the latter two scenarios. We propose a re-\ninforcement learning based optimizer, DQ, which can adapt\nto a speci/f_ic cost model given appropriate training data.\neﬃciency of programmed heuristics andthe plan quality of\noptimal enumeration.\nWe are enthusiastic about the general trend of integrating\nlearning techniques into database systems—not simply by\nblack-box application of AI models to improve heuristics,\nbut by the deep integration of algorithmic principles that\nspan the two /f_ields. Such an integration can facilitate new\nDBMS architectures that take advantage of all of the bene/f_its\nof modern AI: learn from experience, adapt to new scenarios,\nand hedge against uncertainty. Our empirical results with\nDQspan across multiple systems, multiple cost models, and\nworkloads. We show the bene/f_its (and current limitations)\nof an RL approach to join ordering and physical operator\nselection. Understanding the relationships between RL and\nclassical methods allowed us to achieve these results in a data-\neﬃcient way. We hope that DQrepresents a step towards a\nfuture learning query optimizer.\n2 BACKGROUND\n/T_he classic join ordering problem is, of course, NP-hard, and\npractical algorithms leverage heuristics to make the search\nfor a good plan eﬃcient. /T_he design and implementation of\noptimizer search heuristics are well-understood when the\ncost model is roughly linear, i.e., the cost of a join is linear\nin the size of its input relations. /T_his assumption underpins\nmany classical techniques as well as recent work [ 27,40,44,\n49]. However, many practical systems have relevant non-\nlinearities in join costs. For example, an intermediate result\nexceeding the available memory may trigger partitioning, or\na relation may cross a size threshold that leads to a change\nin physical join implementation.\n2\n\nIt is not diﬃcult to construct reasonable scenarios where\nclassical heuristics dramatically fail (Figure 1). Consider\nthe query workload and dataset in the Join Order Bench-\nmark [ 29]. A popular heuristic from the original Selinger\noptimizer is to prune the search space to only include le/f_t-\ndeep join orders. Prior work showed that le/f_t-deep plans are\nextremely eﬀective on this benchmark for cost models that\nprefer index joins [ 29]. Experimentally, we found this to be\ntrue as well: the worst-case cost over the entire workload is\nonly 2x higher than the true optimum (for an exponentially\nsmaller search space). However, when we simply change the\ncost model to be more non-linear, consisting of (1) hybrid\nhash join operators that spill partitions to disk when data\nsize exceeds available memory, or (2) hash join operators\nthat can re-use previously built hash tables, suddenly the\nle/f_t-deep heuristic is no longer a good idea—it is almost 50x\nmore costly than the true optimum.\n/T_hese results illustrate that in a practical sense, the search\nproblem is unforgiving: various heuristics have diﬀerent\nweak spots where they fail by orders of magnitude relative\nto optimal. For example, success on such atypical or non-\nlinear cost models may require searching over “bushy” plans,\nnot just le/f_t-deep ones. With new hardware innovations [ 8]\nand a move towards serverless RDBMS architectures [ 1],\nit is not unreasonable to expect a multitude of new query\ncost models that signi/f_icantly diﬀer from existing literature,\nwhich might require a complete redesign of standard pruning\nheuristics. Ideally, instead of a /f_ixed heuristic, we would want\na strategy to guide the search space in a more data-driven\nway—tailoring the search to a speci/f_ic database instance,\nquery workload, and observed join costs. /T_his sets up the\nmain premise of the paper: would it be possible to use data-\ndriven machine learning methods to identify such a heuristic\nfrom data?\n2.1 Example\nWe focus on the classical problem of searching for a query\nplan made up of binary join operators and unary selections,\nprojections, and access methods. We will use the following\ndatabase of three relations denoting employee salaries as a\nrunning example throughout the paper:\nEmp¹id;name ;rankºPos¹rank ;title ;codeºSal¹code ;amountº\nConsider the following join query:\nSELECT*\nFROM Emp, Pos, Sal\nWHERE Emp.rank = Pos.rank\nAND Pos.code = Sal.code\n/T_here are many possible orderings to execute this query. For\nexample, one could execute the example query as Emp . /\n¹Sal. /Posº, or as Sal. /¹Emp . /Posº.2.2 Reinforcement Learning\nBellman’s “Principle of Optimality” and the characterization\nof dynamic programming is one of the most important re-\nsults in computing [ 12]. In addition to forming the basis of\nrelational query optimization, it has a deep connection to\na class of stochastic processes called Markov Decision Pro-\ncesses (MDPs), which formalize a wide range of problems\nfrom path planning to scheduling. In an MDP model, an agent\nmakes a sequence of decisions with the goal of optimizing a\ngiven objective (e.g., improve performance, accuracy). Each\ndecision is dependent on the current state, and typically leads\nto a new state. /T_he process is “Markovian” in the sense that\nthe system’s current state completely determines its future\nprogression. Formally, an MDP consists of a /f_ive-tuple:\nhS;A;P¹s;aº;R¹s;aº;s0i\nwhere Sdescribes a set of states that the system can be in, A\ndescribes the set of actions the agent can take, s0\u0018P¹s;aº\ndescribes a probability distribution over new states given\na current state and action, and s0de/f_ines a distribution of\ninitial states. R¹s;aºis the reward of taking action a in state\ns. /T_he reward measures the performance of the agent. /T_he\nobjective of an MDP is to /f_ind a decision policy π:S7!A,\na function that maps states to actions, with the maximum\nexpected reward:\narg max\nπE\"T\u00001Õ\nt=0R¹st;atº#\nsubject to st+1=P¹st;atº;at=π¹stº:\nAs with dynamic programming in combinatorial problems,\nmost MDPs are diﬃcult to solve exactly. Note that the greedy\nsolution, eagerly maximizing the reward at each step, might\nbe suboptimal in the long run. Generally, analytical solutions\nto such problems scale poorly in the time horizon.\nReinforcement learning (RL) is a class of stochastic opti-\nmization techniques for MDPs [ 47]. An RL algorithm uses\nsampling, taking randomized sequences of decisions, to build\na model that correlates decisions with improvements in the\noptimization objective (cumulative reward). /T_he extent to\nwhich the model is allowed to extrapolate depends on how\nthe model is parameterized. One can parameterize the model\nwith a table (i.e., exact parameterization) or one can use\nany function approximator (e.g., linear functions, nearest\nneighbors, or neural networks). Using a neural network in\nconjunction with RL, or Deep RL, is the key technique behind\nrecent results like learning how to autonomously play Atari\ngames [39] and the game of Go [45].\n3\n\n2.3 Markov Model of Enumeration\nNow, we will review standard “bo/t_tom-up” join enumeration,\nand then, we will make the connection to a Markov Deci-\nsion Process. Every join query can be described as a query\ngraph, where edges denote join conditions between tables\nand vertices denote tables. Any dynamic programming join\noptimizer implementation needs to keep track of its progress:\nwhat has already been done in a particular subplan (which\nrelations were already joined up) and what options remain\n(which relations–whether base or the result of joins–can still\nbe “joined in” with the subplan under consideration). /T_he\nquery graph formalism allows us to represent this state.\nDe/f_inition 2.1 (/Q_uery Graph). A query graph Gis an undi-\nrected graph, where each relation Ris a vertex and each join\npredicateρde/f_ines an edge between vertices. Let κGdenote\nthe number of connected components of G.\nMaking a decision to join two subplans corresponds to\npicking two vertices that are connected by an edge and merg-\ning them into a single vertex. Let G=¹V;Eºbe a query graph.\nApplying a join c=¹/v.alti;/v.altjºto the graph Gde/f_ines a new\ngraph with the following properties: (1) /v.altiand/v.altjare re-\nmoved from V, (2) a new vertex¹/v.alti+/v.altjºis added to V, and\n(3) the edges of¹/v.alti+/v.altjºare the union of the edges incident\nto/v.altiand/v.altj. Each join reduces the number of vertices by\n1. Each plan can be described as a sequence of such joins\nc1\u000ec2:::\u000ecTuntiljVj=κG. /T_he above description embraces\nanother System R heuristic: “avoiding Cartesian products”.\nWe can relax that heuristic by simply adding edges to Gat\nthe start of the algorithm, to ensure it is fully connected.\nGoing back to our running example, suppose we start with\na query graph consisting of the vertices ¹Emp ;Pos;Salº. Let\nthe /f_irst join be c1=¹Emp ;Posº; this leads to a query graph\nwhere the new vertices are ¹Emp +Pos;Salº. Applying the\nonly remaining possible join, we arrive at a single remaining\nvertex Sal+¹Emp +Posºcorresponding to the join plan\nSal. /¹Emp . /Posº.\n/T_he join optimization problem is to /f_ind the best possi-\nble join sequence—i.e., the best query plan. Also note that\nthis model can be simply extended to capture physical op-\nerator selection as well. /T_he set of allowed joins can be\ntyped with an eligible join type, e.g., c=¹/v.alti;/v.altj;HashJoinº\norc=¹/v.alti;/v.altj;IndexJoinº. We assume access to a cost model\n/J.alt¹cº7!R+, i.e., a function that estimates the incremental\ncost of a particular join.\nP/r.sc/o.sc/b.sc/l.sc/e.sc/m.sc 1 (J/o.sc/i.sc/n.sc O/p.sc/t.sc/i.sc/m.sc/i.sc/z.sc/a.sc/t.sc/i.sc/o.sc/n.sc P/r.sc/o.sc/b.sc/l.sc/e.sc/m.sc). LetGde/f_ine\na query graph and /J.altde/f_ine a cost model. Find a sequence\nc1\u000ec2:::\u000ecTterminating injVj=κGto minimize:\nmin\nc1; : : :;cTTÕ\ni=1/J.alt¹ciº\nsubject to Gi+1=c¹Giº:Symbol De/f_inition\nG A query graph. /T_his is a state in the MDP.\nc A join. /T_his is an action .\nG0/T_he resultant query graph a/f_ter applying a join.\n/J.alt¹cº A cost model that scores joins.\nTable 1: Notation used throughout the paper.\nNote how this problem statement exactly de/f_ines an MDP\n(albeit by convention a minimization problem rather than\nmaximization). Gis a representation of the state ,cis a repre-\nsentation of the action , the vertex merging process de/f_ines\nthe state transition P¹G;cº, and the reward function is the\nnegative cost\u0000/J.alt. /T_he output of an MDP is a function that\nmaps a given query graph to the best next join. Before pro-\nceeding, we summarize our notation in Table 1.\n2.4 Long Term Reward of a Join\nTo introduce how RL gives us a new perspective on this clas-\nsical database optimization problem, let us /f_irst examine the\ngreedy solution. A naive solution is to optimize each ciinde-\npendently (also called Greedy Operator Optimization [ 40]).\n/T_he algorithm proceeds as follows: (1) start with the query\ngraph, (2) /f_ind the lowest cost join, (3) update the query\ngraph and repeat until only one vertex is le/f_t.\n/T_he greedy algorithm, of course, does not consider how\nlocal decisions might aﬀect future costs. For illustration, con-\nsider our running example query with the following simple\ncosts (assume a single join method with symmetric cost):\n/J.alt¹EPº=100;/J.alt¹SPº=90;/J.alt¹¹EPºSº=10;/J.alt¹¹SPºEº=50\n/T_he greedy solution would result in a cost of 140 (because it\nneglects the future eﬀects of a decision), while the optimal\nsolution has a cost of 110. However, there is an upside: this\ngreedy algorithm has a computational complexity of O¹jVj3º,\ndespite the super-exponential search space.\n/T_he greedy solution is suboptimal because the decision\nat each index fails to consider the long-term value of its\naction. One might have to sacri/f_ice a short term bene/f_it for a\nlong term payoﬀ. Consider the optimization problem for a\nparticular query graph G:\nV¹Gº=min\nc1; : : :;cTTÕ\ni=1/J.alt¹ciº (1)\nIn classical treatments of dynamic programming, this func-\ntion is termed the value function . It is noted that optimal\nbehavior over an entire decision horizon implies optimal\nbehavior from any starting index t>1 as well, which is the\nbasis for the idea of dynamic programming. Conditioned on\n4\n\nthe current join, we can write in the following form:\nV¹Gº=min\ncQ¹G;cº\nQ¹G;cº=/J.alt¹cº+V¹G0º\nleading to the following recursive de/f_inition of the Q-function\n(or cost-to-go function):\nQ¹G;cº=/J.alt¹cº+min\nc0Q¹G0;c0º (2)\nIntuitively, the Q-function describes the long-term value of\neach join: the cumulative cost if we act optimally for all\nsubsequent joins a/f_ter the current join decision. Knowing Q\nis equivalent to solving the problem since local optimization\nmin c0Q¹G0;c0ºis suﬃcient to derive an optimal sequence of\njoin decisions.\nIf we revisit the greedy algorithm, and revise it hypotheti-\ncally as follows: (1) start with the query graph, (2) /f_ind the\nlowest Q-value join, (3) update the query graph and repeat,\nthen this algorithm has the same computational complexity\nofO¹jVj3ºbut is provably optimal. To sketch out our solution,\nwe will use Deep RL to approximate a global Q-function (one\nthat holds for all query graphs in a workload), which gives\nus a polynomial-time algorithm for join optimization.\n2.5 Applying Reinforcement Learning\nAn important class of reinforcement learning algorithms,\ncalled Q-learning algorithms, allows us to approximate the Q-\nfunction from samples of data [ 47]. What if we could regress\nfrom features of¹G;cºto the future cumulative cost based on\na small number of observations? Practically, we can observe\nsamples of decision sequences containing ¹G;c;/J.alt¹cº;G0ºtu-\nples, where Gis the query graph, cis a particular join, /J.alt¹cº\nis the cost of the join, and G0is the resultant graph. Such a\nsequence can be extracted from any /f_inal join plan and by\nevaluating the cost model on the subplans.\nLet’s further assume we have a parameterized model for\nthe Q-function, Qθ:\nQθ¹fG;fcº\u0019Q¹G;cº\nwhere fGis afeature vector representing the query graph\nandfcis a feature vector representing a particular join. θ\nis the model parameters that represent this function and\nis randomly initialized at the start. For each training tuple\ni, one can calculate the following label, or the “estimated”\nQ-value:\n/y.alti=/J.alt¹cº+min\nc0Qθ¹G0;c0º\n/T_hef/y.altigcan then be used as labels in a regression problem.\nIfQwere the true Q-function, then the following recurrence\nwould hold:\nQ¹G;cº=/J.alt¹cº+min\nc0Qθ¹G0;c0ºSo, the learning process, or Q-learning , de/f_ines a loss at each\niteration:\nL¹Qº=Õ\nik/y.alti\u0000Qθ¹G;cºk2\n2\n/T_hen parameters of the Q-function can be optimized with\ngradient descent until convergence.\nRL yields two key bene/f_its: (1) the search cost for a sin-\ngle query relative to traditional query optimization is radi-\ncally reduced, since the algorithm has the time-complexity\nof greedy search, and (2) the parameterized model can po-\ntentially learn across queries that have “similar” but non-\nidentical subplans. /T_his is because the similarity between\nsubplans are determined by the query graph and join featur-\nizations, fGandfc; thus if they are designed in a suﬃciently\nexpressive way, then the neural network can be trained to\nextrapolate the Q-function estimates to an entire workload.\n/T_he speci/f_ic choice of Q-learning is important here (com-\npared to other RL algorithms). First, it allows us to take advan-\ntage of optimal substructures during training and greatly re-\nduce data needed. Second, compared to policy learning [ 33],\nQ-learning outputs a score for each join that appears in any\nsubplan rather than simply selecting the best join. /T_his is\nmore amenable to deep integration with existing query opti-\nmizers, which have additional state like interesting orders\nand their own pruning of plans. /T_hird, the scoring model al-\nlows for top-k planning rather than just ge/t_ting the best plan.\nWe note that the design of Q-learning variants is an active\narea of research in AI [ 21,50], so we opted for the simplicity\nof a Deep Q-learning approach and defer incorporation of\nadvanced variants to future work.\n2.6 Reinforcement Learning vs. Supervised\nLearning\nReinforcement Learning and Supervised Learning can seem\nvery similar since the underlying inference methods in RL al-\ngorithms are o/f_ten similar to those used in supervised learn-\ning and statistical estimation. Here is how we justify our\nterminology. In supervised learning, one has paired train-\ning examples with ground-truth labels (e.g., an image with\na labeled object). For join optimization, this would mean a\ndataset where the example is the current join graph and the\nlabel is the next best join decision from an oracle. In the\ncontext of sequential planning, this problem se/t_ting is o/f_ten\ncalled Imitation Learning [ 42]; where one imitates an oracle\nas best as possible.\nAs in [ 30], the term “Reinforcement Learning” refers to\na class of empirical solutions to Markov Decision Process\nproblems where we do nothave the ground-truth, optimal\nnext steps; instead, learning is guided by numeric “rewards”\nfor next steps. In the context of join optimization, these\nrewards are subplan costs. RL rewards may be provided by a\nreal-world experiment, a simulation model, or some other\n5\n\noracular process. In our work below, we explore diﬀerent\nreward functions including both real-world feedback ( §5)\nand simulation via traditional plan cost estimation ( §3.3).\nRL purists may argue that access to any optimization or-\nacle moves our formulation closer to supervised learning\nthan classical RL. We maintain this terminology because\nwe see the pre-training procedure as a useful prior. Rather\nthan expensive, ab initio learning from executions, we learn\na useful (albeit imperfect) join optimization policy oﬄine.\n/T_his process bootstraps a more classical “learning-by-doing”\nRL process online that avoids executing grossly suboptimal\nquery plans.\n/T_here is additionally subtlety in the choice of algorithm.\nMost modern RL algorithms collect data episodically (execute\nan entire query plan and observe the /f_inal result). /T_his makes\nsense in /f_ields like robotics or autonomous driving where\nactions may not be reversible or decomposable. In query\noptimization, every query consists of subplans (each of which\nis its own “query”). Episodic data collection ignores this\ncompositional structure.\n3 OPTIMIZER ARCHITECTURE\nSelinger’s optimizer design separated the problem of plan\nsearch from cost/selectivity estimation [ 44]. /T_his insight al-\nlowed independent innovation on each topic over the years.\nIn our initial work, we follow this lead, and intentionally\nfocus on learning a search strategy only. Even within the\nsearch problem, we focus narrowly on the classical select-\nproject-join kernel. /T_his too is traditional in the literature,\ngoing back to Selinger [ 44] and continuing as recently as\nNeumann et al.’s very recent experimental work [ 40]. It is\nalso particularly natural for illustrating the connection be-\ntween dynamic programming and Deep RL and implications\nfor query optimization. We intend for our approach to plug\ndirectly into a Selinger-based optimizer architecture like that\nof PostgreSQL, DB2 and many other systems.\nIn terms of system architecture, DQcan be simply inte-\ngrated as a learning-based replacement for prior algorithms\nfor searching a plan space. Like any non-exhaustive query\noptimization technique, our results are heuristic. /T_he new\nconcerns raised by our approach have to do with limitations\nof training, including over/f_i/t_ting and avoiding high-variance\nplans. We use this section to describe the extensibility of\nour approach and what design choices the user has at her\ndisposal.\n3.1 Overview\nNow, we describe what kind of training data is necessary\nto learn a Q-function. In supervised regression, we collect\ndata of the form (feature, values) . /T_he learned func-\ntion maps from feature to values. One can think of this\nas a stateless prediction, where the underlying predictionproblem does not depend on some underlying process state.\nOn the other hand, in the Q-learning se/t_ting, there is state.\nSo we have to collect training data of the form (state,\ndecision, new state, cost) . /T_herefore, a training\ndataset has the following format (in Java notation):\nList<Graph, Join, Graph', Cost> dataset\nIn many cases like robotics or game-playing, RL is used in\na live se/t_ting where the model is trained on-the-/f_ly based on\nconcrete moves chosen by the policy and measured in prac-\ntice. Q-learning is known as an “oﬀ-policy” RL method. /T_his\nmeans that its training is independent of the data collection\nprocess and can be suboptimal—as long as the training data\nsuﬃciently covers the decisions to be made.\n3.2 Architecture and API\nDQcollects training data sampled from a cost model and a\nnative optimizer. It builds a model which improves future\nplanning instances. DQmakes relatively minimal assump-\ntions about the structure of the optimizer. Below are the API\nhooks that it requires implemented.\nWorkload Generation. A function that returns a list of training\nqueries of interest. DQ requires a relevant workload for\ntraining. In our experiments, we show that this workload\ncan be taken from query templates or sampled from the\ndatabase schema.\nsample(): List<Queries>\nCost Sampling. A function that given a query returns a list of\njoin actions and their resultant costs. DQrequires the sys-\ntem to have its own optimizer to generate training data. /T_his\nmeans generating feasible join plans and their associated\ncosts. Our experiments evaluate integration with determin-\nistic enumeration, randomized, and heuristic algorithms.\ntrain(query): List<Graph,Join,Graph',Cost>\nPredicate Selectivity Estimation. A function that returns the\nselectivity of a particular single table predicate. DQleverages\nthe optimizer’s own selectivity estimate for featurization\n(§4.1).\nselectivity(predicate): Double\nIn our evaluation ( §6), we will vary these exposed hooks\nto experiment with diﬀerent implementations for each (e.g.,\ncomparing training on highly relevant data from a desired\nworkload vs. randomly sampling join queries directly from\nthe schema).\n6\n\n¹f\nT1 T2IndexJoin T3HashJoin T4HashJoin\n,\nT1 T2IndexJoin T3HashJoin\n,T1 T2IndexJoing,fT1;\u0001\u0001\u0001;T4g;V\u0003º\nT1 T2IndexJoin T3HashJoin T4HashJoin\nPlan from\nNative OptimizerOptimal Sub-plansRelations\nto JoinOptimal\nCostNative\nOptimizer\nFigure 2: Training data collection is eﬃcient ( §3.3). Here, by leveraging the principle of optimality, three training examples\nare emitted from a single plan produced by a native optimizer. /T_hese examples share the same long-term cost and relations to\njoin (i.e., making these local decisions eventually leads to joining fT1;\u0001\u0001\u0001;T4gwith optimal cumulative cost V\u0003).\n3.3 Eﬃcient Training Data Generation\nTraining data generation may seem onerous, but in fact,\nuseful data is automatically generated as a consequence of\nrunning classical planning algorithms. For each join deci-\nsion that the optimizer makes, we can get the incremental\ncost of the join. Suppose, we run a classical bushy dynamic\nprogramming algorithm to optimize a k-way join, we not\nonly get a /f_inal plan but also an optimal plan for every single\nsubplan enumerated along the way. Each query generates\nan optimal query plan for all of the subplans that compose\nit, as well as observations of suboptimal plans that did not\nmake the cut. /T_his means that a single query generates a\nlarge amount of training examples. Figure 2 shows how the\nprinciple of optimality helps enhance a training dataset.\n/T_his data collection scheme diﬀers from that of several\npopular RL algorithms such as PPO and Policy Gradients [ 43]\n(and used in [ 33]). /T_hese algorithms train their models\n“episodically”, where they apply an entire sequence of deci-\nsions and observe the /f_inal cumulative reward. An analogy\nwould be a graph search algorithm that does not backtrack\nbut resets to the starting node and tries the whole search\nagain. While general, this scheme not suited for the structure\nof join optimization, where an optimal plan is composed of\noptimal substructures. Q-learning, an algorithm that does\nnot rely on episodic data and can learn from oﬄine data\nconsisting of a hierarchy of optimal subplans, is a be/t_ter /f_it\nfor join optimization.\nIn our experiments, we bootstrap planning with a bushy\ndynamic program until the number of relations in the join\nexceeds 10 relations. /T_hen, the data generation algorithm\nswitches to a greedy scheme for eﬃciency for the last K\u000010\njoins. Ironically, the data collected from such an optimizer\nmight be “too good” (or too conservative) because it does\nnot measure or learn from a diverse enough space of (costly,\nhence risky) subplans. If the training data only consisted\nof optimal sub-plans, then the learned Q-function may not\naccurately learn the downside of poor subplans. Likewise,\nif purely random plans are sampled, the model might not\nsee very many instances of good plans. To encourage more\n“exploration”, during data collection noise can be injected into\nthe optimizer to force it to enumerate more diverse subplans.We control this via a parameter /uni03F5, the probability of picking\na random join as opposed to a join with the lowest cost. As\nthe algorithm enumerates subplans, if rand() </uni03F5then a\nrandom (valid) join is chosen on the current query graph;\notherwise it proceeds with the lowest-cost join as usual. /T_his\nis an established technique to address such “covariate shi/f_t”,\na phenomenon extensively studied in prior work [28].\n4 REALIZING THE Q-LEARNING MODEL\nNext, we present the mechanics of actually training and\noperating a Q-learning model.\n4.1 Featurizing the Join Decision\nBefore we get into the details, we will give a brief motivation\nof how we should think about featurization in a problem like\nthis. /T_he features should be suﬃciently rich that they capture\nall relevant information to predict the future cumulative cost\nof a join decision. /T_his requires knowing what the overall\nquery is requesting, the tables on the le/f_t side of the proposed\njoin, and the tables on the right side of the proposed join.\nIt also requires knowing how single table predicates aﬀect\ncardinalities on either side of the join.\nParticipating Relations: /T_he overall intuition is to\nuse each column name as a feature, because it identi-\n/f_ies the distribution of that column. /T_he /f_irst step is to\nconstruct a set of features to represent which a/t_tributes\nare participating in the query and in the particular join.\nLetAbe the set of all a/t_tributes in the database (e.g.,\nfEmp :id;Pos:rank ; :::;Sal:code ;Sal:amountg). Each relation\nrel(including intermediate join results) has a set of visible\na/t_tributes ,Arel\u0012A, the a/t_tributes present in the output.\nSimilarly, every query graph Gcan be represented by its\nvisible a/t_tributes AG\u0012A. Each join is a tuple of two rela-\ntions¹L;Rºand we can get their visible a/t_tributes ALandAR.\nEach of the a/t_tribute sets AG;AL;ARcan then be represented\nwith a binary 1-hot encoding : a value 1 in a slot indicates\nthat particular a/t_tribute is present, otherwise 0 represents\nits absence. Using\bto denote concatenation, we obtain the\nquery graph features, fG=AG, and the join decision fea-\ntures, fc=AL\bAR, and, /f_inally, the overall featurization for\n7\n\nSELECT*\nFROM Emp, Pos, Sal\nWHERE Emp.rank\n= Pos.rank\nAND Pos.code\n= Sal.code\n(a) Example queryAG=»E.id, E.name, E.rank,\nP.rank, P.title, P.code,\nS.code, S.amount¼\n=»1 1 1 1 1 1 1 1¼\n(b) /Q_uery graph\nfeaturizationAL=»E.id, E.name, E.rank ¼\n=»1 1 1 0 0 0 0 0¼\nAR=»P.rank, P.title, P.code ¼\n=»0 0 0 1 1 1 0 0¼\n(c) Features of E. /PAL=»E.id, E.name, E.rank,\nP.rank, P.title, P.code ¼\n=»1 1 1 1 1 1 0 0¼\nAR=»S.code, S.amount¼\n=»0 0 0 0 0 0 1 1¼\n(d) Features of¹E. /Pº. /S\nFigure 3: A query and its corresponding featurizations ( §4.1). One-hot vectors encode the visible attributes in the query\ngraph ( AG), the le/f_t side of a join ( AL), and the right side ( AR). Such encoding allows for featurizing both the query graph and\na particular join. A partial join and a full join are shown. /T_he example query covers all relations in the schema, so AG=A.\n/Q_uery:\n<example query>\nAND Emp.id > 200\nSelectivity¹Emp.id >200º=0:2\nfG=AG=»E.id, E.name ;\u0001\u0001\u0001¼\n=»1 1 1 1 1 1 1 1¼\n!»:2 1 1 1 1 1 1 1¼\n(a) Selectivity scaling in\nquery graph features/Q_uery:\n<example query>\nfeat vec¹IndexJoin¹E. /Pºº\n=AL\bAR\b»1 0¼\nfeat vec¹HashJoin¹E. /Pºº\n=AL\bAR\b»0 1¼\n(b) Concatenation of\nphysical operators in join\nfeatures\nFigure 4: Accounting for selections and physical operators.\nSimple changes to the basic form of featurization are needed\nto support selections (le/f_t) and physical operators (right).\nFor example, assuming a system that chooses between only\nIndexJoin andHashJoin , a 2-dimensional one-hot vector is\nconcatenated to each join feature vector. Discussion in §4.1.\na particular¹G;cºtuple is simply fG\bfc. Figure 3 illustrates\nthe featurization of our example query.\nSelections: Selections can change said distribution, i.e., (col,\nsel-pred) is diﬀerent than (col, TRUE) . To handle single table\npredicates in the query, we have to tweak the feature repre-\nsentation. As with most classical optimizers, we assume that\nthe optimizer eagerly applies selections and projections to\neach relation. Next, we leverage the table statistics present\nin most RDBMS. For each selection σin a query we can ob-\ntain the selectivity δσ, which estimates the fraction of tuples\npresent a/f_ter applying the selection.1To account for selec-\ntions in featurization, we simply scale the slot in fGthat the\nrelation and a/t_tribute σcorresponds to, by δr. For instance,\nif selection Emp.id >200 is estimated to have a selectivity\nof 0:2, then the Emp.id slot in fGwould be changed to 0 :2.\nFigure 4a pictorially illustrates this scaling.\nPhysical Operators: /T_he next piece is to featurize the\nchoice of physical operator. /T_his is straightforward: we add\n1We consider selectivity estimation out of scope for this paper. See discus-\nsion in §3 and §7.another one-hot vector that indicates from a /f_ixed set of\nimplementations the type of join used (Figure 4b).\nExtensibility: In this paper, we focus only on the basic\nform of featurization described above and study foreign key\nequality joins.2An ablation study as part of our evaluation\n(Table 9) shows that the pieces we se/t_tled on all contribute\nto good performance. /T_hat said, there is no architectural\nlimitation in DQthat prevents it from utilizing other features.\nAny property believed to be relevant to join cost prediction\ncan be added to our featurization scheme. For example, we\ncan add an additional binary vector findto indicate which\na/t_tributes have indexes built. Likewise, physical properties\nlike sort-orders can be handled by indicating which a/t_tributes\nare sorted in an operator’s output. Hardware environment\nvariables (e.g., available memory) can be added as scalars if\ndeemed as important factors in determining the /f_inal best\nplan. Lastly, more complex join conditions such as inequality\nconditions can also be handled ( §8).\n4.2 Model Training\nDQuses a multi-layer perceptron (MLP) neural network to\nrepresent the Q-function. It takes as input the /f_inal featur-\nization for a¹G;cºpair, fG\bfc. Empirically, we found that a\ntwo-layer MLP oﬀered the best performance under a modest\ntraining time constraint ( <10 minutes). /T_he model is trained\nwith a standard stochastic gradient descent (SGD) algorithm.\n4.3 Execution a/f_ter Training\nA/f_ter training, we obtain a parameterized estimate of the\nQ-function, Qθ¹fG;fcº. For execution, we simply go back to\nthe standard algorithm as in the greedy method but instead\nof using the local costs, we use the learned Q-function: (1)\nstart with the query graph, (2) featurize each join, (3) /f_ind\nthe join with the lowest estimated Q-value (i.e., output from\nthe neural net), (4) update the query graph and repeat.\n2/T_his is due to our evaluation workloads containing only such joins. §8\ndiscusses how DQcould be applied to more general join types.\n8\n\n/T_his algorithm has the time-complexity of greedy enumer-\nation except in greedy, the cost model is evaluated at each\niteration, and in our method, a neural network is evaluated.\nOne pleasant consequence is that DQexploits the abundant\nvectorization opportunities in numerical computation. In\neach iteration, instead of invoking the neural net sequen-\ntially on each join’s feature vector, DQbatches all candidate\njoins (of this iteration) together, and invokes the neural net\nonce on the batch. Modern CPUs, GPUs, and specialized ac-\ncelerators (e.g., TPUs [ 24]) all oﬀer optimized instructions\nfor such single-instruction multiple-data (SIMD) workloads.\n/T_he batching optimization amortizes each invocation’s /f_ixed\noverheads and has the most impact on large joins.\n5 FEEDBACK FROM EXECUTION\nWe have described how DQlearns from sampling the cost\nmodel native to a query optimizer. However, it is well-known\nthat a cost model (costs) may fail to correlate with reality\n(runtimes), due to poor cardinality estimates or unrealistic\nrules used in estimation. To correct these errors, the database\ncommunity has seen proposals of leveraging feedback from\nexecution [ 14,35]. We can perform an analogous operation\non learned Q-functions. Readers might be familiar with the\nconcept of /f_ine-tuning in the deep learning literature [ 54],\nwhere a network is trained on one dataset and “transferred”\nto another with minimal re-training. DQcan optionally ap-\nply this technique to re-train itself on real execution runtimes\nto correlate be/t_ter with the operating environment.\n5.1 Fine-tuning DQ\nFine-tuning DQconsists of two steps: pre-training as usual\nand re-training. First, DQis pre-trained to convergence on\nsamples from the optimizer’s cost model; these are inexpen-\nsive to collect compared to real execution. Next, the weights\nof the /f_irst two layers of the neural network are frozen, and\nthe output layer’s weights are re-initialized randomly. Re-\ntraining is then started on samples of real execution runtimes,\nwhich would only change the output layer’s weights.\nIntuitively, the process can be thought of as /f_irst using\nthe cost model to learn relevant features about the general\nstructure of subplans (e.g., “which relations are generally\nbene/f_icial to join?”). /T_he re-trained output layer then projects\nthe eﬀect of these features onto real runtimes. Due to its\ninexpensive nature, partial re-training is a common strategy\napplied in many machine learning applications.\n5.2 Collecting Execution Data\nFor /f_ine-tuning, we collect a list of real-execution data,\n(Graph, Join, Graph’, OpTime) , where instead\nof the cost of the join, the real runtime a/t_tributed to the\nparticular join operator is recorded. Per-operator runtimescan be collected by instrumenting the underlying system,\nor using the system’s native analysis functionality (e.g., EX-\nPLAIN ANALYZE in Postgres).\n6 EVALUATION\nWe extensively evaluate DQ to investigate the following\nmajor questions:\n\u000fHow eﬀective is DQin producing plans, how good\nare they, and under what conditions ( §6.1.1, §6.1.2,\n§6.1.3)?\n\u000fHow eﬃcient is DQat producing plans, in terms of\nruntimes and required data ( §6.1.4, §6.1.5, §6.1.6)?\n\u000fDoDQ’s techniques apply to real-world scenarios,\nsystems, and workloads ( §6.2,§6.3)?\nTo address the /f_irst two questions, we run experiments on\nstandalone DQ. /T_he last question is evaluated with end-to-\nend experiments on DQ-integrated Postgres and SparkSQL.\n6.1 Standalone Optimization Experiments\nWe implemented DQand a wide variety of optimizer search\ntechniques previously benchmarked in Leis et al. [ 29] in a\nstandalone Java query optimizer harness. Apache Calcite\nis used for parsing SQL and representing the SQL AST. We\n/f_irst evaluate standalone DQand other optimizers for /f_inal\nplan costs; unless otherwise noted, exploration ( §3.3) and\nreal-execution feedback ( §5) are turned oﬀ. We use the Join\nOrder Benchmark (JOB) [ 29], which is derived from the real\nIMDB dataset (3.6GB in size; 21 tables). /T_he largest table\nhas 36 million rows. /T_he benchmark contains 33 templates\nand 113 queries in total. /T_he joins have between 4 and 15\nrelations, with an average of 8 relations per query.\nWe revisit a motivating claim from earlier: heuristics are\nwell-understood when the cost model is linear but non-\nlinearities can lead to signi/f_icant suboptimality. /T_he experi-\nments intend to illustrate that DQoﬀers a form of robustness\nto cost model , meaning, that it prioritizes plans tailored to the\nstructure of the cost model, workload, and physical design—\neven when these plans are bushy.\nWe consider 3 cost models: CM1 is a model for a main-\nmemory database; CM2 additionally considers limited mem-\nory hash joins where a/f_ter a threshold the costs of spilling\npartitions to disk are considered; CM3 additionally considers\nthe re-use of already-built hash tables during upstream oper-\nators. We compare with the following baselines: /Q_uickPick-\n1000 ( QP) [51] selects the best of 1000 random join plans;\nIK-KBZ ( KBZ ) [27] is a polynomial-time heuristic that de-\ncomposes the query graph into chains and orders them; dy-\nnamic programs Right-deep ( RD), Le/f_t-deep ( LD), Zig-zag\n(ZZ) [55], and Exhaustive ( EX) exhaustively enumerate join\nplans with the indicated plan shapes. Details of the setup are\nlisted in Appendix §A.\n9\n\nOptimizer Cost Model 1 Cost Model 2 Cost Model 3\nMin Mean Max Min Mean Max Min Mean Max\n/Q_uickPick ( QP) 1.0 23.87 405.04 7.43 51.84 416.18 1.43 16.74 211.13\nIK-KBZ ( KBZ ) 1.0 3.45 36.78 5.21 29.61 106.34 2.21 14.61 96.14\nRight-deep ( RD) 4.70 53.25 683.35 1.93 8.21 89.15 1.83 5.25 69.15\nLe/f_t-deep ( LD) 1.0 1.08 2.14 1.75 7.31 65.45 1.35 4.21 35.91\nZig-zag ( ZZ) 1.0 1.07 1.87 1.0 5.07 43.16 1.0 3.41 23.13\nExhaustive ( EX) 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\nDQ 1.0 1.32 3.11 1.0 1.68 11.64 1.0 1.91 13.14\nTable 2: DQis robust and competitive under all three cost models ( §6.1). Plan costs are relative to optimal plans produced by\nexhaustive enumeration, i.e., costal/afii10069.italocostEX. Statistics are calculated across the entire Join Order Benchmark.\nResults of this set of experiments are shown in Table 2.\n6.1.1 Cost Model 1. Our results on CM1 reproduce the\nconclusions of Leis et al. [ 29], where le/f_t-deep plans are gen-\nerally good (utilize indexes well) and there is li/t_tle need for\nzigzag or exhaustive enumeration. DQis competitive with\nthese optimal solutions without a priori knowledge of the in-\ndex structure. In fact, DQsigni/f_icantly outperforms the other\nheuristic solutions KBZ andQP. While it is true that KBZ\nalso restricts its search to le/f_t-deep plans, it is suboptimal for\ncyclic join graphs—its performance is hindered since almost\nall JOB queries contain cycles. We found that QPstruggles\nwith the physical operator selection, and a signi/f_icant num-\nber of random samples are required to /f_ind a narrow set of\ngood plans (ones the use indexes eﬀectively).\nUnsurprisingly, these results show that DQ, a learning-\nbased solution, reasonably matches performance on cases\nwhere good heuristics exist. On average DQis within 22%\nof the LDsolution and in the worst case only 1.45 \u0002worse.\n6.1.2 Cost Model 2. By simply changing to a diﬀerent,\nyet realistic, cost model, we can force the le/f_t-deep heuristics\nto perform poorly. CM2 accounts for disk usage in hybrid\nhash joins. In this cost model, none of the heuristics match\nthe exhaustive search over the entire workload. Since the\ncosts are largely symmetric for small relation sizes, there is\nli/t_tle bene/f_it to either le/f_t-deep or right-deep pruning. Simi-\nlarly zig-zag trees are only slightly be/t_ter, and the heuristic\nmethods fail by orders-of-magnitude on their worst queries.\nDQstill comes close to the quality of exhaustive enumer-\nation (1 :68\u0002on average). It does not perform as well as in\nCM1 (with its worst query about 12 \u0002the optimal cost) but is\nstill signi/f_icantly be/t_ter than the alternatives. Results on CM2\nsuggest that as memory becomes more limited, heuristics be-\ngin to diverge more from the optimal solution. We explored\nthis phenomenon further and report results in Table 3.\n6.1.3 Cost Model 3. Finally, we illustrate results on CM3\nthat allows for the reuse of hash tables. Right-deep plans are\nno longer ineﬃcient in this model as they facilitate reuse of\nthe hash table (note right and le/f_t are simply conventionsM=108M=106M=104M=102\nKBZ 1.0 3.31 30.64 41.64\nLD 1.0 1.09 6.45 6.72\nEX 1.0 1.0 1.0 1.0\nDQ 1.04 1.42 1.64 1.56\nTable 3: Cost Model 2: mean relative cost vs. memory limit\n(number of tuples in memory).\nand there is nothing important about the labels). /T_he chal-\nlenge is that now plans have to contain a mix of le/f_t-deep\nand right-deep structures. Zig-zag tree pruning heuristic\nwas exactly designed for cases like this. Surprisingly, DQis\nsigni/f_icantly (1 :7\u0002on average and in the worst) be/t_ter than\nzig-zag enumeration. We observed that bushy plans were\nnecessary in a small number of queries and DQfound such\nlower-cost solutions.\nIn summary, results in Table 2 show that DQis robust\nagainst diﬀerent cost model regimes, since it learns to adapt\nto the workload at hand.\n6.1.4 Planning Latency. Next, we report the planning (op-\ntimization) time of DQand several other optimizers across\nthe entire 113 JOB queries. /T_he same model in DQis used to\nplan all queries. Implementations are wri/t_ten in Java, single-\nthreaded3, and reasonably optimized at the algorithmic level\n(e.g., /Q_uickPick would short-circuit a partial plan already\nestimated to be more costly than the current best plan)—\nbut no signi/f_icant eﬀorts are spent on low-level engineering.\nHence, the relative magnitudes are more meaningful than\nthe absolute values. Experiments were run on an AWS EC2\nc5.9xlarge instance with a 3.0GHz CPU and 72GB memory.\nFigure 5 reports the runtimes grouped by number of rela-\ntions. In the small-join regime, DQ’s overheads are a/t_tributed\ninterfacing with a JVM-based deep learning library, DL4J\n(creating and /f_illing the featurization buﬀers; JNI overheads\ndue to native CPU backend execution). /T_hese could have\nbeen optimized away by targeting a non-JVM engine and/or\n3To ensure fairness, for DQwe con/f_igure the underlying linear algebra\nlibrary to use 1 thread. No GPU is used.\n10\n\n4 5 6 7 8 9 10 11 12 15\nJoin Size101\n100101102103104105106Optimization Latency (ms)\nDQ\nquickpick-1000\nright-deep\nleft-deep\nzigzag\nexhaustiveFigure 5: Optimization latency (log-scale) on all JOB\nqueries grouped by number of relations in each query\n(§6.1.4). A total of 5 trials are run; standard deviations are\nnegligible hence omitted.\nFigure 6: Mean relative cost (in log-scale) as a function\nof the number of training queries seen by DQ. We include\n/Q_uickPick-1000 as a baseline. Cost Model 1 is used.\nGPUs, but we note that when the number of joins is small,\nexhaustive enumeration would be the ideal choice.\nIn the large-join regime, DQachieves drastic speedups:\nfor the largest joins DQruns up to 10,000\u0002faster than ex-\nhaustive enumeration and >10\u0002than le/f_t-deep. DQupper-\nbounds the number of neural net invocations by the number\nof relations in a query, and additionally bene/f_its from the\nbatching optimization ( §4.3). We believe this is a profound\nperformance argument for a learned optimizer—it would\nhave an even more unfair advantage when applied to larger\nqueries or executed on specialized accelerators [24].\n6.1.5 /Q_uantity of Training Data. How much training data\ndoes DQneed to become eﬀective? To study this, we vary the\nnumber of training queries given to DQand plot the mean\nrelative cost using the cross validation technique described\nbefore. Figure 6 shows the relationship. DQrequires about\n60-80 training queries to become competitive and about 30\nqueries to match the plan costs of /Q_uickPick-1000.\nDigging deeper, we found that the break-even point of\n30 queries roughly corresponds to seeing all relations in\nthe schema at least once. In fact, we can train DQon small\nqueries and test it on larger ones—as long as the relations\nare covered well. To investigate this generalization power,\nFigure 7: Relevance of training data vs. DQ’s plan cost. R80\nis a dataset sampled independently of the JOB queries with\nrandom joins/predicates from the schema. R80wp has ran-\ndom joins as before but contains the workload’s predicates.\nWK80 includes 80 actual queries sampled from the workload.\nT80describes a scheme where each of the 33 query templates\nis covered at least once in sampling. /T_hese schemes are in-\ncreasingly “relevant”. Costs are relative w.r.t. EX.\n# Training /Q_ueries Mean Relative Cost\nRandom 80 1.32\nTrain\u00149-way 82 1.61\nTrain\u00148-way 72 9.95\nTable 4: DQtrained on small joins and tested on larger joins.\nCosts are relative to optimal plans.\nwe trained DQon all queries with \u00149 and 8 relations, re-\nspectively, and tested on the remaining queries (out of a total\nof 113). For comparison we include a baseline scheme of\ntraining on 80 random queries and testing on 33; see Table 4.\nTable 4 shows that even when trained on subplans, DQ\nperforms relatively well and generalizes to larger joins (recall,\nthe workload contains up to 15-way joins). /T_his indicates that\nDQindeed learns local structures —eﬃcient joining of small\ncombinations of relations. When those local structures do\nnot suﬃciently cover the cases of interest during deployment,\nwe see degraded performance.\n6.1.6 Relevance and /Q_uality of Training Data. /Q_uantity\nof training data ma/t_ters, and so do relevance andquality . We\n/f_irst study relevance, i.e., the degree of similarity between the\nsampled training data and the test queries. /T_his is controlled\nby changing the training data sampling scheme. Figure 7\nplots the performance of diﬀerent data sampling techniques\neach with 80 training queries. It con/f_irms that the more\nrelevant the training queries can be made towards the test\nworkload, the less data is required for good performance.\nNotably, it also shows that even synthetically generated\nrandom queries ( R80) are useful. DQstill achieves a lower\nrelative cost compared to /Q_uickPick-1000 even with random\nqueries (4.16 vs. 23.87). /T_his experiment illustrates that DQ\ndoes not actually require a priori knowledge of the workload.\n11\n\nFigure 8: /Q_uality of training data vs. DQ’s plan cost. DQ\ntrained on data collected from /Q_uickPick-1000, le/f_t-deep, or\nthe bushy (exhaustive) optimizer. Data variety boosts con-\nvergence speed and /f_inal quality. Costs are relative w.r.t. EX.\nNext, we study the quality of training data, i.e., the opti-\nmality of the native planner DQobserves and gathers data\nfrom. We collect a varying amount of data sampled from the\nnative optimizer, which we choose to be /Q_uickPick-1000, le/f_t-\ndeep, or bushy ( EX). Figure 8 shows that all methods allow\nDQto quickly converge to good solutions. /T_he DP-based\nmethods, le/f_t-deep and bushy, converge faster as they pro-\nduce /f_inal plans and optimal subplans per query. In contrast,\n/Q_uickPick yields only 1000 random full plans per query. /T_he\noptimal subplans from the dynamic programs oﬀer data va-\nriety valuable for training, and they cover be/t_ter the space of\ndiﬀerent relation combinations that might be seen in testing.\n6.2 Real Systems Execution\nIt is natural to ask: how diﬃcult and eﬀective is it for a\nproduction-grade system to incorporate DQ? We address this\nquestion by integrating DQinto two systems, PostgreSQL\nand SparkSQL.4/T_he integrations were found to be straight-\nforward: Postgres and SparkSQL each took less than 300 LoC\nof changes; in total about two person-weeks were spent.\n6.2.1 Postgres Integration. DQintegrates seamlessly with\nthe bo/t_tom-up join ordering optimizer in Postgres. /T_he orig-\ninal optimizer’s DP table lookup is replaced with the invo-\ncation of DQ’ Tensor/f_low (TF) neural network through the\nTF C API. As discussed in §6.1.4, plans are batch-evaluated\nto amortize the TF invocation overhead. We run the Join\nOrder Benchmark experiments on the integrated artifact and\npresent the results below. All of the learning utilizes the cost\nmodel and cardinality estimates provided by Postgres.\nTraining. DQ observes the native cost model and cardi-\nnality estimates from Postgres. We con/f_igured Postgres to\nconsider bushy join plans (the default is to only consider\n4Versions: Spark 2.3; Postgres master branch checked out on 9/17/18.\n0 50 100 150 200050100150200DQ (seconds)Execution Latency\n0.00 0.05 0.10 0.150.000.050.100.15Optimization Latency\nPostgres (seconds)Figure 9: Execution and optimization latencies of DQand\nPostgres on JOB. Each point is a query executed by native\nPostgres (x-axis) and DQ(y-axis). Results below the /y.alt=x\nline represent a speedup. Optimization latency is the time\ntaken for the full planning pipeline, not just join ordering.\nMedian Max\nPostgres, no collection 19.17 ms 149.53 ms\nPostgres, with collection 35.98 ms 184.22 ms\nTable 5: Planning latency with collection turned oﬀ/on.\nle/f_t-deep plans). /T_hese plans generate traces of joins and\ntheir estimated costs in the form described in §3.3. We do not\napply any exploration and execute the native optimizer as is.\nTraining data is collected via Postgres’ logging interface.\nTable 5 shows that DQcan collect training data from an\nexisting system with relatively minimal impact on its normal\nexecution. /T_he overhead can be further minimized if training\ndata is asynchronously, rather than synchronously, logged.\nRuntimes on JOB (Figure 9). We allow the Postgres query\nplanner to plan over 80 of the 113 training queries. We use a\n5-fold cross validation scheme to hold out diﬀerent sets of\n33 queries. /T_herefore, each query has at least one validation\nset in which it was unseen during training. We report the\nworst case planning time and execution time for queries that\nhave multiple such runs. In terms of optimization latency,\nDQis signi/f_icantly faster than Postgres for large joins, up\nto 3\u0002. For small joins there is a substantial overhead due to\nneural network evaluations (even though DQneeds score\nmuch fewer join orders). /T_hese results are consistent with\nthe standalone experiment in Section 6.1.4 and the same\ncomments there on small-join regimes apply. In terms of\nexecution runtimes, DQis signi/f_icantly faster on a number\nof queries; averaging over the entire workload DQyields a\n14% speedup.\n6.2.2 SparkSQL Integration. DQis also integrated into\nSparkSQL, a distributed data analytics engine. To show that\nDQ’s eﬀectiveness applies to more than one workload, we\nevaluate the integrated result on TPC-DS.\nTraining. SparkSQL 2.3 contains a cost-based optimizer\nwhich enumerates bushy plans for queries whose number of\nrelations falls under a tunable threshold. We set this thresh-\nold high enough so that all queries are handled by this bushy\ndynamic program. To score plans, the optimizer invokes\n12\n\n0 5 10 15 20 250510152025DQ (seconds)Execution Latency\n0 200 400 600 800 100002004006008001000Optimization Latency\n0 1 2 30123\nSparkSQL (seconds)Figure 10: Execution and optimization latencies of DQ\nand SparkSQL on TPC-DS (SF1). We use an EC2 c5.9xlarge\ninstance with 36 vCPUs. SparkSQL’s bushy dynamic pro-\ngram takes 1000 seconds to plan the largest query (Q64, 18-\nrelation join); we include a zoomed-in view of the rest of the\nplanning latencies. Results below the /y.alt=xline represent\na speedup. Across the workload, DQ’s mean speedup over\nSparkSQL for execution is 1.0 \u0002and that for optimization is\n3.6\u0002.\nDQ’s trained neural net through TensorFlow Java. We use\nthe native SparkSQL cost model and cardinality estimates.\nAll algorithmic aspects of training data collection remain the\nsame as the Postgres integration.\nEﬀectiveness on TPC-DS (Figure 10). We collect data\nfrom and evaluate on 97 out of all 104 queries in TPC-DS\nv2.4. /T_he data /f_iles are generated with a scale factor of 1 and\nstored as columnar Parquet /f_iles. In terms of execution run-\ntimes, DQmatches SparkSQL over the 97 queries (a mean\nspeedup of 1.0\u0002). In terms of optimization runtimes, DQhas\na mean speedup of 3.6 \u0002but a max speedup of 250 \u0002on the\nquery with largest number of joins (Q64). Note that the mean\noptimization speedup here is less drastic than JOB because\nTPC-DS queries contain much less relations to join.\nDiscussion. In summary, results above show that DQ’s ef-\nfective not only on the one workload designed to stress-\ntest joins, but also on a well-established decision support\nworkload. Further, we demonstrate the ease of integration\ninto production-grade systems including a RDBMS and a\ndistributed analytics engine. We hope these results provide\nmotivation for developers of similar systems to incorporate\nDQ’s learning-based join optimization technique.\n6.3 Fine-Tuning With Feedback\nFinally, we illustrate how DQcan overcome an inaccurate\ncost model by /f_ine-tuning with feedback data ( §5). We focus\non a speci/f_ic JOB query, Q10c, where the cost model particu-\nlarly deviates from the true runtime. Baseline DQis trained\non data collected over 112 queries, which is every query ex-\ncept for Q10c, as usual (i.e., values are costs from Postgres’\nnative cost model). For /f_ine-tuning we execute a varying\namount of these queries and collect their actual runtimes. To\nencourage observing a variety of physical operators, we use\nFigure 11: Eﬀects of /f_ine-tuning DQon JOB Q10c. A\nmodest amount of real execution using around 100\nqueries allows DQto surpass both its original perfor-\nmance (by 3\u0002) as well as Postgres (by 3:5\u0002).\nan exploration parameter of /uni03F5=0:1 when observing run-\ntimes (recall from §3.3 exploration means with probability /uni03F5\nwe form a random intermediate join).\nFigure 11 shows the results as a function of the number\nof queries observed for real execution. Postgres emits a plan\nthat executes in 70 :0s, while baseline DQemits a plan that\nexecutes in 60 :1s. A/f_ter /f_ine-tuning, DQemits a plan that\nexecutes in 20 :3s, outperforming both Postgres and its orig-\ninal performance. /T_his shows true runtimes are useful in\ncorrecting faulty cost model and/or cardinality estimates.\nInterestingly, training a version of DQusing only real run-\ntimes failed to converge to a reasonable model—this suggests\nlearning high-level features from inexpensive samples from\nthe cost model is bene/f_icial.\n7 RELATED WORK\nApplication of machine learning in database internals is still\nthe subject of signi/f_icant debate this year and will continue\nto be a contentious question for years to come [ 11,26,32,37].\nAn important question is what problems are amenable to ma-\nchine learning solutions. We believe that query optimization\nis one such sub-area. /T_he problems considered are generally\nhard and orders-of-magnitude of performance are at stake.\nIn this se/t_ting, poor learning solutions will lead to slow but\nnot incorrect execution, so correctness is not a concern.\nCost Function Learning We are certainly not the /f_irst to\nconsider “learning” in the query optimizer and there are a\nnumber of alternative architectures that one may consider.\n/T_he precursors to this work are a/t_tempts to correct query\noptimizers through execution feedback. One of the seminal\nworks in this area is the LEO optimizer [ 35]. /T_his optimizer\nuses feedback from the execution of queries to correct inac-\ncuracies in its cost model. /T_he underlying cost model is based\non histograms. /T_he basic idea inspired several other impor-\ntant works such as [ 14]. /T_he sentiment in this research still\nholds true today; when Leis et al. extensively evaluated the\n13\n\neﬃcacy of diﬀerent query optimization strategies they noted\nthat feedback and cost estimation errors are still challenges\nin query optimizers [ 29]. A natural /f_irst place to include\nmachine learning would be what we call Cost Function Learn-\ning, where statistical learning techniques are used to correct\nor replace existing cost models. /T_his is very related to the\nproblem of performance estimation of queries [6, 52, 53].\nWe actually investigated this by training a neural network\nto predict the selectivity of a single relation predicate. Results\nwere successful, albeit very expensive from a data perspec-\ntive. To estimate selectivity on an a/t_tribute with 10k distinct\nvalues, the training set had to include 1000 queries. /T_his ar-\nchitecture suﬀers from the problem of featurization of literals ;\nthe results are heavily dependent on learning structure in\nliteral values from the database that are not always straight-\nforward to featurize. /T_his can be especially challenging for\nstrings or other non-numerical data types. A recent work-\nshop paper does show some promising results in using Deep\nRL to construct a good feature representation of subqueries\nbut it still requires >10k queries to train [41].\nLearning in /Q_uery Optimization Recently, there has been\nseveral exciting proposals in pu/t_ting learning inside a query\noptimizer. Ortiz et al. [ 41] applies deep RL to learn a repre-\nsentation of queries, which can then be used in downstream\nquery optimization tasks. Liu [ 31] and Kipf [ 25] use DNNs to\nlearn cardinality estimates. Closer to our work is Marcus et\nal.’s proposal of a deep RL-based join optimizer, ReJOIN [ 33],\nwhich oﬀered a preliminary view of the potential for deep\nRL in this context. /T_he early results reported in [ 33] top out\nat a 20% improvement in plan execution time of Postgres\n(compared to our 3x), and as of that paper they had only\nevaluated on 10 out of the 113 JOB queries that we study\nhere. DQ qualitatively goes beyond that work by oﬀering\nan extensible featurization scheme supporting physical join\nselection. More fundamentally, DQ integrates the dynamic\nprogramming of Q-learning into that of a standard query\noptimizer, which allows us to use oﬀ-policy learning. Due\nto use of on-policy policy gradient methods, [ 33] requires\nabout 8,000 training queries to reach native Postgres/f_i cost\non the 10 JOB queries. DQ exploits optimal substructures of\nthe problem and uses oﬀ-policy Q-learning to increase data-\neﬃciency by two orders of magnitude: 80 training queries to\noutperform Postgres/f_i real execution runtimes on the entire\nJOB benchmark.\nAdaptive /Q_uery Optimization Adaptive query process-\ning [ 9,16] as well as the related techniques to re-optimize\nqueries during execution [ 10,36] is another line of work\nthat we think is relevant to the discussion. Reinforcement\nlearning studies sequential problems and adaptive query op-\ntimization is a sequential decision problem over tuples rather\nthan subplans. We focus our study on optimization in /f_ixeddatabases and the adaptivity that DQoﬀers is at a work-\nload level. Continuously updating a neural network can be\nchallenging for very /f_ine-grained adaptivity, e.g., processing\ndiﬀerent tuples in diﬀerent ways.\nRobustness /T_here are a couple of branches of work that\nstudy robustness to diﬀerent parameters in query optimiza-\ntion. In particular, the /f_ield of “parametric query optimiza-\ntion” [ 22,48], studies the optimization of piecewise linear\ncost models. Interestingly, DQis it is agnostic to this struc-\nture. It learns a heuristic from data identifying diﬀerent\nregimes where diﬀerent classes of plans work. We hope to\ncontinue experiments and a/t_tempt to interpret how DQis\npartitioning the feature space into decisions. /T_here is also a\ndeep link between this work and least expected cost (LEC)\nquery optimization [ 15]. Markov Decision Processes (the\nmain abstraction in RL) are by de/f_inition stochastic and opti-\nmize the LEC objective.\nJoin Optimization At Scale Scaling up join optimization\nhas been an important problem for several decades, most re-\ncently [ 40]. At scale, several randomized approaches can\nbe applied. /T_here is a long history of randomized algo-\nrithms (e.g., the /Q_uickPick algorithm [ 51]) and genetic al-\ngorithms [ 13,46]. /T_hese algorithms are pragmatic and it is\no/f_ten the case that commercial optimizers will leverage such\na method a/f_ter the number of tables grows beyond a certain\npoint. /T_he challenge with these methods is that their eﬃcacy\nis hard to judge. We found that /Q_uickPick o/f_ten varied in\nperformance on the same query quite dramatically.\nAnother heuristic approach is relaxation, or solving the\nproblem exactly under simpli/f_ied assumptions. One straight-\nforward approach is to simply consider greedy search avoid-\ning Cartesian products [ 17], which is also the premise of the\nIK-KBZ algorithms [ 23,27]. Similar linearization arguments\nwere also made in recent work [ 40,49]. Existing heuristics\ndo not handle all types of non-linearities well, and this is\nexactly the situation where learning can help. Interestingly\nenough, our proposed technique has a O¹n3ºruntime, which\nis similar to the linearizedDP algorithm described in [ 40].\nWe hope to explore the very large join regime in the future\nand an interesting direction is to compare DQto recently\nproposed techniques like [40].\n8 DISCUSSION, LIMITATIONS, AND\nCONCLUSION\nWe presented our method with a featurization designed for\ninner joins over foreign key relations as these were the ma-\njor join queries in our benchmarks. /T_his is not a fundamen-\ntal restriction and is designed to ease exposition. It is rela-\ntively straightforward to extend this model to join conditions\ncomposed of conjunctions of binary expressions. Assume\nthe maximum number of expressions in the conjunction is\n14\n\ncapped atN. As before, let Abe the set of all a/t_tributes in the\ndatabase. Each expression has two a/t_tributes and an opera-\ntor. As with featurizing the vertices we can 1-hot encode the\na/t_tributes present. We additionally have to 1-hot encode the\nbinary operatorsf=;,; <; >g. For each of the expressions in\nthe conjunctive predicate, we concatenate the binary feature\nvectors that have its operator and a/t_tributes. Since the maxi-\nmum number of expressions in the conjunction capped at\nN, we can get a /f_ixed sized feature vector for all predicates.\nMore broadly, we believe DQ is a step towards a\nlearning query optimizer. As illustrated by the Cascades\noptimizer [ 19] and follow-on work, cost-based dynamic\nprogramming—whether bo/t_tom up or top-down with\nmemoization—needs not be restricted to select-project-join\nblocks. Most query optimizations can be recast into a space\nof algebraic transformations amenable to dynamic program-\nming, including asymmetric operators like outer joins, cross-\nblock optimizations including order optimizations and “side-\nways information passing”, and even non-relational opera-\ntors like PIVOT. /T_he connection between RL and Dynamic\nProgramming presented in this paper can be easily leveraged\nin those scenarios as well. Of course this blows up the search\nspace, and large spaces are ideal for solutions like the one\nwe proposed.\nIt is popular in recent AI research to try “end-to-end” learn-\ning, where problems that were traditionally factored into\nsubproblems (e.g., self-driving cars involve separate models\nfor localization, obstacle detection and lane-following) are\nlearned in a single uni/f_ied model. One can imagine a simi-\nlar architectural ambition for an end-to-end learning query\noptimizer, which simply maps subplan features to measured\nruntimes. /T_his would require a signi/f_icant corpus of run-\ntime data to learn from, and changes to the featurization and\nperhaps the deep network structure we used here. DQis a\npragmatic middle ground that exploits the structure of the\njoin optimization problem. Further exploring the extremes\nof learning and query optimization in future work may shed\nmore insights.\nREFERENCES\n[1]Amazon Aurora Serverless. h/t_tps://aws.amazon.com/rds/aurora/\nserverless/.\n[2] Apache Calcite. h/t_tps://calcite.apache.org/.\n[3] PostgreSQL. h/t_tps://www.postgresql.org/.\n[4]PostgreSQL: Genetic Query Optimizer. h/t_tps://www.postgresql.org/\ndocs/11/static/geqo.html.\n[5] TPC-DS. h/t_tp://www.tpc.org/tpcds/.\n[6]M. Akdere, U. C ¸etintemel, M. Riondato, E. Upfal, and S. B. Zdonik.\nLearning-based query performance modeling and prediction. In Data\nEngineering (ICDE), 2012 IEEE 28th International Conference on , pages\n390–401. IEEE, 2012.\n[7]M. Armbrust, R. S. Xin, C. Lian, Y. Huai, D. Liu, J. K. Bradley, X. Meng,\nT. Ka/f_tan, M. J. Franklin, A. Ghodsi, et al. Spark sql: Relational dataprocessing in spark. In Proceedings of the 2015 ACM SIGMOD Inter-\nnational Conference on Management of Data , pages 1383–1394. ACM,\n2015.\n[8]J. Arulraj and A. Pavlo. How to build a non-volatile memory database\nmanagement system. In Proceedings of the 2017 ACM International\nConference on Management of Data , pages 1753–1758. ACM, 2017.\n[9]R. Avnur and J. M. Hellerstein. Eddies: Continuously adaptive query\nprocessing. In ACM sigmod record , volume 29, pages 261–272. ACM,\n2000.\n[10] S. Babu, P. Bizarro, and D. DeWi/t_t. Proactive re-optimization. In\nProceedings of the 2005 ACM SIGMOD international conference on Man-\nagement of data , pages 107–118. ACM, 2005.\n[11] P. Bailis, K. S. Tai, P. /T_haker, and M. Zaharia. Don’t throw out\nyour algorithms book just yet: Classical data structures that can out-\nperform learned indexes. h/t_tps://dawn.cs.stanford.edu/2018/01/11/\nindex-baselines/, 2017.\n[12] R. Bellman. Dynamic programming . Princeton University Press, 1957.\n[13] K. Benne/t_t, M. C. Ferris, and Y. E. Ioannidis. A genetic algorithm\nfor database query optimization . Computer Sciences Department,\nUniversity of Wisconsin, Center for Parallel Optimization, 1991.\n[14] S. Chaudhuri, V. Narasayya, and R. Ramamurthy. A pay-as-you-go\nframework for query execution feedback. Proceedings of the VLDB\nEndowment , 1(1):1141–1152, 2008.\n[15] F. Chu, J. Halpern, and J. Gehrke. Least expected cost query opti-\nmization: what can we expect? In Proceedings of the twenty-/f_irst ACM\nSIGMOD-SIGACT-SIGART symposium on Principles of database systems ,\npages 293–302. ACM, 2002.\n[16] A. Deshpande, Z. Ives, V. Raman, et al. Adaptive query processing.\nFoundations and Trends ®in Databases , 1(1):1–140, 2007.\n[17] L. Fegaras. A new heuristic for optimizing large queries. In Interna-\ntional Conference on Database and Expert Systems Applications , pages\n726–735. Springer, 1998.\n[18] R. H. Gerber. Data-/f_low query processing using multiprocessor hash-\npartitioned algorithms. Technical report, Wisconsin Univ., Madison\n(USA), 1986.\n[19] G. Graefe. /T_he cascades framework for query optimization. IEEE Data\nEng. Bull. , 18(3):19–29, 1995.\n[20] G. Graefe and W. McKenna. /T_he volcano optimizer generator. Techni-\ncal report, COLORADO UNIV AT BOULDER DEPT OF COMPUTER\nSCIENCE, 1991.\n[21] T. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul, B. Piot, D. Hor-\ngan, J. /Q_uan, A. Sendonaris, G. Dulac-Arnold, et al. Deep q-learning\nfrom demonstrations. arXiv preprint arXiv:1704.03732 , 2017.\n[22] A. Hulgeri and S. Sudarshan. Parametric query optimization for linear\nand piecewise linear cost functions. In Proceedings of the 28th inter-\nnational conference on Very Large Data Bases , pages 167–178. VLDB\nEndowment, 2002.\n[23] T. Ibaraki and T. Kameda. On the optimal nesting order for computing\nn-relational joins. ACM Transactions on Database Systems (TODS) ,\n9(3):482–502, 1984.\n[24] N. P. Jouppi, C. Young, N. Patil, D. Pa/t_terson, G. Agrawal, R. Bajwa,\nS. Bates, S. Bhatia, N. Boden, A. Borchers, et al. In-datacenter perfor-\nmance analysis of a tensor processing unit. In Computer Architecture\n(ISCA), 2017 ACM/IEEE 44th Annual International Symposium on , pages\n1–12. IEEE, 2017.\n[25] A. Kipf, T. Kipf, B. Radke, V. Leis, P. Boncz, and A. Kemper. Learned\ncardinalities: Estimating correlated joins with deep learning. arXiv\npreprint arXiv:1809.00677 , 2018.\n[26] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. /T_he case\nfor learned index structures. In Proceedings of the 2018 International\nConference on Management of Data , pages 489–504. ACM, 2018.\n15\n\n[27] R. Krishnamurthy, H. Boral, and C. Zaniolo. Optimization of nonre-\ncursive queries. In VLDB , volume 86, pages 128–137, 1986.\n[28] M. Laskey, J. Lee, R. Fox, A. Dragan, and K. Goldberg. Dart: Noise\ninjection for robust imitation learning. Conference on Robot Learning\n2017, 2017.\n[29] V. Leis, A. Gubichev, A. Mirchev, P. Boncz, A. Kemper, and T. Neumann.\nHow good are query optimizers, really? Proceedings of the VLDB\nEndowment , 9(3):204–215, 2015.\n[30] S. Levine, P. Pastor, A. Krizhevsky, J. Ibarz, and D. /Q_uillen. Learn-\ning hand-eye coordination for robotic grasping with deep learning\nand large-scale data collection. /T_he International Journal of Robotics\nResearch , 37(4-5):421–436, 2018.\n[31] H. Liu, M. Xu, Z. Yu, V. Corvinelli, and C. Zuzarte. Cardinality es-\ntimation using neural networks. In Proceedings of the 25th Annual\nInternational Conference on Computer Science and So/f_tware Engineering ,\npages 53–59. IBM Corp., 2015.\n[32] L. Ma, D. Van Aken, A. Hefny, G. Mezerhane, A. Pavlo, and G. J. Gordon.\n/Q_uery-based workload forecasting for self-driving database manage-\nment systems. In Proceedings of the 2018 International Conference on\nManagement of Data , pages 631–645. ACM, 2018.\n[33] R. Marcus and O. Papaemmanouil. Deep reinforcement learning for\njoin order enumeration. arXiv preprint arXiv:1803.00055 , 2018.\n[34] R. Marcus and O. Papaemmanouil. Towards a hands-free query opti-\nmizer through deep learning. arXiv preprint arXiv:1809.10212 , 2018.\n[35] V. Markl, G. M. Lohman, and V. Raman. LEO: An autonomic query\noptimizer for DB2. IBM Systems Journal , 42(1):98–106, 2003.\n[36] V. Markl, V. Raman, D. Simmen, G. Lohman, H. Pirahesh, and M. Cil-\nimdzic. Robust query processing through progressive optimization.\nInProceedings of the 2004 ACM SIGMOD international conference on\nManagement of data , pages 659–670. ACM, 2004.\n[37] M. Mitzenmacher. A model for learned bloom /f_ilters and related\nstructures. arXiv preprint arXiv:1802.00884 , 2018.\n[38] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-\nstra, and M. Riedmiller. Playing atari with deep reinforcement learning.\nInarXiv , 2013.\n[39] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Belle-\nmare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al.\nHuman-level control through deep reinforcement learning. In Nature .\nNature Research, 2015.\n[40] T. Neumann and B. Radke. Adaptive optimization of very large join\nqueries. In Proceedings of the 2018 International Conference on Man-\nagement of Data , pages 677–692. ACM, 2018.\n[41] J. Ortiz, M. Balazinska, J. Gehrke, and S. S. Keerthi. Learning state\nrepresentations for query optimization with deep reinforcement learn-\ning. In Proceedings of the Second Workshop on Data Management for\nEnd-To-End Machine Learning , DEEM’18, pages 4:1–4:4, New York, NY,\nUSA, 2018. ACM.\n[42] T. Osa, J. Pajarinen, G. Neumann, J. A. Bagnell, P. Abbeel, J. Peters,\net al. An algorithmic perspective on imitation learning. Foundations\nand Trends ®in Robotics , 7(1-2):1–179, 2018.\n[43] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proxi-\nmal policy optimization algorithms. arXiv preprint arXiv:1707.06347 ,\n2017.\n[44] P. G. Selinger, M. M. Astrahan, D. D. Chamberlin, R. A. Lorie, and T. G.\nPrice. Access path selection in a relational database management sys-\ntem. In Proceedings of the 1979 ACM SIGMOD international conference\non Management of data , pages 23–34. ACM, 1979.\n[45] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driess-\nche, J. Schri/t_twieser, I. Antonoglou, V. Panneershelvam, M. Lanctot,\net al. Mastering the game of go with deep neural networks and tree\nsearch. In Nature . Nature Research, 2016.[46] M. Steinbrunn, G. Moerko/t_te, and A. Kemper. Heuristic and random-\nized optimization for the join ordering problem. /T_he VLDB Journal/f_i/T_he\nInternational Journal on Very Large Data Bases , 6(3):191–208, 1997.\n[47] R. S. Su/t_ton, A. G. Barto, et al. Reinforcement learning: An introduction .\nMIT press, 1998.\n[48] I. Trummer and C. Koch. Multi-objective parametric query optimiza-\ntion. Proceedings of the VLDB Endowment , 8(3):221–232, 2014.\n[49] I. Trummer and C. Koch. Solving the join ordering problem via mixed\ninteger linear programming. In Proceedings of the 2017 ACM Inter-\nnational Conference on Management of Data , pages 1025–1040. ACM,\n2017.\n[50] H. Van Hasselt, A. Guez, and D. Silver. Deep reinforcement learning\nwith double q-learning. In AAAI , volume 2, page 5. Phoenix, AZ, 2016.\n[51] F. Waas and A. Pellenko/f_t. Join order selection (good enough is easy).\nInBritish National Conference on Databases , pages 51–67. Springer,\n2000.\n[52] W. Wu, Y. Chi, H. Hac ´ıg¨um¨us ¸, and J. F. Naughton. Towards predicting\nquery execution time for concurrent and dynamic database workloads.\nProceedings of the VLDB Endowment , 6(10):925–936, 2013.\n[53] W. Wu, Y. Chi, S. Zhu, J. Tatemura, H. Hacig ¨um¨us, and J. F. Naughton.\nPredicting query execution time: Are optimizer cost models really\nunusable? In Data Engineering (ICDE), 2013 IEEE 29th International\nConference on , pages 1081–1092. IEEE, 2013.\n[54] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How transferable are\nfeatures in deep neural networks? In Advances in neural information\nprocessing systems , pages 3320–3328, 2014.\n[55] M. Ziane, M. Za ¨ıt, and P. Borla-Salamet. Parallel query processing\nwith zigzag trees. /T_he VLDB Journal/f_i/T_he International Journal on Very\nLarge Data Bases , 2(3):277–302, 1993.\nA STANDALONE OPTIMIZATION\nEXPERIMENT SETUP\nWe consider three diﬀerent cost models on the same work-\nload:\nCM1: In the /f_irst cost model (inspired by [ 29]), we model\na main-memory database that performs two types of joins:\nindex joins and in-memory hash joins. Let Odescribe the\ncurrent operator, Olbe the le/f_t child operator, and Orbe the\nright child operator. /T_he costs are de/f_ined with the following\nrecursions:\ncij¹Oº=c¹Olº+match¹Ol;Orº\u0001jOlj\nchj¹Oº=c¹Olº+c¹Orº+jOj\nwhere cdenotes the cost estimation function, j\u0001jis the car-\ndinality function, and match denotes the expected cost of\nan index match, i.e., fraction of records that match the index\nlookup (always greater than 1) multiplied by a constant fac-\ntorλ(we chose 1 :0). We assume indexes on the primary keys.\nIn this cost model, if an eligible index exists it is generally\ndesirable to use it, since match¹Ol;Orº\u0001jOljrarely exceeds\nc¹Orº+jOjfor foreign key joins. Even though the cost model\nis nominally “non-linear”, primary tradeoﬀ between the in-\ndex join and hash join is due to index eligibility and not\ndependent on properties of the intermediate results. For the\nJOB workload, unless λis set to be very high, hash joins have\nrare occurrences compared to index joins.\n16\n\nCM2: In the next cost model, we remove index eligibility\nfrom consideration and consider only hash joins and nested\nloop joins with a memory limit M. /T_he model charges a cost\nwhen data requires additional partitioning, and further falls\nback to a nested loop join when the smallest table exceeds\nthe squared memory:\ncjoin=8>>> <\n>>>:c¹Olº+c¹Orº+jOjifjOrj+jOlj\u0014M\nc¹Olº+c¹Orº+2¹jOrj+jOljº+jOjifmin¹jOrj;jOljº\u0014M2\nc¹Olº+c¹Orº+¹jOrj+l\njOrj\nMm\njOljº\n/T_he non-linearities in this model are size-dependent, so con-\ntrolling the size of intermediate relations is important in the\noptimization problem. We set the memory limit Mto 105\ntuples in our experiments. /T_his limit is low in real-world\nterms due to the small size of the benchmark data. However,\nwe intend for the results to be illustrative of what happens\nin the optimization problems.\nCM3: In the next cost model, we model a database that\naccounts for the reuse of already-built hash tables. We use\nthe Gamma database convention where the le/f_t operator as\nthe “build” operator and the right operator as the “probe”\noperator [ 18]. If the previous join has already built a hash\ntable on an a/t_tribute of interest, then the hash join does not\nincur another cost.\ncnobuild =c¹Olº+c¹Orº\u0000jOrj+jOj\nWe also allow for index joins as in CM1. /T_his model makes\nhash joins substantially cheaper in cases where re-use is\npossible. /T_his model favors some subplans to be right-deep\nplans which maximize the reuse of the built hash tables.\n/T_herefore, optimal solutions have both le/f_t-deep and right-\ndeep segments.\nIn our implementation of these cost models, we use true\ncardinalities on single-table predicates, and we leverage stan-\ndard independence assumptions to construct more compli-\ncated cardinality estimates. (/T_his is not a fundamental limita-\ntion of DQ. Results in §6.2 have shown that when Postgres\nand SparkSQL provide their native cost model and cardinality\nestimates, DQis as eﬀective.) /T_he goal of this work is to eval-\nuate the join ordering process independent of the strength\nor weakness of the underlying cardinality estimation.\nWe consider the following baseline algorithms. /T_hese algo-\nrithms are not meant to be a comprehensive list of heuristics\nbut rather representative of a class of solutions.\n(1)Exhaustive ( EX): /T_his is a dynamic program that ex-\nhaustively enumerates all join plans avoiding Carte-\nsian products.\n(2)le/f_t-deep ( LD): /T_his is a dynamic program that ex-\nhaustively enumerates all le/f_t-deep join plans.\n(3)Right-Deep ( RD): /T_his is a dynamic program that\nexhaustively enumerates all right-deep join plans.(4)Zig-Zag ( ZZ): /T_his is a dynamic program that ex-\nhaustively enumerates all zig-zag trees (every join\nhas at least one base relation, either on the le/f_t or\nthe right) [55].\n(5)IK-KBZ ( KBZ ): /T_his algorithm is a polynomial time\nalgorithm that decomposes the query graph into\nchains and orders the chains based on a linear ap-\nproximation of the cost model [27].\n(6)/Q_uickPick-1000 ( QP): /T_his algorithm randomly se-\nlects 1000 join plans and returns the best of them.\n1000 was selected to be roughly equivalent to the\nplanning latency of DQ[51].\n(7)Minimum Selectivity ( MinSel ): /T_his algorithm se-\nlects the join ordering based on the minimum se-\nlectivity heuristic [ 40]. While MinSel was fast, we\nfound poor performance on the 3 cost models used\nin the paper.\n(8)Linearized Dynamic Program ( LDP ): /T_his approach\napplies a dynamic program in the inner-loop of\nIK-KBZ [ 40]. Not surprisingly, LDP/f_is results were\nhighly correlated with those of IK-KBZ and Le/f_t-\nDeep enumeration, so we chose to omit them from\nthe main body of the paper.\nAll of the algorithms consider join ordering without\nCartesian products, so EXis an optimal baseline. We re-\nport results in terms of the suboptimality w.r.t. EX, namely\ncost al/afii10069.italocost EX. We present results on all 113 JOB queries.\nWe train on 80 queries and test on 33 queries. We do 4-fold\ncross validation to ensure that every test query is excluded\nfrom the training set at least once. /T_he performance of DQis\nonly evaluated on queries not seen in the training workload.\nOur standalone experiments are integrated with Apache\nCalcite [ 2]. Apache Calcite provides libraries for parsing\nSQL, representing relational algebraic expressions, and a\nVolcano-based query optimizer [ 19,20]. Calcite does not han-\ndle physical execution or storage and uses JDBC connectors\nto a variety of database engines and /f_ile formats. We imple-\nmented a package inside Calcite that allowed us to leverage\nits parsing and plan representation, but also augment it with\nmore sophisticated cost models and optimization algorithms.\nStandalone DQis wri/t_ten in single-threaded Java. /T_he ex-\ntended results including omi/t_ted techniques are described in\nTable 6.\nBCOUTCOST MODEL\nWe additionally omi/t_ted experiments with a simpli/f_ied cost\nmodel only searching for join orders and ignoring physical\noperator selection. We fed in true cardinalities to estimate\nthe selectivity of each of the joins, which is a perfect version\nof the “ Cout” model. We omi/t_ted these results as we did not\nsee diﬀerences between the techniques and the goal of the\nstudy was to understand the performance of DQ over cost\n17\n\nOptimizer Cost Model 1 Cost Model 2 Cost Model 3\nMin Mean Max Min Mean Max Min Mean Max\n/Q_uickPick (QP) 1 23.87 405.04 7.43 51.84 416.18 1.43 16.74 211.13\nIK-KBZ (KBZ) 1 3.45 36.78 5.21 29.61 106.34 2.21 14.61 96.14\nRight-deep (RD) 4.7 53.25 683.35 1.93 8.21 89.15 1.83 5.25 69.15\nLe/f_t-deep (LD) 1 1.08 2.14 1.75 7.31 65.45 1.35 4.21 35.91\nZig-zag (ZZ) 1 1.07 1.87 1 5.07 43.16 1 3.41 23.13\nExhaustive (EX) 1 1 1 1 1 1 1 1 1\nDQ 1 1.32 3.11 1 1.68 11.64 1 1.91 13.14\nMinimum Selectivity (MinSel) 2.43 59.86 1083.12 23.46 208.23 889.7 9.81 611.1 2049.13\nIK-KBZ+DP (LDP) 1 1.09 2.72 2.1 10.03 105.32 2.01 3.99 32.19\nTable 6: Extended results including omitted techniques for all three cost models.\nmodels that cause the heuristics to fail. In particular, we\nfound that threshold non-linearities as in CM3 cause the\nmost problems.\nCout Mean\nQP 1.02\nIK-KBZ 1.34\nLD 1.02\nZZ 1.02\nEx 1\nDQ 1.03\nMinSel 1.11\nC ADDITIONAL STANDALONE\nEXPERIMENTS\nIn the subsequent experiments, we try to characterize when\nDQis expected to work and how eﬃciently.\nC.1 Sensitivity to Training Data\nClassically, join optimization algorithms have been deter-\nministic. Except for QP, all of our baselines are deterministic\nas well. Randomness in DQ(besides /f_loating-point compu-\ntations) stems from what training data is seen. We run an\nexperiment where we provide DQwith 5 diﬀerent training\ndatasets and evaluate on a set of 20 hold-out queries. We\nreport the max range (worst factor over optimal minus best\nfactor over optimal) in performance over all 20 queries in\nTable 7. For comparison, we do the same with QPover 5\ntrials (with a diﬀerent random seed each time).\nCM1 CM2 CM3\nQP 2.11\u00021.71\u00023.44\u0002\nDQ 1.59\u00021.13\u00022.01\u0002\nTable 7: Plan variance over trials.We found that while the performance of DQdoes vary\ndue to training data, the variance is relatively low. Even if\nwe were to account for this worst case, DQwould still be\ncompetitive in our macro-benchmarks. It is also substantially\nlower than that of QP, a true randomized algorithm.\nC.2 Sensitivity to Faulty Cardinalities\nIn general, the cardinality/selectivity estimates computed\nby the underlying RDBMS do not have up-to-date accuracy.\nAll query optimizers, to varying degrees, are exposed to\nthis issue since using faulty estimates during optimization\nmay yield plans that are in fact suboptimal. It is therefore\nworthwhile to investigate this sensitivity and try to answer,\n“is the neural network more or less sensitive than classical\ndynamic programs and heuristics?”\nIn this microbenchmark, the optimizers are fed perturbed\nbase relation cardinalities (explained below) during optimiza-\ntion; a/f_ter the optimized plans are produced, they are scored\nby an oracle cost model. /T_his means, in particular, DQonly\nsees noisy relation cardinalities during training and is tested\non true cardinalities. /T_he workload consists of 20 queries\nrandomly chosen out of all JOB queries; the join sizes range\nfrom 6 to 11 relations. /T_he /f_inal costs reported below are the\naverage from 4-fold cross validation.\n/T_he perturbation of base relation cardinalities works as\nfollows. We pick Nrandom relations, the true cardinality\nof each is multiplied by a factor drawn uniformly from\nf2;4;8;16g. AsNincreases, the estimate noisiness increases\n(errors in the leaf operators get propagated upstream in a\ncompounding fashion). Table 8 reports the /f_inal costs with\nrespect to estimate noisiness.\nObserve that, despite a slight degradation in the N=4\nexecution, DQis not any more sensitive than the KBZ heuris-\ntic. It closely imitates exhaustive enumeration—an expected\nbehavior since its training data comes from EX’s plans com-\nputed with the faulty estimates.\n18\n\nN=0N=2N=4N=8\nKBZ 6.33 6.35 6.35 5.85\nLD 5.51 5.53 5.53 5.60\nEX 5.51 5.53 5.53 5.60\nDQ 5.68 5.70 5.96 5.68\nTable 8: Costs ( log10) when Nrelations have perturbed car-\ndinalities.\nFigure 12: We plot the runtime in milliseconds of a single\nquery (q10c) with diﬀerent variations of DQ (fully oﬀline,\n/f_ine tuning, and fully online). We found that the /f_ine-tuned\napproach was the most eﬀective one.\nC.3 Ablation Study\nTable 9 reports an ablation study of the featurization de-\nscribed earlier ( §4.1):\nGraph Features Sel. Scaling Loss\nNo Predicates No No 0.087\nYes No 0.049\nYes Yes 0.049\nPredicates No No 0.071\nYes No 0.051\nYes Yes 0.020\nTable 9: Feature ablation.\nWithout features derived from the query graph (Figure 3b)\nand selectivity scaling (Figure 4a) the training loss is 3.5 \u0002\nmore. /T_hese results suggest that all of the diﬀerent features\ncontribute positively for performance.\nD DISCUSSION ABOUT POSTGRES\nEXPERIMENT\nWe also run a version of DQ where the model is only trained\nwith online data (eﬀectively the se/t_ting considered in Re-\nJOIN [ 33]). Even on an idealized workload of optimizing a\nsingle query (/Q_uery 10c), we could not get that approach toconverge. We believe that the discrepancy from [ 33] is due\nto physical operator selection. In that work, the Postgres op-\ntimizer selects the physical operators given the appropriate\nlogical plans selected by the RL policy. With physical oper-\nator selection, the learning problem becomes signi/f_icantly\nharder (Figure 12).\nWe initially hypothesized the DQoutperforms the native\nPostgres optimizer in terms of execution times since it consid-\ners bushy plans. /T_his hypothesis only partially explains the\nresults. We run the same experiment where DQis restricted\nto producing le/f_t-deep plans; in other words, DQconsiders\nthe same plan space as the native Postgres optimizer. We\nfound that there was still a statistically signi/f_icant speedup:\nMean Max\nDQ:LD 1.09\u00022.68\u0002\nDQ:EX 1.14\u00022.72\u0002\nTable 10: Execution time speedup over Postgres with dif-\nferent plan spaces considered by DQ. Mean is the average\nspeedup over the entire workload and max is the best case\nsingle-query speedup.\nWe speculate that the speedup is caused by imprecision in\nthe Postgres cost model. As a learning technique, DQmay\nsmooth out inconsistencies in the cost model.\nFinally, we compare with Postgres’ genetic optimizer\n(GEQ) on the 10 largest joins in JOB. DQis about 7% slower\nin planning time, but nearly 10 \u0002faster in execution time. /T_he\ndiﬀerence in execution is mostly due to one outlier query on\nwhich GEQ is 37\u0002slower.\n19",
  "textLength": 94775
}