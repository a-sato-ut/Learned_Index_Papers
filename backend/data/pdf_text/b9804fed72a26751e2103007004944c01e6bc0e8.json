{
  "paperId": "b9804fed72a26751e2103007004944c01e6bc0e8",
  "title": "A Survey on Deep Reinforcement Learning for Data Processing and Analytics",
  "pdfPath": "b9804fed72a26751e2103007004944c01e6bc0e8.pdf",
  "text": "A Survey on Deep Reinforcement Learning for Data\nProcessing and Analytics\nQingpeng Cai\u0003y, Can Cui\u0003y, Yiyuan Xiong\u0003y, Wei Wangy,\nZhongle Xiex, Meihui Zhangz\nyNational University of SingaporexZhejiang University\nzBeijing Institute of Techonology\n{qingpeng, cuican, yiyuan, wangwei}@comp.nus.edu.sg xiezl@zju.edu.cn meihui_zhang@bit.edu.cn\nAbstract\nData processing and analytics are fundamental and pervasive. Algorithms play\na vital role in data processing and analytics where many algorithm designs have in-\ncorporated heuristics and general rules from human knowledge and experience to im-\nprovetheireÔ¨Äectiveness. Recently, reinforcementlearning, deepreinforcementlearning\n(DRL) in particular, is increasingly explored and exploited in many areas because it\ncan learn better strategies in complicated environments it is interacting with than\nstatically designed algorithms. Motivated by this trend, we provide a comprehen-\nsive review of recent works focusing on utilizing DRL to improve data processing and\nanalytics. First, we present an introduction to key concepts, theories, and methods\nin DRL. Next, we discuss DRL deployment on database systems, facilitating data\nprocessing and analytics in various aspects, including data organization, scheduling,\ntuning, and indexing. Then, we survey the application of DRL in data processing\nand analytics, ranging from data preparation, natural language processing to health-\ncare, Ô¨Åntech, etc. Finally, we discuss important open challenges and future research\ndirections of using DRL in data processing and analytics.\n1 Introduction\nIn the age of big data, data processing and analytics are fundamental, ubiquitous, and\ncrucial to many organizations which undertake a digitalization journey to improve and\ntransform their businesses and operations. Data analytics typically entails other key op-\nerations such as data acquisition, data cleansing, data integration, modeling, etc., before\ninsights could be extracted. Big data can unleash signiÔ¨Åcant value creation across many\nsectors such as healthcare and retail [56]. However, the complexity of data (e.g., high\nvolume, high velocity, and high variety) presents many challenges in data analytics and\nhence renders the diÔ¨Éculty in drawing meaningful insights. To tackle the challenge and\nfacilitate the data processing and analytics eÔ¨Éciently and eÔ¨Äectively, a large number of\nalgorithms and techniques have been designed and numerous learning systems have also\nbeen developed by researchers and practitioners such as Spark MLlib [63], and RaÔ¨Åki [106].\nTo support fast data processing and accurate data analytics, a huge number of algorithms\nrely on rules that are developed based on human knowledge and experience. For example,\nshortest-job-Ô¨Årst is a scheduling algorithm that chooses the job with the smallest execu-\ntion time for the next execution. However, without fully exploiting characteristics of the\nworkload, it can achieve inferior performance compared to a learning-based scheduling\nalgorithm [58]. Another example is packet classiÔ¨Åcation in computer networking which\nmatches a packet to a rule from a set of rules. One solution is to construct the decision\n\u0003These authors have contributed equally to this work, and M. Zhang is the contact author.\n1arXiv:2108.04526v3  [cs.LG]  4 Feb 2022\n\ntree using hand-tuned heuristics for classiÔ¨Åcation. SpeciÔ¨Åcally, the heuristics are designed\nfor a particular set of rules and thus may not work well for other workloads with dif-\nferent characteristics [47]. We observe three limitations of existing algorithms [97, 46].\nFirst, the algorithms are suboptimal. Useful information such as data distribution could\nbe overlooked or not fully exploited by the rules. Second, the algorithm lacks adaptivity.\nAlgorithms designed for a speciÔ¨Åc workload cannot perform well in another diÔ¨Äerent work-\nload. Third, the algorithm design is a time-consuming process. Developers have to spend\nmuch time trying a lot of rules to Ô¨Ånd one that empirically works.\nLearning-based algorithms have also been studied for data processing and analytics. Two\ntypes of learning methods are often used: supervised learning and reinforcement learning.\nThey achieve better performance by direct optimization of the performance objective. Su-\npervised learning typically requires a rich set of high-quality labeled training data, which\ncould be hard and challenging to acquire. For example, conÔ¨Åguration tuning is impor-\ntant to optimize the overall performance of a database management system (DBMS)[44].\nThere could be hundreds of tuning knobs that are correlated in discrete and continuous\nspace. Furthermore, diverse database instances, query workloads, hardware characteris-\ntics render data collection infeasible, especially in the cloud environment. Compared to\nsupervised learning, reinforcement learning shows good performance because it adopts a\ntrial-and-error search and requires fewer training samples to Ô¨Ånd good conÔ¨Åguration for\ncloud databases [123]. Another speciÔ¨Åc example would be query optimization in query\nprocessing. Database system optimizers are tasked to Ô¨Ånd the best execution plan for a\nquery to reduce its query cost. Traditional optimizers typically enumerate many candi-\ndate plans and use a cost model to Ô¨Ånd the plan with minimal cost. The optimization\nprocess could be slow and inaccurate [42]. Without relying on an inaccurate cost model,\ndeep reinforcement learning (DRL) methods improve the execution plan (e.g., changing\nthe table join orders) by interacting with the database[61, 37]. Figure 1 provides a typical\nworkÔ¨Çow for query optimization using DRL. When the query is sent to the agent (i.e., DRL\noptimizer), it produces a state vector via conducting featurization on essential information,\nsuch as the accessed relations and tables. Taking the state as the input, the agent employs\nneural networks to produce the probability distribution of an action set, where the action\nset could contain all possible join operations as potential actions. Each action denotes a\npartial join plan on a pair of tables, and the state will be updated once an action is taken.\nAfter taking possible actions, a complete plan is generated, which is then executed by a\nDBMS to get the reward. In this query optimization problem, the reward can be calcu-\nlated by the real latency. During the training process with the reward signal, the agent\ncan improve the policy and produce a better join ordering with a higher reward (i.e., less\nlatency).\n2\n\nDBMS EngineABPolicyStateùë†: Featurization ()ABCDA.age,   B.age,   C.age,      ‚Ä¶Action Set a!ùúã!(ùëé|ùë†)SQL Query: SELECT * FROM A, B, C, D WHERE A.age= B.ageAND A.age= C.ageAND ‚Ä¶\n‚ãàA.Age= B.ageACa(‚ãàA.Age= C.age‚Ä¶ ‚Ä¶RewardùëÖ(ùë†,ùëé)TransitionùëÉ(ùë†!|ùë†,ùëé)Agent(DRL Optimizer)\nDCBAcomplete join ordering plan‚ãà‚ãà‚ãàFigure 1: The WorkÔ¨Çow of DRL for Query Optimization. A, B, C and D are four tables.\nReinforcement learning (RL) [89] focuses on learning to make intelligent actions in an\nenvironment. The RL algorithm works on the basis of exploration and exploitation to\nimprove itself with feedback from the environment. In the past decades, RL has achieved\ntremendous improvements in both theoretical and technical aspects [86, 89]. Notably,\nDRL incorporates deep learning (DL) techniques to handle complex unstructured data\nand has been designed to learn from historical data and self-exploration to solve notori-\nously hard and large-scale problems (e.g., AlphaGo[85]). In recent years, researchers from\ndiÔ¨Äerent communities have proposed DRL solutions to address issues in data processing\nand analytics[116, 58, 52]. We categorize existing works using DRL from two perspectives:\nsystem and application. From the system‚Äôs perspective, we focus on fundamental research\ntopics ranging from general ones, such as scheduling, to system-speciÔ¨Åc ones, such as query\noptimization in databases. We shall also emphasize how it is formulated in the Markov\nDecision Process and discuss how the problem can be solved by DRL more eÔ¨Äectively\ncompared to traditional methods. Many techniques such as sampling and simulation are\nadopted to improve DRL training eÔ¨Éciency because workload execution and data collec-\ntion in the real system could be time-consuming [31]. From the application‚Äôs perspective,\nwe shall cover various key applications in both data processing and data analytics to pro-\nvide a comprehensive understanding of the DRL‚Äôs usability and adaptivity. Many domains\nare transformed by the adoption of DRL, which helps to learn domain-speciÔ¨Åc knowledge\nabout the applications.\nIn this survey, we aim at providing a broad and systematic review of recent advancements\nin employing DRL in solving data systems, data processing and analytics issues. In Section\n3\n\n2, we introduce the key concepts, theories, and techniques in RL to lay the foundations. To\ngainadeeperunderstandingofDRL,readerscouldrefertotherecentlypublishedbook[13],\nwhich covers selected DRL research topics and applications with detailed illustrations.\nIn Section 3, we review the latest important research works on using DRL for system\noptimization to support data processing and analytics. We cover fundamental topics such\nas data organization, scheduling, system tuning, index, query optimization, and cache\nmanagement. In Section 4, we discuss using DRL for applications in data processing and\nanalytics ranging from data preparation, natural language interaction to various real-world\napplicationssuchashealthcare, Ô¨Åntech, E-commerce, etc. InSection5, wehighlightvarious\nopen challenges and potential research problems. We conclude in Section 6. This survey\nfocuses on recent advancements in exploring RL for data processing and analytics that\nspurs great interest, especially in the database and data mining community. There are\nsurvey papers discussing DRL for other domains. We refer readers to the survey of DRL\nfor healthcare in [118], communications and networking in [54], and RL explainability in\n[76]. Another work[107] discusses how deep learning can be used to optimize database\nsystem design, and vice versa. In this paper, we use \"DRL\" and \"RL\" interchangeably.\n2 Theoretical Foundation and Algorithms of Reinforcement\nLearning\nRL is targeted to solve the sequential decision making problem and the goal is to take\nactions with maximum expected rewards. In detail, the agent follows a policy to make\na series of decisions (i.e. taking actions) in diÔ¨Äerent states of the environment, and the\nsequence of the states and the actions form a trajectory. To estimate whether the policy\nis good or not, each decision under the policy will be evaluated by the accumulated re-\nwards through the trajectory. After evaluating the policy from the trajectories, the agent\nnext improves the policy by increasing the probabilities of making decisions with greater\nexpected rewards. By repeating these steps, the agent can improve the policy through\ntrial-and-error until the policy reaches the optimal, and such a sequential decision-making\nprocess is modeled via Markov Decision Process (MDP).\n2.1 Markov Decision Process\nMathematically, MDP, shown in Figure 1, is a stochastic control process MdeÔ¨Åned by a\ntuple with 5 elements, M= (S;A;R;P;\r), which are explained as follows.\n‚Ä¢StateS:Sis the space for states that denote diÔ¨Äerent situations in the environment\nandst2Sdenotes the state of the situation at the time t.\n‚Ä¢ActionA:Ais the space for actions that the agent can take; the actions can either\nbe discrete or continuous, and at2Adenotes the action taken at the time t.\n‚Ä¢Reward function R(st;at): It denotes the immediate reward of the action attaken\nunder the state st.\n‚Ä¢Transition function P(st+1=s0jst=s;at=a): It denotes the probability of transi-\ntion to the state s0at the time t+ 1given the current state sand the taken action a\nat the time t.\n‚Ä¢Discount factor \r2[0;1]: The total rewards of a certain action consist of both\nimmediate rewards and future rewards, and the \rquantiÔ¨Åes how much importance\nwe give for future rewards.\n4\n\nWe take the query optimization problem demonstrated in Figure 1 to help explain the\nÔ¨Åve components of the MDP. In this example, the state is expressed as a state vector,\nwhich summarizes the information of relations and tables that are assessed by the query q.\nIn each state, the RL agent produces a probability distribution over all potential actions\nwhere each action denotes a partial join plan on a pair of tables. After repeating these\ntwo processes, it reaches a terminal state where the Ô¨Ånal join ordering is generated for an\nagent to execute, and all actions‚Äô target rewards are measured by the actual performance\n(i.e., latency) or a cost model. As for the transition function, the transitions of the states\nare always deterministic in both this problem and most of the other DB problems.\nIn RL, we aim to train the agent with a good policy \u0019that is a mapping function from\nstate to action. Through the policy, the agent can take a series of actions that will result in\ncontinuous changes in the states, and the sequence of the states and the actions following\nthe policy\u0019form a trajectory \u001c= (s0;a0;s1;a1;:::). From each \u001c, we can evaluate the\neÔ¨Äect of each action by the accumulated rewards G, and it consists of the immediate reward\nof this action and the discounted rewards of its following actions in the trajectory. The\ntotal resultGfor the action atis as follows:G(\u001c) =P\nt=0\rtrt, where\rquantiÔ¨Åes how\nmuch importance we give for future rewards. With a bigger \r, the RL agent will be more\nlikely to take any action that may have a less immediate reward at the current time but\nhas a greater future reward in expectation.\nRL continuously evaluates the policy \u0019and improves it until it reaches the optimal policy\n\u0019\u0003= arg max (\u001c\u0018\u0019)G(\u001c)where the agent always takes actions that maximize the expected\nreturn. To evaluate the policy \u0019, RL algorithms estimate how good or bad it is for a state\nand a state-action pair by the function Vand functionQrespectively. Both of these two\nvalue functions are calculated according to the discounted return Gin expectation which\ncan be written as:\nV\u0019(s) =E\u001c\u0018\u0019[G(\u001c)js0=s] (1)\nQ\u0019(s;a) =E\u001c\u0018\u0019[G(\u001c)js0=s;a 0=a] (2)\nThese two value functions have a close association where the V\u0019(st)is the expectation of\nthe functionQof all possible actions under the state staccording to the policy \u0019, and the\nQ\u0019(st;at)is the combination of the immediate reward of the action atand the expectation\nof all possible states‚Äô values after taking the action atunder the state st. Hence, we have:\nV\u0019(s) =X\na2A\u0019(ajs)Q\u0019(s;a) (3)\nQ\u0019(s;a) =R(s;a) +\rX\ns02SP(s0js;a)V\u0019(s0) (4)\nGiven a policy \u0019, we can evaluate its value functions by Bellman equations [89] which\nutilize the recursive relationships of these value functions. Formally, Bellman equations\ndeduce the relationships between a given state (i.e. function V) or a given state-action pair\n(i.e. functionQ) and its successors which can be written as:\nV\u0019(s) =X\nat2A\u0019(ajs)[R(s;a) +\rX\ns02SP(s0js;a)V\u0019(s0)] (5)\n5\n\nBasic TechniquesModel-based MethodsModel-free MethodsDynamic Programming, Alpha Zero, ‚Ä¶Value-basedPolicy-basedSARSA(On-policy)Q-learning (Off-policy)Policy GradientActor-CriticAdvancedTechniquesData SamplingModelEfficiencyData UtilizationData CorrelationPolicyRewardValuePolicy ExplorationPolicy RepresentationPolicy OptimizationMultiple RewardUnknown RewardValue RepresentationRLDeep Q-learningDDPGFigure 2: Broad categorization of RL techniques.\nQ\u0019(s;a) =X\ns02SP(s0js;a)[R(s;a) +\rX\na02A\u0019(a0js0)Q\u0019(s0;a0)] (6)\nBy iterating the Bellman equations, we can easily obtain the value functions for a policy,\nand to compare policies, we deÔ¨Åne that the policy \u0019is better than \u00190if the functionV\naccording to the \u0019is no less than the function Vaccording to the \u00190for all states, that is\nV\u0019(s)\u0015V\u00190(s);8s. It has been proven in [89] that the existence of the optimal policy \u0019\u0003\nis guaranteed in the MDP problem, where V\u0003(s) = max\u0019V\u0019(s)andQ\u0003(s) = max\u0019Q\u0019(s).\nThese two functions are deÔ¨Åned as the optimal function Vand the optimal function Q. We\ncan obtain the optimal policy \u0019\u0003by maximizing over the Q\u0003(\u0019)which can be written as:\n\u0019\u0003(ajs) = arg maxQ\u0003(s;a) (7)\nTo improve the policy, we apply the Bellman optimality equations [89] to update value\nfunctions by taking the action with maximum value instead of trying all possible actions.\nTofacilitatetheoptimizationofthepolicy, manyRLtechniquesareproposedfromdiÔ¨Äerent\nperspectives, and Figure 2 provides a diagram outlining the broad categorization of these\ntechniques, illustrating how these techniques can be applied.\n2.2 Basic Techniques\nBased on the representation of MDP elements, basic techniques can be categorized into\ntwo classes: model-based method and model-free method. The main diÔ¨Äerence is whether\nthe agent has access to model the environment, i.e. whether the agent knows the transition\nfunction and the reward function. These two functions are already known in the model-\nbased method where Dynamic Programming (DP)[6] and Alpha-Zero [86] are the classical\nmethods which have achieved signiÔ¨Åcant results in numerous applications. In these meth-\nods, agents are allowed to think ahead and plan future actions with known eÔ¨Äects on the\nenvironment. Besides, an agent can learn the optimal policy from the planned experience\nwhich results in high sample eÔ¨Éciency.\n6\n\nIn many RL problems, the reward and the transition function are typically unknown due\nto the complicated environment and its intricate inherent mechanism. For example, as\nillustrated in Figure 1, we are unable to obtain the actual latency as the reward in the\njoint query optimization example. Besides, in the stochastic job scheduling problem [59],\nit is also impossible to directly model the transition function because of the randomness of\nthe job arrivals in the practical scenarios. Hence, in these problems, agents usually employ\nmodel-free methods that can purely learn the policy from the experience gained during the\ninteraction with the environment. Model-free methods can mainly be classiÔ¨Åed into two\ncategories, namelythe value-based methodandthe policy-based method. Inthevalue-based\nmethod, the RL algorithm learns the optimal policy by maximizing the value functions.\nThere are two main approaches in estimating the value functions that are Mento-Carlo\n(MC) methods and Temporal diÔ¨Äerence (TD) methods. MC methods calculate the V(s)\nby directly applying its deÔ¨Ånition, that is Equation 1. MC methods can directly update\nthe value functions once they get a new trajectory \u001cas follows:\nV\u0019(s) V\u0019(s) +\u000b(G\u001c\u0018\u0019(\u001cjs0=s)\u0000V\u0019(s)) (8)\nwhere\u000b2[0;1)denotes the learning rate which controls the rate of updating the policy\nwith new experiences. However, it has an obvious drawback that a complete trajectory\nrequires the agent to reach a terminal state, while it is not practical in some applications,\nsuch as online systems. DiÔ¨Äerent from MC methods, the TD method builds on the recur-\nsive relationship of value functions, and hence, can learn from the incomplete trajectory.\nMathematically, the update of TD methods can be written as:\nV\u0019(s) V\u0019(s) +\u000b(R(s;a) +\rV\u0019(s0)\u0000V\u0019(s)) (9)\nHowever, there is bias when estimating the function Vwith TD methods because they\nlearn from the recursive relationship. To reduce the bias, TD methods can extend the\nlength of the incomplete trajectories and update the function Vby thinking more steps\nahead, which is called n-steps TD methods. As ngrows to the length of whole trajectories,\nMC methods can be regarded as a special case of TD methods where function Vis an\nunbiased estimate. On the other side of the coin, as the length nincreases, the variance of\nthe trajectory also increases. In addition to the above consideration, TD-based methods\nare more eÔ¨Écient and require less storage and computation, thus they are more popular\namong RL algorithms.\nIn value-based methods, we can obtain the optimal policy by acting greedily via Equation\n7. The update of the function Qwith TD methods is similar to the update of the function\nV, and is as follows: Q\u0019(s;a) Q\u0019(s;a) +\u000b(R(s;a) +\rQ\u00190(s0;a0)\u0000Q\u0019(s;a))where the\nagent follows the policy \u0019to take actions and follows the policy \u00190to maximize the function\nQ. If the two policies are the same, that is \u00190=\u0019, we call such RL algorithms the on-policy\nmethods where the SARSA[77] is the representative method. In addition, other policies\ncan also be used in \u00190. For example, in Q-learning [109], the agent applies the greedy\npolicy and updates the function Qwith the maximum value in its successor. Its update\nformula can be written as: Q\u0019(s;a) Q\u0019(s;a)+\u000b(R(s;a)+\rmaxa0Q\u0019(s0;a0)\u0000Q\u0019(s;a)).\nBoth value-based methods can work well without the model of the environment, and Q-\nlearning directly learns the optimal policy, whilst SARSA learns a near-optimal policy\nduring exploring. Theoretically, Q-learning should converge quicker than SARSA, but its\ngenerated samples have a high variance which may suÔ¨Äer from the problems of converging.\nIn RL, storage and computation costs are very high when there is a huge number of states\nor actions. To overcome this problem, DRL, as a branch of RL, adopts Deep Neural\n7\n\nNetwork (DNN) to replace tabular representations with neural networks. For function V,\nDNN takes the state sas input and outputs its state value V\u0012(s)\u0019V\u0019(s)where the\u0012\ndenotes the parameter in the DNN. When comes to function Q, It takes the combination\nof the state sand the action aas input and outputs the value of the state-action pair\nQ\u0012(s;a)\u0019Q\u0019(s;a), As for the neural networks, we can optimize them by applying the\ntechniques that are widely used in deep learning (e.g. gradient descent). Deep Q-learning\nnetwork (DQN) [65], as a representative method in DRL, combines the DNN with Q-\nlearning and its loss function is as follows:\nLw=ED[(R(s;a) +\rmax\na\u00032AQw(s0;a\u0003)\u0000Qw(s;a))2] (10)\nwhereDdenotes the experience replay which accumulates the generated samples and can\nstabilize the training process.\nPolicy-based methods are another branch of the model-free RL algorithm that have a\nclear representation of the policy \u0019(ajs), and they can tackle several challenges that are\nencountered in value-based methods. For example, when the action space is continuous,\nvalue-based methods need to discretize the action which could increase the dimensionality\nof the problem, and memory and computation consumption. Value-based methods learn a\ndeterministic policy that generates the action given a state through an optimal function Q\n(i.e.\u0019(s) =a). However, for policy-based methods, they can learn a stochastic policy (i.e.\n\u0019\u0012(aijs) =pi;P\nipi= 1) as the optimal policy, where pidenotes the probability of taking\nthe actionaigiven a state s, and\u0012denotes the parameters where neural networks can be\nused to approximate the policy. Policy Gradient [90] method is one of the main policy-\nbased methods which can tackle the aforementioned challenges. Its goal is to optimize the\nparameters \u0012by using the gradient ascent method, and the target can be denoted in a\ngeneralized expression:\nr\u0012J(\u0012) =E\u001c\u0018\u0019\u0012[R(\u001c)r\u0019\u0012log\u0019\u0012(ajs)] (11)\nThe speciÔ¨Åc proof process can refer to [89]. Sampling via the MC methods, we will get the\nentire trajectories to improve the policy for the policy-based methods.\nAfter training, the action with higher rewards in expectation will have a higher probability\nto be chosen and vice versa. As for the continuous action, The optimal policy learned\nfrom the Policy Gradient is stochastic which still needs to be sampled to get the action.\nHowever, the stochastic policy still requires lots of samples to train the model when the\nsearch space is huge. Deterministic Policy Gradient (DPG) [87], as an extension of the\nPolicy Gradient, overcomes this problem by using a stochastic policy to perform sampling\nwhile applying deterministic policy to output the action which demands relatively fewer\nsamples.\nBoth value-based methods and policy-based methods have their strengths and weaknesses,\nbut they are not contradictory to each other. Actor-Critic (AC) method, as the integration\nof both methods, divides the model into two parts: actor and critic. The actor part selects\nthe action based on the parameterized policy and the critic part concentrates on evaluating\nthe value functions. DiÔ¨Äerent from previous approaches, AC evaluates the advantage func-\ntionA\u0019(s;a) =Q\u0019(s;a)\u0000V\u0019(s)which reÔ¨Çects the relative advantage of a certain action\nato the average value of all actions. The introduction of the value functions also allows\nAC to update by step through the TD method, and the incorporation of the policy-based\nmethods makes AC be suitable for continuous actions. However, the combination of the\n8\n\ntwo methods also makes the AC method more diÔ¨Écult to converge. Moreover, Deep Deter-\nministic Policy Gradient (DDPG) [49], as an extension of the AC, absorbs the advanced\ntechniques from the DQN and the DPG which enables DDPG to learn the policy more\neÔ¨Éciently.\nIn all the above-mentioned methods, there always exists a trade-oÔ¨Ä between exploring the\nunknown situation and exploiting with learned knowledge. On the one hand, exploiting\nthe learned knowledge can help the model converge quicker, but it always leads the model\ninto a local optimal rather than a globally optimal. On the other hand, exploring unknown\nsituationscanÔ¨Åndsomenewandbettersolutions, butalwaysbeingintheexploringprocess\ncauses the model hard to converge. To balance these two processes, researchers have been\ndevoting much energy to Ô¨Ånding a good heuristics strategy, such as \u000f\u0000greedystrategy,\nBoltzmann exploration (Softmax exploration), upper conÔ¨Ådence bound (UCB) algorithm\n[2], Thompson sampling [92], and so on. Here, we consider the \u000f\u0000greedy, a widely used\nexplorationstrategy,asanexample. \u000f\u0000greedytypicallyselectstheactionwiththemaximal\nQ value to exploit the learned experience while occasionally selecting an action evenly at\nrandom to explore unknown cases. \u000f\u0000greedyexploration strategy with mactions can be\ndenoted as follow:\n\u0019(ajs) =\u001a\u000f=m+ (1\u0000\u000f)a\u0003=argmaxa2AQ(s;a);\n\u000f=m a 6=a\u0003:(12)\n\u000f2[0;1)is an exploration factor. The agent is more likely to select the action at random\nwhen the\u000fis closer to 1, and the \u000fwill be continuously reduced during the training process.\n2.3 Advanced Techniques\nThis section mainly discusses some advanced techniques in RL which focus on eÔ¨Éciently\nusing the limited samples and building sophisticated model structures for better represen-\ntation and optimization. According to the diÔ¨Äerent improvements, they can be broadly\nclassiÔ¨Åed into two parts: data sampling and model eÔ¨Éciency.\n2.3.1 Data Sampling\nDatasamplingisoneofthemostimportantconcernsintrainingtheDRLindataprocessing\nand analytics. In most applications, the sample generation process costs a great amount\nof time and computation resources. For example, a sample may refer to an execution run\nfor workload and repartitioning for the database, which can take about 40 minutes[31].\nHence, to train the model with limited samples, we need to increase data utilization and\nreduce data correlation.\nData utilization : Most DRL algorithms train the optimal policy and sample data at\nthe same time. Instead of dropping samples after being trained, experience replay [50]\naccumulates the samples in a big table where samples are randomly selected during the\nlearning phase. With this mechanism, samples will have a higher utilization rate and a\nlower variance, and hence, it can stabilize the training process and accelerate the training\nconvergence. Samples after several iterations may diÔ¨Äer from the current policy, and hence,\nGrowing-batch [40] can continuously refresh the table and replace these outdated samples.\nInaddition,samplesthatarefarawayfromthecurrentpolicyshouldbepaidmoreattention\nandPrioritized Experience Replay [79] uses TD error as the priority to measure the sample\nimportance, and hence, focus more on learning the samples with high errors. In a nutshell,\n9\n\nwith the experience replay, DRL cannot only stable the learning phase but also eÔ¨Éciently\noptimize the policy with fewer samples.\nData correlation : Strong correlation of training data is another concern that may lead\nthe agent to learn a sub-optimal solution instead of the globally optimal one. Apart\nfrom the experience replay, the mechanism of the distributed environments is another\nresearch direction to alleviate this problem. For example, the asynchronous advantage\nactor-critic (A3C) [64] and Distributed PPO (DPPO) [27] apply multi-threads to build\nmultiple individual environments where multiple agents take actions in parallel, and the\nupdate is calculated periodically and separately which can accelerate the sampling process\nand reduce the data correlation.\n2.3.2 Model EÔ¨Éciency\nRL model with better eÔ¨Éciency is the major driving force of the development of RL, and\nthere are many researchers improving it from three major aspects, namely policy, reward\nfunction, and value function.\nPolicy: The policy-related techniques focus on stably and eÔ¨Äectively learning a compre-\nhensive policy, and the advanced techniques to eÔ¨Éciently learn the policy can be classiÔ¨Åed\ninto three parts in detail, which are policy exploration, policy representation, and policy\noptimization.\na)Policy exploration : Its target is to explore as many actions as possible during the train-\ning process in case the policy will be trapped into the local optimal. For example, entropy\nregularisation [64] adds the entropy of the actions‚Äô probabilities into the loss item which\ncan suÔ¨Éciently explore the actions. Besides, adding noise to the action is another research\ndirection to increase the randomness into policy exploration. The DDPG applies an Orn-\nstein‚ÄìUhlenbeck process [96] to generate temporal noise Nwhich are directly injected into\npolicy.Noisy-Net [22] incorporates the noise into the parameters of neural networks which\nis easy to implement, and it shows a better performance than the \u000f\u0000greedyand entropy\nregularisation methods. Further, Plappert et al. [75] investigate an eÔ¨Äective way to com-\nbine the parameter space noise to enrich the exploratory behaviors which can beneÔ¨Åt both\non-policy methods and oÔ¨Ä-policy methods.\nb)Policy representation : The states in some RL problems are in a huge dimension which\ncauses challenges during the training. To approximate a better policy, a branch of DRL\nmodels improve the policy representation by absorbing convolutional neural networks\n(CNN) into DQN to analyze the data, such as Dueling DQN [108],DRQN[26], and so on.\nIn addition, DRQN also incorporates the LSTM structure to increase the capacity of the\npolicy which is able to capture the temporal information, such as speed, direction.\nc)Policy optimization : The update of the value functions following the Equation 5 and 6\ntends to overestimate the value functions and introduce a bias because they learn estimates\nfrom the estimates. Mnih et al.[66] separate the two estimation process by using two\nsame Q-networks which can reduce the correlation of two estimation processes and hence,\nstabilize the course of training. However, the action with the maximum Q-value may diÔ¨Äer\nbetween two Q-networks which will be hard for convergence. and Double DQN (DDQN)\n[99] alleviate the issue by disaggregating the step of selecting the action and calculating\nthe max Q-value.\nWhen we apply the policy-based RL methods, the learning rate of the policy plays an\nessential role in achieving superior performance. A higher learning rate can always maxi-\n10\n\nmize the improvement on a policy by step, but it also causes the instability of the learning\nphase. Hence, The Trust Region Policy Optimization (TRPO) [80] builds constraints on\nthe old policy and new policy via KL divergence to control the change of the policy in\nan acceptable range. With this constraint, TRPO can iteratively optimize the policy via\na surrogate objective function which can monotonically improve policies. However, the\ndesign of the KL constraint makes it hard to be trained, and Proximal Policy Optimization\n(PPO) [81] simpliÔ¨Åes the constraint through two ways: adding it into the objective func-\ntion, designing a clipping function to control the update rate. Empirically, PPO methods\nare much simpler to implement and are able to perform at least as well as TRPO.\nReward : RewardfunctionasoneofthekeycomponentsintheMDPplaysanessentialrole\nin the RL. In some speciÔ¨Åc problems, the agent has to achieve multiple goals which may\nhave some relationships. For example, the robot can only get out through the door only\nif it has already found the key. To tackle this challenge, Hierarchical DQN [38] proposes\ntwo levels of hierarchical RL (HRL) models to repeatedly select a new goal and achieve the\nchosen goal. However, there is a limitation that the goal needs to be manually predeÔ¨Åned\nwhich may be unknown or unmeasurable in some environments, such as the market and the\neÔ¨Äect of a drug. To overcome it, Inverse RL (IRL) [68] learns the rewards function from\nthe given experts‚Äô demonstrations (i.e. the handcraft trajectories), but the agent in IRL\ncan only prioritize the entire trajectories over others. It will cause a shift when the agent\ncomes to a state that never appears before, and Generative Adversarial Imitation Learning\n(GAIL) [32], as an imitation learning algorithm, applies adversarial training methods to\ngenerate fake samples and is able to learn the expert‚Äôs policy explicitly and directly.\nValue: As we have mentioned earlier, the tabular representation of the value functions\nhas several limitations which can be alleviated via DRL. DiÔ¨Äerent from directly taking the\nstate-action pair as the input to calculate the Q-function, Dueling DQN [108] estimates\nits value by approximating two separate parts that are the state-values and the advantage\nvalues, and hence, can distinguish whether the value is brought by the state or the action.\nThe aforementioned advanced algorithms and techniques improve and enhance the DRL\nfrom diÔ¨Äerent perspectives, which makes DRL-based algorithms be a promising way to\nimprove data processing and analytics. We observe that problems with the following char-\nacteristics may be amenable to DRL-based optimization. First, problems are incredibly\ncomplex and diÔ¨Écult. The system and application involve a complicated operational en-\nvironment (e.g., large-scale, high-dimensional states) and internal implementation mecha-\nnisms, which is hard to construct a white-box model accurately. DRL can process complex\ndata and learn from experience generated from interacting, which is naturally suitable for\ndata processing and analytics where many kinds of data exist and are processed frequently.\nSecond, the optimization objectives can be represented and calculated easily as the reward\nbecause the RL agent improves itself towards maximizing the rewards and rewards could\nbe computed a lot of times during training. Third, the environment can be well described\nas MDP. DRL has been shown to solve MDP with theoretical guarantees and empirical\nresults. Thus, problems involving sequential decision making such as planning, scheduling,\nstructure generation (e.g., tree, graph), and searching could be expressed as MDP and a\ngood Ô¨Åt for DRL. Fourth, collecting required labels of data massively is hard. Compared\nto supervised learning, DRL can utilize data eÔ¨Éciently to gain good performance.\n11\n\n3 Data System Optimizations\nDRLlearnsknowledgeaboutthesystembyinteractingwithitandoptimizesthesystem. In\nthis section, we focus on several fundamental aspects with regards to system optimization\nin data processing and analytics including data organization, scheduling, tuning, indexing,\nquery optimization, and cache management. We discuss how each problem is formulated in\nMDP by deÔ¨Åning three key elements (action, state, and reward) in the system and solved\nby DRL. Generally, the states are deÔ¨Åned by some key characteristics of the system. The\nactions are possible decisions (e.g., system conÔ¨Åguration), that aÔ¨Äect the system perfor-\nmance and the reward is calculated based on the performance metrics (e.g. throughput,\nlatency). Table 1 presents a summary of representative works and the estimated dimension\nranges on the state and action space of each work are added as signals on the DRL train-\ning diÔ¨Éculty. As a comparison, OpenAI Five[8], a Dota-playing AI, observes the state as\n20,000 numbers representing useful game information and about 1,000 valid actions (like\nordering a hero to move to a location) for per hero. Dota is a real-time strategy game\nbetween two teams of Ô¨Åve players where each player controls a character called a ‚Äúhero‚Äù.\n3.1 Data Organization\n3.1.1 Data Partitioning\nEÔ¨Äective data partitioning strategy is essential to accelerate data processing and analytics\nby skipping irrelevant data for a given query. It is challenging as many factors need to be\nconsidered, including the workload and data characteristics, hardware proÔ¨Åles, and system\nimplementation.\nIn data analytics systems, data is split into blocks in main memory or secondary storage,\nwhich are accessed by relevant queries. A query may fetch many blocks redundantly and,\ntherefore, aneÔ¨Äectiveblocklayoutavoidsreadingunnecessarydataandreducesthenumber\nof block accesses, thereby improving the system performance. Yang et al.[116] propose a\nframework called the qd-tree that partitions data into blocks using DRL over the analytical\nworkload. The qd-tree resembles the classic k-d tree and describes the partition of multi-\ndimensional data space where each internal node splits data using a particular predicate\nand represents a subspace. The data in the leaf node is assigned to the same block. In\nthe MDP, each state is a node representing the subspace of the whole data and featured\nas the concatenation of range and category predicates. After the agent takes an action to\ngenerate two child nodes, two new states will be produced and explored later. The available\naction set is the predicates parsed from workload queries. The reward is computed by\nthe normalized number of skipped blocks over all queries. They do not execute queries\nand a sampling technique is used to estimate the reward eÔ¨Éciently. The formulation\nof using DRL to learn a tree is similar to NeuroCuts[47] that learns a tree for packet\nclassiÔ¨Åcation. However, the qd-tree may not support a complex workload containing user-\ndeÔ¨Åned functions (UDFs) queries.\nHorizontal partitioning in the database chooses attributes of large tables and splits them\nacross multiple machines to improve the performance of analytical workloads. The design\nrelies on either the experience of database administrators (DBAs) or cost models that are\noften inaccurate[42] to predict the runtime for diÔ¨Äerent partitions. Data collection is too\nchallenging and costly to train the accurate supervised learning model in the cloud environ-\nment. Hilprecht et al.[31] learn to partition using DRL on analytical workloads in cloud\ndatabases, on the fact that DRL is able to eÔ¨Éciently navigate the partition search and\n12\n\nrequires less training data. In the MDP, the state consists of two parts. The database part\nencodes whether a table is replicated, an attribute is used for partitioning, and which tables\nare co-partitioned. The workload part incorporates normalized frequencies of representa-\ntive queries. Supported actions are: partitioning a table using an attribute, replicating a\ntable, and changing tables co-partition. The reward is the negative of the runtime for the\nworkload. One challenge is that the cost of database partitioning is high during training.\nTo alleviate the problem, the agent is trained in the simulation environment and is further\nreÔ¨Åned in the real environment by estimating the rewards using sampling. One limitation\nis that it may not support new queries well because only the frequency features of queries\nare considered. Durand et al. in [17, 18] utilize DRL to improve vertical partitioning that\noptimizes the physical table layout. They show that the DQN algorithm can easily work\nfor a single workload with one table but is hard to generalize to random workloads.\nFor UDFs analytics workloads on unstructured data, partitioning is more challenging where\nUDFs could express complex computations and functional dependency is unavailable in\nthe unstructured data. Zou et al.[127] propose the Lachesis system to provide automatic\npartitioning for non-relational data analytics. Lachesis translates UDFs to graph-based in-\ntermediate representations (IR) and identiÔ¨Åes partition candidates based on the subgraph\nof IR as a two-terminal graph. Lachesis adopts DRL to learn to choose the optimal candi-\ndate. ThestateincorporatesfeaturesforeachpartitionextractedfromhistoricalworkÔ¨Çows:\nfrequency, the execution interval, time of the most recent run, complexity, selectivity, key\ndistribution, number, and size of co-partition. In addition, the state also incorporates other\nfeatures such as hardware conÔ¨Ågurations. The action is to select one partition candidate.\nThe reward is the throughput speedup compared to the average throughput of the histor-\nical executions of applications. To reduce the training time, the reward is derived from\nhistorical latency statistics without partitioning the data when running the applications.\nOne limitation is that Lachesis largely depends on historical statistics to design the state\nand calculate the reward, which could lead to poor performance when the statistics are\ninadequate.\n3.1.2 Data Compression\nData compression is widely employed to save storage space. The eÔ¨Äectiveness of a compres-\nsion scheme however relies on the data types and patterns. In time-series data, the pattern\ncan change over time and a Ô¨Åxed compression scheme may not work well for the entire\nduration. Yu et al.[120] propose a two-level compression framework, where a scheme space\nis constructed by extracting global features at the top level and a compression schema is se-\nlected for each point at the bottom level. The proposed AMMMO framework incorporates\ncompression primitives and the control parameters, which deÔ¨Åne the compression scheme\nspace. Due to the fact that the enumeration is computationally infeasible, the framework\nproposes to adopt DRL to Ô¨Ånd the compression scheme. The agent takes a block that\nconsists of 32 data points with the compressed header and data segment, timestamps, and\nmetrics value as the state. The action is to select a scheme from compression scheme space\nand then the compression ratio is computed as the reward. The limitation is that the\nmethod may not work for other data types like images and videos.\n3.2 Scheduling\nScheduling is a critical component in data processing and analytics systems to ensure\nthat resources are well utilized. Job scheduling in a distributed computing cluster faces\nmany challenging factors such as workload (e.g., job dependencies, sizes, priority), data\n13\n\nTable 1: Representative Works using DRL for Data System Optimizations. D(X) denotes\nthe approximate dimension of X space.\nDomain Work Algorithm D(State) D(Action) DRL-based Ap-\nproachOpen\nSource\nData\norganiza-\ntionAnalytical system\ndata partition[116]PPO 10 - 100 100 - 1000 Exploit workload pat-\nternsandGeneratethe\ntreeNO\nDatabase horizon-\ntal partition [31]DQN 100 10 Navigate the partition\nsearch eÔ¨ÉcientlyNO\nUDF-centric work-\nload data parti-\ntion [127]A3C 10 1-10 Exploit the features of\npartition and searchYES\nTime series data\ncompression [120]PG 100 10 Search parameters in-\nteractivelyNO\nScheduling Distributed job\nprocessing [58]PG 100 10 Exploit the job de-\npendencies and learn\nschedule decisionYES\nDistributed stream\ndata [45]DDPG 100 10-100 Learn schedule deci-\nsionNO\nTuning Database conÔ¨Ågu-\nration [123] [44]DDPG 100 10 Search conÔ¨Åguration\nparameters interac-\ntivelyYES\nIndex Index Selec-\ntion [84]CEM 100 10 Search the index inter-\nactivelyNO\nR-tree construc-\ntion [24]DQN 10-100 10 Learn to generate the\ntreeNO\nQuery\nOpti-\nmizationJoin order selec-\ntion [61, 37, 119,\n29]PG, DQN,\n...10-100 1-10 Learn to decide the\njoin orderOnly[29]\nCache\nManage-\nmentView Materializa-\ntion [121]DQN 100 10 Model the problem as\nIIP and solveNO\nlocality, and hardware characteristics. Existing algorithms using general heuristics such\nas shortest-job-Ô¨Årst do not utilize these factors well and fail to yield top performance.\nTo this end, Mao et al.[58] propose Decima to learn to schedule jobs with dependent\nstages using DRL for data processing clusters and improve the job completion time. In\nthe data processing systems such as Hive[93], Pig[70], Spark-SQL[1], jobs could have up\nto hundreds of stages and many stages run in parallel, which are represented as directed\nacyclic graphs (DAGs) where the nodes are the execution stages and each edge represents\nthe dependency. To handle parallelism and dependencies in job DAGs, Decima Ô¨Årst applies\ngraph neural network (GNN) to extract features as the state instead of manually designing\nthem while achieving scalability. Three types of feature embeddings are generated. Node\nembedding captures information about the node and its children including the number of\nremaining tasks, busy and available executors, duration, and locality of executors. Job\nembedding aggregates all node embeddings in the job and cluster embedding combines job\nembeddings. To balance possible large action space and long action sequences, The action\ndetermines the job stage to be scheduled next and the parallelism limit of executors. The\nreward is based on the average job completion time. To train eÔ¨Äectively in a job streaming\nenvironment, Decima gradually increases the length of training jobs to conduct curriculum\nlearning[7]. Thevariancereductiontechnique[59]isappliedtohandlestochasticjobarrivals\nfor robustness. However, we note that Decima is non-preemptive and does not re-schedule\nfor higher priority jobs.\n14\n\nIn distributed stream data processing, streams of continuous data are processed at scale\nin a real-time manner. The scheduling algorithm assigns workers to process data where\neach worker uses many threads to process data tuples and aims to minimize average data\ntuple processing time. Li et al.[45] design a scheduling algorithm using DRL for distributed\nstream data processing, which learns to assign tuples to work threads. The state consists of\nthe scheduling plan (e.g., the current assignment of workers) and the workload information\n(e.g., tuple arrival rate). The action is to assign threads to machines. The reward is the\nnegative tuple processing time on average. The work shows that DQN does not work\nwell because the action space is large and applies DDPG to train the actor-critic based\nagent instead. To Ô¨Ånd a good action, the proposed method looks for k nearest neighbors\nof the action that the actornetwork outputs and selects the neighbor with the highest\nvalue that the criticnetwork outputs. The algorithm is implemented on Apache Storm\nand evaluated with representative applications: log stream processing, continuous queries,\nand word count.\nMany works have been recently proposed to improve scheduling using DRL[122, 35]. Query\nscheduling determines the execution order of queries, which has a great inÔ¨Çuence on query\nperformance and resource utilization in the database system. SmartQueue[122] improves\nquery scheduling by leveraging overlapping data access among queries and learns to im-\nprove cache hits using DRL. In addition, Tim et al.[35] design a scheduling system in\nSageDB using RL techniques. Other works using RL for scheduling include Bayesian RL\nfor scheduling in heterogeneous clusters[3], operation scheduling in devices[23], application\ncontainer scheduling in clusters[102], etc.\n3.3 Tuning\nTuning the conÔ¨Åguration of data processing and analytic systems plays a key role to im-\nprove system performance. The task is challenging because up to hundreds of parameters\nand complex relations between them could exist. Furthermore, other factors such as hard-\nware and workload also impact the performance. Existing works often employ search-based\nor supervised learning methods. The former takes much time to get an acceptable conÔ¨Ågu-\nration and the latter such as OtterTune[98] needs large high-quality data that is non-trivial\nto obtain in practice. Zhang et al.[123] design a cloud database tuning system CDBTune\nusing DRL to Ô¨Ånd the best parameter in high-dimensional conÔ¨Åguration space. The CDB-\nTune formulates MDP as follows. The state is represented by the internal metrics (e.g.,\nbuÔ¨Äer size, pages read). The action is to increase or decrease the knob values. The reward\nis the performance diÔ¨Äerence between two states, which is calculated using throughput and\nlatency. CDBTune takes several hours on oÔ¨Ñine training in simulation and online training\nin the real environment. Compared to OtterTune, CDBTune eases the burden of collect-\ning large training data sets. In the experiments, CDBTune is shown to outperform DBA\nexperts and OtterTune and improve tuning eÔ¨Éciency under 6 diÔ¨Äerent workloads on four\ndatabases. One limitation of the approach is that the workload information is ignored and\nthus it may not perform well when the query workload is changed.\nTo address the issue, Li et al.[44] propose QTune that considers query information to tune\nthe database using DRL. First, Qtune extracts features from SQL query including types\n(e.g., insert, delete), tables, and operation (e.g., scan, hash join) costs estimated by the\ndatabase engine. The columns attributes and operations like selection conditions in the\nquery are ignored. Subsequently, Qtune trains a DNN model to predict the diÔ¨Äerence\nof statistics (e.g., updated tuples, the number of committed transactions) in the state\nafter executing the queries in the workload and updates the state using it. The action\n15\n\nand reward design are similar to CDBTune. Additionally, QTune supports three levels of\ntuning granularity for balancing throughput and latency. For query-level, QTune inputs\nquery vector and tries to Ô¨Ånd good knobs for each query. For workload-level, vectors for\nall queries are merged and used. For cluster-level, QTune employs a clustering method\nbased on deep learning to classify queries and merge queries into clusters. One drawback\nof QTune is that the query featurization could lose key information such as query attributes\n(i.e., columns) and hurt the performance especially when the cost estimation is inaccurate.\nThe prediction model for state changes is trained alone and needs accurate training data.\nAn end-to-end training framework is therefore essential and a good direction to undertake.\n3.4 Indexing\n3.4.1 Database Index Selection\nDatabase index selection considers which attributes to create an index to maximize query\nperformance. Sharma et al.[84] show how DRL can be used to recommend an index based\non a given workload. The state encodes selectivity values for workload queries and columns\nin the database schema and current column indexes. The action is to create an index on\na column. The reward is the improvement compared to the baseline without indexes. The\nexperiments show that the approach can perform as well or better as having indexes on\nall columns. Sadri et al.[78] utilize DRL to select the index for a cluster database where\nboth query processing and load balancing are considered. Welborn et al.[110] optimize the\naction space design by introducing task-speciÔ¨Åc knowledge for index selection tasks in the\ndatabase. However, these works only consider the situation where single-column indexes\nare built. Lan et al.[39] propose both single-attribute and multi-attribute indexes selection\nusing DRL. Five rules are proposed to reduce the action and state space, which help the\nagent learn eÔ¨Äective strategy easier. The method uses what-if caller[9] to get the cost\nof queries under speciÔ¨Åc index conÔ¨Ågurations without building indexes physically. These\nworks conduct basic experiments with small and simple datasets. Extensive and large-scale\nexperiments using real datasets are therefore needed to benchmark these methods to ensure\nthat they can scale well.\n3.4.2 Index Structure Construction\nThe learned index is proposed recently as an alternative index to replace the B+-Tree\nand bloom Ô¨Ålter by viewing indexes as models and using deep learning models to act as\nindexes[36]. DRL can enhance the traditional indexes instead of replacing them.\nHierarchical structures such as the B+-tree and R-tree are important indexing mechanisms\nto locate data of interest eÔ¨Éciently without scanning a large portion of the database.\nCompared to the single dimensional counterpart, the R-tree is more complex to optimize\ndue to bounding box eÔ¨Éciency and multi-path traversals. Earlier conventional approaches\nuse heuristics to determine these two operations (i.e. choosing the insertion subtree and\nsplitting an overÔ¨Çowing node) during the construction of the R-tree[71]. Gu et al.[24]\npropose to use DRL to replace heuristics to construct the R-tree and propose the RLR-tree.\nThe approach models two operations ChooseSubtree and Split as two MDPs respectively\nand combines them to generate an R-Tree. For ChooseSubtree, the state is represented as\nthe concatenation of the four features (i.e., area, perimeter, overlap, occupancy rate) of\neach selected child node. More features are evaluated but do not improve the performance\nin the reported experiments. The action is to select a node to insert from top-k child nodes\nin terms of the increase of area. The reward is the performance improvement from the\n16\n\nRLR-tree. For Split MDP, the state is the areas and perimeters of the two nodes created\nby all top-k splits in the ascending order of total area. The action is to choose one split rule\nfrom k rules and the reward is similar to that of ChooseSubtree. The two agents are trained\nalternately. As expected, the optimizations render the RLR-tree improved performance in\nrange and KNN queries.\nGraphs can be used as eÔ¨Äective indexes to accelerate nearest neighbors search[55, 15].\nExisting graph construction methods generally propose diÔ¨Äerent rules to generate graphs,\nwhich cannot provide adaptivity for diÔ¨Äerent workloads[104]. Baranchuk et al.[5] employ\nDRL to optimize the graph for nearest neighbors search. The approach learns the proba-\nbilities of edges in the graph and tries to maximize the search eÔ¨Éciency. It considers the\ninitial graph and the search algorithm as the state. The action is to keep an edge or not.\nThe reward is the performance for search. It chooses the TRPO[80] algorithm to train.\nThe reported experimental results show that the agent can reÔ¨Åne state-of-the-art graphs\nand achieve better performance. However, this approach does not learn to explore and add\nnew edges to the initial graph that may aÔ¨Äect the performance.\nSearchingandconstructinganewindexstructureisanotherlineofinterestingresearch[33].\nInspired by Neural Architecture Search (NAS)[126], Wu et al.[112] propose an RNN-based\nneural index search (NIS) framework that employs DRL to search the index structures\nand parameters given the workload. NIS can generate tree-like index structures layer by\nlayer via formalizing abstract ordered blocks and unordered blocks, which can provide a\nwell-designed search space. The keys in the ordered block are sorted in ascending order,\nand the skip list or B+-Tree can be used. The keys in the unordered block are partitioned\nusing customized functions and the hash bucket can be used. Overall, the whole learning\nprocess is similar to that of NAS.\n3.5 Query Optimization\nQuery optimization aims to Ô¨Ånd the most eÔ¨Écient way to execute queries in database\nmanagement systems. There are many diÔ¨Äerent plans to access the query data that can\nhave a large processing time variance from seconds to hours. The performance of a query\nplan is determined mostly by the table join orders. Traditionally, query optimizers use\ncertain heuristics combined with dynamic programming to enumerate possible eÔ¨Écient\nexecution plans and evaluate them using cost models that could produce large errors[42].\nMarcus et al.[61] propose Rejoin that applies DRL to learn to select better join orders\nutilizing past experience. The state encodes join tree structure and join predicates. The\naction is to combine two subtrees, where each subtree represents an input relation to join.\nThe reward is assigned based on the cost model in the optimizer. The experiments show\nthat ReJOIN can match or outperform the optimizer in PostgreSQL. Compared to ReJoin,\nDQ[37] presents an extensible featurization scheme for state representation and improves\nthe training eÔ¨Éciency using the DQN[65] algorithm. Heitz et al.[29] compare diÔ¨Äerent RL\nalgorithms including DQN[65], DDQN[49], and PPO[81] for join order optimization and\nuse a symmetric matrix to represent the state instead of vector. Yu et al.[119] introduce a\ngraph neural network (GNN) with DRL for join order selection that replaces Ô¨Åxed-length\nhand-tuned vector in Rejoin[61] and DQ[37] with learned scalable GNN representation and\nbetter captures and distinguishes the join tree structure information. These works mainly\ndiÔ¨Äer in encoding what information and how to encode them.\nInstead of learning from past query executions, Trummer et al.[94] propose SkinnerDB to\nlearn from the current query execution status to optimize the remaining execution of a\n17\n\nTable 2: Methods of query optimization.\nMethod Techniques Training Workload\nAdaptivity\nRejoin[61],\nDQ[37]learn from execu-\ntion experienceHigh Low\nSkinnerDB [94] learn from current\nexecution statusMedium Medium\nBao[60] learn to choose ex-\nisting optimizersLow High\nquery using RL. SpeciÔ¨Åcally, SkinnerDB breaks the query execution into many small time\nintervals (e.g., tens to thousands of slices per second) and processes the query adaptively.\nAt the beginning of each time interval, the RL agent chooses the join order and measures\nthe execution progress. SkinnerDB adopts a similar adaptive query processing strategy\nin Eddies[95] and uses the UCT algorithm[34], which provides formal guarantees that the\ndiÔ¨Äerence is bounded between the rewards obtained by the agent and those by optimal\nchoices. The reward is calculated by the progress for the current interval. A tailored exe-\ncution engine is designed to fully exploit the learning strategy with tuple representations\nand specialized multi-way join algorithms. SkinnerDB oÔ¨Äers several advantages. First, it\nis inherently robust to query distribution changes because its execution only depends on\nthe current query. Second, it relies on less assumption and information (e.g., cardinality\nmodels) than traditional optimizers and thus is more suitable for the complicated environ-\nment where cardinality is hard to estimate. Third, it predicts the optimal join order based\non real performance. However, it may introduce overhead caused by join order switching.\nLearning-based methods that have been proposed to replace traditional query optimizers\noften incur a great deal of training overhead because they have to learn from scratch. To\nmitigate the problem, Bao [60] (the Bandit optimizer)) is designed to take advantage of\nthe existing query optimizers. SpeciÔ¨Åcally, Bao learns to choose the best plan from the\nquery plan candidates provided by available optimizers by passing diÔ¨Äerent Ô¨Çags or hints\nto them. Bao transforms query plan trees into vectors and adopts a tree convolutional\nneural network to identify patterns in the tree. Then it formulates the choosing task as a\ncontextual multi-armed bandit problem and uses Thompson sampling[92] to solve it. Bao\nis a hybrid solution for query optimization. It achieves good training time and is robust\nto changes in workload [60].\n3.6 Cache Management\n3.6.1 View Materialization\nView materialization is the process of deciding which view, i.e., results of query or sub-\nquery, to cache. In database systems, a view is represented as a table and other queries\ncould be accelerated by reading this table instead of accessing the original tables. There is\nan overhead of materializing and maintaining the view when the original table is updated.\nExisting methods are based on heuristics, which either rely on simple Least-Recently-Used\nrule or cost-model based approaches[74]. The performance of these approaches is limited\nbecause feedback from the historical performance of view materialization is not incorpo-\nrated. Liang et al.[48] implement Deep Q-Materialization (DQM) system that leverages\nDRL to improve the view materialization process in the OLAP system. First, DQM ana-\nlyzes SQL queries to Ô¨Ånd candidate views for the current query. Second, it trains a DRL\nagent to select from the set of candidates. Third, it uses an eviction policy to delete the\n18\n\nmaterialized views. In the MDP, the state encodes view state and workload information.\nThe action is to create the view or do nothing. The reward is calculated by the query time\nimprovement minus amortized creation cost. Additionally, the eviction policy is based on\ncredit and it evicts the materialized view with the lowest score.\nYuan et al.[121] present a diÔ¨Äerent way that use DRL to automate view generation and\nselect the most beneÔ¨Åcial subqueries to materialize. First, the approach uses a DNN to\nestimate the beneÔ¨Åts of a materialized view where features from tables, queries, and view\nplansareextracted. ThentheapproachmodelsselectionasanIntegerLinearProgramming\n(IIP) problem and introduce an iterative optimization method to Ô¨Ågure it out. However,\nthe method cannot guarantee convergence. To address the issue, the problem is formulated\nas the MDP. The state encodes the subqueries that are selected to materialize and status\nif queries use these materialized views. The action is to choose the subquery to materialize\nor not. The reward is the diÔ¨Äerence between beneÔ¨Åt changes of two states. Both cost\nestimation and view selection models are trained oÔ¨Ñine using the actual cost of queries\nand beneÔ¨Åts. Then the cost estimation model is used for the online recommendation for\nview materialization. Performance study shows its good performance; However, it lacks a\ncomparison with DQM.\n3.6.2 Storage\nCache management impacts the performance of computer systems with hierarchical hard-\nware structures directly. Generally, a caching policy considers which objects to cache, to\nevict when the cache is full to maximize the object hit rate in the cache. In many sys-\ntems, the optimal caching policy depends on workload characteristics. Phoebe[111] is the\nRL-based framework for cache management for storage models. The state encodes the\ninformation from a preceding Ô¨Åxed-length sequence of accesses where for each access, nine\nfeatures are extracted including data block address, data block address delta, frequency,\nreuse distance, penultimate reuse distance, average reuse distance, frequency in the sliding\nwindow, the number of cache misses, and a priority value. The action is to set a priority\nvalue ranging within [\u00001;1]to the data. The reward is computed from if the cache is hit\nor missed and values are 1 and -1 respectively. It applies the DDPG algorithm to train\nthe agent. Periodical training is employed to amortize training costs in online training. In\nnetwork systems, one issue is that the reward delay is very long in systems with a large\ncache, i.e., CDN cache can host up to millions of objects. Wang et al.[100] propose a\nsubsampling technique by hashing the objects to mitigate the issue when applying RL on\ncaching systems.\n4 Data Analytics Applications\nIn this section, we shall discuss DRL applications from the perspective of data processing\nand data analytics. These two categories of DRL applications form indispensable parts\nof a pipeline, in which data processing provides a better basis for data analytics. In\naddition, these two categories share some overlapping topics, making these topics mutually\nmotivating and stimulating. We have summarized the technical comparisons of diÔ¨Äerent\napplications in Table 3. We shall Ô¨Årst discuss DRL applications in data preparation and\nthen in data analytics.\n19\n\nTable 3: Representative works for RL applications. D(X) denotes the approximate dimen-\nsion of X space.\nDomain Work Algorithm D(State) D(Action) DRL-based Ap-\nproach\nData pro-\ncessingEntity matching[11,\n20]PG 100 -\n1000100 - 1000 Select target entity from\nthe candidate entities\napplication Database interac-\ntion with natural\nlanguage [125, 14]PG 100 -\n1000100 - 1000 Learn to generate the\nquery\nFeature engineer-\ning [52]DQN 100 1-10 Select features and\nmodel feature correla-\ntions in states\nExploratory data\nanalysis [4]A3C 10-100 100000 Learn to query a dataset\nfor key characteristics\nAbnormal detec-\ntion [69]IRL 1-10 1-10 Learn the reward\nfunction for normal\nsequences\nAutoML pipeline\ngeneration [28]DQN 10 100 Learn to select modules\nof a pipeline\nHealthcare Treatment recom-\nmendation [103]DDPG 10 100-1000 Select treatment from\ncandidate treatments\nDiagnostic infer-\nence [51]DQN 100-1000 1-10 Learn diagnostic deci-\nsion\nHospital resource al-\nlocation [19]DDPG 100 1000-\n10000Learn resource schedul-\ning\nFintech Portfolio optimiza-\ntion [12]Q-\nLearning100 100 Select the portfolio\nweights for stocks\nTrading [115, 114] IRL 1-10 10 Learn the reward func-\ntion of trading behaviors\nFraud detection [114] IRL 100 10-100 Learn the reward func-\ntion of trading behaviors\nE- Online advertis-\ning [124]DQN 1-10 1-10 Learntoschedulethead-\nvertisements\nCommerce Online recommenda-\ntion [10]DQN 100 10000 Learn to schedule recom-\nmendations\nSearch results aggre-\ngation [91]DQN 10-100 10-100 Learn to schedule search\nresults\nOthers User proÔ¨Åling [105] DQN 100-1000 1000-\n10000Select users‚Äô next activi-\nties by modeling spatial\nsemantics\nSpammer detec-\ntion [16]PG 100 100 Search for the detec-\ntor by interacting with\nspammers\nTransportation [83] PG 1000-\n100001000 Learn to schedule trans-\nportation\n20\n\n4.1 Data Preparation\n4.1.1 Entity Matching\nEntity matching is a data cleaning task that aligns diÔ¨Äerent mentions of the same entity\nin the context. Clark et al. [11] identify the issue that the heuristic loss function cannot\neÔ¨Äectively optimize the evaluation metric B3, and propose using reinforcement learning to\ndirectly optimize the metric. The problem is formulated as a sequential decision problem\nwhere each action is performed on one mention of a document. The action maps the\nmention to an entity in the database at each step by a mention ranking model. Then\nthe reward is calculated using the evaluation metric B3. This work originally proposes\nscaling each action‚Äôs weight by measuring its impact on the Ô¨Ånal reward since each action\nis independent. However, this work does not consider the global relations between entities.\nFang et al. [20] propose a reinforcement learning framework based on the fact that an\neasier entity will create a better context for the subsequent entity matching. SpeciÔ¨Åcally,\nboth local and global representations of entity mentions are modeled and a learned policy\nnetworkisdevisedtochoosefromthenextaction(i.e., whichentitytorecognize). However,\nthe selection of the easier entity to learn the context could be less powerful than context\nmodeling with more recent techniques in NLP such as the transformer.\n4.1.2 Database Interaction With Natural Language\nTo facilitate query formulation for relational databases, there have been eÔ¨Äorts in gener-\nating SQL queries from various other means that do not require knowledge of SQL and\nschema. Zhong et al. [125] propose to generate SQL from a natural language using Rein-\nforcement Learning. For queries formed by a natural language, the model Seq2SQL will\nlearn a policy transforming the queries into SQL queries. The transformed queries will\nthen be executed in the database system to get results. The results will be compared with\nthe ground truth to generate RL rewards. Earlier work [14] using generic autoencoder\nmodel for semantic parsing with Softmax as the Ô¨Ånal layer may generate unnecessarily\nlarge output spaces for SQL query generation tasks. Thus the structure of SQL is used\nto prune the output space of query generating and policy-based reinforcement learning to\noptimize the part which cannot be optimized by cross-entropy. However, RL is observed\nto have limited performance enhancement by [113] due to unnecessary modeling of query\nserialization.\nEÔ¨Éciently querying a database of documents is a promising data processing application.\nKarthik et al. [67] propose collecting evidence from external sources of documents to boost\nextraction accuracy to original sources where data might be scarce. The problem is formu-\nlated as an MDP problem, where each step the agent needs to decide if current extracted\narticles are accepted and stop querying, or these articles are rejected and more relevant\narticles are queried. Both data reconciliation (from original sources) and data retrieval\n(from external sources) are represented as states. Extraction accuracy and penalties for\nextra retrieval actions are reÔ¨Çected in the reward function.\n4.1.3 Feature Engineering\nFeature engineering can be formulated as a single-agent reinforcement learning problem to\nsearch for an optimal subset of features in a large space: the agent selects one feature at\neach action step. The state is the current feature subspace. A reward is assigned to the\nagent based on the predictive performance of the current features subset. Liu et al. [52]\n21\n\npropose a method to reformulate feature engineering as a multi-agent reinforcement learn-\ning problem. The multi-agent RL formulation reduces the large action space of a single\nagent since now each of the agents has a smaller action space for one feature selection.\nHowever, this formulation also brings challenges: interactions between agents, representa-\ntion of the environment, and selection of samples. Three technical methods in [52] have\nbeen proposed to tackle them respectively: adding inter-feature information to reward for-\nmulation, using meta statistics, and deep learning methods to learn the representation of\nthe environment, and Gaussian mixture to independently determine samples. However,\nalthough this formulation reduces the action space, the trade-oÔ¨Ä is using more computing\nresources to support more agents‚Äô learning. Also, the method is diÔ¨Écult to scale to a large\nfeature space.\n4.1.4 Exploratory Data Analysis\nExploratory data analysis (EDA) is useful for users to understand the characteristics of a\nnew dataset. In [4], the problem is formulated as a MDP. The action space is the combina-\ntion of a Ô¨Ånite set of operators and their corresponding parameters to query a dataset. The\nresult of a query shows the characteristics of the dataset. The characteristics are modeled\nas the state, which is represented by descriptive statistics and recent operators. The re-\nward signal measures the interestingness, diversity, and coherency of the characteristics by\nan episode of EDA operations. DRL is applied to the non-diÔ¨Äerential signals and discrete\nstates in MDP. However, challenges arise when applying deep reinforcement learning given\na large number of possible actions as parameterized operations (i.e., for each type of opera-\ntion, the corresponding possible action is the Cartesian product of all parameters‚Äô possible\nvalues). In [4], a two-fold layer architecture is proposed to replace a global softmax layer\ninto two local layers, which eÔ¨Äectively reduces the intractable large numbers of actions.\nHowever, the global interactions of operations and attributes are not considered.\n4.1.5 Abnormal Detection\nAbnormal detection is important for high-stake applications such as healthcare (e.g., pre-\ndicting patients‚Äô status) and Ô¨Åntech (e.g., Ô¨Ånancial crime). Based on the assumptions,\nthere are two approaches to this problem. One approach models the dynamics in the unla-\nbeled datasets as a sequential decision process where the agent performs an action on each\nobservation. Oh et al. [69] propose to use IRL to learn a reward function and a Bayesian\nnetwork to estimate a conÔ¨Ådence score for a potential abnormal observation. To achieve\nthis, the prior distribution of the reward function is assumed. Then a reward function is\nsampled from the distribution to determine the sample generating policy, which generates\nsample background trajectories. As explained by the reward part of Section 2.3.2, experts‚Äô\ntrajectories are observed. With these experts‚Äô trajectories and sample background trajec-\ntories, the parameters of the reward function are updated and thus the policy is improved.\nThe sequence of actions is the input into the neural network. This network is trained\nto learn the normal pattern of a targeted agent and to predict if the next observation is\nabnormal or not. However, this approach relies too much on mining unlabeled datasets\nand ignores the labeled dataset. To address this issue, another approach also uses DRL\nbut focus on the Exploit-Explore trade-oÔ¨Ä on both unlabeled and labeled dataset. Pang et\nal. [73] propose a DRL model with a sampling function to select data instances from both\nthe unlabeled and labeled dataset. This sampling function helps the DRL model to exploit\nthe scarce but useful labeled anomaly data instances and to explore the large unlabeled\ndataset for novel anomaly data instances. Thus, more anomaly data instances are selected\n22\n\nto train the DRL model with better model capacity.\n4.1.6 AutoML Pipeline Generation\nPipeline generation includes generating all data processing and analytics steps or modules\nto perform ML tasks. HeÔ¨Äetz et al. [28] propose a grid-world to represent all possible\nfamilies of each step of a data pipeline as cells and connect all possible cells as a graph.\nSubsequently, a hierarchical method is used to reduce the space of all actions and represent\nall actions by layers of clusters. Finally, the state representations are inputs to the value\nsub-network in a DQN network, and action representations are inputs to evaluate the\nadvantage-to-average sub-network.\n4.2 Healthcare\nHealthcare analytics has gained increasing attention in tandem with the advancement of\nhealthcare treatment and availability of medical data and computational capacity [41].\nNaturally, a great amount of eÔ¨Äort has been spent on applying DRL to healthcare. As\nbefore, implementing DRL-based models in healthcare requires the understanding of the\napplication context and deÔ¨Åning the key elements of MDP. However, diÔ¨Äerences occur in\nthe approaches to learning better decisions: learning the motivation of expert decisions by\nIRL, learning better decisions without an expert by interacting with an environment or\ninteracting with an environment with expert decisions as supervising signals.\n4.2.1 Treatment Recommendation\nTreatment recommendation systems are designed to assist doctors to make better decisions\nbased on electronic health records. However, the doctors‚Äô prescriptions are not ground\ntruth but valuable suggestions for high stake medical cases. The ground truth is the\ndelayed condition of the patients. Thus model predictions must not deviate from the\ndoctors‚Äô judgments too much, and not use those judgments as true labels. To tackle this\nchallenge, Wang et al. [103] propose an architecture to combine supervised learning and\nreinforcement learning. This model reduces the inconsistency between indicator signals\nlearned from doctor‚Äôs prescriptions via supervised learning and evaluation signals learned\nfromthelong-termoutcomeofpatientsviareinforcementlearning. IntheformulatedMDP,\nthe domain expert makes a decision based on an unknown policy. The goal is to learn a\npolicy that simultaneously reduces the diÔ¨Äerence between the chosen action of the agent\nand the expert‚Äôs decision and to maximize the weighted sum of discounted rewards.\n4.2.2 Diagnostic Inference\nUsing DRL to perform diagnosis can provide a second opinion in high-intensity diagnosis\nfrom historical medical records to reduce diagnostic errors. Ling et al. [51] propose mod-\neling the integration of external evidence to capture diagnostic concept as a MDP. The\nobjective is to Ô¨Ånd the optimal policy function. The inputs are case narratives and the\noutputs are improved concepts and inferred diagnoses. The states are a set of measures\nover the similarity of current concepts and externally extracted concepts. The actions\nare whether to accept (part of) the extracted concepts from external evidence. The en-\nvironments are the top extracted case narratives from Wikipedia as the document pool\nfor concepts extraction and a knowledge base for evaluating the intermediate results for\ncurrent best concepts. The rewards are evaluated based on an external knowledge base\nmapping from the concepts to the diagnoses. The whole process is modeled by DQN. At\n23\n\neach step, narrative cases and evidence are extracted, which provide the initial concepts\nand external concepts. The state representing the agent‚Äôs conÔ¨Ådence in the learned con-\ncept is duly calculated. Then the state is sent to the DQN agent to estimate the reward\nto model the long-run accuracy of the learned concept by the agent. Iteratively, the model\nconverges with better concepts and diagnoses.\n4.2.3 Hospital Resource Allocation\nAllocating limited hospital resources is the key to providing timely treatment for patients.\nIn [19], the problem is formulated as a classiÔ¨Åcation problem where the patients‚Äô features\nare given and the target is to predict the location of admissions. The RL framework uses\na student network to solve the classiÔ¨Åcation problem. The weights of the student network\nare used as states, which are fed into a teacher network to generate actions to select which\nbatch of data to train the student network. The accuracy of the classiÔ¨Åcation is used\nas the reward. This method provides a view on the resource allocation problem from a\ncurriculum learning perspective. However, the temporal information of the data samples is\nnot considered but it could aÔ¨Äect resource allocation since some hours during a day could\nhave fewer patients than the others.\n4.3 Fintech\nReinforcement learning has wide applications in the Ô¨Ånance domain. Firstly, reinforcement\nlearning has brought new perspectives to let the Ô¨Ånance research community revisit many\nclassic Ô¨Ånancial research topics. For example, traditional Ô¨Ånancial research topics such as\noption pricing that are typically solved by the classic Black‚ÄìScholes model can be steered\nthrough with a data-driven insight by reinforcement learning [25]. Secondly, portfolio op-\ntimization, typically formulated as a stochastic optimal control problem, can be addressed\nby reinforcement learning. Finally, the agents are Ô¨Ånancial market participants with dif-\nferent intentions. Reward functions can be learned to model these intentions, and hence,\nmake better decisions as illustrated in Figure 3. We refer readers with further interest in\nÔ¨Ånance to [62].\n4.3.1 Dynamic Portfolio Optimization\nThe portfolio optimization problem is challenging because of the high scale of the dimen-\nsionality and the high noise-to-signal ratio nature of stock price data. The latter problem\nof noisy observation can cause uncertainty in a learned policy. Therefore, [12] proposes a\nnovel model structure based on the Q-learning to handle noisy data and to scale to high\ndimensionality. The quadratic form of reward function is shown to have a semi-analytic\nsolution that is computationally eÔ¨Écient. In the problem formulation, the agent‚Äôs actions\nare represented as the changes in the assets at each time step. The states are the concate-\nnation of market signals and the agent‚Äôs holding assets. This method enhances Q-learning\nby introducing an entropy term measuring the noise in the data. This term acts as a regu-\nlarization term forcing the learned policy to be close to a reference policy that is modeled\nby a Gaussian distribution.\n4.3.2 Algorithm Trading Strategy IdentiÔ¨Åcation\nIdentiÔ¨Åcation of algorithm trading strategies from historical trades is important in fraud\ndetection and maintaining a healthy Ô¨Ånancial environment. [114] proposes using IRL to\nlearn the reward function behind the trading behaviors. The problem is formulated as\n24\n\nFigure 3: DRL in Ô¨Åntech applications.\nan Inverse Markov Decision Process (IMDP). The states are the diÔ¨Äerences between the\nvolumes of bid orders and ask orders, which are discretized into three intervals based on\nthe values of the volumes. The actions are the limit and market order discretized into 10\nintervals each by their values. The prior distribution of the reward function is a Gaussian\nProcess parameterized by \u0012. Given\u0012, the approximation of the posterior distribution\nof reward is performed by maximum a posteriori (MAP). This step would give a MAP\nestimated value of the reward. \u0012is optimized by a log-likelihood function on the posterior\nof observations. The optimization process can be proved to be convex which guarantees\nthe global minimum. The learned features are then used to identify and classify trading\nstrategies in the Ô¨Ånancial markets.\n4.3.3 Sentiment-based Trading\nOne of the main predictors in stock trading is sentiment, which drives the demand of bid\norders and asks orders. Sentiment scores are often represented by unstructured text data\nsuch as news or twitters. [115] proposes treating the sentiment as the aggregated action\nof all the market participants, which has the advantage of simplifying the modeling of the\nnumerous market participants. SpeciÔ¨Åcally, the sentiment scores are categorized into three\nintervals: high, medium, and low as the action spaces. Compared to previous works, the\nproposed method can model the dependency between the sentiment and the market state\nby the policy function. This method is based on Gaussian Inverse Reinforcement Learning\n[43] similar to [114] as discussed at the beginning of Section 4.3, which is eÔ¨Äective at dealing\nwith uncertainty in the stock environment. This method provides a method for modeling\nmarket sentiments. However, as IRL faces the challenge of non-uniqueness of reward [13]\nof one agent‚Äôs actions, the method does not address how aggregated actions of multiple\nmarket participants can infer a unique reward function.\n25\n\n4.4 E-Commerce\n4.4.1 Online Advertising\nWith the increasing digitalization of businesses, sales and competition for market shares\nhave moved online in tandem. As a result, online advertising has been increasing in its\npresence and importance and exploiting RL in various aspects. One of the topics in online\nadvertising, bidding optimization, can be formulated as a sequential decision problem: the\nadvertiser is required to have strategic proposals with bidding keywords sequentially to\nmaximize the overall proÔ¨Åt. In [124], the issue of using static transitional probability to\nmodel dynamic environments is identiÔ¨Åed and a new DRL model is proposed to exploit the\npattern discovered from dynamic environments.\nIncluding but not limited to advertising, Feng et al. [21] propose to consider the whole\npicture of multiple ranking tasks that occurred in the sequence of user‚Äôs queries. A new\nmulti-agent reinforcement learning model is proposed to enable multiple agents to partially\nobserve inputs and choose actions through their own actor networks. The agents communi-\ncate through a centralized critic model to optimize a shared objective. This allows diÔ¨Äerent\nranking algorithms to reconcile with each other when taking their own actions and consider\nthe contextual information.\n4.4.2 Online Recommendation\nThe problem of an unstabilized reward function arises because of the dynamic environment\nin the online recommendation. For example, user preference is modeled as the reward in\nDRL and it changes unexpectedly when a special discount happens for some products.\nIn [10], a random stratiÔ¨Åed sampling method is proposed to calculate the optimal way of\nstratifying by allocating more samples to the strata with more weighted variance. Then the\nreplay sampling is improved to consider key attributes of customers (e.g., gender, age, etc.),\nwhich are less volatile in the dynamic environment. This allows the modeling of reward\nfunction based on sampling from a pool with a longer horizon, thus reducing the bias in\nthe estimation of the reward function. Lastly, the dynamic environment poses a challenge\nin setting an optimal policy used in regretting. A new method in [10] is proposed to train\nan oÔ¨Ñine model to calculate a real-time reward for a subset of customers to approximate\na reference policy, that is used as an oÔ¨Äset in the reward recalibration to stabilize the\nperformance of the DRL algorithm.\n4.4.3 Search Results Aggregation\nAggregating useful search results in online shopping search is important to improve the\nshopping experience. However, the challenge of aggregating heterogeneous data sources is\noften encountered. The heterogeneous data sources in online shopping are diÔ¨Äerent product\ncategories such as a shoe brand group or a particular topic group, each of which is a ranking\nsystem. A new model in [91] is proposed to decompose the task into two sub-tasks. The\nÔ¨Årst one is to select a data source for the current page of search results based on historical\nusers‚Äô clicks on previous pages. Learning to select the correct data source for each page is\na sequential decision-making problem. The second sub-task is to Ô¨Åll the sequence of a page\nby selecting the best source from the candidate sources. However, the items from diÔ¨Äerent\nsources cannot be directly compared because of their heterogeneous nature. The problem\nis solved by formulating the sub-task as an RL task to let an agent Ô¨Åll up the sequence.\nHowever, one limitation of this method is that lacking full annotations of item relevance\nscores may constrain the model‚Äôs performance on various scenarios [91].\n26\n\n4.5 Other Applications\nDRL has been applied to various other applications. These DRL methods are often used\nwith a knowledge graph, confounders, or game theory to model application-speciÔ¨Åc dy-\nnamics. These methods are not only well motivated from their respective applications but\nalso general enough to be applied in other applications. However, these methods often fail\nto be evaluated by experiments in other applications.\nThe problem of mobile user proÔ¨Åling aims to identify user proÔ¨Åles to provide personalized\nservices. In [105], the action is the selection of a place of visit. The environment is\ncomprised of all users and a knowledge graph learning the semantic connections between\nthespatialentities. Theknowledgegraphisupdatedonceauser‚Äôsnewactivityisperformed\nand then aÔ¨Äects the agent‚Äôs prediction. The state is the embedding of a user and the\nknowledge graph for the current time step. The reward is determined by several metrics\nmeasuring the similarity between the predicted spatial entities and the ground truth. This\nmethod considers the spatial semantics of entities but does not consider how the change\nof a user‚Äôs key attributes (e.g., career) will aÔ¨Äect activity prediction and policy learning,\nwhich could cause instability in policy updating.\nIn the transportation system, drivers often get recommendations and provide feedback in\nreturn to improve the service. However, the recommendation often fails when drivers make\ndecisions in a complex environment. To address this issue, in [83] a new method is proposed\nto model hidden causal factors, called confounders, in a complex environment. SpeciÔ¨Åcally,\nthe framework in [32] is extended to include the confounders. First, all three elements (i.e.,\npolicy agent, environment, confounder) are treated as agents. The eÔ¨Äect of a confounder\nis modeled as the policy of the hidden agent, which takes the observation and action of the\npolicy agent as inputs and performs an action. The environment in turn takes the action\nbased on inputs of the hidden agent‚Äôs action and the policy agent‚Äôs action and observation.\nThe problem of spammer detection aims to detect spam generating strategies. The chal-\nlenge is that the detectors only detect easier spams while missing spams with strategies.\nIn [16], the problem is formulated as two agents counteracting each other. One agent is\nthe spammer, whose policy is to maintain a distribution of spam strategies and the action\nis to sample from the distribution. Another agent is the detector, whose state is the de-\ntection results after a spam attack and the action is to identify the spam. The rewards\nof two agents are measured by winning or losing revenue manipulation, respectively. The\nlimitation of this method is that there is no guarantee for equilibrium.\n5 Open Challenges and Future Directions\nRL approaches provide strong alternatives to traditional heuristics or supervised learning-\nbasedalgorithms. However, manychallengesremaintobeaddressedtomakeRLapractical\nsolution in the context of data processing and analytics. We also foresee many important\nfuture research directions to be developed.\n5.1 Open Challenges For System Optimization\n5.1.1 MDP Formulation and Lack of JustiÔ¨Åcation\nThe design of MDP impacts the performance and eÔ¨Éciency of the RL algorithm greatly.\nThe state should satisfy Markov property that its representation contains enough relevant\n27\n\ninformation for the RL agent to make the optimal decision. It should summarize the envi-\nronment compactly because a complicated state design will cause more training and infer-\nence costs. The action space should be designed carefully to balance learning performance\nand computational complexity. The reward deÔ¨Ånition directly aÔ¨Äects the optimization di-\nrection and the system performance. Additionally, the process of reward calculation can\ninvolvecostlydatacollectionandcomputationinthedatasystemsoptimization. Currently,\nmany works rely on experimental exploration and experience to formulate MDP while some\nworks exploit domain knowledge to improve the MDP formulation by injecting task-speciÔ¨Åc\nknowledge into action space[110]. Generally, MDP can inÔ¨Çuence computational complex-\nity, data required, and algorithm performance. Unfortunately, many works lack ablation\nstudies of their MDP formulations and do not justify the design in a convincing manner.\nTherefore, automation of MDP formulation remains an open problem.\n5.1.2 RL Algorithm and Technique Selection\nRL algorithms and techniques have diÔ¨Äerent tradeoÔ¨Äs and assumptions. Value-based DRL\nalgorithms like DQN are not stable and guaranteed convergence. Policy-based DRL algo-\nrithms like TRPO and PPO are often not eÔ¨Écient. Model-based DRL algorithms do not\nguarantee that a better model can result in a better policy. Value-based methods assume\nfull observability while policy-based ones assume episodic learning. OÔ¨Ä-policy algorithms\nare usually more eÔ¨Écient than on-policy algorithms in terms of sample eÔ¨Éciency. One\nexample is that DQ[37] uses oÔ¨Ä-policy deep Q-learning to increase data eÔ¨Éciency and re-\nduce the number of training queries needed. Training eÔ¨Éciency can be a big concern for\nDRL-based system optimization, especially when the workload of the system could change\ndramatically and the model needs to be retrained frequently. Generally, RL algorithms\nand techniques selection aÔ¨Äect the training eÔ¨Éciency and eÔ¨Äectiveness greatly.\n5.1.3 Integration with Existing Systems\nIntegrating RL-based methods into the real system more naturally and seamlessly faces\nmany challenges. The RL agent has to be evolved when the system environment changes\n(e.g., workload) and the performance is degraded. We need to design new model manage-\nment mechanisms to monitor, maintain, and upgrade the models. Furthermore, we Ô¨Ånd\nthat the RL-based solutions can be lightweight or intrusive. The lightweight approach in\nwhich the RL agent is not designed as a component of the system, e.g. using RL to gener-\nate the qd-tree[116], is easier to integrate into the system because it does not change the\narchitecture of the system dramatically. In contrast, the intrusive approach such as using\nRL models for join order optimization[61] is deeply embedded in the system and hence\nmay need a redesign and optimization of the original system architecture to support model\ninference eÔ¨Éciently. SageDB[35] proposes to learn various database system components\nby integrating RL and other ML techniques. Nevertheless, the proposed model-driven\ndatabase system is yet to be fully implemented and benchmarked. It is likely that the\ndata system architecture needs to be overhauled or signiÔ¨Åcantly amended in order to graft\ndata-driven RL solutions into the data system seamlessly to yield an overall performance\ngain.\n5.1.4 Reproducibility and Benchmark\nIn the data system optimization problem, RL algorithms are not easy to be reproduced\ndue to many factors such as lacking open source codes, workload, historic statistics used,\nand the unstable performance of RL algorithms. The landscape of problems in system\n28\n\noptimization is vast and diverse. It could prevent fair comparison and optimization for fu-\nture research works and deployments in practice. Lacking benchmarks is another challenge\nto evaluate these RL approaches. The benchmarks are therefore to provide standardized\nenvironments and evaluation metrics to conduct experiments with diÔ¨Äerent RL approaches.\nThere are some eÔ¨Äorts to mitigate the issue. For example, Park[57] is an open platform for\nresearchers to conduct experiments with RL. However, it only provides a basic interface\nand lacks system speciÔ¨Åcations. There is much room to improve with regards to the repro-\nducibility and benchmark in order to promote the development and adoption of RL-based\nmethods[30].\n5.2 Open Challenges For Applications\n5.2.1 Lack of Adaptability\nThere is a lack of adaptability for methods on a single component of a data pipeline to\nthe whole. For example, many works focus on data cleaning tasks such as entity matching.\nHowever, little works have shown their eÔ¨Éciency in deploying their model in an end-to-end\ndata pipeline. These works treat the tasks isolatedly from other tasks in the pipeline,\nthereby limiting the pipeline‚Äôs performance. In healthcare, each method is applied in\ndiÔ¨Äerent steps of the whole treatment process, without being integrated and evaluated as\none pipeline. One possible direction could be considering DRL as a module in the data\npipeline optimization. However, data pipeline optimization has been focusing on models\nsimpler than DRL to enable fast pipeline evaluation [53]. How to eÔ¨Éciently incorporate\nDRL into the data pipeline optimization remains a challenge.\n5.2.2 DiÔ¨Éculty in Comparison with DiÔ¨Äerent Applications\nTo date, most works with generalized contributions are only evaluated domain-speciÔ¨Åcally.\nResearch questions are often formulated in their own platform as in E-Commerce. This\npresents diÔ¨Éculty in evaluating the methods for diÔ¨Äerent environments. For example, the\nconfounders modeling hidden causal factors in [83] can also contribute to DRL modeling in\nE-commerce. This is because modeling customers‚Äô interests are always subject to changing\nenvironments and a new environment may contain hidden causal factors. For example,\nconsumers are more willing to buy relevant products for certain situations such as Covid-\n19. Thus a general DRL method is yet to show the robustness and eÔ¨Äectiveness under the\nenvironment of diÔ¨Äerent applications.\n5.2.3 Lack of Prediction in Multi-modality\nInhealthcareandÔ¨Ånance,multiplesourcesofdatabringdiÔ¨Äerentperspectives. Forexample\ninhealthcare, electronichealthrecords, imagescans, andmedicaltestscanprovidediÔ¨Äerent\nfeatures for accurate prediction. In addition, these sources of data with diÔ¨Äerent sample\nfrequencies provide contextual information for modeling a patient‚Äôs visits to the hospital or\nsymptom development. However, most innovations in healthcare focus on one particular\nsource of data. How to integrate the contextual information with multi-modality eÔ¨Äectively\nremains an unsolved diÔ¨Écult problem.\n5.2.4 Injecting Domain Knowledge in Experience Replay\nIn high-stake applications such as healthcare and Ô¨Ånance, injecting domain knowledge can\nmake decision making in RL more robust and explainable. One possible way is to inject the\n29\n\nknowledge of human beings‚Äô experience into an agent‚Äôs experience pool as a prior distribu-\ntion for the policy. For example, in dynamic portfolio optimization, a portfolio manager\ncould have a large source of experience for risk management and proÔ¨Åt optimization. Such\nexperiencecouldbeusefulforwarminguptheagent‚Äôsexplorationinthesearchspace. Some\nworks have shown positive eÔ¨Äects of domain knowledge injection on selecting important\nexperiences (i.e., transition samples) [79]. Notwithstanding, it remains a big challenge to\ninject useful and relevant knowledge from the experience into the agent‚Äôs experience pool.\n5.3 Future Research Directions\n5.3.1 Data Structure Design\nDRL provides an alternative way to Ô¨Ånd good data structures through feedback instead\nof designing them based on human knowledge and experience, e.g., decision tree[47] and\nthe qd-tree[116]. These trees are optimized better because they are learned by interact-\ning with the environment. DRL has also been eÔ¨Äective in graph designs (e.g., molecular\ngraph[117]). However, large-scale graph generation using DRL is diÔ¨Écult and daunting\nbecause it involves a huge search space. Generating other important structures using DRL\nremains to be explored. Idreos et al.[33] propose a Data Alchemist that learns to synthesize\ndata structures by DRL and other techniques including Genetic Algorithms and Bayesian\nOptimization. In summary, DRL has a role in the design of more eÔ¨Écient data structures\nby interacting and learning from the environment. These indexes have to be adaptive to\ndiÔ¨Äerent data distributions and workloads.\n5.3.2 Interpretability\nThe underlying logic behind the DRL agent is still unknown. In high-risk application areas\nsuchashealthcare, theadoptionofDRLwillbeabigissueinthecasethattheseapproaches\nmakewrongdecisionsandpeopledonotknowwhyithappensduetolackofinterpretability.\nMany techniques have been proposed to mitigate the issue and provide interpretability[76].\nHowever, they neglect domain knowledge from related Ô¨Åelds and applications and the\nexplanations are not eÔ¨Äective to human users. To instill conÔ¨Ådence in the deployment of\nDRL-based systems in practice, interpretability is an important component and we should\navoid treating DRL solutions as black boxes especially in critical applications.\n5.3.3 Robustness by Causal Reasoning\nModeling real-world applications by DRL inevitably suÔ¨Äers from the problem of distribu-\ntion changes. The real world has independent physical mechanisms that can be seen as\ndiÔ¨Äerent modules. For example, an image is subjected to the light of the environment.\nGiven the modular property, a structural type of modeling focusing on factorizing the\ncausal mechanisms can extract the invariant causal mechanisms and show robustness cross\ndistribution changes [82]. One research direction towards DRL robust decision making\nis to perform sampling from past actions from a causal perspective. Given the invari-\nance property of causal mechanisms, past actions can be reused by capturing the invariant\nmechanisms in a changing environment.\n5.3.4 Extension to Other Domains\nBeyond existing works, many classic problems in the data system and analytics could\npotentially be solved by DRL. For example, Polyjuice[101] learns the concurrency control\n30\n\nalgorithm for a given workload by deÔ¨Åning Ô¨Åne-grained actions and states in the context of\nconcurrency control. Though they use an evolutionary algorithm to learn and outperform\na simple DRL baseline, we believe that there are huge potentials to further improve DRL\nfor niche applications. Hence, we expect that more problems will be explored and solved\nwith DRL in various domains in the near future.\n5.3.5 Towards Intelligent and Autonomous Databases\nAlthough DRL algorithms could provide breakthrough performance on many tasks than\ntraditional methods, many issues need to be addressed towards intelligent and autonomous\ndatabases. First, database schema could be updated and DRL models trained on the pre-\nvious snapshots may not work. DRL algorithms need to tackle generalization[72]. Second,\nit would be so costly and infeasible to train models from scratch for each scenario and set-\nting. Transfer learning from existing models could be a potential way to ease the workload\ngreatly. Third, we have to choose appropriate DRL algorithms automatically, in the same\nspirit as AutoML. Fourth, current DBMS systems were designed without considering much\nabout the learning mechanism. A radically new DBMS design may be proposed based on\nthelearning-centric architecture. To supportintelligentand autonomous databasesystems,\nDRL models intelligent behaviors and may provide a solid basis for achieving artiÔ¨Åcial gen-\neral intelligence based on reward maximization and trial-and-error experience[88].\n6 Conclusions\nIn this survey, we present a comprehensive review of recent advances in utilizing DRL\nin data processing and analytics. The DRL agent could learn to understand and solve\nvarious tasks with the right incentives. First, we introduce basic foundations and practical\ntechniques in DRL. Next, we survey and review DRL for data processing and analytics\nfromtwoperspectives, systemsandapplications. Wecoveralargenumberoftopicsranging\nfrom fundamental problems in system areas such as tuning and scheduling to important\napplications such as healthcare and Ô¨Åntech. Finally, we discuss key challenges and future\ndirections for applying DRL in data processing and analytics. We hope the survey would\nserve as a basis for research and development in this emerging area, and better integration\nof DRL techniques into data processing pipelines and stacks.\nReferences\n[1] M. Armbrust, R. S. Xin, C. Lian, Y. Huai, D. Liu, J. K. Bradley, X. Meng, T. Kaftan,\nM. J. Franklin, A. Ghodsi, et al. Spark sql: Relational data processing in spark. In\nACMSIGMOD, pages 1383‚Äì1394, 2015.\n[2] P. Auer. Using conÔ¨Ådence bounds for exploitation-exploration trade-oÔ¨Äs. Journalof\nMachine Learning Research, 3(Nov):397‚Äì422, 2002.\n[3] S. Banerjee, S. Jha, Z. Kalbarczyk, and R. Iyer. Inductive-bias-driven reinforcement\nlearning for eÔ¨Écient schedules in heterogeneous clusters. In ICML, pages 629‚Äì641.\nPMLR, 2020.\n[4] O. Bar El, T. Milo, and A. Somech. Automatically generating data exploration\nsessions using deep reinforcement learning. In ACMSIGMOD, pages 1527‚Äì1537,\n2020.\n31\n\n[5] D. Baranchuk and A. Babenko. Towards similarity graphs constructed by deep rein-\nforcement learning. arXivpreprint arXiv:1911.12122, 2019.\n[6] R. Bellman. Dynamic programming. Science, 153(3731):34‚Äì37, 1966.\n[7] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning. In\nICML, pages 41‚Äì48, 2009.\n[8] C. Berner, G. Brockman, B. Chan, V. Cheung, P. Debiak, C. Dennison, D. Farhi,\nQ. Fischer, S. Hashme, C. Hesse, et al. Dota 2 with large scale deep reinforcement\nlearning. arXivpreprint arXiv:1912.06680, 2019.\n[9] S. Chaudhuri and V. Narasayya. Autoadmin ‚Äúwhat-if‚Äù index analysis utility. ACM\nSIGMOD Record, 27(2):367‚Äì378, 1998.\n[10] S.-Y.Chen,Y.Yu,Q.Da,J.Tan,H.-K.Huang,andH.-H.Tang. Stabilizingreinforce-\nment learning in dynamic environment with application to online recommendation.\nInACMSIGKDD, pages 1187‚Äì1196, 2018.\n[11] K. Clark and C. D. Manning. Deep reinforcement learning for mention-ranking\ncoreference models. arXivpreprint arXiv:1609.08667, 2016.\n[12] M. Dixon and I. Halperin. G-learner and girl: Goal based wealth management with\nreinforcement learning. arXivpreprint arXiv:2002.10990, 2020.\n[13] H. Dong, H. Dong, Z. Ding, S. Zhang, and Chang. DeepReinforcement Learning.\nSpringer, 2020.\n[14] L. Dong and M. Lapata. Language to logical form with neural attention. arXiv\npreprint arXiv:1601.01280, 2016.\n[15] W. Dong, C. Moses, and K. Li. EÔ¨Écient k-nearest neighbor graph construction for\ngeneric similarity measures. In WWW, pages 577‚Äì586, 2011.\n[16] Y. Dou, G. Ma, P. S. Yu, and S. Xie. Robust spammer detection by nash reinforce-\nment learning. In ACMSIGKDD, pages 924‚Äì933, 2020.\n[17] G. C. Durand, M. Pinnecke, R. Piriyev, M. Mohsen, D. Broneske, G. Saake, M. S.\nSekeran, F.Rodriguez,andL.Balami. Gridformation: towardsself-drivenonlinedata\npartitioning using reinforcement learning. In Proceedings oftheFirstInternational\nWorkshop onExploiting ArtiÔ¨Åcial Intelligence Techniques forDataManagement,\npages 1‚Äì7, 2018.\n[18] G. C. Durand, R. Piriyev, M. Pinnecke, D. Broneske, B. Gurumurthy, and G. Saake.\nAutomated vertical partitioning with deep reinforcement learning. In European\nConference onAdvances inDatabases andInformation Systems, pages 126‚Äì134.\nSpringer, 2019.\n[19] R. El-Bouri, D. Eyre, P. Watkinson, T. Zhu, and D. Clifton. Student-teacher cur-\nriculum learning via reinforcement learning: Predicting hospital inpatient admission\nlocation. In ICML, pages 2848‚Äì2857, 2020.\n[20] Z. Fang, Y. Cao, Q. Li, D. Zhang, Z. Zhang, and Y. Liu. Joint entity linking with\ndeep reinforcement learning. In WWW, pages 438‚Äì447, 2019.\n[21] J. Feng, H. Li, M. Huang, S. Liu, W. Ou, Z. Wang, and X. Zhu. Learning to col-\nlaborate: Multi-scenario ranking via multi-agent reinforcement learning. In WWW,\npages 1939‚Äì1948, 2018.\n32\n\n[22] M. Fortunato, M. G. Azar, B. Piot, J. Menick, I. Osband, A. Graves, V. Mnih,\nR. Munos, D. Hassabis, O. Pietquin, et al. Noisy networks for exploration. arXiv\npreprint arXiv:1706.10295, 2017.\n[23] Y. Gao, L. Chen, and B. Li. Spotlight: Optimizing device placement for training\ndeep neural networks. In ICML, pages 1676‚Äì1684. PMLR, 2018.\n[24] T. Gu, K. Feng, G. Cong, C. Long, Z. Wang, and S. Wang. The rlr-tree: A rein-\nforcement learning based r-tree for spatial data. arXivpreprint arXiv:2103.04541,\n2021.\n[25] I. Halperin. The qlbs q-learner goes nuqlear: Ô¨Åtted q iteration, inverse rl, and option\nportfolios. Quantitative Finance, 19(9):1543‚Äì1553, 2019.\n[26] M.HausknechtandP.Stone. Deeprecurrentq-learningforpartiallyobservablemdps.\narXivpreprint arXiv:1507.06527, 2015.\n[27] N. Heess, D. TB, S. Sriram, J. Lemmon, J. Merel, G. Wayne, Y. Tassa, T. Erez,\nZ. Wang, S. Eslami, et al. Emergence of locomotion behaviours in rich environments.\narXivpreprint arXiv:1707.02286, 2017.\n[28] Y. HeÔ¨Äetz, R. Vainshtein, G. Katz, and L. Rokach. Deepline: Automl tool for\npipelines generation using deep reinforcement learning and hierarchical actions Ô¨Ålter-\ning. InACMSIGMOD, pages 2103‚Äì2113, 2020.\n[29] J. Heitz and K. Stockinger. Join query optimization with deep reinforcement learning\nalgorithms. arXivpreprint arXiv:1911.11689, 2019.\n[30] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger. Deep\nreinforcement learning that matters. In AAAI, volume 32, 2018.\n[31] B. Hilprecht, C. Binnig, and U. R√∂hm. Learning a partitioning advisor for cloud\ndatabases. In ACMSIGMOD, pages 143‚Äì157, 2020.\n[32] J. Ho and S. Ermon. Generative adversarial imitation learning. arXivpreprint\narXiv:1606.03476, 2016.\n[33] S. Idreos, K. Zoumpatianos, S. Chatterjee, W. Qin, A. Wasay, B. Hentschel,\nM. Kester, N. Dayan, D. Guo, M. Kang, et al. Learning data structure alchemy.\nTCDE, 42(2), 2019.\n[34] L. Kocsis and C. Szepesv√°ri. Bandit based monte-carlo planning. In European\nconference onmachine learning, pages 282‚Äì293. Springer, 2006.\n[35] T.Kraska, M.Alizadeh, A.Beutel, H.Chi, A.Kristo, G.Leclerc, S.Madden, H.Mao,\nand V. Nathan. Sagedb: A learned database system. In CIDR, 2019.\n[36] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for learned\nindex structures. In ACMSIGMOD, pages 489‚Äì504, 2018.\n[37] S. Krishnan, Z. Yang, K. Goldberg, J. Hellerstein, and I. Stoica. Learning to optimize\njoin queries with deep reinforcement learning. arXivpreprint arXiv:1808.03196, 2018.\n[38] T. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum. Hierarchical\ndeep reinforcement learning: Integrating temporal abstraction and intrinsic motiva-\ntion.arXivpreprint arXiv:1604.06057, 2016.\n33\n\n[39] H. Lan, Z. Bao, and Y. Peng. An index advisor using deep reinforcement learning.\nInCIKM, pages 2105‚Äì2108, 2020.\n[40] S. Lange, T. Gabel, and M. Riedmiller. Batch reinforcement learning. In\nReinforcement learning, pages 45‚Äì73. Springer, 2012.\n[41] C. Lee, Z. Luo, K. Y. Ngiam, M. Zhang, K. Zheng, G. Chen, B. Ooi, and J. Yip. Big\nhealthcare data analytics: Challenges and applications. In Handbook ofLarge-Scale\nDistributed Computing inSmartHealthcare, 2017.\n[42] V. Leis, A. Gubichev, A. Mirchev, P. Boncz, A. Kemper, and T. Neumann. How\ngood are query optimizers, really? VLDB, 9(3):204‚Äì215, 2015.\n[43] S. Levine, Z. Popovic, and V. Koltun. Nonlinear inverse reinforcement learning with\ngaussian processes. NIPS, 24:19‚Äì27, 2011.\n[44] G. Li, X. Zhou, S. Li, and B. Gao. Qtune: A query-aware database tuning system\nwith deep reinforcement learning. VLDB, 12(12):2118‚Äì2130, 2019.\n[45] T. Li, Z. Xu, J. Tang, and Y. Wang. Model-free control for distributed stream data\nprocessing using deep reinforcement learning. VLDB, 11, 2018.\n[46] W. Li, X. Li, H. Li, and G. Xie. Cutsplit: A decision-tree combining cutting and\nsplittingforscalablepacketclassiÔ¨Åcation. In IEEEINFOCOM 2018-IEEE Conference\nonComputer Communications, pages 2645‚Äì2653. IEEE, 2018.\n[47] E. Liang, H. Zhu, X. Jin, and I. Stoica. Neural packet classiÔ¨Åcation. In ACM\nSIGCOMM, pages 256‚Äì269. 2019.\n[48] X. Liang, A. J. Elmore, and S. Krishnan. Opportunistic view materialization with\ndeep reinforcement learning. arXivpreprint arXiv:1903.01363, 2019.\n[49] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and\nD. Wierstra. Continuous control with deep reinforcement learning. arXivpreprint\narXiv:1509.02971, 2015.\n[50] L.-J. Lin. Self-improving reactive agents based on reinforcement learning, planning\nand teaching. Machine learning, 8(3-4):293‚Äì321, 1992.\n[51] Y. Ling, S. A. Hasan, V. Datla, A. Qadir, K. Lee, J. Liu, and O. Farri. Diagnostic in-\nferencing via improving clinical concept extraction with deep reinforcement learning:\nA preliminary study. In MLHC, 2017.\n[52] K. Liu, Y. Fu, P. Wang, L. Wu, R. Bo, and X. Li. Automating feature subspace\nexploration via multi-agent reinforcement learning. In ACMSIGKDD, pages 207‚Äì\n215, 2019.\n[53] Z. Luo, S. H. Yeung, M. Zhang, K. Zheng, L. Zhu, G. Chen, F. Fan, Q. Lin, K. Y.\nNgiam, and B. Chin Ooi. Mlcask: EÔ¨Écient management of component evolution in\ncollaborative data analytics pipelines. In ICDE, pages 1655‚Äì1666, 2021.\n[54] N. C. Luong, D. T. Hoang, S. Gong, D. Niyato, P. Wang, Y.-C. Liang, and D. I. Kim.\nApplications of deep reinforcement learning in communications and networking: A\nsurvey.IEEECommunications Surveys&Tutorials, 21(4):3133‚Äì3174, 2019.\n[55] Y. A. Malkov and D. A. Yashunin. EÔ¨Écient and robust approximate nearest neighbor\nsearch using hierarchical navigable small icde graphs. PAMI, 42(4):824‚Äì836, 2018.\n34\n\n[56] J. Manyika, M. Chui, B. Brown, J. Bughin, R. Dobbs, C. Roxburgh, A. Hung Byers,\net al.Bigdata:Thenextfrontierforinnovation, competition, andproductivity.\nMcKinsey Global Institute, 2011.\n[57] H. Mao, P. Negi, A. Narayan, H. Wang, J. Yang, H. Wang, R. Marcus, R. Addanki,\nM. Khani Shirkoohi, S. He, et al. Park: An open platform for learning-augmented\ncomputer systems. NIPS, 2019.\n[58] H.Mao, M.Schwarzkopf, S.B.Venkatakrishnan, Z.Meng, andM.Alizadeh. Learning\nscheduling algorithms for data processing clusters. In ACMSIGCOMM, pages 270‚Äì\n288. 2019.\n[59] H. Mao, S. B. Venkatakrishnan, M. Schwarzkopf, and M. Alizadeh. Variance reduc-\ntion for reinforcement learning in input-driven environments. In ICLR, 2019.\n[60] R. Marcus, P. Negi, H. Mao, N. Tatbul, M. Alizadeh, and T. Kraska. Bao: Making\nlearned query optimization practical. In ACMSIGMOD, pages 1275‚Äì1288, 2021.\n[61] R. Marcus and O. Papaemmanouil. Deep reinforcement learning for join order enu-\nmeration. In Proceedings oftheFirstInternational Workshop onExploiting ArtiÔ¨Åcial\nIntelligence Techniques forDataManagement, pages 1‚Äì4, 2018.\n[62] F. D. Matthew, H. Igor, and B. Paul. Machine learning in Ô¨Ånance: From theory to\npractice, 2021.\n[63] X. Meng, J. Bradley, B. Yavuz, E. Sparks, S. Venkataraman, D. Liu, J. Freeman,\nD. Tsai, M. Amde, S. Owen, et al. Mllib: Machine learning in apache spark. The\nJournalofMachine Learning Research, 17(1):1235‚Äì1241, 2016.\n[64] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and\nK. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In ICML,\npages 1928‚Äì1937. PMLR, 2016.\n[65] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and\nM. Riedmiller. Playing atari with deep reinforcement learning. arXivpreprint\narXiv:1312.5602, 2013.\n[66] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,\nA. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control\nthrough deep reinforcement learning. nature, 518(7540):529‚Äì533, 2015.\n[67] K. Narasimhan, A. Yala, and R. Barzilay. Improving information extraction\nby acquiring external evidence with reinforcement learning. arXivpreprint\narXiv:1603.07954, 2016.\n[68] A. Y. Ng, S. J. Russell, et al. Algorithms for inverse reinforcement learning. In\nICML, volume 1, page 2, 2000.\n[69] M.-h. Oh and G. Iyengar. Sequential anomaly detection using inverse reinforcement\nlearning. In ACMSIGMOD, pages 1480‚Äì1490, 2019.\n[70] C. Olston, B. Reed, U. Srivastava, R. Kumar, and A. Tomkins. Pig latin: a not-so-\nforeign language for data processing. In ACMSIGMOD, pages 1099‚Äì1110, 2008.\n[71] B. C. Ooi, R. Sacks-Davis, and J. Han. Indexing in spatial databases.\nUnpublished/Technical Papers, 1993.\n35\n\n[72] C. Packer, K. Gao, J. Kos, P. Kr√§henb√ºhl, V. Koltun, and D. Song. Assessing\ngeneralizationindeepreinforcementlearning. arXivpreprint arXiv:1810.12282, 2018.\n[73] G. Pang, A. van den Hengel, C. Shen, and L. Cao. Toward deep supervised anomaly\ndetection: Reinforcement learning from partially labeled anomaly data. In ACM\nSIGKDD, pages 1298‚Äì1308, 2021.\n[74] L. L. Perez and C. M. Jermaine. History-aware query optimization with materialized\nintermediate views. In ICDE, pages 520‚Äì531, 2014.\n[75] M. Plappert, R. Houthooft, P. Dhariwal, S. Sidor, R. Y. Chen, X. Chen, T. Asfour,\nP. Abbeel, and M. Andrychowicz. Parameter space noise for exploration. arXiv\npreprint arXiv:1706.01905, 2017.\n[76] E. Puiutta and E. M. Veith. Explainable reinforcement learning: A survey. In\nCD-MAKE, pages 77‚Äì95. Springer, 2020.\n[77] G. A. Rummery and M. Niranjan. On-lineQ-learning usingconnectionist systems,\nvolume 37. University of Cambridge, Department of Engineering Cambridge, UK,\n1994.\n[78] Z. Sadri, L. Gruenwald, and E. Lead. Drlindex: deep reinforcement learning index\nadvisorforaclusterdatabase. In Proceedings ofthe24thSymposium onInternational\nDatabase Engineering &Applications, pages 1‚Äì8, 2020.\n[79] T. Schaul, J. Quan, I. Antonoglou, and D. Silver. Prioritized experience replay. arXiv\npreprint arXiv:1511.05952, 2015.\n[80] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy\noptimization. In ICML, pages 1889‚Äì1897. PMLR, 2015.\n[81] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy\noptimization algorithms. arXivpreprint arXiv:1707.06347, 2017.\n[82] B. Sch√∂lkopf, F. Locatello, S. Bauer, N. R. Ke, N. Kalchbrenner, A. Goyal, and\nY. Bengio. Toward causal representation learning. Proceedings oftheIEEE,\n109(5):612‚Äì634, 2021.\n[83] W. Shang, Y. Yu, Q. Li, Z. Qin, Y. Meng, and J. Ye. Environment reconstruction\nwith hidden confounders for reinforcement learning based recommendation. In ACM\nSIGKDD, pages 566‚Äì576, 2019.\n[84] A. Sharma, F. M. Schuhknecht, and J. Dittrich. The case for automatic database\nadministration using deep reinforcement learning. arXivpreprint arXiv:1801.05643,\n2018.\n[85] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche,\nJ. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the\ngame of go with deep neural networks and tree search. nature, 529(7587):484‚Äì489,\n2016.\n[86] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot,\nL. Sifre, D. Kumaran, T. Graepel, et al. A general reinforcement learning algorithm\nthat masters chess, shogi, and go through self-play. Science, 362(6419):1140‚Äì1144,\n2018.\n36\n\n[87] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller. Determin-\nistic policy gradient algorithms. In ICML, pages 387‚Äì395. PMLR, 2014.\n[88] D. Silver, S. Singh, D. Precup, and R. S. Sutton. Reward is enough. ArtiÔ¨Åcial\nIntelligence, page 103535, 2021.\n[89] R. S. Sutton and A. G. Barto. Reinforcement learning: Anintroduction. MIT press,\n2018.\n[90] R.S.Sutton, D.A.McAllester, S.P.Singh, andY.Mansour. Policygradientmethods\nfor reinforcement learning with function approximation. In NIPS, pages 1057‚Äì1063,\n2000.\n[91] R. Takanobu, T. Zhuang, M. Huang, J. Feng, H. Tang, and B. Zheng. Aggregating\ne-commerce search results from heterogeneous sources via hierarchical reinforcement\nlearning. In WWW, pages 1771‚Äì1781, 2019.\n[92] W. R. Thompson. On the likelihood that one unknown probability exceeds another\nin view of the evidence of two samples. Biometrika, 25:285‚Äì294, 1933.\n[93] A. Thusoo, J. S. Sarma, N. Jain, Z. Shao, P. Chakka, S. Anthony, H. Liu, P. WyckoÔ¨Ä,\nand R. Murthy. Hive: a warehousing solution over a map-reduce framework. VLDB,\n2(2):1626‚Äì1629, 2009.\n[94] I. Trummer, J. Wang, D. Maram, S. Moseley, S. Jo, and J. Antonakakis. Skinnerdb:\nRegret-bounded query evaluation via reinforcement learning. In ACMSIGMOD,\npages 1153‚Äì1170, 2019.\n[95] K. Tzoumas, T. Sellis, and C. S. Jensen. A reinforcement learning approach for\nadaptive query processing. History, 2008.\n[96] G. E. Uhlenbeck and L. S. Ornstein. On the theory of the brownian motion. Physical\nreview, 36:823, 1930.\n[97] B. Vamanan, G. Voskuilen, and T. Vijaykumar. EÔ¨Écuts: Optimizing packet classi-\nÔ¨Åcation for memory and throughput. ACMSIGCOMM, 40(4):207‚Äì218, 2010.\n[98] D. Van Aken, A. Pavlo, G. J. Gordon, and B. Zhang. Automatic database manage-\nment system tuning through large-scale machine learning. In ACMSIGMOD, pages\n1009‚Äì1024, 2017.\n[99] H. Van Hasselt, A. Guez, and D. Silver. Deep reinforcement learning with double\nq-learning. In AAAI, volume 30, 2016.\n[100] H. Wang, H. He, M. Alizadeh, and H. Mao. Learning caching policies with subsam-\npling. In NeurIPS Machine Learning forSystems Workshop, 2019.\n[101] J. Wang, D. Ding, H. Wang, C. Christensen, Z. Wang, H. Chen, and J. Li. Polyjuice:\nHigh-performance transactions via learned concurrency control. In OSDI, pages 198‚Äì\n216, July 2021.\n[102] L. Wang, Q. Weng, W. Wang, C. Chen, and B. Li. Metis: learning to schedule\nlong-running applications in shared container clusters at scale. In SC20, pages 1‚Äì17.\nIEEE, 2020.\n[103] L. Wang, W. Zhang, X. He, and H. Zha. Supervised reinforcement learning with\nrecurrentneuralnetworkfordynamictreatmentrecommendation. In ACMSIGKDD,\npages 2447‚Äì2456, 2018.\n37\n\n[104] M. Wang, X. Xu, Q. Yue, and Y. Wang. A comprehensive survey and experimental\ncomparison of graph-based approximate nearest neighbor search. arXivpreprint\narXiv:2101.12631, 2021.\n[105] P. Wang, K. Liu, L. Jiang, X. Li, and Y. Fu. Incremental mobile user proÔ¨Åling:\nReinforcement learning with spatial knowledge graph for modeling event streams. In\nACMSIGKDD, pages 853‚Äì861, 2020.\n[106] W. Wang, J. Gao, M. Zhang, S. Wang, G. Chen, T. K. Ng, B. C. Ooi, J. Shao,\nand M. Reyad. RaÔ¨Åki: machine learning as an analytics service system. VLDB,\n12(2):128‚Äì140, 2018.\n[107] W. Wang, M. Zhang, G. Chen, H. Jagadish, B. C. Ooi, and K.-L. Tan. Database\nmeets deep learning: Challenges and opportunities. ACMSIGMOD Record,\n45(2):17‚Äì22, 2016.\n[108] Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot, and N. Freitas. Dueling\nnetwork architectures for deep reinforcement learning. In ICML, pages 1995‚Äì2003.\nPMLR, 2016.\n[109] C. J. Watkins and P. Dayan. Q-learning. Machine learning, 8(3-4):279‚Äì292, 1992.\n[110] J. Welborn, M. Schaarschmidt, and E. Yoneki. Learning index selection with struc-\ntured action spaces. arXivpreprint arXiv:1909.07440, 2019.\n[111] N. Wu and P. Li. Phoebe: Reuse-aware online caching with reinforcement learning\nfor emerging storage models. arXivpreprint arXiv:2011.07160, 2020.\n[112] S. Wu, X. Yu, X. Feng, F. Li, W. Cao, and G. Chen. Progressive neural index search\nfor database system. arXivpreprint arXiv:1912.07001, 2019.\n[113] X. Xu, C. Liu, and D. Song. Sqlnet: Generating structured queries from natural\nlanguage without reinforcement learning. arXivpreprint arXiv:1711.04436, 2017.\n[114] S. Y. Yang, Q. Qiao, P. A. Beling, W. T. Scherer, and A. A. Kirilenko. Gaus-\nsian process-based algorithmic trading strategy identiÔ¨Åcation. Quantitative Finance,\n15(10):1683‚Äì1703, 2015.\n[115] S. Y. Yang, Y. Yu, and S. Almahdi. An investor sentiment reward-based trading\nsystem using gaussian inverse reinforcement learning algorithm. ExpertSystems\nwithApplications, 114:388‚Äì401, 2018.\n[116] Z. Yang, B. Chandramouli, C. Wang, J. Gehrke, Y. Li, U. F. Minhas, P.-√Ö. Larson,\nD. Kossmann, and R. Acharya. Qd-tree: Learning data layouts for big data analytics.\nInACMSIGMOD, pages 193‚Äì208, 2020.\n[117] J. You, B. Liu, R. Ying, V. Pande, and J. Leskovec. Graph convolutional policy\nnetwork for goal-directed molecular graph generation. In NIPS, pages 6412‚Äì6422,\n2018.\n[118] C. Yu, J. Liu, and S. Nemati. Reinforcement learning in healthcare: A survey. arXiv\npreprint, 2019.\n[119] X. Yu, G. Li, C. Chai, and N. Tang. Reinforcement learning with tree-lstm for join\norder selection. In ICDE, pages 1297‚Äì1308. IEEE, 2020.\n38\n\n[120] X. Yu, Y. Peng, F. Li, S. Wang, X. Shen, H. Mai, and Y. Xie. Two-level data\ncompression using machine learning in time series database. In ICDE, pages 1333‚Äì\n1344. IEEE, 2020.\n[121] H. Yuan, G. Li, L. Feng, J. Sun, and Y. Han. Automatic view generation with deep\nlearning and reinforcement learning. In ICDE, pages 1501‚Äì1512. IEEE, 2020.\n[122] C. Zhang, R. Marcus, A. Kleiman, and O. Papaemmanouil. BuÔ¨Äer pool aware query\nscheduling via deep reinforcement learning. arXivpreprint arXiv:2007.10568, 2020.\n[123] J. Zhang, Y. Liu, K. Zhou, G. Li, Z. Xiao, B. Cheng, J. Xing, Y. Wang, T. Cheng,\nL. Liu, et al. An end-to-end automatic cloud database tuning system using deep\nreinforcement learning. In ACMSIGMOD, pages 415‚Äì432, 2019.\n[124] J. Zhao, G. Qiu, Z. Guan, W. Zhao, and X. He. Deep reinforcement learning for\nsponsored search real-time bidding. In ACMSIGKDD, pages 1021‚Äì1030, 2018.\n[125] V. Zhong, C. Xiong, and R. Socher. Seq2sql: Generating structured queries from\nnatural language using reinforcement learning. arXivpreprint, 2017.\n[126] B. Zoph and Q. V. Le. Neural architecture search with reinforcement learning. arXiv\npreprint arXiv:1611.01578, 2016.\n[127] J. Zou, P. Barhate, A. Das, A. Iyengar, B. Yuan, D. Jankov, and C. Jermaine.\nLachesis: Automated generation of persistent partitionings for big data applications.\nVLDB, 14(8), 2021.\n39",
  "textLength": 122661
}