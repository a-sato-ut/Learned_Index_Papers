{
  "paperId": "60033bc7022a6a3614957bf905d5a7a7e263b9f5",
  "title": "Scheduling with Predictions and the Price of Misprediction",
  "pdfPath": "60033bc7022a6a3614957bf905d5a7a7e263b9f5.pdf",
  "text": "Scheduling with Predictions and the Price of Misprediction\nMichael Mitzenmacher\u0003\nAbstract\nIn many traditional job scheduling settings, it is assumed that one knows the time it will take for a job to\ncomplete service. In such cases, strategies such as shortest job ﬁrst can be used to improve performance in terms\nof measures such as the average time a job waits in the system. We consider the setting where the service time\nis not known, but is predicted by for example a machine learning algorithm. Our main result is the derivation,\nunder natural assumptions, of formulae for the performance of several strategies for queueing systems that use\npredictions for service times in order to schedule jobs. As part of our analysis, we suggest the framework of the\n“price of misprediction,” which offers a measure of the cost of using predicted information.\nI. I NTRODUCTION\nWhile machine learning research seems to be growing at an exponential rate, there seems to be surprisingly\nlittle overlap with “traditional” algorithms and data structures and their analysis. Here we attempt to bridge this\ngap for the area of job scheduling, providing a general framework that may prove useful for additional problems.\nWhile we begin with settings with a ﬁnite number of jobs in order to provide insight into our approach, our\nmain results are in the area of queueing systems. In this setting, we assume there is some algorithm (such as\na neural network or other machine learning algorithm) that predicts the job time1upon entry; we model this\npredictor via a density function g(x;y), so thatg(x;y)is the probability density for a job having actual service\ntimexand predicted service time y. We emphasize that only the predicted service time is known to the system on\nthe job’s arrival, and we need not assume that the joint distribution is known in order to perform the scheduling.\nRather, we use the joint distribution to derive equations for queue performance.\nIn standard queueing theory, under standard assumptions such as Poisson arrivals, and independent service\ntimes, one can derive formulae for the behavior of many natural scheduling strategies, including shortest job ﬁrst\n(SJF) and shortest remaining processing time (SRPT), which both minimize the average time a job spends in the\nsystem. (Shortest job ﬁrst assumes no preemption; shortest remaining processing time allows preemption.) In the\nsetting where job times are predicted, we refer to the corresponding natural strategies as shortest predicted job\nﬁrst(SPJF) and shortest predicted remanining processing time (SPRPT). Our main result is to derive formulae for\nthe expected time a job spends in the system for such strategies; the formulae can be computed in terms of the\ndensity function g(x;y). We further provide some empirical evidence from simulations that even weak predictions\ncan yield very good performance.\nMore generally, we consider the cost of using predictions in place of accurate job service times, and introduce\nthe concept of the price of misprediction to describe this cost. Our results provide the price of misprediction for\nthese basic strategies.\nWe emphasize that our goal here is notto develop speciﬁc prediction methods, and we do not do so in this\nwork. Rather, our goal is to show that given a real or hypothetical prediction system matching our assumptions,\nwe can develop equations for its performance. This general framework may apply to a variety of machine learning\nmethods. In this way, we aim to extend traditional queueing theoretic formulations to the general setting where\nmachine learning prediction systems are available. Just as queueing theoretic models and results have historically\nguided many real-world systems (see, e.g., [ 11], [15], [16] for background), our motivation stems from the idea\nthat extending such models and results to setting with predictions will enhance the use of machine learning\nprediction in real-world systems that use queues. Indeed, we further hope that our approach may prove useful for\nthe analysis of other traditional algorithms and data structures.\n\u0003School of Engineering and Applied Sciences, Harvard University. michaelm@eecs.harvard.edu . This work was supported in\npart by NSF grants CCF-1563710 and CCF-1535795.\n1We use the terms job time, service time, and processing time interchangeably; historically these different terms have all been used.arXiv:1902.00732v2  [cs.DS]  23 May 2019\n\nII. R ELATED WORK\nWhile traditional algorithmic analysis focuses on worst-case algorithm behavior, there is a growing movement\nto develop frameworks that go beyond worst-case analysis [ 26]. While such frameworks have existed in the past,\nmost notably via probabilistic analysis (e.g., [ 23]), semi-random models (e.g., [ 7], [8]), and smoothed analysis\n[31], one natural approach that has received little attention is the use of machine-learning-based approaches to\nprovide predictions to algorithms, with the goal of realizing provable performance guarantees. (The idea of using\nmachine learning to give hints as to which heuristic algorithm to employ has been considered in meta-heuristics\nfor several large-scale problems, most notably for satisﬁability [32]; this is a distinct line of work.)\nNotable recent work with this theme is that of Lykouris and Vassilvitskii [ 19], who show how to use prediction\nadvice from machine learning algorithms to improve online algorithms for caching in a way that provides provable\nperformance guarantees, using the framework of competitive analysis. A series of recent papers consider the\nsetting of optimization with noise, such as in settings when sampling data in order to obtain values used in an\noptimization algorithm for submodular functions [ 2], [13], [3], [4], [5], [25]. Other recent works analyze the\nperformance of learned Bloom ﬁlter structures [ 18], [22], a variation on Bloom ﬁlters [ 6] that make use of machine\nlearning algorithms that predict whether an element is in a given ﬁxed set as a subﬁlter structure, and heavy\nhitter algorithms that use predictions [ 14]. One prior work in this vein has speciﬁcally looked at scheduling with\npredictions in the setting of a ﬁxed collection of jobs, and considered variants of shortest predicted processing time\nthat yield good performance in terms of the competitive ratio, with the performance depending on the accuracy of\nthe predictions [28].\nIn scheduling, some works have looked at the effects of using imprecise information, usually for load balancing\nin multiple queue settings. For example, Mitzenmacher considers using old load information to place jobs (in\nthe context of the power of two choices) [ 20]. A strategy called TAGS studies an approach to utilizing multiple\nqueues when no information exists about the service time; jobs that run more than some threshold in the ﬁrst\nqueue are cancelled and passed to the second queue, and so on [ 10]. For single queues, recent work by Scully and\nHarchol-Balter have considered scheduling policies that are based on the amount of service received, where the\nscheduler only knows the service received approximately, subject to adversarial noise, and the goal is to develop\nrobust policies [ 29]. Our work differs from these past works in providing a model speciﬁcally geared toward\nstudying performance with machine-learning based predictions, and corresponding analyses.\nFinally, we note that our policies appear to ﬁt within the more general framework of SOAP policies presented\nby Scully et al. [ 30] This provides an alternative approach to analyzing the policies studied here. We present\nderivations here based on the original analyses of SJF and SRPT, as we feel they are more instructive and\nstraightforward.\nIII. P RICE OF MISPREDICTION\nA. A Simple Example\nTo demonstrate our framework, we start with a simple example. Consider a collection of njobsj1;:::;jn,\neach of one of two types, short or long. Short jobs require time sto process and long jobs require time `to\nprocess, with s < ` . Jobs are to be ordered and then processed sequentially. When the job times are known,\nshortest job ﬁrst is known to minimize the average waiting time over all jobs. (Here and throughout waiting\ntime is the time spent in the system before starting being served.) If there are nsshort jobs and n`long jobs the\naverage waiting time is\n1\nn\u0012\nnsns\u00001\n2s+n`n`\u00001\n2`+n`nss\u0013\n:\nThat is, on average each of the nsjobs waits for half of the remaining ns\u00001short jobs, and similarly for the\nlong jobs, and the long jobs further have to wait for all of the short jobs.\nWe note that asymptotically if we drop lower order terms this is approximately\nn2\nss+n2\n``+ 2nsn`s\n2n:\n\nFor simplicity, we will generally work with asymptotic expressions throughout.\nIf one has no information about the type of all of the jobs, the optimal policy (under the assumption that an\nadversary can present the jobs in a worst-case order) is to randomize the order of the jobs. In this case, using\nlinearity of expectations, we can ﬁnd the overall expected waiting time by ﬁnding the expected waiting time of\neach job. A simple calculation shows that if there are nsshort jobs and n`long jobs the expected waiting time is\n1\nn\u0012\nns\u0012ns\u00001\n2s+n`\n2`\u0013\n+n`\u0012ns\n2s+n`\u00001\n2`\u0013\u0013\n:\nAsymptotically, this is approximately\nn2\nss+n2\n``+nsn`(s+`)\n2n:\nFinally, in the prediction setting, when a job’s type is predicted we assume short jobs are misclassiﬁed as long\njobs with some probability pand long jobs are misclassiﬁed as short jobs with some probability q. We consider\nthe policy of shortest-predicted-job-ﬁrst ; that is, we apply shortest-job-ﬁrst based on the predictions. Our analysis\nrequires multiple cases, as we must consider the expected waiting time for a job conditioned on whether it was\nclassiﬁed correctly or incorrectly. Considering all of these four cases leads to the following expression for the\nexpected waiting time when mispredictions occur:\n1\nn\u0012\n(1\u0000p)ns\u0012(1\u0000p)(ns\u00001)\n2s+qn`\n2`\u0013\n+pns\u0012\n(1\u0000p)(ns\u00001)s+p(ns\u00001)\n2s+(1\u0000q)n`\n2`+qn``\u0013\n+ (1\u0000q)n`\u0012(1\u0000q)(n`\u00001)\n2`+q(n`\u00001)`+pns\n2s+ (1\u0000p)nss\u0013\n+qn`\u0012q(n`\u00001)\n2`+(1\u0000p)ns\n2s\u0013\u0013\n:\nAsymptotically, this is approximately\nn2\nss+n2\n``+nsn`((2\u0000(p+q))s+ (p+q)`)\n2n:\nThis differs from the optimal (asymptotic) expression additively by(p+q)(`\u0000s)\n2n; it depends speciﬁcally on the “total\nerror”p+q.\nFollowing the standard terminology2, we might refer to the ratio between the expected waiting time with\nimperfect information and the expected waiting time with perfect information as the price of misprediction . We\npropose the following deﬁnition:\nDeﬁnition 1: LetMA(Q;I)be the value of some measure (such as the expected waiting time) for a system Q\ngiven information Iabout the system using algorithm A, and letMA(Q;P)be the value of that metric using\npredicted information Pin place of Iwhen using algorithm A. Then the price of misprediction is deﬁned as\nMA(Q;I)=MA(Q;P).\nIn the example of short and long jobs above, the asymptotic price of misprediction for the waiting time is the\nratioRgiven by:\nR=n2\nss+n2\n``+nsn`((2\u0000(p+q))s+ (p+q)`)\nn2ss+n2\n``+nsn`(s+`)\nWe can ﬁnd where the ratio is maximized by considering ns=\rnfor a constant \r. Some algebraic work yields\nthat:\nR\u00141 +(p+q)(p\nl=s\u00001)\n2;\ngiving a bound on the price of misprediction. Note that the setting with no information, or a random ordering, is\nequivalent to the case p=q= 1=2, in which case Ris bounded by 1 + (p\nl=s\u00001)=2.\n2The terms “price of anarchy” [ 17] and “price of stability” [ 1] are commonly used in game theoretic situations, and in particular in job\nscheduling, when multiple players act in their own self interest instead of cooperating. One could also view this as a multiplicative form of\nregret, but we think this terminology is more general and potentially helpful.\n\nB. General Predictions\nMore generally, we can consider a setting where each job can be described as an independent random variable,\nwhere the random variable for a job is given by a density distribution g(x;y); that is,g(x;y)is the density\nfunction for a job, where a job has service time xand predicted service time y. We assume that g(x;y)is\n“well-behaved” throughout this work, so that it is continuous and all necessary derivatives exist. (The analysis can\nbe readily modiﬁed to handle point masses or other discontinuities in the distribution.)\nIt is convenient to let fs(x) =R1\ny=0g(x;y)dybe the density function for the service time, and fp(y) =R1\nx=0g(x;y)dxbe the density function for the predicted service time. If there are ntotal jobs, the expected\nwaiting time for a job using shortest job ﬁrst given full information is given by\n(n\u00001)Z1\nx=0fs(x)\u0012Zx\nz=0zfs(z)dz\u0013\ndx;\nwhile the expected waiting time for a job using predicted information using shortest predicted job ﬁrst is given by\n(n\u00001)Z1\ny=0fp(y)\u0012Z1\nx=0Zy\nz=0xg(x;z)dzdx\u0013\ndy:\nIn words, in the full information case, to compute the expected waiting time for a job, given its service time, we\ncan determine the probability each other job has a smaller service time and the conditional value of that smaller\nservice time to compute the waiting time. In the predicted information case, to compute the expected waiting\ntime for a job given its predicted service time, we must determine the probability each other job has a smaller\npredicted service time and the conditional value of the actual service time of a job given that it has a smaller\npredicted service time to compute the waiting time.\nNote that the factors of n\u00001are cancelled in the ratio of the expected waiting times, and the function\ngsufﬁces to determine to price of misprediction. As an example motivated by the prevalence of exponential\ndistributions in queueing theory, suppose that service times are exponentially distributed with mean 1, and a job\nwith service time xhas a prediction that is distributed according to an exponential distribution with mean x.\nTheng(x;y) =e\u0000x\u0000y=x=x,fs(x) =e\u0000x, andfp(y) =R1\nx=0e\u0000x\u0000y=x\nxdx. We notefp(y)does not appear to have\na simple closed form, though it is expressible in terms of Bessel functions.3We note numerical calculations\nfrom these integrals readily reveal that the price of misprediction appears to be 4=3, and it can be shown to be\nexactly 4=3using the integration features of Mathematica. We can prove it is exactly equal to 4=3through a\nsubtle argument allowing us to evaluate the corresponding integral for the expected waiting time for a job using\npredicted information; this argument is given in the appendix. In comparison, using no information and just\nscheduling in sequential order the expected waiting time is a factor of 2 worse than when using full information.\nThis example, while not meant to match a real-world example,provides the right high-level intuition, in that it\nshows that even a weak predictor can yield signiﬁcant improvements. Indeed, this is natural; for a predictor to\nwork well in this setting, it simply has to order most of the jobs correctly in the queue.\nThe key here is that the price of misprediction can be computed (at least numerically) given the density\ndistributiong. In practice, one might use this framework to determine the beneﬁt of using a predictor; for example,\none might seek to trade off the reduction in total waiting time with the cost of developing or using better prediction\nmethods. While gmay not be known exactly, we expect in practice good approximations for gcan be determined\nempirically, which in turn will allow a good approximation for the price of misprediction or related quantities.\nWe note that, for suitably good prediction schemes, ordering by predicted service time should naturally\ncorrespond to ordering by the expected service time. That is, denote a job by (X;Y ), whereXis a random\nvariable representing the true service time and Yis a random variable representing the predicted service time.\nThen suppose the density distribution gsatisﬁes for any y1;y2withy1<y 2the natural inequality\nE[XjY=y1]<E[XjY=y2]:\nThen ordering by predicted service times yield an ordering according to expected service times, and ordering by\nexpected service times is known to be optimal for minimizing the expected waiting time.\nWe now extend these ideas to queueing theoretical models.\n3This was determined using the integration features of Mathematica 11.3.\n\nIV. S INGLE QUEUE MODELS\nIn this section, we present results providing formulae for prediction-based variants of shortest job ﬁrst and\nshortest remaining processing time for single queue systems, which yield expressions for the price of misprediction.\nWe brieﬂy review the appropriate analysis methods for standard queues, starting with jobs with priorities, and\nthen extend them prediction setting.\nA. Priority-based Systems\nConsider a queueing system with ktypes of jobs, t1;:::;tk. We assume Poisson arrivals, and that the arrival\nrate for type tiis\u0015i, withPk\ni=1\u0015i=\u0015. A natural setting is that the ith type of job has service time qi, with\nq1<q 2<:::<q k. In this case, with complete information about the service times, the shortest job ﬁrst (SJF)\nstrategy (without preemption) corresponds to a priority-based strategy, with the types corresponding to priorities;\nt1has the highest priority, and so on. More generally, the ith type of job may have a service time distribution,\nrather than a ﬁxed service time. We describe this more general case, where job tihas service distribution Si; here\nthe natural setting is E[S1]<E[S2]<:::< E[Sk], and if the types are prioritized by expected service time, the\nstrategy is expected shortest job ﬁrst (ESJF).\nWe describe some standard formula for priority systems, following the framework of [ 11]. Let\u001ai=\u0015iE[Si];\nthis represents the load on the system from jobs of type i. Further, let \u001a=Pk\ni=1\u001ai, andW(i)be the distribution\nof the waiting time in the queue for jobs of type iin equilibrium. Also, let Sbe service time distribution of an\nincoming job, so E[S] =Pk\ni=1\u0015iE[Si]=\u0015, and\u001a=\u0015E[S]. Then the following is known (see Equation (31.1) of\n[11]):\nE[W(i)] =\u001aE[S2]\n2E[S]\u0010\n1\u0000Pi\nj=1\u001aj\u0011\u0010\n1\u0000Pi\u00001\nj=1\u001aj\u0011:\nWe now consider a system where types are not known but are predicted, for example according to a machine\nlearning algorithm. For convenience, going forward, we refer to the true type of a job for its type, and refer to\nthe machine prediction for a job as the predicted type where appropriate. We may represent the machine learning\nalgorithm by a matrix Mwheremijis the probability that a job of true type ihas predicted type j; here we are\nassuming that each job labelling can be treated as independent. In this case, let \u00150\nibe the arrival rate of jobs with\npredicted type i. Then\n\u00150\ni=kX\nj=1\u0015jmji:\nCorrespondingly, the distribution of service times for jobs having predicted type iis that the job has service time\ngiven byS`with probability\n\u0015`m`i=kX\nj=1\u0015jmji:\nIf we useS0\nito represent the distribution of service times for jobs having predicted type i, then\nE[S0\ni] =Pk\nj=1\u0015jE[Sj]mjiPk\nj=1\u0015jmji:\nOf course the expected service time S0over all jobs is\nE[S0] =kX\ni=1\u0015iE[Si] =E[S]:\n\nAssuming we prioritize jobs now according to their predicted type, we may again use the standard formula for\npriority systems. We derive the corresponding result. First, let\n\u001a0\ni=\u00150\niE[Si] =kX\nj=1\u0015jE[Sj]mji:\nAlso letW0(i)be the distribution of the waiting time in the queue for jobs of predicted type iin equilibrium.\nThen\nE[W0(i)] =\u001a0E[(S0)2]\n2E[(S0)]\u0010\n1\u0000Pi\nj=1\u001a0\nj\u0011\u0010\n1\u0000Pi\u00001\nj=1\u001a0\nj\u0011\n=\u001aE[S2]\n2E[S]\u0010\n1\u0000Pi\nj=1\u001a0\nj\u0011\u0010\n1\u0000Pi\u00001\nj=1\u001a0\nj\u0011:\nHence, by summing over all possible types, we can see that the price of misprediction for the expected waiting\ntime corresponds to the following expression:\nPk\ni=1\u00150\ni\u0010\u0010\n1\u0000Pi\nj=1\u001a0\nj\u0011\u0010\n1\u0000Pi\u00001\nj=1\u001a0\nj\u0011\u0011\u00001\nPk\ni=1\u0015i\u0010\u0010\n1\u0000Pi\nj=1\u001aj\u0011\u0010\n1\u0000Pi\u00001\nj=1\u001aj\u0011\u0011\u00001:\nB. Shortest Job First\nWe now show that the performance of shortest predicted job ﬁrst, which we denote as SPJF, can be readily\nexpressed as a limiting case of the priority analysis, similarly to how shortest job ﬁrst is the limiting case of a\npriority queue based on service time. (Here we roughly follow the methodology of Section 31.3 of [ 11].) To start,\nwe recall the formula for shortest job ﬁrst; this is easily obtained as the limit of the priority system setting, where\nthere are an inﬁnitely many possible “priorities”, and the priority corresponds to the service time. Here again\nletSbe the service distribution of an incoming job. Further, let fs(x)be the corresponding density function,\n\u001ax=\u0015Rx\nt=0tfs(t)dt, and\u001a=\u0015R1\nt=0tfs(t)dt. We consider W(x), the time spent waiting in the queue (not being\nserved) for jobs with service time xin equilibrium. Then for standard shortest job ﬁrst without preemption, where\nwe know the exact service times without prediction, it is known that\nE[W(x)] =\u001aE[S2]\n2E[S] (1\u0000\u001ax)2:\nThe overall expected time waiting in a queue, which we denote by E[W]whereWis the waiting time in queue\nof an incoming job, is then simply\nE[W] =Z1\nx=0f(x)E[W(x)]dx:\nWe now generalize this to SPJF. For a non-preemptive queue that uses a service time estimate, if g(x;y)is the\njoint distribution that a job has service time xand predicted service time y, we again let fs(x) =R1\ny=0g(x;y)dy\nbe the density function for the service time, and fp(y) =R1\nx=0g(x;y)dxbe the density function for the predicted\nservice time. \u001a0\ny=\u0015Ry\nt=0R1\nx=0xg(x;t)dxdt to be the load on the system associated with jobs of predicted service\ntime up toy. With the assumption that each job’s service time characteristics are independently determined\naccording to g(x;y), if we letW0(y)be the distribution of time spent waiting in the queue for a job with predicted\nservice time yin equilibrium, then\nE[W0(y)] =\u001aE[S2]\n2E[S]\u0000\n1\u0000\u001a0y\u00012;\n\nwhereW0(y)is the distribution of time in the queue for jobs with service time y. Integrating over service time or\npredicted services time gives us that the price of misprediction is given by:\nR1\ny=0fp(y)\n(1\u0000\u001a0\ny)2dy\nR1\nx=0fs(x)\n(1\u0000\u001ax)2dx:\nLet us again consider the example of service times that are exponentially distributed with mean 1, where a job\nwith service time xhas a prediction that is distributed according to an exponential distribution with mean x. Then\nthe price of misprediction can be expressed as\nR1\ny=0R1\nx=0e\u0000x\u0000y=x\nxdx\n(1\u0000\u0015Ry\nt=0R1\nx=0e\u0000x\u0000y=xdxdt )2dy\nR1\nx=0e\u0000x\n(1\u0000\u0015(1\u0000(x+1)e\u0000x))2dx:\nWhile there does not appear to be a simple closed form for this expression, it can be readily evaluated numerically\nfor a given\u0015.\nWe note that a similar analysis can be used for preemptive shortest predicted job ﬁrst (PSPJF), where a job\nmay be preempted by another job that has an originally shorter predicted time (note that the time a job has been\nserviced is not considered). This is because preemptive shortest job ﬁrst (PSJF) can be represented as the limit\nof a preemptive priority-based system (as in Section 32.3 of [ 11]), leading to a similar analysis. (We provide\nthe analysis in the appendix.) Also, we can consider variations where the machine learning algorithm returns a\ndistribution (described by a small number of parameters) as a prediction; for example, the prediction might be the\nservice time is exponential with mean \r. In this case, we can use the shortest predicted expected processing time\n(SPEPT), which reduces readily to the analysis of shortest predicted job ﬁrst.\nC. Shortest Remaining Processing Time\nA more challenging variation involves extending the shortest remaining processing time (SRPT) policy to\npredictions. With complete information, SRPT maintains the remaining processing time for each job, and the job\nbeing processed can be preempted by an incoming job with service time smaller than the remaining processing\ntime. To generalize to the prediction setting, we follow the framework of Schrage and Miller [ 27], who presented\nan analysis of SRPT. (See also [ 9] for a similar derivation, or [ 11] for an alternative.) Because the system is\npreemptive, it makes sense to consider the total time in the system, rather than the waiting time (as jobs may\nhave further waits after they start being served). Because of the complexity of the expressions, we do not have a\nclean form for the price of misprediction, but they can be found from the derived formulae.\nWe again use g(x;y)for the joint distribution that a job has service time xand predicted service time y, and\nletfs(x) =R1\ny=0g(x;y)dy,fp(y) =R1\nx=0g(x;y)dx,\u001ax=\u0015Rx\nt=0tfs(t)dt, and\u001a0\ny=\u0015Ry\nt=0R1\nx=0g(x;t)xdxdt .\nThe expected time in the system in equilibrium for a job can be expressed as the sum of its residence time (time\nin the system once it has started receiving service) and it waiting time (time spent waiting before being served).\nFor SRPT, a job with service time xhas mean residence time\nZx\nt=0dt\n1\u0000\u001at;\none can think of this as the remaining service times drop from tto 0, at any instant there is a possible addition\n(given by the 1=(1\u0000\u001at)factor) due to preemptions.\nThe corresponding mean residence time for a job of service time xand predicted service time yunder SPRPT\nisZx\nt=0dt\n1\u0000\u001a0\n(y\u0000t)+:\nThat is, here the predicted remaining processing time drops from yto(y\u0000t)+= max(y\u0000t;0); it is possible the\npredicted remaining processing time is 0, but the job continues to require service, in which case we leave its\n\npredicted remaining processing time at 0, and it cannot be preempted. It follows that the mean residence time\nE[R(y)]for a job of predicted service time yis\nE[R(y)] =Z1\nx=0g(x;y)\nfp(y)Zx\nt=0dt\n1\u0000\u001a0\n(y\u0000t)+dx:\nWe now compute the waiting time, which is more difﬁcult. The steady-state probability that an arriving job\nﬁnds the server working on a job whose remaining predicted processing time is less than qis given by\nb(q) =\u001a0\nq+\u0015Z1\nt=qZ1\nx=0g(x;t)(x\u0000(t\u0000q))+dxdt:\nThe ﬁrst term comes from arrivals with predicted service time less than q; the second term comes from jobs that\nstart with predicted service time greater than q, but later their remaining predicted service time falls below q.\nIfY(q)is the length of a busy period where all jobs processed have predicted remaining processing times less\nthanq, then the waiting time W(q)for a job of predicted service time qis given by:\nE[W(q)] =b(q)E[Y(q)2]\n2E[Y(q)]:\nTo ﬁnd the ﬁrst two moments of Y(q), we use the fact that the length of the busy period Y(q)has the same\ndistribution as the busy period for a ﬁrst-come, ﬁrst-served server where the job that initiates the busy period has\nprocessing time according to some distribution Z(q), where additional jobs have a processing time distribution\nX(q), and the arrivals are Poisson with rate \u0015Fp(q), forFp(q) =Rq\nx=0fp(x)dx. We require the ﬁrst two moments\nofZ(q)andX(q).\nThe moments for X(q)are fairly straightforward, as X(q)corresponds to the processing time of a job with\npredicted processing time at most q:\nE[X(q)] =1\nFp(q)Zq\nt=0Z1\nx=0g(x;t)xdxdt;\nand\nE[X(q)2] =1\nFp(q)Zq\nt=0Z1\nx=0g(x;t)x2dxdt:\nTo determine the ﬁrst two moments of Z(q), we note that there are two ways a job can start the corresponding\nbusy period. It either arrives when a busy period is not in progress and has predicted processing time at most q, or\nit is a job with predicted processing time greater than q(which starts a busy period when the predicted processing\ntime reaches q). Note that in the second case, if the predicted processing time tis greater than q, but the actual\nprocessing time xis such that x<t\u0000q, then the job cannot start a busy period, as the job will ﬁnish before the\nremaining predicted processing time reaches q. (Ideally, such situations should not occur with a suitably good\npredictor, but it must be taken into account.) Hence the probability a job initiates a corresponding busy period is\nd(q) = (1\u0000b(q))Fp(q) +Z1\nt=qZ1\nx=t\u0000qg(x;t)dxdt:\nIf we let (for typesetting reasons)\na1(q) = (1\u0000b(q))Zq\nt=0Z1\nx=0g(x;t)xdxdt\n+Z1\nt=qZ1\nx=t\u0000qg(x;t)(x\u0000(t\u0000q))dxdt\nand\na2(q) = (1\u0000b(q))Zq\nt=0Z1\nx=0g(x;t)x2dxdt\n+Z1\nt=qZ1\nx=t\u0000qg(x;t)(x\u0000(t\u0000q))2dxdt\n\nthen\nE[Z(q)] =a1(q)=d(q);\nand\nE[Z(q)2] =a2(q)=d(q):\nWe now use the facts (see, e.g., Problem 49 of [9])\nE[Y(q)] =E[Z(q)]\n1\u0000\u001a0q\nand\nE[Y(q)2] =E[Z(q)2]\n(1\u0000\u001a0q)2+\u0015E[Z(q)]Fp(q)E[X(q)2]\n(1\u0000\u001a0q)3:\nThis yields\nE[W(q)] =b(q)\u0012a2(q)\n2a1(q)(1\u0000\u001a0q)+\u0015Fp(q)E[X(q)2]\n2(1\u0000\u001a0q)2\u0013\n:\nThe expected time in the system for a job is simplyR1\ny=0fp(y)E[W(y) +R(y)]dy. From this value (and the\ncorresponding equations for standard SRPT) one can compute the price of misprediction for the total expected\ntime in the system. (Of course, the expected service time can be subtracted if desired.)\nV. S IMULATION RESULTS\nWe present a small number of simulation results to demonstrate that our equations are accurate and, at least in\nthe cases we have examined, the price of misprediction is generally small. Correspondingly, this implies that\neven a small amount of predictive power yields signiﬁcantly better performance than standard First-In First-Out\n(FIFO) queueing. We focus on high load settings, as under low load all systems perform well. We also note that\nadditional simulations we have performed further substantiate our high-level conclusions.\nWe ﬁrst compare simulation results against the results from our equations; we also provide results for schemes\nwith full information for comparison. Our results are for the setting with Poisson arrivals, service times are\nexponential with mean 1, and predicted service times are exponential with mean xwhen the actual service time\nisx. For consistency, we provide the total expected time in the system (waiting and service). The results of the\nequations were computed using Mathematica 11.3 and numerical integration. The calculations for SPRPT are\nsomewhat lengthy and can lead to numerical stability issues; we found integrating up to predicted times of at most\n50 gives accurate answers while being computable in reasonable time, approximately half an hour on a modern\nlaptop. (Predicted service times greater than 50 are very rare; they occur with probability less than 5\u000110\u00007.) We\ndid not optimize the calculations and expect this could be improved. The results of simulations were from our\nown implementation of a queue simulator. The simulations are the results of averaging the average time over\n1000 trials, where in each trial we recorded the time in system of each completed job. The trials were each run\nfor1 000 000 time units, with jobs completing in the ﬁrst 100 000 time units discarded from the calculations of\nthe averages to remove bias from starting with an empty system.\nTable I shows both that the results from equations for SPJF match very closely to the simulation results, and\nthat the performance is not too much worse than when the service times are known. With regards to accuracy, the\ndifference is less than 1%. With regard to performance, using predicted times naturally becomes increasingly\nworse as load grows, but the difference still shows the beneﬁts of using imperfect information. Recall that, with\nno information, for standard queueing schemes such as FIFO the expected time in the system is 1=(1\u0000\u0015); for\nexample,\u0015= 0:99leads to an expected time in the system of 100. We see that under high loads, the gains from\nprediction remain substantial.\nTable II shows similar results for the same simulation setting using SRPT and SPRPT. For SPRPT, the results\nfrom equations align a little less closely to the simulation results, but the difference remains than 1%. Given the\ncomplexity of the equations, and the higher variability in the time in system for SPRTP, this is unsurprising.\n\nSJF SJF SPJF SPJF FIFO\n\u0015 Eqns Sims Eqns Sims Eqns\n0.5 1.7127 1.7128 1.7948 1.7949 2.00\n0.6 1.9625 1.9625 2.1086 2.1087 2.50\n0.7 2.3122 2.3121 2.5726 2.5730 3.33\n0.8 2.8822 2.8828 3.3758 3.3760 5.00\n0.9 4.1969 4.1987 5.3610 5.3609 10.00\n0.95 6.2640 6.2701 8.6537 8.6541 20.00\n0.98 11.2849 11.2734 16.9502 16.9782 50.00\n0.99 18.4507 18.4237 29.0536 29.1162 100.00\nTABLE I\nRESULTS FROM SIMULATIONS AND EQUATIONS FOR SHORTEST JOBFIRST (SJF) AND SHORTEST PREDICTED JOBFIRST (SPJF).\nSRPT SRPT SPRPT SPRPT FIFO\n\u0015 Eqns Sim Eqns Sim Eqns\n0.5 1.4254 1.4251 1.6531 1.6588 2.00\n0.6 1.6041 1.6039 1.9305 1.9397 2.50\n0.7 1.8746 1.8757 2.3539 2.3684 3.33\n0.8 2.3528 2.3519 3.1168 3.1376 5.00\n0.9 3.5521 3.5486 5.04808 5.0973 10.00\n0.95 5.5410 5.5466 8.3221 8.4075 20.00\n0.98 10.4947 10.5003 16.6239 16.7852 50.00\n0.99 17.6269 17.6130 28.7302 28.7847 100.00\nTABLE II\nRESULTS FROM SIMULATIONS AND EQUATIONS FOR SHORTEST REMAINING PROCESSING TIME(SRPT) AND SHORTEST PREDICTED\nREMAINING PROCESSING TIME(SPRPT).\nFigure 1 provides another example of prediction performance. Here we ﬁx \u0015= 0:95, and consider two types of\nservice distributions: exponential with mean 1, and a Weibull distribution with cumulative distribution 1\u0000e\u0000p\n2x.\n(The Weibull distribution is more heavy-tailed, but also has mean 1.) The simulations are again the average of\nthe measured time in system, averaged from results of 1000 trials, in the same manner as previously. Here the\npredictions depend on a scale parameter \u000b; a job with service time xhas a predicted service time that is uniform\nover[(1\u0000\u000b)x;(1 +\u000b)x]. By varying \u000b, we can see the impact on performance as prediction accuracy diminishes.\nNote that when \u000b= 0 the predicted service time equals the true service time. In these examples, we observe that\nperformance degrades gracefully with \u000b, a feature we see across values of \u0015in other experiments not presented.\nThe main point here is that even weak predictors may perform well under SPJF and SPRPT; as long as they\ngenerally lead jobs to be processed in the right order, they can yield substantial beneﬁts. (We note the standard\ndeviation over trials ranges from 2-4%, with higher variance for simulations with the Weibull distribution.)\nVI. C ONCLUSION\nWe have demonstrated that the analyses of various single-queue job scheduling approaches can be generalized\nto the setting where predicted service times are used in place of true values, under the assumption that the\npredictions can be modeled as joint distribution with a corresponding density function. Such analyses can be used\nto determine the price of misprediction, or the potential beneﬁts of better prediction, for such systems.\nIn future work, we plan to provide analyses of multiple queue systems using predicted service times. Multiple\nqueue systems are quite common in practice, but can have more highly variable performance depending on how\nthe workload is divided among queues. Natural strategies to consider include the power of two choices [ 21] and\n\n 0 2 4 6 8 10\n 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9average time in system\nalpha: prediction parameterSPJF (exp)\nSPRPT (exp)\nSPJF (wei)\nSPRPT (wei)Fig. 1. Results from simulations at \u0015= 0:95for exponential and Weibull distributions. A job with service time xhas predicted service\ntime uniform over [(1\u0000\u000b)x;(1 +\u000b)x]. Performance degrades gracefully with \u000b. Note\u000b= 0 corresponds to the full information case, as\nthen the predicted service time equals the true service time.\nsize interval task assignment (SITA) [ 12]. We expect analysis of such systems may require additional techniques,\nbut will show that in this setting also even mildly accurate predictions can provide signiﬁcant value.\nIn the problems considered here, we were able to determine exact formulae for performance, based on our\nprobabilistic assumptions. It would be interesting to consider more general job scheduling scenarios with fewer\nassumptions, perhaps using methods more akin to online analysis, as in [19].\nWe believe this work suggests there is great potential in analyzing the large variety of job scheduling problems,\nas well as other similar traditional algorithmic problems, in the context of prediction.\n\nREFERENCES\n[1]Elliot Anshelevich, Anirban Dasgupta, Jon Kleinberg, Eva Tardos, Tom Wexler, and Tim Roughgarden. The price of stability for\nnetwork design with fair cost allocation. SIAM Journal on Computing , 38(4):1602–1623, 2008.\n[2]Eric Balkanski, Aviad Rubinstein, and Yaron Singer. The power of optimization from samples. In Advances in Neural Information\nProcessing Systems , pp. 4017–4025, 2016.\n[3]Eric Balkanski, Aviad Rubinstein, and Yaron Singer. The limitations of optimization from samples. In Proceedings of the 49th\nAnnual ACM Symposium on Theory of Computing , pp. 1016–1027, 2017.\n[4]Eric Balkanski and Yaron Singer. The sample complexity of optimizing a convex function. In Proceedings of the 30th Conference on\nLearning Theory , pp. 275–301, 2017.\n[5]Eric Balkanski and Yaron Singer. Approximation guarantees for adaptive sampling. In Proceedings of the 35th International\nConference on Machine Learning , pp. 393–402, 2018.\n[6] Burton H Bloom. Space/time trade-offs in hash coding with allowable errors. Communications of the ACM , 13(7):422–426, 1970.\n[7]Avrim Blum and Joel Spencer. Coloring random and semi-random k-colorable graphs. Journal of Algorithms , 19(2):204–234, 1995.\n[8]Uriel Feige and Robert Krauthgamer. Finding and certifying a large hidden clique in a semirandom graph. Random Structures &\nAlgorithms , 16(2):195–208, 2000.\n[9] Natarajan Gautam. Analysis of queues: methods and applications . CRC Press, 2012.\n[10] Mor Harchol-Balter. Task assignment with unknown duration. J. ACM , 49(2):260–288, 2002.\n[11] Mor Harchol-Balter. Performance modeling and design of computer systems: queueing theory in action . Cambridge University Press,\n2013.\n[12] Mor Harchol-Balter, Mark E Crovella, and Cristina D Murta. On choosing a task assignment policy for a distributed server system.\nJournal of Parallel and Distributed Computing , 59(2):204–228, 1999.\n[13] Avinatan Hassidim and Yaron Singer. Submodular optimization under noise. In Proceedings of the 30th Conference on Learning\nTheory , pp. 1069–1122, 2017.\n[14] Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency estimation algorithms. International Conference\non Learning Representations , 2019.\n[15] Leonard Kleinrock. Queueing systems, volume 1 . Wiley, New York, 1975.\n[16] Leonard Kleinrock. Queueing systems, volume 2: Computer applications . Wiley, New York, 1976.\n[17] Elias Koutsoupias and Christos Papadimitriou. Worst-case equilibria. In Annual Symposium on Theoretical Aspects of Computer\nScience , pages 404–413, 1999.\n[18] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned index structures. In Proceedings of\nthe 2018 International Conference on Management of Data , pages 489–504, 2018.\n[19] Thodoris Lykouris and Sergei Vassilvitskii. Competitive caching with machine learned advice. In Proceedings of the 35th International\nConference on Machine Learning , pp. 3302–3311, 2018.\n[20] Michael Mitzenmacher. How useful is old information? IEEE Trans. Parallel Distrib. Syst. , 11(1):6–20, 2000.\n[21] Michael Mitzenmacher. The power of two choices in randomized load balancing. IEEE Trans. Parallel Distrib. Syst. , 12(10):1094–1104,\n2001.\n[22] Michael Mitzenmacher. A model for learned bloom ﬁlters and optimizing by sandwiching. In Advances in Neural Information\nProcessing Systems , pages 462–471, 2018.\n[23] Michael Mitzenmacher and Eli Upfal. Probability and computing - randomized algorithms and probabilistic analysis . Cambridge\nUniversity Press, 2005.\n[24] Michael Mitzenmacher and Berthold V ¨ocking. The asymptotics of selecting the shortest of two, improved. In Proceedings of the 37th\nAnnual Allerton Conference on Communication, Control, and Computing , pp. 327-328, 1999.\n[25] Nir Rosenfeld, Eric Balkanski, Amir Globerson, and Yaron Singer. Learning to optimize combinatorial functions. In Proceedings of\nthe 35th International Conference on Machine Learning , pp. 4371–4380, 2018.\n[26] Tim Roughgarden. Beyond worst-case analysis. arXiv preprint arXiv:1806.09817 , 2018.\n[27] Linus E Schrage and Louis W Miller. The queue M/G/1 with the shortest remaining processing time discipline. Operations Research ,\n14(4):670–684, 1966.\n[28] Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ml predictions. In Advances in Neural Information\nProcessing Systems , pages 9684–9693, 2018.\n[29] Ziv Scully and Mor Harchol-Balter. SOAP bubbles: robust scheduling under adversarial noise. In Proceedings of the 56th Annual\nAllerton Conference on Communication, Control, and Computing , 2018.\n[30] Ziv Scully and Mor Harchol-Balter and Allen Scheller-Wolf. SOAP : One Clean Analysis of All Age-Based Scheduling Policies. In\nProceedings of the ACM on Measurement and Analysis of Computing Systems , 2018.\n[31] Daniel A Spielman and Shang-Hua Teng. Smoothed analysis of algorithms: Why the simplex algorithm usually takes polynomial\ntime. Journal of the ACM (JACM) , 51(3):385–463, 2004.\n[32] Lin Xu, Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Satzilla: portfolio-based algorithm selection for sat. Journal of\nArtiﬁcial Intelligence Research , 32:565–606, 2008.\n\nAPPENDIX I\nPROOF OF 4=3PRICE OF MISPREDICTION FOR THE FINITE CASE\nWe recall the setting where there are njobs, service times are exponential with mean 1, the predicted service\ntimes are exponential with mean xwhen the actual service time is x, and we seek to determine the expected\nwaiting time. Since we care only about the expectation, we may consider the expected waiting time with just a\npair of jobs; linearity yields the price of misinformation is the same.\nWhen using the correct service times, the expected waiting time of a job with the shortest job ﬁrst is\nZ1\nx=0e\u0000x\u0012Zx\nz=0ze\u0000zdz\u0013\ndx;\nwhich is easily found to evaluate to 1=4. When using predicted service times, the expected waiting time is\nZ1\ny=0fp(y)\u0012Z1\nx=0Zy\nz=0e\u0000x\u0000z=xdzdx\u0013\ndy;\nwherefp(y) =R1\nx=0e\u0000x\u0000y=x\nxdx. This does not appear to evaluate to a closed form of simple functions. However,\nsuppose we use as our prediction an exponentially distributed service time with mean 1=xinstead ofx. This\neffectively reverses the predicted order, but leads to an easier integral calculation. Since the expected waiting time\nfor a job over both orderings is trivially 1, ﬁnding the expected waiting time for the reverse order sufﬁces.\nFor the reversed order,\nfp(y) =Z1\nx=0xe\u0000x\u0000yxdx\n=1\n(y+ 1)2;\nand the integral becomes\nZ1\ny=01\n(y+ 1)2\u0012Z1\nx=0Zy\nz=0x2e\u0000x\u0000xzdzdx\u0013\ndy\n=Z1\ny=01\n(y+ 1)2\u0012Z1\nx=0(xe\u0000x\u0000xe\u0000x\u0000yx)dx\u0013\ndy\n=Z1\ny=01\n(y+ 1)2\u0012Z1\nx=0(xe\u0000x\u0000xe\u0000x\u0000yx)dx\u0013\ndy\n=Z1\ny=0\u00121\n(y+ 1)2\u00001\n(y+ 1)4\u0013\ndy\n= 2=3:\nThe expected waiting time where predictions are exponential with mean xis therefore 1=3, and the price of\nmisprediction is 4=3as claimed.\nAPPENDIX II\nDERIVATION FOR PSPJF\nWe consider the expected time a job spends in the system in equilibrium for preemptive shortest predicted job\nﬁrst (PSPJF), where a job may be preempted by another job that has an originally shorter predicted time (note\nthat the time a job has been serviced is not considered). The analysis is similar to both SPJF and SPRPT.\nHere we consider the expected waiting time and the expected residence time in steady-state. We again use g(x;y)\nfor the joint distribution that a job has service time xand predicted service time y, and letfs(x) =R1\ny=0g(x;y)dy,\nfp(y) =R1\nx=0g(x;y)dx,Fp(y) =Ry\nt=0fp(y)dt, and\u001a0\ny=\u0015Ry\nt=0R1\nx=0g(x;t)xdxdt . As a job will be preempted\nby another job with smaller predicted service time, the mean residence time for a job of service time xand\npredicted service time yis\nx\n1\u0000\u001a0y:\n\nThis is because the residence time with preemptions is the same as the busy period started by a job of length x\nand predicted length y, where the only jobs that need to be considered in the busy period have predicted length\nat mosty. This leads to the additional 1=(1\u0000\u001a0\ny)factor.\nIt follows that the mean residence time E[R(y)]for a job of predicted service time yis\nE[R(y)] =Z1\nx=0xg(x;y)\nfp(y)(1\u0000\u001a0y)dx:\nThe expected waiting time for a job with predicted service time yis the same as for a shortest job ﬁrst system,\nexcept that the job only waits for jobs of predicted service times as most y. It follows that\nE[W(y)] =\u0015Ry\nt=0fp(t)t2dt\n2\u0000\n1\u0000\u001a0y\u00012:\nNote that here we have simpliﬁed the expression, which would originally have had a factor\n\u001a0\ny\u0000Ry\nt=0t2fp(t)dt\u0001\n=Fp(y)\u0000Ry\nt=0tfp(t)dt\u0001\n=Fp(y):\nThe integral expressions are the second and ﬁrst moments of the expected service time for a job with predicted\nservice time at most y. As\u001a0\ny=\u0015Ry\nt=0tfp(t)dt, the expression for E[W(y)]follows.\nThe expected time in the system for a job is then again simplyR1\ny=0fp(y)E[W(y) +R(y)]dy.",
  "textLength": 44107
}