{
  "paperId": "1394c6eaceff6a863f895b86585d33c79c81a464",
  "title": "Learning How To Learn Within An LSM-based Key-Value Store",
  "pdfPath": "1394c6eaceff6a863f895b86585d33c79c81a464.pdf",
  "text": "From WiscKey to Bourbon: A Learned Index for Log-Structured Merge Trees\nYifan Dai, Yien Xu, Aishwarya Ganesan, Ramnatthan Alagappan, Brian Kroth†,\nAndrea C. Arpaci-Dusseau, and Remzi H. Arpaci-Dusseau\nUniversity of Wisconsin – Madison†Microsoft Gray Systems Lab\nAbstract. We introduce BOURBON , a log-structured merge\n(LSM) tree that utilizes machine learning to provide fast\nlookups. We base the design and implementation of\nBOURBON on empirically-grounded principles that we derive\nthrough careful analysis of LSM design. BOURBON employs\ngreedy piecewise linear regression to learn key distributions,\nenabling fast lookup with minimal computation, and applies\na cost-beneﬁt strategy to decide when learning will be worth-\nwhile. Through a series of experiments on both synthetic\nand real-world datasets, we show that BOURBON improves\nlookup performance by 1.23 \u0002-1.78\u0002as compared to state-\nof-the-art production LSMs.\n1 Introduction\nMachine learning is transforming how we build computer ap-\nplications and systems. Instead of writing code in the tradi-\ntional algorithmic mindset, one can instead collect the proper\ndata, train a model, and thus implement a robust and gen-\neral solution to the task at hand. This data-driven, empirical\napproach has been called “Software 2.0” [26], hinting at a\nworld where an increasing amount of the code we deploy is\nrealized in this manner; a number of landmark successes over\nthe past decade lend credence to this argument, in areas such\nas image [32] and speech recognition [24], machine transla-\ntion [46], game playing [44], and many other areas [7,15,17].\nOne promising line of work, for using ML to improve\ncore systems is that of the “learned index” [31]. This ap-\nproach applies machine learning to supplant the traditional\nindex structure found in database systems, namely the ubiq-\nuitous B-Tree [9]. To look up a key, the system uses a learned\nfunction that predicts the location of the key (and value);\nwhen successful, this approach can improve lookup perfor-\nmance, in some cases signiﬁcantly, and also potentially re-\nduce space overhead. Since this pioneering work, numerous\nfollow ups [13, 20, 30] have been proposed that use better\nmodels, better tree structures, and generally improve how\nlearning can reduce tree-based access times and overheads.\nHowever, one critical approach has not yet been trans-\nformed in this “learned” manner: the Log-structured MergeTree (LSM) [37, 39, 42]. LSMs were introduced in the\nlate ’90s, gained popularity a decade later through work at\nGoogle on BigTable [8] and LevelDB [22], and have be-\ncome widely used in industry, including in Cassandra [33],\nRocksDB [18], and many other systems [21,38]. LSMs have\nmany positive properties as compared to B-trees and their\ncousins, including high insert performance [11, 37, 40].\nIn this paper, we apply the idea of the learned index to\nLSMs. A major challenge is that while learned indexes are\nprimarily tailored for read-only settings, LSMs are optimized\nfor writes. Writes cause disruption to learned indexes be-\ncause models learned over existing data must now be updated\nto accommodate the changes; the system thus must re-learn\nthe data repeatedly. However, we ﬁnd that LSMs are well-\nsuited for learned indexes. For example, although writes\nmodify the LSM, most portions of the tree are immutable;\nthus, learning a function to predict key/value locations can\nbe done once, and used as long as the immutable data lives.\nHowever, many challenges arise. For example, variable key\nor value sizes make learning a function to predict locations\nmore difﬁcult, and performing model building too soon may\nlead to signiﬁcant resource waste.\nThus, we ﬁrst study how an existing LSM system, Wisc-\nKey [37], functions in great detail (§3). We focus on Wisc-\nKey because it is a state-of-the-art LSM implementation that\nis signiﬁcantly faster than LevelDB and RocksDB [37]. Our\nanalysis leads to many interesting insights from which we\ndevelop ﬁve learning guidelines : a set of rules that aid an\nLSM system to successfully incorporate learned indexes. For\nexample, while it is useful to learn the stable, low levels in\nan LSM, learning higher levels can yield beneﬁts as well be-\ncause lookups must always search the higher levels. Next,\nnot all ﬁles are equal: some ﬁles even in the lower levels\nare very short-lived; a system must avoid learning such ﬁles,\nor resources can be wasted. Finally, workload- and data-\nawareness is important; based on the workload and how the\ndata is loaded, it may be more beneﬁcial to learn some por-\ntions of the tree than others.\nWe apply these learning guidelines to build BOURBON , a\n1arXiv:2005.14213v2  [cs.DB]  30 Oct 2020\n\nlearned-index implementation of WiscKey (§4). BOURBON\nuses piece-wise linear regression, a simple but effective\nmodel that enables both fast training (i.e., learning) and in-\nference (i.e., lookups) with little space overhead. BOURBON\nemploys ﬁle learning : models are built over ﬁles given\nthat an LSM ﬁle, once created, is never modiﬁed in-place.\nBOURBON implements a cost-beneﬁt analyzer that dynam-\nically decides whether or not to learn a ﬁle, reducing un-\nnecessary learning while maximizing beneﬁts. While most\nof the prior work on learned indexes [13, 20, 31] has made\nstrides in optimizing stand-alone data structures, BOURBON\nintegrates learning into a production-quality system that is\nalready highly optimized. BOURBON ’s implementation adds\naround 5K LOC to WiscKey (which has \u001820K LOC).\nWe analyze the performance of BOURBON on a range of\nsynthetic and real-world datasets and workloads (§5). We\nﬁnd that BOURBON reduces the indexing costs of WiscKey\nsigniﬁcantly and thus offers 1.23 \u0002– 1.78\u0002faster lookups\nfor various datasets. Even under workloads with signiﬁcant\nwrite load, BOURBON speeds up a large fraction of lookups\nand, through cost-beneﬁt, avoids unnecessary (early) model\nbuilding. Thus, BOURBON matches the performance of an\naggressive-learning approach but performs model building\nmore judiciously. Finally, most of our analysis focuses on\nthe case where fast lookups will make the most difference,\nnamely when the data resides in memory (i.e., in the ﬁle-\nsystem page cache). However, we also experiment with\nBOURBON when data resides on a fast storage device (an Op-\ntane SSD) or when data does not ﬁt entirely in memory, and\nshow that beneﬁts can still be realized.\nThis paper makes four contributions. We present the ﬁrst\ndetailed study of how LSMs function internally with learning\nin mind. We formulate a set of guidelines on how to integrate\nlearned indexes into an LSM (§3). We present the design and\nimplementation of BOURBON which incorporates learned in-\ndexes into a real, highly optimized, production-quality LSM\nsystem (§4). Finally, we analyze BOURBON ’s performance in\ndetail, and demonstrate its beneﬁts (§5).\n2 Background\nWe ﬁrst describe log-structured merge trees and explain how\ndata is organized in LevelDB. Next, we describe WiscKey, a\nmodiﬁed version of LevelDB that we adopt as our baseline.\nWe then provide a brief background on learned indexes.\n2.1 LSM and LevelDB\nAn LSM tree is a persistent data structure used in key-value\nstores to support efﬁcient inserts and updates [39]. Unlike\nB-trees that require many random writes to storage upon up-\ndates, LSM trees perform writes sequentially, thus achieving\nhigh write throughput [39].\nAn LSM organizes data in multiple levels , with the size\nof each level increasing exponentially. Inserts are initially\nbuffered in an in-memory structure; once full, this structure\nkey2key1immutable \nmemtable \n. . .    \n. . . \n L6sstablesmemtable diskmemory\n. . .    \n(a) LevelDBL2L1L01  FindFiles\nindex block \ndata-block 1 \ndata-block 2 \ndata-block n \n. . . \n filter block candidate-1\ncandidate-2\n3  SearchIB \n7  ReadValue \nsstablevalue-log\n. . . \n . . . \n \n(b) WiscKeycandidate-3\n2  LoadIB+FB \n5  LoadDB \n6  SearchDB \n4  SearchFB \nIBFBDB\n. . .    Figure 1: LevelDB and WiscKey. (a) shows how data is\norganized in LevelDB and how a lookup is processed. The\nsearch in in-memory tables is not shown. The candidate ssta-\nbles are shown in bold boxes. (b) shows how keys and values\nare separated in WiscKey.\nis merged with the ﬁrst level of on-disk data. This procedure\nresembles merge-sort and is referred to as compaction. Data\nfrom an on-disk level is also merged with the successive level\nif the size of the level exceeds a limit. Note that updates do\nnot modify existing records in-place; they follow the same\npath as inserts. As a result, many versions of the same item\ncan be present in the tree at a time. Throughout this paper,\nwe refer to the levels that contain the newer data as higher\nlevels and the older data as lower levels.\nA lookup request must return the latest version of an item.\nBecause higher levels contain the newer versions, the search\nstarts at the topmost level. First, the key is searched for in\nthe in-memory structure; if not found, it is searched for in\nthe on-disk tree starting from the highest level to the lowest\none. The value is returned once the key is found at a level.\nLevelDB [22] is a widely used key-value store built us-\ning LSM. Figure 1(a) shows how data is organized in Lev-\nelDB. A new key-value pair is ﬁrst written to the memtable ;\nwhen full, the memtable is converted into an immutable table\nwhich is then compacted and written to disk sequentially as\nsstables . The sstables are organized in seven levels ( L0being\nthe highest level and L6the lowest) and each sstable corre-\nsponds to a ﬁle. LevelDB ensures that key ranges of different\nsstables at a level are disjoint (two ﬁles will not contain over-\nlapping ranges of keys); L0is an exception where the ranges\ncan overlap across ﬁles. The amount of data at each level\nincreases by a factor of ten; for example, the size of L1is\n10MB, while L6contains several 100s of GBs. If a level ex-\nceeds its size limit, one or more sstables from that level are\nmerged with the next level; this is repeated until all levels are\nwithin their limits.\nLookup steps. Figure 1(a) also shows how a lookup request\nfor key kproceeds. 1FindFiles : If the key is not found\nin the in-memory tables, LevelDB ﬁnds the set of candidate\nsstable ﬁles that may contain k. A key in the worst case\n2\n\nmay be present in all L0ﬁles (because of overlapping ranges)\nand within one ﬁle at each successive level. 2LoadIB+FB :\nIn each candidate sstable, an index block and a bloom-ﬁlter\nblock are ﬁrst loaded from the disk. 3SearchIB : The in-\ndex block is binary searched to ﬁnd the data block that may\ncontain k.4SearchFB : The ﬁlter is queried to check if kis\npresent in the data block. 5LoadDB : If the ﬁlter indicates\npresence, the data block is loaded. 6SearchDB : The data\nblock is binary searched. 7ReadValue : If the key is found\nin the data block, the associated value is read and the lookup\nends. If the ﬁlter indicates absence or if the key is not found\nin the data block, the search continues to the next candidate\nﬁle. Note that blocks are not always loaded from the disk;\nindex and ﬁlter blocks, and frequently accessed data blocks\nare likely to be present in memory (i.e., ﬁle-system cache).\nWe refer to these search steps at a level that occur as part\nof a single lookup as an internal lookup . A single lookup\nthus consists of many internal lookups. A negative internal\nlookup does not ﬁnd the key, while a positive internal lookup\nﬁnds the key and is thus the last step of a lookup request.\nWe differentiate indexing steps from data-access steps; in-\ndexing steps such as FindFiles ,SearchIB ,SearchFB , and\nSearchDB search through the ﬁles and blocks to ﬁnd the\ndesired key, while data-access steps such as LoadIB+FB ,\nLoadDB , and ReadValue read the data from storage. Our\ngoal is to reduce the time spent in indexing.\n2.2 WiscKey\nIn LevelDB, compaction results in large write ampliﬁcation\nbecause both keys and values are sorted and rewritten. Thus,\nLevelDB suffers from high compaction overheads, affecting\nforeground workloads.\nWiscKey [37] (and Badger [1]) reduces this overhead by\nstoring the values separately; the sstables contain only keys\nand pointers to the values as shown in Figure 1(b). With this\ndesign, compaction sorts and writes only the keys, leaving\nthe values undisturbed, thus reducing I/O ampliﬁcation and\noverheads. WiscKey thus performs signiﬁcantly better than\nother optimized LSM implementations such as LevelDB and\nRocksDB. Given these beneﬁts, we adopt WiscKey as the\nbaseline for our design. Further, WiscKey’s key-value sepa-\nration enables our design to handle variable-size records; we\ndescribe how in more detail in §4.2.\nThe write path of WiscKey is similar to that of LevelDB\nexcept that values are written to a value log . A lookup in\nWiscKey also involves searching at many levels and a ﬁnal\nread into the log once the target key is found. The size of\nWiscKey’s LSM tree is much smaller than LevelDB because\nit does not contain the values; hence, it can be entirely cached\nin memory [37]. Thus, a lookup request involves multiple\nsearches in the in-memory tree, and the ReadValue step per-\nforms one ﬁnal read to retrieve the value.2.3 Optimizing Lookups in LSMs\nPerforming a lookup in LevelDB and WiscKey requires\nsearching at multiple levels. Further, within each sstable,\nmany blocks are searched to ﬁnd the target key. Given that\nLSMs form the basis of many embedded key-value stores\n(e.g., LevelDB, RocksDB [18]) and distributed storage sys-\ntems (e.g., BigTable [8], Riak [38]), optimizing lookups in\nLSMs can have huge beneﬁts.\nA recent body of work, starting with learned indexes [31],\nmakes a case for replacing or augmenting traditional index\nstructures with machine-learning models. The key idea is to\ntrain a model (such as linear regression or neural nets) on the\ninput so that the model can predict the position of a record\nin the sorted dataset. The model can have inaccuracies, and\nthus the prediction has an associated error bound. During\nlookups, if the model-predicted position of the key is correct,\nthe record is returned; if it is wrong, a local search is per-\nformed within the error bound. For example, if the predicted\nposition is posand the minimum and maximum error bounds\naredminanddmax, then upon a wrong prediction, a local\nsearch is performed between pos\u0000dminandpos+dmax.\nLearned indexes can make lookups signiﬁcantly faster. In-\ntuitively, a learned index turns a O(log-n)lookup of a B-tree\ninto a O(1)operation. Empirically, learned indexes provide\n1.5\u0002– 3\u0002faster lookups than B-trees [31]. Given these ben-\neﬁts, we ask the following questions: can learned indexes for\nLSMs make lookups faster? If yes, under what scenarios?\nTraditional learned indexes do not support updates be-\ncause models learned over the existing data would change\nwith modiﬁcations [13, 20, 31]. However, LSMs are attrac-\ntive for their high performance in write-intensive workloads\nbecause they perform writes only sequentially. Thus, we ex-\namine: how to realize the beneﬁts of learned indexes while\nsupporting writes for which LSMs are optimized? We answer\nthese two questions next.\n3 Learned Indexes: a Good Match for LSMs?\nIn this section, we ﬁrst analyze if learned indexes could be\nbeneﬁcial for LSMs and examine under what scenarios they\ncan improve lookup performance. We then provide our in-\ntuition as to why learned indexes might be appropriate for\nLSMs even when allowing writes. We conduct an in-depth\nstudy based on measurements of how WiscKey functions in-\nternally under different workloads to validate our intuition.\nFrom our analysis, we derive a set of learning guidelines.\n3.1 Learned Indexes: Beneﬁcial Regimes\nA lookup in LSM involves several indexing and data-access\nsteps. Optimized indexes such as learned indexes can reduce\nthe overheads of indexing but cannot reduce data-access\ncosts. In WiscKey, learned indexes can thus potentially re-\nduce the costs of indexing steps such as FindFiles ,SearchIB ,\nand SearchDB , while data-access costs (e.g., ReadValue )\n3\n\n 0 20 40 60 80 100\nInMemory SATA NVMe Optane3 µs 13.1 µs 9.3 µs 3.8 µs\ndataaccess\nindex-ingPercentage (%)FindFiles\nSearchIB+SearchDB\nSearchFBLoadIB+FB\nLoadDB\nReadValueOtherFigure 2: Lookup Latency Breakdown. The ﬁgure shows\nthe breakdown of lookup latency in WiscKey. The ﬁrst bar\nshows the case when data is cached in memory. The other\nthree bars show the case where the dataset is stored on dif-\nferent types of SSDs. We perform 10M random lookups on\nthe Amazon Reviews dataset [5]; the ﬁgure shows the break-\ndown of the average latency (shown at the top of each bar).\nThe indexing portions are shown in solid colors; data access\nand other portions are shown in patterns.\ncannot be signiﬁcantly reduced. As a result, learned in-\ndexes can improve overall lookup performance if indexing\ncontributes to a sizable portion of the total lookup latency.\nWe identify scenarios where this is the case.\nFirst, when the dataset or a portion of it is cached in mem-\nory, data-access costs are low, and so indexing costs become\nsigniﬁcant. Figure 2 shows the breakdown of lookup la-\ntencies in WiscKey. The ﬁrst bar shows the case when the\ndataset is cached in memory; the second bar shows the case\nwhere the data is stored on a ﬂash-based SATA SSD. With\ncaching, data-access and indexing costs contribute almost\nequally to the latency. Thus, optimizing the indexing por-\ntion can reduce lookup latencies by about 2 \u0002. When the\ndataset is not cached, data-access costs dominate and thus\noptimizing indexes may yield smaller beneﬁts (about 20%).\nHowever, learned indexes are not limited to scenarios\nwhere data is cached in memory. They offer beneﬁt on fast\nstorage devices that are currently prevalent and can do more\nso on emerging faster devices. The last three bars in Figure 2\nshow the breakdown for three kinds of devices: ﬂash-based\nSSDs over SATA and NVMe, and an Optane SSD. As the\ndevice gets faster, lookup latency (as shown at the top) de-\ncreases, but the fraction of time spent on indexing increases.\nFor example, with SATA SSDs, indexing takes about 17% of\nthe total time; in contrast, with Optane SSDs, indexing takes\n44% and thus optimizing it with learned indexes can po-\ntentially improve performance by 1.8 \u0002. More importantly,\nthe trend in storage performance favors the use of learned\nindexes. With storage performance increasing rapidly and\nemerging technologies like 3D Xpoint memory providing\nvery low access latencies, indexing costs will dominate and\nthus learned indexes will yield increasing beneﬁts.\nSummary. Learned indexes could be beneﬁcial when the\ndatabase or a portion of it is cached in memory. With fast\nstorage devices, regardless of caching, indexing contributesto a signiﬁcant fraction of the lookup time; thus, learned in-\ndexes can prove useful in such cases. With storage devices\ngetting faster, learned indexes will be even more beneﬁcial.\n3.2 Learned Indexes with Writes\nLearned indexes provide higher lookup performance com-\npared to traditional indexes for read-only analytical work-\nloads. However, a major drawback of learned indexes (as\ndescribed in [31]) is that they do not support modiﬁcations\nsuch as inserts and updates [13, 20]. The main problem with\nmodiﬁcations is that they alter the data distribution and so\nthe models must be re-learned; for write-heavy workloads,\nmodels must be rebuilt often, incurring high overheads.\nAt ﬁrst, it may seem like learned indexes are not a good\nmatch for write-heavy situations for which LSMs are opti-\nmized. However, we observe that the design of LSMs ﬁts\nwell with learned indexes. Our key realization is that al-\nthough updates can change portions of the LSM tree, a large\npart remains immutable. Speciﬁcally, newly modiﬁed items\nare buffered in the in-memory structures or present in the\nhigher levels of the tree, while stable data resides at the lower\nlevels. Given that a large fraction of the dataset resides in\nthe stable, lower levels, lookups to this fraction can be made\nfaster with no or few re-learnings. In contrast, learning in\nhigher levels may be less beneﬁcial: they change at a faster\nrate and thus must be re-learned often.\nWe also realize that the immutable nature of sstable ﬁles\nmakes them an ideal unit for learning. Once learned, these\nﬁles are never updated and thus a model can be useful until\nthe ﬁle is replaced. Further, the data within an sstable is\nsorted; such sorted data can be learned using simple models.\nA level, which is a collection of many immutable ﬁles, can\nalso be learned as a whole using simple models. The data in\na level is also sorted: the individual sstables are sorted, and\nthere are no overlapping key ranges across sstables.\nWe next conduct a series of in-depth measurements to vali-\ndate our intuitions. Our experiments conﬁrm that while a part\nof our intuition is indeed true, there are some subtleties (for\nexample, in learning ﬁles at higher levels). Based on these\nexperimental results, we formulate a set of learning guide-\nlines : a few simple rules that an LSM that applies learned\nindexes should follow.\nExperiments: goal and setup. The goal of our experiments\nis to determine how long a model will be useful and how of-\nten it will be useful. A model built for a sstable ﬁle is useful\nas long as the ﬁle exists; thus, we ﬁrst measure and analyze\nsstable lifetimes. How often a model will be used is deter-\nmined by how many internal lookups it serves; thus, we next\nmeasure the number of internal lookups to each ﬁle. Since\nmodels can also be built for entire levels, we ﬁnally mea-\nsure level lifetimes as well. To perform our analysis, we run\nworkloads with varying amounts of writes and reads, and\nmeasure the lifetimes and number of lookups. We conduct\nour experiments on WiscKey, but we believe our results are\n4\n\n100101102103104\n 0.1  1  10  100Average lifetime (s)\nWrite percentage (%)L4\nL3L2\nL1L0\n 0 20 40 60 80 100\n10-210-1100101102103104105CDF\nLifetime (s)L4 L1\n 0 20 40 60 80 100\n10-210-1100101102103104CDF\nLifetime (s)1%\n5%10%\n20%50%\n 0 20 40 60 80 100\n10-210-1100101102103104105CDF\nLifetime (s)1%\n5%10%\n20%50%(i) Level 1 (ii) Level 4\n(a) Average lifetimes with varying write % (b) Lifetime distribution with 5% writes (c) Lifetime distributions with varying write %\nFigure 3: SSTable Lifetimes. (a) shows the average lifetime of sstable ﬁles in levels L 4to L 0. (b) shows the distribution of\nlifetimes of sstables in L 1and L 4with 5% writes. (c) shows the distribution of lifetimes of sstables for different write percentages\nin L 1and L 4.\n100102104106108\n 0.1  1  10  100Avg. internal lookups/file Write percentage (%)L4\nL3L2\nL1L0\n100102104106108\n 0.1  1  10  100Avg. negative lookups/fileWrite percentage (%)L4\nL3L2\nL1L0\n100102104106108\n 0.1  1  10  100Avg. positive lookups/file Write percentage (%)L4\nL3L2\nL1L0\n100102104106108\n 0.1  1  10Avg. internal lookups/file Write percentage (%)L4\nL3L2\nL1L0\n100102104106108\n 0.1  1  10  100Avg. internal lookups/file Write percentage (%)L4 L3 L2\n(i) Total (ii) Negative (iii) Positive (iv) Positive (Zipﬁan)\n(a) Randomly loaded dataset (b) Sequentially loaded dataset\nFigure 4: Number of Internal Lookups Per File. (a)(i) shows the average internal lookups per ﬁle at each level for a\nrandomly loaded dataset. (b) shows the same for sequentially loaded dataset. (a)(ii) and (a)(iii) show the negative and positive\ninternal lookups for the randomly loaded case. (a)(iv) shows the positive internal lookups for the randomly loaded case when\nthe workload distribution is Zipﬁan.\napplicable to most LSM implementations. We ﬁrst load the\ndatabase with 256M key-value pairs. We then run a workload\nwith a single rate-limited client that performs 200M opera-\ntions, a fraction of which are writes. Our workload chooses\nkeys uniformly at random.\nLifetime of SSTables. To determine how long a model will\nbe useful, we ﬁrst measure and analyze the lifetimes of ssta-\nbles. To do so, we track the creation and deletion times of all\nsstables. For ﬁles created during the load phase, we assign\nthe workload-start time as their creation time; for other ﬁles,\nwe record the actual creation times. If the ﬁle is deleted dur-\ning the workload, then we calculate its exact lifetime. How-\never, some ﬁles are not deleted by the end of the workload\nand we must estimate their lifetimes.†\nFigure 3(a) shows the average lifetime of sstable ﬁles at\ndifferent levels. We make three main observations. First, the\naverage lifetime of sstable ﬁles at lower levels is greater than\nthat of higher levels. Second, at lower percentages of writes,\neven ﬁles at higher levels have a considerable lifetime; for\nexample, at 5% writes, ﬁles at L0live for about 2 minutes\non an average. Files at lower levels live much longer; ﬁles\natL4live about 150 minutes. Third, although the average\nlifetime of ﬁles reduces with more writes, even with a high\n†If the ﬁles are created during load, we assign the workload duration as\ntheir lifetimes. If not, we estimate the lifetime of a ﬁle based on its creation\ntime ( c) and the total workload time ( w); the lifetime of the ﬁle is at least\nw\u0000c. We thus consider the lifetime distribution of other ﬁles that have a\nlifetime of at least w\u0000c. We then pick a random lifetime in this distribution\nand assign it as this ﬁle’s lifetime.amount of writes, ﬁles at lower levels live for a long period.\nFor instance, with 50% writes, ﬁles at L4live for about 60\nminutes. In contrast, ﬁles at higher level live only for a few\nseconds; for example, an L0ﬁle lives only about 10 seconds.\nWe now take a closer look at the lifetime distribution. Fig-\nure 3(b) shows the distributions for L1andL4ﬁles with 5%\nwrites. We ﬁrst note that some ﬁles are very short-lived,\nwhile some are long-lived. For example, in L1, the lifetime\nof about 50% of the ﬁles is only about 2.5 seconds. If ﬁles\ncross this threshold, they tend to live for much longer times;\nalmost all of the remaining L1ﬁles live over ﬁve minutes.\nSurprisingly, even at L4, which has a higher average life-\ntime for ﬁles, a few ﬁles are very short-lived. We observe\nthat about 2% of L4ﬁles live less than a second. We ﬁnd\nthat there are two reasons why a few ﬁles live for a very\nshort time. First, compaction of a Liﬁle creates a new ﬁle in\nLi+1which is again immediately chosen for compaction to\nthe next level. Second, compaction of a Liﬁle creates a new\nﬁle in Li+1, which has overlapping key ranges with the next\nﬁle that is being compacted from Li. Figure 3(c) shows that\nthis pattern holds for other percentages of writes too. We ob-\nserved that this holds for other levels as well. From the above\nobservations, we arrive at our ﬁrst two learning guidelines.\nLearning guideline - 1: Favor learning ﬁles at lower levels.\nFiles at lower levels live for a long period even for high write\npercentages; thus, models for these ﬁles can be used for a\nlong time and need not be rebuilt often.\nLearning guideline - 2: Wait before learning a ﬁle. A few\n5\n\nﬁles are very short-lived, even at lower levels. Thus, learning\nmust be invoked only after a ﬁle has lived up to a threshold\nlifetime after which it is highly likely to live for a long time.\nInternal Lookups at Different Levels. To determine how\nmany times a model will be used, we analyze the num-\nber of lookups served by the sstable ﬁles. We run a work-\nload and measure the number of lookups served by ﬁles at\neach level and plot the average number of lookups per ﬁle at\neach level. Figure 4(a) shows the result when the dataset is\nloaded in an uniform random order. The number of internal\nlookups is higher for higher levels, although a large fraction\nof data resides at lower levels. This is because, at higher\nlevels, many internal lookups are negative, as shown in Fig-\nure 4(a)(ii). The number of positive internal lookups is as\nexpected: higher in lower levels as shown in Figure 4(a)(iii).\nThis result shows that ﬁles at higher levels serve many nega-\ntive lookups and thus are worth optimizing. While bloom ﬁl-\nters may already make these negative lookups faster, the in-\ndex block still needs to be searched (before the ﬁlter query).\nWe also conduct the same experiment with another work-\nload where the access pattern follows a zipﬁan distribution\n(most requests are to a small set of keys). Most of the re-\nsults exhibit the same trend as the random workload except\nfor the number of positive internal lookups, as shown in Fig-\nure 4(a)(iv). Under the zipﬁan workload, higher level ﬁles\nalso serve numerous positive lookups, because the workload\naccesses a small set of keys which are often updated and thus\nstored in higher levels.\nFigure 4(b) shows the result when the dataset is sequen-\ntially loaded, i.e., keys are inserted in ascending order. In\ncontrast to the randomly-loaded case, there are no negative\nlookups because keys of different sstable ﬁles do not overlap\neven across levels; the FindFiles step ﬁnds the one ﬁle that\nmay contain the key. Thus, lower levels serve more lookups\nand can have more beneﬁts from learning. From these obser-\nvations, we arrive at the next two learning guidelines.\nLearning guideline - 3: Do not neglect ﬁles at higher lev-\nels.Although ﬁles at lower levels live longer and serve many\nlookups, ﬁles at higher levels can still serve many negative\nlookups and in some cases, even many positive lookups.\nThus, learning ﬁles at higher levels can make both internal\nlookups faster.\nLearning guideline - 4: Be workload- and data-aware. Al-\nthough most data resides in lower levels, if the workload does\nnot lookup that data, learning those levels will yield less ben-\neﬁt; learning thus must be aware of the workload. Further,\nthe order in which the data is loaded inﬂuences which levels\nreceive a large fraction of internal lookups; thus, the system\nmust also be data-aware. The amount of internal lookups acts\nas a proxy for both the workload and load order. Based on\nthe amount of internal lookups, the system must dynamically\ndecide whether to learn a ﬁle or not.\nLifetime of Levels. Given that a level as a whole can also be\nlearned, we now measure and analyze the lifetimes of levels.\n 0\n 0 1 L-1\n 0 1 L-2\n 0 1 L-3\n 0 1\n 0  500  1000  1500  2000L-4 burst interval = 330s#changes/#files\nTime (s)(a) Timeline of changes\n100101102103\n 1  10  100Time b/w bursts (s)\nWrite percentage (%) (b) Time between bursts for L4\nFigure 5: Changes at Levels. (a) shows the timeline\nof ﬁle creations and deletions at different levels. Note that\n#changes/#ﬁles is higher than 1 in L 1as there are more cre-\nations and deletions than the number of ﬁles. (b) shows the\ntime between bursts for L4 for different write percentages.\nLevel learning cannot be applied at L0because it is unsorted:\nﬁles in L0can have overlapping key ranges. Once a level\nis learned, any change to the level causes a re-learning. A\nlevel changes when new sstables are created at that level, or\nexisting ones are deleted. Thus, intuitively, a level would\nlive for an equal or shorter duration than the individual ssta-\nbles. However, learning at the granularity of a level has the\nbeneﬁt that the candidate sstables need not be found in a sep-\narate step; instead, upon a lookup, the model just outputs the\nsstable and the offset within it.\nWe examine the changes to a level by plotting the timeline\nof ﬁle creations and deletions at L1,L2,L3, and L4in Fig-\nure 5(a) for a 5%-write workload; we do not show L0for the\nreason above. On the y-axis, we plot the number of changes\ndivided by the total ﬁles present at that level. A value of\n0 means there are no changes to the level; a model learned\nfor the level can be used as long as the value remains 0. A\nvalue greater than 0 means that there are changes in the level\nand thus the model has to re-learned. Higher values denote a\nlarger fraction of ﬁles are changed.\nFirst, as expected, we observe that the fraction of ﬁles that\nchange reduces as we go down the levels because lower lev-\nels hold a large volume of data in many ﬁles, conﬁrming our\nintuition. We also observe that changes to levels arrive in\nbursts. These bursts are caused by compactions that cause\nmany ﬁles at a level to be rewritten. Further, these bursts\noccur at almost the same time across different levels. The\nreason behind this is that for the dataset we use, levels L0\nthrough L3are full and thus any compaction at one layer\nresults in cascading compactions which ﬁnally settle at the\nnon-full L4level. The levels remain static between these\nbursts. The duration for which the levels remain static is\nlonger with a lower amount of writes; for example, with 5%\nwrites, as shown in the ﬁgure, this period is about 5 minutes.\nHowever, as the amount of writes increases, the lifetime of a\nlevel reduces as shown in Figure 5(b); for instance, with 50%\nwrites, the lifetime of L4reduces to about 25 seconds. From\nthese observations, we arrive at our ﬁnal learning guideline.\nLearning guideline - 5: Do not learn levels for write-heavy\nworkloads. Learning a level as a whole might be more appro-\n6\n\npriate when the amount of writes is very low or if the work-\nload is read-only. For write-heavy workloads, level lifetimes\nare very short and thus will induce frequent re-learnings.\nSummary. We analyzed how LSMs behave internally by\nmeasuring and analyzing the lifetimes of sstable ﬁles and\nlevels, and the amount of lookups served by ﬁles at different\nlevels. From our analysis, we derived ﬁve learning guide-\nlines. We next describe how we incorporate the learning\nguidelines in an LSM-based storage system.\n4 Bourbon Design\nWe now describe BOURBON , an LSM-based store that uses\nlearning to make indexing faster. We ﬁrst describe the model\nthat BOURBON uses to learn the data (§4.1). Then, we discuss\nhow BOURBON supports variable-size values (§4.2) and its\nbasic learning strategy (§4.3). We ﬁnally explain BOURBON ’s\ncost-beneﬁt analyzer that dynamically makes learning deci-\nsions to maximize beneﬁt while reducing cost (§4.4).\n4.1 Learning the Data\nAs we discussed, data can be learned at two granularities:\nindividual sstables or levels. Both these entities are sorted\ndatasets. The goal of a model that tries to learn the data is to\npredict the location of a key in such a sorted dataset. For ex-\nample, if the model is constructed for a sstable ﬁle, it would\npredict the ﬁle offset given a key. Similarly, a level model\nwould output the target sstable ﬁle and the offset within it.\nOur requirements for a model is that it must have low\noverheads during learning and during lookups. Further, we\nwould like the space overheads of the model to be small. We\nﬁnd that piecewise linear regression (PLR) [4, 27] satisﬁes\nthese requirements well; thus, BOURBON uses PLR to model\nthe data. The intuition behind PLR is to represent a sorted\ndataset with a number of line segments. PLR constructs a\nmodel with an error bound; that is, each data point dis guar-\nanteed to lie within the range [ dpos\u0000d,dpos+d], where\ndposis the predicted position of din the dataset and dis the\nerror bound speciﬁed beforehand.\nTo train the PLR model, BOURBON uses the Greedy-PLR\nalgorithm [47]. Greedy-PLR processes the data points one\nat a time; if a data point cannot be added to the current line\nsegment without violating the error bound, then a new line\nsegment is created and the data point is added to it. At the\nend, Greedy-PLR produces a set of line segments that repre-\nsents the data. Greedy-PLR runs in linear time with respect\nto the number of data points.\nOnce the model is learned, inference is quick: ﬁrst, the\ncorrect line segment that contains the key is found (using\nbinary search); within that line segment, the position of the\ntarget key is obtained by multiplying the key with the line’s\nslope and adding the intercept. If the key is not present in\nthe predicted position, a local search is done in the range\ndetermined by the error bound. Thus, lookups take O(log-\ns)time, where sis the number of segments, in addition to aWorkloadBaseline\ntime (s)File model Level model\nTime(s) % model Time(s) % model\nMixed:\nWrite-heavy82.671.5\n(1.16\u0002)74.295.1\n(0.87\u0002)1.5\nMixed:\nRead-heavy89.262.05\n(1.44\u0002)99.874.3\n(1.2\u0002)21.4\nRead-only 48.427.2\n(1.78\u0002)10025.2\n(1.92\u0002)100\nTable 1: File vs. Level Learning. The table compares the\ntime to perform 10M operations in baseline WiscKey, ﬁle-\nlearning, and level-learning. The numbers within the paren-\ntheses show the improvements over baseline. The table also\nshows the percentage of lookups that take the model path;\nremaining take the original path because the models are not\nrebuilt yet.\nconstant time to do the local search. The space overheads of\nPLR are small: a few tens of bytes for every line segment.\nOther models or algorithms such as RMI [31], PGM-\nIndex [19], or splines [29] may also be suitable for LSMs\nand may offer more beneﬁts than PLR. We leave their explo-\nration within LSMs for future work.\n4.2 Supporting Variable-size Values\nLearning a model that predicts the offset of a key-value pair\nis much easier if the key-value pairs are the same size. The\nmodel then can multiply the predicted position of a key by\nthe size of the pair to produce the ﬁnal offset. However,\nmany systems allow keys and values to be of arbitrary sizes.\nBOURBON requires keys to be of a ﬁxed size, while val-\nues can be of any size. We believe this is a reasonable de-\nsign choice because most datasets have ﬁxed-size keys (e.g.,\nuser-ids are usually 16 bytes), while value sizes vary signif-\nicantly. Even if keys vary in size, they can be padded to\nmake all keys of the same size. BOURBON supports variable-\nsize values by borrowing the idea of key-value separation\nfrom WiscKey [37]. With key-value separation, sstables in\nBOURBON just contain the keys and the pointer to the values;\nvalues are maintained in the value log separately. With this,\nBOURBON obtains the offset of a required key-value pair by\ngetting the predicted position from the model and multiply-\ning it with the record size (which is keysize +pointersize .)\nThe value pointer serves as the offset into the value log from\nwhich the value is ﬁnally read.\n4.3 Level vs. File Learning\nBOURBON can learn individual sstables ﬁles or entire levels.\nOur analysis in the previous section showed that ﬁles live\nlonger than levels under write-heavy workloads, hinting that\nlearning at the ﬁle granularity might be the best choice. We\nnow closely examine this tradeoff to design BOURBON ’s ba-\nsic learning strategy. To do so, we compare the performance\nof ﬁle learning and level learning for different workloads.\nWe initially load a dataset and build the models. For the read-\nonly workload, the models need not be re-learned. In the\nmixed workloads, the models are re-learned as data changes.\nThe results are shown in Table 1.\n7\n\nFor mixed workloads, level learning performs worse than\nﬁle learning. For a write-heavy (50%-write) workload, with\nlevel learning, only a small percentage of internal lookups\nare able to use the model because with a steady stream of in-\ncoming writes, the system is unable to learn the levels. Only\na mere 1.5% of internal lookups take the model path; these\nlookups are the ones performed just after loading the data\nand when the initial level models are available. We observe\nthat all the 66 attempted level learnings failed because the\nlevel changed before the learning completed. Because of the\nadditional cost of re-learnings, level learning performs even\nworse than the baseline with 50% writes. On the other hand,\nwith ﬁle models, a large fraction of lookups beneﬁt from the\nmodels and thus ﬁle learning performs better than the base-\nline. For read-heavy mixed workload (5%), although level\nlearning has beneﬁts over the baseline, it performs worse\nthan ﬁle learning for the same reasons above.\nLevel learning can be beneﬁcial for read-only settings: as\nshown in the table, level learning provides 10% improve-\nments over ﬁle learning. Thus, deployments that have only\nread-only workloads can beneﬁt from level learning. Given\nthat BOURBON ’s goal is to provide faster lookups while sup-\nporting writes, levels are not an appropriate choice of granu-\nlarity for learning. Thus, BOURBON uses ﬁle learning by de-\nfault. However, BOURBON supports level learning as a con-\nﬁguration option that can be useful in read-only scenarios.\n4.4 Cost vs. Beneﬁt Analyzer\nBefore learning a ﬁle, BOURBON must ensure that the time\nspent in learning is worthwhile. If a ﬁle is short-lived, then\nthe time spent learning that ﬁle wastes resources. Such a\nﬁle will serve few lookups and thus the model would have\nlittle beneﬁt. Thus, to decide whether or not to learn a ﬁle,\nBOURBON implements an online cost vs. beneﬁt analysis.\n4.4.1 Wait Before Learning\nAs our analysis showed, even in the lower levels, many ﬁles\nare short-lived. To avoid the cost of learning short-lived ﬁles,\nBOURBON waits for a time threshold, Twait, before learning a\nﬁle. The exact value of Twaitpresents a cost vs. performance\ntradeoff. A very low Twaitleads to some short-lived ﬁles\nstill being learned, incurring overheads; a large value causes\nmany lookups to take the baseline path (because there is no\nmodel built yet), thus missing opportunities to make lookups\nfaster. BOURBON sets the value of Twaitto the time it takes\nto learn a ﬁle. Our approach is never more than a factor of\ntwo worse than the optimal solution, where the optimal solu-\ntion knows apriori the lifetime and decides to either immedi-\nately or never learn the ﬁle (i.e., it is two-competitive [25]).\nThrough measurements, we found that the maximum time to\nlearn a ﬁle (which is at most \u00184MB in size) is around 40 ms\non our experimental setup. We conservatively set Twaitto be\n50 ms in BOURBON ’s implementation.4.4.2 To Learn a File or Not\nBOURBON waits for Twaitbefore learning a ﬁle. However,\nlearning a ﬁle even if it lives for a long time may not be ben-\neﬁcial. For example, our analysis shows that although lower-\nlevel ﬁles live longer, for some workloads and datasets, they\nserve relatively fewer lookups than higher-level ﬁles; higher-\nlevel ﬁles, although short-lived, serve a large percentage of\nnegative internal lookups in some scenarios. BOURBON , thus,\nmust consider the potential beneﬁts that a model can bring,\nin addition to considering the cost to build the model. It is\nproﬁtable to learn a ﬁle if the beneﬁt of the model ( Bmodel )\noutweighs the cost to build the model ( Cmodel ).\nEstimating Cmodel .One way to estimate Cmodel is to assume\nthat the learning is completely performed in the background\nand will not affect the rest of the system; i.e., Cmodel is 0.\nThis is true if there are many idle cores which the learning\nthreads can utilize and thus do not interfere with the fore-\nground tasks (e.g., the workload) or other background tasks\n(e.g., compaction). However, BOURBON takes a conservative\napproach and assumes that the learning threads will interfere\nand slow down the other parts of the system. As a result,\nBOURBON assumes Cmodel to be equal to Tbuild. We deﬁne\nTbuild as the time to train the PLR model for a ﬁle. We ﬁnd\nthat this time is linearly proportional to the number of data\npoints in the ﬁle. We calculate Tbuildfor a ﬁle by multiplying\nthe average time to a train a data point (measured ofﬂine) and\nthe number of data points in the ﬁle.\nEstimating Bmodel .Estimating the potential beneﬁt of learn-\ning a ﬁle, Bmodel , is more involved. Intuitively, the bene-\nﬁt offered by the model for an internal lookup is given by\nTb\u0000Tm, where TbandTmare the average times for the lookup\nin baseline and model paths, respectively. If the ﬁle serves\nN lookups in its lifetime, the net beneﬁt of the model is:\nBmodel = (Tb\u0000Tm)\u0003N. We divide the internal lookups into\nnegative and positive because most negative lookups termi-\nnate at the ﬁlter, whereas positive ones do not; thus,\nBmodel = ((Tn:b\u0000Tn:m)\u0003Nn) + (( Tp:b\u0000Tp:m)\u0003Np)\nwhere NnandNpare the number of negative and positive in-\nternal lookups, respectively. Tn:bandTp:bare the time in the\nbaseline path for a negative and a positive lookup, respec-\ntively; Tn:mandTp:mare the model counterparts.\nBmodel for a ﬁle cannot be calculated without knowing the\nnumber of lookups that the ﬁle will serve or how much time\nthe lookups will take. The analyzer, to estimate these quanti-\nties, maintains statistics of ﬁles that have lived their lifetime,\ni.e., ﬁles that were created, served many lookups, and then\nwere replaced. To estimate these quantities for a ﬁle F, the\nanalyzer uses the statistics of other ﬁles at the same level as\nF; we consider statistics only at the same level because these\nstatistics vary signiﬁcantly across levels.\nRecall that BOURBON waits before learning a ﬁle. Dur-\ning this time, the lookups are served in the baseline path.\nBOURBON uses the time taken for these lookups to estimate\n8\n\nTn:bandTp:b. Next, Tn:mandTp:mare estimated as the aver-\nage negative and positive model lookup times of other ﬁles\nat the same level. Finally, NnandNpare estimated as fol-\nlows. The analyzer ﬁrst takes the average negative and pos-\nitive lookups for other ﬁles in that level; then, it is scaled\nby a factor f=s=¯sl, where sif the size of the ﬁle and ¯ slis\nthe average ﬁle size at this level. While estimating the above\nquantities, BOURBON ﬁlters out very short-lived ﬁles.\nWhile bootstrapping, the analyzer might not have enough\nstatistics collected. Therefore, initially, BOURBON runs in an\nalways-learn mode (with Twaitstill in place.) Once enough\nstatistics are collected, the analyzer performs the cost vs.\nbeneﬁt analysis and chooses to learn a ﬁle if Cmodel <Bmodel ,\ni.e., beneﬁt of a model outweighs the cost. If multiple ﬁles\nare chosen to be learned at the same time, BOURBON puts\nthem in a max priority queue ordered by Bmodel\u0000Cmodel , thus\nprioritizing ﬁles that would deliver the most beneﬁt.\nOur cost-beneﬁt analyzer adopts a simple scheme of us-\ning average statistics of other ﬁles at the same level. While\nthis approach has worked well in our initial prototype, us-\ning more sophisticated statistics and considering workload\ndistributions (e.g., to account for keys with different popu-\nlarity) could be more beneﬁcial. We leave such exploration\nfor future work.\n4.5 Bourbon: Putting it All Together\nWe describe how the different pieces of BOURBON work to-\ngether. Figure 6 shows the path of lookups in BOURBON . As\nshown in (a), lookups can either be processed via the model\n(if the target ﬁle is already learned), or in the baseline path\n(if the model is not built yet.) The baseline path in BOURBON\nis similar to the one shown in Figure 1 for LevelDB, except\nthat BOURBON stores the values separately and so ReadValue\nreads the value from the log.\nOnce BOURBON learns a sstable ﬁle, lookups to that ﬁle\nwill be processed via the learned model as shown in Fig-\nure 6(b). 1FindFiles :BOURBON ﬁnds the candidate ssta-\nbles; this step required because BOURBON uses ﬁle learning.\n2LoadIB+FB :BOURBON loads the index and ﬁlter blocks;\nthese blocks are likely to be already cached. 3Model-\nLookup :BOURBON performs a look up for the desired key\nkin the candidate sstable’s model. The model outputs a pre-\ndicted position of kwithin the ﬁle ( pos) and the error bound\n(d). From this, BOURBON calculates the data block that con-\ntains records pos\u0000dthrough pos+d.†4SearchFB : The\nﬁlter for that block is queried to check if kis present. If\npresent, BOURBON calculates the range of bytes of the block\nthat must be loaded; this is simple because keys and pointers\nto values are of ﬁxed size. 5LoadChunk : The byte range\nis loaded. 6LocateKey : The key is located in the loaded\nchunk. The key will likely be present in the predicted po-\n†Sometimes, records pos\u0000dthrough pos+dspan multiple data blocks;\nin such cases, BOURBON consults the index block (which speciﬁes the\nmaximum key in each data block) to ﬁnd the data block for pos.\nmemtables<pos, error> \n7  ReadValue \n2  LoadIB+FB \n. . .    diskmemory\n. . .    1  FindFiles\nIBModel\n3  ModelLookup \nFB\n5  LoadChunk      \n4  SearchFB \n6  LocateKey \nvalue-log. . .    . . .    \nk\n→ <offset, len> δ δ Model \nLookup\nSearch \nIBFind \nFilesLoad \nIB+FBSearch \nFBLoad \nChunk Locate \nKey\nLoad \nDBSearch \nDBRead \nValue \n(b) Lookup via model - detailed steps(a) Lookup pathsmodel exists \nno model \n(baseline)Figure 6: BOURBON Lookups. (a) shows that lookups\ncan take two different paths: when the model is available\n(shown at the top), and when the model is not learned yet\nand so lookups take the baseline path (bottom); some steps\nare common to both paths. (b) shows the detailed steps for\na lookup via a model; we show the case where models are\nbuilt for ﬁles.\nsition (the midpoint of the loaded chunk); if not, BOURBON\nperforms a binary search in the chunk. 7ReadValue : The\nvalue is read from the value log using the pointer.\nPossible improvements. Although BOURBON ’s implemen-\ntation is highly-optimized and provides many features com-\nmon to real systems, it lacks a few features. For example,\nin the current implementation, we do not support string keys\nand key compression (although we support value compres-\nsion). For string keys, one approach we plan to explore is to\ntreat strings as base-64 integers and convert them into 64-bit\nintegers, which could then adopt the same learning approach\ndescribed herein. While this approach may work well for\nsmall keys, large keys may require larger integers (with more\nthan 64 bits) and thus efﬁcient large-integer math is likely es-\nsential. Also, BOURBON does not support adaptive switching\nbetween level and ﬁle models; it is a static conﬁguration. We\nleave supporting these features to future work.\n5 Evaluation\nTo evaluate BOURBON , we ask the following questions:\n• Which portions of lookup does BOURBON optimize? (§5.1)\n• How does BOURBON perform with models available and\nno writes? How does performance change with datasets,\nload orders, and request distributions? (§5.2)\n• How does BOURBON perform with range queries? (§5.3)\n• In the presence of writes, how does BOURBON ’s cost-\nbeneﬁt analyzer perform compared to other approaches\n9\n\nKey00.51Position(a) Linear\nKey   (b) Seg10%\nKey   (c) Normal\nKey   (d) OSM\nFigure 7: Datasets. The ﬁgure shows the cumulative distri-\nbution functions (CDF) of three synthetic datasets (linear,\nsegmented-10%, and normal) and one real-world dataset\n(OpenStreetMaps). Each dataset is magniﬁed around the\n15% percentile to show a detailed view of its distribution.\nthat always or never re-learn? (§5.4)\n• Does BOURBON perform well on real benchmarks? (§5.5)\n• Is BOURBON beneﬁcial when data is on storage? (§5.6)\n• Is BOURBON beneﬁcial with limited memory? (§5.7)\n• What are the error and space tradeoffs of BOURBON ?\n(§5.8)\nSetup. We run our experiments on a 20-core Intel Xeon\nCPU E5-2660 machine with 160-GB memory and a 480-\nGB SATA SSD. We use 16B integer keys and 64B values,\nand set the error bound of BOURBON ’s PLR as 8. Unless\nspeciﬁed, our workloads perform 10M operations. We use a\nvariety of datasets. We construct four synthetic datasets: lin-\near, segmented-1%, segmented-10% , and normal, each with\n64M key-value pairs. In the linear dataset, keys are all con-\nsecutive. In the seg-1% dataset, there is a gap after a con-\nsecutive segment of 100 keys (i.e., every 1% causes a new\nsegment). The segmented-10% dataset is similar, but there\nis a gap after 10 consecutive keys. We generate the normal\ndataset by sampling 64M unique values from the standard\nnormal distribution N(0;1)and scale to integers. We also use\ntwo real-world datasets: Amazon reviews (AR) [5] and New\nYork OpenStreetMaps (OSM) [2]. AR and OSM have 33.5M\nand 21.9M key-value pairs, respectively. These datasets vary\nwidely in how the keys are distributed. Figure 7 shows the\ndistribution for a few datasets. Most of our experiments fo-\ncus on the case where the data resides in memory; however,\nwe also analyze cases where data is present on storage.\n5.1 Which Portions does BOURBON Optimize?\nWe ﬁrst analyze which portions of the lookup BOURBON op-\ntimizes. We perform 10M random lookups on the AR and\nOSM datasets and show the latency breakdown in Figure 8.\nAs expected, BOURBON reduces the time spent in index-\ning. The portion marked Search in the ﬁgure corresponds\ntoSearchIB andSearchDB in the baseline, versus Model-\nLookup andLocateKey inBOURBON . The steps in BOURBON\nhave lower latency than their baseline counterparts. Inter-\nestingly, BOURBON reduces data-access costs too, because\nBOURBON loads a smaller byte range than the entire block\nloaded by the baseline.\n 0 1 2 3 4\nWiscKey   Bourbon WiscKey   Bourbon\nAR OSM2.9x 2.4x2.2x 2xAvg. latency ( µs)FindFiles\nLoadIB+FBSearch\nSearchFBLoadData\nReadValueOtherFigure 8: Latency Breakdown. The ﬁgure shows la-\ntency breakdown for WiscKey and BOURBON . Search de-\nnotes SearchIB and SearchDB in WiscKey; the same denotes\nModelLookup and LocateKey in BOURBON . LoadData de-\nnotes LoadDB in WiscKey; the same denotes LoadChunk in\nBOURBON . These two steps are optimized by BOURBON and\nare shown in solid colors; the number next to a step shows\nthe factor by which it is made faster in BOURBON .\n012345Average latency (us)\nDatasetLinear Seg1% NormalSeg10% AR OSM1.78x 1.43x 1.35x 1.23x 1.61x 1.61xWiscKey Bourbon Bourbon-level\n(a) Average lookup latencyDataset #segslatency\n(ms)\nLinear 900 2.72\nSeg1% 640K 3.11\nNormal 705K 3.3\nSeg10% 6.4M 3.64\nAR 129K 2.66\nOSM 295K 2.65\n(b) Number of segments\nFigure 9: Datasets. (a) compares the average lookup laten-\ncies of BOURBON ,BOURBON -level, and WiscKey for different\ndatasets; the numbers on the top show the improvements of\nBOURBON over WiscKey. (b) shows the number of segments\nfor different datasets in BOURBON .\n5.2 Performance under No Writes\nWe next analyze BOURBON ’s performance when the models\nare already built and there are no updates. For each exper-\niment, we load a dataset and allow the system to build the\nmodels; during the workload, we issue only lookups.\n5.2.1 Datasets\nTo analyze how the performance is inﬂuenced by the dataset,\nwe run the workload on all six datasets and compare\nBOURBON ’s lookup performance against WiscKey. Figure 9\nshow the results. As shown in 9(a), BOURBON is faster than\nWiscKey for all datasets; depending upon the dataset, the im-\nprovements vary (1.23 \u0002to 1.78 \u0002).BOURBON provides the\nmost beneﬁt for the linear dataset because it has the smallest\nnumber of segments (one per model); with fewer segments,\nfewer searches are needed to ﬁnd the target line segment.\nFrom 9(b), we observe that latencies increase with the num-\nber of segments (e.g., latency of seg-1% is greater than that\nof linear). We cannot compare the number of segments in\nAR and OSM with others because the size of these datasets\nis signiﬁcantly different.\nLevel learning . Given that level learning is suitable for read-\nonly scenarios, we conﬁgure BOURBON to use level learn-\n10\n\n0246Average latency (us)seq seq rand rand\nAR OSM1.61x 1.61x 1.47x 1.50xWiscKey Bourbon(a) Average latencyDatasetPositive Negative\n#Speedup #Speedup\nAR 10M 2.15\u000223M 1.83\u0002\nOSM 10M 1.99\u000222M 1.82\u0002\n(b) Positive vs. negative internal lookups\nfor randomly loaded case\nFigure 10: Load Orders. (a) shows the performance\nfor AR and OSM datasets for sequential (seq) and random\n(rand) load orders. (b) compares the speedup of positive and\nnegative internal lookups.\ning and analyze its performance. As shown in Figure 9(a),\nBOURBON -level is 1.33 \u0002– 1.92\u0002faster than the baseline.\nBOURBON -level offers more beneﬁts than BOURBON because\na level-model lookup is faster than ﬁnding the candidate ssta-\nbles and then doing a ﬁle-model lookup. This conﬁrms that\nBOURBON -level is an attractive option for read-only scenar-\nios. However, since level models only provide beneﬁts for\nread-only workloads and give at most 10% improvement\ncompared to ﬁle models, we focus on BOURBON with ﬁle\nlearning for our remaining experiments.\n5.2.2 Load Orders\nWe now explore how the order in which the data is loaded af-\nfects performance. For this experiment, we use the AR and\nOSM datasets and load them in two ways: sequential (keys\nare inserted in ascending order) and random (keys are in-\nserted in an uniformly random order). With sequential load-\ning, sstables do not have overlapping key ranges even across\nlevels; whereas, with random loading, sstables at one level\ncan overlap with sstables at other levels.\nFigure 10 shows the result. First, regardless of the load or-\nder, BOURBON offers signiﬁcant beneﬁt over baseline (1.47 \u0002\n– 1.61\u0002). Second, the average lookup latencies increase in\nthe randomly-loaded case compared to the sequential case\n(e.g., 6 ms vs. 4 ms in WiscKey for the AR dataset). This is\nbecause while there are no negative internal lookups in the\nsequential case, there are many (23M) negative lookups in\nthe random case (as shown in 10(b)). Thus, with random\nload, the total number of internal lookups increases by 3 \u0002,\nincreasing lookup latencies.\nNext, we note that the speedup over baseline in the random\ncase is less than that of the sequential case (e.g., 1.47 \u0002vs.\n1.61\u0002for AR). Although BOURBON optimizes both positive\nand negative internal lookups, the gain for negative lookups\nis smaller (as shown in 10(b)). This is because most negative\nlookups in the baseline and BOURBON end just after the ﬁl-\nter is queried (ﬁlter indicates absence); the data block is not\nloaded or searched. Given there are more negative than posi-\ntive lookups, BOURBON offers less speedup than the sequen-\ntial case. However, this speedup is still signiﬁcant (1.47 \u0002).\n5.2.3 Request Distributions\nNext, we analyze how request distributions affect\nBOURBON ’s performance. We measure the lookup la-\n02468Average latency (us)AR AR AR AR AR AROSM OSM OSM OSM OSM OSM\nSequential Zipfian HotSpot Exponential Uniform Latest1.6x 1.5x 1.5x 1.7x 1.5x 1.6x 1.6x 1.5x 1.6x 1.8x 1.6x 1.6xWiscKey BourbonFigure 11: Request Distributions. The ﬁgure shows the\naverage lookup latencies of different request distributions\nfrom AR and OSM datasets.\n0.00.51.01.52.0Normalized ThroughputAR AR AR AR AR AR OSM OSM OSM OSM OSM OSM\n1 5 10 50 100 5001.90x 1.53x 1.43x 1.18x 1.15x 1.10x 1.93x 1.57x 1.39x 1.19x 1.14x 1.04xWiscKey Bourbon\nFigure 12: Range Queries. The ﬁgure shows the normal-\nized throughput of range queries with different range lengths\nfrom AR and OSM datasets.\ntencies under six request distributions: sequential, zipﬁan,\nhotspot, exponential, uniform, and latest. We ﬁrst randomly\nload the AR and OSM datasets and then run the workloads;\nthus, the data can be segmented and there can be many\nnegative internal lookups. As shown in Figure 11, BOURBON\nmakes lookups faster by 1.54 \u0002– 1.76\u0002than the baseline.\nOverall, BOURBON reduces latencies regardless of request\ndistributions.\nRead-only performance summary. When the models are\nalready built and when there are no writes, BOURBON pro-\nvides signiﬁcant speedup over baseline for a variety of\ndatasets, load orders, and request distributions.\n5.3 Range Queries\nWe next analyze how BOURBON performs on range queries.\nWe perform 1M range queries on the AR and OSM datasets\nwith various range lengths. Figure 12 shows the through-\nput of BOURBON normalized to that of WiscKey. With short\nranges, where the indexing cost (i.e., the cost to locate the\nﬁrst key of the range) is dominant, BOURBON offers the most\nbeneﬁt. For example, with a range length of 1 on the AR\ndataset, BOURBON is 1.90\u0002faster than WiscKey. The gains\ndrop as the range length increases; for example, BOURBON\nis only 1.15 \u0002faster with queries that return 100 items. This\nis because, while BOURBON can accelerate the indexing por-\ntion, it follows a similar path as WiscKey to scan subsequent\nkeys. Thus, with large range lengths, indexing accounts for\nless of the total performance, resulting in lower gains.\n5.4 Efﬁcacy of Cost-beneﬁt Analyzer with Writes\nWe next analyze how BOURBON performs in the presence\nof writes. Writes modify the data and so the models must\nbe re-learned. In such cases, the efﬁcacy of BOURBON ’s\ncost-beneﬁt analyzer (cba) is critical. We thus compare\n11\n\n 0 100 200 300 400 500\n 1  10  100Time (s)\nWrite percentage (%)WiscKey\nofflinealways\ncba\n 0 30 60 90 120 150\n 1  10  100Time (s)\nWrite percentage (%)WiscKey\nofflinealways\ncba\n 0 200 400 600 800\n 1  10  100Time (s)\nWrite percentage (%)WiscKey\nofflinealways\ncba\n 0 20 40 60 80 100\n 1  10  100% baseline lookups\nWrite percentage (%)WiscKey\nofflinealways\ncba(a) Foreground time (b) Learning time (c) Total time (d) Baseline-path internal lookups (%)\nFigure 13: Mixed Workloads. (a) compares the foreground times of WiscKey, BOURBON -ofﬂine (ofﬂine), BOURBON -always\n(always), and BOURBON -cba (cba); (b) and (c) compare the learning time and total time, respectively; (d) shows the fraction\nof internal lookups that take the baseline path.\n0100200300400Throughput (Kops/sec)default default default default default AR AR AR AR AR OSM OSM OSM OSM OSM\nA:write-heavy B:read-heavy C:read-only D:read-heavy F:write-heavy1.06x 1.38x 1.64x 1.34x 1.18x 1.08x 1.31x 1.62x 1.44x 1.1x 1.11x 1.24x 1.61x 1.33x 1.11xWiscKey Bourbon\n010203040Throughput (Kops/sec)default AR OSM\nE:range-heavy1.17x 1.16x 1.19x\nFigure 14: Macrobenchmark-YCSB. The ﬁgure compares the throughput of BOURBON against WiscKey for six YCSB work-\nloads across three datasets.\nBOURBON ’s cba against two strategies: BOURBON -ofﬂine and\nBOURBON -always. BOURBON -ofﬂine performs no learning\nas writes happen; models exist only for the initially loaded\ndata. BOURBON -always re-learns the data as writes happen;\nit always decides to learn a ﬁle without considering cost.\nBOURBON -cba re-learns as well, but it uses the cost-beneﬁt\nanalysis to decide whether or not to learn a ﬁle.\nWe run a workload that issues 50M operations with vary-\ning percentages of writes on the AR dataset. To calculate the\ntotal amount of work performed for each workload, we sum\ntogether the time spent on the foreground lookups and inserts\n(Figure 13(a)), the time spent learning (13(b)), and the time\nspent on compaction (not shown); the total amount of work\nis shown in Figure 13(c). The ﬁgure also shows the fraction\nof internal lookups that take the baseline path (13(d)).\nFirst, as shown in 13(a), all BOURBON variants reduce\nthe workload time compared to WiscKey. The gains are\nlower with more writes because BOURBON has fewer lookups\nto optimize. Next, BOURBON -ofﬂine performs worse than\nBOURBON -always and BOURBON -cba. Even with just 1%\nwrites, a signiﬁcant fraction of internal lookups take the\nbaseline path in BOURBON -ofﬂine as shown in 13(d); this\nshows re-learning as data changes is crucial.\nBOURBON -always learns aggressively and thus almost no\nlookups take the baseline path even for 50% writes. As\na result, BOURBON -always has the lowest foreground time.\nHowever, this comes at the cost of increased learning time;\nfor example, with 50% writes, BOURBON -always spends\nabout 134 seconds learning. Thus, the total time spent in-\ncreases with more writes for BOURBON -always and is even\nhigher than baseline WiscKey as shown in 13(c). Thus, ag-gressively learning is not ideal.\nGiven a low percentage of writes, BOURBON -cba decides\nto learn almost all the ﬁles, and thus matches the charac-\nteristics of BOURBON -always: both have a similar fraction\nof lookups taking the baseline path, both require the same\ntime learning, and both perform the same amount of work.\nWith a high percentage of writes, BOURBON -cba chooses not\nto learn many ﬁles, reducing learning time; for example,\nwith 50% writes, BOURBON -cba spends only 13.9 seconds in\nlearning (10 \u0002lower than BOURBON -always). Consequently,\nmany lookups take the baseline path. BOURBON -cba takes\nthis action because there is less beneﬁt to learning as the data\nis changing rapidly and there are fewer lookups. Thus, it al-\nmost matches the foreground time of BOURBON -always. But,\nby avoiding learning, the total work done by BOURBON -cba\nis signiﬁcantly lower.\nSummary. Aggressive learning offers fast lookups but with\nhigh costs; no re-learning provides little speedup. Neither is\nideal. In contrast, BOURBON provides high beneﬁts similar\nto aggressive learning while lowering total cost signiﬁcantly.\n5.5 Real Macrobenchmarks\nWe next analyze how BOURBON performs under two real\nbenchmarks: YCSB [10] and SOSD [28].\n5.5.1 YCSB\nWe use six workloads that have different read-write ratios\nand access patterns: A (w:50%, r:50%), B (w:5%, r:95%),\nC (read-only), D (read latest, w:5%, r:95%), E (range-heavy,\nw:5%, range:95%), F (read-modify-write:50%, r:50%). We\nuse three datasets: YCSB’s default dataset (created using\n12\n\n0246Average latency (us)Datasetamzn32 face32 logn32 norm32 uden32 uspr321.48x 1.62x 1.68x 1.66x 1.74x 1.55xWiscKey BourbonFigure 15: Macrobenchmark-SOSD. The ﬁgure compares\nlookup latencies from the SOSD benchmark. The numbers on\nthe top show BOURBON ’s improvements over the baseline.\nDatasetWiscKey\nlatency ( ms)BOURBON\nLatency( ms)Speedup\nAmazon Reviews (AR) 3.53 2.75 1.28\u0002\nNewYork OpenStreetMaps (OSM) 3.14 2.51 1.25\u0002\nTable 2: Performance on Fast Storage. The table shows\nBOURBON ’s lookup latencies when the data is stored on an\nOptane SSD.\nycsb-load [3]), AR, and OSM, and load them in a random\norder. Figure 14 shows the results.\nFor the read-only workload (YCSB-C), all operations ben-\neﬁt and BOURBON offers the most gains (about 1.6 \u0002). For\nread-heavy workloads (YCSB-B and D), most operations\nbeneﬁt, while writes are not improved and thus BOURBON\nis 1.24 \u0002– 1.44\u0002faster than the baseline. For write-heavy\nworkloads (YCSB-A and F), BOURBON improves perfor-\nmance only a little (1.06 \u0002– 1.18\u0002). First, a large fraction\nof operations are writes; second, the number of the inter-\nnal lookups taking the model path decreases (by about 30%\ncompared to the read-heavy workload because BOURBON\nchooses not to learn some ﬁles). YCSB-E consists of range\nqueries (range lengths varying from 1 to 100) and 5% writes.\nBOURBON reaches 1.16 \u0002– 1.19\u0002gain. In summary, as ex-\npected, BOURBON improves the performance of read opera-\ntions; at the same time, BOURBON does not affect the perfor-\nmance of writes.\n5.5.2 SOSD\nWe next measure BOURBON ’s performance on the SOSD\nbenchmark designed for learned indexes [28]. We use the\nfollowing six datasets: book sale popularity (amzn32), Face-\nbook user ids (face32), lognormally (logn32) and normally\n(norm32) distributed datasets, uniformly distributed dense\n(uden32) and sparse (uspr32) integers. Figure 15 shows the\naverage lookup latency. As shown, BOURBON is about 1.48 \u0002\n– 1.74\u0002faster than the baseline for all datasets.\n5.6 Performance on Fast Storage\nOur analyses so far focused on the case where the data re-\nsides in memory. We now analyze if BOURBON will offer\nbeneﬁt when the data resides on a fast storage device. We run\na read-only workload on sequentially loaded AR and OSM\ndatasets on an Intel Optane SSD. Table 2 shows the result.\nEven when the data is present on a storage device, BOURBON\noffers beneﬁt (1.25 \u0002– 1.28\u0002faster lookups than WiscKey).\n0100200300400Throughput (Kops/sec) WorkloadA:write-heavy B:read-heavy D:read-heavy F:write-heavy1.05x 1.19x 1.16x 1.06xWiscKey BourbonFigure 16: Mixed Workloads on Fast Storage. The ﬁgure\ncompares the throughput of BOURBON against WiscKey for\nfour read-write mixed YCSB workloads. We use the YCSB\ndefault dataset for this experiment.\nWorkloadWiscKey\nlatency ( ms)BOURBON\nLatency( ms)Speedup\nUniform 98.6 94.4 1.04\u0002\nZipﬁan 18.8 15.1 1.25\u0002\nTable 3: Performance with Limited Memory. The ta-\nble shows BOURBON ’s average lookup latencies from the AR\ndataset on a machine with a SATA SSD and limited memory.\nFigure 16 shows the result for read-write mixed YCSB work-\nloads on the same device with the default YCSB datasest. As\nexpected, while BOURBON ’s beneﬁts are marginal for write-\nheavy workloads (YCSB-A and YCSB-F), it offers consid-\nerable speedup (1.16 \u0002– 1.19\u0002) for read-heavy workloads\n(YCSB-B and YCSB-D). With the emerging storage tech-\nnologies (e.g., 3D XPoint memory), BOURBON will offer\neven more beneﬁts.\n5.7 Performance with Limited Memory\nWe further show that, even with no fast storage and lim-\nited available memory, BOURBON can still offer beneﬁt with\nskewed workloads, such as zipﬁan. We experiment on a ma-\nchine with a SATA SSD and memory that only holds about\n25% of the database. We run a uniform random workload,\nand a zipﬁan workload with consecutive hotspots where 80%\nof the requests access about 25% of the database. Table 3\nshows the result. With the uniform workload, BOURBON is\nonly 1.04 \u0002faster because most of the time is spent loading\nthe data into the memory. With the zipﬁan workload, in con-\ntrast, indexing time instead of data-access time dominates\nbecause a large number of requests access the small portion\nof data that is already cached in memory. BOURBON is able\nto reduce this signiﬁcant indexing time and thus offers 1.25 \u0002\nlower latencies.\n5.8 Error Bound and Space Overheads\nWe ﬁnally discuss the characteristics of BOURBON ’s ML\nmodel, speciﬁcally its error bound ( d) and space overheads.\nFigure 17(a) plots the error bound ( d) against the average\nlookup latency (left y-axis) for AR dataset. As dincreases,\nfewer line segments are created, leading to fewer searches,\nthus reducing latency. However, beyond d=8, although the\ntime to ﬁnd the segment reduces, the time to search within\na segment increases, thus increasing latency. We ﬁnd that\nBOURBON ’s choice of d=8 is optimal for other datasets too.\n13\n\n 2.65 2.7 2.75 2.8 2.85 2.9\n 2  4  8  16  32 0 5 10 15 20 25 30Average latency (us)\nMemory overhead (MB)\nError Bound ( δ)latency\nSpace (MB)(a) Error-bound tradeoffDatasetSpace Overheads\nMB %\nLinear 0.02 0.0\nSeg1% 15.38 0.21\nSeg10% 153.6 2.05\nNormal 16.94 0.23\nAR 3.09 0.08\nOSM 7.08 0.26\n(b) Space overheads\nFigure 17: Error-bound Tradeoffs and Space Overheads.\n(a) shows how the PLR error bound affects lookup latency\nand memory overheads; (b) shows the space overheads for\ndifferent datasets.\nFigure 17(a) also shows how space overheads (right y-axis)\nvary with d. Asdincreases, fewer line segments are created,\nleading to low space overheads. Table 17(b) shows the space\noverheads for different datasets. As shown, for a variety of\ndatasets, the overhead compared to the total dataset size is\nlittle (0% – 2%).\n6 Related Work\nLearned indexes. The core idea of our work, replacing in-\ndexing structures with ML models, is inspired from the pi-\noneering work on learned indexes [31]. However, learned\nindexes do not support updates, an essential operation that\nan storage-system index must support. Recent research tries\nto address this limitation. For instance, XIndex [45], FITing-\nTree [20], and AIDEL [35] support writes using an additional\narray (delta index) and with periodic re-training, whereas\nAlex [13] uses gapped array at the leaf nodes of a B-tree\nto support writes.\nMost prior efforts optimize B- tree variants, while our\nwork is the ﬁrst to deeply focus on LSMs. Further, while\nmost prior efforts implement learned indexes to stand-alone\ndata structures, our work is the ﬁrst to show how learning\ncan be integrated and implemented into an existing, opti-\nmized, production-quality system. While SageDB [30] is\na full database system that uses learned components, it is\nbuilt from scratch with learning in mind. Our work, in con-\ntrast, shows how learning can be integrated into an exist-\ning, practical system. Finally, instead of “ﬁxing” new read-\noptimized learned index structures to handle writes (like pre-\nvious work), we incorporate learning into an already write-\noptimized, production-quality LSM.\nLSM optimizations. Prior work has built many LSM op-\ntimizations. Monkey [11] carefully adjusts the bloom ﬁlter\nallocations for better ﬁlter hit rates and memory utilization.\nDostoevsky [12], HyperLevelDB [16], and bLSM [42] de-\nvelop optimized compaction policies to achieve lower write\nampliﬁcation and latency. cLSM [23] and RocksDB [18] use\nnon-blocking synchronization to increase parallelism. We\ntake a different yet complimentary approach to LSM opti-mization by incorporating models as auxiliary index struc-\ntures to improve lookup latency, but each of the others are\northogonal and compatible to our core design.\nModel choices. Duvignau et al. [14] compare a variety\nof piecewise linear regression algorithms. Greedy-PLR,\nwhich we utilize, is a good choice to realize fast lookups,\nlow learning time, and small memory overheads. Neural\nnetworks are also widely used to approximate data distri-\nbutions, especially datasets with complex non-linear struc-\ntures [34]. However, theoretical analysis [36] and exper-\niments [43] show that training a complex neural network\ncan be prohibitively expensive. Similar to Greedy-PLR, re-\ncent work proposes a one-pass learning algorithm based on\nsplines [29] and identiﬁes that such an algorithm could be\nuseful for learning sorted data in LSMs; we leave their ex-\nploration within LSMs for future work.\n7 Conclusion\nIn this paper, we examine if learned indexes are suitable for\nwrite-optimized log-structured merge (LSM) trees. Through\nin-depth measurements and analysis, we derive a set of\nguidelines to integrate learned indexes into LSMs. Using\nthese guidelines, we design and build BOURBON , a learned-\nindex implementation for a highly-optimized LSM system.\nWe experimentally demonstrate that BOURBON offers signif-\nicantly faster lookups for a range of workloads and datasets.\nBOURBON is an initial work on integrating learned indexes\ninto an LSM-based storage system. More detailed stud-\nies, such as more sophisticated cost-beneﬁt analysis, general\nstring support, and different model choices, could be promis-\ning for future work. In addition, we believe that BOURBON ’s\nlearning approach may work well in other write-optimized\ndata structures such as the Be-tree [6] and could be an inter-\nesting avenue for future work. While our work takes initial\nsteps towards integrating learning into production-quality\nsystems, more studies and experience are needed to under-\nstand the true utility of learning approaches.\nAcknowledgements\nWe thank Alexandra Fedorova (our shepherd) and the anony-\nmous reviewers of OSDI ’20 for their insightful comments\nand suggestions. We thank the members of ADSL for their\nexcellent feedback. We also thank CloudLab [41] for pro-\nviding a great environment to run our experiments and re-\nproduce our results during artifact evaluation. This material\nwas supported by funding from NSF grants CNS-1421033,\nCNS-1763810 and CNS-1838733, Intel, Microsoft, Seagate,\nand VMware. Aishwarya Ganesan is supported by a Face-\nbook fellowship. Any opinions, ﬁndings, and conclusions or\nrecommendations expressed in this material are those of the\nauthors and may not reﬂect the views of NSF or any other\ninstitutions.\n14\n\nReferences\n[1] BadgerDB. https://github.com/dgraph-\nio/badger .\n[2] Open Street Maps. https://www.\nopenstreetmap.org/#map=4/38.01/-\n95.84 .\n[3] Running a Workload. https://github.com/\nbrianfrankcooper/YCSB/wiki/Running-\na-Workload .\n[4] Jayadev Acharya, Ilias Diakonikolas, Jerry Li, and\nLudwig Schmidt. Fast Algorithms for Segmented Re-\ngression. arXiv preprint arXiv:1607.03990 , 2016.\n[5] Amazon. Amazon Customer Reviews Dataset.\nhttps://registry.opendata.aws/\namazon-reviews/ .\n[6] Michael A. Bender, Martin Farach-Colton, William\nJannen, Rob Johnson, Bradley C. Kuszmaul, Donald E.\nPorter, Jun Yuan, and Yang Zhan. An introduction to\nbe-trees and write-optimization. ;login: Operating Sys-\ntems and Sysadmin , (5):23–28, Oct 2015.\n[7] Mariusz Bojarski, Davide Del Testa, Daniel\nDworakowski, Bernhard Firner, Beat Flepp, Pra-\nsoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs\nMuller, and Jiakai Zhang. End to End Learning for\nSelf-driving Cars. arXiv preprint arXiv:1604.07316 ,\n2016.\n[8] Fay Chang, Jeffrey Dean, Sanjay Ghemawat, Wilson C.\nHsieh, Deborah A. Wallach, Michael Burrows, Tushar\nChandra, Andrew Fikes, and Robert Gruber. Bigtable:\nA Distributed Storage System for Structured Data. In\nProceedings of the 7th Symposium on Operating Sys-\ntems Design and Implementation (OSDI ’06) , pages\n205–218, Seattle, WA, November 2006.\n[9] Douglas Comer. The Ubiquitous B-Tree. ACM Com-\nputing Surveys , 11(2), June 1979.\n[10] Brian F. Cooper, Adam Silberstein, Erwin Tam, Raghu\nRamakrishnan, and Russell Sears. Benchmarking\nCloud Serving Systems with YCSB. In Proceedings\nof the ACM Symposium on Cloud Computing (SOCC\n’10), Indianapolis, IA, June 2010.\n[11] Niv Dayan, Manos Athanassoulis, and Stratos Idreos.\nMonkey: Optimal Navigable Key-value Store. In\nProceedings of the 2017 ACM SIGMOD International\nConference on Management of Data (SIGMOD ’17) ,\nChicago, IL, May 2017.[12] Niv Dayan and Stratos Idreos. Dostoevsky: Better\nSpace-time Trade-offs for LSM-tree based Key-value\nStores via Adaptive removal of Superﬂuous Merging.\nInProceedings of the 2018 International Conference\non Management of Data , pages 505–520, 2018.\n[13] Jialin Ding, Umar Farooq Minhas, Hantian Zhang, Yi-\nnan Li, Chi Wang, Badrish Chandramouli, Johannes\nGehrke, Donald Kossmann, and David Lomet. ALEX:\nAn Updatable Adaptive Learned Index. arXiv preprint\narXiv:1905.08898 , 2019.\n[14] Romaric Duvignau, Vincenzo Gulisano, Marina Pa-\npatriantaﬁlou, and Vladimir Savic. Piecewise linear\napproximation in data streaming: Algorithmic imple-\nmentations and experimental analysis. arXiv preprint\narXiv:1808.08877 , 2018.\n[15] Sarah M Erfani, Sutharshan Rajasegarar, Shanika\nKarunasekera, and Christopher Leckie. High-\ndimensional and Large-scale Anomaly Detection using\na Linear One-class SVM with Deep Learning. Pattern\nRecognition , 58:121–134, 2016.\n[16] Robert Escriva, Sanjay Ghemawat, David Grogan,\nJeremy Fitzhardinge, and Chris Mumford. Hyper-\nLevelDB. https://github.com/rescrv/HyperLevelDB,\n2013.\n[17] Andre Esteva, Alexandre Robicquet, Bharath Ramsun-\ndar, V olodymyr Kuleshov, Mark DePristo, Katherine\nChou, Claire Cui, Greg Corrado, Sebastian Thrun, and\nJeff Dean. A Guide to Deep Learning in Healthcare.\nNature medicine , 25(1):24–29, 2019.\n[18] Facebook. RocksDB. http://rocksdb.org/ .\n[19] Paolo Ferragina and Giorgio Vinciguerra. The\npgm-index. Proceedings of the VLDB Endowment ,\n13(10):1162–1175, Jun 2020.\n[20] Alex Galakatos, Michael Markovitch, Carsten Bin-\nnig, Rodrigo Fonseca, and Tim Kraska. FITing-\nTree: A Data-Aware Index Structure. In Proceedings\nof the 2019 ACM SIGMOD International Conference\non Management of Data (SIGMOD ’19) , Amsterdam,\nNetherlands, June 2019.\n[21] Lars George. HBase: The Deﬁnitive Guide: Random\nAccess to Your Planet-size Data . O’Reilly Media, Inc.,\n2011.\n[22] Sanjay Ghemawhat, Jeff Dean, Chris Mumford,\nDavid Grogan, and Victor Costan. LevelDB.\nhttps://github.com/google/leveldb, 2011.\n[23] Guy Golan-Gueta, Edward Bortnikov, Eshcar Hillel,\nand Idit Keidar. Scaling concurrent log-structured data\n15\n\nstores. In Proceedings of the Tenth European Confer-\nence on Computer Systems , pages 1–14, 2015.\n[24] Alex Graves, Abdel rahman Mohamed, and Geoffrey\nHinton. Speech Recognition with Deep Recurrent Neu-\nral Networks. In 2013 IEEE International Conference\non Acoustics, Speech and Signal Processing , 2013.\n[25] Anna R Karlin, Kai Li, Mark S Manasse, and Susan\nOwicki. Empirical Studies of Competitve Spinning for\na Shared-memory Multiprocessor. ACM SIGOPS Op-\nerating Systems Review , 25(5):41–55, 1991.\n[26] Andrej Karpathy. Software 2.0.\nhttps://medium.com/@karpathy/software-2-0-\na64152b37c35, November 2017.\n[27] Eamonn Keogh, Selina Chu, David Hart, and Michael\nPazzani. An Online Algorithm for Segmenting Time\nSeries. In Proceedings 2001 IEEE international con-\nference on data mining , 2001.\n[28] Andreas Kipf, Ryan Marcus, Alexander van Renen,\nMihail Stoian, Alfons Kemper, Tim Kraska, and\nThomas Neumann. SOSD: A Benchmark for Learned\nIndexes, 2019.\n[29] Andreas Kipf, Ryan Marcus, Alexander van Re-\nnen, Mihail Stoian, Alfons Kemper, Tim Kraska,\nand Thomas Neumann. RadixSpline: A Single-Pass\nLearned Index. arXiv preprint arXiv:2004.14541 , may\n2020.\n[30] Tim Kraska, Mohammad Alizadeh, Alex Beutel, Ed H.\nChi, Ani Kristo, Guillaume Leclerc, Samuel Madden,\nHongzi Mao, and Vikram Nathan. SageDB: A Learned\nDatabase System. In Proceedings of 9th Biennial Con-\nference on Innovative Data Systems Research , Asilo-\nmar, CA, January 2019.\n[31] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and\nNeoklis Polyzotis. The Case for Learned Index Struc-\ntures. In Proceedings of the 2018 ACM SIGMOD In-\nternational Conference on Management of Data (SIG-\nMOD ’18) , Houston, TX, June 2018.\n[32] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-\nton. ImageNet Classiﬁcation with Deep Convolutional\nNeural Networks. In Advances in Neural Information\nProcessing Systems 25 (NIPS 2012) , Lake Tahoe, NV ,\nDecember 2012.\n[33] Avinash Lakshman and Prashant Malik. Cassandra – A\nDecentralized Structured Storage System. In The 3rd\nACM SIGOPS International Workshop on Large Scale\nDistributed Systems and Middleware , Big Sky Resort,\nMontana, Oct 2009.[34] St ´ephane Lathuili `ere, Pablo Mesejo, Xavier Alameda-\nPineda, and Radu Horaud. A comprehensive analysis\nof deep regression. IEEE transactions on pattern anal-\nysis and machine intelligence , 2019.\n[35] Pengfei Li, Yu Hua, Pengfei Zuo, and Jingnan Jia. A\nScalable Learned Index Scheme in Storage Systems.\narXiv preprint arXiv:1905.06256 , 2019.\n[36] Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir.\nOn the computational efﬁciency of training neural net-\nworks. In Advances in neural information processing\nsystems , pages 855–863, 2014.\n[37] Lanyue Lu, Thanumalayan Sankaranarayana Pillai,\nAndrea C. Arpaci-Dusseau, and Remzi H. Arpaci-\nDusseau. WiscKey: Separating Keys from Values in\nSSD-conscious Storage. In Proceedings of the 14th\nUSENIX Conference on File and Storage Technologies\n(FAST ’16) , Santa Clara, CA, February 2016.\n[38] Mathias Meyer. The Riak Handbook, 2012.\n[39] Patrick O’Neil, Edward Cheng, Dieter Gawlick, and\nElizabeth O’Neil. The Log-Structured Merge-Tree\n(LSM-Tree). Acta Informatica , 33(4), 1996.\n[40] Pandian Raju, Rohan Kadekodi, Vijay Chidambaram,\nand Ittai Abraham. PebblesDB: Building Key-Value\nStores using Fragmented Log-Structured Merge Trees.\nInProceedings of the 26th ACM Symposium on Oper-\nating Systems Principles (SOSP ’17) , Shangai, China,\nOctober 2017.\n[41] Robert Ricci, Eric Eide, and CloudLab Team. Introduc-\ning CloudLab: Scientiﬁc infrastructure for advancing\ncloud architectures and applications. USENIX ;login: ,\n39(6), 2014.\n[42] Russell Sears and Raghu Ramakrishnan. bLSM: A\nGeneral Purpose Log Structured Merge tree. In Pro-\nceedings of the 2012 ACM SIGMOD International\nConference on Management of Data (SIGMOD ’12) ,\nScottsdale, AZ, May 2012.\n[43] Shaohuai Shi, Qiang Wang, Pengfei Xu, and Xiaowen\nChu. Benchmarking state-of-the-art deep learning soft-\nware tools. In 2016 7th International Conference on\nCloud Computing and Big Data (CCBD) , pages 99–\n104. IEEE, 2016.\n[44] David Silver, Aja Huang, Chris J Maddison, Arthur\nGuez, Laurent Sifre, George van den Driessche, Julian\nSchrittwieser, Ioannis Antonoglou, Veda Panneershel-\nvam, Marc Lanctot, Sander Dieleman, Dominik Grewe,\nJohn Nham, Nal Kalchbrenner, Ilya Sutskever, Timo-\nthy Lillicrap, Madeleine Leach, Koray Kavukcuoglu,\nThore Graepel, and Demis Hassabis. Mastering the\n16\n\nGame of Go With Deep Neural Networks and Tree\nSearch. 529(7587), January 2016.\n[45] Chuzhe Tang, Youyun Wang, Zhiyuan Dong, Gansen\nHu, Zhaoguo Wang, Minjie Wang, and Haibo Chen.\nXIndex: A Scalable Learned Index for Multicore Data\nStorage. In Proceedings of the 25th ACM SIGPLAN\nSymposium on Principles and Practice of Parallel Pro-\ngramming , pages 308–320, 2020.\n[46] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff\nKlingner, Apurva Shah, Melvin Johnson, Xiaobing\nLiu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato,\nTaku Kudo, Hideto Kazawa, Keith Stevens, George\nKurian, Nishant Patil, Wei Wang, Cliff Young, Ja-\nson Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals,\nGreg Corrado, Macduff Hughes, and Jeffrey Dean.\nGoogle’s Neural Machine Translation System: Bridg-\ning the Gap between Human and Machine Translation.\nhttps://arxiv.org/abs/1609.08144, September 2016.\n[47] Qing Xie, Chaoyi Pang, Xiaofang Zhou, Xiangliang\nZhang, and Ke Deng. Maximum Error-bounded Piece-\nwise Linear Representation for Online Stream Approx-\nimation. The VLDB journal , 23(6), 2014.\n17",
  "textLength": 84277
}