{
  "paperId": "0fa8ff340147bec4e384da881566a0352f843f46",
  "title": "From Specific to Generic Learned Sorted Set Dictionaries: A Theoretically Sound Paradigm Yelding Competitive Data Structural Boosters in Practice",
  "pdfPath": "0fa8ff340147bec4e384da881566a0352f843f46.pdf",
  "text": "From Specific to Generic Learned Sorted Set\nDictionaries: A Theoretically Sound Paradigm\nYelding Competitive Data Structural Boosters in\nPractice\nDomenico Amato Giosué Lo Bosco Raffaele Giancarlo\nDipartimento di Matematica e Informatica\nUniversitá degli Studi di Palermo, ITALY\nSeptember 6, 2023\nAbstract\nThis research concerns Learned Data Structures, a recent area that has\nemerged at the crossroad of Machine Learning and Classic Data Structures.\nIt is methodologically important and with a high practical impact. We\nfocus on Learned Indexes, i.e., Learned Sorted Set Dictionaries. The\nproposals available so far are specific in the sense that they can boost,\nindeed impressively, the time performance of Table Search Procedures with\na sorted layout only, e.g., Binary Search. We propose a novel paradigm\nthat, complementing known specialized ones, can produce Learned versions\nof any Sorted Set Dictionary, for instance, Balanced Binary Search Trees or\nBinary Search on layouts other that sorted, i.e., Eytzinger. Theoretically,\nbasedonit, weobtainseveralresultsofinterest, suchas(a)thefirstLearned\nOptimum Binary Search Forest, with mean access time bounded by the\nEntropy of the probability distribution of the accesses to the Dictionary;\n(b) the first Learned Sorted Set Dictionary that, in the Dynamic Case and\nin an amortized analysis setting, matches the same time bounds known\nfor Classic Dictionaries. This latter under widely accepted assumptions\nregarding the size of the Universe. The experimental part, somewhat\ncomplex in terms of software development, clearly indicates the non-\nobvious finding that the generalization we propose can yield effective and\ncompetitive Learned Data Structural Booster, even with respect to specific\nbenchmark models.\n1 Introduction\nWith the aim of obtaining time/space improvements in classic Data Structures,\nan emerging trend is to combine Machine Learning techniques with the ones\n1arXiv:2309.00946v1  [cs.DS]  2 Sep 2023\n\nproper for Data Structures. This new area goes under the name of Learned Data\nStructures. It was initiated by [ 26], it has grown very rapidly [ 14] and now it\nhas been extended to include also Learned Algorithms [ 31], while the number of\nLearned Data Structures grows [ 7]. In particular, the theme common to those\nnew approaches to Data Structures Design and Engineering is that a query to a\ndata structure is either intermixed with or preceded by a query to a Classifier\n[13] or a Regression Model [ 16], those two being the learned part of the data\nstructure. Learned Bloom Filters [ 10,18,26,30,38] are an example of the first\ntype, while Learned Indexes, here referred to as Learned Sorted Set Dictionaries,\nare examples of the second one [ 1,2,3,4,5,14,22,23,28]. A “predecessor\"\nof those models has been proposed in [ 6]. Those latter are the object of this\nresearch.\n1.1 Learned Searching in Sorted Sets\nWith reference to Figure 1, a generic paradigm for learned searching in sorted\nsets consists of a model, trained over the input data. As described in Section 2.2\nhere and Section 3 of the Supplementary File, such a model may be as simple as\na straight line or more complex, with a tree-like structure. It is used to make a\nprediction regarding where a query element may be in the sorted table. Then,\nthe search is limited to the interval so identified and, for the sake of exposition,\nperformed via Binary Search. It is quite remarkable then that the novel model\nproposals, sustaining Learned Dictionaries, are quite effective at speeding up\nBinary Search at an unprecedented scale and are competitive with respect to\neven more complex Dictionary structures, i.e., B-Trees [8]. Indeed, a recent\nbenchmarking study [ 28] (see also [ 22]) shows quite well how competitive those\nLearned Data Structures are. Another, more recent, study offers an in-depth\nanalysis of Learned Dictionaries and provides recommendations on when to use\nthem as opposed to other data structures [ 27]. In theoretic terms, those Learned\nModels yield search procedures that, in the worst-case scenario, are no worse\nthan the basic routines we have mentioned earlier, provided that the prediction\ncan be made in O(logn)time, where ndenotes the size of the set to be searched\ninto. However, de facto they are boosters of the time performance of Sorted\nTable Set procedures, limited to Classic Binary Search [ 25] and Interpolation\nSearch [34]. Indeed, critical to their proper working is the use of one of those\ntwo procedures in the final search stage. Other array layouts for Binary Search,\nfor instance very efficient ones such as Eytzinger [ 21], cannot be used by those\nmodels. More in general, Sorted Set Dictionaries, other than the two already\nmentioned, cannot be used. Therefore, the boosting power of those Models is\nmethodologically limited and a natural question is: to what extent Learned\nModels for Sorted Set Dictionaries can be made generic, i.e., able to work for any\nSorted Set Dictionary handling the final search stage. A related question, to be\nsettled inherently experimentally, is to what extent the boosting effect obtained\nfor Binary and Interpolation Search would apply to relevant classes of Sorted\nSet Dictionaries, e.g., array layout other than sorted [ 21], Balanced Search Trees\n[9], Cache Oblivious Search Trees such as CSStrees [35]. Answers to those two\n2\n\nrelated questions are not available, although they would be methodologically\nand practically important. As a matter of fact, and quite surprisingly, those\nquestions have been overlooked so far.\n1.2 Our Contributions\nWe settle on the positive both related questions by proposing a novel paradigm\nin which to cast the design and engineering of Learned Sorted Set Dictionaries.\nIt is presented in Section 2.3, where we also point out that the RMI[26] and\nPGM[15] can be modified to be part of it, those two models being reference in\nthe current State of the Art.\nA novel class of models, which we refer to as Binning , that comes out of the\nparadigm introduced here, is analyzed theoretically in Section 3. It naturally\nlends itself to providing theoretic guarantees in terms of average access time and\ndynamic operations that are the best known in the Literature. The theoretic\npart is a natural, although surprisingly overlooked, generalization of techniques\ncoming from Dynamic Interpolation Search on non-independent data [ 11], which\nwe cast into a Learned setting. Its methodological merit is to make the theoretic\nanalysis of Learned Sorted Set Dictionaries, an aspect usually addressed poorly,\nsurprisingly simple by allowing the “re-use\" of deep results coming from Data\nStructures.\nAs for the experimental part, we consider the static case only, since it is the\nmost consolidated in the Literature, with accepted benchmarks [ 22,27,28]. All\nour experiments are performed within the Searching on Sorted Sets ( SOSD\n[22,28] andCDFShop [29] reference software platforms, with the datasets\nprovided there. Data and platforms are available under the GPL-3.0 license. We\nanticipate that an experimental study concerning the dynamic case is planned.\nOur experiments are reported in Section 5.1 in regard to boosting ability.\nWe consider the Dictionaries briefly described in Section 2.1. In regard to\nthem, the Binning model yields consistent boosting. The PGMmodel, via an\nimplementation suitably modified to be part of the paradigm, also provides some\nboosting, but not for all procedures we have considered. Additional experiments\nhighlight the reasons, bringing to light non-obvious trade-offs between the time\nto perform a prediction and the time that a procedure takes to search on the\nfull input dataset.\nFinally. in Section 5.2, we report experiments clearly showing that the\nBinning Model can yield instances of Learned Sorted Set Dictionaries that are\ncompetitive, and many times superior to, benchmark Models such as the RMI\nandPGMin their standard and specific implementations. The source code is\navailable https://github.com/globosco/An-implementation-of-Generic-Learned-\nStatic-Sorted-Sets-Dictionaries).\n3\n\nQuery Element\n{1\n5\n11\n14\n58\n59\n60\n97\n100\n101ModelFigure 1: A general paradigm of Learned Searching in a Sorted Set [ 28]. The model is\ntrained on the data in the table. Then, given a query element, it is used to predict the interval in\nthe table where to search (included in brackets in the figure).\n2From Specific to Generic Learned Dictionaries\n2.1 Dictionaries Over Sorted Sets\nGiven an universe Uof integers, on which it is defined a total order relation, a\nStatic Sorted Set Dictionary SDis a data structure that supports the following\noperations over a sorted set A⊆Uofnelements: (a) search (x) =TRUEif\nx∈A, otherwise FALSE , (b) PSP (x) =max{y∈A|y < x}, i.e. predecessor\nsearch; (c) range (x, y) =A∩[x, y]. The dictionary is Dynamic , if it supports\nalso (d) insert (x)ifx /∈A; (e)delete (x)ifx∈A.\nFrom now on, for brevity, we refer to SDsimply as Dictionary, either Static\nor Dynamic. Again, for brevity and unless otherwise specified, we consider\nsearchonly, since PSPis a simple variant of it and rangereduces to search.\nIt is also useful to point out that, for the experimentation regarding this\nresearch that, as already motivated in the Introduction, is limited to Static\nDictionaries, we have used the following algorithms and data structures.\nSorted Table Search Procedures and Variants. Based on established\n“textbook material\" and accounting for extensive benchmarking studies present\nin the Literature [ 21], we use the following algorithms. Standard Binary Search\n[25], Uniform Binary Search [ 21,25], Eytzinger layout Binary Search [ 21], B-trees\nlayout Binary Search [ 21], and Interpolation Search [ 34]. Details regarding their\nimplementation are available in Section 1.1 of the Supplementary File. They are\nabbreviated as BBS, BFS, BFE, BFT, IS , respectively. For BFT, a number\nfollowing the acronym indicates the page size we are using in our experiments.\nSearch Trees: Static and Dynamic. We consider the CSS Trees [ 35], since\nthey are one of the earliest data structures that try to speed up Binary Search,\nwith the use of additional space and being “cache conscious\". We denote them\nhere asCSS. As for Balanced Search Trees, although they have been designed\nfor the dynamic setting of the problems we are considering, they are of interest\nalso in the static case. Among the many possible choices, we consider the\nSelf-Adjusting Binary Search Tree [ 36], denoted SPLAY , since it is one of the\nfew data structures that “learns\" its “best\" organization from the data. It is to\n4\n\nbe pointed out that the very well known B-Trees [8] is well represented by the\nB-trees layout Binary Search.\n2.2ModelsSpecificForBoostingBinaryandInterpolation\nSearch\nKraska et al. [ 26] have proposed an approach that transforms PSPandsearch\ninto a learning-prediction problem when Ais represented as a sorted table.\nConsider the mapping of elements in the table to their relative position within\nthe table. Since such a function is reminiscent of the Cumulative Distribution\nFunction over the universe Uof elements from which the ones in the table\nare drawn, as pointed out by Markus et al. [ 28], we refer to it as CDF. Very\nbriefly, the models proposed so far build an approximation of the Cumulative\nDistributionFunction(CDF)ofthetable, whichisusedtopredictwheretosearch\nfor a given query element. For the convenience of the reader, a simple example of\nthe learning and query process is provided in Section 2 of the Supplementary File.\nFormal definitions are available in [ 28]. The models that have been proposed so\nfar for Learned Sorted Set Dictionaries (see [ 4,1,14,22,23,28] ), when queried,\nall return an index iinto the table and an approximation ϵaccounting for the\nerror in predicting the position of the query element within the table. Then,\nthe search is finalized via Binary Search in the interval [i−ϵ, i+ϵ]ofA. Other\nsearch routines such as Interpolation Search can be used for the final search\nstage. For this research, and following a recent authoritative benchmarking study\n[28], we take as reference the RMIand thePGMmodels. For the convenience\nof the reader, they are described in detail in Section 3 of the Supplementary\nFile, together with an atomic model that summarizes the simple presentation of\nLearned Dictionaries given so far.\n2.3 Models for Boosting Generic Dictionaries\nSince we are interested in testing the generality of the “boosting effect\" discussed\nin the Introduction, we introduce a new class of Models.\nDefinition 2.1. A Generic Model of type Dfor sorted set Ais a “black box\"\nthat returns an explicit partition of the sorted universe Uinto intervals, with the\nelements of Aassigned to intervals and kept sorted. A visit of the partition from\nleft to right provides A. Moreover, in O(logn)time, given an element x∈U, it\nprovides as output the unique interval in the partition where xmust be searched\nfor, in order to assess whether it is in Aor not.\nThe main difference between the Models characterized by Definition 2.1 and\nthe ones used so far for Learned Dictionaries is that, apart from the partition of\nUbeing explicit, no two elements can have an intersecting prediction interval.\nIt is to be noted, however, that the PGMIndex can be easily transformed\ninto a Generic Model. In principle, multi-layer RMIs, with a tree structure,\ncan also be adapted to be Generic Models. However, due to the way they are\nimplemented and “learned\" right now, such a transformation would require a\n5\n\nmajor reorganization of their implementation and “learning\" code. For those\nreasons, among the Models proposed so far, we consider only the PGM, for the\nexperimental part of this research.\nDefinition 2.2. LetˆDbe an instance of a Generic Model of type DforA,\nconsisting of kintervals and let SDbe a Dictionary. Its learned version, with\nrespect to model ˆDand denoted as Dk, is obtained by building separately an\nSDfor each sorted set within each interval in the partition. In order to answer\na query, a prediction query to the model Dkreturns a pointer to the predicted\ninterval in the partition and the SDassociated with it is queried.\nGeneric Models can be subdivided into two families: The ones in which the\nintervals of the partition are of fixed length and the ones in which their lengths\nare variable. We discuss an example of the first type in Section 3, which is\nan extension to the Learned setting of a Dynamic Interpolation Search Data\nStructure proposed by Demaine et al. [ 11]. The method overlooked so far within\nthe development of Learned Sorted Set Dictionaries, is related to the estimation\nof probability density functions via histograms, e.g, [ 17]. As for the second type,\nand as already anticipated, we consider the PGM, suitably modified to fit the\nnew paradigm, for our experiments.\n3Learned Generic Dictionaries: The Case of\nEqual Length Intervals via Binning\nThe static case is presented in Sections 3.1-3.4, while Section 3.5 is dedicated to\nthe dynamic case. For brevity, the proofs on the Theorems listed next are in\nSection 4 of the Supplementary File.\n3.1Definition, Construction and Worst Case Search Time\nFix an integer k=O(n). The universe Uis divided into kbinsB1,···, Bk, each\nrepresenting a range of integers of sizeA[n]−A[1]\nk. Each bin has associated the\ninterval of elements of Afalling into its range. Let SDbe a Dictionary. Its\nLearned version Dkis built as follows. For each of the mentioned intervals, we\nbuild a data structure SDcontaining the elements in that interval. Moreover,\nthere is an auxiliary array such that its j-th entry provides a pointer to the data\nstructure assigned to bin j, which may be empty (no elements in the bin). As for\nquery, given an element x∈U, the bin in which we should search is identified\nvia the formula i=⌈(x−A[1])k\nA[n]−A[1]⌉.\nLet the gap ratio be ∆ =Gmax\nGmin, where Gdenotes the distance between two\nconsecutive elements in A. Notice that Gmin>0, since Ais a set, implying that\n∆is finite. The role of ∆, discussed in Section 3.3, is to “capture\" the amount of\n“pseudo-randomness\" in the input data. We have the following.\nTheorem 3.1. Assume that SDcan be built in O(cg(c))time on celements.\nAssume also that g(c)is convex and non-decreasing. Then, Dkcan be built in\n6\n\nO(ng(n))time. Moreover, given an element x∈U, the time to search for it in\nthe model Dk, built for SD, isO(f(min(n,n∆\nk)), assuming that searching in SD\ncan be done O(f(c))time when it contains celements.\n3.2Search Time and Input Data Distributions: Smooth-\nness\nWe consider a family of probability distributions, from which the elements in\nAare drawn from U, that basically “mimics\" the nice property of the Uniform\nDistribution. Such a family, or specialisations of it, have been used to carry out\naverage case analysis of Static and Dynamic Interpolation Search (see [ 20], and\nreferences therein). Maximum Load Balls and Bins Chernoff Bounds arguments\n[37] play a fundamental role here. Let ηbe a discrete probability distribution\nover the universe U, with unkown parametes. Given two function f1andf2,\n∀x∈U,ηis(f1, f2)-smooth [ 20] if and only if there exists a constant βsuch that\nfor all c1, c2, c3∈U:c1< c2< c3and for all naturals ν≤n, for a randomly\nchosen element X ∈Uit holds:\nPR[c2− ⌊c3−c1\nf1(ν)⌋ ≤ X ≤ c1|c1≤ X ≤ c3]≤βf2(ν)\nν(1)\nEquation (1) states that, when we divide the universe Uintof1(ν)bins, then\nno bin has probability mass more thanβf2(ν)\nν. That is, the probability mass is\nevenly split among the bins.\nTheorem 3.2. Assume that the elements in Aare drawn from an (f1, f2)-\nsmooth distribution, with f1(n) =cn, for some constant 0< c≤1, and f2(n) =\nO(lnO(1)n). Then, given an element x∈U, the time to search for it in the\nLearned Dictionary Df1(n), built for SD, isO(f(min(logO(1)n,∆\nc)), with high\nprobability (i.e. 1−o(1)) and assuming that searching in SDcan be done in\nO(f(m))time when it contains melements.\nIt is to be noted that the Uniform Distribution is a smooth distribution\naccording to the definition given above. Other smooth distributions of interest\nare mentioned in [ 20]. For those distributions, the Binning-based Learned\nDictionary Dk, with k=cnand0< c≤1, can be queried in O(log logn )\nexpected time via a terminal stage of Binary Search, generalizing well-known\nresults regarding Interpolation Search, e.g., [19, 33, 39].\n3.3Search Time and Input Data Distributions: Non-\nIndependence, Real Datasets and the Role of ∆\nIn the original proposal by Demaine et al. [ 11] of their bin data structure that\nwe have extended here, the role of ∆is meant to allow an extension of the\ngood behaviour of Interpolation Search on input data drawn uniformly and at\nrandom from Uto non-random ones, in particular non-independent. That is,\n∆is supposed to capture the amount of “pseudo-randomness\" in the data. To\n7\n\nthis end, Demaine et al. showed that, when the input data is drawn from the\nUniform Distribution and for k=n, then ∆ = O(polilogn). That is, if we\nconsider only ∆in the complexity of searching in Dn, for Uniform input data\nDistributions, then the well known O(log log n)time bound for Interpolation\nSearch would hold for Dn. Unfortunately, their theoretic result is non easily\nextendable, if at all, to Probability Distributions other than Uniform. Therefore,\nwe study ∆experimentally and on real benchmark datasets. The experiment\nwe have performed, due to space limitation, is described with its outcome, in\nSection 6 of the Supplementary File. We limit ourselves to report that the\npolilognbehaviour of ∆depends on the dataset, rather than on the CDF and\nPDF characterizing the dataset. However, there is no harm in using it. Those\nexperiments hint at the fact that ∆may play some role, depending on the\nspecific dataset. To the best of our knowledge, we provide the first study of this\nimportant parameter.\n3.4Search Time and Query Distributions: Learned Opti-\nmal Binary Search Forest with Entropy Bounds\nAssumenowthatwearegiven 2n+1probabilities, p1, p2, . . . , p nandq0, q1, . . . , q n,\nwhere each piis the probability of searching for A[i],q0is the probability of\nsearching for an element less than A[1],qnthe probability of searching for an\nelement larger than A[n], and qiis the probability of searching for an element\nbetween A[i]andA[i+ 1]. We assume that the sum of all those probabilities is\none. Moreover, we denote the first part of the distribution by Pand the second\none by Q. Recall from the Literature [ 24,32,40], that the cost of a Binary Search\nTree Tis is defined as C(T) =/summationtextn\ni=1pi×(depth (A[i]) + 1)) +/summationtextn\ni=0qi×depth (i),\nwhere depth (A[i])is the depth of the node storing A[i]inTanddepth (i)is\nthe the depth of the external leaf (finctuous) corresponding to an unsuccessful\nsearch. The Optimum Binary Search Tree, or good approximations of it, can be\nfound efficiently [ 32]. Assuming for simplicity that no bin is empty, we extend\nthe notion of Optimum Binary Search Tree to Learned Optimum Binary Search\nForest as follows.\nLetℓ1, ℓ2, . . . , ℓ kbe the number of elements in the bins. Consider bin Bsand\nassume that it contains all elements in A[i, j]. The weight of Bscan be defined\nasW(Bs) =Pi,j+Q′\ni,jwhere Pi,jis the portion of Prestricted to the elements\nin the bin. As for Q′, it is as Qi,j(the analogous of Pi,j), except that the failure\nprobabilities preceding piand succeeding pjare lower than the corresponding\nones in Q: elements not present may fall in part in a bin and in part in its\nneighbours. Note that W(Bi)can be interpreted as the probability of searching\nfor an element in Bs. A Learned Binary Search Forest can readily be obtained\nby storing the elements of each bin into a Binary Search Tree. Its cost can be\ndefined as C(Dk, T) = Σk\ni=1W(Bi)(1 + C(T(Bi))), where T(Bi)is the cost of a\nBinary Search Tree storing the elements in Bi, with access weigths Pi,jandQ′\ni,j.\nLetH(P, Q)be the entropy of the probability distribution given by PandQ.\nTheorem 3.3. Consider the access probabilities PandQ, defined earlier, to\n8\n\nelements of Aand let the interval numbers in a partition be bounded by kmax.\nWe have (a) for 1≤k≤kmax, the Optimum Learned Binary Search Forest, i.e,\nthe one with the best cost among all k’s, can be computed in O(n2kmax)time,\nand its cost is bounded by H(P, Q) + 2; (b) an approximate Optimum Learned\nBinary Search Forest, with cost bounded as in the optimum case, in O(nkmax)\ntime.\nIt is to be noted that, to the best of our knowledge, the PGMmodel is the\nonly one for which one can prove “entropy bounds\" on access costs, but only for\nsuccessful searches. The analysis reported in [ 15] is not as general as the one\nreported here in terms of success and failure search probabilities and optimality\nis not considered.\n3.5 The Dynamic Case\nAgain, this is a variant of the Dynamic Case of Interpolation Search proposed\nby Demaine et al. in Section 3 of their paper [ 11]. Letting SDnow denote the\ndynamic dictionary to be coupled with Dk. which is built as in the static case,\nwith a few important changes. Elements in each bin are allocated to a copy of\nSDeach. Let ˆ∆be the gap ratio computed when the structure is initially built.\nDuring its evolution, the structure will be valid for values of ∆≤ˆ∆. When this\ncondition no longer holds, ˆ∆is recomputed from the data in the structure, as\nspecified shortly. The range being considered is now [A[1]−Lˆ∆, A[n] +Lˆ∆],\nwhere L=A[n]−A[1]. That inteval is subdivided into kbins, each of size\n(2ˆ∆+1)) L\nk. The structure is rebilt when n/2updates have taken place without\na rebuild or the value of ˆ∆is no longer valid, i.e., the gap ratio has gone up\ndue to the updates. In this latter case, we rebuild the data structure with\nˆ∆=max(∆new,2ˆ∆old), where ∆newis computed from the data present in the\nstructure and ˆ∆oldis the gap ratio of the last rebuild.\nTheorem 3.4. Assume that SDcan be built in linear time and that it supports\nsearch,insertanddeleteoperations in logtime. For the Learned Dictionary\nDk, the following bounds hold. The worst case cost of search,insertanddelete\nisO(log min (n,n\nk∆max), where ∆maxis the maximum ∆achieved over the\nlifetime of the structure. An analogous bound applies to the case in which the\noperations can be done in log logtime. The rebuilding of the data structure takes\nan amortized cost per element of O(logn\nk∆max)time. When ∆maxis known a\npriori, the total cost of rebuild is now linear in the number of elements in the\ndata structure, over its lifetime.\nCorollary 3.5. When |U|=O(nc),cconstant, under the same assumpions of\nTheorem 3.4, search,insert,deletehave the same worst case time bounds. The\nrebuilding of the data structure takes an amortized cost per element of O(logn)\ntime.\nIt is to be noted that Corollary 3.5 provides the first results concerning\nDynamic Learned Sorted Set Dictionaries with logarithmic performance, both\n9\n\nin terms of operations and rebuild. This is analogous to what one gets with\nclassic data structures and the result here is methodologically important. Prior\nto this, the PGMModel is the only one for which provably good bounds have\nbeen given, but searchis not O(logn)worst-case time, as here. Finally, the\nassumption that the size of the Universe is polynomially related to the size of\nthe dataset is a widespread and commonly accepted assumption, that has its\nroots both in “theory\" and “practice\", e.g., see discussion in [20].\n4 Experimental Methodology\nAll the experiments have been performed on a workstation equipped with an\nIntel Core i7-8700 3.2GHz CPU with 32 Gbyte of DDR4. The operating system\nis Ubuntu LTS 20.04. We use the same real datasets of the benchmarking study\non Learned Indexes [ 22,28]. In particular, we restrict attention to integers only,\neach represented with 64 bits, since the datasets with 32 bits add no discussion\nto the experiments. For the convenience of the reader, a list of those datasets,\nwith an outline of their content, is provided next. They are: (a) amzn: book\npopularity data from Amazon (each key represents the popularity of a particular\nbook); (b) face: randomly sampled Facebook user IDs, (each key uniquely\nidentifies a user); (c) osm: cell IDs from Open Street Map ( each key represents\nan embedded location); wiki: timestamps of edits from Wikipedia (each key\nrepresents the time an edit was committed). Each dataset consists of 200 million\nelements for roughly 1.6Gbytes occupancy. As for query dataset generation, for\neach of the tables mentioned earlier, we extract uniformly and at random (with\nreplacement) from the Universe Ua total of two million elements, 50% of which\nare present and 50% absent, in each table. All these datasets are available in\nhttps://osf.io/ygnw8/?view_only=f531d074c25b4d3c92df6aec97558b39.\n5 Experiments: Results and Discussion\n5.1 Boosting\nAs anticipated in Section 2, we investigate whether Generic Learned Dictionaries\nprovide the “boosting effect\" already known in the Literature for Specific Learned\nDictionaries. So, as anticipated in Section 2.3, we analyze the following two\ncases, in the static scenario.\nBinning. Fix a Dictionary SD, chosen from the ones used for this research\nand described in Section 2.1 and consider its Learned Generic version obtained\nviaBinning . For each dataset, we increase the space occupied by the Generic\nLearned version, growing the number of bins in percentage with respect to\nthe number of elements in the given Tables, i.e. from 0% to 100%, and we\ncalculate the ratio between the mean query time of a Generic Learned version Dk\nand its non-learned version SD. A ratio below one indicated that the Generic\nLearned version boosts the performance of the non-learned version. The results\n10\n\nare reported in Figure 2(a). As it is evident from that Figure, all the Data\nStructures chosen for this research register a boosting effect, except for the face\ndataset. The explanation is quite simple. Although the CDF of face, shown\nin Figure 6 of the Supplementary File, provides the impression of “uniformity\",\nthere are a few outliers that cause most of the bins to be empty, with most of\nthe elements falling in a few bins. That is, the Binning method is sensible\nto outliers that unbalance the binning. Fortunately, for other real CDFs (see\nagain Figure 6 in the Supplementary File), even as complicated as the one of\nosm, the improvement is quite relevant. With reference to those datasets, it is\nquite impressive that the improvement follows the same shape in all of them.\nMoreover, most of the gain of using a Binning Learned Dictionary over the\nstandard Dictionary is concentrated around a low percentage of bins being used.\nThat is, “the spreading of elements over the bins\" has a “diminishing return\"in\nterms of mean query time, as the number of bins grows.\nPGM. The division of the Universe Uinto intervals now depends on the\napproximation parameter ϵ(see the description of this Model in Section 3 of\nthe Supplementary File). We proceed as follows. For each dataset, we choose\nϵas a power of two in the interval [1,n\n2]. That is, we built models with an\nincreasing error that partitions the Universe Ufrom very small intervals to the\none that contains the whole dataset. Then, we proceed as in the Binning case,\ngiven a dictionary SD. The results are reported in Figure 2(b). As evident\nfrom that Figure, we have a boosting effect only for IBSandSPLAY . In\norder to gain insights into the reason for that, we have performed additional\nexperiments, involving Binning and thePGM. They are reported in Figures\n7-12 of the Supplementary File. For the Binning Model, the meantime to\nperform a prediction is negligible with respect to the subsequent search stage\non a reduced set of data, i.e., the search routine can take full advantage of the\nreduction in the size of the dataset to search into. It is worth recalling that the\nprediction in this model takes O(1)time. As for the PGM, a prediction can\nbe made by “navigating\" a tree (see the description of the PGMin Section 3\nof the Supplementary File), i.e., it is no longer a constant since it depends on\nthe number of segments in which Uhas been divided (see the analysis in [ 15]).\nAlthough the reduction in dataset size may be substantial (data not shown and\navailable upon request), it is now the trade-off between prediction time and\nsearch in the full set that determines the boosting effect.\nIn conclusion, those experiments hint at the fact that we get a boosting\neffect out of Generic Learned Models, as long as the “navigation\" time to get to\nthe appropriate interval is rather small, compared to a full-fledged search into\nthe original dataset. It is to be noted that a two-level RMI, which is the one\nrecommended for use in [ 28], also has the potential for an O(1)“navigation\".\nHowever, duetoitscurrenthighlyengineeredimplementation, isnotsoimmediate\nto transform that software into a Generic RMIModel, without risking to\ncompromise its query efficiency.\n11\n\n(a)\n0 20 40 60 80 10000.20.40.60.81\n0 20 40 60 80 10000.20.40.60.811.2\n0 20 40 60 80 10000.20.40.60.81\n0 20 40 60 80 10000.20.40.60.81BBS\nBFS\nIBS\nBFE\nBFT512\nBFT32k\nSPLAY\nCSSamzn face\nosm wiki (b)\n26 24 22 20 18 16 14 12 10  8  6  4  200.511.522.53\n26 24 22 20 18 16 14 12 10  8  6  4  200.511.522.533.5\n26 24 22 20 18 16 14 12 10  8  6  4  2012345\n26 24 22 20 18 16 14 12 10  8  6  4  200.511.522.533.54amzn face\nosm wikiBBS\nBFS\nIBS\nBFE\nBFT512\nBFT32k\nSPLAY\nCSS\nFigure 2: Boosting Property for Binning and PGM Models . The Dictionaries considered for\nthose experiments are provided in the Legend. (a) The xaxis reports, for each dataset, the number\nof bins in percentage with respect to the number of elements in the sorted set. Given a dictionary\nSD, the yaxis shows the ratio between the mean query time of the Binning Learned version Dk\nandSDalone. A yvalue below one indicates the superiority of the Binning Learned Model vs SD\nalone. (b) The xaxis reports, for each dataset, the chosen ϵfor the PGMconstruction. The yaxis\nis analogous to the Binning case.\n5.2Competitiveness of Generic Learned Dictionaries with\nRespect to Specific Ones\nThese experiments provide a comparison with the Models that well represent\nthe State of the Art, i.e. RMIandPGM. In particular, in the following, we\ndiscuss two possible scenarios.\nNo Bounds on Model Space. In this case, how much space the model\nuses with respect to the one occupied by the input sorted set is not a critical\nrequirement, i.e., query time is privileged. For each dataset, we have built a\nBinning Learned Dictionary for each Dictionary SDused for the experiments\nin this research. Then, among all these Learned Dictionaries, we have chosen\nthe one with the smallest mean query time. For each dataset, we have also\ntrainedRMIandPGMmodels, in agreement with the benchmark procedures\nadopted in [ 4,1,28], i.e., we have taken the top (at most) ten performing models\nconsideredby SOSD.Wehavethenchosen, foreachoftheconsideredmodels, the\none that provides the best mean query time. The results are reported in Figure\n3(a). Given the fact that the RMIandPGMare among the best performing\nmodels in the Literature, it is quite remarkable that the Binning strategy, with\nBBSas the final search routine, outperforms those models on three of the four\nbenchmark datasets. Its relatively poor performance on the facedatasets with\nrespect to the RMIis motivated by the presence of outliers that, as already\nobserved, prevent to take full advantage of the Binning strategy. Another\nnotable fact is that the use of layouts other than sorted, e.g., BFEmay be of help.\nThose layouts cannot be used within the current Specific Models. Interestingly,\nwith reference to the CDF figures reported in Fig. 6 of the Supplementary File,\ntheBinning strategy is able to perform well on CDFs of different nature. On\nthe “munus\" side, the Binning approach tends to use more space than the\nother best models.\n12\n\nBounds on Model Space. A model that guarantees a good mean query time\nin small additional space, with respect to the one taken by the input sorted set,\nis desirable in many applications [ 4,1,15]. Therefore, for each dataset, we now\nimpose three space bounds that a model must satisfy, expressed in percentage\nwith respect to the size that the sorted set would occupy if stored in an array.\nNamely, 0.05,0.07,0.2%. Then, for each percentage, we have selected the best\nmean query time Binning Learned Dictionary that satisfies the given percentage\nbound. As for the other Models considered in this research, we have done the\nsame selection, reporting the performance of the best model, for each space\nbound. Finally, we have also considered the SY-RMI Model, which belongs to\nthe class of RMIModels, and that has been specifically designed to yield good\nmean query times in small space [ 4,1]. The results are reported in Figure 3(b).\nThose results bring to light that, even in small space, the Binning strategy\nis competitive with respect to reference models, in particular when used on\ndatasets with a complex CDF, e.g., osm. Again, it is of interest to notice that\ncompetitive performances are obtained via the use of BFEfor the final search\nstage, a routine that can be used by current Specific Models.\n(a)\namzn face osm wiki012345Mean Query Time (s)1e7\n6.29e+00\n1.26e+01\n2.52e+01\n2.52e+012.82e+00\n2.74e+00\n7.43e+00\n8.92e-019.70e+01 -  bbs\n0.00e+00 -  bfe\n9.40e+01 -  bbs\n9.80e+01 -  bbsRMI\nPGM\nBIN (b)\n<=0.05 <=0.7 <=2012345Mean Query Time (s)1e7\n4.92e-02\n3.93e-01\n7.86e-016.11e-03\n2.57e-01\n1.85e+002.50e-02 -  ibs\n3.68e-01 -  bfe\n1.00e+00 -  bfe2.00e-02\n3.50e-01\n1.00e+00amzn\n<=0.05 <=0.7 <=202468Mean Query Time (s)1e7\n4.92e-02\n3.93e-01\n7.86e-012.32e-02\n1.51e-01\n1.35e+005.00e-02 -  bfe\n7.36e-01 -  bfe\n2.00e+00 -  bfe5.00e-02\n7.00e-01\n2.00e+00face\n<=0.05 <=0.7 <=201234567Mean Query Time (s)1e7\n4.92e-02\n1.15e-01\n1.15e-012.71e-02\n4.21e-01\n1.74e+005.00e-02 -  bfe\n7.36e-01 -  bfe\n2.00e+00 -  bfe5.00e-02\n7.00e-01\n2.00e+00osm\n<=0.05 <=0.7 <=20.00.51.01.52.02.53.03.54.0Mean Query Time (s)1e7\n4.92e-02\n3.93e-01\n7.86e-014.72e-02\n2.88e-01\n8.92e-015.00e-02 -  ibs\n7.36e-01 -  ibs\n2.00e+00 -  bbs5.00e-02\n7.00e-01\n2.00e+00wikiRMI\nPGM\nBIN\nSYRMI\nFigure 3: Learned Indexes Query Time. (a) No Bounds on Space. For each dataset, we report\nthe mean query time of the best Learned Indexes including the Generic Learned Dictionary denoted\nwithBIN. Above each bar, we report the model space used as a percentage of the table size. For\nBIN, it is also indicated which dictionary performs best. (b) With Bounds on Space. For each\ndataset, we choose three space bounds as detailed in the main text. For each space-bound, from left\nto right, we report the mean query time of the best RMIand the PGMthat satisfies the imposed\nbound. The next bar is the mean query time for the best Binning Generic Learned Dictionary that\nsatisfies the given bound on space. The last bar indicates the mean query time for the SY-RMI\nthat satisfies the bound on space.\n6 Conclusions and Future Directions\nWe have provided a new paradigm for the design of Learned Dictionaries. As\nopposed to the current state of the Art, it can be applied to any Sorted Set\nDictionary, ratherthantoonlysearchroutineswithasortedlayout. Thetheoretic\nanalysis performed for the Binning Model shows that we can leverage on classic\n13\n\nresults from Data Structures to obtain sound evaluations of the performance\nof Learned Dictionaries, an aspect usually addressed poorly. We have also\ngiven experimental evidence that the new paradigm, as far as the static case is\nconcerned, can yield valid Data Structural Boosters and be competitive with\nreference solutions available in the Literature. For the future, the dynamic case is\nto be considered. It implies a careful re-design of the software solutions available\nso far, i.e., [ 12,15] as well as a well planned experimental setting since it is not\nclear that the one available for the static case may be the best choice to study\nthe dynamic case. We point out that the societal impacts of our contribution\nare in line with general-purpose Machine Learning technology.\nReferences\n[1]D. Amato, G. Lo Bosco, and R. Giancarlo. Learned sorted table search and\nstatic indexes in small model space. In AIxIA 2021 – Advances in Artificial\nIntelligence: 20th International Conference of the Italian Association for\nArtificial Intelligence, Virtual Event, December 1–3, 2021, Revised Selected\nPapers, page 462–477, Berlin, Heidelberg, 2021. Springer-Verlag.\n[2]D. Amato, G. Lo Bosco, and R. Giancarlo. Neural networks as building\nblocks for the design of efficient learned indexes. Neural Computing and\nApplications , 2023.\n[3]D. Amato, G. Lo Bosco, and R. Giancarlo. Standard versus uniform binary\nsearch and their variants in learned static indexing: The case of the searching\non sorted data benchmarking software platform. Software: Practice and\nExperience , 53(2):318–346, 2023.\n[4]Domenico Amato, Raffaele Giancarlo, and Giosué Lo Bosco. Learned sorted\ntable search and static indexes in small-space data models. Data, 8(3), 2023.\n[5]G. Amato, D. nd Lo Bosco and R. Giancarlo. On the suitability of neural\nnetworks as building blocks for the design of efficient learned indexes. In\nLazaros Iliadis, Chrisina Jayne, Anastasios Tefas, and Elias Pimenidis,\neditors,Engineering Applications of Neural Networks , pages 115–127, Cham,\n2022. Springer International Publishing.\n[6]N. Ao, F. Zhang, D. Wu, D. S. Stones, G. Wang, X. Liu, J. Liu, and S. Lin.\nEfficient parallel lists intersection and index compression algorithms using\ngraphics processing units. Proc. VLDB Endow. , 4(8):470–481, May 2011.\n[7]A. Boffa, P. Ferragina, and G. Vinciguerra. A “learned” approach to\nquicken and compress rank/select dictionaries. In Proceedings of the SIAM\nSymposium on Algorithm Engineering and Experiments (ALENEX) , 2021.\n[8]D.Comer. UbiquitousB-Tree. ACM Computing Surveys (CSUR) ,11(2):121–\n137, 1979.\n14\n\n[9]T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Introduction to\nAlgorithms, Third Edition . The MIT Press, 3rd edition, 2009.\n[10]Z. Dai and A. Shrivastava. Adaptive learned bloom filter (Ada-BF): Efficient\nutilization of the classifier with application to real-time information filtering\non the web. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan,\nand H. Lin, editors, Advances in Neural Information Processing Systems ,\nvolume 33, pages 11700–11710. Curran Associates, Inc., 2020.\n[11]E. D. Demaine, T. Jones, and M. Pătraşcu. Interpolation search for non-\nindependent data. In Proceedings of the Fifteenth Annual ACM-SIAM\nSymposium on Discrete Algorithms , SODA ’04, page 529–530, USA, 2004.\nSociety for Industrial and Applied Mathematics.\n[12]J. Ding, U. F. Minhas, J. Yu, C. Wang, J. Do, Y. Li, H. Zhang, B. Chan-\ndramouli, J. Gehrke, D. Kossmann, D. Lomet, and T. Kraska. Alex: An\nupdatable adaptive learned index. In Proceedings of the 2020 ACM SIG-\nMOD International Conference on Management of Data , SIGMOD ’20, page\n969–984, New York, NY, USA, 2020. Association for Computing Machinery.\n[13]Richard O. Duda, Peter E. Hart, and David G. Stork. Pattern Classification,\n2nd Edition . Wiley, 2000.\n[14]P. Ferragina and G. Vinciguerra. Learned data structures. In Recent Trends\nin Learning From Data , pages 5–41. Springer International Publishing, 2020.\n[15]P. Ferragina and G. Vinciguerra. The PGM-index: a fully-dynamic com-\npressed learned index with provable worst-case bounds. PVLDB, 13(8):1162–\n1175, 2020.\n[16]D. Freedman. Statistical Models : Theory and Practice . Cambridge Univer-\nsity Press, August 2005.\n[17]D. Freedman and Persi Diaconis. On the histogram as a density estimator:l2\ntheory.Zeitschrift für Wahrscheinlichkeitstheorie und Verwandte Gebiete ,\n57(4):453–476, Dec 1981.\n[18]G. Fumagalli, D. Raimondi, R. Giancarlo, D. Malchiodi, and M. Frasca.\nOn the choice of general purpose classifiers in learned bloom filters: An\ninitial analysis within basic filters. In Proceedings of the 11th International\nConference on Pattern Recognition Applications and Methods (ICPRAM) ,\npages 675–682, 2022.\n[19]Gaston H. Gonnet, Lawrence D. Rogers, and J. Alan George. An algorithmic\nand complexity analysis of interpolation search. Acta Inf. , 13(1):39–52, jan\n1980.\n[20]Alexis Kaporis, Christos Makris, Spyros Sioutas, Athanasios Tsakalidis,\nKostas Tsichlas, and Christos Zaroliagis. Dynamic interpolation search\nrevisited. In Michele Bugliesi, Bart Preneel, Vladimiro Sassone, and Ingo\n15\n\nWegener, editors, Automata, Languages and Programming , pages 382–394,\nBerlin, Heidelberg, 2006. Springer Berlin Heidelberg.\n[21]P.V. Khuong and P. Morin. Array layouts for comparison-based searching.\nJ. Exp. Algorithmics , 22:1.3:1–1.3:39, 2017.\n[22]A. Kipf, R. Marcus, A. van Renen, M. Stoian, Kemper A., T. Kraska, and\nT. Neumann. SOSD: A benchmark for learned indexes. In ML for Systems\nat NeurIPS, MLForSystems @ NeurIPS ’19 , 2019.\n[23]A. Kipf, R. Marcus, A. van Renen, M. Stoian, A. Kemper, T. Kraska, and\nT. Neumann. Radixspline: A single-pass learned index. In Proceedings\nof the Third International Workshop on Exploiting Artificial Intelligence\nTechniques for Data Management , aiDM ’20, pages 1–5. Association for\nComputing Machinery, 2020.\n[24]D. E. Knuth. Optimum binary search trees. Acta Informatica , 1(1):14–25,\nMar 1971.\n[25]D. E. Knuth. The Art of Computer Programming, Vol. 3 (Sorting and\nSearching) , volume 3. 1973.\n[26]T. Kraska, A. Beutel, E. H Chi, J. Dean, and N. Polyzotis. The case for\nlearnedindexstructures. In Proceedings of the 2018 International Conference\non Management of Data , pages 489–504. ACM, 2018.\n[27]M. Maltry and J. Dittrich. A critical analysis of recursive model indexes.\nCoRR. To appear in: Proceedings of the VLDB Endowment , abs/2106.16166,\n2021.\n[28]R. Marcus, A. Kipf, A. van Renen, M. Stoian, S. Misra, A. Kemper,\nT. Neumann, and T. Kraska. Benchmarking learned indexes. Proc. VLDB\nEndow., 14(1):1–13, sep 2020.\n[29]R. Marcus, E. Zhang, and T. Kraska. CDFShop: Exploring and optimizing\nlearned index structures. In Proceedings of the 2020 ACM SIGMOD Inter-\nnational Conference on Management of Data , SIGMOD ’20, page 2789–2792,\n2020.\n[30]M. Mitzenmacher. A model for learned bloom filters and optimizing by\nsandwiching. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-\nBianchi, and R. Garnett, editors, Advances in Neural Information Processing\nSystems, volume 31. Curran Associates, Inc., 2018.\n[31]Michael Mitzenmacher and Sergei Vassilvitskii. Algorithms with Predictions ,\npage 646–662. Cambridge University Press, 2021.\n[32]S.V. Nagaraj. Optimal binary search trees. Theoretical Computer Science ,\n188:1–44, 1997.\n16\n\n[33]Yehoshua Perl, Alon Itai, and Haim Avni. Interpolation search—a log logn\nsearch.Commun. ACM , 21(7):550–553, jul 1978.\n[34]W. W. Peterson. Addressing for random-access storage. IBM Journal of\nResearch and Development , 1(2):130–146, 1957.\n[35]J. Rao and K. A Ross. Cache conscious indexing for decision-support in\nmain memory. In Proceedings of the 25th International Conference on Very\nLarge Data Bases , pages 78–89. Morgan Kaufmann Publishers Inc., 1999.\n[36]D. D. Sleator and R. E. Tarjan. Self-adjusting binary search trees. J. ACM,\n32:652–686, 1985.\n[37]P. Spirakis. Tail Bounds for Occupancy Problems , pages 942–944. Springer\nUS, Boston, MA, 2008.\n[38]K. Vaidya, E. Knorr, T. Kraska, and M. Mitzenmacher. Partitioned learned\nbloom filter. ArXiv, abs/2006.03176, 2020.\n[39]Andrew C. Yao and F. Frances Yao. The complexity of searching an ordered\nrandom table. In 17th Annual Symposium on Foundations of Computer\nScience (sfcs 1976) , pages 173–177, 1976.\n[40]F. Frances Yao. Efficient dynamic programming using quadrangle inequali-\nties. InProceedings of the Twelfth Annual ACM Symposium on Theory of\nComputing , STOC ’80, page 429–435, New York, NY, USA, 1980. Associa-\ntion for Computing Machinery.\n17\n\nFrom Specific to Generic Learned Sorted Set\nDictionaries: A Theoretically Sound Paradigm\nYelding Competitive Data Structural Boosters in\nPractice -Supplementary File\nDomenico Amato Giosué Lo Bosco Raffaele Giancarlo\nDipartimento di Matematica e Informatica\nUniversitá degli Studi di Palermo, ITALY\nSeptember 6, 2023\nAbstract\nThis document provides additional details with respect to the content\nof the Main Manuscript by the same title.\n1Sorted Table Search Procedure and Variants:\nDetails\n1.1 Array Layouts for Binary Search\nFollowing research in [ 6], and with the use of the shorthand notation in the Main\nText (Section 2.1), we review here three basic layouts.\n1Sorted. It is the classic textbook layout for standard Binary Search,\nreported in Algorithm 1. We also consider an implementation of Uniform\nBinary Search [ 6,9]) (Algorithm 2), that uses prefetching. This routine is\nreferred to as branch-free in [ 6], since there is no branching in the while\nloop. Indeed, as explained in [ 6], the case statement in line 8 of Algorithm 2\nis translated into a conditional move instruction that does not alter the flow\nof the assembly program corresponding to that C++ code. In turn, that\nhas the advantage of a better use of instruction pipelining with respect to\nthe branchy version of the same code. Prefetching, i.e., instructions 6 and\n7 in Algorithm 2, refers to the fact that the procedure loads in the cache\ndata ahead of their possible use. As shown in [ 6], such an implementation\nof Binary Search is substantially faster than its branchy counterpart.\n1arXiv:2309.00946v1  [cs.DS]  2 Sep 2023\n\n2Eytzinger Layout [ 6]. The sorted table is now seen as stored in a virtual\ncomplete balanced binary search tree. Such a tree is laid out in Breadth-\nFirst Search order in an array. An example is provided in Fig. 1. Also, in\nthis case, we adopt a branch-free version with prefetching of the Binary\nSearch procedure corresponding to this layout. It is reported in Algorithm\n3.\n3B-tree. The sorted table is now seen as a B+ 1search tree [ 2], which is\nthen laid out in analogy with an Eytzinger layout. An example is provided\nin Fig. 2. Also, in this case, we adopt a branch-free version of the Binary\nSearch procedure corresponding to this layout, with prefetching. It is taken\nfrom the software associated with [6] and it is reported Algorithm 4.\nAlgorithm 1 Implementation of Standard Binary Search .\n1:int StandardBinarySearch(int *A, int x, int left, int right){\n2:while (left <right) {\n3:int m = (left + right) / 2\n4:if(x<A[m]){\n5:rigth = m;\n6:}else if( x >A[m]){\n7:left = m+1;\n8:}else{\n9:return m\n10:}\n11:}\n12:return right;\n13:}\nAlgorithm 2 Implementation of Uniform Binary Search with Prefetch-\ning. The code is as in [6] (see also [9]).\n1:int prefetchUniformBinarySearch(int *A, int x, int left, int right){\n2:const int *base = A;\n3:int n = right;\n4:while (n >1) {\n5:const int half = n / 2;\n6:__builtin_prefetch(base + half/2, 0, 0);\n7:__builtin_prefetch(base + half + half/2, 0, 0);\n8:base = (base[half] <x) ? &base[half] : base;\n9:n -= half;\n10:}\n11:return (*base <x) + base - A;\n12:}\n2\n\n5\n79\n1113\n15 316\n1819\n2021\n25\n22 27\n20 16 9 215131925371115 18 22 27Figure 1: An example of Eyzinger layout of a table with 15 elements . See also [6].\nAlgorithm 3 Implementation of Uniform Binary Search with Eytzinger\nLayout and Prefetching . The code is as in [6].\nint UniformEytzingerLayout(int *A, int x, int left, int right){\nint i = 0;\n3:int n = right;\nwhile (i <n){\n__builtin_prefetch(A+(multiplier*i + offset));\n6:i = (x <=A[i]) ? (2*i + 1) : (2*i + 2);\n}\nint j = (i+1) >>__builtin_ffs( ∼(i+1));\n9:return (j == 0) ? n : j-1;\n}\n1.2 Interpolation Search\nAs stated in the Main Text, such a Sorted Table Search technique was introduced\nby Peterson [ 14] and it has received some attention in terms of analysis [ 13], since\nit works very well on data following a Uniform or nearly Uniform distribution\nand very poorly on data following other distributions. The procedure adopted\nhere is reported in Algorithm 5.\n2A Simple Example of How to Build and Query\na Model\nWe now outline the basic technique that one can use to build a model for A. It\nrelies on Linear Regression, with Mean Square Error Minimization [ 5]. With\nreference to the example in Figure 3, and assuming that one wants a linear\n16\n16 920\n513\n1925\n37\n11 1518\n2122\n27\n16\n18 22 137 20212527 16 95 19 3 11 15\nFigure 2: An example of (B+ 1)layout of a table with 15 elements and B= 2(see also [6]).\n3\n\nAlgorithm 4 Implementation of Uniform Binary Search with B-Tree\nLayout and Prefetching .\n1:int UniformBTreeSearch(int* a, int x, int left, int right){\n2:int j = right;\n3:int i = left;\n4:int n = right - left +1;\n5:int B = cacheline/sizeof(T);\n6:while (i + B ≤n) {\n7:__builtin_prefetch(a+child(i, B/2, B), 0, 0);\n8:const int *base = &a[i];\n9:const int *pred = uniform_inner_search(base, x, B);\n10:int nth = (*pred <x) + pred - base;\n11:{\n12:const int current = base[nth % B];\n13:int next = i + nth;\n14:j = (current ≥x) ? next : j;\n15:}\n16:i = child(nth, i, B);\n17:}\n18:if (__builtin_expect(i <n, 0)) {\n19:const int *base = &a[i];\n20:int m = n - i;\n21:while (m >1) {\n22:int half = m / 2;\n23:const int *current = &base[half];\n24:base = (*current <x) ? current : base;\n25:m -= half;\n26:}\n27:int ret = (*base <x) + base - a;\n28:return (ret == n) ? j : ret;\n29:}\n30:return j;\n31:}\n32:\n33:int* uniform_inner_search(int *base, int x, int C) {\n34:if (C≤1) return base;\n35:const int half = C / 2;\n36:const T *current = &base[half];\n37:return uniform_inner_search((*current <x) ? current : base, x, C-half);\n38:}\n4\n\nAlgorithm 5 Implementation of Standard Interpolation Search .\n1:int StandardInterpolationSearch(int *arr, int x, int start, int end){\n2:int lo = start, hi = (end - 1);\n3:while (lo ≤hi && x ≥arr[lo] && x ≤arr[hi]) {\n4:if (lo == hi) {\n5:if (arr[lo] == x) return lo;\n6:return -1;\n7:}\n8:int pos = lo + (((double)(hi - lo) / (arr[hi] - arr[lo])) * (x - arr[lo]));\n9:if (arr[pos] == x) return pos;\n10:if (arr[pos] <x) lo = pos + 1;\n11: else hi = pos - 1;\n12:}\n13:return pos;\n14:}\n(a)\n (b)\n (c)\ne\nFigure 3: The Process of Learning a Simple Model via Linear Regression. LetAbe\n[47,105,140,289,316,358,386,398,819,939]. (a) The CDF of A. In the diagram, the abscissa\nindicates the value of an element in the table, while the ordinate is its rank. (b) The straight line\nF(x) =ax+bis obtained by determining aandbvia Linear Regression, with Mean Square Error\nMinimization. (c) The maximum error ϵone can incur in using Fis also important. In this case, it\nisϵ= 3, i.e., accounting for rounding, it is the maximum distance between the rank of a point in\nthe table and its rank as predicted by F. In this case, the interval to search into, for a given query\nelement x, is given by [F(x)−ϵ, F(x) +ϵ].\nmodel, i.e., F(x) =ax+b, Kraska et al. [ 10] note that they can fit a straight\nline to the Empirical Cumulative Distribution Function (CDF) and then use it\nto predict where a point xmay fall in terms of rank and accounting also for\napproximation errors. More in general, in order to perform a query, the model is\nconsulted and an interval in which to search for is returned. Then, to fix ideas,\nBinary Search on that interval is performed.\n3Models Specific for Binary and Interpolation\nSearch\nWith reference to Figure 4, we provide a brief description of three models specific\nfor Binary and Interpolation Search. That is, as of their current implementation,\nthey work when Ais stored in a sorted table.\n5\n\n{5\n11\n14\n58\n59\n60\n97\n100\n101Linear\nLinear\n151114 59 60 97 100 100 578 590 630 58{{{\n.......Linear Quadratic Cubic1 2 b\n{{\nkey:1 Model:fkey:58 Model:f key:97 Model:fkey:58 Model:f key:1 Model:f\n1 2 345\n1 58  97\n11 1\n151114 59 60 97 100 101 58{{{{(a)\n(b)(c)Figure 4: Examples of various Learned Indexes (see also [ 12]). (a) an Atomic Model, where the\nbox linear means that the CDF of the entire dataset is estimated by a linear function via Regression,\nas exemplified in Figure 3. (b) An example of an RMIwith two layers and a branching factor equal\ntob. The top box indicates that the lower models are selected via a linear function. As for the leaf\nboxes, each indicates which Atomic Model is used for prediction on the relevant portion of the table.\n(c) An example of a PGMIndex. At the bottom, the table is divided into three parts. A new table\nis so constructed and the process is iterated.\n•Atomic Models . They are closed-form expressions that can be evaluated\nwhen queried, in order to get the predicted position in the table. An\nexample is provided in Figure 4(a). Usually, the closed-form expression is\na low degree polynomial approximating the CDF. The interested reader\ncan find details in [ 1] on how such a polynomial can be derived from the\ntable via Regression.\n•The RMI Family . Informally, an RMIis a multi-level, directed graph,\nwith Atomic Models at its nodes. When searching for a given key and\nstarting with the first level, a prediction at each level identifies the model\nof the next level to use for the next prediction. This process continues\nuntil a final level model is reached. This latter is used to predict the table\ninterval to search into. An example of RMIis provided in Figure 4(b). As\npointed out in [ 7,12], in most applications, a generic RMIwith two layers,\na tree-like structure and a branching factor bsuffices to handle large tables.\nAmato et al.[1] have shown that they perform well on a wide spectrum of\ntable sizes. It is to be noted that Atomic Models are RMIs, although it\nis more convenient to single them out as “basic building boxes\" of more\ncomplex models.\n•PGM. It is also a multi-stage model, built bottom-up and queried top\ndown. It uses a user-defined approximation parameter ϵ, that controls the\nprediction error at each stage. With reference to Figure 4(c), the table is\nsubdivided into three pieces. A prediction in each piece can be provided\nvia a linear model guaranteeing an error of ϵ. A new table is formed by\nselecting the minimum values in each of the three pieces. This new table\nis possibly again partitioned into pieces, in which a linear model can make\n6\n\na prediction within the given error. The process is iterated until only one\nlinear model suffices, as in the case in the Figure. A query is processed via\na series of predictions, starting at the root of the tree.\n4 Proof Outlines\nThe notation and the numbering of the Theorems are the same as in the main\ntext.\n4.1 Theorem 3.1\nThe proof regarding the construction easily follows from the convexity of g(c)\nand the fact that it is non-decreasing. The proof regarding the search time is a\nsimple extension of the proof of Theorem 2.1. in [4].\n4.2 Theorem 3.2\nLetµmbe the expected number of elements in bin m. We need to prove that\nunder the assumptions of the Theorem, the probability that µm> tlogO(1)n,t\nconstant, is bounded by n−2. Then the result follows by the Union Bound.\nTo this end, we first show the following. Assume that the elements in Aare\ndrawn from Uaccording to some probability distribution and independently.\nThen, the following events hold with probability at most n−2.(A) The Lightly\nLoaded Case . When 0< µ m<2elnn, the number of elements in bin m\nexceeds 2e2lnn.(B) The Heavily Loaded Case . When µm≥2elnn., the\nnumber of elements in that bin exceeds eµm.\nIn order to prove the above statements, we reduce the assignment of the\nelements to the kbins to a Balls and Bins game, in which we are interested\nin the Maximum Expected Load of any bin. Then, the results follow by using\nstandard Chernoff Bounds arguments [16].\nNow, using the assumption that the probability distribution from which the\nelements in Aare drawn from Uis smooth, we show that µm≤βlnO(1)n. Now,\nlett= max( β,2e2). Observe that, for any bin m.\nPR[num. of elements in bin m > t lnO(1)n]≤n−2.\n.\nIndeed, by (A), the result follows immediately for lightly loaded bins, while\nfor heavily loaded bins, we need to use the bound on ummentioned earlier and\nthen (B). Now the result follows by the Union Bound, applied to all bins and\nfrom the fact that we have f1(n) =cnof them, for some constant 0< c≤1.\n4.3 Therem 3.3\nWe need to recall some facts from the Literature. Given the probability dis-\ntibution PandQ, it is well known that one can build an Optimum Binary\n7\n\nSearch Tree [ 8,17], i.e., one minimizing the expected search time C(T), inO(n2)\ntime [8,17]. Moreover, C(Topt)≤H(P, Q) + 1, as shown in [ 3], where it is also\nprovided a linear time approximation algorithm that produces a search tree with\ncost bounded as in the optimum case in terms of entropy.\nBased on the mentioned results, we now prove the Theorem.\nPart (a). Consider Dkfor a fixed k,1≤k≤kmax. For each bin in Dk,\nbuild an Optimum Binary Search Tree for the elements of Afalling in that bin,\nwith the access probabilities relevant for that bin. Let Toptbe the Optimum\nBinary Search Tree corresponding to A, with access probabilities PandQ. Now,\nC(Topt) + 1is the cost of the Optimum Binary Search Forest corresponding to\nD1and it is an upper bound to the cost of the Optimum Binary Search Forest\ncomputed on all k’s. Using the mentioned entropy bound on optimum cost, one\nobtains the claimed bound. As for time, the Optimum Search Forest for Dk\ncan be computed in O(Σn\ni=1ℓ2\ni)time, which is bounded by O(n2). The claimed\nbound now follows.\nPart (b). It follows along the same lines as (a), except that now we use the\napproximation algorithm in [ 3] as a “subroutine\". In this case, for a single k, the\ncost is bounded by O(Σn\ni=1ℓi) =O(n).\n4.4 Theorem 3.4\nWe prove the following, as in [4]:\n(1)None of the n/2update operations immediately after a rebuild can add a\nvalue outside the range [A[1]−Lˆ∆, A[n] +Lˆ∆].\n(2)The maximum number of elements in a bin after nupdate operations is\nO(n/k)ˆ∆2).\nNow, point (1) grants correctness. The worst-case bound comes from point\n(2), the fact that each bin cannot contain more than O(n)elements between\ntwo rebuild operations and the assumptions regarding the cost of each of the\nmentioned operations for SD.\nAs for rebuild operations, we notice that a rebuild can be done in O(n)time\nwhen there are nelements present in the structure.\nWhen a rebuild occurs because n/2updates have been performed, we charge\nthe cost of rebuild to each element involved in those updates, O(1)time each.\nDuring the lifetime of the structure, each element in it is charged at most twice.\nWhen a rebuild occurs because of a change in ∆, we observe the following.\nLet∆0,···,∆maxbe the changes in ∆that cause a rebuilt. We notice: (a)\nCi+1=log∆i+1−log∆i= Ω(1), since ∆i+1≥2∆i0≤i < max; (b) the sum of\ntheCi’s telescopes and it is bounded by log∆max. Now, when a rebuild occurs\nbecause of a change in ∆, we charge Ci+1to each element in the structure at\nthe time of the change, assuming that this is the i-th change. This is enough to\npay for the rebuild, by (a). Because of (b), during the lifetime of the structure,\nan element can be charged at most log ∆ maxunits.\nThe bound on rebuild operations follows.\n8\n\n5 The Role of ∆\nIn order to assess the role of ∆on real datasets, we proceed as follows. From\nthe real datasets used in this research, we produce smaller ones that have the\nsame statistical properties as the original set, i.e., CDF and PDF. Then, we\ncompute the values of ∆for all the datasets we have either available or that we\nhave generated.\nWith reference to the datasets mentioned in Section 4 of the Main Text, we\nproduce sorted sets of varying sizes, so that each fits in a level of the internal\nmemory hierarchy, as follows. Letting nbe the number of elements in a set, for\nthe computer architecture we are using (Section 4 of the Main text), the details\nof the tables we generate are as follows.\n•Fitting in L1 cache: cache size 64Kb. Therefore, we choose n= 3.7K.\nFor each dataset, the table corresponding to this type is denoted with the\nprefix L1, e.g., L1_amzn , when needed. For each dataset, in order to\nobtain a CDF that resembles one of the original tables, we proceed as\nfollows. Concentrating on amzn, since for the other datasets the procedure\nis analogous, we extract uniformly and at random a sample of the data of\nthe required size. For each sample, we compute its CDF. Then, we use the\nKolmogorov-Smirnov test [ 15] in order to assess whether the CDF of the\nsample is different from the amzn32 CDF.\nIf the test returns that we cannot exclude such a possibility, we compute\nthe PDF of the sample and compute its KL divergence [ 11] from the PDF\nofamzn. We repeat such a process 100 times and, for our experiments,\nwe use the sample dataset with the smallest KL divergence.\n•Fitting in L2 cache: cache size 256Kb. Therefore, we choose\nn= 31 .5K. For each dataset, the table corresponding to this type is\ndenoted with the prefix L2, when needed. For each dataset, the generation\nprocedure is the same as the one of the L1dataset.\n•Fitting in L3 cache: cache size 8Mb. Therefore, we choose n= 750 K.\nFor each dataset, the table corresponding to this type is denoted with the\nprefix L3, when needed. For each dataset, the generation procedure is the\nsame as the one of the L1dataset.\nFor completeness, the results of the Kolmogorov-Smirnov Test as well as KL\ndivergence computation are reported in Table 1. For each memory level (first\nrow) and each original dataset (first column), it is reported the percentage of\ntimes in which the Kolmogorov-Smirnov test failed to report a difference between\nthe CDFs of the dataset extracted uniformly and at random from the original\none and this latter, over 100 extractions. Moreover, the KL divergence between\nthe PDFs of the chosen generated dataset and the original one is also reported.\nFrom those results, it is evident that the PDF of the original datasets is quite\nclose to the PDF of the extracted datasets.\nFigure 5 report the full set of experiments regarding the role of ∆on real\ndatasets, briefly discussed in Section 3.3 of the Main text.\n9\n\nTable 1: The results of the Kolmogorov-Smirnov Test and of the KL divergence computation.\nL1 L2 L3\nDatasets %succ KLdiv %succ KLdiv %succ KLdiv\namzn 1009.54e-06 ±7.27e-14 1007.88e-05 ±7.97e-13 1001.88e-03 ±1.52e-11\nface 1001.98e-05 ±1.00e-12 1007.98e-05 ±4.43e-13 1001.88e-03 ±1.24e-11\nosm 1009.38e-06 ±4.51e-14 1007.88e-05 ±3.46e-13 1001.88e-03 ±9.55e-12\nwiki 1009.47e-06 ±5.27e-14 1007.87e-05 ±5.64e-13 1001.88e-03 ±1.25e-11\nL1\nuniform amzn face osm wiki05101520\nL2\nuniform amzn face osm wiki01020304050\nL3\nuniform amzn face osm wiki01020304050L4\nuniform amzn face osm wiki01020304050log(N)\nlog2(N)\nN\nN2\nFigure 5: The Role of ∆. To the collection of datasets discussed in the text of this Supplementary\nFile, we have also added datasets generated from the Uniform Distribution, as a baseline. In each\nFigure, the horizontal lines denote various values of functions of the input size, which is the number\nof elements in each set. The abscissa contains the names of the datasets and the ordinate is the\nvalue of ∆for that dataset. Finally, the label on top of each Figure indicates the size of the dataset,\nas discussed in the text of this Supplementary File.\n10\n\n6Experiments: ResultsandDiscussion-Additional\nFigures\nExperiments have been carried out on Intel Core i7-8700 3.2GHz CPU with three\nlevels of cache memory: (a) 64kb of L1 cache; (b) 256kb of L2 cache; (c)12Mb\nof shared L3 cache. The total amount of system memory is 32 Gbyte of DDR4.\nThe operating system is Ubuntu LTS 20.04.\n0.0 0.2 0.4 0.6 0.8 1.0\n1e90.00.20.40.60.81.0amzn32\n0 2 4 6 8\n1e180.00.20.40.60.81.0amzn64\n0 2 4 6 8\n1e100.00.20.40.60.81.0face\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\n1e190.00.20.40.60.81.0osm\n1.00 1.05 1.10 1.15 1.20\n1e90.00.20.40.60.81.0wiki\nFigure 6: CDF plots . The Empirical Cumulative Distribution Fuction of each dataset considered\nin this research.\n11\n\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\nBin %0.00.51.01.52.02.53.03.54.0Mean Query Time (s)1e 7\n osm\nBBS\nT otal Time (s)\nBinning Time (s)\nSearch Time (s)Figure 7: Binning Query Time on the osm dataset . We report on the xaxis the chosen\npercentage for the Binning construction (see Main Text), while on the yaxis the mean query time is\nexpressed in seconds. The blue bars indicate the mean total time needed to execute a query using\nthe Binning Model. The orange bar shows the mean time needed to make a prediction with the\nBinnig Model. The green bar reports the time to search in the predicted interval using BBS. Finally,\nthe horizontal line is the stand-alone BBSmean query time.\n12\n\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\nBin %0.00.51.01.52.02.53.0Mean Query Time (s)1e5\n osm\nIBS\nT otal Time (s)\nBinning Time (s)\nSearch Time (s)Figure 8: Binning Query Time on the osm dataset . We report on the xaxis the chosen\npercentage for the Binning construction (see Main Text), while on the yaxis the mean query time is\nexpressed in seconds. The blue bars indicate the mean total time needed to execute a query using\nthe Binning Model. The orange bar shows the mean time needed to make a prediction with the\nBinnig Model. The green bar reports the time to search in the predicted interval using IBS. Finally,\nthe horizontal line is the stand-alone IBSmean query time.\n13\n\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\nBin %0123456Mean Query Time (s)1e6\n osm\nSPLAY\nT otal Time (s)\nBinning Time (s)\nSearch Time (s)Figure 9: Binning Query Time on the osm dataset . We report on the xaxis the chosen\npercentage for the Binning construction (see Main Text), while on the yaxis the mean query time is\nexpressed in seconds. The blue bars indicate the mean total time needed to execute a query using\nthe Binning Model. The orange bar shows the mean time needed to make a prediction with the\nBinnig Model. The green bar reports the time to search in the predicted interval using SPLAY.\nFinally, the horizontal line is the stand-alone SPLAY mean query time.\n14\n\n226225224223222221220219218217216215214213212211210292827262524232221\nEpsilon0.00.20.40.60.81.01.2Mean Query Time (s)1e 6\n osm\nT otal Time (s)\nPGM Time (s)\nSearch Time (s)Figure 10: PGM Query Time on the osm dataset . We report on the xaxis the chosen ϵfor the\nPGM construction, while on the yaxis the mean query time is expressed in seconds. The blue bars\nindicate the mean total time to execute a query using the PGM Model. The orange bar shows the\nmean time taken to navigate the PGM Model, in order to get a prediction. The green bar reports\nthe mean time to search in the predicted interval using BBS. Finally, the blue horizontal line is the\nstand-alone BBSmean query time.\n15\n\n226225224223222221220219218217216215214213212211210292827262524232221\nEpsilon0.00.51.01.52.02.53.0Mean Query Time (s)1e5\n osm\nIBS\nT otal Time (s)\nPGM Time (s)\nSearch Time (s)Figure 11: PGM Query Time on the osm dataset . We report on the xaxis the chosen ϵfor the\nPGM construction, while on the yaxis the mean query time is expressed in seconds. The blue bars\nindicate the mean total time to execute a query using the PGM Model. The orange bar shows the\nmean time taken to navigate the PGM Model, in order to get a prediction. The green bar reports\nthe mean time to search in the predicted interval using IBS. Finally, the blue horizontal line is the\nstand-alone IBSmean query time.\n16\n\n226225224223222221220219218217216215214213212211210292827262524232221\nEpsilon0123456Mean Query Time (s)1e6\n osm\nSPLAY\nT otal Time (s)\nPGM Time (s)\nSearch Time (s)Figure 12: PGM Query Time on the osm dataset . We report on the xaxis the chosen ϵfor the\nPGM construction, while on the yaxis the mean query time is expressed in seconds. The blue bars\nindicate the mean total time to execute a query using the PGM Model. The orange bar shows the\nmean time taken to navigate the PGM Model, in order to get a prediction. The green bar reports\nthe mean time to search in the predicted interval using SPLAY. Finally, the blue horizontal line is\nthe stand-alone SPLAY mean query time.\n17\n\nReferences\n[1]Domenico Amato, Raffaele Giancarlo, and Giosué Lo Bosco. Learned sorted\ntable search and static indexes in small-space data models. Data, 8(3), 2023.\n[2]D.Comer. UbiquitousB-Tree. ACM Computing Surveys (CSUR) ,11(2):121–\n137, 1979.\n[3]R. De Prisco and A. De Santis. On binary search trees. Information\nProcessing Letters , 45(5):249–253, 1993.\n[4]E. D. Demaine, T. Jones, and M. Pătraşcu. Interpolation search for non-\nindependent data. In Proceedings of the Fifteenth Annual ACM-SIAM\nSymposium on Discrete Algorithms , SODA ’04, page 529–530, USA, 2004.\nSociety for Industrial and Applied Mathematics.\n[5]D. Freedman. Statistical Models : Theory and Practice . Cambridge Univer-\nsity Press, August 2005.\n[6]P.V. Khuong and P. Morin. Array layouts for comparison-based searching.\nJ. Exp. Algorithmics , 22:1.3:1–1.3:39, 2017.\n[7]A. Kipf, R. Marcus, A. van Renen, M. Stoian, Kemper A., T. Kraska, and\nT. Neumann. SOSD: A benchmark for learned indexes. In ML for Systems\nat NeurIPS, MLForSystems @ NeurIPS ’19 , 2019.\n[8]D. E. Knuth. Optimum binary search trees. Acta Informatica , 1(1):14–25,\nMar 1971.\n[9]D. E. Knuth. The Art of Computer Programming, Vol. 3 (Sorting and\nSearching) , volume 3. 1973.\n[10]T. Kraska, A. Beutel, E. H Chi, J. Dean, and N. Polyzotis. The case for\nlearnedindexstructures. In Proceedings of the 2018 International Conference\non Management of Data , pages 489–504. ACM, 2018.\n[11] S Kullback. Information Theory and Statistics . Citeseer, 1968.\n[12]R. Marcus, A. Kipf, A. van Renen, M. Stoian, S. Misra, A. Kemper,\nT. Neumann, and T. Kraska. Benchmarking learned indexes. Proc. VLDB\nEndow., 14(1):1–13, sep 2020.\n[13]K. Mehlhorn and A. Tsakalidis. Data Structures. In Handbook of Theoretical\nComputer Science (Vol. A): Algorithms and Complexity , page 302–341. MIT\nPress, Cambridge, MA, USA, 1991.\n[14]W. W. Peterson. Addressing for random-access storage. IBM Journal of\nResearch and Development , 1(2):130–146, 1957.\n[15]N. V Smirnov. Estimate of deviation between empirical distribution func-\ntions in two independent samples. Bulletin Moscow University , 2(2):3–16,\n1939.\n18\n\n[16]P. Spirakis. Tail Bounds for Occupancy Problems , pages 942–944. Springer\nUS, Boston, MA, 2008.\n[17]F. Frances Yao. Efficient dynamic programming using quadrangle inequali-\nties. In Proceedings of the Twelfth Annual ACM Symposium on Theory of\nComputing , STOC ’80, page 429–435, New York, NY, USA, 1980. Associa-\ntion for Computing Machinery.\n19",
  "textLength": 72076
}