{
  "paperId": "50b5d2cfb48862f930b821e85f1d89be55a2238b",
  "title": "Learned Indexes for Dynamic Workloads",
  "pdfPath": "50b5d2cfb48862f930b821e85f1d89be55a2238b.pdf",
  "text": "Learned Indexes for Dynamic Workloads\nChuzhe Tang, Zhiyuan Dong, Minjie Wang†, Zhaoguo Wang, Haibo Chen\nShanghai Jiao Tong University, †New York University\nAbstract\nThe recent proposal of learned index structures opens up\na new perspective on how traditional range indexes can\nbe optimized. However, the current learned indexes as-\nsume the data distribution is relatively static and the ac-\ncess pattern is uniform, while real-world scenarios con-\nsist of skew query distribution and evolving data. In\nthis paper, we demonstrate that the missing considera-\ntion of access patterns and dynamic data distribution no-\ntably hinders the applicability of learned indexes. To\nthis end, we propose solutions for learned indexes for\ndynamic workloads (called Doraemon). To improve the\nlatency for skew queries, Doraemon augments the train-\ning data with access frequencies. To address the slow\nmodel re-training when data distribution shifts, Dorae-\nmon caches the previously-trained models and incremen-\ntally ﬁne-tunes them for similar access patterns and data\ndistribution. Our preliminary result shows that, Dorae-\nmon improves the query latency by 45.1% and reduces\nthe model re-training time to 1/20.\n1 Introduction\nThe pioneer study [27] on learned index structures\narouses a lot of excitements around how machine learn-\ning can resculpt system components that have been\ndecades-old, such as bloom ﬁlters [39], join queries [28]\nor even enable self-tuning databases [26].\nThe core insight of learned indexes is to view index\nas a distribution function from the keys to the index\npositions that can be approximated by deep neural net-\nworks. Nevertheless, their preliminary study assumes\na relatively static distribution function, while in many\nreal world scenarios, the data is constantly evolving [12].\nTypical approaches simply rely on re-training the whole\nmodel once the data distribution shifts notably from the\ntraining set used by the current model. However, such re-\ntraining is costly, because not only the model parameters\nneed to be ﬁne-tuned , but also that the model architecture\nneeds to be searched again for better accuracy. Depend-ing on the size of the hyperparamter search space, a basic\narchitecture search technique such as grid search can eas-\nily take up to 10-100x the model training time [4, 31, 6].\nBesides the inefﬁciency in handling dynamic work-\nloads, the learned index paper also assumes a uniform\naccess pattern (or query distribution). However, queries\nin real worlds tend to be skew, where some keys are much\nmore frequently queried than the others [57, 14, 17, 33].\nAs a result, mispredicting a hot key is way more expen-\nsive, and we show that the originally proposed learned in-\ndex model performs poorly under such scenarios. These\ntwo issues hinder the wider adoption of the learned in-\ndexes for real-world workloads.\nIn this paper, we propose Doraemon, a new learned\nindex system for dynamic workloads where the data dis-\ntribution and access pattern may be skew and evolving.\nTo handle skewed access pattern, we ﬁrst investigate and\ndiscuss why the original model fails to address this issue\nand then propose an approach that augments the training\ndata with access frequencies. For the issue of model re-\ntraining, our insight is that the same model architecture\ncan be reused for similar data distribution and access pat-\ntern. Based on this, Doraemon caches the trained models\nand simply ﬁne-tunes them when a similar input distribu-\ntion is encountered again. The preliminary result shows\nthat, by augmenting dataset with the access frequency,\nthe best model architecture has 45.1% performance im-\nprovement; by caching and reusing previous training re-\nsult, the rebuilding time is reduced to 1/20 (from 40 mins\nto 2 mins).\n2 Learned Indexes\nIn this section, we introduce the basic background of\nthe original learned index structures [27]. The insight\nis that indexes can be viewed as functions from the data\n(key) to the values representing either record positions\nin a sorted array (for range index), in an unsorted array\n(for Hash-Index) or whether the data exists or not (for\nBitMap-Index). For the case of range index, the function\n1arXiv:1902.00655v1  [cs.LG]  2 Feb 2019\n\nDatasetWorkload\nSkewed 1 Skewed 2 Skewed 3 Uniform\nArch Time(ns) Arch Time(ns) Arch Time(ns) Arch Time(ns)\nD1 NN16 321 LIN 252 NN16 282 NN16 375\nD2 NN8 319 NN8 316 NN8 301 LIN 344\nD3 LIN 293 LIN 281 LIN 278 LIN 350\nD4 NN8 314 LIN 289 LIN 288 NN8 376\nTable 1: The best model architecture and the corresponding average search time (in ns) with different datasets and workloads.\n0 10901D1\nKey SpaceCDF\n(a)0 109D2\nKey Space\n(b)0 109D3\nKey Space\n(c)0 109D4\nKey Space\n(d)\nFigure 1: Above ﬁgures show the CDF of dataset D1, D2, D3 and D4. The y-axis is the normalizd CDF for each dataset. The\nx-axis indicates the key space where key generated from.\nis effectively a cumulative distribution function (CDF).\nGiven the CDF F, the positions can be predicted by:\np=F(Key)\u0003N\nwhere pis the position of the key and Nis the total num-\nber of keys (see Figure 1 for examples).\nThe core idea is to approximate the CDF function Fby\nmachine learning models such as deep neural networks.\nWhile the choice of the model architectures can vary, the\npaper proposes a staged model architecture inspired by\nthe multi-stage structure of B-Tree. The sub-model at\neach stage predicts which sub-models to be activated in\nthe next stage while the leaf stage directly predicts the\nCDF values. The model is trained from the root stage to\nthe leaf stage, and each stage is trained separately using\nthe following loss function:\nLl=å\n(x;y)(f(bMlfl\u00001(x)=Nc)\nl(x)\u0000y)2;L0=å\n(x;y)(f0(x)\u0000y)2\nHere, (x;y)is the key/position pair from the data to be\nindexed; Llis the loss function of stage l;f(k)\nlis the kth\nsub-model of stage l.fl\u00001recursively executes the above\nequation until the root stage L0.\nTo deploy the learned index, the approximation error\nneeds to be corrected. First, the prediction error can be\nbounded by looking at the maximum distance sbetween\nthe predicted and the true positions for each key. Hence,\nifposis the predicted position by the learned index, the\ntrue position is guaranteed to be within [pos\u0000s;pos+\ns], and a binary search can be used. The error bound sis\nthus a critical indicator of the effectiveness of the learned\nindex. The smaller sis, the more effective is the index.There are several limitations of the original learned in-\ndex. First, the CDF should be relatively static. Other-\nwise, the model needs to be re-trained for better approx-\nimations. Since insertion and deletion are very common,\nlearned indexes can be quite slow due to the high cost of\nre-training. Second, the model assumes all the keys are\nbeing uniformly queried, while in reality, the prediction\nerror of a hotter key has much more impact on the overall\nperformance.\nWe explain how Doraemon addresses these issues in\nthe following sections. Section 3 investigates quantita-\ntively how learned indexes perform under different ac-\ncess patterns (Sec 3.1) and data distribution (Sec 3.2).\nWe then propose our solutions in section (Sec 4) us-\ning data augmentation (Sec 4.1) and model caching\n(Sec 4.2). We also discuss other components in our sys-\ntem (Sec 4.3) and related works (Sec 5).\n3 Challenges with Dynamic Workloads\nIn this section, we will discuss the challenges posed by\ndynamic workloads with a simple example of 2 stages\nlearned index. We found that the choice of model ar-\nchitecture is affected by both query distribution and data\ndistribution.\nTable 1 compares three different model architectures\nwith different datasets and workloads. Each dataset has\n200M integer keys, but with different distributions as\nshown in Figure 1. The uniform workload evenly reads\nevery key. The skewed workloads have 95% queries\nreading 5% hot keys, but in different ranges. All three\narchitectures have 200k linear models at the second stage\nand only their ﬁrst stages are different.\n2\n\n 0 10 20\n0 109Model error bound in log\nKey spaceLIN NN16Figure 2: Errors of 2 conﬁgurations. (left) whole key space,\n(right) hot range of skewed 2. Y-axis is the error bound of the\nmodel having the key (X-axis)\n 0 100 200 300 400\nLIN NN4 NN8 NN16 NN2-4 NN2-8Average time (ns)Binary search Search ModelFigure 3: The additional computations of complex models cancel\noff beneﬁts in binary search time. LIN, NN4/8/16 are deﬁned in\nSec 3. NN2-4/8 means two hidden layes NN of width 4 and 8.\n\u000fLIN: The ﬁrst stage is a linear regression model.\n\u000fNN8: The ﬁrst stage is a one hidden layer 8-width\nNeural Network (NN)\n\u000fNN16: The ﬁrst stage is a one hidden layer 16-width\nNeural Network (NN)\nThere is an interesting observation based on the re-\nsults. By shifting either the workload or the dataset,\nthe best architecture is undecidable . For example, for\nthe ﬁrst row in Table 1, LIN is the best with workload\nSkewed 2, but even worse than B-Tree with workload\nSkewed 1 (1120 vs. 396 ns). Next, we will discuss the\nreasons behind such a phenomenon.\n3.1 The Query Distribution\nQuerying a key with learned index has two steps: ﬁrst,\nit predicts the position by model computation; Second,\nit tries to ﬁnd the actual position using binary search in\na bounded range. However, its latency usually depends\non the binary search, as it takes much longer time than\nmodel computation, (6/7–25/26) in our evaluation. Fur-\nther, the search area is decided by the error bound1of\nthe last stage model who has the key. Thus, we have the\nfollowing observation.\nA skew workload’s performance is dominated by\nthe hot models’ error bound. Hot model is deﬁned\nas the last stage model who holds a hot (frequently ac-\ncessed) key. Given a workload, all models’ error bounds\ncan vary across different model architectures, including\nthe hot models’. As a result, the best architecture varies\nfor the workloads with different query distributions. Fig-\nure 2 shows the error bound (y-axis) of the model where\nthe key (x-axis) is located. Two lines represent two archi-\ntectures, LIN and NN16, trained with dataset of D1. For\nthe average error bound, NN16’s is smaller than LIN’s\n(5.32 vs. 6.58). Thus, with uniform workload, NN16\nhas better performance than LIN (375 ns vs. 406 ns).\nHowever, for the key range from 3.5 \u0002108to 4.6\u0002108,\n1the difference between minimum and maximum prediction errorLIN’s average error bound is smaller than NN16’s (4.56\nvs. 4.86). As a result, LIN has better performance than\nNN16 (252 ns vs. 310 ns) with workload Skewed 2.\n3.2 The Data Distribution\nAn advantage of using complex models (e.g., neural net-\nworks) at the ﬁrst stage is that it can approximate the\ncomplex distribution which cannot be ﬁtted with linear\nmodel. As a result, for those distributions, the complex\nnetwork is able to dispatch the data more evenly than\nsimple models, which is good for the uniform workload.\nFor example, NN16 is better than both NN8 and LIN\n(375 ns vs. 390 ns vs. 406 ns) for D1 with the uniform\nworkload, as it can appoximate the D1 (Figure 1.a) more\nprecisely.\nComplex model is good for the complex distribu-\ntion, but not always. This is because of the computa-\ntion cost of complex models. Figure 3 shows that with\nthe ﬁrst stage model getting more complex, even though\nthe binary search time decreases, but the model compu-\ntation time increases. Because of this tradeoff, for D3\nthat exhibits relatively complex distribution (Figure 1.c),\nLIN has better performance than NN16 (367 vs. 350 ns)\n— NN16 has better performance than LIN at the binary\nsearch (317 vs. 336 ns), but it is also penalized by the\nhigher computation cost (50 vs. 14 ns).\n4 Proposed Solution\nTo achieve learned indexes’ best performance, we pro-\npose a new learned index system for dynamic workloads\ncalled Doraemon (Figure 4). Doraemon incorporates\nread access pattern using the Training Set Generator\nand the Finalizer and reuses pre-trained models using\ntheCounselor .\n4.1 Incorporate Read Access Pattern\nTo incorporate read access pattern, an intuitive solution is\nto increase the contribution of frequently accessed keys\nduring the training process. This can be achieved by cre-\nating multiple copies of those keys in the training set. For\n3\n\nCounselorDatasetWorkloaddist.\nready-to-use modelTraining Set GeneratorAnalyzertraining set\nFinalizerModel CacheAuto-tuner(Background)similarity < ths.Fine-tunerpre-trainedmodel ofmatching dist.12\ntuned model32.12.22.3\ncritical pathnon-critical pathtuned modelFigure 4: Architecture\n0 10901Original Dataset\nKey SpaceCDF\n(a)0 109Stretched Dataset\nKey Space\n(b)\nFigure 5: The left ﬁgure shows CDF of original D1, while the\nright ﬁgure shows CDF of D1 after stretched.\nexample, considering a training set of f(a, 0), (b, 1), (c,\n2)g, where the ﬁrst element is the key and the second is\nits position. If the accessed ratio is 1:2:1, then we double\nbin the training set, which becomes f(a, 0), (b, 1), (b,\n1), (c, 2)g. In this way, the model will be trained with (b,\n1) two times more than others, the prediction accuracy\nofbcan be improved. We evaluate this intuitive solution\nwith the workload of Skewed 3 and the dataset D1. With\nthe new training set, the best architecture we can ﬁnd is\nNN16 with 275 ns average search time, which is close\nto the previous best architecture, 282 ns. This is because\nthe intuitive solution does not improve the error bounds\nof the second stage models which decide the search time.\nIn the above evaluation, the average error bound does not\nimprove much (5.21 vs. 5.31).\n“Stretch” the dataset. Instead of improving the pre-\ndiction accuracy of the hot keys, we should focus on the\nerror bounds of the models containing the hot keys (hot\nmodels). Since the models assigned with few keys tend\nto have small error bounds, we try to reduce the numberof keys handled by the hot models by “stretching” the\ndataset. If a key is frequently accessed, we would like to\nincrease the distance between it with its neighbors, the\nkey before or after it. It can be achieved by simply shift-\ning the position labels. Speciﬁcally, given a key with\nposition pbefore “stretching”, if its access frequency is\nf, and the dataset size is Nthen we need to shift its po-\nsition to be p+(n\u00001)=2, and shift all keys after it with\nn\u00001. For the above example, the training set of f(a, 0),\n(b, 1), (c, 2)gwith access frequency 1:2:1 will be aug-\nmented to bef(a, 0), (b, 1.5), (c, 3) g. Figure 5 shows the\nCDF of dataset 1 before and after “stretching” with the\naccess pattern in workload Skewed 3.\nTraining Set Generator takes the workload and dataset\nas input, extracts the access pattern by uniformly sam-\npling from the workload and stretches the dataset ac-\ncording to the access pattern. Then it sends the stretched\ntraining set to Counselor to get a tuned model.\nBefore using the returned model from Counselor, the\nFinalizer needs to retrain the last stage models with the\noriginal dataset. This is because the position of each key\nin the stretched training set is changed, we need to re-\npair the position information with the original dataset.\nThis process is considerably fast as last models are usu-\nally linear models. For example, it only takes 118 ms to\nretrain one last model with 1000 keys.\n4.2 Reuse Pre-trained Models\nAfter incorporating the access pattern, the only factor af-\nfecting the model architecture is data distribution. We\nnotice that the best model architecture tends to be the\nsame for similar data distributions. As a result, Dorae-\nmon is able to cache a mapping from data distributions\nto models for future reusing.\nThis is done by the Counselor component, which in-\ncludes four modules:\nAnalyzer: extracts distribution information by uni-\nformly sampling K records from the generated training\nset, then normalize both key and position to [0, 1]. How-\never, K needs to be large enough to avoid breaking the\ndistribution.\nModel cache: maintains a mapping from the distri-\nbution of previous training set to their learning model’s\narchiture and parameters. If it receives a distribution\nfrom Analyzer, it will ﬁnds the entry in the map with the\nmost similar distribution based on the mean square error.\nThen, it will send the model’s information in that entry\nto Fine Tuner. Furthermore, if the similarity is below a\nthreshold, it will also start the auto-tuning process.\nFine Tuner: incrementally trains the model retrieved\nfrom the model cache with the training set.\nAuto-tuner: uses grid search to ﬁnd the best model\narchitecture in the given search space. It performs auto-\ntuning at the background and sends the result to the Fi-\n4\n\nnalizer component.\n4.3 Discussion\nDetecting the change of distribution and access pat-\ntern. Doraemon will start to run on detecting the change\nof distribution or access pattern. The detection must\nbe timely with few false positive. For currently design,\nwe simply detect this by monitoring the degradation of\nthe peformance. However, we can use similar technique\nin [24] to improve the accuracy.\nExtract the distribution feature from a dataset.\nCurrently, we simply extract the distribution by uni-\nformly sampling the dataset. However, to avoid breaking\nthe distribution, the sample rate varies across different\ndataset. As a result, it is challengin the decide the sam-\nple rate.\nCompute the similarity. Our sampled distribution\nrepresentation can be regarded as a type of sequential\ndata, for which there are many machine learning models\nare targeting [18, 11]. We believe we can further leverage\nlearning to learn a better similarity metric.\nEfﬁciently ﬁnd the similar distribution in Model\nCache. There can be throusands to millions entires in\nthe Model Cache. As a result, ﬁnding the entry with most\nsimilar distribution is considerably cost. To solve this is-\nsue, we plan to use methods like [38, 22] to ﬁrst ﬁlter out\nthe most relevant entries before the comparison.\nImprove Auto-tuner efﬁciency. Grid search is slow.\nTo speed up the search, there are works that use Gaussian\nprocess to optimize the search process [50, 6, 8]. Similar\nideas are also used in database system optimization [16,\n51].\n5 Related Works\nData Augmentation: Augmenting training data is a\ncommon technique in machine learning to avoid over-\nﬁtting and improve generalizability. Many researchers\nhave been conducted including generating samples\nthrough transformation [1], distortion [49, 54], over-\nsampling [9] and from minority class to deal with data\nimbalance [29, 20, 3]. As a contrast, the goal of our\ndata augmentation is not to improve generalizability, but\nto guide the model to overﬁt more on keys of high fre-\nquency.\nAutomatic Machine Learning (AutoML): Despite the\nsuccess of machine learning, designing models is still\na time-consuming task and require domain expertise.\nTo ease the problem, many works have been focusing\non automatic design and tuning ML models. Auto-\nmatic hyperparameter tuning reduces the tuning efforts\nby means of grid search [4, 31, 6], random search [5, 6],\nBayesian optimization [50, 8], etc. The search time is\nusually proportional to the number of combinations of\nthe hyperparameters to be explored. Neural Architec-ture Search (NAS) [47, 2, 60, 34, 59, 61, 41, 62, 35]\nare more recent attempts to design neural network archi-\ntectures automatically using model-based optimization\nstrategies such as deep reinforcement learning or pro-\ngressive search. These methods usually require tons of\ncomputation resources, making it hard to be deployed in\nresource-critical scenarios like index lookup directly.\nDespite the resource concerns, it is still an open ques-\ntion for AutoML to handle dynamically-changing data\ndistribution [21]. As a result, in Doraemon, we combine\nAutoML with the Model Cache to avoid the costly search\nfor similar data distribution.\nIndexes in Databases Indexing is a fundamental com-\nponent of real-world databases. Some indexes use hy-\nbrid design to serve hot keys and cold keys respectively\nby using different data structures, using different storage,\nand compressing the cold data [57, 14, 17, 33].\nWidely used trie-based indexes [32, 36] usually work\nthe best with near uniform data distribution. To adapt\nthem to less ideal data distributions, Leis et al. use dy-\nnamic fanout to optimize trie height [32], Morrison et al.\nremove unnecessary nodes [40], and Binna et al. aggre-\ngate nodes to form a more balanced structure [7].\nTransfer Learning: Transferring machine learning\nmodels learned from one task to another different but\nrelated task is an active research direction [45, 13, 44].\nCommon practice includes reusing learned representa-\ntions from pre-trained models and ﬁne-tuning from old\nweights [48, 55, 15, 42]. In Doraemon, models are\nﬁne-tuned from the weights obtained from the similar\ndata distribution, which is easier than transferring mod-\nels trained from another distribution.\nData-driven optimizations for system: Many system\noptimizations can be approached by machine learning\nmodels trained from historical data. In the area of\ndatabase, examples include cardinality estimation [30,\n25, 53, 46], join order planning [28, 37, 43] and conﬁg-\nuration tuning [52]. Besides database, works have been\ndone to improve buffer management systems [10], sort-\ning algorithms [58], memory page prefetching [19, 56]\nand memory controller [23] and scheduling [26]. Many\nof these scenarios face similar challenges of dealing with\nshifting data distribution, which could be other applica-\ntions of our model caching mechanism.\n6 Conclusion\nThis paper proposes a system which can incorperate the\nquery distribution in the training set to improve the query\nperformance, and reuse the pre-trained model to reduce\nthe re-trained cost.\nReferences\n[1] Henry S Baird. Document image defect models. In\nStructured Document Image Analysis , pages 546–\n5\n\n556. Springer, 1992.\n[2] Bowen Baker, Otkrist Gupta, Nikhil Naik, and\nRamesh Raskar. Designing neural network ar-\nchitectures using reinforcement learning. arXiv\npreprint arXiv:1611.02167 , 2016.\n[3] Gustavo EAPA Batista, Ronaldo C Prati, and\nMaria Carolina Monard. A study of the behavior\nof several methods for balancing machine learning\ntraining data. ACM SIGKDD explorations newslet-\nter, 6(1):20–29, 2004.\n[4] JC Becsey, Laszlo Berke, and James R Callan.\nNonlinear least squares methods: A direct grid\nsearch approach. Journal of Chemical Education ,\n45(11):728, 1968.\n[5] James Bergstra and Yoshua Bengio. Random\nsearch for hyper-parameter optimization. Journal\nof Machine Learning Research , 13(Feb):281–305,\n2012.\n[6] James S Bergstra, R ´emi Bardenet, Yoshua Bengio,\nand Bal ´azs K ´egl. Algorithms for hyper-parameter\noptimization. In Advances in neural information\nprocessing systems , pages 2546–2554, 2011.\n[7] Robert Binna, Eva Zangerle, Martin Pichl, G ¨unther\nSpecht, and Viktor Leis. Hot: A height optimized\ntrie index for main-memory database systems. In\nProceedings of the 2018 International Conference\non Management of Data , pages 521–534. ACM,\n2018.\n[8] Eric Brochu, Vlad M Cora, and Nando De Fre-\nitas. A tutorial on bayesian optimization of expen-\nsive cost functions, with application to active user\nmodeling and hierarchical reinforcement learning.\narXiv preprint arXiv:1012.2599 , 2010.\n[9] Nitesh V Chawla, Kevin W Bowyer, Lawrence O\nHall, and W Philip Kegelmeyer. Smote: synthetic\nminority over-sampling technique. Journal of arti-\nﬁcial intelligence research , 16:321–357, 2002.\n[10] Xinyun Chen. Deepbm: A deep learning-based dy-\nnamic page replacement policy.\n[11] Kyunghyun Cho, Bart Van Merri ¨enboer, Caglar\nGulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\nger Schwenk, and Yoshua Bengio. Learning\nphrase representations using rnn encoder-decoder\nfor statistical machine translation. arXiv preprint\narXiv:1406.1078 , 2014.\n[12] Transaction Processing Performance Council.\nTpc-h benchmark speciﬁcation. Published athttp://www. tcp. org/hspec. html , 21:592–603,\n2008.\n[13] Wenyuan Dai, Ou Jin, Gui-Rong Xue, Qiang Yang,\nand Yong Yu. Eigentransfer: A uniﬁed framework\nfor transfer learning. In Proceedings of the 26th An-\nnual International Conference on Machine Learn-\ning, ICML ’09, pages 193–200, New York, NY ,\nUSA, 2009. ACM.\n[14] Justin DeBrabant, Andrew Pavlo, Stephen Tu,\nMichael Stonebraker, and Stan Zdonik. Anti-\ncaching: A new approach to database management\nsystem architecture. Proceedings of the VLDB En-\ndowment , 6(14):1942–1953, 2013.\n[15] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy\nHoffman, Ning Zhang, Eric Tzeng, and Trevor Dar-\nrell. Decaf: A deep convolutional activation feature\nfor generic visual recognition. In International con-\nference on machine learning , pages 647–655, 2014.\n[16] Songyun Duan, Vamsidhar Thummala, and Shiv-\nnath Babu. Tuning database conﬁguration parame-\nters with ituned. Proceedings of the VLDB Endow-\nment , 2(1):1246–1257, 2009.\n[17] Ahmed Eldawy, Justin Levandoski, and Per- ˚Ake\nLarson. Trekking through siberia: Managing cold\ndata in a memory-optimized database. Proceedings\nof the VLDB Endowment , 7(11):931–942, 2014.\n[18] Klaus Greff, Rupesh K Srivastava, Jan Koutn ´ık,\nBas R Steunebrink, and J ¨urgen Schmidhuber. Lstm:\nA search space odyssey. IEEE transactions on neu-\nral networks and learning systems , 28(10):2222–\n2232, 2017.\n[19] Milad Hashemi, Kevin Swersky, Jamie A Smith,\nGrant Ayers, Heiner Litz, Jichuan Chang, Chris-\ntos Kozyrakis, and Parthasarathy Ranganathan.\nLearning memory access patterns. arXiv preprint\narXiv:1803.02329 , 2018.\n[20] Haibo He and Edwardo A Garcia. Learning from\nimbalanced data. IEEE Transactions on Knowledge\n& Data Engineering , (9):1263–1284, 2008.\n[21] Frank Hutter, Lars Kotthoff, and Joaquin Van-\nschoren, editors. Automatic Machine Learning:\nMethods, Systems, Challenges . Springer, 2018. In\npress, available at http://automl.org/book.\n[22] Ihab F Ilyas, George Beskales, and Mohamed A\nSoliman. A survey of top-k query processing tech-\nniques in relational database systems. ACM Com-\nputing Surveys (CSUR) , 40(4):11, 2008.\n6\n\n[23] Engin Ipek, Onur Mutlu, Jos ´e F Mart ´ınez, and\nRich Caruana. Self-optimizing memory con-\ntrollers: A reinforcement learning approach. In\nACM SIGARCH Computer Architecture News , vol-\nume 36, pages 39–50. IEEE Computer Society,\n2008.\n[24] Daniel Kang, John Emmons, Firas Abuzaid, Peter\nBailis, and Matei Zaharia. Noscope: optimizing\nneural network queries over video at scale. Pro-\nceedings of the VLDB Endowment , 10(11):1586–\n1597, 2017.\n[25] Andreas Kipf, Thomas Kipf, Bernhard Radke, Vik-\ntor Leis, Peter Boncz, and Alfons Kemper. Learned\ncardinalities: Estimating correlated joins with deep\nlearning. arXiv preprint arXiv:1809.00677 , 2018.\n[26] Tim Kraska, Mohammad Alizadeh, Alex Beutel,\nE Chi, Jialin Ding, Ani Kristo, Guillaume Leclerc,\nSamuel Madden, Hongzi Mao, and Vikram Nathan.\nSagedb: A learned database system. CIDR, 2019.\n[27] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean,\nand Neoklis Polyzotis. The case for learned index\nstructures. In Proceedings of the 2018 International\nConference on Management of Data , pages 489–\n504. ACM, 2018.\n[28] Sanjay Krishnan, Zongheng Yang, Ken Goldberg,\nJoseph Hellerstein, and Ion Stoica. Learning to op-\ntimize join queries with deep reinforcement learn-\ning. arXiv preprint arXiv:1808.03196 , 2018.\n[29] Miroslav Kubat, Stan Matwin, et al. Addressing the\ncurse of imbalanced training sets: one-sided selec-\ntion. In Icml, volume 97, pages 179–186. Nashville,\nUSA, 1997.\n[30] M Seetha Lakshmi and Shaoyu Zhou. Selectivity\nestimation in extensible databases-a neural network\napproach. In Proceedings of the 24rd International\nConference on Very Large Data Bases , pages 623–\n627. Morgan Kaufmann Publishers Inc., 1998.\n[31] Steven M LaValle, Michael S Branicky, and\nStephen R Lindemann. On the relationship between\nclassical grid search and probabilistic roadmaps.\nThe International Journal of Robotics Research ,\n23(7-8):673–692, 2004.\n[32] Viktor Leis, Alfons Kemper, and Thomas Neu-\nmann. The adaptive radix tree: Artful indexing for\nmain-memory databases. In 2013 IEEE 29th Inter-\nnational Conference on Data Engineering (ICDE) ,\npages 38–49. IEEE, 2013.[33] Justin J Levandoski, Per- ˚Ake Larson, and Radu\nStoica. Identifying hot and cold data in main-\nmemory databases. In Data Engineering (ICDE),\n2013 IEEE 29th International Conference on ,\npages 26–37. IEEE, 2013.\n[34] Chenxi Liu, Barret Zoph, Maxim Neumann,\nJonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei,\nAlan Yuille, Jonathan Huang, and Kevin Murphy.\nProgressive neural architecture search. In Proceed-\nings of the European Conference on Computer Vi-\nsion (ECCV) , pages 19–34, 2018.\n[35] Chenxi Liu, Barret Zoph, Maxim Neumann,\nJonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei,\nAlan Yuille, Jonathan Huang, and Kevin Murphy.\nProgressive neural architecture search. In The Eu-\nropean Conference on Computer Vision (ECCV) ,\nSeptember 2018.\n[36] Yandong Mao, Eddie Kohler, and Robert Tappan\nMorris. Cache craftiness for fast multicore key-\nvalue storage. In Proceedings of the 7th ACM euro-\npean conference on Computer Systems , pages 183–\n196. ACM, 2012.\n[37] Ryan Marcus and Olga Papaemmanouil. Deep re-\ninforcement learning for join order enumeration.\nInProceedings of the First International Workshop\non Exploiting Artiﬁcial Intelligence Techniques for\nData Management , page 3. ACM, 2018.\n[38] Ahmed Metwally, Divyakant Agrawal, and Amr\nEl Abbadi. Efﬁcient computation of frequent and\ntop-k elements in data streams. In International\nConference on Database Theory , pages 398–412.\nSpringer, 2005.\n[39] Michael Mitzenmacher. A model for learned bloom\nﬁlters and optimizing by sandwiching. In Advances\nin Neural Information Processing Systems , pages\n462–471, 2018.\n[40] Donald R Morrison. Patriciapractical algorithm to\nretrieve information coded in alphanumeric. Jour-\nnal of the ACM (JACM) , 15(4):514–534, 1968.\n[41] Renato Negrinho and Geoff Gordon. Deeparchi-\ntect: Automatically designing and training deep\narchitectures. arXiv preprint arXiv:1704.08792 ,\n2017.\n[42] Maxime Oquab, Leon Bottou, Ivan Laptev, and\nJosef Sivic. Learning and transferring mid-level\nimage representations using convolutional neural\nnetworks. In Proceedings of the IEEE conference\non computer vision and pattern recognition , pages\n1717–1724, 2014.\n7\n\n[43] Jennifer Ortiz, Magdalena Balazinska, Johannes\nGehrke, and S Sathiya Keerthi. Learning state rep-\nresentations for query optimization with deep re-\ninforcement learning. In Proceedings of the Sec-\nond Workshop on Data Management for End-To-\nEnd Machine Learning , page 4. ACM, 2018.\n[44] Sinno Jialin Pan, Ivor W Tsang, James T Kwok,\nand Qiang Yang. Domain adaptation via transfer\ncomponent analysis. IEEE Transactions on Neural\nNetworks , 22(2):199–210, 2011.\n[45] Sinno Jialin Pan, Qiang Yang, et al. A survey on\ntransfer learning. IEEE Transactions on knowledge\nand data engineering , 22(10):1345–1359, 2010.\n[46] Yongjoo Park, Shucheng Zhong, and Barzan Moza-\nfari. Quicksel: Quick selectivity learning with\nmixture models. arXiv preprint arXiv:1812.10568 ,\n2018.\n[47] Esteban Real, Alok Aggarwal, Yanping Huang,\nand Quoc V Le. Regularized evolution for im-\nage classiﬁer architecture search. arXiv preprint\narXiv:1802.01548 , 2018.\n[48] Ali Sharif Razavian, Hossein Azizpour, Josephine\nSullivan, and Stefan Carlsson. Cnn features off-the-\nshelf: an astounding baseline for recognition. In\nProceedings of the IEEE conference on computer\nvision and pattern recognition workshops , pages\n806–813, 2014.\n[49] Patrice Y Simard, Dave Steinkraus, and John C\nPlatt. Best practices for convolutional neural net-\nworks applied to visual document analysis. In null,\npage 958. IEEE, 2003.\n[50] Jasper Snoek, Hugo Larochelle, and Ryan P\nAdams. Practical bayesian optimization of ma-\nchine learning algorithms. In Advances in neural\ninformation processing systems , pages 2951–2959,\n2012.\n[51] Vamsidhar Thummala and Shivnath Babu. ituned:\na tool for conﬁguring and visualizing database pa-\nrameters. In Proceedings of the 2010 ACM SIG-\nMOD International Conference on Management of\ndata, pages 1231–1234. ACM, 2010.\n[52] Dana Van Aken, Andrew Pavlo, Geoffrey J Gordon,\nand Bohan Zhang. Automatic database manage-\nment system tuning through large-scale machine\nlearning. In Proceedings of the 2017 ACM Interna-\ntional Conference on Management of Data , pages\n1009–1024. ACM, 2017.[53] Chenggang Wu, Alekh Jindal, Saeed Amizadeh,\nHiren Patel, Wangchao Le, Shi Qiao, and Sriram\nRao. Towards a learning optimizer for shared\nclouds. In Proceedings of the 45th International\nConference on Very Large Data Bases (VLDB),\npage to appear , 2019.\n[54] Larry S Yaeger, Richard F Lyon, and Brandyn J\nWebb. Effective training of a neural network char-\nacter classiﬁer for word recognition. In Advances in\nneural information processing systems , pages 807–\n816, 1997.\n[55] Jason Yosinski, Jeff Clune, Yoshua Bengio, and\nHod Lipson. How transferable are features in deep\nneural networks? In Z. Ghahramani, M. Welling,\nC. Cortes, N. D. Lawrence, and K. Q. Weinberger,\neditors, Advances in Neural Information Process-\ning Systems 27 , pages 3320–3328. Curran Asso-\nciates, Inc., 2014.\n[56] Yuan Zeng and Xiaochen Guo. Long short term\nmemory based hardware prefetcher: a case study.\nInProceedings of the International Symposium on\nMemory Systems , pages 305–311. ACM, 2017.\n[57] Huanchen Zhang, David G Andersen, Andrew\nPavlo, Michael Kaminsky, Lin Ma, and Rui Shen.\nReducing the storage overhead of main-memory\noltp databases with hybrid indexes. In Proceedings\nof the 2016 International Conference on Manage-\nment of Data , pages 1567–1581. ACM, 2016.\n[58] Hanqing Zhao and Yuehan Luo. An o(n)sorting al-\ngorithm: Machine learning sorting. arXiv preprint\narXiv:1805.04272 , 2018.\n[59] Zhao Zhong, Junjie Yan, and Cheng-Lin Liu. Prac-\ntical network blocks design with q-learning. arXiv\npreprint arXiv:1708.05552 , 2017.\n[60] Barret Zoph and Quoc V Le. Neural architecture\nsearch with reinforcement learning. arXiv preprint\narXiv:1611.01578 , 2016.\n[61] Barret Zoph, Vijay Vasudevan, Jonathon Shlens,\nand Quoc V Le. Learning transferable architectures\nfor scalable image recognition. In Proceedings of\nthe IEEE conference on computer vision and pat-\ntern recognition , pages 8697–8710, 2018.\n[62] Barret Zoph, Vijay Vasudevan, Jonathon Shlens,\nand Quoc V . Le. Learning transferable architectures\nfor scalable image recognition. In The IEEE Con-\nference on Computer Vision and Pattern Recogni-\ntion (CVPR) , June 2018.\n8",
  "textLength": 34388
}