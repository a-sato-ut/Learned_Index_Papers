{
  "paperId": "15b6fba2bfe6e9cb443d0b6177d6ec5501cff579",
  "title": "Distributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems",
  "pdfPath": "15b6fba2bfe6e9cb443d0b6177d6ec5501cff579.pdf",
  "text": "DISTRIBUTED HIERARCHICAL GPU P ARAMETER SERVER FOR\nMASSIVE SCALE DEEP LEARNING ADSSYSTEMS\nWeijie Zhao1, Deping Xie2, Ronglai Jia2, Yulei Qian2, Ruiquan Ding3, Mingming Sun1, Ping Li1\n1Cognitive Computing Lab, Baidu Research\n2Baidu Search Ads (Phoenix Nest), Baidu Inc.\n3Sys. & Basic Infra., Baidu Inc.\n{weijiezhao, xiedeping01, jiaronglai, qianyulei, dingruiquan, sunmingming01, liping11}@baidu.com\nABSTRACT\nNeural networks of ads systems usually take input from multiple resources, e.g., query-ad relevance, ad features\nand user portraits. These inputs are encoded into one-hot or multi-hot binary features, with typically only a tiny\nfraction of nonzero feature values per example. Deep learning models in online advertising industries can have\nterabyte-scale parameters that do not ﬁt in the GPU memory nor the CPU main memory on a computing node.\nFor example, a sponsored online advertising system can contain more than 1011sparse features, making the\nneural network a massive model with around 10 TB parameters. In this paper, we introduce a distributed GPU\nhierarchical parameter server for massive scale deep learning ads systems. We propose a hierarchical workﬂow\nthat utilizes GPU High-Bandwidth Memory, CPU main memory and SSD as 3-layer hierarchical storage. All the\nneural network training computations are contained in GPUs. Extensive experiments on real-world data conﬁrm\nthe effectiveness and the scalability of the proposed system. A 4-node hierarchical GPU parameter server can\ntrain a model more than 2X faster than a 150-node in-memory distributed parameter server in an MPI cluster. In\naddition, the price-performance ratio of our proposed system is 4-9 times better than an MPI-cluster solution.\n1 I NTRODUCTION\nBaidu Search Ads (a.k.a. “Phoenix Nest”) has been success-\nfully using ultra-high dimensional input data and ultra-large-\nscale deep neural networks for training CTR (Click-Through\nRate) models since 2013 (Fan et al., 2019). Sponsored on-\nline advertising produces many billions of dollar revenues\nfor online ad publishers such as Baidu, Bing, and Google.\nThe task of CTR prediction (Broder, 2002; Fain & Pedersen,\n2006; Fan et al., 2019; Zhao et al., 2019) plays a key role to\ndetermine the best ad spaces allocation because it directly\ninﬂuences user experience and ads proﬁtability. CTR pre-\ndiction takes input from multiple resources–e.g., query-ad\nrelevance, ad features, and user portraits–then estimates the\nprobability that a user clicks on a given ad. These inputs\nare typically one-hot/multi-hot binary features with only a\ntiny fraction of nonzero feature values per example. The\nhigh-dimensional sparse input data ﬁrst go through an em-\nbedding layer to get a low-dimensional embedding; then\nthe embedded results are connected by fully-connected lay-\ners. A real-world sponsored online advertising system can\ncontain more than 1011sparse features, making the neural\nnetwork a massive model with around 10 TB parameters.\nFigure 1 provides an illustration of CTR prediction network.\nProceedings of the 3rdMLSys Conference , Austin, TX, USA,\n2020. Copyright 2020 by the author(s).\nSparse input\nSparse \nparameters\nDense \nparameters\nOutput(~10 TB)\n(< 1 GB)Embedding\nlayer\nFully-connected\nlayersFigure 1. A visual illustration of CTR prediction network. The in-\nput data are sparse—only a tiny proportion of features are non-zero.\nThe parameters of hatched neurons are referenced by the input.\n1.1 CTR Prediction Based on MPI-Cluster Solution\nSince 2013, the standard practice in Baidu Search Ads has\nbeen using an MPI-based solution for training massive-\nscale deep learning CTR prediction. Before that, Baidu had\nadopted a distributed logistic regression (LR) CTR model\nand distributed parameter server around 2010. The MPI\nsolution partitions the model parameters across comput-\ning nodes (e.g., 150 nodes). Each node obtains a batch of\ntraining data streamed from a distributed ﬁle system. The\nnode pulls the required parameters from other nodes andarXiv:2003.05622v1  [cs.DC]  12 Mar 2020\n\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\ncomputes the gradients. The gradients are pushed to the\nnodes that maintain the corresponding parameters through\nMPI communications. Baidu’s MPI solution and distributed\nparameter server was also summarized in (Li et al., 2014)\n(while the ﬁrst author in that paper was with Baidu Inc.).\nWe should mention that this paper focuses on the CTR mod-\nels in the ﬁnal stage of Baidu’s ads system. In the earlier\nstage of the pipeline, Baidu has been extensively using re-\ncent advancements on approximate near neighbor search\n(ANNS) and maximum inner product search (MIPS) (Fan\net al., 2019; Zhou et al., 2019; Zhao et al., 2020; Tan et al.,\n2020) to improve the quality of ads recalls, which is a sepa-\nrate important task in the full pipeline of the ads system.\n1.2 Drawbacks of MPI-Cluster Solution\nCTR model researchers and practitioners train the massive-\nscale model rapidly to test and verify their new ideas. How-\never, it is impractical to reserve a large-scale computing\ncluster for everyone—the hardware maintenance and the\nenergy cost of a large-scale computing cluster is pricey. Be-\nsides, further scaling up the training speed is challenging—\nin this large-scale distributed setting, communication and\nsynchronization cost between computing nodes limits the\ntraining performance. We seek to explore other directions to\noptimize the training system for massive-scale ads models.\n1.3 GPU Parameter Server for CTR Prediction\nRecently, GPUs attract broad attention as a cost-efﬁcient\nmachine learning accelerator. However, it is not straight-\nforward to directly employ GPUs on the CTR prediction\nproblem due to the limited GPU memory and excessive\nCPU-GPU data communications. This amount of parame-\nters exceeds the amount of overall volume of GPU High-\nBandwidth Memory (HBM) of any single computing node,\nand any practical computing cluster. GPUs are considered\ninefﬁcient to train the model that does not ﬁt in the GPU\nmemory (Chilimbi et al., 2014). Prior efforts (Cui et al.,\n2016; Zhao et al., 2019) cache the parameters in the CPU\nmain memory and still require time-consuming data trans-\nferring between CPU and GPU to update parameters.\nWe notice that in the massive-scale ads system, although the\nentire model is huge, the number of non-zero features per\nexample is small. For example, in Figure 1, the referenced\nparameters for the given input are hatched. The sparse\ninput has 2non-zeros: the 1stand the 6thfeatures. For the\nparameters in the embedding layer, only a subset of them\nis used and will be updated for this sparse input (we call\nthem “sparse parameters” for convenience). Following that,\nthe dense parameters in the fully-connected layers are all\nreferenced. The CTR prediction neural network usually\nhas at most a few million dense parameters—the memory\nfootprint of dense parameters is small (around a few hundredmegabytes). The working parameters–the parameters that\nare referenced in the current batch input–can ﬁt in the GPU\nmemory. It allows us to distribute the working parameters\nin the GPU memory and accelerate the training with GPUs.\nIn this paper, we propose a hierarchical parameter server\nthat builds a distributed hash table across multiple GPUs\nand performs direct inter-GPU communications to eliminate\nthe CPU-GPU data transferring overhead. Moreover, the\nhierarchical design on top of SSDs with an in-memory cache\nalso enables us to train a large-scale out-of-main-memory\ndeep neural network on distributed GPU environments.\nChallenges & approaches. There are three major chal-\nlenges in the design of the hierarchical parameter server. 1)\nThe ﬁrst challenge is to construct an efﬁcient distributed\nGPU hash table to store the working parameters in multiple\nGPUs in multiple computing nodes. We design intra-node\nGPU communications and collective communication strate-\ngies among multiple nodes to synchronize the parameters\nacross all GPUs. Remote direct memory access protocols\nare employed to enable direct peer-to-peer GPU communi-\ncations without involving CPUs. 2) The second challenge\nis to devise a data transferring and caching mechanism to\nkeep working parameters in memory. Data transferring and\nSSD I/Os bandwidth are relatively slow compared with the\ndeep neural network training on GPUs. Without a care-\nfully designed architecture, excessive network and disk I/Os\ncan be a dominant performance slowdown. We propose\na 4-stage pipeline that overlaps inter-node network com-\nmunications, SSD I/Os, CPU/GPU data transferring and\nGPU computations. All stages are optimized to match the\ninevitable GPU computations—the pipeline hides the I/O\nlatency. 3) The third challenge is to effectively organize the\nmaterialized parameters on SSDs. The I/O granularity of\nSSDs is a block, while the training parameters we load is in\nkey-value granularity. This mismatch incurs I/O ampliﬁca-\ntion (Lu et al., 2017)—unnecessary data are read from SSDs\nto load required parameters. In contrast, reading and writ-\ning contiguous addresses on SSDs are faster than random\naccesses—larger block size yields better disk I/O bandwidth.\nWe present a ﬁle-level parameter management strategy to\nwrite updates in batches as new ﬁles. A ﬁle compaction\nthread runs in the background to regularly merge ﬁles that\ncontain a large proportion of stale parameters into new ﬁles.\nContributions. The technical contributions we make in this\npaper can be summarized as follows:\n\u000fWe present a hashing method (OP+OSRP) to reduce CTR\nmodels. We observe that combining OP+OSRP with\nDNN-based models can replace the original LR-based\nmodels. However, for DNN-based CTR models, the hash-\ning method will hurt the prediction accuracy to the extent\nthat the revenue would be noticeably affected (Section 2).\n\u000fWe introduce the architecture of a distributed hierarchi-\n\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\ncal GPU parameter server (Section 3). The hierarchical\ndesign employs the main memory and SSDs to store the\nout-of-GPU-memory and out-of-main-memory parame-\nters when training massive-scale neural networks. As\nfar as we know, it is the ﬁrst distributed GPU parameter\nserver for the terabyte-scale sparse input deep learning.\n\u000fWe propose a novel HBM parameter server that keeps\nthe working parameters in distributed GPU HBMs. The\nHBM parameter server enables GPU worker threads to\nefﬁciently fetch/update neural network parameters and\nto directly synchronize parameters across multiple GPU\ncards from multiple computing nodes (Section 4).\n\u000fWe show a CPU main memory parameter server that\nprefetches and buffers the parameters of the future train-\ning batches. The memory parameter server hides the data\nloading and parameter transfer latency (Section 5).\n\u000fWe present an SSD parameter server that materializes the\nout-of-main-memory parameters in ﬁles. Parameter up-\ndates are written in batches onto SSDs as new ﬁles to fully\nutilize the I/O bandwidth. Regular compaction operations\nare performed to bound the disk usage (Section 6).\n\u000fWe perform an extensive set of experiments on 5real CTR\nprediction models/datasets and compare the results with\nan MPI cluster training framework in the production envi-\nronment (Section 7). The 4-node distributed hierarchical\nGPU parameter server is 1.8-4.8X faster than the MPI\nsolution. The price-performance ratio of our proposed\nsystem is 4-9 times better than the MPI solution.\n2 P RIOR EFFORT : H ASHING METHODS\nFOR REDUCING CTR M ODELS\nIt is natural to ask why one could not simply use a good\nhashing method to reduce the model size. In this section, we\nreport a brief summary of our prior efforts back in 2015 for\ndeveloping hashing algorithms to reduce the size of CTR\nmodels, with the hope that hashing would not hurt accuracy.\nHashing algorithms are popular these days (Weinberger\net al., 2009; Li et al., 2011). We had spent serious efforts to\ndevelop effective hashing methods suitable for CTR models.\nThe results are both encouraging and discouraging. Here\nwe would like to make a comment that there is a differ-\nence between academia research and industrial practice. In\nacademia research, we often take it as a good result if, for\nexample, we are able to achieve a 10-fold model reduction\nby losing only 1%of accuracy. For commercial search, how-\never, we often cannot afford to lose an accuracy of 0:1%\nsince that would result in a noticeable decrease in revenue.\nOne permutation + one sign random projection. In the\ncourse of this research, we had tried many versions of hash-\ning. We eventually recommend the method called “one\npermutation + one sign random projection (OP+OSRP)”.\nAs the name suggests, our idea was inspired by several well-known hashing methods, e.g., (Goemans & Williamson,\n1995; Charikar, 2002; Charikar et al., 2004; Cormode &\nMuthukrishnan, 2005; Weinberger et al., 2009; Li et al.,\n2011; 2012; Shrivastava & Li, 2014; Li et al., 2019).\nWe assume the dimensionality of the binary training data is\np. OP+OSRP consists of the following key steps:\n1.Permute the pcolumns (only once). This can be very\nefﬁciently done via standard 2U or 4U hashing.\n2. Break the permuted pcolumns uniformly into kbins.\n3.Apply independent random projection in each bin, for\nexample, in the ﬁrst bin, we let z=Pp=k\ni=1xiri, where\nwe sample ri2f\u0000 1;+1gw. p. 1=2.\n4.Store the sign of zand expand it into a vector of length 2:\n[0 1] ifz >0, [1 0] ifz < 0 , [0 0] ifz = 0. The hashed\ndata will be binary in 2kdimensions. This step is differ-\nent from prior research. This way, we still obtain binary\nfeatures so that we do not need to modify the training\nalgorithm (which is coded efﬁciently for binary data).\nOP+OSRP is very efﬁcient (essentially by touching each\nnonzero entry once) and can naturally handle sparse data\n(i.e., many bins will be empty since kcannot be small).\nExperimental results. This set of experiments was con-\nducted in 2015 using 3 months of sponsored ads click data\nfrom web search and 3 months of data from image search.\nAs one would expect, web search brings in majority of the\nrevenue. Image search is fun and important but its revenue\nis only a tiny fraction of the revenue from web search. Un-\nderstandably, lots of computing/storage resources have been\nallocated for building CTR models for web search.\nTable 1. OP+OSRP for Image Search Sponsored Ads Data\n# Nonzero Weights Test AUC\nBaseline LR 31,949,213,205 0.7112\nBaseline DNN 0.7470\nHash+DNN (k = 234) 6,439,972,994 0.7407\nHash+DNN (k = 223) 3,903,844,565 0.7388\nHash+DNN (k = 222) 2,275,442,496 0.7370\nHash+DNN (k = 231) 1,284,025,453 0.7339\nHash+DNN (k = 230) 707,983,366 0.7310\nHash+DNN (k = 229) 383,499,175 0.7278\nHash+DNN (k = 228) 203,864,439 0.7245\nHash+DNN (k = 227) 106,824,123 0.7208\nHash+DNN (k = 226) 55,363,771 0.7175\nHash+DNN (k = 225) 28,479,330 0.7132\nHash+DNN (k = 224) 14,617,324 0.7113\nTable 1 summarizes the experimental result for applying\nOP+OSRP on image search ads data. There are several\nimportant observations from the results:\n\u000fCompared to the baseline LR model, the DNN model\nvery substantially improves the AUC. This is basically the\n\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\njustiﬁcation of adopting DNN models for CTR prediction.\n\u000fHashing reduces the accuracy. Even with k= 234, the\ntest AUC is dropped by 0:7%.\n\u000fHash+DNN is a good combination for replacing LR.\nCompared to the original baseline LR model, we can re-\nduce the number of nonzero weights from 31B to merely\n14.6M without affecting the accuracy.\nTable 2 summarizes the experiments on web search ads data.\nThe trend is essentially similar to Table 1. The main differ-\nence is that we cannot really propose to use Hash+DNN for\nweb search ads CTR models, because that would reduce the\naccuracy of current DNN-based models and consequently\nwould affect the revenue for the company.\nTable 2. OP+OSRP for Web Search Sponsored Ads Data\n# Nonzero Weights Test AUC\nBaseline LR 199,359,034,971 0.7458\nBaseline DNN 0.7670\nHash+DNN (k = 232) 3,005,012,154 0.7556\nHash+DNN (k = 231) 1,599,247,184 0.7547\nHash+DNN (k = 230) 838,120,432 0.7538\nHash+DNN (k = 229) 433,267,303 0.7528\nHash+DNN (k = 228) 222,780,993 0.7515\nHash+DNN (k = 227) 114,222,607 0.7501\nHash+DNN (k = 226) 58,517,936 0.7487\nHash+DNN (k = 224) 15,410,799 0.7453\nHash+DNN (k = 222) 4,125,016 0.7408\nSummary. This section summarizes our effort on develop-\ning effective hashing methods for ads CTR models. The\nwork was done in 2015 and we had never attempted to pub-\nlish the paper. The proposed algorithm, OP+OSRP, actually\nstill has some novelty to date, although it obviously com-\nbines several previously known ideas. The experiments are\nexciting in a way because it shows that one can use a single\nmachine to store the DNN model and can still achieve a\nnoticeable increase in AUC compared to the original (large)\nLR model. However, for the main ads CTR model used in\nweb search which brings in the majority of the revenue, we\nobserve that the test accuracy is always dropped as soon\nas we try to hash the input data. This is not acceptable in\nthe current business model because even a 0:1%decrease in\nAUC would result in a noticeable decrease in revenue.\nTherefore, this report helps explain why we introduce the\ndistributed hierarchical GPU parameter server in this paper\nto train the massive scale CTR models, in a lossless fashion.\n3 D ISTRIBUTED HIERARCHICAL\nPARAMETER SERVER OVERVIEW\nIn this section, we present the distributed hierarchical param-\neter server overview and describe its main modules from\na high-level view. Figure 2 illustrates the proposed hier-\narchical parameter server architecture. It contains three\nmajor components: HBM-PS, MEM-PS and SSD-PS.\nWorkerspull/pushParameter shards\nGPU3Inter-GPU\ncommunicationsData shardsGPU1\nGPU4GPU2\nHDFSMemory\nLocal \nparameters\nData shards\nSSDBatch load/dump\nMaterialized\nparametersLocal pull/push & Data transfer\nSSD-PSMEM-PSHBM-PS\nRemote \npull/pushRDMA remote \nsynchronizationFigure 2. Hierarchical parameter server architecture.\nWorkﬂow. Algorithm 1 depicts the distributed hierarchi-\ncal parameter server training workﬂow. The training data\nbatches are streamed into the main memory through a net-\nwork ﬁle system, e.g., HDFS (line 2). Our distributed train-\ning framework falls in the data-parallel paradigm (Li et al.,\n2014; Cui et al., 2014; 2016; Luo et al., 2018). Each node\nis responsible to process its own training batches—different\nnodes receive different training data from HDFS. Then, each\nnode identiﬁes the union of the referenced parameters in\nthe current received batch and pulls these parameters from\nthe local MEM-PS/SSD-PS (line 3) and the remote MEM-\nPS (line 4). The local MEM-PS loads the local parameters\nstored on local SSD-PS into the memory and requests other\nnodes for the remote parameters through the network. Af-\nter all the referenced parameters are loaded in the memory,\nthese parameters are partitioned and transferred to the HBM-\nPS in GPUs. In order to effectively utilize the limited GPU\nmemory, the parameters are partitioned in a non-overlapped\nfashion—one parameter is stored only in one GPU. When a\nworker thread in a GPU requires the parameter on another\nGPU, it directly fetches the parameter from the remote GPU\nand pushes the updates back to the remote GPU through\nhigh-speed inter-GPU hardware connection NVLink (Foley\n& Danskin, 2017). In addition, the data batch is sharded\ninto multiple mini-batches and sent to each GPU worker\nthread (line 5-10). Many recent machine learning system\nstudies (Ho et al., 2013; Chilimbi et al., 2014; Cui et al.,\n2016; Alistarh et al., 2018) suggest that the parameter stale-\nness shared among workers in data-parallel systems leads to\nslower convergence. In our proposed system, a mini-batch\ncontains thousands of examples. One GPU worker thread\nis responsible to process a few thousand mini-batches. An\n\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\nAlgorithm 1 Distributed Hierarchical Parameter Server\nTraining Workﬂow.\n1.while not converged do\n2. batch get_batch_from_HDFS ()\n3. working pull_local_MEM-PS_and_SSD-\nPS(batch )\n4. working working[pull_remote_MEM-PS (batch )\n5. minibatches shard_batch (batch,#GPU,#minibatch )\n6. partitions map_parameters_to_GPUs (working ,#GPU)\n7. fori 1to#GPU do\n8. transfer_to_GPU (i,minibatches i)\n9. insert_into_hashtable (i,partitionsi)\n10. end for\n11. forj 1to#minibatch do\n12. pull_HBM-PS_kernel (minibatch j)\n13. \u0001j train_mini-batch_kernel (j)\n14. push_parameters_updates_back_kernel (\u0001j)\n15. end for\n16. \u0001 pull_updates_HBM-PS ()\n17. parameters_to_dump  update_local_cache ()\n18. push_local_SSD-PS (parameters_to_dump )\n19.end while\ninter-GPU parameter synchronization is performed after\neach mini-batch is processed to eliminate the parameter stal-\neness. Each training worker pulls the required parameters\nof its corresponding mini-batch from the HBM-PS (line 12),\nperforms forward and backward propagation to update the\nparameters (line 13), and interacts with the HBM-PS to up-\ndate the referenced parameters on other GPUs (line 14). The\nMEM-PS collects the updates from the HBM-PS (line 16)\nand dumps infrequently used parameters to the SSD-PS\nwhen the MEM-PS does not have sufﬁcient memory (line 17-\n18). An example for Algorithm 1 is shown in Appendix A.\nHBM-PS, MEM-PS, and SSD-PS communicate in a hier-\narchical storage fashion. The upper-level module acts as a\nhigh-speed cache of the lower-level module.\nHBM-PS. HBM-PS is distributed in the High-Bandwidth\nMemory (HBM) across multiple GPUs. Comparing with\nthe conventional distributed parameter servers, workers in\nGPUs can directly request and update parameters in the\nHBM without transferring data between GPU memory and\nCPU memory. Thus the training throughput is signiﬁcantly\nincreased. However, the HBM capacity is limited and pricey.\nIn order to ﬁt all the terabyte-scale parameters into HBMs,\nwe have to maintain a distributed computing cluster with\nhundreds of GPUs—it is not only expensive but also inefﬁ-\ncient, because excessive inter-GPU and inter-node commu-\nnications are required to request and update the parameters.\nTherefore, our proposed hierarchical architecture leverages\nmemory and SSD to store the massive model parameters.\nMEM-PS. The MEM-PS pulls the referenced parame-ters from remote nodes to prepare the data for the train-\ning in GPUs. After the GPU workers complete the for-\nward/backward propagation, the MEM-PS retrieves the up-\ndates from the GPUs, applies the changes, and materializes\nthe updated parameters in the SSDs. The MEM-PS also\ncaches the frequently used parameters to reduce SSD I/Os.\nSSD-PS. We build the SSD-PS to store the materialized pa-\nrameters and provide efﬁcient parameter loading and updat-\ning. Contiguous addresses readings and writings on SSDs\nhave better I/O bandwidth than random disk accesses. Thus,\nthe SSD-PS organizes the parameters in ﬁles. The updated\nparameters in the memory are written to SSDs in batches as\nnew ﬁles. A ﬁle compaction thread runs in the background\nto regularly merge ﬁles that contain a large proportion of\nstale parameters into new ﬁles.\n4-stage pipeline. The training workﬂow majorly involves\n4 time-consuming tasks: data transferring, parameter parti-\ntioning, materialized data loading/dumping and neural net-\nwork training. The 4 tasks correspond to independent hard-\nware resources: network, CPU, SSD and GPU, respectively.\nWe build a 4-stage pipeline to hide the latency of those tasks\nby maintaining a prefetch queue for each stage. A worker\nthread is created for each stage—it extracts jobs from the\nprefetch queue and feeds the corresponding hardware re-\nsource. After that, the worker thread pushes the processed\nresults into the prefetch queue of the next stage. Especially,\nthe worker thread stalls when the prefetch queue of the next\nstage is full—the next stage has already obtained too many\nunprocessed jobs. The capacity of the prefetch queue is pre-\nset according to the execution time of each stage. A detailed\nexplanation is shown in Appendix B.\n4 HBM-PS\nThe HBM-PS stores the working parameters in GPUs and\nprovides efﬁcient accesses. In this section, we introduce the\nmulti-GPU distributed hash table and GPU communications.\nAdditional details of HBM-PS are described in Appendix C.\n4.1 Multi-GPU Distributed Hash Table\nMulti-GPU distributed hash table manages the GPU HBMs\non the same node and provides a uniﬁed interface to the GPU\nworker threads. All referenced parameters of the current\ntraining batch are partitioned and inserted into the local\nhash table of each GPU. Operations such as insert ,getand\naccumulate are provided to interact with the hash table.\nLocal GPU hash table. Each GPU maintains its\nown local hash table in the HBM. In this paper, we\nadopts the concurrent_unordered_map of cuDF li-\nbrary ( https://github.com/rapidsai/cudf ) as\nthe hash table. The hash table implementation uses the\nopen-addressing method to resolve hash collision. In order\n\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\nto avoid the inefﬁcient dynamic memory allocation in GPUs,\nwe ﬁx the hash table capacity when we construct the hash\ntable—we know the number of parameters (key-value pairs)\nto be stored on each GPU. For parallel hash table updates,\nthe data integrity is guaranteed by GPU atomic operations.\nAlgorithm 2 Distributed GPU hash table accumulate.\nInput: A collections of key-value pairs: ( keys,vals).\n1.switch_to_GPU (get_resource_owner (keys,vals))\n2.partitioned parallel_partition (keys,vals)\n3.fori 1to#GPU do\n4. ifpartitionedi6=;then\n5. async_send (i,partitionedi)\n6. end if\n7.end for\n8.wait_send_destination ()\n9.fori 1to#GPU do\n10. switch_to_GPU (i)\n11. hash_tables i.async_accum (partitionedi)\n12.end for\nOperations. The multi-GPU distributed hash table provides\noperations such as insert ,get andaccumulate . Here\nwe introduce the accumulate operation that updates the\nparameters during neural network training in detail. The\ninsert andget operations implement the pull and push\ninterfaces to interact with the HBM-PS. Their workﬂows\nare similar to the accumulate —their implementation details\nare omitted due to the page limit.\nTheaccumulate operation takes a collection of key-value\npairs as input and accumulates the values onto the param-\neters referenced by the corresponding keys. After each\nback propagation is ﬁnished in the neural network training,\naccumulate operations are called to update the parameters.\nAlgorithm 2 illustrates the accumulate operation workﬂow.\nThe key-value pairs are passed in a zero-copy fashion—\nthese pairs are represented as the addresses of two arrays–\nkeys andvals–in the GPU HBM. We ﬁrst switch to the GPU\nthat owns the memory of the input key-value pairs (line 1).\nThen we parallelly partition them according to the partition\npolicy. The partitioned results for each GPU are stored at\npartitioned (line 2). After that, we send the partitioned key-\nvalue pairs to their corresponding GPUs (line 3-7). The send\noperation is executed asynchronously through NVLink—we\ncan send data to other GPUs simultaneously. Finally, we\niterate over all GPUs and apply the actual accumulation on\nthe local hash table of each GPU asynchronously (line 9-12).\n4.2 GPU Communication\nSince each node processes its own input training batch, these\nbatches may share some working parameters—the shared\nparameters are referenced and updated by multiple nodes.The multi-GPU distributed hash table stores the working\nparameters across all GPUs in the same node. We have to\nsynchronize the parameters on different nodes to guarantee\nthe model convergence (Ho et al., 2013; Chilimbi et al.,\n2014; Cui et al., 2016; Alistarh et al., 2018).\nPhysically, GPU communication is performed through\nRemote Direct Memory Access (RDMA) (Potluri et al.,\n2013) hardware. RDMA enables zero-copy network\ncommunication—it allows the network card to transfer data\nfrom a device memory directly to another device memory\nwithout copying data between the device memory and the\ndata buffers in the operating system. Our RDMA hardware\ndesign eliminates the involvement of the CPU and memory.\nLogically, the inter-node GPU parameter synchronization\nrequires an all-reduce communication—each GPU needs to\nreceive all parameter updates from other GPUs and then\nperforms a reduction to accumulate these updates. Ap-\npendix C.3 illustrates the implementation details.\n5 MEM-PS\nThe MEM-PS identiﬁes the referenced parameters from the\ninput and communicates with the local SSD-PS and remote\nMEM-PS to gather the required parameters. Meanwhile, a\nparameter cache is maintained to reduce SSD I/Os. More\nimplementation details are in Appendix D.\nPrepare parameters. As mentioned in the distributed ar-\nchitecture design (Figure 2), each node only maintains a\nshard of parameters—parameters are partitioned according\nto a pre-deﬁned parameter-to-node mapping. We leverage\nthe modulo hashing as the partition scheme in a similar\nmanner to the GPU parameter partition policy discussed in\nSection 4.1. The MEM-PS partitions the referenced parame-\nters according to their keys. For the remote parameters that\nbelong to a remote node, the MEM-PS pulls them from other\nMEM-PS through the network. For the local parameters,\nthe MEM-PS reads the SSDs to fetch the parameters. In\norder to avoid excessive SSD I/Os, an in-memory cache is\nmaintained to store the recently/frequently used parameters\nand to buffer the working parameters.\nUpdate parameters. The MEM-PS pulls the updated pa-\nrameters from the HBM-PS after its GPUs complete the\ncurrent batch and applies these changes in the memory (we\npin the working parameters in the memory). Note that we\ndo not need to push the changes of the remote parameters to\nother MEM-PS, because the remote parameters are synchro-\nnized across GPUs in HBM-PS—the remote node MEM-PS\ncan pull the updates from its own GPUs.\n6 SSD-PS\nThe SSD-PS aims to maintain the materialized out-of-main-\nmemory parameters efﬁciently on SSDs.\n\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\nParameter ﬁles. We organize the model parameters in\nthe ﬁle-level granularity—each ﬁle contains a collection\nof parameters. A parameter-to-ﬁle mapping is maintained\nin the main memory. A ﬁle descriptor occupies much less\nmemory than the parameter value—a ﬁle descriptor can be\nrepresented as couples of bytes while a parameter value\nconsists of multiple numeric attributes. In addition, a node\nonly contains a shard of the parameters. Thus, we can\nassume the mapping can ﬁt in the memory of a single node.\nParameter I/O . We consider a parameter ﬁle as an SSD\nI/O unit. The SSD-PS gathers requested parameter keys and\nreads an entire parameter ﬁle when it contains requested\nparameters. On the other hand, parameters evicted from the\nHBM-PS cache are dumped onto SSDs. It is impractical\nto locate these parameters and perform in-place updates\ninside the original ﬁle because it randomly writes the disk.\nInstead, our SSD-PS chunks these updated parameters into\nﬁles and writes them as new ﬁles on SSDs—data are sequen-\ntially written onto the disk. After the ﬁles are written, we\nupdate the parameter-to-ﬁle mapping of these parameters.\nThe older versions of the parameters stored in the previous\nﬁles become stale—these older values will not be used since\nthe mapping is updated. The SSD usage hikes as we keep\ncreating ﬁles on SSDs. A ﬁle compaction operation is per-\nformed regularly to reduce the disk usage—many old ﬁles\ncontaining a large proportion of stale values can be merged\ninto new ﬁles. Appendix E presents the implementation of\nparameter loading/dumping and ﬁle compaction operations.\n7 E XPERIMENTAL EVALUATION\nThe objective of the experimental evaluation is to inves-\ntigate the overall performance–as well as the impact of\noptimizations–of the proposed system. Speciﬁcally, the\nexperiments are targeted to answer the following questions:\n\u000fHow does the proposed hierarchical GPU parameter\nserver compare with the MPI cluster solution?\n\u000fHow is the execution time of each training stage?\n\u000fHow does the time distribute in each component?\n\u000fHow does the proposed system scale?\nSystem. We execute the distributed hierarchical GPU pa-\nrameter server experiments on 4 GPU computing nodes.\nEach node has 8 cutting-edge 32 GB HBM GPUs, server-\ngrade CPUs with 48 cores (96 threads), \u00181 TB of memory,\n\u001820 TB RAID-0 NVMe SSDs and a 100 Gb RDMA net-\nwork adaptor. The nodes in the MPI cluster for the baseline\ncomparison are maintained in the same data center. CPUs\nin the MPI cluster have similar performance speciﬁcations\nas the ones in the GPU computing nodes. All nodes are\ninter-connected through a high-speed Ethernet switch. The\nhardware and maintenance cost of 1 GPU node roughly\nequals to the cost of 10 CPU-only MPI nodes.Table 3. Model speciﬁcations.\n#Non-zeros #Sparse #Dense Size (GB) MPI\nA 100 8\u00021097\u0002105300 100\nB 100 2\u000210102\u0002104600 80\nC 500 6\u000210102\u00021062,000 75\nD 500 1\u000210114\u00021066,000 150\nE 500 2\u000210117\u000210610,000 128\nModels. We use 5 CTR prediction models in real-world\nonline sponsor advertising applications to investigate the\neffectiveness of our proposed system. Table 3 illustrates the\nspeciﬁcation of these models. The number of sparse param-\neters of each model varies from 8\u0002109(model A) to 1011\n(model E). The number of dense parameters is 4\u00005orders\nof magnitude less than the number of sparse parameters for\nall models. The size of the smallest model is 300GB, while\nthe largest model has 10TB parameters. The MPI column\nshows the number of CPU-only nodes we used to train each\nmodel in the MPI cluster for real-world ads products. Due\nto different training speed requirements for these products,\nthe number of MPI cluster nodes varies from 75 to 150.\nData. We collect user click history logs from our search\nengine as the training dataset. The datasets are chunked\ninto batches—each batch contains \u00184\u0002106examples. The\ntrained CTR models are evaluated over the production envi-\nronment of our search engine in an A/B testing manner.\n7.1 Comparison with MPI Solution\nWe evaluate the performance of the proposed system on the\ntraining execution time and the prediction accuracy. Our\nproposed system runs on 4GPU computing nodes. We\ntake an MPI cluster training framework in the production\nenvironment as the baseline.The baseline solution employs\na distributed parameter server that shards and keeps the\nmassive parameters in the memory across all the nodes.\nTraining time. Figure 3(a) depicts the total training ex-\necution time of the 4-node hierarchical parameter server\n(HPS-4) and the MPI training solution (MPI-cluster). HPS-\n4 outperforms MPI-cluster in all 5models. The ﬁrst row of\nTable 4 shows the speedup over the MPI solution. The\nHPS-4 is 1.8-4.8X faster than the MPI solution. Note\nthat the hardware and maintenance cost of 1 GPU node\nis roughly equivalent to 10 CPU-only nodes in the MPI\ncluster. The 4-GPU-node setting is much cheaper than the\n75-150 nodes in the MPI cluster. The second row of the\ntable illustrates the cost-normalized speedup of our pro-\nposed system. The cost-normalized speedup is computed\nas:speedup =4=10\u0002#MPI. The price-performance ratio of\nTable 4. Training speedup over the MPI-cluster solution.\nA B C D E\nSpeedup over MPI-cluster 1.8 2.7 4.8 2.2 2.6\nCost-normalized speedup 4.4 5.4 9.0 8.4 8.3\n\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\nA (300GB)B (600GB)C (2TB)D (6TB)E (10TB)0.0E+05.0E+41.0E+51.5E+52.0E+5MPI-cluster HPS-4\nModels#Examples trained/sec\nMPI-100MPI-80\nMPI-75MPI-150MPI-128\n(a) Total execution time\nA (300GB)B (600GB)C (2TB)D (6TB)E (10TB)99.94%99.96%99.98%100.00%100.02%100.04%100.06%100.08%\nModelsRelative AUC (b) AUC accuracy\nA (300GB)B (600GB)C (2TB)D (6TB)E (10TB)050100150200250Read examples Pull/push Train DNN\nModelsExecution time (sec) (c) Execution time distribution\nFigure 3. The performance (execution time and accuracy of 4-node hierarchical parameter server (HPS-4).\nour proposed system is 4.4-9.0X better than the MPI cluster.\nAccuracy. We take Area Under the Curve (Huang & Ling,\n2005) (AUC) as the quality measure of our trained models.\nFigure 3(b) shows the relative AUC of HPS-4 (relative to\nthe MPI-cluster method). The AUC of the MPI-cluster\nsolution is 100% . The AUCs of both trained models are\ntested online for 1day in real online ads products. Since the\nCTR prediction accuracy is crucial to the revenue, we have\nto ensure all our optimizations are loss-less. For Model C,\nthe relative AUC loss of HPS-4 is less than 0:01%. For other\n4models, HPS-4 has even slightly better accuracy than the\nMPI-128. Our hierarchical parameter server runs on fewer\nnodes—fewer stale parameters are used than the case on the\nMPI cluster. Therefore, it is possible to have a better AUC\nthan the MPI solution. Overall, the relative differences of\nall5models are within 0:1%. We can conclude that our\nhierarchical parameter server training is loss-less.\n7.2 Time distribution\nThe time distribution of each abstract execution stage is\npresented in Figure 3(c). As illustrated in the training\nworkﬂow (Algorithm 1), Read examples corresponds\nto the operation that reads and extracts the data from HDFS;\nPull/push is the execution time of gathering and updat-\ning the working parameters in the MEM-PS and SSD-PS;\nandTrain DNN is the execution time of the inter-GPU\ncommunication and the deep neural network training in the\nHBM-PS. Since these stages are paralleled in the pipeline,\nthe overall execution time for each batch is dominated by\nthe slowest stage. For the two small models (Model A and\nB), the Read examples stage that relies on the HDFS\nthroughput becomes the bottleneck. Because the models are\nsmall and ﬁt in the memory, the sparse parameter pulling\nand pushing operations are faster than the HDFS I/Os. When\nthe number of sparse parameters grows, the Pull/push\noperations catch up with the example reading (Model C)\nand become the dominant stage in Model D and E.\n7.3 HBM-PS\nThe execution time distribution of HBM-PS operations is\nshown in Figure 4(a). The pull/push operations of HBM-PS depend on the number of non-zero values of training\ninputs. While the training operation (forward/backward\npropagation) correlates to the number of dense parameters\nof the deep model. The execution time of these operations\nshows the same trend in the model speciﬁcation (Table 3).\nModel A and Model B have around 100non-zero input\nfeatures per example, the pull/push HBM-PS operations are\nfaster than the ones of Model C, D and E. Model E has\nthe largest number of dense parameters. Thus its training\noperation costs most of the execution time in HBM-PS.\n7.4 MEM-PS\nLocal/remote parameters. Figure 4(b) illustrates the exe-\ncution time of pulling local/remote parameters in the MEM-\nPS for Model E over 1, 2 and 4 GPU nodes. The results of\nother models show a similar trend. The remote parameter\npulling operation is not applicable when we only have 1\nGPU node—all the parameters are stored in the same node.\nWhen we deploy the proposed system in distributed envi-\nronments with 2 and 4 nodes, the local and remote pulling\noperations are paralleled. The overall execution time is de-\ntermined by the slower operation. We can observe that the\noverall time of pulling MEM-PS parameters does not hike\nmuch when inter-node communications are involved.\nCache performance. The cache hit rate for Model E is\nillustrated in Figure 4(c). In the beginning of the training,\nthe cache hit rate is low because we start from a cold cache—\nno data are cached. We can observe that the cache hit rate\nincreases steeply during the training of the ﬁrst 10batches.\nAfter 40training batches, the cache hit rate goes to 46% and\nbecomes stable in the following training—the frequently\nvisited parameters are captured by our cache policy.\n7.5 SSD-PS\nFigure 5(a) presents the SSD I/O time for Model E. The SSD\ndisk usage threshold is reached and the parameter ﬁle com-\npaction operation is involved from the 54thbatch—the SSD\nI/O time hikes. The regular ﬁle merging in the compaction\noperation causes the I/O performance ﬂuctuation.\n\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\nA (300GB)B (600GB)C (2TB)D (6TB)E (10TB)020406080100Push-HBM-PS Training Pull-HBM-PS\nModelsExecution time (sec)\n(a) Time distribution in HBM-PS\n1 2 4050100150200250Pull-local Pull-remote\n#NodesExecution time (sec)\nN/A (b) Time distribution in MEM-PS\n0204060801001201400%10%20%30%40%50%\n#BatchCache hit rate (c) Cache hit rate\nFigure 4. The time distribution in HBM-PS/MEM-PS and the cache hit rate on Model E.\n020406080100120140050100150200250\n#BatchSSD I/O time (sec)\n(a) SSD-PS I/O time\n1 2 3 40E+03E+46E+49E+4real ideal\n#Nodes#Examples trained/sec (b) Speedup\nFigure 5. The SSD-PS I/O time and speedup on Model E.\n7.6 Scalability\nThe training throughput (number of examples trained per\nsecond) speedup is depicted in Figure 5(b) over 1, 2, and 4\nnodes. The dashed line represents the ideal linear speedup.\nThe reason for the (slightly) sub-linear speedup is that more\nnetwork communications are incurred when we use more\nnodes. A speedup of 3.57 out of 4 is obtained for 4 nodes.\n7.7 Discussion\nBased on the results shown above, we can answer the\nquestions driving the experimental evaluation. The 4-node\ndistributed hierarchical GPU parameter server is 1.8-4.8X\nfaster than the MPI solution in the production environment.\nThe cost of 4 GPU nodes is much less than the cost of main-\ntaining 75-150 CPU nodes in the MPI cluster. After normal-\nizing the execution time by the hardware and maintenance\ncost, the price-performance ratio of our proposed system is\n4.4-9.0X better than the MPI solution. Besides the training\nspeed, the relative accuracy difference to the MPI solution\nis within 0.1%—the training in our proposed system is loss-\nless. The proposed pipeline parallels all training stages.\nThe overall execution time for each batch is dominated by\nthe HDFS when the model is small. When the number of\nsparse parameters grows to 6 and 10 TB, the sparse param-\neter pulling and pushing operations become the dominant\nfactor. The execution time in the HBM-PS depends on the\nmodel speciﬁcations. A larger number of non-zero values\nof training inputs yields longer execution time for pulling\nand pushing parameters in the HBM-PS. The training for-ward and backward propagation time depend on the size of\nfully-connected layers in the deep model. The execution\ntime of pulling working parameters in the MEM-PS is al-\nmost independent of the number of nodes. Although the\nlocal data required to load from SSDs are reduced when we\nhave multiple nodes, more parameter requests from remote\nnodes have to be processed. In expectation, the average\nnumber of parameters to load from SSDs stays the same\nin multi-node cases. The SSD-PS performance has slight\nﬂuctuations when the parameter ﬁle compaction operation\ninvolves in. Our proposed system scales well in distributed\nenvironments—a training throughput speedup of 3.57 out\nof 4 (the ideal speedup) is obtained for 4 nodes.\n8 R ELATED WORK\nIn this section, we discuss relevant work from CTR predic-\ntion models and parameter servers. Additional related work\nabout in-memory cache management systems and key-value\nstores on SSDs are discussed in Appendix F.\nCTR prediction models. The large scale logistic regression\nmodel with careful feature engineering used to dominate the\nCTR prediction strategies (Edelman et al., 2007; Graepel\net al., 2010). Recently, deep neural networks with embed-\nding layers are applied in the CTR prediction problem and\nobtain signiﬁcant improvements. Generally, the sparse input\nfeatures are converted into dense vectors through embedding\nlayers, and then feed into neural components to expose the\ncorrelation/interaction among them. The differences among\nthese models lie in the neural components above the embed-\nding layer. For example, Deep Crossing (Shan et al., 2016),\nWide&Deep Learning (Cheng et al., 2016), YouTube Rec-\nommendation CTR model (Covington et al., 2016) and Deep\nInterest Network (DIN) (Zhou et al., 2018) design special\nfully-connected layers for corresponding tasks to capture the\nlatent interactions among features; Product-based Neural\nNetwork (PNN) (Qu et al., 2016) employs a product layer\nto capture high-order feature interactions; DeepFM (Guo\net al., 2017) and xDeepFM (Lian et al., 2018) use factoriza-\ntion machines (FM) to model both low-order and high-order\n\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\nfeature correlations. Through this approach, the correlations\nbetween sparse features are automatically exploited.\nThe successes of these systems have proven that deep learn-\ning with sparse input is an effective approach for real-world\ncommercial advertisement and recommendation systems.\nHowever, when the scale of the problem becomes extremely\nhuge, for example, with 1011dimensional sparse data and\ntrillions of samples, the architecture of these models must\nbe carefully designed to cope with the challenges mentioned\nin above sections. This paper is our attempts to solving\nthese challenges using an SSD based, three-layer distributed\nGPU training architecture. We believe our solution greatly\nenhances the feasibility of the methodology of deep learning\nwith sparse input in much larger scale problems.\nParameter servers. One important research direction is\nthe parallel mechanism among the computational nodes\nand server nodes. One such topic is the synchronization\npattern of local updates of parameters, where three typical\ntypes of synchronization patterns are proposed: 1) Bulk\nSynchronous Parallel (BSP) (Valiant, 1990), which strictly\nsynchronizes all local updates from all computing nodes\nin each iteration, and thus the learning procedure acts as\nthe same logic as that of learning in a single machine. 2)\nStale Synchronous Parallel (SSP) (Ho et al., 2013), which\nallows the fastest computing node to run a certain num-\nber of iterations ahead of other nodes. 3) Asynchronous\nParallel (ASP), which does not need any synchronization\namong the updates of computing nodes. Instead of using\nthe same parallel strategy throughout the training procedure,\nFlexPS (Huang et al., 2018) proposed a ﬂexible parallelism\ncontrol for multi-stage learning algorithms such as SVRG.\nAnother direction is to design efﬁcient parameter server\narchitecture for different hardware/network situations. Com-\nmunication is a major bottleneck in parallel training. Posei-\ndon (Zhang et al., 2017) develops two strategies wait-free\nback-propagation andhybrid communication to exploit the\nindependency between parameters of layered model struc-\ntures in deep neural networks and conquer communication\nbottleneck; while Gaia (Hsieh et al., 2017) even considered\nthe problem of running learning program on geo-distributed\nclusters that communicate over WANs. Another challenge\nis the big model size for GPU platforms. When the model\nbecomes bigger, the limited GPU memory cannot hold the\nentire model. GeePS (Cui et al., 2016) builds a two-layer\narchitecture to cope with the problems in this situation, in-\ncluding data movement overheads, GPU stalls, and limited\nGPU memory, where the Memory layer holds the entire\nmodel in memory, and the GPU layer fetch the required\npart of the model from the memory layer, do the compu-\ntation and rewrite the updates to the memory layer. Our\nproposed system constructs a distributed hash table across\nGPU HBMs that enables direct GPU peer-to-peer communi-cations. It reduces the excessive CPU-GPU data movement\nand synchronization overhead in GeePS.\nHowever, when the model size becomes even larger, the\nparameters cannot even be stored in the CPU main memory\nof the cluster. Our solution to the challenge is a three-layer\narchitecture that consists of SSD-PS, MEM-PS layer, and\nHBM-PS. The new architecture results in new challenges\nsuch as GPU I/O problems and we develop strategies to\noptimize the performance. Our architecture can efﬁciently\ntrain a 10 TB model, which greatly enhances the capability\nof GPU clusters to cope with massive scale models.\n9 C ONCLUSIONS\nSince 2013, Baidu Search Ads (a.k.a. “Phoenix Nest”)\nhas been successfully using ultra-high dimensional input\ndata and ultra-large-scale deep neural networks for training\nCTR prediction models. In this paper, we introduce the\narchitecture of a distributed hierarchical GPU parameter\nserver for massive scale deep learning ads systems. As\ndiscussed in Section 2, model hashing techniques such as\n“one-permutation + one sign random projection” are not fully\napplicable to the accuracy-crucial ads industry applications.\nThe deep learning model parameter size can become huge\nand cannot ﬁt in the CPU/GPU memory. The proposed\nhierarchical design employs the main memory and SSDs\nto, respectively, store the out-of-GPU-memory and out-of-\nmain-memory parameters of massive scale neural networks.\nWe perform an extensive set of experiments on 5CTR pre-\ndiction models in real-world online sponsor advertising ap-\nplications. The results conﬁrm the effectiveness and the\nscalability of the proposed system. The 4-node distributed\nhierarchical GPU parameter server is 1.8-4.8X faster than\nthe MPI solution in the production environment. For exam-\nple, a 4-node hierarchical GPU parameter server can train a\nmodel more than 2X faster than a 150-node in-memory dis-\ntributed parameter server in an MPI cluster on Model D. The\ncost of 4 GPU nodes is much less than the cost of maintain-\ning an MPI cluster of 75-150 CPU nodes. After normalizing\nthe execution time by the hardware cost and maintenance\nexpenditure, the price-performance ratio of this proposed\nsystem is 4.4-9.0X better than the previous MPI solution.\nThe system described in this paper is being integrated with\nthe PaddlePaddle deep learning platform ( https://www.\npaddlepaddle.org.cn ) to become the “PaddleBox”.\nACKNOWLEDGEMENT\nThe system described in this paper represents the substantial\neffort in the past years involving several major product teams\nat Baidu Inc. We could not include all the names we should\nacknowledge but only a few: Xuewu Jiao, Quanxiang Jia,\nLian Zhao, Lin Liu, Jiajun Zhang, Yue Wang, Anlong Qi.\n\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\nREFERENCES\nMarc Abrams, Charles R. Standridge, Ghaleb Abdulla,\nStephen M. Williams, and Edward A. Fox. Caching\nproxies: Limitations and potentials. World Wide Web\nJournal , 1(1), 1996.\nDan Alistarh, Christopher De Sa, and Nikola Konstantinov.\nThe convergence of stochastic gradient descent in asyn-\nchronous shared memory. In Proceedings of the 2018\nACM Symposium on Principles of Distributed Computing\n(PODC) , pp. 169–178, New York, NY , 2018.\nAshok Anand, Chitra Muthukrishnan, Steven Kappes,\nAditya Akella, and Suman Nath. Cheap and large cams\nfor high performance data-intensive networked systems.\nInProceedings of the 7th USENIX Symposium on Net-\nworked Systems Design and Implementation (NSDI) , pp.\n433–448, San Jose, CA, 2010.\nDavid G. Andersen, Jason Franklin, Michael Kaminsky,\nAmar Phanishayee, Lawrence Tan, and Vijay Vasudevan.\nFawn: A fast array of wimpy nodes. In Proceedings of the\n22nd ACM Symposium on Operating Systems Principles\n(SOSP) , pp. 1–14, Big Sky, MT, 2009.\nAndrei Broder. A taxonomy of web search. SIGIR Forum ,\n36(2):3–10, 2002.\nPei Cao and Sandy Irani. Cost-aware WWW proxy caching\nalgorithms. In 1st USENIX Symposium on Internet Tech-\nnologies and Systems (USITS) , Monterey, CA, 1997.\nMoses Charikar, Kevin Chen, and Martin Farach-Colton.\nFinding frequent items in data streams. Theor. Comput.\nSci., 312(1):3–15, 2004.\nMoses S. Charikar. Similarity estimation techniques from\nrounding algorithms. In Proceedings on 34th Annual\nACM Symposium on Theory of Computing (STOC) , pp.\n380–388, Montreal, Canada, 2002.\nHeng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal\nShaked, Tushar Chandra, Hrishi Aradhye, Glen Ander-\nson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil,\nZakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu,\nand Hemal Shah. Wide & deep learning for recommender\nsystems. In Proceedings of the 1st Workshop on Deep\nLearning for Recommender Systems, (DLRS@RecSys) ,\npp. 7–10, Boston, MA, 2016.\nTrishul M. Chilimbi, Yutaka Suzue, Johnson Apacible, and\nKarthik Kalyanaraman. Project adam: Building an efﬁ-\ncient and scalable deep learning training system. In 11th\nUSENIX Symposium on Operating Systems Design and\nImplementation (OSDI) , pp. 571–582, Broomﬁeld, CO,\n2014.Hong-Tai Chou and David J DeWitt. An evaluation of buffer\nmanagement strategies for relational database systems.\nAlgorithmica , 1(1-4):311–336, 1986.\nGraham Cormode and S. Muthukrishnan. An improved data\nstream summary: the count-min sketch and its applica-\ntions. Journal of Algorithm , 55(1):58–75, 2005.\nPaul Covington, Jay Adams, and Emre Sargin. Deep neural\nnetworks for youtube recommendations. In Proceedings\nof the 10th ACM Conference on Recommender Systems\n(RecSys) , pp. 191–198, Boston, MA, 2016.\nHenggang Cui, James Cipar, Qirong Ho, Jin Kyu Kim, Se-\nunghak Lee, Abhimanu Kumar, Jinliang Wei, Wei Dai,\nGregory R. Ganger, Phillip B. Gibbons, Garth A. Gibson,\nand Eric P. Xing. Exploiting bounded staleness to speed\nup big data analytics. In 2014 USENIX Annual Technical\nConference (USENIX ATC) , pp. 37–48, Philadelphia, PA,\n2014.\nHenggang Cui, Hao Zhang, Gregory R. Ganger, Phillip B.\nGibbons, and Eric P. Xing. Geeps: Scalable deep learning\non distributed GPUs with a GPU-specialized parameter\nserver. In Proceedings of the Eleventh European Con-\nference on Computer Systems (EuroSys) , pp. 4:1–4:16,\nLondon, UK, 2016.\nShaul Dar, Michael J. Franklin, Björn Þór Jónsson, Divesh\nSrivastava, and Michael Tan. Semantic data caching\nand replacement. In Proceedings of 22th International\nConference on Very Large Data Bases (VLDB) , pp. 330–\n341, Mumbai (Bombay), India, 1996.\nBiplob K. Debnath, Sudipta Sengupta, and Jin Li. Flash-\nstore: High throughput persistent key-value store.\nPVLDB , 3(2):1414–1425, 2010.\nBiplob K. Debnath, Sudipta Sengupta, and Jin Li. Skimpys-\ntash: Ram space skimpy key-value store on ﬂash-based\nstorage. In Proceedings of the ACM SIGMOD Interna-\ntional Conference on Management of Data (SIGMOD) ,\npp. 25–36, Athens, Greece, 2011.\nBenjamin Edelman, Michael Ostrovsky, and Michael\nSchwarz. Internet advertising and the generalized second-\nprice auction: Selling billions of dollars worth of key-\nwords. American economic review , 97(1):242–259, 2007.\nAssaf Eisenman, Maxim Naumov, Darryl Gardner, Misha\nSmelyanskiy, Sergey Pupyrev, Kim Hazelwood, Asaf\nCidon, and Sachin Katti. Bandana: Using non-volatile\nmemory for storing deep learning models. arXiv preprint\narXiv:1811.05922 , 2018.\nDaniel C. Fain and Jan O. Pedersen. Sponsored search:\nA brief history. Bulletin of the American Society for\nInformation Science and Technology , 32(2):12–13, 2006.\n\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\nMiao Fan, Jiacheng Guo, Shuai Zhu, Shuo Miao, Mingming\nSun, and Ping Li. MOBIUS: towards the next generation\nof query-ad matching in baidu’s sponsored search. In\nProceedings of the 25th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining,\n(KDD) , pp. 2509–2517, Anchorage, AK, 2019.\nDenis Foley and John Danskin. Ultra-performance pascal\nGPU and nvlink interconnect. IEEE Micro , 37(2):7–17,\n2017.\nMichel X. Goemans and David P. Williamson. Improved\napproximation algorithms for maximum cut and satisﬁa-\nbility problems using semideﬁnite programming. Journal\nof ACM , 42(6):1115–1145, 1995.\nThore Graepel, Joaquin Quiñonero Candela, Thomas\nBorchert, and Ralf Herbrich. Web-scale bayesian click-\nthrough rate prediction for sponsored search advertising\nin microsoft’s bing search engine. In Proceedings of\nthe 27th International Conference on Machine Learning\n(ICML) , pp. 13–20, Haifa, Israel, 2010.\nHuifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and\nXiuqiang He. Deepfm: A factorization-machine based\nneural network for ctr prediction. In Proceedings of the\nTwenty-Sixth International Joint Conference on Artiﬁcial\nIntelligence (IJCAI) , pp. 1725–1731, Melbourne, Aus-\ntralia, 2017.\nQirong Ho, James Cipar, Henggang Cui, Seunghak Lee,\nJin Kyu Kim, Phillip B. Gibbons, Garth A. Gibson, Gre-\ngory R. Ganger, and Eric P. Xing. More effective dis-\ntributed ml via a stale synchronous parallel parameter\nserver. In Advances in Neural Information Processing\nSystems (NIPS) , pp. 1223–1231, Lake Tahoe, NV , 2013.\nKevin Hsieh, Aaron Harlap, Nandita Vijaykumar, Dimitris\nKonomis, Gregory R. Ganger, Phillip B. Gibbons, and\nOnur Mutlu. Gaia: Geo-distributed machine learning\napproaching lan speeds. In 14th USENIX Symposium on\nNetworked Systems Design and Implementation (NSDI) ,\npp. 629–647, Boston, MA, 2017.\nJin Huang and Charles X Ling. Using auc and accuracy in\nevaluating learning algorithms. IEEE Transactions on\nKnowledge and Data Engineering (TKDE) , 17(3):299–\n310, 2005.\nYuzhen Huang, Tatiana Jin, Yidi Wu, Zhenkun Cai, Xiao\nYan, Fan Yang, Jinfeng Li, Yuying Guo, and James Cheng.\nFlexps: Flexible parallelism control in parameter server\narchitecture. PVLDB , 11(5):566–579, 2018.\nTim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and\nNeoklis Polyzotis. The case for learned index structures.\nInProceedings of the 2018 International Conference onManagement of Data (SIGMOD) , pp. 489–504, Houston,\nTX, 2018.\nDonghee Lee, Jongmoo Choi, Jong-Hun Kim, Sam H Noh,\nSang Lyul Min, Yookun Cho, and Chong Sang Kim. Lrfu:\nA spectrum of policies that subsumes the least recently\nused and least frequently used policies. IEEE Transac-\ntions on Computers , 50(12):1352–1361, 2001.\nMu Li, David G. Andersen, Jun Woo Park, Alexander J.\nSmola, Amr Ahmed, Vanja Josifovski, James Long, Eu-\ngene J. Shekita, and Bor-Yiing Su. Scaling distributed\nmachine learning with the parameter server. In 11th\nUSENIX Symposium on Operating Systems Design and\nImplementation (OSDI) , pp. 583–598, Broomﬁeld, CO,\n2014.\nPing Li, Anshumali Shrivastava, Joshua Moore, and\nArnd Christian König. Hashing algorithms for large-scale\nlearning. In Advances in Neural Information Processing\nSystems (NIPS) , pp. 2672–2680, Granada, Spain, 2011.\nPing Li, Art B Owen, and Cun-Hui Zhang. One permutation\nhashing. In Advances in Neural Information Processing\nSystems (NIPS) , pp. 3122–3130, Lake Tahoe, NV , 2012.\nPing Li, Xiaoyun Li, and Cun-Hui Zhang. Re-randomized\ndensiﬁcation for one permutation hashing and bin-wise\nconsistent weighted sampling. In Advances in Neural\nInformation Processing Systems (NeurIPS) , Vancouver,\nCanada, 2019.\nJianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia\nChen, Xing Xie, and Guangzhong Sun. xdeepfm: Com-\nbining explicit and implicit feature interactions for rec-\nommender systems. In Proceedings of the 24th ACM\nSIGKDD International Conference on Knowledge Dis-\ncovery & Data Mining (KDD) , pp. 1754–1763, London,\nUK, 2018.\nHyeontaek Lim, Bin Fan, David G. Andersen, and Michael\nKaminsky. Silt: A memory-efﬁcient, high-performance\nkey-value store. In Proceedings of the 23rd ACM Sympo-\nsium on Operating Systems Principles (SOSP) , pp. 1–13,\nCascais, Portugal, 2011.\nLanyue Lu, Thanumalayan Sankaranarayana Pillai, Hari-\nharan Gopalakrishnan, Andrea C Arpaci-Dusseau, and\nRemzi H Arpaci-Dusseau. Wisckey: Separating keys\nfrom values in ssd-conscious storage. ACM Transactions\non Storage (TOS) , 13(1):5, 2017.\nLiang Luo, Jacob Nelson, Luis Ceze, Amar Phanishayee,\nand Arvind Krishnamurthy. Parameter hub: a rack-scale\nparameter server for distributed deep neural network train-\ning. In Proceedings of the ACM Symposium on Cloud\nComputing (SoCC) , pp. 41–54, Carlsbad, CA, 2018.\n\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\nShamkant Navathe et al. Vertical partitioning algorithms for\ndatabase design. ACM Transactions on Database Systems\n(TODS) , 9(4):680–710, 1984.\nElizabeth J. O’Neil, Patrick E. O’Neil, and Gerhard Weikum.\nThe lru-k page replacement algorithm for database disk\nbuffering. In Proceedings of the 1993 ACM SIGMOD\nInternational Conference on Management of Data (SIG-\nMOD) , pp. 297–306, Washington, DC, 1993.\nSreeram Potluri, Khaled Hamidouche, Akshay Venkatesh,\nDevendar Bureddy, and Dhabaleswar K. Panda. Efﬁcient\ninter-node mpi communication using gpudirect RDMA\nfor inﬁniband clusters with nvidia GPUs. In 42nd Inter-\nnational Conference on Parallel Processing (ICPP) , pp.\n80–89, Lyon, France, 2013.\nYanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu,\nYing Wen, and Jun Wang. Product-based neural networks\nfor user response prediction. In IEEE 16th International\nConference on Data Mining (ICDM) , pp. 1149–1154,\nBarcelona, Spain, 2016.\nYing Shan, T. Ryan Hoens, Jian Jiao, Haijing Wang, Dong\nYu, and J. C. Mao. Deep crossing: Web-scale model-\ning without manually crafted combinatorial features. In\nProceedings of the 22nd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining\n(KDD) , pp. 255–262, San Francisco, CA, 2016.\nAnshumali Shrivastava and Ping Li. Improved densiﬁcation\nof one permutation hashing. In Proceedings of the Thirti-\neth Conference on Uncertainty in Artiﬁcial Intelligence\n(UAI) , pp. 732–741, Quebec City, Canada, 2014.\nLeonid B. Sokolinsky. Lfu-k: An effective buffer manage-\nment replacement algorithm. In 9th International Con-\nference on Database Systems for Advances Applications\n(DASFAA) , pp. 670–681, Jeju Island, Korea, 2004.\nShulong Tan, Zhixin Zhou, Zhaozhuo Xu, and Ping Li. Fast\nitem ranking under neural network based measures. In\nProceedings of the 13th ACM International Conference\non Web Search and Data Mining (WSDM) , Huston, TX,\n2020.\nLeslie G. Valiant. A bridging model for parallel computation.\nCommun. ACM , 33(8):103–111, 1990.Kilian Weinberger, Anirban Dasgupta, John Langford, Alex\nSmola, and Josh Attenberg. Feature hashing for large\nscale multitask learning. In Proceedings of the 26th\nAnnual International Conference on Machine Learning\n(ICML) , pp. 1113–1120, Montreal, Canada, 2009.\nRoland P. Wooster and Marc Abrams. Proxy caching that\nestimates page load delays. Computer Networks , 29(8-\n13):977–986, 1997.\nHao Zhang, Zeyu Zheng, Shizhen Xu, Wei Dai, Qirong Ho,\nXiaodan Liang, Zhiting Hu, Jinliang Wei, Pengtao Xie,\nand Eric P. Xing. Poseidon: An efﬁcient communication\narchitecture for distributed deep learning on GPU clusters.\nIn2017 USENIX Annual Technical Conference (USENIX\nATC) , pp. 181–193, Santa Clara, CA, 2017.\nWeijie Zhao, Yu Cheng, and Florin Rusu. Vertical partition-\ning for query processing over raw data. In Proceedings of\nthe 27th International Conference on Scientiﬁc and Statis-\ntical Database Management, (SSDBM) , pp. 15:1–15:12,\nLa Jolla, CA, 2015.\nWeijie Zhao, Jingyuan Zhang, Deping Xie, Yulei Qian,\nRonglai Jia, and Ping Li. AIBox: CTR prediction model\ntraining on a single node. In Proceedings of the 28th ACM\nInternational Conference on Information and Knowledge\nManagement (CIKM) , pp. 319–328, Beijing, China, 2019.\nWeijie Zhao, Shulong Tan, and Ping Li. Song: Approximate\nnearest neighbor search on gpu. In 35th IEEE Interna-\ntional Conference on Data Engineering (ICDE) , Dallas,\nTX, 2020.\nGuorui Zhou, Xiaoqiang Zhu, Chengru Song, Ying Fan,\nHan Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and\nKun Gai. Deep interest network for click-through rate\nprediction. In Proceedings of the 24th ACM SIGKDD\nInternational Conference on Knowledge Discovery &\nData Mining (KDD) , pp. 1059–1068, London, UK, 2018.\nZhixin Zhou, Shulong Tan, Zhaozhuo Xu, and Ping Li.\nMöbius transformation for fast inner product search on\ngraph. In Advances in Neural Information Processing\nSystems (NeurIPS) , pp. 8216–8227, Vancouver, Canada,\n2019.\n\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\n50, 56, 61\n61, 874, 61\n5, 5611, 87\n98mini-batch1\nmini-batch2\nworking: 4, 5, 11, 50, 53, 56, 61, 87, 98\nNode1:\n1, 3, 5, …, 97, 99Node2:\n2, 4, 6, …, 98, 100\n5, 11, 53, 61, 87 4, 50, 56, 98 MEM:pull local\nMEM-PS/SSD-PSpull remote\nMEM-PS\nGPU1: 4, 5, 11, 50 GPU2: 53, 56, 61, 87, 98 partition parameters\n11, 87\n98GPU1\nmini-batch111           87, 98pull local HBM-PS pull remote HBM-PS\nforward/backward \npropagation87\n4, 53mini-batch3\nmini-batch4\n:\nWorker1  SSD:\n HBM:\nFigure 6. An example for Algorithm 1.\nA H IERARCHICAL PARAMETER SERVER\nWORKFLOW EXAMPLE\nExample. Figure 6 depicts an example for the training work-\nﬂow (Algorithm 1). Consider now we are at node 1. An input\nbatch is streamed from HDFS and is divided into 4mini-\nbatches. The working parameters of the current batch are:\n4;5;11;50;53;56;61;87;98. Parameters are sharded and\nstored on the SSDs of each node. We have 2nodes and shard\nthe parameters in a round-robin method in this example—\nnode 1stores the parameters with odd keys while node 2\nstores the ones with even keys. Here we have 100total pa-\nrameters in this example—there are 1011parameters in real-\nworld large-scale deep learning models. 5;11;53;61;87are\nstored on the local node— node 1. We pull these parame-\nters from the local MEM-PS (for the cached parameters)\nand the local SSD-PS. For the parameters that stored on\nother nodes— 4;50;56;98, we pull these parameters from\nthe MEM-PS on node 2through the network. The MEM-\nPS on node 2interacts with its memory cache and its local\nSSD-PS to load the requested parameters. Now all the work-\ning parameters are retrieved and are stored in the memory\nofnode 1. Here we have 2GPUs on node 1. The working\nparameters are partitioned and transferred to GPU HBMs.\nIn this example, GPU 1obtains the parameters whose keys\nare less than or equal to 50—4;5;11;50, and GPU 2takes\n53;56;61;87;98. The partition strategy can be any hashing\nfunction that maps a parameter key to a GPU id. Consider\ntheworker 1ofGPU 1is responsible to process mini-batch 1.\nworker 1is required to load 11,87and98. Among them,11is stored in the HBM of the local GPU— GPU 1.87and\n98are pulled from GPU 2. Since the GPUs are connected\nwith high-speed interconnections—NVLink, the inter-GPU\ndata transfer has low-latency and high-bandwidth. After the\nparameters are ready in the working memory of worker 1,\nwe can perform the neural network forward and backward\npropagation operations to update the parameters. All the\nupdated parameters are synchronized among all GPUs on all\nnodes after each mini-batch is ﬁnished. When all the mini-\nbatches are ﬁnished, the MEM-PS on each node pulls back\nthe updated parameters and materializes them onto SSDs.\nB 4- STAGE PIPELINE EXAMPLE\nNetwork CPU SSD GPUNetwork CPU SSD GPU\nNetwork CPU SSD GPUNetwork CPU SSD\nNetwork CPU SSD GPUNetwork CPU\nNetwork CPU SSD GPUNetwork\nPull/push\nMEM-PSPartition\nparametersLoad/dump\nparametersTrainingTimeline\nFigure 7. The 4-stage pipeline.\nFigure 7 is an illustration of the 4-stage pipeline. For ex-\nample, when the GPUs are busy training the model, our\n4-stage pipeline enables the proposed system to prepare\nthe referenced parameters of the next training batch at the\nsame time: the HBM-PS pulls remote parameters from other\nnodes and the SSD-PS loads local parameters for the next\nbatch simultaneously. After the training of the current batch\nis ﬁnished, all the required parameters of the next batch are\nready to use in the GPU HBM—GPUs are able to train the\nnext batch immediately.\nC HBM-PS I MPLEMENTATION\nC.1 Multi-GPU Distributed Hash Table\nPartition policy. A partition policy that maps a parame-\nter key to a GPU id is required to partition the parameters.\nA simple modulo hash function yields a balanced parti-\ntioning in general cases, because the features of the input\ntraining data are usually distributed randomly. The modulo\nhash function can be computed efﬁciently with constant\nmemory space. As a trade-off of the memory footprint,\nthe disadvantage of the simple hash partition policy is that\nwe need to pull parameters from other GPUs if the param-\neters referenced in a mini-batch are not stored in the lo-\ncal parameter partition. One possible improvement is to\ngroup parameters with high co-occurrence together (Eisen-\nman et al., 2018), for example, pre-train a learned hash\n\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\nGPU\nHBMPCIe busCPU MEM\nNetwork Card GPU\nHBMPCIe busCPU MEM\nNetwork CardSender Receiver\nRoCEBaseline\nRDMA\nFigure 8. Inter-node RDMA communication.\nfunction (Kraska et al., 2018) to maximize the parameter\nco-occurrence. It is another research axis–vertical partition-\ning (Navathe et al., 1984; Zhao et al., 2015) that is beyond\nthe scope of this paper. Generally, no perfect balanced parti-\ntion solution exists for random inputs. Although the number\nof pulled parameters is reduced, we still have to pull param-\neters from almost all other GPUs even with an optimized\npartition policy. Besides, transferring a large batch of data\ncan better utilize the NVLink bandwidth—the disadvantage\nof the simple hash function partition policy is reduced.\nC.2 GPU RDMA Communication\nGPU Communication mechanism. The inter-node GPU\ncommunication mechanism is depicted in Figure 8. Two\nnodes are shown in the ﬁgure—the left node is the sender\nand the right one is the receiver. For each node, the CPU,\nGPU and network card are connected with a PCIe bus.\nWe ﬁrst examine the baseline method before we introduce\nourRDMA solution. In the ﬁgure, the data ﬂow of the base-\nline method is represented as the dashed line starting from\nthe sender HBM to the receiver HBM. The CPU calls the\nGPU driver to copy the data from GPU HBM into the CPU\nmemory. Then, the CPU reads the data in the memory and\ntransmits the data through the network. The transmitted\ndata are stored in the memory of the receiver node. Finally,\nthe receiver CPU transfers the in-memory data to the GPU\nHBM. In this baseline method, the CPU memory is uti-\nlized as a buffer to store the communication data—it incurs\nunnecessary data copies and CPU consumptions.\nOur RDMA hardware design eliminates the involvement\nof the CPU and memory. Its data ﬂow is represented as\na solid line in the ﬁgure. Remote Direct Memory Access\n(RDMA) (Potluri et al., 2013) enables zero-copy network\ncommunication—it allows the network card to transfer data\nfrom a device memory directly to another device memory\nwithout copying data between the device memory and the\ndata buffers in the operating system. The RDMA data trans-\nfer demands no CPU consumption or context switches. The\nnetwork protocol–RDMA over Converged Ethernet (RoCE)–is employed to allow RDMA over the Ethernet network. The\nsender GPU driver1directly streams the data in HBM to the\nnetwork, while the receiver network card directly stores the\ncollected data into the GPU HBM.\nGPU1GPU1GPU1GPU1Node1Node2Node3Node4\n… ...… ... … ... … ...1 12 2\nGPU8GPU8GPU8GPU81 12 2\n3 3 3 3\nFigure 9. All-reduce communication.\nC.3 Inter-Node GPU Communication\nAll-reduce communication. The parameter synchroniza-\ntion requires an all-reduce communication—each GPU\nneeds to receive all parameter updates from other GPUs\nand then performs a reduction to accumulate these updates.\nFigure 9 presents an example communication workﬂow. 4\nnodes are shown in this example. Each node contains 8\nGPUs. Initially, the GPUs on Node 1exchange their parame-\nter updates with their corresponding GPUs on Node 2(step\n1)—i.e., the ithGPU on Node 1communicates with the ith\nGPU on Node 2. Meanwhile, the GPUs with the same id on\nNode 3andNode 4share their data with each other. Then,\nthe GPUs on Node 1perform the communication with the\nones on Node 3(step 2). Likewise, the GPUs on Node 2\nandNode 4perform the same pattern communication in par-\nallel. After these two steps, each GPU on each node has\ncollected all the parameter updates of its corresponding\n1https://docs.nvidia.com/cuda/\ngpudirect-rdma/index.html\n\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\nGPUs on other nodes. An intra-node GPU tree all-reduce\ncommunication2is executed to share the data across all 8\nGPUs on the same node (step 3). Most of the communi-\ncations are paralleled— log2#nodes non-parallel inter-node\nandlog2#GPUs intra-node all-reduce communications are\nrequired to synchronize the parameters across all nodes.\nC.4 Dense Parameters\nAs we discussed in the CTR prediction neural network ex-\nample (Figure 1), besides the large-scale sparse parameters,\nthere are a small number of dense parameters for the fully-\nconnected layers. For any sparse input, all the dense parame-\nters are referenced and updated. Therefore, we can pin these\ndense parameters in the HBM of all GPUs at the beginning\nof the training for better training performance. In extreme\ncases—we have insufﬁcient HBM memory to replicate the\ndense parameters, we can shard the dense parameters as the\nsparse parameters and distribute them across all GPU HBMs.\nThe dense parameters are also synchronized as the sparse\nones in the HBM-PS after each mini-batch is processed.\nD MEM-PS I MPLEMENTATION\nCache policy. We target to cache the most recently and the\nmost frequently used parameters in the memory to reduce\nSSD I/Os. In this paper, we leverage a cache eviction policy\nthat combines two cache methods—Least Recently Used\n(LRU) (O’Neil et al., 1993) and Least Frequently Used\n(LFU) (Sokolinsky, 2004). Whenever we visit a parameter,\nwe add it into an LRU cache. For the evicted parameters\nfrom the LRU cache, we insert them into an LFU cache. The\nevicted parameters from the LFU cache are collected—we\nhave to ﬂush them into SSDs before releasing their memory.\nTo guarantee the data integrity of our pipeline, we pin the\nworking parameters of the current batch and the pre-fetched\nparameters of next iterations in the LRU cache—they cannot\nbe evicted from the memory until their batch is completed.\nE SSD-PS I MPLEMENTATION\nLoad parameters. The SSD-PS gathers requested param-\neter keys from the MEM-PS and looks up the parameter-\nto-ﬁle mapping to locate the parameter ﬁles to read. We\nhave to read an entire parameter ﬁle when it contains re-\nquested parameters—a larger ﬁle causes more unnecessary\nparameter readings. This is a trade-off between the SSD\nI/O bandwidth and the unnecessary parameter reading—a\nsmall-size ﬁle cannot fully utilize the SSD I/O bandwidth.\nWe tune the ﬁle size to obtain the optimal performance. Fig-\nure 10(a) depicts an example of parameter ﬁles on SSDs. In\n2https://docs.nvidia.com/deeplearning/\nsdk/nccl-developer-guide/docs/usage/\noperations.html#allreducethe example, each parameter ﬁle can store 3parameters.\nDump parameters. Parameters evicted from the HBM-PS\ncache are required to be dumped onto SSDs. It is impracti-\ncal to locate these parameters and perform in-place updates\ninside the original ﬁle—it poorly utilizes the SSD I/O band-\nwidth because it requires us to randomly write the disk.\nInstead, our SSD-PS chunks these updated parameters into\nﬁles and writes them as new ﬁles on SSDs—data are sequen-\ntially written onto the disk. After the ﬁles are written, we\nupdate the parameter-to-ﬁle mapping of these parameters.\nThe older versions of the parameters stored in the previous\nﬁles become stale—these older values will not be used since\nthe mapping is updated. In Figure 10(b), we present an ex-\nample for dumping parameters— 1;2;4;6;8;9are updated\nand dumped to SSD-PS. We chunked them into two ﬁles\nand write these two ﬁles onto the SSD— ﬁle4andﬁle5. The\nunderlined values–the values of the updated parameters in\nthe old ﬁles–are stale.\nFile compaction. The SSD usage hikes as we keep cre-\nating new ﬁles on SSDs. A ﬁle compaction operation is\nperformed regularly to reduce the disk usage—many old\nﬁles containing a large proportion of stale values can be\nmerged into new ﬁles. We adopt the leveled compaction\nalgorithm of LevelDB3to create a lightweight ﬁle merging\nstrategy. A worker thread runs in the background to check\nthe disk usage. When the usage reaches a pre-set thresh-\nold, the SSD-PS scans the old parameter ﬁles, collects the\nnon-stale parameters, merges them into new ﬁles, and erases\nthe old ﬁles. The parameter-to-ﬁle mapping of the merged\nparameters is also updated in the ﬁle compaction operation.\nFigure 10(c) illustrates the ﬁle compaction effects. Before\nthe compaction (Figure 10(b)), the stale values in ﬁle1,ﬁle2\nandﬁle3occupy more than a half of the ﬁle capacity. We\nscan these ﬁles, merge the non-stale values into a new ﬁle\n(ﬁle6), and erase these ﬁles ( ﬁle1andﬁle2). The compaction\noperation may merge a large number of ﬁles on SSDs. In\norder to reduce the excessive merging, we set a threshold\nto limit the number of merged ﬁles—we only merge ﬁles\nthat contain more than 50% stale parameters. By employing\nthis threshold, we can limit the total SSD space usage—the\nsize of all parameter ﬁles will not exceed 2times ( 1=50%)\nof the original non-stale parameter size. Note that we do\nnot need to read the entire ﬁle to obtain the proportion of\nstale parameters—a counter that counts the number of stale\nparameters is maintained as an auxiliary attribute for each\nﬁle. When we update the parameter-to-ﬁle mapping, we\naccumulate the counter of the old ﬁle it previously maps to.\nF A DDITIONAL RELATED WORK\nIn-memory cache management. Many caching policies\nhave been developed for storage systems, such as the LRU-\n3https://github.com/google/leveldb\n\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\n1, 2, 34, 5, 67, 8, 9123456789\n111222333\n1, 2, 34, 5, 67, 8, 91, 2, 4123456789\n441425355\n3, 5, 7123456789\n775522365\nfil\u0002e1fil\u0002e2fil\u0002e3fil\u0002e4fil\u0002e5fil\u0002e4fil\u0002e5fil\u0002e6parameter id\nSSD fil\u0017es6, 8, 9\nfil\u0002e1fil\u0002e2fil\u0002e31, 2, 46, 8, 9\n(a) (b) (c)fil\u0017e id\nFigure 10. SSD-PS examples: (a) parameter-to-ﬁle mapping and parameter ﬁles; (b) 1;2;4;8;9are updated; (c) a compaction operation.\nK (O’Neil et al., 1993), DBMIN (Chou & DeWitt, 1986),\nLRFU(Lee et al., 2001), and Semantic Caching (Dar et al.,\n1996). These algorithms evict cache according to a com-\nbined weight of recently used time-stamp and frequency.\nIn the web context, there is extensive work developed for\nvariable-size objects. Some of the most well-known algo-\nrithms in this space are Lowest-Latency-First(Wooster &\nAbrams, 1997), LRU-Threshold (Abrams et al., 1996), and\nGreedy-Dual-Size(Cao & Irani, 1997). Unlike our caching\nproblem, the parameter we tackle with has a ﬁxed size and a\nclear access pattern in our CTR prediction model training—\nsome parameters are frequently referenced. It is effective\nto keep those “hot parameters” in the cache by applying an\nLFU eviction policy. While our additional LRU linked list\nmaintains the parameters referenced in the current pass to\naccelerate the hash table probing.\nKey-value store for SSDs. There is a signiﬁcant amount\nof work on key-value stores for SSD devices. The majordesigns (Andersen et al., 2009; Lim et al., 2011) follow\nthe paradigm that maintains an in-memory hash table and\nconstructs an append-only LSM-tree-like data structure on\nthe SSD for updates. FlashStore (Debnath et al., 2010)\noptimize the hash function for the in-memory index to com-\npact key memory footprints. SkimpyStash (Debnath et al.,\n2011) moves the key-value pointers in the hash table onto\nthe SSD. BufferHash (Anand et al., 2010) builds multiple\nhash tables with Bloom ﬁlters for hash table selection. Wis-\ncKey (Lu et al., 2017) separates keys and values to minimize\nread/write ampliﬁcations. Our SSD-PS design follows the\nmainstream paradigm, while it is specialized for our training\nproblem. We do not need to confront the challenges to store\ngeneral keys and values. The keys are the index of parame-\nters that distributes uniformly. It is unnecessary to employ\nany sophisticated hashing functions. Also, the values have\na known ﬁxed length, the serialized bucket on SSD exactly\nﬁts in an SSD block—I/O ampliﬁcation is minimized.",
  "textLength": 80008
}