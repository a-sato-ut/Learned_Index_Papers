{
  "paperId": "2a38aeccad784c9b0b87019fefaa2accdc052d5b",
  "title": "Learning State Representations for Query Optimization with Deep Reinforcement Learning",
  "pdfPath": "2a38aeccad784c9b0b87019fefaa2accdc052d5b.pdf",
  "text": "Learning State Representations for Query Optimization with\nDeep Reinforcement Learning\nJennifer Ortiz†, Magdalena Balazinska†, Johannes Gehrke‡, S. Sathiya Keerthi+\nUniversity of Washington†, Microsoft‡, Criteo Research+\nABSTRACT\nDeep reinforcement learning is quickly changing the field of arti-\nficial intelligence. These models are able to capture a high level\nunderstanding of their environment, enabling them to learn difficult\ndynamic tasks in a variety of domains. In the database field, query\noptimization remains a difficult problem. Our goal in this work is to\nexplore the capabilities of deep reinforcement learning in the context\nof query optimization. At each state, we build queries incrementally\nand encode properties of subqueries through a learned representa-\ntion. The challenge here lies in the formation of the state transition\nfunction, which defines how the current subquery state combines\nwith the next query operation (action) to yield the next state. As a\nfirst step in this direction, we focus the state representation problem\nand the formation of the state transition function. We describe our\napproach and show preliminary results. We further discuss how we\ncan use the state representation to improve query optimization using\nreinforcement learning.\n1 INTRODUCTION\nQuery optimization is not a solved problem, and existing database\nmanagement systems (DBMSs) still choose poor execution plans\nfor some queries [ 8]. Because query optimization must be efficient\nin time and resources, existing DBMSs implement a key step of\ncardinality estimation by making simplifying assumptions about\nthe data (e.g., inclusion principle, uniformity or independence as-\nsumptions) [ 5,8]. Additionally, while research papers have shown\ntheir benefits, optimizers shy away from using multidimensional\nhistograms and sampling due to the increased overhead and com-\nplexity they bring [ 3,20]. As a result, in data sets with correlations\nand non-uniform data distributions, cardinality estimation errors are\nfrequent, leading to sub-optimal plan selections [7].\nRecently, thanks to dropping hardware costs and growing datasets\navailable for training, deep learning has successfully been applied\nto solving computationally intensive learning tasks in other domains.\nThe advantage of these type of models comes from their ability to\nlearn unique patterns and features of the data that are difficult to\nmanually find or design [4].\nIn this paper, we explore the idea of training a deep learning\nmodel to predict query cardinalities. Instead of relying entirely on\nbasic statistics to estimate costs, we train a model to automatically\nlearn important properties of the data to more accurately infer these\nestimates. This representation can result in better predictions than\nusing hand-designed feature selection [ 4]. They can automatically\nlearn to retain distinguishing properties of the data [ 9], which in turn\ncan help estimate cardinalities. As of today, there are few studies\nState t+1State tRepresentation\tof\tDatabase\tPropertiesQuery\toperation\tat\ttime\ttSubqueryRepresentation⋈Statet+2⋈⋈Query\toperation\tat\ttime\tt+1State\tTransition\tFunction\"\"#$SubqueryRepresentationAction tAction t+1\nℎ\"#\"ℎ\"$%%&'%&(&)*+*,-)\t+*\t*,/&\t*%&'%&(&)*+*,-)\t+*\t*,/&\t*+1+0*,-)+*\t*,/&\t*Figure 1: State Representation: Given a database and a query,\nwe use a neural network to generate subquery representation\nfor each state. This representation can serve for cardinality esti-\nmation and more importantly, for building optimal query plans\nusing reinforcement learning.\nthat have used deep learning techniques to solve database problems,\nalthough some have started to raise awareness for the potential\nof this method in our field [ 19]. Now is the time to explore this\nspace, since we have the computational capabilities to run these\nmodels. A key challenge of this approach is how to represent the\nqueries and data in the model. If we build a model that takes as\ninput a feature vector representing a complex query on a database,\nthis model would essentially need to learn to predict cardinalities\nforallpossible queries that could run on the data. Such a model\nwould be complex and would require impractical numbers of training\nexamples.\nTo address the above problem, as a first contribution (Section 3),\nwe develop an approach that trains a deep learning model that learns\ntoincrementally generate a succinct representation of each sub-\nquery’s intermediate results: The model takes as input a subquery\nand a new operation to predict the resulting subquery’s properties.\nThese properties can serve to derive the subquery’s cardinality.\nAs a second contribution (Section 4), we present our initial ap-\nproach to using these representation to improve query plan enumer-\nation through reinforcement learning. Reinforcement learning is a\ngeneral purpose framework used for decision-making in contexts\nwhere a system must make step-by-step decisions to reach an ulti-\nmate goal and collect a reward. In our case, we propose to use thisarXiv:1803.08604v1  [cs.DB]  22 Mar 2018\n\nJennifer Ortiz†, Magdalena Balazinska†, Johannes Gehrke‡, S. Sathiya Keerthi+\napproach to incrementally build a query plan by modeling it as a\nMarkov process, where each decision is based on the properties of\neach state.\nIn Figure 1, we show an example that forms the basis of our deep\nreinforcement learning approach. Given a query and a database, the\nmodel incrementally executes a query plan through a series of state\ntransitions. In the initial state tin the figure, the system begins with a\nrepresentation of the entire database. Given an action selected using\nreinforcement learning, the model transitions to a new state at t+1.\nEach action represents a query operation and each state captures\na representation of the subquery’s intermediate results. We train a\nstate transition function (a neural network), N NST, to generate this\nrepresentation. N NSTis a recursive function that takes as input a\nprevious subquery representation as well as an action at time t, to\nproduce the subquery representation for time t+1.\nLet us now motivate the setup that is laid out in Figure 1. Consider\nthe dynamics of a query plan that is executed one operation (action)\nat a time. At any stage tof the query plan execution, let’s say a\nsubquery has been executed; let ht, the state at tbe represented by\nann-dimensional real vector. Applying the next action, atto this\ncurrent database state leads to the next state, ht+1. The mapping,\nN NST:(ht,at)→ht+1is called the state transition function.\nIn most applications of reinforcement learning, the state as well as\nthe state transition function are known. For example, in the game of\nGo, each possible board position is a state and the process of moving\nfrom one board position to the next (the transition) is well-defined.\nUnfortunately, in the case of query plan enumeration, we cannot\neasily anticipate the state. The crux of our approach is to represent\neach state by using a finite dimensional real vector and learn the\nstate transition function using a deep learning model. To guide the\ntraining process for this network, we use input signals and context\ndefined from observed variables that are intimately associated with\nthe state of the database. For example, throughout this work, we\nuse the cardinality of each subquery at any stage of the plan as an\nobserved variable. If a state htis represented succinctly with the right\namount of information, then we should be able to learn a function,\nN Nobser ved, which maps this state to predicted cardinalities at stage\nt. We show both N NSTandN Nobser vedin Figure 2.\nAs we train this model, the parameters of the networks will adjust\naccordingly based on longer sequences of query operations. With this\nmodel, each state will learn to accurately capture a representation.\nOnce trained, we can fix this model and apply reinforcement learning\nto design an optimal action policy, leading to optimal query plans.\nBefore describing our approach in more detail, we first intro-\nduce fundamental concepts about deep learning and reinforcement\nlearning in the following section.\n2 BACKGROUND\nDeep Learning Deep learning models, also known as feedforward\nneural networks, are able to approximate a non-linear function, f[4].\nThese networks define a mapping from an input xto an output y,\nthrough a set of learned parameters across several layers, θ. During\ntraining, the behavior of the inner layers are not defined by the\ninput data, instead these models must learn how to use the layers\nto produce the correct output. Since there is no direct interaction\nℎ\"#\"ℎ\"$%Observed\tVariables…22342256789:8;Figure 2: Learning the State Transition Function N NST: Given\nany htand at, we can extract a representation of a sub-\nquery through N NST. We train the function N NSTby predict-\ning properties from a set of observed variables. The function\nN NObser veddefines the mapping between the hidden state and\nthese observed variables.\nbetween the layers and the input training data, these layers are called\nhidden layers [4].\nThese feedforward networks are critical in the context of repre-\nsentation learning. While training to meet some objective function, a\nneural network’s hidden layers can indirectly learn a representation,\nwhich could then be used for other tasks [ 4]. In representation learn-\ning, there is a trade-off between preserving as much information as\npossible and learning useful properties about the data. Depending on\nthe output of the network, the context of these representations can\nvary. It is up to the user to provide the network with enough hints\nand prior beliefs about the data to help guide the learning [4].\nReinforcement Learning Reinforcement learning models are\nable to map scenarios to appropriate actions, with the goal of maxi-\nmizing a cumulative reward. Unlike supervised learning, the learner\n(theaдent) is not explicitly shown which action is best. Instead the\nagent must discover the best action through trial and error by either\nexploiting current knowledge or exploring unknown states [ 16]. At\neach timestep, t, the agent will observe a state of the environment,\nstand will select an action, at. The action the agent selects depends\non the policy, π. This policy can reenact several types of behaviors.\nAs an example, it can either act greedily or balance between explo-\nration and exploitation through an ϵ-greedy (or better) approach.\nThe policy is driven by the expected rewards of each state, which the\nmodel must learn. Given the action selected, the model will arrive\nat a new state, st+1. At each step, the environment sends the agent\na reward, rt+1, which signals the “goodness” of the action selected.\nThe agent’s goal is to maximize this total reward [ 16]. One approach\nis to use a value-based iteration technique, where the model records\nstate-action values, QL(s,a). These values specify the long-term de-\nsirability of the state by taking into account the rewards for the states\nthat are likely to follow [16].\n3 LEARNING A QUERY REPRESENTATION\nGiven as input a database Dand a query Q, the first component\nof our approach is to apply deep learning to derive compact, yet\ninformative representations of queries and the relations they produce.\nTo ensure that these representations are informative, we focus on\ntraining these representations to predict subquery cardinalities.\n\nLearning State Representations for Query Optimization with Deep Reinforcement Learning\n!\"#\"ℎ%\n&&'()*#%ℎ+\n&&,-&&./012314\nFigure 3: Combined Models N NinitandN NST\n3.1 Approach\nThere are two approaches that we could take. In the first approach,\nwe could transform (Q,D)into a feature vector and train a deep\nnetwork to take such vectors as input and output a cardinality value.\nAs discussed in the introduction, the problem with this approach\nis that the size of the feature vector would have to grow with the\ncomplexity of databases and queries. This would result in very long,\nsparse vectors, which would require large training datasets.\nInstead, we take a different approach, a recursive approach: We\ntrain a model to predict the cardinality of a query consisting of a\nsingle relational operation applied to a subquery as illustrated in\nFigure 2. This model takes as input a pair (ht,at), where htis a\nvector representation of a subquery, while atisa single relational\noperation on ht. Importantly, htis not a manually specified feature\nvector, but it is the latent representation that the model learns itself.\nTheN NSTfunction generates these representations by adjusting the\nweights based on feedback from the N NObser vedfunction. This\nN NObser vedfunction learns to map a subquery representation to\npredict a set of observed variables. As we train this model, we use\nback propagation to adjust the weights for both functions. In this\nwork, we only focus on predicting cardinalities, but we could extend\nthe model to learn representations that enable us to capture additional\nproperties such as more detailed value distributions or features about\nquery execution plans, such as their memory footprint or runtimes.\nBefore using the recursive N NSTmodel, we must learn an ad-\nditional function, N Ninit, as shown in Figure 3. N Ninittakes as\ninput ( x0,a0), where x0is a vector that captures the properties of\nthe database Danda0is a single relational operator. The model\noutputs the cardinality of the subquery that executes the operation\nencoded in a0onD. We define the vector, x0to represent simple\nproperties of the database, D. The list of properties we provide next\nis not definitive and more features can certainly be added. Currently,\nfor each attribute in the dataset D, we use the following features\nto define x0: the min value, the max value, the number of distinct\nvalues, and a representation of a one dimensional histogram.\nAs shown in the figure, we then include the recursive model,\nN NST, that takes(ht,at)as input and predicts the observed variables\nof the subqueries as well as the representation, ht+1of the new\nsubquery. We combine these models to train them together. During\n(a) Predicting Cardinality ( m=3)\n(b) Predicting Cardinality ( m=5)\nFigure 4: Learning h1for Selection Query\n(a) Cardinality Predictions for h1\n(b) Cardinality Predictions for h2\nFigure 5: Learning Cardinalities on the Combined Model\ntraining, the weights are adjusted based on the combined loss from\nobserved variable predictions. Essentially, we want to learn an h1\nrepresentation that captures not only enough information to predict\nthe cardinality of that subquery directly but of other subqueries built\nby extending it.\n3.2 Preliminary Results\nWe use the publicly available Internet Movie Data Base (IMDB)\ndata set from the Join Order Benchmark (JOB) [ 8]. Unlike TPC-\nH and TPC-DS, the IMDB data set is real and includes skew and\ncorrelations across columns [ 8]. In our experiments, we use Python\nTensorflow to implement our approach [1].\nTraining N Ninit: As a first experiment, we initialize x0with\nproperties of the IMDB dataset and train N Ninit to learn h1.a0\nrepresents a conjunctive selection operation over mattributes from\ntheaka_title relation. We generate 20k queries, where 15k are used\nfor training the model and the rest are used for testing. N Ninit\ncontains 50 hidden nodes in the hidden layer. We update the model\nvia stochastic gradient descent with a loss based on relative error\nand a learning rate of .01.\nIn Figure 4a, we show the cardinality estimation results for se-\nlection queries where m=3. On the x-axis, we show the number\nof epochs used during training and on the y-axis we show the rela-\ntive error with the error bars representing the standard deviation. We\ncompare our approach N N Model to estimates from SQL Server [ 13].\nWe use a commercial engine to ensure a strong baseline. With fewer\nepochs (less training) the N Ninit’s cardinality predictions result in\nsignificant errors, but at the 6th epoch, the model performs similarly\nto SQL Server and then it starts to outperform the latter.\nIn Figure 4b, we increase the number of columns in the selection\ntom=5. In general, we have observed that N Ninittakes longer\nto converge once more columns are introduced. This is expected,\n\nJennifer Ortiz†, Magdalena Balazinska†, Johannes Gehrke‡, S. Sathiya Keerthi+\nasN Ninitmust learn about more joint distributions across more\ncolumns. Nevertheless, the model still manages to improve on SQL\nServer’s estimations by the 9th epoch.\nTraining N Ninit and N NST: In the previous experiment, we\nonly trained the N Ninitmodel for selection queries over base data.\nFor this next experiment, we predict the cardinality of a query con-\ntaining both a selection and join operation by using the combined\nmodel. Here, a0represents the selection, while the subsequent ac-\ntiona1represents the join. Through this combined model, we can\nensure that h1(the hidden state for the selection) captures enough\ninformation to be able to predict the cardinality after the join. In\nFigure 5, we show the cardinality prediction for h1andh2. In these\nscatter plots, the x-axis shows the real cardinality, while the y-axis\nshows the predicted cardinality from the model. Although there is\nsome variance, h1was able to hold enough information about the\nunderlying data to make reasonable predictions for h2.\n4 QUERY PLAN ENUMERATION WITH\nREINFORCEMENT LEARNING\nIn this section, we present and discuss our design to leverage the\nsubquery representations from the section above, not only to estimate\ncardinalities, but to build query plans. Given a query, Q, we seek to\nidentify a good query plan by combining our query representations\nfrom N NSTwith reinforcement learning.\nWe assume a model-free environment, where transition probabili-\nties between states are not known. At s0, the model only knows about\nDandQ. At this initial state, no query operations have yet taken\nplace. The reinforcement learning agent transitions to a new state\nby selecting an operation from query Q. At each state, we encode\nan additional contextual vector, ut, which expresses the operations\nthat remain to be done for Q. We now describe how to initialize the\nvector u0at time 0:\nGiven database D, we have a set of nrelationsR={rel1, ...,reln},\nwhere each relicontains a set of mattributes{atti0, ...,attim}. The\nvector utrepresents a fixed set of equi-join predicates and one-\ndimensional selection predicates, C={c1, ...cp}. We set the i-th\ncoordinate inCaccordingly if the corresponding predicate exists in\nthe query Q. For example, c1could represent the following equi-join\npredicate, rel1.att10=rel2.att23. If this predicate exists in Qwe\nencode it in utby updating the value of c1to 1, otherwise we set it to\n0. For selections, we track one-dimensional ranged selections of the\nfollowing form: reli.attij<=v. For now, we allow each attribute to\nhave at most one ranged filter in Q. If the selection predicate exists\ninQ, we place the value vin the corresponding element in C. Once\nthe reinforcement agent selects an action (query operation), at, we\ncan update utby setting the corresponding element to 0.\nTo select good query plans, we need to provide the model with a\nreward. That reward must either be given at each state or once the\nentire query plan has been constructed. We have different options.\nOne option is to use the negative cost estimate (computed by the\nunderlying, traditional query optimizer), as the reward for the plan.\nThe system would then learn to mimic that traditional optimizer\nwith the caveat that we currently build only left-deep plans. A better\noption that we are currently experimenting with is to use the negative\nof our system’s cardinality estimates at each step. The limitation,\nof course, is that this approach only optimizes logical query plans.We plan to extend the reward function in future work to also capture\nphysical query plan properties. In particular, one approach is to use\nthe negative of the query execution time as the reward.\nUltimately, the goal of the agent is to discover an optimal policy,\nπ∗. The policy determines which action the agent will take given\nthe state. As the agent explores the states, the model can update\nthe state-action values, the function QL(s,a), through Q-learning.\nQ-learning is an off-policy algorithm, where it uses two different\npolicies to converge the state-action values [ 14,16,17]. One is a\nbehavior policy which determines which actions to select next. In\npractice, this is usually an ϵ-greedy policy [ 12,14], but other policies\ncan be used as well. The other is the target policy, usually a greedy\nstrategy, which determines how values should be updated.\nInitially, all state-action pairs are random values. At each timestep,\nthe agent selects an action (usually based on an ϵ-greedy policy) and\nobserves the reward, rt+1at state st+1. As the agent explores the\nsearch space, these state-action pairs will converge to represent the\nexpected reward of the states in future timesteps.\nAt each state transition, each QL(s,a)is updated as follows:\nQL(st,at)←QL(st,at)+α[rt+1+γmax a′QL(st+1,a′)−QL(st,at)]\n(1)\nWhere the max a′QL(st+1,a′)represents the maximum value from\nst+1given the target greedy policy. We compute the subsequent state\ngiven the state transition function, N NST.\nOpen Problems: Many open problems remain for the above de-\nsign. As we indicated above, the first open problem is the choice\nof reward function and its impact on query plan selection. Another\nopen problem is that the state-space is large even when we only\nconsider selections and join operators as possible actions. Thus, the\nQ-learning algorithm as initially described is impractical as the state-\naction values are estimated separately for each unique subquery [ 12].\nIn other words, for each query that we train, it is unlikely that we will\nrun into the same exact series of states for a separate query. Thus, a\nbetter approach is to consider approximate solutions to find values\nforQL(s,a). We can learn a function, ˆQL(s,a,w)to approximate\nQL(s,a)given parameter weights w. This allows the model to gener-\nalize the value of a state-action pairs given previous experience with\ndifferent (but similar) states. This function could either represent a\nlinear function or even a neural network [14].\n5 RELATED WORK\nTo correct optimizer errors, previous work has used adaptive query\nprocessing techniques. Eddies [ 2] gets rid of the optimizer altogether\nand instead of building query plan trees, uses an eddy to determine\nthe sequence of operators based on a policy. Tzoumas et al. [18]\ntook this a step further and transformed it into a reinforcement\nlearning problem where each state represents a tuple along with\nmetadata about which operators still need to be applied and each\naction represents which operator to run next.\nLeo [15], was one of the first approaches to automatically adjust\nan optimizer’s estimates. It introduces a feedback loop, allowing the\noptimizer to learn from past mistakes. This requires successive runs\nof similar queries to make adjustments. Liu et al. [10] uses neural\nnetworks to solve the cardinality estimation problem, but primarily\nfocuses on cardinality predictions for selection queries only. Work\nby Kraska et al. [6] uses a mixture of neural networks to learn the\n\nLearning State Representations for Query Optimization with Deep Reinforcement Learning\ndistribution of an attribute to build fast indexes. Instead, our goal is to\nlearn the correlation across several columns and to build query plans.\nMore recently, work by Marcus et al. [11] uses a deep reinforcement\nlearning technique to determine join order for a fixed database. Each\nstate also represents a subquery, but our approach models each state\nas a latent vector that is learned through a neural network and is\npropagated to other subsequent states. Their approach uses a policy\ngradient to determine the best action, while our technique proposes\nto use a value-based iteration approach.\n6 CONCLUSION\nIn this work, we described a model that uses deep reinforcement\nlearning for query optimization. By encoding basic information\nabout the data, we use deep neural networks to incrementally learn\nstate representations of subqueries. As future work, we propose to\nuse these state representations in conjunction with a reinforcement\nlearning model to learn optimal plans.\nAcknowledgements This project was supported in part by NSF\ngrants IIS-1247469 and Teradata.\nREFERENCES\n[1]Martín Abadi et al. TensorFlow: Large-scale machine learning on heterogeneous\nsystems, 2015. Software available from tensorflow.org.\n[2]Ron Avnur et al. Eddies: Continuously adaptive query processing. SIGMOD\nRecord , 2000.\n[3]Todd Eavis et al. Rk-hist: An R-tree based histogram for multi-dimensional\nselectivity estimation. In CIKM ’07 .\n[4]Ian Goodfellow et al. Deep Learning . MIT Press, 2016. http://www.\ndeeplearningbook.org.\n[5]Martin Kiefer et al. Estimating join selectivities using bandwidth-optimized kernel\ndensity models. VLDB , 2017.\n[6] Tim Kraska et al. The case for learned index structures. CoRR , 2017.\n[7]Viktor Leis et al. Cardinality estimation done right: Index-based join sampling. In\nCIDR 2017 .\n[8]Viktor Leis et al. How good are query optimizers, really? Proc. VLDB Endow. ,\n2015.\n[9]Timothée Lesort et al. State representation learning for control: An overview.\nCoRR , 2018.\n[10] Henry Liu et al. Cardinality estimation using neural networks. In CASCON 2015 .\n[11] Ryan Marcus et al. Deep reinforcement learning for join order enumeration. CoRR ,\n2018.\n[12] V olodymyr Mnih et al. Human-level control through deep reinforcement learning.\nNature , 2015.\n[13] SQL Server. https://www.microsoft.com/en-us/sql-server/sql-server-2017.\n[14] David Silver. UCL Course on Reinforcement Learning, 2015.\n[15] Michael Stillger et al. Leo - db2’s learning optimizer. In VLDB 2001 .\n[16] Richard S. Sutton et al. Reinforcement learning I: Introduction, 2016.\n[17] Csaba Szepesvari. Algorithms for reinforcement learning. Morgan and Claypool\nPublishers, 2009.\n[18] Kostas Tzoumas et al. A reinforcement learning approach for adaptive query\nprocessing. In A DB Technical Report , 2008.\n[19] Wei Wang et al. Database Meets Deep Learning: Challenges and Opportunities.\nSIGMOD Record , 2016.\n[20] Wentao Wu et al. Sampling-based query re-optimization. SIGMOD 2016.",
  "textLength": 26278
}