{
  "paperId": "441dbad0518b0786508f02fed66a1c0be1588682",
  "title": "NeurDB: On the Design and Implementation of an AI-powered Autonomous Database",
  "pdfPath": "441dbad0518b0786508f02fed66a1c0be1588682.pdf",
  "text": "NeurDB: On the Design and Implementation of an AI-powered\nAutonomous Database\nZhanhao Zhao1, Shaofeng Cai1, Haotian Gao1, Hexiang Pan1, Siqi Xiang1, Naili Xing1,\nGang Chen2, Beng Chin Ooi1, Yanyan Shen3, Yuncheng Wu4, Meihui Zhang5\n1National University of Singapore2Zhejiang University3Shanghai Jiao Tong University\n4Renmin University of China5Beijing Institute of Technology\n{zhzhao,shaofeng,gaohaotian,xiangsq,xingnl,ooibc}@comp.nus.edu.sg,panh@u.nus.edu\ncg@zju.edu.cn,shenyy@sjtu.edu.cn,wuyuncheng@ruc.edu.cn,meihui_zhang@bit.edu.cn\nABSTRACT\nDatabases are increasingly embracing AI to provide intelligent in-\ndatabase analytics and autonomous system optimization, aiming to\nrelieve end-user burdens across various industry sectors. Nonethe-\nless, most existing approaches fail to account for the dynamic na-\nture of databases, which renders them ineffective for real-world\napplications characterized by evolving data and workloads. This\npaper introduces NeurDB , an AI-powered autonomous database\nthat deepens the fusion of AI and databases with adaptability to\ndata and workload drift. NeurDB establishes a new in-database AI\necosystem that seamlessly integrates AI workflows within the data-\nbase. This integration enables efficient and effective in-database AI\nanalytics and fast-adaptive learned database components. Empirical\nevaluations demonstrate that NeurDB substantially outperforms\nexisting solutions in managing AI analytics tasks, with the proposed\nlearned components more effectively handling data and workload\ndrift than state-of-the-art approaches.\n1 INTRODUCTION\nDatabase management systems (DBMSs) are becoming more intelli-\ngent and autonomous by embracing artificial intelligence (AI), cater-\ning to modern data- and AI-centric applications. AI augments DBMS\nfunctionality to support in-database AI analytics [ 14,30,45,49],\nenabling complex and advanced analytics tasks such as disease pro-\ngression predictions and user purchase recommendations. In paral-\nlel, AI empowers DBMSs to achieve autonomous DBMS optimiza-\ntion, driving a broad range of learned database designs that enhance\nsystem performance with minimal human intervention, includ-\ning learned knob tuners [ 34], learned system drivers [ 21,55], and\nlearned database components [ 15] such as query optimizers [ 24,54],\nindexes [16], and concurrency control [44], etc.\nThe aspiration of integrating DBMSs with AI was first expressed\nforty years ago [ 2], which has been periodically revisited with evolv-\ning technology [ 5,37,46]. With the advancements in both AI and\nDBMSs, considerable progress has been made in deepening their\nfusion. Unfortunately, a huge gap remains between the potential of\nthis integration and its current state of usability, largely due to the\ninherent differences in their paradigms [ 28]. The dynamic nature of\ndatabases, characterized by data and workload drift , poses a funda-\nmental challenge [ 17,18,48]. For example, suppose an e-commerce\ndatabase stores user and product information. While the database\ncontinuously evolves due to user and merchant activities in normal\nperiods, it would experience a sharp increase in workloads during\nflash sales, where huge volumes of updates from sales transactions\nare processed. In contrast, AI models typically derive intelligencefrom static datasets and thus can become outdated quickly in the\nface of database dynamism. For instance, models for user purchase\nrecommendations typically acquire knowledge by training on spe-\ncific user behaviors and product information. However, when the\ndata drifts due to transactional updates, models relying on past\nknowledge may produce inaccurate recommendations since they\nare not updated to reflect new conditions. Similarly, learned query\noptimizers trained based on historical system environments includ-\ning data distributions and workload patterns can struggle to deliver\neffective query plans as continuous data and workload drift.\nAdapting to data and workload drift has gained traction for both\nin-database AI analytics and autonomous DBMS optimization. Ex-\nisting databases supporting in-database AI analytics [ 26,36,40]\noften recommend users to completely retrain models with new data\nand workloads when the models become outdated. This retraining\nprocess is typically performed manually outside the database, and\ntherefore, it may complicate the AI analytics workflow and becomes\ninefficient when dealing with continuous data and workload drift.\nTo tackle this problem, in the context of autonomous DBMS opti-\nmization, recent works [ 18,19,23,48] have started addressing the\nadaptability issues by automatically detecting drift and triggering\nmodel retraining. However, these approaches are often tailored to\ncertain components. For example, approaches [ 18,48] that allow\nlearned query optimizers to withstand workload pattern changes\ncan be ineffective for learned indexes requiring rapid adaptation\nto data distribution drift. Consequently, a paradigm that robustly\nhandles data and workload drift for various learned database com-\nponents remains absent.\nWe envision a deeper integration of AI and DBMSs for enhanced\nadaptability to both data and workload drift. Unlike existing ap-\nproaches that merely overlay AI onto DBMSs or selectively en-\nhance certain system components, we aim for a comprehensive\nfusion of AI with DBMSs that enables continuous adaptation to\ndrift. It promises a cohesive AI-powered DBMS that provides ad-\nvanced in-database AI analytics and deeply integrates AI into all\nits key components. However, achieving this level of integration\npresents significant challenges. First, the integration necessitates a\nfundamental redesign of the entire AI workflow, including train-\ning, inference, and fine-tuning, within the database architecture.\nSecond, models for AI analytics and learned database components\nmust swiftly adapt to data and workload drift without losing effec-\ntiveness and efficiency. Therefore, developing such an AI-powered\ndatabase requires both a system foundation that seamlessly sup-\nports model adaptation and AI models that are inherently adaptive\nin structure.\n1arXiv:2408.03013v2  [cs.DB]  5 Jan 2025\n\nZhanhao Zhao et al.\nIn this paper, we present NeurDB , an AI-powered autonomous\nDBMS that provides efficient and effective in-database AI analytics\nto seamlessly support modern AI-powered applications, and fully\nembraces AI techniques in each major system component to offer\nautonomous system optimization. At the core of NeurDB is an in-\ndatabase AI ecosystem that deeply integrate the AI workflow into\nthe database. In this ecosystem, we develop multiple in-database AI\noperators, such as model training, inference, and fine-tuning, along\nwith an in-database AI engine to handle the execution of these oper-\nators. NeurDB can then directly support in-database AI analytics by\ncalling these AI operators. To further simplify end-users in submit-\nting their AI analytics tasks to NeurDB , we provide a user-friendly\ninterface by extending SQL with PREDICT syntax. Moreover, we\npropose two techniques to optimize the AI ecosystem performance.\nFirst, we devise a data streaming protocol that significantly reduces\ndata transfer overhead, yielding better performance for AI oper-\nations. Second, we develop an incremental update technique to\nminimize the fine-tuning cost, facilitating the fast adaptation of AI\nmodels. Based on the in-database AI ecosystem, we enable two effi-\ncient learned database components, namely a learned concurrency\ncontrol algorithm and a learned query optimizer, that can adapt\nquickly to data and workload drift.\nThe remainder of the paper is structured as follows. The next\nsection provides the necessary preliminaries and further clari-\nfies the motivation. Section 3 describes the system overview of\nNeurDB , and Section 4 details the key system design, including\nthe in-database AI ecosystem and fast-adaptive learned database\ncomponents. Section 5 presents the experimental results. Section 6\ndiscusses the related works before Section 7 concludes.\n2 PRELIMINARIES AND MOTIVATION\nIn this section, we introduce the preliminaries of AI and DBMS\nintegration, outline the design goals of NeurDB , and present the\nenhanced SQL syntax designed for in-database AI analytics.\n2.1 AI and DBMS Integration\nThe integration of AI and DBMSs can offer each other mutual ben-\nefits [ 46], and building on this intuition, various research works on\nAI and DBMS integration have been proposed, which can be broadly\ncategorized into two areas: in-database analytics and autonomous\nsystem optimization. Modern databases, such as Oracle [ 31], Mi-\ncrosoft Azure SQL [ 40], Amazon Redshift [ 38], and Google Big-\nQuery [ 1], support in-database AI analytics, and allow AI tasks to\nbe performed directly through user-friendly SQL interfaces. For\nbetter performance, these databases enable AI-driven knob tuning\nto autonomously refine database configurations.\nAdapting to data and workload drift has been recognized as a\ncritical challenge in advancing the integration of AI and DBMSs [ 17,\n18,48]. Many existing approaches for whether in-database analyt-\nics [14,39] or autonomous system optimization [ 16,25] depend\non static models or complete retraining, limiting their ability to\nhandle continuous data and workload drift. More importantly, these\napproaches are typically layered on top of DBMSs rather than being\nholistically embedded within DBMSs. This separation hinders real-\ntime adaptability, because internal and fine-grained performancemetrics, which are essential for detecting and addressing drift, can\nbe difficult to capture externally [19, 44].\nTo mitigate these limitations, we design and develop NeurDB ,\nan AI-powered database with AI holistically embedded from the\nground up. By deeply integrating AI into all its key components\nand functionalities, NeurDB aims to automatically manage adapta-\ntion workflows for both in-database AI analytics and autonomous\nsystem optimization.\n2.2 Design Goals\nDBMSs are dynamic as data and workloads evolve over time, and\ntherefore, the system must be designed for adaptability while also\nguaranteeing reliability andscalability . Building upon this under-\nstanding, we equip NeurDB with these three key properties.\nAdaptability is the capability of a DBMS to evolve autonomously in\nresponse to drifting data and workloads. With optimal adaptability,\nDBMS can respond to drift in real time.\nReliability depicts the ability of a DBMS to consistently meet per-\nformance and accuracy standards, even during phases of adaptation\nand evolution. With optimal reliability, the system can operate at\npeak performance and maintain high accuracy consistently.\nScalability refers to the system‚Äôs capacity to maintain or enhance\nperformance as the workload increases by introducing more re-\nsources, such as threads or nodes.\nInformally, our design goal is to ensure NeurDB can uphold reli-\nability as quickly as possible when the system adapts or evolves\ndue to data and workload drift, while ensuring good scalability.\nIn addition, we design NeurDB as a general-purpose AI-powered\nDBMS which is expected to serve as a foundational infrastructure\nfor advanced data- and AI-centric applications to enrich various\ndomains with AI capabilities [28].\n2.3 SQL Syntax for AI Analytics\nNeurDB incorporates enhanced SQL to support AI analytics. As\nillustrated in Listing 1 and Listing 2, it extends from the standard\nSQL by introducing a PREDICT keyword to handle two typical AI\ntasks: regression with the VALUE OF clause and classification with\ntheCLASS OF clause. Inspired by the original principle of SQL that\nallows an application developer to write a query with SELECT , and\nthen let the DBMS find the most efficient way to store and retrieve\ndata, we ensure that a developer can submit an AI analytics task sim-\nply withPREDICT . All the following operations, such as retrieving\ntraining data and invoking AI models, are handled automatically\nbyNeurDB . Consequently, unlike existing solutions [ 1,36] that\nrequire application developers to intervene in the execution of AI\nanalytics, such as specifying model parameters, our approach can\nmake the execution transparent to application developers, relieving\nthem from the complexities of performing such analytics. Further,\nwe plan to expose additional SQL for AI model management and\nother AI analytics functionalities, such as generative AI.\nWe now present two real-world analytics scenarios that can be\ndirectly supported by NeurDB usingPREDICT queries.\nRegression. Listing 1 shows the SQL to predict and fill in the\nmissing scores of products based on user reviews. By specifying the\nfeatures for training with TRAIN ON and using data stored in the\n‚Äòreview‚Äô table, we predict the value of the ‚Äòscore‚Äô variable. Notably,\n2\n\nNeurDB: On the Design and Implementation of an AI-powered Autonomous Database\nforTRAIN ON , the asterisk automatically excludes features with\nunique constraints to avoid including meaningless data.\nListing 1: Syntax for a regression task\n1PREDICT VALUE OF score\n2FROMreview\n3WHEREbrand_name = 'Special‚ê£Goods '\n4TRAIN ON *\n5 WITHbrand_name <> 'Special‚ê£Goods '\nClassification. The disease progression prediction in healthcare\ncan be handed by the SQL shown in Listing 2. Similarly, this query\nspecifies the features for training with TRAIN ON and the data table\nusingFROM to predict the class of the ‚Äòoutcome‚Äô variable, which\nindicates whether a patient has diabetes or not. In addition, we\nsupport directly inputting missing data using VALUES .\nListing 2: Syntax for a classification task\n1PREDICT CLASS OF outcome\n2FROMdiabetes\n3TRAIN ON pregnancies, glucose, blood_pressure, ...\n4VALUES (6, 148, 72, ...), (1, 85, 66, ...), ...\n3 SYSTEM OVERVIEW\nWe now describe the system architecture of NeurDB as shown in\nFigure 1, where we achieve the deep fusion of AI and databases by\nestablishing an in-database AI ecosystem. Based on this AI ecosys-\ntem, we enable efficient and effective in-database AI analytics and\ndevelop fast-adaptive learned database components.\nIn-database AI Ecosystem. We establish the in-database AI\necosystem by holistically redesigning existing database compo-\nnents, such as the query executor, and introducing additional mod-\nules, including an AI engine and AI model storage. We first extend\nthe query executor to support AI workflow. Beyond traditional\noperators for fetching and processing data, e.g., scan and join, the\nquery executor in NeurDB includes in-database AI operators for\nmodel training, inference, and fine-tuning. With these AI operators,\nwe effectively integrate the AI workflow into the database query\nprocessing. We are developing additional AI operators, such as\nmodel selection [ 49] and model slicing [ 51], to provide more com-\nprehensive AI services. For example, a query may call the model\nselection operator (denoted as MSelection) to automatically select\nthe best-suited model for a given prediction task, thereby enhancing\naccuracy and efficiency. We then introduce a new module, called\ntheAI engine , into NeurDB . It handles the AI-related processing\nrequests from AI operators and learned components and creates\nAI tasks on their behalf, where the task manager dispatches them\nto the CPU/GPU runtime for execution. Specifically, we propose a\ndata streaming protocol to reduce data transfer overhead, and an\nincremental model update technique to facilitate fast model adap-\ntation, thus enhancing the performance of the AI engine. We shall\nelaborate on these optimizations in Section 4.1. Also, we design\ndedicated AI model storage to store AI models and serve them based\non requests from the AI engine. We further implement a monitor to\ndetect unexpected performance or accuracy issues, based on which\nwe trigger automatic and appropriate model adaptation.\nQuery ExecutorTrainingScan‚Ä¶Data BuÔ¨ÄerItem 1‚Ä¶‚Ä¶Model Storage‚Ä¶Data StorageLearned CCTxn ManagerLearned Query OptimzerPREDICT VALUE OF scoreFROM reviewWHERE brand_name=‚Äòxyz‚ÄôSQL ParserQuery Parser and OptimizerInferenceFine-tuning\n‚Ä¶Model BuÔ¨ÄerTask ManagerTraditional OperatorsJoinAI OperatorsDataAI EngineRuntime‚Ä¶‚Ä¶MSelectionPredict QueryPerfAuc‚Ä¶AI TasksMessageMonitorFigure 1: The System Architecture of NeurDB\nIn-database AI Analytics. In Figure 1, we illustrate a running\nexample of a PREDICT query. We enhance the SQL parser and the\nquery optimizer to produce a customized query plan for PREDICT\nqueries. After parsing and optimizing the query, the query executor\nthen executes the query according to the query plan. In particular,\nit performs the scan operator to retrieve data and then invokes the\ninference operator to deliver the inference task to the AI engine.\nNext, the AI engine uploads the model from the model storage if it\nis not in the model buffer, and conducts model inference to produce\nthe results. If the model is detected to be inaccurate, NeurDB in-\nvokes the fine-tuning operator to update the outdated model with\nthe help of the AI engine.\nAutonomous DBMS Optimization. We also enable autonomous\nsystem optimization based on the proposed in-database AI ecosys-\ntem. For instance, the learned query optimizer interacts with the AI\nengine to train a model responsible for generating efficient query\nplans. The monitor tracks the performance of the generated query\nplans, and if a plan is identified as inefficient, the monitor notifies\nthe AI engine to fine-tune the model, enabling continuous optimiza-\ntion and adaptability. Similarly, the monitor can trigger autonomous\nknob tuning when suboptimal knob settings are detected, ensuring\nthat the system remains well-configured to handle data and work-\nload drift effectively. We will further introduce the detailed design\nof learned database components in Section 4.2.\n4 THE DESIGN OF NeurDB\nIn this section, we present two key subsystems of NeurDB , namely\nan in-database AI ecosystem and learned database components.\n4.1 In-database AI Ecosystem\nAI Engine. The AI engine of NeurDB , pivotal to all AI-related\nactivities for tasks from both user and internal learned components,\noperates on a distributed and event-driven architecture to optimize\nefficiency and throughput. Figure 2 shows the main components\nand the communicative flow with connected external nodes serving\ndistributed AI tasks. In the AI engine, the task manager is the main\ncomponent that coordinates and schedules the tasks and resources.\n3\n\nZhanhao Zhao et al.\nModel BuÔ¨ÄerNode 1Data PipelinesTask ManagerModel ManagerTask 1Task 2‚Ä¶Dispatcher 1Dispatcher N‚Ä¶TrainValidTestDispatcher 2‚Ä¶AI Runtime‚Ä¶Node NNode 2Streaming Data Loader‚Ä¶\nFigure 2: AI Engine of NeurDB\nIt handles and parses the incoming AI tasks, and creates a dispatcher\nfor each task. A dispatcher connects to multiple AI runtimes at ex-\nternal nodes. It also loads and caches the necessary data required\nby the corresponding AI tasks, performs data pipelines on it for\npreprocessing, feature engineering, etc, and pushes the prepared\ndata and model weights to the remote AI runtime to trigger AI activ-\nities. Notably, the data is transferred in a streaming and pipelining\nmanner to minimize the delay in the data preparation steps.\nData Streaming Protocol. During an AI task, the AI runtime\nreceives data continuously from the database. NeurDB ‚Äôs AI engine\noptimizes this process with a dedicated data streaming protocol\nto reduce the time and memory overheads. Specifically, the AI\nruntime establishes a TCP socket connection with the dispatcher.\nWhen a task is assigned to the dispatcher, it first schedules the\nAI runtimes and performs handshakes with them to negotiate (1)\nmodel parameters, such as model structure, model arguments, the\ntraining batch size, etc, and (2) streaming parameters, e.g., the initial\nsize for send and receive buffers and the number of batches per\ntransmission. Then, it starts the data and model transfer through the\nconnection. Notably, to support adaptable control and scheduling\nover resources, these parameters can be dynamically updated for\nan ongoing AI task through a data-driven dispatcher. This makes\nthe AI engine a self-driving component, thereby controlling and\noptimizing its operations autonomously.\nModel Manager. Given the dynamic nature of DBMSs, a typi-\ncal AI lifecycle extends beyond a single model. As new data is\nintroduced, incremental updates and retraining are required to\naddress data and workload drift that degrade predictive perfor-\nmance. This process results in multiple evolving model versions,\ncreating significant management difficulties and storage overheads.\nTo address this challenge, NeurDB leverages the capabilities of\ndatabases to efficiently manage AI models created by either users\nor internal components and handle drift by design. Specifically, it\nintroduces a dedicated model manager in its AI engine, enabling\nfine-grained model management with efficient updates. The model\nmanager provides high-level interfaces for handling AI opera-\ntions, such as training, inference, and fine-tuning, executed via\nmodel views . Similar to data views in DBMSs, model views serve as\nlogical abstractions of AI models tailored for specific tasks, with\nphysical representations maintained in model storage. Formally,\na modelùëÄcomprises a series of layers ùêø(ùëó)(ùëó‚ààZ+). To gener-\nate outputs for data ùëã, the layers are executed sequentially, i.e.,\nùëÄ(ùëã)=ùêø(ùëõ)(ùêø(ùëõ‚àí1)(...(ùêø(1)(ùëã))). This layered model storage ap-\nproach aligns with the structure of deep neural networks (DNNs),\nensuring fast and efficient AI model accessibility. Notably, this\nModel ManagerL1‚Ä¶L2LnModel 1(Version 1)L1‚Ä¶L2LnModel 1(Version 2)L1‚Ä¶LmModel 2(Version 1)Model ViewModel TableTraining HandlerInference HandlerFine-tune Handlert31MIDTimestamp2t21t1\nW21W11W1nDataW1nW2mW12nt31mt221t22t11nMIDLIDTimestampt11211t1ModelsLayersFigure 3: Incremental Update for Model Manager\nmodel formulation can also be generalized to non-linear DNN ar-\nchitectures that are represented as directed acyclic graphs, which\ncan be achieved by executing layers based on the topological order.\nModel Incremental Update. Leveraging its layered model stor-\nage,NeurDB enables versioning for a specific model and supports\nincremental model updates through fine-tuning. In particular, to\nadapt the current model to drifting data distributions, the AI engine\nselectively fine-tunes the final layers using the updated data stored\nin the designated database relation and freezes the preceding lay-\ners. Subsequently, only the updated layers remain persistent in the\nmodel storage, from where they can be extracted and merged with\nthe previously frozen layers to create a new model version.\nFigure 3 illustrates how the model manager supports incremental\nupdates for AI models. Whenever an inference or fine-tuning task\nis initiated, the system retrieves all layers up to the most recent\nversion to construct the model, i.e., for a model ùëÄùëñ,ùë°, uniquely\ndefined by its Model ID (MID) ùëñand the timestamp of creation ùë°,\nconsisting of ùëòlayers,\nùëÄùëñ,ùë°(ùëã)=ùêø(ùëò)\nùëñ,ùë°ùëò(ùêø(ùëò‚àí1)\nùëñ,ùë°ùëò‚àí1(...(ùêø(1)\nùëñ,ùë°1(ùëã))))\nùë†.ùë°.‚àÄùëù,ùëû‚àà{1,...,ùëò},ùêø(ùëù)\nùëñ,ùë°ùëù‚Üíùë°ùëù‚â•ùë°ùëû‚àßùë°ùëù‚â§ùë°.\nLet us consider the example shown in Figure 3. Supposing the\nmodelùëÄ1requires fine-tuning the last layer ùêøùëõ,ùëÄ1,2, namely\nthe second version of ùëÄ1, can then be assembled by layers\n{ùêø(1)\n1,1,...,ùêø(ùëõ‚àí1)\n1,1,ùêø(ùëõ)\n1,2}. This allows ùëÄ1,1andùëÄ1,2to share the ma-\njority of model weights, ensuring adaptability to data drift while\nmaintaining efficient storage.\n4.2 Fast-adaptive Learned Components\nIn this section, we introduce two key learned database components,\nlearned concurrency control and learned query optimizer, which\ncan achieve fast adaptation to data and workload drift. To achieve\nthis, we fundamentally rely on the underlying AI ecosystem. We\nnon-intrusively monitor the system conditions such as transaction\nthroughput and data distributions, which can be used to detect data\nand workload drift in real time. Based on these metrics, NeurDB\nautomatically triggers model fine-tuning to adapt to continuously\nevolving data and workloads, and continually generates valid input\nfor model pre-training, allowing the model for learned database\ncomponents to gain global knowledge of most drift.\nLearned Concurrency Control. We design an efficient learned\nconcurrency control algorithm that can continually adapt to chang-\ning workloads, as shown in Figure 4. Given a transaction ùëáwith\n4\n\nNeurDB: On the Design and Implementation of an AI-powered Autonomous Database\n‚Ä¶Encoding T1T2T6T5Perf MonitorConÔ¨Çict InfoContextual Info Transaction‚Ä¶SelectUpdateContention StateTxn Length: 10ActionAcquire LockBlock/Abort‚Ä¶Two-phase AdaptationFiltering ReÔ¨Ånement Updated ModelDecision Model\nFigure 4: Fast-adaptive Learned Concurrency Control\nmultiple operations, our algorithm incorporates a decision-making\nmodelFto assign each operation ùëúùëùthe optimal concurrency con-\ntrol actionùõøbased on the current system condition represented\nasùë•, i.e.,ùõø=F(ùë•). Unlike state-of-the-art approach [ 44] that sim-\nply adjusts actions based on predefined transaction or operation\npatterns (e.g., transaction type), our approach learns the optimal\naction based on the contention state, which includes both conflict\ninformation (such as dependency) of transactions and contextual\ninformation (such as the transaction length). For instance, when\na write is performed on high-contention data records, we may im-\nmediately abort the transaction to avoid unnecessary costs, as the\ntransaction is likely to be aborted eventually. In contrast, we may ex-\necute a read on low-contention records with optimistic concurrency\ncontrol without locking to avoid extra conflict detection overhead,\nand a long transaction that is supposed to run for a long time should\nhave high priority. By using a modeling paradigm based on the\ncontention state, our model is more likely to generalize to drift-\ning workloads with varying levels of contention. However, since\ntransactions can be completed in milliseconds, the model must be\nefficient so as not to become a bottleneck. To achieve this, we first\ndevelop a fast encoding technique to significantly reduce the di-\nmension of contention state representation, and then compress the\nmodel with a flattened layer to improve inference efficiency.\nAs the model is compressed to trade generalizability for perfor-\nmance, the representation of the contention state may gradually\nbecome inaccurate over time due to workload drift. Consequently,\nwhen we detect workload drift through their impact on transac-\ntion performance, we introduce a fine-tuning process for Fto\nadapt it intoFùëõùëíùë•ùë°. With the leaner architecture of the model, the\nadaptation can be accelerated due to the narrower search space\ncompared to a large model. Specifically, we propose a two-phase\nadaptation algorithm based on the online Reinforcement Learning\n(RL) framework. In the first filtering phase, we generate several\nimproved models using Bayesian optimization and evaluate them\nover a specific timeframe to identify the best-performing model.\nThen, in the refinement phase, we employ reward-based feedback\nto further optimize the selected model.\nLearned Query Optimizer. We propose a learned query optimizer\nthat can efficiently adapt to data and workload drift, as shown\nin Figure 5. In contrast to existing works [ 24,54] that attempt to\nproduce the best plan for a given query Qunder fixed system condi-\ntions (i.e., data distribution and workload), our approach effectively\nidentifies the plan best suited for the current system conditions.\nTo achieve this, we design a dual-module model consisting of an\nencoder and an analyzer. Specifically, in the encoder, we input the\nvector generated by a tree transformer, which includes multiple\nInsert/Update/DeleteData StatisticsBuÔ¨Äer Info Table 1: 10%, ‚Ä¶Optimal PlanCandidate PlansQueryPlan 1Plan N‚Ä¶Tree TransformerEncoder\nPlan NCross-attention LayersAnalyzer\nMulti-head AttentionMLPCol1: Min=10,‚Ä¶ Figure 5: Fast-adaptive Learned Query Optimizer\ncandidate query plans for Q, along with system condition repre-\nsentations (including buffer information depicting buffer usage and\ndata statistics representing each attribute‚Äôs distribution), into cross-\nattention layers to generate a unified embedding. The analyzer then\nuses this embedding as input for a multi-head attention layer fol-\nlowed by a multilayer perceptron (MLP) to deliver the optimal plan.\nWith more knowledge of the mapping between plans and system\nconditions, the model can better generalize to data and workload\ndrift. To maximize this knowledge, we generate various synthetic\ndata distributions and workloads using Bayesian optimization, and\npre-train the model to handle most drift effectively. Consequently,\nthe proposed learned query optimizer provides consistent query\nperformance under evolving data and workloads.\nDiscussion. The main design principle we follow to develop com-\nputational models and algorithms in NeurDB is the filter-and-refine\nprinciple (FRP) [ 11,28,29]. FRP employs a two-stage strategy: a\nfiltering stage quickly eliminates less promising or irrelevant ob-\njects (e.g., data, strategies, etc.) from a large set, followed by a more\nresource-intensive refinement stage on the filtered subset. This\nstrategy enables the efficient execution of computationally demand-\ning tasks while yielding near-optimal results. We therefore leverage\nFRP in both in-database AI analytics and autonomous DBMS op-\ntimization for enhanced adaptability. For example, we integrate\nFRP in the RL framework of learned concurrency control and in-\ncorporate it in the multi-head attention layers of the learned query\noptimizer. However, existing model structures, typically character-\nized by fixed topologies and layer configurations, constrain the full\npotential of FRP. We are exploring a flexible model structure that\ncan be dynamically adjusted with the guidance of FRP, enabling\nseamless adaptation to data and workload drift.\n5 EVALUATION\nIn this section, we present our preliminary evaluation results. We\nfirst introduce the experimental setup, and then evaluate the in-\ndatabase AI ecosystem under real-world AI analytics scenarios, and\nthe learned database components with data and workload drifts.\n5.1 Experimental Setup\nWe conduct experiments on a server equipped with an Intel(R)\nXeon(R) W-2133 CPU@3.60GHz (12 cores, 24 threads), 64 GB mem-\nory, and 3 NVIDIA GeForce RTX 2080 Ti GPUs. All experiments\nare executed within Docker containers based on the official Ubuntu\n22.04 image with CUDA 11.8.0, leveraging the host‚Äôs GPU resources.\n5\n\nZhanhao Zhao et al.\nTable 1: Queries for AI Analytics Evaluations\nWorkload Statement\nE-Commerce (E) PREDICT VALUE OF click_rate FROM avazuTRAIN ON *\nHealthcare (H) PREDICT CLASS OF outcome FROM diabetesTRAIN ON *\n5.1.1 Benchmarks. We construct two real-world applications that\nrequire AI analytics. The AI analytics queries used in our experi-\nments are listed in Table 1.\n‚Ä¢E-commerce (E) Workload performs click-through rate predic-\ntion, a critical task in e-commerce for product recommendations,\nusing the Avazu dataset (Avazu) [ 7], which consists of ~40.4M\nrecords and 22 attributes. We use k-means clustering to create\nfive data clusters namely, ùê∂1toùê∂5, and by switching from one\nto another, we simulate the data distribution drift.\n‚Ä¢Healthcare (H) Workload conducts disease progression predic-\ntion using the UCI Diabetes dataset (Diabetes) [ 9]. After scaling,\nthe dataset comprises ~5.2M data records and 43 attributes.\nWe establish a micro-benchmark to evaluate the learned data-\nbase components. It consists of a transactional benchmark based\non YCSB [ 6], which generates synthetic workloads for large-scale\nInternet applications. Each transaction performs 5 selects and 5\nupdates on a table with 1 million records. In addition, we construct\nan OLAP benchmark based on the STATS dataset [ 8], which con-\nsists of 8 tables from the Stats Stack Exchange network. We execute\ninserts/updates/deletes with randomly generated data values to\nsimulate data distribution drift following a recent work [23].\n5.1.2 Implementation and Default Configuration. We have released\nthe first version of NeurDB [27], in which we integrate our pro-\nposed in-database AI ecosystem and learned components into the\ncodebase of PostgreSQL v16.3. We will gradually introduce new\nmodules such as AI-powered resource scheduling, etc., and replace\nexisting components where necessary. We implement a baseline\nsystem called PostgreSQL+P, which loads data from PostgreSQL in\nbatches, and utilizes an AI runtime built with PyTorch to support AI\nanalytics. By default, we employ ARM-Net [ 3], an adaptive relation\nmodeling network tailored for structured data, as the basic analytics\nmodel for both PostgreSQL+P and NeurDB . In our experiments, we\ninherit the default settings of PostgreSQL unless otherwise speci-\nfied. We set the default window size of the streaming data loader to\n80 data batches. Each batch contains 4096 data records (samples).\n5.2 In-database AI Analytics\nWe evaluate the in-database AI analytics in terms of end-to-end\nlatency, training throughput, and loss variation with data drift.\nEfficiency and Scalability. We study the efficiency of NeurDB on\nAI analytics workloads by comparing it with PostgreSQL+P. As ob-\nserved in Figure 6(a), NeurDB achieves up to 41.3% and 48.6% lower\nend-to-end latency, and 1.96 √óand 2.92√óhigher training throughput\nthan PostgreSQL+P for Workload E and Workload H, respectively.\nThe significant improvement in latency and throughput achieved\nbyNeurDB is due to its in-database AI ecosystem, which efficiently\nsupports AI analytics by utilizing a data streaming protocol. Further,\nto evaluate the impact of data volume on the end-to-end latency, we\nrun Workload E with varying numbers of data batches. As shown\n 0 30 60 90 120 150 180\nEH\nWorkloadLatency (s)PostgreSQL+P\n 0 5 10 15 20 25 30 35\nEHThroughput \n (103 Samples/s)NeurDB(a) Overall Performance\n 0 20 40 60 80 100 120 140\n20 40 80160 320 640Latency (s)\n# of Data BatchesPostgreSQL+P NeurDB (b) Effects of Data Volume\n 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\n 0  50  100  150  200  250  300  350  400Loss\n# of Data Samples (103)NeurDB (w/o Inc. Update) NeurDB\n(c) Effects of Data Distribution Drift\nFigure 6: End-to-end Performance of AI Analytics\n 0 1 2 3 4 5\n4 16Throughput (103 Txns/s)\n# of ThreadsPostgreSQL\nNeurDB\n 0 80 160 240 320 400\n0 300 600 900 1200 1500 18008 Threads \n 1 Warehouse\n8 Threads \n 2 Warehouses16 Threads \n 1 WarehouseThroughput (103 Txns/s)\nElapsed time (s)Polyjuice NeurDB(CC)\n(a) Overall Throughput (b) Throughput under Data and Workload Drift\nFigure 7: Performance of Learned Concurrency Control\nin Figure 6(b), NeurDB consistently outperforms PostgreSQL+P,\nindicating that NeurDB can scale well with increased data volume.\nAdaptability. We investigate NeurDB ‚Äôs ability to adapt to the\ndrifting data and workloads with the model incremental update\ntechnique. To simulate such drift, for ùëñ‚àà[1,4], we let NeurDB per-\nform the training task using cluster ùê∂ùëñof Workload E, and switch\ntoùê∂ùëñ+1when 81,920 samples of ùê∂ùëñare consumed by model train-\ning. Figure 6(c) plots the training losses with and without model\nincremental updates. From the result, we can observe that starting\nfrom the first data drift, the AI engine equipped with incremental\nupdates receives lower loss values during the sudden drift in data\ndistributions. This enables the model to converge faster, and as a\nresult, NeurDB is equipped to serve the new tasks effectively.\n5.3 Learned Database Components\nWe now investigate the performance of our proposed learned data-\nbase components.\nLearned Concurrency Control. To evaluate the proposed con-\ncurrency control algorithm, we compare NeurDB with PostgreSQL\nusing the micro-benchmark with varying thread counts. The re-\nsults, shown in Figure 7(a), demonstrate that NeurDB achieves up\nto 1.44√óhigher transaction throughput than PostgreSQL. We at-\ntribute this performance gain to the proposed algorithm‚Äôs ability to\n6\n\nNeurDB: On the Design and Implementation of an AI-powered Autonomous Database\n10-310-210-1100101102103\n123456781234567812345678Original STATS STATS w. Mild Drift STATS w. Severe Drift Latency (s)\nQuery #PostgreSQL Bao Lero NeurDB\nFigure 8: Performance of Learned Query Optimizers\nschedule transactions more effectively than serializable snapshot\nisolation [ 35], a static concurrency control algorithm employed\nin PostgreSQL. We further evaluate the adaptability of our pro-\nposed algorithm against Polyjuice [ 44], state-of-the-art learned\nalgorithm. Due to the limitation of constraining the transaction\nexecution workflow, implementing Polyjuice on the codebase of\nNeurDB would be cumbersome. We therefore opt to implement\nour algorithm, named NeurDB(CC) , into the Polyjuice codebase\nto facilitate a fair comparison. We set up a drift workload based\non TPCC [ 42] by varying the number of warehouses and threads.\nAs shown in Figure 7(b), NeurDB(CC) adapts quickly to workload\ndrift and outperforms Polyjuice by up to 2.05 √ó. The superior per-\nformance of NeurDB(CC) mainly stems from its design that en-\ncapsulates a fast yet accurate model to find the best concurrency\ncontrol action, while facilitating the fine-tuning process with the\ntwo-phase adaptation algorithm.\nLearned Query Optimizer. We next compare NeurDB with\nPostgreSQL and two state-of-the-art learned query optimizer ap-\nproaches, namely Bao [ 24], and Lero [ 54]. We construct three work-\nloads with different data distributions and randomly select 8 SPJ\nqueries provided by STATS datasets. We use stable models of Bao\nand Lero for the experiment, as they demonstrated good perfor-\nmance in their respective papers.\nAs can be observed in Figure 8, NeurDB achieves up to 20.32% lower\naverage latency of all evaluated queries, which demonstrates its\neffective adaptability to both data and workload drift. Due to the\nproposed dual-module model, NeurDB is able to effectively capture\nsystem conditions and use them to select an efficient query plan.\n6 RELATED WORK\nOur work relates to a broad spectrum of efforts on the fusion of\nAI and databases. The initial concepts [ 2,12] can be traced back\nto the 1980s. At that time, the fusion was far from reality due to\nlimited development in both realms. With advancements in AI and\ndatabase fields over the years, numerous attempts by academia [ 15,\n34] and industry [ 14,21] have been made to advance in-database\nAI analytics and autonomous DBMS optimization. However, many\nproblems remain open, and it is important to consider data and\nworkload drift in both directions. Here we shall briefly discuss\nrecent advancements in AI and DBMS integration, with a particular\nfocus on their approaches to handling data and workload drift.\nIn-database AI Analytics. Recent research increasingly focuses\non optimizing the performance of in-database AI analytics. For\nexample, some studies [ 13,14,33,39] extend the existing query\nand transaction execution framework to improve the efficiencyof analytics tasks. Further, in-database model management sys-\ntems [ 41,43,53] are proposed to minimize model storage costs\nwhile efficiently serving models for analytics tasks. These optimiza-\ntions can be broadly adopted by full-fledged DBMSs supporting\nAI analytics, such as nsDB [ 50], GaussML [ 20], and NeurDB . How-\never, apart from the mentioned approaches, NeurDB establishes\nan in-database AI ecosystem, specifically designed to enhance the\nefficiency and effectiveness of AI analytics under continuous data\nand workload drift.\nAutonomous DBMS Optimization. Traditional AI-driven DBMS\noptimization in areas such as knob tuning [ 34], resource manage-\nment [ 21,22], and learned system components [ 15,16,24,44,54]\ntypically relies on complete retraining to handle data and workload\ndrift. Recently, the development of drift-handling mechanisms be-\ncome popular, particularly for learned query optimizers and learned\nindexes. For example, some works introduce methods to automati-\ncally detect drift and trigger model updates [ 18,19,48], enabling\nlearned query optimizers to withstand data and workload drift. Fur-\nther, updatable learned indexes [ 10,47,52] are enhanced with the\nadaptability to continuous transactional updates. Unlike existing\nworks focusing on certain components, NeurDB introduces a uni-\nfied architecture based on the proposed in-database AI ecosystem\nto facilitate fast adaptation across all its key components.\nWe are extending NeurDB to be an intelligent cloud-native\nHTAP database with built-in support for advanced AI analytics. In\nparticular, we disaggregate the system architecture into separate\ncompute and storage layers [ 4,32], enabling dedicated compute\nnodes to handle OLTP, OLAP, and AI analytics independently. Based\non this foundation, we are devising AI-powered resource sched-\nuling to enable on-demand resource allocation and component\nassembling, thus enhancing system scalability and adaptability.\n7 CONCLUSIONS\nThis paper presents NeurDB , a novel AI-powered autonomous\nDBMS that is adaptable to data and workload drift. NeurDB , with\nits fast-adaptive learned database components and in-database AI\necosystem, facilitates efficient and effective in-database AI analyt-\nics and autonomous system optimization. Empirical evaluations\ndemonstrate the superiority of NeurDB , highlighting its potential\nto realize a seamless fusion of AI and databases.\nACKNOWLEDGMENTS\nWe would like to thank Jiaqi Zhu for her help and comments. The\nwork of NUS researchers is partially supported by the Lee Foun-\ndation in terms of Beng Chin Ooi‚Äôs Lee Kong Chian Centennial\nProfessorship fund and NUS Faculty Development Fund. Gang\nChen‚Äôs work is supported by National Key Research and Develop-\nment Program of China (2022YFB2703100). Yanyan Shen‚Äôs work is\nsupported by National Key Research and Development Program\nof China (2022YFE0200500). Yuncheng Wu‚Äôs work is supported\nby National Key Research and Development Program of China\n(2023YFB4503600). Meihui Zhang‚Äôs work is supported by National\nNatural Science Foundation of China (62072033).\n7\n\nZhanhao Zhao et al.\nREFERENCES\n[1] Google BigQuery. 2024. https://cloud.google.com/bigquery/docs.\n[2] Michael L. Brodie. 1988. Future Intelligent Information Systems: AI and Database\nTechnologies Working Together. In AAAI . AAAI Press / The MIT Press, 844‚Äì845.\n[3] Shaofeng Cai, Kaiping Zheng, Gang Chen, H. V. Jagadish, Beng Chin Ooi, and\nMeihui Zhang. 2021. ARM-Net: Adaptive Relation Modeling Network for Struc-\ntured Data. In SIGMOD Conference . ACM, 207‚Äì220.\n[4]Wei Cao, Yingqiang Zhang, Xinjun Yang, Feifei Li, Sheng Wang, Qingda Hu,\nXuntao Cheng, Zongzhi Chen, Zhenjun Liu, Jing Fang, Bo Wang, Yuhui Wang,\nHaiqing Sun, Ze Yang, Zhushi Cheng, Sen Chen, Jian Wu, Wei Hu, Jianwei Zhao,\nYusong Gao, Songlu Cai, Yunyang Zhang, and Jiawang Tong. 2021. PolarDB\nServerless: A Cloud Native Database for Disaggregated Data Centers. In SIGMOD\nConference . ACM, 2477‚Äì2489.\n[5]Gao Cong, Jingyi Yang, and Yue Zhao. 2024. Machine Learning for Databases:\nFoundations, Paradigms, and Open problems. In SIGMOD Conference Companion .\nACM, 622‚Äì629.\n[6] Brian F. Cooper, Adam Silberstein, Erwin Tam, Raghu Ramakrishnan, and Russell\nSears. 2010. Benchmarking cloud serving systems with YCSB. In SoCC . ACM,\n143‚Äì154.\n[7] Avazu Dataset. 2024. https://www.kaggle.com/c/avazu-ctr-prediction.\n[8] STATS Dataset. 2024. https://github.com/Nathaniel-Han/End-to-End-CardEst-\nBenchmark.\n[9] UCI Diabetes Dataset. 2024. https://archive.ics.uci.edu/ml/datasets.\n[10] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\nDavid B. Lomet, and Tim Kraska. 2020. ALEX: An Updatable Adaptive Learned\nIndex. In SIGMOD Conference . ACM, 969‚Äì984.\n[11] Filter and Refine. 2024. https://en.wikipedia.org/wiki/Filter_and_refine/.\n[12] Goetz Graefe and David J. DeWitt. 1987. The EXODUS Optimizer Generator. In\nSIGMOD Conference . ACM Press, 160‚Äì172.\n[13] Matthias Jasny, Tobias Ziegler, Tim Kraska, Uwe R√∂hm, and Carsten Binnig. 2020.\nDB4ML - An In-Memory Database Kernel with Machine Learning Support. In\nSIGMOD Conference . ACM, 159‚Äì173.\n[14] Konstantinos Karanasos, Matteo Interlandi, Fotis Psallidas, Rathijit Sen,\nKwanghyun Park, Ivan Popivanov, Doris Xin, Supun Nakandala, Subru Krishnan,\nMarkus Weimer, Yuan Yu, Raghu Ramakrishnan, and Carlo Curino. 2020. Extend-\ning Relational Query Processing with ML Inference. In CIDR . www.cidrdb.org.\n[15] Tim Kraska, Mohammad Alizadeh, Alex Beutel, Ed H. Chi, Ani Kristo, Guillaume\nLeclerc, Samuel Madden, Hongzi Mao, and Vikram Nathan. 2019. SageDB: A\nLearned Database System. In CIDR . www.cidrdb.org.\n[16] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In SIGMOD Conference . ACM, 489‚Äì504.\n[17] Meghdad Kurmanji, Eleni Triantafillou, and Peter Triantafillou. 2024. Machine\nUnlearning in Learned Databases: An Experimental Analysis. Proc. ACM Manag.\nData 2, 1 (2024), 49:1‚Äì49:26.\n[18] Meghdad Kurmanji and Peter Triantafillou. 2023. Detect, Distill and Update:\nLearned DB Systems Facing Out of Distribution Data. Proc. ACM Manag. Data 1,\n1 (2023), 33:1‚Äì33:27.\n[19] Beibin Li, Yao Lu, and Srikanth Kandula. 2022. Warper: Efficiently Adapting\nLearned Cardinality Estimators to Data and Workload Drifts. In SIGMOD Con-\nference . ACM, 1920‚Äì1933.\n[20] Guoliang Li, Ji Sun, Lijie Xu, Shifu Li, Jiang Wang, and Wen Nie. 2024. GaussML:\nAn End-to-End In-Database Machine Learning System. In ICDE . IEEE, 5198‚Äì5210.\n[21] Guoliang Li, Xuanhe Zhou, Ji Sun, Xiang Yu, Yue Han, Lianyuan Jin, Wenbo\nLi, Tianqing Wang, and Shifu Li. 2021. openGauss: An Autonomous Database\nSystem. Proc. VLDB Endow. 14, 12 (2021), 3028‚Äì3041.\n[22] Ji-You Li, Jiachi Zhang, Wenchao Zhou, Yuhang Liu, Shuai Zhang, Zhuoming\nXue, Ding Xu, Hua Fan, Fangyuan Zhou, and Feifei Li. 2023. Eigen: End-to-end\nResource Optimization for Large-Scale Databases on the Cloud. Proc. VLDB\nEndow. 16, 12 (2023), 3795‚Äì3807.\n[23] Pengfei Li, Wenqing Wei, Rong Zhu, Bolin Ding, Jingren Zhou, and Hua Lu. 2023.\nALECE: An Attention-based Learned Cardinality Estimator for SPJ Queries on\nDynamic Workloads. Proc. VLDB Endow. 17, 2 (2023), 197‚Äì210.\n[24] Ryan Marcus, Parimarjan Negi, Hongzi Mao, Nesime Tatbul, Mohammad Al-\nizadeh, and Tim Kraska. 2021. Bao: Making Learned Query Optimization Practical.\nInSIGMOD Conference . ACM, 1275‚Äì1288.\n[25] Ryan Marcus, Parimarjan Negi, Hongzi Mao, Chi Zhang, Mohammad Alizadeh,\nTim Kraska, Olga Papaemmanouil, and Nesime Tatbul. 2019. Neo: A Learned\nQuery Optimizer. Proc. VLDB Endow. 12, 11 (2019), 1705‚Äì1718.\n[26] MindsDB. 2024. https://mindsdb.com/.\n[27] NeurDB. 2024. https://github.com/neurdb/neurdb.\n[28] Beng Chin Ooi, Shaofeng Cai, Gang Chen, Yanyan Shen, Kian-Lee Tan, Yuncheng\nWu, Xiaokui Xiao, Naili Xing, Cong Yue, Lingze Zeng, Meihui Zhang, and Zhan-\nhao Zhao. 2024. NeurDB: An AI-powered Autonomous Data System. SCIENCE\nCHINA Information Sciences 67, 200901 (2024). https://doi.org/10.1007/s11432-\n024-4125-9[29] Beng Chin Ooi, HweeHwa Pang, Hao Wang, Limsoon Wong, and Cui Yu. 2002.\nFast Filter-and-Refine Algorithms for Subsequence Selection. In IDEAS . IEEE\nComputer Society, 243‚Äì255.\n[30] Beng Chin Ooi, Kian-Lee Tan, Sheng Wang, Wei Wang, Qingchao Cai, Gang\nChen, Jinyang Gao, Zhaojing Luo, Anthony K. H. Tung, Yuan Wang, Zhongle Xie,\nMeihui Zhang, and Kaiping Zheng. 2015. SINGA: A Distributed Deep Learning\nPlatform. In ACM Multimedia . ACM, 685‚Äì688.\n[31] Oracle. 2024. https://docs.oracle.com/en/database/oracle/machine-learning/.\n[32] Xi Pang and Jianguo Wang. 2024. Understanding the Performance Implications\nof the Design Principles in Storage-Disaggregated Databases. Proc. ACM Manag.\nData 2, 3 (2024), 180.\n[33] Kwanghyun Park, Karla Saur, Dalitso Banda, Rathijit Sen, Matteo Interlandi, and\nKonstantinos Karanasos. 2022. End-to-end Optimization of Machine Learning\nPrediction Queries. In SIGMOD Conference . ACM, 587‚Äì601.\n[34] Andrew Pavlo, Gustavo Angulo, Joy Arulraj, Haibin Lin, Jiexi Lin, Lin Ma,\nPrashanth Menon, Todd C. Mowry, Matthew Perron, Ian Quah, Siddharth San-\nturkar, Anthony Tomasic, Skye Toor, Dana Van Aken, Ziqi Wang, Yingjun Wu,\nRan Xian, and Tieying Zhang. 2017. Self-Driving Database Management Systems.\nInCIDR . www.cidrdb.org.\n[35] Dan R. K. Ports and Kevin Grittner. 2012. Serializable Snapshot Isolation in\nPostgreSQL. Proc. VLDB Endow. 5, 12 (2012), 1850‚Äì1861.\n[36] PostgresML. 2024. https://postgresml.org/.\n[37] Christopher R√©, Divy Agrawal, Magdalena Balazinska, Michael J. Cafarella,\nMichael I. Jordan, Tim Kraska, and Raghu Ramakrishnan. 2015. Machine Learn-\ning and Databases: The Sound of Things to Come or a Cacophony of Hype?. In\nSIGMOD Conference . ACM, 283‚Äì284.\n[38] Amazon Redshift. 2024. https://docs.aws.amazon.com/redshift/latest/dg/\nmachine_learning.html.\n[39] Ricardo Salazar-D√≠az, Boris Glavic, and Tilmann Rabl. 2024. InferDB: In-Database\nMachine Learning Inference Using Indexes. Proc. VLDB Endow. 17, 8 (2024), 1830‚Äì\n1842.\n[40] Azure SQL. 2024. https://learn.microsoft.com/en-us/azure/azure-sql/.\n[41] Zhaoyuan Su, Ammar Ahmed, Zirui Wang, Ali Anwar, and Yue Cheng. 2024.\nEverything You Always Wanted to Know About Storage Compressibility of\nPre-Trained ML Models but Were Afraid to Ask. Proc. VLDB Endow. 17, 8 (2024),\n2036‚Äì2049.\n[42] TPCC. 2024. http://www.tpc.org/tpcc/.\n[43] Manasi Vartak. 2017. MODELDB: A System for Machine Learning Model Man-\nagement. In CIDR . www.cidrdb.org.\n[44] Jia-Chen Wang, Ding Ding, Huan Wang, Conrad Christensen, Zhaoguo Wang,\nHaibo Chen, and Jinyang Li. 2021. Polyjuice: High-Performance Transactions\nvia Learned Concurrency Control. In OSDI . USENIX Association, 198‚Äì216.\n[45] Wei Wang, Gang Chen, Tien Tuan Anh Dinh, Jinyang Gao, Beng Chin Ooi,\nKian-Lee Tan, and Sheng Wang. 2015. SINGA: Putting Deep Learning in the\nHands of Multimedia Users. In ACM Multimedia . ACM, 25‚Äì34.\n[46] Wei Wang, Meihui Zhang, Gang Chen, H. V. Jagadish, Beng Chin Ooi, and Kian-\nLee Tan. 2016. Database Meets Deep Learning: Challenges and Opportunities.\nSIGMOD Rec. 45, 2 (2016), 17‚Äì22.\n[47] Jiacheng Wu, Yong Zhang, Shimin Chen, Yu Chen, Jin Wang, and Chunxiao Xing.\n2021. Updatable Learned Index with Precise Positions. Proc. VLDB Endow. 14, 8\n(2021), 1276‚Äì1288.\n[48] Peizhi Wu and Zachary G. Ives. 2024. Modeling Shifting Workloads for Learned\nDatabase Systems. Proc. ACM Manag. Data 2, 1 (2024), 38:1‚Äì38:27.\n[49] Naili Xing, Shaofeng Cai, Gang Chen, Zhaojing Luo, Beng Chin Ooi, and Jian\nPei. 2024. Database Native Model Selection: Harnessing Deep Neural Networks\nin Database Systems. Proc. VLDB Endow. 17, 5 (2024), 1020‚Äì1033.\n[50] Ye Yuan, Bo Tang, Tianfei Zhou, Zhiwei Zhang, and Jianbin Qin. 2024. nsDB:\nArchitecting the Next Generation Database by Integrating Neural and Symbolic\nSystems (Vision). Proc. VLDB Endow. 17, 11 (2024), 3283‚Äì3289.\n[51] Lingze Zeng, Naili Xing, Shaofeng Cai, Gang Chen, Beng Chin Ooi, Jian Pei,\nand Yuncheng Wu. 2025. Powering In-Database Dynamic Model Slicing for\nStructured Data Analytics. Proc. VLDB Endow. 17, 1 (2025), 1020‚Äì1033.\n[52] Shunkang Zhang, Ji Qi, Xin Yao, and Andr√© Brinkmann. 2024. Hyper: A High-\nPerformance and Memory-Efficient Learned Index via Hybrid Construction. Proc.\nACM Manag. Data 2, 3 (2024), 145.\n[53] Lixi Zhou, Jiaqing Chen, Amitabh Das, Hong Min, Lei Yu, Ming Zhao, and Jia\nZou. 2022. Serving Deep Learning Models with Deduplication from Relational\nDatabases. Proc. VLDB Endow. 15, 10 (2022), 2230‚Äì2243.\n[54] Rong Zhu, Wei Chen, Bolin Ding, Xingguang Chen, Andreas Pfadler, Ziniu Wu,\nand Jingren Zhou. 2023. Lero: A Learning-to-Rank Query Optimizer. Proc. VLDB\nEndow. 16, 6 (2023), 1466‚Äì1479.\n[55] Rong Zhu, Lianggui Weng, Wenqing Wei, Di Wu, Jiazhen Peng, Yifan Wang,\nBolin Ding, Defu Lian, Bolong Zheng, and Jingren Zhou. 2024. PilotScope:\nSteering Databases with Machine Learning Drivers. Proc. VLDB Endow. 17, 5\n(2024), 980‚Äì993.\n8",
  "textLength": 51854
}