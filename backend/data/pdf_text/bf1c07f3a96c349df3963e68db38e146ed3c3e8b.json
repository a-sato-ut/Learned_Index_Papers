{
  "paperId": "bf1c07f3a96c349df3963e68db38e146ed3c3e8b",
  "title": "NN-based Transformation of Any SQL Cardinality Estimator for Handling DISTINCT, AND, OR and NOT",
  "pdfPath": "bf1c07f3a96c349df3963e68db38e146ed3c3e8b.pdf",
  "text": "NN-based Transformation of Any SQL Cardinality\nEstimator for Handling DISTINCT, AND, OR and NOT\nRojeh Hayek\nCS Department, Technion\nHaifa 3200003, Israel\nsrojeh@cs.technion.ac.ilOded Shmueli\nCS Department, Technion\nHaifa 3200003, Israel\noshmu@cs.technion.ac.il\nABSTRACT\nSQL queries, with the AND, OR, and NOT operators, con-\nstitute a broad class of highly used queries. Thus, their\ncardinality estimation is important for query optimization.\nIn addition, a query planner requires the set-theoretic cardi-\nnality (i.e., without duplicates) for queries with DISTINCT\nas well as in planning; for example, when considering sort-\ning options. Yet, despite the importance of estimating query\ncardinalities in the presence of DISTINCT, AND, OR, and\nNOT, many cardinality estimation methods are limited to\nestimating cardinalities of only conjunctive queries with du-\nplicates counted.\nThe focus of this work is on two methods for handling\nthis de\fciency that can be applied to anylimited cardinal-\nity estimation model. First, we describe a specialized deep\nlearning scheme, PUNQ, which is tailored to representing\nconjunctive SQL queries and predicting the percentage of\nunique rows in the querys result with duplicate rows. Using\nthe predicted percentages obtained via PUNQ, we are able\nto transform anycardinality estimation method that only\nestimates for conjunctive queries, and which estimates car-\ndinalities with duplicates (e.g., MSCN), to a method that\nestimates queries cardinalities without duplicates. This en-\nables estimating cardinalities of queries with the DISTINCT\nkeyword. In addition, we describe a recursive algorithm,\nGenCrd, for extending anycardinality estimation method\nMthat only handles conjunctive queries to one that esti-\nmates cardinalities for more general queries (that include\nthe AND, OR, and NOT operators), without changing the\nmethod Mitself.\nOur evaluation is carried out on a challenging, real-world\ndatabase with general queries that include either the DIS-\nTINCT keyword or the AND, OR, and NOT operators. The\nmethods we use are obtained by transforming di\u000berent ba-\nsic cardinality estimation methods. The evaluation shows\nthat the proposed methods obtain accurate cardinality esti-\nmates with the same level of accuracy as that of the original\ntransformed methods.\nThis work is licensed under the Creative Commons Attribution-\nNonCommercial-NoDerivatives 4.0 International License. To view a copy\nof this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. For\nany use beyond those covered by this license, obtain permission by emailing\ninfo@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 12, No. xxx\nISSN 2150-8097.\nDOI: https://doi.org/10.14778/xxxxxxx.xxxxxxxPVLDB Reference Format:\nRojeh Hayek, Oded Shmuile. NN-based Transformation of Any\nSQL Cardinality Estimator for Handling DISTINCT, AND, OR\nand NOT. PVLDB , 12(xxx): xxxx-yyyy, 2020.\nDOI: https://doi.org/10.14778/xxxxxxx.xxxxxxx\n1. INTRODUCTION\nRecently, several neural network (NN) based models were\nproposed for solving the cardinality estimation problem, such\nas the multi-set convolutional network (MSCN) model of\n[17], and the containment rate network (CRN) model of [13].\n[17] introduced the MSCN model, a sophisticated NN tai-\nlored for representing SQL queries and predicting cardinali-\nties, resulting in improved cardinality estimates as compared\nwith state-of-the-art methods.\nIn [13] we introduced a method, Cnt2Crd( M), for estimat-\ning cardinality based on using any model Mfor estimating\nthe percentage (rate) of containment between query results\nof two input queries. To estimate the containment rates be-\ntween pairs of queries, [13] introduced the CRN (NN-based)\nmodel. In addition, [13] proposed a simple transforma-\ntion that converts any cardinality estimation model M(e.g.,\nMSCN) into one for estimating containment rates M0. As a\nresult, Cnt2Crd( M0) obtains more accurate cardinality esti-\nmates than the original model ( M). For this, Cnt2Crd( M0)\nis referred to as Improved M(e.g., Improved MSCN). Using\nthese techniques, [13] showed that Cnt2Crd(CRN), referred\nto henceforth simply as CRN, produced superior predictions\nin most settings, compared to Improved MSCN, Improved\npostgreSQL, MSCN, and postgreSQL.\nBoth MSCN and CRN are superior in estimating cardinal-\nities compared to basic cardinality estimation methods such\nas PostgreSQL [1], Index Based Join Sampling (IBJS) [21]\nand Random Sampling (RS) [5, 27]. However, both MSCN\nand CRN, and other recently developed models, are limited\nto supporting conjunctive queries only, ignoring the fact that\nqueries with the AND, OR, and NOT operators constitute\na broad class of frequently used queries. Furthermore, the\nabovementioned models were designed to only estimate car-\ndinalities with duplicates, thus further limiting the class of\nsupported queries as queries with the DISTINCT keyword\nare simply not supported. In addition to not supporting\nqueries with the DISTINCT keyword, for various intermedi-\nate results, a query planner requires the set-theoretic cardi-\nnality (without duplicates) even when the DISTINCT key-\nword is not used explicitly in the query. For example, in\nemploying counting techniques for handling duplicates, in\nconsidering sorting options, and in creating an index or a\nhash table.\n1arXiv:2004.07009v1  [cs.DB]  15 Apr 2020\n\nWith all the improvements of the recently proposed mod-\nels, the abovementioned limitations pose a signi\fcant prob-\nlem to the applicability of these models. In real world sys-\ntems, supporting (more general) queries with the AND, OR,\nand NOT operators and the DISTINCT keyword is essen-\ntial. The limited models1, lacking AND, OR, NOT and\nDISTINCT support, vary signi\fcantly. Thus, restructur-\ning each one, separately, to support more general queries, is\ncomplex and potentially error-ridden.\nEstimating cardinalities without duplicates (e.g., queries\nwith the DISTINCT keyword) or supporting queries with\nthe AND, OR, and NOT operators using specialized models\nis indeed interesting. However, many existing limited cardi-\nnality estimation models, including state of the art models\n(e.g., MSCN, CRN), have been already developed and were\nshown to be very accurate in estimating conjunctive queries'\ncardinalities. This points to a di\u000berent direction. Instead\nof trying to build, from scratch, specialized models to han-\ndle more general queries2, we propose two di\u000berent meth-\nods that utilize already developed e\u000ecient limited models\nto handle more general queries while retaining the original\nmodels quality of estimates.\nThat is, the thrust of this paper is using two methods for\nsimply and e\u000eciently extend anylimited model to handle\nqueries that were not supported by that original model. Ad-\nditionally we illustrate how these methods can extend any\nlimited model while retaining the original models quality.\nTherefore, the experiments we exhibit aim to illustrate this\nfact, and are not intended to show which model is more\naccurate following the extension, as the original examined\nlimited models, per se, are not the focus of this paper (they\nwere previously developed). The examined limited models\nwere simply used as examples for our extension methods.\nTo tackle the lack of support for the set-theoretic cardi-\nnality estimation (i.e., cardinalities without counting dupli-\ncates), we use a specialized deep learning method, PUNQ.\nThe PUNQ model is tailored to representing conjunctive\nSQL queries and predicting the percentage of unique rows\nwithin the querys result with duplicates. Using the pre-\ndicted percentages, we propose a simple technique to extend\nanylimited model to estimate set-theoretic cardinalities.\nThe PUNQ model uses a tailored vector representation\nfor representing the input queries, which enables us to ex-\npress the querys features. An input query is converted into\nmultiple sets, each representing a di\u000berent part of the query.\nEach element, of one of these sets, is represented by a vec-\ntor. These vectors are then passed into a specialized neu-\nral network that combines them into a single representative\nvector that represents the whole input query. Finally, using\nanother neural network with the representative vector as in-\nput, PUNQ predicts the percentage of unique rows in the\nquerys result with duplicates. Thus, the PUNQ model relies\non the ability of its neural network to learn the (representa-\ntive) vector representation of queries. As a result, we obtain\na small and accurate model for estimating the percentage of\nunique rows in a query result that includes duplicates.\n1In this paper, the term \"limited models\" refers to models\nthat support only conjunctive queries whose estimates count\nduplicates.\n2In this paper, the term \"general queries\" refers, depend-\ning on the context, to queries that include: the DISTINCT\nkeyword and/or the AND, OR and NOT operators.Queries that include the AND, OR, and NOT operators in\ntheir WHERE clauses constitute a broad class of frequently\nused queries. Their expressive power is roughly equivalent to\nthat of the relational algebra. Therefore, estimating cardi-\nnalities for such queries is essential. To estimate the result\ncardinality of such queries, we introduce a recursive algo-\nrithm, GenCrd, that estimates the cardinality of a general\nquery (that includes the AND, OR, and NOT operators),\nusing any limited model that only estimates cardinalities\nfor conjunctive queries, as a \"subroutine\".\nWe evaluated our extension methods (both the PUNQ\nmodel and GenCrd algorithm), on a challenging, real-world\nIMDb database [20] with general queries that either include\nthe DISTINCT keyword or the AND, OR, and NOT opera-\ntors. In addition, the examined queries include join crossing\ncorrelations; these are known to present a special challenge\nto cardinality estimation methods [20, 22, 25].\nThe extension methods were applied to four limited mod-\nels that were used as examples , MSCN [17], CRN [13], Im-\nproved MSCN [13], and Improved PostgreSQL [13]. The\nextended methods were shown to produce cardinality esti-\nmates with the same level of accuracy as that of the orig-\ninal methods. This indicates the practicality of our exten-\nsion methods, o\u000bering a simple and e\u000ecient tool for extend-\ninganylimited cardinality estimation method, to support\nqueries that are more general. As a sanity check, we also\ncompare the four extended methods with PostgreSQL [1],\nas PostgreSQL supports all kinds of queries without using\nour extension methods.\nThis paper has four main contributions:\n1. We introduced two di\u000berent extension methods,\nPUNQ( M) and GenCrd( M), which can extend any\nlimited model Mto supporting more general queries.\n2. Although limited models implementation vary, the pro-\nposed extension methods can be applied to anylimited\nmodel regardless its implementation.\n3. Some of the limited models are considered state-of-the-\nart in terms of estimation accuracy. Our extension\nmethods (on the examined models) proved to retain\nthe original limited model's quality of estimates, thus\npresenting a simple and e\u000ecient blueprint for expand-\ning the limited models class of supported queries.\n4. Using the proposed extension methods, we factor out\nthe code needed to expand each limited model, result-\ning in an improved software architecture.\nThe rest of this paper is organized as follows. In Sec-\ntions 2 and 3, we describe and evaluate the PUNQ model\nfor extending any limited model to support the DISTINCT\nkeyword (i.e. estimating set-theoretic cardinalities). In Sec-\ntions 4 and 5, we describe and evaluate recursive algorithm\nGenCrd, that extends any limited model to support queries\nwith the AND, OR, and NOT operators. Finally, in Section\n6, we present related work, and in Section 7 we present our\nconclusions, and plans for future work.\n2\n\n2. SUPPORTING THE DISTINCT KEYWORD\nSeveral cardinality estimation models are designed to esti-\nmate cardinalities with duplicates, ignoring the importance\nof the set-theoretic cardinality estimation in query optimiza-\ntion. We would like to extend such models, without changing\nthem, to produce set-theoretic cardinality estimates. To this\nend, we design a NN based model, PUNQ, which is tailored\nto estimate the uniqueness rate of its input query (see Sec-\ntion 2.1). We propose this technique as it can be applied\ntoanylimited model (regardless the limited model's imple-\nmentation). Thus, using the PUNQ model, we can trans-\nform any limited model to estimate cardinalities without\nduplicates (thereby supporting queries with the DISTINCT\nkeyword).\n2.1 Uniqueness Rate\nWe de\fne the uniqueness rate of query Qon a speci\fc\ndatabase D, as follows. The uniqueness rate of query Q\nequals to x%on database Dif precisely x%ofQ's execution\nresult rows (with duplicates) on database Dare unique . The\nuniqueness rate is formally a function from QxD to[0,1] ,\nwhere Qis the set of all queries, and Dof all databases. This\nfunction can be directly calculated using the cardinality of\nthe results of query Qas follows:\nx% =jjQ(D)jj\njQ(D)j\nwhere, Q(D) denotes Q's execution result on database D.\nThe operatorjj\u0001jj returns the set theoretic cardinality, and\nthe operatorj\u0001jreturns the cardinality with duplicates. (In\ncaseQ's execution result is empty, then Qhas 0% unique\nrows). For clarity, we do not mention the speci\fc database\nD, that it is usually clear from context.\nFor example, suppose that query Q's one column result,\non database D, includes 10 rows with value 1, and 4 rows\nwith value 2 and 1 row with value 3. By de\fnition, query Q's\ncardinality with duplicates equals to 15, and the cardinality\nwithout duplicates equals to 3. Thus, Q's uniqueness rate\nequals to 20%. This is true, since by de\fnition:\nQ0s Uniqueness Rate =jjQ(D)jj\njQ(D)j=3\n15= 0:2\nSuppose we are given a cardinality estimate Cwith dupli-\ncates of some query QonD, obtained from some cardinal-\nity estimation model. By uniqueness rate de\fnition, we can\neasily obtain the set-theoretic cardinality estimation simply\nby multiplying the cardinality estimate Cwith the unique-\nness rate of query Q. Therefore, using the uniqueness rates\nof queries we can simply transform anylimited cardinality\nestimation method to one that estimates set-theoretic car-\ndinality. Hence, in the following sections we aim to estimate\naccurately and e\u000eciently the uniqueness rates of queries.\nFor this, we use the subsequently described PUNQ model.\n2.2 Learned Uniqueness Rates\nIn applying machine learning to the uniqueness-rate esti-\nmation problem, we face the following common issues that\nare faced by any other deep learning technique.\n\u000fCold start problem: obtaining the initial training dataset.\n\u000fDetermining which supervised model should be used.\n\u000fFeaturization: determining useful representations of\nthe inputs and the outputs of the model.The ways these issues are addressed determine the success\nof the machine learning based model (PUNQ in our case).\nIn the following sections we describe how each issue is ad-\ndressed.\n2.3 Cold Start Problem\n2.3.1 DeÔ¨Åning the Database\nWe generated a training-set of queries, and later on evalu-\nated our model on it, using the IMDb database. The IMDb\ndatabase stores information about over 2.5M movies. There\nare about 4M related movie actors, directors, and produc-\ntion companies. IMDb data has many internal correlations.\nIt is thus challenging for cardinality estimators [20]. As\nIMDb was previously used in [17, 13] by the methods we ex-\ntend, we also use this database to evaluate our PUNQ based\nmethod.\n2.3.2 Generating the Development Dataset\nThis is the \"cold start problem\". We obtain an initial\ntraining corpus using a specialized queries generator. The\ngenerator randomly generates queries based on the IMDB\nschema. It uses potential columns' values for constants.\nThe queries generator is constructed similarly to the ones\nused in [17, 13]. The queries generator forms a query as\nfollows. The number of tables in the query's FROM clause\nis chosen randomly, The speci\fc tables are also chosen ran-\ndomly. Join edges are then randomly set. Next, for each\nchosen table t, column predicates (comparison to constants)\nare chosen. The generator \frst chooses the number ptof\ncolumns predicates on table t. Then, it generates ptran-\ndom column predicates. A column predicate is de\fned by\nrandomly chosen three values: a column name Col, a type\n(<;=; or > ), and a constant within the range of values in\nthe actual database column Col.\nObserve that the speci\fcs of the SELECT clause have no\nimpact on the result cardinality when considering cardinali-\nties with duplicates (i.e., no DISTINCT keyword). Since the\nMSCN model [17] and the CRN model [13] were designed to\nestimate cardinalities with duplicates, the queries generator\nused by [13, 17] can be thought of as assuming a SELECT\n* clause for all the generated queries. However, the speci\fcs\nof the SELECT clause are extremely important for our task\n(predicting uniqueness rates). Therefore, we recon\fgure the\nqueries generator with the following additional step. After\ngenerating the query with the SELECT * clause, we replace\nthe * in the SELECT clause by randomly choosing the num-\nber of columns cto be in the querys SELECT clause, and\nthen choosing at random cdi\u000berent columns from the pos-\nsible columns of the chosen tables.\nIn order to avoid a combinatorial explosion of the num-\nber of possible queries, and to facilitate the learning of the\nmodel, we create queries with up to two joins only. We also\nexamine how the learned models, learned over queries with\nthis small number of joins, generalize to a larger number of\njoins.\nOnce generating the dataset of queries, we execute the\ndataset queries on the IMDb database. This allows us to\nobtain their true uniqueness rates (ground truth). In this\nprocess, we obtain an initial training set for our model. It\nconsists of 20,000 queries with zero to two joins. We split\nthe training samples into 80% training samples and 20%\nvalidation samples.\n3\n\n2.4 Model\nWe form the PUNQ model for estimating the uniqueness\nrates (the percentage of rows in the input querys result with\nduplicates that are unique). PUNQ was inspired by the ar-\nchitectures of the MSCN model [17] and the CRN [13] model.\nThe PUNQ model has three main stages. In the \frst stage,\nit converts the input query into a set Vof vectors. A vec-\ntor represents either the query's columns, tables, joins or\ncolumns predicates. In stage two, using a specialized neu-\nral network, MLP mid, the set Vof vectors is transformed\nintoQvec , a single representative vector that represents the\nwhole input query. In stage three, the percentage of unique\nrows is estimated using MLP out, a two-layers neural net-\nwork.\n2.4.1 First Stage, from QtoV\nWe represent each query Qas a collection of four sets\n(A; T; J; P ).\n\u000fA: set of the attributes in Q's SELECT clause.\n\u000fT: set of the tables in Q's FROM clause.\n\u000fJ: set of the join clauses in Q's WHERE clause.\n\u000fP: set of the columns predicates in Q's WHERE clause.\nEach element in each of the 4 sets is then represented with a\nvector. Together these vectors form the set Vof vectors. To\nfacilitate learning, all the vectors in Vare of the same di-\nmension and the same format as depicted in Table 1, where:\n\u000f#T: the number of tables in the whole database.\n\u000f#C: the number of columns in the whole database.\n\u000f#O: the number of all possible operators in predicates.\nHence, the vector size is # T+ 4\u0003#C+ #O+ 1, denoted as\nL.\nType Att: Table Join Column Predicate\nSegment A-seg T-seg J1-seg J2-seg C-seg O-seg V-seg\nSeg:size #C #T #C #C #C #O 1\nFeat: 1hot vec: 1hot vec: 1hot vec: 1hot vec: 1hot vec: 1hot vec: norm:\nTable 1: Vector Segmentation.\nFor each element of the A; T; P; andJsets, we create\na vector of dimension L, that includes zeros in all its seg-\nments, except for those that are relevant for representing its\nelement, as follows. (see a simple example in Figure 1)\nElements of set A, are represented in the A-seg segment.\nElements of set T, are represented in the T-seg segment.\nElements ( col1;=; col2) of set Jare represented in two seg-\nments. col1 and col2 are represented in J1-seg and J2-seg\nsegments, respectively. Elements ( col; op; val ) of set Pare\nrepresented in three segments. colandopare represented\nin the C-seg and O-seg segments, respectively; valis repre-\nsented in the V-seg segment.\nAll the segments, except for the V-seg, are represented\nby a binary vector with a single non-zero entry, uniquely\nidentifying the corresponding element (one-hot vector). For\nexample, T-seg segment, is represented with one-hot vector\nuniquely identifying the corresponding table name.\nIn the V-seg segment, we represent the predicates values.\nSince their domain is usually large, representing them using\none hot vectors is not practical. Therefore, these values are\nrepresented using their normalized value (a value 2[0;1]).2.4.2 Second Stage, from VtoQvec\nGiven the set of vectors V, we present each vector of V\nas input to MLP mid, a fully-connected one-layer neural net-\nwork. MLP midtransforms each presented vector into a vec-\ntor of dimension H. The \fnal representation for the whole of\nsetVis then given by Qvec , the average over the individual\ntransformed representations of its elements, i.e.,\nQvec =1\njVjX\nv2VMLP mid(v)\nMLP mid(v) =RelU (vUmid+bmid)\nwhere Umid2RLxH,bmid2RHare the learned weights\nand bias, and v2RLis the input row vector. We choose an\naverage (instead of, say, sum) to facilitate generalization to\na di\u000berent number of elements in V. Had we used sum, the\noverall magnitude of Qvec would depend on the cardinality\nof the set V.\n2.4.3 Third Stage, from Qvec toUniqueness rate\nGiven the representative vector of the input query, Qvec ,\nwe aim to predict the uniqueness rate. To do so, we use\nMLP out, a fully-connected two-layer neural network, to com-\npute the estimated uniqueness rate of the input query.\nMLP outtakes as input the Qvec vector of size H. The\n\frst layer in MLP outconverts the input vector into a vector\nof size 0 :5H. The second layer converts the obtained vector\nof size 0 :5H, into a single value representing the uniqueness\nrate. This architecture allows MLP outto learn the unique-\nness rate function.\n^y=MLP out(Qvec )\nMLP out(v) =Sigmoid (ReLU (vUout1+bout1)Uout2+bout2)\nwhere ^ yis the estimated uniqueness rate, Uout12RHx0:5H,\nbout12R0:5HandUout22R0:5Hx1,bout22R1are the\nlearned weights and bias.\nWe use the empirically strong and fast to evaluate ReLU3\nactivation function for hidden layers units in all our neural\nnetworks. The uniqueness rate values are in the range [0,1].\nTherefore, in the \fnal step, we apply the Sigmoid4activa-\ntion function in the second layer to output a \roat value in\nthis range. It follows that we do not apply any featuriza-\ntion on the uniqueness rates (the output of the model) and\nthe model is trained with the actual uniqueness rate values\nwithout any intermediate featurization steps.\n2.5 Training and Testing Interface\nThe PUNQ model building includes two main steps, per-\nformed on an immutable snapshot of the database. We \frst\ngenerate a random training set as described in Section 2.3.\nSecond, we train the PUNQ model, using the training data,\nuntil the mean q-error of the validation test starts to con-\nverge to its best absolute value. In the training phase, we\nuse the early stopping technique [29].\nWe train the PUNQ model to minimize the mean q-error\n[24]. The q-error is a widely-used metric in the cardinality\nestimation problem domain. The q-error is the ratio between\nan estimate and the actual value.\n3ReLU(x) = max(0,x); see [26].\n4Sigmoid(x) = 1 =(1 +e\u0000x); see [26].\n4\n\nFigure 1: Query featurization as sets of feature vectors obtained from sets A,T,JandP(Rest denotes zeroed portions\nof a vector).\nFormally, assuming that yis the true (actual) uniqueness\nrate, and ^ ythe estimated rate, then the q-error is de\fned as\nfollows.\nq\u0000error (y;^y) = ^ y > y ?^y\ny:y\n^y\nFollowing the training phase, and in order to predict the\nuniqueness rate of an input query, the (encoded) query is\nsimply presented to the PUNQ model, and the model out-\nputs the estimated uniqueness rate, as described in Section\n2.4. We train and test our model using the Tensor-Flow\nframework [2], using the Adam training optimizer [15].\n2.6 Hyperparameter Search & Model Costs\nTo optimize our model's performance, we searched the hy-\nperparameter space. We considered di\u000berent settings, where\nwe varied the number of the batch size (16, 32, 64, ..., 2048),\nhidden layer size (16, 32, 64, ..., 1024), and learning rate\n(0.001, 0.01). We examined all the resulting 112 di\u000berent hy-\nperparameters combinations. It turned out that the hidden\nlayer size has the most impact on the models accuracy on the\nvalidation test. Until it reaches the best result, the bigger\nthe hidden layer size is, the more accurate the model is on\nthe validation test. Afterwards, quality declines due to over-\n\ftting. Further, both the learning rate and the batch size\nmainly in\ruence the training convergence behavior rather\nthan the model accuracy.\nAveraged over 5 runs over the validation set, the best con-\n\fguration has a 128 batch size, a 512 hidden layer size, and\na 0.001 learning rate. These settings are thus used through-\nout our model evaluation. Under these settings, the PUNQ\nmodel converges to a mean q-error of approximately 3.5 on\nthe validation set, after running about 200 passes on the\ntraining set (see Figure 2). Averaged over 5 runs, the 200\nepochs training phase takes nearly 30 minutes. The disk-\nserialized model size is about 0.5MB. This includes all the\nlearned wights and biases as described in Section 2.4.\nThe prediction time of uniqueness rates, using the PUNQ\nmodel, consists of performing all the PUNQ model's stages\nas described in Section 2.4. On average, the whole process\ntakes 0.05ms, per query. This includes the overhead intro-\nduced by the Tensor-Flow framework.\nFigure 2: Convergence of the mean q-error.3. PUNQ EVALUATION\nIn this section, we examine how well our extension model\n(PUNQ model), transforms limited cardinality estimation\nmodels that predict cardinalities with duplicates, into ones\nthat predict set-theoretic cardinalities. To this end, we\nextended four di\u000berent limited models, MSCN, CRN, Im-\nproved PostgreSQL, and Improved MSCN [13, 17]. Each\nmodel was extended as detailed below. For clarity, we denote\nthe extended model of model Mas PUNQ( M). Given query\nQ whose set-theoretic cardinality needs to be estimated (i.e.,\nwithout duplicates), PUNQ( M) functions as follows.\n\u000fcalculate C=M(Q), model's Mestimated cardinality\nwith duplicates of query Q.\n\u000fcalculate U=PUNQ (Q), i.e., the uniqueness rate of\nquery Q, obtained from the PUNQ model.\n\u000freturn U\u0001C, the predicted set-theoretic cardinality.\nWe extend each of the four abovementioned limited models\nand test them over two test workloads. In addition, we com-\npare our results with those of the PostgreSQL version 11 car-\ndinality estimation component [1], as postgreSQL supports\nall SQL queries. To estimate cardinality without duplicates\nusing PostgreSQL, we added the DISTINCT keyword in the\nqueries' select clauses, and then used the ANALYZE com-\nmand to obtain the estimated (set-theoretic) cardinalities.\nThe workloads were created using the same queries gen-\nerator (using a di\u000berent random seed) as introduced in Sec-\ntion 2.3.2. In order to ensure a fair comparison between\nthe models, we trained CRN, MSCN and the PUNQ models\nwith the same types of queries, where all the models training\nsets were generated using similar queries generators. In par-\nticular, these models were trained with conjunctive queries\nwith only up to two joins only.\n3.1 Evaluation Workloads\nThe evaluation uses the IMDb dataset, over two di\u000berent\nquery workloads:\n\u000fUnqCrd test1, a synthetic workload with 450 unique\nqueries, with zero to two joins.\n\u000fUnqCrd test2, a synthetic workload with 450 unique\nqueries, with zero to \fvejoins. This dataset is de-\nsigned to examine how the models generalize beyond\n2 joins.\nnumber of joins 0 1 2 3 4 5 overall\nUnqCrd test1 150 150 150 0 0 0 450\nUnqCrd test2 75 75 75 75 75 75 450\nTable 2: Distribution of joins.\n5\n\n3.2 The Quality of Estimates\nWe examined the UnqCrd test1 workload on two state-\nof-the-art limited models, MSCN and CRN. Recall that this\nworkload includes queries with up to two joins, and that\nthe MSCN, CRN and PUNQ models were trained over such\nconjunctive queries. Figure 3 and Table 3 depict the PUNQ\nmodel uniqueness rates estimations. It is clear that the\nPUNQ model estimates are very accurate, where 75% of\nthe examined queries uniqueness rates were predicted accu-\nrately with a q-error smaller than 1.93. That is, the ratio\nbetween the estimated rates and the actual rates does not\nexceed 1.93, whether it is over or under estimated. These\nresults are similar for queries with 0, 1, and 2 joins.\nFigure 3: Uniqueness rates estimation q-errors on the\nUnqCrd test1 workload. In all similar presented plots,\nthe box boundaries are at the 25th/75th percentiles and\nthe horizontal lines mark the 5th/95th percentiles. Thus\n50% of the described test results are located in the boxes,\nand 90% are located within the horizontal lines (boxes\nare clearer in subsequent Figures).\n50th 75th 90th 95th 99th max mean\nPUNQ 1:12 1 :93 3 :71 5 :59 13 :65 139 2 :27\nTable 3: Uniqueness rates estimation q-errors on the\nUnqCrd test1 workload. In all similar presented tables,\nwe depict the percentiles, maximum, and mean q-errors\nof the test (the pth percentile, is the q-error value below\nwhich p% of the test q-errors are found).\nThe fact that the PUNQ model uniqueness rate estimates\n(U) are very accurate, along with the fact that the CRN and\nthe MSCN models cardinality estimates with duplicates ( C)\nare accurate as well, is thus re\rected in accurate cardinality\nestimates without duplicates ( U\u0001C).\nAs a consequence, examining the results in Figure 4 and\nTable 4, it is apparent that when the CRN and MSCN mod-\nels are extended to estimate cardinalities without duplicates\nusing the PUNQ model, the estimates are still very accu-\nrate. Observe that the models we extend were originally\ntailored to estimate cardinalities with duplicates only. Note\nthat postgreSQL model was not extended using the PUNQ\nmodel, as it supports any SQL queries as is. Interestingly,\nit obtained the worst results among the examined models.\nFigure 4: Cardinality estimation q-errors on the Un-\nqCrd test1 workload.\n50th 75th 90th 95th 99th max mean\nPostgreSQL 1 :82 4:93 27 :26 163 926 372207 1214\nPUNQ (MSCN ) 2 :13 4 :83 10 :05 16 :37 60 :53 222 5 :66\nPUNQ (CRN ) 2 :05 4:27 9 :98 15 :24 49 :11 1037 7 :49\nTable 4: Cardinality estimation q-errors on the Un-\nqCrd test1 workload.\n3.3 Generalizing to Additional Joins\nIn this section we examine how the PUNQ model gen-\neralizes to queries with more than 2 joins, and how limited\nmodels adapt to such queries under the PUNQ model exten-\nsion. To do so we use the UnqCrd test2 workload which in-\ncludes queries with zero to \fvejoins. Recall that we trained\nMSCN, CRN, and PUNQ models with conjunctive queries\nthat have only up to twojoins. To examine the limited car-\ndinality estimation models more broadly, we used two ad-\nditional limited models, the Improved PostgreSQL model,\nand the Improved MSCN model (both introduced in [13]).\nFigure 5 and Table 5 depict the PUNQ model unique-\nness rate estimations. Note that the PUNQ model estimates\nare very accurate when considering queries with zero to two\njoins, whereas the estimates tend to over-estimate as the\nnumber of joins increases. This is to be expected as PUNQ\nwas trained with queries with up to two joins only. Never-\ntheless, the overall estimates still relatively accurate.\nFigure 5: Uniqueness rates estimation q-errors on the\nUnqCrd test2 workload.\n50th 75th 90th 95th 99th max mean\nPUNQ 1:75 3 :55 9 :9 18 :08 109 214 6 :62\nTable 5: Uniqueness rates estimation q-errors on the\nUnqCrd test2 workload.\n6\n\nFigure 6: Cardinality estimation q-errors on the UnqCrd test2 workload.\nFigure 6 provides a fuller picture of how the limited mod-\nels estimates are a\u000bected when they are extended to estimate\ncardinalities without duplicates. In the upper part of Fig-\nure 6, we present the limited models' cardinality estimates\nerrors with duplicates, i.e., the models' original estimates.\nIn the bottom part, we show the models' estimates errors on\nthe same queries, when the models are extended to estimate\nset-theoretic cardinalities using the PUNQ model.\nAccording to box-plots de\fnition, 50% of the test queries\nare located within the box boundaries, and 90% are lo-\ncated between the horizontal lines. Observe that the boxes'\nboundaries and the horizontal lines were hardly changed in\nthe extended models box-plots, compared with the original\nmodels box-plots. We conclude that for each model and for\neach number of joins, the majority of the queries are located\nin the same error domains. That is, the models' accuracies\nwere hardly changed after extending them using the PUNQ\nmodel. They maintained almost the same quality of esti-\nmates for set-theoretic cardinality, as the original models'\nquality for estimating cardinality with duplicates.\nSigni\fcant changes in the models quality of estimates may\nbe observed in queries with more than 2 joins. In such\nqueries, the PUNQ model su\u000bers from slightly overestimated\nuniqueness rates, which directly a\u000bect the extended models\nset-theoretic cardinality estimates. This is depicted via their\nbox-plots horizontal lines moving a bit upwards as compared\nwith the box-plots of the original models estimates.\nTable 6 shows the cardinality estimates errors percentiles\nand means using the examined extended models.\n50th 75th 90th 95th 99th max mean\nPostgreSQL 3:95 34 :95 651 2993 20695 331152 2029\nPUNQ (MSCN ) 4 :85 59 :65 1189 5476 86046 288045 3430\nPUNQ (Imp:Post:) 3 :51 20 :97 134 444 9770 136721 563\nPUNQ (Imp:MSCN ) 3 :15 10 :35 33 :73 88 :61 244 3298 24 :46\nPUNQ (CRN ) 3:31 10 :81 39 :95 90 :98 778 3069 35 :71\nTable 6: Cardinality estimation q-errors on the Un-\nqCrd test2 workload.In Table 6 and Figure 6, it is clear that the PUNQ(CRN)\nand PUNQ(Improved MSCN) models are signi\fcantly more\nrobust in generalizing to queries with additional joins as\ncompared with the other examined models. In particular,\nthe two models are more robust than postgreSQL. This is\ndespite the fact that postgreSQL can estimate cardinalities\nwithout duplicates by default, without the need to extend\nit using PUNQ, as was done for the other limited models.\nRecall that the extended models' quality of estimates is\nmainly dependent on the original limited models' quality\nof estimates. Hence, had we used other more (resp., less)\naccurate limited models, we would simply get more (resp.,\nless) accurate results in the extended models. In particular,\nwe can obtain better estimates by extending more accurate\nmodels (not necessarily based on ML) that may be devel-\noped in the future.\nThese results highlight the usefulness of the PUNQ model\nin extending limited models. PUNQ therefore presents a\nsimple tool to extend limited models, while keeping their\nquality of set-theoretic estimates roughly without change.\n3.4 PUNQ Cardinality Prediction Time\nAs described above, to estimate cardinalities without du-\nplicates by extending limited cardinality estimation models,\nwe used the PUNQ model along with the estimated cardi-\nnalities of these models. Therefore, the prediction time of\ncardinalities without duplicates by extending limited mod-\nels, consists of two parts. First, the prediction time of the\nlimited model. Second, the prediction time of the unique-\nness rate, obtained from the PUNQ model.\nThe prediction times of the limited models vary, depend-\ning on the model itself. Table 7 depicts the average pre-\ndiction time in milliseconds, for estimating cardinality with\nduplicates of a single query, using di\u000berent limited models.\nModel MSCN CRN Imp :MSCN Imp :PostgreSQL\nPrediction time 0.25ms 15ms 35ms 75ms\nTable 7: Average prediction time of a single query.\n7\n\nThe prediction time of uniqueness rates using the PUNQ\nmodel is 0.05ms, per query (as described in Section 2.6).\nTherefore, the set-theoretic cardinality prediction time over-\nhead due to using the PUNQ model, when extending limited\nmodels, is minor (0.05ms), as compared with the limited\nmodels' prediction times (of cardinalities with duplicates).\n3.5 Supporting Inequality Joins with PUNQ\nThus far, the PUNQ model described in Section 2.4 sup-\nported queries that have equality joins only. This was done\nsince most of the recently developed cardinality estimation\nmodels M(e.g., MSCN) focus on supporting only such queries.\nThus, even if the PUNQ model were able to estimate the\nuniqueness rates of queries that have inequality joins, still\nthe extended model PUNQ( M) (e.g., PUNQ(MSCN)) will\nnot be able to estimate set-theoretic cardinalities of such\nqueries (inequality joins queries). This is due to the lack of\nsupport of inequality joins in the limited model M.\nHowever, supporting inequality joins is interesting and es-\nsential in real-world databases. Therefore, we describe a\nsimple modi\fcation to the PUNQ model architecture, so\nthat it can support inequality joins as well. As a result,\ngiven anylimited cardinality estimation model Mthat does\nsupport queries that have inequality joins (e.g., PostgreSQL,\nImproved PostgreSQL or CRN), using the revised PUNQ\nmodel, the PUNQ( M) model is also be able to estimates\nthe set-theoretic cardinalities of such queries.\nTo support inequality joins in the PUNQ model, we only\nrevised the query vectors' representation at the \frst stage\nof the model (Section 2.4) as follows (the second and third\nstages were not changed). We revised the join segments, so\nthat instead of using only the J1-seg and J2-seg segments\nthat were used to represent the (equality) joins, we added\na new segment, JO-seg to represent the speci\fc join opera-\ntor. As a result we can represent inequality joins as well as\nequality joins. Given any type of join ( col1; op; col 2),col1,\nopandcol2 are represented using one-hot vectors placed in\nthe J1-seg, JO-seg and J2-seg segments, respectively. The\nrevised vector segmentation scheme is described in Table 8\n(the highlighted cells are the only ones changed).\nType Att: Table Join (any kind ) Column Predicate\nSegment A-seg T-seg J1-seg JO-seg J2-seg C-seg O-seg V-seg\nSeg:size #C #T #C #O #C #C #O 1\nFeat: 1hot vec: 1hot vec: 1hot vec: 1hot vec: 1hot vec: 1hot vec: 1hot vec: norm:\nTable 8: Revised Vector Segmentation.\nExperimentally, by training the PUNQ model with queries\nthat have inequality joins, we found out the the model con-\nvergence time and quality of estimates (of the uniqueness\nrates) are very similar to those described in the previous\nsections. In particular, we found out that the number of\ninequality joins does not have a negative a\u000bect to the model\nestimates as the equality joins do. Due to limited space, we\nbrie\ry describe in Table 9 the q-errors results when examin-\ning a workload that includes 720 queries that have inequality\njoins (equally distributed with zero to \fveequality joins).\n50th 75th 90th 95th 99th max mean\nPUNQ 1:79 5 :29 16 :35 33 :84 130 274 9 :53\nPostgreSQL 4:94 21 :18 185 4417 25832 487069 2411\nPUNQ (Imp:Post:) 3 :18 17 :26 123 414 4932 44407 307\nPUNQ (CRN ) 2 :94 8 :96 47 :72 149 1175 7804 47 :94\nTable 9: Q-errors on inequality joins queries workload.4. SUPPORTING AND, OR & NOT\nSeveral cardinality estimation models consider only con-\njunctive queries (i.e., queries that only use the AND opera-\ntor). Other frequently used operators are the OR and NOT\noperators. To estimate the cardinalities of queries with OR\nand NOT with such models, we need to change their archi-\ntectures. Changing architectures is complex, as each model\nis structured in a di\u000berent way. Therefore, we introduce a\nuniform alternative approach for estimating cardinalities for\ngeneral queries, namely the GenCrd algorithm. Using this\napproach, we can use anylimited model, that only supports\nconjunctive queries, to estimate cardinalities for queries with\nthe AND, OR, and NOT operators.\n4.1 The GenCrd Algorithm\nConsider estimating the cardinality of a general query Q.\nThe GenCrd algorithm relies mainly on two observations.\n4.1.1 First Observation\nGiven a general query Qthat includes AND, OR, and\nNOT operators, we can represent Qas multiple conjunctive\nqueries, union-ed with OR. That is, query Qcan be trans-\nformed to a query of the form Q1OR Q 2OR ::: OR Q n.\nWe can therefore represent query Qas a list of conjunctive\nqueries [ Q1; Q2; :::; Q n] where each Qiincludes only AND\noperators (with the same SELECT and FROM clauses as in\nquery Q). This is done by converting Q's WHERE clause\ninto disjunctive normal form (DNF) [10, 35, 11], using sim-\nple logical transformation rules, and by considering each\nconjunctive disjunct as a separate query. For simplicity, we\nrefer to this list [ Q1; Q2; :::; Q n] as the DNF-list , and denote\nit asQ1;2;:::;n.\n4.1.2 Second Observation\nConsider estimating the cardinality of a general query\nQ, using its representing DNF-list of conjunctive queries\n[Q1; Q2; :::; Q n]. Query Q's cardinality can be calculated by\na simple algorithm, as follows5:\n\u000fCalculate a=jQ1j.\n\u000fCalculate b=jQ2;3;:::;nj.\n\u000fCalculate c=jQ1\\Q2;3;:::;nj.\n\u000fThen,jQj=a+b\u0000c:\nQuantity acan be calculated using any cardinality estima-\ntion model that supports conjunctive queries. Quantity bis\ncalculated, recursively, using the same algorithm, since the\nlist contains only conjunctive queries, and forms a proper\ninput for the algorithm. Similarly, quantity cis calculated,\nrecursively, as described below:\njQ1\\Q2;3;:::;nj=jQ1\\[Q2; Q3; :::; Q n]j (1)\nNote that Equation 1 is equivalent to Equation 2:\nj[Q1\\Q2; Q1\\Q3; :::; Q 1\\Qn]j (2)\nTherefore, we use the same recursive algorithm since the\nresulting list also contains conjunctive queries and forms a\nproper input with fewer queries.\n5We usej\u0001jbut we could also have used jj\u0001jj (j\u0001jandjj\u0001jj\nare de\fned in Section 2.1).\n8\n\nThe algorithm is exponential in the size of the DNF-list .\nGiven a query Qfor cardinality estimation, using the Gen-\nCrd algorithm, we call the cardinality estimation model at\nmost C(m) times, where m is the size of the representing\nDNF-list :\nC(m) = 2m\u00001\nNote that C(m) is an upper bound. As the number of OR\noperators is usually small, this expression is practically not\nprohibitive. Additionally, in Section 4.2 we describe the\nImplyFalse algorithm that reduces the potential exponential\nnumber of calls to the limited cardinality estimation model\nto practically linear number.\nGenCrdRec( DNF list):\nif len( DNF list) == 1:\nif ImplyFalse( DNF list[0]):\nreturn 0\nreturn Cardinality( DNF list[0])\nelse:\ncnjq= [DNF list[0]]\nsmaller list=DNF list[1:end]\nupdated list= [q\\cnjqForqinsmaller list]\nreturn GenCrdRec( cnjq)\n+ GenCrdRec( smaller list)\n- GenCrdRec( updated list)\nGenCrd( Q):\nreturn GenCrdRec(GetDNFlist( Q))\nFigure 7: The GenCrd Algorithm.\nIn Figure 7, Q1\\Q2 is the intersection query of Q1 and\nQ2 whose SELECT and FROM clauses are identical to Q1s\n(and Q2s) clauses, and whose WHERE clause is Q1s AND\nQ2s WHERE clauses. Function GetDNFlist returns the\nlist of conjunctive queries representing query Q. Function\nCardinality( Q) can be implemented by using any limited\ncardinality estimation model for estimating the cardinality\nof the given input conjunctive query Q(see a simple example\nin Figure 9).\nAs described in Figure 7, during the execution of the Gen-\nCrd algorithm we create multiple conjunctive queries from\nmultiple smaller queries. These queries may often contain\ncontradictory predicates. Therefore, to reduce the predic-\ntion time and errors, we use the ImplyFalse algorithm, be-\nfore directly using the cardinality estimation models. If Im-\nplyFlase returns True, then query Qhas zero-cardinality,\nand therefore we do not need to call the cardinality esti-\nmation model subroutine. This way, the actual number of\ntimes we call the cardinality estimation model is practically\nsmaller than the upper bound given in the formula for C(m).\nDepending on its implementation, Function Cardinality( Q)\nreturns the cardinality of Qwith, or without, duplicates. In\nthe \frst (resp., second) case, GenCrd will therefore return\ncardinalities with (resp., without) duplicates. We exhib-\nited a generic method to estimate set-theoretic cardinalities,\nusing PUNQ. Thus, if needed, we can implement function\nCardinality( Q) such that it returns set-theoretic cardinali-\nties.4.2 The ImplyFalse Algorithm\nThe ImplyFalse algorithm takes as input a conjunctive\nquery ( Q) with equality joins, and checks whether there are\nany contradictory predicates in Q.\nThe ImplyFalse algorithm runs in four main stages, as\ndescribed in Figure 8. (1) It \frst initializes three maps with\nsingle element classes. Initially, each class includes a single\ncolumn, with initial values accordingly. (2) In the \frst loop,\nit unions the classes of the columns that must be equal using\nthe function UnionClasses(c1,c2) which unions the classes\nof columns c1 and c2 into a single class. Hence, each class\nincludes all the columns that must have equal values. (3)\nIn the second loop, it updates the maps according to the\ncolumns' predicates. (4) Finally, in the last loop, it checks\nwhether there are any contradictory predicates.\nDetermining whether a conjunctive Boolean expression is\nequivalent to False has been shown to be a co-NP-complete\nproblem in its full generality [33, 3]. However, our case is\ntractable, and the problem is solved in linear time in the\nnumber of columns used in the input query, as described\nin Figure 8. This is due to the form of the examined con-\njunctive queries (comparison to constants). The examined\nconjunctive queries include joins of one type only, equality\njoin (col1,op,col2) where op is =. In addition, the columns'\npredicates are of the form (col,op,val) s.t. op 2[<;=; >]6.\nImplyFalse(Query Q):\nminVals =f[col]:\u00001j col is a column used in Qg\nmaxVals =f[col]:1jcol is a column used in Qg\nexactVals =f[col]:?jcol is a column used in Qg\nFor-each join clause (col1,=,col2) in Q:\nUnionClasses(col1,col2)\nFor-each column predicate (col,op,val) in Q:\nif op == ' >':\nminVals[col] = Max(val, minVals[col])\nif op == ' <':\nmaxVals[col] = Min(val, maxVals[col])\nif op == '=':\nif exactVals[col]6=?andexactVals[col]6= val:\nreturn True\nexactVals[col] = val\nFor-each col in all the columns used in Q:\nif maxVals[col]\u0014minVals[col]:\nreturn True\nif exactVals[col]6=?and not\nminVals[col]\u0014exactVals[col]\u0014maxVals[col]:\nreturn True\nreturn False\nFigure 8: The ImplyFalse Algorithm.\nIn Figure 8, the Map[ c] operator returns the corresponding\nvalue of the appropriate class in which column cis located.\nThe1;?symbols denote an in\fnity and uninitialized value,\nrespectively. Functions Min(x,y) and Max(x,y) return the\nminimum and the maximum between x and y, respectively.\n6Operator\u0014can be expressed with <;= and OR. Similarly,\noperator\u0015.\n9\n\nFigure 9: Converting a general SQL query Qwith AND, OR, and NOT operators into a DNF form, creating a list\nof conjunctive queries [Q1, Q2, Q3]. Q's cardinality is then calculated using the GenCrd algorithm with the DNF-list\nas input. The starred queries imply False, hence, they have zero-cardinality and are detected using the ImplyFalse\nalgorithm.\nThe end result of introducing the GenCrd algorithm is\ne\u000eciently and e\u000bectively extending estimation capabilities\nto a vaster class of queries. This eliminates the need to\ntrain models from scratch on this gigantic class of queries.\nIt also factors out this code for anyoptimizer which results\nin a better software architecture.\n5. GENCRD EVALUATION\nIn this section, we examine how well the GenCrd algo-\nrithm, transforms limited cardinality estimation models that\nonly handles conjunctive queries into ones that support queries\nwith the AND, OR, and NOT operators. To do so, we ex-\ntend the same four models that were used in the PUNQ\nevaluation, as described in Section 3. Each model was ex-\ntended by simply using the model in the GenCrd algorithm\nas a subroutine, replacing the Cardinality function used in\nthe algorithm, see Figure 7. For clarity, we denote the Gen-\nCrd extended model of Mas GenCrd( M).\nWe extend and evaluate each of the four examined limited\nmodels over two test workloads. In addition, we compare our\nresults with those of the PostgreSQL version 11 cardinality\nestimation component [1], as postgreSQL supports all SQL\nqueries. To estimate cardinality of the examined queries us-\ning PostgreSQL, we use the ANALYZE command to obtain\nthe estimated cardinalities.\nThe workloads were created using the same queries gener-\nator (using a di\u000berent random seed) as presented in Section\n2.3.2 with an additional step. Since the queries generator\ncreates only conjunctive queries, we recon\fgured it as fol-\nlows. After generating a conjunctive query Q, it randomly\nchooses a set Pout of Q's column predicates. For each\ncolumn predicate p= (col; op; val ) in the chosen set P, it\nrandomly creates a new predicate p0= (col0; op0; val0) and\nreplaces the original column predicate pinQwith the pred-\nicate p OR p0.col0may be the same as color changed ran-\ndomly. This also holds for op0andval0. We make sure that\nat least one of them is changed so that p0is not equivalent\ntop. Subsequently, the queries generator chooses another\nsetPfrom the updated Q's column predicates. Each col-\numn predicate p, in the chosen set P, is replaced with the\npredicate NOT p inQ. This way, we obtain a query that\nincludes the AND, OR, and NOT operators.\nIn order to ensure a fair comparison, the CRN and MSCN\nmodels were trained similarly, as noted in section 3.5.1 Evaluation Workloads\nThe evaluation uses the IMDb dataset, over two di\u000berent\nquery workloads:\n\u000fGenCrd test1, a synthetic workload with 450 unique\nqueries, with zero to two joins.\n\u000fGenCrd test2, a synthetic workload with 450 unique\nqueries, with zero to \fvejoins. This dataset is de-\nsigned to examine how the models generalize beyond\n2 joins.\nBoth workloads are equally distributed in the number of\njoins. From these workloads we generated DNF-lists , each\nrepresenting a general query. For each number of joins, the\nqueries are uniformly distributed in terms of the size of the\nrepresenting DNF-list , from 1 to 5. That is, each query has\nrepresenting DNF-lists of sizes 1 (conjunctive query without\nany OR and NOT operators), up to 5 (general query whose\nrepresenting DNF-list includes 5 such conjunctive queries).\nnumber of joins 0 1 2 3 4 5 overall\nGenCrd test1 150 150 150 0 0 0 450\nGenCrd test2 75 75 75 75 75 75 450\nTable 10: Distribution of joins. For each number of joins\nthe representing DNF-list size is equally distributed from\n1 to 5.\n5.2 The Quality of Estimates\nWe examined the GenCrd test1 workload using two state-\nof-the-art limited models, MSCN and CRN. Recall that this\nworkload includes queries with up to two joins, as MSCN\nand CRN were trained over such conjunctive queries.\nAlthough the MSCN and CRN models were initially tai-\nlored to estimate cardinalities for conjunctive queries only,\nexamining the results in Figure 11 and Table 11, it is appar-\nent that these models are successfully extended to estimate\ncardinalities accuratly for general queries using the GenCrd\nalgorithm.\n50th 75th 90th 95th 99th max mean\nPostgreSQL 1 :32 2:57 7 :42 16 :57 154 52438 132\nGenCrd (MSCN ) 2 :01 3 :59 7 :68 11 :97 94 :12 92684 232\nGenCrd (CRN ) 1 :68 2:83 6 :45 10 :65 30 :43 538 4 :83\nTable 11: Cardinality estimation q-errors on the Gen-\nCrdtest1 workload.\n10\n\nFigure 10: Cardinality estimation q-errors on the GenCrd test2 workload.\nFigure 11: Cardinality estimation q-errors on the Gen-\nCrdtest1 workload.\nNote that the GenCrd algorithm is an \"analytic\" algo-\nrithm, therefore it does not include any training, in contrast\nof the PUNQ model. Therefore, when using the GenCrd\nalgorithm, the number of joins, in queries in the DNF-list,\nhas no direct e\u000bect on the results accuracy. The quality of\nestimates merely depends on the quality of the limited car-\ndinality estimation models (that are used as subroutines in\nthe GenCrd algorithm).\n5.3 Generalizing to Additional Joins\nHere we examine the GenCrd algorithm with queries with\nmore than 2 joins. To do so we use the GenCrd test2 work-\nload which includes queries with zero to \fvejoins.\nAs can be seen in Figure 10, queries with 3 joins and more\nhave poorer estimates, as compared with the estimates of\nqueries with zero to two joins. This decline in quality is not\ndirectly due to the GenCrd algorithm. The decline stems\nfrom training the MSCN and CRN models over conjunctive\nqueries that have only up to two joins. Thus, when MSCN\nand CRN are used as GenCrd subroutines, DNF-list queries\nwith more joins are estimated not as well by the original\nMSCN, or CRN, models.\nComparing conjunctive queries' cardinality estimates us-\ning a limited model ( M), and general queries' cardinality\nestimates using the extended model (GenCrd( M)), it ap-\npears that there is no signi\fcant change in overall quality\nin terms of q-error. That is, GenCrd maintains the same\nquality of estimates as the original model's quality of es-\ntimates, when Mis used as subroutine. This is expected,\nsince the GenCrd( M) algorithm obtains an estimation for a\ngiven query Qby simply summing and subtracting several\ncardinality estimates (see example in Figure 9), where each\nof these estimates is obtained using the limited model ( M).Table 12 and Figure 10 display the experimental results\nover the GenCrd test2 workload. Recall that all the limited\nmodels were initially tailored to estimate cardinalities for\nconjunctive queries only. Hence, these results highlight the\nusefulness of GenCrd method in extending limited models.\n50th 75th 90th 95th 99th max mean\nPostgreSQL 8:57 168 3139 12378 316826 647815 8811\nGenCrd (MSCN ) 4 :17 84 :92 1887 6769 60405 278050 2611\nGenCrd (Impr :Post:) 2 :18 10 :97 82 :75 286 2363 162894 750\nGenCrd (Impr :MSCN ) 2 :89 8 :45 27 :1 73 :59 537 5183 38 :43\nGenCrd (CRN ) 2:26 6 :03 17 :49 71 :17 632 6025 47 :24\nTable 12: Cardinality estimation q-errors on the Gen-\nCrdtest2 workload.\n5.4 GenCrd Cardinality Prediction Time\nExamining the results of the experiments in Sections 5.2\nand 5.3, we \fnd that the size of the DNF-list does not a\u000bect\nthe quality of the estimates. That is, queries with repre-\nsenting DNF-lists of di\u000berent sizes, are estimated similarly\nas long as they have the same number of joins (the higher\nthe number of joins is, the worst the results are). However,\ntheDNF-list size directly a\u000bects the prediction time. The\nlarger the DNF-list is, the larger the prediction time is.\nTable 13 depicts the average prediction time in millisec-\nonds, for estimating the cardinality of a single query, when\nexamining di\u000berent models, with di\u000berent DNF-list sizes.\nNote that the PostgreSQL prediction time is not a\u000bected\nby the size of the DNF-list , since it does not use GenCrd.\nModel \u001fDNF\u0000list size 1 2 3 4 5\nPostgreSQL 1.82 1.82 1.82 1.82 1.82\nGenCrd (MSCN ) 0.25 0.6 0.87 1.25 1.78\nGenCrd (Imp:Post:) 75 166 261 374 541\nGenCrd (Imp:MSCN ) 35 77 120 186 254\nGenCrd (CRN ) 15 33 53 72 105\nTable 13: Avg prediction time of a single query in ms.\nDespite the prediction time increase, time is still in the\norder of milliseconds7. Furthermore, the GenCrd algorithm\ncan easily be parallelized by estimating the cardinality of all\nthe sub-queries in parallel. I.e., by parallelizing all the calls\nmade to the limited model. The estimation times described\nin Table 13 are based on the sequential version of GenCrd.\n7On average, over the GenCrd test2 workload's queries,\nthe prediction time for a single query when using Gen-\nCrd(CRN), is 55 ms, while the actual query's execution time\nis 240000 ms (4 minutes).\n11\n\n6. RELATED WORK\nConjunctive queries have been intensely researched; see\nfor example [33, 3, 14]. Queries that include the AND, OR,\nand NOT operators in their WHERE clauses constitute a\nbroad class of frequently used queries. Their expressive\npower is roughly equivalent to that of the relational alge-\nbra. Therefore, this class of queries had been extensively\nresearched early on by the DB theory community. Yan-\nnakakis and Sagiv [30] showed that testing equivalence of re-\nlational expressions with the operators select, project, join,\nand union is complete for the \u0005p\n2of the polynomial-time\nhierarchy. Chandra and Merlin [7] showed that determin-\ning containment of conjunctive queries is an NP-complete\nproblem. This also holds under additional settings [33, 3,\n14].\nEstimating cardinalities of such queries was also intensely\nresearched early on, due to its implications for query op-\ntimization [31]. Many techniques were proposed to solve\nthis problem, e.g., Random Sampling techniques [5, 27], In-\ndex based Sampling [21], and recently using neural networks\n(NN) [13, 17].\nThe introduction of machine learning techniques led to\nsigni\fcant improvements in many known estimation prob-\nlems in databases. Query indexing [18], query optimization\n[19, 28], concurrency control [4], are some of the problems\nfor which solutions were improved using machine learning\ntechniques. In particular, cardinality estimates for conjunc-\ntive queries were signi\fcantly improved, by using two recent\nNN-based models, MSCN [17], and the CRN models [13].\nThe MSCN model [17] is a sophisticated NN tailored for\nrepresenting conjunctive SQL queries and predicting car-\ndinalities. Technically, it ingeniously presents queries that\nvary in their structure to a single \fxed NN. MSCN has been\nshown to be superior in estimating cardinalities for conjunc-\ntive queries that have the same number of joins as that in\nthe queries training dataset. However, MSCN proved less\ne\u000bective when considering queries with more than two joins\nas it is not trained on such queries.\nExtending [17], the work [13] proposed a method, Cnt2Crd,\nfor estimating cardinalities using containment rates. Cnt2Crd\nrequires a queries pool that maintains information about\npreviously executed queries. Given a new query Q, whose\ncardinality needs to be estimated, the Cnt2Crd method \frst\nestimates the containment rates of the input query Qwith\nthe other queries in the pool. This is done using any contain-\nment rate estimation model. ([13] uses a NN-based model for\nthis task). Then, using the true cardinalities of the queries\nsaved in the queries pool, along with associated containment\nrates, Q's estimated cardinality is obtained. This approach\nprovides a higher estimation quality for estimating cardi-\nnalities of conjunctive queries, compared with other models\n(e.g. MSCN), especially, in case there are multiple joins.\nRecently, a specialized model for estimating set-theoretic\ncardinalities was proposed in [16]. The model proposed\nin [16] is very similar to the MSCN model, with minor\nchanges in the model architecture. However, the [16] model\nestimates set-theoretic cardinalities on single tables only\n(queries with 0 joins). In this paper, we proposed an alter-\nnative approach, PUNQ, that can be adapted to anymodel,\nand in particular to the MSCN model, without the need to\nchange it. As a result, PUNQ(MSCN) supports a larger\nand harder class of queries (which include several joins than\nsupported by [16]).7. CONCLUSIONS AND FUTURE WORK\nEstimating set-theoretic cardinalities (i.e., without dupli-\ncates), is a fundamental problem in query planning and opti-\nmization. Recently suggested NN-based models only handle\nthe case of predicting cardinality with duplicates. Convert-\ning each such model to provide set-theoretic estimates is\ncomplex. We introduce a uniform way for conversion by\nintroducing a neural network, PUNQ, for providing the ra-\ntio between the set-theoretic estimate and the estimate with\nduplicates.\nAnother de\fciency of recent models is the lack of support\nfor more general queries, i.e., ones employing the AND, OR\nand NOT operators. To overcome this de\fciency without al-\ntering existing restricted models (that handle only conjunc-\ntive queries), we introduce a recursive algorithm, GenCrd.\nGenCrd provides estimates for queries employing the AND,\nOR and NOT operators. GenCrd uses existing models, un-\naltered, as subroutines. This provides a uniform extension of\nany restricted estimator into one supporting general queries.\nFor both extensions, providing set-theoretic estimates and\nestimating richer queries, we conducted extensive experi-\nmentation. The experiments show that both extensions are\naccurate and highly practical. Thus, we believe that these\nextension methods are highly promising and practical for\nsolving the cardinality estimation problem.\nThus far, for the PUNQ model, we assumed that the\ndatabase is static, i.e., no updates. The training of the\nPUNQ model was performed on an immutable snapshot of\nthe database. Nevertheless, changes in the database content\nand even in the database schema will occur in a real-world\ndatabase. Upon schema change, we can rebuild the model\naccording to the updated schema, and retrain it, with a new\nqueries training set; recall that the PUNQ model's training\ntime is in tens of minutes. For retraining the PUNQ model,\nwe need to include the compute cost for obtaining the up-\ndated uniqueness rates of queries (on the updated database).\nMore practically, we can incrementally train the PUNQ\nmodel, with new updated queries along with their unique-\nness rates. In this solution, handling updates to the database\nschema is di\u000ecult as the PUNQ model neural networks di-\nmensions are determined at the outset, according to the\nnumber of tables and columns in the schema when the PUNQ\nmodel is \frst built. To solve this, when \frst building the\nPUNQ model we can use larger numbers than the actual\nnumbers of tables and columns. This allows adding tables\nand columns easily and performing retraining incrementally.\nGenCrd is an analytic algorithm, which is not based on a\nspeci\fc database. Thus, unlike the PUNQ model, the Gen-\nCrd algorithm is not directly a\u000bected by changes that occur\nin the database. However, the models that are used in it as\nsubroutines (e.g., MSCN or CRN), are a\u000bected by changes\nin the database. Therefore, in order to ensure accurate esti-\nmates when using the GenCrd method, the models that are\nused as subroutines in GenCrd need be up-to-date.\nInterestingly, with the extensions we presented, we can\nestimate cardinalities of queries with GROUP BY (the car-\ndinality is that of the same query with a modi\fed SELECT\nCLAUSE, i.e., selecting on DISTINCT grouping attributes).\nIn the future, we plan to generalize our extensions to queries\nwith a HAVING clause as well as ones using EXISTS.\n12\n\n8. REFERENCES\n[1] . Postgresql, the world's most advanced open source\nrelational database. https://www.postgresql.org/ , .\n[2] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis,\nJ. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard,\nM. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G.\nMurray, B. Steiner, P. Tucker, V. Vasudevan,\nP. Warden, M. Wicke, Y. Yu, and X. Zheng.\nTensor\row: A system for large-scale machine learning.\nIn12th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI 16) , pages\n265{283, 2016.\n[3] S. Abiteboul, R. Hull, and V. Vianu. Foundations of\nDatabases . Addison-Wesley, 1995.\n[4] R. Bordawekar and O. Shmueli, editors.\naiDM@SIGMOD . ACM, 2019.\n[5] M. Bressan, E. Peserico, and L. Pretto. Simple set\ncardinality estimation through random sampling.\nCoRR , abs/1512.07901, 2015.\n[6] W. Cai, M. Balazinska, and D. Suciu. Pessimistic\ncardinality estimation: Tighter upper bounds for\nintermediate join cardinalities. In Proceedings\nSIGMOD , pages 18{35, 2019.\n[7] A. K. Chandra and P. M. Merlin. Optimal\nimplementation of conjunctive queries in relational\ndata bases. In Proceedings of the 9th Annual ACM\nSymposium on Theory of Computing , pages 77{90,\n1977.\n[8] M. Charikar, S. Chaudhuri, R. Motwani, and V. R.\nNarasayya. Towards estimation error guarantees for\ndistinct values. In Proceedings of the Nineteenth ACM\nSIGMOD-SIGACT-SIGART Symposium on Principles\nof Database Systems , pages 268{279, 2000.\n[9] J. Cli\u000bord and R. King, editors. Proceedings the 1991\nACM SIGMOD International Conference on\nManagement of Data . ACM Press, 1991.\n[10] B. A. Davey and H. A. Priestley. Introduction to\nLattices and Order, Second Edition . Cambridge\nUniversity Press, 1990.\n[11] W. A. David Hilbert. Principles of Mathematical\nLogic . American Mathematical Soc., 1999.\n[12] P. J. Haas, J. F. Naughton, S. Seshadri, and L. Stokes.\nSampling-based estimation of the number of distinct\nvalues of an attribute. In Proceedings of VLDB , pages\n311{322, 1995.\n[13] R. Hayek and O. Shmueli. Improved cardinality\nestimation by learning queries containment rates. In\nhttps://arxiv.org/abs/1908.07723 , 2019.\n[14] D. S. Johnson and A. C. Klug. Testing containment of\nconjunctive queries under functional and inclusion\ndependencies. J. Comput. Syst. Sci. , 28(1):167{189,\n1984.\n[15] D. P. Kingma and J. Ba. Adam: A method for\nstochastic optimization. In Proceedings ICLR , 2015.\n[16] A. Kipf and M. Freitag. Estimating \fltered group-by\nqueries is hard : Deep learning to the rescue. In\nProceedings AIDB , 2019.\n[17] A. Kipf, T. Kipf, B. Radke, V. Leis, P. A. Boncz, and\nA. Kemper. Learned cardinalities: Estimating\ncorrelated joins with deep learning. In Proceedings\nCIDR , 2019.[18] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and\nN. Polyzotis. The case for learned index structures. In\nProceedings SIGMOD , pages 489{504, 2018.\n[19] S. Krishnan, Z. Yang, K. Goldberg, J. M. Hellerstein,\nand I. Stoica. Learning to optimize join queries with\ndeep reinforcement learning. CoRR , 2018.\n[20] V. Leis, A. Gubichev, A. Mirchev, P. A. Boncz,\nA. Kemper, and T. Neumann. How good are query\noptimizers, really? PVLDB , 9(3):204{215, 2015.\n[21] V. Leis, B. Radke, A. Gubichev, A. Kemper, and\nT. Neumann. Cardinality estimation done right:\nIndex-based join sampling. In Proceedings CIDR , 2017.\n[22] V. Leis, B. Radke, A. Gubichev, A. Mirchev, P. A.\nBoncz, A. Kemper, and T. Neumann. Query\noptimization through the looking glass, and what we\nfound running the join order benchmark. VLDB J. ,\n27(5):643{668, 2018.\n[23] R. Marcus and O. Papaemmanouil. Deep\nreinforcement learning for join order enumeration. In\naiDM@SIGMOD , 2018.\n[24] G. Moerkotte, T. Neumann, and G. Steidl. Preventing\nbad plans by bounding the impact of cardinality\nestimation errors. PVLDB , 2(1):982{993, 2009.\n[25] R. Nambiar and M. Poess, editors. TPCTC , volume\n10661 of Lecture Notes in Computer Science . Springer,\n2018.\n[26] C. Nwankpa, W. Ijomah, A. Gachagan, and\nS. Marshall. Activation functions: Comparison of\ntrends in practice and research for deep learning.\nCoRR , abs/1811.03378, 2018.\n[27] F. Olken and D. Rotem. Random sampling from\ndatabase \fles: A survey. In SSDBM , pages 92{111,\n1990.\n[28] J. Ortiz, M. Balazinska, J. Gehrke, and S. S. Keerthi.\nLearning state representations for query optimization\nwith deep reinforcement learning. In\nDEEM@SIGMOD , pages 4:1{4:4, 2018.\n[29] L. Prechelt. Early stopping - but when? In Neural\nNetworks: Tricks of the Trade - Second Edition , pages\n53{67. 2012.\n[30] Y. Sagiv and M. Yannakakis. Equivalences among\nrelational expressions with the union and di\u000berence\noperators. J. ACM , 27(4):633{655, 1980.\n[31] P. G. Selinger, M. M. Astrahan, D. D. Chamberlin,\nR. A. Lorie, and T. G. Price. Access path selection in\na relational database management system. In\nProceedings ACM SIGMOD , pages 23{34, 1979.\n[32] C. Sirangelo. Positive relational algebra. In\nEncyclopedia of Database Systems, Second Edition .\n2018.\n[33] J. D. Ullman. Principles the Database and\nKnowledge-Base Systems, Volume II . Computer\nScience Press, 1989.\n[34] K. Whang, B. T. V. Zanden, and H. M. Taylor. A\nlinear-time probabilistic counting algorithm for\ndatabase applications. ACM Trans. Database Syst. ,\n15(2):208{229, 1990.\n[35] J. E. Whitesitt. Boolean algebra and its applications .\nCourier Corporation, 1961.\n[36] L. Woltmann, C. Hartmann, M. Thiele, D. Habich,\nand W. Lehner. Cardinality estimation with local deep\nlearning models. In aiDM@SIGMOD , 2019.\n13",
  "textLength": 68179
}