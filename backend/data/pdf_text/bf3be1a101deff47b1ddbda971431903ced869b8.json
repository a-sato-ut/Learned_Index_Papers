{
  "paperId": "bf3be1a101deff47b1ddbda971431903ced869b8",
  "title": "L-rCBF: Learning-Based Key–Value Data Structure for Dynamic Data Processing",
  "pdfPath": "bf3be1a101deff47b1ddbda971431903ced869b8.pdf",
  "text": "Citation: Lee, Y.; Byun, H. L-rCBF:\nLearning-Based Key–Value Data\nStructure for Dynamic Data\nProcessing. Appl. Sci. 2023 ,13, 12116.\nhttps://doi.org/10.3390/\napp132212116\nAcademic Editor: Giacomo Fiumara\nReceived: 5 October 2023\nRevised: 28 October 2023\nAccepted: 4 November 2023\nPublished: 7 November 2023\nCopyright: © 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\napplied  \nsciences \nArticle\nL-rCBF: Learning-Based Key–Value Data Structure for Dynamic\nData Processing\nYejee Lee and Hayoung Byun *\nDepartment of Electronics Engineering, Myongji University, Yongin 17058, Republic of Korea;\nyejee0821@mju.ac.kr\n*Correspondence: hbyun@mju.ac.kr\nAbstract: Learning-based data structures, such as a learned Bloom ﬁlter and a learned functional\nBloom ﬁlter (L-FBF), have recently been proposed to replace traditional structures. However, using\nthese structures for dynamic data processing is difﬁcult because a speciﬁc element cannot be deleted\nfrom a trained model. A counting Bloom ﬁlter with return values (rCBF) is a more efﬁcient key–value\nstructure than a functional Bloom ﬁlter (FBF) for repetitive insertions and deletions. In this study, we\npropose a learned rCBF (L-rCBF) comprising a model, a Bloom ﬁlter, and an rCBF and the deletion\nalgorithm for the L-rCBF. To delete a speciﬁc element from the L-rCBF, two different operations are\nperformed according to four different cases. In the experiments, the proposed L-rCBF is compared\nwith a single rCBF and an L-FBF in terms of undeletables and search failures, and this comparison is\nconducted using two different models. In addition, we present a theoretical analysis of the rCBF with\nexperimental results to demonstrate that a structure with an rCBF is more suitable for dynamic data\nthan a structure with an FBF.\nKeywords: key–value data structure; Bloom ﬁlter; learning-based structure; dynamic data; deep\nlearning\n1. Introduction\nA Bloom ﬁlter (BF) is a simple probabilistic data structure using hash functions [ 1].\nThe BF can be used to identify elements included in a stored set and ﬁlter malicious\ninformation. The BF may return false positives for non-elements; however, it ensures no\nfalse negatives , which means that the BF returns positives for all programmed elements.\nBecause it is space-efﬁcient, numerous variants of BFs have been proposed and used in\nvarious applications, such as IP address lookup [ 2,3], named data networking (NDN) [ 4–6],\npacket classiﬁcation [ 7,8], distributed systems [ 9–11], network security [ 12,13], and cloud\ncomputing [ 14,15]. A counting Bloom ﬁlter (CBF) and stable Bloom ﬁlter (SBF), both BF\nvariants using counters, were proposed to support deletions for dynamic data due to the\nlimitation of the standard BF, which can be used only for static data [16–18].\nA functional Bloom ﬁlter (FBF) [ 19,20], a BF variant, is a key–value data structure\ncapable of returning a value corresponding to a key in a given set, and it can replace tree-\nand hash-based structures in numerous applications. A standard BF is a bit array, while\nan FBF is an array of cells composed of multiple bits. In other words, a BF is used for a\nbinary classiﬁcation task because it can only answer membership queries, while an FBF is\nused for a multi-class classiﬁcation task. The FBF provides insertion and search operations,\nand depending on the type of data (i.e., dynamic or static data), the FBF can be designed to\nsupport or not support a deletion operation. The FBF is more suitable for static data than\ndynamic data because an FBF designed for static data is better than that for dynamic data\nin terms of search performance. In other words, the FBF for dynamic data has a weak point:\nconﬂict cells, which are cells with two or more elements inserted, cannot be used for FBF\noperations; hence, if elements are repeatedly inserted and deleted, the number of conﬂict\ncells increases, resulting in a degradation of the search performance of the FBF.\nAppl. Sci. 2023 ,13, 12116. https://doi.org/10.3390/app132212116 https://www.mdpi.com/journal/applsci\n\nAppl. Sci. 2023 ,13, 12116 2 of 17\nA counting Bloom ﬁlter with return values (rCBF) is a key–value data structure suitable\nfor applications that handle dynamic data, such as the pending interest table (PIT) lookup\nin NDN [ 5]. Unlike a BF, an rCBF is an array of cells composed of multiple bits and is used\nfor a multi-class classiﬁcation task. Each cell in the rCBF consists of two ﬁelds: counter\nand value . The rCBF provides the same functionalities as the FBF and supports dynamic\napplications better than the FBF because the rCBF uses counters. Because cells with two\nor more elements inserted can be used for rCBF operations, if insertions and deletions are\nrepeated, replacing the FBF with an rCBF is appropriate.\nSeveral data structures applying machine learning have recently been proposed [ 20–22].\nLearning-based structures can guarantee the same operations and characteristics as tra-\nditional data structures and enhance the performance of the structures. A learned Bloom\nﬁlter (LBF), a learning-based data structure for a binary classiﬁcation task, provides the\nsame semantic guarantees as a BF and comprises a learned model and a BF [ 21,22]. A\nlearned FBF (L-FBF), a learning-based key–value structure for a multi-class classiﬁcation\ntask, provides the same semantic guarantees as an FBF and comprises a learned model,\nBF, and FBF [ 20]. The LBF and L-FBF can signiﬁcantly improve search performance (i.e.,\nsearch failure rates, including false positive rates) when using the same memory size as\nthe BF and FBF, respectively. However, the LBF and L-FBF cannnot be used for dynamic\ndata processing because the elements stored in the structures cannot be deleted. Hence,\nlearning-based BF structures have the following problems.\n• Designing a deletion algorithm for a learning-based structure is difﬁcult because once\na model is trained, it does not provide a deletion for a speciﬁc element; hence, most\nexisting learning-based structures cannot be used for dynamic data processing [ 21,22].\n• To delete speciﬁc elements, learning-based structures should utilize auxiliary struc-\ntures other than the learned model. If a deletion algorithm for a learning-based struc-\nture can be designed, the application range in which the structure can be used will\nwiden considerably.\n• In the case of the LBF, a deletion operation cannot be provided because the auxiliary\nstructure is a standard BF. For the deletion operation, the standard BF should be\nreplaced with an updatable BF variant for dynamic data. A stable learned Bloom ﬁlter\n(SLBF) is the ﬁrst LBF variant for dynamic data and consists of a learned model and\nSBFs [23]. However, the SLBF can be used only for binary classiﬁcation.\n• The L-FBF was presented for static data in [ 20]; hence, a deletion algorithm of the\nL-FBF has never been proposed. In other words, because the auxiliary structures of\nthe L-FBF in [ 20] are a standard BF and FBF for static data, a deletion operation for the\nL-FBF cannot be provided.\nIn this study, we focus on key–value data structures using a learned model for dynamic\ndata. We propose a learned rCBF (L-rCBF) to improve the search and deletion performances\nunder the constraint of the same memory size as a single rCBF and propose a deletion\nalgorithm for the L-rCBF to process dynamic data. To the best of our knowledge, the L-rCBF\nis the ﬁrst LBF variant for key–value storage that can support deletions. The contributions\nof this study can be summarized as follows.\n• We propose a learning-based key–value structure for dynamic data processing, called\nL-rCBF, comprising a learned model, BF, and rCBF. The L-rCBF can be used for multi-\nclass classiﬁcation. The memory requirement of the learned model does not increase\nin proportion to the size of the data; therefore, as the data size increases, the proposed\nL-rCBF is more efﬁcient than an rCBF.\n• We propose a deletion algorithm for the L-rCBF. Because the rCBF is suitable for\ndynamic data as an auxiliary structure, the deletion algorithm can be designed even\nthough the other auxiliary structure is a standard BF. To delete an element, the algo-\nrithm performs one of two operations according to four cases: deleting the element\nfrom the rCBF in the L-rCBF or programming it into the BF in the L-rCBF.\n\nAppl. Sci. 2023 ,13, 12116 3 of 17\n• The proposed L-rCBF is constructed similarly to the L-FBF. Hence, if the FBF for static\ndata is replaced with that for dynamic data in the L-FBF, the deletion algorithm for\nthe L-rCBF can be applied to the L-FBF.\n• We theoretically analyzed the probabilities of undeletables and search failures for an rCBF\nand demonstrated the superiority of the proposed L-rCBF through simulation results.\nThe remainder of this paper is organized as follows. Section 2 describes BF variants,\nincluding an FBF and rCBF, and network applications using the BF variants. Section 3\ndescribes the proposed L-rCBF and deletion algorithm. Section 4 describes the theoretical\nanalysis, including the undeletable and search failure probabilities of the rCBF. Section 5\ncompares the performances of the L-rCBF with a single rCBF and L-FBF, and compares\nthe probabilities between the theoretical and experimental results for the rCBF. Finally,\nSection 6 concludes the paper.\n2. Related Work\n2.1. Functional Bloom Filter\nThe functional Bloom ﬁlter (FBF) is a data structure that stores values corresponding\nto keys, and the key–value pairs are included in set S=f(x1,v1),(x2,v2),\u0001\u0001\u0001,(xn,vn)g.\nAn FBF is an m-cell array, and each cell stores a value. The FBF can perform three opera-\ntions: insert (program), search (query), and delete. The FBF can be conﬁgured to support\nthe deletion operation depending on whether the data are static or dynamic. The insert\noperation for dynamic data is slightly different from that for static data. If deletions are\ninfrequent, the FBF is constructed for static data because the search performance of the FBF\nfor static data is better than that for dynamic data. Most previous studies described the FBF\nfor static data without considering deletions; however, we explain an FBF that supports\ndeletion for dynamic data.\nThe operations use khash functions to access kcells. The optimal kfor the BF structures\ncan be calculated as follows [24,25]:\nk=m\nnln2, (1)\nwhere ndenotes the number of elements in set S. Let Lbe the number of bits in a single\ncell for a value. Each cell is initialized to 0 and can represent 2L\u00002values because the\nmaximum value 2L\u00001is reserved to represent a conﬂict cell, which implies that two or\nmore elements are stored in the cell.\nIn inserting an element ( xj,vj) into an FBF for dynamic data, for 1\u0014j\u0014n, all cells\nwith value 0 among the cells pointed to by khash functions are set to value vj, and the\nother cells, which have values different from 0, are set to conﬂict cells with 2L\u00001. For an\nFBF for static data, the cells that already have the value vjretain this value even if the keys\nalready stored in the cells are not equal to xj.\nIn searching for an input in set U(i.e., U=S[Sc), an FBF returns one of three search\nresults: negative ,positive , orindeterminable (INDET). If at least one of the kcells has a value of\n0 or if two or more cells among the cells have different values, a negative is returned from\nthe FBF. If every cell except for the conﬂict cells has the same value, the value is returned,\nwhich implies a positive; however, positives may be false owing to hash collisions. If all\ncells have value 2L\u00001(i.e., conﬂict cells), an INDET is returned. False positive (FP) and\nINDET results are classiﬁed as search failures .\nIn deleting an element in an FBF for dynamic data, only cells storing one element can\nbe used among the kcells. That is, among the cells pointed to by the khash functions, the\ncells, except for the conﬂict cells, are changed to 0. Conﬂict cells with two or more elements\ninserted are not used for deletion to prevent false negatives (FNs). If all accessed kcells are\nconﬂict cells, the element cannot be deleted and is called an undeletable . In addition, the FBF\nfor static data does not support deletion operations.\n\nAppl. Sci. 2023 ,13, 12116 4 of 17\nEven though the FBF operations can be adjusted for dynamic data, the FBF is not\nperfectly suitable for frequent insertions and deletions because conﬂict cells cannot be used\nfor the three operations.\nIn addition, in the three operations of BF structures (i.e., FBF and rCBF), the cells\npointed to by khash functions are accessed. Once a BF structure is constructed, the kdoes\nnot change while repeating insertions and deletions, unless the structure is reconstructed.\nHence, the time complexity of the operations is O(1).\n2.2. Counting Bloom Filter with Return Values\nThe counting Bloom ﬁlter with return values (rCBF) is a key–value data structure\nsuitable for dynamic data processing [ 5]. An rCBF is an m-cell array, where each cell\nconsists of counter and value ﬁelds. The rCBF performs three operations using khash\nfunctions: insert (program), search (query), and delete. The optimal kused for the rCBF\ncan be calculated usingm\nnln2as in (1), where nis the number of elements in set S=\nf(x1,v1),(x2,v2),\u0001\u0001\u0001,(xn,vn)g[24,25].\nLetLbe the number of bits in the value ﬁeld and Rthe number of bits in the counter\nﬁeld. Each value ﬁeld can represent 2L\u00001values because the rCBF does not need to reserve\na value to represent a conﬂict cell, unlike the FBF, which can represent 2L\u00002values. Each\ncounter ﬁeld represents the number of inserted elements ( c) in each cell, and the maximum\ncount cmaxis 2R\u00001.\nFigure 1 presents an example of inserting elements into an rCBF with n=3,m=7,\nand k=3. To insert element ( x3,v3) into the cells pointed to by h1(x1),h2(x1), and h3(x1),\nv3is XORed with the values in the cells (i.e., rCBF [hi(x3)].value = rCBF [hi(x3)].value\bv3\nfori= 1, 2, 3), and all counters in the cells are incremented by 1, except for counters with\ncmax(i.e., rCBF [hi(x3)].count = rCBF [hi(x3)].count + 1).\nFigure 1. Counting Bloom ﬁlter with return values (rCBF).\nIn the search operation, an rCBF returns one of three search results: negative ,positive , or\nindeterminable (INDET). For a given input in set U(i.e., U=S[Sc), if at least one counter\nin the kcells is 0 or if two or more cells among the cells with c=1have different values, a\nnegative is returned from the rCBF. If the value ﬁelds in cells with c=1are identical, the\nvalue in the ﬁelds is returned, which implies a positive; however, positives may be false\nowing to hash collisions. If all counters are greater than 1, an INDET is returned. FP and\nINDET results are classiﬁed as search failures .\nIn the rCBF, even cells with two or more elements inserted can be used to delete\nelements; hence, elements can be deleted efﬁciently, improving the deletion performance.\nAlgorithm 1 shows the procedure for deleting an element from the rCBF. To delete element\n(x,v) inS,vis XORed with the values in the cells pointed to by the khash functions, and\nall counters in the cells are decremented by 1, except for counters with 0 or cmax. For a cell\nwith cmax, the counter in the cell should not be decremented to prevent an FN because the\ncell may have more than cmaxelements. If the accessed kcells have counters with cmax, the\nelement cannot be deleted and is called an undeletable . However, the deletion performance\nof the rCBF is better than that of the FBF because the rCBF can use cells with hash collisions,\nexcept for those with cmax.\n\nAppl. Sci. 2023 ,13, 12116 5 of 17\nAlgorithm 1 Deletion algorithm of rCBF\n1:procedure delete _rCBF (rCBF, x,v)\n2: fori 1 to kdo\n3: ifrCBF [hi(x)].count !=cmax\n&& rCBF [hi(x)].count !=0then\n4: rCBF [hi(x)].value = rCBF [hi(x)].value\bv\n5: rCBF [hi(x)].count = rCBF [hi(x)].count\u00001\n2.3. Network Applications Using Bloom Filter Variants\nBF variants have been used in various network applications, such as IP address lookup,\nNDN, packet classiﬁcation, and distributed systems, because BF structures can ﬁlter and\nsearch for speciﬁc information and the structures are space-efﬁcient. Depending on the\npurpose of each application, appropriate BF variants can be utilized. For IP address lookup,\nan efﬁcient preﬁx-matching algorithm is required to identify the longest preﬁx. Because a\nBF can ﬁlter out unnecessary accesses, parallel BF and BF-chaining methods were proposed\nfor longest preﬁx matching [2,3].\nIn NDN, efﬁcient name lookup algorithms are required in forwarding information base\n(FIB) and pending interest table (PIT). For FIB lookup, because name-based longest preﬁx\nmatching should be performed for URL data, an index data structure was proposed with a\nhash table and a CBF to balance the load [ 4]. For PIT lookup, an efﬁcient exact-matching\nalgorithm is required, and frequent update operations (i.e., insertions and deletions) are\nperformed. Hence, BF-based PIT architectures were proposed using the FBF and rCBF [ 5].\nPacket classiﬁcation is a technique to categorize network trafﬁc using multiple header\nﬁelds to implement quality of service. In software deﬁned networking (SDN), because more\nﬁelds are used for packet classiﬁcation, the ﬂow table with all the information is stored in\nan external memory. Packet classiﬁcation algorithms using BF variants, such as a standard\nBF, cuckoo BF, and LBF, were proposed to reduce the number of accesses to the external\nmemory [7,8].\nIn distributed systems, two tasks exist: reconciliation to obtain the union of the sets of\ntwo hosts and data deduplication to eliminate duplicate data between two hosts [ 9,10,15].\nIn the tasks, the set difference between two hosts should be efﬁciently computed, and\ncommunication complexity should be minimized. Because of space and access efﬁciencies,\nBF structures (i.e., a standard BF and an invertible BF) were used for the tasks [9,11].\n3. Proposed Work\nThis paper proposes a learning-based key–value structure, learned rCBF (L-rCBF),\nwhich can provide deletion for dynamic data. Learning-based structures can achieve better\nperformance with the same amount of memory as traditional structures. The basic concept\nof this structure was brieﬂy introduced in [26] by the same authors.\n3.1. Learned rCBF\nWe aim to propose a learning-based data structure that guarantees the same semantic\ncharacteristics as a single rCBF suitable for dynamic data processing. A learned model for\nmulti-class classiﬁcation can be used to construct a key–value data structure because the\nstructure can be considered a multi-class classiﬁcation task with Qclasses if Qvalues are\navailable for the key–value pairs. However, the model is not fully representative of the\ncharacteristics of an rCBF because the model can return false negatives (FNs) and false-class\nresults (FRs), while the rCBF never produces an FN or FR. Hence, appropriate auxiliary\nstructures should be added to provide the same semantic guarantees as the rCBF.\nThe proposed L-rCBF comprises a learned model, false-class result BF (FR-BF), and\nveriﬁcation rCBF (V-rCBF) to guarantee the same semantic characteristics as a single rCBF.\nThe model has the advantage of maintaining a consistent memory requirement regardless\nof the size of the positive data; however, it may produce FNs by returning negative class\n\nAppl. Sci. 2023 ,13, 12116 6 of 17\n0 for elements in positive class v(i.e., classiﬁed as non-elements included in Sc), and FRs\nby returning positive class qfor elements belonging to class vfor1\u0014v,q\u0014Qand v6=q.\nBecause the rCBF never produces an FN or FR, a single BF and rCBF should be constructed\nto prevent FNs and FRs.\nBecause both the rCBF and FBF are BF structures that can be used as key–value\nstructures, even though the rCBF is better than the FBF in terms of dynamic data processing,\nthe only difference between L-rCBF and L-FBF is whether the rCBF or FBF is used as the\nveriﬁcation structure. Hence, because the auxiliary structures of the L-rCBF and L-FBF are\nsimilar, the construction and search procedures of the L-rCBF are the same as those of the\nL-FBF in [ 20], except for the construction and search of the V-rCBF (i.e., using the V-rCBF\ninstead of the veriﬁcation FBF (V-FBF)).\nIn constructing the L-rCBF, the model is ﬁrst trained for sets Sand Sc\ntr(i.e., Scfor\ntraining), and then tested for every element in Sto construct the FR-BF and V-rCBF. The\nmodel learns the distribution of elements included in each positive class (i.e., elements in\nclass vfor1\u0014v\u0014Q) and that of non-elements (i.e., non-elements in negative class 0) from\nthe training data. Improving the accuracy of the model is crucial because as the accuracy\nincreases, the elements that must be stored in the auxiliary structures decrease, resulting in\na reduction in the overall memory requirements of the L-rCBF.\nAfter training the model and before testing it, a threshold ( tm) in the model should\nbe set to adjust the false positive rate (FPR) of the L-rCBF. Even though the additional\nFR-BF and V-rCBF can solve the FRs and FNs from the model, the auxiliary structures\ncannot control the FPR. Furthermore, the elements stored in the FR-BF and V-rCBF vary\ndepending on tm. In other words, a large tmcan reduce the FPR from the model; however,\nit may cause more FNs from the model, which implies that a greater number of elements\nshould be programmed into the V-rCBF. In the experiment, we ﬁrst set the desired FPR of\nthe model, and the tmwas set according to the desired FPR using the validation set Sc\nvd(i.e.,\nScfor setting tm). After setting tm, the model is tested for elements in set S. Elements with\nFR returned from the model (i.e., SFR) are programmed into the FR-BF; elements with FR\nor FN returned from the model (i.e., SFRorSFN), as well as those with FP from the FR-BF\n(i.e., BFP) among those with true positive (TP) from the model (i.e., STP), are programmed\ninto the V-rCBF (i.e., SV=SFR[SFN[BFPand BFP\u001aSTP).\nIn the search procedure for the L-rCBF, the model is ﬁrst tested for a given input in\nsetU(i.e., U=S[Sc\ntsfor the test and Sc\nts=Sc\u0000Sc\ntr\u0000Sc\nvd). If a negative is returned, the\nV-rCBF is queried to prevent FNs and returns the ﬁnal result (i.e., negative, positive (a\nvalue), or indeterminable (INDET)). If the model returns a value, the FR-BF is queried to\nﬁlter out an FR. If a positive is returned from the FR-BF, the V-rCBF is queried to return a\ntrue result because a positive from the FR-BF implies that a positive class (value) returned\nfrom the model is false (i.e., FR). Hence, the V-rCBF returns the ﬁnal result. Otherwise, if a\nnegative is returned from the FR-BF, the value from the model is returned as the ﬁnal result\nbecause the value is not an FR.\n3.2. Deletion Algorithm of L-rCBF\nThe proposed L-rCBF should provide a deletion to guarantee the same characteristics\nas those of a single rCBF. However, once a model in the L-rCBF is trained on a set, speciﬁc\nelements cannot be deleted from the model because it learns the distribution of the set.\nTherefore, we propose a deletion algorithm for elements inserted using the auxiliary\nstructures (i.e., the FR-BF and V-rCBF). In other words, the novelty of this paper lies in\nthe fact that a key–value structure using a learned model can support deletions. Although\nL-rCBF is similar to L-FBF in terms of both construction and search procedures, the L-FBF\nin [20] cannot support deletions because the V-FBF in the L-FBF is for static data without\nconsidering deletions.\nAlgorithm 2 shows the deletion algorithm for the L-rCBF. When searching for an\nelement after deleting it, the L-rCBF should return a negative. Hence, depending on which\nset the element to be deleted belongs to (i.e., SFN,SFR,BFP, or STP\u0000BFP), one of two\n\nAppl. Sci. 2023 ,13, 12116 7 of 17\noperations is performed: OP1) delete the element from the V-rCBF or OP2) program the\nelement to the FR-BF.\nAlgorithm 2 Deletion algorithm of L-rCBF\n1:procedure delete _LrCBF (x,v)\n2: result = testModel( x)\n3: ifresult == 0then .FN from model\n4: operation = OP1\n5: else if result !=vthen .FR from model\n6: operation = OP1\n7: else .TP from model\n8: ifqueryBF(FR-BF, x)then .FP from FR-BF\n9: operation = OP1\n10: else\n11: operation = OP2\n12: ifoperation == OP1then\n13: delete_rCBF(V-rCBF, x,v)\n14: else\n15: programBF(FR-BF, x)\nTo delete element ( x,v) inSfor1\u0014v\u0014Q, a learned model is ﬁrst tested (line 2). If the\nelement is included in SFN, the model returns a negative (i.e., result ==0), which implies\nthat the element is stored in the V-rCBF; hence, for the V-rCBF to return a negative as a\nﬁnal result, the element is deleted from the V-rCBF (lines 3, 4, 12, and 13). If the element is\nincluded in SFR, the model returns a value not equal to v, resulting in a query to the FR-BF.\nBecause every element in SFRis stored in the FR-BF, it returns a positive, and the V-rCBF is\nthen queried. Hence, the element is deleted from the V-rCBF (lines 5, 6, 12, and 13).\nEven though the model returns v(i.e., STP), the element is deleted from the V-rCBF if\nthe FR-BF returns a positive (i.e., BFP, lines 8, 9, 12, and 13). Otherwise, if the FR-BF returns\na negative (i.e., STP\u0000BFP), the element is programmed into the FR-BF (lines 11 and 15) to\nchange the ﬁnal result of the search, from vto negative, after the deletion. In other words,\nwhen searching for the element in STP\u0000BFPafter OP2, the FR-BF returns a positive, and\nthe V-rCBF then returns a negative as the ﬁnal result because the element has never been\nprogrammed into the V-rCBF. In addition, the time complexity of the deletion algorithm is\nO(1)because the model is tested once and one of the auxiliary structures (i.e., V-rCBF or\nFR-BF) is accessed to perform OP1 or OP2.\nAfter performing OP2, the L-rCBF may return a few FNs because additional FPs from\nthe FR-BF may occur. Before deletions, elements in STP\u0000BFPare not stored in both the\nFR-BF and V-rCBF because the model returns true values for the elements. However, if\nadditional FPs from the FR-BF occur after deleting some elements in STP\u0000BFP, the V-rCBF\nis queried for the elements with FPs (i.e., those in a subset of STP\u0000BFP) and returns\nnegatives, which are false results (i.e., FNs). To prevent such FNs, when the number of\ndeleted elements exceeds a threshold ( td), the L-rCBF is reconstructed for the remaining\nelements. tdcan be set according to the number of additional FPs from the FR-BF ( nf p,b)\nbecause the FPs from the FR-BF cause the V-rCBF to return the FNs as the ﬁnal results. Thus,\ntdis set to the number of deleted elements at the point where the theoretically calculated\nnf p,bis not greater than and close to 1.\nThe L-rCBF has two cases of undeletables : undeletable by FP (UNDEL-FP) and un-\ndeletable by conﬂict cells (UNDEL-C). For UNDEL-FPs, the L-rCBF returns a value for a\ndeleted element because an FP occurs in the V-rCBF. For UNDEL-Cs, when performing\nOP1, if all accessed cells have maximum counters in the V-rCBF, the element cannot be\ndeleted from the V-rCBF, resulting in an undeletable from the L-rCBF.\nFigure 2a,b show the access paths of the four types of elements in the L-rCBF when\nsearching before and after deleting them, respectively. For an element ( x1,v1) in set SFN,\nbefore deleting it, the model returns a negative, and the V-rCBF then returns v1as a ﬁnal\n\nAppl. Sci. 2023 ,13, 12116 8 of 17\nresult in the search procedure. After deleting it from the L-rCBF, the model still returns\na negative; however, the V-rCBF would return a negative, as shown in Figure 2b. For an\nelement ( x2,v2) in set SFRor (x3,v3) inBFP, before deleting it, the model returns a positive\nclass, and the FR-BF then returns a positive; subsequently, the V-rCBF returns v2orv3,\nrespectively. After deleting it, the model and FR-BF still return a positive; however, the\nV-rCBF would return a negative as a ﬁnal result. For an element ( x4,v4) in set STP\u0000BFP,\nbefore deleting it, the model returns v4, and the FR-BF then returns a negative. After\ndeleting it, the model still returns v4; however, the FR-BF returns a positive. Hence, the\nV-rCBF is queried and would return a negative as a ﬁnal result. Finally, for the four deleted\ntypes of elements, the proposed L-rCBF returns negatives.\n(a)\n(b)\nFigure 2. Access paths of elements depending on the deletions of the L-rCBF: ( a) Access paths of\nelements before deleting them; ( b) Access paths of elements after deleting them.\nThe proposed deletion algorithm for the L-rCBF can be similarly applied to the L-FBF\nfor dynamic data (i.e., the V-FBF in the L-FBF is that for dynamic data). However, because\nthe deletion performance of the V-rCBF, which can use conﬂict cells, is better than that of\nthe V-FBF, the performance of the L-rCBF is also better than that of the L-FBF.\n4. Theoretical Analysis for rCBF\nIn this section, we theoretically analyze the undeletable and search failure probabilities\nof a single rCBF because the performance of the V-rCBF is a crucial determinant that\nsigniﬁcantly affects the performance of the proposed L-rCBF. Additionally, we brieﬂy\nanalyze the probabilities of a single FBF for dynamic data. Consequently, this section\ndemonstrates that the L-rCBF utilizing an rCBF is more suitable for dynamic data than the\nL-FBF using an FBF.\n\nAppl. Sci. 2023 ,13, 12116 9 of 17\n4.1. Undeletable Probability of rCBF\nIn deleting element ( x,v) from an rCBF, the undeletable of the rCBF means that it\ncannot be deleted because every cell pointed to by khash functions has cmax. The undeletable\nprobability of the rCBF ( P(Ur)) can be obtained in a similar manner to that of a ternary BF\nin [27] because both structures do not allow the decrement of counters with cmax.\nLetp(j)be the probability that a speciﬁc cell is selected by jhash indexes among the\nk(n\u00001)indexes of n\u00001 elements, except for xto be deleted. Then,\np(j)=\u0012k(n\u00001)\nj\u0013\n\u0001\u00101\nm\u0011j\n\u0001\u0010\n1\u00001\nm\u0011k(n\u00001)\u0000j\n. (2)\nIf the speciﬁc cell is the one selected by one of the indexes of xto be deleted, the\nmaximum jto avoid the cell with cmaxiscmax\u00002. In other words, if j=cmax\u00001, the cell\nhascmaxbecause the cell is selected by one index of xand selected by cmax\u00001indexes of\nn\u00001elements. Hence, the elements stored in the cell cannot be deleted. Therefore, P(Ur)\ncan be calculated as follows:\nP(Ur) =\u0010\n1\u0000cmax\u00002\nå\nj=0p(j)\u0011k\n, (3)\nwhere åcmax\u00002\nj=0p(j)denotes the probability that a speciﬁc cell does not have cmax; hence,\n1\u0000åcmax\u00002\nj=0p(j)denotes the probability that a speciﬁc cell has cmax. If at least one of the k\ncells for xdoes not have cmax, then xcan be deleted. Therefore, P(Ur)is the probability that\nallkcells selected by xhave cmax.\n4.2. Search Failure Probability of rCBF\nThe search failures of an rCBF include both false positives (FPs) and indeterminables\n(INDETs). The search failure probability ( P(Fs)) of the rCBF is formulated as follows, based\non [20]:\nP(Fs) =P(I) +P(FP)\n=\u0010\nP(S)P(IjS) +P(Sc)P(IjSc)\u0011\n+P(Sc)P(FPjSc)\n=P(S)P(IjS) +P(Sc)\u0010\nP(IjSc) +P(FPjSc)\u0011\n, (4)\nwhere P(I)is the probability of an INDET for input yin set SorSc, and P(FP)is the\nprobability of an FP for input yinSc.\nP(IjS)is the probability that every counter in the kcells for yincluded in Sis two or\nmore (i.e., c\u00152). Hence,\nP(IjS) = ( pci)k, (5)\nwhere pciis the probability that, for a speciﬁc cell pointed to by at least one of the kindexes\nforyinS, the cell is selected by at least one of the k(n\u00001)indexes. Thus,\npci=1\u0000(1\u00001\nm)k(n\u00001). (6)\nP(IjSc)is the probability that every counter in the kcells for ynot in S(i.e., in Sc) is\ntwo or more (i.e., c\u00152). Hence,\nP(IjSc) = ( pcn)k, (7)\n\nAppl. Sci. 2023 ,13, 12116 10 of 17\nwhere pcnis the probability that a speciﬁc cell is selected by at least two of the knindexes\nof the nelements in S. Thus,\npcn=1\u0000(1\u00001\nm)kn\u0000kn\nm\u0001(1\u00001\nm)kn\u00001, (8)\nwhere (1\u00001\nm)knis the probability that the cell is not selected by any of the knindexes, and\nkn\nm\u0001(1\u00001\nm)kn\u00001is the probability that the cell is selected by one of them.\nP(FPjSc)is the probability that the rCBF returns a value for ynot in S(i.e., in Sc). Let Qbe\nthe number of return values; we assume that the values are uniformly distributed. Then,\nP(FPjSc) =Qk\nå\ni=1\u0012k\ni\u0013\n\u0001(ppn)i\u0001(pcn)k\u0000i, (9)\nwhere ppnis the probability that a speciﬁc cell has one of the Qvalues with c=1. Hence,\nppn=1\nQ\u0001kn\nm\u0001(1\u00001\nm)kn\u00001. (10)\nThe P(FPjSc)of the rCBF in (9)seems to be the same as that of the FBF for static\ndata [ 20]; however, ppnand pcnin(10) and (8)are not the same as those of the FBF for static\ndata because the FBF does not support the deletion operation. Thus, the P(FPjSc)of the\nrCBF is not the same as that of the FBF for static data.\nIf an FBF supporting the deletion operation is constructed, the search failure probability\nof the FBF is the same as that of the rCBF. In addition, the undeletable probability of the\nFBF ( P(Uf)) is the same as P(IjS)in(5). When using the same m,n, and kfor the rCBF and\nFBF, P(Ur)<P(Uf)under the constraint cmax\u00153(i.e., R\u00152). Hence, the L-rCBF using\nan rCBF is more suitable for dynamic data processing than the L-FBF using an FBF.\n5. Performance Evaluation\nSection 5.1 compares the proposed L-rCBF with a single rCBF and L-FBF, and com-\npares the performance of L-rCBFs composed of different models. Section 5.2 compares\nthe theoretical results for undeletable and search failure probabilities of an rCBF with the\nexperimental results.\n5.1. Performance Comparison of rCBF, L-FBF, and L-rCBF\nOur simulation was performed for datasets with speciﬁc distributions because learning-\nbased data structures use the distribution of the elements in each set. A total of 245,514 URLs\nwere used as a positive set (i.e., S) with six return values (i.e., six classes) [ 28], and 1,491,178\nblacklisted URLs were used as a negative set (i.e., Sc) [29].\nWe compared ﬁve BF structures: a single rCBF, L-FBF 1, L-FBF 2, L-rCBF 1, and L-rCBF 2.\nEach of the two L-FBFs supports the deletion operation using a V-FBF for dynamic data.\nThis performance comparison used two models with different classiﬁcation accuracies\nand memory requirements. One model was included in the L-FBF 1and L-rCBF 1, and the\nother model was included in the L-FBF 2and L-rCBF 2. The memory requirement of a model\ngenerally increases with the accuracy of the model in a learning-based structure. However,\nthis does not necessarily imply that the total memory requirement of the structure increases.\nFor a fair comparison, we ﬁrst constructed the L-rCBF 1, and then constructed the other\nfour structures with the same memory requirement as that of the L-rCBF 1.\nTo train the two models, character-level pretrained embeddings were utilized with\nprincipal component analysis (PCA). The models include a long short-term memory (LSTM)\nlayer, two one-dimensional convolutional neural network (CNN) layers, and three fully\nconnected layers with softmax activation. However, because the hyperparameters of the\ntwo models are different, the memory requirements and accuracies of the models are also\ndifferent. Table 1 compares the number of weights ( w) and memory requirements of the\n\nAppl. Sci. 2023 ,13, 12116 11 of 17\ntwo models. The memory requirement of a model depends on wbecause it is calculated\nusing w. Model 2for the L-rCBF 2and L-FBF 2requires more memory than Model 1for the\nL-rCBF 1and L-FBF 1; however, Model 2provides a higher level of accuracy.\nTable 1. Comparison of memory requirements of two models.\nModel 1 Model 2\nNumber of weights ( w) 6207 48,055\nMemory requirements (kB) 24.828 192.22\nBecause an additional FP from the FR-BF causes an FN from the V-rCBF, an L-rCBF\nshould be reconstructed at threshold tdbefore the FR-BF begins to return an additional FP\nowing to too many deletions. Threshold tdcan be set to the number of deleted elements at\nthe point where the theoretically calculated nf p,bis less than and close to 1. Let ntpbejSTPj,\nnbthe number of elements programmed in the FR-BF (i.e., jSFRj),mbthe FR-BF size, kbthe\nnumber of hash functions of the FR-BF, and n0\nbthe number of elements to be programmed\ninto the FR-BF for deletion. Therefore, nf p,bcan be calculated as follows:\nnf p,b= (ntp\u0000n0\nb)\u0001\u0010\n1\u0000(1\u00001\nmb)kb(nb+n0\nb)\u0011kb. (11)\nFigure 3 shows the theoretical and experimental nf p,bin the L-rCBF 1with Model 1\naccording to the elements deleted. When deleting more than 15% of the elements in the\nL-rCBF 1,nf p,b\u00151. Hence, tdcan be predeﬁned as 15% of the elements. In this experiment,\nreconstruction was not considered for a simple performance comparison; hence, to evaluate\nthe deletion performance, 15% of the randomly selected URLs among those in Swere\ndeleted. In addition, a search performance experiment was performed for all URLs in U\n(i.e., U=S[Sc\nts).\nFigure 3. Number of false positives from the FR-BF in the L-rCBF 1with Model 1according to the\nelements deleted.\nIn the construction procedure, the L-rCBF 1is constructed ﬁrst, and then the other\nfour structures are constructed with the same amount of memory as that of the L-rCBF 1.\nLetnvbe the number of elements programmed into a veriﬁcation structure (i.e., jSvj=\njSFR[SFN[BFPj),mvthe number of cells in the veriﬁcation structure, avthe size factor of\nthe veriﬁcation structure (i.e., av=mv\nnv), and abthe size factor of the FR-BF (i.e., ab=mb\nnb).\nThe L-rCBF 1comprises Model 1, a32nb-bit FR-BF, and an 8nv-cell V-rCBF (i.e., ab=32and\nav=8in the L-rCBF 1). To allocate the same amount of memory as in the L-rCBF 1, the size\nfactors of the FR-BFs and veriﬁcation structures should be adjusted in the L-FBF 1, L-FBF 2,\nand L-rCBF 2.\nFor the FR-BF, abdepends on the model accuracy. As the accuracy of the model\nincreases, ntpincreases and nbdecreases. To satisfy the condition nf p,b<1until 15% of the\nelements are deleted in the learning-based structures, each abin the L-rCBF 2and L-FBF 2\n\nAppl. Sci. 2023 ,13, 12116 12 of 17\nusing Model 2with a higher accuracy than Model 1should be increased to increase mb,\nconsidering (11). In other words, abin the L-FBF 1is the same as that in the L-rCBF 1(i.e.,\n32) because both structures use Model 1. However, each abin the L-FBF 2and L-rCBF 2is set\nto 49 to satisfy the condition. Hence, to support deletions, abshould be increased as the\naccuracy of the model increases.\nA veriﬁcation structure is constructed with the remaining memory after constructing\na model and FR-BF from the total memory. We assume that R=2and L=3because six\nvalues exist. A single cell in an rCBF has ﬁve bits (i.e., R+Lbits), whereas a single cell\nin an FBF has three bits (i.e., Lbits). Therefore, if an L-FBF and L-rCBF include the same\nmodel, the avof the V-FBF in the L-FBF is greater than that of the V-rCBF in the L-rCBF.\nTherefore, the avin the L-FBF 1is 13.33, 14.03 in the L-FBF 2, and 8.42 in the L-rCBF 2. In\naddition, the size factor of the single rCBF is 6.19.\nTable 2 compares the undeletable rates of the rCBF, L-FBF 1, L-FBF 2, L-rCBF 1, and L-\nrCBF 2when using the same amount of memory. No UNDEL-FPs from the rCBF and L-FBFs\nare observed because the structures return more UNDEL-Cs than the L-rCBFs. However,\nthe total number of undeletables from each L-rCBF is less than those from the rCBF and\nL-FBFs. In terms of the deletion performance, structures using rCBFs outperform those\nusing FBFs because of their counter ﬁelds. Even though each V-FBF in the L-FBFs has more\ncells than each V-rCBF in the L-rCBFs, the undeletable rates of the L-rCBF 1and L-rCBF 2\nimprove by 83.67% and 76.67% compared with those of the L-FBF 1and L-FBF 2, respectively.\nTable 2. Comparison of undeletable rates when using same amount of memory (%).\nrCBF L-FBF 1 L-FBF 2 L-rCBF 1 L-rCBF 2\nUNDEL-FP 0 0 0 0.014 0.014\nUNDEL-C 0.030 0.098 0.060 0.003 0\nTotal 0.030 0.098 0.060 0.016 0.014\nTable 3 compares the search failure rates of the rCBF, L-FBF 1, L-FBF 2, L-rCBF 1, and\nL-rCBF 2when using the same amount of memory. The reduction rate in search failures\nrepresents the proportion of reduced search failures observed in a learning-based structure\nrelative to the single rCBF. The four learning-based structures improve the search failure\nrates compared with the single rCBF. When comparing L-FBFs with L-rCBFs, the search\nfailure rates of the L-FBFs are better than those of the L-rCBFs because the avof the V-FBFs\n(i.e., 13.33 and 14.03) are greater than those of the V-rCBFs (i.e., 8 and 8.42). However, if\ninsertions and deletions are repeated for dynamic data, the performance gap of the search\nfailure rates between the L-rCBF and L-FBF with the same model would be reduced owing\nto an increase in the number of conﬂict cells in the V-FBF. Furthermore, because of the\nsigniﬁcantly superior deletion performance of the L-rCBFs compared with that of the L-\nFBFs, as shown in Table 2, the L-rCBFs are more appropriate than L-FBFs for dynamic data\nprocessing. In addition, when comparing the L-FBF 1to L-FBF 2or the L-rCBF 1to L-rCBF 2,\nin terms of the search and deletion performances, each structure using Model 2outperforms\neach structure using Model 1, despite Model 2requiring more memory than Model 1, because\nof the higher accuracy of Model 2.\nFor static data, because insertions and deletions are infrequent and searches are\nprimarily performed, using the L-FBF with an improved search performance is more\nefﬁcient than using a single rCBF. Especially if the FBF for dynamic data is replaced with\nthat for static data in the L-FBF, the search performance of the L-FBF improves. Table 4\ncompares the search failure rates of the rCBF, L-FBF 2with the FBF for dynamic data, and\nL-FBF 2with the FBF for static data when using the same amount of memory. Because the\nnumber of conﬂict cells in the FBF for static data is less than that for dynamic data, using\nthe FBF for static data can reduce the number of INDETs included in set S. Hence, the\nL-FBF for static data is more efﬁcient than a single rCBF and the L-FBF for dynamic data\nwhen insertions and deletions are infrequent.\n\nAppl. Sci. 2023 ,13, 12116 13 of 17\nTable 3. Comparison of search failure rates when using same amount of memory (%).\nrCBF L-FBF 1 L-FBF 2 L-rCBF 1 L-rCBF 2\nSTP 94.896 99.901 99.938 98.767 99.181\nINDET ( IjS) 5.104 0.099 0.062 1.233 0.819\nScTN 99.318 99.480 99.453 99.388 99.407\nFP (FPjSc) 0.646 0.520 0.547 0.610 0.593\nINDET ( IjSc) 0.036 0 0 0.002 0\nSearch failures FP+INDET 1.307 0.258 0.245 0.998 0.734\nReduction rate in search failures- 80.26 81.25 23.64 43.84compared with a single rCBF\nTable 4. Comparison of search failure rates between L-FBFs for dynamic and static data (%).\nrCBFL-FBF 2\nDynamic Data Static Data\nSTP 94.896 99.938 99.982\nINDET ( IjS) 5.104 0.062 0.018\nScTN 99.318 99.453 99.453\nFP (FPjSc) 0.646 0.547 0.547\nINDET ( IjSc) 0.036 0 0\nSearch failures FP+INDET 1.307 0.245 0.218\nReduction rate in search failures- 81.25 83.32compared with a single rCBF\nAdditionally, we compare the L-rCBFs (i.e., L-rCBF 1and L-rCBF 2) to two L-FBFs (i.e.,\nL-FBF 3and L-FBF 4), which are identical to the L-FBF 1and L-FBF 2except for the V-FBFs,\nand the avof the V-FBFs have the same values as those of the V-rCBFs in the L-rCBFs: 8 and\n8.42, respectively. Table 5 compares the undeletable rates when using a veriﬁcation structure\nwith the same av. The undeletable rates of the L-rCBF 1and L-rCBF 2are reduced by 98.73%\nand 98.20%, respectively, compared with those of the L-FBF 3and L-FBF 4, respectively. In\nterms of the search performance, if both an L-FBF and L-rCBF with the same model possess\nthe same aband avvalues, the structures have the same search failure rates. Hence, the\nsearch failure rates of the L-rCBF 1and L-FBF 3are the same, and those of the L-rCBF 2and\nL-FBF 4are also the same. However, if insertions and deletions are repeated, the search\nfailure rates of the L-rCBFs would be better than those of the L-FBFs.\nTable 5. Comparison of undeletable rates when using veriﬁcation structure with same av(%).\nL-FBF 3 L-FBF 4 L-rCBF 1 L-rCBF 2\nUNDEL-FP 0.027 0.022 0.014 0.014\nUNDEL-C 1.227 0.758 0.003 0\nTotal 1.255 0.779 0.016 0.014\n5.2. Comparison of Probabilities between Theoretical and Experimental Results for rCBF\nThis section compares the undeletable and search failure probabilities (i.e., P(U)and\nP(Fs)) between the theoretical and experimental results for the rCBF and FBF supporting the\ndeletion operation. To obtain the results for P(U)and P(Fs), experiments were performed\nusing 215random URLs for set Sand 2\u0003215URLs for Sc, and the URLs were obtained from\nALEXA [ 30]. To allocate the same number of bits to a value ﬁeld in the rCBF and a cell in\nthe FBF, we assumed 254 return values; hence, a cell in the rCBF has a two-bit counter and\neight-bit value ﬁeld, and a cell in the FBF has eight bits to store values. However, the rCBF\ncan store up to 255 values for eight bits (i.e., 28\u00001) because the rCBF does not need to\nreserve the maximum value 28\u00001 as a conﬂict value.\n\nAppl. Sci. 2023 ,13, 12116 14 of 17\nLetnbe the number of elements stored in a BF structure, athe size factor of the\nstructure, mthe number of cells in the structure (i.e., BF size m=an), and Mthe memory\nrequirement of the structure. Figures 4 and 5 compare the P(U)and P(Fs)between the\ntheoretical and experimental results according to the BF size, respectively. When each a\nof the FBF is 2,4, and 8, each aof the rCBF 1is1.6,3.2, and 6.4, respectively, because the\nrCBF 1uses the same Mas the FBF. The rCBF 2uses the same aas the FBF. Although the a\nof the rCBF 1is smaller than that of the FBF, the P(U)of the rCBF 1is much smaller than\nthat of the FBF. The P(FS)of the rCBF 1is slightly greater than that of the FBF; however,\nif insertions and deletions are repeated, the P(FS)of the rCBF would be better than that\nof the FBF. Hence, with dynamic data, replacing the FBF with the rCBF can improve the\nperformance of the overall structure (i.e., L-rCBF), even though the FBF is better than the\nrCBF in terms of the P(FS)when using the same M. In addition, the experimental results\nvalidated the theoretical analysis, as shown in Figures 4 and 5.\nFigure 4. Comparison of undeletable probabilities between theoretical and experimental results\naccording to BF size.\nFigure 5. Comparison of search failure probabilities between theoretical and experimental results\naccording to BF size.\n\nAppl. Sci. 2023 ,13, 12116 15 of 17\n6. Conclusions\nIn this paper, we propose an L-rCBF for dynamic data and design a deletion algorithm\nfor the L-rCBF that can be applied to deletions in an L-FBF. The proposed L-rCBF can be\nutilized in various applications involving repetitive update operations (i.e., insertions and\ndeletions), such as PIT lookup in NDN and deduplication in distributed systems. As an\nexpandable and deletable key–value structure, the L-rCBF allows efﬁcient operations and\ncan achieve the purpose of each application without other data structures.\nUsing the same amount of memory, the proposed L-rCBF is better than a single rCBF in\nterms of undeletables and search failures because a model in the L-rCBF consumes a relatively\nsmall amount of memory, regardless of the amount of data. Hence, as the amount of positive\ndata to be stored increases, the L-rCBF becomes more efﬁcient than a single rCBF.\nUsing the same amount of memory, an L-rCBF is better than an L-FBF with the same\nmodel in terms of undeletables because an rCBF can utilize cells with hash collisions for\ndeletion, unlike an FBF. Moreover, as the number of return values increases, the L-rCBF\nbecomes more efﬁcient than the L-FBF. Therefore, for dynamic data processing with frequent\ninsertions and deletions, the proposed L-rCBF is more suitable than the L-FBF, and deletions\nfrom the L-rCBF and L-FBF can be implemented using the proposed deletion algorithm.\nIn addition, the theoretical analyses and experiments demonstrated the superiority of\nstructures using an rCBF.\nFurthermore, the performance of a learning-based structure can vary depending on the\naccuracy of the model used. In other words, the higher the accuracy of a model, the better\nthe deletion and search performance of the learning-based structure. Despite the higher\nmemory requirement of a model with high accuracy compared with a model with lower\naccuracy, the accurate model reduces the sizes of the auxiliary structures (i.e., the FR-BF and\nveriﬁcation structure) by reducing the number of elements stored in the structures, thereby\nenhancing the overall performance. However, owing to the tradeoff between memory\nrequirements and accuracy, the size of a learned model cannot be increased in an unlimited\nmanner to improve accuracy when designing a learning-based structure. In addition, if a\ncomplex model is designed to increase accuracy, the training time of the model increases\nas well as the memory requirement. Therefore, if reconstructions occur, the processing\nspeed of the L-rCBF using the complex model is degraded compared to the L-rCBF using a\nsimpler and smaller model.\nAuthor Contributions: Conceptualization, H.B.; methodology, H.B.; software, Y.L.; validation, Y.L.\nand H.B.; formal analysis, H.B.; investigation, Y.L. and H.B.; resources, H.B.; data curation, Y.L.;\nwriting—original draft preparation, Y.L.; writing—review and editing, H.B.; visualization, Y.L.;\nsupervision, H.B.; project administration, H.B.; funding acquisition, H.B. All authors have read and\nagreed to the published version of the manuscript.\nFunding: This work was supported by the National Research Foundation of Korea (NRF) grant\nfunded by the Korean government (MSIT) (NRF-2021R1F1A1051646).\nInstitutional Review Board Statement: Not applicable.\nInformed Consent Statement: Not applicable.\nData Availability Statement: Not applicable.\nConﬂicts of Interest: The authors declare no conﬂicts of interest.\nAbbreviations\nThe following abbreviations are used in this manuscript:\nBF Bloom ﬁlter\nFBF Functional Bloom ﬁlter\nrCBF Counting Bloom ﬁlter with return values\n\nAppl. Sci. 2023 ,13, 12116 16 of 17\nLBF Learned Bloom ﬁlter\nL-FBF Learned functional Bloom ﬁlter\nL-rCBF Learned counting Bloom ﬁlter with return values\nFR-BF False-class result Bloom ﬁlter\nV-FBF Veriﬁcation functional Bloom ﬁlter\nV-rCBF Veriﬁcation counting Bloom ﬁlter with return values\nNDN Named data networking\nFIB Forwarding information base\nPIT Pending interest table\nSDN Software deﬁned networking\nTP True positive\nFP False positive\nFR False-class result\nFN False negative\nINDET Indeterminable\nUNDEL Undeletable\nPCA Principal component analysis\nLSTM Long short-term memory\nCNN Convolutional neural network\nReferences\n1. Bloom, B. Space/time trade-offs in hash coding with allowable errors. Commun. ACM 1970 ,13, 422–426. [CrossRef]\n2. Dharmapurikar, S.; Krishnamurthy, P .; Taylor, D.E. Longest preﬁx matching using bloom ﬁlters. IEEE/ACM Trans. Netw. 2006 ,14,\n397–409. [CrossRef]\n3. Mun, J.H.; Lim, H. New approach for efﬁcient ip address lookup using a bloom ﬁlter in trie-based algorithms. IEEE Trans. Comput.\n2016 ,65, 1558–1565. [CrossRef]\n4. Dai, H.; Lu, J.; Wang, Y.; Pan, T.; Liu, B. BFAST: High-Speed and Memory-Efﬁcient Approach for NDN Forwarding Engine.\nIEEE/ACM Trans. Netw. 2017 ,25, 1235–1248. [CrossRef]\n5. Jang, S.; Byun, H.; Lim, H. Dynamically Allocated Bloom Filter-Based PIT Architectures. IEEE Access 2022 ,10, 28165–28179.\n[CrossRef]\n6. Wu, Q.; Wang, Q.; Zhang, M.; Zheng, R.; Zhu, J.; Hu, J. Learned bloom-ﬁlter for the efﬁcient name lookup in information-centric\nnetworking. J. Netw. Comput. Appl. 2021 ,186, 103077. [CrossRef]\n7. Reviriego, P .; Martínez, J.; Larrabeiti, D.; Pontarelli, S. Cuckoo ﬁlters and bloom ﬁlters: Comparison and application to packet\nclassiﬁcation. IEEE Trans. Netw. Service Manag. 2020 ,17, 2690–2701. [CrossRef]\n8. Yang, M.; Gao, D.; Foh, C.H.; Qin, Y.; Leung, V .C.M. A Learned Bloom Filter-Assisted Scheme for Packet Classiﬁcation in\nSoftware-Deﬁned Networking. IEEE Trans. Netw. Service Manag. 2022 ,19, 5064–5077. [CrossRef]\n9. Eppstein, D.; Goodrich, M.T.; Uyeda, F.; Varghese, G. What’s the difference? efﬁcient set reconciliation without prior context.\nACM SIGCOMM Comput. Commun. Rev. 2011 ,41, 218–229. [CrossRef]\n10. Xia, W.; Feng, D.; Jiang, H.; Zhang, Y.; Chang, V .; Zou, X. Accelerating content-deﬁned-chunking based data deduplication by\nexploiting parallelism. Future Gener. Comput. Syst. 2019 ,98, 406–418. [CrossRef]\n11. Cheng, G.; Guo, D.; Luo, L.; Xia, J.; Gu, S. LOFS: A Lightweight Online File Storage Strategy for Effective Data Deduplication at\nNetwork Edge. IEEE Trans. Parallel Distrib. Syst. 2022 ,33, 2263–2276. [CrossRef]\n12. Patgiri, R.; Biswas, A.; Nayak, S. deepBF: Malicious URL detection using learned Bloom Filter and evolutionary deep learning.\nComput. Commun. 2023 ,200, 30–41. [CrossRef]\n13. Reviriego, P .; Hernández, J.A.; Dai, A.; Shrivastava, A. Learned bloom ﬁlters in adversarial environments: A malicious URL\ndetection use-case. In Proceedings of the 2021 IEEE 22nd International Conference on High Performance Switching and Routing\n(HPSR), Paris, France, 7–10 June 2021; pp. 1–6.\n14. Xiong, S.; Yao, Y.; Li, S.; Cao, Q.; He, T.; Qi, H.; Tolbert, L.; Liu, Y. kBF: Towards Approximate and Bloom Filter based Key–Value\nStorage for Cloud Computing Systems. IEEE Trans. Cloud Comput. 2017 ,5, 85–98. [CrossRef]\n15. Vijayakumar, P .; Chang, V .; Deborah, L.J.; Kshatriya, B.S.R. Key management and key distribution for secure group communication\nin mobile and cloud network. Future Gener. Comput. Syst. 2018 ,84, 123–125. [CrossRef]\n16. Fan, L.; Cao, P .; Almeida, J.; Broder, A. Summary cache: A scalable wide-area web cache sharing protocol. IEEE/ACM Trans. Netw.\n2000 ,8, 281–293. [CrossRef]\n17. Nayak, S.; Patgiri, R. countBF: A general-purpose high accuracy and space efﬁcient counting bloom ﬁlter. In Proceedings of\nthe 2021 17th International Conference on Network and Service Management (CNSM), Izmir, Turkey, 25–29 October 2021;\npp. 355–359.\n18. Deng, F.; Raﬁei, D. Approximately detecting duplicates for streaming data using stable bloom ﬁlters. In Proceedings of the 2006\nInternational Conference on Management of Data (SIGMOD), Chicago, IL, USA, 27–29 June 2006; pp. 25–36.\n\nAppl. Sci. 2023 ,13, 12116 17 of 17\n19. Bonomi, F.; Mitzenmacher, M.; Panigrah, R.; Singh, S.; Varghese, G. Beyond bloom ﬁlters: From approximate membership checks\nto approximate state machines. In Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols\nfor Computer Communications (SIGCOMM), Pisa, Italy, 11–15 September 2006; pp. 315–326.\n20. Byun, H.; Lim, H. Learned FBF: Learning-Based Functional Bloom Filter for Key–Value Storage. IEEE Trans. Comput. 2022 ,71,\n1928–1938. [CrossRef]\n21. Kraska, T.; Beutel, A.; Chi, E.; Dean, J.; Polyzotis, N. The Case for Learned Index Structures. In Proceedings of the 2018 International\nConference on Management of Data (SIGMOD), Houston, TX, USA, 10–15 June 2018; pp. 489–504.\n22. Mitzenmacher, M. A Model for Learned Bloom Filters, and Optimizing by Sandwiching. In Proceedings of the 32nd International\nConference on Neural Information Processing Systems (NIPS), Montréal, QC, Canada, 3–8 December 2018; pp. 462–471.\n23. Liu, Q.; Zheng L.; Shen, Y.; Chen, L. Stable learned bloom ﬁlters for data streams. Proc. VLDB Endow. 2020 ,13, 2355–2367.\n[CrossRef]\n24. Tarkoma, S.; Rothenberg, C.E.; Lagerspetz, E. Theory and Practice of Bloom Filters for Distributed Systems. IEEE Commun. Surv.\nTutor. 2012 ,14, 131–155. [CrossRef]\n25. Broder, A.; Mitzenmacher, M. Network applications of Bloom ﬁlters: A survey. Internet Math. 2003 ,1, 485–509. [CrossRef]\n26. Lee, Y.; Byun, H. Learned Counting Bloom Filter with Return Values for Deletion of Dynamic Data. In Proceedings of the IEIE\nSummer Conference, Jeju, Republic of Korea, 29 June–1 July 2022. (In Korean)\n27. Lim, H.; Lee, J.; Byun, H.; Yim, C. Ternary Bloom Filter Replacing Counting Bloom Filter. IEEE Commun. Lett. 2017 ,21, 278–281.\n[CrossRef]\n28. Web Directory. Available online: curlie.org/ (accessed on 15 December 2020).\n29. Free Online Dataset of Blacklisted URLs. Available online: www.shallalist.de/ (accessed on 15 December 2020).\n30. Alexa the Web Information Company. Available online: www.alexa.com/ (accessed on 17 May 2020).\nDisclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual\nauthor(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to\npeople or property resulting from any ideas, methods, instructions or products referred to in the content.",
  "textLength": 56401
}