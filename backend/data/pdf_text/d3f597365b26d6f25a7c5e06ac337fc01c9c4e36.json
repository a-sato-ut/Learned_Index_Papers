{
  "paperId": "d3f597365b26d6f25a7c5e06ac337fc01c9c4e36",
  "title": "Micro-architectural analysis of a learned index",
  "pdfPath": "d3f597365b26d6f25a7c5e06ac337fc01c9c4e36.pdf",
  "text": "Micro-architectural Analysis of a Learned Index\nMikkel M√∏ller Andersen\nmikka@itu.dk\nIT University of Copenhagen\nDenmarkPƒ±nar T√∂z√ºn\npito@itu.dk\nIT University of Copenhagen\nDenmark\nAbstract\nSince the publication of The Case for Learned Index Structures\nin 2018 [ 26], there has been a rise in research that focuses\non learned indexes for different domains and with different\nfunctionalities. While the effectiveness of learned indexes as\nan alternative to traditional index structures such as B+Trees\nhave already been demonstrated by several studies, previous\nwork tend to focus on higher-level performance metrics such\nas throughput and index size. In this paper, our goal is to\ndig deeper and investigate how learned indexes behave at a\nmicro-architectural level compared to traditional indexes.\nMore specifically, we focus on previously proposed learned\nindex structure ALEX [ 10], which is a tree-based in-memory\nindex structure that consists of a hierarchy of machine learned\nmodels. Unlike the original proposal for learned indexes,\nALEX is designed from the ground up to allow updates\nand inserts. Therefore, it enables more dynamic workloads\nusing learned indexes. In this work, we perform a micro-\narchitectural analysis of ALEX and compare its behavior to\nthe tree-based index structures that are not based on learned\nmodels, i.e., ART and B+Tree.\nOur results show that ALEX is bound by memory stalls,\nmainly stalls due to data misses from the last-level cache.\nCompared to ART and B+Tree, ALEX exhibits fewer stalls\nand a lower cycles-per-instruction value across different\nworkloads. On the other hand, the amount of instructions re-\nquired to handle out-of-bound inserts in ALEX can increase\nthe instructions needed per request significantly (10X) for\nwrite-heavy workloads. However, the micro-architectural\nbehavior shows that this increase in the instruction foot-\nprint exhibit high instruction-level parallelism, and, there-\nfore, does not negatively impact the overall execution time.\n1 Introduction\nLearned index structures have emerged as an alternative to\ntraditional database indexes in the recent years since their\npotential was demonstrated by Kraska et al. [ 26]. The funda-\nmental idea behind learned indexes is that one can treat the\nindex as a model that predicts the position of a given key in\nthe dataset. A key advantage of this type of index comes as a\nresult of the reduced index size, which can even be orders of\nmagnitude reduction in memory footprint, since the trained\nmodel or the hierarchy of models in the index keep less space\nthan keeping actual data. In addition, such indexes trade-off\nincreased computations to reduced memory accesses, which\ncould boost performance if the overall computation cyclesare cheaper with respect to memory access latency. On the\nother hand, for such an index to be effective in its predictions,\nthe index model has to be trained on the target dataset prior\nto deployment, which limits the dynamic nature of the index.\nThere has been several proposals [ 6,10,12,13,15,24,31]\nfor learned index structures since the proposal from Kraska\net al. [ 26]. These proposals and initial benchmarking efforts\n[23,30] already compare learned indexes to index structures\nthat aren‚Äôt based on learned models. However, these compar-\nisons are done by mainly focusing on higher-level metrics\nsuch as throughput (requests completed per second), latency\n(average time to complete a request), index size, scalabil-\nity with respect to number of threads utilized, etc. While\nthese metrics are extremely important and necessary to un-\nderstand the overall benefits and disadvantages of learned\nindex structures, they are one side of a coin, especially for\nmain-memory-optimized index structures. These metrics\nby themselves do not give a detailed understanding of how\nlearned indexes utilize the micro-architectural resources of\na processor (e.g., utilization of the front-end resources and\nthe different levels of the cache hierarchy). In this paper, our\ngoal is to shed light on this other side of the coin.\nTo achieve our goal, we focus on the learned index ALEX\n[10], which is one of the first learned indexes that is designed\nground up to enable efficient updates and inserts. We perform\na micro-architectural analysis of ALEX, while comparing\nits behavior to two index structures that are not based on\nlearned models; ART [ 7,27], a trie-based index structure,\nand B+tree [ 4], a tree-based index structure. The reason we\npick these two structures to compare against specifically is\nthat ALEX is already compared against them in the original\npaper [ 10] and by other studies [ 6,30]. This would allow our\nresults to be cross-checked.\nWe create a benchmark driver1that generates YCSB-like\n[5] workloads. Using this driver, we generate four workloads\nwith varying read-write intensities and two datasets of dif-\nferent sizes. We then report the breakdown of execution\ncycles into different micro-architectural components follow-\ning Intel‚Äôs Top-down Micro-architecture Analysis Method\n(TMAM) [39]. Our study demonstrates the following:\n‚Ä¢Similar to ART and B+Tree, ALEX spends majority of its\nexecution cycles in memory-bound stalls as a result of the\nlong-latency data stalls from the last-level cache. In con-\ntrast to ART and B+Tree, in general, ALEX exhibits fewer\n1Will be made available once the paper is accepted.\n1arXiv:2109.08495v1  [cs.DS]  17 Sep 2021\n\nConference‚Äô17, July 2017, Washington, DC, USA Mikkel M√∏ller Andersen and Pƒ±nar T√∂z√ºn\nstalls and spends fewer cycles per instruction across dif-\nferent workload patterns.\n‚Ä¢On the other hand, inserts to the right-end of the tree ( out-\nof-bound keys) lead to an order of magnitude increase\nin the per-request instruction footprint of ALEX, while\nthe impact for ART and B+Tree is not significant, due to\nmodel re-training. The instruction-level parallelism for\nsuch instructions, however, is quite high. Therefore, this\nincrease in instruction footprint does not significantly\nimpact the overall execution time for ALEX.\nThe rest of the paper is organized as follows. First, Sec-\ntion 2 surveys related work and gives a brief summary of\nthe index structures used in this study as well as the micro-\narchitecture of a processor. Then, Section 3 presents the\nexperimental setup and methodology, and Section 4 presents\nand discusses the results of the analysis. Finally, Section 5\nconcludes with a summary of our findings and sketches di-\nrections for future research.\n2 Background and Related Work\nBefore diving into our experimental study, Section 2.1 presents\na brief background on the index structures we use in this\npaper, Section 2.2 describes the micro-architectural compo-\nnents of a modern commodity out-of-order (OoO) processor\nto aide in the discussion of the results, and Section 2.3 sur-\nveys related work on benchmarking learned indexes and\nmicro-architectural analysis of data-intensive systems.\n2.1 ALEX, ART, B+Tree\nALEX [10] is an in-memory updatable learned index based\non the recursive model index (RMI) introduced by Kraska et\nal. [26]. There is a hierarchy of index nodes similar to B+Tree,\nbut each node keep a linear regression model instead of keys\nto direct the tree search. During a key lookup, from root to\nleaves, the model at each level predicts the child node to go\nto in the lower level based on the search key. Once a leaf\n(data) node is reached, the model at this node predicts the\nposition of the key being searched. If the prediction was off,\nexponential search is used to find the actual key position.\nDuring a key insert, the same traversal steps are used to first\npredict where the key should be inserted. If the prediction\nis not correct, then an exponential search determines the\ncorrect insert position. The core idea is that if models are\nmostly correct, meaning their predictions are either correct\nor very close to the correct position, an exponential search\nwould be faster than the binary search routine. In addition,\nALEX uses gapped arrays, leaving space in between data\nitems, to help with inserts and supports structural modifica-\ntion operations to adapt the tree dynamically. In this paper,\nwe use the open-source implementation of ALEX [ 9], which\nis provided by the authors of the original ALEX paper [10].\nART (Adaptive Radix Tree) [ 27] is a space-efficient general-\npurpose trie-based index structure designed for in-memorydatabase systems. To reduce the height of the tree and also\nsave space, ART adopts techniques like a high fanout (max\n256), lazy expansion, path compression, and different node\ntypes for more compact node layouts. This leads to better\ncache efficiency and improves overall performance. In this\npaper, we use the open-source implementation of ART [ 7],\nwhich is provided by Armon Dadgar, who does not have any\naffiliation with the authors of the original paper [ 27]. This\nopen-source codebase is also used by Ding et al. [ 10] while\ncomparing ALEX to ART.\nB+Tree [14] is the traditional index structure that is at\nthe core of most database management systems. Over the\nyears, there has been several proposals for different vari-\nants of B+Trees to optimize them for modern processors,\nin-memory- or SSD-optimized systems, etc. The B+Tree im-\nplementation used in this paper is provided by Timo Bing-\nmann [ 4], which is the same codebase used by Ding et al.\n[10] while comparing ALEX to B+Trees as well as by related\nwork that compares learned indexes to B+Trees [6, 30].\n2.2 Micro-architecture of an OoO processor\nThe cores of an out-of-order processor has two building\nblocks at a high-level: front-end andback-end .\nFront-end consists of micro-architectural components\nthat are responsible from processing instructions . More specif-\nically, it handles fetching, decoding, and issuing instructions.\nFetching instructions require going through the memory hi-\nerarchy, which is composed of three levels of caches (L1, L2,\nL3) and main memory (DRAM) on most commodity server\nhardware. L1 instruction cache (L1-I) , which is private for\neach core, is responsible from keeping instructions closer to\nthe core. Ideally, the instructions to be executed next should\nbe found in L1-I. If they miss in L1-I, then they have to be\nfetched from lower-levels of the memory hierarchy, which\nhas higher latency and stalls the entire instruction pipeline.\nIn practice, however, an OoO processor is equipped with\nvarious mechanisms to prevent or overlap such stalls. For\nexample, the decoder component, which decodes fetched\ninstructions into micro-operations ( ùúáOps), can process mul-\ntiple instructions in a cycle. It is extremely important to\nprevent front-end from stalling, since it would naturally lead\nto stalling or underutilization of the back-end as well.\nBack-end consists of micro-architectural components that\nare responsible from the execution of ùúáOps issued by the\nfront-end. These ùúáOps are registered to the reservation sta-\ntion, which tracks the operands and the dependencies for\nùúáOps and forwards them to the corresponding execution\nunit. Execution units can operate in parallel unless there\nare dependencies across, which enables further instruction-\nlevel parallelism. Execution units also have private buffers\nto buffer outstanding data load/store requests, which allows\noverlapping stall time that may happen due to these requests.\nFor example, if a data load request misses from L1 data cache ,\nsame as the case for instructions, one needs to fetch the data\n2\n\nMicro-architectural Analysis of a Learned Index Conference‚Äô17, July 2017, Washington, DC, USA\nfrom lower-levels of the memory hierarchy, which incurs\nhigher latency and may stall the back-end if not overlapped.\nWhen all the operands are in place for the ùúáOp, it is finally\nexecuted and retired .\nOverall, modern processors adopt several methods to pro-\nvide implicit parallelism and overlap the stall time due to\ndifferent operations to prevent underutilization of both the\nfront-end and back-end components. In addition, instruction\nanddata prefetchers ,branch prediction , and speculative execu-\ntionaim at fetching and processing the instructions and data\nto be needed before they are needed to avoid stalling the\ninstruction execution pipeline as much as possible. Despite\nall these techniques, for complex data-intensive systems, it\nis well-known that majority of the execution time may be\nspent in stalls due to instructions or data cache misses as the\nnext section covers.\n2.3 Related work\nWorkload characterization of data-intensive workloads us-\ning micro-architectural analysis is a widely-used method\nto understand both how well commodity processors serve\ndata-intensive applications and how well data-intensive sys-\ntems utilize commodity processors. Prior work on micro-\narchitectural analysis range from investigating the behavior\nof online transaction processing (OLTP) [ 22,34,37,38], to\nonline analytical processing (OLAP) [ 1,16,32,33], to larger-\nscale cloud [ 2,11,21,40] workloads and systems. Overall\nconclusion from these studies is that many widely-used data-\nintensive systems underutilize modern commodity hardware,\nbarely reaching an instructions-per-cycle value of one where\nthe theoretical maximum is four, even though the main cause\nof this underutilization may change from system to system\nand workload to workload.\nBased on the findings of these studies, computer architects\ncan improve modern server hardware to target the needs\nof popular data-intensive applications, and the designers of\ndata-intensive systems can adopt techniques or re-design\nsystems to make their software more hardware-conscious.\nFor example, it was well-known that traditional OLTP sys-\ntems suffered from front-end stalls due to L1 instruction\ncache misses since they had long and complex instruction\nfootprints. Sirin et al. [ 35] show that the improved instruc-\ntion fetch unit of Intel‚Äôs Broadwell architecture reduces the\ninstruction related stall time for OLTP workloads compared\nto running them on Intel‚Äôs IvyBridge. In addition, the leaner\nsystem design of in-memory OLTP systems leads to a smaller\ninstruction footprint per transaction and minimize the im-\npact of the stall time spent in fetching instructions.\nAll the studies mentioned above look at data management\nsystems as a whole and do not investigate the behavior of\ntheir index structures in isolation. This work, in contrast,\nperforms a finer-granularity analysis focusing solely on the\nindex structures themselves. At this finer-granularity, Kowal-\nski et al. [ 25] performed a micro-architectural analysis of twomodern OLTP indexes (BwTree [ 28] and ART [ 27]) focusing\non the impact of locality.\nOur work is orthogonal to all these work as we investi-\ngate how well a learned index structure utilize the micro-\narchitectural resources of a modern processor in comparison\nto non-model based in-memory indexes. As learned indexes\nare fundamentally different than the index structures that\nare not based on machine learning models, the findings of\nthe previous studies are not representative for learned in-\ndexes. As learned indexes are fairly new, this aspect of their\nperformance has not been studied in detail yet. Our work is\nan additional step in understanding the impact and charac-\nteristics of learned indexes.\nThe most comprehensive benchmarking work for learned\nindexes is done by Marcus et al. [ 30]. This work compares\nthree flavors of learned indexes to tree-based, trie-based,\nhash-based, etc. index structures. The authors also touch\nupon the impact of caching and cache misses at a high-level.\nHowever, they do not perform a detailed breakdown of stalls\ninto different micro-architectural components or cache levels.\nTherefore, our work is complementary to this study.\n3 Experimental Methodology and Setup\nAs we highlighted in the previous sections, our high-level\ngoal in this paper is to complement related work and ob-\nserve how learned indexes utilize the micro-architectural\ncomponents of a modern commodity processor. To achieve\nthis goal, we ask the following questions:\n‚Ä¢Where does time go when we use learned indexes? Are\nexecution cycles mostly wasted on memory stalls or used\nto retire instructions?\n‚Ä¢Are memory stalls mainly due to instruction or data\naccesses?\n‚Ä¢How much instruction-level parallelism can learned in-\ndexes exploit?\n‚Ä¢How much do the data size, the workload type, and data\naccess and insertion patterns impact the answers to the\nquestions above?\n‚Ä¢How different is the behavior of the learned indexes in\ncomparison to older non-model based index structures?\nThe following subsections describe our experimental method-\nology and setup to answer these questions.\n3.1 Systems used\n3.1.1 Software. We scope our experimental study to in-\nmemory-optimized indexes for OLTP workloads. Based on\nour methodology, this work could be easily expanded to in-\ndexes optimized for different workloads. As mentioned in\nSection 1 and detailed in Section 2.1, we specifically picked\nlearned index ALEX [ 9], ART [ 7], and B+Tree [ 4]. ALEX\n[10] is a state-of-the-art proposal for an adaptive updatable\nlearned index. Therefore, it fits very well for our scope. Com-\nparing it to ART and B+Tree helps with making our results\n3\n\nConference‚Äô17, July 2017, Washington, DC, USA Mikkel M√∏ller Andersen and Pƒ±nar T√∂z√ºn\nProcessor Intel(R) Xeon(R)\nPlatinum 8256 CPU\nClock Speed 3.80GHz\nNo. of sockets 2\nIssue width 4\nCores per socket 4\nL1 data cache 32 KB\nL1 instruction cache 32 KB\nL2 cache 1 MB\nL3 cache (shared) 16.5 MB\nMain memory 192 GB\nTable 1. Specifications of the server.\ncomparable to existing studies [ 6,10,30]. The open-source\ncodebases used for ALEX and B+Tree are written in cpp and\nART is written in c.\n3.1.2 Hardware. All the experiments were run through\nIntel DevCloud [ 8] on an Intel Xeon processor (from the\nfamily of Intel‚Äôs Scalable Processors), which is representative\nof commodity server hardware used for many data-intensive\napplications and systems. The full parameters of the server\ncan be seen in Table 1. The OS installed on this server was\nUbuntu 18.04.4 LTS.\n3.2 The benchmark driver\nTo compare the three index structures, we have a lightweight\nbenchmark driver written in cpp for each index. This driver\ncreates a YCSB-like [ 5] workload mixes to mimic key-value\nsystem requests considering the index entries as key-value\npairs. More specifically, there are four types of requests:\n(1)read ing a key-value pair‚Äôs value given a key, (2) up-\ndate ing key-value pair‚Äôs value given a key, (3) insert ing\na new key-value pair, and (4) delete ing a key-value pair.\nImplementation-wise, we directly use the read, insert, etc.\ninterfaces provided by the index codebases.\nEach experiment run using this driver is composed of\nfour phases: (1) the data population with the given input\ncharacteristics and size, (2) the warm-up with the given\nnumber of requests, (3) the workload run with the given\nmix of read, update, insert, delete requests, and (4) the wrap-\nupwith the result summary statements. All the reported\nresults in Section 4 is measured from the third phase of the\nexperiments.\nThe key arguments given as input to the benchmark driver\nto generate customized workloads are the following: (1) the\nnumber of initial key-value pairs to populate the index with;\n(2) the number of requests to run after the initial data popu-\nlation and warm-up phases are over; (3) the distribution of\nrequests among reads, inserts, updates, and deletions; (4) the\nupper and lower bound for the input keys in read requests\nduring the workload run, where the keys to be looked upPattern Consecutive Consecutive Random\nKeys after data population\n# keys 160 million 1.6 billion 1.6 billion\nlower range 0 0 0\nupper range 160 million 1.6 billion 3.2 billion\nWorkload run\n# requests 160 million 1.6 billion 1.6 billion\nTable 2. Parameters related to data and workload generation\nfor the three sets of experiments.\nare picked at random; (5) the upper and lower bound for\nkeys to be inserted; (6) and the flag to indicate whether to\ncreate keys to be inserted in consecutive order or randomly\nboth during population phase and workload run. Values can\nbe created based on a range as well, but the exact value for\nthem is less crucial. Finally, the data type for keys and values\nare currently hand-coded to 64bit unsigned integer, and the\nrandom access pattern is uniform across the range given.\n3.3 The generated workloads\nTable 2 gives a summary of the options we used to generate\nthe workloads for the experiments in Section 4. Overall, we\ngenerate three sets of experiments. These sets of experiments\nhelp us observing the impact of both data creation and access\npatterns and data sizes.\nIn the first two sets, we have a consecutive data pattern,\nwhere we experiment with two dataset sizes. As also men-\ntioned above, this means that keys are both populated and\ninserted in consecutive order. This case is good for having a\ndense balanced index structure right after data population.\nIn addition, for a learned index like ALEX, this helps in gen-\nerating precise models right after index population, which\nbenefits read-heavy scenarios. However, it could stress the\nindexes for the insert-heavy cases, since the inserts are all\nout of bounds of the current learned range. The consecutive\ncase also helps us with only requesting keys that exist in the\nindex in read requests, which is performed at random based\non the already known key range.\nTo contrast this, we have the random key generation\nboth for data population and inserts during the workload\nrun based on a given range, which would help ALEX with\nthe inserts because of its adoption of gapped arrays (see\nSection 2.1). In this scenario, the read requests during the\nworkload run may ask for keys that do not exist in the index.\nWe picked the upper key range in this scenario large enough\nto generate a good random pattern for inserts, but small\nenough to avoid too many reads for keys that do no exist in\nthe index.\nWithin these three sets, we generate four workload types\nby changing the distribution of different request types: (1)\nRead only consisting of 100% reads; (2) Read heavy con-\nsisting of 80% reads, 10% updates, and 10% inserts; (3) Write\n4\n\nMicro-architectural Analysis of a Learned Index Conference‚Äô17, July 2017, Washington, DC, USA\nheavy consisting of 40% reads, 30% updates, 20% inserts, and\n10% deletes; and (4) Write only consisting of 100% inserts.\nThe warm-up phase for each experiment is 100,000 ran-\ndom read requests. We report results from a single run of\neach experiment in Section 4, where total number of requests\nare in millions or billions as reported by Table 2. Because\nof our large sample size during runs, we did not repeat the\nexperiments. On the other hand, during our test runs with\nsmaller samples, we did not observe any pathological cases\nor outliers. Finally, all the experiments are single-threaded.\nWhile this is partially the limitation of the codebases we are\nusing, this is not a limitation for the main goal of this study.\nWhen it comes to investigating how well a codebase utilizes\nthe micro-architectural resources of a core, focusing on the\nbest-case scenario, where the program can utilize a single\ncore without waiting for other concurrent threads, is desired.\n3.4 Reported metrics and how they are measured\nTo align ourselves with Intel‚Äôs Top-Down Micro-architecture\nAnalysis Method (TMAM) [ 36,39], we organize our metrics\ninto four levels going from the top-level metrics to finer-\ngranularity lower-level metrics: overall performance ,break-\ndown of execution cycles ,breakdown of back-end stalls ,break-\ndown of memory stalls . Next, we detail these levels as well as\nTMAM. The summary of all metrics can be found in Table 3.\n3.4.1 Overall performance. These metrics quantify the\nbehavior of indexes we focus on at a high-level.\nMemory footprint reports the total memory used by the\nindexes after an experiment with a particular workload is\nover. This is measured with Linux topcommand [ 29], which\nwas set to report results every second and write the output\nto a file. We selected the last updated value before the ex-\nperiment ended to report in this paper. This reported value\ncontains the memory footprint of both the benchmark driver\nand the indexes. However the benchmark driver only stores\na few variables. Therefore, its memory footprint is negligible\ncompared to the index. We also confirmed this by looking\nat the statistics reported by ALEX codebase, which outputs\nthe size of the index, and comparing what was reported by\nALEX to what we got from top. This metric gives us an idea\nabout the data footprint of the different index structures.\nExecution time reports the average time it takes to ex-\necute a workload request (e.g., lookup, insert). Our bench-\nmark driver uses std::chrono::high_resolution_clock\nto capture the time it takes to complete the total number of\nrequests issued by a workload after the indexes are already\npopulated with the initial dataset and a warm-up period\nhas passed. Then, this total execution time is divided to to-\ntal number of requests issued to get the average execution\ntime per request. We confirmed the negligible impact of our\nbenchmark driver to the overall execution time by checking\nthe percentage of time it takes using Intel VTune Profiler‚ÄôsLevel 1: Overall Performance\nMemory footprint\nExecution time\nInstructions retired per request\nCycles per Instruction\nLevel 2: Breakdown of execution cycles\nRetiring\nBad speculation\nFront-end bound\nBack-end bound\nLevel 3: Breakdown of back-end stalls\nCore bound\nMemory bound\nLevel 4: Breakdown of memory stalls\nL1 bound\nL2 bound\nL3 bound\nDRAM bound\nStore bound\nTable 3. Top-down analysis metrics (with levels listed from\ntop to bottom).\nHotspot analysis [ 19]. This metric gives us an idea about the\noverall efficiency of the different index structures.\nInstructions retired per request reports the average num-\nber of instructions retired to execute a workload request (e.g.,\nlookup, insert). The total number of instructions retired from\nrunning a particular experiment is collected from VTune.\nVtune allows us to instrument the benchmark driver code\nto collect this information only for the workload run phase\nof the experiments using Instrumentation and Tracing Tech-\nnology APIs [ 18]. We, then, divide this total number with\nthe total number of requests issued in the experiment. We\nare aware that the benchmark driver itself contributes to the\ntotal number of instructions retired, but the impact is negli-\ngible as mentioned under Execution Time . This metric gives\nus an idea about the instruction footprint of the different\nindex structures.\nCycles per instruction , which is the inverse of instruc-\ntions retired per cycle, reports the average number of execu-\ntion cycles it takes to retire instructions during a workload\nrun. It is calculated by dividing the total cycles used to to-\ntal number instructions retired, which are both reported by\nVTune. It gives us an idea about how much instruction-level\nparallelism the different index structures exhibit. The lower\nthis value is the higher instruction-level parallelism a pro-\ngram has, spending on average fewer cycles per instruction.\nThe theoretical minimum for this value on the hardware used\nin our experiments is 0.25, since the modern Intel processors\ncan issue (and retire) up to four instructions in a cycle.\n5\n\nConference‚Äô17, July 2017, Washington, DC, USA Mikkel M√∏ller Andersen and Pƒ±nar T√∂z√ºn\n3.4.2 Breakdown of execution cycles. It is extremely\ndifficult to perform a precise breakdown of execution cycles\ninto where they come from at the micro-architectural level\non a modern OoO processor. As Section 2.2 mentions, mod-\nern processors adopt various techniques to enable implicit\nparallelism within a core and overlap the stall cycles due\nto various micro-architectural components. Earlier studies\n[1,11,34,38] took the approach of using hardware counters\nto measure cache misses from different levels of the cache hi-\nerarchy and then multiplied them with the associated latency\nfor a miss from that particular level. While this method gives\nan idea of instruction or data access bottlenecks, it ignores\nthe overlaps and may attribute a higher cost to a particular\nmiss than in reality. Therefore, in 2014, folks from Intel pro-\nposed Top-Down Micro-architecture Analysis Method (TMAM)\n[20,39], where the new-generation Intel processors were\nexpanded with hardware event counters and a methodology\nfor using them to more precisely determine which micro-\narchitectural components cause instruction pipeline to stall.\nIn this methodology, the micro-architectural components are\nviewed top-down, and the execution time is broken down\ninto these components starting from front-end and back-\nend (Section 2.2) at the highest-level and then diving deeper.\nSirin et al. [ 36] report precisely which counters are used to\ncalculate the different levels and components of TMAM, and\nIntel‚Äôs VTune Profiler [ 19] provides an API to perform this\nanalysis, which we also utilize in this work.\nFollowing TMAM, we also break down execution cycles\ninto four components at this level: Retiring ,Bad speculation ,\nFront-end bound , and Back-end bound .\nFor a CPU with an issue width of four, like the one used in\nour experiments, the cores have four pipeline slots meaning\nthat for each clock cycle the front-end can fill the pipeline\nslots with up to four ùúáOps and the back-end can retire up to\nfourùúáOps. When there are no stalls, ùúáOps can either retire,\ncontributing to the Retiring category, or they do not retire;\nmainly due to branch misprediction, which is attributed to\ntheBad speculation category.\nIf a pipeline slot is empty during a cycle it will be counted\nas a stall. The stall will be attributed to the front-end, front-\nend bound , if the front-end was unable to fill the pipeline\nslot; for example due to instruction cache misses.\nIf the front-end is ready to deliver the ùúáOps, but the back-\nend is not ready to handle it then the stall is attributed to the\nback-end, back-end bound . In the case that both the front-\nend is not able to deliver a ùúáOp and the back-end is not ready\nto handle it, then the stall will be attributed to the back-end.\nThe reason is that if the the front-end was optimized and\ntherefore ready to deliver the ùúáOps, there would still be a\nstall since the back-end is not ready to handle it.\n3.4.3 Breakdown of back-end stalls. The stalls due to\nfront-end and back-end can be further broken into their\ncomponents. In this work, we report only breakdown ofthe back-end stalls based on two reasons. First, the back-\nend bound is the major component of stalls for the index\nstructures we evaluate (see Section 4). Second, the front-end\nbound typically hints large instruction footprint and poor\ncode layout; it can be attributed to the instruction related\nmisses for the most part.\nThe back-end stalls can be of two main components: Core\nbound orMemory bound .Core bound stalls represent the\nstalls due to data/resource dependencies, whereas memory\nbound stalls are mainly due to data misses.\n3.4.4 Breakdown of memory stalls. Lastly one can iden-\ntify which level of the memory hierarchy the memory bound\nstalls originate from, by digging deeper into the memory\nbound category from the breakdown of the back-end stalls.\nL1 bound category represents stalls despite hitting the L1\ncache, which could be due to data TLB misses, pressuring the\nL1 data cache bandwidth due to a large number of requests,\netc.L2 bound ,L3 bound , and DRAM bound categories,\nrespectively, represent stalls due to data misses that miss in\nL1, L2, and L3, but hit in L2, L3, and DRAM. Finally, Store\nbound category represents how often the core encountered\na stall on store operations because the store buffer is full.\nAt this level, the measurements are less precise than the\nhigher-levels, meaning that slight overlaps in stall time could\nbe double counted [ 17]. This leads to the situation that sum of\n% breakdown values reported by VTune will not necessarily\nmatch with their parent (one higher-level) value. However,\nthey do still give a good indication of where the bottlenecks\nare. In this paper, we normalized these metrics to match the\nparent-level Memory bound % with the formula below, where\nùëÄis the metric we want to normalize, e.g. L1 bound.\nùëÄùëõùëúùëüùëö=ùëÄ¬∑(ùëöùëíùëöùëúùëüùë¶ùëèùëúùë¢ùëõùëë)\nùêø1+ùêø2+ùêø3+ùê∑ùëÖùê¥ùëÄ+ùëÜùë°ùëúùëüùëí\n4 Experimental Results\nFollowing the setup and methodology described in the previ-\nous section, we now present the results of our experiments\nto answer the questions listed at the beginning of Section 3.\nWe group the results in four parts following the four levels\nof metrics Section 3.4 describes. First, Section 4.1 presents\nthe results for overall high-level performance metrics. Then,\nSection 4.2, Section 4.3, and Section 4.4 perform a top-down\nmicro-architectural analysis and break down the execution\ncycles into increasingly finer-granularity components.\n4.1 Overall Performance\n4.1.1 Memory footprint. We first focus on the memory\nfootprint of the indexes, which Table 4 reports. As Section 3.4\nexplains, the reported numbers are from the end of the work-\nload runs including the impact of the newly inserted keys\nduring those runs on the index size. Therefore, the footprint,\nand the size, of the index for the read-only workload is the\nsmallest for all cases.\n6\n\nMicro-architectural Analysis of a Learned Index Conference‚Äô17, July 2017, Washington, DC, USA\nConsecutive Random\nALEX 160M keys 1.6B keys 1.6B keys\nRead Only 3.706 34.431 34.448\nRead Heavy 4.158 38.482 34.448\nWrite Heavy 4.564 42.534 34.448\nInsert Only 7.805 74.765 46.109\nART 160M keys 1.6B keys 1.6B keys\nRead Only 8.128 80.417 70.506\nRead Heavy 8.628 85.185 73.328\nWrite Heavy 8.652 85.415 74.137\nInsert Only 20.270 125 93.267\nB+Tree 160M keys 1.6B keys 1.6B keys\nRead Only 3.100 28.603 28.603\nRead Heavy 3.731 34.638 46.379\nWrite Heavy 4.363 40.673 49.519\nInsert Only 9.426 88.953 61.927\nTable 4. Memory footprint for the different indexes at the\nend of each experiment in GB.\nFrom Table 4, we see that ALEX overall has slightly larger\nmemory footprint compared to B+Tree, except for the case\nofinsert-only workload. On the other hand, both B+Tree and\nALEX are approximately half the size of ART. This trend is\nactually different than what is reported by Ding et al. [ 10],\nespecially for B+Trees. However, we are looking at the whole\nmemory footprint of indexes, rather than just the index size,\nwhich may impact our conclusions. We also have slightly\ndifferent workloads and data patterns.\nWhen we insert out-of-bound keys, meaning that the new\nkey is higher than the current largest key value, the memory\nfootprint of all indexes grow drastically. Keep in mind that\nour experiments are based on total number of requests and\nnot time as Section 3.3 describes. Therefore, we know that\nindexes have exactly the same amount of keys inserted at the\nend of the insert-only experiments for the consecutive pattern.\nThe memory footprint of ALEX is twice of its initial size after\ntheinsert-only workload. B+Tree gets an even bigger hit with\ntripling in size. ART‚Äôs footprint also more than doubles for\nthe initial dataset size of 160 million keys, but when the\ninitial size is already big (1.6 billion keys), the growth is\nsmaller (approx. 50%).\nFinally, the scenario with random inserts behave slightly\ndifferently. ALEX does not exhibit a growth except for the\ninsert-only case, because in the random scenario the inserts\ncould be handled easily by the gapped array structure of\nALEX (Section 2.1), and do not trigger heavy structure modi-\nfication operations. Overall, all the indexes exhibit a smaller\ngrowth in their memory footprint for the insert-only sce-\nnario. We suspect this is due to fewer structural modification\noperations triggered in general with the random pattern.4.1.2 Execution Time. Figure 1 plots the execution time\nper request in ùúásecs for all the experiments. Figure 1a and\nFigure 1b focus on the experiments, where the keys are con-\nsecutive from the given range as Section 3.3 detailed. Except\nfor the insert-only workload, ALEX outperforms both ART\nand B+Tree in all workload cases, completing requests at\nleast in half the time it takes for ART and B+Tree. This is\nas expected and corroborates the results from Ding et al.\n[10]. The consecutive keys help with the model predictions,\nespecially in read-only and read-heavy cases for ALEX.\nOn the other hand, inserting keys consecutively from out-\nof-bound values, is a costly operation for ALEX as the index\nhas to expand the root. Normally, when ALEX expands a\nnode, it either scales or retrains its model and then re-insert\nall the elements into the new expanded node based on the\nprediction of the scaled/retrained model. As described by\nDing et al. [ 10], though, after some point, if ALEX realizes\nthe append-only behavior in inserts, it stops model-based\nre-insertions, and just appends to the expanded space. The\nadvantage of this is to avoid, re-modeling and re-inserting fre-\nquently. This routine leads to interesting micro-architectural\nbehavior, as the following subsections will cover.\nOverall, while the average execution time per request\ndecreases as we increase the insert in ART and B+Tree in the\nconsecutive case, we do not observe this for ALEX, whose\nrelative performance becomes similar to or worse than ART\nand B+Tree. In addition, the increase in data size, cause an\nincrease in the average execution time for requests for each\nindex, which is expected as more data is traversed.\nIn the case of random routine, Figure 1c, where the initial\nkey range and later inserts are all randomly done from a\ngiven range, we observe a different trend. ALEX always\noutperforms the other two indexes and shows fairly stable\nperformance across different workloads, except for a slight\nincrease in the average execution time for the insert-only\nworkload. Compared to the consecutive dataset, the overall\nexecution time is slightly higher for ALEX with random keys.\nWe see similar trends for B+Tree, but its average execution\ntime per request exhibits a performance hit with random\nkeys. For ART, on the other hand, except for the random\ninsert-only workload, the average execution time per request\nis lower in the random case if we compare results for 1.6\nbillion keys from Figure 1b and Figure 1c.\nIn the random scenario, the workloads with read requests\nhave a non-negligible possibility of requesting a key that\ndoes not exist in the index (see Section 3.3). Therefore, this\npattern leads to higher execution time per request in ALEX in\ntwo ways. First, the models could be less precise due to more\nrandom distribution of keys. Second, we pay the penalty of\nincreased number of exponential search routines for non-\nexisting keys compared to random read request in Figure 1b.\nIn addition, in the case of ART, we know that a successful\nsearch in radix trees is slower than an unsuccessful one as\n7\n\nConference‚Äô17, July 2017, Washington, DC, USA Mikkel M√∏ller Andersen and Pƒ±nar T√∂z√ºn\n(a)Consecutive - 160 million keys\n (b)Consecutive - 1.6 billion keys\n (c)Random - 1.6 billion keys\nFigure 1. Execution time per request in ùúásec.\nmentioned by Leis et al. [ 27]. This explains ART‚Äôs opposite\nbehavior compared to ALEX and B+Tree in this scenario.\n4.1.3 Instructions retired per request. Figure 2 has the\ninstructions retired per request across all experiments to\ngive an idea of the instruction footprint of the three indexes.\nWhat jumps out from Figure 2a and Figure 2b is that on aver-\nage ALEX has an order of magnitude increase in the number\nof instructions it uses to complete an out-of-bound insert\nrequest compared to a read operation. This is due to retrain-\ning of the model after so many inserts in the workload. Even\nif ALEX stops re-modeling and re-inserting after detecting\nthe append-only pattern as mentioned in Section 4.1.2, it\nmay have already performed a model update before that stop\nhappens. As a result, this impacts the average execution time\nas well as average instruction footprint of such insert opera-\ntions. On the other hand, as we will see in Figure 3 in the next\nsection, these instructions exhibit a high instruction-level\nparallelism, which, in turn, prevents a drastic increase in the\noverall execution time as we see in Figure 1.\nOverall, the increase in the % of inserts in the workload\nincrease the instruction footprint for all indexes, but the\nimpact is not as drastic as it is in the case of ALEX. In addition,\nfor the case of random inserts, presented in Figure 2c, ALEX\nalso gets effected less as the overheads of such inserts can\nbe prevented by the gapped array structure.\nWhen comparing Figure 2a and Figure 2b, we also observe\nthat increasing the initial index size 10-fold naturally has\nimpact on instruction footprint per request, as it may take\nlonger time to traverse the tree. This impact is not very\nhigh in most cases, except for ART, where it doubles the\ninstructions retired per request for read-only andread-heavy\nworkloads.\n4.1.4 Cycles per instruction (CPI). Figure 3 shows the\ncycles spent retiring an instruction on average across all the\nexperiments. The smaller this value is the better, indicating\nthat the processor core can exploit more instruction-levelparallelism issuing and retiring more instructions in a cycle\n(Section 2.2). On the server we are using for these experi-\nments, the minimum for this value is 0.25 as mentioned in\nSection 3.4.\nData-intensive applications, especially transaction pro-\ncessing applications, are famous for exhibiting low instruction-\nlevel parallelism (Section 2.3). The reason for this, especially\nfor modern in-memory-optimized systems, is that frequent\nrandom data accesses lead to long-latency stalls trying to\nfetch data all the way down from main memory, when they\ndo not hit in the L1 data cache. In addition, the complex\ninstruction footprint of traditional disk-based data manage-\nment systems cause high front-end stalls.\nALEX, ART, and B+Tree are all in-memory index struc-\ntures targeting transaction processing workloads. They do\nnot encapsulate all the instruction complexity for a data\nmanagement system, since they just represent part of a data\nmanagement system. On the other hand, index operations are\nthe most common operations for most transactional work-\nloads. Therefore, our expectation is to observe similar micro-\narchitectural trends to in-memory transaction processing\nsystems with heavy instruction footprint optimizations (as\nin HyPer results from [ 34]) here and in the following subsec-\ntions that detail the micro-architectural behavior.\nResults from Figure 3 demonstrate that ALEX exhibit the\nhighest level of instruction-level parallelism across all the\nexperiments compared to ART and B+Tree. This corrobo-\nrates the fundamental design principle for learned tree in-\ndexes, where a trade-off for increased computation is made\ninstead of memory accesses [ 3]. Especially, with the insert-\nonly workload inserting consecutive keys (Figure 3a and\nFigure 3b), ALEX comes very close to the theoretical min-\nimum with a CPI value of 0.36. This is reasonable as the\nlarge instruction footprint reported in Section 4.1.3 is mainly\ncomputation heavy, which does not lead to too many data\nfetches from main-memory. In addition, both model training\n8\n\nMicro-architectural Analysis of a Learned Index Conference‚Äô17, July 2017, Washington, DC, USA\n(a)Consecutive - 160 million keys\n (b)Consecutive - 1.6 billion keys\n (c)Random - 1.6 billion keys\nFigure 2. Instructions retired per request.\n(a)Consecutive - 160 million keys\n (b)Consecutive - 1.6 billion keys\n (c)Random - 1.6 billion keys\nFigure 3. Cycles per Instruction - CPI (theoretical minimum is 0.25 on our server).\nand possible later append operations do not exhibit com-\nplex program logic that could leave branch predictors and\nnext-line prefetching in modern processors ineffective.\nFor workloads with high percentage of reads or for ran-\ndom insert patterns, all three indexes spend more cycles per\ninstruction as a result of increased random data fetches in\na unit of time. This is a natural side-effect of a tree-index\ntraversal, which the results from the following subsections\nwill further corroborate.\nThe reduction in CPI for read-only andread-heavy work-\nloads as the data size is increased (comparing Figure 2a and\nFigure 2b) also underlies the effect of traditional index tra-\nversal in ART and B+Tree. The way these tree-based indexes\nare designed, the level of the trees do not increase propor-\ntionally to the data size avoiding proportional increase in\nrandom data access requests to main-memory. Furthermore,\nthe traversal code is also a tight-loop with relatively small\ninstruction footprint that can be kept in the L1 instruction\ncache. These two fundamental characteristics of tree-based\nstructures benefit the scalability with respect to data size for\nall three indexes. On the other hand, CPI for ALEX increasesfor the same scenario, while still being smaller than the other\ntwo indexes as a result of the trade-offs mentioned above.\nThis is likely the impact of traversing a deeper hierarchy of\nmodels with the larger dataset.\nWe also see the positive impact of data locality for ART\nand B+Tree in the insert-only workload when comparing the\nconsecutive (Figure 3b) and random (Figure 3c) inserts.\nFinally, overall, the trends in Figure 1 and Figure 3 match\nfor the cases of the large dataset sizes, and can be explained\nwith similar reasoning. On the other hand, for the small\ndataset size, despite exhibiting a high CPI value, ART has\nmore efficient execution time per request compared to the\nB+Tree, which may come as a result of its lower instruction\nfootprint per request in Figure 2.\n4.2 Breakdown of execution cycles\nSection 4.1 demonstrated the top-level performance compar-\nison across ALEX, ART, and B+Tree. While interpreting the\nresults from the top-level metrics, we already hinted at cer-\ntain causes at the micro-architectural-level for the observed\n9\n\nConference‚Äô17, July 2017, Washington, DC, USA Mikkel M√∏ller Andersen and Pƒ±nar T√∂z√ºn\n(a)Consecutive - 160 million keys\n (b)Consecutive - 1.6 billion keys\n (c)Random - 1.6 billion keys\nFigure 4. Breakdown of execution cycles (normalized).\n(a)Consecutive - 160 million keys\n (b)Consecutive - 1.6 billion keys\n (c)Random - 1.6 billion keys\nFigure 5. Breakdown of back-end stalls (normalized).\nbehavior. Now, in this and the following two subsections, we\nquantify such causes in more detail.\nFigure 4 plots the breakdown of execution cycles into top-\nlevel micro-architectural components for all the experiments.\nWe normalize the cycles to 100% and report the breakdowns\nin terms of percentages instead of absolute values, since\nSection 4.1.2 already compared the execution time for all\nthree indexes in absolute values. Later, before closing our\nanalysis of the experimental results, Section 4.4 also gives a\ndetailed breakdown of the execution cycles over the absolute\nexecution time without normalizing, to circle back to the\nresults presented earlier.\nWith the expected exception of the insert-only workload\nwith consecutive pattern, Figure 4 emphasizes that majority\nof the execution time for all index structures goes to back-\nendstalls. The time spent in retiring instructions is relatively\nsmall, as well as the stalls due to bad speculation andfront-\nend. This behavior is expected, since index operations are\ndata access heavy, leading to many random data accesses to\nmain-memory (as Section 4.4 will further underline).The portion of execution time ALEX spends in retiring\ninstructions increases as the workload exhibits more out-of-\nbound inserts (Figure 4a and Figure 4b) as well as the front-\nendcomponent. This is expected looking at the instructions\nrequired per request in Figure 2 for the same cases.\nOverall, ALEX spends a smaller percentage of its execu-\ntion time in back-end stalls compared to ART across all the\nexperiments and percentage of back-end stalls is similar be-\ntween ALEX and B+Tree. However, B+Tree spends a higher\nportion of time in front-end stalls and bad speculation . In\nterms of more stable or robust micro-architectural behavior,\nthough, ART‚Äôs behavior does not change drastically across\nthe experiments, even with the extreme case of the insert-\nonly workload with consecutive out-of-bound inserts. ART is\nthe most back-end bound compared to the other two indexes.\nThis can be attributed to both its larger memory footprint\n(Section 4.1.1) and small instruction footprint (Section 4.1.3)\nincreasing the pressure on the back-end of a processor.\nFinally, for the random case illustrated in Figure 4c, we do\nnot see drastic changes in the behavior of the breakdown\n10\n\nMicro-architectural Analysis of a Learned Index Conference‚Äô17, July 2017, Washington, DC, USA\nacross different workload patterns. Here the random data ac-\ncesses are simply the core operation all the indexes perform,\nwhich leads to this outcome.\n4.3 Breakdown of back-end stalls\nSince the execution cycles are mainly back-end bound in Fig-\nure 4, Figure 5 breaks down the back-end stalls further into\nmemory bound andcore bound components for all the exper-\niments. Figure 5 keeps the total execution time normalized\nat 100%, while the total of the memory bound andcore bound\ncomponents sums up to the back-end bound component from\nFigure 4.\nAs expected, except for the case of out-of-bound inserts,\nmajority of the back-end stalls are memory bound , because\nof the random data accesses. In addition, while small, the\ncore bound component tends to be bigger for ALEX, because\nof the more compute-heavy nature of learned indexes. This\nmay cause slightly increased pressure on the execution units\nin the back-end.\nFor the case of out-of-bound inserts (Figure 5a and Fig-\nure 5b), ALEX spends almost no time in memory bound stalls,\nwhich is interesting to observe and highlights the extremely\ncompute-heavy nature of model re-training triggered by this\nworkload pattern. B+Tree exhibits the impact of data locality\nin this case, whereas ART‚Äôs lower instruction and larger data\nfootprint keeps the memory bound component big.\nFinally, the trends across different dataset sizes and data\ninsert patterns, aligns with the insights we had for the CPI\nresults in Figure 3, and can be attributed to the index traversal\nlogic as explained in Section 4.1.4. To re-iterate, the random\ninserts (Figure 5c) as well as random read requests ( read-only\nandread-heavy ), are bound by the impact of the random data\naccesses. Increasing the data size leads to more cache locality\nin both instruction and data accesses for the tree-based index\nstructures that are not based on models such as ART and\nB+Tree. On the other hand, the increased number of models\nin ALEX with the increased data size leads to an increased\nmemory bound behavior.\n4.4 Breakdown of memory stalls\nSince the back-end bound cycles are mainly memory bound\nin Figure 5, in this section, we break the memory bound stalls\nfurther into where they come from within the memory hier-\narchy. This section illustrates the results in two forms. First,\nFigure 6 plots this breakdown with respect to normalized\nexecution time. This means that the total of the presented\ncomponents in Figure 6 sums up to the memory bound com-\nponent from Figure 5. Then, Figure 7 shows this breakdown\nwith respect to the absolute values for the execution time\nfrom Figure 1 presenting the absolute time for various mem-\nory bound stalls per request. This also allows us to circle back\nto the top-level performance metrics we presented earlier\nthis section. The y-axes in Figure 7 is, therefore, kept the\nsame as the results in Figure 1 for better comparison.Figure 6 demonstrates that majority of the memory bound\nstalls are due to long-latency data misses from the last-level\ncache (L3), which is labeled as DRAM bound , for all indexes.\nThis is followed by the data misses from L2 cache that hit\nin L3 cache, which the L3 Bound component represents. In\ncontrast, L2 Bound component, which shows stall time due\nto L1 data cache misses, is quite small as well as the L1 Bound\ncomponent, which is mostly due to DTLB misses in our case.\nThis behavior of the memory stalls can be explained through\nboth the hardware and software behavior. On the one hand,\nmodern OoO processors are designed to overlap the latency\nfor the L1 data misses for the most part, which is 7-8 cycles.\nHowever, the latency for L2 and L3 misses are larger (17 and\nover 200 cycles, respectively). Therefore, it becomes hard to\noverlap all the associated latency due to frequent random\ndata accesses to L3 and DRAM. On the other hand, the tree-\nbased index traversal logic triggered by the get/put/update\nrequests to indexes in our (and typical transactional) work-\nloads, exhibit very low temporal and spatial locality leading\nto many cache misses from all levels of the cache hierarchy.\nTherefore, except for the accesses to the higher-levels of the\nindex (e.g., root), most data accesses end up in DRAM.\nThe case that shows different behavior is again the out-of-\nbound inserts in Figure 6a and Figure 6b. ALEX and B+Tree\nexhibit almost no stalls due to memory accesses here as\nSection 4.3 also covered.\nFinally, Figure 7 shows that, in absolute values, ALEX\nspends the least amount of time in memory bound stalls com-\npared to ART and B+Tree, which aligns with its normalized\nvalues as well. On the other hand, ART and B+Tree spend\nsimilar amount of time in memory bound stalls in consecutive\ncase, while ART spends shorter time in memory bound stalls\ninrandom case, despite the normalized values in Figure 6.\nThis is a result of ART‚Äôs more efficient utilization of the\nfront-end component, which is very small for ART in the\nnormalized case, increasing the impact and portion of the\nmemory bound . Therefore, it is important to have both the\nnormalized and absolute perspectives while breaking down\nthe execution cycles into micro-architectural components.\n5 Conclusion\nIn this paper, we performed a micro-architectural analysis\nof a learned index, ALEX, and compared its behavior to two\ntree-based indexes that are not based on learned models, ART\nand B+Tree. Our study complements existing experimental\ncomparison results for learned indexes by providing a lower-\nlevel view of how well learned indexes utilize the front-end,\nback-end, memory hierarchy, etc. of modern commodity\nservers with OoO processors.\nWe used a variety of workloads with different % of read\nand write requests as well as different data insert and access\npatterns and sizes. In summary, our results show that, long-\nlatency data misses is at the core of ALEX similar to the other\n11\n\nConference‚Äô17, July 2017, Washington, DC, USA Mikkel M√∏ller Andersen and Pƒ±nar T√∂z√ºn\n(a)Consecutive - 160 million keys\n (b)Consecutive - 1.6 billion keys\n (c)Random - 1.6 billion keys\nFigure 6. Breakdown of memory stalls (normalized).\n(a)Consecutive - 160 million keys\n (b)Consecutive - 1.6 billion keys\n (c)Random - 1.6 billion keys\nFigure 7. Breakdown of memory stalls per request in ùúásec.\ntwo indexes. This is expected as the short get/put requests\ncause frequent random data accesses that lack locality, which\nstress the memory hierarchy of a processor. On the other\nhand, ALEX exhibits higher instruction-level parallelism and\nfewer stalls cycles in general. This is a result of the fact\nthat learned indexes trade-off increased computations to\nreduced memory accesses, which could boost performance\nif the overall computation cycles are cheaper with respect\nto memory access latency [ 3]. In the case of modern OoO\nprocessors, computation cycles are indeed cheaper if the\ninstruction footprint is not complex, i.e., does not have too\nmany dependencies. Even if ALEX‚Äôs instruction footprint\ndrastically increases in the case of inserting out-of-bound\nkeys, these instructions are less stall prone, since they exhibit\nvery high instruction-level parallelism.\nGoing forward, investigating the behavior for the same\nindex structures for workload patterns that dynamically\nchange over time during a single run would be interesting\nto observe the impact of various insert heavy periods on\nlater read heavy periods of execution. In addition, in thisstudy we mainly looked at uniformly random data access\npatterns. Adopting a variety of access patterns as well as\nmore complex insert behavior would also be an interesting\nfuture analysis.\nAcknowledgements\nThe authors would like to thank Intel DevCloud [ 8] for pro-\nviding the server and the maintainers of the open-source\ncodebases used used by the experiments in this paper.\nReferences\n[1]Anastassia Ailamaki, David J. DeWitt, Mark D. Hill, and David A.\nWood. 1999. DBMSs on a Modern Processor: Where Does Time Go?.\nInVLDB . 266‚Äì277.\n[2]Ahsan Javed Awan, Mats Brorsson, Vladimir Vlassov, and Eduard\nAyguade. 2016. Micro-Architectural Characterization of Apache Spark\non Batch and Stream Processing Workloads. In BDCloud-SocialCom-\nSustainCom . 59‚Äì66.\n[3]Peter Bailis, Kai Sheng Tai, Pratiksha Thaker, and Matei Zaharia.\n2018. Don‚Äôt Throw Out Your Algorithms Book Just Yet: Classi-\ncal Data Structures That Can Outperform Learned Indexes. (2018).\nhttps://dawn.cs.stanford.edu/2018/01/11/index-baselines/ .\n12\n\nMicro-architectural Analysis of a Learned Index Conference‚Äô17, July 2017, Washington, DC, USA\n[4]Timo Bingmann. Downloaded on March 15, 2021. STX B+ Tree Version\n0.9. (Downloaded on March 15, 2021). https://panthema.net/2007/stx-\nbtree/ .\n[5]Brian F. Cooper, Adam Silberstein, Erwin Tam, Raghu Ramakrishnan,\nand Russell Sears. 2010. Benchmarking Cloud Serving Systems with\nYCSB. In ACM SoCC . 143‚Äì154.\n[6]Andrew Crotty. 2021. Hist-Tree: Those Who Ignore It Are Doomed to\nLearn. In CIDR . 1‚Äì6.\n[7]Armon Dadgar. Forked on March 9, 2021. libart. (Forked on March 9,\n2021). https://github.com/armon/libart .\n[8]Intel¬ÆDevCloud. [n. d.]. Intel ¬ÆDevCloud: Remote Development\nEnvironments for Any Use Case. ([n. d.]). https://software.intel.com/\ncontent/www/us/en/develop/tools/devcloud.html .\n[9]Jialin Ding. Forked on February 16, 2021. ALEX. (Forked on February\n16, 2021). https://github.com/microsoft/ALEX .\n[10] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do,\nYinan Li, Hantian Zhang, Badrish Chandramouli, Johannes Gehrke,\nDonald Kossmann, David Lomet, and Tim Kraska. 2020. ALEX: An\nUpdatable Adaptive Learned Index. In ACM SIGMOD . 969‚Äì984.\n[11] Michael Ferdman, Almutaz Adileh, Onur Kocberber, Stavros Volos,\nMohammad Alisafaee, Djordje Jevdjic, Cansu Kaynak, Adrian Daniel\nPopescu, Anastasia Ailamaki, and Babak Falsafi. 2012. Clearing the\nClouds: A Study of Emerging Scale-out Workloads on Modern Hard-\nware. In ASPLOS . 37‚Äì48.\n[12] Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-Index: A\nFully-Dynamic Compressed Learned Index with Provable Worst-Case\nBounds. PVLDB 13, 8 (2020), 1162‚Äì1175.\n[13] Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca,\nand Tim Kraska. 2019. FITing-Tree: A Data-Aware Index Structure. In\nACM SIGMOD . 1189‚Äì1206.\n[14] Goetz Graefe. 2011. Modern B-Tree Techniques. Found. Trends\nDatabases 3, 4 (2011), 203‚Äì402.\n[15] Ali Hadian and Thomas Heinis. 2021. Shift-Table: A Low-latency\nLearned Index for Range Queries using Model Correction. In EDBT .\n253‚Äì264.\n[16] Nikos Hardavellas, Ippokratis Pandis, Ryan Johnson, Naju G.\nMancheril, Anastassia Ailamaki, and Babak Falsafi. 2007. Database\nServers on Chip Multiprocessors:Limitations and Opportunities. In\nCIDR . 79‚Äì87.\n[17] Intel¬Æ. [n. d.]. Clockticks Vs. Pipeline Slots Based Metrics.\n([n. d.]). https://software.intel.com/content/www/us/en/\ndevelop/documentation/vtune-help/top/reference/cpu-metrics-\nreference/clockticks-vs-pipeline-slots-based-metrics.html .\n[18] Intel¬Æ. [n. d.]. Instrumentation and Tracing Technology APIs.\n([n. d.]). https://software.intel.com/content/www/us/en/develop/\ndocumentation/vtune-help/top/api-support/instrumentation-and-\ntracing-technology-apis.html .\n[19] Intel¬Æ. [n. d.]. Intel ¬ÆVTune ‚Ñ¢Profiler. ([n. d.]). https://software.intel.\ncom/content/www/us/en/develop/tools/oneapi/components/vtune-\nprofiler.html .\n[20] Intel¬Æ. [n. d.]. Top-down Microarchitecture Analysis Method.\n([n. d.]). https://software.intel.com/content/www/us/en/develop/\ndocumentation/vtune-cookbook/top/methodologies/top-down-\nmicroarchitecture-analysis-method.html .\n[21] Svilen Kanev, Juan Pablo Darago, Kim Hazelwood, Parthasarathy Ran-\nganathan, Tipp Moseley, Gu-Yeon Wei, and David Brooks. 2015. Pro-\nfiling a Warehouse-Scale Computer. In ISCA . 158‚Äì169.\n[22] K. Keeton, D.A. Patterson, Yong Qian He, R.C. Raphael, and W.E. Baker.\n1998. Performance characterization of a quad Pentium Pro SMP using\nOLTP workloads. In ISCA . 15‚Äì26.\n[23] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian,\nAlfons Kemper, Tim Kraska, and Thomas Neumann. 2019. SOSD: A\nBenchmark for Learned Indexes. MLForSystems@NeurIPS (2019).[24] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian,\nAlfons Kemper, Tim Kraska, and Thomas Neumann. 2020. RadixSpline:\nA Single-Pass Learned Index. In aiDM@SIGMOD . 5:1‚Äì5:5.\n[25] Thomas Kowalski, Fotios Kounelis, and Holger Pirk. 2020. High-\nPerformance Tree Indices: Locality matters more than one would\nthink. In ADMS . 1‚Äì6.\n[26] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Poly-\nzotis. 2018. The Case for Learned Index Structures. In ACM SIGMOD .\n489‚Äì504.\n[27] Viktor Leis, Alfons Kemper, and Thomas Neumann. 2013. The Adaptive\nRadix Tree: ARTful Indexing for Main-Memory Databases. ICDE , 38‚Äì\n49.\n[28] Justin Levandoski, David Lomet, and Sudipta Sengupta. 2013. The\nBw-Tree: A B-tree for New Hardware Platforms. In ICDE .\n[29] Linux manual page. [n. d.]. top. ([n. d.]). https://man7.org/linux/man-\npages/man1/top.1.html .\n[30] Ryan Marcus, Andreas Kipf, Alexander van Renen, Mihail Stoian,\nSanchit Misra, Alfons Kemper, Thomas Neumann, and Tim Kraska.\n2020. Benchmarking Learned Indexes. PVLDB 14, 1 (2020), 1‚Äì13.\n[31] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska.\n2020. Learning Multi-Dimensional Indexes. In ACM SIGMOD .\n985‚Äì1000.\n[32] Parthasarathy Ranganathan, Kourosh Gharachorloo, Sarita V. Adve,\nand Luiz Andr√© Barroso. 1998. Performance of Database Workloads\non Shared-Memory Systems with out-of-Order Processors. In ASPLOS .\n307‚Äì318.\n[33] Utku Sirin and Anastasia Ailamaki. 2020. Micro-architectural Analysis\nof OLAP: Limitations and Opportunities. PVLDB 13, 6 (2020), 840‚Äì853.\n[34] Utku Sirin, Pinar T√∂z√ºn, Danica Porobic, and Anastasia Ailamaki. 2016.\nMicro-architectural Analysis of In-memory OLTP. In ACM SIGMOD .\n387‚Äì402.\n[35] Utku Sirin, Pinar T√∂z√ºn, Danica Porobic, Ahmad Yasin, and Anastasia\nAilamaki. 2021. Micro-architectural Analysis of In-memory OLTP:\nRevisited. (2021), 1‚Äì25.\n[36] Utku Sirin, Ahmad Yasin, and Anastasia Ailamaki. 2017. A methodol-\nogy for OLTP micro-architectural analysis. In DaMoN @ ACM SIGMOD .\n1:1‚Äì1:10.\n[37] Stets, Gharachorloo, and Barroso. 2002. A Detailed Comparison of\nTwo Transaction Processing Workloads. In IEEE WWC . 37‚Äì48.\n[38] Pƒ±nar T√∂z√ºn, Ippokratis Pandis, Cansu Kaynak, Djordje Jevdjic, and\nAnastasia Ailamaki. 2013. From A to E: Analyzing TPC‚Äôs OLTP Bench-\nmarks: The Obsolete, the Ubiquitous, the Unexplored. In EDBT . 17‚Äì28.\n[39] Ahmad Yasin. 2014. A Top-Down method for performance analysis\nand counters architecture. In ISPASS . 35‚Äì44.\n[40] Ahmad Yasin, Yosi Ben-Asher, and Avi Mendelson. 2014. Deep-dive\nanalysis of the data analytics workload in CloudSuite. In IISWC . 202‚Äì\n211.\n13",
  "textLength": 63444
}