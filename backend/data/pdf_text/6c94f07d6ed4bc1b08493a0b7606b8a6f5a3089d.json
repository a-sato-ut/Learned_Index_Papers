{
  "paperId": "6c94f07d6ed4bc1b08493a0b7606b8a6f5a3089d",
  "title": "Aleph Filter: To Infinity in Constant Time",
  "pdfPath": "6c94f07d6ed4bc1b08493a0b7606b8a6f5a3089d.pdf",
  "text": "Aleph Filter: To Infinity in Constant Time\nNiv Dayan\nUniversity of Toronto\nCanada\nnivdayan@cs.toronto.eduIoana-Oriana Bercea\nKTH Royal Institute of Technology\nSweden\nbercea@kth.seRasmus Pagh\nBARC, University of Copenhagen\nDenmark\npagh@di.ku.dk\nABSTRACT\nFilter data structures are widely used in various areas of computer\nscience to answer approximate set-membership queries. In many\napplications, the data grows dynamically, requiring their filters to\nexpand along with the data. However, existing methods for expand-\ning filters cannot maintain stable performance, memory footprint,\nand false positive rate (FPR) simultaneously. We address this prob-\nlem with Aleph Filter, which makes the following contributions.\n(1) It supports all operations (insertions, queries, deletes, etc.) in\nconstant time, no matter how much the data grows. (2) Given an\nestimate of how much the data will ultimately grow, Aleph Filter\nprovides a memory vs. FPR trade-off on par with static filters.\nPVLDB Reference Format:\nNiv Dayan, Ioana-Oriana Bercea, and Rasmus Pagh. Aleph Filter: To\nInfinity in Constant Time. PVLDB, 17(11): 3644 - 3656, 2024.\ndoi:10.14778/3681954.3682027\nPVLDB Artifact Availability:\nThe source code, data, and/or other artifacts have been made available at\nhttps://github.com/nivdayan/AlephFilter.\n1 INTRODUCTION\nFilters. A filter is a compact data structure that represents keys in\na set and answers set-membership queries [ 46]. It cannot return a\nfalse negative, but it returns a false positive with a probability that\ndepends on the amount of memory used on average to represent\neach key. A filter is typically stored at a higher layer of the memory\nhierarchy (e.g., DRAM) than the data keys that it represents, which\ntypically reside in storage (i.e., disk or SSD) or over a network. If a\nfilter returns a negative, the sought-after key is guaranteed not to\nexist, meaning the full data set does not need to be searched. Thus,\nfilters eliminate storage accesses and/or network hops to improve\na system‚Äôs performance [18, 20, 21, 35, 51].\nThe Need for Dynamic Filters. Applications with dynamic data\nrequire filters that support deletes and can expand. Various modern\nkey-value stores employ dynamic filters to map data entries in\nstorage [ 2,11,16,22‚Äì25,48,56], while network applications use\nthem to support black lists and multicast routing [59].\nBloom filter [ 6,8,52], the oldest and best-known filter, does not\nsupport deletes or efficient expansion. If the data grows or changes,\nthe only recourse is to recreate its Bloom filter from scratch by\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license. For any use beyond those covered by this license, obtain permission by\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 17, No. 11 ISSN 2150-8097.\ndoi:10.14778/3681954.3682027rereading the original keys. This, however, can be performance-\nprohibitive if the keys reside in storage. In contrast, dynamic filters\nsupport deletes and expansion without rereading the original keys.\nMotivation: Scaling Dynamic Filters. As existing dynamic filters\nexpand, their performance, memory footprint, and/or false positive\nrate (FPR) deteriorate. Our research goal is to better scale these\ncost metrics so that dynamic filters remain effective no matter how\nmuch the data grows or changes.\nQuotient Filters. Quotient filters are a family of dynamic filters\nthat store a fingerprint for each key in a compact hash table [ 9] and\nhandle collisions using linear probing [ 4,14,28,44,45,50]. While\nthey seamlessly support deletions, expanding them efficiently is\nmore challenging. The reason is that we do not have access to the\noriginal keys and can therefore not rehash each key to a unique slot\nwithin a larger hash table. Existing quotient filters address this issue\nby transferring one bit from each entry‚Äôs fingerprint to become a\npart of its slot address to evenly distribute the fingerprints across a\n2x larger hash table [ 4,62]. As a result, the fingerprints shrink as\nthe data grows, causing the FPR to increase rapidly. Eventually, the\nfingerprints run out of bits, making the filter useless by returning a\npositive for any query.\nThe State of the Art: InfiniFilter. The recent InfiniFilter [ 19] is a\nquotient filter that can expand indefinitely while better scaling the\nFPR. It does so using a hash slot format that supports variable-length\nfingerprints. Thus, while fingerprints of older entries shrink across\nexpansions, the fingerprints of new entries can still be initialized to\noccupy the original slot length. This design keeps fingerprints long\non average. As a result, the FPR increases more slowly. Nevertheless,\nInfiniFilter exhibits two remaining scalability challenges.\nProblem 1: Performance Scalability. After the first few expan-\nsions, the fingerprints of older entries within InfiniFilter run out of\nbits. Such entries are referred to as void entries [ 19]. A void entry\ncannot be mapped to a unique slot in a 2x larger hash table since\nthere is no remaining fingerprint bit to transfer to its slot address.\nInfiniFilter tackles this problem by transferring and storing void\nentries along a series of smaller hash tables. However, this causes\nqueries and deletions to potentially search multiple hash tables,\nincreasing their CPU overheads. Can we better scale the costs of\nqueries and deletions?\nProblem 2: FPR vs. Memory Scalability. As InfiniFilter expands,\nthe shorter fingerprints of older entries cause the FPR to increase.\nTo counteract this, InfiniFilter can assign even longer fingerprints\nto newer entries as we expand to cause the FPR to converge [ 19].\nNevertheless, this entails widening the slot width and thus inflat-\ning the memory footprint. Thus, there is an intrinsic scalability\ncontention between the FPR and memory for InfiniFilter and for\nexpandable filters in general [ 43]. Is it possible to alleviate this\ncontention so that the FPR vs. memory trade-off resembles that of\nstatic filters?arXiv:2404.04703v5  [cs.DB]  31 Oct 2024\n\nTable 1: Terms used throughout the paper to describe Aleph\nFilter and other baselines.\nterm definition\nùëÅ current filter capacity divided by initial capacity\nùëã number of expansions so far (i.e., ùëã=‚åàlog2(ùëÅ)‚åâ)\nùëÄ total memory used for the filter (bits / entry)\nùêπ initial fingerprint length when the filter is first allocated\n‚Ñé(...)mother hash generating function\nùõº fraction of occupied slots ( 0‚â§ùõº<1)\nAleph Filter. We propose Aleph Filter1, an infinitely expandable\nfilter with constant time performance for all operations and superior\nmemory vs. FPR trade-offs. It builds on InfiniFilter in three ways.\nContribution 1: Faster Queries by Duplicating Void Entries.\nDuring expansion, Aleph Filter duplicates every void entry in the ex-\npanded hash table across both slots that it could have been mapped\nto if there had been an additional fingerprint bit to sacrifice. This\nkeeps the information about all entries, including void entries,\nwithin one main hash table. As a result, Aleph filter accesses only\none hash table for any query in ùëÇ(1)time. We show how to tune\nthe filter so that duplicated void entries occupy negligible space.\nContribution 2: Faster Deletes using Tombstones. Duplicating\nvoid entries within the main hash table complicates deletions as\npotentially multiple duplicates must be identified and removed\nwhen an older entry is deleted. Aleph filter addresses this challenge\nby first transforming the target void entry into a tombstone. Before\nthe next expansion, it retrieves the original hash of the deleted\nvoid entry and uses it to identify and remove all of its duplicates.\nThe overhead of removing these duplicates is negligible and gets\namortized as a part of the next expansion.\nContribution 3: Inverting the FPR vs Memory Trade-Off. In\nmany applications, the maximum data size can be predicted in\nadvance. We show that given a reasonable estimate of how much\nthe data will grow, we can pre-allocate slightly longer fingerprints\nfrom the onset and assign shorter fingerprints as we expand. When\nwe reach the estimated data size, the filter guarantees an FPR vs.\nmemory trade-off that is on par with that of static filters.\n2 BACKGROUND\nThis section describes Quotient Filter and InfiniFilter, on top of\nwhich we build Aleph Filter.\n2.1 Quotient Filter\nA quotient filter [ 4,14,28] is a hash table that stores a fingerprint for\neach inserted key. It does this by first generating a mother hash for a\nkey consisting of ùêπ+ùê¥bits using some hash function ‚Ñé(...). The least\nsignificantùê¥bits of this mother hash represent the key‚Äôs canonical\nslot. The subsequent ùêπbits are the key‚Äôs fingerprint. Table 1 lists\nterms used throughout the paper.\nA quotient filter resolves hash collisions using Robin Hood hash-\ning [ 10], which is a variant of linear probing [ 47]. This means that\n1The name Aleph is borrowed from set theory, where the Aleph numbers denote\ndifferent orders of infinity.\nFigure 1: A Quotient filter stores a fingerprint for each key in a\nhash table, and it resolves has collisions by organizing fingerprints\ninto runs and clusters. Fingerprints are illustrated in italicized red.\nall fingerprints mapped to a given canonical slot are stored contigu-\nously, and they push to the right any other existing fingerprints\nthat they collide with. A runis defined as a group of contiguous\nfingerprints belonging to the same canonical slot. A cluster is de-\nfined as several adjacent runs where all but the first have all been\npushed to the right due to collisions.\nFigure 1 Part A illustrates a quotient filter with eight slots after\nfour insertions in any order of Keys ùëâ,ùëå,ùëçandùëä. The mother hash\nfor each key is shown at the top. The rightmost (least significant)\nbits for each mother hash represent the key‚Äôs canonical address.\nThe subsequent bits, shown in italicized red, are the fingerprint.\nKeysùëâandùëåshare Canonical Slot 100 while Keys ùëçandùëäshare\nthe adjacent Canonical Slot 101. The result is a cluster consisting\nof two runs, each with two slots.\nMetadata Bits. To keep track of the start and end of runs and\nclusters, a quotient filter employs three metadata bits per slot. The\nis_occupied bit indicates whether the slot is a canonical slot for at\nleast one existing key. The is _shifted bit indicates whether the slot\ncontains a fingerprint that has been shifted to the right relative to\nits canonical slot. The is _continuation bit indicates whether the\nslot contains the start of a new run within a cluster.\nIn Figure 1 Part A, the is _occupied flag is set for Slots 100 and\n101 as each of them is a canonical slot for at least one key. The\nis_shifted flag is set for Slots 101, 110, and 111 as they each contain\na fingerprint that has been shifted to the right from its canonical\nslot. The is _continuation flag is set for Slots 101 and 111 as they\neach contain a fingerprint belonging to a run that starts to its left.\nQuery. To illustrate the query process, consider a query to entry\nùëäin figure 1 Part (A). The query begins at the canonical slot for\nthe target key (Slot 101). If the is _occupied flag for this slot is set\nto 0, the key could not have been inserted so the query returns a\nnegative. In this case, however, the is _occupied flag is set to 1 and\nso the search continues. The query must now find the start of the\ntarget run that might contains the sought-after key, yet this run\ncould have been pushed to the right from its canonical slot due to\ncollisions. To find the target run, the query first moves leftwards\nuntil reaching the start of the cluster. Along the way, it counts the\nnumber of is _occupied flags ùëêset to ones (in our example ùëê=2),\neach of which indicates the existence of a run in the cluster that\nprecedes Run ùëü. After reaching the start of the cluster (at Slot 100),\nthe query moves rightwards while skipping ùëêruns, at which point it\n\n00100011100001000001000001001000001100111000000010100111001011101110001110010000000100Insert Y after expansion h(Y)=0111000011011Insert X before expansion h(X)=1100011expansionFigure 2: The Fingerprint Sacrifice method transfers one bit\nfrom the fingerprint to the slot address while expanding to\nevenly distribute the entries across the larger hash table.\nreaches the target run (at Slot 110). It then scans this run, returning\na positive if it finds a matching fingerprint (at Slot 111).\nInsertion. An insertion commences as a query by first finding the\nstart of the target run for the key we wish to insert. It then adds\na fingerprint to this run, pushing subsequent fingerprints in the\ncluster one slot to the right to clear space. This may cause more\nruns to join the cluster by pushing them from their canonical slots.\nDelete. A delete operation also commences like a query by first\nfinding the target run of the key we wish to remove. It then scans\nthe run and removes the first matching fingerprint. Then, it pulls\nall remaining fingerprints in the cluster one slot to the left to keep\nthe cluster contiguous. For example, Figure 1 Part B illustrates\nhow deleting Key ùëåpulls the second run in the cluster back to its\ncanonical slot, causing the cluster to fragment. As with any tabular\nfilter, it is only permissible to delete keys that we know for sure\nhad been inserted to prevent false negatives.\nFalse Positive Rate. A quotient filter‚Äôs FPR is ‚âàùõº¬∑2‚àíùêπ, where\nùêπis the fingerprint size and ùõºis the fraction of the slots that are\noccupied. The intuition is that the target run contains on average ùõº\nfingerprints, while the probability that each of them matches that\nof the sought-after key is 2‚àíùêπ.\nReaching Capacity. As long as the quotient filter is less than 90%\nfull (i.e.,ùõº=0.9), the clusters stay small on average leading to\nconstant time operations. As it exceeds 90%utilization, however,\nthe clusters‚Äô lengths begin to grow rapidly. This causes performance\nto plummet as all operations on the filter must traverse a greater\nnumber of slots. To keep performance stable and to accommodate\nmore insertions, it is desirable to expand the filter.\nFingerprint Sacrifice. The standard approach for expanding a\nquotient filter is to derive the mother for every key by concatenating\nits canonical slot address to its fingerprint. We then reinsert the\nmother hash into a hash table with twice the capacity of the original\none [ 4]. Figure 2 illustrates an example. As shown, this approach\ntransforms the least significant bit of each fingerprint to the most\nsignificant bit of its canonical slot address in order to to evenly\ndistribute the fingerprints across the larger hash table. Hence, all\nfingerprints shrink by one bit in each expansion, causing the FPR to\ndouble, i.e., the FPR becomes ùëÇ(ùëÅ¬∑2‚àíùêπ)[19]. This method supports\nat mostùêπexpansions, at which point all fingerprints run out of bits.\nWe summarize the properties of this method in Row 1 of Table 2.\n0010001110000100000100000100100000000001010011100101110111Insert Y after expansion h(Y) = 10111000011011Insert X before expansion h(X)=1100011011011011000011110111100111000000000000001011011expansionFigure 3: By supporting variable-length fingerprints, In-\nfiniFilter can set long fingerprints to newer entries inserted\nafter expanding.\nFigure 4: InfiniFilter in the Widening Regime gradually in-\ncreases the slot width across expansions to keep the false\npositive rate constant.\n2.2 InfiniFilter\nInfiniFilter is a quotient filter that sets longer fingerprints to newer\nentries to allow expanding indefinitely while better scaling the FPR.\nUnary Padding. The central innovation of InfiniFilter is the ability\nto store variable-length fingerprints. This is achieved by padding\neach fingerprint with a self-delimiting unary code to occupy the\nrest of the space in the slot. The top part of Figure 3 illustrates an\ninstance of InfiniFilter with four occupied slots and fingerprints\nof lengths 3, 1, 2 and 3 bits from left to right. Unary codes and\nfingerprints and are shown in blue and italicized red, respectively.\nExpansion Algorithm. As with the Fingerprint Sacrifice method,\nInfiniFilter transfers the least significant bit from each fingerprint to\nbecome the most significant bit of the slot address during expansion\nto evenly distribute existing entries across the expanded hash table.\nAfter the expansion, InfiniFilter‚Äôs slot format allows inserting longer\nfingerprints than the older ones that had shrunk. This is shown by\nthe insertion of entry ùëåin Figure 3. This entry is assigned a three\nbit fingerprint despite the fact that fingerprints that existed before\nthe expansion have now all shrunk to two or fewer bits. This keeps\nthe average fingerprint length longer.\nFalse Positive Rate. Since InfiniFilter doubles in capacity every\ntime it expands, the fingerprints within the filter follow a geometric\ndistribution with respect to their lengths: half of them are as long\nas possible, a quarter are shorter by one bit, an eighth are shorter\nby two bits, etc. Generally, the fingerprints created ùëñgenerations\nago comprise a fraction of ‚âàùõº¬∑2‚àíùëñ‚àí1of the occupied slots, while\ntheir fingerprints have a length of ùêπ‚àíùëñbits and hence a collision\nprobability of 2‚àíùêπ+ùëñ. The FPR after ùëãexpansions can be derived\nas a weighted average of these terms:‚àëÔ∏Åùëã\nùëñ=02‚àíùêπ+ùëñ¬∑ùõº¬∑2‚àíùëñ‚àí1‚â≤\n\n110110001110111010011shortest  matching longest  matching non- matching  Delete key Z, where h(Z) = 1110101010‚Ä¶‚Ä¶10111100Figure 5: InfiniFilter deletes the longest matching finger-\nprints in the target run to prevent future false negatives.\n(log2(ùëÅ)+2)¬∑2‚àíùêπ‚àí1¬∑ùõº. After‚âà2ùêπexpansions, the FPR reaches\none, at which point the filter returns a positive for any query.\nFixed-Length vs. Widening Regimes. To further scale the FPR,\nit is possible to widen slots across expansions. For example, Figure\n4 illustrates increasing the slot width by one bit. Entries inserted\nafter the expansion are set fingerprints of length four rather than\nthree bits. By assigning fingerprints of length ùêπ+‚åà2¬∑log2(ùëã+1)‚åâ\nbits to new entries after the ùëãthexpansion, the FPR converges to\na constant smaller than 2‚àíùêπ[19]. The intuition is that the longer\nfingerprints of newer entries make their weighted contribution to\nthe FPR vanishingly small. We refer to this as the Widening Regime,\nin contrast to the Fixed-Width Regime described just before.\nDeletes. InfiniFilter deletes a key by removing the longest matching\nfingerprint in the target run to prevent future false negatives. For\nexample, Figure 5 illustrates a delete operation to Key ùëç. The target\nrun for this key begins at Slot 1010 and consists of three slots.\nWhile the fingerprint at Slot 1010 does not match, both subsequent\nfingerprints, which have different lengths, do match. If we remove\nthe shorter matching fingerprint (at Slot 1011) and it happens to\nbelong to a different key ùëåwith a different mother hash from\nthat of Key ùëç(e.g.,‚Ñé(ùëå)=011010 ), future false negatives would\noccur when querying for Key ùëå. In contrast, removing the longest\nmatching fingerprint (at Slot 1100) guarantees that the remaining\nshorter fingerprint in the run will still match the non-deleted key\nso that future queries to it do not result in false negatives.\nRejuvenation. In many applications (e.g., key-value stores), a posi-\ntive query to a filter is followed by fetching the corresponding data\nentry from storage to return it to the user. In case that the key exists\n(i.e., a true positive), we can rehash it to derive a longer mother\nhash and thus rejuvenate (i.e., lengthen) the key‚Äôs fingerprint, in\ncase it was created before the last expansion. Such rejuvenation\noperations help keep the FPR low. However, they are only effective\nwhen queries target older entries. Similarly to deletes, a rejuvena-\ntion operation must lengthen the longest matching fingerprint in a\nrun to prevent false negatives.\nVoid Entries. After the first ùêπexpansions, the oldest fingerprints\nin InfiniFilter run out of bits. Such entries are referred to as void\nentries. Both Figures 3 and 4 show the creation of a void entry at Slot\n101 after the expansion. As shown, the unary code fully occupies a\nslot that contains a void entry. Any query that encounters a void\nentry in its target run returns a positive. Void entries cannot be\nuniquely remapped to a slot in a larger filter as there is no extra\nfingerprint bit to transfer to the entry‚Äôs canonical slot address.\nSupporting Infinite Expansions. To support more than ùêπexpan-\nsions, InfiniFilter transfers each void entry from the main hash table\ninto a smaller secondary hash table, which has an identical structure\nto that of the main hash table but fewer slots. Figure 6 illustrates\nthis process across four expansions. During the second expansion,\nFigure 6: InfiniFilter transfers void entries into a secondary\nhash table, which also expands when it reaches capacity.\nWhen void entries appear in the secondary hash table, it\nis appended to a chain of hash tables and a new secondary\nhash table is allocated.\nfor instance, the void entry from canonical Slot 110 of the main\nhash table is transferred to the secondary hash table. As 110 is also\nthe mother hash of this entry, its least significant bit (i.e., 0) is used\nas a canonical slot address in the secondary hash table while its\nmore significant two bits (i.e., 11) are used as a fingerprint.\nThe secondary hash table also expands when it reaches capacity.\nEventually, the oldest entries in the Secondary hash table become\nvoid entries, as shown in Figure 6 by the entry at Slot 11 after the\nthird expansion. As this point, the Secondary InfiniFilter is sealed\nand appended to a so-called chain of auxiliary hash tables, and a new\nempty secondary hash table is allocated. While this design supports\nan unlimited number of expansions, it slows down queries, deletes\nand rejuvenation operations, as they must now traverse potentially\nall hash tables along the chain.\n3 PROBLEM ANALYSIS\nChallenge 1: Scaling CPU Costs. The number of hash tables\nacross which InfiniFilter stores its entries determines the CPU costs\nof queries, deletes and rejuvenation operations. In the Fixed-Width\nRegime, each hash table stores entries from across ùêπsubsequent ex-\npansions, and the overall number of expansions is log2(ùëÅ). Hence,\nthere are at most ùëÇ(lg(ùëÅ)/ùêπ)hash tables. In the Widening Regime,\neach hash table stores entries from across ùêπ+ùëÇ(lg lgùëÅ)subse-\nquent expansions on average, and so the number of hash tables is\nùëÇ(lg(ùëÅ)/(ùêπ+lg lgùëÅ))[19]. We summarize these properties in Rows 2\nand 3 of Table 2. Under both regimes, performance deteriorates as\nthe data grows. Can we reduce the worst-case number of hash tables\nthat queries, deletes, and rejuvenation operations must access?\nChallenge 2: Alleviating the Memory vs. FPR Contention. As\nshown in Row 2 of Table 2, InfiniFilter exhibits a logarithmic FPR\nif we fix the number of bits per entry. Alternatively, as shown in\nRow 3, it exhibits a stable FPR if we increase the number of bits\nper entry at a doubly logarithmic rate. Is it possible to alleviate this\nscalability contention to achieve an FPR and memory footprint that\nare both on par with a static filter?\n\nTable 2: A comparison of existing filter expansion techniques against Aleph Filter with respect to the data size ùëÅand the initial\nfingerprint length ùêπ. Aleph Filter provides faster query/delete/rejuvenation operations, and its Predictive Regime requires less\nmemory given an approximation ùëÅùëíùë†ùë°of the ultimate data size.\nquery/\ndelete/rejuvinsert false positive\nrate (FPR)fingerprint\nbits / keymax.\nexpansions\nFingerprint Sacrifice [4, 62] ùëÇ(1)ùëÇ(1)ùëÇ(2‚àíùêπ¬∑ùëÅ)ùêπ‚àíùëÇ(lgùëÅ) ùêπ\nInfiniFilter (Fixed-Width Regime) ùëÇ(lgùëÅ\nùêπ)ùëÇ(1)ùëÇ(2‚àíùêπ¬∑lgùëÅ)ùêπ 2ùêπ\nInfiniFilter (Widening Regime) ùëÇ(lgùëÅ\nùêπ+lg lgùëÅ)ùëÇ(1)ùëÇ(2‚àíùêπ)ùêπ+ùëÇ(lg lgùëÅ)‚àû\nAleph Filter (Fixed-Width Regime) ùëÇ(1)ùëÇ(1)ùëÇ(2‚àíùêπ¬∑lgùëÅ)ùêπ 2ùêπ\nAleph Filter (Widening Regime) ùëÇ(1)ùëÇ(1)ùëÇ(2‚àíùêπ)ùêπ+ùëÇ(lg lgùëÅ)‚àû\nAleph Filter (Predictive Regime) ùëÇ(1)ùëÇ(1)ùëÇ(2‚àíùêπ)ùêπ+ùëÇ(lg|lg(ùëÅ/ùëÅùëíùë†ùë°)|)‚àû\n4 ALEPH FILTER\nWe introduce Aleph Filter, an expandable filter that builds on In-\nfiniFilter to improve its scalability properties. Section 4.1 shows\nhow Aleph Filter supports queries in ùëÇ(1)time by keeping and du-\nplicating void entries within the main hash table. Section 4.2 shows\nanalytically that the fraction of duplicated void entries stays small\nand therefore does not significantly impact the FPR or the maxi-\nmum number of expansions that the filter supports. Sections 4.3\nand 4.4 show how to support deletes and rejuvenation operations in\nùëÇ(1)time without introducing false negatives by lazily identifying\nand removing void duplicates corresponding to the entry with the\nlongest matching mother hash.\nThroughout Sections 4.1 to 4.4, we analyze Aleph Filter in both\nthe the Fixed-Width and Widening Regimes and summarize their\nproperties in Rows 4 and 5 of Table 2. In Section 5, we introduce\nthe Predictive Regime. Given a rough, conservative estimate of\nhow much the data will grow, Aleph filter in the Predictive Regime\nachieves FPR vs. memory trade-offs that are on par with static\nfilters. We summarize its properties in Row 6 of Table 2.\n4.1 Fast Queries by Duplicating Void Entries\nWhile expanding, Aleph Filter duplicates each void entry in the\nmain hash table across both canonical slots that it could have\nmapped to if there were an additional fingerprint bit to sacrifice. In\nFigure 7, for example, there is a void entry at Slot 11 of the main\nhash table before the expansion. During the expansion, Aleph Filter\nduplicates it across Canonical Slots 011 and 111. In the next expan-\nsion, each of these duplicates will be duplicated again, resulting in\nfour duplicates at Canonical Slots 0011, 0111, 1011, and 1111.\nAs a result of duplicating void entries, a query targeting any old\nentry returns a positive after one access to the main hash table.\nFigure 7 illustrates a query to Key ùëå, which corresponds to the\nvoid entry that was duplicated. The query visits canonical slot 111\n(based on the first three bits of the key‚Äôs mother hash), finds a void\nentry and returns a positive. Had Key Y‚Äôs mother hash started with\n011, the query would have found the other void duplicate at Slot 011\nand also terminated after one hash table access. As there is a void\nentry in every possible run that would have contained the entry if\nwe had all bits of its mother hash, a query to the the original key\nalways returns a positive. Hence, no false negatives can occur.\nFigure 7: While expanding, Aleph Filter duplicates each void\nentry so that one duplicate would still be found in constant\ntime when querying for the original key.\nAnother result of duplicating void entries is that any query to a\nnon-existing key terminates after one access to the main hash table.\nThe reason is that the main hash table contains either a fingerprint\nor a void entry in any existing key‚Äôs canonical slot. This means that\nif we do not find a matching entry in ùëÇ(1)time, the target entry is\nguaranteed not to exist.\nHence, all queries to Aleph Filter are processed in worst-case\nùëÇ(1)time. This is an improvement over InfiniFilter, where queries\ntargeting non-existing or older keys must search an increasing\nnumber of hash tables as the data grows.\n4.2 Analysis\nDuplicating void entries in the main hash brings up two plausible\nconcerns. The first is whether duplicated void entries significantly\nincrease the FPR, seeing as a query that encounters a void entry\nimmediately returns a positive. The second is whether duplicated\nvoid entries take up significant space within the main hash table.\nThis section shows that the proportion of void entries stays small\nand therefore does not significantly increase the FPR or cause the\nhash table to fill up prematurely.\nGenerational Distribution. Aleph Filter doubles in capacity dur-\ning each expansion. This means that keys inserted zero, one or\ntwo expansions ago comprise approximately a half, a quarter, or\nan eighth of the data set, respectively, and so on. More generally,\nconsider the Set ùë†ùëóof keys inserted in-between the ùëóthand(ùëó+1)th\nexpansions, i.e., in Generation ùëó. Suppose we are now in Generation\nùëã(ùëã‚â•ùëó), i.e., before Expansion ùëã+1. Equation 1 approximates\n\nthe size of Set ùë†ùëóas a fraction of the whole key set.\nùëì(ùëó)‚âà2‚àíùëã+ùëó‚àí1(1)\nFPR in the Fixed-Width Regime. Consider some Generation ùëó\nof non-void entries (i.e., ùëã‚àíùëó<ùêπ). These entries‚Äô fingerprints\neach consist of ùêπ‚àí(ùëã‚àíùëó)bits, since they are initialized with ùêπ\nbits and lose one bit in each expansion. The probability for a query\nto encounter such an entry in a given slot and falsely match its\nfingerprint is ùõº¬∑ùëì(ùëó)¬∑2‚àíùêπ+ùëã‚àíùëó=ùõº¬∑2‚àíùêπ‚àí1.\nNow consider a Generation ùëóof void entries (i.e., ùëã‚àíùëó‚â•ùêπ).\nThere are 2‚àíùêπ+ùëã‚àíùëóduplicates per entry since the number of dupli-\ncates increases by a factor of two in each expansion. The fraction\nof slots occupied by void duplicates originating from Generation ùëó\nisùõº¬∑ùëì(ùëó)¬∑2‚àíùêπ+ùëã‚àíùëó=ùõº¬∑2‚àíùêπ‚àí1. Hence, the probability of encoun-\ntering a void duplicate from Generation ùëóduring a query and thus\nreturning a false positive is ùõº¬∑2‚àíùêπ‚àí1.\nAs shown, the contribution of each generation of entries to the\nFPR is equal (i.e., ùõº¬∑2‚àíùêπ‚àí1). For non-void entries, this is because each\ntime their fingerprints lose one bit, their fraction in the overall filter\nhalves. For void entries, this is because they always return a positive,\nand their fraction of void entries emanating from a given generation\nstays fixed since they duplicate. Equation 2 expresses the overall\nFPR afterùëãexpansions. We see that Aleph Filter has approximately\nthe same FPR as InfiniFilter in the Fixed-Width Regime.\nùêπùëÉùëÖ‚âàùëã+1‚àëÔ∏Ç\nùëó=0ùõº¬∑2‚àíùêπ‚àí1‚â≤ùõº¬∑(ùëôùëúùëî2(ùëÅ)+2)¬∑2‚àíùêπ‚àí1(2)\nFPR in the Widening Regime. In the Widening Regime, entries\nin Generation ùëóare assigned fingerprints of ‚Ñì(ùëó)=ùêπ+2¬∑log2(ùëó+1)\nbits, and they lose one bit in each expansion.\nLet us consider a generation Generation ùëóof non-void entries\n(i.e.,ùëã‚àíùëó<‚Ñì(ùëó)). By Generation ùëã, the fingerprints of Generation ùëó\nwill have shrunk by ùëã‚àíùëóbits to‚Ñì(ùëó)‚àí(ùëã‚àíùëó)bits. The probability\nfor a query to encounter an entry from Generation ùëóin a given slot\nand falsely match its fingerprint is therefore ùõº¬∑ùëì(ùëó)¬∑2‚àí‚Ñì(ùëó)+ùëã‚àíùëó.\nLet us now suppose Generation ùëóconsists of void entries (i.e.,\nùëã‚àíùëó‚â•‚Ñì(ùëó)). The number of void duplicates for each entry is\n2‚àí‚Ñì(ùëó)+ùëã‚àíùëó. The probability of encountering a void duplicate origi-\nnating from this generation is therefore the same expression: ùõº¬∑\nùëì(ùëó)¬∑2‚àí‚Ñì(ùëó)+ùëã‚àíùëó.\nEquation 3 sums up this expression across all generations to\nderive the FPR. The derivation uses a well-known identity that\nthe sum of the reciprocals of the square numbers (i.e., Œ£‚àû\nùëñ=0ùëñ‚àí2)\nconverges to ùúã2/6. The result is that Aleph Filter‚Äôs FPR matches\nInfiniFilter‚Äôs FPR in the Widening Regime.\nùêπùëÉùëÖ‚âàùëã+1‚àëÔ∏Ç\nùëó=0ùõº¬∑ùëì(ùëó)¬∑2‚àí‚Ñì(ùëó)+ùëã‚àíùëó\n‚âàùõº¬∑2‚àíùêπ‚àí1¬∑ùëñ‚àëÔ∏Ç\nùëó=0¬∑1\n(ùëó+1)2‚â≤ùõº¬∑2‚àíùêπ‚àí1¬∑ùúã2\n6‚â≤ùõº¬∑2‚àíùêπ(3)\nExpansion Limit in Fixed-Width Regime. We saw above that\nthe fraction of slots in the filter that are occupied by void duplicates\nof entries created in generation ùëóisùõº¬∑ùëì(ùëó)¬∑2‚àíùêπ+ùëã‚àíùëó=ùõº¬∑2‚àíùêπ‚àí1.\nThe number of generations of void entries is ùëã‚àíùêπ+1. Hence, the\nfraction of slots occupied by void duplicates by the time we reach\nGenerationùëã(ùëã‚â•ùêπ) isùõæ(ùëã)=ùõº¬∑2‚àíùêπ‚àí1¬∑(ùëã‚àíùêπ+1). We may now\nMain hash tableSecondary hash tableVoid entriesAuxiliary hash tablesInsertsExpandExpandAppendFigure 8: Aleph filter stores the mother hashes of all void\nentries along the secondary and auxiliary hash tables.\nask at which point the number of duplicates takes up half the space\nin the filter, meaning that even after we expand, void duplicates\nfully take up the expanded capacity so there is no space for new\ninsertions. We obtain it by setting ùõºto one, equating ùõæ(ùëã)to1/2, and\nsolving forùëãto obtainùêπ+2ùêπ‚àí1. Recall from Section 2 that the Fixed-\nWidth Regime only supports 2ùêπexpansions anyways (as at this\npoint the FPR reaches one and the filter becomes useless). Hence,\nAleph Filter does not reduce the maximum number of supported\nexpansions in the Fixed-Width Regime.\nExpansion Limit in Widening Regime. As we saw, the fraction\nof slots occupied by void duplicates from Generation ùëóisùõº¬∑ùëì(ùëó)¬∑\n2‚àí‚Ñì(ùëó)+ùëã‚àíùëó. Letùë£denote the total number of generations of void\nentries. The overall fraction of slots occupied by void entries is‚àëÔ∏Åùë£\nùëó=0ùõº¬∑ùëì(ùëó)¬∑2‚àí‚Ñì(ùëó)+ùëã‚àíùëó. This expression is subsumed by Equation\n3 and is therefore lower than ùõº¬∑2‚àíùêπ. This establishes that the\nfraction of slots occupied by void entries converges to a small\nconstant as the filter expands. The intuition is that newer entries\nare assigned longer fingerprints, and so it takes them increasingly\nlonger to become void and start duplicating. Hence, Aleph Filter\nsupports an infinite number of expansions in the Widening Regime.\n4.3 Fast Deletes Using Tombstones\nDuplicating void entries makes delete operations intricate to handle.\nThe reason is that every duplicate of a void entry that we wish to\ndelete must be identified and removed. The challenge is doing so\nin constant time, without using a significant amount of additional\nmetadata, and without creating the possibility of false negatives.\nIn the simple case that a delete operation is targeting a run with\nat least one matching non-void entry, the entry with the longest\nmatching fingerprint will be removed as shown in Section 2. This\nsection focuses on processing deletes when the only matching\nentries in the target run are void entries.\nIdentifying a Void Entry‚Äôs Duplicates. An entry becomes void\nwhen the length of the mother hash that it was assigned when\nit was inserted matches the logarithm base two of the number of\nslots in the filter. In every subsequent expansion, the number of\nduplicates for the entry and the number of slots in the filter both\nmultiply by a factor of two. If the entry‚Äôs original mother hash\nconsists ofùëèbits and the filter currently comprises 2ùëòslots, then\nthe entry participated in ùëò‚àíùëèexpansions since it became a void\nentry. Therefore, it has 2ùëò‚àíùëèvoid duplicates. For example, suppose\nthe filter consists of 26slots and the mother hash we wish to remove\nis 0011. This mother hash consists of ùëè=4bits while the power\nof the size of the filter is ùëò=6, meaning that its number of void\nduplicates is 26‚àí4=4.\nWe can also infer which canonical slots contain these void du-\nplicates by using the mother hash of the original entry as the least\n\nsignificantùëèbits of their addresses, and applying all possible per-\nmutations to the remaining ùëò‚àíùëèbits. In the example just given, we\nuse the mother hash 0011 as the least significant bits and permute\nthe remaining two bits to obtain the following four slots addresses:\n000011, 010011, 100011, 110011. To execute the delete correctly,\nwe would need to remove a void duplicate from each one of these\ncanonical slots. The question becomes how to efficiently store and\nretrieve the mother hash of any void entry so that we can identify\nand remove all of its void duplicates?\nSecondary Hash Table. Similarly to the Chained InfiniFilter, Aleph\nfilter adds the mother hash of any entry that turns void into a\nsecondary hash table, which has an identical structure to that of\nthe main hash table yet fewer slots. Figure 7 illustrates an example.\nBefore the expansion, the secondary hash table stores only the\nmother hash of the void entry at Slot 11 of the main hash table. The\nsecondary hash table then expands alongside the main hash table.\nAfter the expansion, another entry turns void at Slot 010 of the main\nhash table, and so its mother hash is stored in the Secondary hash\ntable. Note that while a void entry may have multiple duplicates in\nthe main hash table (e.g., at Slots 011 and 111), its mother hash is\nonly stored once in the secondary hash table.\nAuxiliary Hash Tables. As the Secondary Hash Table fills up,\nit must expand as well. Expanding it entails transferring one bit\nfrom each entry‚Äôs fingerprint to its canonical slot address in the ex-\npanded hash table. In Figure 7, for example, the entry at Slot 1 of the\nsecondary hash table after the expansion corresponds to the entry\nwith fingerprint 11 before the expansion, since the least significant\nbit of its fingerprint is repurposed as the most significant bit of its\nslot address. Eventually, void entries appear in the secondary hash\ntable as well. At this point, we seal the secondary hash table and\nadd it to a chain of auxiliary hash tables. A new empty secondary\nhash table is then allocated. Figure 8 illustrates the high-level work-\nflow. This architecture is similar to that of the Chained InfiniFilter\nfrom Section 2. The core difference is that InfiniFilter traverses the\nsecondary and auxiliary hash tables to process queries and deletes\nwhile Aleph Filter does not.\nTombstones. A delete operation commences by modifying a void\nentry in the canonical slot of the key to be deleted into a tombstone\nat the main hash table. This causes subsequent queries to the deleted\nkey to likely return a negative (unless there is some other void\nentry in the slot, which would lead to a false positive). To encode a\ntombstone, we employ a special bit string of all 1s. Figure 9 shows\nan example of a delete operation of Key ùëãlanding at Canonical Slot\n101, where the only matching entry is a void entry. The first step is\nchanging the content of the slot from a void entry encoding (i.e.,\n1110) into a tombstone (i.e., 1111). As a result, subsequent queries\nto Keyùëãin the example will now return a negative.\nDeferred Removal of Duplicates. The second step of a delete\noperation is to add the canonical slot of a void entry to be removed\ninto a deletion queue, which is structured as an append-only array.\nRight before the next expansion, Aleph Filter pops one canonical\nslot address at a time from the deletion queue. It uses this address\nas a search key to traverse the secondary and auxiliary hash tables\nfrom largest to smallest. The search terminates as soon as it finds a\nmatching entry. Since larger hash tables store longer mother hashes\nand we search the hash tables from largest to smallest, the first\n00100001001001001001001001001000000010100111001011101111110111011100101000110100111Delete Key X, h(X) = 001101000Main hash tableDelete a void duplicate at  Canonical Slots 001 and 101Secondary  hash table(1) change 1110 to 1111(4)(5)(6)‚Ä¶Deletion Queue101(2) add  canonical address(3) Before next expansion, look up mother hash00110111Figure 9: Aleph filter deletes a void entry by first transform-\ning it into a tombstone and storing the canonical address in\na deletion queue. Before the next expansion, void duplicates\nare identified and removed.\nmatching entry that we find corresponds to the longest matching\nmother hash. Based on this mother hash, we identify all duplicates\nand remove them from the main hash table2.\nIn Figure 9, for example, we pop Slot 101 from the deletion queue\nand use it as a search key to probe the secondary hash table. We\nfind a matching entry at Slot 1 with fingerprint of 0. We concatenate\nthese to obtain a longest matching mother hash of 01. Since the\nmain hash table currently has 23slots while the longest matching\nmother hash consists of two bits, we infer that the void entry has\n23‚àí2=2duplicates in the main hash table, and that their canonical\nslots are 001 and 101. We proceed to delete one void entry from\neach of these canonical slots. Finally, we also delete the longest\nmatching mother hash from the secondary hash table.\nPreventing False Negatives. Figure 10 shows a more complex\nexample of a delete operation where there are two void entries in\nthe target canonical slot. When we visit the Secondary Hash Table,\nwe find a run containing two mother hashes of different lengths for\nthese void entries: 00 and 000. The different lengths of these mother\nhashes indicate that one of the void entries has two duplicates in the\nfilter while the other has one. There is now a question of whether\nto remove the entry with the single void entry or the one with the\ntwo void duplicates.\nLet us first suppose the entry with the smaller mother hash\n(and hence more duplicates) is removed. If this entry happens to\ncorrespond to a different key Y with a different extended mother\nhash than that of key X (e.g., ‚Ñé(ùëå)=...100), we would get false\nnegatives later when querying for Key ùëå. Particularly, a query for\nKeyùëåwould reach Slot 100, fail to find a matching entry, and return\na false negative. To prevent false negatives, we must delete the void\nentry with the fewest duplicates. As before, this entry corresponds\nto the one with the longest matching mother hash. This ensures that\nthe remaining void entry‚Äôs duplicates will still match whichever\nentry still exists. Hence, in Figure 10, one void entry is removed\nfrom Slot 001 of the main hash table, and the entry with Slot 0 is\nremoved from the secondary hash table.\nComputational Analysis. Turning a void entry into a tombstone\nand adding its address to the deletion queue take ùëÇ(1)time to\nexecute. The cost of identifying and removing all duplicates for\na given void entry is deferred and incurred right before the next\nexpansion. To quantify this cost in the worst-case, suppose the user\n2Note that if many such deletes of void entries take place before the next expansion\nand cause the filter utilization to drop significantly below the expansion threshold\nagain, we delay the expansion until utilization reaches the threshold again.\n\n0000000101001110010111011100001011(1) Delete Key X, h(X) = ‚Ä¶000Main hash tableSecondary hash table111001011010000001110001(2) find longest matching mother hash(3) Delete a void entry at Canonical Slot 000011001001001000001001111runrun001011000100Figure 10: Aleph filter identifies which void duplicates to\nremove based on the longest matching mother hash of the\ndeleted key.\ndeletes all void entries at once. The dominating part of the cost is to\nremove each individual duplicate from the main filter, while the cost\nof retrieving every mother hash from the secondary and auxiliary\nhash tables is a lower-order term (i.e., ùëÇ(ùëã/ùêπ)). In Section 4.2, we\nsaw that there are at most ùëÇ(2‚àíùêπ¬∑ùëã¬∑ùëÅ)void duplicates in the\nmain hash table in the Fixed-Width Regime. Each of them takes\nconstant time to remove. Hence, the overall amount of work is\nùëÇ(2‚àíùêπ¬∑ùëã¬∑ùëÅ). As this work is performed after ùëÅinsertions, it\nremains sub-constant as long as ùëã<2ùêπ. Since this is the maximum\nnumber of supported expansions anyways, the cost of removing\nvoid duplicates is amortized constant for the filter‚Äôs whole lifetime.\nIn the Widening Regime, we saw in Section 4.2 that there are at\nmostùëÇ(2‚àíùêπ¬∑ùëÅ)void duplicates in the main hash table. Removing\neach of them after ùëÅinsertions entails ùëÇ(2‚àíùêπ)additional overhead\nper insertion. Hence, Aleph Filter supports deletes in constant time\nwhile being able to expand indefinitely in the Widening Regime.\nMemory Analysis. The secondary and auxiliary hash tables store\nat mostùëÅ¬∑2‚àíùêπmother hashes altogether. The size of these hash\ntables is therefore smaller by a factor of at least 2‚àíùêπfrom the main\nhash table and therefore does not take up much memory.\n4.4 Fast Rejuvenation Operations\nIn Section 2, we saw that a rejuvenation operation rehashes a\nqueried key after retrieving it from storage to lengthen the longest\nmatching fingerprint in the target run. This helps reduce the FPR.\nIn Aleph Filter, a rejuvenation operation can be trickier to handle\nif the only matching entry in the target run is a void entry. In this\ncase, we must also eliminate its void duplicates.\nAleph Filter handles the case where the target run only contains\nmatching void entries by immediately rejuvenating one void entry\ninto the full fingerprint of the queried key. It also adds the cor-\nresponding canonical slot into a Rejuvenation Queue (similar to\nthe Deletion Queue from Section 4.3). Just before the next expan-\nsion, Aleph filter pops one address at a time from the Rejuvenation\nQueue. For each address, it finds the longest matching mother hash\nin the Secondary or Auxiliary Hash Tables. It compares the length\nof this mother hash to the log of the number of slots in the main\nhash table to infer how many void duplicates correspond to this\nmother hash and what their locations are. It then removes each of\nthese void duplicates. This process is identical to how deletes are\nprocessed with the only exception that a void duplicate need not\nbe removed from the queried key‚Äôs canonical slot, as the void entry\nthere has already been transformed into a full fingerprint upfront.\n000110010010010010010010010000000101001110010111011111101110011100010111Rejuvenate X, h(X) = 001100100Main  hash tableDelete a void duplicate at  Canonical Slot 000, 100Secondary Hash table(1) change 1110 to 0001(4)(5)‚Ä¶Rejuvenation  Queue 100(2) add  address(3) Before next expansion, find & remove mother hashrun111001010011000run0000Figure 11: Aleph filter rejuvenates a void entry by turning\nit into a full fingerprint and adding the canonical slot to a\nrejuvenation queue. It removes duplicates lazily.\nFigure 11 shows an example of rejuvenating a void entry at\nCanonical Slot 100. First, Aleph Filter transforms the void entry\ninto a full fingerprint and adds the canonical slot address to the\nRejuvenation Queue. Before the next expansion, Aleph filter pops\naddress 100 from the Rejuvenation Queue and uses it as a search key\nto query the Secondary Hash Table. It finds the longest matching\nfingerprint of 0 at Slot 0 (the other mother hash in this run is 000,\nwhich doesn‚Äôt match 100 along the third bit). Hence, the longest\nmatching mother hash is 00. This implies that the void entry had\nbeen duplicated twice at Canonical Slots 000 and 100 of the main\nhash table. However, since this is a rejuvenation operation, we\nknow we have already replaced one void entry by a fingerprint at\nCanonical Slot 100, so the only remaining duplicate to remove is at\nCanonical Slot 000. Finally, we also remove the longest matching\nmother hash from the Secondary Hash Table.\nRejuvenation operations do a constant amount of work upfront\nwhile deferring and amortizing the removal of duplicates to the\nnext expansion. Hence, their cost is ùëÇ(1).\n5 PREDICTIVE REGIME\nSo far, we have seen two complementary methods of scaling the FPR\nas the data grows. Rejuvenation operations lengthen fingerprints to\nreduce the FPR, but they are only effective when the query workload\nis targeting older entries (i.e., with shorter fingerprints). On the\nother hand, the Widening Regime scales the FPR by increasing\nthe fingerprint length assigned to newer entries, yet the cost is a\nhigher memory footprint of ùêπ+ùëÇ(lg lgùëÅ)bits / entry. This begs the\nquestion of whether there are ways to fix the FPR without relying\non rejuvenation operations or using more memory. This section\nintroduces the Predictive Regime to address this challenge. The\nPredictive Regime is an orthogonal contribution to Aleph Filter. It\nis also applicable to other expandable filters (e.g., InfiniFilter [ 19]).\nThe Predictive Regime takes as a parameter an estimate from the\nuser of how much the data will grow. We denote this estimate as\nùëÅùëíùë†ùë°, and it is measured as the ratio between what we think the final\ndata size will be to the initial filter capacity. Using this estimate, the\nPredictive Regime uses Equation 4 to assign fingerprints of length\n‚Ñì(ùëó)to entries inserted at Generation ùëó. The termùëãùëíùë†ùë°=log2(ùëÅùëíùë†ùë°)\nrefers to the number of expansions before reaching the estimate.\nAt Generation 0, Equation 4 assigns longer fingerprints of length\nùêπ+‚åà2¬∑log(ùëãùëíùë†ùë°‚àí1)‚åâbits. As the data size grows towards the estimate\n(1‚â§ùëó‚â§ùëãùëíùë†ùë°), it assigns an equal or shorter fingerprint length to\nevery subsequent generation. When we reach the data size estimate\n\n(i.e.,ùëó=ùëãùëíùë†ùë°+1), the Equation assigns fingerprints of length ùêπbits,\nwhile all fingerprints assigned in previous generations will have\nshrunk to at most ùêπbits. Note that this is in contrast to the Widening\nRegime, in which we begin with fingerprints of length ùêπbits and\nassign monotonically longer fingerprints as the filter expands. The\nmax function in Equation 4 keeps the inner part of the logarithm\nnon-zero so that the equation is defined for all ùëó‚â•0.\n‚Ñì(ùëó)=ùêπ+2¬∑‚åàlog2(max(|ùëãùëíùë†ùë°‚àí1‚àíùëó|,1))‚åâ (4)\nAfter surpassing the data size estimate (i.e., ùëó>ùëãùëíùë†ùë°+1), the\nPredictive Regime assumes the Widening Regime‚Äôs behavior. The\nabsolute value function within the logarithm in Equation 4 sets\nincreasing fingerprint lengths to subsequent generations to keep\nthe FPR stable. Note that when we set ùëÅùëíùë†ùë°=1(implyingùëãùëíùë†ùë°=0),\nthe Predictive Regime is identical to the Widening Regime from\nthe get-go. Generally, the memory complexity for the Predictive\nRegime isùêπ+ùëÇ(lg|lg(ùëÅ/ùëÅùëíùë†ùë°)|).\nVisualization. Figure 12 illustrates how the fingerprint length\nassigned to newer entries first drops during the first ùëãùëíùë†ùë°expansions\nand then increases again afterwards. The Predictive Regime initially\nrequires more memory than the Widening Regime. As the data\ngrows, however, the Predictive Regime comes to improve upon the\nWidening Regime across the board, especially when the real data\nsize is close to the estimate. This is a good trade-off; it is better\nto take up fewer bits per entry when the data is large rather than\nwhen it is small.\nFPR Analysis. The analysis of the FPR with the Predictive Regime\nis similar to the analysis of the Widening Regime from Section 4.2.\nThe contribution of any generation 0‚â§ùëó‚â§ùëãto the FPR is ùõº¬∑\nùëì(ùëó)¬∑2‚Ñì(ùëó)‚àí(ùëã‚àíùëó). Equation 5 sums this up across all generations\nto obtain the overall FPR.\nùêπùëÉùëÖ=ùëã+1‚àëÔ∏Ç\nùëó=0ùõº¬∑ùëì(ùëó)¬∑2‚Ñì(ùëó)‚àí(ùëã‚àíùëó)(5)\nBy plugging in ùëãùëíùë†ùë°forùëãin Equation 5 and simplifying, we\nobtain the maximum FPR until the moment we reach the data size\nestimate. This turns out to be at most 2‚àíùêπby the same analysis\nwe saw in Section 4.2 for the FPR in the Widening Regime. At this\npoint, all fingerprints consist of at most ùêπbits. Hence, the memory\nvs. FPR trade-off is on par with a static filter by the time we reach\nthe data size estimate.\nBy plugging infinity for ùëãin Equation 5 and simplifying, we\nobtain a maximum FPR of 2‚àíùêπ+1as we surpass the data size estimate.\nThe intuition is that the left-hand and right-hand curves of the\nPredictive Regime in Figure 12 each contribute an additive factor\nof2‚àíùêπ. It is possible to use one extra bit in advance to maintain a\ngiven FPR target as we surpass the data size estimate.\n6 EVALUATION\nWe evaluate Aleph Filter against the Fingerprint Sacrifice (FS)\nmethod and InfiniFilter, the state-of-the-art techniques for expand-\ning filters, which were described and analyzed in Sections 2 and 3.\nImplementation. We built Aleph filter as a fork and subclass\nof InfiniFilter to reuse its core machinery (e.g., for parsing slots,\nmigrating entries, etc.). All baselines inherit from the same Quotient\nFilter base class. Reusing code across the baselines means that any\nperformance differences arise due to their expansion algorithms\nPredictive  RegimeWidening Regime# expansionFingerprint length  (bits)F+2‚ãÖlog2(Xest‚àí1)F00XestXFigure 12: Given a rough estimate of how much the data size\nwill grow, Aleph filter requires far fewer bits per entry by\nthe time we reach the estimate and pass it.\nrather than implementation idiosyncrasies. We employ version\n11.0.16 of the Java compiler.\nWorkload Description. All experimental trials begin with a filter\nconsisting of 256canonical slots and issuing insertions, causing\nthe filter to expand multiple times. Unless otherwise mentioned, all\nqueries are issued to a given baseline right before the next expan-\nsion to measure the worst-case query performance (when clusters\nare longest). Other than Figures 13 Parts (C) and (D), the queries in\nall experiments target non-existing keys. Each query experiment\nissues 10k queries and averages their latency. Aside from Figure 17\nPart (B), all experiments employ uniformly random workloads. For\nall baselines, we use java.util.Random to generate random keys and\nxxhash [ 15] as the hash function. All data keys are eight-byte inte-\ngers before being hashed. Each baseline expands when 80% of the\nhash table is occupied, though we vary this threshold in Figure 16.\nFigure 14 focuses on the Widening and Predictive Regimes, while\nall other experiments use the Fixed-Width Regime with 12-bit slots.\nHardware. Our system is equipped with two Intel Xeon E5-2690v4\nprocessors, each running at 2.6 GHz with 14 cores and two hyper-\nthreads per core. The machine contains 512GB of RAM, 35MB of L3\ncache, 256KB of L2 cache, and 32KB of L1 cache. Storage includes\ntwo 960GB SSDs and four 1.8TB HDDs, though these drives are not\nused in the experiments. The system runs on Ubuntu 18.04.5 LTS.\nLower Query Cost. Figure 13 Parts (A) measures latency for ran-\ndom negative queries (i.e., to non-existing keys). On each curve,\none expansion occurs between two adjacent points, indicating a\ndoubling of the data size. As the data grows, the average latency\nincreases across all baselines as they outgrow the CPU caches. Nev-\nertheless, InfiniFilter‚Äôs query cost deteriorates more rapidly since\neach query checks a growing number of hash tables. The FS method\ncannot expand indefinitely as eventually, all fingerprints run out of\nbits. Its performance is also more erratic as the filter‚Äôs slot width\nchanges, leading to cache misalignment. In contrast, Aleph Filter\nsupports unlimited expansions while maintaining stabler latency\nas each query checks at most one hash table.\nPart (B) measures the false positive rate (FPR) for the same\nqueries as in Part (A). The FS method exhibits a skyrocketing FPR as\nfingerprints shrink across expansions. InfiniFilter and Aleph Filter\nexhibit stabler FPRs that match the model in Equation 2.\nPart (C) measures query latency for uniformly random existing\nkeys. InfiniFilter is only slightly slower than Aleph Filter as most\nqueries terminate after finding a matching entry in the main hash\ntable. In contrast, Part (D) measures latency for queries targeting\n\nneg. query (ns)\n(A)\nfalse positive rate\n(B)\npos. query (ns)\n(C)\nworst pos. query (ns)\n(D)Figure 13: Aleph Filter exhibits faster worst-case queries while matching InfiniFilter in terms of the false positive rate.\nquery latency (ns)\n(A)\nfalse positive rate\n(B)\nbits / entry\n(C)\ninsert latency (ns)\n(D)\nFigure 14: The Predictive Regime is best when we have a lower bound on how much the data will grow.\nthe oldest existing entries. InfiniFilter traverses multiple hash ta-\nbles in this case, thus incurring significantly higher latency. Aleph\nFilter is faster as each query returns a positive immediately after\nencountering a void entry in the main hash table.\nThe Widening and Predictive Regimes. Figure 14 considers an\napplication that requires an FPR of at most ‚âà1%while expecting\nthe data size to grow to ‚âà106entries. We initialize each baseline\nwith the smallest memory footprint such that when the data size\nreaches‚âà106entries, the FPR is at most ‚âà1%. InfiniFilter and\nAleph Filter in the Widening Regime are each assigned 13 bits per\nentry. We initialize the FS method with 30 bits per entry and Aleph\nfilter in the Predictive Regime with 22 bits per entry. We measure\nperformance as the data size approaches and exceeds ‚âà106entries.\nParts (B) and (C) show the takeaways. The FS method cannot\nmeet the FPR requirement after the data size exceeds ‚âà106en-\ntries since all fingerprints shorten in each expansion. In contrast,\nInfiniFilter and Aleph Filter in the Widening Regime maintain a\nconstant FPR from the get-go and even after the data size surpasses\n‚âà106entries. The trade-off is a growing memory footprint. Aleph\nfilter in the Predictive Regime also meets the FPR target while\nrequiring a memory footprint on par with static filters when we\nmeet the target data size. Even as the data outgrows our estimation,\nAleph filter still requires less memory than in the Widening Regime\nas predicted by our model in Equations 4 and 5 and in Figure 12.\nThe trade-off is that it requires a few more bits per entry from the\nget-go, though this is a good deal: it is better to use more bits per\nentry when the data is small rather than large.\nFor experimental control, Part (A) verifies that both variants of\nAleph Filter exhibit the fastest queries, while Part (D) shows that\nall baselines have approximately the same insertion speed.Cheaper Deletes. Figure 15 Part (A) focuses on deletion latency.\nWe show two variants of Aleph Filter with greedy vs. lazy deletes.\nThe former identifies and removes void duplicates immediately\nduring a deletion. The latter uses tombstones and removes void du-\nplicates lazily during the next expansion as described in Section 4.3.\nWe compare these baselines to InfiniFilter. All baselines are in the\nFixed-Width Regime with 12-bit slots. We initialize each baseline\nwith 29slots and perform insertions until it expands to 225slots. We\nthen clone each baseline. For each clone, we measure latency for\ndeleting 512random entries from the same generation (i.e., entries\ninserted in-between the same two expansions). As we move from\nleft to right on the x-axis, we delete entries from earlier generations\n(i.e., older entries).\nThe figure shows that for all baselines, deletion latency is ‚âà600\nns when deleting non-void entries (on the left-hand side). As we\ndelete older entries, the latency for InfiniFilter increases to ‚âà800\nsince more hash tables are accessed to find and remove them. For\nAleph Filter with greedy deletes, latency skyrockets as we target\nolder entries since each entry has exponentially more duplicates in\nthe main hash table, each of which must be removed. In contrast, for\nAleph Filter with lazy deletes, latency decreases as we delete older\nentries since we only replace one void entry with a tombstone (i.e.,\nwe do not need to shift the remaining fingerprints in the cluster\nbackward by one slot). Hence, Aleph filter‚Äôs use of tombstones\nachieves constant time deletions. An experiment with rejuvenation\noperations yields a nearly identical figure, and so we omit it.\nDeletion Cost Gets Amortized. When Aleph filter deletes a void\nentry by replacing it with a tombstone, the removal of the entry‚Äôs\npotentially many duplicates is deferred to the next expansion. To\nensure that this does not degrade expansion speed, Figure 15 Part (B)\n\ndelete latency (ns)\n(A)\nexpansion parts (ns)\n(B)Figure 15: Aleph Filter supports fast deletions by using tomb-\nstones and deferring the removal of void duplicates to the\nnext expansion.\nquery latency (ns)\n(A)\ninsert latency (ns)\n(B)\nFigure 16: The expansion threshold allows trading between\nspace and performance. Aleph Filter exhibits more robust\nquery performance as we vary this parameter.\nmeasures this overhead in both the Fixed-Width and Widening\nRegimes against the cost of migrating entries into an expanded hash\ntable. We run the experiment by inserting entries into an initially\nsmall empty filter, causing it to expand multiple times. From the\n10thexpansion onward, we delete the oldest remaining generation\nof entries before the next expansion (e.g., After Expansion 15, we\ndelete all entries inserted in Generation 5, etc.). During the next\nexpansion, all void duplicates for this generation of entries are\nremoved. This experiment represents the worst-case consistent toll\nthat deletes can exact on expansions.\nIn the Fixed-Width Regime, removing void duplicates takes ap-\nproximately two orders of magnitude less time than the overhead\nof migrating entries into the expanded hash table. The reason is\nthat the maximum number of void duplicates from each generation\nas a proportion of the filter size is small (i.e., ‚âà2‚àíùêπas shown in Sec-\ntion 4.2). By contrast, in the Widening Regime, entries are slower\nto become void as they are created with increasing fingerprint\nlengths. Therefore, there are only a few generations of void entries\nto remove. Once they are removed, all subsequent deletions target\nentries in the main hash table, and so the removal of void duplicates\ninflicts no toll on expansion. The experiment demonstrates that in\nboth regimes, the removal of void duplicates is heavily amortized\nwith respect to the cost of expansion. Thus, insertions stay fast no\nmatter how many deletes targeting old entries are in the workload.\nExpansion Threshold. Figure 16 varies the threshold at which\nwe expand InfiniFilter and Aleph Filter. A higher threshold makes\nthese filters more compact and memory efficient, though it hurts\nperformance as clusters in the underlying quotient filter become\nlonger. Part (A) of the figure measures the average query latency\nmemory (GB)\n(A)\ninsert latency (ns)\n(B)Figure 17: All baselines exhibit space spikes while expanding\n(Part A) and slowdown due to skewed insertions (Part B).\nright before each expansion as we insert 226keys. Each dot in this\nfigure represents a trial starting with a small empty filter. With a\nhigher threshold, InfiniFilter‚Äôs query latency significantly increases\nas longer clusters must be traversed across multiple hash tables.\nIn Aleph Filter, query latency increases more slowly as only one\nhash table is accessed per query. Part (B) shows that insertion\nperformance is similar across these baselines as longer clusters need\nto be traversed before finding the target run for a new fingerprint.\nTransient Space Cost. Figure 17 Part (A) uses an independent\nthread to measure the filter‚Äôs total memory footprint every two\nseconds as we insert 229keys. We observe significant memory\nspikes during expansion. A spike begins whenever we allocate a\nlarger main hash table and transfer fingerprints into it from the\nformer main hash table. A spike ends when we deallocate the former\nmain hash table. Handling expansions without such memory spikes\nis an intriguing future work direction.\nSkew. Figure 17 Part (B) examines the impact of insertion skew.\nWe insert keys from a Gaussian distribution with a mean of one\nbillion and varying the standard deviation on the x-axis (from 1\nmillion to 16 million) to control the likelihood of repeated insertions\nof the same key. Each dot represents a trial starting with a small\nempty filter followed by 224insertions. With both Aleph Filter and\nInfiniFilter, more skew (i.e., smaller standard deviation) leads to\nlonger clusters and thus higher insertion latency. This problem can\nbe alleviated by embedding a counter in the filter to count repeating\nidentical fingerprints rather than materializing each instance [ 44],\nthough this feature is not yet implemented in our library.\nSlot Width. Figure 18 compares Aleph Filter to InfiniFilter as we\nvary the initial slot width in the Fixed-Width Regime. Each dot\nrepresents a trial starting from a small empty filter and inserting\n226keys. Part (A) shows the FPR dropping for both baselines as\nthe wider slots support longer fingerprints. Part (B) shows that\nwith wider slots, InfiniFilter becomes more competitive with Aleph\nFilter in terms of query cost as there are fewer auxiliary hash tables\nto access along InfiniFilter‚Äôs chain. Nevertheless, Aleph Filter still\noutperforms InfiniFilter across the board. Parts (C) and (D) highlight\na limitation of Aleph Filter when operating with narrow slots (i.e.,\nfewer than 10 bits per entry). In this case, the fraction of void entries\nin the filter becomes significant. This leads to premature expansion,\nwhich compromises the overall memory footprint as shown in\nPart (C). This also degrades insertion throughput as the void entries\nentail more overhead to copy during expansion as shown in Part (D).\nThe figure shows that it is best to operate Aleph Filter with 10 bits\nper entry or more. In this range, Aleph Filter provides a significant\n\nfalse positive rate\n(A)\nquery latency (ns)\n(B)\nmemory (bits / entry)\n(C)\ninsert latency (ns)\n(D)Figure 18: Aleph Filter improves on InfiniFilter across the board in terms of query cost, though its memory footprint and\naverage insertion latency suffer when using a narrow slot width (i.e., fewer than 10 bits per entry).\nquery cost improvement without a significant degradation to the\noverall memory footprint or insertion latency. In the real world,\nfilters are typically allocated with 10-16 bits per entry, making\nAleph Filter viable across many applications [16, 22, 48].\n7 RELATED WORK\nSome filter expansion approaches allocate additional empty filters\ninto which more insertions can be made [ 1,13,33,34,58,60]. The\nissue is that multiple filters potentially need to be checked during\na query or deletion. Other approaches [ 39,61] form a hash ring of\nbuckets to support elastic expansion, yet this causes all operations to\ntakeùëÇ(lgùëÅ)time to search a binary tree for a given entry‚Äôs bucket.\nYet another approach involves fetching a larger fingerprint for a\nkey from storage, yet this entails expensive I/Os [ 59]. In contrast,\nAleph filter provides constant time operations and does not require\nstorage I/Os.\nComplementary approaches expand at a finer granularity to\nprevent blocking regular operations and save space [ 54,55,57].\nApplying such techniques to Aleph filter could be impactful.\nPagh, Segev, and Wieder prove a lower bound that if we initialize\na filter to constant capacity and expand it to contain ùëÅkeys, the\nfilter must at some point use at least lg lgùëÅbits per key in addition\nto what is required for a filter with a fixed capacity of ùëÅkeys [ 43].\nAleph Filter‚Äôs Widening and Predictive Regimes both meet this\nlower bound. The former meets it after the ùëÅdata entries are\ninserted, while the latter meets it before any entries are inserted.\nThe idea of duplicating void entries within a filter hash table\nto support indefinite expansion with constant time queries can\nalso be traced back to Pagh, Segev, and Wieder [ 43]. Taffy Cuckoo\nFilter [ 3] is an expandable Cuckoo filter [ 30] based on these ideas,\nthough it does not support deletes or rejuvenation operations. The\ndata structure by Liu, Yin, and Yu [ 38] performs constant time\noperations with high probability and a lower space overhead of\nlg lgùëÅ+ùëÇ(lg lg lgùëÅ)bits per entry if ùëÅandùëàare polynomially\nrelated. This matches the leading term of the lower bound. Never-\ntheless, this method has not been shown to support deletes, and it\ndoes not support unbounded growth beyond a universe size of ùëà.\nAleph filter improves on these works in several ways. (1) In the\nWidening Regime, it requires ùëÇ(lg lgùëÅ)rather thanùëÇ(lg lgùëà)bits\nper key to support unbounded growth while maintaining a stable\nFPR. This is a significant difference if ùëà, the universe size, is un-\nknown or significantly larger than ùëÅ, the data size. (2) To support\ndeletes, the data structure in [ 43] keeps track of the age of entries\nusing a binary age counter alongside each slot for lg lgùëàadditionalbits per key. In contrast, Aleph filter keeps track of the ages of en-\ntries using a chain of exponentially smaller hash tables that take up\nnegligible space. (3) The data structure in [ 43] handles fingerprint\ncollisions by storing the full keys of entries with colliding finger-\nprints in an auxiliary dictionary. In contrast, Aleph filter can store\nmultiple identical fingerprints in the main hash table by employing\nRobin Hood hashing. This eliminates one dictionary access from\nthe query path. (4) Aleph Filter introduces the Predictive Regime,\nwhich significantly reduces the memory footprint.\n8 CONCLUSION\nWe introduced Aleph Filter, an infinitely expandable filter with\nconstant time operations and improved FPR vs. memory trade-offs.\nApplying the Aleph Filter on top of other point filters [ 5,7,27,29,\n32,44], range filters [ 12,17,31,36,40,42,53,63], and adaptive\nfilters [26, 37, 41, 49] can offer intriguing future work directions.\nACKNOWLEDGMENTS\nWe thank the reviewers for their valuable feedback. This research\nwas supported NSERC grant #RGPIN-2023-03580.\nREFERENCES\n[1]Paulo S√©rgio Almeida, Carlos Baquero, Nuno Pregui√ßa, and David Hutchison.\n2007. Scalable Bloom Filters. Inform. Process. Lett. (2007).\n[2]David G. Andersen, Jason Franklin, Michael Kaminsky, Amar Phanishayee,\nLawrence Tan, and Vijay Vasudevan. 2009. FAWN: A Fast Array of Wimpy\nNodes. SOSP (2009).\n[3]Jim Apple. 2022. Stretching your data with taffy filters. Software: Practice and\nExperience (2022).\n[4] Michael A. Bender, Martin Farach-Colton, Rob Johnson, Russell Kraner, Bradley C.\nKuszmaul, Dzejla Medjedovic, Pablo Montes, Pradeep Shetty, Richard P. Spillane,\nand Erez Zadok. 2012. Don‚Äôt Thrash: How to Cache Your Hash on Flash. PVLDB\n(2012).\n[5] Ioana O Bercea and Guy Even. 2020. Fully-Dynamic Space-Efficient Dictionaries\nand Filters with Constant Number of Memory Accesses. SWAT (2020).\n[6] Burton H. Bloom. 1970. Space/Time Trade-offs in Hash Coding with Allowable\nErrors. CACM (1970).\n[7]Alex D Breslow and Nuwan S Jayasena. 2018. Morton Filters: Faster, Space-\nEfficient Cuckoo Filters via Biasing, compression, and decoupled logical sparsity.\nPVLDB (2018).\n[8]Andrei Z. Broder and Michael Mitzenmacher. 2002. Network Applications of\nBloom Filters: A Survey. Internet Mathematics 1 (2002), 636‚Äì646.\n[9] Larry Carter, Robert Floyd, John Gill, George Markowsky, and Mark Wegman.\n1978. Exact and Approximate Membership Testers. In STOC .\n[10] Pedro Celis, Per-Ake Larson, and J Ian Munro. 1985. Robin Hood Hashing. FOCS\n(1985).\n[11] Badrish Chandramouli, Guna Prasaad, Donald Kossmann, Justin J Levandoski,\nJames Hunter, and Mike Barnett. 2018. FASTER: A Concurrent Key-Value Store\nwith In-Place Updates. SIGMOD (2018).\n[12] Guanduo Chen, Zhenying He, Meng Li, and Siqiang Luo. 2024. Oasis: An Optimal\nDisjoint Segmented Learned Range Filter. PVLDB (2024).\n\n[13] Hanhua Chen, Liangyi Liao, Hai Jin, and Jie Wu. 2017. The Dynamic Cuckoo\nFilter. ICNP (2017).\n[14] John G. Clerry. 1984. Compact hash tables using bidirectional linear probing.\nIEEE Trans. Comput. (1984).\n[15] Yann Collet. 2023. XXHash. https://github.com/Cyan4973/xxHash (2023).\n[16] Alex Conway, Mart√≠n Farach-Colton, and Rob Johnson. 2023. SplinterDB and\nMaplets: Improving the Tradeoffs in Key-Value Store Compaction Policy. SIGMOD\n(2023).\n[17] Marco Costa, Paolo Ferragina, and Giorgio Vinciguerra. 2024. Grafite: Taming\nAdversarial Queries with Optimal Range Filters. SIGMOD (2024).\n[18] Niv Dayan, Manos Athanassoulis, and Stratos Idreos. 2017. Monkey: Optimal\nNavigable Key-Value Store. SIGMOD (2017).\n[19] Niv Dayan, Ioana Bercea, Pedro Reviriego, and Rasmus Pagh. 2023. InfiniFilter:\nExpanding Filters to Infinity and Beyond. SIGMOD (2023).\n[20] Niv Dayan and Stratos Idreos. 2018. Dostoevsky: Better Space-Time Trade-Offs\nfor LSM-Tree Based Key-Value Stores via Adaptive Removal of Superfluous\nMerging. SIGMOD (2018).\n[21] Niv Dayan and Stratos Idreos. 2019. The Log-Structured Merge-Bush & the\nWacky Continuum. SIGMOD (2019).\n[22] Niv Dayan and Moshe Twitto. 2021. Chucky: A Succinct Cuckoo Filter for\nLSM-Tree. SIGMOD (2021).\n[23] Niv Dayan, Moshe Twitto, Yuval Rochman, Uri Beitler, Itai Ben Zion, Edward\nBortnikov, Shmuel Dashevsky, Ofer Frishman, Evgeni Ginzburg, Igal Maly, et al .\n2021. The End of Moore‚Äôs Law and the Rise of the Data Processor. PVLDB (2021).\n[24] Biplob Debnath, Sudipta Sengupta, and Jin Li. 2010. FlashStore: High Throughput\nPersistent Key-Value Store. PVLDB (2010).\n[25] Biplob Debnath, Sudipta Sengupta, and Jin Li. 2011. SkimpyStash: RAM space\nskimpy key-value store on flash-based storage. SIGMOD (2011).\n[26] Kyle Deeds, Brian Hentschel, and Stratos Idreos. 2020. Stacked filters: learning\nto filter by structure. PVLDB (2020).\n[27] Peter C Dillinger, Lorenz H√ºbschle-Schneider, Peter Sanders, and Stefan Walzer.\n2022. Fast Succinct Retrieval and Approximate Membership Using Ribbon. SEA\n(2022).\n[28] Peter C. Dillinger and Panagiotis Pete Manolios. 2009. Fast, All-Purpose State\nStorage. SPIN (2009).\n[29] Tomer Even, Guy Even, and Adam Morrison. 2022. Prefix Filter: Practically and\nTheoretically Better Than Bloom. PVLDB (2022).\n[30] Bin Fan, David G. Andersen, Michael Kaminsky, and Michael Mitzenmacher.\n2014. Cuckoo Filter: Practically Better Than Bloom. CoNEXT (2014).\n[31] Mayank Goswami, Allan Gr√∏nlund, Kasper Green Larsen, and Rasmus Pagh.\n2014. Approximate Range Emptiness in Constant Time and Optimal Space. SODA\n(2014).\n[32] Thomas Mueller Graf and Daniel Lemire. 2020. Xor Filters: Faster and Smaller\nThan Bloom and Cuckoo Filters. JEA(2020).\n[33] Deke Guo, Jie Wu, Honghui Chen, and Xueshan Luo. 2006. Theory and Network\nApplications of Dynamic Bloom Filters. INFOCOM (2006).\n[34] Deke Guo, Jie Wu, Honghui Chen, Ye Yuan, and Xueshan Luo. 2009. The Dynamic\nBloom Filters. IEEE Trans Knowl Data Eng (2009).\n[35] Stratos Idreos, Niv Dayan, Wilson Qin, Mali Akmanalp, Sophie Hilgard, Andrew\nRoss, James Lennon, Varun Jain, Harshita Gupta, David Li, and Zichen Zhu. 2019.\nDesign Continuums and the Path Toward Self-Designing Key-Value Stores that\nKnow and Learn. In CIDR .\n[36] Eric R Knorr, Baptiste Lemaire, Andrew Lim, Siqiang Luo, Huanchen Zhang,\nStratos Idreos, and Michael Mitzenmacher. 2022. Proteus: A Self-Designing\nRange Filter. SIGMOD (2022).\n[37] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. SIGMOD (2018).\n[38] Mingmou Liu, Yitong Yin, and Huacheng Yu. 2020. Succinct Filters for Sets of\nUnknown Sizes. ICALP (2020).\n[39] Lailong Luo, Deke Guo, Ori Rottenstreich, Richard TB Ma, Xueshan Luo, and\nBangbang Ren. 2019. The Consistent Cuckoo Filter. INFOCOM (2019).\n[40] Siqiang Luo, Subarna Chatterjee, Rafael Ketsetsidis, Niv Dayan, Wilson Qin, and\nStratos Idreos. 2020. Rosetta: A Robust Space-Time Optimized Range Filter forKey-Value Stores. SIGMOD (2020).\n[41] Michael Mitzenmacher, Salvatore Pontarelli, and Pedro Reviriego. 2018. Adaptive\ncuckoo filters. SIAM ALENEX (2018).\n[42] Bernhard M√∂√üner, Christian Riegger, Arthur Bernhardt, and Ilia Petrov. 2022.\nbloomRF: On performing range-queries in Bloom-Filters with piecewise-\nmonotone hash functions and prefix hashing. EDBT (2022).\n[43] Rasmus Pagh, Gil Segev, and Udi Wieder. 2013. How to Approximate a Set\nWithout Knowing its Size in Advance. FOCS (2013).\n[44] Prashant Pandey, Michael A Bender, Rob Johnson, and Rob Patro. 2017. A\nGeneral-Purpose Counting Filter: Making Every Bit Count. SIGMOD (2017).\n[45] Prashant Pandey, Alex Conway, Joe Durie, Michael A Bender, Martin Farach-\nColton, and Rob Johnson. 2021. Vector Quotient Filters: Overcoming the\nTime/Space Trade-Off in Filter Design. In SIGMOD .\n[46] Prashant Pandey, Mart√≠n Farach-Colton, Niv Dayan, and Huanchen Zhang. 2024.\nBeyond Bloom: A Tutorial on Future Feature-Rich Filters. SIGMOD (2024).\n[47] W Wesley Peterson. 1957. Addressing for random-access storage. IBM journal of\nResearch and Development (1957).\n[48] Kai Ren, Qing Zheng, Joy Arulraj, and Garth Gibson. 2017. SlimDB: A Space-\nEfficient Key-Value Storage Engine For Semi-Sorted Data. PVLDB (2017).\n[49] Pedro Reviriego, Jim Apple, Alvaro Alonso, Otmar Ertl, and Niv Dayan. 2023.\nCardinality Estimation Adaptive Cuckoo Filters (CE-ACF): Approximate Mem-\nbership Check and Distinct Query Count for High-Speed Network Monitoring.\nIEEE/ACM Trans. Netw. (2023).\n[50] Pedro Reviriego, Miguel Gonz√°lez, Niv Dayan, Gabriel Huecas, Shanshan Liu,\nand Fabrizio Lombardi. 2024. On the Security of Quotient Filters: Attacks and\nPotential Countermeasures. IEEE Trans. Comput. (2024).\n[51] Subhadeep Sarkar, Niv Dayan, and Manos Athanassoulis. 2023. The LSM Design\nSpace and its Read Optimizations. ICDE (2023).\n[52] Sasu Tarkoma, Christian Esteve Rothenberg, and Eemil Lagerspetz. 2012. Theory\nand Practice of Bloom Filters for Distributed Systems. IEEE Commun. Surv. Tutor\n(2012).\n[53] Kapil Vaidya, Subarna Chatterjee, Eric Knorr, Michael Mitzenmacher, Stratos\nIdreos, and Tim Kraska. 2022. SNARF: a Learning-Enhanced Range Filter. PVLDB\n(2022).\n[54] Hancheng Wang, Haipeng Dai, Shusen Chen, Meng Li, Rong Gu, Huayi Chai,\nJiaqi Zheng, Zhiyuan Chen, Shuaituan Li, Xianjun Deng, et al .2024. Bamboo\nFilters: Make Resizing Smooth and Adaptive. IEEE/ACM Trans. Netw. (2024).\n[55] Hancheng Wang, Haipeng Dai, Meng Li, Jun Yu, Rong Gu, Jiaqi Zheng, and\nGuihai Chen. 2022. Bamboo Filters: Make Resizing Smooth. In ICDE .\n[56] Hengrui Wang, Te Guo, Junzhao Yang, and Huanchen Zhang. 2024. GRF: A\nGlobal Range Filter for LSM-Trees with Shape Encoding. Proceedings of the ACM\non Management of Data (2024).\n[57] Minmei Wang, Mingxun Zhou, Shouqian Shi, and Chen Qian. 2019. Vacuum\nFilters: More Space-Efficient and Faster Replacement for Bloom and Cuckoo\nFilters. PVLDB (2019).\n[58] Robert Williger and Tobias Maier. 2019. Concurrent Dynamic Quotient Filters:\nPacking Fingerprints into Atomics . Ph.D. Dissertation. Karlsruher Institut f√ºr\nTechnologie (KIT).\n[59] Yuhan Wu, Jintao He, Shen Yan, Jianyu Wu, Tong Yang, Olivier Ruas, Gong\nZhang, and Bin Cui. 2021. Elastic Bloom Filter: Deletable and Expandable Filter\nUsing Elastic Fingerprints. IEEE Trans Comput (2021).\n[60] Kun Xie, Yinghua Min, Dafang Zhang, Jigang Wen, and Gaogang Xie. 2007. A\nScalable Bloom Filter for Membership Queries. GLOBECOM (2007).\n[61] Minghao Xie, Quan Chen, Tao Wang, Feng Wang, Yongchao Tao, and Lianglun\nCheng. 2022. Towards Capacity-Adjustable and Scalable Quotient Filter Design\nfor Packet Classification in Software-Defined Networks. IEEE Open Journal of\nthe Computer Society (2022).\n[62] Fan Zhang, Hanhua Chen, Hai Jin, and Pedro Reviriego. 2021. The Logarithmic\nDynamic Cuckoo Filter. ICDE (2021).\n[63] Huanchen Zhang, Hyeontaek Lim, Viktor Leis, David G Andersen, Michael\nKaminsky, Kimberly Keeton, and Andrew Pavlo. 2018. SuRF: Practical Range\nQuery Filtering with Fast Succinct Tries. SIGMOD (2018).",
  "textLength": 77202
}