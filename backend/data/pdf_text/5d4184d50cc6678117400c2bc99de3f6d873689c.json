{
  "paperId": "5d4184d50cc6678117400c2bc99de3f6d873689c",
  "title": "Hands-off Model Integration in Spatial Index Structures",
  "pdfPath": "5d4184d50cc6678117400c2bc99de3f6d873689c.pdf",
  "text": "Hands-off Model Integration in Spatial Index Structures\nAli Hadian\nImperial College London\nhadian@imperial.ac.ukAnkit Kumar\nIIT Delhi\nmt1170727@iitd.ac.inThomas Heinis\nImperial College London\nt.heinis@imperial.ac.uk\nABSTRACT\nSpatial indexes are crucial for the analysis of the increas-\ning amounts of spatial data, for example generated through\nIoT applications. The plethora of indexes that has been de-\nveloped in recent decades has primarily been optimised for\ndisk. With increasing amounts of memory even on commod-\nity machines, however, moving them to main memory is an\noption. Doing so opens up the opportunity to use additional\noptimizations that are only amenable to main memory.\nIn this paper we thus explore the opportunity to use light-\nweight machine learning models to accelerate queries on spa-\ntial indexes. We do so by exploring the potential of using\ninterpolation and similar techniques on the R-tree, arguably\nthe most broadly used spatial index. As we show in our\nexperimental analysis, the query execution time can be re-\nduced by up to 60% while simultaneously shrinking the in-\ndex's memory footprint by over 90%.\nAIDB Workshop Reference Format:\nAli Hadian, Ankit Kumar, Thomas Heinis. Hands-o\u000b Model In-\ntegration in Spatial Index Structures. AIDB 2020.\n1. INTRODUCTION\nSpatial data is generated and in need of analysis in many\ndi\u000berent applications. The proliferation of GPS devices, in-\ntegrated in mobile devices, along with the growing use of\nhigh precision sensors, e.g., LIDAR, to map our surround-\nings has lead to a deluge of spatial datasets.\nE\u000eciency and speed are key to analyse data in general\nand spatial data in particular as having timely, actionable\ninsight is crucial. No wonder analytics has moved into main\nmemory where feasible and while spatial datasets are big in\nsize, many of them still \ft into main memory, particularly in\ncase of today's machines where main memory is abundantly\navailable.\nIn addition to simply move the data into main memory,\nrecent research also has developed a novel class of indexes\nthat use machine learning methods at their core [18]. More\nThis article is published under a Creative Commons Attribution License\n(http://creativecommons.org/licenses/by/3.0/), which permits distribution\nand reproduction in any medium as well allowing derivative works, pro-\nvided that you attribute the original work to the author(s) and AIDB 2020.\n2nd International Workshop on Applied AI for Database Systems and Ap-\nplications (AIDB’20), August 31, 2020, Tokyo, Japan.precisely, simple machine learning models are used in tradi-\ntional indexes to better approximate the speci\fc data dis-\ntribution of the data set at hand.\nMost notably, machine learning models like interpolation\nhave been used in B+-Trees and similar indexes with con-\nsiderable success in that the models helped to accelerate\nqueries by 50% [13] - a considerable improvement for data\nstructures, such as the B+-Tree which have been carefully\noptimised and tuned for decades [10].\nIn this paper, we consequently study the use of machine\nlearning techniques in spatial indexes. Straightforward reuse\nof the techniques used to optimise the B+-Tree [13] is un-\nfortunately not possible as multidimensional data with mul-\ntiple correlated dimensions is more complex. The higher\ncomplexity, however, also gives us more degrees of freedom\nto adapt the index structure. Our suggested solution accel-\nerates the spatial data structure using a simple predictive\nmodel, but at the same time re-arranges the physical layout\nsuch that the predictive models are the most e\u000bective. As\nwe show, use of interpolation techniques accelerates queries\non spatial data in main memory by up to 1.8X (4.2X on\nmulti-threaded execution) for a variety of datasets.\nThe remainder of the paper is organised as follows. We\n\frst discuss related work in Section2 and then motivate the\nidea of using simple machine learning models in spatial in-\ndexes in Sections 3 and 5. We then discuss how we imple-\nmented the machine learning models in the spatial indexes\nin Section 4. In Section 6 we discuss our experimental setup\nand, more importantly, the experimental results. We \fnally\nconclude the paper in Section 7.\n2. RELATED WORK\nA plethora of spatial indexes [6] has been developed in\nrecent decades. In the following discussion, we focus on\nrelevant work related to learned indexes for spatial data as\nwell as to interpolation used for other, lower-dimensional\nindexes.\nLearned index structures. In the past few years, tons\nof research have been done on using machine learning to\noptimize database systems [17], most notably learned index\nstructures [18] in which a machine learning model replaces\ntraditional index structures (such as B-tree and hash ta-\nbles) for locating the physical position of records. Several\nlearned models have been suggested so di\u000berent indexing\nproblems, such as range indexing [5, 7], bloom \flters [25,\n24], distributed indexing [20], and handling updates [3, 12].\nHybrid learned indexes . Some hybrid methods have\nbeen suggested to integrate machine learning into a well-\n1arXiv:2006.16411v2  [cs.DB]  9 Aug 2020\n\nknown algorithmic model such as B+tree [13] or run an\nauxiliary algorithmic index alongside a learned model [22].\nSuch hybrid approaches ensure that while the learned model\naccelerates the search, the algorithmic index still guarantees\nthe worst-case scenario when modelling is not e\u000bective for\nthe given dataset, and also help in handling updates.\nLinear models and interpolation. Among all learned\nindex models, simple linear models have been arguably the\nmost common [16, 7, 16, 22, 12, 3]. Even for a generic\nlearned index such as the RMI model that supports a vari-\nety of linear and non-linear models [18], it is experimentally\nshown that the best con\fguration found for most real-world\ndatasets is simply a linear spline model [15, 23]. Linear in-\nterpolation has been recently studied by some works [28, 9,\n13]. Despite that an interpolation model is not as accurate\nas other linear models such as linear regression, it has the\ninteresting feature that it does not need training, making in-\nterpolation an e\u000bective choice to be embedded inside hybrid\nindexes [13, 14].\nLearned spatial indexing. Recently, some e\u000borts have\nbeen done on accelerating spatial and multidimensional in-\ndexes with matching learning techniques. Nathan et al. sug-\ngested a speci\fc learned multidimensional index that learns\nfrom the query distribution how to optimize a grid index\nstructure [26]. Also, LISA [21] is a learned disk-resident\nR-tree. The di\u000berence between LISA and our work is that\nLISA focuses on minimizing the IO on disk by transforming\nthe data into a single dimension using a lattice regression\nmodel. However, such data transformation is not worth-\nwhile in a main-memory index due to the closer ratios of\nCPU and memory access overheads. Following a di\u000berent\nline of work, some e\u000bort has been made to learning and soft\nfunctional dependencies between the dimensions in spatial\nindexes and exploiting them for performance optimisation\non spatial indexes [8, 4].\n3. MOTIVATION\nDi\u000berent approaches have been developed for indexing\nmultidimensional datasets. One common approach is to use\ntree structures, based on space-oriented partitioning such as\nKD-Tree, Octree, Quadtree, or based on data-oriented par-\ntitioning like the R-tree. Hierarchical trees are easy to man-\nage and update, and are e\u000ecient choices for databases. Ar-\nguably the most prominent spatial index is the R-Tree [11],\nthe de-facto spatial index of a modern DBMS and used in\nIBM Informix, MySQL, PostgreSQL, Oracle, and PostGIS.\nHowever, in main memory, hierarchical trees require exces-\nsive pointer-chasing to execute queries. While on disk, for\nwhich the R-Tree historically has been developed, the time\nto follow pointers is insigni\fcant (compared to the overhead\nof retrieving data from disk) but in main memory the time\nfor chasing pointers contributes signi\fcantly to the overall\nquery execution time.\nNew hardware thus calls for new approaches for design-\ning indexing algorithms. First, the new indexes we design\nneed to reduce latency at the cost of bandwidth, e.g., having\nbigger nodes in search trees addresses the issue of excessive\npointer chasing. Second, given that the compute power of\nmodern CPUs has improved considerably (as opposed to the\nmemory bandwidth), the use of the CPU can and should be\nincreased to reduce the need to access data. Third, new in-\ndexes should give preference to computations that are more\nfriendly for new hardware, e.g., preference should be givento arithmetic operations over branch-heavy operators which\ncan deteriorate the pipeline. In this regard, machine learn-\ning is an e\u000bective tool that facilitates designing hardware-\ne\u000ecient indexes that exploit patterns in data to reduce data\nretrieval.\nWhile there is great potential for applying advanced ma-\nchine learning techniques powered by SIMD operations, this\npaper focuses entirely on linear interpolation | the sim-\nplest possible model | and surprisingly we show signi\fcant\nperformance improvement even in this case.\n4. INTERPOLATION FRIENDLY SPATIAL\nINDEXES\n4.1 Spatial Indexing Principles\nA vast number of spatial indexing methods have been de-\nveloped in the last decades [6]. Arguably the most broadly\nused spatial indexes are the R-Tree, the KD-Tree and the\nOctree (and its two dimensional equivalent, the Quadtree).\nWe primarily focus on these indexes in our work, but the\ntechniques described can be used to optimise further spatial\nindexes as well.\nAll these indexes work based on the same principles: they\nrecursively partitions space and create a hierarchical struc-\nture so as to guide query execution. At the bottom of the\ntrees, the data (or pointers to the data) is stored in the\nleaf nodes. The other nodes, the non-leaf nodes, are used\nto guide query execution and store the partitioning of the\nspace.\nThe major di\u000berence between the R-Tree and the KD-tree\nas well as the Octree is the approach to space partitioning\nused. Historically developed for use on disk, the R-Tree uses\na data-oriented partitioning, i.e., it partitions space such\nthat each leaf node neatly \flls a disk page (and therefore\noptimises the use of the disk bandwidth). Doing so leads\nto a partitioning entirely driven by the distribution of the\ndata. The partitioning is stored in the non-leaf nodes, i.e.,\neach non-leaf node Nstores a number of pairs < M;P > .\nMis the minimum bounding rectangle (MBR) enclosing all\nMBRs (or spatial objects in case of the leaf node) of one\nchild nodeCwhilePis the pointer to C.\nThe other approaches use a space-oriented partitioning,\ni.e., they partition the space recursively. The KD-tree cycles\nthrough the dimensions and at every level takes the median\nin one of the dimensions and splits space accordingly into\ntwo half spaces. It is essentially a binary tree, split on the\nmedian of the data indexed, i.e., using a hyperplane to split\nthe space. Instead of only two children, the Octree and the\nQuadtree use eight and four children respectively and split\nthe space/subtree in the middle rather than on the median.\nIn case of all indexes, a spatial range or point query (es-\nsentially a range query with extent zero), is executed by\nstarting at the root node and traversing down the hierarchy\nusing the MBRs (in case of the R-tree) or hyperplanes (in all\nother cases) in non-leaf nodes to ultimately \fnd the answer\nto the query stored in the leaf nodes.\nPrimarily due to their broad use in practice, we focus on\nthe R-tree, KD-Tree and the Octree (and its two dimensional\nequivalent, the Quadtree). While the R-Tree has historically\nbeen developed to speed up execution of queries on data\nstored on hard disk, it is still competitive when used in main\nmemory as we will also show experimentally.\n2\n\n4.2 Approach & Contribution\nIn line with previous work on the B+-tree we therefore\naim to use a model to predict the location of the records,\nprimarily on the leaf nodes. Doing so will accelerate scan-\nning the leaf nodes. At the same time, as scanning the leaf\nnodes becomes substantially faster, the optimal setting for\nthe leaf node size is likely to be bigger as well. Bigger leaf\nnode sizes in turn mean fewer leaf nodes which means fewer\npointers and thus less pointer chasing, thus speeding up ac-\ncess further.\nOne particular issue on making a learning-augmented spa-\ntial index is to understand which part of the spatial in-\ndex can be augmented with prediction models. Unlike one-\ndimensional range indexes such as B+-trees that have a\nnearly identical layout on all levels, the R-Tree has entirely\ndi\u000berent nodes as the leaf and non-leaf (internal) nodes. In-\nternal nodes of the R-Tree store the MBRs of their children\nalong with pointers to the children while the leaf nodes only\nstore data, i.e., points. The MBRs stored in internal nodes\nconsist of four values in 2D (and six in 3D) and the optimisa-\ntion potential for internal nodes is therefore limited as we,\nfor example, only can interpolate on one dimension/value\nyet we store four or six dimensions (plus a pointer per child\nnode). We consequently focus on optimising access to leaf\nnodes where we can, for example, interpolate on one dimen-\nsion but only have to store one additional dimension (or two\nin 3D).\n4.2.1 Deﬁning a Storage Order\nThe IF-X indexes enrich their unmodi\fed equivalent in-\ndexes with insights from learned indexes. A major chal-\nlenge, however, is how to de\fne an order for the records\nso that we can best predict. In a range index like B-tree,\nrecords are sorted by the key, hence the learned model sim-\nply learns the cumulative density function which maps the\nvalue of a key to the position of the key in the sorted array.\nIn multi-dimensional data, however, there is no such total\norder de\fned for the records as there are multiple di\u000berent\ndimensions over which the data can be sorted. One ap-\nproach is to learn a projection L:Rd!R that maps each\nd-dimensional record into a single dimension, hence record-\ning the data points [17]. Nonetheless, such a conversion is\ncomputationally expensive, and this approach has only been\ne\u000bective for disk-based R-trees where the CPU time is neg-\nligible compared to IO cost [21].\nWe take a di\u000berent approach that does not need data\ntransformation: sorting based on one of the existing dimen-\nsions. We choose for each leaf node individually the dimen-\nsion in which it is the most predictable, i.e., the dimension\nfor which the error of the model is the smallest. Figure 1a\nshows the data distribution of a leaf node containing 1024\n2D records (spatial points), and the 1D projection of the\npoints in each of the dimensions.\nFigure 1b illustrates this by showing the CDF of the val-\nues in each dimension, along with two prediction models:\nlinear interpolation (based on minimum and maximum val-\nues), and a quadratic polynomial with degree of 4 ( pos=\nax4+bx3+cx2+dx+e) that is \ft to the data. The dashed\nlines show the error of the models. In the example leaf, the\nlatitude values are more uniform and more predictable for\nboth models.\nNote that more complex models such as polynomial or\nRBF models have a higher capacity to \ft the CDF and are\n41.01 41.02 41.03 41.04 41.05\nLatitude76.2\n76.0\n75.8\n75.6\n75.4\nLongitude050\n0 50(a) Data distribution in a leaf node and the 1D mappings\n41.02 41.03 41.04 41.05\nLatitude02004006008001000Position\n76.25\n 76.00\n 75.75\n 75.50\n 75.25\nLongitude02004006008001000\nEmpirical CDF\nInterpolationQuadratic deg 4\nInterpolation ErrQuadratic Err\n(b) Evaluating the prediction error on di\u000berent dimensions\nFigure 1: Choosing the most predictable axis in IF-X\nable to predict the positions more accurately. However, such\ncomplex models need to store more parameters for predic-\ntion and are also slower to compute, hence we only use linear\ninterpolation. An additional bene\ft of interpolation is that\nthere is no need to run an expensive training process.\n4.2.2 Maximum Versus Average Error\n. The choice of the best dimension for physical storage\norder also depends on the local search algorithm. Once the\nlocation for a key is predicted by the model, a local search is\nperformed around the predicted position to \fnd the correct\nresult of the query. Local search in a learned index can\nbe done using binary search, which requires specifying a\nrange, or by other algorithms that do not need a speci\fed\nrange, such as linear search and exponential search. The\ncommon practice in learned indexes is to keep track of the\nmaximum prediction error per node, say \u0001, so that a search\ncan be done on [ ^ pos(x)\u0006\u0001] where the result is guaranteed\nto be. Therefore, the complexity of the search is in the leaf\nnode isO(log \u0001). Binary search is very e\u000bective and can be\nimplemented branch-free [2]. However, if \u0001 is large, binary\nsearch can incur multiple TLB- and cache-misses, which can\ndrastically reduce the performance. One e\u000bective solution\nto counter this issue is to check if the average prediction\nerror , say \u0016\u0001, is far less than the maximum prediction error\n(\u0001). In this case, it is more e\u000ecient to use linear search or\n3\n\n#ΔpDimACRecords sorted by column pDimNum recordsMax errorPrediction axisSlopeBaseModel’s metadataData pageFigure 2: Layout of leaf nodes of IF-X indexes\nexponential search, which rely on the average error instead\nof the maximum error. Linear search, for example, starts\nfrom the predicted location and scans towards left or right\nuntil it \fnds the \frst result belonging to the query, where it\nstops searching.\nIn this case, it is more e\u000ecient to use linear search, which\nisO(\u0016\u0001) or exponential search ( O(log\u0016\u0001)). This also a\u000bects\nthe choice of the dimension. If the search algorithm is un-\nbounded (linear or exponential search), then the dimension\nwhich has the smallest \u0016\u0001 is chosen as the storage order.\nTherefore, the most predictable dimension (storage order)\nin each leaf node is chosen with respect to the speci\fc model\nbeing used as well as the in-leaf search algorithm.\n5. IMPLEMENTATION\n5.1 Data Structures\n5.1.1 Leaf Node Layout\nAs discussed earlier, IF-X indexes use the same index\nstructure as of their basic/plain indexes, except on the leaf\nlevel. None of the indexes considers any speci\fc order for the\nrecords stored in each leaf node. Therefore, if the boundaries\nof a leaf node overlaps with the query, the indexes examine\n(compare) all records in the leaf node and select the ones\nthat match with the query. As scanning large nodes is gen-\nerally costly, in-memory the indexes are best con\fgured have\nsmaller leaf nodes which increases the depth of the tree.\nIF-X indexes, on the other hand, sort the records in each\nleaf node based on the best order using which the inter-\npolation error is minimized, as explained earlier. Our goal\nis to store all necessary information in the header of the\nleaf node, such that no extra memory lookup or excessive\ncomputation is required other than loading the data pages.\nFigure 2 shows the layout of the leaf node. The leaf con-\ntains the number of records K, the most predictable dimen-\nsion used as the storage order ( pDim), and model parameters\n(the slopeAand baseCof the line for linear interpolation).\nAlso, we store the maximum error in case that binary search\nis used as the local search algorithm. Nothing else needs to\nbe stored.\nNote that in case of linear interpolation, it is not essen-\ntial to store the model parameters in the leaf node. Linear\ninterpolation does not need training and can be done using\nthe minimum and maximum values of the chosen dimension,\nwhich can be fetched from the records page. However, ac-\ncessing both ends of the records' page incurs further memory\nlookups and TLB/cache misses. Instead, at build time, we\nconsider the minimum and maximum values in each node,\npre-compute the model parameters (slope Aand the base\nC), and store those parameters in the header of the leaf\nnode. The model parameters for linear interpolation are:A=maxPos\u0000minPos\nmaxVal\u0000minVal\nB=minPos\u0000A\u0002minVal(1)\nThe position of a query point in a leaf node can be pre-\ndicted as ^pos=A\u0002q[pDim ] +B. In fact, slope pre-\ncomputation and re-use is shown to be an e\u000bective optimiza-\ntion for iterative interpolation search [28] and we observed\nthe same bene\ft when interpolation is used as a model.\n5.1.2 Storage Order Implementation\nEvaluating the predictability of each dimension requires\nsorting data on each dimension ( O(Klog(K)) where K is the\nnumber of records in page), and then training and evaluating\nthe model on that order. In case that linear interpolation is\nused, the model requires no training and takes a single scan\nto evaluate, hence the total complexity is O(dKlog(K)).\nAn additional optimization we use is to de\fne a second\nsort dimension for records that have the same value for\nthe primary storage dimension. In case that many records\nhave the same value in the primary sort dimension, the sec-\nond dimension allows for a faster linear search among those\nrecords with the same value on the primary sort dimen-\nsion. We go one step further and sort the records lexico-\ngraphically based on all columns, i.e., sorting the records by\n[pDim; 0;1;\u0001\u0001\u0001;d\u00001]. This enables further optimization\non the scan operator, e.g., reducing branch misprediction in\nunrolled loops.\n5.2 Building & Updating\nWe use the STR [19] bulkloading strategy to partition the\ndata and then use our leaf node creation algorithm to create\nthe leaf nodes. All other nodes are subsequently created\nusing the STR approach. Any other approach to build an\nR-Tree or the other trees can be used whilst modifying it to\nuse our leaf node creation approach.\nOur suggested modi\fcation to spatial indexes does not af-\nfect the ability of IF-X indexes to handle updates. Indeed,\nany update strategy can be used in connection with recently\nproposed methods to out\ft one-dimensional learned indexes\nfor update-heavy workloads [3, 12], extended to multidimen-\nsional learned index structures.\n5.3 Querying\n5.3.1 Point Queries\nProcessing point queries is straightforward, the only dif-\nference in implementation with IF-X indexes is on the leaf\nnode search. Given a query q=x0;x1;\u0001\u0001\u0001;xd\u00001, we \frst\nconsider the predictable storage dimension of the leaf ( pDim )\nand try to \fnd the \frst record where points [i][pDim ] =\nq[pDim ]. To do so, we use the learned model of the leaf\nnode to predict the location of the record. For a linear\nmodel (including linear interpolation), the predicted posi-\ntion is ^pos=A\u0002points [pDim ] +B. Then, a local search is\nperformed around the predicted location. Local search can\nbe done using either binary, linear, or exponential search.\nIf the local search algorithm is binary search, the boundary\nfor search would be pos[ ^pos\u0006\u0001][pDim ]. Once the \frst (left-\nmost) record with points [i][pDim ] =q[pDim ] is found, all\nrecords with equal value on pDim are compared with query\non all dimension to examine if they match.\nAlgorithm 1 describes the search algorithm for point queries.\n4\n\nAlgorithm 1 Local search in leaf nodes for point query\n1:procedure LeafSearch (q, points, model, pDim,\nsDim)\n2: ^pos= model.predictLoc(q[pDim])\n3: ifq[pDim]>points[ ^pos][pDim] then\n4: pos = search(from= ^ pos\u0000\u0001, to = ^pos, axis=d)\n5: else if q[pDim]<points[ ^pos][pDim] then\n6: pos = search(from= ^ pos, to = ^pos+ \u0001, axis=d)\n7: end if\n8: ifq[pDim] = points[pos][pDim] then\n9: // Use the second dimension for guiding search\n10: ifq[sDim]>points[pos][sDim] then\n11: pos = LinearSearch(from=pos, direc-\ntion=left, axis=sDim)\n12: else\n13: pos = LinearSearch(from=pos, direc-\ntion=right, axis=sDim)\n14: end if\n15: while q[pDim,sDim] = pos[pos][pDim,sDim] do\n16: ifquery = points[pos] then\n17: return data[pos]\n18: pos += 1\n19: end if\n20: end while\n21: end if\n22: return null\n23:end procedure\n5.3.2 Range Queries\nIn range queries, the query is de\fned by a rectangle, i.e.,\nq= [(l0;u0);\u0001\u0001\u0001;(ld\u00001;ud\u00001)]. Since the records are sorted\nbypDim , we \fnd the \frst record where q[pDim ]:l\u0015\npoints [i][pDim ]. To \fnd such a record, we do the same\nrange search as in range indexes [18], which is similar to the\npoint query search (Algorithm 1), but replaces the equal-\nity check with inequality ( \u0014) in the matching constraint.\nOnce the \frst record is found, we perform a scan from the\nfound position and compare all records with the query con-\nstraints, until we reach the \frst record that hits the upper\nlimit of the query on pDim 's dimension, i.e., q[pDim ]:u <\npoints [i][pDim ], after which no further records will match\nthe query.\n6. EXPERIMENTAL ANALYSIS\n6.1 Experimental Setup\nHardware. Single-core experiments are performed on a\nsystem with 16 GB of memory and Intel Core i7-6700 (Sky-\nlake), which has four cores and is running at 3.4 GHz with\n32 KB L1, 256 KB L2, and 8 MB L3 caches. Multithreaded\nexperiments (Figure 5b) are conducted on an AMD EPYC\n7401 server running at 2GHz with Virtualization, having 24\navailable cores with 32 KB L1, 512 KB L2, and 64 MB L3\ncaches. Both machines have 16GB of memory.\nSoftware. All indexes are implemented in C++ and com-\npiled with GCC 9.2. Experiments are run on Ubuntu 18.04\nwith kernel version 4.15.0-65.\nDatasets. We used two spatial datasets: osm: latitude\nand longitude of landmarks from the US northeast section of\nOpenStreetMap data [1], with the timestamp being used as\nthe third dimension in the 3D experiments; and 3DScans :\n40 42 44 46\nlatitude80\n75\n70\nlongitudeData Distribution\n42.0 42.1 42.2 42.3\nlatitude76.0\n75.9\n75.8\n75.7\n75.6\n75.5\nlongitudeDistribution in a Sample Leaf Node(a) OSM (OpenStreetMap, Northeast section)\n2200\n 2100\n 2000\n 1900\nx34\n32\n30\n28\n26\nyData Distribution\n1975\n 1970\n 1965\n 1960\nx31.0\n30.5\n30.0\n29.5\nyDistribution in a Sample Leaf Node\n(b) 3DScans\nFigure 3: Distribution of the datasets\nA point clouds collected for robotic experiments, built by\na 3D scan (containing X,Y,Z coordinates) taken in front\nof the cathedral in Zagreb, Croatia [27]. We used 1 M,\n10Mand 100Msample records from each dataset. Fig-\nure 3 shows the heatmap of data distribution in both OSM\nand 3DScans data, along with an illustration of data distri-\nbution in sample leaf nodes within each dataset. We used\n2D data with 10M records as the default con\fguration for\nthe experiments. Values from all dimensions are stored as\nsingle-precision \roating point values.\n6.2 Tuning\nEach spatial index is tuned with its internal parameters,\nmost notably with node sizes between 2 and 32K records per\nnode (2i;1\u0014i\u001415). Figure 4, shows how the optimization\nhas been done on R-tree index, optimized for point queries.\nFor each data size and query type, we took the optimum\nleaf size for each index and compare the indexes with best\ncon\fguration. However, the optimal con\fguration for each\nindexing algorithm does not vary much for di\u000berent dimen-\nsions.\nIn our default con\fguration (2D index, 10M records, tun-\ning for point queries), the best leaf sizes found were: R-\ntree: 16, KD-tree: 128, Octree: 256, IF-RTree: 512, fIF-\nKDTree, IF-QuadTree, IF-Octree g: 2048. The optimal con-\n\fgurations found for 2D and 3D data are similar.\n6.3 Point Queries\nOverall performance We \frst benchmark how well our\nIF-X indexes can optimize their base versions. The point\nqueries are randomly sampled from the datasets. Figure 5a\nshows the speedup obtained by each IF-X index over its own\nbaseline for each dataset. IF-RTree has the highest speedup\n(1.8X speedup over R-tree in osm, 1.9X in 3DScans; for\nboth 2D and 3D data). Other algorithms, IF-KDTree and\nIF-[Quad/Oc]-Tree have speedup between 1.15X and 1.6X\n5\n\n28321285122K050010001500Avg lookup time (ns)\nN = 1M\n28321285122K\nN = 10M\n28321285122K\ndataset = 3Dscans N = 100M\n28321285122K\nLeaf capacity050010001500Avg lookup time (ns)\n28321285122K\nLeaf capacity\n28321285122K\nLeaf capacity\ndataset = osmAlgorithm\nRTree IF-RTree(a) 2D data\n28321285122K010002000Avg lookup time (ns)\nN = 1M\n28321285122K\nN = 10M\n28321285122K\ndataset = 3Dscans N = 100M\n28321285122K\nLeaf capacity010002000Avg lookup time (ns)\n28321285122K\nLeaf capacity\n28321285122K\nLeaf capacity\ndataset = osmAlgorithm\nRTree IF-RTree (b) 3D data\nFigure 4: Index parameter tuning: e\u000bect of leaf capacity on R-tree and IF-RTree indexes\n3Dscans osm1.01.21.41.61.8Speedupdim = 2\n3Dscans osm1.001.251.501.752.00dim = 3 \nIF-RTree IF-KDTree IF-[Quad/Oc]Tree\n(a) Single-threaded\n3Dscans osm1234Speedupdim = 2\n3Dscans osm1234dim = 3\n(b) Multi-threaded (24 Cores)\nFigure 5: Speedup of the IF-X indexes to their original ver-\nsion for point queries\nfor di\u000berent datasets and dimensions.\nDimensionality An interesting observation is that IF-X\nindexes almost retain their speedup for the 3D data, with\neven a rise in the speedup of IF-Octree (1.5X speedup on\n3D osm, compared to 1.15 in 2D). This is due to the fact\nthat even though the interpolation is done on one dimension,\nwith higher dimensions the IF-X indexes have more freedom\nto choose the best storage order (separately for each leaf),\ntherefore some of the leaf nodes that are skew in 2D become\nmore predictable when sorted on the third dimensions. We\ncan think of it as, for example, how in Figure 3b(right) thex axis makes the data more predictable compared to a one-\ndimensional index over y only.\nScalability. Along with the single-threaded experiments,\nwe compared the speedup of the IF-X indexes in a multi-\nthreaded setting using a 24-core machine (explained in sec-\ntion 6.1). In this setting, we use the total execution of\nthe query batch, hence the measurements and the reported\nspeedup shows the improvement in throughput, not the av-\nerage query times of individual queries. All the three spatial\nindexes have bene\fted from model-assisted acceleration in\nall datasets, with over 4X speedup in IF-RTree.\nPerformance breakdown. We are interested to see\nwhere does the speedup of IF-X indexes over the baselines\ncome from. To further analyze the speedups gained for the\ndefault con\fguration (Figure 5a-left), we measure the index\nsize and average lookup times.\nMemory footprint. The optimal parameters found in\nthe tuning phase suggest that IF-X indexes prefer much\nlarger leaf nodes than the original X indexes. Larger leaf\nsizes manifest in smaller footprints in the IF-X index. By\nfootprint, we mean the internal nodes of the indexes and put\naside the size of the leaf nodes (which equals the data size).\nFigure 6a shows, in logarithmic scale, the memory usage\nof each index in its optimal setting for 2D data with 10M\nrecords. The di\u000berence in memory footprint is signi\fcant.\nFor example, IF-RTree takes up only 5.5% of the size of R-\ntree, and the other IF-X indexes are no larger than 10% of\ntheir base version. Such a saving in memory footprint is typ-\nically enough to push the index up in the memory hierarchy\nand \ft the internal nodes into processor's caches.\nAverage lookup times. Comparing the lookup times of\nthe algorithms provides insights into why some IF-X indexes\nhave a comparably smaller speedup compared to the others.\nLookup times are shown in Figure 6b, which show that all\nIF-X algorithms derived from di\u000berent base indexes result\nin almost identical lookup time. This suggests that the IF-\nX family from di\u000berent base methods converge to a similar:\nA cache-resident index structure along with large, \rattened\nleaf nodes that are sorted to optimize the predictability of\n6\n\n3Dscans osm103Memory Footprint (KB)(a) Memory footprint\n3Dscans osm0200400600Lookup time (ns) (b) Average lookup time\n3Dscans osm02468Build time (sec)Algorithm\nRTree\nIF-RTree\nKDTree\nIF-KDTree\nQuadTree\nIF-QuadTree (c) Build time\nFigure 6: Comparison of indexes on their best con\fguration\nthe record locations.\n3Dscans osm05001000Avg lookup time (ns)knn = 10 | N = 1M\n3Dscans osm050010001500knn = 10 | N = 10M\n3Dscans osm01000200030004000knn = 10 | N = 100M\n3Dscans osm01000200030004000Avg lookup time (ns)knn = 100 | N = 1M\n3Dscans osm020004000knn = 100 | N = 10M\n3Dscans osm025005000750010000knn = 100 | N = 100M\n3Dscans osm025005000750010000Avg lookup time (ns)knn = 1000 | N = 1M\n3Dscans osm050001000015000knn = 1000 | N = 10M\n3Dscans osm010000200003000040000knn = 1000 | N = 100M\n3Dscans osm\nDataset010000200003000040000Avg lookup time (ns)knn = 10000 | N = 1M\n3Dscans osm\nDataset0200004000060000knn = 10000 | N = 10M\n3Dscans osm\nDataset050000100000150000200000knn = 10000 | N = 100MAlgorithm\nRTree\nIF-RTreeKDTree\nIF-KDTreeQuadTree\nIF-QuadTree\nFigure 7: Performance comparison on range queries\nBuild times We also report the build times of the in-\ndexes. Building IF-RTree takes 37% longer time than RTree,\nand the other IF-X indexes take less than 10% longer to\nbuild. Note that each algorithm in the chart has a di\u000berent\nleaf size. Unlike the base indexes in which a signi\fcant share\nof the build time is taken on building the internal nodes,\nthe build time of IF-X indexes are mostly taken by sorting\nthe large leaf nodes on multiple axes in order to choose the\nmost predictable storage order. This shows a considerable\npotential in scaling IF-X methods with parallel execution,because sorting large leaf nodes for storage selection in IF-\nX can be done independently for each node using multiple\nthreads; while paralleizing the baseline algorithms requires\nconcurrent access control mechanisms on tree indexes and is\nharder to scale. However, scaling up the build algorithm is\nnot considered in this work.\n6.4 Range Queries\nFor range queries, we considered four sets of queries with\ndi\u000berent selectivities, \u001b2S=f10;100;1K;10Kg. We build\na KD-Tree for the chosen datasets (for appropriate sizes).\nFor each\u001b2S, we then choose random points in the KD-\nTree, and \fnd the set of nearest \u001bpoints (with euclidean\nmetric),P. The MBR of Pthen gives us the desired query.\nFigure 7 shows the performance of the indexes for range\nqueries. For lower selectivities (small K), query time is\nmainly a\u000bected by the initial lookup of the corner point of\nthe query rectangle and hence the speedups are more simi-\nlar to that of point queries. For larger selectitivies, however,\nquery times are be mostly taken by the scan operations. Yet,\nsince all IF-X indexes favour larger leaf nodes, the scan op-\nerations enjoys a considerable speedup.\n7. CONCLUSION AND FUTURE WORK\nIn this paper we have integrated learned models into the\nmost broadly used spatial indexes. We dub the resulting\nindexes IF-X indexes for all of them as they all are based\non space-partitioning trees for the organisation of data. In-\ntegrating learned models is not straightforward in spatial\nindexes. Multiple dimensions and the absence of a total\norder make it challenging to \fnd computationally e\u000ecient\nmodels that accelerate query execution. As our experiments\nshow, however, our IF-X indexes are consistently consider-\nably faster than the their plain, unmodi\fed equivalents and\nthey also consistently have a smaller footprint.\nIn our current work we did not make any assumptions\nabout the query workload. In case that the query workload\nis known a priori, the leaf creation algorithm can be modi\fed\nto use the dimension that is most predictable and most help-\nful for executing the workload. For example, if most queries\ndo not put a constraint on a speci\fc dimension, then that\ncolumn is not selected for ordering the records. We leave\nthis as future work.\n7\n\n8. REFERENCES\n[1] Openstreetmap on aws.\nhttps://aws.amazon.com/public-datasets/osm.\nAccessed 2020-05-29.\n[2] Performance comparison: linear vs binary search.\nhttps://dirtyhandscoding.github.io/posts/performance-\ncomparison-linear-search-vs-binary-search.html.\n[3] J. Ding, U. F. Minhas, J. Yu, C. Wang, J. Do, Y. Li,\nH. Zhang, B. Chandramouli, J. Gehrke, D. Kossmann,\nD. Lomet, and T. Kraska. Alex: An updatable\nadaptive learned index. page 969984, 2020.\n[4] J. Ding, V. Nathan, M. Alizadeh, and T. Kraska.\nTsunami: A learned multi-dimensional index for\ncorrelated data and skewed workloads, 2020.\n[5] P. Ferragina and G. Vinciguerra. The PGM-index: a\nfully-dynamic compressed learned index with provable\nworst-case bounds. Proceedings of the VLDB\nEndowment , 13(8), 2020.\n[6] V. Gaede and O. G unther. Multidimensional Access\nMethods. ACM Computing Survey , 30(2):170{231,\n1998.\n[7] A. Galakatos, M. Markovitch, C. Binnig, R. Fonseca,\nand T. Kraska. Fiting-tree: A data-aware index\nstructure. In Proceedings of the International\nConference on Management of Data , 2019.\n[8] B. Gha\u000bari, A. Hadian, and T. Heinis. Leveraging soft\nfunctional dependencies for indexing\nmulti-dimensional data. arXiv preprint\narXiv:2006.16393 , 2020.\n[9] G. Graefe. B-tree Indexes, Interpolation Search, and\nSkew. In DaMoN , 2006.\n[10] G. Graefe et al. Modern B-tree Techniques.\nFoundations and Trends in Databases , 3(4):203{402,\n2011.\n[11] A. Guttman. R-trees: A Dynamic Index Structure for\nSpatial Searching. In Proceedings of the International\nConference on Management of Data , 1984.\n[12] A. Hadian and T. Heinis. Considerations for Handling\nUpdates in Learned Index Structures. In Proceedings\nof the Second International Workshop on Exploiting\nArti\fcial Intelligence Techniques for Data\nManagement , 2019.\n[13] A. Hadian and T. Heinis. Interpolation-friendly\nB-trees: Bridging the Gap Between Algorithmic and\nLearned Indexes. In EDBT , 2019.\n[14] A. Hadian and T. Heinis. Madex: Learning-augmented\nalgorithmic index structures. In Proceedings of the 2nd\nInternational Workshop on Applied AI for Database\nSystems and Applications , 2020.\n[15] A. Kipf, R. Marcus, A. van Renen, M. Stoian,\nA. Kemper, T. Kraska, and T. Neumann. SOSD: ABenchmark for Learned Indexes. NeurIPS Workshop\non Machine Learning for Systems , 2019.\n[16] A. Kipf, R. Marcus, A. van Renen, M. Stoian,\nA. Kemper, T. Kraska, and T. Neumann.\nRadixSpline: A Single-Pass Learned Index. In\nProceedings of the Second International Workshop on\nExploiting Arti\fcial Intelligence Techniques for Data\nManagement , 2020.\n[17] T. Kraska, M. Alizadeh, A. Beutel, E. H. Chi, J. Ding,\nA. Kristo, G. Leclerc, S. Madden, H. Mao, and\nV. Nathan. SageDB: A Learned Database System.\n2019.\n[18] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and\nN. Polyzotis. The case for learned index structures. In\nSIGMOD , 2018.\n[19] S. Leutenegger, M. Lopez, and J. Edgington. STR: A\nSimple and E\u000ecient Algorithm for R-Tree Packing.\n1997.\n[20] P. Li, Y. Hua, P. Zuo, and J. Jia. A scalable learned\nindex scheme in storage systems. arXiv preprint\narXiv:1905.06256 , 2019.\n[21] P. Li, H. Lu, Q. Zheng, L. Yang, and G. Pan. LISA: A\nLearned Index Structure for Spatial Data. In\nProceedings of the International Conference on\nManagement of Data , 2020.\n[22] A. Llavesh, U. Sirin, R. West, and A. Ailamaki.\nAccelerating b+tree search by using simple machine\nlearning techniques. In Proceedings of the 1st\nInternational Workshop on Applied AI for Database\nSystems and Applications , 2019.\n[23] R. Marcus, E. Zhang, and T. Kraska. CDFShop:\nExploring and Optimizing Learned Index Structures.\nInProceedings of the International Conference on\nManagement of Data , 2020.\n[24] M. Mitzenmacher. A model for learned bloom \flters\nand related structures. arXiv preprint\narXiv:1802.00884 , 2018.\n[25] M. Mitzenmacher. Optimizing learned bloom \flters by\nsandwiching. In NIPS , 2018.\n[26] V. Nathan, J. Ding, M. Alizadeh, and T. Kraska.\nLearning multi-dimensional indexes. In Proceedings of\nthe International Conference on Management of Data ,\n2020.\n[27] J. Schauer. Robotic 3D scan repository.\nhttp://kos.informatik.uni-osnabrueck.de/3Dscans/,\n2017.\n[28] P. Van Sandt, Y. Chronis, and J. M. Patel. E\u000eciently\nsearching in-memory sorted arrays: Revenge of the\ninterpolation search? In Proceedings of the\nInternational Conference on Management of Data ,\n2019.\n8",
  "textLength": 39567
}