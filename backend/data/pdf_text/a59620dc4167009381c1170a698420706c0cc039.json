{
  "paperId": "a59620dc4167009381c1170a698420706c0cc039",
  "title": "Updateable Data-Driven Cardinality Estimator with Bounded Q-error",
  "pdfPath": "a59620dc4167009381c1170a698420706c0cc039.pdf",
  "text": "Updateable Data-Driven Cardinality Estimator with Bounded\nQ-error\nYingze Li\nHarbin Institute of Technology\nChina\n23B903046@stu.hit.edu.cnXianglong Liu\nHarbin Institute of Technology\nChina\n23S003029@stu.hit.edu.cnHongzhi Wangâˆ—\nHarbin Institute of Technology\nChina\nwangzh@hit.edu.cn\nKaixin Zhang\nHarbin Institute of Technology\nChina\n21B903037@stu.hit.edu.cnZixuan Wang\nHarbin Institute of Technology\nChina\n2023113027@stu.hit.edu.cn\nAbstract\nModern cardinality estimators struggle with data updates. This\nresearch tackles this challenge within a single table. We introduce\nICE, an Index-based Cardinality Estimator, the first data-driven\nestimator that enables instant, tuple-leveled updates.\nICE has learned two key lessons from the multidimensional in-\ndex and applied them to solve CE in dynamic scenarios: (1) Index\npossesses the capability for swift training and seamless updating\namidst vast multidimensional data. (2) Index offers precise data\ndistribution, staying synchronized with the latest database version.\nThese insights endow the index with the ability to be a highly accu-\nrate, data-driven model that rapidly adapts to data updates and is\nresilient to out-of-distribution challenges during query testing. To\nenable a solitary index to support CE, we have crafted specific algo-\nrithms for training, updating, and estimating. Furthermore, we have\nanalyzed the unbiasedness and variance of the estimation results.\nExtensive experiments demonstrate the superiority of ICE. ICE\noffers precise estimations and fast updates/constructions across\ndiverse workloads. Compared to state-of-the-art real-time query-\ndriven models, ICE boasts superior accuracy (2-3 orders of magni-\ntude more precise), faster updates ( 4.7âˆ’6.9Ã—faster), and signifi-\ncantly reduced training time (up to 1-3 orders of magnitude faster).\nCCS Concepts\nâ€¢Information systems â†’Query optimization ;Multidimen-\nsional range search .\nKeywords\nCardinality Estimation\nACM Reference Format:\nYingze Li, Xianglong Liu, Hongzhi Wang[1], Kaixin Zhang, and Zixuan\nWang . 2024. Updateable Data-Driven Cardinality Estimator with Bounded\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nConferenceâ€™17, July 2017, Washington, DC, USA\nÂ©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-XXXX-X/18/06\nhttps://doi.org/XXXXXXX.XXXXXXXQ-error. In .ACM, New York, NY, USA, 13 pages. https://doi.org/XXXXXXX.\nXXXXXXX\n1 Introduction\nCardinality estimation (CE) is crucial for query optimization [ 27],\npredicting query selectivity without execution. Despite its impor-\ntance, unresolved issues persist, causing estimation errors up to\n104in both open-source and commercial systems [ 30]. These er-\nrors can deteriorate query plans and database performance [ 11].\nThus, addressing CE is vital for optimizing query execution and\nenhancing database performance.\nTo tackle CE, data-driven [ 12,21,33] and query-driven meth-\nods [ 14,20] have emerged. Data-driven methods learn joint data\ndistributions for accurate estimations, while query-driven models\nlearn query to cardinality mappings. Data-driven models, with-\nout prior workload knowledge, excel in generalizing to unknown\nqueries, promising broader applications.\nHowever, both data-driven and query-driven paradigms face a\ncommon Achillesâ€™ heel, data updates . For query-driven models,\nthe cardinalities of all known training queries may change after data\nupdates, necessitating the re-execution of queries in the training\nset to obtain the true cardinalities and retraining the model on the\nnew workload. For example, as shown in Table 1, MSCN [ 14] has to\nre-execute the query and be retrained after each data update, which\nresults in a huge overhead 2.5Ã—109ğœ‡ğ‘ per update. For data-driven\nmodels, whenever the original data is updated, the joint distribution\nof the relational table changes accordingly [ 20]. Compared with the\ntraditional estimators like histograms [ 24], the learned data-driven\nmodels require a much slower finetuning process to adapt to the\nnew database state [ 20,30]. Although this process is prolonged,\nupdating these models in a dynamically changing environment is\nindispensable. Using the stale model for prediction or estimation\nmay result in errors as high as 103or even more, as evidenced by the\nstale Naru and MSCNâ€™s max Q-error in Table 1. This level of error\ncannot be overlooked. These challenges make it difficult for these\nmodels to adapt to real-world scenarios with real-time data updates.\nIn order to deal with the CE in dynamic scenarios, attempts\nhave been made using the query-driven paradigm. ALECE [ 20],\na query-driven model, utilizes transformers to learn the correla-\ntion between histogram and query representations. When data is\nupdated, ALECE modifies the histogram representation without\n1Corresponding author.arXiv:2408.17209v1  [cs.DB]  30 Aug 2024\n\nConferenceâ€™17, July 2017, Washington, DC, USA Yingze Li, Xianglong Liu, Hongzhi Wang1, Kaixin Zhang, and Zixuan Wang\nTable 1: Modern cardinality estimators in dynamic environments. For models that update slowly, we use Nto denote the New\nfine-tuned model and Sto denote the Stale model. We report the result on the Insert-Heavy workload of the DMV [ 2] dataset.\nMore details can be found in Section 6.2.\nICE(Ours) MSCN Naru CardIndex ALECE\nModel Index Only Conv Network AR Network AR Network + Index Transformer + Hist\nType Data-driven Query-driven Data-driven Data-driven Query-driven\nTraining Time 13.8ğ‘  76(ğ‘‡ğ‘Ÿğ‘ğ‘–ğ‘›)+2514(ğ¿ğ‘ğ‘ğ‘’ğ‘™)ğ‘  2972ğ‘  112(ğ·ğ‘’ğ‘’ğ‘)+15.9(ğ¼ğ‘›ğ‘‘ğ‘’ğ‘¥)ğ‘  69(ğ‘‡ğ‘Ÿğ‘ğ‘–ğ‘›)+2514(ğ¿ğ‘ğ‘ğ‘’ğ‘™)ğ‘ \nUpdate Time 6.2ğœ‡ğ‘ /ğ‘¡ğ‘¢ğ‘ğ‘™ğ‘’ 2.5Ã—109ğœ‡ğ‘ /ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’ 453ğœ‡ğ‘ /ğ‘¡ğ‘¢ğ‘ğ‘™ğ‘’ 36.7(ğ·ğ‘’ğ‘’ğ‘)+8.1(ğ¼ğ‘›ğ‘‘ğ‘’ğ‘¥)ğœ‡ğ‘ /ğ‘¡ğ‘¢ğ‘ğ‘™ğ‘’ 19ğœ‡ğ‘ /ğ‘¡ğ‘¢ğ‘ğ‘™ğ‘’\nInference Time 8.12ğ‘šğ‘  0.71ğ‘šğ‘  17.6ğ‘šğ‘  8.46ğ‘šğ‘  1.51ğ‘šğ‘ \nQMAX 12 1Ã—103(N)| 4Ã—103(S) 23(N)| 6Ã—103(S) 3Ã—103525\nfine-tuning the transformer model. Although ALECE utilizes coarse\ndata features such as histogram representation, when there is a cer-\ntain distance between the query distribution in the testing set and\nthat in the training set, ALECE still makes a significant prediction\nerror over a magnitude of 525, as shown in Table 1. This means its\ngeneralization ability on unseen queries still cannot reach the same\nlevel as data-driven models.\nThe fundamental reason why existing deep data-driven models\ncannot effectively support data updates in databases lies in the\ninefficiency of neural networks in storing or learning the repre-\nsentations of database tuples. For a trained neural network, the rep-\nresentation of a single data tuple is dispersed across every learned\nnetwork parameter in each network layer. Suppose that we want\nthe network to \"insert\" new tuples or \"forget\" old ones. In this case,\nwe have to iteratively perform gradient descent on each parameter\nin the network and update the parameters one by one [ 16]. This\nissue results in inefficient updates, and even with the parallel com-\nputing power of hundreds or thousands of cores on modern GPU\nhardware. As shown in Table 1, Naruâ€™s update latency is still only\n453 microseconds per updated tuple. In contrast, the index stores\ndifferent tuples in the leaf slots of corresponding subtrees, main-\ntaining mutual isolation. For each insertion/deletion/modification\noperation, only local parameters of the scale of ğ‘™ğ‘œğ‘”(ğ‘)will be mod-\nified in the subtrees of the model, where ğ‘is the total size of the\ndata. In contrast, the parameters in the rest of the model remain\nunchanged. Therefore, the index is highly efficient and widely de-\nployed in updates for many dynamic database scenarios [ 4,13,32].\nWe observe that index structure serve as an efficient model for\nboth preserving lossless data distribution and supporting rapid\ndata updates. These characteristics have sparked our reflection:\nIs it possible to leverage existing mature index structures in\ndatabases to create a fully index-driven cardinality estimator?\nRegarding estimation accuracy, as the index fully preserves the\nlossless distribution of data, we believe that such a cardinality\nestimator will be able to achieve high-precision predictions of query\nselectivity. Furthermore, in terms of update speed, given the rapid\ndevelopment of index technology over the past four decades [ 5,9,\n10,15], we can expect this cardinality estimator to enable fast and\nefficient updates of data distributions, thus significantly enhancing\nthe performance of database queries.\nHowever, relying solely on index structure in data-driven CE\nposes a challenge. It is difficult to imagine how an index can trim\nthe sampling space online like an AutoRegressive(AR) model and\nachieves high estimation efficiency. Existing index-supported cardi-\nnality estimators like CardIndex [ 21] try a compromise by \"gluing\"AR network with learned index, but this falls short in accuracy and\nefficiency. CardIndexâ€™s accuracy suffers as it relies on neural net-\nworks for high-cardinality queries, but limited model parameters\nyield inaccurate probability density predictions, causing significant\nerrors (see Table 1). In terms of efficiency, every database update\nrequires a slow, thorough update of the neural network, which sig-\nnificantly slows down CardIndexâ€™s training and updating process,\nwith neural network training and fine-tuning accounting for most\nof the time (see Table 1).\nTo address these challenges, we draw inspiration from exist-\ning multidimensional indexing works [ 9,28] by leveraging data-\nskipping technique of multidimensional indexes to enhance sam-\npling efficiency. We first utilize the data-skipping technique to filter\nthe query space. Then, we use the index to convert the filtered\nsubregions into a compact rank space and sample points on the\nrank space. At last, we use the index again to map those sampled\nvalues back to the original tuple representations and aggregate the\nresults. In essence, we have devised an utterly index-driven CE\nmethodology that attains excellent estimation accuracy and seam-\nlessly facilitates instant updates encompassing insertions, deletions,\nand modifications. All the update operations can be accomplished\nwithinğ‘‚(ğ‘™ğ‘œğ‘”(ğ‘))time.\nThe contributions of the paper are summarized below:\nS1.We proposed ICE, an Index-based Cardinality Estimator(ICE)\n(Section 4.1). It is the first high-precision, data-driven learned struc-\nture that supports instant data insertion/deletion/modification.\nS2.We designed efficient bulk-loading (Section 4.2), updating\n(Section 4.3) algorithms for ICE, enabling ICE to train rapidly from\nmassive data and update quickly in dynamic scenes.\nS3.We designed an efficient CE algorithm based on ICE (Sec-\ntion 5.2). The core idea is to sample in the filtered latent space, i.e.,\nthe rank space. We also analyzed the unbiasedness and variance\nof the method as well as the probability of the algorithmâ€™s estima-\ntion exceeding the preset maximum Q-error when predicting the\ncardinality of low-cardinality queries (Section 5.3). By executing\nthese low-cardinality queries with a fast index scan, we can bound\nthe Q-error to any user-specified requirements.\nS4.Extensive experiments have demonstrated the superiority of\nICE (Section 6). ICE has achieved rapid updates and accurate esti-\nmation in multiple datasets and various dynamic scenes. Compared\nto SOTA real-time query-driven models, ICE boasts 2-3 orders of\nmagnitude higher accuracy, 4.7âˆ’6.9Ã—faster updates, and training\ntime expedited by up to 1-3 orders of magnitude.\nDue to space limitations, the scope of discussion in this work\nwill be limited to the multidimensional CE task within a single table.\n\nUpdateable Data-Driven Cardinality Estimator with Bounded Q-error Conferenceâ€™17, July 2017, Washington, DC, USA\nAdapting the multidimensional cardinality estimator from a single\ntable to multiple tables is straightforward. We can perform a similar\ntransformation like CardIndex [ 21], which samples from the full\nouter join of all tables and then builds a multidimensional index on\nthe materialized full outer join sample.\n2 Related Work\nModern cardinality estimators : Modern cardinality estimators\ncan be broadly categorized into two paradigms: query-driven [ 7,14,\n20] and data-driven[ 12,21,33]. Query-driven estimators employ\nneural networks like MLP [ 7], CNN [ 14], and Transformer [ 20] to\nbuild regression models that predict query cardinalities by learning\nmappings from query representations to true cardinalities. Two\nchallenges persist in this paradigm: data updates (necessitating re-\nacquiring training labels and retraining models after update [ 30])\nand workload shifts (unseen workload patterns in test queries com-\nparing to training data [ 33]). To tackle data updates, ALECE [ 20]\nintroduces additional histogram features and leverages attention\nmechanisms to learn correlations between query representations\nand histogram encodings. Since histogram features can be swiftly\nupdated, ALECE circumvents the need to re-acquire true cardi-\nnalities and retrain based on them, enhancing update speeds for\nquery-driven models. On the other hand, data-driven estimators\nlearn joint data distributions, utilizing statistical inference on Sum-\nProduct Network (SPN) [ 12] or sampling on deep model [ 33] to\nestimate cardinalities. The former often lags in estimation accu-\nracy due to assumptions of independence among data attributes.\nData-driven estimators are inherently robust against workload\nshifts [ 30,33]. However, efficiently updating data-driven models\nremains challenging: SPN requires reconstruction when new corre-\nlations appears [ 12], and deep models suffer from inefficient tuple\nrepresentation storage in neural networks, necessitating entire net-\nwork parameter updates via gradient descent [ 33]. CardIndex [ 21]\nmitigates update issues by stacking a lightweight AR network with\na learned index, delegating difficult small-cardinality queries to\nthe index to search, and reserving neural networks for estimating\nlarge-cardinalities. This reduces the network size and enhances\ntraining and update speed. Nevertheless, CardIndexâ€™s reliance on\nAR networks at its root node means that it cannot fundamentally\nresolve data update challenges.\nZ-ordered multidimensional index : To index multidimen-\nsional data, Z-ordering curves are commonly employed, mapping\ndata to one dimension for indexing [9, 13, 25, 28]. Handling range\nqueries based on Z-order involves calculating ğ‘ğ‘šğ‘–ğ‘›andğ‘ğ‘šğ‘ğ‘¥ for\nthe query box and scanning data within. However, this includes non-\nrelevant data, causing gaps. Tropf et al [ 28] proposes getBIGMIN\nandgetLITMAX methods to skip these gaps efficiently in ğ‘‚(ğ‘›)time,\nwhereğ‘›is the bit length. LMSFC [ 9] further optimizes Z-order index-\ning by switching bit order and leveraging optimal-1 split for range\nquery performance improvements based on historical workloads.\n3 Preliminary\nIn this section, we will introduce some basic concepts of CE and mul-\ntidimensional indexing (Section 3.1) and revisit the index structure\nfrom the perspective of CE (Section 3.2).3.1 Basic Concepts\nFor a relational table ğ‘‡containingğ‘tuples andğ‘šattributes{ğ´1,ğ´2,\n...,ğ´ğ‘š}, where each attribute is encoded using ğ›½binary bits, i.e.,\nğ´ğ‘–=Bğ‘–1Bğ‘–2...Bğ‘–ğ›½. A query predicate ğœƒcan be viewed as a func-\ntion that takes a tuple ğ‘¡âˆˆğ‘‡as input and returns ğœƒ(ğ‘¡)=1ifğ‘¡\nsatisfies the predicateâ€™s conditions, and ğœƒ(ğ‘¡)=0otherwise. The set\nof tuples in table ğ‘‡that satisfy the predicate ğœƒcomprises the result\nsetğ‘…={ğ‘¡âˆˆğ‘‡:ğœƒ(ğ‘¡)=1}. To expedite the retrieval of the result\nsetğ‘…, an index relies on an auxiliary structure. Furthermore, CE\nnecessitates the computation of the size of the result set, which is\ndenoted byğ‘ğ‘ğ‘Ÿğ‘‘(ğœƒ)=|ğ‘…|.\nThe Probability Density Function(PDF) of a tuple ğ‘¡reflects its\nproportion in the relational table ğ‘ƒğ·ğ¹(ğ‘¡)=ğ‘œ(ğ‘¡)/ğ‘whereğ‘œ(ğ‘¡)\nrepresents the frequency of ğ‘¡in tableğ‘‡. It is inherently linked to\nthe query selectivity ğ‘ ğ‘’ğ‘™(ğœƒ), Consequently,\nğ‘ ğ‘’ğ‘™(ğœƒ)=âˆ‘ï¸\nğ‘¡âˆˆğ‘‡ğœƒ(ğ‘¡)Ã—ğ‘ƒğ·ğ¹(ğ‘¡) (1)\nGiven a table ğ‘‡sorted under a specific bitwise ordering Î©, ex-\npressed as Î©=âŸ¨Bğ‘¥1,Bğ‘¥2,...,Bğ‘¥ğ‘šÃ—ğ›½âŸ©, the relationship ğ‘¡2<ğ‘¡1\nholds if and only if there exists a value of ğœ…âˆˆ[1,ğ‘šÃ—ğ›½]such that\nğ‘¡2.Bğ‘¥ğœ…<ğ‘¡1.Bğ‘¥ğœ…while for all ğ‘—<ğœ…, we haveğ‘¡2.Bğ‘¥ğ‘—=ğ‘¡1.Bğ‘¥ğ‘—.\nUnder such particular ordering Î©, and the rank of tuple ğ‘¡,ğ‘Ÿğ‘¡\nneeds to count all the tuples whose keys are smaller than ğ‘¡.\nğ‘Ÿğ‘¡=âˆ‘ï¸\nğ‘¡ğ‘–<ğ‘¡ğ‘œ(ğ‘¡ğ‘–)=âˆ‘ï¸\nğ‘¡ğ‘–<ğ‘¡ğ‘ƒğ·ğ¹(ğ‘¡ğ‘–)Ã—ğ‘ (2)\nTheCumulative Distribution Function(CDF) is the result of scal-\ning the rank of ğ‘¡byğ‘times, namely ğ¶ğ·ğ¹(ğ‘¡)=ğ‘Ÿğ‘¡/ğ‘.\nTo process multidimensional data more efficiently, we adopt the\nZ-order bit ordering convention Î©ğ‘, which is denoted as: Î©ğ‘=\nâŸ¨B11,B21,...,Bğ‘š1,B12,B22,...,Bğ‘š2,...B1ğ›½,B2ğ›½,...,Bğ‘šğ›½âŸ©. To sim-\nplify the notation, we use ğ‘›to replaceğ‘šğ›½in representing the total\ncoding length, set the ordering notation as Î©ğ‘=âŸ¨ğ‘1,ğ‘2,...ğ‘ğ‘›âŸ©,\nand denote tuple ğ‘¡under Z-ordering as ğ‘¡=(ğ‘§1,ğ‘§2,...ğ‘§ğ‘›).\n3.2 Revisiting Index Structures from CE\nPerspectives\nAn index is a precise learned model that takes the query key as the\ninput and efficiently predicts its CDF [ 15]. Theoretically, the PDF\nof a tuple is the derivative of its CDF (rank) as follows.\nğ‘ƒğ·ğ¹(ğ‘¡)=ğ‘‘\nğ‘‘ğ‘¡ğ¶ğ·ğ¹(ğ‘¡)=1\nğ‘Ã—ğ‘‘\nğ‘‘ğ‘¡ğ‘Ÿğ‘¡ (3)\nIn practice, we can avoid the differentiation operation to obtain\nthe PDF more ingeniously by utilizing an additional counter in leaf\nnodes of the index to maintain the frequency of tuples ğ‘œ(ğ‘¡). And\nwe useğ‘ƒğ·ğ¹(ğ‘¡)=ğ‘œ(ğ‘¡)/ğ‘to obtain the PDF.\nTherefore, not only limited to CardIndex [ 21], any index can\nimplement CDF-call and PDF-call within one point query.\nFrom the perspective of CE, compared with existing deep models,\nthe PDF obtained by index has at least two advantages over that\nobtained by deep learning models:\nS1.Accurate : The PDF information obtained through the index\nis lossless and will not cause prediction errors due to insufficient\nmodel training or too many distinct values in columns.\n\nConferenceâ€™17, July 2017, Washington, DC, USA Yingze Li, Xianglong Liu, Hongzhi Wang1, Kaixin Zhang, and Zixuan Wang\nIndex\nModelKey\nCDFOriginal\nTuplesCounter\no(t)\nPDF\nDeep\nModelKeyProgressive\nSampled\nTuples\nPDFUp-to-date\nOut-of-date\nWrongTuple status Deep Model \nBased CEIndex Model \nBased CE\nFigure 1: Deep network vs. index in obtaining PDF\nS2.Fresh : For updates in the database, we can directly perform\nphysical insert/delete/modification on the index, ensuring that the\nmodel is always up-to-date when outputting PDF for estimation.\nHowever, deep models require an additional and slow fine-tuning\nprocess to synchronize the model with the updated database.\n4 Index for Cardinality Estimation (ICE)\nWe observe that for updates, B+-tree and similar tree-shaped in-\ndexes can efficiently fine-tune their parameters within ğ‘™ğ‘œğ‘”(ğ‘)com-\nplexity, making them well-suited for handling massive dynamic mul-\ntidimensional data [ 10,31]. In this section, we elaborate on adapting\nthese prevalent structures for CE. The ICE structure is shown in Fig-\nure 2. It is similar to a B+-tree but maintains additional lightweight\ncounters to assist CE. In this section, we introduce the layout of ICE\nin Section 4.1 and describe the bulk-loading algorithm in Section 4.2.\nWe also elaborate on the support of ICE for real-time insertion, dele-\ntion, and modification in Section 4.3. Finally, we leverage ICE in\nSection 4.4 to achieve the bijection from the key space to the rank\nspace within ğ‘™ğ‘œğ‘”(ğ‘)time, facilitating the subsequent CE task.\n23\nNon-leaf \nNodeğğ®ğ¦\nKeys\nChild\n10\n376\n1327\n421Leaf \nNodeğğ®ğ¦\nKeys\nO(t)\nFigure 2: Layout of ICE\n4.1 Index Layout\nTo make the estimator achieve ğ‘™ğ‘œğ‘”(ğ‘)time update, we choose a\ntree-based index layout similar to the B+-tree [ 10,31]. We also\nmaintain two lightweight counters, ğ¶ğ‘ğ‘¢ğ‘š andğ‘œ(ğ‘¡)to guarantee\nthat we can implement a fast bidirectional mapping from the Z-\norder representation of a tuple (key space) to the rank of a tuple\n(rank space) in Section 4.4. We define these two counters as follows.\nTuple frequency counter ğ‘œ(ğ‘¡).For a given tuple ğ‘¡stored in the\nleaf node, we maintain a counter ğ‘œ(ğ‘¡)to get the frequency of tuple\nğ‘¡. Fromğ‘œ(ğ‘¡), we can easily derive the PDF of tuple ğ‘¡by the formula\nğ‘ƒğ·ğ¹(ğ‘¡)=ğ‘œ(ğ‘¡)/ğ‘whereğ‘is the total data size. This counter avoids\nadditional derivative operations or any neighboring scanning.Node cover counter ğ¶ğ‘ğ‘¢ğ‘š.This counter maintains how many\ntuples the current node will eventually cover at the leaf node. This\ncounter is defined recursively as follows. For a Non-leaf node\nğ‘0,ğ‘0.ğ¶ğ‘ğ‘¢ğ‘š =Ã\nğ‘ğ‘–âˆˆğ‘0.ğ¶â„ğ‘–ğ‘™ğ‘‘ğ‘ğ‘–.ğ¶ğ‘ğ‘¢ğ‘š , and for a leaf node ğ‘1,\nğ‘1.ğ¶ğ‘ğ‘¢ğ‘š=Ã\nğ‘¡ğ‘—âˆˆğ‘1.ğ¾ğ‘’ğ‘¦ğ‘ ğ‘¡ğ‘–.ğ‘œ(ğ‘¡). The purpose of maintaining this\ncounter is to convert the index structure into a bidirectional map-\nping between the tupleâ€™s rank and the tupleâ€™s key in ğ‘™ğ‘œğ‘”(ğ‘)time.\nMoreover, we hope that this counter can be properly isolated from\nthe index subtree that has been changed. This counter maintains\nthe consistency of the bidirectional mapping before and after the\nupdate and makes the update algorithm more efficient.\nApart from that, we designed the index structure similar to\nthe B+-tree for simplicity and efficiency. Despite the significant\nprogress in the learned index [ 15,31], we still adopt this \"retro\"\nindexing implementation. The reason for using a B+-tree-style\nstructure is that, with the help of the additional ğ¶ğ‘ğ‘¢ğ‘š counter\nand the point query mechanism of B+-tree, we can easily achieve\nbidirectional mapping between tupleâ€™s rank and representation in\nğ‘™ğ‘œğ‘”(ğ‘)time. This enables us to perform effective sampling within\nthe rank space. In contrast, the linear functions in the learned index\ncan only handle the injection from the tupleâ€™s key to their ranks. To\nsupport bidirectional mapping, we have to maintain an additional\nlearned index to learn the mapping from data ranks to their keys,\nwhich is redundant and inefficient.\n4.2 Bulk-loading\nInspired by existing indexes [ 5,10], we propose the bulk-loading\nalgorithm of ICE to achieve high-throughput training from massive\nmultidimensional data. The basic idea is to sort the tuples according\nto their Z-values, scan layer by layer from the bottom to the top,\nand learn the local parameters corresponding to each layer of nodes.\nThe algorithm is shown in Algorithm 1:\nAlgorithm 1 Bulk-loading.\nRequire: Tableğ‘‡; Node fanout number: ğ‘›ğ‘“\nEnsure: ICE structure ğ¼ğ¶ğ¸;\n1:ğ‘˜ğ‘’ğ‘¦ğ‘ =ğ‘†ğ‘œğ‘Ÿğ‘¡(ğ‘‡);\n2:â„“=0;ğ‘ =ğ‘˜ğ‘’ğ‘¦ğ‘ .ğ‘ ğ‘–ğ‘§ğ‘’();ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ğ‘  =ğœ™;\n3:whileğ‘ >1do âŠ²Bottom-up training\n4:ğ‘›ğ‘œğ‘‘ğ‘’ğ‘  =ğœ™;ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘œğ‘‘ğ‘’ =ğ‘ğ‘’ğ‘¤ğ‘ğ‘œğ‘‘ğ‘’();\n5: forğ‘˜âˆˆğ‘˜ğ‘’ğ‘¦ğ‘  do\n6: ifâ„“=0then âŠ²Leaf nodes\n7: ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘œğ‘‘ğ‘’.ğ¶ ğ‘ğ‘¢ğ‘š=ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘œğ‘‘ğ‘’.ğ¶ ğ‘ğ‘¢ğ‘š+ğ‘˜.ğ‘œ(ğ‘¡);\n8: else âŠ²Nonleaf nodes\n9: ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘œğ‘‘ğ‘’.ğ¶ ğ‘ğ‘¢ğ‘š=ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘œğ‘‘ğ‘’.ğ¶ ğ‘ğ‘¢ğ‘š+ğ‘˜.ğ¶ğ‘ğ‘¢ğ‘š ;\n10:ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘œğ‘‘ğ‘’.ğ‘ğ‘ğ‘ğ‘’ğ‘›ğ‘‘(ğ‘˜);\n11: ifğ‘ğ‘¢ğ‘Ÿğ‘ğ‘œğ‘‘ğ‘’.ğ‘ ğ‘–ğ‘§ğ‘’()â‰¥ğ‘›ğ‘“then\n12: ğ‘›ğ‘œğ‘‘ğ‘’ğ‘ .ğ‘ğ‘ğ‘ğ‘’ğ‘›ğ‘‘(ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘œğ‘‘ğ‘’);ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘œğ‘‘ğ‘’ =ğ‘ğ‘’ğ‘¤ğ‘ğ‘œğ‘‘ğ‘’();\n13:ğ‘˜ğ‘’ğ‘¦ğ‘ =[ğ‘›ğ‘œğ‘‘ğ‘’ğ‘ [ğ‘–].ğ‘¡ğ‘¢ğ‘ğ¹ğ‘–ğ‘Ÿğ‘ ğ‘¡,ğ‘–âˆˆ{0,1,...ğ‘ âˆ’1}];\n14:ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ğ‘ [â„“]=ğ‘›ğ‘œğ‘‘ğ‘’ğ‘  ;â„“++;ğ‘ =ğ‘˜ğ‘’ğ‘¦ğ‘ .ğ‘ ğ‘–ğ‘§ğ‘’();\n15:ğ¼ğ¶ğ¸=ğ‘›ğ‘’ğ‘¤ğ¼ğ¶ğ¸(ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ğ‘ );\n16:returnğ¼ğ¶ğ¸;\nWe first sort the tuples in table ğ‘‡under the Z-order (line 1) and\nthen initialize the variables (line 2). Subsequently, we train the\n\nUpdateable Data-Driven Cardinality Estimator with Bounded Q-error Conferenceâ€™17, July 2017, Washington, DC, USA\nICE model bottom-up (lines 3-14). Specifically, we maintain the\nlocal cumulative information of ğ¶ğ‘ğ‘¢ğ‘š layer by layer (lines 5-9) and\ninsert the keys from the lower layer into the nodes of the upper\nlayer (lines 10-12). Finally, we aggregate the layer-leveled trained\ninformation into the ICE model (line 15) and return it.\nComplexity analysis. Given that each node of ICE is scanned\nonly once during the bottom-up construction, the complexity of\nICEâ€™s bottom-up process (lines 3-14) is ğ‘‚(ğ‘), whereğ‘denotes\nthe size of the data. Considering that the complexity of sorting the\ndata in line 1 is in ğ‘‚(ğ‘Ã—ğ‘™ğ‘œğ‘”(ğ‘)), the overall time complexity of\nour construction algorithm is ğ‘‚(ğ‘Ã—ğ‘™ğ‘œğ‘”(ğ‘)), which is determined\nprimarily by the complexity of sorting. Meanwhile, the bottom-up\nconstruction guarantees the depth of ICE to be ğ‘‚(ğ‘™ğ‘œğ‘”(ğ‘)).\nCompared to the existing deep networkâ€™s training process, the\nbulk-loading process of ICE is efficient. Considering that the main\nbottleneck of deep network training lies in the inefficiency of grad-\nually iterating through small batches of data to compute gradients\nand perform backpropagation for the neural network. In contrast,\nthe lionâ€™s share of ICEâ€™s training time lies in the ğ‘‚(ğ‘Ã—ğ‘™ğ‘œğ‘”(ğ‘))\ncomplexity required for sorting multidimensional data. Specifically,\nif the input data is sorted, the bulk-loading algorithm mentioned\nabove will achieve ğ‘‚(ğ‘)performance.\n4.3 Index Maintenance\nIn this section, we describe instant tuple-leveled insertion, deletion,\nand modification on ICE within ğ‘™ğ‘œğ‘”(ğ‘)time.\n+X\n+X\n+X\n+X-Y\n-Y\n-Y\n-YDeletion\nModified \nNode\nUnmodified \nSubtree\nPointerLook-Up \nPath\nDetails Insertion\nFigure 3: Insertion and deletion of ICE\nInsertion and Deletion. For insertion and deletion, as Figure 3\nshows, we first recursively look up and locate the leaf nodes that\nrequire updating and then conduct updates on the leaf nodes. If the\nleaf node stores the key to update, we perform arithmetic addition\nor subtraction on the leaf nodeâ€™s counter ğ‘œ(ğ‘¡). If there is no available\nkey for insertion, we allocate a new slot for the corresponding data\nslot address of the leaf node. If the counter indicates one during\ndeletion, we remove such a slot from the leaf node. Subsequently,\nfor each node on the search path, we adjust the value of its child\nnode counter ğ¶ğ‘ğ‘¢ğ‘š . If a node reaches a state requiring adjustment,\nwe apply a strategy similar to that of B+-tree adjustment while\nmaintaining the nature of the counter.\nModification. For tuple-leveled modification, we split the op-\neration into one deletion to the old tuple and one insertion on the\nnew tuple. The deletion and insertion operations above ensure that\nnode updates can be accomplished within ğ‘™ğ‘œğ‘”(ğ‘)time, and the\nupdate to the counter is limited to the scale of ğ‘™ğ‘œğ‘”(ğ‘). Therefore,\nthe modification can be finished within ğ‘™ğ‘œğ‘”(ğ‘)time.4.4 Bijection between Keys and Ranks\nIn this section, we will discuss how to achieve a bidirectional map-\nping between the tuple representation under Z-order encoding(i.e.,\nindex key) and its corresponding rank value with the aid of ICE.\nThe key idea of Key2Rank mapping leverages the point query of a\nB+-tree, gradually accumulating the sizes of subtrees along the path\nof the point query to obtain the rank value of the key. Conversely,\nthe mapping process from rank space to key space is the reverse\nprocess of Key2Rank, utilizing the accumulated subtree sizes on\nthe scanning path to perform corresponding pruning.\nAlgorithm 2 Bijection between keys and ranks\nRequire: ICE structure ğ¼ğ¶ğ¸;\nEnsure: The bijection between tuple ğ‘¡â€™s keyğ‘˜ğ‘¡and rankğ‘Ÿğ‘¡;\n1:procedure Key2Rank (ğ¼ğ¶ğ¸,ğ‘˜ğ‘¡)âŠ²Key to rank space mapping\n2:ğ‘Ÿğ‘¡=0;ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘œğ‘‘ğ‘’ =ğ¼ğ¶ğ¸.ğ‘Ÿğ‘œğ‘œğ‘¡ ;\n3: while !ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘œğ‘‘ğ‘’.ğ‘–ğ‘ ğ‘™ğ‘’ğ‘ğ‘“ do\n4: forğ‘ğ‘–âˆˆğ‘ğ‘¢ğ‘Ÿğ‘ğ‘œğ‘‘ğ‘’.ğ¶â„ğ‘–ğ‘™ğ‘‘ do\n5: ifğ‘ğ‘–.ğ‘˜ğ‘’ğ‘¦ <ğ‘˜ğ‘¡then\n6: ğ‘Ÿğ‘¡=ğ‘Ÿğ‘¡+ğ‘ğ‘–.ğ¶ğ‘ğ‘¢ğ‘š ;\n7: else ifğ‘ğ‘–.ğ‘˜ğ‘’ğ‘¦=ğ‘˜ğ‘¡then\n8: ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘œğ‘‘ğ‘’ =ğ‘ğ‘–;\n9: else\n10: ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘œğ‘‘ğ‘’ =ğ‘ğ‘–.ğ‘ƒğ‘Ÿğ‘’ ;ğ‘Ÿğ‘¡=ğ‘Ÿğ‘¡âˆ’ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘œğ‘‘ğ‘’.ğ¶ ğ‘ğ‘¢ğ‘š ;\n11: forğ‘¡ğ‘–âˆˆğ‘ğ‘¢ğ‘Ÿğ‘ğ‘œğ‘‘ğ‘’.ğ¾ğ‘’ğ‘¦ğ‘  do\n12: ifğ‘¡ğ‘–.ğ‘˜ğ‘’ğ‘¦â‰¤ğ‘˜ğ‘¡then\n13: ğ‘Ÿğ‘¡=ğ‘Ÿğ‘¡+ğ‘¡ğ‘–.ğ‘œ(ğ‘¡);returnğ‘Ÿğ‘¡;\n14:procedure Rank2Key (ğ¼ğ¶ğ¸,ğ‘Ÿğ‘¡)âŠ²Rank to key space mapping\n15:ğ‘ğ‘¢ğ‘Ÿğ‘…ğ‘ğ‘›ğ‘˜ =0;ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘œğ‘‘ğ‘’ =ğ¼ğ¶ğ¸.ğ‘Ÿğ‘œğ‘œğ‘¡ ;\n16: while !ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘œğ‘‘ğ‘’.ğ‘–ğ‘ ğ‘™ğ‘’ğ‘ğ‘“ do\n17: forğ‘ğ‘–âˆˆğ‘ğ‘¢ğ‘Ÿğ‘ğ‘œğ‘‘ğ‘’.ğ¶â„ğ‘–ğ‘™ğ‘‘ do\n18: ifğ‘ğ‘¢ğ‘Ÿğ‘…ğ‘ğ‘›ğ‘˜+ğ‘ğ‘–.ğ¶ğ‘ğ‘¢ğ‘šâ‰¤ğ‘Ÿğ‘¡then\n19: ğ‘ğ‘¢ğ‘Ÿğ‘…ğ‘ğ‘›ğ‘˜ =ğ‘ğ‘¢ğ‘Ÿğ‘…ğ‘ğ‘›ğ‘˜+ğ‘ğ‘–.ğ¶ğ‘ğ‘¢ğ‘š ;\n20: else\n21: ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘œğ‘‘ğ‘’ =ğ‘ğ‘–;ğ‘ğ‘Ÿğ‘’ğ‘ğ‘˜ ;\n22:ğ‘˜ğ‘¡=0;\n23: forğ‘¡ğ‘–âˆˆğ‘ğ‘¢ğ‘Ÿğ‘ğ‘œğ‘‘ğ‘’.ğ¾ğ‘’ğ‘¦ğ‘  do\n24: ifğ‘ğ‘¢ğ‘Ÿğ‘…ğ‘ğ‘›ğ‘˜+ğ‘¡ğ‘–.ğ‘œ(ğ‘¡)<ğ‘Ÿğ‘¡then\n25: ğ‘ğ‘¢ğ‘Ÿğ‘…ğ‘ğ‘›ğ‘˜ =ğ‘ğ‘¢ğ‘Ÿğ‘…ğ‘ğ‘›ğ‘˜+ğ‘¡ğ‘–.ğ‘œ(ğ‘¡);ğ‘˜ğ‘¡=ğ‘¡ğ‘–.ğ‘˜ğ‘’ğ‘¦ ;\nreturnğ‘˜ğ‘¡;\nIn Algorithm 2, we show the bidirectional mapping algorithm\nbetween index keys and key ranks. Specifically, this algorithm can\nbe divided into two procedures: Key2Rank mapping (lines 1-13) and\nRank2Key mapping (lines 14-25). These two procedures share simi-\nlarities in their operations and can both be viewed as extensions of\npoint queries in B+-trees. For Key2Rank mapping, it progressively\naccumulates the sizes of the traversed subtrees (line 6 and line 13)\nalong the search path for a key ğ‘˜ğ‘¡(lines 3-13) in ICE, ultimately\nobtaining the rank value ğ‘Ÿğ‘¡corresponding to the target tuple. In\ncontrast, Rank2Key mapping searches for the target tupleâ€™s pres-\nence within the subtree associated with the current node based on\ntheğ¶ğ‘ğ‘¢ğ‘š information of that node and the accumulated rank sum\nğ‘ğ‘¢ğ‘Ÿğ‘…ğ‘ğ‘›ğ‘˜ traversed so far (lines 16-21). If the target falls within the\nrange, it switches to the corresponding subtree for further search\n\nConferenceâ€™17, July 2017, Washington, DC, USA Yingze Li, Xianglong Liu, Hongzhi Wang1, Kaixin Zhang, and Zixuan Wang\n(line 21). Given that the depth of the ICE index is ğ‘‚(ğ‘™ğ‘œğ‘”(ğ‘)), the\ntime complexity of both search procedures above is ğ‘‚(ğ‘™ğ‘œğ‘”(ğ‘))in\nterms of search complexity.\nModern AutoEncoder ICE structure\nDecoder\nLatent Space\nEncoder\nInOut\nğŸRank2Key\nRank Space\nKey2Rank\nInOut\nğŸ\nFigure 4: ICE vs. existing AutoEncoder\nDiscussions. From a modern perspective, the proposed bidirec-\ntional mapping transforms the traditional index structure into an\nAutoEncoder with 0 loss(see Figure 4). In other words, given a tuple\nto be queried, ICE can encode it into a representation between 1and\nğ‘. Meanwhile, given any positive integer from 1toğ‘in the rank\nspace, ICE can decode and rebuild it into a tuple currently in the\ndata table. Compared to existing AutoEncoders, ICEâ€™s latent space,\nnamely the rank space, ensures data compactness and integrity.\nThat is, every tuple in the relational table corresponds one-to-one\nto the integer ranks in the space, and this bijection will always\nbe consistent with the latest state of the relational table. These\nadvantages shall thereby facilitate subsequent CE algorithms.\n5 Cardinality Estimation\nIn this section, we leverage the extra counters and the bijections\nmaintained in the previous section to transform the ICE into a\nzero-errored AutoEncoder and sample in its latent space to accom-\nplish CE. As shown in Figure 5, we first pre-filter the coarse query\nspace by utilizing the data-skipping technique under Z-order. Subse-\nquently, we map the filtered query space into a compact rank space\nusing an index structure and perform efficient sampling within such\nrank space. Finally, we utilize the index to losslessly restore the\nsampled values in the rank space back into tuples, perform the final\nfiltering in conjunction with the query, and aggregate the results.\nIn Section 5.1, we discuss how to use the data-skipping technique\nto enhance the sampling efficiency. Then, in Section 5.2, we present\nour CE algorithm. Lastly, in Section 5.3, we conduct the analysis\non the sampling algorithm.\n5.1 Key Space Filtering\nAfter being mapped by a space-filling curve, a query box for range\nqueries will encompass numerous tuples irrelevant to the query.\nConsequently, sampling directly within the re-mapped query box\nwould result in low efficiency and large estimation errors. Therefore,\nit is necessary to pre-filter the original sampling space.\nFurthermore, we observe that when handling range queries with\na multidimensional index based on a space-filling curve, a common\npractice is to leverage the inherent properties of the curve to re-\ncursively or iteratively subdivide the query box before or during\nthe query execution. This approach enables the \"pruning\" of points\n1.Query \nInput2.Key Space\nFilter3.Key 2 Rank \nMapping\n4.Rank Space\nSampling5.Rank 2 Key\nMapping6.Aggregate\nResultsQuery\nOriginal Key\nSpaceQuery\nFiltered Key\nSpace\nKey\nRank ğŸğŸğŸğŸğŸğŸğŸğŸ”ğŸ\n30 60 90Sampled \nRanksKey\n19\n59\n61Boundary \nKeysRank\nğ’•ğŸR1.Low\nR1.Up\nR2.Low\nR2.Up15\n21\n56\n65R1\nR2\nğ’•ğŸAggregateğğ«ğâˆ’ğğ«ğ«=ğ’’ğ’ƒâ‰¤ğŸâˆ’ğœ?Estimation Result\nTrueHybrid Estimation\nFalse\nğ’•ğŸFigure 5: Overview of ICEâ€™s CE process\noutside the query box, thereby reducing unnecessary scan over-\nheads and enhancing query execution efficiency. Consequently, a\nstraightforward intuition arises: Can we draw inspiration from\nthe data-skipping techniques employed in multidimensional\nindexing execution [ 9,25] to narrow the sampling space and\nenhance sampling efficiency? Thus, this section will discuss how\nwe can improve sampling efficiency by using data-skipping tech-\nniques commonly used in multidimensional indexing.\nAlgorithm 3 Recursive filtering.\nRequire: Query boxğ‘„, Current recursive depth ğ‘‘.\nEnsure: A list of filtered query regions ğ¿;\n1:procedure RecursiveFiltering (ğ‘„,ğ‘‘)\n2: ifğ‘‘=0then âŠ²Initialize\n3:ğ¿=[];\n4: ifğ‘‘â‰¥ğ‘‘ğ‘šğ‘ğ‘¥then âŠ²Reached maximum depth\n5:ğ¿.ğ‘ğ‘ğ‘ğ‘’ğ‘›ğ‘‘(ğ‘„);\n6: else âŠ²Separate and filter\n7:ğ‘=ğ‘“ğ‘–ğ‘›ğ‘‘ğ‘†ğ‘’ğ‘ğ‘ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ƒğ‘œğ‘–ğ‘›ğ‘¡ (ğ‘„);\n8:ğ‘„1,ğ‘„2=ğœ™;\n9: ifğ‘âˆˆğ‘„then\n10: ğ‘„1.ğ¿ğ‘œğ‘¤ =ğ‘„.ğ¿ğ‘œğ‘¤ ;ğ‘„1.ğ‘ˆğ‘=ğ‘;\n11: ğ‘„2.ğ¿ğ‘œğ‘¤ =ğ‘;ğ‘„2.ğ‘ˆğ‘=ğ‘„.ğ‘ˆğ‘ ;\n12: else\n13: ğ‘„1.ğ¿ğ‘œğ‘¤ =ğ‘„.ğ¿ğ‘œğ‘¤ ;ğ‘„1.ğ‘ˆğ‘=ğ‘”ğ‘’ğ‘¡ğ¿ğ¼ğ‘‡ğ‘€ğ´ğ‘‹(ğ‘,ğ‘„);\n14: ğ‘„2.ğ¿ğ‘œğ‘¤ =ğ‘”ğ‘’ğ‘¡ğµğ¼ğºğ‘€ğ¼ğ‘(ğ‘,ğ‘„);ğ‘„2.ğ‘ˆğ‘=ğ‘„.ğ‘ˆğ‘ ;\n15:ğ‘…ğ‘’ğ‘ğ‘¢ğ‘Ÿğ‘ ğ‘–ğ‘£ğ‘’ğ¹ğ‘–ğ‘™ğ‘¡ğ‘’ğ‘Ÿğ‘–ğ‘›ğ‘” (ğ‘„1,ğ‘‘+1);\n16:ğ‘…ğ‘’ğ‘ğ‘¢ğ‘Ÿğ‘ ğ‘–ğ‘£ğ‘’ğ¹ğ‘–ğ‘™ğ‘¡ğ‘’ğ‘Ÿğ‘–ğ‘›ğ‘” (ğ‘„2,ğ‘‘+1);\nWe recursively implement the idea above in Algorithm 3. Given\na query box ğ‘„and the maximum search depth ğ‘‘ğ‘šğ‘ğ‘¥, we recursively\npartition and filter the current query box. We initialize the filtered\nlistğ¿at depth 0 (lines 2-3). If the current search depth reaches the\nmaximum depth ğ‘‘ğ‘šğ‘ğ‘¥, we add the current subdivided query to the\nregions list ğ¿(lines 4-5). Otherwise, we progressively recursively\npartition and filter using the data-skipping tricks of space-filling\ncurves (lines 6-16). We find the appropriate partition point ğ‘(line 7)\nand initialize the query box ğ‘„1,ğ‘„2for further recursive searching\n(line 8). If the partition point ğ‘falls within the query box, we use ğ‘\nas the left and right endpoints of ğ‘„1,ğ‘„2(lines 9-11); otherwise, we\nutilize theğ‘”ğ‘’ğ‘¡ğµğ¼ğºğ‘€ğ¼ğ‘ andğ‘”ğ‘’ğ‘¡ğ¿ğ¼ğ‘‡ğ‘€ğ´ğ‘‹ methods from the Z-order\n\nUpdateable Data-Driven Cardinality Estimator with Bounded Q-error Conferenceâ€™17, July 2017, Washington, DC, USA\nspace-filling curve[ 25,28] to skip the irrelevant areas and filter the\ncurrent query box into new sub-region ğ‘„1andğ‘„2(lines 12-14).\nFinally, we conduct a further recursive search on refined space\n(lines 15-16). Given the data encoding length ğ‘›, the above algorithm\nperforms pre-filtering within ğ‘‚(ğ‘›Ã—2ğ‘‘ğ‘šğ‘ğ‘¥)time.\nNote that the ğ‘“ğ‘–ğ‘›ğ‘‘ğ‘†ğ‘’ğ‘ğ‘ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ƒğ‘œğ‘–ğ‘›ğ‘¡ function is employed in line 7\nof our algorithm, which selects a separation point within the range\nfromğ‘„.ğ¿ğ‘œğ‘¤ toğ‘„.ğ‘ˆğ‘ to partition the subquery region. Regard-\ning how to choose such a separation point, existing works adopt\ndifferent approaches. CardIndex[ 21] directly selects the midpoint\nbetweenğ‘„.ğ¿ğ‘œğ‘¤ andğ‘„.ğ‘ˆğ‘ , whereas LMSFC[ 9] formulates the se-\nlection as an optimization procedure, namely the \"optimal 1-split\".\nThat is, under the condition of fixing the values of other columns, a\nsplit value is selected only on a fixed column at a time to maximize\nthe length of the skipped gaps. Through experiments, we find out\nthat the \"optimal 1-split\" is prone to falling into local optima dur-\ning recursive selection, resulting in inefficient tuples filtering and\nrelatively large estimation errors during sampling. Consequently,\nwe ultimately choose to select the midpoint of the query box.\n5.2 Rank Space Sampling\nIn this section, we will devise a CE algorithm based on sampling\nin the rank space, integrating the key space filtering technique\ndesigned in the previous sections with the bijective mapping tech-\nnique from the key space to the rank space learned by the index.\nSampling in the rank space is primarily due to its compactness,\nwhere each rank corresponds to a unique tuple. In contrast, the\nkey space is extremely sparse, and two adjacent tuples in the rank\nspace may exhibit significant differences in their keys. Considering\nthe following example:\nExample. Suppose we have a list of data: [210,220,230,240]. If\nwe need to sample from this sorted list to estimate the number of\ntuples less than 220, sampling in the key space would require se-\nlecting two tuples less than 220from a space of 240, which is highly\ninefficient. In contrast, the rank space of the aforementioned list\nis[1,2,3,4]. When sampling the rank space, we can directly select\ntwo tuples from a set with size equals four. This greatly enhances\nthe sampling efficiency.\nThe CE algorithm is illustrated in Algorithm 4. Firstly, variable\ninitialization is performed in line 1, followed by recursively filtering\nthe query region within the key space (line 2). After obtaining the\nfiltered key space within list ğ¿, we encode each sub-region into the\nrank space and calculate the total length of these regions (lines 3-4).\nSubsequently, we sample ğ‘samples from these sub-regions (line 5).\nDuring the aggregation process (lines 7-10), we map these sam-\nples through the Rank2Key transformation (line 8), decoding them\nback to their tuple representations, i.e., the key values ğ‘˜ğ‘–. Then, we\ncheck whether the tuples are within the query box (line 9); if so,\nthe counter ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡ is incremented (line 10). We estimate the results\nin line 11 and calculate the probability that the estimation is out\nof bound (line 12). When the cardinality of the query is too small,\ninsufficient sampling cannot effectively cover the query region, and\nthe estimated result is likely to have a high probability of significant\nerrors (line 13). We then utilize the index execution (i.e., hybrid\nestimation) to conduct the last-mile search for this low-cardinality\nquery (line 14). When getting the binomial distribution probabilityAlgorithm 4 Index-based CE\nRequire: Query boxğ‘„; ICE structure ğ¼ğ¶ğ¸; Sample budget ğ‘; Q-\nerror bound ğ‘ğ‘; Confidence ğ‘;\nEnsure: Estimated cardinality ğ‘’ğ‘ ğ‘¡;\n1:ğ¿=ğœ™;\n2:ğ‘…ğ‘’ğ‘ğ‘¢ğ‘Ÿğ‘ ğ‘–ğ‘£ğ‘’ğ¹ğ‘–ğ‘™ğ‘¡ğ‘’ğ‘Ÿğ‘–ğ‘›ğ‘” (ğ‘„,0); âŠ²Filter key space\n3:ğ‘…=ğ¾ğ‘’ğ‘¦ 2ğ‘…ğ‘ğ‘›ğ‘˜(ğ¿); âŠ²Key to rank encoding\n4:ğ‘Ÿğ‘†ğ‘¢ğ‘š =ğ‘….ğ‘ ğ‘¢ğ‘š();\n5:ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘  =ğ‘†ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ¹ğ‘Ÿğ‘œğ‘šğ‘…ğ‘ğ‘›ğ‘˜ğ‘  (ğ‘…,ğ‘);âŠ²Rank space sampling\n6:ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡ =0;\n7:forğ‘ ğ‘–âˆˆğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘  do âŠ²Aggregation\n8:ğ‘˜ğ‘–=ğ‘…ğ‘ğ‘›ğ‘˜ 2ğ¾ğ‘’ğ‘¦(ğ‘ ğ‘–); âŠ²Rank to Key decoding\n9: ifğ¼ğ‘›ğ‘„ğ‘¢ğ‘’ğ‘Ÿğ‘¦ğµğ‘œğ‘¥(ğ‘˜ğ‘–,ğ‘„)then\n10:ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡ =ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡+1;\n11:ğ‘’ğ‘ ğ‘¡=(ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡/ğ‘)Ã—ğ‘Ÿğ‘†ğ‘¢ğ‘š ;\n12:ğ‘ƒ=ğ¶ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡\nğ‘’ğ‘ ğ‘¡Â·ğ‘ğ‘Ã—(1âˆ’ğ‘\nğ‘Ÿğ‘†ğ‘¢ğ‘š)ğ‘’ğ‘ ğ‘¡Â·ğ‘ğ‘âˆ’ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡Ã—(ğ‘\nğ‘Ÿğ‘†ğ‘¢ğ‘š)ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡;\n13:ifğ‘ƒ>(1âˆ’ğ‘)then âŠ²Estimates is prone to big errors\n14:ğ‘’ğ‘ ğ‘¡=ğ‘…ğ‘ğ‘›ğ‘”ğ‘’ğ‘„ğ‘¢ğ‘’ğ‘Ÿğ‘¦(ğ¼ğ¶ğ¸,ğ‘„); âŠ²Hybrid estimation\n15:returnğ‘’ğ‘ ğ‘¡\nvalue in line 12, we use Gaussian approximation for simplified cal-\nculation when ğ‘’ğ‘ ğ‘¡Â·ğ‘ğ‘is greater than 20. In Section 5.3, we prove\nthat such last mile search could bound the Q-error within ğ‘ğ‘with\na confidence of ğ‘.\nThe complexity of the Algorithm 4 is ğ‘‚(ğ‘Ã—ğ‘™ğ‘œğ‘”(ğ‘)), whereğ‘\nis the input sampling budget, and ğ‘is the total size of the data.\nBecause we perform once point query on the index for each sample\npoint to decode from the rank space to the data representation.\nConditional Generation ICE Cardinality Estimation\nDecoder\nLatent Space\nInput:\nA Blue CatRank2Key\nRank Space\nInput:\nSelect count(*)Estimated\nCardinalityProjectSample\nFilterSampleRescale\nFigure 6: ICEâ€™s sampling, a generative model perspective\nDiscussions. From a generative model perspective(see Figure 6),\nthe above sampling algorithm offers a more profound understand-\ning that transcends simple sampling with replacement. Considering\nthe discussions in Section 4.4, ICE can be viewed as an AutoEn-\ncoder that transforms representations from the key space to its\nlatent space, i.e., the rank space. Consequently, in Algorithm 4, the\nrecursive filtering at line 2 essentially projects the query conditions\ninto the ICEâ€™s latent space, while lines 6-11 employ the Rank2Key\nmapping to achieve conditional generation within this latent space.\nThis conditional generation paradigm is widely adopted in com-\nputer vision fields [ 19,29,35], where, for instance, the semantic\ndescription of a blue cat is projected into the latent space, and a\ndecoder subsequently samples from the projected latent space and\ngenerates an image of a blue cat based on this subspace. Similar\ntechniques have also been applied to databases for controlled tu-\nple generation [ 22]. Compared to existing conditional generation\n\nConferenceâ€™17, July 2017, Washington, DC, USA Yingze Li, Xianglong Liu, Hongzhi Wang1, Kaixin Zhang, and Zixuan Wang\ntechniques, the advantage of CE based on ICE lies in the nature of\nits latent space, the rank space, which is low-dimensional, ordered,\nand compact. The size of the conditional subspace ( ğ‘Ÿğ‘†ğ‘¢ğ‘š ) projected\nby the query in the rank space is straightforward to compute. We\nsimply need to incrementally accumulate the differences in ranks\nbetween interval endpoints. This enables us to directly examine\nthe number of generated tuples that satisfy the query conditions\nand rescale this count to obtain the predicted cardinality.\n5.3 Analysis of ICE Sampling Algorithm\nNext, we will prove the unbiasedness of ICEâ€™s sampling algorithm\n(Algorithm 4) and give its variance analysis. Finally, we will prove\nthe correctness of our Q-error bounding technique.\nTheorem 5.1. Given a query ğ‘„, the estimation result of ICEâ€™s\nsampling algorithm, ğ‘’ğ‘ ğ‘¡, is unbiased, i.e. ğ¸[ğ‘’ğ‘ ğ‘¡]=ğ‘ğ‘ğ‘Ÿğ‘‘(ğ‘„).\nProof. (Sketch): Regarding Algorithm 4â€™s second line, the re-\ncursive filtering does not exclude tuples in Q, hence preserving the\nestimated cardinality. ICEâ€™s bidirectional mapping between rank\nand key spaces ensures a one-to-one correspondence between tu-\nples, ensuring true and predicted cardinalities align in both spaces.\nMeanwhile, the hybrid estimation at line 14 of the Algorithm 4\nretrieves the true cardinality for low-cardinality queries, thus not\naffecting the unbiasedness. Therefore, we attempt to prove that the\nresults of sampling and aggregation in the rank space in lines 5-11\nof Algorithm 4 are unbiased as follows.\nIn the rank space, we perform sampling with replacement. We\nuse the variable ğ‘’ğ‘–=1to indicate the event that a single sample\nfalls within the query box. The probability of this event is ğ‘ƒğ‘Ÿ(ğ‘’ğ‘–=\n1)=ğ‘ğ‘ğ‘Ÿğ‘‘(ğ‘„)/ğ‘Ÿğ‘†ğ‘¢ğ‘š , whereğ‘Ÿğ‘†ğ‘¢ğ‘š is the total size of the filtered\nrank space (Line 4 Algorithm 4). Therefore, ğ¸[ğ‘’ğ‘ ğ‘¡]=ğ¸[ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡/ğ‘Ã—\nğ‘Ÿğ‘†ğ‘¢ğ‘š]=ğ‘Ÿğ‘†ğ‘¢ğ‘š\nğ‘Ã—ğ¸[Î£ğ‘\nğ‘–=1ğ‘’ğ‘–]=ğ‘Ÿğ‘†ğ‘¢ğ‘š\nğ‘Ã—\u0010\nğ‘Ã—ğ‘ğ‘ğ‘Ÿğ‘‘(ğ‘„)\nğ‘Ÿğ‘†ğ‘¢ğ‘š\u0011\n=ğ‘ğ‘ğ‘Ÿğ‘‘(ğ‘„). In\nsummary, our estimation algorithm is unbiased.\nâ–¡\nIn order to give the derivation of the variance, we now define the\nefficiency of a multidimensional index range query. The filtering\nefficiencyğœ‚when an index proceeds a range query ğ‘„is defined\nas the cardinality of query ğ‘„divided by the tuples scanned when\nexecutingğ‘„, i.e.ğœ‚=ğ‘ğ‘ğ‘Ÿğ‘‘(ğ‘„)\nTuples scanned.\nFrom the above formula, it is not difficult to find that the least\nefficient way to execute a query is to scan the entire table, with an\nefficiency of ğœ‚ğ‘ ğ‘ğ‘ğ‘›=ğ‘ğ‘ğ‘Ÿğ‘‘(ğ‘„)/ğ‘, whereğ‘is the full table size. An\nefficient multidimensional index will try to improve the efficiency\nas much as possible, making ğœ‚as close to 1 as possible. Therefore,\nsimilar concepts to ğœ‚are often used as a loss function for optimizing\nmultidimensional indexes [ 6,9,23]. Based on the above concepts,\nwe derive ICEâ€™s estimationâ€™s variance:\nTheorem 5.2. The variance of the estimation result, ğ‘‰ğ‘ğ‘Ÿ[ğ‘’ğ‘ ğ‘¡]â‰¤\n1\nğ‘Ã—ğ‘ğ‘ğ‘Ÿğ‘‘(ğ‘„)2Ã—(1\nğœ‚ğ¼âˆ’1), whereğ‘is the sampling budget and ğœ‚ğ¼is the\nefficiency when the range query ğ‘„is processed on ICE. When hybrid\nestimation is turned off, the inequality can be an equality.\nProof. (Sketch): We first consider the case when hybrid estima-\ntion is turned off. Considering Theorem 5.1, our further analysis\nis still carried out in the rank space. Flag variable ğ‘’ğ‘–indicates theevent that a single sample ğ‘ ğ‘–falls within the query box, and with\na probability of ğ‘ƒğ‘Ÿ(ğ‘’ğ‘–=1)=ğ‘ğ‘ğ‘Ÿğ‘‘(ğ‘„)/ğ‘Ÿğ‘†ğ‘¢ğ‘š . Since ICE can take\na proactive skipping strategy that scans every tuple within the\nfiltered rank space, the efficiency ğœ‚ğ¼of ICE when executing query\nğ‘„is alsoğ‘ğ‘ğ‘Ÿğ‘‘(ğ‘„)\nğ‘Ÿğ‘†ğ‘¢ğ‘š. We obtain that ğ‘ƒğ‘Ÿ(ğ‘’ğ‘–=1)=ğœ‚ğ¼. In the process of\nsampling in the rank space, the aggregation result count follows a\nbinomial distribution ğµ(ğ‘,ğœ‚ğ¼). Consequently, the variance of ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡\nis given byğ‘‰ğ‘ğ‘Ÿ[ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡]=ğ‘Ã—ğœ‚ğ¼Ã—(1âˆ’ğœ‚ğ¼). Since the final estimated\nresultğ‘’ğ‘ ğ‘¡is scaled by a factor of ğ‘Ÿğ‘†ğ‘¢ğ‘š/ğ‘=ğ‘ğ‘ğ‘Ÿğ‘‘(ğ‘„)/ğœ‚ğ¼fromğ‘ğ‘œğ‘¢ğ‘›ğ‘¡ ,\nthe variance of the result ğ‘’ğ‘ ğ‘¡when hybrid estimation turned off is:\nğ‘‰ğ‘ğ‘Ÿ[ğ‘’ğ‘ ğ‘¡]=\u0012ğ‘ğ‘ğ‘Ÿğ‘‘(ğ‘„)\nğ‘Ã—ğœ‚ğ¼\u00132\nÃ—ğ‘‰ğ‘ğ‘Ÿ[ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡]=1\nğ‘Ã—ğ‘ğ‘ğ‘Ÿğ‘‘(ğ‘„)2Ã—\u00121\nğœ‚ğ¼âˆ’1\u0013\nFinally, when the hybrid estimation is enabled, we can execute\nthose potentially incorrectly estimated low-cardinality queries via\nindex search to obtain their true cardinalities without affecting the\nestimations of high-cardinality queries. This results in a reduction\nof the overall variance, which proves the inequality.\nâ–¡\nFrom the above variance formula, it is evident that there are two\nfactors independent of the query, the sampling size ğ‘and the index\nexecution efficiency ğœ‚ğ¼. To achieve a more precise estimation, we\nneed to make the variance near 0. Based on Theorem 5.2, we have\ntwo approaches of increasing the sampling budget to make the term\n1/ğ‘approach zero (A1), and improving the index efficiency to make\n(1/ğœ‚ğ¼âˆ’1)approach zero (A2). We can improve the efficiency of the\nlatter approach by increasing the recursive search depth ğ‘‘ğ‘šğ‘ğ‘¥ in\nAlgorithm 3. These theorems connect two widely studied problems\nin the field of estimation and index, as A1is extensively studied\nin the field of CE [ 8,18], andA2is a focus of index optimization\nresearch [6, 9, 23].\nNext, in Theorem 5.3, we will prove that, without the hybrid\nestimation in line 14 of Algorithm 4, the probability that ICE un-\nderestimates the queryâ€™s cardinality, thereby resulting in a Q-error\nofğ‘ğ‘, equals the probability calculated in line 12 of Algorithm 4.\nTheorem 5.3. When hybrid estimation turned off, ğ‘ƒğ‘Ÿ(ğ‘„âˆ’ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ =\nğ‘ğ‘)=ğ¶ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡\nğ‘’ğ‘ ğ‘¡Â·ğ‘ğ‘Ã—(1âˆ’ğ‘\nğ‘Ÿğ‘†ğ‘¢ğ‘š)ğ‘’ğ‘ ğ‘¡Â·ğ‘ğ‘âˆ’ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡Ã—(ğ‘\nğ‘Ÿğ‘†ğ‘¢ğ‘š)ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡, whereğ‘,\nğ‘Ÿğ‘†ğ‘¢ğ‘š ,ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡ are the intermediate results in the Algorithm 4.\nProof. (Sketch): We derive the above theorem by examining\nhow the points in the query box are distributed within the sam-\npling pool. Given that ğ‘’ğ‘ ğ‘¡is underestimated by a factor of ğ‘ğ‘, we\nknow that the true cardinality is ğ‘ğ‘ğ‘Ÿğ‘‘=ğ‘ğ‘Ã—ğ‘’ğ‘ ğ‘¡. Since the proba-\nbility of a single sample point falling into the sampling pool during\nsampling isğ‘/ğ‘Ÿğ‘†ğ‘¢ğ‘š , the event that ğ‘’ğ‘ ğ‘¡has a Q-error of ğ‘ğ‘will follow\na binomial distribution ğµ(ğ‘ğ‘Â·ğ‘’ğ‘ ğ‘¡,ğ‘\nğ‘Ÿğ‘†ğ‘¢ğ‘š), therefore,ğ‘ƒ(ğ‘„âˆ’ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ =\nğ‘ğ‘)=ğ¶ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡\nğ‘ğ‘Â·ğ‘’ğ‘ ğ‘¡Ã—(1âˆ’ğ‘\nğ‘Ÿğ‘†ğ‘¢ğ‘š)ğ‘ğ‘Â·ğ‘’ğ‘ ğ‘¡âˆ’ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡Ã—(ğ‘\nğ‘Ÿğ‘†ğ‘¢ğ‘š)ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡. â–¡\nThe above theorem provides us with the idea that after obtaining\nthe sampling results in ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡ , we can utilize some intermediate\ninformation(e.g., ğ‘,ğ‘Ÿğ‘†ğ‘¢ğ‘š ) from the sampling to calculate the prob-\nabilityğ‘ƒthat the estimated result has a Q-error reaching the preset\nthresholdğ‘ğ‘. The user can specify the maximum allowable Q-error\nğ‘ğ‘and the probability confidence ğ‘. Ifğ‘ƒexceeds 1âˆ’ğ‘, it indicates\nthat the cardinality of the query is too small, and sampling cannot\n\nUpdateable Data-Driven Cardinality Estimator with Bounded Q-error Conferenceâ€™17, July 2017, Washington, DC, USA\nobtain an accurate estimate. We will then perform an index execu-\ntion (i.e., hybrid estimation) for such small-cardinality queries to\nobtain the true cardinality result, thereby ensuring that the model\ndoes not make estimates exceeding the maximum Q-error.\n6 Experiments\nIn this section, we attempt to answer the following questions via\nour experiments.\n1. Compared with the state-of-the-art cardinality estimators,\nunder various kinds of dynamic and static environments, how does\nICE perform regarding the estimation accuracy and inference time?\nIs the estimation robust under workload drifting? (Section 6.2)\n2. Compared with the state-of-the-art cardinality estimators,\nhow long does it take to train and update an ICE from massive\ndynamic multidimensional data? How much space does it consume?\n(Section 6.3)\n3. How does the depth of the recursive filtering and the sampling\nbudget affect the estimation accuracy of ICE? How will the selection\nof different split strategies affect the estimation accuracy? Does\nthe experimental result of Q-error bounding match the theory and\nbound the Q-error? (Section 6.4)\n6.1 Experimental Setup\nDatasets. We use three real-world datasets for experimental study\non CE tasks. We opt to conduct experiments on these datasets as\neach of them has been utilized in the assessment of at least one\nprior study in the fields of CE [21, 30, 33].\n(1)Power [3]: An electric power consumption data, which owns\na large domain size in all attributes (each â‰ˆ2M). Our snapshot\ncontains 2,049,280 tuples with 6 columns.\n(2)DMV [2]: A real-world dataset consisting of vehicle registra-\ntion information in New York. We use the following 11 columns\nwith widely differing data types and domain sizes (the numbers are\nin parentheses): record type (4), reg class(75), state (89), county (63),\nbody type (59), fuel type (9), valid date (2101), color (225), sco ind\n(2), sus ind (2), rev ind(2). Our snapshot contains 12,300,116 tuples.\n(3)OSM [1]: Real-world geographical data. We use the dataset of\nCentral America from OpenSteetMarket and select two columns of\nlatitude and longitude. This dataset has 80M tuples(1.8GB in size).\nOur snapshot contains 80,000,000 tuples with 2 columns.\nWorkloads. For each dataset, we adopt the conventions in the lit-\nerature [ 20] and create three different dynamic types of workloads,\neach of which is a random mix of insertions, deletions, modifica-\ntions, and query statements. Each insertion/deletion/modification\nstatement only influences one tuple. The division of the query train-\ning set and test set is consistent with the literature [ 20]. Finally, we\nuniformly mix queries and data update operations as follows:\nStatic :#ğ‘–ğ‘›ğ‘ ğ‘’ğ‘Ÿğ‘¡ : #ğ‘‘ğ‘’ğ‘™ğ‘’ğ‘¡ğ‘’ : #ğ‘šğ‘œğ‘‘ğ‘–ğ‘“ğ‘¦ =0 : 0 : 0\nInsert-Heavy :#ğ‘–ğ‘›ğ‘ ğ‘’ğ‘Ÿğ‘¡ : #ğ‘‘ğ‘’ğ‘™ğ‘’ğ‘¡ğ‘’ : #ğ‘šğ‘œğ‘‘ğ‘–ğ‘“ğ‘¦ =2 : 1 : 1\nUpdate-Heavy :#ğ‘–ğ‘›ğ‘ ğ‘’ğ‘Ÿğ‘¡ : #ğ‘‘ğ‘’ğ‘™ğ‘’ğ‘¡ğ‘’ : #ğ‘šğ‘œğ‘‘ğ‘–ğ‘“ğ‘¦ =1 : 1 : 2\nRegarding the construction of updating tuples, literature [ 20]\ndid not mention its construction process. But this is crucial as two\nscenarios may arise where the old model continues to perform\nwell [ 30], rendering update unnecessary: 1) All updating tuples\nfall outside the scope of existing queries; 2) Updates are evenly\n/s80/s111/s119/s101/s114 /s68/s77/s86 /s79/s83/s77/s49/s48/s45/s57/s49/s48/s45/s56/s49/s48/s45/s55/s49/s48/s45/s54/s49/s48/s45/s53/s49/s48/s45/s52/s49/s48/s45/s51/s49/s48/s45/s50/s49/s48/s45/s49/s49/s48/s48/s83/s101/s108/s101/s99/s116/s105/s118/s105/s116/s121\n/s32/s58/s83/s116/s97/s116/s105/s99\n/s32/s58/s73/s110/s115/s45/s72/s101/s97/s118/s121\n/s32/s58/s85/s112/s100/s45/s72/s101/s97/s118/s121Figure 7: Distribution of workload selectivity (Sampled 10%\nfrom the query workloads)\ndistributed across the dataset, resulting in a new data distribution\nthat is close to the existing one.\nTo address these challenges, we draw inspiration from PACE [ 34]\nby adopting an adversarial approach. We first generate the query\nboxes and their cardinalities of all the original old data. Given the\nknown query boxes and the cardinalities of the original dataset, we\nselect a small proportion (20%) of updating tuples based on the orig-\ninal data to degrade the performance of the old model. Our strategy\nfor selecting inserting tuples is weighted sampling from the old ta-\nble with replacement, assigning tuples selected by low cardinalities\nqueries with a higher weight. Specifically, we picked 10% of the\nold query boxes as ğ‘Šand formulate our update weight as ğ‘¤(ğ‘¡)=Ã\nğ‘„ğ‘–âˆˆğ‘Šğœƒ(ğ‘¡âˆˆğ‘„ğ‘–)Ã—ğ‘šğ‘–ğ‘›(1\nğ‘ ğ‘’ğ‘™(ğ‘„ğ‘–),105). It will maximize the proba-\nbility of tuples being repeatedly selected in low-cardinality queries\nas much as possible. Additionally, a threshold of 105is set to prevent\noversampling of the same type of tuples. For deletion operations, we\nemploy uniform sampling without replacement. For update opera-\ntions, we decompose them into separate delete and then insert. We\ngenerated 2048 test queries for each dataset and distributed them\nuniformly among data modification operations. A single query is\ncombined with the modification operation before it and is called\na batch. For the query boxes of the three datasets, our cardinality\ndistribution before and after modification is shown in Figure 7:\nCompetitors. We choose the following baseline competitors.\n(1)Naru [33]: A data-driven AR network that uses progressive\nsampling to estimate the cardinality.\n(2)Sample : This approach samples several tuples in memory for\nCE. The sampling budget is set at 1/1000 of the original data size.\n(3)DeepDB [12]: A data-driven method that uses SPN to estimate\ncardinality.\n(4)MSCN [14]: A query-driven method that uses a multi-set\nconvolutional network.\n(5)ALECE [20]: The state-of-the-art query-driven model that\nuses a transformer to learn query representation and histogram\nfeatures to cardinality. It can quickly support data updates.\n(6)CardIndex [21]: A data-driven estimator that stacks learned\nindex with an AR model. It can use index scans to execute probed\nlow-cardinality queries and achieve precise estimates.\nEnvironment. The experiments are conducted on a computer\nwith an AMD Ryzen 7 5800H CPU, NVIDIA RTX3060 GPU(Laptop),\n64 GB RAM, and a 1 TB hard disk. We use C++ on the CPU to infer\nICE and CardIndex. For the other baselines, we implement them\n\nConferenceâ€™17, July 2017, Washington, DC, USA Yingze Li, Xianglong Liu, Hongzhi Wang1, Kaixin Zhang, and Zixuan Wang\nin Python and use parallelization acceleration if possible. In other\nwords, we leverage GPU to accelerate MSCN and Naru and enable\nmulti-threading acceleration on CPU for CardIndex and ICE.\nEvaluation metrics. To better evaluate cardinality estimators,\nwe adopt the following evaluation metrics: Accuracy metric : We use\nQ-error to evaluate the estimatorâ€™s accuracy. Q-error is defined as\nğ‘„(ğ¸,ğ‘‡)=ğ‘šğ‘ğ‘¥{ğ¸\nğ‘‡,ğ‘‡\nğ¸}whereğ¸is the estimated cardinality value,\nandğ‘‡is the real cardinality. We report each workloadâ€™s entire Q-\nerror distribution as (50%, 95%, 99%, and Q-Max quantile). Latency\nmetric : We report the average time for each estimation inference and\nupdate operation. Size metric : We report the total size of estimators.\nParameter settings. We adopt the original settings of all the\nbaseline methods. We set the budget of ICE sampling at 20ğ‘˜in all\ndatasets. The fan-out number of the ICE index node is set to 100. And\nthe max recursive depth ğ‘‘ğ‘šğ‘ğ‘¥ is set to be 6. We set the confidence\nğ‘at1âˆ’10âˆ’7and the maximum tolerable Q-error ğ‘ğ‘to be 20.\n6.2 Estimation Evaluation\nComparison on static accuracy. We conduct CE tests on static\nworkloads in Table 2. We find that ICE achieves the best estimation\nperformance across nearly all Q-error metrics. Specifically, it im-\nproves the estimation accuracy up to 50 times compared to Naru,\nup to 160 times compared to CardIndex, and up to 80 times com-\npared to ALECE. This is because ICEâ€™s key space filtering trick can\neffectively filter out irrelevant query areas, leading to higher sam-\npling efficiency. Meanwhile, after enabling hybrid estimation, ICE\ncan effectively utilize the information obtained during sampling to\ncalculate the probability that a large estimation error occurs. For\nsmall cardinality queries with high odds having large-scale errors,\nICE can use fast index-scan to obtain precise results, preventing\nterrible mistakes and bound the maximum Q-error.\nComparison on dynamic accuracy. We conduct tests on both\nInsert-Heavy and Update-Heavy workloads. In real-world environ-\nments, not all models support real-time updates; therefore, esti-\nmators with excessively long update times will directly adopt the\nstale model for prediction[ 20]. To determine which models will be\ninstantly updated after the update operations, we first test the time\nrequired to update all models for one batch and report the results\nin Figure 8(a). Using a threshold of 1 second per update batch in\nFigure 8(a), we determine whether these baseline methods could per-\nform instant updates. In other words, we perform real-time updates\nfor the four estimators: CardIndex(CI), ALECE, Sample, and ICE.\nDue to the slow update time of the remaining models, we employ\nthe old model to predict unknown query cardinalities. The results\ncan be found in Table 2. We find out that ICE achieves the best esti-\nmation in both dynamic workloads. Regarding estimation accuracy,\nit is up to 2 orders of magnitude better than ALECE and up to 4\norders of magnitude better than the remaining baselines. The rea-\nsons for ICEâ€™s outstanding performance in dynamic scenarios are as\nfollows: (1) The data distribution learned by ICE remains consistent\nand up-to-date with the data source, preventing ICE from predicting\nsignificantly erroneous cardinalities due to outdated models, as seen\nin Naru and DeepDB. (2) ICE is a data-driven model, meaning that\nit will not produce huge estimation errors due to out-of-distribution\n(OOD) phenomena on the testing workload, as ALECE and MSCNmight suffer from. (3) CIâ€™s update strategy is not scalable. CI sam-\nplesğ‘‚(ğ¼)tuples from merged data to fine-tune its AR network[ 21],\nwhereğ¼is the size of the update operation. However, within larger\ndatasets like OSM and DMV, such sparse data cannot effectively\nupdate the model, which is still outdated. Considering that the AR\nnetwork is also CIâ€™s root node, insufficient updates cause a chain re-\naction where both the point queries of the index and the progressive\nsampling of high cardinalities exhibit significant errors, drastically\nimpacting its estimation accuracy and introducing errors on the\norder of 104. (4)After opening the hybrid estimation knob, ICE can\nreach the lossless data distribution and use the index execution to\navoid inaccurate estimation and bound the Q-error.\nComparison on estimation latency. In terms of inference la-\ntency, the ICEâ€™s inference overhead is also low enough, as shown in\nTable 2, ICEâ€™s inference speed, on average, is nearly twice as fast as\nNaruâ€™s. This is because the major part of its time consumption, sam-\npling, does not require as much computational resources as deep\nAR models such as Naru. For each sampling point, only twice point\nqueries are needed, one for Key2Rank and another for Rank2Key.\nThe average cost of a single sampling is 2 microseconds, which can\nbe effectively calculated even on a CPU. In contrast, deep models\nrequire expensive GPU resources, utilizing thousands of computing\ncores in GPUs to parallelize the inference of model parameters,\nresulting in extremely significant computational overhead.\nRobustness on out-of-distribution queries. In Table 3, we\nmake two types of Out-Of-Distribution (OOD) workloads in static\nscenarios of the Power dataset to test the robustness of the cardinal-\nity estimators, i.e., data drift and query drift workloads. Specifically,\nfor the data drift workload, we sort the data by the first column,\nmaking the training queries focus on the first 50%of the original\ndata distribution, while the test queries centered on the last 50%.\nAs for the query drift workload, we concentrate the predicates of\nthe training set mainly on the first five columns, whereas the test\nsetâ€™s predicates are mainly in the last column. We find that, on\nthese OOD testing workloads, all data-driven methods significantly\noutperform the query-driven methods. Although ALECE can utilize\ncoarse-grained information such as histograms to mitigate these\nissues, it is powerless against rare predicates on unseen columns.\nIn contrast, data-driven cardinality estimators do not face such\ndilemmas, achieving over two orders of magnitude higher accuracy\nmeasured by Q-Max than query-driven models. Furthermore, ICE\nachieved the best performance in both workloads.\n6.3 Construction and Updating Evaluation\n/s80/s111/s119/s101/s114 /s68/s77/s86 /s79/s83/s77\n/s49/s48/s45/s51\n/s49/s48/s45/s50\n/s49/s48/s45/s49\n/s49/s48/s48\n/s49/s48/s49\n/s49/s48/s50\n/s49/s48/s51\n/s65/s118/s103/s84/s105/s109/s101/s32/s80/s101/s114/s32/s85/s112/s100/s97/s116/s101/s40/s115/s41/s32/s73/s67/s69\n/s32/s67/s97/s114/s100/s73/s110/s100/s101/s120\n/s32/s65/s76/s69/s67/s69\n/s32/s77/s83/s67/s78\n/s32/s78/s97/s114/s117\n/s32/s68/s101/s101/s112/s68/s66\n/s68/s97/s116/s97/s68/s105/s115/s116\n(a) Update time per batch\n/s80/s111/s119/s101/s114 /s68/s77/s86 /s79/s83/s77\n/s48/s46/s49 /s49 /s49/s48 /s49/s48/s48\n/s65/s118/s103/s32/s76/s97/s116/s101/s110/s99/s121/s40 /s109 /s115/s47/s116/s117/s112/s108/s101/s41\n/s68/s97/s116/s97/s68/s105/s115/s116/s32/s73/s67/s69/s45/s66/s117/s108/s107/s76/s111/s97/s100\n/s32/s73/s67/s69/s45/s68/s101/s108/s101/s116/s101\n/s32/s73/s67/s69/s45/s73/s110/s115/s101/s114/s116\n/s32/s67/s97/s114/s100/s73/s110/s100/s101/s120\n/s32/s65/s76/s69/s67/s69 (b) Latency breakdown\nFigure 8: Results on batched updating time and its breakdown\n\nUpdateable Data-Driven Cardinality Estimator with Bounded Q-error Conferenceâ€™17, July 2017, Washington, DC, USA\nTable 2: Q-errors and inference latency on dynamic and static workloads of 3 read-world datasets\nDataset MethodStatic Insert-Heavy Update-Heavy Inference\nLatency(ms) 50 95 99 Max 50 95 99 Max 50 95 99 Max\nPowerNaru 1.11 2.00 3.7 16 1.23 10.3 501 1ğ‘’31.22 25.8 741 1ğ‘’39.6\nMSCN 2.19 26.1 111 284 4.13 58.43 465 7ğ‘’33.54 75.8 402 1ğ‘’30.6\nALECE 1.99 9.61 45.1 757 2.3 33.6 65 99.5 2.18 30.7 72.7 74 1.44\nDeepDB 1.13 4.02 8.95 15.3 1.27 14.7 840 2ğ‘’31.31 26.5 1ğ‘’32ğ‘’37.2\nSample 1.69 657 1ğ‘’32ğ‘’31.52 722 2ğ‘’34ğ‘’31.42 857 2ğ‘’33ğ‘’32.38\nCardIndex 1.51 23.39 6.19 87 1.65 40.6 710 2ğ‘’32.08 66.1 1ğ‘’32ğ‘’312\nICE 1.03 1.46 2.83 4.33 1.02 1.54 2.25 3.56 1.02 1.61 3.04 8.68 4.76\nDMVNaru 1.07 1.73 4.68 30 1.14 10.9 465 6ğ‘’31.14 12.8 263 785 17.6\nMSCN 1.8 30.1 401 1ğ‘’34.55 112 399 4ğ‘’33.74 112 1ğ‘’34ğ‘’30.71\nALECE 1.63 34.3 151 323 2.47 27.9 71 525 3.32 42.1 74.5 315 1.51\nDeepDB 1.06 2.72 32.3 210 1.18 47. 354 3ğ‘’31.16 17.6 384 2ğ‘’45.9\nSample 1.23 475 1ğ‘’32ğ‘’31.1 217 1ğ‘’36ğ‘’31.15 287 1ğ‘’34ğ‘’34.7\nCardIndex 1.62 4.68 7.99 57 1.71 57.1 157 3ğ‘’31.43 74.7 142 1ğ‘’38.46\nICE 1.03 2.35 4.54 17.1 1.02 1.55 2.75 12 1.03 2.35 4.54 11.5 8.12\nOSMNaru 1.09 4.82 30.5 347 1.34 43.5 9ğ‘’31ğ‘’41.29 30.5 8ğ‘’33ğ‘’46.4\nMSCN 2.96 314 2ğ‘’38ğ‘’35.76 456 3ğ‘’31ğ‘’46.47 657 3ğ‘’36ğ‘’30.79\nALECE 1.79 66 218 463 3.89 105 189 829 4.92 122 583 1ğ‘’31.35\nDeepDB 1.03 5.9 174 2ğ‘’31.27 80.3 9ğ‘’36ğ‘’41.21 74.9 1ğ‘’45ğ‘’46.5\nSample 1.06 117 626 4ğ‘’31.04 140 788 2ğ‘’31.05 93 454 1ğ‘’32.3\nCardIndex 1.81 61 181 1ğ‘’32.1 364 6ğ‘’33ğ‘’41.65 577 1ğ‘’44ğ‘’49.47\nICE 1 1.61 2.63 6.13 1 1.43 3.62 12.5 1 1.44 2 3.51 4.31\nTable 3: Q-errors on different workload drift scenes\nMethodDataDrift QueryDrift\n50 95 99 MAX 50 95 99 MAX\nALECE 3.65 54.1 127 201 9.27 137 634 668\nMSCN 8.04 668 1ğ‘’32ğ‘’314.1 352 3ğ‘’35ğ‘’3\nNaru 1.11 2.33 3.98 5 1.12 2.06 2.89 3.33\nICE 1 1.05 1.12 2.46 1.02 1.27 1.33 1.83\nDiscussions on updating time. In Figure 8(a), we report the\ntime required for the model to perform a single update batch. We\nseparately select the methods that require less than 1s per update\nbatch, decompose the batched update time, and report it separately\nin Figure 8(b). We find that ICE also achieves the fastest update\nspeed compared to both query-driven and data-driven methods.\nSpecifically, it is 5-6 times faster than the fastest data-driven method,\nCardIndex, and 2 times faster than the fastest query-driven method,\nALECE. Compared to CardIndex, ICE achieves rapid updates by\ncompletely replacing the neural network with an index structure, re-\nsulting in significant advantages in parameter storage and updating.\nWhen inserting or modifying a single tuple, we only need to modify\na local model parameter of ğ‘™ğ‘œğ‘”(ğ‘)scale, avoiding a full update of\nthe model parameters. Meanwhile, ALECE inefficiently maintains\nexcessive status information when updating statistical information,\nleading to a non-negligible constant time overhead. At the same\ntime, we also record the average time that bulk-loading takes to\nload a single tuple. We observe that \"ICE-Bulkload\" is about one\ntime faster than \"ICE-Insert\". This is because the bulk-loading of the\nmodel does not need to adjust the treeâ€™s structure frequently, whichbrings lower maintenance overhead. Therefore, when rapid cold\nstart is required, users can directly use the bulk-loading algorithm\nto quickly build the ICE model.\nDiscussions on training time and space consumption. We\nreport the modelsâ€™ training time and space consumption in Fig-\nure 9(a) and Figure 9(b). We single out the time taken by the query-\ndriven model to obtain the true cardinality label as \"Label\". We find\nout that in terms of training time, ICE achieves the fastest model\ntraining. Depending on whether the data is ordered, in the case of\nordered data, ICE can be up to three orders of magnitude faster than\nthe fastest existing model in terms of construction speed. In the case\nof unordered data, ICE can still be up to 40 times faster than existing\nmodels in terms of construction speed. Of course, since the distri-\nbution of the original data needs to be preserved as completely as\npossible, the index requires much space for maintenance, and both\nICE and CardIndex occupy considerable space. In summary, the ICE\nmodel effectively trades space for accuracy and time (CE accuracy,\ntraining time, update time, and inference latency). Considering that\ncompared to expensive GPU resources and the limited time budget\nfor query optimization, the memory in cloud databases is relatively\ncheap [ 17,26]. Also, maintaining an additional index in memory for\nindividual frequently queried tables can even accelerate the entire\nquery processing speed. Therefore, the above trade-off of space for\ntime and accuracy is actually acceptable in real-world scenarios.\n6.4 Variance Evaluation\nIn this section, we alter the parameters of our method, including\nsearch depth, sampling budget, selection strategy of the split point,\nQ-error bound, and confidence. We investigate their impact on the\nestimation accuracy and efficiency of ICE. We test on the static\n\nConferenceâ€™17, July 2017, Washington, DC, USA Yingze Li, Xianglong Liu, Hongzhi Wang1, Kaixin Zhang, and Zixuan Wang\n/s80/s111/s119/s101/s114 /s68/s77/s86 /s79/s83/s77\n/s49/s48/s45/s49\n/s49/s48/s48\n/s49/s48/s49\n/s49/s48/s50\n/s49/s48/s51\n/s49/s48/s52\n/s84/s114/s97/s105/s110/s105/s110/s103/s84/s105/s109/s101/s40/s115/s41/s32/s73/s67/s69/s45/s79/s114/s100/s101/s114/s101/s100\n/s32/s73/s67/s69/s45/s85/s110/s111/s114/s100/s101/s114/s101/s100\n/s32/s67/s97/s114/s100/s73/s110/s100/s101/s120\n/s32/s65/s76/s69/s67/s69\n/s32/s77/s83/s67/s78\n/s32/s76/s97/s98/s101/s108\n/s32/s78/s97/s114/s117\n/s32/s68/s101/s101/s112/s68/s66\n/s68/s97/s116/s97/s68/s105/s115/s116\n(a) Training time\n/s80/s111/s119/s101/s114 /s68/s77/s86 /s79/s83/s77\n/s49/s48/s45/s50\n/s49/s48/s45/s49\n/s49/s48/s48\n/s49/s48/s49\n/s49/s48/s50\n/s49/s48/s51\n/s77/s111/s100/s101/s108/s83/s105/s122/s101/s40/s77/s66/s41/s32/s73/s67/s69\n/s32/s67/s97/s114/s100/s73/s110/s100/s101/s120\n/s32/s65/s76/s69/s67/s69\n/s32/s77/s83/s67/s78\n/s32/s83/s97/s109/s112/s108/s101\n/s32/s78/s97/s114/s117\n/s32/s68/s101/s101/s112/s68/s66\n/s68/s97/s116/s97/s68/s105/s115/s116 (b) Model size\nFigure 9: Results on training time and model size\nworkload of the Power dataset. To better explore the influence of\nparameters on the estimation performance, we turn off the knob for\nhybrid estimation on the evaluation of the search depth, sampling\nbudget, and split point selection.\n/s48 /s53 /s49/s48/s49/s48/s48/s49/s48/s49/s49/s48/s50/s81/s69/s114/s114/s111/s114\n/s82/s101/s99/s117/s114/s115/s105/s118/s101/s83/s116/s101/s112/s32/s81/s53/s48\n/s32/s81/s57/s53\n/s32/s81/s57/s57\n/s32/s81/s77/s65/s88\n(a) Q-errors on recursive depth\n/s48 /s53 /s49/s48/s49/s50/s51/s32/s81/s45/s77/s101/s97/s110\n/s32/s76/s97/s116/s101/s110/s99/s121\n/s82/s101/s99/s117/s114/s115/s105/s118/s101/s83/s116/s101/s112/s81/s45/s77/s101/s97/s110\n/s50/s52/s54/s56/s49/s48\n/s32/s76/s97/s116/s101/s110/s99/s121 /s40/s109/s115/s41 (b) Efficiency on recursive depth\n/s48 /s52/s107 /s56/s107 /s49/s50/s107 /s49/s54/s107 /s50/s48/s107/s49/s48/s48/s49/s48/s49/s49/s48/s50/s49/s48/s51/s81/s69/s114/s114/s111/s114\n/s83/s97/s109/s112/s108/s101/s78/s117/s109/s32/s81/s53/s48\n/s32/s81/s57/s53\n/s32/s81/s57/s57\n/s32/s81/s77/s97/s120\n(c) Q-errors on sample number\n/s48 /s52/s107 /s56/s107 /s49/s50/s107 /s49/s54/s107 /s50/s48/s107/s48/s49/s48/s50/s48/s51/s48/s52/s48/s53/s48\n/s32/s81/s45/s77/s101/s97/s110\n/s32/s76/s97/s116/s101/s110/s99/s121\n/s83/s97/s109/s112/s108/s101/s78/s117/s109/s81/s45/s77/s101/s97/s110\n/s49/s46/s53/s50/s46/s48/s50/s46/s53/s51/s46/s48/s51/s46/s53/s52/s46/s48/s52/s46/s53\n/s32/s76/s97/s116/s101/s110/s99/s121 /s40/s109/s115/s41 (d) Efficiency on sample number\nFigure 10: Evaluation of sample number and recursive depth\nEffect on recursive depth and sampling number. We explore\nhow recursive depth and sampling budget affect the estimation qual-\nity and efficiency. For the exploration of recursive depth, we fix\nthe sampling budget at 20ğ‘˜. The results are in Figure 10(a) and Fig-\nure 10(b). We can observe a deeper recursive partitioning depth can\nbetter filter the sampling space, leading to a lower Q-error. Specif-\nically, when the depth increases from 1 to 9, the maximum Q-error\ndecreases from 231 to 19, representing a reduction by an order of\nmagnitude. And for the evaluation of the sampling budget, we fix\nthe recursive depth as 6. We report the results in Figure 10(c) and\nFigure 10(d). We can find out that with the increase in the number\nof samples, the estimation accuracy is also improved by two orders\nof magnitude. The reason for both experimental observations lies\nin the fact that, as we analyzed in Section 5.3, ICEâ€™s estimation\nvariance is inversely proportional to the sampling budget ğ‘and\nthe filtering efficiency ğœ‚of the index. A deeper depth represents a\nstronger ability of the index to filter irrelevant areas, resulting in\na higher filtering efficiency ğœ‚. Higher index filtering efficiency and\nmore sampling budgets lead to lower sampling variance and higher\naccuracy of estimation.Effect on split point selection. In Table 4, we investigate the\nimpact of different split point selection strategies on the modelâ€™s\nestimation quality while maintaining a sampling budget of 20ğ‘˜. Our\nfindings indicate that both sampling strategies can effectively filter\nthe multidimensional query space. Moreover, when the recursion\ndepth is shallow, the \"Optimal 1 Split\" strategy more efficiently\nfilters a larger portion of the query space. However, as the recur-\nsion depth increases, selecting the central point strategy achieves\na higher filtering efficiency. This can be attributed to the limited\nselection space for split points in the \"Optimal 1 Split\" strategy,\ncoupled with the early termination strategy in [ 9], which hinders\nfiner-grained partitioning. Consequently, this split strategy tends to\nlead the filtering process into a local optimum, resulting in good ef-\nficiency only at shallow filtering depths but struggling to maintain\nhigh efficiency at deeper levels. Ultimately, this limits the improve-\nment in estimation accuracy.\nTable 4: Q-errors on different split strategies with different\nrecursive depth\nMethod Depth 50th 95th 99th MAX\nOpt 1 Split 3 1.04 4.3 48 83\nMiddle 3 1.06 5 42 198\nOpt 1 Split 6 1.04 3.7 26 83\nMiddle 6 1.03 3 19 35\n/s50 /s52 /s56 /s49/s54 /s51/s50 /s54/s52 /s49/s50/s56 /s50/s53/s54/s48/s49/s48/s48/s50/s48/s48/s51/s48/s48\n/s81/s101/s114/s114/s111/s114\n/s113\n/s98/s32/s113\n/s98\n/s32/s82/s101/s97/s108/s45/s81/s57/s57\n/s32/s82/s101/s97/s108/s45/s81/s77/s97/s120\n/s32/s76/s97/s116/s101/s110/s99/s121\n/s50/s51/s52/s76/s97/s116/s101/s110/s99/s121 /s40/s109/s115/s41\n(a) Error bound analysis\n/s49/s69/s45/s49/s48 /s49/s69/s45/s56 /s49/s69/s45/s54 /s49/s69/s45/s52 /s48/s46/s48/s49 /s49/s48/s50/s48/s52/s48/s54/s48/s56/s48/s49/s48/s48/s81/s101/s114/s114/s111/s114\n/s49/s45/s67/s111/s110/s102/s105/s100/s101/s110/s99/s101/s32/s82/s101/s97/s108/s45/s81/s57/s57\n/s32/s82/s101/s97/s108/s45/s81/s77/s97/s120\n/s32/s76/s97/s116/s101/s110/s99/s121\n/s50/s51/s52\n/s76/s97/s116/s101/s110/s99/s121 /s40/s109/s115/s41 (b) Confidence analysis\nFigure 11: Evaluation on the Q-error bounding\nEffect on error bound and confidence. We explore how the\ninput Q-error bound and confidence level affect estimation quality\nand latency. Here, we enable hybrid estimation and conduct exper-\niments on a small-cardinality static workload on the Power dataset,\nwith the number of samples set to 2000. The confidence level in\nFigure 11(a) is 10âˆ’7, and the bound in Figure 11(b) is set to 2. From\nFigure 11(a) and Figure 11(b), we find that the preset bound can\nwell bound the maximum Q-error and the 99th percentile Q-error.\nAsğ‘ğ‘decreases and the confidence level increases, more and more\nlow-cardinality queries are handed over to the index for execution,\nresulting in more accurate estimates and relatively longer latency.\n7 Conclusions and Future Work\nIn this work, we designed ICE, a fully index-based cardinality es-\ntimation model, which addresses the slow update/build speed of\ndata-driven cardinality estimators. We introduce a robust alterna-\ntive approach for CE in dynamic scenarios, where the estimation\nremains highly accurate without being disturbed by the drift of\ntesting queries. Our future work will focus on two aspects: (1) Op-\ntimize index efficiency using historical workloads. According to\n\nUpdateable Data-Driven Cardinality Estimator with Bounded Q-error Conferenceâ€™17, July 2017, Washington, DC, USA\nvariance analysis in Section 5.3, it can improve sampling efficiency\nand estimation quality; (2) Compress the index to reduce the model\nparameter scale and memory overhead.\nReferences\n[1] 2017. OpenStreetMap data set. https://www.openstreetmap.org.\n[2]2019. Vehicle, snowmobile, and boat registrations. https://catalog.data.gov/\ndataset/vehicle-snowmobile-and-boat-registrations.\n[3]2021. Individual household electric power consumption data set. http://archive.\nics.uci.edu/ml/datasets/Individual+household+electric+power+consumption.\n[4]Daniar Achakeev and Bernhard Seeger. 2013. Efficient bulk updates on multiver-\nsion b-trees. Proceedings oftheVLDB Endowment 6, 14 (2013), 1834â€“1845.\n[5]Douglas Comer. 1979. Ubiquitous B-tree. ACM Computing Surveys (CSUR) 11,\n2 (1979), 121â€“137.\n[6]Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. 2020.\nTsunami: A Learned Multi-Dimensional Index for Correlated Data and Skewed\nWorkloads. Proc. VLDB Endow. 14, 2 (oct 2020), 74â€“86. https://doi.org/10.14778/\n3425879.3425880\n[7]Anshuman Dutt, Chi Wang, Azade Nazi, Srikanth Kandula, Vivek Narasayya, and\nSurajit Chaudhuri. 2019. Selectivity estimation for range predicates using light-\nweight models. Proceedings oftheVLDB Endowment (May 2019), 1044â€“1057.\nhttps://doi.org/10.14778/3329772.3329780\n[8]Cristian Estan and Jeffrey F Naughton. 2006. End-biased samples for join cardinal-\nity estimation. In 22nd International Conference onData Engineering (ICDEâ€™06) .\nIEEE, 20â€“20.\n[9]Jian Gao, Xin Cao, Xin Yao, Gong Zhang, and Wei Wang. 2023. LMSFC: A Novel\nMultidimensional Index Based on Learned Monotonic Space Filling Curves. Proc.\nVLDB Endow. 16, 10 (aug 2023), 2605â€“2617. https://doi.org/10.14778/3603581.\n3603598\n[10] Goetz Graefe et al .2011. Modern B-tree techniques. Foundations andTrends Â®\ninDatabases 3, 4 (2011), 203â€“402.\n[11] Yuxing Han, Ziniu Wu, Peizhi Wu, Rong Zhu, Jingyi Yang, Liang Wei Tan, Kai\nZeng, Gao Cong, Yanzhao Qin, Andreas Pfadler, Zhengping Qian, Jingren Zhou,\nJiangneng Li, and Bin Cui. 2021. Cardinality estimation in DBMS: a compre-\nhensive benchmark evaluation. Proc. VLDB Endow. 15, 4 (dec 2021), 752â€“765.\nhttps://doi.org/10.14778/3503585.3503586\n[12] Benjamin Hilprecht, Andreas Schmidt, Moritz Kulessa, Alejandro Molina, Kristian\nKersting, and Carsten Binnig. 2020. DeepDB: Learn from Data, Not from Queries!\nProc. VLDB Endow. 13, 7 (mar 2020), 992â€“1005. https://doi.org/10.14778/3384345.\n3384349\n[13] Christian S Jensen, Dan Lin, and Beng Chin Ooi. 2004. Query and update effi-\ncient B+-tree based indexing of moving objects. In Proceedings oftheThirtieth\ninternational conference onVery large data bases-Volume 30. 768â€“779.\n[14] Andreas Kipf, Thomas Kipf, Bernhard Radke, Viktor Leis, Peter A. Boncz, and\nAlfons Kemper. 2018. Learned Cardinalities: Estimating Correlated Joins with\nDeep Learning. ArXiv abs/1809.00677 (2018). https://api.semanticscholar.org/\nCorpusID:52154172\n[15] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe case for learned index structures. In Proceedings ofthe2018 international\nconference onmanagement ofdata. 489â€“504.\n[16] Meghdad Kurmanji, Eleni Triantafillou, and Peter Triantafillou. 2024. Machine\nUnlearning in Learned Databases: An Experimental Analysis. Proc. ACM Manag.\nData 2, 1, Article 49 (mar 2024), 26 pages. https://doi.org/10.1145/3639304\n[17] Tirthankar Lahiri, Shasank Chavan, Maria Colgan, Dinesh Das, Amit Ganesh,\nMike Gleeson, Sanket Hase, Allison Holloway, Jesse Kamp, Teck-Hua Lee, et al .\n2015. Oracle database in-memory: A dual format in-memory database. In 2015\nIEEE 31st International Conference onData Engineering. IEEE, 1253â€“1258.\n[18] Per-Ake Larson, Wolfgang Lehner, Jingren Zhou, and Peter Zabback. 2007. Car-\ndinality estimation using sample views with quality assurance. In Proceedings\nofthe2007 ACM SIGMOD international conference onManagement ofdata.\n175â€“186.\n[19] Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, and Philip Torr. 2019. Controllable\ntext-to-image generation. Advances inneural information processing systems\n32 (2019).\n[20] Pengfei Li, Wenqing Wei, Rong Zhu, Bolin Ding, Jingren Zhou, and Hua Lu.\n2023. ALECE: An Attention-based Learned Cardinality Estimator for SPJ Queries\non Dynamic Workloads. Proc. VLDB Endow. 17, 2 (oct 2023), 197â€“210. https:\n//doi.org/10.14778/3626292.3626302\n[21] Yingze Li, Hongzhi Wang, and Xianglong Liu. 2024. One Seed, Two Birds: A\nUnified Learned Structure for Exact and Approximate Counting. Proc. ACM\nManag. Data 2, 1, Article 15 (mar 2024), 26 pages. https://doi.org/10.1145/3639270\n[22] Tongyu Liu, Ju Fan, Nan Tang, Guoliang Li, and Xiaoyong Du. 2024. Controllable\nTabular Data Synthesis Using Diffusion Models. Proc. ACM Manag. Data 2, 1,\nArticle 28 (mar 2024), 29 pages. https://doi.org/10.1145/3639283\n[23] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020.\nLearning Multi-Dimensional Indexes. In Proceedings ofthe 2020 ACMSIGMOD International Conference onManagement ofData (Portland, OR, USA)\n(SIGMOD â€™20) . Association for Computing Machinery, New York, NY, USA,\n985â€“1000. https://doi.org/10.1145/3318464.3380579\n[24] Viswanath Poosala, Peter J. Haas, Yannis Ioannidis, and Eugene J. Shekita. 1996.\nImproved histograms for selectivity estimation of range predicates. International\nConference onManagement ofData (1996).\n[25] Frank Ramsak, Volker Markl, Robert Fenk, Martin Zirkel, Klaus Elhardt, and\nRudolf Bayer. 2000. Integrating the UB-tree into a database system kernel.. In\nVLDB, Vol. 2000. 263â€“272.\n[26] Abu Sebastian, Manuel Le Gallo, Riduan Khaddam-Aljameh, and Evangelos\nEleftheriou. 2020. Memory devices and applications for in-memory computing.\nNature nanotechnology 15, 7 (2020), 529â€“544.\n[27] Ji Sun, Jintao Zhang, Zhaoyan Sun, Guoliang Li, and Nan Tang. 2021. Learned Car-\ndinality Estimation: A Design Space Exploration and a Comparative Evaluation.\n15, 1 (sep 2021), 85â€“97. https://doi.org/10.14778/3485450.3485459\n[28] Herbert Tropf and Helmut Herzog. 1981. Multidimensional Range Search in\nDynamically Balanced Trees. ANGEWANDTE INFO. 2 (1981), 71â€“77.\n[29] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex\nGraves, et al .2016. Conditional image generation with pixelcnn decoders.\nAdvances inneural information processing systems 29 (2016).\n[30] Xiaoying Wang, Changbo Qu, Weiyuan Wu, Jiannan Wang, and Qingqing Zhou.\n2021. Are We Ready for Learned Cardinality Estimation? Proc. VLDB Endow. 14,\n9 (may 2021), 1640â€“1654. https://doi.org/10.14778/3461535.3461552\n[31] Jiacheng Wu, Yong Zhang, Shimin Chen, Jin Wang, Yu Chen, and Chunxiao Xing.\n2021. Updatable Learned Index with Precise Positions. Proc. VLDB Endow. 14, 8\n(apr 2021), 1276â€“1288. https://doi.org/10.14778/3457390.3457393\n[32] Sai Wu, Dawei Jiang, Beng Chin Ooi, and Kun-Lung Wu. 2010. Efficient B-tree\nbased indexing for cloud data processing. Proceedings oftheVLDB Endowment\n3, 1-2 (2010), 1207â€“1218.\n[33] Zongheng Yang, Eric Liang, Amog Kamsetty, Chenggang Wu, Yan Duan, Xi\nChen, Pieter Abbeel, Joseph M. Hellerstein, Sanjay Krishnan, and Ion Stoica. 2019.\nDeep Unsupervised Cardinality Estimation. Proc. VLDB Endow. 13, 3 (nov 2019),\n279â€“292. https://doi.org/10.14778/3368289.3368294\n[34] Jintao Zhang, Chao Zhang, Guoliang Li, and Chengliang Chai. 2024. PACE:\nPoisoning Attacks on Learned Cardinality Estimation. Proc. ACM Manag. Data\n2, 1, Article 37 (mar 2024), 27 pages. https://doi.org/10.1145/3639292\n[35] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding conditional\ncontrol to text-to-image diffusion models. In Proceedings oftheIEEE/CVF\nInternational Conference onComputer Vision. 3836â€“3847.\nReceived 20 February 2007; revised 12 March 2009; accepted 5 June 2009",
  "textLength": 79955
}