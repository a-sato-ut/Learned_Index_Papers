{
  "paperId": "22787d48714cefc9065dadc312c5fddd8dd02970",
  "title": "Discovering Data Structures: Nearest Neighbor Search and Beyond",
  "pdfPath": "22787d48714cefc9065dadc312c5fddd8dd02970.pdf",
  "text": "DISCOVERING DATA STRUCTURES :\nNEAREST NEIGHBOR SEARCH AND BEYOND\nOmar Salemohamed∗\nUniversit ´e de Montr ´eal, MilaLaurent Charlin†\nHEC Montr ´eal, MilaShivam Garg†\nMicrosoft Research\nVatsal Sharan†\nUniversity of Southern CaliforniaGregory Valiant†\nStanford University\nABSTRACT\nWe propose a general framework for end-to-end learning of data structures. Our\nframework adapts to the underlying data distribution and provides fine-grained\ncontrol over query and space complexity. Crucially, the data structure is learned\nfrom scratch, and does not require careful initialization or seeding with candidate\ndata structures. We first apply this framework to the problem of nearest neighbor\nsearch. In several settings, we are able to reverse-engineer the learned data struc-\ntures and query algorithms. For 1D nearest neighbor search, the model discovers\noptimal distribution (in)dependent algorithms such as binary search and variants\nof interpolation search. In higher dimensions, the model learns solutions that re-\nsemble k-d trees in some regimes, while in others, elements of locality-sensitive\nhashing emerge. Additionally, the model learns useful representations of high-\ndimensional data and exploits them to design effective data structures. We also\nadapt our framework to the problem of estimating frequencies over a data stream,\nand believe it could be a powerful discovery tool for new problems.\n1 I NTRODUCTION\nCan deep learning models be trained to discover data structures from scratch?\nThere are several motivations for this question. The first is scientific. Deep learning models are\nincreasingly performing tasks once considered exclusive to humans, from image recognition and\nmastering the game of Go to engaging in natural language conversations. Designing data structures\nand algorithms, along with solving complex math problems, are particularly challenging tasks. They\nrequire searching through a vast combinatorial space with a difficult to define structure. It is there-\nfore natural to ask what it would take for deep learning models to solve such problems. There are\nalready promising signs: these models have discovered fast matrix-multiplication algorithms (Fawzi\net al., 2022), solved SAT problems (Selsam et al., 2018), and learned optimization algorithms for\nvarious learning tasks (Garg et al., 2022; Aky ¨urek et al., 2022; Fu et al., 2023; V on Oswald et al.,\n2023). In this work, we investigate the problem of data structure discovery, with a focus on nearest\nneighbor search.\nThe second motivation is practical. Data structures are ubiquitous objects that enable efficient query-\ning. Traditionally, they have been designed to be worst-case optimal and therefore agnostic to the\nunderlying data and query distributions. However, in many applications there are patterns in these\ndistributions that can be exploited to design more efficient data structures. This has motivated recent\nwork on learning-augmented data structures which leverages knowledge of the data distribution to\nmodify existing data structures with predictions (Lykouris & Vassilvitskii, 2018; Ding et al., 2020;\nLin et al., 2022a; Mitzenmacher & Vassilvitskii, 2022). In much of this work, the goal of the learning\nalgorithm is to learn distributional properties of the data, while the underlying query algorithm/data\nstructure is hand-designed. Though this line of work clearly demonstrates the potential of leverag-\ning distributional information, it still relies on expert knowledge to incorporate learning into such\n∗Corresponding Author: omar.salemohamed@mila.quebec\n†Authors listed in alphabetical order.\n1arXiv:2411.03253v1  [cs.LG]  5 Nov 2024\n\nstructures. In our work, we ask if it is possible to go one step further and let deep learning models\ndiscover entire data structures and query algorithms in an end-to-end manner.\n1.1 F RAMEWORK FOR DATA STRUCTURE DISCOVERY\n... ... ...\nFigure 1: Our model has two components: 1)A data-processing network that transforms raw data\ninto structured data, arranging it for efficient querying and generating additional statistics when\ngiven extra space (not shown in the figure). 2)A query-execution network that performs Mlookups\ninto the output of the data-processing network in order to retrieve the answer to some query q. Each\nlookup iis managed by a separate query model Qi, which takes qand the lookup history Hi, and\noutputs a one-hot lookup vector miindicating the position to query.\nData structure problems can often be decomposed into two steps: 1) data structure construction and\n2) query execution. The first step transforms a raw dataset Dinto a structured database ˆD, while\nquery-execution performs lookups into ˆDto retrieve the answer for some query q. The performance\nof a data structure is typically quantified in terms of two measures: space complexity —how much\nmemory is required to store the data structure, and query complexity —how many lookups into the\ndata structure are required to answer some query. One can typically tradeoff larger space complexity\nfor smaller query complexity, and vice versa. We focus on these criteria as they are widely studied\nand have clear practical connections to efficiency.\nTo learn such data structures, we have a data-processing network which learns how to map a raw\ndataset to a data structure, and a query network which learns an algorithm for using the data structure\nto answer queries (Figure 1). In order to learn efficient data structures and query algorithms we\nimpose constraints on the size of the data structures and on the number of lookups that the query\nnetwork can make into the data structure. Crucially, we propose end-to-end training of both networks\nsuch that the learned data structure and query algorithm are optimized for one another. Moreover,\nin settings where it is beneficial to learn lower-dimensional representations from high-dimensional\ndata, end-to-end training encourages the representations to capture features of the problem that the\ndata structure can exploit.\nOn the one hand, learning the data-processing network and query network jointly in an end-to-end\nfashion seems obvious, especially given the many successes of end-to-end learning over the past\ndecade. On the other hand, it might be hard to imagine such learning getting off the ground. For\ninstance, if the data-processing network produces a random garbled function of the dataset, we\ncannot hope the query model to do anything meaningful. This is further complicated by the fact\nthat these data structure tasks are more discrete and combinatorial in terms of how the query model\naccesses the data structure.\n1.2 S UMMARY OF RESULTS\nWe apply this framework to the problem of nearest neighbor (NN) search in both low and high\ndimensions. Given the extensive theoretical work on this topic, along with its widespread practical\napplications, NN search is an ideal starting point for understanding the landscape of end-to-end\ndata structure discovery. Beyond NN search, we explore the problem of frequency estimation in\nstreaming data and discuss other potential applications of this framework. Our findings are:\n2\n\nSorting and searching in 1D (Section 2.2) For 1D nearest neighbor search, the data-processing\nnetwork learns to sort, while the query network simultaneously learns to search over the sorted\ndata. When the data follows a uniform or Zipfian distribution, the query network exploits this\nstructure to outperform binary search. On harder distributions lacking structure, the network adapts\nby discovering binary search, which is worst-case optimal. Importantly, the model discovers that\nsorting followed by the appropriate search algorithm is effective for NN search in 1D without explicit\nsupervision for these primitives.\nK-d trees in 2D (Section 2.3) In 2D, when the data is drawn from a uniform distribution, the\nmodel discovers a data structure that outperforms k-d trees. On harder distributions, the learned data\nstructure shows surprising resemblance to a k-d tree. This is striking as a k-d tree is a non-trivial data\nstructure, constructed by recursively partitioning the data and finding the median along alternating\ndimensions.\nUseful representations in high dimensions (Section 2.4) For high-dimensional data, the model\nlearns representations that make NN search efficient. For example, with data from a uniform distri-\nbution on a 30-dimensional hypersphere, the model partitions the space by projecting onto a pair of\nvectors, similar to locality-sensitive hashing. When trained on an extended 3-digit MNIST dataset,\nthe model finds features that capture the relative ordering of the digits, sorts the images using these\nfeatures, and performs a search on the sorted images—all of which is learned jointly from scratch.\nTrading off space and query efficiency (Section 2.5) An ideal data structure can use extra space\nto improve query efficiency by storing additional statistics. The learned model demonstrates this\nbehavior, with performance improving monotonically as more space is provided, in both low and\nhigh dimensions. Thus, the model learns to effectively trade off space for query efficiency.\nBeyond nearest neighbor search (Section 3) We also explore the classical problem of frequency\nestimation, where a memory-constrained model observes a stream of items and must approximate\nthe frequency of a query item. The learned structure exploits the underlying data distribution to out-\nperform baselines like CountMin sketch, demonstrating the broader applicability of the framework\nbeyond nearest neighbor search.\n2 N EAREST NEIGHBOR SEARCH\nGiven a dataset D={x1, ..., x N}ofNpoints where xi∈Rdand a query q∈Rd, the nearest\nneighbor yofqis defined as y= arg min xi∈Ddist(xi, q). We mostly focus on the case where\ndist(·)corresponds to the Euclidean distance. Our objective is to learn a data structure ˆDforD\nsuch that given qand a budget of Mlookups, we can output a (approximate) nearest neighbor of q\nby querying at most Melements in ˆD. When M≥N,ycan be trivially recovered via linear search\nsoˆD=Dis sufficient. Instead, we are interested in the case when M≪N.1\n2.1 S ETUP\nData-processing Network Recall that the role of the data-processing network is to transform a raw\ndataset into a data structure. The backbone of our data-processing network is an 8-layer transformer\nmodel based on the NanoGPT architecture (Karpathy, 2024). In the case of NN search, we want\nthe data structure to preserve the original inputs and just reorder them appropriately as the answer\nto the nearest neighbor query should be one of elements in the dataset. The model achieves this\nby outputting a rank associated with each element in the dataset, which is then used to reorder\nthe elements. More precisely, the transformer takes as input the dataset Dand outputs a scalar\noi∈Rrepresenting the rank for each point xi∈D. These rankings {o1, ..., o N}are then sorted\nusing a differentiable sort function, sort({o1, o2. . . , o N})(Grover et al., 2019; Cuturi et al., 2019;\nPetersen et al., 2022), which produces a permutation matrix Pthat encodes the order based on\nthe rankings. By applying Pto the input dataset D, we obtain ˆDP, where the input data points\nare arranged in order of their rankings. By learning to rank rather than directly outputting the\n1E.g. in 1D, binary search requires M= log(N)lookups given a sorted list.\n3\n\ntransformed dataset, the transformer avoids the need to reproduce the exact inputs. Note that this\ndivision into a ranking model followed by sorting is without loss of generality as the overall model\ncan represent any arbitrary ordering of the inputs.\nWe also consider scenarios where the data structure can use additional space. To support this use\ncase, the transformer outputs Textra vectors b1, ..., b T∈Rdwhich can be retrieved by the query-\nexecution network. We form the data structure ˆDby concatenating the permuted inputs and the extra\nvectors: ˆD= [ˆDP, b1, ..., b T].\nQuery Execution Network The role of the query-execution network is to output a (approximate)\nnearest-neighbor of some query qgiven a budget of Mlookups into the data structure ˆD. The query-\nexecution network consists of MMLP query models Q1, ..., QM. The query models do not share\nweights. Each query model Qioutputs a one-hot vector mi∈RN+Twhich represents a lookup\nposition in ˆD. To execute the lookup, we compute the value viat the position denoted by miinˆDas\nvi=m⊤\niˆD. In addition to the query q, each query model Qialso takes as input the query execution\nhistory Hi={(m1, v1), ...,(mi−1, vi−1)}where H1=∅. The final answer of the network for the\nnearest-neighbor query is given by ˆy=m⊤\nMˆD.\nTo restrict our model to exactly Mlookups, we enforce each lookup vector mito be a one-hot vector.\nEnforcing this constraint during training poses a challenge as it is a non-differentiable operation.\nInstead, during training, our model outputs soft-lookups where miis the output of the softmax\nfunction andP\njmij= 1. This alone, however, leads to non-sparse queries. To address this, we\nadd noise to the logits before the softmax operation (only during training). This regularizes the query\nnetwork, encouraging it to produce sparser solutions (see App C.1 for details as to why this occurs).\nIntuitively, the network learns a function that is robust to noise, and the softmax output becomes\nrobust when the logits are well-separated. Well-separated logits, in turn, lead to sparser solutions.\nAt inference time, we do not add this noise and we ensure the lookup vector miis a one-hot vector\nby applying a hardmax function to the network output instead of a softmax.\nData Generation and Training Each training example is a tuple (D, q, y )consisting of a dataset\nD, query q, and nearest neighbor ygenerated as follows: (i) sample dataset D={x1, ..., x N}from\ndataset distribution PD, (ii) sample query qfrom query distribution Pq, (iii) compute nearest neigh-\nbory= arg min xi∈Ddist(xi−q). Unless otherwise specified, dist corresponds to the Euclidean\ndistance. The dataset and query distributions PD, Pqvary across the different settings we consider\nand are defined later. Given a training example (D, q, y ), the data-processing network transforms D\ninto the data structure ˆD. Subsequently, the query-execution network, conditioned on q, queries the\ndata structure to output ˆy. We use SGD to minimize either the squared loss between yandˆy, or the\ncross-entropy loss between the corresponding vectors encoding their positions. This is an empirical\nchoice, and in some settings one loss function performs better than the other. All models are trained\nfor at most 4 million gradient steps with early-stopping using a batch size of 1024. After training,\nwe test our model on 10k inputs (D, q, y )generated in the same way. We describe the exact model\narchitecture and training hyper-parameters in App A.1.\nEvaluation and Baselines We evaluate our end-to-end model (referred to as E2E) on one-\ndimensional, two-dimensional, and high-dimensional nearest-neighbor problems. We primarily fo-\ncus on data structures that do not use extra space, but in Section 2.5, we also explore scenarios with\nadditional space.\nWe compare against suitable NN data structures in each setting (e.g., sorting followed by binary\nsearch in 1D), and also against several ablations to study the impact of various model components.\nTheE2E (frozen) model does not train the data-processing network, relying on rankings generated\nby the initial weights. The E2E (no-permute) model removes the permutation component of the\ndata-processing network so that the transformer has to learn to transform the data points directly.\nTheE2E (non-adaptive) model ablation conditions each query model Qion only the query qand\nnot the query history Hi. From the Mquery models, we select the prediction that is closest to the\nquery as the final prediction ˆy.\n4\n\n1 2 3 4 5 6 7\nLookup Index0.00.20.40.60.81.0Accuracy\nModel\nE2E\nE2E (frozen)\nE2E (non-adaptive)\nBinary Search\nFigure 2: (Left) Our model (E2E) trained with 1D data from the uniform distribution over (−1,1)\noutperforms binary search and several ablations. (Center) Distribution of lookups by the first query\nmodel. Unlike binary search, the model does not always start in the middle but rather closer to the\nquery’s likely position in the sorted data. (Right) When trained on data from a “hard” distribution\nfor which the query value does not reveal information about the query’s relative position, the model\nfinds a solution similar to binary search. The figure shows an example of the model performing\nbinary search (’X’ denotes the nearest neighbor location).\n2.2 O NE-DIMENSIONAL DATA\nUniform Distribution We consider a setting where the data distribution PDand query distribution\nPqcorrespond to the uniform distribution over (−1,1),N= 100 andM= 7. We plot the accuracy2,\nwhich refers to zero-one loss in identifying the nearest neighbor, after each lookup in Figure 2 (Left).\nRecall that vicorresponds to the output of the i-th lookup. Let v∗\nidenote the closest element to the\nquery so far: v∗\ni= arg min v∈{v1,...,v i}||v−q||2\n2. At each lookup index we plot the nearest neighbor\naccuracy corresponding to v∗\ni. We do this for all the methods.\nA key component in being able to do NN search in 1D is sorting. We observe that the trained model\ndoes indeed learn to sort. We verify this by measuring the fraction of inputs that are mapped to the\ncorrect position in the sorted order, averaged over multiple datasets. The trained model correctly\npositions approximately 99.5% of the inputs. This is interesting as the model never received explicit\nfeedback to sort the inputs and figured it out in the end-to-end training. The separate sorting function\naids the process, but the model still has to learn to output the correct rankings.\nThe second key component is the ability to search over the sorted inputs. Here, our model learns\na search algorithm that outperforms binary search, which is designed for the worst case. This is\nbecause unlike binary search, our model exploits knowledge of the data distribution to start its\nsearch closer to the nearest neighbor, similar to interpolation search (Peterson, 1957). For instance,\nif the query q≈1, the model begins its search near the end of the list (Figure 2 (Center)). The minor\nsorting error ( ∼0.5%) our model makes likely explains its worse performance on the final query.\nTo understand the relevance of different model components, we compare against various ablations.\nTheE2E (frozen) model (untrained transformer) positions only about 9% of inputs correctly, ex-\nplaining its under-performance. This shows that the transformer must learn to rank the inputs, and\nthat merely using a separate function for sorting the transformer output is insufficient. The E2E\n(non-adaptive) baseline, lacking query history access, underperforms as it fails to learn adaptive\nsolutions crucial for 1D NN search. The E2E (no-permute) ablation does not fully retain inputs and\nso we do not measure accuracy for this baseline. We verify this by measuring the average minimum\ndistance between each of the transformer’s inputs to its outputs. These ablations highlight the crucial\nrole of both learned orderings and query adaptivity for our model.\nZipfian Distribution Prior work has shown that several real-world query distributions follow a\nZipfian trend whereby a few elements are queried far more frequently than others, leading to the\ndevelopment of learning-augmented algorithms aimed at exploiting this (Lin et al., 2022b). We\nconsider a setting where PDis the discrete uniform distribution over {1, ...,200}andPqis a Zipfian\ndistribution over {1, ...,200}skewed towards smaller numbers such that the number iis sampled\nwith probability proportional to1\niα. We set α= 1.2. Again, in this setting N= 100 andM= 7.\n2We include MSE plots as well in App. B.\n5\n\n1 2 3 4 5 6 7\nLookup Index0.00.20.40.60.81.0Accuracy\nModel\nE2E\nBinary Search\nZipf TreapFigure 3: For 1D Zipfian\nquery distribution, our model\nperforms slightly better than\nthe the learning-augmented\ntreap algorithm from Hsu\net al. (2019) and both meth-\nods significantly outperforms\nbinary search.In Figure 3 we compare our model to both binary search and the\nlearning-augmented treap from Lin et al. (2022a). Our model per-\nforms slightly better than the learning-augmented treap and both\nalgorithms significantly outperform binary search with less than\nlog(N)queries. This setting highlights a crucial difference in spirit\nbetween our work and much of the existing work on learning-\naugmented algorithms. While the Zipfian treap incorporates learn-\ning in the algorithm, the authors still had to figure out how an ex-\nisting data structure could be modified to support learning. On the\nother hand, by learning end-to-end, our framework altogether re-\nmoves the need for the human-in-the-loop. This is promising as\nit could be useful in settings where we lack insight on appropriate\ndata structures. The flip side, however, is that learning-augmented\ndata structures usually come with provable guarantees which are\ndifficult to get when training models in an end-to-end fashion.\nHard Distribution To verify that our model can also learn worst-\ncase optimal algorithms such as binary search, we set PDto be a\n“hard” distribution, with the property that for any given query there\ndoes not exist a strong prior over the position of its nearest neighbor\nin the sorted data (see App. B.1 for more details). To produce a problem instance, we first sample\na dataset from PD. We then generate the query by sampling a point (uniformly at random) from\nthis dataset, and adding standard Gaussian noise to it. The hard distribution generates numbers at\nseveral scales, and this makes it challenging to train the model with larger N. Thus, we use N= 15\nandM= 3. In general, we find that training models is easier when there is more structure in the\ndistribution to be exploited.\nThe model does indeed discover a search algorithm similar to binary search. In Figure 2 (Right), we\nshow a representative example of the model’s search behavior, resembling binary search (see Figure\n16 for more examples). The error curve in Figure 14 also closely matches that of binary search.\nIn summary, in all the above settings, starting from scratch, the data-processing network discovers\nthat the optimal way to arrange the data is in sorted order. Simultaneously, the query-execution\nnetwork learns to efficiently query this sorted data, leveraging the properties of the data distribution.\n2.3 T WO-DIMENSIONAL DATA\nBeyond one dimension it is less clear how to optimally represent a collection of points as there is\nno canonical notion of sorting along multiple dimensions. In fact, we observe in these experiments\n1.00\n 0.75\n 0.50\n 0.25\n 0.00 0.25 0.50 0.75 1.00\nDimension 11.00\n0.75\n0.50\n0.25\n0.000.250.500.751.00Dimension 2\nRaw Dataset\n1.00\n 0.75\n 0.50\n 0.25\n 0.00 0.25 0.50 0.75 1.00\nDimension 11.00\n0.75\n0.50\n0.25\n0.000.250.500.751.00Dimension 2\nTransformed Dataset\n0.00.20.40.60.81.0\n(Normalized) Index Position\nFigure 4: Our model’s learned data structure for an instance from the uniform distribution in 2D.\nWhile the original order of the stored points showed no structure, the learned data structure arranges\npoints that are close together in the Euclidean plane next to each other.\n6\n\nFigure 5: The learned data structure resembles a k-d tree in 2D. We show the average pairwise\ndistances (along the first, second, and both dimensions) between points for the learned structure and\nthe k-d tree, with darker colors indicating smaller distances. For the k-d tree, we arrange the points\nby in-order traversal. It recursively splits the points into two groups based on whether their value\nis smaller or larger than the median along a given dimension, alternating between dimensions at\neach level, starting with dimension 1. The learned data structure approximately mirrors this pattern,\nsplitting by dimension 2 followed by dimension 1.\nthat different data/query distributions lead to altogether different data structures. This reinforces the\nvalue in learning both the data structure and query algorithm together end-to-end.\nUniform Distribution We use a setup similar to 1D, sampling both coordinates independently\nfrom the uniform distribution on (−1,1). We set N= 100 andM= 6, and compare to a k-d tree\nbaseline. A k-d tree is a binary tree for organizing points in k-dimensional space, with each node\nsplitting the space along one of the k axes, cycling through the axes at each tree level. Here, our\nE2E model achieves an accuracy of 75% vs52% for the k-d tree (Fig. 10 in App. B). The model\noutperforms the k-d tree as it can exploit distributional information. By studying the permutations,\nwe find that our model learns to put points that are close together in the 2D plane next to each other\nin the permuted order (see Fig. 4 for an example).\nHard Distribution We also consider the case where we sample both coordinates independently\nfrom the hard distribution considered in the 1D setup (see Figure 17 for the corresponding error\ncurve). We observe that the data structure learned by our model is surprisingly similar to a k-d\ntree (see Fig 5). This is striking as a k-d tree is a non-trivial data structure, requiring recursively\npartitioning the data and finding the median along alternating dimensions at each level of the tree.\n2.4 H IGH-DIMENSIONAL DATA\nHigh-dimensional NN search poses a challenge for traditional low-dimensional algorithms due to\nthe curse of dimensionality. K-d trees, for instance, can require an exponential number of queries\nin high dimensions (Kleinberg, 1997). This has led to the development of approximate NN search\nmethods such as locality sensitive hashing (LSH) which have a milder dependence on d(Andoni\net al., 2018), relying on hash functions that map closer points in the space to the same hash bucket.\nWe train our model on datasets uniformly sampled from the d-dimensional unit hypersphere. The\nquery is sampled to have a fixed inner-product ρ∈[0,1]with a dataset point. When ρ= 1, the\nquery matches a data point, making regular hashing-based methods sufficient. For ρ < 1, LSH-\nbased solutions are competitive. We train our model for ρ= 0.8and compare it to an LSH baseline\nwhen N= 100 , M= 6, andd= 30 . The LSH baseline partitions R30using Krandom vectors and\nbuckets each point in the dataset according to its signed projection onto each of the Kvectors. To\n7\n\n1 2 3 4 5 6\nLookup Index0.050.100.150.200.250.300.35Accuracy\nModel\nE2E\nE2E (frozen)\nE2E (non-adaptive)\nLSH (K=3)\nLSH (K=4)\n0 25 50 75 100 125 150 175 200\nLabel10\n8\n6\n4\n2\n024Learned Feature\nFigure 6: (Left) For NN search in higher dimensions (d = 30), the trained models perform com-\nparably to (E2E) or better than (E2E (non-adaptive)) locality-sensitive hashing (LSH) baselines.\n(Center) When trained with a single query, the model partitions the query space based on projec-\ntion onto two vectors, similar to LSH. We show the query projection onto the subspace spanned by\nthese vectors and the lookup positions for different queries. (Right) When trained end-to-end to do\nnearest neighbor search over 3-Digit MNIST Images, our model learns 1D features that capture the\nrelative ordering of the numbers in the images.\nretrieve the nearest neighbor of a query point, the baseline maps the query vector to its corresponding\nbucket and selects the closest vector among Mcandidates (refer App D for more details).\nIn Figure 6 (Left), we observe that our model performs competitively with LSH baselines.3The\nnon-adaptive model does slightly better as adaptivity is not needed to do well here (e.g., LSH is\nnon-adaptive), and lack of adaptivity likely makes training easier. To better understand the data\nstructure our model learns we consider a smaller setting where N= 8andM= 1. We find that the\nmodel learns an LSH like solution, partitioning the space by projecting onto two vectors in R30(see\nFigure 6 (Center)). We provide more details in App C.3.\nLearning useful representations High-dimensional data often contains low-dimensional struc-\nture, such as data lying on a manifold, which can be leveraged to improve the efficiency of NN\nsearch. ML models are particularly well-suited to exploit these structures. Here, we explore whether\nour end-to-end learning framework can learn representations that capture such structures. This is a\nchallenging task as it involves jointly optimizing the learned representation, data structure, and query\nalgorithm.\nWe consider the following task: given a dataset of distinct 3-digit handwritten number images, and\na query image, find its nearest neighbor in the dataset, which corresponds to the image encoding the\nclosest number to the query image (i.e., nearest is defined over the label space).\nWe generate images of 3-digit numbers by concatenating digits from MNIST (see Figure 13 for\nimage samples). To construct a nearest-neighbor dataset D, we sample N= 50 labels (each label\ncorresponds to a number) uniformly from 0 to 199. For each label, we then sample one of its\nassociated training images from 3-digit MNIST. Additionally, we sample a query label (uniformly\nover{0, ..,199}) and its corresponding training image and find its nearest neighbor in D, which\ncorresponds to the image with the same label. We emphasize that the model has no label supervision\nbut rather only has access to the query’s nearest neighbor. After training, we evaluate the model\nusing the same data generation process but with images sampled from the 3-digit MNIST test set.\nAs both the data-processing and query-execution networks should operate over the same low-\ndimensional representation we train a CNN feature model Fϕas well. Our setup remains the\nsame as before except now the data-processing network and query-execution network operate on\n{Fϕ(x1), ..., F ϕ(xN)}andFϕ(q), respectively. As the underlying distance metric does not corre-\nspond to the Euclidean distance, we minimize the cross-entropy loss instead of the MSE loss. Note\nthat the cross-entropy loss only requires supervision about the nearest neighbor of the query, and\ndoes not require the exact metric structure, so it can be used even where the exact metric structure is\nunknown.\n3We exclude LSH baselines with larger K as they under-perform.\n8\n\nFigure 7: Trained on 3-digit MNIST images, our data-processing model learns to sort the images\nwithout explicit supervision for sorting. While we train our model with datasets of size N= 50 , we\nshow a smaller instance with 5 images for better visualization.\nIdeally, the feature model Fshould learn 1d features encoding the relative ordering of the numbers,\nthe data model sorts them, and the query model should do some form of interpolation search where\nit can use the fact that the data distribution is uniform to do better than binary search. This is almost\nexactly what all models learn to do, from scratch, in an end-to-end fashion, without any explicit\nsupervision about which image encodes which number. In Figure 6 (Right) we plot the learned\nfeatures of the model. We find that the data model learns to sort the features (Figure 7) with 98%\naccuracy and the query model finds the nearest neighbor with almost 100% accuracy (Figure 12).\n2.5 L EVERAGING EXTRA SPACE\n021222324252627\n# Extra Spaces0.20.40.60.8Accuracy\nModel\nE2E\nBucket Baseline\nFigure 8: For NN search in 1D the\nmodel learns to use extra space and\noutperforms a bucketing baseline.The previous experiments demonstrate our model’s ability to\nlearn useful orderings for efficient querying. However, data\nstructures can also store additional pre-computed information\nto speed up querying. For instance, with infinite extra space,\na data structure could store the nearest neighbor for every pos-\nsible query, enabling O(1)search. Here, we evaluate if our\nmodel can effectively use extra space.\nWe run an experiment where the data and query distri-\nbution are uniform over (−1,1)with N= 50 , M =\n2. We allow the data-processing network to output T∈\n{0,21,22,23,24,25,26,27}numbers b1, ..., b T∈Rin addi-\ntion to the Nrankings. We plot the NN accuracy as a function\nofTin Figure 6 (Right) compared to a simple bucketing base-\nline. This baseline partitions [−1,1]intoTevenly-sized buck-\nets and in each bucket stores arg minxj∈D||xj−li||where li\nis the midpoint of the segment corresponding to bucket i. The\nbaseline maps a query to its corresponding bucket and predicts\nthe input stored in that bucket as the nearest-neighbor. Our model’s accuracy monotonically in-\ncreases with extra space demonstrating that the data-processing network learns to pre-compute use-\nful statistics that enable more efficient querying. We provide some insights into the learned solution\nin App C.4 and show that our model can be trained to use extra space in the high-dimensional case\nas well (App C.5).\n3 B EYOND NEAREST NEIGHBOR SEARCH\nMany other data structure problems beyond nearest neighbor search can be modeled by our frame-\nwork. Here, we illustrate this broader applicability by applying the framework to the classical prob-\nlem of frequency estimation : a memory-constrained model observes a stream of elements, and is\nsubsequently asked to approximate the number of times a query element has appeared (Cormode &\nMuthukrishnan, 2005; Cormode & Hadjieleftheriou, 2010). In Section 3.2 we describe several other\ndata structure problems that the framework can be applied to.\n9\n\n3.1 F REQUENCY ESTIMATION\nGiven a sequence of Telements e(1), ..., e(T)drawn from some universe, the task is to estimate the\nfrequency of a query element equp until time-step T. Specifically, we aim to minimize the mean\nabsolute error4between the true count and the estimated count. As in the nearest neighbor setup,\nthe two constraints of interest are the size of the data structure and the number of lookups for query\nexecution. Consequently, our framework can be easily adapted to model this problem. We also\nchoose this problem to highlight the versatility of our framework as it can be applied to streaming\nsettings.\nData processing Network We model the data structure as a kdimensional vector ˆDand use an\nMLP as the data-processing network which is responsible for writing to ˆD. When a new element\narrives in our stream, we allow the model to update M values in the data structure. Specifically,\nwhen an element arrives at time-step t, the data-processing network outputs M k -dimensional one-\nhot update position vectors u1, ..., u Mand M corresponding scalar update values v1, ..., v M. We\nthen apply the update, obtaining ˆDt+1=ˆDt+PM\ni=1ui∗vi. Unlike in the NN setting where we did\nnot constrain the construction complexity of the data structure, here we have limited each update to\nthe data structure to a budget of Mlookups. We do so as in the streaming settings updates typically\noccur often, so it is less reasonable to consider them as a one-time construction overhead cost.\nQuery processing Network Query processing is handled in a similar fashion to NN search — we\nhaveMquery MLP models that output lookup positions. Finally, we also train a MLP predictor\nnetwork ψ(v1, ..., v M)that takes in the Mvalues retrieved from the lookups and outputs the final\nprediction.\nEXPERIMENTS\n20 40 60 80 100\nStream Index012345Mean Absolute Error\n# Queries\n1\n2\n4\nModel\nE2E\nCountMinSketch\nFigure 9: When estimating fre-\nquencies of elements drawn from\na randomly ordered Zipfian distri-\nbution, our model outperforms the\nCountMinSketch baseline given 1,\n2, and 4 queries.Zipfian Distribution We evaluate our model in a setting\nwhere both the stream and query distributions follow a Zipfian\ndistribution. This simulates a common feature of frequency-\nestimation datasets where a few “heavy hitter” elements are\nupdated or queried more frequently than others (Hsu et al.,\n2019). For any fixed training instance, the rank order of the el-\nements in the domain is consistent across both the stream and\nquery distributions, but it is randomized across different train-\ning instances. As a result, the model cannot rely on knowing\nwhich specific elements are more frequent than others; only\nthe overall Zipfian skew is consistent across training instances.\nWe use a data structure of size k= 32 and train our model\nwithM∈ {1,2,4}queries. Both the data and query dis-\ntributions are Zipfian over {1, ...,1000}with a fixed skew of\nα= 1.2. We evaluate the mean absolute error over streams of\nlength 100 and compare with the CountMinSketch algorithm,\na hashing-based method for frequency estimation (Cormode\n& Muthukrishnan, 2005) (See App. E for an overview). Our\nmodel’s performance improves with more queries and outper-\nforms CountMinSketch (Figure 9). In this case, CountMinS-\nketch degrades with more queries as for a fixed size memory ( k= 32 ), it is more effective for this\ndistribution to apply a single hash function over the whole memory than to split the memory into k\npartitions of size k/M and use separate hash functions. We look at the learned algorithm in more\ndetail and find that our model learns an algorithm similar to CountMinSketch, but with an important\ndifference: it uses an update delta of less than 1 when a new item arrives, instead of the delta of\n1 used by CountMinSketch. We find that this can be particularly useful when the size of the data\nstructure is small and collisions are frequent. We hypothesize that the better performance of the\nlearned solution is at least partially due to the smaller delta. In Figure 22, we show that we are able\n4We use absolute error as this is the metric commonly used in prior work (Cormode & Muthukrishnan,\n2005; Cormode & Hadjieleftheriou, 2010) but our setup works for squared error as well.\n10\n\nto recover the performance of our learned data structure with 1 and 2 queries when we use a smaller\ndelta.\nLearning Heavy Hitter Features In the previous experiment, the Zipfian distribution shape was\nfixed across training instances but the rank ordering of elements was random. In some settings,\nhowever, it may be possible to predict which elements are more likely to occur in the stream. While\nthe exact elements may vary between streams, frequently occurring items could share features across\nstreams. For instance, Hsu et al. (2019) show that in frequency estimation for network flows, certain\ntypes of IP addresses receive much more traffic than others. We simulate such a setting by fixing the\nrank ordering of the Zipfian distribution. However, instead of using a universe of integer elements\n{1, ..., K}, we instead use their corresponding 3-digit MNIST images with K= 100 (constructed\nas in the MNIST NN experiment). Given a stream of integers, we map them to their corresponding\nMNIST labels and then for each label we sample a corresponding image from the training set.\nDuring evaluation, we use images samples from the test set. As the distribution is skewed and the\nranking is fixed, images with smaller numbers are sampled much more frequently than those with\nlarger numbers. As in the MNIST NN experiment, we also use a feature-learning CNN model to\nprocess the images before passing them to the data-processing and query-execution networks.\nWe compare our model to CountMinSketch with 1-query that is given the underlying labels instead\nof the images. Our model has a significantly lower error than the baseline (0.15 vs 2.81 averaged\nover a stream of size 100 (see Fig. 23)) as the latter is distribution-independent. By training from the\ndata-distribution end-to-end, our framework is able to simultaneously learn features of heavy hitters\n(in this case, clustering images with the same label) and use this information to design an efficient\nfrequency estimation data structure. We investigate the learned structure and find that the model has\nreserved separate memory positions for heavy hitters, thereby preventing collisions (Fig. 24).\n3.2 O THER POTENTIAL APPLICATIONS\nHere, we outline several other potential applications of our framework to facilitate future work.\nGraph data structures : Many graph-related problems require an efficient representation to sup-\nport connectivity or distance queries between vertices. For distance queries, one approach is to use\nquadratic space to store the distances between all vertex pairs, allowing O(1)query time. Alter-\nnatively, one could use no extra space and simply store the graph (which may require significantly\nless than quadratic space) and run a shortest-path algorithm at query time. The challenge is to find\na middle ground: using sub-quadratic space while still answering distance queries faster than a full\nshortest-path computation (Thorup & Zwick, 2005).\nSparse matrices: Another common problem that can be framed as a data structure problem is that\nof compressing sparse matrices. Given an M×Nmatrix, on one hand, one could store the full\nmatrix and access elements in O(1)time. However, depending on the number and distribution of 0s\nin the matrix, different data structures could be designed that use less than O(MN)space. There is\nan inherent trade-off between how compressed the representation is and the time required to access\nelements of the matrix to solve various linear algebraic tasks involving the matrix such as matrix-\nvector multiplication (Buluc ¸ et al., 2011; Chakraborty et al., 2018).\nLearning statistical models: Our framework can also handle problems such as learning statistical\nmodels like decision trees, where the input to the data-processing network is a training dataset, and\nthe output is a model such as a decision tree. The query algorithm would then access a subset of the\nmodel at inference time, such as by doing a traversal on the nodes of the decision tree. This could\nbe used to explore questions around optimal algorithms and heuristics for learning decision tress,\nwhich are not properly understood (Blanc et al., 2021; 2022).\n4 R ELATED WORK\nLearning-Augmented Algorithms Recent work has shown that traditional data structures and\nalgorithms can be made more efficient by learning properties of the underlying data distribution.\nKraska et al. (2018) introduced the concept of learned index structures which use ML models to\nreplace traditional index structures in databases, resulting in significant performance improvements\nfor certain query workloads. By learning the cumulative distribution function of the data distribution\n11\n\nthe model has a stronger prior over where to start the search for a record, which can lead to provable\nimprovements to the query time over non-learned structures (Zeighami & Shahabi, 2023). Other\nworks augment the data structure with predictions instead of the query algorithm. For example, Lin\net al. (2022a) use learned frequency estimation oracles to estimate the priority in which elements\nshould be stored in a treap. Perhaps more relevant to the theme of our work is Dong et al. (2019),\nwhich trains neural networks to learn a partitioning of the space for efficient nearest neighbor search\nusing locality sensitive hashing, and the body of work on learned hash functions (Wang et al., 2015;\nSabek et al., 2022). While all these works focus on augmenting data structure design with learning,\nwe explore whether data structures can be discovered entirely end-to-end using deep learning.\nNeural Algorithmic Learners There is a significant body of work on encoding algorithms into\ndeep networks. Graves et al. (2014) introduced the Neural Turing Machine (NTM), which uses\nexternal memory to learn tasks like sorting and copying. Veli ˇckovi ´c et al. (2019) used graph neural\nnetworks (GNNs) to encode classical algorithms such as breadth-first search. These works train\ndeep networks with a great degree of supervision with the aim of encoding known algorithms. For\ninstance, Graves et al. (2014) use the ground truth sorted list as supervision to train the model to sort.\nThere has also been work on learning algorithms in an end-to-end fashion. Fawzi et al. (2022) train a\nmodel using reinforcement learning to discover matrix multiplication algorithms, while Selsam et al.\n(2018) train neural networks to solve SAT problems. Garg et al. (2022) show that transformers can\nbe trained to encode learning algorithms for function classes such as linear functions and decision\ntrees. Our work adds to this line of research on end-to-end learning, focusing on discovering data\nstructures.\n5 C ONCLUSION\nWe began with the question of whether deep learning models can be trained to discover data struc-\ntures from scratch. This work provides initial evidence that it is possible. For both nearest neighbor\nsearch and frequency estimation, the models—trained end-to-end—discover distribution-dependent\ndata structures that outperform worst-case baselines. We hope this research inspires further explo-\nration into data structure and algorithm discovery.\nOne limitation that future research could address is scale. Due to computational constraints, most of\nour experiments are conducted with datasets of size N= 100 , although in App. F we scale some to\nN= 500 . While this scale can be sufficient for gaining insights into data structure design, practical\nend-to-end use would require further scaling. We believe both larger models and better inductive\nbiases could enable scaling up further (see App. F for details).\n6 A CKNOWLEDGEMENTS\nOS and LC acknowledge the support of the CIFAR AI Chair program. This research was also\nenabled in part by compute resources provided by Mila – Quebec AI Institute (mila.quebec). GV is\nsupported by a Simons Foundation Investigator Award, NSF award AF-2341890 and UT Austin’s\nFoundations of ML NSF AI Institute. VS was supported by NSF CAREER Award CCF-2239265\nand an Amazon Research Award.\nREFERENCES\nEkin Aky ¨urek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algo-\nrithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661 ,\n2022.\nAlexandr Andoni, Piotr Indyk, and Ilya Razenshteyn. Approximate nearest neighbor search in high\ndimensions. In Proceedings of the International Congress of Mathematicians: Rio de Janeiro\n2018 , pp. 3287–3318. World Scientific, 2018.\nSunil Arya, David M Mount, Nathan S Netanyahu, Ruth Silverman, and Angela Y Wu. An optimal\nalgorithm for approximate nearest neighbor searching fixed dimensions. Journal of the ACM\n(JACM) , 45(6):891–923, 1998.\n12\n\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450 , 2016.\nGuy Blanc, Jane Lange, Mingda Qiao, and Li-Yang Tan. Decision tree heuristics can fail, even in\nthe smoothed setting. arXiv preprint arXiv:2107.00819 , 2021.\nGuy Blanc, Jane Lange, Mingda Qiao, and Li-Yang Tan. Properly learning decision trees in almost\npolynomial time. Journal of the ACM , 69(6):1–19, 2022.\nAydın Buluc ¸, John Gilbert, and Viral B Shah. Implementing sparse matrices for graph algorithms.\nGraph Algorithms in the Language of Linear Algebra , pp. 287–313, 2011.\nDiptarka Chakraborty, Lior Kamma, and Kasper Green Larsen. Tight cell probe bounds for suc-\ncinct boolean matrix-vector multiplication. In Proceedings of the 50th Annual ACM SIGACT\nSymposium on Theory of Computing , pp. 1297–1306, 2018.\nGraham Cormode and Marios Hadjieleftheriou. Methods for finding frequent items in data streams.\nThe VLDB Journal , 19:3–20, 2010.\nGraham Cormode and Shan Muthukrishnan. An improved data stream summary: the count-min\nsketch and its applications. Journal of Algorithms , 55(1):58–75, 2005.\nMarco Cuturi, Olivier Teboul, and Jean-Philippe Vert. Differentiable ranking and sorting using\noptimal transport. Advances in neural information processing systems , 32, 2019.\nJialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li, Hantian Zhang,\nBadrish Chandramouli, Johannes Gehrke, Donald Kossmann, et al. Alex: an updatable adaptive\nlearned index. In Proceedings of the 2020 ACM SIGMOD International Conference on Manage-\nment of Data , pp. 969–984, 2020.\nYihe Dong, Piotr Indyk, Ilya Razenshteyn, and Tal Wagner. Learning space partitions for nearest\nneighbor search. arXiv preprint arXiv:1901.08544 , 2019.\nAlhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Moham-\nmadamin Barekatain, Alexander Novikov, Francisco J R Ruiz, Julian Schrittwieser, Grzegorz\nSwirszcz, et al. Discovering faster matrix multiplication algorithms with reinforcement learning.\nNature , 610(7930):47–53, 2022.\nDeqing Fu, Tian-Qi Chen, Robin Jia, and Vatsal Sharan. Transformers learn higher-order op-\ntimization methods for in-context learning: A study with linear models. arXiv preprint\narXiv:2310.17086 , 2023.\nShivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn\nin-context? a case study of simple function classes. Advances in Neural Information Processing\nSystems , 35:30583–30598, 2022.\nAlex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint\narXiv:1410.5401 , 2014.\nAditya Grover, Eric Wang, Aaron Zweig, and Stefano Ermon. Stochastic optimization of sorting\nnetworks via continuous relaxations. In International Conference on Learning Representations ,\n2019. URL https://openreview.net/forum?id=H1eSS3CcKX .\nChen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency estimation\nalgorithms. In International Conference on Learning Representations , 2019. URL https:\n//openreview.net/forum?id=r1lohoCqY7 .\nEric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. 2017.\nURL https://arxiv.org/abs/1611.01144 .\nAndrej Karpathy. nanogpt. https://github.com/karpathy/nanoGPT , 2024. Accessed:\n2024-05-28.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.\n13\n\nJon M Kleinberg. Two algorithms for nearest-neighbor search in high dimensions. In Proceedings\nof the twenty-ninth annual ACM symposium on Theory of computing , pp. 599–608, 1997.\nTim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned\nindex structures. In Proceedings of the 2018 international conference on management of data ,\npp. 489–504, 2018.\nHonghao Lin, Tian Luo, and David Woodruff. Learning augmented binary search trees. In Interna-\ntional Conference on Machine Learning , pp. 13431–13440. PMLR, 2022a.\nHonghao Lin, Tian Luo, and David P. Woodruff. Learning augmented binary search trees, 2022b.\nURLhttps://arxiv.org/abs/2206.12110 .\nThodoris Lykouris and Sergei Vassilvitskii. Better caching with machine learned advice. 2018.\nMichael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. Communications of\nthe ACM , 65(7):33–35, 2022.\nVinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In\nProceedings of the 27th international conference on machine learning (ICML-10) , pp. 807–814,\n2010.\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,\nZeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in\npytorch. 2017.\nFelix Petersen, Christian Borgelt, Hilde Kuehne, and Oliver Deussen. Monotonic differentiable\nsorting networks, 2022.\nW. W. Peterson. Addressing for random-access storage. IBM Journal of Research and Development ,\n1(2):130–146, 1957. doi: 10.1147/rd.12.0130.\nIbrahim Sabek, Kapil Vaidya, Dominik Horn, Andreas Kipf, Michael Mitzenmacher, and Tim\nKraska. Can learned models replace hash functions? Proceedings of the VLDB Endowment ,\n16(3), 2022.\nDaniel Selsam, Matthew Lamm, Benedikt B ¨unz, Percy Liang, Leonardo de Moura, and David L\nDill. Learning a SAT solver from single-bit supervision. arXiv preprint arXiv:1802.03685 , 2018.\nMikkel Thorup and Uri Zwick. Approximate distance oracles. Journal of the ACM (JACM) , 52(1):\n1–24, 2005.\nPetar Veli ˇckovi ´c, Rex Ying, Matilde Padovano, Raia Hadsell, and Charles Blundell. Neural execu-\ntion of graph algorithms. arXiv preprint arXiv:1910.10593 , 2019.\nJohannes V on Oswald, Eyvind Niklasson, Ettore Randazzo, Jo ˜ao Sacramento, Alexander Mordv-\nintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient\ndescent. In International Conference on Machine Learning , pp. 35151–35174. PMLR, 2023.\nJun Wang, Wei Liu, Sanjiv Kumar, and Shih-Fu Chang. Learning to hash for indexing big data—a\nsurvey. Proceedings of the IEEE , 104(1):34–57, 2015.\nSepanta Zeighami and Cyrus Shahabi. On distribution dependent sub-logarithmic query time of\nlearned indexing. In International Conference on Machine Learning , pp. 40669–40680. PMLR,\n2023.\n14\n\nAPPENDIX\nA T RAINING DETAILS\nA.1 N EAREST NEIGHBORS\nThe transformer in the data-processing network is based on the NanoGPT architecture (Karpathy,\n2024) and has 8 layers with 8 heads each and an embedding size of 64. Each query model Qi\nis a 3-layer MLP with a hidden dimension of size 1024. Each hidden layer consists of a linear\nmapping followed by LayerNorm (Ba et al., 2016) and the ReLU activation function (Nair & Hinton,\n2010). In all experiments we use a batch size of 1024, 1e-3 weight decay and the Adam optimizer\n(Kingma & Ba, 2017) with default PyTorch (Paszke et al., 2017) settings. We do a grid search over\n{0.0001,0.00001 ,0.00005}to find the best learning rate for both models. All models are trained\nfor at most 4 million gradient steps with early-stopping. We apply the Gumbel Softmax (Jang et al.,\n2017) with a temperature of 2to the lookup vectors to encourage sparsity. All experiments are run\non a single NVIDIA RTX8000 GPU.\nA.2 F REQUENCY ESTIMATION\nWe follow the same setup as the nearest neighbors training except for frequency estimation, the data-\nprocessing network is a 3-layer MLP with a hidden dimension of size 1024. We do a grid search\nover{0.0001,0.00005 ,0.00001}to find the best learning rate for both models. Models are trained\nfor 200k gradient steps with early stopping. All experiments are run on a single NVIDIA RTX8000\nGPU.\nB A DDITIONAL NEAREST NEIGHBOR PERFORMANCE PLOTS\n1 2 3 4 5 6\nLookup Index0.00.10.20.30.40.50.60.7Accuracy\nModel\nE2E\nE2E (frozen)\nE2E (non-adaptive)\nKD Tree\nFigure 10: 2D Uniform Accuracy.\n1 2 3 4 5 6 7\nLookup Index0104\n103\n102\n101\nSquare Error\nModel\nE2E\nE2E (frozen)\nE2E (non-adaptive)\nE2E (no-permute)\nBinary Search\n1 2 3 4 5 6\nLookup Index102\n101\nSquare Error\nModel\nE2E\nE2E (frozen)\nE2E (non-adaptive)\nKD Tree\n1 2 3 4 5 6\nLookup Index0.0350.0400.0450.0500.0550.060Square Error\nModel\nE2E\nE2E (frozen)\nE2E (non-adaptive)\nLSH (K=3)\nLSH (K=4)\nFigure 11: Mean square error plots for (Left) 1D Uniform distribution, (Center) 2D Uniform dis-\ntribution, (Right) 30D Uniform distribution over unit hyper-sphere.\n15\n\n1 2 3 4 5 6 7\nLookup Index0.00.20.40.60.81.0Accuracy\nModel\nE2E\nBinary Search*Figure 12: 3-Digit MNIST Nearest Neighbors Accuracy. Even though binary search (over the un-\nderlying digits) is an unfair comparison, we include it as a reference to compare our model’s perfor-\nmance with.\nFigure 13: Samples from 3-Digit MNIST\nB.1 H ARD DISTRIBUTION\nTo generate data from the hard distribution, we first sample the element at the 50th percentile from\nthe uniform distribution over a large range. We then sample the 25th and 75th percentile elements\nfrom a smaller range and so on. The intuition behind this distribution is to reduce concentration such\nthatp(NN|q)is roughly uniform where NN denotes the index of the nearest-neighbor of qin the\nsorted list.\nPrecisely, to sample Npoints from the hard distribution we generate a random balanced binary tree\nof size N. All vertices are random variables of the form Uniform (0, alogn−k)where ais some\nconstant and kis the level in the tree that the vertice belongs to. If the i−thnode in the tree is the\nleft-child of its parent, we generate the point xiasxi=xp(i)−diwhere p(i)denotes the parent\nof the i−thnode and diis a sample from node iof the random binary tree. Similarly, if node i\nis the right child of its parent, xi=xp(i)+di. For the root element x0=d0. In our experiments\nwe set a= 7. The larger the value of a, the greater the degree of anti-concentration. We found\nit challenging to train models with N > 16as the range of values that xican take increases with\nN. Thus for larger N, the model needs to deal with numbers at several scales, making learning\nchallenging.\n16\n\n1 2 3 4\nLookup Index0.20.40.60.81.0Accuracy\nModel\nE2E\nBinary SearchFigure 14: Our model’s performance is closely aligned with binary search on the hard distribution\nin 1D. By design, this distribution does not have a useful prior our model can exploit and so it learns\na binary search like solution.\nFigure 15: The positional distribution per lookup in the 1D Hard experiment. Our model closely\naligns with binary search, first looking at the middle element, then (approximately) either the 25th\nor 75th percentile elements, and so on.\n17\n\nFigure 16: Binary Search vs. our model on the hard distribution in 1D.\n18\n\n1 2 3 4\nLookup Index0.20.40.60.81.0Accuracy\nModel\nE2E\nKD TreeFigure 17: On the 2D hard distribution our model roughly tracks the performance of a k-d tree.\nC A DDITIONAL EXPERIMENT FINDINGS\nC.1 N OISE INJECTION FOR LOOKUP SPARSITY\nWe find that adding noise prior to applying the soft-max on the lookup vector mileads to sparser\nqueries. We hypothesize that this is because the noise injection forces the model to learn a noise-\nrobust solution which corresponds to a sparse solution. Consider a simplified setup in 1D where the\nquery model is not conditioned on qand is only allowed one lookup ( M= 1) and Dis a sorted\nlist of three elements: D= [x1, x2, x3]. For a given query qand its nearest neighbor y, the query-\nexecution network is trying to find the optimal vector ˆm∈R3that minimizes ||y−mTD||2\n2where\nm=softmax ( ˆm+ϵ), ϵ∼Gumbel distribution Jang et al. (2017). Given that M= 1, the model\ncannot always make enough queries to identify yand so in the absence of noise the model may try\nto predict the ’middle’ element by setting ˆm1= ˆm2= ˆm3. However, when noise is added to the\nlogits ˆmthis solution is destabilized. Instead, in the presence of noise, the model can robustly select\nthe middle element by making ˆm2much greater than ˆm1,ˆm3. We test this intuition by running this\nexperiment for large values of Nand find that with noise the average gradient is much larger for\nˆmN/2.\nC.2 2D U NIFORM DISTRIBUTION\nC.3 N=8, M=1 30D E XPERIMENT\nTo determine if our model has learned an LSH-like solution, we try to reverse engineer the query\nmodel in a simple setting where N= 8 andM= 1. The query-execution model is only allowed\none lookup. We fit 8 one-vs-rest logistic regression classifiers using queries sampled from the query\ndistribution and the output of the query model (lookup position) as features and labels, respectively.\nWe then do PCA on the set of 8 classifier coefficients. We find that the top 2 principal components\nexplain all of the variance which suggests that the query model’s mapping can be explained by the\nprojection onto these two components. In Figure 19 we plot the projection of queries onto these\ncomponents and color them based on the position they were assigned by the query model. We do the\nsame for inputs xi∈Dand color them by the position they were permuted to. The plot on the right\nsuggests that the data-processing network permutes the input vectors based on their projection onto\nthese two components. This assignment is noisy because there may be multiple inputs in a dataset\nthat map to the same bucket and because the model can only store a permutation, some buckets\nexperience overflow. Similarly, the query model does a lookup in the position that corresponds to\nthe query vector’s bucket. This behaviour suggests the model has learned a locality-sensitive hashing\ntype solution!\n19\n\nFigure 18: k-d search vs. our model on the uniform distribution in 2D. Unlike the k-d tree, our\nmodel has a stronger prior over where to begin its search.\nC.4 1D E XTRA SPACE\nC.4.1 B UCKET BASELINE\nWe create a simple bucket baseline that partitions [−1,1]intoTevenly sized buckets. In each bucket\nbiwe store argmin xj∈D||xj−li||where liis the midpoint of the segment partitioned in bi. This\nbaseline maps a query to its corresponding bucket and predicts the input stored in that bucket as the\nnearest-neighbor. As T→ ∞ this becomes an optimal hashing-like solution.\nC.4.2 U NDERSTANDING EXTRA SPACE USAGE\nBy analyzing the lookup patterns of the first query model, we can better understand how the model\nuses extra space. In Figure 20 we plot the decision boundary of the first query model. The plot\ndemonstrates that the model chunks the query space ([−1,1])into different buckets. To get a sense\nof what the model stores in the extra space, we fit a linear function on the sorted inputs and regress\n20\n\nFigure 19: (Left) Projection of queries onto top two PCA components of the decision boundaries of\nthe query model, colored by the lookup position the query is mapped to. (Right) Projection of inputs\nonto the same PCA components colored by the position the data-processing model places them in.\nBoth the data-processing and query models map similar regions to the same positions, suggesting\nan LSH-like bucketing solution has been learned.\n0 10 20 30\n(Sorted) Input Position0.2\n0.00.20.40.6Regression CoefficientExtra space position: 39\n0 10 20 30\n(Sorted) Input Position0.00.20.40.6Regression CoefficientExtra space position: 38\n0 10 20 30\n(Sorted) Input Position0.00.20.40.6Regression CoefficientExtra space position: 37\n0 10 20 30\n(Sorted) Input Position0.2\n0.00.20.40.6Regression CoefficientExtra space position: 35\n0 10 20 30\n(Sorted) Input Position0.000.250.500.75Regression CoefficientExtra space position: 32\n0 10 20 30\n(Sorted) Input Position0.000.250.500.751.00Regression CoefficientExtra space position: 36\nFigure 20: (Top) Decision boundary of the first query model. (Bottom) The regression coefficients\nof the values stored in extra positions as a linear function of the (sorted) inputs.\nthe values stored in each of the extra space tokens biand plot the coefficients for several of the extra\nspaces in Figure 20. For a given subset of the query range, the value stored at its corresponding\nextra space is approximately a weighted sum of the values stored at the indices that correspond to\nthe percentile of that query range subset. This is useful information as it tells the model for a given\nquery percentile how ’shifted’ the values in the current dataset stored in the corresponding indices\nare from model’s prior.\n21\n\nC.5 30D E XTRA SPACE\n021222324252627\n# Extra Spaces0.10.20.30.40.50.6Accuracy\nModel\nE2E\nE2E (Coefficients)\nLSH Baseline\nFigure 21: Our unconstrained model (E2E) and a more interpretable version (E2E (Coefficients))\nboth learn to effectively leverage an increasing amount of extra space in 30D, with the unconstrained\nmodel outperforming an LSH baseline.\nIn high-dimensions it is less clear what solutions there are to effectively leverage extra space, and in\nfact understanding optimal tradeoffs in this case is open theoretically (Arya et al., 1998).\nWe follow a similar setup to the 1D extra space experiments but use the data and query distributions\nfrom section 2.4. We experiment with two versions of extra space (unrestricted) and (coefficients).\nFor the unrestricted version the data model can store whatever 30 dimensional vector it chooses in\neach of the extra spaces. For the coefficient model, instead of outputting a 30 dimensional vector, for\neach extra space, the model outputs a separate N dimensional vector of coefficients. We then take\na linear combination of the (permuted) input dataset using these coefficients and store the resulting\nvector in the corresponding extra positions. While the unrestricted version is more expressive the\ncoefficient version is more interpretable. We include both versions to demonstrate the versatility\nof our framework. If one is only interested in identifying a strong lower-bound of how well one\ncan use a fixed budget of extra space they may use the unrestricted model. However, if they are\nmore concerned with investigating specific classes of solutions or would like a greater degree of\ninterpretability they can easily augment the model with additional inductive biases such as linear\ncoefficients.\nWe plot the performance of both models along with an LSH baseline in Figure 21. While both mod-\nels perform competitively with an LSH baseline and can effectively leverage an increasing amount\nof extra space, the unrestricted model outperforms the coefficient model at a certain point.\nC.6 F REQUENCY ESTIMATION\nD LSH B ASELINE\nOur LSH baseline samples Krandom vectors r1, ...,rKfrom the standard normal distribution in Rd.\nFor a given vector v∈Rd, its hash code is computed as hash(v) = [sign(vTr1), ..., sign (vTrK)].\nIn total, there are 2Kpossible hash codes. To create a hash table, we assign each hash code a bucket\nof size N/2K. For a given dataset D={x1, ..., x N}, we place each input in its corresponding\nbucket (determined by its hash code hash(xi). If the bucket is full, we place xiin a vacant bucket\nchosen at random. Given a query qand a budget of Mlookups, the baseline retrieves the first M\nvectors in the bucket corresponding to hash(q). If there are less than Mvectors in the bucket, we\nchoose the remaining vectors at random from other buckets. We design this setup like so to closely\nalign with the constraints of our model (i.e. only learning a permutation).\n22\n\n0 20 40 60 80 100\nStream Index0.00.51.01.52.02.53.0Mean Absolute ErrorM=1_delta=1\nM=1_learned_delta\nM=2_delta=1\nM=2_learned_deltaFigure 22: We apply the insight that we learned from our E2E model to improve CountMinSketch on\nthe Zipfian distribution. By changing the CountMinSketch update delta of 1 to our model’s learned\ndelta ( ∆ = 0 .87forM= 1 and∆ = 0 .93forM= 2), we can improve the performance of\nCountMinSketch on M∈ {1,2}queries.\n0 20 40 60 80 100\nStream Index012345Mean Absolute Error\nModel\nE2E\nCountMinSketch\nFigure 23: On the MNIST heavy-hitters frequency estimation experiment, our model significantly\noutperforms CountMinSketch. This is because our model can learn features predictive of heavy\nhitters, as opposed to the distribution-agnostic CountMinSketch.\nE C OUNT MINSKETCH\nCountMinSketch (Cormode & Muthukrishnan, 2005) is a probabilistic data structure used for esti-\nmating the frequency of items in a data stream with sublinear space. It uses a two-dimensional array\nof counters and multiple independent hash functions to map each item to several buckets. When a\nnew item xarrives, the algorithm computes dhash functions h1(x), h2(x), . . . , h d(x), each of which\nmaps the item to one of wbuckets in different rows of the array. The counters in the corresponding\nbuckets are incremented by 1. To estimate the frequency of an item x, the minimum value across\nall counters C[1, h1(x)], C[2, h2(x)], . . . , C [d, hd(x)]is returned. The sketch guarantees that the\nestimated frequency ˆf(x)of an item xis at least its true frequency f(x), and at most f(x) +ϵN,\nwhere Nis the total number of items processed, ϵ=1\nw, and wis the width of the sketch. The\nprobability that the estimate exceeds this bound is at most δ=1\nd, where dis the depth of the sketch\n23\n\nFigure 24: We show the decision boundary learned by the query/data-processing network in the\nMNIST heavy hitters experiment. As images with smaller numbers occur more frequently in the\nstream, the memory-constrained model learns to reserve separate memory positions for these items\nin order to prevent collisions among them.\n(i.e., the number of hash functions). These guarantees hold even in the presence of hash collisions,\nproviding strong worst-case accuracy with O(w·d)space.\nF L IMITATIONS AND FUTURE WORK\n1 2 3 4 5\nLookup Index0.00.10.20.30.4Accuracy\nModel\nE2E\nBinary Search\n1 2 3 4 5\nLookup Index0.04250.04500.04750.05000.05250.05500.05750.0600Mean Square Error\nModel\nE2E (non-adaptive)\nLSH (K=5)\nLSH (K=6)\nLSH (K=7)\n1 2 3 4 5 6 7\nLookup Index0.00.20.40.60.81.0Accuracy\nModel\nE2E\nE2E (shared)\nFigure 25: We scale both the 1D ( Left) and 30D ( Center ) experiments to datasets of size N= 500 .\n(Right ) We compare our E2E model with a version where the query-execution network is only\ncomposed of one query-model (E2E (shared)) that is used in a loop for M= 7 queries during\ntraining on the 1D Uniform distribution, thereby conserving parameters by reusing weights. This\ncould be a promising direction for problem settings where there is a recursive structure to the query\nalgorithm.\nOne limitation of our work is the scale at which we learn data structures. Most of our nearest\nneighbor search experiments are done with input dataset sizes around N= 100 , however, we are\nalso able to scale up to N= 500 (Figure 25 (Left/Center)), though with less than log(N)queries.\nWhile we demonstrate that useful data structures can still be learned at this scale, it is possible that\nother classes of structures only emerge for larger datasets. We also believe that many of the insights\nthat can be derived from our models’ learned solutions would scale to larger N. For instance,\nsorting in 1D and locality-sensitive hashing in higher dimensions. We limit ourselves to datasets\nof these sizes due to computational constraints, and because our primary goal was to understand\nwhether end-to-end data structure design is feasible at any reasonable scale. However, we believe\nour framework could scale to datasets with thousands of points by increasing the parameter counts\nof the data-processing and query-execution models. Moreover, as transformers become increasingly\n24\n\nefficient at handling larger context sizes in language modeling settings, some of these modeling\nadvancements may also be used for scaling models in the context of data structure discovery.\nComplementary to our work, it could also be valuable to explore better inductive biases for the query\nand data-processing networks, and other methods to ensure sparse lookups, enabling smaller models\nto scale to larger datasets. For instance, using shared weights among query models can be helpful\nin scaling up the number of queries. As a first step in this direction we show that a single query\nmodel can be used in-a-loop for NN search in 1D (Figure 25 (Right)). However, we leave further\ninvestigation for future work.\n25",
  "textLength": 70196
}