{
    "paperId": "a5c8bb65f59f5990b750e34f9fb9f5147ed8c666",
    "title": "Meta-Learning Neural Bloom Filters",
    "year": 2019,
    "venue": "International Conference on Machine Learning",
    "authors": [
        "Jack W. Rae",
        "Sergey Bartunov",
        "T. Lillicrap"
    ],
    "doi": null,
    "arxivId": "1906.04304",
    "url": "https://www.semanticscholar.org/paper/a5c8bb65f59f5990b750e34f9fb9f5147ed8c666",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle",
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "There has been a recent trend in training neural networks to replace data structures that have been crafted by hand, with an aim for faster execution, better accuracy, or greater compression. In this setting, a neural data structure is instantiated by training a network over many epochs of its inputs until convergence. In applications where inputs arrive at high throughput, or are ephemeral, training a network from scratch is not practical. This motivates the need for few-shot neural data structures. In this paper we explore the learning of approximate set membership over a set of data in one-shot via meta-learning. We propose a novel memory architecture, the Neural Bloom Filter, which is able to achieve significant compression gains over classical Bloom Filters and existing memory-augmented neural networks.",
    "citationCount": 38,
    "referenceCount": 39
}