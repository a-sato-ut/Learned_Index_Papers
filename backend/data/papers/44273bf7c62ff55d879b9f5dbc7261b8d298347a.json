{
    "paperId": "44273bf7c62ff55d879b9f5dbc7261b8d298347a",
    "title": "Enhancing the Reinforcement Learning-based Tuning System with a Dynamic Reward Function",
    "year": 2024,
    "venue": "2024 Sixth International Conference on Next Generation Data-driven Networks (NGDN)",
    "authors": [
        "Na Guo",
        "Wenli Sun",
        "Xiufeng Xia"
    ],
    "doi": "10.1109/NGDN61651.2024.10744094",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/44273bf7c62ff55d879b9f5dbc7261b8d298347a",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "Conference"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Existing reinforcement learning-based data indexes optimize structures using fixed objectives, making them unsuitable for dynamic environments with varying resources and latency requirements. Retraining models for changes in the environment is costly and impractical for online training. To overcome these issues, this paper introduces a dynamic reward function enhanced (DRF) reinforcement learning algorithm. The DRF allows modification of the reward function during training or testing, enabling adaptability without costly retraining. The proposed algorithm combines Genetic Algorithm and Deep Q-Network to determine the optimal solution under DRF. Continuous exploration of optimal actions and state spaces ensures the agent can provide optimal actions under different environments. The paper introduces a parameter matrix controlling the learning-based index, utilizing a reinforcement learning algorithm with a dynamic reward function to derive this matrix. This approach enables adaptation to dynamic hardware resources, varying memory overheads, and diverse query latency requirements. The experimental results demonstrate that, with the support of a dynamic reward function, the learned index surpasses the current state-of-the-art approaches and can adapt to dynamic environments.",
    "citationCount": 0,
    "referenceCount": 19
}