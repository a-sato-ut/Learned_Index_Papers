{
    "paperId": "7d4753683bbefcd1d06599774c09d0a7977b7567",
    "title": "A Tailored Regression for Learned Indexes: Logarithmic Error Regression",
    "year": 2021,
    "venue": "aiDM@SIGMOD",
    "authors": [
        "Martin Eppert",
        "Philipp Fent",
        "Thomas Neumann"
    ],
    "doi": "10.1145/3464509.3464891",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/7d4753683bbefcd1d06599774c09d0a7977b7567",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "Book",
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Although linear regressions are essential for learned index structures, most implementations use Simple Linear Regression, which optimizes the squared error. Since learned indexes use exponential search, regressions that optimize the logarithmic error are much better tailored for the use-case. By using this fitting optimization target, we can significantly improve learned indexâ€™s lookup performance with no architectural changes. While the log-error is harder to optimize, our novel algorithms and optimization heuristics can bring a practical performance improvement of the lookup latency. Even in cases where fast build times are paramount, log-error regressions still provide a robust fallback for degenerated leaf models. The resulting regressions are much better suited for learned indexes, and speed up lookups on data sets with outliers by over a factor of 2.",
    "citationCount": 8,
    "referenceCount": 22
}