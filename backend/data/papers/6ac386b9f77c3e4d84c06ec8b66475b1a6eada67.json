{
    "paperId": "6ac386b9f77c3e4d84c06ec8b66475b1a6eada67",
    "title": "Spreading vectors for similarity search",
    "year": 2018,
    "venue": "International Conference on Learning Representations",
    "authors": [
        "Alexandre Sablayrolles",
        "Matthijs Douze",
        "C. Schmid",
        "H. JÃ©gou"
    ],
    "doi": null,
    "arxivId": "1806.03198",
    "url": "https://www.semanticscholar.org/paper/6ac386b9f77c3e4d84c06ec8b66475b1a6eada67",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Discretizing multi-dimensional data distributions is a fundamental step of modern indexing methods. State-of-the-art techniques learn parameters of quantizers on training data for optimal performance, thus adapting quantizers to the data. In this work, we propose to reverse this paradigm and adapt the data to the quantizer: we train a neural net which last layer forms a fixed parameter-free quantizer, such as pre-defined points of a hyper-sphere. As a proxy objective, we design and train a neural network that favors uniformity in the spherical latent space, while preserving the neighborhood structure after the mapping. We propose a new regularizer derived from the Kozachenko--Leonenko differential entropy estimator to enforce uniformity and combine it with a locality-aware triplet loss. Experiments show that our end-to-end approach outperforms most learned quantization methods, and is competitive with the state of the art on widely adopted benchmarks. Furthermore, we show that training without the quantization step results in almost no difference in accuracy, but yields a generic catalyzer that can be applied with any subsequent quantizer.",
    "citationCount": 136,
    "referenceCount": 55
}