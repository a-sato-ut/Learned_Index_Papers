{
    "paperId": "e99c2702711e30792b6eba12cf3b3d9d356bb242",
    "title": "Pre-training Summarization Models of Structured Datasets for Cardinality Estimation",
    "year": 2021,
    "venue": "Proceedings of the VLDB Endowment",
    "authors": [
        "Yao Lu",
        "Srikanth Kandula",
        "A. König",
        "S. Chaudhuri"
    ],
    "doi": "10.14778/3494124.3494127",
    "arxivId": null,
    "url": "https://www.semanticscholar.org/paper/e99c2702711e30792b6eba12cf3b3d9d356bb242",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "We consider the problem of pre-training models which convert structured datasets into succinct summaries that can be used to answer cardinality estimation queries. Doing so avoids per-dataset training and, in our experiments, reduces the time to construct summaries by up to 100×. When datasets change, our summaries are incrementally updateable. Our key insights are to use multiple summaries per dataset, use learned summaries for columnsets for which other simpler techniques do not achieve high accuracy, and that analogous to similar pre-trained models for images and text, structured datasets have some common frequency and correlation patterns which our models learn to capture by pre-training on a large and diverse corpus of datasets.",
    "citationCount": 32,
    "referenceCount": 48
}