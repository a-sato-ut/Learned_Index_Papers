{
    "paperId": "86f014f66e484f477fe729023ee5d30537a299d6",
    "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
    "year": 2024,
    "venue": "arXiv.org",
    "authors": [
        "Xiaoniu Song",
        "Zihang Zhong",
        "Rong Chen"
    ],
    "doi": "10.48550/arXiv.2410.22134",
    "arxivId": "2410.22134",
    "url": "https://www.semanticscholar.org/paper/86f014f66e484f477fe729023ee5d30537a299d6",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "The promising applications of large language models are often limited by the constrained GPU memory capacity available on edge devices. Mixture-of-Experts (MoE) models help address this issue by activating only a subset of the model's parameters during computation. This approach allows the unused parameters to be offloaded to host memory, thereby reducing the overall GPU memory demand. However, existing cache-based offloading solutions handle cache misses reactively, which significantly impacts system performance. In this paper, we introduce ProMoE, a novel proactive caching system that utilizes intermediate results to predict subsequent expert usage. By proactively fetching experts in advance, ProMoE eliminates passive cache misses, removes loading time from the critical path, and reduces the performance overhead associated with offloading. Our evaluations demonstrate that ProMoE achieves an average speedup of 2.20x (up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages, respectively, compared to existing offloading solutions.",
    "citationCount": 11,
    "referenceCount": 42
}