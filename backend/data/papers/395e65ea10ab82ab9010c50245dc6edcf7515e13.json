{
    "paperId": "395e65ea10ab82ab9010c50245dc6edcf7515e13",
    "title": "A Lazy Approach for Efficient Index Learning",
    "year": 2021,
    "venue": "arXiv.org",
    "authors": [
        "Guanli Liu",
        "Lars Kulik",
        "Xingjun Ma",
        "Jianzhong Qi"
    ],
    "doi": null,
    "arxivId": "2102.08081",
    "url": "https://www.semanticscholar.org/paper/395e65ea10ab82ab9010c50245dc6edcf7515e13",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Learned indices using neural networks have been shown to outperform traditional indices such as B-trees in both query time and memory. However, learning the distribution of a large dataset can be expensive, and updating learned indices is difficult, thus hindering their usage in practical applications. In this paper, we address the efficiency and update issues of learned indices through agile model reuse. We pre-train learned indices over a set of synthetic (rather than real) datasets and propose a novel approach to reuse these pre-trained models for a new (real) dataset. The synthetic datasets are created to cover a large range of different distributions. Given a new dataset DT, we select the learned index of a synthetic dataset similar to DT, to index DT. We show a bound over the indexing error when a pre-trained index is selected. We further show how our techniques can handle data updates and bound the resultant indexing errors. Experimental results on synthetic and real datasets confirm the effectiveness and efficiency of our proposed lazy (model reuse) approach.",
    "citationCount": 1,
    "referenceCount": 16
}