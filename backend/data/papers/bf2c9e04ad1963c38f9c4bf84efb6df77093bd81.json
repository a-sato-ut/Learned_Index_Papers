{
    "paperId": "bf2c9e04ad1963c38f9c4bf84efb6df77093bd81",
    "title": "Learning Sublinear-Time Indexing for Nearest Neighbor Search",
    "year": 2019,
    "venue": "arXiv.org",
    "authors": [
        "Yihe Dong",
        "P. Indyk",
        "Ilya P. Razenshteyn",
        "Tal Wagner"
    ],
    "doi": null,
    "arxivId": "1901.08544",
    "url": "https://www.semanticscholar.org/paper/bf2c9e04ad1963c38f9c4bf84efb6df77093bd81",
    "isOpenAccess": false,
    "openAccessPdf": "",
    "publicationTypes": [
        "JournalArticle"
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        }
    ],
    "abstract": "Most of the efficient sublinear-time indexing algorithms for the high-dimensional nearest neighbor search problem (NNS) are based on space partitions of the ambient space $\\mathbb{R}^d$. Inspired by recent theoretical work on NNS for general metric spaces [Andoni, Naor, Nikolov, Razenshteyn, Waingarten STOC 2018, FOCS 2018], we develop a new framework for constructing such partitions that reduces the problem to balanced graph partitioning followed by supervised classification. We instantiate this general approach with the KaHIP graph partitioner [Sanders, Schulz SEA 2013] and neural networks, respectively, to obtain a new partitioning procedure called Neural Locality-Sensitive Hashing (Neural LSH). On several standard benchmarks for NNS, our experiments show that the partitions found by Neural LSH consistently outperform partitions found by quantization- and tree-based methods.",
    "citationCount": 12,
    "referenceCount": 38
}